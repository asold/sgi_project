{
  "title": "Implicit Representations of Meaning in Neural Language Models",
  "url": "https://openalex.org/W3169878837",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A4226176131",
      "name": "Li, Belinda Z.",
      "affiliations": [
        "Massachusetts Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A4226865281",
      "name": "Nye, Maxwell",
      "affiliations": [
        "Massachusetts Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1965627067",
      "name": "Andreas, Jacob",
      "affiliations": [
        "Massachusetts Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3133702157",
    "https://openalex.org/W2963372003",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2946359678",
    "https://openalex.org/W2914081143",
    "https://openalex.org/W2141599568",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W3097977265",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W1585104069",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2071089353",
    "https://openalex.org/W2970120757",
    "https://openalex.org/W3138301265",
    "https://openalex.org/W2906152891",
    "https://openalex.org/W2149305746",
    "https://openalex.org/W2970862333",
    "https://openalex.org/W3103054319",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W3027086341",
    "https://openalex.org/W1927197813",
    "https://openalex.org/W3172224317",
    "https://openalex.org/W3034723486",
    "https://openalex.org/W2974273066",
    "https://openalex.org/W2972324944"
  ],
  "abstract": "Does the effectiveness of neural language models derive entirely from accurate modeling of surface word co-occurrence statistics, or do these models represent and reason about the world they describe? In BART and T5 transformer language models, we identify contextual word representations that function as models of entities and situations as they evolve throughout a discourse. These neural representations have functional similarities to linguistic models of dynamic semantics: they support a linear readout of each entity's current properties and relations, and can be manipulated with predictable effects on language generation. Our results indicate that prediction in pretrained neural language models is supported, at least in part, by dynamic representations of meaning and implicit simulation of entity state, and that this behavior can be learned with only text as training data. Code and data are available at https://github.com/belindal/state-probes .",
  "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing, pages 1813–1827\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n1813\nImplicit Representations of Meaning in Neural Language Models\nBelinda Z. Li Maxwell Nye Jacob Andreas\nMassachusetts Institute of Technology\n{bzl,mnye,jda}@mit.edu\nAbstract\nDoes the effectiveness of neural language mod-\nels derive entirely from accurate modeling of\nsurface word co-occurrence statistics, or do\nthese models represent and reason about the\nworld they describe? In BART and T5 trans-\nformer language models, we identify contex-\ntual word representations that function asmod-\nels of entities and situations as they evolve\nthroughout a discourse. These neural represen-\ntations have functional similarities to linguis-\ntic models of dynamic semantics: they support\na linear readout of each entity’s current prop-\nerties and relations, and can be manipulated\nwith predictable effects on language genera-\ntion. Our results indicate that prediction in pre-\ntrained neural language models is supported,\nat least in part, by dynamic representations of\nmeaning and implicit simulation of entity state,\nand that this behavior can be learned with only\ntext as training data.1\n1 Introduction\nNeural language models (NLMs), which place\nprobability distributions over sequences of words,\nproduce contextual word and sentence embeddings\nthat are useful for a variety of language process-\ning tasks (Peters et al., 2018; Lewis et al., 2020).\nThis usefulness is partially explained by the fact\nthat NLM representations encode lexical relations\n(Mikolov et al., 2013) and syntactic structure (Ten-\nney et al., 2019). But the extent to which NLM\ntraining also induces representations of meaning\nremains a topic of ongoing debate (Bender and\nKoller, 2020; Wu et al., 2021). In this paper, we\nshow that NLMs represent meaning in a speciﬁc\nsense: in simple semantic domains, they build rep-\nresentations of situations and entities that encode\nlogical descriptions of each entity’s dynamic state.\n1Code and data are available at https://github.com/\nbelindal/state-probes.\nYou see an open chest. The only thing in the chest is an old key. There is a locked wooden door leading east.\n         Next, you… (c1)   …use the key to unlock the door. (c2)   …drop an apple on the ground. (c3)   …remove an apple from the             chest.\nYou pick up the key.\n(a) chestopen\ncontains\nkey\nyou\n(a )′ \n(b)\nLM encodersemantic probeLM decoder\n(b )′ \ndoorlocked\nchest\nopen\npossesses\nkey\nyou\ndoor\nlockedempty\nFigure 1: Neural language models trained on text\nalone (a–c) produce semantic representations that en-\ncode properties and relations of entities mentioned in\na discourse (a′). Representations are updated when the\ndiscourse describes changes to entities’ state (b′).\nConsider the text in the left column of Fig. 1.\nSentences (a) describe the contents of a room; this\nsituation can be formally characterized by the graph\nof entities, properties, and relations depicted in (a′).\nSentence (b), You pick up the key, causes the situa-\ntion to change: a chest becomes empty, and a key\nbecomes possessed by you rather than contained\nby the chest (b′). None of these changes are explic-\nitly described by sentence (b). Nevertheless, the\nset of sentences that can follow (a)–(b) to form a\nsemantically coherent discourse is determined by\nthis new situation. An acceptable next sentence\nmight feature the person using the key (c1) or per-\nforming an unrelated action (c2). But a sentence in\nwhich the person takes an apple out of the chest (c3)\ncannot follow (a)–(b), as the chest is now empty.\nFormal models of situations (built, like (a′)–(b′),\nfrom logical representations of entities and their\n1814\nattributes) are central to linguistic theories of mean-\ning. NLMs face the problem of learning to generate\ncoherent text like (a–c) without access to any ex-\nplicit supervision for the underlying world state\n(a′)–(b′). Indeed, recent work in NLP points to the\nlack of exposure of explicit representations of the\nworld external to language as prima facie evidence\nthat LMs cannot represent meaning at all, and thus\ncannot in general output coherent discourses like\n(a)–(c) (Bender and Koller, 2020).\nThe present paper can be viewed as an empirical\nresponse to these arguments. It is true that cur-\nrent NLMs do not reliably output coherent descrip-\ntions when trained on data like (a)–(c). But from\ntext alone, even these imperfect NLMs appear to\nlearn implicit models of meaning that are translat-\nable into formal state representations like (a′)–(b′).\nThese state representations capture information like\nthe emptiness of the chest in (b′), which is not\nexplicitly mentioned and cannot be derived from\nany purely syntactic representation of (a)–(b), but\nfollows as a semantically necessary consequence.\nThese implicit semantic models are roughly analo-\ngous to the simplest components of discourse repre-\nsentation theory and related formalisms: they rep-\nresent sets of entities, and update the facts that are\nknown about these entities as sentences are added\nto a discourse. Like the NLMs that produce them,\nthese implicit models are approximate and error-\nprone. Nonetheless, they do most of the things\nwe expect of world models in formal semantics:\nthey are structured, queryable and manipulable. In\nthis narrow sense, NLM training appears to induce\nnot just models of linguistic form, but models of\nmeaning.\nThis paper begins with a review of existing ap-\nproaches to NLM probing and discourse represen-\ntation that serve as a foundation for our approach.\nWe then formalize a procedure for determining\nwhether NLM representations encode representa-\ntions of situations like Fig. 1 (a′)–(b′). Finally, we\napply this approach to BART and T5 NLMs trained\non text from the English-language Alchemy and\nTextWorld datasets. In all cases, we ﬁnd evidence\nof implicit meaning representations that:\n1. Can be linearly decoded from NLM encodings\nof entity mentions.\n2. Are primarily attributable to open-domain pre-\ntraining rather than in-domain ﬁne-tuning.\n3. Inﬂuence downstream language generation.\nWe conclude with a discussion of the implications\nof these results for evaluating and improving factu-\nality and coherence in NLMs.\n2 Background\nWhat do LM representations encode? This pa-\nper’s investigation of state representations builds\non a large body of past work aimed at under-\nstanding how other linguistic phenomena are rep-\nresented in large-scale language models. NLM\nrepresentations have been found to encode syn-\ntactic categories, dependency relations, and coref-\nerence information (Tenney et al., 2019; Hewitt\nand Manning, 2019; Clark et al., 2019). Within\nthe realm of semantics, existing work has identi-\nﬁed representations of word meaning (e.g., ﬁne-\ngrained word senses; Wiedemann et al. 2019) and\npredicate–argument structures like frames and se-\nmantic roles (Kovaleva et al., 2019).\nIn all these studies, the main experimental\nparadigm is probing (Shi et al., 2016; Belinkov\nand Glass, 2019): given a ﬁxed source of repre-\nsentations (e.g. the BERT language model; Devlin\net al. 2019) and a linguistic label of interest (e.g.\nsemantic role), a low-capacity “probe” (e.g a linear\nclassiﬁer) is trained to predict the label from the\nrepresentations (e.g. to predict semantic roles from\nBERT embeddings). A phenomenon is judged to\nbe encoded by a model if the probe’s accuracy can-\nnot be explained by its accuracy when trained on\ncontrol tasks (Hewitt and Liang, 2019) or baseline\nmodels (Pimentel et al., 2020).\nOur work extends this experimental paradigm\nto a new class of semantic phenomena. As in past\nwork, we train probes to recover semantic annota-\ntions, and interpret these probes by comparison to\nnull hypotheses that test the role of the model and\nthe difﬁculty of the task. The key distinction is that\nwe aim to recover a representation of the situation\ndescribed by a discourse rather than representa-\ntions of the sentences that make up the discourse.\nFor example, in Fig. 1, we aim to understand not\nonly whether NLMs encode the (sentence-level)\nsemantic information that there was a picking up\nevent whose patient was you and whose agent was\nthe key—we also wish to understand whether LMs\nencode the consequences of this action for all en-\ntities under discussion, including the chest from\nwhich the key was (implicitly) taken.\nHow might LMs encode meaning? Like in\nother probing work, an attempt to identify neural\n1815\ncontains : The only thing in the chest is an old key.\nx0\ncontains\neaten containseaten\ncontains\n: contains(chest, key) = ? : contains(chest, apple) = ? : eaten(apple) = ?\nϕ0,0ϕ0,1ϕ0,2\nI0 I1\n: contains(chest, key) = T : contains(chest, apple) = F : eaten(apple) = ?\nϕ1,0ϕ1,1ϕ1,2\ncontains\n…\nFigure 2: A collection of possible situations is repre-\nsented as an information state ( I0). Information states\nassign values to propositions φi,j according to whether\nthey are true, false, or undetermined in all the situa-\ntions that make up an information state. Appending\na new sentence discourse causes the information state\nto be updated ( I1). In this case, the sentence The only\nthing in the chest is an old keycauses contains(chest,\nkey) to become true, contains(chest, apple) to be-\ncome false, and leaves eaten(apple) undetermined.\nencodings of entities and situations must begin with\na formal framework for representing them. This\nis the subject of dynamic semantics in linguis-\ntics (Heim, 2008; Kamp et al., 2010; Groenendijk\nand Stokhof, 1991). The central tool for represent-\ning meaning in these approaches is the information\nstate: the set of possible states of the world consis-\ntent with a discourse (I0 and I1 in Fig. 2). Before\nanything is said, all logically consistent situations\nare part of the information state ( I0). Each new\nsentence in a discourse provides an update (that\nconstrains or otherwise manipulates the set of pos-\nsible situations). As shown in the ﬁgure, these\nupdates can affect even unmentioned entities: the\nsentence the only thing in the chest is a key en-\nsures that the proposition contains(chest, x) is\nfalse for all entities xother than the key. This is\nformalized in §3 below.2\nThe main hypothesis explored in this paper is\nthat LMs represent (a particular class of) informa-\ntion states. Given an LM trained on text alone, and\na discourse annotated post-hoc with information\nstates, our probes will try to recover these informa-\ntion states from LM representations. The semantics\nliterature includes a variety of proposals for how\ninformation states should be represented; here, we\nwill represent information states logically, and de-\ncode information states via the truth values that\nthey assign to logical propositions (φi,j in Fig. 2).3\n2See also Yalcin (2014) for an introductory survey.\n3In existing work, one of the main applications of dynamic\nLMs and other semantic phenomena In addi-\ntion to work on interpretability, a great deal of past\nresearch uses language modeling as a pretraining\nscheme for more conventional (supervised) seman-\ntics tasks in NLP. LM pretraining is useful for se-\nmantic parsing (Einolghozati et al., 2019), instruc-\ntion following (Hill et al., 2020), and even image\nretrieval (Ilharco et al., 2021). Here, our primary\nobjective is not good performance on downstream\ntasks, but rather understanding of representations\nthemselves. LM pretraining has also been found\nto be useful for tasks like factoid question answer-\ning (Petroni et al., 2019; Roberts et al., 2020). Our\nexperiments do not explore the extent to which\nLMs encode static background knowledge, but in-\nstead the extent to which they can build representa-\ntions of novel situations described by novel text.\n3 Approach\nOverview We train probing models to test\nwhether NLMs represent the information states\nspeciﬁed by the input text. We speciﬁcally probe\nfor the truth values of logical propositions about\nentities mentioned in the text. For example, in Fig-\nure 1, we test whether a representation of sentences\n(a)–(b) encodes the fact that empty(chest) is true\nand contains(chest, key) is false.\nMeanings as information states To formalize\nthis: given a universe consisting of a set of entities,\nproperties, and relations, we deﬁne a situation as\na complete speciﬁcation of the properties and rela-\ntions of each entity. For example, the box labeledI0\nin Fig. 2 shows three situations involving achest, a\nkey, an apple, an eaten property and a contains\nrelation. In one situation, the chest contains the key\nand the apple is eaten. In another, the chest contains\nthe apple, and the apple is not eaten. In general, a\nsituation assigns a value of true or false to every\nlogical proposition of the form P(x) or R(x,y)\n(e.g. locked(door) or contains(chest, key)).\nNow, given a natural language discourse, we can\nview that discourse as specifying a set of possible\nsituations. In Fig. 2, the sentence x0 picks out\nthe subset of situations in which the chest contains\nthe key. A collection of situations is called an\ninformation state, because it encodes a listener’s\nsemantics is a precise treatment of quantiﬁcation and scope\nat the discourse level. The tasks investigated in this paper\ndo not involve any interesting quantiﬁcation, and rely on the\nsimplest parts of the formalism. More detailed exploration of\nquantiﬁcation in NLMs is an important topic for future study.\n1816\nTT\nkey\nchest\nopen\npossesses\nyou\nempty\nFT\nLM encoder\nLM decoder\nTEXTWORLD\nDrain 2 from the first beaker.\nLocalizer\nLM encoder\nLM decoder\n> unlock wooden door You unlock the wooden door with the old key. \nLocalizer\nx ALCHEMY\nx\nE(x) E(x)\nClassifierClassifier\nEmbed?\nEmbed\nThe first beaker has 1 green, the second beaker has 2 red, the third beaker has 3 red. Pour the last red beaker into beaker 1. Mix.\nhas-4-brown(beaker 1)\nhas-2-red(beaker 2) matches(old key, wooden door)\nin(old key, chest)open(chest)\nYou see an open chest. The only thing in the chest is an old key. There is a locked wooden door leading east. > take old key You take the old key from the chest.\nFigure 3: Overview of the probe model. Left: Alchemy. Right: Textworld. The LM is ﬁrst trained to generate the\nnext instruction from prior context (left side, both ﬁgures). Next, the LM encoder is frozen and a probe is trained\nto recover (the truthfulness of) propositions about the current state from speciﬁc tokens of encoder outputs.\nknowledge of (and uncertainty about) the state of\nthe world resulting from the events described in a\ndiscourse.4 In a given information state, the value\nof a proposition might betrue in all situations, false\nin all situations, or unknown: true in some but\nfalse in others. An information state (or an NLM\nrepresentation) can thus be characterized by the\nlabel it assigns to every proposition.\nProbing for propositions We assume we are\ngiven:\n• A sequence of sentences x1:n = [x1,...,x n].\n• For each i, the information state Ii that results\nfrom the sentences x1:i. We write I(φ) ∈\n{T,F, ?}for the value of the proposition φin\nthe information state I.\n• A language model encoder Ethat maps sen-\ntences to sequences of d-dimensional word\nrepresentations.\nTo characterize the encoding of semantic informa-\ntion in E(x), we design a semantic probe that\ntries to recover the contents of Ii from E(x1:i)\nproposition-by-proposition. Intuitively, this probe\naims to answer three questions: (1)How is the truth\nvalue of a given proposition φencoded? (Linearly?\nNonlinearly? In what feature basis?) (2) Where is\ninformation about φencoded? (Distributed across\nall token embeddings? Local to particular tokens?)\n(3) How well is semantic information encoded?\n(Can it be recovered better than chance? Perfectly?)\n4An individual sentence is associated with acontext change\npotential: a map from information states to information states.\nThe probe is built from three components, each\nof which corresponds to one of the questions above:\n1. A proposition embedder embed : L →Rd\n(where Lis the set of logical propositions).\n2. A localizer loc : L ×Rd → Rd which\nextracts and aggregates LM representations\nas candidates for encoding φ. The localizer\nextracts tokens of E(x) at positions corre-\nsponding to particular tokens in the under-\nlying text x. We express this in notation as\nE(x)[*], where * is a subsequence of x. (For\nexample, if x = “the third beaker is empty”.\nE(x) = [v1,v2,v3,v4,v5] has one vector per\ntoken. E(x)[“third beaker”] = [v2,v3].)\n3. A classiﬁer clsθ : Rd ×Rd → {T,F, ?},\nwhich takes an embedded proposition and a\nlocalized embedding, and predicts the truth\nvalue of the proposition.\nWe say that a proposition φis encoded by E(x) if:\nclsθ(embed(φ),loc(φ,E(x))) = I(φ) . (1)\nGiven a dataset of discourses D, we attempt to ﬁnd\na classiﬁer parameters θ from which all proposi-\ntions can be recovered for all sentences in Eq. (1).\nTo do so, we label each with the truth/falsehood\nof every relevant proposition. We then train the\nparameters of a clsθ on a subset of these propo-\nsitions and test whether it generalizes to held-out\ndiscourses.\n1817\n4 Experiments\nOur experiments aim to discover to what extent\n(and in what manner) information states are en-\ncoded in NLM representations. We ﬁrst present a\nspeciﬁc instantiation of the probe that allows us to\ndetermine how well information states are encoded\nin two NLMs and two datasets (§4.2); then provide\na more detailed look at where speciﬁc propositions\nare encoded by varying loc (§4.3). Finally, we de-\nscribe an experiment investigating the causal role\nof semantic representations by directly manipulat-\ning E(x) (§4.4).5\n4.1 Preliminaries\nModel In all experiments, the encoder Ecomes\nfrom a BART (Lewis et al., 2020) or T5 (Raffel\net al., 2020) model. Except where noted, BART\nis pretrained on OpenWebText, BookCorpus, CC-\nNews, and Stories (Lewis et al., 2020), T5 is pre-\ntrained on C4 (Raffel et al., 2020), and both are\nﬁne-tuned on the TextWorld or Alchemy datasets\ndescribed below. Weights of Eare frozen during\nprobe training.\nData: Alchemy Alchemy, the ﬁrst dataset\nused in our experiments, is derived from the\nSCONE (Long et al., 2016) semantic parsing tasks.\nWe preserve the train / development split from the\noriginal dataset (3657 train / 245 development).\nEvery example in the dataset consists of a human-\ngenerated sequence of instructions to drain, pour,\nor mix a beaker full of colored liquid. Each instruc-\ntion is annotated with the ground-truth state that\nresults from following that instruction (Figure 3).\nWe turn Alchemy into a language modeling\ndataset by prepending a declaration of the initial\nstate (the initial contents of each beaker) to the\nactions. The initial state declaration always fol-\nlows a ﬁxed form (“the ﬁrst beaker has [amount]\n[color], the second beaker has [amount] [color],\n...”). Including it in the context provides enough\ninformation that it is (in principle) possible to deter-\nministically compute the contents of each beaker af-\nter each instruction. The NLM is trained to predict\nthe next instruction based on a textual description\nof the initial state and previous instructions.\nThe state representations we probe for in\nAlchemy describe the contents of each beaker. Be-\ncause execution is deterministic and the initial state\n5Sections here are also discussed in more detail in Ap-\npendix A.1 (for §4.1), A.2 (for §4.2), and A.3 (for §4.3).\nis fully speciﬁed, the information state associated\nwith each instruction preﬁx consists of only a single\npossible situation, deﬁned by a set of propositions:\nΦ =\n{\nhas-v-c(b) :\nb∈{beaker 1,beaker 2,... },\nv∈1..4,\nc∈{red,orange,yellow,... }\n}\n.\n(2)\nIn the experiments below, it will be useful to have\naccess to a natural language representation of each\nproposition. We denote this:\nNL(has-v-c(b)) = “the bbeaker has vc” . (3)\nTruth values for each proposition in each instruc-\ntion sequence are straightforwardly derived from\nground-truth state annotations in the dataset.\nData: TextWorld TextWorld (Cˆot´e et al., 2018)\nis a platform for generating synthetic worlds for\ntext-based games, used to test RL agents. The game\ngenerator produces rooms containing objects, sur-\nfaces, and containers, which the agent can interact\nwith in various predeﬁned ways.\nWe turn TextWorld into a language modeling\ndataset by generating random game rollouts follow-\ning the “simple game” challenge, which samples\nworld states with a ﬁxed room layout but chang-\ning object conﬁgurations. For training, we sample\n4000 rollouts across 79 worlds, and for develop-\nment, we sample 500 rollouts across 9 worlds. Con-\ntexts begin with a description of the room that the\nplayer currently stands in, and all visible objects in\nthat room. This is followed by a series of actions\n(preceded by >) and game responses (Fig. 3).\nThe NLM is trained to generate both an action\nand a game response from a history of interactions.\nWe probe for both the properties of and relations\nbetween entities at the end of a sequence of actions.\nUnlike Alchemy, these may be undetermined, as\nthe agent may not have explored the entire envi-\nronment by the end of an action sequence. (For\nexample, in Fig. 3, the truth value of matches(old\nkey, door) is unknown). The set of propositions\navailable in the TextWorld domain has form\nΦ ={p(o) : o∈O,p ∈P}\n∪{r(o1,o2) : o1,o2 ∈O,r ∈R} (4)\nfor objects O = {player,chest,... }, proper-\nties P = {open,edible,... }and relations R =\n1818\nAlchemy TextWorld\nState EM Entity EM State EM Entity EM\nBART T5 BART T5 BART T5 BART T5\nmain probe (§4.2) 7.6 14.3 75.0 75.5 48.7 53.8 95.2 96.9\n+pretrain, -ﬁne-tune 1.1 4.3 69.3 74.1 23.2 38.9 91.1 94.3\nbaselines & -pretrain, +ﬁne-tune 1.5 62.8 14.4 81.2\nmodel ablations random init. 0.4 64.9 11.3 74.5\n(§4.2) no change 0.0 62.7 9.73 74.1\nno LM 0.0 32.4 1.77 81.8\nlocality (§4.3) ﬁrst - - 49.6 51.5 93.6 95.9\nlast - - 55.1 58.6 96.5 97.6\nTable 1: Probing results. For each dataset, we report Entity EM , the % of entities for which all propositions\nwere correct, and State EM, the % of states for which all proposition were correct. For non-pretrained baselines\n(-pretrain, +ﬁne-tune and random init.), we report the single best result from all model conﬁgurations examined.\nSemantic state information can be recovered at the entity level from both language models on both datasets, and\nsuccessful state modeling appears to be primarily attributable to pretraining rather than ﬁne-tuning.\n{on,in,... }. We convert propositions to natural\nlanguage descriptions as:\nNL(p(o)) = “the pis o”\nNL(r(o1,o2)) = “the o1 is ro2” . (5)\nThe set of propositions and their natural language\ndescriptions are pre-deﬁned by TextWorld’s sim-\nulation engine. The simulation engine also gives\nus the set of true propositions, from which we can\ncompute the set of false and unknown propositions.\nEvaluation We evaluate probes according to two\nmetrics. Entity Exact-Match (EM) ﬁrst aggre-\ngates the propositions by entity or entity pair, then\ncounts the percentage of entities for which all\npropositions were correctly labeled. State EM\naggregates propositions by information state (i.e.\ncontext), then counts the percentage of states for\nwhich all facts were correctly labeled.\n4.2 Representations encode entities’ ﬁnal\nproperties and relations\nWith this setup in place, we are ready to ask our ﬁrst\nquestion: is semantic state information encoded at\nall by pretrained LMs ﬁne-tuned on Alchemy and\nTextWorld? We instantiate the probing experiment\ndeﬁned in §3 as follows:\nThe proposition embedder converts each\nproposition φ∈Φ to its natural language descrip-\ntion, embeds it using the same LM encoder that is\nbeing probed, then averages the tokens:\nembed(φ) = mean(E(NL(φ))) (6)\nThe localizer associates each proposition φwith\nspeciﬁc tokens corresponding to the entity or en-\ntities that φdescribes, then averages these tokens.\nIn Alchemy, we average over tokens in theinitial\ndescription of the beaker in question. For example,\nlet xbe the discourse in Figure 3 (left) and φbe a\nproposition about the ﬁrst beaker. Then, e.g.,\nloc(has-1-red(beaker 1),E(x)) =\nmean(E(x)[The ﬁrst beaker has 2 green,]). (7)\nIn TextWorld, we average over tokens inall men-\ntions of each entity. Letting xbe the discourse in\nFigure 3 (right), we have:\nloc(locked(wooden door),E(x)) =\nmean(E(x)[wooden door]) . (8)\nRelations, with two arguments, are localized by\ntaking the mean of the two mentions.\nFinally, the classiﬁer is a linear model which\nmaps each NLM representation and proposition to\na truth value. In Alchemy, a linear transformation\nis applied to the NLM representation, and then the\nproposition with the maximum dot product with\nthat vector is labelled T (the rest are labelled F).\nIn TextWorld, a bilinear transformation maps each\n(proposition embedding, NLM representation) pair\nto a distribution over {T,F, ?}.\nAs noted by Liang and Potts (2015), it is easy\nto construct examples of semantic judgments that\ncannot be expressed as linear functions of purely\nsyntactic sentence representations. We expect (and\nverify with ablation experiments) that this probe\nis not expressive enough to compute information\nstates directly from surface forms, and only expres-\nsive enough to read out state information already\ncomputed by the underlying LM.\n1819\nT\nthe [pos] be aker has [amount][color], the [pos]+1 [amount][color]\n39.4 41.4 42.1 41.5 58.5 68.6 74.3 64.8 45.1 35.0 35.7 34.5\n42.1 58.1 42.4 40.7 64.8 66.7 67.9 63.5 42.1 41.9 32.4 32.9\nthe third be aker has 4 blue , the fourth 2 red\n… Drain 2 from beaker 3.\nProbe\nLocalizer has-2-blue(beaker3)\n58.5% / 64.8% accuracy\nEmbed\n(a)\n(B)\n(T5)\nFigure 4: Locality of information state in Alchemy. We\nfocus on the initial state declaration. Linear probes\nare trained to decode the ﬁnal state of a beaker condi-\ntioned on the individual contextualized representations\nfor each word. Separate probes are trained for each\nposition. Tokens in the same relative position (with re-\nspect to the target beaker) are superimposed and the av-\neraged entity EM is reported in (B) for BART and (T5)\nfor T5. (a) shows the paradigm on a concrete example.\nResults Results are shown in Table 1. A probe\non T5 can exactly recover 14.3% of information\nstates in Alchemy, and 53.8% in TextWorld. For\ncontext, we compare to two baselines: a no LM\nbaseline, which simply predicts the most frequent\nﬁnal state for each entity, and a no change baseline,\nwhich predicts that the entity’s ﬁnal state in the\ndiscourse will be the same as its initial state. The\nno LM baseline is correct 0% / 1.8% of the time\nand the no change baseline is correct 0% / 9.7% of\nthe time—substantially lower than the main probe.\nTo verify that this predictability is a property\nof the NLM representations rather than the text\nitself, we apply our probe to a series of model\nablations. First, we evaluate a randomly initial-\nized transformer rather than the pretrained and\nﬁne-tuned model, which has much lower probe\naccuracy. To determine whether the advantage\nis conferred by LM pretraining or ﬁne-tuning,\nwe ablate either open-domain pretraining, in a\n-pretrain,+ﬁne-tune ablation, or in-domain ﬁne-\ntuning, in a +pretrain,-ﬁne-tune ablation. (For all\nexperiments not using a pretrained model check-\npoint, we experimented with both a BART-like and\nT5-like choice of depth and hidden size, and found\nthat the BART-like model performed better.) While\nboth ﬁne-tuning and pretraining contribute to the\nﬁnal probe accuracy, pretraining appears to play a\nmuch larger role: semantic state can be recovered\nwell from models with no in-domain ﬁne-tuning.\nFinally, we note that there may be lexical over-\nlap between the discourse and natural language\ndescriptions of propositions. How much of the\nprobe’s performance can be attributed to this over-\nlap? In Alchemy, the no change baseline (which\nState EM Entity EM\nBART T5 BART T5\nremap 50.2 50.4 88.9 93.2\nmain probe 50.2 53.8 91.3 94.6\nTable 2: Locality of information state in TextWorld\n(T5). Entity state information tends to be slightly more\npresent in mentions of the target entity (main probe)\nrather than of other entities (remap), but not by much.\nperforms much worse than our probe) also acts\nas a lexical overlap baseline—there will be lexi-\ncal overlap between true propositions and the ini-\ntial state declaration only if the beaker state is un-\nchanged. In TextWorld, each action induces mul-\ntiple updates, but can at most overlap with one of\nits affected propositions (e.g. You close the chest\ncauses closed(chest) and ¬open(chest), but\nonly overlaps with the former). Moreover, only\n∼50% of actions have lexical overlap with any\npropositions at all. Thus, lexical overlap cannot\nfully explain probe performance in either domain.\nIn summary, pretrained NLM representations\nmodel state changes and encode semantic informa-\ntion about entities’ ﬁnal states.\n4.3 Representations of entities are local to\nentity mentions\nThe experiment in §4.2 assumed that entity state\ncould be recovered from a ﬁxed set of input tokens.\nNext, we conduct a more detailed investigation\ninto where state information is localized. To this\nend, we ask two questions: ﬁrst, can we assume\nstate information is localized in the corresponding\nentity mentions, and second, if so, which mention\nencodes the most information, and what kind of\ninformation does it encode?\n4.3.1 Mentions or other tokens?\nWe ﬁrst contrast tokens within mentions of the tar-\nget entity to tokenselsewhere in the input discourse.\nIn Alchemy, each beaker b’s initial state declara-\ntion is tokenized as: toksb = {theb, [position]b,\nbeb, akerb, hasb, [volume]b, [color]b, ,b}, where b\nsigniﬁes the beaker position. Rather than pooling\nthese tokens together (as in §4.2), we construct a\nlocalizer ablation that associates beaker b’s state\nwith single tokens tin either the initial mention of\nbeaker b, or the initial mention of other beakers at\nan integer offset ∆. For each (t,∆) pair, we con-\nstruct a localizer that matches propositions about\nbeaker b with tb+∆. For example, the (has,+1)\nlocalizer associates the third beaker’s ﬁnal state\n1820\nwith the vector in E(x) at the position of the “has”\ntoken in the fourth beaker has 2 red.\nIn TextWorld, which does not have such easily\ncategorizable tokens, we investigate whether infor-\nmation about the state of an entity is encoded in\nmentions of different entities. We sample a random\nmapping remap between entities, and construct a\nlocalizer ablation in which we decode propositions\nabout wfrom mentions of remap(w). For example,\nwe probe the value ofopen(chest) from mentions\nof old key. These experiments use a different eval-\nuation set—we restrict evaluation to the subset of\nentities for which both wand remap(w) appear in\nthe discourse. For comparability, we re-run the\nmain probe on this restricted set.6\nResults Fig. 4 shows the locality of BART and\nT5 in the Alchemy domain. Entity EM is highest\nfor words corresponding to the correct beaker, and\nspeciﬁcally for color words. Decoding from any\ntoken of an incorrect beaker barely outperforms the\nno LM baseline (32.4% entity EM). In TextWorld,\nTable 2 shows that decoding from a remapped en-\ntity is only 1-3% worse than decoding from the\nright one. Thus, the state of an entity eis (roughly)\nlocalized to tokens in mentions of e, though the\ndegree of locality is data- and model-dependent.\n4.3.2 Which mention?\nTo investigate facts encoded in different mentions\nof the entity in question, we experiment with decod-\ning from the ﬁrst and last mentions of the entities\nin x. The form of the localizer is the same as 4.2,\nexcept instead of averaging across all mentions of\nentities, we use the ﬁrst mention or the last mention.\nWe also ask whether relational propositions can be\ndecoded from just one argument (e.g.,in(old key,\nchest) from just mentions of old key, rather than\nthe averaged encodings of old key and chest).\nResults As shown in Table 1, in TextWorld, prob-\ning the last mention gives the highest accuracy. Fur-\nthermore, as Table 3 shows, relational facts can be\ndecoded from either side of the relation.\n4.4 Changes to entity representations cause\nchanges in language model predictions\nThe localization experiments in Section 4.3 indi-\ncate that state information is localized within con-\n6The remap and ∆ ̸= 0 probes described here are analo-\ngous to control tasks (Hewitt and Liang, 2019). They measure\nprobes’ abilities to predict labels that are structurally similar\nbut semantically unrelated to the phenomenon of interest.\nRelations Properties\nBART T5 BART T5\n1-arg 41.4 55.5 83.2 90.9\n2-arg 49.6 54.4 94.5 98.5\nrandom init. 21.9 35.4\nTable 3: State EM for decoding each type of fact (re-\nlations vs. properties), with each type of probe (1- vs.\n2- argument decoding). Though decoding from two en-\ntities is broadly better, one entity still contains a non-\ntrivial amount of information, even regarding relations.\ntextual representations in predictable ways. This\nsuggests that modifying the representations them-\nselves could induce systematic and predictable\nchanges in model behavior. We conduct a series\nof causal intervention experiments in the Alchemy\ndomain which measure effect of manipulating en-\ncoder representations on NLM output. We replace\na small subset of token representations with those\nfrom a different information state, and show that\nthis causes the model to behave as if it were in the\nnew information state.7\nA diagram of the procedure is shown in Fig. 5.\nWe create two discourses, x1 and x2, in which\none beaker’s ﬁnal volume is zero. Both discourses\ndescribe the same initial state, but for each xi,\nwe append the sentence drain vi from beaker bi,\nwhere vi is the initial volume of beaker bi’s con-\ntents. Though the underlying initial state tokens are\nthe same, we expect the contextualized representa-\ntion C1 = E(x1)[the ith beaker ... ] to differ from\nC2 = E(x2)[the ith beaker ... ] due to the different\nﬁnal states of the beakers. Let CONT (x) denote\nthe set of sentences constituting semantically ac-\nceptable continuations of a discourse preﬁx x. (In\nFig. 1, CONT (a, b) contains c1 and c2 but not c3.)8\nIn Alchemy, CONT (x1) should not contain mixing,\ndraining, or pouring actions involving b1 (similarly\nfor CONT (x2) and b2). Decoder samples given Ci\nshould fall into CONT (xi).\nFinally, we replace the encoded description of\nbeaker 2 in C1 with its encoding from C2, creating\na new representation Cmix. Cmix was not derived\nfrom any real input text, but implicitly represents\na situation in which both b1 and b2 are empty. A\ndecoder generating from Cmix should generate in-\nstructions in CONT (x1) ∩CONT (x2) to be consis-\ntent with this situation.\n7This experiment is inspired by Geiger et al. (2020).\n8In order to automate evaluation of consistency, we use\na version of Alchemy with synthetically generated text. The\nunderlying LM has also been ﬁne-tuned on synthetic data.\n1821\n(g1) Mix the first beaker.\n(g3) Mix the third beaker.(g2) Mix the second beaker.\nLM decoder\n(C2)\n(C1)\nLM encoder\nThe first beaker has 2 green, the second beaker has 2 red, the third beaker has 1 green. Drain 2 from first beaker.\nThe first beaker has 2 green, the second beaker has 2 red, the third beaker has 1 green. Drain 2 from second beaker.\nInconsistentInconsistentConsistent\n(Cmix)\nLM encoder\nFigure 5: Intervention experiments. Construct C1,C2\nby appending text to empty one of the beakers (e.g.\nthe ﬁrst and second beakers) and encoding the result.\nThen, create Cmix by taking encoded tokens from C1\nand replacing the encodings corresponding to the sec-\nond beaker’s initial state declaration with those from\nC2. This induces the LM to model both the ﬁrst and\nsecond beakers as empty, and the LM decoder should\ngenerate actions consistent with this state.\n% of generations within...\nCONT (x1) ∩ CONT (x2) CONT (x1) CONT (x2)\nBART T5 BART T5 BART T5\nC1 20.4 37.9 96.2 93.0 21.6 40.8\nC2 16.1 29.1 24.1 37.9 87.7 87.2\nCmix 57.7 75.4 86.7 86.8 64.8 84.5\nTable 4: Results of intervention experiments. Though\nimperfect, generations from Cmix are more often con-\nsistent with both contexts compared to those from C1\nor C2, indicating that its underlying information state\n(approximately) models both beakers as empty.\nResults We generate instructions conditioned on\nCmix and check whether they are in the expected\nsets. Results, shown in Table 4, align with this pre-\ndiction. For both BART and T5, substantially more\ngenerations from Cmix fall within CONT (x1) ∩\nCONT (x2) than from C1 or C2. Though imper-\nfect (compared to C1 generations within CONT (x1)\nand C2 generations within CONT (x2)), this sug-\ngests that the information state associated with the\nsynthetic encoding Cmix is (approximately) one in\nwhich both beakers are empty.\n5 Limitations\n...of large NLMs: It is important to emphasize\nthat both LM output and implicit state representa-\ntions are imperfect: even in the best case, complete\ninformation states can only be recovered 53.8% of\nthe time in tasks that most humans would ﬁnd very\nsimple. (Additional experiments described in Ap-\npendix A.5 offer more detail about these errors.)\nThe success of our probing experiments should not\nbe taken to indicate that the discovered semantic\nrepresentations have anything near the expressive-\nness needed to support human-like generation.\n...of our experimental paradigm: While our\nprobing experiments in §4.2 provide a detailed pic-\nture of structured state representations in NLMs,\nthe intervention experiments in §4.4 explain the re-\nlationship between these state representations and\nmodel behavior in only a very general sense. They\nleave open the key question of whether errors in\nlanguage model prediction are attributable to er-\nrors in the underlying state representation. Finally,\nthe situations we model here are extremely simple,\nfeaturing just a handful of objects. Thought experi-\nments on the theoretical capabilities of NLMs (e.g.\nBender and Koller’s “coconut catapult”) involve\nfar richer worlds and more complex interactions.\nAgain, we leave for future work the question of\nwhether current models can learn to represent them.\n6 Conclusion\nEven when trained only on language data, NLMs\nencode simple representations of meaning. In ex-\nperiments on two domains, internal representations\nof text produced by two pretrained language mod-\nels can be mapped, using a linear probe, to repre-\nsentations of the state of the world described by the\ntext. These internal representations are structured,\ninterpretably localized, and editable. This ﬁnding\nhas important implications for research aimed at\nimproving factuality and and coherence in NLMs:\nfuture work might probe LMs for the the states and\nproperties ascribed to entities the ﬁrst time they are\nmentioned (which may reveal biases learned from\ntraining data; Bender et al. 2021), or correct errors\nin generation by directly editing representations.\nAcknowledgments\nThanks to Ekin Aky ¨urek, Evan Hernandez, Joe\nO’Connor, and the anonymous reviewers for feed-\nback on early versions of this paper. MN is sup-\nported by a NSF Graduate Research Fellowship.\nThis work was supported by a hardware donation\nfrom NVIDIA under the NV AIL program.\n1822\nImpact Statement\nThis paper investigates the extent to which neu-\nral language models build meaning representations\nof the world, and introduces a method to probe\nand modify the underlying information state. We\nexpect this can be applied to improve factuality, co-\nherence, and reduce bias and toxicity in language\nmodel generations. Moreover, deeper insight into\nhow neural language models work and what exactly\nthey encode can be important when deploying these\nmodels in real-world settings.\nHowever, interpretability research is by nature\ndual-use and improve the effectiveness of models\nfor generating false, misleading, or abusive lan-\nguage. Even when not deliberately tailored to\ngeneration of harmful language, learned seman-\ntic representations might not accurately represent\nthe world because of errors both in prediction (as\ndiscussed in §5) and in training data.\nReferences\nYonatan Belinkov and James Glass. 2019. Analysis\nmethods in neural language processing: A survey.\nTransactions of the Association for Computational\nLinguistics, 7:49–72.\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM Confer-\nence on Fairness, Accountability, and Transparency,\npages 610–623.\nEmily M. Bender and Alexander Koller. 2020. Climb-\ning towards NLU: On meaning, form, and under-\nstanding in the age of data. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 5185–5198, Online. As-\nsociation for Computational Linguistics.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019. What does BERT\nlook at? an analysis of BERT’s attention. In Pro-\nceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for\nNLP, pages 276–286, Florence, Italy. Association\nfor Computational Linguistics.\nMarc-Alexandre Cˆot´e, ´Akos K´ad´ar, Xingdi Yuan, Ben\nKybartas, Tavian Barnes, Emery Fine, James Moore,\nRuo Yu Tao, Matthew Hausknecht, Layla El Asri,\nMahmoud Adada, Wendy Tay, and Adam Trischler.\n2018. Textworld: A learning environment for text-\nbased games. CoRR, abs/1806.11532.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nArash Einolghozati, Panupong Pasupat, Sonal Gupta,\nRushin Shah, Mrinal Mohit, Mike Lewis, and Luke\nZettlemoyer. 2019. Improving semantic parsing for\ntask oriented dialog.\nAtticus Geiger, Kyle Richardson, and Christopher\nPotts. 2020. Neural natural language inference mod-\nels partially embed theories of lexical entailment and\nnegation. In Proceedings of the Third BlackboxNLP\nWorkshop on Analyzing and Interpreting Neural Net-\nworks for NLP, pages 163–173, Online. Association\nfor Computational Linguistics.\nJeroen Groenendijk and Martin Stokhof. 1991. Dy-\nnamic predicate logic. Linguistics and Philosophy ,\n14:39–100.\nIrene Heim. 2008. File Change Semantics and the Fa-\nmiliarity Theory of Deﬁniteness, pages 223 – 248.\nJohn Hewitt and Percy Liang. 2019. Designing and\ninterpreting probes with control tasks. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 2733–2743, Hong\nKong, China. Association for Computational Lin-\nguistics.\nJohn Hewitt and Christopher D. Manning. 2019. A\nstructural probe for ﬁnding syntax in word repre-\nsentations. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4129–4138, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nFelix Hill, Sona Mokra, Nathaniel Wong, and Tim\nHarley. 2020. Human instruction-following with\ndeep reinforcement learning via transfer-learning\nfrom text.\nGabriel Ilharco, Rowan Zellers, Ali Farhadi, and Han-\nnaneh Hajishirzi. 2021. Probing contextual lan-\nguage models for common ground with visual rep-\nresentations. In Proceedings of the 2021 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, pages 5367–5377, Online. As-\nsociation for Computational Linguistics.\nHans Kamp, Josef Genabith, and Uwe Reyle. 2010.\nDiscourse Representation Theory, pages 125–394.\nOlga Kovaleva, Alexey Romanov, Anna Rogers, and\nAnna Rumshisky. 2019. Revealing the dark secrets\nof BERT. In Proceedings of the 2019 Conference on\n1823\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n4365–4374, Hong Kong, China. Association for\nComputational Linguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880, Online. Association\nfor Computational Linguistics.\nPercy Liang and Christopher Potts. 2015. Bringing ma-\nchine learning and compositional semantics together.\nAnnu. Rev. Linguist., 1(1):355–376.\nReginald Long, Panupong Pasupat, and Percy Liang.\n2016. Simpler context-dependent logical forms via\nmodel projections. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1456–\n1465, Berlin, Germany. Association for Computa-\ntional Linguistics.\nTomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.\n2013. Linguistic regularities in continuous space\nword representations. In Proceedings of the 2013\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies , pages 746–751, Atlanta,\nGeorgia. Association for Computational Linguistics.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nFabio Petroni, Tim Rockt ¨aschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 2463–2473, Hong Kong, China. As-\nsociation for Computational Linguistics.\nTiago Pimentel, Naomi Saphra, Adina Williams, and\nRyan Cotterell. 2020. Pareto probing: Trading off\naccuracy for complexity. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 3138–3153, On-\nline. Association for Computational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring\nthe limits of transfer learning with a uniﬁed text-to-\ntext transformer. Journal of Machine Learning Re-\nsearch, 21(140):1–67.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 5418–5426,\nOnline. Association for Computational Linguistics.\nXing Shi, Inkit Padhi, and Kevin Knight. 2016. Does\nstring-based neural MT learn source syntax? In Pro-\nceedings of the 2016 Conference on Empirical Meth-\nods in Natural Language Processing , pages 1526–\n1534, Austin, Texas. Association for Computational\nLinguistics.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4593–\n4601, Florence, Italy. Association for Computational\nLinguistics.\nGregor Wiedemann, Steffen Remus, Avi Chawla, and\nChris Biemann. 2019. Does BERT make any sense?\nInterpretable word sense disambiguation with con-\ntextualized embeddings.\nZhaofeng Wu, Hao Peng, and Noah A. Smith. 2021.\nInfusing Finetuning with Semantic Dependencies.\nTransactions of the Association for Computational\nLinguistics, 9:226–242.\nSeth Yalcin. 2014. Introductory notes on dynamic se-\nmantics.\n1824\nA Appendix\nA.1 Datasets Details (§4.1)\nAlchemy Alchemy is downloadable at https://\nnlp.stanford.edu/projects/scone/. Alchemy propo-\nsitions are straightforwardly derived from existing\nlabels in the dataset. We preserve the train/dev\nsplit from the original dataset (3657 train/245 dev),\nwhich we use for training the underlying LM and\nthe probe. In subsequent sections, we include ad-\nditional results from a synthetic dataset that we\ngenerated (3600 train/500 dev), where actions are\ncreated following a ﬁxed template, making it easy\nto evaluate consistency.\nTextworld We generate a set of worlds for train-\ning, and a separate set of worlds for testing. We\nobtain transcripts from three agents playing on\neach game: a perfect agent and two (semi-)random\nagents, which intersperse perfect actions with sev-\neral steps of random actions. For training, we\nsample 4000 sequences from the 3 agents across\n79 worlds. For development, we sample 500 se-\nquences from the 3 agents across 9 worlds.\nDuring game generation, we are given the set\nof all propositions that are True in the world, and\nhow the set updates after each player action. How-\never, the player cannot infer the full state before\ninteracting with and seeing all objects, and neither\n(we suspect) can a language model trained on par-\ntial transcripts. For example, a player that starts in\nthe bedroom cannot infer is-in(refrigerator,\nkitchen) without ﬁrst entering the kitchen. One\nsolution would be to hard-code rules–a player can\nonly know about the states of entities it has directly\naffected or seen. However, since synthetically-\ngenerated worlds might share some commonalities,\na player that has played many games before (or an\nLM trained on many transcripts) might be able to\ndraw particular conclusions about entities in unseen\nworlds, even before interacting with them.\nTo deal with these factors, we train a labeller\nmodel label to help us classify propositions as\nknown true, known false, and unknown. We gener-\nate a training set (separate from the training set for\nthe probe and probed LM) to train the labeller. We\nagain use BART, but we give it the text transcripts\nand train it to directly decode the full set of True\npropositions and False proposition by the end of\nthe transcript (recall we have the ground-truth full\nTrue set, and we label all propositions that aren’t\nin the True set as False). This allows the labeller\nmodel to pick up both patterns between the dis-\ncourse and its information state , as well as infer\ngeneral patterns among various discourses. Thus,\non unknown worlds, given text T, if proposition A\nis True most or all of the time given T, the model\nshould be conﬁdent in predicting A. We label Aas\nTrue in these cases. However, if proposition Ais\nTrue only half of the time given T, the model is un-\nconﬁdent. We label Aas Unknown in these cases.\nThus, we create ourunknown set using a conﬁdence\nthreshold τ on label’s output probability.\nA.2 Probe Details + Additional Results (§4.2)\nBelow, we give a more detailed account of our prob-\ning paradigm in each domain, including equations.\nAlchemy Probe The proposition embedder con-\nverts propositions φ to natural language descrip-\ntions ˜φ(“the bth beaker has v c”) and encodes\nthem with the BART or T5 LM encoder.\nGiven a proposition has-v-c(b), the localizer\nloc maps has-v-c(b) to tokens in E(x) that cor-\nresponding to the initial state of beaker b. Since x\nalways begins with an initial state declaration of\nthe form “the ﬁrst beaker has [amount] [color], the\nsecond beaker has [amount] [color], ...”, tokens at\nposition 8b−8 ···8b−1 of xcorrespond to the\ninitial state of beaker b. (Each state has 8 tokens:\n‘the’, ‘bth’, ‘be’, ‘aker’, ‘has’, ‘[amount]’, ‘[color]’,\n‘,’). Thus,\nloc(has-v-c(b),E(x)) =\nmean(E(x)[8b−8],··· ,E(x)[8b−1])\nWe train a linear probe clsθ to predict the ﬁnal\nbeaker state given the encoded representation E(x)\nof text x. For our probe, we learn linear projec-\ntion weights W(d×d) and bias b(d) to maximize the\ndot product between the LM representation and\nthe embedded proposition. Formally, it computes\nv(max)\nb ,c(max)\nb as\nv(max)\nb ,c(max)\nb = arg max\nv′,c′\n(\nembed(has-v′-c′(b))·\n(W ·loc(has-v′-c′(b),E(x)) + b)\n) (9)\nIn other words, v(max)\nb ,c(max)\nb are the values of v\nand cthat maximize this dot product. The probe\nthen returns\nclsθ(embed(has-v-c(b)),loc(has-v-c(b),E(x)))\n=\n{\nT if v,c = v(max)\nb ,c(max)\nb\nF if v,c ̸= v(max)\nb ,c(max)\nb\n(10)\n1825\nFigure 6: Alchemy locality - full results. Top: T5, Finetuned+probed on real data. Middle: BART, Fine-\ntuned+probed on real data. Bottom: BART, Finetuned+probed on synthetic data. We note that for the synthetic\ndata, accurate decoding is possible from a much wider set of tokens, but all still correspond to the relevant beaker.\nNote that clsθ selects the optimal ﬁnal state per\nbeaker, from the set of all possible states of beaker\nb, taking advantage of the fact that only one propo-\nsition can be true per beaker.\nTextworld Probe For Textworld, the proposition\nembedder converts propositions φto natural lan-\nguage descriptions ˜φ(“the o is p” for properties\nand “the o1 is r o2” for relations) and encodes\nthem with the BART or T5 LM encoder.\nGiven a proposition p(o) pertaining to an entity\nor r(o1,o2) pertaining to an entity pair, we deﬁne\nlocalizer loc to map the proposition to tokens of\nE(x) corresponding to all mentions of its argu-\nments, and averages across those tokens:\nall idx(e) = set of indices of xcorrespond\n-ing to all instances of e\nloc(r(o1,o2),E(x)) = mean\n(\nE(x)[all idx(o1)∪\nall idx(o2)]\n)\nloc(p(o),E(x)) = mean (E(x)[all idx(o)])\n(11)\nWe train a bilinear probe clsθ that classiﬁes\neach (proposition embedding, LM representation)\npair to {T,F,?}. The probe has parameters\nW(3×d×d),b(3) and performs the following bilinear\noperation:\nscr(φ,E(x)) = embed(φ)T ·W ·loc(φ,E(x)) + b\n(12)\nwhere scr is a vector of size 3, with one score per\nT,F,? label. The probe then takes the highest-\nscoring label\nclsθ(embed(φ),loc(φ,E(x))) =\n\n\nT if scr(φ,E(x))[0] >scr(φ,E(x))[1],scr(φ,E(x))[2]\nF if scr(φ,E(x))[1] >scr(φ,E(x))[2],scr(φ,E(x))[0]\n? if scr(φ,E(x))[2] >scr(φ,E(x))[0],scr(φ,E(x))[1]\n(13)\nA.3 Localization Experiment Details +\nAdditional Results (§4.3)\nBelow we provide a speciﬁc, formulaic account of\neach of our localizer experiments.\nMentions vs. Other Tokens (§4.3.1) – Alchemy\nRecall that we train a probe for each (t,∆) pair\nto extract propositions about bfrom token tb+∆ ∈\ntoksb+∆, where ∆ is the beaker offset. Speciﬁ-\ncally, the localizer for this probe takes form\noff :\n{‘the’→0,[position] →1,‘be’→2,‘aker’→3,\n‘has’→4,[amount] →5,[color] →6,‘, ’→7}\nloc(t,∆)(has-v-c(b,E(x))) =\nmean(E(x)[8(b+ ∆) −8 + off(t)])\nThe full token-wise results for beaker states in\na 3-beaker (24-token) window around the target\nbeaker is shown in Figure 6 (Top/Middle).\nAdditional localizer ablations results for a BART\nprobe trained and evaluated on synthetic Alchemy\ndata are shown in Figures 6 (Bottom). Similar to\nthe non-synthetic experiments, we point the local-\nizer to just a single token of the initial state. Inter-\nestingly, BART’s distribution looks very different\nin the synthetic setting. Though state information\nis still local to the initial state description of the\ntarget beaker, it is far more distributed within the\ndescription–concentrated in not just the amount\nand color tokens, but also the mention tokens.\nMentions vs. Other Tokens (§4.3.1) – Textworld\nThe speciﬁc localizer for this experiment has form\nloc(r(o1,o2),E(x)) =\nmean(E(x)[all idx(remap(o1))∪\nall idx(remap(o2))])\nloc(p(o),E(x)) = mean(E(x)[all idx(remap(o))])\nNote the evaluation set for this experiment is\nslightly different as we exclude contexts which do\nnot mention remap(w).\n1826\nAlchemy\nEntity EM State EM\nmain probe (§4.2) 75.0 7.55\nhuman-grounded-features (§A.4) 45.7 0.71\nsynthetic data 88.2 35.9\nTable 5: Additional Alchemy results. We compare our\nfull encoded-NL embedder with a featurized embedder\n(§A.4). We also report results on synthetic data.\n.\nWhich Mention? (§4.3.2) – ﬁrst/last The local-\nizer for this experiment is constructed by replac-\ning all instances of all idx in Eq. 10 with either\nfirst idx or last idx, deﬁned as:\nfirst idx(e) = set of indices of xcorrespond-\ning to ﬁrst instance of e\nlast idx(e) = set of indices of xcorrespond-\ning to last instance of e\nWhich Mention? (§4.3.2) – single- vs. both-\nentity probe. The speciﬁc localizer for the\nsingle-entity probe has form\nloc(r(o1,o2),E(x)) =\n{\nmean(E(x)[all idx(o1)]),\nmean(E(x)[all idx(o2)])\n}\nloc(p(o),E(x)) = mean(E(x)[all idx(o)])\nNote the localizer returns a 2-element set of en-\ncodings from each relation. We train the probe to\ndecode r(o1,o2) from both elements of this set.\nThe full results are in Table 3. As shown, the\nboth-mentions probe is slightly better at both de-\ncoding relations and properties. However, this may\nsimply be due to having less candidate proposi-\ntions per entity pair, than per entity (which in-\ncludes relations from every other entity paired\nwith this entity). For example, entity pair (ap-\nple, chest) has only three possibilities: in(apple,\nchest) is True/Unknown/False, while singular en-\ntity (chest) has much more: in(apple, chest),\nin(key, chest) , open(chest), etc. can each\nbe True/Unknown/False. A full set of results bro-\nken down by property/relation can be found in Ta-\nble 6. Overall, the single-entity probe outperforms\nall baselines, suggesting that each entity encoding\ncontains information about its relation with other\nentities.\nA.4 Proposition Embedder Ablations\nWe experiment with a featurized embed function\nin the Alchemy domain. Recall from Section 4.2\nand A.2 that our main probe uses encoded natural-\nlanguage assertions of the state of each beaker\n(Eq. 6). We experiment with featurized vector\nwhere each beaker proposition is the concatena-\ntion of a 1-hot vector for beaker position and\na sparse vector encoding the amount of each\ncolor in the beaker (with 1 position per color).\nFor example, if there are 2 beakers and 3 col-\nors [green,red,brown], has-3-red(2) is repre-\nsented as [0,1,0,3,0]. A multi-layer perceptron is\nused as the embed function to map this featurized\nrepresentation into a dense vector, which is used in\nthe probe as described by Eq. 10. In this setting,\nthe embed MLP is optimized jointly with the probe.\nResults are shown in Table 5. Using a featur-\nized representation (45.7) is signiﬁcantly worse\nthan using an encoded natural-language represen-\ntation (75.0), suggesting that the form of the fact\nembedding function is important. In particular, the\nencoding is linear in sentence-embedding space,\nbut nonlinear in human-grounded-feature space.\nA.5 Error Analysis\nWe run error analysis on the BART model. For\nthe analysis below, it is important to note that we\nmake no distinction between probe errors and rep-\nresentation errors—we do not know whether the\nerrors are attributable to the linear probe’s lack of\nexpressive power, or whether the underlying LM\nindeed does fail to capture certain phenomena. We\nnote that a BART decoder trained to decode the\nﬁnal information state from E(x) is able to achieve\n53.5% state EM on Alchemy (compared to 0% on\nrandom initialization baseline) whereas the linear\ndecoder was only able to achieve 7.55% state EM—\nsuggesting that certain state information may be\nnon-linearly encoded in NLMs.\nAlchemy The average number of incorrect\nbeakers per sample is 25.0% (2.7 beakers out of 7).\nWe note that the distribution is skewed towards\nlonger sequences of actions, where the % of wrong\nbeakers increases from 11.3% (at 1 action) to 24.7%\n(2 actions), 30.4% (3 actions), 33.4% (at 4 actions).\nFor beakers not acted upon (ﬁnal state unchanged),\nthe error rate is 13.3%. For beakers acted upon\n(ﬁnal state changed), the error rate is 44.6%. Thus,\nerrors are largely attributed to failures in reasoning\nabout the effects of actions, rather than failures in\ndecoding the initial state. (This is unsurprising, as\nin Alchemy, the initial state is explicitly written in\nthe text—and we’re decoding from those tokens).\nFor beakers that were predicted wrong, 36.8%\nwere predicted to be its unchanged initial state and\n1827\nOverall Relations Properties True Facts False Facts\nEM EM EM Pre Rec F1 Pre Rec F1\n48.7 49.6 94.5 95.1 98.1 96.4 99.6 98.9 99.2\n+pretrain, -ﬁnetune 23.2 32.7 75.4 93.2 94.9 93.8 97.0 96.1 96.4\n-pretrain, +ﬁnetune 14.4 26.8 44.0 87.3 86.6 86.3 93.3 93.0 92.9\nrandomly initialized 11.3 21.9 35.4 91.5 83.8 87.2 91.8 84.1 86.5\nno LM 1.77 24.8 33.4 88.3 80.9 84.4 88.8 86.9 87.6\nno change 9.73 30.1 9.73 77.9 73.0 75.3 79.1 61.8 68.9\nlocality (ﬁrst) 49.6 50.9 88.5 95.4 97.2 96.1 99.1 98.7 98.9\nlocality (last) 55.1 56.6 96.5 96.1 98.7 97.3 99.7 98.9 99.3\nTable 6: TextWorld Probing. Metrics are reported on whole state. Precision is computed against all gold, ground-\ntruth True facts in the state. Recall is computed against the label-model-generated True facts in the state. All\nnumbers reported are averaged across all samples. Relations are overall much harder to probe than properties.\nthe remaining 63.2% were predicted to be empty\n— thus, probe mistakes are largely attributable to\na tendency to over-predict empty beakers. This\nsuggests that the downstream decoder may tend\nto generate actions too conservatively (as empty\nbeakers cannot be acted upon). Correcting this\ncould encourage LM generation diversity.\nFinally, we examine what type of action tends to\nthrow off the probe. When there is apouring or mix-\ning-type action present in the sequence, the model\ntends to do worse (25.3% error rate for drain-type\nvs. 31.4 and 33.3% for pour- and mix-type), though\nthis is partially due to the higher concentration of\ndrain actions in short action sequences.\nTextworld Textworld results, broken down by\nproperties/relations, are reported in Table 6. The\nprobe seems to be especially bad at classifying\nrelations, which make sense as relations are of-\nten expressed indirectly. A breakdown of error\nrate for each proposition type is shown in Table 7,\nwhere we report what % of that type of propo-\nsition was labelled incorrectly, each time it ap-\npeared. This table suggests that the probe consis-\ntently fails at decoding locational relations, i.e. fail-\ning to classify east-of(kitchen,bedroom) and\nwest-of(kitchen,bedroom) as True, despite the\nlayout of the simple domain being ﬁxed. One\nhypothesis is that location information is made\nmuch less explicit in the text, and usually re-\nquire reasoning across longer contexts and ac-\ntion sequences. For example, classifying in(key,\ndrawer) as True simply requires looking at a single\naction: >put key in drawer. However, classifying\neast-of(kitchen,bedroom) as True requires rea-\nsoning across the following context:\nYou are in the bedroom [. . . ]\nProposition Type Error Rate\n{north|south|east|west} 11.8%-of(A,B)\nis-on(A,B) 6.17%\nis-in(A,B) 1.20%\nlocked(A) 0.47%\nclosed(A) 0.35%\neaten(A) 0.049%\nopen(A) 0.039%\nTable 7: Error rate per proposition type in Textworld.\n>go east\nYou enter the kitchen.\nwhere the ellipses possibly encompass a long se-\nquence of other actions.\nA.6 Infrastructure and Reproducibility\nWe run all experiments on a single 32GB NVIDIA\nTesla V100 GPU. On both Alchemy and Textworld,\nwe train the language models to convergence, then\ntrain the probe for 20 epochs. In Alchemy and\nTextworld, both training the language model and\nthe probe takes approximately a few (less than 5)\nhours. We probe BART-base, a 12-layer encoder-\ndecoder Transformer model with 139M parameters,\nand T5-base, a 24-layer encoder-decoder Trans-\nformer model which has 220M parameters. Our\nprobe itself is a linear model, with only two param-\neters (weights and bias).",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7682328224182129
    },
    {
      "name": "Language model",
      "score": 0.6788124442100525
    },
    {
      "name": "Natural language processing",
      "score": 0.6232666969299316
    },
    {
      "name": "Meaning (existential)",
      "score": 0.5624499320983887
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5576846599578857
    },
    {
      "name": "Transformer",
      "score": 0.552627444267273
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.4543832540512085
    },
    {
      "name": "Word (group theory)",
      "score": 0.44427162408828735
    },
    {
      "name": "Linguistics",
      "score": 0.3321640193462372
    },
    {
      "name": "Psychology",
      "score": 0.1506195068359375
    },
    {
      "name": "Programming language",
      "score": 0.09488934278488159
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Psychotherapist",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I63966007",
      "name": "Massachusetts Institute of Technology",
      "country": "US"
    }
  ]
}