{
    "title": "Poly-encoders: Transformer Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring",
    "url": "https://openalex.org/W2969574947",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A3087822700",
            "name": "Humeau, Samuel",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3088553283",
            "name": "Shuster, Kurt",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4281541773",
            "name": "Lachaux, Marie-Anne",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3088846263",
            "name": "Weston, Jason",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2785414597",
        "https://openalex.org/W1997460147",
        "https://openalex.org/W2908602207",
        "https://openalex.org/W2822830299",
        "https://openalex.org/W2963838657",
        "https://openalex.org/W2963475460",
        "https://openalex.org/W2129921015",
        "https://openalex.org/W2890394457",
        "https://openalex.org/W2913443447",
        "https://openalex.org/W2909777606",
        "https://openalex.org/W2903533488",
        "https://openalex.org/W630532510",
        "https://openalex.org/W2798392716",
        "https://openalex.org/W2998702515",
        "https://openalex.org/W2127589108",
        "https://openalex.org/W2963825865",
        "https://openalex.org/W2197546379",
        "https://openalex.org/W2913340405",
        "https://openalex.org/W2891416139",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2962854379",
        "https://openalex.org/W2970252402",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3023029948",
        "https://openalex.org/W2970049541",
        "https://openalex.org/W10957333",
        "https://openalex.org/W2973054254",
        "https://openalex.org/W2898236449",
        "https://openalex.org/W1566289585",
        "https://openalex.org/W2147152072",
        "https://openalex.org/W2165612380",
        "https://openalex.org/W2971883198",
        "https://openalex.org/W2962779279",
        "https://openalex.org/W2914204778"
    ],
    "abstract": "The use of deep pre-trained bidirectional transformers has led to remarkable progress in a number of applications (Devlin et al., 2018). For tasks that make pairwise comparisons between sequences, matching a given input with a corresponding label, two approaches are common: Cross-encoders performing full self-attention over the pair and Bi-encoders encoding the pair separately. The former often performs better, but is too slow for practical use. In this work, we develop a new transformer architecture, the Poly-encoder, that learns global rather than token level self-attention features. We perform a detailed comparison of all three approaches, including what pre-training and fine-tuning strategies work best. We show our models achieve state-of-the-art results on three existing tasks; that Poly-encoders are faster than Cross-encoders and more accurate than Bi-encoders; and that the best results are obtained by pre-training on large datasets similar to the downstream tasks.",
    "full_text": "Published as a conference paper at ICLR 2020\nPoly-encoders: architectures and pre-training\nstrategies for fast and accurate multi-sentence scoring\nSamuel Humeau∗, Kurt Shuster∗, Marie-Anne Lachaux, Jason Weston\nFacebook AI Research\n{samuelhumeau,kshuster,malachaux,jase}@fb.com\nAbstract\nThe use of deep pre-trained transformers has led to remarkable progress in a num-\nber of applications (Devlin et al., 2019). For tasks that make pairwise compar-\nisons between sequences, matching a given input with a corresponding label, two\napproaches are common: Cross-encoders performing full self-attention over the\npair and Bi-encoders encoding the pair separately. The former often performs\nbetter, but is too slow for practical use. In this work, we develop a new trans-\nformer architecture, the Poly-encoder, that learns global rather than token level\nself-attention features. We perform a detailed comparison of all three approaches,\nincluding what pre-training and ﬁne-tuning strategies work best. We show our\nmodels achieve state-of-the-art results on four tasks; that Poly-encoders are faster\nthan Cross-encoders and more accurate than Bi-encoders; and that the best results\nare obtained by pre-training on large datasets similar to the downstream tasks.\n1 I ntroduction\nRecently, substantial improvements to state-of-the-art benchmarks on a variety of language under-\nstanding tasks have been achieved through the use of deep pre-trained language models followed by\nﬁne-tuning (Devlin et al., 2019). In this work we explore improvements to this approach for the class\nof tasks that require multi-sentence scoring: given an input context, score a set of candidate labels,\na setup common in retrieval and dialogue tasks, amongst others. Performance in such tasks has to\nbe measured via two axes: prediction quality and prediction speed, as scoring many candidates can\nbe prohibitively slow.\nThe current state-of-the-art focuses on using BERT models for pre-training (Devlin et al., 2019),\nwhich employ large text corpora on general subjects: Wikipedia and the Toronto Books Corpus\n(Zhu et al., 2015). Two classes of ﬁne-tuned architecture are typically built on top: Bi-encoders and\nCross-encoders. Cross-encoders (Wolf et al., 2019; Vig & Ramea, 2019), which perform full (cross)\nself-attention over a given input and label candidate, tend to attain much higher accuracies than their\ncounterparts, Bi-encoders (Mazar ´e et al., 2018; Dinan et al., 2019), which perform self-attention\nover the input and candidate label separately and combine them at the end for a ﬁnal representa-\ntion. As the representations are separate, Bi-encoders are able to cache the encoded candidates, and\nreuse these representations for each input resulting in fast prediction times. Cross-encoders must\nrecompute the encoding for each input and label; as a result, they are prohibitively slow at test time.\nIn this work, we provide novel contributions that improve both the quality and speed axes over the\ncurrent state-of-the-art. We introduce the Poly-encoder, an architecture with an additional learnt at-\ntention mechanism that represents more global features from which to perform self-attention, result-\ning in performance gains over Bi-encoders and large speed gains over Cross-Encoders. To pre-train\nour architectures, we show that choosing abundant data more similar to our downstream task also\nbrings signiﬁcant gains over BERT pre-training. This is true across all diﬀerent architecture choices\nand downstream tasks we try.\nWe conduct experiments comparing the new approaches, in addition to analysis of what works best\nfor various setups of existing methods, on four existing datasets in the domains of dialogue and in-\nformation retrieval (IR), with pre-training strategies based on Reddit (Mazar´e et al., 2018) compared\n∗ Joint First Authors.\n1\narXiv:1905.01969v4  [cs.CL]  25 Mar 2020\nPublished as a conference paper at ICLR 2020\nto Wikipedia/Toronto Books (i.e., BERT). We obtain a new state-of-the-art on all four datasets with\nour best architectures and pre-training strategies, as well as providing practical implementations for\nreal-time use. Our code and models will be released open-source.\n2 R elated Work\nThe task of scoring candidate labels given an input context is a classical problem in machine learn-\ning. While multi-class classiﬁcation is a special case, the more general task involves candidates as\nstructured objects rather than discrete classes; in this work we consider the inputs and the candidate\nlabels to be sequences of text.\nThere is a broad class of models that map the input and a candidate label separately into a com-\nmon feature space wherein typically a dot product, cosine or (parameterized) non-linearity is used\nto measure their similarity. We refer to these models as Bi-encoders. Such methods include vector\nspace models (Salton et al., 1975), LSI (Deerwester et al., 1990), supervised embeddings (Bai et al.,\n2009; Wu et al., 2018) and classical siamese networks (Bromley et al., 1994). For the next utterance\nprediction tasks we consider in this work, several Bi-encoder neural approaches have been con-\nsidered, in particular Memory Networks (Zhang et al., 2018a) and Transformer Memory networks\n(Dinan et al., 2019) as well as LSTMs (Lowe et al., 2015) and CNNs (Kadlec et al., 2015) which\nencode input and candidate label separately. A major advantage of Bi-encoder methods is their abil-\nity to cache the representations of a large, ﬁxed candidate set. Since the candidate encodings are\nindependent of the input, Bi-encoders are very eﬃcient during evaluation.\nResearchers have also studied a more rich class of models we refer to as Cross-encoders, which\nmake no assumptions on the similarity scoring function between input and candidate label. Instead,\nthe concatenation of the input and a candidate serve as a new input to a nonlinear function that scores\ntheir match based on any dependencies it wants. This has been explored with Sequential Matching\nNetwork CNN-based architectures (Wu et al., 2017), Deep Matching Networks (Yang et al., 2018),\nGated Self-Attention (Zhang et al., 2018b), and most recently transformers (Wolf et al., 2019; Vig &\nRamea, 2019; Urbanek et al., 2019). For the latter, concatenating the two sequences of text results\nin applying self-attention at every layer. This yields rich interactions between the input context and\nthe candidate, as every word in the candidate label can attend to every word in the input context,\nand vice-versa. Urbanek et al. (2019) employed pre-trained BERT models, and ﬁne-tuned both\nBi- and Cross-encoders, explicitly comparing them on dialogue and action tasks, and ﬁnding that\nCross-encoders perform better. However, the performance gains come at a steep computational cost.\nCross-encoder representations are much slower to compute, rendering some applications infeasible.\n3 T asks\nWe consider the tasks of sentence selection in dialogue and article search in IR. The former is a task\nextensively studied and recently featured in two competitions: the Neurips ConvAI2 competition\n(Dinan et al., 2020), and the DSTC7 challenge, Track 1 (Yoshino et al., 2019; Jonathan K. Kummer-\nfeld & Lasecki, 2018; Chulaka Gunasekara & Lasecki, 2019). We compare on those two tasks and\nin addition, we also test on the popular Ubuntu V2 corpus (Lowe et al., 2015). For IR, we use the\nWikipedia Article Search task of Wu et al. (2018).\nThe ConvAI2 task is based on the Persona-Chat dataset (Zhang et al., 2018a) which involves dia-\nlogues between pairs of speakers. Each speaker is given a persona, which is a few sentences that\ndescribe a character they will imitate, e.g. “I love romantic movies”, and is instructed to get to know\nthe other. Models should then condition their chosen response on the dialogue history and the lines\nof persona. As an automatic metric in the competition, for each response, the model has to pick the\ncorrect annotated utterance from a set of 20 choices, where the remaining 19 were other randomly\nchosen utterances from the evaluation set. Note that in a ﬁnal system however, one would retrieve\nfrom the entire training set of over 100k utterances, but this is avoided for speed reasons in common\nevaluation setups. The best performing competitor out of 23 entrants in this task achieved 80.7%\naccuracy on the test set utilizing a pre-trained Transformer ﬁne-tuned for this task (Wolf et al., 2019).\nThe DSTC7 challenge (Track 1) consists of conversations extracted from Ubuntu chat logs, where\none partner receives technical support for various Ubuntu-related problems from the other. The\n2\nPublished as a conference paper at ICLR 2020\nbest performing competitor (with 20 entrants in Track 1) in this task achieved 64.5% R@1 (Chen\n& Wang, 2019). Ubuntu V2 is a similar but larger popular corpus, created before the competition\n(Lowe et al., 2015); we report results for this dataset as well, as there are many existing results on it.\nFinally, we evaluate on Wikipedia Article Search (Wu et al., 2018). Using the 2016-12-21 dump\nof English Wikipedia (∼5M articles), the task is given a sentence from an article as a search query,\nﬁnd the article it came from. Evaluation ranks the true article (minus the sentence) against 10,000\nother articles using retrieval metrics. This mimics a web search like scenario where one would like\nto search for the most relevant articles (web documents). The best reported method is the learning-\nto-rank embedding model, StarSpace, which outperforms fastText, SVMs, and other baselines.\nWe summarize all four datasets and their statistics in Table 1.\nConvAI2 DTSC7 Ubuntu V2 Wiki Article Search\nTrain Ex. 131,438 100,000 1,000,000 5,035,182\nValid Ex. 7,801 10,000 19,560 9,921\nTest Ex. 6634 5,000 18,920 9,925\nEval Cands per Ex. 20 100 10 10,001\nTable 1: Datasets used in this paper.\n4 M ethods\nIn this section we describe the various models and methods that we explored.\n4.1 T ransformers andPre-training Strategies\nTransformers Our Bi-, Cross-, and Poly-encoders, described in sections 4.2, 4.3 and 4.4 respec-\ntively, are based on large pre-trained transformer models with the same architecture and dimension\nas BERT-base (Devlin et al., 2019), which has 12 layers, 12 attention heads, and a hidden size of\n768. As well as considering the BERT pre-trained weights, we also explore our own pre-training\nschemes. Speciﬁcally, we pre-train two more transformers from scratch using the exact same archi-\ntecture as BERT-base. One uses a similar training setup as in BERT-base, training on 150 million of\nexamples of [INPUT, LABEL] extracted from Wikipedia and the Toronto Books Corpus, while the\nother is trained on 174 million examples of [INPUT, LABEL] extracted from the online platform\nReddit (Mazar´e et al., 2018), which is a dataset more adapted to dialogue. The former is performed\nto verify that reproducing a BERT-like setting gives us the same results as reported previously, while\nthe latter tests whether pre-training on data more similar to the downstream tasks of interest helps.\nFor training both new setups we used XLM (Lample & Conneau, 2019).\nInput Representation Our pre-training input is the concatenation of input and label [IN-\nPUT,LABEL], where both are surrounded with the special token [S], following Lample & Conneau\n(2019). When pre-training on Reddit, the input is the context, and the label is the next utterance.\nWhen pre-training on Wikipedia and Toronto Books, as in Devlin et al. (2019), the input is one\nsentence and the label the next sentence in the text. Each input token is represented as the sum of\nthree embeddings: the token embedding, the position (in the sequence) embedding and the segment\nembedding. Segments for input tokens are 0, and for label tokens are 1.\nPre-training Procedure Our pre-training strategy involves training with a masked language\nmodel (MLM) task identical to the one in Devlin et al. (2019). In the pre-training on Wikipedia and\nToronto Books we add a next-sentence prediction task identical to BERT training. In the pre-training\non Reddit, we add a next-utterance prediction task, which is slightly diﬀerent from the previous one\nas an utterance can be composed of several sentences. During training 50% of the time the candi-\ndate is the actual next sentence /utterance and 50% of the time it is a sentence /utterance randomly\ntaken from the dataset. We alternate between batches of the MLM task and the next-sentence /next-\nutterance prediction task. Like in Lample & Conneau (2019) we use the Adam optimizer with\nlearning rate of 2e-4, β1 = 0.9, β2 = 0.98, no L2 weight decay, linear learning rate warmup, and\ninverse square root decay of the learning rate. We use a dropout probability of 0.1 on all layers, and\n3\nPublished as a conference paper at ICLR 2020\na batch of 32000 tokens composed of concatenations [INPUT, LABEL] with similar lengths. We\ntrain the model on 32 GPUs for 14 days.\nFine-tuning After pre-training, one can then ﬁne-tune for the multi-sentence selection task of\nchoice, in our case one of the four tasks from Section 3. We consider three architectures with which\nwe ﬁne-tune the transformer: the Bi-encoder, Cross-encoder and newly proposed Poly-encoder.\n4.2 B i-encoder\nIn a Bi-encoder, both the input context and the candidate label are encoded into vectors:\nyctxt = red(T1(ctxt)) ycand = red(T2(cand))\nwhere T1 and T2 are two transformers that have been pre-trained following the procedure described\nin 4.1; they initially start with the same weights, but are allowed to update separately during ﬁne-\ntuning. T(x) = h1,.., hN is the output of a transformer T and red(·) is a function that reduces that\nsequence of vectors into one vector. As the input and the label are encoded separately, segment\ntokens are 0 for both. To resemble what is done during our pre-training, both the input and label are\nsurrounded by the special token [S] and therefore h1 corresponds to [S].\nWe considered three ways of reducing the output into one representation via red(·): choose the ﬁrst\noutput of the transformer (corresponding to the special token [S]), compute the average over all\noutputs or the average over the ﬁrst m ≤N outputs. We compare them in Table 7 in the Appendix.\nWe use the ﬁrst output of the transformer in our experiments as it gives slightly better results.\nScoring The score of a candidatecandi is given by the dot-products(ctxt,candi) = yctxt ·ycandi . The\nnetwork is trained to minimize a cross-entropy loss in which the logits areyctxt ·ycand1 ,..., yctxt ·ycandn ,\nwhere cand1 is the correct label and the others are chosen from the training set. Similar to Mazar ´e\net al. (2018), during training we consider the other labels in the batch as negatives. This allows for\nmuch faster training, as we can reuse the embeddings computed for each candidate, and also use a\nlarger batch size; e.g., in our experiments on ConvAI2, we were able to use batches of 512 elements.\nInference speed In the setting of retrieval over known candidates, a Bi-encoder allows for the\nprecomputation of the embeddings of all possible candidates of the system. After the context em-\nbedding yctxt is computed, the only operation remaining is a dot product between yctxt and every\ncandidate embedding, which can scale to millions of candidates on a modern GPU, and potentially\nbillions using nearest-neighbor libraries such as FAISS (Johnson et al., 2019).\n4.3 C ross-encoder\nThe Cross-encoder allows for rich interactions between the input context and candidate label, as\nthey are jointly encoded to obtain a ﬁnal representation. Similar to the procedure in pre-training,\nthe context and candidate are surrounded by the special token [S] and concatenated into a single\nvector, which is encoded using one transformer. We consider the ﬁrst output of the transformer as\nthe context-candidate embedding:\nyctxt,cand = h1 = f irst(T(ctxt,cand))\nwhere f irst is the function that takes the ﬁrst vector of the sequence of vectors produced by the\ntransformer. By using a single transformer, the Cross-encoder is able to perform self-attention be-\ntween the context and candidate, resulting in a richer extraction mechanism than the Bi-encoder. As\nthe candidate label can attend to the input context during the layers of the transformer, the Cross-\nencoder can produce a candidate-sensitive input representation, which the Bi-encoder cannot. For\nexample, this allows it to select useful input features per candidate.\nScoring To score one candidate, a linear layer W is applied to the embedding yctxt,cand to reduce it\nfrom a vector to a scalar:\ns(ctxt,candi) = yctxt,candi W\nSimilarly to what is done for the Bi-encoder, the network is trained to minimize a cross entropy loss\nwhere the logits are s(ctxt,cand1),..., s(ctxt,candn), where cand1 is the correct candidate and the\n4\nPublished as a conference paper at ICLR 2020\nFigure 1: Diagrams of the three model architectures we consider. (a) The Bi-encoder encodes\nthe context and candidate separately, allowing for the caching of candidate representations during\ninference. (b) The Cross-encoder jointly encodes the context and candidate in a single transformer,\nyielding richer interactions between context and candidate at the cost of slower computation. (c)\nThe Poly-encoder combines the strengths of the Bi-encoder and Cross-encoder by both allowing for\ncaching of candidate representations and adding a ﬁnal attention mechanism between global features\nof the input and a given candidate to give richer interactions before computing a ﬁnal score.\nothers are negatives taken from the training set. Unlike in the Bi-encoder, we cannot recycle the\nother labels of the batch as negatives, so we use external negatives provided in the training set. The\nCross-encoder uses much more memory than the Bi-encoder, resulting in a much smaller batch size.\nInference speed Unfortunately, the Cross-encoder does not allow for precomputation of the can-\ndidate embeddings. At inference time, every candidate must be concatenated with the input context\nand must go through a forward pass of the entire model. Thus, this method cannot scale to a large\namount of candidates. We discuss this bottleneck further in Section 5.4.\n4.4 P oly-encoder\nThe Poly-encoder architecture aims to get the best of both worlds from the Bi- and Cross-encoder.\nA given candidate label is represented by one vector as in the Bi-encoder, which allows for caching\ncandidates for fast inference time, while the input context is jointly encoded with the candidate, as\nin the Cross-encoder, allowing the extraction of more information.\nThe Poly-encoder uses two separate transformers for the context and label like a Bi-encoder, and\nthe candidate is encoded into a single vector ycandi . As such, the Poly-encoder method can be im-\nplemented using a precomputed cache of encoded responses. However, the input context, which is\ntypically much longer than a candidate, is represented with m vectors (y1\nctxt ..ym\nctxt ) instead of just one\nas in the Bi-encoder, where m will inﬂuence the inference speed. To obtain these m global features\nthat represent the input, we learn m context codes (c1,..., cm), where ci extracts representation yi\nctxt\nby attending over all the outputs of the previous layer. That is, we obtain yi\nctxt using:\nyi\nctxt =\n∑\nj\nwci\nj hj where ( wci\n1 ,.., wci\nN ) = softmax(ci ·h1,.., ci ·hN )\n5\nPublished as a conference paper at ICLR 2020\nThe m context codes are randomly initialized, and learnt during ﬁnetuning.\nFinally, given our m global context features, we attend over them using ycandi as the query:\nyctxt =\n∑\ni\nwiyi\nctxt where ( w1,.., wm) = softmax(ycandi ·y1\nctxt ,.., ycandi ·ym\nctxt )\nThe ﬁnal score for that candidate label is then yctxt ·ycandi as in a Bi-encoder. As m <N, where N is\nthe number of tokens, and the context-candidate attention is only performed at the top layer, this is\nfar faster than the Cross-encoder’s full self-attention.\n5 E xperiments\nWe perform a variety of experiments to test our model architectures and training strategies over four\ntasks. For metrics, we measure Recall@ k where each test example has C possible candidates to\nselect from, abbreviated to R@k/C, as well as mean reciprocal rank (MRR).\n5.1 B i-encoders andCross-encoders\nWe ﬁrst investigate ﬁne-tuning the Bi- and Cross-encoder architectures initialized with the weights\nprovided by Devlin et al. (2019), studying the choice of other hyperparameters (we explore our own\npre-training schemes in section 5.3). In the case of the Bi-encoder, we can use a large number of neg-\natives by considering the other batch elements as negative training samples, avoiding recomputation\nof their embeddings. On 8 Nvidia V olta v100 GPUs and using half-precision operations (i.e. ﬂoat16\noperations), we can reach batches of 512 elements on ConvAI2. Table 2 shows that in this setting,\nwe obtain higher performance with a larger batch size, i.e. more negatives, where 511 negatives\nyields the best results. For the other tasks, we keep the batch size at 256, as the longer sequences\nin those datasets uses more memory. The Cross-encoder is more computationally intensive, as the\nembeddings for the (context, candidate) pair must be recomputed each time. We thus limit its batch\nsize to 16 and provide negatives random samples from the training set. For DSTC7 and Ubuntu V2,\nwe choose 15 such negatives; For ConvAI2, the dataset provides 19 negatives.\nNegatives 31 63 127 255 511\nR@1/20 81.0 81.7 82.3 83.0 83.3\nTable 2: Validation performance on ConvAI2 after ﬁne-tuning a Bi-encoder pre-trained with BERT,\naveraged over 5 runs. The batch size is the number of training negatives + 1 as we use the other\nelements of the batch as negatives during training.\nThe above results are reported with Bi-encoder aggregation based on the ﬁrst output. Choosing the\naverage over all outputs instead is very similar but slightly worse (83.1, averaged over 5 runs). We\nalso tried to add further non-linearities instead of the inner product of the two representations, but\ncould not obtain improved results over the simpler architecture (results not shown).\nWe tried two optimizers: Adam (Kingma & Ba, 2015) with weight decay of 0.01 (as recommended\nby (Devlin et al., 2019)) and Adamax (Kingma & Ba, 2015) without weight decay; based on val-\nidation set performance, we choose to ﬁne-tune with Adam when using the BERT weights. The\nlearning rate is initialized to 5e-5 with a warmup of 100 iterations for Bi- and Poly-encoders, and\n1000 iterations for the Cross-encoder. The learning rate decays by a factor of 0.4 upon plateau of the\nloss evaluated on the valid set every half epoch. In Table 3 we show validation performance when\nﬁne-tuning various layers of the weights provided by (Devlin et al., 2019), using Adam with decay\noptimizer. Fine-tuning the entire network is important, with the exception of the word embeddings.\nWith the setups described above, we ﬁne-tune the Bi- and Cross-encoders on the datasets, and report\nthe results in Table 4. On the ﬁrst three tasks, our Bi-encoders and Cross-encoders outperform\nthe best existing approaches in the literature when we ﬁne-tune from BERT weights. E.g., the Bi-\nencoder reaches 81.7% R@1 on ConvAI2 and 66.8% R@1 on DSTC7, while the Cross-encoder\nachieves higher scores of 84.8% R@1 on ConvAI2 and 67.4% R@1 on DSTC7. Overall, Cross-\nencoders outperform all previous approaches on the three dialogue tasks, including our Bi-encoders\n(as expected). We do not report ﬁne-tuning of BERT for Wikipedia IR as we cannot guarantee the\n6\nPublished as a conference paper at ICLR 2020\nFine-tuned parameters Bi-encoder Cross-encoder\nTop layer 74.2 80.6\nTop 4 layers 82.0 86.3\nAll but Embeddings 83.3 87.3\nEvery Layer 83.0 86.6\nTable 3: Validation performance (R@1 /20) on ConvAI2 using pre-trained weights of BERT-base\nwith diﬀerent parameters ﬁne-tuned. Average over 5 runs (Bi-encoders) or 3 runs (Cross-encoders).\ntest set is not part of the pre-training for that dataset. In addition, Cross-encoders are also too slow\nto evaluate on the evaluation setup of that task, which has 10k candidates.\nDataset ConvAI2 DSTC 7 Ubuntu v2 Wikipedia IR\nsplit test test test test\nmetric R@1/20 R@1/100 MRR R@1/10 MRR R@1/10001\n(Wolf et al., 2019) 80.7\n(Gu et al., 2018) - 60.8 69.1 - - -\n(Chen & Wang, 2019) - 64.5 73.5 - - -\n(Yoon et al., 2018) - - - 65.2 - -\n(Dong & Huang, 2018) - - - 75.9 84.8 -\n(Wu et al., 2018) - - - - - 56.8\npre-trained BERT weights from (Devlin et al., 2019) - Toronto Books+ Wikipedia\nBi-encoder 81.7 ±0.2 66.8 ±0.7 74.6 ±0.5 80.6 ±0.4 88.0 ±0.3 -\nPoly-encoder 16 83.2 ±0.1 67.8 ±0.3 75.1 ±0.2 81.2 ±0.2 88.3 ±0.1 -\nPoly-encoder 64 83.7 ±0.2 67.0 ±0.9 74.7 ±0.6 81.3 ±0.2 88.4 ±0.1 -\nPoly-encoder 360 83.7 ±0.2 68.9 ±0.4 76.2 ±0.2 80.9 ±0.0 88.1 ±0.1 -\nCross-encoder 84.8 ±0.3 67.4 ±0.7 75.6 ±0.4 82.8 ±0.3 89.4 ±0.2 -\nOur pre-training on Toronto Books + Wikipedia\nBi-encoder 82.0 ±0.1 64.5 ±0.5 72.6 ±0.4 80.8 ±0.5 88.2 ±0.4 -\nPoly-encoder 16 82.7 ±0.1 65.3 ±0.9 73.2 ±0.7 83.4 ±0.2 89.9 ±0.1 -\nPoly-encoder 64 83.3 ±0.1 65.8 ±0.7 73.5 ±0.5 83.4 ±0.1 89.9 ±0.0 -\nPoly-encoder 360 83.8 ±0.1 65.8 ±0.7 73.6 ±0.6 83.7 ±0.0 90.1 ±0.0 -\nCross-encoder 84.9 ±0.3 65.3 ±1.0 73.8 ±0.6 83.1 ±0.7 89.7 ±0.5 -\nOur pre-training on Reddit\nBi-encoder 84.8 ±0.1 70.9 ±0.5 78.1 ±0.3 83.6 ±0.7 90.1 ±0.4 71.0\nPoly-encoder 16 86.3 ±0.3 71.6 ±0.6 78.4 ±0.4 86.0 ±0.1 91.5 ±0.1 71.5\nPoly-encoder 64 86.5 ±0.2 71.2 ±0.8 78.2 ±0.7 85.9 ±0.1 91.5 ±0.1 71.3\nPoly-encoder 360 86.8 ±0.1 71.4 ±1.0 78.3 ±0.7 85.9 ±0.1 91.5 ±0.0 71.8\nCross-encoder 87.9 ±0.2 71.7 ±0.3 79.0 ±0.2 86.5 ±0.1 91.9 ±0.0 -\nTable 4: Test performance of Bi-, Poly- and Cross-encoders on our selected tasks.\n5.2 P oly-encoders\nWe train the Poly-encoder using the same batch sizes and optimizer choices as in the Bi-encoder\nexperiments. Results are reported in Table 4 for various values of m context vectors.\nThe Poly-encoder outperforms the Bi-encoder on all the tasks, with more codes generally yielding\nlarger improvements. Our recommendation is thus to use as large a code size as compute time allows\n(see Sec. 5.4). On DSTC7, the Poly-encoder architecture with BERT pretraining reaches 68.9% R1\nwith 360 intermediate context codes; this actually outperforms the Cross-encoder result (67.4%) and\nis noticeably better than our Bi-encoder result (66.8%). Similar conclusions are found on Ubuntu\nV2 and ConvAI2, although in the latter Cross-encoders give slightly better results.\nWe note that since reporting our results, the authors of Li et al. (2019) have conducted a human\nevaluation study on ConvAI2, in which our Poly-encoder architecture outperformed all other models\ncompared against, both generative and retrieval based, including the winners of the competition.\n7\nPublished as a conference paper at ICLR 2020\nScoring time (ms)\nCPU GPU\nCandidates 1k 100k 1k 100k\nBi-encoder 115 160 19 22\nPoly-encoder 16 122 678 18 38\nPoly-encoder 64 126 692 23 46\nPoly-encoder 360 160 837 57 88\nCross-encoder 21.7k 2.2M* 2.6k 266k*\nTable 5: Average time in milliseconds to predict the next dialogue utterance from C possible candi-\ndates on ConvAI2. * are inferred.\n5.3 D omain-specific Pre-training\nWe ﬁne-tune our Reddit-pre-trained transformer on all four tasks; we additionally ﬁne-tune a trans-\nformer that was pre-trained on the same datasets as BERT, speciﬁcally Toronto Books+ Wikipedia.\nWhen using our pre-trained weights, we use the Adamax optimizer and optimize all the layers of the\ntransformer including the embeddings. As we do not use weight decay, the weights of the ﬁnal layer\nare much larger than those in the ﬁnal layer of BERT; to avoid saturation of the attention layer in the\nPoly-encoder, we re-scaled the last linear layer so that the standard deviation of its output matched\nthat of BERT, which we found necessary to achieve good results. We report results of ﬁne-tuning\nwith our pre-trained weights in Table 4. We show that pre-training on Reddit gives further state-of-\nthe-art performance over our previous results with BERT, a ﬁnding that we see for all three dialogue\ntasks, and all three architectures.\nThe results obtained with ﬁne-tuning on our own transformers pre-trained on Toronto Books +\nWikipedia are very similar to those obtained with the original BERT weights, indicating that the\nchoice of dataset used to pre-train the models impacts the ﬁnal results, not some other detail in our\ntraining. Indeed, as the two settings pre-train with datasets of similar size, we can conclude that\nchoosing a pre-training task (e.g. dialogue data) that is similar to the downstream tasks of interest\n(e.g. dialogue) is a likely explanation for these performance gains, in line with previous results\nshowing multi-tasking with similar tasks is more useful than with dissimilar ones (Caruana, 1997).\n5.4 I nference Speed\nAn important motivation for the Poly-encoder architecture is to achieve better results than the Bi-\nencoder while also performing at a reasonable speed. Though the Cross-encoder generally yields\nstrong results, it is prohibitively slow. We perform speed experiments to determine the trade-o ﬀ of\nimproved performance from the Poly-encoder. Speciﬁcally, we predict the next utterance for 100\ndialogue examples in the ConvAI2 validation set, where the model scoresC candidates (in this case,\nchosen from the training set). We perform these experiments on both CPU-only and GPU setups.\nCPU computations were run on an 80 core Intel Xeon processor CPU E5-2698. GPU computations\nwere run on a single Nvidia Quadro GP100 using cuda 10.0 and cudnn 7.4.\nWe show the average time per example for each architecture in Table 5. The di ﬀerence in timing\nbetween the Bi-encoder and the Poly-encoder architectures is rather minimal when there are only\n1000 candidates for the model to consider. The di ﬀerence is more pronounced when considering\n100k candidates, a more realistic setup, as we see a 5-6x slowdown for the Poly-encoder variants.\nNevertheless, both models are still tractable. The Cross-encoder, however, is 2 orders of magnitude\nslower than the Bi-encoder and Poly-encoder, rendering it intractable for real-time inference, e.g.\nwhen interacting with a dialogue agent, or retrieving from a large set of documents. Thus, Poly-\nencoders, given their desirable performance and speed trade-oﬀ, are the preferred method.\nWe additionally report training times in the Appendix, Table 6. Poly-encoders also have the beneﬁt\nof being 3-4x faster to train than Cross-encoders (and are similar in training time to Bi-encoders).\n8\nPublished as a conference paper at ICLR 2020\n6 C onclusion\nIn this paper we present new architectures and pre-training strategies for deep bidirectional trans-\nformers in candidate selection tasks. We introduced the Poly-encoder method, which provides a\nmechanism for attending over the context using the label candidate, while maintaining the ability to\nprecompute each candidate’s representation, which allows for fast real-time inference in a produc-\ntion setup, giving an improved trade oﬀ between accuracy and speed. We provided an experimental\nanalysis of those trade-oﬀs for Bi-, Poly- and Cross-encoders, showing that Poly-encoders are more\naccurate than Bi-encoders, while being far faster than Cross-encoders, which are impractical for\nreal-time use. In terms of training these architectures, we showed that pre-training strategies more\nclosely related to the downstream task bring strong improvements. In particular, pre-training from\nscratch on Reddit allows us to outperform the results we obtain with BERT, a result that holds for all\nthree model architectures and all three dialogue datasets we tried. However, the methods introduced\nin this work are not speciﬁc to dialogue, and can be used for any task where one is scoring a set of\ncandidates, which we showed for an information retrieval task as well.\nReferences\nBing Bai, Jason Weston, David Grangier, Ronan Collobert, Kunihiko Sadamasa, Yanjun Qi, Olivier\nChapelle, and Kilian Weinberger. Supervised semantic indexing. InProceedings of the 18th ACM\nconference on Information and knowledge management, pp. 187–196. ACM, 2009.\nJane Bromley, Isabelle Guyon, Yann LeCun, Eduard S¨ackinger, and Roopak Shah. Signature veriﬁ-\ncation using a” siamese” time delay neural network. InAdvances in neural information processing\nsystems, pp. 737–744, 1994.\nRich Caruana. Multitask learning. Machine learning, 28(1):41–75, 1997.\nQian Chen and Wen Wang. Sequential attention-based network for noetic end-to-end response\nselection. CoRR, abs/1901.02609, 2019. URL http://arxiv.org/abs/1901.02609.\nLazaros Polymenakos Chulaka Gunasekara, Jonathan K. Kummerfeld and Walter S. Lasecki. Dstc7\ntask 1: Noetic end-to-end response selection. In 7th Edition of the Dialog System Technol-\nogy Challenges at AAAI 2019 , January 2019. URL http://workshop.colips.org/dstc7/\npapers/dstc7_task1_final_report.pdf.\nScott Deerwester, Susan T Dumais, George W Furnas, Thomas K Landauer, and Richard Harshman.\nIndexing by latent semantic analysis. Journal of the American society for information science, 41\n(6):391–407, 1990.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), pp. 4171–4186, Minneapolis, Minnesota, June\n2019. Association for Computational Linguistics. doi: 10.18653 /v1/N19-1423.\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wizard\nof Wikipedia: Knowledge-powered conversational agents. In Proceedings of the International\nConference on Learning Representations (ICLR), 2019.\nEmily Dinan, Varvara Logacheva, Valentin Malykh, Alexander Miller, Kurt Shuster, Jack Urbanek,\nDouwe Kiela, Arthur Szlam, Iulian Serban, Ryan Lowe, Shrimai Prabhumoye, Alan W. Black,\nAlexander Rudnicky, Jason Williams, Joelle Pineau, Mikhail Burtsev, and Jason Weston. The\nsecond conversational intelligence challenge (convai2). In Sergio Escalera and Ralf Herbrich\n(eds.), The NeurIPS ’18 Competition, pp. 187–208, Cham, 2020. Springer International Publish-\ning. ISBN 978-3-030-29135-8.\nJianxiong Dong and Jim Huang. Enhance word representation for out-of-vocabulary on ubuntu\ndialogue corpus. CoRR, abs/1802.02614, 2018. URL http://arxiv.org/abs/1802.02614.\n9\nPublished as a conference paper at ICLR 2020\nJia-Chen Gu, Zhen-Hua Ling, Yu-Ping Ruan, and Quan Liu. Building sequential inference models\nfor end-to-end response selection. CoRR, abs/1812.00686, 2018. URL http://arxiv.org/\nabs/1812.00686.\nJ. Johnson, M. Douze, and H. Jgou. Billion-scale similarity search with gpus. IEEE Transactions\non Big Data, pp. 1–1, 2019. ISSN 2372-2096. doi: 10.1109 /TBDATA.2019.2921572.\nJoseph Peper Vignesh Athreya Chulaka Gunasekara Jatin Ganhotra Siva Sankalp Patel Lazaros Poly-\nmenakos Jonathan K. Kummerfeld, Sai R. Gouravajhala and Walter S. Lasecki. Analyzing as-\nsumptions in conversation disentanglement research through the lens of a new dataset and model.\nArXiv e-prints, October 2018. URL https://arxiv.org/pdf/1810.11118.pdf.\nRudolf Kadlec, Martin Schmid, and Jan Kleindienst. Improved deep learning baselines for ubuntu\ncorpus dialogs. CoRR, abs/1510.03753, 2015. URL http://arxiv.org/abs/1510.03753.\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9,\n2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980.\nGuillaume Lample and Alexis Conneau. Cross-lingual language model pretraining. Advances in\nNeural Information Processing Systems (NeurIPS), 2019.\nMargaret Li, Jason Weston, and Stephen Roller. Acute-eval: Improved dialogue evaluation with\noptimized questions and multi-turn comparisons. arXiv preprint arXiv:1909.03087, 2019.\nRyan Lowe, Nissan Pow, Iulian Serban, and Joelle Pineau. The ubuntu dialogue corpus: A large\ndataset for research in unstructured multi-turn dialogue systems. In SIGDIAL Conference, 2015.\nPierre-Emmanuel Mazar´e, Samuel Humeau, Martin Raison, and Antoine Bordes. Training millions\nof personalized dialogue agents. In EMNLP, 2018.\nGerard Salton, Anita Wong, and Chung-Shu Yang. A vector space model for automatic indexing.\nCommunications of the ACM, 18(11):613–620, 1975.\nJack Urbanek, Angela Fan, Siddharth Karamcheti, Saachi Jain, Samuel Humeau, Emily Dinan, Tim\nRockt¨aschel, Douwe Kiela, Arthur Szlam, and Jason Weston. Learning to speak and act in a\nfantasy text adventure game. In Proceedings of the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th International Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pp. 673–683, Hong Kong, China, November 2019. Association\nfor Computational Linguistics. doi: 10.18653 /v1/D19-1062.\nJesse Vig and Kalai Ramea. Comparison of transfer-learning approaches for response selection in\nmulti-turn conversations. Workshop on DSTC7, 2019.\nThomas Wolf, Victor Sanh, Julien Chaumond, and Clement Delangue. Transfertransfo: A\ntransfer learning approach for neural network based conversational agents. arXiv preprint\narXiv:1901.08149, 2019.\nLedell Yu Wu, Adam Fisch, Sumit Chopra, Keith Adams, Antoine Bordes, and Jason Weston.\nStarspace: Embed all the things! In Thirty-Second AAAI Conference on Artiﬁcial Intelligence ,\n2018.\nYu Ping Wu, Wei Chung Wu, Chen Xing, Ming Zhou, and Zhoujun Li. Sequential matching net-\nwork: A new architecture for multi-turn response selection in retrieval-based chatbots. In ACL,\n2017.\nLiu Yang, Minghui Qiu, Chen Qu, Jiafeng Guo, Yongfeng Zhang, W Bruce Croft, Jun Huang,\nand Haiqing Chen. Response ranking with deep matching networks and external knowledge in\ninformation-seeking conversation systems. In The 41st International ACM SIGIR Conference on\nResearch & Development in Information Retrieval, pp. 245–254. ACM, 2018.\n10\nPublished as a conference paper at ICLR 2020\nSeunghyun Yoon, Joongbo Shin, and Kyomin Jung. Learning to rank question-answer pairs using\nhierarchical recurrent encoder with latent topic clustering. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long Papers), pp. 1575–1584, New Orleans, Louisiana, June\n2018. Association for Computational Linguistics. doi: 10.18653 /v1/N18-1142.\nKoichiro Yoshino, Chiori Hori, Julien Perez, Luis Fernando D’Haro, Lazaros Polymenakos, R. Chu-\nlaka Gunasekara, Walter S. Lasecki, Jonathan K. Kummerfeld, Michel Galley, Chris Brockett,\nJianfeng Gao, William B. Dolan, Xiang Gao, Huda AlAmri, Tim K. Marks, Devi Parikh, and\nDhruv Batra. Dialog system technology challenge 7. CoRR, abs/1901.03461, 2019.\nSaizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, and Jason Weston. Per-\nsonalizing dialogue agents: I have a dog, do you have pets too? InProceedings of the 56th Annual\nMeeting of the Association for Computational Linguistics, pp. 2204–2213, Melbourne, Australia,\nJuly 2018a. Association for Computational Linguistics.\nZhuosheng Zhang, Jiangtong Li, Pengfei Zhu, Hai Zhao, and Gongshen Liu. Modeling multi-turn\nconversation with deep utterance aggregation. In COLING, 2018b.\nYukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan R. Salakhutdinov, Raquel Urtasun, Antonio\nTorralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations\nby watching movies and reading books.2015 IEEE International Conference on Computer Vision\n(ICCV), pp. 19–27, 2015.\n11\nPublished as a conference paper at ICLR 2020\nA T raining Time\nWe report the training time on 8 GPU V olta 100 for the 3 datasets considered and for 4 types of\nmodels in Table 6.\nDataset ConvAI2 DSTC7 UbuntuV2\nBi-encoder 2.0 4.9 7.9\nPoly-encoder 16 2.7 5.5 8.0\nPoly-encoder 64 2.8 5.7 8.0\nCross-encoder64 9.4 13.5 39.9\nTable 6: Training time in hours.\nB R eduction layer inBi-encoder\nWe provide in Table 7 the results obtained for diﬀerent types of reductions on top of the Bi-encoder.\nSpeciﬁcally we compare the Recall@1/20 on the ConvAI2 validation set when taking the ﬁrst output\nof BERT, the average of the ﬁrst 16 outputs, the average of the ﬁrst 64 outputs and all of them except\nthe ﬁrst one ([S]).\nSetup ConvAI2 valid Recall@1/20\nFirst output 83.3\nAvg ﬁrst 16 outputs 82.9\nAvg ﬁrst 64 outputs 82.7\nAvg all outputs 83.1\nTable 7: Bi-encoder results on the ConvAI2 valid set for diﬀerent choices of function red(·).\nC A lternativeChoices forContext Vectors\nWe considered a few other ways to derive the context vectors (y1\nctxt ,..., ym\nctxt ) of the Poly-encoder from\nthe output (h1\nctxt ,..., hN\nctxt ) of the underlying transformer:\n• Learn m codes (c1,..., cm), where ci extracts representation yi\nctxt by attending over all the\noutputs (h1\nctxt ,..., hN\nctxt ). This method is denoted “Poly-encoder (Learnt-codes)” or “Poly-\nencoder (Learnt-m)”, and is the method described in section 4.4\n• Consider the ﬁrst m outputs (h1\nctxt ,..., hm\nctxt ). This method is denoted “Poly-encoder (First\nm outputs)” or “Poly-encoder (First-m)”. Note that when N < m, only m vectors are\nconsidered.\n• Consider the last m outputs.\n• Consider the last m outputs concatenated with the ﬁrst one, h1\nctxt which plays a particular\nrole in BERT as it corresponds to the special token [S].\nThe performance of those four methods is evaluated on the validation set of Convai2 and DSTC7\nand reported on Table 8. The ﬁrst two methods are shown in Figure 2. We additionally provide the\ninference time for a given number of candidates coming from the Convai2 dataset on Table 9.\n12\nPublished as a conference paper at ICLR 2020\nDataset ConvAI2 DSTC 7\nsplit dev test dev test\nmetric R@1/20 R@1/20 R@1/100 R@1/100\n(Wolf et al., 2019) 82.1 80.7 - -\n(Chen & Wang, 2019) - - 57.3 64.5\n1 Attention Code\nLearnt-codes 81.9 ±0.3 81.0 ±0.1 56.2 ±0.1 66.9 ±0.7\nFirst m outputs 83.2 ±0.2 81.5 ±0.1 56.4 ±0.3 66.8 ±0.7\nLast m outputs 82.9 ±0.1 81.0 ±0.1 56.1 ±0.4 67.2 ±1.1\nLast m outputs and h1\nctxt - - - -\n4 Attention Codes\nLearnt-codes 83.8 ±0.2 82.2 ±0.5 56.5 ±0.5 66.8 ±0.7\nFirst m outputs 83.4 ±0.2 81.6 ±0.1 56.9 ±0.5 67.2 ±1.3\nLast m outputs 82.8 ±0.2 81.3 ±0.4 56.0 ±0.5 65.8 ±0.5\nLast m outputs and h1\nctxt 82.9 ±0.1 81.4 ±0.2 55.8 ±0.3 66.1 ±0.8\n16 Attention Codes\nLearnt-codes 84.4 ±0.1 83.2 ±0.1 57.7 ±0.2 67.8 ±0.3\nFirst m outputs 85.2 ±0.1 83.9 ±0.2 56.1 ±1.7 66.8 ±1.1\nLast m outputs 83.9 ±0.2 82.0 ±0.4 56.1 ±0.3 66.2 ±0.7\nLast m outputs and h1\nctxt 83.8 ±0.3 81.7 ±0.3 56.1 ±0.3 66.6 ±0.2\n64 Attention Codes\nLearnt-codes 84.9 ±0.1 83.7 ±0.2 58.3 ±0.4 67.0 ±0.9\nFirst m outputs 86.0 ±0.2 84.2 ±0.2 57.7 ±0.6 67.1 ±0.1\nLast m outputs 84.9 ±0.3 82.9 ±0.2 57.0 ±0.2 66.5 ±0.5\nLast m outputs and h1\nctxt 85.0 ±0.2 83.2 ±0.2 57.3 ±0.3 67.1 ±0.5\n360 Attention Codes\nLearnt-codes 85.3 ±0.3 83.7 ±0.2 57.7 ±0.3 68.9 ±0.4\nFirst m outputs 86.3 ±0.1 84.6 ±0.3 58.1 ±0.4 66.8 ±0.7\nLast m outputs 86.3 ±0.1 84.7 ±0.3 58.0 ±0.4 68.1 ±0.5\nLast m outputs and h1\nctxt 86.2 ±0.3 84.5 ±0.4 58.3 ±0.4 68.0 ±0.8\nTable 8: Validation and test performance of Poly-encoder variants, with weights initialized from\n(Devlin et al., 2019). Scores are shown for ConvAI2 and DSTC 7 Track 1. Bold numbers indicate\nthe highest performing variant within that number of codes.\nScoring time (ms)\nCPU GPU\nCandidates 1k 100k 1k 100k\nBi-encoder 115 160 19 22\nPoly-encoder (First m outputs) 16 119 551 17 37\nPoly-encoder (First m outputs) 64 124 570 17 39\nPoly-encoder (First m outputs) 360 120 619 17 45\nPoly-encoder (Learnt-codes) 16 122 678 18 38\nPoly-encoder (Learnt-codes) 64 126 692 23 46\nPoly-encoder (Learnt-codes) 360 160 837 57 88\nCross-encoder 21.7k 2.2M* 2.6k 266k*\nTable 9: Average time in milliseconds to predict the next dialogue utterance from N possible candi-\ndates. * are inferred.\n13\nPublished as a conference paper at ICLR 2020\nFigure 2: (a) The Bi-encoder (b) The Cross-encoder (c) The Poly-encoder with ﬁrst m vectors. (d)\nThe Poly-encoder with m learnt codes.\nDataset ConvAI2 DSTC 7 Ubuntu v2\nsplit dev test dev test dev test\nmetric R@1/20 R@1/20 R@1/100 R@1/100 R@10/100 MRR R@1/10 R@1/10 R@5/10 MRR\nHugging Face 82.1 80.7 - - - - - - - -(Wolf et al., 2019)\n(Chen & Wang, 2019) - - 57.3 64.5 90.2 73.5 - - - -\n(Dong & Huang, 2018) - - - - - - - 75.9 97.3 84.8\npre-trained weights from (Devlin et al., 2019) - Toronto Books+ Wikipedia\nBi-encoder 83.3±0.2 81.7±0.2 56.5±0.4 66.8±0.7 89.0±1.0 74.6±0.5 80.9±0.6 80.6±0.4 98.2±0.1 88.0±0.3\nPoly-encoder (First-m) 16 85.2±0.1 83.9±0.2 56.7±0.2 67.0±0.9 88.8±0.3 74.6±0.6 81.7±0.5 81.4±0.6 98.2±0.1 88.5±0.4\nPoly-encoder (Learnt-m) 1684.4±0.1 83.2±0.1 57.7±0.2 67.8±0.3 88.6±0.2 75.1±0.2 81.5±0.1 81.2±0.2 98.2±0.0 88.3±0.1\nPoly-encoder (First-m) 64 86.0±0.2 84.2±0.2 57.1±0.2 66.9±0.7 89.1±0.2 74.7±0.4 82.2±0.6 81.9±0.5 98.4±0.0 88.8±0.3\nPoly-encoder (Learnt-m) 6484.9±0.1 83.7±0.2 58.3±0.4 67.0±0.9 89.2±0.2 74.7±0.6 81.8±0.1 81.3±0.2 98.2±0.1 88.4±0.1\nPoly-encoder (First-m) 360 86.3±0.1 84.6±0.3 57.8±0.5 67.0±0.5 89.6±0.9 75.0±0.6 82.7±0.4 82.2±0.6 98.4±0.1 89.0±0.4\nPoly-encoder (Learnt-m) 36085.3±0.3 83.7±0.2 57.7±0.3 68.9±0.4 89.9±0.5 76.2±0.2 81.5±0.1 80.9±0.1 98.1±0.0 88.1±0.1\nCross-encoder 87.1±0.1 84.8±0.3 59.4±0.4 67.4±0.7 90.5±0.3 75.6±0.4 83.3±0.4 82.8±0.3 98.4±0.1 89.4±0.2\nOur pre-training on Toronto Books+ Wikipedia\nBi-encoder 84.6±0.1 82.0±0.1 54.9±0.5 64.5±0.5 88.1±0.2 72.6±0.4 80.9±0.5 80.8±0.5 98.4±0.1 88.2±0.4\nPoly-encoder (First-m) 16 84.1±0.2 81.4±0.2 53.9±2.7 63.3±2.9 87.2±1.5 71.6±2.4 80.8±0.5 80.6±0.4 98.4±0.1 88.1±0.3\nPoly-encoder (Learnt-m) 1685.4±0.2 82.7±0.1 56.0±0.4 65.3±0.9 88.2±0.7 73.2±0.7 84.0±0.1 83.4±0.2 98.7±0.0 89.9±0.1\nPoly-encoder (First-m) 64 86.1±0.4 83.9±0.3 55.6±0.9 64.3±1.5 87.8±0.4 72.5±1.0 80.9±0.6 80.7±0.6 98.4±0.0 88.2±0.4\nPoly-encoder (Learnt-m) 6485.6±0.1 83.3±0.1 56.2±0.4 65.8±0.7 88.4±0.3 73.5±0.5 84.0±0.1 83.4±0.1 98.7±0.0 89.9±0.0\nPoly-encoder (First-m) 360 86.6±0.3 84.4±0.2 57.5±0.4 66.5±1.2 89.0±0.5 74.4±0.7 81.3±0.6 81.1±0.4 98.4±0.2 88.4±0.3\nPoly-encoder (Learnt-m) 36086.1±0.1 83.8±0.1 56.5±0.8 65.8±0.7 88.5±0.6 73.6±0.6 84.2±0.2 83.7±0.0 98.7±0.1 90.1±0.0\nCross-encoder 87.3±0.5 84.9±0.3 57.7±0.5 65.3±1.0 89.7±0.5 73.8±0.6 83.2±0.8 83.1±0.7 98.7±0.1 89.7±0.5\nOur pre-training on Reddit\nBi-encoder 86.9±0.1 84.8±0.1 60.1±0.4 70.9±0.5 90.6±0.3 78.1±0.3 83.7±0.7 83.6±0.7 98.8±0.1 90.1±0.4\nPoly-encoder (First-m) 16 89.0±0.1 86.4±0.3 60.4±0.3 70.7±0.7 91.0±0.4 78.0±0.5 84.3±0.3 84.3±0.2 98.9±0.0 90.5±0.1\nPoly-encoder (Learnt-m) 1688.6±0.3 86.3±0.3 61.1±0.4 71.6±0.6 91.3±0.3 78.4±0.4 86.1±0.1 86.0±0.1 99.0±0.1 91.5±0.1\nPoly-encoder (First-m) 64 89.5±0.1 87.3±0.2 61.0±0.4 70.9±0.6 91.5±0.5 78.0±0.3 84.0±0.4 83.9±0.4 98.8±0.0 90.3±0.3\nPoly-encoder (Learnt-m) 6489.0±0.1 86.5±0.2 60.9±0.6 71.2±0.8 91.3±0.4 78.2±0.7 86.2±0.1 85.9±0.1 99.1±0.0 91.5±0.1\nPoly-encoder (First-m) 360 90.0±0.1 87.3±0.1 61.1±1.9 70.9±2.1 91.5±0.9 77.9±1.6 84.8±0.5 84.6±0.5 98.9±0.1 90.7±0.3\nPoly-encoder (Learnt-m) 36089.2±0.1 86.8±0.1 61.2±0.2 71.4±1.0 91.1±0.3 78.3±0.7 86.3±0.1 85.9±0.1 99.1±0.0 91.5±0.0\nCross-encoder 90.3±0.2 87.9±0.2 63.9±0.3 71.7±0.3 92.4±0.5 79.0±0.2 86.7±0.1 86.5±0.1 99.1±0.0 91.9±0.0\nTable 10: Validation and test performances of Bi-, Poly- and Cross-encoders. Scores are shown for\nConvAI2, DSTC7 Track 1 and Ubuntu v2, and the previous state-of-the-art models in the literature.\n14"
}