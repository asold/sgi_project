{
  "title": "Pyramidal Recurrent Unit for Language Modeling",
  "url": "https://openalex.org/W2888862264",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A2126016355",
      "name": "Sachin Mehta",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A5033228519",
      "name": "Rik Koncel-Kedziorski",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A2066712224",
      "name": "Mohammad Rastegari",
      "affiliations": [
        "Allen Institute"
      ]
    },
    {
      "id": "https://openalex.org/A91410043",
      "name": "Hannaneh Hajishirzi",
      "affiliations": [
        "University of Washington"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2549416390",
    "https://openalex.org/W1938755728",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2553397501",
    "https://openalex.org/W2963494889",
    "https://openalex.org/W4231697575",
    "https://openalex.org/W2964265128",
    "https://openalex.org/W2952436057",
    "https://openalex.org/W2963537482",
    "https://openalex.org/W2951559648",
    "https://openalex.org/W2963984224",
    "https://openalex.org/W2605203995",
    "https://openalex.org/W2083844448",
    "https://openalex.org/W2618101654",
    "https://openalex.org/W2103504761",
    "https://openalex.org/W2963748792",
    "https://openalex.org/W1601924930",
    "https://openalex.org/W2962971773",
    "https://openalex.org/W2963318827",
    "https://openalex.org/W1924770834",
    "https://openalex.org/W2553303224",
    "https://openalex.org/W2565031282",
    "https://openalex.org/W2963363070",
    "https://openalex.org/W2962858109",
    "https://openalex.org/W2962832505",
    "https://openalex.org/W2100649405",
    "https://openalex.org/W2751185861",
    "https://openalex.org/W2962964385",
    "https://openalex.org/W2963385194",
    "https://openalex.org/W2964159778",
    "https://openalex.org/W4299838440",
    "https://openalex.org/W2951595529",
    "https://openalex.org/W4300427683",
    "https://openalex.org/W2613904329",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2767693128",
    "https://openalex.org/W4294555862",
    "https://openalex.org/W2124386111",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2964182247",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W4297692367",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W2963418739",
    "https://openalex.org/W2212703438",
    "https://openalex.org/W2964045325",
    "https://openalex.org/W2963374479",
    "https://openalex.org/W2786951478",
    "https://openalex.org/W2963938518",
    "https://openalex.org/W2963266340",
    "https://openalex.org/W2757047188",
    "https://openalex.org/W1826234144"
  ],
  "abstract": "LSTMs are powerful tools for modeling contextual information, as evidenced by their success at the task of language modeling. However, modeling contexts in very high dimensional space can lead to poor generalizability. We introduce the Pyramidal Recurrent Unit (PRU), which enables learning representations in high dimensional space with more generalization power and fewer parameters. PRUs replace the linear transformation in LSTMs with more sophisticated interactions such as pyramidal or grouped linear transformations. This architecture gives strong results on word-level language modeling while reducing parameters significantly. In particular, PRU improves the perplexity of a recent state-of-the-art language model by up to 1.3 points while learning 15-20% fewer parameters. For similar number of model parameters, PRU outperforms all previous RNN models that exploit different gating mechanisms and transformations. We provide a detailed examination of the PRU and its behavior on the language modeling tasks. Our code is open-source and available at https://sacmehta.github.io/PRU/.",
  "full_text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4620–4630\nBrussels, Belgium, October 31 - November 4, 2018.c⃝2018 Association for Computational Linguistics\n4620\nPyramidal Recurrent Unit for Language Modeling\nSachin Mehta1, Rik Koncel-Kedziorski1, Mohammad Rastegari2, and Hannaneh Hajishirzi1\n1University of Washington, Seattle, W A, USA\n{sacmehta, kedzior, hannaneh}@uw.edu\n2Allen Institute for AI and XNOR.AI, Seattle, W A, USA\nmohammadr@allenai.org\nAbstract\nLSTMs are powerful tools for modeling con-\ntextual information, as evidenced by their suc-\ncess at the task of language modeling. How-\never, modeling contexts in very high dimen-\nsional space can lead to poor generalizability.\nWe introduce the Pyramidal Recurrent Unit\n(PRU), which enables learning representations\nin high dimensional space with more gener-\nalization power and fewer parameters. PRUs\nreplace the linear transformation in LSTMs\nwith more sophisticated interactions including\npyramidal and grouped linear transformations.\nThis architecture gives strong results on word-\nlevel language modeling while reducing the\nnumber of parameters signiﬁcantly. In partic-\nular, PRU improves the perplexity of a recent\nstate-of-the-art language model Merity et al.\n(2018) by up to 1.3 points while learning 15-\n20% fewer parameters. For similar number of\nmodel parameters, PRU outperforms all previ-\nous RNN models that exploit different gating\nmechanisms and transformations. We provide\na detailed examination of the PRU and its be-\nhavior on the language modeling tasks. Our\ncode is open-source and available at https:\n//sacmehta.github.io/PRU/.\n1 Introduction\nLong short term memory (LSTM) units (Hochre-\niter and Schmidhuber, 1997) are popular for many\nsequence modeling tasks and are used extensively\nin language modeling. A key to their success\nis their articulated gating structure, which al-\nlows for more control over the information passed\nalong the recurrence. However, despite the so-\nphistication of the gating mechanisms employed\nin LSTMs and similar recurrent units, the input\nand context vectors are treated with simple linear\ntransformations prior to gating. Non-linear trans-\nformations such as convolutions (Kim et al., 2016)\nhave been used, but these have not achieved the\nFigure 1: Comparison of training (solid lines) and\nvalidation (dashed lines) perplexities on the Penn\nTreebank with standard dropout for pyramidal re-\ncurrent units (PRU) and LSTM. PRUs learn latent\nrepresentations in very high-dimensional space\nwith good generalizability and fewer parameters.\nSee Section 3 for more details about PRUs. Best\nviewed in color.\nperformance of well regularized LSTMs for lan-\nguage modeling (Melis et al., 2018).\nA natural way to improve the expressiveness\nof linear transformations is to increase the num-\nber of dimensions of the input and context vec-\ntors, but this comes with a signiﬁcant increase in\nthe number of parameters which may limit gen-\neralizability. An example is shown in Figure 1,\nwhere LSTMs performance decreases with the in-\ncrease in dimensions of the input and context vec-\ntors. Moreover, the semantics of the input and con-\ntext vectors are different, suggesting that each may\nbeneﬁt from specialized treatment.\nGuided by these insights, we introduce a new\nrecurrent unit, the Pyramidal Recurrent Unit\n(PRU), which is based on the LSTM gating struc-\nture. Figure 2 provides an overview of the PRU. At\n4621\nFigure 2: Block diagram visualizing the transformations in pyramidal recurrent unit (left) and the LSTM\n(bottom right) along with the LSTM gating architecture (top right). Blue, red, green (or orange), and\npurple signify the current input xt, output of the previous cell ht−1, the output of transformations,\nand the fused output, respectively. The color intensity is used to represent sub-sampling and grouping\noperations.\nthe heart of the PRU is the pyramidal transforma-\ntion (PT), which uses subsampling to effect multi-\nple views of the input vector. The subsampled rep-\nresentations are combined in a pyramidal fusion\nstructure, resulting in richer interactions between\nthe individual dimensions of the input vector than\nis possible with a linear transformation. Context\nvectors, which have already undergone this trans-\nformation in the previous cell, are modiﬁed with\na grouped linear transformation (GLT) which al-\nlows the network to learn latent representations in\nhigh dimensional space with fewer parameters and\nbetter generalizability (see Figure 1).\nWe show that PRUs can better model contextual\ninformation and demonstrate performance gains\non the task of language modeling. The PRU im-\nproves the perplexity of the current state-of-the-art\nlanguage model (Merity et al., 2018) by up to 1.3\npoints, reaching perplexities of 56.56 and 64.53 on\nthe Penn Treebank and WikiText2 datasets while\nlearning 15-20% fewer parameters. Replacing an\nLSTM with a PRU results in improvements in per-\nplexity across a variety of experimental settings.\nWe provide detailed ablations which motivate the\ndesign of the PRU architecture, as well as detailed\nanalysis of the effect of the PRU on other compo-\nnents of the language model.\n2 Related work\nMultiple methods, including a variety of gating\nstructures and transformations, have been pro-\nposed to improve the performance of recurrent\nneural networks (RNNs). We ﬁrst describe these\napproaches and then provide an overview of recent\nwork in language modeling.\nGating-based mechanisms: The performance\nof RNNs have been greatly improved by gat-\ning mechanisms such as LSTMs (Hochreiter and\nSchmidhuber, 1997), GRUs (Chung et al., 2014),\npeep-hole connections (Gers and Schmidhuber,\n2000), SRUs (Lei et al., 2018), and RANs (Lee\net al., 2017). In this paper, we extend the gating\narchitecture of LSTMs (Hochreiter and Schmid-\nhuber, 1997), a widely used recurrent unit across\ndifferent domains.\nTransformations: Apart from the widely used\nlinear transformation for modeling the tempo-\nral data, another transformation that has gained\npopularity is convolution (LeCun et al., 1995).\nConvolution-based methods have gained attention\nin computer vision tasks (Krizhevsky et al., 2012)\nas well as some of the natural language process-\ning tasks including machine translation (Gehring\net al., 2017). Convolution-based methods for lan-\nguage modeling, such as CharCNN (Kim et al.,\n2016), have not yet achieved the performance of\nwell regularized LSTMs (Melis et al., 2018). We\ninherit ideas from convolution-based approaches,\nsuch as sub-sampling, to learn richer representa-\ntions (Krizhevsky et al., 2012; Han et al., 2017).\n4622\nRegularization: Methods such as dropout (Sri-\nvastava et al., 2014), variational dropout (Kingma\net al., 2015), and weight dropout (Merity et al.,\n2018) have been proposed to regularize RNNs.\nThese methods can be easily applied to PRUs.\nOther efﬁcient RNN networks: Recently, there\nhas been an effort to improve the efﬁciency of\nRNNs. These approaches include quantization\n(Xu et al., 2018), skimming (Seo et al., 2018;\nYu et al., 2017), skipping (Campos et al., 2018),\nand query reduction (Seo et al., 2017). These\napproaches extend standard RNNs and therefore,\nthese approaches are complementary to our work.\nLanguage modeling: Language modeling is a\nfundamental task for NLP and has garnered sig-\nniﬁcant attention in recent years (see Table 1 for\ncomparison with state-of-the-art methods). Merity\net al. (2018) introduce regularization techniques\nsuch as weight dropping which, coupled with a\nnon-monotonically triggered ASGD optimization,\nachieves strong performance improvements. Yang\net al. (2018) extend Merity et al. (2018) with the\nmixture of softmaxes (MoS) technique, which in-\ncreases the rank of the matrix used to compute\nnext-token probabilities. Further, Merity et al.\n(2017) and Krause et al. (2018) propose methods\nto improve inference by adapting models to recent\nsequence history. Our work is complementary to\nthese recent softmax layer and inference proce-\ndure improvements.\nWe extend state-of-the-art language model in\nMerity et al. (2018) by replacing the LSTM with\nthe PRU. We show by experiments that the PRU\nimproves the performance of Merity et al. (2018)\nwhile learning fewer parameters.\n3 Pyramidal Recurrent Units\nWe introduce Pyramidal Recurrent Units (PRUs),\na new RNN architecture which improves modeling\nof context by allowing for higher dimensional vec-\ntor representations while learning fewer parame-\nters. Figure 2 provides an overview of PRU. We\nﬁrst elaborate on the details of the pyramidal trans-\nformation and the grouped linear transformation.\nWe then describe our recurrent unit, PRU.\n3.1 Pyramidal transformation for input\nThe basic transformation in many recurrent units\nis a linear transformation FL deﬁned as:\ny = FL(x) =W ·x, (1)\nwhere W ∈RN×M are learned weights that lin-\nearly map x ∈RN to y ∈RM . To simplify nota-\ntion, we omit the biases.\nMotivated by successful applications of sub-\nsampling in computer vision (e.g., (Burt and Adel-\nson, 1987; Lowe, 1999; Krizhevsky et al., 2012;\nMehta et al., 2018)), we subsample input vec-\ntor x into K pyramidal levels to achieve repre-\nsentation of the input vector at multiple scales.\nThis sub-sampling operation produces K vectors,\nrepresented as xk ∈ R\nN\n2k−1 , where 2k−1 is the\nsampling rate and k = {1,··· ,K}. We learn\nscale-speciﬁc transformations Wk ∈ R\nN\n2k−1 ×M\nK\nfor each k = {1,···K}. The transformed sub-\nsamples are concatenated to produce the pyrami-\ndal analog to y, here denoted as ¯ y∈RM :\n¯ y= FP (x) =\n[\nW1 ·x1,··· ,WK ·xK]\n, (2)\nwhere [·,·] indicates concatenation. We note that\npyramidal transformation with K = 1is the same\nas the linear transformation.\nTo improve gradient ﬂow inside the recurrent\nunit, we combine the input and output using an\nelement-wise sum (when dimension matches) to\nproduce residual analog of pyramidal transforma-\ntion, as shown in Figure 2 (He et al., 2016).\nSub-sampling: We sub-sample the input vector\nx into K pyramidal levels using the kernel-based\napproach (LeCun et al., 1995; Krizhevsky et al.,\n2012). Let us assume that we have a kernel κwith\n2e+ 1elements. Then, the input vector x can be\nsub-sampled as:\nxk =\nN/s∑\ni=1\ne∑\nj=−e\nxk−1[si]κ[j], (3)\nwhere srepresents the stride andk= {2,··· ,K}.\nReduction in parameters: The number of pa-\nrameters learned by the linear transformation and\nthe pyramidal transformation with K pyramidal\nlevels to map x ∈ RN to ¯ y∈ RM are NM\nand NM\nK\nK∑\nk=1\n2(1−k) respectively. Thus, pyramidal\ntransformation reduces the parameters of a linear\ntransformation by a factor of K(∑K\nk=1 2(1−k))−1.\nFor example, the pyramidal transformation (with\nK = 4 and N = M = 600) learns 53% fewer\nparameters than the linear transformation.\n4623\n3.2 Grouped linear transformation for\ncontext\nMany RNN architectures apply linear transforma-\ntions to both the input and context vector. How-\never, this may not be ideal due to the differing se-\nmantics of each vector. In many NLP applications\nincluding language modeling, the input vector is\na dense word embedding which is shared across\nall contexts for a given word in a dataset. In con-\ntrast, the context vector is highly contextualized\nby the current sequence. The differences between\nthe input and context vector motivate their sepa-\nrate treatment in the PRU architecture.\nThe weights learned using the linear transfor-\nmation (Eq. 1) are reused over multiple time steps,\nwhich makes them prone to over-ﬁtting (Gal and\nGhahramani, 2016). To combat over-ﬁtting, var-\nious methods, such as variational dropout (Gal\nand Ghahramani, 2016) and weight dropout (Mer-\nity et al., 2018), have been proposed to regularize\nthese recurrent connections. To further improve\ngeneralization abilities while simultaneously en-\nabling the recurrent unit to learn representations at\nvery high dimensional space, we propose to use\ngrouped linear transformation (GLT) instead of\nstandard linear transformation for recurrent con-\nnections (Kuchaiev and Ginsburg, 2017). While\npyramidal and linear transformations can be ap-\nplied to transform context vectors, our experimen-\ntal results in Section 4.4 suggests that GLTs are\nmore effective.\nThe linear transformation FL : RN → RM\nmaps h ∈ RN linearly to z ∈ RM . Grouped\nlinear transformations break the linear interac-\ntions by factoring the linear transformation into\ntwo steps. First, a GLT splits the input vector\nh ∈ RN into g smaller groups such that h =\n{h1,··· ,hg},∀hi ∈R\nN\ng . Second, a linear trans-\nformation FL : R\nN\ng →R\nM\ng is applied to map hi\nlinearly to zi ∈ R\nM\ng , for each i = {1,··· ,g}.\nThe gresultant output vectors zi are concatenated\nto produce the ﬁnal output vector ¯ z∈RM .\n¯ z= FG(h) =\n[\nW1 ·h1,··· ,Wg ·hg]\n(4)\nGLTs learn representations at low dimensional-\nity. Therefore, a GLT requires g fewer parame-\nters than the linear transformation. We note that\nGLTs are subset of linear transformations. In a lin-\near transformation, each neuron receives an input\nfrom each element in the input vector while in a\nGLT, each neuron receives an input from a subset\nof the input vector. Therefore, GLT is the same as\na linear transformation when g= 1.\n3.3 Pyramidal Recurrent Unit\nWe extend the basic gating architecture of LSTM\nwith the pyramidal and grouped linear transfor-\nmations outlined above to produce the Pyramidal\nRecurrent Unit (PRU), whose improved sequence\nmodeling capacity is evidenced in Section 4.\nAt time t, the PRU combines the input vectorxt\nand the previous context vector (or previous hid-\nden state vector) ht−1 using the following trans-\nformation function as:\nˆGv(xt,ht−1) = ˆFP (xt) +FG(ht−1), (5)\nwhere v ∈{f,i,c,o }indexes the various gates in\nthe LSTM model, and ˆFP (·) and FG(·) represents\nthe pyramidal and grouped linear transformations\ndeﬁned in Eqns. 2 and 4, respectively.\nWe will now incorporate ˆGv(·,·) into LSTM\ngating architecture to produce PRU. At time t,\na PRU cell takes xt ∈ RN , ht−1 ∈ RM , and\nct−1 ∈RM as inputs to produce forget ft, input it,\noutput ot, and content ˆ ct gate signals. The inputs\nare combined with these gate signals to produce\ncontext vector ht ∈RM and cell state ct ∈RM .\nMathematically, the PRU with the LSTM gating\narchitecture can be deﬁned as:\nft = σ\n(\nˆGf (xt,ht−1)\n)\nit = σ\n(\nˆGi(xt,ht−1)\n)\nˆ ct = tanh\n(\nˆGc(xt,ht−1)\n)\not = σ\n(\nˆGo(xt,ht−1)\n)\nct = ft ⊗ct−1 + it ⊗ˆ ct\nht = ot ⊗tanh(ct)\n(6)\nwhere ⊗represents the element-wise multiplica-\ntion operation, andσand tanhare the sigmoid and\nhyperbolic tangent activation functions. We note\nthat LSTM is a special case of PRU wheng=K=1.\n4 Experiments\nTo showcase the effectiveness of the PRU, we\nevaluate the performance on two standard datasets\nfor word-level language modeling and compare\nwith state-of-the-art methods. Additionally, we\nprovide a detailed examination of the PRU and its\nbehavior on the language modeling tasks.\n4624\n4.1 Set-up\nDataset: Following recent works, we compare\non two widely used datasets, the Penn Tree-\nbank (PTB) (Marcus et al., 1993) as prepared\nby Mikolov et al. (2010) and WikiText2 (WT-2)\n(Merity et al., 2017). For both datasets, we follow\nthe same training, validation, and test splits as in\nMerity et al. (2018).\nLanguage Model: We extend the language\nmodel, AWD-LSTM (Merity et al., 2018), by re-\nplacing LSTM layers with PRU. Our model uses\n3-layers of PRU with an embedding size of 400.\nThe number of parameters learned by state-of-the-\nart methods vary from 18M to 66M with major-\nity of the methods learning about 22M to 24M\nparameters on the PTB dataset. For a fair com-\nparison with state-of-the-art methods, we ﬁx the\nmodel size to 19M and vary the value of g and\nhidden layer sizes so that total number of learned\nparameters is similar across different conﬁgura-\ntions. We use 1000, 1200, and 1400 as hidden\nlayer sizes for values ofg=1,2, and 4, respectively.\nWe use the same settings for the WT-2 dataset. We\nset the number of pyramidal levels K to two in\nour experiments and use average pooling for sub-\nsampling. These values are selected based on our\nablation experiments on the validation set (Section\n4.4). We measure the performance of our models\nin terms of word-level perplexity. We follow the\nsame training strategy as in Merity et al. (2018).\nTo understand the effect of regularization meth-\nods on the performance of PRUs, we perform ex-\nperiments under two different settings: (1) Stan-\ndard dropout: We use a standard dropout (Srivas-\ntava et al., 2014) with probability of 0.5 after em-\nbedding layer, the output between LSTM layers,\nand the output of ﬁnal LSTM layer. (2) Advanced\ndropout: We use the same dropout techniques with\nthe same dropout values as in Merity et al. (2018).\nWe call this model as AWD-PRU.\n4.2 Results\nTable 1 compares the performance of the PRU\nwith state-of-the-art methods. We can see that the\nPRU achieves the best performance with fewer pa-\nrameters.\nStandard dropout: PRUs achieve either the\nsame or better performance than LSTMs. In par-\nticular, the performance of PRUs improves with\nthe increasing value of g. At g = 4, PRUs out-\nperform LSTMs by about 4 points on the PTB\ndataset and by about 3 points on the WT-2 dataset.\nThis is explained in part by the regularization ef-\nfect of the grouped linear transformation (Figure\n1). With grouped linear and pyramidal transfor-\nmations, PRUs learn rich representations at very\nhigh dimensional space while learning fewer pa-\nrameters. On the other hand, LSTMs overﬁt to\nthe training data at such high dimensions and learn\n1.4×to 1.8×more parameters than PRUs.\nAdvanced dropouts: With the advanced\ndropouts, the performance of PRUs improves by\nabout 4 points on the PTB dataset and 7 points\non the WT-2 dataset. This further improves with\nﬁnetuning on the PTB (about 2 points) and WT-2\n(about 1 point) datasets.\nComparison with state-of-the-art: For similar\nnumber of parameters, the PRU with standard\ndropout outperforms most of the state-of-the-art\nmethods by large margin on the PTB dataset (e.g.\nRAN (Lee et al., 2017) by 16 points with 4M less\nparameters, QRNN (Bradbury et al., 2017) by 16\npoints with 1M more parameters, and NAS (Zoph\nand Le, 2017) by 1.58 points with 6M less param-\neters). With advanced dropouts, the PRU delivers\nthe best performance. On both datasets, the PRU\nimproves the perplexity by about 1 point while\nlearning 15-20% fewer parameters.\nInference: PRU is a drop-in replacement for\nLSTM, therefore, it can improve language mod-\nels with modern inference techniques such as dy-\nnamic evaluation (Krause et al., 2018). When we\nevaluate PRU-based language models (only with\nstandard dropout) with dynamic evaluation on the\nPTB test set, the perplexity of PRU ( g = 4,k =\n2,M = 1400) improves from 62.42 to 55.23 while\nthe perplexity of an LSTM (M = 1000) with simi-\nlar settings improves from 66.29 to 58.79; suggest-\ning that modern inference techniques are equally\napplicable to PRU-based language models.\n4.3 Analysis\nIt is shown above that the PRU can learn represen-\ntations at higher dimensionality with more gener-\nalization power, resulting in performance gains for\nlanguage modeling. A closer analysis of the im-\npact of the PRU in a language modeling system\nreveals several factors that help explain how the\nPRU achieves these gains.\n4625\nWT-2 PTB\nModel Params Val Test Params Val Test\nVariational LSTM (Gal and Ghahramani, 2016) – – – 20 M – 78.6\nCharCNN (Kim et al., 2016) – – – 19 M – 78.9\nPointer Sentinel-LSTM (Merity et al., 2017) – – – 19 M 72.4 70.9\nRHN (Zilly et al., 2016) – – – 23 M 67.9 65.4\nNAS Cell (Zoph and Le, 2017) – – – 25 M – 64.0\nVariational LSTM - (Inan et al., 2017) 28 M 91.5 87 24 M 75.7 73.2\nSRU - 6 layers (Lei et al., 2018) – – – 24 M 63.4 60.3\nQRNN (Bradbury et al., 2017) – – – 18 M 82.1 78.3\nRAN (Lee et al., 2017) – – – 22 M – 78.5\n4-layer skip-connection LSTM (Melis et al., 2018) – – – 24 M 60.9 58.3\nAWD-LSTM - (Merity et al., 2018) 33 M 69.1 66 24 M 60.7 58.8\nAWD-LSTM - (Merity et al., 2018)-ﬁnetuned 33 M 68.6 65.8 24 M 60 57.3\nVariational LSTM (Gal and Ghahramani, 2016) – – – 66 M – 73.4\nNAS Cell (Zoph and Le, 2017) – – – 54 M – 62.4\nQuantized LSTM - Full precision (Xu et al., 2018) – – 100.1 – – 89.8\nQuantized LSTM - 2 bit (Xu et al., 2018) – – 106.1 – – 95.8\nWith standard dropout\nLSTM (M= 1000) 29 M 78.93 75.08 20 M 68.57 66.29\nLSTM (M= 1200) 35 M 77.93 74.48 26 M 69.17 67.16\nLSTM (M= 1400) 42 M 77.55 74.44 33 M 70.88 68.55\nOurs -PRU (g= 1, K= 2, M= 1000) 28 M 79.15 76.59 19 M 69.8 67.78\nOurs -PRU (g= 2, K= 2, M= 1200) 28 M 76.62 73.79 19 M 67.17 64.92\nOurs -PRU (g= 4, K= 2, M= 1400) 28 M 75.46 72.77 19 M 64.76 62.42\nWith advanced dropouts\nOurs - AWD-PRU (g= 1, K= 2, M= 1000) 28 M 71.84 68.6 19 M 61.72 59.54\nOurs - AWD-PRU (g= 2, K= 2, M= 1200) 28 M 68.57 65.7 19 M 60.81 58.65\nOurs - AWD-PRU (g= 4, K= 2, M= 1400) 28 M 68.17 65.3 19 M 60.62 58.33\nOurs - AWD-PRU (g= 4, K= 2, M= 1400)-ﬁnetuned 28 M 67.19 64.53 19 M 58.46 56.56\nTable 1: Comparison of single model word-level perplexity of our model with state-of-the-art on vali-\ndation and test sets of Penn Treebank and Wikitext-2 dataset. For evaluation, we select the model with\nminimum validation loss. Lower perplexity value represents better performance.\nConﬁdence: As exempliﬁed in Table 2a, the\nPRU tends toward more conﬁdent decisions, plac-\ning more of the probability mass on the top next-\nword prediction than the LSTM. To quantify this\neffect, we calculate the entropy of the next-token\ndistribution for both the PRU and the LSTM using\n3687 contexts from the PTB validation set. Fig-\nure 3 shows a histogram of the entropies of the dis-\ntribution, where bins of size 0.23 are used to effect\ncategories. We see that the PRU more often pro-\nduces lower entropy distributions corresponding to\nhigher conﬁdences for next-token choices. This is\nevidenced by the mass of the red PRU curve lying\nin the lower entropy ranges compared to the blue\nLSTM’s curve. The PRU can produce conﬁdent\ndecisions in part because more information is en-\ncoded in the higher dimensional context vectors.\nVariance in word embeddings: The PRU has\nthe ability to model individual words at differ-\nent resolutions through the pyramidal transform;\nwhich provides multiple paths for the gradient to\nthe embedding layer (similar to multi-task learn-\ning) and improves the ﬂow of information. When\nconsidering the embeddings by part of speech, we\nﬁnd that the pyramid level 1 embeddings exhibit\nhigher variance than the LSTM across all POS cat-\negories (Figure 4), and that pyramid level 2 em-\nbeddings show extremely low variance 1. We hy-\npothesize that the LSTM must encode both coarse\ngroup similarities and individual word differences\ninto the same vector space, reducing the space be-\ntween individual words of the same category. The\nPRU can rely on the subsampled embeddings to\n1POS categories are computed using NLTK toolkit.\n4626\nFigure 3: Histogram of the entropies of next-token\ndistributions predicted by the PRU (mean 3.80)\nand the LSTM (mean 3.93) on the PTB validation\nset. Lower entropy values indicate higher conﬁ-\ndence decisions, which is desirable if decisions are\noften correct.\nFigure 4: Variance of learned word embeddings for\ndifferent categories of words on the PTB valida-\ntion set. We compute the variance of a group of\nembeddings as the average squared euclidean dis-\ntance to their mean. Higher variance may allow for\nbetter intra-category distinctions. The PRU with\npyramid levels 1 and 2 is shown.\naccount for coarse-grained group similarities, al-\nlowing for ﬁner individual word distinctions in the\nembedding layer. This hypothesis is strengthened\nby the entropy results described above: a model\nwhich can make ﬁner distinctions between indi-\nvidual words can more conﬁdently assign proba-\nbility mass. A model that cannot make these dis-\ntinctions, such as the LSTM, must spread its prob-\nability mass across a larger class of similar words.\nGradient-based analysis: Saliency analysis us-\ning gradients help identify relevant words in a\ntest sequence that contribute to the prediction\n(Gevrey et al., 2003; Li et al., 2016; Arras et al.,\n2017). These approaches compute the relevance\nas the squared norm of the gradients obtained\nthrough back-propagation. Table 2a visualizes the\nheatmaps for different sequences. PRUs, in gen-\neral, give more relevance to contextual words than\nLSTMs, such as southeast (sample 1), cost (sam-\nple 2), face (sample 4), and introduced (sample\n5), which help in making more conﬁdent deci-\nsions. Furthermore, when gradients during back-\npropagation are visualized (Selvaraju et al., 2017)\n(Table 2b), we ﬁnd that PRUs have better gradient\ncoverage than LSTMs, suggesting PRUs use more\nfeatures than LSTMs that contributes to the deci-\nsion. This also suggests that PRUs update more\nparameters at each iteration which results in faster\ntraining. Language model in (Merity et al., 2018)\ntakes 500 and 750 epochs to converge with PRU\nand LSTM as a recurrent unit, respectively.\n4.4 Ablation studies\nIn this section, we provide a systematic analysis\nof our design choices. Our training methodology\nis the same as described in Section 4.1 with the\nstandard dropouts. For a thorough understanding\nof our design choices, we use a language model\nwith a single layer of PRU and ﬁx the size of em-\nbedding and hidden layers to 600. The word-level\nperplexities are reported on the validation sets of\nthe PTB and the WT-2 datasets.\nPyramidal levels K and groups g: The two\nhyper-parameters that control the trade-off be-\ntween performance and number of parameters in\nPRUs are the number of pyramidal levels K and\ngroups g. Figure 5 provides a trade-off between\nperplexity and recurrent unit (RU) parameters2.\nVariable K and ﬁxed g: When we increase the\nnumber of pyramidal levels K at a ﬁxed value of\ng, the performance of the PRU drops by about 1 to\n4 points while reducing the total number of recur-\nrent unit parameters by up to 15%. We note that\nthe PRU with K = 4at g= 1delivers similar per-\nformance as the LSTM while learning about 15%\nfewer recurrent unit parameters.\nFixed K and variable g: When we vary the\nvalue of gat ﬁxed number of pyramidal levels K,\nthe total number of recurrent unit parameters de-\ncreases signiﬁcantly with a minimal impact on the\nperplexity. For example, PRUs with K = 2 and\ng = 4learns 77% fewer recurrent unit parameters\nwhile its perplexity (lower is better) increases by\nabout 12% in comparison to LSTMs. Moreover,\nthe decrease in number of parameters at higher\nvalue of g enables PRUs to learn the representa-\ntions in high dimensional space with better gener-\nalizability (Table 1).\n2# total params = # embedding params + # RU params\n4627\nGradient-based sensitivity analysis heatmaps LSTM top-5 PRU top-5\nReference:the tremor was centered near<unk>southeast ofsanfrancisco\nReference:the massages last N minutes and typically cost about$N.\nReference:he visits the same department every two or threeweeks.\nReference:but pipeline companies estimate they still face $ N billion in liabilities from<unk>disputes including $ Nbillion.\nReference:chicken chains also are feeling more pressure from mcdonald’s corp. which introduced its<unk><unk>thisyear.\n(a) Gradient-based saliency analysis. Salience score is proportional to cell coverage in red.\nLSTM\nPRU\nl\n(b) Gradients during back-propagation for a test sequence (x-axis: dimensions of word vector, y-axis: test sequence)\nTable 2: Qualitative comparison between the LSTM and the PRU: (a) Gradient-based saliency analysis\nalong with top-5 predicted words. (b) Gradients during back-propagation. For computing the gradients\nfor a given test sequence, the top-1 predicted word was used as the true predicted word. Best viewed in\ncolor.\n(a) PTB\n (b) WT-2\nFigure 5: Impact of number of groups g and pyramidal levels K on the perplexity. Reduction in recur-\nrent unit (RU) parameters is computed with respect to LSTM. Lower perplexity value represents better\nperformance.\n4628\nTransformations: Table 3 shows the impact of\ndifferent transformations of the input vectorxt and\nthe context vector ht−1. We make following ob-\nservations: (1) Using the pyramidal transforma-\ntion for the input vectors improves the perplex-\nity by about 1 point on both the PTB and WT-\n2 datasets while reducing the number of recur-\nrent unit parameters by about 14% (see R1 and\nR4). We note that the performance of the PRU\ndrops by up to 1 point when residual connections\nare not used (R4 and R6). (2) Using the grouped\nlinear transformation for context vectors reduces\nthe total number of recurrent unit parameters by\nabout 75% while the performance drops by about\n11% (see R3 and R4). When we use the pyrami-\ndal transformation instead of the linear transfor-\nmation, the performance drops by up to 2% while\nthere is no signiﬁcant drop in the number of pa-\nrameters (R4 and R5).\nSubsampling: We set sub-sampling kernel κ\n(Eq. 3) with stride s = 2 and size of 3 ( e = 1)\nin four different ways: (1) Skip: We skip every\nother element in the input vector. (2)Convolution:\nWe initialize the elements ofκrandomly from nor-\nmal distribution and learn them during training the\nmodel. We limit the output values between -1 and\n1 using tanhactivation function to make training\nstable. (3) Avg. pool: We initialize the elements\nof κto 1\n3 . (4) Max pool: We select the maximum\nvalue in the kernel window κ.\nTable 4 compares the performance of the PRU\nwith different sampling methods. Average pooling\nperforms the best while skipping give comparable\nPTB WT-2\nTransformationsPPL # ParamsPPL # Params\nContext Input (total/RU) (total/RU)\nR1 LT LT 74.80 8.8/2.9 89.30 22.8/2.9\nR2 GLT GLT 84.38 6.5/0.5 104.1320.46/0.5\nR3 GLT PT 82.67 6.6/0.64 99.57 20.6/0.64\nR4 LT PT 74.18 8.5/2.5 88.31 22.5/2.5\nR5 PT PT 75.80 8.1/2.1 90.56 22.1/2.1\nR6 LT PT † 75.61 8.5/2.5 89.27 22.5/2.5\nTable 3: Impact of different transformations used\nfor processing input and context vectors (LT - lin-\near transformation, PT - pyramidal transformation,\nand GLT - grouped linear transformation). Here,\n† represents that PT was used without residual\nconnection, PPL represents word-level perplexity\n(lower is better), and the number of parameters are\nin million. We used K=g=4 in our experiments.\nDataset Skip Max pool Avg. Pool Convolution\nPTB 75.12 87.6 73.86 81.56\nWT-2 89.24 107.63 88.88 93.16\nTable 4: Impact of different sub-sampling methods\non the word-level perplexity (lower is better). We\nused g=1 and K=4 in our experiments.\nperformance. Both of these methods enable the\nnetwork to learn richer word representations while\nrepresenting the input vector in different forms,\nthus delivering higher performance. Surprisingly,\na convolution-based sub-sampling method does\nnot perform as well as the averaging method. The\ntanh function used after convolution limits the\nrange of output values which are further limited\nby the LSTM gating structure, thereby impeding\nin the ﬂow of information inside the cell. Max\npooling forces the network to learn representations\nfrom high magnitude elements, thus distinguish-\ning features between elements vanishes, resulting\nin poor performance.\n5 Conclusion\nWe introduce the Pyramidal Recurrent Unit, which\nbetter model contextual information by admitting\nhigher dimensional representations with good gen-\neralizability. When applied to the task of language\nmodeling, PRUs improve perplexity across several\nsettings, including recent state-of-the-art systems.\nOur analysis shows that the PRU improves the\nﬂow of gradient and expand the word embedding\nsubspace, resulting in more conﬁdent decisions.\nHere we have shown improvements for language\nmodeling. In future, we plan to study the perfor-\nmance of PRUs on different tasks, including ma-\nchine translation and question answering. In ad-\ndition, we will study the performance of the PRU\non language modeling with more recent inference\ntechniques, such as dynamic evaluation and mix-\nture of softmax.\nAcknowledgments\nThis research was supported by NSF (IIS\n1616112, III 1703166), Allen Distinguished In-\nvestigator Award, and gifts from Allen Institute\nfor AI, Google, Amazon, and Bloomberg. We are\ngrateful to Aaron Jaech, Hannah Rashkin, Man-\ndar Joshi, Aniruddha Kembhavi, and anonymous\nreviewers for their helpful comments.\n4629\nReferences\nLeila Arras, Gr´egoire Montavon, Klaus-Robert M¨uller,\nand Wojciech Samek. 2017. Explaining recurrent\nneural network predictions in sentiment analysis. In\n8th Workshop on Computational Approaches to Sub-\njectivity, Sentiment and Social Media Analysis,.\nJames Bradbury, Stephen Merity, Caiming Xiong, and\nRichard Socher. 2017. Quasi-recurrent neural net-\nworks. In International Conference for Learning\nRepresentations (ICLR).\nPeter J Burt and Edward H Adelson. 1987. The lapla-\ncian pyramid as a compact image code. In Readings\nin Computer Vision. Elsevier.\nV´ıctor Campos, Brendan Jou, Xavier Gir ´o-i Nieto,\nJordi Torres, and Shih-Fu Chang. 2018. Skip rnn:\nLearning to skip state updates in recurrent neural\nnetworks. In International Conference for Learning\nRepresentations (ICLR).\nJunyoung Chung, Caglar Gulcehre, KyungHyun Cho,\nand Yoshua Bengio. 2014. Empirical evaluation of\ngated recurrent neural networks on sequence model-\ning. arXiv preprint arXiv:1412.3555.\nYarin Gal and Zoubin Ghahramani. 2016. A theoret-\nically grounded application of dropout in recurrent\nneural networks. In Advances in neural information\nprocessing systems (NIPS).\nJonas Gehring, Michael Auli, David Grangier, Denis\nYarats, and Yann N Dauphin. 2017. Convolutional\nsequence to sequence learning. In International\nConference on Machine Learning (ICML).\nFelix A Gers and J ¨urgen Schmidhuber. 2000. Recur-\nrent nets that time and count. In IEEE-INNS-ENNS\nInternational Joint Conference on Neural Networks\n(IJCNN).\nMuriel Gevrey, Ioannis Dimopoulos, and Sovan Lek.\n2003. Review and comparison of methods to study\nthe contribution of variables in artiﬁcial neural net-\nwork models. Ecological modelling.\nDongyoon Han, Jiwhan Kim, and Junmo Kim. 2017.\nDeep pyramidal residual networks. In IEEE Confer-\nence on Computer Vision and Pattern Recognition\n(CVPR).\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In IEEE conference on computer vision and\npattern recognition (CVPR).\nSepp Hochreiter and Jrgen Schmidhuber. 1997. Long\nshort-term memory. Neural computation.\nHakan Inan, Khashayar Khosravi, and Richard Socher.\n2017. Tying word vectors and word classiﬁers: A\nloss framework for language modeling. In Inter-\nnational Conference for Learning Representations\n(ICLR).\nYoon Kim, Yacine Jernite, David Sontag, and Alexan-\nder M Rush. 2016. Character-aware neural language\nmodels. In Association for the Advancement of Ar-\ntiﬁcial Intelligence (AAAI).\nDiederik P Kingma, Tim Salimans, and Max Welling.\n2015. Variational dropout and the local reparame-\nterization trick. In Advances in Neural Information\nProcessing Systems (NIPS).\nBen Krause, Emmanuel Kahembwe, Iain Murray, and\nSteve Renals. 2018. Dynamic evaluation of neural\nsequence models. In International Conference on\nMachine Learning (ICML).\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin-\nton. 2012. Imagenet classiﬁcation with deep con-\nvolutional neural networks. In Advances in neural\ninformation processing systems (NIPS).\nOleksii Kuchaiev and Boris Ginsburg. 2017. Factoriza-\ntion tricks for lstm networks. In International Con-\nference for Learning Representations (ICLR) Work-\nshop.\nYann LeCun, Yoshua Bengio, et al. 1995. Convolu-\ntional networks for images, speech, and time series.\nThe handbook of brain theory and neural networks.\nKenton Lee, Omer Levy, and Luke Zettlemoyer.\n2017. Recurrent additive networks. arXiv preprint\narXiv:1705.07393.\nTao Lei, Yu Zhang, and Yoav Artzi. 2018. Training\nrnns as fast as cnns. In Empirical Methods in Natu-\nral Language Processing (EMNLP).\nJiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky.\n2016. Visualizing and understanding neural models\nin nlp. In North American Chapter of the Associa-\ntion for Computational Linguistics (NAACL).\nDavid G Lowe. 1999. Object recognition from local\nscale-invariant features. In IEEE international con-\nference on Computer vision (ICCV).\nMitchell P Marcus, Mary Ann Marcinkiewicz, and\nBeatrice Santorini. 1993. Building a large annotated\ncorpus of english: The penn treebank. Computa-\ntional linguistics.\nSachin Mehta, Mohammad Rastegari, Anat Caspi,\nLinda Shapiro, and Hannaneh Hajishirzi. 2018. Esp-\nnet: Efﬁcient spatial pyramid of dilated convolutions\nfor semantic segmentation. In European Conference\nin Computer Vision (ECCV).\nG´abor Melis, Chris Dyer, and Phil Blunsom. 2018. On\nthe state of the art of evaluation in neural language\nmodels. In International Conference for Learning\nRepresentations (ICLR).\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2018. Regularizing and optimizing lstm\nlanguage models. In International Conference for\nLearning Representations (ICLR).\n4630\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture\nmodels. In International Conference for Learning\nRepresentations (ICLR).\nTom´aˇs Mikolov, Martin Karaﬁ ´at, Luk ´aˇs Burget, Jan\nˇCernock`y, and Sanjeev Khudanpur. 2010. Recur-\nrent neural network based language model. In\nEleventh Annual Conference of the International\nSpeech Communication Association.\nRamprasaath R Selvaraju, Michael Cogswell, Ab-\nhishek Das, Ramakrishna Vedantam, Devi Parikh,\nand Dhruv Batra. 2017. Grad-cam: Visual explana-\ntions from deep networks via gradient-based local-\nization. In IEEE International Conference on Com-\nputer Vision (ICCV).\nMinjoon Seo, Sewon Min, Ali Farhadi, and Hannaneh\nHajishirzi. 2017. Query-reduction networks for\nquestion answering. In International Conference on\nLearning Representations (ICLR).\nMinjoon Seo, Sewon Min, Ali Farhadi, and Hannaneh\nHajishirzi. 2018. Neural speed reading via skim-\nrnn. In International Conference on Learning Rep-\nresentations (ICLR).\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: A simple way to prevent neural networks\nfrom overﬁtting. The Journal of Machine Learning\nResearch (JMLR).\nChen Xu, Jianqiang Yao, Zhouchen Lin, Wenwu Ou,\nYuanbin Cao, Zhirong Wang, and Hongbin Zha.\n2018. Alternating multi-bit quantization for recur-\nrent neural networks. In International Conference\nfor Learning Representations (ICLR).\nZhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and\nWilliam W Cohen. 2018. Breaking the softmax bot-\ntleneck: a high-rank rnn language model. In Inter-\nnational Conference for Learning Representations\n(ICLR).\nAdams Wei Yu, Hongrae Lee, and Quoc Le. 2017.\nLearning to skim text. In Association for Compu-\ntational Linguistics (ACL).\nJulian Georg Zilly, Rupesh Kumar Srivastava,\nJan Koutn ´ık, and J ¨urgen Schmidhuber. 2016.\nRecurrent highway networks. arXiv preprint\narXiv:1607.03474.\nBarret Zoph and Quoc V Le. 2017. Neural architecture\nsearch with reinforcement learning. In International\nConference for Learning Representations (ICLR).",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.9333100318908691
    },
    {
      "name": "Generalizability theory",
      "score": 0.8903202414512634
    },
    {
      "name": "Computer science",
      "score": 0.8485534191131592
    },
    {
      "name": "Language model",
      "score": 0.7928388714790344
    },
    {
      "name": "Generalization",
      "score": 0.702489972114563
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5644326210021973
    },
    {
      "name": "Word (group theory)",
      "score": 0.5465871095657349
    },
    {
      "name": "Code (set theory)",
      "score": 0.5091592669487
    },
    {
      "name": "Natural language processing",
      "score": 0.4781138300895691
    },
    {
      "name": "Exploit",
      "score": 0.4392528235912323
    },
    {
      "name": "Machine learning",
      "score": 0.3482625484466553
    },
    {
      "name": "Programming language",
      "score": 0.14309564232826233
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    }
  ]
}