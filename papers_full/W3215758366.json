{
  "title": "Improved Multiscale Vision Transformers for Classification and Detection",
  "url": "https://openalex.org/W3215758366",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2227365593",
      "name": "Yanghao Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4314044258",
      "name": "Chao-Yuan Wu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2170556487",
      "name": "Haoqi Fan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3029598822",
      "name": "Karttikeya Mangalam",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1901544738",
      "name": "Bo Xiong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2117791194",
      "name": "Jitendra Malik",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2276051864",
      "name": "Christoph Feichtenhofer",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3216272314",
    "https://openalex.org/W2553594924",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2156303437",
    "https://openalex.org/W2949605076",
    "https://openalex.org/W3153842237",
    "https://openalex.org/W2302255633",
    "https://openalex.org/W2981385151",
    "https://openalex.org/W3175515048",
    "https://openalex.org/W2964241181",
    "https://openalex.org/W2984287396",
    "https://openalex.org/W2961193895",
    "https://openalex.org/W2097117768",
    "https://openalex.org/W2998508940",
    "https://openalex.org/W2963524571",
    "https://openalex.org/W3171087525",
    "https://openalex.org/W3206471682",
    "https://openalex.org/W2789541106",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W2935837427",
    "https://openalex.org/W2768282280",
    "https://openalex.org/W2992308087",
    "https://openalex.org/W3034572008",
    "https://openalex.org/W3035303837",
    "https://openalex.org/W3016719260",
    "https://openalex.org/W2518108298",
    "https://openalex.org/W2990503944",
    "https://openalex.org/W1677182931",
    "https://openalex.org/W2990152177",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3211432419",
    "https://openalex.org/W3202053489",
    "https://openalex.org/W2955874753",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W3176258108",
    "https://openalex.org/W2773514261",
    "https://openalex.org/W3128633047",
    "https://openalex.org/W2342662179",
    "https://openalex.org/W2963399829",
    "https://openalex.org/W3173407577",
    "https://openalex.org/W3176659256",
    "https://openalex.org/W2565639579",
    "https://openalex.org/W2963563276",
    "https://openalex.org/W3146097248",
    "https://openalex.org/W1536680647",
    "https://openalex.org/W3126337037",
    "https://openalex.org/W2549139847",
    "https://openalex.org/W2331143823",
    "https://openalex.org/W3174068320",
    "https://openalex.org/W3126721948",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W3035452548",
    "https://openalex.org/W2963037989",
    "https://openalex.org/W2962835968",
    "https://openalex.org/W2938458886",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2625366777",
    "https://openalex.org/W3147387781",
    "https://openalex.org/W3173621652",
    "https://openalex.org/W2147800946",
    "https://openalex.org/W3214395992",
    "https://openalex.org/W2963849369",
    "https://openalex.org/W3204575409",
    "https://openalex.org/W3102631365",
    "https://openalex.org/W2622263826",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W3203444029",
    "https://openalex.org/W3203606893",
    "https://openalex.org/W3171206729",
    "https://openalex.org/W3034429256",
    "https://openalex.org/W3161838454",
    "https://openalex.org/W3175859344",
    "https://openalex.org/W3114896399",
    "https://openalex.org/W2964121718",
    "https://openalex.org/W2793904650",
    "https://openalex.org/W3174738881",
    "https://openalex.org/W2154579312",
    "https://openalex.org/W2770804203",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2946948417",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2963246338",
    "https://openalex.org/W3139773203",
    "https://openalex.org/W2619947201"
  ],
  "abstract": "In this paper, we study Multiscale Vision Transformers (MViT) as a unified architecture for image and video classification, as well as object detection. We present an improved version of MViT that incorporates decomposed relative positional embeddings and residual pooling connections. We instantiate this architecture in five sizes and evaluate it for ImageNet classification, COCO detection and Kinetics video recognition where it outperforms prior work. We further compare MViTs' pooling attention to window attention mechanisms where it outperforms the latter in accuracy/compute. Without bells-and-whistles, MViT has state-of-the-art performance in 3 domains: 88.8% accuracy on ImageNet classification, 56.1 box AP on COCO object detection as well as 86.1% on Kinetics-400 video classification. Code and models will be made publicly available.",
  "full_text": "MViTv2: Improved Multiscale Vision Transformers\nfor Classification and Detection\nYanghao Li*, 1 Chao-Yuan Wu*, 1 Haoqi Fan 1\nKarttikeya Mangalam 1, 2 Bo Xiong 1 Jitendra Malik 1, 2 Christoph Feichtenhofer *, 1\n∗equal technical contribution\n1Facebook AI Research 2UC Berkeley\nAbstract\nIn this paper, we study Multiscale Vision Transformers\n(MViTv2) as a unified architecture for image and video\nclassification, as well as object detection. We present an\nimproved version of MViT that incorporates decomposed\nrelative positional embeddings and residual pooling con-\nnections. We instantiate this architecture in five sizes and\nevaluate it for ImageNet classification, COCO detection\nand Kinetics video recognition where it outperforms prior\nwork. We further compare MViTv2s’ pooling attention to\nwindow attention mechanisms where it outperforms the latter\nin accuracy/compute. Without bells-and-whistles, MViTv2\nhas state-of-the-art performance in 3 domains: 88.8% ac-\ncuracy on ImageNet classification, 58.7 AP box on COCO\nobject detection as well as 86.1% on Kinetics-400 video\nclassification. Code and models are available at https:\n//github.com/facebookresearch/mvit.\n1. Introduction\nDesigning architectures for different visual recognition\ntasks has been historically difficult and the most widely\nadopted ones have been the ones that combine simplicity\nand efficacy, e.g. VGGNet [67] and ResNet [37]. More\nrecently Vision Transformers (ViT) [17] have shown promis-\ning performance and are rivaling convolutional neural net-\nworks (CNN) and a wide range of modifications have re-\ncently been proposed to apply them to different vision\ntasks [1, 2, 21, 55, 68, 73, 78, 90].\nWhile ViT [17] is popular in image classification, its\nusage for high-resolution object detection and space-time\nvideo understanding tasks remains challenging. The den-\nsity of visual signals poses severe challenges in compute\nand memory requirements as these scale quadratically in\ncomplexity within the self-attention blocks of Transformer-\nbased [76] models. The community has approached this\nburden with different strategies: Two popular ones are (1)\nlocal attention computation within a window [55] for object\ndetection and (2) pooling attention that locally aggregates\nfeatures before computing self-attention in video tasks [21].\n(a) Image classification\n (b) Object detection\n (c) Video recognition\nFigure 1. Our MViTv2 is a multiscale transformer with state-of-\nthe-art performance across three visual recognition tasks.\nThe latter fuels Multiscale Vision Transformers\n(MViT) [21], an architecture that extends ViT in a simple\nway: instead of having a fixed resolution throughout the net-\nwork, it has a feature hierarchy with multiple stages starting\nfrom high-resolution to low-resolution. MViT is designed\nfor video tasks where it has state-of-the-art performance.\nIn this paper, we develop two simple technical improve-\nments to further increase its performance and study MViT as\na single model family for visual recognition across 3 tasks:\nimage classification, object detection and video classifica-\ntion, in order to understand if it can serve as a general vision\nbackbone for spatial as well as spatiotemporal recognition\ntasks (see Fig. 1). Our empirical study leads to an improved\narchitecture (MViTv2) and encompasses the following:\n(i) We create strong baselines that improve pooling at-\ntention along two axes: (a) shift-invariant positional embed-\ndings using decomposed location distances to inject position\ninformation in Transformer blocks; (b) a residual pooling\nconnection to compensate the effect of pooling strides in\nattention computation. Our simple-yet-effective upgrades\nlead to significantly better results.\n(ii) Using the improved structure of MViT, we employ\na standard dense prediction framework: Mask R-CNN [36]\nwith Feature Pyramid Networks (FPN) [53] and apply it to\nobject detection and instance segmentation.\nWe study if MViT can process high-resolution visual\ninput by using pooling attention to overcome the compu-\ntation and memory cost involved. Our experiments sug-\ngest that pooling attention is more effective than local win-\ndow attention mechanisms ( e.g. Swin [55]). We further\ndevelop a simple-yet-effective Hybrid window attention\nscheme that can complement pooling attention for better\naccuracy/compute tradeoff.\n(iii) We instantiate our architecture in five sizes of in-\ncreasing complexity (width, depth, resolution) and report a\npractical training recipe for large multiscale transformers.\nThe MViT variants are applied to image classification, object\ndetection and video classification, with minimal modifica-\ntion, to study its purpose as a generic vision architecture.\nExperiments reveal that our MViTv2 achieves 88.8%\naccuracy for ImageNet-1K classification, with pretrain-\ning on ImageNet-21K (and 86.3% without), as well as\n58.7 AP box on COCO object detection using only Cas-\ncade Mask R-CNN [6]. For video classification tasks,\nMViT achieves unprecedented accuracies of 86.1% on\nKinetics-400, 87.9% on Kinetics-600, 79.4% on Kinetics-\n700, and 73.3% on Something-Something-v2. Our video\ncode will be open-sourced in PyTorchVideo1,2 [19, 20].\n2. Related Work\nCNNs serve as the primary backbones for computer vision\ntasks, including image recognition [10, 15, 34, 39, 46, 48, 62,\n67, 69, 71], object detection [6, 29, 38, 53, 63, 93] and video\nrecognition [8, 22, 23, 25, 28, 42, 51, 61, 66, 75, 79, 84, 92].\nVision transformers have generated a lot of recent enthusi-\nasm since the work of ViT [17], which applies a Transformer\narchitecture on image patches and shows very competitive\nresults on image classification. Since then, different works\nhave been developed to further improve ViT, including ef-\nficient training recipes [73], multi-scale transformer struc-\ntures [21, 55, 78] and advanced self-attention mechanism\ndesign [11, 21, 55]. In this work, we build upon the Multi-\nscale Vision Transformers (MViT) and study it as ageneral\nbackbone for different vision tasks.\nVision transformers for object detection tasks [11, 55, 78,\n89] address the challenge of detection typically requiring\nhigh-resolution inputs and feature maps for accurate object\nlocalization. This significantly increases computation com-\nplexity due to the quadratic complexity of self-attention oper-\nators in transformers [76]. Recent works develop technology\nto alleviate this cost, including shifted window attention [55]\nand Longformer attention [89]. Meanwhile, pooling atten-\ntion in MViT is designed to compute self-attention efficiently\nusing a different perspective [21]. In this work, we study\n1https://github.com/facebookresearch/pytorchvideo\n2https://github.com/facebookresearch/SlowFast\nMViT for detection and more generally compare pooling\nattention to local attention mechanisms.\nVision transformers for video recognition have also re-\ncently shown strong results, but mostly [1, 3, 56, 59] rely on\npre-training with large-scale external data (e.g. ImageNet-\n21K [14]). MViTv1 [21] reports a good training-from-\nscratch recipe for Transformer-based architectures on Ki-\nnetics data [44]. In this paper, we use this recipe and im-\nprove the MViT architecture with improved pooling attention\nwhich is simple yet effective on accuracy; further, we study\nthe (large) effect of ImageNet pre-training for video tasks.\n3. Revisiting Multiscale Vision Transformers\nThe key idea of MViTv1 [21] is to construct different\nstages for both low- and high-level visual modeling instead\nof single-scale blocks in ViT [17]. MViT slowly expands\nthe channel width D, while reducing the resolution L (i.e.\nsequence length), from input to output stages of the network.\nTo perform downsampling within a transformer block,\nMViT introduces Pooling Attention. Concretely, for an input\nsequence, X ∈ RL×D, it applies linear projections WQ,\nWK, WV ∈ RD×D followed by pooling operators ( P) to\nquery, key and value tensors, respectively:\nQ = PQ (XWQ) , K= PK (XWK) , V = PV (XWV ) ,\n(1)\nwhere the length ˜L of Q ∈ R˜L×D can be reduced by PQ\nand K and V length can be reduced by PK and PV .\nSubsequently, pooled self-attention,\nZ := Attn(Q, K, V) = Softmax\n\u0010\nQK⊤/\n√\nD\n\u0011\nV, (2)\ncomputes the output sequence Z ∈ R˜L×D with flexible\nlength ˜L. Note that the downsampling factors PK and PV\nfor key and value tensors can be different from the ones\napplied to the query sequence, PQ.\nPooling attention enables resolution reduction between\ndifferent stages of MViT by pooling the query tensor Q, and\nto significantly reduce compute and memory complexity by\npooling the key, K, and value, V , tensors.\n4. Improved Multiscale Vision Transformers\nIn this section, we first introduce an empirically powerful\nupgrade to pooling attention (§4.1). Then we describe how\nto employ our generic MViT architecture for object detection\n(§4.2) and video recognition (§4.3). Finally, §4.4 shows five\nconcrete instantiations for MViTv2 in increasing complexity.\n4.1. Improved Pooling Attention\nWe start with re-examining two important implications of\nMViTv2 for potential improvement and introduce techniques\nto understand and address them.\nFigure 2. The improved Pooling Attention mechanism that incor-\nporating decomposed relative position embedding, Rp(i),p(j), and\nresidual pooling connection modules in the attention block.\nDecomposed relative position embedding. While MViT\nhas shown promises in their power to model interactions\nbetween tokens, they focus on content, rather than structure.\nThe space-time structure modeling relies solely on the “ab-\nsolute” positional embedding to offer location information.\nThis ignores the fundamental principle of shift-invariance\nin vision [47]. Namely, the way MViT models the interac-\ntion between two patches will change depending on their\nabsolute position in images even if their relative positions\nstay unchanged. To address this issue, we incorporate rela-\ntive positional embeddings [65], which only depend on the\nrelative location distance between tokens into the pooled\nself-attention computation.\nWe encode the relative position between the two input el-\nements, i and j, into positional embedding Rp(i),p(j) ∈ Rd,\nwhere p(i) and p(j) denote the spatial (or spatiotemporal)\nposition of element i and j.3 The pairwise encoding repre-\nsentation is then embedded into the self-attention module:\nAttn(Q, K, V) = Softmax\n\u0010\n(QK⊤ + E(rel))/\n√\nd\n\u0011\nV,\nwhere E(rel)\nij = Qi · Rp(i),p(j). (3)\nHowever, the number of possible embeddingsRp(i),p(j)\nscale in O(T W H), which can be expensive to compute. To\nreduce complexity, we decompose the distance computation\nbetween element i and j along the spatiotemporal axes:\nRp(i),p(j) = Rh\nh(i),h(j) + Rw\nw(i),w(j) + Rt\nt(i),t(j), (4)\nwhere Rh, Rw, Rt are the positional embeddings along the\nheight, width and temporal axes, and h(i), w(i), and t(i)\n3Note that Q and (K, V ) can reside in different scales due to potentially\ndifferent pooling. p maps the index of all of them into a shared scale.\ndenote the vertical, horizontal, and temporal position of\ntoken i, respectively. Note that Rt is optional and only\nrequired to support temporal dimension in the video case. In\ncomparison, our decomposed embeddings reduce the number\nof learned embeddings to O(T + W + H), which can have\na large effect for early-stage, high-resolution feature maps.\nResidual pooling connection. As demonstrated [21], pool-\ning attention is very effective to reduce the computation\ncomplexity and memory requirements in attention blocks.\nMViTv1 has larger strides on K and V tensors than the\nstride of the Q tensors which is only downsampled if the res-\nolution of the output sequence changes across stages. This\nmotivates us to add the residual pooling connection with the\n(pooled) Q tensor to increase information flow and facilitate\nthe training of pooling attention blocks in MViT.\nWe introduce a new residual pooling connection inside\nthe attention blocks as shown in Fig. 2. Specifically, we add\nthe pooled query tensor to the output sequence Z. So Eq. (2)\nis reformulated as:\nZ := Attn (Q, K, V) +Q. (5)\nNote that the output sequence Z has the same length as the\npooled query tensor Q.\nThe ablations in§6.2 and §5.3 shows that both the pooling\noperator (PQ) for query Q and the residual path are neces-\nsary for the proposed residual pooling connection. This\nchange still enjoys the low-complexity attention computa-\ntion with large strides in key and value pooling as adding the\npooled query sequence in Eq. (5) comes at a low cost.\n4.2. MViT for Object Detection\nIn this section, we describe how to apply the MViT back-\nbone for object detection and instance segmentation tasks.\nFPN integration. The hierarchical structure of MViT pro-\nduces multiscale feature maps in four stages, and there-\nfore naturally integrates into Feature Pyramid Networks\n(FPN) [53] for object detection tasks, as shown in Fig. 3.\nThe top-down pyramid with lateral connections in FPN con-\nstructs semantically strong feature maps for MViT at all\nscales. By using FPN with the MViT backbone, we apply it\nto different detection architectures (e.g. Mask R-CNN [36]).\nHybrid window attention. The self-attention in Transform-\ners has quadratic complexity w.r.t. the number of tokens.\nThis issue is more exacerbated for object detection as it\ntypically requires high-resolution inputs and feature maps.\nIn this paper, we study two ways to significantly reduce\nthis compute and memory complexity: First, the pooling\nattention designed in attention blocks of MViT. Second, win-\ndow attention used as a technique to reduce computation for\nobject detection in Swin [55].\nPooling attention and window attention both control the\ncomplexity of self-attention by reducing the size of query,\nFigure 3. MViT backbone used with FPN for object detection.\nThe multiscale transformer features naturally integrate with stan-\ndard feature pyramid networks (FPN).\nkey and value tensors when computing self-attention. Their\nintrinsic nature however is different: Pooling attention pools\nfeatures by downsampling them via local aggregation, but\nkeeps a global self-attention computation, while window\nattention keeps the resolution of tensors but performs self-\nattention locally by dividing the input (patchified tokens)\ninto non-overlapping windows and then only compute local\nself-attention within each window. The intrinsic difference\nof the two approaches motivates us to study if they could\nperform complementary in object detection tasks.\nDefault window attention only performs local self-\nattention within windows, thus lacking connections across\nwindows. Different from Swin [55], which uses shifted win-\ndows to mitigate this issue, we propose a simple Hybrid\nwindow attention (Hwin) design to add cross-window con-\nnections. Hwin computes local attention within a window\nin all but the last blocks of the last three stages that feed\ninto FPN. In this way, the input feature maps to FPN contain\nglobal information. The ablation in §5.3 shows that this\nsimple Hwin performs consistently better than Swin [55]\non image classification and object detection tasks. Further,\nwe will show that combining pooling attention and Hwin\nachieves the best performance for object detection.\nPositional embeddings in detection. Different from Ima-\ngeNet classification where the input is a crop of fixed resolu-\ntion (e.g. 224×224), object detection typically encompasses\ninputs of varying size in training. For the positional embed-\ndings in MViT (either absolute or relative), we first initialize\nthe parameters from the ImageNet pre-training weights cor-\nresponding to positional embeddings with 224 ×224 input\nsize and then interpolate them to the respective sizes for\nobject detection training.\n4.3. MViT for Video Recognition\nMViT can be easily adopted for video recognition tasks\n(e.g. the Kinetics dataset) similar to MViTv1 [21] as the\nupgraded modules in §4.1 generalize to the spatiotemporal\ndomain. While MViTv1 only focuses on the training-from-\nscratch setting on Kinetics, in this work, we also study the\n(large) effect of pre-training from ImageNet datasets.\nInitialization from pre-trained MViT. Compared to the\nModel #Channels #Blocks #Heads FLOPs Param\nMViT-T [96-192-384-768] [1-2-5-2] [1-2-4-8] 4.7 24\nMViT-S [96-192-384-768] [1-2-11-2] [1-2-4-8] 7.0 35\nMViT-B [96-192-384-768] [2-3-16-3] [1-2-4-8] 10.2 52\nMViT-L [144-288-576-1152] [2-6-36-4] [2-4-8-16] 39.6 218\nMViT-H [192-384-768-1536] [4-8-60-8] [3-6-12-24] 120.6 667\nTable 1. Configuration for MViT variants. #Channels, #Blocks\nand #Heads specify the channel width, number of MViT blocks\nand heads in each block for the four stages, respectively. FLOPs\nare measured for image classification with 224 × 224 input. The\nstage resolutions are [562, 282, 142, 72].\nimage-based MViT, there are only three differences for video-\nbased MViT: 1) the projection layer in thepatchification stem\nneeds to project the input into space-time cubes instead of 2D\npatches; 2) the pooling operators now pool spatiotemporal\nfeature maps; 3) relative positional embeddings reference\nspace-time locations.\nAs the projection layer and pooling operators in 1) and 2)\nare instantiated by convolutional layers by default 4, we use\nan inflation initialization as for CNNs [8, 24]. Specifically,\nwe initialize the conv filters for the center frame with the\nweights from the 2D conv layers in pre-trained models and\ninitialize other weights as zero. For 3), we capitalize on our\ndecomposed relative positional embeddings in Eq. 4, and\nsimply initialize the spatial embeddings from pre-trained\nweights and the temporal embedding as zero.\n4.4. MViT Architecture Variants\nWe build several MViT variants with different number of\nparameters and FLOPs as shown in Table 1, in order to have\na fair comparison with other vision transformer works [9, 55,\n72, 81]. Specifically, we design five variants (Tiny, Small,\nBase, Large and Huge) for MViT by changing the base\nchannel dimension, the number of blocks in each stage and\nthe number of heads in the blocks. Note that we use a smaller\nnumber of heads to improve runtime, as more heads lead to\nslower runtime but have no effect on FLOPs and Parameters.\nFollowing the pooling attention design in MViT [21], we\nemploy Key and Value pooling in all pooling attention blocks\nby default and the pooling stride is set to 4 in the first stage\nand adaptively decays stride w.r.t resolution across stages.\n5. Experiments: Image Recognition\nWe conduct experiments on ImageNet classification [14]\nand COCO object detection [54]. We first show state-of-the-\nart comparisons and then perform comprehensive ablations.\nMore results and discussions are in §A.\n5.1. Image Classification on ImageNet-1K\nSettings. The ImageNet-1K [14] (IN-1K) dataset has\n∼1.28M images in 1000 classes. Our training recipe for\nMViTv2 on IN-1K is following MViTv1 [21, 72]. We train\nall MViTv2 variants for 300 epochs without using EMA. We\n4Note that no initialization is needed if using max-pooling variants.\nAcc\nmodel center resize FLOPs (G) Param (M)\nRegNetZ-4GF [15] 83.1 4.0 28\nEfficientNet-B4 ↑ 3802 [71] 82.9 4.2 19\nDeiT-S [72] 79.8 4.6 22\nTNT-S [33] 81.5 5.2 24\nPVTv2-V2 [77] 82.0 4.0 25\nCoAtNet-0 [13] 81.6 4.2 25\nXCiT-S12 [18] 82.0 4.8 26\nSwin-T [55] 81.3 4.5 29\nCSWin-T [16] 82.7 4.3 23\nMViTv2-T 82.3 4.7 24\nRegNetY-8GF [62] 81.7 8.0 39\nEfficientNet-B5 ↑ 4562 [71] 83.6 9.9 30\nTwins-B [11] 83.2 8.6 56\nPVTv2-V2-B3 [77] 83.2 6.9 45\nSwin-S [55] 83.0 8.7 50\nCSWin-S [16] 83.6 6.9 35\nMViT-v1-B-16 [21] 83.0 7.8 37\nMViTv2-S 83.6 7.0 35\nRegNetZ-16GF [15] 84.1 15.9 95\nEfficientNet-B6 ↑ 5282 [71] 84.2 19 43\nDeiT-B [72] 81.8 17.6 87\nPVTv2-V2-B5 [77] 83.8 11.8 82\nCaiT-S36 [74] 83.3 13.9 68\nCoAtNet-2 [13] 84.1 15.7 75\nXCiT-M24 [18] 82.7 16.2 84\nSwin-B [55] 83.3 15.4 88\nCSWin-B [16] 84.2 15.0 78\nMViTv1-B-24 [21] 83.4 10.9 54\nMViTv2-B 84.4 10.2 52\nEfficientNet-B7 ↑ 6002 [71] 84.3 37.0 66\nNFNet-F1 ↑ 3202 [5] 84.7 35.5 133\nDeiT-B ↑ 3842 [72] 83.1 55.5 87\nCvT-32 ↑ 3842 [81] 83.3 24.9 32\nCaiT-S36↑ 3842 [74] 85.0 48 68\nSwin-B ↑ 3842 [55] 84.2 47.0 88\nMViT-v1-B-24↑ 3202 [21] 84.8 32.7 73\nMViTv2-B ↑ 3842 85.2 85.6 36.7 52\nNFNet-F2 ↑ 3522 [5] 85.1 62.6 194\nCoAtNet-3 [13] 84.5 34.7 168\nXCiT-M24 [18] 82.9 36.1 189\nMViTv2-L 85.3 42.1 218\nNFNet-F4 ↑ 5122 [5] 85.9 215.3 316\nCoAtNet-3 [13] ↑ 3842 85.8 107.4 168\nMViTv2-L ↑ 3842 86.0 86.3 140.2 218\nTable 2. Comparison to published work on ImageNet-1K. Input\nimages are 224×224 by default and ↑ denotes using different sizes.\nMViT is trained for 300 epochs without any external data or models.\nWe report ↑ 3842 MViT tested with center crop or a resized view\nof the original image, to compare to prior work. Full Table in A.3\nalso explore pre-training on ImageNet-21K (IN-21K) with\n∼14.2M images and ∼21K classes. See §B for details.\nResults using ImageNet-1K. Table 2 shows our MViTv2\nand state-of-the-art CNNs and Transformers (without exter-\nnal data or distillation models [43, 74, 86]). The models are\nsplit into groups based on computation and compared next.\nCompared to MViTv1 [21], our improved MViTv2 has\nbetter accuracy with fewer flops and parameters. For ex-\nample, MViTv2-S (83.6%) improves +0.6% over MViTv1-\nB-16 (83.0%) with 10% fewer flops. On the base model\nsize, MViTv2-B (84.4%) improves +1.0% over MViTv1-\nB-24 (83.4%) while even being lighter. This shows clear\neffectiveness of the MViTv2 improvements in §4.1.\nAcc\nmodel center resize FLOPs (G) Param (M)\nSwin-L [55] 86.3 34.5 197\nMViTv2-L 87.5 42.1 218\nMViTv2-H 88.0 120.6 667\nViT-L/16↑ 3842 [17] 85.2 190.7 307\nViL-B-RPB ↑ 3842 [89] 86.2 43.7 56\nSwin-L ↑ 3842 [55] 87.3 103.9 197\nCSwin-L ↑ 3842 [16] 87.5 96.8 173\nCvT-W24 ↑ 3842 [81] 87.6 193.2 277\nCoAtNet-4 [13] ↑ 5122 88.4 360.9 275\nMViTv2-L ↑ 3842 88.2 88.4 140.7 218\nMViTv2-H ↑ 3842 88.3 88.6 388.5 667\nMViTv2-H ↑ 5122 88.3 88.8 763.5 667\nTable 3. ImageNet-1K fine-tunning results using IN-21K data.\nFine-tuning is with 2242 input size (default) or with ↑ 3842 size.\nCenter denotes testing with a center crop, while resize is scaling\nthe full image to the inference resolution (including more context).\nOur MViTv2 outperforms other Transformers, including\nDeiT [72] and Swin [55], especially when scaling up models.\nFor example, MViTv2-B achieves 84.4% top-1 accuracy, sur-\npassing DeiT-B and Swin-B by 2.6% and 1.1% respectively.\nNote that MViTv2-B has over 33% fewer flops and parame-\nters comparing DeiT-B and Swin-B. The trend is similar with\n384×384 input and MViTv2-B has further +0.8% gain from\nthe high-resolution fine-tuning under center crop testing.\nIn addition to center crop testing (with a 224/256=0.875\ncrop ratio), we report a testing protocol that has been adopted\nrecently in the community [55, 74, 81]: This protocol takes\na full-sized crop of the (resized) original validation images.\nWe observe that full crop testing can increase our MViTv2-L\n↑ 3842 from 86.0 to 86.3%, which is the highest accuracy on\nIN-1K to date (without external data or distillation models).\nResults using ImageNet-21K. Results for using the large-\nscale IN-21K pre-training are shown in Table 3. The IN-21K\ndata adds +2.2% accuracy to MViTv2-L.\nCompared to other Transformers, MViTv2-L achieves bet-\nter results than Swin-L (+1.2%). We lastly finetune MViTv2-\nL with 3842 input to directly compare to prior models of size\nL: MViTv2-L achieves 88.4%, outperforming other large\nmodels. We further train a huge MViTv2-H with accuracy\n88.0%, 88.6% and 88.8% at 2242, 3842 and 5122 resolution.\n5.2. Object Detection on COCO\nSettings. We conduct object detection experiments on the\nMS-COCO dataset [54]. All the models are trained on 118K\ntraining images and evaluated on the 5K validation images.\nWe use standard Mask R-CNN [36] and Cascade Mask\nR-CNN [6] detection frameworks implemented in Detec-\ntron2 [82]. For a fair comparison, we follow the same recipe\nas in Swin [55]. Specifically, we pre-train the backbones on\nIN and fine-tune on COCO using a 3×schedule (36 epochs)\nby default. Detailed training recipes are in §B.3.\nFor MViTv2, we take the backbone pre-trained from IN\nand add our Hybrid window attention (Hwin) by default. The\nwindow sizes are set as [56, 28, 14, 7] for the four stages,\nwhich is consistent with the self-attention size used in IN\npre-training which takes 224×224 as input.\n(a) Mask R-CNN\nmodel APbox APbox\n50 APbox\n75 APmask APmask\n50 APmask\n75 FLOPs Param\nRes50 [38] 41.0 61.7 44.9 37.1 58.4 40.1 260 44\nPVT-S [78] 43.0 65.3 46.9 39.9 62.5 42.8 245 44\nSwin-T [55] 46.0 68.2 50.2 41.6 65.1 44.8 264 48\nViL-S-RPB [89] 47.1 68.7 51.5 42.7 65.9 46.2 277 45\nMViTv1-T [21] 45.9 68.7 50.5 42.1 66.0 45.4 326 46\nMViTv2-T 48.2 70.9 53.3 43.8 67.9 47.2 279 44\nRes101 [38] 42.8 63.2 47.1 38.5 60.1 41.3 336 63\nPVT-M [78] 44.2 66.0 48.2 40.5 63.1 43.5 302 64\nSwin-S [55] 48.5 70.2 53.5 43.3 67.3 46.6 354 69\nViL-M-RPB [89] 48.9 70.3 54.0 44.2 67.9 47.7 352 60\nMViTv1-S [21] 47.6 70.0 52.2 43.4 67.3 46.9 373 57\nMViTv2-S 49.9 72.0 55.0 45.1 69.5 48.5 326 54\nX101-64 [83] 44.4 64.9 48.8 39.7 61.9 42.6 493 101\nPVT-L [78] 44.5 66.0 48.3 40.7 63.4 43.7 364 81\nSwin-B [55] 48.5 69.8 53.2 43.4 66.8 46.9 496 107\nViL-B-RPB [89] 49.6 70.7 54.6 44.5 68.3 48.0 384 76\nMViTv1-B [21] 48.8 71.2 53.5 44.2 68.4 47.6 438 73\nMViTv2-B 51.0 72.7 56.3 45.7 69.9 49.6 392 71\nMViTv2-L 51.8 72.8 56.8 46.2 70.4 50.0 1097 238\nMViTv2-L† 52.7 73.7 57.6 46.8 71.4 50.8 1097 238\n(b) Cascade Mask R-CNN\nmodel APbox APbox\n50 APbox\n75 APmask APmask\n50 APmask\n75 FLOPs Param\nR50 [38] 46.3 64.3 50.5 40.1 61.7 43.4 739 82\nSwin-T [55] 50.5 69.3 54.9 43.7 66.6 47.1 745 86\nMViTv2-T 52.2 71.1 56.6 45.0 68.3 48.9 701 76\nX101-32 [83] 48.1 66.5 52.4 41.6 63.9 45.2 819 101\nSwin-S [55] 51.8 70.4 56.3 44.7 67.9 48.5 838 107\nMViTv2-S 53.2 72.4 58.0 46.0 69.6 50.1 748 87\nX101-64 [83] 48.3 66.4 52.3 41.7 64.0 45.1 972 140\nSwin-B [55] 51.9 70.9 56.5 45.0 68.4 48.7 982 145\nMViTv2-B 54.1 72.9 58.5 46.8 70.6 50.8 814 103\nMViTv2-B† 54.9 73.8 59.8 47.4 71.5 51.6 814 103\nMViTv2-L 54.3 73.1 59.1 47.1 70.8 51.7 1519 270\nMViTv2-L†† 55.8 74.3 60.9 48.3 71.9 53.2 1519 270\nMViTv2-H†† 56.1 74.6 61.0 48.5 72.4 53.2 3084 718\nMViTv2-L††∗ 58.7 76.7 64.3 50.5 74.2 55.9 - 270\nTable 5. Results on COCO object detection with (a) Mask R-\nCNN [36] and (b) Cascade Mask R-CNN [6]. † indicates that the\nmodel is initialized from IN-21K pre-training. †† denotes using a\nstronger large-scale jittering training [26] and longer schedule (50\nepochs) with IN-21K pre-training ∗ indicates using SoftNMS and\nmultiscale testing. FLOPs / Params are in Giga (109) / Mega (106).\nMain results. Table 5a shows the results on COCO\nusing Mask R-CNN. Our MViTv2 surpasses CNN ( i.e.\nResNet [38] and ResNeXt [83]) and Transformer back-\nbones ( e.g. Swin [55], ViL [89] and MViTv1 [21] 5).\nE.g., MViTv2-B outperforms Swin-B by +2.5/+2.3 in\nAPbox/APmask, with lower compute and smaller model size.\nWhen scaling up, our deeper MViTv2-L improves over\nMViTv2-B by +0.8 AP box and using IN-21K pre-training\nfurther adds +0.9 to achieve 52.7 APbox with Mask R-CNN\nand a standard 3×schedule.\nIn Table 5b we observe a similar trend among backbones\nfor Cascade Mask R-CNN [6] which lifts Mask R-CNN\naccuracy (5a). We also ablate the use of a longer training\nschedule with large-scale jitter that boosts our APbox to 55.8.\nMViTv2-H increases this to 56.1 APbox and 48.5 APmask.\nWe further adopt two inference strategies (SoftNMS [4]\nand multi-scale testing) on MViTv2-L with Cascade Mask\nR-CNN for system-level comparison (See Table§A.1). They\nboosts our APbox to 58.7, which is already better than the\nbest results from Swin (58.0 APbox), even MViTv2 does not\nuse the improved HTC++ detector [55] yet.\n5.3. Ablations on ImageNet and COCO\nDifferent self-attention mechanism. We first study our\npooling attention and Hwin self-attention mechanism in\nMViTv2 by comparing with different self-attention mecha-\nnisms on ImageNet and COCO. For a fair comparison, we\nconduct the analysis on both ViT-B and MViTv2-S networks.\nIn Table 4a we compare different attention schemes on IN-\n1K. We compare 5 attention mechanisms: global (full), win-\ndowed, Shifted window (Swin), our Hybrid window (Hwin)\nand pooling. We observe the following:\n(i) For ViT-B based models, default win reduces both\nFLOPs and Memory usage while the top-1 accuracy also\ndrops by 2.0% due to the missing cross-window connection.\nSwin [55] attention can recover 0.4% overdefault win. While\n5We adapt MViTv1 [21] as a detection baseline combined with Hwin.\n(a) ImageNet-1K classification\nvariant attention Acc FLOPs (G) Mem (G)\nViT-B\nfull 82.0 17.5 12.4\nfixed win 80.0 17.0 9.7\nSwin [55] 80.4 17.0 9.7\nHwin 82.1 17.1 10.4\npooling 81.9 10.9 8.3\nMViTv2-S\npooling 83.6 7.0 6.8\npooling (stride=8) 83.2 6.3 5.5\npooling + Swin [55] 82.8 6.4 6.0\npooling + Hwin 83.0 6.5 6.2\n(b) Mask R-CNN on COCO detection\nvariant attention APbox Train(iter/s) Test(im/s) Mem(G)\nViT-B\nfull 46.6 2.3 4.6 24.7\nfixed win 43.4 3.3 7.8 5.6\nSwin [55] 45.1 3.1 7.5 5.7\nHwin 46.1 3.1 6.8 11.0\npooling 47.2 2.9 7.9 8.8\npooling + Hwin 46.9 3.1 8.8 5.5\nMViTv2-S\npooling 50.8 1.5 4.2 19.5\npooling (stride=8) 50.0 2.5 8.3 7.8\npooling + Swin [55] 48.9 2.6 9.2 4.9\npooling + Hwin 49.9 2.7 9.4 5.2\nTable 4. Comparison of attention mechanisms on ImageNet and COCO using ViT-B and MViTv2-S backbones. fixed win: non-\noverlapping window-attention in all Transformer blocks. Swin: shifted window attention [55]. Hwin: our Hybrid window attention. Pooling:\nour pooling attention, the K, V pooling stride is 2 (ViT-B) and 4 on the first stage of MViTv2, orpooling (stride=8). Accuracy, FLOPs and\npeak training memory are measured on IN-1K. For COCO, we report APbox, average training iterations per-second, average testing frames\nper-second and peak training memory, which are measured in Detectron2 [82] with 8 V100 GPUs under the same settings. Default is in gray.\nour Hybrid window (Hwin) attention fully recovers the per-\nformance and outperforms Swin attention by +1.7%. Finally,\npooling attention achieves the best accuracy/computation\ntrade-off by getting similar accuracy for ViT-B with signifi-\ncant compute reduction (∼38% fewer FLOPs).\n(ii) For MViTv2-S, pooling attention is used by default.\nWe study if adding local window attention can improve\nMViT. We observe that adding Swin or Hwin both can re-\nduce the model complexity with slight performance decay.\nHowever, directly increasing the pooling stride (from 4 to 8)\nachieves the best accuracy/compute tradeoff.\nTable 4b shows the comparison of attention mechanisms\non COCO: (i) For ViT-B based models, pooling and pool-\ning + Hwin achieves even better results (+0.6/0.3 AP box)\nthan standard full attention with ∼2× test speedup. (ii) For\nMViTv2-S, directly increasing the pooling stride (from 4 to\n8) achieves better accuracy/computation tradeoff than adding\nSwin. This result suggests that simple pooling attention can\nbe a strong baseline for object detection. Finally, combining\nour pooling and Hwin achieves the best tradeoff.\npositional embeddings IN-1K COCO\nAcc APbox Train(iter/s) Test(im/s) Mem(G)\n(1) no pos. 83.3 49.2 3.1 10.3 5.0\n(2) abs. pos. 83.5 49.3 3.1 10.1 5.0\n(3) joint rel. pos. 83.6 49.9 0.7 ↓4.4× 3.4 ↓3× 15.3\n(4) decomposed rel. pos. 83.6 49.9 2.7 9.4 5.2\n(5) abs. + dec. rel. pos. 83.7 49.8 2.7 9.5 5.2\nTable 6. Ablation of positional embeddings on MViTv2-S.\nPositional embeddings. Table 6 compares different posi-\ntional embeddings. We observe that: ( i) Comparing (2) to\n(1), absolute position only slightly improves over no pos..\nThis is because the pooling operators (instantiated by conv\nlayers) already model positional information. (ii) Comparing\n(3, 4) and (1, 2), relative positions can bring performance\ngain by introducing shift-invariance priors to pooling atten-\ntion. Finally, our decomposed relative position embedding\ntrain 3.9× faster than joint relative position on COCO.\nresidual pooling IN-1K COCO\nAcc APbox Train(iter/s) Test(im/s) Mem(G)\n(1) w/o 83.3 48.5 3.0 10.0 4.7\n(2) residual 83.6 49.3 2.9 9.8 4.7\n(3) full Q pooling + residual 83.6 49.9 2.7 9.4 5.2\n(4) full Q pooling 83.1 48.5 2.8 9.5 5.1\nTable 7. Ablation of residual pooling connectionson MViTv2-S.\nResidual pooling connection. Table 7 studies the impor-\ntance of our residual pooling connection. We see that simply\nadding the residual path ( 2) can improves results on both\nIN-1K (+0.3%) and COCO (+0.8 for APbox) with negligible\ncost. (3) Using residual pooling and also adding Q pooling\nto all other layers (with stride=1) leads to a significant boost,\nespecially on COCO (+1.4 AP box). This suggests both Q\npooling blocks and residual paths are necessary in MViTv2.\n(4) just adding (without residual) more Q pooling layers with\nstride=1 does not help and even decays (4) vs. (1).\nRuntime comparison. We conduct a runtime compari-\nmodel IN-1K COCO\nAcc Test (im/s) APbox Train(iter/s) Test(im/s) Mem(G)\nSwin-B [55] 83.3 276 48.5 2.5 9.4 6.3\nMViTv2-S 83.6 341 49.9 2.7 9.4 5.2\nMViTv2-B 84.4 253 51.0 2.1 7.2 6.9\nTable 8. Runtime comparison on IN-1K and COCO. We report\naccuracy and throughput on IN-1K, measured with a V100 GPU as\nin [55]. COCO models are measured similarly and also for training\nthroughput and memory. Batch size for all measures is identical.\nson for MViTv2 and Swin [55] in Table 8. We see that\nMViTv2-S surpasses Swin-B on both IN-1K (+0.3%) and\nCOCO (+1.4%) while having a higher throughput (341 im/s\nvs. 276 im/s) on IN-1K and also trains faster (2.7iter/s vs.\n2.5iter/s) on COCO with less memory cost (5.2G vs. 6.3G).\nMViTv2-B is slightly slower but significantly more accurate\n(+1.1% on IN-1K and +2.5APbox on COCO).\nSingle-scale vs. multi-scale for detection. Table 9 com-\npares the default multi-scale (FPN) detector with the single-\nscale detector for ViT-B and MViTv2-S. As ViT produces\nfeature maps at a single scale in the backbone, we adopt a\nsimple scheme [50] to up-/downsample features to integrate\nwith FPN. For single-scale, we directly apply the detection\nheads to the last Transformers block.\nvariant FPN APbox APmask FLOPs (G)\nViT-B no 45.1 40.6 725\nViT-B yes 46.6 42.3 879\nMViTv2-S no 47.0 41.4 276\nMViTv2-S yes 49.9 45.1 326\nTable 9. Single-scale vs. Multi-scale (FPN) on COCO. ViT-B\nand MViTv2-S models are equipped with or w/o a feature pyramid\nnetwork (FPN). Both FPN models outperforms their single-scale\nvariant while while MViTv2 achieves even larger gains.\nAs shown in Table 9, FPN significantly improves perfor-\nmance for both backbones while MViTv2-S is consistently\nbetter than ViT-B. Note that the FPN gain for MViTv2-S\n(+2.9 AP box) is much larger than those for ViT-B (+1.5\nAPbox), which shows the effectiveness of a native hierar-\nchical multi-scale design for dense object detection tasks.\n6. Experiments: Video Recognition\nWe apply our MViTv2 on Kinetics-400 [44] (K400),\nKinetics-600 (K600) [8], and Kinetics-700 (K700) [7] and\nSomething-Something-v2 [31] (SSv2) datasets.\nSettings. By default, our MViTv2 models are trained from\nscratch on Kinetics and fine-tuned from Kinetics models for\nSSv2. The training recipe and augmentations follow [19,21].\nWhen using IN-1K or IN-21K as pre-training, we adopt the\ninitialization scheme introduced in §4.3 and shorter training.\nFor the temporal domain, we sample aT ×τ clip from the\nfull-length video which contains T frames with a temporal\nstride of τ. For inference, we follow testing strategies in [21,\n23] and get final score by averaged from sampled temporal\nclips and spatial crops. Implementation and training details\nare in §B.\nmodel pre-train top-1 top-5 FLOPs ×views Param\nSlowFast 16×8 +NL [23] - 79.8 93.9 234×3×10 59.9\nX3D-XL [22] - 79.1 93.9 48.4×3×10 11.0\nMoViNet-A6 [45] - 81.5 95.3 386×1×1 31.4\nMViTv1, 16 ×4 [21] - 78.4 93.5 70.3×1×5 36.6\nMViTv1, 32 ×3 [21] - 80.2 94.4 170×1×5 36.6\nMViTv2 -S, 16×4 - 81.0 94.6 64×1×5 34.5\nMViTv2 -B, 32×3 - 82.9 95.7 225×1×5 51.2\nViT-B-VTN [59]\nIN-21K\n78.6 93.7 4218×1×1 114.0\nViT-B-TimeSformer [3] 80.7 94.7 2380×3×1 121.4\nViT-L-ViViT [1] 81.3 94.7 3992×3×4 310.8\nSwin-L ↑ 3842 [56] 84.9 96.7 2107×5×10 200.0\nMViTv2 -L↑ 3122, 40×3 86.1 97.0 2828×3×5 217.6\nTable 10. Comparison with previous work on Kinetics-400. We\nreport the inference cost with a single “view” (temporal clip with\nspatial crop) × the number of views (FLOPs×viewspace×viewtime).\nMagnitudes are Giga (109) for FLOPs and Mega (106) for Param.\nmodel pretrain top-1 top-5 FLOPs×views Param\nSlowFast 16×8 +NL [23] - 81.8 95.1 234×3×10 59.9\nX3D-XL [22] - 81.9 95.5 48.4×3×10 11.0\nMoViNet-A6 [45] - 84.8 96.5 386×1×1 31.4\nMViTv1-B-24, 32×3 [21] - 84.1 96.5 236×1×5 52.9\nMViTv2-B, 32×3 - 85.5 97.2 206×1×5 51.4\nViT-L-ViViT [1]\nIN-21K\n83.0 95.7 3992×3×4 310.8\nSwin-B [56] 84.0 96.5 282×3×4 88.1\nSwin-L↑ 3842 [56] 86.1 97.3 2107×5×10 200.0\nMViTv2-L↑ 3122, 32×3 87.2 97.6 2063×3×4 217.6\nMViTv2-L↑ 3122, 40×3 87.5 97.8 2828×3×4 217.6\nMViTv2-L↑ 3522, 40×3 87.9 97.9 3790×3×4 217.6\nTable 11. Comparison with previous work on Kinetics-600 .\nmodel pretrain top-1 top-5 FLOPs×views Param\nSlowFast 16×8 +NL [23] K600 71.0 89.6 234×3×10 59.9\nMoViNet-A6 [45] N/A 72.3 N/A 386×1×1 31.4\nMViTv2-B, 32×3 - 76.6 93.2 206×3×3 51.4\nMViTv2-L↑ 3122, 40×3 IN-21K 79.4 94.9 2828×3×3 217.6\nTable 12. Comparison with previous work on Kinetics-700 .\n6.1. Main Results\nKinetics-400. Table 10 compares MViTv2 to prior work,\nincluding state-of-the-art CNNs and ViTs.\nWhen training from scratch, our MViTv2-S & B models\nproduce 81.0% & 82.9% top-1 accuracy which is +2.6% &\n+2.7% higher than their MViTv1 [21] counterparts. These\ngains stem solely from the improvements in §4.1, as the\ntraining recipe is identical.\nPrior ViT-based models require large-scale pre-training\non IN-21K to produce best accuracy on K400. We fine-tune\nour MViTv2-L with large spatiotemporal input size 40×3122\n(time ×space2) to reach 86.1% top-1 accuracy, showing the\nperformance of our architecture in a large-scale setting.\nKinetics-600/-700. Table 11 shows the results on K600. We\ntrain MViTv2-B, 32×3 from scratch and achieves 85.5%\ntop-1 accuracy, which is better than the MViTv1 counter-\npart (+1.4%), and even better than other ViTs with IN-21K\npre-training(e.g. +1.5% over Swin-B [56]) while having\n∼2.2×and ∼40% fewer FLOPs and parameters. The larger\nMViTv2-L 40×3 sets a new state-of-the-art at 87.9%.\nmodel pretrain top-1 top-5 FLOPs×views Param\nTEA [49] IN-1K 65.1 89.9 70×3×10 -\nMoViNet-A3 [45] N/A 64.1 88.8 24×1×1 5.3\nViT-B-TimeSformer [3] IN-21K 62.5 - 1703×3×1 121.4\nMViTv1-B-24, 32×3 K600 68.7 91.5 236.0×3×1 53.2\nSlowFast R101, 8×8 [23]\nK400\n63.1 87.6 106×3×1 53.3\nMViTv1-B, 16×4 64.7 89.2 70.5×3×1 36.6\nMViTv1-B, 64×3 67.7 90.9 454×3×1 36.6\nMViTv2-S, 16×4 68.2 91.4 64.5×3×1 34.4\nMViTv2-B, 32×3 70.5 92.7 225×3×1 51.1\nSwin-B [56] IN21K + K400 69.6 92.7 321×3×1 88.8\nMViTv2-B, 32×3 IN21K + K400 72.1 93.4 225×3×1 51.1\nMViTv2-L↑ 3122, 40×3 IN21K + K400 73.3 94.1 2828×3×1 213.1\nTable 13. Comparison with previous work on SSv2.\nIn Table 12, our MViTv2-L achieves 79.4% on K700\nwhich greatly surpasses the previous best result by +7.1%.\nSomething-something-v2. Table 13 compares methods on\na more ‘temporal modeling’ dataset SSv2. Our MViTv2-\nS with 16 frames first improves over MViTv1 counterpart\nby a large gain (+3.5%), which verifies the effectiveness of\nour proposed pooling attention for temporal modeling. The\ndeeper MViTv2-B achieves 70.5% top-1 accuracy, surpass-\ning the previous best result Swin-B with IN-21K and K400\npre-training by +0.9% while using ∼30% and 40% fewer\nFLOPs and parameters and only K400. With IN-21K pre-\ntraining, MViTv2-B boosts accuracy by 1.6% and achieves\n72.1%. MViTv2-L achieves 73.3% top-1 accuracy.\n6.2. Ablations on Kinetics\nIn this section, we carry out MViTv2 ablations on K400.\nThe video ablation our technical improvements share trends\nwith Table 6 & 7 and are in §A.5.\nmodel T×τ scratch IN1k IN21k FLOPs Param\nMViTv2-S 16×4 81.2 82.2 82.6 64 34.5\nMViTv2-B 32×3 82.9 83.3 84.3 225 51.2\nMViTv2-L 40×3 81.4 83.4 84.5 1127 217.6\nMViTv2-L↑ 3122 40×3 81.8 84.4 85.7 2828 217.6\nTable 14. Effect of pre-training on K400 . We use\nviewspace×viewtime = 1×10 crops for inference.\nEffect of pre-training datasets. Table 14 compares the ef-\nfect different pre-training schemes on K400. We observe\nthat: (i) For MViTv2-S and MViTv2-B models, using either\nIN1K or IN21k pre-training boosts accuracy compared to\ntraining from scratch, e.g.MViTv2-S gets +1.0% and 1.4%\ngains with IN1K and IN21K pre-training. (ii) For large mod-\nels, ImageNet pre-training is necessary as they are heavily\noverfitting when trained from scratch (cf . Table 10).\n7. Conclusion\nWe present an improved Multiscale Vision Transformer\nas a general hierarchical architecture for visual recognition.\nIn empirical evaluation, MViT shows strong performance\ncompared to other vision transformers and achieves state-of-\nthe-art accuracy on widely-used benchmarks across image\nclassification, object detection, instance segmentation and\nvideo recognition. We hope that our architecture will be\nuseful for further research in visual recognition.\nAppendix\nThis appendix provides further details for the main paper:\n§A contains further results for COCO object detection\n(§A.1) A V A action detection (§A.2) and ImageNet classifica-\ntion (§A.3), as well as ablations for ImageNet classification\nand COCO object detection (§A.4) and Kinetics action clas-\nsification (§A.5).\n§B contains additional MViTv2 upgrade details (§B.1),\nand additional implementation details for: ImageNet clas-\nsification (§B.2), COCO object detection ( §B.3), Kinetics\naction classification (§B.4), SSv2 action classification (§B.5),\nand A V A action detection (§B.6).\nA. Additional Results\nA.1. Results: COCO Object Detection\nSystem-level comparsion on COCO. Table A.1 shows the\nsystem-level comparisons on COCO data. We compare our\nresults with previous state-of-the-art models. We adopt Soft-\nNMS [4] during inference, following [55]. MViTv2-L ∗\nachieves 58.7 APbox with multi-scale testing, which is al-\nready +0.7 AP better than the best results of Swin-L ∗ that\nrelies on the improved HTC++ detector [55].\nmodel framework APbox APmask Flops Param\nCopy-Paste [26] Cascade, NAS-FPN 55.9 47.2 1440 185\nSwin-L [55] HTC++ 57.1 49.5 1470 284\nSwin-L [55]∗ HTC++ 58.0 50.4 - 284\nMViTv2-L Cascade 56.9 48.6 1519 270\nMViTv2-L∗ Cascade 58.7 50.5 - 270\nTable A.1. System-level comparison on COCO object detection\nand segmentation. The detection frameworks include Cascade\nMask R-CNN [6] (Cascade), the improved Hybrid Task Cascade\n(HTC++) [55] and Cascade Mask R-CNN with NAS-FPN [27].\n∗ indicates multi-scale testing. FLOPs and Params are in Giga\n(109) and Mega (106).\nA.2. Results: A V A Action Detection\nResults on A V A.Table A.2 shows the results of our MViTv2\nmodels compared with prior state-of-the-art works on the\nA V A dataset [32] which is a dataset for spatiotemporal-\nlocalization of human actions.\nWe observe that MViT consistently achieves better re-\nsults compared to MViTv1 [21] counterparts. For example,\nMViTv2-S 16×4 (26.8 mAP) improves +2.3 over MViTv1-\nB 16×4 (24.5 mAP) with fewer flops and parameters (both\nwith the same recipe and default K400 pre-training). For\nK600 pre-training, MViTv2-B 32×3 (29.9 mAP) improves\n+1.2 over MViTv1-B-24 32×3. This again validates the ef-\nfectiveness of the proposed MViTv2 improvements in§4.1\nof the main paper. Using full-resolution testing (without\ncropping) can further improve MViTv2-B by +0.6 to achieve\n30.5 mAP. Finally, the larger MViTv2-L 40×3 achieves the\nval mAP\nmodel pretrain center full FLOPs Param\nSlowFast, 4×16, R50 [23]\nK400\n21.9 - 52.6 33.7\nSlowFast, 8×8, R101 [23] 23.8 - 137.7 53.0\nMViTv1-B, 16×4 [21] 24.5 - 70.5 36.4\nMViTv1-B, 64×3 [21] 27.3 - 454.7 36.4\nMViTv2-S, 16×4 26.8 27.6 64.5 34.3\nMViTv2-B, 32×3 28.1 29.0 225.2 51.0\nSlowFast, 8×8 R101+NL [23]\nK600\n27.1 - 146.6 59.2\nSlowFast, 16×8 R101+NL [23] 27.5 - 296.3 59.2\nX3D-XL [22] 27.4 - 48.4 11.0\nObject Transformer [80] 31.0 - 243.8 86.2\nACAR 8×8, R101-NL [60] - 31.4 N/A N/A\nMViTv1-B, 16×4 [21] 26.1 - 70.4 36.3\nMViTv1-B-24, 32×3 [21] 28.7 236.0 52.9\nMViTv2-B, 32×3 29.9 30.5 225.2 51.0\nACAR 8×8, R101-NL [60] K700 - 33.3 N/A N/A\nMViTv2-B, 32×3 K700 31.3 32.3 225.2 51.0\nMViTv2-L↑ 3122, 40×3 IN21K+K700 33.5 34.4 2828 213.0\nTable A.2. Comparison with previvous work on A V A v2.2. We\nadopt two test strategies: 1) center (single center crop): we resize\nthe shorter spatial side to 224 pixels and takes a 2242 center crop\nfor inference. 2) full (full-resolution): we resize the shorter spatial\nside to 224 pixels and take the full image for inference. We report\ninference cost with the center testing strategy ( i.e. 2242 input).\nMagnitudes are Giga (109) for FLOPs and Mega (106) for Param.\nstate-of-the-art results at 34.4 mAP using IN-21K and K700\npre-training.\nA.3. Results: ImageNet Classification\nResults of ImageNet-1K. Table A.3 shows the comparison\nof our MViTv2 with more prior work (without external data\nor distillation models) on ImageNet-1K. As shown in the\nTable, our MViTv2 achieves better results than any previ-\nously published methods for a variety of model complexities.\nWe note that our improvements to pooling attention bring\nsignificant gains over the MViTv1 [21] counterparts which\nuse exactly the same training recipes (for all datasets we\ncompare on); therefore the gains over MViTv1 stem solely\nfrom our technical improvements in §4.1 of the main paper.\nA.4. Ablations: ImageNet and COCO\nDecomposed relative position embeddings.As introduced\nin Sec. 4.1, our Relative position embedding is only applied\nfor Qi by default. We could further extend it to all Q, K and\nV terms for attention layers:\nAttn(Q, K, V) =AV + E(relv),\nwhere A = Softmax\n\u0010\n(QK⊤ + E(relq) + E(relk))/\n√\nd\n\u0011\n.\nAcc\nmodel center resize FLOPs (G) Param (M)\nRegNetY-4GF [62] 80.0 4.0 21\nRegNetZ-4GF [15] 83.1 4.0 28\nEfficientNet-B4 ↑ 3802 [71] 82.9 4.2 19\nDeiT-S [72] 79.8 4.6 22\nPVT-S [78] 79.8 3.8 25\nTNT-S [33] 81.5 5.2 24\nT2T-ViTt-14 [85] 81.7 6.1 22\nCvT-13 [81] 81.6 4.5 20\nTwins-S [11] 81.7 2.9 24\nViL-S-RPB [89] 82.4 4.9 25\nPVTv2-V2 [77] 82.0 4.0 25\nCrossViTc-15 [9] 82.3 6.1 28\nXCiT-S12 [18] 82.0 4.8 26\nSwin-T [55] 81.3 4.5 29\nCSWin-T [16] 82.7 4.3 23\nMViTv2-T 82.3 4.7 24\nRegNetY-8GF [62] 81.7 8.0 39\nEfficientNet-B5 ↑ 4562 [71] 83.6 9.9 30\nPVT-M [78] 81.2 6.7 44\nT2T-ViTt-19 [85] 82.4 9.8 39\nCvT-21 [81] 82.5 7.1 32\nTwins-B [11] 83.2 8.6 56\nViL-M-RPB [89] 83.5 8.7 40\nPVTv2-V2-B3 [77] 83.2 6.9 45\nCrossViTc-18 [9] 82.8 9.5 44\nXCiT-S24 [18] 82.6 9.1 48\nSwin-S [55] 83.0 8.7 50\nCSWin-S [16] 83.6 6.9 35\nMViT-v1-B-16 [21] 83.0 7.8 37\nMViTv2-S 83.6 7.0 35\nRegNetY-16GF [62] 82.9 15.9 84\nRegNetZ-16GF [15] 84.1 15.9 95\nEfficientNet-B6 ↑ 5282 [71] 84.2 19 43\nNFNet-F0 ↑ 2562 [5] 83.6 12.4 72\nDeiT-B [72] 81.8 17.6 87\nPVT-L [78] 81.7 9.8 61\nT2T-ViTt-21 [85] 82.6 15.0 64\nTNT-B [33] 82.9 14.1 66\nTwins-L [11] 83.7 15.1 99\nViL-B-RPB [89] 83.7 13.4 56\nPVTv2-V2-B5 [77] 83.8 11.8 82\nCaiT-S36 [74] 83.3 13.9 68\nXCiT-M24 [18] 82.7 16.2 84\nSwin-B [55] 83.3 15.4 88\nCSWin-B [16] 84.2 15.0 78\nMViTv1-B-24 [21] 83.4 10.9 54\nMViTv2-B 84.4 10.2 52\nEfficientNet-B7 ↑ 6002 [71] 84.3 37.0 66\nNFNet-F1 ↑ 3202 [5] 84.7 35.5 133\nDeiT-B ↑ 3842 [72] 83.1 55.5 87\nTNT-B ↑ 3842 [33] 83.9 N/A 66\nCvT-32 ↑ 3842 [81] 83.3 24.9 32\nCaiT-S36↑ 3842 [74] 85.0 48 68\nSwin-B ↑ 3842 [55] 84.2 47.0 88\nMViT-v1-B-24↑ 3202 [21] 84.8 32.7 73\nMViTv2-B ↑ 3842 85.2 85.6 36.7 52\nNFNet-F2 ↑ 3522 [5] 85.1 62.6 194\nXCiT-M24 [18] 82.9 36.1 189\nCoAtNet-3 [13] 84.5 34.7 168\nMViTv2-L 85.3 42.1 218\nNFNet-F4 ↑ 5122 [5] 85.9 215.3 316\nCoAtNet-3 [13] ↑ 3842 85.8 107.4 168\nMViTv2-L ↑ 3842 86.0 86.3 140.2 218\nTable A.3. Comparison to previous work on ImageNet-1K. In-\nput images are 224×224 by default and ↑ denotes using different\nsizes. MViT is trained for 300 epochs without any external data or\nmodels. We report our ↑ 3842 models tested using a center crop or\na resized full crop of the original image, to compare to prior work.\nrel pos IN-1K COCO\nrelq relk relv Acc Mem(G) Test (im/s) APbox APmask\n✓ ✕ ✕ 83.6 6.2 316 49.9 45.0\n✕ ✓ ✕ 83.4 6.2 321 49.7 44.8\n✓ ✓ ✕ 83.6 6.4 300 50.0 45.0\n✕ ✕ ✓ 83.6 30.8 109 OOM OOM\n✓ ✕ ✓ 83.7 30.9 104 OOM OOM\n✓ ✓ ✓ 83.6 30.9 103 OOM OOM\nTable A.4. Ablation of rel pos embeddings on ImageNet-1K and\nCOCO with MViT-S.\nAnd the rel pos terms are defined as:\nE(relq)\nij =Qi · Rq\np(i),p(j),\nE(relk)\nij =Rk\np(i),p(j) · Ki,\nE(relv)\ni =\nX\nj\nAij ∗ Rv\np(i),p(j).\nTable A.4 shows the ablation experiments: different variants\nachieve similar accuracy on ImageNet and COCO. However\nrelv requires more GPU memory ( e.g. 30.8G vs 6.2G on\nImageNet and out-of-memory (OOM) on COCO) and has\na ∼2.9×lower test throughput on ImageNet. For simplicity\nand efficiency, we use only relq by default.\nEffect of pre-training datasets for detection. In §6.2 of\nthe main paper we observe that ImageNet pre-training can\nhave very different effects for different model sizes for video\nclassification. Here, we are interested in the impact of pre-\ntraining on the larger IN-21K vs. IN-1K for COCO object\ndetection tasks. Table A.5 shows our ablation: The large-\nscale IN-21K pre-training is more helpful for larger models,\ne.g. MViT-B and MViT-L have +0.5 and +0.9 gains in APbox.\nvariant APbox APmask\nIN-1k IN-21k IN-1k IN-21k\nMViTv2-S 49.9 50.2 45.1 45.1\nMViTv2-B 51.0 51.5 45.7 46.4\nMViTv2-L 51.8 52.7 46.2 46.8\nTable A.5. Effect of pre-training datasets for COCO. Detection\nmethods are initialized from IN-1K or IN-21K pre-trained weights.\nA.5. Ablations: Kinetics Action Classification\nIn §5.3 of the main paper we ablated the impact of our\nimprovements to pooling attention, i.e. decomposed relative\npositional embeddings & residual pooling connections, for\nimage classification and object detection. Here, we ablate\nthe effect of our improvements for video classification.\nPositional embeddings for video. Table A.6 compares dif-\nferent positional embeddings for MViTv2 on K400. Similar\nto image classification and object detection (Table 6 of the\nmain paper), relative positional embeddings surpass absolute\nrel. pos. abs. pos. Top-1 Train Param\nspace time (%) (clip/s) (M)\n(1) no pos. 80.1 91.5 34.4\n(2) abs. pos. ✓ 80.4 91.0 34.7\n(3) time-only rel. ✓ 80.8 80.5 34.4\n(4) space-only rel. dec. 80.6 76.2 34.5\n(5) dec. space rel. + time rel. dec. ✓ 81.0 66.6 34.5\n(6) joint space rel. + time rel. joint ✓ 81.1 33.6 37.1\n(7) joint space/time rel. joint - 8.4 73.7\nTable A.6. Ablation of positional embeddings on K400 with\nMViTv2-S 16×4. Training throughput is measured by average clips\nper-second with 8 V100 GPUs. Our ( 5) decomposed space/time\nrel. positional embeddings are accurate and significantly faster\nthan other joint versions. Note that we do not finish the full train-\ning for (7) joint space/time rel. as the training speed is too slow\n(∼8× slower than ours) and (6) joint space rel. already shows large\ndrawbacks (∼2× slower) of joint rel. positional embeddings.\npositional embeddings by ∼0.6% comparing (2) and (5, 6).\nComparing (5) to (6), our decomposed space/time rel. po-\nsitional embeddings achieve nearly the same accuracy as\nthe joint space rel. embeddings while being ∼2× faster in\ntraining. For joint space/time rel. (5 vs. 7), our decomposed\nspace/time rel. is even ∼8×faster with ∼2×fewer parame-\nters. This demonstrates the effectiveness of our decomposed\ndesign for relative positional embeddings.\nResidual pooling connection for video. Table A.7 studies\nthe effect of residual pooling connections on K400. We ob-\nserve similar results as for image classification and object\ndetection (Table 7 of the main paper), that: both Q pool-\ning blocks and residual paths are essential in our improved\nMViTv2 and combining them together leads to +1.7% ac-\ncuracy on K400 while using them separately only improves\nslightly (+0.4%).\nresidual pooling Top-1 FLOPs\n(1) w/o 79.3 64\n(2) full Q pooling 79.7 65\n(3) residual 79.7 64\n(4) full Q pooling + residual 81.0 65\nTable A.7. Ablation of residual pooling connections on K400\nwith MViTv2-S 16×4 architecture.\nB. Additional Implementation Details\nB.1. Other Upgrades in MViT\nBesides the technical improvements introduced in §4.1 of\nthe main paper, MViT entails two further changes: (i) We\nconduct the channel dimension expansion in the attention\ncomputation of the first transformer block of each stage,\ninstead of performing it in the last MLP block of the prior\nstage as in MViTv1 [21]. This change has similar accuracy\n(±0.1%) to the original version, while reducing parameters\nand FLOPs. (ii) We remove the class token in MViT by\ndefault as this has no advantage for image classification\ntasks. Instead, we average the output tokens from the last\ntransformer block and apply the final classification head\nupon it. In practice, we find this modification could reduce\nthe training time by ∼8%.\nB.2. Details: ImageNet Classification\nIN-1K training. We follow the training recipe of\nMViTv1 [21,72] for IN-1K training. We train for300 epochs\nwith 64 GPUs. The batch size is 32 per GPU by default.\nWe use truncated normal distribution initialization [35] and\nadopt synchronized AdamW [58] optimization with a base\nlearning rate of 2 × 10−3 for batch size of 2048. We use a\nlinear warm-up strategy in the first 70 epochs and a decayed\nhalf-period cosine schedule [72].\nFor regularization, we set weight decay to 0.05\nfor MViTv2-T/S/B and 0.1 for MViTv2-L/H and label-\nsmoothing [70] to 0.1. Stochastic depth [41] (i.e. drop-path\nor drop-connect) is also used with rate 0.1 for MViTv2-T &\nMViTv2-S, rate 0.3 for MViTv2-B, rate 0.5 for MViTv2-L\nand rate 0.8 for MViTv2-H. Other data augmentations have\nthe same (default) hyperparameters as in [21, 73], includ-\ning mixup [88], cutmix [87], random erasing [91] and rand\naugment [12].\nFor 384×384 input resolution, we fine-tune the models\ntrained on 224×224 resolution. We decrease the batch size\nto 8 per GPU and fine-tune 30 epochs with a base learning\nrate of 4 × 10−5 per 256 batch-size samples. For MViTv2-\nL and MViTv2-H, we disable mixup and fine-tune with a\nlearning rate of 5 × 10−4 per batch of 64. We linearly scale\nlearning rates with the number of overall GPUs ( i.e. the\noverall batch-size).\nIN-21K pre-training and fine-tuning on IN-1K.We down-\nload the latest winter-2021 version of IN-21K from the offi-\ncial website. The training recipe follows the IN-1K training\nintroduced above except for some differences described next.\nWe train the IN-21K models on the joint set of IN-21K\nand 1K for 90 epochs (60 epochs for MViTv2-H) with a\n6.75 × 10−5 base learning rate for MViTv2-S and MViTv2-\nB, and 10−4 for MViTv2-L and MViTv2-H, per batch-size\nof 256. The weight decay is set as 0.01 for MViTv2-S and\nMViTv2-B, and 0.1 for MViTv2-L and MViTv2-H.\nWhen fine-tuning IN-21K MViTv2 models on IN-1K for\nMViTv2-L and MViTv2-H, we disable mixup and fine-tune\nfor 30 epochs with a learning rate of 7 × 10−5 per batch of\n64. We use a weight decay of 5 × 10−2. The MViTv2-H ↑\n5122 model is initialized from the 3842 variant and trained\nfor 3 epochs with mixup enabled and weight decay of 10−8.\nB.3. Details: COCO Object Detection\nFor object detection experiments, we adopt two typical\nobject detection framework: Mask R-CNN [36] and Cascade\nMask R-CNN [6] in Detectron2 [82]. We follow the same\ntraining settings from [55]: multi-scale training (scale the\nshorter side in [480, 800] while longer side is smaller than\n1333), AdamW optimizer [58] ( β1, β2 = 0.9, 0.999, base\nlearning rate 1.6×10−4 for base size of 64, and weight decay\nof 0.1), and 3×schedule (36 epochs). The drop path rate is\nset as 0.1, 0.3, 0.4, 0.5 and 0.6 for MViTv2-T, MViTv2-S,\nMViTv2-B, MViTv2-L and MViTv2-H, respectively. We\nuse PyTorch’s automatic mixed precision during training.\nFor the stronger recipe for MViTv2-L and MViTv2-H\nin Table. 5 of the main paper, we use large-scale jittering\n(1024×1024 resolution) as the training augmentation [26]\nand a longer schedule (50 epochs) with IN-21K pre-training.\nB.4. Details: Kinetics Action Classification\nTraining from scratch. We follow the training recipe and\naugmentations from [19, 21] when training from scratch for\nKinetics datasets. We adopt synchronized AdamW [58] and\ntrain for 200 epochs with 2 repeated augmentation [40] on\n128 GPUs. The mini-batch size is 4 clips per GPU. We adopt\na half-period cosine schedule [57] of learning rate decaying.\nThe base learning rate is set as1.6×10−3 for 512 batch-size.\nWe use weight decay of 0.05 and set drop path rate as 0.2\nand 0.3 for MViTv2-S and MViTv2-B.\nFor the input clip, we randomly sample a clip (T frames\nwith a temporal stride of τ; denoted as T × τ [23]) from the\nfull-length video during training. For the spatial domain, we\nuse Inception-style [69] cropping (randomly resize the input\narea between a [min, max], scale of [0.08, 1.00], and jitter\naspect ratio between 3/4 to 4/3). Then we take an H × W =\n224×224 crop as the network input.\nDuring inference, we apply two testing strategies follow-\ning [21, 23]: (i) Temporally, uniformly samples K clips (e.g.\nK=5) from a video. (ii) in spatial axis, scales the shorter\nspatial side to 256 pixels and takes a 224×224 center crop\nor 3 crops of 224×224 to cover the longer spatial axis. The\nfinal score is averaged over all predictions.\nFor the input clips, we perform the same data augmen-\ntations across all frames, including random horizontal flip,\nmixup [88] and cutmix [87], random erasing [91], and rand\naugment [12].\nFor Kinetics-600 and Kinetics-700, all hyper-parameters\nare identical to K400.\nFine-tuning from ImageNet. When using IN-1K or IN-\n21K as pre-training, we adopt the initialization scheme intro-\nduced in §4.3 of the main paper and shorter training sched-\nules. For example, we train 100 epochs with base learning\nrate as 4.8 × 10−4 for 512 batch-size when fine-tuning from\nIN-1K for MViTv2-S and MViTv2-B, and 75 epochs with\nbase learning as 1.6 × 10−4 when fine-tuning from IN-21K.\nFor long-term models with 40 ×3 sampling, we initialize\nfrom the 16 ×4 counterparts, disable mixup, train for 30\nepochs with learning rate of 1.6 × 10−5 at batch-size of 128,\nand use a weight decay of 10−8.\nB.5. Details: Something-Something V2 (SSv2)\nThe SSv2 dataset [31] contains 169k training, and 25k\nvalidation videos with 174 human-object interaction classes.\nWe fine-tune the pre-trained Kinetics models and take the\nsame recipe as in [21]. Specifically, we train for 100 epochs\n(40 epochs for MViTv2-L) using 64 or 128 GPUs with 8\nclips per GPU and a base learning rate of 0.02 (for batch\nsize of 512) with half-period cosine decay [57]. We adopt\nsynchronized SGD and use weight decay of 10−4 and drop\npath rate of 0.4. The training augmentation is the same as\nKinetics in §B.4, except we disable random flipping and\nrepeated augmentations in training.\nWe use the segment-based input frame sampling [21, 52]\n(split each video into segments, and sample one frame from\neach segment to form a clip). During inference, we take a\nsingle clip with 3 spatial crops to form predictions over a\nsingle video.\nB.6. Details: A V A Action Detection\nThe A V A action detection dataset [32] assesses the spa-\ntiotemporal localization of human actions in videos. It has\n211k training and 57k validation video segments. We evalu-\nate methods on A V A v2.2 and use mean Average Precision\n(mAP) metric on 60 classes as is standard in prior work [23].\nWe use MViTv2 as the backbone and follow the same de-\ntection architecture in [21,23] that adapts Faster R-CNN [64]\nfor video action detection. Specifically, we extract region-of-\ninterest (RoI) features [29] by frame-wise RoIAlign [36] on\nthe spatiotemporal feature maps from the last MViTv2 layer.\nThe RoI features are then max-pooled and fed to a per-class,\nsigmoid classifier for action prediction.\nThe training recipe is identical to [21] and summarized\nnext. We pre-train our MViTv2 models on Kinetics. The re-\ngion proposals are identical to the ones used in [21, 23]. We\nuse proposals that have overlaps with ground-truth boxes by\nIoU > 0.9 for training. The models are trained with synchro-\nnized SGD training on 64 GPUs (8 clips per GPU). The base\nlearning rate is set as 0.6 with a half-period cosine schedule\nof learning rate decaying. We train for 30 epochs with linear\nwarm-up [30] for the first 5 epochs and use a weight decay\nof 1 × 10−8 and drop-path rate of 0.4.\nC. Additional Discussions\nSocietal impact. Our MViTv2 is a general vision backbone\nfor various vision tasks, including image recognition, object\ndetection, instance segmentation, video classification and\nvideo detection. Though we are not providing any direct\napplications, it could potentially apply to a wide range of\nvision-related applications, which then might have a wide\nrange of societal impacts. On the positive side, the bet-\nter vision backbone could potentially improve the perfor-\nmance of many different computer vision applications, e.g.\nvisual inspection and quality management in manufactur-\ning, cancer and tumor detection in healthcare, and vehicle\nre-identification and pedestrian detection in transportation.\nOn the other hand, the advanced vision recognition tech-\nnologies could also have potential negative societal impact\nif they are adopted by harmful or mismanaged applications,\ne.g. usage in surveillance systems that violate privacy. It is\nimportant to be aware when vision technologies are deployed\nin practical applications.\nLimitations. Our MViTv2 is a general vision backbone and\nwe demonstrate its effectiveness on various recognition tasks.\nTo reduce the full hyperparameter tuning space for MViTv2\non different datasets and tasks, we mainly follow the ex-\nisting standard recipe for each task from the community\n(e.g. [21, 55, 73]) with lightweight tuning (e.g. learning rate,\nweight decay). Therefore, the choice of hyperparameters for\ndifferent MViTv2 variants may be suboptimal.\nIn addition, MViTv2 provides five different variants from\ntiny to huge models with different complexity as a general\nbackbone. In the future, we think there are two potential\ninteresting research directions: scaling down MViTv2 to\neven smaller models for mobile applications, and scaling up\nMViTv2 to even larger models for large-scale data scenarios.\nReferences\n[1] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun,\nMario Luˇci´c, and Cordelia Schmid. Vivit: A video vision\ntransformer. arXiv preprint arXiv:2103.15691, 2021. 1, 2, 8\n[2] Josh Beal, Eric Kim, Eric Tzeng, Dong Huk Park, Andrew\nZhai, and Dmitry Kislyuk. Toward transformer-based object\ndetection. arXiv preprint arXiv:2012.09958, 2020. 1\n[3] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is\nspace-time attention all you need for video understanding?\narXiv preprint arXiv:2102.05095, 2021. 2, 8\n[4] Navaneeth Bodla, Bharat Singh, Rama Chellappa, and Larry S\nDavis. Soft-nms–improving object detection with one line of\ncode. In Proc. ICCV, 2017. 6, 9\n[5] Andrew Brock, Soham De, Samuel L Smith, and Karen Si-\nmonyan. High-performance large-scale image recognition\nwithout normalization. arXiv preprint arXiv:2102.06171 ,\n2021. 5, 10\n[6] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delving\ninto high quality object detection. In Proc. CVPR, 2018. 2, 5,\n6, 9, 12\n[7] Jo˜ao Carreira, Eric Noland, Chloe Hillier, and Andrew Zisser-\nman. A short note on the kinetics-700 human action dataset.\narXiv preprint arXiv:1907.06987, 2019. 7\n[8] Joao Carreira and Andrew Zisserman. Quo vadis, action\nrecognition? a new model and the kinetics dataset. In Proc.\nCVPR, 2017. 2, 4, 7\n[9] Chun-Fu Chen, Quanfu Fan, and Rameswar Panda. Crossvit:\nCross-attention multi-scale vision transformer for image clas-\nsification. In Proc. ICCV, 2021. 4, 10\n[10] Yunpeng Chen, Haoqi Fang, Bing Xu, Zhicheng Yan, Yan-\nnis Kalantidis, Marcus Rohrbach, Shuicheng Yan, and Jiashi\nFeng. Drop an octave: Reducing spatial redundancy in con-\nvolutional neural networks with octave convolution. arXiv\npreprint arXiv:1904.05049, 2019. 2\n[11] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing\nRen, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. Twins: Re-\nvisiting the design of spatial attention in vision transformers.\nIn NIPS, 2021. 2, 5, 10\n[12] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V\nLe. Randaugment: Practical automated data augmentation\nwith a reduced search space. In Proc. CVPR, 2020. 11, 12\n[13] Zihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan.\nCoatnet: Marrying convolution and attention for all data sizes.\narXiv preprint arXiv:2106.04803, 2021. 5, 10\n[14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li\nFei-Fei. Imagenet: A large-scale hierarchical image database.\nIn Proc. CVPR, pages 248–255. Ieee, 2009. 2, 4\n[15] Piotr Doll ´ar, Mannat Singh, and Ross Girshick. Fast and\naccurate model scaling. In Proc. CVPR, 2021. 2, 5, 10\n[16] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang,\nNenghai Yu, Lu Yuan, Dong Chen, and Baining Guo. Cswin\ntransformer: A general vision transformer backbone with\ncross-shaped windows. arXiv preprint arXiv:2107.00652 ,\n2021. 5, 10\n[17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 1, 2, 5\n[18] Alaaeldin El-Nouby, Hugo Touvron, Mathilde Caron, Piotr\nBojanowski, Matthijs Douze, Armand Joulin, Ivan Laptev,\nNatalia Neverova, Gabriel Synnaeve, Jakob Verbeek, et al.\nXcit: Cross-covariance image transformers. arXiv preprint\narXiv:2106.09681, 2021. 5, 10\n[19] Haoqi Fan, Yanghao Li, Bo Xiong, Wan-Yen Lo, and\nChristoph Feichtenhofer. PySlowFast. https://github.\ncom/facebookresearch/slowfast, 2020. 2, 7, 12\n[20] Haoqi Fan, Tullie Murrell, Heng Wang, Kalyan Vasudev Al-\nwala, Yanghao Li, Yilei Li, Bo Xiong, Nikhila Ravi, Meng Li,\nHaichuan Yang, Jitendra Malik, Ross Girshick, Matt Feiszli,\nAaron Adcock, Wan-Yen Lo, and Christoph Feichtenhofer.\nPyTorchVideo: A deep learning library for video understand-\ning. In Proceedings of the 29th ACM International Conference\non Multimedia, 2021. https://pytorchvideo.org/.\n2\n[21] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li,\nZhicheng Yan, Jitendra Malik, and Christoph Feichtenhofer.\nMultiscale vision transformers. In Proc. ICCV, 2021. 1, 2, 3,\n4, 5, 6, 7, 8, 9, 10, 11, 12, 13\n[22] Christoph Feichtenhofer. X3D: Expanding architectures for\nefficient video recognition. In Proc. CVPR, pages 203–213,\n2020. 2, 8, 9\n[23] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and\nKaiming He. SlowFast networks for video recognition. In\nProc. ICCV, 2019. 2, 7, 8, 9, 12\n[24] Christoph Feichtenhofer, Axel Pinz, and Richard Wildes. Spa-\ntiotemporal residual networks for video action recognition.\nIn NIPS, 2016. 4\n[25] Christoph Feichtenhofer, Axel Pinz, and Andrew Zisserman.\nConvolutional two-stream network fusion for video action\nrecognition. In Proc. CVPR, 2016. 2\n[26] Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-\nYi Lin, Ekin D Cubuk, Quoc V Le, and Barret Zoph. Simple\ncopy-paste is a strong data augmentation method for instance\nsegmentation. In Proc. CVPR, 2021. 6, 9, 12\n[27] Golnaz Ghiasi, Tsung-Yi Lin, and Quoc V Le. Nas-fpn:\nLearning scalable feature pyramid architecture for object de-\ntection. In Proc. CVPR, 2019. 9\n[28] Rohit Girdhar, Joao Carreira, Carl Doersch, and Andrew\nZisserman. Video action transformer network. InProc. CVPR,\n2019. 2\n[29] Ross Girshick. Fast R-CNN. In Proc. ICCV, 2015. 2, 12\n[30] Priya Goyal, Piotr Doll ´ar, Ross Girshick, Pieter Noord-\nhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch,\nYangqing Jia, and Kaiming He. Accurate, large minibatch\nSGD: training ImageNet in 1 hour. arXiv:1706.02677, 2017.\n12\n[31] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michal-\nski, Joanna Materzynska, Susanne Westphal, Heuna Kim,\nValentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-\nFreitag, et al. The “Something Something” video database\nfor learning and evaluating visual common sense. In ICCV,\n2017. 7, 12\n[32] Chunhui Gu, Chen Sun, David A. Ross, Carl V ondrick, Car-\noline Pantofaru, Yeqing Li, Sudheendra Vijayanarasimhan,\nGeorge Toderici, Susanna Ricco, Rahul Sukthankar, Cordelia\nSchmid, and Jitendra Malik. A V A: A video dataset of spatio-\ntemporally localized atomic visual actions. In Proc. CVPR,\n2018. 9, 12\n[33] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu,\nand Yunhe Wang. Transformer in transformer. In NIPS, 2021.\n5, 10\n[34] Zhang Hang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Zhi\nZhang, Haibin Lin, and Yue Sun. Resnest: Split-attention\nnetworks. 2020. 2\n[35] Boris Hanin and David Rolnick. How to start training:\nThe effect of initialization and architecture. arXiv preprint\narXiv:1803.01719, 2018. 11\n[36] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Gir-\nshick. Mask R-CNN. In Proc. ICCV, 2017. 1, 3, 5, 6, 11,\n12\n[37] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDelving deep into rectifiers: Surpassing human-level perfor-\nmance on imagenet classification. In Proc. CVPR, 2015. 1\n[38] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proc. CVPR,\n2016. 2, 6\n[39] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nIdentity mappings in deep residual networks. In Proc. ECCV,\n2016. 2\n[40] Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten\nHoefler, and Daniel Soudry. Augment your batch: Improving\ngeneralization through instance repetition. In Proc. CVPR,\npages 8129–8138, 2020. 12\n[41] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q\nWeinberger. Deep networks with stochastic depth. In Proc.\nECCV, 2016. 11\n[42] Boyuan Jiang, MengMeng Wang, Weihao Gan, Wei Wu, and\nJunjie Yan. Stm: Spatiotemporal and motion encoding for\naction recognition. In Proc. CVPR, pages 2000–2009, 2019.\n2\n[43] Zihang Jiang, Qibin Hou, Li Yuan, Daquan Zhou, Xiaojie\nJin, Anran Wang, and Jiashi Feng. Token labeling: Training\na 85.5% top-1 accuracy vision transformer with 56m param-\neters on imagenet. arXiv preprint arXiv:2104.10858, 2021.\n5\n[44] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,\nChloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola,\nTim Green, Trevor Back, Paul Natsev, et al. The kinetics\nhuman action video dataset. arXiv:1705.06950, 2017. 2, 7\n[45] Dan Kondratyuk, Liangzhe Yuan, Yandong Li, Li Zhang,\nMingxing Tan, Matthew Brown, and Boqing Gong.\nMoViNets: Mobile video networks for efficient video recog-\nnition. In Proc. CVPR, 2021. 8\n[46] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Ima-\ngeNet classification with deep convolutional neural networks.\nIn NIPS, 2012. 2\n[47] Yann LeCun, Bernhard Boser, John Denker, Donnie Hender-\nson, Richard Howard, Wayne Hubbard, and Lawrence Jackel.\nHandwritten digit recognition with a back-propagation net-\nwork. In NIPS, 1989. 3\n[48] Yann LeCun, Bernhard Boser, John S Denker, Donnie Hen-\nderson, Richard E Howard, Wayne Hubbard, and Lawrence D\nJackel. Backpropagation applied to handwritten zip code\nrecognition. Neural computation, 1(4):541–551, 1989. 2\n[49] Yan Li, Bin Ji, Xintian Shi, Jianguo Zhang, Bin Kang, and\nLimin Wang. Tea: Temporal excitation and aggregation for\naction recognition. In Proc. CVPR, pages 909–918, 2020. 8\n[50] Yanghao Li, Saining Xie, Xinlei Chen, Piotr Dollar, Kaim-\ning He, and Ross Girshick. Benchmarking detection\ntransfer learning with vision transformers. arXiv preprint\narXiv:2111.11429, 2021. 7\n[51] Zhenyang Li, Kirill Gavrilyuk, Efstratios Gavves, Mihir Jain,\nand Cees GM Snoek. VideoLSTM convolves, attends and\nflows for action recognition. Computer Vision and Image\nUnderstanding, 166:41–50, 2018. 2\n[52] Ji Lin, Chuang Gan, and Song Han. Temporal shift module\nfor efficient video understanding. In Proc. ICCV, 2019. 12\n[53] Tsung-Yi Lin, Piotr Doll ´ar, Ross Girshick, Kaiming He,\nBharath Hariharan, and Serge Belongie. Feature pyramid\nnetworks for object detection. In Proc. CVPR, 2017. 1, 2, 3\n[54] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence\nZitnick. Microsoft COCO: Common objects in context. In\nProc. ECCV, 2014. 4, 5\n[55] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. arXiv\npreprint arXiv:2103.14030, 2021. 1, 2, 3, 4, 5, 6, 7, 9, 10, 12,\n13\n[56] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang,\nStephen Lin, and Han Hu. Video swin transformer. arXiv\npreprint arXiv:2106.13230, 2021. 2, 8\n[57] Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradient\ndescent with warm restarts. arXiv:1608.03983, 2016. 12\n[58] Ilya Loshchilov and Frank Hutter. Fixing weight decay regu-\nlarization in adam. 2018. 11, 12\n[59] Daniel Neimark, Omri Bar, Maya Zohar, and Dotan As-\nselmann. Video transformer network. arXiv preprint\narXiv:2102.00719, 2021. 2, 8\n[60] Junting Pan, Siyu Chen, Mike Zheng Shou, Yu Liu, Jing Shao,\nand Hongsheng Li. Actor-context-actor relation network for\nspatio-temporal action localization. In Proc. CVPR, 2021. 9\n[61] Zhaofan Qiu, Ting Yao, and Tao Mei. Learning spatio-\ntemporal representation with pseudo-3d residual networks. In\nProc. ICCV, 2017. 2\n[62] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaim-\ning He, and Piotr Doll´ar. Designing network design spaces.\nIn Proc. CVPR, June 2020. 2, 5, 10\n[63] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali\nFarhadi. You only look once: Unified, real-time object detec-\ntion. In Proc. CVPR, 2016. 2\n[64] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster R-CNN: Towards real-time object detection with region\nproposal networks. In NIPS, 2015. 12\n[65] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-\nattention with relative position representations. arXiv preprint\narXiv:1803.02155, 2018. 3\n[66] Karen Simonyan and Andrew Zisserman. Two-stream convo-\nlutional networks for action recognition in videos. In NIPS,\n2014. 2\n[67] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. In Proc.\nICLR, 2015. 1, 2\n[68] Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia\nSchmid. Segmenter: Transformer for semantic segmentation.\narXiv preprint arXiv:2105.05633, 2021. 1\n[69] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\nVanhoucke, and Andrew Rabinovich. Going deeper with\nconvolutions. In Proc. CVPR, 2015. 2, 12\n[70] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe,\nJonathon Shlens, and Zbigniew Wojna. Rethinking the in-\nception architecture for computer vision. arXiv:1512.00567,\n2015. 11\n[71] Mingxing Tan and Quoc V Le. Efficientnet: Rethinking model\nscaling for convolutional neural networks. arXiv preprint\narXiv:1905.11946, 2019. 2, 5, 10\n[72] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv´e J´egou. Training\ndata-efficient image transformers & distillation through at-\ntention. arXiv preprint arXiv:2012.12877, 2020. 4, 5, 10,\n11\n[73] Hugo Touvron, Matthieu Cord, Matthijs Douze, Fran-\ncisco Massa, Alexandre Sablayrolles, and Herv ´e J ´egou.\nDeiT: Data-efficient image transformers. arXiv preprint\narXiv:2012.12877, 2020. 1, 2, 11, 13\n[74] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles,\nGabriel Synnaeve, and Herv ´e J ´egou. Going deeper with\nimage transformers. arXiv preprint arXiv:2103.17239, 2021.\n5, 10\n[75] Du Tran, Heng Wang, Lorenzo Torresani, and Matt Feiszli.\nVideo classification with channel-separated convolutional net-\nworks. In Proc. ICCV, 2019. 2\n[76] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Il-\nlia Polosukhin. Attention is all you need. arXiv preprint\narXiv:1706.03762, 2017. 1, 2\n[77] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pvtv2:\nImproved baselines with pyramid vision transformer. arXiv\npreprint arXiv:2106.13797, 2021. 5, 10\n[78] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyra-\nmid vision transformer: A versatile backbone for dense pre-\ndiction without convolutions. In IEEE ICCV, 2021. 1, 2, 6,\n10\n[79] Chao-Yuan Wu, Christoph Feichtenhofer, Haoqi Fan, Kaim-\ning He, Philipp Kr¨ahenb¨uhl, and Ross Girshick. Long-term\nfeature banks for detailed video understanding. In Proc.\nCVPR, 2019. 2\n[80] Chao-Yuan Wu and Philipp Krahenbuhl. Towards long-form\nvideo understanding. In Proc. CVPR, 2021. 9\n[81] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang\nDai, Lu Yuan, and Lei Zhang. Cvt: Introducing convolutions\nto vision transformers. arXiv preprint arXiv:2103.15808 ,\n2021. 4, 5, 10\n[82] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen\nLo, and Ross Girshick. Detectron2. https://github.\ncom/facebookresearch/detectron2 , 2019. 5, 6,\n12\n[83] Saining Xie, Ross Girshick, Piotr Doll´ar, Zhuowen Tu, and\nKaiming He. Aggregated residual transformations for deep\nneural networks. In Proc. CVPR, 2017. 6\n[84] Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and\nKevin Murphy. Rethinking spatiotemporal feature learning\nfor video understanding. arXiv:1712.04851, 2017. 2\n[85] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,\nZi-Hang Jiang, Francis E.H. Tay, Jiashi Feng, and Shuicheng\nYan. Tokens-to-token vit: Training vision transformers from\nscratch on imagenet. In Proc. ICCV, 2021. 10\n[86] Li Yuan, Qibin Hou, Zihang Jiang, Jiashi Feng, and\nShuicheng Yan. V olo: Vision outlooker for visual recognition,\n2021. 5\n[87] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk\nChun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regu-\nlarization strategy to train strong classifiers with localizable\nfeatures. In Proc. ICCV, 2019. 11, 12\n[88] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David\nLopez-Paz. Mixup: Beyond empirical risk minimization. In\nProc. ICLR, 2018. 11, 12\n[89] Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu\nYuan, Lei Zhang, and Jianfeng Gao. Multi-scale vision long-\nformer: A new vision transformer for high-resolution image\nencoding. In Proc. ICCV, 2021. 2, 5, 6, 10\n[90] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu,\nZekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xi-\nang, Philip HS Torr, et al. Rethinking semantic segmentation\nfrom a sequence-to-sequence perspective with transformers.\nIn Proc. CVPR, 2021. 1\n[91] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and\nYi Yang. Random erasing data augmentation. In Proceedings\nof the AAAI Conference on Artificial Intelligence, volume 34,\npages 13001–13008, 2020. 11, 12\n[92] Bolei Zhou, Alex Andonian, Aude Oliva, and Antonio Tor-\nralba. Temporal relational reasoning in videos. In ECCV,\n2018. 2\n[93] Xingyi Zhou, Dequan Wang, and Philipp Kr¨ahenb¨uhl. Objects\nas points. arXiv preprint arXiv:1904.07850, 2019. 2",
  "topic": "Pooling",
  "concepts": [
    {
      "name": "Pooling",
      "score": 0.8707983493804932
    },
    {
      "name": "Computer science",
      "score": 0.7679842710494995
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7185425162315369
    },
    {
      "name": "Contextual image classification",
      "score": 0.616287887096405
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.6048628687858582
    },
    {
      "name": "Object detection",
      "score": 0.5885235667228699
    },
    {
      "name": "Residual",
      "score": 0.4593198299407959
    },
    {
      "name": "Transformer",
      "score": 0.44899648427963257
    },
    {
      "name": "Architecture",
      "score": 0.43752095103263855
    },
    {
      "name": "Machine learning",
      "score": 0.41718801856040955
    },
    {
      "name": "Computer vision",
      "score": 0.339822918176651
    },
    {
      "name": "Image (mathematics)",
      "score": 0.24591422080993652
    },
    {
      "name": "Algorithm",
      "score": 0.08989140391349792
    },
    {
      "name": "Engineering",
      "score": 0.07585370540618896
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}