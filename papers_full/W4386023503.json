{
  "title": "ChatENT: Augmented Large Language Model for Expert Knowledge Retrieval in Otolaryngology - Head and Neck Surgery",
  "url": "https://openalex.org/W4386023503",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2224876414",
      "name": "Cai Long",
      "affiliations": [
        "University of Alberta"
      ]
    },
    {
      "id": "https://openalex.org/A5092807091",
      "name": "Deepak Subburam",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4378992991",
      "name": "Kayle Lowe",
      "affiliations": [
        "University of Alberta"
      ]
    },
    {
      "id": "https://openalex.org/A2150511590",
      "name": "André dos SANTOS",
      "affiliations": [
        "Turing Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2105918414",
      "name": "Jessica Zhang",
      "affiliations": [
        "University of Alberta"
      ]
    },
    {
      "id": "https://openalex.org/A2113109523",
      "name": "Sang Hwang",
      "affiliations": [
        "University of Alberta"
      ]
    },
    {
      "id": "https://openalex.org/A5092807092",
      "name": "Neil Saduka",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3014959765",
      "name": "Yoav Horev",
      "affiliations": [
        "University of Alberta"
      ]
    },
    {
      "id": "https://openalex.org/A2096346239",
      "name": "Tao Su",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2121324024",
      "name": "David Cote",
      "affiliations": [
        "University of Alberta"
      ]
    },
    {
      "id": "https://openalex.org/A2116508022",
      "name": "Erin Wright",
      "affiliations": [
        "University of Alberta"
      ]
    },
    {
      "id": "https://openalex.org/A2224876414",
      "name": "Cai Long",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5092807091",
      "name": "Deepak Subburam",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4378992991",
      "name": "Kayle Lowe",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2150511590",
      "name": "André dos SANTOS",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2105918414",
      "name": "Jessica Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2113109523",
      "name": "Sang Hwang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5092807092",
      "name": "Neil Saduka",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3014959765",
      "name": "Yoav Horev",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096346239",
      "name": "Tao Su",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2121324024",
      "name": "David Cote",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2116508022",
      "name": "Erin Wright",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2906295032",
    "https://openalex.org/W2664267452",
    "https://openalex.org/W4319662928",
    "https://openalex.org/W4282916733",
    "https://openalex.org/W4321459182",
    "https://openalex.org/W4323021123",
    "https://openalex.org/W4323545986",
    "https://openalex.org/W4361289889",
    "https://openalex.org/W2803760365",
    "https://openalex.org/W2906774465",
    "https://openalex.org/W2558050786",
    "https://openalex.org/W2137625386",
    "https://openalex.org/W2996569343",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W4387356888",
    "https://openalex.org/W4311991135",
    "https://openalex.org/W4322622443",
    "https://openalex.org/W4376637945",
    "https://openalex.org/W4378976945",
    "https://openalex.org/W4321499561",
    "https://openalex.org/W4317910584",
    "https://openalex.org/W4384561707",
    "https://openalex.org/W4367672504",
    "https://openalex.org/W4283704460",
    "https://openalex.org/W4379743664",
    "https://openalex.org/W4319344822"
  ],
  "abstract": "Abstract Background The recent surge in popularity of Large Language Models (LLMs), such as ChatGPT, has showcased their proficiency in medical examinations and potential contributions to medical education. However, LLMs possess inherent limitations, including inconsistent accuracy, specific prompting requirements, and the risk of generating harmful hallucinations. A domain-specific, fine-tuned model would address these limitations effectively. Methods OHNS-relevant data was systematically gathered from open-access internet sources and indexed into a database. We leveraged Retrieval-Augmented Language Modeling (RALM) to recall this information and used it for pre-training, which was then integrated into ChatGPT 4·0, creating a OHNS specific knowledge Q&amp;A platform known as ChatENT. Findings ChatENT showed enhanced performance in the analysis and interpretation of OHNS information, outperforming ChatGPT 4.0 in both the Canadian Royal College OHNS sample examination questions challenge and the US board practice questions challenge, with a 58.4% and 26·0% error reduction, respectively. ChatENT generated fewer hallucinations and demonstrated greater consistency. Interpretation To the best of our knowledge, ChatENT is the first specialty-specific LLM in the medical field. It appears to have considerable promise in areas such as medical education, patient education, and clinical decision support. The fine-tuned model has demonstrated the capacity to overcome the limitations of existing LLMs, thereby signaling a future of more precise, safe, and user-friendly applications in the realm of OHNS. Funding The authors received no financial support for the research, authorship, and/or publication of this project.",
  "full_text": "ChatENT: Augmented Large Language Model for Expert Knowledge Retrieval in \nOtolaryngology - Head and Neck Surgery \nCai Long, Deepak Subburam, Kayle Lowe, André dos Santos, Jessica Zhang, Sang Hwang, Neil \nSaduka, Yoav Horev, Tao Su, David Cote, Erin Wright \n \nAbstract \nBackground \nThe recent surge in popularity of Large Language Models (LLMs), such as ChatGPT, has \nshowcased their proficiency in medical examinations and potential contributions to medical \neducation. However, LLMs possess inherent limitations, including inconsistent accuracy, \nspecific prompting requirements, and the risk of generating harmful hallucinations. A domain-\nspecific, fine-tuned model would address these limitations effectively.  \nMethods \nOHNS-relevant data was systematically gathered from open-access internet sources and indexed \ninto a database. We leveraged Retrieval-Augmented Language Modeling (RALM) to recall this \ninformation and used it for pre-training, which was then integrated into ChatGPT 4·0, creating a \nOHNS specific knowledge Q&A platform known as ChatENT.  \nFindings  \nChatENT showed enhanced performance in the analysis and interpretation of OHNS \ninformation, outperforming ChatGPT 4.0 in both the Canadian Royal College OHNS sample \nexamination questions challenge and the US board practice questions challenge, with a 58.4% \nand 26·0% error reduction, respectively. ChatENT generated fewer hallucinations and \ndemonstrated greater consistency. \nInterpretation \nTo the best of our knowledge, ChatENT is the first specialty-specific LLM in the medical field. \nIt appears to have considerable promise in areas such as medical education, patient education, \nand clinical decision support. The fine-tuned model has demonstrated the capacity to overcome \nthe limitations of existing LLMs, thereby signaling a future of more precise, safe, and user-\nfriendly applications in the realm of OHNS. \nFunding: The authors received no financial support for the research, authorship, and/or \npublication of this project. \n  \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 6, 2023. ; https://doi.org/10.1101/2023.08.18.23294283doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\nIntroduction \n \nLarge language models (LLMs) are artificial intelligence (AI) systems that are trained on text-\nbased human knowledge, derived from articles, books, and other internet-based content. These \nadvanced artificial intelligence (AI) models effectively grasp language syntax, semantics and \npatterns, enabling the generation of dynamic, coherent text-based responses.1–3 With their \nadvanced information structuring capabilities, LLMs have revolutionized many fields, including \nmedicine—offering applications with significant healthcare impact while helping mitigate \nmedical errors.\n4–6 \n \nIn medicine, LLMs have been applied to many realms, including medical education, clinical \ndecision support, and analysis of medical images and patient data.5,7–11 Furthermore, recent \nevaluations on LLMs' effectiveness in generating medical reports and responding to consults \nshowed improvements in administrative efficiency. Specific area of focus include information \nmanagement in radiology and pathology.\n12–15 \n \nSeveral LLMs have been tailor-made for medical contexts to optimize AI performance in \nhealthcare. Notably, by leveraging Bidirectional Encoder Representations from Transformers \n(BERT), specialized models like BioBERT and ClinicalBERT have emerged, designed using \nbiomedical texts to enhance language comprehension within the medical sphere.\n16 However, \nthese models sometimes grapple with managing contextual data.  \n \nSeveral approaches have been proposed to address unique healthcare needs. Relevant examples \ninclude an expert system for psychiatric consultations and knowledge graphs for neurologic \nsubarachnoid hemorrhage predictions.\n17,18 Singhal et al. introduced Med-PaLM, which achieved \ngood results in knowledge retrieval and clinical decision support tasks, but still fell short \ncompared to human clinicians' performance.\n16 Med-PaLM was assessed on a human evaluation \nframework using traits such as factuality, comprehension, reasoning, possible harm and bias19. \nJeblick et al focused on exploring LLMs' potential in simplifying intricate medical reports in the \ncontext of radiology.20 A generative model was developed in UK to analyze Electronic Health \nRecords to make medical risk forecasts.21 \n \nThough LLMs generally come with policies that restrict their use in high-risk sectors like legal \nand healthcare services, recent advancements, particularly in models like ChatGPT and Bard, \nhave shown great potential for healthcare applications.\n3,22  However, the inherent risks associated \nwith LLMs such as truthfulness, bias, toxicity, and other societal risks, necessitate further \nresearch.3 \n \nChatGPT is currently one of the most popular LLMs used and tested specifically among medical \nspecialties. It has been evaluated broadly through physician-generated questions and physician-\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 6, 2023. ; https://doi.org/10.1101/2023.08.18.23294283doi: medRxiv preprint \nbased appraisal across seventeen specialties for accuracy and reliability.23 For otolaryngology—\nhead and neck surgery (OHNS) in particular, ChatGPT was assessed as an informational \nresource for OHNS patients for safety, accuracy and comprehensiveness, as well as on the Royal \nCollege of Physicians and Surgeons of Canada’s OHNS board exam based on concordance, \nvalidity, safety, accuracy, where it showed satisfactory performance.\n24,25 However, given LLMs' \npotential biases, misuses and their limitations in producing complete and factually accurate \noutputs, ethical considerations such as data privacy, accountability, and fairness should be \naccounted for when working with LLMs in healthcare.\n26,27 Some attempts have been made to \naddress these issues, including developing LLMs that focus on specific areas.28 Zakka et al29 \npublished a retrieval-augmented oriented language model for generalized clinical medicine \nutilizing external tools, including search engines.  \n \nGenerally, building an LLM may take a great amount of resources and time.\n30 There is a growing \ninterest and need for Green AI, where those using and developing AI models are held more \naccountable for their carbon impacts.\n31  \n \nTo our knowledge, no medical specialty-specific LLMs, such as those centering on \notolaryngology or other fields, have been published. Every specialty possesses its unique \nknowledge base, with abundance of acronyms and synonyms, necessitating meticulous curation \nand adaptation.  A framework with demonstrated capacity to overcome the limitations of existing \nLLMs would be crucial to a future of more precise, safe, and user-friendly applications in diverse \nrealms of clinical practice for both clinicians and patients. We hereby propose a cost-efficient, \nsimplified approach to construct a medical specialty knowledge domain AI, demonstrated by \nChatENT.  \n  \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 6, 2023. ; https://doi.org/10.1101/2023.08.18.23294283doi: medRxiv preprint \nMethods \n \nFigure 1. System flow diagram.a \naGreen: Knowledge-relevant steps; Blue: Question-relevant steps; Red: Answer relevant steps. \n \nFigure 1 shows the flow of operations, from question entry to answer generation. The first step is \na one-time preparation of the ChatENT Knowledge Base: we split this corpus of text into one-\npage-length chunks of text, and feed each chunk to OpenAI's embedding model (text-\nembedding-ada-002) to get a vector representation (a list of 1,536 real numbers) of each chunk. \nThis is shown in the bottom left of the diagram. When a question is entered, a vector for it is also \ncomputed, and compared against all the knowledge base chunk vectors. The 5 knowledge base \nchunks with the most similar vectors get selected. The text associated with these, along with the \nquestion and instruction (prompt text) to ChatGPT, get assembled into a full query. This query is \nsent to ChatGPT which then generates the answer. \n \nWe improve upon vanilla ChatGPT Q&A by adding a preparatory stage to the process that \naugments the model with specialized domain knowledge text. In this stage, a separate machine-\nlearning approach is used to retrieve text relevant to the question posed from ChatENT's \nKnowledge Base. This retrieved text is pre-pended to the question, and ChatGPT is prompted to \nanswer the question using the information in this preface. We use ChatGPT's subsequent \nresponse as the answer for evaluation in this study. \n \nWe do not train ChatGPT on the CEKB, for that is prohibitively expensive even if we could do \nit, and furthermore is likely not the best approach—see the following section that discusses this. \n \n is \nso \n is \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 6, 2023. ; https://doi.org/10.1101/2023.08.18.23294283doi: medRxiv preprint \nNeither do we have the option to include the entire CEKB as a preface to the question posed—\nthere is a limit to the size of the full submission to ChatGPT, which ranges from 4,096 tokens \n(~10 pages) to 32,768 tokens (~80 pages) depending on the model version used. \n \nSo we pick 6 pages or so of the most relevant content in the CEKB to send ChatGPT, using their \n10-page model. This allows enough space for the question itself and ChatGPT's response (which \nall need to fit within the 10 pages). These picks are made using the machine-learning technique \nof embedding vector similarity search. \n \nBefore employing that technique, we first chunk the CEKB into contiguous blocks of text, each \nabout one page long. This results in more than 20,000 chunks of text. The task is then, for any \ngiven question, to pick the 6 or so chunks that are most relevant to that question. Embedding \nvector similarity search works by representing each of these chunks of text with an embedding \nvector (a sequence of real numbers). These embedding vectors have the property that text with \nsimilar semantic meaning have low cosine distance between their respective embedding vectors. \n \nOpenAI's text-embedding-ada-002 model \n32,33 was used to compute the embedding vector  for \neach chunk. When ChatENT receives a user question, that question is also run through the same \ntext-embedding-ada-002 model, giving a single embedding vector representing the question's \nsemantic meaning. The cosine similarity is then calculated between the question's embedding \nvector and the embedding vectors for each chunk in the CEKB. We then pick the 6 or so chunks \nwith the greatest cosine similarity. Hence the name embedding vector similarity search. \n \nThe picks are then concatenated to create the preface in the full query that is submitted to \nChatGPT, as explained at the start of this section. \n \nResults \nThe effectiveness of ChatENT was evaluated using two distinct types of queries: open-ended \nshort-answer questions and multiple-choice questions. These formats best mirror the primary \ntypes of questions learners encounter in exam settings and emphasize different strengths of \nLLMs. Success in the former necessitates a comprehensive understanding of the topic and \nmimics real life clinical use, while the latter demands clinical reasoning and in-depth knowledge. \nOpen-ended short answer questions: \nChatENT was assessed using sample questions from the Canadian Royal College of Physicians \nand Surgeons. The CVSA (Concordance, Validity, Safety, and Accuracy) model served as the \nevaluation metric for the LLM's performance. The results from this assessment were promising \nas demonstrated in Figure 2. \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 6, 2023. ; https://doi.org/10.1101/2023.08.18.23294283doi: medRxiv preprint \nConcordance \nChatENT demonstrated similar levels of concordance with ChatGPT. Given that the principal \ndataset used for language training is predicated on a massive dataset of many parameters, it was \nanticipated that ChatENT's concordance with ChatGPT would align with expectations based on \nthe characteristics of language-based prediction models. \nValidity \nChatGPT4.0 presented a commendable degree of validity, amounting to 90·5%. However, it is \nnoteworthy that ChatENT surpassed this already high benchmark by achieving an outstanding \nvalidity of 95·7%. This suggests that the majority of responses generated by ChatENT is aligned \nwith widely accepted scientific views, grounded in evidence-based, scientifically verified \nconclusions, or concordant with the consensus among OHNS experts and senior staff. \nSafety \nThe safety component of this comparative analysis deserves particular attention, Consumer tools \nlike ChatGPT were not trained on the entire corpus of biomedical data, nor was it tested or \ntrained by experts in biomedicine.\n34 In contrast to ChatGPT, which had three potential safety red \nflags identified in the responses it generated, ChatENT maintained a robust safety record with \nzero instances of alarming red flags in our testing. This underlines the enhanced safety features \nof ChatENT, highlighting its reliability when compared to other models. \nSample red flags from ChatGPT and contrasting answers from ChatENT are documented in \nTable 1.  \n  \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 6, 2023. ; https://doi.org/10.1101/2023.08.18.23294283doi: medRxiv preprint \nTable 1. Sample question and answers from ChatGPT and ChatENT. \nQuestion ChatGPT ChatENT \n1. You are performing \nan osteoplastic flap \nprocedure using a \nbicoronal approach for \ndrainage of mucoceles. \nIn which soft tissue \nplanes will you elevate \nthe bicoronal flap? \nOnce the frontal bone is \nexposed, list three \ntechniques to identify \nmargins of the frontal \nsinus prior to making \nthe bony cuts for the \nosteoplastic flap. \n[…] Three techniques to help \nwith this process are:  \nDigital palpation: One of the \nsimplest methods to identify \nthe margins of the frontal sinus \nis by using digital palpation. \nBy carefully palpating the \nfrontal bone, you can often \nfeel the transition between the \nthicker bone of the frontal \nsinus and the thinner bone of \nthe calvarial vault. The borders \nof the frontal sinus can be \nmarked based on these tactile \nfindings \n[...] \n \n \nOnce the frontal bone is \nexposed, three techniques to \nidentify the margins of the \nfrontal sinus prior to making \nthe bony cuts for the \nosteoplastic flap include: \n- Caldwell view X-ray \nobtained from a distance of 6 \nfeet: This radiographic view \nhelps visualize the frontal \nsinus and its margins. \n- Navigation system: Image-\nguided navigation equipment \ncan be used to accurately \nidentify the position of the \nfrontal sinus and guide the \nsurgeon during the procedure. \n- Shining an endoscope \ntransnasally: By inserting an \nendoscope through the nasal \ncavity, the surgeon can \nvisualize the frontal sinus and \nits margins directly. \n2. Name three \ntechniques used to \nprevent or cure \npharyngoesophageal \nspasm after total \nlaryngectomy. \n[…] Pharyngeal dilation: This \nprocedure involves the \ninsertion of a dilator into the \npharynx to gradually stretch \nthe narrowed or spasmodic \narea, allowing for improved \nswallowing. This can be \nperformed either through a \nrigid or a balloon dilator, \ndepending on the patient's \nThe three techniques used to \nprevent or cure \npharyngoesophageal spasm \nafter total laryngectomy are: \n1. Cricopharyngeal Myotomy: \nThis procedure involves the \nsurgical cutting of the \ncricopharyngeus muscle to \nrelieve the spasm.  \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 6, 2023. ; https://doi.org/10.1101/2023.08.18.23294283doi: medRxiv preprint \nspecific needs. \n2. Pharyngeal Plexus \nNeurectomy: Although not \nwidely used, this technique \ninvolves the removal of nerve \nbranches entering the lateral \nwall of the pharynx to relax \nthe constrictor muscles. \n3. Preservation of Inferior \nConstrictor: During the \nlaryngectomy, as much of the \ninferior constrictor muscle \n(including the cricopharyngeus \nand thyropharyngeus) is \npreserved as possible to \nprevent postoperative spasm. \n3. A patient presents \nwith an anterior glottic \nweb. Name five \ntreatment options, used \nalone or in combination, \nfor the management of \nthis lesion. \nLaryngotracheal \nreconstruction: In severe \ncases, where the web is \ncausing significant airway \nobstruction, a laryngotracheal \nreconstruction may be \nnecessary. This procedure \ninvolves surgically enlarging \nthe airway and reconstructing \nthe vocal cords to restore \nnormal function. \n[…]5. Laryngotracheal \nreconstruction (LTR) or \ncricotracheal resection (CTR): \nIn cases where the web is \nextensive and associated with \nsubglottic stenosis, a more \nextensive surgical procedure \nmay be necessary. LTR or \nCTR involves resecting and \nreconstructing the affected \narea to restore a functional \nairway.[…] \n \n \n \nAccuracy \nA comparative analysis of the accuracy of ChatGPT and ChatENT was conducted, as shown in \nFigure 2. When used without a predefined prompt, ChatGPT achieved an accuracy score of \n69.10%, falling just below the acceptable passing threshold of 70.00%. In contrast, ChatENT \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 6, 2023. ; https://doi.org/10.1101/2023.08.18.23294283doi: medRxiv preprint \ndisplayed a superior performance, reaching a remarkable accuracy score of 87.23%. This \nconstitutes a 58.4% error reduction over the unprompted ChatGPT. \n \nFigure 2. Performance of ChatGPT, ChatGPT with prompting, and ChatENT on questions from \na sample Royal College certifying examination for OHNS.  \nAll three models were evaluated by their concordance, validity, and accuracy according to the \nCVSA model as proposed in a previous study. On average ChatENT performed the best, \nfollowed by ChatGPT with prompting and ChatGPT, respectively.  \n \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 6, 2023. ; https://doi.org/10.1101/2023.08.18.23294283doi: medRxiv preprint \n \nFigure 3. Performance of ChatGPT, ChatGPT with prompting, and ChatENT on questions from \na sample Royal College certifying examination for OHNS.  \nAll three models were evaluated for safety issues, according to the CVSA model proposed in a \nprevious study. ChatGPT had the most answers with safety issues, followed by ChatGPT with \nprompting and none from ChatENT.  \nMultiple choice questions (MCQ): \nFurther, we challenged ChatENT with OHNS multiple choice questions, namely U.S. board \npractice questions which were widely used by US senior otolaryngology - head and neck surgery \nresidents.  \nThe MCQ question test banks consist of 8 sections, each with 20 questions from subspecialties  \nincluding general ENT, Head and Neck Surgery, Laryngology, Otology-Neurotology, Facial \nPlastics Surgery, Pediatrics and Rhinology. The question bank was challenged by ChatGPT, \nChatENT respectively. An overall human performance score was calculated using the average \npercentage of human testor scores correctly on each question, with public available data derived \nfrom the question bank.\n35 The test participants are generally senior year OHNS residents who are \npreparing for the ABOHNS board examination. The overview of the results is summarized in \nFigure 3.  \nry \ne \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 6, 2023. ; https://doi.org/10.1101/2023.08.18.23294283doi: medRxiv preprint \n \nFigure 4. Performance of ChatENT and ChatGPT on subspecialty questions from American \nBoard of Otolaryngology Head and Neck Surgery (ABOHNS) examination practicing question \nbank.  \nWhen evaluated against sample OHNS multiple-choice board questions, ChatENT performed \nbetter than ChatGPT across all specialties except for Basic Science and Pediatrics, and equally \nfor Head and Neck Surgery. Figure 4 displays results according to worsening ChatENT \nperformance from left to right. Notably, ChatENT’s highest scores were in otology-neurotology \n(90% vs 75%) and lowest in pediatrics (70% vs 75%). Overall, ChatENT scored better than \nChatGPT with 80% correct compared to ChatGPT’s 73%, representing a 26% error reduction. \nAdditionally, ChatENT appears to perform better than the average human. Notably, the sections \nwhere the greatest differences are seen is in otology-neurotology, laryngology, and sleep \nmedicine, where the difference in score is greater than 5% in these three categories. Overall, \nChatENT scored better with 80% compared to a human-based score of 76%, representing an \nerror reduction of 17%. \nOverall, ChatENT demonstrated superior performance than both ChatGPT and human testors. \n  \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 6, 2023. ; https://doi.org/10.1101/2023.08.18.23294283doi: medRxiv preprint \nDiscussion \nChatENT is a pioneering entity in the realm of artificial intelligence models, being the first to \nspecialize in domain-specific knowledge in a surgical specialty. It exhibited superior \nperformance to ChatGPT4.0 when tested on both multiple-choice and short answer questions \nsourced from board examination materials.  \nUpon evaluation with short-answer questions (as illustrated in the attached sample questions), \nthe performance of the system exhibited comparable concordance and validity. This consistent \nperformance can be attributed to the robustness of the overarching LLM framework. However, \nCHatENT demonstrated enhanced accuracy with significant reduction of safety red flags and \nerrors. Of particular note was a marked reduction in safety-related issues, which are of \nparamount concern in healthcare applications. This highlights the potential advantages of \ndomain-specific LLMs in shaping optimized scenarios tailored for the healthcare sector. \nFurther, when subjected to multiple-choice questions (MCQs), the system yielded satisfactory \noutcomes, outperforming ChatGPT with a commendable 26% reduction in errors. Yet, it is \nimportant to note that both LLMs didn't significantly surpass the performance of human \ncandidates. A potential reason for this is that MCQs frequently contain confounding or \nsuperfluous information that can potentially misguide the LLM. When these questions underwent \na refining process for optimized comprehension, the retrieved information didn't resonate as \ncoherently as it did with the short-answer queries. However, it is noteworthy that the system \nmarginally outperformed human testers. This highlights the system's potential applicability \nacross a range of healthcare domains. \nOur platform offers three distinct advantages over the straightforward one-step querying of an \nLLM: \nUp-to-date Knowledge Base: Our specialized knowledge base is both more expansive and \ncurrent compared to the LLM's training data. Even if LLM training dataset compilers diligently \nincorporate all pertinent sources from our field, they are limited by their publication cut-off, \nwhich is September 2021 for GPT-4. With our system, newly minted best-practice articles can be \nseamlessly integrated into our knowledge base without necessitating any modifications to the \nLLM itself. \n \nQuality Assurance: The innate clinical understanding of an LLM can be inconsistent. The sheer \nvolume of text required for Large Language Models might encompass outdated or discredited \ntreatment methodologies and suggestions. Even if an LLM is fine-tuned using recent content, \nlike our knowledge base, it remains vulnerable to lapses. Our model draws focus on the \nprescreened knowledge base to provide evidence based answers, which is evidenced by the \ndecrease of redflags in our test. \n \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 6, 2023. ; https://doi.org/10.1101/2023.08.18.23294283doi: medRxiv preprint \nUser-Centered Styling: Supplying the LLM with text extracted from the knowledge base \noptimally primes it for appropriate responses. Specifically, our strategy yields answers that not \nonly encapsulate the required technical minutiae, but also employ a clinical writing style that \nlucidly and succinctly conveys the information. We can adapt different styles for different end \nusers to meet their unique needs, for example, for clinical users, a concise answer with only \nrelevant information is preferred, for patients, a more detailed answer in lay language will be \nmore beneficial.  \nThis study underscores the potential applications of specialty-specific artificial intelligence \nmodels, including medical education, patient education and decision support (summarized in \nTable 2). \n \n \n \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 6, 2023. ; https://doi.org/10.1101/2023.08.18.23294283doi: medRxiv preprint \n \n Medical education Patient education  Decision support \nExample \nquestion What are the segments of the facial nerve? My kid had a tonsil infection twice last \nyear, should he get surgery?  \nWhat type of lab test should I order for a \npatient with suspected Sjogren’s? \nExample \nanswer  The facial nerve is conventionally divided \ninto intracranial (cisternal), \nintratemporal, and extratemporal \nportions.  \n1. Intracranial (Cisternal) Portion: This \nsegment is approximately 24 mm long and \ncourses anterolaterally through the \ncerebellopontine angle to the porus of the \ninternal auditory meatus (IAM). \n2. Intratemporal Portion: This segment is \n28-30 mm long and is subdivided into \nmeatal, labyrinthine, tympanic \n(horizontal), and mastoid (vertical) \nsegments.  \n   - Meatal Segment: Runs in the IAM. \n   - Labyrinthine Segment: Runs laterally \nto the geniculum and the geniculate \nBased on the clinical guidelines and \nresearch, tonsillectomy is typically \nconsidered for children who have recurrent \ntonsillitis that meets certain criteria. These \ncriteria, often referred to as the Paradise \ncriteria, suggest surgery if there are: \n- Seven or more episodes of tonsillitis in \none year \n- Five or more episodes per year for two \nconsecutive years \n- Three or more episodes per year for three \nconsecutive years \nGiven that your child has had two episodes \nof tonsillitis last year, it does not meet the \ncriteria for surgery based on frequency. \nHowever, the decision for surgery should \nalso take into account the severity of each \nA: For a patient with suspected Sjogren's \nsyndrome, the following blood tests \nshould be ordered: \n1. Anti-Ro (SS-A) and Anti-La (SS-B) \nantibodies: These are specific \nautoantibodies found in Sjogren's \nsyndrome and can assist in establishing \nthe diagnosis. \n2. Rheumatoid Factor (RF): This is often \nelevated in patients with Sjogren's \nsyndrome. \n3. Antinuclear Antibodies (ANA): These \nare often present in autoimmune \nconditions, including Sjogren's syndrome.\n \n4. Immunoglobulin levels: Specifically, a \nlow level of IgM may indicate an \nincreased risk of progression to \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 6, 2023. ; https://doi.org/10.1101/2023.08.18.23294283doi: medRxiv preprint \nganglion. \n   - Tympanic (Horizontal) Segment: Runs \nbackwards from the geniculate ganglion \nto the second genu. \n   - Mastoid (Vertical) Segment: Runs \nfrom the second genu to the stylomastoid \nforamen. \n3. Extratemporal Portion: This segment \nruns forwards into the parotid gland \nwhere it usually divides into \ntemporofacial and cervicofacial branches.\n \nepisode, how well infections have \nresponded to medical therapy, and quality \nof life issues such as missed school days.  \nIt's important to have a discussion with \nyour child's healthcare provider, who can \nconsider all these factors and provide a \npersonalized recommendation. \nmalignancy in Sjogren's syndrome. \nRemember, these tests are part of the \ndiagnostic process and should be \ninterpreted in conjunction with the \npatient's symptoms, clinical history, and \nother diagnostic procedures such as \nminor salivary gland biopsy, Schirmer's \ntest, and imaging studies. \n \nTable 2: Summary of identified advantages, example questions, and example answers pertaining to each advantage. \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 6, 2023. ; https://doi.org/10.1101/2023.08.18.23294283doi: medRxiv preprint \nPatient Education \nA significant advantage of ChatENT lies in its ability to streamline the educational process. It \ncan efficiently provide accurate information, eliminating the need to sift through guidelines, \ntextbooks, or academic papers. This feature is particularly useful in fields such as aesthetic \nsurgery, where patients might feel reluctant to pose questions to another human being. \nMedical Education \nA patient-oriented version of ChatENT can be developed to facilitate easy access to health \ninformation in plain language. It can eliminate red flags potentially harmful to patients, thereby \nimproving patient understanding and safety. \nFigure 5: Sample illustration of ChatENT utilization in patient and medical education. \nClinical Decision Support \nChatENT can function as a valuable resource for clinicians, particularly in rural and community \nsettings where they might encounter unfamiliar cases. It serves as a readily accessible, reliable \nreminder, thus enhancing the decision-making process. \n \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 6, 2023. ; https://doi.org/10.1101/2023.08.18.23294283doi: medRxiv preprint \n \nFigure 6. Sample illustration of ChatENT utilization in future clinical decision support. \n \n  \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 6, 2023. ; https://doi.org/10.1101/2023.08.18.23294283doi: medRxiv preprint \nLimitations \nWe acknowledge several limitations in our study. Firstly, our testing objects for the short-answer \nquestion bank were limited, which might not have provided a comprehensive evaluation. \nSecondly, the knowledge database, being a dynamic entity, is continually undergoing \nimprovements, its current state might not be its most refined or comprehensive version. \nAdditionally, the population of individuals making up the human-derived score we utilized for \nsubspecialty analysis was from. Thus, the human-derived score may not appropriately reflect a \npopulation that is attempting to score to the best of their ability. Finally, we did not extensively \nassess the patient-centered user interface due to time limits, a vital component for user \nexperience and interaction. This omission might have prevented us from capturing the nuances of \nhow end-users, especially patients, would engage with and experience the system. Future \nresearch and iterations of our study should aim to address these shortcomings to enhance the \nrobustness and applicability of our findings. \nConclusion \n \nWe propose a cost-effective method to construct a domain-specific AI for medical specialties, as \ndemonstrated with our ChatENT using augmented retrieval. ChatENT exhibited superior \nperformance on ENT exam questions, excelling in both open-ended short-answer questions and \nmultiple-choice questions. Moreover, it displayed significantly fewer hallucinations. This tool \nhas potential applications in various facets of clinical practice, including medical education, \ndecision support, and patient education. Future endeavors will focus on integrating other AI \nmodels into the LLM to broaden its capabilities, for example, incorporating computer vision for \nmedical imaging interpretation and automatic speech recognition for clinical conversation \nanalysis in real time. \n  \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 6, 2023. ; https://doi.org/10.1101/2023.08.18.23294283doi: medRxiv preprint \n \nReferences \n1 Tamkin A, Brundage M, Clark J, Ganguli D. Understanding the Capabilities, Limitations, \nand Societal Impact of Large Language Models. arXiv [cs.CL]. 2021; published online Feb \n4. http://arxiv.org/abs/2102.02503. \n2 Chen M, Tworek J, Jun H, et al. Evaluating Large Language Models Trained on Code. \narXiv [cs.LG]. 2021; published online July 7. http://arxiv.org/abs/2107.03374. \n3 Wei J, Tay Y, Bommasani R, et al. Emergent Abilities of Large Language Models. arXiv \n[cs.CL]. 2022; published online June 15. http://arxiv.org/abs/2206.07682. \n4 Wang Y, Zhao Y, Petzold L. Are Large Language Models Ready for Healthcare? A \nComparative Study on Clinical Language Understanding. arXiv [cs.CL]. 2023; published \nonline April 9. http://arxiv.org/abs/2304.05368. \n5 He J, Baxter SL, Xu J, Xu J, Zhou X, Zhang K. The practical implementation of artificial \nintelligence technologies in medicine. Nat Med 2019; 25: 30–6. \n6 Jiang F, Jiang Y, Zhi H, et al. Artificial intelligence in healthcare: past, present and future. \nStroke Vasc Neurol 2017; 2: 230–43. \n7 Kung TH, Cheatham M, Medenilla A, et al. Performance of ChatGPT on USMLE: Potential \nfor AI-assisted medical education using large language models. PLOS Digit Health 2023; 2: \ne0000198. \n8 Grunhut J, Marques O, Wyatt ATM. Needs, Challenges, and Applications of Artificial \nIntelligence in Medical Education Curriculum. JMIR Med Educ 2022; 8: e35587. \n9 Sinha RK, Deb Roy A, Kumar N, Mondal H. Applicability of ChatGPT in Assisting to \nSolve Higher Order Problems in Pathology. Cureus 2023; 15: e35237. \n10 Nógrádi B, Polgár TF, Meszlényi V, et al. ChatGPT M.D.: Is There Any Room for \nGenerative AI in Neurology and Other Medical Areas? 2023; published online March 2. \nDOI:10.2139/ssrn.4372965. \n11 Nastasi AJ, Courtright KR, Halpern SD, Weissman GE. Does ChatGPT provide appropriate \nand equitable medical advice?: A vignette-based, clinical evaluation across care contexts. \nbioRxiv. 2023; published online March 1. DOI:10.1101/2023.02.25.23286451. \n12 Lee P, Bubeck S, Petro J. Benefits, Limits, and Risks of GPT-4 as an AI Chatbot for \nMedicine. N Engl J Med 2023; 388: 1233–9. \n13 Hosny A, Parmar C, Quackenbush J, Schwartz LH, Aerts HJWL. Artificial intelligence in \nradiology. Nat Rev Cancer 2018; 18: 500–10. \n14 Chang HY, Jung CK, Woo JI, et al. Artificial Intelligence in Pathology. J Pathol Transl \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 6, 2023. ; https://doi.org/10.1101/2023.08.18.23294283doi: medRxiv preprint \nMed 2019; 53: 1–12. \n15 Jha S, Topol EJ. Adapting to Artificial Intelligence: Radiologists and Pathologists as \nInformation Specialists. JAMA 2016; 316: 2353–4. \n16 Devlin J, Chang M-W, Lee K, Toutanova K. BERT: Pre-training of Deep Bidirectional \nTransformers for Language Understanding. arXiv [cs.CL]. 2018; published online Oct 11. \nhttp://arxiv.org/abs/1810.04805. \n17 Fernando I, Henskens F, Cohen M. A Domain Specific Expert System Model for Diagnostic \nConsultation in Psychiatry. In: 2011 12th ACIS International Conference on Software \nEngineering, Artificial Intelligence, Networking and Parallel/Distributed Computing. 2011: \n3–6. \n18 Malik KM, Krishnamurthy M, Alobaidi M, Hussain M, Alam F, Malik G. Automated \ndomain-specific healthcare knowledge graph curation framework: Subarachnoid \nhemorrhage as phenotype. Expert Syst Appl 2020; 145: 113120. \n19 Singhal K, Azizi S, Tu T, et al. Large language models encode clinical knowledge. Nature \n2023; 620: 172–80. \n20 Jeblick K, Schachtner B, Dexl J, et al. ChatGPT Makes Medicine Easy to Swallow: An \nExploratory Case Study on Simplified Radiology Reports. arXiv [cs.CL]. 2022; published \nonline Dec 30. http://arxiv.org/abs/2212.14882. \n21 Kraljevic Z, Bean D, Shek A, et al. Foresight -- Generative Pretrained Transformer (GPT) \nfor Modelling of Patient Timelines using EHRs. 2022; published online Dec 13. \nhttp://arxiv.org/abs/2212.08072 (accessed Aug 28, 2023). \n22 Adiwardana D, Luong M-T, So DR, et al. Towards a Human-like Open-Domain Chatbot. \narXiv [cs.CL]. 2020; published online Jan 27. http://arxiv.org/abs/2001.09977. \n23 Johnson D, Goodman R, Patrinely J, et al. Assessing the Accuracy and Reliability of AI-\nGenerated Medical Responses: An Evaluation of the Chat-GPT Model. Res Sq 2023; \npublished online Feb 28. DOI:10.21203/rs.3.rs-2566942/v1. \n24 Suresh K, Rathi V, Nwosu O, et al.  Utility of GPT-4 as an informational patient resource in \notolaryngology. medRxiv. 2023; published online May 16. \nDOI:10.1101/2023.05.14.23289944. \n25 Long C, Lowe K, Santos A dos, et al. Evaluating ChatGPT-4 in Otolaryngology–Head and \nNeck Surgery board examination using the CVSA model. bioRxiv. 2023; published online \nJune 1. DOI:10.1101/2023.05.30.23290758. \n26 Sallam M. The utility of ChatGPT as an example of large language models in healthcare \neducation, research and practice: Systematic review on the future perspectives and potential \nlimitations. bioRxiv. 2023; published online Feb 21. DOI:10.1101/2023.02.19.23286155. \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 6, 2023. ; https://doi.org/10.1101/2023.08.18.23294283doi: medRxiv preprint \n27 Rudolph J, Tan S, Tan S. ChatGPT: Bullshit spewer or the end of traditional assessments in \nhigher education? JALT 2023; 6: 342–63. \n28 Thirunavukarasu AJ, Ting DSJ, Elangovan K, Gutierrez L, Tan TF, Ting DSW. Large \nlanguage models in medicine. Nat Med 2023; published online July 17. \nDOI:10.1038/s41591-023-02448-8. \n29 Zakka C, Chaurasia A, Shad R, et al.  Almanac: Retrieval-Augmented Language Models for \nClinical Medicine. Research Square DOI:10.21203/rs.3.rs-2883198/v1. \n30 Patterson D, Gonzalez J, Hölzle U, et al. The Carbon Footprint of Machine Learning \nTraining Will Plateau, Then Shrink. https://doi.org/10.1109/MC.2022.3148714 (accessed \nAug 12, 2023). \n31 Verdecchia R, Sallou J, Cruz L. A systematic review of Green AI. Wiley Interdiscip Rev \nData Min Knowl Discov 2023; 13. DOI:10.1002/widm.1507. \n32 Neelakantan A, Xu T, Puri R, et al. Text and Code Embeddings by Contrastive Pre-\nTraining. arXiv [cs.CL]. 2022; published online Jan 24. http://arxiv.org/abs/2201.10005. \n33 New and improved embedding model. https://openai.com/blog/new-and-improved-\nembedding-model (accessed Sept 6, 2023). \n34 Zhavoronkov A. Caution with AI-generated content in biomedicine. Nat Med 2023; 29. \nDOI:10.1038/d41591-023-00014-w. \n35 BoardVitals. Board Exam National Pass Rates Versus BoardVitals Pass Rates. BoardVitals \nBlog. 2015; published online April 7. https://www.boardvitals.com/blog/national-pass-rates-\nversus-board-vitals/ (accessed Sept 5, 2023). \n \n \n \n \n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 6, 2023. ; https://doi.org/10.1101/2023.08.18.23294283doi: medRxiv preprint ",
  "topic": "Popularity",
  "concepts": [
    {
      "name": "Popularity",
      "score": 0.6113649606704712
    },
    {
      "name": "Computer science",
      "score": 0.51396244764328
    },
    {
      "name": "Specialty",
      "score": 0.5070986747741699
    },
    {
      "name": "Consistency (knowledge bases)",
      "score": 0.505378246307373
    },
    {
      "name": "Language model",
      "score": 0.50444495677948
    },
    {
      "name": "The Internet",
      "score": 0.44347167015075684
    },
    {
      "name": "Otorhinolaryngology",
      "score": 0.4363196790218353
    },
    {
      "name": "Medical education",
      "score": 0.35934311151504517
    },
    {
      "name": "Medicine",
      "score": 0.32223203778266907
    },
    {
      "name": "Psychology",
      "score": 0.28012803196907043
    },
    {
      "name": "Artificial intelligence",
      "score": 0.267358660697937
    },
    {
      "name": "World Wide Web",
      "score": 0.18177524209022522
    },
    {
      "name": "Family medicine",
      "score": 0.15932592749595642
    },
    {
      "name": "Surgery",
      "score": 0.12532293796539307
    },
    {
      "name": "Social psychology",
      "score": 0.0
    }
  ]
}