{
  "title": "Structured Language Modeling for Speech Recognition",
  "url": "https://openalex.org/W1644582922",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4293488235",
      "name": "Chelba, Ciprian",
      "affiliations": [
        "Johns Hopkins University"
      ]
    },
    {
      "id": null,
      "name": "Jelinek, Frederick",
      "affiliations": [
        "Johns Hopkins University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3021713638",
    "https://openalex.org/W2949237929",
    "https://openalex.org/W2160645305",
    "https://openalex.org/W1597533204"
  ],
  "abstract": "A new language model for speech recognition is presented. The model develops hidden hierarchical syntactic-like structure incrementally and uses it to extract meaningful information from the word history, thus complementing the locality of currently used trigram models. The structured language model (SLM) and its performance in a two-pass speech recognizer --- lattice decoding --- are presented. Experiments on the WSJ corpus show an improvement in both perplexity (PPL) and word error rate (WER) over conventional trigram models.",
  "full_text": "arXiv:cs/0001023v1  [cs.CL]  25 Jan 2000\nSTRUCTURED LANGUAGE MODELING FOR\nSPEECH RECOGNITION†\nCiprian Chelba and Frederick Jelinek\nAbstract\nA new language model for speech recognition is presented. Th e model develops hidden hierarchi-\ncal syntactic-like structure incrementally and uses it to e xtract meaningful information from the\nword history, thus complementing the locality of currently used trigram models. The structured\nlanguage model (SLM) and its performance in a two-pass speec h recognizer — lattice decoding\n— are presented. Experiments on the WSJ corpus show an improv ement in both perplexity\n(PPL) and word error rate (WER) over conventional trigram mo dels.\n1 Structured Language Model\nAn extensive presentation of the SLM can be found in [1]. The model a ssigns a probability\nP (W, T ) to every sentence W and its every possible binary parse T . The terminals of T are\nthe words of W with POStags, and the nodes of T are annotated with phrase headwords and\nnon-terminal labels.\nLet W be a sentence of length n words to which we have prepended <s> and appended </s>\nso that w0 =<s> and wn+1 =</s>. Let Wk be the word k-preﬁx w0 . . . w k of the sentence and\nWkTk the word-parse k-preﬁx . Figure 1 shows a word-parse k-preﬁx; h_0 .. h_{-m} are the\nexposed heads, each head being a pair(headword, non-terminal label), or (word, POStag) in the\ncase of a root-only tree.\nh_0 = (h_0.word, h_0.tag)h_{-1}h_{-m} = (<s>, SB)\n            \n(<s>, SB) .  .  .  .  .  . ...  (w_r, t_r)  ....   (w_p, t_p) (w_{p+1}, t_{p+1}) ........ (w_k, t_k) w_{k+1}.... </s>\nFigure 1: A word-parse k-preﬁx\n1.1 Probabilistic Model\nThe probability P (W, T ) of a word sequence W and a complete parse T can be broken into:\nP (W, T ) =\nn+1∏\nk=1\n[P (wk/W k− 1Tk− 1) · P (tk/W k− 1Tk− 1, w k) ·\nNk∏\ni=1\nP (pk\ni /W k− 1Tk− 1, w k, t k, p k\n1 . . . p k\ni− 1)]\nwhere:\n• Wk− 1Tk− 1 is the word-parse ( k − 1)-preﬁx\n†This work was funded by the NSF IRI-19618874 grant STIMULATE\n...............\nT'_0\nT_{-1} T_0<s> T'_{-1}<-T_{-2}\nh_{-1} h_0\nh'_{-1} = h_{-2}\nT'_{-m+1}<-<s>\nh'_0 = (h_{-1}.word, NTlabel)\nFigure 2: Result of adjoin-left under NTlabel\n............... T'_{-1}<-T_{-2} T_0\nh_0h_{-1}\n<s>\nT'_{-m+1}<-<s>\nh'_{-1}=h_{-2}\nT_{-1}\nh'_0 = (h_0.word, NTlabel)\nFigure 3: Result of adjoin-right under NTlabel\n• wk is the word predicted by WORD-PREDICTOR\n• tk is the tag assigned to wk by the TAGGER\n• Nk −1 is the number of operations the PARSER executes at sentence po sition k before passing\ncontrol to the WORD-PREDICTOR (the Nk-th operation at position k is the null transition);\nNk is a function of T\n• pk\ni denotes the i-th PARSER operation carried out at position k in the wo rd string; the\noperations performed by the PARSER are illustrated in Figures 2-3 a nd they ensure that all\npossible binary branching parses with all possible headword and non- terminal label assignments\nfor the w1 . . . w k word sequence can be generated. Our model is based on three pro babilities,\nestimated using deleted interpolation (see [3]), parameterized as fo llows:\nP (wk/W k− 1Tk− 1) = P (wk/h 0, h − 1) (1)\nP (tk/w k, W k− 1Tk− 1) = P (tk/w k, h 0.tag, h − 1.tag ) (2)\nP (pk\ni /W kTk) = P (pk\ni /h 0, h − 1) (3)\nIt is worth noting that if the binary branching structure developed by the parser were always\nright-branching and we mapped the POStag and non-terminal label vocabularies to a single\ntype then our model would be equivalent to a trigram language model.\nSince the number of parses for a given word preﬁx Wk grows exponentially with k, |{Tk}| ∼\nO(2k), the state space of our model is huge even for relatively short se ntences so we had to use\na search strategy that prunes it. Our choice was a synchronous m ulti-stack search algorithm\nwhich is very similar to a beam search.\nThe probability assignment for the word at position k + 1 in the input sentence is made using:\nP (wk+1/W k) =\n∑\nTk∈ Sk\nP (wk+1/W kTk) · [ P (WkTk)/\n∑\nTk∈ Sk\nP (WkTk) ] (4)\nwhich ensures a proper probability over strings W ∗, where Sk is the set of all parses present in\nour stacks at the current stage k. An N-best EM variant is employed to reestimate the model\nparameters such that the PPL on training data is decreased — the lik elihood of the training\ndata under our model is increased. The reduction in PPL is shown exp erimentally to carry over\nto the test data.\n2 A∗ Decoder for Lattices\nThe speech recognition lattice is an intermediate format in which the hypotheses produced by\nthe ﬁrst pass recognizer are stored. For each utterance we sav e a directed acyclic graph in which\nthe nodes are a subset of the language model states in the composite hidden M arkov model and\nthe arcs — links — are labeled with words. Typically, the ﬁrst pass acoustic/language model\nscores associated with each link in the lattice are saved and the node s contain time alignment\ninformation.\nThere are a couple of reasons that make A∗ [4] appealing for lattice decoding using the SLM:\n• the algorithm operates with whole preﬁxes, making it ideal for incorp orating language models\nwhose memory is the entire sentence preﬁx;\n• a reasonably good lookahead function and an eﬃcient way to calculat e it using dynamic\nprogramming techniques are both readily available using the n-gram la nguage model.\n2.1 A∗ Algorithm\nLet a set of hypotheses L = {h : x1, . . . , x n}, x i ∈ W ∗ ∀ i be organized as a preﬁx tree.\nWe wish to obtain the maximum scoring hypothesis under the scoring f unction f : W∗ → ℜ :\nh∗ = arg max h∈ L f(h) without scoring all the hypotheses in L, if possible with a minimal\ncomputational eﬀort. The A∗ algorithm operates with preﬁxes and suﬃxes of hypotheses —\npaths — in the set L; we will denote preﬁxes — anchored at the root of the tree — with x\nand suﬃxes — anchored at a leaf — with y. A complete hypothesis h can be regarded as the\nconcatenation of a x preﬁx and a y suﬃx: h = x.y .\nTo be able to pursue the most promising path, the algorithm needs to evaluate all the possible\nsuﬃxes that are allowed in L for a given preﬁx x = w1, . . . , w p — see Figure 4. Let CL(x) be\nthe set of suﬃxes allowed by the tree for a preﬁx x and assume we have an overestimate for\nthe f(x.y ) score of any complete hypothesis x.y : g(x.y ) .= f(x) + h(y|x) ≥ f(x.y ). Imposing\nthat h(y|x) = 0 for empty y, we have g(x) = f(x), ∀ complete x ∈ L that is, the overestimate\nbecomes exact for complete hypotheses h ∈ L. Let the A∗ ranking function gL(x) be:\nCL\nw\nw\nw\n1\n2\np\n(x)\nFigure 4: Preﬁx Tree Organization of a Set of Hypotheses L\ngL(x) .= max\ny∈ CL(x)\ng(x.y ) = f(x) + hL(x), where (5)\nhL(x) .= max\ny∈ CL(x)\nh(y|x) (6)\ngL(x) is an overestimate for the f(·) score of any complete hypothesis that has the preﬁx x;\nthe overestimate becomes exact for complete hypotheses. The A∗ algorithm uses a potentially\ninﬁnite stack in which preﬁxes x are ordered in decreasing order of the A∗ ranking function\ngL(x);at each extension step the top-most preﬁx x = w1, . . . , w p is popped from the stack, ex-\npanded with all possible one-symbol continuations of x in L and then all the resulting expanded\npreﬁxes — among which there may be complete hypotheses as well — a re inserted back into\nthe stack. The stopping condition is: whenever the popped hypoth esis is a complete one, retain\nit as the overall best hypothesis h∗.\n2.2 A∗ Lattice Rescoring\nA speech recognition lattice can be conceptually organized as a preﬁ x tree of paths. When\nrescoring the lattice using a diﬀerent language model than the one t hat was used in the ﬁrst\npass, we seek to ﬁnd the complete path p = l0 . . . l n maximizing:\nf(p) =\nn∑\ni=0\n[logPAM (li) + LMweight · logPLM (w(li)|w(l0) . . . w (li− 1)) − logPIP ] (7)\nwhere:\n• logPAM (li) is the acoustic model log-likelihood assigned to link li;\n• logPLM (w(li)|w(l0) . . . w (li− 1)) is the language model log-probability assigned to link li given\nthe previous links on the partial path l0 . . . l i;\n• LMweight > 0 is a constant weight which multiplies the language model score of a link ; its\ntheoretical justiﬁcation is unclear but experiments show its usefu lness;\n• logPIP > 0 is the “insertion penalty”; again, its theoretical justiﬁcation is un clear but exper-\niments show its usefulness.\nTo be able to apply the A∗ algorithm we need to ﬁnd an appropriate stack entry scoring funct ion\ngL(x) where x is a partial path and L is the set of complete paths in the lattice. Going back\nto the deﬁnition (5) of gL(·) we need an overestimate g(x.y ) = f(x) + h(y|x) ≥ f(x.y ) for all\npossible y = lk . . . l n complete continuations of x allowed by the lattice. We propose to use the\nheuristic:\nh(y|x) =\nn∑\ni=k\n[logPAM (li) + LMweight · (logPNG (li) + logPCOMP ) − logPIP ]\n+LMweight · logPF INAL · δ(k < n ) (8)\nA simple calculation shows that if logPLM (li) satisﬁes: logPNG (li)+ logPCOMP ≥ logPLM (li), ∀li\nthen gL(x) = f(x) + maxy∈ CL(x)h(y|x) is a an appropriate choice for the A∗ stack entry scoring\nfunction. In practice one cannot maintain a potentially inﬁnite stack . The logPCOMP and\nlogPF INAL parameters controlling the quality of the overstimate in (8) are adj usted empirically.\nA more detailed description of this procedure is precluded by the leng th limit on the article.\n3 Experiments\nAs a ﬁrst step we evaluated the perplexity performance of the SLM relative to that of a baseline\ndeleted interpolation 3-gram model trained under the same conditio ns: training data size 5Mwds\n(section 89 of WSJ0), vocabulary size 65kwds, closed over test se t. We have linearly interpolated\nthe SLM with the 3-gram model: P (·) = λ · P3gram (·) + (1 − λ) · PSLM (·) showing a 16%\nrelative reduction in perplexity; the interpolation weight was determ ined on a held-out set\nto be λ = 0 . 4. A second batch of experiments evaluated the performance of t he SLM for\nTrigram + SLM\nλ 0.0 0.4 1.0\nPPL 116 109 130\nLattice Trigram + SLM\nWER 11.5 9.6 10.6\nTable 1: Test Set Perplexity and Word Error Rate Results\ntrigram lattice decoding 1. The results are presented in Table 1. The SLM achieved an absolute\nimprovement in WER of 1% (10% relative) over the lattice 3-gram base line; the improvement is\nstatistically signiﬁcant at the 0.0008 level according to a sign test. A s a by-product, the WER\nperformance of the structured language model on 10-best list re scoring was 9.9%.\n4 Experiments: ERRATA\nWe repeated the WSJ lattice rescoring experiments reported in [2] in a standard setup. We\nchose to work on the DARPA’93 evaluation HUB1 test set — 213 utter ances, 3446 words. The\n20kwds open vocabulary and baseline 3-gram model are the standa rd ones provided by NIST.\nAs a ﬁrst step we evaluated the perplexity performance of the SLM relative to that of\na deleted interpolation 3-gram model trained under the same condit ions: training data size\n20Mwds (a subset of the training data used for the baseline 3-gram model), standard HUB1\nopen vocabulary of size 20kwds; both the training data and the voc abulary were re-tokenized\nsuch that they conform to the Upenn Treebank tokenization. We h ave linearly interpolated the\nSLM with the above 3-gram model:\nP (·) = λ · P3gram (·) + (1 − λ) · PSLM (·)\nshowing a 10% relative reduction over the perplexity of the 3-gram m odel. The results are\n1The lattices were generated using a language model trained on 45Mwds and using a 5kwds vocabulary\nclosed over the test data.\npresented in Table 2. The SLM parameter reestimation procedure 2 reduces the PPL by 5% (\n2% after interpolation with the 3-gram model). The main reduction in P PL comes however from\nthe interpolation with the 3-gram model showing that although over lapping, the two models\nsuccessfully complement each other. The interpolation weight was d etermined on a held-out\nset to be λ = 0 . 4. Both language models operate in the UPenn Treebank text token ization.\nTrigram(20Mwds) + SLM\nλ 0.0 0.4 1.0\nPPL, initial SLM, iteration 0 152 136 148\nPPL, reestimated SLM, iteration 1 144 133 148\nTable 2: Test Set Perplexity Results\nA second batch of experiments evaluated the performance of the SLM for 3-gram 3 lattice\ndecoding. The lattices were generated using the standard baseline 3-gram language model\ntrained on 40Mwds and using the standard 20kwds open vocabulary . The best achievable\nWER on these lattices was measured to be 3.3%, leaving a large margin f or improvement over\nthe 13.7% baseline WER.\nFor the lattice rescoring experiments we have adjusted the opera tion of the SLM such that\nit assigns probability to word sequences in the CSR tokenization and t hus the interpolation\nbetween the SLM and the baseline 3-gram model becomes valid. The r esults are presented in\nTable 3. The SLM achieved an absolute improvement in WER of 0.7% (5% r elative) over the\nbaseline despite the fact that it used half the amount of training dat a used by the baseline\n3-gram model. Training the SLM does not yield an improvement in WER wh en interpolating\nwith the 3-gram model, although it improves the performance of the SLM by itself.\nLattice Trigram(40Mwds) + SLM\nλ 0.0 0.4 1.0\nWER, initial SLM, iteration 0 14.4 13.0 13.7\nWER, reestimated SLM, iteration 1 14.3 13.2 13.7\nTable 3: Test Set Word Error Rate Results\n5 Acknowledgements\nThe authors would like to thank to Sanjeev Khudanpur for his insight ful suggestions. Also\nthanks to Bill Byrne for making available the WSJ lattices, Vaibhava Go el for making available\n2Due to the fact that the parameter reestimation procedure for the SLM is computationally expensive we\nran only a single iteration\n3In the previous experiments reported on WSJ we have accidentally used bigram lattices\nthe N-best decoder, Adwait Ratnaparkhi for making available his ma ximum entropy parser, and\nVaibhava Goel, Harriet Nock and Murat Saraclar for useful discuss ions about lattice rescoring.\nSpecial thanks to Michael Riley and Murat Saraclar for help in genera ting the WSJ lattices\nused in the revised experiments.\nReferences\n[1] C. CHELBA and F. JELINEK. Exploiting syntactic structure for language modeling. InProceed-\nings of COLING-ACL , volume 1, pages 225–231. Montreal, Canada, 1998.\n[2] C. CHELBA and F. JELINEK. Structured language modeling for speech recognition. InProceed-\nings of NLDB99 . Klagenfurt, Austria, 1999.\n[3] F. JELINEK and R. MERCER. Interpolated estimation of markov source parameters from sparse\ndata. In E. Gelsema and L. Kanal, editors,Pattern Recognition in Practice , pages 381–397. 1980.\n[4] N. NILSSON. Problem Solving Methods in Artiﬁcial Intelligence , pages 266–278. McGraw-Hill,\nNew York, 1971.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.639591634273529
    },
    {
      "name": "Speech recognition",
      "score": 0.5423605442047119
    },
    {
      "name": "Natural language processing",
      "score": 0.45377805829048157
    },
    {
      "name": "Linguistics",
      "score": 0.35002660751342773
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I145311948",
      "name": "Johns Hopkins University",
      "country": "US"
    }
  ],
  "cited_by": 24
}