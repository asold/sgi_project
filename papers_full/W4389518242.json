{
  "title": "Identifying and Adapting Transformer-Components Responsible for Gender Bias in an English Language Model",
  "url": "https://openalex.org/W4389518242",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5093104446",
      "name": "Abhijith Chintam",
      "affiliations": [
        "University of Amsterdam"
      ]
    },
    {
      "id": "https://openalex.org/A5093104447",
      "name": "Rahel Beloch",
      "affiliations": [
        "University of Amsterdam"
      ]
    },
    {
      "id": "https://openalex.org/A5007928903",
      "name": "Willem Zuidema",
      "affiliations": [
        "University of Amsterdam"
      ]
    },
    {
      "id": "https://openalex.org/A5101459729",
      "name": "Michael Hanna",
      "affiliations": [
        "University of Amsterdam"
      ]
    },
    {
      "id": "https://openalex.org/A5058013252",
      "name": "Oskar van der Wal",
      "affiliations": [
        "University of Amsterdam"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4389520380",
    "https://openalex.org/W4287889471",
    "https://openalex.org/W2893425640",
    "https://openalex.org/W4310279401",
    "https://openalex.org/W2996728628",
    "https://openalex.org/W4287889476",
    "https://openalex.org/W4384918485",
    "https://openalex.org/W4315881234",
    "https://openalex.org/W3037831233",
    "https://openalex.org/W2963078909",
    "https://openalex.org/W2950866572",
    "https://openalex.org/W4281621415",
    "https://openalex.org/W3035241006",
    "https://openalex.org/W4206292552",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W3197577761",
    "https://openalex.org/W4385572928",
    "https://openalex.org/W4308023630",
    "https://openalex.org/W2511234952",
    "https://openalex.org/W3172415559",
    "https://openalex.org/W3155744586",
    "https://openalex.org/W4378498576",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W2963526187",
    "https://openalex.org/W4306313145",
    "https://openalex.org/W4287124808",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4404783771",
    "https://openalex.org/W3121505931",
    "https://openalex.org/W4385468994",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W2949227999",
    "https://openalex.org/W4287649558",
    "https://openalex.org/W4287116904",
    "https://openalex.org/W3176477796",
    "https://openalex.org/W3105882417",
    "https://openalex.org/W4200629913",
    "https://openalex.org/W2963457723",
    "https://openalex.org/W3104142662",
    "https://openalex.org/W3134354193",
    "https://openalex.org/W4379925099",
    "https://openalex.org/W3159900299",
    "https://openalex.org/W4385571122",
    "https://openalex.org/W4386566794",
    "https://openalex.org/W4367628394",
    "https://openalex.org/W2963503967",
    "https://openalex.org/W4281657280",
    "https://openalex.org/W3106325613",
    "https://openalex.org/W2922523190",
    "https://openalex.org/W4285199616",
    "https://openalex.org/W4362655426",
    "https://openalex.org/W2963116854",
    "https://openalex.org/W4287110638",
    "https://openalex.org/W4367694478",
    "https://openalex.org/W2963612262",
    "https://openalex.org/W2950018712",
    "https://openalex.org/W2963828549",
    "https://openalex.org/W4287887133",
    "https://openalex.org/W2768282280",
    "https://openalex.org/W2970211217",
    "https://openalex.org/W2972668795",
    "https://openalex.org/W3206487987",
    "https://openalex.org/W4285273243"
  ],
  "abstract": "Language models (LMs) exhibit and amplify many types of undesirable biases learned from the training data, including gender bias. However, we lack tools for effectively and efficiently changing this behavior without hurting general language modeling performance. In this paper, we study three methods for identifying causal relations between LM components and particular output: causal mediation analysis, automated circuit discovery and our novel, efficient method called DiffMask+ based on differential masking. We apply the methods to GPT-2 small and the problem of gender bias, and use the discovered sets of components to perform parameter-efficient fine-tuning for bias mitigation. Our results show significant overlap in the identified components (despite huge differences in the computational requirements of the methods) as well as success in mitigating gender bias, with less damage to general language modeling compared to full model fine-tuning. However, our work also underscores the difficulty of defining and measuring bias, and the sensitivity of causal discovery procedures to dataset choice. We hope our work can contribute to more attention for dataset development, and lead to more effective mitigation strategies for other types of bias.",
  "full_text": "Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP, pages 379–394\nDecember 7, 2023. ©2023 Association for Computational Linguistics\n379\nIdentifying and Adapting Transformer-Components Responsible for\nGender Bias in an English Language Model\nAbhijith Chintam12, Rahel Beloch1\n1Master AI, University of Amsterdam, 2Pegasystems, Amsterdam, The Netherlands\narchintam@gmail.com, mail@rahelbeloch.de\nWillem Zuidema, Michael Hanna∗, Oskar van der Wal∗\nInstitute for Logic, Language & Computation, University of Amsterdam\n{w.h.zuidema, m.w.hanna, o.d.vanderwal}@uva.nl\nAbstract\nLanguage models (LMs) exhibit and amplify\nmany types of undesirable biases learned from\nthe training data, including gender bias. How-\never, we lack tools for effectively and efficiently\nchanging this behavior without hurting general\nlanguage modeling performance. In this pa-\nper, we study three methods for identifying\ncausal relations between LM components and\nparticular output: causal mediation analysis,\nautomated circuit discovery and our novel, ef-\nficient method called DiffMask+ based on dif-\nferential masking. We apply the methods to\nGPT-2 small and the problem of gender bias,\nand use the discovered sets of components to\nperform parameter-efficient fine-tuning for bias\nmitigation. Our results show significant over-\nlap in the identified components (despite huge\ndifferences in the computational requirements\nof the methods) as well as success in mitigat-\ning gender bias, with less damage to general\nlanguage modeling compared to full model fine-\ntuning. However, our work also underscores\nthe difficulty of defining and measuring bias,\nand the sensitivity of causal discovery proce-\ndures to dataset choice. We hope our work can\ncontribute to more attention for dataset devel-\nopment, and lead to more effective mitigation\nstrategies for other types of bias.\n1 Introduction\nModern neural language models exhibit social bi-\nases, such as biases based on gender, religion, eth-\nnicity and other protected attributes. These bi-\nases may lead to real harms when used in down-\nstream applications (e.g. Hovy and Spruit, 2016;\nWeidinger et al., 2021). Detecting and mitigating\nbiases in language models has therefore become an\nimportant area of research.\nEarly detection methods relied on lists of words\nto measure associations with e.g., specific genders\n(e.g. Caliskan et al., 2017). Most current detection\n∗Shared senior authorship.\nmethods work with curated sets of sentence pairs\nor triplets, and measure differences in sentence\nprobabilities or anaphora resolution probabilities\n(e.g. May et al., 2019; Nadeem et al., 2021; Nangia\net al., 2020; Basta et al., 2019). Proposed miti-\ngation strategies include targeted changes to the\ntraining data (e.g., CDA; Lu et al., 2020), train-\ning procedure (e.g., adversarial learning; Zhang\net al., 2018), model parameters (e.g., INLP; Ravfo-\ngel et al., 2020), or language generation procedure\n(e.g., “self-debiasing”; Schick et al., 2021).\nDespite this work, we still lack a proper under-\nstanding of how to best measure biases (how do\nwe guarantee the representativeness for real-world\nharm of a set of sentence pairs, or of a linguistic\nphenomenon such as anaphora resolution?), how\nbiases are implemented in the language model inter-\nnals (is there a unified locus, or is, e.g., gender bias\nthe aggregate effect of many independent model\ndecisions?), and what techniques are effective at\nreducing undesirable downstream behavior (e.g., is\ndata curation more or less effective than filtering\noutput? Is intervening in the model internals feasi-\nble?). Empirically, success in detecting and miti-\ngating biases depends on many factors, including\nthe choice of embeddings, training regimes, data\nsets and model choices (Blodgett et al., 2020, 2021;\nTalat et al., 2022; Delobelle et al., 2022; Barrett\net al., 2019; Van Der Wal et al., 2022).\nThe “black-box” nature of LMs makes it diffi-\ncult to identify and interpret how bias manifests\nand propagates in them, especially relying solely\non correlational methods. The starting point for\nthe current paper is the intuition that if, instead, it\nwere possible to find causal relationships between\nthe model’s internal representations and its down-\nstream bias, we could more effectively measure\nand intervene on these undesirable behaviors.\nWe therefore turn to a recent series of papers on\ninterpretability methods that focus on causal dis-\ncovery. In Section 2 we discuss three such methods,\n380\nof which we adapt one (DiffMask) for our needs in\nSection 3. Our new method is more efficient than\nother causal methods, which is especially relevant\nwhen applied to large language models (LLMs). In\nSection 3 we also report results from these three\nmethods when applied to GPT2-small and the prob-\nlem of gender bias, and find that they discover\nlargely overlapping sets of components, despite\nhuge differences in computation requirements. In\nSection 4 we use the identified components to adapt\nGPT-2 small, using parameter-efficient fine-tuning\nprocedures. We demonstrate how gender bias in\nLMs can be reduced with minimal effect to their\nlanguage modelling performance by making tar-\ngeted interventions to their components. However,\nwe also recognize the limitations of operational-\nizing gender bias as we do, using minimal pairs\nof contrasting sentences—which simplify gender\nas a binary construct and may not work so well\nfor other languages than English—and call for fu-\nture research to develop reliable and validated bias\nmeasures (see van der Wal et al., 2023).\n2 Related Work\nWhere and how LMs implement output behaviors—\nfrom high-level phenomena like gender stereotypes,\nto lower-level ones like subject-verb agreement—is\nan active field of study. In providing an overview of\nrelated work, we focus on causal methods for locat-\ning mechanisms in section 2.1, as non-causal meth-\nods can yield misleading conclusions (Ravichander\net al., 2021; Elazar et al., 2021). Further, we review\nprevious work on targeted changes to Language\nmodels and their behavior in section 2.2\n2.1 Locating Mechanisms in Language\nModels\nCausal methods study model processing by inter-\nvening in (altering) model processing, and observ-\ning the changes in model behavior caused by these\ninterventions. They aim to address the shortcom-\nings in observational methods by ensuring a causal\nlink between mechanisms found in model internals,\nand model behavior.\nMany such techniques determine which repre-\nsentations or components are important to model\nprocessing by ablating them. Ablations can range\nfrom zeroing out neurons (Lakretz et al., 2019;\nMohebbi et al., 2023), to replacing them with a\nbaseline (De Cao et al., 2021a; Bau et al., 2018), or\nreplacing them with another example’s activation\n(Vig et al., 2020; Geiger et al., 2021). All of these\ntechniques return unstructured sets of important\ncomponents without specifying their interaction.\nIn recent years, the circuits abstraction of trans-\nformer models (Elhage et al., 2021) has become\npopular. This framework views transformer mod-\nels as computational graphs, and aims to find sub-\ngraphs responsible for certain tasks. This technique\nhas been used to find circuits for indirect object de-\ntection and the greater-than operation in GPT-2\n(Wang et al., 2023; Hanna et al., 2023), as well as\nto study larger models (Lieberum et al., 2023); it\nhas also been automated (Conmy et al., 2023).\nNote that although causal methods can provide\na higher degree of confidence in localizing mecha-\nnisms, they are not foolproof. For example, Meng\net al. (2023) propose causal tracing, a method for\nlocating fact storage in LMs; they then edit GPT-2\nXL’s factual knowledge by performing edits at rel-\nevant locations. However, recent work has showed\nthat although edits may be successful, the local-\nization found by causal tracing is not predictive of\nedit success (Hase et al., 2023). So, even causal\nlocalizations should be assessed thoroughly.\n2.2 Targeted Changes to Language Models\nand Their Behavior\nOne way to mitigate bias in LMs is to change their\nparameters or internal representations; however,\nmaking large changes can be computationally ex-\npensive and have unintended side-effects on model\nbehavior. Past work has studied how to make tar-\ngeted changes to LMs that avoid these pitfalls. We\nonly discuss works on intervening in the model’s\nrepresentations and parameter-efficient fine-tuning\non curated datasets, but other bias mitigation strate-\ngies exist as well (see e.g., Meade et al., 2022).\nModel Interventions One line of research fo-\ncuses on removing undesirable concepts from a\nLM’s representations directly. Early methods like\nhard-debias based on principal component analy-\nsis (Bolukbasi et al., 2016) and iterated null-space\nprojection (INLP, Ravfogel et al., 2020) identify\nand remove linear representations of gender (bias)\nfrom embedding spaces; while others make tar-\ngeted changes to the activations of LMs (De Cao\net al., 2021b; Belrose et al., 2023) or edit the com-\nponents directly (Meng et al., 2022, 2023).\nAltering activations at run-time is one promising\nway to mitigate (gender) bias in LMs. LEACE\n(Belrose et al., 2023), for example, convincingly\n381\nremoves linearly-encoded gender information from\nactivations. Similarly, De Cao et al. (2021b) use an\napproach called differentiable masking (DiffMask)\nto identify small neuron subsets responsible for\nbias and intervene on them for reducing bias.\nHowever, a downside of these activation-altering\nmethods is that they require an intervention on the\nactivations at each inference step. Moreover, it is\nnot obvious which model activations we should run\nthese on; for instance, it is unlikely that we want to\nremove gender information from every input token.\nParameter-Efficient Fine-tuning Another ap-\nproach that avoids some of the pitfalls of chang-\ning the LM’s representations directly, is to fine-\ntune on a carefully constructed dataset. Previ-\nous work has shown the importance of consider-\ning the training data in understanding the biases\nlearned by LMs (e.g., Zhao et al., 2018; Zmigrod\net al., 2019; Bordia and Bowman, 2019; Lu et al.,\n2020; Bender et al., 2021; Sellam et al., 2022; Van\nDer Wal et al., 2022; Biderman et al., 2023). Given\nthis, fine-tuning on curated datasets is a promising\nstrategy for mitigating gender bias in LMs (So-\nlaiman and Dennison, 2021; Levy et al., 2021; Gira\net al., 2022; Kirtane and Anand, 2022). Falling\nwithin this paradigm is parameter-efficient fine-\ntuning, where only some of the model parame-\nters are updated—this may not only be compu-\ntationally more efficient, but even yield better re-\nsults (Lauscher et al., 2021; Gira et al., 2022; Xie\nand Lukasiewicz, 2023).\nOur work is most similar to Gira et al. (2022),\nwho also use parameter-efficient fine-tuning for\ndebiasing GPT-2 small. However, we study the ef-\nfect of fine-tuning individual attention heads, while\nthey focus on embedding layers, LayerNorm pa-\nrameters, adding linear input/output transformation\nparameters, and a combination thereof. Moreover,\nGira et al. do not adhere to any specific strategy\nwhen selecting the components to fine-tune. In con-\ntrast, our method provides a principled approach to\nidentify the components that are causally important\nfor the task at hand and then fine-tune them.\nXie and Lukasiewicz’s (2023) work is also re-\nlated to ours. They verify the effectiveness of\nparameter-efficient bias mitigation techniques like\nadapter tuning (Houlsby et al., 2019) and prefix\ntuning (Li and Liang, 2021) on various types of\nLMs and biases. These methods introduce extra\ntuneable parameters instead of directly tuning the\nmodel parameters themselves.\nOur approach could mitigate gender bias to an\nextent with minimal degradation in language mod-\nelling performance, similar to the results of Xie\nand Lukasiewicz (2023) and Gira et al. (2022).\nHowever, making a direct comparison is challeng-\ning due to differences in evaluation criteria and\nemployed datasets. Gira et al. (2022) exclusively\nassess their method on StereoSet (Nadeem et al.,\n2021), whereas we have evaluated our approach on\nmultiple benchmarks, as discussed in Section 4.2.\nXie and Lukasiewicz (2023) evaluate their fine-\ntuning methods using similar benchmarks as ours,\nbut they employ the older CrowS-Pairs (Nangia\net al., 2020) dataset for stereotype score and Wiki-\nText2 (Merity et al., 2016) for perplexity. We use a\nnewer, improved version of CrowS-Pairs (Névéol\net al., 2022) and the much larger WikiText-103\n(Merity et al., 2016) instead.\n3 Locating Gender Bias\nIn this section, we investigate the question: where\nin a given LM is gender bias introduced? We\nstudy this in GPT-2 small (Radford et al., 2019),\nan English-language, auto-regressive pre-trained\ntransformer LM.1 Its small size—12 transformer\nlayers, with 12 attention heads and 1 multi-layer\nperceptron (MLP) each—makes it a good object of\nclose studies like we perform. We seek to identify\nthe subset of the 144 attention heads that introduce\ngender bias into the last position of GPT-2’s input,\nwhere GPT-2 produces next-token predictions. We\nidentify these heads in the context of inputs that\nlead to gender-biased next-tokens from GPT-2.\nThis study thus focuses on attention heads.\nThough prior work has emphasized the role of\nMLPs in gender bias and memorization (Vig et al.,\n2020; Geva et al., 2022; Meng et al., 2023), we\nargue that attention heads are also an interesting\nsubject of analysis. Unless the final word of the\ninput contains gender information that causes the\nproduction of biased next-tokens, this information\nmust be introduced from other positions via atten-\ntion heads.\nTo determine where GPT-2 small introduces gen-\nder bias into its output, we use three methods:\ncausal mediation analysis (CMA), automated cir-\ncuit discovery, and our own novel method that com-\nbines the first approach with differential masking.\nWe then compare the results of these three methods.\n1The code for our experiments can be found here: https:\n//github.com/iabhijith/bias-causal-analysis\n382\n3.1 Methodology\nAll methods we use rely on a core technique as\noutlined in Vig et al. (2020): swapping model com-\nponent activations during a forward pass on one\ninput, with activations taken from the model when\nrun on another input which induces an opposite be-\nhaviour in the model. For this purpose, we use the\nProfessions dataset from Vig et al. (2020), which\ncontains templated sentences designed to elicit gen-\nder bias. The sentences in the dataset take the form\n“The {profession} said that”. GPT-2’s continuations\non these sentences tend to be stereotypical—if the\nprofession is nurse, GPT-2 outputs she, while if it\nis doctor, GPT-2 outputs he.\nFor each sentence in the dataset we generate a\ncorresponding counterfactual sentence with the pro-\nfession word replaced by anti-stereotypical gender-\nspecific word. If the normal sentence’s profession\nis female-stereotyped, its corresponding counter-\nfactual sentence is “The man said that”; for male-\nstereotyped professions, the counterfactual con-\ntains woman. These sentences are designed to max-\nimize the change in model behavior with respect\nto the predicted pronoun; this makes it easier to\nidentify important components. The dataset con-\ntains sentences generated from 17 templates and\n299 professions resulting in 5083 sentences in total.\nFor all methods that follow, we intervene on the\nlast position of the sentence.\n3.1.1 Causal Mediation Analysis\nVig et al. (2020) were the first to use CMA (Pearl,\n2014) to locate gender bias in GPT-2; we adopt\ntheir methods as a baseline. CMA relies on a sim-\nple hypothesis: if a component is important to the\nmodel’s behavior on a task, swapping its output\nactivation with another will change model behav-\nior. More formally, let x and ˜ xbe normal and\ncounterfactual inputs respectively, and leti be the\nindex of the component (attention head or MLP)\nunder investigation. We first run the model on x,\nand observe its output distribution p(y|x), Then,\nwe run the model on ˜ xand save ˜hi, the counter-\nfactual output of component i. Then we run the\nmodel on x again, but replace hi with ˜hi during the\nforward pass. This yields an altered model output\ndistribution ˜p(y|x). Vig et al. (2020) measure how\nimportant a component i is to a model behaviour b\nusing Natural Indirect Effect (NIE), the expected\nproportional difference in model behavior after in-\ntervening on component i. If bnull is the original\nbehaviour of the model and bi,intv is the behaviour\nof the model after intervening on componenti, then\nNIE can be evaluated as shown in Equation (1):\nNIE(i, b) =E(x,˜ x)∈D\n\u0014bi,intv\nbnull\n− 1\n\u0015\n(1)\nVig et al. (2020) use the definition in eq. (2) to\nmeasure biased behaviour in a LM. It is the ratio of\nthe probabilities assigned by the model to an anti-\nstereotypical continuation as against a stereotypical\ncontinuation given a context. In case of Professions\ndataset (Vig et al., 2020), it is the ratio of proba-\nbility assigned to anti-stereotypical pronoun versus\nthe probability assigned to stereotypical pronoun.\nb(x) =p(y = anti-stereo|x)\np(y = stereo|x) (2)\nThe aforementioned technique analyzes individ-\nual components; Vig et al. propose two methods\nto gather a set of important components. Using the\ntop-k strategy, they evaluate every component, and\nselect the k components that cause the most change\nin model behavior. Using the k-greedy strategy,\nthey evaluate all components, and add the most im-\npactful one. Then, they evaluate each component\nagain, ablating both itand their set; they once again\nadd the most impactful component. They repeat\nthe latter step until they have a set of size k.\n3.1.2 Circuit Discovery\nThe circuits framework, which views models as\ncomputational graphs, provides a related technique\nfor identifying mechanisms in LMs. While Vig\net al.’s CMA approach generates a component set\n(nodes) relevant to a task, the circuits approach\ngenerates a set of edges, resulting in a detailed\nsubgraph. However, the underlying methodology\nis similar to CMA: we ablate edges via swaps,\nand see which edges hurt performance once ab-\nlated. Though our fine-tuning techniques only tar-\nget nodes (not edges), comparing CMA and circuits\nlocalisations of bias could still be insightful.\nWe use Conmy et al.’s (2023) automated circuit\ndiscovery code (ACDC) to identify model com-\nponents relevant to (gender) bias. This technique\niteratively tests model edges, removing those that\ncan be ablated without changing task performance.\nWe use ACDC on the same professions dataset as\nCMA, and measure task performance as the differ-\nence in probability assigned to stereotypical and\nnon-stereotypical pronoun continuations.\n383\n3.1.3 Differentiable Masking With CMA\nWe finally propose our own method for localizing\nrelevant LM components that combines two ap-\nproaches: Vig et al.’s (2020) CMA and De Cao\net al.’s (2021a) differentiable masking (DiffMask).\nOur method is motivated by a notable challenge\nwith CMA, namely, how to select the best size-\nk subset of model components that contributes to\nbias. Vig et al.’s two strategies for this (top-k and\nk-greedy as discussed in Section 3.1.1) both have\ndownsides. A top-k strategy assumes that compo-\nnents’ importance is independent, while a k-greedy\nstrategy is expensive, requiring k evaluations of\nall components’ importance. A full sweep of the\nsearch space would be combinatorially expensive.\nThis combinatorial search problem can be refor-\nmulated as an optimization problem using a differ-\nentiable relaxation (Louizos et al., 2018; Bastings\net al., 2019; De Cao et al., 2021a,b; Schlichtkrull\net al., 2021). DiffMask, proposed by De Cao et al.\n(2021b) precisely apply the reformulation to learn\nan almost-binary differentiable stochastic mask\nover a model’s components, indicating which are\nimportant, and which are not. Unimportant com-\nponents are those whose outputs can be ablated\nwithout changing model behavior.\nWe adapt DiffMask in two ways, and label our\nvariant DiffMask+. First, instead of using surrogate\nmodels that instantiate distribution per input, we\ndirectly learn a distribution for the stochastic mask.\nThis change is crucial because it helps us identify\na single, generalizable set of components respon-\nsible for bias in the language model across the\nentire dataset, which is essential for downstream\nfine-tuning. Second, instead of learning interven-\ntions to ablate a component’s activations, we use\ncorresponding activations generated from the coun-\nterfactual sentences.\nBesides these changes, training and inference\nwith this mask proceed as in De Cao et al. (2021b).\nAt every time step, we run a forward pass of the\nmodel on an example from the Professions dataset.\nWe stochastically replace component outputs with\ncorresponding counterfactual outputs, according to\nthe mask; components with higher mask weights\nare replaced to a greater degree. We train the mask\nto induce the largest change in gendered pronoun\nprediction possible, while minimizing both the\nnumber of non-zero mask entries, and the magni-\ntude of overall changes made to the model’s output\ndistribution. This procedure yields a mask over our\ncomponents, whose expected values lie in [0, 1];\nhigher values indicate more important components.\nFor more details, see Appendix B.\n3.2 Experiments\nWe use the three methods discussed above to dis-\ncover the components that cause gender bias in\nGPT-2 small. For CMA and DiffMask+, we limit\nour analysis to attention heads. All experiments\nwere implemented using the TransformerLens2 li-\nbrary (Nanda and Bloom, 2022). For CMA, we\nused Vig et al.’s top-k strategy and selected only\nthe top 10 heads as the NIE quickly diminishes\nbeyond this point. Similarly, for DiffMask+, we\nchose the 10 heads with the highest expected mask\nvalue at the end of training. To find our circuit,\nwe ran ACDC, finding a whole circuit containing\nattention heads and other components as shown in\nFigure 4 in Appendix A. For hyperparameters and\ntraining details, see Appendix C.\n3.3 Results\nFigure 1 shows the attention heads selected using\neach method. For ACDC, we show only the atten-\ntion heads from the full circuit. All methods find\nattention heads located mostly in the final layers\nof the model; this contrasts with Vig et al. (2020),\nwho find heads in middle layers. This may be\ndue to the fact that Vig et al. (2020) mainly assess\ngender bias in co-reference resolution in their atten-\ntion intervention experiments and accordingly use\nthe WinoBias (Zhao et al., 2018) and Winogender\n(Rudinger et al., 2018) datasets. The results sug-\ngest that the dataset used for discovery influences\nthe components picked by these methods.\nThe Venn diagram in Figure 1 shows the overlap\nof heads across methods. We observe a signifi-\ncant overlap: 5 of the top 10 heads are shared by\nall three methods. Attention heads selected using\nCMA and ACDC have more overlap and as ob-\nserved in the mitigation results in Section 4.3 the\ntwo methods perform similarly on different metrics.\nThe fact that DiffMask+ yields 4 heads that are not\nshared might be due to its objective: DiffMask+\nattempts to maximally change gendered pronoun\nprediction while still minimally changing the distri-\nbution overall. This latter constraint is absent from\nthe other two methods.\nWe also note that the selected heads are located\nin the later half of the model. We hypothesize that\n2https://github.com/neelnanda-io/\nTransformerLens\n384\n0 2 4 6 8 10\n0\n2\n4\n6\n8\n10Layers\nCMA\n0 2 4 6 8 10\nHeads\nDiffMask\n0 2 4 6 8 10\nACDC\n3\n4\n0\n2\n2 1\n5\nCMA DiffMask\nACDC\nFigure 1: Top 10 attention heads selected using CMA, DiffMask+ and ACDC. Overlapping heads are shown in red.\nThe Venn diagram shows the overlap counts between all combinations of the sets.\nthis may be because these heads are transferring\ngender information from the profession position to\nthe end position of the sentence. Although earlier\nheads can also attend to gender tokens, prior work\nsuggests that entities are enriched by lower-layer\nMLPs before information is extracted from them\nby later attention heads (Geva et al., 2023).\n4 Mitigating Gender Bias\nHaving identified components responsible for gen-\nder bias in GPT-2 small, we test whether this infor-\nmation can be used to mitigate the bias. To this end,\nwe fine-tune the model on a dataset carefully cu-\nrated to be gender balanced—this has been shown\nto lead to a reduction in gender bias (Gira et al.,\n2022). We compare the effectiveness of fine-tuning\nonly the components found in the previous section\nto various baselines, both fine-tuned and not.\n4.1 Fine-tuning Dataset and Models\nWe test the effectiveness of parameter-efficient fine-\ntuning with the identified GPT-2 components at\nmitigating gender bias. We fine-tune on the BUG\ndataset3 (Levy et al., 2021), which contains an-\nnotated natural sentences containing one or more\ngendered pronouns. We use the balanced version of\nBUG, which has an equal number of masculine and\nfeminine pronouns, to counteract GPT-2’s gender\nbias in pronouns. For each model in Table 1, we\nfine-tune only the specified subset of GPT-2’s pa-\nrameters and compare our methods to the not fine-\ntuned GPT-2 model, our baseline. Appendix D\ncontains fine-tuning details.\n4.2 Metrics\nWe use several metrics and baselines to evaluate\nthe effectiveness of the bias mitigation under the\ndifferent conditions. To measure gender bias, we\n3https://github.com/SLAB-NLP/BUG\nTable 1: All fine-tuned models and corresponding com-\nponents selected for fine-tuning in Section 4. DM means\nour proposed method DiffMask+.\nModel Name Selected Components\nFull Model Entire model.\nRandom Attn Heads Set of 10 randomly selected at-\ntention heads not found by CMA,\nACDC or DM.\nAll Attn Layers All attention layers including the at-\ntention projection.\nLast 4 Attn Layers Last 4 attention layers.\nACDC MLPs, attention heads, and embed-\nding layers found by ACDC.\nACDC Attn Heads Attention heads from the ACDC cir-\ncuit.\nCMA Attn Heads Top 10 attention heads found by\nCMA.\nDM Attn Heads Top 10 attention heads found by\nDiffMask+.\nuse WinoBias (Zhao et al., 2018) and the gen-\nder bias subset of CrowS-Pairs by Névéol et al.\n(2022). We also measure model performance on\nthe original Professions dataset using which im-\nportant components were found. To ensure that\nfine-tuning did not harm models’ general language\nmodeling abilities, we also measure these, via Wiki-\nText perplexity (Merity et al., 2016) and accuracy\non BLiMP (Warstadt et al., 2020). All metrics,\nexcept for the perplexity, are defined as the ratio\nof times that the model prefers the correct/anti-\nstereotypical over the incorrect/stereotypical vari-\nant. Given a dataset D with pairs of stereotypical\nand anti-stereotypical sentences (x, ˜ x), the Stereo-\ntype Score is defined as follows.\nSS = 1\n|D|\nX\n(x,˜ x)∈D\nIp(x)>p(˜ x) (3)\nWinoBias We measure the models’ gender bias\nusing WinoBias. Even if this dataset with its small\nlinguistic variety might not exactly reflect real-\nworld biased language (Lior and Stanovsky, 2023),\n385\nit is widely used as its simplicity allows for con-\ntrolled experiments. We measure models’ gender\nbias using WinoBias’ type 2 dataset4 (Zhao et al.,\n2018). This dataset consists of sentences contain-\ning two occupation terms and one gendered pro-\nnoun; models must determine which occupation\nthe pronoun refers to. In type 2 examples, the\nsentence’s syntax always determines the correct\noccupation (regardless of the pronoun’s gender).\nFor each sentence there is one pro- and one anti-\nstereotypical version, which differ only in the gen-\nder of the pronoun used. We consider a model\nbiased if it consistently assigns higher probability\nto the pro-stereotypical sentence. We record the\nproportion of examples where the model assigns\nhigher probability to the pro-stereotypical version.\nNote that our metric differs from the original met-\nric, which was formulated in terms of co-reference\nresolution accuracy.\nCrowS-Pairs The gender bias subset of CrowS-\nPairs measures gender bias in LMs, construed more\nbroadly than occupation-gender associations. It\nconsists of minimal pairs, a more and a less stereo-\ntypical sentence. We consider a systematic prefer-\nence for more stereotypical sentences (by compar-\ning perplexities) to indicate a biased model. As in\nWinoBias, the bias is measured as the proportion\nof examples where the model prefers the stereo-\ntypical sentence. In our experiments, we use an\nupdated version from Névéol et al. (2022) where\npotential validity issues (including those identified\nby Blodgett et al. (2021)) have been addressed.\nProfessions We use the Professions dataset, with\nwhich we found bias-relevant components, to as-\nsess gender bias in the fine-tuned models. For every\nsentence in the dataset, we measure the probabil-\nity assigned to the pro-/anti-stereotypical continua-\ntions (either he or she, depending on the example).\nWe measure the proportion of examples where the\npro-stereotypical continuation is more probable.\nBLiMP We evaluate our models’ linguistic abili-\nties using BLiMP. BLiMP consists of a number of\ndatasets, each of which targets a specific linguis-\ntic phenomenon. Each dataset contains examples,\neach of which is a minimal sentence pair: one sen-\ntence is correct and the other incorrect, with respect\n4We choose not to discuss the results for the type 1 dataset\nbecause we do not test an actual co-reference resolution task,\nbut rather compute the perplexities of continuing with one or\nthe other gendered pronoun.\nto the targeted phenomenon. The model should\nsystematically assign a higher probability to the\ncorrect sentence. We report accuracy on BLiMP as\na whole, as well as on the Gender Anaphor Agree-\nment (AGA) and Subject Verb Agreement (SV A)\nsubtasks. We do this to understand the effect of our\nfine-tuning on these specific linguistic phenomena,\nwhere gender is only relevant for one of these tasks.\nWikiText We evaluate our models’ general lan-\nguage modeling performance by computing their\nperplexity on the test split of the WikiText-103 cor-\npus5 (4358 examples) (Merity et al., 2016), which\nconsists of “Good” and “Featured” Wikipedia ar-\nticles. Higher perplexity might indicate that fine-\ntuning hurt general language modeling abilities.\n4.3 Results\nTable 2 presents the average bias evaluation results\nfor CrowS-Pairs, WinoBias, and Professions, as\nwell as for the perplexity and BLiMP metrics.\nBias Metrics We find that all types of fine-tuning\nimprove performance on the Professions dataset\n(details in the appendix; Figure 5). This suggests\nthat the fine-tuning procedure successfully changed\nmodel behavior. However, not all types of fine-\ntuning are equal: fine-tuning strategies that targeted\nlate attention heads yielded models with lower\nstereotyping and variance than those that targeted\nother components, spread throughout the model.\nSimilarly, the CrowS-Pairs results in Figure 2\nshow that models where only the attention heads\ndiscovered using the three methods from Section 3\nwere fine-tuned, achieve the best results in terms of\ngender bias reduction. In contrast, fine-tuning ran-\ndom attention heads yields no reduction in gender\nbias. The DM Attention Heads model in particular\nsignificantly reduces bias with an average stereo-\ntype score as defined in eq. (3) from 0.58 of the\nbaseline to 0.55. Additionally, the scores of DM\nAttention Heads model have low variance while\nfine-tuning all attention layers, the full model, or\nACDC components yields high-variance results.\nEvaluation on WinoBias yields contrasting re-\nsults (Table 2). Fine-tuning the attention heads\nonly marginally reduced the gender bias on aver-\nage. Surprisingly, fine-tuning the last 4 attention\nlayers achieved the best reduction in gender bias.\nAt first glance, the CrowS-Pairs and WinoBias\nresults are mixed. Fine-tuning the full model, last\n5https://huggingface.co/datasets/wikitext\n386\nTable 2: Effect comparison of the different fine-tuning interventions. Reported are perplexity (PPL, measured on\nWikiText), three measures of linguistic adequacy (full BLiMP as well as subject-verb and anaphora agreement\nportions of BLiMP), and the gender bias measures from CrowS-Pairs, WinoBias, and the Professions bench-\nmarks/datasets. The cells show the % improvement (positive is better as indicated by ↑) w.r.t. the original GPT-2\nbefore fine-tuning, averaged over 5 seeds (absolute scores are in Appendix E). * indicates p <0.05 for two-sided\none sample t-test, where the original GPT-2 performance serves as the population mean.\nperplexity ↑ linguistic adequacy ↑ gender bias measures ↑\nPPL BLiMP SV AGA CrowS. WinoB. Prof.\nbaselines full model -44.2 -3.9* -2.9* 1.2* -1.4 4.6* 2.3\nrandom attn heads -17.0 -3.0* -0.9* 0.2 -0.2 1.9 1.3\nbroad interventions\nall attn layers -19.1 -2.0* -1.4* 1.4* -0.6 0.1 0.6*\nlast 4 attn layers -12.6 -3.4* 0.4* -1.2* -0.2 4.2* 3.2*\nacdc -38.8 -4.6* -1.8* 0.6 -0.9 3.3* 3.1\nnarrow interventions\nacdc attn heads -16.6 -3.0* 0.3* 0.2 3.5* 1.4 1.9*\ncma attn heads -16.6 -3.0* 0.3* 0.2 3.5* 1.4 1.9*\ndm attn heads -17.5 -2.4* 0.2 -0.0 4.8* 0.9 2.9*\nfull\nmodel\nrandom\nattn\nheads\nall\nattn\nlayers\nlast 4\nattn\nlayers\nacdc acdc\nattn\nheads\ncma\nattn\nheads\ndm\nattn\nheads\n0.55\n0.56\n0.57\n0.58\n0.59\n0.6\n0.61\n0.62\nCrowS-Pairs Stereotype Score\nFigure 2: CrowS-Pairs results (here: lower is better).\nPurple models are baselines; the dotted line shows the\nnon-fine-tuned GPT-2 performance.\n4 attention layers, or ACDC components yields\nthe most improvement on WinoBias, but these\nmodels score badly on CrowS-Pairs. However,\nthe reverse is not true: the models that improved\nmost on CrowS-Pairs also improved on WinoBias—\nalthough not consistently (Figure 3). We postulate\nmany potential explanations for the divergent out-\ncomes seen between WinoBias and CrowS-Pairs.\nFirst, WinoBias could simply be rewarding models\nthat perform randomly or poorly at co-reference res-\nolution, although good overall BLiMP AGA scores\nsuggest this is not the case. Second, gender bias\nin co-reference resolution might stem from a com-\nponent set distinct from the ones we discovered.\nThis is supported by Vig et al.’s findings, which\nrevealed a distinct set of attention heads that con-\ntribute to gender bias in co-reference resolution.\nfull\nmodel\nrandom\nattn\nheads\nall\nattn\nlayers\nlast 4\nattn\nlayers\nacdc acdc\nattn\nheads\ncma\nattn\nheads\ndm\nattn\nheads\n0.57\n0.58\n0.59\n0.6\n0.61\n0.62\n0.63\n0.64\nWinoBias Type2 Score\nFigure 3: WinoBias Type2 Stereotype Score (here:\nlower is better). Purple models are baselines; the dotted\nline shows the non-fine-tuned GPT-2 performance.\nFinally, this might be linked to how the bias mea-\nsures are operationalized, which we will come back\nto in Section 5.\nWikiText & BLiMP Both the perplexity mea-\nsured on WikiText and accuracies on BLiMP in-\nform us about the general language modeling ca-\npability before and after fine-tuning. For Wiki-\nText, we observe that fine-tuning more parameters—\nas when we fine-tune the full model or ACDC\ncircuit—hurts the perplexity more; the fully fine-\ntuned model performs the worst, increasing per-\nplexity to 34.16 from 23.69. In contrast, targeted\nfine-tuning of attention heads increases perplexity\nby a much lower margin. This trade-off motivates\nfinding a minimal component set to fine-tune, in\norder to mitigate bias while maintaining general\nlanguage modeling ability.\n387\nAll fine-tuned models attain lower performance\non BLiMP overall than the pre-trained baseline; as\nin the WikiText case, the more components fine-\ntuned, the more performance drops. However, ex-\namining the performance on agreement subtasks\nreveals more nuance. On SV A, fine-tuning only\nthe top-10 attention heads found using the meth-\nods from Section 3 improved performance by a\nsmall margin. On AGA, almost all fine-tuned mod-\nels attained scores on par with the baseline. So,\nwhile fine-tuning small sets of attention heads hurt\nBLiMP performance overall, the maintained perfor-\nmance on SV A and AGA suggest that agreement\nability, gender-related or not, are not hurt.\n5 Discussion & Conclusions\nWith this work, we provide an exploratory study of\nthe identification and mitigation of gender bias in\nGPT-2. Our three different methods identify model\ncomponents relevant to gender bias—according to\nour results, they largely agree on the most rele-\nvant attention heads: most of the heads responsi-\nble for gender bias are found mainly in the last\nfour attention layers. We then intervene on each\nmethod’s found components to mitigate the gen-\nder bias but maintain language modeling perfor-\nmance. We find that language modeling perfor-\nmance deteriorates only minimally for our ‘narrow’\ninterventions, but deteriorates more in conditions\nwhere a larger amount of components/parameters\nare adapted by fine-tuning.\nRegarding computational efficiency, we find that\nthe circuits approach is computationally inefficient\ncompared to the other methods. For explanatory\nand exploratory work, like ours, circuits are very\nuseful and can yield fine-grained insights into the\nmodel mechanisms. However, if resource effi-\nciency is a high priority, we suggest using other\nmethods than (automatic) circuit discovery. One\nkey contribution of this paper is a new and very ef-\nficient method, DiffMask+, which finds a minimal\nset of attention heads for fine-tuning, while being\ncomputationally less prohibitive than methods such\nas automatic circuit discovery.\nLimitations Have we reached our goal of reduc-\ning bias, using computational efficient methods?\nConsidering the measured gender bias, we success-\nfully reduced the bias on two out of three datasets.\nThis is encouraging, but our results also reveal\nsome inconsistencies between different ways of\nmeasuring bias. This is not unexpected; in fact,\nmuch previous work has highlighted many issues\nthat put the validity and reliability of current bias\nmeasures into question (e.g., Blodgett et al., 2021;\nTalat et al., 2022; Dev et al., 2022). Bias mea-\nsures may target very different manifestations of\nthe bias of interest (van der Wal et al., 2023). We\ntherefore attribute the observed inconsistencies to\nthe implicit versus explicit gender bias in different\ndatasets, which could be represented differently in\nmodel components, and thus also targeted differ-\nently by fine-tuning.\nDespite these challenges, we tried to address\nsome of these concerns by using multiple different\nbias metrics and testing the consistency of these\nacross different seeds. We believe that the suc-\ncess of our approach is heavily contingent upon the\ndatasets employed for both component identifica-\ntion and the subsequent fine-tuning of the chosen\ncomponents. For example, using template-based\ndatasets such as WinoBias or Professions could re-\nduce the identified components’ generalizability,\nas components that contribute to one form of gen-\nder bias may not contribute to another. The same\napplies to the fine-tuning stage as well. Using a\ndataset with limited variability in structure might\nresult in only partial mitigation of the behavior.\nWe therefore conclude that for even better bias re-\nduction, it is essential to use and develop datasets\nthat are diverse and representative of the behaviour\nbeing studied.\nFuture work For a wider picture of how our\nfindings integrate in bias identification and miti-\ngation studies, we would like to compare our ap-\nproaches to other promising methods in the liter-\nature like concept erasure at the activation level\n(e.g., LEACE; Belrose et al., 2023) and changes\nto the language generation procedure (e.g., “self-\ndebiasing”; Schick et al., 2021). Future work\nshould also test whether these mitigation strategies\ngeneralize to different conditions, for example, lan-\nguage models larger than GPT-2 small. Lastly, we\nalso stress the importance of developing method-\nologies for operationalizing other forms of bias\nthan binary gender in English, and to overcome dif-\nficulties we currently face when using contrastive\nsets and existing bias benchmarks.\n6 Acknowledgements\nOW’s contributions are financed by the Dutch\nResearch Council (NWO) as part of project\n406.DI.19.059.\n388\nReferences\nMaria Barrett, Yova Kementchedjhieva, Yanai Elazar,\nDesmond Elliott, and Anders Søgaard. 2019. Adver-\nsarial removal of demographic attributes revisited. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 6330–\n6335, Hong Kong, China. Association for Computa-\ntional Linguistics.\nChristine Basta, Marta R. Costa-jussà, and Noe Casas.\n2019. Evaluating the underlying gender bias in con-\ntextualized word embeddings. In Proceedings of the\nFirst Workshop on Gender Bias in Natural Language\nProcessing, page 33–39, Florence, Italy. Association\nfor Computational Linguistics.\nJasmijn Bastings, Wilker Aziz, and Ivan Titov. 2019.\nInterpretable neural predictions with differentiable\nbinary variables. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 2963–2977, Florence, Italy. Associa-\ntion for Computational Linguistics.\nAnthony Bau, Yonatan Belinkov, Hassan Sajjad, Nadir\nDurrani, Fahim Dalvi, and James R. Glass. 2018.\nIdentifying and controlling important neurons in neu-\nral machine translation. ArXiv, abs/1811.01157.\nNora Belrose, David Schneider-Joseph, Shauli Ravfogel,\nRyan Cotterell, Edward Raff, and Stella Biderman.\n2023. Leace: Perfect linear concept erasure in closed\nform. arXiv preprint arXiv:2306.03819.\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM confer-\nence on fairness, accountability, and transparency,\npages 610–623.\nStella Biderman, Hailey Schoelkopf, Quentin Gregory\nAnthony, Herbie Bradley, Kyle O’Brien, Eric Hal-\nlahan, Mohammad Aflah Khan, Shivanshu Purohit,\nUSVSN Sai Prashanth, Edward Raff, et al. 2023.\nPythia: A suite for analyzing large language mod-\nels across training and scaling. In International\nConference on Machine Learning, pages 2397–2430.\nPMLR.\nSu Lin Blodgett, Solon Barocas, Hal Daumé III, and\nHanna Wallach. 2020. Language (technology) is\npower: A critical survey of “bias” in nlp. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 5454–5476.\nSu Lin Blodgett, Gilsinia Lopez, Alexandra Olteanu,\nRobert Sim, and Hanna Wallach. 2021. Stereotyping\nNorwegian salmon: An inventory of pitfalls in fair-\nness benchmark datasets. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 1004–1015, Online. Association\nfor Computational Linguistics.\nTolga Bolukbasi, Kai-Wei Chang, James Y Zou,\nVenkatesh Saligrama, and Adam T Kalai. 2016. Man\nis to computer programmer as woman is to home-\nmaker? debiasing word embeddings. Advances in\nneural information processing systems, 29.\nShikha Bordia and Samuel Bowman. 2019. Identify-\ning and reducing gender bias in word-level language\nmodels. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Student Research Work-\nshop, pages 7–15.\nAylin Caliskan, Joanna J. Bryson, and Arvind\nNarayanan. 2017. Semantics derived automatically\nfrom language corpora contain human-like biases.\nScience, 356(6334):183–186. ArXiv:1608.07187\n[cs].\nArthur Conmy, Augustine N. Mavor-Parker, Aengus\nLynch, Stefan Heimersheim, and Adrià Garriga-\nAlonso. 2023. Towards automated circuit discovery\nfor mechanistic interpretability. (arXiv:2304.14997).\nArXiv:2304.14997 [cs].\nNicola De Cao, Michael Schlichtkrull, Wilker Aziz,\nand Ivan Titov. 2021a. How do decisions emerge\nacross layers in neural models? interpretation\nwith differentiable masking. (arXiv:2004.14992).\nArXiv:2004.14992 [cs, stat].\nNicola De Cao, Leon Schmid, Dieuwke Hupkes,\nand Ivan Titov. 2021b. Sparse interventions\nin language models with differentiable masking.\n(arXiv:2112.06837). ArXiv:2112.06837 [cs].\nPieter Delobelle, Ewoenam Tokpo, Toon Calders, and\nBettina Berendt. 2022. Measuring fairness with bi-\nased rulers: A comparative study on bias metrics for\npre-trained language models. In Proceedings of the\n2022 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, page 1693–1706, Seat-\ntle, United States. Association for Computational\nLinguistics.\nSunipa Dev, Emily Sheng, Jieyu Zhao, Aubrie Amstutz,\nJiao Sun, Yu Hou, Mattie Sanseverino, Jiin Kim, Ak-\nihiro Nishi, Nanyun Peng, et al. 2022. On measures\nof biases and harms in nlp. In Findings of the Associ-\nation for Computational Linguistics: AACL-IJCNLP\n2022, pages 246–267.\nYanai Elazar, Shauli Ravfogel, Alon Jacovi, and Yoav\nGoldberg. 2021. Amnesic probing: Behavioral expla-\nnation with amnesic counterfactuals. Transactions of\nthe Association for Computational Linguistics, 9:160–\n175.\nNelson Elhage, Neel Nanda, Catherine Olsson, Tom\nHenighan, Nicholas Joseph, Ben Mann, Amanda\nAskell, Yuntao Bai, Anna Chen, Tom Conerly,\nNova DasSarma, Dawn Drain, Deep Ganguli, Zac\n389\nHatfield-Dodds, Danny Hernandez, Andy Jones,\nJackson Kernion, Liane Lovitt, Kamal Ndousse,\nDario Amodei, Tom Brown, Jack Clark, Jared Ka-\nplan, Sam McCandlish, and Chris Olah. 2021. A\nmathematical framework for transformer circuits.\nTransformer Circuits Thread. Https://transformer-\ncircuits.pub/2021/framework/index.html.\nAtticus Geiger, Hanson Lu, Thomas Icard, and Christo-\npher Potts. 2021. Causal abstractions of neural net-\nworks. (arXiv:2106.02997). ArXiv:2106.02997 [cs].\nMor Geva, Jasmijn Bastings, Katja Filippova, and Amir\nGloberson. 2023. Dissecting recall of factual associ-\nations in auto-regressive language models.\nMor Geva, Avi Caciularu, Kevin Wang, and Yoav Gold-\nberg. 2022. Transformer feed-forward layers build\npredictions by promoting concepts in the vocabulary\nspace. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 30–45, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nMichael Gira, Ruisu Zhang, and Kangwook Lee. 2022.\nDebiasing pre-trained language models via efficient\nfine-tuning. In Proceedings of the Second Workshop\non Language Technology for Equality, Diversity and\nInclusion, pages 59–69, Dublin, Ireland. Association\nfor Computational Linguistics.\nMichael Hanna, Ollie Liu, and Alexandre Variengien.\n2023. How does gpt-2 compute greater-than?: In-\nterpreting mathematical abilities in a pre-trained lan-\nguage model.\nPeter Hase, Mohit Bansal, Been Kim, and Asma Ghan-\ndeharioun. 2023. Does localization inform editing?\nsurprising differences in causality-based localization\nvs. knowledge editing in language models.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin de Laroussilhe, Andrea Ges-\nmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for nlp. In Inter-\nnational Conference on Machine Learning.\nDirk Hovy and Shannon L. Spruit. 2016. The social\nimpact of natural language processing. In Proceed-\nings of the 54th Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Pa-\npers), pages 591–598, Berlin, Germany. Association\nfor Computational Linguistics.\nDiederik P. Kingma and Jimmy Ba. 2014. Adam:\nA method for stochastic optimization. CoRR,\nabs/1412.6980.\nNeeraja Kirtane and Tanvi Anand. 2022. Mitigating\ngender stereotypes in Hindi and Marathi. In Proceed-\nings of the 4th Workshop on Gender Bias in Natu-\nral Language Processing (GeBNLP), pages 145–150,\nSeattle, Washington. Association for Computational\nLinguistics.\nYair Lakretz, German Kruszewski, Theo Desbordes,\nDieuwke Hupkes, Stanislas Dehaene, and Marco Ba-\nroni. 2019. The emergence of number and syntax\nunits in LSTM language models. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and\nShort Papers), pages 11–20, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nAnne Lauscher, Tobias Lueken, and Goran Glavaš. 2021.\nSustainable modular debiasing of language models.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2021, pages 4782–4797, Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\nShahar Levy, Koren Lazar, and Gabriel Stanovsky. 2021.\nCollecting a large-scale gender bias dataset for coref-\nerence resolution and machine translation. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2021, pages 2470–2480, Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. Pro-\nceedings of the 59th Annual Meeting of the Associa-\ntion for Computational Linguistics and the 11th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers), abs/2101.00190.\nTom Lieberum, Matthew Rahtz, János Kramár, Neel\nNanda, Geoffrey Irving, Rohin Shah, and Vladimir\nMikulik. 2023. Does circuit analysis interpretability\nscale? evidence from multiple choice capabilities in\nchinchilla.\nGili Lior and Gabriel Stanovsky. 2023. Comparing hu-\nmans and models on a similar scale: Towards cogni-\ntive gender bias evaluation in coreference resolution.\nIlya Loshchilov and Frank Hutter. 2017. Fixing\nweight decay regularization in adam. ArXiv,\nabs/1711.05101.\nChristos Louizos, Max Welling, and Diederik P. Kingma.\n2018. Learning sparse neural networks through L0\nregularization. In International Conference on Learn-\ning Representations.\nKaiji Lu, Piotr Mardziel, Fangjing Wu, Preetam Aman-\ncharla, and Anupam Datta. 2020. Gender bias in\nneural natural language processing. Logic, Language,\nand Security: Essays Dedicated to Andre Scedrov on\nthe Occasion of His 65th Birthday, pages 189–202.\nChandler May, Alex Wang, Shikha Bordia, Samuel R.\nBowman, and Rachel Rudinger. 2019. On measur-\ning social biases in sentence encoders. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), page 622–628, Minneapo-\nlis, Minnesota. Association for Computational Lin-\nguistics.\n390\nNicholas Meade, Elinor Poole-Dayan, and Siva Reddy.\n2022. An empirical survey of the effectiveness of\ndebiasing techniques for pre-trained language models.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 1878–1898, Dublin, Ireland.\nAssociation for Computational Linguistics.\nKevin Meng, David Bau, Alex Andonian, and Yonatan\nBelinkov. 2023. Locating and editing factual associa-\ntions in gpt. (arXiv:2202.05262). ArXiv:2202.05262\n[cs].\nKevin Meng, Arnab Sen Sharma, Alex Andonian,\nYonatan Belinkov, and David Bau. 2022. Mass-\nediting memory in a transformer. arXiv preprint\narXiv:2210.07229.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer sentinel mixture mod-\nels. ArXiv, abs/1609.07843.\nHosein Mohebbi, Willem Zuidema, Grzegorz Chrupała,\nand Afra Alishahi. 2023. Quantifying context mixing\nin transformers. In Proceedings of the 17th Confer-\nence of the European Chapter of the Association\nfor Computational Linguistics , pages 3378–3400,\nDubrovnik, Croatia. Association for Computational\nLinguistics.\nMoin Nadeem, Anna Bethke, and Siva Reddy. 2021.\nStereoset: Measuring stereotypical bias in pretrained\nlanguage models. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), page 5356–5371, Online. Association for\nComputational Linguistics.\nNeel Nanda and Joseph Bloom. 2022. Transformerlens.\nNikita Nangia, Clara Vania, Rasika Bhalerao, and\nSamuel R. Bowman. 2020. CrowS-pairs: A chal-\nlenge dataset for measuring social biases in masked\nlanguage models. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1953–1967, Online. As-\nsociation for Computational Linguistics.\nAurélie Névéol, Yoann Dupont, Julien Bezançon, and\nKarën Fort. 2022. French crows-pairs: Extending a\nchallenge dataset for measuring social bias in masked\nlanguage models to a language other than english.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 8521–8531.\nJudea Pearl. 2014. Interpretation and identification of\ncausal mediation. Psychological methods, 19.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nShauli Ravfogel, Yanai Elazar, Hila Gonen, Michael\nTwiton, and Yoav Goldberg. 2020. Null it out: Guard-\ning protected attributes by iterative nullspace projec-\ntion. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n7237–7256, Online. Association for Computational\nLinguistics.\nAbhilasha Ravichander, Yonatan Belinkov, and Eduard\nHovy. 2021. Probing the probing paradigm: Does\nprobing accuracy entail task relevance? In Proceed-\nings of the 16th Conference of the European Chap-\nter of the Association for Computational Linguistics:\nMain Volume, pages 3363–3377, Online. Association\nfor Computational Linguistics.\nRachel Rudinger, Jason Naradowsky, Brian Leonard,\nand Benjamin Van Durme. 2018. Gender bias in\ncoreference resolution. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 2 (Short Papers) ,\npages 8–14, New Orleans, Louisiana. Association for\nComputational Linguistics.\nTimo Schick, Sahana Udupa, and Hinrich Schütze. 2021.\nSelf-diagnosis and self-debiasing: A proposal for re-\nducing corpus-based bias in NLP. Transactions of the\nAssociation for Computational Linguistics, 9:1408–\n1424.\nMichael Sejr Schlichtkrull, Nicola De Cao, and Ivan\nTitov. 2021. Interpreting graph neural networks for\n{nlp} with differentiable edge masking. In Interna-\ntional Conference on Learning Representations.\nThibault Sellam, Steve Yadlowsky, Ian Tenney, Jason\nWei, Naomi Saphra, Alexander D’Amour, Tal Linzen,\nJasmijn Bastings, Iulia Raluca Turc, Jacob Eisenstein,\nDipanjan Das, and Ellie Pavlick. 2022. The multib-\nerts: BERT reproductions for robustness analysis. In\nThe Tenth International Conference on Learning Rep-\nresentations, ICLR 2022, Virtual Event, April 25-29,\n2022. OpenReview.net.\nIrene Solaiman and Christy Dennison. 2021. Process\nfor adapting language models to society (palms) with\nvalues-targeted datasets. In Advances in Neural Infor-\nmation Processing Systems, volume 34, pages 5861–\n5873. Curran Associates, Inc.\nZeerak Talat, Aurélie Névéol, Stella Biderman, Miruna\nClinciu, Manan Dey, Shayne Longpre, Sasha Luc-\ncioni, Maraim Masoud, Margaret Mitchell, Dragomir\nRadev, et al. 2022. You reap what you sow: On\nthe challenges of bias evaluation under multilingual\nsettings. In Proceedings of BigScience Episode# 5–\nWorkshop on Challenges & Perspectives in Creating\nLarge Language Models, pages 26–41.\nOskar van der Wal, Dominik Bachmann, Alina Lei-\ndinger, Leendert van Maanen, Willem Zuidema, and\nKatrin Schulz. 2023. Undesirable biases in nlp:\nAverting a crisis of measurement. arXiv preprint\narXiv:2211.13709.\n391\nOskar Van Der Wal, Jaap Jumelet, Katrin Schulz, and\nWillem Zuidema. 2022. The birth of bias: A case\nstudy on the evolution of gender bias in an English\nlanguage model. In Proceedings of the 4th Work-\nshop on Gender Bias in Natural Language Process-\ning (GeBNLP), pages 75–75, Seattle, Washington.\nAssociation for Computational Linguistics.\nJesse Vig, Sebastian Gehrmann, Yonatan Belinkov,\nSharon Qian, Daniel Nevo, Yaron Singer, and Stu-\nart Shieber. 2020. Investigating gender bias in lan-\nguage models using causal mediation analysis. In\nAdvances in Neural Information Processing Systems,\nvolume 33, page 12388–12401. Curran Associates,\nInc.\nKevin Wang, Alexandre Variengien, Arthur Conmy,\nBuck Shlegeris, and Jacob Steinhardt. 2023. Inter-\npretability in the wild: A circuit for indirect object\nidentification in gpt-2 small.\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-\nhananey, Wei Peng, Sheng-Fu Wang, and Samuel R.\nBowman. 2020. BLiMP: The benchmark of linguis-\ntic minimal pairs for English. Transactions of the\nAssociation for Computational Linguistics , 8:377–\n392.\nLaura Weidinger, John F. J. Mellor, Maribeth Rauh,\nConor Griffin, Jonathan Uesato, Po-Sen Huang, Myra\nCheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh,\nZachary Kenton, Sande Minnich Brown, William T.\nHawkins, Tom Stepleton, Courtney Biles, Abeba\nBirhane, Julia Haas, Laura Rimell, Lisa Anne Hen-\ndricks, William S. Isaac, Sean Legassick, Geoffrey\nIrving, and Iason Gabriel. 2021. Ethical and so-\ncial risks of harm from language models. ArXiv,\nabs/2112.04359.\nZhongbin Xie and Thomas Lukasiewicz. 2023. An\nempirical analysis of parameter-efficient meth-\nods for debiasing pre-trained language models.\n(arXiv:2306.04067). ArXiv:2306.04067 [cs].\nBrian Hu Zhang, Blake Lemoine, and Margaret Mitchell.\n2018. Mitigating unwanted biases with adversarial\nlearning. In Proceedings of the 2018 AAAI/ACM\nConference on AI, Ethics, and Society , pages 335–\n340.\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-\ndonez, and Kai-Wei Chang. 2018. Gender bias in\ncoreference resolution: Evaluation and debiasing\nmethods. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), pages 15–20, New\nOrleans, Louisiana. Association for Computational\nLinguistics.\nRan Zmigrod, Sabrina J Mielke, Hanna Wallach, and\nRyan Cotterell. 2019. Counterfactual data augmenta-\ntion for mitigating gender stereotypes in languages\nwith rich morphology. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 1651–1661.\nA Circuit Discovery\nThe circuit discovered in GPT-2 small model using\nprofessions datset is shown in Figure 4\n<a11.8>\n<resid_post>\n<a11.1>\n<a10.9>\n<a11.8_v>\n<a9.7>\n<a10.9_v>\n<a9.5> <m8> <a8.11> <a8.10> <a8.3> <a7.5>\n<a6.0>\n<m7>\n<a9.7_v>\n<a4.3>\n<a6.0_v>\n<m0>\n<m5>\n<m3>\n<a4.3_v>\n<a10.9_q><a10.9_k>\n<m9>\n<a9.7_q>\n<m6>\n<a8.11_v> <a7.5_v>\nembed\nFigure 4: Circuit discovered in the GPT-2 small model\nusing Professions dataset.\nB DiffMask+ Implementation Details\nDuring inference, DiffMask+ works as follows. We\nhave two inputs—our normal input x and our coun-\nterfactual input ˜ x—as well as a k-dimensional bi-\nnary mask m ∈ {0, 1}k; for GPT-2 small, the num-\nber of components k is 144 as we choose to only\nselect attention heads. We run forward passes on\nboth inputs, recording each component’s output\non the normal dataset ( h1, . . . ,hk) and the coun-\nterfactual dataset ( ˜h1, . . . ,˜hk). Finally, we run\nthe model once more on the normal input, apply-\ning the mask: we replace each original compo-\nnent output hi with the potentially masked output\nh′\ni = (1− mi) · hi + mi · ˜hi6. If our mask cap-\ntures which components are important, our masked\nmodel should behave as if it were receiving the\ncounterfactual input.\nDiffMask+’s training setup is slightly different.\nWe cannot learn a purely binary mask, as that\nwould not be differentiable. Instead, we learn a\n6We can apply our mask either at every time step, or at\nonly the final time step.\n392\nparameterization of a hard concrete distribution\n(Louizos et al., 2018), a type of distribution that\nfalls in [0, 1] and assigns non-zero probability to\nboth 0 and 1. This distribution is parameterized by\na location vector z ∈ [0, 1]k, and can be sampled\nto produce a mask m ∈ [0, 1]k. When it comes\ntime to mask the model, we simply sample a mask\nfrom the distribution pz(m); note that this mask\nmay no longer be strictly binary. However, we can\ngenerate a deterministic and truly binary mask for\nuse at inference time in expectation (mask set to 0\nif expected value < 0.5, and 1 otherwise).\nWith this setup, we can train our mask; we begin\nby initializing the location vector to[0.5]k. We then\ntrain it on our dataset D, optimizing a loss adapted\nfrom De Cao et al. (2021b) which is composed\nof three individual loss terms. The first, targets\nour task of interest—gender bias. If the original\ninput would lead to a prediction of stereotypical\npronoun yo, e.g. “she”, and corresponding anti-\nstereotypical pronoun is yc, e.g. “he”, we mini-\nmize ˜p(yo|x)/˜p(yc|x) where ˜p is the intervened or\nmasked model’s output distribution. This is min-\nimized when the anti-stereotypical prediction is\nmuch more likely than the original stereotypical\nprediction, i.e. when the relevant model compo-\nnents are intervened with the corresponding coun-\nterfactual output.\nThe second loss term is the expected number\nof non-zero elements in our sampled mask; we\nwant our mask to be sparse. Ideally, this would\nbe a hard constraint, where the number of non-\nzero elements is ≤ α for a chosen α; we will in-\nstead use a Lagrangian relaxation of this constraint.\nThe third term is the KL divergence between the\nunmasked model’s output distribution p(y|x) and\nmasked model’s output distribution ˜p(y|x) ; we\nwant our masking to minimally change model out-\nput, besides task-relevant output. Formally, and\nmuch like De Cao et al. (2021b), we optimize:\nmax\nλ\nmin\nz\nX\nx,yo,yc∈D\n˜p(yo|x)\n˜p(yc|x)\n+ λ\n kX\ni=1\nEpzi (mi)[mi ̸= 0]− α\n!\n+ βDKL(p(y|x)||˜p(y|x))\n(4)\nHere, α and β are hyperparameters regulating\nsparsity and KL-divergence weight, respectively;\nλ ∈ R≥0 is our Lagrangian multiplier. Optimizing\nthis loss should produce a mask that captures the\ncomponents relevant to gender bias, while being\nmaximally sparse, and still mostly preserving the\nmodel’s output distribution.\nC Component Discovery\nHyperparameters\nWe optimized the DiffMask loss using Adam\n(Kingma and Ba, 2014) for 200 epochs on the pro-\nfessions dataset with a learning rate 10−3 and a\nconstant schedule. We choose the sparsity hyper-\nparameter α = 10for selecting 10 attention heads\nand the KL-Divergence weight β = 1as proposed\nin De Cao et al. (2021b). At the end of the train-\ning, we choose the top-10 heads with the highest\nexpected value of the location parameter of the\nstochastic mask.\nFor the ACDC experiment, we chose a threshold\nof 0.01, eliminating edges if ablating them caused\na change in performance of less than 0.01, as mea-\nsured by our pronoun probability difference metric.\nD Fine-tuning experiment\nIn Section 4, we fine-tune each model for a\nmaximum of 20 epochs using AdamW optimizer\n(Loshchilov and Hutter, 2017) with an initial learn-\ning rate 10−4 and a linear schedule. We opti-\nmize Cross Entropy Loss. The BUG balanced\ndataset contains 25844 sentences, which we split\ninto gender-balanced training and validation sets,\ncontaining 90% and 10% of the data respectively.\nWe use the validation loss both for selecting the\nbest model and early stopping with a patience of\n10 epochs.\nE Additional Results\nTable 3 shows all results of fine-tuned models and\nbaselines rounded to up to 2 decimals. Figure 5\nshows the stereotype scores of different models\nevaluated on the Professions dataset. Figure 6\nshows the perplexity of different models evaluated\non WikiText-103. Figure 7 shows the BLiMP over-\nall results measured over 5 different iterations. Sim-\nilarly, Figure 8 and Figure 9 shows the AGA and\nSV A results respectively.\n393\nTable 3: Comparison of the effect of the different fine-tuning interventions. Reported are perplexity (PPL, measured\non WikiText), three measures of linguistic adequacy (full BLiMP, and subject-verb and anaphora agreement\nportions of BLiMP), as well as the gender biases measures from CrowS-Pairs, WinoBias, and the Professions\nbenchmarks/datasets.\nperplexity linguistic adequacy gender bias measures\nPPL BLiMP SV AGA CrowS. WinoB. Prof.\nbaseline original gpt2 23.69 0.80 0.90 0.95 0.58 0.63 0.84\nfull model 34.16 0.77 0.87 0.97 0.59 0.60 0.82\nrandom attn heads 27.72 0.77 0.89 0.96 0.58 0.61 0.83\nbroad interventions\nall attn layers 28.22 0.78 0.89 0.97 0.58 0.63 0.83\nlast 4 attn layers 26.67 0.77 0.90 0.94 0.58 0.60 0.81\nacdc 32.89 0.76 0.88 0.96 0.58 0.61 0.81\nnarrow interventions\nacdc attn heads 27.62 0.77 0.90 0.96 0.56 0.62 0.82\ncma attn heads 27.62 0.77 0.90 0.96 0.56 0.62 0.82\ndm attn heads 27.84 0.78 0.90 0.95 0.55 0.62 0.81\nfull\nmodel\nrandom\nattn\nheads\nall\nattn\nlayers\nlast 4\nattn\nlayers\nacdc acdc\nattn\nheads\ncma\nattn\nheads\ndm\nattn\nheads\n0.77\n0.78\n0.79\n0.8\n0.81\n0.82\n0.83\n0.84\n0.85\nProfessions Stereotype Score\nFigure 5: Professions Stereotype Score (here: lower\nis better). Purple models are baselines; the dotted line\nshows the non-fine-tuned GPT-2 performance.\nfull\nmodel\nrandom\nattn\nheads\nall\nattn\nlayers\nlast 4 \nattn\nlayers\nacdc acdc\nattn\nheads\ncma\nattn\nheads\ndm\nattn\nheads\n2 4 \n2 6 \n2 8 \n3 0 \n3 2 \n3 4 \nPerplexity \nFigure 6: Test perplexity (lower is better) on WikiText-\n103. Purple models are baselines; the dotted line shows\nthe non-fine-tuned GPT-2 performance.\n394\nfull\nmodel\nrandom\nattn\nheads\nall\nattn\nlayers\nlast 4\nattn\nlayers\nacdc acdc\nattn\nheads\ncma\nattn\nheads\ndm\nattn\nheads\n0.76\n0.765\n0.77\n0.775\n0.78\n0.785\n0.79\n0.795\nBLiMP Overall Score\nFigure 7: BLiMP Overall results (higher is better). Pur-\nple models are baselines; the dotted line shows the non-\nfine-tuned GPT-2 performance.\nfull\nmodel\nrandom\nattn\nheads\nall\nattn\nlayers\nlast 4\nattn\nlayers\nacdc acdc\nattn\nheads\ncma\nattn\nheads\ndm\nattn\nheads\n0.94\n0.945\n0.95\n0.955\n0.96\n0.965\n0.97\n0.975\nBLiMP AGA Score\nFigure 8: BLiMP Anaphor Gender Agreement results\n(higher is better). Purple models are baselines; the dot-\nted line shows the non-fine-tuned GPT-2 performance.\nfull\nmodel\nrandom\nattn\nheads\nall\nattn\nlayers\nlast 4\nattn\nlayers\nacdc acdc\nattn\nheads\ncma\nattn\nheads\ndm\nattn\nheads\n0.86\n0.865\n0.87\n0.875\n0.88\n0.885\n0.89\n0.895\n0.9\nBLiMP SV Score\nFigure 9: BLiMP Subject Verb Agreement results\n(higher is better). Purple models are baselines; the dot-\nted line shows the non-fine-tuned GPT-2 performance.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7640228271484375
    },
    {
      "name": "Gender bias",
      "score": 0.6461610794067383
    },
    {
      "name": "Masking (illustration)",
      "score": 0.5980905294418335
    },
    {
      "name": "Language model",
      "score": 0.5498079657554626
    },
    {
      "name": "Transformer",
      "score": 0.5071136951446533
    },
    {
      "name": "Causal inference",
      "score": 0.48000115156173706
    },
    {
      "name": "Machine learning",
      "score": 0.46138837933540344
    },
    {
      "name": "Causal model",
      "score": 0.45780086517333984
    },
    {
      "name": "Mediation",
      "score": 0.42582929134368896
    },
    {
      "name": "Artificial intelligence",
      "score": 0.39138755202293396
    },
    {
      "name": "Data mining",
      "score": 0.352886438369751
    },
    {
      "name": "Econometrics",
      "score": 0.15535619854927063
    },
    {
      "name": "Psychology",
      "score": 0.10015243291854858
    },
    {
      "name": "Mathematics",
      "score": 0.08355340361595154
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Social psychology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I887064364",
      "name": "University of Amsterdam",
      "country": "NL"
    }
  ]
}