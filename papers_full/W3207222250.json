{
  "title": "SLAM: A Unified Encoder for Speech and Language Modeling via Speech-Text Joint Pre-Training",
  "url": "https://openalex.org/W3207222250",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4221359245",
      "name": "Bapna, Ankur",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4286961604",
      "name": "Chung, Yu-An",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097176646",
      "name": "Wu, Nan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4364850335",
      "name": "Gulati, Anmol",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2108933690",
      "name": "Jia Ye",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221359257",
      "name": "Clark, Jonathan H.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221359262",
      "name": "Johnson, Melvin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221359261",
      "name": "Riesa, Jason",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221359244",
      "name": "Conneau, Alexis",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102355683",
      "name": "Zhang, Yu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2547875792",
    "https://openalex.org/W2270070752",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W3154596443",
    "https://openalex.org/W3211483028",
    "https://openalex.org/W2842511635",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2970049541",
    "https://openalex.org/W3099782249",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W3037057938",
    "https://openalex.org/W3119866685",
    "https://openalex.org/W3097777922",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W3104681546",
    "https://openalex.org/W3165666670",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W2138204974",
    "https://openalex.org/W3139918052",
    "https://openalex.org/W3204696009",
    "https://openalex.org/W3093502935",
    "https://openalex.org/W11314411",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2336585117",
    "https://openalex.org/W3093579165",
    "https://openalex.org/W3035579820",
    "https://openalex.org/W2125336414",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2996383576",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2958953787",
    "https://openalex.org/W1494198834",
    "https://openalex.org/W3197324626",
    "https://openalex.org/W2995181338",
    "https://openalex.org/W3122317902",
    "https://openalex.org/W2250357346",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W2963799213",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2973049979",
    "https://openalex.org/W2936295285",
    "https://openalex.org/W2995929068",
    "https://openalex.org/W2787560479",
    "https://openalex.org/W2983040767",
    "https://openalex.org/W3054645415",
    "https://openalex.org/W2546744831",
    "https://openalex.org/W2950797609",
    "https://openalex.org/W3153287399",
    "https://openalex.org/W3032816972",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W3152609875",
    "https://openalex.org/W3144173820",
    "https://openalex.org/W3126261584",
    "https://openalex.org/W3193521535",
    "https://openalex.org/W2963748441"
  ],
  "abstract": "Unsupervised pre-training is now the predominant approach for both text and speech understanding. Self-attention models pre-trained on large amounts of unannotated data have been hugely successful when fine-tuned on downstream tasks from a variety of domains and languages. This paper takes the universality of unsupervised language pre-training one step further, by unifying speech and text pre-training within a single model. We build a single encoder with the BERT objective on unlabeled text together with the w2v-BERT objective on unlabeled speech. To further align our model representations across modalities, we leverage alignment losses, specifically Translation Language Modeling (TLM) and Speech Text Matching (STM) that make use of supervised speech-text recognition data. We demonstrate that incorporating both speech and text data during pre-training can significantly improve downstream quality on CoVoST~2 speech translation, by around 1 BLEU compared to single-modality pre-trained models, while retaining close to SotA performance on LibriSpeech and SpeechStew ASR tasks. On four GLUE tasks and text-normalization, we observe evidence of capacity limitations and interference between the two modalities, leading to degraded performance compared to an equivalent text-only model, while still being competitive with BERT. Through extensive empirical analysis we also demonstrate the importance of the choice of objective function for speech pre-training, and the beneficial effect of adding additional supervised signals on the quality of the learned representations.",
  "full_text": "Preprint\nSLAM: A U NIFIED ENCODER FOR SPEECH AND\nLANGUAGE MODELING VIA SPEECH -TEXT JOINT\nPRE-TRAINING\nAnkur Bapna∗1, Yu-An Chung∗1,2, Nan Wu1,3, Anmol Gulati1, Ye Jia1,\nJonathan H. Clark1, Melvin Johnson1, Jason Riesa1, Alexis Conneau∗1, Yu Zhang∗1\n1Google Research\n2MIT Computer Science and Artiﬁcial Intelligence Laboratory\n3Center for Data Science, New York University\nABSTRACT\nUnsupervised pre-training is now the predominant approach for both text and\nspeech understanding. Self-attention models pre-trained on large amounts of\nunannotated data have been hugely successful when ﬁne-tuned on downstream\ntasks from a variety of domains and languages. This paper takes the universality\nof unsupervised language pre-training one step further, by unifying speech and\ntext pre-training within a single model. We build a single encoder with the BERT\nobjective on unlabeled text together with the w2v-BERT objective on unlabeled\nspeech. To further align our model representations across modalities, we leverage\nalignment losses, speciﬁcally Translation Language Modeling (TLM) and Speech\nText Matching (STM) that make use of supervised speech-text recognition data.\nWe demonstrate that incorporating both speech and text data during pre-training\ncan signiﬁcantly improve downstream quality on CoV oST 2 speech translation, by\naround 1 BLEU compared to single-modality pre-trained models, while retaining\nclose to SotA performance on LibriSpeech and SpeechStew ASR tasks. On four\nGLUE tasks and text-normalization, we observe evidence of capacity limitations\nand interference between the two modalities, leading to degraded performance\ncompared to an equivalent text-only model, while still being competitive with\nBERT. Through extensive empirical analysis we also demonstrate the importance\nof the choice of objective function for speech pre-training, and the beneﬁcial effect\nof adding additional supervised signals on the quality of the learned representations.\n1 I NTRODUCTION\nSelf-supervised learning of text and speech representations has been particularly impactful in natural\nlanguage processing and speech processing. Since GPT (Radford et al., 2018), BERT (Devlin et al.,\n2019) and their variations (Yang et al., 2019; Conneau & Lample, 2019; Lewis et al., 2020; Raffel et al.,\n2020; Joshi et al., 2020), performance on natural language understanding downstream tasks (Socher\net al., 2013; Rajpurkar et al., 2016; Agirre et al., 2007; Williams et al., 2018) and monolingual (e.g.,\nGLUE (Wang et al., 2019b), SuperGLUE (Wang et al., 2019a)) and multilingual (e.g., XTREME (Hu\net al., 2020), XTREME-R (Ruder et al., 2021)) benchmarks has largely improved thanks to evolving\npre-trained models, which leverage increasing amounts of unannotated data (Radford et al., 2019;\nLiu et al., 2019; Conneau et al., 2019; Wenzek et al., 2020; Xue et al., 2021b) and increased model\ncapacity (Brown et al., 2020; Xue et al., 2021b; Lepikhin et al., 2020; Fedus et al., 2021). Similarly\nfor speech, unsupervised pre-training has emerged as a predominant approach. Wav2vec 2.0 (Baevski\net al., 2020b) and newer variants (Zhang et al., 2020) initially showed the strength of pre-training\non speech recognition (Panayotov et al., 2015; Kahn et al., 2020; Zhang et al., 2021) on multiple\ndomains (Hsu et al., 2021) and languages (Conneau et al., 2020).\nSelf-supervised learning methods in language understanding are designed to be used universally, i.e.\na single large pre-trained model for all domains and languages. One big advantage of these universal\n∗Equal contribution. Correspondence to {ankurbpn, aconneau, ngyuzh}@google.com.\n1\narXiv:2110.10329v1  [cs.CL]  20 Oct 2021\nPreprint\nmodels is the ability to leverage data skew across domains, tasks and languages; the availability of\ntask or domain-speciﬁc data in one language can boost model performance for several languages that\nthe model was pre-trained on. Extending this generalization capability across modalities by having\nneural networks understand both text and speech at the same time is a natural next step.\nJointly pre-training models on speech and text is a natural choice for multimodal self-supervised\nlearning, given the similarities between the two modalities and the abundance of unannotated text\ndata compared to speech. Recent work has also shown that self-supervised speech representations\ncan be aligned to text with little to no supervision (Baevski et al., 2021), suggesting the possibility\nof learning both modalities within a single neural network. However, past work in multilingual\nmodeling in particular has demonstrated the difﬁculty of learning representations of different data\nstructures, however similar, within a shared network, exposing the so-called transfer interference\nproblem (Arivazhagan et al., 2019). We show in this work that this trade-off also applies to joint\nspeech-text self-supervised learning.\nWe study a new multimodal speech-text pre-training approach that leverages data from one modality\nto improve representations of the other, but also suffers from transfer interference and capacity\ndilution. Our Speech and LAnguage Model (SLAM) consists of a single Conformer (Gulati et al.,\n2020) trained with the SpanBERT objective for text (Joshi et al., 2020) and the w2v-BERT (Chung\net al., 2021) objective for speech. We show that a model using only self-supervised objectives\nleads to good performance on both modalities, but is outperformed by mono-modal pre-trained\nmodels, suffering from signiﬁcant transfer interference. To reduce the gap, we leverage supervised\nalignment losses, speciﬁcally a translation language model (Conneau & Lample, 2019; Zheng\net al., 2021) and speech-text matching (Li et al., 2021) loss. We train our model in a multi-task\nfashion with the self-supervised and alignment losses. This leads to performance competitive with\nthe state-of-the-art on SpeechStew and LibriSpeech ASR and on CoV oST 2 speech translation tasks.\nOn speech translation, we demonstrate further quality improvements by continuing pre-training on\nspeech-only, outperforming previous approaches by 1 BLEU on average. On text tasks, our joint\nmodel loses quality compared to equivalent mono-modal pre-trained models, but remains competitive\nwith initial BERT results (Devlin et al., 2019), demonstrating the capacity limitations with modeling\ntwo high-resource modalities simultaneously. To the best of our knowledge, our work is the ﬁrst\nto study and underline the beneﬁts and limitations of speech-text unsupervised pre-training over\nmono-modal models, on various speech and text downstream tasks. Our initial results set a new\nchallenge in multimodal self-supervised language understanding.\n2 R ELATED WORK\nSelf-supervised learning of language representations using neural networks has a long history. In\nthe deep learning era, word2vec (Mikolov et al., 2013) initially trained word representations from\nunannotated data using noise contrastive estimation (Gutmann & Hyv ¨arinen, 2012; Mnih & Teh,\n2012). Word2vec was followed by a series of papers that expanded the approach to contextual\nrepresentations of sentences, including ELMo (Peters et al., 2018), GPT (Radford et al., 2018),\nBERT (Devlin et al., 2019) and T5 (Raffel et al., 2019). They rely on either generative language\nmodelling (Bengio et al., 2003) or masked language modeling (Taylor, 1953) (MLM) and these\nself-supervised pre-training approaches have led to signiﬁcant improvements on a wide variety of\ndownstream tasks (Wang et al., 2019b; Hu et al., 2020).\nIn parallel, similar approaches were explored in speech understanding. Chung et al. (2016) follows\nthe word2vec approach to learn vector representations of variable-length audio segments. Oord\net al. (2018) introduces contrastive predictive coding (CPC) which leverages language modeling and\nnegative sampling to learn speech representations. The ﬁrst wav2vec model (Schneider et al., 2019)\nclosely follows this architecture using a noise contrastive binary classiﬁcation task for unsupervised\npre-training. vq-wav2vec (Baevski et al., 2020a) proposes to add a vector quantizer similar to\nVQ-V AE (van den Oord et al., 2018), using Gumbel softmax (Jang et al., 2016) or online k-means\nclustering to quantize the dense speech representations (Eloff et al., 2019). When quantized, speech\nutterances become sequences of discrete tokens belonging to a ﬁxed vocabulary, similar to text, on\nwhich BERT is applied. wav2vec 2.0 merges those two separate steps (quantization and contrastive\nlearning) into a uniﬁed end-to-end learning procedure that pre-trains a Transformer model. They\nshow signiﬁcant gains on LibriSpeech (Panayotov et al., 2015) as well as on few-shot learning for\n2\nPreprint\nlow-resource languages (Conneau et al., 2020). w2v-BERT (Chung et al., 2021) expands wav2vec 2.0\nby combining contrastive learning and MLM. Zhang et al. (2020) and BigSSL (Zhang et al., 2021)\nexplore the limits of large-scale semi-supervised learning with Conformers (Gulati et al., 2020).\nOne approach to utilize data across modalities could involve synthetically transforming the modality\nof the data; one example being Chen et al. (2021) where the authors utilize text-to-speech (TTS) to\ntransform text data into speech, and utilize it for monomodal speech pre-training. Recent advances\nin self-supervised learning for text, speech and images have led to a new frontier: multimodal\nself-supervised learning, where a single model learns representations of all modalities using both\nunannotated and aligned data. V ATT Transformer (Akbari et al., 2021) leverages datasets of more\nthan 100M video-audio-text triplets to learn representations on all modalities at once with noise\ncontrastive estimation. Li et al. (2021) jointly learns to do masked language modeling on text as\nwell as matching image representations to text with parallel data through alignment losses. Jia et al.\n(2021) learns language representation for text-to-speech synthesis by jointly training on phoneme and\ngrapheme representations with MLM. Perhaps most similar to our work, Zheng et al. (2021) learn\njoint speech-text representations by adapting a translation language modeling (TLM) loss (Conneau\n& Lample, 2019) to the speech-text setting and studies downstream effect on speech translation.\nThis work investigates the possibility of developing truly multimodal pre-trained models building\non state-of-the-art speech and text pre-training approaches, and highlights the advantages and chal-\nlenges associated with multimodal pre-trained models by evaluating on a variety of speech and text\ndownstream tasks.\n3 M ETHOD\nIn this section, we describe each component of our speech-text pre-training framework, SLAM,\nstarting with the model architecture in Section 3.1. We then present the pre-training objectives and\nour multi-stage pre-training strategy in Sections 3.2 and 3.4, followed by introducing the pre-training\ndata in Section 4.1. Figure 1 illustrates the overall pre-training framework.\n3.1 M ODEL ARCHITECTURE\nOur model contains a speech encoder, a text encoder, and a multimodal encoder. At a high level, the\nspeech and text encoders take speech and text signals as input respectively and extract latent features\nfrom them. The latent features from the two modalities are then fed to the same multimodal encoder\nfor learning speech-text joint representations. Next, we describe each of these components.\nSpeech Encoder The speech encoder is composed of a convolutional feature encoder followed\nby a stack of Conformer layers (Gulati et al., 2020), each of which is a series of multi-headed\nself attention (Vaswani et al., 2017), depth-wise convolutions, and feed-forward layers. Given an\nacoustic feature sequence A = (a1, a2, ..., aN) (we use 80-dimensional log Mel spectrograms in this\nwork, i.e., ai ∈ R80), the feature encoder—which consists of two 2D-convolutional layers 1 both\nwith strides (2, 2)—acts as a sub-sampling block that extracts latent speech representations X =\n(x1, x2, ..., xT ) with a 4x reduction in the sequence length from the input A. A linear projection layer\nis then applied to map the latent representations’ dimensionality to that of the subsequent Conformer\nstack, which further extracts higher-level contextualized speech representations C = (c1, c2, ..., cT )\nfrom the feature encoder output X. For the Conformer stack, we follow the layout described in Gulati\net al. (2020), with a model dimension of 1024, feedforward hidden dimension of 4096, convolution\nkernel size 5 and 8 attention heads. Different from Gulati et al. (2020), we use group normalization\ninstead of batch norm in our convolution layers which was empirically found to perform better\non multimodal mixed batch training. For all our w2v-BERT experiments we use 8 layers in the\nspeech-speciﬁc conformer layer stack (N = 8in Figure 1). Outputs of the speech encoder are layer\nnormalized before being fed to the multimodal encoder. When training with wav2vec 2.0, we skip\nthe additional Conformer block in the speech encoder.\n1We combine information along the time and the Mel spectrogram frequency dimensions.\n3\nPreprint\nLanguage understanding \n(NLI, QA, GLUE)\nConformer layer\nConformer layer\nRushing is useless; one has to leave on time. To such Truth witness is given by theTortoise and the Hare. \"Let’s make a bet,\" the former once said, \"that you won’ttouch That line as soon as I.\" \"As soon? Are you all there, Neighbor?\" said the rapidbeast. \"You need a purge: four grains at least Of hellebore, you’re now so fargone.\" \"All there or not, the bet’s still on.\" So it was done; the wagers of thetwo Were placed at the finish, in view. It doesn’t matter what was down atstake, Nor who was the judge that they got. Our Hare had, at most, four steps or soto take. \nSpeech ConvNet layers\nText embedding layer\nRushing is useless; one has to leave on time. To such Truth witness is given by theTortoise and the Hare. \"Let’s make a bet,\" the former once said, \"that you won’ttouch That line as soon as I.\" \"As soon? Are you all there, Neighbor?\" said the rapidbeast. \"You need a purge: four grains at least Of hellebore, you’re now so fargone.\" \"All there or not, the bet’s still on.\" So it was done; the wagers of thetwo Were placed at the finish, in view. It doesn’t matter what was down atstake, Nor who was the judge that they got. Our Hare had, at most, four steps... \nConformer layer\nConformer layer\n...\nSpeech or text downstream tasks\nRushing is useless; one has to leave on time. To such Truth witness is given by the Tortoise and the Hare. \"Let’s make a bet,\" the former once said, \"thatyou won’t touch That line as soon as I.\" \"As soon? Are you all there, Neighbor?\" said the rapid beast. \"You need a purge: four grains at least Of hellebore,you’re now so far gone.\" \"All there or not, the bet’s still on.\" So it was done; the wagers of the two Were placed at the finish, in view. It doesn’t matterwhat was down at stake, Nor who was the judge that they got. Our Hare had, at most, four steps or so to take. \nSpeech ConvNet layers\nText embedding layer\nConformer layer\nSelf-Supervised LearningSupervised Alignment\nUnsupervised speech-text pretraining\nConformer layer\nx N\nx M\nContrastive vectors\nContext vectors\nConformer layer\nConformer layer...\nRushing is useless; one has to leave on time. To such Truth witness is given by the Tortoise and the Hare. \"Let’s make a bet,\" the former once said, \"thatyou won’t touch That line as soon as I.\" \"As soon? Are you all there, Neighbor?\" said the rapid beast. \"You need a purge: four grains at least Of hellebore,you’re now so far gone.\" \"All there or not, the bet’s still on.\" So it was done; the wagers of the two Were placed at the finish, in view. It doesn’t matterwhat was down at stake, Nor who was the judge that they got. Our Hare had, at most, four steps or so to take. \nTLM STM\nW2v-BERT \nContrastive \nW2v-BERT \nMLM BERT\nSpeech understanding \n(ASR, AST) \nConformer layer\nConformer layer\nRushing is useless; o ne has to leave on time. T o such T ruth witness is giv en by the\nT ortoise and the Hare. \"Let’ s make a bet,\"  the former once said, \"that you won’ t\ntouch That line as soon as I.\" \"As soon? Are you all there, Neighbor?\" said the rapid\nbeast. \"Y ou need a pur ge: four grains at least Of hellebore, you’re now so far\ngone.\" \"All there or not, the bet’ s still on.\" So it was done; the wagers of the\ntwo W ere placed at the finish, in view . It doesn’ t matter what was down at stake, Nor\nwho was the judge that they got. Our Hare had, at most, four steps or so to take. \nSpeech ConvNet layers\nT ext embedding layer\nConformer layer\nConformer layer\n...\nRushing is useless; o ne has to leave on time. T o such T ruth witness is giv en by the\nT ortoise and the Hare. \"Let’ s make a bet,\"  the former once said, \"that you won’ t\ntouch That line as soon as I.\" \"As soon? Are you all there, Neighbor?\" said the rapid\nbeast. \"Y ou need a pur ge: four grains at least Of hellebore, you’re now so far\ngone.\" \"All there or not, the bet’ s still on.\" So it was done; the wagers of the\ntwo W ere placed at the finish, in view . It doesn’ t matter what was down at stake, Nor\nwho was the judge that they got. Our Hare had, at most, four steps or so to take. \nFigure 1: (Left)Our model consists of a text embedding layer and a speech-speciﬁc stack similar to w2v-\nBERT, the latter consisting of a ConvNet and a series ofN Conformer layers. Both the text and speech output\nembeddings are fed to a series of N shared Conformer layers. Our unsupervised speech-text pre-training\napproach consists of self-supervised learning objectives (in blue), including w2v-BERT masked language\nmodeling and contrastive losses, as well as the text BERT objective. This can be combined with supervised\nalignment losses (in red) which leverage speech-text annotated pairs. We leverage in particular the MLM variant\nof translation language modeling (TLM) and the ranking loss of speech-text matching (STM). (Right) Once\npre-trained, the speech part of the shared architecture can be ﬁne-tuned on speech understanding datasets like\nrecognition or translation. The text part of the architecture can be ﬁne-tuned on language understanding tasks.\nText Encoder The text encoder is a simple token embedding layer that transforms input text into a\nsequence of token vector embeddings W = (w1, w2, ..., wT′ ). 2 The textual tokens are combined\nwith sinusoidal positional encodings and layer normalized before being fed to the multimodal encoder.\nWe utilize a SentencePiece model (Kudo & Richardson, 2018) with a 32k token vocabulary.\nMultimodal Encoder The multimodal encoder is a deep stack of Conformer layers that can take\neither just speech, or just text, or concatenated speech-text pairs as input. The Conformer layers\nused in the multimodal encoder are identical to the ones used in the speech encoder. When training\nwith w2v-BERT we use M = 16Conformer layers in the multimodal stack, while we use M = 24\nlayers when training with wav2vec 2.0. Depending on the type of input - i.e. just speech, text or a\nspeech-text pair - the model is tasked to solve different self-supervised pre-training objectives.\n3.2 P RE-TRAINING OBJECTIVES\nWe pre-train the model with four objectives: SpanBERT (Joshi et al., 2020) on unlabeled text,\nw2v-BERT (Chung et al., 2021) on unlabeled speech, Translation Language Modeling (Conneau &\nLample, 2019; Zheng et al., 2021) on paired speech and text data, and Speech-Text Matching (Li\net al., 2021) on paired and non-paired speech and text data.\n3.2.1 S ELF -SUPERVISED LEARNING OBJECTIVES\nWe use two self-supervised learning objectives that are trained on unannotated text or speech data.\nBERT is the self-supervised learning objective applied to unannotated text input (Devlin et al., 2019).\nIt aims to learn contextualized textual representations via solving a masked language modeling (MLM)\ntask. We mask spans of text as in SpanBERT (Joshi et al., 2020).\nw2v-BERT is the self-supervised learning objective used for pre-training on unannotated speech\ndata (Chung et al., 2021). It combines contrastive learning and MLM, where the former trains the\nmodel to discretize continuous speech signals into a ﬁnite set of discriminative speech tokens, and the\nlatter trains the model to learn contextualized speech representations via solving a masked prediction\ntask consuming the discretized tokens.\n2We evaluated using a deep Transformer or Conformer stack for the text encoder but did not ﬁnd it empirically\nuseful for speech translation or ASR.\n4\nPreprint\n3.2.2 A LIGNMENT LOSSES\nWithout the presence of paired data, the only incentive for the model to learn joint representation is the\ninductive bias of having a shared set of Conformer layers. Because this is such a strong assumption,\nwe also leverage alignment losses, which use speech-text paired ASR data to explicitly incentivize the\nmodel to share representations within the model. We will see below that this leads to better alignment\nbetween the speech and text representations, as indicated by better performance on downstream tasks.\nTranslation Language Modeling (TLM) was ﬁrst introduced to align representations between\ntwo languages within a shared Transformer. With TLM, parallel sentences are concatenated and sent\nto a Transformer MLM which predicts missing words, encouraging the model to leverage context\nfrom both input languages. In this work, we concatenate speech utterances with their transcriptions\nusing ASR supervised data, similar to Zheng et al. (2021). We then train the model to predict masked\ntext or speech spans with BERT or w2v-BERT, encouraging the use of cross-modal context.\nSpeech-Text Matching (STM) predicts whether a pair of speech and text is positive (matched) or\nnegative (not matched). We use the multimodal encoder’s output embedding of the [CLS] token as\nthe joint representation of the speech-text pair, and append a fully-connected (FC) layer followed by\nsoftmax to predict a two-class probability pSTM. The STM loss is:\nLSTM = E(I,T )∼D\n[\nH\n(\nySTM, pSTM(I, T)\n)]\nwhere ySTM is a 2-dimensional one-hot vector representing the ground-truth label, and H is cross-\nentropy. The STM objective explicitly trains the model to align speech-text pairs, a signal which\nself-supervised learning cannot explicitly provide.\n3.3 I MPLEMENTATION DETAILS\nWhen the input only contains speech, the latent speech features X = (x1, x2, ..., xT ) extracted\nby the speech encoder are directly fed to the multimodal encoder as input. The speech branch of\nthe model (i.e., the speech encoder along with the multimodal encoder) is trained to optimize the\nw2v-BERT objective. Following Chung et al. (2021), we mask approximately 50% of the speech\nframes with spans of length 10. Analogously, when the input only contains text, the latent textual\nfeatures W = (w1, w2, ..., wT′ ) extracted by the text encoder are fed to the multimodal encoder as\ninput, and the text branch (i.e., the text encoder along with the multimodal encoder) is trained to\noptimize the SpanBERT objective. We mask 15% of text tokens with spans of length 5.\nWhen the input is a speech-text pair, the latent speech and text representations C and W extracted\nrespectively by the speech and text encoders are concatenated, forming a sequence with a total length\nof T + T′that is fed to the multimodal encoder as input. The multimodal encoder is then trained\nto simultaneously predict the masked speech features (as in the w2v-BERT objective) and masked\ntext features (SpanBERT). We use more aggressive masking when using paired data to increase\nthe difﬁculty-level of the task, and to encourage the multimodal encoder to learn to extract useful\nfeatures across modalities. We mask a single span consisting of 50% of text tokens, and multiple\nspans masking out 75% of the speech features when training with paired data.\nWe train the model simultaneously on all these objectives; at every training step the model is trained\non a batch of (i) unlabeled speech, (ii) unlabeled text, and (iii) paired speech and text. The gradients\nof all objectives are aggregated and used to update the model parameters.\n3.4 M ULTI -STAGE PRE-TRAINING\nIn practice, we ﬁnd that pre-training the model simultaneously with unpaired and paired data results\nin the model overﬁtting to the relatively small paired dataset. To avoid this we pre-train the model in\na multi-stage fashion, where we ﬁrst pre-train the model just on unpaired text and speech, and then\noptimize it with unpaired and paired data simultaneously. This multi-stage pre-training approach\nachieves better downstream performance than optimizing all four losses from scratch. Concretely, we\ntrain on 500k updates with the self-supervised losses, and between 250k and 500k additional steps\nwith the alignment losses. We observed improvements of 0.1 to 0.2 WER on LibriSpeech dev-other\nand 0.3 average BLEU on CoV oST 2 when using the multi-stage strategy as against training with all\nlosses from scratch. In all models that use TLM and/or STM, we utilize multi-stage pre-training.\n5\nPreprint\n4 E XPERIMENTS\nWe ﬁrst describe our pre-training and ﬁne-tuning setup, including the speech and text datasets used\nfor pre-training as well as all our downstream tasks. We then present our results, including ablations\nof our approach and comparisons between multimodal and mono-modal models.\n4.1 P RE-TRAINING DATA\nLibri-light (speech only): The Libri-light (LL-60k) dataset contains 60k hours of unlabeled speech\nand is used to pre-train all our Masked Speech Models (MSM). LL-60k is the most widely used\nlarge unsupervised speech corpus for various pre-training techniques. Each input speech sequence is\nconstructed by ﬁrst randomly selecting 32-64 seconds segments from the original utterance. From\nthese segments, a contiguous 32 second region is extracted from a random starting point on-the-ﬂy\nduring MSM pre-training as described in (Zhang et al., 2020)\nLibriLM (text only): The Librispeech text corpus comprises of nearly 803 million tokens from 40M\nutterances of ﬁltered text derived from 14.5K Project Gutenberg books (Panayotov et al., 2015).\nmC4-En (text only): The mC4-En dataset (Xue et al., 2021a) consists of multiple terabytes of\nEnglish text data, mined from CommonCrawl. The dataset is publicly available.3\nLibriSpeech (paired data): We use LibriSpeech (Panayotov et al., 2015) fullset (960h) as paired\ndata for Translation Language Modeling (TLM) and Speech-Text Matching (STM).\n4.2 D OWNSTREAM TASKS\nWe present results on publicly available, well-benchmarked downstream tasks including speech\nrecogntion, speech translation, text normalization and language understanding.\nSpeech translation: CoV oST 2 (Wang et al., 2021a) is a large-scale multilingual speech translation\ncorpus covering translations from 21 languages into English and from English into 15 languages.\nThis represents the largest open dataset available to date from total volume and language coverage\nperspective. Following Wang et al. (2021b), we use four English to X directions, speciﬁcally German,\nCatalan, Arabic and Turkish. To evaluate our pre-trained encoders on speech-translation, we ﬁne-tune\nit as part of a sequence-to-sequence model with a 4-layer Transformer decoder. The decoder uses a\n384 embedding dimension, 1536 feed-forward hidden dimension, 4 attention heads and a 8192 token\nmultilingual sub-word vocabulary.\nASR: SpeechStew has 6 public benchmarks, including LibriSpeech (Panayotov et al., 2015),\nAMI (Carletta et al., 2005), TEDLIUM (Rousseau et al., 2012), Common V oice (Ardila et al., 2019),\nSwitchboard/Fisher (Cieri et al., 2003) and Wall Street Journal (LDC93S6B, LDC94S13B). In our\nexperiments, LibriSpeech is the same domain as our pre-training speech data (Libri-light) and others\nare evaluated as out-of-domain corpora. Following Chan et al. (2021a), the whole dataset (approx. 5k\nhours) is used to perform our ﬁnetuning experiments. Each dataset used is speciﬁc to a certain target\ndata condition, for instance LS-960 closely matches LL-60k, AMI dataset is distinct from the LL-60k\ncondition and it contains speech from two kinds of microphones: (i) independent head microphone\n(IHM); and (ii) single distant microphone (SDM). SpeechStew is composed of datasets chosen from\nmultiple conditions to create a mixed domain aggregate corpus. Details of its processing are described\nin Chan et al. (2021a). To evaluate on ASR, we ﬁne-tune our encoder as a conformer-transducer\nmodel, following Chung et al. (2021).\nLanguage understanding: We consider four main tasks from the GLUE natural language under-\nstanding benchmark: the MNLI natural language inference benchmark (Williams et al., 2018), the\nQuora Question Pair (QQP) classiﬁcation dataset,4 the QNLI question answering task (Wang et al.,\n2019b) and the SST-2 sentiment analysis dataset (Socher et al., 2013). We report accuracy on the dev\nsets of each dataset (except SST-2 where we report test accuracy) and compare our results to BERT,\nSpanBERT and RoBERTa.\n3https://huggingface.co/datasets/mc4\n4https://www.kaggle.com/c/quora-question-pairs\n6\nPreprint\nTable 1:BLEU on CoV oST 2 speech translation comparing our speech-text pre-trained models against speech-\nonly pre-training and pre-existing baselines.\n# Model # Params Text data En-De En-Ca En-Ar En-Tr Avg\nPrior Work\n1 wav2vec-2.0 (Wang et al., 2021b) 300M - 23.8 32.4 17.4 15.4 22.3\n2 wav2vec-2.0 + LM (Wang et al., 2021b) - - 24.9 34.0 18.0 16.7 23.4\nOur Work Speech-only\n3 w2v-conformer 600M - 27.1 33.1 18.8 15.6 23.7\n4 w2v-bert 600M - 27.4 33.9 19.0 15.9 24.1\nOur Work Speech-text\n5 w2v-conformer + bert 600M mC4-En 25.4 30.5 18.5 15.2 22.4\n6 w2v-bert + bert (SLAM) 600M mC4-En 26.9 33.1 18.1 16.1 23.5\n7 SLAM-TLM 600M mC4-En 27.5 33.4 18.9 16.6 24.1\n8 SLAM-TLM-STM 600M mC4-En 27.2 33.3 18.5 16.8 24.0\nOur Work Speech-text→Speech-only\n9 SLAM-TLM-STM→w2v-bert 600M mC4-En 27.1 34.2 21.2 17.5 25.0\nText normalization: Text normalization — also referred to as text verbalization — is a core task\nin the text-to-speech (TTS) community. Text normalization takes as input raw unverbalized text—as\ntypically found in written form—and produces a verbalized form of that text, expanding numbers,\ndates, abbreviations, etc. The output of this task is a word-for-word spoken transcript—the input\nformat expected by TTS systems. For example, “A 1951 Lancia V6” would become “a nineteen\nﬁfty one lancia v six” while “Dial 1951 for 6V batteries” might become “dial one nine ﬁve one for\nsix volt batteries. ”. We consider the English task data from Sproat & Jaitly (2016) and compare our\nresults to those of Stahlberg & Kumar (2020), known to be previously state-of-the-art. We report\nsentence error-rate on the test set for all our experiments. When evaluating on text normalization,\nwe ﬁne-tune our encoder with a 3-layer transformer decoder with a model dimension of 512, hidden\ndimension of 1024 and 4 attention heads.\n4.3 M AIN RESULTS\nIn this section, we analyze the results of ﬁne-tuning our models on speech and text downstream tasks.\n4.3.1 S PEECH TRANSLATION\nWe present our results on CoV oST 2 En-X translation in Table 1. We compare our models against\nresults from Wang et al. (2021b), speciﬁcally against ﬁne-tuning wav2vec 2.0 on speech-translation,\nwith and without LM fusion. Our speech-only baselines trained using wav2vec 2.0 improve over\nWang et al. (2021b) by over 1 BLEU, possible due to increased encoder capacity. Our w2v-BERT\nspeech-only encoder further improves performance by around 0.4 BLEU.\nThe addition of mC4-En data to pre-training results in a drop of around 1.3 BLEU for the w2v-\nconformer, a concrete example of the interference issue. In comparison, the w2v-BERT speech-text\nmodel is only worse than its speech-only counterpart by approximately 0.6 BLEU. The addition\nof alignment losses results in the joint pre-trained model matching the speech-only baseline and\nalleviates interference.\nWe continue training our TLM + STM joint model on unlabeled speech-only data to alleviate the\ncapacity limitation in the multimodal pre-trained model. Fine-tuning this speech-adapted model on\nCoV oST results in a model that outperforms our best speech-only model by almost 1 BLEU point,\nillustrating positive cross-modal transfer and the advantages of multimodal pre-training.\n4.3.2 S PEECH RECOGNITION\nIn Table 2, we present our results on the Librispeech 960h ASR benchmark. We compare our uniﬁed\nspeech-text encoders to a number of state-of-the-art self-supervised representation learning methods\nfrom the literature, including wav2vec 2.0 (Baevski et al., 2020b) and w2v-bert (Chung et al., 2021).\nAs shown in Table 2, w2v-BERT is consistently better than w2v-Conformer with the text modality by\n17% relative (line 6 and 7). However, simply adding the text modality with LibriLM data hurts ASR\nperformance by 14% relative compared to the speech-only model, from 2.9 to 3.3 average WER (line\n7\nPreprint\nTable 2:WERs (%) when using the LibriSpeech 960h as supervised data. For all methods, both self-training\nand LM fusion are not used. References are where the numbers are quoted from.\n# Model #Params Text datadev dev-other test test-other\nPrior Work (no LM)1 wav2vec 2.0-CTC (Baevski et al., 2020b) 300M - 2.1 4.5 2.2 4.52 w2v-Conformer XL (Zhang et al., 2020) 600M - 1.7 3.5 1.7 3.53 w2v-bert XL (Chung et al., 2021) 600M - 1.5 2.9 1.5 2.94 w2v-bert XXL (Chung et al., 2021) 1B - 1.5 2.7 1.5 2.8\nOur Work Speech-only5 w2v-bert XL (Group Norm) 600M - 1.5 2.9 1.6 2.9Our Work Speech-text6 w2v-conformer + bert 600M LibriLM 1.7 4.0 1.9 4.07 w2v-bert + bert (SLAM) 600M LibriLM 1.6 3.2 1.7 3.39 SLAM-TLM (one stage) 600M LibriLM 1.6 3.1 1.7 3.28 SLAM-TLM 600M LibriLM 1.5 2.9 1.6 3.19 SLAM-TLM-STM 600M LibriLM 1.7 3.0 1.6 3.210 SLAM-TLM-STM 600M C4 1.7 3.2 1.7 3.2\nOur Work Speech-text→Speech-only11 SLAM-TLM-STM→w2v-bert 600M mC4-En 1.6 3.0 1.6 3.1\nTable 3:WERs (%) across multiple tasks and settings from the SpeechStew benchmark compared against\npre-existing baselines. †We follow Likhomanenko et al. (2020) and remove punctuations during evaluation.\n# Model #Params Text data AMI Common V oice† Switchboard/Fisher TED-LIUM WSJ Avg\nIHM SDM1 SWBD CH eval92\nPrior Work1 w2v-conformer (Chan et al., 2021b) 1B - 9.5 22.7 8.4 4.8 10.6 5.7 1.3 9.1\nOur Work Speech-only2 w2v-conformer 600M - 9.6 23.8 8.4 4.7 9.2 5.6 1.4 9.13 w2v-bert 600M - 9.1 23.1 8.6 4.5 9.0 5.4 1.3 8.7\nOur Work Speech+text5 w2v-bert + bert (SLAM) 600M LibriLM 9.4 24.3 9.7 4.8 11.0 5.9 1.5 9.56 SLAM-TLM 600M LibriLM 9.2 23.8 8.7 4.9 8.9 5.8 1.3 9.37 SLAM-TLM-STM 600M LibriLM 9.3 23.5 8.6 4.6 9.1 5.6 1.3 9.08 SLAM-TLM 600M mC4-En 9.4 24.7 8.9 4.8 9.2 5.1 1.5 9.19 SLAM-TLM-STM 600M mC4-En 9.5 25.3 9.0 4.6 9.0 5.3 1.5 9.2\n5 and 7) on testother. By adding TLM loss (line 8), we are able to reduce the interference and we\nbridge most of this gap, matching performance on dev/devother/test, and only 0.2% worse on testother\ncompared to the mono-modal model. We conclude that the alignment losses help the model align the\ntwo modalities, resulting in better use of shared parameters and reduction in the interference between\nthe speech and text modalities. Further introducing STM loss does not improve ASR performance\n(line 9), but it still performs better than the model without alignment losses. As we increase the\namount of text data from LibriLM to mC4-En (line 10), we observed a regression on devother and\ntestother. We conclude that the model needs more capacity to learn from the out-of-domain and larger\ntext dataset. Similar to speech translation, if we further pre-train the model with speech only data,\nthere is 0.1% consistent improvement over all the test sets (line 10 and 11).\nIn Table 3, we present our results on 5 ASR benchmarks using SpeechStew supervised data. Note that\nthe uniﬁed encoder model has not seen any paired data during pre-training on these out-of-domain\nbenchmarks. We notice that the alignment losses still improve over the baseline multimodal model\n(line 5 to 7). Interestingly, mC4-En data improves performance on TED-LIUM but is worse on AMI\ncompared to pre-training on LibriLM.5\n4.3.3 N ATURAL LANGUAGE UNDERSTANDING\nWe report results on four natural language understanding tasks from GLUE in Table 4. We com-\npare our methods to the original BERT model of Devlin et al. (2019) and its extended versions,\nSpanBERT (Joshi et al., 2020) and RoBERTa (Liu et al., 2019) which are trained on comparable\nobjectives and comparable text data respectively. We report dev results for MNLI, QNLI and QQP, as\ntest sets are not available for these tasks. We see that our SpanBERT-conformer text-only baseline\nobtains competitive results with SpanBERT but is outperformed by RoBERTa, possibly because of the\n5TED-LIUM is clean speech from the TED-talks domain and thus likely to beneﬁt from more text data,\nwhereas AMI has natural speech from meetings and might beneﬁt from additional capacity devoted to acoustic\nmodeling.\n8\nPreprint\nTable 4:Performance on four GLUE tasks and text-normalization against text-only baselines. We report dev\naccuracy scores for MNLI, QNLI and QQP, test accuracy for SST-2 and test sentence-error-rate for Text-Norm.\n# Model # Params Text data MNLI QNLI QQP SST-2 Text-Norm\nPrior Work\n1 BERT (Devlin et al., 2019) 340M Wiki+Books 86.6 92.3 91.3 93.2 -\n2 SpanBERT (Joshi et al., 2020) 340M Wiki+Books 87.0 93.3 - 94.8 -\n3 RoBERTa (Liu et al., 2019) 340M CC 90.2 94.7 92.2 96.4 -\n4 Seq2Edits (Stahlberg & Kumar, 2020) - - - - - - 1.36\nOur Work Text-only\n5 SpanBERT-conformer 450M mC4-En 87.9 92.6 91.8 95.4 1.11\nOur Work Speech-text\n6 w2v-bert + bert (SLAM) 450M mC4-En 82.3 88.4 90.6 94.2 1.30\n7 SLAM-TLM 450M mC4-En 83.6 90.1 91.0 94.3 1.28\n8 SLAM-TLM-STM 450M mC4-En 83.3 90.0 91.0 93.9 1.19\nConformer architecture and the optimized pre-training and ﬁne-tuning of the RoBERTa architecture.\nDoing an apples-to-apples comparison of our text-only model and our speech-text architectures,\nwe observe signiﬁcant decrease in performance when adding the speech modality. On MNLI, for\ninstance, we go from 87.9% accuracy (line 5) down to 83.3% accuracy with our full model (line 8), or\nfrom 95.4 on SST-2 to 93.9%. We observe some gains in performance when using alignment losses\nover the fully self-supervised learning approach (line 6) which only slightly alleviates the interference\nproblem. Given the large amount of data in both speech and text for English, it is likely that the\ncapacity of the model is a limiting factor for understanding both modalities simultaneously. We\nbelieve that alleviating capacity limitations by inducing better cross-modal alignments is an important\nchallenge. We leave the investigation of larger-capacity models and lower-resource languages for\nfuture work.\n4.3.4 T EXT NORMALIZATION\nIn addition to GLUE, we also evaluate and report sentence error rate for text normalization and\ncompare our approach to Stahlberg & Kumar (2020) in Table 4. Our baseline text-only model\nimproves over the previous state-of-the-art by 0.25% absolute (lines 5 and 6). Adding speech during\npre-training results in worse performance compared to our text-only pre-training, but the addition of\nTLM and STM alignment losses is able to recover some of the lost quality (lines 6 to 8). Based on\nthis, we suspect that future work in cross-modality alignment may yield improvements on this task.\n5 D ISCUSSION\nIn this work, we demonstrate that a single encoder model can be pre-trained to learn strong contex-\ntualized representations of speech and text simultaneously. We combine self-supervised learning\nobjectives for text (BERT) and self-supervised approaches for speech (w2v-BERT) to learn a joint\nSpeech and LAnguage Model (SLAM). Downstream evaluations on speech and language understand-\ning tasks, including LibriSpeech and SpeechStew ASR, CoV oST 2 speech translation, four GLUE\ntasks, and text-normalization uncover signiﬁcant interference challenges when pre-training simulta-\nneously on high-resource modalities. Using alignment losses such as translation language modeling\nand speech-text matching which leverage speech-text supervised aligned data, we show that we can\nimprove the cross-modal representation alignment and improve over mono-modal models on the\nspeech translation tasks, while maintaining state-of-the-art performance on speech recognition. We\nhope that this work would motivate further research on extending the universality of self-supervised\nlearning of language representations to the multimodal speech-text setting.\n9\nPreprint\nREFERENCES\nEneko Agirre, Llu´ıs M`arquez, and Richard Wicentowski (eds.). Proceedings of the Fourth Inter-\nnational Workshop on Semantic Evaluations (SemEval-2007) . Association for Computational\nLinguistics, Prague, Czech Republic, June 2007.\nHassan Akbari, Linagzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, and Boqing\nGong. Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text.\narXiv preprint arXiv:2104.11178, 2021.\nRosana Ardila et al. Common voice: A massively-multilingual speech corpus. arXiv preprint\narXiv:1912.06670, 2019.\nNaveen Arivazhagan, Ankur Bapna, Orhan Firat, Dmitry Lepikhin, Melvin Johnson, Maxim Krikun,\nMia Xu Chen, Yuan Cao, George Foster, Colin Cherry, et al. Massively multilingual neural machine\ntranslation in the wild: Findings and challenges. arXiv preprint arXiv:1907.05019, 2019.\nAlexei Baevski, Steffen Schneider, and Michael Auli. vq-wav2vec: Self-supervised learning of\ndiscrete speech representations, 2020a.\nAlexei Baevski, Henry Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A framework\nfor self-supervised learning of speech representations. In NeurIPS, 2020b.\nAlexei Baevski, Wei-Ning Hsu, Alexis Conneau, and Michael Auli. Unsupervised speech recognition.\narXiv preprint arXiv:2105.11084, 2021.\nYoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. A neural probabilistic\nlanguage model. The journal of machine learning research, 3:1137–1155, 2003.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. arXiv preprint arXiv:2005.14165, 2020.\nJean Carletta et al. The ami meeting corpus: A pre-announcement. In International workshop on\nmachine learning for multimodal interaction. Springer, 2005.\nWilliam Chan, Daniel Park, Chris Lee, Yu Zhang, Quoc Le, and Mohammad Norouzi. SpeechStew:\nSimply mix all available speech recognition data to train one large neural network. arXiv preprint\narXiv:2104.02133, 2021a.\nWilliam Chan, Daniel Park, Chris Lee, Yu Zhang, Quoc Le, and Mohammad Norouzi. Speechstew:\nSimply mix all available speech recognition data to train one large neural network. arXiv preprint\narXiv:2104.02133, 2021b.\nZhehuai Chen, Yu Zhang, Andrew Rosenberg, Bhuvana Ramabhadran, Gary Wang, and Pedro\nMoreno. Injecting text in self-supervised speech pretraining, 2021.\nYu-An Chung, Chao-Chung Wu, Chia-Hao Shen, Hung-Yi Lee, and Lin-Shan Lee. Audio word2vec:\nUnsupervised learning of audio segment representations using sequence-to-sequence autoencoder.\nIn Interspeech, 2016.\nYu-An Chung, Yu Zhang, Wei Han, Chung-Cheng Chiu, James Qin, Ruoming Pang, and Yonghui Wu.\nw2v-BERT: Combining contrastive learning and masked language modeling for self-supervised\nspeech pre-training. In ASRU, 2021.\nChristopher Cieri, David Miller, and Kevin Walker. From switchboard to ﬁsher: Telephone collection\nprotocols, their uses and yields. In Eurospeech, 2003.\nAlexis Conneau and Guillaume Lample. Cross-lingual language model pretraining. In NeurIPS,\n2019.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Fran-\ncisco Guzm´an, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised\ncross-lingual representation learning at scale. In ACL, 2019.\n10\nPreprint\nAlexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, and Michael Auli.\nUnsupervised cross-lingual representation learning for speech recognition. arXiv preprint\narXiv:2006.13979, 2020.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. In NAACL-HLT, 2019.\nRyan Eloff, Andr´e Nortje, Benjamin van Niekerk, Avashna Govender, Leanne Nortje, Arnu Pretorius,\nElan Van Biljon, Ewald van der Westhuizen, Lisa van Staden, and Herman Kamper. Unsupervised\nacoustic unit discovery for speech synthesis using discrete latent-variable neural networks. arXiv\npreprint arXiv:1904.07556, 2019.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter\nmodels with simple and efﬁcient sparsity, 2021.\nAnmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo\nWang, Zhengdong Zhang, Yonghui Wu, and Ruoming Pang. Conformer: Convolution-augmented\ntransformer for speech recognition. In Interspeech, 2020.\nMichael U Gutmann and Aapo Hyv¨arinen. Noise-contrastive estimation of unnormalized statistical\nmodels, with applications to natural image statistics. Journal of Machine Learning Research, 13\n(2), 2012.\nWei-Ning Hsu, Anuroop Sriram, Alexei Baevski, Tatiana Likhomanenko, Qiantong Xu, Vineel Pratap,\nJacob Kahn, Ann Lee, Ronan Collobert, Gabriel Synnaeve, et al. Robust wav2vec 2.0: Analyzing\ndomain shift in self-supervised pre-training. arXiv preprint arXiv:2104.01027, 2021.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and Melvin Johnson.\nXtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation.\nIn International Conference on Machine Learning, pp. 4411–4421. PMLR, 2020.\nEric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv\npreprint arXiv:1611.01144, 2016.\nYe Jia, Heiga Zen, Jonathan Shen, Yu Zhang, and Yonghui Wu. Png bert: Augmented bert on\nphonemes and graphemes for neural tts. In Interspeech, 2021.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy. Spanbert:\nImproving pre-training by representing and predicting spans. Transactions of the Association for\nComputational Linguistics, 8:64–77, 2020.\nJ. Kahn, M. Rivi`ere, W. Zheng, E. Kharitonov, Q. Xu, P. E. Mazar ´e, J. Karadayi, V . Liptchinsky,\nR. Collobert, C. Fuegen, T. Likhomanenko, G. Synnaeve, A. Joulin, A. Mohamed, and E. Dupoux.\nLibri-light: A benchmark for asr with limited or no supervision. In ICASSP 2020 - 2020 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 7669–7673,\n2020. https://github.com/facebookresearch/libri-light.\nTaku Kudo and John Richardson. SentencePiece: A simple and language independent subword\ntokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing: System Demonstrations, pp. 66–71, Brussels,\nBelgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-2012.\nURL https://aclanthology.org/D18-2012.\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,\nMaxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional\ncomputation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy,\nVes Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for\nnatural language generation, translation, and comprehension. In ACL, 2020.\nJunnan Li, Ramprasaath R. Selvaraju, Akhilesh Deepak Gotmare, Shaﬁq Joty, Caiming Xiong, and\nSteven Hoi. Align before fuse: Vision and language representation learning with momentum\ndistillation, 2021.\n11\nPreprint\nTatiana Likhomanenko, Qiantong Xu, Vineel Pratap, Paden Tomasello, Jacob Kahn, Gilad Avidov,\nRonan Collobert, and Gabriel Synnaeve. Rethinking Evaluation in ASR: Are Our Models Robust\nEnough? arXiv preprint arXiv:2010.11745, 2020.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining\napproach. arXiv preprint arXiv:1907.11692, 2019.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. Distributed representations\nof words and phrases and their compositionality. In NIPS, 2013.\nAndriy Mnih and Yee Whye Teh. A fast and simple algorithm for training neural probabilistic\nlanguage models, 2012.\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive\ncoding. arXiv preprint arXiv:1807.03748, 2018.\nVassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: An ASR\ncorpus based on public domain audio books. In ICASSP, 2015.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and\nLuke Zettlemoyer. Deep contextualized word representations. arXiv preprint arXiv:1802.05365,\n2018.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-\nstanding by generative pre-training. 2018.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text\ntransformer. arXiv preprint arXiv:1910.10683, 2019.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text\ntransformer. Journal of Machine Learning Research, 21:1–67, 2020.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions\nfor machine comprehension of text. In Proceedings of EMNLP, pp. 2383–2392. Association for\nComputational Linguistics, 2016.\nAnthony Rousseau, Paul Del´eglise, and Yannick Esteve. Ted-lium: an automatic speech recognition\ndedicated corpus. In LREC, pp. 125–129, 2012.\nSebastian Ruder, Noah Constant, Jan Botha, Aditya Siddhant, Orhan Firat, Jinlan Fu, Pengfei Liu,\nJunjie Hu, Graham Neubig, and Melvin Johnson. Xtreme-r: Towards more challenging and\nnuanced multilingual evaluation. arXiv preprint arXiv:2104.07412, 2021.\nSteffen Schneider, Alexei Baevski, Ronan Collobert, and Michael Auli. wav2vec: Unsupervised\npre-training for speech recognition. arXiv preprint arXiv:1904.05862, 2019.\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and\nChristopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank.\nIn Proceedings of EMNLP, pp. 1631–1642, 2013.\nRichard Sproat and Navdeep Jaitly. RNN approaches to text normalization: A challenge. CoRR,\nabs/1611.00068, 2016. URL http://arxiv.org/abs/1611.00068.\nFelix Stahlberg and Shankar Kumar. Seq2Edits: Sequence transduction using span-level edit op-\nerations. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP), pp. 5147–5159, Online, November 2020. Association for Computational\nLinguistics. URL https://aclanthology.org/2020.emnlp-main.418.\n12\nPreprint\nWilson L Taylor. “cloze procedure”: A new tool for measuring readability. Journalism quarterly, 30\n(4):415–433, 1953.\nAaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning,\n2018.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,Łukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel R Bowman. Superglue: A stickier benchmark for general-purpose language\nunderstanding systems. arXiv preprint arXiv:1905.00537, 2019a.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.\nGLUE: A multi-task benchmark and analysis platform for natural language understanding. In\nICLR, 2019b.\nChanghan Wang, Anne Wu, and Juan Pino. CoV oST 2 and massively multilingual speech-to-text\ntranslation. In interspeech, 2021a.\nChanghan Wang, Anne Wu, Juan Pino, Alexei Baevski, Michael Auli, and Alexis Conneau. Large-\nscale self-and semi-supervised learning for speech translation. arXiv preprint arXiv:2104.06678,\n2021b.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzm´an,\nArmand Joulin, and ´Edouard Grave. CCNet: Extracting high quality monolingual datasets from\nweb crawl data. In LREC, 2020.\nAdina Williams, Nikita Nangia, and Samuel R. Bowman. A broad-coverage challenge corpus for\nsentence understanding through inference. In Proceedings of NAACL-HLT, 2018.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\nBarua, and Colin Raffel. mt5: A massively multilingual pre-trained text-to-text transformer. In\nNAACL, 2021a.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\nBarua, and Colin Raffel. mT5: A massively multilingual pre-trained text-to-text transformer.\nIn Proceedings of the 2021 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, pp. 483–498, Online, June 2021b.\nAssociation for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.41. URL https:\n//aclanthology.org/2021.naacl-main.41.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R. Salakhutdinov, and Quoc V . Le.\nXLNet: Generalized autoregressive pretraining for language understanding. In NeurIPS, 2019.\nYu Zhang, James Qin, Daniel S. Park, Wei Han, Chung-Cheng Chiu, Ruoming Pang, Quoc V . Le,\nand Yonghui Wu. Pushing the limits of semi-supervised learning for automatic speech recognition.\narXiv preprint arXiv:2010.10504, 2020.\nYu Zhang, Daniel S Park, Wei Han, James Qin, Anmol Gulati, Joel Shor, Aren Jansen, Yuanzhong\nXu, Yanping Huang, Shibo Wang, Zongwei Zhou, Bo Li, Min Ma, William Chan, Jiahui Yu,\nYongqiang Wang, Liangliang Cao, Khe Chai Sim, Bhuvana Ramabhadran, Tara N. Sainath,\nFranc ¸oise Beaufays, Zhifeng Chen, Quoc V . Le, Chung-Cheng Chiu, Ruoming Pang, and Yonghui\nWu. BigSSL: Exploring the frontier of large-scale semi-supervised learning for automatic speech\nrecognition. arXiv preprint arXiv:2109.13226, 2021.\nRenjie Zheng, Junkun Chen, Mingbo Ma, and Liang Huang. Fused acoustic and text encoding for\nmultimodal bilingual pretraining and speech translation, 2021.\n13",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.805705189704895
    },
    {
      "name": "Speech recognition",
      "score": 0.6019189953804016
    },
    {
      "name": "Natural language processing",
      "score": 0.5817602872848511
    },
    {
      "name": "Encoder",
      "score": 0.5732764601707458
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.5602481961250305
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5383386015892029
    },
    {
      "name": "Language model",
      "score": 0.530473530292511
    },
    {
      "name": "Machine translation",
      "score": 0.4635763466358185
    },
    {
      "name": "Normalization (sociology)",
      "score": 0.455333411693573
    },
    {
      "name": "Training set",
      "score": 0.42955392599105835
    },
    {
      "name": "Modalities",
      "score": 0.4105146527290344
    },
    {
      "name": "Anthropology",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Social science",
      "score": 0.0
    }
  ],
  "institutions": []
}