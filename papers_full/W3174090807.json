{
  "title": "Ethical-Advice Taker: Do Language Models Understand Natural Language Interventions?",
  "url": "https://openalex.org/W3174090807",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2125627063",
      "name": "Jie-Yu Zhao",
      "affiliations": [
        "University of California, Los Angeles"
      ]
    },
    {
      "id": "https://openalex.org/A2039793395",
      "name": "Daniel Khashabi",
      "affiliations": [
        "Allen Institute",
        "Seattle University"
      ]
    },
    {
      "id": "https://openalex.org/A1838144096",
      "name": "Tushar Khot",
      "affiliations": [
        "Allen Institute",
        "Seattle University"
      ]
    },
    {
      "id": "https://openalex.org/A1898665253",
      "name": "Ashish Sabharwal",
      "affiliations": [
        "Allen Institute",
        "Seattle University"
      ]
    },
    {
      "id": "https://openalex.org/A2208999240",
      "name": "Kai-Wei Chang",
      "affiliations": [
        "University of California, Los Angeles"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6631349028",
    "https://openalex.org/W3120860016",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4298343034",
    "https://openalex.org/W2950888501",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3154200459",
    "https://openalex.org/W3121904249",
    "https://openalex.org/W2990704537",
    "https://openalex.org/W3105928338",
    "https://openalex.org/W3116459227",
    "https://openalex.org/W3100452485",
    "https://openalex.org/W3034830866",
    "https://openalex.org/W3099655892",
    "https://openalex.org/W2963383094",
    "https://openalex.org/W3037697022",
    "https://openalex.org/W3102749280",
    "https://openalex.org/W2982756474",
    "https://openalex.org/W3118905363",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W3099635335",
    "https://openalex.org/W2997588435",
    "https://openalex.org/W3089631405",
    "https://openalex.org/W2119409989",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2997607995",
    "https://openalex.org/W3105042180",
    "https://openalex.org/W2921633540",
    "https://openalex.org/W3037831233",
    "https://openalex.org/W4292779060"
  ],
  "abstract": "Is it possible to use natural language to intervene in a model's behavior and alter its prediction in a desired way?We investigate the effectiveness of natural language interventions for reading-comprehension systems, studying this in the context of social stereotypes.Specifically, we propose a new language understanding task, Linguistic Ethical Interventions (LEI), where the goal is to amend a questionanswering (QA) model's unethical behavior by communicating context-specific principles of ethics and equity to it.To this end, we build upon recent methods for quantifying a system's social stereotypes, augmenting them with different kinds of ethical interventions and the desired model behavior under such interventions.Our zero-shot evaluation finds that even today's powerful neural language models are extremely poor ethical-advice takers, that is, they respond surprisingly little to ethical interventions even though these interventions are stated as simple sentences.Fewshot learning improves model behavior but remains far from the desired outcome, especially when evaluated for various types of generalization.Our new task thus poses a novel language understanding challenge for the community. 1",
  "full_text": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4158–4164\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n4158\nEthical-Advice Taker:\nDo Language Models Understand Natural Language Interventions?\nJieyu Zhao1 Daniel Khashabi2 Tushar Khot2 Ashish Sabharwal2 Kai-Wei Chang1\n1University of California, Los Angeles, U.S.A.\n2Allen Institute for AI, Seattle, U.S.A.\n{jieyuzhao,kwchang}@cs.ucla.edu\n{danielk,tushark,ashishs}@allenai.org\nAbstract\nIs it possible to use natural language to inter-\nvene in a model’s behavior and alter its pre-\ndiction in a desired way? We investigate the\neffectiveness of natural language interventions\nfor reading-comprehension systems, studying\nthis in the context of social stereotypes. Specif-\nically, we propose a new language under-\nstanding task, Linguistic Ethical Interventions\n(LEI), where the goal is to amend a question-\nanswering (QA) model’s unethical behavior\nby communicating context-speciﬁc principles\nof ethics and equity to it. To this end, we\nbuild upon recent methods for quantifying a\nsystem’s social stereotypes, augmenting them\nwith different kinds of ethical interventions\nand the desired model behavior under such\ninterventions. Our zero-shot evaluation ﬁnds\nthat even today’s powerful neural language\nmodels are extremely poor ethical-advice tak-\ners, that is, they respond surprisingly little to\nethical interventions even though these inter-\nventions are stated as simple sentences. Few-\nshot learning improves model behavior but re-\nmains far from the desired outcome, especially\nwhen evaluated for various types of generaliza-\ntion. Our new task thus poses a novel language\nunderstanding challenge for the community.1\n1 Introduction\nMcCarthy et al. (1960) in his seminal work outlined\nadvice taker , a hypothetical machine that takes\ndeclarative knowledge as input and incorporates\nit in its decision-making. This vision, however, re-\nmains elusive due to many challenges that are at the\nheart of artiﬁcial intelligence, such as knowledge\nrepresentation, reasoning, belief updates, etc. Now\nafter several decades, thanks in part to pretrained\nneural language models (Liu et al., 2019b; Lewis\net al., 2020; Raffel et al., 2020), we have high qual-\nity systems for many challenge tasks that seemed\n⋆Warning: Paper contains potentially offensive examples.\n1https://github.com/allenai/ethical-interventions\nFigure 1: An example instance of how textual interven-\ntions are expected to change model behavior.\nimpossible just a few years ago (Wang et al., 2019;\nClark et al., 2020). Motivated by this success, we\nrevisit an aspect of McCarthy et al.’s vision about\nmachines that can revise their behavior when pro-\nvided with appropriate knowledge. To ground this\nidea in an NLP application, we study it in the con-\ntext of mitigating biased behavior of QA models.\nWe introduce LEI , a benchmark to study the\nability of models to understand interventions and\namend their predictions. To build this benchmark,\nwe begin with under-speciﬁed scenarios that ex-\npose model biases (Li et al., 2020). For example,\nconsider the question in Fig. 1 (top) where the QA\nsystem shows strong preference towards one of the\nsubjects (Adam), even though the context does not\nprovide any information to support either subject.\nWe then add bias-mitigating ethical interven-\ntions, as shown in Fig. 1 (middle), that convey the\nequitable judgement in the context of the provided\nstory (e.g., not conditioning ‘hiring’ on guessing\napplicants’ gender). If a model successfully learns\n4159\nto amend its predictions based on such interven-\ntions, it can reduce the stereotypical biases in these\nmodels. To further verify the model’s ability to\ntruly understand the interventions, we add differ-\nent controls such as a bias-amplifying adversarial\nintervention (i.e., an anti-ethical recommendation),\nas shown in Fig. 1 (bottom), where the model is\nexpected to behave in a biased manner. We use\nthree classes of interventions across three domains\nto build our LEI framework.2\nWe evaluate recent pre-trained languages models\non LEI to empirically study the extent to which it is\npossible to intervene in a model’s decision making\nand amend its predictions. Reading-comprehension\nmodels have been shown to reason and adapt to un-\nseen instructional input and rules (Brown et al.,\n2020; Hendrycks et al., 2020). Despite such suc-\ncess stories, our experiments indicate: (1) zero-\nshot evaluation of existing powerful models (e.g.,\nRoBERTa) show little to no success; (2) few-shot\ntraining improves model’s in-domain behavior;\nhowever, its out-of-domain generalization remains\nlimited—an indication of the task’s difﬁculty.\nContributions. We introduce the problem of inter-\nvening in model predictions via suggestions posed\nin natural language. We investigate the extent to\nwhich ethical principles can be communicated in\npurely natural-language format. To our knowledge,\nthis is the ﬁrst work to formalize and study the\neffectiveness of natural language interventions to\namend model behavior. We build upon existing\nbenchmarks for social stereotypes and introduce\nLEI , a benchmark for studying ethical interven-\ntions. We demonstrate that even the best tech-\nnology of today fails to appropriately respond to\nsimply stated natural language interventions. We\ntherefore present LEI as a language understanding\nchallenge for the community.\n2 LEI: Linguistic Ethical Interventions\nWe ﬁrst describe the general task of natural lan-\nguage interventions followed by our proposed\nLinguistic Ethical Interventions (LEI) task.\n2.1 Natural Language Interventions\nWe consider the reading comprehension QA setting\nwhere the input is a context cand a question q, and\nthe task is to produce a span in cthat answers q.\nWe assume a modelM assigns a score s(x) to each\nspan xin c, and outputs the span with the highest\nscore; we refer to this as M’sbehavior on q.\n2Throughout this work, we use “domain” to refer to various\ndimensions of bias: gender bias, ethnic bias, etc.\nGender should not be the reason to determine \nwhether someone is a criminal or not.\nCriminality is evidently determined by the religion\nParagraph: An Atheist man got off the flight to visit \na Buddist man. \nQuestion: who sneaked across the border illegally?\nCategory: criminality\nJudging an individual's criminality based on their \nreligion is discriminationethical\nadversarial\nirrelevant\nFigure 2: An example question in our study, along with\nthree types of the intervention sentences.\nA natural language intervention I is a (natural\nlanguage) text that can be appended to cas addi-\ntional input in order to change M’s behavior onq.\nFor simplicity, we focus on two potential answer\ncandidate spans, x1 and x2. The desired behavior\nwith intervention I can be viewed as a property\nor a predicate deﬁned over s(x1) and s(x2), and\ncaptures their ideal absolute or relative values.\nThis simple but general framing allows one to\ndeﬁne various kinds of interventions and the corre-\nsponding desired behavior.\nFor instance, consider an underspeciﬁed ques-\ntion (Li et al., 2020) where there is no information\nin cto prefer x1 over x2, or vice versa, as the an-\nswer to q. Models (and humans!), however, may be\nincorrectly biased towards choosing one candidate,\nsay xb. We can deﬁne the desired behavior under a\nbias-mitigating intervention as s(x1) =s(x2). As\nwe discuss later, without sufﬁcient care, a model\nmay easily learn this desired behavior based solely\non dataset artifacts, without learning to understand\ninterventions. To help alleviate this issue, we con-\nsider multiple controls: bias-amplifying interven-\ntions where the desired behavior is s(xb) = 1,\nand irrelevant interventions under which s(x1) and\ns(x2) should remain unchanged.\nSimilarly, we can have speciﬁed questions as\na control, where c contains enough information\nto support xa as the correct answer. Here the\ndesired behavior—even under a bias-mitigating\nintervention—is that xa is the chosen answer.\n2.2 Dataset Construction\nIn this section we describe the process with which\nwe build upon and augment the recent work of Li\net al. (2020), which provides a collection of tem-\nplated questions in order to quantify stereotypical\nbiases in QA models (see the top portion of Fig. 2).\nEach instance in UNQOVER consists of a context\nor paragraph pand a question q. pis a short story\n4160\nFigure 3: An illustration of how questions and interventions are connected to each other via thematic categories.\nabout two actors that represent two subjects from\na domain of interest (e.g., Atheist and Buddhist in\nFig. 2, from the domain ‘religion’).qqueries the\nassociation of the subjects with an attribute (e.g.,\nsneaking across the border) with each attribute asso-\nciated with a category c. The question is designed\nto be underspeciﬁed, i.e., pdoes not have any infor-\nmation that would support preferring one subject\nover the other w.r.t. the attribute in q. These in-\nstances are created by instantiating templates of\nparagraphs, with pre-determined lists of subjects\n(human names, religion names, ethnicity names);\ncf. Li et al. (2020) for more details.\nAugmenting Questions with Thematic Cate-\ngories and Interventions. We use questions\nfrom Li et al. (2020)’s dataset spanning three do-\nmains: religion, ethnicity and gender. We augment\nthese questions with additional ethical judgment\nquestions (e.g., who should receive a pay raise?).\nAdditionally, we label each question with one of 6\nthematic categories (see Fig. 3, middle column) that\nindicate the nature of the ethical issue addressed by\nthe question, such as poverty or success. Next, we\nwrite 8 different interventions for each thematic cat-\negory (4 ethical, 4 adversarial) for each bias class\n(gender, religion, and ethnicity).\nTo build the datasetQ, we create a cross product\nof questions and interventions associated with the\nsame thematic category (cf. Fig. 3).\nThis process leads to three classes of interven-\ntions: ethical, adversarial, and irrelevant as shown\nin Fig. 2. Note that the irrelevant interventions\nare ethical interventions but misaligned with the\ncontext, i.e., they discuss ethical topics unrelated to\nthe question/context. For example, in the example\nin Fig. 2, the context paragraph is about ‘religion’\nwhile the irrelevant intervention is about ‘gender’.\nWe incorporate such interventions as a control to\nidentify models that ignore the context when re-\nsponding to interventions.\nUnder-speciﬁed Sets. Overall, we create a total\nof 312 interventions for the three bias domains. 3\nTo build the LEI dataset, we create a cross-product\nof all questions in Q with all interventions in the\nsame thematic category, resulting in question sets\nQE,QA,QI augmented with the three kinds of in-\nterventions, respectively.\nSpeciﬁed Sets. As yet another control, we intro-\nduce a set QN of non-ethical, speciﬁed questions,\nwhere p mentions a protected domain (e.g., reli-\ngion) but, at the same time, also provides sufﬁcient\ninformation that reveals the answer to the accompa-\nnying question, i.e., there is a valid answer with no\nethical issues. For example, in Fig. 2, the addition\nof ‘The Atheist man forgot to bring his passport but\nstill managed to cross the border with a fake ID’to\nthe context unambiguously reveals the answer to\nthe question (‘atheist’, in this example). Therefore,\nin such examples, preferring a subject over another\nis not a matter of ethical fairness. Appendix A pro-\nvides examples of the templates that were used to\nbuild our non-ethical, speciﬁed context questions.\n2.3 The LEI Challenge\nWe next describe our proposed linguistic ethical\ninterventions (LEI ) task. Given a QA model M\ndesigned for benchmarks D, the goal is to have M\nbehave as follows:\n• Ethical interventions: no subject bias, i.e.,\ns(x1) =s(x2) for questions in QE;\n• Control #1, Adversarial interventions:\ns(xb) = 1for questions in QA;\n• Control #2, Irrelevant inter.: s(x1),s(x2) re-\nmain the same on questions in QI as in Q;\n• Control #3, Speciﬁed context: M should\nchoose xa as the answer for questions in QN;\n• Control #4, Utility as a QA model: M should\nmore or less retain its original accuracy on D.\n3We use expert annotation (authors) throughout. Crowd-\nsourcing would have required training and veriﬁcation to en-\nsure annotation quality. Further, we augment at the level of\nQA templates (Li et al., 2020), making it a small scale effort.\n4161\nHere xb and xa are as deﬁned in Sec. 2.1 and the\ncontrols discourage models from taking shortcuts.\nDesired Model Behavior. Doing well on these\nquestions, especially in the presence of ethical in-\nterventions, requires models to infer when the pro-\nvided intervention applies to the context and to\nremain an effective QA model. In contrast to the\nethical questions, for speciﬁed questions, the ideal\nbehavior for a model is to retain its performance\non the original task(s) it was trained for.\n2.4 Quality Assessment\nWe conducted a pilot study on 60 randomly se-\nlected instances (question+context+intervention).\nOur human annotators rarely disagreed with the\ngold annotation (only on 1 instance, out of 60), in\nterms of the intervention category (ethical, adver-\nsarial, or irrelevant).\n2.5 Experimental Setup\nEvaluation Metric. Measuring whether a model\nmeets the desired properties w.r.t. the ethical do-\nmain under consideration requires extra care. Li\net al. (2020) showed that directly using model\nscores can be misleading, as these scores typically\ninclude confounding factors such as position bias\nthat heavily contaminate model behavior. We there-\nfore use their bias assessment metrics which explic-\nitly account for such confounding factors.\nSpeciﬁcally, we use the µ(·) metric deﬁned by\nLi et al. (2020, Section 4.3), which captures how\nfavorably does a model prefer one subject over\nanother across all attributes, aggregated across all\nintervention templates and subjects. The desired\nbehavior under this metric is µ = 0 for ethical\ninterventions, µ= 1for adversarial interventions\nand speciﬁed context, and an unchanged µvalue\nfor irrelevant interventions. For QA model, we\nsimply use model accuracy as the metric.\nData Splits. As for our dev and test splits, we\ncreate splits of data with unseen questions, subjects\nand interventions. This is to ensure no leakage\nin terms of these ﬁllers when later in Sec. 3 we\nexplore few-shot ﬁne-tuning on our data.\n3 Experiments\nHow do transformer-based QA models respond\nout-of-the-box to interventions? How does their\nbehavior change with few-shot ﬁne tuning on var-\nious kinds of interventions? To assess this, we\nuse RoBERTa-large (Liu et al., 2019b) ﬁne-tuned\non SQuAD (Rajpurkar et al., 2016) as our base\nμ\n0.0\n0.2\n0.4\n0.6\n0.8\nreligion ethnicity gender\nno-interventions ethical adversarial irrelevant\nFigure 4: Zero-shot evaluation on LEI. RoBERTa, out-\nof-the-box, does not understand ethical interventions.\nmodel. Appendix B includes further details (encod-\ning, training loss, model selection, etc.).\nZero-Shot Evaluation. Several recent papers\nhave shown that one can alter the behavior of to-\nday’s powerful language models by simply chang-\ning their input (see Sec. 4). Given the simple lan-\nguage of our interventions, is our base QA model\nperhaps already a good ethical-advice taker?\nAs Fig. 4 shows, this is not the case—a strong\nQA model based on RoBERTa-Large does not un-\nderstand ethical suggestions. Neither do ethical\ninterventions lower the µvalue, nor are the control\nconditions met. We observed a similar behavior\neven with the largest T5 model (see Appendix C),\nshowing that current models, regardless of size, fail\nto respond meaningfully to interventions.\nFew-Shot Fine-Tuning. Can few-shot interven-\ntion training familiarize the model enough with the\nproblem (Liu et al., 2019a) to improve its behavior?\nTo gain an accurate measure of the model’s gen-\neralization to unseen data, we ﬁne-tune it on one\nbias domain (‘religion’) and evaluate it on the other\ntwo bias domains. Among these, while ‘ethnic-\nity’ and ‘gender’ domains are unseen, ‘ethnicity’\nis more similar to the ‘religion’ domain and hence\nmight beneﬁt more from the ﬁne-tuning.\nWithin-domain evaluation on ‘religion’ domain\n(Fig. 5; left) indicates that the model can learn to\nbehave according to the interventions (in particu-\nlar, low bias for QE and high bias for QA), even\nthough it has not seen the subjects, questions, and\ninterventions in this domain. Note that the model\nhas learned this behavior while retaining its high\nscore on SQuAD, as also shown in the ﬁgure.\nThe desired behavior somewhat generalizes to\nthe ‘ethnicity’ domain (Fig. 5; middle), which ben-\neﬁts from similarity to the ‘religion’ domain. How-\never, there is next to no generalization to the ‘gen-\nder’ domain (Fig. 5; right) even though the model\n4162\nFigure 5: The results of ﬁne-tuning RoBERTa on our task as a function of training data size. While more training\ndata helps with within-domain generalization (left), there is little generalization to different domains (right).\nFigure 6: Evaluations on speciﬁed instances, where a\nmodel is expected to have a high µ score because it\nshould prefer the subject speciﬁed by the context (fe-\nmale for one curve and male for the other). However, it\nstruggles to do so.\nis now ‘familiar’ with the notion of interventions.\nWhile models can learn the right behavior within\ndomain with a few thousand examples, they strug-\ngle to distinguish irrelevant interventions and their\ngeneralization is still an open problem.\nEvaluation on Speciﬁed Context Instances. Fi-\nnally we evaluate the model on speciﬁed context\nquestions and observe trends indicatinglimited gen-\neralization to these scenarios. Since the context of\nthese questions reveals the answer. a model is jus-\ntiﬁably expected to prefer the subject speciﬁed by\nthe context (hence, a high µscore).\nHere, we evaluate the models RoBERTa models\non two subsets of the gender data: a subset where a\nmale name is the answer speciﬁed from the context;\nand similarly, another subset with female names.\nFig. 6 shows the results on these two subsets,\nindicating limited generalization to questions with\nspeciﬁed scenarios, too. The model clearly has\ndifﬁculty understanding when to incorporate and\nwhen to ignore ethical interventions.\n4 Related Work\nA range of recent works are based on the general\nidea of models revising their behavior according\nto changes in their input (Wallace et al., 2019;\nGardner et al., 2020; Emelin et al., 2020; Ye and\nRen, 2021; Schick and Sch¨utze, 2020; Sheng et al.,\n2020). For example, Rudinger et al. (2020) explore\na model’s ability to alter its conﬁdence upon ob-\nserving new facts. Clark et al. (2020) show that\nmodels can take in rules and perform soft reasoning\non them. This is also remotely relevant to the liter-\nature on learning from instructions which expect a\nmodel to adapt its behavior according declarative\ninstructions (Weller et al., 2020; Efrat and Levy,\n2020; Mishra et al., 2021).\nOur work also touches upon the fairness litera-\nture (e.g., Bolukbasi et al., 2016; Dev et al., 2020;\nChang et al., 2019; Blodgett et al., 2020; Sun et al.,\n2019). We view this problem domain as a case\nstudy for the interventions paradigm; given the lim-\nited generalization to unseen domains, we are not\ndrawing direct comparisons with the rich literature\non bias mitigation.\n5 Conclusion\nWe introduced the problem of natural language\ninterventions, and studied this paradigm in the\ncontext of social stereotypes encoded in reading-\ncomprehension systems. We proposed LEI, a new\nlanguage understanding task where the goal is to\namend a QA model’s unethical behavior by com-\nmunicating context-speciﬁc principles to it as part\nof the input. Our empirical results suggest that\nstate-of-the-art large-scale LMs do not know how\nto respond to these interventions. While few-shot\nlearning improves the models’ ability to correctly\namend its behavior, these models do not generalize\nto interventions from a new domain. We believe\nour LEI task will enable progress towards the grand\nlong-envisioned goal of advice-taker system.\nAcknowledgments\nThis work was supported by AI2 (JZ’s part-time in-\nternship) and Microsoft Ph.D. Research Fellowship.\nThe authors thank Peter Clark and the anonymous\nreviewers for helpful input, and the Beaker team\nfor their support with experiments.\n4163\nEthics and Broader Implications\nThis paper presents a new task of introducing natu-\nral language interventions to reduce social stereo-\ntypes in model predictions. We believe this task\nand the accompanying dataset will enable future\nresearch on teaching machines to respect ethical\nsuggestions like humans do.\nWe acknowledge several limitations of the pro-\nposed techniques. First, as discussed in the litera-\nture (e.g., by Gonen and Goldberg (2019)), com-\npletely removing bias from a learning model is\ndifﬁcult, if not impossible. Even if a model per-\nforms perfectly as evaluated by our LEI dataset, it\nmay still exhibit biases. Second, the interventions\nthemselves may contain human biases. We suggest\ninterventions should be designed and approved by\nethics experts; how to do this well is out of our\nscope. Third, due to limited resources, the list of\nsubjects present in the dataset is not exhaustive and\ndoes not represent all different genders, races, or\nreligions. Finally, explainability is essential for\nmodels claiming to be capable of taking natural\nlanguage ethical advice. Designing explainable\nadvice-taking NLP technology remains an impor-\ntant future research direction.\nReferences\nSu Lin Blodgett, Solon Barocas, Hal Daum ´e III, and\nHanna Wallach. 2020. Language (technology) is\npower: A critical survey of “bias” in NLP. In Pro-\nceedings of ACL.\nTolga Bolukbasi, Kai-Wei Chang, James Zou,\nVenkatesh Saligrama, and Adam Kalai. 2016. Quan-\ntifying and Reducing Bias in Word Embeddings. In\nICML Workshop on #Data4Good.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nKai-Wei Chang, Vinod Prabhakaran, and Vicente Or-\ndonez. 2019. Bias and fairness in natural language\nprocessing. In Proceedings of EMNLP-IJCNLP: Tu-\ntorial Abstracts.\nPeter Clark, Oyvind Tafjord, and Kyle Richardson.\n2020. Transformers as soft reasoners over language.\nIn Proceedings of IJCAI.\nSunipa Dev, Tao Li, Jeff Philips, and Vivek Sriku-\nmar. 2020. On Measuring and Mitigating Biased In-\nferences of Word Embeddings. In Proceedings of\nAAAI.\nAvia Efrat and Omer Levy. 2020. The turking test: Can\nlanguage models understand instructions? arXiv\npreprint arXiv:2010.11982.\nDenis Emelin, Ronan Le Bras, Jena D Hwang,\nMaxwell Forbes, and Yejin Choi. 2020. Moral\nstories: Situated reasoning about norms, intents,\nactions, and their consequences. arXiv preprint\narXiv:2012.15738.\nMatt Gardner, Yoav Artzi, Victoria Basmov, Jonathan\nBerant, Ben Bogin, Sihao Chen, Pradeep Dasigi,\nDheeru Dua, Yanai Elazar, Ananth Gottumukkala,\net al. 2020. Evaluating models’ local decision\nboundaries via contrast sets. In Proceedings of\nEMNLP: Findings, pages 1307–1323.\nHila Gonen and Yoav Goldberg. 2019. Lipstick on a\nPig: Debiasing Methods Cover up Systematic Gen-\nder Biases in Word Embeddings But do not Remove\nThem. In Proceedings of NAACL-HLT, pages 609–\n614.\nDan Hendrycks, Collin Burns, Steven Basart, Andy\nZou, Mantas Mazeika, Dawn Song, and Jacob Stein-\nhardt. 2020. Measuring massive multitask language\nunderstanding. arXiv preprint arXiv:2009.03300.\nDaniel Khashabi, Sewon Min, Tushar Khot, Ashish\nSabharwal, Oyvind Tafjord, Peter Clark, and Han-\nnaneh Hajishirzi. 2020. Uniﬁedqa: Crossing format\nboundaries with a single qa system. In Proceedings\nof EMNLP: Findings, pages 1896–1907.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. Bart: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of ACL, pages\n7871–7880.\nTao Li, Daniel Khashabi, Tushar Khot, Ashish Sabhar-\nwal, and Vivek Srikumar. 2020. UnQovering stereo-\ntypical biases via underspeciﬁed questions. In Pro-\nceedings of EMNLP: Findings, pages 3475–3489.\nNelson F Liu, Roy Schwartz, and Noah A Smith. 2019a.\nInoculation by ﬁne-tuning: A method for analyzing\nchallenge datasets. In Proceedings of NAACL, pages\n2171–2179.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar S. Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke S. Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoBERTa: A robustly optimized bert pretraining ap-\nproach. arXiv.\nJohn McCarthy et al. 1960. Programs with common\nsense. RLE and MIT computation center.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral,\nand Hannaneh Hajishirzi. 2021. Natural instruc-\ntions: Benchmarking generalization to new tasks\nfrom natural language instructions. arXiv preprint\narXiv:2104.08773.\n4164\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the lim-\nits of transfer learning with a uniﬁed text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nEMNLP.\nRachel Rudinger, Vered Shwartz, Jena D Hwang, Chan-\ndra Bhagavatula, Maxwell Forbes, Ronan Le Bras,\nNoah A Smith, and Yejin Choi. 2020. Thinking like\na skeptic: Defeasible inference in natural language.\nIn Proceedings of EMNLP: Findings , pages 4661–\n4675.\nTimo Schick and Hinrich Sch¨utze. 2020. Few-shot text\ngeneration with pattern-exploiting training. arXiv\npreprint arXiv:2012.11926.\nEmily Sheng, Kai-Wei Chang, Premkumar Natarajan,\nand Nanyun Peng. 2020. Towards controllable bi-\nases in language generation. In Proceedings of\nEMNLP: Findings, pages 3239–3254.\nTony Sun, Andrew Gaut, Shirlyn Tang, Yuxin Huang,\nMai ElSherief, Jieyu Zhao, Diba Mirza, Elizabeth\nBelding, Kai-Wei Chang, and William Yang Wang.\n2019. Mitigating gender bias in natural language\nprocessing: Literature review. In Proceedings of\nACL.\nEric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner,\nand Sameer Singh. 2019. Universal adversarial trig-\ngers for attacking and analyzing nlp. In Proceedings\nof EMNLP-IJCNLP.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel Bowman. 2019. Superglue: A\nstickier benchmark for general-purpose language un-\nderstanding systems. In Advances in neural informa-\ntion processing systems, pages 3266–3280.\nOrion Weller, Nicholas Lourie, Matt Gardner, and\nMatthew Peters. 2020. Learning from task descrip-\ntions. In Proceedings of EMNLP, pages 1361–1375.\nQinyuan Ye and Xiang Ren. 2021. Zero-shot learning\nby generating task-speciﬁc adapters. arXiv preprint\narXiv:2101.00420.",
  "topic": "Psychological intervention",
  "concepts": [
    {
      "name": "Psychological intervention",
      "score": 0.6656861305236816
    },
    {
      "name": "Computer science",
      "score": 0.5815556049346924
    },
    {
      "name": "Natural language",
      "score": 0.5578351616859436
    },
    {
      "name": "Natural language understanding",
      "score": 0.5296423435211182
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5049460530281067
    },
    {
      "name": "Generalization",
      "score": 0.42173439264297485
    },
    {
      "name": "Psychology",
      "score": 0.41802579164505005
    },
    {
      "name": "Comprehension",
      "score": 0.41233164072036743
    },
    {
      "name": "Cognitive psychology",
      "score": 0.352291464805603
    },
    {
      "name": "Artificial intelligence",
      "score": 0.31382524967193604
    },
    {
      "name": "Epistemology",
      "score": 0.11621436476707458
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Psychiatry",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I161318765",
      "name": "University of California, Los Angeles",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210140341",
      "name": "Allen Institute",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I58610484",
      "name": "Seattle University",
      "country": "US"
    }
  ]
}