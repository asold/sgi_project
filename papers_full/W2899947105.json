{
    "title": "Transfer learning of language-independent end-to-end ASR with language model fusion",
    "url": "https://openalex.org/W2899947105",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A3137038995",
            "name": "Inaguma, Hirofumi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1993303458",
            "name": "Cho JaeJin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3085115137",
            "name": "Baskar, Murali Karthick",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1971034612",
            "name": "Kawahara, Tatsuya",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2575256866",
            "name": "Watanabe, Shinji",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3034729383",
        "https://openalex.org/W2758310181",
        "https://openalex.org/W2962824709",
        "https://openalex.org/W2526425061",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2766219058",
        "https://openalex.org/W2888779557",
        "https://openalex.org/W1686810756",
        "https://openalex.org/W2127141656",
        "https://openalex.org/W2564058731",
        "https://openalex.org/W1915251500",
        "https://openalex.org/W1524333225",
        "https://openalex.org/W1494198834",
        "https://openalex.org/W2891816510",
        "https://openalex.org/W2799800213",
        "https://openalex.org/W2292087804",
        "https://openalex.org/W2555428947",
        "https://openalex.org/W2963336460",
        "https://openalex.org/W6908809"
    ],
    "abstract": "This work explores better adaptation methods to low-resource languages using an external language model (LM) under the framework of transfer learning. We first build a language-independent ASR system in a unified sequence-to-sequence (S2S) architecture with a shared vocabulary among all languages. During adaptation, we perform LM fusion transfer, where an external LM is integrated into the decoder network of the attention-based S2S model in the whole adaptation stage, to effectively incorporate linguistic context of the target language. We also investigate various seed models for transfer learning. Experimental evaluations using the IARPA BABEL data set show that LM fusion transfer improves performances on all target five languages compared with simple transfer learning when the external text data is available. Our final system drastically reduces the performance gap from the hybrid systems.",
    "full_text": "arXiv:1811.02134v2  [cs.CL]  7 May 2019\nTRANSFER LEARNING OF LANGU AGE-INDEPENDENT END-TO-END ASR\nWITH LANGU AGE MODEL FUSION\nHirofumi Inaguma 1∗ , Jaejin Cho 2, Murali Karthick Baskar 3, T atsuya Kawahara 1, Shinji W atanabe 2\n1Graduate School of Informatics, Kyoto University, Kyoto, J apan\n2Johns Hopkins University, Baltimore, MD, USA , 3Brno University of T echnology, Czech Republic\nABSTRACT\nThis work explores better adaptation methods to low-resour ce lan-\nguages using an external language model (LM) under the frame -\nwork of transfer learning. W e ﬁrst build a language-indepen dent\nASR system in a uniﬁed sequence-to-sequence (S2S) architec ture\nwith a shared vocabulary among all languages. During adapta tion,\nwe perform LM fusion transfer , where an external LM is integrated\ninto the decoder network of the attention-based S2S model in the\nwhole adaptation stage, to effectively incorporate lingui stic context\nof the target language. W e also investigate various seed mod els for\ntransfer learning. Experimental evaluations using the IAR P A BA-\nBEL data set show that LM fusion transfer improves performan ces\non all target ﬁve languages compared with simple transfer le arning\nwhen the external text data is available. Our ﬁnal system dra stically\nreduces the performance gap from the hybrid systems.\nIndex T erms— end-to-end ASR, multilingual speech recogni-\ntion, low-resource language, transfer learning\n1. INTRODUCTION\nFast system development for low-resourced new languages is one\nof the challenges in automatic speech recognition (ASR). Re cently ,\nend-to-end ASR systems based on the sequence-to-sequence ( S2S)\narchitecture [1, 2] are ﬁlling up the gap of performance from the\nconventional HMM-based hybrid systems and showing promisi ng\nresults in many tasks with its extremely simpliﬁed training and de-\ncoding schemes [3–5]. This is very attractive when building systems\nin new languages quickly . However, models tend to suffer fro m the\ndata sparseness problems in the low-resource scenario, esp ecially in\nS2S models due to its data-driven optimization.\nOne possible approach to this problem is to utilize data of\nother languages. There are various approaches to leverage o ther\nlanguages: (a) to train a model multilingually (multi-task learn-\ning with other languages), and then further ﬁne-tune to a par ticu-\nlar language [6], and (b) to adapt a multilingual model to a ne w\nlanguage using transfer learning [6–9] and additional feat ures ob-\ntained from the multilingual model such as multilingual bot tleneck\nfeatures (BNF) [10–13] and language feature vectors (LFV) [ 14]\n(cross-lingual adaptation). T o obtain a multilingual S2S m odel, a\npart of parameters can be shared while preparing the output l ayers\nper language [6], and we can further use a uniﬁed architectur e with\na shared vocabulary among multiple languages [15–17]. Sinc e it\nwould take much time to train such systems from scratch for ma ny\nlanguages including new languages, we focus on the cross-li ngual\nadaptation approach (b).\n*Part of the work reported here was conducted while the autho r was\nvisiting Johns Hopkins University .\nWhile a majority of the conventional transfer learning is co n-\ncerned with acoustic model, using linguistic context durin g adap-\ntation has not been investigated yet. The research question in this\npaper is: Is linguistic context also helpful for adaptation to new lan -\nguages? The most common approach to integrate the external lan-\nguage model (LM) is referred to as shallow fusion , where LM scores\nare interpolated with scores from the S2S model [5,18,19]. R ecently ,\nseveral methods to leverage an external LM during training o f S2S\nmodels are proposed: deep fusion [20] and cold fusion [21]. In deep\nfusion, the decoder network in the pre-trained S2S model and an ex-\nternal Recurrent neural network language model (RNNLM) are in-\ntegrated into a single architecture by the gating mechanism and only\nthe gating part is re-trained. In contrast, cold fusion inte grates an\nexternal LM during the entire training stage.\nIn this paper, we investigate methods to fully utilize text d ata\nfor adaptation to unseen low-resource languages. W e propos e LM\nfusion transfer , where an external LM is integrated into the decoder\nnetwork of the S2S model only in the adaptation stage 1 , as an ex-\ntension of cold fusion. Since the decoder network is already well-\ntrained in a language-independent manner, the model can bet ter in-\ncorporate linguistic context from the external LM. The extr a cost to\nintegrate the external LM during adaptation is trivial in th e resource\nconstrained condition. W e also investigate various seed mu ltilingual\nmodels trained with 600 to 2200-hours speech data and show th e\neffect of the amount and variety of multilingual training da ta.\nExperimental evaluations on the IARP A BABEL corpus show\nthat the LM fusion transfer improves performance compared t o sim-\nple transfer learning with shallow fusion when the addition al text\ndata is available. The performance of the transferred model s is dras-\ntically improved by increasing the model capacity and incor porating\nthe external LM, and the resulting models perform comparabl y with\nthe latest BLSTM-HMM hybrid systems [10]. T o our best knowl-\nedge, this is the ﬁrst results for the S2S model to show the com -\npetitive performance to the conventional hybrid systems in the low-\nresource scenario ( ∼50 hours).\n2. RELA TED WORK\nThe traditional usage of unpaired text data in the S2S framew ork is\ncategorized to four approaches: LM integration, pre-train ing, multi-\ntask learning (MTL), and data augmentation. In the LM integr ation\napproach, there are three methods: shallow fusion , deep fusion , and\ncold fusion as described in Section 1. Their differences are in the\ntiming to integrate an external LM and the existence of addit ional\nparameters of the gating mechanism. W e depict these fusion m eth-\n1 Although we can perform LM fusion during training of the seed multi-\nlingual model, we focus on applying it during adaptation bec ause our goal is\nto adapt it to a particular language rapidly .\nods in Fig. 1. In, [19], these fusion methods are compared in m iddle-\nsize English conversational speech ( ∼300h) and large-scale Google\nvoice search data. However, none of previous works investig ated the\neffect of them in other languages especially for low-resour ce lan-\nguages, which is the focus of this paper. In [21], the authors show\nthe effectiveness of cold fusion in a cross-domain scenario . Since the\nexternal LM is more likely to be changed frequently than the a cous-\ntic model, it is time-consuming to train a new S2S model with t he\nLM integration from scratch every time the external LM is upd ated.\nIn this work, we conduct LM fusion during adaptation to targe t lan-\nguages, which is regarded as a more realistic scenario.\nAnother usage of the external LM is to initialize the lower la yer\nin the decoder network with the pre-trained LM [19, 22]. Howe ver,\nwe transfer almost all parameters in a multilingual S2S mode l (both\nencoder and decoder networks), and thus we do not explore thi s di-\nrection. Apart from the external LM, the MTL approach with LM\nobjective are investigated in [19, 23]. Although the MTL app roach\ndoes not require any additional parameters, it gets minor ga ins com-\npared to LM fusion methods [19].\nRecently , data augmentation of speech data based on text-to -\nspeech (TTS) synthesis is investigated in the S2S framework [24,25].\nSince we are interested in the usage of linguistic context du ring adap-\ntation, we leave this direction to the future work.\n3. END-TO-END ASR\n3.1. Attention-based sequence-to-sequence\nW e build all models with attention-based sequence-to-sequ ence\n(S2S) models, which can learn soft alignments between input and\noutput sequences of variable lengths [1]. They are composed of en-\ncoder and decoder networks. The encoder network transforms input\nfeatures x = (x1, . . . , xT ) to a high-level continuous representa-\ntion h = (h1, . . . , hT ′ ) (T ′ ≤ T ), interleaved with subsampling\nlayers to reduce the computational complexity [26]. The dec oder\nnetwork generates a probability distribution PS2S of the correspond-\ning U-length transcription y = (y1, . . . , y U ) conditioned over all\nprevious generated tokens:\nsS2S\nu = Decoder(sS2S\nu−1, y u−1, cu)\nPS2S(y|x) = softmax(W osS2S\nu + bo)\nwhere W o and bo are trainable parameters, sS2S\nu is a decoder state\nat the u-th timestep, and cu is a context vector summarizing notable\nparts from the encoder states h. W e adopt the location-based scor-\ning function [2]. T o encourage monotonic alignments, the au xiliary\nConnectionist T emporal Classiﬁcation (CTC) [27] objectiv e is lin-\nearly interpolated [28].\nDuring the inference stage, scores from the softmax layer us ed\nfor the CTC objective are linearly interpolated in log-scal e with a\ntunable parameter λ (0 ≤ λ ≤ 1) to avoid generating incomplete\nand repeated hypotheses as follows [4]:\nln PASR(y|x) = (1− λ) lnPS2S(y|x) +λ ln PCTC(y|x)\n3.2. LM fusion\n3.2.1. Shallow fusion\nIn the conventional decoding paradigm with an external LM, r eferred\nto as shallow fusion , scores from both the S2S model and LM are\nlinearly interpolated to maximize the following criterion :\ny∗ = arg max\ny∈Ω ∗\n{ln PASR(y|x) +β ln PLM(y)}\n/gid11 /gid12 /gid1 /gid22 /gid34 /gid32 /gid25 /gid29 /gid28 /gid1 /gid33 /gid31 /gid18 /gid28 /gid32 /gid22 /gid21 /gid31 \n/gid17 /gid30 /gid20 /gid18 /gid33 /gid21 /gid1 /gid18 /gid26 /gid26 /gid1 /gid30 /gid18 /gid31 /gid18 /gid27 /gid21 /gid33 /gid21 /gid31 /gid32 \n/gid2 /gid7 /gid20 /gid18 /gid30 /gid33 /gid18 /gid33 /gid25 /gid29/gid28 /gid3 \n/gid8 /gid29 /gid26 /gid20 /gid1 /gid22 /gid34 /gid32/gid25 /gid29/gid28 /gid1 /gid33 /gid31 /gid18 /gid28 /gid32 /gid22 /gid21 /gid31 \n/gid2 /gid17 /gid30 /gid20 /gid18 /gid33 /gid21 /gid1 /gid18 /gid26 /gid26 /gid1 /gid30 /gid18 /gid31/gid18 /gid27 /gid21 /gid33 /gid21 /gid31/gid32 /gid3\n/gid9 /gid21 /gid21 /gid30 /gid1 /gid22 /gid34 /gid32/gid25 /gid29/gid28 /gid1 /gid33 /gid31 /gid18 /gid28 /gid32 /gid22 /gid21 /gid31 \n/gid2 /gid17 /gid30 /gid20 /gid18 /gid33 /gid21 /gid1 /gid33 /gid24 /gid21 /gid1 /gid23/gid18 /gid33 /gid25 /gid28 /gid23/gid1 /gid30 /gid18 /gid31/gid33 /gid3 \n/gid15 /gid24 /gid18 /gid26 /gid26 /gid29/gid35/gid1 /gid22 /gid34 /gid32 /gid25 /gid29/gid28 \n/gid16 /gid31 /gid18 /gid25 /gid28 /gid25 /gid28 /gid23 /gid1 /gid22 /gid31 /gid29 /gid27 /gid1 /gid32 /gid19 /gid31 /gid18 /gid33 /gid19 /gid24 \n/gid17 /gid30 /gid20 /gid18 /gid33 /gid21 /gid1 /gid18 /gid26 /gid26 /gid1 /gid30 /gid18 /gid31 /gid18 /gid27 /gid21 /gid33 /gid21 /gid31 /gid32 \n/gid8 /gid29 /gid26 /gid20 /gid1 /gid22 /gid34 /gid32 /gid25 /gid29 /gid28 \n/gid2 /gid17 /gid30 /gid20 /gid18 /gid33 /gid21 /gid1 /gid18 /gid26 /gid26 /gid1 /gid30 /gid18 /gid31/gid18 /gid27 /gid21 /gid33 /gid21 /gid31/gid32 /gid3 \n/gid9 /gid21 /gid21 /gid30 /gid1 /gid22 /gid34 /gid32 /gid25 /gid29/gid28 \n/gid2 /gid17 /gid30 /gid20 /gid18 /gid33 /gid21 /gid1 /gid33 /gid24 /gid21 /gid1 /gid23/gid18 /gid33 /gid25 /gid28 /gid23/gid1 /gid30 /gid18 /gid31/gid33 /gid3 \n/gid15 /gid24 /gid18 /gid26 /gid26 /gid29/gid35/gid1 /gid22 /gid34 /gid32 /gid25 /gid29/gid28 \n/gid14 /gid18 /gid28 /gid20 /gid29 /gid27 /gid1 /gid25 /gid28 /gid25 /gid33 /gid25 /gid18 /gid26 /gid25 /gid36 /gid21 /gid20 \n/gid10 /gid6 /gid10 /gid1 /gid7 /gid15 /gid14 \n/gid11 /gid18 /gid28 /gid23 /gid34 /gid18 /gid23 /gid21 /gid5 /gid25 /gid28 /gid20 /gid21 /gid30 /gid21 /gid28 /gid20 /gid21 /gid28 /gid33 \n/gid10 /gid6 /gid10 /gid1 /gid27 /gid29/gid20 /gid21 /gid26 /gid1 \n/gid4 /gid1 /gid14 /gid13 /gid13 /gid11 /gid12 \n/gid4 /gid1 /gid14 /gid13 /gid13 /gid11 /gid12 \n/gid4 /gid1 /gid14 /gid13 /gid13 /gid11 /gid12 \n/gid4 /gid1 /gid14 /gid13 /gid13 /gid11 /gid12 \n/gid4 /gid1 /gid14 /gid13 /gid13 /gid11 /gid12 \n/gid4 /gid1 /gid14 /gid13 /gid13 /gid11 /gid12 \nFig. 1 : Overview of language model fusion transfer. LM fusion\ntransfer is conducted with monolingual data only .\nwhere β is a tunable parameter to deﬁne the importance of the ex-\nternal LM. The separate LM, especially trained with a larger exter-\nnal text, has complementary effects to the implicit LM model ed in\nthe decoder network. Therefore, shallow fusion shows perfo rmance\ngains in many ASR tasks [5, 18, 19].\n3.2.2. Cold fusion (ﬂat-start fusion)\nWhile shallow fusion uses the external LM only in the inferen ce\nstage, cold fusion [21] uses the pre-trained LM during training of the\nS2S model to provide effective linguistic context. The ﬁne- grained\nelement-wise gating function is equipped to ﬂexibly rely on the LM\ndepending on the uncertainty of prediction:\nsLM\nu = W LMdLM\nu + bLM\ngu = σ (W g[sS2S\nu ; sLM\nu ] +bg)\nsCF\nu = W CF[sS2S\nu ; gu ⊙ sLM\nu ] +bCF\nPS2S(y|x) = softmax(ReLU(W outsCF\nu + bo))\nwhere W ∗ and b∗ are trainable parameters, dLM\nu is a hidden state of\nRNNLM, sLM\nu is a feature from the external LM, sCF\nu is a bottleneck\nfeature before the ﬁnal softmax layer, gu is a gating function, and ⊙\nrepresents element-wise multiplication. ReLU non-linear function is\ninserted before the softmax layer as suggested in [21]. W e us e the\nhidden state as a feature from RNNLM instead of logits becaus e we\nuse the universal character vocabulary for multilingual ex periments,\nwhich results in the large softmax layer and increases the co mputa-\ntional time [19].\nIn the original formulation in [19, 21], scores from the exte rnal\nLM are not used. W e found that linear interpolation of log pro babili-\nties from the LM with those from the S2S model during the infer ence\nas in shallow fusion still has complementary effects to impr ove per-\nformance. Therefore, we adopt it in all experiments.\n3.2.3. Deep fusion (ﬁne-tuning fusion)\nDeep fusion [20] is another method to integrate an external LM dur-\ning training. Unlike cold fusion, deep fusion is applied onl y for ﬁne-\ntuning the gating part after parameters of both the pre-trai ned S2S\nmodel and RNNLM are frozen. Although deep fusion is formulat ed\nwith a scalar gating function in [20], we use the same archite cture as\ncold fusion in Section 3.2.2 to make a strict comparison. The n, the\ndifference from the cold fusion is in the timing to integrate the exter-\nnal LM (from scratch or in the middle stage) and which paramet ers\nto update after integration (see Figure 1).\n4. TRANSFER LEARNING OF MUL TILINGUAL ASR\n4.1. Adaptation to a target language\nW e adapt a seed language-independent end-to-end ASR model t o an\n(unseen) target language. W e investigate the following fou r scenar-\nios:\nmulti10: From non-target 10 languages to an unseen target lan-\nguage\nhigh2: From 2 high resource languages (English and Japanese) to\nan unseen target language\nmulti10+high2: From the mix of non-target 10 languages and 2\nhigh resource languages to an unseen target language\nmulti15: From the mix of non-target 10 languages and target 5\nlanguages to a particular target language\nThe top three conditions are regarded as cross-lingual adap tation.\n4.2. LM fusion transfer\nDuring adaptation, all parameters are copied from the seed l anguage-\nindependent S2S model, then training is continued toward a t arget\nlanguage. W e investigate improved adaptation methods by in tegrat-\ning the external LM during and/or after transfer learning fr om the\nseed model. Three methods are considered as follows:\nT ransfer + SF:Shallow fusion in Section 3.2.1 is conducted in the\ninference stage after adaptation.\nCold fusion transfer (CF-transfer): Cold fusion in Section 3.2.2\nis conducted during adaptation. W e integrate the external\nRNNLM from the start point of adaptation to a target lan-\nguage. The softmax layer is randomly initialized before ada p-\ntation due to the additional gating part.\nDeep fusion transfer (DF-transfer): Deep fusion in Section 3.2.3\nis conducted after adaptation. DF-transfer is composed of t wo\nstages: (1) adaptation by updating the whole parameters un-\ntil convergence, and (2) ﬁne-tuning only the gating part aft er\nintegrating the external RNNLM. The softmax layer is ran-\ndomly initialized before stage (2).\n5. EXPERIMENT AL EV ALUA TION\n5.1. Experimental setting\nW e used data from the IARP A BABEL project [29] and selected 10\nlanguages as non-target languages for training the seed lan guage-\nindependent model: Cantonese, Bengali, Pashto, Turkish, V iet-\nnamese, Haitian, T amil, Kurmanji, T okpisin and Georgian, a nd 5\nlanguages for adaptation: Assamese (AS), Swahili (SW), Lao (LA),\nT agalog (T A) and Zulu (ZU). Full language pack (FLP) is used f or all\nexperiments except for Section 5.2.3, where limited langua ge pack\n(LLP) which consists of about 10% of FLP is used for adaptatio n.\nW e sampled 10% of data from the training data for each languag e as\nthe validation set. In addition, we used Librispeech corpus [30] and\nthe Corpus of Spontaneous Japanese (CSJ) [31] as additional high\nresources.\nW e used Kaldi toolkit [32] for feature extraction. The input\nfeatures were static 80-channel log-mel ﬁlterbank outputs appended\nwith 3-dimensional pitch features computed with a 25ms wind ow\nand shifted every 10ms. The features were normalized by the m ean\nand the standard deviation on the whole training set. For the vocabu-\nlary , we used the universal character set including all char acters from\nT able 1: Results of baseline monolingual systems. None of adapta-\ntion methods is conducted.\nModel\nWER (%)\nAS SW LA T A ZU\n(54h) (39h) (58h) (75h) (54h)\nOld baseline [7] 73.9 66.5 64.5 73.6 76.4\nNew baseline 64.5 56.6 56.2 56.4 69.5\n+ large units 59.9 50.9 51.7 52.7 65.5\n+ shallow fusion 57.4 46.5 49.8 49.9 62.9\nBLSTM-HMM 49.1 38.3 45.7 46.3 61.1\nall languages [15], resulting in the vocabulary size of 5,35 3 classes\nincluding 17 language IDs, sos, eos, unk, and blank labels. For mul-\ntilingual experiments, we prepended the corresponding lan guage ID\nso that the decoder network can jointly identify the correct target\nlanguage while recognizing speech [15].\nOur encoder network is composed of two VGG-like CNN blocks\n[33] followed by a max-pooling layer with a stride of 2 × 2, and 5\nlayers of bidirectional long short-term memory (BLSTM) [34 ] with\n1024 memory cells, which results in time reduction by a facto r of\n4. The decoder network consists of two layers of LSTM with 102 4\nmemory cells. For both monolingual and multilingual experi ments,\nwe used the same architecture. Training was performed on the mini-\nbatch size of 15 utterances using Adadelta [35] algorithm wi th an\ninitial epsilon 1e − 8. Epsilon was divided by a factor of 0.01 when\nthe teacher-forcing accuracy does not improve for the valid ation set\nat each epoch. Scheduled sampling [36] with probability 0.4 and\ndropout for the encoder network with probability 0.2 were pe rformed\nin all experiments during adaptation. W e set the CTC weight d uring\ntraining and decoding to 0.5 and 0.3, respectively . W e also s et the\nbeam width to 20 and the LM weight to 0.3.\nFor RNNLM, we used two layers of LSTM with 650 memory\ncells. All RNNLMs were trained with transcriptions in the pa rallel\ndata except for experiments in T able 4. W e used stochastic gr adient\ndescent (SGD) for RNNLM optimization. All networks are impl e-\nmented by ESPnet toolkit [37] with pytorch backend [38].\n5.2. Results\n5.2.1. Baseline monolingual systems for target 5 languages\nFirst, we present the results of the baseline monolingual en d-to-end\nsystems in T able 1. Our new systems (line 2) signiﬁcantly out per-\nformed the old baseline reported on [7]. The gain mostly came from\nadding VGG blocks before BLSTM encoder and one more decoder\nLSTM layer though we also tuned other hyper-parameters. Nex t,\nchanging the unit sizes of each LSTM layer from 320 to 1024 dra s-\ntically improved the performance. This is surprising becau se in-\ncreasing the number of parameters often makes the model over ﬁt\nto the small amount of training data. Finally , shallow fusio n with the\nmonolingual RNNLM further boosted the performance althoug h the\nRNNLM was trained with the small amount of transcriptions on ly .\nW e use this setting as default in the rest of experiments.\nW e also built BLSTM-HMM hybrid systems for comparison.\nThe BLSTM-HMM architecture includes 3 BLSTM layers each wit h\n512 memory cells and 300 projection units 2 . The BLSTM acous-\ntic model was trained using the latency control technique wi th 22\npast frames and 21 future frames. The acoustic model receive s 40-\ndimensional ﬁlterbank features as input. N-gram language m odel is\nbuilt with the training transcriptions. WERs by our end-to- end sys-\n2 Increasing the unit size did not lead to any improvement.\nT able 2: Results of adaptation from the different seed language-\nindependent models. Shallow fusion with the corresponding mono-\nlingual RNNLM was conducted.\nSeed hours WER (%)\nAS SW LA T A ZU\nmulti10 643 53.4 41.3 46.1 46.4 60.2\nhigh2 1,472 57.8 45.0 48.6 49.4 61.9\nmulti10+high2 2,115 53.2 40.7 45.1 45.3 58.5\nmulti15 929 53.4 40.6 45.0 46.1 58.8\nmulti15 w/o FT 929 56.2 44.2 47.1 47.8 60.6\n(FT: ﬁne-tuning to a target language)\ntems with shallow fusion are close to those of the hybrid syst em, just\n3.6 and 1.8 % absolute difference for T agalog and Zulu, respe ctively .\n5.2.2. Comparison of seed language-independent models\nW e compared the seed language-independent models for adapt ation\nto target languages. All models were transferred, and shall ow fu-\nsion with the corresponding monolingual RNNLM trained with the\nparallel data was performed. The results are shown in T able 2 . The\noverall performance was signiﬁcantly improved by transfer learning.\nThe transferred S2S models achieved comparable WER to BLSTM -\nHMM for T agalog and outperformed for Zulu in T able 1. W e can se e\nthat multi10 model is generally better than high2 model despite the\nsmaller data size, and combination of them ( multi10+high2) gives\nslight improvement. On the other hand, multi15 model that includes\nthe target language does not lead to further improvement eve n af-\nter ﬁne-tuning. W e can conclude that the diversity of langua ges\nis more important than the total amount of training data, and 10\nlanguages are almost sufﬁcient for learning language-inde pendent\nfeature representation and generalized to other languages well [6].\nSince multi10 shows the competitive results to multi10+high2 only\nwith one third training data, we use multi10 as the seed model and\ninvestigate cross-lingual adaptation in the following exp eriments.\n5.2.3. Effect of LM fusion transfer\nThe results of our proposed LM fusion transfer are given in T a ble\n3. When training S2S models from scratch, there is no differe nce\namong all fusion methods. When transferred from the languag e-\nindependent S2S model, signiﬁcant improvement is observed by in-\ntegrating the external RNNLM. Shallow fusion was more effec tive\nthan when training the S2S models from scratch in T able 1 beca use\nthe multilingual training led to generalization and the afﬁ nity for the\nexternal LM was enhanced. CF-transfer got some improvement s\ncompared to transfer learning with shallow fusion for 3 targ et lan-\nguages, but the effects of DF-transfer and CF-transfer are n ot signif-\nicant. This is because RNNLMs were trained with text in the sm all\nparallel data only , therefore linguistic context during ad aptation was\nnot so effective. However, CF-transfer in T agalog outperfo rmed the\nmonolingual hybrid system in T able 1. When compared to the pr evi-\nous work using the same data [7], CF-transfer yielded 21.6% g ains\nrelatively on average. Furthermore, 6.8% gains were achiev ed from\ntransfer learning without the external LM.\nT o investigate the effect of additional text data, we evalua te the\nLM fusion transfer with LLP on each target language ( ∼10 hours).\nThe results are shown in T able 4. W e used monolingual RNNLM\ntrained with LLP (parallel data) and FLP ( ∼50 hours), respectively .\nThe latter setting of a small speech data set ( ∼10 hours) and a larger\ntext data set ( ∼50 hours) is regarded as a more realistic scenario in\nlow-resource languages. When training S2S models from scra tch,\nT able 3: Results of LM fusion transfer on FLP ( ∼50h)\nModel WER (%)\nAS SW LA T A ZU\nTransfer [7] SF 65.3 56.2 57.9 64.3 71.1\nScratch\n— 59.9 50.9 51.7 52.7 65.5\nSF 57.4 46.5 49.8 49.9 62.9\nDF+SF 57.5 46.4 49.9 49.9 62.6\nCF+SF 57.5 47.3 50.0 50.2 62.9\nTransfer\n(multi10)\n— 56.4 46.4 48.6 50.1 63.5\nSF 53.4 41.3 46.1 46.4 60.2\nDF+SF 53.5 41.2 46.2 46.2 59.9\nCF+SF 53.6 41.6 45.9 46.2 59.5\n(SF: shallow fusion, DF: deep fusion, CF: cold fusion)\nT able 4: Results of LM fusion transfer on LLP ( ∼10h)\nModel LM\ndata\nWER (%)\nAS SW LA T A ZU\n(8h) (9h) (9h) (9h) (9h)\nScratch — — not converge\nTransfer\n(multi10)\n— — 67.5 59.7 60.3 66.2 75.4\nSF\nLLP\n63.3 52.8 57.2 60.8 71.2\nDF+SF 68.0 52.4 57.3 60.7 70.9\nCF+SF 63.2 52.8 58.4 60.6 71.0\nSF\nFLP\n62.7 51.7 56.4 60.0 71.0\nDF+SF 66.8 50.7 56.1 60.0 69.9\nCF+SF 61.7 50.3 56.0 57.9 69.8\nall models could not converge in our implementation even whe n re-\nducing the unit sizes. The Babel corpus is mostly composed of con-\nversational telephone speech (CTS), so it is difﬁcult to opt imize the\nS2S model from scratch with just around 10-hour training dat a. In\nthe transfer learning approach, all three fusion methods go t signif-\nicant gains by using the external LM except for deep fusion in As-\nsamese. For RNNLM trained with LLP , all fusion methods achie ved\na larger improvement than in T able 3. Interestingly , WER sig niﬁ-\ncantly dropped even when each RNNLM was trained with 10-hour\ndata only . But all fusion methods show similar performance. In con-\ntrast, CF-transfer signiﬁcantly outperformed simple tran sfer learning\nwith shallow fusion on all 5 target languages when the RNNLM w as\ntrained with FLP , which is ﬁve-times larger than LLP . Theref ore, we\ncan conclude that linguistic context is helpful for adaptat ion when\nadditional text data is available. This shows CF-transfer i n T able\n3 has the potential to surpass transfer learning with shallo w fusion\nif we can access to additional text data 3 . In summary , CF-transfer\nyielded relative 10.4% and 2.3% gains on average compared to trans-\nfer learning without and with shallow fusion, respectively .\n6. CONCLUSION\nW e explored the usage of linguistic context from the externa l LM\nduring adaptation of the language-independent S2S model to target\nlow-resource languages. W e empirically compared various L M fu-\nsion methods and conﬁrmed their effectiveness in resource l imited\nsituations. W e showed that cold fusion transfer is more effe ctive than\nsimply applying shallow fusion after adaptation when addit ional text\nis available, which means linguistic context is also helpfu l in addi-\ntion to acoustic adaptation. Our S2S model drastically clos ed the\ngap from the BLSTM-HMM hybrid system.\n3 Since the provided data only can be used for system training in BABEL\nrules, we do not explore to crawl text data from the WEB.\n7. REFERENCES\n[1] Dzmitry Bahdanau, Kyunghyun Cho, and Y oshua Bengio, “Ne ural ma-\nchine translation by jointly learning to align and translat e, ” in Proc. of\nICLR, 2015.\n[2] Jan K Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyu nghyun\nCho, and Y oshua Bengio, “ Attention-based models for speech recogni-\ntion, ” in Proc. of NIPS , 2015, pp. 577–585.\n[3] Chung-Cheng Chiu, T ara N Sainath, Y onghui Wu, Rohit Prab havalkar,\nPatrick Nguyen, Zhifeng Chen, Anjuli Kannan, Ron J W eiss, Ka nishka\nRao, Katya Gonina, et al., “State-of-the-art speech recogn ition with\nsequence-to-sequence models, ” in Proc. of ICASSP , 2018, pp. 4774–\n4778.\n[4] Shinji W atanabe, T akaaki Hori, Suyoun Kim, John R Hershe y , and\nT omoki Hayashi, “Hybrid CTC/attention architecture for en d-to-end\nspeech recognition, ” IEEE Journal of Selected T opics in Signal Pro-\ncessing, vol. 11, no. 8, pp. 1240–1253, 2017.\n[5] Albert Zeyer, Kazuki Irie, Ralf Schl ¨ uter, and Hermann N ey , “Improved\ntraining of end-to-end attention models for speech recogni tion, ” in\nProc. of Interspeech , 2018, pp. 7–11.\n[6] Siddharth Dalmia, Ramon Sanabria, Florian Metze, and Al an W Black,\n“Sequence-based multi-lingual low resource speech recogn ition, ” in\nProc. of ICASSP , 2018, pp. 4909–4913.\n[7] Jaejin Cho, Murali Karthick Baskar, Ruizhi Li, Matthew W iesner,\nSri Harish Mallidi, Nelson Y alta, Martin Karaﬁat, Shinji W a tanabe, and\nT akaaki Hori, “Multilingual sequence-to-sequence speech recognition:\narchitecture, transfer learning, and language modeling, ” in Proc. of\nSLT, 2018.\n[8] Sibo T ong, Philip N Garner, and Herv´ e Bourlard, “Multil ingual train-\ning and cross-lingual adaptation on CTC-based acoustic mod el, ” arXiv\npreprint arXiv:1711.10025 , 2017.\n[9] Sibo T ong, Philip N Garner, and Herv´ e Bourlard, “Cross- lingual adap-\ntation of a CTC-based multilingual acoustic model, ” Speech Commu-\nnication, vol. 104, pp. 39–46, 2018.\n[10] Martin Karaﬁ ´ at, Murali Karthick Baskar, Karel V esel ` y, Frantiˇ sek Gr´ ezl,\nLuk´ aˇ s Burget, et al., “ Analysis of multilingual blstm aco ustic model on\nlow and high resource languages, ” in Proc. of ICASSP , 2018, pp. 5789–\n5793.\n[11] Karel V esel ` y, Martin Karaﬁ ´ at, Frantiˇ sek Gr´ ezl, Mi loˇ s Janda, and Eka-\nterina Egorova, “The language-independent bottleneck fea tures, ” in\nProc. of SLT , 2012, pp. 336–341.\n[12] Frantisek Gr´ ezl and Martin Karaﬁ ´ at, “ Adapting multi lingual neural net-\nwork hierarchy to a new language, ” in Spoken Language T echnologies\nfor Under-Resourced Languages , 2014.\n[13] Ngoc Thang V u, David Imseng, Daniel Povey , Petr Motlice k, T anja\nSchultz, and Herv´ e Bourlard, “Multilingual deep neural ne twork based\nacoustic modeling for rapid language adaptation, ” in Proc. of ICASSP ,\n2014, pp. 7639–7643.\n[14] Markus M ¨ uller, Sebastian St ¨ uker, and Alex W aibel, “M ultilingual\nadaptation of RNN based ASR systems, ” in Proc. of ICASSP , 2018,\npp. 5219–5223.\n[15] Shinji W atanabe, T akaaki Hori, and John R Hershey , “Lan guage in-\ndependent end-to-end architecture for joint language iden tiﬁcation and\nspeech recognition, ” in Proc. of ASRU , 2017, pp. 265–271.\n[16] Shubham T oshniwal, T ara N Sainath, Ron J W eiss, Bo Li, Pe dro\nMoreno, Eugene W einstein, and Kanishka Rao, “Multilingual speech\nrecognition with a single end-to-end model, ” in Proc. of ICASSP , 2018,\npp. 4904–4908.\n[17] Suyoun Kim and Michael L Seltzer, “T owards language-un iversal end-\nto-end speech recognition, ” in Proc. of ICASSP , 2018, pp. 4914–4918.\n[18] Anjuli Kannan, Y onghui Wu, Patrick Nguyen, T ara N Saina th, Zhifeng\nChen, and Rohit Prabhavalkar, “ An analysis of incorporatin g an exter-\nnal language model into a sequence-to-sequence model, ” in Proc. of\nICASSP, 2017, pp. 5824–5828.\n[19] Shubham T oshniwal, Anjuli Kannan, Chung-Cheng Chiu, Y onghui Wu,\nT ara N Sainath, and Karen Livescu, “ A comparison of techniqu es for\nlanguage model integration in encoder-decoder speech reco gnition, ”\narXiv preprint arXiv:1807.10857 , 2018.\n[20] Caglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun Cho , Loic Bar-\nrault, Huei-Chi Lin, Fethi Bougares, Holger Schwenk, and Y o shua\nBengio, “On using monolingual corpora in neural machine tra nslation, ”\narXiv preprint arXiv:1503.03535 , 2015.\n[21] Anuroop Sriram, Heewoo Jun, Sanjeev Satheesh, and Adam Coates,\n“Cold fusion: Training seq2seq models together with langua ge mod-\nels, ” in Proc. of Interspeech , 2018, pp. 387–391.\n[22] Prajit Ramachandran, Peter J Liu, and Quoc V Le, “Unsupe rvised pre-\ntraining for sequence to sequence learning, ” in Proc. of EMNLP , 2017.\n[23] T obias Domhan and Felix Hieber, “Using target-side mon olingual data\nfor neural machine translation through multi-task learnin g, ” in Proc. of\nEMNLP, 2017, pp. 1500–1505.\n[24] T omoki Hayashi, Shinji W atanabe, Y u Zhang, T omoki T oda , T akaaki\nHori, Ramon Astudillo, and Kazuya T akeda, “Back-translati on-style\ndata augmentation for end-to-end ASR, ” in Proc. of SLT , 2018, to ap-\npear .\n[25] Masato Mimura, Sei Ueno, Hirofumi Inaguma, Shinsuke Sa kai, and\nT atsuya Kawahara, “Leveraging sequence-to-sequence spee ch synthe-\nsis for enhancing acoustic-to-word speech recognition, ” i n Proc. of\nSLT, 2018, to appear .\n[26] William Chan, Navdeep Jaitly , Quoc Le, and Oriol V inyal s, “Listen,\nattend and spell: A neural network for large vocabulary conv ersational\nspeech recognition, ” in Proc. of ICASSP , 2016, pp. 4960–4964.\n[27] Alex Graves, Santiago Fern´ andez, Faustino Gomez, and J ¨ urgen\nSchmidhuber, “Connectionist temporal classiﬁcation: lab elling un-\nsegmented sequence data with recurrent neural networks, ” i n Proc.\nof ICML , 2006, pp. 369–376.\n[28] Suyoun Kim, T akaaki Hori, and Shinji W atanabe, “Joint C TC-attention\nbased end-to-end speech recognition using multi-task lear ning, ” in\nProc. of ICASSP , 2017, pp. 4835–4839.\n[29] Mark JF Gales, Kate M Knill, Anton Ragni, and Shakti P Rat h, “Speech\nrecognition and keyword spotting for low-resource languag es: BA-\nBEL project research at CUED, ” in Spoken Language T echnologies\nfor Under-Resourced Languages , 2014.\n[30] V assil Panayotov , Guoguo Chen, Daniel Povey , and Sanje ev Khudan-\npur, “Librispeech: an ASR corpus based on public domain audi o\nbooks, ” in Proc. of ICASSP , 2015, pp. 5206–5210.\n[31] Kikuo Maekawa, “Corpus of Spontaneous Japanese: Its de sign and\nevaluation, ” in ISCA & IEEE W orkshop on Spontaneous Speech Pro-\ncessing and Recognition , 2003.\n[32] Daniel Povey , Arnab Ghoshal, Gilles Boulianne, Lukas B urget, Ondrej\nGlembek, Nagendra Goel, Mirko Hannemann, Petr Motlicek, Y a nmin\nQian, Petr Schwarz, et al., “The Kaldi speech recognition to olkit, ” in\nProc. of ASRU , 2011.\n[33] Karen Simonyan and Andrew Zisserman, “V ery deep convol u-\ntional networks for large-scale image recognition, ” arXiv preprint\narXiv:1409.1556, 2014.\n[34] Sepp Hochreiter and J ¨ urgen Schmidhuber, “Long short- term memory , ”\nNeural Computation , vol. 9, no. 8, pp. 1735–1780, 1997.\n[35] Matthew D Zeiler, “ Adadelta: an adaptive learning rate method, ” arXiv\npreprint arXiv:1212.5701 , 2012.\n[36] Samy Bengio, Oriol V inyals, Navdeep Jaitly , and Noam Sh azeer,\n“Scheduled sampling for sequence prediction with recurren t neural net-\nworks, ” in Proc. of NIPS , 2015, pp. 1171–1179.\n[37] Shinji W atanabe, T akaaki Hori, Shigeki Karita, T omoki Hayashi, Jiro\nNishitoba, Y uya Unno, Nelson Enrique Y alta Soplin, Jahn Hey mann,\nMatthew Wiesner, Nanxin Chen, et al., “ESPnet: End-to-End S peech\nProcessing T oolkit, ” in Proc. of Interspeech , 2018, pp. 2207–2211.\n[38] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chan an, Edward\nY ang, Zachary DeV ito, Zeming Lin, Alban Desmaison, Luca Ant iga,\nand Adam Lerer, “ Automatic differentiation in PyT orch, ” 20 17."
}