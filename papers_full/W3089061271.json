{
  "title": "Multi-timescale representation learning in LSTM Language Models",
  "url": "https://openalex.org/W3089061271",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5069576681",
      "name": "Shivangi Mahto",
      "affiliations": [
        "The University of Texas at Austin"
      ]
    },
    {
      "id": "https://openalex.org/A5018108126",
      "name": "Vy A. Vo",
      "affiliations": [
        "Intel (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5025310174",
      "name": "Javier S. Turek",
      "affiliations": [
        "Intel (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5077310477",
      "name": "Alexander G. Huth",
      "affiliations": [
        "The University of Texas at Austin"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2159518412",
    "https://openalex.org/W2962832505",
    "https://openalex.org/W2963735467",
    "https://openalex.org/W2787560479",
    "https://openalex.org/W2963951265",
    "https://openalex.org/W2112971401",
    "https://openalex.org/W2967697509",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2138660131",
    "https://openalex.org/W2083398114",
    "https://openalex.org/W3098692095",
    "https://openalex.org/W2994759459",
    "https://openalex.org/W2936652946",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2166481425",
    "https://openalex.org/W2963627187",
    "https://openalex.org/W2963751529",
    "https://openalex.org/W2963748792",
    "https://openalex.org/W2963411763",
    "https://openalex.org/W3034302643",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2964204621",
    "https://openalex.org/W2798819017",
    "https://openalex.org/W2537027648",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W1847088711",
    "https://openalex.org/W2251189452",
    "https://openalex.org/W2185726469",
    "https://openalex.org/W2099257174",
    "https://openalex.org/W2947012833",
    "https://openalex.org/W2464989499",
    "https://openalex.org/W2973122905",
    "https://openalex.org/W2962776659",
    "https://openalex.org/W2951682322",
    "https://openalex.org/W2134800885",
    "https://openalex.org/W2963494889"
  ],
  "abstract": "Representations within Language Models (LMs) are difficult to interpret. For example, how different layers of an LSTM LM retain information over different periods of time is unclear. In this paper, we present methods to interpret and control the timescale of information routing through an LSTM unit. We found out that a standard LSTM LM favors representations of small timescale information (up to 20 tokens). We then introduce a prior based on statistical properties of natural language, which is applied on the distribution of timescale across LSTM units to achieve an effective multi-timescale LM. The proposed model learns representations of both short as well long timescale. It also achieves better prediction performance than a standard LSTM LM on Penn Treebank and WikiText-2 datasets, especially on rare words.",
  "full_text": "Published as a conference paper at ICLR 2021\nMULTI -TIMESCALE REPRESENTATION LEARNING IN\nLSTM L ANGUAGE MODELS\nShivangi Mahto∗ Vy A. Vo\nDepartment of Computer Science Brain-Inspired Computing Lab\nThe University of Texas at Austin Intel Labs\nAustin, TX, USA Hillsboro, OR, USA\nshivangi@utexas.edu vy.vo@intel.com\nJavier S. Turek Alexander G. Huth\nBrain-Inspired Computing Lab Depts. of Computer Science & Neuroscience\nIntel Labs The University of Texas at Austin\nHillsboro, OR, USA Austin, TX, USA\njavier.turek@intel.com huth@cs.utexas.edu\nABSTRACT\nLanguage models must capture statistical dependencies between words at\ntimescales ranging from very short to very long. Earlier work has demonstrated\nthat dependencies in natural language tend to decay with distance between words\naccording to a power law. However, it is unclear how this knowledge can be\nused for analyzing or designing neural network language models. In this work,\nwe derived a theory for how the memory gating mechanism in long short-term\nmemory (LSTM) language models can capture power law decay. We found that\nunit timescales within an LSTM, which are determined by the forget gate bias,\nshould follow an Inverse Gamma distribution. Experiments then showed that\nLSTM language models trained on natural English text learn to approximate this\ntheoretical distribution. Further, we found that explicitly imposing the theoretical\ndistribution upon the model during training yielded better language model per-\nplexity overall, with particular improvements for predicting low-frequency (rare)\nwords. Moreover, the explicit multi-timescale model selectively routes information\nabout different types of words through units with different timescales, potentially\nimproving model interpretability. These results demonstrate the importance of\ncareful, theoretically-motivated analysis of memory and timescale in language\nmodels.\n1 I NTRODUCTION\nAutoregressive language models are functions that estimate a probability distribution over the next\nword in a sequence from past words, p(wt|wt−1,...,w 1). This requires capturing statistical de-\npendencies between words over short timescales, where syntactic information likely dominates\n(Adi et al., 2017; Linzen et al., 2016), as well as long timescales, where semantic and narrative\ninformation likely dominate (Zhu et al., 2018; Conneau et al., 2018; Gulordava et al., 2018). Because\nthis probability distribution grows exponentially with sequence length, some approaches simplify\nthe problem by ignoring long-range dependencies. Classical n-gram models, for example, assume\nword wt is independent of all but the last n−1 words, with typical n= 5 (Heaﬁeld, 2011). Hidden\nMarkov models (HMMs) assume that the inﬂuence of previous words decays exponentially with\ndistance from the current word (Lin & Tegmark, 2016).\nIn contrast, neural network language models such as recurrent (Hochreiter & Schmidhuber, 1997;\nMerity et al., 2018; Melis et al., 2018) and transformer networks (Melis et al., 2019; Krause et al.,\n2019; Dai et al., 2019) include longer-range interactions, but simplify the problem by working\nin lower-dimensional representational spaces. Attention-based networks combine position and\n∗Current afﬁliation: Apple Inc.\n1\narXiv:2009.12727v2  [cs.CL]  18 Mar 2021\nPublished as a conference paper at ICLR 2021\ncontent-based information in a small number of attention heads to ﬂexibly capture different types\nof dependencies within a sequence (Vaswani et al., 2017; Cordonnier et al., 2019). Gated recurrent\nneural networks (RNNs) compress information about past words into a ﬁxed-length state vector\n(Hochreiter & Schmidhuber, 1997). The inﬂuence each word has on this state vector tends to decay\nexponentially over time. However, each element of the state vector can have a different exponential\ntime constant, or “timescale” (Tallec & Ollivier, 2018), enabling gated RNNs like the long short-term\nmemory (LSTM) network to ﬂexibly learn many different types of temporal relationships (Hochreiter\n& Schmidhuber, 1997). Stacked LSTM networks reduce to a single layer (Turek et al., 2020), showing\nthat network depth has an insigniﬁcant inﬂuence on how the LSTM captures temporal relationships.\nYet in all these networks the shape of the temporal dependencies must be learned directly from the\ndata. This seems particularly problematic for very long-range dependencies, which are only sparsely\ninformative (Lin & Tegmark, 2016).\nThis raises two related questions: what should the temporal dependencies in a language model look\nlike? And how can that information be incorporated into a neural network language model?\nTo answer the ﬁrst question, we look to empirical and theoretical work that has explored the de-\npendency statistics of natural language. Lin & Tegmark (2016) quantiﬁed temporal dependencies\nin English and French language corpora by measuring the mutual information between tokens as a\nfunction of the distance between them. They observed that mutual information decays as a power\nlaw, i.e. MI(wk,wk+t) ∝t−d for constant d. This behavior is common to hierarchically structured\nnatural languages (Lin & Tegmark, 2016; Sainburg et al., 2019) as well as sequences generated from\nprobabilistic context-free grammars (PCFGs) (Lin & Tegmark, 2016).\nNow to the second question: if temporal dependencies in natural language follow a power law, how\ncan this information be incorporated into neural network language models? To our knowledge, little\nwork has explored how to control the temporal dependencies learned in attention-based models.\nHowever, many approaches have been proposed for controlling gated RNNs, including updating\ndifferent groups of units at different intervals (El Hihi & Bengio, 1996; Koutnik et al., 2014; Liu et al.,\n2015; Chung et al., 2017), gating units across layers (Chung et al., 2015), and explicitly controlling\nthe input and forget gates that determine how information is stored and removed from memory (Xu\net al., 2016; Shen et al., 2018; Tallec & Ollivier, 2018). Yet none of these proposals incorporate a\nspeciﬁc shape of temporal dependencies based on the known statistics of natural language.\nIn this work, we build on the framework of Tallec & Ollivier (2018) to develop a theory for how the\nmemory mechanism in LSTM language models can capture temporal dependencies that follow a\npower law. This relies on deﬁning the timescale of an individual LSTM unit based on how the unit\nretains and forgets information. We show that this theory predicts the distribution of unit timescales\nfor LSTM models trained on both natural English (Merity et al., 2018) and formal languages (Suzgun\net al., 2019). Further, we show that forcing models to follow this theoretical distribution improves\nlanguage modeling performance. These results highlight the importance of combining theoretical\nmodeling with an understanding of how language models capture temporal dependencies over multiple\nscales.\n2 M ULTI -TIMESCALE LANGUAGE MODELS\n2.1 T IMESCALE OF INFORMATION\nWe are interested in understanding how LSTM language models capture dependencies across time.\nTallec & Ollivier (2018) elegantly argued that memory in individual LSTM units tends to decay\nexponentially with a time constant determined by weights within the network. We refer to the time\nconstant of that exponential decay as the unit’s representational timescale.\nTimescale is directly related to the LSTM memory mechanism (Hochreiter & Schmidhuber, 1997),\nwhich involves the LSTM cell state ct, input gate it and forget gate ft,\nit = σ(Wixxt + Wihht−1 + bi)\nft = σ(Wfxxt + Wfhht−1 + bf)\n˜ct = tanh(Wcxxt + Wchht−1 + bc)\nct = ft ⊙ct−1 + it ⊙˜ct,\n2\nPublished as a conference paper at ICLR 2021\nwhere xt is the input at time t, ht−1 is the hidden state, Wih,Wix,Wfh,Wfx,Wch,Wcx are the\ndifferent weights andbi,bf,bcthe respective biases. σ(·) and tanh(·) represent element-wise sigmoid\nand hyperbolic tangent functions. Input and forget gates control the ﬂow of information in and out\nof memory. The forget gate ft controls how much memory from the last time step ct−1 is carried\nforward to the current state ct. The input gate it controls how much information from the input xt\nand hidden state ht−1 at the current timestep is stored in memory for subsequent timesteps.\nTo ﬁnd the representational timescale, we consider a “free input” regime with zero input to the LSTM\nafter timestep t0, i.e., xt = 0 for t>t 0. Ignoring information leakage through the hidden state (i.e.,\nassuming Wch = 0, bc = 0, and Wfh = 0) the cell state update becomes ct = ft ⊙ct−1. For t>t 0,\nit can be further simpliﬁed as\nct = ft−t0\n0 ⊙c0\n= e(log f0)(t−t0) ⊙c0,\n(1)\nwhere c0 = ct0 is the cell state at t0, and f0 = σ(bf) is the value of the forget gate, which depends\nonly on the forget gate bias bf. Equation 1 shows that LSTM memory exhibits exponential decay\nwith characteristic forgetting time\nT = − 1\nlog f0\n= 1\nlog(1 + e−bf). (2)\nThat is, values in the cell state tend to shrink by a factor of eevery T timesteps. We refer to the\nforgetting time in Equation 2 as the representational timescale of an LSTM unit.\nBeyond the “free input” regime, we can estimate the timescale for a LSTM unit by measuring the\naverage forget gate value over a set of test sequences,\nTest = − 1\nlog ¯f, (3)\nwhere ¯f = 1\nKN\n∑N\nj=1\n∑K\nt=1 fj\nt, in which fj\nt is the forget gate value of the unit at t-th timestep for\nj-th test sequence, N is the number of test sequences, and Kis the test sequence length.\n2.2 C OMBINING EXPONENTIAL TIMESCALES TO YIELD A POWER LAW\nFrom earlier work, we know that temporal dependencies in natural language tend to decay following\na power law (Lin & Tegmark, 2016; Sainburg et al., 2019). Yet from Equation 1 we see that LSTM\nmemory tends to decay exponentially. These two decay regimes are fundamentally different–the ratio\nof a power law divided by an exponential always tends towards inﬁnity. However, LSTM language\nmodels contain many units, each of which can have a different timescale. Thus LSTM language\nmodels might approximate power law decay through a combination of exponential functions. Here\nwe derive a theory for how timescales should be distributed within an LSTM in order to yield overall\npower law decay.\nLet us assume that the timescale T for each LSTM unit is drawn from a distribution P(T). We want\nto ﬁnd a P(T) such that the expected value over T of the function e\n−t\nT approximates a power law\ndecay t−d for some constant d,\nt−d ∝ET[e−t/T] =\n∫ ∞\n0\nP(T)e−t/TdT. (4)\nSolving this problem reveals thatP(T) is an Inverse Gamma distributionwith shape parameter α= d\nand scale parameter β = 1 (see Section A.1 for derivation). The probability density function of the\nInverse Gamma distribution is given as P(T; α,β) = βα\nΓ(α) (1/T)α+1e(−β/T).\nThis theoretical result suggests that in order to approximate the power law decay of information\nin natural language, unit timescales in LSTM language models should follow an Inverse Gamma\ndistribution. We next perform experiments to test whether this prediction holds true for models trained\non natural language and models trained on samples from a formal language with known temporal\nstatistics. We then test whether enforcing an Inverse Gamma timescale distribution at training time\nimproves model performance.\n3\nPublished as a conference paper at ICLR 2021\n2.3 C ONTROLLING LSTM UNIT TIMESCALES\nTo enforce a speciﬁc distribution of timescales in an LSTM and thus create an explicit multi-timescale\nmodel, we drew again upon the methods developed in Tallec & Ollivier (2018). Following the\nanalysis in Section 2.1, the desired timescale Tdesired for an LSTM unit can be controlled by setting\nthe forget gate bias to the value\nbf = −log(e\n1\nTdesired −1). (5)\nThe balance between forgetting information from the previous timestep and adding new information\nfrom the current timestep is controlled by the relationship between forget and input gates. To maintain\nthis balance we set the input gate bias bi to the opposite value of the forget gate, i.e., bi = −bf. This\nensures that the relation it ≈1 −ft holds true. Importantly, these bias values remain ﬁxed (i.e. are\nnot learned) during training, in order to keep the desired timescale distribution across the network.\n3 E VALUATION\n3.1 E XPERIMENTAL SETUP\nHere we describe our experimental setup. We examined two regimes: natural language data,\nand synthetic data from a formal language. All models were implemented in pytorch (Paszke\net al., 2019) and the code can be downloaded from https://github.com/HuthLab/\nmulti-timescale-LSTM-LMs .\n3.1.1 N ATURAL LANGUAGE\nWe experimentally evaluated LSTM language models trained on the Penn Treebank (PTB) (Marcus\net al., 1999; Mikolov et al., 2011) and WikiText-2 (WT2) (Merity et al., 2017) datasets. PTB contains\na vocabulary of 10K unique words, with 930K tokens in the training, 200K in validation, and 82K in\ntest data. WT2 is a larger dataset with a vocabulary size of 33K unique words, almost 2M tokens in\nthe training set, 220K in the validation set, and 240K in the test set. As a control, we also generated\na Markovian version of the PTB dataset. Using the empirical bigram probabilities from PTB, we\nsampled tokens sequentially until a new corpus of the same size had been generated.\nWe compared two language models: a standard stateful LSTM language model (Merity et al., 2018)\nas the baseline, and our multi-timescale language model. Both models comprise three LSTM layers\nwith 1150 units in the ﬁrst two layers and 400 units in the third layer, with an embedding size of\n400. Input and output embeddings were tied. All models were trained using SGD followed by\nnon-monotonically triggered ASGD for 1000 epochs. Training sequences were of length 70 with a\nprobability of 0.95 and 35 with a probability of 0.05. During inference, all test sequences were length\n70. For training, all embedding weights were uniformly initialized in the interval [−0.1,0.1]. All\nweights and biases of the LSTM layers in the baseline language model were uniformly initialized\nbetween\n[−1\nH , 1\nH\n]\nwhere H is the output size of the respective layer.\nThe multi-timescale language model has the same initialization, except for the forget and input gate\nbias values that are assigned following Equation 5 and ﬁxed during training. For layer 1, we assigned\ntimescale T = 3 to half the units and T = 4 to the other half. For layer 2, we assigned timescales\nto each unit by selecting values from an Inverse Gamma distribution. The shape parameter of the\ndistribution was set to α= 0.56 after testing different values (see A.5). The timescales in this layer\nhad 80% of the units below 20, and the rest ranging up to the thousands. For layer 3, the biases were\ninitialized like the baseline language model and trained (not ﬁxed).\n3.1.2 F ORMAL LANGUAGE : THE DYCK -2 G RAMMAR\nWe also tested the LSTM language models on a formal language with known temporal statistics.\nThe Dyck-2 grammar (Suzgun et al., 2019) deﬁnes all possible valid sequences using two types\nof parentheses. Because Dyck-2 is a probabilistic context-free grammar (PCFG), its temporal\ndependencies will tend to decay following a power law (Lin & Tegmark, 2016). Further, timescales\nwithin particular sequences can be measured as the distance between matching opening and closing\nparenthesis pairs. We randomly sampled from the grammar with probabilities p1 = 0.25, p2 = 0.25,\n4\nPublished as a conference paper at ICLR 2021\nFigure 1: Empirical timescale distributions and best distribution ﬁts for an LSTM language model\ntrained on natural language (PTB; blue), and the same model trained on a Markovian corpus (Markov-\nPTB; red). Left: An Inverse Gamma distribution is the best ﬁt for the PTB corpus (dotted blue), while\na narrow Gaussian is the best ﬁt for the Markovian corpus (dotted red). Right: We determined the\nbest ﬁts by varying distribution parameters (αfor the Inverse Gamma, µfor the narrow Gaussian)\nand computing the Kolmogorov-Smirnov statistic to measure the difference between the empirical\nand theoretical distributions (lower is better). The LSTM trained on natural language (blue) is best ﬁt\nby the Inverse Gamma distribution with α= 1.4, while the LSTM trained on a Markovian corpus\n(red) is best ﬁt by the narrow Gaussian distribution with µ= 0.5.\nand q = 0 .25 to generate training, validation, and test sets with 10K, 2K, and 5K sequences.\nSequences contain a maximum of 200 elements.\nTo solve this task, we followed Suzgun et al. (2019). Models were trained to predict which closing\nparentheses could appear next. Elements in the sequence were one-hot encoded and fed as input to\nthe models. We trained a baseline model and a multi-timescale version. Each model consists of a\n256-unit LSTM layer followed by a linear output layer. The output layer reduces the dimension to\n2, the number of possible closing parentheses. These output values are each converted to a (0,1)\ninterval with a sigmoid function. Parameters were initialized using a uniform distribution. The\nmulti-timescale model has the forget and input gate bias values assigned using an Inverse Gamma\ndistribution with α = 1.50, and ﬁxed during training. This parameter best matched the measured\ndistribution of timescales. Training minimized the mean squared error (MSE) loss using the Adam\noptimizer (Kingma & Ba, 2015) with learning rate 1e−4, β1 = 0.9, β2 = 0.999, and ε= 1e−8 for\n2000 epochs.\n3.2 E XPERIMENTAL RESULTS\n3.2.1 D O EMPIRICAL LSTM UNIT TIMESCALES APPROXIMATE LANGUAGE STATISTICS ?\nIn Section 2.2 we predicted that the distribution of unit timescales within an LSTM language model\nshould follow an Inverse Gamma distribution. To test this prediction, we trained an LSTM language\nmodel on a natural language corpus (PTB), and then trained the same model on a Markovian dataset\ngenerated with the bigram statistics of PTB (Markov-PTB). We estimated the timescale of each\nunit using Equation 3. While representing PTB should require an Inverse Gamma distribution of\ntimescales, representing Markov-PTB should only require a single timescale (i.e. a delta function,\nwhich can be approximated as a narrow Gaussian with σ=0.1). We tested this by generating a\nfamily of Inverse Gamma distributions or narrow Gaussian functions, and measuring the difference\nbetween the empirical distributions and these theoretical distributions. This was quantiﬁed with the\nKolmogorov-Smirnov test statistic (Massey Jr, 1951).\nThe results in Figure 1 suggest that our theoretical prediction is correct. The Inverse Gamma\ndistribution better ﬁts the model trained on natural language, while the approximate delta function\n(narrow Gaussian distribution) better ﬁts the model trained on the Markovian corpus.\n5\nPublished as a conference paper at ICLR 2021\nTable 1: Perplexity of the multi-timescale and baseline models for tokens across different frequency\nbins for the Penn TreeBank (PTB) and WikiText-2 (WT2) test datasets. We also report the mean\ndifference in perplexity (baseline −multi-timescale) across 10,000 bootstrapped samples, along with\nthe 95% conﬁdence interval (CI).\nDataset: Penn TreeBank\nModel above 10K 1K-10K 100-1K below 100 All tokens\nBaseline 6.82 27.77 184.19 2252.50 61.40\nMulti-timescale 6.84 27.14 176.11 2100.89 59.69\nMean diff. -0.02 0.63 8.08 152.03 1.71\n95% CI [-0.06, 0.02] [0.38, 0.88] [6.04, 10.2] [119.1,186.0] [1.41, 2.02]\nDataset: WikiText-2\nBaseline 7.49 49.70 320.59 4631.08 69.88\nMulti-timescale 7.46 48.52 308.43 4318.72 68.08\nMean diff. 0.03 1.17 12.20 312.13 1.81\n95% CI [0.01,0.06] [0.83,1.49] [9.96,14.4] [267.9,356.3] [1.61,2.01]\n3.2.2 D OES IMPOSING AN INVERSE GAMMA DISTRIBUTION IMPROVE MODEL\nPERFORMANCE ?\nThe previous section showed that the empirical distribution of timescales in an LSTM LM trained on\nnatural language is well-approximated by an Inverse Gamma distribution, as predicted. However, the\nmodel had to learn this distribution from a small and noisy dataset. If the Inverse Gamma distribution\nis indeed optimal for capturing power law dependencies, then it should be beneﬁcial to simply enforce\nthis distribution. To test this hypothesis, we constructed models with enforced Inverse Gamma\ntimescales, which we call multi-timescale (MTS) language models (see Section 3.1.1 for details).\nCompared to the baseline LSTM model, the multi-timescale model incorporates a timescale prior\nthat matches the statistical temporal dependencies of natural language. The multi-timescale model is\nparticularly enriched in long timescale units (see A.5).\nIf this is an effective and useful prior, the multi-timescale model should perform better than the\nbaseline when very long timescales are needed to accomplish a task. We ﬁrst compared total language\nmodeling performance between the baseline and multi-timescale models. The overall perplexities\non the test datasets are shown in the far right column of Table 1. The multi-timescale language\nmodel outperforms the baseline model for both datasets by an average margin of 1.60 perplexity.\nBootstrapping over the test set showed that this improvement is statistically signiﬁcant. Test data were\ndivided into 100-word sequences and resampled with replacement 10,000 times. For each sample,\nwe computed the difference in model perplexity (baseline −multi-timescale) and reported the 95%\nconﬁdence intervals (CI) in Table 1. Differences are signiﬁcant at p< 0.05 if the CI does not overlap\nwith 0.\nPrevious work has demonstrated that common, typically closed-class, words rely mostly on short\ntimescale information, whereas rare, typically open-class, words require longer timescale informa-\ntion (Khandelwal et al., 2018; Grifﬁths et al., 2005; Rosenfeld, 1994; Iyer & Ostendorf, 1996). To\ntest whether the improved performance of the multi-timescale model could be attributed to the very\nlong timescale units, we computed model perplexities across different word frequency bins. We\nhypothesized that the multi-timescale model should show the greatest improvements for infrequent\nwords, while showing little to no improvement for common words.\nWe divided the words in the test dataset into 4 bins depending on their frequencies in the training\ncorpus: more than 10,000 occurrences; 1000-10,000 occurrences; 100-1000 occurrences; and fewer\nthan 100 occurrences. Then we compared performance of the models for words in each bin in Table 1.\nThe multi-timescale model performed signiﬁcantly better than baseline in both datasets for the 2\nless frequent bins (bootstrap test), with increasing difference for less frequent words. This suggests\nthat the performance advantage of the multi-timescale model is highest for infrequent words, which\nrequire very long timescale information.\n6\nPublished as a conference paper at ICLR 2021\nFigure 2: The left plot shows the performance of the baseline (blue) and multi-timescale (red) models\non the Dyck-2 grammar as a function of the maximum timescale in a sequence (20 repetitions,\nstandard deviation). A correctly predicted sequence is considered to have all the outputs correct.\nThe graph shows that the multi-timescale model is predicting better up to 10% more sequences in\nthe longer timescales regime (T >75). The right plot describe the distribution of all timescales as\ncomputed from the training data. The timescale distribution decays as a power law. The sharp decay\nnear T = 200 is due to the limitation of the sequence lengths imposed to generate the training data.\nThese results strongly support our claim that the Inverse Gamma timescale distribution is a good\nprior for models trained on natural language that is known to have power law temporal dependencies.\n3.2.3 G ENERALIZATION TO FORMAL LANGUAGES\nComplex dependencies in natural language data may not necessarily follow any precise statistical\nlaw, so we also turned to formal languages to evaluate our hypotheses. In this experiment we used\nthe Dyck-2 grammar, which enables us to precisely measure the distribution of timescales within\nthe data (see Figure 2 right). We ﬁrst used this capability to ﬁnd the best αparameter value for the\nInverse Gamma distribution ﬁtting a power law function, and then to measure model performance as\na function of timescale.\nFollowing Suzgun et al. (2019), baseline and multi-timescale language models were trained to predict\nclosing parenthesis types. A sequence is considered correctly predicted when every output from\nthe model is correct across the entire sequence. The baseline model was able to correctly predict\nthe output of 91.66% of test sequences, while the multi-timescale model achieved 93.82% correct.\nFigure 2 (left) shows percent correct as a function of the maximum timescale in a sequence over\n20 training repetitions. Both models succeeded at predicting most sequences shorter than 75 steps.\nHowever, for longer timescales, the multi-timescale model correctly predicted 5-10% more sequences\nthan the baseline model. This result supplements the idea that enforcing Inverse Gamma timescales\nimproves the model’s ability to capture longer-range dependencies. Nevertheless, both models are\nfar from perfect on longer timescales, raising the question of how to further improve these longer\ntimescales.\n3.2.4 R OUTING OF INFORMATION THROUGH LSTM UNITS WITH DIFFERENT TIMESCALES\nOur previous results demonstrated successful control the timescale distribution in our multi-timescale\nmodel, and that this improved model performance. Breaking out performance results by word\nfrequency also showed that the multi-timescale model was better able to capture some words–\nspeciﬁcally, low-frequency words–than the baseline LSTM. This suggests that the multi-timescale\nmodel, which is enriched in long-timescale units relative to the baseline model, may be selectively\nrouting information about infrequent words through speciﬁc long-timescale units. More broadly, the\nmulti-timescale model could be selectively routing information to each unit based on its timescale.\nThis could make the multi-timescale model more interpretable, since at least one property of each\nunit–its timescale–would be known ahead of time. To test this hypothesis, we again divided the\ntest data into word frequency bins. If long timescale information is particularly important for low\nfrequency words, then we would expect information about those words to be selectively routed\nthrough long timescale units. We tested the importance of each LSTM unit for words in each\n7\nPublished as a conference paper at ICLR 2021\nFigure 3: Information routing across different units of the multi-timescale LSTM for PTB dataset.\nEach line shows results for words in a different frequency bin (i.e. rare words occur <100 times, red\nline). The ordinate axis shows the ratio of model perplexity with and without ablation of a group\nof 50 LSTM units, sorted and grouped by assigned timescale. Ratios above 1 indicate a decrease\nin performance following ablation, suggesting that the ablated units are important for predicting\nwords in the given frequency bin. Abscissa shows the average timescale of each group (left is longer\ntimescale).\nfrequency bin by selectively ablating the units during inference, and then measuring the effect on\nprediction performance.\nWe divided the LSTM units from layer 2 of the multi-timescale model into 23 groups of 50 consecutive\nunits, sorted by assigned timescale. We ablated one group of units at a time by explicitly setting their\noutput to 0, while keeping the rest of the units active. We then computed the model perplexity for\ndifferent word frequency bins, and plotted the ratio of the perplexity with and without ablation. If\nperformance gets worse for a particular word frequency bin when ablating a particular group of units,\nit implies that the ablated units are important for predicting those words.\nFigure 3 shows this ratio across all frequency bins and groups for the PTB dataset (similar results\nfor WikiText-2 are shown in the supplement). Ablating units with long timescales (integrating\nover 20-300 timesteps) causes performance to degrade the most for low frequency words (below\n100 and 1K-10K occurences); ablating units with medium timescales (5-10 timesteps) worsens\nperformance for medium frequency words (1K-10K occurrences); and ablating units with the shortest\ntimescales (<1 timestep) resulted in worse performance on the highest frequency words. These\nresults demonstrate that timescale-dependent information is routed through different units in this\nmodel, suggesting that the representations that are learned for different timescales are interpretable.\n4 C ONCLUSION\nIn this paper we developed a theory for how LSTM language models can capture power law temporal\ndependencies. We showed that this theory predicts the distribution of timescales in LSTM language\nmodels trained on both natural and formal languages. We also found that explicit multi-timescale\nmodels that are forced to follow this theoretical distribution give better performance, particularly over\nvery long timescales. Finally, we show evidence that information dependent on different timescales\nis routed through speciﬁc units, demonstrating that the unit timescales are highly interpretable. This\nenhanced interpretability makes it possible to use LSTM activations to predict brain data, as in (Jain\n& Huth, 2018), and estimate processing timescales for different brain regions (Jain et al., 2020).\nThese results highlight the importance of theoretical modeling and understanding of how language\nmodels capture dependencies over multiple timescales.\n8\nPublished as a conference paper at ICLR 2021\nACKNOWLEDGMENTS\nWe would like to thank Shailee Jain for valuable feedback on the manuscript and useful discussions,\nand the anonymous reviewers for their insights and suggestions. Funding support for this work came\nfrom the Burroughs Wellcome Fund Career Award at the Scientiﬁc Interface (CASI), Intel Research\nAward, and Alfred P. Sloan Foundation Research Fellowship.\nREFERENCES\nYossi Adi, Einat Kermany, Yonatan Belinkov, Ofer Lavi, and Yoav Goldberg. Fine-grained analysis\nof sentence embeddings using auxiliary prediction tasks. In International Conference on Learn-\ning Representations, 2017. URL https://openreview.net/forum?id=BJh6Ztuxl&\nnoteId=BJh6Ztuxl.\nJunyoung Chung, C ¸aglar G¨ulc ¸ehre, Kyunghyun Cho, and Yoshua Bengio. Gated feedback recurrent\nneural networks. In ICML, pp. 2067–2075, 2015. URLhttp://proceedings.mlr.press/\nv37/chung15.html.\nJunyoung Chung, Sungjin Ahn, and Yoshua Bengio. Hierarchical multiscale recurrent neural networks.\nIn Proceedings of the 5th International Conference on Learning Representations, 2017. URL\nhttps://openreview.net/forum?id=S1di0sfgl.\nAlexis Conneau, Germ´an Kruszewski, Guillaume Lample, Lo¨ıc Barrault, and Marco Baroni. What\nyou can cram into a single vector: Probing sentence embeddings for linguistic properties. In\nProceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume\n1: Long Papers), pp. 2126–2136, 2018. URL https://www.aclweb.org/anthology/\nP18-1198/.\nJean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. On the relationship between self-\nattention and convolutional layers. In International Conference on Learning Representations,\n2019.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime G Carbonell, Quoc Le, and Ruslan Salakhutdinov.\nTransformer-xl: Attentive language models beyond a ﬁxed-length context. In Proceedings of the\n57th Annual Meeting of the Association for Computational Linguistics, pp. 2978–2988, 2019. URL\nhttps://www.aclweb.org/anthology/P19-1285/.\nSalah El Hihi and Yoshua Bengio. Hierarchical recurrent neural networks\nfor long-term dependencies. In Advances in neural information processing\nsystems, pp. 493–499, 1996. URL http://papers.nips.cc/paper/\n1102-hierarchical-recurrent-neural-networks-for-long-term-dependencies.\npdf.\nThomas L Grifﬁths, Mark Steyvers, David M Blei, and Joshua B Tenenbaum. Integrating topics and\nsyntax. In Advances in neural information processing systems, pp. 537–544, 2005.\nKristina Gulordava, Piotr Bojanowski, ´Edouard Grave, Tal Linzen, and Marco Baroni. Colorless\ngreen recurrent networks dream hierarchically. In Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers), pp. 1195–1205, 2018. URL https://www.aclweb.\norg/anthology/N18-1108/.\nKenneth Heaﬁeld. Kenlm: Faster and smaller language model queries. In Proceedings of the sixth\nworkshop on statistical machine translation, pp. 187–197, 2011.\nSepp Hochreiter and J ¨urgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):\n1735–1780, 1997. doi: 10.1162/neco.1997.9.8.1735. URL https://doi.org/10.1162/\nneco.1997.9.8.1735.\nRukmini Iyer and Mari Ostendorf. Modeling long distance dependence in language: Topic mixtures\nvs. dynamic cache models. In Proceeding of Fourth International Conference on Spoken Language\nProcessing. ICSLP’96, volume 1, pp. 236–239. IEEE, 1996.\n9\nPublished as a conference paper at ICLR 2021\nShailee Jain and Alexander Huth. Incorporating context into language encod-\ning models for fmri. In Advances in neural information processing sys-\ntems, pp. 6628–6637, 2018. URL http://papers.nips.cc/paper/\n7897-incorporating-context-into-language-encoding-models-for-fmri .\nShailee Jain, Vy V o, Shivangi Mahto, Amanda LeBel, Javier S Turek, and Alexander Huth.\nInterpretable multi-timescale models for predicting fmri responses to continuous natural\nspeech. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Ad-\nvances in Neural Information Processing Systems, volume 33, pp. 13738–13749. Curran As-\nsociates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/\n9e9a30b74c49d07d8150c8c83b1ccf07-Paper.pdf.\nUrvashi Khandelwal, He He, Peng Qi, and Dan Jurafsky. Sharp nearby, fuzzy far away: How neural\nlanguage models use context. In Proceedings of the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pp. 284–294, Melbourne, Australia, July\n2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1027. URL https:\n//www.aclweb.org/anthology/P18-1027.\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua\nBengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR\n2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http:\n//arxiv.org/abs/1412.6980.\nJan Koutnik, Klaus Greff, Faustino Gomez, and Juergen Schmidhuber. A clockwork rnn. In Interna-\ntional Conference on Machine Learning, pp. 1863–1871, 2014. URL http://proceedings.\nmlr.press/v32/koutnik14.html.\nBen Krause, Emmanuel Kahembwe, Iain Murray, and Steve Renals. Dynamic evaluation of trans-\nformer language models. arXiv preprint arXiv:1904.08378, 2019. URL https://arxiv.org/\nabs/1904.08378.\nHenry W Lin and Max Tegmark. Critical behavior from deep dynamics: a hidden dimension in\nnatural language. arXiv preprint arXiv:1606.06737, 2016. URL https://cbmm.mit.edu/\nsites/default/files/publications/1606.06737.pdf.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg. Assessing the ability of lstms to learn syntax-\nsensitive dependencies. Transactions of the Association for Computational Linguistics, 4:521–535,\n2016. URL https://www.aclweb.org/anthology/Q16-1037/.\nPengfei Liu, Xipeng Qiu, Xinchi Chen, Shiyu Wu, and Xuan-Jing Huang. Multi-timescale long\nshort-term memory neural network for modelling sentences and documents. In Proceedings of the\n2015 Conference on Empirical Methods in Natural Language Processing, pp. 2326–2335, 2015.\nURL https://www.aclweb.org/anthology/D15-1280/.\nMitchell P Marcus, Beatrice Santorini, Mary Ann Marcinkiewicz, and Ann Taylor. Treebank-3.\nLinguistic Data Consortium, Philadelphia, 14, 1999. URL https://catalog.ldc.upenn.\nedu/LDC99T42.\nFrank J Massey Jr. The kolmogorov-smirnov test for goodness of ﬁt. Journal of the American\nstatistical Association, 46(253):68–78, 1951.\nG´abor Melis, Tom ´aˇs Koˇcisk`y, and Phil Blunsom. Mogriﬁer lstm. In International Conference\non Learning Representations, 2019. URL https://openreview.net/forum?id=\nSJe5P6EYvS&utm_campaign=NLP%20News&utm_medium=email&utm_source=\nRevue%20newsletter.\nG´abor Melis, Chris Dyer, and Phil Blunsom. On the state of the art of evaluation in neural language\nmodels. In International Conference on Learning Representations, 2018. URL https://\nopenreview.net/forum?id=ByJHuTgA-.\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture\nmodels. In International Conference on Learning Representations, 2017. URL https://\nopenreview.net/forum?id=Byj72udxe.\n10\nPublished as a conference paper at ICLR 2021\nStephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and optimizing LSTM\nlanguage models. In International Conference on Learning Representations, 2018. URL https:\n//openreview.net/forum?id=SyyGPP0TZ.\nTom´aˇs Mikolov, Anoop Deoras, Stefan Kombrink, Luk ´aˇs Burget, and Jan ˇCernock`y. Empirical\nevaluation and combination of advanced language modeling techniques. In Twelfth Annual\nConference of the International Speech Communication Association, 2011. URL https://www.\nisca-speech.org/archive/interspeech_2011/i11_0605.html.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,\nLu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance\ndeep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d’ Alch ´e-Buc,\nE. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp.\n8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/\n9015-pytorch-an-imperative-style-high-performance-deep-learning-library.\npdf.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and\nLuke Zettlemoyer. Deep contextualized word representations. In Proceedings of the 2018 Con-\nference of the North American Chapter of the Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long Papers), pp. 2227–2237, New Orleans, Louisiana,\nJune 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1202. URL\nhttps://www.aclweb.org/anthology/N18-1202.\nRonald Rosenfeld. A hybrid approach to adaptive statistical language modeling. Technical report,\nCARNEGIE-MELLON UNIV PITTSBURGH PA SCHOOL OF COMPUTER SCIENCE, 1994.\nTim Sainburg, Brad Theilman, Marvin Thielk, and Timothy Q Gentner. Parallels in the sequential\norganization of birdsong and human speech. Nature communications, 10(1):1–11, 2019. URL\nhttps://www.nature.com/articles/s41467-019-11605-y .\nYikang Shen, Shawn Tan, Alessandro Sordoni, and Aaron Courville. Ordered neurons: Integrating\ntree structures into recurrent neural networks. In International Conference on Learning Represen-\ntations, 2018. URL https://openreview.net/forum?id=B1l6qiR5F7&source=\npost_page--------------------------- .\nMirac Suzgun, Sebastian Gehrmann, Yonatan Belinkov12, and Stuart M Shieber. Lstm networks can\nperform dynamic counting. ACL 2019, pp. 44, 2019.\nCorentin Tallec and Yann Ollivier. Can recurrent neural networks warp time? In International\nConference on Learning Representations, 2018. URL https://openreview.net/forum?\nid=SJcKhk-Ab.\nJavier Turek, Shailee Jain, Vy V o, Mihai Capot˘a, Alexander Huth, and Theodore Willke. Approxi-\nmating stacked and bidirectional recurrent architectures with the delayed recurrent neural network.\nIn Hal Daum´e III and Aarti Singh (eds.), Proceedings of the 37th International Conference on Ma-\nchine Learning, volume 119 ofProceedings of Machine Learning Research, pp. 9648–9658. PMLR,\n13–18 Jul 2020. URL http://proceedings.mlr.press/v119/turek20a.html.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,Łukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information\nprocessing systems, pp. 5998–6008, 2017.\nJiacheng Xu, Danlu Chen, Xipeng Qiu, and Xuan-Jing Huang. Cached long short-term memory neural\nnetworks for document-level sentiment classiﬁcation. In Proceedings of the 2016 Conference\non Empirical Methods in Natural Language Processing, pp. 1660–1669, 2016. URL https:\n//www.aclweb.org/anthology/D16-1172/.\nXunjie Zhu, Tingfeng Li, and Gerard De Melo. Exploring semantic properties of sentence em-\nbeddings. In Proceedings of the 56th Annual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pp. 632–637, 2018. URL https://www.aclweb.org/\nanthology/P18-2100/.\n11\nPublished as a conference paper at ICLR 2021\nA S UPPLEMENTARY MATERIAL\nA.1 D ERIVATION OF INVERSE GAMMA TIMESCALE DISTRIBUTION\nIn Section 2.2 we examined how exponentials can be additively combined to yield a power law. Here\nwe give a detailed derivation of how choosing exponential timescales according to the Inverse Gamma\ndistribution with shape parameter α= dand scale parameter β = 1 yields power law decay with\nexponent −d.\nRecall that our goal is to identify the probability distribution over exponential timescales P(T) such\nthat the expected value over T of the exponential decay function e−t\nT approximates a power law\ndecay t−d with some constant d,\nt−d ∝ET[e−t/T] =\n∫ ∞\n0\nP(T)e−t\nT dT.\nNoting its similarity to the integral we are attempting to solve, we begin with the deﬁnition of the\nGamma function Γ(a),\nΓ(a) =\n∫ ∞\n0\nua−1e−udu. (6)\nNow we perform the substitution u(T) = t/T inside the integral, assuming that t≥0 and T >0.\nThis substitution makes du = −t\nT2 dT, and also changes the limits of integration from (0,∞) to\n(u(0),u(∞)) = (∞,0), giving\nΓ(a) =\n∫ 0\n∞\n(t\nT\n)a−1\ne−t\nT\n(−t\nT2 dT\n)\n(7)\n= −\n∫ 0\n∞\n(\nta−1)\n(t)\n(\nT−a+1)(\nT−2)\ne−t\nT dT (8)\n= ta\n∫ ∞\n0\nT−a−1e−t\nT dT. (9)\nNow we set a= dand isolate the polynomial in t, giving\nt−d = Γ( d)−1\n∫ ∞\n0\nT−d−1e−t\nT dT. (10)\nThis form closely resembles Equation 4, suggesting that P(T) = Γ( d)−1T−d−1. However, note\nthat we did not rigorously deﬁne the domain of tin the original formulation. In reality, we are only\nconcerned with t≥1, since 0 ≤t< 1 corresponds to distances of less than 1 word, andt−d explodes\nfor values of tnear zero. We thus neither want nor need to approximate t−d for 0 ≤t< 1.\nWe can adjust Equation 10, which is deﬁned for t≥0, to reﬂect this limitation by simply substituting\nt→s+ 1, giving\n(s+ 1)−d = Γ( d)−1\n∫ ∞\n0\nT−d−1e−s+1\nT dT (11)\n= Γ( d)−1\n∫ ∞\n0\nT−d−1e−1\nT e−s\nT dT (12)\n=\n∫ ∞\n0\nP(T)e−s\nT dT, where (13)\nP(T) = T−d−1\nΓ(d) e−1\nT = InverseGamma(T; α= d,β = 1). (14)\nA.2 R ELATIONSHIP BETWEEN FORGET GATE AND TIMESCALE\nIn Section 2.3, we showed that the forget gate bias controls the timescale of the unit, and derived a\ndistribution of assigned timescales for the multi-timescale language model. After training this model,\n12\nPublished as a conference paper at ICLR 2021\nFigure 4: Estimated timescale is highly correlated with assigned timescale, shown for all 1150 units\nin LSTM layer 2 of the multi-timescale language model.\nwe tested whether this control was successful by estimating the empirical timescale of each unit based\non their mean forget gate values using Equation 2. Figure 4 shows that the assigned and estimated\ntimescales in layer 2 are strongly correlated. This demonstrates that the timescale of an LSTM unit\ncan be effectively controlled by the forget gate bias.\nFigure 1 shows estimated timescales in the LSTM language model from Merity et al. (2018) trained\non Penn Treebank (Marcus et al., 1999; Mikolov et al., 2011). These timescales lie between 0 and 150\ntimesteps, with more than 90% of timescales being less than 10 timesteps, indicating that this network\nskews its forget gates to process shorter timescales during training. This resembles the ﬁndings by\nKhandelwal et al. (2018), which showed that the model’s sensitivity is reduced for information farther\nthan 20 timesteps in the past. Ideally, we would like to control the timescale of each unit to counter\nthis training effect and select globally a distribution that follows from natural language data.\nA.3 F ORGET GATE VISUALIZATION\nTo further examine representational timescales, we next visualized forget gate values of units from all\nthree layers of both the multi-timescale and baseline language models as described in Section A.3.\nThe goal is to compare the distribution of these forget gate values across the two language models,\nand to assess how these values change over time for a given input.\nFirst, we sorted the LSTM units of each layer according to their mean forget gate values over a test\nsequence. For visualization purposes, we then downsampled these values by calculating the average\nforget gate value of every 10 consecutive sorted units for each timestep. Heat maps of these sorted\nand down-sampled forget gate values are shown in Figure 5. The horizontal axis shows timesteps\n(words) across a sample test sequence, and the vertical axis shows different units. Units with average\nforget gate values close to 1.0 (bottom) are retaining information across many timesteps, i.e. they\nare capturing long timescale information. Figure 5 shows that the baseline language model contains\nfewer long timescale units than the multi-timescale language model. They are also more evenly\ndistributed across the layers than the multi-timescale language model. Figure 5b also shows the\n(approximate) assigned timescales for units in the multi-timescale language model. As expected,\nlayer 1 contains short timescales and layer 2 contains a range of both short and long timescales. Layer\n1 units with short (assigned) timescales have smaller forget gate values across different timesteps.\nIn layer 2, we observe that units with large assigned timescale have higher mean forget gate values\nacross different timesteps, for example the units with assigned timescale of 362 in 5b have forget\ngate values of almost 1.0 across all timesteps. Similar to the previous analysis, this demonstrates that\nour method is effective at controlling the timescale of each unit, and assigns a different distribution of\ntimescales than the baseline model.\nA.4 W ORD ABLATION\nAnother way to interpret timescale of information retained by the layers is to visualize the decay of\ninformation in the cell state over time. We estimated this decay with word ablation experiments as\ndescribed in Section A.3.\n13\nPublished as a conference paper at ICLR 2021\n(a) Baseline Language Model\n(b) Multi-timescale Language Model\nFigure 5: Forget gate values of LSTM units for a test sentence from the PTB dataset. Units are sorted\ntop to bottom by increasing mean forget gate value, and averaged in groups of 10 units to enable\nvisualization. Figure 5b also shows average assigned timescale (rounded off) of the units.\n14\nPublished as a conference paper at ICLR 2021\n(a) PTB dataset\n(b) WikiText-2 dataset\nFigure 6: Change in cell state of all the three layers for both the baseline and Multi-timescale language\nmodels in word ablation experiment. A curve with a steep slope indicates that cell state difference\ndecays quickly over time, suggesting that the LSTM layer retains information of shorter timescales.\n15\nPublished as a conference paper at ICLR 2021\nIn Figure 6, we show the normalized cell state difference averaged across test sentences for all three\nlayers of both baseline (blue) and multi-timescale (orange) models. In the PTB dataset, information\nin layer 1 of the baseline model decays more slowly than in layer 2. In this case, layer 2 of the\nbaseline model retains shorter timescale information than layer 1. In the WikiText-2 dataset, the\ndifference between layer 1 and layer 2 of the baseline model is inverted, with layer 2 retaining longer\ntimescale information. However, in the multi-timescale model the trend is nearly identical for both\ndatasets, with information in layer 2 decaying more slowly than layer 1. This is expected for our\nmulti-timescale model, which we designed to have short timescale dependencies in layer 1 and longer\ntimescale dependencies in layer 2. Furthermore, the decay curves are very similar across datasets\nfor the multi-timescale model, but not for the baseline model, demonstrating that controlling the\ntimescales gives rise to predictable behavior across layers and datasets. Layer 3 has similar cell state\ndecay rate across both models. In both models, layer 3 is initialized randomly, and we expect its\nbehavior to be largely driven by the language modeling task.\nNext, we explored the rate of cell state decay across different groups of units in layer 2 of the\nmulti-timescale language model. We ﬁrst sorted the layer 2 units according to their assigned timescale\nand then partitioned these units into groups of 100 before estimating the cell state decay curve for\neach group. As can be seen in Figures 7 and 8, units with a shorter average timescale have faster\ndecay rates, whereas units with longer average timescale have slower information decay. While the\nprevious section demonstrated that our manipulation could control the forget gate values, this result\ndemonstrates that we can directly inﬂuence how information is retained in the LSTM cell states.\nFigure 7: Change in cell states of 100-unit groups having different average timescale of layer 2 in\nMulti-timescale model in word ablation experiment for PTB dataset. As the assigned timescale to the\ngroup decreases the slope of the curve decreases indicating retained information of smaller timescale.\nFigure 8: Change in cell states of 100-unit groups having different average timescale of layer 2 in\nMulti-timescale model in word ablation experiment for WikiText-2 dataset.\nA.5 S HAPE PARAMETER FOR INVERSE GAMMA DISTRIBUTION\nWe compared the performance of multi-timescale language model for different shape parameters\nin inverse gamma distribution. Figure 9 shows timescales assigned to LSTM units of layer 2\n16\nPublished as a conference paper at ICLR 2021\ncorresponding to different shape parameters. These shape parameters cover a wide range of possible\ntimescale distribution to the units. Figure 10 shows that multi-timescale models performs best for\nα= 0.56.\nFigure 9: Assigned timescale to LSTM units of layer2 of multi-timescale language model correspond-\ning to different shape parameter α.\nFigure 10: Performance of multi-timescale models for different shape parameters αon both PTB and\nWikiText-2 dataset.\nA.6 S ELECTING THE TIMESCALES FOR EACH LAYER\nWith the purpose to select proper timescales to each layer in Section 3.1, we conducted experiments\non designing LSTM language models with different combinations of timescales across the three\nlayers. We found that layer 1 (the closest layer to input) always prefers smaller timescales within the\nrange from 1 to 5. This is consistent with what has been observed in literature: the ﬁrst layer focuses\nmore on syntactic information present in short timescales Peters et al. (2018); Jain & Huth (2018).\nWe also observed that the layer 3, i.e., the layer closest to the output, does not get affected by the\nassigned timescale. Since we have tied encoder-decoder settings while training, layer 3 seems to\nlearn global word representation with a speciﬁc timescale of information control by the training task\n(language modeling). The middle LSTM layer (layer 2) was more ﬂexible, which allowed us to select\nspeciﬁc distributions of timescales. Therefore, we achieve the Multi-timescale Language Model\nin Section 3.1 by setting layer 1 biases to small timescales, layer 2 to satisfy the inverse gamma\ndistribution and thus aim to achieve the power-law decay of the mutual information, and layer 3 with\nfreedom to learn the timescales required for the current task.\nA.7 I NFORMATION ROUTING EXPERIMENTS ON WIKI TEXT-2 DATASET\nWe also performed the information routing experiments for multi-timescale model trained on\nWikiText-2 dataset. Figure 11 shows timescale-dependent routing in the model, same as what\nwe observed for PTB dataset in Section 3.2.4.\n17\nPublished as a conference paper at ICLR 2021\nFigure 11: Information routing across different units of the multi-timescale LSTM for WikiText-2\ndataset.\nTable 2: Perplexity of the baseline and multi-timescale models over 5 different training instances.\nValues are the mean and standard error over the training instances.\nDataset Model Performance\nPTB Baseline 61.64 ±0.28\nMulti-timescale 59.63 ±0.18\nWikiText-2 Baseline 70.23 ±0.24\nMulti-timescale 68.33 ±0.12\nA.8 R OBUSTNESS OF MODEL PERFORMANCE\nWe quantiﬁed the variability in model performance due to stochastic differences in training with\ndifferent random seeds. Table 2 shows the mean perplexity and standard error across 5 different\ntraining instances. The variance due to training is similar across the two models.\nA.9 M ODEL COMPARISON REPRODUCING A PREVIOUS REPORT\nPreviously, the LSTM baseline we used in this work was reported to achieve a perplexity of 58.8\nwithout additional ﬁne-tuning (Merity et al., 2018). To more closely reproduce this value, we retrained\nthe models using the legacy version of pytorch (0.4.1) used to train the LSTM in the original paper.\nWe report both the overall perplexity and the performance in each word frequency bin (Table 3).\nWhile the overall perplexity still falls short of the original value, we speculate this could be due to the\nCUDA version, which was not reported. Still, we ﬁnd that the multi-timescale model signiﬁcantly\nimproves performance for the the most infrequent words, mirroring the results reported in the main\ntext.\nWe also performed the information routing experiment on the multi-timescale model trained with the\nolder version of pytorch (Figure 12). Our results are very similar to those reported in the main text in\nSection 3.2.4, which demonstrate that timescale-dependent information is routed through different\nunits.\n18\nPublished as a conference paper at ICLR 2021\nTable 3: Perplexity of the baseline and multi-timescale models trained with a legacy version of\npytorch. Performance is also reported separately for tokens across different frequency bins. Last row\nis the mean difference in perplexity (baseline −multi-timescale) across 10,000 bootstrapped samples,\nalong with the 95% conﬁdence interval (CI).\nDataset: Penn TreeBank\nModel above 10K 1K-10K 100-1K below 100 All tokens\nBaseline 6.59 27.04 178.62 2069.81 58.98\nMulti-timescale 6.79 26.68 167.97 1902.37 57.58\nMean diff. -0.2 0.37 10.65 167.27 1.4\n95% CI [-0.27,-0.14] [-0.02,0.73] [7.77,13.78] [120.22,215.97] [0.91,1.86]\nFigure 12: Information routing across different units of the multi-timescale LSTM trained with a\nlegacy version of pytorch on the Penn Treebank dataset.\nA.10 T HE DYCK -2 GRAMMAR\nThe Dyck-2 grammar (Suzgun et al., 2019) is given by a probabilistic context-free grammar (PCFG)\nas follows:\nS →\n\n\n\n(S) with probability p1\n[S] with probability p2\nSS with probability q\nε with probability 1 −(p1 + p2 + q)\nwhere 0 <p1,p2,q <1 and p1 + p2 + q <1.\n19",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.8869836330413818
    },
    {
      "name": "Interpretability",
      "score": 0.8392242789268494
    },
    {
      "name": "Computer science",
      "score": 0.7323225140571594
    },
    {
      "name": "Language model",
      "score": 0.6702955961227417
    },
    {
      "name": "Inverse-gamma distribution",
      "score": 0.6580368876457214
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5878465175628662
    },
    {
      "name": "Natural language",
      "score": 0.5630703568458557
    },
    {
      "name": "Natural language processing",
      "score": 0.5456110835075378
    },
    {
      "name": "Representation (politics)",
      "score": 0.5012271404266357
    },
    {
      "name": "Artificial neural network",
      "score": 0.4238443374633789
    },
    {
      "name": "Probability distribution",
      "score": 0.24562478065490723
    },
    {
      "name": "Mathematics",
      "score": 0.12035855650901794
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Distribution fitting",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Inverse-chi-squared distribution",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I86519309",
      "name": "The University of Texas at Austin",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1343180700",
      "name": "Intel (United States)",
      "country": "US"
    }
  ]
}