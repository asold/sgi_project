{
  "title": "Language Models Explain Word Reading Times Better Than Empirical Predictability",
  "url": "https://openalex.org/W4225647542",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2166811011",
      "name": "Markus J. Hofmann",
      "affiliations": [
        "University of Wuppertal"
      ]
    },
    {
      "id": "https://openalex.org/A2292917444",
      "name": "Steffen Remus",
      "affiliations": [
        "Universität Hamburg"
      ]
    },
    {
      "id": "https://openalex.org/A1441874829",
      "name": "Chris Biemann",
      "affiliations": [
        "Universität Hamburg"
      ]
    },
    {
      "id": "https://openalex.org/A14669676",
      "name": "Ralph Radach",
      "affiliations": [
        "University of Wuppertal"
      ]
    },
    {
      "id": "https://openalex.org/A113798925",
      "name": "Lars Kuchinke",
      "affiliations": [
        "International Psychoanalytic University Berlin"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2161475963",
    "https://openalex.org/W2154461458",
    "https://openalex.org/W1967163642",
    "https://openalex.org/W2125001590",
    "https://openalex.org/W6691678659",
    "https://openalex.org/W1489614140",
    "https://openalex.org/W6691599571",
    "https://openalex.org/W2106850936",
    "https://openalex.org/W3087624537",
    "https://openalex.org/W2136525955",
    "https://openalex.org/W2158195707",
    "https://openalex.org/W2147152072",
    "https://openalex.org/W2139450036",
    "https://openalex.org/W2110485445",
    "https://openalex.org/W2070586582",
    "https://openalex.org/W1999364590",
    "https://openalex.org/W2063104203",
    "https://openalex.org/W2068558378",
    "https://openalex.org/W6629258772",
    "https://openalex.org/W2118276816",
    "https://openalex.org/W2153705299",
    "https://openalex.org/W2082283091",
    "https://openalex.org/W6691371945",
    "https://openalex.org/W1984251878",
    "https://openalex.org/W1991766386",
    "https://openalex.org/W1983870751",
    "https://openalex.org/W2623565235",
    "https://openalex.org/W2885315449",
    "https://openalex.org/W2101073968",
    "https://openalex.org/W6784197384",
    "https://openalex.org/W2492610080",
    "https://openalex.org/W6606598612",
    "https://openalex.org/W2057458773",
    "https://openalex.org/W1573659391",
    "https://openalex.org/W2067359214",
    "https://openalex.org/W2030330674",
    "https://openalex.org/W6640281019",
    "https://openalex.org/W2141138276",
    "https://openalex.org/W1983578042",
    "https://openalex.org/W2919115771",
    "https://openalex.org/W3123713096",
    "https://openalex.org/W2470606929",
    "https://openalex.org/W2415973339",
    "https://openalex.org/W1975849605",
    "https://openalex.org/W2138640694",
    "https://openalex.org/W1996005069",
    "https://openalex.org/W6747248625",
    "https://openalex.org/W2152885317",
    "https://openalex.org/W2165410821",
    "https://openalex.org/W2428904845",
    "https://openalex.org/W2092427462",
    "https://openalex.org/W2005181355",
    "https://openalex.org/W2137738843",
    "https://openalex.org/W6678961370",
    "https://openalex.org/W2417742387",
    "https://openalex.org/W2019159956",
    "https://openalex.org/W2080248237",
    "https://openalex.org/W2017952998",
    "https://openalex.org/W2013112874",
    "https://openalex.org/W2154796476",
    "https://openalex.org/W2149480836",
    "https://openalex.org/W2081709670",
    "https://openalex.org/W2030831236",
    "https://openalex.org/W2101597441",
    "https://openalex.org/W2015121227",
    "https://openalex.org/W2108010971",
    "https://openalex.org/W2887750129",
    "https://openalex.org/W2126929879",
    "https://openalex.org/W1682431627",
    "https://openalex.org/W1979900603",
    "https://openalex.org/W2270070752",
    "https://openalex.org/W1965914122",
    "https://openalex.org/W2169244873",
    "https://openalex.org/W2155466302",
    "https://openalex.org/W2567682148",
    "https://openalex.org/W3112717819",
    "https://openalex.org/W3193071044",
    "https://openalex.org/W1574901103",
    "https://openalex.org/W2398936787",
    "https://openalex.org/W2070845553",
    "https://openalex.org/W2128808215",
    "https://openalex.org/W4391156274",
    "https://openalex.org/W2606747632"
  ],
  "abstract": "Though there is a strong consensus that word length and frequency are the most important single-word features determining visual-orthographic access to the mental lexicon, there is less agreement as how to best capture syntactic and semantic factors. The traditional approach in cognitive reading research assumes that word predictability from sentence context is best captured by cloze completion probability (CCP) derived from human performance data. We review recent research suggesting that probabilistic language models provide deeper explanations for syntactic and semantic effects than CCP. Then we compare CCP with three probabilistic language models for predicting word viewing times in an English and a German eye tracking sample: (1) Symbolic n-gram models consolidate syntactic and semantic short-range relations by computing the probability of a word to occur, given two preceding words. (2) Topic models rely on subsymbolic representations to capture long-range semantic similarity by word co-occurrence counts in documents. (3) In recurrent neural networks (RNNs), the subsymbolic units are trained to predict the next word, given all preceding words in the sentences. To examine lexical retrieval, these models were used to predict single fixation durations and gaze durations to capture rapidly successful and standard lexical access, and total viewing time to capture late semantic integration. The linear item-level analyses showed greater correlations of all language models with all eye-movement measures than CCP. Then we examined non-linear relations between the different types of predictability and the reading times using generalized additive models. N-gram and RNN probabilities of the present word more consistently predicted reading performance compared with topic models or CCP. For the effects of last-word probability on current-word viewing times, we obtained the best results with n-gram models. Such count-based models seem to best capture short-range access that is still underway when the eyes move on to the subsequent word. The prediction-trained RNN models, in contrast, better predicted early preprocessing of the next word. In sum, our results demonstrate that the different language models account for differential cognitive processes during reading. We discuss these algorithmically concrete blueprints of lexical consolidation as theoretically deep explanations for human reading.",
  "full_text": "ORIGINAL RESEARCH\npublished: 02 February 2022\ndoi: 10.3389/frai.2021.730570\nFrontiers in Artiﬁcial Intelligence | www.frontiersin.org 1 February 2022 | Volume 4 | Article 730570\nEdited by:\nMassimo Stella,\nUniversity of Exeter, United Kingdom\nReviewed by:\nAaron Veldre,\nThe University of Sydney, Australia\nMarco Silvio Giuseppe Senaldi,\nMcGill University, Canada\nDavid Balota,\nWashington University in St. Louis,\nUnited States, in collaboration with\nreviewer AK\nAbhilasha Kumar,\nIndiana University, United States, in\ncollaboration with reviewer DB\n*Correspondence:\nMarkus J. Hofmann\nmhofmann@uni-wuppertal.de\nSpecialty section:\nThis article was submitted to\nLanguage and Computation,\na section of the journal\nFrontiers in Artiﬁcial Intelligence\nReceived: 25 June 2021\nAccepted: 28 December 2021\nPublished: 02 February 2022\nCitation:\nHofmann MJ, Remus S, Biemann C,\nRadach R and Kuchinke L (2022)\nLanguage Models Explain Word\nReading Times Better Than Empirical\nPredictability.\nFront. Artif. Intell. 4:730570.\ndoi: 10.3389/frai.2021.730570\nLanguage Models Explain Word\nReading Times Better Than Empirical\nPredictability\nMarkus J. Hofmann1*, Steffen Remus2, Chris Biemann2, Ralph Radach1 and\nLars Kuchinke3\n1 Department of Psychology, University of Wuppertal, Wupper tal, Germany, 2 Department of Informatics, Universität\nHamburg, Hamburg, Germany, 3 International Psychoanalytic University, Berlin, Germany\nThough there is a strong consensus that word length and frequ ency are the most\nimportant single-word features determining visual-ortho graphic access to the mental\nlexicon, there is less agreement as how to best capture synta ctic and semantic factors.\nThe traditional approach in cognitive reading research ass umes that word predictability\nfrom sentence context is best captured by cloze completion p robability (CCP) derived\nfrom human performance data. We review recent research sugg esting that probabilistic\nlanguage models provide deeper explanations for syntactic and semantic effects than\nCCP . Then we compare CCP with three probabilistic language m odels for predicting\nword viewing times in an English and a German eye tracking sam ple: (1) Symbolic\nn-gram models consolidate syntactic and semantic short-ra nge relations by computing\nthe probability of a word to occur, given two preceding words . (2) Topic models rely\non subsymbolic representations to capture long-range sema ntic similarity by word\nco-occurrence counts in documents. (3) In recurrent neural networks (RNNs), the\nsubsymbolic units are trained to predict the next word, give n all preceding words in the\nsentences. To examine lexical retrieval, these models were used to predict single ﬁxation\ndurations and gaze durations to capture rapidly successful and standard lexical access,\nand total viewing time to capture late semantic integration . The linear item-level analyses\nshowed greater correlations of all language models with all eye-movement measures than\nCCP . Then we examined non-linear relations between the diff erent types of predictability\nand the reading times using generalized additive models. N- gram and RNN probabilities\nof the present word more consistently predicted reading per formance compared with\ntopic models or CCP . For the effects of last-word probabilit y on current-word viewing\ntimes, we obtained the best results with n-gram models. Such count-based models\nseem to best capture short-range access that is still underw ay when the eyes move on to\nthe subsequent word. The prediction-trained RNN models, in contrast, better predicted\nearly preprocessing of the next word. In sum, our results dem onstrate that the different\nlanguage models account for differential cognitive proces ses during reading. We discuss\nthese algorithmically concrete blueprints of lexical cons olidation as theoretically deep\nexplanations for human reading.\nKeywords: language models, n-gram model, topic model, recurre nt neural network model, predictability,\ngeneralized additive models, eye movements\nHofmann et al. Language Models Explain Viewing Times\nINTRODUCTION\nConcerning the inﬂuence of single-word properties, there is a\nstrong consensus in the word recognition literature that wo rd\nlength and frequency are the most reliable predictors of lexi cal\naccess (e.g.,\nReichle et al., 2003; New et al., 2006; Adelman and\nBrown, 2008; Brysbaert et al., 2011 ). Though for instance, Baayen\n(2010) suggests that a large part of the variance explained by\nword frequency is better explained by contextual word featur es,\nwe here use these single-word properties as a baseline to set\nthe challenge for contextual word properties to explain more\nvariance than the single-word properties.\nIn contrast to single-word frequency, the question of how\nto best capture contextual word properties is controversial.\nThe traditional psychological predictor variables are based on\nhuman performance. When aiming to quantify how syntactic\nand semantic contextual word features inﬂuence the reading of\nthe present word,\nTaylor’s (1953) cloze completion probability\n(CCP) still represents the performance-based state of the art\nfor predicting sentence reading in psychological research (\nKutas\nand Federmeier, 2011; Staub, 2015 ). Participants of a pre-\nexperimental study are given a sentence with a missing word,\nand the relative number of participants completing the respectiv e\nword are then taken to deﬁne CCP. This human performance\nis then used to account for another human performance such\nas reading.\nWestbury (2016) , however, suggests that a to-be-\nexplained variable, the explanandum, should be selected from\na diﬀerent domain than the explaining variable, the explanans\n(\nHempel and Oppenheim, 1948 ). When two directly observable\nvariables, such as CCP and reading times, are connected, for\ninstance\nFeigl (1945, p. 285 ) suggests that this corresponds to\na ‘“low-grade’ explanation.” Models of eye movement control,\nhowever, were “not intended to be a deep explanation of langua ge\nprocessing, [. . . because they do] not account for the many eﬀec ts\nof higher-level linguistic processing on eye movements” (\nReichle\net al., 2003 , p. 450).\nLanguage models oﬀer a deeper level of explanation, because\nthey computationally specify how the prediction is generated.\nTherefore, they incorporate what can be called the three mnes tic\nstages of the mental lexicon (cf.\nPaller and Wagner, 2002;\nHofmann et al., 2018 ). All memory starts with experience, which\nis reﬂected by a text corpus (cf. Hofmann et al., 2020 ). The\nlanguage models provide an algorithmic description of how lon g-\nterm lexical knowledge is consolidated from this experience\n(\nLandauer and Dumais, 1997; Hofmann et al., 2018 ). Based\non the consolidated syntactic and semantic lexical knowled ge,\nlanguage models are then exposed to the same materials that\nparticipants read and thus predict lexical retrieval. In the pre sent\nstudy, we evaluate their predictions for viewing times durin g\nsentence reading (e.g.,\nStaub, 2015).\nWe will compare CCP as a human-performance based\nexplanation of reading against three types of language models.\nThe probability that a word occurs, given two preceding words,\nis reﬂected in n-gram models, which capture syntactic and\nshort-range semantic knowledge (cf. e.g.,\nKneser and Ney, 1995;\nMcDonald and Shillcock, 2003a ). This is a fully symbolic model,\nbecause the smallest unit of meaning representation consist\nof words. Second, we test topic models that are trained from\nword co-occurrence in documents, thus reﬂecting long-rang e\nsemantics (\nLandauer and Dumais, 1997; Blei et al., 2003; Griﬃths\net al., 2007; Pynte et al., 2008a ). Finally, recurrent neural networks\n(RNNs) most closely reﬂect the cloze completion procedure,\nbecause their hidden units are trained to predict a target wor d\nby all preceding words in a sentence (\nElman, 1990; Frank, 2009;\nMikolov, 2012 ). In contrast to the n-gram model, topic and\nRNN models distribute the meaning of a word across several\nsubsymbolic units that do not represent human-understandab le\nmeaning by themselves.\nEye Movements and Cloze Completion\nProbabilities\nWhile the eyes sweep over a sequence of words during reading,\nthey remain relatively still for some time, which is general ly\ncalled ﬁxation duration (e.g.,\nInhoﬀ and Radach, 1998; Rayner,\n1998). A very fast and eﬃcient word recognition is obtained\nwhen a word can be recognized at a single glance. In this type of\nﬁxation event, the single-ﬁxation duration (SFD) informs ab out\nthis rapid and successful lexical access. When further ﬁxati ons\nare required to recognize the word before the eyes move on to\nthe next word, the duration of these further ﬁxations is adde d to\nthe gaze duration (GD). This is an eye movement measure that\nreﬂects “standard” lexical access in all words, while it may a lso\nrepresent syntactic and semantic integration (cf. e.g.,\nInhoﬀ and\nRadach, 1998; Rayner, 1998; Radach and Kennedy, 2013 ). Finally,\nthe eyes may come back to the respective word, and when the\nﬁxation times of these further ﬁxations are added, this is re ﬂected\nin the total viewing time (TVT)—an eye movement measure that\nreﬂects the full semantic integration of a word into the curr ent\nlanguage context (e.g.,\nRadach and Kennedy, 2013 ).\nThough CCP is known to aﬀect all three types of ﬁxation\ntimes, the result patterns considerably vary between studies\n(see e.g.,\nFrisson et al., 2005; Staub, 2015; Brothers and\nKuperberg, 2021 , for reviews). A potential explanation is that\nCCP represents an all-in variable ( Staub, 2015). The cloze can be\ncompleted because the word is expected from syntactic, semanti c\nand/or event-based information—a term that refers to idiom atic\nexpressions in very frequently co-occurring words (cf. Staub\net al., 2015 ).\nBy shedding light on the consolidation mechanisms, language\nmodels are expected to complement future models of eye-\nmovement control, which do not provide a deep explanation\nto linguistic processes (\nReichle et al., 2003 , p. 450). Models of\neye-movement control, however, provide valuable insights h ow\nlexical access and eye-movements interact. These models as sume\nthat lexical access is primarily driven by word length, frequ ency\nand CCP-based predictability of the presently ﬁxated word (e. g.,\nReichle et al., 2003; Engbert et al., 2005; Snell et al., 2018 ). This\nreﬂects the simplifying eye-mind assumption, which “posits tha t\nthe interpretation of a word occurs while that word is being\nﬁxated, and that the eye remains ﬁxated on that word until\nthe processing has been completed” (\nJust and Carpenter, 1984 ,\np. 169). Current models of eye-movement control, however,\nreject the idea that lexical processing exclusively occurs du ring\nFrontiers in Artiﬁcial Intelligence | www.frontiersin.org 2 February 2022 | Volume 4 | Article 730570\nHofmann et al. Language Models Explain Viewing Times\nthe ﬁxation of a word ( Reichle et al., 2003; Engbert et al.,\n2005; see also Anderson et al., 2004; Kliegl et al., 2006 ). Lexical\nprocessing can still be underway when the eyes move on to\nthe subsequent word, which can occur, for instance, if the ﬁr st\nword is particularly diﬃcult to process (\nReilly and Radach, 2006 ).\nTherefore, lexical processing of the last word can still have a\nconsiderable impact on the viewing times of the currently ﬁxa ted\nword. Moreover, when a word is currently ﬁxated at a point in\ntime, lexical access of the next word can already start (\nReilly and\nRadach, 2006).\nWhen trying to characterize the time course of single-word\nand contextual word properties, for instance the EZ-reader mo del\nsuggests that there are two stages of lexical processing that\nare both inﬂuenced by word frequency and predictability. The\nﬁrst stage represents a “familiarity” check and the identiﬁc ation\nof an orthographic word form—this stage is primarily driven\nby word frequency. The second stage additionally involves th e\n(phonological and) semantic word form—therefore, CCP has a\nstronger impact on this stage of processing. Please also note t hat\nattention can already shift to the next word, while the presen t\nword is ﬁxated. When the next word is short, highly frequent\nand/or highly predictable, it can be skipped, and the saccade is\nprogrammed toward the word after the next word (\nReichle et al.,\n2003).\nLanguage Models in Eye Movement\nResearch\nSymbolic Representations in N-Gram Models\nSymbolic n-gram models are so-called count-based models\n(\nBaroni et al., 2014; Mandera et al., 2017 ). Cases in which\nall n words co-occur are counted and related to the count\nof the preceding n-1 words in a text corpus.\nMcDonald and\nShillcock (2003a) were the ﬁrst who tested whether a simple 2-\ngram model can predict eye movement data. They calculated\nthe transitional probability that a word occurs at position n\ngiven the preceding word at position n-1. Then they paired\npreceding verbs with likely and less likely target nouns and\nshowed signiﬁcant eﬀects on early SFD, but no eﬀects on later GD\n(but see\nFrisson et al., 2005 ). Eﬀects on GD were subsequently\nrevealed using multiple regression analyses of eye movement s,\nsuggesting that 2-gram models also account for lexical acce ss in\nall words ( McDonald and Shillcock, 2003b ; see also Demberg and\nKeller, 2008 ). McDonald and Shillcock (2003b) discussed that\nthe 2-gram transitional probability reﬂects a relatively lo w-level\nprocess, while it does probably not capture high-level conceptu al\nknowledge, corroborating the assumption that n-gram models\nreﬂect syntactic and short-range semantic information.\nBoston\net al. (2008) analyzed the viewing times in the Potsdam Sentence\nCorpus (PSC, Kliegl et al., 2004 ) and found eﬀects of transitional\nprobability for all three ﬁxation measures (SFD, GD, and TVT) .\nMoreover, they found that these eﬀects were descriptively larg er\nthan the CCP eﬀects (see also Hofmann et al., 2017 ).\nSmith and Levy (2013, p. 303) examined larger sequences of\nwords by using a 3-gram model to show last- and present-word\nprobability eﬀects on GD during discourse reading (\nKneser and\nNey, 1995). Moreover, they showed that these n-gram probability\neﬀects are logarithmic (but cf. Brothers and Kuperberg, 2021 ).\nFor their statistical analyses, Smith and Levy (2013) selected\ngeneralized additive models (GAMs) that can well capture the\nphenomenon that a predictor may perform better or worse in\ncertain range of the predictor variable. They showed that the 3-\ngram probability of the last word still has a considerable impact\non the GDs of the current word. Therefore, this type of languag e\nmodel can well predict that contextual integration of the las t word\nis still underway at the ﬁxation of the current word. Of some\ninterest is that\nSmith and Levy (2013) suggest that CCP may\npredict reading performance well, when comparing extremely\npredictable with extremely unpredictable words.\nHofmann et al.\n(2017, e.g., Figure 3), however, provide data showing that a 3-\ngram model may provide more accurate predictions at the lower\nend of the predictability distribution.\nLatent Semantic Dimensions\nThe best-known computational approach to semantics in\npsychology is probably latent semantic analysis (LSA,\nLandauer\nand Dumais, 1997 ). A factor-analytic-inspired approach is used\nto compute latent semantic dimensions that determine which\nwords do frequently occur together in documents. This allow s\nto address the long-range similarity of words and sentences by\ncalculating the cosine distance (\nDeerwester et al., 1990 ). Wang\net al. (2010) addressed the inﬂuence of transitional probability\nand LSA similarity of the target to the preceding content word .\nThey found that transitional probability predicts lexical ac cess,\nwhile the long-range semantics reﬂected by LSA particularly\npredicts late semantic integration [but see\nPynte et al. (2008a,b)\nfor LSA eﬀects on SFD and GD, and Luke and Christianson\n(2016) for LSA eﬀects on TVT, GD, and even earlier viewing\ntime measures].\nIn a recent study, Bianchi et al. (2020) contrasted the GD\npredictions of an n-gram model with the predictions of an\nLSA-based match of the current word with the preceding nine\nwords during discourse reading. They found that LSA did not\nprovide eﬀects over and above the n-gram model. The LSA-\nbased predictions improved, however, when further adding the\nLSA-based contextual match of the next word. This indicates\nthat such a document-level, long-range type of semantics mig ht\nbe particularly eﬀective when taking the predictabilities of t he\nnon-ﬁxated words into account.\nLSA has been challenged by another dimension-reducing\napproach in accounting for eye movement data.\nBlei et al. (2003)\nintroduced the topic model as a Bayesian, mere probabilistic\nlanguage modeling alternative. Much as LSA, topic models are\ntrained to reﬂect long-range relations based on the co-occu rrence\nof words in documents.\nGriﬃths et al. (2007) showed that topic\nmodels provide better model performance than LSA in many\npsychological tasks, such as synonym judgment or semantic\npriming. They calculated the probability of a word to occur, gi ven\ntopical matches with the preceding words in the sentence. This\ntopic model-based predictor, but not LSA cosine accounted for\nSereno’s et al. (1992)ﬁnding that GDs and TVTs of a subordinate\nmeaning are larger than in a frequency-matched non-ambiguo us\nword (Sereno et al., 1992 ).\nFrontiers in Artiﬁcial Intelligence | www.frontiersin.org 3 February 2022 | Volume 4 | Article 730570\nHofmann et al. Language Models Explain Viewing Times\nThough Hofmann et al. (2017) also found topic model\neﬀects on SFD data, their results suggested that long-range\nsemantics provides comparably poor predictions. The short-\nrange semantics and syntax provided by n-gram models, in\ncontrast, provided a much better performance, particularly wh en\nthe language models are trained by a corpus consisting of movi e\nand ﬁlm subtitles. In sum, the literature on document-level\nsemantics presently provides no consistent picture. Long-ran ge\nsemantic eﬀects might be comparably small (e.g.,\nHofmann et al.,\n2017), but they may be more likely to deliver consistent results\nwhen the analysis is not constrained to the long-range conte xtual\nmatch of the present, but also of other words ( Bianchi et al.,\n2020). A more consistent picture might emerge, when also short-\nrange predictability is considered, as reﬂected e.g., in n-g ram\nmodels (Wang et al., 2010; Bianchi et al., 2020 ).\n(Recurrent) Neural Networks\nNeural network models are deeply rooted in the tradition\nof connectionist modeling (e.g.,\nSeidenberg and McClelland,\n1989; McClelland and Rogers, 2003 ). In the last decade, these\nmodels were advanced in the machine learning community\nto successfully recognize pictures or machine translation ( e.g.,\nLeCun et al., 2015 ). In the processing of word stimuli, one of\nthe most well-known of these models is the word2vec model,\nin which a set of hidden units is for instance trained to predic t\nthe surrounding words by the present word (\nMikolov et al.,\n2013). This model is able to predict association ratings ( Hofmann\net al., 2018 ) or semantic priming (e.g., Mandera et al., 2017 ). The\nneural network that most closely approximates the cloze task,\nhowever, is the recurrent neural network model (RNN), becaus e\nit is trained to predict the next word by the preceding sentence\ncontext. In RNN models, words are presented at an input layer,\nand a set of hidden units is trained to predict the probability o f the\nnext word at the output layer (\nElman, 1990). The hidden layer is\ncopied to a (recurrent) context layer after the presentation o f each\nword. Thus, the network gains a computationally concrete form\nof short-term memory (\nMikolov et al., 2013 ). Such a network\nprovides large hidden-unit cosine distances between syntac tic\nclasses such as verbs and nouns, lower between non-living and\nliving objects, and even lower between mammals and ﬁshes,\nsuggesting that RNNs reﬂect syntactic and short-range seman tic\ninformation at the level of the sentence (\nElman, 1990 ). Frank\nand Bod (2011) show that RNNs can account for syntactic\neﬀects in viewing times, because they absorb variance previou sly\nexplainable by a hierarchical phrase-structure approach.\nFrank (2009) used a simple RNN to successfully predict GDs\nduring discourse reading. When adding transitional probabi lity\nto their multiple regression analyses, both predictors revea led\nsigniﬁcant eﬀects. Such a result demonstrates that predictio n-\nbased models such as RNNs and count-based n-gram models\nprobably reﬂect diﬀerent types of “predictability.”\nHofmann et al.\n(2017) showed that an n-gram model, a topic model, and an\nRNN model together can signiﬁcantly outperform CCP for the\nprediction of SFD. It is, however, unclear whether this ﬁndin g\ncan be replicated in a diﬀerent data set and generalized to other\nviewing time measures.\nSome recent studies compared other types of neural network\nmodels to CCP (\nBianchi et al., 2020; Wilcox et al., 2020;\nLopukhina et al., 2021 ). For example, Bianchi et al. (2020)\nexplored the usefulness of word2vec. Because they did not ﬁnd\nstable word2vec predictions for eye movement data, they deci ded\nagainst a closer examination of this approach. Rather they rel ied\non fasttext—another non-recurrent neural model, in which t he\nhidden units are trained to predict the present word by the\nsurrounding language context (\nMikolov et al., 2018 ). Moreover,\nBianchi et al. (2020) evaluated the performance of an n-gram\nmodel and LSA. When comparing the performance of these\nlanguage models, they obtained the most reliable GD predictio ns\nfor their n-gram model, followed by CCP , while LSA and fasttext\nprovided relatively poor predictions. In sum, studies comparing\nCCP to language models support the view that CCP-based and\nlanguage-model-based predictors account for diﬀerent thoug h\npartially overlapping variances in eye-movement data (\nBianchi\net al., 2020; Lopukhina et al., 2021 ) that seem related to syntactic,\nas well as early and late semantic processing during reading.\nThe Present Study\nThe present study was designed to overcome the limitations\nof the pilot study of\nHofmann et al. (2017) , which compared\nan n-gram, a topics and an RNN model with respect to the\nprediction of CCP , electrophysiological and SFD data in only\nthe PSC data set. They found that RNN models and n-gram\nmodels provide a similar performance in predicting these data,\nwhile the topics model made remarkably worse predictions. In\nthe present study, we focused on eye movements and aimed\nto replicate the SFD eﬀects with a second sample, which was\npublished by\nSchilling et al. (1998) . Moreover, we aimed to\nexamine the dynamics of lexical processing. By modeling a set\nof three viewing time parameters (SFD, GD and TVT), we\nwill be able to compare the predictions of CCP and diﬀerent\nlanguage models for early rapid (SFD) and standard (GD) lexical\naccess, and their predictions for full semantic integration (TVT).\nIn their linear multiple regression analysis on item-level d ata,\nHofmann et al. (2017) found that the three language models\ntogether account for around 30% of reproducible variance in\nSFD data—as opposed to 18% for the CCP model. Though the\nthree language models together signiﬁcantly outperformed th e\nCCP-based approach, they used Fisher-Yates signiﬁcance z-to-t-\ntests as a conservative approach, because aggregating over it ems\nresults in a strong loss of variance. Therefore, n-gram and RN N\nmodels alone outperformed CCP always at a descriptive level,\nbut the diﬀerences were not signiﬁcant. Here we applied a model\ncomparison approach to evaluate the model ﬁt in comparison to\na baseline model, using standard log likelihood tests (e.g.,\nBaayen\net al., 2008 ). This approach will also test the assumptions of\ndiﬀerent short- and long-range syntactic and semantic proces ses\nthat we expect to be reﬂected by the parameters of the three\ndiﬀerent language models selected.\nSuch a statistical approach, however, is based on unaggregate d\ndata. As\nLopukhina et al. (2021) pointed out, the predictability\neﬀects in such analyses are relatively small. For instance Kliegl\net al. (2006; cf. Table 5) found that CCP can account for\nup to 0.38% of the variance in viewing times – thus it is\nFrontiers in Artiﬁcial Intelligence | www.frontiersin.org 4 February 2022 | Volume 4 | Article 730570\nHofmann et al. Language Models Explain Viewing Times\nimportant to evaluate the usefulness of language models in hig hly\npowered samples. On the other hand, a smaller sample reﬂects\na more typical experimental situation. The present study was\ndesigned to replicate and extend previous analyses of viewing\ntime parameters using two independent eye-movement data sets ,\na very large sample of CCP and eye movement data, the PSC,\nand a sample that is more typical for eye movement experiments,\nthe SRC.\nIn addition to a simple item-level analysis as a standard\nbenchmark for visual word recognition models (\nSpieler and\nBalota, 1997 ), that were applied more thoroughly in a previous\nset of analyses ( Hofmann et al., 2017 ), we here applied Smith\nand Levy’s (2013)generalized additive modeling approach with a\nlogarithmic link function (but cf. Brothers and Kuperberg, 2021 ).\nThe computed GAMs rely on ﬁxation-event-level viewing time\nparameters as the dependent variables. We used a standard set\nof baseline predictors for reading and lexical access, and th en\nextended this baseline model by CCP- and/or language-model -\nbased predictors for the present, last and next words. To test\nfor reproducibility, our analyses will be based on the two eye -\nmovement data sets that are most frequently used for testing\nmodels of eye-movement control: the EZ-reader model (\nReichle\net al., 2003 ) was tested with the SRC data set; and the SWIFT and\nthe OB-1 reader models were used to predict viewing times in th e\nPSC (Kliegl et al., 2004 ; cf. Engbert et al., 2005; Snell et al., 2018 ).\nGAMs are non-linear extensions of the generalized linear mode ls\nthat allow predictors to be modeled as a sum of smooth function s\nand therefore allow better adaptations to curvilinear and wi ggly\npredictor-criterion relationships ( Wood, 2017).\nDiﬀerent language models are expected to explain diﬀerential\nand independent proportions of variance in the viewing\ntime parameters. While an n-gram model reﬂects short-range\nsemantics, we expect it to be predictor of all viewing time\nmeasures (e.g.,\nBoston et al., 2008 ). A subsymbolic topic model\nthat reﬂects long-range semantics should be preferred over\nthe other language models in predicting GD and TVT and\nsemantic integration into memory (\nSereno et al., 1992; Griﬃths\net al., 2007 ), particularly when other forms of predictability are\nadditionally taken into account ( Wang et al., 2010; Bianchi et al.,\n2020). Previous studies examining RNN models found eﬀects on\nSFD and GD (e.g., Frank, 2009; Hofmann et al., 2017 ). Thus, it\nis an open empirical question whether predict-based models do\nnot only aﬀect lexical access, but also late semantic integra tion.\nAs these models are trained to predict the next word, they may b e\nparticularly useful to examine early lexical preprocessing of the\nnext word.\nMETHOD\nLanguage Model Simulations\nAll language models were trained by corpora derived from\nmovie and ﬁlm subtitles. 1 The English Subtitles training corpus\nconsisted of 110 thousand ﬁlms and movies that were used for\ndocument-level training of the topic models. We used the 128\nmillion utterances as the sentence-level, in order to train the\n1www.opensubtitles.org\nn-gram and RNN models in the English corpus, which in all\nconsisted of 716 million tokens. The German corpus consisted\nof 7 thousand movies, 7 million utterances/sentences comprisi ng\nof 54 million tokens.\nStatistical n-gram models for words are deﬁned by a sequence\nof n words, in which the probability of the nth word depends\non a Markov chain of the previous n-1 words (see, e.g.,\nChen\nand Goodman, 1999; Manning and Schütze, 1999 ). Here we set\nn = 3 and thus computed the conditional probability of a word\nwn, given the two previous words ( wn− 1 . . . w 1; Smith and Levy,\n2013).\np (wn|w1 . . . wn− 1) = p ( w1 . . . wn)\np ( w1 . . . wn− 1) (1)\nWe used Kneser-Ney-smoothed 3-gram models, relying on the\nBerkleyLM implementation (\nPauls and Klein, 2011 ).2 These\nmodels were trained by the subtitles corpora to capture lexical\nmemory consolidation (cf.\nHofmann et al., 2018 ). For modeling\nlexical retrieval, we computed the conditional probabilitie s for\nthe sentences presented in the SRC and the PSC data set (cf.\nbelow). Since n-gram models only rely on the most recent histo ry\nfor predicting the next word, they fail to account for longer- range\nphenomena and semantic coherence (see\nBiemann et al., 2012 ).\nFor training the topic models, we used the procedure\nby Griﬃths and Steyvers (2004), who infer per-topic word\ndistributions and per-document topic distributions through a\nGibbs sampling process. The empirically observable probability\nof a word w to occur in a document d is thus approximated by\nthe sum of the products of the probabilities a word, given the\nrespective topic z, and the topic, given the respective word:\np =\n∑\ni= 1...N\np (w|zi) ∗ p(zi|d) (2)\nTherefore, words frequently co-occurring in the same docum ents\nreceive a high probability in the same topic. We use\nPhan\nand Nguyen’s (2007) Gibbs-LDA implementation 3 for training\na latent dirichlet allocation (LDA) model with N = 200 topics\n(default values for α = 0.25 and β = 0.001; Blei et al., 2003 ).\nThe per-document topic distributions are trained in form of a\ntopic-document matrix [ p(zi|d)], allowing to classify documents\nby topical similarities, and used for inference of new (unseen )\n“documents” at retrieval.\nFor modeling lexical retrieval of the SRC and PSC text\nsamples, we successively iterate over the words of the particul ar\nsentence and create a new LDA document representation d\nfor each word at time i and its entire history of words in the\nsame sentence:\np (wi| d) = p(wi|wi . . . w1) (3)\nIn this case, d refers to the history of the current sentence\nincluding the current word wi, where we are only interested in\nthe probability of wi. We here computed the probabilities of\n2https://code.google.com/p/berkeleylm/\n3http://gibbslda.sourceforge.net/\nFrontiers in Artiﬁcial Intelligence | www.frontiersin.org 5 February 2022 | Volume 4 | Article 730570\nHofmann et al. Language Models Explain Viewing Times\nthe current word wi given its history as a mixture of its topical\ncomponents (cf. Griﬃths et al., 2007 , p. 231f), and thus address\nthe topical matches of the present word with the preceding words\nin the sentence context.\nFor the RNN model simulations, we relied on the faster RNN-\nLM implementation4, which can be trained on huge data sets and\nvery large vocabularies (cf.\nMikolov, 2012 ). The input and target\noutput units consist of so-called one-hot vectors with one en try\nfor each word in the lexicon of this model. If the respective wo rd\nis present, the entry corresponds to 1, while the entries remai n 0\nfor all other words. At the input level, the entire sentence hi story\nis given word-by-word and the models objective is to predict\nthe probability of the next word at the output level. Therefore ,\nthe connection weights of the input and output layer to the\nhidden layer are optimized. At model initialization, all wei ghts\nare assigned randomly. As soon as the ﬁrst word is presented to\nthe input layer, the output probability of the respective word un it\nis compared to the actual word, and the larger the diﬀerence, th e\nlarger will be the connection weight change (i.e., backpropag ation\nby a simple delta rule). When the second word of a sentence\nthen serves as input, the state of the hidden layer after the ﬁr st\nword is copied to a context layer (cf. Figure 2 in\nElman, 1990 ).\nThis (recurrent) context layer is used to inform the current\nprediction. Therefore, the RNN obtains a form of short-term\nmemory (\nMikolov, 2012 ; cf. Mikolov et al., 2013 ). We trained a\nmodel with 400 hidden units and used the hierarchical softma x\nprovided by faster-RNN with a temperature of 0.6, using a\nsigmoid activation function for all layers. For computing le xical\nretrieval, we used the entire history of a sentence up to the c urrent\nword and computed the probability for that particular word.\nCloze Completion and Eye Movement Data\nThe CCP and eye movement data of the SRC and the PSC\nwere retrieved from\nEngelmann et al. (2013) .5 The SRC data set\ncontains incremental cloze task and eye movement data for 48\nsentences and 536 words that were initially published by\nSchilling\net al. (1998) . The PSC data set provides the same data for 144\nsentences and 1,138 words ( Kliegl et al., 2004, 2006 ).\nThe sentence length of the PSC ranges from 5 to 11 words\n(M = 7.9; SD = 1.39) and from 8 to 14 words in the SRC ( M =\n11.17; SD = 1.36). As last-word probability cannot be computed\nfor the ﬁrst word in a sentence, and next-word probability can not\nbe computed for the last word of a sentence, we excluded ﬁxatio n\ndurations on the ﬁrst and the last words of each sentence from\nanalyses. Four words of the PSC (e.g., “Andendörfern, ” villa ges\nof the Andes) did not occur in the training corpus and were\nexcluded from analyses. This resulted in the 440 target word s\nfor the SRC and the 846 target words for the PSC analyses.\nThe respective participant sample sizes and number of sentences\nare summarized in Table 1 (see\nSchilling et al., 1998; Kliegl\net al., 2004, 2006 , for further details). Table 2 shows example\nsentences, in which one type of predictability is higher than\n4https://github.com/yandex/faster-rnnlm\n5https://clarinoai.informatik.uni-leipzig.de/fedora/objects/mrr:\n11022000000001F2FB/datastreams/EngelmannVasishthEngbertKliegl2013_1.\n0/content\nthe other predictability scores. In general, CCP distributes the\nprobability space across a much smaller number of potential\ncompletion candidates. Therefore, the mean probabilities are\ncomparably high (SRC: p = 0.3; PSC: p = 0.2). The mean\nof the computed predictability scores, in contrast, provide 2– 3\nleading zeros. Moreover, the computed predictability scores b y\nfar provide greater probability ranges.\nTo compute SRC-based CCP scores comparable to the PSC\n(\nKliegl et al., 2006 ), we used the empirical cloze completion\nprobabilities ( ccp) and logit-transformed them ( CCP in formula\n4). Because Kliegl’s et al. (2004)sample was based on 83 complete\npredictability protocols, cloze completion probabilities of 0 and\n1 were replaced by 1/(2 ∗83) and 1 − [1/(2∗83)] for the SRC, to\nobtain the same extreme values.\nCCP = 0.5∗ log( ccp\n1 − ccp ) (4)\nSince lexical processing eﬃciency varies with landing positio n of\nthe eye within a word (e.g., O’Regan and Jacobs, 1992; Vitu et al.,\n2001), we computed relative landing positions by dividing the\nlanding letter by the word length. The optimal viewing position\nis usually slightly left to the middle of the word, granting o ptimal\nvisual processing of the word (e.g.,\nNuthmann et al., 2005 ).\nTherefore, we will use the landing position as a covariate to part ial\nout variance explainable by suboptimal landing positions (cf. e .g.,\nVitu et al., 2001; Kliegl et al., 2006; Pynte et al., 2008b ). For all\neye movement measures, we excluded ﬁxation durations below\n70 ms (e.g.,\nRadach et al., 2013 ). The upper cutoﬀ was deﬁned\nby examining the data distributions and excluding the range\nin which only a few trials remained for analyses. We excluded\ndurations 800 ms or greater for SFD (21 ﬁxation durations for\nSRC and 13 for PSC), 1,200 ms for GD (12 for SRC and 0 for\nPSC), and 1,600 ms for TVT analyses (7 for SRC and 0 for PSC).\nThis resulted in the row numbers used for the respective analys es\ngiven in Table 1.\nData Analysis\nFirst, we calculated simple linear item-level correlations between\nthe predictor variables and the mean SFD, GD and TVT data\n(see Table 3). In addition to the logit-transformed CCPs and\nthe log10-transformed language model probabilities (\nKliegl et al.,\n2006; Smith and Levy, 2013 ), we also explored the correlations\nof the non-transformed probability values with SFD, GD and\nTVT data, respectively: In the SRC data set, CCP provided\ncorrelations of − 0.28, − 0.33, and − 0.39; n-gram models of\n− 0.11, − 0.16 and − 0.21; topic models of − 0.35, − 0.47 and\n− 0.52; and RNN models provided correlations of − 0.16, − 0.23,\nand − 0.25, respectively. In the PSC data set, the SFD, GD\nand TVT correlations with CCP were − 0.20, − 0.26, − 0.31;\nthose of n-gram models were − 0.16, − 0.18 and − 0.19; topics\nmodels provided correlations of − 0.19, − 0.18 and − 0.17; and\nRNN models of − 0.19, − 0.21, − 0.22. In sum, the transformed\nprobabilities always provided higher correlations with all ﬁ xation\ndurations than the untransformed probabilities (cf. Table 3).\nTherefore, the present analyses focus on transformed values.\nFor non-linear ﬁxation-event based analyses of the non-\naggregated eye-movement data, we relied on GAMs using thin\nFrontiers in Artiﬁcial Intelligence | www.frontiersin.org 6 February 2022 | Volume 4 | Article 730570\nHofmann et al. Language Models Explain Viewing Times\nTABLE 1 |Overview about cloze completion and eye movement (EM) data u sed for the present study.\nData set Sentences Targets Language Participants Rows of da ta in analysis\nCCP EM SFD GD TVT\nSRC 48 440 English 20 30 6,451 8,671 8,736\nPSC 144 846 German 272 222 100,975 134,835 135,021\nTABLE 2 |Example sentences and the probabilities of the four types of predictability.\nSRC PSC\nWord CCP N-gram Topic RNN Word CCP N-gram Topic RNN\nBill 6e-3 1e-4 1e-3 2e-5 In 1e-2 2e-3 3e-2 4e-3\ncomplained 6e-3 3e-6 1e-4 1e-6 der 7e-1 1e-1 1e-2 1e-1\nthat 3e-1 1e-1 2e-3 6e-2 Klosterschule 6e-3 2e-6 4e-5 4e-5\nthe 2e-1 1e-1 2e-3 1e-2 herrschen 2e-2 1e-6 8e-4 3e-4\nmagazine 6e-3 1e-4 3e-4 3e-5 Schwester 6e-3 5e-5 4e-2 1e-9\nincluded 6e-3 3e-7 2e-4 1e-5 Agathe 1e-2 7e-7 1e-4 1e-8\nmore 6e-3 4e-4 2e-3 6e-4 und 9e-1 5e-3 4e-3 4e-2\nadds 4e-1 6e-7 2e-5 1e-8 Schwester 5e-1 1e-4 2e-2 8e-5\nthan 9e-1 2e-4 2e-3 1e-3 Maria 1e-1 5e-4 2e-3 2e-3\narticles 8e-1 4e-6 5e-5 4e-6\nThe 6e-3 6e-4 3e-3 2e-2 Er 6e-3 1e-2 2e-2 2e-2\ndrunk 6e-3 6e-5 9e-4 2e-5 hätte 6e-3 5e-3 2e-3 2e-3\ndriver 6e-3 2e-2 2e-4 2e-3 nicht 2e-2 4e-2 2e-2 3e-2\nlost 6e-3 6e-6 2e-3 1e-5 auch 6e-3 3e-4 9e-4 4e-3\ncontrol 4e-1 4e-1 3e-3 6e-3 noch 7e-1 1e-1 1e-3 3e-2\ncrashed 5e-2 9e-8 1e-4 4e-7 am 1e-2 3e-3 3e-3 5e-3\ninto 4e-1 9e-2 2e-3 1e-1 Telefon 6e-3 4e-3 4e-3 4e-2\na 6e-1 2e-1 2e-3 1e-1 nörgeln 6e-3 8e-8 5e-5 6e-8\nstreet 6e-3 9e-4 2e-3 4e-3 sollen 7e-1 2e-4 2e-3 6e-4\nsign 6e-1 2e-2 3e-3 6e-4\nand 8e-1 1e-3 2e-3 3e-3\ndied 7e-1 3e-5 2e-3 4e-4\nM 3e-1 5e-2 1e-3 3e-2 M 2e-1 2e-2 8e-3 1e-2\nSD 4e-1 1e-1 1e-3 7e-2 SD 3e-1 7e-2 2e-2 3e-2\nMin 6e-3 1e-9 3e-6 2e-10 Min 6e-3 1e-10 2e-6 4e-13\nMax 1e+ 0 1e + 0 2e-2 5e-1 Max 1e+ 0 9e-1 2e-1 5e-1\nExamples sentences were selected to illustrate one case, in which one type of predictability is particularly high (bold). Translations (PSC): In the convent school, nun Agathe, and nun\nMaria rule (upper). He should not have moaned at the telephone, as wel l (lower sentence).\nplate regression splines from the mgcv-package (version 1.8) i n R\n(Hastie and Tibshirani, 1990; Wood, 2017 ). As several models of\neye-movement control rely on the gamma distribution ( Reichle\net al., 2003; Engbert et al., 2005 ), we here also used gamma\nfunctions with a logarithmic link function (cf. Smith and Levy,\n2013). GAMs have the advantage to model non-linear smooth\nfunctions, i.e., the GAM aims to ﬁnd the best value for the\nsmoothing parameter in an iterative process. Because smooth\nfunctions are modeled by additional parameters, the amount\nof smoothness is penalized in GAMs, i.e., the model aims to\nreduce the number of parameters of the smooth function and\nthus to avoid overﬁtting. The eﬀective degrees of freedom (ed f)\nparameter describes the resulting amount of smoothness (see\nTable 8 below). Of note is, that an edf of 1 is present if the\nmodel penalized the smooth term to a linear relationship. Edf’s\nclose to 0 indicate that the predictor has zero wiggliness and\ncan be interpreted to be penalized out of the model (\nWood,\n2017). Though Baayen (2010) suggested that word frequency\ncan be seen as a collector variable that actually also contain s\nvariance from contextual word features (cf. Ong and Kliegl,\n2008), our baseline GAMs contained single-word properties.\nWe computed a baseline GAM consisting of the length and\nfrequency of the present, last and next word as predictors\n(cf.\nKliegl et al., 2006 ). To reduce the correlations between\nthe language models trained by the subtitles corpora and the\nfrequency measures, word frequency estimates were taken fr om\nFrontiers in Artiﬁcial Intelligence | www.frontiersin.org 7 February 2022 | Volume 4 | Article 730570\nHofmann et al. Language Models Explain Viewing Times\nTABLE 3 |Correlations between word properties for the SRC (below dia gonal) and PSC (above diagonal), and the item-level means of the SFD, GD, and TVT data of the\npresent word.\n1 2. 3 4 5 6 7 8 9\n1. Length − 0.62 − 0.40 − 0.47 − 0.46 − 0.51 0.28 0.62 0.57\n2. Frequency − 0.76 0.52 0.71 0.70 0.75 − 0.35 − 0.49 − 0.50\n3. CCP − 0.48 0.58 0.56 0.36 0.56 − 0.27 − 0.34 − 0.40\n4. N–gram − 0.58 0.74 0.63 0.61 0.79 − 0.41 − 0.49 − 0.51\n5. Topic − 0.67 0.80 0.45 0.69 0.61 − 0.35 − 0.43 − 0.41\n6. RNN − 0.65 0.81 0.58 0.84 0.75 − 0.47 − 0.51 − 0.53\n7. SFD 0.35 − 0.50 − 0.33 − 0.39 − 0.40 − 0.44 0.81 0.79\n8. GD 0.54 − 0.62 − 0.38 − 0.51 − 0.55 − 0.55 0.86 0.95\n9. TVT 0.61 − 0.66 − 0.44 − 0.55 − 0.58 − 0.60 0.78 0.90\nHighlighting was used to illustrate that the language models (italic s and bold) provide always larger correlations with the three viewing time me asures than CCP (bold) (see below\nfor discussions).\nTABLE 4 |Generalized additive models (GCV , R2) for single-ﬁxation duration (SFD) and χ 2 tests (df) against the previous model for signiﬁcant increme nts in explained\nvariance ( *p < 0.05).\nSRC PSC\nGCV % /Delta1R2 Deviance (df) GCV % /Delta1R2 Deviance (df)\nBaseline 0.1273 4.17 Baseline 0.0969 3.99 Baseline\nCCP Baseline\nBaseline + present 0.1273 0.02 0.4 (1.9) 0.0968 0.1 10.7 (8)*\n+ Last 0.1272 0.06 1.1 (2.7)* 0.0967 0.05 7.5 (9.1)*\n+ Next 0.1271 0.02 0.5 (1.1) 0.0964 0.31 36.4 (9.5)*\nN-gram Baseline\nBaseline + present 0.127 0.29 3.1 (5.6)* 0.0966 0.27 33 (9.1)*\n+ Last 0.1265 0.44 5.5 (9.4)* 0.0964 0.14 15.5 (8.7)*\n+ Next 0.1265 − 0.02 0 (0.8) 0.0963 0.07 9.6 (8.5)*\nTopic Baseline\nBaseline + present 0.1272 0.12 1.4 (4.6) 0.0967 0.21 23.1 (8.4)*\n+ Last 0.1272 0.04 1.1 (5.8) 0.0963 0.28 33.8 (9.2)*\n+ Next 0.1272 0.09 1.3 (4.4) 0.0963 0.07 9.6 (9.2)*\nRNN Baseline\nBaseline + present 0.1271 0.16 1.6 (1.2)* 0.0966 0.27 31.3 (9.6)*\n+ Last 0.1271 − 0.02 0 (0.9) 0.0966 0.03 3.7 (8.7)*\n+ Next 0.1269 0.27 3.6 (11)* 0.0964 0.16 18 (8.7)*\nN-gram + Topic + RNN Full CCP model\n(Present + last + next) 0.1262 1.08 13 (33.6)* 0.0957 0.69 81.5 (49.1)*\nConsistent GAM model improvements in both data sets are marked bold.\nthe Leipzig corpora collection 6 The English corpus consisted of\n105 million unique sentences and 1.8 billion words, and the\nGerman corpus consisted of 70 million unique sentences and\n1.1 billion words (\nGoldhahn et al., 2012 ). We used Leipzig word\nfrequency classes that relate the frequency of each word to t he\nfrequency of the most frequent word using the deﬁnition that\nthe most common word is 2 class more frequent than the word\nof which the frequency is given (“der” in German and “the” in\nEnglish; e.g.,\nHofmann et al., 2011, 2018 ). Moreover, we inserted\n6http://www.corpora.uni-leipzig.de/en?corpusId=deu_newscrawl-pu blic_201\nlanding site into the baseline GAM (e.g., Pynte et al., 2008b ), to\nabsorb variance resulting from mislocated ﬁxations.\nWe added the diﬀerent types of predictability of the present\nword to the baseline model and tested whether the resulting\nGAM performs better than the baseline GAM ( Tables 4–6).\nThen we successively added the predictability scores of the l ast\nand next words and tested whether the novel GAM performs\nbetter than the preceding GAM. Finally, we also tested whether\na GAM model including all language-model-based predictors\nprovides better predictions than the GAM including CCP scores\n(\nHofmann et al., 2017 ).\nFrontiers in Artiﬁcial Intelligence | www.frontiersin.org 8 February 2022 | Volume 4 | Article 730570\nHofmann et al. Language Models Explain Viewing Times\nTABLE 5 |Generalized additive models (GCV , R2) for gaze duration (GD) and χ 2 tests (df) against the previous model for signiﬁcant increme nts in explained variance (* p <\n0.05).\nSRC PSC\nGCV % /Delta1R2 Deviance (df) GCV % /Delta1R2 Deviance (df)\nBaseline 0.1656 8.19 Baseline 0.145 10.52 Baseline\nCCP Baseline\nBaseline + present 0.1656 0.02 0.5 (1.6) 0.1448 0.09 31.2 (8.3)*\n+ Last 0.1654 0.14 5 (11.1)* 0.1447 0.05 14.2 (9)*\n+ Next 0.1652 0.08 2 (0.6)* 0.1444 0.17 45.8 (9.4)*\nN-gram Baseline\nBaseline + present 0.1651 0.32 4.3 (0.9)* 0.1446 0.24 66.8 (9)*\n+ Last 0.1648 0.14 3.8 (5.2)* 0.1443 0.13 34.1 (8.8)*\n+ Next 0.1648 0.02 1.1 (5.7) 0.1443 0.04 13.5 (8.7)*\nTopic Baseline\nBaseline + present 0.1657 0.01 − 0.6 (0.6) 0.1448 0.15 38.1 (7.7)*\n+ Last 0.1656 0.06 2.4 (6.9) 0.1444 0.14 47 (9.1)*\n+ Next 0.1656 0 0.1 (1.2) 0.1444 0.02 10.2 (9)*\nRNN Baseline\nBaseline + present 0.1653 0.21 4 (5.3)* 0.1446 0.22 64.1 (8.9)*\n+ Last 0.1652 0.1 2.2 (5.7) 0.1446 0.02 6.6 (7.9)*\n+ Next 0.1651 0.09 2.4 (4.8)* 0.1444 0.12 26 (7.8)*\nN-gram + Topic + RNN Full CCP model\n(Present + last + next) 0.1644 0.67 14.1 (28.2)* 0.1434 0.55 145.8 (52)*\nConsistent GAM model improvements in both data sets are marked bold.\nTABLE 6 |Generalized additive models (GCV , R2) for total viewing time (TVT) and χ 2 tests (df) against the previous model for signiﬁcant increme nts in explained variance\n(*p < 0.05).\nSRC PSC\nGCV % /Delta1R2 Deviance (df) GCV % /Delta1R2 Deviance (df)\nBaseline 0.1933 9.73 Baseline 0.1952 9.94 Baseline\nCCP Baseline\nBaseline + present 0.1931 0.13 2.8 (2.2)* 0.1943 0.32 134.4 (8.3)*\n+ Last 0.1931 0.02 0.6 (1.9) 0.194 0.14 42.8 (9.3)*\n+ Next 0.1929 0.14 4.2 (7.8)* 0.1938 0.09 32.6 (9.2)*\nN-gram Baseline\nBaseline + present 0.1926 0.39 8.8 (7.2)* 0.1942 0.33 139.6 (8.9)*\n+ Last 0.1925 0.09 2.7 (5.5)* 0.1937 0.23 81.8 (8.9)*\n+ Next 0.1925 0.01 1.7 (5.6) 0.1935 0.07 26.4 (8.8)*\nTopic Baseline\nBaseline + present 0.1932 0.15 4.1 (7.9)* 0.1949 0.12 48.2 (8.8)*\n+ Last 0.1932 0 2.1 (8.1) 0.1945 0.15 56.8 (8.8)*\n+ Next 0.1933 − 0.01 0 (0.9) 0.1944 0.03 17.1 (9.3)*\nRNN Baseline\nBaseline + present 0.1926 0.33 6.1 (0.2)* 0.1942 0.34 138.2 (8.2)*\n+ Last 0.1925 0.12 3.4 (6.9)* 0.194 0.1 32.5 (8.7)*\n+ Next 0.1923 0.16 5.4 (11.5)* 0.1939 0.04 18.8 (8.8)*\nN-gram + Topic + RNN Full CCP model\n(Present + last + next) 0.1917 0.64 16.6 (19.9)* 0.1924 0.52 206 (53.5)*\nConsistent GAM model improvements in both data sets are marked bold.\nFrontiers in Artiﬁcial Intelligence | www.frontiersin.org 9 February 2022 | Volume 4 | Article 730570\nHofmann et al. Language Models Explain Viewing Times\nTABLE 7 |χ 2 tests whether the respective language model performed bett er than CCP .\nSRC PSC\nSFD GD TVT SFD GD TVT\n%/Delta1R2 Deviance % /Delta1R2 Deviance % /Delta1R2 Deviance % /Delta1R2 Deviance % /Delta1R2 Deviance % /Delta1R2 Deviance\n(df) (df) (df) (df) (df) (df)\nN-gram 0.6 6.5 (10.1)* 0.24 1.8 ( − 1.4) 0.21 5.6 (6.5)* 0.02 3.6 ( − 0.2) 0.09 23.2 ( − 0.2) 0.09 38 ( − 0.2)\nTopic 0.14 1.8 (9) − 0.17 − 5.7 (− 4.6)* − 0.15 − 1.4 (5) 0.1 12 (0.2)* 0 4 ( − 1) − 0.24 − 87.7 (0)\nRNN 0.31 3.2 (7.4)* 0.17 1 (2.5) 0.33 7.3 (6.7)* 0 − 1.6 (0.3) 0.04 5.4 ( − 2.1) − 0.06 − 20.3 (− 1.1)*\nPositive deviance (df) suggests better performance of the language model, a nd negative deviance indicates that CCP ﬁts better ( *p < 0.05).\nFor a better overview, language models performing better were marked bold, and CCP performing better was marked in italics and bold.\nAs model benchmarks, we report the generalized cross-\nvalidation score (GCV). This is an estimate of the mean\nprediction error based on a leave-one-out cross validation\nprocess. Better models provide a lower GCV (\nWood, 2017 ). We\nalso report the diﬀerence in the percentage of explained varianc e\nrelative to the preceding or baseline model (% )Delta1R2, derived from\nadjusted R2-values). We also tested whether a subsequent GAM\nprovides signiﬁcantly greater log likelihood than the previo us\nmodel using χ 2-tests (anova function in R; p = 0.05, cf. Tables 4–\n6). To provide a measure that can be interpreted in a similar\nfashion as the residual sum of squares for linear models, we\nfurther report the diﬀerence of the deviance of the last and the\npresent model (e.g.,\nWood, 2017 ). If this term is negative, this\nindicates that the latter model provides a better account for the\ndata. We also report the diﬀerence of the degrees of freedom (df )\nof the models to be compared. Negative values indicate that th e\nprevious GAM is more complex.\nIn the second set of GAM comparisons, we compare the\nperformance of each single language model to the performance\nof the CCP. For this purpose, we use the predictability scores\nfor all positions (present, last, and next word), and compared\neach language model to CCP (see Table 7 below). To examine\nthe predictors themselves and to be able to directly compare th e\ncontribution of human-generated and model-based predictors in\nexplaining variance in viewing times, we also generated a ﬁna l\nGAM model for each viewing time parameter comprising all\ntypes of predictability. For these models we ﬁnally report the F-\nvalues, eﬀective degrees of freedom and the levels of signiﬁc ance\n(cf. Table 8). We evaluate the functional forms of the eﬀects\nthat are most reproducible across all analyses in the ﬁnal mod el,\nwhile setting all non-inspected variables to their mean valu e (cf.\nFigures 1–3 below).\nRESULTS\nOur simple item-level correlations revealed that all language\nmodels provided larger correlations with SFD, GD, and TVT dat a\nthan CCP (Table 3), demonstrating that language models provide\na better account for viewing times than CCP. Moreover, there a re\nsubstantial correlations between all predictor variables, making\nanalyses with multiple predictors prone to ﬁt error variance.\nTherefore, we will focus our conclusions on those ﬁndings th at\ncan be reproduced in diﬀerent types of analyses ( Tables 4–8;\nWagenmakers et al., 2011 ). When turning to these non-linear\nGAM analyses at the level of each ﬁxation event, we found that\nnearly any predictor accounts for variance in the PSC data set.\nThis suggests that all types of predictability account for vie wing\ntime variance, once there is suﬃcient statistical power. When\nwe examined typically sized samples in the SRC, only the most\nrobust eﬀects make a contribution. Therefore, we will also foc us\nour conclusions on those eﬀects that can be reproduced in both\nsamples (see Table 8 for a summary of all results).\nConcerning the CCP analyses, the only ﬁndings that can\nbe reproduced in both data samples and across all non-linear\nanalyses was the inﬂuence of last- and next-word CCP on\nGD data ( Tables 5, 8). These eﬀects seem quite robust and\ncan be examined in Figure 1. When all types of predictability\nare included in the GAM ( Table 8), CCP of the last and next\nword seems to prolong GDs particularly in the range of logit-\ntransformed CCPs of around 0.5–1.5 ( Figure 1). We see this as\na preliminary evidence that this type of predictability might be\nparticularly prone to predict that high-CCP last or next words\nare processed during present-word ﬁxations.\nIn general, CCP eﬀects seem to be most reliable in GD data.\nCCP outperformed the topic model in the SRC data set for\nGD predictions ( Table 7). Please also note that the only type of\npredictability that failed to improve the GAM predictions in th e\nhighly powered PSC sample was the CCP-eﬀect of the last word\nin SFD data ( Table 8).\nThe language models not only showed greater correlations\nwith all viewing time measures than CCP ( Table 3), but they also\ndelivered a larger number of consistent ﬁndings in our extensiv e\nset of non-linear analyses ( Tables 4–8). There are some CCP\neﬀects worth to be highlighted that are exclusively apparent in t he\nanalyses using only a single type of predictability. There are la st-\nword CCP eﬀects in SFD data ( Table 4), and a present-word eﬀect\nin the TVT data ( Table 6). In addition to the fully consistent\nlast-word GD eﬀects ( Tables 5, 8, Figure 1), this lends further\nsupport to the hypothesis that CCP is particularly prone to reﬂect\nlate processes. Moreover, there are also consistent next-word\neﬀects in the GD and TVT analyses of both samples ( Tables 5,\n6) that are, however, often better explained by other types of\npredictability in the analysis containing all predictors ( Table 8).\nOur non-linear analyses revealed that the n-gram-based\nprobabilities of the present word can account for variance in a ll\nFrontiers in Artiﬁcial Intelligence | www.frontiersin.org 10 February 2022 | Volume 4 | Article 730570\nHofmann et al. Language Models Explain Viewing Times\nTABLE 8 |The F-values of the predictors (effective degrees of freedom), t heir signiﬁcance level and the total amount of variance expla ined in models including all\npredictors at the same time.\nSFD GD TVT\nBaseline SRC PSC SRC PSC SRC PSC\nLanding site 2.9 (1.9)* 12.8 (8.6)*** 8.4 (4.1)*** 12.8 (8.7 )*** 10.5 (4.5)*** 10.7 (8.5)***\nLength 0.6 (2.0) 19.6 (8.8)*** 10.2 (3.2)*** 372.5 (9.0)*** 17.2 (2.2)*** 248.9 (9.0)***\nLength_last 6.9 (7.0)*** 29.1 (8.9)*** 6.5 (6.3)*** 26.8 (8 .8)*** 10.2 (8.5)*** 24.0 (8.8)***\nLength_next 2.1 (6.5)* 12.3 (8.2)*** 3.7 (8.7)*** 10.5 (8.5 )*** 3.4 (8.6)*** 8.9 (8.8)***\nFrequency 5.0 (4.7)*** 29.4 (8.7)*** 6.9 (4.6)*** 65.2 (9.0 )*** 4.8 (6.1)*** 81.3 (8.9)***\nFrequency_last 1.8 (1.1) 6.2 (8.8)*** 1.6 (1.0) 6.6 (8.6)** * 6.6 (3.3)*** 24.0 (8.8)***\nFrequency_next 1.5 (5.5) 17.5 (8.5)*** 2.0 (7.5)* 27.2 (8.7 )*** 2.7 (7.5)** 22.6 (8.9)***\nCCP 0.7 (1.0) 12.1 (8.8)*** 0.8 (3.0) 17.4 (8.8)*** 0.7 (1.0) 37. 1 (8.8)***\nCCP_last 1.9 (6.0). 2.1 (2.3) 2.5 (6.0)* 5.5 (8.8)*** 0.9 (2.0) 14.0 (8.9)***\nCCP_next 2.0 (1.0) 19.0 (8.7)*** 7.0 (1.0)** 14.1 (8.8)*** 1.9 (1.0) 8.5 (8.8)***\nN-gram 2.7 (4.2)* 14.0 (6.4)*** 3.8 (6.4)*** 12.8 (8.9)*** 4 .2 (3.5)** 10.5 (8.8)***\nN-gram_last 2.9 (7.4)** 12.3 (8.0)*** 7.4 (1.0)** 14.6 (8.9)*** 2.0 (5.6). 18.6 (8.9)***\nN-gram_next 3.7 (1.0). 6.0 (8.4)*** 2.7 (1.0) 8.5 (7.7)*** 5 .3 (1.0)* 10.7 (7.8)***\nTopic 2.1 (2.4). 14.6 (8.8)*** 1.6 (2.9) 14.2 (8.8)*** 1.8 (2.1) 13 .4 (8.8)***\nTopic_last 1.7 (5.4). 19.6 (8.3)*** 3.1 (6.5)** 19.0 (8.4)* ** 1.9 (6.6). 19.7 (8.5)***\nTopic_next 1.8 (2.4) 9.2 (8.6)*** 0.3 (1.1) 19.0 (8.4)*** 0. 2 (1.0) 11.2 (8.9)***\nRNN 4.4 (1.0)* 5.3 (8.5)*** 1.5 (3.6) 5.8 (8.1)*** 8.6 (1.0)** 8.5 (8.4)***\nRNN_last 1.5 (1.0) 9.2 (8.8)*** 3.3 (5.5)** 5.7 (8.5)*** 3.6 (3.9)** 7.4 (8.8)***\nRNN_next 2.3 (5.6)* 6.7 (7.8)*** 1.6 (8.2) 11.0 (7.4)*** 1.8 (3.1) 8.3 (8.7)***\nTotal R2 (%) 5.45 5.36 9.16 11.5 10.7 11.3\np < 0.1; * p < 0.05; ** p < 0.01; *** p < 0.001. Consistent ﬁndings over both data sets in Tables 4–6and this Table and are marked bold.\nthree viewing time measures. Moreover, the n-gram probabilit y\nof the last word reproducibly accounted for variance in SFD an d\nGD data. We illustrate these eﬀects in Figure 2, suggesting that\nhigh log-transformed n-gram probabilities exhibit approxima tely\nlinear decreases in GD, particularly in the range of log-\ntransformed probabilities from − 8 to − 2 (cf.\nSmith and Levy,\n2013).\nThe present and last-word n-gram eﬀects can be consistently\nobtained in the analyses of only a single type of predictabilit y\n(Tables 4–6), as well as in the analysis containing all predictors\n(Table 8). Moreover, the n-gram-based GAM, including the\npresent, the last and the next word predictor, provided\nsigniﬁcantly better predictions than the CCP-based GAM in th e\nSRC data set for SFD and TVT data ( Table 7). This result pattern\nsuggests that an n-gram model is more appropriate than CCP for\npredicting eye-movements in relatively small samples.\nConcerning the non-linear analyses of the topic models, we\nfound no eﬀects that can be reproduced across all analyses\n(Tables 4–6, 8). Thus, we found an even less consistent picture\nfor the topic model than for CCP. When tested against each othe r,\nCCP provided better predictions than the topics model in the GD\ndata of the SRC, while the reverse result pattern was obtained for\nSFD data in the PSC ( Table 7). In the analyses relying on a single\ntype of predictability, we obtained a TVT eﬀect for the present\nword that can be reproduced across both samples ( Table 6). In\nthe analyses containing all types of predictability, only the last-\nword’s topical ﬁt with the preceding sentence revealed a GD\neﬀect in the SRC data set that can be reproduced across both\nsamples ( Table 8). These result patterns may tentatively point at\na late semantic integration eﬀect of long-range semantics by topic\nmodel predictions.\nThe examination of RNN models revealed consistent ﬁndings\nacross all non-linear analyses for the present word in SFD\nand TVT data ( Tables 4–6, 8). Though consistent next-word\npredictability eﬀects were obtained for all viewing time meas ures\nin the analyses containing only a single type of predictability\n(Tables 4–6), only the next-word RNN eﬀect in SFD data was\nreproducible in the analyses containing all predictors ( Table 8).\nThis result pattern indicates that an RNN may be particularly\nuseful for investigating (parafoveal) preprocessing in rapidl y\nsuccessful lexical access.\nTherefore, we relied on SFD data to illustrate the functiona l\nform of the RNN eﬀects in Figure 3. RNN probabilities of the\npresent word reduce SFDs, particularly at a log-transformed\nprobability of − 10 and higher. Concerning the inﬂuence of\nthe next word, log-probabilities lower than − 7 seem to delay\nthe SFDs of the current word. This provides some preliminary\nevidence that an extremely low RNN probability of the word in\nthe right parafovea might leads to parafoveal preprocessing of t he\nnext word.\nNext-word probability eﬀects, however, are not the only\ndomain, in which RNN models can account for other variance\nthan the other types of predictability. We also obtained\nconsistent last-word RNN-based GAM model ﬁt improvements\nFrontiers in Artiﬁcial Intelligence | www.frontiersin.org 11 February 2022 | Volume 4 | Article 730570\nHofmann et al. Language Models Explain Viewing Times\nFIGURE 1 |Cloze completion probability effects on gaze duration. The x-axes show logit-transformed cloze completion probabili ty (CCP), while the y-axes\ndemonstrate its inﬂuence on gaze durations (ms) of the full GA M models (cf. Table 8). (A,B) Illustrate effects of last-word CCP on gaze durations of the c urrently\nﬁxated word. (C,D) Demonstrate that the CCP of the next word has an inﬂuence on the gaze durations of the currently ﬁxated word. (A,C) Display effects in the SRC\ndata set and (B,D) illustrate the results of the PSC data set. Particularly in a logit-range around 0.5–1.5, the CCP of the surrounding word s seems to delay ﬁxations.\nShaded areas indicate standard errors.\nand signiﬁcant eﬀects of last-word probabilities in the analys is\nincluding all predictors for TVT data ( Tables 6, 8)—a result\nthat probably points at the generalization capabilities of thi s\nsubsymbolic model. For the SRC data set, the RNN provided\nsigniﬁcantly better predictions in SFD and TVT data, while CC P\noutperformed the RNN model in the TVT data of the PSC data\nset (Table 7).\nWhen summing up the results of language-model-based vs.\nthe CCP-based GAM models, language models outperformed\nCCP in 5 comparisons, while CCP provided signiﬁcantly better\nﬁtting GAM models in 2 comparisons ( Table 7). Moreover,\nthe three language models together always accounted for\nmore viewing time variance than CCP ( Tables 4–6). When the\npredictors of all three language models are together incorpor ated\ninto a ﬁnal GAM, this accounted for more viewing time variance\nthan CCP ( Tables 4–6). Thus, a combined set of language model\npredictors is appropriate to explain eye-movement patterns.\nGENERAL DISCUSSION\nLanguage Models Account for More\nReproducible Variance\nIn the present study, we compared CCP to word probabilities\ncalculated from n-gram-, topic- and RNN-models for predicting\nﬁxation durations in the SRC and PSC data set. Already simple\nitem-level analyses showed that all language models provided\nFrontiers in Artiﬁcial Intelligence | www.frontiersin.org 12 February 2022 | Volume 4 | Article 730570\nHofmann et al. Language Models Explain Viewing Times\nFIGURE 2 |N-gram probability effects on gaze durations. The x-axes di splay log-transformed n-gram probability, while the y-axe s demonstrate its inﬂuence on gaze\ndurations (ms) of the full GAM models (cf. Table 8). (A,B) Demonstrate the effects of the present-word n-gram probabi lities, while (C,D) Illustrate the inﬂuence of the\nn-gram probability of the last word on the gaze durations of t he currently ﬁxated word. Results of the SRC data set are given in (A,C), while PSC data are illustrated in\n(B,D). Large log-transformed n-gram probabilities lead to an app roximately linear decrease of gaze durations, particularl y in a log-range of − 8 to − 1. Shaded areas\nindicate standard errors.\ngreater linear correlations with all viewing time measures than\nCCP (Table 3). We also explored the possibility that the raw word\nprobabilities provide a linear relationship with reading tim es\n(Brothers and Kuperberg, 2021 ). The transformed probabilities,\nhowever, always provided greater correlations with the thre e\nreading time measures than the raw probabilities. Therefore , our\nanalyses support Smith and Levy’s (2013) conclusion that the\nbest prediction is obtained with log-transformed language m odel\nprobabilities (cf. Wilcox et al., 2020 ).\nWhen contrasting each language model against CCP in our\nnon-linear analyses, there was no single language model tha t\nprovided consistently better performance than CCP for the sam e\nviewing time measure in both data sets ( Table 7). Rather, such\ncomparisons seem to depend on a number of factors such\nas the chosen data set, language, participants and materials,\ndemonstrating the need for further studies. A particularly\nimportant factor should be the number of participants in the\nCCP sample: In the small SRC data set, the language models\noutperformed CCP in 4 cases, while CCP was signiﬁcantly better\nonly 1 time. In the large CCP data set of the PSC, in contrast,\nboth CCP and the language models outperformed each other\nfor 1 time. When examining the amount of explained variance,\nhowever, the language models usually provided greater gains in\nexplained variances than CCP: The n-gram and RNN models\nprovided increments in explained variance ranging between 0. 33\nand 0.6% over CCP in the SFD and TVT data of the SRC\nFrontiers in Artiﬁcial Intelligence | www.frontiersin.org 13 February 2022 | Volume 4 | Article 730570\nHofmann et al. Language Models Explain Viewing Times\nFIGURE 3 |RNN probability effects on single-ﬁxation durations. X-axe s show log-transformed RNN probabilities, while the y-axes demonstrate the most reliable\npresent and next-word RNN effects obtained in single-ﬁxatio n durations. The (A,B) demonstrate the effect of present-word RNN probability. Re latively linear decreases\nof single-ﬁxation durations are obtained, particularly at a log-RNN probability of − 10 and higher. (C,D) Illustrate the effects of log RNN probability of the next word . Log\nRNN probabilities of the next word lower than − 7 seem to delay the ﬁxation duration of the currently ﬁxated wor d. (A,C) show SRC data and (B,D) illustrate PSC data.\nShaded areas indicate standard errors.\ndata set, in which CCP however provided better predictions\nthan the topic model (0.17%, Table 7). For the PSC data, there\nwas a topic model gain of 0.1% over CCP in SFD data, but\na CCP gain over the RNN model of 0.06% of variance. In\nsum, the language models provided better predictions than\nCCP in 5 cases—CCP provided better predictions in 2 cases\n(Table 7). Finally, the three language models together always\noutperformed CCP ( Tables 4–6), supporting\nHofmann’s et al.\n(2017) conclusion derived from linear item-level based multiple\nregression analysis. Therefore, language models not only pro vide\na deeper explanation for the consolidation mechanisms of the\nmental lexicon, but they also often perform better than CCP in\naccounting for viewing times.\nCCP Effects Set a Challenge for\nUnexplained Predictive Processes\nNevertheless, CCP can still make a large contribution toward\nunderstanding the complex interplay of diﬀerential predictive\nprocesses. Though the results are less consistent than in the\nn-gram and RNN analyses, at least the last- and next-word\nCCP eﬀects on GD data are reproducible in two eye movement\nsamples and over several analyses ( Tables 5, 8; cf. Table 7). This\nsuggests that CCP accounts for variance that is not reﬂected\nby the language models we investigated (cf. e.g.,\nBianchi et al.,\n2020). When exploring the functional form of the GD eﬀects\nof the surrounding words, Figure 1 indicated that a high CCP\nof the last and next word leads to a relatively linear increas e in\nFrontiers in Artiﬁcial Intelligence | www.frontiersin.org 14 February 2022 | Volume 4 | Article 730570\nHofmann et al. Language Models Explain Viewing Times\nGD, particularly in a logit-transformed predictability rang e of\naround 0.5–1.5. This might indicate that when between aroun d\n73 to 95% of the participants of the cloze completion sample\nagree that the cloze can be completed with this particular word ,\nCCP might represent a reliable predictor of GD. As opposed to\nfunctional forms of the language model eﬀects, CCP was the\nonly variable that predicted an increase of viewing times wit h\nincreasing levels of predictability. So CCP might represent a n\nongoing processing of the last word, or a type of parafoveal\npreprocessing that delays the present ﬁxation (cf. Figures 3C,D,\nfor other parafoveal preprocessing eﬀects).\nWhen having a large CCP sample available, the usefulness\nof CCP increases, as can be examined in the PSC eﬀects of\nthe present study ( Tables 4–8). Though CCP can be considered\nan “all-in” variable, containing to some degree co-occurre nce-\nbased, semantic and syntactic responses (\nStaub et al., 2015 ), CCP\nsamples might probably vary with respect to the amount of these\ndiﬀerent types of linguistic structure that is contained in th ese\nsamples. This might in part explain some inconsistency between\nour SRC and PSC samples. We suggest that future studies should\nmore closely evaluate which type of information is contained in\nwhich particular CCP sample, in order to obtain a scientiﬁcall y\ndeeper explanation for the portions of the variance that can\npresently still be better accounted for by CCP (\nShaoul et al., 2014;\nLuke and Christianson, 2016; Hofmann et al., 2017; Lopukhina\net al., 2021\n). Table 3 shows that the correlations of n-gram and\nRNN models with the CCP data are larger than the correlations\nwith topics models in both data samples. This suggests that\nthe present cloze completion procedures were more sensitive to\nshort-range syntax and semantics rather to long-range sema ntics.\nThese CCP scores were based on sentence context—a stronger\ncontribution of long-range semantics could be probably expec ted\nwhen the cloze (and reading) tasks are based on paragraph data\n(e.g.,\nKennedy et al., 2013 ).\nThough last-word SFD eﬀects ( Table 4) and present-word\nTVT eﬀects ( Table 6) of CCP seem to be better explained by\nthe language models ( Table 8), this result pattern conﬁrms the\nprediction of the EZ-reader model that CCP particularly reﬂec ts\nlate processes during reading ( Reichle et al., 2003 ).\nSymbolic Short-Range Semantics and\nSyntax in N-Gram Models\nWhile it has often been claimed that CCP reﬂects semantic\nprocesses (e.g.,\nStaub et al., 2015 ), it is diﬃcult to deﬁne\nwhat “semantics” exactly means. Here we oﬀer scientiﬁcally\ndeep explanations relying on the computationally concrete\ndeﬁnitions of probabilistic language models (\nReichle et al., 2003;\nWestbury, 2016 ), which allow for a deeper understanding of the\nconsolidation mechanisms. An n-gram model is a simple count-\nbased model that relies exclusively on symbolic representat ions\nof words. We call it a short-range “semantics and syntax” mod el,\nbecause it is trained from the immediately preceding words.\nThe n-gram model reﬂects sequential-syntagmatic “low-lev el”\ninformation (e.g.,\nMcDonald and Shillcock, 2003b ).\nThe present word’s n-gram probability accounted for early\nsuccessful lexical access as reﬂected in SFD, standard lexi cal\naccess as reﬂected in the GD, as well as late integration\nas reﬂected in the TVT. Moreover, the last word’s n-gram\nprobability accounted for lagged eﬀects on SFD and GD data,\nwhich replicates and extends\nSmith and Levy’s (2013) ﬁndings.\nThe examination of the functional form also conﬁrms their\nconclusion that last- and present-word log n-gram probabilit y\nprovides a (near-)linear relationship with GD (see also\nWilcox\net al., 2020 )—at least in the log-transformed probability range\nof − 8 to − 2 ( Figure 2). Such a near-linear relationship was also\nobtained for the log RNN probability of the present word with\nSFD data ( Figures 3A,B).\nThe present- and last-word eﬀects of the n-gram model were\nremarkably consistent across the two diﬀerent eye-movement\nsamples, as well as over diﬀerent analyses ( Tables 4–6, 8; cf.\nWagenmakers et al., 2011 ). Our data support the view that n-\ngram models not only explain early eﬀects of lexical access (e.g .,\nMcDonald and Shillcock, 2003a; Frisson et al., 2005 ), but can\nalso be used for the study of late semantic integration (e.g. ,\nBoston et al., 2008 ). Moreover, they seem to be a highly useful\ntool, when aiming to demonstrate the limitations of the eye-\nmind assumption (\nJust and Carpenter, 1984 ). N-gram models\nconsistently predict lagged eﬀects that reﬂect the sustained\nsemantic processing of the last word during the current ﬁxati on\nof a word. In sum, count-based syntactic and short-range\nsemantic processes can reliably explain last-word and present -\nword probability eﬀects (e.g.,\nSmith and Levy, 2013; Baroni et al.,\n2014).\nLess Consistent Findings in Long-Range\nTopic Models\nTopic models provided the least consistent picture over the two\ndiﬀerent samples and the diﬀerent types of analyses ( Tables 4–\n6, 8). Topic models are count-based subsymbolic approaches\nto long-range semantics, which is consolidated from the co-\noccurrences in whole documents. As the same is true for LSA,\nthey can well be compared to previous LSA-ﬁndings (\nKintsch and\nMangalath, 2011 ). For instance, Bianchi et al. (2020) obtained\nremarkably weaker LSA-based eﬀects than n-gram-based GD\neﬀects—thus our results are in line with the results of this\nstudy (cf.\nHofmann et al., 2017 ). When they combined LSA\nwith an n-gram model and additionally included next-word\nsemantic matches in their regression model, some of the LSA-\nbased eﬀects became signiﬁcant. This greater robustness of th e\nLSA eﬀects in\nBianchi et al. (2020) may be well-explained by\nKintsch and Mangalath’s (2011) proposal, that syntactic factors\nshould be additionally considered when aiming to address long -\nrange meaning and comprehension. As opposed to Bianchi’s et al.\n(2020) next-word GD eﬀects, however, the present study revealed\nlast-word GD eﬀects of long-range semantics in the analysis\nconsidering all predictors ( Table 8). And when examining only\nthe present word’s topical ﬁt with the preceding sentence,\nWang’s\net al. (2010) conclusion was corroborated that (long-range)\nlexical knowledge of whole documents is best reﬂected in TVT\ndata (see Table 6).\nIn sum, long-range semantic models provide a less consistent\npicture than the other language models (cf.\nLopukhina et al.,\n2021). The results become somewhat more consistent when\nshort-range relations are additionally taken into account . Given\nFrontiers in Artiﬁcial Intelligence | www.frontiersin.org 15 February 2022 | Volume 4 | Article 730570\nHofmann et al. Language Models Explain Viewing Times\nthat the eﬀects occur in the last, present or next word when\nshort-range semantics is included, this may point at a positio nal\nvariability of long-range semantics that is integrated at s ome\npoint in time during retrieval. We think that this results fro m\nthe position-insensitive training procedure. Therefore, ra ther\nthan completely rejecting the eye-mind assumption by proposing\nthat “the process of retrieval is independent of eye movements ”\n(\nAnderson et al., 2004 , p. 225), we suggest that long-range\nsemantics is position-insensitive at consolidation. There fore, it\nis also position-insensitive at lexical retrieval and the eﬀe cts can\nhardly be constrained to the last, present or next word, even i f\nshort-range relations are considered ( Wang et al., 2010; Bianchi\net al., 2020 ).\nFinally, it should be taken into account that we here examine d\nsingle sentence reading. This may explain the superiority\nof language models that are trained at the sentence level.\nDocument-level training might be superior when examining\nwords embedded in paragraphs or documents. This hypothesis\nis in part conﬁrmed by\nLuke and Christianson (2016) . They\ncomputed the similarity of each word to the preceding paragraph\nand found relatively robust LSA eﬀects (\nLuke and Christianson,\n2016, e.g., Tables 41–45).\nRNN Models: An Alternative View on the\nMental Lexicon\nShort-range semantics and syntax can be much more reliably\nconstrained to present-, last-, or next-word processing, as a lready\ndemonstrated by the consistent present and last-word n-gram\neﬀects. For the RNN probabilities, the simple linear item-\nlevel correlations with SFD, GD, and TVT data were largest,\nreplicating, and extending the results of\nHofmann et al. (2017) .\nFor the non-linear analyses, the present word’s RNN probabilit ies\nprovided the most consistent results for SFD and TVT ( Tables 5,\n6, 8). Though GD was signiﬁcant when only examining a single\nlanguage model ( Table 5), this result could not be conﬁrmed\nin the analysis in which all predictors competed for viewing\ntime variance ( Table 8). We propose that this diﬀerence can\nbe explained by considering how these short-range models are\ntrained for consolidated information. The n-gram model is\nsymbolic; thus, the prediction is only made for a particular\nword form. And when testing for standard lexical access (tha t\nis reﬂected in the GD), a perfect match of the predicted and the\nobserved word form may explain the superiority of the n-gram\nmodel in this endeavor ( Table 8).\nOn the other hand, both language models accounted for\nSFD and TVT variance in the analysis containing all types of\npredictability ( Table 8). The co-existence of these eﬀects may be\nexplained by the proposal that both models represent diﬀerent\nviews on the mental lexicon (\nElman, 2004; Frank, 2009 ). The n-\ngram model represents a “static” view, in which large lists of word\nsequences and their frequencies are stored, to see which wor d is\nmore likely to occur in a context, given this large “dictiona ry”\nof word sequences. The RNN model, in contrast, has only a\nfew hundred hidden units that reﬂect the “mental state” of th e\nmodel (\nElman, 2004 ). As a result of this small “mental space, ”\nneural models have to compress the word information, which\nmay, for instance, explain their generalization capabilitie s: When\nsuch a model is trained to learn statements such as “robin is\na bird” and “robin can ﬂy, ” and it later learns only a few facts\nabout a novel bird, e.g., “sparrow can ﬂy, ” “sparrow” obtains a\nsimilar hidden unit representation as “robin” (\nMcClelland and\nRogers, 2003 ). Therefore, a neural model can complete the cloze\n“sparrow is a . . . ” with “bird, ” even if it never was presented\nthis particular combination of words. For instance, in our PS C\nexample stimulus sentence “Die [The] Richter [judges] der [of\nthe] Landwirtschaftsschau [agricultural show] prämieren [a ward\na price to] Rhabarber [rhubarb] und [and] Mangold [mangold], ”\nthe word “prämieren” (award a price) has a relatively low n-\ngram probability of 1.549e-10, but a higher RNN probability of\n1.378e-5, because the n-gram model has never seen the word\nn-gram “der Landwirtschaftsschau prämieren [agricultural s how\naward a price to], ” but the RNN model’s hidden units are\ncapable of inferring such information from similar contexts (e.g.,\nWu et al., 2021 ). In sum, both views on the mental lexicon\naccount for diﬀerent portions of variance in word viewing time s\n(Table 8). The n-gram model may explain variance resulting\nfrom an exact match of a word form in that context, while for\ninstance generalized information may be better explained by t he\nRNN model.\nThere are, however, also diﬀerences in the result patterns\nthat are best captured by the two language models, respectively .\nA notable diﬀerence between count-based, symbolic knowledg e\nin the n-gram vs. predict-based, subsymbolic knowledge in th e\nRNN lies in their capability to account for last-word vs. next-\nword eﬀects. While the n-gram model obtained remarkably\nconsistent ﬁndings for the last word, next-word SFD eﬀects ar e\nbetter captured by an RNN ( Tables 4, 8). This corroborates our\nconclusion that the two views on the mental lexicon account\nfor diﬀerent eﬀects. The search for a concrete word form, given\nthe preceding word forms in the n-gram model, may take some\ntime. Therefore, the probabilities of the last word still aﬀect the\nprocessing of the present word. As opposed to this static view\non a huge mental lexicon, the hidden units in the RNN model\nare trained to predict the next word (\nBaroni et al., 2014 ). When\nsuch a predicted word to the right of the present ﬁxation position\nmay cross a log RNN probability of − 7, its presence can be\nveriﬁed (Figures 3C,D). Therefore, the RNN-based probability of\nthe next word may elicit fast and successful lexical access o f the\npresent word, as reﬂected in the SFD.\nLimitations and Outlook\nModel comparison is known to be related to the numbers of\nparameters included in the models, thus a comparison of the\nGAMs comparing all three language models with the CCP\npredictor might overemphasize the eﬀects of the language model s\n(Tables 4–6). However, we think that language models allow\nfor a deeper understanding of natural language processing of\nhumans than CCP does, because language models provide a\ncomputational deﬁnition how “predictability” is consolidate d\nfrom experience. Moreover, the conclusion that language mode ls\naccount for more variance than CCP is also corroborated by all\nother analyses ( Tables 3–8). Sometimes it is stated that GAMs\nin general are prone to overﬁtting. CCP and even more so\nthe model-generated predictability scores are highly correl ated\nwith word frequency, potentially leading to overﬁtting on the\none hand. On the other hand, it is more diﬃcult for the\nFrontiers in Artiﬁcial Intelligence | www.frontiersin.org 16 February 2022 | Volume 4 | Article 730570\nHofmann et al. Language Models Explain Viewing Times\nlanguage models to account for additional variance, becaus e\ntheir higher correlation (i.e., their higher shared varian ces)\nwith word frequency ( Table 3). Wood (2017) discusses that the\npenalization procedure that leads to the smoothing parameters\nand also the GCV procedure inherent in the gam() function in R\nspeciﬁcally tackles overﬁtting. We further addressed this q uestion\nby focusing on consistent results, visible across the comput ation\nfor two independent eye-movement samples and diﬀerent types\nof analyses. This approach reduced the number of robust and\nconsistent ﬁndings. If one would have ignored the non-linear\nnature of the relationships between predictors and dependent\nvariables and only examined the simple linear eﬀects reported\nin Table 3, however, the advantages of the language models over\nCPP would have been much clearer.\nThis also leads to the typical concern for analyses on\nunaggregated data that they account for only a small “portion ”\nof variance. For instance,\nDuncan (1975, p. 65 ) suggests to\n“eschew altogether the task of dividing up R2 into unique\ncausal components” (quoted from Kliegl et al., 2006 , p. 22).\nIt is clear that unaggregated data contain a lot of noise, for\ninstance resulting from a random walk in the activation of\nlexical units, random errors in saccade length, and also “[s ]accade\nprograms are generated autonomously, so that ﬁxation durati ons\nare basically realizations of a random variable” (\nEngbert et al.,\n2005, p. 781). Therefore, if we like to estimate the “unique casual\ncomponent” of semantic and syntactic processes, we need to rel y\non aggregated data. Table 3 suggests that these top-down factors\nrepresented by language account for a reasonable 15–36% of th e\nviewing time variance, while CCP accounted for 7–19%.\nIt also becomes clear that the two data sets (PSC and\nSRC), diﬀer in important aspects. First, the CCP samples\ndiﬀer in size, thus they probably provide a diﬀerent signal-\nto-noise ratio. A second diﬀerence is that also the PSC eye-\nmovement sample is larger and thus has more statistical power\nto identify signiﬁcant eye-movement eﬀects of single predicto rs.\nThird, the CCP measure is derived from diﬀerent participant\nsamples. We would also point to the fact, that the eye-\nmovement and CCP samples were collected at diﬀerent times\nand come from diﬀerent countries, which may also explain\nsome diﬀerences between the obtained eﬀects. Fourth, also\nthe English and the German subtitles training corpora may\ncontain slightly diﬀerent information. Having these limita tions\nin mind, we feel that the most consistent ﬁndings discussed\nin the previous sections represent robust eﬀects to evaluate\nthe functioning and the predictions of language models. Our\nrather conservative approach might miss some eﬀects that are\nactually apparent, though they are explainable by these four\nmajor diﬀerences between the samples. Therefore, future stud ies\nmight more closely characterize the CCP participant samples.\nThey may examine the same participants’ eye movements to\nobtain predictability estimates that are representative for the\nparticipants—which might increase the amount of explainable\nvariance (cf.\nHofmann et al., 2020 ). Finally, google n-gram\ntraining corpora may help to obtain training corpora stemming\nfrom the same time as the eye-movement data collection.\nThough we were able to discriminate between long-range\nsemantics and short-range relations that can be diﬀerentiat ed\ninto count-based symbolic and predict-based subsymbolic\nrepresentations, we like to point at the fact that the short-ra nge\nrelations could also be separated into semantic and syntacti c\neﬀects. For example, RNN models have previously also been\nrelated to syntax processing (\nElman, 1990 ). Therefore, syntactic\ninformation may alternatively explain the very early and ver y\nlate eﬀects ( Friederici, 2002 ). To examine whether semantic\nor syntactic eﬀects are at play, a promising start for further\nevaluations may examine content vs. function words, which m ay\neven lead to more consistent ﬁndings for long-range semanti c\nmodels. Further analysis may focus on language models that tak e\ninto account syntactic information (e.g.,\nPadó and Lapata, 2007 ;\ncf. Frank, 2009).\nCONCLUSION\nUnderstanding the complex interplay of diﬀerent types of\npredictability for reading is a challenging endeavor, but we think\nthat our review and our data point at diﬀerential contribution s\nof count-based and predict-based models in the domain of\nshort-range knowledge. Count-based models better capture l ast-\nword eﬀects, predict-based models better capture early next-\nword eﬀects, while present-word probabilities both make an\nindependent contribution to viewing times. In contrast, CCP\nis a rather all-in predictor, that probably covers both types of\nsemantics: short-range and long-range. But we have shown th at\nlanguage models with their diﬀerential foci are better suited\nfor a deeper explanation for eye-movement behavior, and thus\napplicable in theory development for models of eye-movement\ncontrol. Finally, we hope that we made clear that these relativ ely\nsimple language models are highly useful for understanding\ndiﬀerential lexical access and semantic integration paramet ers\nthat are reﬂected in diﬀerential viewing time parameters.\nDATA AVAILABILITY STATEMENT\nPublicly available datasets were analyzed in this study.\nThese data can be found at: https://clarinoai.informatik.\nuni-leipzig.de/fedora/objects/mrr:11022000000001F2FB/\ndatastreams/EngelmannVasishthEngbertKliegl2013_1.0/\ncontent. The data and analysis examples of the present\nstudy can be found under https://osf.io/z7d3y/?view_only=\nbe48ab71ccd14da5b0413269c150d2f9.\nETHICS STATEMENT\nEthical review and approval was not required for the study\non human participants in accordance with the local legislation\nand institutional requirements. Written informed consent fo r\nparticipation was not required for this study in accordance wi th\nthe national legislation and the institutional requirement s.\nAUTHOR CONTRIBUTIONS\nSR provided the language models. MH analyzed the data. LK\nchecked and reﬁned the analyses. All authors wrote the paper\n(major writing: MH).\nFrontiers in Artiﬁcial Intelligence | www.frontiersin.org 17 February 2022 | Volume 4 | Article 730570\nHofmann et al. Language Models Explain Viewing Times\nFUNDING\nThis paper was funded by a grant of the Deutsche\nForschungsgemeinschaft to MH (HO 5139/2-1\nand 2-2).\nACKNOWLEDGMENTS\nWe like to thank Albrecht Inhoﬀ, Arthur Jacobs, Reinhold Klie gl,\nand the reviewers of previous submissions for their helpful\ncomments.\nREFERENCES\nAdelman, J. S., and Brown, G. D. (2008). Modeling lexical decision : The\nform of frequency and diversity eﬀects. Psychol. Rev. 115, 214–229.\ndoi: 10.1037/0033-295X.115.1.214\nAnderson, J. R., Bothell, D., and Douglass, S. (2004). Eye movemen ts do not reﬂect\nretrieval processes: limits of the eye-mind hypothesis. Psychol. Sci. 15, 225–231.\ndoi: 10.1111/j.0956-7976.2004.00656.x\nBaayen, H. B. (2010). Demythologizing the word frequency eﬀect: a discriminative\nlearning perspective. Ment. Lex. 5, 436–461. doi: 10.1075/ml.5.3.10baa\nBaayen, R. H., Davidson, D. J., and Bates, D. M. (2008). Mixed- eﬀects modeling\nwith crossed random eﬀects for subjects and items. J. Mem. Lang. 59, 390–412.\ndoi: 10.1016/j.jml.2007.12.005\nBaroni, M., Dinu, G., and Kruszewski, G. (2014). “Don’t count, predict!\na systematic comparison of context-counting vs. context-predict ing\nsemantic vectors, ” in Proceedings of the 52nd Annual Meeting of the\nAssociation for Computational Linguistics, Vol. 1 (Baltimore, MD), 238–247.\ndoi: 10.3115/v1/P14-1023\nBianchi, B., Bengolea Monzón, G., Ferrer, L., Fernández Slezak, D ., Shalom, D.\nE., and Kamienkowski, J. E. (2020). Human and computer estimations\nof Predictability of words in written language. Sci. Rep. 10:4396.\ndoi: 10.1038/s41598-020-61353-z\nBiemann, C., Roos, S., and Weihe, K. (2012). “Quantifying sema ntics using\ncomplex network analysis, ” in 24th International Conference on Computational\nLinguistics–Proceedings of COLING 2012: Technical Papers (Mumbai), 263–278.\nBlei, D. M., Ng, A. Y., and Jordan, M. I. (2003). Latent dirichlet a llocation. J. Mach.\nLearn. Res. 3, 993–1022. Available online at: https://www.jmlr.org/papers/\nvolume3/blei03a/blei03a.pdf?TB_iframe=true&width=370.8&height=658.8\nBoston, M. F., Hale, J., Kliegl, R., Patil, U., and Vasishth, S. (20 08).\nParsing costs as predictors of reading diﬃculty: an evaluation us ing the\npotsdam sentence corpus. J. Eye Move. Res. 2, 1–12. doi: 10.16910/jemr.\n2.1.1\nBrothers, T., and Kuperberg, G. R. (2021). Word predictability eﬀe cts are\nlinear, not logarithmic: implications for probabilistic models of sente nce\ncomprehension. J. Mem. Lang. 116:104174. doi: 10.1016/j.jml.2020.10\n4174\nBrysbaert, M., Buchmeier, M., Conrad, M., Jacobs, A. M., Bölte, J. , and Böhl,\nA. (2011). The word frequency eﬀect: a review of recent developments and\nimplications for the choice of frequency estimates in German. Exp. Psychol. 58,\n412–424. doi: 10.1027/1618-3169/a000123\nChen, S. F., and Goodman, J. (1999). Empirical study of smoothing\ntechniques for language modeling. Comp. Speech Lang. 13, 359–394.\ndoi: 10.1006/csla.1999.0128\nDeerwester, S., Dumais, S. T., Furnas, G. W., Landauer,\nT. K., and Harshman, R. (1990). Indexing by latent\nsemantic analysis. J. Am. Soc. Inform. Sci. 41, 391–407.\ndoi: 10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9\nDemberg, V., and Keller, F. (2008). Data from eye-tracking corpora as evidence\nfor theories of syntactic processing complexity. Cognition 109, 193–210.\ndoi: 10.1016/j.cognition.2008.07.008\nDuncan, O. D. (1975). Introduction to Structural Equation Models, 1st Edn. New\nYork, NY: Academic Press.\nElman, J. L. (1990). Finding structure in time. Cogn. Sci. 14, 179–211.\ndoi: 10.1207/s15516709cog1402_1\nElman, J. L. (2004). An alternative view of the mental lexicon. Trends Cogn. Sci. 8,\n301–306. doi: 10.1016/j.tics.2004.05.003\nEngbert, R., Nuthmann, A., Richter, E. M., and Kliegl, R. (2005). SWIFT: a\ndynamical model of saccade generation during reading. Psychol. Rev. 112,\n777–813. doi: 10.1037/0033-295X.112.4.777\nEngelmann, F., Vasishth, S., Engbert, R., and Kliegl, R. (2013). A framework for\nmodeling the interaction of syntactic processing and eye movement c ontrol.\nTop. Cogn. Sci. 5, 452–474. doi: 10.1111/tops.12026\nFeigl, H. (1945). Rejoinders and second thoughts (Symposium on o perationism).\nPsychol. Rev. 52, 284–288. doi: 10.1037/h0063275\nFrank, S. (2009). “Surprisal-based comparison between a symbolic and a\nconnectionist model of sentence processing, ” in Proceedings of the 31st Annual\nConference of the Cognitive Science Society (Amsterdam), 1139–1144.\nFrank, S. L., and Bod, R. (2011). Insensitivity of the human sen tence-\nprocessing system to hierarchical structure. Psychol. Sci. 22, 829–834.\ndoi: 10.1177/0956797611409589\nFriederici, A. D. (2002). Towards a neural basis of auditory sente nce processing.\nTrends Cogn. Sci. 6, 78–84. doi: 10.1016/S1364-6613(00)01839-8\nFrisson, S., Rayner, K., and Pickering, M. J. (2005). Eﬀects of contextual\npredictability and transitional probability on eye movements during re ading. J.\nExp. Psychol. Learn. Mem. Cogn. 31, 862–877. doi: 10.1037/0278-7393.31.5.862\nGoldhahn, D., Eckart, T., and Quasthoﬀ, U. (2012). “Building larg e monolingual\ndictionaries at the leipzig corpora collection: from 100 to 200 languag es, ” in\nProceedings of the 8th International Conference on Language R esources and\nEvaluation (Istanbul), 759–765.\nGriﬃths, T. L., Steyvers, M., and Tenenbaum, J. B. (2007). Topic s in semantic\nrepresentation. Psychol. Rev. 114, 211–244. doi: 10.1037/0033-295X.114.2.211\nHastie, T., and Tibshirani, R. (1990). Exploring the nature of cov ariate eﬀects\nin the proportional hazards model. Int. Biometr. Soc. 46, 1005–1016.\ndoi: 10.2307/2532444\nHempel, C. G., and Oppenheim, P. (1948). Studies on the logic of explanat ion.\nPhilos. Sci. 15, 135–175. doi: 10.1086/286983\nHofmann, M. J., Biemann, C., and Remus, S. (2017). “Benchmarking n- grams,\ntopic models and recurrent neural networks by cloze completions, EEGs and\neye movements, ” in Cognitive Approach to Natural Language Processing , eds\nB. Sharp, F. Sedes, and W. Lubaszewsk (London: ISTE Press Ltd, Els evier),\n197–215. doi: 10.1016/B978-1-78548-253-3.50010-X\nHofmann, M. J., Biemann, C., Westbury, C. F., Murusidze, M., Conrad , M.,\nand Jacobs, A. M. (2018). Simple co-occurrence statistics reproduci bly predict\nassociation ratings. Cogn. Sci. 42, 2287–2312. doi: 10.1111/cogs.12662\nHofmann, M. J., Kuchinke, L., Biemann, C., Tamm, S., and Jacobs, A. M. (2011).\nRemembering words in context as predicted by an associative read-ou t model.\nFront. Psychol. 2, 1–11. doi: 10.3389/fpsyg.2011.00252\nHofmann, M. J., Müller, L., Rölke, A., Radach, R., and Biemann, C. (2 020).\n“Individual corpora predict fast memory retrieval during reading, ” in\nProceedings of the 6th Workshop on Cognitive Aspects of the Le xicon (CogALex-\nVI) (Barcelona).\nInhoﬀ, A. W., and Radach, R. (1998). “Deﬁnition and computatio n of\noculomotor measures in the study of cognitive processes, ” in Eye Guidance\nin Reading and Scene Perception (Amsterdam: Elsevier Science Ltd), 29–53.\ndoi: 10.1016/B978-008043361-5/50003-1\nJust, M. A., and Carpenter, P. A. (1984). “Using eye ﬁxations to study reading\ncomprehension, ” inNew Methods in Reading Comprehension Research , eds D.\nE. Kieras and M. A. Just (Hillsdale, NJ: Erlbaum), 151–169.\nKennedy, A., Pynte, J., Murray, W. S., and Paul, S. A. (2013). Freq uency and\npredictability eﬀects in the dundee corpus: an eye movement analysis . Q. J. Exp.\nPsychol. 66, 601–618. doi: 10.1080/17470218.2012.676054\nKintsch, W., and Mangalath, P. (2011). The construction of mean ing.\nTop. Cogn. Sci. 3, 346–370. doi: 10.1111/j.1756-8765.2010.01\n107.x\nKliegl, R., Grabner, E., Rolfs, M., and Engbert, R. (2004). Length ,\nfrequency, and predictability eﬀects of words on eye movements in\nreading. Euro. J. Cogn. Psychol. 16, 262–284. doi: 10.1080/095414403400\n00213\nFrontiers in Artiﬁcial Intelligence | www.frontiersin.org 18 February 2022 | Volume 4 | Article 730570\nHofmann et al. Language Models Explain Viewing Times\nKliegl, R., Nuthmann, A., and Engbert, R. (2006). Tracking the mind during\nreading: the inﬂuence of past, present, and future words on ﬁxation durations.\nJ. Exp. Psychol. Gen. 135, 12–35. doi: 10.1037/0096-3445.135.1.12\nKneser, R., and Ney, H. (1995). “Improved backing-oﬀ for m-gram lang uage\nmodeling, ” in Proceeding IEEE International Conference on Acoustics, Spe ech\nand Signal Processing (Detroit, MI: IEEE), 181–184.\nKutas, M., and Federmeier, K. D. (2011). Thirty years and counti ng: ﬁnding\nmeaning in the N400 component of the event-related brain potential ( ERP).\nAnnu. Rev. Psychol. 62, 621–647. doi: 10.1146/annurev.psych.093008.131123\nLandauer, T. K., and Dumais, S. T. (1997). A solution to platos proble m: the\nlatent semantic analysis theory of acquisition, induction and re presentation of\nknowledge. Psychol. Rev. 104, 211–240. doi: 10.1037/0033-295X.104.2.211\nLeCun, Y., Bengio, Y., and Hinton, G. (2015). Deep learning. Nature 521, 436–444.\ndoi: 10.1038/nature14539\nLopukhina, A., Lopukhin, K., and Laurinavichyute, A. (2021). M orphosyntactic\nbut not lexical corpus-based probabilities can substitute for cloz e\nprobabilities in reading experiments. PLoS ONE 16:e246133.\ndoi: 10.1371/journal.pone.0246133\nLuke, S. G., and Christianson, K. (2016). Limits on lexical predic tion during\nreading. Cogn. Psychol. 88, 22–60. doi: 10.1016/j.cogpsych.2016.06.002\nMandera, P., Keuleers, E., and Brysbaert, M. (2017). Explaining hu man\nperformance in psycholinguistic tasks with models of semantic similari ty based\non prediction and counting: a review and empirical validation. J. Mem. Lang.\n92, 57–78. doi: 10.1016/j.jml.2016.04.001\nManning, C. D., and Schütze, H. (1999). Foundations of Statistical Natural\nLanguage Processing. Cambridge, MA: The MIT Press.\nMcClelland, J. L., and Rogers, T. T. (2003). The parallel distributed\nprocessing approach to semantic cognition. Nat. Rev. Neurosci. 4, 310–322.\ndoi: 10.1038/nrn1076\nMcDonald, S. A., and Shillcock, R. C. (2003a). Eye movements reveal the on-line\ncomputation of lexical probabilities during reading. Psychol. Sci. 14, 648–652.\ndoi: 10.1046/j.0956-7976.2003.psci_1480.x\nMcDonald, S. A., and Shillcock, R. C. (2003b). Low-level predictiv e inference in\nreading: the inﬂuence of transitional probabilities on eye movemen ts. Vision\nRes. 43, 1735–1751. doi: 10.1016/S0042-6989(03)00237-2\nMikolov, T. (2012). Statistical Language Models Based on Neural Networks. (PhD\nthesis), Miyazaki: Brno University of Technology, Brno (Czechi a).\nMikolov, T., Chen, K., Corrado, G., and Dean, J. (2013). Eﬃcient Estimation of\nWord Representations in Vector Space. Available online at: https://arxiv.org/abs/\n1301.3781.\nMikolov, T., Grave, É., Bojanowski, P., Puhrsch, C., and Joulin , A. (2018).\n“Advances in pre-training distributed word representations, ” in Proceedings of\nthe Eleventh International Conference on Language Resource s and Evaluation\n(LREC 2018).\nNew, B., Ferrand, L., Pallier, C., and Brysbaert, M. (2006). Reexami ning the word\nlength eﬀect in visual word recognition: new evidence from the en glish lexicon\nproject. Psychon. Bull. Rev. 13, 45–52. doi: 10.3758/BF03193811\nNuthmann, A., Engbert, R., and Kliegl, R. (2005). Mislocated ﬁxat ions during\nreading and the inverted optimal viewing position eﬀect. Vision Res. 45,\n2201–2217. doi: 10.1016/j.visres.2005.02.014\nOng, J. K. Y., and Kliegl, R. (2008). Conditional co-occurrence pro bability acts\nlike frequency in predicting ﬁxation durations. J. Eye Mov. Res. 2, 1–7.\ndoi: 10.16910/jemr.2.1.3\nO’Regan, J. K., and Jacobs, A. M. (1992). Optimal viewing positi on eﬀect in\nword recognition: a challenge to current theory. J. Exp. Psychol. Hum. Percept.\nPerform. 18, 185–197. doi: 10.1037/0096-1523.18.1.185\nPadó, S., and Lapata, M. (2007). Dependency-based constructio n of semantic space\nmodels. Comput. Lingu. 33, 161–199. doi: 10.1162/coli.2007.33.2.161\nPaller, K. A., and Wagner, A. D. (2002). Observing the transformat ion\nof experience into memory. Trends Cogn. Sci. 6, 93–102.\ndoi: 10.1016/S1364-6613(00)01845-3\nPauls, A., and Klein, D. (2011). “Faster and smaller n-gram language models, ”\nin ACL-HLT 2011–Proceedings of the 49th Annual Meeting of the Associat ion\nfor Computational Linguistics: Human Language Technologies , Vol. 1, Portland,\nOR, 258–267.\nPhan, X.-H., and Nguyen, C.-T. (2007). GibbsLDA++ : A C/C ++ Implementation\nof Latent Dirichlet Allocation (LDA). Available online at: http://gibbslda.\nsourceforge.net\nPynte, J., New, B., and Kennedy, A. (2008a). A multiple regression a nalysis of\nsyntactic and semantic inﬂuences in reading normal text. J. Eye Mov. Res. 2,\n1–11. doi: 10.16910/jemr.2.1.4\nPynte, J., New, B., and Kennedy, A. (2008b). On-line contextu al inﬂuences during\nreading normal text: a multiple-regression analysis. Vision Res. 48, 2172–2183.\ndoi: 10.1016/j.visres.2008.02.004\nRadach, R., Inhoﬀ, A. W., Glover, L., and Vorstius, C. (2013). Contextual constraint\nand N + 2 preview eﬀects in reading. Q. J. Exp. Psychol. 66, 619–633.\ndoi: 10.1080/17470218.2012.761256\nRadach, R., and Kennedy, A. (2013). Eye movements in reading: so me theoretical\ncontext. Q. J. Exp. Psychol. 66, 429–452. doi: 10.1080/17470218.2012.750676\nRayner, K. (1998). Eye movements in reading and information process ing: 20 years\nof research. Psychol. Bull. 124, 372–422. doi: 10.1037/0033-2909.124.3.372\nReichle, E. D., Rayner, K., and Pollatsek, A. (2003). The E-Z read er model of eye-\nmovement control in reading: comparisons to other models. Behav. Brain Sci.\n26, 445–476. doi: 10.1017/S0140525X03000104\nReilly, R. G., and Radach, R. (2006). Some empirical tests of an inte ractive\nactivation model of eye movement control in reading. Cogn. Syst. Res. 7, 34–55.\ndoi: 10.1016/j.cogsys.2005.07.006\nSchilling, H. H., Rayner, K., and Chumbley, J. I. (1998). Comparing nami ng,\nlexical decision, and eye ﬁxation times: word frequency eﬀects a nd individual\ndiﬀerences. Mem. Cogn. 26, 1270–1281. doi: 10.3758/BF03201199\nSeidenberg, M. S., and McClelland, J. L. (1989). A distributed, d evelopmental\nmodel of word recognition and naming. Psychol. Rev. 96, 523–568.\ndoi: 10.1037/0033-295X.96.4.523\nSereno, S. C., Pacht, J. M., and Rayner, K. (1992). The eﬀect of meaning frequency\non processing lexically ambiguous words: evidence from eye ﬁxations . Psychol.\nSci. 3, 296–300. doi: 10.1111/j.1467-9280.1992.tb00676.x\nShaoul, C., Baayen, R. H., and Westbury, C. F. (2014). N -gram probab ility eﬀects\nin a cloze task. Ment. Lex. 9, 437–472. doi: 10.1075/ml.9.3.04sha\nSmith, N. J., and Levy, R. (2013). The eﬀect of word predictability on reading time\nis logarithmic. Cognition 128, 302–319. doi: 10.1016/j.cognition.2013.02.013\nSnell, J., van Leipsig, S., Grainger, J., and Meeter, M. (2018). O B1-reader: a model\nof word recognition and eye movements in text reading. Psychol. Rev. 125,\n969–984. doi: 10.1037/rev0000119\nSpieler, D. H., and Balota, D. (1997). Bringing computational models\nof word naming down to the item level. Psychol. Sci. 8, 411–416.\ndoi: 10.1111/j.1467-9280.1997.tb00453.x\nStaub, A. (2015). The eﬀect of lexical predictability on eye moveme nts in reading:\ncritical review and theoretical interpretation. Lang. Linguist. Compass 9,\n311–327. doi: 10.1111/lnc3.12151\nStaub, A., Grant, M., Astheimer, L., and Cohen, A. (2015). The i nﬂuence of cloze\nprobability and item constraint on cloze task response time. J. Mem. Lang. 82,\n1–17. doi: 10.1016/j.jml.2015.02.004\nTaylor, W. L. (1953). “Cloze” procedure: A new tool for measuring read ability. J. Q.\n30, 415–433. doi: 10.1177/107769905303000401\nVitu, F., McConkie, G. W., Kerr, P., and O’Regan, J. K. (2001). Fixation location\neﬀects on ﬁxation durations during reading: an inverted optimal vi ewing\nposition eﬀect. Vision Res. 41, 3513–3533. doi: 10.1016/S0042-6989(01)00166-3\nWagenmakers, E.-J., Wetzels, R., Borsboom, D., and van der Maas , H. L. J.\n(2011). Why psychologists must change the way they analyze their data: the\ncase of psi: comment on Bem (2011). J. Pers. Soc. Psychol. 100, 426–432.\ndoi: 10.1037/a0022790\nWang, H., Pomplun, M., Chen, M., Ko, H., and Rayner, K. (2010). Esti mating the\neﬀect of word predictability on eye movements in Chinese reading usi ng latent\nsemantic analysis and transitional probability. Q. J. Exp. Psychol. 63, 37–41.\ndoi: 10.1080/17470210903380814\nWestbury, C. (2016). Pay no attention to that man behind the curt ain. Ment. Lex.\n11, 350–374. doi: 10.1075/ml.11.3.02wes\nWilcox, E. G., Gauthier, J., Hu, J., Qian, P., and Levy, R. (2020 ). On the Predictive\nPower of Neural Language Models for Human Real-Time Comprehen sion\nBehavior. Available online at: https://arxiv.org/abs/2006.01912\nWood, S. N. (2017). Generalized Additive Models: An Introduction\nWith R. Boca Raton, FL: CRC press. doi: 10.1201/978131537\n0279\nWu, Z., Rincon, D., Gu, Q., and Christoﬁdes, P. D. (2021). Stat istical machine\nlearning in model predictive control of nonlinear processes. Mathematics 9,\n1–37. doi: 10.3390/math9161912\nFrontiers in Artiﬁcial Intelligence | www.frontiersin.org 19 February 2022 | Volume 4 | Article 730570\nHofmann et al. Language Models Explain Viewing Times\nConﬂict of Interest: The authors declare that the research\nwas conducted in the absence of any commercial or ﬁnancial\nrelationships that could be construed as a potential conﬂict of\ninterest.\nPublisher’s Note: All claims expressed in this article are solely those\nof the authors and do not necessarily represent those of their aﬃlia ted\norganizations, or those of the publisher, the editors and the re viewers.\nAny product that may be evaluated in this article, or claim that may\nbe made by its manufacturer, is not guaranteed or endorsed by the\npublisher.\nCopyright © 2022 Hofmann, Remus, Biemann, Radach and Kuchinke. Thi s is an\nopen-access article distributed under the terms of the Crea tive Commons Attribution\nLicense (CC BY). The use, distribution or reproduction in ot her forums is permitted,\nprovided the original author(s) and the copyright owner(s) a re credited and that the\noriginal publication in this journal is cited, in accordanc e with accepted academic\npractice. No use, distribution or reproduction is permitte d which does not comply\nwith these terms.\nFrontiers in Artiﬁcial Intelligence | www.frontiersin.org 20 February 2022 | Volume 4 | Article 730570",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7369407415390015
    },
    {
      "name": "Natural language processing",
      "score": 0.6695175766944885
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5624229311943054
    },
    {
      "name": "Sentence",
      "score": 0.5598094463348389
    },
    {
      "name": "Predictability",
      "score": 0.509961724281311
    },
    {
      "name": "Language model",
      "score": 0.5085736513137817
    },
    {
      "name": "Probabilistic logic",
      "score": 0.49161702394485474
    },
    {
      "name": "Word (group theory)",
      "score": 0.473514586687088
    },
    {
      "name": "Context (archaeology)",
      "score": 0.4130088984966278
    },
    {
      "name": "Linguistics",
      "score": 0.2548310160636902
    },
    {
      "name": "Mathematics",
      "score": 0.13301819562911987
    },
    {
      "name": "Statistics",
      "score": 0.12471860647201538
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I167360494",
      "name": "University of Wuppertal",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I159176309",
      "name": "Universität Hamburg",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I4210091510",
      "name": "International Psychoanalytic University Berlin",
      "country": "DE"
    }
  ],
  "cited_by": 31
}