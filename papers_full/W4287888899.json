{
    "title": "LM-CORE: Language Models with Contextually Relevant External Knowledge",
    "url": "https://openalex.org/W4287888899",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4287891205",
            "name": "Jivat Kaur",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2152564817",
            "name": "Sumit Bhatia",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2190566276",
            "name": "Milan Aggarwal",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2898147170",
            "name": "Rachit Bansal",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2293959707",
            "name": "Balaji Krishnamurthy",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2127795553",
        "https://openalex.org/W2962881743",
        "https://openalex.org/W3007672467",
        "https://openalex.org/W2949428332",
        "https://openalex.org/W2962985038",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W3166986030",
        "https://openalex.org/W2561529111",
        "https://openalex.org/W2022166150",
        "https://openalex.org/W3114916066",
        "https://openalex.org/W2998385486",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2963101081",
        "https://openalex.org/W3027879771",
        "https://openalex.org/W3166699508",
        "https://openalex.org/W2080133951",
        "https://openalex.org/W2970641574",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W3121694563",
        "https://openalex.org/W3098267758",
        "https://openalex.org/W2953356739",
        "https://openalex.org/W2951434086",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3171434230",
        "https://openalex.org/W2753329127",
        "https://openalex.org/W2252136820",
        "https://openalex.org/W2951048068",
        "https://openalex.org/W3100283070",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W2970476646",
        "https://openalex.org/W2970986510",
        "https://openalex.org/W3099700870",
        "https://openalex.org/W3175604467",
        "https://openalex.org/W3169283738",
        "https://openalex.org/W3151929433",
        "https://openalex.org/W3102659883"
    ],
    "abstract": "Large transformer-based pre-trained language models have achieved impressive performance on a variety of knowledge-intensive tasks and can capture factual knowledge in their parameters. We argue that storing large amounts of knowledge in the model parameters is sub-optimal given the ever-growing amounts of knowledge and resource requirements. We posit that a more efficient alternative is to provide explicit access to contextually relevant structured knowledge to the model and train it to use that knowledge. We present LM-CORE – a general framework to achieve this– that allows decoupling of the language model training from the external knowledge source and allows the latter to be updated without affecting the already trained model. Experimental results show that LM-CORE, having access to external knowledge, achieves significant and robust outperformance over state-of-the-art knowledge-enhanced language models on knowledge probing tasks; can effectively handle knowledge updates; and performs well on two downstream tasks. We also present a thorough error analysis highlighting the successes and failures of LM-CORE. Our code and model checkpoints are publicly available.",
    "full_text": "Findings of the Association for Computational Linguistics: NAACL 2022, pages 750 - 769\nJuly 10-15, 2022 ©2022 Association for Computational Linguistics\nLM-CORE: Language Models with Contextually Relevant External\nKnowledge\nJivat Neet Kaur∗ α, Sumit Bhatiaβ, Milan Aggarwalβ,\nRachit Bansal∗γ, and Balaji Krishnamurthyβ\nα Microsoft Research, India\nβ Media and Data Science Research Lab, Adobe, India\nγ Delhi Technological University, India\nt-kaurjivat@microsoft.com, sumit.bhatia@adobe.com\nmilaggar@adobe.com, racbansa@gmail.com, kbalaji@adobe.com\nAbstract\nLarge transformer-based pre-trained language\nmodels have achieved impressive performance\non a variety of knowledge-intensive tasks and\ncan capture factual knowledge in their param-\neters. We argue that storing large amounts\nof knowledge in the model parameters is sub-\noptimal given the ever-growing amounts of\nknowledge and resource requirements. We\nposit that a more efficient alternative is to pro-\nvide explicit access to contextually relevant\nstructured knowledge to the model and train it\nto use that knowledge. We present LM-CORE\n– a general framework to achieve this– that al-\nlows decoupling of the language model training\nfrom the external knowledge source and allows\nthe latter to be updated without affecting the\nalready trained model. Experimental results\nshow that LM-CORE, having access to external\nknowledge, achieves significant and robust out-\nperformance over state-of-the-art knowledge-\nenhanced language models on knowledge prob-\ning tasks; can effectively handle knowledge\nupdates; and performs well on two downstream\ntasks. We also present a thorough error anal-\nysis highlighting the successes and failures of\nLM-CORE. Our code and model checkpoints\nare publicly available1.\n1 Introduction\nLarge pre-trained language models (PLMs) (Peters\net al., 2018; Devlin et al., 2019; Brown et al., 2020)\nhave achieved state-of-the-art performance on a va-\nriety of NLP tasks. Much of this success can be\nattributed to the significant semantic and syntactic\ninformation captured in the contextual representa-\ntions learned by PLMs. In addition to applications\nrequiring linguistic knowledge, PLMs have also\n∗Work done as an intern at the Media and Data Science\nResearch Lab, Adobe, India.\n1https://github.com/sumit-research/\nlmcore\nbeen useful for a variety of tasks involving factual\nknowledge and it has been shown that models such\nas BERT (Devlin et al., 2019) and T5 (Raffel et al.,\n2020) store significant world knowledge in their\nparameters (Petroni et al., 2019).\nPLMs are typically fed a large amount of unstruc-\ntured text which leads to the linguistic nuiances and\nworld knowledge being captured in the model pa-\nrameters. This implicit storage of the knowledge\nin the form of the parameter weights not only leads\nto poor interpretability while analyzing model pre-\ndictions but also poses constraints on the amount\nof knowledge that can be stored. It is not practical\nto pack all the ever-evolving world knowledge in\nthe language model parameters due to the great\nfinancial and environmental costs incurred by train-\ning of the PLMs. Further, since the PLMs acquire\nknowledge from the text corpora they are trained\non, they tend to become sensitive to the contextual\nand linguistic variations (Jiang et al., 2020). More-\nover, PLMs do not contain explicit grounding to\nreal world entities, and hence, often find it difficult\nto recall factual knowledge (Logan et al., 2019).\nFor example, the model may not be able to recall\ncorrect information and successfully complete the\nsentence, “The birthplace of Barack Obama is ”,\nif the LM has seen this fact in a different context\nduring training (e.g., “Barack Obama was born in\nHonolulu, Hawaii.”).\nLarge scale structured knowledge bases (KBs)\nsuch as YAGO (Suchanek et al., 2007) and Wiki-\ndata (Vrandeˇci´c and Krötzsch, 2014) offer a rich\nresource of high quality structured knowledge that\ncan provide the PLMs with explicit grounding to\nreal world entities. Consequently, efforts have been\nmade to integrate factual knowledge into PLMs\nand create entity-enhanced language models (Pe-\nters et al., 2019; Zhang et al., 2019; Sun et al., 2020;\nLiu et al., 2020; Wang et al., 2021a,b). However,\n750\n  author                 \n  creator                 \nlocation               \n  publisher                \n occupation               \n nationality               \n                             \n    \n(Batman, creator, Bob Kane)\n(Batman, creator, Bill Finger)\n(Batman, publisher, DC Comics)\n(American University, location, Washington DC)\n(The American Language, author, H. L. Mencken)\n                                    \nBatman is a superhero who appears in American\ncomic books published by DC Comics.\nNER Tagger\nContextual Knowledge\nRetriever\nKnowledge Base\nsample a mask\n randomly\nselect top-k triples using \ninput-triple and input-relation \nsimilarity\nLanguage Model\n[MASK] = DC Comics\n1\n2\n3\n4\n5\nBatman is a superhero who appears in American\ncomic books published by [MASK].\nPERSON NORP\nPre-training \nCorpus\n.\n.\n \nFigure 1: Language Model Pre-Training with Contextually Relevant External Knowledge: 1 Using a sentence\nsampled from the pre-training corpus, an input (x) is created by selecting an entity mention at random from the\npotential mask candidates (underlined in red). 2 An NER tagger is then applied to the masked input sequence (x)\nto identify named entities (underlined in black). 3 For the identified entities, the Knowledge Retrieval module\nfetches the set Tx of all the triples from the Knowledge Base and then 4 scores all the retrieved triples using\ninput-triple and input-relation similarity (details in Section 3.2). 5 The top-k triples are fed to the Language Model\nencoder along with the input sequence (x) and the model is trained to predict the masked token.\nthese works either update the PLM parameters or\nmodify the architecture to facilitate the storage of\nfactual knowledge in the model layers and parame-\nters, making it expensive to update knowledge.\nIn this work, we step back and ask – what if in-\nstead of focusing on storing the knowledge in the\nlanguage model parameters, we provide the model\nwith contextually relevant external knowledge and\ntrain it to use this knowledge? This approach offers\nseveral potential advantages – (i) we can utilize the\nalready available high-quality large-scale knowl-\nedge bases such as YAGO and Wikidata; (ii) not\nall the knowledge needs to be packed in the pa-\nrameters of the model resulting in lighter, smaller\nand greener models; and (iii) as new knowledge\nbecomes available, the knowledge base can be up-\ndated independently of the language model.\nOur Contributions: We present LM-CORE, a\nframework for augmenting language models with\ncontextually relevant external knowledge. The LM-\nCORE framework is summarized in Figure 1 and\nconsists of a contextual knowledge retriever that\nfetches relevant knowledge from an external KB\nand passes it to the language model along with the\ninput text. The language model is then trained with\na modified entity-oriented masked language mod-\neling objective (Section 3). Our proposed solution\nis simple, yet highly effective. Experiments on\nbenchmark knowledge probes show that the pro-\nposed approach leads to significant performance\nimprovements over base language models as well\nas state-of-the-art knowledge enhanced variants\nof the language models (Section 4). We find that\nwith access to contextually relevant external knowl-\nedge, LM-CORE is less sensitive to the contextual\nvariations in input text. We also show how LM-\nCORE can handle knowledge updates without any\nre-training and compare the performance of LM-\nCORE on two knowledge-intensive downstream\ntasks. Finally, we present an in-depth analysis of\ncases where our proposed approach gives incorrect\nanswers paving the way for further research in this\ndirection (Section 4.7).\n2 Related Work\nAugmenting Additional Knowledge in PLMs:\nPrevious works on augmenting PLMs with addi-\ntional knowledge can be grouped into two cate-\ngories. One line of work adopts a retrieve and\nread framework where the model is trained to re-\ntrieve relevant information followed by a reading\ncomprehension step to perform the downstream\ntask (Lee et al., 2019a; Guu et al., 2020; Agarwal\net al., 2021). While our proposal has similarities\nwith this line of work in terms of retrieving the\ncontextual knowledge, there are two major differ-\nences. First, most of these works consider external\nknowledge in the form of unstructured text (such as\n751\nWikipedia documents). However, extracting factual\nknowledge from unstructured text is hard and error-\nprone due to ambiguities in natural language and\ninfrequent mentions of entities of interest (Peters\net al., 2019). This issue can be alleviated by using\na structured knowledge base where the knowledge\nis represented (mostly) unambiguously – each fact\nis a triple in the knowledge base. Further, these\napproaches employ explicit supervision during pre-\ntraining to train the model to fetch relevant pas-\nsages from the text. This results in systems that are\nmore complex and resource-hungry than the base\nPLMs used and also make it difficult to reuse or\nadapt the models to different sources of knowledge.\nThe second body of work has focused on inject-\ning the factual knowledge directly into the model\nparameters by feeding more data to the model\nduring pre-training (Poerner et al., 2020; Roberts\net al., 2020). A promising direction explored re-\ncently is utilizing structured knowledge bases to\naugment Transformer-based LMs. ERNIE (Zhang\net al., 2019) and KnowBERT (Peters et al., 2019)\nare notable efforts in this direction where the en-\ntity information from knowledge bases is explic-\nitly linked with the input text during pre-training\nyielding entity-enhanced variants of BERT models\nwith entity representations integrated within the\nTransformer layers. An alternative way of train-\ning entity-aware language models is illustrated by\nframeworks such as CoLAKE (Sun et al., 2020) and\nKEPLER (Wang et al., 2021b) that jointly learn the\nlanguage and knowledge representations thereby\nproducing language models augmented with factual\nknowledge and knowledge embeddings enhanced\nwith textual context. However, these approaches,\nby design, will lead to larger and larger models to\nstore the ever-growing abundant knowledge. Fur-\nther, due to the strong coupling between the knowl-\nedge and language signals, updating or adding\nknowledge requires re-training of the model.\nExamining the knowledge contained in PLMs:\nPetroni et al. (2019) posit that while training over\nlarge amounts of input text, PLMs may also be\nstoring (implicit) relational knowledge in their pa-\nrameters and proposed the Language Model Analy-\nsis (LAMA) framework to measure the relational\nknowledge stored in a PLM. Jiang et al. (2020) ar-\ngue that due to the sensitivity of the PLMs on the\ninput context, such manually created prompts are\nsub-optimal and might fail to retrieve facts that the\nPLM does know, thus providing only a lower bound\nPerson \n    \nBatman          Human\nBob_Kane \nanchor text for  \n<wikipedia.org/wiki/Bob_Kane>\nIllustrative external knowledge for identified entity Bob_Kane\n  = valid [MASK]\ncreator instanceOf\nisA\nFigure 2: We create our pre-training corpus from\nWikipedia by masking entity spans detected using an-\nchor text of hyperlinks.\nestimate of the knowledge contained in it. Subse-\nquent work (Shin et al., 2020; Zhong et al., 2021)\nhas attempted to generate better prompts in order\nto tighten this estimate. Poerner et al. (2020) intro-\nduced LAMA-UHN (UnHelpfulNames), a much\nharder subset of LAMA where the input probes\nprovide little or no helpful contextual signals from\nother tokens in the probe, thus measuring the innate\nability of the PLM to recall information.\n3 LM-CORE: Knowledge Retrieval and\nTraining Framework\nTask setting and Overview: Consider a language\nmodel L(such as BERT and RoBERTa) and a\nknowledge base K= {thrt =< h, r, t >|h, t∈\nE; r ∈R}. Here, we consider the knowledge base\nKas a set of triples such that each triple thrt rep-\nresents the relationship r between entities h and t.\nEis the set of all the entities, and Ris the set of\nall the relationship types present in the knowledge\nbase. Given a text inputx, the proposed LM-CORE\nframework retrieves a set of triples Tx ∈K such\nthat the triples in Tx are contextually relevant to x.\nThe language model is then presented with the orig-\ninal input x and the contextually relevant knowl-\nedge in the form ofTxand is trained to make predic-\ntions using this additional knowledge. We posit that\nthe model essentially needs to learn relevant seman-\ntic associations between natural language input text\nand various relation types present in the knowledge\nbase. Identifying the correct relation types will\nhelp the model leverage the corresponding relevant\nfacts in order to make an accurate prediction. This\nis accomplished via a modified Masked Language\nModeling (MLM) (Devlin et al., 2019) pre-training\nobjective. Figure 1 summarizes the complete work-\nflow of our proposed LM-CORE framework and\nwe describe the three main components in detail in\nthe following sub-sections.\n752\n3.1 Entity span masking\nMasked Language Modelling is a popular task used\nfor training PLMs where the objective is to predict\nthe masked token in the input sequence. In order to\nimprove model’s grounding to real world entities,\nprevious works have adopted different strategies\nfor explicitly masking entity information in the in-\nput text by using entity representations obtained by\nknowledge base embeddings (Zhang et al., 2019),\nusing named entity recognizers (NER) and regu-\nlar expressions (Guu et al., 2020), and verbaliz-\ning knowledge base triples (Agarwal et al., 2021).\nThese approaches often result in noisy masks due\nto the limitations of underlying rules, and NER\nand entity linking systems. To overcome these\nlimitations, we propose a novel way of creating\nhigh-quality and accurate entity masks by using\nWikipedia as the base corpus for training. Note\nthat in order to create entity masks, we need to\nidentify corresponding entity mentions in the in-\nput text for which we utilize the human-annotated\nlinks in Wikipedia. The official style guidelines\nof Wikipedia require the editors to link mentions\nof topics to their corresponding Wikipedia pages.\nIn Figure 2, the left textbox shows a screenshot of\nthe Wikipedia article about Batman where vari-\nous other related topics, or concepts, are linked to\ntheir corresponding Wikipedia pages (underlined in\nred in the figure, and displayed as blue anchor-text\nin Wikipedia). This information provides us with\nhigh-quality human annotation of entity mentions\nin the input text. As illustrated in Figure 2, the un-\nderlined tokens (such as DC Comics, Bob Kane,\nBill Finger) constitute the set of entity tokens that\ncould be masked. For each such mask, we can\nalso obtain the corresponding contextual knowl-\nedge from the external knowledge base (illustrated\nfor Bob Kanein the right text box). By masking\nonly the entity tokens (instead of randomly sam-\npled words) and providing contextually relevant\nknowledge to the model retrieved from the knowl-\nedge base (as described in next subsection), we\nexpect the model to learn to predict the masked\nentity tokens by utilizing the external knowledge.\n3.2 Contextual Knowledge Retrieval\nAfter preparing the masked input for training, the\nsecond component in our framework fetches con-\ntextually relevant knowledge to feed to the lan-\nguage model.Consider the sentence, “Warren Buf-\nfet is the chairman of [MASK]\", where the masked\ntoken is Berkshire Hathaway. In the typical MLM\nsetting, the model only has access to the linguistic\nand contextual clues present in the input text to\npredict the masked token. However, if contextually\nrelevant information is available as additional input,\nthe model can use it to output the correct token.\nWe consider the problem of finding contextually\nrelevant facts given the input query text as an infor-\nmation retrieval (IR) problem and adopt a retrieve\nand re-rank approach that has empirically been\nfound to perform well in a variety of tasks (Chen\net al., 2017; Wang et al., 2017; Das et al., 2019;\nYang et al., 2019). Recall the example input dis-\ncussed above – “ Warren Buffett is the chairman\nof [MASK]\". Intuitively, in this input text, there\nare two important signals that the retriever needs\nto utilize – entity and relation information. The\nentity mention Warren Buffett indicates that we\nneed to fetch facts related to Warren Buffet from\nthe knowledge base. Typically, there are numer-\nous facts related to a given entity in the knowledge\nbase, especially for popular entities such as War-\nren Buffett. Thus, the retriever also needs to utilize\nthe presence of the word chairman to retrieve facts\n(KB triples) representing the management or exec-\nutive relation.\nGiven an input text, our retriever pipeline per-\nforms Named Entity Recognition (NER) to iden-\ntify named entity mentions in the input text. We\nuse the NER model from FLAIR (Akbik et al.,\n2019) to identify named entity mentions and then\nselect KB entities having maximum overlap with\nthe mention-span of the identified entities. For\ninstance, if the input query is “ Buffett was born\nin [MASK] \", all of the entities containing Buf-\nfett - Warren_Buffett, Howard_Warren_Buffett,\nHoward_Graham_Buffett, Volcano_(Jimmy_\nBuffett_song) etc. are selected, but if the query is\n“Warren Buffett was born in [MASK]\", only the first\ntwo entities will be chosen). Once these entities are\nselected, all the facts from the KB involving these\nentities are retrieved (denoted by Tx in Figure 1).\nAfter retrieving the facts involving the entities\nmentioned in the input, we next need to rank these\ntriples based on their relevance to the input. In or-\nder to measure the contextual relevance of a given\ntriple t to the input x, we compute the following\ntwo scores.\nQuery-Triple similarity: We obtain representa-\ntions of the input text x as well as the triple t and\ncompute the inner product of the representations to\n753\nobtain the similarity score as follows.\nsim(x, t) =Emb(x)TEmb(t), t∈Tx (1)\nHere, Emb(·) is obtained using the Sentence Trans-\nformer (Reimers and Gurevych, 2019). While it is\nstraightforward to obtain representations of inputx,\nsentence transformer can not be applied directly to\nKB triples. Application of KB embeddings such as\nTransE (Bordes et al., 2013) is also not feasible as\nthen the representations of the input text and triples\nwill be in different embedding spaces. To overcome\nthis, we adopt a simple approach of verbalizing the\nknowledge base triples by concatenating the head\nentity, relationship and the tail entity, and obtain\nthe representation of the verbalized triple from the\nsentence transformer. For example, the triple (War-\nren_Buffett, hasOccupation, Investor)is verbal-\nized as Warren Buffett has occupation Investorand\nis fed as input to the sentence transformer.\nRelation-based scoring: A triple is highly rele-\nvant for the input text if the triple represents the\nsame relationship that is being talked about in the\ntext. To capture this intuition, we embed all the\nrelation types in the KB in the same embedding\nspace as triples using the sentence transformer and\ncompute the similarity between the input text and\nthe relation type of the triple as follows.\nsim(x, r) =Emb(x)TEmb(r), r∈R (2)\nwhere Ris the set of all relations in the KB. The fi-\nnal relevance score for the triple t, relevance(x, t)\nis obtained by taking a product of the above two\nscores. Based on this final score, we select the top-\nk triples that constitute the contextual knowledge\nto be fed as input along with x to the LM. We use\nk = 8 in this work (See Appendix 4.3 for effect of\nvarying k). Some illustrative examples of the final\nretrieved knowledge base triples are presented in\nAppendix A.3.\n3.3 Language Model Pre-training with\nContextual Knowledge\nWith the masked training corpus and the module\nto fetch contextually relevant knowledge, we now\ntrain the model to utilize the additional contextual\nknowledge to predict the masked token. From the\nmasked corpus, we select a sentence and a valid\nentity span is chosen at random out of all the po-\ntential spans in the sentence. We mask this span\nto create the input text x. We filter out sentences\nstarting with pronouns such as he, she, her , and\nthey as we observed that most of such sentences\ndo not contain other useful signals to unambigu-\nously predict the masked words. For instance, if\nthe input example is - “ He developed an interest\nin investing in his youth, eventually entering the\nWharton School of the University of Pennsylvania\"\nand Wharton School of the University of Pennsylva-\nnia is masked, the remaining words in the sentence\nare not providing any informative signals to the\nmodel to predict the masked tokens. Given the\ninput sentence thus selected, the contextual knowl-\nedge retriever fetches the relevant triples from the\nknowledge base. The representations of the input\nsentence and the retrieved triples are then concate-\nnated and fed to the model and the model is trained\nto minimize the following MLM loss.\nLMLM = 1\nM\nM∑\nm= 1\nlog p(xindm |x, t1, t2, ..., tk)\n(3)\nwhere M is the total number of [MASK] tokens in\nx and indm is the index of the mth masked token.\nWith the additional contextual information avail-\nable to the model, we expect the model to learn\nthe associations between linguistic cues in the in-\nput text and relevant relationship information in\nthe triples. For example, we expect the model to\nassociate different ways in which someone’s date\nof birth could be mentioned in natural language\n(such as X was born on, the birthday of X is, and\nnumerous other linguistic variations) to the KB rela-\ntion birthDate and utilize the information from the\ncorresponding triple. Note that since the types of\nrelations in the knowledge base are relatively small\nin number, and do not change often, we expect the\nmodel to generalize well and be more robust to\nlinguistic variations.\n4 Experiments and Discussions\nData Sources and Pre-processing: We create our\npre-training corpus using the December 20, 2018\nsnapshot of English Wikipedia that contains about\n5.5M documents. Processing all the articles follow-\ning the masking strategy described in Section 3.1\nresulted in a total of ∼46.3M sentences with valid\nmasks, from which we randomly sample sentences\nto create input examples.\nIn order to illustrate the general nature of\nLM-CORE, we used two different PLMs as our\nLM encoders – BERT-base (uncased) model and\nRoBERTa-base (cased) model. We use Y AGO and\n754\nTable 1: Mean precision at one (P@1) of various models on LAMA probe. We group all the models based on the\nbase language model used (BERT or RoBERTa). For LM-CORE,(·, ·) indicates the variant – (b, r corresponds to\nBERT and RoBERTa, respectively, and y, w indicate YAGO and Wikidata5M, respectively). Best results in each\ncolumn are highlighted in bold and the second best performance is underlined\nComplete Google-RE T-REx SQuAD Concept\nNetDoB PoB PoD All 1-1 N-1 N-M All\nBERT-based models\nBERT-base 24.73 1.59 15.46 10.33 9.12 67.94 32.67 23.54 30.83 14.29 15.88\nBERT-large 25.44 1.59 15.53 12.16 9.76 74.23 31.30 25.30 31.05 17.61 18.72\nERNIE 22.16 1.42 13.48 4.97 6.62 61.51 28.57 21.93 27.58 13.62 14.83\nLM-CORE(b,y) 39.64 64.44 52.71 50.98 56.04 74.37 51.18 34.57 45.83 15.61 14.78\nLM-CORE(b,w) 42.83 0.66 37.62 31.11 23.13 81.79 59.86 45.48 55.32 17.28 16.15\nRoBERTa-based models\nRoBERTa 20.46 1.85 12.98 1.23 5.35 57.49 23.14 21.59 24.21 12.94 18.47\nRoBERTa-large 24.24 1.41 12.48 0.46 4.78 70.24 29.08 23.28 28.82 18.88 22.09\nKEPLER 19.36 1.47 11.73 3.08 5.43 52.32 21.58 21.41 23.01 9.10 17.25\nCoLAKE 23.38 1.79 15.72 10.79 9.43 64.08 29.40 23.54 28.80 8.39 17.17\nLM-CORE(r,y) 34.60 46.33 43.47 26.35 38.71 68.21 45.30 30.40 40.60 13.29 17.53\nLM-CORE(r,w) 41.96 0.38 33.11 28.20 20.56 70.21 60.30 43.18 54.11 15.73 18.38\nWikidata as two different knowledge bases giving\nus four variants of LM-CORE ({bert, roberta}×\n{yago, wikidata}). We use the English Wikipedia\nversion of Y AGO 4 (Suchanek et al., 2007) and pre-\nprocess it to obtain our retrieval corpus consisting\nof roughly 17M triples spanning over 4.9M enti-\nties and 131 unique relations. For Wikidata, we\nused the Wikidata5M version (Wang et al., 2021b)\nthat consists of roughly 21M triples covering 821\nunique relations and 4.8M entities. Further details\nregarding retrieval corpus generation and process-\ning can be found in the Appendix (Section A.2).\nFor computing triple representations for retrieval\n(Section 3.2), we concatenate the subject (head),\nrelation, and object (tail) of triples and embed them\nusing the Sentence Transformers (Reimers and\nGurevych, 2019) and obtain the 768-dimensional\nembeddings (same as LM encoder dimensions).\n4.1 Does External Knowledge Help PLMs in\nKnowledge Intensive Tasks?\nWe now present an analysis of how much, and\nif, having access to external knowledge can help\nPLMs in knowledge-intensive tasks. A popular way\nof assessing a model’s ability to perform at such\ntasks is by using benchmark knowledge probes.\nWe use the LAMA probe (Petroni et al., 2019) that\nprovides a cloze-style sentence representation of\nfacts and the model being evaluated is required to\npredict the masked words in these sentences (e.g.,\nBarack Obama was born in .).\nTable 1 reports the performance of various PLMs\non the LAMA probe as measured by Precision at\n1 (P@1). The numbers in the Table are grouped\nbased on the base language model used by different\nmodels. We use ERNIE (Zhang et al., 2019) (based\non BERT and Wikidata), and KEPLER (Wang et al.,\n2021b) and CoLAKE (Sun et al., 2020) (based\non RoBERTa) as the representative knowledge en-\nhanced language models. Both KEPLER and Co-\nLAKE have used Wikidata5M as the knowledge\nbase. We used author provided code and check-\npoints for obtaining the reported numbers. For LM-\nCORE, we use four variants with different knowl-\nedge base and language encoder combinations as\ndescribed above.\nWe observe that our approach of providing ex-\nternal knowledge to the PLMs leads to substan-\ntially improved performance over the base lan-\nguage models and their SoTA knowledge enhanced\nvariants. LM-CORE(b,w) achieves P@1 of 42.83%\ncompared to 25.44% for BERT-large. Likewise,\nLM-CORE(r,w) achieves a P@1 of 41.96% signif-\nicantly outperforming RoBERTa-large (24.24%).\nWe also report the numbers on the four different\nsubsets of LAMA revealing interesting insights.\nFor all the models considered, we note that the\nperformance on T-REx subset is higher than the\nGoogle-RE subset. We attribute this to the nature\nof knowledge required for probes in the four sub-\nsets. Note especially the column for Date of Birth\n(DoB) in the Table. All the models, except for LM-\nCORE(b,y) and LM-CORE(r,y) perform extremely\npoorly. This is because the Wikidata5M KB does\n755\nTable 2: P@1 for different models on LAMA-UHN.\nLAMA LAMA\nUHN\nPercentage\nChange\nBERT-based models\nBERT-base 24.73 18.72 -24.31\nBERT-large 25.44 19.92 -21.67\nERNIE 22.16 15.81 -28.66\nLM-CORE(b,y) 39.64 41.33 +4.26\nLM-CORE(b,w) 42.83 45.50 +6.23\nRoBERTa-based models\nRoBERTa-base 20.46 13.66 -33.24\nRoBERTa-large 24.24 17.99 -25.78\nKEPLER 19.36 12.46 -35.64\nCoLAKE 23.38 17.16 -13.74\nLM-CORE(r,y) 34.60 34.25 -1.01\nLM-CORE(r,w) 41.96 44.75 +6.65\nnot have date entity type and hence, the poor per-\nformance of models using Wikidata. We also note\nthat on the SQuAD and Concept Net subsets, the\nknowledge enhanced models do not offer signifi-\ncant improvements over the base language models.\nWhile Google-RE and T-REx focus more on factual\nworld knowledge (present in abundance in Y AGO\nand Wikidata), SQuAD and ConceptNet concen-\ntrate more on commonsense knowledge (limited in\nY AGO and Wikidata). This is a major focus of our\ncontinuing work on enhancing the external knowl-\nedge with commonsense knowledge bases such as\nConceptNet (Speer et al., 2017) Atomic (Sap et al.,\n2019).\n4.2 Sensitivity to Contextual Signals in Input\nPLMs are often sensitive to the linguistic variations\nin the input and are overly reliant on the surface\nform of entity names for making its predictions Po-\nerner et al. (2020). For example, BERT can predict\nthat a person with an Italian-sounding name was\nborn in Italy even if this is factually incorrect. In or-\nder to evaluate the sensitivity and robustness of dif-\nferent models, we report the P@1 numbers for the\nLAMA-UHN (UnHelpfulNames) probing bench-\nmark (Table 2) – a much harder subset of LAMA\nwhere input probes with helpful entity names are\nremoved and the PLM has little or no helpful con-\ntextual signals from other tokens in the probe. We\nobserve that the LM-CORE variants significantly\noutperform the base language models and their\nknowledge enhanced variants. Further, note that\nwhile all the baseline models suffer a significant\nfall in performance (expected due to the hardness\nof LAMA-UHN), the drop in performance of LM-\nCORE variants is much less. This indicates that\n0 2 4 6 8 10\n# Input Triples (K)\n10\n20\n30\n40\n50\n60Precision @ 1\nT-REx(b,y) \nGoogle-RE(b,y) \nGoogle-RE(r,w) \nT-REx(r,w) \nFigure 3: Effect of k on performance of different mod-\nels.\nhaving access to relevant external knowledge helps\nreduce the dependence on linguistic signals and\nresults in the robust outperformance of LM-CORE\nvariants.\n4.3 Effect of Varying Number of Input Triples\nto LM-CORE\nWe analyze the effect of varying the number of can-\ndidates (k) during retrieval in Figure 3. We discuss\nwith respect to Google-RE and T-REx subsets as\nour factual knowledge triples are most relevant for\nanswering queries in these subsets (in comparison\nto commonsense queries in ConceptNet).\nWe plot the Precision@1 (P@1) against increas-\ning k values from 1 to 10 for LM-CORE(b,y) and\nLM-CORE(r,w) variants. We do not observe any\nconsistent optimal k value across variants and data\nsubsets. To add, there is no significant difference\nbetween P@1 values as k varies from 4 to 10.\nHence, in order to maximize our recall while keep-\ning the computational expense in mind, we select\nk = 8for our experiments.\n4.4 Role of LM-CORE Pre-training and\nRetrieved Knowledge\nWe now study the role LM-CORE pre-training\nplays in helping the model access and utilize the re-\ntrieved knowledge and ensure that the model does\nnot just rely on the knowledge stored in its param-\neters. We also study the effect of augmenting the\nbase LMs with knowledge retrieved by LM-CORE.\nIn addition to providing an insight into the qual-\nity of the knowledge retrieved by LM-CORE, this\nwill also help us better understand the ability of\nLM-CORE to utilize the retrieved knowledge.\nWe consider the following four variants on the\nLAMA probe (Table 3):\n1. RoBERTa-base\n756\nTable 3: Ablation study analyzing the effectiveness\nof LM-CORE pre-training and contextually retrieved\nknowledge (Precision@1 values).\nAll Google\nRE T-REx SQuAD Concept\nNet\nRoBERTa\n(base) 20.46 5.35 24.21 12.94 18.47\nRoBERTa-base\n+LM-CORE triples30.06 9.79 38.71 7.69 16.26\nLM-CORE(r,w)+\nrandom triples 19.51 9.05 22.74 13.99 16.92\nLM-CORE(r,w) 41.69 20.56 54.11 15.73 18.38\n2. RoBERTa-base + triples retrieved by LM-\nCORE’s Contextual Knowledge Retriever\n3. LM-CORE(r,w) + random triples\n4. LM-CORE(r,w)\nWe observe that LM-CORE(r,w)’s performance\n(41.69 P@1) significantly exceeds RoBERTa-\nbase’s performance using the same triples in input\n(30.06 P@1) , demonstrating that our training pro-\ncedure equips the model with the capability of iden-\ntifying and using relevant external knowledge effec-\ntively. There is a large drop in performance (from\n41.69 P@1 to 19.51 P@1) when LM-CORE(r,w) is\nprovided with random triples in input. This shows\nthat the model exclusively accesses external knowl-\nedge to answer queries correctly. While the perfor-\nmance drops, it is important to note that the P@1\nis similar to RoBERTa-base (20.46 P@1), high-\nlighting that our training procedure does not lead to\ncatastrophic forgettingand the model is able to rely\non the knowledge stored in its parameters when se-\nmantically relevant triples are not provided in the\ninput. Finally, although RoBERTa-base when aug-\nmented with contextually relevant triples does not\nperform competitively with LM-CORE, it demon-\nstrates considerable improvement over the base\nRoBERTa model. This shows that high-quality\nrelevant external knowledge has the potential to\nimprove factual prediction, further reinforcing our\nmotivation to train models to efficiently retrieve\nand use this knowledge.\n4.5 Downstream Tasks\nWe consider two downstream tasks to study the\neffectiveness of LM-CORE for different NLP ap-\nplications. We take Zero-Shot Relation Extrac-\ntion (ZSRE) (Levy et al., 2017) and open-domain\nquestion answering over Web Questions (WQ) (Be-\nrant et al., 2013) dataset as the representative\nknowledge-intensive tasks. Tables 4 and 5 re-\nport the performance of LM-CORE and various\nother baselines for the two tasks, respectively. We\nuse the LM-CORE(b,w) variant for these experi-\nments as most baselines use BERT as the LM and\nWikipedia as the knowledge base. For the ZSRE\ntask, we use the data splits and evaluation systems\nprovided as part of the KILT benchmark (Petroni\net al., 2021). We find that for the ZSRE task, LM-\nCORE achieves a significantly higher F-1 score\n(74.80) compared to the second-best RAG model\n(49.95). Also, note that the online evaluator for the\ntask considers exact string match (including casing,\npunctuations, etc.) for computing accuracy num-\nbers but not for computing other metrics. Hence,\nthe reported accuracy number for LM-CORE rep-\nresent a lower bound as we don’t have access to the\nsame pre-processing pipeline to process its output.\nFor the WQ dataset, we find that LM-CORE out-\nperforms BERT with BM25 and neural retrievers,\nand the DrQA system. We observe that LM-CORE\nis outperformed by ORQA, designed explicitly for\nthis task, and RAG (a retrieval augmented gener-\native model). However, do note that all the mod-\nels except LM-CORE have access to much larger\nknowledge source (complete Wikipedia corpus. ≈\n2B words), whereas LM-CORE only has access to\nthe KB triples (21M triples, ≈140M words). As\nwe show in the following subsection, with access\nto additional external knowledge, the performance\nof LM-CORE can improve significantly.\nTable 4: F1 and Accuracy on Zero Shot RE. ∗ The\naccuracy for LM-CORE is the lower-bound number\nas the online evaluator considers exact string match to\ncompute accuracy.\n#params F1 Accuracy\nBERT+DPR (Karpukhin et al., 2020)330M 37.28 6.93\nT5 (base) 220M 13.52 9.02\nBART (large) 406M 12.21 9.14\nBART+DPR 626M 34.47 30.43\nRAG (Lewis et al., 2020) 626M 49.95 44.74\nLM-CORE(b,w) 110M 74.80 14.24∗\n4.6 Handling Knowledge Updates\nOnce PLMs have been trained, it is expensive to\nretrain them with new and updated knowledge. LM-\nCORE, on the other hand, can easily access updated\nknowledge as it is external to the model. Instead of\nstoring all the KB facts in the PLM, LM-CORE es-\nsentially learns semantically relevant associations\nbetween the input text and KB relations and can\neasily query for the relevant knowledge by using\nthe learned relationship associations. We illustrate\nthis ability to handle dynamic knowledge by in-\n757\nTable 5: Accuracy on Web Questions. BM25, Neur. Re-\ntriever, DRQA and LM-CORE perform static retrieval.\nAccuracy\nBM25 + BERT 17.7\nNeur. Retriever + BERT 7.3\nDrQA (Chen et al., 2017) 20.7\nLM-CORE (b,w) 21.9\nORQA (Lee et al., 2019b) 36.4\nRAG (Lewis et al., 2020) 45.5\ntroducing new triples in the KB and verifying if\nLM-CORE is able to leverage this new information\nto correct its earlier predictions. We consider the\nLM-CORE(b,y) variant for this experiment. We\nrandomly sample 100 instances from the LAMA\nprobe where the model failed and manually analyze\nthese instances to identify the cases where the cor-\nresponding fact was not present in the Y AGO KB.\nThere were a total of of 41 such instances and we\nmanually added the correct facts needed to answer\nthe corresponding questions in YAGO. We then\npresented the 41 inputs again to the model with the\nupdated KB. This time, the model used this newly\nadded knowledge and was able to correct its predic-\ntion without any re-training for 36 out of 41 cases\n(87.8%). As discussed in the following sub-section\n(4.7), a majority of errors made by LM-CORE are\ndue to missing facts in the KB and we expect that\nmost of such errors can be corrected by having ac-\ncess to a larger, more comprehensive Knowledge\nBase.\n4.7 Discussions\nWe now present some representative examples to\nillustrate the successes and failures of LM-CORE.\nConsider a test probe from the Google-RE sub-\nset of LAMA – Phil Mogg is a member of .\nHere, the correct output token is UFO, the band\nand BERT model incorrectly predictsparliament as\nthe output token. This highlights the sensitivity of\nPLMs on context; BERT’s prediction seems to be\nderived from its memorization of the frequently en-\ncountered phrase member of parliament during pre-\ntraining. We argue that the contextual knowledge\nretrieved by LM-CORE which includes the rele-\nvant fact <Phil Mogg; member of; UFO (band)>\nhas helped the model to produce the correct output.\nWe present more such successful examples in the\nAppendix (Tables 10 and 11).\nNext, we analyzed the cases where the proposed\nframework produced incorrect output and observed\nthree major reasons for erros – (i) the required\nknowledge was not present in the knowledge base;\n(ii) the required knowledge was not retrieved de-\nspite being present in the knowledge base; and (iii)\nthe system made errors after retrieving the rele-\nvant knowledge. The first problem cause could\nbe addressed by enhancing the knowledge base as\nshown in Section 4.6. The other two causes of\nfailure highlight the scope of improvement in our\nretrieval module as well as pre-training module,\nwhere further training could help the model make\nbetter use of the retrieved knowledge. Some rep-\nresentative examples of these different cases are\npresented in the Appendix (Table 12). Finally, we\nnoticed some errors that could be attributed to the\ncharacteristics of the LAMA probe. Specifically,\nthere are input probes that refer to entities without\nproviding any additional context for disambigua-\ntion. For example, the sentence “ James Johnson\nwas born in \" has no clues to determine whether\nthe prompt is referring to the basketball player, Vir-\nginia congressman, or the Governor of Georgia\nwith this name. We also noticed certain probes\nwhere there are multiple correct completions and\nthe benchmark considers only one of these as the\ncorrect answer. For example, “Michelangelo is a\nby profession\" can be correctly completed by\npoet, painter or architect, but the evaluation con-\nsiders only poet as the correct answer. We also\nnoticed some input examples with highly unam-\nbiguous language. For example, “ X died in \",\ncan refer to either X’s place of death or date of\ndeath but only the former is accepted as the cor-\nrect answer. Lastly, there are cases where slight\n(and correct) variations of the expected answer are\nevaluated as incorrect by the probe. For example,\nfor the prompt “Harashima is citizen.\" Japan\nis provided as the correct answer while the predic-\ntion made by LM-CORE (Japanese) is considered\nincorrect.\n5 Conclusion\nWe presented LM-CORE, a framework to train lan-\nguage models with contextually relevant external\nknowledge. We show that having access to exter-\nnal knowledge leads to significant and robust out-\nperformance over base language models and their\nknowledge enhanced versions on knowledge prob-\ning and two downstream tasks. We also showed\nhow LM-CORE can handle knowledge updates and\npresented a thorough error analysis that helped us\nidentify possible directions of future work.\n758\nReferences\nOshin Agarwal, Heming Ge, Siamak Shakeri, and Rami\nAl-Rfou. 2021. Knowledge graph based synthetic\ncorpus generation for knowledge-enhanced language\nmodel pre-training. In Proceedings of the 2021 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 3554–3565, Online. As-\nsociation for Computational Linguistics.\nAlan Akbik, Tanja Bergmann, Duncan Blythe, Kashif\nRasul, Stefan Schweter, and Roland V ollgraf. 2019.\nFlair: An easy-to-use framework for state-of-the-art\nnlp. In NAACL 2019, 2019 Annual Conference of\nthe North American Chapter of the Association for\nComputational Linguistics (Demonstrations), pages\n54–59.\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on freebase from\nquestion-answer pairs. In EMNLP, pages 1533–1544.\nACL.\nAntoine Bordes, Nicolas Usunier, Alberto Garcia-\nDurán, Jason Weston, and Oksana Yakhnenko.\n2013. Translating embeddings for modeling multi-\nrelational data. In Proceedings of the 26th Interna-\ntional Conference on Neural Information Processing\nSystems - Volume 2, NIPS’13, page 2787–2795, Red\nHook, NY , USA. Curran Associates Inc.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading Wikipedia to answer open-\ndomain questions. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1870–1879,\nVancouver, Canada. Association for Computational\nLinguistics.\nRajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer,\nand Andrew McCallum. 2019. Multi-step retriever-\nreader interaction for scalable open-domain question\nanswering. In ICLR.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,\nand Ming-Wei Chang. 2020. REALM: Retrieval-\naugmented language model pre-training. arXiv\npreprint arXiv:2002.08909.\nZhengbao Jiang, Frank F Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics, 8:423–438.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6769–6781,\nOnline. Association for Computational Linguistics.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n2019a. Latent retrieval for weakly supervised open\ndomain question answering. In Proceedings of the\n57th Annual Meeting of the Association for Computa-\ntional Linguistics, pages 6086–6096, Florence, Italy.\nAssociation for Computational Linguistics.\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n2019b. Latent retrieval for weakly supervised open\ndomain question answering. In Proceedings of the\n57th Conference of the Association for Computa-\ntional Linguistics, ACL 2019, Florence, Italy, July\n28- August 2, 2019, Volume 1: Long Papers, pages\n6086–6096. Association for Computational Linguis-\ntics.\nOmer Levy, Minjoon Seo, Eunsol Choi, and Luke\nZettlemoyer. 2017. Zero-shot relation extraction via\nreading comprehension. In Proceedings of the 21st\nConference on Computational Natural Language\nLearning (CoNLL 2017), pages 333–342, Vancouver,\nCanada. Association for Computational Linguistics.\nPatrick S. H. Lewis, Ethan Perez, Aleksandra Piktus,\nFabio Petroni, Vladimir Karpukhin, Naman Goyal,\nHeinrich Küttler, Mike Lewis, Wen tau Yih, Tim\nRocktäschel, Sebastian Riedel 0001, and Douwe\nKiela. 2020. Retrieval-augmented generation for\nknowledge-intensive nlp tasks. In Advances in Neu-\nral Information Processing Systems 33: Annual Con-\nference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual.\nWeijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju,\nHaotang Deng, and Ping Wang. 2020. K-BERT: en-\nabling language representation with knowledge graph.\n759\nIn The Thirty-Fourth AAAI Conference on Artificial\nIntelligence, AAAI 2020, The Thirty-Second Innova-\ntive Applications of Artificial Intelligence Conference,\nIAAI 2020, The Tenth AAAI Symposium on Educa-\ntional Advances in Artificial Intelligence, EAAI 2020,\nNew York, NY, USA, February 7-12, 2020 , pages\n2901–2908. AAAI Press.\nRobert Logan, Nelson F. Liu, Matthew E. Peters, Matt\nGardner, and Sameer Singh. 2019. Barack’s wife\nhillary: Using knowledge graphs for fact-aware lan-\nguage modeling. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 5962–5971, Florence, Italy. Associa-\ntion for Computational Linguistics.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, NAACL-HLT 2018, New Or-\nleans, Louisiana, USA, June 1-6, 2018, Volume 1\n(Long Papers), pages 2227–2237. Association for\nComputational Linguistics.\nMatthew E. Peters, Mark Neumann, Robert L Lo-\ngan, Roy Schwartz, Vidur Joshi, Sameer Singh, and\nNoah A. Smith. 2019. Knowledge enhanced contex-\ntual word representations. In EMNLP.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nS. H. Lewis, Majid Yazdani, Nicola De Cao, James\nThorne, Yacine Jernite, Vladimir Karpukhin, Jean\nMaillard, Vassilis Plachouras, Tim Rocktäschel, and\nSebastian Riedel 0001. 2021. Kilt: a benchmark for\nknowledge intensive language tasks. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, NAACL-HLT 2021,\nOnline, June 6-11, 2021, pages 2523–2544. Associa-\ntion for Computational Linguistics.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 2463–2473, Hong Kong, China. Association\nfor Computational Linguistics.\nNina Poerner, Ulli Waltinger, and Hinrich Schütze. 2020.\nE-BERT: Efficient-yet-effective entity embeddings\nfor BERT. In Findings of the Association for Compu-\ntational Linguistics: EMNLP 2020, pages 803–818,\nOnline. Association for Computational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert:\nSentence embeddings using siamese bert-networks.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing. Associa-\ntion for Computational Linguistics.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 5418–5426,\nOnline. Association for Computational Linguistics.\nMaarten Sap, Ronan Le Bras, Emily Allaway, Chan-\ndra Bhagavatula, Nicholas Lourie, Hannah Rashkin,\nBrendan Roof, Noah A Smith, and Yejin Choi. 2019.\nAtomic: An atlas of machine commonsense for if-\nthen reasoning. In Proceedings of the AAAI Con-\nference on Artificial Intelligence, volume 33, pages\n3027–3035.\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV , Eric\nWallace, and Sameer Singh. 2020. AutoPrompt: Elic-\niting Knowledge from Language Models with Auto-\nmatically Generated Prompts. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 4222–4235,\nOnline. Association for Computational Linguistics.\nRobyn Speer, Joshua Chin, and Catherine Havasi. 2017.\nConceptnet 5.5: An open multilingual graph of gen-\neral knowledge. In Proceedings of the AAAI Confer-\nence on Artificial Intelligence, volume 31.\nFabian M. Suchanek, Gjergji Kasneci, and Gerhard\nWeikum. 2007. Yago: A core of semantic knowledge.\nIn Proceedings of the 16th International Conference\non World Wide Web, WWW ’07, page 697–706, New\nYork, NY , USA. Association for Computing Machin-\nery.\nTianxiang Sun, Yunfan Shao, Xipeng Qiu, Qipeng Guo,\nYaru Hu, Xuanjing Huang, and Zheng Zhang. 2020.\nCoLAKE: Contextualized language and knowledge\nembedding. In Proceedings of the 28th International\nConference on Computational Linguistics , pages\n3660–3670, Barcelona, Spain (Online). International\nCommittee on Computational Linguistics.\nDenny Vrandeˇci´c and Markus Krötzsch. 2014. Wiki-\ndata: A free collaborative knowledgebase. Commun.\nACM, 57(10):78–85.\nRuize Wang, Duyu Tang, Nan Duan, Zhongyu Wei,\nXuanjing Huang, Jianshu Ji, Guihong Cao, Daxin\nJiang, and Ming Zhou. 2021a. K-adapter: Infusing\nknowledge into pre-trained models with adapters. In\nFindings of the Association for Computational Lin-\nguistics: ACL/IJCNLP 2021, Online Event, August\n1-6, 2021, volume ACL/IJCNLP 2021 of Findings\nof ACL, pages 1405–1418. Association for Computa-\ntional Linguistics.\nShuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang,\nTim Klinger, Wei Zhang, Shiyu Chang, Gerald\n760\nTesauro, Bowen Zhou, and Jing Jiang. 2017. R 3:\nReinforced reader-ranker for open-domain question\nanswering.\nXiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan\nZhang, Zhiyuan Liu, Juanzi Li, and Jian Tang. 2021b.\nKEPLER: A Unified Model for Knowledge Em-\nbedding and Pre-trained Language Representation.\nTransactions of the Association for Computational\nLinguistics, 9:176–194.\nWei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen\nTan, Kun Xiong, Ming Li, and Jimmy Lin. 2019.\nEnd-to-end open-domain question answering with\nBERTserini. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Associa-\ntion for Computational Linguistics (Demonstrations),\npages 72–77, Minneapolis, Minnesota. Association\nfor Computational Linguistics.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019. ERNIE: En-\nhanced language representation with informative en-\ntities. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, pages\n1441–1451, Florence, Italy. Association for Compu-\ntational Linguistics.\nZexuan Zhong, Dan Friedman, and Danqi Chen. 2021.\nFactual probing is [MASK]: Learning vs. learning\nto recall. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 5017–5033, Online. Association\nfor Computational Linguistics.\n761\nAppendix\nA LM-CORE Training Details\nWe used the Hugging Face Transformer 2 mod-\nels BERT and RoBERTa as base models for\nLM-CORE pre-training. We use the BASE\n(12-layer, 768-hidden, 12-heads) size models\nand initialize from bert-base-uncased and\nroberta.base parameters, respectively. This\nis consistent with the initialization in Peters et al.\n(2019); Zhang et al. (2019); Wang et al. (2021b)\nand is a common practice to save up on pre-training\ntime.\nWe used the Adam (Kingma and Ba, 2015) opti-\nmizer and a learning rate of 3e-5 across all settings.\nWe could not perform a lot of hyperparameter tun-\ning owing to the computational requirements of\nthe task. Pre-training was done using 8 Nvidia\nA100 GPUs with a batch size of 512 using gradient\naccumulation. The masked LM loss continued to\ndecrease at the end of pre-training, suggesting fur-\nther improvement in performance can be expected.\nThe pre-trained checkpoints for all four variants of\nLM-CORE can be found here.\nA.1 Pre-Training corpus\nWe use the English Wikipedia (December 20,\n2018) snapshot 3 to create our pre-training cor-\npus and WikiExtractor 4 to process the dumps.\nThis Wikipedia version contains about 5.5M doc-\numents. We retain the hyperlinks while extracting\nWikipedia articles as we use them for creating en-\ntity masks (Section 3.1). Following the entity mask-\ning strategy described in Section 3.1, we obtain our\npre-training corpus which contains ∼46.3M sen-\ntences in total.\nDuring pre-training the base LMs, we sample\nsentences containing valid masks. The pre-training\ncorpus is maintained consistent across all LM-\nCORE variants.\nA.2 Retrieval corpus\nWe use two popular knowledge bases (KBs) in\nLM-CORE - Y AGO 4 (Suchanek et al., 2007) and\nWikidata5M (Wang et al., 2021b). The statistics of\nthe KBs – number of facts, entities and relations\n2https://github.com/huggingface/\ntransformers\n3https://archive.org/\ndownload/enwiki-20181220/\nenwiki-20181220-pages-articles.xml.bz2\n4https://github.com/attardi/\nwikiextractor\ncan be found in Table 6. We describe the prepro-\ncessing steps followed to obtain the respective final\nretrieval corpora in the following subsections.\nA.2.1 YAGO\nYAGO 45 is in RDFS format. YAGO facts are\nderived from Wikidata, however, all the entities are\narranged in a taxonomy mapped to schema.org.\nWe pre-process Y AGO to remove triples involv-\ning relationships such as image, logo and url that\npoint to meta-data such as images and other files.\nWe also filter out triples that point to RDF literals\nor Wikidata URLs.\nA.2.2 Wikidata5M\nWe use the Wikidata5M subset of Wikidata as made\navailable by Wang et al. (2021b)6. This subset of\nWikidata is aligned with Wikipedia such that each\nentity in Wikidata5M has a corresponding entry in\nWikipedia. We used the raw graph as provided in\nthe dataset, the statistics of which are reported in\nTable 6.\nTable 6: Statistics of the knowledge bases used for re-\ntrieval in terms of number of triples, entities and rela-\ntions\nFacts Entities Relations\nYago 17,421,942 4,927,897 131\nWikidata5M 21,285,880 4,797,808 821\nA.3 Example Retrieved Triples\nWe provide a closer look into our pre-training ap-\nproach by showing examples of masked input sen-\ntences and the retrieved triple candidates from the\nknowledge base (Table 7). We observe that the\nfacts retrieved are highly relevant for predicting the\nmasked entities in the input context.\nB Additional Experiments\nB.1 How LM-CORE Compares with Other\nRetrieval Paradigms\nTable 8 also reports results of REALM (Guu\net al., 2020) – a retrieval-based language model\nthat retrieves relevant documents from a text cor-\npus during pre-training. We observe that LM-\nCORE outperforms REALM on the ConceptNet,\n5https://yago-knowledge.org/downloads/\nyago-4\n6https://deepgraphlearning.github.io/\nproject/wikidata5m\n762\nInput Masked Token Candidates (correct fact in bold)\nHenri Jules Louis Marie Rendu\n(24 July 1844 – 16 April 1902) was\na French physician born in [MASK].\nParis\n(Henri Jules Louis Marie Rendu; birth date; 1844-07-24)\n(Henri Jules Louis Marie Rendu; birth place; Paris)\n(Henri Jules Louis Marie Rendu; death date; 1902-04-16)\n(Henri Jules Louis Marie Rendu; nationalit;y France)\n(Henri Jules Louis Marie Rendu; given name; Henri)\nWeisenborn attended the [MASK].University\nof Chicago\n(Gordon Weisenborn; alumni of; University of Chicago)\n(Clara Weisenborn; member of; Republican Party (United States)\n(Günther Weisenborn; nationality; Germany)\n(Günther Weisenborn; death place; West Berlin)\n(Clara Weisenborn; nationality; United States)\nDehorokkhi (English: Bodyguard)\nis a Bangladeshi [MASK] directed\nby Iftakar Chowdhury.\naction film\n(Dehorokkhi; director; Iftakar Chowdhury)\n(Dehorokkhi; in language; Bengali language)\n(Dehorokkhi; genre; Action film)\n(Bangladeshi Idol; in language; Bengali language)\n(British Bangladeshi Who’s Who; in language; English language)\nPalaemon macrodactylus is\na [MASK] of shrimp of the\nfamily Palaemonidae.\nspecies\n(Palaemon macrodactylus; parent taxon Palaemon (genus))\n(Palaemon macrodactylus; parent taxon; Palaemon (genus))\n(Palaemon macrodactylus; taxonomic rank; Species)\n(Palaemonidae; taxonomic rank; Family (biology))\n(Palaemonidae; parent taxon; Palaemonoidea)\nTable 7: Examples of masked input sentences (from Wikipedia) and top-5 retrieved candidates during pre-training.\nDoB (Google-RE), and 1-1 (T-REx) subsets, while\nREALM outperforms the proposed solution in\nother subsets of the LAMA probe. We specifically\nhighlight an absolute 15 points improvement on\nthe date-of-birth relation despite REALM\nusing explicit date masks while training whereas\nour training corpus only has entity masks. This\nindicates that our model can use the the contextual\nknowledge provided by the retriever module even\nthough it is not explicitly shown such knowledge\nduring training.\nNote that while REALM is similar to our pro-\nposed solution as far as the idea of retrieving rel-\nevant knowledge is concerned, the key difference\nin the two approaches lies in the source of knowl-\nedge being used. REALM relies on an unstructured\ntext corpus (Wikipedia) as the source of knowledge\nand employs a computationally complex retrieve\nand read paradigm requiring additional training\nof the knowledge retriever model. Our proposed\nsolution, on the other hand, uses structured knowl-\nedge which offers the advantage of being (almost)\nunambiguous and less resource-hungry compared\nto unstructured text. We present the resource re-\nquirements of our approach and REALM in Table\n9. Note that the size of the external knowledge\n(in number of words) used by REALM is an order\nof magnitude greater, and requires three times the\nnumber of parameters compared to our model. Fur-\nthermore, REALM was trained for 200K steps with\na batch size of 512 on an 80 TPU cluster, whereas\nour proposed solution is much more efficient being\ntrained for 1K steps with a batch size of 512 on a\nmachine with 8 Nvidia A100 GPUs. This compu-\ntational efficiency of our proposed solution allows\nus to continue further work on improving our per-\nformance by enhancing the structured knowledge\nbase and bridge the performance gap with more\ncomplex and computationally expensive models\nsuch as REALM.\nC LAMA Evaluation\nWe use the official LAMA data code7 for evaluat-\ning P@1 numbers in Table 1. All the BERT-based\nmodels are evaluated using this repository. The\nLAMA code provides functionality for evaluating\nRoBERTa models trained in the fairseq framework.\nHence, we evaluate RoBERTa-base, RoBERTa-\nlarge and KEPLER (Wang et al., 2021b) using this\ncode. The KEPLER repo also points to this code\nfor evaluation. CoLAKE (Sun et al., 2020), has\nadapted the official code 8 to allow huggingface\ntransformer checkpoints as input, and hence this\ncode is used for CoLAKE, LM-CORE(r,y) and\nLM-CORE(r,w) evaluation. We ensure the model\n7https://github.com/facebookresearch/\nLAMA\n8https://github.com/txsun1997/CoLAKE\n763\nTable 8: Precision at one (P@1) on LAMA probe. We consider only LM-CORE(b,y) and LM-CORE(b,w) as\nREALM is trained on BERT. Best results in each column are highlighted in bold and the second best performance is\nunderlined.\nGoogle-RE T-REx SQuAD Concept\nNetDoB PoB PoD All 1-1 N-1 N-M All\nBERT-base 1.59 15.46 10.33 9.12 67.94 32.67 23.54 30.83 14.29 15.88\nBERT-large 1.59 15.53 12.16 9.76 74.23 31.30 25.30 31.05 17.61 18.72\nLM-CORE(b,y) 64.44 52.71 50.98 56.04 74.37 51.18 34.57 45.83 15.61 14.78\nLM-CORE(b,w) 0.66 37.62 31.11 23.13 81.79 59.86 45.48 55.32 17.28 16.15\nREALM 49.06 79.56 64.13 67.36 55.81 69.54 66.98 68.18 27.96 4.78\nModel no. of Retrieval Resources\nparams corpus size used\nLM-CORE(b,y) 110M 17M KB triples 8 GPUs\n(∼100M words)\nLM-CORE(b,w) 110M 21M KB triples 8 GPUs\n(∼140M words)\nREALM 330M 5.5M documents 80 TPUs\n(∼2B words)\nTable 9: Resource requirements of LM-CORE and\nREALM. REALM requires additional ICT pre-training\nover all Wikipedia documents for initialization.\nvocabularies and data is consistent across evalua-\ntion. We have used author provided/recommended\ncode and publicly available checkpoints from the\nofficial code repositories for all baselines.\nC.1 LAMA: Qualitative Analysis\nTable 10 and 11 show examples spanning different\nrelationships in LAMA where LM-CORE(b,y) and\nLM-CORE(r,w) are able to make correct predic-\ntions. We also compare the predictions with BERT-\nbase and RoBERTa-base respectively and highlight\nhow these PLMs struggle to make knowledgeable\npredictions.\nC.2 LAMA: Error Analysis\nWe present various failure cases for LM-CORE in\nTable 12. These are representative of the type of\nerrors we encountered, however, we observed that\nmajority of the errors resulted due to correct facts\nmissing from the KB.\nC.3 Complete LAMA-UHN results\nThe complete LAMA-UHN results over all subsets\nof Google-RE and T-REx can be found in Table 13.\nD Downstream Evaluation\nWe discuss the experimental setup and hyperparam-\neter settings for our downstream tasks.\nD.1 Zero Shot RE\nWe consider the open domain version of Zero Short\nRE (Levy et al., 2017) from Petroni et al. (2021).\nThe dataset is split into three disjoint sets – train\n(147,909 samples, 84 relations), dev (3,724 sam-\nples, 12 relations) and test (4,966 samples, 24 re-\nlations). The systems are evaluated on relations\nnever seen during training.\nWe fine tune our model for 2 epochs with a batch\nsize of 96. We use the Adam (Kingma and Ba,\n2015) optimizer and a learning rate of 3e-5. We\nperformed multiple trials by tuning the number of\nepochs in {1, 2, 5}.\nD.2 Web Questions\nWeb Questions (Berant et al., 2013) was created\nusing questions that were sampled from the Google\nSuggest API. We used the same splits as Lee et al.\n(2019b) with training, dev and test sets containing\n3417, 361 and 2032 samples respectively.\nWe fine tuned our model for 20 epochs – we\nexperimented with number of epochs in {10, 20,\n30, 50}. We use the Adam (Kingma and Ba, 2015)\noptimizer and a learning rate of 3e-5.\nE Risks Statement\nThis work considers training of large language mod-\nels using large textual corpora as well as structured\nknowledge bases. The model learns the nuances\nof the language and correlations between differ-\nent real-world entities based on the data that is\nbeing used for training the model. Hence, there is\na chance that the biases and noise in the training\ndata will creep into the model parameters as well\nthat can lead to a biased model behavior. We need\n764\nto be careful in deploying the model and extrapo-\nlating the output of the model in applications such\nas search, conversational systems and recommen-\ndation systems where model’s inherent biases can\nlead to catastrophic impacts on the user.\n765\nRelation Input query LM-CORE(b,y) BERT-baseCandidatesprediction prediction\nGoogle-RE\nbirth-place Stanley Corrsin was born\nin .\nPhiladelphia London (Stanley Corrsin birth date 1920-04-03)\n(Stanley Corrsin nationality United States)\n(Stanley Corrsin birth place Philadelphia)\n(Stanley Corrsin given name Stanley (given name))\n(Stanley Corrsin death date 1986-06-02)\n(Stanley Corrsin has occupation Physicist)\n(Stanley Corrsin alumni of University of Pennsylvania)\n(Stanley Corrsin member of\nAmerican Academy of Arts and Sciences)\nbirth-date Tom Coppola (born). 1945 1975 (Tom Coppola birth date 1945-06-06)\n(Tom Coppola given name Tom (given name))\n(Tom Coppola nationality United States)\n(Tom Coppola family name Coppola (surname))\n(Tom Coppola alumni of USC Thornton School of Music)\n(Christopher Coppola birth date 1962-01-25)\n(Anton Coppola nationality United States)\n(Chris Coppola birth date 1962-01-25)\ndeath-place Aglaja Orgeni died\nin .\nVienna Bucharest (Aglaja Orgeni death date 1926-03-15)\n(Aglaja Orgeni death place Vienna)\n(Aglaja Orgeni birth date 1841-12-17)\n(Aglaja Orgeni birth place Rimavská Sobota)\n(Aglaja Orgeni nationality Austria)\n(Aglaja Orgeni nationality Hungary)\n(Aglaja Orgeni has occupation Opera singer)\n(Aglaja Orgeni death place Vienna)\nT-REx\nP106 Cigoli is a\nby profession.\narchitect lawyer (Cigoli has occupation Architect)\n(Cigoli nationality Italy)\n(Cino Cinelli has occupation Businessperson)\n(Francesco Cirio has occupation Businessperson)\n(Cigoli birth place Cigoli, San Miniato)\n(Emilio Cigoli has occupation Stage actor)\n(Francesco Cigalini has occupation Mathematician)\n(Ciputra has occupation Businessperson)\nP463 Phil Mogg is a\nmember of .\nUFO parliament (Phil Mogg member of UFO (band))\n(Phil Mogg nationality United Kingdom)\n(Phil Mogg birth date 1948-04-15)\n(Phil Mogg birth place London)\n(Mo Mozzali member of Minneapolis Millers)\n(John Mogg, Baron Mogg nationality United Kingdom)\n(Jamie Moyer member of Colorado Rockies)\n(Jamie Moyer member of Philadelphia Phillies)\nP407 Summerfolk was written\nin .\nrussian english (Summerland (novel) in language English language)\n(Summerfolk in language Russian language)\n(The World That Summer genre Neofolk)\n(Summerfolk author Maxim Gorky)\n(Summer (novel) in language English language)\n(Summertime (novel) in language English language)\n(A Summer Tale date published 2000)\n(Summerteeth in language English language )\nP1303 Nigel Pulsford plays\n.\nguitar sgt (Nigel Pulsford has occupation Guitarist)\n(Nigel Pulsford given name Nigel)\n(Nigel Pulsford birth date 1963-04-11)\n(Nigel Pulsford nationality United Kingdom)\n(Nigel Pulsford nationality Wales)\n(Nigel Pulsford birth place Newport, Wales)\n(William Pulsford nationality\nUnited Kingdom of Great Britain and Ireland)\n(Reginald Purdell has occupation Actor)\nTable 10: Illustrative examples of cases where LM-CORE(b,y) model successfully output the correct completions\nfor various probes in LAMA. Candidates containing correct fact are in bold.\n766\nRelation Input query LM-CORE(r,w) RoBERTa-baseCandidatesprediction prediction\nGoogle-RE\nbirth-place Sebastiano Maffettone\nwas born in.\nNaples Rome (Sebastiano Maffettone place of birth Naples)\n(Sebastiano Mazzoni place of birth Florence)\n(Sebastiano Mocenigo place of birth Venice)\n(Sebastiano Martinelli place of birth Italy)\n(Sebastiano Baggio place of birth Italy)\n(Stanley Corrsin has occupation Physicist)\n(Sebastiano Vassalli place of birth Genoa)\n(Sebastiano Poma place of birth Parma)\nT-REx\nP413 Rivaldo plays in\nposition.\nmidfielder the (Rivaldo position played on team forward)\n(Rivaldo position played on team midfielder)\n(Rivaldo Gonzalez position played on team midfielder)\n(Rivaldo Coetzee position played on team defender)\n(Rivaldo Vítor Mosca Ferreira Júnior position played on team forward)\n(Rivaldo member of sports team brazil national football team)\n(Rivaldo member of sports team brazil national under-20 football team)\n(Rivaldo member of sports team São Paulo fc)\nP176 Amiibo is\nproduced by.\nNintendo Samsung (amiibo manufacturer nintendo)\n(amiibo tap: nintendo’s greatest bits publisher nintendo)\n(animal crossing: amiibo festival publisher nintendo)\n(amiibo tap: nintendo’s greatest bits platform wii u)\n(animal crossing: amiibo festival developer nintendo entertainment\nplanning & development)\n(animal crossing: amiibo festival platform wii u)\n(amiibo instance of internet protocol)\n(animal crossing: amiibo festival genre party game)\nP138 Uraninite is named\nafter .\nuranium the (uraninite named after uranium)\n(uraniborg named after urania)\n(uranopilite named after compound)\n(uranopilite named after uranium)\n(uraniinae instance of taxon)\n(urania parent taxon uraniinae)\n(uranocircite-ii named after uranium)\n(30 urania named after urania\nP159 The headquarter of\nStelco is in.\nHamilton Madrid (Stelco headquarters location Hamilton)\n(Stelco lake erie works located in the administrative\nterritorial entity Ontario)\n(Stelco owned by U.S. steel)\n(Stelco lake erie works country Canada)\n(Stelco industry ferrous metallurgy)\n(Stelco instance of business)\n(Stec, inc. headquarters location California\n(Stekey located in the administrative territorial entity louisiana)\nP37 The official language\nof Virrat is.\nFinnish English (Virrat official language Finnish)\n(Virrat country Finland)\n(Virrat located in the administrative territorial entity Pirkanmaa)\n(Virrat located in time zone utc+2)\n(Virrat located in time zone utc+03:00)\n(Virrat instance of municipality of Finland)\n(Virrat instance of town)\n(Virrat instance of city)\nTable 11: Illustrative examples of cases where LM-CORE(r,w) model successfully output the correct completions\nfor various probes in LAMA. Candidates containing correct fact are in bold.\n767\nInput\nQuery\nExpected\nAnswer\nModel\nOutput\nRetrieved\nCandidates Comments\nHans Gefors was\nborn in .\nStockholm Hamburg (Hans Raj Hans birth date 1953-11-30) Corresponding fact not\npresent in KB. We\nspeculate that the\ncandidate in bold led the\nmodel to predict\nHamburg. BERT\npredictedOsloas the\nanswer.\n(Hans Raj Hans given name Hans (name))\n(Hans Raj Hans nationality India)\n(Hans Raj Hans has occupation Politician)\n(Hans Raj Hans member of Indian National Congress)\n(Claus Gerson birth place Hamburg)\n(Hans Geister birth date 1928-09-28)\n(Hans Gericke nationality Germany)\nVictor Salvi plays\n.\nharp quarterback (Victor Salvi given name Victor (name))\nCorresponding fact not\npresent in KB\n(Victor Salvi nationality United States)\n(Victor Salvi death place Milan)\n(Victor Salvi death date 2015-05-10)\n(Victor Salvi birth place Chicago)\n(Victor Salvi birth date 1920-03-04)\n(Joan Lui actor Francesco Salvi)\n(Victor Salvi birth date 1920-03-04)\nCBeebies is owned\nby .\nBBC Microsoft (CBeebies founding date 2002)\nCorresponding fact not\npresent in KB.\n(Gigglebiz creator CBeebies)\n(CBEF contained in place Ontario)\n(CBEF location Ontario)\n(Bambi production company The Walt Disney Company)\n(CBE Software founding date 2006)\n(Paddington Bear (TV series) production company ITV Central)\n(CBS Interactive parent organization CBS Corporation)\nIvan Petch was\nborn in .\nConcord Sydney (Ivan Petch birth date 1939-03-01)\nCorrect fact is retrieved.\nHowever, the model is\nstill not able to predict\ncorrect output.\n(Ivan Petch birth place Concord, New South Wales)\n(Ivan Petch family name Petch)\n(Ivan Petch given name Ivan (name))\n(Ivan Petch nationality Australia)\n(Ivan Petch has occupation Politician)\n(Ivan Petch has occupation Electrical engineer)\n(Ivan Petch alumni of Fort Street High School)\nScientist was\nborn in .\nKingston London (Scientist (musician) birth date 1960-04-18)\nAmbiguous query, leads\nto poor retrieval results.\n(Thomas Young (scientist) has occupation Physicist)\n(I Am a Scientist date published 1994)\n(Thomas Prince (scientist) has occupation Physicist)\n(Bambi production company The Walt Disney Company)\n(Allen Taylor (scientist) nationality United States)\n(Lawrence Roberts (scientist) nationality United States)\n(David Thomas (Canadian scientist) has occupation Biochemist)\nMoldova shares\nborder with .\nUkraine Romania (Moldova shares border with Ukraine)\nMultiple answers correct,\nhowever, LAMA\nconsiders only one.\n(Moldova shares border with Romania)\n(Moldova shares border with aa)\n(Moldova shares border with Jabara)\n(Moldova Nouã shares border with Bela Crkva)\n(Moldova contains administrative territorial entity Transnistria)\n(Moldova diplomatic relation Russia)\n(Moldova diplomatic relation European Union)\nTable 12: Illustrative examples of cases where the proposed solution produced incorrect completions.\n768\nTable 13: Mean precision at one (P@1) of various models on LAMA-UHN probe. Best results are highlighted in\nbold and the second best performance is underlined\nComplete Google-RE T-REx\nDoB PoB PoD All 1-1 N-1 N-M All\nBERT-based models\nBERT-base 18.72 1.59 6.98 3.98 4.18 62.86 21.99 17.32 22.16\nBERT-large 19.92 1.59 7.71 5.66 49.86 70.13 22.35 19.62 23.62\nERNIE 15.81 1.42 6.57 1.38 3.12 55.68 17.90 15.40 18.76\nLM-CORE(b,y) 41.33 64.44 46.80 46.02 52.42 71.04 44.42 29.11 39.75\nLM-CORE(b,w) 45.50 0.66 30.93 23.39 18.33 80.05 55.16 41.17 50.92\nRoBERTa-based models\nRoBERTa 13.66 1.85 4.18 0.55 2.19 53.36 13.99 15.80 16.62\nRoBERTa-large 17.99 1.41 5.68 0.36 2.48 67.27 20.47 17.86 21.74\nKEPLER 12.46 1.47 4.88 0.91 2.42 48.70 12.60 14.44 15.08\nCoLAKE 17.16 1.79 6.89 5.83 4.84 59.84 18.85 17.61 20.37\nLM-CORE(r,y) 34.25 46.33 35.66 19.31 33.77 63.83 37.74 25.20 34.12\nLM-CORE(r,w) 44.75 0.38 24.80 20.04 15.07 67.53 55.72 39.77 50.07\n769"
}