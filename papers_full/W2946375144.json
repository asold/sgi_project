{
  "title": "Levenshtein Transformer",
  "url": "https://openalex.org/W2946375144",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4222615513",
      "name": "Gu, Jiatao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2049064756",
      "name": "Wang, Changhan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4224482625",
      "name": "Zhao, Jake",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2099471712",
    "https://openalex.org/W2963434219",
    "https://openalex.org/W2996186646",
    "https://openalex.org/W2890501761",
    "https://openalex.org/W2962965405",
    "https://openalex.org/W2153653739",
    "https://openalex.org/W2963736842",
    "https://openalex.org/W2970690146",
    "https://openalex.org/W2963018920",
    "https://openalex.org/W2752047430",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2964026424",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2149327368",
    "https://openalex.org/W2759088880",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963946353",
    "https://openalex.org/W2101105183"
  ],
  "abstract": "Modern neural sequence generation models are built to either generate tokens step-by-step from scratch or (iteratively) modify a sequence of tokens bounded by a fixed length. In this work, we develop Levenshtein Transformer, a new partially autoregressive model devised for more flexible and amenable sequence generation. Unlike previous approaches, the atomic operations of our model are insertion and deletion. The combination of them facilitates not only generation but also sequence refinement allowing dynamic length changes. We also propose a set of new training techniques dedicated at them, effectively exploiting one as the other's learning signal thanks to their complementary nature. Experiments applying the proposed model achieve comparable performance but much-improved efficiency on both generation (e.g. machine translation, text summarization) and refinement tasks (e.g. automatic post-editing). We further confirm the flexibility of our model by showing a Levenshtein Transformer trained by machine translation can straightforwardly be used for automatic post-editing.",
  "full_text": "Levenshtein Transformer\nJiatao Gu†, Changhan Wang†, and Jake Zhao (Junbo)‡⋄\n†Facebook AI Research\n‡New York University ⋄Tigerobo Inc.\n†{jgu, changhan}@fb.com ‡jakezhao@cs.nyu.edu\nAbstract\nModern neural sequence generation models are built to either generate tokens\nstep-by-step from scratch or (iteratively) modify a sequence of tokens bounded\nby a ﬁxed length. In this work, we develop Levenshtein Transformer, a new\npartially autoregressive model devised for more ﬂexible and amenable sequence\ngeneration. Unlike previous approaches, the basic operations of our model are\ninsertion and deletion. The combination of them facilitates not only generation\nbut also sequence reﬁnement allowing dynamic length changes. We also propose\na set of new training techniques dedicated at them, effectively exploiting one as\nthe other’s learning signal thanks to their complementary nature. Experiments\napplying the proposed model achieve comparable or even better performance\nwith much-improved efﬁciency on both generation (e.g. machine translation, text\nsummarization) and reﬁnement tasks (e.g. automatic post-editing). We further\nconﬁrm the ﬂexibility of our model by showing a Levenshtein Transformer trained\nby machine translation can straightforwardly be used for automatic post-editing. 1\n1 Introduction\nNeural sequence generation models are widely developed and deployed in tasks such as machine\ntranslation (Bahdanau et al., 2015; Vaswani et al., 2017). As we examine the current frameworks,\nthe most popular autoregressive models generate tokens step-by-step. If not better, recent non-\nautoregressive approaches (Gu et al., 2018; Kaiser et al., 2018; Lee et al., 2018) have proved it\npossible to perform generation within a much smaller number of decoding iterations.\nIn this paper, we propose Levenshtein Transformer (LevT), aiming to address the lack of ﬂexibility of\nthe current decoding models. Notably, in the existing frameworks, the length of generated sequences\nis either ﬁxed or monotonically increased as the decoding proceeds. This remains incompatible\nwith human-level intelligence where humans can revise, replace, revoke or delete any part of their\ngenerated text. Hence, LevT is proposed to bridge this gap by breaking the in-so-far standardized\ndecoding mechanism and replacing it with two basic operations — insertion and deletion.\nWe train the LevT using imitation learning. The resulted model contains two policies and they are\nexecuted in an alternate manner. Empirically, we show that LevT achieves comparable or better results\nthan a standard Transformer model on machine translation and summarization, while maintaining\nthe efﬁciency advantages beneﬁted from parallel decoding similarly to (Lee et al., 2018). With this\nmodel, we argue that the decoding becomes more ﬂexible. For example, when the decoder is given an\nempty token, it falls back to a normal sequence generation model. On the other hand, the decoder acts\nas a reﬁnement model when the initial state is a low-quality generated sequence. Indeed, we show\nthat a LevT trained from machine translation is directly applicable to translation post-editing without\n1Codes for reproducing this paper are released in https://github.com/pytorch/fairseq/tree/\nmaster/examples/nonautoregressive_translation\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\narXiv:1905.11006v2  [cs.CL]  28 Oct 2019\nany change. This would not be possible with any framework in the literature because generation and\nreﬁnement are treated as two different tasks due to the model’s inductive bias.\nOne crucial component in LevT framework is the learning algorithm. We leverage the characteristics\nof insertion and deletion — they are complementary but also adversarial. The algorithm we propose\nis called “dual policy learning”. The idea is that when training one policy (insertion or deletion),\nwe use the output from its adversary at the previous iteration as input. An expert policy, on the\nother hand, is drawn to provide a correction signal. Despite that, in theory, this learning algorithm is\napplicable to other imitation learning scenarios where a dual adversarial policy exists, in this work we\nprimarily focus on a proof-of-concept of this algorithm landing at training the proposed LevT model.\nTo this end, we summarize the contributions as follows:\n• We propose Levenshtein Transformer (LevT), a new sequence generation model composed of the\ninsertion and deletion operations. This model achieves comparable or even better results than a\nstrong Transformer baseline in both machine translation and text summarization, but with much\nbetter efﬁciency (up to ×5 speed-up in terms of actual machine execution time);\n• We propose a corresponding learning algorithm under the theoretical framework of imitation\nlearning, tackling the complementary and adversarial nature of the dual policies;\n• We recognize our model as a pioneer attempt to unify sequence generation and reﬁnement, thanks\nto its built-in ﬂexibility. With this uniﬁcation, we empirically validate the feasibility of applying a\nLevT model trained by machine translation directly to translation post-editing, without any change.\n2 Problem Formulation\n2.1 Sequence Generation and Reﬁnement\nWe unify the general problems of sequence generation and reﬁnement by casting them to a Markov\nDecision Process (MDP) deﬁned by a tuple (Y,A,E,R,y0). We consider the setup consisting an\nagent interacting with an environment Ewhich receives the agent’s editing actions and returns the\nmodiﬁed sequence. We deﬁne Y= VNmax as a set of discrete sequences up to length Nmax where V\nis a vocabulary of symbols. At every decoding iteration, the agent receives an inputy drawn from\nscratch or uncompleted generation, chooses an action a and gets a reward r. We use Ato denote\nthe set of actions and Rfor the reward function. Generally the reward function Rmeasures the\ndistance between the generation and the ground-truth sequence, R(y) =−D(y,y∗) which can be\nany distance measurement such as the Levenshtein distance (Levenshtein, 1965). It is crucial to\nincorporate y0 ∈Y into the our formulation. As the initial sequence, the agent receives—when y0 is\nan already generated sequence from another system, the agent essentially learns to do reﬁnement\nwhile it falls back to generation if y0 is an empty sequence. The agent is modeled by a policy, π, that\nmaps the current generation over a probability distribution over A. That is, π: Y→ P(A).\n2.2 Actions: Deletion & Insertion\nFollowing the above MDP formulation, with a subsequence yk = (y1,y2,...,y n), the two basic\nactions – deletion and insertion – are called to generate yk+1 = E(yk,ak+1). Here we let y1 and yn\nbe special symbols <s> and </s>, respectively. Since we mainly focus on the policy of a single round\ngeneration, the superscripts are omitted in this section for simplicity. For conditional generation like\nMT, our policy also includes an input of source information x which is also omitted here.\nDeletion The deletion policy reads the input sequence y, and for every token yi ∈y, the deletion\npolicy πdel(d|i,y) makes a binary decision which is 1 (delete this token) or 0 (keep it). We additionally\nconstrain πdel(0|1,y) =πdel(0|n,y) = 1to avoid sequence boundary being broken. The deletion\nclassiﬁer can also be seen as a ﬁne-grained discriminator used in GAN (Goodfellow et al., 2014)\nwhere we predict “fake” or “real” labels for every predicted token.\nInsertion In this work, it is slightly more complex to build the insertion atomic because it involves\ntwo phases: placeholder prediction and token prediction so that it is able to insert multiple tokens\nat the same slot. First, among all the possible inserted slots (yi,yi+1) in y, πplh(p|i,y) predicts the\npossibility of adding one or several placeholders. In what follows, for every placeholder predicted as\n2\n+ + + + + +\n<s>\ncat sit mat\nLevenshtein Transformer\na sat on\nLevenshtein Transformer\nLevenshtein Transformer\n<s> </s>\n✓ ✘ ✓\ncat mat<s> </s>\n[1] [3] [0]\ncat mat </s>[PLH] [PLH] [PLH] [PLH]\ncat mat<s> </s>a sat on the\nthe\nTransformer Block_2\nTransformer Block_1\nTransformer Block_L\n…\nBOS x2 x3 3/+ x5 EOS\n1 2 3 4 5 6\nh1 h2 h3 h4 h5 h6\nPlaceholder \nClassiﬁer\nToken \nClassiﬁer\nDeletion \nClassiﬁer\nToken \nEmbeddings\nPosition \nEmbeddings\nClassiﬁers\nDelete Tokens\nInsert \nPlaceholders\nFill-in Tokens\nFigure 1: The illustration of the proposed Levenshtein Transformer decoder for one reﬁnement\niteration. The same architecture can be applied for three different tasks with speciﬁc classiﬁers. For\nsimplicity, the encoder-decoder attention is omitted within each Transformer-Block.\nabove, a token prediction policy πtok(t|i,y) replaces the placeholders with actual tokens in the vocab-\nulary. The two-stage insertion process can also be viewed as a hybrid of Insertion Transformer (Stern\net al., 2019) and masked language model (MLM, Devlin et al., 2018; Ghazvininejad et al., 2019).\nPolicy combination Recall that our two operations are complementary. Hence we combine them\nin an alternate fashion. For example in sequence generation from the empty, insertion policy is ﬁrst\ncalled and it is followed by deletion, and then repeat till the certain stopping condition is fulﬁlled.\nIndeed, it is possible to leverage the parallelism in this combination. We essentially decompose\none iteration of our sequence generator into three phases: “delete tokens – insert placeholders –\nreplace placeholders with new tokens”. Within each stage, all operations are performed in parallel.\nMore precisely, given the current sequence y = (y0,...,y n), and suppose the action to predict is\na = {d0,...d n  \nd\n; p0,...,p n−1  \np\n; t1\n0,...t p0\n0 ,...,t pn−1\nn−1  \nt\n}, the policy for one iteration is:\nπ(a|y) =\n∏\ndi∈d\nπdel(di|i,y) ·\n∏\npi∈p\nπplh(pi|i,y′) ·\n∏\nti∈t\nπtok(ti|i,y′′), (1)\nwhere y′= E(y,d) and y′′= E(y′,p). We parallelize the computation within each sub-tasks.\n3 Levenshtein Transformer\nIn this section, we cover the specs of Levenshtein Transformer and the dual-policy learning algorithm.\nOverall our model takes a sequence of tokens (or none) as the input then iteratively modify it by\nalternating between insertion and deletion, until the two policies combined converge. We describe\nthe detailed learning and inference algorithms in the Appendix.\n3.1 Model\nWe use Transformer (Vaswani et al., 2017) as the basic building block. For conditional generation,\nthe source x is included in each TransformerBlock. The states from the l-th block are:\nh(l+1)\n0 ,h(l+1)\n1 ,..., h(l+1)\nn =\n{ Ey0 + P0,Ey1 + P1,...,E yn + Pn, l = 0\nTransformerBlockl(h(l)\n0 ,h(l)\n1 ,..., h(l)\nn ), l> 0 (2)\nwhere E ∈R|V|×dmodel and P ∈RNmax×dmodel are the token and position embeddings, respectively. We\nshow the illustration of the proposed LevT model for one reﬁnement (delete, insert) as Figure 1.\n3\nPolicy Classiﬁers The decoder outputs (h0,h2,..., hn) are passed to three policy classiﬁers:\n1. Deletion Classiﬁer: LevT scans over the input tokens (except for the boundaries) and predict\n“deleted” (0) or “kept” (1) for each token position,\nπdel\nθ (d|i,y) =softmax\n(\nhi ·A⊤)\n, i= 1,...n −1, (3)\nwhere A∈R2×dmodel , and we always keep the boundary tokens.\n2. Placeholder Classiﬁer: LevT predicts the number of tokens to be inserted at every consecutive\nposition pairs, by casting the representation to a categorical distribution:\nπplh\nθ (p|i,y) =softmax\n(\nconcat(hi,hi+1) ·B⊤)\n, i= 0,...n −1, (4)\nwhere B ∈R(Kmax+1)×(2dmodel). Based on the number (0 ∼Kmax) of tokens it predicts, we insert\nthe considered number of placeholders at the current position. In our implementation, placehoder\nis represented by a special token <PLH> which was reserved in the vocabulary.\n3. Token Classiﬁer: following the placeholder prediction, LevT needs to ﬁll in tokens replacing all\nthe placeholders. This is achieved by training a token predictor as follow:\nπtok\nθ (t|i,y) =softmax\n(\nhi ·C⊤)\n, ∀yi = <PLH>, (5)\nwhere C ∈R|V|×dmodel with parameters being shared with the embedding matrix.\nWeight Sharing Our default implementation always assumes the three operations to share the\nsame Transformer backbone to beneﬁt features learned from other operations. However, it is also\npossible to disable weight sharing and train separate decoders for each operations, which increases\nthe capacity of the model while does not affect the overall inference time.\nEarly Exit Although it is parameter-efﬁcient to share the same Transformer architecture across\nthe above three heads, there is room for improvement as one decoding iteration requires three full\npasses of the network. To make trade-off between performance and computational cost, we propose\nto perform early exit (attaching the classiﬁer to an intermediate block instead of the last one) for πdel\nand πplh to reduce computation while keeping πtok always based on the last block, considering that\ntoken prediction is usually more challenging than the other two tasks.\n3.2 Dual-policy Learning\nImitation Learning We use imitation learning to train the Levenshtein Transformer. Essentially\nwe let the agent imitate the behaviors that we draw from some expert policyπ∗. The expert policy\nis derived from direct usage of ground-truth targets or less noisy version ﬁltered by sequence\ndistillation (Kim and Rush, 2016). The objective is to maximize the following expectation:\nEydel∼d˜πdel\nd∗∼π∗\n∑\nd∗\ni∈d∗\nlog πdel\nθ (d∗\ni|i,ydel)\n  \nDeletion Objective\n+ Eyins∼d˜πins\np∗,t∗∼π∗\n\n ∑\np∗\ni∈p∗\nlog πplh\nθ (p∗\ni|i,yins) +\n∑\nt∗\ni∈t∗\nlog πtok\nθ (t∗\ni|i,y′\nins)\n\n\n  \nInsertion Objective\n,\nwhere y′\nins is the output after inserting palceholders p∗upon yins. ˜πdel, ˜πins are the roll-in polices and\nwe repeatedly draw states (sequences) from their induced state distribution d˜πdel ,d˜πins . These states\nare ﬁrst executed by the expert policy returning the suggested actions by the expert, and then we\nmaximize the conditional log-likelihood over them. By deﬁnition, the roll-in policy determines the\nstate distribution fed to πθ during training. In this work, we have two strategies to construct the roll-in\npolicy — adding noise to the ground-truth or using the output from the adversary policy. Figure 2\nshows a diagram of this learning paradigm. We formally write down the roll-in policies as follows.\n1. Learning to Delete: we design the ˜πdel as a stochastic mixture between the initial input y0 or the\noutput by applying insertion from the model with some mixture factor α∈[0,1]:\nd˜πdel = {y0 if u<α else E\n(\nE(y′,p∗) ,˜t\n)\n, p∗∼π∗,˜t ∼πθ} (6)\nwhere u∼Uniform[0,1] and y′is any sequence ready to insert tokens. ˜t is obtained by sampling\ninstead of doing argmax from Eq. (5).\n4\nyyy\n⇤\n<latexit sha1_base64=\"Wtt9s3YXXOeLs27BHa4kJ46A93Y=\">AAAB8XicbVBNSwMxEJ2tX7V+VT16CRZBPJTdKuix6MVjBfuB7VqyabYNTbJLkhXK0n/hxYMiXv033vw3Zts9aOuDgcd7M8zMC2LOtHHdb6ewsrq2vlHcLG1t7+zulfcPWjpKFKFNEvFIdQKsKWeSNg0znHZiRbEIOG0H45vMbz9RpVkk780kpr7AQ8lCRrCx0kMvFkE6mT6elfrlilt1Z0DLxMtJBXI0+uWv3iAiiaDSEI617npubPwUK8MIp9NSL9E0xmSMh7RrqcSCaj+dXTxFJ1YZoDBStqRBM/X3RIqF1hMR2E6BzUgvepn4n9dNTHjlp0zGiaGSzBeFCUcmQtn7aMAUJYZPLMFEMXsrIiOsMDE2pCwEb/HlZdKqVb3zau3uolK/zuMowhEcwyl4cAl1uIUGNIGAhGd4hTdHOy/Ou/Mxby04+cwh/IHz+QMMf5CA</latexit>\nyyy\n0\n<latexit sha1_base64=\"V8zKO6P7hdH/aXQcpp91kWIoQtQ=\">AAAB8XicbVBNS8NAEJ34WetX1aOXxSJ4KkkV9Fj04rGC/cA2ls120y7d3YTdjRBC/4UXD4p49d9489+4aXPQ1gcDj/dmmJkXxJxp47rfzsrq2vrGZmmrvL2zu7dfOThs6yhRhLZIxCPVDbCmnEnaMsxw2o0VxSLgtBNMbnK/80SVZpG8N2lMfYFHkoWMYGOlh34sgiydPrrlQaXq1twZ0DLxClKFAs1B5as/jEgiqDSEY617nhsbP8PKMMLptNxPNI0xmeAR7VkqsaDaz2YXT9GpVYYojJQtadBM/T2RYaF1KgLbKbAZ60UvF//zeokJr/yMyTgxVJL5ojDhyEQofx8NmaLE8NQSTBSztyIyxgoTY0PKQ/AWX14m7XrNO6/V7y6qjesijhIcwwmcgQeX0IBbaEILCEh4hld4c7Tz4rw7H/PWFaeYOYI/cD5/ABWdkIY=</latexit>\n⇡\n⇤\n<latexit sha1_base64=\"rBOh873YWdsUV2dnSe/9s9KK7l4=\">AAAB7XicbVBNSwMxEJ2tX7V+VT16CRZBPJTdKuix6MVjBfsB7VqyabaNzSZLkhXK0v/gxYMiXv0/3vw3Zts9aOuDgcd7M8zMC2LOtHHdb6ewsrq2vlHcLG1t7+zulfcPWlomitAmkVyqToA15UzQpmGG006sKI4CTtvB+Cbz209UaSbFvZnE1I/wULCQEWys1OrF7OGs1C9X3Ko7A1omXk4qkKPRL3/1BpIkERWGcKx113Nj46dYGUY4nZZ6iaYxJmM8pF1LBY6o9tPZtVN0YpUBCqWyJQyaqb8nUhxpPYkC2xlhM9KLXib+53UTE175KRNxYqgg80VhwpGRKHsdDZiixPCJJZgoZm9FZIQVJsYGlIXgLb68TFq1qnderd1dVOrXeRxFOIJjOAUPLqEOt9CAJhB4hGd4hTdHOi/Ou/Mxby04+cwh/IHz+QOhpY6B</latexit>\n⇡\nrnd\n<latexit sha1_base64=\"3ckLkj2tzZPk6phZlg2lTs3XkDs=\">AAAB+nicbVBNS8NAEN3Ur1q/Uj16WSyCp5JUQY9FLx4r2A9oYtlsNu3SzSbsTtQS+1O8eFDEq7/Em//GpM1BWx8MPN6bYWaeFwuuwbK+jdLK6tr6RnmzsrW9s7tnVvc7OkoUZW0aiUj1PKKZ4JK1gYNgvVgxEnqCdb3xVe5375nSPJK3MImZG5Kh5AGnBDJpYFadmN85wB5B01RJf1oZmDWrbs2Al4ldkBoq0BqYX44f0SRkEqggWvdtKwY3JQo4FWxacRLNYkLHZMj6GZUkZNpNZ6dP8XGm+DiIVFYS8Ez9PZGSUOtJ6GWdIYGRXvRy8T+vn0Bw4aZcxgkwSeeLgkRgiHCeA/a5YhTEJCOEKp7diumIKEIhSysPwV58eZl0GnX7tN64Oas1L4s4yugQHaETZKNz1ETXqIXaiKIH9Ixe0ZvxZLwY78bHvLVkFDMH6A+Mzx+E7JQp</latexit>\n⇡ ✓\n<latexit sha1_base64=\"j+DOH/xpXeGPsDU2xTbW7a1iH5s=\">AAAB8nicbVBNS8NAEN34WetX1aOXxSJ4KkkV9Fj04rGC/YAklM120y7d7IbdiVBCf4YXD4p49dd489+4aXPQ1gcDj/dmmJkXpYIbcN1vZ219Y3Nru7JT3d3bPzisHR13jco0ZR2qhNL9iBgmuGQd4CBYP9WMJJFgvWhyV/i9J6YNV/IRpikLEzKSPOaUgJX8IOWDAMYMSHVQq7sNdw68SryS1FGJ9qD2FQwVzRImgQpijO+5KYQ50cCpYLNqkBmWEjohI+ZbKknCTJjPT57hc6sMcay0LQl4rv6eyElizDSJbGdCYGyWvUL8z/MziG/CnMs0AybpYlGcCQwKF//jIdeMgphaQqjm9lZMx0QTCjalIgRv+eVV0m02vMtG8+Gq3rot46igU3SGLpCHrlEL3aM26iCKFHpGr+jNAefFeXc+Fq1rTjlzgv7A+fwB4X+Q/A==</latexit>\nyyy\n0\n<latexit sha1_base64=\"LitoktbSswRyr4WTdhpYZESx9S4=\">AAAB8HicbVBNS8NAEJ34WetX1aOXxSJ6KkkV9Fj04rGC/ZA2lM120y7d3YTdjRBCf4UXD4p49ed489+4aXPQ1gcDj/dmmJkXxJxp47rfzsrq2vrGZmmrvL2zu7dfOThs6yhRhLZIxCPVDbCmnEnaMsxw2o0VxSLgtBNMbnO/80SVZpF8MGlMfYFHkoWMYGOlx34sgiydnpUHlapbc2dAy8QrSBUKNAeVr/4wIomg0hCOte55bmz8DCvDCKfTcj/RNMZkgke0Z6nEgmo/mx08RadWGaIwUrakQTP190SGhdapCGynwGasF71c/M/rJSa89jMm48RQSeaLwoQjE6H8ezRkihLDU0swUczeisgYK0yMzSgPwVt8eZm06zXvola/v6w2boo4SnAMJ3AOHlxBA+6gCS0gIOAZXuHNUc6L8+58zFtXnGLmCP7A+fwBUK2QFQ==</latexit>\nyyy\n00\n<latexit sha1_base64=\"9xA/5UBNuCmQCMsElGehQTuSSDU=\">AAAB8XicbVBNSwMxEJ2tX7V+VT16CRapp7JbBT0WvXisYD+wXUo2zbahSXZJskJZ+i+8eFDEq//Gm//GbLsHbX0w8Hhvhpl5QcyZNq777RTW1jc2t4rbpZ3dvf2D8uFRW0eJIrRFIh6pboA15UzSlmGG026sKBYBp51gcpv5nSeqNIvkg5nG1Bd4JFnICDZWeuzHIkins2q1NChX3Jo7B1olXk4qkKM5KH/1hxFJBJWGcKx1z3Nj46dYGUY4nZX6iaYxJhM8oj1LJRZU++n84hk6s8oQhZGyJQ2aq78nUiy0norAdgpsxnrZy8T/vF5iwms/ZTJODJVksShMODIRyt5HQ6YoMXxqCSaK2VsRGWOFibEhZSF4yy+vkna95l3U6veXlcZNHkcRTuAUzsGDK2jAHTShBQQkPMMrvDnaeXHenY9Fa8HJZ47hD5zPH7QXkEY=</latexit>\n↵\n<latexit sha1_base64=\"wlES0PlLDlylr5MeneqMfVQVd5w=\">AAAB7nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mqoMeCF48V7Ae0oUy2m3bpZrPsboQS+iO8eFDEq7/Hm//GpM1BWx8MPN6bYWZeoAQ31nW/ndLG5tb2Tnm3srd/cHhUPT7pmDjRlLVpLGLdC9AwwSVrW24F6ynNMAoE6wbTu9zvPjFteCwf7UwxP8Kx5CGnaDOpO0ChJlgZVmtu3V2ArBOvIDUo0BpWvwajmCYRk5YKNKbvucr6KWrLqWDzyiAxTCGd4pj1MyoxYsZPF+fOyUWmjEgY66ykJQv190SKkTGzKMg6I7QTs+rl4n9eP7HhrZ9yqRLLJF0uChNBbEzy38mIa0atmGUEqebZrYROUCO1WUJ5CN7qy+uk06h7V/XGw3Wt2SziKMMZnMMleHADTbiHFrSBwhSe4RXeHOW8OO/Ox7K15BQzp/AHzucPwq2PLg==</latexit>\n1 \u0000 ↵\n<latexit sha1_base64=\"Jd9oPXUZVfRWve3/aEZ4+ityyNk=\">AAAB8nicbVBNS8NAEJ34WetX1aOXxSJ4sSRV0GPBi8cK9gPSUCbbTbt0sxt2N0Ip/RlePCji1V/jzX9j0uagrQ8GHu/NMDMvTAQ31nW/nbX1jc2t7dJOeXdv/+CwcnTcNirVlLWoEkp3QzRMcMlallvBuolmGIeCdcLxXe53npg2XMlHO0lYEONQ8ohTtJnke+SS9FAkIyz3K1W35s5BVolXkCoUaPYrX72BomnMpKUCjfE9N7HBFLXlVLBZuZcaliAd45D5GZUYMxNM5yfPyHmmDEikdFbSkrn6e2KKsTGTOMw6Y7Qjs+zl4n+en9roNphymaSWSbpYFKWCWEXy/8mAa0atmGQEqebZrYSOUCO1WUp5CN7yy6ukXa95V7X6w3W10SjiKMEpnMEFeHADDbiHJrSAgoJneIU3xzovzrvzsWhdc4qZE/gD5/MHS2GP9A==</latexit>\n1 \u0000 \u0000\n<latexit sha1_base64=\"7TYo9lSQqQisUt/IY8YDGeWAk1I=\">AAAB8XicbVBNS8NAEN3Ur1q/qh69LBbBiyWpgh4LXjxWsB/YhrLZTtqlm03YnQgl9F948aCIV/+NN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgpr6xubW8Xt0s7u3v5B+fCoZeJUc2jyWMa6EzADUihookAJnUQDiwIJ7WB8O/PbT6CNiNUDThLwIzZUIhScoZUePXpBewEgK/XLFbfqzkFXiZeTCsnR6Je/eoOYpxEo5JIZ0/XcBP2MaRRcwrTUSw0kjI/ZELqWKhaB8bP5xVN6ZpUBDWNtSyGdq78nMhYZM4kC2xkxHJllbyb+53VTDG/8TKgkRVB8sShMJcWYzt6nA6GBo5xYwrgW9lbKR0wzjjakWQje8surpFWrepfV2v1VpV7P4yiSE3JKzolHrkmd3JEGaRJOFHkmr+TNMc6L8+58LFoLTj5zTP7A+fwBgqSPgA==</latexit>\n\u0000\n<latexit sha1_base64=\"FZAYzfr+4hXu2uQ6OzUjuUrR4bE=\">AAAB7XicbVBNS8NAEN34WetX1aOXxSJ4KkkV9Fjw4rGC/YA2lM120q7dbMLuRCih/8GLB0W8+n+8+W/ctDlo64OBx3szzMwLEikMuu63s7a+sbm1Xdop7+7tHxxWjo7bJk41hxaPZay7ATMghYIWCpTQTTSwKJDQCSa3ud95Am1ErB5wmoAfsZESoeAMrdTuB4CsPKhU3Zo7B10lXkGqpEBzUPnqD2OeRqCQS2ZMz3MT9DOmUXAJs3I/NZAwPmEj6FmqWATGz+bXzui5VYY0jLUthXSu/p7IWGTMNApsZ8RwbJa9XPzP66UY3viZUEmKoPhiUZhKijHNX6dDoYGjnFrCuBb2VsrHTDOONqA8BG/55VXSrte8y1r9/qraaBRxlMgpOSMXxCPXpEHuSJO0CCeP5Jm8kjcndl6cd+dj0brmFDMn5A+czx/6lo66</latexit>\nLearn to Insert\nLearn to Delete\nyyy\n⇤\n<latexit sha1_base64=\"Wtt9s3YXXOeLs27BHa4kJ46A93Y=\">AAAB8XicbVBNSwMxEJ2tX7V+VT16CRZBPJTdKuix6MVjBfuB7VqyabYNTbJLkhXK0n/hxYMiXv033vw3Zts9aOuDgcd7M8zMC2LOtHHdb6ewsrq2vlHcLG1t7+zulfcPWjpKFKFNEvFIdQKsKWeSNg0znHZiRbEIOG0H45vMbz9RpVkk780kpr7AQ8lCRrCx0kMvFkE6mT6elfrlilt1Z0DLxMtJBXI0+uWv3iAiiaDSEI617npubPwUK8MIp9NSL9E0xmSMh7RrqcSCaj+dXTxFJ1YZoDBStqRBM/X3RIqF1hMR2E6BzUgvepn4n9dNTHjlp0zGiaGSzBeFCUcmQtn7aMAUJYZPLMFEMXsrIiOsMDE2pCwEb/HlZdKqVb3zau3uolK/zuMowhEcwyl4cAl1uIUGNIGAhGd4hTdHOy/Ou/Mxby04+cwh/IHz+QMMf5CA</latexit>\nyyy\n⇤\n<latexit sha1_base64=\"Wtt9s3YXXOeLs27BHa4kJ46A93Y=\">AAAB8XicbVBNSwMxEJ2tX7V+VT16CRZBPJTdKuix6MVjBfuB7VqyabYNTbJLkhXK0n/hxYMiXv033vw3Zts9aOuDgcd7M8zMC2LOtHHdb6ewsrq2vlHcLG1t7+zulfcPWjpKFKFNEvFIdQKsKWeSNg0znHZiRbEIOG0H45vMbz9RpVkk780kpr7AQ8lCRrCx0kMvFkE6mT6elfrlilt1Z0DLxMtJBXI0+uWv3iAiiaDSEI617npubPwUK8MIp9NSL9E0xmSMh7RrqcSCaj+dXTxFJ1YZoDBStqRBM/X3RIqF1hMR2E6BzUgvepn4n9dNTHjlp0zGiaGSzBeFCUcmQtn7aMAUJYZPLMFEMXsrIiOsMDE2pCwEb/HlZdKqVb3zau3uolK/zuMowhEcwyl4cAl1uIUGNIGAhGd4hTdHOy/Ou/Mxby04+cwh/IHz+QMMf5CA</latexit>\nApply Deletion\nApply Insertion\nFigure 2: The data-ﬂow of learning.\n2. Learning to Insert: similar to the deletion step, we apply a mixture of the deletion output and\na random word dropping sequence of the round-truth, inspired by recent advances of training\nmasked language model (Devlin et al., 2018). We use random dropping as a form of noise injection\nto encourage more exploration. Let β ∈[0,1] and u∼Uniform[0,1],\nd˜πins = {E\n(\ny0,d∗)\n, d∗∼π∗ if u<β else E\n(\ny∗, ˜d\n)\n, ˜d ∼πRND } (7)\nExpert Policy It is crucial to construct an expert policy in imitation learning which cannot be too\nhard or too weak to learn from. Speciﬁcally, we considered two types of experts:\n1. Oracle: One way is to build an oracle which accesses to the ground-truth sequence. It returns the\noptimal actions a∗(either oracle insertion p∗,t∗or oracle deletion d∗) by:\na∗= argmin\na\nD(y∗,E(y,a)) (8)\nHere, we use the Levenshtein distance (Levenshtein, 1965) 2 as Dconsidering it is possible to\nobtain the action suggestions efﬁciently by dynamic programming.\n2. Distillation: We also explore to use another teacher model to provide expert policy, which is\nknown as sequence-level knowledge distillation (Kim and Rush, 2016). This technique has been\nwidely used in previous approaches for nonauoregressive generation (Gu et al., 2018). More\nprecisely, we ﬁrst train an autoregressive teacher model using the same datasets and then replace\nthe ground-truth sequence y∗ by the beam-search result of this teacher-model, yAR . We use the\nsame mechanism to ﬁnd the suggested option as using the ground-truth oracle.\n3.3 Inference\nGreedy Decoding At inference time, we apply the trained model over the initial sequence y0 for\nseveral iterations. We greedily pick up the actions associated with high probabilities in Eq. (3)(4)(5).\nMoreover, we ﬁnd that using search (instead of greedy decoding) or nosiy parallel decoding (Cho,\n2016) does not yield much gain in LevT. This observation is quite opposite to what has been widely\ndiscovered in autoregressive decoding. We hypothesize there may be two reasons leading to this\nissue: (i) The local optimal point brought by greedy decoding in autoregressive models is often far\nfrom the optimal point globally. Search techniques resolve this issue with tabularization. In our case,\nhowever, because LevT inserts or deletes tokens dynamically, it could easily revoke the tokens that\nare found sub-optimal and re-insert better ones; (ii) the log-probability of LevT is not a good metric\nto select the best output. However, we do believe to see more improvements if we include an external\nre-ranker, e.g. an autoregressive teacher model. We leave this discussion in the future work.\nTermination Condition The decoding stops when one of the following two conditions is fulﬁlled:\n1. Looping: Generation is terminated if two consecutive reﬁnement iterations return the same output\nwhich can be (i) there are no words to delete or insert; (ii) the agent gets stuck in an inﬁnite loop:\ni.e. the insertion and deletion counter each other and keep looping.\n2. Timeout: We further set a maximum number of iterations (timeout) to guarantee a constant-time\ncomplexity in the worst case (Lee et al., 2018; Ghazvininejad et al., 2019).\nPenalty for Empty Placeholders Similar to Stern et al. (2019), we add a penalty to insert “empty”\nplaceholder in decoding. Overly inserting “empty” placeholders may result in shorter output. A\npenalty term γ ∈[0,3] is subtracted from the logits of 0 in Eq. (4).\n2We only consider the variant which only computes insertion and deletion. No substitution is considered.\n5\nTable 1: Generation quality (BLEU ↑, ROUGE-1/2/L ↑) and latency (ms ↓) as well as the average\nnumber of decoder iterations (IDEC ) on the standard test sets for LevT and the autoregressive baseline\n(with both greedy and beam-search outputs). We show the results of LevT trained from both oracle\nand the autoregressive teacher model.\nDataset Metric Transformer Levenshtein Transformer\ngreedy beam4 oracle distillation\nQuality ↑\nRo-En BLEU 31.67 32.30 33.02 33.26\nEn-De BLEU 26.89 27.17 25.20 27.27\nEn-Ja BLEU 42.86 43.68 42.36 43.17\nGigaword\nROUGE-1 37.31 37.87 36.14 37.40\nROUGE-2 18.10 18.92 17.14 18.33\nROUGE-L 34.65 35.13 34.34 34.51\nSpeed ↓\nRo-En Latency (ms) / IDEC 326 / 27.1 349 / 27.1 97 / 2.19 90 / 2.03\nEn-De Latency (ms) / IDEC 343 / 28.1 369 / 28.1 126 / 2.88 92 / 2.05\nEn-Ja Latency (ms) / IDEC 261 / 22.6 306 / 22.6 112 / 2.61 106 / 1.97\nGigaword Latency (ms) / IDEC 116 / 10.1 149 / 10.1 98 / 2.32 84 / 1.73\n̂The ̂latter ̂coil ̂generated ̂2.2 T ̂in ̂liquid ̂helium . ̂஍ᘏ΄παϸ΅ ͭ͵ ̶\nnothing to delete >>\n>̂ϥϷγϭ@>ϥϷγϭ@>ҋ҇ҋ@>ҋ҇ҋ@>ҭ@>\u0003̶@\n(iteration 1)\n(iteration 2)\n>̂@>ϥϷγϭ@>ϥϷγϭ@>ҋ҇ҋ@>ҋ҇ҋ@>ҭ@>\u0003̶@\n>̂@>஍ᘏ@>΄@>παϸ@>΅@ϥϷγϭ@>Ӿͽ@>ҋ҇ҋ@>ҭ@>咲ኞͭ͵@>\u0003̶@\ninsert >>\ndelete >>\ninsert >>\nnothing to delete, nothing to insert >> [Terminate]\nFigure 3: An example of W AT’17 En-Ja translation with two decoder iterations by LevT. We present\nthe inserted tokens in purple and deleted tokens with red strikethrough\n.\n4 Experiments\nWe validate the efﬁciency, effectiveness, and ﬂexibility of Levenshtein Transformer extensively across\nthree different tasks — machine translation (MT), text summarization (TS) and automatic post-editing\n(APE) for machine translation, from both generation (§4.1) and reﬁnement (§4.2) perspectives.\n4.1 Sequence Generation\nFor the sequence generation perspective, we evaluate LevT model on MT and TS. As a special case,\nsequence generation assumes empty y0 = <S></ S> as input and no initial deletion is applied.\nData & Evaluation We use three diversiﬁed language pairs for MT experiments: WMT’16\nRomanian-English (Ro-En) 3, WMT’14 English-German (En-De) 4 and WAT2017 Small-NMT\nEnglish-Japanese (En-Ja, Nakazawa et al., 2017) 5. The TS experiments use preprocessed data\nfrom the Annotated English Gigaword (Gigaword, Rush et al., 2015)6. We learn byte-pair encod-\ning (BPE, Sennrich et al., 2016) vocabulary on tokenized data. Detailed dataset statistics can be\nfound in the Appendix. For evaluation metrics, we use BLEU (Papineni et al., 2002) for MT and\nROUGE-1,2,L (Lin, 2004) for TS. Before computing the BLEU scores for Japanese output, we\nalways segment Japanese words using KyTea7.\nModels & Training We adopt the model architecture of Transformer base (Vaswani et al., 2017)\nfor the proposed LevT model and the autoregressive baseline. All the Transformer-based models are\n3http://www.statmt.org/wmt16/translation-task.html\n4http://www.statmt.org/wmt14/translation-task.html\n5http://lotus.kuee.kyoto-u.ac.jp/WAT/WAT2017/snmt/index.html\n6https://github.com/harvardnlp/sent-summary\n7http://www.phontron.com/kytea/\n6\nTable 2: Ablation study for Levenshtein Transformer on En-De (a) and Ro-En (b) translation tasks.\n(a) Test BLEU for variant weight sharing. Baseline scores from Lee et al.\n(IT, 2018), Ghazvininejad et al. (MaskT, 2019) are included for reference.\nsharing none plh ,ins ins ,del all IT MaskT\noracle − 25.50 − 25.20 − −\ndistill 25.11 27.73 24.90 27.27 21.61 26.56\n(b) Test BLEU and deletion loss\nwith variant roll-in polices.\nroll-in BLEU NLL(del)\nOurs 33.02 ≈0.202\nDAE 31.78 ≈0.037\n0 20 40 60 80 100 120\nsentence length\n0\n2\n4\n6\n8\n10\n12number of iterations\nLevT Translation\nLogarithm Time\nLinear Time\nConstant Time (4)\nConstant Time (10)\n(a) Average number of reﬁnement iterations v.s. length measured\non monolingual corpus. For most of the time, LevT decodes with\nmuch smaller number (generally, 1∼4) of iterations.\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nx speed-up\n31.6\n31.8\n32.0\n32.2\n32.4\n32.6\n32.8\n33.0x BLEU scores\nLevT(2-2)\nLevT(6-6)\nLevT(1-1)\nLevT(3-1)\nAT (beam4)\nAT (greedy)\n(b) BLEU v.s. speed-up for LevT across\nvariant early-exits and the autoregressive\nbaselines on the test set of Ro-En.\nFigure 4: Plots showing the decoding efﬁciency of the proposed Levenshtein Transformer.\ntrained on 8 Nvidia V olta GPUs with maximum300Ksteps and a total batch-size of around 65,536\ntokens per step (We leave more details to the Appendix).\nOverall results We present our main results on the generation quality and decoding speed in\nTable 1. We measure the speed by the averaged generation latency of generating one sequence at a\ntime on single Nvidia V100 GPU. To remove the implementation bias, we also present the number of\ndecoder iterations as a reference. It can be concluded that for both MT and summarization tasks, our\nproposed LevT achieves comparable and sometimes better generation quality compared to the strong\nautoregressive baseline, while LevT is much more efﬁcient at decoding. A translation example is\nshown in Figure 3 and we leave more in Appendix. We conjecture that this is due to that the output\nof the teacher model possesses fewer modes and much less noisy than the real data. Consequently,\nLevT needs less number of iterations to converge to this expert policy.\nAblation on Efﬁciency As shown in Figure 4a, we plot the average number of iterations over\nthe length of input over a monolingual corpus. LevT learns to properly adjust the decoding time\naccordingly. We also explore the variants of “early exit” where we denote LevT(m-n) as a model with\nmand nblocks for deletion (Eq. (3)) and placeholder prediction (Eq. (4)) respectively. Figure 4b\nshows that although it compromises the quality a bit, our model with early exit achieves up to ×5\nspeed-up (execution time) comparing against a strong autoregressive Transformer using beam-search.\nAblation on Weight Sharing We also evaluate LevT with different weight sharing as noted in\n§3.1. The results of models trained with oracle or distillation are listed in Table 2a. We observe that\nweight-sharing is beneﬁcial especially between the two insertion operations (placeholder and token\nclassiﬁers). Also, it shows another +0.5 BLEU improvement by not sharing the deletion operation\nwith insertion compared to the default setting, which may indicate that insertion and deletion capture\ncomplementary information, requiring larger capacity by learning them separately.\nImportance of mixture roll-in policy We perform an ablation study on the learning algorithm.\nSpeciﬁcally, we train a model with no mixing of the πθ in Equation (6). We name this experiment\nby DAE due to its resemblance to a denoising autoencoder. We follow closely a standard pipeline\nestablished by Lee et al. (2018). Table 2b shows this comparison. As we can see that the deletion loss\n7\nTable 3: Performance (BLEU ↑/ case-sensitive TER ↓) comparison on APE. “do nothing” represents\nthe results of the original MT system output; the autoregressive model uses beam-size 4. For the\nproposed LevT, we use “scratch” to denote training from scratch on the APE triple data, and use\n“zero-shot” to denote applying an MT pre-trained LevT model directly for post-editing tasks. The\nsame model can be further ﬁne-tuned. All scores with underlines are from the model trained with an\nautoregressive teacher model (distillation) as the expert policy.\nDataset MT Do-Nothing Transformer Levenshtein Transformer\nsystem Scratch Zero-shot Fine-tune\nSynthetic\nRo-En PBMT 27.5 / 52.6 28.9 / 52.8 29.1 / 50.4 30.1 / 51.7 −\nNMT 26.2 / 56.5 26.9 / 55.6 28.3/ 53.6 28.0 / 55.8 −\nEn-De PBMT 15.4 / 69.4 22.8 / 61.0 25.8/ 56.6 16.5/ 69.6 −\nEn-Ja NMT 37.7 / 48.0 41.0 / 44.9 42.2/ 44.3 39.4/ 47.5 −\nReal En-De PBMT 62.5 / 24.5 67.2 / 22.1 66.9 / 21.9 59.6 / 28.7 70.1/ 19.2\n1 2 3 4 5 6\nMaximum Iterations\n20\n25\n30\n35\n40\n45\n50BLEU scores\nNon-Autoregressive\nLevT\nLevT + oracle (D)\nLevT + oracle (D, P)\nTransformer (beam4)\n(a) Test set BLEU scores for WMT Ro-En\n0 1 2 3 4 5 6\nMaximum Iterations\n5\n10\n15\n20\n25\n30Translation Error Rate (TER)\nMT Output\nLevT\nLevT + oracle (D)\nLevT + oracle (D, P)\nTransformer (beam4) (b) Test set TER scores for Real APE En-De\nFigure 5: MT & PE Performance v.s. Timeout iterations w/o oracle instructions.\nfrom DAE is much smaller while the generation BLEU score is inferior. We conjecture that this is\ncaused by the mismatch between the states from the model and the roll-in policy in training the DAE.\nv.s. Exiting Reﬁnement-based Models Table 2a also includes results from two relevant recent\nworks which also incorporate iterative reﬁnement in non-autoregressive sequence generation. For fair\ncomparison, we use the result with length beam 1 from Ghazvininejad et al. (2019). Although both\napproaches use similar “denosing” objectives to train the reﬁnement process, our model explicitly\nlearns “insertion” and “deletion” in a dual-policy learning fashion, and outperforms both models.\n4.2 Sequence Reﬁnement\nWe evaluate LevT’s capability of reﬁning sequence outputs on the APE task. In this setting, inputs\nare pairs of the source sequence and a black-box MT system generation. The ground-truth outputs\nare from real human edits with expansion using synthetic data.\nDataset We follow a normal protocol in the synthetic APE experiments (Grangier and Auli, 2017):\nwe ﬁrst train the input MT system on half of the dataset. Then we will train a reﬁnement model on\nthe other half based on the output produced by the MT model trained in the previous phase. For the\nreal APE tasks, we use the data from WMT17 Automatic Post-Editing Shared Task8 on En-De. It\ncontains both real PE triples and a large-scale synthetic corpus.\nModels & Evaluation The baseline model is a standard Transformer encoding the concatenation\nof the source and the MT system’s output. For the MT system here, we want some imperfect systems\nthat need to be reﬁned. We consider a statistical phrase-based MT system (PBMT, Koehn et al., 2003)\nand an RNN-based NMT system (Bahdanau et al., 2015). Apart from BLEU scores, we additionally\napply translation error rate (TER, Snover et al., 2006) as it is widely used in the APE literature.\n8http://www.statmt.org/wmt17/ape-task.html\n8\nOverall results We show the major comparison in Table 3. When training from scratch, LevT\nconsistently improves the performance of the input MT system (either PBMT or NMT). It also\nachieves better performance than the autoregressive Transformer in most of the cases.\nPre-training on MT Thanks to the generality of the LevT model, we show it is feasible to directly\napply the LevT model trained by generation onto reﬁnement tasks — in this case — MT and APE.\nWe name this a “zero-shot post-editing” setting. According to Table 3, the pre-trained MT models are\nalways capable of improving the initial MT input in the synthetic tasks.\nThe real APE task, however, differs quite a bit from the synthetic tasks because human translators\nnormally only ﬁx a few spotted errors. This ends up with very high BLEU scores even for the\n“Do-nothing” column. However, the pre-trained MT model achieves the best results by ﬁne-tuning on\nthe PE data indicating that LevT is able to leverage the knowledge for generation and reﬁnement.\nCollaborate with Oracle Thanks to the saperation of insertion and deletion operations, LevT has\nbetter interpretability and controllability. For example, we test the ability that LevT adapts oracle (e.g.\nhuman translators) instructions. As shown in Figure 5, both MT and PE tasks have huge improvement\nif every step the oracle deletion is given. This goes even further if the oracle provides both the correct\ndeletion and the number of placehoders to insert. It also sheds some light upon computer-assisted\ntext editing for human translators.\n5 Related Work\nNon-Autoregressive and Non-Monotonic Decoding Breaking the autoregressive constraints and\nmonotonic (left-to-right) decoding order in classic neural sequence generation systems has recently\nattracted much interest. Stern et al. (2018); Wang et al. (2018) designed partially parallel decoding\nschemes to output multiple tokens at each step. Gu et al. (2018) proposed a non-autoregressive\nframework using discrete latent variables, which was later adopted in Lee et al. (2018) as iterative\nreﬁnement process. Ghazvininejad et al. (2019) introduced the masked language modeling objective\nfrom BERT (Devlin et al., 2018) to non-autoregressively predict and reﬁne translations. Welleck et al.\n(2019); Stern et al. (2019); Gu et al. (2019) generate translations non-monotonically by adding words\nto the left or right of previous ones or by inserting words in arbitrary order to form a sequence.\nEditing-Based Models Several prior works have explored incorporating “editing” operations for\nsequence generation tasks. For instance, Novak et al. (2016) predict and apply token substitutions\niteratively on phase-based MT system outputs using convolutional neural network. QuickEdit (Grang-\nier and Auli, 2017) and deliberation network (Xia et al., 2017) both consist of two autoregressive\ndecoders where the second decoder reﬁnes the translation generated by the ﬁrst decoder. Guu et al.\n(2018) propose a neural editor which learned language modeling by ﬁrst retrieving a prototype and\nthen editing over that. Freitag et al. (2019) correct patterned errors in MT system outputs using\ntransformer models trained on monolingual data. Additionally, the use of Levenshtein distance with\ndynamic programming as the oracle policy were also proposed in Sabour et al. (2018); Dong et al.\n(2019). Different from these work, the proposed model learns a non-autoregressive model which\nsimultaneously inserts and deletes multiple tokens iteratively.\n6 Conclusion\nWe propose Levenshtein Transformer, a neural sequence generation model based on insertion and\ndeletion. The resulted model achieves performance and decoding efﬁciency, and embraces sequence\ngeneration to reﬁnement in one model. The insertion and deletion operations are arguably more\nsimilar to how human writes or edits text. For future work, it is potential to extend this model to\nhuman-in-the-loop generation.\nAcknowledgement\nWe would like to thank Kyunghyun Cho, Marc’Aurelio Ranzato, Douwe Kiela, Qi Liu and our\ncolleagues at Facebook AI Research for valuable feedback, discussions and technical assistance.\n9\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly\nlearning to align and translate. In 3rd International Conference on Learning Representations, ICLR\n2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.\nKyunghyun Cho. 2016. Noisy parallel approximate decoding for conditional recurrent language\nmodel. arXiv preprint arXiv:1605.03835.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: pre-training of\ndeep bidirectional transformers for language understanding. CoRR, abs/1810.04805.\nYue Dong, Zichao Li, Mehdi Rezagholizadeh, and Jackie Chi Kit Cheung. 2019. Editnts: An neural\nprogrammer-interpreter model for sentence simpliﬁcation through explicit editing. arXiv preprint\narXiv:1906.08104.\nMarkus Freitag, Isaac Caswell, and Scott Roy. 2019. Text repair model for neural machine translation.\narXiv preprint arXiv:1904.04790.\nMarjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer. 2019. Constant-time machine\ntranslation with conditional masked language models. CoRR, abs/1904.09324.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\nAaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. In Advances in neural\ninformation processing systems, pages 2672–2680.\nDavid Grangier and Michael Auli. 2017. Quickedit: Editing text & translations by crossing words\nout. arXiv preprint arXiv:1711.04805.\nJiatao Gu, James Bradbury, Caiming Xiong, Victor O.K. Li, and Richard Socher. 2018. Non-\nautoregressive neural machine translation. In 6th International Conference on Learning Represen-\ntations, ICLR 2018, Vancouver, Canada, April 30-May 3, 2018, Conference Track Proceedings.\nJiatao Gu, Qi Liu, and Kyunghyun Cho. 2019. Insertion-based decoding with automatically inferred\ngeneration order. arXiv preprint arXiv:1902.01370.\nKelvin Guu, Tatsunori B Hashimoto, Yonatan Oren, and Percy Liang. 2018. Generating sentences by\nediting prototypes. Transactions of the Association of Computational Linguistics, 6:437–450.\nLukasz Kaiser, Samy Bengio, Aurko Roy, Ashish Vaswani, Niki Parmar, Jakob Uszkoreit, and Noam\nShazeer. 2018. Fast decoding in sequence models using discrete latent variables. In International\nConference on Machine Learning, pages 2395–2404.\nYoon Kim and Alexander Rush. 2016. Sequence-level knowledge distillation. In EMNLP.\nPhilipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation.\nIn Proceedings of the 2003 Conference of the North American Chapter of the Association for\nComputational Linguistics on Human Language Technology-Volume 1, pages 48–54. Association\nfor Computational Linguistics.\nJason Lee, Elman Mansimov, and Kyunghyun Cho. 2018. Deterministic non-autoregressive neural\nsequence modeling by iterative reﬁnement. In Proceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018 ,\npages 1173–1182.\nVladimir Iosifovich Levenshtein. 1965. Binary codes capable of correcting deletions, insertions, and\nreversals. In Doklady Akademii Nauk, volume 163, pages 845–848. Russian Academy of Sciences.\nChin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text Summa-\nrization Branches Out: Proceedings of the ACL-04 Workshop , pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nToshiaki Nakazawa, Shohei Higashiyama, Chenchen Ding, Hideya Mino, Isao Goto, Hideto Kazawa,\nYusuke Oda, Graham Neubig, and Sadao Kurohashi. 2017. Overview of the 4th workshop on\nAsian translation. In Proceedings of the 4th Workshop on Asian Translation (WAT2017), pages\n1–54, Taipei, Taiwan. Asian Federation of Natural Language Processing.\n10\nRoman Novak, Michael Auli, and David Grangier. 2016. Iterative reﬁnement for machine translation.\narXiv preprint arXiv:1610.06602.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic\nevaluation of machine translation. In Proceedings of the 40th annual meeting on association for\ncomputational linguistics, pages 311–318. Association for Computational Linguistics.\nAlexander M. Rush, Sumit Chopra, and Jason Weston. 2015. A neural attention model for abstrac-\ntive sentence summarization. In Proceedings of the 2015 Conference on Empirical Methods in\nNatural Language Processing, pages 379–389, Lisbon, Portugal. Association for Computational\nLinguistics.\nSara Sabour, William Chan, and Mohammad Norouzi. 2018. Optimal completion distillation for\nsequence learning. arXiv preprint arXiv:1810.01398.\nRico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th Annual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers), pages 1715–1725, Berlin, Germany. Association for\nComputational Linguistics.\nMatthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A\nstudy of translation edit rate with targeted human annotation. In In Proceedings of Association for\nMachine Translation in the Americas, pages 223–231.\nMitchell Stern, William Chan, Jamie Kiros, and Jakob Uszkoreit. 2019. Insertion transformer:\nFlexible sequence generation via insertion operations. arXiv preprint arXiv:1902.03249.\nMitchell Stern, Noam Shazeer, and Jakob Uszkoreit. 2018. Blockwise parallel decoding for deep\nautoregressive models. In Advances in Neural Information Processing Systems , pages 10107–\n10116.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proceedings of the Annual\nConference on Neural Information Processing Systems (NIPS).\nChunqi Wang, Ji Zhang, and Haiqing Chen. 2018. Semi-autoregressive neural machine translation.\nIn Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,\npages 479–488, Brussels, Belgium. Association for Computational Linguistics.\nSean Welleck, Kianté Brantley, Hal Daumé III, and Kyunghyun Cho. 2019. Non-monotonic sequential\ntext generation. arXiv preprint arXiv:1902.02192.\nYingce Xia, Fei Tian, Lijun Wu, Jianxin Lin, Tao Qin, Nenghai Yu, and Tie-Yan Liu. 2017. Delibera-\ntion networks: Sequence generation beyond one-pass decoding. In Advances in Neural Information\nProcessing Systems, pages 1784–1794.\n11\nA Learning & Inference Algorithm\nWe present the detailed algorithms for learning and decoding from Levenshtein Transformer as\nfollows. For simplicity, we always omit the source information x in conditional sequence generation\ntasks such as machine translation which is handled by the cross-attention with an encoder on x.\nThe learning algorithm is shown in Algorithm 1. Eis the environment and Dis denoted as the\nLevenshtein distance, and we can easily back-track the optimal insertion and deletion operations\nthrough dynamic programming. We only show the the case with single batch-size for convenience.\nWe also present the inference algorithm in Algorithm 2. If the initial sequencey0 is empty (<s></s>),\nthe proposed model will skip the ﬁrst deletion and do sequence generation. Otherwise, the model\nstarts with deletion operations and reﬁne the input sequence.\nAlgorithm 1 Learning for Levenshtein Transformer\nInitialize: Training set T, expert policy π∗, model policy πθ, random deletion policy πRND , α, β\nrepeat\nSample a training pair (y0,y∗) ∼Y\nif expert π∗is a teacher model then\nSet the teacher’s output as the targety∗= yAR\nend if\nSample u,v ∼Uniform[0,1]\nif u<β then\nyins = E(y0, ˜d), where ˜d = argmind D(y∗,E\n(\ny0,d)\n)\nelse\nyins = E(y∗, ˜d), where ˜d ∼πRND (·|y∗)\nend if\ny′\nins = E(yins,p∗), where p∗,t∗= argminp,t D(y∗,E(yins,{p,t}))\nif v <αthen\nydel = y0\nelse\nydel = E(y′\nins,ˆt), where ˆt = argmaxt\n∑\nyi∈y′\nins,yi=<PLH> log πtok\nθ (ti|i,y′\nins)\nend if\nLins\nθ = −\n[∑\nyi∈yins,p∗\ni∈p∗log πplh\nθ (p∗\ni|i,yins) +∑\nyi∈y′\nins,yi=<PLH>,t∗\ni∈t∗log πtok\nθ (t∗\ni|i,y′\nins)\n]\nLdel\nθ = −∑\nyi∈ydel,d∗\ni∈d∗log πdel\nθ (d∗\ni|i,ydel), where d∗= argmind D(y∗,E(ydel,d))\nθ= θ−λ·▽θ\n[\nLins\nθ + Ldel\nθ\n]\nuntil Maximum training steps reached\nB Dataset and Preprocessing Details\nTable 4 and 5 list the statistics (# of sentences, vocabulary) for all the datasets used in this work. We\nlearn BPE vocabulary with 32,000 joint operations for WMT En-De and Gigaword and 40,000 joint\noperations for WMT Ro-En. For W AT En-Ja, we adopt the ofﬁcial16,384 BPE vocabularies learned\nseparately on source and target side.\nTable 4: Dataset statistics for sequence generation tasks (MT and TS).\nDataset Train Valid Test V ocabulary\nTranslation\nWMT’16 Ro-En 608,319 1999 1999 34,983\nWMT’14 En-De 4,500,966 3000 3003 37,009\nW AT’17 En-Ja 2,000,000 1790 1812 17,952 / 17,801\nSummarization English Gigaword 3,803,957 189,651 1951 30,004\n12\nAlgorithm 2 Decoding for Levenshtein Transformer\nInitialize: Input y = y0, step t= 0, maximum step Tmax, model policy πθ.\nrepeat\nif y = <s></s> then\nEmpty sequence, skip deletion: y′= y\nelse\nDelete tokens: y′= E(y, ˆd), where ˆd = argmaxd\n∑\nyi∈y log πdel\nθ (di|i,y)\nend if\nif (t > 0) & (y′= ˜y) then\nTermination condition satisﬁed: direct loop\nbreak\nend if\nAssign deleted output for back-up ˜y = y′\nInsert placeholders: y′′= E(y′,ˆp), where ˆp = argmaxp\n∑\nyiyi+1∈y′log πplh\nθ (pi|i,y′)\nif y′′= y′= y then\nTermination condition satisﬁed: nothing to delete, nothing to insert.\nbreak\nend if\nif y′′= y′then\nNothing to insert, skip insertion: y = y′′\nelse\nReplace placeholders: y = E(y′′,ˆt), where ˆt = argmaxt\n∑\nyi∈y′′,yi=<PLH> log πtok\nθ (ti|i,y′′)\nend if\nUpdate steps: t = t + 1\nuntil Reach the maximum length t= Tmax\nreturn y\nTable 5: Dataset statistics for sequence reﬁnement tasks (APE).\nDataset MT-Train APE-Train Valid Test V ocabulary\nSynthetic\nWMT’16 Ro-En 300,000 308,319 1999 1999 34,983\nWMT’14 En-De 2,250,000 2,250,967 3000 3003 37,009\nW AT’17 En-Ja 1,000,000 1,000,000 1790 1812 17,952 / 17,801\nReal WMT’17 APE\nEn-De 4,391,180 526,368 (fake)\n+ 24,000 (real) 2000 2000 40,349\nC Model and Training Details\nC.1 Sequence Generation Tasks\nTransformer models are used for autoregressive baselines as well as teacher models (for the expert\npolicy). By default, we set dmodel = 512, dhidden = 2048, nheads = 8, nlayers = 6, lrmax = 0.0005,\nlabel-smooth = 0.1, warmup = 10000and dropout = 0.3. Source and target side share embeddings\nin all the training pairs except for WAT En-Ja where BPE vocabularies of both side are learned\nseparately and are almost non-overlapping.\nSince the training objectives for Levenshtein Transformer contains randomness terms (Eq.(6) (7)),\nwe instead use BLEU (for MT) or ROUGE-2 (for TS) to select the best checkpoint by validation\nscores. We do not average checkpoints in this work.\nC.2 Sequence Reﬁnement Tasks\nFor synthetic APE tasks, we keep the same training conditions for LevT as those for MT tasks (§C.1).\nAs described earlier in §4.2, we build the baseline Transformer by concatenating the source and MT\nsystem’s output as the input sequence for the encoder. Specially, we restart the positional embeddings\n13\nTable 6: The percentage of WMT En-De test sentence generation terminated at each iteration using\nLevT(T) with a maximum iteration of 10.\nIterations 1 2 3 4 5 6 7 8 9 10 2.43\n% 12.3 48.1 28.5 8.5 2.0 0.4 0.1 0 0 0.1 A VG\nfor the MT output, add an additional language embedding for each token of the input sequence to\nshow its language type. The detailed hyperpameters are the same as the standard Transformer.\nAs described in §4.2, we consider the following two different imperfect MT systems to provide the\nreﬁnement inputs. Firstly, we consider the traditional statistical phrase-based machine translation\nsystem (PBMT). We follow the instruction to build the basic baseline model via moses9. As for the\nNMT-based model, we use a single layer attention-based model composed by LSTM. We build this\nmodel on fairseq-py10 with the default conﬁguration.\nFor the real APE task, we follow the procedures introduced in Junczys-Dowmunt and Grundkiewicz\n(2016). Synthetic corpus has two subsets: a 500K one and a 4M one. We over-sample real data by 10\ntimes and merge it with the 500K synthetic data to train APE models. Besides, we also train a LevT\nMT model on the bigger (4M) synthetic corpus where we only use the source and target pairs.\nC.3 Implementation\nBoth the proposed Levenshtein Transformer and the baseline Transformer are implemented using\nPyTorch11. The codes are released as part of the Fairseq-py 12.\nC.4 Maximum Number of Iterations\nWe also presented in general how many sentences will be generated using the maximum iteration (for\ninstance 10). As shown in Table 6, surprisingly, most predictions are gotten in 1-4 iterations, and the\naverage number of iterations is 2.43. Only a tiny portion (∼0.1%) require the maximum number of\niterations demonstrating the efﬁciency of the proposed approach.\n9http://www.statmt.org/moses/?n=Moses.Baseline\n10https://github.com/pytorch/fairseq/blob/master/fairseq/models/lstm.py\n11https://pytorch.org/\n12https://github.com/pytorch/fairseq/tree/master/examples/nonautoregressive_\ntranslation\n14\nD More Decoding Examples\nWe present more examples from the proposed Levenshtein Transformer as follows.\n̂7KH\u0003̂WRR\u0003̂KLJK\u0003̂URWDWLRQ\u0003̂VSHHG\u0003̂SURGXFHV\u0003\n̂WKH\u0003̂UHYHUVH\u0003̂GHIRUPDWLRQ\u0003\u0011 ͣ΄䄜୵͢ኞͮΡ̶\nnothing to delete >>\n敢@>ͯͤ@>Ρ;@>ᭋ@>䄜୵@>͢ኞͮΡ@>̶@\n(iteration 1)\n(iteration 2)\n敢@>ͯͤ@>Ρ;@>ᭋ@>䄜୵@>͢ኞͮΡ@>̶@\n敢@>᭛ଶ͢@>ͯͤ@>Ρ;@>҅@>ᭋ@>䄜୵@>͢ኞͮΡ@>̶@\ninsert >>\ndelete >>\ninsert >>\nnothing to delete, nothing to insert >> [Terminate]\n敢@>᭛ଶ͢@>ͯͤ@>Ρ;@>҅@>ᭋ@>䄜୵@>͢ኞͮΡ@>̶@\n敢@>᭛ଶ͢@>ṛ@>ͯͤ@>Ρ;@>҅@>ᭋ@>䄜୵@>͢ኞͮΡ@>̶@\n(iteration 3) nothing to delete >>\ninsert >>\n̂6RPH\u0003̂SRVVLEOH\u0003̂VWUXFWXUHV\u0003̂DQG\u0003̂FLUFXLWV\u0003\n̂ZHUH\u0003̂SURSRVHG\u0003̂DQG\u0003̂YHULƉHG\u0003\u0011 ᪠Ψ൉ໜͭ䭥戣ͭ͵̶\nnothing to delete >>\n᪠@>Ψ൉ໜͭ@>҅@>䭥戣ͭ͵@>̶@\n(iteration 1)\n(iteration 2)\n᪠@>Ψ൉ໜͭ@>҅@>䭥戣ͭ͵@>̶@\n>̂@>͚ͥͺ͡΄@ᚆ΀@>䯤᭜;@᪠@>Ψ൉ໜͭ@>҅@>䭥戣ͭ͵@>̶@\ninsert >>\ndelete >>\ninsert >>\nnothing to delete, nothing to insert >> [Terminate]\n(a)\n(b)\nFigure 6: Translation examples for W AT’17 Small-NMT En-Ja with the Levenshtein Transformer.\nSULPLL\u0003RDPHQL\u0003REVHUYD\u0003DELD\u0003GXSD\u0003FDWHYD\u0003PLQXWH\u0003\u0011 WKH\u0003ƉUVW\u0003SHRSOH\u0003QRWLFHG\u0003DIWHU\u0003D\u0003IHZ\u0003PLQXWHV\u0003\u0011\nWKH\u0003ƉUVW\u0003SHRSOH\u0003VHH\u0003VHH\u0003D\u0003IHZ\u0003PLQXWHV\u0003ODWHU\u0003\u0011\n(iteration 1)\n(iteration 2)\ninsert >>\ndelete >>\ninsert >>\nWKH\u0003ƉUVW\u0003SHRSOH\u0003VHH\u0003VHH\u0003D\u0003IHZ\u0003PLQXWHV\u0003ODWHU\u0003\u0011\nWKH\u0003ƉUVW\u0003SHRSOH\u0003FDQ\u0003VHH\u0003RQO\\\u0003D\u0003IHZ\u0003PLQXWHV\u0003ODWHU\u0003\u0011\nnothing to delete, nothing to insert >>\nnothing to delete >>\nFLQHPD\u0003FLW\\\u0003FDXWD\u0003\tTXRW\u001e\u0003PLFLOH\u0003YRFL\u0003PDUL\u0003\tTXRW\u001e FLQHPD\u0003FLW\\\u0003VHHNV\u0003\tTXRW\u001e\u0003VPDOO\u0003ELJ\u0003YRLFHV\u0003\tTXRW\u001e\ninsert >> FLQHPD\u0003FLW\\\u0003VHHNV\u0003\tTXRW\u001e\u0003VPDOO\u0003YRLFHV\u0003YRLFHV\u0003\tTXRW\u001e\n(iteration 1)\n(iteration 2)\ndelete >>\ninsert >>\nnothing to delete, nothing to insert >>\nFLQHPD\u0003FLW\\\u0003VHHNV\u0003\tTXRW\u001e\u0003VPDOO\u0003YRLFHV\u0003YRLFHV\u0003\tTXRW\u001e\nFLQHPD\u0003FLW\\\u0003VHHNV\u0003\tTXRW\u001e\u0003WKH\u0003VPDOO\u0003ELJ\u0003YRLFHV\u0003\u0011\u0003\tTXRW\u001e\nnothing to delete >>\nYLHWLOH\u0003ORU\u0003DYHDX\u0003VD\u0003VH\u0003GHV##\u0003SDUWD\u0003GHƉQLWLY\u0003\u0011 WKHLU\u0003OLYHV\u0003ZRXOG\u0003IRUHYHU\u0003SDUW\u0003ZD\\V\u0003\u0011\ninsert >> WKHLU\u0003OLYHV\u0003ZHUH\u0003WR\u0003WR\u0003D\u0003SHUPDQHQWO\\\u0003DSDUW\u0003\u0011\n(iteration 1)\n(iteration 2)\ndelete >>\ninsert >> WKHLU\u0003OLYHV\u0003ZHUH\u0003WR\u0003EUHDN\u0003DSDUW\u0003SHUPDQHQWO\\\u0003\u0011\nnothing to delete, nothing to insert >>\nnothing to delete >>\nWKHLU\u0003OLYHV\u0003ZHUH\u0003WR\u0003WR\u0003D\u0003SHUPDQHQWO\\\u0003DSDUW\u0003\u0011\n(a)\n(b)\n(c)\n[Terminate]\n[Terminate]\n[Terminate]\nFigure 7: Translation examples for WMT’16 Ro-En with the Levenshtein Transformer.\n15\n2U\u0003VHDUFK\u0003IRU\u0003SODQ##\u0003HWV\u0003VLPLODU\u0003WR\u0003WKH\u0003(DUWK\u0003\nDQG\u0003WKXV\u0003SHUKDSV\u0003GLVFRYHU\u0003H[##\u0003WU##\u0003DWHU##\u0003\nUHVWUL##\u0003DO\u0003OLIH\u0003\"\n2GHU\u0003VXFKHQ\u00036LH\u0003QDFK\u00033ODQHWHQ\u0003§KQOLFK\u0003GHU\u0003(UGH\u0003XQG\u0003\nHQWGHFNHQ\u00036LH\u0003GDPLW\u0003YLHOOHLFKW\u0003GDV\u0003H[##\u0003WU##\u0003DWHU##\u0003\nUHV##\u0003WU##\u0003LVFKH\u0003/HEHQ\u0003\"\n(iteration 1)\n(iteration 2)\nnothing to delete, nothing to insert >> \n2GHU\u0003VXFKHQ\u00036LH\u0003QDFK\u00033ODQHWHQ\u0003§KQOLFK\u0003GHU\u0003(UGH\u0003XQG\u0003\nHQWGHFNHQ\u0003VR\u0003YLHOOHLFKW\u0003DX¢HU##\u0003WU##\u0003GLVFKH\u0003/HEHQ\u0003\"\n2GHU\u0003VXFKHQ\u00036LH\u0003QDFK\u00033ODQHWHQ\u0003§KQOLFK\u0003GHU\u0003(UGH\u0003XQG\u0003\nHQWGHFNHQ\u0003VR\u0003YLHOOHLFKW\u0003DX¢HU##\u0003WU##\u0003GLVFKH\u0003/HEHQ\u0003\"\n2GHU\u0003VXFKHQ\u00036LH\u0003QDFK\u00033ODQHWHQ\u0003§KQOLFK\u0003GHU\u0003(UGH\u0003XQG\u0003\nHQWGHFNHQ\u0003VR\u0003YLHOOHLFKW\u0003GDV\u0003/HEHQ\u0003\"\n2GHU\u0003VXFKHQ\u00036LH\u0003QDFK\u00033ODQHWHQ\u0003§KQOLFK\u0003GHU\u0003(UGH\u0003XQG\u0003\nHQWGHFNHQ\u0003VR\u0003YLHOOHLFKW\u0003GDV\u0003HLJHQH\u0003/HEHQ\u0003\"\ninsert >>\ndelete >>\ninsert >>\ndelete >>\ninsert >>\ninsert >>\n(iteration 3)\n(iteration 4)\nnothing to delete >>\nnothing to delete >>\n2GHU\u0003VXFKHQ\u00036LH\u0003QDFK\u00033ODQHWHQ\u0003§KQOLFK\u0003GHU\u0003(UGH\u0003XQG\u0003\nHQWGHFNHQ\u0003VR\u0003YLHOOHLFKW\u0003GDV\u0003/HEHQ\u0003\"\n[Terminate]\n(a)\n2GHU\u0003VXFKHQ\u00036LH\u00033ODQHWHQ\u00033ODQHWHQ\u00033ODQHWHQ\u0003(UGH\u0003(UGH\u0003\n(UGH\u0003(UGH\u0003XQG\u0003YLHOOHLFKW\u0003([##\u0003WU##\u0003WU##\u0003WU##\u0003WU##\u0003\nLVFKH\u0003HQWGHFNHQ\u0003\"\n2GHU\u0003VXFKHQ\u00036LH\u00033ODQHWHQ\u00033ODQHWHQ\u00033ODQHWHQ\u0003(UGH\u0003(UGH\u0003\n(UGH\u0003(UGH\u0003XQG\u0003YLHOOHLFKW\u0003([##\u0003WU##\u0003WU##\u0003WU##\u0003WU##\u0003\nLVFKH\u0003HQWGHFNHQ\u0003\"\n/RFDO\u0003SXEOLF\u0003WUDQVSRUW\u0003ZLOO\u0003DOVR\u0003EHFRPH\u0003PRUH\u0003\nH[SHQVLYH\u0003\u0011 'HU\u0003¹ƈHQWOLFKH\u00031DK##\u0003YHUNHKU\u0003ZHUGH\u0003DXFK\u0003WH##\u0003XUHU\u0003\u0011\n$XFK\u0003GHU\u0003¹ƈHQWOLFKH\u00031DK##\u0003YHUNHKU\u0003ZHUGHQ\u0003WH##\u0003XUHU\u0003\u0011(iteration 1)\n(iteration 2)\ninsert >>\ndelete >>\ninsert >>\nnothing to delete, nothing to insert >>\nnothing to delete >>\n(b)\n[Terminate]\n$XFK\u0003GHU\u0003¹ƈHQWOLFKH\u00031DK##\u0003YHUNHKU\u0003ZHUGHQ\u0003WH##\u0003XUHU\u0003\u0011\n$XFK\u0003GHU\u0003¹ƈHQWOLFKH\u00031DK##\u0003YHUNHKU\u0003ZLUG\u0003WH##\u0003XUHU\u0003\u0011\nFigure 8: Translation examples for WMT’14 En-De with the Levenshtein Transformer.\nVWUHQJWKHQHG\u0003ERUGHU\u0003SDWURO\u0003KDV\u0003OHG\u0003WR\u0003D\u0003\u0006\u0006\u0010SHUFHQW\u0003GURS\u0003\nLQ\u0003DUUHVWV\u0003RI\u0003XQGRFXPHQWHG\u0003PLJUDQWV\u0003WKLV\u0003\\HDU\u0003DW\u0003WKH\u0003\nX\u0011V\u0011\u0010PH[LFR\u0003IURQWLHU\u0003\u000f\u0003LW\u0003ZDV\u0003UHSRUWHG\u0003RQ\u0003ZHGQHVGD\\\u0003\u0011\nDUUHVWV\u0003RI\u0003ERU##\u0003GHU\u0010##\u0003FURVV##\u0003HUV\u0003GURS\n(iteration 1)\n(iteration 2)\nnothing to delete, nothing to insert >> \nERUGHU\u0003SDWURO\u0003UHGXFHV\u0003DUUHVWV\u0003RI\u0003PLJUDQWV\u0003DW\u0003X\u0011V\u0011\u0010PH[LFR\u0003ERUGHU\ninsert >>\ndelete >>\ninsert >>\nnothing to delete >>\n[Terminate]\n(a)\nERUGHU\u0003SDWURO\u0003OHDGV\u0003DUUHVWV\u0003\u0006\u0006\u0003PLJUDQWV\u0003DW\u0003X\u0011V\u0011\u0010PH[LFR\u0003ERUGHU\nERUGHU\u0003SDWURO\u0003OHDGV\u0003DUUHVWV\u0003\u0006\u0006\u0003PLJUDQWV\u0003DW\u0003X\u0011V\u0011\u0010PH[LFR\u0003ERUGHU\nXV\u0003ODZ\\HU\u0003HG\u0003I##\u0003DJDQ\u0003VDLG\u0003ZHGQHVGD\\\u0003KH\u0003ZLOO\u0003EULQJ\u0003D\u0003\nPXOWL\u0010PLOOLRQ\u0003GROODU\u0003ODZVXLW\u0003LQ\u0003WKH\u0003XQLWHG\u0003VWDWHV\u0003DJDLQVW\u0003WKH\u0003\nSROLVK\u0003JRYHUQPHQW\u0003XQOHVV\u0003LW\u0003WDNHV\u0003FRQFUHWH\u0003VWHSV\u0003WR\u0003UHSD\\\u0003D\u0003\nKXJH\u0003GHEW\u0003WR\u0003KROGHUV\u0003RI\u0003ERQGV\u0003LVVXHG\u0003EHIRUH\u0003ZRUOG\u0003ZDU\u0003LL\u0003\u0011\nXV\u0003VWDU\u0003ODZ\\HU\u0003HG\u0003I##\u0003DJDQ\u0003WR\u0003VXH\u0003SRODQG\u0003IRU\u0003XQSDLG\u0003ERQGV\n(iteration 1)\n(iteration 2)\nnothing to delete, nothing to insert >> \nI##\u0003DJDQ\u0003WKUHDWHQV\u0003WR\u0003VXH\u0003SRODQG\u0003RYHU\u0003GHEW\u0003UHSD\\PHQW\ninsert >>\ndelete >>\ninsert >>\nnothing to delete >>\n[Terminate]\n(b)\nI##\u0003DJDQ\u0003WR\u0003PXOWL\u0010PLOOLRQ\u0003VXH\u0003SRODQG\u0003SRODQG\u0003RYHU\u0003GHEW\u0003GHEW\nI##\u0003DJDQ\u0003WR\u0003PXOWL\u0010PLOOLRQ\u0003VXH\u0003SRODQG\u0003SRODQG\u0003RYHU\u0003GHEW\u0003GHEW\nFigure 9: Translation examples for English Gigaword with the Levenshtein Transformer.\n16\n,Q\u0003WKH\u0003WDJ\u0003\u000f\u0003LQVHUW\u0003WKH\u0003$FWLRQ6FULSW\u0003FRGH\u0003WR\u0003\nFUHDWH\u0003WKH\u0003EHKDYLRU\u0003\u0011\n)¿JHQ\u00036LH\u0003LP\u00037DJ\u0003GHQ\u0003$FWLRQ6FULSW\u0010##\u0003&RGHV\u0003HLQ\u0003\u000f\u0003XP\u0003\nGDV\u00039HUKDOWHQ\u0003]X\u0003HUVWHOOHQ\u0003\u0011\n(iteration 1)\nnothing to delete, nothing to insert >> \ninsert >>\ndelete >>\n[Terminate]\n(a)\n)¿JHQ\u00036LH\u0003LP\u00037DJ\u0003GHV\u0003$FWLRQ6FULSW\u0010##\u0003&RGHV\u0003HLQ\u0003\u000f\u0003XP\u0003GDV\u0003\n9HUKDOWHQ\u0003]X\u0003HUVWHOOHQ\u0003\u0011\n.OLFNHQ\u00036LH\u0003LP\u0003\u00107DJ\u0003GHV\u0003$FWLRQ6FULSW\u0010##\u0003&RGHV\u0003HLQI¿JHQ\u0003\u000f\u0003\nXP\u0003GDV\u00039HUKDOWHQ\u0003]X\u0003HUVWHOOHQ\u0003\u0011\n,Q\u0003WKH\u0003WDJ\u0003\u000f\u0003LQVHUW\u0003WKH\u0003$FWLRQ6FULSW\u0003FRGH\u0003WR\u0003\nFUHDWH\u0003WKH\u0003EHKDYLRU\u0003\u0011\n9HUZHQGHQ\u00036LH\u0003GLH\u00036FKDOWƊ§FKH\u0003\u0005\u0003%HDUEHLWHQ\u0003\u000f\u0003\u0005\u0003XP\u0003HLQH\u0003\nQHXH\u0003-DYD##\u00036FULSW##\u0003$NWLRQ\u0003]X\u0003§QGHUQ\u0003RGHU\u0003]X\u0003HUVWHOOHQ\u0003\u0011\n(iteration 1)\nnothing to delete, nothing to insert >> \ninsert >>\ndelete >>\n[Terminate]\n(b)\n9HUZHQGHQ\u00036LH\u0003GLH\u00036FKDOWƊ§FKH\u0003\u0005\u0003%HDUEHLWHQ\u0003\u000f\u0003\u0005\u0003XP\u0003HLQH\u0003\nQHXH\u0003-DYD6FULSW\u0010##\u0003$NWLRQ\u0003]X\u0003§QGHUQ\u0003RGHU\u0003]X\u0003HUVWHOOHQ\u0003\u0011\n9HUZHQGHQ\u00036LH\u0003GLH\u00036FKDOWƊ§FKH\u0003\u0005\u0003%HDUEHLWHQ\u0003\u000f\u0003\u0005\u0003XP\u0003HLQH\u0003\nQHXH\u0003-DYD6FULSW\u0003$NWLRQ\u0003]X\u0003§QGHUQ\u0003RGHU\u0003HUVWHOOHQ\u0003\u0011\n7R\u0003UHVL]H\u0003WKH\u0003FDQYDV\u0003\u000f\u0003GUDJ\u0003WKH\u0003IUDPH\u0003FRUQHUV\u0003\u0011 8P\u0003GLH\u0003*U¹¢H\u0003GHU\u0003/HLQZDQG\u0003]X\u0003YHU§QGHUQ\u0003\u000f\u0003]LHKHQ\u00036LH\u0003GLH\u0003\n5DKPHQ##\u0003H##\u0003FNHQ\u0003\u0011\n(iteration 1)\nnothing to delete, nothing to insert >> \ninsert >>\ndelete >>\n[Terminate]\n(c)\n8P\u0003GLH\u0003*U¹¢H\u0003GHU\u0003/HLQZDQG\u0003]X\u0003§QGHUQ\u0003\u000f\u0003]LHKHQ\u00036LH\u0003GLH\u0003\n5DKPHQ##\u0003H##\u0003FNHQ\u0003\u0011\n8P\u0003GLH\u0003*U¹¢H\u0003GHU\u0003/HLQZDQG\u0003\u000f\u0003]LHKHQ\u00036LH\u0003GHQ\u00035DKPHQ\u0003\u0011\nFigure 10: Post-editing examples for WMT’17-APE En-De with the Levenshtein Transformer.\nDGPLQLVWUDWLD\u0003W##\u0003Y##\u0003U\u0003D\u0003IDFXW\u0003FRQVWDQW\u0003HIRUWXUL\u0003SHQWUX\u0003\nGLPLQXDUHD\u0003FKHOWXLHOLORU\u0003FX\u0003SHUVRQDOXO\u0003VL\u0003SURGXFWLD\u0003WY\u0003\u0011\nthe administration of t@@ v@@ r has made constant eﬀorts to \nreduce personnel and tv production expenses .\n(iteration 1)\nnothing to delete, nothing to insert >> \ninsert >>\nnothing to delete >>\n[Terminate]\nthe t@@ v@@ r administration has constantly constantly eﬀorts to \ncut spending on personnel and tv production .\n(iteration 1)\nnothing to delete, nothing to insert >> \ninsert >>\ndelete >>\n[Terminate]\nthe t@@ v@@ r made constant eﬀorts to reduce expenditure on \nstaﬀ and tv production .\nMT\nthe t@@ v@@ r administration has constantly constantly eﬀorts to \ncut spending on personnel and tv production .\nthe t@@ v@@ r administration has constantly made eﬀorts to cut \nspending on personnel and tv production .\ndelete >>\ninsert >>\nAPE (zero-shot on PBMT)\nthe t@@ v@@ r administration administration has making constant \neﬀorts to reduce expenditure on staﬀ and tv production .\nthe t@@ v@@ r administration administration has making constant \neﬀorts to reduce expenditure on staﬀ and tv production .\nthe t@@ v@@ r administration has made constant eﬀorts to \nreduce expenditure on staﬀ and tv production .insert >>\ndelete >>\n(iteration 2)\n(iteration 2)\nFigure 11: An example for machine translation and zero-shot post-editing over a PBMT system’s\noutput on WMT’16 Ro-En with the Levenshtein Transformer (LevT) trained for MT. It is clear to ﬁnd\nthat, the pre-trained LevT can directly adapt to the PBMT’s output and have a different reﬁnement\nresults compared to translate from scratch.\n17",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7700290083885193
    },
    {
      "name": "Transformer",
      "score": 0.7362537384033203
    },
    {
      "name": "Automatic summarization",
      "score": 0.7102881073951721
    },
    {
      "name": "Levenshtein distance",
      "score": 0.646846354007721
    },
    {
      "name": "Machine translation",
      "score": 0.6304029822349548
    },
    {
      "name": "Scratch",
      "score": 0.5900126099586487
    },
    {
      "name": "Artificial intelligence",
      "score": 0.49103307723999023
    },
    {
      "name": "Autoregressive model",
      "score": 0.4244653582572937
    },
    {
      "name": "Programming language",
      "score": 0.17725679278373718
    },
    {
      "name": "Voltage",
      "score": 0.15911197662353516
    },
    {
      "name": "Engineering",
      "score": 0.08890709280967712
    },
    {
      "name": "Mathematics",
      "score": 0.07816728949546814
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Econometrics",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 186
}