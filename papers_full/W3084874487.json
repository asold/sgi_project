{
  "title": "Contrastive Triple Extraction with Generative Transformer",
  "url": "https://openalex.org/W3084874487",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2544540884",
      "name": "Hongbin Ye",
      "affiliations": [
        "Knowledge Foundation",
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2132377640",
      "name": "Ningyu Zhang",
      "affiliations": [
        "Knowledge Foundation",
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2099075082",
      "name": "Shumin Deng",
      "affiliations": [
        "Zhejiang University",
        "Knowledge Foundation"
      ]
    },
    {
      "id": "https://openalex.org/A3092821652",
      "name": "Mosha Chen",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2475478252",
      "name": "Chuanqi Tan",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A1936961387",
      "name": "Fei Huang",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2114954316",
      "name": "Huajun Chen",
      "affiliations": [
        "Knowledge Foundation",
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2544540884",
      "name": "Hongbin Ye",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2132377640",
      "name": "Ningyu Zhang",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2099075082",
      "name": "Shumin Deng",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2114954316",
      "name": "Huajun Chen",
      "affiliations": [
        "Zhejiang University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2799125718",
    "https://openalex.org/W2138627627",
    "https://openalex.org/W2987499785",
    "https://openalex.org/W3023166997",
    "https://openalex.org/W2905462022",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2945260553",
    "https://openalex.org/W6843761164",
    "https://openalex.org/W6762471145",
    "https://openalex.org/W2783378231",
    "https://openalex.org/W2741956709",
    "https://openalex.org/W6697155078",
    "https://openalex.org/W3094909070",
    "https://openalex.org/W2134033474",
    "https://openalex.org/W2739722817",
    "https://openalex.org/W2515462165",
    "https://openalex.org/W2229639163",
    "https://openalex.org/W2020278455",
    "https://openalex.org/W1604644367",
    "https://openalex.org/W6679257740",
    "https://openalex.org/W2899996857",
    "https://openalex.org/W3020923281",
    "https://openalex.org/W2251135946",
    "https://openalex.org/W2989790118",
    "https://openalex.org/W2798734500",
    "https://openalex.org/W4323063355",
    "https://openalex.org/W3213368993",
    "https://openalex.org/W2984616234",
    "https://openalex.org/W2890299555",
    "https://openalex.org/W2774383346",
    "https://openalex.org/W3034408002",
    "https://openalex.org/W2626837907",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2768957049",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2414484917",
    "https://openalex.org/W3085190015",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W2971274815",
    "https://openalex.org/W2798463315",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2296283641",
    "https://openalex.org/W2946375144",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2739874095",
    "https://openalex.org/W2465041517",
    "https://openalex.org/W3012542822",
    "https://openalex.org/W2919278763",
    "https://openalex.org/W2951211963",
    "https://openalex.org/W2808142148",
    "https://openalex.org/W3035317912",
    "https://openalex.org/W2963620441",
    "https://openalex.org/W4297933772",
    "https://openalex.org/W2996825178",
    "https://openalex.org/W2739945392",
    "https://openalex.org/W2181042685",
    "https://openalex.org/W2997876626",
    "https://openalex.org/W3118855211",
    "https://openalex.org/W3080532247",
    "https://openalex.org/W2962913831",
    "https://openalex.org/W3116645343",
    "https://openalex.org/W3034617555",
    "https://openalex.org/W3103673392",
    "https://openalex.org/W2971167892",
    "https://openalex.org/W3034863243",
    "https://openalex.org/W2991550749",
    "https://openalex.org/W2964167098",
    "https://openalex.org/W2981861606",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2963653592",
    "https://openalex.org/W3102756401",
    "https://openalex.org/W3012584427",
    "https://openalex.org/W2989902860",
    "https://openalex.org/W2131774270",
    "https://openalex.org/W3088717877",
    "https://openalex.org/W3134559341"
  ],
  "abstract": "Triple extraction is an essential task in information extraction for natural language processing and knowledge graph construction. In this paper, we revisit the end-to-end triple extraction task for sequence generation. Since generative triple extraction may struggle to capture long-term dependencies and generate unfaithful triples, we introduce a novel model, contrastive triple extraction with a generative transformer. Specifically, we introduce a single shared transformer module for encoder-decoder-based generation. To generate faithful results, we propose a novel triplet contrastive training object. Moreover, we introduce two mechanisms to further improve model performance (i.e., batch-wise dynamic attention-masking and triple-wise calibration). Experimental results on three datasets (i.e., NYT, WebNLG, and MIE) show that our approach achieves better performance than that of baselines.",
  "full_text": "Contrastive Triple Extraction with Generative Transformer\nHongbin Ye1;2*, Ningyu Zhang1;2\u0003y, Shumin Deng1;2, Mosha Chen3, Chuanqi Tan3,\nFei Huang3, Huajun Chen1;2†\n1 Zhejiang University\n2 AZFT Joint Lab for Knowledge Engine\n3 Alibaba Group\nfyehb,zhangningyu,231sm,huajunsirg@zju.edu.cn, fchenmosha.cms,chuanqi.tcq,f.huangg@alibaba-inc.com\nAbstract\nTriple extraction is an essential task in information extraction\nfor natural language processing and knowledge graph con-\nstruction. In this paper, we revisit the end-to-end triple ex-\ntraction task for sequence generation. Since generative triple\nextraction may struggle to capture long-term dependencies\nand generate unfaithful triples, we introduce a novel model,\ncontrastive triple extraction with a generative transformer.\nSpeciﬁcally, we introduce a single shared transformer mod-\nule for encoder-decoder-based generation. To generate faith-\nful results, we propose a novel triplet contrastive training ob-\nject. Moreover, we introduce two mechanisms to further im-\nprove model performance (i.e., batch-wise dynamic attention-\nmasking and triple-wise calibration). Experimental results on\nthree datasets (i.e., NYT, WebNLG, and MIE) show that our\napproach achieves better performance than that of baselines.\nIntroduction\nTriple extraction is an essential information extraction task\nfor natural language processing (NLP) and knowledge graph\n(KG), which is used to detect pairs of entities and their rela-\ntions from unstructured text. Consider this sentence: “Paris\nis known as the romantic capital of France.” From this, an\nideal triple extraction would comprise hParis, Capital\nof,\nFrancei, in which Capital of is the relation of Paris and\nFrance.\nResearchers have proposed pipeline approaches in the\npast (Lample et al. 2016; Zeng et al. 2015) in which they typ-\nically deconstructed the triple extraction problem into two\nseparate tasks: named-entity recognition (NER) (used to ex-\ntract entities) and relation classiﬁcation. Thus, they ﬁrst rec-\nognized the entities; then, they predicted their relationships.\nUnfortunately, this and similar pipeline approaches suffer\ndrawbacks (Roth and Yih 2007) in that they omit the evident\ncorrelations between entity recognition and relation extrac-\ntion tasks, resulting in error propagation.\nRecently, several neural-network-based models (Zeng\net al. 2018a) have been proposed to jointly extract entities\nand relations from sentences. These models use a parameter-\nsharing mechanism to extract entities and relations from\n*Equal contribution and shared co-ﬁrst authorship.\n†Corresponding author.\nCopyright © 2021, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nInput The United States President Trump was raised\nin the borough of Queens in New York City,\nand lived there until age 13.\nOutput Trump!president!of!United!States!\n[S2S SEQ]!Trump!born!in!Queens!\n[S2S SEQ]!Trump!live!in!Queens\nGold\n(Trump, president of, United States)\n(Trump, born in, Queens)\n(Trump, live in, Queens)\nNegative\n(Trump, president of, Queens)\n(Trump, born in, 13)\n(Trump, live in, 13)\nTable 1: Contrastive triple extraction as sequence genera-\ntion. We encourage the model to generate gold triples and\ndoes not generate negative ones.\nthe same network. Apart from those approaches, Zeng\net al. (2018b) proposed a recurrent neural-network-based\nencoder-decoder model (i.e., CopyRE) to extract triples with\noverlapping entities. Such end-to-end generative triple ex-\ntraction not only directly obtain the triples and mitigate the\nerror propagation issue, but also enable the generation of out\nof domain entities and relations in a T5-style (Raffel et al.\n2019) (text-to-text). Besides, Zeng, Zhang, and Liu (2020)\nproposed a multi-task learning framework equipped with a\ncopy mechanism (i.e., CopyMTL) to allow the prediction\nof multi-token entities. Nayak and Ng (2019) introduced\na representation scheme for triples and a pointer-network-\nbased decoding approach, which further improved the per-\nformance of CopyRE.\nEncoder-decoder models are powerful tools that have seen\nsuccess in many NLP tasks, including machine translation\n(Cho et al. 2014), and open information extraction (Zhang,\nDuh, and Van Durme 2017). Although signiﬁcant progress\nhas been achieved, there remain two key problems with the\nexisting methods. First, owing to the intrinsic shortfalls of\nrecurrent neural networks (RNN), they cannot capture long-\nterm dependencies, which results in the loss of important in-\nformation otherwise reﬂected in the sentence. Such a draw-\nback prevents the model from being applied to longer texts.\nTheThi rty-Fi fth AAA ICon ferenceon A rti fi ci al Intellig ence(AAAI-21)\n14257\nSecond, there is a scarcity of work that has focused on gen-\nerating faithful triples. As a previous study (Zhu et al. 2020)\nindicated, a sequence-to-sequence architecture can generate\nunfaithful sequences that create contradictions of meaning.\nFor example, given the sentence “The United States Presi-\ndent Trump was raised in the borough ofQueens in New York\nCity, and lived there until age 13,” the model could gener-\nate the fact “(Trump, born\nin, Queens).” Although logically\ntrue, we cannot ﬁnd direct evidence from the given sentence\nto support it.\nTo address these issues, we introduce a framework of\nContrastive triple extraction with Generative Transformer\n(CGT), which is a single shared transformer module with a\ntriplet contrastive object that supports encoder-decoder gen-\neration. To begin with, we concatenate the input sequence\nwith the target sequence using a separator token and lever-\nage partial causal masking (Du, Rush, and Cardie 2020) to\ndistinguish the encoder-decoder representations. Our model\nrequires no additional parameters beyond those of the pre-\ntrained model. Then, we introduce a novel triplet contrastive\nlearning object, which utilizes ground-truth triples as posi-\ntive instances and leverages random token sampling to con-\nstruct corrupt triples as negative instances. To jointly opti-\nmize the triple generation and contrastive object, we intro-\nduce a batch-wise dynamic attention-masking mechanism,\nwhich allows us to dynamically choose different objects and\njointly optimize tasks. Lastly, we introduce a novel triple-\nwise calibrating algorithm to ﬁlter out any remaining false\ntriples in the inference stage.\nThe contributions of this work are as follows:\n• We revisit triple extraction as a sequence generation task\nand introduce a novel CGT model. In light of the added\nextraction capability, CGT requires no additional param-\neters beyond those found in the pre-trained language\nmodel.\n• We introduce two mechanisms to further improve model\nperformance (i.e., batch-wise dynamic attention-masking\nand triple-wise calibration). The ﬁrst enables joint op-\ntimization of different objects, and the second ensures\nfaithful inference.\n• We evaluate CGT on three benchmark datasets. Our\nmodel empirically outperforms other substantially strong\nbaseline models. We also demonstrate that CGT is bet-\nter than existing triple extraction approaches at captur-\ning long-term dependencies, thus, achieving better perfor-\nmance with long sentences.\nRelated Work\nTriple Extraction\nTwo main methods have been proposed for triple extraction:\npipeline (Nadeau and Sekine 2007; Bunescu and Mooney\n2005; Lin et al. 2016; Lin, Liu, and Sun 2017; Li et al. 2020;\nWang et al. 2020) and joint learning (Miwa and Bansal 2016;\nKatiyar and Cardie 2017; Cao et al. 2017a; Zhang et al.\n2020a; Dai et al. 2019a). A pipeline method ﬁrst extracts en-\ntities, then it identiﬁes their relations (Hendrickx et al. 2019;\nZeng et al. 2015; Wu et al. 2021). Although pipeline models\nhave achieved great progress (Zhang et al. 2018; He et al.\n2018; Zhang et al. 2019a, 2020c), they introduce an error\npropagation problem (Li and Ji 2014), which does harm to\nthe overall performance.\nBecause joint learning can implicitly model correlations\nbetween tasks, many approaches have been proposed. Bek-\noulis et al. (2018) formulated the triple extraction task as a\nmulti-head selection problem. Takanobu et al. (2019) pro-\nposed a hierarchical reinforcement-learning framework for\ntriple extraction. Chen et al. (2019) utilized triplet atten-\ntion to exploit connections between the relation and its cor-\nresponding entity pairs. Dai et al. (2019b) introduced a\nposition-attention mechanism to produce different tag se-\nquences for triple extraction.Wei et al. (2020a) revisited the\nrelational triple extraction task and proposed a novel cascade\nbinary-tagging framework. Apart from those approaches,\nZeng et al. (2018a) proposed CopyRE, a joint model based\non a copy mechanism, which converted the joint extraction\ntask into a triplet-generation task. Other researchers intro-\nduced multiple strategies, such as multi-task learning (Zeng,\nZhang, and Liu 2020) and one-word generation (Nayak and\nNg 2019) to improve CopyRE. For the ﬁrst time, we utilize\nthe transformer as an encoder-decoder architecture to extract\ntriples from sentences.\nNatural Language Generation\nNatural language generation has been intensively studied in\nthe recent literature. Most models employed an encoder-\ndecoder architecture (i.e., seq2seq) using RNNs (Schus-\nter and Paliwal 1997; Zhang et al. 2020b; Krause et al.\n2020). Recently, owing to the powerful representation abil-\nity of transformers, several researchers have introduced\ntransformer-based natural language generation methods. Gu,\nWang, and Zhao (2019) developed the Levenshtein trans-\nformer, a new partially autoregressive model, which is de-\nvised for a more ﬂexible and amenable sequence genera-\ntion. Chen et al. (2020) present a novel approach, Condi-\ntional Masked Language Modeling (C-MLM), to enable the\nﬁnetuning of BERT (Devlin et al. 2019) on target generation\ntasks. Dong et al. (2019) proposed a new uniﬁed pre-trained\nlanguage model with different masking strategies, which can\nbe used for both language understanding and generation. Du,\nRush, and Cardie (2020) proposed a generative transformer-\nbased encoder-decoder framework for document-level infor-\nmation extraction.\nSince the generation procedure was unconditional, it was\nnon-trivial to judge the faithfulness of the generated se-\nquence. Zhang et al. (2019b) approached the factual cor-\nrectness problem in the medical domain, where the space\nof facts was limited and could be depicted with a descriptor\nvector. Cao et al. (2017b) extracted relational information\nfrom an article and mapped it to a sequence as input to the\nencoder. The decoder then attended to both article tokens\nand their relations. Gunel et al. (2020) employed an entity-\naware transformer structure to boost the factual correctness\nof abstractive summarization, where the entities came from\nthe Wikidata knowledge graph. By comparison, our model\nutilizes contrastive learning to encourage the model to im-\nplicitly generate faithful triples.\n14258\nToken Embedding\nPosition Embedding\nSegment Embedding\n[CLS]The United States President\nTrump was raised in the borough\nof Queens in New York City, and\nlived there until age 13.  [SEP]\nTransformer Encoder 1\nTransformer Encoder N\nTransformer Encoder 1\nh1 h2 h3 h4 h5 h6 S1\nS2\nS1 S2\nGenerative Transformer\nTriplet Contrastive Learning\n[Negative triple]:\n(Trump, president_of, Queens) \n(Trump, born_in, 13) \n(Trump, live_in, 13) \n[Positive triple]:\n(Trump, president_of, United States)\n(Trump, born_in, Queens)\n(Trump, live_in, Queens) \n[SOS]Trump->president->of->United->States->\n[S2S_SEQ]->Trump, born->in->Queens->\n[S2S_SEQ] ->Trump->live->in->Queens[EOS]\nInput Encoder\nFigure 1: The architecture of Contrastive triple extraction withGenerative Transformer (CGT). The top-right component refers\nto the generative transformer, and the bottom-right component represents triplet contrastive learning. Those two parts are\noptimized jointly. The left is the input encoder (best viewed in color).\nOverview\nPreliminary\nWe treat triple extraction as a sequence-to-sequence task to\nbetter model the cross dependencies between entities and re-\nlations. We deﬁne the input text and output triples as source\nand target sequence. As shown in Figure 1, the source se-\nquence simply consists of the tokens of the input sentence\nlike “[CLS] The United States President Trump was raised in\nthe borough of Queens ...[SEP]”. We concatenate the triples\nfor each entity/relation separated by a special token token\n[S2S\nSEQ] as the target sequence. We also add the begin-\nning ([SOS]) and end ([EOS]) tokens for each target se-\nquence as:\n[SOS]h(1);r(1);t(1) ::: [S2S\nSEQ]\nh(2);r(2);t(2) ::: [S2S SEQ]\nh(3);r(3);t(3) ::: [S2S SEQ]\n:::\nh(N);r(N);t(N) ::: [EOS];\nwhere hi, ri, and ti refer to the i-th generated head entity,\nrelation, and tail entity.\nFramework\nWe denote the sequence of input source tokens asx0, x1, ...,\nxm and the sequence of target tokens as y0, y1, ..., yn. Note\nthat the generated tokens contain all extracted triples. Our\nmodel CGT consists of three components, as follows:\nInput Encoder.We utilize the input representation which\nis the same as BERT (Devlin et al. 2019) and tokenize texts\nby WordPiece (Yonghui et al. 2016). We compute the repre-\nsentation by summing the corresponding token embedding,\nposition embedding, and segment embedding.\nGenerative Transformer.We use partial causal masking\nto distinguish the encoder-decoder representations. For in-\nference, we leverage the beam search (Wiseman and Rush\n2016) to generate multiple triples.\nTriplet Contrastive Learning. We introduce a triplet\ncontrastive object to enhance the faithfulness of generated\ntriples. We introduce a batch-wise dynamic attention mask-\ning mechanism for joint optimization. We also provide a\ntriple-wise calibrating algorithm for the faithful triple gen-\neration.\nOur Model\nInput Encoder\nGiven the input text x, we add a special start-of-sequence\ntoken [SOS] at the beginning of the target input. We use\nthe representation of the whole input for the output vector.\nFurthermore, we append a special token, namely, end-of-\nsequence [EOS], to the end of each output sequence. The\n14259\n[EOS] token is used as a special token to terminate the de-\ncoding process for the triple generation.\nThe input representation is the same as the one used for\nBERT (Devlin et al. 2019). We tokenize the text to subword\nunits using WordPiece (Yonghui et al. 2016). For example,\nthe word, “forecasted,” is split into “forecast” and “##ed,”\nwhere “##” refers to the pieces belong to one word. We com-\npute each input token vector representation by summing the\ncorresponding token embedding, position embedding, and\nsegment embedding.\nGenerative Transformer\nWe utilize a transformer architecture as a backbone to en-\ncode contextual features which is consist of stacked self-\nattention layers. In this paper, we use a 12-layer trans-\nformer architecture as a single shared transformer module\nfor encoder-decoder-based generation. Having the input vec-\ntors, fsigjLj\ni=1, we ﬁrstly feed them intoH0 =\n\u0002\ns1;\u0001\u0001\u0001 ;sjLj\n\u0003\n.\nThen, we use the transformer to encode the input:\nHl = Transformerl\n\u0000\nHl\u00001\u0001\n: (1)\nThere are multiple self-attention heads in each trans-\nformer block which are used to aggregate the output vec-\ntors of the previous layer. We compute the output of a self-\nattention head, Al, in the l-th transformer layer as follows:\nQl = Hl\u00001WQ\nl ; Kl = Hl\u00001WK\nl : (2)\nMij =\n\u001a\n0; allow to attend\n\u00001; prevent from attending (3)\nAl = softmax\n\u0012QlK>\npdk\n+ M\n\u0013\u0000\nHl\u00001Vl\n\u0001\n; (4)\nwhere Ql;Kl;Vl 2Rdh\u0002dk are matrices which are the\nprojection of the the previous layer’s output. The mask ma-\ntrix, M 2RjLj\u0002jLj, is aimed to control the context that can\nbe attended by the token. Speciﬁcally, we leverage differ-\nent mask matrices, M, when computing its contextualized\nrepresentation. As illustrated by the examples in Figure 1,\nfor triple generation, we leverage partial causal masking,\nin which the upper right part is set to \u00001to block atten-\ntion from the source segment to the target segment; the left\npart of M is set to all 0s which indicates that the tokens is\nable to attend to the ﬁrst segment. We utilize cross-entropy\nlossgeneration to optimize the triple generation procedure.\nWe also utilize masking strategies in which the elements of\nthe mask matrix are all 0s for triplet contrastive learning.\nDetails are provided in the next section. Formally, the gener-\native transformer obtain contextualized representations and\noptimize the following object:\n^x0;^x1;:::; ^xm;^y0 :::; ^yn\n= Transformer (x0;x1;:::; xm;y0;:::; yn) (5)\nlossgeneration =\nX\n(\nmX\n1\nxilog(^xi) +\nnX\n1\nyilog(^yi)) (6)\nAlgorithm 1Triplet Contrastive Learning\n1: Require: Train instances X = x1;:::;x N , labels Y =\ny1;:::;y N , batch size k, POS = \b, NEG = \b, tem-\nperature t\n2: while i\u0014N=k do\n3: batch = [(x;y)1;::; (x;y)k]\n4: for (x;y)j in batch do\n5: POS = decompose triple(yj)\n6: for posin POS do\n7: neg = random permute entity(pos)\n8: l pos = Contrastive Classify(x,pos)\n9: l neg = Contrastive Classify(x,neg)\n10: z = cat([l pos, l neg], dim=1)\n11: labels = zeros(2)\n12: loss = CrossEntropyLoss(z/t, labels)\n13: loss.backward()\n14: update(Contrastive Classiﬁer.params)\n15: return DataLoader\nTriplet Contrastive Learning\nThe previous generation-based approach usually neglects\nthe fact that triple should be faithful and consistent with the\ninput sentence. For example, given the instance “Obama was\nborn in Honolulu,” we should engorge the model to gener-\nate triples like “(Obama, was\nborn, Honolulu)” rather than\n“(Obama, live in, Honolulu),” though the latter may be cor-\nrect but cannot be induced from the given sentence. Moti-\nvated by this, we introduce a triplet contrastive learning to\nenhance the faithfulness of generated triples.\nTo be speciﬁc, we leverage the triple contrastive learn-\ning as binary classiﬁcation with all 0s masking. We use gold\ntriples as positive instances and generate corrupt triples by\nreplacing one entity with random tokens in the instances.\nWe use those corrupt triples as negative instances. We con-\ncatenate the input sentence with only one triple as x0, x1,\n..., xm[SEP];hi;ri;ti and feed it into the input encoder.\nWe utilize the representation of [CLS] with an MLP layer to\ncompute classiﬁcation logits z. We utilize cross-entropy for\noptimization with losscontrastive:\nlosscontrastive =\nX\n(z+\ni log(^z+\ni )+(1 \u0000z\u0000\ni )log((1\u0000^z\u0000\ni )))\n(7)\nwhere ^z+\ni and ^z\u0000\ni are the positive and negative logits,\nrespectively. Formally, the triplet contrastive learning algo-\nrithm for triple extraction is shown as Algorithm 1.\nTraining and Inference Details\nDuring the training stage, the entities and relations are all to-\nkens from the vocabulary, whereas [S2S\nSEQ], [SOS], and\n[EOS] are all unused tokens (e.g., [unused1]). We split the\nentity and relation label mentions with different tokens dur-\ning the data preprocessing procedure, meaning that the entity\nand relation may contain multiple tokens.\nNote that triplet contrastive learning and triple generation\nare two different tasks, and optimizing them jointly is non-\ntrivial, owing to the leakage of generated labels. For exam-\n14260\nAlgorithm 2Batch-wise Dynamic Attention Masking\n1: Require: Train instances X = x1;:::;x N , labels Y =\ny1;:::;y N , negative instances Y0, batch size ksampling\nratio \r\n2: while i\u0014N=k do\n3: old batch = [(x;y;y 0)1;::; (x;y;y 0)k]\n4: for (x;y;y 0)j in old batch do\n5: condition = Bernoulli(\r)\n6: if condition == 1 then\n7: instance = Partial Causal Mask((x;y;y 0)j)\n8: else\n9: instance = All Zero Mask((x;y;y 0)j)\n10: batch  batch \\instance\n11: DataLoader  DataLoader \\batch\n12: batch = \b\n13: return DataLoader\nple, if we optimize generation and contrastive learning with\nthe same instance, the model can see all of the tokens be-\ncause of the all 0s masking. To address this issue, we intro-\nduce batch-wise dynamic attention masking. With this, we\nsample instances from a Bernoulli distribution as generation\ninstances, and the rest is sampled as contrastive learning sen-\ntences. Formally, the algorithm is shown as Algorithm 2.\nThe overall optimization object is as follows:\nloss = lossgenerative + \u000blosscontrastive (8)\nwhere \u000bis the hyperparameter to balance different objects.\nDuring the inference stage, we ﬁrst generate triplet se-\nquences via beam search (Wiseman and Rush 2016). Then,\nwe introduce a triple-wise calibrating algorithm to ﬁlter-\nout unfaithful triples. We calculate the matching score with\nthe contrastive classiﬁer and ﬁlter out those triples with the\nmatch\nscore<\u0012 . Besides, we also leverage heuristic rules\nto generate reasonable triples such as the relation should be\nfollowed by the head entities.\nExperiment\nDataset\nWe conducted experiments on three benchmark datasets:\nNew York Times (NYT), WebNLG1, and MIE 2. The NYT\ndataset is produced using a distant supervision method and is\nwidely used for triplet extraction (Riedel, Yao, and McCal-\nlum 2010). It contains 56,195 sentences for training, 5,000\nsentences for validation, and 5,000 sentences for test. The\nWebNLG dataset (Gardent et al. 2017) was used for natu-\nral language generation, but was later used for triplet ex-\ntraction (Zeng et al. 2018a). It consists of 5,019/500/703\ninstances for training, validation, and testing, respectively.\nMIE (Zhang et al. 2020d) is a large-scale Chinese dialogue\ninformation extraction dataset for the medical domain. It\ncontains 800 instances for training, 160 instances for val-\nidation, and 160 instances for testing. We used the origi-\n1https://github.com/weizhepei/CasRel\n2https://github.com/nlpir2020/MIE-ACL-2020\nDataset NYT WebNLG MIE\nDomain News Web Medical\nRelation 24 246 343\nTriplets 104,518 12,863 18,212\nTable 2: Statistics of four datasets in the domain, the number\nof relation types, and the triple number.\nnal dataset splitting for NYT, WebNLG, and MIE. Detailed\nstatistics of the three datasets are shown in Table 2.\nSettings\nWe utilized UniLM-base-uncased for both English 3 and\nChinese4 datasets. We utilized Pytorch to implement\nour CGT model and conducted experiments using four\nNvidia 1080-Ti graphical processing units. We employed\nAdam (Kingma and Ba 2014) as the optimizer. The initial\nlearning rate was set to 2e-5, and we reduced the rate by\n20% at every eight epochs. The batch size was 64 for En-\nglish and 32 for Chinese, and the total number of epochs\nwas 50 for all datasets. The beam size was set to 4, \u000bwas\nset to 0.1, \r was set to 0.2, and \u0012 was set to 0.6. We care-\nfully tuned the hypermeters on the valid set (Detailed search\nspace in supplementary materials).\nBaselines and Evaluation Metrics\nWe compared the performance of CGT with various baseline\nmodels and evaluated the performance with precision, recall,\nand F1 score. CGT(Random) and CGT(UniLM) refer to the\nmodel initialized randomly, and the model initialized with\nUniLM, respectively.\nGenerative Baseline Models:\nCopyRE (Zeng et al. 2018a) is a Seq2Seq learning frame-\nwork having a copy mechanism wherein multiple decoders\nare applied to generate triples to handle overlapping rela-\ntions.\nPNDec (Nayak and Ng 2019) provides two novel ap-\nproaches using encoder-decoder architecture for triples hav-\ning multiple tokens.\nCopyMTL (Zeng, Zhang, and Liu 2020) proposes a mul-\ntitask learning framework used to complete the entities.\nExtractive Baselines:\nTagging (Zheng et al. 2017b) is an end-to-end method\nthat uses a novel tagging scheme.\nHRL (Takanobu et al. 2019) addresses relation extrac-\ntions by regarding related entities as the arguments of the\nrelation via hierarchical reinforcement learning.\nMrMep (Chen et al. 2019) is an approach that utilizes\ntriplet attention to exploit connections between relations and\ntheir corresponding entity pairs.\nCasRel (Wei et al. 2020a) is an approach that models re-\nlations as functions, which map subjects to objects in a sen-\ntence.\n3https://github.com/microsoft/unilm\n4https://github.com/YunwenTechnology/Unilm\n14261\nModel NYT WebNLG\nP R F P R F\nExtractive\nTagging (Zheng et al. 2017a) 61.5 41.4 49.5 - - -\nHRL(Takanobu et al. 2019) 71.4 58.6 64.4 53.8 53.8 53.8\nMrMep (Chen et al. 2019) 77.9 76.6 77.1 69.4 77.0 73.0\nCasRel (Wei et al. 2020b) 89.7 89.5 89.6 93.4 90.1 91.8\nGenerative\nCopyRE (Zeng et al. 2018a) 61.0 56.6 58.7 37.7 36.4 37.1\nPNDec (Nayak and Ng 2019) 80.6 77.3 78.9 38.1 36.9 37.5\nCopyMTL (Zeng, Zhang, and Liu 2020) 75.7 68.7 72.0 58.0 54.9 56.4\nOurs\nCGT(Random) 90.8 77.7 83.7 87.6 70.5 78.1\nCGT(UniLM) 94.7* 84.2 89.1 92.9* 75.6 83.4\nw/o contrastive 87.3 81.5 84.3 94.6 70.5 80.8\nTable 3: Main results of NYT and WebNLG. The top section refers to the extractive models, the middle section indicates the\ngenerative approaches, the bottom is our model with different settings. * indicates pvalue <0:01 for a paired t-test evaluation.\nModel P R F1\nBi-LSTM 53.13 49.46 50.69\nMIE-multi 70.24 64.96 66.40\nCGT(random) 70.75 66.96 68.80\nCGT(UniLM) 80.53 78.83 79.42\nTable 4: Main results on the MIE dataset.\nBi-LSTM (Zhang et al. 2020d) is a baseline approach that\nutilizes a bi-directional long-short term memory network for\ninformation extraction.\nMIE-multi (Zhang et al. 2020d) is another baseline\nmodel that uses a max-pooling operation to obtain the ﬁnal\nscore, considering the turn-interaction.\nMain Results\nFrom Table 3, we observe that our approach achieved sig-\nniﬁcant improvements compared with all generation-based\nbaseline models for both NYT and WebNLG datasets. Our\nCGT model had a relative 10.2 F1 score improvement on\nNYT compared with PNDec, and a relative 27.0 F1 score\nimprovement on NYT compared with CopyMTL, illustrat-\ning the power of our proposed model. Our approach also ob-\ntained comparable results compared with extractive models,\nsuch as CasRel. Note that the search space of the generative\nmodel was much larger than the extractive ones, which indi-\ncates that the generative model was challenging to optimize\nthan extractive approaches. In contrast, generative methods\ncan generate triples beyond the entity and relation domain,\nwhich is promising for the open domain setting. The empir-\nical results reveal that the generative approach could obtain\ncomparable performance with extractive models, motivating\nfuture research directions.\nFrom Table 4, we observe that our approach achieved sig-\nniﬁcant improvements (relative 13.02 F1 score) compared\nwith all baselines on the MIE dataset. MIE is a dialogue-\nbased information-extraction dataset that is challenging to\noptimize. Thus, we argue that our CGT can implicitly model\nthe relations among entities, boosting performance.\nAblation Study\nWe conducted ablation studies further to demonstrate the ef-\nﬁcacy of different strategies in our model. From Table 3, we\nnotice that the performance decayed without contrastive ob-\nject, which illustrates that triplet contrastive learning can en-\nhance the faithfulness of generated triples, thus boosting the\nperformance. We also observe that our approach with ran-\ndom initialization CGT(Random) achieves signiﬁcantly bet-\nter performance than generative baselines on all three bench-\nmark datasets, which further indicates that our improve-\nments are not only from the pre-trained language model but\nalso the model architecture itself.\nAnalysis\nTo better analyze the performance of our proposed CGT\nmodel, we conducted a detailed analysis and attempted to\nanswer the questions of whether CGT can capture long-term\ndependence or not. Intrusively, transformers having self-\nattention can better capture long-term dependencies than\nRNNs. To investigate this issue, we evaluated the instances\nat different lengths. From Figure 2, we notice that all mod-\nels have a performance decay when the sentence length in-\ncreases, which indicates that the sequence generation is chal-\nlenging when the input sentence is long. We observe that\nour approach could obtain better performance than that of\nCopyRE when the sentence length increased. When the sen-\ntence was longer than 60, CopyRE archived worse perfor-\nmance, while CGT performed relatively better. This demon-\nstrates that the proposed approach can capture long-term de-\npendencies, compared with RNN-based approaches.\nError Analysis\nTo further analyze the drawbacks of our approach and pro-\nmote future works of triple extraction, we select instances\nand conduct error analysis. We random select incorrect in-\nstances and classify them into three categories bellows, as\nshown in Table 5:\n14262\n0 10 20 30 40 50 60 70 80 90 100 110\n0\n20\n40\n60\n80\n100\n120f1(%)\nsentence length\n CGT\n CopyRE\n(a) NYT\n0 10 20 30 40 50 60 70\n0\n20\n40\n60\n80\n100\n120f1(%)\nsentence length\n CGT\n CopyRE (b) WebNLG\n0 100 200 300 400 500 600\n0\n20\n40\n60\n80\n100\n120f1(%)\nsentence character length\n CGT\n MIE-multi (c) MIE\nFigure 2: Model performance #sentence length.\nInstance\ninstance #1 Batchoy\nis originates from the Philippines and served as a soup.Its main ingredients are noodles,\npork organs, vegetables, chicken, shrimp and beef.\ngenerated triple: hBatchoy, location, Philippinesi\nground truth: hBatchoy, country, Philippinesi\ninstance #2 Alan Shepard\nwas a crew member of NASA operated Apollo 14 who died in California which\nis represented by Dianne Feinstein.\ngenerated triple: hShepard, deathPlace, Californiai\nground truth: hAllan Shepard, deathPlace, Californiai\ninstance #3 Saranac Lak\ne, which is served by Adirondack Regional Airport, is part of Harrietstown, Essex\nCounty, New York, US.\ngenerated triple: hAirport, cityServed, New Yorki\nground truth: hAirport, cityServed, Yorki\nTable 5: Error anslysis.\nDistract Context.As instance #1 shows, we observe that\nour approach may fail to those ambiguous contexts that may\nbe expressed in a similar context but differ only in the ﬁne-\ngrained type of entities. We argue that this may be caused\nby the unbalanced learning problems that models tend to\njudge the sentence with similar context to high-frequency\nrelations.\nWrong Boundaries. As instance #2 shows, generated\ntriples had incorrect boundaries, which indicates the difﬁ-\nculty of entity recognition during triple extraction. We argue\nthat since our approach is an end-to-end generation method,\nit is challenging to capture ﬁne-grained entity boundaries\nwithout sequence token information.\nWrong Triples.As instance #3 shows, many generated\ntriples had entities that did not exist in the gold-standard set.\nGenerally, this occurs with sentences having multiple triples.\nThe WebNLG datasets are noisy, and several of its cases pro-\nduced incorrect results. We leave this for future works with\nmore suitable benchmarks.\nConclusion and Future Work\nIn this paper, we revisited triple extraction as a sequence\ngeneration task, which jointly extracts entities and relations.\nTo address the long-term dependence and faithfulness is-\nsues, we proposed a novel CGT model to generate faith-\nful triples. To the best of our knowledge, we are the ﬁrst to\nintegrate sequence generation with contrastive learning for\ninformation extraction, which may inspire future research\ndirections and motivate new ideas. Experimental results on\nthree datasets demonstrated the efﬁcacy of our approach.\nIn the future, we will utilize stronger transformer architec-\ntures, such as Longformer (Beltagy, Peters, and Cohan 2020)\nto generate relational knowledge from documents. We will\nalso delve into injection ontology knowledge using condi-\ntion generation methods. It will also be useful to apply our\napproach to other scenarios, such as event extractions.\nAcknowledgments\nWe want to express gratitude to the anonymous re-\nviewers for their hard work and kind comments.\nWe thank Ning Ding for helpful discussions and\nfeedback on this paper. This work is funded by\n2018YFB1402800/NSFC91846204/NSFCU19B2027.\nReferences\nBekoulis, G.; Deleu, J.; Demeester, T.; and Develder, C.\n2018. Joint entity recognition and relation extraction as a\n14263\nmulti-head selection problem. Expert Systems with Applica-\ntions 114: 34–45.\nBeltagy, I.; Peters, M. E.; and Cohan, A. 2020. Long-\nformer: The long-document transformer. arXiv preprint\narXiv:2004.05150 .\nBunescu, R. C.; and Mooney, R. J. 2005. A shortest path\ndependency kernel for relation extraction. In Proceedings\nof the conference on human language technology and em-\npirical methods in natural language processing, 724–731.\nAssociation for Computational Linguistics.\nCao, Y .; Huang, L.; Ji, H.; Chen, X.; and Li, J. 2017a. Bridge\nText and Knowledge by Learning Multi-Prototype Entity\nMention Embedding. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), 1623–1633. Vancouver, Canada:\nAssociation for Computational Linguistics. doi:10.18653/\nv1/P17-1149. URL https://www.aclweb.org/anthology/P17-\n1149.\nCao, Z.; Wei, F.; Li, W.; and Li, S. 2017b. Faithful to the\noriginal: Fact aware neural abstractive summarization.arXiv\npreprint arXiv:1711.04434 .\nChen, J.; Yuan, C.; Wang, X.; and Bai, Z. 2019. MrMep:\nJoint Extraction of Multiple Relations and Multiple Entity\nPairs Based on Triplet Attention. In Proceedings of the 23rd\nConference on Computational Natural Language Learning\n(CoNLL), 593–602.\nChen, Y .-C.; Gan, Z.; Cheng, Y .; Liu, J.; and Liu, J. 2020.\nDistilling Knowledge Learned in BERT for Text Generation.\nIn Proceedings of the 58th Annual Meeting of the Associa-\ntion for Computational Linguistics, 7893–7905.\nCho, K.; Van Merri¨enboer, B.; Gulcehre, C.; Bahdanau, D.;\nBougares, F.; Schwenk, H.; and Bengio, Y . 2014. Learning\nphrase representations using RNN encoder-decoder for sta-\ntistical machine translation. arXiv preprint arXiv:1406.1078\n.\nDai, D.; Xiao, X.; Lyu, Y .; Dou, S.; She, Q.; and Wang,\nH. 2019a. Joint extraction of entities and overlapping re-\nlations using position-attentive sequence labeling. In Pro-\nceedings of the AAAI Conference on Artiﬁcial Intelligence ,\nvolume 33, 6300–6308.\nDai, D.; Xiao, X.; Lyu, Y .; Dou, S.; and Wang, H. 2019b.\nJoint Extraction of Entities and Overlapping Relations Using\nPosition-Attentive Sequence Labeling. Proceedings of the\nAAAI Conference on Artiﬁcial Intelligence 33: 6300–6308.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), 4171–4186. Min-\nneapolis, Minnesota: Association for Computational Lin-\nguistics. doi:10.18653/v1/N19-1423.\nDong, L.; Yang, N.; Wang, W.; Wei, F.; Liu, X.; Wang, Y .;\nGao, J.; Zhou, M.; and Hon, H.-W. 2019. Uniﬁed language\nmodel pre-training for natural language understanding and\ngeneration. In Advances in Neural Information Processing\nSystems, 13063–13075.\nDu, X.; Rush, A.; and Cardie, C. 2020. Document-level\nEvent-based Extraction Using Generative Template-ﬁlling\nTransformers. arXiv preprint arXiv:2008.09249 .\nGardent, C.; Shimorina, A.; Narayan, S.; and Perez-\nBeltrachini, L. 2017. Creating training corpora for nlg\nmicro-planning. In 55th annual meeting of the Association\nfor Computational Linguistics (ACL).\nGu, J.; Wang, C.; and Zhao, J. 2019. Levenshtein trans-\nformer. In Advances in Neural Information Processing Sys-\ntems, 11181–11191.\nGunel, B.; Zhu, C.; Zeng, M.; and Huang, X. 2020. Mind\nThe Facts: Knowledge-Boosted Coherent Abstractive Text\nSummarization. arXiv preprint arXiv:2006.15435 .\nHe, Z.; Chen, W.; Li, Z.; Zhang, M.; Zhang, W.; and Zhang,\nM. 2018. SEE: Syntax-aware Entity Embedding for Neural\nRelation Extraction. In Proceedings of AAAI.\nHendrickx, I.; Kim, S. N.; Kozareva, Z.; Nakov, P.;\nS´eaghdha, D. O.; Pad ´o, S.; Pennacchiotti, M.; Romano, L.;\nand Szpakowicz, S. 2019. Semeval-2010 task 8: Multi-way\nclassiﬁcation of semantic relations between pairs of nomi-\nnals. arXiv preprint arXiv:1911.10422 .\nKatiyar, A.; and Cardie, C. 2017. Going out on a limb: Joint\nextraction of entity mentions and relations without depen-\ndency trees. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Volume 1:\nLong Papers), 917–928.\nKingma, D. P.; and Ba, J. 2014. Adam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980 .\nKrause, B.; Gotmare, A. D.; McCann, B.; Keskar, N. S.;\nJoty, S.; Socher, R.; and Rajani, N. F. 2020. Gedi: Generative\ndiscriminator guided sequence generation. arXiv preprint\narXiv:2009.06367 .\nLample, G.; Ballesteros, M.; Subramanian, S.; Kawakami,\nK.; and Dyer, C. 2016. Neural Architectures for Named En-\ntity Recognition. In HLT-NAACL.\nLi, J.; Wang, R.; Zhang, N.; Zhang, W.; Yang, F.; and Chen,\nH. 2020. Logic-guided Semantic Representation Learning\nfor Zero-Shot Relation Classiﬁcation. In Proceedings of\nthe 28th International Conference on Computational Lin-\nguistics, 2967–2978. Barcelona, Spain (Online): Interna-\ntional Committee on Computational Linguistics. URL https:\n//www.aclweb.org/anthology/2020.coling-main.265.\nLi, Q.; and Ji, H. 2014. Incremental joint extraction of en-\ntity mentions and relations. In Proceedings of the 52nd An-\nnual Meeting of the Association for Computational Linguis-\ntics (Volume 1: Long Papers), 402–412.\nLin, Y .; Liu, Z.; and Sun, M. 2017. Neural relation extrac-\ntion with multi-lingual attention. In Proceedings of ACL,\nvolume 1, 34–43.\nLin, Y .; Shen, S.; Liu, Z.; Luan, H.; and Sun, M. 2016. Neu-\nral relation extraction with selective attention over instances.\nIn Proceedings of ACL, volume 1, 2124–2133.\n14264\nMiwa, M.; and Bansal, M. 2016. End-to-End Relation Ex-\ntraction using LSTMs on Sequences and Tree Structures. In\nProceedings of ACL, volume 1, 1105–1116.\nNadeau, D.; and Sekine, S. 2007. A survey of named entity\nrecognition and classiﬁcation.\nNayak, T.; and Ng, H. T. 2019. Effective Modeling of\nEncoder-Decoder Architecture for Joint Entity and Relation\nExtraction. arXiv preprint arXiv:1911.09886 .\nRaffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;\nMatena, M.; Zhou, Y .; Li, W.; and Liu, P. J. 2019. Explor-\ning the limits of transfer learning with a uniﬁed text-to-text\ntransformer. arXiv preprint arXiv:1910.10683 .\nRiedel, S.; Yao, L.; and McCallum, A. 2010. Modeling rela-\ntions and their mentions without labeled text. In Joint Euro-\npean Conference on Machine Learning and Knowledge Dis-\ncovery in Databases, 148–163. Springer.\nRoth, D.; and Yih, W.-t. 2007. Global inference for entity\nand relation identiﬁcation via a linear programming formu-\nlation. Introduction to statistical relational learning 553–\n580.\nSchuster, M.; and Paliwal, K. K. 1997. Bidirectional recur-\nrent neural networks. IEEE transactions on Signal Process-\ning 45(11): 2673–2681.\nTakanobu, R.; Zhang, T.; Liu, J.; and Huang, M. 2019. A hi-\nerarchical framework for relation extraction with reinforce-\nment learning. In In AAAI, volume 33, 7072–7079.\nWang, Z.; Wen, R.; Chen, X.; Huang, S.-L.; Zhang, N.;\nand Zheng, Y . 2020. Finding inﬂuential instances for\ndistantly supervised relation extraction. arXiv preprint\narXiv:2009.09841 .\nWei, Z.; Jia, Y .; Tian, Y .; Hosseini, M. J.; Steedman, M.; and\nChang, Y . 2020a. A Novel Cascade Binary Tagging Frame-\nwork for Relational Triple Extraction. In Proceedings of\nACL 2020.\nWei, Z.; Su, J.; Wang, Y .; Tian, Y .; and Chang, Y . 2020b.\nA Novel Cascade Binary Tagging Framework for Relational\nTriple Extraction. In Proceedings of ACL, 1476–1488.\nWiseman, S.; and Rush, A. M. 2016. Sequence-to-sequence\nlearning as beam-search optimization. arXiv preprint\narXiv:1606.02960 .\nWu, T.; Li, X.; Li, Y .-F.; Haffari, R.; Qi, G.; Zhu, Y .; and Xu,\nG. 2021. Curriculum-Meta Learning for Order-Robust Con-\ntinual Relation Extraction. arXiv preprint arXiv:2101.01926\n.\nYonghui, W.; Schuster, M.; Chen, Z.; Le, Q.; Norouzi, M.;\nMacherey, W.; Krikun, M.; Cao, Y .; Gao, Q.; Macherey, K.;\net al. 2016. Bridging the gap between human and machine\ntranslation. arXiv preprint arXiv:1609.08144 .\nZeng, D.; Liu, K.; Chen, Y .; and Zhao, J. 2015. Distant su-\npervision for relation extraction via piecewise convolutional\nneural networks. In Proceedings of EMNLP, 1753–1762.\nZeng, D.; Zhang, H.; and Liu, Q. 2020. CopyMTL: Copy\nMechanism for Joint Extraction of Entities and Relations\nwith Multi-Task Learning. In AAAI, 9507–9514.\nZeng, X.; Zeng, D.; He, S.; Liu, K.; and Zhao, J. 2018a. Ex-\ntracting relational facts by an end-to-end neural model with\ncopy mechanism. In Proceedings of ACL, 506–514.\nZeng, X.; Zeng, D.; He, S.; Liu, K.; and Zhao, J. 2018b.\nExtracting Relational Facts by an End-to-End Neural Model\nwith Copy Mechanism. In Proceedings of ACL, 506–514.\nMelbourne, Australia: Association for Computational Lin-\nguistics. doi:10.18653/v1/P18-1047. URL https://www.\naclweb.org/anthology/P18-1047.\nZhang, N.; Deng, S.; Bi, Z.; Yu, H.; Yang, J.; Chen, M.;\nHuang, F.; Zhang, W.; and Chen, H. 2020a. OpenUE: An\nOpen Toolkit of Universal Extraction from Text. In Pro-\nceedings of the 2020 Conference on Empirical Methods in\nNatural Language Processing: System Demonstrations, 1–\n8. Online. doi:10.18653/v1/2020.emnlp-demos.1.\nZhang, N.; Deng, S.; Li, J.; Chen, X.; Zhang, W.; and\nChen, H. 2020b. Summarizing Chinese Medical Answer\nwith Graph Convolution Networks and Question-focused\nDual Attention. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2020, 15–24. Online. doi:\n10.18653/v1/2020.ﬁndings-emnlp.2.\nZhang, N.; Deng, S.; Sun, Z.; Chen, J.; Zhang, W.; and Chen,\nH. 2020c. Relation Adversarial Network for Low Resource\nKnowledge Graph Completion. In Proceedings of The Web\nConference 2020, 1–12.\nZhang, N.; Deng, S.; Sun, Z.; Chen, X.; Zhang, W.; and\nChen, H. 2018. Attention-Based Capsule Networks with\nDynamic Routing for Relation Extraction. In Proceedings\nof EMNLP.\nZhang, N.; Deng, S.; Sun, Z.; Wang, G.; Chen, X.; Zhang,\nW.; and Chen, H. 2019a. Long-tail Relation Extraction via\nKnowledge Graph Embeddings and Graph Convolution Net-\nworks. arXiv preprint arXiv:1903.01306 .\nZhang, S.; Duh, K.; and Van Durme, B. 2017. Selective\ndecoding for cross-lingual open information extraction. In\nProceedings of IJCNLP, 832–842.\nZhang, Y .; Jiang, Z.; Zhang, T.; Liu, S.; Cao, J.; Liu, K.; Liu,\nS.; and Zhao, J. 2020d. MIE: A Medical Information Ex-\ntractor towards Medical Dialogues. In Proceedings of ACL,\n6460–6469.\nZhang, Y .; Merck, D.; Tsai, E. B.; Manning, C. D.; and Lan-\nglotz, C. P. 2019b. Optimizing the factual correctness of a\nsummary: A study of summarizing radiology reports. arXiv\npreprint arXiv:1911.02541 .\nZheng, S.; Wang, F.; Bao, H.; Hao, Y .; Zhou, P.; and Xu,\nB. 2017a. Joint Extraction of Entities and Relations Based\non a Novel Tagging Scheme. CoRR abs/1706.05075. URL\nhttp://arxiv.org/abs/1706.05075.\nZheng, S.; Wang, F.; Bao, H.; Hao, Y .; Zhou, P.; and Xu, B.\n2017b. Joint Extraction of Entities and Relations Based on a\nNovel Tagging Scheme. InProceedings of ACL, 1227–1236.\nZhu, C.; Hinthorn, W.; Xu, R.; Zeng, Q.; Zeng, M.; Huang,\nX.; and Jiang, M. 2020. Boosting factual correctness of\nabstractive summarization with knowledge graph. arXiv\npreprint arXiv:2003.08612 .\n14265",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7796957492828369
    },
    {
      "name": "Transformer",
      "score": 0.741620659828186
    },
    {
      "name": "Generative grammar",
      "score": 0.6723150610923767
    },
    {
      "name": "Generative model",
      "score": 0.5301417112350464
    },
    {
      "name": "Encoder",
      "score": 0.4897603988647461
    },
    {
      "name": "Artificial intelligence",
      "score": 0.48631390929222107
    },
    {
      "name": "Inference",
      "score": 0.4770614206790924
    },
    {
      "name": "Natural language processing",
      "score": 0.3503929674625397
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.33634042739868164
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I76130692",
      "name": "Zhejiang University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210095624",
      "name": "Alibaba Group (United States)",
      "country": "US"
    }
  ],
  "cited_by": 119
}