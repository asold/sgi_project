{
    "title": "SLViT: Scale-Wise Language-Guided Vision Transformer for Referring Image Segmentation",
    "url": "https://openalex.org/W4385764483",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5045662308",
            "name": "Shuyi Ouyang",
            "affiliations": [
                "Zhejiang University"
            ]
        },
        {
            "id": "https://openalex.org/A5100701032",
            "name": "Hongyi Wang",
            "affiliations": [
                "Zhejiang University"
            ]
        },
        {
            "id": "https://openalex.org/A5066963650",
            "name": "Shiao Xie",
            "affiliations": [
                "Zhejiang University"
            ]
        },
        {
            "id": "https://openalex.org/A5034985742",
            "name": "Ziwei Niu",
            "affiliations": [
                "Zhejiang University"
            ]
        },
        {
            "id": "https://openalex.org/A5057181928",
            "name": "Ruofeng Tong",
            "affiliations": [
                "Zhejiang Lab",
                "Zhejiang University"
            ]
        },
        {
            "id": "https://openalex.org/A5044216245",
            "name": "Yenâ€Wei Chen",
            "affiliations": [
                "Ritsumeikan University"
            ]
        },
        {
            "id": "https://openalex.org/A5090814258",
            "name": "Lanfen Lin",
            "affiliations": [
                "Zhejiang University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6863994431",
        "https://openalex.org/W3105906614",
        "https://openalex.org/W3191278083",
        "https://openalex.org/W3158763510",
        "https://openalex.org/W2302548814",
        "https://openalex.org/W3034692043",
        "https://openalex.org/W3034325957",
        "https://openalex.org/W3089797362",
        "https://openalex.org/W3169150690",
        "https://openalex.org/W6790825729",
        "https://openalex.org/W4226365997",
        "https://openalex.org/W6803771590",
        "https://openalex.org/W2798556392",
        "https://openalex.org/W2605127024",
        "https://openalex.org/W4224988000",
        "https://openalex.org/W2144960104",
        "https://openalex.org/W2505639562",
        "https://openalex.org/W3216314363",
        "https://openalex.org/W2894964039",
        "https://openalex.org/W6864014924",
        "https://openalex.org/W3131500599",
        "https://openalex.org/W3216551675",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W3169998662",
        "https://openalex.org/W4200631575",
        "https://openalex.org/W2936707910",
        "https://openalex.org/W2489434015",
        "https://openalex.org/W2784458614",
        "https://openalex.org/W6868564194",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3108748824",
        "https://openalex.org/W2964284374",
        "https://openalex.org/W3122818000",
        "https://openalex.org/W3201770677",
        "https://openalex.org/W4322615526",
        "https://openalex.org/W2963109634",
        "https://openalex.org/W3004019157",
        "https://openalex.org/W3172522282",
        "https://openalex.org/W3035097537",
        "https://openalex.org/W4287324101",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W3093025045",
        "https://openalex.org/W4296544717",
        "https://openalex.org/W2964345792",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W3020827971",
        "https://openalex.org/W2980088508",
        "https://openalex.org/W4312543911",
        "https://openalex.org/W3187664142",
        "https://openalex.org/W4327534052",
        "https://openalex.org/W4313160444",
        "https://openalex.org/W3178075329"
    ],
    "abstract": "Referring image segmentation aims to segment an object out of an image via a specific language expression. The main concept is establishing global visual-linguistic relationships to locate the object and identify boundaries using details of the image. Recently, various Transformer-based techniques have been proposed to efficiently leverage long-range cross-modal dependencies, enhancing performance for referring segmentation. However, existing methods consider visual feature extraction and cross-modal fusion separately, resulting in insufficient visual-linguistic alignment in semantic space. In addition, they employ sequential structures and hence lack multi-scale information interaction. To address these limitations, we propose a Scale-Wise Language-Guided Vision Transformer (SLViT) with two appealing designs: (1) Language-Guided Multi-Scale Fusion Attention, a novel attention mechanism module for extracting rich local visual information and modeling global visual-linguistic relationships in an integrated manner. (2) An Uncertain Region Cross-Scale Enhancement module that can identify regions of high uncertainty using linguistic features and refine them via aggregated multi-scale features. We have evaluated our method on three benchmark datasets. The experimental results demonstrate that SLViT surpasses state-of-the-art methods with lower computational cost. The code is publicly available at: https://github.com/NaturalKnight/SLViT.",
    "full_text": "SLViT: Scale-Wise Language-Guided Vision Transformer for\nReferring Image Segmentation\nShuyi Ouyang1 , Hongyi Wang1 , Shiao Xie1 , Ziwei Niu1 , Ruofeng Tong1,3 ,\nYen-Wei Chen2âˆ— and Lanfen Lin1âˆ—\n1Zhejiang University\n2Ritsumeikan University\n3Zhejiang Lab\nAbstract\nReferring image segmentation aims to segment an\nobject out of an image via a specific language ex-\npression. The main concept is establishing global\nvisual-linguistic relationships to locate the object\nand identify boundaries using details of the im-\nage. Recently, various Transformer-based tech-\nniques have been proposed to efficiently leverage\nlong-range cross-modal dependencies, enhancing\nperformance for referring segmentation. However,\nexisting methods consider visual feature extraction\nand cross-modal fusion separately, resulting in in-\nsufficient visual-linguistic alignment in semantic\nspace. In addition, they employ sequential struc-\ntures and hence lack multi-scale information inter-\naction. To address these limitations, we propose a\nScale-Wise Language-Guided Vision Transformer\n(SLViT) with two appealing designs: (1) Language-\nGuided Multi-Scale Fusion Attention, a novel at-\ntention mechanism module for extracting rich lo-\ncal visual information and modeling global visual-\nlinguistic relationships in an integrated manner. (2)\nAn Uncertain Region Cross-Scale Enhancement\nmodule that can identify regions of high uncer-\ntainty using linguistic features and refine them via\naggregated multi-scale features. We have evalu-\nated our method on three benchmark datasets. The\nexperimental results demonstrate that SLViT sur-\npasses state-of-the-art methods with lower compu-\ntational cost. The code is publicly available at:\nhttps://github.com/NaturalKnight/SLViT.\n1 Introduction\nReferring segmentation refers to the task of segmenting an\nobject based on a given text description that may contain in-\nformation about the targetâ€™s action, category, color, position\nin the image, etc [Cheng et al., 2014; Hu et al., 2016 ]. It\nhas a promising application prospects in many fields, such\nas language-based man-machine interaction. Unlike the con-\nventional semantic and instance segmentation tasks, referring\nâˆ—Corresponding Authors: Lanfen Lin (llf@zju.edu.cn), Yen-Wei\nChen (chen@is.ritsumei.ac.jp).\nVision Feature Extraction\nCross-Modal Fusion\nDecoder\nCross-Scale Enhancement\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nğ‘£ğ‘–ğ‘ ğ‘¢ğ‘ğ‘™ input\nğ‘™ğ‘–ğ‘›ğ‘”ğ‘¢ğ‘–ğ‘ ğ‘¡ğ‘–ğ‘input\n(a) (b)\n(c)\nEncoder\nFigure 1: Comparison of existing Transformer-based architectures\n((a) and (b)) for referring segmentation with our SLViT (c).\nsegmentation task requires precise perception of the loca-\ntions of different objects in an image, making global visual-\nlinguistic relationships modeling indispensable. Moreover,\neffective edge detection of the target objects requires details\nof the image, necessitating high-quality local visual features.\nIn contrast to linear fusion methods [Hu et al., 2016;\nLiu et al., 2017 ] adopting Fully Convolutional Networks\n(FCN) for feature learning and prediction, various attention\nmechanisms [Shi et al., 2018; Ye et al., 2019] have been pro-\nposed to learn rich visual-linguistic information. Transform-\ners can naturally model long-distance dependencies via atten-\ntion mechanisms, which are well suited to cross-modal fusion\nand hence an appropriate choice for referring segmentation\ntask. Therefore, several Vision Transformer (ViT) [Dosovit-\nskiy et al., 2020] methods have been put forth which signif-\nicantly improve performance for this task. Figure 1(a) il-\nlustrates a Transformer-based architecture for referring im-\nage segmentation, i.e., VLT [Ding et al., 2021], which fuses\nvision and language features after vision feature extraction\nProceedings of the Thirty-Second International Joint Conference on Artiï¬cial Intelligence (IJCAI-23)\n1294\nâ€œ the first half of  . \nthe sandwich to the left â€\nâ€œ top left bowl â€\n(c) SLViT (w/o URCE) (d) SLViT(b) LAVT(a) VLT\n(g) SLViT (w/o URCE) (h) SLViT(f) LAVT(e) VLT\nGround Truth\nGround Truth\nFigure 2: Qualitative results of different approaches. The prediction masks for the image with the referring textâ€œthe first half of the sandwich\nto the leftâ€ and â€œtop left bowlâ€ are shown in (a)-(d) and (e)-(h), respectively.\nthrough encoders. The architecture shown in Fig.1(b) is em-\nployed in EFN [Feng et al., 2021 ] and LA VT[Yang et al.,\n2022]. This architecture includes a fusion module at the end\nof each stage to fuse extracted visual features with linguistic\nmodality information. The existing Transformer-based de-\nsigns take advantage of long-range dependencies and hier-\narchical structure to enhance performance, although Trans-\nformer designs can be further improved. Specifically, visual\nfeature extraction and cross-modal fusion are considered into\ntwo independent steps in the existing works, which leaves\nroom for improvement in visual-linguistic alignment in se-\nmantic space. Additionally, current approaches adopt se-\nquential structures that result in single-scale representations\nat each level, despite the fact that multi-scale feature interac-\ntion has been shown to be more beneficial for capturing the\ncore semantic information.\nBy revisiting previous successful works and analyzing re-\nquirements of the referring segmentation task, we argue a ef-\nfective method for such task should have the following char-\nacteristics: (i) A robust fusion encoder network to capture\nlocal visual and global visual-linguistic information. To ac-\ncurately pinpoint the target instance with varying character-\nistics, both rich local visual features and positional global\ncross-modal relationships are crucial. (ii) Multi-scale infor-\nmation interaction to capture cross-scale dependencies and\naddress complex scale differences. For dense prediction\ntasks like referring segmentation, the incorporation of com-\nplementing information from multiple scales is helpful.\nTherefore, taking the aforementioned analysis into ac-\ncount, we propose a novel referring image segmentation ar-\nchitecture (in Figure 1(c)), namely Scale-Wise Language-\nGuided Vision Transformer (SLViT). In SLViT, we propose\nan integrated vision-language encoder network design, with\na novel attention mechanism called Language-Guided Multi-\nScale Fusion Attention (LMFA) to comprehensively extract\nmulti-scale local visual features and model global cross-\nmodal relationships. It improves visual-linguistic alignment\nin semantic space in a lightweight manner. As shown in\nFigure 2(a)-(c), LMFA significantly improves performance\nin locating objects. Considering the spatial correlation be-\ntween patches at different scales through downsampling, it\nis beneficial to perform interactions at different scales of the\nsame region for feature refinement. Furthermore, there are\nregions where the semantic information is temporarily uncer-\ntain, making targeted cross-scale enhancement needed. We\ndesign a cross-scale feature fusing module named Uncertain\nRegion Cross-Scale Enhancement (URCE) to identify regions\nof high uncertainty, represented by variance of cross-modal\nattention scores between scales, and then refine features of\nthe regions using complementary information from multiple\nscales. The accuracy of boundary identification is improved\nby URCE, which is qualitatively shown in Figure 2(e)-(h).\nIn summary, our contributions are three-folded:\n1. We propose Language-Guided Multi-Scale Fusion At-\ntention (LMFA) module in our integrated vision-\nlanguage encoder with the ability of integrated local vi-\nsual feature extraction and global cross-modal relation-\nships modeling in referring segmentation. LMFA im-\nproves visual-linguistic alignment in semantic space.\n2. We design a multi-scale feature fusion module named\nUncertain Region Cross-Scale Enhancement (URCE).\nURCE uses the variance of cross-modal correlations be-\ntween adjacent stages to identify regions of high uncer-\ntainty and refines features of these regions with comple-\nmenting information from multiple stages, which helps\nin identifying satisfactory boundaries.\n3. Based on the aforementioned moudules, we design a\nnovel framework named SLViT for referring segmenta-\ntion task. We conducted thorough experiments on SLViT\nwith three benchmark datasets, and the experimental re-\nsults show that SLViT outperforms current state-of-the-\nart methods with lower computational cost.\n2 Related Works\nReferring segmentation. For referring segmentation, the\nearly methods [Hu et al., 2016; Liu et al., 2017; Li et\nal., 2018] directly concatenate visual and linguistic features,\nadopting FCN for cross-modal feature learning and predic-\ntion, lacking attention to the relationship between modalities.\nDifferently, numerous attention-based fusion methods have\nbeen proposed for this task. Vision-guided linguistic attention\n[Shi et al., 2018] and Cross-Modal Self-Attention module[Ye\net al., 2019] are proposed to learn visual content correspond-\nProceedings of the Thirty-Second International Joint Conference on Artiï¬cial Intelligence (IJCAI-23)\n1295\na\nBERT\nIntegrated Vision-\nLanguage Encoder\nDecoder\n\"bag on far left\"\nUncertain Region \nCross-Scale Enhance\nDown-Sampling\nEmbedding Block\nUncertain Region Extraction\nUncertain Region Reverse\nChannel Restore\nMulti-head Self Attention\nChannel Unify\nCross-Scale Fusing Attention\nğ¹ğ‘– âˆˆ ğ¶ğ‘£ğ‘– Ã—ğ»ğ‘– Ã—ğ‘Šğ‘– ğ‘†ğ‘– âˆˆ ğ»ğ‘–ğ‘Šğ‘– Ã—ğ‘‡\nğ¹1 ğ‘†1\nğ¹2 ğ‘†2\nğ¹3 ğ‘†3\nğ¹4 ğ‘†4\nà·©ğ¹1\nà·ªğ¹2\nà·ªğ¹3\nà·©ğ¹4\nğ¹1, ğ¹2, ğ¹3, ğ¹4 ğ‘†1, ğ‘†2, ğ‘†3,ğ‘†4\nà·ªğ¹1, à·ªğ¹2, à·ªğ¹3, à·ªğ¹4,\nÃ—K\nÃ—Ni\n(c)\n(a)\n(b)\nConv 1Ã—k1\nConv k1Ã—1\nConv 1Ã—k2\nConv k2Ã—1\nConv 1Ã—k3\nConv k3Ã—1\nConv 5Ã—5\nğ‘‰ğ‘– âˆˆ ğ¶ğ‘£ğ‘– Ã—ğ»ğ‘– Ã—ğ‘Šğ‘– ğ¿ âˆˆ Cğ‘™ Ã—ğ‘‡\nğ‘‰ğ‘–ğ‘ âˆˆ ğ»ğ‘–ğ‘Šğ‘– Ã—ğ¶ğ‘£ğ‘– ğ¿ğ‘–ğ‘˜ âˆˆ ğ‘‡Ã—ğ¶ğ‘£ğ‘– ğ¿ğ‘–ğ‘£ âˆˆ ğ‘‡Ã—ğ¶ğ‘£ğ‘–\nCross Attention\nGate\nConv 1Ã—1\nConvolutional \nBranches\nCross-Modal\nBranch\nğ‘‰1\nğ‘‰2\nğ‘‰3\nğ‘‰4\nğ¿\nğœ”ğ‘–ğ‘ ğœ”ğ‘–ğ‘˜ ğœ”ğ‘–ğ‘£\nLMFA URCE\nFigure 3: An illustration of SLViT. First, the input image and referring expression pass the embedding block and language encoder BERT\nrespectively to get visual feature V1 and linguistic feature L, which are sent to Integrated Vision-Language Encoder. Encoders learn useful\ncross-modal features Fi, iâˆˆ {1,2, 3, 4} and records the correlation mapsSi, iâˆˆ {1, 2, 3, 4} between the two modalities, in which Language-\nGuided Multi-Scale Fusion Attention (LMFA) captures local visual details and global visual-linguistic cues. The Uncertain Region Cross-\nScale Enhancement (URCE) then identifies uncertain regions in the image and enhances patches inFi, iâˆˆ {1, 2, 3, 4} corresponding to them,\nwhich interacts among different scales. Finally, the reinforced featuresËœFi, iâˆˆ {1, 2, 3, 4} are sent to the decoder block for final segmentation.\ning to verbal expression. [Hu et al., 2020 ] capture the mu-\ntual guidance between two modalities by a bi-directional re-\nlationship inferring network. [Yu et al., 2018] and [Huang et\nal., 2020] use knowledge about sentence structure to capture\nattributes in cross-modal features, while [Hui et al., 2020 ]\nexploits syntactic structures between words to guide cross-\nmodal context aggregation. Recently, Transformer-based\nmethods have improved the ability to model long-distance\ncross-modal dependencies and made significant progress in\nreferring segmentation. A VLT framework with encoder-\ndecoder is proposed in [Ding et al., 2021 ] using attention\nmechanism to enhance global context information. In EFN\n[Feng et al., 2021 ], a collaborative attention mechanism is\npresented to gradually refine multi-modal features using lan-\nguage and promote cross-modal expression. LA VT [Yang et\nal., 2022 ] achieves the early fusion of linguistic and visual\nfeatures between encoders of ViT.\nVision transformer and multi-scale architecture. Trans-\nformer models have been widely used for several computer\nvision tasks. The ViT model applies self-attention in shal-\nlow layers enhancing performance for vision tasks. Since\nthe computational complexity of self-attention is a quadratic\npolynomial in the number of tokens, it is difficult to di-\nrectly apply it to a large number of tokens. Therefore,\nin order to improve the performance of fine-grained tasks\nsuch as segmentation, various attention mechanisms have\nbeen developed in some recent works [Liu et al., 2021b;\nRen et al., 2022; Wang et al., 2021; Guo et al., 2022 ]\nto reduce computational cost while retaining valuable vi-\nsual information. Various studies [Zheng et al., 2021;\nGu et al., 2022 ] using Transformer with multi-scale designs\nfor segmentation tasks have been presented using knowl-\nedge between different scales. For referring segmentation\ntask, many Transformer-based methods [Ding et al., 2021;\nFeng et al., 2021; Yang et al., 2022 ] exploiting sequential\nstructures have been proposed. These methods leverage se-\nquential structures and lack sufficient multi-scale interaction.\n3 Scale-Wise Language-Guided Vision\nTransformer\n3.1 Overview\nThe proposed SLViT learns local visual and global visual-\nlinguistic cues within scales in an integrated way as well as\nmodeling inter-scale dependencies of uncertain regions. The\nstructure of SLViT is shown in Figure 3.\nIn a hierarchical manner, we propose the [integrated\nvision-language encoder] - [cross-scale enhancement] - de-\ncoder framework. The encoder (Sec.3.2) includes a novel\nlightweight attention module (Sec.3.3) that uses simultaneous\nmulti-scale convolutional operations and gated cross-modal\nattention to capture local visual features and global visual-\nlinguistic correlations. We also propose to use variance be-\ntween adjacent stages of cross-modal correlation to assess the\nuncertainty of regions in the image. For regions of high un-\ncertainty, we design a novel cross-scale feature fusion module\n(Sec.3.4) to automatically refine mutual regions at different\nscales via the complementary information among them. Fi-\nnally, the enhanced representations are sent to the decoder\nProceedings of the Thirty-Second International Joint Conference on Artiï¬cial Intelligence (IJCAI-23)\n1296\nblock (Sec.3.5) for the final prediction. In the following sub-\nsections, we describe each components of SLViT in detail.\n3.2 Integrated Vision-Language Encoder\nIn order to improve the alignment of visual-linguistic features\nin semantic space, we propose an integrated vision-language\nencoder to capture visual and cross-modal features in an in-\ntegrated way. The block structure of our encoder follows\nthe design of ViT [Dosovitskiy et al., 2020 ] but we design\na novel attention mechanism (Sec.3.3) replacing the conven-\ntional self-attention mechanism.\nAs shown in Figure 3(a), our encoder has a pyramid struc-\nture, which contains 4 stages with decreasing spatial reso-\nlutions. There are Ni blocks in our encoder for i-th stage.\nGiven input of a pair of an image and a referring expression,\nour model outputs a segmentation mask for the specified in-\nstance. We extract language features via a language encoder\nBERT [Devlin et al., 2018 ]. The language feature provided\ninto encoders are denoted as L âˆˆ RClÃ—T , where Cl is the\nnumber of channel, T is the number of words. The given\nimage passes through an embedding block to obtain initial\nvision input V1 âˆˆ RCv1Ã—H1Ã—W1 of the encoder, where Cv1 is\nthe number of channels, H1 and W1 are height and width of\nthe feature maps in first stage. Each stage contains a down-\nsampling block and a stack of integrated vision-language en-\ncoders. The down-sampling block consists of a convolution\nwith stride of 2 and kernel size of 3 Ã— 3, followed by a batch\nnormalization layer. For each stage, the stack of integrated\ncross-modal feature maps Fi can be represented as:\nFi =\n\u001aIlve(V1, L), i = 1\nIlve(Down(Fiâˆ’1), L), i = 2,3, 4 (1)\nwhere function Down(Â·) indicates the down-sampling block,\nfunction Ilve(Â·) indicates blocks in our encoder to catch in-\ntegrated cross-modal features, i indexes the stage. We obtain\nthe visual inputs of stages 2, 3, 4 through Vi = Down(Fiâˆ’1).\n3.3 Language-Guided Multi-Scale Fusion\nAttention\nAs depicted in Figure 3(b), our proposed attention mecha-\nnism, namely Language-Guided Multi-Scale Fusion Atten-\ntion (LMFA), contains four parts: a convolution operation\nto capture preliminary local feature, a multi-scale convolu-\ntional activation to aggregate multi-scale local visual features,\na gated cross-modal activation to aggregate global visual-\nlinguistic relationships, and a 1 Ã— 1 convolution operation to\nmodel relationships between branches. In i-th stage, given\nthe visual input Vi âˆˆ RCviÃ—HiÃ—Wi and the linguistic input\nL âˆˆ RClÃ—T , we obtain the preliminary local visual feature\nmap V Local\ni employing a 5 Ã— 5 convolution operation.\nMulti-scale convolutional activation. There are three con-\ncurrent convolutional branches with different kernel sizes\nto capture local features of different receptive fields, which\nhas spatial inductive-bias in modelling rich local visual in-\nformation. Multi-scale convolutional activation Attconv\ni âˆˆ\nRCviÃ—HiÃ—Wi can be obtained using the following equation:\nAttconv\ni =\n3X\nt=1\nConvt\nR(Convt\nC(V Local\ni )), (2)\nwhere t indexes the convolutional branch, Convt\nR indicates\na 1 Ã— kt convolution function for horizontal linear features,\nConvt\nC indicates a kt Ã— 1 convolution function for vertical\nlinear features. The strip-like convolution kernels aims in ob-\ntaining detailed local visual information with low cost.\nGated cross-modal activation. We utilize a gated cross-\nmodal attention to model global visual-linguistic relation-\nships. The steps to get gated cross-modal activation\nAttcross\ni âˆˆ RCviÃ—HiÃ—Wi are described as follows:\nViq = flatten(Ï‰iq(Vi)), (3)\nLik, Liv = Ï‰ik(L), Ï‰iv(L), (4)\nSi = V T\niq Lik, (5)\nAttcross\ni = Gate(unflatten(softmax( Si\nâˆšCl\n)LT\niv)), (6)\nwhere Ï‰iq, Ï‰ik, Ï‰iv are projection functions, Gate(Â·) indi-\ncates a 1Ã—1 convolution and a GELU function, flatten(Â·)\nmeans unrolling the two spatial dimensions into one dimen-\nsion in row-major, and unflatten(Â·) indicates the opposite\noperation. Here, Si âˆˆ RHiWiÃ—T is the attention scores be-\ntween the Viq and Lik, which represents the degree of corre-\nlation between two modalities. In the last block of each stage,\nSi is provided to URCE. Ï‰iq is implemented as a 1Ã—1 convo-\nlution followed by instance normalization with Cvi number\nof output channels. Each of Ï‰ik and Ï‰iv is implemented as a\n1 Ã— 1 convolution with Cvi number of output channels.\nIntegrated attention. We apply a convolution to coordi-\nnate convolutional branches and the cross-modal branch ob-\ntaining integrated attention weights and reweight the inputVi\nof LMFA. We obtain the integrated cross-modal feature map\nFi âˆˆ RCviÃ—HiÃ—Wi using the following equation:\nFi = Conv1Ã—1(Attconv\ni + Attcross\ni + V Local\ni ) âŠ™ Vi, (7)\nwhere âŠ™ is element-wise matrix multiplication operation, and\nConv1Ã—1 indicates a 1Ã—1 convolution function to model rela-\ntionships between branches.\n3.4 Uncertain Region Cross-Scale Enhancement\nMulti-scale information is crucial for capturing boundary de-\ntails. To optimize spatial correspondence and minimize re-\ndundancy, we propose an Uncertain Region Cross-Scale En-\nhancement (URCE) module. URCE targets high-uncertainty\nregions and facilitates interaction across scales within our hi-\nerarchical model. Refer to Figure 3(c) for the URCE pipeline.\nUncertain region extraction. Considering computational\ncost and efficiency, we perform cross-scale enhancement only\nfor the regions with the highest uncertainty.\nVisual-linguistic correlation of each patch in i-th stage is\nindicated by Ri âˆˆ HviWvi, which is obtained by Ri =PCl\n1 Si. Here, Si is the cross-modal attention score map\nfrom LMFA in i-th stage. The variation of visual-linguistic\nProceedings of the Thirty-Second International Joint Conference on Artiï¬cial Intelligence (IJCAI-23)\n1297\nUncertain Region Restore\nCross-Scale Fusing \nAttentionChannel Unify\nChannel ReverseUncertain Region Extraction\n(c)\nâ€œ yellow and blue vehicle      \n. closest to cameraâ€\n(a)\n(b)\nCross-Scale Fusing \nAttentionBlock\nCross-Scale Fusing \nAttentionBlock\nğ‘ğ‘–\n1\nğ‘ğ‘–+1\n1\nğ‘ğ‘–+2\n1\nğ‘ğ‘–\n2\nğ‘ğ‘–+1\n2\nğ‘ğ‘–+2\n2\nğ¹ğ‘– âˆˆ 2ğ‘–ğ¶ğ‘£ Ã— ğ»\n2ğ‘–\nğ‘Š\n2ğ‘–\n 2ğ‘–ğ¶ğ‘£ Ã— â„\n2ğ‘– Ã— ğ‘¤\n2ğ‘–\n2ğ‘–ğ¶ğ‘£ Ã— â„\n2ğ‘– Ã— ğ‘¤\n2ğ‘–\n2 Ã—Cross-Scale Fusing Attention\nMulti-Head\nSelf Attention\nğ‘4\n1\nğ‘1\n1\nğ‘2\n1\nğ‘3\n1\nà·©ğ¹4\nà·ªğ¹3\nà·ªğ¹2\nà·©ğ¹1ğ¹1\nğ¹2\nğ¹3\nğ¹4\nğ¹ğ‘–+1\nğ¹ğ‘–+2\nğ‘— = 1\nà·ªğ‘4\n1\nà·ªğ‘1\n1\nà·ªğ‘2\n1\nà·ªğ‘3\n1\nğ‘— = 2\nğ‘— = 1\nğ‘— = 2\nğ‘1\n2\nğ‘2\n2\nğ‘3\n2\nğ‘4\n2\n à·ªğ‘4\n2\nà·ªğ‘1\n2\nà·ªğ‘2\n2\nà·ªğ‘3\n2\nFigure 4: An illustration of the Uncertain Region Cross-Scale En-\nhancement.\ncorrelation Ri at each coordinate between adjacent stages is\nused to represent the uncertainty of the corresponding region\nin the image. We select K most uncertain regions to utilize\ncross-scale enhancement. The steps are described as follows:\nmapU =\n4X\ni=2\n|Down(Ri) âˆ’ Down(Riâˆ’1)|, (8)\nIndex = TopK (mapU ), (9)\nwhere mapU âˆˆ Hv4Wv4 means uncertainty map of each co-\nordinate, which is obtained by the correlation difference be-\ntween adjacent stages. Here, Down(Â·) indicates function to\ndownsample Ri into size of feature maps of 4-th stage, and\nTopK (Â·) indicates the function to find the array index of un-\ncertain regions with largest K values in the array mapU .\nThe K uncertain regions marked by Index, which corre-\nspond to patches in different scales, are first rearranged into\n2D tensor. Let pj\ni denotes the patch of j-th uncertain region\nin i-th stage. As an example of K = 2, Figure 4(b) illustrates\nthe process of finding the target patches in three successive\nfeature maps Fi, Fi+1, Fi+2 from i-th to (i+2)-th stage with\nsample inputs (in Figure 4(a)).\nCross-scale fusing attention. We model cross-scale clues\nfor uncertain patches correlated spatially, as shown in Figure\n4(c). For j-th uncertain region, patches from multiple stages\nare passed through Channel Unify to have the same channel\ndimension Cf . Then we concatenate them to obtain cross-\nscale feature Pj\ncross corresponding to j-th uncertain region,\nwhich can be described as follows:\nConcat[Ï‰p(pj\n1), Ï‰p(pj\n2), Ï‰p(pj\n3), Ï‰p(pj\n4)] âˆ’ â†’Pj\ncross, (10)\nwhere pj\ni âˆˆ 2iCv Ã— h\n2i\nw\n2i is the feature map of j-th uncertain\npatch in i-th stage, h and w are size of patches in first stage,\nand Ï‰p indicates functions to unify the channel to Cf and to\nrearrange the feature map into 2D tensor. Then we model\ncross-scale dependencies as follows:\nËœPj\ncross = MSA(LN (Pj\ncross)) +Pj\ncross, (11)\nwhere LN(Â·) indicates the layer normalization operator, and\nMSA(Â·) indicates multi-head self-attention. Then we reverse\nthe enhanced sequence back to patches according to the order\nof concatenation:\nÏ‰reverse\np (Split( ËœPj\ncross)) âˆ’ â†’Ëœpj\n1, Ëœpj\n2, Ëœpj\n3, Ëœpj\n4, (12)\nwhere Ï‰reverse\np indicates Channel Reverse. Here, Ï‰reverse\np\nand Split(Â·) are inverse operations of previous operations Ï‰p\nand Concat[Â·], respectively. Then we employ Uncertain Re-\ngion Restore that replaces pj\ni with Ëœpj\ni to obtain final cross-\nmodal feature maps ËœFi for each stage.\n3.5 Decoder and Segmentation\nAfter cross-scale enhancement, a decoder network is em-\nployed to capture high-level semantics. We aggregate fea-\ntures ËœF1, ËœF2, ËœF3, ËœF4 from URCE and use a lightweight Ham-\nburger [Geng et al., 2021] to further model the global context.\nWe obtain the final prediction results by the following equa-\ntion:\nOut = Seg(Ham(Concat[ ËœF1, ËœF2, ËœF3, ËœF4])), (13)\nwhere Fi is the cross-modal feature maps from stages,\nHam(Â·) indicates a Hamburger function, and Seg(Â·) indi-\ncates a 1Ã—1 convolution and an upsampling function for final\nprediction.\n4 Experiments\n4.1 Dataset and Evaluation\nWe perform experiments on three widely used benchmark\ndatasets for referring image segmentation, including Ref-\nCOCO [Yu et al., 2016 ], RefCOCO+ [Yu et al., 2016 ], and\nG-Ref [Mao et al., 2016; Nagaraja et al., 2016 ]. They have\n19,994, 19,992, and 26,711 images respectively, containing\n50,000, 49,856, and 54,822 references and 142,209, 141,564,\nand 104,560 reference expressions.\nFollowing previous works [Wang et al., 2022; Yang et\nal., 2022 ], we evaluate our proposed method with overall\nintersection-over-union (oIoU), mean intersection-over-union\n(mIoU), and precision at various thresholds. The oIoU is the\nratio between the total intersection area and the total union\nareas. Precision refers to the proportion of test samples with\nIoU values higher than the threshold.\nProceedings of the Thirty-Second International Joint Conference on Artiï¬cial Intelligence (IJCAI-23)\n1298\nMethod Language RefCOCO RefCOCO+ G-Ref\nModel val test A\ntest B val te st\nA test B val(U) test(U) v\nal(G)\nMAttNet [Yu et al.,\n2018] Bi-LSTM 56.51 62.37 51.70 46.67 52.39 40.08 47.64 48.61 -\nCMSA [Ye et\nal., 2019] None 58.32 60.61 55.09 43.76 47.60 37.89 - - 39.98\nCAC [Chen et\nal., 2019b] Bi-LSTM 58.90 61.77 53.81 - - - 46.37 46.95 44.32\nSTEP [Chen et al.\n, 2019a] Bi-LSTM 60.04 63.46 57.97 48.19 52.33 40.41 - - 46.40\nBRINet [Hu et al.\n, 2020] LSTM 60.98 62.99 59.21 48.17 52.32 42.11 - - 48.04\nLSCM [Hui et al.\n, 2020] LSTM 61.47 64.99 59.55 49.34 42.12 43.50 - - 48.05\nCMPC+ [Liu et al.\n, 2021a] LSTM 62.47 65.08 60.82 50.25 54.04 43.47 - - 49.89\nMCN [Luo et al.\n, 2020b] Bi-GRU 62.44 64.20 59.71 50.62 54.99 44.69 49.22 49.40 -\nEFN [Feng et al.\n, 2021] Bi-GRU 62.76 65.69 59.67 51.50 55.24 43.01 - - 51.93\nBUSNet [Y\nang et al., 2021] Self-Att 63.27 66.41 61.39 51.76 56.87 44.13 - - 50.56\nCGAN [Luo et al.\n, 2020a] Bi-GRU 64.86 68.04 62.07 51.03 55.51 44.06 51.01 51.69 46.54\nISFP [Liu et al.\n, 2022] Bi-GRU 65.19 68.45 62.73 52.70 56.77 46.39 52.67 53.00 50.08\nLTS [Jing et\nal., 2021] Bi-GRU 65.43 67.76 63.08 54.21 58.32 48.02 54.40 54.25 -\nVLT [Ding et\nal., 2021] Bi-GRU 65.65 68.29 62.73 55.50 59.20 49.36 52.99 56.65 49.76\nReSTR [Kim et al.\n, 2022] Transformer 67.22 69.30 64.45 55.78 60.44 48.27 54.48 - -\nCRIS [Wang et\nal., 2022] Transformer 70.47 73.18 66.10 62.27 68.08 53.68 59.87 60.36 -\nLAVT [Y\nang et al., 2022] BERT 72.73 75.82 68.79 62.14 68.38 55.10 61.24 62.09 60.50\nOurs (w/o URCE) BERT 73.34 75.98 70.21 63.72 68.81 55.72 62.74 63.23 60.55\nOurs BERT 74.02 76.91 70.62 64.07 69.28 56.14 62.75 63.57 60.94\nTable 1: Comparison with state-of-the-art methods in terms of overall IoU on three benchmark datasets. U: The UMD partition. G: The\nGoogle partition. Language model shows the the main learnable function that transforms word embeddings before multi-modal feature\nfusion.\n4.2 Implementation Details\nWe conduct experiments using PyTorch library and use\nBERT implementation from HuggingFaceâ€™s Transformer li-\nbrary [Wolf et al., 2020]. Convolutions in LMFAâ€™s convolu-\ntional branches and our decoder are initialized with weights\npre-trained on ImageNet-22K from the SegNeXt [Guo et al.,\n2022]. Language encoder of our model is initialized using\nofficial pre-trained weights of BERT with 12 layers and hid-\nden size 768. In convolutional branches of LMFA, we use\nk1 = 7, k2 = 11, k3 = 21kernel sizes for our convolutions.\nThe rest of weights in our model are randomly initialized.\nFollowing, we use AdamW optimizer with weight decay\n0.01. The learning rate is initialed as 3e-5 and scheduled by\npolynomial learning rate decay with a power of 0.9. All the\nmodels are trained for60 epochs with a batch size of16. Each\nreference has 2-3 sentences on average, and we randomly\nsample one referring expression per object in a epoch. Im-\nage size is adjusted to 480 Ã— 480 without data augmentation.\n4.3 Comparison with the State-of-the-Arts\nWe compare the performance of our proposed method with\nstate-of-the-art methods on three widely-used datasets using\nthe oIoU metric. Experimental results are reported in Table 1\nand the best results are highlighted in bold. As shown, our\nSLViT without Uncertain Region Cross-Scale Enhancement\n(w/o URCE) outperforms all other methods. This has an im-\nprovement of 1.58 oIoU over the Val Split set of the Ref-\nCOCO+ dataset, while on average an improvement of 0.83\noIoU across all 9 validation sets of the three datasets. It\nindicates the efficacy of integrated vision-language encoder\nwith LMFA to improve visual-linguistic alignment in seman-\ntic space. It is helpful in capturing detailed local visual fea-\n(a) \n Params.(M)\nGFLOPs oIoU\nOurs\nLAVT\nVLT\nEFN\n225.68\n181.33\n203.7\n191.71\n153.1\n273.36\nâ†“\nâ†“ â†‘\n74.02\n133.59\n52.22\n72.73\n62.72\n65.65\n132.03\n50.04\nOurs\n73.34\n(b) \n(w/o URCE)\nFigure 5: (a) Ablation study using different numbers of uncer-\ntain regions. (b) Comparison of our SLViT (w/o URCE), SLViT\nwith Transformer-based methods EFN, VLT and LA VT on Params,\nGFLOPs and oIoU.\ntures and modeling global visual-linguistic relationships in\nan integrated manner. Additionally, an increment of oIoU\n(+0.93 at most) is achieved when utilizing URCE, which\nachieves the new SOTA on these datasets. Furthermore, our\nproposed SLViT uses significantly less GFLOPs and parame-\nters than previously proposed Transformer-based approaches\nwhile achieving a higher oIoU score, as shown in Figure 5(b).\n4.4 Ablation Study\nAblation on LMFA design. We have conducted an abla-\ntion study on LMFA design on the RefCOCO validation set.\nResults are shown in Table 2(a)(b). ktB indicates the convo-\nlutional branch containing a 1 Ã— kt convolution and a kt Ã— 1\nconvolution. Gate represents a 1Ã—1 convolution and a GELU\nfunction in the cross-modal branch, enhancing the networkâ€™s\nadaptive ability. We employed a single convolution branch in\nProceedings of the Thirty-Second International Joint Conference on Artiï¬cial Intelligence (IJCAI-23)\n1299\nP@0.5 P@0.7 P@0.9 Mean IoU Overall IoU\n(a) Comparison of k\nernel sizes with a single convolutional branch\n5 B 81.03 70.36 25.88 71.09 69.91small 7 B 81.80 71.28 26.33 71.83 70.08\n11 B 82.17 71.79 27.96 72.63 71.55medium 15 B 82.23 71.75 27.92 72.60 71.56\n21 B 82.68 72.96 29.47 73.02 71.79large 23 B 82.59 72.78 29.36 72.99 71.77\n(b) Ablation on design\nchoices\n7B 11B 21B Gate\nâœ“ âœ“ 83.26 73.47 29.97 73.44 72.26\nâœ“ âœ“ 84.31 73.51 30.11 73.89 72.53\nâœ“ âœ“ 84.55 73.64 30.37 74.08 72.68\nâœ“ âœ“ âœ“ 85.16 74.12 31.00 75.07 73.31\nâœ“ âœ“ âœ“ âœ“ 86.74 75.84 35.10 75.96 74.02\n(c) Effectiv\neness of URCE\nSLViT(w/o URCE) 85.23 74.57 31.36 75.29 73.34\nSLViT(w/ URCE) 86.74 75.84 35.10 75.96 74.02\n(d) URCE on v\narious stages\nS1 S2 S3 S4\nâœ“ âœ“ 85.66 74.59 30.73 74.49 72.97\nâœ“ âœ“ âœ“ 86.42 75.51 35.17 75.50 73.76\nâœ“ âœ“ âœ“ 86.48 75.65 33.79 75.45 73.66\nâœ“ âœ“ âœ“ âœ“ 86.74 75.84 35.10 75.96 74.02\n(e) Features used for\nfinal prediction\nF2, F3, F4 84.87 74.13 30.39 75.09 73.27\nF1, F2, F3, F4 85.23 74.57 31.36 75.29 73.34\nËœF2, ËœF4, ËœF4 86.12 75.23 34.79 75.57 73.86\nF1, ËœF2, ËœF4, ËœF4 85.52 74.43 33.11 75.45 73.64\nËœF1, ËœF2, ËœF3, ËœF4 86.74 75.84 35.10 75.96 74.02\nTable 2: Ablation studies on the RefCOCO validation set.\nLMFA to evaluate the impact of various convolution kernel\nsizes kt. In Table 2(a), sizes 7 and 21 show superior per-\nformance among those with comparable computational costs.\nObserving Table 2(b), it follows that each part contributes to\nthe final performance.\nNumber of uncertain regions to enhance. We explore the\nnumber of uncertain regionsK to utilize cross-scale enhance-\nment. In Figure 5(a), when increasing the number of selected\nuncertain regions, the oIoU metrics increase sharply at the be-\nginning and then tends to stabilize. The increase in the value\nof K is linearly correlated with the increasement of comput-\ning cost. We choose K = 32as the default setting.\nEffectiveness of URCE. To verify the performance of\nURCE, we have compared SLViT to SLViT (w/o URCE) in\nTable 2(c). It shows this ablation leads to a drop of 0.68 and\n0.67 absolute point in overall IoU and mean IoU respectively,\nand a drop of an average of 2.18 points in precision across the\nthree thresholds. In addition, Ours and Ours (w/o URCE) in\nFigure 5(b) also show that URCE improves performance with\na slight increase in parameters and GFLOPs.\nAblation of URCE on various stages. Given integrated\ncross-modal features from different stages, URCE forms cor-\nresponding regions of them into a sequence for joint refine-\nment in single forward pass. Si means the integrated cross-\nmodal feature Fi from i-th stage used as input for URCE.\nMultiple input sequences are compared in the Table 2(d). The\nâ€œ guy in backâ€\nğ¹1\nà·©ğ¹1 à·ªğ¹2 à·ªğ¹3 à·ªğ¹4\nğ¹2 ğ¹3 ğ¹4\nGround Truth SLViT\nSLViT(w/o URCE)\nğ‘Œ1 ğ‘Œ2 ğ‘Œ3 ğ‘Œ4LAVT\nFigure 6: Visualization of the feature maps from different stages in\nLA VT, SLViT (w/o URCE) and SLViT, respectively.\nperformance boost shows the benefit of multi-scale feature in-\nteraction and detailed context in global reasoning.\nAblation of features used for final prediction. We con-\nduct several experiments to assess the influence of the de-\ncoder with various sequences as input. As shown in Ta-\nble 2(e), features ËœF1, ËœF2, ËœF3, ËœF4 , which are enhanced by\nURCE, are the best choices to be sent to the decoder network.\n4.5 Interpretation of SLViT\nIn Figure 6, we visualize the feature maps from LA VT[Yang\net al., 2022], our SLViT (w/o URCE) and SLViT. Comparing\nF1, F2, F3, F4 from our SLViT (w/o URCE) toY1, Y2, Y3, Y4\nfrom LA VTâ€™s various stages, the feature mapsF1 to F4 grad-\nually focus on the target instance and show more visual-\nlinguistic alignment in semantic space as stages go deeper.\nIt indicates that in LMFA, multi-scale convolutional activa-\ntion learns valuable local visual information, and gated cross-\nmodal activation is useful in identifying the relative position\nof the target object. We interpret the role of URCE by com-\nparing the segmentation results and the feature maps Fi, ËœFi.\nImpressively in F2 and ËœF2, the fence as the background in ËœF2\nhas been eliminated, which indicates that URCE is able to fil-\nter out irrelevant objects. As F3 and ËœF3 are being observed,\nthe edge of the target object in ËœF3 is more concerned. There-\nfore, URCE can eliminate interference items and improve the\naccuracy of boundary prediction by cross-scale enhancement\nof uncertain regions.\n5 Conclusion\nIn this paper, we propose a novel Transformer-based frame-\nwork named SLViT for referring image segmentation. SLViT\ncaptures rich local visual features and models global visual-\nlinguistic relationships in an integrated manner at each stage.\nThe proposed network design interacts cross-modal features\nof uncertain regions between different scales with spatial cor-\nrespondence. Experiments show that SLViT outperforms ex-\nisting methods on three benchmark datasets with lower com-\nputational cost.\nProceedings of the Thirty-Second International Joint Conference on Artiï¬cial Intelligence (IJCAI-23)\n1300\nAcknowledgments\nThis work was supported in part by the National Key Re-\nsearch and Development Project (No. 2022YFC2504605),\nZhejiang Provincial Natural Science Foundation of China\n(No. LZ22F020012), Major Technological Innovation Project\nof Hangzhou (No. 2022AIZD0147), Major Scientific Re-\nsearch Project of Zhejiang Lab (No. 2020ND8AD01).\nReferences\n[Chen et al., 2019a] Ding-Jie Chen, Songhao Jia, Yi-Chen\nLo, Hwann-Tzong Chen, and Tyng-Luh Liu. See-through-\ntext grouping for referring image segmentation. In Pro-\nceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 7454â€“7463, 2019.\n[Chen et al., 2019b] Yi-Wen Chen, Yi-Hsuan Tsai, Tiantian\nWang, Yen-Yu Lin, and Ming-Hsuan Yang. Referring ex-\npression object segmentation with caption-aware consis-\ntency. arXiv preprint arXiv:1910.04748, 2019.\n[Cheng et al., 2014] Ming-Ming Cheng, Shuai Zheng, Wen-\nYan Lin, Vibhav Vineet, Paul Sturgess, Nigel Crook,\nNiloy J Mitra, and Philip Torr. Imagespirit: Verbal guided\nimage parsing. ACM Transactions on Graphics (ToG),\n34(1):1â€“11, 2014.\n[Devlin et al., 2018] Jacob Devlin, Ming-Wei Chang, Ken-\nton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805, 2018.\n[Ding et al., 2021] Henghui Ding, Chang Liu, Suchen Wang,\nand Xudong Jiang. Vision-language transformer and query\ngeneration for referring segmentation. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vi-\nsion, pages 16321â€“16330, 2021.\n[Dosovitskiy et al., 2020] Alexey Dosovitskiy, Lucas Beyer,\nAlexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Min-\nderer, Georg Heigold, Sylvain Gelly, et al. An image is\nworth 16x16 words: Transformers for image recognition\nat scale. arXiv preprint arXiv:2010.11929, 2020.\n[Feng et al., 2021] Guang Feng, Zhiwei Hu, Lihe Zhang,\nand Huchuan Lu. Encoder fusion network with co-\nattention embedding for referring image segmentation. In\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 15506â€“15515,\n2021.\n[Geng et al., 2021] Zhengyang Geng, Meng-Hao Guo,\nHongxu Chen, Xia Li, Ke Wei, and Zhouchen Lin.\nIs attention better than matrix decomposition? arXiv\npreprint arXiv:2109.04553, 2021.\n[Gu et al., 2022] Jiaqi Gu, Hyoukjun Kwon, Dilin Wang,\nWei Ye, Meng Li, Yu-Hsin Chen, Liangzhen Lai, Vikas\nChandra, and David Z Pan. Multi-scale high-resolution\nvision transformer for semantic segmentation. InProceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 12094â€“12103, 2022.\n[Guo et al., 2022] Meng-Hao Guo, Cheng-Ze Lu, Qibin\nHou, Zhengning Liu, Ming-Ming Cheng, and Shi-Min Hu.\nSegnext: Rethinking convolutional attention design for se-\nmantic segmentation. arXiv preprint arXiv:2209.08575,\n2022.\n[Hu et al., 2016] Ronghang Hu, Marcus Rohrbach, and\nTrevor Darrell. Segmentation from natural language ex-\npressions. In European Conference on Computer Vision,\npages 108â€“124, 2016.\n[Hu et al., 2020] Zhiwei Hu, Guang Feng, Jiayu Sun, Lihe\nZhang, and Huchuan Lu. Bi-directional relationship in-\nferring network for referring image segmentation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 4424â€“4433, 2020.\n[Huang et al., 2020] Shaofei Huang, Tianrui Hui, Si Liu,\nGuanbin Li, Yunchao Wei, Jizhong Han, Luoqi Liu, and\nBo Li. Referring image segmentation via cross-modal pro-\ngressive comprehension. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 10488â€“10497, 2020.\n[Hui et al., 2020] Tianrui Hui, Si Liu, Shaofei Huang, Guan-\nbin Li, Sansi Yu, Faxi Zhang, and Jizhong Han. Linguistic\nstructure guided context modeling for referring image seg-\nmentation. In European Conference on Computer Vision,\npages 59â€“75. Springer, 2020.\n[Jing et al., 2021] Ya Jing, Tao Kong, Wei Wang, Liang\nWang, Lei Li, and Tieniu Tan. Locate then segment: A\nstrong pipeline for referring image segmentation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 9858â€“9867, 2021.\n[Kim et al., 2022] Namyup Kim, Dongwon Kim, Cuiling\nLan, Wenjun Zeng, and Suha Kwak. Restr: Convolution-\nfree referring image segmentation using transformers. In\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 18145â€“18154,\n2022.\n[Li et al., 2018] Ruiyu Li, Kaican Li, Yi-Chun Kuo,\nMichelle Shu, Xiaojuan Qi, Xiaoyong Shen, and Jiaya\nJia. Referring image segmentation via recurrent refine-\nment networks. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages 5745â€“\n5753, 2018.\n[Liu et al., 2017] Chenxi Liu, Zhe Lin, Xiaohui Shen, Jimei\nYang, Xin Lu, and Alan Yuille. Recurrent multimodal in-\nteraction for referring image segmentation. InProceedings\nof the IEEE International Conference on Computer Vision,\npages 1271â€“1280, 2017.\n[Liu et al., 2021a] Si Liu, Tianrui Hui, Shaofei Huang, Yun-\nchao Wei, Bo Li, and Guanbin Li. Cross-modal pro-\ngressive comprehension for referring segmentation. IEEE\nTransactions on Pattern Analysis and Machine Intelli-\ngence, 2021.\n[Liu et al., 2021b] Ze Liu, Yutong Lin, Yue Cao, Han Hu,\nYixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.\nSwin transformer: Hierarchical vision transformer using\nProceedings of the Thirty-Second International Joint Conference on Artiï¬cial Intelligence (IJCAI-23)\n1301\nshifted windows. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 10012â€“\n10022, 2021.\n[Liu et al., 2022] Chang Liu, Xudong Jiang, and Henghui\nDing. Instance-specific feature propagation for referring\nsegmentation. IEEE Transactions on Multimedia, 2022.\n[Luo et al., 2020a] Gen Luo, Yiyi Zhou, Rongrong Ji, Xi-\naoshuai Sun, Jinsong Su, Chia-Wen Lin, and Qi Tian. Cas-\ncade grouped attention network for referring expression\nsegmentation. In Proceedings of the 28th ACM Interna-\ntional Conference on Multimedia, pages 1274â€“1282, 2020.\n[Luo et al., 2020b] Gen Luo, Yiyi Zhou, Xiaoshuai Sun, Li-\nujuan Cao, Chenglin Wu, Cheng Deng, and Rongrong Ji.\nMulti-task collaborative network for joint referring expres-\nsion comprehension and segmentation. In Proceedings of\nthe IEEE/CVF Conference on computer vision and pattern\nrecognition, pages 10034â€“10043, 2020.\n[Mao et al., 2016] Junhua Mao, Jonathan Huang, Alexander\nToshev, Oana Camburu, Alan L Yuille, and Kevin Mur-\nphy. Generation and comprehension of unambiguous ob-\nject descriptions. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages 11â€“20,\n2016.\n[Nagaraja et al., 2016] Varun K Nagaraja, Vlad I Morariu,\nand Larry S Davis. Modeling context between objects for\nreferring expression understanding. In European Confer-\nence on Computer Vision, pages 792â€“807. Springer, 2016.\n[Ren et al., 2022] Sucheng Ren, Daquan Zhou, Shengfeng\nHe, Jiashi Feng, and Xinchao Wang. Shunted self-\nattention via multi-scale token aggregation. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 10853â€“10862, 2022.\n[Shi et al., 2018] Hengcan Shi, Hongliang Li, Fanman\nMeng, and Qingbo Wu. Key-word-aware network for re-\nferring expression image segmentation. In Proceedings\nof the European Conference on Computer Vision (ECCV),\npages 38â€“54, 2018.\n[Wang et al., 2021] Wenhai Wang, Enze Xie, Xiang Li,\nDeng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping\nLuo, and Ling Shao. Pyramid vision transformer: A ver-\nsatile backbone for dense prediction without convolutions.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 568â€“578, 2021.\n[Wang et al., 2022] Zhaoqing Wang, Yu Lu, Qiang Li, Xun-\nqiang Tao, Yandong Guo, Mingming Gong, and Tongliang\nLiu. Cris: Clip-driven referring image segmentation. In\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 11686â€“11695,\n2022.\n[Wolf et al., 2020] Thomas Wolf, Lysandre Debut, Victor\nSanh, Julien Chaumond, Clement Delangue, Anthony\nMoi, Pierric Cistac, Tim Rault, R Â´emi Louf, Morgan Fun-\ntowicz, et al. Transformers: State-of-the-art natural lan-\nguage processing. In Proceedings of the 2020 conference\non empirical methods in natural language processing: sys-\ntem demonstrations, pages 38â€“45, 2020.\n[Yang et al., 2021] Sibei Yang, Meng Xia, Guanbin Li,\nHong-Yu Zhou, and Yizhou Yu. Bottom-up shift and rea-\nsoning for referring image segmentation. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 11266â€“11275, 2021.\n[Yang et al., 2022] Zhao Yang, Jiaqi Wang, Yansong Tang,\nKai Chen, Hengshuang Zhao, and Philip HS Torr. Lavt:\nLanguage-aware vision transformer for referring image\nsegmentation. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pages\n18155â€“18165, 2022.\n[Ye et al., 2019] Linwei Ye, Mrigank Rochan, Zhi Liu, and\nYang Wang. Cross-modal self-attention network for refer-\nring image segmentation. InProceedings of the IEEE/CVF\nconference on computer vision and pattern recognition,\npages 10502â€“10511, 2019.\n[Yu et al., 2016] Licheng Yu, Patrick Poirson, Shan Yang,\nAlexander C Berg, and Tamara L Berg. Modeling con-\ntext in referring expressions. In European Conference on\nComputer Vision, pages 69â€“85. Springer, 2016.\n[Yu et al., 2018] Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei\nYang, Xin Lu, Mohit Bansal, and Tamara L Berg. Mat-\ntnet: Modular attention network for referring expression\ncomprehension. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages 1307â€“\n1315, 2018.\n[Zheng et al., 2021] Sixiao Zheng, Jiachen Lu, Hengshuang\nZhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei\nFu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al.\nRethinking semantic segmentation from a sequence-to-\nsequence perspective with transformers. In Proceedings\nof the IEEE/CVF conference on computer vision and pat-\ntern recognition, pages 6881â€“6890, 2021.\nProceedings of the Thirty-Second International Joint Conference on Artiï¬cial Intelligence (IJCAI-23)\n1302"
}