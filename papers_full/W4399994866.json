{
  "title": "Using large language model to guide patients to create efficient and comprehensive clinical care message",
  "url": "https://openalex.org/W4399994866",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2555062769",
      "name": "Siru Liu",
      "affiliations": [
        "Vanderbilt University Medical Center",
        "Vanderbilt University"
      ]
    },
    {
      "id": "https://openalex.org/A2231056103",
      "name": "Aileen P Wright",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2157272911",
      "name": "Allison B. McCoy",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2720371605",
      "name": "Sean S. Huang",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2227829513",
      "name": "Julian Z. Genkins",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2142701830",
      "name": "Josh F Peterson",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": null,
      "name": "Yaa A Kumah-Crystal",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2088495663",
      "name": "William Martinez",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2984257326",
      "name": "Babatunde Carew",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A3158892542",
      "name": "Dara Mize",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2783176254",
      "name": "Bryan Steitz",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2035694469",
      "name": "Adam. Wright",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4200043507",
    "https://openalex.org/W2947493504",
    "https://openalex.org/W2799571175",
    "https://openalex.org/W2296048788",
    "https://openalex.org/W2954484941",
    "https://openalex.org/W3005890082",
    "https://openalex.org/W3216484856",
    "https://openalex.org/W4367310920",
    "https://openalex.org/W4392928544",
    "https://openalex.org/W3003721587",
    "https://openalex.org/W6796581206",
    "https://openalex.org/W2041282815",
    "https://openalex.org/W2061554433",
    "https://openalex.org/W2327037637",
    "https://openalex.org/W4392063639",
    "https://openalex.org/W4392542153",
    "https://openalex.org/W3168867926"
  ],
  "abstract": "Abstract Objective This study aims to investigate the feasibility of using Large Language Models (LLMs) to engage with patients at the time they are drafting a question to their healthcare providers, and generate pertinent follow-up questions that the patient can answer before sending their message, with the goal of ensuring that their healthcare provider receives all the information they need to safely and accurately answer the patientâ€™s question, eliminating back-and-forth messaging, and the associated delays and frustrations. Methods We collected a dataset of patient messages sent between January 1, 2022 to March 7, 2023 at Vanderbilt University Medical Center. Two internal medicine physicians identified 7 common scenarios. We used 3 LLMs to generate follow-up questions: (1) Comprehensive LLM Artificial Intelligence Responder (CLAIR): a locally fine-tuned LLM, (2) GPT4 with a simple prompt, and (3) GPT4 with a complex prompt. Five physicians rated them with the actual follow-ups written by healthcare providers on clarity, completeness, conciseness, and utility. Results For five scenarios, our CLAIR model had the best performance. The GPT4 model received higher scores for utility and completeness but lower scores for clarity and conciseness. CLAIR generated follow-up questions with similar clarity and conciseness as the actual follow-ups written by healthcare providers, with higher utility than healthcare providers and GPT4, and lower completeness than GPT4, but better than healthcare providers. Conclusion LLMs can generate follow-up patient messages designed to clarify a medical question that compares favorably to those generated by healthcare providers.",
  "full_text": null,
  "topic": "CLARITY",
  "concepts": [
    {
      "name": "CLARITY",
      "score": 0.946223258972168
    },
    {
      "name": "Health care",
      "score": 0.7427797317504883
    },
    {
      "name": "Completeness (order theory)",
      "score": 0.4957984387874603
    },
    {
      "name": "Computer science",
      "score": 0.47159823775291443
    },
    {
      "name": "Medical education",
      "score": 0.3666267395019531
    },
    {
      "name": "Psychology",
      "score": 0.3341594338417053
    },
    {
      "name": "Medical emergency",
      "score": 0.33221837878227234
    },
    {
      "name": "Medicine",
      "score": 0.3220764696598053
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Economic growth",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I200719446",
      "name": "Vanderbilt University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I901861585",
      "name": "Vanderbilt University Medical Center",
      "country": "US"
    }
  ],
  "cited_by": 22
}