{
  "title": "Generating clickbait spoilers with an ensemble of large language models",
  "url": "https://openalex.org/W4389009561",
  "year": 2023,
  "authors": [
    {
      "id": null,
      "name": "Woźny, Mateusz",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5093538450",
      "name": "Lango Mateusz",
      "affiliations": [
        "Charles University",
        "Poznań University of Technology"
      ]
    },
    {
      "id": null,
      "name": "Wo\\'zny, Mateusz",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2964303773",
    "https://openalex.org/W4393475271",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W2149427297",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2952861497",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W2949535404",
    "https://openalex.org/W2123301721",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4285119699"
  ],
  "abstract": "Clickbait posts are a widespread problem in the webspace. The generation of\\nspoilers, i.e. short texts that neutralize clickbait by providing information\\nthat satisfies the curiosity induced by it, is one of the proposed solutions to\\nthe problem. Current state-of-the-art methods are based on passage retrieval or\\nquestion answering approaches and are limited to generating spoilers only in\\nthe form of a phrase or a passage. In this work, we propose an ensemble of\\nfine-tuned large language models for clickbait spoiler generation. Our approach\\nis not limited to phrase or passage spoilers, but is also able to generate\\nmultipart spoilers that refer to several non-consecutive parts of text.\\nExperimental evaluation demonstrates that the proposed ensemble model\\noutperforms the baselines in terms of BLEU, METEOR and BERTScore metrics.\\n",
  "full_text": "Proceedings of the 16th International Natural Language Generation Conference, pages 431–436\nSeptember 11–15, 2023. ©2023 Association for Computational Linguistics\n431\nGenerating clickbait spoilers with an ensemble of large language models\nMateusz Wo´zny1 and Mateusz Lango1,2\n1 Poznan University of Technology, Faculty of Computing and Telecommunications, Poznan, Poland\n2 Charles University, Faculty of Mathematics and Physics, Prague, Czech Republic\nmateusz.wozny@student.put.edu.pl, mlango@cs.put.edu.pl\nAbstract\nClickbait posts are a widespread problem in the\nwebspace. The generation of spoilers, i.e. short\ntexts that neutralize clickbait by providing in-\nformation that satisfies the curiosity induced\nby it, is one of the proposed solutions to the\nproblem. Current state-of-the-art methods are\nbased on passage retrieval or question answer-\ning approaches and are limited to generating\nspoilers only in the form of a phrase or a pas-\nsage. In this work, we propose an ensemble of\nfine-tuned large language models for clickbait\nspoiler generation. Our approach is not limited\nto phrase or passage spoilers, but is also able\nto generate multipart spoilers that refer to sev-\neral non-consecutive parts of text. Experimen-\ntal evaluation demonstrates that the proposed\nensemble model outperforms the baselines in\nterms of BLEU, METEOR and BERTScore\nmetrics.\n1 Introduction and related works\nCatchy headlines or social media posts designed\nto entice users to click, known as clickbait, are\nwidespread on the internet. Although they often\nincrease website traffic and generate revenue, they\nusually fall short of readers’ expectations, wasting\ntheir time and causing disappointment (Molyneux\nand Coddington, 2020).\nTo deal with this problem, Rubin et al. (2015)\nproposed the clickbait detection task, which re-\nceived some research attention (Potthast et al.,\n2016; Chakraborty et al., 2016). More recently,\nHagen et al. (2022a) found that clickbaits can be\nneutralized by providing short texts that clarify\nwhat the reader can expect from the linked article,\noften making the clickbait uninteresting. They de-\nveloped corpora of clickbait spoilers and classified\nthem into three types: phrase spoilers (containing\na single word or a short phrase), passage spoilers\n(a few sentences at most), and multi-part spoilers\ntype clickbait spoiler\nphrase You’re missing\nthis major way to\nsave money\npromotional\ncode\npassage Scientists unearth\nbig surprise near\ncelebrated pyra-\nmids\nremains of a\nbustling port\nand barracks for\nsailors or troops\nmulti This is what RE-\nALLY happens\nwhen you don’t\nbrush your teeth\nBad breath,\nCoronary heart\ndisease, Bleed-\ning gums, (...)\nTable 1: Abbreviated examples of spoilers and clickbaits\nfrom Webis-Clickbait-22 corpus (Hagen et al., 2022a)\n(containing many non-consecutive phrases and/or\npassages). See the examples in Tab. 1.\nHagen et al. (2022a) also experimented with 20\napproaches for clickbait spoiler generation, which\nwere based on passage retrieval or extractive ques-\ntion answering algorithms. However, all of these\nmethods were only evaluated on phrase and pas-\nsage spoilers, as they are not suitable for generating\nmulti-part spoilers and their generation requires a\nspecialised approach.\nIn this work, we demonstrate that all three types\nof spoilers can be effectively generated by means\nof conditional language generation with large lan-\nguage models. We put forward a simple yet effec-\ntive proposal of an ensemble of LLMs that selects\nthe final spoiler by exploiting learning-to-rank tech-\nniques. Finally, we verify the performance of the\nproposed approach and investigate the possibility\nof combining it with previously developed methods\nthat provide phrase and passage spoilers.\n432\n2 Ensemble of LLMs for clickbait spoiler\ngeneration\nThe task of clickbait spoiler generation is defined\nas follows. For a given clickbait text c, the content\nof the linked article a and the requested spoiler\ntype t, generate a textual spoiler s whose goal is\nto make the clickbait c uninteresting for the user\nby providing the additional information from the\nreferred article a. The possible types of spoilers t\nare phrase, passage, and multi-part.\nIn this paper, we propose an ensemble of lan-\nguage models for clickbait spoiler generation. The\nproposed approach consists of three steps: convert-\ning the text of a clickbait c into a question q, gen-\nerating candidate spoilers from various prompted\nlarge language models, and finally selecting the\nfinal spoiler by a trained scoring model.\n2.1 Converting clickbaits to questions\nClickbaits usually take the form of declarative or\nexclamatory sentences. In contrast, question an-\nswering, which is one of the most related tasks ac-\ncording to related works, naturally deals with prob-\nlems structured as interrogative sentences. Due to\ntheir popularity, QA datasets are often used as a\npart of LLMs’ (pre)training sets, enabling better\nknowledge transfer for these tasks. Therefore, to\nbetter exploit knowledge acquired by LLMs dur-\ning pretraining, we convert each clickbait into a\nquestion before passing it for further processing.\nThe conversion is made in a zero-shot fashion\nusing the recent Vicuna language model (Chiang\net al., 2023). For each clickbait c, we construct\nthe following prompt: ”Below is a sentence from\nwhich write a question. \\n Sentence: c \\n Ques-\ntion:”, where \\n is the sign of a new line. The\nresulting question q is generated by initializing the\nlanguage model with the prompt and completing\nthe text with the greedy search algorithm until the\nsign of a new line is generated.\n2.2 Generating spoilers with LLMs\nThe next step of our approach is to use a set of\ndifferent pretrained language models to produce a\ndiversified set of spoiler candidates.\nEach component of our ensemble is fine-tuned\non the standard language modeling task using an\nadapter-based approach LoRA (Hu et al., 2021).\nSuch transfer learning approaches allow parameter-\nefficient fine-tuning by leaving all the pretrained\nweights unchanged and modifying the model op-\nerations by adding shallow, trainable feed-forward\nnetworks between the transformer layers. The re-\nsults of these feed-forward networks are incorpo-\nrated into the transformer architecture by adding\ntheir output to the output of successive transformer\nlayers. Such fine-tuning approaches have proven\nto be well-suited for relatively small supervised\ndatasets like ours (Houlsby et al., 2019).\nIn order to create training corpora for the lan-\nguage modeling task, each training example was\nconverted to a textual form by filling in the hand-\ndesigned prompt template: ”Below is a question\npaired with a context for which you should gener-\nate an answer. Write an answer with type t that\nappropriately completes the question.\\n Question:\nq \\n Context: a \\n Answer: s\\n ”. During training,\ncross-entropy loss was optimized, i.e.\n−\nnX\ni=1\nlog P(wi|w1, w2, ..., wi−1)\nwhere wi is the i-th token of the filled template\nand n is its length. During testing, standard greedy\ndecoding was used to retrieve the clickbait spoiler.\n2.3 Selecting the final spoiler with scoring\nmodel\nAfter generating several clickbait spoiler candi-\ndates, the final step of the approach is to select\nthe most appropriate spoiler by running a trainable\nmodel that evaluates them. This problem can be\nviewed as a learning-to-rank problem (Liu, 2009),\nwhere our goal is to construct the ranking of spoiler\ncandidates and later select the best candidate i.e.\nthe spoiler at the top of the ranking. We experiment\nwith two popular learning-to-rank approaches: 1) a\npointwise approach, which assigns a score to each\ncandidate and later uses it to sort the list of candi-\ndates. 2) a pairwise approach, which compares all\npairs of candidates and decides which spoiler from\nthe pair is more suitable.\n2.3.1 Pointwise approach\nTo evaluate each candidate, we develop a regressor\nthat tries to predict the value of BLEU score for\neach spoiler. As a regressor fine-tuned DeBERTa\nmodel (He et al., 2021) with one linear layer on top\nof CLS token is used. The input to the model con-\nsists of a question (clickbait) q, candidate spoiler\ns, and article a, separated by the sign of a new line\nand concatenated into one input text. The output of\nthe model is the predicted BLEU score.\n433\nThe training data for the regressor was generated\nby running all the LLMs used in the ensemble on\nthe training data and evaluating their BLEU score\nagainst the available gold standard. During training,\nthe classical sum of squared errors was optimized.\n2.3.2 Pairwise approach\nThe second method considered for selecting the\nbest spoiler among the candidates is the pairwise\napproach. This approach relies on a classifier that,\nfor a given pair of spoiler candidates, decides which\nof them is more suitable. More specifically, the\nBERT-based classifier receives the same input as\nin the pointwise approach, but with two spoiler\ncandidates s1 and s2. The output of the binary\nclassification model is 1 if s1 is better than s2 in\nterms of BLEU, and 0 otherwise.\nThe classifier is trained on the generated data as\nfollows. First, all LLMs used in the ensemble were\nrun on the training set, generating a collection of\nspoiler candidates for each clickbait. Later, all pos-\nsible pairs from each collection were considered\nand converted into binary classification instances\nby comparing the BLEU scores of the candidates.\nThe pairs containing spoilers with BLEU = 0 or\npairs containing identical spoilers were removed\nfrom the training data. During training, the stan-\ndard cross-entropy loss was optimised.\n3 Experiments\nWe have performed computational experiments\naimed at verifying the effectiveness of ensembling\nLLMs with pointwise and pairwise rankers, and\ncomparing its effectiveness with the previous SOTA\nmethods based on question answering. In addition,\nwe also investigate the possibility of combining the\nprevious QA approaches, which are best suited for\npassage and phrase spoiler types, with the proposed\napproach for multi-part spoilers.\n3.1 Experimental setup\nWe experiment with an ensemble of three fine-\ntuned LLMs, which were constructed from two\npretrained models: LLaMA (Touvron et al., 2023)\nand Vicuna (Chiang et al., 2023). Both of these\nmodels are open-source and were fine-tuned us-\ning the prompt described in Sec. 2.2. However,\nwe observed additional improvements with Vicuna\nmodel while using specially tailored prompts for\neach spoiler type separately (see appendix), there-\nfore we also report the results of this fine-tuned\nmodel and use it as a part of the ensemble.\nThe obtained results were compared against the\nperformance of two extractive QA approaches,\nwhich on top of pretrained encoder perform be-\ngin/end span classification 1. These approaches\nare based on RoBERTa (Liu et al., 2019) and De-\nBERTa (He et al., 2021) models since among 20\napproaches compared on clickbait spoiler genera-\ntion task by Hagen et al. (2022a) these two were the\nmost effective ones. Note, that the results reported\nfor these approaches in this work are significantly\nlower than therein, since we report the performance\nover all three types of spoilers, including multipart.\nThe ensemble approach with the pointwise\nranker used DeBERTa-based regressor, which ob-\ntained MSE of 0.384 on test set. Similarly, the\nclassifier used in the pairwise approach achieved\nbalanced accuracy of 90,8% on the test data.\nFollowing earlier works, we evaluated the ap-\nproaches with three metrics: BLEU (Papineni et al.,\n2002), METEOR (Banerjee and Lavie, 2005), and\nBERT Score (Zhang et al., 2020). All experiments\nwere performed on a single Nvidia A100 GPU.\nSome additional experiment details and results\ncan be found in the online appendix2.\n3.2 Comparing the proposed approaches with\nrelated works\nThe results of QA-based approaches, ensemble\nmodels as well as individual fine-tuned LLMs can\nbe found in Table 2. The best-performing ap-\nproach according to BLEU, METEOR, and BERT\nScore F1 is the proposed ensemble with a point-\nwise ranker. This ensemble provides the improve-\nment of approx. 2 percentage points in terms of\nBLEU and METEOR over the best of its compo-\nnents i.e. Vicuna model with specific prompts for\neach spoiler type. The second-best approach was\nthe ensemble with pairwise ranker which offered\nlimited improvement over the individual LLMs.\nOverall, each of the approaches using LLMs ob-\ntained better results than previous state-of-the-art\napproaches based on extractive question answer-\ning. The only metric on which the QA-based ap-\nproaches (RoBERTa and DeBERTa) stand out is\nBERTScore Precision. Still, BERTScore Recall is\nhigher for LLM and ensemble approaches, making\nthem more effective in terms of F1 measure, which\ncombines both precision and recall.\n1This is the default fine-tuning approach for QA-task of\nBERT (Devlin et al., 2019), more details therein.\n2https://www.cs.put.poznan.pl/mlango/\npublications/inlg23.pdf\n434\nBERT Score\nModel BLEU METEOR Precision Recall F1\nRoBERTa 31,78 0,387 0,904 0,883 0,893\nDeBERTa 32,20 0,398 0,907 0,884 0,894\nLLaMA 13B 37,70 0,474 0,895 0,901 0,897\nVicuna 13B 38,80 0,481 0,898 0,903 0,900\nVicuna 13B with type-based prompts 40,02 0,492 0,899 0,905 0,901\nEnsemble with pairwise ranker 40,76 0,500 0,901 0,907 0,904\nEnsemble with pointwise ranker 42,13 0,517 0,902 0,909 0,905\nTable 2: The experimental results of previous state-of-the-art QA-based methods compared with our ensembling\napproaches and LLMs. All the metrics are computed on Webis Clickbait 22 corpora (Hagen et al., 2022b).\nBERT Score\nModel BLEU METEOR Precision Recall F1\nDeBERTa 32,20 0,398 0,907 0,884 0,894\nDeBERTa trained on questions 37,82 0,451 0,913 0,895 0,903\nBaseline ensemble 42,28 0,506 0,910 0,908 0,909\nEnsemble with pairwise ranker 43,57 0,520 0,912 0,911 0,911\nEnsemble with pointwise ranker 44,45 0,532 0,911 0,913 0,911\nTable 3: The experimental results of the ensemble that combines previously proposed methods for passage and\nphrase spoiler types (DeBERTa) with the newly proposed approaches for clickbait spoiler generation.\n3.3 Combining previous SOTA models with\nthe proposed ones\nIn the final experiment, we decided to verify\nwhether it is possible to obtain even better results\nby combining our approaches with QA-based mod-\nels previously designed for clickbait spoilers of\nphrase and passage types only.\nAs a QA model, we use the fine-tuned DeBERTa\nmodel, since it gave the best results both in our\nexperiments from the previous section and in the\nexperiments of Hagen et al. (2022a). As we men-\ntioned in Sec. 2.1, our approaches generate spoilers\nfor clickbaits converted into interrogative sentences\nin order to facilitate better knowledge transfer from\nthe pre-trained models. We also fine-tuned De-\nBERTa on such preprocessed data and found im-\nproved performance. Therefore, this model (later\ndenoted DeBERTa-q or ”DeBERTa trained on ques-\ntions”) was used for ensembling.\nThe operation of ensembles reported in this sec-\ntion slightly differs from what was described in\nSec 2.3 to better account for QA-based approaches’\nsuitability for passage and phrase spoiler types. If\nthe generated spoiler is of mutli-part type, the list\nof candidate spoilers is generated as previously, i.e.\nfrom three fine-tuned LLMs. However, if the gen-\nerated spoiler is of phrase/passage type then only\nspoiler candidates from DeBERTa-q and Vicuna\nwith customised prompts are considered. As previ-\nously, the selection of the best spoiler among the\ncandidates is performed by a ranker.\nThe results of these approaches are reported in\nTable 3. It can be seen that DeBERTa-q achieves\nsignificantly better results than DeBERTa for all\nmetrics considered. However, the ensemble with\nLLMs provides further significant improvements.\nAs a form of sanity check for our ensemble ap-\nproach that uses a ranker to select the best spoiler,\nwe have also implemented a trivial ensemble (base-\nline ensemble) that uses DeBERTa-q for all passage\nand phrase spoilers and Vicuna with type-based\nprompts for multi-part spoilers. Although such a\nform of ensembling is also advantageous in that the\nperformance obtained is better than that of the in-\ndividual models, using any variant of the proposed\nensemble with a ranker still improves the results.\nFor example, for the METEOR measure, the im-\nprovement is almost 2% for the pairwise approach\nand 3% for the pointwise approach.\nTable 4 presents a more detailed analysis of\nthe performance of the spoiler generation meth-\nods, i.e. the results are reported separately for each\nspoiler type. Although the newly proposed ensem-\nble approaches achieve better performance for each\nspoiler type, the most significant improvement is\nobserved for the most difficult multi-part spoilers,\n435\nBERT Score\nModel spoiler type BLEU METEOR Recall F1 Precision\nDeBERTa\nphrase 56,00 0,569 0,934 0,932 0,931\npassage 20,10 0,304 0,858 0,869 0,883\nmulti-part 2,07 0,204 0,822 0,860 0,902\nDeBERTa trained on questions\nphrase 62,50 0,627 0,946 0,942 0,939\npassage 21,60 0,319 0,861 0,872 0,885\nmulti-part 4,10 0,224 0,829 0,862 0,900\nEnsemble with pairwise ranker\nphrase 63,30 0,617 0,945 0,943 0,942\npassage 30,00 0,416 0,882 0,887 0,894\nmulti-part 27,30 0,525 0,894 0,886 0,879\nEnsemble with pointwise ranker\nphrase 65,30 0,645 0,950 0,946 0,942\npassage 30,30 0,424 0,884 0,887 0,892\nmulti-part 28,60 0,522 0,894 0,887 0,881\nTable 4: The performance of clickbait spoiler generation models for each spoiler type.\nClickbait Reference DeBERTa Vicuna\nAgency might plant\na garden on the\nmoon.\nNASA NASA lunar sunlight\n10 habits of incredi-\nbly happy people\n1 they slow down to appre-\nciate life’s little pleasure 2\nthey exercise 3 they spend\nmoney on other people (...)\nthey have a growth\nmindset\n1 they slow down to appre-\nciate life’s little pleasure 2\nthey exercise 3 they spend\nmoney on other people (...)\nTable 5: Two examples of spoilers generated by different methods. The spoilers in italics were selected by pointwise\nranker (The ensemble also includes the LLaMA model, the output of which is not shown due to page limits.).\nwhich are the focus of our paper. For example, the\nbest ensemble model achieves an improvement of\nover 25 in terms of BLEU score over the previous\nDeBERTa model.\nTwo examples of generated spoilers are pre-\nsented in Table 5 (more examples in the appendix).\nIt can be observed that the previous SOTA approach\n(DeBERTa) fails to extract the correct spoiler of the\nmulti-part type, but Vicuna model generates it cor-\nrectly. In contrast, DeBERTa extracts the correct\nphrase spoiler and Vicuna fails to do so. In both\ncases, the pointwise ranker was able to indicate the\ncorrect spoiler.\n4 Summary\nIn this paper, we have shown that using fine-tuned\nLLMs can be a simple, yet effective way of deal-\ning with clickbait spoiler generation for all three\nspoiler types considered, i.e. including multi-part\nspoilers. We also demonstrated that ensembling\nseveral such models with a ranker that selects the\nmost suitable spoiler leads to improved results, es-\npecially when using a pointwise ranker. Finally, we\nshow that combining state-of-the-art approaches\nfor phrase and passage spoiler types based on ques-\ntion answering with the newly proposed ones based\non LLMs leads to further improvements.\nSupplementary Materials Availability State-\nment: Source code is available on Github reposi-\ntory3. All experiments were performed on Webis\nClickbait 22 corpora which is available on Zen-\nodo (Hagen et al., 2022b).\nAcknowledgments M. Wo´zny is supported by\nAI Tech project sponsored from POPC programme\nPOPC.03.02.00-00-0001/20. M. Lango is sup-\nported by European Research Council (Grant agree-\nment No. 101039303 NG-NLG).\nReferences\nSatanjeev Banerjee and Alon Lavie. 2005. METEOR:\nAn automatic metric for MT evaluation with im-\n3https://github.com/mateusz-wozny/\nspoiler_generation\n436\nproved correlation with human judgments. In Pro-\nceedings of the ACL Workshop on Intrinsic and Ex-\ntrinsic Evaluation Measures for Machine Transla-\ntion and/or Summarization, pages 65–72, Ann Arbor,\nMichigan. Association for Computational Linguis-\ntics.\nAbhijnan Chakraborty, Bhargavi Paranjape, Sourya\nKakarla, and Niloy Ganguly. 2016. Stop clickbait:\nDetecting and preventing clickbaits in online news\nmedia. CoRR, abs/1610.09786.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nMatthias Hagen, Maik Fr ¨obe, Artur Jurk, and Martin\nPotthast. 2022a. Clickbait Spoiling via Question\nAnswering and Passage Retrieval. In 60th Annual\nMeeting of the Association for Computational Lin-\nguistics (ACL 2022), pages 7025–7036. Association\nfor Computational Linguistics.\nMatthias Hagen, Maik Fr ¨obe, Artur Jurk, and Martin\nPotthast. 2022b. Webis clickbait spoiling corpus\n2022.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2021. Deberta: Decoding-enhanced\nbert with disentangled attention. In International\nConference on Learning Representations.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin de Laroussilhe, Andrea Ges-\nmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for NLP. In Pro-\nceedings of the 36th International Conference on Ma-\nchine Learning, ICML 2019, 9-15 June 2019, Long\nBeach, California, USA, volume 97 of Proceedings\nof Machine Learning Research, pages 2790–2799.\nPMLR.\nEdward Hu, Yelong Shen, Phil Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Lu Wang, and Weizhu Chen. 2021.\nLora: Low-rank adaptation of large language models.\nTie-Yan Liu. 2009. Learning to rank for information\nretrieval. Found. Trends Inf. Retr., 3(3):225–331.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nLogan Molyneux and Mark Coddington. 2020. Ag-\ngregation, clickbait and their effect on perceptions\nof journalistic credibility and quality. Journalism\nPractice, 14(4):429–446.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei\njing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. pages 311–318.\nMartin Potthast, Sebastian K ¨opsel, Benno Stein, and\nMatthias Hagen. 2016. Clickbait detection. In Euro-\npean Conference on Information Retrieval.\nVictoria Rubin, Nadia Conroy, and Yimin Chen. 2015.\nTowards news verification: Deception detection meth-\nods for news discourse.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth´ee Lacroix,\nBaptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. Bertscore: Eval-\nuating text generation with bert. In International\nConference on Learning Representations.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8308098316192627
    },
    {
      "name": "Phrase",
      "score": 0.7974416017532349
    },
    {
      "name": "Language model",
      "score": 0.6430033445358276
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5680583119392395
    },
    {
      "name": "Natural language processing",
      "score": 0.5269145369529724
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I46597724",
      "name": "Poznań University of Technology",
      "country": "PL"
    },
    {
      "id": "https://openalex.org/I21250087",
      "name": "Charles University",
      "country": "CZ"
    }
  ]
}