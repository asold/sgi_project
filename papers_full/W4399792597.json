{
    "title": "Exploiting Privacy Vulnerabilities in Open Source LLMs Using Maliciously Crafted Prompts",
    "url": "https://openalex.org/W4399792597",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5099186121",
            "name": "Géraud Choquet",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5099186122",
            "name": "Aimée Aizier",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5099186123",
            "name": "Gwenaëlle Bernollin",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4399383377",
        "https://openalex.org/W4319301677",
        "https://openalex.org/W4398196388",
        "https://openalex.org/W4396762959",
        "https://openalex.org/W4380993086",
        "https://openalex.org/W4392449489",
        "https://openalex.org/W4393191843",
        "https://openalex.org/W4392756413",
        "https://openalex.org/W4394846382",
        "https://openalex.org/W4393024052",
        "https://openalex.org/W4386080903",
        "https://openalex.org/W4221045317",
        "https://openalex.org/W4390497390",
        "https://openalex.org/W4391234699",
        "https://openalex.org/W4386420753",
        "https://openalex.org/W4387559390",
        "https://openalex.org/W4396701991",
        "https://openalex.org/W4398774455",
        "https://openalex.org/W4399915806",
        "https://openalex.org/W4389153356",
        "https://openalex.org/W4398782346",
        "https://openalex.org/W4327810158",
        "https://openalex.org/W3174335585",
        "https://openalex.org/W4387929855",
        "https://openalex.org/W4383469362",
        "https://openalex.org/W4391555522",
        "https://openalex.org/W4395082149",
        "https://openalex.org/W4396914983",
        "https://openalex.org/W4385187421",
        "https://openalex.org/W4380559074"
    ],
    "abstract": "<title>Abstract</title> The proliferation of AI technologies has brought to the forefront concerns regarding the privacy and security of user data, particularly with the increasing deployment of powerful language models such as Llama. A novel concept investigated involves inducing privacy breaches through maliciously crafted prompts, highlighting the potential for these models to inadvertently reveal sensitive information. The study systematically evaluated the vulnerabilities of the Llama model, employing an automated framework to test and analyze its responses to a variety of crafted inputs. Findings reveal significant security flaws, demonstrating the model's susceptibility to adversarial attacks that could compromise user privacy. Comprehensive analysis provided insights into the types of prompts most effective in eliciting private data, and the study demonstrates the necessity for robust regulatory frameworks and advanced security measures. The implications of these findings are profound, calling for immediate action to enhance the security protocols of LLMs and protect against potential privacy breaches. Enhanced regulatory oversight and continuous innovation in privacy-preserving techniques are crucial to ensuring the safe deployment of LLMs in various applications. The insights derived from this research contribute to a deeper understanding of LLM vulnerabilities and the urgent need for improved safeguards to prevent data leakage and unauthorized access.",
    "full_text": null
}