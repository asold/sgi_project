{
  "title": "Ask an Expert: Leveraging Language Models to Improve Strategic Reasoning in Goal-Oriented Dialogue Models",
  "url": "https://openalex.org/W4385570282",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5100382017",
      "name": "Qiang Zhang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5082047528",
      "name": "Jason Naradowsky",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5004444958",
      "name": "Yusuke Miyao",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3156470785",
    "https://openalex.org/W3174696767",
    "https://openalex.org/W3186138538",
    "https://openalex.org/W3198963017",
    "https://openalex.org/W4206816337",
    "https://openalex.org/W2951697502",
    "https://openalex.org/W4361866031",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W4303939357",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W4288113479",
    "https://openalex.org/W2988937804",
    "https://openalex.org/W4321392130",
    "https://openalex.org/W4318716950",
    "https://openalex.org/W4287887667",
    "https://openalex.org/W4226198591",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4290771878",
    "https://openalex.org/W2317914832",
    "https://openalex.org/W2623779865",
    "https://openalex.org/W4324304837",
    "https://openalex.org/W3166593409",
    "https://openalex.org/W2761590056",
    "https://openalex.org/W3155584966",
    "https://openalex.org/W3037763555",
    "https://openalex.org/W2963789888",
    "https://openalex.org/W3034533785",
    "https://openalex.org/W3186804217",
    "https://openalex.org/W4226156057",
    "https://openalex.org/W3172642864",
    "https://openalex.org/W2795668206",
    "https://openalex.org/W3192478068",
    "https://openalex.org/W2951583236",
    "https://openalex.org/W3034600233",
    "https://openalex.org/W2007099156",
    "https://openalex.org/W2889335577",
    "https://openalex.org/W3174116563",
    "https://openalex.org/W3153427360",
    "https://openalex.org/W3100523370",
    "https://openalex.org/W3206381865",
    "https://openalex.org/W3196731672",
    "https://openalex.org/W4287900772",
    "https://openalex.org/W2963520511",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3207166518",
    "https://openalex.org/W2963206148",
    "https://openalex.org/W2963491014",
    "https://openalex.org/W2962974452",
    "https://openalex.org/W3201090304",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2171802301",
    "https://openalex.org/W4200509401",
    "https://openalex.org/W3176456866",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W3102788578",
    "https://openalex.org/W4206636317",
    "https://openalex.org/W2998617917"
  ],
  "abstract": "Existing dialogue models may encounter scenarios which are not well-represented in the training data, and as a result generate responses that are unnatural, inappropriate, or unhelpful. We propose the \"Ask an Expert\" framework in which the model is trained with access to an \"expert\" which it can consult at each turn. Advice is solicited via a structured dialogue with the expert, and the model is optimized to selectively utilize (or ignore) it given the context and dialogue history. In this work the expert takes the form of an LLM.We evaluate this framework in a mental health support domain, where the structure of the expert conversation is outlined by pre-specified prompts which reflect a reasoning strategy taught to practitioners in the field. Blenderbot models utilizing \"Ask an Expert\" show quality improvements across all expert sizes, including those with fewer parameters than the dialogue model itself. Our best model provides a ~10% improvement over baselines, approaching human-level scores on \"engingingness\" and \"helpfulness\" metrics.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 6665–6694\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nAsk an Expert: Leveraging Language Models\nto Improve Strategic Reasoning in Goal-Oriented Dialogue Models\nQiang Zhang, Jason Naradowsky, Yusuke Miyao\nDepartment of Computer Science\nThe University of Tokyo\n{qiangzhang714, narad, yusuke}@is.s.u-tokyo.ac.jp\nAbstract\nExisting dialogue models may encounter sce-\nnarios which are not well-represented in the\ntraining data, and as a result generate responses\nthat are unnatural, inappropriate, or unhelpful.\nWe propose the “Ask an Expert” framework in\nwhich the model is trained with access to an\n“expert” which it can consult at each turn. Ad-\nvice is solicited via a structured dialogue with\nthe expert, and the model is optimized to se-\nlectively utilize (or ignore) it given the context\nand dialogue history. In this work the expert\ntakes the form of an LLM. We evaluate this\nframework in a mental health support domain,\nwhere the structure of the expert conversation is\noutlined by pre-specified prompts which reflect\na reasoning strategy taught to practitioners in\nthe field. Blenderbot models utilizing “Ask an\nExpert” show quality improvements across all\nexpert sizes, including those with fewer param-\neters than the dialogue model itself. Our best\nmodel provides a ∼ 10% improvement over\nbaselines, approaching human-level scores on\n“engingingness” and “helpfulness” metrics.\n1 Introduction\nDialogue systems based on pre-trained language\nmodels (PLMs) can be easily tailored via fine-\ntuning to exhibit particular characteristics, such\nas empathy (Roller et al., 2021) and emotion (Adi-\nwardana et al., 2020). However, it has been previ-\nously observed that such models tend to produce\nvacuous “fallback” responses when presented with\nunfamiliar situations (e.g., extraneous (Li et al.,\n2016; Adiwardana et al., 2020)). For instance, we\nobserve that fine-tuned BlenderBot (Roller et al.,\n2021) models have a propensity to use the response,\n“Do you have any hobbies? ” as a substitute for\nfurthering the conversation in helpful ways when\nthe situation becomes too complicated. For goal-\ndirected dialogues, where the discourse should con-\nsistently move towards a desired resolution or ef-\nfect (Ham et al., 2020), frequent reliance on such\nI am so worried about the coming\nexams. What should I do?\nI know you can do it!\nAsk an expert\nSeeker\nSeeker\nI am so worried about the coming\nexams. What should I do?\nWhat is the emotional status of\nthe seeker?\nThe seeker feels worried.\nWhy is the seeker worried?\nThe seeker feels worried about\nperformance in upcoming exams.\nHow can the seeker improve the\nperformance in the exams?\nIf you worry about the exams, you\nmay consider joining a study\ngroup to share ideas with others.\nStudying in a group and sharing\nideas with others can help.\nFigure 1: The proposed method of consulting the expert,\nwhere the dialogue model interactively obtains advice\nfrom the LLM via prompting (e.g. GPT3). Without the\naid of expert knowledge and reasoning, dialogue models\nare less able to generate useful and engaging responses.\nfallback responses may result in them performing\npoorly.\nWe hypothesize that the use of fallback re-\nsponses may stem from the model being unable to\nformulate a more suitable reply in the absence of ap-\npropriate knowledge of the situation. In this study,\nwe propose a framework called \"Ask an Expert\"\nto enhance dialogue responses through on-the-fly\nknowledge acquisition. Our approach involves in-\ntegrating dialogue models with an external “expert”\nby the following tenets: (a) the expert is a large\nlanguage model (LLM) which is available both dur-\ning training and inference, (b) the act of soliciting\ninformation from the expert itself takes the form of\n6665\na dialogue, which can span multiple turns in order\nto identify relevant information and strategies, and\n(c) the knowledge is integrated into the dialogue\nmodel via the context. Recently many efforts have\nsought to utilize text as an API to chain together\nmultiple models to perform complex tasks (Shen\net al., 2023; Chase, 2022). Our approach differs\nin that the model interaction takes place within the\noptimization loop, and thus allows the dialogue\nmodel to learn to selectively choose which advice\nto incorporate, and when use it.\nWe apply “Ask an Expert” to the domain of men-\ntal health support (MHS) systems. MHS is notable\nin being one of many domains in which practition-\ners are formally trained to follow specific discourse\nstrategies (Pudlinski, 2005). We incorporate an\nMHS strategy into the model via a series of hand-\ncrafted prompts, which are designed to shape the\nexpert conversation to reflect the inner monologue\nof a human expert (Figure 1). The resulting con-\nversation is then provided in a structured way as\nconditioning context to the dialogue model.\nWe perform human evaluations on the models\nfollowing the method of ACUTE-Eval (Li et al.,\n2019) to assess the system on six dimensions, in-\ncluding the ability to both have general conversa-\ntions and provide helpful suggestions. We find\nmodels with reasoning processes significantly out-\nperform the baseline model (without reasoning)\nin providing constructive suggestions and sharing\nsimilar experiences while remaining engaging and\nempathetic. Contributions of this work are as fol-\nlows:\n• We propose a novel way of formulating knowl-\nedge acquisition in dialogue models via a chat-\nbased interaction with a LLM expert, both\nduring training and inference.\n• We explore several design decisions for struc-\nturing the expert reasoning process, and evalu-\nate the effect of different prompts and formats,\n• We demonstrate that our approach results in\ndialogues that are deemed more engaging and\nhelpful as evaluated by human judges.\n• We study the effect of different experts on\ndialogue quality and present ablation experi-\nments on expert model size.\n2 Related Work\nIncorporating Knowledge in Dialogue Models\nVarious approaches have been proposed to incor-\nporate external knowledge into dialogue models.\nWithin the scope of deep learning-based models,\ninformation may be retrieved from a knowledge\nbase using key-value lookups (Eric et al., 2017)\nor as relation tuples (Young et al., 2018), or as\nencoded vectors from knowledge bases (Madotto\net al., 2018). Similar to our work, on-the-fly ac-\nquisition of knowledge is possible using the in-\nternet as an expert, and integrating search results\ninto the model (Wu et al., 2020; Komeili et al.,\n2022). In addition to relying on external knowl-\nedge sources, dialogue models can incorporate\nknowledge sources, such as pre-trained language\nmodels, directly into the decoding process to pro-\nduce responses grounded in knowledge. (Roller\net al., 2021; Xu et al., 2022; Shuster et al., 2022).\nOur approach instead leverage advances in prompt-\nbased text generation and the increasing capacity\nof LLMs to serve as knowledge bases in order to\nacquire knowledge as a set of dialogue responses.\nLLMs as Source of Expert Knowledge Large\nlanguage models (LLMs) exhibit a remarkable ca-\npacity to extract and retain knowledge embedded in\nthe training data. Prior studies have demonstrated\ntheir ability to extract different forms of general\nknowledge, including factual knowledge (Petroni\net al., 2019) and commonsense knowledge (Sap\net al., 2020), without requiring fine-tuning. Fur-\nthermore, LLMs can effectively store and re-\ntrieve domain-specific knowledge, such as phys-\nical knowledge (Bisk et al., 2020) and biomedical\nknowledge (Yuan et al., 2021b), through knowl-\nedge distillation training (Qin et al., 2022). Promi-\nnent models like ChatGPT 1 and Bard 2 demon-\nstrate impressive proficiency across various natural\nlanguage processing (NLP) tasks and find practical\napplications in diverse domains, such as health-\ncare (Biswas, 2023) and finance (Zaremba and\nDemir, 2023). These models not only possess\nextensive knowledge access but also effectively\nexpress this knowledge in natural language, ben-\nefiting from instruct-tuning technology (Ouyang\net al., 2022) and reinforcement learning from hu-\nman feedback (RLHF) (Christiano et al., 2017).\nLLMs for Data Generation and Augmenta-\ntion LLMs can be used to generate additional\nexamples to augment datasets across various\nNLP tasks and domains, such as text classifica-\n1https://openai.com/blog/chatgpt\n2https://bard.google.com/\n6666\ntion task (Wang et al., 2021), textual similarity\ntask (Schick and Schütze, 2021b), and knowledge\ndistillation task (West et al., 2022). Unlike previ-\nous works, we focus on the data augmentation task\nfor a dialogue dataset in the domain of mental peer\nsupport, ESConv (Liu et al., 2021) with additional\nannotations that come in the form of reasoning sup-\nport (emotion identification, cause, solution).\nChatbots for Mental Health Given the com-\nplexity of providing mental support, rule-based\napproaches are commonly employed to ensure the\ngenerated text adheres to the common behavior\nof practitioners in the domain. For MHS, these\nguiding rules and principles are agreed upon and\nproposed by human experts, such as PTSD Check-\nlist (DeVault et al., 2013), Cognitive Behavioural\nTherapy (CBT) (Fitzpatrick et al., 2017), Solution-\nfocused Brief Therapy (SFBT) (Fulmer et al., 2018)\nand mindfulness (Lee et al., 2019). However,\nsuch an approach requires significant efforts to be\nspent on designing rules and can not handle non-\npredefined situations. Our approach differs in that\nwe reduce the reliance on handcrafting rules by\nturning to simpler prompt templates, which can\nthen be used together with an LLM to acquire rele-\nvant expert knowledge and reasoning for a broad\nrange of different scenarios.\nAn alternative is a data-driven approach, wherein\ndeep learning-based dialogue models (Zhang et al.,\n2019b; Adiwardana et al., 2020; Roller et al.,\n2021) are trained or fine-tuned on emotion-related\ndatasets such as DailyDialogue (Li et al., 2017),\nEmpatheticDialogues (Rashkin et al., 2019), and\nEDOS (Welivita et al., 2021). Such models are\nable to produce more empathetic responses, how-\never, possibly due to the lack of explicit strategy,\nthey frequently generate vacuous or unrelated re-\nsponses.\n3 Ask an Expert\nThe architecture we propose, Ask an Expert, con-\nsists of a dialogue model, and a separate expert\nmodel. In this work the expert is a (presumably\nlarger or specialized) LLM. The key distinction be-\ntween ours and other work which uses additional\nknowledge acquisition in dialogue systems is that\nours takes the form of another dialogue, in which\nwe utilize prompts to guide the expert towards pro-\nviding important reasoning to guide the dialogue\nsystem’s response. The dialogue model is trained to\noptimize dialogue quality while working together\nContext conversation\nseeker: whenever we have family gathering, my aunts\nand uncles would brag about how much their children\nmake. I have higher degree but will only make half of\ntheir salary so I feel bad.\nsupporter: So, you feel that your family is judging you\nfor your earning potential?\nseeker: yes, my parents won't say it to me but they\nnever show they're proud either.\nGuideline\nGive a conversation between a seeker and a supporter,\npredict the emotion status of the seeker, the reason\ncausing that emotion and some conversation\ninstructions for the supporter. \nReasoning process \nIn this conversation, the seeker feels ashamed from\nwhat others in her family say about her. The\nsupporter could help the seeker by reminding her that\nshe can't control what others in her family says or think.\nShe should just focus on her own opinions and\nthoughts instead.\nInstance x N\nseeker: During this pandemic situation, most of the\ncompanies laid off their employees.\nsupporter: Just from these few messages, I can tell you\nare very anxious about this situation.\nseeker: Yes. I don't know what to do.\nThe seeker feels anxious about losing her job. The\nsupporter could suggest the seeker to search some job\ninformation online. \nFigure 2: An example of the dialogue-level prompt used\nfor knowledge acquisition in our setting. The green\nparts are generated by language models.\nwith the expert suggestions, and can therefore learn\nhow best to make use of advice in a context-specific\nmanner.\n3.1 Knowledge Acquisition via Dialogue\nIn mental health support (MHS), a seeker (person\nseeking help) engages in conversation with a sup-\nporter (the MHS practitioner) as a way of seeking\nmedical help. Like other medical professionals,\nguidelines and strategies exist for providing men-\ntal health support. Following the literature, we\nidentify a three-part strategy which involves: (1)\nidentifying the emotional status of the seeker, (2)\nidentifying the reason for that state if undesirable,\nand (3) providing suggestions that aim to allevi-\nate the underlying cause of the distress (Pudlinski,\n2005; Tietbohl, 2022). By designing prompts to\ncollect this information and provide it to the dia-\nlogue model, we aim to improve the model’s ability\nto provide useful support and reduce the extent to\nwhich it relies on unhelpful fallback responses.\n6667\nDesigning Prompts We compare two different\nstyles of prompts. The first, which we refer to ask\nquestion-answering (QA), phrases the prompts in\nthe form of questions (e.g., “Why does the seeker\nfeel upset with her mother?”). The second, which\nwe refer to as text-generation (TG) style echos the\nmasked language modeling objective of LLMs and\ntasks the model to complete a sentence with miss-\ning information (e.g., “The seeker feels upset with\nher mother because...”). Results of our initial ex-\nperiments comparing the two prompt styles can\nbe found in Appendix A. The remainder of the\nexperiments in this paper use TG-style prompts\nfollowing the previous works as in Schick and\nSchütze (2021a); Mishra et al. (2022a).\nThe second consideration in prompt design is\nthe available length of the prompt. We evaluate\nthe Ask an Expert architecture on a variety of base\nLLMs, ranging in size from GPT to GPT3, mean-\ning that the length of prompts that can fit within the\ncontextual window of the LLMs will vary greatly.\nHence we designed two different levels of prompt:\ndialogue-level prompt, in which the instances and\ncontext conversation are given as multi-turn dia-\nlogue pieces to provide more conversation context,\nand utterance-level prompt, in which they are re-\nduced to a two-turn dialogue reflecting the current\nseeker input and the previous supporter’s reply. Fig-\nure 2 shows examples of these prompt styles. Both\ntypes of prompts begin with a guideline to describe\nthe task because providing instructions helps LLMs\nto interpret the task better (Mishra et al., 2022b).\nThe guideline could also help LLMs to generate\nthe results with the required format as shown in\nAppendix B.\nThe context conversation is the history of\nthe preceding dialogue. In the utterance-level\nprompt, several utterances at the beginning of the\nconversation are trimmed to fit the input length of\nthe LLM. The result of this prompted conversation\nwith the expert is a piece of useful information that\na human practitioner may very well consider when\nshaping their responses to the human seeker. For\ninstance, a generated reasoning process may be as\nfollows:\n“The seeker feels overwhelmed and stressed. He is\nworried about his upcoming test. The supporter\nshould mention the idea of a study group or a zoom\nstudy group. The supporter could also mention\nFacetime with friends. ”\n3.2 Data Collection\nWe generate a training set consisting of partial dia-\nlogues annotated with the additional reasoning in-\nformation provided by the expert at each step. The\ndialogues are obtained from ESConv (Liu et al.,\n2021), a dataset of mental health support dialogues.\nESConv is especially well-suited for our research\nbecause crowdsourcing workers are trained to be-\ncome supporters when collecting the dataset, and\nthe original annotations on emotion, situation, and\nstrategy can be referred to when designing prompts.\nThe Ask an Expert architecture is modular, and\nmany models (or humans) could theoretically take\nthe role of the expert. In this work we wish to assess\nthe importance of model size on reasoning ability\nand quality of dialogue, and we use the following\nLLMs as experts: OpenAI GPT (GPT1) (Radford\net al., 2018), GPT2 (Radford et al., 2019), and\nGPT3 (ada and davinci) (Brown et al., 2020).\nWe balance the data by selecting batches of 8\ninstances with different combinations of 5 emotion\nstates and 5 problem types (identified from the\noriginal annotations in ESConv) with respect to the\noptimal length of the prompt. In utterance-level\nprompt situations, the instances are 16 two-turn\nshort conversations. We also empirically adjust the\norder of instances given the potential influence it\ncould have on the final results (Lu et al., 2022).\nWe preprocess the conversations in the ESConv\ndataset, in which speakers can make multiple con-\nsecutive utterances, into a turn-based dialogue\nformat by grouping consecutive utterances (if a\nspeaker said, \"Why?\", and then, \"Did anything\nhappen?\", they would be combined into a single\nutterance: \"Why? Did anything happen?\"). The re-\nsulting dataset consists of 9k annotated pairs of\nseeker-supporter utterances, encompassing 1.5k\nconversations. We partition the data using a ra-\ntio of 70%/10%/20% for training, validation, and\ntesting, respectively.\n4 Training Dialogue Models\nTo evaluate the effect of incorporating our knowl-\nedge acquisition procedure into a state-of-the-art\ndialogue model, we train the following:\nVanilla BlenderBot 2.7B (BB) The transformer\nbased baseline BlenderBot model fine-tuned on\nEmpatheticDialogues, ConvAI, WizardofWiki, and\nBlendedSkillTalks in a multi-task style. We choose\n6668\nExpert Model Similarity Scores Entailment scores\nBLEU-4 ROUGE-L BERTScore BARTScore RoBERTa DeBERTa\nGPT1 0.00 0.17 86.37 - 5.27 0.74 0.24\nGPT2 0.06 0.24 88.14 - 4.41 1.23 0.74\nada 0.08 0.29 89.23 - 4.04 2.81 4.06\ndavinci 0.23 0.46 92.03 - 3.06 27.40 24.44\nTable 1: Results of automatic evaluation on the reasoning processes from different PLMs.\nExpert Model Voting rates\nEmotion Prediction Reason Summarization Suggestion Generation Total\nGPT1 32.23 27.69 21.90 27.27\nGPT2 44.63 42.15 36.36 41.05\nada 61.98 57.85 57.85 59.23\ndavinci 93.39 89.26 88.17 90.22\nTable 2: Human evaluation results three sub-tasks for the information in reasoning processes. Values represent the\nvoting rates of the workers for each sub-task. Total represents overall scores.\nthis model as the base model because it shows state-\nof-the-art performance on being empathetic and\nknowledgable (Smith et al., 2020).\nBlenderBot for Mental Health (BBMH) A\nBlenderBot model fine-tuned on the original ES-\nConv dataset, to serve as an in-domain baseline\nmodel. BBMH is fine-tuned in a multi-task style\non both BlendedSkillTalks and ESConv with equal\ntraining weight. This allows BBMH to have a simi-\nlar conversational ability to BB while having access\nto mental health-related conversations.\nBlenderbot for Mental Health with Reasoning\n(BBMHR) This is a model utilizing the Ask an\nExpert architecture as applied to mental health sup-\nport systems, fine-tuned on the reasoning processes\nthat are collected through prompting as described\nin Section 3.1. At training time, seeker utterances\nand associated reasoning processes that we col-\nlected from LLM expert models are concatenated\nas inputs. At inference time, we modify the ParlAI\nframework to allow communications between the\ndialogue model and the LLM experts to get ad-hoc\nreasoning annotations. Like BBMH, BBMHR is\nalso fine-tuned in a multi-task style on both Blend-\nedSkillTalks and ESConv (with reasoning) for the\nsame purpose.\nAll models are fine-tuned with ParlAI frame-\nwork (Miller et al., 2017) using BlenderBot-BST\n2.7B (Roller et al., 2021) as the initial model 3.\nBoth BBMH and BBMHR are trained on 4 Tesla\nv100 GPUs for 96 hours. To be noticed, we train\nmultiple BBMHR models with reasoning processes\nfrom different LLMs. In the following, BBMHR\n+ LLMs denote the dialogue model with reasoning\nprocesses from the specific LLM (e.g. BBMHR +\nGPT1 denotes the BBMHR model with reasoning\nprocesses from GPT1).\n5 Evaluation & Results\n5.1 Assessing the Expert Advice\nThe first question we aim to answer is: how good\nis the mental health support advice provided by the\nLLM experts? We perform both automatic evalu-\nation and human evaluation to assess the quality\nof reasoning processes. We randomly select 50\nconversations and manually label the conversations\n(via Mechanical Turk) with reasoning processes.\nAutomatic Evaluation We calculate the similar-\nity and entailment scores between generated reason-\ning processes and human labels. For similarity, we\ncalculate ROUGE (Lin, 2004), BLEU (Papineni\net al., 2002), BERTScore (Zhang et al., 2019a)\nand BARTScore (Yuan et al., 2021a). Entail-\nment scores are calculated using inferences models,\nRoBERTa (Zhuang et al., 2021) and DeBERTa (He\n3The code and data for this work are available at:\nhttps://github.com/QZx7/BBMHReasoning/tree/main\n6669\nModel Model Winning Percentages Against Human\nEngagingness Humanness Empathy Specificity Helpfulness Experience Total\nin-context davinci - 35.87 - 28.89 - 24.29 - 14.33 - 29.65 - 24.29 -47.30\nBB - 36.78 - 22.92 - 15.67 - 28.91 - 30.15 - 17.64 - 42.68\nBBMH - 26.07 - 21.60 - 11.95 - 10.53 - 22.90 - 12.47 - 30.19\nBBMHR:\nGPT1 - 23.17 - 9.89 - 12.51 -18.48 - 20.07 - 10.43 - 26.20\nGPT2 - 24.82 - 8.15 - 3.64 - 14.02 - 19.65 - 9.21 - 22.33\nada - 24.02 - 7.04 - 7.16 - 11.52 - 15.59 - 2.48 - 19.41\ndavinci - 12.10 - 1.96 + 1.26 - 8.60 - 7.09 + 0.91 - 10.93\nTable 3: Human evaluation results of the winning percentages of different trained dialogue models against human\nconversations in ESConv. Positive numbers show that the model wins human and negative numbers show that the\nmodel loses to human in the comparison.\net al., 2020) to score the possibilities of the entail-\nment relationship between generated and manual\nlabels by treating it as a textual inference task.\nTable 1 shows the results of automatic evaluation\non reasoning processes. We can observe clear im-\nprovement in both similarity and entailment scores\nfrom GPT1 to davinci, where the gap between\ndavinci and other models is especially large.\nHuman Evaluation We perform human evalua-\ntion to assess the LLMs’ ability to generate each\npiece of information generated in the reasoning pro-\ncesses generation task. More specifically, we mea-\nsure the quality of reasoning processes with three\nsub-tasks: emotional prediction, reason summariza-\ntion and suggestions generation. Each sub-task is\nused to assess one piece of information in the rea-\nsoning processes. Crowdsourcing workers are then\nasked to vote for each sub-task by answering ques-\ntions such as “Does the annotation contain correct\nemotion description of the seeker?” We report the\nvoting rates on each sub-task for each expert model\nused in the prompting phase. A complete list of the\nquestions can be found in Appendix C.\nTable 2 shows the results of human evaluation\nwith an average inter-rater agreement of 83.7%,\nand we are able to observe similar results as in\nautomatic evaluation. Davinci outperforms other\nmodels on all three sub-tasks, which shows that\ndavinci may have more knowledge of the reason-\ning processes. Such results hint that the reasoning\nknowledge by consulting LLMs can provide valid\nreasoning information to be used for dialogue mod-\nels, especially those generated by expert models\nwith a larger size.\n5.2 Evaluation on Dialogue Models\nWe perform the human evaluation on the mod-\nels following the ACUTE-Eval (Li et al., 2019)\nmethod, in which conversations generated by two\ndifferent models are collected, and annotators are\nasked to make binary judgments between two mod-\nels. We set up experiments and compare conversa-\ntions between humans in ESConv to conversations\ngenerated by different models. The compared mod-\nels are divided into three groups: human vs. BB,\nhuman vs. BBMH, and human vs. BBMHR. For\neach group, we perform ACUTE-Eval and calculate\nthe win percentages of the models, where positive\nnumbers represent that models win and negative\nnumbers represent that human wins. As compari-\nson, we also follow the methods in (Zheng et al.,\n2022) and prompt in-context davinci with the same\nprompts to generate conversations in the domain of\nemotional support.\nSelf-Chats We perform self-chats (Jaques et al.,\n2020; Bao et al., 2019) to collect conversations\nfrom models following the experiments in ACUTE-\nEval (Li et al., 2019). Self-chats could reduce the\nefforts of collecting objective conversations and\nshow high agreements with human-model evalua-\ntions (Li et al., 2019). For each model, we collect\n100 conversations across 5 known topics in ES-\nConv, 20 for each topic. Initial utterances of the\nconversations are pre-defined to generate diverse\ndialogue content for each topic (Bao et al., 2021).\nThe generated conversations are compared against\nhuman-human conversations with the same topic\n6670\nSuggestive responses by BBMHR  + davinci \nYeah. I just feel so difficult to face\neverything in my life now. It seems that\nI'm not needed by anyone in the world.\nI know that this might sound personal,\nbut it's completely normal to have\nthese feelings after a breakup. Have\nyou considered talking to your friends\nand family about these feelings?\nWell, I thought about that. But I think\nthat will only make them worry about\nme. You know, because of the\npandemic, I'm at home by myself.\nI understand. However, I think that they\nwould be a great support system after\nyou've processed your feelings. After\nmy breakup, I found that my friends\nwere very helpful in helping me get\nthrough my feelings.\nYeah, I understand that. You're right, I\nthink I do need to talk to someone\nbefore I really stress out.\nI think that's a great idea. I also found\nthat writing out my feelings in a journal\nhelped me a lot..\nBBMH  (baseline)\nYeah. I just feel so difficult to face\neverything in my life now. It seems that\nI'm not needed by anyone in the world.\nI think the best thing you can do is to\nhang out with your friends and talk to\nthem about it.  Get your mind off of it.\nYeah, that could be a good option if\nthere's no pandemic. I don't want to put\nmy friends in danger because of me.\nI think you will be okay.  You seem like\na very strong person.  You have taken\nthe right steps to get help.  And I know\nthat you will make the right choices.\nNon-suggestive responses by BB \nYeah. I just feel so difficult to face\neverything in my life now. It seems that\nI'm not needed by anyone in the world.\nI know what you mean. It will take\nsome time to get over it but you will get\nthere.\nI think there might be some methods to\nhelp me quickly recover from it. But I\ndon't know how.\nI'm sure you will find something that\nworks for you. Just take it one day at a\ntime.\nThat sounds great! What do you write?\nI wrote how I felt about the relationship\nand how it affected my life. \nFigure 3: Samples of conversation pieces generated by different models handling the counseling process during the\nmental support session.\nin ESConv for evaluation.\nQuestionnaire Annotators are asked to answer\n17 questions across 6 dimensions: engagingness,\nhumanness, empathy, specificity, helpfulness, and\nexperience. Engagingness and humanness are used\nto evaluate the ability to have general and long\nconversations. Questions for these two dimensions\nare same as the questions used in (Li et al., 2019).\nEmpathy represents the model’s ability to catch the\nemotional status and feelings of the seekers. Speci-\nficity reflects the ability to produce task-specific\nresponses. Helpfulness indicates the feasibility of\nsuggestions given by the models. Experience is\nused to measure the ability to share relevant and\nsimilar experiences based on the seeker’s problems.\nWe adapted the evaluation method in O’Leary et al.\n(2018) and crafted questions for the newly added\nfour dimensions based on the components of the\n“guided chat tool”, which proved to be more effec-\ntive in terms of problem-solving. A complete list\nof questions can be found in Appendix D.\nResults Table 3 shows the results of human eval-\nuation, with an average inter-rater agreement of\n80.4%. Both BBMH and BBMHR outperform\nvanilla BB in terms of all 6 dimensions, owing\nto the use of additional in-domain data. When\nassessing the effect of the knowledge acquisition\nprocedure, BBMHR outperforms BBMH in most\naspects, especially humanness, helpfulness, and ex-\nperience, which are the primary criteria that we aim\nto improve as being especially useful to the goal-\noriented aspects of the dialogue model as a mental\nhealth support system. Additionally, we find a\nstrong correlation with the degree of improvement\non these metrics and the size of the model. Other\nattributes , such as specificity, do not appear to ben-\nefit strongly from additional reasoning information.\nAmong all BBMHR models, BBMHR + davinci\n6671\nEmpathetic responses by BBMHR  + davinci \nHi there.\nHow are you friend?\nI'm feeling emotionally sad right now.\nI'm so sorry to hear that. Why are you\nfeeling sad?\nBBMH  (baseline)\nHi there.\nHello, how are you today.\nI'm feeling emotionally sad right now.\nCan you tell me a little about what is\nmaking you feel emotionally sad?\nNon-engaging responses by BB \nHi there.\nHello, how are you doing today? I am\ndoing well, thank you for asking. How\nare you?\nThanks for asking. Actually, I feel\nemotionally sad recently.\nI am sorry to hear that. I hope you feel\nbetter soon. Do you have any\nhobbies?\nFigure 4: Samples of conversation pieces by different\nmodels to initialize the conversation of a mental support\nsession.\nachieves the best performance in almost all aspects\nwhich also shows that consulting better reasoning\nmodels contributes to better responses.\n5.3 Crowdsourcing & Filtering Details\nThe workers are required to be fluent in English\nin both evaluation tasks of the reasoning processes\nand dialogue models. For reasoning process evalu-\nation, the workers are asked to answer some ques-\ntions about the content of the conversation to ensure\nthat they clearly understand the context. For each\nquestion, they also need to provide justifications\nfor their answer to be valid. For dialogue model\nevaluation, while answering the binary selective\nquestions, the workers are asked to write down\nbrief justifications from time to time (Q2, Q5, Q8,\nQ12, Q14, and Q17) to ensure that they are en-\ngaging. We perform filtering on the annotations to\nremove the annotations that are completed in an\nextremely short time (less than 300 seconds) and\nwith invalid justifications (samples of invalid justifi-\nFailure case by BBMHR  + davinci \nHello . How are you doing?\nGood. You?\nWhy?\nHi . How are you doing?\nI'm doing well. How are you doing?\nI'm feeling emotionally sad right now.\nI'm sorry to hear that. Would you mind\ntelling me more details about why\nyou're feeling bad?\nFailure fixed\nI'm feeling emotionally sad right now.\nFigure 5: Failure cases by BBMHR + davinci, where\nthe responses of the model are short and non-empathetic.\nIt can be fixed when the opening of the conversation is\nchanged.\ncations can be found in Appendix E). The workers\nare paid an average of 10$ per hour in line with\nregional guidelines on ethical compensation.\n6 Sample Conversations & Failure Cases\nSample Conversations Figure 3 shows the con-\nversational strategies used by different models\nwhen the seeker looks for mental support because\nof a breakup. BBMHR is able to provide suggestive\nresponses based on strategies provided in the rea-\nsoning process. We also find that BBMHR provides\nmore empathetic and engaging responses when ini-\ntializing the conversation (In Figure 4, BB tends\nto ask non-engaging questions such as “Do you\nhave any hobbies?”). More samples can be found\nin Appendix G.\nFailure Cases Figure 5 shows a failure case\nwhere the responses can occasionally be short and\nnot empathetic. All models have a tendency to de-\nfault to such cases at the opening of conversations,\nwhen the conversation history is limited and the ex-\npert would have difficulty inferring any additional\nuseful details (similar errors are observed in Ung\net al. (2022); Tyen et al. (2022)). Moreover, we\nobserve that the frequency of such failure cases\ndecreases as size of LLM increases, and implies\nthat some of these mistakes may be resolved with\nbetter experts. For instance, an expert practitioner\nin this case may be more pro-active in gathering\nthe necessary details to form an analysis. By inter-\n6672\nfacing with the expert purely by text prompts, and\ncollecting the expert advice as text (and inserting it\ninto the dialogue model context window), we allow\nfor the opportunity for the expert model to also\nhelp the dialogue model take a more active role in\nprogressing the conversation toward the goal when\nnecessary.\n7 Discussion\nWhat are the advantages of utilizing LLMs for\nstrategic reasoning? Goal-oriented dialogue sys-\ntems not based upon LLMs often rely on inferring\ndialogue states to carry out only meaningful conver-\nsations, and thus significantly rely on the definition\nof the task and an ontology of possible dialogue tra-\njectories (Xie et al., 2022). This makes the systems\nbrittle and open to catastrophic errors when the\ndialogue breaks significantly from the categories\nof the ontology. LLMs show similar ontological\nknowledge and planning ability in many domains,\nbut are more flexible. As language models, in-\nterfacing with LLM experts is as straightforward\nas establishing a short goal-oriented conversation,\nand incorporating their responses into the dialogue\nmodel via the model’s context is similarly easy.\nIn that sense, utilizing LLMs greatly reduces the\nefforts defining a complicated ontology and dia-\nlogue state tracking module by providing necessary\nreasoning power and knowledge.\nWhy not use GPT-3 directly for dialogue gen-\neration? Is the dialogue model still necessary\nwhen there is an expert model? Our results (Ta-\nble 3) show that utilizing LLMs as dialogue models\ndirectly can lead to worse performance than even\nbaseline dialogue models such as Blenderbot. We\nfind that in-context davinci performs worse than\nBB both in terms of generating human-like and\nempathetic dialogues. One alternative is to fine-\ntune LLMs specifically for dialogue generation,\nbut this process often requires expensive hardware,\ntime, and training data (Shuster et al., 2022). It\nis unclear whether fine-tuning even larger models\nwould uncover the heuristic strategies inherent in\ngoal-oriented conversations, which can be easily\nspecified via prompts using an “Ask an Expert” ar-\nchitecture.\nDeploying Ask an Expert? A natural restriction\nin the Ask an Expert is that it requires the expert to\nbe present at inference time and during deployment.\nIf a motivation of Ask an Expert is to allow dia-\nlogue models to be deployed on simpler hardware,\nhaving a large expert model limits its usefulness\nin such situations. However, recent advancements\nin technology, such as ChatGPT and Bard, offer\nAPI services that facilitate convenient access to ex-\npert knowledge. Furthermore, software tools like\nLangChain efficiently manage prompts, computa-\ntions, and knowledge, presenting an alternative to\nlocal deployment of extensive expert models.\nAnother scenario that imposes limitations on\nthe adoption of Ask an Expert pertains to certain\ndomains where the system must be deployed lo-\ncally to uphold privacy concerns, such as mental\nhealth systems aiming to safeguard patient data. In\nsuch instances, relying on external API services\nbecomes less feasible. However, it is not always\nnecessary to utilize all the knowledge of large ex-\npert models. And for specific domain use cases,\nsuch as mental health, it is unlikely that the full size\nof the model is indispensable. Given the effective-\nness of our approach, in future work we would like\nto explore the extent to which the expert model can\nbe distilled (Sanh et al., 2019; Schick and Schütze,\n2021c) into models which are able to run locally\non consumer-grade hardware.\n8 Conclusion\nIn this work we propose the “Ask an Expert” frame-\nwork for building more robust dialogue systems us-\ning external knowledge obtained via prompt-based\nconversations with LLM “experts”. The prompts\nare designed to elicit a step-by-step expert analysis\nof the current discourse context, intended to mimic\nthe inner monologue of a human professional coun-\nselor, and provide it at each turn to the dialogue\nmodel. As the expert consultation process occurs\nboth during training and inference time, the dia-\nlogue model itself can learn useful strategies for\nflexibly incorporating the advice of the expert. We\nhave shown in both human and automatic evalua-\ntions that the addition of such reasoning knowledge\nresults in models which are more suggestive, help-\nful, and engaging than comparable baseline models\nwhich do not consult the expert. Our result sup-\nports the hypothesis that current dialogue models\noften fail to implicitly learn effective goal-oriented\nstrategies from dialogue data alone, and provides\nevidence that combination with other models may\nhelp alleviate current shortcomings.\n6673\n9 Limitations and Ethical Considerations\nLimitations Our proposed approach relies heav-\nily on LLMs and is subject to the same limitations,\nnamely, known biases in the training data and the\nability to hallucinate incorrect information. Addi-\ntionally, we perform the research in English only.\nIt is known that for different cultures, the strategies\nof showing empathy can be very diverse which\nrequires cultural background knowledge and rea-\nsoning processes (Atkins et al., 2016).\nPertinent to our intended use-case where models\nwould be deployed locally, LLMs remain compu-\ntationally intensive even during inference. Despite\ndemonstrating that even smaller models (such as\nGPT1 and GPT2) do yield performance enhance-\nments for BBMHR, their performance scales with\ntheir parameter size and even small-scale models\ncan require expensive hardware for deployment.\nConsequently, it becomes imperative to explore\nalternative approaches, such as domain-specific\nlightweight reasoning models, or distilled or low-\nprecision inference models, as viable alternatives\nto resource-intensive LLMs.\nEthical Considerations Working within the field\nof mental health support demands additional con-\nsiderations. In terms of safety, we acknowledge the\nlimitations of the proposed models and the poten-\ntial risks associated with directly deploying them to\nemotionally vulnerable individuals. We do not rec-\nommend the deployment of the models presented\nin this work. Consequently, we emphasize that\nthe models presented in this study are intended to\n(at most) function in a human-in-the-loop capac-\nity, serving as an assistant to trained mental health\npractitioners.\nFurthermore, we take into account the possibility\nof negative impacts that the present research could\nhave on the community. Despite our intention to\ndevelop models for social good, it is important to\nacknowledge that the dataset contains content that\ncould be problematic (inputs from seekers, and rea-\nsoning processes that could potentially be exploited\nto generate negative or offensive content). We re-\nlease all data collected for this work to help support\nfuture work towards improving MHS systems.\nAcknowledgements\nWe thank the anonymous reviewers for their help-\nful suggestions and feedback. This work was\nsupported by JSPS KAKENHI Grant Number\nJP19H05692.\nReferences\nDaniel Adiwardana, Minh-Thang Luong, David R So,\nJamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang,\nApoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu,\net al. 2020. Towards a human-like open-domain chat-\nbot. arXiv preprint arXiv:2001.09977.\nDavid Atkins, Ayse K Uskul, and Nicholas R Cooper.\n2016. Culture shapes empathic responses to physical\nand social pain. Emotion, 16(5):587.\nSiqi Bao, Huang He, Fan Wang, Rongzhong Lian, and\nHua Wu. 2019. Know more about each other: Evolv-\ning dialogue strategy via compound assessment. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 5382–\n5391, Florence, Italy. Association for Computational\nLinguistics.\nSiqi Bao, Huang He, Fan Wang, Hua Wu, Haifeng Wang,\nWenquan Wu, Zhen Guo, Zhibin Liu, and Xinchao\nXu. 2021. PLATO-2: Towards building an open-\ndomain chatbot via curriculum learning. In Find-\nings of the Association for Computational Linguis-\ntics: ACL-IJCNLP 2021, pages 2513–2525, Online.\nAssociation for Computational Linguistics.\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi,\net al. 2020. Piqa: Reasoning about physical com-\nmonsense in natural language. In Proceedings of the\nAAAI conference on artificial intelligence, volume 34,\npages 7432–7439.\nSom S Biswas. 2023. Role of chat gpt in public health.\nAnnals of Biomedical Engineering, pages 1–2.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nHarrison Chase. 2022. Langchain.\nPaul F Christiano, Jan Leike, Tom Brown, Miljan Mar-\ntic, Shane Legg, and Dario Amodei. 2017. Deep\nreinforcement learning from human preferences. In\nAdvances in Neural Information Processing Systems,\nvolume 30. Curran Associates, Inc.\nDavid DeVault, Kallirroi Georgila, Ron Artstein, Fab-\nrizio Morbini, David Traum, Stefan Scherer, Al-\nbert Skip Rizzo, and Louis-Philippe Morency. 2013.\nVerbal indicators of psychological distress in interac-\ntive dialogue with a virtual human. In Proceedings of\nthe SIGDIAL 2013 Conference, pages 193–202, Metz,\nFrance. Association for Computational Linguistics.\nMihail Eric, Lakshmi Krishnan, Francois Charette, and\nChristopher D. Manning. 2017. Key-value retrieval\nnetworks for task-oriented dialogue. In Proceedings\n6674\nof the 18th Annual SIGdial Meeting on Discourse\nand Dialogue, pages 37–49, Saarbrücken, Germany.\nAssociation for Computational Linguistics.\nKathleen Kara Fitzpatrick, Alison Darcy, and Molly\nVierhile. 2017. Delivering cognitive behavior ther-\napy to young adults with symptoms of depression\nand anxiety using a fully automated conversational\nagent (woebot): a randomized controlled trial. JMIR\nmental health, 4(2):e7785.\nRussell Fulmer, Angela Joerin, Breanna Gentile,\nLysanne Lakerink, Michiel Rauws, et al. 2018. Using\npsychological artificial intelligence (tess) to relieve\nsymptoms of depression and anxiety: randomized\ncontrolled trial. JMIR mental health, 5(4):e9782.\nDonghoon Ham, Jeong-Gwan Lee, Youngsoo Jang, and\nKee-Eung Kim. 2020. End-to-end neural pipeline\nfor goal-oriented dialogue systems using GPT-2. In\nProceedings of the 58th Annual Meeting of the Associ-\nation for Computational Linguistics, pages 583–592,\nOnline. Association for Computational Linguistics.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2020. Deberta: Decoding-enhanced\nbert with disentangled attention. arXiv preprint\narXiv:2006.03654.\nNatasha Jaques, Judy Hanwen Shen, Asma Ghandehari-\noun, Craig Ferguson, Agata Lapedriza, Noah Jones,\nShixiang Gu, and Rosalind Picard. 2020. Human-\ncentric dialog training via offline reinforcement learn-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing\n(EMNLP), pages 3985–4003, Online. Association\nfor Computational Linguistics.\nMojtaba Komeili, Kurt Shuster, and Jason Weston. 2022.\nInternet-augmented dialogue generation. In Proceed-\nings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 8460–8478, Dublin, Ireland. Association\nfor Computational Linguistics.\nMinha Lee, Sander Ackermans, Nena Van As, Hanwen\nChang, Enzo Lucas, and Wijnand IJsselsteijn. 2019.\nCaring for vincent: a chatbot for self-compassion. In\nProceedings of the 2019 CHI Conference on Human\nFactors in Computing Systems, pages 1–13.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\nand Bill Dolan. 2016. A diversity-promoting ob-\njective function for neural conversation models. In\nProceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 110–119, San Diego, California. Association\nfor Computational Linguistics.\nMargaret Li, Jason Weston, and Stephen Roller. 2019.\nAcute-eval: Improved dialogue evaluation with opti-\nmized questions and multi-turn comparisons. arXiv\npreprint arXiv:1909.03087.\nYanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang\nCao, and Shuzi Niu. 2017. DailyDialog: A manually\nlabelled multi-turn dialogue dataset. In Proceedings\nof the Eighth International Joint Conference on Nat-\nural Language Processing (Volume 1: Long Papers),\npages 986–995, Taipei, Taiwan. Asian Federation of\nNatural Language Processing.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nSiyang Liu, Chujie Zheng, Orianna Demasi, Sahand\nSabour, Yu Li, Zhou Yu, Yong Jiang, and Minlie\nHuang. 2021. Towards emotional support dialog\nsystems. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 3469–3483, Online. Association for Computa-\ntional Linguistics.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,\nand Pontus Stenetorp. 2022. Fantastically ordered\nprompts and where to find them: Overcoming few-\nshot prompt order sensitivity. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n8086–8098, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nAndrea Madotto, Chien-Sheng Wu, and Pascale Fung.\n2018. Mem2Seq: Effectively incorporating knowl-\nedge bases into end-to-end task-oriented dialog sys-\ntems. In Proceedings of the 56th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 1468–1478, Melbourne,\nAustralia. Association for Computational Linguistics.\nAlexander Miller, Will Feng, Dhruv Batra, Antoine Bor-\ndes, Adam Fisch, Jiasen Lu, Devi Parikh, and Jason\nWeston. 2017. ParlAI: A dialog research software\nplatform. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 79–84, Copenhagen,\nDenmark. Association for Computational Linguis-\ntics.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin\nChoi, and Hannaneh Hajishirzi. 2022a. Reframing\ninstructional prompts to GPTk’s language. In Find-\nings of the Association for Computational Linguistics:\nACL 2022, pages 589–612, Dublin, Ireland. Associa-\ntion for Computational Linguistics.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and\nHannaneh Hajishirzi. 2022b. Cross-task generaliza-\ntion via natural language crowdsourcing instructions.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 3470–3487, Dublin, Ireland.\nAssociation for Computational Linguistics.\nKathleen O’Leary, Stephen M. Schueller, Jacob O. Wob-\nbrock, and Wanda Pratt. 2018. “suddenly, we got to\n6675\nbecome therapists for each other”: Designing peer\nsupport chats for mental health. In Proceedings of\nthe 2018 CHI Conference on Human Factors in Com-\nputing Systems, CHI ’18, page 1–14, New York, NY ,\nUSA. Association for Computing Machinery.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul F Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. In Advances in Neural Information\nProcessing Systems, volume 35, pages 27730–27744.\nCurran Associates, Inc.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 2463–2473, Hong Kong, China. Association\nfor Computational Linguistics.\nChristopher Pudlinski. 2005. Doing empathy and sym-\npathy: Caring responses to troubles tellings on a peer\nsupport line. Discourse studies, 7(3):267–288.\nYujia Qin, Yankai Lin, Jing Yi, Jiajie Zhang, Xu Han,\nZhengyan Zhang, Yusheng Su, Zhiyuan Liu, Peng\nLi, Maosong Sun, and Jie Zhou. 2022. Knowledge\ninheritance for pre-trained language models. In Pro-\nceedings of the 2022 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n3921–3937, Seattle, United States. Association for\nComputational Linguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nHannah Rashkin, Eric Michael Smith, Margaret Li, and\nY-Lan Boureau. 2019. Towards empathetic open-\ndomain conversation models: A new benchmark and\ndataset. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 5370–5381, Florence, Italy. Association for\nComputational Linguistics.\nStephen Roller, Emily Dinan, Naman Goyal, Da Ju,\nMary Williamson, Yinhan Liu, Jing Xu, Myle Ott,\nEric Michael Smith, Y-Lan Boureau, and Jason We-\nston. 2021. Recipes for building an open-domain\nchatbot. In Proceedings of the 16th Conference of\nthe European Chapter of the Association for Compu-\ntational Linguistics: Main Volume, pages 300–325,\nOnline. Association for Computational Linguistics.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nMaarten Sap, Vered Shwartz, Antoine Bosselut, Yejin\nChoi, and Dan Roth. 2020. Commonsense reason-\ning for natural language processing. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics: Tutorial Abstracts, pages\n27–33, Online. Association for Computational Lin-\nguistics.\nTimo Schick and Hinrich Schütze. 2021a. Exploiting\ncloze-questions for few-shot text classification and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics: Main Volume,\npages 255–269, Online. Association for Computa-\ntional Linguistics.\nTimo Schick and Hinrich Schütze. 2021b. Generating\ndatasets with pretrained language models. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing , pages 6943–\n6951, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nTimo Schick and Hinrich Schütze. 2021c. It’s not just\nsize that matters: Small language models are also few-\nshot learners. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 2339–2352, Online. Association\nfor Computational Linguistics.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,\nWeiming Lu, and Yueting Zhuang. 2023. Hugging-\ngpt: Solving ai tasks with chatgpt and its friends in\nhuggingface. arXiv preprint arXiv:2303.17580.\nKurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju,\nEric Michael Smith, Stephen Roller, Megan Ung,\nMoya Chen, Kushal Arora, Joshua Lane, et al. 2022.\nBlenderbot 3: a deployed conversational agent that\ncontinually learns to responsibly engage. arXiv\npreprint arXiv:2208.03188.\nEric Michael Smith, Mary Williamson, Kurt Shuster,\nJason Weston, and Y-Lan Boureau. 2020. Can you\nput it all together: Evaluating conversational agents’\nability to blend skills. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 2021–2030, Online. Association\nfor Computational Linguistics.\n6676\nCaroline K Tietbohl. 2022. Empathic validation in\nphysician–patient communication: An approach to\nconveying empathy for problems with uncertain solu-\ntions. Qualitative Health Research, 32(3):413–425.\nGladys Tyen, Mark Brenchley, Andrew Caines, and\nPaula Buttery. 2022. Towards an open-domain chat-\nbot for language practice. In Proceedings of the 17th\nWorkshop on Innovative Use of NLP for Building Ed-\nucational Applications (BEA 2022), pages 234–249,\nSeattle, Washington. Association for Computational\nLinguistics.\nMegan Ung, Jing Xu, and Y-Lan Boureau. 2022. SaFeR-\nDialogues: Taking feedback gracefully after conver-\nsational safety failures. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 6462–\n6481, Dublin, Ireland. Association for Computational\nLinguistics.\nZirui Wang, Adams Wei Yu, Orhan Firat, and Yuan Cao.\n2021. Towards zero-label language learning. arXiv\npreprint arXiv:2109.09193.\nAnuradha Welivita, Yubo Xie, and Pearl Pu. 2021. A\nlarge-scale dataset for empathetic response gener-\nation. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1251–1264, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nPeter West, Chandra Bhagavatula, Jack Hessel, Jena\nHwang, Liwei Jiang, Ronan Le Bras, Ximing Lu,\nSean Welleck, and Yejin Choi. 2022. Symbolic\nknowledge distillation: from general language mod-\nels to commonsense models. In Proceedings of the\n2022 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 4602–4625, Seat-\ntle, United States. Association for Computational\nLinguistics.\nSixing Wu, Ying Li, Dawei Zhang, and Zhonghai\nWu. 2020. Improving knowledge-aware dialogue re-\nsponse generation by using human-written prototype\ndialogues. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2020 , pages 1402–\n1411, Online. Association for Computational Linguis-\ntics.\nTian Xie, Xinyi Yang, Angela S Lin, Feihong Wu,\nKazuma Hashimoto, Jin Qu, Young Mo Kang, Wen-\npeng Yin, Huan Wang, Semih Yavuz, et al. 2022.\nConverse–a tree-based modular task-oriented dia-\nlogue system. arXiv preprint arXiv:2203.12187.\nJing Xu, Arthur Szlam, and Jason Weston. 2022. Be-\nyond goldfish memory: Long-term open-domain con-\nversation. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 5180–5197, Dublin,\nIreland. Association for Computational Linguistics.\nTom Young, Erik Cambria, Iti Chaturvedi, Hao Zhou,\nSubham Biswas, and Minlie Huang. 2018. Augment-\ning end-to-end dialogue systems with commonsense\nknowledge. In Proceedings of the AAAI conference\non artificial intelligence, volume 32.\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021a.\nBartscore: Evaluating generated text as text gener-\nation. Advances in Neural Information Processing\nSystems, 34:27263–27277.\nZheng Yuan, Yijia Liu, Chuanqi Tan, Songfang Huang,\nand Fei Huang. 2021b. Improving biomedical pre-\ntrained language models with knowledge. In Pro-\nceedings of the 20th Workshop on Biomedical Lan-\nguage Processing, pages 180–190, Online. Associa-\ntion for Computational Linguistics.\nAdam Zaremba and Ender Demir. 2023. Chatgpt: Un-\nlocking the future of nlp in finance. Available at\nSSRN 4323643.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Wein-\nberger, and Yoav Artzi. 2019a. Bertscore: Eval-\nuating text generation with bert. arXiv preprint\narXiv:1904.09675.\nYizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen,\nChris Brockett, Xiang Gao, Jianfeng Gao, Jingjing\nLiu, and Bill Dolan. 2019b. Dialogpt: Large-scale\ngenerative pre-training for conversational response\ngeneration. arXiv preprint arXiv:1911.00536.\nChujie Zheng, Sahand Sabour, Jiaxin Wen, and\nMinlie Huang. 2022. Augesc: Large-scale data\naugmentation for emotional support conversation\nwith pre-trained language models. arXiv preprint\narXiv:2202.13047.\nLiu Zhuang, Lin Wayne, Shi Ya, and Zhao Jun. 2021. A\nrobustly optimized BERT pre-training approach with\npost-training. In Proceedings of the 20th Chinese\nNational Conference on Computational Linguistics,\npages 1218–1227, Huhhot, China. Chinese Informa-\ntion Processing Society of China.\n6677\nA Different Prompts\nTable 4 shows the results by different styles of\nprompt. We attempted two types of prompt, ques-\ntions answering (QA) and text generation (TG). In\nthe QA style, we design a series questions asking\nthe information needed for reasoning processes.\nAnd for TG style, we prompt PLMs to generate full\nreasoning processes as a paragraph of natural text.\nAs shown in the table (text in red color represents\nthe errors), answers by QA style are less accurate\nand less suggestive than TG style.\nB Expert Advice samples\nTable 5 shows the examples of reasoning processes\ngenerated by different PLMs used in the experi-\nments.\nC Questions for Human Evaluation of\nExpert Advice\nTable 6 shows the questions we asked the crowd-\nsourcing workers in the human evaluation of rea-\nsoning processes. Each question is targeting at each\npiece of information.\nD Questions for Dialogue Evaluation\nTable 7 shows the questions we used in the ACUTE-\nEval of the dialogue models. For each dimension,\nwe design 2-3 questions and we calculate the inter-\nrater agreement for each question to be valid. Bold\nfont indicates that the inter-rater agreements are\nhigher than 85% and thus are selected for the results\ncalculation.\n6678\nStyle Sample\nContext:\nseeker: I was recently let go from my job due to the covid pandemic, and am now\njobless before the holidays.\nsupporter: I’m sorry, that’s tough anytime but that’s gotta be brutal because\nof the year and upcoming holidays.. what kind of work were you doing?\nseeker: It really is, I was in a family owned cafe as a waitress. Due to the rise\nof the virus they ordered everyone to shut back down.\nQA Q1: How did the seeker feel?\nA1: The seeker feels tough because of the holiday season.\nQ2: Why did the seeker feel that way?\nA2: The seeker was recently let go from her job and is now jobless.\nQ3: What could the supporter do?\nA3: The supporter could tell the seeker that she will help her to find a job.\nTG In this conversation, the seeker feels down because of being jobless. The supporter\ncould look for some job openings or tell the seeker to start a small business.\nContext:\nseeker: I’m very upset.\nsupporter: I’m sorry. Would you like to tell me about it?\nseeker: Yes. I invited my friend over to watch my new puppy while I painted my\nroom because my dog has separation anxiety and can’t be alone. then she showed\nup with her new boyfriend.\nsupporter: What bothered you about that?\nseeker: Well I don’t know him and my stuff was all out in the living room where\nhe was alone for most of the day. I actually didn’t suspect anything would\nhappen at first.\nQA Q1: how did the seeker feel?\nA1: The seeker feels upset.\nQ2: why did the seeker feel that way?\nA2: The seeker was surprised that her friend brought her new boyfriend over without warning.\nQ3: what could the supporter do?\nA3: The supporter should ask the seeker how she would like to handle the situation.\nTG In this conversation, the seeker feels upset because her friend brought the new boyfriend\nover to the seeker’s house without notifying. The supporter could tell the seeker that it is\nbetter to talk to the friend about the issue.\nTable 4: Some sample reasoning processes by different styles of the prompt.\n6679\nModel Sample\nContext:\nseeker: I am ok, I’m having a hard time dealing with the pandemic though.\n...\nseeker: What do you like to do for yourself when you feel bad?\nsupporter: It happens anyway, but not all the time. when did it start and what do you\nthink that can cause that.\nseeker: It started with the pandemic. I think it is because we have to stay home to be\nsafe but other people don’t listen and more people are dying because of it.\nGPT-1 the seeker tried to keep her words from being too painful but it was hard and the\nsource of the pain was more difficult, because her voice was so low. the seeker was\nafraid to talk about this because it was too painful to admit that she had to be truthful.\nGPT-2 feels scared and depressed about losing his job. The supporter could share some\nexperience and suggest the seeker to stay positive.\nAda feels bad because of the pandemic. The supporter could offer a suggestion.\nDavinci feels bad because of the pandemic. The supporter could tell the seeker to focus on the\nthings that the seeker can control and try to make a plan for each day. The supporter\ncould also suggest the seeker to join an online community to share experiences.\nContext:\nseeker: Ok I guess. I do not know how to tell my husband that I am lonely and\nI want out of the marriage.\n...\nseeker: He would go to sleep, and then he thinks I am crazy he says we are no\nkids anymore that need to go on a date we are married.\nsupporter: Do you think you both might be open to talking to a marriage counselor?\nseeker: I want that but I doubt he will talk he tells me he has no problems I have\nthem, and all it does is I doubt myself. I turned this situation in every\ndirection and I just am not able to find a solution.\nGPT-1 the seeker felt depressed. she thought about what she had to do now. she did not\nknow how to talk to her boyfriend. she could not get him to talk to her but she could\nnot stop him from talking to her. she tried to calm down. i ’ll talk to him. she thought.\nGPT-2 is worried about losing the job and getting laid off. The supporter could share some\nrelated knowledge and advice.\nAda is worried about being alone and not having a husband. The supporter should tell the\nseeker that marriage counseling is available.\nDavinci feels lonely and frustrated. The supporter could suggest the seeker to talk to a marriage\ncounselor.\nTable 5: Samples of reasoning processes generated by different models.\nQuestion\nDoes the reasoning describe correct emotion status of the seeker?\nDoes the reasoning summarize the seeker’s problem correctly?\nDoes the conversational advice for the supporter make sense?\nTable 6: Questions for human evaluations of the reasoning results.\n6680\nQuestion Choice 1\nEngagingness\nWhich supporter is more engaging to talk to? Supporter 1 is more engaging\nWho would you prefer to talk to for a I would prefer to talk to Supporter 1\nlong conversation?\nWhich supporter do you think is more captivating? Supporter 1 is more captivating\nthan Supporter 2\nHumanness\nWhich supporter sounds more human? Supporter 1 sounds more human\nIf you had to guess that one supporter is human Supporter 1 sounds human\nand one is a bot, which do you think is human?\nWhich supporter sounds more like a real person? Supporter 1 sounds more like a real person\nEmpathy\nWhich supporter understands the feelings Supporter 1 understands the feeling better\nof the seeker better?\nIf you had to say one of these supporters Supporter 1 understands emotion better\nunderstands human emotion better, who would\nyou say is better?\nWhich supporter shows more empathy on the seeker? Supporter 1 shows more empathy\nSpecificity\nWhich supporter responds more specifically Supporter 1 talks more relatively\nThe responses of which supporter are less Supporter 1’s responses are less\nout-of-context? out-of-context\nWhich supporter do you think care more about the Supporter 1 cares more about the.\nseeker’s problem? seeker’s problem\nHelpfulness\nWhich supporter gets a stronger urge to help? Supporter 1 gets a stronger urge to help\nWhich supporter would you prefer to get I would prefer to get suggestions\nsuggestions from? from Supporter 1\nFor the suggestions given by the two supporters, Supporter 1’s suggestion is a better fit\nwhich one is a better fit for the seeker? than Supporter 2’s\nExperience\nWhich supporter shares better similar experience? Supporter 1 shares better experience\nIf you were the seeker, after hearing the experience Supporter 1’s experience would make\nof which supporter would you feel better? me feel better\nTable 7: Questions for human evaluation of the dialogue models. We design 2-3 questions for each dimensions.\n6681\nE Interface for Crowdsourcing\nFigure 6 shows the interface for crowdsourcing that\nis used in the evaluation of reasoning processes.\nThe crowdsourcing workers are first given the dia-\nlogue followed by validation questions asking some\ndetails about the conversations. The answers to\nthese questions are then used to filter out invalid\nquestions. Results containing non-sense answers\nsuch as “GOOD, GOOD, GOOD” are removed\nfrom the results. After answering the validation\nquestions, the worker will read through reasoning\nprocesses, namely analyses, by different PLMs.\nThe order of the analyses are random for each HIT\nso that the workers will not capture the pattern\nfor further annotations. Then for each analysis,\nthe workers are asked to answer the questions in\nTable 6. To be noticed, for each question, the work-\ners will also need to provide a brief justification\nwhich will be used as future validation judgement\nevidence.\nFigure 7 shows the interface we used for\nACUTE-Eval of the dialogue models. The work-\ners are first shown two conversations, in which\none is directly taken from ESConv, namely human-\nhuman and one is generated by the self-chats of\nthe model. The order of the conversations are ran-\ndomly selected for each HIT. After reading the two\nconversations, the workers are then asked to answer\nthe questions listed in Table 7. From time to time,\nwe ask the workers to provide brief justifications\nfor their choice and such justifications will be used\nto filter out invalid results.\nF Responses that apply ’online’ strategy\nin ESConv\nThe responses tend not to follow the reasoning\nfrom PLMs when same strategies are frequently\nrepeated in the training data of ESConve for the\nconversation with same context. From the collected\nconversations, we are able to find that in most cases,\nBBMHR will follow the suggestions in annotations.\nAnd for all the cases where BBMHR doesn’t follow\nthe suggestions, they follow frequently repeated\nstrategies applied in the training data of ESConv.\nFor instance, one case where BBMHR tends to not\nfollow the reasoning annotations is in the topic of\nongoing depression. When the seeker inputs like\n“I feel really depressed because of the pandemic. ”,\nBBMHR tends to produce a response like “Have\nyou tried hanging out with your friends online?”\neven the reasoning annotation is like “The sup-\nporter could suggest the seeker to go out and take\na break.” And in ESConv, we are able to find that\nmore than 75% of conversations with the topic of\nongoing depression have applied similar responses.\nSuch ignorance of reasoning annotations also hap-\npens in the context of job crisis where “searching\nfor online information” is a repeated strategy. How-\never, the ignorance of reasoning annotations do\nnot appear for other topics that do not share a fre-\nquently repeated strategy.\nTable 8 shows examples of frequently repeated\nanswers and strategies in the ESConv dataset that\ncan affect the responses. When the BBMHR mod-\nels take such context as input, they tend to ignore\nthe reasoning processes from PLMs and follow the\nstrategies stated in the dataset.\n6682\nFigure 6: The crowdsourcing interface used to collect evaluation results for the reasoning processes.\n6683\nFigure 7: The crowdsourcing interface used for dialogue evaluation.\n6684\nSeeker Supporter\nOngoing depression on pandemic\nYes, I pay musical instruments but do to COVID Could you perhaps set up Zoom meetings\ncould not play with the band. where you could play together online?\nHmm what specific hobbies would you Whichever you enjoy.. pick one. There are a\nrecommend? lots of online resources you cloud use.\nDo you have any suggestions? You can play online games with your friends.\nThat actually sounds like a good idea. I hope If you are not comfortable going out due to\nthe shelter near me will take volunteers with COVID, you could involve some activities\nCOVID and all. online promoting dog adaption and create\nawareness online and through social media...\nAll I have to do is think about how alone I am. Do you have any friends or people you can set\nup an online zoom call with?\nI have tried to use zoom and facetime but video There are online resources to have some fun\nchat gives me anxiety. with friends too–many blogs suggest hosting\na group game night or a shared movie night.\nJob crisis\nHmm that seems like a good idea, to find video to well for me i just searched for motivational\nhelp uplift me. Do you recommend anything? speaker or top 10 online?work from home jobs.\nyes It is my main concern. Have you consulted with a job center, a life\ncoach, or any other resource such as online\nwebsites? These may be useful.\nYes , I also dont want them to have to support me with keeping your family in mind while trying\nand my family either . to find a job have you considered looking for\nan online job? Just from chatting with you I can\ntell how much it stresses you out.\nI would be open to seeking other employment Luckily, there are many platforms online\nonline;work from home on the computer. that allow you to work from home. I know\nany suggestions? of several that allow you to do side ¨gigs¨.\nPerhaps you can search and find a few of these.\nI, myself have had success doing these..\nI found it really difficult finding a job right now Have you tried searching a job from some\nbecause of the pandemic. online job-hunting platforms?\nTable 8: Some sample responses under the topic of ongoing depression and job crisis because of COVID pandemic\nin ESConv. 75% percent of the responses are replying about using online resources (online meeting, online gaming,\nonline party, etc.)\n6685\nG Sample Conversations from Different\nModels\nFigure 8 ˜ 13 show sample conversations generated\nby BBMHR, BBMH and BB models on various top-\nics. We are able to observe generally more specific\nand suggestive responses from BBMHR models.\n6686\nFigure 8: Sample conversations by BBMHR + davinci on the topics of “academic” and “ongoing depression”.\n6687\nFigure 9: Sample conversations by BBMHR + ada on the topics of “problems with friends” and “break up with\npartner”.\n6688\nFigure 10: Sample conversations by BBMHR + gpt2 on the topics of “job crisis” and “ongoing depression”.\n6689\nFigure 11: Sample conversations by BBMHR + gpt1 on the topics of “academic” and “job crisis”.\n6690\nFigure 12: Sample conversations by BBMH on the topics of “academic” and “ongoing depression”.\n6691\nFigure 13: Sample conversations by BB on the topics of “problems with friends” and “break up with partner”.\n6692\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nSection 8,9\n□\u0013 A2. Did you discuss any potential risks of your work?\nSection 9\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nAbstract\n□\u0013 A4. Have you used AI writing assistants when working on this paper?\nSection 7. We use ChatGPT to purely paraphrase and polish the content. The input to ChatGPT is the\ntext we wrote. The prompt is: Rephrase the following paragraph to ﬁx the grammatical errors while\nkeep the exactly same semantics <paragraph>. The output is a grammatically correct same-meaning\nparagraph of text.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nSection 4\n□\u0013 B1. Did you cite the creators of artifacts you used?\nSection 1,4\n□\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nSection 1\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNot applicable. Left blank.\n□\u0013 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nSection 3, 5\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNot applicable. Left blank.\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nSection 3\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n6693\nC □\u0013 Did you run computational experiments?\nSection 5\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nSection 4\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSection 5\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 5, 6\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nSection 4\nD □\u0013 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nSection 5.1, 5.2\n□\u0013 D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nAppendix C, D, E\n□\u0013 D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nSection 3, 5.3\n□\u0013 D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nAppendix E\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□\u0013 D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nSection 5.3\n6694",
  "topic": "Ask price",
  "concepts": [
    {
      "name": "Ask price",
      "score": 0.789757490158081
    },
    {
      "name": "Computer science",
      "score": 0.7868887186050415
    },
    {
      "name": "Helpfulness",
      "score": 0.7602535486221313
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5453968644142151
    },
    {
      "name": "Conversation",
      "score": 0.5274805426597595
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5122922658920288
    },
    {
      "name": "Subject-matter expert",
      "score": 0.49805140495300293
    },
    {
      "name": "Expert system",
      "score": 0.4784717261791229
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.45649707317352295
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.4367360472679138
    },
    {
      "name": "Field (mathematics)",
      "score": 0.42308974266052246
    },
    {
      "name": "Human–computer interaction",
      "score": 0.3580082058906555
    },
    {
      "name": "Data science",
      "score": 0.3312826156616211
    },
    {
      "name": "Machine learning",
      "score": 0.32135143876075745
    },
    {
      "name": "Psychology",
      "score": 0.11349740624427795
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Social psychology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Economy",
      "score": 0.0
    },
    {
      "name": "Communication",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Pure mathematics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 20
}