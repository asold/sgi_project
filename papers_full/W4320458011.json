{
  "title": "Do Language Models Plagiarize?",
  "url": "https://openalex.org/W4320458011",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2099471407",
      "name": "Joo-Young Lee",
      "affiliations": [
        "Pennsylvania State University"
      ]
    },
    {
      "id": "https://openalex.org/A2096317137",
      "name": "Thai Le",
      "affiliations": [
        "University of Mississippi"
      ]
    },
    {
      "id": "https://openalex.org/A2103730595",
      "name": "Jinghui Chen",
      "affiliations": [
        "Pennsylvania State University"
      ]
    },
    {
      "id": "https://openalex.org/A2115417150",
      "name": "Dongwon Lee",
      "affiliations": [
        "Pennsylvania State University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2773143256",
    "https://openalex.org/W3020013471",
    "https://openalex.org/W2147528976",
    "https://openalex.org/W3112689365",
    "https://openalex.org/W4229623782",
    "https://openalex.org/W2115269749",
    "https://openalex.org/W2998431971",
    "https://openalex.org/W1966924370",
    "https://openalex.org/W3208416306",
    "https://openalex.org/W2582715692",
    "https://openalex.org/W3058517477",
    "https://openalex.org/W2098862431",
    "https://openalex.org/W2006407062",
    "https://openalex.org/W1790180460",
    "https://openalex.org/W2469132828",
    "https://openalex.org/W3101891351",
    "https://openalex.org/W2566373727",
    "https://openalex.org/W3027379683",
    "https://openalex.org/W4205507439",
    "https://openalex.org/W2396655261",
    "https://openalex.org/W3096214574"
  ],
  "abstract": "THIS IS PART OF THE ACCEPTED PAPER \"Do Language Models Plagiarize?\" @ Proceedings of the ACM Web Conference 2023\\n\\nPast literature has illustrated that language models (LMs) often memorize parts of training instances and reproduce them in natural\\nlanguage generation (NLG) processes. However, it is unclear to what extent LMs “reuse” a training corpus. For instance, models can generate paraphrased sentences that are contextually similar to training samples. In this work, therefore, we study three types of plagiarism (i.e., verbatim, paraphrase, and idea) among GPT-2 generated texts, in comparison to its training data, and further analyze the plagiarism patterns of fine-tuned LMs with domain-specific corpora which are extensively used in practice. Our results suggest that (1) three types of plagiarism widely exist in LMs beyond memorization, (2) both size and decoding methods of LMs are strongly associated with the degrees of plagiarism they exhibit, and (3) fine-tuned LMs’ plagiarism patterns vary based on their corpus similarity and homogeneity. Given that a majority of LMs’ training data is scraped from the Web without informing content owners, their reiteration of words, phrases, and even core ideas from training sets into generated texts has ethical implications. Their patterns are likely to exacerbate as both the size of LMs and their training data increase, raising concerns about indiscriminately pursuing larger models with larger training corpora. Plagiarized content can also contain individuals’ personal and sensitive information. These findings overall cast doubt on the practicality of current LMs in mission-critical writing tasks and urge more discussions around the observed phenomena. Data and source code are available at https://github.com/ Brit7777/ LM-plagiarism.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6194741725921631
    },
    {
      "name": "World Wide Web",
      "score": 0.3696843385696411
    },
    {
      "name": "Information retrieval",
      "score": 0.3384626805782318
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I130769515",
      "name": "Pennsylvania State University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I368840534",
      "name": "University of Mississippi",
      "country": "US"
    }
  ]
}