{
    "title": "Automatic text classification of actionable radiology reports of tinnitus patients using bidirectional encoder representations from transformer (BERT) and in-domain pre-training (IDPT)",
    "url": "https://openalex.org/W4288685128",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5101855470",
            "name": "Jia Li",
            "affiliations": [
                "Beijing Friendship Hospital",
                "Capital Medical University"
            ]
        },
        {
            "id": "https://openalex.org/A5066874145",
            "name": "Yucong Lin",
            "affiliations": [
                "Beijing Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5100627241",
            "name": "Pengfei Zhao",
            "affiliations": [
                "Beijing Friendship Hospital",
                "Capital Medical University"
            ]
        },
        {
            "id": "https://openalex.org/A5100430646",
            "name": "Wenjuan Liu",
            "affiliations": [
                "Beijing Friendship Hospital",
                "Capital Medical University"
            ]
        },
        {
            "id": "https://openalex.org/A5008139526",
            "name": "Linkun Cai",
            "affiliations": [
                "Beihang University"
            ]
        },
        {
            "id": "https://openalex.org/A5107791204",
            "name": "Jing Sun",
            "affiliations": [
                "Beijing Friendship Hospital",
                "Capital Medical University"
            ]
        },
        {
            "id": "https://openalex.org/A5100344069",
            "name": "Lei Zhao",
            "affiliations": [
                "Beijing Friendship Hospital",
                "Capital Medical University"
            ]
        },
        {
            "id": "https://openalex.org/A5045874160",
            "name": "Zhenghan Yang",
            "affiliations": [
                "Beijing Friendship Hospital",
                "Capital Medical University"
            ]
        },
        {
            "id": "https://openalex.org/A5100769854",
            "name": "Hong Song",
            "affiliations": [
                "Beijing Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5024638516",
            "name": "Han Lv",
            "affiliations": [
                "Beijing Friendship Hospital",
                "Capital Medical University"
            ]
        },
        {
            "id": "https://openalex.org/A5077913648",
            "name": "Zhenchang Wang",
            "affiliations": [
                "Beihang University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2766959110",
        "https://openalex.org/W2332062914",
        "https://openalex.org/W2118772689",
        "https://openalex.org/W2938474759",
        "https://openalex.org/W3198293575",
        "https://openalex.org/W2559874490",
        "https://openalex.org/W2905714442",
        "https://openalex.org/W2896983844",
        "https://openalex.org/W3016846378",
        "https://openalex.org/W2338526423",
        "https://openalex.org/W3112303649",
        "https://openalex.org/W3210661083",
        "https://openalex.org/W3035833355",
        "https://openalex.org/W3100221118",
        "https://openalex.org/W3200849552",
        "https://openalex.org/W3031326204",
        "https://openalex.org/W2901643192",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2981133974",
        "https://openalex.org/W3198659451",
        "https://openalex.org/W3011574394",
        "https://openalex.org/W1484393529",
        "https://openalex.org/W6636364444",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W2952370363",
        "https://openalex.org/W3155192455",
        "https://openalex.org/W3192540284",
        "https://openalex.org/W2908140981",
        "https://openalex.org/W2949795957",
        "https://openalex.org/W2971898452",
        "https://openalex.org/W2461049481",
        "https://openalex.org/W2927599034",
        "https://openalex.org/W3216247144",
        "https://openalex.org/W3186583010",
        "https://openalex.org/W3215985540",
        "https://openalex.org/W3196101258",
        "https://openalex.org/W2920860670",
        "https://openalex.org/W3021636956",
        "https://openalex.org/W2980708516",
        "https://openalex.org/W2792974269",
        "https://openalex.org/W3094444847",
        "https://openalex.org/W2622567663",
        "https://openalex.org/W2412972264",
        "https://openalex.org/W3102725307",
        "https://openalex.org/W3088409176"
    ],
    "abstract": null,
    "full_text": "Li et al. \nBMC Medical Informatics and Decision Making          (2022) 22:200  \nhttps://doi.org/10.1186/s12911-022-01946-y\nRESEARCH\nAutomatic text classification of actionable \nradiology reports of tinnitus patients using \nbidirectional encoder representations \nfrom transformer (BERT) and in-domain \npre-training (IDPT)\nJia Li1†, Yucong Lin4†, Pengfei Zhao1, Wenjuan Liu1, Linkun Cai2, Jing Sun1, Lei Zhao1, Zhenghan Yang1, \nHong Song3*, Han Lv1* and Zhenchang Wang1,2* \nAbstract \nBackground: Given the increasing number of people suffering from tinnitus, the accurate categorization of patients \nwith actionable reports is attractive in assisting clinical decision making. However, this process requires experienced \nphysicians and significant human labor. Natural language processing (NLP) has shown great potential in big data \nanalytics of medical texts; yet, its application to domain-specific analysis of radiology reports is limited.\nObjective: The aim of this study is to propose a novel approach in classifying actionable radiology reports of tinnitus \npatients using bidirectional encoder representations from transformer BERT-based models and evaluate the benefits \nof in domain pre-training (IDPT) along with a sequence adaptation strategy.\nMethods: A total of 5864 temporal bone computed tomography(CT) reports are labeled by two experienced radi-\nologists as follows: (1) normal findings without notable lesions; (2) notable lesions but uncorrelated to tinnitus; and \n(3) at least one lesion considered as potential cause of tinnitus. We then constructed a framework consisting of deep \nlearning (DL) neural networks and self-supervised BERT models. A tinnitus domain-specific corpus is used to pre-train \nthe BERT model to further improve its embedding weights. In addition, we conducted an experiment to evaluate \nmultiple groups of max sequence length settings in BERT to reduce the excessive quantity of calculations. After a \ncomprehensive comparison of all metrics, we determined the most promising approach through the performance \ncomparison of F1-scores and AUC values.\n© The Author(s) 2022. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory \nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this \nlicence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/. The Creative Commons Public Domain Dedication waiver (http:// creat iveco \nmmons. org/ publi cdoma in/ zero/1. 0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nOpen Access\n†Jia Li and Yucong Lin contributed equally to this work.\n*Correspondence:  songhong@bit.edu.cn; chrislvhan@126.com; cjr.\nwzhch@vip.163.com\n1 Department of Radiology, Beijing Friendship Hospital, Capital Medical \nUniversity, No. 95 YongAn Road, Beijing 100050, People’s Republic of China\n3 School of Computer Science and Technology, Beijing Institute \nof Technology, No. 5, South Street, Zhongguancun, Haidian District, \nBeijing 100050, People’s Republic of China\nFull list of author information is available at the end of the article\nPage 2 of 17Li et al. BMC Medical Informatics and Decision Making          (2022) 22:200 \nIntroduction\nThe overall prevalence of tinnitus among the general \npopulation ranges from 10% to 14.5% [1, 2], and 30% of \npeople with tinnitus report ‘moderate’ to ‘very big’ diffi -\nculties in daily life [3]. There are a variety of conditions \nthat can cause tinnitus, such as jugular bulb diverticu -\nlum, acoustic neuroma or defect of bone plate in sigmoid \nsinus. Medical imaging is one of the most common tools \nfor detecting the presence of tinnitus. However, as radiol-\nogy reports offer a comprehensive description of visible \nlesions, lesions related to tinnitus account for only a small \nproportion compared with commonly reported degener -\nation or chronic lesions [4]. Especially in elderly patients, \nphysicians may fail to notice such findings for many rea -\nsons, such as lack of experience in diagnosis, moreover, \nclassifying findings correlated with tinnitus requires high \nexpertise in ENT radiology  [5].\nHence, an automatic identification tool for action -\nable reports is needed, so that physicians achieve better \ndecision making without spending extra time on select -\ning appropriate patients from massive radiology reports. \nThus, it is challenging as well as attractive to develop an \nautomated approach of accurately classify the actionable \nreports.\nRadiology reports are constructed with domain-specific \nterms and patterns, and most of them contain unstruc -\ntured data [6]. The typical format of a free-text radiol -\nogy report consists of four sections: the demographics \nsection describes basic information such as the patient’s \nname, age, gender, etc., the clinical information section \nrefers to the medical history or current syndrome. The \nimaging findings section is the main body of the report \nwhich uses anatomic, pathologic, and radiologic termi -\nnology to describe all the normal and abnormal findings \nwithin the scanning field. Finally, the Impression section \nincludes specific diagnosis or differential diagnosis by the \nradiologist,an example of a Chinese radiology report used \nin this study is shown in Additional file  1: FigureS1. The \nwritten style of reports varies among radiologists, and \nthey could contain a number of literature errors [7, 8]. \nManual classification of actionable reports from a large \ndatabase is time-consuming, error-prone, and requires \nexperience to rectify possible errors [9]. Despite the use \nof prompting for structured reporting, free-text radiol -\nogy reports are still favored for their flexibility and low \ncost in major hospitals [10]; this trend necessitates the \napplication of modern informatics to improve the effec -\ntiveness of radiology reports in clinical workflow and bio-\nmedical research.\nNatural language processing (NLP) techniques have \nintroduced a new era for free-text analysis and data min -\ning [11, 12]. Traditional symbolic and statistical NLP \nmethods may perform well on questions that can be \ndefined exactly by a certain set of rules or decomposed \nsimply with statistical patterns of terms in a document; \nboth of them have good results in research cases, includ -\ning data mining of radiology reports [13, 14]. Deep learn-\ning methods with modified neural networks achieved \nstate-of-the-art results with simplicity, flexibility, and task \nspecificity on large-scale complex tasks [15–17]. The con-\nvolutional neural network (CNN) and recurrent neural \nnetwork (RNN) framework has been widely used in clas -\nsification tasks due to their distinguishing performance \nin representation learning. CNNs can capture features \nbetween consecutive words and shift-invariant classifi -\ncation of input information according to its hierarchical \nstructure. The RNN framework has gained attention for \nits ability to deal with variable-length input and output \n[18]; yet, RNNs typically show poor performance when \ndealing with long sequences due to the gradient vanish -\ning and exploding problem. For this problem, a variant \nof RNN named long short-term memory (LSTM) net -\nwork has been developed through controlling the weight \nof previous inputs by adding and regulating “gates”; the \ngates act as controllers to enable the network to retain \nlong-range connections in training.\nApart from RNN-based models, self-attention based \ntransformer models have gained much attention in \nResults: In the first experiment, the BERT finetune model achieved a more promising result (AUC-0.868, F1-0.760) \ncompared with that of the Word2Vec-based models(AUC-0.767, F1-0.733) on validation data. In the second experi-\nment, the BERT in-domain pre-training model (AUC-0.948, F1-0.841) performed significantly better than the BERT \nbased model(AUC-0.868, F1-0.760). Additionally, in the variants of BERT fine-tuning models, Mengzi achieved the \nhighest AUC of 0.878 (F1-0.764). Finally, we found that the BERT max-sequence-length of 128 tokens achieved an AUC \nof 0.866 (F1-0.736), which is almost equal to the BERT max-sequence-length of 512 tokens (AUC-0.868,F1-0.760).\nConclusion: In conclusion, we developed a reliable BERT-based framework for tinnitus diagnosis from Chinese \nradiology reports, along with a sequence adaptation strategy to reduce computational resources while maintaining \naccuracy. The findings could provide a reference for NLP development in Chinese radiology reports.\nKeywords: Artificial intelligence, Natural language processing, Deep learning, Radiology report, Bidirectional \nencoding representation of transformer\nPage 3 of 17\nLi et al. BMC Medical Informatics and Decision Making          (2022) 22:200 \n \nproviding more feasible representations by forming more \ndense correlations between words in a sequence [19]. In \ncontrast to RNNs that rely on constructing the relation -\nship between words in a sequential manner, which turns \nout to be a drawback when extracting the relationship \nbetween two long separated words, self-attention mecha-\nnisms construct the relationship information between \ntokens by building an attention-matrix; this distributes \nproper weights to each token according to the relation -\nship between tokens and importance of token informa -\ntion [20]. The transformer has achieved state-of-the-art \nperformances in a variety of downstream tasks, resulting \nin a significant improvement in NLP . Nevertheless, it still \nrelies on the training corpus that limits its utilization.\nRecently, an advancement in NLP involved a novel self-\nattention based representation model namely bidirec -\ntional encoder representations from transformer (BERT), \nwhich was proposed by Google [21]. By pre-training on \na large plain text corpus, the BERT model can focus on \ngeneral human language understanding and distinguish \namong different use cases for a word [22]. In combination \nwith fine-tuning of downstream tasks, BERT has achieved \nstate-of-the-art results for a variety of NLP tasks [23]. In \nthis way, recently modified versions of BERT-based mod-\nels, such as Roberta [24], Albert [25], and Ernie [26] have \nenriched innovative methods in NLP , as they were devel-\noped through extensive large corpora such wikipedia and \nhave been further optimized for model structure; there -\nfore, they are worthy of investigation for NLP .\nIn the biomedical field, BERT models focusing on med-\nical tasks were developed using large-scale biomedical \ncorpora, such as ClinicalBERT [27] and BioBERT [28]. \nAdditionally, recent studies have shown promising results \nusing the BERT framework in the medical domain. \nAlthough an increasing number of Chinese BERT mod -\nels have been released as open source and demonstrated \nstate-of-the-art performance in NLP benchmarks [29–\n31], the research in Chinese clinical text data mining, \nespecially in the analysis of radiology reports, is very lim -\nited compared with the global trend; this may be associ -\nated with barriers in accessing high quality report data \nand a lack of research pipelines [32]. The existing stud -\nies that use NLP techniques to classify actionable radiol -\nogy reports are summarized in Table 1. To the best of our \nknowledge, there have been no attempts at using BERT \nand in-domain pre-training techniques in the classifica -\ntion tasks of Chinese actionable reports.\nThe first contribution of this study is a novel approach \nin Chinese actionable report classification using BERT, \nCNN, multilayer perceptron (MLP), bi-directional LSTM \nFig. 1 Workflow of study\nPage 4 of 17Li et al. BMC Medical Informatics and Decision Making          (2022) 22:200 \n(Bi-LSTM) and Bi-LSTM-CNN. In addition, we compre -\nhensively evaluated and compared the benefits of three \nrecently proposed Chinese variants of the BERT model: \nchinese-roberta-wwm-ext(abbreviated as Roberta), \nmengzi-bert-base(abbreviated as Mengzi), and chinese-\nbert-wwm-ext(abbreviated as Bert-wwm-ext). Second, \nwith the help of in domain pre-training techniques, we \nfurther improved the performance of the BERT model \nwith respect to accuracy (i.e., F1-score and AUC); these \nresults illustrate the potential of further improvement \nwith additional pre-training. Third, we conducted exten -\nsive experiments using max sequence length as a hyper -\nparameter in the model fine-tuning strategy; the results \ndemonstrated better performance in tokenizing with \na length of 128 and 512 compared with other lengths \ntokenizing methods. Overall, our results have identified \nkey information distribution in Chinese radiology reports \nand could improve related NLP studies in health-related \ntexts, especially in ultra-long text tokenizing.\nMaterials and methods\nStudy overview\nFigure  1 presents the workflow of the study, which is \nmainly composed of five sections: (1) data collection \nand labeling, (2) data preprocessing and partition, (3) \ntext representation and pre-training, (4) classifier train -\ning, and (5) performance evaluation comparison. In the \ndiscussion section, the perspective of this study and the \nfuture utilization of the proposed model is summarized.\nData collection and labeling\nWe retrospectively collected the Electronic Healthcare \nRecording (EHR) data from patients who were admitted \nwith tinnitus and received temporal bone CT exami -\nnations between September 2014 and December 2021 \nfrom a tertiary hospital in Beijing, all the radiology \nreports were written in Chinese and stored in PACS \n(Picture Archiving And Communication System) devel -\noped by DJ HealthUnion. Patients with the intention of \na subsequent visit after initial treatment were excluded \nfrom the study as their reports may contain postopera -\ntive features.\nIn temporal bone imaging, the critical part is the iden -\ntification of imaging findings, which fully covers the \nfeature of lesions in imaging and varies greatly due to \ncomplexity of related diseases. In contrast, the impres -\nsion section may not offer useful information in this task \nbecause imaging is insufficient to give a clinical diagnosis \nof tinnitus. Therefore, imaging finding blocks were seg -\nmented and used in this study. Additionally, all patients’ \nprivate information was removed from reports.\nReports were reviewed and manually labeled by two \nradiologists with at least three years of experience in tem-\nporal bone CT reports. The labeling criteria were based \non the diagnosis framework of tinnitus by Cima et  al. \n[44]. Three classes were manually labeled as follows: nor -\nmal, tinnitus unrelated finding, or tinnitus related find -\ning. The details of labeling criteria are listed in Table  2. \nBefore the start of the experiment, a Kappa test was \nconducted to verify the consistency of labeling perfor -\nmance using a sample of 300, which eventually resulted \nin a Kappa value of 0.79. The details of the Kappa test are \nlisted in Additional file  1: Table  S1. In the labeling pro -\ncess, discrepancies were addressed by a senior expert to \nachieve consensus; the reports that were not eventually \ndefined were excluded from the study.\nTable 1 Summary of NLP studies focusing on actionable radiology reports (ML: Machine Learning, DL: Deep Learning, BERT: \nBidirectional Encoding Representation of Transformer).\nAuthor(s) Language Number of \nradiology reports\nAlgorithm Section of report Research objective\nCarrodeguas et al. [33] English 2306 ML/DL Impression Classifying recommendation\nHelibrun et al. [34] English 851 Rule-based Impression Detecting critical finding\nLou et al. [35] English 6000 ML Not mentioned Classifying recommendation\nEsteban et al. [36] English 3401 Software Findings, impression Classifying recommendation\nMorioka et al. [37] English 1402 Rule-based Not mentioned Classifying disease condition\nFu et al. [38] English 1000 Rule-based ML/DL Not mentioned Classifying disease condition\nNakamura et al. [39] Japanese 63646 BERT Order, findings, impression Detecting critical finding\nJujjavarapu et al. [40] English 871 ML Not mentioned Classifying disease condition\nLiu et al.. [15] Chinese 1089 BERT/ML Findings Classifying disease condition\nZhang et al. [41] Chinese 359 BERT Pre-training Findings Classifying disease condition\nZaman et al. [42] English 1503 BERT Pre-training Findings Classifying disease condition\nLiu et al.. [43] English 594 BERT Not mentioned Classifying certainty\nProposed study Chinese 5864 BERT Pre-training DL Findings Classifying disease condition\nPage 5 of 17\nLi et al. BMC Medical Informatics and Decision Making          (2022) 22:200 \n \nAfter the screening, a total of 5864 reports were ulti -\nmately considered and annotated. They were then divided \ninto 70% for training (n = 4104), 15% for validation (n = \n880), and 15% for testing (n = 880) datasets. The label \nTable 2 Data labeling criteria\nClassification Potentially clinically important findings Label instruction\nNormal finding (labeled as 0) NA The scenarios when all organs are described as normal\nIrrelevant finding (labeled as 1) Bone: If any lesion is observed and should be reported; \nmeanwhile, the clinician is confident that the image \nfinding provided limited information for diagnosis of \ntinnitus.\nAny Degeneration\nBrain:\n-Brain degeneration\nNose and Sinus:\n-Nasosinusitis (frontal sinus, sphenoid sinus, ethmoid sinus, \nmaxillary sinus (except acute inflammation involving adjacent \nbone structures))\n-Nasal turbinate hypertrophy\n-Deviation of nasal septum\n-Sinus cyst\nExternal/middle ear: -\nChronic middle ear mastoiditis (except acute inflammation \ninvolving adjacent bone structures)\n-Auditory canal cerumen\n-Low middle cranial fossa\nRelative finding (labeled as 2) Bone: - If one or more image findings should be reported \nin detail, and lead to a certain diagnosis for further \nexamination or clinical evaluation. Or the image find-\ning addressed a need for urgent communication with \nclinicians for timely treatment. Since there is variability \nin language expression, the labeler’s judgment is used \nas reference.\nSigmoid sinus bone wall deficiency\n-Superior semicircular canal dehiscence\n-Auditory ossicle abnormality\n-Bone fracture\n-High jugular fossa\nBrain:\n-Neoplasms\n-Intracranial hemorrhage\n-Cerebral infarction\n-Cerebral herniation\nNose and sinus cavity:\n-Neoplasms\n-Nasosinusitis (morphologically altered bone or sinus tract \nobstruction)\nExternal/Middle ear:\n-Tympanic lesion (inflammation, neoplasm or perforation)\n-Otosclerosis\n-Cholesteatoma\n-Other neoplasm observed within the imaging field\nPage 6 of 17Li et al. BMC Medical Informatics and Decision Making          (2022) 22:200 \nwas controlled as a hierarchical indicator. The details \nfor labeling and text preprocessing are illustrated in Fig -\nures 1 and 2.\nData preprocessing\nAll reports were preprocessed before further modeling, \nthe punctuations and linguistic tags were first removed \nfrom the text using a preprocessing pipeline. In unstruc -\ntured radiology reports, although radiologists typically \nFig. 2 Details of report annotation and text preprocessing. Yellow characters represent findings irrelevant to tinnitus; red characters represent \nfindings relevant to tinnitus and should be actionably reported in communication with physicians. *All reports were written in Chinese, and English \ntranslations are presented in the figure for illustration\nPage 7 of 17\nLi et al. BMC Medical Informatics and Decision Making          (2022) 22:200 \n \npresent information based on standard free-text tem -\nplates of reporting, they may contain linguistic and com -\nputational symbols such as end-of-line (EOL), blank \ncharacter (BC) or line break (LB). These are considered \nnoisy data and lengthen the text; this phenomenon is \nmore common when collecting data from long term his -\ntorical datasets. Then, a Chinese stopword corpus is uti -\nlized to filter meaningless stopwords from the text; those \nwords may contain Chinese auxiliary words used for lit -\neral sense of formality. Finally, comments that were noted \nin the report were removed from the text; the comments \nwere used for internal communication between the \nworkstation and radiologists to notify them of remarka -\nble events in clinical workflow, and are not essential com-\nponents of radiology reports.\nText representation\nWord embedding is an essential technique in NLP used \nto represent language based characters or words in \nquantitative values before further analysis. Typically, the \nembedded words could be represented in neighboring \nhigh-dimensional spaces according to the similarity of \ntheir actual meaning. Word embedding can be achieved \nthrough different embedding techniques, and each with \nits own pros and cons. In this study, we utilized two \nmajor methods for Chinese language embedding: Word -\n2Vec and BERT.\nWord2Vec as embedding method\nWord2Vec is an algorithm that generates a high-dimen -\nsional vector according to the given input when accepting \na certain training corpus; this means that words with sim-\nilar literal meaning may stay in a relatively closer space to \neach other, and this feature enables the generated matrix \nto maintain certain information within the text. Further -\nmore, the Word2Vec algorithm also has shortcomings \nsince it cannot accept new words if it is not included in \nthe training process. In this study, we accepted one of the \nmain architectures in the Word2Vec-Skip-Gram model \nas an embedding method, the details of embedding \nparameters is provided in  Additional file 1: Table S2.\nBERT as embedding method\nThe BERT has been recently proposed by researchers in \nGoogle. The novelty of BERT is the application of the \nbi-layer transformer architecture - an attention-based \nmechanism that can accurately extract contextual rela -\ntionships in words to realize unsupervised learning by \ncombining text input and output through the decoder-\nencoder framework. The BERT was initially proposed \nafter being trained on ultra large datasets such as Wiki -\npedia, and an optimal performance may be achieved after \nfine-tuning datasets of downstream tasks.\nClassifier training\nIn our study, we used four recent state-of-the-art NLP \nclassifiers: CNN, MLP , Bi-LSTM and a hybrid Bi-LSTM-\nCNN. As previously stated, CNN has the advantage of \nmaximizing and extracting local features of neighbor -\ning words by convolutional and maxpooling layers, \nwhereas Bi-LSTM models could store features of words \nin whole sentences by using cells and gates from both left \nand right directions to combine past and future context \ninformation from long-sentence radiology reports. The \nMLP model is a baseline deep learning model that is uti -\nlized for comparison, while the Bi-LSTM-CNN model \nis proposed as a hybrid neural network. After hyperpa -\nrameter grid search, the optimum model performance \nis presented; the detailed description of those training \nparameters is listed in Table 3.\nFine‑tuning of BERT‑based models\nAs the BERT model can be applied to a variety of differ -\nent natural language processing downstream tasks and \nonly requires minimal adjustments, fine-tuning BERT \nwith our labeled radiology reports has offered a great \nopportunity to exploit the advantages of the BERT frame-\nwork and achieve competitive results. In the biomedical \nfield, the fine-tuning technique has attracted much atten-\ntion in classification tasks [21,45], however, the attempts \nin NLP tasks of Chinese radiology reports seems sparse. \nHowever, recently proposed novel variants of BERT-\nbased Chinese embedding models have offered greater \npotential for further boosting NLP research in Chinese. \nTo evaluate the benefits of BERT fine-tuning ,in the clas -\nsification of tinnitus in Chinese radiology, BERT-base \nand the 3 variant models were used for fine-tuning in this \nstage.\nRecently, many variants of BERT in Chinese have \nbeen published such as hfl/chinese-bert-wwm-ext \n(BERT-wwm-ext) and hfl/chinese-roberta-wwm-ext \n(Roberta) by Cui et  al. [46], and Langboat/mengzi-\nbert-base (Mengzi) recently published by Zhang et  al. \n [47]. These models were trained on large scale corpora, \npre-trained with optimized strategy such as whole \nword masking, and achieved state-of-the-art (SOTA) \nTable 3 Hyperparameters of model training\nModel Layers Epochs Batch size Optimizer\nCNN 16 20 32 Adam\nMLP 16 20 32 Adam\nBi-LSTM 16 20 32 Adam\nBi-LSTM-CNN 32 20 32 Adam\nBERT (variants) -fine tune 768 10 8 Adam\nBERT-pre-training 768 10 8 Adam\nPage 8 of 17Li et al. BMC Medical Informatics and Decision Making          (2022) 22:200 \nperformance in multiple official benchmarks such as \nGLUE, MNLI and QNLI. It is therefore attractive to \ntestify the benefits of these models in Chinese medical \ndomain tasks and evaluate their performance by fine-\ntuning. In this study, BERT-base-Chinese (BERT), hfl/\nchinese-bert-wwm-ext (BERT-wwm-ext), hfl/chinese-\nroberta-wwm-ext (Roberta), and Langboat/mengzi-\nbert-base (Mengzi) were enrolled in the framework and \ncompared. Additionally, the hyperparameters of each \nmodel are provided in Additional file 1 :Table S3.\nFor fine-tuning, one full-connection(FC) layer was \nadded after BERT in combination with a softmax layer \nfor the label output. For major hyperparameters, the \nmax sequence length was set to 512, the training batch \nsize was set to 16, and the training epoch was set to 10. \nThe hyperparameters were chosen based on the mem -\nory and computing power of our GPU resources. We \nfine-tuned the mainstream BERT-based models in Chi -\nnese text NLP tasks: BERT-base-Chinese (BERT), hfl/\nchinese-bert-wwm-ext (BERT-wwm-ext) and hfl/chi -\nnese-roberta-wwm-ext (Roberta) along with Langboat/\nmengzi-bert-base (Mengzi). The basic architecture of \nBERT based models is illustrated in Fig. 3 .\nIn‑domain pre‑training of BERT\nFor further exploration of the potential of BERT-based \nmodels in language representation, and as it has been \nproven that pre-training can effectively improve model \nperformance with limited data, we hypothesize that an \nin-domain pre-training task (IDPT) could be consid -\nered as a way for greater utilization of BERT in this task. \nThe IDPT could be regarded as a process of transferring \nlearning to integrate the domain-specific knowledge into \nthe original BERT model, in this way, the initial weights \nof BERT could be adjusted adequately to maximize per -\nformance and accuracy in domain specific tasks [48]. \nHence, in order to transfer the domain-specific knowl -\nedge and language representation of tinnitus to form \na domain-specific healthcare BERT model, we used a \nlarge-scale database of tinnitus related clinical notes for \nIDPT of BERT.\nIn the IDPT stage, we collected 3873 clinical cases \nand 1431 radiology reports, which accounts for about 1 \nFig. 3 Illustration of architecture in BERT based models. The English subtitle is a translation of a sentence in a Chinese radiology report\nPage 9 of 17\nLi et al. BMC Medical Informatics and Decision Making          (2022) 22:200 \n \nmillion words. For the preprocessing stage, the space and \nnewline symbols were removed from the corpus to form \nthe corpus data; thereafter, no further processing was \nperformed. The pre-trained tinnitus-BERT model was \ntrained in a way described in literature [48], the hyper -\nparameters, computing resource and training time in the \nIDPT stage is listed in Table 4.\nToken length optimizing strategy (TLOS) based on max \nsequence length\nMax sequence length is a critical hyperparameter in the \nBERT model. For long sequence embedding, the token \noverflowing the max sequence length would be cut, while \nshort sequences would be “padded” (i.e., filled with zeros \nor specific number) to the same length; this mechanism \naims for constant length alignment of all input text. The \nChinese-based BERT models use each character as a \ntoken; however, the token length of sequences in within \neach group varies widely. Figure  4 shows the disparity \nwith two “peaks” along with the “long tail” in token distri-\nbution number, whereas the statistics in Table 5 show this \nvariation more precisely. Previous studies have suggested \nthat report length is affected by the amount of confidence \nthe radiologist has in their analysis, we hypothesize that \nthe token length in this study may lead to further investi -\ngation of patterns of Chinese radiology reports[49].\nThis phenomenon may be caused by the following rea -\nsons: (1) for patients with multiple or complex lesions, \ndetailed reporting of radiological manifestation is needed, \nand many radiologists typically write an individual sec -\ntion of the foremost imaging findings before all findings \nfor timely attention, thus prolonging the whole report; (2) \nthe standards of reporting across historical timelines may \nhave changed, as the EHR system may have progressed \nTable 4 Training time, computing resource and hyperparameters in IDPT\nData size Train epochs Train batch_size Eval batch_size Eval strategy Eval steps GPU Pre‑training time per epoch\n10.7 MByte 10 16 16 Steps 100 Nvidia GTX1070 32 min\nFig. 4 Token length distribution in training dataset\nTable 5 Token length distribution in all training datasets.\nLabel Average token length \n(±standard deviation)\nNumber \nof \nsamples\nNormal finding 0 182.92±12.62 1100\nUnrelated finding 1 237.73±28.45 2851\nRelated finding 2 262.52±47.13 1913\nPage 10 of 17Li et al. BMC Medical Informatics and Decision Making          (2022) 22:200 \nwith further requests for more detailed reporting; (3) there \nis a variance in the writing style of different radiologists, \nparticularly considering the differences in experience and \nskills.\nA long sequence would consume more GPU memory \nand computational resources, especially when deploying \nlarge models such as BERT[50], and clinical departments \nwould not commonly deploy high performance GPUs and \nRAMs. To address this uncertainty, we hypothesize that \nthe foremost section of radiology reports be considered as \na priority in classification; this could be testified by using \nmax sequence length as a variable to evaluate the per -\nformance of models in encoding. In this experiment, we \napplied the sequence length values of 128, 256, 328, 468 \nand 512 (default), and compared the results to test our \nconjecture.\nResults\nEvaluation method\nThe performance of each method was evaluated using the \nreceiver operating characteristic (ROC) curve, along with \naccuracy, precision, recall and F1-score. Further, true pos-\nitive (TP) and false positive (FP) are the number of posi -\ntive cases correctly and incorrectly predicted, while true \nnegative (TN) and false negative (FN) are the number of \nnegative cases correctly and incorrectly predicted. Equa -\ntions 1 (1)-(4) describe the performance metrics, and the \nconfusion matrix of results is presented in Additional file 1: \nTable S4.\n(1)Accuracy= TP + TN\nTP + TN + FP + FN\n(2)Precision= TP\nTP + FP\n(3)recall= TP\nTP + FN\nEqs 1: Equation for performance metrics\nExperiment 1:BERT finetuning model in comparison \nwith Word2Vec based deep learning models\nAs a main goal of this study is to test the benefit of \nusing BERT fine-tune in radiology classification task, \nthe first experiment compares the results of the pro -\nposed Word2Vec embedding and classifiers: CNN, MLP , \nBi-LSTM, Bi-LSTM-CNN with the BERT-based fine-\ntuning approach for classifying normal, tinnitus unre -\nlated, and tinnitus related actionable radiology reports \nusing collected data. The results are shown in Tables  6, \n7, and the ROC curve is shown in Figure  5. The BERT \nfine-tune model outperformed the Word2Vec based \nmodels, BERT fine-tune achieved both the highest AUC \nof 0.868 and F1-score of 0.760; however, the difference \nbetween BERT fine-tune and second highest model (i.e., \nWord2Vec+CNN) is not larger than 1%. In addition, the \napproaches which combined BERT with classifiers were \nalso evaluated, and the results are shown in Figure 6.\nExperiment 2: evaluation of BERT fine‑tune and BERT \nin‑domain pre‑training(IDPT) and comparison with BERT \noriginal\nThe IDPT technique is applied in this study to further \nexploit the advantage of using BERT frameworks in \npre-training. We compared the performance of the clas -\nsification task certainty using three stages of BERT mod -\nels: BERT original,BERT fine-tune and BERT-IDPT. The \nROC curve and AUC values are shown in Figure  7 and \nthe metrics are shown in Table 7. Compared to the results \nof BERT fine-tune, the BERT IDPT model obtained a sig-\nnificant improvement with AUC of 0.948 and F1-score \nof 0.841. In addition, the results demonstrated that after \nfine-tuning, the BERT model was efficiently adjusted to \nfit the task, when compared with original state, the AUC \nof the BERT fine-tune model increased from 0.419 to \n0.868.\n(4)F1-score = 2\n1/precision + 1/recall\nTable 6 Training Time and Hyperparameters in TLOS\nToken length Train epochs Batch size Optimizer Learning rate Training time \nper epoch (Min)\n128 10 16 Adam 2e-5 12±0.24\n256 10 16 Adam 2e-5 23±0.70\n328 10 16 Adam 2e-5 31±0.47\n468 10 16 Adam 2e-5 39±1.21\n512 10 16 Adam 2e-5 43±0.62\nPage 11 of 17\nLi et al. BMC Medical Informatics and Decision Making          (2022) 22:200 \n \nExperiment 3: Evaluation of BERT Variants models \nwith fine‑tuning\nTo evaluate the benefits of using novel BERT-variant \nmodels in Chinese radiology reports classification, we \ncompared the results of the four models after fine-tuning, \nthe ROC curve and AUC values are shown in Figure 8 and \nthe metrics are shown in Table 7. As a result, the Mengzi-\nmodel yielded the best AUC of 0.878 and F1-score of \n0.764. Meanwhile, Roberta and BERT-wwm-ext achieved \na relatively equal score compared to the BERT model. In \ngeneral, the results are promising but not supportive to \ndemonstrate a comprehensive improvement.\nExperiment 4: token length optimizing strategy (TLOS) \nbased on max sequence length\nAs a result, the BERT model with full sequence length \n(512) achieved the highest AUC value and F1-score but \nalso cost the longest training time-43 minutes per epoch. \nTable 7 Comparison of model performance metrics\nThe highest index is highlighted in bold\nEmbedding Classifier Accuracy Precision Recall AUC F1‑score\nWord2Vec CNN 0.729 0.744 0.729 0.767 0.733\nMLP 0.644 0.643 0.644 0.711 0.644\nBi-LSTM 0.737 0.740 0.737 0.677 0.738\nBi-LSTM-CNN 0.728 0.729 0.728 0.692 0.728\nBERT CNN 0.770 0.788 0.777 0.908 0.781\nMLP 0.719 0.714 0.719 0.874 0.712\nBi-LSTM 0.777 0.792 0.780 0.888 0.774\nBi-LSTM-CNN 0.698 0.696 0.698 0.861 0.690\nFine-tune 0.760 0.761 0.759 0.868 0.760\nIDPT 0.842 0.843 0.842 0.948 0.841\nBERT-wmm-ext Fine-tune 0.756 0.756 0.756 0.883 0.754\nMengzi Fine-tune 0.751 0.751 0.751 0.846 0.750\nRoberta Fine-tune 0.767 0.767 0.767 0.878 0.764\nFig. 5 ROC curve and AUC values of BERT fine-tune and deep learning models. FPR:False Prediction Rate; TPR: True Prediction Rate.\nPage 12 of 17Li et al. BMC Medical Informatics and Decision Making          (2022) 22:200 \nHowever, we noticed that the second highest score was \nyielded for the shortest sequence length (i.e., 128-token \ngroup), with a relatively equal AUC score of 0.868 (ver -\nsus 0.878 in the 512-token group); also in addition, the \nF1-score in the 128-token group was 0.736 compared \nwith 0.760 in the 512-token group. However, the accu -\nracy declined when the sequence length increased from \n128 to 468, with the lowest score in the 468-token group. \nThe metrics are shown in Table  8, the Training Time and \nHyperparameters in TLOS are descripted in Table  6, the \nROC curve and AUC values are shown in Figure  9, and \nthe relationship between accuracy and token length is \nshown by a dot plot in Figure 10.\nDiscussion\nRadiology reports are an essential component of big med-\nical data. Previous studies have fully demonstrated the \nfeasibility of extracting evidence from radiology reports \nto assist clinical diagnosis and prognosis and promote \nautomatic communication between physicians, radiolo -\ngists and patients[37,39,42].However, the full potential \nof NLP remains to be further discovered, whereas deep \nlearning-based algorithms have nearly revolutionized \nthe paradigm of medical imaging. Radiology reports are \nprimarily intended to provide information to assist with \ndiagnosis; this information must be interpreted by physi -\ncians before being transmitted to patients. However, this \nmay not be guaranteed because of the busy schedules of \nphysicians and lack of expertise who are knowledgeable \nabout tinnitus diagnosis and treatment, which can nega -\ntively affect doctor-patient interactions and potentially \nadversely impact patient outcomes[51]. In addition, there \nis still controversy regarding the appropriate imaging of \ntinnitus[52]. Despite the consensus declaration of multi -\nple medical societies[1,4,44], large-scale real-world evi -\ndence for quantifying the effectiveness of imaging results \nis urgently needed to justify their opinions. Therefore, it \nis necessary to promote the research into the application \nof NLP-based technology to tinnitus radiology reports.\nThe language representation method is one of the high-\nlights of NLP studies. Recent NLP studies in actionable \nradiology reports include two types of approaches: (1) \nrule/pattern-based framework, and (2) deep learning/\nBERT-based framework. Many studies that have used \nthe former technique report promising results with pre-\ndefined patterns while having poor generalization ability \nin other tasks. Meanwhile, BERT-based approaches have \ngradually performed well with more skillful fine-tuning \nand pre-training strategies. However, the studies focusing \non Chinese radiology reports are rare.\nFig. 6 ROC curve and AUC values of BERT combined with deep learning models. FPR:False Prediction Rate; TPR: True Prediction Rate.\nPage 13 of 17\nLi et al. BMC Medical Informatics and Decision Making          (2022) 22:200 \n \nFrom the same starting point, Aaron et  al. reported a \nmachine learning-based classification of temporal bone \nimaging reports for the identification of inner/middle/\nouter and mastoid abnormalities[53]. Although this \nmethod has achieved good results, it fails to classify the \nabnormal patients with clinical significance. In contrast \nto previous studies that use BERT to classify actionable \nradiology reports with term-specific strategy or covering \nmultiple pathological characteristics, our study has three \nnovelties: (1) a framework of fine-grained labeling strat -\negy to improve practical value in clinical scenarios, (2) \nthe utilization of a relatively large disease-specific corpus \nin-domain pre-training strategy to improve the model \nperformance; and (3) the feasibility of using shorter \nsequence length to accelerate model building while main-\ntaining its performance. These innovations may con -\ntribute to further use of BERT in Chinese medical text \nanalysis through NLP technology.\nIn the first experiment, we demonstrated the benefit \nof using BERT compared to other deep learning models \nincluding Bi-LSTM, CNN and hybrid Bi-LSTM-CNN. \nAlthough the results did not show large difference in \nF1-score (BERT:0.760, CNN: 0.733), BERT fine-tuning \nachieved a higher AUC value of 0.866(CNN:0.767). In \nexperiment 2, we further used transfer learning in BERT \nby pre-training an in-domain corpus that elevated the \nF1-score (BERT: 0.760 versus BERT-IDPT: 0.841) and \nAUC (BERT: 0.868 versus BERT-IDPT:0.948); this indi -\ncated competitive performance in the classification task.\nPre-training is an important technique in NLP field, \nthis approach has recently attracted increasing attention, \nespecially in healthcare related fields. For instance, Zhang \net al. [41] designed and evaluated the feasibility of using \npre-training models to extract key information from \nChinese radiology reports fort lung cancer staging, the \nmodel achieved an F1 of 85.96%,while our study achieved \nan F1 of 84.10%. More recently, Nakamura et  al. [39] \napplied BERT without IDPT to classify actionable Japa -\nnese radiology reports, and attempted to predict a posi -\ntive/negative “actionable tag” , the results seem promising \nwith highest AUC of 0.95. In comparison with previous \nstudies on radiology report classification, the labeling \nmethods applied in this study were more complex, which \nrequire both physicians’ clinical experience and priori \nanatomic knowledge of radiology. Moreover, we utilized \nIDPT to the improve the BERT model with domain spe -\ncific knowledge, which has reported to be state-of-the-art \nperformance.\nFinally, we define this study as customized research \nwith practical purpose, considering that the large \nFig. 7 ROC curve and AUC values of BERT original, BERT-finetune and BERT-IDPT model.FPR:False Prediction Rate; TPR: True Prediction Rate.\nPage 14 of 17Li et al. BMC Medical Informatics and Decision Making          (2022) 22:200 \ncomputational demand of BERT in long sentence \nprocessing may not be fully satisfied under common \ndeployment situations. Based on the authors’ working \nexperience as radiologists, we propose a max sequence \ntoken adaptation strategy to assess the performance \nwith partial embedding. The results showed that the \n128-token embedding achieved a relatively equal per -\nformance compared with whole sentence embedding \n(F1-score of 0.736 in 128 tokens versus 0.760 in 512 \ntokens, AUC of 0.866 in 128 tokens versus 0.868 in \n512 tokens). This result may be partially explained by \nthe tacit occupational habit of radiologists to record \nthe most emergent finding in an individual paragraph \nbefore normal findings.\nLastly, this study has several limitations that need to \nbe discussed. First, although the data size of this study \n(5864 reports for training, 3873 clinical cases and 1431 \nradiology reports for in domain pre-training) is rela -\ntively large compared to related studies (presented in \nTable 1), the bias should be considered as it is a single-\ncenter study. More data from multiple centers and bias \ncorrection may enable more efficient transfer learning \nof BERT to yield promising results in real world sce -\nnarios, which we will pursue in the near future. Second, \nwe developed and evaluated the BERT-based frame -\nwork to identify actionable radiology reports from \ntemporal bone imaging. However, the generalizability \nof this model to other types of radiology reports, such \nFig. 8 ROC curve and AUC values of BERT and Variant models. FPR:False Prediction Rate; TPR: True Prediction Rate.\nTable 8 Comparison of BERT finetune with different max sequence lengths\nModel Max sequence length Accuracy Precision Recall AUC F1‑score\nBERT fine-tune 128 0.741 0.738 0.741 0.866 0.736\n256 0.71 0.707 0.71 0.843 0.708\n328 0.616 0.627 0.616 0.797 0.601\n468 0.551 0.557 0.551 0.759 0.546\n512 0.760 0.761 0.759 0.868 0.760\nPage 15 of 17\nLi et al. BMC Medical Informatics and Decision Making          (2022) 22:200 \n \nFig. 9 ROC curve and AUC values of the BERT model using different max sequence lengths. FPR:False Prediction Rate; TPR: True Prediction Rate.\nFig. 10 Comparison of accuracy in the BERT model using different max sequence lengths\nPage 16 of 17Li et al. BMC Medical Informatics and Decision Making          (2022) 22:200 \nas head CT, MRI, and so on needs to be further evalu -\nated with more fine-tuning strategies. Furthermore, it \nis worth noting that our proposed framework is a semi-\nautomated pipeline that requires no further remod -\neling of the base architecture. In this regard, physicians \nwith clear purpose of research demand should ben -\nefit by merely focusing on the labeling criteria. Third, \nthe hyperparameters of this model, such as batch size, \ntraining epoch or learning rate are limited by comput -\ning resources. To pre-train a wider range of data and \nrealize more comprehensive results, a more advanced \noperating environment would be necessary. Some \nexamples could be ClinicalBERT, which was trained \nby 2,000,000 clinical notes from the MIMIC-III data -\nbase and BioBERT that was trained using all PubMed \npublications.\nConclusion\nIn this study, we proposed a BERT based framework \nusing an in domain pretraining technique to classify \nactionable radiology reports in tinnitus patients. The \nexperimental results show that our model outperforms \nthe benchmark deep learning base models, BERT-base \nmodel and BERT variants. Additionally, we proposed \na max-sequence-length adaption method for process -\ning long text Chinese radiology reports. This study may \npromote the using of BERT in clinical decision support \nand academic research.\nSupplementary Information\nThe online version contains supplementary material available at https:// doi. \norg/ 10. 1186/ s12911- 022- 01946-y.\nAdditional file 1. Related information of data analysis and  model-con-\nstruction for this paper.\nAcknowledgements\nNot applicable.\nAuthor contributions\nJL wrote the main manuscript, YL, LC and HS developed the model, PZ, and \nWL labeled the data, JS and LZ collected and analyzed the data, ZY reviewed \nand revised the manuscript, ZW and HL designed the study. All authors read \nand approved the final manuscript.\nFunding\nThis work was supported by Grant 61931013 (Wang Zhenchang), 62171297 \n(Lv Han) 82171886 (Zhao Pengfei) from the National Natural Science Founda-\ntion of China,Beijing Hospitals Authority Clinical Medicine Development of \nSpecial Funding Support NO:ZYLX202101,the Beijing Municipal Science and \nTechnology Commission [Grant Number Z201100005620009]\nAvailability of data and materials\nThe datasets generated during and analyzed during the current study are not \npublicly available due to the institution’s policies involved in Human genetics \nresources, but are available from the corresponding author on reasonable \nrequest. The code used during the current study are available in https:// \ngithub. com/ curry lee92/ BERT.\nDeclarations\nEthics approval and consent to participate\nEthical approval and the waived informed consent was gained from the \nBeijing Friendship Hospital ethics committee, Capital Medical University \n(Research Application System number 2021-P2-142-01) according to 《Dec-\nlaration of Helsinki》and《Ethical review of biomedical research involving \npeople》by Ministry of Public Health of China.\nConsent for publication\nNot applicable, as no identifiable participant data, pictures or illustrations that \nrequire consent for publishing are included in this manuscript.\nCompeting interests\nThe authors declare that they have no competing interests.\nAccordance Statement\nAll methods were performed in accordance with the relevant guidelines and \nregulations.\nAuthor details\n1 Department of Radiology, Beijing Friendship Hospital, Capital Medical Univer-\nsity, No. 95 YongAn Road, Beijing 100050, People’s Republic of China. 2 School \nof Biological Science and Medical Engineering, Beihang University, No.37 \nXueYuan Road, Beijing 100191, People’s Republic of China. 3 School of Com-\nputer Science and Technology, Beijing Institute of Technology, No. 5, South \nStreet, Zhongguancun, Haidian District, Beijing 100050, People’s Republic \nof China. 4 School of Medical Technology, Beijing Institute of Technology, No.5 \nZhongguancun East Road, Beijing 100050, People’s Republic of China. \nReceived: 14 May 2022   Accepted: 18 July 2022\nReferences\n 1. Kessler MM, Moussa M, Bykowski J, et al. ACR Appropriateness criteria((R)) \ntinnitus. J Am Coll Radiol. 2017;14(11S):S584–91. https:// doi. org/ 10. 1016/j. \njacr. 2017. 08. 052.\n 2. Xu X, Bu X, Zhou L, et al. An epidemiologic study of tinnitus in a popula-\ntion in Jiangsu Province, China. J Am Acad Audiol. 2011;22(9):578–85. \nhttps:// doi. org/ 10. 3766/ jaaa. 22.9.3.\n 3. American Tinnitus Association(ATA)[EB/OL]. Accessed at February 1. 2022. \nhttps:// www. ata. org/ under stand ing- facts/ demog raphi cs.\n 4. Tunkel DE, Bauer CA, Sun GH, et al. Clinical practice guideline: tinnitus. \nOtolaryngol Head Neck Surg. 2014;151(2):S1–40. https:// doi. org/ 10. 1177/ \n01945 99814 545325.\n 5. Gomes RLE. Review and update of temporal bone imaging. Radiol Brasil. \n2019;52(2):7–8. https:// doi. org/ 10. 1590/ 0100- 3984. 2019. 52. 2e2.\n 6. Mozayan A, Fabbri AR, Maneevese M, et al. Practical guide to natural \nlanguage processing for radiology. Radiographics. 2021;41(5):1446–53. \nhttps:// doi. org/ 10. 1148/ rg. 20212 00113.\n 7. Brady AP . Error and discrepancy in radiology: inevitable or avoid-\nable? Insights Imag. 2017;8(1):171–82. https:// doi. org/ 10. 1007/ \ns13244- 016- 0534-1.\n 8. Shinagare AB, Lacson R, Boland GW, et al. Radiologist preferences, agree-\nment, and variability in phrases used to convey diagnostic certainty in \nradiology reports. J Am Coll Radiol. 2019;16(4):458–64. https:// doi. org/ 10. \n1016/j. jacr. 2018. 09. 052.\n 9. Itri JN, Tappouni RR, McEachern RO, et al. Fundamentals of diagnostic \nerror in imaging. Radiographics. 2018;38(6):1845–65. https:// doi. org/ 10. \n1148/ rg. 20181 80021.\n 10. Kim SH, Sobez LM, Spiro JE, et al. Structured reporting has the potential \nto reduce reporting times of dual-energy x-ray absorptiometry exams. \nBMC Musculoskelet Disord. 2020;21(1):248. https:// doi. org/ 10. 1186/ \ns12891- 020- 03200-w.\n 11. Pons E, Braun LM, Hunink MG, et al. Natural language processing in \nradiology: a systematic review. Radiology. 2016;279(2):329–43. https:// doi. \norg/ 10. 1148/ radiol. 16142 770.\nPage 17 of 17\nLi et al. BMC Medical Informatics and Decision Making          (2022) 22:200 \n \n 12. Chen TL, Emerling M, Chaudhari GR, et al. Domain specific word embed-\ndings for natural language processing in radiology. J Biomed Inform. \n2021;113: 103665. https:// doi. org/ 10. 1016/j. jbi. 2020. 103665.\n 13. Steinkamp J, Cook TS. Basic artificial intelligence techniques: natu-\nral language processing of radiology reports. Radiol Clin North Am. \n2021;59(6):919–31. https:// doi. org/ 10. 1016/j. rcl. 2021. 06. 003.\n 14. Ong CJ, Orfanoudaki A, Zhang R, et al. Machine learning and natural \nlanguage processing methods to identify ischemic stroke, acuity and \nlocation from radiology reports. PLoS ONE. 2020;15(6): e234908. https:// \ndoi. org/ 10. 1371/ journ al. pone. 02349 08.\n 15. Liu H, Zhang Z, Xu Y, et al. Use of BERT (bidirectional encoder representa-\ntions from transformers)-based deep learning method for extracting evi-\ndences in chinese radiology reports: development of a computer-aided \nliver cancer diagnosis framework. J Med Internet Res. 2021;23(1): e19689. \nhttps:// doi. org/ 10. 2196/ 19689.\n 16. Nakamura Y, Hanaoka S, Nomura Y, et al. Automatic detection of action-\nable radiology reports using bidirectional encoder representations from \ntransformers. BMC Med Inform Decis Mak. 2021;21(1):262. https:// doi. org/ \n10. 1186/ s12911- 021- 01623-6.\n 17. Datta S, Ulinski M, Godfrey-Stovall J, et al. Rad-spatialnet: a frame-based \nresource for fine-grained spatial relations in radiology reports. LREC Int \nConf Lang Resour Eval. 2020;2020:2251–60.\n 18. Banerjee I, Ling Y, Chen MC, et al. Comparative effectiveness of con-\nvolutional neural network (CNN) and recurrent neural network (RNN) \narchitectures for radiology text report classification. Artif Intell Med. \n2019;97:79–88. https:// doi. org/ 10. 1016/j. artmed. 2018. 11. 004.\n 19. Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need. Adv \nNeural Inf Process Systems, 2017,30.\n 20. Gao S, Qiu JX, Alawad M, et al. Classifying cancer pathology reports with \nhierarchical self-attention networks. Artif Intell Med. 2019;101: 101726. \nhttps:// doi. org/ 10. 1016/j. artmed. 2019. 101726.\n 21. Devlin J, Chang M, Lee K, et al. Bert: Pre-training of deep bidirectional \ntransformers for language understanding. arXiv preprint arXiv: 1810. \n04805, 2018.\n 22. Han X, Zhang Z, Ding N, et al. Pre-trained models: past, present and \nfuture. AI Open. 2021;2:225–50. https:// doi. org/ 10. 1016/j. aiopen. 2021. 08. \n002.\n 23. Qiu X, Sun T, Xu Y, et al. Pre-trained models for natural language process-\ning: a survey. Science China Technol Sci. 2020;63(10):1872–97.\n 24. Liu Y, Ott M, Goyal N, et al. Roberta: a robustly optimized bert pretraining \napproach. arXiv preprint arXiv: 1907. 11692, 2019.\n 25. Lan Z, Chen M, Goodman S, et al. Albert: A lite bert for self-supervised \nlearning of language representations. arXiv preprint arXiv: 1909. 11942, \n2019.\n 26. Sun Y, Wang S, Li Y, et al. Ernie: Enhanced representation through knowl-\nedge integration. arXiv preprint arXiv: 1904. 09223, 2019.\n 27. Huang K, Altosaar J, Ranganath R. Clinicalbert: Modeling clinical notes \nand predicting hospital readmission. arXiv preprint arXiv: 1904. 05342, \n2019.\n 28. Lee J, Yoon W, Kim S, et al. BioBERT: a pre-trained biomedical language \nrepresentation model for biomedical text mining. Bioinformatics. \n2020;36(4):1234–40.\n 29. Cui Y, Che W, Liu T, et al. Pre-training with whole word masking for Chi-\nnese Bert. IEEE/ACM Trans Audio Speech Lang Process. 2021;29:3504–14. \nhttps:// doi. org/ 10. 1109/ TASLP . 2021. 31243 65.\n 30. Xiang B, Yang C, Li Y, et al. CLiMP: a benchmark for Chinese language \nmodel evaluation. arXiv preprint arXiv: 2101. 11131, 2021.\n 31. Wang B, Pan B, Li X, et al. Towards evaluating the robustness of chinese \nbert classifiers. arXiv preprint arXiv: 2004. 03742, 2020.\n 32. Soffer S, Glicksberg BS, Zimlichman E, et al. BERT for the processing of \nradiological reports: an attention-based natural language processing \nalgorithm. Acad Radiol. 2022;29(4):634–5.\n 33. Carrodeguas E, Lacson R, Swanson W, et al. Use of Machine learning \nto identify follow-up recommendations in radiology reports. J Am Coll \nRadiol JACR. 2019;16(3):336–43. https:// doi. org/ 10. 1016/j. jacr. 2018. 10. 020.\n 34. Heilbrun ME, Chapman BE, Narasimhan E, et al. Feasibility of natural lan-\nguage processing-assisted auditing of critical findings in chest radiology. \nJ Am Coll Radiol JACR. 2019;16(9):1299–304. https:// doi. org/ 10. 1016/j. jacr. \n2019. 05. 038.\n 35. Lou R, Lalevic D, Chambers C, et al. Automated detection of radiol-\nogy reports that require follow-up imaging using natural language \nprocessing feature engineering and machine learning classifica-\ntion. J Digit Imaging. 2020;33(1):131–6. https:// doi. org/ 10. 1007/ \ns10278- 019- 00271-7.\n 36. Gershanik EF, Lacson R, Khorasani R. Critical finding capture in the impres-\nsion section of radiology reports. AMIA Symp. 2011;2011:465–9.\n 37. Morioka C, Meng F, Taira R, et al. Automatic classification of ultrasound \nscreening examinations of the abdominal aorta. J Digital Imaging. \n2016;29(6):742–8.\n 38. Fu S, Leung LY, Wang Y, et al. Natural language processing for the iden-\ntification of silent brain infarcts from neuroimaging reports. JMIR Med \nInform. 2019;7(2):e12109. https:// doi. org/ 10. 2196/ 12109.\n 39. Nakamura Y, Hanaoka S, Nomura Y, et al. Automatic detection of action-\nable radiology reports using bidirectional encoder representations from \ntransformers. BMC Med Inform Decision Mak. 2021;21(1):262. https:// doi. \norg/ 10. 1186/ s12911- 021- 01623-6.\n 40. Jujjavarapu C, Pejaver V, Cohen TA, et al. A Comparison of natural lan-\nguage processing methods for the classification of lumbar spine imaging \nfindings related to lower back pain. Acad Radiol. 2022;29(3):S188–200. \nhttps:// doi. org/ 10. 1016/j. acra. 2021. 09. 005.\n 41. Zhang H, Hu D, Duan H, et al. A novel deep learning approach to extract \nChinese clinical entities for lung cancer screening and staging. BMC Med \nInform Decision Making. 2021;21(Suppl 2):214. https:// doi. org/ 10. 1186/ \ns12911- 021- 01575-x.\n 42. Zaman S, Petri C, Vimalesvaran K, et al. Automatic diagnosis labeling of \ncardiovascular mri by using semisupervised natural language processing \nof text reports. Radiol Artif Intell. 2022;4(1):e210085. https:// doi. org/ 10. \n1148/ ryai. 210085.\n 43. Liu F, Zhou P , Baccei SJ, et al. qualifying certainty in radiology reports \nthrough deep learning-based natural language processing. AJNR Am J \nNeuroradiol. 2021;42(10):1755–61. https:// doi. org/ 10. 3174/ ajnr. A7241.\n 44. Cima R, Mazurek B, Haider H, et al. A multidisciplinary European guideline \nfor tinnitus: diagnostics, assessment, and treatment. HNO. 2019;67(Suppl \n1):10–42. https:// doi. org/ 10. 1007/ s00106- 019- 0633-7.\n 45. Mosbach M, Andriushchenko M, Klakow D. On the stability of fine-tuning \nbert: Misconceptions, explanations, and strong baselines. arXiv preprint \narXiv: 2006. 04884, 2020.\n 46. Cui Y, Che W, Liu T, et al. Revisiting pre-trained models for Chinese natural \nlanguage processing. arXiv preprint arXiv: 2004. 13922, 2020.\n 47. Zhang Z, Zhang H, Chen K, et al. Mengzi: towards lightweight yet ingen-\nious pre-trained models for Chinese. arXiv preprint arXiv: 2110. 06696, \n2021.\n 48. Sun C, Qiu X, Xu Y, et al. How to fine-tune bert for text classification? In: \nChina national conference on Chinese computational linguistics, 2019. \nSpringer.\n 49. Brady AP . Radiology reporting-from Hemingway to HAL? Insights Imag-\ning. 2018;9(2):237–46. https:// doi. org/ 10. 1007/ s13244- 018- 0596-3.\n 50. Lu W, Jiao J, Zhang R. Twinbert: Distilling knowledge to twin-structured \ncompressed bert models for large-scale retrieval. In: Proceedings of the \n29th ACM International Conference on Information & Knowledge Man-\nagement, 2020.\n 51. Hardavella G, Aamli-Gaagnat A, Frille A, et al. Top tips to deal with \nchallenging situations: doctor-patient interactions. Breathe (Sheff ). \n2017;13(2):129–35. https:// doi. org/ 10. 1183/ 20734 735. 006616.\n 52. Gregory W. Rutecki. Tinnitus recommendations: what to do when there is \nringing in the Ears. Consultant. 2016;56(11):1036.\n 53. Masino AJ, Grundmeier RW, Pennington JW, et al. Temporal bone radiol-\nogy report classification using open source machine learning and natural \nlangue processing libraries. BMC Med Inform Decis Mak. 2016;16:65. \nhttps:// doi. org/ 10. 1186/ s12911- 016- 0306-3.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in pub-\nlished maps and institutional affiliations."
}