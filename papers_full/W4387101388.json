{
  "title": "PMVT: a lightweight vision transformer for plant disease identification on mobile devices",
  "url": "https://openalex.org/W4387101388",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2105889206",
      "name": "Guoqiang Li",
      "affiliations": [
        "Henan Academy of Agricultural Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2098582533",
      "name": "Yuchao Wang",
      "affiliations": [
        "Henan Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2095722350",
      "name": "Qing Zhao",
      "affiliations": [
        "Henan Academy of Agricultural Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2115341779",
      "name": "Peiyan Yuan",
      "affiliations": [
        "Henan Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2221998313",
      "name": "Baofang Chang",
      "affiliations": [
        "Henan Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2105889206",
      "name": "Guoqiang Li",
      "affiliations": [
        "Henan Academy of Agricultural Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2098582533",
      "name": "Yuchao Wang",
      "affiliations": [
        "Henan Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2095722350",
      "name": "Qing Zhao",
      "affiliations": [
        "Henan Academy of Agricultural Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2115341779",
      "name": "Peiyan Yuan",
      "affiliations": [
        "Henan Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2221998313",
      "name": "Baofang Chang",
      "affiliations": [
        "Henan Normal University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4282974753",
    "https://openalex.org/W3167628032",
    "https://openalex.org/W3195133177",
    "https://openalex.org/W6799772243",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W6790690058",
    "https://openalex.org/W3089559094",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W6762585180",
    "https://openalex.org/W2339460098",
    "https://openalex.org/W4394670654",
    "https://openalex.org/W4206706211",
    "https://openalex.org/W4306955484",
    "https://openalex.org/W6792155083",
    "https://openalex.org/W6810938606",
    "https://openalex.org/W4220947986",
    "https://openalex.org/W6753767121",
    "https://openalex.org/W4286910290",
    "https://openalex.org/W2969292408",
    "https://openalex.org/W2913500366",
    "https://openalex.org/W6784879909",
    "https://openalex.org/W4296896315",
    "https://openalex.org/W6762718338",
    "https://openalex.org/W4281846758",
    "https://openalex.org/W6788135285",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6753412334",
    "https://openalex.org/W6791943378",
    "https://openalex.org/W3120101602",
    "https://openalex.org/W4210370376",
    "https://openalex.org/W6805224724",
    "https://openalex.org/W4310454508",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3095117790",
    "https://openalex.org/W3133696297",
    "https://openalex.org/W3190492058",
    "https://openalex.org/W4214493665",
    "https://openalex.org/W2884585870",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W4312820606",
    "https://openalex.org/W2883780447"
  ],
  "abstract": "Due to the constraints of agricultural computing resources and the diversity of plant diseases, it is challenging to achieve the desired accuracy rate while keeping the network lightweight. In this paper, we proposed a computationally efficient deep learning architecture based on the mobile vision transformer (MobileViT) for real-time detection of plant diseases, which we called plant-based MobileViT (PMVT). Our proposed model was designed to be highly accurate and low-cost, making it suitable for deployment on mobile devices with limited resources. Specifically, we replaced the convolution block in MobileViT with an inverted residual structure that employs a 7×7 convolution kernel to effectively model long-distance dependencies between different leaves in plant disease images. Furthermore, inspired by the concept of multi-level attention in computer vision tasks, we integrated a convolutional block attention module (CBAM) into the standard ViT encoder. This integration allows the network to effectively avoid irrelevant information and focus on essential features. The PMVT network achieves reduced parameter counts compared to alternative networks on various mobile devices while maintaining high accuracy across different vision tasks. Extensive experiments on multiple agricultural datasets, including wheat, coffee, and rice, demonstrate that the proposed method outperforms the current best lightweight and heavyweight models. On the wheat dataset, PMVT achieves the highest accuracy of 93.6% using approximately 0.98 million (M) parameters. This accuracy is 1.6% higher than that of MobileNetV3. Under the same parameters, PMVT achieved an accuracy of 85.4% on the coffee dataset, surpassing SqueezeNet by 2.3%. Furthermore, out method achieved an accuracy of 93.1% on the rice dataset, surpassing MobileNetV3 by 3.4%. Additionally, we developed a plant disease diagnosis app and successfully used the trained PMVT model to identify plant disease in different scenarios.",
  "full_text": "PMVT: a lightweight vision\ntransformer for plant disease\nidentiﬁcation on mobile devices\nGuoqiang Li1, Yuchao Wang2,3, Qing Zhao1, Peiyan Yuan2,3\nand Baofang Chang2,3*\n1Institute of Agricultural Economics and Information, Henan Academy of Agricultural Sciences,\nZhengzhou, Henan, China,2College of Computer and Information Engineering, Henan Normal\nUniversity, Xinxiang, Henan, China,3Key Laboratory of Artiﬁcial Intelligence and Personalized Learning\nin Education of Henan Province, Xinxiang, Henan, China\nDue to the constraints of agricultural computing resources and the diversity of\nplant diseases, it is challenging to achieve the desired accuracy rate while\nkeeping the network lightweight. In this paper, we proposed a computationally\nefﬁcient deep learning architecture based on the mobile vision transformer\n(MobileViT) for real-time detection of plant diseases, which we called plant-\nbased MobileViT (PMVT). Our proposed model was designed to be highly\naccurate and low-cost, making it suitable for deployment on mobile devices\nwith limited resources. Speci ﬁcally, we replaced the convolution block in\nMobileViT with an inverted residual structure that employs a 7×7 convolution\nkernel to effectively model long-distance dependencies between different leaves\nin plant disease images. Furthermore, inspired by the concept of multi-level\nattention in computer vision tasks, we integrated a convolutional block attention\nmodule (CBAM) into the standard ViT encoder. This integration allows the\nnetwork to effectively avoid irrelevant information and focus on essential\nfeatures. The PMVT network achieves reduced parameter counts compared to\nalternative networks on various mobile devices while maintaining high accuracy\nacross different vision tasks. Extensive experiments on multiple agricultural\ndatasets, including wheat, coffee, and rice, demonstrate that the proposed\nmethod outperforms the current best lightweight and heavyweight models.\nOn the wheat dataset, PMVT achieves the highest accuracy of 93.6% using\napproximately 0.98 million (M) parameters. This accuracy is 1.6% higher than that\nof MobileNetV3. Under the same parameters, PMVT achieved an accuracy of\n85.4% on the coffee dataset, surpassing SqueezeNet by 2.3%. Furthermore, out\nmethod achieved an accuracy of 93.1% on the rice dataset, surpassing\nMobileNetV3 by 3.4%. Additionally, we developed a plant disease diagnosis app\nand successfully used the trained PMVT model to identify plant disease in\ndifferent scenarios.\nKEYWORDS\nplant disease identi ﬁcation, vision transformer, lightweight model, attention\nmodule, APP\nFrontiers inPlant Science frontiersin.org01\nOPEN ACCESS\nEDITED BY\nCe Yang,\nUniversity of Minnesota Twin Cities,\nUnited States\nREVIEWED BY\nCongliang Zhou,\nUniversity of Florida, United States\nLingxian Zhang,\nChina Agricultural University, China\n*CORRESPONDENCE\nBaofang Chang\nchangbaofang@htu.edu.cn\nRECEIVED 11 July 2023\nACCEPTED 08 September 2023\nPUBLISHED 26 September 2023\nCITATION\nLi G,Wang Y,Zhao Q,Yuan P andChang B\n(2023) PMVT: a lightweight vision\ntransformer for plant disease\nidentiﬁcation on mobile devices.\nFront. Plant Sci.14:1256773.\ndoi: 10.3389/fpls.2023.1256773\nCOPYRIGHT\n© 2023 Li, Wang, Zhao, Yuan and Chang.\nThis is an open-access article distributed\nunder the terms of theCreative Commons\nAttribution License (CC BY).The use,\ndistribution or reproduction in other\nforums is permitted, provided the original\nauthor(s) and the copyright owner(s) are\ncredited and that the original publication in\nthis journal is cited, in accordance with\naccepted academic practice. No use,\ndistribution or reproduction is permitted\nwhich does not comply with these terms.\nTYPE Methods\nPUBLISHED 26 September 2023\nDOI 10.3389/fpls.2023.1256773\n1 Introduction\nPlant disease is one of the contributing factors to the global\ndecrease in grain production ( Savary et al., 2019 ), and real-time\ndetection of plant disease has an important impact on the\nagricultural industry. Applying deep learning models signi ﬁcantly\nsimpliﬁes the entire process and enables end-to-end technical\nservices. Currently, there are two typical architectures for plant\ndisease recognition: convolutional neural network (CNN)-based\narchitectures and vision transformer (ViT)-based architectures.\nThese methods extract explicit features from images and\nautomatically perform classi ﬁcation, which is key for plant\ndisease recognition.\nOver the past few years, the application of CNNs to\nidentifying plant diseases has gained in popularity with the\ndevelopment of arti ﬁcial intelligence technology. For instance,\nAkshai and Anitha (2021) compared various CNNs using the\nPlantVillage dataset ( Hughes and Salathe 2015 )a n dr e p o r t e d\nthat the DenseNet model with feature map reuse achieved the\nhighest accuracy of 98.27%. Another study by Yu et al. (2022a)\nused a ResNet network with a residual structure to identify apple\nleaf diseases, and it obtained a n average F1-score of 95.70%.\nCNNs can ef ﬁciently extract signi ﬁcant features from images and\naccomplish plant disease identi ﬁcation automatically. The\nprimary reason for this is that CNNs have the characteristic of\nparameter sharing, which reduces the number of parameters in\nthe model and addresses the over ﬁtting issue seen in computer\nvision tasks. Therefore, the application of deep learning\ntechnology based on CNNs has made signi ﬁcant progress in\nplant disease diagnosis ( Hasan et al., 2020 ; Xiong et al., 2021 ;\nAhmad et al., 2022 ). Nonetheless, there will be an increase in\nunnecessary computational overhead as a network ’sd e p t h\nincreases. Simultaneously, the convolutional layer of CNNs\nonly considers the characteristics of the local area during\nconvolution and does not explicitly incorporate the positional\ninformation of pixels. This will im pact the effectiveness of a plant\ndisease identi ﬁcation model.\nTo address the above issues, Dosovitskiy et al. (2020) proposed\na vision transformer (ViT) architecture based on a self-attention\nmechanism ( Vaswani et al., 2017 ) to replace the traditional CNN\nfor image recognition. A ViT architecture divides an image into\nnon-overlapping patches and applies multi-head self-attention\nwithin the transformer encoder to learn representations of\npatches. Although this paradigm considers the global relationship\nof images and has achieved satisfactory results in plant disease\nrecognition, it usually requires a large quantity of training data to\nachieve relatively high accuracy. Hence, alternating the use of CNNs\nand ViTs to extract more comprehensive features has become a\nbetter choice in plant disease diagnosis. Take a classic case: Lu et al.\n(2022) introduced a ghost module into the ViT encoder, which\nextracts different levels of features in an image. Their model\nachieved an accuracy rate of 98 .14% in detecting grape leaf\ndiseases and insect pests in the ﬁeld. Similarly, Yu et al. (2023)\nused inception blocks to enhance the ability of the ViT encoder to\nextract local information; they achieved optimal performance on\nfour typical plant disease datasets. As an alternative architectural\nparadigm to CNNs, the ViT has attracted signi ﬁcant attention and\nachieved considerable success in the ﬁeld of computer vision ( Khan\net al., 2022 ; Lin et al., 2022 ).\nWith the signi ﬁcant advancements of CNNs and ViT networks\nin plant disease recognition technology, a prevailing trend among\nnetwork models is to augment the number of parameters in order to\nenhance performance. These enhancements in performance are\naccompanied by an increase in model size (network parameters)\nand latency ( Han et al., 2021 ; Wu et al., 2021 ; Yu et al., 2022b ). They\noverlook a common issue: plant disease identi ﬁcation is typically\nconducted on edge devices, such as smartphones and embedded\ndevices. Such devices usually have restricted computing power,\nstorage capacity, and energy supply. Hence, using a lightweight\nnetwork can decrease the size and computational complexity of the\nmodel, thereby improving its compatibility with resource\nconstraints. Numerous researchers have recently been studying\nthe application of affordable network models for real-time plant\ndisease detection. Concretely, Bao et al. (2021) proposed SimpleNet,\nwhich achieved 94.10% wheat recognition accuracy with only 2.13\nmillion (M) parameters. In addition, the apple leaf disease\nidentiﬁcation method based on the cascade backbone network\n(CBNet) proposed by ( Sheng et al., 2022 ) achieved an accuracy\nrate of 96.76%. Moreover, the VGG-ICNN model proposed by\nThakur et al. (2023) has 6 M parameters, which is lower than most\ndeep learning models; and it performs well on multiple datasets\nsuch as apple, corn, and rice. Generally, the methods mentioned\nabove primarily concentrate on identifying a single plant disease,\nwhile other methods exhibit imbalances in identi ﬁcation accuracy\nand calculation cost. Hence, to enhance the real-time performance\nof plant disease identi ﬁcation, it is crucial to employ a low-latency\nand highly accurate network model.\nAchieving high-accuracy and low-cost plant disease\nidenti ﬁcation in agricultural environments with limited\ncomputing resources presents a signi ﬁcant challenge. The\nmajority of existing lightweight networks focus on a single plant\ndisease. However, when faced with numerous types of plant\ndiseases, they fail to deliver sa tisfactory performance. In this\npaper, we introduced a lightweight model for plant disease\ndiagnosis based on MobileViT ( Mehta and Rastegari, 2021 ),\nwhich has a low computational cost and is competitive in terms\nof inference speed. In particular, the crisscrossing leaves in the\nagricultural dataset lead to an unsatisfactory recognition effect with\nMobileViT. Thus, we consider using a larger convolution kernel (7\n× 7) to analyze the connection between different leaves. Using larger\nconvolution kernels allows us to model the dependencies between\nlong-distance pixels ( Liu et al., 2021 ; Liu et al., 2022 ) and enhance\nthe ability of the model to capture global information from plant\ndisease images. Additionally, focusing on the salient leaf regions in\nplant images can improve the robustness of the model. We used the\nCBAM ( Woo et al., 2018 ) to adjust feature weights in various\nchannels of the transformer encoder. Finally, we employed a\nresidual network to fuse the initial feature map and improve the\nﬁtting ability of the model. We named this model plant-based\nMobileViT (PMVT) and deployed it to identify plant diseases in\ndatasets and in various scenarios. Experimental results indicate that\nPMVT surpasses the current leading lightweight networks and\nLi et al. 10.3389/fpls.2023.1256773\nFrontiers inPlant Science frontiersin.org02\nheavyweight models, thereby demonstrating its effectiveness as a\nversatile backbone network across various datasets.\nThe main contributions of this paper are as follows.\n We used a low-cost ViT model for plant disease diagnosis.\nThis model is computationally ef ﬁcient and can function as\na generic backbone network on mobile devices.\n We introduced a 7 × 7-sized convolution kernel into the\nconvolution block for modeling long-distance pixel-to-pixel\ndependencies. Moreover, the CBAM guides the network to\nlearn the weights between various channels, which\nenhances the ﬁtting ability of MobileViT to image feature\nrepresentation.\n We conducted comparative experiments on several datasets\nobtained under different scenarios, and the results revealed\nthat our method not only competes with similarly sized\nlightweight networks but also outperforms state-of-the-art\nheavyweight networks.\n2 Materials and methods\n2.1 Datasets\nWe randomly divided three datasets into a training set,\nvalidation set, and testing set according to the ratio of 8:1:1.\nTable 1 shows the details of each dataset and how many samples\ncomprised each subset. Figure 1 displays some samples of\nthe datasets.\n2.1.1 Wheat\nThe wheat (Lian, 2022) dataset comprises 4087 images of varying\nsizes depicting seven different categories of wheat diseases. The\nimages include the real-world environmental factors that interfere\nwith identifying the wheat crop, such as sky, soil, and weeds.\n2.1.2 Coffee\nThe coffee ( Parraga-Alava et al., 2019 ) dataset contains three\ntypes of coffee leaves: healthy, red spider mite, and rust. Images of\nthe same size and resolution are included in each category of\nleaves. The dataset was collected in a natural ﬁeld environment,\nwhere the background of the pictures contains various\ndisturbances such as weeds and soil. Since some sample features\nare not signi ﬁcant enough, we selected a thousand of them to build\nan e wd a t a s e t .\n2.1.3 Rice\nThe rice ( Sethy, 2020 ) dataset lends itself to the classical binary\nclassiﬁcation problem as it contains samples classi ﬁed simply as\neither healthy or unhealthy rice. The resolution of the images in this\ndataset varies in size. Furthermore, some of the images in this\ndataset have a uniform white background, which makes the dataset\nideal for testing model performance in both a controlled laboratory\nenvironment and a real ﬁeld environment.\n2.2 Our proposed method\n2.2.1 Overall structure of PMVT\nFigure 2 depicts the overall structure of our model, which\ncomprises ﬁve layers. Before pushing input into the block, the\nfeature map is downsampled using a 3 × 3 convolution; this is\nfollowed by an inverted residual block or a standard transformer\nencoder. The inverted residual block is used to extract local features\nof the image and capture the long-distance dependencies between\ndistant pixels. The MobileViT block uses a self-attention\nmechanism to model the global relationship of the image and\nemploys a CBAM block to make up the channel attention and\nspatial attention information. The channel dimension is expanded\nby four times using a 1 × 1 convolution in the last layer of the\nnetwork to better adapt to computer classi ﬁcation tasks. PMVT\ncontains three different network sizes: extra extra small (XXS); extra\nTABLE 1 Data distributions for the datasets used in our comparative experiments.\nName Class Diseases Training set size Validation set size Testing set size\nWheat 0 health 528 65 59\n1 rust 673 83 77\n2 mildew 282 34 32\n3 smut 674 83 75\n4 root rot 381 46 41\n5 scab 391 48 45\n6 leaf spot 378 47 45\nCoffee 0 healthy red 353 43 39\n1 spider mite 136 16 15\n2 rust 324 39 35\nRice 0 healthy 407 50 44\n1 unhealthy 413 50 43\nLi et al. 10.3389/fpls.2023.1256773\nFrontiers inPlant Science frontiersin.org03\nsmall (XS); and small (S)). These sizes correspond to those\nin MobileViT.\n2.2.2 Inverted residual block\nAn inverted residual block is a standard convolutional structure\ncomprising three convolution kernels. Before extracting image\nfeatures, a 1 × 1 convolution kernel is used to increase the\nchannel dimension, generally by two times. Then, we replace the\n3 × 3 convolution kernel of the original MobileViT with a 7 × 7\nconvolution kernel, thus making it easier to capture long-distance\ndependencies between pixels. In addition, depthwise separable\nconvolutions are used to reduce the computational complexity of\nthe model and increase the inference speed. Finally, we use a 1 × 1\nconvolution kernel to restore the channel dimension of the image.\nFigure 3 shows the overall structure of the inverted residual block.\n2.2.3 Mobile ViT block\nAs described in Figure 4A , learning global representations of\nfeature maps using 1 × 1 and 3 × 3 convolutions. Before entering the\nstandard transformer encoder, the same color patch at the same\nposition is taken out and put into the same sequence for self-\nattention calculation. This measure allows us to learn the global\nrepresentation information of the image in a more blocky manner\nand reduce the computational cost of the self-attention mechanism.\nThrough the 1 × 1 convolution kernel, the output of the transformer\nis restored to the original channel dimension, and the channel\nattention and spatial attention information are learned through the\nCBAM block. Finally, the obtained feature map is spliced with the\noriginal feature map to prevent loss of feature information and is\nthen input to the next stage after a 3 × 3 convolution.\n2.2.4 Vision transformer encoder\nAs shown in Figure 4B, the encoder used to learn image features\nconsisting of standard transformer blocks. First, an image with\ndimensions [C ,H,W] is divided into patches of P size, and a linear\ntransformation is applied to each patch for ﬂattening. Positional\nencoding information is then applied to each patch; through this,\neach patch then has dimensions of ½H\nPi\n, W\nPi\n, C/C138 . Next, we use three\nlearnable parameter matrices to multiply each patch to get queries\n(WQ), keys ( WK), and values ( WV). For patch i, we apply the dot\nproduct to the query matrix with the key matrix of the remaining\npatches, and then we divide by the number of key matrix elements.\nFinally, we apply the softmax function to obtain the attention scores\nof the remaining patches for patch i. These attention scores are\nFIGURE 2\nOverview of the PMVT model.↓2 means to downsample the feature map twice, and L stands for repeated stacking of L MobileViT blocks. For\ncomputer vision classiﬁcation tasks, we use a classiﬁer composed of an average pooling layer and a fully connected layer.\nB\nC\nA\nFIGURE 1\nSample images from the(A) wheat dataset,(B) coffee dataset, and(C) rice dataset.\nLi et al. 10.3389/fpls.2023.1256773\nFrontiers inPlant Science frontiersin.org04\nmultiplied by the value matrix of patch i to obtain the feature\ninformation. Equation 1 illust r a t e st h ep r o c e s so ft h ee n t i r e\nattention mechanism. MLP comprises two fully connected layers\nand employs an incentive compression mechanism to learn\ninteraction information between different dimensions.\nself − attention = soft max ( QKT\nﬃﬃﬃﬃ ﬃ\ndk\np )  /C2  V (1)\n2.2.5 CBAM block\nThe CBAM block is composed of a channel attention module\nand a spatial attention module, and it uses a 3 × 3 convolution\nkernel to preprocess the feature map before insertion. We pass the\ninput feature map through a parallel average pooling layer and max\npooling layer, and then we change the feature map from [C ,H,W]t o\n[C,1,1] dimensions. The shared MLP module comprises two 1 × 1\nconvolution kernels, which compress the number of channels to R\ntimes the original number and then expand it back to the original\nnumber of channels. The feature maps obtained by the average\npooling layer and the max pooling layer are spliced to obtain the\nweights of each channel, which are ﬁnally multiplied by the original\nfeature map. Equation 2 describes the weight assignment process of\nthe channel attention module. s stands for using Sigmoid as the\nactivation function, W 1 ∈ RC/r×C, and W 1 ∈ RC/r×C.W 1 and W0 are\nshared weights for the two inputs of the max pooling layer and the\naverage pooling layer.\nMc(F)= s(MLP(AvgPool(F)) + MLP(MaxPool(F)))\n= s(W1(W0(Fc\navg )) + W1(W0(Fc\nmax)))         \n(2)\nThe output of the channel attention module is obtained through\nthe max pooling layer and average pooling layer. We acquire two\nB\nC\nA\nFIGURE 4\nDetailed description of the vision transformer block.(A) The overall structure of the vision transformer block;(B) the structure of the vision\ntransformer block encoder; and(C) the architecture of the CBAM block, where⊗ represents the multiplication with the original feature map.\nFIGURE 3\nStructure of the inverted residual block. C× represents the feature\ninformation obtained by convolving each channel of the feature\nmap using a convolution kernel.\nLi et al. 10.3389/fpls.2023.1256773\nFrontiers inPlant Science frontiersin.org05\nfeature maps with dimensions of [1 ,H,W], and then we splice them.\nThrough a 7 × 7 convolution, we obtain a feature map of one\nchannel and multiply it by the original feature map. Equation 3\nshows the forward process of the spatial attention module, while\nFigure 4C shows the forward process of the entire CBAM block.\nMs(F)= s(f 7/C2 7(½AvgPool(F); MaxPool(F)/C138 ))\n= s(f 7/C2 7(½Fs\navg ; Fs\nmax/C138 ))                \n(3)\n2.3 App for plant disease identiﬁcation\nWe export the trained model to an open neural network\nexchange (ONNX) ﬁle format to preserve crucial details such as\nstructure and weights. The model is converted into an NCNN ﬁle\nformat for storage to facilitate deployment on a mobile terminal for\ninference because the NCNN format is a high-performance neural\nnetwork inference framework o ptimized for mobile platforms.\nSubsequently, the structure and weight information of the model\nare extracted for plant disease identi ﬁcation using the C++\nlanguage. The XML language is used to de ﬁne the layout and\nappearance of the application fro nt-end interface. Lastly, the\nback-end interaction of the application is developed using the\nJAVA language, while the MySQL database is used for storing\nplant diseases and related information. As shown in Figure 5 , the\napp possesses the capability to perform photo identi ﬁcation using\nthe camera of the device ( Figure 5B). Alternatively, it allows users to\nselect pictures from their album for identi ﬁcation ( Figure 5C ).\nFurthermore, users have the option to search for plant diseases\nbased on speci ﬁc conditions or criteria ( Figure 5D). The application\nthen presents the relevant categories of plant diseases based on the\nselected pictures or conditions. Figure 5E displays the ﬁnal\nidentiﬁcation results of plant dise ases and the corresponding\ncontrol methods.\n2.4 Experimental details\nData augmentation has been shown to improve model\nrobustness and generalization. Before training the network, all\nimages are uniformly resized to 224 × 224. The samples in the\ntraining, validation, and test sets are randomly rotated and cropped\nalong the center. Finally, we normalize all images using standard\ndeviation and mean square deviation. Table 2 describes our\nhyperparameter settings for model training.\n2.5 Model evaluation\nIn this study, we use top-1 accuracy (Equation 4) to determine\nthe highest accuracy that the model can achieve. We also use\nprecision (Equation 5) and recall (Equation 6) to evaluate the\nperformance of the model. Parameters, ﬂoating point operations\nper second (FLOPs), and frames per second (FPS; the number of\nimages the model processes per second) are used to express the\nBCDEA\nFIGURE 5\nIntroduction of plant disease identiﬁcation app.(A) the main page of the app;(B) the page for camera recognition;(C) the page to select local\nalbums for recognition;(D) the page for disease search; and(E) the page displaying disease identiﬁcation results.\nTABLE 2 Hyperparameter settings for training.\nName Value Description\nEpochs 100 Number of times the model was trained\nBatch size 32 Number of samples selected for one training\nOptimizer AdamW Tool used to bootstrap network update parameters\nLearning rate 0.0001 Tunes parameters in optimization algorithms\nLoss function Cross Entropy Evaluates the gap between the predicted value and the true value\nLi et al. 10.3389/fpls.2023.1256773\nFrontiers inPlant Science frontiersin.org06\ninference speed of the model. True positive (TP) means that the\npredicted positive sample is actually a positive sample; false positive\n(FP) indicates that the predicted positive sample is actually a\nnegative sample; true negative (TN) means that the predicted\nnegative sample is actually a negative sample; and false negative\n(FN) means that the predicted negative sample is actually a positive\nsample.\nTop − 1 Accuracy = TP + TN\nTP + TN + EP + FN (4)\nPr ecision = TP\nTP + FP (5)\nRe call = TP\nTP + FN (6)\n2.6 Experimental setup\nAll experiments run on a deep learning – based cloud platform.\nThe hardware con ﬁguration is a 14-Core VV Intel(R) Xeon(R) Gold\n6330 CPU @ 2.00 GHz, with 45 GB of RAM and an NVIDIA\nGeForce RTX 3090 GPU. The operating system is Ubuntu 18.04,\nand PyTorch 1.9.0 and Python 3.8 are used as software support.\n3 Results and conclusions\n3.1 Results\nWe selected several typically used CNN-based and ViT-based\nnetworks for comparison with our model. These include lightweight\nnetworks such as SqueezeNet ( Iandola et al., 2016 ), Shuf ﬂeNetV2\n(Ma et al., 2018 ), MobileNetV3 ( Howard et al., 2019 ),\nMobileFormer ( Chen et al., 2022 ), Ef ﬁcientNet ( Tan and Le,\n2019), and Deit ( Touvron et al., 2021 ) models. We also chose\nmany heavyweight networks such as PoolFormer ( Yu et al., 2022b ),\nCVT ( Wu et al., 2021 ), TNT ( Han et al., 2021 ), and ResNet ( He\net al., 2016 ) for comparison. Additionally, we chose a wheat dataset\nwith multiple components (such as roots, stems, and leaves) to\nevaluate model performance on images depicting diverse\nconditions. The coffee dataset was employed to assess the\nperformance of our method when confronted with complex\nbackgrounds. Moreover, the rice dataset was used to investigate\nthe classical binary classi ﬁcation problem.\nWe chose the wheat dataset to verify the generalizability of\nPMVT under a real crop growth cycle. We can see from Table 3 that\nour proposed network achieved the best top-1 accuracy when\ncompared with networks with similar parameters. Among the\nlightweight networks, MobileNetV3 achieved an accuracy rate of\nTABLE 3 Comparison of the PMVT model with other backbone models on three datasets (the FPS indicator is calculated on the desktop computer,\nand bold text highlights the best-performing network).\nMethods Top-1 Accuracy(%) Parameters (M) FLOPs (G) FPS (img/s)\nWheat Coffee Rice\nSqueezeNet-1.0 70.0 79.7 86.2 0.74 0.73 293.0\nSqueezeNet-1.1 86.1 83.1 85.1 0.73 0.26 311.5\nShufﬂeNetV2-1.0 89.6 68.5 82.7 1.27 0.15 151.9\nMobileNetV3-Small 92.0 66.3 89.7 1.54 0.06 170.2\nPMVT-XXS (ours) 93.6 85.4 93.1 0.98 0.31 88.5\nShufﬂeNetV2-1.5 92.5 73.0 86.2 2.50 0.31 148.4\nMobileFormer-26M 91.4 77.5 90.8 2.22 0.03 53.1\nMobileFormer-52M 92.8 79.2 83.9 2.46 0.05 60.7\nMobileFormer-96M 92.8 84.2 87.3 3.33 0.09 58.8\nMobileNetV3-Large 92.8 72.0 91.9 4.22 0.23 141.0\nEfﬁcientNet-B0 94.1 84.2 88.5 4.03 0.41 109.9\nPMVT-XS (ours) 94.7 86.5 97.7 2.01 0.85 85.3\nShufﬂeNetV2-2.0 93.6 70.0 91.4 5.38 0.60 146.2\nMobileFormer-151M 94.4 75.3 88.5 6.34 0.10 42.3\nEfﬁcientNet-B1 94.4 79.8 90.8 6.53 0.61 75.3\nEfﬁcientNet-B2 93.3 83.1 87.3 7.72 0.70 76.6\nDeit-Tiny 91.4 78.7 84.0 5.49 1.08 161.7\nPoolFormer-S12 91.4 85.4 85.1 11.39 1.81 178.3\n(Continued)\nLi et al. 10.3389/fpls.2023.1256773\nFrontiers inPlant Science frontiersin.org07\n92.0%, whereas Ef ﬁcientNet-B0 achieved a higher accuracy rate of\n94.1%. Our PMVT reached state-of-the-art accuracy with rates of\n93.6 and 94.7, respectively. In comparing heavyweight networks, the\nPMVT model achieved an accuracy rate of 94.9% using only 5.06 M\nparameters, outperforming ResNet-101, which achieved an\naccuracy of 94.1% but used 42.5 M parameters. This proves that\nthe proposed model is effective compared to the original\nMobileViT. Figure 6 presents the confusion matrix of our\nproposed model. Figure 7 depicts the precision of the PMVT\nmodel, while Figure 8 illustrates its recall.\nThe coffee dataset was used to compare the performance of the\nPMVT models in the ﬁeld environment. As can be seen from\nTable 3 , the traditional lightweight networks did not achieve\nacceptable accuracy rates. The XXS version of the PMVT model\nachieved a top-1 accuracy rate of 85.4%, which was 3.5% higher\nthan that of the SqueezeNet-1.1 model. Compared with the\nMobileFormer-96M model, the XS version of the PMVT model\nimproved accuracy by 2.3% to reach 86.5%. Finally, the S version of\nthe PMVT model achieved an accuracy rate of 87.6% on this\ndataset; this was an improvement of 2.2% over that obtained by\nthe PoolFormer-S12 model. Figures 9 and 10 present the confusion\nmatrix, precision, and recall of the PMVT model. It can be seen\nfrom the ﬁgures that our model does not achieve satisfactory results\nin identifying red spider mite diseases.\nWe applied the rice dataset to simultaneously testing the ﬁtting\nability of the PMVT model in a controlled laboratory environment\nand in a real natural condition. Surprisingly, the XS version of\nPMVT achieved 97.7% accuracy on this dataset, which was 5.8%\nhigher than the second-highe st accuracy (obtained by the\nMobileNetV3-large model). In addition, the XXS version attained\nan accuracy of 93.1%, which was 3.4% higher than the baseline of\nthe MobileNetV3-small model. The S version of the PMVT model\nperformed the worst, with an accuracy of 92%; however, it still\noutperformed the Shuf ﬂeNetV2-2.0 model with similar parameters\nFIGURE 6\nConfusion matrix of the PMVT model on the wheat dataset.\nTABLE 3 Continued\nMethods Top-1 Accuracy(%) Parameters (M) FLOPs (G) FPS (img/s)\nWheat Coffee Rice\nCVT-Tiny 93.6 82.0 86.2 19.63 4.08 62.2\nTNT-Small 92.8 80.9 88.5 23.40 4.85 67.3\nResNet50 93.9 70.8 90.8 23.53 4.13 125.1\nResNet101 94.1 63.0 88.5 42.50 7.86 66.3\nPMVT-S (ours) 94.9 87.6 92.0 5.06 1.59 81.3\nFIGURE 7\nPrecision of the PMVT model on the wheat dataset.\nLi et al. 10.3389/fpls.2023.1256773\nFrontiers inPlant Science frontiersin.org08\nby 0.6%. Upon comparing models with similar sizes, we found that\nthe PMVT model has achieved the best accuracy rate. This proved\nthat our model is very competitive on the classic binary\nclassiﬁcation problem. Figures 11 and 12 depict the confusion\nmatrix, precision, and recall of the PMVT model on the rice dataset.\nAs seen in Table 3 , our method does not excel in terms of FPS\nand FLOPs metrics. This because the self-attention mechanism\ncomputes the weights between image patches, resulting in\nnumerous matrix calculations and multiplication operations\nduring inference. Consequently, this increases the computational\ntime. Additionally, because of the current immaturity of deep\nlearning framework technology, numerous attention-weight\nmatrices must be stored and processed, thereby occupying a\nsigniﬁcant amount of memory. Nevertheless, PMVT achieves the\nbest accuracy with only 0.98M parameters. This makes it low-cost\nand high-accuracy for plant disease identi ﬁcation. As arti ﬁcial\nintelligence technology advances, ViT can be better applied to the\nvisual task of plant disease identi ﬁcation.\n3.2 Ablation studies\nThe data given in Table 4 , it demonstrates the effectiveness of\neach module in our models. +Conv7 × 7 represents using a\nconvolution kernel of size 7 instead of the 3 × 3 convolution in\nthe CNN block based on the MobileViT model. +CBAM uses\nFIGURE 8\nRecall of the PMVT model on the wheat dataset.\nFIGURE 9\nConfusion matrix of the PMVT model on the coffee dataset.\nFIGURE 10\nPrecision and recall of the PMVT model on the coffee dataset.\nLi et al. 10.3389/fpls.2023.1256773\nFrontiers inPlant Science frontiersin.org09\nchannel attention and spatial attention integrated in the ViT block\nbased on the MobileViT model. PMVT represents a new backbone\nnetwork built on the basis of MobileViT using both 7 × 7\nconvolution kernels and CBAM modules. It can be seen that each\ncomponent can improve the accuracy of the model to\nvarying degrees.\n3.3 Conclusion\nIn this paper, we constructed a computationally ef ﬁcient vision\ntransformer (ViT) model, referred to as PMVT, for the identiﬁcation of\nplant diseases. Furthermore, larger convolution kernels and CBAM\nmodules enhanced the model ’s feature extraction capability.\nFIGURE 11\nConfusion matrix of the PMVT model on the rice dataset.\nFIGURE 12\nPrecision and recall of the PMVT model on the rice dataset.\nTABLE 4 Ablation experiments investigating each component in the PMVT model (bold text highlights the best-performing network).\nMethods Wheat(%) Coffee(%) Rice(%) Params(M) FLOPs(G)\nMobileViT-XXS 91.4 83.1 92.0 0.96 0.27\n+Conv7x7 92.2(+0.8) 84.0(+1.1) 92.8(+0.8) 0.97(+0.01) 0.30(+0.03)\n+CBAM 92.5(+1.1) 84.1(+1.0) 92.6(+0.6) 0.97(+0.01) 0.27\nPMVT-XXS 93.6(+2.2) 85.3(+2.1) 93.1(+1.1) 0.98(+0.02) 0.31(+0.04)\nMobileViT-XS 93.3 84.2 94.2 1.94 0.74\n+Conv7x7 93.9(+0.6) 85.3(+1.1) 95.8(+1.6) 1.99(+0.05) 0.84(+0.1)\n+CBAM 93.6(+0.3) 85.6(+1.4) 96.5(+2.3) 1.95(+0.01) 0.76(+0.02)\nPMVT-XS 94.7(+1.4) 86.5(+2.3) 97.7(+3.5) 2.01(+0.07) 0.85(+0.11)\nMobileViT-S 93.9 84.3 89.7 4.95 1.46\n+Conv7x7 94.4(+0.5) 85.4(+1.1) 90.9(+1.2) 5.02(+0.07) 1.59(+0.13)\n+CBAM 94.4(+0.5) 84.7(+1.4) 91.1(+1.4) 4.98(+0.03) 1.47(+0.01)\nPMVT-S 94.9(+1.0) 87.6(+3.3) 92.0(+2.3) 5.06(+0.11) 1.59(+0.13)\nLi et al. 10.3389/fpls.2023.1256773\nFrontiers inPlant Science frontiersin.org10\nComparative experiments were conducted on multiple datasets\ncontaining images of plant disease s, thus demonstrating that PMVT\noutperforms both lightweight and heavyweight networks. Additionally,\nPMVT outperforms both lightweight and heavyweight networks.\nPMVT has more powerful generalization capabilities and can be\ndeployed on mobile devices for diagnosing plant diseases in ﬁeld\nenvironments. However, due to the shorter development time of\nViT, lightweight ViT models ar e comparatively slower than\ntraditional lightweight CNNs when processing images. The\nadvancement of deep learning framework technology enables ViT to\nperform computer vision tasks more effectively.\nData availability statement\nThe original contributions presented in the study are included\nin the article/supplementary material. Further inquiries can be\ndirected to the corresponding author.\nAuthor contributions\nGL: Conceptualization, Methodology, Writing – review &\nediting. YW: Software, Visualization, Writing – original draft. QZ:\nData curation, Validation, Writing – review & editing. PY:\nResources, Writing – review & editing. BC: Funding acquisition,\nSupervision, Writing – review & editing.\nFunding\nThe authors declare ﬁnancial support was received for the\nresearch, authorship, and/or pu blication of this article. This\nresearch was funded by the major project of science and\ntechnology of Henan Province (Grant No. 221100110800), the\nindependent innovation project of Henan Academy of\nAgricultural Sciences (Grant No. 2023ZC067), and the innovation\nteam of Agricultural inform ation technology (Grant\nNo. 2023TD10).\nAcknowledgments\nWe thank LetPub ( www.letpub.com) for its linguistic assistance\nduring the preparation of this manuscript.\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial or ﬁnancial relationships that could be\nconstrued as a potential con ﬂict of interest.\nPublisher's note\nAll claims expressed in this article are solely those of the authors\nand do not necessarily represent those of their af ﬁliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nReferences\nAhmad, A., Saraswat, D., and El Gamal, A. (2022). A survey on using deep learning\ntechniques for plant disease diagnosis and recommendations for development of\nappropriate tools. Smart Agric. Technol.3, 100083. doi: 10.1016/j.atech.2022.100083\nAkshai, K., and Anitha, J. (2021).“Plant disease classiﬁcation using deep learning,” in2021 3rd\nInternational Conference on Signal Processing and Communication (ICPSC) (IEEE). 407–411.\nBao, W., Yang, X., Liang, D., Hu, G., and Yang, X. (2021). Lightweight convolutional\nneural network model for ﬁeld wheat ear disease identi ﬁcation. Comput. Electron.\nAgric. 189, 106367. doi: 10.1016/j.compag.2021.106367\nChen, Y., Dai, X., Chen, D., Liu, M., Dong, X., Yuan, L., et al. (2022). “Mobile-former:\nBridging mobilenet and transformer, ” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition. 5270 – 5279.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T.,\net al. (2020). An image is worth 16x16 words: Transformers for image recognition at\nscale. doi: 10.48550/arXiv.2010.11929\nHan, K., Xiao, A., Wu, E., Guo, J., Xu, C., and Wang, Y. (2021). Transformer in\ntransformer. Adv. Neural Inf. Process. Syst.34, 15908 – 15919.\nHasan, R. I., Yusuf, S. M., and Alzubaidi, L. (2020). Review of the state of the art of\ndeep learning for plant diseases: A broad analysis and discussion. Plants 9, 1302. doi:\n10.3390/plants9101302\nHe, K., Zhang, X., Ren, S., and Sun, J. (2016). “Deep residual learning for image\nrecognition,” in Proceedings of the IEEE conference on computer vision and pattern\nrecognition. 770 – 778.\nHoward, A., Sandler, M., Chu, G., Chen, L.-C., Chen, B., Tan, M., et al. (2019).\n“Searching for mobilenetv3, ” in Proceedings of the IEEE/CVF international conference\non computer vision. 1314 – 1324.\nHughes, D., and Salathe, M. (2015). An open access repository of images on plant\nhealth to enable the development of mobile disease diagnostics. doi: 10.48550/\narXiv.1511.08060\nIandola, F. N., Han, S., Moskewicz, M. W., Ashraf, K., Dally, W. J., and Keutzer, K.\n(2016). Squeezenet: Alexnet-level accuracy with 50x fewer parameters and 0.5 mb\nmodel size. doi: 10.48550/arXiv.1602.07360\nKhan, S., Naseer, M., Hayat, M., Zamir, S. W., Khan, F. S., and Shah, M. (2022).\nTransformers in vision: A survey. ACM Computing Surveys (CSUR) 54, 1 – 41. doi:\n10.1145/3505244\nLian, R. (2022). Wheat disease classi ﬁcation.\nLin, T., Wang, Y., Liu, X., and Qiu, X. (2022). A survey of transformers. AI Open.3 ,\n111– 132. doi: 10.1016/j.aiopen.2022.10.001\nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., et al. (2021). “Swin transformer:\nHierarchical vision transformer using shifted windows, ” in Proceedings of the IEEE/\nCVF international conference on computer vision. 10012 – 10022.\nLiu, Z., Mao, H., Wu, C.-Y., Feichtenhofer, C., Darrell, T., and Xie, S. (2022). “A\nconvnet for the 2020s, ” in Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition. 11976 – 11986.\nLu, X., Yang, R., Zhou, J., Jiao, J., Liu, F., Liu, Y., et al. (2022). A hybrid model of\nghost-convolution enlightened transformer for effective diagnosis of grape leaf disease\nand pest. J. King Saud University-Computer Inf. Sci. 34, 1755 – 1767. doi: 10.1016/\nj.jksuci.2022.03.006\nMa, N., Zhang, X., Zheng, H.-T., and Sun, J. (2018). “Shufﬂenet v2: Practical\nguidelines for ef ﬁcient cnn architecture design, ” in Proceedings of the European\nconference on computer vision (ECCV). 116 – 131.\nLi et al. 10.3389/fpls.2023.1256773\nFrontiers inPlant Science frontiersin.org11\nMehta, S., and Rastegari, M. (2021). Mobilevit: light-weight, general-purpose, and\nmobile-friendly vision transformer. doi: 10.48550/arXiv.2110.02178\nParraga-Alava, J., Cusme, K., Loor, A., and Santander, E. (2019). Rocole: A robusta\ncoffee leaf images dataset for evaluation of machine learning based methods in plant\ndiseases recognition. Data Brief25, 104414. doi: 10.1016/j.dib.2019.104414\nSavary, S., Willocquet, L., Pethybridge, S. J., Esker, P., McRoberts, N., and Nelson, A.\n(2019). The global burden of pathogens and pests on major food crops. Nat. Ecol. Evol.\n3, 430 – 439. doi: 10.1038/s41559-018-0793-y\nSethy, P. K. (2020). Rice leaf disease image samples. Mendeley Data1.\nSheng, X., Wang, F., Ruan, H., Fan, Y., Zheng, J., Zhang, Y., et al. (2022). Disease\ndiagnostic method based on cascade backbone network for apple leaf disease\nclassiﬁcation. Front. Plant Sci.13, 994227. doi: 10.3389/fpls.2022.994227\nTan, M., and Le, Q. (2019). “Efﬁcientnet: Rethinking model scaling for convolutional\nneural networks,” in International conference on machine learning (PMLR). 6105– 6114.\nT h a k u r ,P .S . ,S h e o r e y ,T . ,a n dO j h a ,A .( 2 0 2 3 ) .V g g - i c n n :Al i g h t w e i g h tc n nm o d e lf o rc r o p\ndisease identiﬁcation.Multimedia Tools Appl.82, 497–520. doi: 10.1007/s11042-022-13144-z\nTouvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and Jegou, H. (2021).\n“Training dataef ﬁcient image transformers & distillation through attention, ” in\nInternational conference on machine learning (PMLR). 10347 – 10357.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al.\n(2017). Attention is all you need. Adv. Neural Inf. Process. Syst.30.\nWoo, S., Park, J., Lee, J.-Y., and Kweon, I. S. (2018). “Cbam: Convolutional block\nattention module, ” in Proceedings of the European conference on computer vision\n(ECCV).3 – 19.\nWu, H., Xiao, B., Codella, N., Liu, M., Dai, X., Yuan, L., et al. (2021). “Cvt:\nIntroducing convolutions to vision transformers, ” in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision.2 2 – 31.\nXiong, J., Yu, D., Liu, S., Shu, L., Wang, X., and Liu, Z. (2021). A review of plant\nphenotypic image recognition technology based on deep learning. Electronics 10, 81.\ndoi: 10.3390/electronics10010081\nYu, H., Cheng, X., Chen, C., Heidari, A. A., Liu, J., Cai, Z., et al. (2022a). Apple leaf\ndisease recognition method with improved residual network. Multimedia Tools Appl.\n81, 7759 – 7782. doi: 10.1007/s11042-022-11915-2\nYu, W., Luo, M., Zhou, P., Si, C., Zhou, Y., Wang, X., et al. (2022b). “Metaformer is\nactually what you need for vision, ”\nin Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition. 10819 – 10829.\nYu, S., Xie, L., and Huang, Q. (2023). Inception convolutional vision transformers for\nplant disease identi ﬁcation. Internet Things21, 100650. doi: 10.1016/j.iot.2022.100650\nLi et al. 10.3389/fpls.2023.1256773\nFrontiers inPlant Science frontiersin.org12",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7081425786018372
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6084586381912231
    },
    {
      "name": "Mobile device",
      "score": 0.5353389978408813
    },
    {
      "name": "Residual",
      "score": 0.508647084236145
    },
    {
      "name": "Encoder",
      "score": 0.5039243102073669
    },
    {
      "name": "Deep learning",
      "score": 0.48066771030426025
    },
    {
      "name": "Block (permutation group theory)",
      "score": 0.4718981683254242
    },
    {
      "name": "Preprocessor",
      "score": 0.42079252004623413
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.37818390130996704
    },
    {
      "name": "Machine learning",
      "score": 0.34342968463897705
    },
    {
      "name": "Algorithm",
      "score": 0.14912930130958557
    },
    {
      "name": "Mathematics",
      "score": 0.09634023904800415
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210096196",
      "name": "Henan Academy of Agricultural Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I75955062",
      "name": "Henan Normal University",
      "country": "CN"
    }
  ]
}