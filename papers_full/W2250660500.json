{
  "title": "An Empirical Comparison Between N-gram and Syntactic Language Models for Word Ordering",
  "url": "https://openalex.org/W2250660500",
  "year": 2015,
  "authors": [
    {
      "id": "https://openalex.org/A2567533313",
      "name": "Jiangming Liu",
      "affiliations": [
        "Singapore University of Technology and Design"
      ]
    },
    {
      "id": "https://openalex.org/A2098449489",
      "name": "Yue Zhang",
      "affiliations": [
        "Singapore University of Technology and Design"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2171795729",
    "https://openalex.org/W2096407857",
    "https://openalex.org/W2147258359",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W3198494294",
    "https://openalex.org/W2012561700",
    "https://openalex.org/W1984527503",
    "https://openalex.org/W1551202288",
    "https://openalex.org/W2161575515",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W2119182603",
    "https://openalex.org/W4243602314",
    "https://openalex.org/W4241645538",
    "https://openalex.org/W2296207831",
    "https://openalex.org/W2116983617",
    "https://openalex.org/W2166905217",
    "https://openalex.org/W129942754",
    "https://openalex.org/W2153653739",
    "https://openalex.org/W2202795099",
    "https://openalex.org/W2126433015",
    "https://openalex.org/W2437005631",
    "https://openalex.org/W2104917081",
    "https://openalex.org/W1999383775",
    "https://openalex.org/W2096253869",
    "https://openalex.org/W201441355"
  ],
  "abstract": "Syntactic language models and N-gram language models have both been used in word ordering.In this paper, we give an empirical comparison between N-gram and syntactic language models on word order task.Our results show that the quality of automatically-parsed training data has a relatively small impact on syntactic models.Both of syntactic and N-gram models can benefit from large-scale raw text.Compared with N-gram models, syntactic models give overall better performance, but they require much more training time.In addition, the two models lead to different error distributions in word ordering.A combination of the two models integrates the advantages of each model, achieving the best result in a standard benchmark.",
  "full_text": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 369–378,\nLisbon, Portugal, 17-21 September 2015.c⃝2015 Association for Computational Linguistics.\nAn Empirical Comparison Between N-gram and Syntactic Language\nModels for Word Ordering\nJiangming Liu and Yue Zhang\nSingapore University of Technology and Design,\n8 Somapah Road, Singapore, 487372\n{jiangming liu, yue zhang}@sutd.edu.sg\nAbstract\nSyntactic language models and N-gram\nlanguage models have both been used in\nword ordering. In this paper, we give\nan empirical comparison between N-gram\nand syntactic language models on word or-\nder task. Our results show that the quality\nof automatically-parsed training data has a\nrelatively small impact on syntactic mod-\nels. Both of syntactic and N-gram mod-\nels can beneﬁt from large-scale raw text.\nCompared with N-gram models, syntac-\ntic models give overall better performance,\nbut they require much more training time.\nIn addition, the two models lead to differ-\nent error distributions in word ordering. A\ncombination of the two models integrates\nthe advantages of each model, achieving\nthe best result in a standard benchmark.\n1 Introduction\nN-gram language models have been used in a wide\nrange of the generation tasks, such as machine\ntranslation (Koehn et al., 2003; Chiang, 2007;\nGalley et al., 2004), text summarization (Barzilay\nand McKeown, 2005) and realization (Guo et al.,\n2011). Such models are trained from large-scale\nraw text, capturing distributions of local word N-\ngrams, which can be used to improve the ﬂuency\nof synthesized text.\nMore recently, syntactic language models have\nbeen used as a complement or alternative to N-\ngram language models for machine translation\n(Charniak et al., 2003; Shen et al., 2008; Schwartz\net al., 2011), syntactic analysis (Chen et al.,\n2012) and tree linearization (Song et al., 2014).\nCompared with N-gram models, syntactic mod-\nels capture rich structural information, and can be\nmore effective in improving the ﬂuency of large\nconstituents, long-range dependencies and over-\nall sentential grammaticality. However, Syntactic\nmodels require annotated syntactic structures for\ntraining, which are expensive to obtain manually.\nIn addition, they can be slower compared to N-\ngram models.\nIn this paper, we make an empirical compari-\nson between syntactic and N-gram language mod-\nels on the task of word ordering (Wan et al., 2009;\nZhang and Clark, 2011a; De Gispert et al., 2014),\nwhich is to order a set of input words into a gram-\nmatical and ﬂuent sentence. The task can be re-\ngarded as an abstract language modeling problem,\nalthough methods have been explored extending it\nfor tree linearization (Zhang, 2013), broader text\ngeneration (Song et al., 2014) and machine trans-\nlation (Zhang et al., 2014).\nWe choose the model of Liu et al.(2015) as the\nsyntactic language model. There has been two\nmain types of syntactic language models in the\nliterature, the ﬁrst being relatively more oriented\nto syntactic structure, without an explicit empha-\nsis on word orders (Shen et al., 2008; Chen et al.,\n2012). As a result, this type of syntactic language\nmodels are typically used jointly with N-gram\nmodel for text-to-text tasks. The second type mod-\nels syntactic structures incrementally, thereby can\nbe used to directly score surface orders (Schwartz\net al., 2011; Liu et al., 2015). We choose the dis-\ncriminative model of Liu et al. (2015), which gives\nstate-of-the-art results for word ordering.\nWe try to answer the following research ques-\ntions by comparing the syntactic model and theN-\ngram model using the same search algorithm.\n• What is the inﬂuence of automatically-\nparsed training data on the performance of\nsyntactic models. Because manual syntac-\ntic annotations are relatively limited and highly\nexpensive, it is necessary to use large-scale\nautomatically-parsed sentences for training syn-\ntactic language models. As a result, the syntac-\ntic structures that a word ordering system learns\ncan be inaccurate. However, this might not affect\n369\nInitial State ([],set(1...n),Ø)\nFinal State ([],Ø,A)\nInduction Rules:\nSHIFT\n(σ,ρ,A)\n([σ|i],ρ−{i},A)\nL-ARC\n([σ|j i],ρ,A)\n([σ|i],ρ,A∪{j←i})\nR-ARC\n([σ|j i],ρ,A)\n([σ|i],ρ,A∪{j→i})\nFigure 1: Deduction system for transition-based\nlinearization.\nthe quality of the synthesized output, which is a\nstring only. We quantitatively study the inﬂuence\nof parsing accuracy of syntactic training data on\nword ordering output.\n•What is the inﬂuence of data scale on the\nperformance. N-gram language models can be\ntrained efﬁciently over large numbers of raw sen-\ntences. In contrast, syntactic language models can\nbe much slower to train due to rich features. We\ncompare the output quality of the two models on\ndifferent scales of training data, and also on differ-\nent amounts of training time.\n•What are the errors characteristics of each\nmodel. Syntactic language models can poten-\ntially be better in capturing larger constituents and\noverall sentence structures. However, compared\nwith N-gram models, little work has been done\nto quantify the difference between the two mod-\nels. We characterise the outputs using a set of dif-\nferent measures, and show empirically the relative\nstrength and weakness of each model.\n•What is the effect of model combination.\nFinally, because the two models make different\ntypes of errors, they can be combined to give bet-\nter outputs. We develop a combined model by dis-\ncretizing probability from N-gram model, and us-\ning them as features in the syntactic model. The\ncombined model gives the best results in a stan-\ndard benchmark.\n2 Systems\n2.1 Syntactic word ordering\nSyntactic word ordering algorithms take a multi-\nset of input words constructing an output sen-\ntence and its syntactic derivation simultaneously.\nTransition-based syntactic word ordering can be\nmodelled as an extension to transition-based pars-\ning (Liu et al., 2015), with the main difference be-\nstep action σ ρ A\ninit [] (0 1 2) Ø\n0 shift [1] (0 2)\n1 shift [1 2] (0)\n2 L-arc [2] (0) A∪{1 ←2}\n3 shift [2 0] ()\n4 R-arc [2] () A∪{2 →0}\nFigure 2: Transition-based process for ordering\n{“potatoes0”, “Tom1”, “likes2”}.\ning that the order of words is not given in the input,\nwhich leads to a much larger search space.\nWe take the system of Liu, et al. 1, which gives\nstate-of-the-art performance and efﬁciencies in\nstandard word ordering benchmark. It maintains\noutputs in stack σ, and orders the unprocessed in-\ncoming words in a set ρ. Given an input bag of\nwords, ρ is initialized to the input and σ is ini-\ntialized as empty. The system repeatedly applies\ntransition actions to consume words from ρ and\nconstruct output on σ.\nFigure 1 shows the deduction system, where ρ\nis unordered and any word inρcan be shifted onto\nthe stack σ. The set of actions are SHIFT , L- ARC\nand R- ARC . The SHIFT actions add a word to the\nstack. For the L- ARC and R- ARC actions, new\narcs {j ← i}and {j → i}are constructed re-\nspectively. Under these possible actions, the un-\nordered word set “ potatoes0 Tom1 likes2” is gen-\nerated as shown in Figure 2, and the result is\n“Tom1 ←likes2→potatoes0”.\nWe apply the learning and search framework\nof Zhang and Clark (2011a). Pseudocode of the\nsearch algorithm is shown in Algorithm 1.[] refers\nto an empty stack, and set(1...n) represents the\nfull set of input words W and nis the number of\ndistinct words. candidatesstores possible states,\nand agendastores temporary states transited from\npossible actions. GETACTIONS generates a set of\npossible actions depending on the current state s.\nAPPLY generates a new state by applying action on\nthe current states. N-BEST produces the topkcan-\ndidates in agenda. Finally, the algorithm returns\nthe highest-score state bestin the agenda.\nA global linear model is used to score search\nhypotheses. Given a hypothesis h, its score is cal-\nculated by:\nScore(h) = Φ(h) ·⃗θ,\n1http://sourceforge.net/projects/zgen/\n370\nAlgorithm 1 Transition-based linearisation\nInput: W, a set of input word\nOutput: the highest-scored ﬁnal state\n1: candidates←([],set(1..n),Ø)\n2: agenda←Ø\n3: N ←2n\n4: for i←1..N do\n5: for sin candidatesdo\n6: for actionin GETACTIONS (s) do\n7: agenda←APPLY (s,action)\n8: end for\n9: end for\n10: candidates←N-BEST (agenda)\n11: agenda←Ø\n12: end for\n13: best←BEST (candidates)\n14: return best\nwhere Φ(h) is the feature vector of h, extracted\nby using the same feature templates as Liu et\nal.(2015), which are shown in Table 1 and ⃗θ is\nthe parameter vector of the model. The feature\ntemplates essentially represents a syntactic lan-\nguage model. As shown in Figure 2, from the hy-\npotheses produced in steps 2 and 4, the features\n“Tom1 ←likes2” and “likes2 →potatoes0” are\nextracted, which corresponds to P(Tom1|likes2)\nand P(potatoes0|likes2) respectively in the de-\npendency language model of Chen et al.,(2012).\nTraining. We apply perceptron with early-update\n(Collins and Roark, 2004), and iteratively tune re-\nlated parameters on a set of development data. For\neach iteration, we measure the performance on the\ndevelopment data, and choose best parameters for\nﬁnal tests.\n2.2 N-gram word ordering\nWe build an N-gram word ordering system under\nthe same beam-search framework as the syntac-\ntic word ordering system. In particular, search is\nperformed incrementally, from left to right, adding\none word at each step. The decoding process can\nbe regarded as a simpliﬁed version of Algorithm 1,\nwith only SHIFT being returned by GETACTIONS ,\nand the score of each transition is given by a stan-\ndard N-gram language model. We use the same\nbeam size for both N-gram and the syntactic word\nordering. Compared with the syntactic model,\nthe N-gram model has less information for disam-\nbiguation, but also has less structural ambiguities,\nand therefore a smaller search space.\nUnigram\nS0w; S0p; S0,lw; S0,lp; S0,rw; S0,rp;\nS0,l2w; S0,l2p; S0,r2w; S0,r2p;\nS1w; S1p; S1,lw; S1,lp; S1,rw; S1,rp;\nS1,l2w; S1,l2p; S1,r2w; S1,r2p;\nBigram\nS0wS0,lw; S0wS0,lp; S0pS0,lw; S0pS0,lpS0,lp;\nS0wS0,rw; S0wS0,rp; S0pS0,rw; S0pS0,rpS0,rp;\nS1wS1,lw; S1wS1,lp; S1pS1,lw; S1pS1,lpS1,lp;\nS1wS1,rw; S1wS1,rp; S1pS1,rw; S1pS1,rpS1,rp;\nS0wS1w; S0wS1p; S0pS1w; S0pS1p;\nTrigram\nS0wS0pS0,lw; S0wS0,lwS0,lp; S0wS0pS0,lp;\nS0pS0,lwS0,lp; S0wS0pS0,rw; S0wS0,lwS0,rp;\nS0wS0pS0,rp; S0pS0,rwS0,rp;\nS1wS1pS1,lw; S1wS1,lwS1,lp; S1wS1pS1,lp;\nS1pS1,lwS1,lp; S1wS1pS1,rw; S1wS1,lwS1,rp;\nS1wS1pS1,rp; S1pS1,rwS1,rp;\nLinearization\nw0; p0; w−1w0; p−1p0; w−2w−1w0; p−2p−1p0;\nS0,lS0,l2w; S0,lpS0,l2p; S0,r2wS0,rw; S0,r2pS0,rp;\nS1,lS1,l2w; S1,lpS1,l2p; S1,r2wS1,rw; S1,r2pS1,rp;\nTable 1: Feature templates.\nname domain # of sents # of tokens\ntraining data\nAFP News 35,390,025 844,395,322\nXIN News 18,095,371 401,769,616\nWSJ Finance 39,832 950,028\ntesting data\nWSJ Finance 2,416 56,684\nWPB News 2,000 43,712\nSANCL Blog 1,015 20,356\nTable 2: Data.\nTraining. We train N-gram language models\nfrom raw text using modiﬁed Kneser-Ney smooth-\ning without pruning. The text is true-case tok-\nenized, and we train 4-gram language modes using\nKenLM2, which gives high efﬁciencies in standard\nN-gram language model construction.\n3 Experimental settings\n3.1 Data\nFor training data, we use the Wall Street Journal\n(WSJ) sections 1-22 of the Penn Treebank (Mar-\n2https://kheaﬁeld.com/code/kenlm/\n371\ndomain sentence example\nFinance\nThe $ 409 million bid includes the assum-\nption of an estimated $ 300 million in sec-\nured liabilities on those properties , accor-\nding to those making the bid.\nNews\nBut after rising steadily during the quarter-\ncentury following World War II , wages ha-\nve stagnated since the manufacturing sector\nbegan to contract .\nBlog\nThe freaky thing here is that these bozos\nare seriously claiming the moral high grou-\nnd ?\nTable 3: Domain examples.\ncus et al., 1993), and the Agence France-Presse\n(AFP) and Xinhua News Agency (XIN) subsets of\nthe English Giga Word Fifth Edition (Parker et al.,\n2011). As the development data, we use WSJ sec-\ntion 0 for parameter tuning. For testing, we use\ndata from various domain, which consist of WSJ\nsection 23, Washington Post/Bloomberg(WPB)\nsubsets of the English Giga Word Fifth Edition and\nSANCL blog data, as shown in Table 2. Example\nsentence in various test domains are shown in Ta-\nble 3.\n3.2 Evaluation metrics\nWe follow previous work and use the BLEU met-\nric (Papineni et al., 2002) for evaluation. Since\nBLEU only scores N-gram precisions, it can be in\nfavour of N-gram language models. We addition-\nally use METEOR3(Denkowski and Lavie, 2010)\nto evaluate the system performances. The BLEU\nmetric measures the ﬂuency of generated sentence\nwithout considering long range ordering. The ME-\nTEOR metric can potentially ﬁx this problem us-\ning a set of mapping between generated sentences\nand references to evaluate distortion. The fol-\nlowing example illustrates the difference between\nBLEU and METEOR on long range reordering,\nwhere the reference is\n(1) [The document is necessary for developer ,]0 [so you\ncan not follow this document to get right options .]1\nand the generated output sentence is\n(2) [so you can not follow this document to get right op-\ntions .]1 [The document is necessary for developer ,]0 .\nThere is a big distortion in the output. The BLEU\nmetric gives a score of 90.09 out of 100, while\n3http://www.cs.cmu.edu/∼alavie/METEOR/\nID # training sent # iter Avg F1\nset57 900 1 57.31\nset66 1800 1 66.82\nset78 9000 1 78.73\nset83 all 1 83.93\nset88 all 30 88.10\nTable 4: Parsing accuracy settings.\nthe METEOR gives a score of 61.34 out of 100.\nThis is because that METEOR is based on ex-\nplicit word-to-word matches over the whole sen-\ntence. For word ordering, word-to-word matches\nare unique, which facilitates METEOR evaluation\nbetween generated sentences and references. As\ncan bee seen from the example, long range dis-\ntortion can highly inﬂuence the METEOR scores\nmaking the METEOR metric more suitable for\nevaluating word ordering distortions.\n3.3 Data preparation\nFor all the experiments, we assume that the in-\nput is a bag of words without order, and the out-\nput is a fully ordered sentence. Following previ-\nous work (Wan et al., 2009; Zhang, 2013; Liu et\nal., 2015), we treat base noun phrases (i.e. noun\nphrases do not contains other noun phrases, such\nas ‘Pierre Vinken’ and ‘a big cat ’) as a single\nword. This avoids unnecessary ambiguities in\ncombination between their subcomponents.\nThe syntactic model requires that the train-\ning sentences have syntactic dependency struc-\nture. However, only the WSJ data contains gold-\nstandard annotations. In order to obtain automati-\ncally annotated dependency trees, we train a con-\nstituent parser using the gold-standard bracketed\nsentences from WSJ, and automatically parse the\nGiga Word data. The results are turned into de-\npendency trees using Penn2Malt4, after base noun\nphrases are extracted. In our experiments, we use\nZPar5 (Zhu et al., 2013) for automatic constituent\nparsing.\nIn order to study the inﬂuence of parsing ac-\ncuracy of the training data, we also use ten-fold\njackkniﬁng to construct WSJ training data with\ndifferent accuracies. The data is randomly split\ninto ten equal-size subsets, and each subset is auto-\nmatically parsed with a parser trained on the other\n4http://stp.lingﬁl.uu.se/∼nivre/research/Penn2Malt.html\n5http://people.sutd.edu.sg/∼yue zhang/doc/doc/con-\nparser.html\n372\nin-domain on WSJ test cross-domain on WPB test cross-domain on SANCL test\nBLEU (%) METEOR (%) BLEU (%) METEOR (%) BLEU (%) METEOR (%)\nsyntax-set57 48.76 48.98 37.31 46.78 37.60 46.79\nsyntax-set66 48.79 48.98 37.52 46.81 38.28 46.90\nsyntax-set78 49.27 49.08 38.10 46.89 38.76 46.96\nsyntax-set83 49.74 49.16 37.68 46.84 38.67 46.93\nsyntax-set88 49.73 49.17 38.27 46.92 38.52 46.93\nsyntax-gold 50.82 49.33 37.76 46.84 39.97 47.26\nTable 5: Inﬂuence result of parsing accuracy.\nnine subset. In order to obtain datasets with dif-\nferent parsing accuracies, we randomly sample a\nsmall number of sentences from each training sub-\nset, as shown in Table 4. The dependency trees\nof each set are derived from these bracketed sen-\ntences using Penn2Malt after base noun phrase are\nextracted as a single word.\n4 Inﬂuence of parsing accuracy\n4.1 In-domain word ordering\nWe train the syntactic models on the WSJ training\nparsing data with different accuracies. The WSJ\ndevelopment data are used to ﬁnd out the optimal\nnumber of training iterations for each experiments,\nand the WSJ test results are shown in Table 5.\nTable 5 shows that the parsing accuracy can af-\nfect the performance of the syntactic model. A\nhigher parsing accuracy can lead to a better syn-\ntactic language model. It conforms to the intu-\nition that syntactic quality affects the ﬂuency of\nsurface texts. On the other hand, the inﬂuence is\nnot huge, the BLEU scores decrease by 1.0 points\nas the parsing accuracy decreases from 88.10% to\n57.31%\n4.2 Cross-domain word ordering\nThe inﬂuence of parsing accuracy of the training\ndata on cross-domain word ordering is measured\nby using the same training settings, but testing on\nthe WPB and SANCL test sets. Table 5 shows\nthat the performance on cross-domain word order-\ning cannot reach that of in-domain word ordering\nusing the syntactic models. Compared with the\ncross-domain experiments, the inﬂuence of pars-\ning accuracy becomes smaller. In the WPB test,\nthe ﬂuctuation of performance decline to about 0.9\nBLEU points, and in the SANCL test, the ﬂuctua-\ntion is about 1.1 BLEU points.\nIn conclusion, the experiments show that pars-\ning accuracies have a relatively small inﬂuence on\nthe syntactic models. This suggests that it is possi-\nble to use large automatically-parsed data to train\nsyntactic models. On the other hand, when the\ntraining data scale increases, syntactic models can\nbecome much slower to train compared with N-\ngram models. The inﬂuence on data scale, which\nincludes output quality and training time, is further\nstudied in the next section.\n5 Inﬂuence of data scale\nWe use the AFP news data as the training data\nfor the experiments of this section. The syntac-\ntic models are trained using automatically-parsed\ntrees derived from ZPar, as described in Section\n3.3. The WPB test data is used to measure in-\ndomain performance, and the SANCL blog data\nis used to measure cross-domain performance.\n5.1 Inﬂuence on BLEU and METEOR\nThe Figure 3 and 4 shows that using both the\nBLEU and the METEOR metrics, the perfor-\nmance of the syntactic model is better than that\nof the N-gram models. It suggests that sentences\ngenerated by the syntactic model have both bet-\nter ﬂuency and better ordering. The performance\nof the syntactic models is not highly weakened in\ncross-domain tests.\nThe grey dot in each ﬁgure shows the perfor-\nmance of the syntactic model trained on the gold\nWSJ training data, and evaluated on the same\nWPB and SANCL test data sets. A comparison\nbetween the grey dots and the dashed lines shows\nthat the syntactic model trained on the WSJ data\nperform better than the syntactic model trained on\nsimilar amounts of AFP data. This again shows\nthe effect of syntactic quality of the training data.\nOn the other hand, as the scale of automatically-\nparsed AFP data increases, the performance of the\n373\n0 1 2 3 4\n35\n40\n45\ntraining data size (million sentences)\nBLEU(%)\n4-gram\nsyntax\nWSJ\n0 1 2 3 4\n46\n46.5\n47\n47.5\n48\ntraining data size (million sentences)\nMETEOR(%)\n4-gram\nsyntax\nWSJ\nFigure 3: In-domain results on different training data size.\n0 1 2 3 4\n35\n40\n45\ntraining data size (million sentences)\nBLEU(%)\n4-gram\nsyntax\nWSJ\n0 1 2 3 446\n47\n48\ntraining data size (million sentences)\nMETEOR(%)\n4-gram\nsyntax\nWSJ\nFigure 4: Cross-domain results on different training data sizes.\n1 2 3 4 5 630\n35\n40\n45\n50\ntraining time (log seconds)\nBLEU (%)\n4-gram\nsyntax\nFigure 5: BLEU on different training times.\nsyntactic model rapidly increases, surpassing the\nsyntactic model trained on the high-quality WSJ\ndata. This observation is important, showing that\nlarge-scale data can be used to alleviate the prob-\nlem of lower syntactic quality in automatically-\nparsed data, which can be leveraged to address the\nscarcity issue of manually annotated data in both\nin-domain and cross-domain settings.\n5.2 Inﬂuence on training time\nThe training time of both syntactic models and\nN-gram models increases as the size of training\ndata increases. Figure 5 shows the BLEU of the\ntwo systems under different amounts of training\ntime. There is no result reported for the syntac-\ntic model beyond 1 million training sentences, be-\ncause training becomes infeasibly slow 6. On the\n6Our experiments are carried on a single thread of\n3.60GHz CPU. If the training time is over 90 hours for a\nmodel, we consider it infeasible.\nother hand, theN-gram model can be trained using\nall the WSJ, AFP, XIN training sentences, which\nare 53 millions, within 103.2 seconds. As a result,\nthere is no overlap between the syntactic model\nand the N-gram model curves.\nAs can be seen from the ﬁgure, the syntactic\nmodel is much slower to train. However, it ben-\neﬁts more from the scale of the training data, with\nthe slope of the dashed curve being steeper than\nthat of the solid curve. The N-gram model can\nbe trained with more data thanks to the fast train-\ning speed. However, the performance of the N-\ngram model ﬂattens when the training data size\nreaches beyond 3 million. Projection of the solid\ncurve suggests that the performance of theN-gram\nmodel may not surpass that of the syntactic model\neven if sufﬁciently large data is available for train-\ning the N-gram model in more time.\n6 Error analysis\nAlthough giving overall better performance, the\nsyntactic model does not perform better than the\nN-gram model in all cases. Here we analyze the\nstrength of each model via more ﬁne-grained com-\nparison.\nIn this set of experiments, the syntactic model is\ntrained using gold-standard annotated WSJ train-\ning parse trees, and the N-gram model is trained\nusing the data containing WSJ training data, AFP\nand XIN. The WSJ test data, which contains\n374\n5 10 15 20 25 30\n40\n60\n80\n100\naverage length of test sentences\nBLEU(%)\n4-gram\nsyntax\n5 10 15 20 25 3040\n60\n80\n100\naverage length of test sentences\nMETEOR(%)\n4-gram\nsyntax\nFigure 6: Performance on sentences with different length.\n0.2 0.4 0.6 0.8 1\n2,000\n4,000\n6,000\n8,000\ndistortion rate\n4-gram\nsyntax\nFigure 7: The distribution of distortion.\ngolden constituent trees, is used to analyze errors\nin different aspects.\n6.1 Sentence length\nThe BLEU and METEOR scores of the two sys-\ntems on various sentence lengths are shown in\nFigure 6. The results are measured by binning\nsentences according to their lengths, so that each\nbin contains about the same number of sentences.\nAs shown by the ﬁgure, the N-gram model per-\nforms better on short sentences (less than 8 to-\nkens), and the syntactic model performs better on\nlonger sentences. This can be explained by the\nfact that longer sentences have richer underlying\nsyntactic structures, which can better captured by\nthe syntactic model. In contrast, for shorter sen-\ntences, the syntactic structure is relatively simple,\nand therefore the N-gram model can give better\nperformance based on string patterns, which form\nsmaller search spaces.\n6.2 Distortion range\nWe measure the average distortion rate of output\nword wusing the following metric:\ndistortion(w) = |iw −i′\nw|\nlen(Sw) ,\nwhere iw is index of wordwin the output sentence\nSw, i′\nw is the index of the word w in the refer-\nence sentence. len(Sw) is the number of tokens in\nTemplate distribution\nNLM-LOW set 1 if p<e −12.5, else 0\nNLM-20 use 20 bins to scatter probability\nNLM-10 use 10 bins to scatter probability\nNLM-5 use 5 bins to scatter probability\nNLM-2 use 2 bins to scatter probability\nTable 6: NLM feature templates.\nsentence Sw. Figure 7 shows distributions of dis-\ntortion respectively by the syntactic and N-gram\nmodel. The N-gram model makes relatively fewer\nshort-range distortions, but more long-range dis-\ntortions. This can be explained by the local scor-\ning nature of the N-gram model. In contrast, the\nsyntactic model makes less long-range distortions,\nwhich can suggest better sentence structure.\n6.3 Constituent span\nWe further evaluate sentence structure correctness\nby evaluating the recalls of discovered constituent\nspan in output two systems, respectively. As\nshown in Figure 8. The syntactic model performs\nbetter in most constituent labels. However, the\nN-gram model performs better in WHPP, SBARQ\nand WHNP.\nIn the test data, WHPP, SBARQ and WHNP\nare much less than PP, NP, VP, ADJP, ADVP and\nCONJP, on which the syntactic model gives bet-\nter recalls. WHNP spans are small and most of\nthem consist of a question word (WP$) and one or\ntwo nouns (e.g. “whose (WP$) parents (NNS)”).\nWHPP spans are also small and usually consist\nof a preposition (IN) and a WHNP span (e.g “at\n(IN) what level (WHNP)”). The N-gram model\nperforms better on these small spans. The syntac-\ntic model also performs better on S, which covers\nthe whole sentence structure. This veriﬁes the hy-\npothesis introduce that syntactic language models\nbetter capture overall sentential grammaticality.\n375\nCONJPWHADVPWHPPFRAGSBARQWHADJPWHNP PP NP VP QP ADJP ADVP S SBAR\n0\n50\n100constituents recall (%)\n4-gram\nsyntax\nFigure 8: Recalls of different constituents.\nin-domain on WSJ test cross-domain on WPB test cross-domain on SANCL test # of\nBLEU (%) METEOR (%) BLEU (%) METEOR (%) BLEU (%) METEOR (%) sent/s\nsyntax 50.82 49.33 37.76 46.84 39.97 47.26 17.9\n4-gram 42.26 48.00 37.71 46.90 39.72 47.08 177.0\ncombined 52.38 49.66 39.12 47.07 40.60 47.38 15.4\nTable 7: Final results on various domains.\n7 Combining the syntactic and N-gram\nmodels\nThe results above show the respective error char-\nacteristics of each model, which are complimen-\ntary. This suggests that better results can be\nachieved by model combination.\n7.1 N-gram language model feature\nWe integrate the two types of models by using\nN-gram language model probabilities as features\nin the syntactic model. N-gram language model\nprobabilities, which ranges from 0 to 1. Direct\nuse of real value probabilities as features does not\nwork well in our experiments, and we use dis-\ncretized features instead. For the L- ARC and R-\nARC actions, because no words are pushed onto\nthe stack, The NLM feature is set to NULL by de-\nfault. For the SHIFT action, different feature values\nare extracted depending on the NLM from 0 to 1.\nIn order to measure the N-gram probabilities\non our data, we train the 4-gram language model\nWSJ, AFP and XIN data, and randomly sample 4-\ngram probabilities from the syntactic model output\non the WSJ development data, ﬁnding that most\nof 4-gram probabilities pare larger than 10−12.5.\nIn this way, if plower than 10−12.5, NLM feature\nvalue is set to LOW. As for plarger than 10−12.5,\nwe extract the discrete features by assigning them\ninto different bins. We bin the 4-gram probabil-\nities with different granularities without overlap\nfeatures. As shown in Table 6, NLM-20, NLM-\n10, NLM-5 and NLM-2 respectively use 20, 10, 5\nBLEU (%) on WSJ test\nWan et al. (2009) 33.70*\nZhang and Clark (2011b) 40.10*\nZhang et al. (2012) 43.80*\nZhang (2013) 44.70\nsyntax (Liu et al., 2015) 50.82\n4-gram 42.26\ncombined 52.38\nTable 8: Final results of all systems, where “*”\nmeans that the system uses extra POS input.\nand 2 bins to capture NLM feature values.\n7.2 Final results\nWe use the WSJ, AFP and XIN for training theN-\ngram model7. The same WSJ, WPB and SANCL\ntest data are used to measure performances on dif-\nferent domains.\nThe experimental results are shown in Tables\n7 and 8. In both in-domain and cross-domain\ntest data, the combined system outperforms all\nother systems, with a BLEU score of 52.38 been\nachieved in the WSJ domain. It would be overly\nexpensive to obtain a human oracle on discusses.\nHowever, according to Papineni (2002), a BLEU\n7For the combined model, we used the WSJ training data\nfor training, because the syntactic model is slower to train us-\ning large data. However, we did a set of experiments to scale\nup the training data by sampling 900k sentences from AFP.\nResults show that the combined model gives BLEU scores of\n42.86 and 44.44 on the WPB and SANCL tests, respectively.\nCross-domain BLEU on WSJ, however falls to 49.84.\n376\nBLEU sentences\nref For weeks , the market had been nervous\nabout takeovers , after Campeau Corp. ’s cash\ncrunch spurred concern about the prospects for\nfuture highly leveraged takeovers .\n41.37 For weeks , Campeau Corp. ’s cash had\nthe prospects for takeovers after the market\ncrunch spurred concern about future highly\nleveraged takeovers , nervous been about .\nref Now , at 3:07 , one of the market ’s post-\ncrash “ reforms ” took hold as the S&P\n500 futures contract had plunged 12 points\n, equivalent to around a 100-point drop in\nthe Dow industrials .\n51.39 Now , one of the market ’s reforms plunged\n12 points in the Dow industrials as “ post-\ncrash , the S&P 500 futures contract ,\nequivalent to 3:07 took hold at around a\n100-point drop had . ”\nref Canadian Utilities had 1988 revenue of C$\n1.16 billion , mainly from its natural gas\nand electric utility businesses in Alberta ,\nwhere the company serves about 800,000\ncustomers .\n64.38 Canadian Utilities , Alberta , where the\ncompany had 1988 revenue of C$ 1.16\nbillion in its natural gas and electric utility\nbusinesses serves mainly from about\n800,000 customers .\nTable 9: Output samples.\nscore of over 52.38 indicate an easily understood\nsentence. Some sample outputs with different\nBLEU scores are shown in Table 9\nIn addition, Table 7 shows that the N-gram\nmodel is the fastest among the models due to its\nsmall search space. The running time of the com-\nbined system is larger than the pure syntactic sys-\ntem, because of N-gram probability computation.\nTable 8 compare our results with different previ-\nous methods on word ordering. Our combined\nmodel gives the best reported performance on this\nstandard benchmarks.\n8 Conclusion\nWe empirically compared the strengths and er-\nror distributions of syntactic and N-gram lan-\nguage models on word ordering, showing that both\ncan beneﬁt from large-scale raw text. The inﬂu-\nence of parsing accuracies has relatively small im-\npact on the syntactic language model trained on\nautomatically-parsed data, which enables scaling\nup of training data for syntactic language mod-\nels. However, as the size of training data in-\ncreases, syntactic language models can become in-\ntolerantly slow to train, making them beneﬁt less\nfrom the scale of training data, as compared with\nN-gram models.\nSyntactic models give better performance com-\npared with N-gram models, despite trained with\nless data. On the other hand, the two models lead\nto different error distributions in word ordering.\nAs a result, we combined the advantages of both\nsystems by integrating a syntactic model trained\nwith relatively small data and an N-gram model\ntrained with relatively large data. The resulting\nmodel gives better performance than both single\nmodels and achieves the best reported scores in a\nstandard benchmark for word ordering.\nWe release our code under GPL at https://\ngithub.com/SUTDNLP/ZGen. Future work\nincludes application of the system on text-to-text\nproblem such as machine translation.\nAcknowledgments\nThe research is funded by the Singapore min-\nistry of education (MOE) ACRF Tier 2 project\nT2MOE201301. We thank the anonymous review-\ners for their detailed comments.\nReferences\nRegina Barzilay and Kathleen R McKeown. 2005.\nSentence fusion for multidocument news summa-\nrization. Computational Linguistics , 31(3):297–\n328.\nEugene Charniak, Kevin Knight, and Kenji Yamada.\n2003. Syntax-based language models for statistical\nmachine translation. In Proceedings of MT Summit\nIX, pages 40–46. Citeseer.\nWenliang Chen, Min Zhang, and Haizhou Li. 2012.\nUtilizing dependency language models for graph-\nbased dependency parsing models. In Proceedings\nof ACL, pages 213–222.\nDavid Chiang. 2007. Hierarchical phrase-based trans-\nlation. computational linguistics, 33(2):201–228.\nMichael Collins and Brian Roark. 2004. Incremental\nparsing with the perceptron algorithm. In Proceed-\nings of ACL, page 111.\n377\nA De Gispert, M Tomalin, and W Byrne. 2014. Word\nordering with phrase-based grammars. In Proceed-\nings of EACL, pages 259–268.\nMichael Denkowski and Alon Lavie. 2010. Extending\nthe meteor machine translation evaluation metric to\nthe phrase level. In HLT/NAACL, pages 250–253.\nMichel Galley, Mark Hopkins, Kevin Knight, and\nDaniel Marcu. 2004. What’s in a translation rule.\nTechnical report, DTIC Document.\nYuqing Guo, Haifeng Wang, and Josef Van Genabith.\n2011. Dependency-based n-gram models for gen-\neral purpose sentence realisation. Natural Language\nEngineering, 17(04):455–483.\nPhilipp Koehn, Franz Josef Och, and Daniel Marcu.\n2003. Statistical phrase-based translation. In Pro-\nceedings of NAACL, pages 48–54.\nYijia Liu, Yue Zhang, Wanxiang Che, and Bing Qin.\n2015. Transition-based syntactic linearization. In\nProceedings of NAACL/HLT, pages 113–122, Den-\nver, Colorado, May–June.\nMitchell P Marcus, Mary Ann Marcinkiewicz, and\nBeatrice Santorini. 1993. Building a large anno-\ntated corpus of english: The penn treebank. Compu-\ntational linguistics, 19(2):313–330.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic\nevaluation of machine translation. In Proceedings\nof ACL, pages 311–318. Association for Computa-\ntional Linguistics.\nRobert Parker, David Graff, Junbo Kong, Ke Chen,\nand Kazuaki Maeda. 2011. English gigaword\nﬁfth edition, june. Linguistic Data Consortium,\nLDC2011T07.\nLane Schwartz, Chris Callison-Burch, William\nSchuler, and Stephen Wu. 2011. Incremental syn-\ntactic language models for phrase-based translation.\nIn Proceedings of ACL/HLT, pages 620–631.\nLibin Shen, Jinxi Xu, and Ralph M Weischedel. 2008.\nA new string-to-dependency machine translation al-\ngorithm with a target dependency language model.\nIn Proceedings of ACL, pages 577–585.\nLinfeng Song, Yue Zhang, Kai Song, and Qun Liu.\n2014. Joint morphological generation and syntactic\nlinearization. In Twenty-Eighth AAAI Conference on\nArtiﬁcial Intelligence.\nStephen Wan, Mark Dras, Robert Dale, and C ´ecile\nParis. 2009. Improving grammaticality in statisti-\ncal sentence generation: Introducing a dependency\nspanning tree algorithm with an argument satisfac-\ntion model. In Proceedings of EACL , pages 852–\n860.\nYue Zhang and Stephen Clark. 2011a. Syntactic pro-\ncessing using the generalized perceptron and beam\nsearch. Computational Linguistics, 37(1):105–151.\nYue Zhang and Stephen Clark. 2011b. Syntax-based\ngrammaticality improvement using ccg and guided\nsearch. In Proceedings of EMNLP , pages 1147–\n1157.\nYue Zhang, Graeme Blackwood, and Stephen Clark.\n2012. Syntax-based word ordering incorporating\na large-scale language model. In Proceedings of\nEACL, pages 736–746. Association for Computa-\ntional Linguistics.\nYue Zhang, Kai Song, Linfeng Song, Jingbo Zhu, and\nQun Liu. 2014. Syntactic smt using a discriminative\ntext generation model. In Proceedings of EMNLP,\npages 177–182, Doha, Qatar, October.\nYue Zhang. 2013. Partial-tree linearization: general-\nized word ordering for text synthesis. In Proceed-\nings of IJCAI, pages 2232–2238. AAAI Press.\nMuhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang,\nand Jingbo Zhu. 2013. Fast and accurate shift-\nreduce constituent parsing. In ACL, pages 434–443.\n378",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7515357732772827
    },
    {
      "name": "n-gram",
      "score": 0.6861936450004578
    },
    {
      "name": "Natural language processing",
      "score": 0.6382590532302856
    },
    {
      "name": "Word (group theory)",
      "score": 0.5407971739768982
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4661698639392853
    },
    {
      "name": "Linguistics",
      "score": 0.38944268226623535
    },
    {
      "name": "Language model",
      "score": 0.3680611848831177
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}