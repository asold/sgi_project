{
  "title": "Quantized-Dialog Language Model for Goal-Oriented Conversational Systems",
  "url": "https://openalex.org/W2906153282",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4302753271",
      "name": "Gunasekara, R. Chulaka",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4290682537",
      "name": "Nahamoo, David",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4302753273",
      "name": "Polymenakos, Lazaros C.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2742392552",
      "name": "Ganhotra, Jatin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4301884094",
      "name": "Fadnis, Kshitij P.",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2964210218"
  ],
  "abstract": "We propose a novel methodology to address dialog learning in the context of goal-oriented conversational systems. The key idea is to quantize the dialog space into clusters and create a language model across the clusters, thus allowing for an accurate choice of the next utterance in the conversation. The language model relies on n-grams associated with clusters of utterances. This quantized-dialog language model methodology has been applied to the end-to-end goal-oriented track of the latest Dialog System Technology Challenges (DSTC6). The objective is to find the correct system utterance from a pool of candidates in order to complete a dialog between a user and an automated restaurant-reservation system. Our results show that the technique proposed in this paper achieves high accuracy regarding selection of the correct candidate utterance, and outperforms other state-of-the-art approaches based on neural networks.",
  "full_text": "Quantized-Dialog Language Model for Goal-Oriented Conversational Systems\nR. Chulaka Gunasekara, David Nahamoo, Lazaros C. Polymenakos,\nJatin Ganhotra, and Kshitij P . Fadnis\nIBM Thomas J. Watson Research Center, Yorktown Heights, NY , USA\nchulaka.gunasekara@ibm.com, nahamoo@us.ibm.com, lcpolyme@us.ibm.com,\njatinganhotra@us.ibm.com, kpfadnis@us.ibm.com\nAbstract\nWe propose a novel methodology to address dialog learning in\nthe context of goal-oriented conversational systems. The key\nidea is to quantize the dialog space into clusters and create a lan-\nguage model across the clusters, thus allowing for an accurate\nchoice of the next utterance in the conversation. This quantized-\ndialog language model methodology has been applied to the\nend-to-end goal-oriented track of the latest Dialog System Tech-\nnology Challenges (DSTC6). The objective is to ﬁnd the correct\nsystem utterance from a pool of candidates in order to complete\na dialog between a user and an automated restaurant-reservation\nsystem. Our results show that the technique proposed in this\npaper achieves high accuracy regarding selection of the correct\ncandidate utterance, and outperforms other state-of-the-art ap-\nproaches based on neural networks.\nIndex Terms: language models, goal-oriented conversational\nsystems, dialog learning, deep learning for dialog\n1. Introduction\nThe Dialog State Tracking Challenge (DSTC; now rebranded\nas Dialog System Technology Challenges) initiative was orig-\ninally conceived to provide a benchmark for the evaluation\nof dialog-management systems. The latest of these challenges,\nDSTC6 ( http://workshop.colips.org/dstc6/), is\ndivided into three different tracks, and our work addresses the\nend-to-end goal-oriented dialog track. This track focuses on a\nrestaurant-reservation problem: the objective is to book a table\nin a restaurant satisfying a number of requirements given by the\nuser.\nIt is interesting to note that, as suggested in [1, 2], method-\nologies based on rules may solve the goal-oriented dialog prob-\nlem proposed in DSTC6 with full accuracy (i.e., no errors at\nall). By contrast, data-driven conversational systems (see, e.g.,\n[3, 4, 5]), are typically easier to apply to new domains and often\nperform in a satisfactory manner, but usually are less accurate\nthan rule-based methods.\nIn the implicit approach of developing dialog systems, the\nmodels are learned from the available interaction data, with-\nout using the traditional feature engineering and conversational\ncomponents such as natural language understanding (NLU), di-\nalog management, semantic matching etc. These models have\nthe capability to continuously learn from the data and improve\neven after deployment, without cost and time consuming re-\ndesign of the features of individual components. Recent ap-\nproaches of implicit dialog systems rely on applying deep learn-\ning techniques (e.g. sequence-to-sequence architectures) on the\ndialog data for prediction or generation of the next utterance in a\ndialog [6, 7, 8]. But, the accuracies of these models on a variety\nof dialog datasets have been low [9].\nDuring prediction, the current approaches select the next ut-\nterance from all possible responses that can be given at any turn,\nalong with all their possible syntactic variations. This explodes\nthe space of possible choices and leads to poor performance. To\nalleviate this problem, we propose the quantized representation\nof dialogs, which reduces the state space for prediction and im-\nproves the performance of data-driven conversational systems.\nThe paper is structured as follows. In the next section, we\nbrieﬂy describe the problem and dataset considered. Then, in\nSection 3 we introduce the quantized-dialog language model\nand, based on it, we develop a next utterance selection method\nfor our goal-oriented dialog. In Section 4, we present results\ncomparing the new approach with two reference schemes that\nrely on neural networks. Finally, we conclude our work with a\nfew observations and indicate directions of future work.\n2. Problem statement and dataset\nThe main problem in the goal-oriented dialog-learning track in\nDSTC6 is further divided into four major subtasks: i) Issuing\nAPI calls, ii) Updating API calls, iii) Displaying options and iv)\nProviding extra information. The ﬁrst two subtasks address dia-\nlog interaction for the collection of user requirements for restau-\nrant reservation: atmosphere, location, cuisine, number of peo-\nple in the party, and price range. Once these ﬁve requirements\nhave been fulﬁlled, an API call is issued in order to retrieve (at\nleast three) restaurants from a knowledge database that satisfy\nthe requirements. Subtask three is about presenting the user\nwith possible restaurant choices and subtask four is about pro-\nviding additional information related to the restaurant option the\nuser selected (i.e., restaurant address and phone number). There\nis a ﬁfth subtask that combines the four subtasks mentioned and\naims at modeling a complete dialog system for restaurant reser-\nvation.\nThe training data released in the challenge consists of\n10,000 dialogs per subtask (including the ﬁfth task). The au-\ntomated dialog system evaluated as part of the challenge has to\nselect the correct next utterance in each dialog subtask out of a\nnumber of possible candidates. Test data consists of four test\nsets, each with 5,000 dialogs split equally (1000 dialogs) over\nthe ﬁve tasks. These four sets will be referred to in this paper\nas tst1, tst2, tst3 and tst4. The ﬁrst test set is based on the same\ntype of dialogs that occur in the training data. The second set\nincorporates out-of-vocabulary entities (e.g., new cuisine types)\nand restaurants extracted from a knowledge database that is dif-\nferent from the one used in the training data. The third test set\nincludes six entity types in the user requirements (the additional\nentity type is related to dietary restrictions) whereas there are\nonly ﬁve in the training data. Out-of-vocabulary entities corre-\nsponding to the six types as well as the restaurants from the new\nknowledge database are combined in the fourth set.\narXiv:1812.10356v1  [cs.CL]  26 Dec 2018\n3. A quantized language model for\ngoal-oriented dialog learning\nIn this section we discuss our main contribution, which is the\nquantized-dialog language model (QDLM) . In Figure 1 we\nshow the model components for training and prediction (run-\ntime), that we discuss in the sequel.\n3.1. Utterance preprocessing\nNatural language utterances usually contain information that\nmay be unnecessary in a decision-making process. To remove\nirrelevant information, we preprocess the utterances via delex-\nicalization of entities (or more generally parameters) relevant\nto the dialog goal. User utterances that contain entity values\nfound in the knowledge base are transformed into a correspond-\ning vector of entity types. During this transformation, the en-\ntity types that were identiﬁed in previous utterances are also\npreserved. The order of the entity types in the vector is: cui-\nsine type, location, num people, price range and atmosphere.\nAn example of preprocessed dialog is shown in Table 1.\nWe also detect dubious statements regarding entity/parameter\nselection by the user (see, e.g., utterance 11 in that table; more\ndetails will be discussed later in Section 3.4). Constructing the\nlanguage model with this delexicalized representation helps the\nquantized language model learn the dialog policy more accu-\nrately.\nIn Task 3 and 4, the set of restaurants recommended (ob-\ntained by means of the API call) is also preprocessed to identify\nthe order in which the restaurants should be presented to the\nuser. Consistent with our delexicalization approach, the name\nof the restaurant which should be proposed in the ith place is\nmodiﬁed to RESTAURANT i NAME, and each of its properties\n(e.g., location, price, rating, etc.) are accordingly changed to\nRESTAURANT i PROPERTY.\n3.2. Utterance quantization and language model\nOnce the preprocessing is completed, the utterances are embed-\nded into a vector space representation using the Bag-of-Words\nencoding [10] scheme. More complex procedures such as the\nSkip-Thought algorithm (see, e.g., [11]) can also be considered\nfor this process. Next, the utterances are clustered in the em-\nbedding space to create a quantized representation. In this im-\nplementation, each different vector was identiﬁed with a differ-\nent cluster. Once the clusters are identiﬁed, each utterance can\nbe represented by the identiﬁer associated with the cluster that\nparticular utterance belongs to. This yields to a quantized rep-\nresentation of utterances. Once the utterances are quantized, a\nconversation can be represented as a sequence of clusters. For\nexample, consider a dialog D, which comprises the sequence\nof utterances {u1, u2, ..., uN }, where ui is a natural language\nutterance. Following the quantization, the dialog D can be rep-\nresented as a set of numbers{c1, c2, ..., cN }, where each ci cor-\nresponds to the cluster identiﬁer where ui belongs.\nOne of the simplest methods to assign probabilities to se-\nquences of tokens is the n-gram language model [12]. The\nhigh-level idea is that the probability values for the candidate\nutterances are calculated using an n-gram language model con-\nstructed on the clusters. In constructing the language model, we\nconsider cluster transitions in all dialogs to calculate the tran-\nsition probabilities between the clusters. This n-gram language\nmodel estimates the probability P(ci|ci−n, ..., ci−1).\n3.3. Runtime and utterance prediction\nThe utterance prediction problem can be formalized as\narg maxu P(u|u1, ...ui−1), where u denotes the utterance\nthat maximizes the conditional probability with respect to\nall previous utterances. In the quantized dialog space, this\nproblem is transformed into the cluster prediction problem\narg maxc P(c|c1, ...ci−1), where c is the cluster identiﬁer that\nmaximizes the conditional probability with respect to the clus-\nters associated with all the previous utterances. Note that we\ncan estimate this by the n-gram language model as follows:\narg maxc P(c|c1, ...ci−1) ≈arg maxc P(c|ci−1, ...ci−n).\nAs the utterances within a cluster are similar to one another, any\nutterance in the predicted cluster c can be selected as the pre-\ndicted utterance u. In this implementation, previous 7 clusters\nwere used to determine the probability distribution of the next\ncluster using the language model.\nDuring the evaluation of all the candidates for a response,\nthe cluster to which each candidate belongs is identiﬁed, and\nthen the language model is used to calculate the probability as-\nsociated with that cluster. Then we rank the candidate responses\nusing this probability value together with the constructed dialog\nstate (discussed in Section 3.4).\n3.4. Dialog-state update\nIn the approach presented in this paper, the dialog state corre-\nsponds to a key-value store associated with each entity speci-\nﬁed by the user and its respective value (e.g., CUISINE TYPE\nand indian). The dialog-state update is based on a keyword\nmatching algorithm, assisted by a hierarchical sentence classi-\nﬁer. The keyword matcher compares each word mentioned in\na user utterance against all the entities in the knowledge base.\nFigure 2 illustrates the operation of the dialog-state update mod-\nule, and the last column of Table 1 refers to the corresponding\ndialog state maintained through the dialog. The hierarchical\nclassiﬁer is implemented using a publicly available text classi-\nﬁcation service1, and the operations of individual classiﬁers are\nexplained below.\nWhen an entity is detected in a user utterance, the ﬁrst clas-\nsiﬁer (denoted as Classiﬁer 1 in Figure 2) conﬁrms the selection\nof that entity as a user speciﬁcation. For example, the entity as-\nsociated with the CUISINE TYPE is not conﬁrmed as part of\na user request in one minute please, i am asking\nmy friend if she wants to do spanish, let’s\nsee; by contrast italian is a conﬁrmed requirement\nin the user utterance wait, i am asking my friend\nand she wants italian, let’s do that. Utter-\nances with detected but unconﬁrmed entities are labeled as du-\nbious. When preparing training data for this classiﬁer, we label\nas dubious the user utterances with entities detected, followed\nby the system utterance ‘whenever youre ready’, and we label as\nnot dubious the other user utterances with just the entities\ndetected.\nIf two or more entities of the same type are identiﬁed\nin the same utterance but only one indicates user require-\nments, the second classiﬁer (denoted as Classiﬁer 2 in Fig-\nure 2) allows selection of the correct entity in order to up-\ndate the dialog state. The training data for this classi-\nﬁer were prepared by, ﬁrst, transforming utterances such\nas let’s do moderate price range, and keep\n1https://www.ibm.com/watson/services/natural-language-\nclassiﬁer/.\n(a) Pipeline of the training process\n(b) Pipeline of the runtime process\nFigure 1: The training and runtime processes of the quantized-dialog language model.\nexpensive price range for another dayinto the\nform let’s do ENTITY 1 price range, and keep\nENTITY 2 price range for another day, and then\nidentifying the correct entity ENTITY 1 (label) from the sub-\nsequent API call.\nOnce the language model predicts that the next utterance\nis an API call, the key-value pairs associated with the current\ndialog state are used to ﬁll in the slots of the API call. The API\ncall thus constructed is compared against the given candidates\nusing (word-level) Levenshtein distance (see, e.g., [13]) in order\nto rank them.\nIt is important to note that, because QDLM has been trained\non delexicalized data, out-of-vocabulary entities and restaurants\nfrom new knowledge databases can be handled without modi-\nfying the approach - with the caveat that the restaurants need\nto have the same information ﬁelds as in the database used for\ntraining. We stress that the only modiﬁcation we introduced\nin order to deal with additional parameters in the user require-\nments was to increase accordingly the number of key-value\npairs in the dialog state (i.e., six pairs were considered in two of\nthe test sets). Therefore, when QDLM predicts that the next ut-\nterance should be an API call, the number of slots necessary for\nthe call are directly given by the existing number of key-value\npairs in the dialog state.\n4. Experiments and results\nIn this section, we ﬁrst present the two neural network ap-\nproaches that were used for comparison with QDLM. Then, we\nreport and brieﬂy analyze the results of the three techniques on\nthe DSTC6 datasets.\n4.1. Baselines considered\nFor our ﬁrst baseline, we choose a relatively simple architec-\nture: a multi-layer feed-forward neural network ([14, 15]) 2.\nThis network is applied to each dialog and candidate answer\nto yield a conﬁdence metric deﬁned in the interval [0, 1]. In all\ncases, a logistic function is considered after the output layer.\nThe ﬁrst two tasks in DSTC6 are addressed by means of\na network with one hidden layer of size 100 with rectiﬁed lin-\near unit as activation function. The networks for Task 3 and\n4 have three and two hidden layers, respectively, all of them of\nsize 100 (and also followed by rectiﬁed linear units). These net-\nworks were optimized, for each task individually, for accuracy\nand convergence rate by varying number of hidden layers, hid-\nden layer sizes and activation layer functions. The dialogs in\nTask 5 were ﬁrst classiﬁed into four subtasks and then four net-\nworks of the same type as those were used to predict the next\ncandidate utterance. Binary cross-entropy was minimized for\neach network by means of the Adam optimizer (see, e.g., [16]).\nFrom all the dialogs in the dataset, 75% of them are used to train\nthe networks.\nThe input to each network was designed to allow ready ap-\nplication of the approach to other domains than restaurant reser-\nvations (e.g., hotel-booking engines). In all ﬁve tasks, the input\nvector includes the last four dialog utterances, each mapped to\na 50-dimensional vector. In the ﬁrst four tasks, the input in-\ncorporates the 50-dimensional vector encoding of the candidate\nutterance as well. Additionally, Task 1 and 2 input encodes\nentities in the dialog and candidate answer by means of two\n20-dimensional vectors each corresponding to the 20 different\n2For this baseline approach we would like to acknowledge David\nEcheverr´ıa Ciaurri who contributed to its design and implementation.\nTable 1: Example of dialog from Task 1 with the original utterances, the respective preprocessed representations and the dialog state\nconstructed during the runtime.\nTurn Original Utterance Preprocessed representation Dialog state\n1 hello hello {}\n2 hello what can i help you\nwith today\nhello what can i help you\nwith today\n{}\n3 i’d like to book\na table with a\nbusiness atmosphere with\nspanish cuisine\nCUISINE TYPE\nNONE\nNONE\nNONE\nATMOSPHERE\nCUISINE TYPE : spanish,\nATMOSPHERE : business\n4 i am on it i am on it CUISINE TYPE : spanish,\nATMOSPHERE : business\n5 <silence> silence CUISINE TYPE : spanish,\nATMOSPHERE : business\n6 where should it be where should it be CUISINE TYPE : spanish,\nATMOSPHERE : business\n7 find me one in bombay,\nparis will be too\ncomplicated\nCUISINE TYPE\nLOCATION\nNONE\nNONE\nATMOSPHERE\nCUISINE TYPE : spanish,\nATMOSPHERE : business,\nLOCATION : bombay\n8 how many people would be\nin your party\nhow many people would be\nin your party\nCUISINE TYPE : spanish,\nATMOSPHERE : business,\nLOCATION : bombay\n9 for eight people please CUISINE TYPE\nLOCATION\nNUM PEOPLE\nNONE\nATMOSPHERE\nCUISINE TYPE : spanish,\nATMOSPHERE : business,\nLOCATION : bombay,\nNUM PEOPLE : eight\n10 which price range are you\nlooking for\nwhich price range are you\nlooking for\nCUISINE TYPE : spanish,\nATMOSPHERE : business,\nLOCATION : bombay,\nNUM PEOPLE : eight\n11 expensive is tempting\nbut cheap may be more\nreasonable\nCUISINE TYPE\nLOCATION\nNUM PEOPLE\nDUBIOUS\nATMOSPHERE\nCUISINE TYPE : spanish,\nATMOSPHERE : business,\nLOCATION : bombay,\nNUM PEOPLE : eight\n12 whenever you’re ready whenever you’re ready CUISINE TYPE : spanish,\nATMOSPHERE : business,\nLOCATION : bombay,\nNUM PEOPLE : eight\n13 let’s do moderate price\nrange, and keep expensive\nprice range for another\nday\nCUISINE TYPE\nLOCATION\nNUM PEOPLE\nPRICE RANGE\nATMOSPHERE\nCUISINE TYPE : spanish,\nATMOSPHERE : business,\nLOCATION : bombay,\nNUM PEOPLE : eight,\nPRICE RANGE : moderate\nentities in the training data. For Task 3, we use instead a vec-\ntor with ﬁve components (the encodings for up to ﬁve names\nof restaurants that can be recommended by the system) and one\nscalar (the encoding for the name of the restaurant, if any, in the\ncandidate utterance). Similarly, for Task 4 we use a vector with\nthree components (the encodings of the name of the restaurant\nselected by the user and the corresponding address and phone\nnumber). The current implementation for the neural-network\nclassiﬁer described here only handles ﬁve different types of en-\ntities (thus, it is not applicable to requests based on six or more\ntypes of entities). An additional limitation is discussed in [17],\nwhere the feed-forward neural-network approach is shown to be\nunable to capture the functional dependency between a given\noutput and past inputs in the network.\nOur second baseline is based on end-to-end memory net-\nworks, recently proposed in [1]. This approach has exhibited\npromising results on natural language processing tasks such as\nquestion answering (see, e.g., [18]). Memory Networks [19]\noperate by writing and then iteratively reading from memory.\nThey compute multiple internal states using hops and the mem-\nory values are learned by soft-attention of user query with the\nprevious dialog utterances. The internal state, combined with\nFigure 2: Operation of the hierarchical classiﬁcation process.\nthe context is later used to predict the required response. The\nconcept of multiple computation hops allows to attend over dif-\nferent parts of context (previous dialog utterances) during each\nhop and hence captures an improved internal state for the overall\ndialog. In this implementation, the previous utterances and the\nuser query were passed to the network encoded in bag-of-words\nrepresentation, and the network learns the representations for\nthe vocabulary, using cross-entropy loss objective function for\nthe predictions.\nAn advantage of using end-to-end memory networks for the\nDSTC6 challenge is that they provide a common architecture\nfor all ﬁve tasks and require less ‘feature engineering than the\nfeed-forward neural network based classiﬁer and QDLM ap-\nproaches discussed above. In these experiments, we split the\ntraining data into two parts, 80% of it for training the network\nand the remaining 20% for testing the network trained.\n4.2. Discussion of results\nAccuracy results for the training data (all ﬁve tasks) obtained\nwith quantized-dialog language model (QDLM), feed-forward\nneural network (FFNN) and memory network (MN) are shown\nin Table 2. The last row in that table represents the average\nperformance. We had to choose one method for submitting our\nresults for the test data for the challenge. The decision for drop-\nping our MN method was justiﬁed by the fact that the average\naccuracy for this method, i.e., 0.948, was clearly lower than the\naverage accuracy of the other two techniques. Despite the fact\nthat QDLM and FFNN show comparable performance for the\ntraining data, we observed that QDLM is better suited to handle\nadditional parameters (as the ones present in the sets tst3 and\ntst4). Therefore, we only evaluated QDLM on the test data.\nThe results obtained are summarized in Table 3 for four\ntest data tst1, tst2, tst3 and tst4, with an average accuracy of\n0.981. The results for tst1 and tst2 are similar to what we ob-\ntained for the training set with accuracy values very close to\none. We conclude that out-of-vocabulary entities and restau-\nrants are handled successfully. QDLM shows lower accuracy\nin tst3 and tst4. While our technique can handle the API-call\ncandidates with additional entity type, it does not properly deal\nwith candidate utterances that query about the value of the addi-\ntional entity type. We are working on a solution to this problem\nby further quantization of entity types.\n5. Conclusions and future work\nIn this paper, we have introduced the Quantized-Dialog Lan-\nguage Model (QDLM) technique for constructing goal-oriented\nconversational systems. The method has been applied to the\nend-to-end goal-oriented dialog learning track of the sixth edi-\ntion of the Dialog System Technology Challenges (DSTC6).\nThe dataset in this track is a large collection of restaurant-\nreservation conversations. The challenge is constructed for the\ndesign of systems that can accurately select the correct utterance\nfrom a pool of candidate utterances that appropriately com-\npletes a given dialog.\nThe QDLM technique achieved around 98% candidate se-\nlection accuracy across 4 test sets each having 5000 conversa-\ntions. Most of the errors were in test sets 3 and 4 where the\ncandidate selection for dialogs with unseen entity types were\nnot properly handled. Since the submission of the results for\nthe challenge, we have made progress in this respect by further\nquantization in the candidate space for entity types to eliminate\nthe dependency on multiple entity types. We have also started\nworking on the integration of QDLM with sequence to sequence\nbased natural language generation methods and we will report\nresults in a future publication.\n6. References\n[1] A. Bordes and J. Weston, “Learning end-to-end goal-oriented\ndialog,” CoRR, vol. abs/1605.07683, 2016. [Online]. Available:\nhttp://arxiv.org/abs/1605.07683\n[2] J. D. Williams, K. Asadi, and G. Zweig, “Hybrid code networks:\npractical and efﬁcient end-to-end dialog control with supervised\nTable 2: Results for the training data obtained with the methods\nbased on the quantized-dialog language model (QDLM), with\nthe feed-forward neural network (FFNN) and with the memory\nnetwork (MN).\nTask QDLM FFNN MN\n1 0.999 0.995 (0.995) 0.992 (0.997)\n2 1.000 0.997 (0.996) 1.000 (1.000)\n3 1.000 0.999 (0.998) 0.932 (1.000)\n4 1.000 1.000 (0.999) 0.897 (1.000)\n5 1.000 0.996 (0.999) 0.920 (0.975)\nAverage 1.000 0.997 (0.999) 0.948 (0.999)\nNotes: For each task and method, the ﬁrst number is the accu-\nracy for the testing subset of dialogs and the second number (in\nparenthesis) is the accuracy for the training subset of dialogs.\nNo accuracy is reported for QDLM regarding the training sub-\nset since this technique, unlike the other two approaches, does\nnot explicitly perform a training iteration. The last row is the\naverage accuracy over the ﬁve tasks in the dataset.\nTable 3: Results for the test data obtained with the quantized-\ndialog language model approach.\nTask tst1 tst2 tst3 tst4\n1 1.000 1.000 0.866 0.882\n2 1.000 1.000 1.000 1.000\n3 0.998 0.997 1.000 1.000\n4 1.000 1.000 1.000 1.000\n5 0.986 0.985 0.957 0.953\nAverage 0.997 0.996 0.965 0.967\nNotes: A brief description of the four sets (tst1, tst2, tst3 and\ntst4) was given in Section 2. The accuracy obtained is given for\neach task and set. The last row is the average accuracy over the\nﬁve tasks in each of these sets.\nand reinforcement learning,” CoRR, vol. abs/1702.03274, 2017.\n[Online]. Available: http://arxiv.org/abs/1702.03274\n[3] Z. Wang and O. Lemon, “A simple and generic belief tracking\nmechanism for the dialog state tracking challenge: On the\nbelievability of observed information,” in Proceedings of the\nSpecial Interest Group on Discourse and Dialogue Conference\n(SIGDIAL 2013) , Metz, France, August 2013, p. 423–432.\n[Online]. Available: http://www.aclweb.org/anthology/W13-4067\n[4] L. Shang, Z. Lu, and H. Li, “Neural responding machine\nfor short-text conversation,” CoRR, vol. abs/1503.02364, 2015.\n[Online]. Available: http://arxiv.org/abs/1503.02364\n[5] S. J. Young, M. Gasic, B. Thomson, and J. D. Williams,\n“POMDP-based statistical spoken dialog systems: A review,”\nProceedings of the IEEE , vol. 101, no. 5, pp. 1160–1179,\n2013. [Online]. Available: https://doi.org/10.1109/JPROC.2012.\n2225812\n[6] I. Sutskever, O. Vinyals, and Q. V . Le, “Sequence to sequence\nlearning with neural networks,” inAdvances in neural information\nprocessing systems, 2014, pp. 3104–3112.\n[7] J. Li, W. Monroe, A. Ritter, M. Galley, J. Gao, and D. Juraf-\nsky, “Deep reinforcement learning for dialogue generation,”arXiv\npreprint arXiv:1606.01541, 2016.\n[8] I. V . Serban, A. Sordoni, Y . Bengio, A. C. Courville, and J. Pineau,\n“Building end-to-end dialogue systems using generative hierar-\nchical neural network models.” inAAAI, 2016, pp. 3776–3784.\n[9] R. T. Lowe, N. Pow, I. V . Serban, L. Charlin, C.-W. Liu, and\nJ. Pineau, “Training end-to-end dialogue systems with the ubuntu\ndialogue corpus,”Dialogue & Discourse, vol. 8, no. 1, pp. 31–65,\n2017.\n[10] J. Clark, I. Koprinska, and J. Poon, “A neural network based\napproach to automated e-mail classiﬁcation,” in Proceedings\nIEEE/WIC International Conference on Web Intelligence (WI\n2003), 2003, pp. 702–705.\n[11] R. Kiros, Y . Zhu, R. R. Salakhutdinov, R. Zemel, R. Urtasun,\nA. Torralba, and S. Fidler, “Skip-thought vectors,” in Advances\nin Neural Information Processing Systems, 2015, pp. 3294–3302.\n[12] P. F. Brown, P. V . Desouza, R. L. Mercer, V . J. D. Pietra, and J. C.\nLai, “Class-based n-gram models of natural language,”Computa-\ntional Linguistics, vol. 18, no. 4, pp. 467–479, 1992.\n[13] W. J. Heeringa, “Measuring Dialect Pronunciation Differences\nUsing Levenshtein Distance,” Ph.D. dissertation, University of\nGroningen, 2004.\n[14] T. W. Simpson, J. Peplinski, P. N. Koch, and J. K. Allen, “Meta-\nmodels for computer-based engineering design: Survey and rec-\nommendations,” Engineering with Computers , vol. 17, pp. 129–\n150, 2001.\n[15] S. Haykin, Neural Networks: A Comprehensive Foundation ,\n2nd ed. Prentice-Hall, 1998.\n[16] D. P. Kingma and J. Ba, “Adam: A method for stochastic opti-\nmization,” in Proceedings of the 3rd International Conference on\nLearning Representations (ICLR), 2014.\n[17] O. Vinyals, S. V . Ravuri, and D. Povey, “Revisiting recurrent neu-\nral networks for robust ASR,” in2012 IEEE International Confer-\nence on Acoustics, Speech and Signal Processing (ICASSP), 2012,\npp. 4085–4088.\n[18] J. Weston, A. Bordes, S. Chopra, and T. Mikolov, “Towards\nAI-complete question answering: A set of prerequisite toy\ntasks,” CoRR, vol. abs/1502.05698, 2015. [Online]. Available:\nhttp://arxiv.org/abs/1502.05698\n[19] J. Weston, S. Chopra, and A. Bordes, “Memory networks,”\nCoRR, vol. abs/1410.3916, 2014. [Online]. Available: http:\n//arxiv.org/abs/1410.3916",
  "topic": "Dialog box",
  "concepts": [
    {
      "name": "Dialog box",
      "score": 0.9441295862197876
    },
    {
      "name": "Computer science",
      "score": 0.8509976863861084
    },
    {
      "name": "Utterance",
      "score": 0.8359236717224121
    },
    {
      "name": "Dialog system",
      "score": 0.7121074199676514
    },
    {
      "name": "Conversation",
      "score": 0.6385566592216492
    },
    {
      "name": "Natural language processing",
      "score": 0.6015142202377319
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5719914436340332
    },
    {
      "name": "Context (archaeology)",
      "score": 0.550574541091919
    },
    {
      "name": "Language model",
      "score": 0.5024552345275879
    },
    {
      "name": "Reservation",
      "score": 0.48700839281082153
    },
    {
      "name": "Selection (genetic algorithm)",
      "score": 0.447139173746109
    },
    {
      "name": "Key (lock)",
      "score": 0.42838168144226074
    },
    {
      "name": "Human–computer interaction",
      "score": 0.3700677752494812
    },
    {
      "name": "Speech recognition",
      "score": 0.3231712579727173
    },
    {
      "name": "Linguistics",
      "score": 0.16533327102661133
    },
    {
      "name": "World Wide Web",
      "score": 0.10096865892410278
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Computer network",
      "score": 0.0
    }
  ]
}