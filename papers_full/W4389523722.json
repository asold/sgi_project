{
  "title": "Tuna: Instruction Tuning using Feedback from Large Language Models",
  "url": "https://openalex.org/W4389523722",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2099815881",
      "name": "Haoran Li",
      "affiliations": [
        "Singapore University of Technology and Design"
      ]
    },
    {
      "id": "https://openalex.org/A2128468363",
      "name": "Yiran Liu",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2101736369",
      "name": "Xingxing Zhang",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A1978197011",
      "name": "Wei Lu",
      "affiliations": [
        "Singapore University of Technology and Design"
      ]
    },
    {
      "id": "https://openalex.org/A2171151462",
      "name": "Furu Wei",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6631349028",
    "https://openalex.org/W4312091890",
    "https://openalex.org/W4362707064",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W2964190861",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4385570984",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4385571633",
    "https://openalex.org/W4321392130",
    "https://openalex.org/W4302010020",
    "https://openalex.org/W4362679631",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3035050380",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W4377297670",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W4311000038",
    "https://openalex.org/W4385570777",
    "https://openalex.org/W4225934689",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W4378509182",
    "https://openalex.org/W4391876619",
    "https://openalex.org/W2176263492",
    "https://openalex.org/W4385572845",
    "https://openalex.org/W4380136143",
    "https://openalex.org/W4378468563",
    "https://openalex.org/W4389519254",
    "https://openalex.org/W2963736842",
    "https://openalex.org/W4365211596",
    "https://openalex.org/W2970785793",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3196731672",
    "https://openalex.org/W2736601468",
    "https://openalex.org/W3176456866",
    "https://openalex.org/W3105214104",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4319793767",
    "https://openalex.org/W3205068155"
  ],
  "abstract": "Instruction tuning of open-source large language models (LLMs) like LLaMA, using direct outputs from more powerful LLMs such as Instruct-GPT and GPT-4, has proven to be a cost-effective way to align model behaviors with human preferences. However, the instruction-tuned model has only seen one response per instruction, lacking the knowledge of potentially better responses. In this paper, we propose finetuning an instruction-tuned LLM using our novel probabilistic ranking and contextual ranking approaches to increase the likelihood of generating better responses. Probabilistic ranking enables the instruction-tuned model to inherit the relative rankings of high-quality and low-quality responses from the teacher LLM. On the other hand, learning with contextual ranking allows the model to refine its own response distribution using the contextual understanding ability of stronger LLMs. Furthermore, we apply probabilistic ranking and contextual ranking sequentially to the instruction-tuned LLM. The resulting model, which we call Tuna, consistently improves the performance on Super Natural Instructions (119 test tasks), LMentry (25 test tasks), Vicuna QA, and can even obtain better results than several strong reinforcement learning baselines. Our code and data are available at https://github.com/microsoft/LMOps.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 15146–15163\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nTuna: Instruction Tuning using Feedback from Large Language Models\nHaoran Li1,†,Yiran Liu3,‡,Xingxing Zhang2,Wei Lu1,Furu Wei2\n1StatNLP Research Group, Singapore University of Technology and Design\n2Microsoft Research Asia, 3Tsinghua University\nhaoran2_li@mymail.sutd.edu.sg, wei_lu@sutd.edu.sg\nliu-yr21@mails.tsinghua.edu.cn, {xizhang,fuwei}@microsoft.com\nAbstract\nInstruction tuning of open-source large lan-\nguage models (LLMs) like LLaMA, using di-\nrect outputs from more powerful LLMs such\nas Instruct-GPT and GPT-4, has proven to\nbe a cost-effective way to align model behav-\niors with human preferences. However, the\ninstruction-tuned model has only seen one re-\nsponse per instruction, lacking the knowledge\nof potentially better responses. In this pa-\nper, we propose finetuning an instruction-tuned\nLLM using our novel probabilistic ranking\nand contextual rankingapproaches to increase\nthe likelihood of generating better responses.\nProbabilistic ranking enables the instruction-\ntuned model to inherit the relative rankings of\nhigh-quality and low-quality responses from\nthe teacher LLM. On the other hand, learn-\ning with contextual ranking allows the model\nto refine its own response distribution using\nthe contextual understanding ability of stronger\nLLMs. Furthermore, we apply probabilistic\nranking and contextual ranking sequentially\nto the instruction-tuned LLM. The resulting\nmodel, which we call Tuna, consistently im-\nproves the performance on Super Natural In-\nstructions (119 test tasks), LMentry (25 test\ntasks), Vicuna QA, and can even obtain better\nresults than several strong reinforcement learn-\ning baselines. Our code and data are available\nat https://github.com/microsoft/LMOps.\n1 Introduction\nLarge language models (LLMs) have made signif-\nicant progress by scaling up model size and data\nsize (Peters et al., 2018; Devlin et al., 2019; Rad-\nford et al., 2019; Brown et al., 2020; OpenAI, 2023)\nfor unsupervised pre-training and subsequently ap-\nplying reinforcement learning from human feed-\nback (RLHF) to align model responses with human\npreferences (Christiano et al., 2017; Ouyang et al.,\n2022). More recently, instruction tuning (Wei et al.,\n†, ‡ Work done during internship at MSRA.\nInstruction-\nTuned LLMs\n R1, -0.9\nText-Davinci\n003\nLearn the \nRanking\nInstruction\nResponses\nR3>R1>R2\nGPT-4 \nLearn the Ranking\nContextual  Ranking\nInstruction\n R1\n R2\n R3\nResponses\nInstruction-\nTuned LLMs\nR3>R1>R2\nProbabilistic Ranking\nProbabilistic \nRanking\nContextual \nRanking\nInstruction-\nTuned LLMs\n Tuna\nContextual Prompt\n R2, -1.2\n R3, -0.7\nFigure 1: The finetuning process using probabilistic\nranking (top), contextual ranking (middle), and a com-\nbination of both (bottom).\n2022) with Self-Instruct algorithm (Wang et al.,\n2022a) has emerged as a cost-effective method for\naligning with human preferences. In this approach,\nopen LLMs like LLaMA (Touvron et al., 2023)\ncan be finetuned on instruction-following data gen-\nerated by OpenAI GPT using the Self-Instruct al-\ngorithm. The Alpaca model (Taori et al., 2023)\nexemplifies this technique, which enables close\nalignment with human preferences while reducing\ndependence on human-labeled data.\nHowever, instruction tuning offers only a broad\nguideline for the base LLMs to transition from\n“next token prediction” to a more interactive,\ninstruction-following style. As a result, the model\nmay learn some superficial features or styles from\nthe instruction data but still lacks a deeper under-\nstanding of what constitutes a preferred response.\nFor instance, when given a question like “Give\n15146\nthree tips for staying healthy”, a base LLM may\ngenerate fluent yet undesirable continuations, while\nan instruction-tuned LLM could offer three general\ntips. Humans might prefer more detailed tips over\ngeneral tips, but such tips are less likely to be sam-\npled since they have lower likelihood within the\ncurrent model distribution. This can be attributed\nto the fact that they are either unseen during in-\nstruction tuning or hard to be sampled due to the\nexposure bias (Ranzato et al., 2015).\nTo address this, we propose further finetuning\nof an instruction-tuned LLM to discern the qual-\nity of multiple responses more precisely, using our\nnovel probabilistic ranking (Sec. 2.2; Fig. 1 top)\nand contextual ranking (Sec. 2.3; Fig. 1 mid-\ndle) approaches. Probabilistic ranking enables the\ninstruction-tuned LLM to inherit the high-quality\nand low-quality responses as well as their rela-\ntive rankings from the teacher LLM (e.g., text-\ndavinci-003). In contrast, contextual ranking aims\nto re-balance the instruction-tuned model’s own re-\nsponse distribution with the help of stronger LLMs\n(e.g., GPT-4), mitigating the exposure bias issue.\nWe apply probabilistic ranking and contextual\nranking sequentially to an instruction-tuned model,\ni.e., Alpaca (Taori et al., 2023), resulting in a model\ncalled Tuna (Sec. 2.4; Fig. 1 bottom). We evaluate\nTuna on various benchmarks, including Super Nat-\nural Instructions (Wang et al., 2022b), which con-\ntains 119 diverse test tasks; LMentry (Efrat et al.,\n2022), comprising 25 tasks to assess the basic ca-\npabilities and robustness of LLMs; and Vicuna QA\n(Chiang et al., 2023) which evaluates the model’s\nability to answer a diverse set of questions with the\nassistance of GPT-4. Experimental results demon-\nstrate that the Tuna model not only consistently\noutperforms the standard instruction-tuned mod-\nels on all benchmarks, but also surpasses several\nstrong RLHF baselines (Ouyang et al., 2022).\nTo summarize, our contributions are as follows:\n• We propose probabilistic rankingand contex-\ntual ranking, which enable the instruction-\ntuned model to distinguish high-quality and\nlow-quality responses and assign higher prob-\nability to the former accordingly.\n• The Tuna model, obtained by sequentially\napplying probabilistic ranking and contex-\ntual ranking on an instruction-tuned LLM,\nachieves better results than several strong\nbenchmarks, including RLHF models;\n• Our model, data and code will be released to\nfacilitate future research.\n2 Methodology\nIn this section, we describe how to obtain ourTuna\nmodel using the feedback from LLMs. We first\ndescribe the vanilla instruction tuning. We then\nintroduce our probabilistic ranking and contextual\nranking approaches. Lastly, we describe how to\nintegrate both ranking approaches.\n2.1 Instruction Tuning\nLLMs like GPT-3 (Brown et al., 2020) have been\ntrained on a massive text corpus using maximum\nlikelihood estimation (MLE):\nLMLE(y) =−1\n|y|\n∑\nt\nlog p(yt|y<t; θ), (1)\nwhere θ represents the parameters of the base\nmodel. The pre-training objective function compels\nthe model to predict the next token yt given its pre-\nfix y<t = [y0,y1,...,y t−1]. A sufficiently-trained\nLLM can generate fluent continuations given al-\nmost any prefix. However, the generated continua-\ntions may not align well with human preferences.\nAs the primary goal of an LLM is to assist humans,\nit becomes essential to encourage the generation of\ncontent that follows human instructions and aligns\nwith human preferences. The current dominant\napproach to enhance LLMs’ instruction-following\nability is called instruction tuning(Mishra et al.,\n2021; Wei et al., 2022; Taori et al., 2023), which\nfinetunes the base LLMs in a supervised manner\non instruction-response pairs {i,r}(where iis an\ninstruction and ris its response) using MLE:\nLMLE(i,r) =−1\n|r|log p(r|i; θ′), (2)\nwhere θ′ represents the parameters of the\ninstruction-tuned model. After instruction tuning,\nwe expect the model distribution p(·|i; θ′) to allo-\ncate higher probabilities to proper responses like r\nrather than undesirable continuations.\nNote that the responses in instruction-response\npairs can either be annotated by humans 1 or gen-\nerated by strong LLMs, such as Instruct-GPT or\nGPT-4 (Wang et al., 2022a). A prevalent and cost-\neffective approach for generating instruction tun-\ning data is the Self-Instruct algorithm (Wang et al.,\n1https://huggingface.co/datasets/databricks/\ndatabricks-dolly-15k\n15147\n2022a). Specifically, it uses a strong LLM, e.g.,\ntext-davinci-003, to create instructions based on a\nfew seed instructions, and then generates a single\nresponse for each instruction using the same LLM.\n2.2 Probabilistic Ranking\nInstruction tuning with the data generated by the\nSelf-Instruct algorithm is essentially a form of\nsequence-level distillation (Kim and Rush, 2016).\nThe rationale behind this class of distillation\nmethod is that the current commercial LLMs have\nsignificantly better capabilities than their open-\nsource counterparts. Instead of learning from the\nsingle-response data, our probabilistic rankingap-\nproach leverages the relative rankings of multiple\nresponses based on the teacher model’s probabili-\nties for better pseudo label distillation (see Fig. 1\ntop).\nLet r denote the original response for in-\nstruction i in the instruction tuning dataset.\nWe query strong (teacher) LLMs, such as\ntext-davinci-003, to generate N new responses\nfor i. Let r(0),r(1),...,r (N−1) denote these new\nresponses, and p(r(0)|i),p(r(1)|i),...,p (r(N−1)|i)\ndenote their probabilities. While the teacher LLMs\nare expected to produce responses of comparable\nquality on average, there will inevitably be some\nvariation in the quality of the generated responses.\nThis inherent variability manifests itself in vari-\nous aspects, such as differences in accuracy (Wang\net al., 2023a), response length, and level of details\nprovided (Wang et al., 2023b).\nIntuitively, if a model is perfectly distilled,\nthe relative probabilities assigned to two samples\nshould be the same as those of the teacher model.\nSpecifically, let p(r(j)|i; θ′) and p(r(k)|i; θ′) de-\nnote the probabilities of r(j) and r(k) w.r.t. the\nstudent model. If p(r(j)|i) > p(r(k)|i), then\np(r(j)|i; θ′) > p(r(k)|i; θ′). We use the follow-\ning normalized log-likelihood as the teacher model\nquality score to account for differences in response\nlengths:\ns(i,r(k)) =log p(r(k)|i)\n|r(k)|β , k = {0,...,N −1}\n(3)\nwhere |r(k)|is the length of r(k) and βrepresents\nthe length penalty.\nWe then rank those responses in decreasing or-\nder based on s(i,r(k)). The resulting instruction-\nresponse pairs become {i,r,(r[0],...r[N−1])},\nwhere i,r are from the original instruction tuning\ndata, and r[j] is considered to have better quality\nthan r[k], if j < k. Once we obtain the ranked\nresponses, we can encourage our model to learn\nfrom these rankings using a pairwise ranking ob-\njective, which has been successfully employed in\nprevious work (Zhong et al., 2020; Liu et al., 2022;\nZhang et al., 2022; Zhao et al., 2023). The ranking\nobjective function is as follows:\nLrank =\n∑\n0≤j<k≤N−1\nLj,k\nrank (4)\nLj,k\nrank = max\n(\n0,vk\nθ′ −vj\nθ′ + m×(k−j)\n)\n,j <k\n(5)\nwhere vk\nθ′ = 1\n|r[k]|log p\n(\nr[k]|i; θ′)\n, m >0 is the\nmargin hyper-parameter. The ranking loss, Lrank,\naims to teach the model to distinguish good re-\nsponses from bad ones based on the teacher LLM’s\nperspective. In addition to Lrank, we also apply\na cross-entropy loss on the original response as\nregularization:\nL= Lrank + λLMLE, L MLE = 1\n|r|log p(r|i; θ′)\n(6)\nwhere ris the original response, andλ> 0 controls\nthe importance of LMLE, which helps prevent over-\noptimization of the ranking loss.\nAfter learning with probabilistic ranking, the\nmodel can better assign probabilities to superior\nand inferior responses.\n2.3 Contextual Ranking\nDuring the instruction tuning or the probabilistic\nranking stage, the model is finetuned to generate a\ngood rgiven an instruction i. However, given the\nsame iduring inference, the model may still gen-\nerate a relatively low-quality response r′. This is\nrelated to the exposure bias problem (Ranzato et al.,\n2015), where the model fails to generate rdue to\naccumulated errors during the auto-regressive gen-\neration process. To address this issue, we use our\ncontextual ranking approach to refine the distri-\nbution of responses generated by the model itself,\nassigning higher probabilities to better responses\nwith the help of strong LLMs (Fig. 1 middle), thus\nalleviating exposure bias (Ranzato et al., 2015).\nFor each instruction, we first sample N re-\nsponses from the instruction-tuned model itself, i.e.,\nr(0),r(1),...,r (N−1) ∼p(·|i; θ′). We hope the sam-\nples to be diverse enough so that better responses\nare more likely to appear in the sampled results.\n15148\nTo ensure diversity, we impose a constraint on the\nROUGE-L (Lin, 2004) score between each pair of\nresponses, requiring it to be less than a threshold\nτ. If the ROUGE-L score exceeds τ, we increase\nthe sampling temperature and resample another re-\nsponse. If multiple trials still result in a ROUGE-L\nscore above τ, we retain the least similar response\nfrom the trials. After obtaining N responses, we\nleverage the contextual understanding ability of\ncommercial LLMs, such as GPT-4 (OpenAI, 2023),\nto rank them based on various aspects. The ranking\nprocess consists of multiple steps. First, we ask\nGPT-4 to assess whether the instruction requires\nan open-ended answer (e.g., story generation) or\na close-ended answer (e.g., solving a math prob-\nlem). We then request GPT-4 to generate its own\nresponse as a reference. Next, GPT-4 compares\nthe reference response with the N responses from\ndifferent aspects and assign scores to each response.\nFor open-ended instructions, GPT-4 evaluates rel-\nevance (score 0-5), level of details /justification\n(score 0-5), and accuracy (score 0-5) of the model\nresponses compared to its reference response. For\nclose-ended instructions, the evaluation criteria are\naccuracy (score 0-5), level of details/justification\n(score 0-5), and clarity (score 0-5). Finally, GPT-4\nranks responses in decreasing order based on the\nsum of their scores (see Appendix E for our com-\nplete prompt). We also manually evaluated GPT-4\nrankings, which have achieved a strong correlation\nwith human judgements (see Appendix G, H).\nAs in Sec. 2.2, the resulting instruction tun-\ning dataset becomes {i,r,(r[0],...r[N−1])}. Note\nthat the r[k],0 ≤k ≤N −1, is derived from the\ninstruction-tuned model itself. Lastly, we use the\nsame objective function as in Eq. 6 to encourage\nthe model to assign higher probabilities to better\nresponses.\n2.4 Integrating Probabilistic and Contextual\nRanking\nGiven an instruction-tuned model, there are sev-\neral options for further finetuning: 1) learning with\nprobabilistic ranking alone; 2) learning with con-\ntextual ranking alone; 3) learning with probabilistic\nranking followed by contextual ranking (see Fig.\n1 bottom). We refer to the models finetuned with\nthese three methods as Tunap, Tunac, and Tuna,\nrespectively.\nTo optimally integrate both probabilistic rank-\ning and contextual ranking techniques, it is recom-\nmended to first obtain a Tunap model, followed by\napplying contextual ranking to Tunap’s response\ndistribution, resulting in the Tuna model. There\nare two reasons for this choice. First, although it\nis beneficial to learn the ranking of different re-\nsponses from the teacher LLM’s perspective (prob-\nabilistic ranking), the model might not fully capture\nthe teacher’s ranking knowledge due to its limited\ncapacity. Second, contextual ranking enables the\nmodel to better adapt to its own capacity by work-\ning with the model’s own generations. By gener-\nating its own responses, the model can finetune its\nunderstanding with the help of stronger LLMs and\nmore effectively produce responses that are both\ncloser to human preferences and compatible with\nits capacity constraints, alleviating the exposure\nbias issue (Ranzato et al., 2015).\n3 Experiments\n3.1 Model and Data\nIn our experiments, we use a 7B LLaMA model\n(Touvron et al., 2023) as the base model. The\ninstruction tuning data is sourced from Alpaca\n(Taori et al., 2023), which consists of 52K instruc-\ntions paired with responses that are generated by\ntext-davinci-003 using the Self-Instruct algo-\nrithm (Wang et al., 2022a). We perform instruction\ntuning on 52K Alpaca data using recommended hy-\nperparameters, such as a learning rate of 2e-5 and\nthe AdamW optimizer (0.9,0.999) (Loshchilov\nand Hutter, 2019).2 For simplicity, we also refer to\nthe instruction-tuned model as Alpaca.\nFor probabilistic ranking, we input 52K instruc-\ntions from Alpaca dataset into text-davinci-003\nto produce N = 4responses per instruction along\nwith their log-likelihoods3, with an inference tem-\nperature of 1. We calculate response scores using\nEq. 3 with β being 1.3, and rank the responses\naccordingly. Subsequently, we finetune the Alpaca\nmodel for 1 epoch with a learning rate 1e-5, mar-\ngin m= 0.1, and cross entropy regularizer weight\nλ= 1.0. We denote the model trained exclusively\nwith probabilistic ranking as Tunap.\nFor contextual ranking, we sample N = 4 re-\nsponses from the Alpaca model with temperature\nT = 1for each instruction. To avoid similar gen-\nerations, we ensure the pairwise ROUGE-L (Lin,\n2004) between responses is less than τ = 0.8. Oth-\n2https://github.com/AetherCortex/Llama-X\n3GPT-4 is more powerful but it does not return log-\nlikelihoods.\n15149\nSuper NI LMentry Vicuna QA\n0-shot 2-shot LMentry Score Win Lose Tie\nLLaMA 11.0 23.6 26.3 4% 92% 4%\nT5-LM 11B - 30.2 20.6 - - -\nT0 11B - 32.3 31.6 - - -\nInstructGPT 175B - 52.1 48.4 - - -\nAlpaca 36.0 44.5 31.4 - - -\n+ PPO-sim 31.9 (-4.1) 37.5 (-7.0) 27.8 (-3.6) 79% 16% 5%\n+ PPO-sim-GPT4-20K 37.1 (+1.1) 44.9 (+0.4) 27.8 (-3.6) 74% 22% 4%\nTunap 39.4 (+3.4) 43.9 (-0.6) 35.0 (+3.6) 68% 27% 5%\nTunac 37.7 (+1.7) 46.6 (+2.1) 32.2 (+0.8) 74% 20% 6%\nTunac (PRM) 34.2 (-1.8) 40.1 (-4.4) 32.2 (+0.8) 75% 19% 6%\nTuna 38.7 (+2.7) 45.0 (+0.5) 34.7 (+3.3) 86% 10% 4%\nTable 1: Performance comparison of different models on Super NI, LMentry and Vicuna QA. The numbers in bold\nindicate the top-2 results. The numbers in parentheses indicate the performance differences compared to Alpaca.\nThe results of T5-LM 11B (Raffel et al., 2020), T0-11B (Sanh et al., 2022), InstructGPT 175B (Ouyang et al., 2022)\nare taken from Wang et al. (2022b); Efrat et al. (2022). The RLHF baselines PPO-sim and PPO-sim-GPT4-20K,\nwhich apply the PPO algorithm (Schulman et al., 2017), are taken from Dubois et al. (2023).\nModel Alpaca Alpaca+PPO-sim Tuna p Tunac Tuna\nScore 2.13 2.95 ∗ 2.98∗ 3.15∗ 3.80∗†\nTable 2: Human evaluation on Vicuna QA. * denotes that the model is significantly (p< 0.01) better than Alpaca,\nwhile †denotes that Tuna is significantly (p< 0.01) better than other models.\nerwise, we remove the similar response, increase\nthe temperature by 0.1, and resample. If three tri-\nals fail to produce unique enough responses, we\nkeep the least similar one. We then employ GPT-\n4 to rank responses for the first 13K instruction\ndata with the GPT-4 inference temperature to be 0.\nThe contextual ranking prompt is shown in Table\n9.4 The finetuning hyperprameters follow those\nof probabilistic ranking. We refer to the model\ntrained on 13K contextual ranking data of the Al-\npaca model as Tunac.\nFurthermore, we use the 13K GPT-4 ranking\ndata to train a proxy ranking model (PRM) based on\nStableLM-3B.5 The PRM is employed to re-rank\nAlpaca’s responses on 52K instructions. We refer\nto the Alpaca model trained with 52K ranking data\ntotally generated by the PRM as Tunac (PRM).\nLastly, we also collect 13K GPT-4 contextual\nranking data based on Tunap’s responses instead\nof Alpaca’s. We refer to the model finetuned on\nTunap as Tuna.\nWe also included strong reinforcement learning\n4The cost of calling OpenAI API is listed in Appendix B.\n5https://github.com/Stability-AI/StableLM\nbaselines for comparison (i.e., PPO-sim and PPO-\nsim-GPT4-20K models from AlpacaFarm (Dubois\net al., 2023)).6\n3.2 Evaluation\nSuper Natural Instruction (Super NI)Super\nNI (Wang et al., 2022b) contains 119 test tasks\ndesigned to evaluate a model’s cross-task general-\nization ability. It includes a variety of classification\nand generation tasks, such as textual entailment\nand title generation. We report both 0-shot and\n2-shot performance, where 0-shot provides only\nan instruction (referred to as “definition” in their\nliterature) and 2-shot offers two additional positive\nexamples. The evaluation metric for all 119 tasks\nis ROUGE-L (Lin, 2004), which is strongly corre-\nlated with human evaluation with a Pearson coef-\nficient of 0.998 according to Wang et al. (2022b).\nGreedy decoding is applied during inference.\nLMentry LMentry (Efrat et al., 2022) is a bench-\nmark that primarily focuses on the accuracy and ro-\n6We also trained our own RLHF model, which is not as\ngood as the ones in AlpacaFarm. The comparison can be\nfound in Appendix I\n15150\nbustness aspects of LLMs’ generations. It contains\n25 short tasks that are trivial to humans but chal-\nlenging for LLMs. The final metric is LMentry\nscore, which is calculated by multiplying its mean\naccuracy on 25 tasks with the robustness score. The\nmodel will be evaluated in a 0-shot manner, and\ngreedy decoding is applied during inference.\nVicuna QA Vicuna QA (Chiang et al., 2023)\ncomprises 80 test questions across 9 categories\nthat measure an LLM’s ability to generate rele-\nvant, detailed and accurate responses and it has\nbeen widely adopted in many works. Instead of\nhaving a ground truth for evaluation, it conducts\npairwise comparisons with the help of GPT-4 (Ope-\nnAI, 2023). It prompts GPT-4 to compare the out-\nputs of our models to the Alpaca model. We report\nthe win/lose/tie rate against the Alpaca model.\nHuman Evaluation Additionally, we conduct\nhuman evaluations on Vicuna QA. Specifically, re-\nsponses from five anonymous systems, namely Al-\npaca, Alpaca + PPO-sim, Tuna, Tunap, and Tunac,\nwere randomly shuffled and presented to annota-\ntors who were then asked to rank these outputs.\nThe scoring was designed such that the i-th ranked\nsystem receives a score of 6 −i, meaning the best-\nranked system receives a score of 5, and the worst-\nranked system receives a score of 1. Each question\nwas annotated by two different annotators, and the\nscore was averaged.\n3.3 Main Results\nThe main results are presented in Table 1. Af-\nter instruction tuning, Alpaca demonstrates sig-\nnificant performance improvements over LLaMA\non all three benchmarks. This highlights the suc-\ncessful transition from the “next token predic-\ntion” paradigm to a more interactive instruction-\nfollowing paradigm.\nFurthermore, both contextual and probabilis-\ntic ranking enhance performance across all three\nbenchmarks. Specifically, Tunac exhibits more im-\nprovement on the Super NI 7 2-shot results while\nTunap performs better on Super NI 0-shot and\nLMentry, narrowing the performance gap with\nmuch larger models like InstructGPT-175B. Since\nthe 2-shot input is longer than 0-shot, we conjec-\nture that contextual ranking might be more benefi-\ncial for longer sequence generation than probabilis-\n7ROUGE is used as the default metric on Super NI. How-\never, our results follow the same trend using BERTScore (see\nAppendix J).\ntic ranking. On the Vicuna QA benchmark, both\nTunap and Tunac outperform Alpaca significantly\non nearly 70% of the questions, as evaluated by\nGPT-4. Upon comparison with the RLHF base-\nlines, Tunap and Tunac consistently demonstrate\nsuperior performances on both the Super NI and\nLMentry benchmarks. However, when it comes to\nthe Vicuna QA benchmark, their performance is\nmarginally lower than that of the RLHF baselines.\nMoreover, Tuna achieves the best performance on\nVicuna QA while maintaining competitive scores\non Super-NI and LMentry. Human results on Vi-\ncuna QA (see Table 2) also confirm that humans\nprefer the responses from our models.\nFurthermore, Tunac (PRM) demonstrates com-\nparable performance to Tunac on Vicuna QA and\nLMentry, but it underperforms both Tunac and Al-\npaca on Super NI. This suggests that although the\nPRM has primarily learned ranking from the GPT-\n4 contextual ranking data, it also introduces some\nnoise during the learning process. Overall, it is\nmore effective to learn directly from GPT-4 contex-\ntual ranking data.8\n3.4 Ablation Study\nIn this subsection, we delve deeper into the per-\nformance of our approach by examining several\naspects, including: (a) the effect of more responses\nin instruction tuning, (b) the order of applying two\nranking methods, (c) the influence of the cross en-\ntropy regularization, (d) the amount of probabilistic\nranking data, and (e) the risks of GPT-4 evaluation.\nMore Responses in Instruction TuningWe ex-\nplore whether Tuna’s effectiveness is solely due\nto the increased response data by examining the\nimpact of adding more responses per instruction\nduring instruction tuning. We create a new model,\nAlpaca-Mul, by adding four extra responses from\nthe probabilistic ranking dataset to the Alpaca\ndataset and fine-tuning the LLaMA model using\nEq. 2. The results are presented in Table 3.\nUpon evaluation on Super NI, Alpaca-Mul’s per-\nformance is nearly identical to that of Alpaca but\nfalls short when compared to the 0-shot settings\nof Tunap and Tuna. On LMentry, Alpaca-Mul out-\nperforms Alpaca, yet it still does not reach the\nperformance levels of Tunap and Tuna. Interest-\ningly, in the Vicuna QA task, Alpaca-Mul slightly\nunderperforms compared to Alpaca.\n8Experiments with more PRMs can be found in App. D.\n15151\nSuper NI LMentry Vicuna QA\n0-shot 2-shot LMentry Score Win Lose Tie\nAlpaca 36.0 44.5 31.4 - - -\nAlpaca-Mul 34.7 (-1.3) 45.7 (+1.2) 33.9 (+2.5) 42% 53% 5%\nTunap 39.4 (+3.4) 43.9 (-0.6) 35.0 (+3.6) 68% 27% 5%\nTuna 38.7 (+2.7) 45.0 (+0.5) 34.7 (+3.3) 86% 10% 4%\nTunac 37.7 (+1.7) 46.6 (+2.1) 32.2 (+0.8) 74% 20% 6%\nTunacp-13K 35.7 (-0.3) 44.0 (-0.5) 33.5 (+2.1) 58% 37% 5%\nTunacp-39K 34.8 (-1.2) 43.4 (-1.1) 35.4 (+4.0) 46% 48% 6%\nTunacp-52K 35.0 (-1.0) 42.6 (-1.9) 33.8 (+2.4) 51% 41% 8%\nmix-Tuna-52K 37.7 (+1.7) 44.2 (-0.3) 30.0 (-1.4) 70% 23% 7%\nmix-Tuna-104K 36.0 (+0.0) 40.0 (-4.5) 32.6 (+1.2) 55% 40% 5%\nTable 3: Different combinations of probabilistic ranking data and contextual ranking data. The numbers in bold\nrepresent the top-2 results. The numbers in parentheses represent the performance difference compared to Alpaca.\nThese findings suggest that merely adding more\nresponses without differentiating them does not\nnecessarily lead to improved response generation.\nOverall, the results of Alpaca-Mul indicate that\nTuna’s superior performance cannot be solely at-\ntributed to the availability of more response data.\nIntegration Order An alternative approach to\nTuna involves first training the Tunac model, and\nsubsequently continuing training the Tunac model\nwith probabilistic ranking data. The resulting\nmodel is referred to as Tunacp.\nWe explore various strategies for trainingTunacp:\n1) finetuning Tunac with the first 13K probabilis-\ntic ranking data (Tunacp-13K); 2) finetuing Tunac\nmodel with last 39K probabilistic ranking data\n(Tunacp-39K); 3) finetuning Tunac model with\n52K probabilistic ranking data (Tunacp-52K). Ad-\nditionally, we also try to finetune original Alpaca\nmodel with a combination of 13K GPT-4 contextual\nranking data (generated from Alpaca model’s re-\nsponses) and the last 39K probabilistic ranking data\n(mix-Tuna-52K). We also finetune Alpaca model\nwith 52K contextual ranking data (13K GPT-4 con-\ntextual ranking + 39K ranking-model-generated\ndata) plus 52K probabilistic ranking data (mix-\nTuna-104K). The training details are listed in the\nAppendix C. The results are listed in Table 3.\nNone of the combination strategies consistently\noutperform both Tunap and Tunac across the Vi-\ncuna QA and Super NI benchmarks. On LMen-\ntry, however, finetuning Tunac with probabilistic\nranking data is beneficial, especially when no du-\nplicate data is present (Tunacp-39K). This suggests\nrank 1 2 3 4\ncontextual ranking 66.4 55.2 51.4 44.8\nprob. ranking 55.8 54.3 52.5 49.4\nPRM 69.2 57.8 50.9 44.7\nTable 4: The average ranking lengths of contextual rank-\ning data, probabilistic ranking data and the data gener-\nated by the proxy ranking model (PRM).\nthat shorter probabilistic ranking data are beneficial\nwhen high accuracy and robustness are top priority.\nInterestingly, Tunacp is not comparable to Tuna,\nindicating that the order in which the model is\ntrained with contextual and probabilistic ranking\nmatters. One plausible explanation is that both the\noriginal Alpaca data and the probabilistic ranking\ndata are generated by text-davinci-003, while\nTunac has significantly shifted the model distribu-\ntion by re-ranking the Alpaca model’s responses,\nmaking it challenging to finetune Tunac with prob-\nabilistic ranking data again.\nThe Effect of Cross Entropy RegularizerWe\nexamine the influence of the weight λof the cross\nentropy regularizer in Eq. 6 on performance by\nvarying λacross different values: {0,0.1,1,5,10}\nwhile training the Tunac model. Fig. 2 illustrates\nthat as λincreases, the performance on accuracy-\noriented benchmarks such as Super NI and LMen-\ntry improves, while the performance on open ques-\ntions does not necessarily follow the same trend.\nOn one hand, this finding suggests that with a small\nλ, learning with contextual ranking may induce\n15152\n/uni00000016/uni00000013\n/uni00000017/uni00000013\n/uni00000018/uni00000013/uni00000035/uni00000032/uni00000038/uni0000002a/uni00000028/uni00000010/uni0000002f\n/uni00000013/uni00000010/uni00000056/uni0000004b/uni00000052/uni00000057\n/uni00000015/uni00000010/uni00000056/uni0000004b/uni00000052/uni00000057\n/uni00000015/uni00000018\n/uni00000016/uni00000013\n/uni00000016/uni00000018\n/uni00000017/uni00000013/uni0000002f/uni00000030/uni00000048/uni00000051/uni00000057/uni00000055/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000014/uni00000014/uni00000011/uni00000013/uni00000018/uni00000011/uni00000013/uni00000014/uni00000013/uni00000011/uni00000013\n/uni00000013\n/uni00000018/uni00000013\n/uni00000014/uni00000013/uni00000013/uni00000035/uni00000044/uni00000057/uni00000048\n/uni0000003a/uni0000004c/uni00000051\n/uni0000002f/uni00000052/uni00000056/uni00000048\n/uni00000037/uni0000004c/uni00000048\nFigure 2: The effect of varying the weight λof cross en-\ntropy regularization in Eq. 6 on Tunac. The win/lose/tie\nrate on Vicuna is computed against Alpaca.\nlong and detailed answers, but those answers are\nnot always accurate. On the other hand, it implies\nthat accuracy-oriented benchmarks and open QA\nbenchmarks are complementary, and researchers\nshould consider more diverse test cases to thor-\noughly evaluate a model (Wang et al., 2023b).\nThe Amount of Probabilistic Ranking Data\nWe investigate the impact of varying the amount\nof probabilistic ranking data used for finetuning\nthe Tunap model by testing different data sizes, i.e.,\n{0,13000,24000,52000}. 0 refers to the Alpaca\nmodel. The results, shown in Fig. 3, reveal that for\nprobabilistic ranking, 13K data points are sufficient\nfor Super NI and LMentry, while Vicuna QA re-\nquires 24K data points. We conjecture that this satu-\nration phenomenon can be attributed to two reasons.\nFirst, 52K Alpaca instructions generated by Self-\nInstruct algorithm are not diverse enough, as new\ninstructions are produced by text-davinci-003\nusing prompt instructions sampled from a limited\nseed task pool. Second, instruction tuning itself\nmay only require a limited amount of data to per-\nform behavior cloning, as discussed in Zhou et al.\n(2023). Thus, we can further reduce the cost of\nprobabilistic ranking data generation by half.\nThe Risks in GPT-4 EvaluationWe present evi-\ndence that evaluating a model on open QA with the\nhelp of GPT-4 may be risky. Table 4 displays the\nranking length of our proxy ranking model (PRM).\nIt shows that the PRM has inherited GPT-4 rank-\ning’s bias towards longer outputs (Li et al., 2023).\n/uni00000016/uni00000013\n/uni00000017/uni00000013\n/uni00000018/uni00000013/uni00000035/uni00000032/uni00000038/uni0000002a/uni00000028/uni00000010/uni0000002f\n/uni00000013/uni00000010/uni00000056/uni0000004b/uni00000052/uni00000057\n/uni00000015/uni00000010/uni00000056/uni0000004b/uni00000052/uni00000057\n/uni00000016/uni00000013\n/uni00000016/uni00000018\n/uni00000017/uni00000013/uni0000002f/uni00000030/uni00000048/uni00000051/uni00000057/uni00000055/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048\n/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000013/uni00000013\n/uni00000027/uni00000044/uni00000057/uni00000044/uni00000003/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055\n/uni00000013\n/uni00000018/uni00000013/uni00000035/uni00000044/uni00000057/uni00000048\n/uni0000003a/uni0000004c/uni00000051\n/uni0000002f/uni00000052/uni00000056/uni00000048\n/uni00000037/uni0000004c/uni00000048\nFigure 3: The effect of varying the number of proba-\nbilistic ranking data on Tunap.\nHowever, as we discussed in Sec. 3.3, the data\ngenerated by the PRM is not as good as the orig-\ninal 13K contextual ranking data, as assessed by\nmore targeted automatic evaluations like Super NI\nand LMentry. Despite the inferior quality of the\nPRM-generated data, the performance on Vicuna\nQA remains almost unaffected (see Tunac (PRM)\nin Table 1). This observation suggests that evaluat-\ning LLMs on open QA with GPT-4 may not always\nbe as accurate as it appears, echoing the findings of\nWang et al. (2023b). It highlights the need for more\nrepresentative test questions or additional targeted\nbenchmarks for evaluation.\n4 Related Work\nInstruction Tuning Instruction tuning aims to\nimprove the usability of base language models\n(Brown et al., 2020; Raffel et al., 2020; Chowd-\nhery et al., 2022) by finetuning them on instruction-\nresponse pairs in a zero-shot (Wei et al., 2022) or\nfew-shot manner (Mishra et al., 2021; Wang et al.,\n2022b; Mallen et al., 2023). The instruction data\ncan be sourced from off-the-shelf NLP benchmarks\n(Mishra et al., 2021; Wei et al., 2022; Wang et al.,\n2022b) or generated by LLMs (Wang et al., 2022a;\nHonovich et al., 2022; Taori et al., 2023; Peng et al.,\n2023).\nRanking Loss Learning through re-ranking\nsequence-level outputs has been studied in\nsequence-to-sequence models (Wiseman and Rush,\n2016; Edunov et al., 2018; Liu et al., 2022; Zhang\net al., 2022). BRIO and MoCa algorithms (Liu\n15153\net al., 2022; Zhang et al., 2022) adopt a pair-\nwise ranking loss to guide the model to gener-\nate summaries with higher ROUGE scores (Lin,\n2004). In this paper, we use GPT-4’s (OpenAI,\n2023) strong contextual understanding ability and\ntext-davinci-003’s (Ouyang et al., 2022) intrin-\nsic probability measures for ranking. In parallel\nwith our work, Yuan et al. (2023) also propose\npairwise ranking loss for finetuning LLMs. Key\ndifferences include: 1) our pipeline finetuning strat-\negy; 2) our focus on ranking the model’s responses;\n3) our use of the original response for cross entropy\nregularization, while they select the highest-reward\nresponse. Additionally, Liu et al. (2023c) also em-\nploys GPT models for finetuning BART (Lewis\net al., 2019) on the summarization task.\nPre-Trained Model Evaluation Large pre-\ntrained models are powerful evaluation metrics\ndue to their strong contextual understanding abil-\nity, such as BERTScore (Zhang* et al., 2020),\nBARTScore (Yuan et al., 2021), MoverScore (Zhao\net al., 2019), COMET (Rei et al., 2020), and\nGPTScore (Fu et al., 2023). More recently, there\nare more evaluation strategies based on GPT-3.5\nand GPT-4 (Liu et al., 2023b; Gao et al., 2023).\n5 Conclusion\nIn this paper, we propose to finetune an instruction-\ntuned LLM using our probabilistic ranking ap-\nproach ( Tunap), contextual ranking approach\n(Tunac), and a combination of both (Tuna). Our\ncomprehensive experiments demonstrate consis-\ntent performance improvements across three bench-\nmarks: Super Natural Instructions (119 test tasks),\nLMentry (25 test tasks), and vicuna QA. Further-\nmore, our methods outperform popular reinforce-\nment learning from human feedback baselines that\nrely on the proximal policy optimization algo-\nrithm. These findings underscore the effectiveness\nof our approach in enhancing the performance of\ninstruction-tuned LLMs and pave the way for fu-\nture research in this area.\nLimitations\nDespite the promising results achieved by our Tuna\nmodel, there are several limitations that should be\nacknowledged. The first limitation is GPT-4 rank-\ning inconsistency. In our experiments, we relied\non GPT-4 for contextual ranking, which may in-\ntroduce bias due to the inconsistency in its rank-\ning performance. As a powerful LLM, GPT-4 is\ngenerally expected to provide accurate and reli-\nable rankings; however, it may still be sensitive\nto the phrasing or structure of prompts (Dubois\net al., 2023). This inconsistency may lead to sub-\noptimal rankings and potentially affect the overall\nperformance of the Tuna model. In future work, it\nwould be beneficial to design more robust prompts\nthat can mitigate the impact of GPT-4’s ranking in-\nconsistencies. Another limitation is the evaluation\nbenchmark. In this paper, we evaluated the Tuna\nmodel on three benchmarks, which provided a di-\nverse range of tasks and challenges. However, it is\nunclear how well the Tuna model would generalize\nto other types of tasks, domains, or languages. Fur-\nther research is needed to explore the applicability\nof the Tuna model to a broader range of problems\nand settings. The last limitation is the reliance\non the use of proprietary LLMs, such as GPT-4\nand text-davinci-003, for generating responses\nand rankings. This dependency may limit the ac-\ncessibility and reproducibility of our method for\nresearchers who do not have access to these pro-\nprietary models. Developing alternative methods\nthat can leverage open-source LLMs or other rank-\ning mechanisms would be a valuable direction for\nfuture research.\nAcknowledgements\nWe would like to thank reviewers for their valuable\nfeedback. This research/project is supported by\nMinistry of Education, Singapore, under its Tier 3\nProgramme (The Award No.: MOET320200004),\nthe National Research Foundation Singapore and\nDSO National Laboratories under the AI Singa-\npore Program (AISG Award No: AISG2-RP-2020-\n016), and Ministry of Education, Singapore, under\nits Academic Research Fund (AcRF) Tier 2 Pro-\ngramme (MOE AcRF Tier 2 Award No: MOE-\nT2EP20122-0011). Any opinions, findings and\nconclusions or recommendations expressed in this\nmaterial are those of the authors and do not reflect\nthe views of the Ministry of Education, Singapore.\nReferences\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\n15154\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Advances\nin Neural Information Processing Systems.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebas-\ntian Gehrmann, Parker Schuh, Kensen Shi, Sasha\nTsvyashchenko, Joshua Maynez, Abhishek Rao,\nParker Barnes, Yi Tay, Noam M. Shazeer, Vinod-\nkumar Prabhakaran, Emily Reif, Nan Du, Benton C.\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier García,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pillai,\nMarie Pellat, Aitor Lewkowycz, Erica Moreira, Re-\nwon Child, Oleksandr Polozov, Katherine Lee, Zong-\nwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Díaz,\nOrhan Firat, Michele Catasta, Jason Wei, Kathleen S.\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways. ArXiv, abs/2204.02311.\nPaul F Christiano, Jan Leike, Tom Brown, Miljan Mar-\ntic, Shane Legg, and Dario Amodei. 2017. Deep\nreinforcement learning from human preferences. In\nAdvances in Neural Information Processing Systems.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In North American Chapter of the Associa-\ntion for Computational Linguistics.\nYann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang,\nIshaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy\nLiang, and Tatsunori Hashimoto. 2023. Alpacafarm:\nA simulation framework for methods that learn from\nhuman feedback. ArXiv, abs/2305.14387.\nSergey Edunov, Myle Ott, Michael Auli, David Grang-\nier, and Marc’Aurelio Ranzato. 2018. Classical\nstructured prediction losses for sequence to sequence\nlearning. In North American Chapter of the Associa-\ntion for Computational Linguistics.\nAvia Efrat, Or Honovich, and Omer Levy. 2022. Lmen-\ntry: A language model benchmark of elementary\nlanguage tasks. ArXiv, abs/2211.02069.\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei\nLiu. 2023. Gptscore: Evaluate as you desire. ArXiv,\nabs/2302.04166.\nMingqi Gao, Jie Ruan, Renliang Sun, Xunjian Yin,\nShiping Yang, and Xiaojun Wan. 2023. Human-\nlike summarization evaluation with chatgpt. ArXiv,\nabs/2304.02554.\nOr Honovich, Thomas Scialom, Omer Levy, and Timo\nSchick. 2022. Unnatural instructions: Tuning lan-\nguage models with (almost) no human labor.\nYoon Kim and Alexander M. Rush. 2016. Sequence-\nlevel knowledge distillation. In Conference on Em-\npirical Methods in Natural Language Processing.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdel rahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2019. Bart:\nDenoising sequence-to-sequence pre-training for nat-\nural language generation, translation, and compre-\nhension. In Annual Meeting of the Association for\nComputational Linguistics.\nXuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori,\nIshaan Gulrajani, Carlos Guestrin, Percy Liang, and\nTatsunori B. Hashimoto. 2023. Alpacaeval: An au-\ntomatic evaluator of instruction-following models.\nhttps://github.com/tatsu-lab/alpaca_eval.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Annual Meeting of the\nAssociation for Computational Linguistics.\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran-\njape, Michele Bevilacqua, Fabio Petroni, and Percy\nLiang. 2023a. Lost in the middle: How language\nmodels use long contexts.\nYang Liu, Dan Iter, Yichong Xu, Shuo Wang, Ruochen\nXu, and Chenguang Zhu. 2023b. G-eval: Nlg evalua-\ntion using gpt-4 with better human alignment. ArXiv,\nabs/2303.16634.\nYixin Liu, Alexander R. Fabbri, Pengfei Liu,\nDragomir R. Radev, and Arman Cohan. 2023c. On\nlearning to summarize with large language models as\nreferences. ArXiv, abs/2305.14239.\nYixin Liu, Pengfei Liu, Dragomir R. Radev, and Graham\nNeubig. 2022. Brio: Bringing order to abstractive\nsummarization. In Annual Meeting of the Association\nfor Computational Linguistics.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In International Confer-\nence on Learning Representations.\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,\nDaniel Khashabi, and Hannaneh Hajishirzi. 2023.\nWhen not to trust language models: Investigating\neffectiveness of parametric and non-parametric mem-\nories. In Proceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers).\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and\nHannaneh Hajishirzi. 2021. Cross-task generaliza-\ntion via natural language crowdsourcing instructions.\n15155\nIn Annual Meeting of the Association for Computa-\ntional Linguistics.\nOpenAI. 2023. GPT-4 Technical Report. ArXiv,\nabs/2303.08774.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Gray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. In Advances in Neural Information\nProcessing Systems.\nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Gal-\nley, and Jianfeng Gao. 2023. Instruction tuning with\ngpt-4. ArXiv, abs/2304.03277.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In North American Chapter of the Associ-\nation for Computational Linguistics.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. Journal of Machine Learning Research.\nMarc’Aurelio Ranzato, Sumit Chopra, Michael Auli,\nand Wojciech Zaremba. 2015. Sequence level train-\ning with recurrent neural networks. In International\nConference on Learning Representations.\nRicardo Rei, Craig Stewart, Ana C Farinha, and Alon\nLavie. 2020. COMET: A neural framework for MT\nevaluation. In Conference on Empirical Methods in\nNatural Language Processing.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Arun Raja, Manan Dey,\nM Saiful Bari, Canwen Xu, Urmish Thakker,\nShanya Sharma Sharma, Eliza Szczechla, Taewoon\nKim, Gunjan Chhablani, Nihal Nayak, Debajyoti\nDatta, Jonathan Chang, Mike Tian-Jian Jiang, Han\nWang, Matteo Manica, Sheng Shen, Zheng Xin Yong,\nHarshit Pandey, Rachel Bawden, Thomas Wang, Tr-\nishala Neeraj, Jos Rozen, Abheesht Sharma, An-\ndrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan\nTeehan, Teven Le Scao, Stella Biderman, Leo Gao,\nThomas Wolf, and Alexander M Rush. 2022. Multi-\ntask prompted training enables zero-shot task gener-\nalization. In International Conference on Learning\nRepresentations.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec\nRadford, and Oleg Klimov. 2017. Proximal policy\noptimization algorithms. ArXiv, abs/1707.06347.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aur’elien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models. ArXiv,\nabs/2302.13971.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le,\nEd H. Chi, Sharan Narang, Aakanksha Chowdhery,\nand Denny Zhou. 2023a. Self-consistency improves\nchain of thought reasoning in language models. In\nInternational Conference on Learning Representa-\ntions.\nYizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack\nHessel, Tushar Khot, Khyathi Raghavi Chandu,\nDavid Wadden, Kelsey MacMillan, Noah A. Smith,\nIz Beltagy, and Hanna Hajishirzi. 2023b. How far\ncan camels go? exploring the state of instruction\ntuning on open resources.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-\nisa Liu, Noah A. Smith, Daniel Khashabi, and Han-\nnaneh Hajishirzi. 2022a. Self-instruct: Aligning lan-\nguage model with self generated instructions. ArXiv,\nabs/2212.10560.\nYizhong Wang, Swaroop Mishra, Pegah Alipoor-\nmolabashi, Yeganeh Kordi, Amirreza Mirzaei,\nAnjana Arunkumar, Arjun Ashok, Arut Selvan\nDhanasekaran, Atharva Naik, David Stap, Eshaan\nPathak, Giannis Karamanolakis, Haizhi Gary Lai, Is-\nhan Purohit, Ishani Mondal, Jacob Anderson, Kirby\nKuznia, Krima Doshi, Maitreya Patel, Kuntal Kumar\nPal, M. Moradshahi, Mihir Parmar, Mirali Purohit,\nNeeraj Varshney, Phani Rohitha Kaza, Pulkit Verma,\nRavsehaj Singh Puri, Rushang Karia, Shailaja Keyur\nSampat, Savan Doshi, Siddharth Deepak Mishra, Su-\njan Reddy, Sumanta Patro, Tanay Dixit, Xudong\nShen, Chitta Baral, Yejin Choi, Noah A. Smith,\nHanna Hajishirzi, and Daniel Khashabi. 2022b.\nSuper-naturalinstructions: Generalization via declar-\native instructions on 1600+ nlp tasks. In Conference\non Empirical Methods in Natural Language Process-\ning.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,\nAdams Wei Yu, Brian Lester, Nan Du, Andrew M.\nDai, and Quoc V Le. 2022. Finetuned language mod-\nels are zero-shot learners. In International Confer-\nence on Learning Representations.\nSam Wiseman and Alexander M. Rush. 2016. Sequence-\nto-sequence learning as beam-search optimization. In\nConference on Empirical Methods in Natural Lan-\nguage Processing.\n15156\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.\nBartscore: Evaluating generated text as text genera-\ntion. In Advances in Neural Information Processing\nSystems.\nZheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang,\nSongfang Huang, and Feiran Huang. 2023. Rrhf:\nRank responses to align language models with human\nfeedback without tears. ArXiv, abs/2304.05302.\nTianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. Bertscore: Eval-\nuating text generation with bert. In International\nConference on Learning Representations.\nXingxing Zhang, Yiran Liu, Xun Wang, Pengcheng He,\nYang Yu, Si-Qing Chen, Wayne Xiong, and Furu Wei.\n2022. Momentum calibration for text generation.\nArXiv, abs/2212.04257.\nWei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris-\ntian M. Meyer, and Steffen Eger. 2019. Moverscore:\nText generation evaluating with contextualized em-\nbeddings and earth mover distance. In Conference on\nEmpirical Methods in Natural Language Processing.\nYao Zhao, Mikhail Khalman, Rishabh Joshi, Shashi\nNarayan, Mohammad Saleh, and Peter J Liu. 2023.\nCalibrating sequence likelihood improves conditional\nlanguage generation. In International Conference on\nLearning Representations.\nMing Zhong, Pengfei Liu, Yiran Chen, Danqing Wang,\nXipeng Qiu, and Xuanjing Huang. 2020. Extractive\nsummarization as text matching. In Annual Meeting\nof the Association for Computational Linguistics.\nChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao\nSun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu,\nL. Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke\nZettlemoyer, and Omer Levy. 2023. Lima: Less is\nmore for alignment. ArXiv, abs/2305.11206.\nA The Length Penaltyβfor Probabilistic\nRanking Data\nIn our preliminary experiments, we found that the\nlength penalty β = 1.3 is able to induce detailed re-\nsponses and validated this choice on LIMA (Zhou\net al., 2023) dataset. We finetune the βparameter\nin Eq. 3 using the LIMA training dataset, which\ncontains 1030 high-quality expert instruction an-\nnotations, allowing LLaMA-65B to be finetuned\nand achieve remarkably strong performance across\na wide range of topics. Note that the training set\nalso contains 50 modified Super NI examples but\nthey are from the training tasks while we test our\nmodels on 119 Super NI test tasks. Specifically, we\nfirst obtain Tunap models with probabilistic rank-\ning data scored with differentβ. Then, we compute\nthe token-level negative log-likelihood (NLL) of\nthe output of each LIMA instance under differ-\nent Tunap models and average the token likelihood\nover the whole LIMA training set. The results are\nshown in Table 5. It can be seen that with β = 1.3,\nthe model can achieve the best NLL on LIMA train-\ning set. Thus, we set β = 1.3 in our experiments.\nβ 0.9 1.0 1.1 1.2 1.3 1.4\nNLL 2.14 2.12 2.11 2.10 2.09 2.09\nTable 5: The token-level log-likelihood of LIMA train-\ning set under Tunap models trained with probabilistic\nranking data scored with different β.\nB OpenAI API Pricing\nWe list the cost of calling OpenAI API models in\nTable 6.9 The human labeling cost per 1K examples\nis estimated based on the pricing listed in Dubois\net al. (2023), at 0.25$ for each comparison. For\neach data example, there are 4 responses and thus\n4 ∗(4 −1)/2 = 6comparisons. Thus, the human\nlabor cost per 1K examples is 1500$.\nC Training Details of Tunacp and\nmix-Tuna\nThe hyperparameters are listed in Table 7. For\nmodels finetuned from Alpaca, i.e., Tunac, Tunap,\nand mix-Tuna, the learning rate is 1e-5. The only\nexception is mix-Tuna-104K, whose learning rate\nis 5e-6 since it contains 52K duplicate data. For\nmodels finetuned from Tunac or Tunap, i.e., Tunacp\nand Tuna, the learning rate is 1e-6. We use 8 Nvidia\nV100-32GB GPUs for all experiments in this paper.\nD Other Proxy Ranking Models (PRM)\nSimilar to the PRM introduced in Sec. 3.1, we\nuse the 13K GPT-4 ranking data to train another\nPRM based on LLaMA-7B, which we refer to as\nPRM-7B. We denote the PRM based on StableLM-\n3B as PRM-3B. These two ranking models are\nemployed to re-rank Alpaca’s responses on 52K\ninstructions. The Alpaca model trained with 52K\ndata totally generated by the ranking models are\nreferred to as Tunac (PRM-3B-52K) and Tunac\n(PRM-7B-52K). Note that Tunac (PRM-3B-52K)\nis the Tunac (PRM) listed in Table 1. We denote the\nAlpaca model trained with 13K GPT-4 contextual\nranking data plus the last 39K data generated by\n9https://openai.com/pricing\n15157\nData Num Model Price\nProbabilistic Ranking 52K text-davinci-003 275$\nContextual Ranking 13K GPT-4-0314 380$\nHuman 1K - 1500$\nTable 6: The estimated cost of calling OpenAI API and human labeling.\nLR epoch batch size warmup\nTunac 1e-5 1 128 2\nTunap 1e-5 1 128 2\nmix-Tuna-52K 1e-5 1 128 2\nmix-Tuna-104K 5e-6 1 128 2\nTuna 1e-6 1 128 2\nTunacp-13K 1e-6 1 128 2\nTunacp-39K 1e-6 1 128 2\nTunacp-52K 1e-6 1 128 2\nTable 7: The hyperparameters of training different models.\nthe ranking models as Tunac (PRM-3B-39K) and\nTunac (PRM-7B-39K).\nThe results are listed in Table 8. We can observe\nthat models trained with ranking data generated\nby both PRMS do not achieve better results on Su-\nper NI compared to Tunac. The performances of\nTunac (PRM-3/7B-39K) is close to Tunac (PRM-\n3/7B-52K), implying that the ranking model have\nlearned 13K contextual ranking data well. Using\na larger ranking model, such as 7B, does not gain\nbetter performance, which indicates that the rank-\ning ability might not necessarily scale with the\npre-training model’s capacity. In general, the best\nstrategy is still to learn directly from GPT-4 con-\ntextual ranking data, which contains less noise.\nE Contextual Ranking Prompt\nWe show the prompt that we use for GPT-4 contex-\ntual ranking in Table 9.\nF Is the Contextual Ranking Prompt Too\nLong?\nIn (Liu et al., 2023a), the authors found the \"lost in\nthe middle\" phenomenon occurs at around 2K (20\ndocuments * 100 tokens/document) tokens for GPT-\n4 (note we used GPT-4 in contextual ranking). We\ncomputed the average length of the prompt (includ-\ning four responses and the ranking guidelines) used\nin GPT-4 ranking. The average length is 650 tokens,\nwhich is significantly shorter than 2K. Thus, the\ninput length does not seem to be an issue in GPT-4\nranking. Our human experiments above also con-\nfirm that the GPT-4 ranking is closely aligned with\nhuman assessments (see Appendix G).\nG Human Evaluation of GPT-4 Ranking\nWe conducted human evaluations of GPT-4 rank-\nings on 50 questions used for contextual ranking.\nWe asked annotators to rank the four system out-\nputs produced by our model and we observe that\nthe ranking quality by GPT-4 is reasonably good\n(the Spearman coefficient between the human rank-\nings and GPT-4 rankings is 0.72). Furthermore, we\nalso manually inspected the explanations given by\nGPT-4 for the ranking results. We found these ex-\nplanations to be well-reasoned and plausible. Per-\nhaps this is not surprising given the fact that several\nrecent papers found GPT can be good evaluators\nin multiple NLP tasks (Fu et al., 2023; Gao et al.,\n2023). We believe the ranking feedback of this\nlevel is sufficient for guiding our model for better\ntraining (our experiments also proved this).\nH Why Not Choose Pairwise Ranking in\nGPT-4 Ranking\nThere are several reasons why ranking 4 responses\ntogether is preferred over pairwise rankings.\n1. API cost: Pairwise ranking for four responses\nrequires (4 * 3) / 2 = 6 API calls, significantly\nincreasing the total cost. Moreover, a loop\n15158\nSuper NI LMentry Vicuna QA\n0-shot 2-shot LMentry Score Win Lose Tie\nAlpaca 36.0 44.5 31.4 - - -\nTunap 39.4 (+3.4) 43.9 (-0.6) 35.0 (+3.6) 68% 27% 5%\nTunac 37.7 (+1.7) 46.6 (+2.1) 32.2 (+0.8) 74% 20% 6%\nTunac (PRM-3B-39K) 35.6 (-0.4) 40.4 (-4.1) 33.4 (+2.0) 79% 15% 6%\nTunac (PRM-7B-39K) 33.5 (-2.5) 40.3 (-4.2) 32.5 (+1.1) 73% 20% 7%\nTunac (PRM-3B-52K) 34.2 (-1.8) 40.1 (-4.4) 32.2 (+0.8) 75% 19% 6%\nTunac (PRM-7B-52K) 34.6 (-1.4) 41.1 (-3.4) 32.0 (+0.6) 73% 20% 7%\nTuna 38.7 (+2.7) 45.0 (+0.5) 34.7 (+3.3) 86% 10% 4%\nTable 8: Performance comparison of different models. The numbers in bold indicate the top-2 results. The numbers\nin parentheses indicate the performance difference compared to Alpaca.\n(e.g., R1 > R2, R2 > R3, R3 > R1) could\noccur when R1/2/3 are of similar qualities, po-\ntentially requiring extra API calls for further\nvalidation.\n2. The GPT-4 ranking quality is good enough,\nsee Appendix G.\nI Comparison between Our RLHF\nModels and\nPPO-sim/PPO-sim-GPT4-20K\nWe compare our RLHF models and PPO-sim/PPO-\nsim-GPT4-20K from Dubois et al. (2023) on Vi-\ncuna QA. The results can be found in Table\n10. PPO-sim/PPO-sim-GPT4-20K have better re-\nsponses and thus we choose to report the results of\ntheir models.\nJ BERTScore Results on Super NI\nBy default, ROUGE is employed on Super NI. We\nadditionally reported BERTScore (which is proven\nto be a better alternative for ROUGE) in Table 11\nand the results follow the same trends (also see\nTable 1).\nK Examples\nWe show some examples in 12.\n15159\nBelow is an instruction that describes a task, paired with an input that provides further context.\nWrite a response that appropriately completes the request.\n### Instruction:\n{Instruction}\n### Input:\n{Input}\n### Response:\n###Response 0:\n{Response 0}\n###Response 1:\n{Response 1}\n###Response 2:\n{Response 2}\n###Response 3:\n{Response 3}\nWe would like you to rate Response 0/1/2/3 in reply to the given instruction displayed above.\nFirst, identify if the instruction requires open-ended or close-ended responses.\nSecond, you need to generate one high quality ‘###Response 4’ in answer to the instruction. It\nneeds to have the same format as other responses and will be used as a reference later.\nThird, identify if there are duplicate responses and keep only one of the duplicate responses for the\nfollowing steps.\nFourth, compare Response 4 with Response 0/1/2/3/4 and assign each response an overall score\non a scale of 0 to 15 where a higher score indicates better overall quality. For an open-ended\ninstruction, please rate based on the relevance (score 0 to 5), level of details/justification: (score 0\nto 5) and accuracy (score 0 to 5) of each response; for a close-ended instruction, please rate based\non the accuracy (score 0 to 5), level of details/justification (score 0 to 5) and clarity (score 0 to 5) of\neach response. The ratings should have the format: ‘Response k: [ sum of the 3 individual\nscores you give to response k]’.\nLast, rank the responses in decreasing order of their overall scores. The ranking should have the\nformat: ‘rank: [i, j ,k, l, m]’. If there are duplicate responses, keep only one of them in the rank,\nthat is, the ranking may become: ‘rank: [i, j, k, l]’, ‘rank: [i, j, k]’ ‘rank: [i, j]’ or even ‘rank: [i]’.\nTable 9: Contextual Ranking Prompt for GPT-4.\n15160\nWin Lose Tie\nPPO-sim 79% 16% 5%\nPPO-sim-GPT4-20K 74% 22% 4%\nOur PPO (w/ GPT4 ranking data) 55% 35% 10%\nTable 10: Vicuna QA results of our RLHF models and\n(Dubois et al., 2023).\nMODEL 0-shot 2-shot\nAlpaca 0.638 0.705\nPPO-sim 0.594 0.652\nPPO-sim-GPT4-20K 0.641 0.714\nTunap 0.667 0.690\nTunac 0.655 0.727\nTuna 0.665 0.699\nTable 11: BERTScore evaluation of the model outputs\nin Super NI. Higher is better.\n15161\nInstruction: \"Why is open-source important? Answer in 3-5 sentences\"\nAlpaca:\nOpen-source software is important because it allows for collaboration and innovation. It allows\ndevelopers to work together to create better software, and it allows users to access the source\ncode and make changes to the software to meet their own needs. Open-source software also\nencourages transparency and allows users to see how the software works and how it can be\nimproved. Additionally, open-source software is often more secure and reliable than proprietary\nsoftware, as it has been tested and reviewed by a large number of developers. Finally, open-source\nsoftware is often free or low-cost, making it accessible to everyone.\nTunac:\nOpen-source software is important because it allows for collaboration and innovation. It allows\ndevelopers to work together to improve existing software and create new software, and it allows\nusers to view and modify the source code to customize the software to their own needs. Open-source\nsoftware also encourages transparency and accountability, as anyone can view and inspect the\ncode to ensure that it is secure and reliable. Additionally, open-source software is often more\ncost-effective than proprietary software, as users can access the source code and modify it to suit\ntheir own needs without having to pay for a license. Finally, open-source software is often more\nreliable and secure than proprietary software, as it is developed and tested by a large community of\ndevelopers.\nTunap:\nOpen-source software is important because it allows developers to access the source code of a\nprogram and make changes to it. This allows developers to customize the software to their needs\nand to fix any bugs or errors that may be present. Additionally, open-source software is often free to\nuse and modify, which makes it more accessible to those who may not be able to afford proprietary\nsoftware. Open-source software also allows developers to collaborate and share ideas, which can\nlead to the development of better and more efficient software. Finally, open-source software can\nbe used as a learning tool, as it allows developers to see how the software works and how it is put\ntogether. This can help them to better understand programming and coding in general.\nTuna:\nOpen-source software is important because it allows developers to collaborate on projects, share\ncode, and create better products. By making the source code available to the public, developers\ncan easily collaborate on projects, allowing them to create better products and services. This\ncollaboration also allows developers to quickly identify and fix bugs, as well as add new features\nand functionality. Additionally, open-source software is often more secure than proprietary software,\nas it can be more easily audited and reviewed by the public. Finally, open-source software is often\nmore cost-effective than proprietary software, as it does not require licensing fees or other costs\nassociated with closed-source software. Open-source software is an important part of the software\nindustry, and is used by many businesses and individuals.\nTable 12: Example responses of different models.\n15162\nInstruction: \"Write a Python function that generates a random number between 1 and 10 (inclusive).\"\nListing 1: Alpaca\n1 def random_number():\n2 return random.randint(1, 11)\nListing 2: Tunac\n1 def random_number():\n2 return random.randint(1, 11)\nListing 3: Tunap\n1 \"\"\"\n2 Generate a random number between 1 and 10 (inclusive)\n3 \"\"\"\n4\n5 import random\n6\n7 def random_number():\n8 return random.randint(1, 10)\n9\n10 if __name__ == '__main__':\n11 print(random_number())\nListing 4: Tuna\n1 \"\"\"\n2 Generate a random number between 1 and 10 (inclusive)\n3 \"\"\"\n4\n5 import random\n6\n7 def random_number():\n8 return random.randint(1, 10)\n9\n10 if __name__ == '__main__':\n11 print(random_number())\nTable 13: Example responses of different models.\n15163",
  "topic": "Ranking (information retrieval)",
  "concepts": [
    {
      "name": "Ranking (information retrieval)",
      "score": 0.8077858686447144
    },
    {
      "name": "Computer science",
      "score": 0.7806401252746582
    },
    {
      "name": "Probabilistic logic",
      "score": 0.7207427024841309
    },
    {
      "name": "Code (set theory)",
      "score": 0.5923048257827759
    },
    {
      "name": "Reinforcement learning",
      "score": 0.5222747325897217
    },
    {
      "name": "Machine learning",
      "score": 0.5168381333351135
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.47237899899482727
    },
    {
      "name": "Test (biology)",
      "score": 0.44764798879623413
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4435173273086548
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I152815399",
      "name": "Singapore University of Technology and Design",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210113369",
      "name": "Microsoft Research Asia (China)",
      "country": "CN"
    }
  ],
  "cited_by": 3
}