{
  "title": "Alternating Recurrent Dialog Model with Large-scale Pre-trained Language Models",
  "url": "https://openalex.org/W2980207396",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2222386577",
      "name": "Wu, Qingyang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2200501251",
      "name": "Zhang, Yichi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1969430579",
      "name": "Li Yu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2105593998",
      "name": "Yu Zhou",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2914204778",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2996287690",
    "https://openalex.org/W2988647680",
    "https://openalex.org/W2963134326",
    "https://openalex.org/W2963825865",
    "https://openalex.org/W2251058040",
    "https://openalex.org/W2952607215",
    "https://openalex.org/W2798914047",
    "https://openalex.org/W2963491014",
    "https://openalex.org/W2963159690",
    "https://openalex.org/W2915295540",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2953071719",
    "https://openalex.org/W2962886331"
  ],
  "abstract": "Existing dialog system models require extensive human annotations and are difficult to generalize to different tasks. The recent success of large pre-trained language models such as BERT and GPT-2 (Devlin et al., 2019; Radford et al., 2019) have suggested the effectiveness of incorporating language priors in down-stream NLP tasks. However, how much pre-trained language models can help dialog response generation is still under exploration. In this paper, we propose a simple, general, and effective framework: Alternating Roles Dialog Model (ARDM). ARDM models each speaker separately and takes advantage of the large pre-trained language model. It requires no supervision from human annotations such as belief states or dialog acts to achieve effective conversations. ARDM outperforms or is on par with state-of-the-art methods on two popular task-oriented dialog datasets: CamRest676 and MultiWOZ. Moreover, we can generalize ARDM to more challenging, non-collaborative tasks such as persuasion. In persuasion tasks, ARDM is capable of generating human-like responses to persuade people to donate to a charity.",
  "full_text": "Alternating Recurrent Dialog Model with Large-scale Pre-trained\nLanguage Models\nQingyang Wu1, Yichi Zhang2, Yu Li1, Zhou Yu1\n1University of California, Davis, 2Tsinghua University,\n{wilwu, yooli, joyu}@ucdavis.edu, zhangyic17@mails.tsinghua.edu.cn\nAbstract\nExisting dialog system models require exten-\nsive human annotations and are difﬁcult to gen-\neralize to different tasks. The recent success\nof large pre-trained language models has sug-\ngested the effectiveness of incorporating lan-\nguage priors in down-stream NLP tasks. How-\never, how much pre-trained language models\ncan help dialog response generation is still un-\nder exploration. In this paper, we propose a\nsimple, general, and effective framework: Al-\nternating Recurrent Dialog Model (ARDM).\nARDM models each speaker separately and\ntakes advantage of large pre-trained language\nmodels. It requires no supervision from human\nannotations such as belief states or dialog acts\nto achieve effective conversations. ARDM out-\nperforms or is on par with the state-of-the-\nart methods on two popular task-oriented di-\nalog datasets: CamRest676 and MultiWOZ.\nMoreover, we can generalize ARDM to more\nchallenging, non-collaborative tasks such as\npersuasion. In the PersuasionForGood task,\nARDM is capable of generating human-like re-\nsponses to persuade people to donate to a char-\nity.1\n1 Introduction\nIt has been a long-standing ambition for artiﬁcial\nintelligence researchers to create an intelligent con-\nversational agent that can generate human-like re-\nsponses. Recently, data-driven dialog models are\nmore and more popular. However, most current\nstate-of-the-art approaches still heavily rely on ex-\ntensive human annotations such as belief states and\ndialog acts (Lei et al., 2018). However, dialog con-\ntent can vary considerably in different dialog tasks.\nHaving a different intent or dialog act annotation\nscheme for each task is costly and even impossible\nfor tasks such as open-domain social chat. Thus, it\n1Code is available at https://github.com/qywu/ARDM\nis difﬁcult to utilize these methods on challenging\ndialog tasks where dialog states and acts are difﬁ-\ncult to annotate such as persuasion and negotiation.\nEric and Manning (2017) proposed a simple\nsequence-to-sequence architecture that requires no\nexplicit annotations. The model learns to extract\ninformation from dialog history with attention and\ncopy mechanism. However, due to the limited lan-\nguage modeling capability in the previous model,\nSequicity (Lei et al., 2018), which uses belief states\nas inputs for supervision, outperforms Eric and\nManning (2017)’s method signiﬁcantly in the re-\ncent dialog datasets. But with the success of large\npre-trained language models such as BERT (Devlin\net al., 2019) and GPT-2 (Radford et al., 2019), we\ninvestigate how large-scale pre-trained language\nmodels can help dialog tasks.\nPrevious sequence-to-sequence models are used\nto tackle documents with only one narrator. How-\never, in dialogs, two speakers have different roles;\ntherefore, their language model distributions are\nvery different from each other. To address this\nissue, we propose ARDM, a dialog model that\nencodes and decodes different speaker utterances\nin alternating order. This structure makes the\nmodel more ﬂexible and efﬁcient than traditional\nsequence-to-sequence models in processing vari-\nous dialogs. We evaluate our model on three dif-\nferent task-oriented dialog datasets: CamRes676,\nMultiWOZ, and PersuasionForGood. The ﬁrst two\ndatasets are traditional information request dialog\ndatasets with well-deﬁned automatic evaluation\nmetrics on task completion. By contrast, Persua-\nsionForGood is a new dataset that focuses on per-\nsuading people to donate to a charity. Due to the\ncomplexity of dialog content, there is no explicit\ndialog state deﬁned in this task.\nWe observe that ARDM is capable of improv-\ning task-oriented dialog tasks performance over\nthe previous state-of-the-art methods without in-\narXiv:1910.03756v3  [cs.CL]  26 Apr 2021\ncorporating any explicit supervision from belief\nstates or dialog acts. Also, because of ARDM’s\nsimplicity and generality, one can rapidly build a\ndialog prototype on different types of applications\nusing only conversations without additional human\nannotations. We also found that ARDM works\nwell on complex dialogs, such as persuasion. The\nmodel generates dialog responses that successfully\npersuade people to donate to a charity, suggesting\nthe potential of ARDM being used in wide-scale\nreal-world settings.\n2 Related Work\nTraditional dialog systems consist of a dialog man-\nager to maintain dialog states and control the con-\nversation ﬂow. However, a dialog manager re-\nquires extensive manual annotations for training\nthe sub-modules such as dialog state tracker or\npolicy decision-maker. An alternative is to model\ndialog without explicitly modeling belief states.\nSpeciﬁcally, Eric and Manning (2017) proposed\na sequence-to-sequence model that utilizes copy-\nmechanism to copy history information directly\nfrom raw dialog history. This method achieved\nthe state-of-the-art results on DSTC2 (Henderson\net al., 2014), which is a simple dialog restaurant\nbooking task with abundant data. However, such\nmethod did not perform well on more complex di-\nalog task data sets CamRes676 (Wen et al., 2017)\nand KVRET (Eric et al., 2017). Sequicity (Lei\net al., 2018) attributed the bad performance of Eric\nand Manning (2017)’s method to the omission of\nbelief tracker. They introduced the concept of be-\nlief span and added belief tracker back to the model\nand achieved state-of-the-art performance.\nCompared to Sequicity, Eric and Manning\n(2017)’s method provides a more general frame-\nwork that reduces manual dialog state, user intent,\nand dialog act labeling by bypassing any symbolic\nannotations. Such a model can apply to datasets\nwith no or partial annotations of belief states. In\na real-world setting, if the dialog task introduces\nnew slot values in belief states (i.e. a new type of\nfood), Sequicity will suffer from the belief span de-\ncoder error in response generation. Thus, Eric and\nManning (2017)’s method may be potentially more\nrobust than Sequicity in this situation. Besides, if\nthe task requires belief states for database search,\nwe can treat belief tracking as a separate task. We\ncan train a good belief tracking with only a small\namount of annotated data, which reduces the an-\nnotation required and is easier to ﬁx errors. Also,\nsince belief states are a set of important entities con-\ndensed from dialog history (i.e., often exact words\nfrom utterances), they do not introduce extra infor-\nmation to the model. Therefore, a dialog model\nwith powerful representation learning should learn\na form of belief states information automatically\nwithout human annotations as the scaffold.\nRecent success of BERT (Devlin et al., 2019)\nand GPT2 (Radford et al., 2019) suggests the possi-\nbility of applying large pre-trained language mod-\nels to dialog systems. There are some studies\nof applying large pre-trained language model to\ndialog generation. TransferTransfo (Wolf et al.,\n2019) ﬁne-tuned the pre-trained language model\nGPT (Radford et al., 2018) on Persona-Chat dataset\n(Zhang et al., 2018) and obtained signiﬁcant im-\nprovements on chitchat response generation, sug-\ngesting the potential of ﬁne-tuning large pre-trained\nlanguage models on other dialog response gener-\nation tasks. A more recent work (Budzianowski\nand Vulic, 2019) adopted the framework of Trans-\nferTransfo. They made the ﬁrst attempt to lever-\nage large pre-trained language models GPT and\nGPT-2 on task-oriented dialog generation, but it\nincluded belief states modeling as the input and\ndid not achieve better results than the baseline. We\npropose to model dialogs without any annotation\nbut rely on pre-trained large-scale language models\nthat alternate.\n3 Alternating Recurrent Dialog Model\nWe propose Alternating Recurrent Dialog Model\n(ARDM) by compositing two separate pre-trained\nlanguage model in alternate order to learn the user\nand system utterance distribution through memory\nrecurrence. Figure 1 shows an overview of ARDM.\n3.1 Recurrent Modeling for User and System\nWe aim to model both user and system utter-\nances distribution recurrently. Given a multi-\nturn dialog ( d) between a user ( u) and a system\n(s), we can represent d as a series of utterances\n{u1,s1,u2,s2,...,u T ,sT }, where T denotes the\ntotal number of turns. We decompose the prob-\nability distributions over the utterances in dinto\ntwo language models for the user and system re-\nspectively, denoted as pu and ps. Then we deﬁne a\n(a)\n (b)\nFigure 1: Alternating Recurrent Dialog Model (ARDM) Overview. (a) shows how we feed the entire dialog to\nARDM. (b) shows the recurrence mechanism we used to preserve memory.\ndialog model p(d) with the equation:\np(d) =\nT∏\nt=1\npu(ut|u<t,s<t) ps(st|u≤t,s<t) (1)\npu and ps are standard language models where\nthe task is to predict the next token given the pre-\nceding context. For an utterance ut or st with m\ntokens {w1,...,w m}, the joint probability of an\nutterance is as follows:\npu(ut|u<t,s<t) =\nmut∏\ni=1\nP(wi|w<i,u<t,s<t) (2)\nps(st|u≤t,s<t) =\nmst∏\ni=1\nP(wi|w<i,u≤t,s<t) (3)\nFinally, we train the dialog model by maximizing\nthe likelihood over Equation 1. However, in order\nto model multi-turn dialog distributions efﬁciently,\nwe need a memory mechanism to encode the his-\ntory. In the next section, we introduce the memory\nmechanism in detail.\n3.2 Memory Recurrence\nWe apply a memory mechanism to grant the model\nthe capability of memorizing conversation history.\nThis method is similar to Transformer-XL (Dai\net al., 2019), which enables the model to learn\nlonger dependency. For an utterance at turn t, we\nreuse the hidden states h≤t−1 stored in the mem-\nory Mt−1 to obtain ht, and store the ht back to the\nmemory as Mt. As for the pre-trained Transformer\nlanguage model, we implement the memory mecha-\nnism using self-attention given the query/key/value\nfeatures denoted as Q,K,V , where the equation is\ndeﬁned as:\nAttention(Q,K,V ) =softmax(QKT V) (4)\nFor simplicity, we assume there is only one layer\nin Transformer, and ht is the hidden states which\nconsist of N vectors for the current input N tokens\nin the utterance at time t. Then a recurrence rela-\ntion for ht is deﬁned by computing Qt, K≤t, V≤t\nfrom h≤t−1 and the current utterance. In practice,\nwe reuse K≤t−1 and V≤t−1 (i.e. history keys and\nvalues) as Mt−1 instead of ht−1 to avoid recomput-\ning history information. Therefore, the ﬁnal ht is\ncomputed as:\nMt−1 = [K≤t−1,V≤t−1] (5)\nK≤t,V≤t = [K≤t−1; Kt],[V≤t−1; Vt] (6)\nht = Attention(Qt,K≤t,V≤t) (7)\nOne can use ht (consisting of vectors for each to-\nken) to get each token’s probability to calculate\nthe language model cross entropy loss to maximize\np(wi|w<i,u <t,s<t), shown in Figure 1.\n3.3 Training Details\nWe initialize the user and the system language\nmodel with a large pre-trained language model\nGPT-2 small with 117M parameters (Radford et al.,\n2019). It is a Transformer (Vaswani et al., 2017)\nmodel with 12 heads, 768 hidden size, and 12 lay-\ners. The model is trained on a large scale corpus\ncalled WebText extracted from Reddit with at least\nthree upvotes. The tokenizer is 50,257 size byte\npair encoding (BPE) (Sennrich et al., 2016) that\ncan encode and decode any text in a lossless man-\nner to avoid out-of-vocabulary tokens. We follow\na special format in GPT-2 as the “trigger” so that\nthe model can zero-shot dialog response genera-\ntion, by preﬁxing the user role token “A:” or “B:”,\nand sufﬁxing the end of utterance token “\\n\\n\\n”.\nThis “trigger” approach is similar in other zero-shot\nscenarios mentioned in GPT-2 paper (e.g., that a\n”TL;DR” token can trigger GPT-2 to summarize\nthe input text.) We further ﬁne-tune ARDM on the\nspeciﬁc task dataset. We apply AdamW optimizer\n(Loshchilov and Hutter, 2019), and the number of\nwarm-up steps is set to be the number of batches\nin one epoch. The learning rate is set to 3 ×10−5,\nand the dropout rate is set to 0.1 for all tasks. We\nconduct all experiments on a 11GB GPU.\n3.4 Decoding Details\nWe decode utterances by nucleus sampling (Holtz-\nman et al., 2019) with different hyper-parameters\n(top-p, top-k) for down-stream dialog tasks. We\nalso vary the temperature of T <1 to ﬁnd the best\nsetting for the speciﬁc down-stream dialog task.\nTo handle both situations in the evaluation and the\nreal-world use case, we have two decoding modes.\nFor evaluation mode, we feed all past ground truth\nhistory before turn t to generate the correspond-\ning utterance, so that we can evaluate the quality\nof generated dialog responses without concerning\nabout the conversion ﬂow. While in a real-world\nuse case, we do not have ground truth history, and\ntherefore we use the memory from previously gen-\nerated responses and let the model dynamically\ninteract with a human or another bot in turns. Be-\ncause dialogs have different lengths, it is hard for\nARDM to efﬁciently decode responses using tra-\nditional batch padding method. As a solution, we\ndevelop a dynamic dialog ﬁltering algorithm to sup-\nport fast decoding in batch. Such method speeds\nup the generation eight times faster. Please refer to\nAppendix for the method’s details.\n3.4.1 Belief Tracking\nDatabase state means the database query results. In\nthe example, since there is no match in the database\nsatisfying the constraint ”cheap french restaurant”,\nit returns ”0” and the domain ”restaurant”. Then\nthe system can condition on the database state to\ngenerate ”There is no such restaurant.” We do not\nincorporate belief tracking because we want to ﬁt\ndifferent dialog tasks in which there may be no\nannotations of belief states. Also, (Eric and Man-\nning, 2017) pointed out that user utterances already\ncontains the belief state information to generate sys-\ntem response, meaning it is not necessary to explic-\nitly input belief state into the model. For database\nsearch, one can train a separate belief tracker with\na small portion of data that is annotated, which is\nmore common in the real world cases.\n4 Experiments and Results\nData scarcity is one of the biggest challenges in di-\nalog research. It is costly to collect human-human\nconversations under a speciﬁc setting. It is even\nmore time-consuming to annotate the correspond-\ning belief states and dialog acts. With the success\nof transfer learning in NLP, we aim to mitigate the\nlow-resource problem with the large pre-trained\nlanguage model. We validate our proposed ARDM\non three task-oriented dialog datasets, CamRest676,\nMulitWOZ, and PersuasionForGood.\n4.1 CamRest676\nCamRest676 is a relatively small dataset with\n408/136/136 dialogs for train/validation/test. We\nfollow Sequicity (Lei et al., 2018) to delexicalize\ntokens such as restaurant names, phone numbers,\npostcodes by replacing them with their slot names\nin utterances. We prepend database search results\nto the system utterance. An example database\nsearch results are “restaurant;3”, where the ﬁrst\nslot indicates its dialog domain, which is always\n“restaurant” in CamRest767, and the second slot\nrepresents the number of matched items in the\ndatabase. We use greedy sampling for all methods\nduring decoding. We use BLEU-4 and Success F1\nto evaluate language generation quality and Suc-\ncess F1 to evaluate task success. Success F1 com-\nputes the F1 score of the generated responses on re-\nquested slots such as an address, phone number, or\nfood type. Other than Sequicity, we also compare\nresults by using GPT-2 alone as a language model\nfor the entire dialog. This baseline is equivalent to\nARDM without alternating recurrent modeling.\n4.1.1 Results\nWe ﬁrst test our method on a restaurant search\ndataset, CamRest676 (Wen et al., 2017). Table 1\nshows all models’ results with ground truth belief\nstate or generated belief state. We ﬁrst use ground\ntruth belief state in all methods to evaluate their\nresponse generation quality. ARDM achieves the\nbest BLEU and Success F1 score. We observe\nthat after ﬁne-tuning GPT-2 on the CamRest676, it\nachieves similar results compared to the previous\nstate-of-the-art method, Sequicity with reinforce-\nment ﬁne-tuning. This suggests pre-trained large-\nscale language model, such as GPT-2, transfers\nthe meaningful representations to help ﬁne-tuning.\nGround Truth Database State Generated Database State\nModel Entity Match rate BLEU-4 Success. F1 BLEU-4 Success. F1\nRegular Expression 0.960 - - - -\nSequicity 0.923 21.4 0.852 21.4 0.853\nSequicity (w/o RL) 0.940 22.9 0.821 23.4 0.834\nGPT-2-ﬁnetune - 21.8 0.851 19.2 0.862\nARDM - 26.2 0.864 25.4 0.862\nARDM (50% data) - 25.9 0.859 23.4 0.851\nTable 1: Results on CamRest676 dataset. Our method does not require human annotations from dialog states and\ndialog acts in comparison to Sequicity.\nHowever, without the alternating mechanism, GPT-\n2 alone does not perform as well as ARDM in\nterms of both BLEU-4 and Success F1, especially\nin BLEU-4 (improved 19%). Without the recur-\nrent modeling, the model blends two speakers lan-\nguage distribution and ignores the role difference.\nMoreover, to test if our model preserves its per-\nformance with even less training data, we reduce\nthe training data to 50%, and the performance only\ndrops slightly. With half of the training data, our\nmethod still performs signiﬁcantly better than Se-\nquicity. This result suggests ARDM is robust on\nlow-resource settings due to the advantage of the\nlarge-scale pre-training language model.\nWe also evaluate all models with generated belief\nstates instead of ground truth belief states. Sequic-\nity generates belief tracker results, and its Entity\nMatch rate is 0.927. Our model does not have a\nstate tracker, so we write a separate simple regular\nexpression to extract the occurrence of entities that\nappear in the database to support our model. Such\nstate tracker achieves 0.960 in Entity Match rate. It\nsuggests that state tracking may be accomplished\nin more straightforward ways other than training a\nneural network model on a large set of annotated\ndata. With a simple state tracker, our proposed\nmethod still performs better than Sequicity, which\ntrains the belief state and the response generation\ntask jointly.\n4.2 MultiWOZ\nHere, we only use the ground truth database search\nresult to be consistent with other methods. We per-\nform delexicalization which is mentioned in the\noriginal MultiWOZ (Budzianowski et al., 2018).\nWe prepend the database search results to the sys-\ntem response for as conditional input. Also, the\ndatabase results now contain information about\nwhether the booking is successful or not (i.e., suc-\nceed or fail). Again, we preﬁx the database results\nto the system response. Note that we do not use\nbelief state or dialog act annotation provided by the\ndataset to train ARDM. We set the top-p to 0.2 and\nthe temperature to 0.7.\nWe normalize the time’s slot value in all dialogs\ninto the 24-hour format and perform tokenization\nvia spaCy 2. We found that different papers re-\nport results with different versions of the evaluator,\nwhich makes it difﬁcult to compare different meth-\nods fairly. We explain the differences among all\nversions of the evaluator in Appendix. In this pa-\nper, we follow LaRL’s evaluator implementation,\nas it is more reasonable than others. We re-evaluate\nresults for all methods with the same evaluator to\nensure fairness.\nWe compare our model to the attention-based\nseq2seq model which is proposed as the MultiWOZ\nBaseline (Budzianowski et al., 2018), the HDSA\n(Chen et al., 2019) model that incorporates dialog\nact supervision as an inductive prior for model ar-\nchitecture, and the LaRL (Zhao et al., 2019) model\nwhich leverages latent action modeling and rein-\nforcement learning to improve performance. We do\nnot compare with GPT-2-ﬁnetune with our model\nin MultiWOZ because GPT-2-ﬁnetune’s perfor-\nmance on CamRest676 is signiﬁcantly worse than\nour model. Note that GPT-2-ﬁnetune is equivalent\nto ARDM without alternating parameterization.\nThe results are evaluated on BLEU-4, Inform\nRate, and Success Rate. Inform and Success Rate\nmeasure whether the system response provides the\nrecommendations and requested information given\nin the goal.\n2https://spacy.io/\nModel Supervision Inform (%) Success (%) BLEU-4Dialog State Dialog Act\nHuman - - 98.9 96.5 -\nBaseline ✓ × 82.5 72.9 18.9\nHDSA ✓ ✓ 87.7 73.4 23.6\nLaRL ✓ × 82.8 79.2 12.8\nARDM × × 87.4 72.8 20.6\nTable 2: Results on MultiWOZ. Supervision denotes whether a model leverages dialog state or/and dialog act\nannotations. All models use the ground truth dialog state for database search. ARDM without supervision from\nannotation can still achieve comparable results.\n4.2.1 Results\nThe evaluation results are shown in Table 2. With-\nout any supervision from dialog states or dialog\nacts, ARDM signiﬁcantly outperforms the Multi-\nWOZ Baseline and LaRL on BLEU-4 and Inform\nrate, and is on par with HDSA. However, HDSA\nuses dialog act supervision and a large pretrained\nlanguage model, BERT. Our model requires no\nannotation and can achieve similar results. This\nsuggests our recurrent modeling and large-scale\npre-training methods work similarly as the useful\ndialog act annotations. All the results show that our\nmethod’s excellent performance remains consistent\nin multi-domain dialogs.\nWe analyze the generated responses and ﬁnd that\nif multiple domains have appeared in the conver-\nsation history, our model tends to make mistakes\nin answering the right domain for user requests.\nThis ﬁnding suggests that the Maximum Likeli-\nhood Estimation (MLE) has limitations in directly\noptimizing the metric, while reinforcement Learn-\ning (RL) can hugely improve the task completion\nin a dialog system. This is why LaRL has a higher\nSuccess rate. However, we also observe that LaRL\nhas a low BLEU-4 score, which indicates low read-\nability in responses. Therefore, there is a trade-off\nbetween the generation quality and the task success\nrate in the RL setting.\n4.3 PersuasionForGood\nTo showcase ARDM’s performance on a dialog\ndataset where it is much more difﬁcult to obtain be-\nlief states and dialog act annotations, we train and\nevaluate our model on PersuasionForGood (Wang\net al., 2019) dataset. In this dataset, the persuader\nmust persuade an assigned persuadee (i.e., a person\nwho is asked to donate) to donate money (from\ntheir task payment) to a charity called “Save the\nChildren”.\n4.3.1 Setting\nThis dataset has a much larger vocabulary size\n(8,141) than the previous task-oriented dialog\ndatasets due to its non-collaborative dialog prop-\nerty. The conversation content is richer because\ntwo speakers are negotiating back and forth. The\ndataset consists of 1,017 dialogs where only 300\ndialogs are annotated with dialog acts. Therefore,\nmodels that require dialog state or dialog act anno-\ntation are not applicable in this dataset. As ARDM\ndoes not require dialog acts for training, it ﬁts well\non the PersuasionForGood dataset. We remove\nthe preﬁx for system inputs because the task does\nnot involve any database search. We use Trans-\nferTransfo (Wolf et al., 2019) model as a baseline,\nsince it does not require extra human annotations.\nTransferTransfo is based on large pre-trained lan-\nguage model, and it uses token type embedding to\nencode role information of the speaker. We ﬁne-\ntune it on the PersuasionForGood dataset.\nTo generate diverse responses, we decode the\nresponse using the nucleus sampling (Holtzman\net al., 2019) with a top-p of 0.9 and a temperature\nof 0.7. It is impossible to conduct an automatic\nevaluation on task success on this task due to the\nlack of annotation. We use perplexity to evalu-\nate each model’s language generation quality. We\nalso conduct a human evaluation to validate each\nmodel’s task success rate. We show some gener-\nated examples in the Appendix to provide more\ninformation on both models’ generation quality.\n4.3.2 Results\nTable 3 shows the results for PersuasionForGood.\nBecause ARDM applies better speaker modeling\nand recurrence mechanism, our model achieves\nPerplexity↓ Human Preference↑ Average Donation Amount↑\nTransferTransfo 19.9 34.7% 0.538\nARDM 10.1 65.3% 0.807\nTable 3: Automatic Evaluation and Human Evaluation Results on the PersuasionForGood dataset.\nlower perplexity compared to TransferTransfo. We\ndid not report BLEU, because on the Persuasion-\nForGood dataset, BLEU score cannot reﬂect the\nactual generation quality as a random sentence with\ncommon tokens “the, of, is, are. . . ” already has\n10.0+ BLEU-1 score. Also because the validation\nset only contains 100 samples, the result can have\na high variance. Then the only way to evaluate this\ntype of system is by human evaluation.\nTo comprehensively evaluate each model’s per-\nformance, we recruit 14 human evaluators to chat\nwith the two persuasive systems ten times to avoid\nthe randomness produced by each model. In total,\nwe collected 140 ratings. We ask them to select a\npreferred chat-bot and indicate how much they are\nwilling to donate after talking to the chat-bot. As\na result, human judges prefer ARDM over Trans-\nferTransfo and tends to donate more when talking\nto ARDM produced chat-bot. Our model achieved\n27% more donations compared to TransferTransfo.\nThis indicates that our systems are more persuasive.\nIn some examples, such as the one in Table 4, our\nmodel generates coherent, natural, and persuasive\nresponses.\n5 Error Analysis\nSince CamRest676 is similar to MultiWOZ in\nterms of task content and dialog structure, we only\ndescribe the errors in MultiWOZ for simplicity.\nWe randomly selected 30 generated error responses\nfrom our model with zero inform and success score.\nTo our surprise, we observed that nearly 63.3%\nof errors are not really mistakes. It is mainly due\nto the limitation of the automatic evaluator. For\nexample, at turn one, the user asks about a restau-\nrant, and the ground truth system response is “the\n[restaurant name] is located at . . . ”, but the gener-\nated system response is “what food preference do\nyou have?”. Our generated response is correct with\nrespect to the dialog context. It is narrowing down\nthe restaurant choices before providing a restaurant\nrecommendation. However, the evaluator sticks to\nthe only possible response it has. Unless the user\ncan dynamically interact with the system, there is\nno good way to change such mistakes in the auto-\nmatic evaluator.\nWe ﬁnd that another 20% errors our model\nmakes are when the system asks information the\nuser already provided. This type of errors calls for\na better history representation. Another 10% errors\nare due to ignoring the user’s request for informa-\ntion, such as phone number. However, when we\nlook at the ground truth responses, some crowd\nworkers also made such errors. So resolving these\nerrors requires a cleaner training dataset. Finally,\nthe rest of 6.7% errors are about incorrect dialog\ndomain understanding. For example, the user is\nasking for a hotel, but we present a restaurant rec-\nommendation. This is because of the data noise\nduring the delexicalization process in which some\ndomain labels are wrong.\nThe donation persuasion system trained with\nTransferTransfo and our model has some common\nproblems, such as inconsistency, lack of logic, and\nhallucination. For example, if the persuader pro-\nvides the information about “Save the Children”,\nthen the persuadee asks “Can you tell me more\nabout it?”. The system ends up providing the same\ninformation as before. It also sometimes makes\nup facts that have never happened, such as “Save\nthe Children has an operation about a hurricane in\nHawaii”. All those errors would prevent users from\ntrusting the bot, and therefore resulting in less do-\nnation. However, we also observe that users have a\nhigher tolerance for errors in the persuasion setting\nthan the customer service setting.\nOverall, our model performs better on Persua-\nsionForGood by having longer and diversiﬁed ut-\nterances. This suggests our model which utilizes\nalternating parameters is more effective than Trans-\nferTransfo.\n6 Discussions and Ethical Consideration\nARDM models speakers separately on top of a\nlarge pre-trained language model. Such simple\nadaptation demonstrates substantial performance\ngain. We suspect it is because the interleaved struc-\nture of two language models provides a collabora-\nRole Utterance\nSYS Hi, how are you doing today?\nUSR I am good!\nSYS Have you ever heard of the charity Save the Children?\nUSR No, I have not heard about it. Would you like to talk about it?\nSYS Save the Children is an international non-governmental organization that promotes children’s\nrights, provides relief and helps support children in developing countries.\nUSR That’s cool. It is an organization for the good of this world.\nSYS Yes, it is. In the ﬁrst two months of 2018 alone, 1,000 children were reportedly killed or\ninjured in intensifying violence in Syria.\nUSR That’s sad.\nSYS\nWe do need to do something about that. One of the things that we can do is make sure that\nwe have a system in place that will ensure that every penny we spend goes directly to the\nchildren in need.\nUSR I agree.\nSYS How much do you like to donate to the charity now? Your donation will be directly deducted\nfrom your task payment. You can choose any amount from $0 to all your payment ($2).\nUSR I am willing to donate full my payment $2.\nSYS Wow, that is awesome! That is so kind of you!\nUSR You are welcome.\nTable 4: A conversation between a machine persuader trained by ARDM and a human persuadee.\ntive learning frame of both the user and the system\nlanguage distribution modeling. The memory is\nthe only way for the user and system to commu-\nnicate, as they do not share any weights in their\nnetworks. Thus, the user encoder needs to learn\nuseful representations to make the system model\nfor understanding its intent. Similarly, the system\nneeds to do the same for the user model to improve\nits understanding. This alternative repeating pro-\ncess forces both the user and system models to\npreserve the dialog history effectively in the mem-\nory. One can interpret the memory as the implicit\nrepresentation of belief states or dialog acts.\nAnother beneﬁt of ARDM is that we will obtain\nboth user and system utterance generators. We can\nlet the two models talk to each other to generate\nnew self-play dialogs (Silver et al., 2017). With\nself-play, one can rapidly build a large scale dialog\ndataset using adversarial ﬁltering (Zellers et al.,\n2018). Such models can also potentially be used in\nreinforcement learning as user simulator to study\ncomplex dialog strategies as well. We will explore\nthose possibilities in the future work.\nPersuasion is a double-edged sword. Given the\nfast development of dialog systems, an ethical de-\nsign principle must be in place throughout all stages\nof the development and evaluation. We choose the\ndonation task is because it is a relatively simple task\nthat beneﬁts children. Second, when deploying the\npersuasive agents in real conversations, we need to\nkeep the users informed of the nature of the system.\nBy revealing the identity of the persuasive agent,\nthe user should also have options to communicate\ndirectly with the human team behind the system.\nLastly, by investigating persuasive dialog systems,\nwe also envision to use them as an educational tool\nfor the general public to learn to defend themselves\nagainst machine persuasion.\n7 Conclusions\nWe propose to build Alternating Recurrent Dialog\nModel (ARDM), a simple, general, and effective\ndialog method that models user and system sepa-\nrately with large-scale pre-trained language models.\nSince ARDM does not require any annotations, it\ngeneralizes to different dialog applications. Ex-\nperimental results on CamRest676 and MultiWOZ\nsuggest that ARDM outperforms or is on-par with\nthe current state-of-the-art methods that use manual\nannotation information, such as belief states and\ndialog acts. Furthermore, we ﬁnd our model’s ex-\ncellent performance generalizes to more complex\nnon-collaborative dialog settings. It can generate\nhigh-quality responses to persuade people to do-\nnate to charity. However, the easiness of training\nARDM raises concerns about the misuse of the\nmodel in scenarios such as sales, harassment, or\nscam on a mass scale. We caution the public in\ndeploying such systems in the real world.\nReferences\nPawel Budzianowski and Ivan Vulic. 2019. Hello, it’s\nGPT-2 - how can I help you? towards the use of pre-\ntrained language models for task-oriented dialogue\nsystems. CoRR, abs/1907.05774.\nPawel Budzianowski, Tsung-Hsien Wen, Bo-Hsiang\nTseng, I ˜nigo Casanueva, Stefan Ultes, Osman Ra-\nmadan, and Milica Gasic. 2018. Multiwoz - A large-\nscale multi-domain wizard-of-oz dataset for task-\noriented dialogue modelling. In (Riloff et al., 2018),\npages 5016–5026.\nWenhu Chen, Jianshu Chen, Pengda Qin, Xifeng Yan,\nand William Yang Wang. 2019. Semantically con-\nditioned dialog response generation via hierarchical\ndisentangled self-attention. In Proceedings of the\n57th Conference of the Association for Computa-\ntional Linguistics, ACL 2019, Florence, Italy, July\n28- August 2, 2019, Volume 1: Long Papers, pages\n3696–3709. Association for Computational Linguis-\ntics.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Car-\nbonell, Quoc Viet Le, and Ruslan Salakhutdinov.\n2019. Transformer-xl: Attentive language models\nbeyond a ﬁxed-length context. In Proceedings of\nthe 57th Conference of the Association for Compu-\ntational Linguistics, ACL 2019, Florence, Italy, July\n28- August 2, 2019, Volume 1: Long Papers, pages\n2978–2988. Association for Computational Linguis-\ntics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nMihail Eric, Lakshmi Krishnan, Francois Charette, and\nChristopher D. Manning. 2017. Key-value retrieval\nnetworks for task-oriented dialogue. In Proceedings\nof the 18th Annual SIGdial Meeting on Discourse\nand Dialogue, Saarbr¨ucken, Germany, August 15-\n17, 2017, pages 37–49. Association for Computa-\ntional Linguistics.\nMihail Eric and Christopher D. Manning. 2017. A\ncopy-augmented sequence-to-sequence architecture\ngives good performance on task-oriented dialogue.\nCoRR, abs/1701.04024.\nMatthew Henderson, Blaise Thomson, and Jason D.\nWilliams. 2014. The second dialog state tracking\nchallenge. In Proceedings of the SIGDIAL 2014\nConference, The 15th Annual Meeting of the Special\nInterest Group on Discourse and Dialogue, 18-20\nJune 2014, Philadelphia, PA, USA, pages 263–272.\nThe Association for Computer Linguistics.\nAri Holtzman, Jan Buys, Maxwell Forbes, and Yejin\nChoi. 2019. The curious case of neural text degener-\nation. CoRR, abs/1904.09751.\nWenqiang Lei, Xisen Jin, Min-Yen Kan, Zhaochun\nRen, Xiangnan He, and Dawei Yin. 2018. Sequicity:\nSimplifying task-oriented dialogue systems with sin-\ngle sequence-to-sequence architectures. In Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 1437–1447, Melbourne, Australia. As-\nsociation for Computational Linguistics.\nIlya Loshchilov and Frank Hutter. 2019. Decou-\npled weight decay regularization. In 7th Inter-\nnational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nOpenReview.net.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training. OpenAI.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nEllen Riloff, David Chiang, Julia Hockenmaier, and\nJun’ichi Tsujii, editors. 2018. Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing, Brussels, Belgium, October\n31 - November 4, 2018. Association for Computa-\ntional Linguistics.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics, ACL 2016, August 7-12, 2016, Berlin, Ger-\nmany, Volume 1: Long Papers. The Association for\nComputer Linguistics.\nDavid Silver, Thomas Hubert, Julian Schrittwieser,\nIoannis Antonoglou, Matthew Lai, Arthur Guez,\nMarc Lanctot, Laurent Sifre, Dharshan Kumaran,\nThore Graepel, Timothy P. Lillicrap, Karen Si-\nmonyan, and Demis Hassabis. 2017. Mastering\nchess and shogi by self-play with a general reinforce-\nment learning algorithm. CoRR, abs/1712.01815.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. CoRR, abs/1706.03762.\nXuewei Wang, Weiyan Shi, Richard Kim, Yoojung\nOh, Sijia Yang, Jingwen Zhang, and Zhou Yu. 2019.\nPersuasion for good: Towards a personalized per-\nsuasive dialogue system for social good. CoRR,\nabs/1906.06725.\nTsung-Hsien Wen, Yishu Miao, Phil Blunsom, and\nSteve J. Young. 2017. Latent intention dialogue\nmodels. In Proceedings of the 34th International\nConference on Machine Learning, ICML 2017, Syd-\nney, NSW, Australia, 6-11 August 2017, volume 70\nof Proceedings of Machine Learning Research ,\npages 3732–3741. PMLR.\nThomas Wolf, Victor Sanh, Julien Chaumond, and\nClement Delangue. 2019. Transfertransfo: A trans-\nfer learning approach for neural network based con-\nversational agents. CoRR, abs/1901.08149.\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and\nYejin Choi. 2018. SW AG: A large-scale adversar-\nial dataset for grounded commonsense inference. In\n(Riloff et al., 2018), pages 93–104.\nSaizheng Zhang, Emily Dinan, Jack Urbanek, Arthur\nSzlam, Douwe Kiela, and Jason Weston. 2018. Per-\nsonalizing dialogue agents: I have a dog, do you\nhave pets too? In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics, ACL 2018, Melbourne, Australia, July 15-\n20, 2018, Volume 1: Long Papers, pages 2204–2213.\nAssociation for Computational Linguistics.\nTiancheng Zhao, Kaige Xie, and Maxine Esk ´enazi.\n2019. Rethinking action spaces for reinforcement\nlearning in end-to-end dialog agents with latent vari-\nable models. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, Minneapolis, MN,\nUSA, June 2-7, 2019, Volume 1 (Long and Short Pa-\npers), pages 1208–1218. Association for Computa-\ntional Linguistics.\nA MultiWOZ Evaluator Inconsistency\nWe rerun baseline models to compare our methods and ﬁnd discrepancy among different papers’ reported\nresults. In order to understand the reason, we compared between LaRL’s evaluator 3 and MultiWOZ\nBaseline’s evaluator4. We found that they make different assumptions to handle the “train” domain (line\n637-639 at LaRL evaluator.py). After carefully analyzing the code and discussing with authors of these\ntwo papers, we believe that LaRL’s evaluator is more reasonable. However, in LaRL, the authors reported\nMultiWOZ Baseline’s scores with a different evaluator. Therefore, we re-evaluated all methods, including\nLaRl, HDSA, and MultiWOZ Baseline using the same evaluator for fairness.\nBaseline Evaluator LaRL Evaluator\nInform Success Inform Success\nHuman 75.7% 67.9% 90.0% 82.3%\nHuman (the cleaned version) 82.4% 78.9% 98.9% 96.5%\nMultiWOZ Baseline 71.3% 61.0% 82.5% 72.9%\nTable 5: Re-evaluation Results on MultiWOZ.\nB Dynamic Dialog Filtering Algorithm\nSince ARDM takes the entire dialog as input, so different input has different number of turns and different\nlength in each turn. Also, there are two sub-language models in ARDM. If we pad the utterance in each\nturn and concatenate results of all turns, it is inefﬁcient. Instead, we introduce a ﬁltering algorithm to\ndynamically release memory once a dialog is ﬁnished. This enables us to speed up our model four times\nin training and eight times in inference. Please refer to our code for details.\nAlgorithm 1Dialog Filtering algorithm\nBatch Size = n\nInitialize Memory M for nworkers.\nfor Batch = 1,2,3,... do\nT = maximum number of turns in Batch\nfor Turn = 1,2,3,...,T in Batch do\nFilter M and Batch for the ﬁnished worker\nFeed remaining Batch to the model\nCompute loss and gradients\nθold ←θ\nend for\nend for\n3https://github.com/snakeztc/NeuralDialog-LaRL/blob/master/latent dialog/evaluators.py\n4https://github.com/budzianowski/multiwoz/blob/master/model/evaluator.py\nC TransferTransfo Examples\nRole Utterance\nER hello!\nEE Hello\nER have you heard of the charity save the children?\nEE no, I haven’t heard it\nER it’s an international non - governmental organization that promotes children’s\nrights, provides relief and helps support children in developing countries.\nEE ok\nER so would you like to donate some of your payment to this charity?\nEE can you tell me more information about it?\nER save the children is an international non - governmental organization that pro-\nmotes children’s rights, provides relief and helps support children in developing\ncountries.\nEE which developing countries?\nER the united states and latin america. have you heard of this charity?\nEE no, I haven’t\nER it’s an organization that helps ensure children’s rights to health, education and\nsafety.\nEE sounds cool\nER have you ever donated to this organization?\nEE no\nTable 6: A conversation between a machine persuader (ER) trained by TransferTransfo and a human persuadee\n(EE)\nRole Utterance\nER hello!\nEE hi\nER how are you today?\nEE I am good\nER have you ever heard of save the children?\nEE no, I haven’t\nER it is an organization that helps children around the world. it’s a great organiza-\ntion that does good work and is highly rated with many positive rewards.\nEE good\nER have you ever heard of it?\nEE no, I haven’t heard of it\nER it’s a great organization that does good work and is a huge part of the world’s\npopulation.\nEE okay\nER so, are you interested in donating?\nEE yes, I am\nER how much would you like to donate to the charity?\nEE $1\nER wow, that is really great. that is really great to hear. so, are you going to donate\nanything at this time??\nTable 7: A conversation between a machine persuader (ER) trained by TransferTransfo and a human persuadee\n(EE)\nD PersuasionForGood Human-Machine Examples\nWe show some more examples of dialogs generated using our proposed ARDM.\nRole Utterance\nER Hello! How are you doing today?\nEE I am doing well!\nER Awesome. Have you heard about the Save the Children charity?\nEE Yes I have heard about it.\nER Save the Children is an international non-governmental organization that pro-\nmotes children’s rights, provides relief and helps support children in developing\ncountries.\nEE So which developing countries do they help?\nER Save the Children is working on supporting children in the US and all over the\nworld.\nEE that’s cool. I want to help children as well.\nER That’s awesome! I know there are a lot of kids in the world that are in dire\nsituations.\nEE Right. What I should I do to help them?\nER Save the Children is a great organization, so you should donate some of your\npayment to them.\nEE how much is my task payment?\nER I am not sure exactly how much you will donate, but you can donate any amount\nfrom $0 to all of your payment ($2).\nEE I would like to donate $1.00.\nER That’s awesome. Your donation will make a huge impact for those kids.\nEE Thank you.\nTable 8: A conversation between a machine persuader (ER) trained by ARDM and a human persuadee (EE)\nRole Utterance\nER Hi, how are you doing today?\nEE I am good!\nER Have you ever heard of the charity Save the Children?\nEE No, I have not heard about it. Would you like to talk about it?\nER Save the Children is an international non-governmental organization that pro-\nmotes children’s rights, provides relief and helps support children in developing\ncountries.\nEE That’s cool. It is a organization for the good of this world.\nER Yes, it is. In the ﬁrst two months of 2018 alone, 1,000 children were reportedly\nkilled or injured in intensifying violence in Syria.\nEE That’s sad.\nER\nWe do need to do something about that. One of the things that we can do is\nmake sure that we have a system in place that will ensure that every penny we\nspend goes directly to the children in need.\nEE I agree.\nER\nHow much do you like to donate to the charity now? Your donation will be\ndirectly deducted from your task payment. You can choose any amount from\n$0 to all your payment ($2).\nEE I am willing to donate full my payment $2.\nER Wow, that is awesome! That is so kind of you!\nEE You are welcome.\nTable 9: A conversation between a machine persuader (ER) trained by ARDM and a human persuadee (EE)\nRole Utterance\nER Hello\nEE Hi\nER How are you doing today?\nEE I am good. thank you\nER I was wondering if you would be interested in donating a portion of your\npayment from this HIT to a children’s charity called Save the Children?\nEE Of course, I would like to. Can I know their website address?\nER Yes, URL\nEE Oh, cool. They are a good organization.\nER How much would you like to donate?\nEE I would like to donate $1.00\nER That is great! The research team will collect all donations and send it to Save\nthe Children.\nEE You are welcome.\nTable 10: A conversation between a machine persuader (ER) trained by ARDM and a human persuadee (EE)\nE PersuasionForGood Self-Play Examples\nWe also present several self-play examples generated using our proposed ARDM.\nRole Utterance\nER Hello! How are you doing today?\nEE Good, thank you. How are you?\nER I am good, thank you. Have you heard of the charity Save the Children?\nEE Yes, I have heard about it. I would like to donate a dollar to it.\nER That is wonderful, it is so important to provide education and support to the\nchildren. I am sure you will agree that this is a great cause. Do you donate to\ncharities?\nEE Yes, I do donate to several charities.\nER\nAre you sure you want to donate a dollar to Save the Children? Children all\nover the world are suffering due to poverty, war, and other issues. Would you\nconsider donating a little bit of your incoming task payment?\nEE I may donate a dollar. I don’t have a lot of money right now but I would like to\nhelp.\nER\nThat is great to hear. I know your donation is a small amount but the impact\nthis will have on the world can be signiﬁcant. Your donation would be directly\ndeducted from your task payment.\nEE That is true. I will donate one dollar to the charity.\nER Thank you so much for your donation, it will help many children.\nEE Thank you. I hope you will continue to donate to the charity.\nTable 11: A conversation between a machine persuader (ER) and a machine persuadee (EE) trained by ARDM in\nself-play setting\nRole Utterance\nER Hi there!\nEE Hi, I’m Carolyn. How are you today?\nER I am good, thank you. I am also here to talk about a charity called Save the\nChildren. Do you have any idea about that?\nEE I think it’s something that I should know about, but I don’t have the funds.\nER I understand that, I know that for sure.\nEE I am glad to hear that. How does it help children in the USA?\nER It is really great. I am here to talk to you about something that is really important\nto me. I am here to tell you about something that I am really passionate about.\nEE I think it’s a very important issue. I don’t want to donate to charities that I know\nare not helping children.\nER Yes, it is.\nEE Do you donate regularly to others?\nER I have not yet, I am not sure.\nEE I feel a little guilty for not donating when I think about the children I have lost.\nER Yeah, I totally understand.\nEE I would like to donate to a charity that I feel would be able to help all the\nchildren in the world.\nER Awesome! That is great. How much would you like to donate to Save the\nChildren?\nEE I would like to donate $1.00\nER Great! That is awesome.\nEE I am happy to hear that. I have heard about Save the Children, and it is\nsomething that I really, truly need.\nER That is so great of you.\nEE Thank you for the opportunity.\nTable 12: A conversation between a machine persuader (ER) and a machine persuadee (EE) trained by ARDM in\nself-play setting\nRole Utterance\nER hello how are you?\nEE I’m doing well, how are you?\nER I’m doing well, thanks for asking. I was wondering if you would be willing to\ndonate a portion of your task payment to save the children.\nEE I’m interested, how much will you donate?\nER I would like to donate the entire amount, but I’m not sure how much I’d be able\nto donate. Do you have any other questions?\nEE I think I know the amount I would donate.\nER\nI completely understand, I am hoping you would be willing to donate even a\nsmall amount. Save the children is an amazing charity that helps children in\nneed around the world.\nEE How much do you think I can donate?\nER I would like to donate a minimum of $2. I was wondering if you would be\nwilling to donate any more, would you be willing to donate even a little bit of\nyour task payment?\nEE I think I can donate $1.\nER That’s very kind of you. I would be happy to do that, but I can’t think of any\nother charities I would be able to donate to. I am glad to hear that.\nEE I think you are very generous, and I am sure this will help in the future.\nER It is wonderful to hear. Thank you for your donation.\nEE You are very welcome, and thank you for the opportunity to help.\nTable 13: A conversation between a machine persuader (ER) and a machine persuadee (EE) trained by ARDM in\nself-play setting\nRole Utterance\nER Hi, how are you today?\nEE I’m doing well! How are you?\nER I am doing well as well, I hope you have a nice day.\nEE I am as well.\nER Do you have children?\nEE Yes, I do. I have three boys and one girl.\nER Have you ever donated to a charity?\nEE No. I don’t.\nER Have you heard of Save the Children?\nEE I have not.\nER The charity is an international non-governmental organization that promotes\nchildren’s rights, provides relief and helps support children in developing\ncountries.\nEE I think they are a good organization.\nER I am sure they would be happy to help.\nEE I think they would.\nER Do you donate to charities?\nEE I do.\nER\nHow much do you like to donate to the charity now? Your donation will be\ndirectly deducted from your task payment. You can choose any amount from\n$0 to all your payment ($2).\nEE I would like to donate $0.50.\nER That is very generous of you.\nEE I hope they are able to help a lot of children.\nTable 14: A conversation between a machine persuader (ER) and a machine persuadee (EE) trained by ARDM in\nself-play setting",
  "topic": "Dialog box",
  "concepts": [
    {
      "name": "Dialog box",
      "score": 0.9600445032119751
    },
    {
      "name": "Computer science",
      "score": 0.8356385231018066
    },
    {
      "name": "Persuasion",
      "score": 0.8029898405075073
    },
    {
      "name": "Language model",
      "score": 0.746299147605896
    },
    {
      "name": "Natural language processing",
      "score": 0.6503137350082397
    },
    {
      "name": "Dialog system",
      "score": 0.605857789516449
    },
    {
      "name": "Task (project management)",
      "score": 0.5882025361061096
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5596759915351868
    },
    {
      "name": "Scale (ratio)",
      "score": 0.4825246036052704
    },
    {
      "name": "Simple (philosophy)",
      "score": 0.4192139208316803
    },
    {
      "name": "Human–computer interaction",
      "score": 0.3371008038520813
    },
    {
      "name": "Linguistics",
      "score": 0.1742660105228424
    },
    {
      "name": "World Wide Web",
      "score": 0.12565839290618896
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I84218800",
      "name": "University of California, Davis",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    }
  ]
}