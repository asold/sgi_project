{
    "title": "Galileo at SemEval-2020 Task 12: Multi-lingual Learning for Offensive Language Identification Using Pre-trained Language Models",
    "url": "https://openalex.org/W3117614783",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2938124241",
            "name": "Shuohuan Wang",
            "affiliations": [
                "Baidu (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2104604027",
            "name": "Jiaxiang Liu",
            "affiliations": [
                "Baidu (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2580804885",
            "name": "Xuan Ou-yang",
            "affiliations": [
                "Baidu (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2097179453",
            "name": "Yu Sun",
            "affiliations": [
                "Baidu (China)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3140418309",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2585712495",
        "https://openalex.org/W4287824654",
        "https://openalex.org/W2963490918",
        "https://openalex.org/W2944815030",
        "https://openalex.org/W2952468927",
        "https://openalex.org/W3012507282",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W1821462560",
        "https://openalex.org/W3014459433",
        "https://openalex.org/W2508035001",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2954226438",
        "https://openalex.org/W3032237992",
        "https://openalex.org/W3082274269",
        "https://openalex.org/W4360886147",
        "https://openalex.org/W2747187574",
        "https://openalex.org/W2922580172",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2739978796",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2962932155",
        "https://openalex.org/W2970854433",
        "https://openalex.org/W3032261270",
        "https://openalex.org/W2963943967",
        "https://openalex.org/W3022992164",
        "https://openalex.org/W2938830017",
        "https://openalex.org/W3115903740",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W3030332779",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W3154956759",
        "https://openalex.org/W4300849630",
        "https://openalex.org/W2912102236",
        "https://openalex.org/W2595653137",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W2996035354",
        "https://openalex.org/W2971207485",
        "https://openalex.org/W2997200074",
        "https://openalex.org/W2970049541",
        "https://openalex.org/W2914120296",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2937297214",
        "https://openalex.org/W3107826490"
    ],
    "abstract": "This paper describes Galileo’s performance in SemEval-2020 Task 12 on detecting and categorizing offensive language in social media. For Offensive Language Identification, we proposed a multi-lingual method using Pre-trained Language Models, ERNIE and XLM-R. For offensive language categorization, we proposed a knowledge distillation method trained on soft labels generated by several supervised models. Our team participated in all three sub-tasks. In Sub-task A - Offensive Language Identification, we ranked first in terms of average F1 scores in all languages. We are also the only team which ranked among the top three across all languages. We also took the first place in Sub-task B - Automatic Categorization of Offense Types and Sub-task C - Offence Target Identification.",
    "full_text": "Proceedings of the 14th International Workshop on Semantic Evaluation, pages 1448–1455\nBarcelona, Spain (Online), December 12, 2020.\n1448\nGalileo at SemEval-2020 Task 12: Multi-lingual Learning for Offensive\nLanguage Identiﬁcation using Pre-trained Language Models\nShuohuan Wang, Jiaxiang Liu, Xuan Ouyang, Yu Sun\nBaidu Inc., China\n{wangshuohuan,liujiaxiang,ouyangxuan,sunyu02}@baidu.com\nAbstract\nThis paper describes Galileo’s performance in SemEval-2020 Task 12 on detecting and catego-\nrizing offensive language in social media. For Offensive Language Identiﬁcation, we proposed\na multi-lingual method using Pre-trained Language Models, ERNIE and XLM-R. For offensive\nlanguage categorization, we proposed a knowledge distillation method trained on soft labels gen-\nerated by several supervised models. Our team participated in all three sub-tasks. In Sub-task A -\nOffensive Language Identiﬁcation, we ranked ﬁrst in terms of average F1 scores in all languages.\nWe are also the only team which ranked among the top three across all languages. We also took the\nﬁrst place in Sub-task B - Automatic Categorization of Offense Types and Sub-task C - Offence\nTarget Identiﬁcation.\n1 Introduction\nDue to the growing number of Internet users, cyber-violence emerged with offensive language pervasive\nacross social media. With anonymity as a “privilege”, netizens hide behind the screens, behaving in a\nmanner most of them would not otherwise in reality. Thus, government organizations, online communities,\nand technology companies are all striving for ways to detect aggressive language in social media and help\nbuild a more friendly online environment.\nManual ﬁltering is very time consuming and it can cause post-traumatic stress disorder-like symptoms\nto human annotators. One of the most common strategies (Waseem et al., 2017; Davidson et al., 2017;\nMalmasi and Zampieri, 2018; Kumar et al., 2018) to tackle the problem is to train systems capable of\nrecognizing offensive content, which can then be deleted or set aside human moderation.\nSemEval 2020 Task-12 (Zampieri et al., 2020) is the second edition of OffensEval (Zampieri et al.,\n2019). In this competition, organizers offers 5 languages datasets including Arabic (Mubarak et al., 2020),\nDanish (Sigurbergsson and Derczynski, 2020), English (Rosenthal et al., 2020), Turkish (C ¸¨oltekin, 2020)\nand Greek (Pitenis et al., 2020). In Sub-task A, the participants need to predict whether a post uses\noffensives language. Besides, the organizers provide other two sub-tasks which mainly focus on English,\nto predict the type and target of offensive language.\nParticipating in all 3 Sub-tasks, we proposed several methods based on pre-training language models\nincluding ERNIE and XLM-R. In Sub-task A, we scored 0.9199, 0.851, 0.8258, 0.802, 0.8989 in English,\nGreek, Turkish, Danish and Arabic respectively. We ranked ﬁrst in average F1 scores, and ranked in top\nthree across all languages. In Sub-task B and Sub-task C, we also took the ﬁrst place with 0.7462 and\n0.7145. In the following sections, we will elaborate the methods, dataset and experiments of our system.\n2 Relate Work\n2.1 Monolingual Pre-trained Language Models\nPre-training and ﬁne-tuning have become a new paradigm in natural language processing, where the\ngeneral knowledge is ﬁrstly learnt from large-scale corpus through self-supervised learning and then\nThis work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://\ncreativecommons.org/licenses/by/4.0/.\n1449\ntransferred to down-stream tasks for task-speciﬁc ﬁne-tuning. The following are some representatives.\n(Peters et al., 2018) proposed context-sensitive word vectors (ELMo) that enhance downstream tasks by\nacting as features. (Radford et al., 2018) proposed GPT which enhanced the context-sensitive embedding\nby adjusting the Transformer (Vaswani et al., 2017). (Devlin et al., 2019) modeled a bidirectional language\nmodel (BERT) through a task similar to Cloze. (Yang et al., 2019) proposed a permuted language model\n(XLNet) which is a generalized autoregressive pre-training method. (Liu et al., 2019b) remove the next\nprediction task and pre-train longer to get a better pre-trained model (RoBERTa) . (Clark et al., 2019)\nproposed a method to joint generator and discriminator in ELECTRA. (Lan et al., 2019) and (Raffel et al.,\n2019) explored the larger model structure while optimizing the pre-training strategy in ALBERT and T5.\n(Sun et al., 2019) enhanced pre-trained language models with full masking of spans in ERNIE. (Sun\net al., 2020) proposed continuous multi-task pre-training and several pre-training tasks in ERNIE 2.0.\nThe researchers of ERNIE 2.0 released a new version recently which made a few improvements on\nknowledge masking and application-oriented tasks, with the aim to advance the model’s general semantic\nrepresentation capability. In order to improve the knowledge masking strategy, they proposed a new\nmutual information based dynamic knowledge masking algorithm. They also constructed pre-training\ntasks that are speciﬁc for different applications. For example, they added a coreference resolution task to\nidentify all expressions in a text that refer to the same entity. For more details, please go to this website1 .\n2.2 Cross-lingual Pre-trained Language Models\nIn addition, there are also a lot of works on multilingual language models. (Devlin et al., 2019) provided a\nmultilingual version of BERT that demonstrates surprising cross-language capabilities (Wu and Dredze,\n2019). (Conneau and Lample, 2019) proposed two tasks, Masked Language Model and Translation\nLanguage Model, to model monolingual corpus and bilingual parallel corpus respectively. (Huang et al.,\n2019) proposed Unicoder incorporate more bilingual parallel corpus modeling methods. (Song et al.,\n2019) and (Liu et al., 2020) proposed modeling methods that are more suitable for machine translation\ntasks in MASS and MBART. (Conneau et al., 2019) used the ideas of RoBERTa in XLM-R and achieved\nbetter results than XLM.\n2.3 Methods of Offensive Language Detection and Categorization\nIn the last few years, there have been several studies on the application of computational methods to cope\nwith offensive language. (Waseem et al., 2017) proposed a typology that captures central similarities\nand differences between subtasks. (Davidson et al., 2017) trained a multi-class classiﬁer to distinguish\nbetween these different categories. (Malmasi and Zampieri, 2018) employed supervised classiﬁcation\nalong with a set of features that includes n-grams, skip-grams and clustering-based word representations.\nThere are also several workshops for this problem. Such as AWL2 and TRAC3 (Kumar et al., 2018) .\nBesides, there are several works focus on offensive language identiﬁcation in languages other than\nEnglish, such as Chinese (Su et al., 2017), Dutch (Tulkens et al., 2016), German (Ross et al., 2016),\nSlovene (Fiˇser et al., 2017) and Arabic (Mubarak et al., 2017). There are also some researches (Basile et\nal., 2019; Mandl et al., 2019) about multilingual offensive language identiﬁcation.\n3 Methodology\n3.1 Multi-lingual Offensive Language Detection\nIn Sub-task A, we expected to build a uniﬁed approach to detect offensive language in all languages.\nOur algorithm has two steps. In the ﬁrst step, pre-training using large scale multilingual unsupervised\ntexts yields a uniﬁed pre-training model that can learn all the language representations together. In the\nsecond step, the pre-trained model was ﬁne-tuned with labeled data. The detailed process is shown in\nFigure 1.\n1http://research.baidu.com/Blog/index-view?id=128\n2https://sites.google.com/view/alw3/\n3https://sites.google.com/view/trac1/home\n1450\nModel\nAstronomy is one of the \noldest [M] sciences.\nΣε [M] καριέρα που \nδιαρκεί [M] και έξι \nδεκαετίες\nAstronomy is one of the \noldest natural sciences.\nCelsius bugün astronomi \nalanındaki \nçalışmalarında çok\nVerdi, Busetto’ya\ndöndüğünde şehir \norkestrasının \nΣε μια καριέρα που \nδιαρκεί εδώ και έξι \nδεκαετίες\n اﻟﺗﻧظﯾم ﻓﻲ ﻣﻧﺎطق ﻣﻌﯾﻧﺔ\nﻣن إﯾطﺎﻟﯾﺎ، واﻟﺗﻲ\nCelsius bugün [M]\nalanındaki \nçalışmalarında çok\nVerdi, Busetto’ya\ndöndüğünde [M]\norkestrasının \n اﻟﺗﻧظﯾم ﻓﻲ ﻣﻧﺎطق ﻣﻌﯾﻧﺔ\nﻣن إﯾطﺎﻟﯾﺎ، واﻟﺗﻲ\n[M]\nModel\n@USER Antifa has TS \nlevel influence. It's scary.\nΙταλός είναι ο Αλεξις; \n#gntmgr\nNOT\nOFF\nNOT\nOFF\nOFF\nFuckit, har upvoted\ndummere i dag.\nاي ﯾﻠﻌﻦ اي ﯾﻀﺢ دوﺳﻞ\nSadece iki takipçim \nvarken aşk acısı çekeyim \nbari\nMulti-lingual Pretraining\nwith Unsupervised Data\nMulti-lingual Fine-tuning\nwith Offensive Detection Data\nFigure 1: The Framework of Multi-lingual Offensive Language Detection\nTo skip large-scale pre-training, we used an existing open-source model, XLM-R, as our ﬁrst step. With\nTransformer as the backbone structure, XLM-R was pretrained with masked language model on Common\nCrawl dataset in over 100 languages.\nWe add a full connected layer for classiﬁcation upon the [CLS] position of the top layer of XLM-R,\nusing the same parameter for all languages.\nThis approach can beneﬁt from dataset in other languages and enhance the generality of the model. We\nwill compare methods trained on multilingual data with those on monolingual data in Section 5.\n3.2 Offensive Language Categorization using Knowledge Distillation trained on Soft Labels\nIn Sub-task B and Sub-task C, we constructed a knowledge distillation approach (Hinton et al., 2015;\nDavidson et al., 2017; Liu et al., 2019a) . Several supervised models provided calculated the probability\nof each label and generated a weighted probability (here we call it soft label). Then the student model was\ntrained on those soft labels. Detailed process is shown in Figure 2.\nSuppose that X is the contextual embedding of the token [CLS], which can be viewed as the semantic\nrepresentation of input sentence. Let Q(c|X) be the class probabilities produced by the ensemble of\nseveral supervised models. The probability Pr(c|X) that X is labeled as class c is predicted by a softmax\nlayer. We use the standard cross entropy loss to learn the soft target:\nLoss = −\n∑\nc\nQ(c|X) log(Pr(c|X)) (1)\nWe used ERNIE 2.0 and ALBERT as our candidates of pre-training language models in Sub-task B and\nSub-task C.\n4 Dataset\nWe used datasets of OffensEval 2019 and OffensEval 2020 as our training data. In OffensEval 2019,\nthe organizers provide a dataset containing English tweets annotated using a hierarchical three-level\nannotation. In OffensEval 2020, the organizers did not provide additional data in English for training.\nThey provided training data for four other languages, Turkish, Danish, Greek and Arabic. In addition,\nthey provide a large amount of weakly labeled data generated by several supervised models.\n1451\nStudent Model\nTeacher Model C\nTeacher Model B\nTeacher Model A\npost ensemble\npredict\nModel Distillation for Offensive Categorization\nFigure 2: The Framework of Offensive Language Categorization\n4.1 Sub-Task A - Offensive Language Identiﬁcation\nIn Sub-task A, the goal is to discriminate between offensive and non-offensive posts. Offensive posts\ninclude insults, threats, and posts containing any form of untargeted profanity. Each instance is assigned\none of the following two labels. ’NOT’ means posts which do not contain offense or profanity. ’OFF’\nmeans posts containing offense any form of non-acceptable language or a targeted offense.\nIn order to avoid uneven proportions of data across languages, we did not use the unannotated English\ndata from OffensEval 2020. Instead, we used a mix of English data from OffensEval 2019 (including\ntraining data and test data) and training data from OffensEval 2020 in the other 4 languages as our training\ndata. Details are shown in Table 1.\nLanguages Train Test\nOFF NOT TOTAL OFF NOT TOTAL\nEnglish 4640 9460 14100 1080 2807 3887\nTurkish 6131 25625 31756 716 2812 3528\nArabic 1589 6411 8000 402 1598 2000\nDanish 384 2577 2961 41 288 329\nGreek 2486 6257 8743 242 1302 1546\nTable 1: Dataset Statistics for Sub-task A\n4.2 Sub-Task B - Automatic Offense Language Categorization\nIn Sub-task B, the goal is to predict the type of offense. There are two types in sub-task B are the following.\n’TIN’ means posts containing an insult or threat to an individual, group, or others. ’UNT’ means posts\ncontaining non-targeted profanity and swearing. The dataset consists of two parts, a small portion of the\nmanually annotated dataset from OffensEval 2019 and a large portion of the dataset from OffensEval\n2020 constructed based on multiple supervision models. All the training data in OffensEval 2020 provides\nthe conﬁdence that it has a target to attack. Details are shown in Table 2.\nDataSet Train Test\nTIN UNT TOTAL TIN UNT TOTAL\nOffensEval 2019 4089 551 4640 - - -\nOffensEval 2020 149550 39424 188974 850 572 1422\nTable 2: Dataset Statistics for Sub-Task B\n1452\n4.3 Sub-Task C - Offense Target Identiﬁcation\nIn Sub-Task C, the goal is to predict the target of offense. The three labels in Sub-task C are the following.\n’IND’ means posts targeting an individual. ’GRP’ means the target of these offensive posts is a group\nof people. ’OTH’ means the target of these offensive posts does not belong to any of the previous two\ncategories. As with Sub-task B, all training data in OffensEval 2020 provide the conﬁdence level for each\nlabel. Details are shown in Table 3.\nDataSet Train Test\nIND GRP OTH TOTAL IND GRP OTH TOTAL\nOffensEval 2019 2507 1152 430 4089 - - - -\nOffensEval 2020 152562 24917 11494 188973 580 190 80 850\nTable 3: Dataset Statistics for Sub-Task C\n5 Experiments\n5.1 Results of Sub-task A\nWe validated our proposed methods based on two models, XLM-R Base and XLM-R Large. The metric\nused is the average F1 score of all labels. To make the results more reliable, we repeated the experiment 5\ntimes and used the average F1 score. In Table 4, we can see that the result of multilingual ﬁne-tuning is\nbetter in all languages except Turkish. It might be caused by it taking up the largest proportion among all\nlanguages, leading to data of other languages being ignored. In the table below, we also listed the ﬁnal\nsubmitted results and ranks in the contest, where we used ten-fold cross-validation-based ensemble of\nXLM-R Large.\nLanguages XLM-R BASE XLM-R LARGE Submitted Result(Ensemble)\nSingle Multi Single Multi Multi rank in all teams\nEnglish 0.9150 0.9214 0.9186 0.9255 0.9199 3\nTurkish 0.8081 0.8084 0.8265 0.8224 0.8258 1\nArabic 0.8649 0.8730 0.8969 0.9015 0.8989 3\nDanish 0.7733 0.7922 0.7908 0.8136 0.8020 2\nGreek 0.8266 0.8356 0.8356 0.8392 0.8510 2\nAverage 0.8376 0.8461 0.8537 0.8604 0.8595 1\nTable 4: Results for Sub-Task A. We report the F1 score of all languages.\n5.2 Results of Sub-task B and Sub-task C\nIn both Sub-Task B and Sub-Task C, we made a comparison between hard target-based approach and soft\ntarget-based approach. Two models were used for validation, which are ALBERT-XXLarge and ERNIE\n2.0. The results are shown in Table 5 and Table 6, where it can be seen that the knowledge distillation\napproach is helpful for offensive categorization.\nSame with Sub-Task A, the metric used is the average F1 score of all labels. Again, to make it more\nreliable, the average score of 5 repeated experiments was adopted. We also listed our ﬁnal submitted\nresults below, which were obtained using ten-fold cross-validation-based ensemble of ERNIE 2.0.\n6 Conclusion\nIn this paper, we presented our approach on detecting and categorizing offensive language in social\nmedia. We proposed a multi-lingual learning method to detect offensive language and a knowledge\n1453\nMethod ALBERT(F1) ERNIE 2.0(F1)\nLearning with Hard Target 0.6810 0.6883\nLearning with Soft Target 0.7043 0.7124\nSubmitted Result - 0.7462\nTable 5: Results for Sub-task B\nMethod ALBERT(F1) ERNIE 2.0(F1)\nLearning with Hard Target 0.6727 0.6773\nLearning with Soft Target 0.6864 0.6894\nSubmitted Result - 0.7145\nTable 6: Results for Sub-task C\ndistillation method to categorize offensive language. We will further our exploration of multilingual\noffensive language identiﬁcation in future, e.g. validating the zero-shot performance of our model in more\nlanguages.\nReferences\nValerio Basile, Cristina Bosco, Elisabetta Fersini, Debora Nozza, Viviana Patti, Francisco Manuel Rangel Pardo,\nPaolo Rosso, and Manuela Sanguinetti. 2019. Semeval-2019 task 5: Multilingual detection of hate speech\nagainst immigrants and women in twitter. In Proceedings of the 13th International Workshop on Semantic\nEvaluation, pages 54–63.\nC ¸ a˘grı C ¸¨oltekin. 2020. A Corpus of Turkish Offensive Language on Social Media. In Proceedings of the 12th\nInternational Conference on Language Resources and Evaluation. ELRA.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. 2019. Electra: Pre-training text\nencoders as discriminators rather than generators. In International Conference on Learning Representations.\nAlexis Conneau and Guillaume Lample. 2019. Cross-lingual language model pretraining. In Advances in Neural\nInformation Processing Systems, pages 7059–7069.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm´an,\nEdouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Unsupervised cross-lingual repre-\nsentation learning at scale. arXiv preprint arXiv:1911.02116.\nThomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. 2017. Automated hate speech detection\nand the problem of offensive language. In Eleventh international aaai conference on web and social media.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. In NAACL-HLT (1).\nDarja Fiˇser, Tomaˇz Erjavec, and Nikola Ljube ˇsi´c. 2017. Legal Framework, Dataset and Annotation Schema for\nSocially Unacceptable On-line Discourse Practices in Slovene. In Proceedings of the Workshop Workshop on\nAbusive Language Online (ALW), Vancouver, Canada.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in a neural network.arXiv preprint\narXiv:1503.02531.\nHaoyang Huang, Yaobo Liang, Nan Duan, Ming Gong, Linjun Shou, Daxin Jiang, and Ming Zhou. 2019. Uni-\ncoder: A universal language encoder by pre-training with multiple cross-lingual tasks. In Proceedings of the\n2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Con-\nference on Natural Language Processing (EMNLP-IJCNLP), pages 2485–2494.\nRitesh Kumar, Atul Kr Ojha, Shervin Malmasi, and Marcos Zampieri. 2018. Benchmarking aggression iden-\ntiﬁcation in social media. In Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying\n(TRAC-2018), pages 1–11.\n1454\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019.\nAlbert: A lite bert for self-supervised learning of language representations. In International Conference on\nLearning Representations.\nXiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. 2019a. Improving multi-task deep neural net-\nworks via knowledge distillation for natural language understanding. arXiv preprint arXiv:1904.09482.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. 2019b. Roberta: A robustly optimized bert pretraining approach. arXiv\npreprint arXiv:1907.11692.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke\nZettlemoyer. 2020. Multilingual denoising pre-training for neural machine translation. arXiv preprint\narXiv:2001.08210.\nShervin Malmasi and Marcos Zampieri. 2018. Challenges in discriminating profanity from hate speech. Journal\nof Experimental & Theoretical Artiﬁcial Intelligence, 30(2):187–202.\nThomas Mandl, Sandip Modha, Prasenjit Majumder, Daksh Patel, Mohana Dave, Chintak Mandlia, and Aditya\nPatel. 2019. Overview of the hasoc track at ﬁre 2019: Hate speech and offensive content identiﬁcation in indo-\neuropean languages. In Proceedings of the 11th Forum for Information Retrieval Evaluation, pages 14–17.\nHamdy Mubarak, Darwish Kareem, and Magdy Walid. 2017. Abusive Language Detection on Arabic Social\nMedia. In Proceedings of the Workshop on Abusive Language Online (ALW), Vancouver, Canada.\nHamdy Mubarak, Ammar Rashed, Kareem Darwish, Younes Samih, and Ahmed Abdelali. 2020. Arabic offensive\nlanguage on twitter: Analysis and experiments. arXiv preprint arXiv:2004.02192.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettle-\nmoyer. 2018. Deep contextualized word representations. In Proceedings of NAACL-HLT, pages 2227–2237.\nZeses Pitenis, Marcos Zampieri, and Tharindu Ranasinghe. 2020. Offensive Language Identiﬁcation in Greek. In\nProceedings of the 12th Language Resources and Evaluation Conference. ELRA.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding\nby generative pre-training.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li,\nand Peter J Liu. 2019. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. arXiv\npreprint arXiv:1910.10683.\nSara Rosenthal, Pepa Atanasova, Georgi Karadzhov, Marcos Zampieri, and Preslav Nakov. 2020. A Large-Scale\nSemi-Supervised Dataset for Offensive Language Identiﬁcation. In arxiv.\nBj¨orn Ross, Michael Rist, Guillermo Carbonell, Benjamin Cabrera, Nils Kurowsky, and Michael Wojatzki. 2016.\nMeasuring the Reliability of Hate Speech Annotations: The Case of the European Refugee Crisis. In Proceed-\nings of the Workshop on Natural Language Processing for Computer-Mediated Communication (NLP4CMC) ,\nBochum, Germany.\nGudbjartur Ingi Sigurbergsson and Leon Derczynski. 2020. Offensive Language and Hate Speech Detection for\nDanish. In Proceedings of the 12th Language Resources and Evaluation Conference. ELRA.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. 2019. Mass: Masked sequence to sequence pre-\ntraining for language generation. In ICML.\nHuei-Po Su, Chen-Jie Huang, Hao-Tsung Chang, and Chuan-Jie Lin. 2017. Rephrasing Profanity in Chinese Text.\nIn Proceedings of the Workshop Workshop on Abusive Language Online (ALW), Vancouver, Canada.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao\nTian, and Hua Wu. 2019. Ernie: Enhanced representation through knowledge integration. arXiv preprint\narXiv:1904.09223.\nYu Sun, Shuohuan Wang, Yu-Kun Li, Shikun Feng, Hao Tian, Hua Wu, and Haifeng Wang. 2020. Ernie 2.0: A\ncontinual pre-training framework for language understanding. In AAAI, pages 8968–8975.\nSt´ephan Tulkens, Lisa Hilte, Elise Lodewyckx, Ben Verhoeven, and Walter Daelemans. 2016. A Dictionary-based\nApproach to Racism Detection in Dutch Social Media. In Proceedings of the Workshop Text Analytics for\nCybersecurity and Online Safety (TA-COS), Portoroz, Slovenia.\n1455\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and\nIllia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems, pages\n5998–6008.\nZeerak Waseem, Thomas Davidson, Dana Warmsley, and Ingmar Weber. 2017. Understanding abuse: A typology\nof abusive language detection subtasks. In Proceedings of the First Workshop on Abusive Language Online ,\npages 78–84.\nShijie Wu and Mark Dredze. 2019. Beto, bentz, becas: The surprising cross-lingual effectiveness of bert. In\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 833–844.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. 2019. Xlnet:\nGeneralized autoregressive pretraining for language understanding. In Advances in neural information process-\ning systems, pages 5754–5764.\nMarcos Zampieri, Shervin Malmasi, Preslav Nakov, Sara Rosenthal, Noura Farra, and Ritesh Kumar. 2019.\nSemeval-2019 task 6: Identifying and categorizing offensive language in social media (offenseval). In Pro-\nceedings of the 13th International Workshop on Semantic Evaluation, pages 75–86.\nMarcos Zampieri, Preslav Nakov, Sara Rosenthal, Pepa Atanasova, Georgi Karadzhov, Hamdy Mubarak, Leon\nDerczynski, Zeses Pitenis, and C ¸ a˘grı C ¸¨oltekin. 2020. SemEval-2020 Task 12: Multilingual Offensive Language\nIdentiﬁcation in Social Media (OffensEval 2020). In Proceedings of SemEval."
}