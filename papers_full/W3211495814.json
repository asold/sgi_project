{
  "title": "Vision Guided Generative Pre-trained Language Models for Multimodal Abstractive Summarization",
  "url": "https://openalex.org/W3211495814",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A4222272757",
      "name": "Yu, Tiezheng",
      "affiliations": [
        "University of Hong Kong",
        "Hong Kong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2362940212",
      "name": "Dai Wen-liang",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2358019462",
      "name": "Liu, Zihan",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2743188307",
      "name": "Fung, Pascale",
      "affiliations": [
        "University of Hong Kong",
        "Hong Kong University of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2101105183",
    "https://openalex.org/W3120237956",
    "https://openalex.org/W1947481528",
    "https://openalex.org/W2964010806",
    "https://openalex.org/W2997591391",
    "https://openalex.org/W2116492146",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2150824314",
    "https://openalex.org/W1933349210",
    "https://openalex.org/W2962934715",
    "https://openalex.org/W2560730294",
    "https://openalex.org/W3016211260",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W3104210310",
    "https://openalex.org/W3006320872",
    "https://openalex.org/W2619947201",
    "https://openalex.org/W3100439847",
    "https://openalex.org/W3014611590",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2970263339",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2969876226",
    "https://openalex.org/W2964051877",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2612675303",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2133459682",
    "https://openalex.org/W3114963861",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W2998356391",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W3168745292",
    "https://openalex.org/W3167118264",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2981851019",
    "https://openalex.org/W2963407669",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2996264288",
    "https://openalex.org/W1503933356",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3126792443",
    "https://openalex.org/W3034715004",
    "https://openalex.org/W1956340063",
    "https://openalex.org/W3034266838",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W2966683369",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W1889081078",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W2953104586",
    "https://openalex.org/W3168391820",
    "https://openalex.org/W2963115613",
    "https://openalex.org/W2759570332",
    "https://openalex.org/W3169565655",
    "https://openalex.org/W3103670612",
    "https://openalex.org/W3090449556",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W2963542740",
    "https://openalex.org/W3087434251",
    "https://openalex.org/W2963530300",
    "https://openalex.org/W2899274165",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2123301721"
  ],
  "abstract": "Multimodal abstractive summarization (MAS) models that summarize videos (vision modality) and their corresponding transcripts (text modality) are able to extract the essential information from massive multimodal data on the Internet. Recently, large-scale generative pretrained language models (GPLMs) have been shown to be effective in text generation tasks. However, existing MAS models cannot leverage GPLMs’ powerful generation ability. To fill this research gap, we aim to study two research questions: 1) how to inject visual information into GPLMs without hurting their generation ability; and 2) where is the optimal place in GPLMs to inject the visual information? In this paper, we present a simple yet effective method to construct vision guided (VG) GPLMs for the MAS task using attention-based add-on layers to incorporate visual information while maintaining their original text generation ability. Results show that our best model significantly surpasses the prior state-of-the-art model by 5.7 ROUGE-1, 5.3 ROUGE-2, and 5.1 ROUGE-L scores on the How2 dataset (Sanabria et al., 2018), and our visual guidance method contributes 83.6% of the overall improvement. Furthermore, we conduct thorough ablation studies to analyze the effectiveness of various modality fusion methods and fusion locations.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3995–4007\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n3995\nVision Guided Generative Pre-trained Language Models for\nMultimodal Abstractive Summarization\nTiezheng Yu∗, Wenliang Dai∗, Zihan Liu, Pascale Fung\ncenter for artiﬁcial intelligence research HcaireI\ndepartment of electronic and computer engineering\nthe hong kong university of science and technologyL clear water bayL hong kong\n{tyuah,wdaiai,zliucr}@connect.ust.hkL pascale@ece.ust.hk\nAbstract\nmultimodal abstractive summarization HmasI\nmodels that summarize videos Hvision modalM\nityI and their corresponding transcripts Htext\nmodalityIareabletoextracttheessentialinforM\nmation from massive multimodal data on the\ninternetN recentlyL largeMscale generative preM\ntrained language models HgplmsI have been\nshown to be e;ective in text generation tasksN\nhoweverL existing mas models cannot leverM\nage gplmsG powerful generation abilityN to\nﬁll this research gapL we aim to study two reM\nsearch questionsZ QI how to inject visual inM\nformation into gplms without hurting their\ngeneration ability[ and RI where is the optiM\nmal place in gplms to inject the visual inM\nformation? in this paperL we present a simM\nple yet e;ective method to construct vision\nguided HvgI gplms for the mas task using\nattentionMbasedaddMonlayerstoincorporateviM\nsual information while maintaining their origM\ninal text generation abilityN results show that\nourbestmodelsigniﬁcantlysurpassestheprior\nstateMofMtheMart model by UNW rougeMQL UNS\nrougeMRL and UNQ rougeMl scores on the\nhowR dataset Hsanabria et alNL RPQXIL and our\nvisual guidance method contributes XSNVE of\ntheoverallimprovementN furthermoreLweconM\nductthoroughablationstudiestoanalyzetheefM\nfectivenessofvariousmodalityfusionmethods\nand fusion locationsN/one.sup\n1 Introduction\nmultimodalabstractivesummarizationHmasIaims\nto take advantage of data from multiple modalities\nand provides a shortL concise and readable textual\nsummary to let users quickly acquire their essential\ninformation Hsanabria et alNL RPQX[ palaskar et alNL\nRPQY[ liu et alNL RPRPIN mas has become an\nincreasingly popular research area thanks to the\nproliferation of online multimedia content and the\nincreasing availability of multimodal dataN\n∗∗the two authors contribute equallyN\n/one.supthe code is available atZ https://github.com/\nHLTCHKUST/VG-GPLMs\nVideo Frames\nTranscript: so  now  we  are  going  to  go  over  some  basics  sheet music\nreadings for the key of g flat major. so you noticed the key of g flat, when\nyou are reading real books, there is going to be a treble cleft here. it is going\nto have 6 flats 1, b flat, e flat, a flat, d flat, g flat and c flat.  so 6 flats equals\nkey of g flat.  [...] so if  you have a flat and there is a natural sign, play the a.\nso go through the scale and you've got g flat, a flat, d flat, c, flat, d flat, e flat\nand f, so f is your only 9 flat note in the scale. (No mention of the piano)\nReference Summary: learn   how  to  read  and  write  music  intervals  for\nimproving  your playing and improvisational skills on  the piano in this free\nvideo clip series.\nSummary from Transcript (BART): learn  tips  on  how  to  read and write\nintervals  on sheet music  in  this  free video clip on  music theory and music\nlessons.\nSummary from Transcript+Video (VG-BART): learn  how  to  sight read in\nthe key of g flat for improving your playing and improvisational skills on the\npiano in this free video clip series.\nFigure1: anexampleofmasNasinputdataLweshow\ntworepresentativevideoframesandthetranscriptLwith\n{NNN} representing omitted unimportant textN as illusM\ntratedL some information is emphasized HeNgNthe key\nofgﬂat IoronlyexistsHeNgN pianoIinthevisualsignalN\nwe also compare the humanMgenerated reference sumM\nmaryandourmodelMgeneratedsummarieswithOwithout\nvideo frames in the input dataN\nas illustrated in figure QL the mas models need\nto generate a concise summary by e;ectively utiM\nlizing two modalitiesZ a video and its transcriptN\nthereforeLweemphasizethatleveragingapowerful\ntext generation model and an e;ective combination\nofthevisionandtextmodalitiesarekeytoconstructM\ning good mas modelsN recentlyL transformerM\nbasedHvaswanietalNLRPQWbIsequenceMtoMsequence\nHseqRseqI largeMscale generative preMtrained lanM\nguagemodelsHgplmsILsuchasbartHlewisetalNL\nRPQYIL tU Hra;el et alNL RPQYIL pegasus Hzhang\netalNLRPRPaIandprophetnetHqietalNLRPRPILhave\nshown remarkable performance on text generation\ntasksL including abstractive text summarizationN\nhoweverL leveraging and adapting gplms to mas\n3996\nis still an unexplored research directionN to exM\nplore this directionL two main questions need to be\nansweredZ firstlyLhowcanweinjectvisualinformaM\ntionintothetextMonlygplmssothatthemodelscan\nunderstand both modalities and allow crossMmodal\ninteractionsL and more importantlyL how can this\ninjectionoperationbeconductedwithoutdamaging\ngplmsG original text generation ability? secondlyL\nwhereis the optimal place in gplms to inject the\nvisual information? this needs to be exploredL as\nthere are many subMlayers in the encoder and deM\ncoder of gplms and a subMoptimal location might\nresult in unsatisfactory performanceN\ninthispaperLtoﬁlltheresearchgapLwepresenta\nsimpleyetverye;ectivemethodtoconstructvision\nguided HvgI gplms HvgMbart and vgMtUI for\nthemastaskN speciﬁcallyLtoanswertheﬁrstofthe\naforementionedquestionsLweinsertattentionMbased\naddMonlayerstogplmstoincorporatevisualinforM\nmation without modifying the original architectureN\nin this wayL all the preMtrained model weights can\nbe used during ﬁneMtuning so as to preserve their\noriginal text generation abilityN we try with two\ntypes of attention mechanisms for the textMvision\nfusionandinteractionZ QIcrossMmodaldotMproduct\nattention[ and RI crossMmodal multiMhead attenM\ntionN moreoverL we also investigate the e;ects of\nusingaforgetgateandavisualtransformerencoder\nalongwiththeattentionmechanismsN toanswerthe\nsecond questionL we enumerate almost all possible\nlocations in gplms for injecting addMon layersL\nand show a thorough comparison and analysis in\nsection UN we evaluate our models on the howR\ndataset Hsanabria et alNL RPQXIN experimental reM\nsults demonstrate that our best model surpasses the\nprior stateMofMtheMart model by UNW rougeMQL UNS\nrougeMRL and UNQ rougeMl scoresN to ensure\nthis improvement does not purely come from the\ngplmsL we also evaluate the corresponding textM\nonly modelL and the results show that the injected\nvisual guidance contributes XSNVE of the overall\nimprovement on average of all rouge scoresN\nour contributions in this work are threefoldZ\n• to the best of our knowledgeL we are the\nﬁrst to inject visual information into textMonly\ngplmsL and to use it for the mas taskN\n• we systematically study two research quesM\ntionsZ QI how to inject visual information\ninto gplms without hurting their generation\nability[ and RI where is the optimal place in\ngplms to inject the visual information?\n• our model signiﬁcantly outperforms the stateM\nofMtheMart model on the howR datasetL and the\ninjected visual guidance contributes XSNVE of\nthe overall improvementN\n2 Related Work\n2.1 Abstractive Text Summarization\nabstractive text summarization aims to generate\nshortLconciseandreadabletextthatcancapturethe\nmost salient information of the input documentsN\nthanks to the seqRseq framework Hsutskever et alNL\nRPQTI and attention mechanismsL deep neural netM\nworks have achieved remarkable results on sumM\nmarization tasks Hpaulus et alNL RPQW[ zhang et alNL\nRPRPb[ yu et alNL RPRQIN recentlyL gplms Hlewis\net alNL RPQY[ ra;el et alNL RPQY[ zhang et alNL RPRPa[\nqietalNLRPRPIhavebeenwidelyusedinabstractive\ntext summarization and have achieved startMofMtheM\nart performanceN the most signiﬁcant di;erence\nbetween abstractive text summarization and mulM\ntimodal abstractive summarization lies in whether\nthe input contains data of more than one modalityN\n2.2 Multimodal Abstractive Summarization\nrecentlyLmanystudieshavebeenperformedonmulM\ntimodal learning Hmroueh et alNL RPQU[ antol et alNL\nRPQU[ donahue et alNL RPQU[ zadehet alNL RPQW[ dai\netalNLRPRPLRPRQIN howeverL onlyafewhaveinvesM\ntigatedmasNlietalNHRPQWIcollectedamultimodal\ncorpus of news articles containing UPP videos of\nenglish news articles paired with humanMannotated\nsummariesN sanabria et alN HRPQXI introduced the\nhowR datasetL which contains about RLPPP hours\nof short instructional videosL each coming with a\nsummary of two to three sentencesN palaskar et alN\nHRPQYI proposed a multiMsource seqRseq model\nwith hierarchical attention to integrate information\nfrom di;erent modalities into a coherent summaryN\nmeanwhileL liu et alN HRPRPI proposed a multiM\nstage fusion network with the fusion forget gate\nmoduleL which can model the ﬁneMgrained interacM\ntions between multiMsource modalitiesN to the best\nof our knowledgeL no previous work has leveraged\ngplmsG generation ability to tackle the mas taskL\nand we are the ﬁrst to systematically study multiple\nmultimodal fusion methods based on gplmsN\n2.3 Vision-Language Large Pre-trained\nTransformer Models\nwith the remarkable success of largeMscale unsuM\npervised preMtraining in nlp Hdevlin et alNL RPQY[\n3997\n...\nPositional\nEncoding\nMulti-head \nSelf-Attention\nAdd & Norm\nFFN\nAdd & Norm\nText-Vision\nFusion\nAdd & Norm\n...\nVision Features\nText Inputs\n...\nPositional\nEncoding\n Masked Multi-head \nSelf-Attention\nAdd & Norm\nAdd & Norm\nAdd & Norm\nMulti-head \nEnc-Dec Attention\nFFN\nLinear\nSoftmax\nOutput Probabilities\nOutputs (shifted right)\nCross-modal \nDot-Product Attention\nLinear LinearLinear\nCross-modal\nMulti-head Attention\n(1) \n(2) \n... ...\n...\n... ...\n...\nLinear\nFigure 2:an overview of our proposed vg gplmsN it is built based on the transformerMbased seqRseq gplms\nHleftIN to inject visual informationL we insert addMon subMlayers Hthe green dashed blockI by mainly leveraging\ntwo kinds of attentionMbased textMvision fusion mechanism HrightIZ QI crossMmodal dotMproduct attention[ and RI\ncrossMmodalmultiMheadattentionN althoughwedrawtheaddMonsubMlayersintheencoderLtheycanalsobeplaced\nin the decoder in a similar wayN we compare the e;ects of di;erent injection locations in section UN\nliu et alNL RPQY[ radford et alNL RPQYIL preMtraining\nlargevisionMlanguageHvlImodelshasalsobecome\nmoreandmorepopularinrecentyearsN ratherthan\ndesigning taskMspeciﬁc architecturesL preMtraining\nresults in a general backbone model by feeding it\nwith a large amount of data and then ﬁneMtune it\nto di;erent downstream tasksN among the current\nvl preMtraining workL most has been focusing on\nvl understanding by training bertMstyle transM\nformer models Hsun et alNL RPQY[ tan and bansalL\nRPQY[ su et alNL RPRP[ li et alNL RPRP[ chen et alNL\nRPRPI and ﬁnetune them on various vl classiM\nﬁcation tasks Hgoyal et alNL RPQW[ zellers et alNL\nRPQY[ suhr et alNL RPQYIN these models usually\nreceive a pair of text and image as inputL where\nthe image is processed into objects Hzhang et alNL\nRPRQIL patches Hkim et alNL RPRQIL or pixels Hhuang\net alNL RPRPI before feeding into the vl modelN for\nvl text generationL zhou et alN HRPRPI presented\na model for both visual question answering and\nimage captioning Hchen et alNL RPQUIN additionallyL\ncho et alN HRPRQI introduced an encoderMdecoder\ntransformer model that uniﬁes all vl tasks as\ngenerative tasksN although prior work has made\nmuch progress on vl preMtrainingL the problem of\ngenerating text given text and video input HeNgN the\nhowRdatasetIisnotwellstudiedunderthevlpreM\ntraining settingL except by luo et alN HRPRPIL who\nproposed a dualMstream model for both vl classiM\nﬁcation and generation with video dataN howeverL\ncomparedtogplmsinnlpsuchasbartHlewis\net alNL RPQYI and tU Hra;el et alNL RPQYIL their text\ngeneration ability is limited as the training data is\nmuch smallerN\nin this paperL we propose to tackle vl tasks and\nutilizetheadvantageofpreMtrainingfromadi;erent\nangle by inserting addMon layers to the textMonly\ngplms and ﬁneMtuning them on multimodal tasks\nto incorporate visual informationN this takes adM\nvantage of gplmsG superior generation ability to\ngenerate visionMaware textsN of the very few works\nthat have also considered this directionL rahman\net alN HRPRPI proposed the multimodal adaptation\ngateLwhichfusesdataofothermodalitiestothetexM\ntual embeddings in bertN howeverL their method\nrequires all modalities to have the same sequence\n3998\nlengthL which is rare for most datasetsN additionM\nallyL they only attempted to address the sentiment\nanalysis task and did not explore text generationN\n3 Vision Guided GPLMs\nto take advantage of the superior text generaM\ntion ability of the textMonly seqRseq gplms and\nadapt them to the mas taskL we present vision\nguided HvgI gplmsN speciﬁcallyL we leverage\nbart Hlewis et alNL RPQYI and tU Hra;el et alNL\nRPQYI to construct vgMbart and vgMtUN\ninthissectionLwestartbyrevisitingthetextMonly\nseqRseq gplms in section SNQN these serve as\nthe backbone of our proposed model and also one\nof the baselinesN thenL we discuss the approach\nfor extracting visual features from video clips in\nsection SNRL as well as how to further process themN\nfinallyL in section SNSL we introduce two types of\ntextMvision fusion mechanism to guide the gplms\nto generate visionMaware summariesN\n3.1 Overview of GPLMs for Summarization\ntransformerMbasedHvaswanietalNLRPQWbIseqRseq\ngplms generalize architectures like bert HdeM\nvlin et alNL RPQYI and gpt Hradford et alNL RPQXI\nby including a biMdirectional encoder and a uniM\ndirectional HleftMtoMrightI decoderN an overview\nof this architecture is depicted on the left side of\nfigure R Hexcept the green dashed blockIN\nat the entry of the gplmL the input text is ﬁrst\ntokenized and converted to a sequence of token\nembeddings -C ∈R#×3C L in which# is the seM\nquence length and3C is the feature dimensionN to\nretain the positional informationL positional encodM\ningsHvaswanietalNLRPQWaI \u001a?4 ∈R#×3C areadded\ntothetokenembeddingspointwiselyHeqNQILwhich\nforms the input features/4=2\nP to the encoderN\n/4=2\nP = -C +\u001a?4 HQI\nas illustrated in figure RL the encoder is composed\nof a stack of!encoder layersL each containing two\nsubMlayersZ QI multiMhead selfMattention HmsaL\neqN RI and RI feedMforward network HffnL eqN SIN\nin additionL after each subMlayerL there is a residual\nconnection Hhe et alNL RPQU[ wang et alNL RPQYI\nfollowed by a layer normalization HlnI Hba et alNL\nRPQVIN see appendix a and b for more details of\nthe msa and ffnN\n/4=2′\n; = ln(msa(/4=2\n;−Q )+ /4=2\n;−Q ) HRI\n/4=2\n; = ln(ffn(/4=2′\n; )+ /4=2′\n; ) HSI\nsimilar to the encoderL the decoder also consists\nof a stack of!decoder layersL but with two di;erM\nencesN firstlyL the msa is masked to prevent posiM\ntions from attending to subsequent positions Hkeep\nthe decoder in a leftMtoMright directionIN secondlyL\nthereisonemoremultiMheadencoderMdecoderattenM\ntion subMlayerL which uses the decoder embeddings\ntoattendovertheoutputembeddingsoftheencoder\nto incorporate the encoded informationN\nspeciﬁcallyLinourexperimentsLweadoptthepreM\ntrained bart Hlewis et alNL RPQYI and tU Hra;el\net alNL RPQYIL which both follow this architecture\nwith di;erent training schemesN to ﬁneMtune them\nontheabstractivetextsummarizationtaskLtheinput\nto the encoder is the article or transcriptL and the\ndecoder learns to generate the summariesN\n3.2 Video Feature Extraction\nfor each video clipL following previous\nworks Hsanabria et alNL RPQX[ palaskar et alNL RPQY[\nkhullar and aroraL RPRPIL a RPTXMdimensional\nfeature representation is extracted for every QV\nnonMoverlapping frames using a Sd resnextMQPQ\nmodel Hhara et alNL RPQXIL which is preMtrained on\nthe kinetics dataset Hkay et alNL RPQWIN thereforeL\neachdatasamplewillhaveasequenceofRPTXM 3viM\nsionfeaturevectorsoflength \"N thesefeaturescan\nbeuseddirectlyasthevisualinputtothetextMvision\nfusion mechanismN\nin additionL in order to better model the intraM\nmodal dynamics and enhance the vision speciﬁc\ntemporal informationL we further process the exM\ntracted sequence of visual features using a transM\nformer Hvaswani et alNL RPQWaI encoder HvtfI with\npositional encodingsN experiments illustrate that\nthis additional encoding process can further boost\nthe performance of our model Hsection UIN\n3.3 Text-vision Fusion\nas exhibited in figure RL we insert a third subMlayer\nHthe green dashed blockI into each encoder layerL\nwhich contains the textMvision fusion mechanism\nand also a residual connection followed by a layer\nnormalizationN weproposetwotypesoftextMvision\nfusion mechanismL as shown on the rightMhand\nside of the ﬁgureN given the textual input/C ∈\nR#×3C and visual input/E ∈R\"×3C L the fusion\nmechanism produces vision guided output/′\nC ∈\nR#×3C that has a same dimension as the textual\ninputL which allows the continual stacking of layersN\n3999\nDot-product Attention Based Fusion. before\nperforming dotMproduct attention between the texM\ntual and visual featuresL we ﬁrst project the visual\nfeatures /E to the same dimensional space as the\ntextual features HeqN TIN thenL we calculate the\ndotMproduct and apply the softmax function to get\nthe attention score matrix\u0016HeqN UIN finallyL the\ninput textual features/C are concatenated with the\nattention weighted visual features\u0016/E and then\nprojectedbyanotherlineartransformationtooutput\nthe vision guided textual features/′\nC HeqN VIN\n/′\nE = /E,Q, /′\nE ∈R\"×3C HTI\n\u0016= softmax(/C/′)\nE ), \u0016∈R#×\" HUI\n/′\nC = concat(/C,\u0016/ E),R HVI\nadditionallyLwebuildavariantofthisfusionLwhich\nuses the linearly transformed visual features\u0016/′\nE\nfortheconcatenationineqNVinsteadoftheoriginal\n\u0016/EN a comparison of their performance is shown\nin section UN\nMulti-head Attention Based Fusion. inspired\nby prior works Hyu et alNL RPQY[ tsai et alNL RPQYIL\nwe propose a vision guided multiMhead attention\nmechanism for the textMvision fusionN the query&\nis linearly projected from the input textual featuresL\nand the key and value+ are linearly projected\nfrom the visual features HeqN W M YIN thenL a crossM\nmodal multiMhead attention HcmaI is applied to\nget the text queried visual features$ HeqN QPIN\nfinallyL we obtain the vision guided output/′\nC by\nconcatenating the input textual features/C and$L\nand linearly project it to the desired dimension\nHeqN QQIN\n&= /C,@, &∈R#×32 HWI\n = /E,:,  ∈R\"×32 HXI\n+ = /E,E, +∈R\"×32 HYI\n$= cma(&, ,+ ), $∈R#×32 HQPI\n/′\nC = concat(/C,$),S HQQI\nin additionL we also explore the e;ects of using a\nforgetgateHliuetalNLRPRPIinthetextMvisionfusionN\ngiven the cma output$ ∈R#×32 in eqN QPL we\nconstruct a forget gate mask\u001b ∈R#×32 HeqN QRI\nanddoapointMwisemultiplicationwith $tooutput\nthe updated$′HeqN QSIN\n\u001b = sigmoid(concat($,/C),5 ) HQRI\n$′= \u001b⊗$ HQSI\nthe forget gate can potentially remove redundant\nand noisy information from the video featuresL\nwhich also helps the model to learn to discard\nneedless visual information to retain its preMtrained\ntext generation abilityN\n4 Experimental Settings\n4.1 How2 Dataset\nhowRHsanabriaetalNLRPQXIisalargeMscaledataset\nofopenMdomainvideosLcoveringRRdi;erenttopics\nsuch as cookingL exerciseL yoga and musicN it\nconsistsofWYLQQTshortinstructionalvideosHWSLYYS\nfor trainingL RLYVU for validation and RLQUV for\ntestingIN each video is accompanied by a humanM\ngenerated transcript and a short text summaryN at\nthe word levelL the average lengths of transcripts\nand summaries are RYQ and SSL respectivelyN\n4.2 Implementation Details\nData pre-processing. we preMprocess the tranM\nscripts data by truncating or padding them into\nsequences of UQR tokens after tokenizationN for the\nvideosL after the feature extraction as described in\nsection SNRL we also truncate or pad the sequence\nlength to RUVN\nHyper-parameters. we use bartMbase and tUM\nbase as the preMtrained gplms to construct vgM\nbartandvgMtULinwhich != V forbothencoder\nanddecoderN forthevtfmentionedinsectionSNRL\nweuseaTMlayerencoderwithXattentionheadsand\na RPTX feedMforward dimensionN in the decoding\nstageL we use beam search with a beam size of UN\nthe decoding process will not stop until an endM\nofMsequence HeosI token is emitted or the length\nof the generated summary reaches to VT tokensN\nfollowing lewis et alN HRPQYI and ra;el et alN\nHRPQYIL we use learning ratesVe−T and Se−U to ﬁneM\ntune the preMtrained parts of model weightsN while\nfor the newly added layersL we set the learning rate\nto Q.Ue−TN for all of our experimentsL we use a\nbatch size of QRPN\nOptimizer. during trainingL we use the adam\noptimizer Hkingma and baL RPQUI withVQ = P.YL\nVR = P.YYY and a weight decay ofQe−UN additionM\nallyLweapplyaschedulertodecaythelearningrate\ntoYUEofthecurrentoneaftereveryQPepochsN we\ntrainallthemodelsforVPepochswithanearlystop\nof U using the rougeMR score Hxiao and careniniL\nRPQYI on the validation setN\n4000\nInput Method R-1 R-2 R-L B-1 B-2 B-3 B-4 M C CF\ntranscript\nsRsJ UXNV TPNV USNX UUNR TUNV SYNY SUNX RWNV RNSU M\npgJ UWNR SYNU URNX UUNS TUNV SYNX SUNW RVNX RNQS M\ntfJ UYNP TQNP UTNS UVNV TVNW TPNX SVNV RWNW RNSP M\ntU VRNX TUNP UWNU VPNU UPNT TTNR SYNV SPNV RNWV VQNW\nbart VTNP TVNT UXNY VRNT URNV TVNT TRNP SQNW RNYW VSNY\ntranscript\nKvideo\nha HrnnIJ VPNS TRNU UUNW UWNR TWNW TQNX SWNU RXNX RNTX M\nha HtfIJ VPNR TSNQ UUNY UXNV TXNS TSNS SXNQ RXNY RNUQ M\nmffg HrnnI†J VRNS TVNQ UXNR UYNQ UPNT TUNQ TQNQ SPNQ RNVY M\nmffg HtfIJ VQNV TUNQ UWNT VPNP UPNY TUNS TQNS RYNY RNVW M\nvgMtU HdotMproductI VSNP TTNY UWNV VPNQ TYNX TSNT SXNX SPNS RNWT VQNT\nvgMtU HmultiMheadI VSNS TUNS UXNP VPNW UPNX TTNW TPNR SQNP RNXV VRNX\nvgMbart HdotMproductIVVNQ TYNS VQNR 64.5 55.1 49.2 44.8 33.2 3.18 VVNY\nvgMbart HmultiMheadI 66.3 49.4 61.4 VTNQ UTNX TXNY TTNV SSNQ 3.18 67.3\nTable 1: evaluation results of baselines and our proposed models on the howR datasetN we compare the\nperformance of using transcript only and transcriptKvideoN the†indicates the previous stateMofMtheMart modelN\nresults with J mark are taken from the previous work Hliu et alNL RPRPIN we denote rougeL bleuL meteorL\ncider and content fQ by rL bL mL c and cf respectivelyN\nInput Method R-1 R-2 R-L B-1 B-2 B-3 B-4 M C CF\ntranscript\nKvideo\nvgMbart HmultiMheadIVVNS TYNT VQNT VTNQ UTNX TXNY TTNV SSNQ SNQX VWNS\nwO fg VWNS UPNW VRNT VUNP UUNY UPNQ TUNW SSNX SNRU 72.5\nwO vtf VWNS UPNY VRNV VTNY UVNP UPNQ TUNW SSNW SNRP WRNQ\nwO fgKvtf 68.0 51.4 63.3 65.2 56.3 50.4 46.0 34.0 3.28 VYNW\nTable 2:further evaluation of adding forget gate HfgI and visual transformer encoder HvtfI to our best model\nsettingintableQonthehowRdatasetN vgMbartKfgKvtflargelysurpassesthepreviousstateMofMtheMartmodelN\nSoftware and hardware. we use the deep learnM\ning framework pytorch Hpaszke et alNL RPQYI to imM\nplement our code and pytorchMlightning/two.supfor the\ndistributed trainingN we use four nvidia geforce\nrtx RPXP ti gpus for all of our experimentN\n4.3 Baselines\napart from the textMonly gplms bart Hlewis\netalNLRPQYIandtUHra;eletalNLRPQYIL weusethe\nfollowing baselines to compare with our proposed\nmodelsL including simple models that only accept\ntext inputL as well as prior stateMofMtheMart models\nthat accept text and vision modalitiesN\nS2S (Luong et al., 2015). sRs is a standard\nseqRseqmodelthatusesrnnsforbothencoderand\ndecoder with a global attention mechanism HbahM\ndanau et alNL RPQTIN\nPG(Seeetal.,2017). thepointergeneratorHpgI\nnetwork augments sRs by having a copy module\n/two.suphttps://github.com/PyTorchLightning/\npytorch-lightning\nto reproduce key information accurately as well as\nmitigating the outMofMvocabulary issueN\nTF (Vaswani et al., 2017b).tf is the standard\ntransformerMbasedseqRseqmodelLwhichproposes\nthe novel multiMhead attention mechanismN\nHA (RNN/Transformer) (Palaskar et al., 2019).\na multiMsource seqRseq model with hierarchical\nattention HhaI Hlibovick`y and helclL RPQWI that\ncanintegratesinformationfromdi;erentmodalities\ninto a coherent outputN\nMFFG (RNN/Transformer) (Liu et al., 2020).\nthe multistage fusion with forget gate HmffgI\nmodel proposes a cross fusion block with forget\ngate and a hierarchical fusion decoder to improve\nmultimodal generationN\n4.4 Evaluation Metrics\nfollowingHliuetalNLRPRPILweuserougeLbleuL\nmeteorL and cider to evaluate the summariesN\nrougeM{QL RL l} Hthe standard metrics for abM\nstractivesummarizationIHlinandhovyLRPPSIand\n4001\nbleuM{QL RL SL T} Hpapineni et alNL RPPRI are used\nto calculate the recall and precision of nMgram overM\nlapsL respectivelyL between the references and the\ngenerated summariesN mentor Hdenkowski and\nlavieL RPQQI is used to match the word stemsL synM\nonyms and paraphrases between the reference and\nthe generated summaryN cider Hvedantam et alNL\nRPQUI is an image captioning metric to compute\nthe cosine similarity between tfMidf weighted\nnMgramsN\nin additionL we use content fQ Hpalaskar et alNL\nRPQYI to measure the fQ score of the content words\nof the generated summary based on a monolingual\nalignmentN firstlyL meteor toolkit Hbanerjee and\nlavieLRPPU[denkowskiandlavieLRPQTIisusedto\nobtainthealignmentbetweenthesummariesandrefM\nerencesN thenLthefunctionwordsandtaskMspeciﬁc\nstop words are removed from the summaries and\nreferencesN finallyL the remaining content words\nfrom the summaries and references are treated as\ntwobagsofwordsL andthefQscoresarecalculated\noverthealignmentN contentfQfocusesmoreonthe\ncontentanditcanavoidtheincreaseoftherouge\nscore from the stop wordsN\nwe usenlg-eval /three.supto compute the bleuL\nmentor and cider scoresL and userouge /four.supto\ncompute rouge scoresN the implementation of\ncontent fQ scores follows Hpalaskar et alNL RPQYIN\n5 Results and Analysis\n5.1 Main Results\nfrom table QL we can see that when there is only\ntranscript in the input dataL sRs and pg reach simiM\nlar scores in terms of all evaluation metricsN this\ncould be attributed to the fact that pg tends to copy\nthe content in the transcripts while the reference\nsummariesinthehowRdatasethaveagreatnumber\nof novel nMgramsL which are deﬁned to be novel\nwith respect to the transcriptN we also observe that\ntf performs better than rnnMbased modelsN it is\nbecause tf can learn better relationships between\nwords by multiMhead attention mechanism and poM\nsitional embeddingsN furthermoreL both textMonly\ntU and bart outperform all the baseline models\nby a large gap owe to their preMtrained text genM\neration abilityN compared to tUL bart achieves\nhigher scores mainly because it introduces a novel\npreMtraining objective named sentence permutationN\n/three.suphttps://github.com/Maluuba/nlg-eval\n/four.suphttps://github.com/\nneural-dialogue-metrics/rouge\nsentence permutation requires the model to genM\nerate the original uncorrupted text from randomly\nshu?ed sentencesL which enhances the understandM\ningoflongtextandbeneﬁtsthesummarizationtaskN\nmoreoverL bart is even better than all previous\nmultimodal models trained on transcript and videoN\nthe visual guidance consistently boosts the perM\nformance of tU and bart by a large stepN as\nshownintableRLourbestmodel VG-BART+FG+VTF\nwiththecrossMmodalmultiMheadattentionsurpasses\nthe previous stateMofMtheMart model HmffgI by\nUNW rougeMQL UNS rougeMRL and UNQ rougeMl\nscoresN the visual guidance contributes XSNVE of\nthe overall improvement on average of all rouge\nscoresN\nthe results of content fQ scores in table Q show\nsimilar trends with other evaluation metricsN by inM\njecting visual informationL the models can generate\nsummarieswithmuchrichercontentN tableRshows\nthat both forget gate HfgI and visual transformer\nencoder HvtfI beneﬁt the modelGs performanceN\nhoweverL the content fQ score is not boosted when\ncombining fg and vtf togetherL which is contraM\ndictory to all other metricsN we conjecture that it is\nbecausethecontentfQfocusesmoreonthecontent\naspectLitmayhavesomevariancecomparetoother\nmetricsN\n5.2 How to Inject Visual Information\nas illustrated in section SNSL we mainly adopt two\ntextMvisionfusionmechanismstoinjectvisualinforM\nmationL the crossMmodal dotMproduct attention and\nmultiMhead attentionN as shown in table QL for the\nvgMbart modelL these two fusion mechanisms\nconsistently improve its performance on all metrics\nby a comparable marginN howeverL for the vgMtU\nmodelLthecrossMmodaldotMproductattentionbased\nfusiondoesnotshowanyimprovementcomparedto\nthetextMonlytULwhilethemultiMheadattentionbase\nfusionstillincreaseitsperformanceN wethinkthere\nare two reasons behind this phenomenonN firstlyL\nas discussed in section UNQL bart leverages the\nsentence permutation method as its preMtraining obM\njectiveL which increases its robustness on attentionM\nbased fusionN secondlyL multiMhead attention can\ncapture di;erent key components in the visual inM\nformation from multiple aspectsL which makes it\nmore potent than the dotMproduct based fusionN adM\nditionallyL as mentioned in section SNSL we build a\nvariant of the dotMproduct attention based fusionL\nwhich achieves VVNQ rougeMQL TYNS rougeMR\n4002\nInput Method R-1 R-2 R-L\ntranscripttU VRNX TUNP UWNU\nbart 64.0 TVNT 58.9\ntranscript\nKnoise\nvgMtU HdotMproductIVRNU TSNY UWNP\nvgMtU HmultiMheadI VRNX TTNV UWNT\nvgMbart HdotMproductIVSNY TUNV UXNV\nvgMbart HmultiMheadIVSNY TVNU UXNW\ntranscript\nKvideo\nvgMtU HdotMproductIVSNP TTNY UWNV\nvgMtU HmultiMheadI VSNS TUNS UXNP\nvgMbart HdotMproductIVVNQ TYNS VQNR\nvgMbart HmultiMheadI66.3 49.4 61.4\nTable 3:results of using uniform noise to replace the\nvisual featuresN\nand VQNT rougeMl on vgMbartN this comparaM\nble result shows that the variant does not provide\nfurther improvementN\nto ensure the visual features really help in the\nlearning and our addMon layers aid the understandM\ning of themL we conduct further experiments by\nreplacing the visual features in the input data with\nrandom noise of the same dimension and sequence\nlengthN the noise is sampled from a uniform disM\ntribution from P to SL in a similar value range of\nthe original visual featuresN as depicted in table SL\nvg gplms with random noise as visual features\nachieve similar or slightly worse performance comM\npared to the textMonly gplmsN this shows the\ne;ectiveness of our method to keep gplmsG text\ngeneration abilityN furthermoreL compared to the\ndotMproduct attention based fusionL the multiMhead\nfusion is better at retaining gplmsG performanceL\nwhich again demonstrates its superiorityN\nas mentioned in section SL we use a forget gate\nHfgItodealwiththeredundancyandnoisyinformaM\ntion in the visual featuresN additionallyL we further\nencode the visual features by a visual transformer\nencoder HvtfIN table R shows that using either fg\norvtfcanincreasetheperformanceofvgMbartN\njointly leveraging them boosts the performance by\nQNWL RNPL and QNY of rougeMQL rougeMRL and\nrougeMlL respectivelyN\n5.3 Where to Inject Visual Information\nas discussed in section QL one of the main chalM\nlengesofbuildingvggplmsistoﬁndtheoptimal\nlocation to inject the visual information HiNeNL the\ntextMvision fusionIN a subMoptimal location might\nlead to a less e;ective modality fusion and even\nhurt the gplmsG original text generation abilityN\nas gplms have a stack of layers in the encoder\nEncoder Layer(BART-base)\nR-1 R-2 R-L\nQ R S T U V\n\u0017 \u0017 \u0017 \u0017 \u0017 \u0017 VTNP TVNT UXNY\n\u0013 \u0017 \u0017 \u0017 \u0017 \u0017 VVNW TYNY VQNX\n\u0017 \u0013 \u0017 \u0017 \u0017 \u0017 VWNP UPNU VRNR\n\u0017 \u0017 \u0013 \u0017 \u0017 \u0017 VWNS UPNX VRNT\n\u0017 \u0017 \u0017 \u0013 \u0017 \u0017 VWNT UPNY VRNV\n\u0017 \u0017 \u0017 \u0017 \u0013 \u0017 VWNT UPNX VRNU\n\u0017 \u0017 \u0017 \u0017 \u0017 \u0013 VWNW UQNS VSNP\n\u0013 \u0013 \u0013 \u0013 \u0013 \u0013 VPNT TSNT UUNX\n\u0017 \u0013 \u0013 \u0013 \u0013 \u0013 VTNQ TWNP UYNS\n\u0017 \u0017 \u0013 \u0013 \u0013 \u0013 VUNS TYNR VPNP\n\u0017 \u0017 \u0017 \u0013 \u0013 \u0013 VWNU UPNY VRNW\n\u0017 \u0017 \u0017 \u0017 \u0013 \u0013 68.0 51.4 63.3\nTable 4: performance of di;erent textMvision fuM\nsion locations in the encoder of our best model\nHVG-BART+FG+VTF with crossMmodal multiMhead attenM\ntionIN \u0013 indicates the occurrence of fusion at a certain\nlayer and\u0017 indicates nonMoccurrenceN the ﬁrst row is\nthe result of bart using transcript onlyN\nand also the decoderL we explore this problem from\ntwo aspectsZ QI which single layer has the best\nfusion e;ect[ and RI does multiple times of fusion\nhelp gplms to understand the visual information\nbetter?\nas depicted in table T and UL ﬁrstlyL we enumerM\nate each single layer in the encoder and decoder\nof our best model HVG-BART+FG+VTFI to perform\nthe textMvision fusionN in terms of rouge scoresL\nwe can clearly tell that injecting visual information\ninto the encoder can generally boost the modelGs\nperformancebyalargestepLwhileinjectingintothe\ndecoder only shows negligible improvementN furM\nthermoreL in the encoderL we observe that injecting\natahigherlayerHclosertotheencoderoutputIbrings\nmoreimprovementN insteadLinthedecoderLthereis\nno clear pattern showing the inﬂuence of injecting\nlocationN we speculate that an early textMvision\nfusion in the encoder makes the visual information\nslightly fades away after passing through the stack\nof encoder layersN additionallyL during the decodM\ning stageL the model utilizes visual information\nbetter through the encoderMdecoder attention layers\nthandirectlyinjectingintothedecoderLwhichcould\npotentially hurts the generation abilityN secondlyL\nas shown in the lower part of table TL we conduct\nmultiple times of fusion in the encoderGs di;erent\n4003\nDecoder Layer(BART-base)\nR-1 R-2 R-L\nQ R S T U V\n\u0017 \u0017 \u0017 \u0017 \u0017 \u0017 VTNP TVNT UXNY\n\u0013 \u0017 \u0017 \u0017 \u0017 \u0017 VTNV TWNQ UYNV\n\u0017 \u0013 \u0017 \u0017 \u0017 \u0017 65.2 48.0 60.3\n\u0017 \u0017 \u0013 \u0017 \u0017 \u0017 VTNY TVNY UYNV\n\u0017 \u0017 \u0017 \u0013 \u0017 \u0017 VTNX TVNY UYNW\n\u0017 \u0017 \u0017 \u0017 \u0013 \u0017 VTNS TVNV UYNQ\n\u0017 \u0017 \u0017 \u0017 \u0017 \u0013 VTNT TVNW UYNP\nTable 5: performance of di;erent fusion locations in\nthe decoder of our best model HVG-BART+FG+VTF with\ncrossMmodal multiMhead attentionIN\nlocationsN we observe that when fusing at all enM\ncoderlayerssimultaneouslyLthemodelconvergesto\namuchworseperformanceN weconjecturethatthis\ncauses the catastrophic forgetting of the preMtrained\nknowledgeingplmsN weﬁndthatfusingatthelast\nseveral layers HeNgNL U and VI in the encoder is able\nto further improve the summarization performanceN\n5.4 Eﬀects of the Forget Gate\nasmentionedinsectionSNSL weapplyaforgetgate\nHeqNQRI to ﬁlter out noise and let the model focus\non more important visual informationN to have a\ndeeper understanding of the e;ects of the forget\ngateL we calculate the average forget gate score\nHaveragedoverthewholesequenceIforeachsample\nfrom the howR test setN as shown in figure SL\nmost scores are distributed between PNTW and PNTXN\nthere is one data sample the score reaches PNU\nbecauseitstranscriptisnotavailableN asillustrated\nin table VL the model can still generate reasonable\nsummary for it by paying more attention to the\nvisual informationN the meaning of the generated\nsummary is still highly aligned with the reference\nsummaryLwhichshowsthecapabilityandﬂexibility\nof our model to utilize visual informationN\n6 Conclusion and Future Work\nin this paperL we introduce a simple yet e;ective\nmethod to construct vision guided largeMscale genM\nerative preMtrained language models HvgMbart\nand vgMtUI for the multimodal abstractive summaM\nrization task by inserting attentionMbased addMon\nlayersN we propose two types of attention mechaM\nnisms for the textMvision fusion and interactionZ QI\ncrossMmodal dotMproduct attention[ and RI crossM\nmodal multiMhead attentionN moreoverL we also\nTranscriptZ transcript not available\nSummary fromTranscript + VideoZ learn tips\non how to write 0cane1 in chinese radicals with\nmandarin characters in the free video clipN get free\nforeign language lessons from an expertN\nReference SummaryZ learn what ticks are in chiM\nnesecalligraphyinthisfreevideocliponlanguages\nand writingN\nTable 6: an example from howR testing dataset that\nhas high forget gate scoreN\n0.470 0.475 0.480 0.485 0.490 0.495 0.500\nAverage Forget Gate Score\n0\n200\n400\n600\n800\n1000\n1200\n1400Number of Samples\nFigure 3:the distribution of average forget gate score\non the howR test setN the model is the vgMbart with\ndotMproduct attentionN\ninvestigate the e;ects of using the forget gate and\nvisual transformer encoder along with the attention\nmechanismsN in additionL we enumerate almost\nall possible locations in gplms for injecting addM\non layersN experimental results show that our\napproaches signiﬁcantly outperform the prior stateM\nofMtheMart on the howR datasetN further analysis\nillustrates that multiMhead attention is more robust\nthan the dotMproduct attention and higher layers of\nthe encoder is the optimal place to inject vision\ninformationN for future workL we believe that our\nanalyses on thehow and where to inject visual\ninformation into gplms can be applied to other\nmultimodal tasksN\n7 Acknowledgments\nwe want to thank the anonymous reviewers for\ntheir constructive feedbackN this work is partially\nfunded by itsOSUSOQYfp and and mrpOPUUOQX of\nthe innovation technology commissionL the hong\nkong sar governmentN\n4004\nReferences\nstanislaw antolL aishwarya agrawalL jiasen luL marM\ngaret mitchellL dhruv batraL c lawrence zitnickL\nand devi parikhN RPQUN vqaZ visual question anM\nsweringN in proceedings of the ieee international\nconference on computer visionL pages RTRU5RTSSN\njimmy baL jN kirosL and geo;rey eN hintonN RPQVN\nlayer normalizationNarxivL absOQVPWNPVTUPN\ndzmitry bahdanauL kyunghyun choL and yoshua benM\ngioN RPQTN neural machine translation by jointly\nlearning to align and translateN arxiv preprint\narxiv:1TP9NPTWSN\nsatanjeevbanerjee andalonlavieN RPPUN meteorZ an\nautomatic metric for mt evaluation with improved\ncorrelation with human judgmentsN inproceedings\nof the acl workshop on intrinsic and extrinsic evaluM\nation measures for machine translation andOor sumM\nmarizationL pages VU5WRN\nxN chenL hN fangL ty linL rN vedantamL sN guptaL\npN dollárL and cN lN zitnickN RPQUN microsoft\ncococaptionsZ datacollectionandevaluationserverN\narxiv:15PTNPPSR5N\nyenMchun chenL linjie liL licheng yuL ahmed el\nkholyL faisal ahmedL zhe ganL yu chengL and\njingjing liuN RPRPN uniterZ universal imageMtext\nrepresentation learningN ineccvN\njaeminchoLjieleiLhaochentanLandmNbansalNRPRQN\nunifying visionMandMlanguage tasks via text generaM\ntionN inicmlN\nwenliang daiL samuel cahyawĳayaL zihan liuL and\npascale fungN RPRQN multimodal endMtoMend sparse\nmodel for emotion recognitionN inproceedings of\nthe RPR1 conference of the north american chapM\nteroftheassociationforcomputationallinguistics:\nhuman language technologiesL pages USPU5USQVL\nonlineN association for computational linguisticsN\nwenliang daiL zihan liuL tiezheng yuL and pascale\nfungN RPRPN modalityMtransferable emotion embedM\ndings for lowMresource multimodal emotion recogniM\ntionN in proceedings of the 1st conference of the\nasiaMpaciﬁc chapter of the association for compuM\ntational linguistics and the 1Pth international Joint\nconference on natural language processingL pages\nRVY5RXPL suzhouL chinaN association for computaM\ntional linguisticsN\nmichaeldenkowskiandalonlavieNRPQQN meteorQNSZ\nautomaticmetricforreliableoptimizationandevaluM\nationofmachinetranslationsystemsN in proceedings\nof the sixth workshop on statistical machine translaM\ntionL pages XU5YQN\nmichael denkowski and alon lavieN RPQTN meteor\nuniversalZ language speciﬁc translation evaluation\nfor any target languageN inproceedings of the ninth\nworkshop on statistical machine translationL pages\nSWV5SXPN\njacob devlinL mingMwei changL kenton leeL and\nkristina toutanovaN RPQYN bertZ preMtraining of\ndeep bidirectional transformers for language underM\nstandingN inproceedings of the RP19 conference of\nthe north american chapter of the association for\ncomputational linguistics: human language techM\nnologiesL volume 1 Hlong and short papersIL pages\nTQWQ5TQXVN\nje;rey donahueL lisa anne hendricksL sergio guadarM\nramaL marcus rohrbachL subhashini venugopalanL\nkate saenkoL and trevor darrellN RPQUN longMterm\nrecurrent convolutional networks for visual recogniM\ntion and descriptionN in proceedings of the ieee\nconference on computer vision and pattern recogniM\ntionL pages RVRU5RVSTN\nyashgoyalLtejaskhotLdouglassummersMstayLdhruv\nbatraLanddeviparikhNRPQWN makingthevinvqa\nmatterZ elevating the role of image understanding\nin visual question answeringN in conference on\ncomputer vision and pattern recognition HcvprIN\nkensho haraL hirokatsu kataokaL and yutaka satohN\nRPQXN canspatiotemporalSdcnnsretracethehistory\nof Rd cnns and imagenet? in proceedings of the\nieee conference on computer vision and pattern\nrecognition HcvprIN\nkaiming heL xiangyu zhangL shaoqing renL and jian\nsunN RPQUN deep residual learning for image recogM\nnitionN arxiv preprint arxiv:151RNPSSX5N\ndan hendrycks and kevin gimpelN RPQVN gaussian\nerror linear units HgelusINarxiv: learningN\nzhicheng huangL zhaoyang zengL bei liuL dongmei\nfuL and jianlong fuN RPRPN pixelMbertZ aligning\nimage pixels with text by deep multiMmodal transM\nformersN arxivL absORPPTNPPXTYN\nwill kayL joão carreiraL kN simonyanL brian zhangL\nchloe hillierL sudheendra vĳayanarasimhanL fabio\nviolaL tim greenL tN backL aN natsevL mustafa suM\nleymanL and andrew zissermanN RPQWN the kinetics\nhuman action video datasetNarxivL absOQWPUNPVYUPN\naman khullar and udit aroraN RPRPN mastZ multiM\nmodal abstractive summarization with trimodal hierM\narchical attentionN inproceedings of the first interM\nnationalworkshoponnaturallanguageprocessing\nbeyond textL pages VP5VYL onlineN association for\ncomputational linguisticsN\nwonjaekimLbokyungsonLandildookimNRPRQN viltZ\nvisionMandMlanguage transformer without convoluM\ntion or region supervisionN in proceedings of the\nSXthinternationalconferenceonmachinelearning L\nvolumeQSYof proceedingsofmachinelearningreM\nsearchL pages UUXS5UUYTN pmlrN\ndiederik pN kingma and jimmy baN RPQUN adamZ\na method for stochastic optimizationN corrL\nabsOQTQRNVYXPN\n4005\nmike lewisL yinhan liuL naman goyalL marM\njan ghazvininejadL abdelrahman mohamedL omer\nlevyL ves stoyanovL and luke zettlemoyerN RPQYN\nbartZ denoising sequenceMtoMsequence preMtraining\nfornaturallanguagegenerationLtranslationLandcomM\nprehensionN arxiv preprint arxiv:191PN1STV1N\ngen liL nan duanL yuejian fangL daxin jiangL and\nmN zhouN RPRPN unicoderMvlZ a universal encoder\nfor vision and language by crossMmodal preMtrainingN\nin aaaiN\nhaoran liL junnan zhuL cong maL jiajun zhangL and\nchengqingzongNRPQWN multiMmodalsummarization\nforasynchronouscollectionoftextLimageLaudioand\nvideoN in proceedings of the RP1W conference on\nempiricalmethodsinnaturallanguageprocessing L\npages QPYR5QQPRN\njindřich libovick`y and jindřich helclN RPQWN attention\nstrategies for multiMsource sequenceMtoMsequence\nlearningN arxiv preprint arxiv:1WPTNPV5VWN\nchinMyewlinandeduardhovyNRPPSN automaticevalM\nuation of summaries using nMgram coMoccurrence\nstatisticsN in proceedings of the RPPS human lanM\nguage technology conference of the north ameriM\ncan chapter of the association for computational\nlinguisticsL pages QUP5QUWN\nnayuliuLxiansunLhongfengyuLwenkaizhangLand\nguangluan xuN RPRPN multistage fusion with forget\ngate for multimodal summarization in openMdomain\nvideosN in proceedings of the RPRP conference on\nempirical methods in natural language processing\nHemnlpIL pages QXST5QXTUN\nyinhan liuL myle ottL naman goyalL jingfei duL\nmandar joshiL danqi chenL omer levyL mike\nlewisL luke zettlemoyerL and veselin stoyanovN\nRPQYN robertaZ a robustly optimized bert pretrainM\ning approachNarxiv preprint arxiv:19PWN11V9RN\nhuaishaoluoLleijiLbotianshiLhNhuangLnanduanL\ntianruiliLxilinchenLandmNzhouNRPRPN univilmZ\na uniﬁed video and language preMtraining model for\nmultimodal understanding and generationN arxivL\nabsORPPRNPVSUSN\nminhMthang luongL hieu phamL and christopher d\nmanningN RPQUN e;ective approaches to attentionM\nbased neural machine translationN arxiv preprint\narxiv:15PXNPTPR5N\nyoussef mrouehL etienne marcheretL and vaibhava\ngoelN RPQUN deep multimodal learning for audioM\nvisual speech recognitionN in RP15 ieee internaM\ntional conference on acousticsL speech and signal\nprocessing HicasspIL pages RQSP5RQSTN ieeeN\nshruti palaskarL jindrich libovick`yL spandana gellaL\nand florian metzeN RPQYN multimodal abstractive\nsummarization for howR videosN arxiv preprint\narxiv:19PVNPW9P1N\nkishore papineniL salim roukosL todd wardL and weiM\njing zhuN RPPRN bleuZ a method for automatic evalM\nuation of machine translationN inproceedings of the\nTPthannualmeetingoftheassociationforcomputaM\ntional linguisticsL pages SQQ5SQXN\nadam paszkeL sN grossL francisco massaL aN lererL\njames bradburyL gregory chananL trevor killeenL\nzN linL nN gimelsheinL lN antigaL alban desmaisonL\nandreas köpfL edward yangL zach devitoL marM\ntin raisonL alykhan tejaniL sasank chilamkurthyL\nbenoit steinerL lu fangL junjie baiL and soumith\nchintalaN RPQYN pytorchZ an imperative styleL highM\nperformance deep learning libraryN inneuripsN\nromain paulusL caiming xiongL and richard socherN\nRPQWN a deep reinforced model for abstractive sumM\nmarizationN arxiv preprint arxiv:1WP5NPTSPTN\nweizhen qiL yu yanL yeyun gongL dayiheng liuL\nnan duanL jiusheng chenL ruofei zhangL and ming\nzhouN RPRPN prophetnetZ predicting future nMgram\nfor sequenceMtoMsequencepreMtrainingN in findings\nof the association for computational linguistics:\nemnlp RPRPL pages RTPQ5RTQPL onlineN associaM\ntion for computational linguisticsN\nalecradfordLkarthiknarasimhanLtimsalimansLand\nilya sutskeverN RPQXN improving language underM\nstanding by generative preMtrainingN\nalec radfordL je;rey wuL rewon childL david luanL\ndario amodeiL and ilya sutskeverN RPQYN language\nmodelsareunsupervisedmultitasklearnersN openai\nblogL QHXIZYN\ncolin ra;elL noam shazeerL adam robertsL katherine\nleeL sharan narangL michael matenaL yanqi zhouL\nwei liL and peter j liuN RPQYN exploring the limits\nof transfer learning with a uniﬁed textMtoMtext transM\nformerN arxiv preprint arxiv:191PN1PVXSN\nwasifur rahmanL md kamrul hasanL sangwu leeL\namirali bagher zadehL chengfeng maoL louisM\nphilippe morencyL and ehsan hoqueN RPRPN inteM\ngrating multimodal information in large pretrained\ntransformersN in proceedings of the 5Xth annual\nmeeting of the association for computational linM\nguisticsL pages RSUY5RSVYL onlineN association for\ncomputational linguisticsN\nramon sanabriaL ozan caglayanL shruti palaskarL\ndesmond elliottL loïc barraultL lucia speciaL and\nflorianmetzeNRPQXN howRZ alargeMscaledatasetfor\nmultimodal language understandingNarxiv preprint\narxiv:1X11NPPSTWN\nabigail seeL peter j liuL and christopher d manM\nningN RPQWN get to the pointZ summarization\nwith pointerMgenerator networksN arxiv preprint\narxiv:1WPTNPTSVXN\nnitish srivastavaL geo;rey eN hintonL aN krizhevskyL\nilyasutskeverLandrNsalakhutdinovNRPQTN dropoutZ\na simple way to prevent neural networks from overM\nﬁttingN JN machN learnN resNL QUZQYRY5QYUXN\n4006\nweĳie suL xizhou zhuL yue caoL bin liL lewei luL\nfuru weiL and jifeng daiN RPRPN vlMbertZ preM\ntraining of generic visualMlinguistic representationsN\nin international conference on learning represenM\ntationsN\nalane suhrL stephanie zhouL ally zhangL iris zhangL\nhuajun baiL and yoav artziN RPQYN a corpus for\nreasoning about natural language grounded in phoM\ntographsN in proceedings of the 5Wth annual meetM\ningoftheassociationforcomputationallinguistics L\npages VTQX5VTRXL florenceL italyN association for\ncomputational linguisticsN\ncN sunL austin myersL carl vondrickL kN murphyL and\ncN schmidN RPQYN videobertZ a joint model for\nvideo and language representation learningN RP19\nieeeOcvf international conference on computer\nvision HiccvIL pages WTVS5WTWRN\nilyasutskeverLoriolvinyalsLandquocvleNRPQTN seM\nquencetosequencelearningwithneuralnetworksN in\nadvances in neural information processing systemsL\npages SQPT5SQQRN\nhaotanandmohitbansalNRPQYN lxmertZlearning\ncrossMmodality encoder representations from transM\nformersN in proceedings of the RP19 conference\non empirical methods in natural language proM\ncessing and the 9th international Joint conference\nonnaturallanguageprocessingHemnlpMĲcnlpI L\npages UQPP5UQQQL hong kongL chinaN association\nfor computational linguisticsN\nyaoMhung hubert tsaiL shaojie baiL paul pu liangL\njN zico kolterL louisMphilippe morencyL and ruslan\nsalakhutdinovNRPQYN multimodaltransformerforunM\nalignedmultimodallanguagesequencesN in proceedM\nings of the 5Wth annual meeting of the association\nfor computational linguistics Hvolume 1: long paM\npersILflorenceLitalyNassociationforcomputational\nlinguisticsN\nashish vaswaniL noam shazeerL niki parmarL jakob\nuszkoreitL llion jonesL aidan n gomezL Ł ukasz\nkaiserL and illia polosukhinN RPQWaN attention is\nall you needN in advances in neural information\nprocessing systemsL volume SPN curran associatesL\nincN\nashish vaswaniL noam shazeerL niki parmarL jakob\nuszkoreitL llion jonesL aidan n gomezL lukasz\nkaiserL and illia polosukhinN RPQWbN attention is\nall you needNarxiv preprint arxiv:1WPVNPSWVRN\nramakrishnavedantamLclawrencezitnickLanddevi\nparikhN RPQUN ciderZ consensusMbased image deM\nscription evaluationN in proceedings of the ieee\nconference on computer vision and pattern recogniM\ntionL pages TUVV5TUWUN\nqiang wangL bei liL tong xiaoL jingbo zhuL\nchangliang liL derek fN wongL and lidia sN chaoN\nRPQYN learning deep transformer models for maM\nchinetranslationN in proceedingsofthe5Wthannual\nmeeting of the association for computational linM\nguisticsL pages QXQP5QXRRL florenceL italyN associaM\ntion for computational linguisticsN\nwen xiao and giuseppe careniniN RPQYN extractive\nsummarization of long documents by combining\nglobal and local contextN in proceedings of the\nRP19 conference on empirical methods in natuM\nral language processing and the 9th international\nJoint conference on natural language processing\nHemnlpMĲcnlpIL pages SPPR5SPQRN\ntiezheng yuL zihan liuL and pascale fungN RPRQN\nadaptsumZ towardslowMresourcedomainadaptation\nforabstractivesummarizationN in proceedingsofthe\nRPR1 conference of the north american chapter of\nthe association for computational linguistics: huM\nman language technologiesL pages UXYR5UYPTN\nzhouyuLjunyuLyuhaocuiLdachengtaoLandqitianN\nRPQYN deep modular coMattention networks for viM\nsual question answeringN in proceedings of the\nieeeOcvf conference on computer vision and patM\ntern recognition HcvprIN\namirzadehLminghaichenLsoujanyaporiaLerikcamM\nbriaL and louisMphilippe morencyN RPQWN tensor\nfusion network for multimodal sentiment analysisN\narxiv preprint arxiv:1WPWNPWR5PN\nrowan zellersL yonatan biskL ali farhadiL and yejin\nchoiN RPQYN from recognition to cognitionZ viM\nsual commonsense reasoningN inthe ieee conferM\nence on computer vision and pattern recognition\nHcvprIN\njingqing zhangL yao zhaoL mohammad salehL and peM\nter liuN RPRPaN pegasusZ preMtraining with exM\ntracted gapMsentences for abstractive summarizationN\ninproceedingsoftheSWthinternationalconference\non machine learningL volume QQY ofproceedings\nofmachinelearningresearch LpagesQQSRX5QQSSYN\npmlrN\njingqing zhangL yao zhaoL mohammad salehL and peM\nter liuN RPRPbN pegasusZ preMtraining with extracted\ngapMsentences for abstractive summarizationN ininM\nternationalconferenceonmachinelearning Lpages\nQQSRX5QQSSYN pmlrN\npengchuan zhangL xiujun liL xiaowei huL jianwei\nyangL lei zhangL lĳuan wangL yejin choiL and\njianfeng gaoN RPRQN vinvlZ making visual repreM\nsentations matter in visionMlanguage modelsNcvpr\nRPR1N\nluowei zhouL hN palangiL lei zhangL houdong huL jaM\nsonjNcorsoLandjianfenggaoNRPRPN uniﬁedvisionM\nlanguage preMtraining for image captioning and vqaN\nin aaaiN\nA Multi-head Self-Attention\nthe query H&IL key H IL value H+I based selfM\nattention is the core building block of the transM\nformer model Hvaswani et alNL RPQWbIN given the\n4007\ninput / ∈RB×3I L we calculate&L  L and+ by\n&= /,@,& ∈RB×3:\n = /,:, ∈RB×3:\n+ = /,E,+ ∈RB×3E ,\nin which,@ ∈R3I×3: L ,: ∈R3I×3: L and,E ∈\nR3I×3E are the projection weightsN thenL a singleM\nhead selfMattention is calculated by\nattention(&, ,+ )= softmax(& )\n√3:\n)+,\nwhere Q√3:\nis the scaling factor to mitigate the exM\ntremelysmallgradientsissuementionedbyvaswani\net alN HRPQWbIN for multiMhead selfMattentionL it can\nbe calculated by\nmultihead(&, ,+ )= concat(headQ,..., headℎ),>\nand\nhead8 = attention(&,8\n@, ,8\n:,+,8\nE).\nB Feed-Forward Network\ngiven the input/ ∈RB×3I L the feedMforward netM\nworkHffnIprocessesitwithtwolinearprojections\n,Q\n5 ∈ R3I×35 L ,R\n5 ∈ R35 ×3I and a nonMlinear\nfunction gelus Hhendrycks and gimpelL RPQVIL\nffn(/)= gelu(/,Q\n5 ),R\n5 .\nin additionL after each linear projectionL there is a\ndropout Hsrivastava et alNL RPQTI layer to improve\ngeneralizationN",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.7867753505706787
    },
    {
      "name": "Computer science",
      "score": 0.7588882446289062
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.7346456050872803
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6506337523460388
    },
    {
      "name": "Generative grammar",
      "score": 0.6136784553527832
    },
    {
      "name": "Natural language processing",
      "score": 0.5409324169158936
    },
    {
      "name": "Modality (human–computer interaction)",
      "score": 0.4587439000606537
    },
    {
      "name": "Generative model",
      "score": 0.4468725621700287
    },
    {
      "name": "Multimodal learning",
      "score": 0.4263978600502014
    },
    {
      "name": "Machine learning",
      "score": 0.4084150195121765
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I200769079",
      "name": "Hong Kong University of Science and Technology",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I889458895",
      "name": "University of Hong Kong",
      "country": "HK"
    }
  ]
}