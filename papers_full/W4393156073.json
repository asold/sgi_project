{
    "title": "FusionFormer: A Concise Unified Feature Fusion Transformer for 3D Pose Estimation",
    "url": "https://openalex.org/W4393156073",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A3137552226",
            "name": "Yanlu Cai",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A2102805196",
            "name": "Weizhong Zhang",
            "affiliations": [
                "Fudan University",
                "EduInnovation"
            ]
        },
        {
            "id": "https://openalex.org/A2099802555",
            "name": "Yuan Wu",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A2140465146",
            "name": "Cheng Jin",
            "affiliations": [
                "Fudan University",
                "EduInnovation"
            ]
        },
        {
            "id": "https://openalex.org/A2102805196",
            "name": "Weizhong Zhang",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A2140465146",
            "name": "Cheng Jin",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2769331938",
        "https://openalex.org/W2981660954",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2773305749",
        "https://openalex.org/W4221159661",
        "https://openalex.org/W4287185310",
        "https://openalex.org/W2977639561",
        "https://openalex.org/W6687483927",
        "https://openalex.org/W6777407129",
        "https://openalex.org/W2993041304",
        "https://openalex.org/W6762635240",
        "https://openalex.org/W2586735627",
        "https://openalex.org/W2940165378",
        "https://openalex.org/W3217333840",
        "https://openalex.org/W2954572199",
        "https://openalex.org/W3014899985",
        "https://openalex.org/W3206636652",
        "https://openalex.org/W4296556136",
        "https://openalex.org/W2903549000",
        "https://openalex.org/W6766577339",
        "https://openalex.org/W3015042687",
        "https://openalex.org/W3037374099",
        "https://openalex.org/W4283825932",
        "https://openalex.org/W6796152957",
        "https://openalex.org/W4224992933",
        "https://openalex.org/W3043817001",
        "https://openalex.org/W6600120041",
        "https://openalex.org/W3094830752",
        "https://openalex.org/W3098612954",
        "https://openalex.org/W3136525061",
        "https://openalex.org/W2984313141",
        "https://openalex.org/W4312249545",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2964291722",
        "https://openalex.org/W2962896489",
        "https://openalex.org/W4312667155",
        "https://openalex.org/W3010350980",
        "https://openalex.org/W3106882556",
        "https://openalex.org/W2964016027",
        "https://openalex.org/W2101032778",
        "https://openalex.org/W3035068106",
        "https://openalex.org/W4226147466",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W3009246422",
        "https://openalex.org/W3205717647",
        "https://openalex.org/W4287265181",
        "https://openalex.org/W4389298891",
        "https://openalex.org/W2964221239",
        "https://openalex.org/W2982627166",
        "https://openalex.org/W3212454889",
        "https://openalex.org/W3165265377",
        "https://openalex.org/W4312614783",
        "https://openalex.org/W4297971608",
        "https://openalex.org/W4385767582"
    ],
    "abstract": "Depth uncertainty is a core challenge in 3D human pose estimation, especially when the camera parameters are unknown. Previous methods try to reduce the impact of depth uncertainty by multi-view and/or multi-frame feature fusion to utilize more spatial and temporal information. However, they generally lead to marginal improvements and their performance still cannot match the camera-parameter-required methods. The reason is that their handcrafted fusion schemes cannot fuse the features flexibly, e.g., the multi-view and/or multi-frame features are fused separately. Moreover, the diverse and complicated fusion schemes make the principle for developing effective fusion schemes unclear and also raises an open problem that whether there exist more simple and elegant fusion schemes. To address these issues, this paper proposes an extremely concise unified feature fusion transformer (FusionFormer) with minimized handcrafted design for 3D pose estimation. FusionFormer fuses both the multi-view and multi-frame features in a unified fusion scheme, in which all the features are accessible to each other and thus can be fused flexibly. Experimental results on several mainstream datasets demonstrate that FusionFormer achieves state-of-the-art performance. To our best knowledge, this is the first camera-parameter-free method to outperform the existing camera-parameter-required methods, revealing the tremendous potential of camera-parameter-free models. These impressive experimental results together with our concise feature fusion scheme resolve the above open problem. Another appealing feature of FusionFormer we observe is that benefiting from its effective fusion scheme, we can achieve impressive performance with smaller model size and less FLOPs.",
    "full_text": "FusionFormer: A Concise Unified Feature Fusion Transformer for 3D Pose\nEstimation\nYanlu Cai1, Weizhong Zhang1,2,*\n, Yuan Wu1, Cheng Jin1,2,*\n1Fudan University, Shanghai, China\n2Innovation Center of Calligraphy and Painting Creation Technology, MCT, China\n{ylcai20, weizhongzhang, wuyuan, jc}@fudan.edu.cn\nAbstract\nDepth uncertainty is a core challenge in 3D human pose\nestimation, especially when the camera parameters are un-\nknown. Previous methods try to reduce the impact of depth\nuncertainty by multi-view and/or multi-frame feature fusion\nto utilize more spatial and temporal information. However,\nthey generally lead to marginal improvements and their per-\nformance still cannot match the camera-parameter-required\nmethods. The reason is that their handcrafted fusion schemes\ncannot fuse the features flexibly, e.g., the multi-view and/or\nmulti-frame features are fused separately. Moreover, the di-\nverse and complicated fusion schemes make the principle for\ndeveloping effective fusion schemes unclear and also raises\nan open problem that whether there exist more simple and el-\negant fusion schemes. To address these issues, this paper pro-\nposes an extremely concise unified feature fusion transformer\n(FusionFormer) with minimized handcrafted design for 3D\npose estimation. FusionFormer fuses both the multi-view and\nmulti-frame features in a unified fusion scheme, in which all\nthe features are accessible to each other and thus can be fused\nflexibly. Experimental results on several mainstream datasets\ndemonstrate that FusionFormer achieves state-of-the-art per-\nformance. To our best knowledge, this is the first camera-\nparameter-free method to outperform the existing camera-\nparameter-required methods, revealing the tremendous poten-\ntial of camera-parameter-free models. These impressive ex-\nperimental results together with our concise feature fusion\nscheme resolve the above open problem. Another appealing\nfeature of FusionFormer we observe is that benefiting from\nits effective fusion scheme, we can achieve impressive per-\nformance with smaller model size and less FLOPs.\nIntroduction\n3D human pose estimation (Wang et al. 2021) is a fun-\ndamental task in computer vision, which aims to estimate\n3D locations of the keypoints on the human body from im-\nages or videos. Although great efforts have been made in\nthe last decade, it remains challenging due to the depth un-\ncertainty. Recent approaches (Zheng et al. 2021; Liu et al.\n2021; Iskakov et al. 2019; Zhang et al. 2021b) try to reduce\ndepth uncertainty by leveraging the clues contained in the\nfeatures from multiple views and frames. Promising results\n*Corresponding authors\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nhave been reported in the literature. To be precise, the multi-\nframe methods, such as PoseFormer (Zheng et al. 2021)\nand MHFormer (Li et al. 2022), take advantage of Trans-\nformer’s strong capability in long-range relationship mod-\nelling to extract robust features and reduce the impact of the\ninaccuracies in 2D pose estimation. The multi-view meth-\nods (Iskakov et al. 2019; Ma et al. 2021; He et al. 2020)\nfuse the features of the images from multiple views via geo-\nmetric constraints, which can be derived from the camera\nparameters. There also exist some camera-parameter-free\nmethods (Gordon et al. 2022; Shuai, Wu, and Liu 2022).\nThey leverage Transformers to infer the camera parameters\nexplicitly or implicitly. Moreover, these camera-parameter-\nfree methods do not require image or voxel features for ge-\nometric alignment, which leads to significant computational\ncost savings and enables these multi-view methods to jointly\nlearn from multiple frames instead of single frame as the\ncamera-parameter-required methods.\nIn this paper, we focus on the camera-parameter-free ap-\nproaches as they are more practical in real applications.\nHowever, we notice that the performance of these ap-\nproaches (Ma et al. 2022; Gordon et al. 2022; Shuai, Wu,\nand Liu 2022) can not match the camera-parameter-required\nmethods although they are able to learn from multiple\nframes. We argue that this results from their complex hand-\ncrafted feature fusion schemes, which cannot fuse the fea-\ntures flexibly. For instance, grouping keypoint feature fusion\nscheme first groups the keypoints within the same limb and\nthen fuses the features separately for each group, which hin-\nders the cooperative interactions between limbs. The pair-\nwise feature fusion scheme may suffer from a lack of global\nperspective and the reference to other views. As a result,\ncommon features may be repeatedly extracted while cetrain\ninformative features may be overlooked. Spatio-temporal\nseparated feature fusion scheme restricts the features from\ncommunicating along either in the spatial or temporal di-\nmension. Moreover, we find that the above handcrafted fu-\nsion schemes have diverse and complex structures, which\nmake the principle for developing effective fusion scheme\nunclear. They also raise an open problem that whether there\nexist more simple, elegant yet effective fusion schemes.\nTo address the above issues, this paper proposes a concise\nunified Fusion Transformer (FusionFormer) with minimized\nhandcrafted design for 3D pose estimation. FusionFormer\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n900\nfirst encodes 2D pose estimation results into pose features,\nand then leverages transformer encoder to jointly fuse multi-\nview and multi-frame features into the global feature. Thus,\nall the features are accessible to each other and thus can\nbe fused flexibly. Subsequently, Transformer decoder is\nadopted to estimate 3D human pose under each view by in-\ntegrating the global features with the view-specific features\nindividually. The extensive experimental results demonstrate\nthat our method outperforms the state-of-the-art methods\nwith a large margin. To the best of our knowledge, Fusion-\nFormer1 is the first camera-parameter-free method to sur-\npass existing camera-parameter-required methods benefit-\ning from our effective fusion scheme, revealing the tremen-\ndous potential of camera-parameter-free models. Further-\nmore, the experimental results demonstrate that when using\n2D ground truth as input, our method achieves extremely\nhigh accuracy with MPJPE error less than 10mm. This im-\nplies that with more accurate 2D pose estimation techniques\nour method can be further reinforced in the future. The im-\npressive experimental results together with our concise fea-\nture fusion scheme resolve the above open problem. Our\nmain contributions are summarized as follows:\n1. We propose a concise unified feature fusion transformer\n(FusionFormer) for 3D pose estimation, which enables\nthe features to be fused from different frames and views.\n2. Experiments on several datasets demonstrate that Fusion-\nFormer outperforms all the state-of-the-art methods, with\nan accurracy improvement more than 23%. This demon-\nstrates that the impact of depth uncertainty is effectively\nreduced by FusionFormer, revealing the tremendous po-\ntential of camera-parameter-free methods.\n3. To the best of our knowledge, FusionFormer is the first\ncamera-parameter-free method that surpasses existing\ncamera-parameter-required methods benefiting from our\neffective fusion scheme.\n4. The success of FusionFormer verifies the existence of\nconcise yet effective 3D pose estimation approaches,\nwhich can inspire the researchers in developing more ad-\nvanced approaches in the future.\nRelated Work\nMonocular 3D Pose Estimation\nRecovering 3D pose information from a single view is an ill-\nposed problem. Therefore, single-view methods usually in-\ntroduce the prior knowledge of the human body to constrain\nthe location of keypoints, reducing the depth uncertainty.\nCommon constraints include reprojection, e.g., Occlusion-\naware Network (OA-Net) (Cheng et al. 2019), relationship\nbetween adjacent keypoints or within the same limb, e.g.,\nSemGCN (Zhao et al. 2019), HDFormer (Chen et al. 2023)\nand SRNet (Zeng et al. 2020), human body symmetry, e.g.,\nPoseGrammar (Fang et al. 2018), bone length invariance,\ne.g., MotioNet (Shi et al. 2020), temporal consistency, e.g.,\nVideoPose3D (Pavllo et al. 2019), and temporal motion con-\nstraints, e.g., UGCN and GASTNet (Liu et al. 2021).\n1Code and Supplementary materials are available at:\nhttps://github.com/DoUntilFalse/FusionFormer\nBefore the Transformer was introduced into 3D Human\nPose Estimation, monocular methods used various hand-\ncrafted schemes to encode prior knowledge. However, in\nthese methods, the keypoint features can only be fused along\na pre-designed graph, which limits the exploration of rela-\ntionships between two non-adjacent keypoints in the graph.\nPoseFormer (Zheng et al. 2021) proposes the first\nTransformer-based method, and reveals the powerful po-\ntential of the Transformer in 3D pose estimation. Inspired\nby MDN (Li and Lee 2019) and other Multi-Hypothsis\nmethods (Jahangiri and Yuille 2017; Liu et al. 2023), MH-\nFormer (Li et al. 2022) proposes a multi-hypothesis trans-\nformer, which first generates multiple possible 3D poses and\nthen fuses the final 3D pose to reduce depth uncertainty.\nMulti-view 3D Pose Estimation\nCamera-parameter-required Approaches. Multi-view\nmethods with camera parameters model the position re-\nlationships between cameras to construct epipolar geom-\netry constraints, reducing depth uncertainty and the im-\npact of occlusion. Learnable Triangulation of Human Pose\n(LToHP) (Iskakov et al. 2019), CanonFusion (Remelli et al.\n2020) and other studies (Li et al. 2019; G ¨unel et al. 2019)\npropose various triangulation methods to determine the\n3D keypoint positions. DeepFuse (Huang et al. 2020) and\nCrossFusion (Qiu et al. 2019) reproject the 2D feature\ninto 3D voxel space. AdaFuse (Zhang et al. 2021b) and\nEpipolar Transformer (He et al. 2020) leverage epipolar\nline to fuse multi-view features. TransFusion (Ma et al.\n2021), MvP (Zhang et al. 2021a) and MTF-Transformer+\n(MTF+) (Shuai, Wu, and Liu 2022) feed camera parameters\ninto their position encoding to guide the model in modeling\nthe relationships between views.\nCamera-parameter-free Approaches. Multi-PPT (Ma\net al. 2022) employs Transformer to extract human fea-\ntures from images in multiple views, and then feeds them\ninto a shared Transformer encoder for feature fusion to\nobtain 3D human poses. Recently, some methods attempt\nto utilize both multi-frame and multi-view feature fusion\nto boost the performance. FLEX (Gordon et al. 2022) de-\nsigns a viewpoint-independent skeleton representation that\nuses skeleton lengths and angles to represent the human\nbody and leverage the prior knowledge of invariant skele-\nton lengths. Notice that hierarchical keypoints representa-\ntion leads to error accumulation, resulting in large errors\nat the end-point keypoints, such as wrist and ankle. MTF-\nTransformer (MTF) (Shuai, Wu, and Liu 2022) fuses key-\npoint features, multi-view features and multi-frame features\nseparately to obtain more spatial and temporal information.\nMethod\nIn this section, we present our unified feature fusion trans-\nformer, dubbed FusionFormer, which jointly fuses features\nfrom multiple frames and views to estimate 3D human poses\naccurately. As shown in Figure 1, FusionFormer consists of\nfour modules, i.e., 2D pose estimator, pose feature extractor,\nunified feature fusion scheme, and 3D pose regression head.\nWe would like to point out that our feature fusion scheme\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n901\nFigure 1: The architecture of FusionFormer. FusionFormer decompose the 3D Human pose estimation into four stage: 2D\nPose estimation, Feature Extraction, Feature Fusion and 3D Pose Regression Head. FusionFormer unifies spatial and temporal\nfeature fusion to further exploit the powerful modeling capabilities of Transformer.\ncan be integrated with existing techniques for other three\nmodules flexibly and therefore we only provide a brief in-\ntroduction to the formal expressions of these three modules.\n2D Pose Estimator and Feature Extractor\nA general 2D pose estimator, which estimates the poses in\nthe images I obtained from V views with T frames in each\nview, can be formulated as the following function\nF2D : RT×V ×H×W×3 → RT×V ×J×2,\nwhere J are the number of keypoints for one person, H and\nW are height and width of each image. We denote the esti-\nmation result as P2D, that is\nP2D = F2D(I) ∈ RT×V ×J×2.\nFeature extractor takes the 2D poses P2D as input and\nmaps it into a high-dimensional space to obtain the feature\nFembed ∈ RT×V ×J×CJ , that is\nFembed = Embed(P),\nwhere CJ is the number of channels of each keypoint.\nSubsequently, the feature extractor employs several lay-\ners to extract the relationships between keypoints, resulting\nin the pose feature F(0)\npose ∈ RT×V ×CP with CP being the\nnumber of channels of each pose, which aggregates the fea-\ntures of all J keypoints. That is,\nF(0)\npose = Epose(Fembed).\nUnified Feature Fusion Scheme\nWe argue that the features from multiple views and frames\nshould be accessible from each other in the feature fusion\nprocess to reduce the impact of depth uncertainty. Therefore,\nwe propose a unified feature fusion scheme, which is com-\nposed of several Encoder-Decoder blocks. The encoder is\nused to fuse all the features of V Timages to obtain a global\nfeature Fglobal. The decoder integrates the global features\nwith the view-specific features individually to provide more\ninformative features for 3D human pose under each view.\nThe impact of depth uncertainty can be reduced as our fused\nfeatures have the global perspective. The details of our en-\ncoders and decoders are presented below.\nEncoder. Before being fed into the encoder, the feature\nF(0)\npose needs to be reshaped and position encoding needs\nto be added. To perform feature communication and fusion\nacross both spatial and temporal dimensions with Trans-\nformer, we treat F(0)\npose as V ∗ T tokens. Common position\nencodings include cosine position encoding, learnable posi-\ntion encoding and MLP-based position encoding. We note\nthat cosine position encoding (Vaswani et al. 2017) assumes\nthat there is correlation among tokens that decreases with\ndistance, which may not hold true in multi-view and multi-\nframe feature fusion. Moreover, MLP-based position encod-\ning is mainly suitable for scenarios where the number of to-\nkens changes dynamically. When the number of tokens is\nfixed, it will degrade to a learnable position encoding. There-\nfore, inspired by ViT (Dosovitskiy et al. 2021), we adopt\na simple learnable position encoding. After position encod-\ning were added, Layer Normalization are applied. The whole\nprocess can be formulated as\nF(0)\nenc = LN(f(F(0)\npose) +PEenc),\nwhere f reshapes the feature into V ∗ T tokens, PEenc ∈\nR(V ∗T)×CP is the learnable position encoding and LN is\nLayer Normalization.\nF(0)\nenc is then fed into L layers of Transformer Encoder to\nobtain the global feature F(0)\nglobal, i.e.,\nF(0)\nglobal = Encoder(F(0)\nenc).\nTo clarify, when referring to the Transformer Encoder\nor Decoder in this paper, we adopt the vanilla Trans-\nformer (Vaswani et al. 2017), and we will not elaborate on\nits specific structure.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n902\nAs we know, Transformer Encoder is composed of two\nkey modules: a self-attention module and an MLP mod-\nule. The self-attention module is employed for feature fu-\nsion among tokens to obtain the global feature, whereas the\nMLP module serves for inter-channel feature communica-\ntion. One advantage of flattening features across V views\nand T frames into a sequence is that direct communica-\ntion can be performed between any feature pair from the\nV ∗T features. Consequently, the attention matrix is sized of\nV T×V Tinstead of two matrices sized ofV ×V and T ×T\nas previous approaches. We would like to point out that as\n1) V in the typical datasets (Ionescu et al. 2013) is no larger\nthan 4; 2) our Frames T = 27and Blocks B = 2are signif-\nicantly smaller than the baselines (e.g., T = 81 and B = 8\nin PoseFormer (Zheng et al. 2021), T = 351 and B = 3 in\nMHFormer (Li et al. 2022)), our computation and memory\ncost is comparable with the baselines.\nMoreover, this design endows each unit pair with an inde-\npendent attention weight, leading to more effective and flexi-\nble feature fusion compared to the separated fusion methods\nthat rely on shared weights for every view in multi-frame\nfeature fusion or every frame in multi-view feature fusion.\nIn contrast to pairwise multi-view feature fusion methods,\nwhere attention weights are normalized separately, a uni-\nform normalization for our attention weights is performed\nin the self-attention module. This allows for different sums\nof attention weights across the views, therefore, we allow\ndifferent views to have different importance weights in the\nfused feature. In pairwise methods, the sum of attention\nweights for every pair of views remains constant due to the\nseparate normalization, regardless of their importance.\nIn practice, the relationships between the V views of-\nten differ greatly due to factors such as occlusion (Ghafoor\nand Mahmood 2022), camera position, body orientation, and\nother factors, making it difficult to manually summarize\nthem. This also explains why previous handcrafted struc-\ntures struggle to achieve optimal results.\nDecoder. As our goal is to predict the 3D pose inV views,\nwe try to maintain the diversity between features from differ-\nent views, so we partition the featuresF(0)\npose along the views\nand then feed them separately into the decoder. Therefore,\nmulti-view feature fusion is only performed in the encoder.\nTo be precise, the features are first partitioned and normal-\nized together with the positional encoding as follows:\n{F(0)\ndec}v = LN({F(0)\npose}v + PEdec).\nThen global features are integrated with the above view-\nspecific features {F(0)\ndec}v individually in our decoder withL\nlayers to provide more informative features for 3D human\npose under each view. That is\n{F(0)\nfused }v = Decoder(F(0)\nglobal, {F(0)\ndec}v).\nAfter these operations, the features for each view are con-\ncatenated into F(0)\nfused ∈ RV ×T .\nEncoder-Decoder Block. The aforementioned Encoder-\nDecoder is treated as a single block. As shown in Figure\n1, FusionFormer contains B blocks with shared parameters,\nwhere the output F(b)\nfused of the b-th block serves as the input\nof the subsequent block. Finally, the output of this stage is\nobtained as F(B)\nfused .\n3D Pose Regression Head and Loss Function\nTo maximize the capabilities of our feature fusion network,\nwe employ a simple 3D pose regression head to extract the\n3D poses of the center frame (the T+1\n2 -th frame) from each\nview separately following MTF-Transformer. We adopt a\nConv1d layer to perform the weighted summation of all\nframes, aggregating information from all frames to obtain\nfeatures Fagg ∈ RV ×CP that are used for 3D pose regres-\nsion, i.e.,\nFagg = Conv1d(F(B)\npose).\nAfterward, Fagg is fed into two linear layers to obtain the 3D\nposes of the center frame for each view, denoted as P3D ∈\nRV ×J×3, i.e.,\nP3D = Linear(Relu(Linear(Fagg))).\nWe adopt Mean Per Joint Position Error (Wang et al. 2021)\nas our loss function and denote it as MPJPE. MPJPE first\naligns the root (central hip) of predicted 3D pose and\nthe ground truth, and then calculates the averaged Eu-\nclidean Distance between each joints. We adopt the averaged\nMPJPE over V views as the final loss function, i.e.,\nL = 1\nV ∗ J\nVX\nv=1\nJX\nj=1\n\r\rpv,j − pgt\nv,j\n\r\r,\nwhere pv,j ∈ P3D represents the predicted 3d poses after\nalignment, and pgt\nv,j represents the ground truth.\nRemark. We adopt PoseFormer as the feature extractor in\nthe main experiments. Note that FusionFormer achieves im-\npressive results even with a 2-layer FCN (Table 7). It verifies\nthat FusionFormer is not a simple extension of PoseFormer.\nExperiments\nExperiment Setting\nDataset. Human3.6M is the most widely used 3D human\npose estimation dataset, containing over 3 million frames\nof images synchronized captured from four cameras. To-\ntalCapture dataset utilizes 8 completely synchronized cam-\neras to collect 4 types of actions (rom, acting, walking, and\nfreestyle) from 5 subjects (S1, S2, and S3 as Seen sub-\njects and S4 and S5 as Unseen subjects). HumanEva (≈\n50K frames) and MPI-INF-3DHP (≈ 500K frames) are two\nmuch smaller datasets. HumanEva contains 3 calibrated rgb\nvideo sequences from 4 subjects performing 6 common ac-\ntions. MPI-INF-3DHP consists of both constrained indoor\nand complex outdoor scenes captured from 14 cameras.\nEvaluation Metrics. Mean Per Joint Position Error\n(MPJPE) and Procrustes-aligned MPJPE (P-MPJPE) (Wang\net al. 2021) are used as the evaluation metrics.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n903\nMethod Dir. Disc. Eat. Greet Phone Photo Pose Purch. Sit. Smoke Wait Walk Avg.\nMonocular methods\nMDN* (T = 1) 43.8 48.6 49.1 49.8 57.6 61.5 45.9 48.3 62.0 54.8 50.6 43.4 52.7\nSRNet* (T = 243) 46.6 47.1 43.9 41.6 45.8 49.6 46.5 40.0 53.4 46.1 42.6 31.5 44.8\nUGCN* (T = 96) 40.2 42.5 42.6 41.1 46.7 56.7 41.4 42.3 56.2 46.3 42.2 31.7 44.5\nPoseFormer* (T = 81) 41.5 44.8 39.8 42.5 46.5 51.6 42.1 42.0 53.3 45.5 43.3 31.8 44.3\nMHFormer* (T = 351) 39.2 43.1 40.1 40.9 44.9 51.2 40.6 41.3 53.5 43.7 41.1 29.8 43.0\nOA-Net* (T = 128) 38.3 41.3 46.1 40.1 41.6 51.9 41.8 40.9 51.5 42.2 44.6 33.7 42.9\nMulti-view methods with camera parameters\nCanonFusion (T = 1) 27.3 32.1 25.0 26.5 29.3 35.4 28.8 31.6 36.4 31.2 29.9 33.7 30.2\nEpipolar (T = 1) 25.7 27.7 23.7 24.8 26.9 31.4 24.9 26.5 28.8 28.2 26.4 28.3 26.9\nCrossFusion (T = 1) 24.0 26.7 23.2 24.3 24.8 22.8 24.1 28.6 32.1 31.0 25.6 28.0 26.2\nTransFusion (T = 1) 24.4 26.4 23.4 21.1 25.2 23.2 24.7 33.8 29.8 26.8 24.2 26.1 25.8\nLToHP (T = 1) 19.9 20.0 18.9 18.5 20.5 19.4 18.4 22.1 22.5 21.2 20.8 22.1 20.8\nAdaFuse (T = 1) 17.8 19.5 17.6 20.7 19.3 16.8 18.9 20.2 25.7 19.2 20.5 20.5 19.5\nMvP (T = 1) - - - - - - - - - - - - 18.6\nMTF+* (T = 27) 23.4 25.2 23.1 24.4 27.4 28.5 22.8 25.2 28.7 25.9 23.6 22.6 25.8\nMulti-view methods without camera parameters\nMulti-PPT (T = 1) 21.8 26.5 21.0 22.4 23.7 23.1 23.2 27.9 30.7 26.7 23.3 25.3 24.4\nFLEX* (T = 27) - - - - - - - - - - - - 31.7\nMTF* (T = 27) 23.1 25.4 24.7 24.5 27.9 28.3 23.9 24.6 30.7 25.8 24.2 22.8 26.2\nMTF† (T = 27) 17.6 21.3 15.0 18.6 17.6 23.9 16.5 16.9 17.5 18.5 17.0 15.4 18.7\nOurs*(T=27) 22.2 25.3 22.9 23.6 26.0 27.0 22.4 23.9 30.4 25.6 22.8 22.3 25.4\nOurs† (T = 27) 15.7 15.6 13.0 15.9 13.9 15.6 14.9 15.5 15.5 14.3 15.2 14.6 15.1\nFLEX‡ (T = 27) - - - - - - - - - - - - 22.9\nMTF‡ (T = 27) 15.5 17.1 13.7 15.5 14.0 16.2 15.8 16.5 15.8 14.5 14.5 14.3 15.3\nOurs‡ (T = 27) 7.84 8.04 7.39 8.33 7.13 9.02 8.00 8.19 7.57 7.37 7.83 7.26 7.90\nTable 1: Results on Human3.6M. MPJPE is adopted as the evaluation metric. We adopt CPN (*) and ViTPose (†) as the 2D\npose estimator for fair comparison. T is the number of frames. We adopt 2DGT (‡) as input to explore the theoretical upper\nbound of the model. Due to space constraints, we only report detailed results for partial actions.\n2D Pose Estimator. We employ two off-the-shelf 2D pose\nestimators CPN (Chen et al. 2018) and ViTPose (Xu et al.\n2022) on Human3.6M for fair comparison with 2D-to-3D\nmethods and images-to-3D methods, respectively. Detailed\nconsiderations are given in Main Results section. Following\nMTF-Transformer, we employ ResNet101 (He et al. 2015)\nas the 2D pose estimator on TotalCapture dataset.\nFeature Extractor. We choose PoseFormer as the pose\nfeature extractor, denoted as EP , for the main experiment.\nTo show the flexibility of FusionFormer in integrating with\nfeature extractors, we propose 2 baselines, i.e.EFC and ET .\nEFC is a 3-layer FCN. ET is a 2-layer FCN followed by 2\nvanilla Transformer layers with learnable position encoding.\nMore detailed configuration, e.g., dataset partitioning and\nlearning rate, is postponed to the supplementary materials.\nMain Results\nWe report the general comparison results on Human3.6M\nand TotalCapture and the generalization ability evaluation\nresults on two small datasets HumanEva and MPI-INF-\n3DHP. Moreover, to show the superiority of FusionFormer\nfurther, we give more comparison results with the previ-\nous state-of-the-art method MTF-Transformer on extra as-\npects, including scalability, computational efficiency and vi-\nsual analysis. Finally, we reveal the reasons behind the supe-\nriority of FusionFormer by designing a camera extrinsic pa-\nrameter regression task as well as visualizing attention maps.\nHuman3.6M. Note that 2D-to-3D methods use CPN as\nthe 2D pose estimator, whereas image-to-3D methods usu-\nally adopt advanced structures, e.g., Transformer, to achieve\nhigher accuracy. For fair comparison, we report the results\nof FusionFormer with CPN or ViTPose as the pose es-\ntimator. We replace 2D pose estimation with 2D ground\ntruth (2DGT) to explore the theoretical upper bound of our\nmethod. All the results are given in Table 1.\nWe can observe that FusionFormer outperforms both the\n2D-to-3D and image-to-3D methods with large margins,\ni.e., 25.3mm v.s. 26.2mm (MTF-Transformer) and 31.7mm\n(FLEX) with CPN, 15.1mmv.s.18.6mm (MvP) and 24.4mm\n(PPT) with Transformer-based structures.\nNotably, Table 1 demonstrates that FusionFormer with\nCPN outperforms some camera-parameter-required meth-\nods (e.g., MTF-Transformer+ and Cross-view Fusion) and\nsome methods with advanced Transformer based structures\n(e.g., Epipolar Transformer and TransFusion). When ViT-\nPose is adopted, it can beat all the methods consistently.\nMoreover, Table 1 shows that when 2D pose estimation\nresults are replaced with 2DGT, the performance of Fu-\nsionFormer is significantly boosted. Precisely, it achieves an\nMPJPE of 7.91mm, far lower than other 2D-to-3D methods,\ne.g., MTF-Transformer (15.3mm) and FLEX (22.9mm).\nThis implies that the accuracy of the 2D pose estimator has\nbecome the main bottleneck in accurate 3D pose estima-\ntion. It is also worth noting that the MPJPE of FusionFormer\nwith ViTPose is even lower than the theoretical upper bound\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n904\nMethod\nSeen Cameras(1,3,5,7) Unseen Cameras(2,4,6,8)\nMeanSeen Subjects Unseen Subjects Mean Seen Subjects Unseen Subjects MeanW2 FS3 A3 W2 FS3 A3 W2 FS3 A3 W2 FS3 A3\nCrossFusion(*)† 19.0 28.0 21.0 32.0 54.0 33.0 29.0 - - - - - - - -\nCanonFusion(*)† 10.6 30.4 16.3 27.0 65.0 34.2 27.5 22.4 47.1 27.8 39.1 75.7 43.1 38.2 32.9\nMTF+(Res101)† 10.7 26.5 16.7 27.4 49.4 34.1 25.1 13.9 29.2 18.1 29.2 49.5 35.6 27.0 26.1\nFLEX(Res101) 33.2 81.0 34.2 38.3 124 59.5 49.4 109 152 105 114 176 123 125 87.4\nMTF(Res101) 9.30 26.5 14.5 26.7 53.1 33.8 24.7 23.7 40.3 27.4 37.0 61.8 42.9 36.6 30.7\nOurs(Res101) 5.50 15.0 5.68 18.1 37.6 20.6 15.0 22.1 35.4 23.4 23.2 42.6 28.4 28.3 21.7\nTable 2: Comparison results on TotalCapture. MPJPE is adopted as the evaluation metric. We adopt Res101 (He et al. 2015) as\nthe 2D pose estimator for fair comparison. † marks methods that require camera parameters as input.\nDatasets HumanEva MPI-INF-3DHP\nPoseformer(T=27) 35.9 38.5\nMTF-Transformer(T=3) 22.8 14.6\nOurs(T=3) 15.4 5.4\nTable 3: Generalization experiment. 2DGT are used as input.\nof MTF-Transformer, i.e., 15.1mm v.s. 15.3mm, strongly\ndemonstrating the superiority of our feature fusion scheme.\nFinally, to verify that the superiority of FusionFormer\ncomes from effective fuison scheme instead of 2D pose es-\ntimator ViTPose, we re-implement MTF-Transformer using\nViTPose as 2D pose estimator. The results verify that Fu-\nsionFormer still achieves lower error (15.1mmv.s. 18.7mm).\nTotalCapture. We report the results across seen/unseen\ncamera and seen/unseen subjects in Table 2. Due to the sig-\nnificant variations in freestyle videos, the freestyle action\ncan be considered as an unseen action.\nFusionFormer demonstrates better performance than\nall camera-parameter-free methods and outperforms the\ncamera-parameter-required methods in the majority of sce-\nnarios, including the most challenging task with unseen\ncameras and unseen subjects. We notice that FusionFormer\ncan not perform better than MTF-Transformer+, which is a\ncamera-parameter-required method. It must be pointed that\nthis is not a fair comparison, since with the given camera\nparameters as input, it can be expected that the impact of\nunseen cameras on camera-parameter-required methods is\nnegligible. Nevertheless, the significant superiority of Fu-\nsionFormer’s over camera-parameter-required methods on\nthe most challenging tasks with both unseen cameras and\nsubjects demonstrates its impressive generalization ability.\nGeneralization across Datasets. We finetune a pretrained\nmodel on Human3.6M for 10 epochs on HumanEva or MPI-\nINF-3DHP, comparing our method’s generalization capabil-\nity with that of Poseformer and MTF-Transformer. All the\nmodels are trained with the same setting, which are given\nin the appendix. The results in Table 3 shows that Fusion-\nFormer outperforms the above two methods with a large\nmargin, which indicates that our feature fusion scheme ex-\ntracts robust feature representation in pretraining.\nScalability and Computational Efficiency. We report the\nresults of MPJPE v.s. parameters/FLOPs in Figure 3 to show\nthe superiority of FusionFormer in scalability and computa-\nMethods Axis Angle Trans.\nPoseFormer 20◦ 11◦ 1.1m\nMTF-Transformer 6◦ 5◦ 0.3m\nOurs 4◦ 2◦ 0.2m\nTable 4: Average error of Camera Extrinsic Parameter Re-\ngression.\ntional efficiency. We find that existing inflexible handcrafted\nfusion schemes lead to a low performance upper bound in-\ndicated with a dotted line in Figure 3(a), since some features\ncan not been fused directly. To be precise, as shown in Figure\n3(a), MTF-Transformer achieves the optimal performance\nat the model with 10M parameters, which is significantly\nlarger than our model with only 1.89M parameters. Figure\n3(b) shows that the small model size of FusionFormer finally\nleads to low computational complexity. Moreover, Figure 3\nindicates that FusionFormer outperforms MTF-Transfromer\nwith equal parameters/FLOPS.\nCamera Extrinsic Parameter Regression. We conduct\nan experiment to evaluate the ability of models to predict\ncamera position and orientation, which we refer to as Cam-\nera Extrinsic Parameter Regression. We freeze the param-\neters of each pre-trained model, extract the last layer fea-\ntures before the 3D pose regression head, and feed them into\na 3-layer FCN to regress the camera extrinsic parameters,\ni.e., rotation axis, rotation angle, and translation. The re-\nsults in Table 4 show that our method is more accurate than\nMTF-Transformer and PoseFormer. We speculate that the\nreason of why our method can accurately regress 3D human\nposes would stem from our ability to encode the relation-\nships among cameras in feature fusion. All three methods\nperform poorly in predicting camera translation, which we\nattribute to the insensitivity of MPJPE to translation.\nVisual Analysis. We visualize some results in Figure\n2. It shows that FusionFormer performs well when self-\nocclusion and uncommon poses are present. In contrast,\nMTF-Transformer exhibits poor performance in such cases\ndue to its handcrafted feature fusion scheme, which is con-\nsistent with our analysis in the introduction section.\nFusion Scheme Visualization. Figure 4 shows the atten-\ntion matrix of FusionFormer to visualize our fusion scheme.\nIn each block, the horizontal and vertical axes stand for 27\nframes from two views. Thus, the 4 blocks in the diagonal\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n905\nFigure 2: Results of MTF-Transformer and FusionFormer on Human 3.6M.\nFigure 3: Comparison of MPJPE between our FusionFormer\nand MTF-Transformer over different model sizes.\nFigure 4: Visualization of attention map for feature fusion\nin FusionFormer decoder. The Attention map is divided into\n4×4 blocks for better visualization. Each block contains the\nrelationship between all frames (T=27) from a pair of views.\npresent the attention values of feature fusion among multiple\nframes in each view, while the diagonals in all 16 blocks are\nthe values for fusion among multiple views. The large values\nout of the aforementioned 4 blocks and 16 diagonals stand\nfor the feature pairs fused by FusionFormer, which come\nfrom different views and frames and are ignored by previ-\nous methods. Together with the superiority of FusionFormer,\nthese large values imply that the fusion across the features\nfrom different views and frames is valuable and necessary.\nAblation Study\nFrames and Views. The effects of frames and view num-\nbers are shown in Table 5 and 6. They indicate that both tem-\nporal and multi-view information are helpful in reducing the\nimpact of depth uncertainty. Additionally, more accurate 2D\nMethod MPJPE P-MPJPE\n2DGT CPN 2DGT CPN\nT = 81 8.77 25.7 4.64 20.8\nT = 27 7.91 25.3 4.35 20.5\nT = 9 7.32 26.2 3.43 21.3\nT = 3 7.66 26.1 3.98 21.0\nT = 1 8.87 27.3 4.93 21.7\nTable 5: Ablation study for the number of frames T.\nMethod MPJPE P-MPJPE\n2DGT CPN 2DGT CPN\nV = 4 7.91 25.3 4.35 20.5\nV = 2 13.1 30.8 8.26 24.9\nV = 1 42.5 39.9 32.8 31.0\nTable 6: Ablation study for the number of views V .\nMethod MPJPE P-MPJPE\n2DGT CPN 2DGT CPN\nEFC 9.00 27.5 4.90 21.9\nET 8.43 26.2 4.54 21.1\nEP 7.91 25.3 4.35 20.5\nTable 7: Ablation study for Feature Extractor.\ninputs require less temporal information.\nFeature Extractor. We evaluate the performance of Fu-\nsionFormer with different feature extractors in Table 7. The\nresults indicate that more complex feature extractors can in-\ndeed extract more discriminative features. However, even\nusing just a few simple fully connected layers for feature\nextraction, FusionFormer achieves impressive performance,\nfully demonstrating the effectiveness of our scheme.\nConclusion\nWe propose a concise unified Feature Fusion Transformer\nfor 3D pose estimation to reduce the impact of depth un-\ncertainty, performing multi-view and multi-frame feature fu-\nsion in one step. Empirical results show that the superiority\nof our method with the accuracy improvement up to 23%.\nAcknowledgments\nThis work was supported by National Natural Science Fund\nof China (62176064) and Shanghai Municipal Science and\nTechnology Commission (22dz1204900).\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n906\nReferences\nChen, H.; He, J.-Y .; Xiang, W.; Liu, W.; Cheng, Z.-Q.; Liu,\nH.; Luo, B.; Geng, Y .; and Xie, X. 2023. HDFormer: High-\norder Directed Transformer for 3D Human Pose Estimation.\narXiv preprint arXiv:2302.01825.\nChen, Y .; Wang, Z.; Peng, Y .; Zhang, Z.; Yu, G.; and Sun,\nJ. 2018. Cascaded pyramid network for multi-person pose\nestimation. In Proceedings of the IEEE conference on com-\nputer vision and pattern recognition, 7103–7112.\nCheng, Y .; Yang, B.; Wang, B.; Yan, W.; and Tan, R. T. 2019.\nOcclusion-aware networks for 3d human pose estimation in\nvideo. In Proceedings of the IEEE/CVF international con-\nference on computer vision, 723–732.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021.\nAn Image is Worth 16x16 Words: Transformers for Image\nRecognition at Scale. In 9th International Conference on\nLearning Representations, ICLR 2021, Virtual Event, Aus-\ntria, May 3-7, 2021. OpenReview.net.\nFang, H.-S.; Xu, Y .; Wang, W.; Liu, X.; and Zhu, S.-C. 2018.\nLearning pose grammar to encode human body configura-\ntion for 3d pose estimation. In Proceedings of the AAAI\nconference on artificial intelligence, volume 32.\nGhafoor, M.; and Mahmood, A. 2022. Quantification of oc-\nclusion handling capability of 3D human pose estimation\nframework. IEEE Transactions on Multimedia.\nGordon, B.; Raab, S.; Azov, G.; Giryes, R.; and Cohen-Or,\nD. 2022. FLEX: Extrinsic Parameters-free Multi-view 3D\nHuman Motion Reconstruction. In Computer Vision–ECCV\n2022: 17th European Conference, Tel Aviv, Israel, October\n23–27, 2022, Proceedings, Part XXXIII, 176–196. Springer.\nG¨unel, S.; Rhodin, H.; Morales, D.; Campagnolo, J.;\nRamdya, P.; and Fua, P. 2019. DeepFly3D, a deep learning-\nbased approach for 3D limb and appendage tracking in teth-\nered, adult Drosophila. Elife, 8: e48571.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2015. Deep Residual\nLearning for Image Recognition. CoRR, abs/1512.03385.\nHe, Y .; Yan, R.; Fragkiadaki, K.; and Yu, S.-I. 2020. Epipo-\nlar transformers. In Proceedings of the ieee/cvf conference\non computer vision and pattern recognition, 7779–7788.\nHuang, F.; Zeng, A.; Liu, M.; Lai, Q.; and Xu, Q. 2020.\nDeepfuse: An imu-aware network for real-time 3d human\npose estimation from multi-view image. In Proceedings of\nthe IEEE/CVF Winter Conference on Applications of Com-\nputer Vision, 429–438.\nIonescu, C.; Papava, D.; Olaru, V .; and Sminchisescu, C.\n2013. Human3. 6m: Large scale datasets and predictive\nmethods for 3d human sensing in natural environments.\nIEEE transactions on pattern analysis and machine intel-\nligence, 36: 1325–1339.\nIskakov, K.; Burkov, E.; Lempitsky, V .; and Malkov, Y .\n2019. Learnable triangulation of human pose. In Proceed-\nings of the IEEE/CVF international conference on computer\nvision, 7718–7727.\nJahangiri, E.; and Yuille, A. L. 2017. Generating multiple\ndiverse hypotheses for human 3d pose consistent with 2d\njoint detections. In Proceedings of the IEEE International\nConference on Computer Vision Workshops, 805–814.\nLi, C.; and Lee, G. H. 2019. Generating multiple hypotheses\nfor 3d human pose estimation with mixture density network.\nIn Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, 9887–9895.\nLi, W.; Liu, H.; Tang, H.; Wang, P.; and Van Gool, L. 2022.\nMhformer: Multi-hypothesis transformer for 3d human pose\nestimation. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 13147–13156.\nLi, X.; Fan, Z.; Liu, Y .; Li, Y .; and Dai, Q. 2019. 3d pose de-\ntection of closely interactive humans using multi-view cam-\neras. Sensors, 19: 2831.\nLiu, H.; He, J.-Y .; Cheng, Z.-Q.; Xiang, W.; Yang, Q.; Chai,\nW.; Wang, G.; Bao, X.; Luo, B.; Geng, Y .; et al. 2023.\nPoSynDA: Multi-Hypothesis Pose Synthesis Domain Adap-\ntation for Robust 3D Human Pose Estimation. In Proceed-\nings of the 31st ACM International Conference on Multime-\ndia, 5542–5551.\nLiu, J.; Rojas, J.; Li, Y .; Liang, Z.; Guan, Y .; Xi, N.; and Zhu,\nH. 2021. A graph attention spatio-temporal convolutional\nnetwork for 3D human pose estimation in video. In 2021\nIEEE International Conference on Robotics and Automation\n(ICRA), 3374–3380. IEEE.\nMa, H.; Chen, L.; Kong, D.; Wang, Z.; Liu, X.; Tang, H.;\nYan, X.; Xie, Y .; Lin, S.-Y .; and Xie, X. 2021. TransFusion:\nCross-view Fusion with Transformer for 3D Human Pose\nEstimation. In British Machine Vision Conference.\nMa, H.; Wang, Z.; Chen, Y .; Kong, D.; Chen, L.; Liu, X.;\nYan, X.; Tang, H.; and Xie, X. 2022. PPT: token-Pruned\nPose Transformer for monocular and multi-view human\npose estimation. In Computer Vision–ECCV 2022: 17th Eu-\nropean Conference, Tel Aviv, Israel, October 23–27, 2022,\nProceedings, Part V, 424–442. Springer.\nPavllo, D.; Feichtenhofer, C.; Grangier, D.; and Auli, M.\n2019. 3d human pose estimation in video with temporal\nconvolutions and semi-supervised training. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern\nrecognition, 7753–7762.\nQiu, H.; Wang, C.; Wang, J.; Wang, N.; and Zeng, W. 2019.\nCross view fusion for 3d human pose estimation. In Pro-\nceedings of the IEEE/CVF international conference on com-\nputer vision, 4342–4351.\nRemelli, E.; Han, S.; Honari, S.; Fua, P.; and Wang, R.\n2020. Lightweight multi-view 3d pose estimation through\ncamera-disentangled representation. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, 6040–6049.\nShi, M.; Aberman, K.; Aristidou, A.; Komura, T.; Lischin-\nski, D.; Cohen-Or, D.; and Chen, B. 2020. Motionet:\n3d human motion reconstruction from monocular video\nwith skeleton consistency. ACM Transactions on Graphics\n(TOG), 40: 1–15.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n907\nShuai, H.; Wu, L.; and Liu, Q. 2022. Adaptive multi-view\nand temporal fusing transformer for 3d human pose estima-\ntion. IEEE Transactions on Pattern Analysis and Machine\nIntelligence.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. Advances in neural information pro-\ncessing systems, 30.\nWang, J.; Tan, S.; Zhen, X.; Xu, S.; Zheng, F.; He, Z.; and\nShao, L. 2021. Deep 3D human pose estimation: A review.\nComputer Vision and Image Understanding, 210: 103225.\nXu, Y .; Zhang, J.; Zhang, Q.; and Tao, D. 2022. ViTPose:\nSimple Vision Transformer Baselines for Human Pose Esti-\nmation. In Advances in Neural Information Processing Sys-\ntems.\nZeng, A.; Sun, X.; Huang, F.; Liu, M.; Xu, Q.; and Lin, S.\n2020. Srnet: Improving generalization in 3d human pose es-\ntimation with a split-and-recombine approach. In Computer\nVision–ECCV 2020: 16th European Conference, Glasgow,\nUK, August 23–28, 2020, Proceedings, Part XIV 16, 507–\n523. Springer.\nZhang, J.; Cai, Y .; Yan, S.; Feng, J.; et al. 2021a. Direct\nmulti-view multi-person 3d pose estimation. Advances in\nNeural Information Processing Systems, 34: 13153–13164.\nZhang, Z.; Wang, C.; Qiu, W.; Qin, W.; and Zeng, W. 2021b.\nAdafuse: Adaptive multiview fusion for accurate human\npose estimation in the wild. International Journal of Com-\nputer Vision, 129: 703–718.\nZhao, L.; Peng, X.; Tian, Y .; Kapadia, M.; and Metaxas,\nD. N. 2019. Semantic graph convolutional networks for 3d\nhuman pose regression. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition ,\n3425–3435.\nZheng, C.; Zhu, S.; Mendieta, M.; Yang, T.; Chen, C.; and\nDing, Z. 2021. 3d human pose estimation with spatial and\ntemporal transformers. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision, 11656–11665.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n908"
}