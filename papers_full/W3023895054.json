{
  "title": "Multiresolution and Multimodal Speech Recognition with Transformers",
  "url": "https://openalex.org/W3023895054",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5026032406",
      "name": "Georgios Paraskevopoulos",
      "affiliations": [
        "National Technical University of Athens"
      ]
    },
    {
      "id": "https://openalex.org/A5103091951",
      "name": "Srinivas Parthasarathy",
      "affiliations": [
        "Amazon (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A5013650472",
      "name": "Aparna Khare",
      "affiliations": [
        "Amazon (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A5113697636",
      "name": "Shiva Sundaram",
      "affiliations": [
        "Amazon (Germany)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2890197052",
    "https://openalex.org/W2327501763",
    "https://openalex.org/W2143612262",
    "https://openalex.org/W3042657922",
    "https://openalex.org/W2972892814",
    "https://openalex.org/W2902348614",
    "https://openalex.org/W2943493972",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W1503933356",
    "https://openalex.org/W2963303028",
    "https://openalex.org/W2897067191",
    "https://openalex.org/W2884254529",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W2941814890",
    "https://openalex.org/W2890952074",
    "https://openalex.org/W2964182350",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2962778134",
    "https://openalex.org/W2962929176",
    "https://openalex.org/W98035269",
    "https://openalex.org/W2127141656",
    "https://openalex.org/W2962934715",
    "https://openalex.org/W2972451902",
    "https://openalex.org/W2530876040",
    "https://openalex.org/W2884975363",
    "https://openalex.org/W2981165461"
  ],
  "abstract": "This paper presents an audio visual automatic speech recognition (AV-ASR) system using a Transformer-based architecture. We particularly focus on the scene context provided by the visual information, to ground the ASR. We extract representations for audio features in the encoder layers of the transformer and fuse video features using an additional crossmodal multihead attention layer. Additionally, we incorporate a multitask training criterion for multiresolution ASR, where we train the model to generate both character and subword level transcriptions. Experimental results on the How2 dataset, indicate that multiresolution training can speed up convergence by around 50% and relatively improves word error rate (WER) performance by upto 18% over subword prediction models. Further, incorporating visual information improves performance with relative gains upto 3.76% over audio only models. Our results are comparable to state-of-the-art Listen, Attend and Spell-based architectures.",
  "full_text": "Multiresolution and Multimodal Speech Recognition with Transformers\nGeorgios Paraskevopoulos Srinivas Parthasarathy Aparna Khare Shiva Sundaram\nAmazon Lab126\ngeopar@central.ntua.gr, {parsrini,apkhare,sssundar}@amazon.com\nAbstract\nThis paper presents an audio visual automatic\nspeech recognition (A V-ASR) system using a\nTransformer-based architecture. We particu-\nlarly focus on the scene context provided by\nthe visual information, to ground the ASR. We\nextract representations for audio features in\nthe encoder layers of the transformer and fuse\nvideo features using an additional crossmodal\nmultihead attention layer. Additionally, we in-\ncorporate a multitask training criterion for mul-\ntiresolution ASR, where we train the model to\ngenerate both character and subword level tran-\nscriptions. Experimental results on the How2\ndataset, indicate that multiresolution training\ncan speed up convergence by around 50% and\nrelatively improves word error rate (WER) per-\nformance by upto 18% over subword predic-\ntion models. Further, incorporating visual in-\nformation improves performance with relative\ngains upto 3.76% over audio only models. Our\nresults are comparable to state-of-the-art Lis-\nten, Attend and Spell-based architectures.\n1 Introduction\nAutomatic speech recognition is a fundamental\ntechnology used on a daily basis by millions of\nend-users and businesses. Applications include au-\ntomated phone systems, video captioning and voice\nassistants providing an intuitive and seemless in-\nterface between users and end systems. Current\nASR approaches rely solely on utilizing audio in-\nput to produce transcriptions. However, the wide\navailability of cameras in smartphones and home\ndevices acts as motivation to build A V-ASR models\nthat rely on and beneﬁt from multimodal input.\nTraditional A V-ASR systems focus on tracking\nthe user’s facial movements and performing lipread-\ning to augment the auditory inputs (Potamianos\net al., 1997; Mroueh et al., 2015; Tao and Busso,\n2018). The applicability of such models in real\nworld environments is limited, due to the need for\naccurate audio-video alignment and careful camera\nplacement. Instead, we focus on using video to\ncontextualize the auditory input and perform multi-\nmodal grounding. For example, a basketball court\nis more likely to include the term “lay-up” whereas\nan ofﬁce place is more likely include the term “lay-\noff”. This approach can boost ASR performance,\nwhile the requirements for video input are kept\nrelaxed (Caglayan et al., 2019; Hsu et al., 2019).\nAdditionally we consider a multiresolution loss\nthat takes into account transcriptions at the charac-\nter and subword level. We show that this scheme\nregularizes our model showing signiﬁcant improve-\nments over subword models. Multitask learning on\nmultiple levels has been previously explored in the\nliterature, mainly in the context of CTC (Sanabria\nand Metze, 2018; Krishna et al., 2018; Ueno et al.,\n2018). A mix of seq2seq and CTC approaches\ncombine word and character level (Kremer et al.,\n2018; Ueno et al., 2018) or utilize explicit phonetic\ninformation (Toshniwal et al., 2017; Sanabria and\nMetze, 2018).\nModern ASR systems rely on end-to-end, align-\nment free neural architectures, i.e. CTC (Graves\net al., 2006) or sequence to sequence models\n(Graves et al., 2013; Zhang et al., 2017). The use of\nattention mechanisms signiﬁcantly improve results\nin (Chorowski et al., 2015) and (Chan et al., 2016).\nRecently, the success of transformer architectures\nfor NLP tasks (Vaswani et al., 2017; Devlin et al.,\n2019; Dai et al., 2019) has motivated speech re-\nsearchers to investigate their efﬁcacy in end-to-end\nASR (Karita et al., 2019b). Zhou et. al., apply\nan end-to-end transformer architecture for Man-\ndarin Chinese ASR (Zhou et al., 2018). Speech-\nTransformer extends the scaled dot-product atten-\ntion mechanism to 2D and achieves competitive\nresults for character level recognition (Dong et al.,\n2018; Karita et al., 2019a). Pham et. al. introduce\nthe idea of stochastically deactivating layers dur-\narXiv:2004.14840v1  [eess.AS]  29 Apr 2020\ning training to achieve a very deep model (Pham\net al., 2019). A major challenge of the transformer\narchitecture is the quadratic memory complexity\nas a function of the input sequence length. Most\narchitectures employ consecutive feature stacking\n(Pham et al., 2019) or CNN preprocessing (Dong\net al., 2018; Karita et al., 2019b) to downsample\ninput feature vectors. Mohamed et al. (2019) use\na VGG-based input network to downsample the\ninput sequence and achieve learnable positional\nembeddings.\nMultimodal grounding for ASR systems has\nbeen explored in (Caglayan et al., 2019), where\na pretrained RNN-based ASR model is ﬁnetuned\nwith visual information through Visual Adaptive\nTraining. Furthermore, Hsu et al. (2019) use a\nweakly supervised semantic alignment criterion to\nimprove ASR results when visual information is\npresent. Multimodal extensions of the transformer\narchitecture have also been explored. These exten-\nsions mainly fuse visual and language modalities\nin the ﬁelds of Multimodal Translation and Image\nCaptioning. Most approaches focus on using the\nscaled dot-product attention layer for multimodal\nfusion and cross-modal mapping. Afouras et al.\n(2018) present a transformer model for A V-ASR\ntargeted for lip-reading in the wild tasks. It uses a\nself attention block to encode the audio and visual\ndimension independently. A decoder individually\nattends to the audio and video modalities producing\ncharacter transcriptions. In comparison our study\nuses the video features to provide contextual infor-\nmation to our ASR. Libovick`y et al. (2018) employ\ntwo encoder networks for the textual and visual\nmodalities and propose four methods of using the\ndecoder attention layer for multimodal fusion, with\nhierarchical fusion yielding the best results. Yu\net al. (2019) propose an encoder variant to fuse\ndeep, multi-view image features and use them to\nproduce image captions in the decoder. Le et al.\n(2019) use cascaded multimodal attention layers to\nfuse visual information and dialog history for a mul-\ntimodal dialogue system. Tsai et al. (2019) present\nMultimodal Transformers, relying on a deep pair-\nwise cascade of cross-modal attention mechanisms\nto map between modalities for multimodal senti-\nment analysis.\nIn relation to the previous studies, the main con-\ntributions of this study are a) a fusion mechanism\nfor audio and visual modalities based on the cross-\nmodal scaled-dot product attention, b) an end to\nend training procedure for multimodal grounding\nin ASR and c) the use of a multiresolution training\nscheme for character and subword level recognition\nin a seq2seq setting without relying on explicit pho-\nnetic information. We evaluate our system in the\n300 hour subset of the How2 database (Sanabria\net al., 2018), achieving relative gains up to 3.76%\nwith the addition of visual information. Further we\nshow relative gains of 18% with the multiresolution\nloss. Our results are comparable to state-of-the-art\nASR performance on this database.\n2 Proposed Method\nOur transformer architecture uses two transformer\nencoders to individually process acoustic and vi-\nsual information (Fig. 1). Audio frames are fed to\nthe ﬁrst set of encoder layers. We denote the space\nof the encoded audio features as the audio space\nA. Similarly, video features are projected to the\nvideo space V using the second encoder network.\nFeatures from audio and visual space are passed\nthrough a tied feed forward layer that projects them\ninto a common space before passing them to their\nindividual encoder layers respectively. This tied\nembedding layer is important for fusion as it helps\nalign the semantic audio and video spaces. We then\nuse a cross-modal attention layer that maps pro-\njected video representations to the projected audio\nspace (Section 2.1). The outputs of this layer are\nadded to the original audio features using a learn-\nable parameter αto weigh their contributions. The\nfused features are then fed into the decoder stack\nfollowed by dense layers to generate character and\nsubword outputs. For multiresolution predictions\n(Section 2.2), we use a common decoder for both\ncharacter and subword level predictions, followed\nby a dense output layer for each prediction. This\nreduces the model parameters and enhances the\nregularization effect of multitask learning.\n2.1 Cross-modal Attention\nScaled dot-product attention operates by construct-\ning three matrices, K, V and Qfrom sequences\nof inputs. K and V may be considered keys and\nvalues in a “soft” dictionary, whileQis a query that\ncontextualizes the attention weights. The attention\nmechanism is described in Eq. 1, where σdenotes\nthe softmax operation.\nY = σ(KQT )V (1)\nThe case where K, V and Q are constructed\nDot product attention\nEncoder Layer\nEncoder Layer…\nEncoder Layer\nN x\nDecoder Layer\nDecoder Layer…\nDecoder Layer\nM x\nAudio frames\nSubword\nprediction\nVideo frames\nKVQ\nVideo Encoder\nDown-sampling\n!\n\" !\n Character \nprediction\n#\n1−#\nTied Dense LayerFusion\nSubword\nTranscription\nCharacter \nTranscription\nTied Dense Layer\nFigure 1: Overall system architecture. A cross-modal scaled dot-product attention layer is used to project the visual\ndata into the audio feature space followed by an additive fusion.\nusing the same input sequence consists a self-\nattention mechanism. We are interested in cross-\nmodal attention, where K and V are constructed\nusing inputs from one modality M1, video in our\ncase (Fig. 1) and Q using another modality M2,\naudio. This conﬁguration as an effective way to\nmap features from M1 to M2 (Tsai et al., 2019).\nNote, that such a conﬁguration is used in the de-\ncoder layer of the original transformer architecture\n(Vaswani et al., 2017) where targets are attended\nbased on the encoder outputs.\n2.2 Multiresolution training\nWe propose the use of a multitask training scheme\nwhere the model predicts both character and sub-\nword level transcriptions. We jointly optimize the\nmodel using the weighted sum of character and\nsubword level loss, as in Eq. 2:\nL= γ∗Lsubword + (1−γ) ∗Lcharacter (2)\nwhere γis a hyperparameter that controls the im-\nportance of each task.\nThe intuition for this stems from the reasoning\nthat character and subword level models perform\ndifferent kinds of mistakes. For character predic-\ntion, the model tends to predict words that sound\nphonetically similar to the ground truths, but are\nsyntactically disjoint with the rest of the sentence.\nSubword prediction, yields more syntactically cor-\nrect results, but rare words tend to be broken down\nto more common words that sound similar but are\nsemantically irrelevant. For example, character\nlevel prediction may turn “old-fashioned” into “old-\nfashioning”, while subword level turns the sentence\n“ukuleles are different” to “you go release are differ-\nent”. When combining the losses, subword predic-\ntion, which shows superior performance is kept as\nthe preliminary output, while the character predic-\ntion is used as an auxiliary task for regularization.\n3 Experimental Setup\nWe conduct our experiments on the How2 instruc-\ntional videos database (Sanabria et al., 2018). The\ndataset consists of300 hours of instructional videos\nfrom the YouTube platform. These videos depict\npeople showcasing particular skills and have high\nvariation in video/audio quality, camera angles and\nduration. The transcriptions are mined from the\nYouTube subtitles, which contain a mix of automat-\nically generated and human annotated transcrip-\ntions. Audio is encoded using 40 mel-ﬁlterbank\ncoefﬁcients and 3 pitch features with a frame size\nof 10 ms, yielding 43-dimensional feature vec-\ntors. The ﬁnal samples are segments of the original\nvideos, obtained using word-level alignment. We\nfollow the video representation of the original pa-\nInput handling Recognition level WER\nFiltering Character 33.0\nFiltering Subword 29.7\nChunking Character 31.3\nChunking Subword 29.9\nStacking Character 28.3\nStacking Subword 26.1\nStacking MR 21.3\nTable 1: Results for different methods of input ﬁlter-\ning for different prediction resolutions. MR stands for\nmultiresolution.\nper (Caglayan et al., 2019), where a 3D ResNeXt-\n101 architecture, pretrained on action recognition,\nis used to extract2048D features (Hara et al., 2018).\nVideo features are average pooled over the video\nframes yielding a single feature vector. For our ex-\nperiments, we use the train, development and test\nsplits proposed by (Sanabria et al., 2018), which\nhave sizes 298.2 hours, 3.2 hours and 3.7 hours\nrespectively.\nOur model consists of 6 encoder layers and 4\ndecoder layers. We use transformer dimension 480,\nintermediate ReLU layer size1920 and 0.2 dropout.\nAll attention layers have 6 attention heads. The\nmodel is trained using Adam optimizer with learn-\ning rate 10−3 and 8000 warmup steps. We employ\nlabel smoothing of 0.1. We weigh the multitask\nloss with γ = 0.5 which gives the best perfor-\nmance. A coarse search was performed for tuning\nall hyperparameters over the development set. For\ncharacter-level prediction, we extract41 graphemes\nfrom the transcripts. For subword-level predic-\ntion, we train a SentencePiece tokenizer (Kudo and\nRichardson, 2018) over the train set transcriptions\nusing byte-pair encoding and vocabulary size 1200.\nFor decoding we use beam search with beam size 5\nand length normalization parameter 0.7. We train\nmodels for up to 200 epochs and the model achiev-\ning the best loss is selected using early stopping.\nAny tuning of the original architecture is performed\non the development split. No language model or\nensemble decoding is used in the output.\n4 Results and Discussion\nOne of the challenges using scaled dot-product at-\ntention is the quadratic increase of layerwise mem-\nory complexity as a function of the input sequence\nlength. This issue is particularly prevalent in ASR\n⇑\nFeatures Level WER over audio\nAudio Subword 26.1 -\nAudio + ResNeXt Subword 25.0 3 .45%\nAudio MR 21.3 -\nAudio + ResNeXt MR 20.5 3 .76%\nAudio (B) Subword 19.2 -\nAudio + ResNext (B) Subword 18.4 3.13%\nTable 2: Comparison of audio only ASR models ver-\nsus A V ASR models with ResNeXt image features.MR\nstands for multiresolution. (B) shows the results for the\nLAS model (Caglayan et al., 2019)\ntasks, with large input sequences. We explore three\nsimple approaches to work around this limitation.\nFirst, we ﬁlter out large input sequences (x> 15s),\nleading to loss of 100 hours of data. Second we,\nchunk the input samples to smaller sequences, us-\ning forced-alignment with a conventional DNN-\nHMM model to ﬁnd pauses to split the input and\nthe transcriptions. Finally, we stack 4 consecutive\ninput frames into a single feature vector, thus re-\nducing the input length by 4. Note that this only re-\nshapes the input data as the dimension of our input\nis increased by the stacking process 1. Results for\nthe downsampling techniques for character and sub-\nword level predictions are summarized in Table 1.\nWe observe that subword-level model performs bet-\nter than the character level (upto 10% relative) in\nall settings. This can be attributed to the smaller\nnumber of decoding steps needed for the subword\nmodel, where error accumulation is smaller. Fur-\nthermore, we see that the naive ﬁltering of large\nsequences yields to underperforming systems due\nto the large data loss. Additionally, we see that\nframe stacking has superior performance to chunk-\ning. This is not surprising as splitting the input\nsamples to smaller chunks leads to the loss of con-\ntextual information which is preserved with frame\nstacking. We evaluate the proposed multiresolution\ntraining technique with the frame stacking tech-\nnique, observing a signiﬁcant improvement(18.3%)\nin the ﬁnal WER. We thus observe that predict-\ning ﬁner resolutions as an auxiliary task can be\nused as an effective means of regularization for\nthis sequence to sequence speech recognition task.\nFurthermore, we have empirically observed that\nwhen training in multiple resolutions, models can\nconverge around 50% faster than single resolution\nmodels.\n1We tried to use the convolutional architecture from (Mo-\nhamed et al., 2019), but it failed to converge in our experi-\nments, possibly due to lack of data\nMissing input handling WER\nZeros 23.1\nGaussian Noise σ=0.2 22.6\nGating visual input α=0 22.8\nTable 3: Experimental evaluation of A V-ASR model for\nhandling missing visual input. Here σdenotes the stan-\ndard deviation of the noise\nNext, we evaluate relative performance improve-\nment obtained from utilizing the visual features\n(Table 2). We observe that incorporating visual\ninformation improves ASR results. Our A V-ASR\nsystem yields gains > 3% over audio only mod-\nels for both subword and multiresolution predic-\ntions. Finally, we observe that while the Listen,\nAttend and Spell-based architecture of (Caglayan\net al., 2019) is slightly stronger than the transformer\nmodel, the gains from adding visual information\nis consistent across models. It is important to note\nthat our models are trained end-to-end with both\naudio and video features.\nAn important question for real-world deploy-\nment of multimodal ASR systems is their perfor-\nmance when the visual modality is absent. Ideally,\na robust system satisfactorily performs when the\nuser’s camera is off or in low light conditions. We\nevaluate our A V-ASR systems in the absence of\nvisual data with the following experiments - a) re-\nplace visual feature vectors by zeros b) initialize\nvisual features with gaussian noise with standard\ndeviation 0.2 c) tweak the value α to 0 on infer-\nence, gating the visual features completely. Table 3\nshows the results for the different experiments. Re-\nsults indicate gating visual inputs works better than\nzeroing them out. Adding a gaussian noise per-\nforms best which again indicates the limited avail-\nability of data. Overall, in the absence of visual\ninformation, without retraining, the A V-ASR model\nrelatively worsens by 6% compared to audio only\nmodels.\n5 Conclusions\nThis paper explores the applicability of the trans-\nformer architecture for multimodal grounding in\nASR. Our proposed framework uses a crossmodal\ndot-product attention to map visual features to au-\ndio feature space. Audio and visual features are\nthen combined with a scalar additive fusion and\nused to predict character as well as subword tran-\nscriptions. We employ a novel multitask loss that\ncombines the subword level and character losses.\nResults on the How2 database show that a) mul-\ntiresolution losses regularizes our model producing\nsigniﬁcant gains in WER over character level and\nsubword level losses individually b) Adding visual\ninformation results in relative gains of 3.76% over\naudio model’s results validating our model.\nDue to large memory requirements of the atten-\ntion mechanism, we apply aggressive preprocess-\ning to shorten the input sequences, which may hurt\nmodel performance. In the future, we plan to alle-\nviate this by incorporating ideas from sparse trans-\nformer variants (Kitaev et al., 2020; Child et al.,\n2019). Furthermore, we will experiment with more\nellaborate, attention-based fusion mechanisms. Fi-\nnally, we will evaluate the multiresolution loss on\nlarger datasets to analyze it’s regularizing effects.\nReferences\nTriantafyllos Afouras, Joon Son Chung, Andrew Se-\nnior, Oriol Vinyals, and Andrew Zisserman. Deep\naudio-visual speech recognition. IEEE transactions\non pattern analysis and machine intelligence, 2018.\nOzan Caglayan, Ramon Sanabria, Shruti Palaskar, Loic\nBarraul, and Florian Metze. Multimodal ground-\ning for sequence-to-sequence speech recognition.\nIn ICASSP 2019-2019 IEEE International Confer-\nence on Acoustics, Speech and Signal Processing\n(ICASSP), pages 8648–8652. IEEE, 2019.\nWilliam Chan, Navdeep Jaitly, Quoc V . Le, and Oriol\nVinyals. Listen, attend and spell: A neural network\nfor large vocabulary conversational speech recogni-\ntion. In Proc. IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP),\npages 4960–4964, 2016.\nRewon Child, Scott Gray, Alec Radford, and Ilya\nSutskever. Generating long sequences with sparse\ntransformers. arXiv preprint arXiv:1904.10509,\n2019.\nJan Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk,\nKyunghyun Cho, and Yoshua Bengio. Attention-\nbased models for speech recognition. In Proceed-\nings of the 28th International Conference on Neural\nInformation Processing Systems - Volume 1, NIPS15,\npage 577585, Cambridge, MA, USA, 2015. MIT\nPress.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime G\nCarbonell, Quoc Le, and Ruslan Salakhutdinov.\nTransformer-xl: Attentive language models beyond\na ﬁxed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2978–2988, 2019.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understand-\ning. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota, June 2019. As-\nsociation for Computational Linguistics. doi: 10.\n18653/v1/N19-1423. URL https://www.aclweb.\norg/anthology/N19-1423.\nLinhao Dong, Shuang Xu, and Bo Xu. Speech-\ntransformer: A no-recurrence sequence-to-sequence\nmodel for speech recognition. In Proc. IEEE Inter-\nnational Conference on Acoustics, Speech and Sig-\nnal Processing (ICASSP), pages 5884–5888. IEEE,\n2018.\nAlex Graves, Santiago Fern ´andez, Faustino Gomez,\nand J ¨urgen Schmidhuber. Connectionist tempo-\nral classiﬁcation: Labelling unsegmented sequence\ndata with recurrent neural networks. In Proceed-\nings of the 23rd International Conference on Ma-\nchine Learning, ICML 06, page 369376, New\nYork, NY , USA, 2006. Association for Computing\nMachinery. ISBN 1595933832. doi: 10.1145/\n1143844.1143891. URL https://doi.org/10.\n1145/1143844.1143891.\nAlex Graves, Abdel-rahman Mohamed, and Geof-\nfrey E. Hinton. Speech recognition with deep recur-\nrent neural networks. In Proc. IEEE International\nConference on Acoustics, Speech, and Signal Pro-\ncessing (ICASSP). IEEE, 2013.\nKensho Hara, Hirokatsu Kataoka, and Yutaka Satoh.\nCan spatiotemporal 3d cnns retrace the history of 2d\ncnns and imagenet? In Proceedings of the IEEE\nconference on Computer Vision and Pattern Recog-\nnition, pages 6546–6555, 2018.\nWei-Ning Hsu, David Harwath, and James Glass.\nTransfer learning from audio-visual grounding to\nspeech recognition. Proc. Interspeech 2019, pages\n3242–3246, 2019.\nShigeki Karita, Nelson Enrique Yalta Soplin, Shinji\nWatanabe, Marc Delcroix, Atsunori Ogawa, and To-\nmohiro Nakatani. Improving Transformer-Based\nEnd-to-End Speech Recognition with Connection-\nist Temporal Classiﬁcation and Language Model In-\ntegration. In Proc. INTERSPEECH, pages 1408–\n1412, 2019a.\nShigeki Karita, Xiaofei Wang, Shinji Watanabe,\nTakenori Yoshimura, Wangyou Zhang, Nanxin\nChen, Tomoki Hayashi, Takaaki Hori, Hirofumi In-\naguma, Ziyan Jiang, Masao Someki, Nelson En-\nrique Yalta Soplin, and Ryuichi Yamamoto. A com-\nparative study on transformer vs RNN in speech\napplications. In IEEE Automatic Speech Recog-\nnition and Understanding Workshop, ASRU 2019,\nSingapore, December 14-18, 2019, pages 449–456.\nIEEE, 2019b. doi: 10.1109/ASRU46091.2019.\n9003750. URL https://doi.org/10.1109/\nASRU46091.2019.9003750.\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.\nReformer: The efﬁcient transformer. In Inter-\nnational Conference on Learning Representations,\n2020. URL https://openreview.net/forum?\nid=rkgNKkHtvB.\nJan Kremer, Lasse Borgholt, and Lars Maaløe. On\nthe inductive bias of word-character-level multi-task\nlearning for speech recognition. 2018.\nKalpesh Krishna, Shubham Toshniwal, and Karen\nLivescu. Hierarchical multitask learning for\nctc-based speech recognition. arXiv preprint\narXiv:1807.06234, 2018.\nTaku Kudo and John Richardson. Sentencepiece: A\nsimple and language independent subword tokenizer\nand detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66–71, 2018.\nHung Le, Doyen Sahoo, Nancy Chen, and Steven\nHoi. Multimodal transformer networks for end-to-\nend video-grounded dialogue systems. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 5612–5623,\n2019.\nJindˇrich Libovick`y, Jindˇrich Helcl, and David Mareˇcek.\nInput combination strategies for multi-source trans-\nformer decoder. In Proc. 3rd Conference on Ma-\nchine Translation, pages 253–260, 2018.\nAbdelrahman Mohamed, Dmytro Okhonko, and Luke\nZettlemoyer. Transformers with convolutional con-\ntext for asr. CoRR, 2019.\nYoussef Mroueh, Etienne Marcheret, and Vaibhava\nGoel. Deep multimodal learning for audio-visual\nspeech recognition. In 2015 IEEE International\nConference on Acoustics, Speech and Signal Pro-\ncessing (ICASSP), pages 2130–2134. IEEE, 2015.\nNgoc-Quan Pham, Thai-Son Nguyen, Jan Niehues,\nMarkus M ¨uller, and Alex Waibel. Very deep self-\nattention networks for end-to-end speech recogni-\ntion. Proc. Interspeech 2019, pages 66–70, 2019.\nGerasimos Potamianos, Eric Cosatto, Hans Peter Graf,\nand David B Roe. Speaker independent audio-visual\ndatabase for bimodal asr. In Proc. European Tuto-\nrial and Research Workshop on Audio-Visual Speech\nProcessing, pages 65–68, 1997.\nRamon Sanabria and Florian Metze. Hierarchical mul-\ntitask learning with ctc. In 2018 IEEE Spoken Lan-\nguage Technology Workshop (SLT), pages 485–490.\nIEEE, 2018.\nRamon Sanabria, Ozan Caglayan, Shruti Palaskar,\nDesmond Elliott, Lo ¨ıc Barrault, Lucia Specia, and\nFlorian Metze. How2: a large-scale dataset for mul-\ntimodal language understanding. In Proceedings of\nthe Workshop on Visually Grounded Interaction and\nLanguage (ViGIL). NeurIPS, 2018.\nFei Tao and Carlos Busso. Aligning audiovisual fea-\ntures for audiovisual speech recognition. In 2018\nIEEE International Conference on Multimedia and\nExpo (ICME), pages 1–6. IEEE, 2018.\nShubham Toshniwal, Hao Tang, Liang Lu, and Karen\nLivescu. Multitask learning with low-level auxiliary\ntasks for encoder-decoder based speech recognition.\nProc. Interspeech 2017, pages 3532–3536, 2017.\nYao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang,\nJ Zico Kolter, Louis-Philippe Morency, and Rus-\nlan Salakhutdinov. Multimodal transformer for un-\naligned multimodal language sequences. In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 6558–\n6569, 2019.\nSei Ueno, Hirofumi Inaguma, Masato Mimura, and Tat-\nsuya Kawahara. Acoustic-to-word attention-based\nmodel complemented with character-level ctc-based\nmodel. In 2018 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP),\npages 5804–5808. IEEE, 2018.\nAshish Vaswani, Noam Shazeer, Niki Parmar,\nJakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁ ukasz Kaiser, and Illia Polosukhin. At-\ntention is all you need. In I. Guyon, U. V .\nLuxburg, S. Bengio, H. Wallach, R. Fergus,\nS. Vishwanathan, and R. Garnett, editors, Ad-\nvances in Neural Information Processing Systems\n30, pages 5998–6008. Curran Associates, Inc.,\n2017. URL http://papers.nips.cc/paper/\n7181-attention-is-all-you-need.pdf .\nJun Yu, Jing Li, Zhou Yu, and Qingming Huang. Multi-\nmodal transformer with multi-view visual represen-\ntation for image captioning. IEEE Transactions on\nCircuits and Systems for Video Technology, 2019.\nYu Zhang, William Chan, and Navdeep Jaitly. Very\ndeep convolutional networks for end-to-end speech\nrecognition. pages 4845–4849, 2017.\nShiyu Zhou, Linhao Dong, Shuang Xu, and Bo Xu.\nSyllable-based sequence-to-sequence speech recog-\nnition with the transformer in mandarin chinese.\nProc. Interspeech 2018, pages 791–795, 2018.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8277629017829895
    },
    {
      "name": "Speech recognition",
      "score": 0.7259331345558167
    },
    {
      "name": "Transformer",
      "score": 0.6322821378707886
    },
    {
      "name": "Encoder",
      "score": 0.5652562975883484
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4789324700832367
    },
    {
      "name": "Word error rate",
      "score": 0.47367244958877563
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}