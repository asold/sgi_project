{
  "title": "Adapting vs. Pre-training Language Models for Historical Languages",
  "url": "https://openalex.org/W4282597027",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4223688170",
      "name": "Manjavacas, Enrique",
      "affiliations": [
        "Leiden University"
      ]
    },
    {
      "id": "https://openalex.org/A4225312389",
      "name": "Fonteyn, Lauren",
      "affiliations": [
        "Leiden University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2801930304",
    "https://openalex.org/W4206981623",
    "https://openalex.org/W3175205311",
    "https://openalex.org/W3096186608",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3084721699",
    "https://openalex.org/W6968617151",
    "https://openalex.org/W3210204379",
    "https://openalex.org/W1597195725",
    "https://openalex.org/W2578642394",
    "https://openalex.org/W2963099212",
    "https://openalex.org/W2971277088",
    "https://openalex.org/W4394027648",
    "https://openalex.org/W3210517762",
    "https://openalex.org/W3114431132",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2992448193",
    "https://openalex.org/W2576614118",
    "https://openalex.org/W3209692886",
    "https://openalex.org/W2956105246",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W88980739",
    "https://openalex.org/W2971301409",
    "https://openalex.org/W3095348722",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2952411148",
    "https://openalex.org/W2972309718",
    "https://openalex.org/W2795782019",
    "https://openalex.org/W1662133657",
    "https://openalex.org/W2972893188",
    "https://openalex.org/W3024226488",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2939507640",
    "https://openalex.org/W2121879602",
    "https://openalex.org/W2953320089",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3135427360",
    "https://openalex.org/W46679369",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W3170883728",
    "https://openalex.org/W2971028401",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W4381192993",
    "https://openalex.org/W4297792210",
    "https://openalex.org/W4226333603",
    "https://openalex.org/W3095771422",
    "https://openalex.org/W4297847885",
    "https://openalex.org/W3035582413",
    "https://openalex.org/W2763088512",
    "https://openalex.org/W3011279327",
    "https://openalex.org/W2750845122",
    "https://openalex.org/W3117543199",
    "https://openalex.org/W2900671509"
  ],
  "abstract": "As large language models such as BERT are becoming increasingly popular in Digital Humanities (DH), the question has arisen as to how such models can be made suitable for application to specific textual domains, including that of 'historical text'. Large language models like BERT can be pretrained from scratch on a specific textual domain and achieve strong performance on a series of downstream tasks. However, this is a costly endeavour, both in terms of the computational resources as well as the substantial amounts of training data it requires. An appealing alternative, then, is to employ existing 'general purpose' models (pre-trained on present-day language) and subsequently adapt them to a specific domain by further pre-training. Focusing on the domain of historical text in English, this paper demonstrates that pre-training on domain-specific (i.e. historical) data from scratch yields a generally stronger background model than adapting a present-day language model. We show this on the basis of a variety of downstream tasks, ranging from established tasks such as Part-of-Speech tagging, Named Entity Recognition and Word Sense Disambiguation, to ad-hoc tasks like Sentence Periodization, which are specifically designed to test historically relevant processing.",
  "full_text": "Adapting vs. Pre-Training Language Models for Historical Languages\nEnrique Manjavacas1 and Lauren Fonteyn1\n1Leiden University, The Netherlands\nCorresponding author: Enrique Manjavacas , enrique.manjavacas@gmail.com\nAbstract\nAs large language models such as BERT are becoming increasingly popular in Digital Humanities (DH),\nthe question has arisen as to how such models can be made suitable for application to specific textual\ndomains, including that of ‘historical text’. Large language models like BERT can be pre-trained from\nscratch on a specific textual domain and achieve strong performance on a series of downstream tasks.\nHowever, this is a costly endeavour, both in terms of the computational resources as well as the substantial\namounts of training data it requires. An appealing alternative, then, is to employ existing ‘general purpose’\nmodels (pre-trained on present-day language) and subsequently adapt them to a specific domain by further\npre-training. Focusing on the domain of historical text in English, this paper demonstrates that pre-training\non domain-specific (i.e. historical) data from scratch yields a generally stronger background model than\nadapting a present-day language model. We show this on the basis of a variety of downstream tasks,\nranging from established tasks such as Part-of-Speech Tagging, Named Entity Recognition and Word\nSense Disambiguation, to ad-hoc tasks like Sentence Periodization, which are specifically designed to test\nhistorically relevant text processing.\nKeywords\nNatural Language Processing;Historical NLP;Language Models\nI INTRODUCTION\nIn recent years, there has been a great interest within the Digital Humanities research community\nto utilize semantic vector representation algorithms for the computer-aided retrieval, annotation,\nand analysis of textual data. By means of such algorithms, the contextual distribution of linguistic\nitems – which could be words, but also phrases, sentences, or even longer chunks of text – can\nbe represented as (compressed) numeric vectors that serve as a proxy of their meaning [e.g.\nTurney and Pantel, 2010, Erk, 2012, Lenci, 2018]. As these semantic vector algorithms grew in\npopularity, it soon became clear that they could be of service to the Digital Humanist in various\nways. By training a computational model to generate a vector representation – or ‘embedding’ –\nfor a given word, the Digital Humanist can for instance use these embeddings to automatically\nretrieve documents that discuss the concept the word refers to without naming it explicitly\n[Wevers and Koolen, 2020], or to extract all semantically related words (e.g.friendship: near-\nsynonyms, amity; antonyms, hostility) from a target text collection [e.g. van Eijnatten and Ros,\n2019, Ehrmanntraut et al., 2021]. Moreover, it has been shown that such word embeddings may\nalso be used as a data-driven means of revealing gender bias in textual material [e.g. Wevers,\n2019], mapping character relations in novels [e.g. Grayson et al., 2016], and, when applied to\ndiachronic text collections, detecting changes in word meaning over time [Sagi et al., 2011,\nTahmasebi et al., 2018, Kutuzov et al., 2018, Sommerauer and Fokkens, 2019, Marjanen et al.,\nJournal of Data Mining and Digital Humanities\nISSN 2416-5999, an open-access journal\n1 http://jdmdh.episciences.org\n2019, Martinez-Ortiz et al., 2019]. Furthermore, for research questions where word polysemy\n(when a word has multiple related senses, e.g. foot ‘body part’ and ‘base/lowermost part’) and\nhomonymy (when a character string has multiple, unrelated meanings, e.g. bark ‘outer layer\nof tree trunk’, ‘sound made by dog’) could create methodological issues, researchers have also\nbeen catered to by models equipped to create contextualized vector representations for individual\nword tokens. Such token-based models have proven helpful in word-level tasks, such as sense\ndisambiguation [Fonteyn, 2020, Beelen et al., 2021], Named-Entity Recognition Labusch et al.\n[2019], Konle and Jannidis [2020], Schweter and Baiter [2019], Schweter and M ¨arz [2020],\nEhrmann et al. [2020a], Boros et al. [2020], Brandsen et al. [2021], Ehrmann et al. [2021], and\nthe automated detection of semantic narrowing (i.e. when a word loses one or more senses\nand becomes more restricted in it usage) or broadening (i.e. when a word gains one or more\nnew senses and becomes more varied in its usage) [Sagi et al., 2011, Giulianelli et al., 2020].\nMoreover, these models can easily be adapted to perform a plethora of higher-level downstream\ntasks with great accuracy, including automated text classification [Adhikari et al., 2019, Jiang\net al., 2021], text segmentation [Pagel et al., 2021] or event detection [Sims et al., 2019], to name\na few.\nIn part, the appeal of semantic vector representation algorithms lies in their potential to automate\nsemantic (rather than formal) data retrieval and annotation, which helps increase the amount of\ndata that can be processed by researchers. At the same time, the application of large language\nmodels in humanities research – and in particular, humanities research that focuses on the inter-\npretation and analysis of historical text – may also offer a more objective means to analyse textual\ndata [e.g. Sagi et al., 2011]. Researchers who interpret and analyse historical textual material\nare well-aware that the interpretation of historical textual material must not be approached with\npresent-day intuitions [Tahmasebi and Risse, 2017]: because there are substantial differences\nbetween the way in which concepts and discourses of class, gender, norms and prestige are\nlinguistically represented in different time periods, present-day intuitive judgments of historical\nlanguage are likely to lead to inaccurate, ‘anachronistic’ interpretations of the data. A method-\nological set-up where a computational language model ‘substitutes’ the manual involvement of\nthe present-day analyst, then, is an attractive approach, as it helps minimize (or even eliminate)\nsuch potentially biased, intuitive judgments in the process of data annotation and analysis.\nOf course, processing historical text poses a series of challenges for vector representation\nalgorithms, but these may be overcome by large token-based language models. Historical text\ninvolves, for instance, high degrees of orthographic variation. This is especially true in the case\nof Western European languages, which only acquired their modern spelling standards roughly\naround the 18th or 19th centuries. This introduces a ‘layer of variation’ that type-based model\nwill struggle with: an algorithm that produces word vectors for each unique string of letters will\nnot conflate the contextual distribution of remembring and remembering, despite the fact that\nthese are spelling variants of the same word type. Secondly, historical text requires digitization\nbefore it can be processed by computational means, but current OCR and HTR technology is\nerror-prone, and manual correction is costly. As a result, digitized historical text often involves\nan additional layer of variation, which, in contrast to orthographic variation, is characterized by\nnear-random distributions (and hence difficult to reduce). Yet, due to the enhanced capacity to\nleverage context of large, token-based language models, they should be able to abstract over the\nmentioned layers of variation and produce more accurate semantic representations than their\ntype-based counterparts.\nFurthermore, the incipient paradigm-shift associated with the dawn of large language models\nJournal of Data Mining and Digital Humanities\nISSN 2416-5999, an open-access journal\n2 http://jdmdh.episciences.org\nhas also proven appealing because it can bring infrastructural advantages to less tech-savvy\ncommunities. The new paradigm – known as the ‘pre-train-then-fine-tune’ paradigm – involves\nfirst preparing a language model by means of ‘pre-training’ on a large background collection of\ntext, and then ‘fine-tuning’ this language model in order to perform a particular task. Pre-training\ntypically involves very large collections but no annotation, and once pre-trained, fine-tuning\nnecessitates a manually labeled dataset exemplifying the target task. Importantly, thanks to their\nhigh “sample efficiency” [Kaplan et al., 2020], large language models can achieve significant\nperformance on the basis of comparatively smaller training datasets than those required by other\nMachine Learning architectures. This allows for an advantageous collaboration model, in which\ntool developers focus on producing high quality language models, and humanities researchers\nfocus on creating annotations for the desired task, on which these language models can be\nfine-tuned following standard procedures.\nStill, despite these positive notes, researchers who want to call upon these language models to\ntarget historical text will face practical hurdles. In particular, the training of large models such as\nBERT is a costly endeavour, both in terms of computational resources, as well as the substantial\namounts of training data it requires. For historical data, digitized corpora are often exhaustive, but\nstill small. As a result, past work with historical data has resorted to employing pre-fab, present-\nday language models [Giulianelli et al., 2020, Hosseini et al., 2021a], which are occasionally\nadapted for historical usage by further pre-training on historical datasets. Unfortunately, such a\nset-up may be problematic for several reasons.\nFirst, current large language model implementations rely on tokenization procedures – like\nByte-Pair-Encoding (BPE) [Gage, 1994, Sennrich et al., 2016] or WordPiece [Schuster and\nNakajima, 2012] – that break down character sequences into so-called sub-word tokens, optimiz-\ning a particular information-theoretic measure. Originally, the motivation for this tokenization\napproach was tackling the out-of-vocabulary word problem. For any given character sequence\nthat was not included in the training set, traditional approaches would struggle to generate a\nvector representation, given that no vector was assigned to it in the original vector space. With\nthe current tokenization approach, a vector representation is computed through composition of\nthe vector representation of the sub-word tokens into which the original string is decomposed.\nHowever, since the adaptation of a pre-existing model implies a tokenizer that has been optimized\non present-day language data, the application of such a model on historical data may result in\nuninformative sub-word tokenizations and ultimately in out-of-domain sequences of sub-word\ntokens for which the model can only generate low quality vector representations.\nSecond, it could be problematic to employ a model trained on data that may import the prob-\nlematic, anachronistic biases towards grammar and semantics that researchers are trying to\navoid. In fact, some classification error analyses of historical sense disambiguation tasks indicate\nthat present-day language models indeed erroneously impose a present-day interpretation onto\nthe historical material [e.g. Fonteyn, 2020]. Intuitively, one would expect that pre-training on\npresent-day data (for which there is no issue of sparsity) would provide an advantageous starting\npoint, assuming that large parts of the core grammar and lexical semantics of a language have\nstayed constant over time. However, the reservations on imported biases and the aforementioned\nfixed tokenization schemes may run counter to any observations that present the adaptation of\npre-existing models as the more data-efficient alternative.\nIn this paper, we investigate which of the two strategies is bound to produce higher quality vector\nrepresentations: (i) pre-training from scratch or (ii) adapting a pre-existing model. To this end,\nwe extend the experiments of previous work on pre-training MacBERTh – a large historical\nJournal of Data Mining and Digital Humanities\nISSN 2416-5999, an open-access journal\n3 http://jdmdh.episciences.org\nlanguage model for English [Manjavacas and Fonteyn, 2021]. More specifically, we compare the\nperformance of a number of models representative of both methods (pre-training vs. adaptation)\non a number of downstream tasks. Our experiments are consistent with previous results, which\nhighlight that pre-training from scratch may be a better strategy. The results also suggest that\nthe fixed tokenization models that are currently in vogue may be a bottleneck in the process of\nsuccessfully adapting large language models.\nII MODEL OVERVIEW\nWhile there is a variety of model architectures that could be used for historical NLP, the present\nstudy relies on BERT, a stack of transformer layers with a self-attention mechanism [Vaswani\net al., 2017] that optimize a Masked Language Model (MLM) objective [Devlin et al., 2019].\nDespite the existence of several MLM alternatives, our choice to work with BERT is motivated by\nthe fact that (i) it is well-established and thoroughly studied, and that (ii) the on-going evaluation\nof alternative choices—mostly focused on Natural Language Understanding (NLU) tasks—has\nnot yielded a clearly superior architecture and (iii) experimenting with alternative architectures\ninvolves a multiplicative cost factor on the pre-training and fine-tuning part of the experiments\npresented in the current study, which would, unfortunately, surpass the available budget.\nIn order to quantify the relative advantage of the alternative pre-training methods for historical\nlanguage, we compare the following instantiations of BERT.\nThe first BERT model we consider is trained on present-day Englishdata only, corresponding\nto “BERT-Base Uncased” in the original repository. This model, to which we will henceforth\nrefer as BERT, is trained on ca. 3.3B tokens—i.e. the BookCorpus [Zhu et al., 2015] and the\nEnglish Wikipedia—using a WordPiece [Schuster and Nakajima, 2012] vocabulary of 30,000.\nSecond, we consider two variants of BERT—i.e. “BERT-Base Uncased”—which are subse-\nquently adapted by further pre-training on historical English data. The first ‘historically\nadapted’ model we consider has been fine-tuned at the Alan Turing Institute on 5.1B tokens of\nhistorical English text published between 1760 and 1900 [Hosseini et al., 2021a] .1 We will refer\nto this model as TuringBERT.\nConsidering the relatively limited time span covered byTuringBERT, we also created a second\nadapted model, which we will refer to as BERT-Adapted. This model also corresponds to\nan instantiation of “BERT-Base Uncased”, but, in this case, it is further pre-trained on the\nsame historical collection that served as the basis for developing MacBERTh [Manjavacas\nand Fonteyn, 2021]. This collection contains a large sample of English text covering a time\nspan from 1473 to 1950, and includes the Early English Books Online (EEBO) corpus (1473-\n1700), the Evans Early American Imprints Collection (EV ANS; 1639-1800), Eighteenth Century\nCollections Online (ECCO; 1701-1800), the Corpus of Late Modern English Texts (CLMET3.1;\n1710-1920), the Hansard corpus (Hansard; 1803-1950), and the Corpus of Historical American\nEnglish (COHA; 1810-1950). The resulting corpus has a total size of ca. 3.9B (tokenized)\nwords, covering a varied range of text types, including literary works, religious and legal text,\nparliamentary debate transcriptions, as well as news reports and magazine articles. The pre-\nprocessing procedure involved the removal of foreign text, for which we used an ensemble\nof the Google’s Compact Language Identifier (v3) and the FastText Language Identification\nsystem [Grave, 2017], operating over chunks of 500 characters, which were flagged as foreign\nwhenever both systems indicated a language other than English as the highest probability\n1The model is available through the accompanying online repository [Hosseini et al., 2021b].\nJournal of Data Mining and Digital Humanities\nISSN 2416-5999, an open-access journal\n4 http://jdmdh.episciences.org\nlanguage. Subsequently, the text was split into sentences using the NLTK built-in sentence\ntokenizer [Bird, 2006].\nFinally, we consider the performance of the Present-day and historically adapted models in light\nof the historically pre-trainedmodel MacBERTh [Manjavacas and Fonteyn, 2021]. For the\ncreation of MacBERTh, we relied on the seminal implementation of BERT, 2 with the hyper-\nparameterization corresponding to the “BERT-base Uncased” architecture.3 Pre-training was\ndone with default parameters, except for the maximum sequence length (set to 128 subtokens)\nfor 1,000,000 training steps. A summary of all compared models can be found in Table 1.4\nModel Source Historical Adapted Training Data Time Span V ocabulary\nBERT BERT-base Uncased ✗ 3.3B 30,000\nTuringBERT BERT-base Uncased ✓ ✓ 5.1B 1760-1900 30,000\nBERT-AdaptedBERT-base Uncased ✓ ✓ 3.9B 1450-1950 30,000\nMacBERTh ✓ ✗ 3.9B 1450-1950 30,000\nTable 1: Overview of all the models involved in the present experiments.\nIII EXPERIMENTS\nIn order to assess the relative merit of the alternative approaches, we put the four competing\nmodels through a set of downstream evaluation tasks. These tasks were selected on the basis of\ntheir relevance for historical text processing.\n3.1 Part-Of-Speech Tagging\nThe first task we consider is Part-Of-Speech (POS) Tagging for Historical English. A particularly\nsuited dataset for our evaluation purposes is the Penn-Helsinki Parsed Corpus of Early Modern\nEnglish (PPCEME) Kroch et al. [2004]. The PPCEME consists of a collection of Early Modern\nEnglish letters (time span: 1450-1700) that have been annotated manually with morphological\nand syntactic information. The collection is divided in 448 individual documents, and comprises\napproximately 1.7M words.\nIn order to accomplish POS-tagging, a language model is fine-tuned to perform token-level\npredictions over the input sequence. For each input token, the vector representation for that token\nis used as features in order to perform classification over the set of possible output POS-tags.5\nFor all experiments, we replicate the training and test splits from Han and Eisenstein [2019],\nwhich reserve a total of 115 files for testing and from the remaining 333 uses 316 for training\nand 17 randomly sampled files (ca. 5%) for development.\nOne of the expected advantages of robust pre-trained language models is their so-called “sample\nefficiency” – or the ability to generalize from comparatively smaller amounts of training data. In\norder to compare the candidate models from this perspective, we also run a series of experiments\nin which we increase the size of the training data, starting at just 50 files up until we reach the\nfull training set of 316 files. For evaluation purposes, we compute accuracy for over all tokens,\n2Available on the following URL: https://github.com/google-research/bert.\n3See the original paper [Devlin et al., 2019] for a description of these parameters.\n4MacBERTh itself is available through the HuggingFace hub: https://huggingface.co/\nemanjavacas/MacBERTh.\n5Due to sub-word tokenization, several vector representations may be available for a single input token if this\nhas been split. In those cases, we follow a strategy that ignores all but the first sub-word token.\nJournal of Data Mining and Digital Humanities\nISSN 2416-5999, an open-access journal\n5 http://jdmdh.episciences.org\n100 200 300\ndataset\n0.97\n0.98Accuracy\nEvaluation = total\n100 200 300\nTraining data size\n0.97\n0.98\nEvaluation = known\n100 200 300\ndataset\n0.84\n0.86\n0.88\nEvaluation = unknown\nBERT BERT-Adapted MacBERTh TuringBERT\nFigure 1: Line plots assessing the sample efficiency of the different candidates. The x-axis represents the\nnumber of training files, and the y-axis represents the accuracy. The evaluation is further divided into\ntotal, known and unknown tokens, depending on whether input tokens were seen during training or not.\ntokens that were seen during training (known), and tokens that were not seen during training\n(unknown).\nFigure 1 shows the results of this experiment. Here, MacBERTh shows peak performance across\nall conditions, with the difference being larger in the lower data regime. TuringBERT follows\nup and equals MacBERTh in higher data regimes (starting with 100 files in the training set).\nFurther down the ranking, we find BERT-Adapted, which overall shows lower performance\nthan the other adapted modelTuringBERT – except in the lower data regimes when considering\nunknown tokens. Finally, BERT-Adapted surpasses its non-adapted variant BERT, except for\nthe higher data regimes. It is interesting to note thatBERT-Adapted shows considerably worse\nperformance than TuringBERT. As noted in the introduction, these two models were adapted\nfrom the same present-day model, and differ only on the underlying historical dataset used\nfor pre-training. However, in light of the performance obtained by MacBERTh, the historical\npre-training dataset underlying BERT-Adapted would be expected to provide a stronger model\n– but this does not seem to be the case.\nFinally, we inspect the performance of the different models as a function of the the period from\nwhich the target sentences stem. In order to simplify the visualization, we compute accuracy\noffsets of the different models with respect to the BERT baseline. The results are shown in\nFigure 2.\nAgain, the largest performance advantages of MacBERTh are located in the lower training data\nregimes. Factoring in the period of the target sentence, we also observe that the advantage is\nnot restricted to the time periods to which only MacBERTh and BERT-Adapted had access\n– i.e. the earlier periods – but also appears in the later periods. Furthermore, it appears that\nthe adapted models, TuringBERT and BERT-Adapted, behave in a largely similar manner.\nHowever, their behaviour deviates in the larger training data regimes, whereBERT-Adapted’s\nperformance dips considerably, eventually underperforming even the present-day BERT baseline\non known tokens.\n3.2 Named Entity Recognition\nThe second task we approach is Named Entity Recognition (NER) in Historical texts. We use\nthe dataset provided for the second iteration of the CLEF-HIPE (Named Entity Processing in\nHistorical Newspapers) shared task [Ehrmann et al., 2020b]. For this iteration, the organizers\nproposed two major tasks – Named Entity Recognition and Classification (NERC) and Entity\nLinking (EL) [Ehrmann et al., 2022] – covering 5 languages across 6 datasets.\nJournal of Data Mining and Digital Humanities\nISSN 2416-5999, an open-access journal\n6 http://jdmdh.episciences.org\n1500 1550 1600 1650 1700\n0.005\n0.000\n0.005\n0.010\n0.015\n0.020\nDifference\ntotal\n50\n1500 1550 1600 1650 1700\n100\n1500 1550 1600 1650 1700\n150\n1500 1550 1600 1650 1700\n200\n1500 1550 1600 1650 1700\n316\n1500 1550 1600 1650 1700\n0.005\n0.000\n0.005\n0.010\n0.015\n0.020\nDifference\nknown\n1500 1550 1600 1650 1700 1500 1550 1600 1650 1700 1500 1550 1600 1650 1700 1500 1550 1600 1650 1700\n1500 1550 1600 1650 1700\nYear\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10Difference\nunknown\n1500 1550 1600 1650 1700\nYear\n1500 1550 1600 1650 1700\nYear\n1500 1550 1600 1650 1700\nYear\n1500 1550 1600 1650 1700\nYear\n0\n20000\n40000\n60000\n80000\n100000\n0\n20000\n40000\n60000\n80000\n0\n1000\n2000\n3000\n4000\n5000\nvs-TuringBERT vs-MacBERTh vs-BERT-Adapted\nFigure 2: Difference in part-of-speech tagging accuracy in known, unknown and all tokens of all historical\nmodels with respect to the present day baseline BERT, across different sizes of training datasets.\nMacBERTh TuringBERTBERT-Adapted BERT\nmodel\n0.77\n0.78\n0.79\n0.80\n0.81\n0.82f1\nFigure 3: Evaluation in terms of F1-score on the\nHIPE2022 dataset for Named Entity Recognition\nin English.\n<1840 >=1840\nperiod\n0.725\n0.750\n0.775\n0.800\n0.825\n0.850\n0.875\n0.900f1\nmodel\nBERT\nBERT-Adapted\nMacBERTh\nTuringBERT\nFigure 4: Evaluation in terms of F1-score on the\nHIPE2022 dataset for Named Entity Recognition\nin English. Results are split into an earlier and\na later period taking 1840 as the median date of\nthe texts in the dataset.\nJournal of Data Mining and Digital Humanities\nISSN 2416-5999, an open-access journal\n7 http://jdmdh.episciences.org\nWe focused on the training and development splits from thetopres19th subset. This subset\nconsists of British Library newspapers from the 18th and 19th centuries, and the named entities\ncorrespond exclusively to geographical locations Ardanuy et al. [2022]. A total of approximately\n3,300 entities are annotated, with 236 being reserved for development. We fine-tune the different\nmodels to perform token-level classification over sentences, with a total of 5,874 sentences for\ntraining and 646 for development. We fine-tune each model over 5 epochs and perform a total of\n10 fine-tuning rounds per model in order to take into account random variation in the training\nprocedure.\nFigure 3 and Figure 4 show the results of this experiment. We report F1-score per model on the\nentire dataset (Figure 3) as well as the F1-score per model for ‘earlier’ and ‘later’ texts, taking\nthe median year of the sentences as reference point (1840; see Figure 4). The difference in\nperformance between the models is relatively small for NER. Overall, it appears TuringBERT\nis likely to outperform BERT-Adapted and possibly MacBERTh too. It should be noted,\nhowever, that the absolute difference in F1-scores is negligible. Furthermore, it is possible that\nTuringBERT’s marginally superior performance is due to the fact that the NER data stems\nfrom the same collection as the pre-training data for TuringBERT. Thus, TuringBERT may\nbe able to produce better features for entities that it has already processed in the pre-training\nphase. Finally, we observe that TuringBERT loses its marginal advantage when applied to\nearlier texts in the dataset (for which the results are overall worse for all models).\n3.3 Word Sense Disambiguation\nThe next task we tackle is Word Sense Disambiguation (WSD), which we approach from a\ndiachronic angle. For this – and the following – tasks, we rely on a custom evaluation dataset\nextracted from the Oxford English Dictionary [OED Simpson and Weiner, 1989]. With its large\nreservoir of sense distinctions, categorizations and exemplifications, the OED is an authoritative\nresource for historical and contemporary lexical semantics in English. We refer to Manjavacas\nand Fonteyn [2021] for the details on the compilation of the evaluation dataset used for the\npresent experiments. In total, we evaluate the four candidate models on historical WSD from\ntwo distinct angles.\n3.3.1 Non-Parametric Word Sense Disambiguation\nThe first historical WSD setting involves no fine-tuning, and relies entirely on vector similarity\nmetrics to assign a target word in a given context to its corresponding sense. For a given historical\ninput sentence like “They must haue houses warme, as your Pigions haue, crossed through with\nsmall Pearches”, exemplifying a sense of the word “cross”, we compute the contextualized\nvector representation of “cross” and measure its similarity to abstract vector representations of\nthe different senses of the word “cross”. These abstract representations are computed as sense\ncentroids, by averaging over the vector representations of the different exemplifications of a\ngiven sense in the OED dataset.\nWe evaluate the models using a total of 191 OED lemmata. These test lemmata contain all at least\n50 example sentences and at least two different senses (the minimum number of senses required\nto perform word sense disambiguation). The resulting dataset consists of 17,878 sentences,\nwhich we split into a training and a test set, proportional to the number of sentences per sense.\nThe training set is used to estimate the sense centroids. The test set is used to estimate the\naccuracy of this method for each of the language models.\nJournal of Data Mining and Digital Humanities\nISSN 2416-5999, an open-access journal\n8 http://jdmdh.episciences.org\n1400 1500 1600 1700 1800 1900\nperiod\n0.4\n0.6\n0.8Accuracy\ntype = content\n1400 1500 1600 1700 1800 1900\nYear\ntype = function\nBERT\nBERT-Adapted\nMacBERTh\nTuringBERT\nMajority\nRandom\nFigure 5: Results in terms of accuracy of the non-parametric WSD evaluation, differentiating between\ncontent and function words. On the x-axis, we aggregate over the time period of the corresponding test\nsentences. For comparison, a random and a majority baselines have been added.\n1 2 5 10 15 25 full\nTraining examples\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70Accuracy\nBERT\nBERT-Adapted\nMacBERTh\nTuringBERT\nMajority\nRandom\nFigure 6: Results in terms of accuracy of the non-\nparametric WSD evaluation taking into account the\nnumber of sentences per sense in the training set. Ma-\njority and random baselines are added for reference.\nFigure 5 shows the results of the experiment,\ndifferentiating between lemmata that belong\nto content words – i.e. nouns (e.g. dog, sky),\nadjectives (e.g. nice, beautiful) and verbs (e.g.\njump, think) – and those that belong to func-\ntion words – i.e. prepositions (e.g. in, about),\npronouns (e.g. me, who), conjunctions (e.g.\nand, since) and interjections (e.g. hey). The\nreason why we differentiate between these two\ngroups is that lexical or ‘contentful’ semantics\ncan be considered distinct from grammatical\nor ‘function’ semantics, both in terms of how\nsenses can be derived from contextual informa-\ntion (with the context of function words being\nmore diverse than that of content words), and\nin terms of the diachronic dynamics [Hamilton\net al., 2016].\nFocusing on content words, we observe that all models perform above baseline level across all\ntime periods, with MacBERTh outperforming the other models. Furthermore, BERT-Adapted\nhas an advantage over the non-adapted variantBERT up until the 18th century, where we observe\na leap in performance with respect to the previous periods and a convergence of all models,\nindicating that the advantages of historical adaptation may be limited to the earlier periods.\nWhen considering function words, MacBERTh again seems to have an advantage – except in the\nearliest period and in the post-18thcentury data. Beyond this, no clear patterns can be observed.\nThe effectiveness of this method is largely conditioned by the quantity and quality of the sense\ncentroids. As we took an even split between training and test sets, we assumed that sense\ncentroids can be estimated on data as large as the target data of interest. This is, however,\nunrealistic: in real-word scenarios, it is often the case that the amount of available labeled data –\ni.e. in this case, the number of example sentences per sense – is much smaller. In order to test\nwhether models diverge in their requirements for training data, we ran an experiment in which\nwe limit the number of sentences per sense to several values by sampling the target number of\nJournal of Data Mining and Digital Humanities\nISSN 2416-5999, an open-access journal\n9 http://jdmdh.episciences.org\nsentences 20 times. This gives us an indication of not only the amount of data required, but also\nof the dependency on the specific sentences in the training set for achieving strong performance.\nFigure 6 shows the results of this experiment. For each target number of sentences on the x-axis,\nthe y-axis shows the mean accuracy and its dispersion for each of the alternative models. Notably,\nall models are very robust against variation in the sentences used for estimating the centroids.\nAs a result, there is very little variance in the obtained accuracy scores – i.e. there is almost\nno dispersion around the mean line –, even in the smaller data regimes. When examining the\nscores, we see that MacBERTh outperforms the other candidate models in all conditions, and is\nalso able to outperform the strongest baseline – i.e. the majority baseline – with just 2 sentences\nper sense. After 10 sentences – i.e. approximately a fifth of the average number of sentences\nper sense in the dataset –, models start to converge to their optimal performance on this dataset.\nAgain, BERT-Adapted shows a slight improvement over BERT, and TuringBERT performs\nin par with the non-historical model BERT.\n3.3.2 Word-in-Context\nThe second approach to historical WSD reformulates the task as a binary task [Pilehvar and\nCamacho-Collados, 2019] (see also [Beelen et al., 2021], where the task is called “targeted sense\ndisambiguation”). For any given pair of sentences exemplifying senses of the same lemma, we\nfine-tune our models to predict whether the two sentences exemplify the same word sense or\nnot. In order to evaluate following this approach, we carve out a dataset from the OED similar\nto the one described by Manjavacas and Fonteyn [2021]. This dataset covers 416 lemmata and\n84,712 sentences, from which we generate positive and negative pairs in the following manner:\nfor each sentence in the dataset, we first sample a positive example from the set of sentences\nexemplifying the same sense. Subsequently, we sample a different sense belonging to the same\nlemma, and from the set of sentences illustrating that sense, we sample one negative example.\nIn order to fine-tune the models, we replicate the settings described in [Devlin et al., 2019,\nSection 4.1], using the last hidden activation corresponding to the [CLS] token, adding a linear\nprojection layer in order to compute the probabilities that the sentences belong to the same sense,\noptimizing a cross entropy loss. In order to let the model focus on the word that corresponds to\nthe underlying lemma, we add [TGT] tokens around the focus word in both members of the\ninput pair.6 This is exemplified by the sentences shown in Table 2.\nThe results of this experiment are shown in Figure 7, where we report accuracy numbers based\non the centuries from which the left and right sentences stem (shown respectively on the y-axis\nand x-axis.) For each bin, we show the accuracy achieved by each model. Overall, the results\nare high, ranging from 85% to 98%. The plots highlight that the most difficult examples stem\nfrom the 15th to 17th centuries. In these bins, both MacBERTh and BERT-Adapted have an\nadvantage.\nIn order to highlight the relative advantage of MacBERTh and BERT-Adapted on this task,\nFigure 8 and Figure 9 show, respectively, the differences in performance of MacBERTh and\nBERT-Adapted in comparison to the alternative models. Overall,MacBERTh outperforms the\nother models, with a larger performance difference in the earlier bins – i.e. the bottom-left part\nof the plot. The differences are surprisingly large when comparing with TuringBERT across\nall time periods. Moreover, BERT-Adapted has an advantage over BERT, especially when\nconsidering the earlier periods. This highlights the effectiveness of adapting present-day language\n6We use the “sbert” library Reimers and Gurevych [2019] to fine-tune the models, training for 5 epochs with\nbatch size of 16 on a single GPU.\nJournal of Data Mining and Digital Humanities\nISSN 2416-5999, an open-access journal\n10 http://jdmdh.episciences.org\nLeft Quotation Right Quotation\nExample He lov’d his Country with too unskilful\na tenderness.\nI love it to be grieved when he hideth\nhis smiles.\nInput He [TGT] lov’d [TGT] his Country with\ntoo unskilful a tenderness.\nI [TGT] love [TGT] it to be grieved\nwhen he hideth his smiles.\nSense 1.a “To have or feel love towards (a per-\nson, a thing personified) (for a quality\nor attribute); to entertain a great affec-\ntion, fondness, or regard for; to hold\ndear. ”\n3.c “With direct object and infinitive or\nclause: to desire or like (something to\nbe done). Also (chiefly U.S.) with for\npreceding the notional subject of the in-\nfinitive clause. ”\nTable 2: An example negative pair for lemma ‘love’ showcasing the modification in order to fine-tune the\nmodel.\n140015001600170018001900\n1900\n1800\n1700\n1600\n1500\n1400\n0.96 0.93 0.93 0.95 0.91 0.94\n0.95 0.93 0.93 0.93 0.94 0.95\n0.95 0.92 0.92 0.95 0.95 0.97\n0.9 0.88 0.91 0.94 0.95 0.95\n0.88 0.87 0.9 0.93 0.92 0.97\n0.89 0.85 0.88 0.93 0.96 0.93\nBERT\n140015001600170018001900\n0.97 0.94 0.92 0.95 0.92 0.94\n0.96 0.93 0.93 0.94 0.95 0.96\n0.95 0.93 0.92 0.96 0.95 0.97\n0.93 0.9 0.92 0.95 0.95 0.96\n0.88 0.91 0.91 0.94 0.94 0.96\n0.91 0.86 0.9 0.93 0.93 0.98\nBERT-Adapted\n140015001600170018001900\n0.98 0.94 0.93 0.96 0.93 0.94\n0.97 0.94 0.95 0.95 0.95 0.96\n0.96 0.94 0.94 0.96 0.96 0.98\n0.92 0.92 0.93 0.95 0.96 0.96\n0.91 0.92 0.93 0.95 0.95 0.96\n0.88 0.92 0.93 0.96 0.95 0.95\nMacBERTh\n140015001600170018001900\n0.98 0.91 0.91 0.93 0.91 0.92\n0.95 0.92 0.92 0.93 0.93 0.94\n0.95 0.92 0.91 0.94 0.94 0.97\n0.89 0.88 0.9 0.93 0.95 0.94\n0.85 0.89 0.9 0.93 0.94 0.96\n0.91 0.86 0.83 0.95 0.93 0.94\nT uringBERT\nFigure 7: Accuracy in the Word-in-Context WSD task by period of the left and right examples shown\nrespectively in the y-axis and the x-axis. The color matches the accuracy in the corresponding bins.\n1450150015501600165017001750180018501900\n1450\n1500\n1550\n1600\n1650\n1700\n1750\n1800\n1850\n1900\nMacBERTh vs BERT\n1450150015501600165017001750180018501900\nMacBERTh vs BERT-Adapted\n1450150015501600165017001750180018501900\nMacBERTh vs TuringBERT\n0.10\n0.05\n0.00\n0.05\n0.10\nFigure 8: Circle plot showing the difference in accuracy between MacBERTh and the alternative models\nper period. The size of the circles correspond to the number of predictions in disagreement between the\ncompared models. The color corresponds to the difference in accuracy.\nmodels for targeted WSD. Still, the plots show that MacBERTh outperforms BERT-Adapted\nacross all time periods, which offers an indication that pre-training from scratch is a stronger\nmethod than adaptation.\n3.4 Fill-In-The-Blank\nThe next downstream task approaches Natural Language Understanding using a fill-in-the-blank\nevaluation scheme. This task does not require fine-tuning. Instead, we rely on the dataset of\nsense-exemplified quotations from the OED, and poll each model for the underlying lemma\nthat the sentence is exemplifying after masking the word that corresponds to that lemma. The\nJournal of Data Mining and Digital Humanities\nISSN 2416-5999, an open-access journal\n11 http://jdmdh.episciences.org\n1450150015501600165017001750180018501900\n1450\n1500\n1550\n1600\n1650\n1700\n1750\n1800\n1850\n1900\nBERT-Adapted vs BERT\n1450150015501600165017001750180018501900\nBERT-Adapted vs MacBERTh\n1450150015501600165017001750180018501900\nBERT-Adapted vs TuringBERT\n0.10\n0.05\n0.00\n0.05\n0.10\nFigure 9: Circle plot showing the difference in accuracy between BERT-Adapted and the alternative\nmodels per period. The size of the circles correspond to the number of predictions in disagreement\nbetween the compared models. The color corresponds to the difference in accuracy.\nmodels, thus, need to gather as much information as possible from the context in order to produce\naccurate guesses. Importantly, since the sentences are chosen in order to illustrate a particular\nusage of the given word, we can assume that they contain – otherwise the task would become\nartificially difficult, and less informative for benchmarking purposes.\nFollowing the original masking loss [Devlin et al., 2019], a language model outputs a probability\ndistribution over the model’s own vocabulary for each masked word in the input. In order to\nassess the plausibility that each model assigns to the true target word, we compute its rank in\nthis probability distribution – i.e. we do not use the probability itself, since this quantity would\nbe difficult to compare due to differing vocabularies. For similar reasons, we restrict ourselves\nto sentences in which the target word is not sub-tokenized by any of the models. The resulting\ndataset comprises 42,961 sentences, covering 731 different words (with an average of 58.8\nsentences per word.)\nIn order to summarize the model performance, we compute the Mean Reciprocal Rank (MRR),\nwhich in this case corresponds to averaging over the inverse of the individual ranks. Again,\nwe factor the time dimension into the evaluation, binning the results into spans of 50 years.\nFigure 10 shows the results of this experiment. Here, MacBERTh has an advantage across all\nperiods, except in the most recent bin – where all models seem to converge. BERT-Adapted\nalso improves over the non-adapted variant (BERT), except for the 19th century.\n3.5 Sentence Periodization\nFinally, we submit the models to a sentence periodization task. For a given input sentence, we fine-\ntune the models to make predictions about the year in which they were written. Algorithmically,\nwe approach this task using a two-step setup.7 First, we fine-tune the language models to perform\na binary task in which the goal is to predict whether the first of two sentences stems from a\nlater period than the second. Then, in order to predict the year of a given input sentence, we\nrun the binary classifier comparing the input sentence with each of the sentences in a separate\ncorpus – the background corpus. This background corpus has been sampled so as to have an\neven distribution of sentences over time periods, and consists of 5,000 sentences. Finally, we\nuse the individual predictions comparing the input sentence to the sentences in afore-mentioned\nbackground corpus in order to construct a single year prediction over the entire range of years in\nthe background corpus. The latter step uses the cumulative distribution of predictions and the\n7Our first attempts involved using an ordinal regression on top of language model sentence embeddings, but\nresults were not informative.\nJournal of Data Mining and Digital Humanities\nISSN 2416-5999, an open-access journal\n12 http://jdmdh.episciences.org\n1500 1600 1700 1800 1900\nYear\n0.1\n0.2Mean Reciprocal Rank\nBERT\nBERT-Adapted\nMacBERTh\nTuringBERT\n0\n5000\nFigure 10: Results of the fill-in-the-blank task\nover time in terms of Mean Reciprocal Rank.\nOverlayed are the number of sentences included\nin each bin.\n20 40 60 80 100\nSamples per Bin\n50\n60\n70\n80\n90\n100Mean Absolute Error\nBERT\nBERT-Adapted\nMacBERTh\nTuringBERT\nFigure 11: Mean Absolute Error (on the y-axis)\non the sentence periodization task using back-\nground corpora of increasingly larger sizes (on\nthe x-axis).\nknee method for finding the cutoff point in this distribution [Satopaa et al., 2011] – we refer to\nthe original paper for more details on this method [Manjavacas and Fonteyn, 2021].\nWe train the models using the cross-encoder implementation provided by “sbert” [Reimers and\nGurevych, 2019]. The training data consists of 100,000 sentence pairs sampled from the OED\ndataset. The test set is constructed in a similar way, comprising a total of 5,000 sentence pairs.\nThe performance of this method depends on the size of the background corpus. In order to\nquantify this dependency, we ran a first experiment where we computed the Mean Absolute\nError (MAE) over increasingly larger sub-samples of the background corpus (all sub-samples\nare uniformly sampled over the entire range to ensure that no time spans are over-represented.)\nFor each size, we re-ran the experiment 10 times in order to quantify the dispersion due to the\nbackground corpus sample. Figure 11 shows the results of this experiment. We observe that all\nmodels reach their top performance when using the full background corpus – i.e. a total of 5,000\ninstances with 100 instances per each bin of 50 years – and very small dispersion is present\nstarting with 70 instances per bin. Comparing the models on the full background corpus, we\nobserve that MacBERTh has an advantage of slightly below a mean absolute error of 10 years.\nFigure 12 shows finer-grained results of this experiment, factoring in the time dimension. We\nalso include a baseline that predicts years randomly following the distribution of years in the test\nset. The random baseline reflects the fact that predicting sentences towards the middle of the\nrange results in inherently smaller MAE. In order to remove this artifact from the visualization,\nFigure 13 reports the relative improvement of each model over the random baseline. This\nmodification does not directly affect the comparison between the models, while letting us assess\nwhen the differences between the models are located in easier or more difficult periods.8 Most\nof MacBERTh’s advantage is located in the years starting in 1750. Before that period, the\ndifferences between the models are small, with MacBERTh and BERT-Adapted at the top.\nInterestingly, the results for BERT-Adapted dip towards the later section of the background\ncorpus.\n8Note that in contrast to the original MAE scores, now higher means better – i.e. larger improvement.\nJournal of Data Mining and Digital Humanities\nISSN 2416-5999, an open-access journal\n13 http://jdmdh.episciences.org\n1500 1600 1700 1800 1900\nYear\n50\n100\n150\n200\n250Mean Absolute Error\nBERT\nBERT-Adapted\nT uringBERT\nMacBERTh\nRandom\n0\n500\n1000\n1500\nFigure 12: Mean Absolute Error of the compared\nmodels binned over periods of 50 years. We also\ninclude results for a random baseline.\n1500 1600 1700 1800 1900\nYear\n60\n80\n100\n120\n140Improvement\nBERT\nBERT-Adapted\nMacBERTh\nT uringBERT\n0\n500\n1000\n1500\nFigure 13: Average improvement in Mean Ab-\nsolute Error of the different models over the ran-\ndom baseline.\nIV DISCUSSION & CONCLUSION\nSummarizing over the results of the various experiments, it seems reasonable to state that the\nmost reliable means of making a BERT model suitable for applications to historical text is\nto pre-train a BERT model from scratch on historical corpus data. Overall, the historically\npre-trained model MacBERTh had an advantage over the competitor models in POS-tagging,\nboth types of Word Sense Disambiguation, Filling-In-the-Blank, and the Sentence Periodization\ntasks. The sole exception to these result is the NER evaluation, where TuringBERT performed\nmarginally better on the post-1840 data. Yet, TuringBERT’s advantage in NER may in fact be\ndue to overlap in the evaluation data and the data used to adapt TuringBERT.\nIn certain cases – and particularly in POS-tagging – the advantages of MacBERTh were stronger\nwhen the training data available for fine-tuning was scarce. Since the pre-training dataset of\nMacBERTh covers a larger diachronic window than those of BERT and TuringBERT, it can\nbe assumed that this dataset represents a more varied collection of texts, which could potentially\nexplain the stronger sample efficiency that MacBERTh seems to have. Still, BERT-Adapted –\na model adapted to the same pre-training dataset – does not seem to profit from this diversity\nas much, which may be due to imported biases derived from either the original present-day\npre-training dataset or the tokenizer.\nInterestingly, BERT-Adapted sometimes underperforms in the later periods – this is the case\nfor the Word-in-Context (see Figure 9), Fill-in-the-Blank (see Figure 10) and the Sentence\nPeriodization (see Figure 13) tasks. The cut-off point seems to be at around the 1800s.\nWhile it is interesting in itself to state that there is a difference in the quality of the embeddings\ngenerated by the historically pre-trained model MacBERTh and the historically adapted models\n(TuringBERT), these results do raise the question of what exactly causes this difference. A\nsimilar result was obtained by the development team of the SciBERT model for Biomedical\nNLP [Beltagy et al., 2019]. This model was pre-trained from scratch on a large corpus of research\npapers mined from Semantic Scholar [Ammar et al., 2018] amounting to ca. 3.17B tokens. In this\ninvestigation, SciBERT was compared to BioBERT [Lee et al., 2020], another domain-specific\nmodel for Biomedical NLP that was adapted from BERT to an even larger corpus of articles\nfrom PubMed emcompassing ca. 18B tokens. Their results also highlight that the adapted model\nwas subpar to the model that was pre-trained from scratch, even though the pre-training dataset\nof the later an order of magnitude smaller.\nIn this respect, a hypothesis that would need more thorough testing is whether the current\nJournal of Data Mining and Digital Humanities\nISSN 2416-5999, an open-access journal\n14 http://jdmdh.episciences.org\ntokenization approaches may hinder a successful adaptation of pre-trained model to different\ndomains. Sun et al. [2020] showed that BERT models are brittle in the presence of misspelling,\nespecially when the misspelling resulted in particularly awkward sub-word tokenization. In the\ncase of historical material, Baptiste et al. [2021] have shown that CharBERT [Ma et al., 2020] –\na BERT variant that processes input tokens character by character – produces more robust results\nagainst the presence of variation stemming from OCR noise. More generally, current research\nefforts have focused on producing tokenization-free models, which do not require language (or\ndomain) specific tokenizers and can be thus applied more robustly across languages [Clark et al.,\n2022]. It remains to be tested whether these models have the potential to be efficiently adapted\nto new domains, and whether doing so produces more powerful features than models that are\npre-trained from scratch.\nFinally, it is also important to note that, while the historically adapted models were generally\noutperformed by MacBERTh, both TuringBERT and BERT-Adapted still showed some\nadvantage over the non-adapted, present-day English model BERT. That historical adaptation\n(and adaptation more generally) is still a fruitful undertaking is of great value for the DH\ncommunity: in some cases, adaptation is the only possibility, due to the sparsity of text in the\ntarget domain [e.g. Brandsen et al., 2021].\nACKNOWLEDGMENTS\nThe creation of MacBERTh has been made possible by the Platform Digital Infrastructure (Social\nSciences and Humanities) fund (PDI-SSH). We want to thank the organizers and audience of the\nNLP4DH workshop, and the members of Text mining and Retrieval group (TMRL) from the\nUniversity of Leiden for their valuable feedback on earlier versions of this paper. In particular, we\nthank Hugo de V os for suggesting the addition of a random baseline in the Sentence Periodization\ntask.\nReferences\nAshutosh Adhikari, Achyudh Ram, Raphael Tang, and Jimmy Lin. DocBERT: BERT for Document Classification.\narXiv:1904.08398 [cs], August 2019. URL http://arxiv.org/abs/1904.08398. arXiv: 1904.08398.\nWaleed Ammar, Dirk Groeneveld, Chandra Bhagavatula, Iz Beltagy, Miles Crawford, Doug Downey, Jason\nDunkelberger, Ahmed Elgohary, Sergey Feldman, Vu Ha, Rodney Kinney, Sebastian Kohlmeier, Kyle Lo, Tyler\nMurray, Hsu-Han Ooi, Matthew Peters, Joanna Power, Sam Skjonsberg, Lucy Wang, Chris Wilhelm, Zheng Yuan,\nMadeleine van Zuylen, and Oren Etzioni. Construction of the literature graph in semantic scholar. InProceedings\nof the 2018 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 3 (Industry Papers), pages 84–91, New Orleans - Louisiana, June 2018.\nAssociation for Computational Linguistics. doi: 10.18653/v1/N18-3011. URL https://aclanthology.\norg/N18-3011.\nMariona Coll Ardanuy, David Beavan, Kaspar Beelen, Kasra Hosseini, Jon Lawrence, Katherine McDonough,\nFederico Nanni, Daniel van Strien, and Daniel CS Wilson. A dataset for toponym resolution in nineteenth-century\nenglish newspapers. Journal of Open Humanities Data, 8, 2022.\nBlouin Baptiste, Benoit Favre, Jeremy Auguste, and Christian Henriot. Transferring modern named entity recognition\nto the historical domain: How to take the step? In Workshop on Natural Language Processing for Digital\nHumanities (NLP4DH), 2021.\nKaspar Beelen, Federico Nanni, Mariona Coll Ardanuy, Kasra Hosseini, Giorgia Tolfo, and Barbara McGillivray.\nWhen time makes sense: A historically-aware approach to targeted sense disambiguation. In Findings of the\nAssociation for Computational Linguistics: ACL-IJCNLP 2021, pages 2751–2761. Association for Computational\nLinguistics, 2021. doi: 10.18653/v1/2021.findings-acl.243. URL https://aclanthology.org/2021.\nfindings-acl.243.\nIz Beltagy, Kyle Lo, and Arman Cohan. Scibert: A pretrained language model for scientific text. arXiv preprint\narXiv:1903.10676, 2019.\nJournal of Data Mining and Digital Humanities\nISSN 2416-5999, an open-access journal\n15 http://jdmdh.episciences.org\nSteven Bird. Nltk: the natural language toolkit. In Proceedings of the COLING/ACL 2006 Interactive Presentation\nSessions, pages 69–72, 2006.\nEmanuela Boros, Elvys Linhares Pontes, Luis Adri´an Cabrera-Diego, Ahmed Hamdi, Jos´e Moreno, Nicolas Sid`ere,\nand Antoine Doucet. Robust named entity recognition and linking on historical multilingual documents. In\nConference and Labs of the Evaluation Forum (CLEF 2020), volume 2696, pages 1–17. CEUR-WS Working\nNotes, 2020.\nAlex Brandsen, Suzan Verberne, Karsten Lambers, and Milco Wansleeben. Can BERT Dig It? – Named Entity\nRecognition for Information Retrieval in the Archaeology Domain. arXiv:2106.07742 [cs], June 2021. URL\nhttp://arxiv.org/abs/2106.07742. arXiv: 2106.07742.\nJonathan H. Clark, Dan Garrette, Iulia Turc, and John Wieting. Canine: Pre-training an Efficient Tokenization-Free\nEncoder for Language Representation. Transactions of the Association for Computational Linguistics, 10:73–91,\n01 2022. ISSN 2307-387X. doi: 10.1162/tacl a 00448. URL https://doi.org/10.1162/tacl_a_\n00448.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional\ntransformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short\nPapers), pages 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi:\n10.18653/v1/N19-1423. URL https://www.aclweb.org/anthology/N19-1423.\nMaud Ehrmann, Matteo Romanello, Alex Fl¨uckiger, and Simon Clematide. Overview of CLEF HIPE 2020: Named\nentity recognition and linking on historical newspapers. In Avi Arampatzis, Evangelos Kanoulas, Theodora\nTsikrika, Stefanos Vrochidis, Hideo Joho, Christina Lioma, Carsten Eickhoff, Aur´elie N´ev´eol, Linda Cappellato,\nand Nicola Ferro, editors, Experimental IR Meets Multilinguality, Multimodality, and Interaction, pages 288–310.\nSpringer International Publishing, 2020a. ISBN 978-3-030-58219-7.\nMaud Ehrmann, Matteo Romanello, Alex Fl¨uckiger, and Simon Clematide. Overview of clef hipe 2020: Named\nentity recognition and linking on historical newspapers. In Avi Arampatzis, Evangelos Kanoulas, Theodora\nTsikrika, Stefanos Vrochidis, Hideo Joho, Christina Lioma, Carsten Eickhoff, Aur´elie N´ev´eol, Linda Cappellato,\nand Nicola Ferro, editors, Experimental IR Meets Multilinguality, Multimodality, and Interaction, pages 288–310,\nCham, 2020b. Springer International Publishing. ISBN 978-3-030-58219-7.\nMaud Ehrmann, Ahmed Hamdi, Elvys Linhares Pontes, Matteo Romanello, and Antoine Doucet. Named entity\nrecognition and classification on historical documents: A survey. arXiv preprint arXiv:2109.11406, 2021.\nMaud Ehrmann, Matteo Romanello, Antoine Doucet, and Simon Clematide. HIPE 2022 Shared Task Participation\nGuidelines, February 2022. URL https://doi.org/10.5281/zenodo.6045662.\nAnton Ehrmanntraut, Thora Hagen, Leonard Konle, and Fotis Jannidis. Type- and Token-based Word Embeddings\nin the Digital Humanities. In Proceedings of the Workshop on Computational Humanities Research (CHR\n2021), volume 2989 of CEUR Workshop Proceedings, pages 16–38, 2021. URL http://ceur-ws.org/\nVol-2989/long_paper35.pdf.\nKatrin Erk. Vector Space Models of Word Meaning and Phrase Meaning: A Survey: Vector Space Models of Word\nand Phrase Meaning. Language and Linguistics Compass, 6(10):635–653, October 2012. ISSN 1749818X. doi:\n10.1002/lnco.362. URL https://onlinelibrary.wiley.com/doi/10.1002/lnco.362.\nLauren Fonteyn. What about grammar? Using BERT embeddings to explore functional-semantic shifts of\nsemi-lexical and grammatical constructions. In Proceedings of the Workshop on Computational Humanities\nResearch (CHR 2020), volume 2723 of CEUR Workshop Proceedings, pages 257–268, 2020. URL http:\n//ceur-ws.org/Vol-2723/short15.pdf.\nPhilip Gage. A new algorithm for data compression. C Users Journal, 12(2):23–38, 1994.\nMario Giulianelli, Marco Del Tredici, and Raquel Fern´andez. Analysing lexical semantic change with contextualised\nword representations. In Proceedings of the 58th annual meeting of the association for computational linguistics,\npages 3960–3973, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.\nacl-main.365. URL https://www.aclweb.org/anthology/2020.acl-main.365.\nEdouard Grave. Language Identification · fastText. https://fasttext.cc/blog/2017/10/02/blog-post.html, 2017.\nSiobh´an Grayson, Maria Mulvany, Karen Wade, Gerardine Meaney, and Derek Greene. Novel2vec: Characterising\n19th century fiction via word embeddings. In 24th Irish Conference on Artificial Intelligence and Cognitive\nScience (AICS’16), University College Dublin, Dublin, Ireland, 20-21 September 2016, 2016.\nWilliam L. Hamilton, Jure Leskovec, and Dan Jurafsky. Cultural shift or linguistic drift? Comparing two com-\nputational measures of semantic change. In Proceedings of the 2016 conference on empirical methods in\nnatural language processing, pages 2116–2121, Austin, Texas, November 2016. Association for Computational\nLinguistics. doi: 10.18653/v1/D16-1229. URL https://www.aclweb.org/anthology/D16-1229.\nXiaochuang Han and Jacob Eisenstein. Unsupervised domain adaptation of contextualized embeddings for sequence\nlabeling. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and\nJournal of Data Mining and Digital Humanities\nISSN 2416-5999, an open-access journal\n16 http://jdmdh.episciences.org\nthe 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4238–4248,\nHong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1433.\nURL https://aclanthology.org/D19-1433.\nKasra Hosseini, Kaspar Beelen, Giovanni Colavizza, and Mariona Coll Ardanuy. Neural Language Models for\nNineteenth-Century English. Journal of Open Humanities Data, 7:22, September 2021a. ISSN 2059-481X.\ndoi: 10.5334/johd.48. URL http://openhumanitiesdata.metajnl.com/articles/10.5334/\njohd.48/.\nKasra Hosseini, Kaspar Beelen, Giovanni Colavizza, and Mariona Coll Ardanuy. Neural Language Models for\nNineteenth-Century English (dataset; language model zoo). https://doi.org/10.5281/zenodo.4782245, May 2021b.\nMing Jiang, Yuerong Hu, Glen Worthey, Ryan C Dubnicek, Ted Underwood, and J Stephen Downie. Impact of\nOCR Quality on BERT Embeddings in the Domain Classification of Book Excerpts. In Proceedings of the\nWorkshop on Computational Humanities Research (CHR 2021), volume 2989 of CEUR Workshop Proceedings,\npages 266–279, 2021. URL http://ceur-ws.org/Vol-2989/long_paper43.pdf.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint\narXiv:2001.08361, 2020.\nLeonard Konle and Fotis Jannidis. Domain and Task Adaptive Pretraining for Language Models. In Proceedings\nof the Workshop on Computational Humanities Research (CHR 2020) , volume 2723 of CEUR Workshop\nProceedings, pages 248–256, 2020. URL http://ceur-ws.org/Vol-2723/short33.pdf.\nAnthony Kroch, Beatrice Santorini, and Lauren Delfs. Penn-helsinki parsed corpus of early modern english, 2004.\nAndrey Kutuzov, Lilja Øvrelid, Terrence Szymanski, and Erik Velldal. Diachronic word embeddings and semantic\nshifts: a survey. arXiv preprint arXiv:1806.03537, 2018.\nKai Labusch, Clemens Neudecker, and David Zellh¨ofer. Bert for named entity recognition in contemporary and\nhistorical german. In Proceedings of the 15th Conference on Natural Language Processing, Erlangen, Germany,\npages 8–11, 2019.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. Biobert:\na pre-trained biomedical language representation model for biomedical text mining. Bioinformatics, 36(4):\n1234–1240, 2020.\nAlessandro Lenci. Distributional Models of Word Meaning. Annual Review of Linguistics , 4(1):151–\n171, 2018. doi: 10.1146/annurev-linguistics-030514-125254. URL https://doi.org/10.1146/\nannurev-linguistics-030514-125254 . eprint: https://doi.org/10.1146/annurev-linguistics-030514-\n125254.\nWentao Ma, Yiming Cui, Chenglei Si, Ting Liu, Shijin Wang, and Guoping Hu. Charbert: Character-aware\npre-trained language model. arXiv preprint arXiv:2011.01513, 2020.\nEnrique Manjavacas and Lauren Fonteyn. Macberth: Development and evaluation of a historically pre-trained\nlanguage model for english (1450-1950). In Proceedings of the Workshop on NLP4DH @ ICON 2021, online,\nDecember 2021. NLP Association of India (NLPAI).\nJani Marjanen, Lidia Pivovarova, Elaine Zosa, and Jussi Kurunmaki. Clustering Ideological Terms in Historical\nNewspaper Data with Diachronic Word Embeddings. In The 5th International Workshop on Computational\nHistory (HistoInformatics 2019), volume 2461 of CEUR Workshop Proceedings, pages 21–29, 2019. URL\nhttp://ceur-ws.org/Vol-2461/paper_4.pdf.\nCarlos Martinez-Ortiz, Tom Kenter, Melvin Wevers, Pim Huijnen, and Joris van Eijnatten. Design and implementa-\ntion of ShiCo: Visualising shifting concepts over time. In The 5th International Workshop on Computational\nHistory (HistoInformatics 2019), volume 1632 of CEUR Workshop Proceedings, pages 11–19, 2019. URL\nhttp://ceur-ws.org/Vol-1632/paper_2.pdf.\nJanis Pagel, Nidhi Sihag, and Nils Reiter. Predicting Structural Elements in German Drama. In Proceedings of the\nWorkshop on Computational Humanities Research (CHR 2021), volume 2989 of CEUR Workshop Proceedings,\npages 217–227, 2021. URL http://ceur-ws.org/Vol-2989/short_paper34.pdf.\nMohammad Taher Pilehvar and Jose Camacho-Collados. WiC: the word-in-context dataset for evaluating context-\nsensitive meaning representations. In Proceedings of the 2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short\nPapers), pages 1267–1273, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi:\n10.18653/v1/N19-1128. URL https://aclanthology.org/N19-1128.\nNils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using Siamese BERT-networks. In\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982–3992, Hong\nKong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1410. URL\nhttps://aclanthology.org/D19-1410.\nJournal of Data Mining and Digital Humanities\nISSN 2416-5999, an open-access journal\n17 http://jdmdh.episciences.org\nEyal Sagi, Stefan Kaufmann, and Brady Clark. Tracing semantic change with Latent Semantic Analysis. In Kathryn\nAllan and Justyna A. Robinson, editors, Current Methods in Historical Semantics . DE GRUYTER, Berlin,\nBoston, January 2011. ISBN 978-3-11-025290-3. doi: 10.1515/9783110252903.161.\nVille Satopaa, Jeannie Albrecht, David Irwin, and Barath Raghavan. Finding a ”kneedle” in a haystack: Detecting\nknee points in system behavior. In 2011 31st International Conference on Distributed Computing Systems\nWorkshops, pages 166–171, June 2011. doi: 10.1109/ICDCSW.2011.20.\nMike Schuster and Kaisuke Nakajima. Japanese and korean voice search. In 2012 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP), pages 5149–5152. IEEE, 2012.\nStefan Schweter and Johannes Baiter. Towards robust named entity recognition for historic German. In Proceedings\nof the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019) , pages 96–103, Florence, Italy,\nAugust 2019. Association for Computational Linguistics. doi: 10.18653/v1/W19-4312. URL https://www.\naclweb.org/anthology/W19-4312.\nStefan Schweter and Luisa M ¨arz. Triple e-effective ensembling of embeddings and language models for ner of\nhistorical german. In Conference and Labs of the Evaluation Forum (CLEF 2020), volume 2696. CEUR-WS\nWorking Notes, 2020.\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units.\nIn Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pages 1715–1725, Berlin, Germany, August 2016. Association for Computational Linguistics. doi:\n10.18653/v1/P16-1162. URL https://aclanthology.org/P16-1162.\nJohn Simpson and Edmund Weiner. Oxford English Dictionary. Oxford University Press, 1989.\nMatthew Sims, Jong Ho Park, and David Bamman. Literary Event Detection. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics, pages 3623–3634, Florence, Italy, 2019. Association for\nComputational Linguistics. doi: 10.18653/v1/P19-1353. URL https://www.aclweb.org/anthology/\nP19-1353.\nPia Sommerauer and Antske Fokkens. Conceptual Change and Distributional Semantic Models: An Exploratory\nStudy on Pitfalls and Possibilities. In Proceedings of the 1st International Workshop on Computational Ap-\nproaches to Historical Language Change, pages 223–233. Association for Computational Linguistics, 2019. doi:\n10.18653/v1/W19-4728. URL https://www.aclweb.org/anthology/W19-4728.\nLichao Sun, Kazuma Hashimoto, Wenpeng Yin, Akari Asai, Jia Li, Philip Yu, and Caiming Xiong. Adv-bert: Bert\nis not robust on misspellings! generating nature adversarial samples on bert. arXiv preprint arXiv:2003.04985,\n2020.\nNina Tahmasebi and Thomas Risse. On the Uses of Word Sense Change for Research in the Digital Humanities. In\nJaap Kamps, Giannis Tsakonas, Yannis Manolopoulos, Lazaros Iliadis, and Ioannis Karydis, editors,Research and\nAdvanced Technology for Digital Libraries, volume 10450, pages 246–257. Springer International Publishing,\nCham, 2017. ISBN 978-3-319-67007-2 978-3-319-67008-9. doi: 10.1007/978-3-319-67008-9 20. URL\nhttp://link.springer.com/10.1007/978-3-319-67008-9_20 . Series Title: Lecture Notes in\nComputer Science.\nNina Tahmasebi, Lars Borin, Adam Jatowt, et al. Survey of computational approaches to diachronic conceptual\nchange. arXiv preprint arXiv:1811.06278, 2018.\nP. D. Turney and P. Pantel. From Frequency to Meaning: Vector Space Models of Semantics. Journal of\nArtificial Intelligence Research, 37:141–188, February 2010. ISSN 1076-9757. doi: 10.1613/jair.2934. URL\nhttps://jair.org/index.php/jair/article/view/10640.\nJoris van Eijnatten and Ruben Ros. The Eurocentric Fallacy. A Digital-Historical Approach to the Concepts\nof ‘Modernity’, ‘Civilization’ and ‘Europe’ (1840–1990). International Journal for History, Culture and\nModernity, 7(1):686–736, November 2019. ISSN 2666-6529, 2213-0624. doi: 10.18352/hcm.580. URL\nhttps://brill.com/view/journals/hcm/7/1/article-p686_33.xml.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages\n5998–6008, 2017.\nMelvin Wevers. Using Word Embeddings to Examine Gender Bias in Dutch Newspapers, 1950-1990.\narXiv:1907.08922 [cs, stat] , July 2019. URL http://arxiv.org/abs/1907.08922. arXiv:\n1907.08922.\nMelvin Wevers and Marijn Koolen. Digital begriffsgeschichte: Tracing semantic change using word embeddings.\nHistorical Methods: A Journal of Quantitative and Interdisciplinary History , 53(4):226–243, October 2020.\nISSN 0161-5440, 1940-1906. doi: 10.1080/01615440.2020.1760157. URL https://www.tandfonline.\ncom/doi/full/10.1080/01615440.2020.1760157.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler.\nAligning books and movies: Towards story-like visual explanations by watching movies and reading books. In\nJournal of Data Mining and Digital Humanities\nISSN 2416-5999, an open-access journal\n18 http://jdmdh.episciences.org\nProceedings of the IEEE international conference on computer vision, pages 19–27, 2015.\nJournal of Data Mining and Digital Humanities\nISSN 2416-5999, an open-access journal\n19 http://jdmdh.episciences.org",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.828282356262207
    },
    {
      "name": "Language model",
      "score": 0.6669309735298157
    },
    {
      "name": "Natural language processing",
      "score": 0.6595445871353149
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.6542532444000244
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5724126100540161
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5176823139190674
    },
    {
      "name": "Sentence",
      "score": 0.5164453983306885
    },
    {
      "name": "Scratch",
      "score": 0.5089445114135742
    },
    {
      "name": "Programming language",
      "score": 0.08412972092628479
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I121797337",
      "name": "Leiden University",
      "country": "NL"
    }
  ],
  "cited_by": 29
}