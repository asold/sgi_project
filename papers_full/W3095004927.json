{
    "title": "Tabular Transformers for Modeling Multivariate Time Series",
    "url": "https://openalex.org/W3095004927",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2768377415",
            "name": "Inkit Padhi",
            "affiliations": [
                "IBM (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A3095725884",
            "name": "Yair Schiff",
            "affiliations": [
                "IBM (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2132303832",
            "name": "Igor Melnyk",
            "affiliations": [
                "IBM (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A1951822702",
            "name": "Mattia Rigotti",
            "affiliations": [
                "IBM (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2245551374",
            "name": "Youssef Mroueh",
            "affiliations": [
                "IBM (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A4361058491",
            "name": "Pierre Dognin",
            "affiliations": [
                "IBM (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2563014241",
            "name": "Jerret Ross",
            "affiliations": [
                "IBM (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2115729987",
            "name": "Ravi Nair",
            "affiliations": [
                "IBM (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2123595309",
            "name": "Erik Altman",
            "affiliations": [
                "IBM (United States)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6780725165",
        "https://openalex.org/W6794791811",
        "https://openalex.org/W6773909831",
        "https://openalex.org/W3008736151",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W2981851019",
        "https://openalex.org/W6761672038",
        "https://openalex.org/W6769113659",
        "https://openalex.org/W3035140194",
        "https://openalex.org/W6767220000",
        "https://openalex.org/W7062580218",
        "https://openalex.org/W3035231859",
        "https://openalex.org/W6765451912",
        "https://openalex.org/W6756556786",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W6785151444",
        "https://openalex.org/W2174574259",
        "https://openalex.org/W6752378359",
        "https://openalex.org/W6765779288",
        "https://openalex.org/W2980282514",
        "https://openalex.org/W3043127315",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2970528773",
        "https://openalex.org/W2902901670",
        "https://openalex.org/W3004690810",
        "https://openalex.org/W2962785754",
        "https://openalex.org/W3204818612",
        "https://openalex.org/W2963981733",
        "https://openalex.org/W2946345909",
        "https://openalex.org/W2980111207",
        "https://openalex.org/W2937845937",
        "https://openalex.org/W2969724595",
        "https://openalex.org/W2807747378",
        "https://openalex.org/W2970533824",
        "https://openalex.org/W3094090798"
    ],
    "abstract": "Tabular datasets are ubiquitous in data science applications. Given their importance, it seems natural to apply state-of-the-art deep learning algorithms in order to fully unlock their potential. Here we propose neural network models that represent tabular time series that can optionally leverage their hierarchical structure. This results in two architectures for tabular time series: one for learning representations that is analogous to BERT and can be pre-trained end-to-end and used in downstream tasks, and one that is akin to GPT and can be used for generation of realistic synthetic tabular sequences. We demonstrate our models on two datasets: a synthetic credit card transaction dataset, where the learned representations are used for fraud detection and synthetic data generation, and on a real pollution dataset, where the learned encodings are used to predict atmospheric pollutant concentrations. Code and data are available at https://github.com/IBM/TabFormer.",
    "full_text": "TABULAR TRANSFORMERS FOR MODELING MULTIV ARIATE TIME SERIES\nInkit Padhi, Yair Schiff, Igor Melnyk, Mattia Rigotti, Youssef Mroueh, Pierre Dognin\nJerret Ross, Ravi Nair, Erik Altman\nIBM Research, T.J Watson Research Center & MIT-IBM Watson AI Lab\nABSTRACT\nTabular datasets are ubiquitous in data science applications.\nGiven their importance, it seems natural to apply state-of-\nthe-art deep learning algorithms in order to fully unlock their\npotential. Here we propose neural network models that rep-\nresent tabular time seriesthat can optionally leverage their\nhierarchical structure. This results in two architectures for\ntabular time series: one for learning representations that is\nanalogous to BERT and can be pre-trained end-to-end and\nused in downstream tasks, and one that is akin to GPT and\ncan be used for generation of realistic synthetic tabular se-\nquences. We demonstrate our models on two datasets: a syn-\nthetic credit card transaction dataset, where the learned rep-\nresentations are used for fraud detection and synthetic data\ngeneration, and on a real pollution dataset, where the learned\nencodings are used to predict atmospheric pollutant concen-\ntrations. Code and data are available athttps://github.\ncom/IBM/TabFormer.\nIndex Terms— Tabular time series, BERT, GPT\n1. INTRODUCTION\nTabular datasets are ubiquitous across many industries, espe-\ncially in vital sectors such as healthcare and ﬁnance. Such\nindustrial datasets often contain sensitive information, raising\nprivacy and conﬁdentiality issues that preclude their public re-\nlease and limit their analysis to methods that are compatible\nwith an appropriate anonymization process. We can distin-\nguish between two types of tabular data: static tabular data\nthat corresponds to independent rows in a table, and dynamic\ntabular data that corresponds to tabular time series, also re-\nferred to also as multivariate time series. The machine learn-\ning and deep learning communities have devoted considerable\neffort to learning from static tabular data, as well as generat-\ning synthetic static tabular data that can be released as a pri-\nvacy compliant surrogate of the original data. On the other\nhand, less effort has been devoted to the more challenging\ndynamic case, where it is important to also account for the\ntemporal component of the data. The purpose of this paper is\nto remedy this gap by proposing deep learning techniques to:\n1) learn useful representation of tabular time series that can be\nused in downstream tasks such as classiﬁcation or regression\nand 2) generate realistic synthetic tabular time series.\nTabular time seriesrepresent a hierarchical structure that\nwe leverage by endowing transformer-based language mod-\nels with ﬁeld-level transformers, which encode individual\nrows into embeddings that are in turn treated as embedded\ntokens that are passed to BERT [1]. This results in an alter-\nnative architectures for tabular time series encoding that can\nbe pre-trained end-to-end for representation learning that we\ncall Tabular BERT (TabBERT). Another important contribu-\ntion is adapting state-of-the-art (SOTA) language generative\nmodels GPT [2] to produce realistic synthetic tabular data\nthat we call Tabular GPT (TabGPT). A key ingredient of our\nlanguage metaphor in modeling tabular time series is the\nquantization of continuous ﬁelds, so that each ﬁeld is deﬁned\non a ﬁnite vocabulary, as in language modeling.\nAs mentioned, static tabular data have been widely ana-\nlyzed in the past, typically with feature engineering and clas-\nsical learning schemes such as gradient boosting or random\nforests. Recently, [3] introduced TabNet, which uses attention\nto perform feature selection across ﬁelds and shows the ad-\nvantages of deep learning over classical approaches. A more\nrecent line of work [4, 5] concurrent to ours, deals with the\njoint processing of static tabular and textual data using trans-\nformer architectures, such as BERT, with the goal of querying\ntables with natural language. These works consider the static\ncase, and to the best of our knowledge, our work is the ﬁrst to\naddress tabular time series using transformers.\nOn the synthetic generation side, a plethora of work [6,\n7, 8, 9, 10] are dedicated to generating static tabular data\nusing Generative Adversarial Networks (GANs), conditional\nGANs, and variational Auto-Encoders. [11, 12] argue for the\nimportance of synthetic generation on ﬁnancial tabular data\nin order to preserve user privacy and to allow for training\non cloud-based solutions without compromising real users’\nsensitive information. Nevertheless, their generation scheme\nfalls short of modeling the temporal dependency in the data.\nOur work addresses this crucial aspect in particular. In sum-\nmary, the main contributions of our paper are:\n•We propose Hierarchical Tabular BERT to learn represen-\ntations of tabular time series that can be used in downstream\ntasks such as classiﬁcation or regression.\n•We propose TabGPT to synthesize realistic tabular time se-\nries data.\narXiv:2011.01843v2  [cs.LG]  11 Feb 2021\n•We train our proposed models on a synthetic credit card\ntransaction dataset, where the learned encodings are used for\na downstream fraud detection task and for synthetic data gen-\neration. We also showcase our method on a public real-world\npollution dataset, where the learned encodings are used to pre-\ndict the concentration of pollutant.\n• We open-source our synthetic credit-card transactions\ndataset to encourage future research on this type of data.\nThe code to reproduce all experiments in this paper is avail-\nable at https://github.com/IBM/TabFormer. Our\ncode is built within HuggingFace’s framework [13].\n2. TABBERT: UNSUPERVISED LEARNING OF\nMULTIV ARIATE TIME SERIES REPRESENTATION\n2.1. From Language Modeling to Tabular Time Series\nFig. 1: An example of sequential tabular data, where each\nrow is a transaction. A to M are the ﬁelds of the transac-\ntions. Some of the ﬁelds are categorical, others are contin-\nuous, but through quantization we convert all ﬁelds into cat-\negorical. Each ﬁeld is then processed to build its own local\nvocabulary. A single sample is deﬁned as some number of\ncontiguous transactions, for example rows 1 through 10, as\nshown in this ﬁgure.\nIn Fig. 1, we give an example of tabular time series, that\nis a sequence of card transactions for a particular user. Each\nrow consists of ﬁelds that can be continuous or categorical. In\norder to unlock the potential of language modeling techniques\nfor tabular data, we quantize continuous ﬁelds so that each\nﬁeld is deﬁned on its own local ﬁnite vocabulary. We deﬁne\na sample as a sequence of rows (transactions in this case).\nThe main difference with NLP is that we have a sequence\nof structured rows consisting each of ﬁelds deﬁned on local\nvocabulary. As introduced in previous sections, unsupervised\nlearning for multivariate time series representations require\nmodeling both inter- and intra-transaction dependencies. In\nthe next section, we show how to exploit this hierarchy in\nlearning unsupervised representations for tabular time series.\n2.2. Hierarchical Tabular BERT\nIn order to learn representations for multivariate tabular data,\nwe use a recipe similar to the one employed for training\nlanguage representation using BERT. The encoder is trained\nthrough masked language modeling (MLM), i.e by predict-\ning masked tokens. More formally, given a table with M\nField11 Field12 Mask · · · FieldT1 Mask FieldT3\nRow1 RowT\nField Transformer Field Transformer\nRE1 RET\nTabular BERT: Sequence Encoding Transformer Module\nSE1 SET\nv12v11 ˆv13\nEmbedded Row1\nˆvT2vT1 vT3\nEmbedded RowT\nRandom\nMasking\nRow\nEmbedding\nSequence\nEmbedding\n· · ·\nFig. 2: TabBERT: Field level masking and cross entropy.\nrows and N columns (or ﬁelds), an input, t, to TabBERT is\nrepresented as a windowed series of T time-dependent rows,\nt= [ti+1,ti+2,...,t i+T ], (1)\nwhere T (≪M) is the number of consecutive rows, and se-\nlected with a window offset (or stride). TabBERT is a vari-\nant of BERT, which accommodates the temporal nature of\nrows in the tabular input. As shown in Fig. 2, TabBERT en-\ncodes the series of transactions in a hierarchical fashion. The\nﬁeld transformer processes rows individually, creating trans-\naction/row embeddings. These transaction embeddings are\nthen fed to a second-level transformer to create sequence em-\nbeddings. In other words, the ﬁeld transformer tries to capture\nintra-transaction relationships (local), whereas the sequence\ntransformer encodes inter-transaction relationships (global),\ncapturing the temporal component of the data. Note that hi-\nerarchical transformer has already been proposed in NLP in\norder to hierarchically model documents [14, 15].\nMany pre-trained transformer models on domain speciﬁc\ndata have recently been successfully applied in various down-\nstream tasks. More speciﬁcally, BioBERT [16], VideoBERT\n[17], ClinicalBERT [18] are pre-trained efﬁciently on various\ndomains, such as biomedical, YouTube videos, and clinical\nelectronic health record, respectively. Representations from\nthese BERT variants achieve SOTA results on tasks ranging\nfrom video captioning to hospital readmission. In order to as-\ncertain the richness of learned TabBERT representation, we\nstudy two downstream tasks: classiﬁcation (Section 2.3) and\nregression (Section 2.4).\n2.3. TabBERT Features for Classiﬁcation\nTransaction Dataset One of the contributions of this work is\nin introducing a new synthetic corpus for credit card transac-\ntions. The transactions are created using a rule-based gener-\nator where values are produced by stochastic sampling tech-\nniques, similar to a method followed by [19]. Due to privacy\nconcerns, most of the existing public transaction datasets are\neither heavily anonymized or preserved through PCA-based\ntransformations, and thus distort the real data distributions.\nThe proposed dataset has 24 million transactions from 20,000\nusers. Each transaction (row) has 12 ﬁelds (columns) consist-\ning of both continuous and discrete nominal attributes, such\nas merchant name, merchant address, transaction amount, etc.\nTraining TabBERT For training TabBERT on our transac-\ntion dataset, we create samples as sliding windows of 10\ntransactions, with a stride of 5. The continuous and cate-\ngorical values are quantized, and a vocabulary is created,\nas described in Section 2.1. Note that during training we\nexclude the label column isFraud?, to avoid biasing the\nlearned representation for the downstream fraud detection\ntask. Similar to strategies used by [1], we mask 15% of a\nsample’s ﬁelds, replacing them with the [M ASK ] token, and\npredict the original ﬁeld token with cross entropy loss.\nFraud Detection Task For fraud detection, we create sam-\nples by combining 10 contiguous rows (with a stride of 10)\nin a time-dependent manner for each user. In total, there are\n2.4M samples with 29,342 labeled as fraudulent. Since in\nreal world the fraudulent transactions are very rare events, a\nsimilar trend is observed in our synthetic data, resulting in an\nimbalanced, non-uniform distribution of fraudulent and non-\nfraudulent class labels. To account for this imbalance, during\ntraining, we upsample the fraudulent class to roughly equal-\nize the frequencies of both classes. We evaluate performance\nof different methods using F1 binary score, on a test set con-\nsisting of 480K samples. As a baseline, we use a multi-layer\nperceptron (MLP) trained directly on the embeddings of the\nraw features. In order to model temporal dependencies, we\nalso use an LSTM network baseline on the raw embedded fea-\ntures. In both cases, we pool the encoder outputs at individual\nrow level to create Ei (see Fig. 2) before doing classiﬁcation.\nIn Tab. 1, we compare the baselines and the methods based on\nTabBERT features while using the same architectures for the\nprediction head. During MLP and LSTM networks training,\nwith TabBERT as the feature extractor, we freeze the Tab-\nBERT network foregoing any update of its weights. As can\nbe seen from the table, the inclusion of TabBERT features\nboosts the F1 for the fraud detection task.\n2.4. TabBERT Features for Regression Tasks\nPollution Dataset For the regression task, we use a public\nUCI dataset (Beijing PM2.5 Data) for predicting both PM2.5\nand PM10 air concentration for 12 monitoring sites, each con-\ntaining around 35k entries (rows). Every row has 11 ﬁelds\nwith a mix of both continuous and discrete values. For a de-\ntailed description of the data, please refer to [20]. Similar to\nthe pre-processing steps for our transaction dataset, we quan-\ntize the continuous features, remove the targets ( PM2.5 and\nPM10), and create samples by combining 10 time-dependent\nrows with a stride of 5. We use 45K samples for training and\nreport a combined RMSE for both targets from the test set\nFraud PRSA\nFeatures Prediction Head F1 RMSE\nRaw MLP 0.74 38.5\nLSTM 0.83 43.3\nTabBERT MLP 0.76 34.2\nLSTM 0.86 32.8\nTable 1: Performance comparison on the classiﬁcation task of\nfraud detection (Fraud), and the regression task of pollution\nprediction (PRSA) for two approaches: one based on Tab-\nBERT features and the baseline using raw data. We compare\ntwo architectures: MLP and LSTM for the downstream tasks.\nof 15K samples. As reported in Tab. 1, using TabBERT fea-\ntures shows signiﬁcant improvement in terms of RMSE over\nthe case of using simple raw embedded features. This consis-\ntent performance gain when using TabBERT features for both\nclassiﬁcation and regression tasks underlines the richness of\nrepresentations learned from TabBERT.\n3. TABGPT: GENERATIVE MODELING OF\nMULTIV ARIATE TIME SERIES TABULAR DATA\nAnother useful application of language modeling in the con-\ntext of tabular, time-series data is the preservation of data\nprivacy. GPT models trained on large corpora have demon-\nstrated human-level capabilities in the domain of text gener-\nation. In this work, we apply the generative capabilities of\nGPT as a proof of concept for creating synthetic tabular data\nthat is close in distribution to the true data, with the advan-\ntage of not exposing any sensitive information. Speciﬁcally,\nwe train a GPT model (referred to throughout as TabGPT) on\nuser-level data from the credit card dataset in order to generate\nsynthetic transactions that mimic a user’s purchasing behav-\nior. This synthetic data can subsequently be used in down-\nstream tasks without the precautions that would typically be\nnecessary when handling private information.\nWe begin, as with TabBERT, by quantizing the data to\ncreate a ﬁnite vocabulary for each ﬁeld. To train the TabGPT\nmodel, we select speciﬁc users from the dataset. By ordering\na user’s transactions chronologically and segmenting them\ninto sequences of ten transactions, the model learns to predict\nfuture behavior from past transactions, similar to how GPT\nlanguage models are trained on text data to predict future to-\nkens from past context. We apply this approach to two of the\nusers that have a relatively high volume of transactions, each\nwith ∼60k transactions. For each user, we train a separate\nTabGPT model, which is depicted in Fig. 3. Unlike with Tab-\nBERT, we do not employ the hierarchical structure of passing\neach ﬁeld into a ﬁeld-level transformer, but rather we pass\nsequences of transactions separated by a special [S EP] token\ndirectly to the GPT encoder network.\nAfter training, synthetic data is generated by again seg-\nmenting a user’s transaction data into sequences of ten trans-\nactions, passing the ﬁrst transaction of each group of ten to\nField11 Field12 Field13\nRow1\n· · · FieldT1 FieldT2 FieldT3\nRowT\nTabular GPT EncoderTabular GPT Encoder\nTabular GPT Causal Decoder\nField11 Field12 Field13\nSynthetic Row1\n· · · FieldT1 FieldT2 FieldT3\nSynthetic RowT\nFig. 3: TabGPT: Synthetic Transaction GPT Generator.\nthe model, and predicting the remaining nine. To evaluate a\nmodel’s generative capabilities we examine how it captures\nboth the aggregate and time-dependent features of the data.\nThe quantization of non-categorical data, which enables\nthe use of a ﬁnite vocabulary for each ﬁeld, renders ﬁeld\nlevel evaluation of the ﬁdelity of TabGPT to the real data\nmore straightforward. Namely, for each ﬁeld, we compute\nand compare histograms for both ground truth and generated\ndata on an aggregate level over all timestamps. To measure\nproximity of the true and synthetic distributions we calculate\nthe χ2 distance between histograms, deﬁned as: χ2(X,X′) =\n1\n2\n∑n\ni=1\n(xi−x′\ni)2\n(xi+x′\ni) where xi,x′\ni are columns from the corre-\nsponding transactions (i= 1..n) from the true (X) and gener-\nated (X′) distributions, respectively. In Fig. 4, we plot results\nFig. 4: For each column in the tabular data, we compare the\ngenerated and ground truth distributions for the user’s data\nrows. The entropy of each feature is represented by the bars\nand displayed on the left vertical axis andχ2 distance between\nreal and synthetic data distributions is represented by the line\nand displayed on right vertical axis.\nof this evaluation for the two selected users. Overall, we see\nthat for both users, their respective TabGPT models are able to\ngenerate synthetic distributions that are similar to the ground\ntruth for each feature of the data, even for columns with high\nentropy, such asAmount. The TabGPT model foruser 1pro-\nduces distributions that are generally closer to ground truth,\nbut for user 2, most column distributions also align closely.\nWhile ﬁeld distribution matching evaluates ﬁdelity of\ngenerated data to real data on an aggregate level, this anal-\nysis does not capture the sequential nature of the gen-\neration. Hence we use an additional metric that com-\npares two datasets of time series (ta\n1,i,...t a\nT,i)i=1...N and\n(tb\n1,i,...t b\nT,i)i=1...N . Inspired by the Fr´echet Inception Dis-\ntance (FID) [21] used in computer vision and Fr´echet In-\nferSent Distance (FD) [22] in NLP, we use our TabBERT\nmodel to embed real and generated sequence to a ﬁxed length\nvector for each instance va\ni = TabBERT((ta\n1,i,...t a\nT,i)), and\nvb\ni = TabBERT((tb\n1,i,...t b\nT,i)). va\ni is obtained by mean\npooling all time-wise embeddings SEt in TabBERT. Then we\ncompute mean and covariance for each dataset (µa,Σa) and\n(µb,Σb), respectively. The FID score is deﬁned as follows:\nFIDa,b = ||µa −µb||2\n2 + Tr(Σa + Σb −2(ΣaΣb)\n1\n2 ) (2)\nFID Real\nUser 1 User 2\nReal User 1 - 492.92\nGPT-Gen User 1 22.90 497.68\nUser 2 515.94 49.08\nTable 2: FID between real and GPT-generated transactions.\nFID scores between the transaction datasets foruser 1and\nuser 2are presented in Tab. 2. For the real user data, we see\nthat they have different behaviors, with FID of 492.95. In\ncontrast, the TabGPT generated data (GPT-Gen) for user 1\nmatches the real user 1 more closely, as can be seen from\nthe relatively low FID score. The same conclusion holds for\nGPT-Gen user 2. Interestingly the cross distances between\nthe generated user and the other real user are also maintained.\nThe combination of the aggregate histogram and FID analyses\nindicates that TabGPT is able to learn the behavior of each\nuser and to generate realistic synthetic transactions.\n4. CONCLUSION\nIn this paper, we introduce Hierarchical Tabular BERT and\nTabular GPT for modeling multivariate times series. We also\nopen-source a synthetic card transactions dataset and the code\nto reproduce our experiments. This type of modeling for se-\nquential tabular data via transformers is made possible thanks\nto the quantization of the continuous ﬁelds of the tabular data.\nWe show that the representations learned by TabBERT pro-\nvide consistent performance gains in different downstream\ntasks. TabBERT features can be used in fraud detectionin lieu\nof hand-engineered features as they better capture the intra-\ndependencies between the ﬁelds as well as the temporal de-\npendencies between rows. Finally, we show that TabGPT can\nreliably synthesise card transactions that can replace real data\nand alleviate the privacy issues encountered when training off\npremise or with cloud based solutions [11, 12].\n5. REFERENCES\n[1] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova, “Bert: Pre-training of deep bidirec-\ntional transformers for language understanding,” arXiv\npreprint arXiv:1810.04805, 2018.\n[2] Alec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever, “Language models\nare unsupervised multitask learners,” 2018.\n[3] Sercan O Arik and Tomas Pﬁster, “Tabnet: Atten-\ntive interpretable tabular learning,” arXiv preprint\narXiv:1908.07442, 2019.\n[4] Jonathan Herzig, Pawel Krzysztof Nowak, Thomas\nM¨uller, Francesco Piccinno, and Julian Eisenschlos,\n“TaPas: Weakly supervised table parsing via pre-\ntraining,” in Proceedings of the 58th ACL. July 2020,\npp. 4320–4333, ACL.\n[5] Pengcheng Yin, Graham Neubig, Wen tau Yih, and Se-\nbastian Riedel, “TaBERT: Pretraining for joint under-\nstanding of textual and tabular data,” in Annual Confer-\nence of the Association for Computational Linguistics\n(ACL), July 2020.\n[6] Lei Xu, Synthesizing Tabular Data using Conditional\nGAN, Ph.D. thesis, Massachusetts Institute of Technol-\nogy, 2020.\n[7] Lei Xu and Kalyan Veeramachaneni, “Synthesizing tab-\nular data using generative adversarial networks.(2018),”\narXiv preprint arXiv:1811.11264, 2018.\n[8] Lei Xu, Maria Skoularidou, Alfredo Cuesta-Infante, and\nKalyan Veeramachaneni, “Modeling tabular data using\nconditional gan,” in Advances in Neural Information\nProcessing Systems, 2019, pp. 7335–7345.\n[9] Anton Karlsson and Torbj ¨orn Sj ¨oberg, “Synthesis of\ntabular ﬁnancial data using generative adversarial net-\nworks,” 2020.\n[10] Ramiro Daniel Camino, Christian Hammerschmidt,\net al., “Working with deep generative models and tabu-\nlar data imputation,” 2020.\n[11] Samuel Assefa, Danial Dervovic, Mahmoud Mahfouz,\nTucker Balch, Prashant Reddy, and Manuela Veloso,\n“Generating synthetic data in ﬁnance: opportunities,\nchallenges and pitfalls,” Challenges and Pitfalls (June\n23, 2020), 2020.\n[12] Dmitry Eﬁmov, Di Xu, Luyang Kong, Alexey Nefe-\ndov, and Archana Anandakrishnan, “Using genera-\ntive adversarial networks to synthesize artiﬁcial ﬁnan-\ncial datasets,” 2020.\n[13] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pierric\nCistac, Tim Rault, R ´emi Louf, Morgan Funtowicz, Joe\nDavison, Sam Shleifer, Patrick von Platen, Clara Ma,\nYacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao,\nSylvain Gugger, Mariama Drame, Quentin Lhoest, and\nAlexander M. Rush, “Huggingface’s transformers:\nState-of-the-art natural language processing,” ArXiv,\nvol. abs/1910.03771, 2019.\n[14] Raghavendra Pappagari, Piotr Zelasko, Jes ´us Villalba,\nYishay Carmiel, and Najim Dehak, “Hierarchical trans-\nformers for long document classiﬁcation,” in2019 IEEE\nASRU Workshop. IEEE, 2019, pp. 838–844.\n[15] Xingxing Zhang, Furu Wei, and Ming Zhou, “HIB-\nERT: Document level pre-training of hierarchical bidi-\nrectional transformers for document summarization,”\nFlorence, Italy, July 2019, pp. 5059–5069, Association\nfor Computational Linguistics.\n[16] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang,\n“Biobert: a pre-trained biomedical language representa-\ntion model for biomedical text mining,”Bioinformatics,\nvol. 36, no. 4, pp. 1234–1240, 2020.\n[17] Chen Sun, Austin Myers, Carl V ondrick, Kevin Mur-\nphy, and Cordelia Schmid, “Videobert: A joint model\nfor video and language representation learning,” inPro-\nceedings of the IEEE International Conference on Com-\nputer Vision, 2019, pp. 7464–7473.\n[18] Kexin Huang, Jaan Altosaar, and Rajesh Ran-\nganath, “Clinicalbert: Modeling clinical notes and\npredicting hospital readmission,” arXiv preprint\narXiv:1904.05342, 2019.\n[19] Erik R Altman, “Synthesizing credit card transactions,”\narXiv preprint arXiv:1910.03033, 2019.\n[20] Xuan Liang, Tao Zou, Bin Guo, Shuo Li, Haozhe\nZhang, Shuyi Zhang, Hui Huang, and Song Xi Chen,\n“Assessing beijing’s pm2. 5 pollution: severity, weather\nimpact, apec and winter heating,” Proceedings of the\nRoyal Society A: Mathematical, Physical and Engineer-\ning Sciences, vol. 471, no. 2182, pp. 20150257, 2015.\n[21] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter, “Gans trained\nby a two time-scale update rule converge to a local nash\nequilibrium,” in Advances in neural information pro-\ncessing systems, 2017, pp. 6626–6637.\n[22] Stanislau Semeniuta, Aliaksei Severyn, and Sylvain\nGelly, “On accurate evaluation of gans for language\ngeneration,” arXiv preprint arXiv:1806.04936, 2018."
}