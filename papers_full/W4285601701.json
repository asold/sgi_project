{
  "title": "FQ-ViT: Post-Training Quantization for Fully Quantized Vision Transformer",
  "url": "https://openalex.org/W4285601701",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2000929903",
      "name": "Yang Lin",
      "affiliations": [
        "Vi Technology (United States)",
        "Megvii (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2127955102",
      "name": "Tianyu Zhang",
      "affiliations": [
        "Vi Technology (United States)",
        "Megvii (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2105046369",
      "name": "Peiqin Sun",
      "affiliations": [
        "Vi Technology (United States)",
        "Megvii (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2107695934",
      "name": "Zheng Li",
      "affiliations": [
        "Vi Technology (United States)",
        "Megvii (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2135537441",
      "name": "Shuchang Zhou",
      "affiliations": [
        "Vi Technology (United States)",
        "Megvii (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3035078980",
    "https://openalex.org/W2998218113",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W2903972532",
    "https://openalex.org/W3168124404",
    "https://openalex.org/W3170233084",
    "https://openalex.org/W2469490737",
    "https://openalex.org/W1836465849",
    "https://openalex.org/W2998183051",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3211787299",
    "https://openalex.org/W4308536459",
    "https://openalex.org/W2946355854",
    "https://openalex.org/W4287812978",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2962298324",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W4297813615",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W2963122961",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W4214588794"
  ],
  "abstract": "Network quantization significantly reduces model inference complexity and has been widely used in real-world deployments. However, most existing quantization methods have been developed mainly on Convolutional Neural Networks (CNNs), and suffer severe degradation when applied to fully quantized vision transformers. In this work, we demonstrate that many of these difficulties arise because of serious inter-channel variation in LayerNorm inputs, and present, Power-of-Two Factor (PTF), a systematic method to reduce the performance degradation and inference complexity of fully quantized vision transformers. In addition, observing an extreme non-uniform distribution in attention maps, we propose Log-Int-Softmax (LIS) to sustain that and simplify inference by using 4-bit quantization and the BitShift operator. Comprehensive experiments on various transformer-based architectures and benchmarks show that our Fully Quantized Vision Transformer (FQ-ViT) outperforms previous works while even using lower bit-width on attention maps. For instance, we reach 84.89% top-1 accuracy with ViT-L on ImageNet and 50.8 mAP with Cascade Mask R-CNN (Swin-S) on COCO. To our knowledge, we are the first to achieve lossless accuracy degradation (~1%) on fully quantized vision transformers. The code is available at https://github.com/megvii-research/FQ-ViT.",
  "full_text": "FQ-ViT: Post-Training Quantization for Fully Quantized Vision Transformer\nYang Lin∗† , Tianyu Zhang∗ , Peiqin Sun‡ , Zheng Li and Shuchang Zhou\nMEGVII Technology\nlinyang.zhh@gmail.com, {zhangtianyu, sunpeiqin, lizheng02, zsc}@megvii.com\nAbstract\nNetwork quantization significantly reduces model\ninference complexity and has been widely used in\nreal-world deployments. However, most existing\nquantization methods have been developed mainly\non Convolutional Neural Networks (CNNs), and\nsuffer severe degradation when applied to fully\nquantized vision transformers. In this work, we\ndemonstrate that many of these difficulties arise\nbecause of serious inter-channel variation in Lay-\nerNorm inputs, and present, Power-of-Two Fac-\ntor (PTF), a systematic method to reduce the per-\nformance degradation and inference complexity of\nfully quantized vision transformers. In addition,\nobserving an extreme non-uniform distribution in\nattention maps, we propose Log-Int-Softmax (LIS)\nto sustain that and simplify inference by using 4-\nbit quantization and the BitShift operator. Compre-\nhensive experiments on various transformer-based\narchitectures and benchmarks show that our Fully\nQuantized Vision Transformer (FQ-ViT) outper-\nforms previous works while even using lower bit-\nwidth on attention maps. For instance, we reach\n84.89% top-1 accuracy with ViT-L on ImageNet\nand 50.8 mAP with Cascade Mask R-CNN (Swin-\nS) on COCO. To our knowledge, we are the first\nto achieve lossless accuracy degradation (∼1%)\non fully quantized vision transformers. The code\nis available at https://github.com/megvii-research/\nFQ-ViT.\n1 Introduction\nTransformer-based architectures have achieved competitive\nperformance in various computer vision (CV) tasks, in-\ncluding image classification [Dosovitskiy et al., 2021; Tou-\nvron et al., 2021 ], object detection [Carion et al., 2020;\nLiu et al., 2021a], semantic segmentation[Zheng et al., 2021]\nand so on. Compared to the CNN counterparts, transform-\ners usually have more parameters and higher computational\n*Equal contribution.\n†Work done while interning at MEGVII Technology.\n‡Corresponding author.\nDeiT-T DeiT-S DeiT-B Swin-T Swin-S Swin-B ViT-B ViT-L\nModel\n0\n20\n40\n60\n80\n100Top-1 Accuracy on ImageNet\nFull Precision\nw/o Fully Quantized\n w/  Fully Quantized (Vanilla)\n w/  Fully Quantized (Ours)\nFigure 1: Top-1 accuracy on ImageNet for full precision and quan-\ntized vision transformers. w/o Fully Quantized: LayerNorm and\nSoftmax remain floating-point, while other modules are quantized\nto 8-bit by MinMax. w/ Fully Quantized (Vanilla): all modules are\nquantized to 8-bit by MinMax. w/ Fully Quantized (Ours): all mod-\nules are quantized to 8-bit by our method.\ncosts. For example, ViT-L has 307M parameters and 190.7G\nFLOPs, reaching the accuracy of 87.76% in ImageNet with\nlarge-scale pre-training. However, the large number of pa-\nrameters and computational overhead of transformer-based\narchitectures present a challenge when deployed to resource-\nconstrained hardware devices.\nTo facilitate deployment, several techniques have been pro-\nposed, including quantization [Zhou et al., 2016; Nagel et al.,\n2020; Shen et al., 2020; Liu et al., 2021b ], pruning [Han et\nal., 2016 ], distillation [Jiao et al., 2020 ] and adaptation of\narchitecture design [Graham et al., 2021]. We focus on the\nquantization technique in this paper and note that pruning,\ndistillation, and architecture adaptation are orthogonal to our\nwork and can be combined.\nMost existing quantization approaches have been designed\nand tested on CNNs and lack proper handling of transformer-\nspecific constructs. Previous work [Liu et al., 2021b ] finds\nthere have been a significant accuracy degradation when\nquantizing LayerNorm and Softmax of vision transformers.\nIn this case, the models are not fully quantized, resulting in\nthe need to retain floating-point units in the hardware, which\nwill bring large consumption and significantly reduce the in-\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1173\nference speed [Lian et al., 2019 ]. So we revisit these two\nexclusive modules of the vision transformers and discover\nthe reasons of degradation. Firstly, we find a serious inter-\nchannel variation of LayerNorm inputs, which some channel\nranges even exceed 40×of the median. Traditional methods\ncannot handle such large fluctuations of activations, which\nwill lead to large quantization error. Secondly, we find that\nthe values of the attention map have an extreme non-uniform\ndistribution, with most values clustered in 0∼0.01, and a few\nhigh attention values close to 1.\nBased on the analysis above, we propose Power-of-Two\nFactor (PTF) to quantize the inputs of the LayerNorm. In this\nway, the quantization error is greatly reduced, and the over-\nall computational efficiency is the same as that of layer-wise\nquantization thanks to the BitShift operator. In addition, we\npropose Log-Int-Softmax (LIS), which provides higher quan-\ntization resolution for small values and presents a more effi-\ncient integer inference for Softmax. Combining these meth-\nods, we are the first work to achieve post-training quantiza-\ntion for fully quantized vision transformers. As shown in Fig-\nure 1, our method significantly improves the performance of\nfully quantized vision transformers and obtains comparable\naccuracy with full precision counterparts.\nOur contributions are four-fold:\n• We revisit the fully quantized vision transformers and\nattribute the accuracy degradation to the serious inter-\nchannel variation in LayerNorm inputs. Meanwhile, we\nobserve an extreme non-uniform distribution of attention\nmaps, resulting in another part of the quantization error.\n• We propose Power-of-Two Factor (PTF), a simple yet\nefficient post-training method that can achieve accurate\nquantization on LayerNorm inputs with only one layer-\nwise quantization scale.\n• We propose Log-Int-Softmax (LIS), a novel method that\ncan perform 4-bit quantization on attention maps. With\nLIS, we can store attention maps on an aggressively low-\nbit and replace multiplication with BitShift operator. We\nachieve integer-only inference on Softmax modules, sig-\nnificantly reducing the inference consumption.\n• We conduct extensive experiments on image classifica-\ntion and object detection with various transformer-based\narchitectures. The results show that our fully quantized\nvision transformers with 8-bit weights/activations and 4-\nbit attention maps, can achieve comparable performance\nto floating-point versions.\n2 Related Work\n2.1 Vision Transformer\nRecently, transformer-based architecture shows great power\nin CV tasks. Emerging works based on ViT [Dosovitskiy\net al., 2021 ] demonstrated the effectiveness across all vi-\nsion tasks such as classification [Touvron et al., 2021 ], de-\ntection [Carion et al., 2020 ] and segmentation [Zheng et\nal., 2021 ]. The newly proposed Swin Transformer [Liu\net al., 2021a ] even surpasses the state-of-the-art CNNs on\nalmost tranditional CV tasks, presenting strong expressive\nand generalization capability of transformer. However, these\nhigh-performing vision transformers are attributed to the\nlarge number of parameters and high computational over-\nhead, limiting their adoption. Therefore, innovating a smaller\nand faster vision transformer becomes a new trend. Le-\nViT [Graham et al., 2021 ] makes progress in faster infer-\nence with down-sampling, patch descriptors, and a redesign\nof Attention-MLP block. DynamicViT [Rao et al., 2021 ]\npresents a dynamic token sparsification framework to prune\nredundant tokens progressively and dynamically, achieving\ncompetitive complexity and accuracy trade-off. Evo-ViT[Xu\net al., 2021 ] proposes a slow-fast updating mechanism that\nguarantees information flow and spatial structure, trimming\ndown both the training and inference complexity. While the\nabove works focus on efficient model designing, this paper\nboosts the compression and acceleration in the track of quan-\ntization.\n2.2 Network Quantization\nCurrent quantization methods can be divided into two cat-\negories: Quantization-Aware Training (QAT) and Post-\nTraining Quantization (PTQ). QAT [Zhou et al., 2016; Ja-\ncob et al., 2018 ] depends on training to achieve aggres-\nsively low-bit (e.g. 2-bit) quantization and promising per-\nformance, while it often requires a high-level expert knowl-\nedge and huge GPU resources for training or fine-tuning. To\nreduce above costs of quantization, PTQ, which is training-\nfree, has received more widespread attention and lots of ex-\ncellent works arise. OMSE [Choukroun et al., 2019 ] pro-\nposes to determine the value range of activation by minimiz-\ning the quantization error. AdaRound [Nagel et al., 2020 ]\npresents a novel rounding mechanism to adapt the data and\nthe task loss. Besides works above specific to CNNs, Liu et\nal. proposes a post-training quantization method for vision\ntransformers with similarity-aware and rank-aware strategies.\nHowever, this work does not quantize Softmax and Layer-\nNorm modules, resulting in an incomplete quantization. In\nour FQ-ViT, we aim to implement an accurate, fully quan-\ntized vision transformer under the PTQ paradigm.\n3 Proposed Method\nIn this section, we will introduce our proposed approach in\ndetail. First in Section 3.1, we present the preliminary of net-\nwork quantization. Then in Section 3.2 and 3.3, we analyze\nthe reasons of degradation in fully quantized vision trans-\nformers and propose two novel quantization methods, Power-\nof-Two Factor (PTF) and Log-Int-Softmax (LIS), for Layer-\nNorm and Softmax.\n3.1 Preliminary\nIn this section, we explain the notations of network quantiza-\ntion. Assuming the quantization bit-width is b, the quantizer\nQ(X|b) can be formulated as a function that maps a floating-\npoint number X ∈R to the nearest quantization bin:\nQ(X|b) :R →q, (1)\nq =\n(\n{−2b−1, ··· , 2b−1 −1} Signed,\n{0, 1 ··· , 2b −1} Unsigned.\n(2)\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1174\nR50 R101 R152 ViT-B ViT-L DeiT-T DeiT-S DeiT-B Swin-T Swin-S Swin-B\nModel\n100\n101\n102\n103\nChannel-wise ranges\n0 200 400 600 800 1000\nChannel's index\n200\n0\n200\n400\n600Value\nMinimum Value\nMaximum Value\nFigure 2: Left: Boxplot of the last LayerNorm inputs’ channel-wise ranges in each model. Right: Channel-wise minimum and maximum\nvalues of the last LayerNorm inputs in full precision Swin-B. The above two figures show that there exists more serious inter-channel variation\nin vision transformers than CNNs, which leads to unacceptable quantization errors with layer-wise quantization.\nThere are various quantizer Q(X|b), where uniform [Jacob\net al., 2018] and log2 [Cai et al., 2018] are typically used.\nUniform Quantization is well supported on most hard-\nware platforms. Its quantizer Q(X|b) can be defined as:\nQ(X|b) = clip(⌊X\ns ⌉+ zp, 0, 2b −1), (3)\nwhere s (scale) and zp (zero-point) are quantization parame-\nters determined by the lower bound l and the upper bound u\nof X, which are usually minimum and maximum values:\nl = min(X), u= max(X), (4)\ns = u −l\n2b −1\n, zp= clip(⌊−l\ns⌉, 0, 2b −1). (5)\nLog2 Quantization converts the quantization process from\nlinear to exponential variation. Its quantizer Q(X|b) can be\ndefined as:\nQ(X|b) = sign(X) ·clip(⌊−log2\n|X|\nmax(|X|)⌉, 0, 2b−1 −1).\n(6)\nIn this paper, to achieve a fully quantized vision trans-\nformer, we quantize all modules, including Conv, Linear,\nMatMul, LayerNorm, Softmax, etc. Especially, uniform Min-\nMax quantization is used for Conv, Linear and MatMul mod-\nules and our following methods are used for LayerNorm and\nSoftmax.\n3.2 Power-of-Two Factor for LayerNorm\nQuantization\nDuring inference, LayerNorm [Ba et al., 2016] computes the\nstatistics µX,σX in each forward step and normalizes input X.\nThen, affine parameters γ, β rescale the normalized input to\nanother learned distribution. The above process can be writ-\nten as:\nLayerNorm(X) = X −µX\np\nσ2\nX + ϵ\n·γ + β. (7)\nUnlike BatchNorm [Ioffe and Szegedy, 2015], commonly\nused in CNNs, LayerNorm cannot be folded into the previ-\nous layer due to its dynamic computational property, so we\nhave to quantize it separately. However, we observe a signif-\nicant performance degradation while applying post-training\nquantization on it. Looking into the inputs of LayerNorm\nlayers, we find there is a serious inter-channel variation. Fig-\nure 2 presents the channel-wise ranges of activation in the\nlast LayerNorm layer. In addition, we also display the cases\nof ResNets [He et al., 2016] for comparison. Considering that\nthere is no LayerNorm in ResNets, we choose the activations\nat the same position (fourth stage’s outputs) to exhibit.\nIt is observed that the channel-wise ranges fluctuate more\nwildly in vision transformers than those in ResNets. For\ninstance, the maximum range/median range of ResNet152\nis only 21.6/4.2, while it goes up to 622.5/15.5 in Swin-B.\nBased on such extreme inter-channel variation, layer-wise\nquantization, which applies the same quantization parameters\nto all channels, will lead to an intolerable quantization error.\nA possible solution is using group-wise quantization [Shen\net al., 2020 ] or channel-wise quantization [Li et al., 2019 ],\nwhich assign different quantization parameter to different\ngroup or channel. However, these will still induce the cal-\nculation of mean and variance in the floating-point domain,\nresulting in a high hardware overhead.\nIn this paper, we propose a simple yet efficient method,\nPower-of-Two Factor (PTF), for LayerNorm quantization.\nThe core idea of PTF is to equip different channels with\ndifferent factors, rather than different quantization parame-\nters. Given the quantization bit-width b, the input activa-\ntion X ∈ RB×L×C, the layer-wise quantization parameters\ns, zp∈R1, and the PTF α ∈NC, then the quantized activa-\ntion XQ can be formulated as:\nXQ = Q(X|b) = clip(⌊X\n2αs⌉+ zp, 0, 2b −1), (8)\nwith\ns = max(X) −min(X)\n2b −1\n/ 2K, (9)\nzp = clip(⌊−min(X)\ns ⌉, 0, 2b −1), (10)\nαc = arg min\nαc∈{0,1,···,K}\n\r\r\r\rXc −⌊ Xc\n2αc s⌉·2αc s\n\r\n\r\r\r\n2\n. (11)\nNoticing c represents the channel index for X and α. The\nhyperparameter K could meet different scaling requirements.\nIn order to cover the different inter-channel variation across\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1175\nFigure 3: Distribution of attention maps in ViT-L with visualizing\nthe 4-bit quantized bins of uniform and log2 quantization. X-axis\nis in log-scale and we can observe that log2 quantization preserves\nmore bins than uniform for small values.\nall models, we set K = 3 as default. Detailed experiments\ncan be found in supplementary materials.\nAt this point, each channel has its own Power-of-Two Fac-\ntor α and layer-wise parameters s,zp. During inference,\nlayer-wise parameters s and zp can be extracted, so the\ncomputation of µ,σ could be done in the integer domain\nrather than floating-point, which reduces the energy and area\ncosts [Lian et al., 2019]. Meanwhile, thanks to the nature of\npowers of two, PTFα can be efficiently combined with layer-\nwise quantization by BitShift operator, avoiding floating-\npoint calculations of group-wise or channel-wise quantiza-\ntion. The whole process can be processed with two phases:\nPhase 1: Shift the quantized activation with Power-of-Two\nFactor α:\nbXQ = (XQ −zp) << α. (12)\nPhase 2: Calculate the mean and variance based on the\nshifted activation bXQ:\nµ(X) ≈µ(2αs ·(XQ −zp)) =s ·µ(bXQ), (13)\nσ(X) ≈σ(2αs ·(XQ −zp)) =s ·σ(bXQ). (14)\n3.3 Log-Int-Softmax for Softmax Quantization\nMulti-head Self-Attention (MSA) is one of the most impor-\ntant components in transformer-based architectures, while it\nis considered the most resource-intensive due to the quadratic\ncomplexity to the number of token, the division of the im-\nage resolution by the patch size. As model performance\nproved to benefit from higher resolution and smaller patch\nsize [Dosovitskiy et al., 2021 ], when the increasing resolu-\ntion and reducing patch size, the storage and computation of\nattention maps become the bottleneck which directly affect\nthe throughput and latency of inference. Therefore, smaller\nattention maps and more efficient inference become an urgent\nneed.\nLog2 Quantization for Attention Map\nIn order to compress the attention maps to smaller size and\nspeed up the inference, we quantize attention maps to lower\nFigure 4: Comparison of using full precision Softmax and Log-Int-\nSoftmax in quantized multi-head self-attention inference. Full pre-\ncision Softmax needs to dequantize and requantize around Softmax,\nwhile LIS keeps an integer-only data type in the whole MSA infer-\nence.\nbit-width. As experimenting on quantization of attention\nmaps from 8-bit to 4-bit with uniform quantization, all vi-\nsion transformers show severe performance drop. For ex-\nample, DeiT-T only results in 8.69% top-1 accuracy on Im-\nageNet with 4-bit uniform quantized attention maps, decreas-\ning 63.05% from 8-bit case.\nInspired by the idea of sparse attention in Dynam-\nicViT [Rao et al., 2021 ], we probe into the distribution of\nattention maps, as Figure 3 shows. We observe a distribution\ncentering at a fairly small value, while only a few outliers\nhave larger values close to 1. Averaging on all attention maps\nin ViT-L, about 98.8% of values are smaller than1/16. Com-\npared with 4-bit uniform quantization which only assigns 1\nbin for such many values, log2 method can allocate 12 bins\nfor them. Moreover, following the purpose of ranking-aware\nloss [Liu et al., 2021b ], log2 quantization can retain much\norder consistency between full precision and quantized atten-\ntion maps. Consequently, we save the extreme degradation in\n4-bit quantization of attention maps and achieve equal perfor-\nmance as 8-bit uniform quantization with 50% less memory\nfootprint.\nLog2 quantization proved to be suitable combining with\nMSA from two aspects. Firstly, comparing to Equation (6),\nthe fixed output range (0, 1) of Softmax makes the log2 func-\ntion calibration-free:\nAttnQ = Q(Attn|b) = clip(⌊−log2(Attn)⌉, 0, 2b −1). (15)\nThis ensures that the quantization of attention maps will not\nbe affected by the fluctuation of calibration data.\nSecondly, it also introduces the merits of converting\nthe MatMul to BitShift between the quantized attention\nmap (AttnQ) and values (VQ) as:\nAttn ·VQ = 2−AttnQ ·VQ = VQ >> AttnQ (16)\n= 1\n2N ·(VQ << (N −AttnQ)), (17)\nwith N = 2b −1. Noticing that directly right shift V Q with\nthe results of Attn Q may lead to severe truncation error. We\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1176\nMethod W/A/Attn DeiT-T DeiT-S DeiT-B ViT-B ViT-L Swin-T Swin-S Swin-B\nFull Precision 32/32/32 72.21 79.85 81.85 84.53 85.81 81.35 83.20 83.60\nMinMax 8/8/8 70.94 75.05 78.02 23.64 3.37 64.38 74.37 25.58\nEMA [Jacob et al., 2018] 8/8/8 71.17 75.71 78.82 30.30 3.53 70.81 75.05 28.00\nPercentile [Li et al., 2019] 8/8/8 71.47 76.57 78.37 46.69 5.85 78.78 78.12 40.93\nOMSE [Choukroun et al., 2019] 8/8/8 71.30 75.03 79.57 73.39 11.32 79.30 78.96 48.55\nBit-Split∗ [Wang et al., 2020] 8/8/8 - 77.06 79.42 - - - - -\nPTQ for ViT∗ [Liu et al., 2021b] 8/8/8 - 77.47 80.48 - - - - -\nFQ-ViT 8/8/8 71.61 79.17 81.20 83.31 85.03 80.51 82.71 82.97\n8/8/4 71.07 78.40 80.85 82.68 84.89 80.04 82.47 82.38\nTable 1: Comparison of the top-1 accuracy with state-of-the-art methods on ImageNet dataset. ∗ indicates that all LayerNorm and Softmax\nmodules are not quantized.\nuse (N −AttnQ) as the quantized output with a scale equaling\n1/2N , which ensures a left-shift operation to prevent from\ntruncation error.\nInteger-only Inference\nPrevious works [Liu et al., 2021b] chose to not quantize Soft-\nmax because the negligibility of calculation amount in Soft-\nmax, and quantization may lead to significant accuracy degra-\ndation. However, data moving between CPU and GPU/NPU,\ndoing dequantization and requantization, will induce great\ndifficulties in hardware design, which is not a negligible con-\nsumption.\nCombining log2 quantization withi-exp [Kim et al., 2021],\nwhich is a polynomial approximation of exponential function,\nwe propose Log-Int-Softmax, an integer-only, faster, low con-\nsuming Softmax:\nexp(s ·XQ) ≈s′·i-exp(XQ), (18)\nLIS(s ·XQ) =N −log2⌊\nPi-exp(XQ)\ni-exp(XQ) ⌉, (19)\nwith N = 2b −1. An integer log2 function can be eas-\nily implemented by using BitShift to find the first bit index\nwhere the value is 1 (we call it Find\nFirst One function),\nand adding the value of bit right behind that. Detailed deriva-\ntion can be found in the supplementary materials.\nThe difference between normal MSA and our method are\nshown in Figure 4, with the data type of every stage labeled.\nIn the multi-head self-attention with unquantized Softmax\nshown on the left, the matrix multiplication of queries (Q)\nand keys (K) needs to be dequantized to full precision be-\nfore Softmax, and requantized after it. When our Log-Int-\nSoftmax adopted, shown on the right, the entire data type\ncan be in pure integer, with quantization scale individually\nand paralleling calculated. It is worth noting that LIS uses\nan aggressively 4-bit representation on attention maps, which\nsignificantly reduces memory footprint.\n4 Experiments\nIn this section, we present experimental results on vision\ntransformers for image classification and object detection.\nWe state the detailed experimental configuration firstly and\nthen exhibit the comparison of our method with existing\npost-training quantization methods in ImageNet [Krizhevsky\net al., 2012 ] and COCO [Lin et al., 2014 ] benchmarks.\nIn the end, ablation studies are conducted to evaluate the\neffectiveness of Power-of-Two Factor (PTF) and Log-Int-\nSoftmax (LIS).\n4.1 Implementation Details\nWe randomly sample 1000 training images from ImageNet\nor COCO as the calibration data, and use the validation set\nto evaluate performance. Apart from special notes, we per-\nform symmetric channel-wise quantization for weights and\nasymmetric layer-wise quantization for activations. For a fair\ncomparison, the quantization for weights is fixed as MinMax.\nThe hyperparameter K in Power-of-Two Factor is set to 3.\n4.2 Comparison with State-of-the-art Methods\nThis paper employs several current post-training quantization\nmethods, including MinMax, EMA [Jacob et al., 2018], Per-\ncentile [Li et al., 2019], OMSE [Choukroun et al., 2019], Bit-\nSplit [Wang et al., 2020] and PTQ for ViT[Liu et al., 2021b].\nNote that PTQ for ViT [Liu et al., 2021b ] is closest to our\nwork, however it does not quantize the LayerNorm and Soft-\nmax, while we quantize all modules.\nImage Classification on ImageNet\nTo demonstrate the effectiveness of proposed methods, we\nconduct extensive experiments in ImageNet [Krizhevsky et\nal., 2012] with various vision transformers, i.e., ViT [Doso-\nvitskiy et al., 2021], DeiT [Touvron et al., 2021], Swin Trans-\nformer [Liu et al., 2021a]. The overall top-1 accuracy results\nare reported in Table 1. It is obvious that all current methods\ncan’t capture fully quantized vision transformers, while our\nFQ-ViT does it and achieves a nearly lossless quantization\neven with an aggressively low-bit on attention maps. Mean-\nwhile, our FQ-ViT significantly exceeds PTQ for ViT [Liu\net al., 2021b], whose LayerNorm and Softmax are not quan-\ntized. For instance, our FQ-ViT achieves 81.20% accuracy\non DeiT-B in the case of all modules quantized to 8-bit, and\nit can still achieve 80.85% accuracy when the attention maps\nare compressed to 4-bit.\nObject Detection on COCO\nWe also conduct experiments on the object detection bench-\nmark COCO [Lin et al., 2014]. We choose Swin series [Liu\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1177\nMethod W/A/Attn Mask R-CNN Cascade Mask R-CNN\nw/ Swin-S w/ Swin-S\nFull Precision 32/32/32 48.5 52.0\nMinMax 8/8/8 32.8 35.2\nEMA [Jacob et al., 2018] 8/8/8 37.9 40.4\nPercentile [Li et al., 2019] 8/8/8 41.6 44.7\nOMSE [Choukroun et al., 2019] 8/8/8 42.6 44.9\nFQ-ViT 8/8/8 47.8 51.4\n8/8/4 47.2 50.8\nTable 2: Comparison of the bbox mAP with state-of-the-art methods on COCO dataset.\nMethod PTF LIS BitOPs (G) Acc. (%)\nFull Precision - - - 84.53\nBaseline #8 ✗ ✗ 1118.51 23.64\n✓ ✗ 1118.52 83.31\nBaseline #4 ✓ ✗ 1118.34 7.96\n✓ ✓ 1117.76 82.68\nTable 3: Effect of the Power-of-Two Factor (PTF) and Log-Int-\nSoftmax (LIS). We evaluate the performance of full precision and\nquantized ViT-B on ImageNet validation set. We choose MinMax\nwith 8-bit weights and activations as ”Baseline” and # indices the\nbit-width of attention maps.\net al., 2021a ] detectors for experiments and the results can\nbe found in Table 2. It is observed that all current meth-\nods have poor performance on fully quantized detectors.\nOur FQ-ViT significantly improves the quantization accu-\nracy and achieves 47.2 mAP on Mask R-CNN (Swin-S) and\n50.8 mAP on Cascade Mask R-CNN (Swin-S) with 8-bit on\nweights/activations and 4-bit on attention maps.\n4.3 Ablation Studies\nTo study the effect of our methods, Power-of-Two Fac-\ntor (PTF) and Log-Int-Softmax (LIS), we fully quantize ViT-\nB under a variety of strategies and report the results in Ta-\nble 3. We design two baselines. ”Baseline #8” indices the\nmodel is fully quantized by MinMax with 8-bit weights, acti-\nvations and attention maps, while ”Baseline #4” has a lower\nbit-width (4-bit) on attention maps. From the results, we have\nseveral observations. Firstly, the model with PTF and LIS\noutperforms the baseline and achieves almost lossless accu-\nracy. Secondly, thanks to the BitShift operator, PTF only in-\ntroduces little amount of BitOPs, while LIS reduces that.\n4.4 Visualization of Quantized Attention Map\nWe visualize the attention maps to see the difference between\nuniform quantization and our LIS as Figure 5 shows. When\nboth using 8-bit, uniform quantization focuses on the high\nactivation area, while LIS keeps more texture in the low acti-\nvation area, which retains more relative rank of the attention\nmap. This divergence does not make a big difference in the\ncase of 8-bit. However, when quantized to lower bit-width, as\nthe 6-bit and 4-bit cases show, uniform quantization sharply\ndegrades and even deactivates all the attention area. On the\n(a)\n(b)\n8-bit 6-bit 4-bit\nFigure 5: Attention map visualization. (a) shows the results of uni-\nform quantization and (b) shows the results of our Log-Int-Softmax.\ncontrary, LIS still presents acceptable performance similar to\n8-bit.\n5 Conclusions\nIn this paper, we propose a method to fully quantize vision\ntransformers. Specifically, we propose Power-of-Two Fac-\ntor (PTF) to deal with the serious inter-channel variations in\nthe inputs of LayerNorm. In addition, we propose an inte-\ngrated quantization solution Log-Int-Softmax (LIS) to im-\nplement 4-bit quantization of attention maps and utilize the\nBitShift operator instead of MatMul during inference, which\neffectively reduces hardware resource requirement. Experi-\nmental results show that our fully quantized vision transform-\ners achieve comparable performance with the full precision\nmodels. Altogether, we provide a higher baseline for future\nworks, hoping that FQ-ViT’s strong performance will encour-\nage research into even lower bit-width quantization of vision\ntransformers, which will boost their real-world adoptions.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1178\nReferences\n[Ba et al., 2016] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E\nHinton. Layer normalization. arXiv preprint arXiv:1607.06450,\n2016.\n[Cai et al., 2018] Jingyong Cai, Masashi Takemoto, and Hironori\nNakajo. A deep look into logarithmic quantization of model pa-\nrameters in neural networks. In Proceedings of the 10th Interna-\ntional Conference on Advances in Information Technology, 2018.\n[Carion et al., 2020] Nicolas Carion, Francisco Massa, Gabriel\nSynnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey\nZagoruyko. End-to-end object detection with transformers. In\nEuropean Conference on Computer Vision. Springer, 2020.\n[Choukroun et al., 2019] Yoni Choukroun, Eli Kravchik, Fan Yang,\nand Pavel Kisilev. Low-bit quantization of neural networks for\nefficient inference. In IEEE/CVF International Conference on\nComputer Vision Workshops. IEEE, 2019.\n[Dosovitskiy et al., 2021] Alexey Dosovitskiy, Lucas Beyer,\nAlexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer,\nGeorg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil\nHoulsby. An image is worth 16x16 words: Transformers for\nimage recognition at scale. In International Conference on\nLearning Representations, 2021.\n[Graham et al., 2021] Ben Graham, Alaaeldin El-Nouby, Hugo\nTouvron, Pierre Stock, Armand Joulin, Herv´e J´egou, and Matthijs\nDouze. Levit: a vision transformer in convnet’s clothing for faster\ninference. arXiv preprint arXiv:2104.01136, 2021.\n[Han et al., 2016] Song Han, Huizi Mao, and William J Dally. Deep\ncompression: Compressing deep neural networks with pruning,\ntrained quantization and huffman coding. In International Con-\nference on Learning Representations, 2016.\n[He et al., 2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and\nJian Sun. Deep residual learning for image recognition. In Pro-\nceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2016.\n[Ioffe and Szegedy, 2015] Sergey Ioffe and Christian Szegedy.\nBatch normalization: Accelerating deep network training by re-\nducing internal covariate shift. In International Conference on\nMachine Learning. PMLR, 2015.\n[Jacob et al., 2018] Benoit Jacob, Skirmantas Kligys, Bo Chen,\nMenglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam,\nand Dmitry Kalenichenko. Quantization and training of neural\nnetworks for efficient integer-arithmetic-only inference. In Pro-\nceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2018.\n[Jiao et al., 2020] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin\nJiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. Tinybert:\nDistilling BERT for natural language understanding. In Findings\nof the Association for Computational Linguistics: EMNLP, 2020.\n[Kim et al., 2021] Sehoon Kim, Amir Gholami, Zhewei Yao,\nMichael W Mahoney, and Kurt Keutzer. I-bert: Integer-only bert\nquantization. In International Conference on Machine Learning.\nPMLR, 2021.\n[Krizhevsky et al., 2012] Alex Krizhevsky, Ilya Sutskever, and Ge-\noffrey E. Hinton. Imagenet classification with deep convolutional\nneural networks. In Advances in Neural Information Processing\nSystems, 2012.\n[Li et al., 2019] Rundong Li, Yan Wang, Feng Liang, Hongwei\nQin, Junjie Yan, and Rui Fan. Fully quantized network for ob-\nject detection. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2019.\n[Lian et al., 2019] Xiaocong Lian, Zhenyu Liu, Zhourui Song, Jiwu\nDai, Wei Zhou, and Xiangyang Ji. High-performance fpga-based\ncnn accelerator with block-floating-point arithmetic.IEEE Trans-\nactions on Very Large Scale Integration (VLSI) Systems, 2019.\n[Lin et al., 2014] Tsung-Yi Lin, Michael Maire, Serge Belongie,\nJames Hays, Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and\nC Lawrence Zitnick. Microsoft coco: Common objects in con-\ntext. In European Conference on Computer Vision. Springer,\n2014.\n[Liu et al., 2021a] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan\nWei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin trans-\nformer: Hierarchical vision transformer using shifted windows.\narXiv preprint arXiv:2103.14030, 2021.\n[Liu et al., 2021b] Zhenhua Liu, Yunhe Wang, Kai Han, Wei\nZhang, Siwei Ma, and Wen Gao. Post-training quantization for\nvision transformer. In Thirty-Fifth Conference on Neural Infor-\nmation Processing Systems, 2021.\n[Nagel et al., 2020] Markus Nagel, Rana Ali Amjad, Mart van\nBaalen, Christos Louizos, and Tijmen Blankevoort. Up or down?\nadaptive rounding for post-training quantization. InInternational\nConference on Machine Learning. PMLR, 2020.\n[Rao et al., 2021] Yongming Rao, Wenliang Zhao, Benlin Liu, Ji-\nwen Lu, Jie Zhou, and Cho-Jui Hsieh. Dynamicvit: Efficient\nvision transformers with dynamic token sparsification. arXiv\npreprint arXiv:2106.02034, 2021.\n[Shen et al., 2020] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma,\nZhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt\nKeutzer. Q-bert: Hessian based ultra low precision quantization\nof bert. In Proceedings of the AAAI Conference on Artificial In-\ntelligence, 2020.\n[Touvron et al., 2021] Hugo Touvron, Matthieu Cord, Matthijs\nDouze, Francisco Massa, Alexandre Sablayrolles, and Herv ´e\nJ´egou. Training data-efficient image transformers & distilla-\ntion through attention. In International Conference on Machine\nLearning. PMLR, 2021.\n[Wang et al., 2020] Peisong Wang, Qiang Chen, Xiangyu He, and\nJian Cheng. Towards accurate post-training network quantiza-\ntion via bit-split and stitching. In International Conference on\nMachine Learning. PMLR, 2020.\n[Xu et al., 2021] Yifan Xu, Zhijie Zhang, Mengdan Zhang, Kekai\nSheng, Ke Li, Weiming Dong, Liqing Zhang, Changsheng Xu,\nand Xing Sun. Evo-vit: Slow-fast token evolution for dynamic\nvision transformer. arXiv preprint arXiv:2108.01390, 2021.\n[Zheng et al., 2021] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao,\nXiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng\nFeng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic seg-\nmentation from a sequence-to-sequence perspective with trans-\nformers. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, 2021.\n[Zhou et al., 2016] Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu\nZhou, He Wen, and Yuheng Zou. Dorefa-net: Training low\nbitwidth convolutional neural networks with low bitwidth gra-\ndients. arXiv preprint arXiv:1606.06160, 2016.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1179",
  "topic": "Softmax function",
  "concepts": [
    {
      "name": "Softmax function",
      "score": 0.703532338142395
    },
    {
      "name": "Quantization (signal processing)",
      "score": 0.6868132948875427
    },
    {
      "name": "Convolutional neural network",
      "score": 0.6709187030792236
    },
    {
      "name": "Computer science",
      "score": 0.6690168976783752
    },
    {
      "name": "Transformer",
      "score": 0.6140832901000977
    },
    {
      "name": "Inference",
      "score": 0.6046870946884155
    },
    {
      "name": "Artificial intelligence",
      "score": 0.433645099401474
    },
    {
      "name": "Algorithm",
      "score": 0.37737977504730225
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.34334176778793335
    },
    {
      "name": "Engineering",
      "score": 0.09518977999687195
    },
    {
      "name": "Voltage",
      "score": 0.0945720374584198
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}