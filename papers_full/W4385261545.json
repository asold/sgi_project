{
    "title": "<i>CommonsenseVIS</i>: Visualizing and Understanding Commonsense Reasoning Capabilities of Natural Language Models",
    "url": "https://openalex.org/W4385261545",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5086326351",
            "name": "Xingbo Wang",
            "affiliations": [
                "Cornell University"
            ]
        },
        {
            "id": "https://openalex.org/A5007553875",
            "name": "Renfei Huang",
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "University of Hong Kong"
            ]
        },
        {
            "id": "https://openalex.org/A5005458050",
            "name": "Zhihua Jin",
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "University of Hong Kong"
            ]
        },
        {
            "id": "https://openalex.org/A5046409172",
            "name": "Tianqing Fang",
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "University of Hong Kong"
            ]
        },
        {
            "id": "https://openalex.org/A5091466289",
            "name": "Huamin Qu",
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "University of Hong Kong"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2891503716",
        "https://openalex.org/W2137406659",
        "https://openalex.org/W3135835076",
        "https://openalex.org/W2981731882",
        "https://openalex.org/W6766937060",
        "https://openalex.org/W6851147835",
        "https://openalex.org/W2998617917",
        "https://openalex.org/W4224980442",
        "https://openalex.org/W3101056292",
        "https://openalex.org/W6678830454",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W3034350582",
        "https://openalex.org/W3175277088",
        "https://openalex.org/W3152884768",
        "https://openalex.org/W4206118214",
        "https://openalex.org/W6632702419",
        "https://openalex.org/W6849686967",
        "https://openalex.org/W3097986428",
        "https://openalex.org/W4318621382",
        "https://openalex.org/W3038035611",
        "https://openalex.org/W2970780738",
        "https://openalex.org/W3174464510",
        "https://openalex.org/W4315778472",
        "https://openalex.org/W4210513286",
        "https://openalex.org/W6768299147",
        "https://openalex.org/W6809674555",
        "https://openalex.org/W3099655892",
        "https://openalex.org/W4385567227",
        "https://openalex.org/W4283269716",
        "https://openalex.org/W6839020486",
        "https://openalex.org/W2983995706",
        "https://openalex.org/W3166444100",
        "https://openalex.org/W3175270222",
        "https://openalex.org/W3207553988",
        "https://openalex.org/W3175910413",
        "https://openalex.org/W6737947904",
        "https://openalex.org/W3176825161",
        "https://openalex.org/W6748816842",
        "https://openalex.org/W6810241339",
        "https://openalex.org/W6846320025",
        "https://openalex.org/W2964200170",
        "https://openalex.org/W6802629465",
        "https://openalex.org/W3098323839",
        "https://openalex.org/W6810738896",
        "https://openalex.org/W2970476646",
        "https://openalex.org/W2973319951",
        "https://openalex.org/W6769627184",
        "https://openalex.org/W2962833140",
        "https://openalex.org/W6767594909",
        "https://openalex.org/W2282821441",
        "https://openalex.org/W3194676777",
        "https://openalex.org/W2963101081",
        "https://openalex.org/W2970062726",
        "https://openalex.org/W6767608665",
        "https://openalex.org/W3104499181",
        "https://openalex.org/W3175287561",
        "https://openalex.org/W6796332046",
        "https://openalex.org/W2561529111",
        "https://openalex.org/W2752194699",
        "https://openalex.org/W6734194636",
        "https://openalex.org/W6755829550",
        "https://openalex.org/W6799626271",
        "https://openalex.org/W2083897630",
        "https://openalex.org/W2946417913",
        "https://openalex.org/W3101662419",
        "https://openalex.org/W3104235057",
        "https://openalex.org/W3204047821",
        "https://openalex.org/W2971296908",
        "https://openalex.org/W2956281901",
        "https://openalex.org/W3174057701",
        "https://openalex.org/W4285606726",
        "https://openalex.org/W3172335055",
        "https://openalex.org/W2946609015",
        "https://openalex.org/W3034918576",
        "https://openalex.org/W2991223644",
        "https://openalex.org/W4288088628",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W1542713999",
        "https://openalex.org/W4288262459",
        "https://openalex.org/W4283810228",
        "https://openalex.org/W2963995027",
        "https://openalex.org/W2962862931",
        "https://openalex.org/W4221166832",
        "https://openalex.org/W4288058769",
        "https://openalex.org/W2594633041",
        "https://openalex.org/W4361806395",
        "https://openalex.org/W4320202382",
        "https://openalex.org/W2127795553",
        "https://openalex.org/W2948771346",
        "https://openalex.org/W4287125421",
        "https://openalex.org/W2977235550",
        "https://openalex.org/W4226251122",
        "https://openalex.org/W1574901103",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2786672974",
        "https://openalex.org/W4286897388",
        "https://openalex.org/W4306313145",
        "https://openalex.org/W4221143736",
        "https://openalex.org/W3103795814"
    ],
    "abstract": "Recently, large pretrained language models have achieved compelling performance on commonsense benchmarks. Nevertheless, it is unclear what commonsense knowledge the models learn and whether they solely exploit spurious patterns. Feature attributions are popular explainability techniques that identify important input concepts for model outputs. However, commonsense knowledge tends to be implicit and rarely explicitly presented in inputs. These methods cannot infer models' implicit reasoning over mentioned concepts. We present CommonsenseVIS, a visual explanatory system that utilizes external commonsense knowledge bases to contextualize model behavior for commonsense question-answering. Specifically, we extract relevant commonsense knowledge in inputs as references to align model behavior with human knowledge. Our system features multi-level visualization and interactive model probing and editing for different concepts and their underlying relations. Through a user study, we show that CommonsenseVIS helps NLP experts conduct a systematic and scalable visual analysis of models' relational reasoning over concepts in different situations.",
    "full_text": "To appear in IEEE Transactions on Visualization and Computer Graphics.\nCommonsenseVIS: Visualizing and Understanding Commonsense\nReasoning Capabilities of Natural Language Models\nXingbo Wang, Renfei Huang, Zhihua Jin, Tianqing Fang, and Huamin Qu\nQ&A\nKnowledge graph\n/gid00046/gid00078/gid00067/gid00068/gid00075/gid00001/gid00066/gid00078/gid00077/gid00083/gid00068/gid00087/gid00083/gid00084/gid00064/gid00075/gid00072/gid00089/gid00064/gid00083/gid00072/gid00078/gid00077\nVisualization systemCommonsense QA\nModel editing\nModel understanding\nModel \nprediction\nBehavior \ndiagnosis\nCommonsense \nknowledge \nextraction\nMultilevel \nvisualization\nwhere do adults \nuse glue sticks?\nA. classroom\nB. oﬃce\nC. desk drawer\n/gid00034/gid00067/gid00084/gid00075/gid00083/gid00082/gid00001/gid00064/gid00081/gid00068/gid00001/gid00076/gid00078/gid00081/gid00068/gid00001\n/gid00075/gid00072/gid00074/gid00068/gid00075/gid00088/gid00001/gid00083/gid00078/gid00001/gid00084/gid00082/gid00068/gid00001/gid00070/gid00075/gid00084/gid00068/gid00001\n/gid00082/gid00083/gid00072/gid00066/gid00074/gid00082/gid00001/gid00072/gid00077/gid00001/gid00064/gid00001\n/gid00079/gid00081/gid00078/gid00069/gid00068/gid00082/gid00082/gid00072/gid00078/gid00077/gid00064/gid00075/gid00001\n/gid00086/gid00078/gid00081/gid00074/gid00001/gid00068/gid00077/gid00085/gid00072/gid00081/gid00078/gid00077/gid00076/gid00068/gid00077/gid00083\nadult glue stick\noﬃce\nwork\nuse\natLocation\ncommonsense\nDo NLP models have commonsense?\nUsersNLP model\nFig. 1: Since commonsense knowledge is not explicitly stated, it is challenging to conduct a scalable analysis of what commonsense\nknowledge NLP models do (not) learn. We employ a knowledge graph to derive implicit commonsense in the model input as context\ninformation. Then, we use it to align model behavior with human reasoning through multi-level interactive visualizations. Thereafter,\nusers can understand, diagnose, and edit specific knowledge areas where models do not perform well.\nAbstract—Recently, large pretrained language models have achieved compelling performance on commonsense benchmarks.\nNevertheless, it is unclear what commonsense knowledge the models learn and whether they solely exploit spurious patterns. Feature\nattributions are popular explainability techniques that identify important input concepts for model outputs. However, commonsense\nknowledge tends to be implicit and rarely explicitly presented in inputs. These methods cannot infer models’ implicit reasoning over\nmentioned concepts. We present CommonsenseVIS, a visual explanatory system that utilizes external commonsense knowledge bases\nto contextualize model behavior for commonsense question-answering. Specifically, we extract relevant commonsense knowledge in\ninputs as references to align model behavior with human knowledge. Our system features multi-level visualization and interactive model\nprobing and editing for different concepts and their underlying relations. Through a user study, we show that CommonsenseVIS helps\nNLP experts conduct a systematic and scalable visual analysis of models’ relational reasoning over concepts in different situations.\nIndex Terms—Commonsense reasoning, visual analytics, XAI, natural language processing\n1 I NTRODUCTION\nCommonsense knowledge describes the general facts and beliefs about\nthe world that are obvious and intuitive to most humans. It allows peo-\nple to smoothly explore and reason over everyday events and situations.\nFor example, “my parents are older than me” and “take an umbrella\nwhen it rains”. Equipping machines with humanlike commonsense\nreasoning abilities can benefit the development of social robots and\nintelligent personal agents to assist humans in daily tasks.\nCommonsense knowledge and reasoning have been important and\nlong-standing challenging topics in the natural language processing\n(NLP) community. Many researchers have devoted their efforts to\nbuilding commonsense knowledge bases by extracting information\n• Xingbo Wang is with Weill Cornell Medical College, Cornell University.\nThis work was done at the Hong Kong University of Science and Technology.\nE-mail: xingbo.wang@{connect.ust.hk, med.cornell.edu}.\n• Renfei Huang, Zhihua Jin, Tianqing Fang, and Huamin Qu are with the\nHong Kong University of Science and Technology. E-mail: {rhuangan,\nzjinak, tfangaa, huamin}@ust.hk.\nManuscript received xx xxx. 201x; accepted xx xxx. 201x. Date of Publication\nxx xxx. 201x; date of current version xx xxx. 201x. For information on\nobtaining reprints of this article, please send e-mail to: reprints@ieee.org.\nDigital Object Identifier: xx.xxxx/TVCG.201x.xxxxxxx\nfrom existing data sources (e.g., Wikipedia) or acquiring it from domain\nexperts or crowd workers. Generally, the commonsense knowledge\nis represented as graphs, where nodes denote the conceptual entities\n(e.g., cars, people) and links describe the relations between different\nconcepts (e.g., people “is capable of” driving cars). Building upon\nthe knowledge bases, a few benchmark datasets have been designed\nto evaluate and improve NLP models for automated commonsense\nreasoning. Particularly, question answering (QA) is the primary and\npopular form of benchmarks [5, 7, 21, 57, 62, 63, 75].\nRecent advances in large pre-trained language models (PLMs) of\nNLP (e.g., BERT [65], GPT [11], and T5 [48]) have yielded impressive\nand even human-level performance [73] on commonsense benchmarks.\nHowever, these models lack interpretability and transparency, which\nhinders model debugging and development for real-world applications.\nIt is unclear what commonsense knowledge the models have learned and\nused in the process of reasoning, and whether they merely explore the\nspurious correlation in the datasets. This issue has led to a rallying call\nfor explaining NLP models to reflect their real commonsense reasoning\ncapabilities and to build more robust benchmarks and models.\nTo help NLP experts understand the NLP model’s reasoning process,\nfeature attribution methods (e.g., LIME [51], SHAP [36]) are popu-\nlar explainability techniques, which quantify the importance of input\nfeatures (e.g., words and phrases) to the model outputs. Therefore,\nNLP experts can identify critical concepts for model predictions and\n1\narXiv:2307.12382v1  [cs.CL]  23 Jul 2023\ndetermine whether they are aligned with human knowledge. We have\nwitnessed the success of these methods for various applications (e.g.,\nsentiment analysis [12] and fake news detection [3]).\nHowever, feature attribution methods cannot be directly applied to\nexplain models for commonsense reasoning tasks. First, they are in-\ncapable of revealing models’ relational reasoning over concepts (e.g.,\nentities) in different contexts since their relations may require back-\nground knowledge and not be explicitly presented in the input. For\nexample, in “take an umbrella when it rains”, the inherent common-\nsense is that the umbrella “is used for” protection from the rain, which\nis not mentioned in the original statement. Moreover, contexts signif-\nicantly influence the reasoning over these implicit relations between\nconcepts. For instance, depending on the weather, the umbrella can\n“be used for” protection from the rain or sun. Furthermore, feature\nattributions often focus on individual instances. Given the complexity\nand vastness of the commonsense knowledge space the models operate\non, where concepts are intertwined with various relations and contexts,\nit is challenging to scale up these methods to efficiently build high-level\nabstractions of model behavior (e.g., under what contexts a relation\nis well learned) and generalize model understanding to large datasets.\nFurthermore, to better align the model with human knowledge and\nexpectations, it is crucial to not only understand its reasoning but also\nactively inject and update the desired knowledge within the model, such\nas using human feedback to finetune ChatGPT [45].\nVisual analytics [2,8,71] have been an effective approach for summa-\nrizing complex data characteristics and facilitating data-driven model\nunderstanding at scale. Motivated by this, we design and develop a\nvisual analytics system, CommonsenseVIS, which enables NLP ex-\nperts (e.g., model developers) to conduct a systematic and scalable\nanalysis of the commonsense reasoning capabilities of NLP models\n(outlined in Figure 1). Going beyond many existing visual explana-\ntion tools [2, 8, 66, 71] that focus on input-output behaviors of models,\nour system integrates an external knowledge base to derive implicit\ncommonsense knowledge from the input and uses them as additional\ncontexts to align model behavior with human reasoning through inter-\nactive visualizations. We focus on commonsense question answering\n(CQA), a popular task for evaluating commonsense reasoning abili-\nties, and showcase our system on the representative CSQA benchmark\ndataset [62]. ConceptNet [59], a commonsense knowledge graph, is\nused to extract commonsense knowledge from data as concept-relation\ntriplets for model contextualizations. Our system provides multi-level\nvisualizations of model behavior by comparing important input features\nfor model decisions with the extracted triplets from ConceptNet. At\nthe global level, the system adopts data transformation and projection\nstrategies to summarize model performance on questions and relations\nand assesses the overall relation learning. At the subset level, the sys-\ntem presents a contextual summary of the alignment between model\nbehavior and related ConceptNet knowledge for different subsets. And\nat the local level, the system provides visual explanations for individual\ninstances and allows for model probing and editing to identify and en-\nhance specific knowledge areas where models underperform. Through\na user study using CommonsenseQA (CSQA) [62] dataset, we show\nthat CommonsenseVIS can help NLP experts effectively understand,\ndiagnose, and edit model knowledge on concepts and their implicit\nrelations in different contexts.\nThe major contributions of this paper are summarized as follows:\n• CommonsenseVIS, a visual analytics system that supports a sys-\ntematic and scalable analysis of the model’s reasoning on com-\nmonsense tasks involving a large number of concepts and their\nrelations. Particularly, it helps align model behavior with human\nreasoning through model contextualization, multi-level visualiza-\ntions, and interactive model probing and editing.\n• A user study with cases that shows the effectiveness and usability\nof our system in revealing, diagnosing, and editing underlying\ncommonsense knowledge the language model does not learn.\n2 R ELATED WORK\nWe discuss related work in commonsense reasoning, explainable AI\nmethods, and visualization for NLP models.\n2.1 Commonsense Reasoning\nHere, we introduce the most relevant work, including large knowledge\ngraphs, benchmark datasets, and commonsense reasoning methods.\nLarge-scale knowledge graphsact as the representation of com-\nmonsense knowledge for NLP models to access and exploit. The\ncommonsense knowledge graphs (CSKGs) can be divided into two\ncategories, which are human-annotated CSKGs (e.g., ConceptNet [59],\nATOMIC [22, 53], and GLUCOSE [44]) and web content extracted\nCSKGs [64,76]. ConceptNet [59] is a comprehensive large-scale knowl-\nedge graph with over 3.4M entity-relation tuples to connect concepts\n(words and phrases) by 36 types of relations. It primarily focuses on\ntaxonomic, lexical, and physical knowledge. It is collected by crowd-\nsourcing and merged with high-quality knowledge databases. Here, we\nuse ConceptNet to reveal commonsense knowledge in data instances\nand contextualize model behavior.\nBenchmark datasetsfor evaluating NLP models’ commonsense\nreasoning abilities typically involve question-answering tasks, reading\ncomprehension [21, 75], open-ended question answering [9, 32], and\nmultiple-choice questions [7, 52, 54, 57, 63]. One example is Comon-\nsenseQA (CSQA) dataset, which consists of 12k commonsense ques-\ntions authored by crowd workers in a 5-way multiple-choice format.\nAmong the five answer choices, three are directly extracted from Con-\nceptNet, with one being the correct answer. Then the crowd workers\ncreate two additional distractors, one from ConceptNet and another\nauthored by themselves. CSQA evaluates models mainly on factual and\nphysical commonsense relations (e.g., atlocation) between entities.\nAnd we use CSQA to showcase how our system enables scalable and\nsystematic analysis of NLP models’ commonsense reasoning abilities.\nCommonsense reasoning modelsinclude large language models\n(LLMs) [11,26,27,35,48] pretrained on large text corpora. They achieve\nimpressive performance on commonsense benchmarks. Nevertheless,\nthese models exhibit limitations in their capacity to possess and effec-\ntively utilize commonsense knowledge for reasoning tasks [6, 37]. To\nenhance models’ commonsense knowledge, some methods [31, 73, 74]\nintegrate external knowledge bases and/or linguistic theories into the\nmodels to provide more contexts and facts for improving model accu-\nracy. In addition, pretrained language models can be used as knowledge\nbases to generate clarification questions [56], commonsense explana-\ntions [49], and prompts [34] to enhance commonsense reasoning. How-\never, they do not explain what commonsense knowledge is injected\n(un)successfully. And we present a model-agnostic explanation system\nto systematically evaluate commonsense knowledge that these NLP\nmodels possess and utilize for reasoning tasks.\n2.2 Explainable AI Techniques for NLP\nExplainable AI (XAI) is critical to promote model transparency and\nreliability [4]. We focus on post-hoc model explanations via a model-\nagnostic approach. One popular post-hoc explainability technique is\nfeature attribution [36, 51, 61], which quantifies the contribution of\ninput features to the model output. We use a model-agnostic method,\nSHAP [36]. Another related direction is counterfactual analysis [25,72],\nwhich uses examples to reverse the target label, helping understand\nmodel decision boundaries. Our system enables question manipulation\nto probe model behavior regarding specific concepts or relations.\nWhile previous work [28, 77] has explained Natural Language Pro-\ncessing (NLP) models via zero-shot or few-shot accuracy evaluations\non pre-trained language models, our research conducts a detailed, sys-\ntematic analysis of model behavior on diverse commonsense concepts\nand relations. Compared to other methods [13, 46, 50, 65] that design\nauxiliary classification tasks to understand linguistic knowledge in NLP\nmodels, we aim to reveal the role of commonsense knowledge in the\nmodel’s reasoning process. Directly prompting large language models\nhelps probe the simple facts embedded in them. However, many NLP\nmodels [27, 31, 35] cannot be easily prompted due to their inherent\ndesigns. Finally, some studies [14, 40, 41] conduct causal analysis that\nattributes a piece of knowledge to specific neurons in the models. How-\never, these methods do not efficiently summarize how a model learns\ndifferent commonsense knowledge. Our system adopts the model-\nagnostic feature attribution method to quantify model behavior and\n2\nTo appear in IEEE Transactions on Visualization and Computer Graphics.\ncontextualizes it with a knowledge graph. It then employs multi-level\nvisualizations to facilitate systematic exploration of model behavior\nacross various commonsense concepts and relations.\n2.3 Visualization for Understanding NLP Models\nVisualizations can effectively help understand NLP models [1]. Model-\nspecific visualizations [20, 24, 42, 60, 67] reveal the model’s inner work-\nings, such as the behavior of neurons, layers, and attention maps. By\nexamining these visualizations, users can gain insights into hidden state\ndynamics [60], the relationships between hidden states and words [42],\nand diagnose model bias [67].\nMany model-agnostic visualizations [2, 8, 17, 30, 69, 71] focus on\ninput-output model behavior and are generally applicable to different\nmodels. For example, What-If Tool [71] allows users to understand\nmodel behavior concerning feature importance, different inputs, and\nother hypothetical situations. M 2lens [69] characterizes intra- and\ninter-modal interactions learned by multimodal models. Shared Inter-\nest [8] compares the reasoning of models and humans using saliency\nresults and ground truths. DeepNLPVis [29] and MultiViz [30] present\nmulti-level visualizations to explore both the behavior and working\nmechanisms of different NLP models across different tasks.\nHowever, these studies do not provide insights into the commonsense\nknowledge that models may (not) learn. To fill the gap, we propose a\nmodel-agnostic approach that leverages multi-level visualization and\nan external knowledge base to contextualize the implicit reasoning of\nmodels over concepts and relations in commonsense questions.\n3 D ESIGN REQUIREMENTS\nWe aim to develop a visual analytics system to help NLP experts un-\nderstand and diagnose commonsense reasoning capabilities of NLP\nmodels in a systematic and scalable manner. Explaining such model\nabilities helps users determine whether models are suitable and trust-\nworthy for downstream applications and enhance specific knowledge\nthat models do not learn well. However, it is challenging to depict\nand summarize the vast and complex space of commonsense knowl-\nedge that models learn, as it is not directly presented in the input, and\nconcepts are entangled with various relations and contexts.\nWe first conducted a literature review on explainability techniques\n[25, 36, 51, 72] and visual analytics [8, 19, 23, 71] for NLP, and com-\nmonsense reasoning [13, 59]. To further characterize users’ common\npractices and needs, we collaborated with three NLP experts (E1-E3,\nE1 is the coauthor) through regular weekly meetings for about six\nmonths. E1 is a Ph.D. candidate who investigates commonsense knowl-\nedge acquisition and reasoning. E2 has obtained a Ph.D. degree in HCI\nand has rich experience in building human-centered interactive NLP\nmodels. And E3 is a research scientist from an international media\ncompany whose expertise is in explainable AI and visualization for\nNLP. During the meetings, we asked them about 1) the general methods\nof NLP model evaluation; 2) what types of explanations for models’\ncommonsense reasoning capabilities; and 3) the desired system task\nsupport. Meanwhile, we developed our system prototypes iteratively\nand collected their feedback for further improvement.\nCurrent practice and limitations.Our users usually start with per-\nformance metrics (e.g., accuracy) to locate data instances (esp., wrong\npredictions) and manually summarize what commonsense knowledge\nis needed for inference. Specifically, users identify important relations\nand concepts for commonsense reasoning and combine performance\nmetrics with feature attribution methods to determine whether models\ncapture important concepts or superficial word associations. Moreover,\nthey can probe the models by modifying the data instances to verify\ntheir hypotheses. However, this analysis process is tedious, mentally\ndemanding, and difficult to generalize to larger data subsets. They\ndesire a visual analytics tool to analyze what commonsense knowledge\nis contained in data instances and (not) learned by NLP models.\nR1. Reveal commonsense knowledge in data instances.Our\nusers need to distill the external commonsense knowledge from data\ninstances, which helps verify if model behavior aligns well with human\nknowledge [8,23]. Since concepts and relations are critical components\nof commonsense knowledge [13,59], the system should extract relevant\nconcepts and their relations in questions as references to understand\ndata itself and model behavior:\nQ1: What concepts (e.g., entities) are mentioned in the instances?\nQ2: What are the latent relations between the mentioned concepts?\nR2. Summarize model performance on different concepts and\nrelations. Our users usually depend on accuracy scores to pinpoint\ncases where models fail and prioritize exploring them. To scale up\nthe analysis of individual instances to large datasets, it is necessary to\nsummarize model performance from multiple aspects [8, 19, 71].\nE3 said that a concept-driven summary can reveal what topics mod-\nels perform well. E1 mentioned that compared to the vast concept\nspace, relations have more summative power and connect concepts\nmeaningfully. E3 suggested relating model performance to linguis-\ntic contexts to assess their ability to use commonsense knowledge in\ndifferent situations. For instance, testing models on instances where\nadults and children use staplers helps understand whether models can\ndistinguish between them. Therefore, the system should answer:\nQ3: What concepts, relations, and their combinations are predicted\nright or wrong by the models?\nQ4: What are the contexts of the relations and concepts? What is\nthe model performance?\nR3. Infer model relational reasoning over concepts based on\nrelevant commonsense knowledge.To develop a mental model about\nmodels’ commonsense knowledge and reasoning, users need to first\nuse their own prior knowledge to build the relevant reasoning paths\nthat connect important words in statements. Then, they need to check\nwhether models capture these meaningful concepts in statements based\non their importance to model predictions. Although sometimes models\nare correct, they may rely on task-unrelated linguistic features (e.g.,\nstop words) to make decisions. Moreover, E2 and E3 thought that\nto better surface the patterns of how models regard unmentioned re-\nlations, it is necessary to show whether models attach importance to\nthe mentioned words in statements connected by those relations. By\nconcept-driven comparison between important concepts recognized\nby models and humans, users can generate hypotheses about models’\nrelational reasoning over concepts:\nQ5: What concepts are important for model predictions? Are they\nreasonable?\nQ6: What unexpressed relations are necessary for inference? Do\nmodels cover the concepts connected by these relations?\nQ7: What are the differences between the important concepts for\ncommonsense reasoning and for model predictions?\nR4. Allow interactive probing and editing of NLP models.One\nstraightforward and useful way to understand and debug models is\ninteractively interrogating them [25, 71, 72]. To generate and verify the\nwhat-if hypothesis about model behavior, users can conduct counter-\nfactual analysis by manipulating specific input components and seeing\nhow models react to these changes. This helps disentangle influences\nof individual concepts in statements for model predictions and check\nwhether models are biased towards some concepts. Moreover, modi-\nfying the input components can test the robustness of models against\nnoisy concepts and probe the underlying relations of interest that link\nthe mentioned concepts in the input. After model probing, users may\ndesire to conduct posthoc editing of model behavior to inject their\ndesired knowledge and make a flexible localized update about specific\nknowledge areas that models do not learn well [15, 43].\nGiven the requirements, we consolidated a series of system tasks\nthat guides the systematic exploration of models’ commonsense rea-\nsoning capabilities: Initially, commonsense knowledge from data is\nextracted as concept-relation triplets (R1). Then, the system summa-\nrizes model performance across these concepts and relations (R2), and\nfurther assesses the overall relation learning ( R3). Next, it enables\nusers to pinpoint error instances related to specific concepts or relations\n(R2). For these instances, the system summarizes the concepts con-\nsidered important by models in varying contexts, aligning them with\nthose in the extracted triplets (R3). Moreover, visualization is utilized\nin conjunction with model probing (R4) to comprehensively explain\nmodels’ input-output behavior on these instances (R3). Finally, users\ncan bookmark instances for targeted model refinement (R4).\n3\n4 S YSTEM & METHODS\nOur system, CommonsenseVIS, leverages an external knowledge graph\nto summarize and derive commonsense knowledge and facilitate multi-\nlevel exploration and diagnosis of model behaviors in commonsense\nquestion-answering tasks. We focus on question-answering tasks be-\ncause they are a common evaluation method for natural language un-\nderstanding, and most commonsense reasoning benchmarks adopt the\nQA format [7, 27, 52, 54, 62, 63].\n4.1 System Overview\nFigure 2 provides an overview of our system. CommonsenseVIS takes\nin QA instances and an NLP model to compute model answers. Then,\nit identifies important concepts (i.e., words) in questions using feature\nattribution methods and extracts relevant commonsense knowledge\nfrom input data using an external knowledge base. This knowledge\nhelps align the model behavior with ConceptNet. The user interface\nenables multi-level exploration, interactive probing, and editing.\n4.2 System Data & Model\nHere, we introduce the system input, including the QA data, model,\nand external knowledge base for contextualizing model behavior.\nQA data.Each QA instance contains a question concept, a target\nconcept (i.e., answer), alternative answers (if any), and a question stem.\nFollowing the previous commonsense QA benchmarks [53, 62, 63],\nconcepts are defined as words, and question stems provides contexts\nfor the commonsense relations between the question and target concepts.\nAs shown in Figure 2A, the question concept is air conditioning, the\ntarget concept is house, and air conditioning is located at the house.\nAnd the context in the question stem is: “A man...watches the game\non Saturday...”. If a question concept is absent, knowledge graph\nembedding methods [10] can be used to determine the relation strength\nbetween the target concept and words in the question stem. The word\nwith the highest score becomes the question concept [31].\nWe utilize one representative commonsense QA benchmark,\nCSQA [62], for demonstration. The dataset has 12,102 multiple-choice\nquestions, covering diverse topics and various forms of commonsense.\nEach human-authored question contextualizes relations between a ques-\ntion concept and a target concept (i.e., the correct answer among five\ncandidates). Triplets of these concepts and relations are drawn from\nConceptNet [59]. The most frequent question concepts are about peo-\nple, water, and animals, probing various relations such as spatial (41%)\nand causal (23%). Questions are formulated in diverse forms (e.g.,\nwh-questions, statements, and hypotheses) with 13 words on average.\nQA model.Our system is designed to accommodate various NLP\nmodels that select answers to given questions, as it focuses on the\ninput-output model behavior and we can adopt model-agnostic feature\nattribution methods to quantify this behavior.\nFor the purpose of system demonstration, we have chosen Uni-\nfiedQA1 [26,27] as an example for model analysis. It is an open-source,\ngeneral QA model that has been pre-trained across various QA datasets,\nshowing great generalization capabilities. We use SHAP to compute\nthe importance scores of model inputs because of its strong theoretical\nfoundation and widespread adoption in various domains.\nCommonsense knowledge base.We utilize an external knowledge\nbase to capture the commonsense knowledge in the QA data, which\nprovides context for inferring the model’s implicit reasoning. To ensure\nmeaningful and helpful context for model analysis, the knowledge base\nmust sufficiently cover relevant commonsense reflected in the QA data.\nWe adopt ConceptNet [59] as an external resource, a large-scale\ncommonsense knowledge graph connecting concepts (i.e., words) with\nrelations. The graph integrates diverse knowledge sources with over 8\nmillion nodes and over 21 million links. Particularly, it uses 36 general\nrelations (e.g., “IsA”, “UsedFor”) to connect words, mostly covering\ntaxonomic, lexical knowledge, and physical commonsense knowledge.\nConceptNet is widely used to enhance NLP models with commonsense\ncapabilities [18,31] and build reasoning benchmarks [33,62,63]. For ex-\nample, the questions and answers in CSQA are based on word-relation\n1https://huggingface.co/allenai/unifiedqa-v2-t5-large-1363200\ntriplets (A, Relation, B) from ConceptNet. The prevalent relations\ninclude AtLocation (A is typically located at B), Causes (A is the\ntypical cause for B), and CapableOf (A can typically do B). Moreover,\nover 98% of words in CSQA questions are covered in ConceptNet.\nTherefore, ConceptNet is a suitable resource for contextualizing model\nbehaviors on CSQA and other commonsense QA datasets [18].\n4.3 Extract Relevant Commonsense Knowledge\nTo help users build a concrete understanding of commonsense ques-\ntions and their connections with model behavior, we distill relevant\ncommonsense knowledge in data instances based on ConceptNet (R1).\nThe commonsense knowledge extraction consists of two major steps\n(Figure 2B), including recognizing mentioned concepts in the questions\nand constructing sub-graphs on the concepts. To reflect the reasoning\npaths from the question concept to the target concept/answer, we per-\nform tokenization of the question stem by n-gram ( n = 1,2,32) and\nmatch the tokens (i.e., words of length n) with the concepts in Concept-\nNet to identify a set of candidate concepts for commonsense reasoning.\nSince the matched concepts (with different lengths) may have overlaps,\nwe reduce the redundancy by keeping the longest matched concepts\nin ConceptNet. Moreover, to enhance the robustness of matching, we\nconduct soft matching by lemmatization and removal of stop words\nand punctuations. For example, after token matching, the candidate\nconcepts in a question “A man wants air conditioning, ...” will be\n{man, want, air conditioning, ...}. Next, those tokens are used to con-\nstruct a knowledge graph that contains the question concept and the\ntarget concept to describe the reasoning process. By leveraging the\nconnections among the candidate concepts, question concept, and tar-\nget concept in ConceptNet, we establish relational paths, employing\na two-hop relation search. We set the hop size to two to balance the\ncomputation scalability and coverage of reasoning paths, following the\nprior work [31, 74]. Thereafter, the resulting graph of concepts and\nrelations (in Figure 2B) describes the relevant commonsense knowledge\nfor the question. This graph is referred as ConceptNet knowledge.\n4.4 Align Model Behavior with ConceptNet Knowledge\nTo help users build mental models about the model’s relational rea-\nsoning over concepts, we align the model input-output behavior with\nConceptNet knowledge regarding different concepts and relations (R3).\nFor concept alignment (Figure 2C), SHAP is used to calculate the im-\nportance scores of the input concepts to the model outputs. And we call\nthose with large positive influences on the model predictions as model\nconcepts. Then, we compute the differences between the set of model\nconcepts and the set of ConceptNet concepts (i.e., question concepts and\nconcepts in question stems derived in Section 4.3). For relation align-\nment, we mainly consider the key relations (i.e., the relations between\nquestion concepts and target concepts) for correctly answering the\nquestions (Figure 2C). Noticing that question concepts are included in\nquestion stems asmodel inputs and target concepts are ground truths for\nmodel outputs, we surface the model learning of their relations by inves-\ntigating the relationships of model inputs and outputs. Specifically, the\ninputs and outputs are high-dimensional embeddings that the model op-\nerates on. And we compute the linear transformation matrixW ∈ Rd×d′\nbetween model input embeddings X ∈ RN×d and output embeddings\nY ∈ RN×d′\n. Particularly, to reflect relations between question concepts\nand target concepts encoded in W, we use those correctly-predicted\ninstances (i.e., model predictions P are equal to target concepts) as the\nanchor points for the transformation. And we adopt a least-square error\nobjective to compute the linear matrix W: argminW∈Rd×d′ ||XW −Y ||2,\nwhere (X,Y ) ={(xi,yi) | TCi = Pi}, i = 1,..., N.\nThe general idea is that the input-output relationships can be modeled\nby translations in the model embedding space [10, 16]: if a model can\ncapture the relations between question concepts and target concepts,\nthen question concept embeddings transformed with the matrix W\nshould be close to target concept embeddings.\n2To balance the coverage of meaningful phrases with varying lengths and\ncomputational complexity, we limit maximum gram size to be three [38].\n4\nTo appear in IEEE Transactions on Visualization and Computer Graphics.\nQA data\nquestion concept\nUni/f_iedQA\nquestion stem\nchoices\nA man wants air conditioning while we watches the game on \nSaturday, where will it likely be installed?\nair conditioning\ntarget concept\nA car B house C oﬃces D park E movie theatre\nhouse\nModel\nmodel results E movie theatre\nKnowledge graph\nConceptNet\nCommonsense knowledge extraction\nhouseair conditioning\nDesires\nHasSubevent\nAtLocation\nSimpli/f_ied Contexts subgraph\nman\ninstall\nwatch game on\nSaturday\nCapableOf\nCreatedBy\nCapableOf\nFeature importance\nModel behavior contextualization\nconcept alignment\nConceptNet concepts Model concepts\nman\n watch game\nwhile\n likely\nSaturday\n install\nwant\nair conditioning\nthe will\non what\nit\nOriginal question\nstems (QS)\nOriginal target\nconcepts (TC)\nTransformed QS Tranformed TC\ntransformation\n(AtLocation)\nInteractive visualization\nModel probing Model editing\nA man wants air conditioning while we watches \nthe game on Saturday, where will it likely be \ninstalled?\nrelation alignment\nA B C D\nFig. 2: The system overview. (A) First, the QA data, NLP model, and knowledge graph are input into the system. (B) Then our system extracts\ncommonsense knowledge in data input based on ConceptNet, and calculates the feature importance scores for individual words in questions. (C)\nWe then align model behavior with ConceptNet knowledge regarding various concepts and underlying relations. (D) Finally, the results are integrated\ninto the interactive visualization system with extended support of model probing and model editing.\n4.5 Model Editing\nAfter identifying model deficits in specific commonsense knowledge,\nwe present editor networks to modify model parameters that can\ncorrect problematic model answers ( “reliability”), as well as other\nsemantically-equivalent questions (“generality”) without affecting un-\nrelated knowledge much ( “locality”). Particularly, editor networks\nare neural networks trained to modify model parameters (from θ to\nθ′) with the objectives that maximize editing accuracy on both edit-\ning targets ( xe,ye) and their equivalence ( x′e,y′e) while minimizing\ndifferences (KL divergence) in model predictions on locality exam-\nples (xloc,yloc) before and after the edits: Le = −logp′\nθ (y′e|x′e),Lloc =\nKL(pθ (·|xloc)||p′\nθ (·|xloc)). The total loss is Ltotal = −we ·Le + Lloc,\nwhere we is a weight factor. The editing examples come from QA\npairs in CSQA train/val set, where their equivalences are generated\nby popular data augmentation techniques, i.e., back-translation and\nEDA [70]. Locality examples are independently sampled. We adopt\ngradient decomposition techniques [43] to train editor networks on the\nlast two transformer layers of the model. More technical details are\nincluded in Suppl. A.\n4.6 User Interface of CommonsenseVIS\nThe user interface (Figure 3) enables a multi-level exploration of model\nbehavior following an overview-to-detail flow, contextualized by Con-\nceptNet. The exploration process starts with the Global View, which\nsummarizes model performance on different concepts and relations and\nassesses overall relation learning. Users then can pinpoint error cases,\nand the system summarizes the contexts of alignment between model\nbehavior and ConceptNet on different subsets. Upon selecting instance\nsubsets in the Global View or Subset View, Instance View shows statis-\ntics and visual explanations for these instances. It facilitates interactive\nmodel probing for comprehensive understanding and enables users\nto bookmark particular instances for targeted model refinement. The\nsystem uses red to encode the model error, green to indicate accuracy,\ncategorical colors to encode different relations and statistics.\n4.6.1 Global View\nInitially, users can refer to the Global View to gain an overview of\nthe model performance regarding different concepts and commonsense\nrelations contained in QA data (R1, R2). Specifically, the Global View\n(Figure 3A) adopts different projection strategies to group question\nstems and target concepts (i.e., answers) and visualize them as two\nseparate scatter plots. For projection, we choose UMAP [39] with\ncosine similarity measures to cluster model embeddings for question\nstems and target concepts because of its good processing speed and\npreservation of embeddings’ global structure. After projection, similar\nquestion stems (i.e., similar contextualizations of question concepts)\nor target concepts are close to each other. To further analyze error\nand relation distributions among these instances, users can adjust the\ndot color schemes at the header. When the “Correctness” scheme is\nselected, dots are colored in red and green to show distributions of\nincorrect and correct instances. Alternatively, selecting the “Relation”\nscheme applies categorical colors to the dots, highlighting instances\nwith different relations. Meanwhile, users can change projection mode\ninto “Correctness” or “Relation” at the header to accentuate the dif-\nferences between instances with high and low errors or instances with\nvaried relations. To achieve this, we utilize instance correctness and\nrelations between question concepts and target concepts as additional\nlabels for UMAP to perform supervised dimension reduction for clear\ncluster separation in the scatter plots. To mitigate the overplotting in the\nscatter plots, the system supports semantic zooming that allows users\nto navigate specific areas of interest (e.g., error instances) within dense\ndata points. Moreover, users can filter out the instances with particular\nrelations by clicking the rectangles between the two scatter plots, where\neach rectangle encodes relation frequency and accuracy. For each rect-\nangle, we use green (instead of categorical colors) to emphasize model\naccuracy for that relation, where the width of the green bar denotes\naccuracy, and its height corresponds to relation frequency. The system\nsorts these rectangles by relation frequency, allowing users to prioritize\nmodel performance exploration of more prevalent relations.\nBesides, the Global View assesses how the model regards latent\nrelations between questions and answers ( R3). In the “Relation X\nTransformed” projection mode (in Figure 3), the Global View separates\ninstances with different relations in the scatter plots and supports the\nalignment and comparison of transformed question stems with target\nconcepts. If there is a good correspondence between transformed\nclusters of question stems and target concepts in the scatter plots, then\nthe relations between question and target concepts could possibly be\nlearned. Finally, users may lasso a group of instances or click specific\nrelation bars to inspect the context summary in the Subset View.\nAlternative design. We have considered an alternative—grouped\nbar charts—to visualize the relations between question and target con-\ncepts (Figure 4A). For each relation, green bars show accuracy while\nblue bars encode frequency. The longer the bars, the larger the encoded\nvalues. We collected experts’ feedback on this alternative. E1 said\nthat our final design using a single color looks simpler and cleaner. E3\ncommented that horizontally aligning green bars next to blue bars in the\ngrouped bar charts could be confusing since the frequency and accuracy\nhave different units of measurement. E2 reported that our final design\ncan reflect the proportion of different relations in the whole dataset\nmore clearly. In addition, it sorts the frequent relations in descending\norder, helping prioritize the exploration.\n4.6.2 Subset View\nAfter selecting a group of instances with specific concepts or relations\nin the Global View, users can utilize the Subset View to explore the\nconcept alignment between the model behavior and ConceptNet knowl-\n5\ncauses Accuracy: 0.73\nClicked\n feel, center, senior, tv, energy, analyse, bus, stabbing, being_arrested, headRelated concepts:realising_energy Causes\nBeforeeditAfteredit\nUser can edit to probe model behavior \nA\nB\nDC\n“causes”undercorrectnesscolorscheme\nFig. 3: The user interface of CommonsenseVIS: The Global View (A) summarizes the model performance by the projection plots of question\nstems and target concepts and the relations between them. The Subset View (B) summarizes the context alignment between model behavior and\nConceptNet knowledge over different subsets. The Instance View (C) provides the statistics and detailed local explanations of instances selected\nusing Global View or Subset View. The current instance is highlighted as larger points in the Global View. The Instance View also enables users to\nprobe the model by editing the questions directly. Furthermore, users can bookmark instances and edit the model in the Model Editing Panel (D).\nFig. 4: Alternative designs in the system for QC-TC relations visualization\n(A) and the cluster glyph in the Subset View (B).\nedge across different subsets (R2). This view employs cluster glyphs\nto analyze model behavior across instances with semantically similar\nquestion concepts, question stems, and target concepts. Hierarchical\nclustering of ConceptNet Numberbatch embeddings [59] is performed\nfor question stems, question concepts, and target concepts. We use Con-\nceptNet Numberbatch embeddings because they encode word meanings\nbased on ConceptNet’s semantic network and perform well on word-\nrelatedness benchmarks [59]. Then, users can scan through the cluster\nglyphs and sneak peek into the corresponding model performance, the\nimportant words for model decisions, and how they are aligned with\nConceptNet concepts (Figure 4B). For each cluster glyph, two bars\nare presented at the top showing the average accuracy (between 0 and\n1) and overlap ratio (between 0 and 1) between model concepts and\nConceptNet concepts. The lower parts display the differences between\nthe model and ConceptNet concepts. The first row displays the top\nConceptNet concepts frequently missed by the model. And an orange\nbar is put to the left, revealing the frequency. Then, the second row\nshows the frequent model concepts and their frequency (with blue bars).\nTo further explore concept associations across different questions, and\nquestion and target concepts, their cluster glyphs are connected with\nlinks if their data instances overlap—the wider the link, the greater\nthe shared data instances. To reduce the visual clutter of links, the\nsystem allows users to adjust the cluster numbers at the header. When\nusers hover over a specific cluster glyph, the system highlights only the\nconnections relevant to that cluster, while keeping other links hidden.\nAlternative design. Initially, we considered using a word cloud\n(Figure 4B) to summarize the most frequent concepts (not) covered\nby the model. And the word size relates to frequency. However, the\nword cloud is not space-efficient and mixes the model concepts with\nConceptNet concepts and increases the visual complexity, making\nthe system less user-friendly. More importantly, our users prioritize\nreading the concept words in plain style. Therefore, we chose our\ncurrent design.\n4.6.3 Instance View\nAfter selecting instances in the Global View or Subset View, the In-\nstance View (Figure 3C) provides statistics and local explanations about\nthe model. It enables probing of the model with different inputs and\noutputs to test its learning of concepts and commonsense relations (R1,\nR4). The top stacked bars show model accuracy and average question\nconcept (QC) hit ratio (between 0 and 1). Users can click the green\n(or gray) segments with the stacked bars to filter the data instances\ncorrectly (or wrongly) predicted. The histogram below displays the\ntop frequent concepts considered important to the model. Users can\nexplore individual instances and model explanations with pagination.\nThe question stems that strongly contribute to model outputs are high-\nlighted with green backgrounds, and question concept is underlined.\nModel choices colored red indicate a wrong answer, and the ground\ntruth is colored green. Users can verify and generalize their findings by\nsearching for linguistic patterns in data instances that contain certain\nwords or structures (e.g., “many NOUN”) at the top. For instance, after\nsearching a question concept of interest, users can review the model\nperformance on different contextualizations (i.e., question stems) of\nthat concept and associated relations in the Global View. Then, further\ndetail can be investigated, including statistics and model explanations\nfor individual instances, in the Instance View.\nFor individual instances, users can edit them to form and validate\nhypotheses about the model learning of relations. For example, if\nthe model is wrong, users may hover over different answer choices\nto see their relations with ConceptNet concepts in question stems.\nIf both model answers and target concepts share the same relations\nwith question concepts, the model potentially does not understand the\ncontexts. Then, users can edit the text content of question stems and\nindividual answer choices (e.g., remove some words in questions and\nchange answer choices), followed by re-running the model on the edited\nQA pairs. The new model answers will be highlighted in blue. By\nexamining the new results, users can validate whether the relations\nbetween the question and target concepts are learned.\nUsers can bookmark instances about specific knowledge that the\nmodel does not learn well. Then, they can conduct model editing in the\nModel Editing Panel (Figure 3D), where information about questions,\nrelations, ground truths, and model results are summarized in a table.\nUsers can apply editing to instances of interest and inspect the editing\nperformance. Moreover, they can load the edited model for exploration.\n5 E VALUATION\nWe conducted a user study to evaluate how CommonsenseVIS helps ex-\nperts analyze the commonsense reasoning abilities of language models.\n6\nTo appear in IEEE Transactions on Visualization and Computer Graphics.\nSpecifically, we invite 10 experts (E4-E13) to evaluate our system on a\ncommonsense reasoning benchmark. The experts are NLP researchers\nor practitioners, and all of them have rich experience in natural language\nunderstanding topics. We delay the introduction of their backgrounds\nin Section 5.2. We evaluate the system by using a state-of-the-art\nQA model UnifiedQA [27] and the CSQA [62] validation set. The\nCSQA validation set contains 1,221 multiple-choice commonsense QA\ninstances, and the model performance is 71.00%. We also randomly\nsampled 100 instances from the validation set to evaluate the com-\nmonsense coverage of ConceptNet for CSQA. For each instance, an\nNLP expert (E14, not a co-author) from a tech company assessed if the\nrelational paths extracted from ConceptNet accurately covered the nec-\nessary commonsense knowledge to answer the questions. The results\nshow that ConceptNet knowledge covered the necessary commonsense\nin 91 out of 100 instances. More details are in Suppl. C.\nNext, we present cases of usingCommonsenseVIS for model analysis.\nThe cases were found by E4 and E5 during their system exploration of\nmodel behavior in the user study. Afterward, we summarize the user\nbehaviors under the characterized system workflow. And we report\nusers’ ratings and feedback on the system designs and workflow.\n5.1 Cases of Using CommonsenseVIS\nUsing CommonsenseVIS, experts discovered that the model has learned\nthe relation atlocation in the context of “office” and “room” properly\n(details are in Suppl. D.1). However, it has limitations incause relation\nlearning and “movie” context understanding.\n5.1.1 Reveal Model Deficiencies in Cause Relation Learning\nvia Multi-level Exploration and Instance Editing\nGlobal Summary(R1, R2) E4 first diagnosed the model learning of\ncauses relation by clicking the second largest bar between the scatter\nplots (Figure 3A). He selected “Relation X Transformed” projection\nscheme with the “correctness” coloring to examine the alignment\nbetween question stem and target concept clusters considering model\nperformance (dashed area in Figure 3A). In the left scatter,E4 observed\nthat the transformed question stem projection does not form a neat\ncluster and does not align well with the target concept projection. It\nsuggests insufficient learning of this relation. In addition, there is a\ngroup of outliers at the top. Almost all of them have red color, which\nindicates a low model accuracy. He wondered if it is the poor learning\nof causes relation that causes such high errors. Afterward, E4 lassoed\nthese instances to inspect their details further in the Subset View.\nSubset Exploration(R1, R2, R3) In the Subset View, E4 noticed\nlow model accuracy across all clusters, indicated by short green bars at\nthe top of glyphs in Figure 3B. Moreover, he spotted that question stem\nclusters have long blue bars compared to short green bars, implying\nthat the model considers ConceptNet concepts in question stems but\nstill fails to answer correctly. This strengthened E4’s concerns over the\nmodel learning of the cause relation. He clicked on the first question\nstem cluster (Figure 3B) to explore its instances in the Instance View.\nInstance Exploration and Editing(R1, R4) E4 first clicked the\ngray part of the accuracy bars to focus on the incorrect instances. When\nscanning the model concepts (with stop words filtered) in the histogram,\nE4 found that the model usually attaches importance to words related to\nmood and emotion, such as “happiness”, “exhaustion”, and “boredom”.\nGiven the low model accuracy,E4 surmised that the model is not aware\nof the causes for human emotion. To verify his thought, he explored\nand edited the detailed instances below. He found that most of these\nquestions are very short, directly asking QC-TC relations. For example,\nas shown in Figure 3C, even after simplifying the original questions\nto ask straight at the relation between “releasing built-up energy” and\n“wonderful”, the model still chose the wrong answer “exhaustion”. He\nbookmarked these instances for model editing.\nModel Editing(R4) E4 concluded that emotion-related causes is\nnot sufficiently learned. And he applied model edits on the previously\nsaved instances in the Model Editing Panel (Figure 3D). He found that\nthe editing accuracy is 100% and model performance decrement is\nsmall (i.e., 71.01%-70.93%=0.08%). He was satisfied with the edits.\n5.1.2 Probe Model Limitations in Understanding Relation Con-\ntexts via Instance Exploration, Editing, and Querying\nGlobal Summary\nSubset Exploration\nlassoed\nInstance Level Probing\nmodel result\nafter lasso\n edit\nDModel Editing\nModel Editing\nA\nB\nC\nD\nFig. 5: The workflow E5 performed in case two: (A) E5 was interested in\nthe border area and lassoed these points for further inspection. (B) E5\nnoticed a cluster with lower accuracy and clicked to inspect further.\nGlobal Summary(R1, R2) Another expert E5 used our system\nto investigate under what circumstances the model might fail to use\ncontexts for relational reasoning over concepts. He first chose the\nlargest relation group (i.e., atlocation, the first green bar) in the\nGlobal View (Figure 5A). And he found good correspondence between\nquestion stem and target concept clusters under the “Relation X Trans-\nformed” projection scheme. It implies good learning of atlocation\nin general. E5 wondered when the model might fail to reason about\ncontexts. Then, he noticed that a group of dense red dots appear at\nthe bottom left (Figure 5A). The accuracy of this group is low, and E5\ndecided to lasso the group for further exploration.\nSubset Exploration(R1, R2, R3) In the Subset View, E5 noticed\nthat QSs fall into three clusters with varied accuracies (as suggested\nby the green bars) (Figure 5B). Particularly, he was interested in the\nleftmost cluster since it has the lowest accuracy yet a similarly high\nquestion stem hit ratio (also indicated by tall dark blue bars at the\nleft), compared to the other two. As he hovered over the cluster glyph\n(Figure 5B), he discovered that the top model concepts are not so mean-\ningful (e.g., a, the, to, you). He speculated that the model frequently\nrelies on superficial information in question stems for answering the\nquestions. E5 then clicked the cluster to explore the instances and\nmodel explanations.\nInstance Manipulation (R1, R4) In the Instance View, E5 was\ncurious about the incorrect instances and thus clicked the gray parts\nin the bar of model accuracy at the top to filter them. When exploring\nthe cases below, he found several interesting ones whose contexts\nassociate with “movie”. For example, in one question (Figure 5C),\nE5 found that although “air conditioning” can also locate at “movie\ntheatre”, in this case, the model ignores the important context “watch\nthe game” (without a green background), which normally happens in\n“house”. Then, he further modified this instance to verify his finding.\nSpecifically, through several edits around “watches” in the original\nquestion (Figure 5C), the model still chooses “movie theatre” even\nthough those contents such as television or live shows usually do not\nhappen at “movie theatre”. Therefore, E5 thought that the model\nattaches superficial information of “watch” to “movie” and does not\nunderstand the contexts. In addition, other similar cases were observed\nwhere the model chooses “movie” without understanding what normally\ndoes not occur when watching movies, such as “curtains drawing back”\nor “audiences clapping”.\nInstance query & model editing(R4) E5 concluded that the model\nprobably does not understand the contexts around “movie” well. He\nthen located the related instances by searching keywords “movie” in the\nsearch input of Instance View. He added those incorrect instances for\nmodel editing in the Model Editing Panel (Figure 5D). Finally, he saw\nthat the editing accuracy is 100% and the model maintains nearly the\nsame performance as the original version (i.e., 70.84% v.s. 71.01%).\n7\n5.2 User Study\nWe describe a user study that investigates how NLP experts utilize\ndifferent components of CommonsenseVIS to understand and diagnose\nmodels’ commonsense reasoning capabilities. We also summarize their\nfeedback on our system workflow and designs.\n5.2.1 Experiment Design\nParticipants We recruited 10 postgraduate students and alumni (eight\nmales and two females, age: 20-30, referred as E4-E13) from the\ncomputer science department of a local university through emails and\nword-of-mouth. They had at least two years of experience in developing\nand evaluating natural language understanding models in academia or\nindustry. None of the participants had prior involvement in our system’s\ndesign or usage. Each participant received a cash compensation of $13.\nUser tasksParticipants were required to use CommonsenseVIS to\nanalyze UnifiedQA [26] on the CSQA validation set. They needed to\nfinish the following tasks: 1) gain an overview of model performance\nfor different concepts and relations; 2) Find a relation of interest and\nassess the overall model learning of that relation; 3) Find a cluster of in-\nstances in the question stem/target concept scatter plots with large/small\nerrors; 4) Summarize the model behavior on the cluster of instances;\n5) Explore individual instances and infer if the model learns some\ncommonsense to reason about concepts and their underlying relations.\nProcedures The whole study lasted about one hour. It started with\na 20-minute tutorial, where we collected participants’ demographics,\nasked for their permission to use their log data generated during the\nstudy, and introduced the background and the system usage. Afterward,\nparticipants could freely explore the system and get familiar with it\n(15 minutes). Then, they were asked to use our system to finish the\naforementioned tasks. They were encouraged to speak out their hy-\npotheses and findings about the model following a think-aloud protocol\n(20 minutes). During the task exploration, all their user interaction\nactivities (e.g., lasso, clicking, and hovering), together with the times-\ntamps, were automatically recorded. Finally, participants needed to\nfinish a questionnaire about system workflow, designs, and usability on\na 5-point Likert scale. And we collected their post-study feedback on\nthe experience of using CommonsenseVIS.\n5.2.2 Results and Analysis\nWe report the analysis of user log data, the questionnaire, and partici-\npants’ feedback. For user logs, we extracted the frequency and duration\nof individual interactions (e.g., clicking) and aggregated them to derive\nthe total usage frequency and duration of corresponding views.\nModel behavior contextualization and alignmentParticipants\nfound it reasonable and helpful to use ConceptNet to contextualize the\nmodel’s commonsense reasoning abilities on the CSQA dataset.\nMost participants agreed that CommonsenseVIS helped them un-\nderstand the data (MeanQ1 = 4.70, SDQ1 = 0.67), model performance\n(MeanQ2 = 4.60, SDQ2 = 0.70) regarding different types of common-\nsense knowledge, and infer the model’s implicit reasoning over con-\ncepts and their latent relations ( MeanQ3 = 4.40, SDQ3 = 0.70). For\nexample, E6 appreciated the intuitive assessment of overall relation\nlearning by comparing transformed embeddings. E7 mentioned, “...\ngetting overlap between the set of entities mapped in ConceptNet and\nmodel entities (concepts) can explain whether LMs are focusing on the\nright things.” E8 added, “Aggregating the concept hit ratio and model\nperformance over the whole dataset and organizing them by relation is\na great way to understand the model from a global scale.” Moreover,\nparticipants were fairly confident in their findings ( MeanQ3 = 4.40,\nSDQ3 = 0.70). Besides, the extra context information provided by the\nConceptNet helped develop hypotheses about model learning and probe\nthe models’ behavior on specific concepts and relations (E6, E9, E10).\nNevertheless, participants also raised some concerns over using\nConceptNet. E10 said that sometimes the retrieved relations from\nConceptNet are not guaranteed to be true reasoning paths for solving\nthe questions. Similarly, the extracted ConceptNet concepts were\nconsidered generic or not informative for some questions (E8).\nSystem usage analysisParticipants thought that CommonsenseVIS\nsupports a more systematic and scalable analysis of model behavior,\nTable 1: The frequencies and durations of user interactions.\nView Frequency Duration (s)\nGlobal View 59.63 ±32.32 468.48 ±289.01\nSubset View 22.50 ±17.12 66.82 ±60.15\nInstance View 33.38±23.65 212.15 ±204.15\ncompared to conventional analysis of ad-hoc instances. They appreci-\nated the multi-level model explanations, especially the global under-\nstanding of model behavior. As shown in Table 1, the Global View\nwhich provides an overview of model behavior clearly dominates the\nuser interactions. It takes up 51.62% and 62.68% of total system in-\nteraction frequency and duration respectively, exceeding those of the\nSubset View and Instance View by a large margin. Moreover, as shown\nin Figure 6, participants showed great interest in examining model per-\nformance for different relations and projection modes. They spent much\ntime selecting and exploring instances in the scatter plots (indicated\nby lasso and zoom interactions). The Subset View has the least user\ninteractions regarding frequency and duration. Participants usually used\nit to quickly preview the details of different cluster glyphs (indicated\nby “cluster_glyph” interactions). The Instance View was considered\nto represent the traditional analysis of model behavior. Participants\ngenerally used it to check different instances (indicated by “pagination”\ninteraction) and probe the model (indicated by “run_model” interac-\ntion). Also, they spent the longest time searching words and phrases in\nthe Instance View (indicated by “search” interaction).\nPatterns and insightsParticipants also reported many interesting\nfindings about the model behavior and dataset issues. For example,\nthe model was found to rely on spurious correlations to solve many\nquestions. For the question “Minerals can be obtained in what way\nfor a person who avoids leafy greens? (answer: multivitamin)” , the\nmodel attached importance to “minerals obtained” and selects “ore”,\nignoring the important contexts “person” and “leafy green”. Also in\nmany instances, the model just focuses on some irrelevant words like\n“what” and “at” ( E9, E11). E10 suggested that we can convert the\nquestions into statements and check the model behavior change. “We\ncan eliminate those word biases by using counterfactuals.” (E11).\nBesides, participants noticed that some CSQA questions are poorly\ndesigned. These questions have multiple plausible answers or typos\nthat entirely invalidate the whole question instances. For example, E11\nmentioned, for the question “Where is seaweed from?”, the model\noutputs “sea”, while the true answer is defined as “ocean”. However,\nboth “sea” and “ocean” seem correct. And “‘what can a person with a\nwhat can do?’ should’ve been ‘...with a watch... ’”(E7).\nVisual designs and interactionsParticipants generally agreed that\nour system is easy to use, but it required some effort to learn. They\nfound the Instance View to be the most intuitive, followed by the Global\nView. participants found the Instance View the most helpful in diagnos-\ning if the model uses proper information and learns relations between\nquestion and target concepts. They thought that SHAP explanations\nand model probing complement each other to deepen the model un-\nderstanding. The Global View was thought useful in summarizing the\nlearning of relations and concepts, though it was sometimes a bit hard\nto visually align dot clusters between the projection plots due to the\nembedding rotation effect and scarcity of instances. The Subset View\nwas considered the most difficult to understand. But it was considered\nhelpful to compare the model concepts and ConceptNet concepts across\ndifferent subsets. Their ratings and detailed feedback are in Suppl. E.2.\nFig. 6: Average frequencies and durations of system interactions.\n8\nTo appear in IEEE Transactions on Visualization and Computer Graphics.\nSuggestions for improvementE10 desired an automatic zoom-\nin function when lassoing dots in the Global View. E11 suggested\nshowing the neighboring dots when hovering over a dot. E8 proposed\nto summarize concepts that are not covered by both ConceptNet and\nthe model. E7 recommended integrating other knowledge bases (e.g.,\nATOMIC [22, 53]) to broaden the commonsense coverage.\n6 D ISCUSSION\n6.1 Human-AI Alignment with Contextualization\nConsidering that commonsense knowledge is implicit and not explicitly\nstated (in model input), many interpretability techniques [8, 36], which\nrely on existing model input and output, cannot explain models for\ncommonsense reasoning tasks. We introduce an external knowledge\ngraph, ConceptNet, to characterize the commonsense knowledge in\nthe model input with a group of concepts connected by different rela-\ntions. This external knowledge is set as contextual information to align\nmodel behavior with human commonsense knowledge and reasoning.\nGiven the large space of commonsense knowledge, achieving human-\nAI alignment on commonsense reasoning tasks with additional contexts\nis challenging. Our multi-level visualizations enable the exploration\nof model behavior on different concepts and their underlying relations\nin a scalable and systematic way. Moreover, visualizations produce\nactionable insights into what specific knowledge the model underper-\nforms and guides the model probing and editing. With pre-trained\nlanguage models becoming so large and powerful (e.g., ChatGPT), it\nposes significant challenges to understand, diagnose, and adjust model\nbehavior after deployment. CommonsenseVIS presents “exploration-\nexplanation-editing” posthoc analysis pipeline to contextualize and\nalign model behavior with users’ expectations.\n6.2 Commonsense Knowledge Bases for Contextualization\nTo reflect implicit commonsense knowledge in models, we use an exter-\nnal knowledge graph (ConceptNet) as the contextual reference of com-\nmonsense knowledge and align the model behavior with the ConceptNet\nknowledge. Given that ConceptNet is large and comprehensive with\ngood generality and CSQA is built upon ConceptNet, it is reasonable\nand sufficient to use ConceptNet to cover the commonsense knowledge\nin CSQA. Our quantitative evaluation with CSQA examples and quali-\ntative users’ feedback also show that most commonsense knowledge in\nCSQA questions can be grounded in ConceptNet, justifying its use in\nour study. However, ConceptNet mainly contains taxonomic, lexical,\nand physical commonsense. It has limitations in covering other com-\nmonsense knowledge, such as temporal and inferential commonsense,\nthus impacting the effectiveness of model behavior contextualization\nand visualization for understanding models’ true reasoning capabilities\nin these knowledge areas. To support model analysis for more common-\nsense reasoning benchmarks, we can integrate diverse commonsense\nknowledge bases, such as ATOMIC [22, 53], GLUCOSE [44], or large\npretrained language models [56], to contextualize model behavior. It\nis worth noting that to ensure meaningful model contextualization, we\nshould choose a knowledge base that has sufficient and relevant cover-\nage of the commonsense knowledge in the target benchmark dataset.\nBesides knowledge graphs, other types of commonsense knowledge\nrepresentations (e.g., arithmetic and logical operations) can be used to\nimprove the expressiveness of model behavior contextualization.\n6.3 Generalizability and Scalability\nWe showcaseCommonsenseVIS through a state-of-the-art QA language\nmodel (i.e., UnifiedQA) and CSQA dataset. Our system can be im-\nmediately used to explain any other language modelsfor common-\nsense question answering (CQA) since our explanations center around\nthe model’s input-output behavior with a model-agnostic approach.\nMoreover, our system can be used for other CQA datasets. For ex-\nample, Social IQA [54] is a multiple-choice QA dataset about social\ninteractions in everyday events. It is built upon ATOMIC [22, 53]—a\ncommonsense knowledge graph about the causes and effects of differ-\nent events. Therefore, our system can integrate ATOMIC to retrieve\nrelevant causes or events for a given event extracted from questions in\nSocial IQA to contextualize the model’s social commonsense reasoning.\nFurthermore, our model contextualization method has the potential to\nsupport other commonsense reasoning tasks. For instance, for visual\nquestion-answering (VQA) tasks, we can extract concepts (e.g., person,\ndog) in the images. Then, by combining the concepts in the images\nand text questions, we can utilize an external knowledge base to build\na relevant knowledge graph that covers these concepts. The resulting\nknowledge graph can be used to contextualize models’ reasoning. Our\nmulti-level visual designsfacilitate NLP model analysis for tasks like\nmachine translation. The scatter plots of the Global View can summa-\nrize frequent associations and translation errors between source and\ntarget concepts. Aligning embeddings helps assess translation quality.\nThen, the Instance View shows correlations between source and target\nsentences, enabling users to evaluate translation robustness.\nOur approach faces scalability issues due to the computation cost of\nfeature attribution methods, which can take several hours to compute\nSHAP for thousands of instances. To mitigate this impact, we have\nprecomputed and integrated SHAP values into the system to enable\nseamless interactions for posthoc model analysis. To further speed\nup the process, we can adopt faster feature attribution methods (e.g.,\nCXplain [55]) and techniques like data sampling, caching, and parallel\ncomputing. Regarding visual designs, the links between cluster glyphs\nin the Subset View can be cluttered when the cluster number exceeds\nfive, requiring horizontal scrolling to examine different clusters.\n6.4 Limitations and Future Work\nCommonsenseVIS also has some limitations: 1) To extract relevant com-\nmonsense knowledge for answering a question, we build a sub-graph\ncontaining the words in the question stem that are within two hops of\nthe question concept and answer using ConceptNet. However, some\nconcepts in the sub-graph may not so relevant for solving the question.\nAlso, some important concepts could be connected with the question\nconcept through multiple hops. 2) We perform n-gram tokenization to\nmatch the words in a question with the words in ConceptNet. However,\nthis may exclude some longer phrases or sentences (in question stems\nand answers), which affects the relation extraction between the question\nand answer. 3) To reflect the model’s overall learning of relations, we\napply the translation to the input embedding and align it with the output\nembedding. However, it is also possible that after linear transformation,\nquestion and target concepts are not close to each other, but the models\nstill capture the relations between question and target concepts through\nnon-linear transformation. 4) Model behavior probing may lead to\nincorrect model understanding [58]. To mitigate this, the system can\nimprove the reliability of probing [47,68] or integrate multiple explana-\ntion methods to cross-validate the model insights discovered by model\nprobing. Moreover, larger-scale evaluation across different datasets and\nlonger-term user studies with our experts can further validate the model\nunderstanding facilitated by our system.\nIn the future, we can improve the system designs to handle more\ncomplex questions with multiple plausible answers and explanations,\noften influenced by diverse arguments and opinions. Moreover, we can\nimprove the usability of CommonsenseVIS by displaying prediction\nscores for answer choices in the Instance View and enhancing our\nmodel editing methods for larger-scale editing.\n7 C ONCLUSION\nWe presented CommonsenseVIS to help NLP experts to conduct a\nsystematic and scalable analysis of the commonsense reasoning ca-\npabilities of language models. It utilized an external commonsense\nknowledge base to contextualize and visualize the model behavior on\ndifferent concepts and underlying relations from different levels of\ndetail. Users can interactively probe and edit the model behavior to im-\nprove the model’s reasoning abilities in specific knowledge areas. The\nuser study with cases showed the effectiveness ofCommonsenseVIS for\ndiagnosing what commonsense knowledge a language model learns.\nACKNOWLEDGMENTS\nThe authors wish to thank anonymous reviewers for their valuable\nfeedback. This research was supported in part by Hong Kong Theme-\nbased Research Scheme grant T41-709/17N and a grant from MSRA.\n9\nREFERENCES\n[1] A. Adadi and M. Berrada. Peeking inside the black-box: A survey on\nexplainable artificial intelligence (xai). IEEE Access, 6:52138–52160,\n2018. doi: 10.1109/ACCESS.2018.2870052 3\n[2] S. Amershi, M. Chickering, S. M. Drucker, B. Lee, P. Simard, and J. Suh.\nModeltracker: Redesigning performance analysis tools for machine learn-\ning. In Proc. CHI, pp. 337–346. ACM, New York, 2015. doi: 10.1145/\n2702123.2702509 2, 3\n[3] J. Ayoub, X. J. Yang, and F. Zhou. Combat covid-19 infodemic using\nexplainable natural language processing models. Inf. Process. Manage.,\n58(4):102569, 2021. doi: 10.1016/j.ipm.2021.102569 2\n[4] A. Barredo Arrieta, N. DÃaz-RodrÃguez, J. Del Ser, A. Bennetot, S. Tabik,\nA. Barbado, S. Garcia, S. Gil-Lopez, D. Molina, R. Benjamins, R. Chatila,\nand F. Herrera. Explainable artificial intelligence (xai): Concepts, tax-\nonomies, opportunities and challenges toward responsible ai. Inf. Fusion,\n58:82–115, 2020. doi: 10.1016/j.inffus.2019.12.012 2\n[5] C. Bhagavatula, R. L. Bras, C. Malaviya, K. Sakaguchi, A. Holtzman,\nH. Rashkin, D. Downey, S. W.-t. Yih, and Y . Choi. Abductive common-\nsense reasoning. In ICLR, 2020. 1\n[6] N. Bian, X. Han, L. Sun, H. Lin, Y . Lu, and B. He. Chatgpt is a knowledge-\nable but inexperienced solver: An investigation of commonsense problem\nin large language models. arXiv preprint arXiv:2303.16421, 2023. doi: 10\n.48550/arXiv.2303.16421 2\n[7] Y . Bisk, R. Zellers, R. L. Bras, J. Gao, and Y . Choi. Piqa: Reasoning\nabout physical commonsense in natural language. In AAAI, 2020. doi: 10.\n1609/aaai.v34i05.6239 1, 2, 4\n[8] A. Boggust, B. Hoover, A. Satyanarayan, and H. Strobelt. Shared interest:\nMeasuring human-ai alignment to identify recurring patterns in model\nbehavior. In Proc. CHI, pp. 1–17. ACM, New York, 2022. doi: 10.1145/\n3491102.3501965 2, 3, 9\n[9] M. Boratko, X. L. Li, R. Das, T. O’Gorman, D. Le, and A. McCallum.\nProtoqa: A question answering dataset for prototypical common-sense\nreasoning. In Proc. EMNLP, pp. 1122–1136. ACL, Online, 2020. doi: 10.\n18653/v1/2020.emnlp-main.85 2\n[10] A. Bordes, N. Usunier, A. Garcia-Duran, J. Weston, and O. Yakhnenko.\nTranslating embeddings for modeling multi-relational data. In NeurIPS,\nvol. 26, pp. 2787–2795, 2013. 4\n[11] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-\nV oss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler,\nJ. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray,\nB. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever,\nand D. Amodei. Language models are few-shot learners. In NeurIPS,\nvol. 33, pp. 1877–1901, 2020. 1, 2\n[12] H. Chen, G. Zheng, and Y . Ji. Generating hierarchical explanations on\ntext classification via feature interaction detection. In Proc. ACL, pp.\n5578–5593. ACL, Online, 2020. doi: 10.18653/v1/2020.acl-main.494 2\n[13] L. Cui, S. Cheng, Y . Wu, and Y . Zhang. On commonsense cues in bert\nfor solving commonsense tasks. In Findings of ACL: ACL/IJCNLP, pp.\n683–693. ACL, online, 2021. doi: 10.18653/v1/2021.findings-acl.61 2, 3\n[14] D. Dai, L. Dong, Y . Hao, Z. Sui, B. Chang, and F. Wei. Knowledge\nneurons in pretrained transformers. In Proc. ACL, pp. 8493–8502. ACL,\nDublin, Ireland, 2022. doi: 10.18653/v1/2022.acl-long.581 2\n[15] N. De Cao, W. Aziz, and I. Titov. Editing factual knowledge in language\nmodels. In Proc. EMNLP, pp. 6491–6506. ACL, Online and Punta Cana,\nDominican Republic, 2021. doi: 10.18653/v1/2021.emnlp-main.522 3\n[16] G. Dinu and M. Baroni. Improving zero-shot learning by mitigating the\nhubness problem. In ICLR, 2015. 4\n[17] N. Feldhus, A. M. Ravichandran, and S. Möller. Mediators: Conversational\nagents explaining nlp model behavior. arXiv preprint arXiv:2206.06029,\n2022. doi: 10.48550/arXiv.2206.06029 3\n[18] Y . Feng, X. Chen, B. Y . Lin, P. Wang, J. Yan, and X. Ren. Scalable\nmulti-hop relational reasoning for knowledge-aware question answering.\nIn Proc. EMNLP, pp. 1295–1309. ACL, Online, 2020. doi: 10.18653/v1/\n2020.emnlp-main.99 4\n[19] Y . Feng, X. Wang, B. Pan, K. K. Wong, Y . Ren, S. Liu, Z. Yan, Y . Ma,\nH. Qu, and W. Chen. Xnli: Explaining and diagnosing nli-based visual\ndata analysis. IEEE Trans. Visual Comput. Graphics , 2023. doi: 10.\n1109/TVCG.2023.3240003 3\n[20] B. Hoover, H. Strobelt, and S. Gehrmann. exBERT: A visual analysis\ntool to explore learned representations in transformer models. In Proc.\nACL: System Demonstrations, pp. 187–196, 2020. doi: 10.18653/v1/2020.\nacl-demos.22 3\n[21] L. Huang, R. L. Bras, C. Bhagavatula, and Y . Choi. Cosmos qa: Machine\nreading comprehension with contextual commonsense reasoning. In Proc.\nEMNLP, pp. 2391–2401. ACL, Hong Kong, 2019. doi: 10.18653/v1/D19\n-1243 1, 2\n[22] J. D. Hwang, C. Bhagavatula, R. L. Bras, J. Da, K. Sakaguchi, A. Bosselut,\nand Y . Choi. Comet-atomic 2020: On symbolic and neural commonsense\nknowledge graphs. In AAAI, pp. 6384–6392, 2021. doi: 10.1609/aaai.\nv35i7.16792 2, 9\n[23] Z. Jin, X. Wang, F. Cheng, C. Sun, Q. Liu, and H. Qu. Shortcutlens:\nA visual analytics approach for exploring shortcuts in natural language\nunderstanding dataset. IEEE Trans. Visual Comput. Graphics, 2023. doi:\n10.1109/TVCG.2023.3236380 3\n[24] Z. Jin, Y . Wang, Q. Wang, Y . Ming, T. Ma, and H. Qu. Gnnlens: A visual\nanalytics approach for prediction error diagnosis of graph neural networks.\nIEEE Trans. Visual Comput. Graphics, 2022. doi: 10.1109/TVCG.2022.\n3148107 3\n[25] D. Kaushik, E. Hovy, and Z. C. Lipton. Learning the difference that makes\na difference with counterfactually-augmented data. In ICLR, 2020. 2, 3\n[26] D. Khashabi, Y . Kordi, and H. Hajishirzi. Unifiedqa-v2: Stronger general-\nization via broader cross-format training.arXiv preprint arXiv:2202.12359,\n2022. doi: 10.48550/arXiv.2202.12359 2, 4, 8\n[27] D. Khashabi, S. Min, T. Khot, A. Sabhwaral, O. Tafjord, P. Clark, and\nH. Hajishirzi. Unifiedqa: Crossing format boundaries with a single qa\nsystem. In Findings of ACL: EMNLP, pp. 1896–1907. ACL, online, 2020.\ndoi: 10.18653/v1/2020.findings-emnlp.171 2, 4, 7\n[28] X. L. Li, A. Kuncoro, J. Hoffmann, C. de Masson d’Autume, P. Blunsom,\nand A. Nematzadeh. A systematic investigation of commonsense knowl-\nedge in large language models. In Proc. EMNLP, pp. 11838–11855. ACL,\nAbu Dhabi, 2022. 2\n[29] Z. Li, X. Wang, W. Yang, J. Wu, Z. Zhang, Z. Liu, M. Sun, H. Zhang, and\nS. Liu. A unified understanding of deep nlp models for text classification.\nIEEE Trans. Visual Comput. Graphics, 28(12):4980–4994, 2022. doi: 10.\n1109/TVCG.2022.3184186 3\n[30] P. P. Liang, Y . Lyu, G. Chhablani, N. Jain, Z. Deng, X. Wang, L.-P.\nMorency, and R. Salakhutdinov. Multiviz: Towards visualizing and under-\nstanding multimodal models. In ICLR, 2022. 3\n[31] B. Y . Lin, X. Chen, J. Chen, and X. Ren. Kagnet: Knowledge-aware graph\nnetworks for commonsense reasoning. In Proc. EMNLP, pp. 2829–2839.\nACL, Hong Kong, 2019. doi: 10.18653/v1/D19-1282 2, 4\n[32] B. Y . Lin, H. Sun, B. Dhingra, M. Zaheer, X. Ren, and W. W. Cohen.\nDifferentiable open-ended commonsense reasoning. In Proc. NAACL, pp.\n4611–4625. ACL, Online, 2021. doi: 10.18653/v1/2021.naacl-main.366 2\n[33] B. Y . Lin, Z. Wu, Y . Yang, D.-H. Lee, and X. Ren. Riddlesense: Reasoning\nabout riddle questions featuring linguistic creativity and commonsense\nknowledge. In Findings of ACL: ACL/IJCNLP , pp. 1504–1515. ACL,\nOnline, 2021. doi: 10.18653/v1/2021.findings-acl.131 4\n[34] J. Liu, A. Liu, X. Lu, S. Welleck, P. West, R. L. Bras, Y . Choi, and\nH. Hajishirzi. Generated knowledge prompting for commonsense reason-\ning. In Proc. ACL, pp. 3154–3169. ACL, Dublin, Ireland, 2022. doi: 10.\n18653/v1/2022.acl-long.225 2\n[35] N. Lourie, R. L. Bras, C. Bhagavatula, and Y . Choi. Unicorn on rainbow:\nA universal commonsense reasoning model on a new multitask benchmark.\nIn AAAI, pp. 13480–13488, 2021. doi: 10.1609/aaai.v35i15.17590 2\n[36] S. M. Lundberg and S.-I. Lee. A unified approach to interpreting model\npredictions. In NeurIPS, vol. 30, pp. 4765–4774, 2017. 1, 2, 3, 9\n[37] K. Ma, F. Ilievski, J. Francis, Y . Bisk, E. Nyberg, and A. Oltramari.\nKnowledge-driven data construction for zero-shot evaluation in common-\nsense question answering. In AAAI, vol. 35, pp. 13507–13515, 2021. doi:\n10.1609/aaai.v35i15.17593 2\n[38] C. Manning and H. Schutze. Foundations of statistical natural language\nprocessing. MIT press, 1999. 4\n[39] L. McInnes, J. Healy, and J. Melville. Umap: Uniform manifold ap-\nproximation and projection for dimension reduction. arXiv preprint\narXiv:1802.03426, 2018. doi: 10.48550/arXiv.1802.03426 5\n[40] K. Meng, D. Bau, A. Andonian, and Y . Belinkov. Locating and editing\nfactual knowledge in gpt. In ICLR, 2022. 2\n[41] K. Meng, A. S. Sharma, A. Andonian, Y . Belinkov, and D. Bau. Mass-\nediting memory in a transformer. In ICLR, 2023. 2\n[42] Y . Ming, S. Cao, R. Zhang, Z. Li, Y . Chen, Y . Song, and H. Qu. Under-\nstanding hidden memories of recurrent neural networks. In Proc. VAST,\npp. 13–24. IEEE, 2017. doi: 10.1109/V AST.2017.8585721 3\n[43] E. Mitchell, C. Lin, A. Bosselut, C. Finn, and C. D. Manning. Fast model\n10\nTo appear in IEEE Transactions on Visualization and Computer Graphics.\nediting at scale. In ICLR, 2022. 3, 5, 12\n[44] N. Mostafazadeh, A. Kalyanpur, L. Moon, D. Buchanan, L. Berkowitz,\nO. Biran, and J. Chu-Carroll. GLUCOSE: GeneraLized and COntextual-\nized story explanations. In Proc. EMNLP, pp. 4569–4586. ACL, Online,\n2020. doi: 10.18653/v1/2020.emnlp-main.370 2, 9\n[45] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin,\nC. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton,\nL. Miller, M. Simens, A. Askell, P. Welinder, P. F. Christiano, J. Leike,\nand R. Lowe. Training language models to follow instructions with human\nfeedback. In NeurIPS, vol. 35, pp. 27730–27744, 2022. 2\n[46] F. Petroni, T. Rocktäschel, P. Lewis, A. Bakhtin, Y . Wu, A. H. Miller, and\nS. Riedel. Language models as knowledge bases? In Proc. EMNLP, pp.\n2463–2473. ACL, Hong Kong, 2019. doi: 10.18653/v1/D19-1250 2\n[47] R. Poyiadzi, K. Sokol, R. Santos-Rodriguez, T. De Bie, and P. Flach. Face:\nFeasible and actionable counterfactual explanations. In Proc. AIES, pp.\n344–350, 2020. doi: 10.1145/3375627.3375850 9\n[48] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y . Zhou,\nW. Li, and P. J. Liu. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. J. Mach. Learn. Res., 21(140):1–67, 2020. 1, 2\n[49] N. F. Rajani, B. McCann, C. Xiong, and R. Socher. Explain yourself!\nleveraging language models for commonsense reasoning. In Proc. ACL,\npp. 4932–4942. ACL, Florence, Italy, 2019. doi: 10.18653/v1/p19-1487 2\n[50] E. Reif, A. Yuan, M. Wattenberg, F. B. Viegas, A. Coenen, A. Pearce, and\nB. Kim. Visualizing and measuring the geometry of bert. In NeurIPS,\nvol. 32, pp. 8592–8600, 2019. 2\n[51] M. T. Ribeiro, S. Singh, and C. Guestrin. \"Why should i trust you?\":\nExplaining the predictions of any classifier. In Proc. KDD, pp. 1135–1144.\nACM, New York, 2016. doi: 10.1145/2939672.2939778 1, 2, 3\n[52] K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y . Choi. Winogrande: An\nadversarial winograd schema challenge at scale. Communications of the\nACM, 64(9):99–106, 2021. doi: 10.1145/3474381 2, 4\n[53] M. Sap, R. Le Bras, E. Allaway, C. Bhagavatula, N. Lourie, H. Rashkin,\nB. Roof, N. A. Smith, and Y . Choi. Atomic: An atlas of machine com-\nmonsense for if-then reasoning. In AAAI, vol. 33, pp. 3027–3035, 2019.\ndoi: 10.1609/aaai.v33i01.33013027 2, 4, 9\n[54] M. Sap, H. Rashkin, D. Chen, R. L. Bras, and Y . Choi. Social iqa:\nCommonsense reasoning about social interactions. In Proc. EMNLP, pp.\n4462–4472. ACL, Hong Kong, 2019. doi: 10.18653/v1/D19-1454 2, 4, 9\n[55] P. Schwab and W. Karlen. Cxplain: Causal explanations for model inter-\npretation under uncertainty. In NeurIPS, vol. 32, pp. 10220–10230, 2019.\n9\n[56] V . Shwartz, P. West, R. L. Bras, C. Bhagavatula, and Y . Choi. Unsupervised\ncommonsense question answering with self-talk. In Proc. EMNLP, pp.\n4615–4629. ACL, Online, 2020. doi: 10.18653/v1/2020.emnlp-main.373\n2, 9\n[57] S. Singh, N. Wen, Y . Hou, P. Alipoormolabashi, T.-L. Wu, X. Ma, and\nN. Peng. Com2sense: A commonsense reasoning benchmark with com-\nplementary sentences. In Findings of ACL: ACL/IJCNLP, pp. 883–898.\nACL, online, 2021. doi: 10.18653/v1/2021.findings-acl.78 1, 2\n[58] D. Slack, A. Hilgard, H. Lakkaraju, and S. Singh. Counterfactual ex-\nplanations can be manipulated. In NeurIPS, vol. 34, pp. 62–75, 2021.\n9\n[59] R. Speer, J. Chin, and C. Havasi. Conceptnet 5.5: An open multilingual\ngraph of general knowledge. In AAAI, pp. 4444–4451, 2017. 2, 3, 4, 6\n[60] H. Strobelt, S. Gehrmann, H. Pfister, and A. M. Rush. Lstmvis: A tool\nfor visual analysis of hidden state dynamics in recurrent neural networks.\nIEEE Trans. Visual Comput. Graphics , 24(1):667–676, 2018. doi: 10.\n1109/TVCG.2017.2744158 3\n[61] M. Sundararajan, A. Taly, and Q. Yan. Axiomatic attribution for deep\nnetworks. In Proc. ICML, pp. 3319–3328. PMLR, 2017. 2\n[62] A. Talmor, J. Herzig, N. Lourie, and J. Berant. Commonsenseqa: A\nquestion answering challenge targeting commonsense knowledge. In Proc.\nNAACL, pp. 4149–4158. ACL, Minneapolis, Minnesota, 2019. doi: 10.\n18653/v1/n19-1421 1, 2, 4, 7\n[63] A. Talmor, O. Yoran, R. L. Bras, C. Bhagavatula, Y . Goldberg, Y . Choi,\nand J. Berant. Commonsenseqa 2.0: Exposing the limits of AI through\ngamification. In NeurIPS Datasets and Benchmarks Track, vol. 1, 2021.\n1, 2, 4\n[64] N. Tandon, G. de Melo, F. Suchanek, and G. Weikum. Webchild: Harvest-\ning and organizing commonsense knowledge from the web. In Proc.\nWSDM, p. 523–532. ACM, New York, 2014. doi: 10.1145/2556195.\n2556245 2\n[65] I. Tenney, D. Das, and E. Pavlick. Bert rediscovers the classical nlp\npipeline. In Proc. ACL, pp. 4593–4601. ACL, Florence, Italy, 2019. doi:\n10.18653/v1/p19-1452 1, 2\n[66] I. Tenney, J. Wexler, J. Bastings, T. Bolukbasi, A. Coenen, S. Gehrmann,\nE. Jiang, M. Pushkarna, C. Radebaugh, E. Reif, et al. The language\ninterpretability tool: Extensible, interactive visualizations and analysis for\nnlp models. In Proc. EMNLP: System Demonstrations, pp. 107–118, 2020.\ndoi: 10.18653/v1/2020.emnlp-demos.15 2\n[67] J. Vig. Bertviz: A tool for visualizing multihead self-attention in the bert\nmodel. In ICLR Workshop: Debugging Machine Learning Models, 2019.\n3\n[68] E. V oita and I. Titov. Information-theoretic probing with minimum de-\nscription length. In Proc. EMNLP, pp. 183–196. ACL, Online, Nov. 2020.\ndoi: 10.18653/v1/2020.emnlp-main.14 9\n[69] X. Wang, J. He, Z. Jin, M. Yang, Y . Wang, and H. Qu. M2lens: Visualizing\nand explaining multimodal models for sentiment analysis. IEEE Trans.\nVisual Comput. Graphics, 28(1):802–812, 2021. doi: 10.1109/TVCG.2021\n.3114794 3\n[70] J. W. Wei and K. Zou. EDA: easy data augmentation techniques for\nboosting performance on text classification tasks. In Proc. EMNLP, pp.\n6381–6387. ACL, Hong Kong, 2019. doi: 10.18653/v1/D19-1670 5, 12\n[71] J. Wexler, M. Pushkarna, T. Bolukbasi, M. Wattenberg, F. Viégas, and\nJ. Wilson. The what-if tool: Interactive probing of machine learning\nmodels. IEEE Trans. Visual Comput. Graphics, 26(1):56–65, 2019. doi:\n10.1109/TVCG.2019.2934619 2, 3\n[72] T. Wu, M. T. Ribeiro, J. Heer, and D. S. Weld. Polyjuice: Generating\ncounterfactuals for explaining, evaluating, and improving models. In Proc.\nACL, pp. 6707–6723. ACL, Online, 2021. doi: 10.18653/v1/2021.acl-long\n.523 2, 3\n[73] Y . Xu, C. Zhu, S. Wang, S. Sun, H. Cheng, X. Liu, J. Gao, P. He, M. Zeng,\nand X. Huang. Human parity on commonsenseqa: Augmenting self-\nattention with external attention. In Proc. IJCAI, pp. 2762–2768, 2022.\ndoi: 10.24963/ijcai.2022/383 1, 2\n[74] M. Yasunaga, H. Ren, A. Bosselut, P. Liang, and J. Leskovec. Qa-gnn:\nReasoning with language models and knowledge graphs for question\nanswering. In Proc. NAACL, pp. 535–546. ACL, online, 2021. doi: 10.\n18653/v1/2021.naacl-main.45 2, 4\n[75] R. Zellers, A. Holtzman, Y . Bisk, A. Farhadi, and Y . Choi. Hellaswag:\nCan a machine really finish your sentence? In Proc. ACL, pp. 4791–4800.\nACL, Florence, Italy, 2019. doi: 10.18653/v1/p19-1472 1, 2\n[76] H. Zhang, D. Khashabi, Y . Song, and D. Roth. Transomcs: From linguistic\ngraphs to commonsense knowledge. In Proc. IJCAI, pp. 4004–4010, 2020.\ndoi: 10.24963/ijcai.2020/554 2\n[77] X. Zhou, Y . Zhang, L. Cui, and D. Huang. Evaluating commonsense in\npre-trained language models. In AAAI, vol. 34, pp. 9733–9740, 2020. doi:\n10.1609/aaai.v34i05.6523 2\n11\nA M ODEL EDITING IMPLEMENTATION\nHere, we introduce the details of technical implementation and eval-\nuation of editor networks for model editing. We adopt the MEND\nnetwork [43] for model editing.\nA.1 Editor networks\nMEND leverages a collection of small auxiliary editing networks that\nuse a single desired input-output pair to make fast, local edits to pre-\ntrained models. Specifically, it uses the low-rank structure of fine-tuning\ngradients to enable scalable and efficient editing of very large pretrained\nlanguage models on specified layers of transformers. MEND uses the\nfact that gradients for MLPs are rank-1 matrix and apply the theory to\nTransformers by summing elements over sequence indices. The model\nediting gradient update function is derived as:\n˜∇Wℓ =\nB\n∑\ni=1\n˜δi\nℓ+1 ˜ui⊤\nℓ .\nWhere ˜ui\nℓ and ˜δi\nℓ+1 are pseudo-activations and pseudo-delta by\ntaking the sequence sum of the gradient of the loss for batch i with\nrespect to the pre-activations at layer l + 1, and the sequence sum\nof the inputs to layer l for batch element i. B is the number of total\nbatches. ˜∇Wℓ is the gradient update to be applied on the MLP layers of\ntransformers. For more details, please refer to the original paper [43].\nA.2 Model editing training\nPractically, for the T5-based QA model that we use 3, we only edit\nthe MLP layers of the last two encoder and decoder blocks of the\ntransformer. We follow the official implementation of MEND4 to build\nour model and conduct the experiments.\nTo train editor networks that can edit our T5 model on CSQA, we\nneed to collect editing targets, equivalence examples, and locality ex-\namples. Specifically, editing targets contain a question and a target\nchoice, where the question comes from train set of CSQA dataset,\nand the target choice does not necessarily be the ground truths. We\nrandomly sample one choice from five alternatives in the original QA\ninstance as the editing target. For equivalence examples generation,\nwe use data augmentation techniques to perturbate the original in-\nstances to get meaning-preserving augmentations as much as possi-\nble. We use back-translation implemented in nlpaug5 to translate\nthe original sentences to German and then back to English using the\nfacebook/wmt19-en-de machine translation checkpoint. We also\nadopt Easy Data Augmentation (EDA) [70] to do synonym replacement\nand random insert/delete/replace on the original sentences to ensure the\nrobustness of the model training. For locality examples, we indepen-\ndently sample negative examples different from editing targets from\nthe same original dataset.\nOnce the editor networks are trained, they can be applied to conduct\nposthoc editing on the original model at inference time.\nB S YSTEM INTERACTION DESIGNS\nCommonsenseVIS offers various interactions to support multi-level\nanalysis of model behavior with details on demand.\nLasso and pan-and-zoom. In the Global View, users can lasso a\ngroup of data instances in the scatter plots to examine the details in the\nSubset View and Instance View. And users can use pan-and-zoom in\nthe scatter plots to navigate local clusters more easily.\nHovering and clicking. To make the interface cleaner and less\noverwhelming, we hide lots of details, and users can hover or click to\nsee the details on demand. For example, in the Global View, users can\nhover over the dots in the scatter plots and the relation bars in the middle\nto see the pairs of question stems and target concepts and relation\naccuracy, respectively. When hovering the cluster glyphs in the Subset\nView, detailed concepts and statistics of the clusters, together with their\n3https://github.com/allenai/unifiedqa\n4https://github.com/eric-mitchell/mend/\n5https://github.com/makcedward/nlpaug\nconnections with other clusters, will be displayed. In the Instance View,\nhovering over the charts will display the detailed numbers. Also, users\ncan hover over the answer choices to query their relations with the\nquestion stem concepts.\nMoreover, users can filter or highlight the information by clicking.\nFor example, relation bars in the Global View, and stacked bars in\nthe Subset View can be clicked to filter data instances. In addition,\nusers can navigate through instances by clicking the pagination buttons.\nMeanwhile, its corresponding dots and clusters will be highlighted in\nthe Global View and Subset View, respectively.\nC EVALUATION OF COMMONSENSE KNOWLEDGE COVERAGE\nOF CONCEPT NET\nWe conducted an evaluation using a random sample of 100 examples\ndrawn from the CSQA validation set6. We have invited an NLP expert\n(E14, not our co-author) to evaluate the relational paths extracted by\nour algorithm based on ConceptNet. For each QA instance, the expert\nexamined the QA instance and the extracted relational paths built by\nretrieved concept-relation triplets from ConceptNet. Then, he decided\nwhether the paths could accurately cover the necessary commonsense\nknowledge to answer the question. Finally, we calculated and reported\nthe proportion of instances for which the necessary commonsense\nknowledge is covered by the extracted ConceptNet knowledge.\nThe results show that the retrieved ConceptNet knowledge can cover\nthe commonsense knowledge in 91 out of 100 instances. It further\nhelps validate the use of ConceptNet for model contextualization on\nthe CSQA dataset. However, although CSQA is built upon ConceptNet,\nit still cannot cover some commonsense knowledge in the data. For\nexample, for the question “The potato might be the official vegetable of\nwhat? (correct answer: Maryland)”, retrieved concept-relation triplets\nfrom ConceptNet fail to build a connection between “potato” and\n“Maryland” or “official vegetable”. In addition, for the question“Where\nhas the newest baseball stadium? (correct answer: Phoenix)”, although\nretrieved concept-relation triplets can associate “baseball stadium“ with\ndifferent locations using AtLocation realation. However, it lacks the\nknowledge to determine which city has the “newest” stadium.\nD A DDITIONAL CASES OF USING CommonsenseVIS\nD.1 Relation of atlocation regarding room and office is\nrelatively well-learned\nGlobal Summary(R1, R2) After loading the system and dataset, the\nexpert E4 first referred to the Global View to explore the model perfor-\nmance regarding different relations. After hovering over the green bars\nbetween the scatter plots, he was able to quickly observe that although\nthere is an imbalanced relation distribution (as indicated by varied bar\nheight), accuracies for most relations are about 0.70 (Figure 7A). It\nindicates that the model may have learned a fair amount of relations\nbetween different concepts. Then, E4 felt curious about what relations\nare and under what contexts the model learns well. He started with\nthe relation atlocation with the highest green bar at the top. After\nclicking the bar, he selected the “Correctness X Tranformed” projection\nmode and “Correctness” coloring scheme to explore the distribution of\nthe correctly-answered instances of atlocation in the question and\nanswer (target concept) scatter plots (Figure 7A). He noticed that there\nis a large cluster with green dots in the answer scatter plot. He won-\ndered whether the models have really learned atlocation between\nquestion and target concepts in these instances. Therefore, he switched\nto “Relation X Tranformed” projection mode to see how the relation\nis learned by examining the correspondence between question stems\nand target concepts after transformation (Figure 7A). And he discov-\nered two well-formed and well-aligned clusters in the two scatter plots,\nwhich provides support for a good learning of atlocation relation.\nSubset Exploration(R1, R2, R3) To further explore the contexts of\nselected instances, E4 looked at the question stem cluster glyphs in the\nSubset View (Figure 7B), where the green and blue bars nearly occupy\nthe two stacked bars at the top. It indicates a high model accuracy\nand overlap between the model concepts and ConceptNet concepts.\n6Data samples fo evaluation are available at https://bit.ly/3PCGbze\n12\nTo appear in IEEE Transactions on Visualization and Computer Graphics.\nFig. 7: Interactive exploration of case three by E4 using CommonsenseVIS. (A) E4 used “Correctness X Tranformed” projection mode to select the\ncorrect instances and changed into “Relation X Tranformed” mode to visualize the corresponding result. (B) E4 went through the cluster glyphs in the\nSubset View to gain general information about the model’s behavior over correctly answered subsets. (C) After clicking on a cluster,E4 was able to\ncheck more statistics about this group on the top (the accuracy, the average question concept (QC) coverage ratio, and the frequent model concepts).\nThen E4 quickly went through pages to check the detailed model behavior, such as feature importance scores, to check what kind of commonsense\nknowledge the model learns sufficiently.\nMoreover, he observed the yellow rectangles on the left of question\nstem clusters are much shorter than the dark blue ones (Figure 7B),\nconfirming that very few ConceptNet concepts are not covered by the\nmodel. He then hovered over the first cluster glyph to see the details of\nthose concepts, where words like “man” and “want” appear. He thought\nthat these concepts, not important to model predictions, might not affect\nthe reasoning about atlocation. Therefore, he hypothesized that the\nquestion contexts are properly considered by the model. And he clicked\nthis glyph to explore detailed instances and their explanations in the\nInstance View to verify his hypothesis. By scanning the top frequent\nmodel concepts in the histogram (Figure 7C) (e.g., “where”, “what”,\n“store”, “office”, “room”, “building”), he reasoned that many of these\ninstances of atlocation are “what”, “where” questions and associate\nwith “office” and “room”.\nFig. 8: Instances found by E4 confirmed his hypothesis that the relation\nof atlocation regarding scenarios about room and office is well-learned\nby the model.\nInstance Exploration and Searching(R1, R4) Finally, through\nexploration of individual questions in the Instance View, E4 found that\nthe model truly captures important words for answering commonsense\nquestions. For example, in Figure 8, SHAP values show “office” and\n“put” as important contexts for where the “check” can be located, which\nis “desk drawer”. Another example in Figure 8 shows that the model\nproperly considered contexts like “room” and “contemplation” for\nwhere the “bookcases” should be located in a “study room”, which\naligns with human knowledge. Then, E4 reasoned that the model\nhas a good sense of atlocation in the situations of “office” and\n“room”. And he lassoed all the instances of atlocation in the Global\nView and typed “office” and “room” to search relevant instances in the\nInstance View, where the model achieves 90.00% and 88.89% accuracy,\nrespectively (much higher than the overall 71.00% accuracy). E4 was\nconvinced that the model has learned the relation atlocation in the\ncontext of “office” and “room” properly.\nFig. 9: Case two: Instances related to “movie” found by E5.\nE A DDITIONAL USER STUDY RESULTS\nE.1 User study questionnaire\nThe user study questionnaire is presented in Table 2.\n13\nTable 2: The first section of our questionnaire is designed to collect\nfeedback on the system’s effectiveness in evaluating the model’s com-\nmonsense abilities (Q1-Q4). The second section is designed to evaluate\nthe usefulness and usability of Global View (Q5-Q7), Subset View (Q8-\nQ10) and Instance View (Q11-Q13). The last section is designed to\nevaluate personal opinions of our system (Q14-Q17). The original sen-\ntences without the words in brackets are the positive statements at the\nright end of the scale points, while the sentences with words in the\nbrackets are the negative statements at the left end.\nQ1 The system can (cannot) help me identify the target com-\nmonsense knowledge in data instances.\nQ2 The system does (does not) contextualize model performance\nregarding different concepts and their underlying relations.\nQ3 The system can (cannot) help me infer the model’s relational\nreasoning over different concepts.\nQ4 I am (not) confident in my findings about the model’s com-\nmonsense reasoning abilities.\nQ5 The Global View can (cannot) help me relate model perfor-\nmance to different concepts and relations.\nQ6 The Global View can (cannot) help me infer how the relations\nare learned by models.\nQ7 The Global View is easy (difficult) to understand.\nQ8 The Subset View can (cannot) help me align model behavior\nwith ConceptNet knowledge.\nQ9 The Subset View can (cannot) help me summarize model\nbehavior on different groups of question concepts/question\nstems/target concepts.\nQ10 The Subset View is easy (difficult) to understand.\nQ11 The Instance View can (cannot) help me diagnose if the\nmodel uses proper information for reasoning.\nQ12 The Instance View can (cannot) help me infer if a relation\nbetween question concepts and target concepts is learned or\nnot.\nQ13 The Instance View is easy (difficult) to understand.\nQ14 It is easy (difficult) to learn the system.\nQ15 It is easy (difficult) to use the system.\nQ16 I will (will not) use it in the future for understanding and\ndiagnosing language models.\nQ17 I will (will not) recommend this system to other colleagues\nfor understanding and diagnosing language models.\nE.2 User ratings and feedback\nE.2.1 Visual designs and interactions\nAs shown in Figure 10, participants generally agreed that our system is\neasy to use (MeanQ15 = 4.20, SDQ15 = 1.03) while it required some ef-\nforts for learning (MeanQ14 = 3.80, SDQ14 = 0.79). They were willing\nto use (MeanQ16 = 4.50, SDQ16 = 0.85) and recommend our system\n(MeanQ17 = 4.70, SDQ17 = 0.67) for understanding and diagnosing\ncommonsense reasoning capabilities of language models. The most in-\ntuitive view of CommonsenseVIS is the Instance View, then the Global\nView. And the Subset View was thought to be the most difficult to un-\nderstand. We summarize participants’ feedback (as shown in Figure 11)\non our visual designs as follows.\nFor the Global View, participants found it quite useful for finding\nrelations/concepts with large/small prediction errors (MeanQ5 = 4.30,\nSDQ5 = 0.95). “I can quickly observe the correctness distribution\namong instances” (E6, E8, E12) and “narrow down to specific cluster\nof instances” (E7, E9). And the question and answer scatter plots\nhelped them infer if the relations are generally learned well (MeanQ6 =\n4.40, SDQ6 = 1.26). Furthermore, E6 and E8 added that the correctness\ncoloring of the dots (i.e., model accuracy) is really helpful when they\nanalyze relation learning and were not sure about the quality of the\nalignment between questions and answers. However, some participants\nreported that sometimes it is a bit hard to visually align and match\nclusters of dots in the question and answer scatter plots due to the\nembedding rotation effect (E10) and scarcity of instances (E11).\nFor the Subset View, participants thought it was a bit complex\n(Mean10 = 3.20, SDQ10 = 0.63). And after they got familiar with it,\nthey considered it helpful to compare the model concepts and Concept-\nNet concepts (MeanQ8 = 4.20, SDQ8 = 0.63) and summarize model\nFig. 10: The results of the questionnaire about overall impressions of our\nsystem, including the effectiveness (Q1-Q4) and the usability (Q14-Q17).\nbehavior on different question concepts/question stems/target concepts\n(MeanQ9 = 4.50, SDQ9 = 0.53). For example, E9 commented “The\nhit ratio and top missed concepts are very useful in understanding\nwhat types of concepts the model focuses on.” Some participants felt\nSubset View could be a bit too informative sometimes. E10 suggested,\n“Choosing the number of clusters and viewing the concepts and linkers\nare a bit messy and too informative, can we simplify the design and\nonly the most impactful one?”\nThe Instance View was the most favored by participants for its intu-\nitiveness and helpfulness in diagnosing if the model uses proper infor-\nmation (MeanQ11 = 4.80, SDQ11 = 0.42) and learns relations between\nquestion concepts and target concepts for reasoning (MeanQ12 = 4.50,\nSDQ12 = 0.97). Participants generally thought that SHAP explanations\nand model probing complement each other to deepen the model un-\nderstanding. “I tested many examples and found that the explanation\nresults were very satisfactory. And the instance probing also helped me\nto do some further investigation and testing on model behavior.” (E7).\n“The model probing is a great tool to change the input to the model and\ncheck the behavioral change of the model. This can be used to do some\ncausal analysis of concepts.” (E8).\nFig. 11: The results of the questionnaire about the helpfulness and\nintuitiveness of using our system to evaluate commonsense reasoning\nabilities regarding the Global View (Q5-Q7), the Subset View (Q8-Q10),\nand the Instance View (Q11-Q13).\n14"
}