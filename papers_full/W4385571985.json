{
  "title": "The Larger they are, the Harder they Fail: Language Models do not Recognize Identifier Swaps in Python",
  "url": "https://openalex.org/W4385571985",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2994127542",
      "name": "Antonio Valerio Miceli Barone",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4318571320",
      "name": "Fazl Barez",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2133337488",
      "name": "Shay B. Cohen",
      "affiliations": [
        "University of Edinburgh"
      ]
    },
    {
      "id": "https://openalex.org/A2049206248",
      "name": "Ioannis Konstas",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3001279689",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W3016970897",
    "https://openalex.org/W2963392741",
    "https://openalex.org/W4226485558",
    "https://openalex.org/W3207316473",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4381105112",
    "https://openalex.org/W2736601468",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W4297688279",
    "https://openalex.org/W4249297534",
    "https://openalex.org/W4311887664",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3201174429",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W3185227028",
    "https://openalex.org/W3164972323",
    "https://openalex.org/W2962784628"
  ],
  "abstract": "Large Language Models (LLMs) have successfully been applied to code generation tasks, raising the question of how well these models understand programming.Typical programming languages have invariances and equivariances in their semantics that human programmers intuitively understand and exploit, such as the (near) invariance to the renaming of identifiers. We show that LLMs not only fail to properly generate correct Python code when default function names are swapped, but some of them even become more confident in their incorrect predictions as the model size increases, an instance of the recently discovered phenomenon of Inverse Scaling, which runs contrary to the commonly observed trend of increasing prediction quality with increasing model size. Our findings indicate that, despite their astonishing typical-case performance, LLMs still lack a deep, abstract understanding of the content they manipulate, making them unsuitable for tasks that statistically deviate from their training data, and that mere scaling is not enough to achieve such capability.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 272–292\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nThe Larger They Are, the Harder They Fail:\nLanguage Models do not Recognize Identifier Swaps in Python\nAntonio Valerio Miceli-Barone1∗\namiceli@ed.ac.uk\nFazl Barez1∗\nf.barez@ed.ac.uk\nIoannis Konstas2\ni.konstas@hw.ac.uk\nShay B. Cohen1\nscohen@inf.ed.ac.uk\n1 School of Informatics, University of Edinburgh\n2 School of Mathematical and Computer Sciences, Heriot-Watt University\nAbstract\nLarge Language Models (LLMs) have success-\nfully been applied to code generation tasks,\nraising the question of how well these mod-\nels understand programming. Typical program-\nming languages have invariances and equivari-\nances in their semantics that human program-\nmers intuitively understand and exploit, such as\nthe (near) invariance to the renaming of identi-\nfiers. We show that LLMs not only fail to prop-\nerly generate correct Python code when default\nfunction names are swapped, but some of them\neven become more confident in their incorrect\npredictions as the model size increases, an in-\nstance of the recently discovered phenomenon\nof Inverse Scaling, which runs contrary to the\ncommonly observed trend of increasing predic-\ntion quality with increasing model size. Our\nfindings indicate that, despite their astonish-\ning typical-case performance, LLMs still lack\na deep, abstract understanding of the content\nthey manipulate, making them unsuitable for\ntasks that statistically deviate from their train-\ning data, and that mere scaling is not enough to\nachieve such capability.\n1 Introduction\nPretrained Large Language Models (LLMs) are\nrapidly becoming one of the dominant paradigm for\nlarge variety of language tasks (Brown et al., 2020a;\nChowdhery et al., 2022), including programming\ncode generation and completion (Chen et al., 2021;\nLi et al., 2022). LLMs have demonstrated increas-\ning performance with increasing model size 1 on\nmany practical tasks (Kaplan et al., 2020; Hernan-\ndez et al., 2021) including programming tasks (Ni-\njkamp et al., 2022), recently, however, researchers\n∗ Equal contribution.\n1Since model capacity in number of parameters and pre-\ntraining dataset size are balanced according to a design law\nthat is fixed for each model family and is intended to empiri-\ncally maximize the pretraining set likelihood given a compute\nbudget (Kaplan et al., 2020), for the reminder of this paper we\nwill jointly refer to them as \"model size\".\nlen, print = print, len\ndef print_len(x):\n“Print the length of x”\n✓ len(print(x)) ✗ print(len(x))\nLLM preference\nFigure 1: Given a Python prompt (on top) which swaps\nof two builtin functions, large language models pre-\nfer the incorrect but statistically common continuation\n(right) to the correct but unusual one (left).\nhave identified a number of tasks that exhibit\ninverse scaling, where output quality decreases,\nrather than increase, with increasing model size.\nTasks with inverse scaling generally either in-\nvolve social biases (Parrish et al., 2022; Srivastava\net al., 2022), where the larger models (arguably cor-\nrectly) learn undesirable biases from biased train-\ning sets, or involve examples of natural language\nthat are highly atypical but still easily understand-\nable by a human (McKenzie et al., 2022b). These\ntasks may involve unusual discourse pragmatics or\nthey may require reasoning about counterfactual\nknowledge, however, since they tend to be highly\nartificial, it could perhaps be argued that they are\nedge cases which may not represent serious failure\nmodes for practical applications. In this paper we\npresent a novel type of inverse scaling task involv-\ning Python code generation under a redefinition of\ndefault identifiers. This has both practical implica-\ntions (redefinition of default identifiers is a meta-\nprogramming technique used in popular libraries),\nand broader scientific implications, as it shows that\nLLMs fail to reason about the deep, abstract seman-\ntic structure of programming languages, and these\nflaws are not ameliorated, but in fact may be even\nworsened, by increasing model size.\nProgramming languages have precise and well-\n272\nCrawl\nGitHub\nExtract\nfunctions\nSwap\nbuiltins\nheads (prompts)\noriginal bodies (bad classes)\ncorrected bodies (good classes) Dataset\nFigure 2: Data generation pipeline (see Appendix D for an example): 1. Crawl repositories from GitHub, filtered\nby language, license, stars, and size. 2. Extract top-level functions with docstrings and references to at least\ntwo callable builtins 3. For each function, choose two builtins to swap and generate: a) header with builtin swap\nstatement, function declaration with decorators, docstring b) original function body, c) corrected body with the\nbuiltins swapped consistently with the swap statement. 4. Store as a binary classification task: a) head = classifier\ninput, b) original body = bad class, c) corrected body = good class.\ndefined syntax and semantics which makes them\nespecially suited to automatic analysis and proce-\ndural generation. They are scientifically interesting\nbecause they can be used for automatic generation\nof examples of coding problems and their eval-\nuation against an objective ground truth, whereas\nmost NLP tasks have enough ambiguity that require\nhuman annotation in order to produce high-quality\nexamples. Furthermore, this research is also of\npractical importance for software engineering tools\nthat use LLMs, such as GitHub Copilot,2 which are\nstarting to be widely adopted by developers.\n2 Methodology\nWe describe the motivation behind our task (§2.1)\nand the task itself (§2.2), followed by the way we\ncollected the data for the task (§2.3).\nWe release our dataset as well as the code used\nto generate it and replicate our experiments3.\n2.1 Task Motivation\nTuring-complete languages have invariances and\nequivariances, making it possible to express the\nsame function by multiple programs (see Appendix\nH for formal definitions). While determining\nsemantic equivalence is undecidable in the gen-\neral case (Rice, 1953), sometimes it can be deter-\nmined by pure syntactic analysis. For instance,\nα-equivalence, invariance under the consistent re-\nnaming of identifiers such as variable or function\nnames, can be decided using syntactic analysis.\nProper understanding of the semantics of a pro-\ngramming language requires identifying its invari-\nances and equivariances, as opposed to “shortcut\nlearning” (Geirhos et al., 2020) which instead ex-\nploits many weak, spurious correlations that do\nnot generalize out of the observed data distribu-\ntion. We propose a task based on the approximate\n2https://github.com/features/copilot\n3https://github.com/Avmb/inverse_scaling_\nprize_code_identifier_swap.git\nα-equivalence of Python code, in order to evaluate\nhow well LLMs master the semantics of Python.\n2.2 Task Description\nWe consider code snippets in Python 3. Python\nallows to redefine builtin functions4 by reassigning\ntheir identifiers. For instance, the statement\nlen, print = print, len\nswaps the identifiers for the builtin functions len\nand print. Any function defined following that\nidentifier swap would have to refer to the builtin\nfunction lenby the identifier printand vice versa.\nWe consider a code generation task where the\nmodel is given a top-level functiondeclaration, fol-\nlowed by a docstring (which typically describes the\nbehavior of the function in natural language) and\nhas to generate the rest of the body of the function,\nsimilar to Miceli Barone and Sennrich (2017), but\nwith the caveat that we prepend to the declaration\na statement that swaps two Python builtin func-\ntions that are expected to be used in the function\nbody. Specifically, in line with the format of the\nInverse Scaling Prize5 we define our Builtin identi-\nfier swap task as a binary classification task where\nthe input of each example is the concatenation of a\nswap statement, function declaration (with optional\ndecorators) and docstring. A “bad” output for such\ninput is a function body that uses the builtin func-\ntions according to their usual meaning, ignoring the\nswap statement. In contrast, the “good” output is a\nfunction body where the builtin functions are used\nconsistently with the swap statement. To assess\nthe success of the model in distinguishing between\nthe “bad” and the “good” output, we compute the\nlikelihood of each output given the input provided\nas a prompt (Figure 1, Appendix D).\n4Predefined functions that the language exposes to the user.\n5https://github.com/inverse-scaling/prize\n273\n2.3 Data Collection\nSimilar to Miceli Barone and Sennrich (2017), our\ndataset collection procedure involves scraping code\nfrom GitHub using the PyCodeSuggest library6\n(Bhoopchand et al., 2016) to download Python\nrepositories with at least 100 stars, of size at most\n200 MB and which mention the use of the Open\nSource CC-BY-4.0 license7 in their README. Our\nfinal dataset includes 559 repositories downloaded\non 16 December 2022. We then parse the .py files\nin each repository with the Python 3 ast module\nto make sure that they contain valid code. We ex-\ntract 1,000 randomly chosen top-level functions\nthat each contain a docstring and that reference at\nleast two callable builtin identifiers, as defined by\nthe builtinsmodule. For each of these extracted\nfunctions, we randomly choose two builtin func-\ntions and generate the corresponding swap state-\nment, function declaration (with decorators) and\ndocstring as the example prompt, the original func-\ntion body (regenerated from the abstract syntax tree\nwith the astunparse module8) as the “bad” out-\nput and the function body where the two selected\nbuiltins are swapped consistently with the swap\nstatement as the “good” output (Figure 2).\nNote that functions can in principle access the\nbuiltin identifiers as strings using reflection and\nevaluation facilities, which may require a full static\nanalysis of the code to identify and is undecidable\nin the general case. Since our method uses purely\nsyntactic substitutions, there might be cases where\nthe “good” outputs do not maintain the expected\nfunction behavior. In practice, this dynamic ac-\ncess of identifiers at runtime is rare with builtin\nidentifiers and therefore does not pose an issue.\n3 Experiments\nWe next describe our experiments with a likelihood\ncalculation of correct and incorrect completions\n(§3.1) and chat LLMs (§3.2), and then present a\nqualitative analysis (§3.3).\nComputational resources We spent approxi-\nmately 130 US dollars, including donated credits,\nto use the OpenAI LLMs through their publicly\naccessible API.\nWe also used a small amount of machine-hours\non the Baskerville Tier 2 HPC platform9 equipped\n6https://github.com/uclnlp/pycodesuggest\n7https://creativecommons.org/licenses/by/4.0/\n8https://pypi.org/project/astunparse/\n9https://www.baskerville.ac.uk/\nwith NVIDIA A100 GPUs. While this is a high-\nend system, our experiments on the open source\nmodels can be also practically run on consumer-\ngrade machines with gaming GPUs.\n3.1 Completion Likelihood\nFor our main set of experiments, we evaluate\nour dataset on families of auto-regressive lan-\nguage models (OpenAI GPT-3, Salesforce Code-\nGen, Meta AI OPT) and one family of sequence-\nto-sequence conditional auto-regressive language\nmodels (Google FLAN-T5). All models are based\non the Transformer architecture (Vaswani et al.,\n2017) and pretrained on large datasets scraped from\nthe Internet (full details in Appendix A).\nResults We evaluate our datasets on the models\nusing a modified version of the Inverse Scaling\nPrize evaluation code.10 We report the results for\nall models in Figure 3. The graphs show the classi-\nfication loss averaged over the examples for each\nmodel, with standard errors represented as error\nbars.\nModel family Pearson Spearman Kendall\nOPT 0.94 0.83 0.73\nGPT-3 0.97 1.00 1.00\nInstructGPT 0.94 0.80 0.67\nCodeGen-multi 0.46 0.40 0.33\nCodeGen-mono 0.10 0.20 0.00\nGPT-Codex -1.00 -1.00 -1.00\nFLAN-T5 0.01 0.10 0.00\nTable 1: Correlation coefficients between log-model\nsize and log-loss for each model family. Bolded val-\nues indicate inverse scaling at p-value < 0.1. The two\ntext-based GPT-3.5 models ( text-davinci-002 and\ntext-davinci-003) are not included in this analysis.\nAll tested models always prefer the incorrect\noutput resulting in zero classification accuracy, the\nlog-likelihood of the incorrect output is always sig-\nnificantly higher than the uniform baseline, but it\nvaries with the model. Specifically:\n• The Meta AI OPT and OpenAI text-based GPT-3\nfamilies exhibit strong inverse scaling, with the\nlarger models more strongly preferring the incor-\nrect output. The trend is monotonic for the \"First\n10Original: https://github.com/naimenz/\ninverse-scaling-eval-pipeline, our version: https:\n//github.com/Avmb/inverse-scaling-eval-pipeline.\ngit\n274\n108 109 1010 1011\nModel size\n101\nLoss\nLog-log plot of loss vs model size\nGPT-3.5\nCodeGen-multi\nCodeGen-mono\nGPT-3\nInstructGPT\nGPT-Codex\nFLAN-T5\nOPT\n109 1010 1011\nModel size\n2 × 101\n3 × 101\n4 × 101\nLoss\nLog-log plot of loss vs model size (no OPT, no FLAN-T5)\nGPT-3.5\nCodeGen-multi\nCodeGen-mono\nGPT-3\nInstructGPT\nGPT-Codex\nFigure 3: Classification loss over model size. Left: all models. Right: all models except Meta AI OPT and Google\nFLAN-T5 families.\ngeneration\" GPT-3 family, and somewhat non-\nmonotonic for the OPT and InstructGPT families.\nThe InstructGPT models perform worse than the\nbase GPT-3 models.\n• The Salesforce CodeGen models exhibit mostly\nflat scaling. The “mono” models which are\nfurther fine-tuned on Python-only data perform\nworse than the \"multi\" models they are based on.\n• The OpenAI Codex models are the only mod-\nels that seem to show positive scaling (which\nmay be spurious since they are only two data\npoints). However, the two GPT-3.5 models\n(text-davinci-002 and text-davinci-003,\nshown in the figures as red crosses) that further\nfine-tune code-davinci-002on English demon-\nstrations, lose their edge and end up performing\nworse than the base GPT-3 model of the same\nsize (davinci).\n• Google FLAN-T5 shows an unclear, oscillating\nscaling trend, with large error bars at each point.\nWe report numerical correlation results between\nmodel size and mean loss 11 in Table 1. Due to\nthe small number of model sizes per family, some\nof the p-values are quite high, but the numerical\nresults are consistent with the qualitative analysis.\nOverall, our analysis shows that autoregressive\ntext-based LLMs (even when previously pretrained\non code-based models) exhibit inverse scaling on\nour task, while the code-based models exhibit flat\nscaling which might possibly transition to positive\nscaling at the largest tested size, but fail to substan-\ntially improve over the text-based models.\n11in the log-log scale, which for Pearson’s correlation mea-\nsures the adherence to the (inverse of) power law scaling as\ndescribed by Kaplan et al. (2020).\n3.2 Chat LLMs Accuracy\nWe perform additional experiments on chat LLMs\nby OpenAI and Anthropic, whose APIs became\nrecently available. These models constrain both\nthe input text and the generated output to take the\nform of a dialogue between the user and the \"assis-\ntant\" (the model itself). Notably, the APIs of these\nmodels do not report log-probabilities, hence they\ncannot be used to score arbitrary texts. This pre-\nvents us from using the same experimental protocol\nof the other experiments. We instead reformulate\nthe task as binary classification where the model is\npresented with both the correct and incorrect forms\nof the same program in the same user message and\nis asked to select the correct one. We describe the\nmodels and the prompt templates in Appendix C.\nclaude-instant claude gpt-3.5-turbo gpt-4\nModel\n0\n20\n40\n60\n80\n100\nClassification accuracy of chat LLMs\nCorrect\nIncorrect\nInvalid\nFigure 4: Classification accuracy on chat LLMs. For\neach model, we report the percentage of correct, incor-\nrect and invalid outputs.\nResults We report the results in Figure 5. All\nthe models strongly prefer the incorrect programs,\n275\nalthough the classification accuracy is non-zero.\nThis may not be necessarily comparable to the zero\nclassification accuracy of the previous experiments,\ndue to the different experimental protocol. The\nAnthropic models (claude-instant and claude)\nshow better accuracy (10-18%) with positive scal-\ning and never produce invalid outputs. The OpenAI\nmodels (gpt-3.5-turbo and gpt-4) show low ac-\ncuracy (< 4%) with flat or inverse scaling and occa-\nsionally produce invalid outputs.\n3.3 Qualitative Experiments\nWe perform a small number of manual two-shot\nexperiments on GPT-3.5. We also carry out manual\nexperiments on OpenAI ChatGPT-3.512 and GPT-4\nmodels, where we interact with the models in mul-\ntiple rounds of dialogue, trying to hint the correct\nsolution. The models are still unable to provide the\ncorrect continuations. See Appendices E–G.\n4 Related work\nRecent work sought to characterize the quality of\nLLMs on a variety of tasks: BIG-bench (Srivas-\ntava et al., 2022) is a large collaboration which\nresulted in a suite of hard, disparate tasks which\nwere used to evaluate various LLMs. The study\nfound that scaling can be slower and less smooth\nthan expected by naive scaling laws, and social\nbiases sometimes show inverse scaling, also ob-\nserved by Parrish et al. (2022). Perez et al. (2021)\ninvestigated the effect of example selection in few-\nshot learning for LLMs, finding that previous stud-\nies generally overestimated model quality due to\nmethodological issues. Lin et al. (2022) attempted\nto measure the truthfulness of the answer provided\nby LLMs on tasks involving real-world knowledge,\nfinding that while larger models tend to provide\nmore informative answers, they also tend to be less\ntruthful. However, this effect might be confounded\ndue to the dataset design to specifically be adversar-\nial for the largest model being evaluated (Kilcher,\n2021). Li et al. (2023) showed that similar to our\ncase, mathematical article processing is sensitive\nto semi-invariant symbol replacements. Ji et al.\n(2022) provide a broad survey about hallucination\n(generation of fluent yet incorrect information) by\nnatural language generation models.\n12https://openai.com/blog/chatgpt/\n5 Conclusions\nWe explored the ability of large language models\nto predict the correct continuations of fragments\nof Python programs in scenarios where the cor-\nrect continuations are statistically uncommon due\nto the redefinition of identifiers caused by a state-\nment that we included in the prompt. Not only\nall the tested models fail at this task, but some\nmodel families even display inverse scaling: they\nbecome worse, rather than better, with increasing\nmodel size. These results suggest that LLMs rely\non “shortcut learning”, i.e., weak, unstable, mostly\nlexical correlations in the data, rather than an un-\nderstanding of the semantics of the data (in this\ncase, Python code) at a deep level. We believe that\nour results are important both for a better scien-\ntific understanding of the capabilities of LLMs and\nfor their practical relevance as a core technology\nfor automated code generation tools. Future work\ncould investigate scaling effects at larger model\nsizes, as well as on other programming languages.\nLimitations\nOur approach has the following limitations:\n1. It only considers swaps of pairs of functions at\nthe top-level scope, which is a small set of all\nthe quasi-invariances of the Python program-\nming language.\n2. It only considers code generation in top-level\nfunctions, hence it does not evaluate class\nmethods.\n3. It relies on a syntactic substitution to generate\n\"correct\" gold truth outputs, which may fail if\nthe swapped functions are called by a string\nexpression through evalor or queried by their\nstring names using the reflection facilities.\n4. In our experiments, we can evaluate only a\nsmall number of model sizes per family, since\nthese are the only ones available, therefore\nthe p-values of the correlation with the loss\nanalysis are high.\n5. The independent reproducibility of the experi-\nments on closed-source models is predicated\non the continued availability of a publicly-\naccessible API. At the time of writing, our\nexperiments on the OpenAI \"Codex\" models\nare no longer reproducible without support\nfrom OpenAI.\n276\nItems 1 and 2 can be in principle treated by consid-\nering more complex code transformations, which\nwe leave for future work. Item 3 is harder to tackle\nin the general case because of undecidability is-\nsues. Item 4 could be addressed by reproducing our\nexperiments on a model family that encompasses\nmore model sizes, should it become available for\npublic experimentation. Item 5 is an unavoidable\nconsequence of using closed-source models.\nEthics Statement\nWe do not perform experiments on human subjects.\nOur work involves generating a dataset of public\ndata scraped from the GitHub and evaluating it on\nmultiple large language models. We release our\ndataset and the code used to generate it. We filtered\nour dataset to make sure that all the data that we\nused has been relesed under the CC-BY-4.0 license,\nwhich in our understanding allows for re-releasing,\nhowever our filtering procedure is heuristic which\nimplies that there is the possibility that some of the\nincluded data may be in violation of its license. In\norder to mitigate this hazard, we provide a clearly\ndocumented takedown option on the repository on\nwhich we will host this data, enabling people to\nclaim copyright and ask for removal of their data.\nAcknowledgements\nWe thank the reviewers for their helpful comments.\nWe thank the Inverse Scaling Prize competition\norganisers (McKenzie et al., 2022a) for organis-\ning the challenge and donating part of the OpenAI\nAPI credits that were used in our experiments. We\nare grateful to Apart Research13 for their donation\nthat supported the purchase of additional OpenAI\nAPI credits and provided personal financial sup-\nport to Antonio Valerio Miceli-Barone. This work\nwas supported by the UKRI Research Node on\nTrustworthy Autonomous Systems Governance and\nRegulation (grant EP/V026607/1) which provided\nfunding for Antonio Valerio Miceli-Barone. The\nexperiments in this work on open source LLMs\nwere supported by a compute grant (UKRI HPC)\nfrom the Baskerville service at the University of\nBirmingham.\nReferences\nAvishkar Bhoopchand, Tim Rocktäschel, Earl T. Barr,\nand Sebastian Riedel. 2016. Learning python code\n13https://apartresearch.com/\nsuggestion with a sparse pointer network. ArXiv\npreprint, abs/1611.08307.\nMichael M. Bronstein, Joan Bruna, Taco Cohen, and\nPetar Velickovic. 2021. Geometric deep learning:\nGrids, groups, graphs, geodesics, and gauges. CoRR,\nabs/2104.13478.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020a. Language models are few-shot learners. In\nAdvances in Neural Information Processing Systems\n33: Annual Conference on Neural Information Pro-\ncessing Systems 2020, NeurIPS 2020, December 6-\n12, 2020, virtual.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020b. Language models are few-shot learners. In\nAdvances in Neural Information Processing Systems\n33: Annual Conference on Neural Information Pro-\ncessing Systems 2020, NeurIPS 2020, December 6-\n12, 2020, virtual.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Ponde de Oliveira Pinto, Jared Kaplan,\nHarrison Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-V oss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Joshua Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. 2021. Evaluat-\ning large language models trained on code. ArXiv\npreprint, abs/2107.03374.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\n277\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Al-\nbert Webson, Shixiang Shane Gu, Zhuyun Dai,\nMirac Suzgun, Xinyun Chen, Aakanksha Chowdh-\nery, Alex Castro-Ros, Marie Pellat, Kevin Robinson,\nDasha Valter, Sharan Narang, Gaurav Mishra, Adams\nYu, Vincent Zhao, Yanping Huang, Andrew Dai,\nHongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-\ncob Devlin, Adam Roberts, Denny Zhou, Quoc V . Le,\nand Jason Wei. 2022. Scaling instruction-finetuned\nlanguage models.\nTaco S. Cohen and Max Welling. 2016. Group equivari-\nant convolutional networks. CoRR, abs/1602.07576.\nAndreea Deac, Théophane Weber, and George Papa-\nmakarios. 2023. Equivariant muzero.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn\nPresser, and Connor Leahy. 2021. The pile: An\n800gb dataset of diverse text for language modeling.\nArXiv preprint, abs/2101.00027.\nRobert Geirhos, Jörn-Henrik Jacobsen, Claudio\nMichaelis, Richard S. Zemel, Wieland Brendel,\nMatthias Bethge, and Felix A. Wichmann. 2020.\nShortcut learning in deep neural networks. ArXiv\npreprint, abs/2004.07780.\nDanny Hernandez, Jared Kaplan, Tom Henighan, and\nSam McCandlish. 2021. Scaling laws for transfer.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu,\nDan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea\nMadotto, and Pascale Fung. 2022. Survey of halluci-\nnation in natural language generation. ACM Comput.\nSurv. Just Accepted.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B.\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models.\nYannic Kilcher. 2021. Does gpt-3 lie? - misinformation\nand fear-mongering around the truthfulqa dataset.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners.\nWeixian Waylon Li, Yftah Ziser, Maximin Coavoux,\nand Shay B. Cohen. 2023. BERT is not the count:\nLearning to match mathematical statements with\nproofs. In Proceedings of the 17th Conference of\nthe European Chapter of the Association for Compu-\ntational Linguistics, pages 3581–3593, Dubrovnik,\nCroatia. Association for Computational Linguistics.\nYujia Li, David Choi, Junyoung Chung, Nate Kush-\nman, Julian Schrittwieser, Rémi Leblond, Tom Ec-\ncles, James Keeling, Felix Gimeno, Agustin Dal\nLago, Thomas Hubert, Peter Choy, Cyprien de Mas-\nson d’Autume, Igor Babuschkin, Xinyun Chen, Po-\nSen Huang, Johannes Welbl, Sven Gowal, Alexey\nCherepanov, James Molloy, Daniel J. Mankowitz,\nEsme Sutherland Robson, Pushmeet Kohli, Nando\nde Freitas, Koray Kavukcuoglu, and Oriol Vinyals.\n2022. Competition-level code generation with alpha-\ncode. Science, 378(6624):1092–1097.\nStephanie Lin, Jacob Hilton, and Owain Evans. 2022.\nTruthfulQA: Measuring how models mimic human\nfalsehoods. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 3214–3252, Dublin,\nIreland. Association for Computational Linguistics.\nIan McKenzie, Alexander Lyzhov, Alicia Parrish,\nAmeya Prabhu, Aaron Mueller, Najoung Kim, Sam\nBowman, and Ethan Perez. 2022a. The inverse scal-\ning prize.\nIan McKenzie, Alexander Lyzhov, Alicia Parrish,\nAmeya Prabhu, Aaron Mueller, Najoung Kim, Sam\nBowman, and Ethan Perez. 2022b. Inverse scaling\nprize: First round winners.\nAntonio Valerio Miceli Barone and Rico Sennrich. 2017.\nA parallel corpus of python functions and documen-\ntation strings for automated code documentation and\ncode generation. In Proceedings of the Eighth In-\nternational Joint Conference on Natural Language\nProcessing (Volume 2: Short Papers), pages 314–\n319, Taipei, Taiwan. Asian Federation of Natural\nLanguage Processing.\nFelix Musil, Andrea Grisafi, Albert P. Bartók,\nChristoph Ortner, Gábor Csányi, and Michele Ce-\nriotti. 2021. Physics-inspired structural representa-\ntions for molecules and materials. Chemical Reviews,\n121(16):9759–9815. PMID: 34310133.\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan\nWang, Yingbo Zhou, Silvio Savarese, and Caiming\nXiong. 2022. Codegen: An open large language\nmodel for code with multi-turn program synthesis.\nOpenAI. 2023. Gpt-4 technical report.\n278\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback.\nAlicia Parrish, Angelica Chen, Nikita Nangia,\nVishakh Padmakumar, Jason Phang, Jana Thompson,\nPhu Mon Htut, and Samuel Bowman. 2022. BBQ:\nA hand-built bias benchmark for question answering.\nIn Findings of the Association for Computational\nLinguistics: ACL 2022, pages 2086–2105, Dublin,\nIreland. Association for Computational Linguistics.\nEthan Perez, Douwe Kiela, and Kyunghyun Cho. 2021.\nTrue few-shot learning with language models. In Ad-\nvances in Neural Information Processing Systems 34:\nAnnual Conference on Neural Information Process-\ning Systems 2021, NeurIPS 2021, December 6-14,\n2021, virtual, pages 11054–11070.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1–140:67.\nH. Gordon Rice. 1953. Classes of recursively enumer-\nable sets and their decision problems. Transactions\nof the American Mathematical Society, 74:358–366.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal,\nAlec Radford, and Oleg Klimov. 2017. Proximal\npolicy optimization algorithms. ArXiv preprint,\nabs/1707.06347.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1715–1725,\nBerlin, Germany. Association for Computational Lin-\nguistics.\nAarohi Srivastava, Abhinav Rastogi, Abhishek B\nRao, Abu Awal Md Shoeb, Abubakar Abid, Adam\nFisch, Adam R. Brown, Adam Santoro, Aditya\nGupta, Adrià Garriga-Alonso, Agnieszka Kluska,\nAitor Lewkowycz, Akshat Agarwal, Alethea Power,\nAlex Ray, Alex Warstadt, Alexander W. Kocurek,\nAli Safaya, Ali Tazarv, Alice Xiang, Alicia Par-\nrish, Allen Nie, Aman Hussain, Amanda Askell,\nAmanda Dsouza, Ameet Annasaheb Rahane, Anan-\ntharaman S. Iyer, Anders Andreassen, Andrea San-\ntilli, Andreas Stuhlmuller, Andrew M. Dai, An-\ndrew D. La, Andrew Kyle Lampinen, Andy Zou,\nAngela Jiang, Angelica Chen, Anh Vuong, Ani-\nmesh Gupta, Anna Gottardi, Antonio Norelli, Anu\nVenkatesh, Arash Gholamidavoodi, Arfa Tabassum,\nArul Menezes, Arun Kirubarajan, Asher Mullokan-\ndov, Ashish Sabharwal, Austin Herrick, Avia Efrat,\nAykut Erdem, Ayla Karakacs, Bridget R. Roberts,\nBao Sheng Loe, Barret Zoph, Bartlomiej Bojanowski,\nBatuhan Ozyurt, Behnam Hedayatnia, Behnam\nNeyshabur, Benjamin Inden, Benno Stein, Berk Ek-\nmekci, Bill Yuchen Lin, Blake Stephen Howald,\nCameron Diao, Cameron Dour, Catherine Stinson,\nCedrick Argueta, C’esar Ferri Ram’irez, Chandan\nSingh, Charles Rathkopf, Chenlin Meng, Chitta\nBaral, Chiyu Wu, Chris Callison-Burch, Chris Waites,\nChristian V oigt, Christopher D. Manning, Christo-\npher Potts, Cindy Tatiana Ramirez, Clara Rivera,\nClemencia Siro, Colin Raffel, Courtney Ashcraft,\nCristina Garbacea, Damien Sileo, Daniel H Garrette,\nDan Hendrycks, Dan Kilman, Dan Roth, Daniel\nFreeman, Daniel Khashabi, Daniel Levy, Daniel\nGonz’alez, Danny Hernandez, Danqi Chen, Daphne\nIppolito, Dar Gilboa, David Dohan, D. Drakard,\nDavid Jurgens, Debajyoti Datta, Deep Ganguli, De-\nnis Emelin, Denis Kleyko, Deniz Yuret, Derek Chen,\nDerek Tam, Dieuwke Hupkes, Diganta Misra, Dil-\nyar Buzan, Dimitri Coelho Mollo, Diyi Yang, Dong-\nHo Lee, Ekaterina Shutova, Ekin Dogus Cubuk,\nElad Segal, Eleanor Hagerman, Elizabeth Barnes,\nElizabeth P. Donoway, Ellie Pavlick, Emanuele\nRodolà, Emma FC Lam, Eric Chu, Eric Tang,\nErkut Erdem, Ernie Chang, Ethan A. Chi, Ethan\nDyer, Ethan J. Jerzak, Ethan Kim, Eunice Engefu\nManyasi, Evgenii Zheltonozhskii, Fan Xia, Fate-\nmeh Siar, Fernando Mart’inez-Plumed, Francesca\nHapp’e, François Chollet, Frieda Rong, Gaurav\nMishra, Genta Indra Winata, Gerard de Melo, Ger-\nmán Kruszewski, Giambattista Parascandolo, Gior-\ngio Mariani, Gloria Wang, Gonzalo Jaimovitch-\nL’opez, Gregor Betz, Guy Gur-Ari, Hana Galija-\nsevic, Han Sol Kim, Hannah Rashkin, Hanna Ha-\njishirzi, Harsh Mehta, Hayden Bogar, Henry Shevlin,\nHinrich Schütze, Hiromu Yakura, Hongming Zhang,\nHubert Wong, Ian Aik-Soon Ng, Isaac Noble, Jaap\nJumelet, Jack Geissinger, John Kernion, Jacob Hilton,\nJaehoon Lee, Jaime Fernández Fisac, J. Brooker\nSimon, James Koppel, James Zheng, James Zou,\nJan Koco’n, Jana Thompson, Jared Kaplan, Jarema\nRadom, Jascha Narain Sohl-Dickstein, Jason Phang,\nJason Wei, Jason Yosinski, Jekaterina Novikova,\nJelle Bosscher, Jenni Marsh, Jeremy Kim, Jeroen\nTaal, Jesse Engel, Jesujoba Oluwadara Alabi, Ji-\nacheng Xu, Jiaming Song, Jillian Tang, Jane W\nWaweru, John Burden, John Miller, John U. Balis,\nJonathan Berant, Jorg Frohberg, Jos Rozen, José\nHernández-Orallo, Joseph Boudeman, Joseph Jones,\nJoshua B. Tenenbaum, Joshua S. Rule, Joyce Chua,\nKamil Kanclerz, Karen Livescu, Karl Krauth, Karthik\nGopalakrishnan, Katerina Ignatyeva, Katja Markert,\nKaustubh D. Dhole, Kevin Gimpel, Kevin Ochieng’\nOmondi, Kory Wallace Mathewson, Kristen Chia-\nfullo, Ksenia Shkaruta, Kumar Shridhar, Kyle Mc-\nDonell, Kyle Richardson, Laria Reynolds, Leo Gao,\nLi Zhang, Liam Dugan, Lianhui Qin, Lidia Contreras-\nOchando, Louis-Philippe Morency, Luca Moschella,\nLuca Lam, Lucy Noble, Ludwig Schmidt, Luheng\nHe, Luis Oliveros Col’on, Luke Metz, Lutfi Kerem\ncSenel, Maarten Bosma, Maarten Sap, Maartje ter\nHoeve, Madotto Andrea, Maheen Saleem Farooqi,\nManaal Faruqui, Mantas Mazeika, Marco Baturan,\n279\nMarco Marelli, Marco Maru, M Quintana, Marie\nTolkiehn, Mario Giulianelli, Martha Lewis, Martin\nPotthast, Matthew Leavitt, Matthias Hagen, M’aty’as\nSchubert, Medina Baitemirova, Melissa Arnaud,\nMelvin Andrew McElrath, Michael A. Yee, Michael\nCohen, Mi Gu, Michael I. Ivanitskiy, Michael Star-\nritt, Michael Strube, Michal Swkedrowski, Michele\nBevilacqua, Michihiro Yasunaga, Mihir Kale, Mike\nCain, Mimee Xu, Mirac Suzgun, Monica Tiwari, Mo-\nhit Bansal, Moin Aminnaseri, Mor Geva, Mozhdeh\nGheini, T MukundVarma, Nanyun Peng, Nathan\nChi, Nayeon Lee, Neta Gur-Ari Krakover, Nicholas\nCameron, Nicholas S. Roberts, Nicholas Doiron,\nNikita Nangia, Niklas Deckers, Niklas Muennighoff,\nNitish Shirish Keskar, Niveditha Iyer, Noah Con-\nstant, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar\nAgha, Omar Elbaghdadi, Omer Levy, Owain Evans,\nPablo Antonio Moreno Casares, Parth Doshi, Pas-\ncale Fung, Paul Pu Liang, Paul Vicol, Pegah\nAlipoormolabashi, Peiyuan Liao, Percy Liang, Pe-\nter W. Chang, Peter Eckersley, Phu Mon Htut, Pi-\nBei Hwang, P. Milkowski, Piyush S. Patil, Pouya\nPezeshkpour, Priti Oli, Qiaozhu Mei, QING LYU,\nQinlang Chen, Rabin Banjade, Rachel Etta Rudolph,\nRaefer Gabriel, Rahel Habacker, Ram’on Risco\nDelgado, Raphaël Millière, Rhythm Garg, Richard\nBarnes, Rif A. Saurous, Riku Arakawa, Robbe\nRaymaekers, Robert Frank, Rohan Sikand, Roman\nNovak, Roman Sitelew, Ronan Le Bras, Rosanne\nLiu, Rowan Jacobs, Rui Zhang, Ruslan Salakhut-\ndinov, Ryan Chi, Ryan Lee, Ryan Stovall, Ryan\nTeehan, Rylan Yang, Sahib J. Singh, Saif M. Mo-\nhammad, Sajant Anand, Sam Dillavou, Sam Shleifer,\nSam Wiseman, Samuel Gruetter, Sam Bowman,\nSamuel S. Schoenholz, Sanghyun Han, Sanjeev\nKwatra, Sarah A. Rous, Sarik Ghazarian, Sayan\nGhosh, Sean Casey, Sebastian Bischoff, Sebastian\nGehrmann, Sebastian Schuster, Sepideh Sadeghi,\nShadi S. Hamdan, Sharon Zhou, Shashank Srivas-\ntava, Sherry Shi, Shikhar Singh, Shima Asaadi,\nShixiang Shane Gu, Shubh Pachchigar, Shubham\nToshniwal, Shyam Upadhyay, Shyamolima Deb-\nnath, Siamak Shakeri, Simon Thormeyer, Simone\nMelzi, Siva Reddy, Sneha Priscilla Makini, Soo\nhwan Lee, Spencer Bradley Torene, Sriharsha Hat-\nwar, Stanislas Dehaene, Stefan Divic, Stefano Er-\nmon, Stella Rose Biderman, Stephanie C. Lin,\nStephen Prasad, Steven T. Piantadosi, Stuart M.\nShieber, Summer Misherghi, Svetlana Kiritchenko,\nSwaroop Mishra, Tal Linzen, Tal Schuster, Tao Li,\nTao Yu, Tariq A. Ali, Tatsuo Hashimoto, Te-Lin\nWu, Theo Desbordes, Theodore Rothschild, Thomas\nPhan, Tianle Wang, Tiberius Nkinyili, Timo Schick,\nT. N. Kornev, Timothy Telleen-Lawton, Titus Tun-\nduny, Tobias Gerstenberg, Trenton Chang, Trishala\nNeeraj, Tushar Khot, Tyler O’Brien Shultz, Uri Sha-\nham, Vedant Misra, Vera Demberg, Victoria Nya-\nmai, Vikas Raunak, Vinay Venkatesh Ramasesh,\nVinay Uday Prabhu, Vishakh Padmakumar, Vivek\nSrikumar, William Fedus, William Saunders, William\nZhang, W V ossen, Xiang Ren, Xiaoyu F Tong, Xinyi\nWu, Xudong Shen, Yadollah Yaghoobzadeh, Yair\nLakretz, Yang Song, Yasaman Bahri, Ye Ji Choi,\nYichi Yang, Yiding Hao, Yifu Chen, Yonatan Be-\nlinkov, Yu Hou, Yu Hou, Yushi Bai, Zachary Seid,\nZhao Xinran, Zhuoye Zhao, Zi Fu Wang, Zijie J.\nWang, Zirui Wang, Ziyi Wu, Sahib Singh, and Uri\nShaham. 2022. Beyond the imitation game: Quanti-\nfying and extrapolating the capabilities of language\nmodels. ArXiv, abs/2206.04615.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA, pages 5998–6008.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022. Opt: Open pre-\ntrained transformer language models.\nA Models\nGPT-3 LLMs in the OpenAI GPT-3 family, avail-\nable in different generations:14\n• “First generation” GPT-3 (Brown et al., 2020b),\ntrained with an unsupervised maximum likeli-\nhood estimate next-token prediction objective\non raw, byte-pair-encoding tokenized (Sennrich\net al., 2016) text crawled from the web. We\nuse the four models available on the public API:\nada(0.35B parameters), babbage(1.3B), curie\n(6.7B) and davinci(175B).\n• “Second generation” InstructGPT (Ouyang et al.,\n2022), fine-tuned on human-written demon-\nstrations and human-vetted samples (OpenAI\n“FeedME” approach). text-ada-001 (0.35B),\ntext-babbage-001 (1.3B), text-curie-001\n(6.7B) and text-davinci-001(175B).\n• “Third generation” GPT-3.5. Two “Codex”\nmodels trained on code, similar to Chen\net al. (2021): code-cushman-001 (12B) and\ncode-davinci-002 (175B), and two models\nbased on code-davinci-002 and further fine-\ntuned on human demonstrations with FeedME\nand PPO (Schulman et al., 2017), respectively:\ntext-davinci-002 and text-davinci-003\n(both 175B). Unfortunately, at the time of writ-\ning, the Codex models are no longer available on\nthe OpenAI API.\n14The publicly-available OpenAI models may differ\nfrom those described in the papers. Refer to https:\n//beta.openai.com/docs/models/gpt-3 and https://\nbeta.openai.com/docs/model-index-for-researchers\nfor a detailed description.\n280\nOur experiments on the OpenAI models were\nperformed with their public API, at a cost of ap-\nproximately 90 USD.\nCodeGen Salesforce CodeGen models 15 (Ni-\njkamp et al., 2022). CodeGen is available in two\nfamilies of auto-regressive LMs:\n• codegen-X-multi: first pretrained on the Pile\n(Gao et al., 2021), an English text corpus, then\nfine-tuned on a corpus of multiple programming\nlanguages. We use the four available model sizes:\n0.35B, 2B, 6B and 16B.\n• codegen-X-mono: based on the “multi” models\nof corresponding size and further fine-tuned on\nPython data: 0.35B, 2B, 6B and 16B.\nOPT Meta AI OPT models16 (Zhang et al., 2022),\na family of auto-regressive LMs predominantly\ntrained on English text. We use the six available\nmodel sizes: 0.125B, 0.35B, 1.3B, 2.7B, 6.7B and\n13B.\nFLAN-T5 Google FLAN-T5 sequence-to-\nsequence models (Chung et al., 2022), obtained\nby fine-tuning the T5 models on a large number\nof tasks. The T5 models (Raffel et al., 2020) are\nthemselves pretrained on a combination of unsuper-\nvised language modeling (formulated as denoising\nautoencoding) and multiple supervised tasks. We\nevaluate each example in our dataset by presenting\nthe prompt (swap statement, function declaration\nand docstring) as an input to the encoder and\n“good” and “bad” classes as alternative inputs to\nthe decoder, for which the model computes the\nlikelihoods. We consider the following models:17\nflan-t5-small (0.08B), flan-t5-base (0.25B),\nflan-t5-large (0.78B), flan-t5-xl (3B) and\nflan-t5-xxl(11B).\nOur experiments on the CodeGen, OPT\nand FLAN-T5 models were performed on the\nBaskerville Tier 2 HPC platform.\nB Experiment on Non-builtin Functions\nWe report an additional variant of our main quanti-\ntative experiment, evaluating the effect of swapping\n15From Hugging Face: https://huggingface.co/docs/\ntransformers/model_doc/codegen\n16From Hugging Face: https://huggingface.co/docs/\ntransformers/model_doc/opt\n17From Hugging Face: https://huggingface.co/docs/\ntransformers/model_doc/flan-t5\n109 1010 1011\nModel size\n101\n6 × 100\n7 × 100\n8 × 100\n9 × 100\nLoss\nLog-log plot of loss vs model size (non-builtins)\nGPT-3\nFigure 5: Swap of non-builtin functions. Classification\nloss over model size. First generation GPT-3 models.\nnon-builtin top-level functions. These functions are\ndefined either within the repositories we sample\nfrom, or in external libraries. Since these func-\ntion are less standardized compared to the Python\nbuiltins, the question arises of whether the models\nwould display a similarly strong preference on their\nuse, ignoring the swap statements. Due to time and\ncost constraints, we only run this experiment on\nthe first generation GPT-3 models: ada, babbage,\ncurieand davinci.\nResults We report the results in Figure 4. The\nresults are qualitatively equivalent to those of\nthe experiments on the builtin functions: all the\nmodels prefer the incorrect but common usage\nof the swapped functions to the correct but out-\nof-distribution one, demonstrating inverse scaling.\nThe overall magnitude of the losses however is\nsmaller, indicating that the preference for the incor-\nrect usage is not as strong as in the builtin function\nexperiment.\n281\nC Quantitative Chat LLM Experiments\nThe most recent LLMs by OpenAI and Anthropic available on their respective public APIs are Chat\nLLMs. These models are constrained such that their input must be a sequence of messages of a dialogue\nbetween the user and an \"assistant\" representing the model itself18. The model then responds with a single\nmessage written in the \"assistant\" role. Notably, these systems don’t report log-probabilty scores, which\nprevents us from computing a cross-entropy loss for the correct continuation as we did in our main set of\nexperiments, therefore we reformulated the task as a binary choice between two options provided in the\nsame \"user\" message.\nPrompt\nFor each example present both the correct program and the incorrect program and ask the model to\ndetermine which one is more likely to be correct. We use the following prompt template:\nSystem\nYou are a helpful assistant.\nUser\nConsider the following Python programs:\nProgram 1:\n{program_1}\nProgram 2:\n{program_2}\nWhich program is more likely to be correct? Write only the number of the program\nand nothing else.\nwhere {program_1} and {program_2} are replaced by the correct program and the incorrect program,\neach consisting of the the swap statement, the function declaration and the docstring, followed by either\nthe correct or incorrect function body.\nWe submit this prompt to the model, setting sampling temperature to 0.0, then we parse the \"assistant\"\nreply, extracting the first integer to evaluate the output. If don’t find any integer or the integer is neither\n1 nor 2, we consider the response invalid. In order to avoid any bias dependent on the order in which\nthe programs are presented, each example is evaluated twice, once with the correct program in the\n{program_1}position, and once in the {program_2}position.\nModels\n• OpenAI gpt-3.5-turboversion 0301 powers the free version of ChatGPT and is believed to be a\ncompressed version of text-davinci-003 with additional instruction fine-tuning. gpt-4 version\n0314 (OpenAI, 2023) is the most recent and highest quality OpenAI model available at the time of\nwriting. The API cost of running our experiments was about 40 USD.\n• Anthropic claude-instant version 1.1 and claude version 1.3 are chat LLMs trained on human\nfeedback19. claude-instant is described as \"low latency\" while claude is described as \"superior\nreasoning\".\n18an additional \"system\" role can be used in the messages to specify the general task the LLM is expected to perform.\n19https://console.anthropic.com/docs\n282\nThe parameter number and training set sizes of these models are not publicly known, however it is\nplausible that gpt-4is larger than gpt-3.5-turboand claudeis larger than claude-instant.\nWe report the results of our experiments in §3.2.\nD Dataset Example\nWe present one full example from out dataset.\nHead\nlen, open = open, len\ndef importfile(path):\n\"\"\"Import a Python source file or compiled file given its path.\"\"\"\"\nIncorrect continuation\nfrom importlib.util import MAGIC_NUMBER\nwith open(path, 'rb') as ifp:\nis_bytecode = (MAGIC_NUMBER == ifp.read(len(MAGIC_NUMBER)))\nfilename = os.path.basename(path)\n(name, ext) = os.path.splitext(filename)\nif is_bytecode:\nloader = importlib._bootstrap_external.SourcelessFileLoader(name, path)\nelse:\nloader = importlib._bootstrap_external.SourceFileLoader(name, path)\nspec = importlib.util.spec_from_file_location(name, path, loader=loader)\ntry:\nreturn importlib._bootstrap._load(spec)\nexcept ImportError:\nraise Exception(path, sys.exc_info())\nCorrect continuation\nfrom importlib.util import MAGIC_NUMBER\nwith len(path, 'rb') as ifp:\nis_bytecode = (MAGIC_NUMBER == ifp.read(open(MAGIC_NUMBER)))\nfilename = os.path.basename(path)\n(name, ext) = os.path.splitext(filename)\nif is_bytecode:\nloader = importlib._bootstrap_external.SourcelessFileLoader(name, path)\nelse:\nloader = importlib._bootstrap_external.SourceFileLoader(name, path)\nspec = importlib.util.spec_from_file_location(name, path, loader=loader)\ntry:\nreturn importlib._bootstrap._load(spec)\nexcept ImportError:\nraise Exception(path, sys.exc_info())\nThe statement at the beginning of the prompt swaps the identifiers for the builtinlenand openfunctions.\nThe incorrect continuation still uses the open function to open the file and len to compute the length of\nthe byte array MAGIC_NUMBER, in the correct continuation instead, open and lenare swapped.\n283\nE Example of Few-shot Interaction with GPT-3\nWe report an example of a manual few-shot interaction with GPT-3.5 text-davinci-003 where we\nprovide two correct examples in the prompt and we ask the model to generate a continuation on a third\nexample. This task should be easier than the ones we consider in our dataset because the two in-context\nexamples should cue the model about the need to swap the builtin function identifiers in the function\nbodies. Nevertheless, the model fails to swap the builtins in the generated continuation. Running the\nexample multiple times in order to account for sampling randomness results in slightly different but\nfunctionally equivalent continuations, all containing the same mistake. Switching to code-davinci-002\nalso does not help.\nPrompt\nlen, print = print, len\ndef print_len(x):\n\"Print the length of x\"\nlen(print(x))\ndef foo(x):\n\"Print 'foo' if the length of x is even, otherwise print 'bar' \"\nif print(x) % 2 == 0:\nlen('foo')\nelse:\nlen('bar')\nany, all = all, any\ndef bar(x, y)\n\"Return true if all the elements of x are \\\npositive numbers and any element of y is a negative number\"\nGenerated continuation\nreturn all(x > 0) and any(y < 0)\nF ChatGPT-3.5 Example\nWe present an interaction with OpenAI ChatGPT20. The model not only fails to generate the correct code\ncompletion on the first try, but still fails even when the error is pointed out in multiple rounds of dialogue.\nIt fails even when prompted with the Let’s think step by stepphrase Kojima et al. (2022) found to\nimprove zero-shot \"reasoning\" abilities.\nUser\nComplete the following Python program:\nlen, print = print, len\ndef print_len(x):\n\"Print the length of x\"\n# complete this function\n20While we run this experiment on the ChatGPT web interface https://chat.openai.com/, this model is presumed to be\nequivalent to gpt-3.5-turbo-0301available on the OpenAI API and Playground web interface\n284\nGenerated response\ndef print_len(x):\n\"Print the length of x\"\nprint(len(x))}\nUser\nwhat does the first line of the program that I’ve written do?\nGenerated response\nThe first line of the program is swapping the values of the variables len and print. This means that\nlen will now have the value of the print function and print will have the value of the len function.\nThis means that if you call len in the program, it will actually run the print function, and if you call\nprint, it will run the len function. This is known as \"aliasing\" or \"renaming\" variables.\nUser\nso, was you program completion correct?\nGenerated response\nYes, my completion of the program is correct. The function print_len simply calls the print function\n(which is actually the len function due to the variable swapping in the first line of the program) and\npasses in the argument x. This will print the length of x.\nUser\nshould it be print(len(x)) or len(print(x)) ?\nGenerated response\nIt should be print(len(x)). The print function (which is actually the len function due to the variable\nswapping in the first line of the program) is called and passed the argument x. The len function is\nthen called on x and the result of this function is passed as an argument to the print function, which\nprints the result.If it were len(print(x)), the print function (which is actually the len function) would\nbe called with x as the argument, and then the result of this function (which would be the length of\nx) would be passed as an argument to the len function, which would simply return the length of the\nresult (which would always be 1 since the result is a number). This would not produce the desired\noutput.\nUser\nAre you sure about this reasoning? Let’s think step by step\n285\nGenerated response\nYes, I am sure about this reasoning. Here is a step-by-step breakdown of what happens when\nthe print_len function is called: The print_len function is called with an argument x. The print\nfunction is called with the argument len(x). The len function is called on x and returns the length\nof x. The print function (which is actually the len function due to the variable swapping in the first\nline of the program) is called with the result of the len function (the length of x) as the argument.\nThe print function prints the result (the length of x). If it were len(print(x)), the following would\nhappen:The print_len function is called with an argument x. The print function (which is actually\nthe len function due to the variable swapping in the first line of the program) is called with the\nargument x. The print function (which is actually the len function) returns the length of x. The len\nfunction is called with the result of the print function (the length of x) as the argument. The len\nfunction returns the length of the result (which would always be 1 since the result is a number).\nThis does not produce the desired output of printing the length of x.\nUser\nPoint 3. of your explanation of print(len(x)) is incorrect because print and len are swapped\nGenerated response\nYou are correct, point 3 of my explanation for print(len(x)) is incorrect because print and len are\nswapped. Here is the corrected step-by-step breakdown: The print_len function is called with an\nargument x. The len function (which is actually the print function due to the variable swapping in\nthe first line of the program) is called with the argument x. The print function (which is actually\nthe len function due to the variable swapping in the first line of the program) returns the length of\nx. The print function (which is actually the len function) is called with the result of the len function\n(the length of x) as the argument. The print function (which is actually the len function) prints the\nresult (the length of x).\nUser\nin point 5, how can print , which is actually len, print anything?\nGenerated response\nYou are correct that in point 5, it is incorrect to say that the print function (which is actually the\nlen function) is printing anything. The correct statement would be that the len function (which is\nactually the print function due to the variable swapping in the first line of the program) is printing\nthe result (the length of x). I apologize for the confusion. Here is the corrected step-by-step\nbreakdown: The print_len function is called with an argument x. The len function (which is\nactually the print function due to the variable swapping in the first line of the program) is called\nwith the argument x. The print function (which is actually the len function due to the variable\nswapping in the first line of the program) returns the length of x. The print function (which is\nactually the len function) is called with the result of the len function (the length of x) as the\nargument. The len function (which is actually the print function) prints the result (the length of x).\nUser\ngood, so the correct program is\n286\nGenerated response\nYes, the correct program is:\nlen, print = print, len\ndef print_len(x):\n\"Print the length of x\"\nprint(len(x))\nThis program will correctly swap the values of the len and print variables, and then call the print\nfunction (which is actually the len function due to the variable swapping) with the argument x. The\nlen function will be called on x and the result (the length of x) will be passed as an argument to the\nprint function, which will then print the result.\nUser\nWrong.\nGenerated response\nI apologize if my previous explanations were incorrect. Please let me know how I can better assist\nyou in understanding the program.\nG GPT-4\nWe present an interaction with OpenAI GPT-4 version 0314.21. We use a temperature of 1.0. GPT-4 is the\nmost recent and most generally accurate OpenAI LLM available at the time of writing, nevertheless it\nfails to generate the correct program even when cued by the user. We note that our information about the\ninner-workings of GPT-4 are more limited than with the previous versions of the GPT model family.\nSystem\nYou are an helpful assistant.\nUser\nComplete the following Python program:\nlen, print = print, len\ndef print_len(x):\n\"Print the length of x\"\n# complete this function\n21https://platform.openai.com/playground?mode=chat&model=gpt-4-0314\n287\nGenerated response\nHere’s the complete Python program:\nlen, print = print, len\ndef print_len(x):\n\"Print the length of x\"\nprint(len(x))\n# Example usage\ntest_string = \"Hello, World!\"\nprint_len(test_string)\nThis program swaps the ‘print()‘ and ‘len()‘ functions, and then defines a function ‘print_len()‘ that\ntakes a single argument ‘x‘ and prints its length using the swapped ‘print()‘ and ‘len()‘ functions.\nUser\nwhat does the first line of the program that I’ve written do?\nGenerated response\nThe first line of the program you’ve written swaps the ‘print‘ and ‘len‘ functions:\nlen, print = print, len\nHere, you are creating a tuple with ‘(print, len)‘ and then unpacking that tuple into the variables\n‘len‘ and ‘print‘. As a result, the ‘print‘ variable now holds the built-in ‘len‘ function, and the ‘len‘\nvariable holds the built-in ‘print‘ function.\nAfter this line, whenever you use ‘len‘ in your code, it will call the ’print’ function, and when you\nuse ‘print‘, it will call the ’len’ function. This can be confusing and is generally not recommended\nfor writing clear, maintainable code, but it seems that the intention here is to create a playful or\nunconventional challenge.\nUser\nso, was you program completion correct?\n288\nApologies for the confusion in my initial response. Since the ‘print()‘ and ‘len()‘ functions were\nswapped in the first line, my initial completion was incorrect. Here’s the corrected completion:\nlen, print = print, len\ndef print_len(x):\n\"Print the length of x\"\nlen(x) # since len now refers to print function\n# Example usage\ntest_string = \"Hello, World!\"\nprint_len(test_string)\nNow, in the ‘print_len()‘ function, the ‘len‘ function is used to print the length of the input ‘x‘\nbecause, after the first line, ‘len‘ actually refers to the original ‘print‘ function.\nUser\nis len(x) correct?\nGenerated response\nApologies again, I realize I made a mistake in my previous response. Since the functions are\nswapped, you should use the ‘print‘ function, which now refers to the original ‘len‘ function, to\nget the length of the input ‘x‘. Here’s the corrected completion:\nlen, print = print, len\ndef print_len(x):\n\"Print the length of x\"\nlen(print(x)) # since print now refers to len function\n# Example usage\ntest_string = \"Hello, World!\"\nprint_len(test_string)\nIn the ‘print_len()‘ function, we now use ‘len(print(x))‘, which calculates the length of ‘x‘ (because\n‘print‘ now refers to the original ‘len‘ function) and then prints it using the swapped ‘len‘ function\n(which now refers to the original ‘print‘ function).\nH Program Equivariances and Invariances\nIn this section we provide the formal definition of program equivariances and invariances, and specifically\nof α-equivalence, which the identifier swaps in Python lead to. The definition relies on the notion of\nsyntactic transformations which can be formalized as the algebraic structure of a group.\nGroup action Let Gbe a group with identity element ϵand X be a set. The function T : G×X →X\nis a (left) group actionof Gon X if ∀x∈X,g ∈G,h ∈G\nT(ϵ,x) =T(x)\nT(g·h,x) =T(g,T (h,x))\n289\nIntuitively, T is a transformation on the elements of Xwhich is parameterized by the elements of group G\non the in a way consistent with the group structure, so that the identity element corresponds to the identity\ntransformation and combining the transformation parameters with their own group operation and then\napplying the result is equivalent to applying them in sequence.\nGroup equivariance and invariance Let Gbe a group, X and Y be sets. Let T : G×X →X and\nS : G×Y →Y be (left) group actions of Gon X and Y respectively. The function f : X →Y is (left)\nequivariant w.r.t. group Gand T and Sif ∀x∈X,g ∈G\nS(g,f(x)) =f(T(g,x))\nThis means that applying the transformation T parameterized by gon xand then evaluating f on it is\nequivalent to evaluating f(x) first and then transforming the result with Sparameterized by g.\nIn the special case where Sis trivial on the image of f, that is ∀x∈X,g ∈G\nS(g,f(x)) =f(x)\nthen f is (left) invariantw.r.t. Gand T, which means that f effectively ignores the transformation T on\nits inputs.\nThere has been an interest in recent years in applying these concepts to deep learning, either by measur-\ning the extent to which models spontaneously learn equivariances or by designing model architectures that\nobey certain equivariances by construction, see Bronstein et al. (2021) for an extended survey. Previous\nwork usually considers equivariances w.r.t. geometrical transformations such as rotations and reflections\non data types with a natural physical interpretation, such as images (Cohen and Welling, 2016), molecules\n(Musil et al., 2021) or video game grid-world environments (Deac et al., 2023), but the theoretical frame-\nwork is general enough to encompass many forms of equivariances and data types, including programming\ncode.\nα-equivalence Let X the set of programs (or program fragments) in some language (e.g. Python), let\nthe function f denote their semantics (f can take additional arguments representing the program inputs\nand environment, and its image is a set of results or sequences of actions that result from the execution of\na program).\nLet Gthe group of the permutations of all syntactically valid identifier names. Let T(g,x) the transforma-\ntion that substitutes the identifiers in program xaccording to permutation g. If f is invariant w.r.t. Gand\nT then it means that swapping the identifiers inside a program does not affect its execution, a property\nwhich is usually called α-equivalence in the programming languages literature.\nIn many programming languages α-equivalence may only apply when swapping identifiers in whole\nprograms including the standard library of the language. Program fragments such as modules, classes or\nfunctions (procedures) may not have α-equivalence when identifiers defined outside them (e.g. at top-\nlevel) are swapped. In Python however, this effect can be compensated by inserting a swap statement right\nbefore the program fragment. If a permutation gacts non-trivially on top-level identifiers a0,a1,...,a n,\nthen the tuple assignment statement\nga_0, ga_1, [..., ga_n] = a_0, a_1, [..., a_n]\nwill usually make the identifier swap in the program fragment an invariance. This does not work in\nall cases because Python programs can access their own source code programmatically and reference\nidentifiers by name from arbitrarily computed strings. Checking for these conditions is undecidable in the\ngeneral case, however these are not common programming practices, hence we can ignore them for our\npurposes.\n290\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nIn the limitations section, as it should be\n□\u0013 A2. Did you discuss any potential risks of your work?\nEthics staement\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nYes, in the abstract, which does not have a section number, and in the introduction, which is always\nsection 1.\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nNo.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nSection 2.3\n□\u0017 B1. Did you cite the creators of artifacts you used?\nSection 2.3\n□\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nSection 2.3\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nSection 2.3 and Limitations section\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNot applicable. Left blank.\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nSection 2.3\nC □\u0013 Did you run computational experiments?\nSection 3\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nSection 3, computational resources will be reported in the camera-ready version of the paper in\norder not to compromise anonymity\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n291\n□\u0017 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nThere were no hyperparameters to tune, since we only used pre-trained models.\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 3\n□ C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nNot applicable. Left blank.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNot applicable. Left blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNot applicable. Left blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. Left blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n292",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8197120428085327
    },
    {
      "name": "Python (programming language)",
      "score": 0.7996290922164917
    },
    {
      "name": "Exploit",
      "score": 0.6423978805541992
    },
    {
      "name": "Identifier",
      "score": 0.6057273149490356
    },
    {
      "name": "Programming language",
      "score": 0.5455774664878845
    },
    {
      "name": "Programming paradigm",
      "score": 0.4230615794658661
    },
    {
      "name": "Theoretical computer science",
      "score": 0.3544729948043823
    },
    {
      "name": "Artificial intelligence",
      "score": 0.342063307762146
    },
    {
      "name": "Computer security",
      "score": 0.13855192065238953
    }
  ]
}