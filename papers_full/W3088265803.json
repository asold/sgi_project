{
    "title": "State-of-the-art augmented NLP transformer models for direct and single-step retrosynthesis",
    "url": "https://openalex.org/W3088265803",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2538747543",
            "name": "Tetko Igor V.",
            "affiliations": [
                "Society for Chemical Engineering and Biotechnology",
                "Helmholtz Zentrum München"
            ]
        },
        {
            "id": "https://openalex.org/A2567954057",
            "name": "Karpov Pavel",
            "affiliations": [
                "Society for Chemical Engineering and Biotechnology",
                "Helmholtz Zentrum München"
            ]
        },
        {
            "id": "https://openalex.org/A4298541231",
            "name": "van Deursen, Ruud",
            "affiliations": [
                "Firmenich (Switzerland)"
            ]
        },
        {
            "id": "https://openalex.org/A3086674991",
            "name": "Godin, Guillaume",
            "affiliations": [
                "Firmenich (Switzerland)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2001496974",
        "https://openalex.org/W2580919858",
        "https://openalex.org/W2606363443",
        "https://openalex.org/W2747592475",
        "https://openalex.org/W2724493970",
        "https://openalex.org/W3014689923",
        "https://openalex.org/W3023042104",
        "https://openalex.org/W2324964582",
        "https://openalex.org/W2064535969",
        "https://openalex.org/W2769423117",
        "https://openalex.org/W2621742623",
        "https://openalex.org/W2972608805",
        "https://openalex.org/W1975147762",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2947423323",
        "https://openalex.org/W2994678679",
        "https://openalex.org/W2769756736",
        "https://openalex.org/W2991279511",
        "https://openalex.org/W2903262661",
        "https://openalex.org/W2038702914",
        "https://openalex.org/W2972498877",
        "https://openalex.org/W3030978062",
        "https://openalex.org/W3036578548",
        "https://openalex.org/W3009202547",
        "https://openalex.org/W3010145447",
        "https://openalex.org/W2010611103",
        "https://openalex.org/W2046747992",
        "https://openalex.org/W2954996726",
        "https://openalex.org/W2963396480"
    ],
    "abstract": null,
    "full_text": "ARTICLE\nState-of-the-art augmented NLP transformer\nmodels for direct and single-step retrosynthesis\nIgor V. Tetko 1,2✉, Pavel Karpov1,2, Ruud Van Deursen 3 & Guillaume Godin 3✉\nWe investigated the effect of different training scenarios on predicting the (retro)synthesis of\nchemical compounds using text-like representation of chemical reactions (SMILES) and\nNatural Language Processing (NLP) neural network Transformer architecture. We showed\nthat data augmentation, which is a powerful method used in image processing, eliminated the\neffect of data memorization by neural networks and improved their performance for pre-\ndiction of new sequences. This effect was observed when augmentation was used simulta-\nneously for input and the target data simultaneously. The top-5 accuracy was 84.8% for the\nprediction of the largest fragment (thus identifying principal transformation for classical\nretro-synthesis) for the USPTO-50k test dataset, and was achieved by a combination of\nSMILES augmentation and a beam search algorithm. The same approach provided sig-\nniﬁcantly better results for the prediction of direct reactions from the single-step USPTO-MIT\ntest set. Our model achieved 90.6% top-1 and 96.1% top-5 accuracy for its challenging mixed\nset and 97% top-5 accuracy for the USPTO-MIT separated set. It also signiﬁcantly improved\nresults for USPTO-full set single-step retrosynthesis for both top-1 and top-10 accuracies.\nThe appearance frequency of the most abundantly generated SMILES was well correlated\nwith the prediction outcome and can be used as a measure of the quality of reaction\nprediction.\nhttps://doi.org/10.1038/s41467-020-19266-y OPEN\n1 Institute of Structural Biology, Helmholtz Zentrum München—Research Center for Environmental Health (GmbH), Ingolstädter Landstraße 1, D-85764\nNeuherberg, Germany.2 BIGCHEM GmbH, Valerystr. 49, D-85716 Unterschleißheim, Germany.3 Firmenich International SA, D-Lab by Firmenich, Rue de la\nBergère 7, CH-1242 Meyrin-Satigny, Switzerland.✉email: i.tetko@helmholtz-muenchen.de; guillaume.godin@ﬁrmenich.com\nNATURE COMMUNICATIONS|         (2020) 11:5575 | https://doi.org/10.1038/s41467-020-19266-y | www.nature.com/naturecommunications 1\n1234567890():,;\nT\no synthesize an organic compound is to solve a puzzle with\nmany pieces and potentially several pieces missing. Here,\nthe pieces are single reactions, andﬁnding their sequential\ncombination to create aﬁnal product is the retrosynthesis task.\nThe success of the logic of organic synthesis developed by\nCorey et al.1,2 triggered the development of computer programs\naiming to ﬁnd appropriate ways to synthesize a molecule. The\nﬁrst retrosynthesis program LHASA2 utilizes a template-based3,4\napproach. Every template (rule, synthon) in a curated database of\nknown transformations is sequentially applied to a target mole-\ncule, and then sets of reagents are selected according to a speciﬁed\nstrategy. Reagents, in turn, undergo the same decompositions\nuntil a set of commercially available compounds is found. Ret-\nrosynthesis always has multiple routes— a retrosynthetic tree—\nending with different starting materials. Thus, a practical algo-\nrithm for retrosynthesis has to solve not only the rule acquisition\nand selection problem but also has capabilities to effectively\nnavigate this tree5, taking into account different strategies. These\ntasks relate directly to artiﬁcial intelligence strategies6–8.\nDue to the difﬁculty of maintaining template databases, most\nprojects dependent on them, including LHASA, did not become\nwidely used tools. The only major exception is, perhaps, the\nprogram Synthia™ (previously CHEMATICA9) which is a suc-\ncessful commercial product. In the Synthia™ program, rules are\nautomatically extracted from atom-mapped reaction examples10.\nHowever, there is an ambiguity in the mapping deﬁnition and,\nmore importantly, the automatic rule does not take into account\nother undeﬁned possible reactive centers in a molecule. Applying\nsuch transformations may result in molecules that fail to react as\npredicted, e.g.,“out-of-scopes” and special care toﬁlter out these\ncases has to be taken5. An alternative approach for the extraction\nof these rules is to apply a data-driven deep learning technique\nthat corresponds to a machine learning approach where an\nalgorithm (usually in the form of a neural network) is trained on\nthe raw data. After the trainingﬁnishes, the network contains all\nthe implicitly encoded features (rules) of the corresponding input\nvia its parameters. Works on reaction prediction outcomes11 and\nretrosynthesis12,13 showed the feasibility of a symbolic approach,\nwhere reactions are written as SMILES14 strings as in a machine\ntranslation. The product is written in the “source language”,\nwhereas the set of reactants is written in the“target language”. For\nthe “reaction translation” task both languages, however, are\nSMILES strings, having the same alphabet and grammar. Theﬁrst\nworks on symbolic (retro)synthesis12,15 were carried out with\nSeq2Seq16 models following robust and more easy to train natural\nlanguage processing (NLP) transformer approaches 17,18 that\nbring state-of-the-art results11,19. Meanwhile other approaches\nbased on similarity20, convolutional21–23, and graphs24,25 show\npromising results.\nThe SMILES representation of molecules is ambiguous. Though\nthe canonicalization procedure exists26, it has been shown that\nmodels beneﬁt from using a batch of random SMILES (augmen-\ntation) during training and inference27–30. Recently, such aug-\nmentation was also applied to reaction modeling11,18,31,32. The\naugmented (also sometimes called“random”) SMILES are all valid\nstructures with the exception that the starting atom and the\ndirection of the graph enumerations are selected randomly.\nIn this article, we scrutinize the various augmentation regimes\nand show that augmentation leads to better performance compared\nto the standard beam search inference or evaluation of the model\nunder different temperatures. We clearly mention that our study is\nto predict single-step and not multi-step retrosynthesis, which has\nbeen also targeted using transformer33,34.W es h o wt h a tb yu s i n g\nmore complicated data augmentation strategies we decrease over-\nﬁtting35 of neural networks and increase their accuracy to achieve\ntop performances for both direct and retro-synthesis. We observe\nthat the harder are the data to train the model, the better it will\npredict new ones. Moreover, we introduce a new measure MaxFrag\naccuracy for the prediction of the largest fragment (thus identifying\nprincipal transformation for classical retro-synthesis).\nResults\nThe baseline dataset contained only canonical SMILES. The other\ndatasets also contained SMILES, augmented as described in\nthe section Supplementary methods. Four different scenarios\nwere used to augment training set sequences. Namely, we used\naugmentation of products only (xN), augmentation of products\nand reactants/reagents (xNF), augmentation of products and\nreactants/reagents followed by shufﬂing of the order of reactant/\nreagents (xNS), and ﬁnally mixed forward/reverse reactions,\nwhere each retrosynthesis reaction from xNS was followed by the\ninverse (forward synthesis) reaction (xNM). Only the simplest\naugmentation xN was used for test sets because no information\nabout reactant/reagents could be used for the retrosynthesis\nprediction. At least one copy of canonical SMILES for each\nreaction was present in all augmentation scenarios.\nReaction synthesis data. We used a training set ﬁltered from\nUSPTO database\n36 containing 50 k reactions classiﬁed into 10\nreaction types. We used splitting proposed by Liu et al.12 and\ndivided it into 40, 5, and 5 k reactions for the training, validation,\nand test sets, respectively. As in the previous study13, after obser-\nving that early stopping using the validation set did not improve\nmodel test accuracy (the model performance for each of the sets\nwas monotonically increasing with number of iterations, see Sup-\nplementary Fig. 1), we combined the training and the validation\nsets into a combined training set. The 5 k test reactions were\npredicted only once the model training wasﬁnished and were not\nused at any stage of the model development. In a similar way we\njoined training and validation sets of USPTO-MIT22 dataset for\ndirect reaction prediction. In order to provide a more straightfor-\nward comparison with results of the previous studies we also\nreported performances when developing models using only the\nrespective training sets. Moreover a model with the largest pub-\nlished USPTO-full set24 was also developed.\nAnalysis of canonical datasets. The development of a model with\ncanonical SMILES (x1) as the training set provided 40.9% accu-\nracy for prediction of the canonical test set. An attempt to use this\nmodel to predict the augmented test set (x5, x10), resulted in\nmuch lower top-1 predictions of 23.3% and 18.4%, respectively.\nThis result was to be expected, because the model trained with\nonly canonical sequences was not able to generalize and predict\naugmented SMILES, which use different styles of molecular\nrepresentation.\nAugmentation of products only (xN). The augmentation of the\nproducts (input data), with just one additional augmented\nSMILES x2, increased top-1 accuracy to 43.7% for the test data\ncomposed of canonical sequences. Increasing the number of\naugmentations in the training set did not increase the top-1\nprediction accuracy. Thus, the augmentation of the training set\nwith just one random SMILES contributed the best performance.\nThis result is in concordance with another study where only one\nrandom SMILES was used to augment data18.\nAnalysis of character and exact sequence-based prediction\naccuracy. To better understand the model training, we also\ndeveloped several models where approximately 10% of the\ndataset did not participate in training but was used to monitor its\nprediction performance. Different from the test set, which tested\nARTICLE NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-020-19266-y\n2 NATURE COMMUNICATIONS|         (2020) 11:5575 | https://doi.org/10.1038/s41467-020-19266-y | www.nature.com/naturecommunications\nthe performance of models when predicting a new reaction, the\nmonitoring set tested the ability of the Transformer to predict\ndifferent SMILES generated for the same reaction. The Trans-\nformer was able to recognize different representations of the\nsame reaction. For example, when training x1, the character and\nexact sequence-based accuracies when predicting the monitoring\nsequences were 96.5% and 34.5%, respectively. The ﬁnal per-\nformance for the test set, 40.9%, was higher because some\nreaction products from the transformer provided noncanonical\nSMILES, which were correctly matched after transformation to\ncanonical ones. When using augmented training sequences\n(x10), the accuracies increased to 99.97% and 98.9%, for char-\nacter and exact sequence-based accuracy, respectively (see Fig.1).\nThe transformer recognized different representations of SMILES\nfor reactants and reagents of the same training set reaction, and\nwas able to exactly restore the target products which were\nmemorized. Demonstrably, it was also able to memorize any\nrandom sequences. To show this, we used a random SMILES\nsequence (xNR set in Supplementary Tables 1 and 2, and Sup-\nplementary Fig. 2) instead of the canonical sequences as the\ntarget for prediction. While this task was more difﬁcult and took\nmore epochs to train, the transformer was able to perfectly\nmemorize random sequences. Since the SMILES prediction tar-\nget was random, the transformer was not able to learn canoni-\ncalization rules on how to write the target. Despite this fact, it\nstill calculated a top-1 prediction accuracy of 26.8% for the test\nset which was, however, signi ﬁcantly lower compared to the\n42.2% achieved using the x10 dataset with canonical sequences as\nthe target.\nAugmentation of reactants and reagents . A boost of the\nTransformer performance was observed when, in addition to\nproducts, i.e., the inputs SMILES, we also augmented the target\nSMILES, i.e., reactants and reagents. This task was more difﬁcult\nfor the transformer, which resulted in a drop in both character\nand sequence based scores for monitoring sequences during the\ntraining stage. For example, when using the training dataset with\none augmented SMILES, x2F, the character based accuracy\ndropped to 91.3%, which was lower than 98.6% calculated with\nthe x2 dataset composed of canonical product SMILES (Fig.1).\nFor a larger number of augmentations, the character-based\naccuracy converged to a plateau, e.g., 89.96% and 89.67% for the\nx5F and x20F training sets, respectively. The character-based\naccuracy was calculated as the percentage of exact matches\nbetween target and predicted sequences, e.g., “CCCCN” and\n“NCCCC” have an accuracy of 80%, despite being the same\nSMILES but written from different starting atoms. Thus despite\nthe fact that the Transformer faced a prediction of random\nSMILES, it was still able to provide a reasonable prediction of\ntheir character composition.\nHowever, of course, the transformer was not able to predict the\nexact random product SMILES. This resulted in a decrease in\nsequence-based accuracy based on the number of augmentations\nfor xNF training datasets (Fig.1, cyan circle). Still the transformer\nwas able to predict some of the sequences, which corresponded to\nthe subset of canonical sequences in the monitoring set.\nInterestingly, the sequence accuracy normalized to the percentage\nof canonical SMILES in the monitoring sets increased with the\nnumber of augmentations since some randomly generated\nsequences were canonical SMILES.\nTop-1 performance analysis. For augmentations with 1 or 2\nrandom SMILES, the top-1 prediction performance of the models\ntrained with augmentation of reactants and reagents only, xN,\nand full reaction augmentation, xNF, were similar. For a larger\nnumber of augmentations the models trained with xNF sets had\nsystematically better performances than those developed with xN\nsets (Fig. 2). The training with the x80F set provided the highest\ntop-1 performance of 52.3% when this model was applied to the\ntest set generated with x20 number of augmented sequences.\nWhile it was possible that further increase in the augmentations\ncould still increase the top-1 performance, we did not perform\nsuch calculations due to limitations on available computational\nresources.\nShufﬂing order of reactants. In addition to augmenting the full\nreaction, we also randomly shufﬂ\ned the orders of reactants (see\nxNS set description in Supplementary Tables 1 and 2). The effect\nof this additional data perturbation improved top-1 performance\nto 53.1% for the x20S training dataset applied to the test set with\nthe same number of augmentations (Supplementary Fig. 3).\n12345 1 0 2 0 4 0\n0\n20\n40\n60\n80\n100\nTraining set augmentation, N\nTraining set accuracy, %\nChar (xN sets)\nSequence (xN)\nChar (xNF)\nSequence (xNF)\nNormalised sequence (xNF)\nFig. 1 Character and exact sequence-based accuracies calculated for the\nmonitoring set.The transformer memorized the target sequences if the\ntarget sequences were all canonical SMILES (red dots). It also reasonably\npredicted the sequence composition for randomized target SMILES (cyan\nrectangle, dashed) but its performance decreased for prediction of exact\nfull SMILES (cyan circle). The performance normalized by the percentage of\ncanonical sequences increased with the number of augmentations,N, since\nsome of the random sequences were canonical ones.\n1 3 4 5 20 40\n40\n45\n50\nTraining set augmentation, N\nTest set accuracy, %\nTest x1, training xN\nTest x20, training xN\nTest x1, training xNF\nTest x20, training xNF\n1028 0\nFig. 2 Top-1 performance of models developed with different number of\naugmentation (shown onx-axis) and different augmentation scenarios\napplied to both test and training sets (red color: only products were\naugmented; cyan color: full reactions were augmented).The use of the\nlarge number of augmentations for the test set (solid lines) improved\nprediction accuracy for models developed with augmentation of full\nreactions but did not inﬂuence the performance of models where only input\ndata were augmented.\nNATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-020-19266-y ARTICLE\nNATURE COMMUNICATIONS|         (2020) 11:5575 | https://doi.org/10.1038/s41467-020-19266-y | www.nature.com/naturecommunications 3\nFurther increasing the number of augmentations resulted in the\nloss of top-1 prediction accuracy.\nShufﬂing and mixing of retrosynthesis and direct reactions.\nThe training of retrosynthesis and direct reactions simultaneously\ncould create a mixed representation of latent space and further\nincrease the ability of the transformer to make generalizations.\nWe tested this hypothesis by combining direct and reverse\nreactions in one training set by reversing the order of product/\nreactant + reagents and adding a dot to distinguish direct reac-\ntions (see e.g., Supplementary Table 2, x2M augmentation).\nContrary to previous analysis, which required 20 augmentations\nof training set sequences to achieve the highest performance, the\nmixed dataset achieved it with only 10 augmentations (Supple-\nmentary Fig. 3). Since the mixed dataset also included direct\nreactions, it had the same number of 19 augmented SMILES per\ncanonical SMILES as in the previous analyses. Thus, this number\nof augmentations was optimal for the training of the transformer.\nA smaller number of augmentations did not allow it to fully\nexplore the data diversity while a larger number created too much\nnoise and made it difﬁcult to learn canonization rules, which were\ninjected by the single canonical sequence. For the x10M training\nset, the transformer calculated 52.8%, which was similar to 53.1%\ncalculated using the x20S training dataset.\nTop-5 performance analysis. This measure provided a relaxed\nestimation of the performance of the model by measuring if the\ncorrect reaction is listed in the top-5 predicted reactions. Actually,\nit is questionable whether for retrosynthetic models having the\nhighest top-1 accuracy is desirable. The goal of a retrosynthetic\nmodel is to obtain several precursor suggestions and not exclu-\nsively the ones stated in the literature. Moreover, multiple reac-\ntions for the same product exist. An example includes the\naromatic substitution of an aryl halide (R-X) to an aryl amine (R-\nNH2) or aryl hydroxide (R-OH). Models with higher top-n scores\ndo suggest other probable reactions (indeed, all reactions amid\ntop-n have similar probability) which may correspond to those\nnot reported in the literature for the analyzed example. Thus\nmodels with higher top-N scores but with similar top-1 scores\ncould be interesting for a chemist since they do propose the\ncorrect prediction along with similar quality top-1 reactions.\nFor each number of augmentations, the top-5 performance\ngenerally increased with the number of augmented sequences. The\nhighest top-5 value was consistently calculated across different\nscenarios for training sets with 4–5 augmentations only (Fig.3).\nThe highest accuracy, 78.9%, was calculated for the mixture\ndataset using the x5M training set augmentation. This number\nhad approximately 1% higher accuracy than that calculated using\nthe x5S training set (Fig.3).\nReference USPTO-50 k model. For all studies we used aﬁxed\nnumber of epochsN = 100. However, we needed to conﬁrm that\nthis was a sufﬁcient number of epochs and to determine if we\ncould calculate better results by training for longer. We selected\nthe model developed with the x5M training set, which provided\nthe highest performance for top-5 accuracy, and trained it for an\nadditional 400 iterations (in total 500). This additional training\nimproved top-1 accuracy to 53.3% while top-5 performance\nincreased to 79.4% (Table1) when using beam= 5, e.g., the same\nas in previous analyses.\nFurther improvement was achieved by using a large number of\naugmentations, and x100 as the test set. With this setting the\nmodel achieved an accuracy of 53.6% and 80.8% for top-1 and\ntop-5 predictions, respectively.\nInﬂuence of temperature. In our previous study\n13, we observed\nthat using higher temperatures during beam search increased\nmodel accuracy for the top-1 prediction. It should be mentioned\nthat no augmentation was used in that study. Under the same\nexperimental setup with no augmentation, i.e., when predicting\ntest set composed of only canonical sequences, x1, the top-1\naccuracy of the model increased from 48.3% to 49.1% and 49.2%\nwhen using temperatures 1.3 or 1.5, respectively. However, the\ntop-1 and top-5 performances for the augmented data (x20)\ndecreased from 53.3% to 52.7% and 52.4%, respectively. For the\nsame test set the top-5 accuracies also decreased from 79.4% to\n77.7% and 77.4% for both temperatures, respectively. Thus, while\nhigher temperatures increased the variability of predictions and\nthus performance for prediction of canonical sequences, its effect\nwas negative for the augmented data. In particular, it resulted in\nthe lower accuracy of top-5 predictions.\nInﬂuence of beam search. In the above studies we consistently\nused a beam size of 5 for all analyses. The goal of the beam search\nwas to generate multiple predictions for the same data and thus to\nbetter explore the variability of predictions. For example, when\nusing the x20 test set and a beam size of 5, we obtained up to 100\nindividual predictions, which were used to select the most fre-\nquently appearing top-1 and top-5 sequences. Increasing the\nbeam size to 10 further improved top-1 by 0.2 to 53.5% and top-5\nby 0.6% to 80% for the test set. The decrease of the beam size to 3\nprovided a slightly higher top-1 score of 53.4% but decreased the\ntop-5 to 78.5% for the same test set. The use of beam size 1\nresulted in a top-1 accuracy of 53.3% and a reduced top-5\naccuracy of 75.3% (Table 1). These results were expected: the\nvariation of the beam size slightly inﬂuenced the identiﬁcation of\nthe highest ranked sequence but its smaller number reduced\nexploration of the space of other top-reactions for largern.\nBoth beam search and augmentation increased the number of\npredicted SMILES which in turn led to better accuracy of model\npredictions. Thus both of these methods could contribute to the\ngeneration of multiple predictions to be used to identify top-\nranked sequences. The maximum size of the beam was restricted\nby the size of the target vocabulary (number of characters in the\ntarget SMILES), which was 44 characters for our dataset. Because\nof the design of the beam search and because we explicitly\nexcluded duplicated predictions (see section“Analysis of predicted\nSMILES” as well as Supplementary Table 3), the dataset used for\n101\n50\n60\n70\n80\nTraining set augmentation, N\nAccuracy, %\nx\nxF\nxS\nxM\n54322 0 4 0\nFig. 3 Top-5 performance of transformer models developed with\ndifferent training set augmentation protocols for prediction of the x20\ntest set. Supplementary Tables 1 and 2 for description of the respective\nprotocols.\nARTICLE NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-020-19266-y\n4 NATURE COMMUNICATIONS|         (2020) 11:5575 | https://doi.org/10.1038/s41467-020-19266-y | www.nature.com/naturecommunications\nanalysis did not generate duplicated sequences for the same beam\nsearch. However, such sequences were indeed generated at\ndifferent positions of the beam as different representations of\nthe same SMILES. The number of non-unique sequences\ngenerated within the same beam search increased with the length\nof the beam. Interestingly, the use of canonical SMILES as input\ndata contributed to the largest number of unique SMILES, which\nwere 86%, 82% and 78% for beam searches of size 5, 10, and 44,\nrespectively. The use of augmented random SMILES as input\ncontributed smaller numbers of unique sequences, e.g., 42%, 28%\nand 13% for beam searches of size 5, 10, and 44, respectively. For\nboth types of SMILES some generated SMILES were erroneous\nand could not be correctly converted by RDKit. Such sequences\nwere excluded from analysis. For large beam sizes, canonical\nSMILES produced a much bigger percentage of incorrect SMILES,\nas compared to the use of random SMILES (see Supplementary\nFig. 4). The large difference in the results generated when starting\nfrom canonical and random SMILES was also observed for\nanalysis of the percentage of correct predictions for each beam\nposition. In general, the number of erroneous SMILES was low,\ne.g., on average it was less than 1% and 3% for beam search 10,\nwhen using augmented and canonical SMILES as input,\nrespectively (Supplementary Fig. 4). While graph-based methods\npredict exact chemical structures and thus have 0% syntactically\ninvalid SMILES, a few percentage points of incorrectly predicted\nstructures by the transformer model does not make a large\ndifference to these methods.\nThe use of canonical SMILES provided (Supplementary Fig. 4)\na higher accuracy for the ﬁrst beam position, but its accuracy\nwas much lower for other beams. This was because the\nTransformer generated canonical SMILES for the canonical\ninput sequences (e.g., 91% of valid SMILES produced at the\nposition 1 of the beam search for input canonical SMILES were\ncanonical ones) and since only one valid canonical SMILES\ncould be produced, it failed to generate new correct SMILES.\nIndeed, during the training phase, the transformer always had a\npair of canonical SMILES as input and target sequences.\nContrary to that, using augmented SMILES allowed more\nfreedom and allowed it to contribute valid but not necessarily\ncanonical SMILES (e.g., only 33% of generated SMILES at the\nposition one of the beam search were canonical ones if\naugmented SMILES were used as input).\nThe decrease in performance of SMILES generated when using\ncanonical SMILES was one of the main reasons to implement\ndeduplication of data and retain only theﬁrst SMILES for the\nprediction of reactions (see section “Analysis of predicted\nSMILES”). When deduplication was not performed and all\nSMILES generated during the beam search were used to rank\npredictions (compare Supplementary Tables 3 and 4), the top-1\nperformances of models were most signiﬁcantly affected when\nusing only few augmentations, e.g., for the reference model its\naccuracy dropped from 48.3% (reference prediction, Table1)t o\n47% but did not change for, e.g., top-5 performance. In principle,\nthe analysis retaining multiple predicted sequences was based on\nmore data and thus was more stable. Therefore, it could be used\nwhen several augmentations and/or large values of top-n are used\nfor analysis.\nAs it was mentioned above, both data augmentation and beam\nsearch could be used to generate multiple predictions. For the\nsame number of generated sequences, 1000 per SMILES, using a\nbeam = 10 search for the x100 set produced lower accuracy,\n53.5% compared to 53.7% using augmented data with the x1000\ntest set without any beam search. The performance of both\nmethods were the same and equal to 53.7% when the\ndeduplication procedure was not used. However, the beam search\ncontributed to better accuracy, i.e., 81% vs. 80% and 85.7% vs.\n84.3% compared to the use of augmententation alone for top-5\nand top-10, respectively. Thus, using beam search allowed a better\nexploration of data when suggesting several alternative reactions.\nIn any case the augmentation was a very important part of the\nbeam search and for the best performance, both of these\napproaches should be used simultaneously. We also do not\nexclude that optimization of the augmentation may improve its\nresults in the future. Moreover, data augmentation used alone\nwithout a beam search contributed superior models to the beam\nsearch used without any data augmentation.\nTable 1 Analysis of the reference model performance depending on the parameters of the application protocol.\nTest set x1 Test set x20 Test set x100\nApply model setting Top-1 Top-5 Top-1 Top-5 Top-1 Top-5 Top-10\nReference accuracya 48.5 72.5 53.3 79.4 53.6 80.8 85\nTemperature, t = 1.3 49.1 67.7 52.7 77.7 53.3 78.4 83.2\nNo beam search, i.e., beam= 1 47.7 47.7 53.3 75.3 53.8* 78.8 81.7\nBeam size, beam= 10 48.3 73.4 53.5 80 53.5 81* 85.7\nBeam size, beam= 44 48.3 72.5 53.5 80 53.5 80.5 85.8*\naThe ﬁnal reference model was built using 500 iterations for the x5M training set. Its reference performance was evaluated using beam size= 5, temperature= 1. The altered parameters are shown for\nseveral other application scenarios. For beam= 1 and x1000 augmentations the model calculated 53.7, 80 and 84.3 for top-1, top-5, and top-10 predictions, respectively. This augmentation as well asthe\none with beam size= 10 applied to x100 analyzed the same number of predicted sequences.\n* Indicate the best results. Larger beam sizes contributed better results for larger top-n predictions.\n0.25 0.5 0.75 1\n0\n20\n40\n60\n80\n100\nFrequency of the top−1 predicted reactions\nMaxFarg accuracy/percentage of reactions\nUSPTO 50k accuracy\nIBM−MIT mixture accuracy\nUSPTO−full\nIBM−MIT mixture density\nUSPTO 50k density\nUSPTO−full density\n<0.1\nFig. 4 Accuracy and density (fraction of predictions) of the Transformer\nfor MaxFrag top-1 retrosynthesis accuracy.The accuracy and density are\nshown as a function of the frequency of appearance of the top-1 SMILES in\nthe output of the Transformer for the respective test sets of the models.\nNATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-020-19266-y ARTICLE\nNATURE COMMUNICATIONS|         (2020) 11:5575 | https://doi.org/10.1038/s41467-020-19266-y | www.nature.com/naturecommunications 5\nAccuracy of prediction. For some reaction predictions without\nthe use of augmented sequences or position at the beam search\nthe majority of predicted sequences were identical, while for other\nreactions the Transformer generated as many different SMILES as\npossible reactants (see Supplementary Table 5). While the beam\ngeneration procedure guaranteed that each prediction had exactly\nthe same sequence of characters, in many cases the Transformer\nproduced multiple noncanonical instances of the same SMILES.\nThe frequency of the appearance of the most frequent (after\nconversion to the canonical representation) SMILES could,\ntherefore, indicate the conﬁdence of the transformer in the pre-\ndiction. Figure 4 indicates such frequency (which was calculated\non 100x augmented dataset) correlated well with the accuracy of\nprediction and could be used as a conﬁdence score for the che-\nmist. Indeed, the reactions in which the most frequent SMILES\ndominated amid all predictions for Top-1 were likely to be pre-\ndicted correctly. If the most frequent SMILES had low fre-\nquencies, such predictions were likely to be incorrect ones. For\nabout 20% of the most frequent predictions, the accuracy of the\nretrosynthesis prediction was above 80% and for 4% more than\n90%. It should be mentioned, that for a practical implementation\nwhich critically depends on the speed, e.g., multistep synthesis,\nthere is no reason to always run all 100 predictions to get the\nconﬁdence estimations. One can always estimate the probability\nof the most frequent SMILES and its conﬁdence interval based on\na much smaller number of predictions thus decreasing the\nnumber of calculations.\nAs shown in Fig.4, the same correlations were observed for\ntwo other datasets USPTO-MIT and USPTO-Full, which are\nanalyzed in the following section. The same approach can be used\nfor top- n predictions by suggesting one or more plausible\npathways for retrosynthesis. An example of such correlations\nfor Top-5 MaxFrag accuracy were shown in Supplementary\nFig. 5. Moreover, the same approach also predicted the accuracy\nfor the direct synthesis as it was demonstrated at Supplementary\nFig. 6. It should be mentioned that use of data augmentation is\nnot the only approach to estimate the accuracy of predictions, and\nother methods based on the likelihood of the direct reaction\nprediction were also proposed\n18,34 and were shown to correlate\nwith the accuracy of the predictions. A comparison of such\nmethods is beyond the scope of this study.\nAnalysis of prediction accuracy for different scenarios. The\naccuracy of the reference model was about 5% to 7% (top-1) and\n10% (top-5) higher for reactions without stereochemistry than for\nthose with it (Table2). 20% of the reactions in the test set con-\ntained molecules with stereochemistry. An increase in the num-\nber of augmentations of the test set increased the accuracy of\nboth stereo and non-stereochemical reactions. Stereochemical\nreactions in the dataset may also suffer from a larger number of\nannotation errors or/and can have lower prediction scores since\nsuch data were underrepresented in the training set. In addition,\nfor some reactions despite the relative stereochemistry being\nconserved it may still deﬁne confusing information for the model\ndue to the reactant satellite effect. The R/S could be also affected\nby the way the SMILES was written, e.g., from A to Z or Z to A.\nClassical retro-synthesis accuracy: recognition accuracy for the\nlargest fragment. The prediction of SMILES for retro-synthesis\nincludes exact prediction of the reactants. However, the same\nreaction performed using different reactants can result in a\nsimilar yield. In general the database does not contain all possible\nreaction conditions to make a given product. Therefore, a pre-\ndiction of only the main (the largest) reactant can be considered\nmore relevant for retro-synthesis predictions, since we need to\nﬁrst identify the reaction type. Indeed, a chemist generally writes\na retrosynthesis by decomposing a target molecule into pieces.\nThis classical procedure, focusing only on main compound\ntransformations, is the minimal information required to get an\nefﬁcient retrosynthesis route and at the same time all reactions\nneeded (see Fig. 5). The selection of reaction conditions of the\nreactions can be considered as a subsequent task.\nThat is why we decided to consider the recognition of the\nlargest reactant as a new measure of the model performance:\nclassical retro-synthesis accuracy, i.e., the accuracy of prediction\nof the“Maximal Fragment” (MaxFrag). The MaxFrag was 85.4%\nfor the top-5 reaction prediction (Table 2). The MaxFrag is\nimportant to estimate an ability of a system to automatically\ndeduce the correct reaction class. This strategy is orthogonal to\nexplicitly providing reaction class information as input to a\nmodel24. Adding the reaction class as prior information is\nequivalent to getting a hint on an exam: this is impractical and\nalso reduces the chance of proposing alternate feasible reactions.\nUsing MaxFrag is more accurate and logical than providing a\nreaction class as prior information. Besides MaxFrag and Top-n,\nother scores were proposed to evaluate the success of retro\nsuggestions/reactions, e.g., the matching score by Satoh and\nFunatsu37, the “in-scope” ﬁlter by Segler et al.5, and the forward\ntransformer score by Schwaller et al.34. However, MaxFrag is the\neasiest and the interpretable one.\nRetrosynthesis data quality and MaxFrag accuracy.T h eu s eo f\nthe classical retro-synthesis accuracy (MaxFrag top-n)c a l c u l a t e da\nsystematic higher accuracy in comparison to the traditional top-n\ns c o r e s .T oe x p l a i nt h i sf a c t ,w ea n a l y z e do u rd a t a s e t sa n df o u n df o u r\ntypes of reactions: non-reagent reactions, one reagent reactions,\nmultiple reagent reactions, and unclear reagent reactions. Non-\nreagent reactions were reactio n st h a td i dn o tw o r k( i . e . ,A+B−>A ) .\nTable 2 Prediction accuracy of the reference model for different subsets of the test set of USPTO-50k using a beam search of\nsize 10.\nTop-1 Top-5 Top-10\nTest set\naugmentation\nAll Stereo (20%) No stereo (80%) All Stereo (20%) No stereo (80%) All Stereo (20%) No stereo (80%)\nx1 48.3 44.7 49.2 73.4 67.3 74.9 77.4 71 79\nx20 53.4 47.3 55 80 73.3 81.9 84.2 79.2 85.4\nx100 53.5* 47.1 55.1 81* 74.6 82.6 85.7* 81.2 86.8\nMaxFrag,a x1 53.5 48.7 54.7 79.2 72.7 80.9 81.6 75.1 83.3\nMaxFrag, x20 58.5 52 60.1 84.7 79 86.1 88.6 83.6 89.8\nMaxFrag, x100 58.5* 51.2 60.3 85.4* 79.4 86.9 90* 85.1 91.2\naThe classical retro-synthesis accuracy was estimated as the percentage of correctly predicted largest fragments, i.e.,“maximum fragment” (MaxFrag) accuracy.\n* Indicate the best results.\nARTICLE NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-020-19266-y\n6 NATURE COMMUNICATIONS|         (2020) 11:5575 | https://doi.org/10.1038/s41467-020-19266-y | www.nature.com/naturecommunications\nOne reagent reactions had only one starting material for the product\n(A−> P), multiple reagents had multiple starting materials for the\nsame end product (A+B−>P ) , a n dﬁnally unclear reagents where\nthe reaction conditions, solvent, salts, and so on, were included as\nreagents (A+B +N−>P , w h e r e N w e r e c h e m i c a l s t h a t d i d n o t\nparticipate to form the product).D e p e n d i n go nt h ed a t a s e tt h e\nproportions of these reaction categories slightly varied. In the MIT\ndataset around 0.5% of reactionswere non-reagent reactions, and\naround 10% of the reactions were unclear reagent reactions while\nthere were less than 1% of such reactions in the USPTO-50 k dataset.\nThus, for the MIT set it would be impossible to fully predict about\n10% of reactions for the retrosynthesis, since they contained che-\nmicals “N” that did not form the reaction, but only conditions, sol-\nvent, etc. This more challenging problem of predicting not only the\nreactants but also the reagents, while still keeping diverse precursor\nsuggestions was addressed elsewhere34. For the direct synthesis that\nwas not a severe problem since the transformer could correctly\nidentify and pick-up the interacting components (“A” and “B”)a n d\npredict the product. However, the use of Top-n for retrosynthesis is\nquestionable due to the aforementioned problem. The use of Max-\nFrag accuracy decreased those effects by focusing on the main\nreagent. That is why, in our opinion, the MaxFrag score better\nreﬂected the chemistry than Top-1 accuracy.\nStill there is an unsolved challenge with this score due to the\npossibilities to synthesize the same products starting from multiple\nreagents. Both Top-n and MaxFrag Top-n scores were calculated by\nusing the exact match of the predicted and target molecules. But, for\nexample, in the reaction R-R1+NH3−> R-NH2 multiples choices\nof R1, i.e., -OH, -Cl, -Br, -I, or -F, would be all correct predictions.\nThe only difference would be the reaction rates and yields, which\nare not part of the prediction algorithms. Unfortunately the\ncurrently used scores, including MaxFrag, cannot yet correctly\naccount for this problem. The problem to some extent could be\nalleviated by using Top/MaxFrag- n instead of Top/MaxFrag-\n1 scores: by considering multiple reagents generated by the model,\nwe could also get the one provided in the initial reaction. Thus, the\nretrosynthesis task is not about getting high Top-1 accuracy. Any\nclassical organic synthesis book, such as the famous Larock’s\n“Comprehensive Organic Transformations”38 indicates multiple\nways to synthesize chemical compounds and this has to be reﬂected\nin the score. The classical retro-synthesis accuracy measured by\nMaxFrag is aﬁrst attempt to better handle those data ambiguities\nduring the validation process and we highly encourage other users\nto use it. However, in order to enable a comparison with the\nprevious studies we also reported traditional Top-n scores.\nAnalysis of prediction accuracy for different classes. The ori-\nginal dataset USPTO-50 k set12 provides a reaction type label for\nevery reaction. In total, ten reaction classes ranging from pro-\ntection/deprotection, to carbon –carbon bond and heterocycle\nformation present the most common reactions in organic\nsynthesis. The comparison of accuracy for each class of reactions\nwas presented in Fig.6. Our best model showed excellent results,\noutperforming the state-of-the-art Self-Corrected Transformer19.\nFunctional group interconversion and addition, as well as\ncarbon–carbon bond formation were the most difﬁcult for the\nmodels to predict. It was not surprising, due to the diverse pos-\nsibilities for choosing reactions and corresponding reactants for\nC–C bond creation compared to more straightforward oxidation\nor protection where the set of groups and reactants is more\nnarrow.\nPrediction of direct reactions. The same strategy described in\nthis work was applied to predicting direct reactions from the\nUSPTO-MIT dataset22. We used 439 k reactions (training and\nvalidation set were joined together) as the training set and pre-\ndicted 40 k reactions from the test set by training the transformer\nwith the same architecture and parameters. The separated and\nmixed sets were used. In the separated set reactants and reagents\nwere separated with the“>” sign while in mixed set all“>” are\nsubstituted with “.” and the order of reactants and reagents was\nadditionally shuf ﬂed. The mixed set was more dif ﬁcult for\ntraining since the transformer had to identify the reaction center\nfrom a larger number of molecules. However, such a set better\nreﬂected a practical application since separation of data on\nreactants and reagents in some cases would not be possible\nwithout a knowledge of the target product, and thus it did provide\na hint to the transformer about the reaction center. We have\nremoved 316 reactions from the training set where the largest\nproducts had length smaller than ﬁve characters (no reactions\nwere removed from the test set). The Transformer was training\nusing the x5N augmentation protocol for the separated set as well\nas the x5S and x5M protocols for the mixed set. Since it would be\nimpractical to predict all reagents and reactants for the retro-\nsynthesis task, which was used to additionally augment data in\nthe x5M protocol, only the largest reactant was retained as a\ntarget for the reverse reactions. Augmented test sets were pre-\ndicted using beam size 10 (Table3). For the mixed test set the\norder of reactants and reagents was shufﬂed.\nAs in previous studies, separation of reagent and reactants with\n“>” symbols contributed to a model (x5N) with higher prediction\nscores than for models with mixed sets (x5S and x5M). The\nadditional augmentation of data using retrosynthesis reactions\n(x5M) did not improve the model. This could be due to the fact\nthat the data for direct reactions were much larger and already\ncontained sufﬁcient information to develop accurate models.\nN\nS N\nNH\nNHNH\nH3C\nH3C\nN\nN\nS NS\nNH\nNH\nH3C\nH3C\nS\nNH\nNH2N\nH3C\nN\nNHH3C\nOH\nN\nNHH3C\nO\nO\nCH3\nH3C\nCl\nO O\nO CH3\nN\nFig. 5 Classical representation of the retrosynthesis of cimetidine focusing on the principal transformations, as is typically written by synthetic\nchemists (adapted fromhttps://de.wikipedia.org/wiki/Cimetidin under CC BY-SA 3.0 license).The currently used top-n accuracy measures also\ninclude prediction of other reactants12,13,19,20.24,32, which may not be necessary for classical retrosynthesis planning.\nNATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-020-19266-y ARTICLE\nNATURE COMMUNICATIONS|         (2020) 11:5575 | https://doi.org/10.1038/s41467-020-19266-y | www.nature.com/naturecommunications 7\nWhile using the x100 test set still contributed better prediction\naccuracy than using x20, the improvements were in the order of\n0.1% or no improvement at all. Thus the effect of using larger\naugmentations on model performance reached saturation for the\nx100 test set.\nComparison with published models for direct synthesis using\nUSPTO-MIT set. The USPTO-MIT was used as benchmarking\nfor direct synthesis predictions in multiple articles. The AT\nprovided the highest gain in performance for prediction of the\nmore challenging mixed dataset (Table4). Since the model was\ntrained with randomly shufﬂed augmented data, it was able to\ngeneralize very well and provide excellent predictions for the new\nmixed data. In order to provide a more adequate comparison with\nprevious studies we also developed a model based on exactly the\nsame training data of 400 k. Interestingly, the use of a smaller\ndataset slightly increased Top-1 performance to 90.6% but\ndecreased Top-5 performance to 96.1. It should be noted that\nimprovements for direct synthesis look small, i.e., just few per-\ncentages. Indeed, the model performance for the direct synthesis\nincreased from 88.6 to 90.6 (Top-1) and 96.1 from 94.2 (Top-5)\nas compared to the single model reported in ref.18. Actually, this\nis a signiﬁcant increase in performance since AT decreased the\nrelative errors by 15% and 30% for both sets, respectively, if we\nconsider that we can predict direct synthesis with 100%. In reality\nwe approach the experimental accuracy and further decrease of\nthe errors will be challenging.\nComparison with published models for retrosynthesis tasks\nUSPTO-50 k. The proposed augmentation protocol achieved the\nbest published results on the USPTO-50 k dataset (Table5). In\nthe previous studies with this set the authors separated data on\ntraining, validation and test sets. In all our analyses, since the\nvalidation set was not used for model selection and we did not\nobserve the model overﬁtting35, we joined training and validation\nsets to use all data in order to develop better models. While we\nthink this is a fair comparison (it is up to the developers of the\nmodel to decide on how to best use the available data), we also\nadded results when the model was developed with only the 40 k\ncompounds for USPTO-50 k set (Table5). The accuracies of the\nmodels developed with 40 and 45 k sets were very similar for the\ntest set. Thus, the data augmentation allowed to compensate for\nthe smaller size of the training set.\nUSPTO-MIT. We also analyzed the performance of the model at\nretrosynthesis of the USPTO-MIT set. Compared to USPTO-50 k\nthis database also contained multiple reagents and possible cata-\nlysts. In our previous analysis (Table3) we used the retrosynthesis\nThis work\n100\n90\n80\n70\n60\n50\n40\n30\n20\n10\nSCORP\n10 functional group addition (FGA)\n(227)\n9 functional group interconversion\nFGI (1834)\n8 oxidations\n(814)\n7 reductions\n(4585)\n6 deprotections\n(8353)\n5 protections\n(650)\n1 heteroatom alkylation and arylation\n(15,122)\n2 acylation and related processes\n(11,913)\n3 C–C bond formation\n(5639)\n4 heterocycle formation\n(900)\nFig. 6 Top-10 accuracy of prediction of different classes of reactions.\nTable 3 Prediction accuracy for direct reaction from USPTO-MIT test set using beam size= 10.\nTest set x1 Test set x20 Test set x100\nTraining set Top-1 Top-5 Top-10 Top-1 Top-5 Top-10 Top-1 Top-5 Top-10\nx5N (separated) 91.1 96.3 96.7 91.8 96.9 97.3 91.9 97 97.4\nx5S (mixed) 90 95.8 96.2 90.4 96.4 96.9 90.4 96.5 97\nx5M (mixed) 90 95.5 95.7 90.2 96.1 96.5 90.2 96.2 96.8\nARTICLE NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-020-19266-y\n8 NATURE COMMUNICATIONS|         (2020) 11:5575 | https://doi.org/10.1038/s41467-020-19266-y | www.nature.com/naturecommunications\nof the largest fragment as part of the“mix” protocol (x5M), i.e.,\nthe products were used as input data contained with“.” to predict\nthe largest reactant (as explained in the Supplementary materials,\nin order to distinguish both direct and retrosynthesis reactions,\none of them started with a dot). The dot in front of the SMILES\nallowed the Transformer to distinguish retrosynthesis from the\nprimary studied direct synthesis reaction. But, of course, the\nmodel trained with such data could be also used for retrosynthesis,\nprovided that input data also started with“.”. We also developed a\nnew retrosynthesis model for this set by making it more compa-\ntible to USPTO-50 k. For this we kept only the 1–2 largest frag-\nments as the targets for retrosynthesis prediction and trained a\nnew model using the x5S protocol. Both models were used to\npredict the 40 k test set which was augmented 100 times. The\nMaxFrag performances of x5S model, 61.9% (Top-1), 84.4% (Top-\n5), and 86.9% (Top-10) were very similar to those calculated for\nthe USPTO-50 k set (58.5, 85.4, and 90— see Table 5). The x5M\nmodel, which as aforementioned was a“by-product” of our direct\nreaction predictions, calculated MaxFrag of 61.1%, 84.4% and\n88.2% for the MaxFrag Top1-,Top-5, and Top-10, respectively.\nConsidering that the USPTO-MIT set contained more diverse\nreactions than USPTO-50 k, this result clearly demonstrated the\nexcellent performance of the developed approach and its scal-\nability. The augmented transformer (AT) was able to improve its\nperformance for the Top-1 by extracting knowledge from a much\nlarger dataset of reactions.\nUSPTO-full.T h eﬁnal testing was done using a USPTO-full set by\nDai et al.\n24. The authors created a large dataset from the entire\nset of reactions from USPTO 1976-2016. For reactions with\nmultiple products they duplicated them into multiple ones with\none product each. The authors also removed duplications in\nreactions as well as those with wrong mapping to obtain train/\nvalid/test datasets with 800/100/100 k sizes. Our analysis\nidentiﬁed that some reactions in these sets were still invalid,\ne.g., contained no products or just single ions as reactants (e.g.,\nUS08163899B2,>>[OH2:11]; US06048982,CC( =O)OCCCCC\n[I:22]>>[I-:22]; US07425593B2,>>[K:12]; US08114877B2,CC\n[I:13]>>[I-:13]). We eliminated such reactions as well as those\nwhere reactants had less thanﬁve atoms in total, since these were\nunlikely to be correct reactions. This procedure decreased sizes of\nthe train/valid/test sets on average by 4% to 769/96/96 k. The AT\ntrained using x5M protocol using the 769 k training set calcu-\nlated the higher performance compared to results from the\nprevious study (Table 6). Considering that after the removal of\nthe 4% erroneous reactions the test dataset was decreased, we\nalso included recalculated performance for it by assuming the\nworst case scenario: that AT and other tested methods failed for\nall excluded sequences. Even for this very conservative estima-\ntion the AT provided signi ﬁcant improvements compared to\npreviously reported results. The MaxFrag accuracies for USPTO-\nfull were lower compared to that of other analyzed sets due to the\nmuch higher diversity of this set.\nTable 4 Comparison of recently published methods for direct synthesis prediction on the USPTO-MIT set.\nModel Top-1 Top-2 Top-5\nSeparated Mixed Separated Mixed Separated Mixed Ref. #\nTransformer (single model) 90.4 88.6 93.7 92.4 95.3 94.2 18\nTransformer (ensemble of models) 91 94.3 95.8 18\nSeq2Seq 80.3 87.5 11\nWLDN 79.6 89.2 32\nGTPN 83.2 86.5 40\nWLDN5 85.6 93.4 23\nAT, this worka 91.9 90.4 95.4 94.6 97 96.5\nAT trained with same training set as in\nref. 22.\n92 90.6 95.4 94.4 97 96.1\naThe results of the models applied to x100 augmented dataset using beam size= 10. Model was trained on a set of 439 k reactions, which combines both the training set of 400 k and the validation set of\n39 k from ref.22. The model was trained on the 400 k training set to better match performance of previous models.\nTable 5 Comparison of retrosynthesis recently published methods for retrosynthesis prediction on USPTO-50 k.\nModel Top-1 Top-2 Top-5 Top-10 Ref. # Comments\nSeq2Seq 37.4 57.0 61.7 12 40/5/5 split; splitting any reactions with multiple products into multiple\nsingle product and removal of trivial products\nTransformer (3*6) 42.7 52.5 69.8 – 13 45/5 split: no validation set was used\nTransformer (6*8),\n(self corrected)\n43.7 65.2 68.7 19 40/5/5 split, reagents from reactants are removed\nTransformer, augmentation 44.8 57.1 57.7 79.4 32 Same as in ref.12.\nSimilarity-based 37.3 63.3 74.1 20 Same as in ref.12.\nGraph Logic Network 52.5 75.6 83.7 24 Same as in refs.12,19.\nG2Gs 48.9 72.5 75.5 25 Same as in ref.12.\nATa 53.5 69.4 81 85.7 Same as in ref. 13.\nAT 53.2 68.1 80.5 85.2 Only 40 k samples were used as training set to match the other results\nAT MaxFragb 58.5 73 85.4 90 Same as in ref. 13.\nAT MaxFrag 58 73.4 84.8 89.1 Only 40 k samples were used as training set to match the other results\naThe results of the reference model applied to x100 augmented dataset using beam size= 10.\nbThe classical retro-synthesis accuracy was estimated as accuracy for prediction of the largest fragment (MaxFrag).\nNATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-020-19266-y ARTICLE\nNATURE COMMUNICATIONS|         (2020) 11:5575 | https://doi.org/10.1038/s41467-020-19266-y | www.nature.com/naturecommunications 9\nThus for all analyzed data sets the AT provided an outstanding\nperformance by consistently and signiﬁcantly overperforming all\npreviously published models for all statistical performances.\nDiscussion\nThis study showed that careful design of the training set was of\nparamount importance for the performance of the Transformer.\nTraining the model to learn different representations of the same\nreaction by distorting the initial canonical data eliminated the\neffect of memorization and increased the generalization perfor-\nmance of models. These ideas are intensively used, e.g., for image\nrecognition\n39, and have been already successfully used in the\ncontext of several chemical problems 27–30, including reaction\npredictions18,31, but were limited to the input data. For theﬁrst\ntime we showed that application of augmentation to the target\ndata signiﬁcantly boosts the quality of the reaction prediction. We\nalso showed for the ﬁrst time that the frequency of predicted\nSMILES could be used as a conﬁdence metric for (retro)synthesis\nprediction and can provide quantitative estimation of the most\nprobable reactions amid top-n predicted outcomes. It is very\ncritical to estimate the quality of the reaction prediction since it\ncould help to better prioritize multi-step retrosynthesis. The\ndeveloped methodology is unique to the use of augmentation\ntechniques, currently unavailable to GCNs 24, which directly\noperates with graphs. The estimated accuracy of prediction can\nhelp to distinguish reactions, which are difﬁcult to predict, from\ntypo and erroneous reaction data, which will be important to\nclean up the reaction data and further improve model quality. We\nalso introduced a new MaxFrag measure, classical retro-synthesis\naccuracy, which in our opinion better reﬂects the requirements\nfor retrosynthesis analysis.\nIt should be mentioned that use augmentation wasﬁrst studied\nby authors of ref.18, who introduced transformer to chemistry\nand applied it to chemical reactions by using SMILES instead of\nthe text sequences. The augmentation of input data, which was\ndone in that article, provided only a minor improvement of their\nmodels. Because of its small impact it was not followed in several\nother Transformer-based works, including our own study13,19.I n\nthis article we brought an original idea on how to augment\nchemical data, which provided a signiﬁcant improvement of the\nresults for all analyzed datasets.\nSMILES random augmentation had the ability to stabilize the\nmodel’s learning by adding more data and adding more ran-\ndomness and freedom into the network. Remarkably, this aug-\nmentation functioned similarly to ensemble learning, allowing for\nbetter statistics and improving the performance of the model.\nBeam search and augmentation were complementary and our\nreference model in essence got better results than models devel-\noped using graph representation of molecules24, for which the use\nof similar data augmentation technique is currently not possible.\nData availability\nData and predictions that support the results of this study are available athttps://github.\ncom/bigchem/synthesis.\nCode availability\nSource code to perform the data augmentation is available athttps://github.com/\nbigchem/synthesis.\nReceived: 19 March 2020; Accepted: 5 October 2020;\nReferences\n1. Corey, E. J. & Cheng, X.-M. The Logic of Chemical Synthesis. (John Wiley &\nSons, New York, 1995).\n2. Corey, E. J., Long, A. K. & Rubenstein, S. D. Computer-assisted analysis in\norganic synthesis. Science 228, 408–418 (1985).\n3. Segler, M. H. S. & Waller, M. P. Neural-symbolic machine learning for\nretrosynthesis and reaction prediction.Chemistry 23, 5966–5971 (2017).\n4. Coley, C. W., Barzilay, R., Jaakkola, T. S., Green, W. H. & Jensen, K. F.\nPrediction of organic reaction outcomes using machine learning.ACS Cent.\nSci. 3, 434–443 (2017).\n5. Segler, M. H. S., Preuss, M. & Waller, M. P. Planning chemical syntheses with\ndeep neural networks and symbolic AI.Nature 555, 604–610 (2018).\n6. Baskin, I. I., Madzhidov, T. I., Antipin, I. S. & Varnek, A. A. Artiﬁcial\nintelligence in synthetic chemistry: achievements and prospects.Russian\nChem. Rev. 86, 1127–1156 (2017).\n7. Struble, T. J. et al. Current and future roles of artiﬁcial intelligence in\nmedicinal chemistry synthesis.J. Med. Chem.63, 8667–8682 (2020).\n8. Muratov, E. N. et al. QSAR without borders.Chem. Soc. Rev.49, 3525–3564\n(2020).\n9. Szymku ć, S. et al. Computer-assisted synthetic planning: the end of the\nbeginning. Angew. Chem. Int. Ed. Engl.55, 5904–5937 (2016).\n10. Law, J. et al. Route Designer: a retrosynthetic analysis tool utilizing automated\nretrosynthetic rule generation.J. Chem. Inf. Model.49, 593–602 (2009).\n11. Schwaller, P., Gaudin, T., Lanyi, D., Bekas, C. & Laino, T.“Found in\nTranslation”: predicting outcomes of complex organic chemistry reactions\nusing neural sequence-to-sequence models.Chem. Sci. 9, 6091–6098 (2018).\n12. Liu, B. et al. Retrosynthetic reaction prediction using neural sequence-to-\nsequence models. ACS Cent. Sci.3, 1103–1113 (2017).\n13. Karpov, P., Godin, G. & Tetko, I. V. InA Transformer Model for\nRetrosynthesis, Artiﬁcial Neural Networks and Machine Learning— ICANN\n2019: Workshop and Special Sessions, Münich, 17–19th September 2019. (eds\nTetko, I. V., Kůrková, V., Karpov, P. & Theis, F.). 817–830 (Springer\nInternational Publishing, Münich, 2019).\n14. Weininger, D. Smiles, a chemical language and information-system.1.\nIntroduction to methodology and encoding rules.J. Chem. Inf. Comput. Sci.\n28,3 1–36 (1988).\n15. Nam, J. & Kim, J. Linking the neural machine translation and the prediction\nof organic chemistry reactions. Preprint athttps://arxiv.org/abs/1612.09529\n(2016).\n16. Sutskever, I., Vinyals, O. & Le, Q. V. InSequence to Sequence Learning with\nNeural Networks, Advances in Neural Information Processing Systemsvol 27.\n(eds Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N. D. & Weinberger,\nK. Q.). 3104–3112 (Curran Associates, Inc., 2014).\n17. Vaswani, A. et al. Attention Is All You Need. Preprint athttps://arxiv.org/abs/\n1706.03762 (2017).\n18. Schwaller, P. et al. Molecular transformer: a model for uncertainty-calibrated\nchemical reaction prediction.ACS Cent. Sci.5, 1572–1583 (2019).\n19. Zheng, S., Rao, J., Zhang, Z., Xu, J. & Yang, Y. Predicting retrosynthetic\nreactions using self-corrected transformer neural networks.J. Chem. Inf.\nModel. 60,4 7–55 (2020).\n20. Coley, C. W., Rogers, L., Green, W. H. & Jensen, K. F. Computer-assisted\nretrosynthesis based on molecular similarity.ACS Cent. Sci.3, 1237–1245\n(2017).\n21. Ishida, S., Terayama, K., Kojima, R., Takasu, K. & Okuno, Y. Prediction and\ninterpretable visualization of retrosynthetic reactions using graph\nconvolutional networks. J. Chem. Inf. Model.59, 5026–5033 (2019).\nTable 6 Top-k accuracy for retrosynthesis prediction on\nUSPTO-full dataset.\nRetrosim20 Neuralsym3 GLN24 ATa\nTop-1 32.8 35.8 39.3 46.2 (44.4) b\nTop-2 57.2 (54.9)\nTop-10 56.1 60.8 63.7 73.3 (70.4)\nMaxFrag\ntop-1\n54\nMaxFrag\ntop-2\n66.3\nMaxFrag\ntop-5\n77.3\nMaxFrag\ntop-10\n80.6\naModel was trained using aﬁltered training set of 769 k from ref.20.\nbAccuracies in parentheses correspond to those recalculated for the test set by assuming that\nAT failed for all 4% of excluded reactions. Results for retrosim and neuralsym approaches as\nreported by Dai et al.\n24.\nARTICLE NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-020-19266-y\n10 NATURE COMMUNICATIONS|         (2020) 11:5575 | https://doi.org/10.1038/s41467-020-19266-y | www.nature.com/naturecommunications\n22. Jin, W., Coley, C., Barzilay, R. & Jaakkola, T. InPredicting Organic Reaction\nOutcomes with Weisfeiler-Lehman Network, 31st Conference on Neural\nInformation Processing Systems (NIPS 2017), Long Beach, CA, USA. (eds\nGuyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan,\nS. & Garnett, R.). 2607–2616 (Long Beach, CA, USA, 2017).\n23. Coley, C. W. et al. A graph-convolutional neural network model for the\nprediction of chemical reactivity.Chem. Sci. 10, 370–377 (2019).\n24. Dai, H., Li, C., Coley, C., Dai, B. & Song, L. InRetrosynthesis Prediction with\nConditional Graph Logic Network, Advances in Neural Information Processing\nSystems vol 32. (eds Wallach, H. et al.). 8872–8882 (Curran Associates, Inc.,\n2019).\n25. Shi, C., Xu, M., Guo, H., Zhang, M. & Tang, J. A graph to graphs framework\nfor retrosynthesis prediction. Preprint at https://arxiv.org/abs/2003.12725\n(2020).\n26. Weininger, D., Weininger, A. & Weininger, J. L. Smiles.2. Algorithm for\ngeneration of unique smiles notation.J. Chem. Inf. Comput. Sci.29,9 7–101\n(1989).\n27. Tetko, I. V., Karpov, P., Bruno, E., Kimber, T. B. & Godin, G. In\nAugmentation Is What You Need!, Artiﬁcial Neural Networks and Machine\nLearning— ICANN 2019: Workshop and Special Sessions, Münich, 17–19th\nSeptember 2019. (eds Tetko, I. V., Kůrková, V., Karpov, P. & Theis, F.).\n831–835 (Springer International Publishing, Münich, 2019).\n28. Kimber, T. B., Engelke, S., Tetko, I. V., Bruno, E. & Godin, G. Synergy Effect\nbetween Convolutional Neural Networks and the multiplicity of SMILES for\nimprovement of molecular prediction. Preprint athttps://arxiv.org/abs/\n1812.04439 (2018).\n29. Bjerrum, J. E. SMILES enumeration as data augmentation for neural network\nmodeling of molecules. Preprinthttps://arxiv.org/abs/1703.07076 (2017).\n30. Karpov, P., Godin, G. & Tetko, I. V. Transformer-CNN: Swiss knife for QSAR\nmodeling and interpretation.J. Cheminform. 12, 17 (2020).\n31. Fortunato, M. E., Coley, C. W., Barnes, B. C. & Jensen, K. F. Data augmentation\nand pretraining for template-based retrosynthetic prediction in computer-aided\nsynthesis planning.J. Chem. Inf. Model.60, 3398–3407 (2020).\n32. Chen, B., Shen, T., Jaakkola, T. S. & Barzilay, R. Learning to make\ngeneralizable and diverse predictions for retrosynthesis. Preprint athttps://\narxiv.org/abs/1910.09688 (2019).\n33. Lin, K., Xu, Y., Pei, J. & Lai, L. Automatic retrosynthetic route planning using\ntemplate-free models. Chem. Sci. 11, 3355–3364 (2020).\n34. Schwaller, P. et al. Predicting retrosynthetic pathways using transformer-based\nmodels and a hyper-graph exploration strategy.Chem. Sci.11, 3316–3325\n(2020).\n35. Tetko, I. V., Livingstone, D. J. & Luik, A. I. Neural network studies. 1.\nComparison of overﬁtting and overtraining.J. Chem. Inf. Comput. Sci.35,\n826–833 (1995).\n36. Lowe, D. M.\nExtraction of Chemical Structures and Reactions from the\nLiterature (University of Cambridge, 2012).\n37. Satoh, H. & Funatsu, K. SOPHIA, a knowledge base-guided reaction\nprediction system— utilization of a knowledge base derived from a reaction\ndatabase. J. Chem. Inf. Comput. Sci.35,3 4–44 (1995).\n38. Larock, R. C Comprehensive Organic Transformations: A Guide to Functional\nGroup Preparations (John Wiley & Sons: Hoboken, NJ, 1999).\n39. Shorten, C. & Khoshgoftaar, T. M. A survey on image data augmentation for\ndeep learning. J. Big Data6, 60 (2019).\n40. Do, K., Tran, T. & Venkatesh, S. Graph transformation policy network\nfor chemical reaction prediction. InProceedings of the 25th ACM SIGKDD\nInternational Conference on Knowledge Discovery & Data Mining,\nAssociation for Computing Machinery: Anchorage, AK, USA, 750–760\n(2019).\nAcknowledgements\nThis study was partially funded by the European Union’s Horizon 2020 research and\ninnovation program under the Marie Skłodowska-Curie Innovative Training Network Eur-\nopean Industrial Doctorate grant agreement No. 676434,“Big Data in Chemistry” and ERA-\nCVD “CardioOncology” project, BMBF 01KL1710 as well as by a grant from Intel. The article\nreﬂects only the author’s view and neither the European Commission nor the Research\nE x e c u t i v eA g e n c y( R E A )a r er e s p o n s i b l ef o ra n yu s et h a tm a yb em a d eo ft h ei n f o r m a t i o ni t\ncontains. The authors thank NVIDIA Corporation for donating Quadro P6000, Titan Xp,\na n dT i t a nVg r a p h i c sc a r d sf o rt h i sr e s e a r ch work. The authors thank Michael Withnall\n(Apheris AI) and Alli Michelle Keys (Stanford University) for their comments and English\ncorrections as well as Marios Theodoropoulos(University of Geneva) for interesting dis-\ncussions. We also would like to thank the anonymous reviewers for their insightful and\nsometimes even provocative comments answering of which signiﬁcantly increased the value\nof this study.\nAuthor contributions\nG.G., R.v.D., and I.V.T. initially designed the study and proposed several different\naugmentation protocols and started initial analysis. I.V.T. and P.K. performed the\nadvanced analysis. All authors contributed equally to the paper. All authors read and\napproved the ﬁnal paper.\nFunding\nOpen Access funding enabled and organized by Projekt DEAL.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary informationis available for this paper athttps://doi.org/10.1038/s41467-\n020-19266-y.\nCorrespondence and requests for materials should be addressed to I.V.T. or G.G.\nPeer review informationNature Communications thanks Pierre Baldi and the other,\nanonymous, reviewers for their contribution to the peer review of this work.\nReprints and permission informationis available athttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to jurisdictional claims in\npublished maps and institutional afﬁliations.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as long as you give\nappropriate credit to the original author(s) and the source, provide a link to the Creative\nCommons license, and indicate if changes were made. The images or other third party\nmaterial in this article are included in the article’s Creative Commons license, unless\nindicated otherwise in a credit line to the material. If material is not included in the\narticle’s Creative Commons license and your intended use is not permitted by statutory\nregulation or exceeds the permitted use, you will need to obtain permission directly from\nthe copyright holder. To view a copy of this license, visithttp://creativecommons.org/\nlicenses/by/4.0/.\n© The Author(s) 2020\nNATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-020-19266-y ARTICLE\nNATURE COMMUNICATIONS|         (2020) 11:5575 | https://doi.org/10.1038/s41467-020-19266-y | www.nature.com/naturecommunications 11"
}