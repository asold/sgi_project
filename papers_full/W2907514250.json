{
  "title": "Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation",
  "url": "https://openalex.org/W2907514250",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4282320121",
      "name": "Garbacea, Cristina",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4224947293",
      "name": "Carton, Samuel",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2003264471",
      "name": "Yan Shiyan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2748366560",
      "name": "Mei Qiaozhu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W648786980",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2593383075",
    "https://openalex.org/W1810943226",
    "https://openalex.org/W2155027007",
    "https://openalex.org/W2605035112",
    "https://openalex.org/W1525783482",
    "https://openalex.org/W2557508245",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2963527228",
    "https://openalex.org/W1999965501",
    "https://openalex.org/W2107878631",
    "https://openalex.org/W2962879001",
    "https://openalex.org/W2962968835",
    "https://openalex.org/W2122261166",
    "https://openalex.org/W2123301721",
    "https://openalex.org/W2547875792",
    "https://openalex.org/W2729046720",
    "https://openalex.org/W2963248348",
    "https://openalex.org/W2078483536",
    "https://openalex.org/W2250234233",
    "https://openalex.org/W2150290224",
    "https://openalex.org/W1956340063",
    "https://openalex.org/W1724972948",
    "https://openalex.org/W2616969219",
    "https://openalex.org/W1590378318",
    "https://openalex.org/W2603766943",
    "https://openalex.org/W2596458094",
    "https://openalex.org/W2328886022",
    "https://openalex.org/W2950040358",
    "https://openalex.org/W2900260828",
    "https://openalex.org/W2605287558",
    "https://openalex.org/W2577946330",
    "https://openalex.org/W2524985544",
    "https://openalex.org/W2963142510",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2964268978",
    "https://openalex.org/W2581637843",
    "https://openalex.org/W2951714314",
    "https://openalex.org/W2018758420",
    "https://openalex.org/W2126316555",
    "https://openalex.org/W2941169998",
    "https://openalex.org/W2963857374",
    "https://openalex.org/W2296073425",
    "https://openalex.org/W2792795990",
    "https://openalex.org/W2963373786",
    "https://openalex.org/W2807747378",
    "https://openalex.org/W2950178297",
    "https://openalex.org/W1993509040",
    "https://openalex.org/W2169279899",
    "https://openalex.org/W2171928131",
    "https://openalex.org/W2786472963",
    "https://openalex.org/W2099471712",
    "https://openalex.org/W2174424190",
    "https://openalex.org/W2740167620",
    "https://openalex.org/W2963456134",
    "https://openalex.org/W1489525520",
    "https://openalex.org/W590442793",
    "https://openalex.org/W1982897610",
    "https://openalex.org/W2951004968",
    "https://openalex.org/W2963730239",
    "https://openalex.org/W1514535095",
    "https://openalex.org/W3102476541",
    "https://openalex.org/W2963018920",
    "https://openalex.org/W2799184518",
    "https://openalex.org/W2104626092",
    "https://openalex.org/W2620623908",
    "https://openalex.org/W2028602070",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2161283199",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2964017345",
    "https://openalex.org/W2925962955",
    "https://openalex.org/W179179905",
    "https://openalex.org/W2060833990",
    "https://openalex.org/W2883158411",
    "https://openalex.org/W2152921782",
    "https://openalex.org/W2963223306",
    "https://openalex.org/W2603089249",
    "https://openalex.org/W2962788902",
    "https://openalex.org/W2138857742",
    "https://openalex.org/W2951413914",
    "https://openalex.org/W2542835211",
    "https://openalex.org/W2553897675",
    "https://openalex.org/W2016589492",
    "https://openalex.org/W2259472270",
    "https://openalex.org/W2898718449",
    "https://openalex.org/W2784823820",
    "https://openalex.org/W2519536754",
    "https://openalex.org/W2583679610",
    "https://openalex.org/W2058373514",
    "https://openalex.org/W2100059775",
    "https://openalex.org/W2949541494",
    "https://openalex.org/W273955616"
  ],
  "abstract": "We conduct a large-scale, systematic study to evaluate the existing evaluation methods for natural language generation in the context of generating online product reviews. We compare human-based evaluators with a variety of automated evaluation procedures, including discriminative evaluators that measure how well machine-generated text can be distinguished from human-written text, as well as word overlap metrics that assess how similar the generated text compares to human-written references. We determine to what extent these different evaluators agree on the ranking of a dozen of state-of-the-art generators for online product reviews. We find that human evaluators do not correlate well with discriminative evaluators, leaving a bigger question of whether adversarial accuracy is the correct objective for natural language generation. In general, distinguishing machine-generated text is challenging even for human evaluators, and human decisions correlate better with lexical overlaps. We find lexical diversity an intriguing metric that is indicative of the assessments of different evaluators. A post-experiment survey of participants provides insights into how to evaluate and improve the quality of natural language generation systems.",
  "full_text": "Judge the Judges: A Large-Scale Evaluation Study of Neural\nLanguage Models for Online Review Generation\nCristina Gˆarbacea1, Samuel Carton2, Shiyan Yan2, Qiaozhu Mei1,2\n1Department of EECS, University of Michigan, Ann Arbor, MI, USA\n2School of Information, University of Michigan, Ann Arbor, MI, USA\n{garbacea, scarton, shiyansi, qmei}@umich.edu\nAbstract\nWe conduct a large-scale, systematic study\nto evaluate the existing evaluation methods\nfor natural language generation in the con-\ntext of generating online product reviews. We\ncompare human-based evaluators with a va-\nriety of automated evaluation procedures, in-\ncluding discriminative evaluators that measure\nhow well machine-generated text can be dis-\ntinguished from human-written text, as well as\nword overlap metrics that assess how similar\nthe generated text compares to human-written\nreferences. We determine to what extent these\ndifferent evaluators agree on the ranking of a\ndozen of state-of-the-art generators for online\nproduct reviews. We ﬁnd that human eval-\nuators do not correlate well with discrimina-\ntive evaluators, leaving a bigger question of\nwhether adversarial accuracy is the correct ob-\njective for natural language generation. In gen-\neral, distinguishing machine-generated text is\nchallenging even for human evaluators, and\nhuman decisions correlate better with lexical\noverlaps. We ﬁnd lexical diversity an intrigu-\ning metric that is indicative of the assessments\nof different evaluators. A post-experiment sur-\nvey of participants provides insights into how\nto evaluate and improve the quality of natural\nlanguage generation systems 1.\n1 Introduction\nRecent developments in neural language models\n(Mikolov and Zweig, 2012), (Reiter and Belz,\n2009), (Mikolov et al., 2011b), (Mikolov et al.,\n2011a) have inspired the use of neural network\nbased architectures for the task of natural language\ngeneration (NLG). Despite fast development of al-\ngorithms, there is an urgency to ﬁll the huge gap in\nevaluating NLG systems. On one hand, a rigorous,\n1 The experimental setup, data, and annotations are pub-\nlicly available at: https://github.com/Crista23/\nJudgeTheJudges\nefﬁcient, and reproducible evaluation procedure is\ncritical for the development of any machine learn-\ning technology and for correct interpretation of the\nstate-of-the-art. On the other hand, evaluating the\nquality of language generation is inherently dif-\nﬁcult due to the special properties of text, such\nas subjectivity and non-compositionality. Indeed,\n“there is no agreed objective criterion for comparing\nthe goodness of texts” (Dale and Mellish, 1998),\nand there lacks a clear model of text quality (Hard-\ncastle and Scott, 2008).\nConventionally, most NLG systems have been\nevaluated in a rather informal manner. (Reiter\nand Belz, 2009) divide existing evaluation meth-\nods commonly employed in text generation into\nthree categories: i) evaluations based on task per-\nformance, ii) human judgments and ratings, where\nhuman subjects are recruited to rate different di-\nmensions of the generated texts, and iii) evalua-\ntions based on comparison to a reference corpus\nusing automated metrics. Task based evaluation\nconsiders that the value of a piece of functional text\nlies in how well it serves the user to fulﬁll a speciﬁc\napplication. It can be expensive, time-consuming,\nand often dependent on the good will of partici-\npants in the study. Besides that, it is hard to toss\nout the general quality of text generation from the\nspecial context (and confounds) of the task, or to\ngeneralize the evaluation conclusions across tasks.\nHuman annotation is able to assess the quality of\ntext more directly than task based evaluation. How-\never, rigorously evaluating NLG systems with real\nusers can be expensive and time consuming, and it\ndoes not scale well (Reiter et al., 2001). Human as-\nsessments also need to be consistent and repeatable\nfor a meaningful evaluation (Lopez, 2012). Alter-\nnative strategies which are more effective in terms\nof cost and time are used more frequently.\nAutomated evaluation compares texts generated\nby the candidate algorithms to human-written texts.\narXiv:1901.00398v2  [cs.CL]  5 Sep 2019\nWord overlap metrics and more recent automated\nadversarial evaluators are widely employed in NLG\nas they are cheap, quick, repeatable, and do not re-\nquire human subjects when a reference corpus is\nalready available. In addition, they allow devel-\nopers to make rapid changes to their systems and\nautomatically tune parameters without human in-\ntervention. Despite the beneﬁts, however, the use\nof automated metrics in the ﬁeld of NLG is con-\ntroversial (Reiter and Belz, 2009), and their results\nare often criticized as not meaningful for a number\nof reasons. First, these automatic evaluations rely\non a high-quality corpus of references, which is\nnot often available. Second, comparisons with a\nreference corpus do not assess the usefulness and\nthe impact of the generated text on the readers as in\nhuman-based evaluations. Third, creating human\nwritten reference texts speciﬁcally for the purpose\nof evaluation could still be expensive, especially if\nthese reference texts need to be created by skilled\ndomain experts. Finally and most importantly, us-\ning automated evaluation metrics is sensible only\nif they correlate with results of human-based eval-\nuations and if they are accurate predictors of text\nquality, which is never formally veriﬁed at scale.\nWe present a large-scale, systematic experiment\nthat evaluates the evaluators for NLG. We compare\nthree types of evaluators including human evalu-\nators, automated adversarial evaluators trained to\ndistinguish human-written from machine-generated\nproduct reviews, and word overlap metrics (such as\nBLEU and ROUGE) in a particular scenario, gen-\nerating online product reviews. The preferences of\ndifferent evaluators on a dozen representative deep-\nlearning based NLG algorithms are compared with\nhuman assessments of the quality of the generated\nreviews. Our ﬁndings reveal signiﬁcant differences\namong the evaluators and shed light on the poten-\ntial factors that contribute to these differences. The\nanalysis of a post experiment survey also provides\nimportant implications on how to guide the devel-\nopment of new NLG algorithms.\n2 Related Work\n2.1 Deep Learning Based NLG\nRecently, a decent number of deep learning\nbased models have been proposed for text gener-\nation. Recurrent Neural Networks (RNNs) and\ntheir variants, such as Long Short Term Memory\n(LSTM) (Hochreiter and Schmidhuber, 1997) mod-\nels, Google LM (Jozefowicz et al., 2016), and\nScheduled Sampling (SS) (Bengio et al., 2015) are\nwidely used for generating textual data.\nGenerative Adversarial Networks (Goodfellow\net al., 2014), or GANs, train generative models\nthrough an adversarial process. Generating text\nwith GANs is challenging due to the discrete nature\nof text data. SeqGAN (Yu et al., 2017) is one of the\nearliest GAN-based model for sequence generation,\nwhich treats the procedure as a sequential decision\nmaking process. RankGAN (Lin et al., 2017) pro-\nposes a framework that addresses the quality of\na set of generated sequences collectively. Many\nGAN-based models (Yu et al., 2017), (Lin et al.,\n2017), (Rajeswar et al., 2017), (Che et al., 2017),\n(Li et al., 2017), (Zhang et al., 2017) are only capa-\nble of generating short texts. LeakGAN (Guo et al.,\n2018) is proposed for generating longer texts.\nDeep learning architectures other than LSTM or\nGAN have also been proposed for text generation.\n(Tang et al., 2016) study NLG given particular con-\ntexts or situations and proposes two approaches\nbased on the encoder-decoder framework. (Dong\net al., 2017) address the same task and employ an\nadditional soft attention mechanism. Pre-training\nenables better generalization in deep neural net-\nworks (Erhan et al., 2010), especially when com-\nbined with supervised discriminative ﬁne-tuning\nto learn universal robust representations (Radford\net al., 2018), (Devlin et al., 2018), (Radford et al.,\n2019). (Guu et al., 2018) use a prototype-then-edit\ngenerative language model for sentences.\n2.2 Automated Evaluation Metrics\nThe variety of NLG models are also evaluated with\nvarious approaches. Arguably, the most natural\nway to evaluate the quality of a generator is to\ninvolve humans as judges, either through some\ntype of Turing test (Machinery, 1950) to distinguish\ngenerated text from human input texts, or to directly\ncompare the texts generated by different generators\n(Mellish and Dale, 1998). Such approaches are\nhard to scale and have to be redesigned whenever a\nnew generator is included. Practically, it is critical\nto ﬁnd automated metrics to evaluate the quality\nof a generator independent of human judges or an\nexhaustive set of competing generators.\nPerplexity (Jelinek et al., 1977) is commonly\nused to evaluate the quality of a language model,\nwhich has also been employed to evaluate genera-\ntors (Yarats and Lewis, 2018), (Ficler and Goldberg,\n2017), (Gerz et al., 2018) even though it is com-\nmonly criticized for not being a direct measure of\nthe quality of generated text (Fedus et al., 2018).\nPerplexity is a model dependent metric, and “how\nlikely a sentence is generated by a given model” is\nnot comparable across different models. Therefore\nwe do not include perplexity in this study.\nDiscriminative Evaluation is an alternative\nway to evaluate a generator, which measures how\nlikely its generated text can fool a classiﬁer that\naims to distinguish the generated text from human-\nwritten texts. In a way, this is an automated approx-\nimation of the Turing test, where machine judges\nare used to replace human judges. Discriminative\nmachine judges can be trained either using a data\nset with explicit labels (Ott et al., 2011), or using\na mixture of text written by real humans and those\ngenerated by the model being evaluated. The lat-\nter is usually referred to as adversarial evaluation.\n(Bowman et al., 2016) proposes one of the earliest\nstudies that uses adversarial error to assess the qual-\nity of generated sentences. Notably, maximizing\nthe adversarial error is consistent to the objective\nof the generator in generative adversarial networks.\n(Kannan and Vinyals, 2017) propose an adversarial\nloss to discriminate a dialogue model’s output from\nhuman output. The discriminator prefers longer\noutput and rarer language instead of the common\nresponses generated. There however lacks evidence\nthat a model that obtains a lower adversarial loss is\nbetter according to human evaluations.\nAutomatic dialogue evaluation is formulated as a\nlearning problem in (Lowe et al., 2017), who train\nan RNN to predict the scores a human would assign\nto dialogue responses. RNN predictions correlate\nwith human judgments at the utterance and system\nlevel, however each response is evaluated in a very\nspeciﬁc context and the system requires substantial\nhuman judgments for training. (Li et al., 2017)\nemploy a discriminator (analogous to the human\nevaluator in the Turing test) both in training and\ntesting and deﬁne adversarial success. Other work\nﬁnds the performance of a discriminative agent\n(e.g., attention-based bidirectional LSTM binary\nclassiﬁer) is comparable with human judges at dis-\ntinguishing between real and fake dialogue excerpts\n(Bruni and Fern ´andez, 2017). However, results\nshow there is limited consensus among humans on\nwhat is considered as coherent dialogue passages.\nWord Overlap Metrics, such as BLEU (Pap-\nineni et al., 2002), ROUGE (Lin, 2004), and ME-\nTEOR (Banerjee and Lavie, 2005), are commonly\nused to measure the similarity between the gener-\nated text and human written references. (Liu et al.,\n2016) ﬁnd that word overlap metrics present weak\nor no correlation with human judgments in non-task\noriented dialogue systems and thus should be used\nwith caution or in combination with user studies.\nIn contrary, it is reported in (Sharma et al., 2017)\nthat text overlap metrics are indicative of human\njudgments in task-oriented dialogue settings, when\nused on datasets which contain multiple ground\ntruth references. (Dai et al., 2017) ﬁnd text overlap\nmetrics too restrictive as they focus on ﬁdelity of\nwording instead of ﬁdelity of semantics. (Callison-\nBurch et al., 2006) consider an increase in BLEU\ninsufﬁcient for an actual improvement in the quality\nof a system and posit in favor of human evaluation.\nBLEU and its variants (e.g., Self-BLEU) are\nused to evaluate GAN models (Caccia et al., 2018;\nZhu et al., 2018). (Shi et al., 2018) compare frame-\nworks for text generation including MLE, SeqGAN,\nLeakGAN and Inverse Reinforcement Learning us-\ning a simulated Turing test. A benchmarking exper-\niment with GAN models is conducted in (Lu et al.,\n2018); results show LeakGAN presents the highest\nBLEU scores on the test data. Similarly, BLEU and\nMETEOR present highest correlations with human\njudgements (Callison-Burch et al., 2008), (Graham\nand Baldwin, 2014). However, evaluation metrics\nare not robust across conditions, and no single met-\nric consistently outperforms other metrics across\nall correlation levels (Przybocki et al., 2009).\nConventional neural language models trained\nwith maximum likelihood can be on par or better\nthan GANs (Caccia et al., 2018), (Semeniuta et al.,\n2018), (Tevet et al., 2018). However, log-likelihood\nis often computationally intractable (Theis et al.,\n2016). Models with good likelihood can produce\nbad samples, and vice-versa (Goodfellow, 2016).\nGenerative models should be evaluated with re-\ngards to the task they are intended for over the\nfull quality-diversity spectrum (C´ıfka et al., 2018),\n(Hashimoto et al., 2019), (Montahaei et al., 2019).\nWhile many generators are proposed and eval-\nuated with various metrics, no existing work has\nsystematically evaluated the different evaluators at\nscale, especially in the context of online review\ngeneration. Our work ﬁlls in this gap.\n3 Experiment Design\nWe design a large-scale experiment to systemati-\ncally analyze the procedures and metrics used for\nevaluating NLG models. To test the differentevalu-\nators, the experiment carefully chooses a particular\napplication context and a variety of natural lan-\nguage generators in this context. Ideally, a sound\nautomated evaluator should be able to distinguish\ngood generators from suboptimal ones. Its prefer-\nences (on ordering the generators) should be con-\nsistent to humans in the exact application context.\n3.1 Experiment Context and Procedure\nWe design the experiment in the context of gen-\nerating online product reviews. There are several\nreasons why review generation is a desirable task\nfor the experiment: 1) online product reviews are\nwidely available, and it is easy to collect a large\nnumber of examples for training/ testing the gener-\nators; 2) Internet users are used to reading online\nreviews, and it is easy to recruit capable human\njudges to assess the quality of reviews; and 3) com-\nparing to tasks like image caption generation or\ndialogue systems, review generation has minimal\ndependency on the conversation context or on non-\ntextual data, which reduces possible confounds.\nFigure 1: Overview of the Experiment Procedure.\nThe general experiment procedure is presented\nin Figure 1. We start from the publicly available\nAmazon Product Reviews dataset 2 and select three\nmost popular domains: books, electronics, and\nmovies. After ﬁltering rare products, inactive users,\nand overly long reviews, the dataset is randomly\nsplit into three parts, to train, to validate, and to\n2http://jmcauley.ucsd.edu/data/amazon/\ntest the candidate review generators (denoted as G-\ntrain, G-valid, and G-test). Every generative model\nis trained and validated using the same datasets,\nand then charged to generate a number of product\nreviews (details are included in the next section).\nThese generated reviews, mixed with the real re-\nviews in G-test, are randomly split into three new\nsubsets for training, validating, and testing candi-\ndate (discriminative) evaluators, denoted asD-train,\nD-valid, and D-test. Finally, a random sample of\nreviews from D-test are sent for human evaluation.\n3.2 Review Generators\nAlthough our goal is to evaluate the evaluators, it is\ncritical to include a wide range of text generators\nwith various degrees of quality. A good evalua-\ntor should be able to distinguish the high-quality\ngenerators from the low-quality ones. We select a\ndiverse set of generative models from recent litera-\nture. The goal of this study is not to name the best\ngenerative model, and it is unfeasible to include all\nexisting models. Our criteria are: (1) the models\nare published before 2018, when our experiment is\nconducted; (2) the models represent different learn-\ning strategies and quality levels; (3) the models\nhave publicly available implementations, for repro-\nducibility purposes. In Table 1 we list the candidate\ngenerators. It is not an exhaustive list of what are\ncurrently available. For implementation details of\nthese models please see Appendix A.1.\nTable 1: Candidate models for review generation.\nGenerative Model Adversarial\nFramework\nWord LSTM temp 1.0 (Hochreiter and Schmidhuber, 1997)No\nWord LSTM temp 0.7 (Hochreiter and Schmidhuber, 1997)No\nWord LSTM temp 0.5 (Hochreiter and Schmidhuber, 1997)No\nScheduled Sampling (Bengio et al., 2015) No\nGoogle LM (Jozefowicz et al., 2016) No\nAttention Attribute to Sequence* (Dong et al., 2017)No\nContexts to Sequences* (Tang et al., 2016)No\nGated Contexts to Sequences* (Tang et al., 2016)No\nMLE SeqGAN (Yu et al., 2017) Yes\nSeqGAN (Yu et al., 2017) Yes\nRankGAN (Lin et al., 2017) Yes\nLeakGAN (Guo et al., 2018) Yes\n* indicates that review generation using these models are conditional on\ncontext information such as product ids; other models are context independent.\nEvery generator (except Google LM) is trained\nand validated on G-train and G-valid datasets, and\nused to generate the same number of machine-\ngenerated (a.k.a., fake) reviews (see Table 2). We\nfollow the best practice in literature to train these\nmodels, although it is possible that the performance\nof models might not be optimal due to various con-\nstraints. This will not affect the validity of the\nexperiment as our goal is to evaluate the evalua-\ntors instead of the individual generators. Google\nLM was not trained on reviews, but it provides a\nsanity check for the experiment - a reasonable eval-\nuator should not rank it higher than those trained\nfor generating reviews.\nTable 2: Number of generated reviews by each model.\nGenerative Model TotalD-TrainD-ValidD-Test\n∀model in Table 1 except Google LM32,50022,750 3,2506,500Google LM 6,680 4,676 668 1,336\n3.3 Evaluators\nWe include a comprehensive set of evaluators for\nthe quality of the aforementioned generators: i)\nhuman evaluators, ii) discriminative evaluators, and\niii) text overlap evaluators. The evaluators are the\nmain subjects of the experiment.\n3.3.1 Human evaluators\nWe conduct a careful power analysis (Christensen,\n2007), which suggests that at least 111 examples\nper generative model should be human annotated to\ninfer that the machine-generated reviews are com-\nparable in quality to human-written reviews, at a\nminimal statistically signiﬁcance level of 0.05. Per\nthis calculation, we sample 150 examples for each\nof the 12 generators for human evaluation. This\ntotals 1,800 machine-generated reviews, to which\nwe add 1,800 human-written reviews, or a total\nof 3,600 product reviews sent for human annota-\ntion. We markup out-of-vocabulary words in both\nhuman-written and machine-generated reviews to\ncontrol for confounds of using certain rare words.\nThere is no signiﬁcant difference in proportion of\nthe markup token between the two classes (2.5%-\nreal vs. 2.2%-fake). We recruit 900 human annota-\ntors through the Amazon Mechanical Turk (AMT)\nplatform. Each annotator is presented 20 reviews,\na mixture of 10 real (i.e., human written) and 10\nfake (i.e., machine generated), and they are charged\nto label each review as real or fake based on their\nown judgment. Clear instructions are presented\nto the workers that markup tokens are present in\nboth classes and cannot be used to decide whether\na review is real or fake. Each page is annotated\nby 5 distinct human evaluators. The 5 judgments\non every review are used to assemble two distinct\nhuman evaluators: H1 - individual votes, treat-\ning all human annotations independently, and H2\n- majority votesof the 5 human judgments. For\nevery annotated review, the human evaluator (H1\nor H2) makes a call which can be either right or\nwrong with regard to the ground truth. A generator\nis high quality if the human evaluator achieves low\naccuracy identifying the reviews as fake.\n3.3.2 Discriminative evaluators\nThe inclusion of multiple generators provides the\nopportunity of creating meta-adversarial evalu-\nators, trained using a pool of generated reviews\nby many generators, mixed with a larger number\nof “real” reviews (D-train and D-valid datasets).\nSuch a “pooling” strategy is similar to the standard\npractice used by the TREC conferences to evalu-\nate different information retrieval systems (Harman\nand V oorhees, 2006). Comparing to individual ad-\nversarial evaluators, a meta-evaluator is supposed\nto be more robust and fair, and it can be applied to\nevaluate new generators without being retrained. In\nour experiment, we ﬁnd that the meta-adversarial\nevaluators rank the generators in similar orders to\nthe best individual adversarial evaluators.\nWe employ a total of 7 meta-adversarial eval-\nuators: 3 deep, among which one using LSTM\n(Hochreiter and Schmidhuber, 1997), one using\nConvolutional Neural Network (CNN) (LeCun\net al., 1998), and one using a combination of\nLSTM and CNN architectures; 4 shallow, based on\nNaive Bayes (NB) (Rish, 2001), Random Forest\n(RF) (Liaw et al., 2002), Support Vector Machines\n(SVM) (Cortes and Vapnik, 1995), and XGBoost\n(Chen and Guestrin, 2016), with unigrams, bigrams,\nand trigrams as features and on balanced training\nsets. We ﬁnd the best hyper-parameters using ran-\ndom search and prevent the models from overﬁtting\nby using early stopping. For every review in D-test\n(either annotated or not), a meta-adversarial evalu-\nator makes a judgment call. A generator is consid-\nered high quality if the meta-adversarial evaluator\nmakes more mistakes on reviews it generated.\n3.3.3 Word-overlap evaluators\nWe include a set of 4 text-overlap metrics used for\nNLG evaluation: BLEU and METEOR (speciﬁc to\nmachine translation), ROUGE (used in text summa-\nrization), and CIDEr (Vedantam et al., 2015) (used\nin image description evaluation). These metrics\nrely on matching n-grams in the target text (i.e.,\ngenerated reviews) to the “references” (i.e., human-\nwritten reviews). The higher the overlap (similar-\nity), the higher the quality of generated text. For\nevery generated review inD-test Fake, we assemble\nthe set of references by retrieving the top-10 most\nsimilar human-written reviews in D-test Real using\na simple vector space model. We compute 600-\ndimensional vector representation of reviews using\nSent2Vec (Pagliardini et al., 2018), pretrained on\nEnglish Wikipedia, and retrieve the top-k nearest\nneighbors for each review based on cosine similar-\nity of the embedding vectors. The rationale of using\nnearest neighbors of each generated review as ref-\nerences is that to appear “real”, a generated review\njust need to be similar to some real reviews instead\nof all. A generator is considered high quality if its\ngenerated reviews obtain a high average score by\na text overlap evaluator. In total, we analyze and\ncompare 13 candidate evaluators (2 human evalua-\ntors, 7 discriminative evaluators, and 4 text-overlap\nmetrics), based on the D-test dataset.\n4 Results\nFirst, we are interested in the accuracy of individual\nevaluators - how well they can distinguish “fake”\n(machine-generated) reviews from “real” (human-\nwritten) reviews. Second, we are interested in how\nan evaluator assesses the quality of the 12 gener-\nators instead of individual reviews. The absolute\nscores an evaluator gives to the generators are not\nas informative as how it ranks them: a good evalu-\nator should be able to rank good generators above\nbad generators. Last but not least, we are inter-\nested in how the rankings by different evaluators\ncorrelate with each other. Intuitively, an automated\nevaluator that ranks the generators in similar orders\nas the human evaluators is more reasonable and can\npotentially be used as the surrogate of humans.\n4.1 Results of Individual Evaluators\n4.1.1 Human evaluators\nEvery review is annotated by 5 human judges as\neither “fake” or “real.” The inter-annotator agree-\nment (Fleiss-Kappa score (Fleiss et al., 2013)) is\nk = 0 .2748. This suggests that distinguishing\nmachine-generated reviews from real in general is\na hard task even for humans; there is limited con-\nsensus on what counts as a realistic review. The low\nagreement also implies that any automated evalu-\nator that mimics human judges is not necessarily\nthe most “accurate.”\nIn Figure 2 we present the accuracy of two hu-\nman evaluators on individual annotated reviews,\nbased on either all 5 annotations or their majority\nFigure 2: Accuracy of human evaluators on individual\nreviews: H1 - individual votes; H2 - majority votes.\nvotes for each review. Comparing to the ground-\ntruth (of whether a review is machine-generated\nor collected from Amazon), individual human de-\ncisions are 66.61% accurate, while their majority\nvotes can reach 72.63%. Neither of them is close to\nperfect. We observe that human evaluators gener-\nally do better at correctly labelling human-written\nreviews as real (true positive rate of 78.96% for\nH1 and 88.31% for H2), and they are confused\nby machine-generated reviews in close to half of\nthe cases (true negative rate of 54.26% forH1 and\n56.95% for H2). This trend reassures previous\nobservations (Tang et al., 2016).\nWe then look at how the human evaluators rank\nthe 12 generators, according to the accuracy of hu-\nman evaluators on all (fake) reviews generated by\neach of the generators. The lower the accuracy,\nthe more likely the human evaluator is confused\nby the generated reviews, and thus the better the\ngenerator. We observe a substantial variance in\nthe accuracy of both human evaluators on different\ngenerators, which suggests that human evaluators\nare able to distinguish between generators. The\ngenerator ranked the highest by both human evalu-\nators is Gated Contexts to Sequences. Google LM\nis ranked on the lower side, which makes sense as\nthe model is not trained to generate reviews. Inter-\nestingly, humans tend not to be fooled by reviews\ngenerated by the GAN-based models (MLE Seq-\nGAN, SeqGAN, RankGAN and LeakGAN), even\nthough their objective is to confuse fake from real.\nGAN-generated reviews tend to be easily distin-\nguishable from the real reviews by human judges.\n4.1.2 Discriminative evaluators\nWe then analyze the 7 meta-adversarial evaluators.\nDifferent from human evaluators that are applied\nto the 3,600 annotated reviews, the discriminative\nevaluators are applied to all reviews in D-test.\nMeta-adversarial Evaluators. On individual\nreviews, the three deep learning based and the\none SVM based evaluators achieve higher accu-\nracy than the two human evaluators, indicating\nthat adversarial evaluators can distinguish a sin-\ngle machine-generated review from human-written\nbetter than humans (Figure 3 and Table 4 in Ap-\npendix A.3.2). Their true positive rates and true\nnegative rates are more balanced than human eval-\nuators. Meta-discriminators commonly rank GAN-\nbased generators the highest. This makes sense as\nthe objective of GAN is consistent to the (reversed)\nevaluator accuracy. Interestingly, by simply setting\nthe temperature parameter of Word LSTM to 1.0,\nit achieves comparable performance to the GANs.\nFigure 3: Accuracy of human (H1, H2) and meta-\nadversarial evaluators (LSTM, SVM) on reviews gener-\nated by individual generators. The lower the accuracy,\nthe better the generator.\n4.1.3 Word-Overlap Evaluators\nThe generators are ranked based on the average\nscores of their generated reviews. In Figure 4 we\npresent the average scores of the 12 generators by\neach evaluator. Different word-overlap evaluators\nalso tend to rank the generators in similar orders.\nInterestingly, the top-ranked generator according\nto three evaluators is Contexts to Sequences, while\nCIDEr scores highest the Gated Contexts to Se-\nquences model. GAN-based generators are gener-\nally ranked low; please also see Appendix A.3.3.\n4.2 Comparing Evaluators\nTo what degree do the evaluators agree on the rank-\ning of generators? We are more interested in how\nthe automated evaluators compare to the human\nevaluators, and whether there is any suitable au-\ntomated surrogate for human judges at all. To do\nFigure 4: Text-Overlap Evaluators (BLEU and CIDEr)\nscores for individual generators. The higher the bet-\nter. The rankings are overall similar, as GAN-based\ngenerators are ranked low.\nthis, we compute the correlations between H1, H2\nand each discriminative evaluator and correlations\nbetween H1, H2 and the text-overlap evaluators,\nbased on either their decisions on individual re-\nviews, their scores of the generators (by Pearson’s\ncoefﬁcient (Fieller et al., 1957)), and their rank-\nings of the generators (by Spearman’sρ(Spearman,\n1904) and Kendall’sτ (Daniel et al., 1978)). Pat-\nterns of the three correlation metrics are similar;\nplease see Figure 5 and Table 5 in Appendix A.3.4.\nFigure 5: Kendall τ-b between human and automated\nevaluators. Human’s ranking is positively correlated\nto text-overlap evaluators and negatively correlated to\nadversarial evaluators (∗ is p≤0.05).\nSurprisingly, none of the discriminative evalu-\nators have a positive correlation with the human\nevaluators. That says, generators that fool ma-\nchine judges easily are less likely to confuse human\njudges, and vice versa. Word-overlap evaluators\ntend to have a positive correlation with the human\nevaluators in ranking the generators. Among them,\nBLEU appears to be closer to humans. This pat-\ntern is consistent in all three types of correlations.\nThese two observations are intriguing, which indi-\ncates that when identifying fake reviews, humans\nmight focus more on word usage rather than trying\nto construct a “decision boundary” mentally.\nIn summary, we ﬁnd that 1) human evalua-\ntors cannot distinguish machine-generated reviews\nfrom real reviews perfectly, with signiﬁcant bias\nbetween the two classes; 2) meta-adversarial evalu-\nators can better distinguish individual fake reviews,\nbut their rankings at the generator level tend to be\nnegatively correlated with human evaluators; and 3)\ntext-overlap evaluators are highly correlated with\nhuman evaluators in ranking generators.\n5 Discussion\nWe carried a systematic experiment that evaluates\nthe evaluators for NLG. Results have intriguing im-\nplications to both the evaluation and the construc-\ntion of natural language generators. We conduct\nin-depth analysis to discover possible explanations.\n5.1 Granularity of Judgments\nWe charged the Turkers to label individual reviews\nas either fake or real instead of evaluating each\ngenerator as a whole. Comparing to an adversarial\ndiscriminator, a human judge has not seen many\n“training” examples offake reviews or generators.\nThat explains why the meta-adversarial evaluators\nare better at identifying fake reviews. In this con-\ntext, humans are likely to judge whether a review\nis real based on how “similar” it appears to the true\nreviews they are used to seeing online.\nThis ﬁnding provides interesting implications to\nthe selection of evaluation methods for different\ntasks. In tasks that are set up to judge individual\npieces of generated text (e.g., reviews, translations,\nsummaries, captions, fake news) where there ex-\nists human-written ground-truth, it is better to use\nword-overlap metrics instead of adversarial eval-\nuators. When judgments are made on the agent/\nsystem level (e.g., whether a Twitter account is a\nbot), signals like how similar the agent outputs are\nor how much the agent memorizes the training ex-\namples may become more useful than word usage,\nand a discriminative evaluator may be more effec-\ntive than word-overlap metrics. Our ﬁnding also\nimplies that adversarial accuracy might not be the\noptimal objective for NLG if the goal is to generate\ndocuments that humans consider as real. Indeed, a\nfake review that fools humans does not necessarily\nneed to fool a machine that has seen everything. In\nAppendix B.2 we provide more details.\n5.2 Imperfect Ground Truth\nOne important thing to note is that all discrimi-\nnative evaluators are trained using natural labels\n(i.e., treating all examples from the Amazon review\ndataset as positive and examples generated by the\ncandidate models as negative) instead of human-\nannotated labels. Some reviews posted on Amazon\nmay have been generated by bots, and if that is the\ncase, treating them as human-written examples may\nbias the discriminators. To verify this, we apply the\nalready trained meta-discriminators to the human-\nannotated subset (3,600 reviews) instead of the full\nD-test set, and we use the majority vote of human\njudges (whether a review is fake or real) to surro-\ngate the natural “ground-truth” labels (whether a\nreview is generated or sampled from Amazon).\nFigure 6: Kendall τ-b correlation coefﬁcients between\nhuman evaluators and automated evaluators, tested on\nthe annotated subset ofD-test with majority votes as\nground-truth (∗ denotes p≤0.05).\nWhen the meta-adversarial evaluators are tested\nusing human majority-votes as ground-truth, the\nscores and rankings of these discriminative eval-\nuators are more inline with the human evaluators,\nalthough still not as highly correlated as BLEU;\nplease see Figure 6. Indeed, discriminative evalua-\ntors suffer the most from low-quality labels, as they\nwere directly trained using the imperfect ground-\ntruth. Word-overlap evaluators are more robust, as\nthey only take the most relevant parts of the test\nset as references (more likely to be high quality).\nOur results also suggest that when adversarial train-\ning is used, selection of training examples must be\ndone with caution. If the “ground-truth” is hijacked\nby low quality or “fake” examples, models trained\nby GAN may be signiﬁcantly biased. This ﬁnding\nis related to the recent literature of the robustness\nand security of machine learning models (Papernot\net al., 2017). Appendix B.3 contains further details.\n5.3 Role of Diversity\nWe assess the role diversity plays in rankings the\ngenerators. Diversity of a generator is measured\nby either the lexical diversity (Bache et al., 2013)\nor Self-BLEU (Zhu et al., 2018) of the samples\nproduced by the generator. Results obtained (see\nFigure 7) indicate generators that produce the least\ndiverse samples are easily distinguished by the\nmeta-discriminators, while they confuse humans\nthe most. This conﬁrms that adversarial discrimi-\nnators capture limitations of generative models in\nlack of diversity (Kannan and Vinyals, 2017).\nFigure 7: Self-BLEU scores (the lower the more di-\nverse) and lexical diversity scores (the higher the more\ndiverse) are highly correlated in ranking the generators.\nSimilarly, we measure to what extent the gen-\nerators are memorizing the training set G-train as\nthe average BLEU scores of generated reviews us-\ning their nearest neighbors in G-train as references.\nWe observe the generators do not memorize the\ntraining set, and GAN models generate reviews\nthat have fewer overlap with G-train; this ﬁnding\nis in line with recent theoretical studies on memo-\nrization in GANs (Nagarajan et al.).\nThe effects of diversity indicate that when hu-\nmans are distinguishing individual reviews as real\nor fake, whether or not a fake review is sufﬁciently\ndifferent from the other generated reviews is not a\nmajor factor for their decision. Instead, they tend\nto focus on whether the review looks similar to the\nreviews they have seen in reality. A discriminative\nevaluator is more powerful in making decisions at\na system level (e.g., a dialog system or a bot ac-\ncount), where diversity becomes a major factor. In\nAppendix B.4 we provide more details.\n5.4 User Study\nFinally, we are interested in the reasons why human\nannotators label certain reviews as fake (machine-\nwritten). After annotating a batch of reviews, work-\ners are asked to explain their decisions by ﬁlling in\nan optional free-text comment. This enables us to\nhave a better understanding of what differentiates\nmachine-generated from human-written reviews\nfrom human’s perspective. Analyzing their com-\nments, we identify the main reasons why human\nevaluators annotate a review as machine-written.\nThese are mainly related to the presence of gram-\nmatical errors in the review text, wrong wording or\ninappropriate choice of expressions, redundant use\nof speciﬁc phrases or contradictory arguments in\nthe review. Interestingly, human evaluators’ innate\nbiases are also reﬂected in their decisions: they\nare likely to categorize a review as fake if it is too\nformal, lacks emotion and personal pronouns, or is\ntoo vague and generic. Please see Appendix B.1.\n5.5 Summary\nIn summary, our ﬁndings represent a preliminary\nfoundation for proposing more solid and robust\nevaluation metrics and objectives of natural lan-\nguage generation. The low inter-rater agreement\nwe observe suggests that judging individual pieces\nof text as machine- or human-generated is a dif-\nﬁcult task even for humans. In this context, dis-\ncriminative evaluators are not as correlated with\nhuman judges as word-overlap evaluators. That\nimplies that adversarial accuracy might not be the\noptimal objective for generating individual docu-\nments when realism is the main concern. In con-\ntrast, GAN based models may more easily pass a\nTuring test on a system level, or in a conversational\ncontext. When the judges have seen enough exam-\nples from the same generator, the next example had\nbetter be somewhat different.\nOur results also suggest that when adversarial\nevaluation is used, the training examples must be\ncarefully selected to avoid false-positives. We also\nﬁnd that when humans are distinguishing fake re-\nviews from real ones, they tend to focus more on the\nusage of words, expressions, emotions, and other\ndetails. This may affect the design of objectives for\nthe next generation of NLG models.\nAcknowledgement\nWe thank Wei Ai for his help on the power analysis,\nand Yue Wang and Teng Ye for helpful discussions.\nThis work is in part supported by the National Sci-\nence Foundation under grant numbers 1633370 and\n1620319 and by the National Library of Medicine\nunder grant number 2R01LM010681-05.\nReferences\nKevin Bache, David Newman, and Padhraic Smyth.\n2013. Text-based measures of document diversity.\nIn Proceedings of the 19th ACM SIGKDD interna-\ntional conference on Knowledge discovery and data\nmining, pages 23–31. ACM.\nPhilip Bachman and Doina Precup. 2015. Data gener-\nation as sequential decision making. In Advances\nin Neural Information Processing Systems , pages\n3249–3257.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint\narXiv:1409.0473.\nSatanjeev Banerjee and Alon Lavie. 2005. Meteor: An\nautomatic metric for mt evaluation with improved\ncorrelation with human judgments. In Proceedings\nof the acl workshop on intrinsic and extrinsic evalu-\nation measures for machine translation and/or sum-\nmarization, pages 65–72.\nSamy Bengio, Oriol Vinyals, Navdeep Jaitly, and\nNoam Shazeer. 2015. Scheduled sampling for se-\nquence prediction with recurrent neural networks.\nIn Advances in Neural Information Processing Sys-\ntems, pages 1171–1179.\nYoshua Bengio, J ´erˆome Louradour, Ronan Collobert,\nand Jason Weston. 2009. Curriculum learning. In\nProceedings of the 26th annual international confer-\nence on machine learning, pages 41–48. ACM.\nYoshua Bengio, Patrice Simard, and Paolo Frasconi.\n1994. Learning long-term dependencies with gradi-\nent descent is difﬁcult. IEEE transactions on neural\nnetworks, 5(2):157–166.\nSamuel R Bowman, Luke Vilnis, Oriol Vinyals, An-\ndrew Dai, Rafal Jozefowicz, and Samy Bengio.\n2016. Generating sentences from a continuous\nspace. In Proceedings of The 20th SIGNLL Confer-\nence on Computational Natural Language Learning,\npages 10–21.\nAndrew Brock, Theodore Lim, James M Ritchie, and\nNick Weston. 2016. Neural photo editing with in-\ntrospective adversarial networks. arXiv preprint\narXiv:1609.07093.\nCameron B Browne, Edward Powley, Daniel White-\nhouse, Simon M Lucas, Peter I Cowling, Philipp\nRohlfshagen, Stephen Tavener, Diego Perez, Spyri-\ndon Samothrakis, and Simon Colton. 2012. A sur-\nvey of monte carlo tree search methods.IEEE Trans-\nactions on Computational Intelligence and AI in\ngames, 4(1):1–43.\nElia Bruni and Raquel Fern ´andez. 2017. Adversarial\nevaluation for open-domain dialogue generation. In\nProceedings of the 18th Annual SIGdial Meeting on\nDiscourse and Dialogue, pages 284–288.\nMassimo Caccia, Lucas Caccia, William Fedus, Hugo\nLarochelle, Joelle Pineau, and Laurent Charlin.\n2018. Language gans falling short. arXiv preprint\narXiv:1811.02549.\nChris Callison-Burch, Cameron Fordyce, Philipp\nKoehn, Christof Monz, and Josh Schroeder. 2008.\nFurther meta-evaluation of machine translation. In\nProceedings of the third workshop on statistical ma-\nchine translation , pages 70–106. Association for\nComputational Linguistics.\nChris Callison-Burch, Miles Osborne, and Philipp\nKoehn. 2006. Re-evaluation the role of bleu in ma-\nchine translation research. In 11th Conference of the\nEuropean Chapter of the Association for Computa-\ntional Linguistics.\nTong Che, Yanran Li, Ruixiang Zhang, R Devon\nHjelm, Wenjie Li, Yangqiu Song, and Yoshua Ben-\ngio. 2017. Maximum-likelihood augmented discrete\ngenerative adversarial networks. arXiv preprint\narXiv:1702.07983.\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005.\nTianqi Chen and Carlos Guestrin. 2016. Xgboost: A\nscalable tree boosting system. In Proceedings of\nthe 22nd acm sigkdd international conference on\nknowledge discovery and data mining , pages 785–\n794. ACM.\nErik Christensen. 2007. Methodology of superiority vs.\nequivalence trials and non-inferiority trials. Journal\nof hepatology, 46(5):947–954.\nOndˇrej C ´ıfka, Aliaksei Severyn, Enrique Alfonseca,\nand Katja Filippova. 2018. Eval all, trust a few,\ndo wrong to none: Comparing sentence generation\nmodels. arXiv preprint arXiv:1804.07972.\nCorinna Cortes and Vladimir Vapnik. 1995. Support-\nvector networks. Machine learning, 20(3):273–297.\nBo Dai, Sanja Fidler, Raquel Urtasun, and Dahua Lin.\n2017. Towards diverse and natural image descrip-\ntions via a conditional gan. In Proceedings of the\nIEEE International Conference on Computer Vision,\npages 2970–2979.\nRobert Dale and Chris Mellish. 1998. Towards evalu-\nation in natural language generation. In In Proceed-\nings of First International Conference on Language\nResources and Evaluation.\nWayne W Daniel et al. 1978. Applied nonparametric\nstatistics. Houghton Mifﬂin.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nLi Dong, Shaohan Huang, Furu Wei, Mirella Lapata,\nMing Zhou, and Ke Xu. 2017. Learning to generate\nproduct reviews from attributes. In Proceedings of\nthe 15th Conference of the European Chapter of the\nAssociation for Computational Linguistics: Volume\n1, Long Papers, volume 1, pages 623–632.\nDumitru Erhan, Yoshua Bengio, Aaron Courville,\nPierre-Antoine Manzagol, Pascal Vincent, and Samy\nBengio. 2010. Why does unsupervised pre-training\nhelp deep learning? Journal of Machine Learning\nResearch, 11(Feb):625–660.\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\n2018. Maskgan: Better text generation via ﬁlling in\nthe . arXiv preprint arXiv:1801.07736.\nJessica Ficler and Yoav Goldberg. 2017. Controlling\nlinguistic style aspects in neural language genera-\ntion. EMNLP 2017, page 94.\nEdgar C Fieller, Herman O Hartley, and Egon S Pear-\nson. 1957. Tests for rank correlation coefﬁcients. i.\nBiometrika, 44(3/4):470–481.\nJoseph L Fleiss, Bruce Levin, and Myunghee Cho Paik.\n2013. Statistical methods for rates and proportions.\nJohn Wiley & Sons.\nDaniela Gerz, Ivan Vuli´c, Edoardo Ponti, Jason Narad-\nowsky, Roi Reichart, and Anna Korhonen. 2018.\nLanguage modeling for morphologically rich lan-\nguages: Character-aware modeling for word-level\nprediction. Transactions of the Association of Com-\nputational Linguistics, 6:451–465.\nIan Goodfellow. 2016. Nips 2016 tutorial: Gen-\nerative adversarial networks. arXiv preprint\narXiv:1701.00160.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\nBing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. 2014. Generative ad-\nversarial nets. In Advances in neural information\nprocessing systems, pages 2672–2680.\nYvette Graham and Timothy Baldwin. 2014. Testing\nfor signiﬁcance of increased correlation with human\njudgment. In Proceedings of the 2014 Conference\non Empirical Methods in Natural Language Process-\ning, pages 172–176.\nAlex Graves. 2013. Generating sequences with\nrecurrent neural networks. arXiv preprint\narXiv:1308.0850.\nJiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong\nYu, and Jun Wang. 2018. Long text generation\nvia adversarial training with leaked information. In\nThirty-Second AAAI Conference on Artiﬁcial Intelli-\ngence.\nKelvin Guu, Tatsunori B Hashimoto, Yonatan Oren,\nand Percy Liang. 2018. Generating sentences by\nediting prototypes. Transactions of the Association\nof Computational Linguistics, 6:437–450.\nDavid Hardcastle and Donia Scott. 2008. Can we evalu-\nate the quality of generated text? In LREC. Citeseer.\nDonna K Harman and Ellen M V oorhees. 2006. Trec:\nAn overview. Annual review of information science\nand technology, 40(1):113–155.\nTatsunori B Hashimoto, Hugh Zhang, and Percy Liang.\n2019. Unifying human and statistical evaluation\nfor natural language generation. arXiv preprint\narXiv:1904.02792.\nSepp Hochreiter, Yoshua Bengio, Paolo Frasconi,\nJ¨urgen Schmidhuber, et al. 2001. Gradient ﬂow in\nrecurrent nets: the difﬁculty of learning long-term\ndependencies.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation ,\n9(8):1735–1780.\nFerenc Husz´ar. 2015. How (not) to train your genera-\ntive model: Scheduled sampling, likelihood, adver-\nsary? arXiv preprint arXiv:1511.05101.\nEric Jang, Shixiang Gu, and Ben Poole. 2016. Categor-\nical reparameterization with gumbel-softmax. arXiv\npreprint arXiv:1611.01144.\nFred Jelinek, Robert L Mercer, Lalit R Bahl, and\nJames K Baker. 1977. Perplexity?a measure of the\ndifﬁculty of speech recognition tasks. The Journal\nof the Acoustical Society of America , 62(S1):S63–\nS63.\nRafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam\nShazeer, and Yonghui Wu. 2016. Exploring\nthe limits of language modeling. arXiv preprint\narXiv:1602.02410.\nAnjuli Kannan and Oriol Vinyals. 2017. Adversar-\nial evaluation of dialogue models. arXiv preprint\narXiv:1701.08198.\nYoon Kim. 2014. Convolutional neural net-\nworks for sentence classiﬁcation. arXiv preprint\narXiv:1408.5882.\nDiederik P Kingma and Max Welling. 2013. Auto-\nencoding variational bayes. arXiv preprint\narXiv:1312.6114.\nAlex M Lamb, Anirudh Goyal ALIAS PARTH\nGOY AL, Ying Zhang, Saizheng Zhang, Aaron C\nCourville, and Yoshua Bengio. 2016. Professor forc-\ning: A new algorithm for training recurrent net-\nworks. In Advances In Neural Information Process-\ning Systems, pages 4601–4609.\nYann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick\nHaffner. 1998. Gradient-based learning applied to\ndocument recognition. Proceedings of the IEEE ,\n86(11):2278–2324.\nJiwei Li, Will Monroe, Tianlin Shi, S ´ebastien Jean,\nAlan Ritter, and Dan Jurafsky. 2017. Adversarial\nlearning for neural dialogue generation. In Proceed-\nings of the 2017 Conference on Empirical Methods\nin Natural Language Processing, pages 2157–2169.\nAndy Liaw, Matthew Wiener, et al. 2002. Classiﬁ-\ncation and regression by randomforest. R news ,\n2(3):18–22.\nChin-Yew Lin. 2004. Rouge: A package for auto-\nmatic evaluation of summaries. Text Summarization\nBranches Out.\nKevin Lin, Dianqi Li, Xiaodong He, Zhengyou Zhang,\nand Ming-Ting Sun. 2017. Adversarial ranking for\nlanguage generation. In Advances in Neural Infor-\nmation Processing Systems, pages 3155–3165.\nChia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Nose-\nworthy, Laurent Charlin, and Joelle Pineau. 2016.\nHow not to evaluate your dialogue system: An em-\npirical study of unsupervised evaluation metrics for\ndialogue response generation. In Proceedings of the\n2016 Conference on Empirical Methods in Natural\nLanguage Processing, pages 2122–2132.\nAdam Lopez. 2012. Putting human assessments of ma-\nchine translation systems in order. InProceedings of\nthe Seventh Workshop on Statistical Machine Trans-\nlation, pages 1–9. Association for Computational\nLinguistics.\nRyan Lowe, Michael Noseworthy, Iulian Vlad Ser-\nban, Nicolas Angelard-Gontier, Yoshua Bengio, and\nJoelle Pineau. 2017. Towards an automatic turing\ntest: Learning to evaluate dialogue responses. In\nProceedings of the 55th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 1116–1126.\nSidi Lu, Yaoming Zhu, Weinan Zhang, Jun Wang, and\nYong Yu. 2018. Neural text generation: Past, present\nand beyond. arXiv preprint arXiv:1803.07133.\nFujun Luan, Sylvain Paris, Eli Shechtman, and Kavita\nBala. 2017. Deep photo style transfer. CoRR,\nabs/1703.07511.\nPauline Luc, Camille Couprie, Soumith Chintala,\nand Jakob Verbeek. 2016. Semantic segmenta-\ntion using adversarial networks. arXiv preprint\narXiv:1611.08408.\nComputing Machinery. 1950. Computing machinery\nand intelligence-am turing. Mind, 59(236):433.\nChris Mellish and Robert Dale. 1998. Evaluation in the\ncontext of natural language generation. Computer\nSpeech & Language, 12(4):349–373.\nTom´aˇs Mikolov, Stefan Kombrink, Luk ´aˇs Burget,\nJan ˇCernock`y, and Sanjeev Khudanpur. 2011a.\nExtensions of recurrent neural network language\nmodel. In Acoustics, Speech and Signal Processing\n(ICASSP), 2011 IEEE International Conference on ,\npages 5528–5531. IEEE.\nTomas Mikolov, Stefan Kombrink, Anoop Deoras,\nLukar Burget, and Jan Cernocky. 2011b. Rnnlm-\nrecurrent neural network language modeling toolkit.\nIn Proc. of the 2011 ASRU Workshop , pages 196–\n201.\nTomas Mikolov and Geoffrey Zweig. 2012. Context\ndependent recurrent neural network language model.\nSLT, 12:234–239.\nEhsan Montahaei, Danial Alihosseini, and Mahdieh So-\nleymani Baghshah. 2019. Jointly measuring diver-\nsity and quality in text generation models. arXiv\npreprint arXiv:1904.03971.\nVaishnavh Nagarajan, Colin Raffel, and Ian J. Good-\nfellow. Theoretical Insights into Memorization in\nGANs.\nMyle Ott, Yejin Choi, Claire Cardie, and Jeffrey T\nHancock. 2011. Finding deceptive opinion spam\nby any stretch of the imagination. In Proceed-\nings of the 49th Annual Meeting of the Association\nfor Computational Linguistics: Human Language\nTechnologies-Volume 1, pages 309–319. Association\nfor Computational Linguistics.\nMatteo Pagliardini, Prakhar Gupta, and Martin Jaggi.\n2018. Unsupervised learning of sentence embed-\ndings using compositional n-gram features. In Pro-\nceedings of NAACL-HLT, pages 528–540.\nNicolas Papernot, Patrick McDaniel, Ian Goodfellow,\nSomesh Jha, Z Berkay Celik, and Ananthram Swami.\n2017. Practical black-box attacks against machine\nlearning. In Proceedings of the 2017 ACM on Asia\nconference on computer and communications secu-\nrity, pages 506–519. ACM.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of\nthe 40th annual meeting on association for compu-\ntational linguistics, pages 311–318. Association for\nComputational Linguistics.\nMark A. Przybocki, Kay Peterson, Sebastien Bronsart,\nand Gregory A. Sanders. 2009. The NIST 2008\nmetrics for machine translation challenge - overview,\nmethodology, metrics, and results. Machine Trans-\nlation, 23(2-3):71–103.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training. URL https://s3-\nus-west-2. amazonaws. com/openai-assets/research-\ncovers/languageunsupervised/language understand-\ning paper. pdf.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1:8.\nSai Rajeswar, Sandeep Subramanian, Francis Dutil,\nChristopher Pal, and Aaron Courville. 2017. Adver-\nsarial generation of natural language. arXiv preprint\narXiv:1705.10929.\nEhud Reiter and Anja Belz. 2009. An investigation into\nthe validity of some metrics for automatically evalu-\nating natural language generation systems. Compu-\ntational Linguistics, 35(4):529–558.\nEhud Reiter, Roma Robertson, A Scott Lennox, and\nLiesl Osman. 2001. Using a randomised controlled\nclinical trial to evaluate an nlg system. In Proceed-\nings of the 39th Annual Meeting on Association for\nComputational Linguistics, pages 442–449. Associ-\nation for Computational Linguistics.\nIrina Rish. 2001. An empirical study of the naive bayes\nclassiﬁer. In IJCAI 2001 workshop on empirical\nmethods in artiﬁcial intelligence , volume 3, pages\n41–46. IBM.\nTim Salimans, Ian Goodfellow, Wojciech Zaremba,\nVicki Cheung, Alec Radford, and Xi Chen. 2016.\nImproved techniques for training gans. In Advances\nin Neural Information Processing Systems , pages\n2234–2242.\nJ¨urgen Schmidhuber and Sepp Hochreiter. 1997. Long\nshort-term memory. Neural Comput , 9(8):1735–\n1780.\nStanislau Semeniuta, Aliaksei Severyn, and Syl-\nvain Gelly. 2018. On accurate evaluation of\ngans for language generation. arXiv preprint\narXiv:1806.04936.\nShikhar Sharma, Layla El Asri, Hannes Schulz, and\nJeremie Zumer. 2017. Relevance of unsuper-\nvised metrics in task-oriented dialogue for evalu-\nating natural language generation. arXiv preprint\narXiv:1706.09799.\nZhan Shi, Xinchi Chen, Xipeng Qiu, and Xuanjing\nHuang. 2018. Toward diverse text generation with\ninverse reinforcement learning. In Proceedings of\nthe 27th International Joint Conference on Artiﬁcial\nIntelligence, pages 4361–4367. AAAI Press.\nNasim Souly, Concetto Spampinato, and Mubarak\nShah. 2017. Semi and weakly supervised semantic\nsegmentation using generative adversarial network.\narXiv preprint arXiv:1703.09695.\nCharles Spearman. 1904. The proof and measurement\nof association between two things. The American\njournal of psychology, 15(1):72–101.\nAkash Srivastava, Lazar Valkoz, Chris Russell,\nMichael U Gutmann, and Charles Sutton. 2017. Vee-\ngan: Reducing mode collapse in gans using implicit\nvariational learning. In Advances in Neural Informa-\ntion Processing Systems, pages 3310–3320.\nRupesh Kumar Srivastava, Klaus Greff, and J ¨urgen\nSchmidhuber. 2015. Highway networks. arXiv\npreprint arXiv:1505.00387.\nRichard S Sutton, David A McAllester, Satinder P\nSingh, and Yishay Mansour. 2000. Policy gradient\nmethods for reinforcement learning with function ap-\nproximation. In Advances in neural information pro-\ncessing systems, pages 1057–1063.\nYaniv Taigman, Adam Polyak, and Lior Wolf.\n2016. Unsupervised cross-domain image genera-\ntion. arXiv preprint arXiv:1611.02200.\nJian Tang, Yifan Yang, Sam Carton, Ming Zhang, and\nQiaozhu Mei. 2016. Context-aware natural lan-\nguage generation with recurrent neural networks.\narXiv preprint arXiv:1611.09900.\nGuy Tevet, Gavriel Habib, Vered Shwartz, and\nJonathan Berant. 2018. Evaluating text gans as lan-\nguage models. arXiv preprint arXiv:1810.12686.\nL Theis, A van den Oord, and M Bethge. 2016. A note\non the evaluation of generative models. In Inter-\nnational Conference on Learning Representations\n(ICLR 2016), pages 1–10.\nRamakrishna Vedantam, C Lawrence Zitnick, and Devi\nParikh. 2015. Cider: Consensus-based image de-\nscription evaluation. In Proceedings of the IEEE\nconference on computer vision and pattern recogni-\ntion, pages 4566–4575.\nRonald J Williams. 1992. Simple statistical gradient-\nfollowing algorithms for connectionist reinforce-\nment learning. In Reinforcement Learning, pages\n5–32. Springer.\nRonald J Williams and David Zipser. 1989. A learn-\ning algorithm for continually running fully recurrent\nneural networks. Neural computation , 1(2):270–\n280.\nKelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,\nAaron Courville, Ruslan Salakhudinov, Rich Zemel,\nand Yoshua Bengio. 2015. Show, attend and tell:\nNeural image caption generation with visual at-\ntention. In International Conference on Machine\nLearning, pages 2048–2057.\nDenis Yarats and Mike Lewis. 2018. Hierarchical text\ngeneration and planning for strategic dialogue. InIn-\nternational Conference on Machine Learning, pages\n5587–5595.\nLantao Yu, Weinan Zhang, Jun Wang, and Yong Yu.\n2017. Seqgan: Sequence generative adversarial nets\nwith policy gradient. In AAAI, pages 2852–2858.\nYizhe Zhang, Zhe Gan, Kai Fan, Zhi Chen, Ricardo\nHenao, Dinghan Shen, and Lawrence Carin. 2017.\nAdversarial feature matching for text generation. In\nProceedings of the 34th International Conference\non Machine Learning-Volume 70, pages 4006–4015.\nJMLR. org.\nJun-Yan Zhu, Philipp Kr¨ahenb¨uhl, Eli Shechtman, and\nAlexei A Efros. 2016. Generative visual manip-\nulation on the natural image manifold. In Euro-\npean Conference on Computer Vision , pages 597–\n613. Springer.\nJun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A\nEfros. 2017. Unpaired image-to-image translation\nusing cycle-consistent adversarial networks. arXiv\npreprint arXiv:1703.10593.\nYaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo,\nWeinan Zhang, Jun Wang, and Yong Yu. 2018. Texy-\ngen: A benchmarking platform for text generation\nmodels. In The 41st International ACM SIGIR Con-\nference on Research & Development in Information\nRetrieval, pages 1097–1100. ACM.\nA Appendix A\nA.1 Implementation Details for Review\nGenerators\nRecurrent Neural Networks (RNNs) directly model\nthe generation process of text sequences, and pro-\nvide an end-to-end solution to learning the gener-\nating function from large quantities of data. These\nnetworks maintain a hidden layer of neurons with\nrecurrent connections to their own previous val-\nues, which in theory gives them the potential to\nmodel long span dependencies. For an input se-\nquence x = x1,x2,...,x t, the hidden state ht\nwhich summarizes the information of the entire\nsequence up to timestep tis recursively updated\nas ht = f(ht−1,xt), where f(.,.) denotes a non-\nlinear transformation function. The overall proba-\nbility of the sequence is calculated as:\np(x) =\nT∏\nt=1\np(xt|ht−1), (1)\nand the probability of generating the next word\nxt+1 given its low dimensional continuous repre-\nsentation Oxt+1 and input sequence xt is deﬁned\nas:\np(xt+1|x≤t) = p(xt+1|ht) ∝exp(OT\nxt+1 ht)\n(2)\nHowever, in practice the gradient computation is\ndifﬁcult to propagate back in time due to explod-\ning or vanishing gradients (Hochreiter et al., 2001),\n(Bengio et al., 1994), making the learning of arbi-\ntrarily long phenomena challenging in RNNs. Long\nShort Term Memory networks (LSTMs) (Hochre-\niter and Schmidhuber, 1997) effectively address\nthese limitations by relying on a memory state and\ngating functions to control the ﬂow of the infor-\nmation throughout the network – and in particular\nwhat information is written to the memory state,\nwhat information is read from the memory state,\nand what information is removed (or forgotten)\nfrom the memory state. The mathematical formula-\ntion of LSTM units can be expressed as follows:\ni(t) = σ(W(i)x(t) + U(i)h(t−1)) (Input gate)\nf(t) = σ(W(f)x(t) + U(f)h(t−1)) (Forget gate)\no(t) = σ(W(o)x(t) + U(o)h(t−1)) (Output gate)\n˜c(t) = tanh(W(c)x(t) + U(c)h(t−1) (New memory cell)\nc(t) = f(t) ◦˜c(t−1) + i(t) ◦˜c(t) (Final memory cell)\nh(t) = o(t) ◦tanh(c(t))\n(3)\nIn the above set of equations, the input word x(t)\nand the past hidden state h(t−1) are used to gen-\nerate new memory ˜c(t) which includes features of\nthe new word x(t) without prior determination of\nwhether x(t) is important and worth keeping. The\nrole of the input gate is to check whether it is sensi-\nble to store the new input word given the word x(t)\nitself and the past hidden state h(t−1); the input\ngate produces i(t) as output, which encapsulates\nthe worthiness decision of preserving the input in-\nformation. Similarly to the input gate, the forget\ngate also determines the usefulness of a word by\ninferring whether the past memory cell is used to\ncompute the current memory cell by looking at the\ninput word wordx(t) itself and the past hidden state\nh(t−1); it produces f(t) as output, which encapsu-\nlates the worthiness decision of preserving the past\nmemory cell. In the ﬁnal memory generation stage,\nthe advice of the input gate i(t) to gate the new\nmemory ˜c(t) and the advice of the forget gate f(t)\nto forget the past memory ˜c(t−1) are both consid-\nered, and the two results are summed up to produce\nthe ﬁnal memory c(t). The output gate is used to\nseparate the hidden state ht from the ﬁnal memory\nof the network c(t). Given that every state of the\nLSTM is relying on hidden states and that the ﬁnal\nmemory c(t) contains a lot of information not nec-\nessarily required to be saved in the hidden state, the\noutput gate discriminatively assesses which parts\nof the memory c(t) should be kept inside the hidden\nstate ht. In our experiments we employ an LSTM\ngenerative model trained at word level. Sampling\nfrom a trained word language model can be done in\ntwo ways: beam search (Bahdanau et al., 2014) and\nrandom sampling (Graves, 2013). Following (Tang\net al., 2016), we use random sampling with differ-\nent values for the temperature parameter. Sampling\nfrom the LSTM model with a high temperature re-\nsults in the model generating diverse samples at the\ncost of introducing some mistakes, while small tem-\nperatures generate conservative samples without a\nlot of content diversity. In our experiments, we\nempirically set the temperatures to the following\nvalues: 1.0, 0.7 and 0.5.\nRNNs, and LSTMs in particular, have become\nthe standard for modeling machine learning prob-\nlems that involve temporal and sequential data\nincluding text. The data is modeled via a fully-\nobserved directed graphical model, where the distri-\nbution over a discrete time sequencey1,y2,...,y T\nis decomposed into an ordered product of condi-\ntional distributions over tokens:\nP(y1,y2,...,y T) = P(y1)\nT∏\nt=1\nP(yt|y1,...,y t−1)\n(4)\nFor models with recurrent connections from their\noutputs leading back into the model, teacher forc-\ning (Williams and Zipser, 1989) is the most popular\ntraining strategy. This procedure emerges from the\nmaximum likelihood criterion, in which at training\ntime t+ 1 the model receives as input the ground\ntruth output yt:\nlog p(y(1),y(2)|x(1),x(2)) = log p(y(2)|y(1),x(1),x(2))\n+ logp(y(1)|x(1),x(2))\n(5)\nThe model in Equation 5 above illustrates the con-\nditional maximum likelihood criterion at timestep\nt= 2. The model is trained to maximize the con-\nditional probability of y(2) given the sequence x\ngenerated so far and the previous y(1) value. There-\nfore, maximum likelihood speciﬁes that at training\ntime the previous token generated by the model is\nreplaced with ground-truth examples yt that are fed\nback into the model for predicting outputs at later\ntime steps. Feeding back ground truth samples at\ntraining time forces the RNN to stay close to the\nground-truth sequence. However, at inference time,\nthe ground truth sequence is no longer available\nconditioning, and each yt is generated by the model\nitself (i.e. sampled from its conditional distribution\nover the sequence given the previously generated\nsamples). This discrepancy between training time\nand inference time causes errors in the model pre-\ndictions that accumulate and amplify quickly over\nthe generated sequence as the model is in a part\nof the state space it has never seen during train-\ning time. Small prediction errors compound in the\nRNN’s conditioning context, and as the generated\nsample starts to diverge from sequences it has seen\nduring training, the prediction performance of the\nRNN worsens (Lamb et al., 2016).\nTo alleviate this problem, Bengio et al (Bengio\net al., 2015) propose Scheduled Sampling (SS), a\nlearning strategy for training RNNs which mixes\ninputs from the ground-truth sequence with inputs\ngenerated by the model itself at training time. SS\nrelies on curriculum learning (Bengio et al., 2009)\nto change the training process from a fully guided\nscheme using the true previous token to a less\nguided scheme mostly using the generated token.\nThe choice of replacing the ground truth with the\nmodel’s prediction is determined by a coin ﬂip\nwith some probability, independently for each to-\nken. The probability of using the ground truth is\nset to a high value initially. As the model gradually\nkeeps improving, samples from the model become\nmore frequent and the model is partially fed with\nits own synthetic data as preﬁx in a similar way to\ninference mode. Therefore, the training objective\nis slowly changed from an easy task where the pre-\nvious token is known, to a realistic task where the\nprevious token is provided by the model itself. The\nscheduled sampling training scheme is meant to\nmake the model more robust and forces it to deal\nwith its own mistakes at training time, in a similar\nway to inference time. However, as the model gen-\nerates several consecutive tokensyt-s, it is not clear\nwhether the correct target distribution remains the\nsame as in the ground truth sequence. The authors\npropose two solutions: i) make the self-generated\nsequences short, and ii) anneal the probability of\nusing self-generated vs. ground-truth samples to 0,\naccording to some schedule.\nDespite its impressive empirical performance,\nHuszar et al (Husz´ar, 2015) show that SS is an in-\nconsistent training strategy which pushes models\ntowards memorising the distribution of symbols\nconditioned on their position in the sequence in-\nstead of on the preﬁx of preceding symbols. Ac-\ncording to the authors, SS pays no attention to the\ncontent of the sequence preﬁx, and uses the hid-\nden states to implement a simple counter which\nmakes the model likely to recover from its own\nmistakes. Moreover, it is possible that the good\nperformance of the model on image captioning\ndatasets is either due to the algorithm not running\nuntil convergence, or to a lucky combination of fac-\ntors including the model structure, early stopping,\nrandom restarts, and the annealing schedule. The\nauthors recommend adversarial training strategies\nas a much better choice for generative models.\nTang et al (Tang et al., 2016) study the the prob-\nlem of NLG at particular contexts or situations.\nThe authors focus on user review data due to its\nrichness of context, sentiments and opinions ex-\npressed. They propose two approaches built on top\nof the encoder-decoder framework to generate user\nreviews as text sequences from user product con-\ntexts. In the ﬁrst approach, Contexts to Sequences,\nthe authors encode the product context information− →C = {− →ci}i=1,...,K, where − →ci denotes a type of\ncontext and Kthe number of context types, into a\ncontinuous semantic representation, which is fed\ninto an LSTM decoder to generate text sequences.\nDespite promising results shown by the method,\nthe authors consider that for long generated se-\nquences the information from contexts is not prop-\nagated to distant words. In their second approach,\nGated Contexts to Sequences, the authors add skip-\nconnections to directly build the dependency be-\ntween contexts hC and each word when predicting\nthe next word xt+1 in a sequence. When a new\nword in a sequence is generated, it does not only\ndepend on the current hidden state ht, but it also\ndepends on the context representation hC. Similar\nto the ﬁrst model, the decoder is a vanilla recurrent\nneural network with LSTM unit.\nFocusing on the same problem as Tang et al\n(Tang et al., 2016), Dong et al (Dong et al., 2017)\npropose Attention Enhanced Attribute to Sequence\nModel. The model learns to encode product at-\ntributes into vectors by means of an encoder net-\nwork, and then generate reviews by condition-\ning on the encoded vectors inside a sequence de-\ncoder, and an attention mechanism (Bahdanau et al.,\n2014), (Xu et al., 2015) which learns soft align-\nments between the input attributes and the gener-\nated words. The product review generation prob-\nlem is formally deﬁned as follows. Given input\nattributes a = ( a1,...,a |a|), generate a product\nreview r = ( y1,...,y |r|) which maximizes the\nconditional probability p(r|a):\np(r|a) =\n|r|∏\nt=1\np(yt|(y1,...,y t−1),a) (6)\nWhile the number of attributes |a|is ﬁxed for each\nproduct, the review text ris a sequence of variable\nlength. In our experiments we use the two mod-\nels proposed by Tang et al (Tang et al., 2016) and\nDong et al (Dong et al., 2017) to generate use prod-\nuct reviews given the context information and the\nreview text of each product in the Amazon dataset.\nIn addition to the already mentioned models,\nwe also employ a pre-trained model released by\nGoogle, commonly referred to as Google LM (Joze-\nfowicz et al., 2016). The model is an important\ncontribution to the ﬁeld of neural language model-\ning which emphasizes large scale recurrent neural\nnetwork training. The model was trained on the\nOne Billion Word Benchmark (Chelba et al., 2013),\na publicly available dataset containing mainly news\ndata and used as a reference standard for measur-\ning the progress of statistical language modeling.\nThe dataset includes 1 billion words in total with\na vocabulary of 800,000 unique words. While for\ncount based language models it is considered a\nmedium-sized dataset, for neural network based\nlanguage models the benchmark is regarded as a\nvery large dataset. In terms of the model archi-\ntecture, the GoogleLM model is a 2-layer LSTM\nneural network with 8,192 and respectively 1,024\nhidden units in each layer, the largest Google was\nable to ﬁt into GPU memory. The model uses\nConvolutional Neural Networks (CNNs) character\nembeddings as input, and makes predictions one\ncharacter at a time, which presents the advantage\nthat the model does not need to learn long-term\ndependencies in the data. We employ GoogleLM\nto generate sentences with a topic which identiﬁes\nwith the existing three categories (books, electron-\nics and movies) present in the Amazon dataset we\nused.\nGenerative Adversarial Networks (GANs)\n(Goodfellow et al., 2014) represent a training\nmethodology for generative models via an adver-\nsarial process, and are aimed at generating syn-\nthetic data which resembles the real data. The\nGAN framework works through the interplay be-\ntween two feedforward neural network models, a\ngenerative model G and a discriminative model\nD, trained simultaneously by competing against\neach other. The generative model Gaims to cap-\nture the data distribution and generate high quality\nsynthetic data, while the discriminative model D\nestimates the probability a sample comes from the\nreal training data and not from the synthetic data\ngenerated by G. Concretely, the generator Gtakes\nas input a vector of random numbers z, and trans-\nforms it into the form of the data we are interested\nin imitating; the discriminator Dtakes as input ei-\nther the real data x or generated data G(z), and\noutputs probability P(x) of the respective data be-\ning real. The GAN framework is equivalent to a\nminimax two-player game between the two models\nGand D:\nmin\nG\nmax\nD\nV(D,G) = Ex∼pdata(x)[log D(x)]\n+ Ez∼pz(z)[log(1 −D(G(z)))]\n(7)\nAdversarial learning algorithms iteratively sam-\nple batches from the data and noise distributions,\nand use noisy gradient information to simulate-\nnously ascend in the parameters θd of D, while\ndescending in the parameters θg of G. The discrim-\ninator Dis optimized to increase the likelihood of\nassigning a high probability to the real data xand\na low probability to the fake generated data G(z).\nThe gradient for the discriminator can be expressed\nas follows:\n▽θd\n1\nm\nm∑\ni=1\n[\nlog D(x(i)) + log(1−D(G(z(i))))\n]\n(8)\nAlternatively, the generator G is optimized to\nincrease the probability the generated data G(z) is\nrated highly:\n▽θg\n1\nm\nm∑\ni=1\n[\nlog(1 −D(G(z(i))))\n]\n(9)\nThe goal of the generator Gis to maximize the\nprobability of discriminator Dmaking a mistake by\ngenerating highly realistic data, while the discrim-\ninator D is learnt to distinguish whether a given\ndata instance is real or not. The gradient of the\ntraining loss from the discriminator Dis used as\nguidance for updating the parameters of the genera-\ntor G. Gradient optimization is alternated between\nthe two networks Dand Gas illustrated in Equa-\ntions 8 and 9 on batches of real and generated data\nuntil GAN converges, at which point the data pro-\nduced by GAN is the most realistic the network is\ncapable of modeling.\nHowever, GAN’s applicability to discrete data\nis limited, despite the great success at generating\nrealistic real valued synthetic samples in many com-\nputer vision tasks for eg., image generation (Brock\net al., 2016), (Zhu et al., 2016), (Taigman et al.,\n2016), image style transfer (Luan et al., 2017),\n(Zhu et al., 2017) and semantic segmentation (Luc\net al., 2016), (Souly et al., 2017). Training gen-\nerative models of text using GANs is challenging\ndue to the discrete nature of text data, which makes\nit difﬁcult to backpropagate the gradient from the\ndiscriminator Dto the generator G. GANs are de-\nsigned for generating real-valued, continuous data,\nand the gradient of the loss from discriminator D\nw.r.t. the output of generator Gis used to guide\nGto slightly change the generated value to make\nit more realistic (i.e. the gradient of the output\nof the discriminator network with respect to the\nsynthetic data indicates how to slightly change the\nsynthetic data to make it more plausible). Changes\ncan be made to the synthetic data if it is based on\nreal numbers, however for discrete tokens the slight\nchange guidance is not a useful signal, as it is very\nlikely that there is no corresponding token to the\nslight change given the limited vocabulary space3.\nIn addition, a further reason why GANs cannot\nbe applied to text data is because the discrimina-\ntor Dcan only asses a complete sequence. When\nhaving to provide feedback for partially generated\nsequences, it is non-trivial to balance the current\nscore of the partially generated sequence with the\nfuture score after the entire sequence has been gen-\nerated (Yu et al., 2017). In the literature there are\ntwo approaches on how to deal with the problem of\nnon-differentiable output and ﬁnding the optimal\nweights in a neural network: the REINFORCE al-\ngorithm, and Gumbel-Softmax reparameterization.\nWe present each method below.\nREINFORCE (Williams, 1992) algorithms, also\nknown as REward Increments, score-function es-\ntimators, or likelihood-ratio methods adjust the\nweights of a neural network based on the log deriva-\ntive trick in a direction that lies along the gradi-\nent of expected reinforcement without explicitly\ncomputing gradient estimates. It is a policy gra-\ndient method which uses the likelihood ratio trick(▽θp(X,θ)\nP(X,θ) = ▽θlog p(X,θ); ∂\n∂x log f(x) = f′(x)\nf(x)\n)\nto update the parameters of an agent and increase\nthe probability that the agent’s policy will select a\nrewarding action given a state. Given the trajectory\nτt = (u1,...,u t−1,x0,...,x t) made up of a se-\nquence of states xk and control actions uk, the goal\nof policy gradient is to ﬁnd policy πϑ which takes\n3https://www.reddit.com/r/\nMachineLearning/comments/40ldq6/\ngenerative_adversarial_networks_for_\ntext/\nas input trajectory τt and outputs a new control\naction that maximizes the total reward after Ltime\nsteps. πϑ is a parametric randomized policy which\nassumes a probability distribution over actions:\np(τ; ϑ) =\nL−1∏\nt=0\np(xt+1|xt,ut)πv(ut|τt) (10)\nIf we deﬁne the reward of a trajectory as:\nR(τ) =\nN∑\nt=0\nRt(xt,ut), (11)\nthe reinforcement learning optimization problem\nbecomes:\nmax\nϑ\nJ(ϑ) = max\nϑ\nEp(τ|ϑ)[R(τ)] (12)\nThen policy gradient can be derived as follows:\n▽ϑJ(ϑ) =\n∫\nR(τ)▽ϑp(τ; ϑ)dτ\n=\n∫\nR(τ)▽ϑp(τ; ϑ)\np(τ; ϑ) p(τ; ϑ)dτ\n=\n∫\n(R(τ)▽ϑlog p(τ; ϑ))p(τ; ϑ)dτ\n= Ep(τ;ϑ)[R(τ)▽ϑlog p(τ; ϑ)]\n(13)\nFrom Equation 13 we have that the gradient of J\nw.r.t. ϑis equal to the expected value of the func-\ntion G(τ,ϑ) = R(τ)▽ϑlog p(τ; ϑ). This function\nprovides an unbiased estimate of the gradient of\nJ and can be computed by running policy πϑ and\nsampling a trajectory τ without knowing the dy-\nnamics of the system since p(xt+1|xt,ut) does not\ndepend on parameter ϑ. Following this direction is\nequivalent to running stochastic gradient descent\non J.\n▽ϑlog p(τ; ϑ) =\nL−1∑\nt=0\n▽ϑlog πϑ(ut|τt) (14)\nThe policy gradient algorithm can be summarized:\n1. Choose ϑ0, stepsize sequence αk, and set k=\n0;\n2. Run the simulator with policy πϑk and sample\nτk;\n3. ϑk+1 = ϑk +\nαkR(τk) ∑L−1\nt=0 ▽ϑlog πϑ(utk|τt);\n4. k= k+ 1, go to step 2.\nThe policy gradient algorithm can be run on\nany problem if sampling from πϑ can be done ef-\nﬁciently. Policy gradient is simple as it optimizes\nover a parametric familyp(u; ϑ) instead of optimiz-\ning over the space of all probability distributions.\nHowever, there are constraints regarding the proba-\nbility distribution, which should be easy to sample\nfrom, easy to search by gradient methods, and rich\nenough to approximate delta functions. In addition,\nthe complexity of the method depends on the di-\nmensionality of the search space and can be slow\nto converge. Finally, the policy gradient update\nis noisy, and its variance increases proportionally\nwith the simulation length L.\nThe other solution to the problem of dealing\nwith non-differentiable output is to use the the\nGumbel-Softmax (Jang et al., 2016) approach, and\nreplace the non-differentiable sample from the cat-\negorical distribution with a differentiable sample\nfrom a Gumbel-Softmax distribution. The Gumbel-\nSoftmax distribution is a continuous distribution on\nthe simplex that can approximate categorical sam-\nples. Parameter gradients can be easily computed\nby applying the reparameterization trick (Kingma\nand Welling, 2013), a popular technique used in\nvariational inference and adversarial learning of\ngenerative models in which the expectation of a\nmeasurable function g of a random variable ϵ is\ncalculated by integrating g(ϵ) with respect to the\ndistribution of ϵ:\nE(g(ϵ)) =\n∫\ng(ϵ)dFϵ (15)\nTherefore, in order to compute the expectation of\nz= g(ϵ) we do not need to know explicitly the dis-\ntribution of z, but only know gand the distribution\nof ϵ. This can alternatively be expressed as:\nEϵ∼p(ϵ)(g(ϵ)) = Ez∼p(z)(z) (16)\nIf the distribution of variable zdepends on param-\neter φ, i.e. z ∼ pφ(z), and if we can assume\nz = g(ϵ,φ) for a known function g of parame-\nters φand noise distribution ϵ∼N(0,1), then for\nany measurable function f:\nEϵ∼p(ϵ)(f(g(ϵ,φ))) = Ez∼pφ(z)(f(z))\nEϵ∼p(ϵ)(▽f(g(ϵ,φ))) = ▽φEϵ∼p(ϵ)(f(g(ϵ,φ)))\n= ▽φEz∼pφ(z)(f(z))\n(17)\nIn equation 17, zhas been conveniently expressed\nsuch that functions of zcan be deﬁned as integrals\nw.r.t. to a density that does not depend on the\nparameter φ. Constructing unbiased estimates of\nthe gradient is done using Monte Carlo methods:\n▽φEz∼pφ(z)(f(z)) ∼ 1\nM\nM∑\ni=1\n▽f(g(ϵi,φ)) (18)\nThe reparameterization trick aims to make the ran-\ndomness of a model an input to that model instead\nof letting it happen inside the model. Given this,\nthe network model is deterministic and we can dif-\nferentiate with respect to sampling from the model.\nAn example of applying the reparameterization\ntrick is to rewrite samples drawn from the normal\ndistribution z ∼ N(µ,σ) as z = µ+ σϵ, with\nϵ ∼ N(0,1). In this way stochastic nodes are\navoided during backpropagation. However, the re-\nparameterization trick cannot be directly applied to\ndiscrete valued random variables, for eg. text data,\nas gradients cannot backpropagate through discrete\nnodes in the computational graph.\nThe Gumbel-Softmax trick attempts to over-\ncome the inability to apply the reparameteriza-\ntion trick to discrete data. It parameterizes a dis-\ncrete distribution in terms of a Gumbel distribu-\ntion, i.e. even if the corresponding function is not\ncontinuous, it will be made continuous by apply-\ning a continuous approximation to it. A random\nvariable Ghas a standard Gumbel distribution if\nG = −log(−log(U)),U ∼Unif[0,1]. Any dis-\ncrete distribution can be parameterized in terms\nof Gumbel random variables as follows. If X is\na discrete random variable with P(X = k) ∝αk\nrandom variable and {Gk}k≤K an i.i.d. sequence\nof standard Gumbel random variables, then:\nX = arg max\nk\n(log αk + Gk) (19)\nEquation 19 illustrates sampling from a categorical\ndistribution: draw Gumbel noise by transforming\nuniform samples, add it to log αk, then take the\nvalue of kthat yields the maximum. The arg max\noperation that relates the Gumbel samples is not\ncontinuous, however discrete random variables can\nbe expressed as one-hot vectors and take values in\nthe probability simplex:\n∆K−1 = {x∈RK\n+ ,\nK∑\nk=1\nxk = 1} (20)\nA one hot vector corresponds to a discrete category,\nand since thearg maxfunction is not differentiable,\na softmax function can be used instead as a contin-\nuous approximation of arg max:\nfτ(x)k = exp(xk/τ)∑K\nk=1 exp(xk/τ)\n(21)\nTherefore, the sequence of simplex-valued random\nvariables Xτ is:\nXτ = (Xτ\nk)k = fτ(log α+ G)\n= exp((log αk + Gk)/τ)∑K\ni=1 exp((log αi + Gi)/τ)\n(22)\nEquation 22 is known as the Gumbel-Softmax dis-\ntribution and can be evaluated exactly for different\nvalues of x, α and τ, where τ is a temperature\nparameter that controls how closely the samples\nfrom the Gumbel-Softmax distribution approxi-\nmate those from the categorical distribution. When\nτ →0, the softmax function becomes an arg max\nfunction and the Gumbel-Softmax distribution be-\ncomes the categorical distribution. At training time\nτ is a set to a value greater than 0 which allows\ngradients to backpropagate past the sample, and\nthen is gradually annealed to a value close to 0.\nThe Gumbel Softmax trick is important as it allows\nfor the inference and generation of discrete objects.\nA direct application of this technique is generating\ntext via GANs.\nIn summary, GANs have shown impressive per-\nformance at generating natural images nearly indis-\ntinguishable from real images, however applying\nGANs to text generation is a non-trivial task due to\nthe special nature of the linguistic representation.\nAccording to Dai et al (Dai et al., 2017), the two\nmain challenges to overcome when using GANs\nwith textual input are:\ni) ﬁrst, text generation is a sequential non-\ndifferentiable sampling procedure which samples\na discrete token at each time step (vs. image\ngeneration where the transformation from the in-\nput random vector to the produced output image\nis a deterministic continuous mapping); the non-\ndifferentiability of text makes it difﬁcult to apply\nback-propagation directly, and to this end, classical\nreinforcement learning methods such as Policy Gra-\ndient (Sutton et al., 2000) have been used. In policy\ngradient the production of each word is considered\nas an action for which the reward comes from the\nevaluator, and gradients can be back-propagated\nby approximating the stochastic policy with a para-\nmetric function.\nii) second, in the GAN setting the generator re-\nceives feedback from the evaluator when the entire\nsample is produced, however for sequence genera-\ntion this causes difﬁculties during training, such as\nvanishing gradients and error propagation. To al-\nlow the generator to get early feedback when a text\nsequence is partly generated, Monte Carlo rollouts\nare used to calculate the approximated expected\nfuture reward. This has been found empirically to\nimprove the efﬁciency and stability of the training\nprocess.\nUnlike in conventional GAN settings that deal\nwith image generation, the production of sentences\nis a discrete sampling process, which is also non-\ndifferentiable. A natural question that arises is\nhow can the feedback be back-propagated from the\ndiscriminator to the generator under such a formu-\nlation. Policy gradient considers a sentence as a\nsequence of actions, where each word wt is an ac-\ntion and the choices of such actions are governed by\na policy πθ. The generative procedure begins with\nan initial state S1:0 which is the empty sentence,\nand at each time step tthe policy πθ takes as input\nthe previously generated wordsS1:t−1 up until time\nt−1, as well as the noise vector z, and yields a\nconditional distribution πθ(wt|z,S1:t−1) over the\nvocabulary words. The computation is done one\nstep at a time moving along the LSTM network\nand sampling an action wt from the conditional\ndistribution up until wt will be equal to the end of\nsentence indicator, in which case the sentence is\nterminated. The reward for the generated sequence\nof actions S will be a score r calculated by the\ndiscriminator. However, this score can be com-\nputed only after the sentence has been completely\ngenerated, and in practice this leads to difﬁculties\nsuch as vanishing gradients and very slow training\nconvergence. Early feedback is used to evaluate\nthe expected future reward when the sentence is\npartially generated, and the expectation can be ap-\nproximated using Monte Carlo rollouts. The Monte\nCarlo rollout method is suitable to use when a part\nof the sentence S1:t has been already generated,\nand we continue to sample the remaining words of\nthe sentence from the LSTM network until the end\nof sentence token is encountered. The conditional\nsimulation is conducted ntimes, which results in\nn sentences. For each sentence we compute an\nevaluation score, and the rewards obtained by the\nsimulated sentences are averaged to approximate\nthe expected future reward of the current sentence.\nIn this way updating the generator is possible with\nfeedback coming from the discriminator. The util-\nity of the policy gradient method is that by using\nthe expected future reward the generator is pro-\nvided with early feedback and becomes trainable\nwith gradient descent.\nYu et al propose SeqGAN (Yu et al., 2017), a\nGAN-based sequence generation framework with\npolicy gradient, which is the ﬁrst work to employ\nGANs for generating sequences of discrete tokens\nto overcome the limitations of GANs on textual\ndata. SeqGAN treats the sequence generation pro-\ncedure as a sequential decision making process\n(Bachman and Precup, 2015). A discriminator is\nused to evaluate the generated sequence and pro-\nvide feedback to the generative model to guide its\nlearning. It is a well known problem of GANs that\nfor text data (discrete ouputs) the gradient cannot\nbe passed back from the discriminator to the gener-\nator. SeqGAN addresses this problem by treating\nthe generator as a stochastic parameterized policy\ntrained via policy gradient (Sutton et al., 2000) and\noptimized by directly performing gradient policy\nupdate, therefore avoiding the differentiation difﬁ-\nculty for discrete data. The reinforcement learning\nreward comes from the discriminator based on the\nlikelihood that it would be fooled judged on a com-\nplete sequence of tokens, and is passed back to the\nintermediate state-action steps using Monte Carlo\nsearch (Browne et al., 2012).\nThe sequence generation problem is deﬁned\nas follows. Given a dataset of human writ-\nten sequences, train a generative model Gθ pa-\nrameterized by θ to output sequence Y1:T =\n(y1,...,y t,...,y T),yt ∈Y, where Y is the word\nvocabulary. The current state is the sequence of to-\nkens (y1,...,y t−1) generated until timestep t, and\nthe action ataken from this state is the selection\nof next token yt. The policy model Gθ(yt|Y1:t−1)\nis stochastic and will select an action according\nto the leant probability distribution of the input to-\nkens. The state transition from the current state\ns= Y1:t−1 to the next state s\n′\n= Y1:t after choos-\ning action a = y is deterministic, i. e. δa\ns,s′ = 1\nfor next state s\n′\n, and δa\ns,s′′ = 0 for other next states\ns\n′′\n. The discriminative model Dφ(Y1:T) is used to\nguide the generator Gθ, and outputs a probability\nindicating how likely a sequence Y1:T produced by\nGθ comes from real sequence data. Dφ is trained\nwith both real and fake examples from the real se-\nquence data and the synthetic data generated by\nGθ. The objective of the generator model (policy)\nGθ(yy|Y1:t−1) is to maximize its expected end re-\nward RT which comes from the discriminator Dφ\nfor a sequence which is generated starting from\ninitial state s0:\nJ(θ) = E[RT|s0,θ] =\n∑\ny1∈Y\nGθ(y1|s0)QGθ\nDφ\n(s0,y1)\n(23)\nThe action-value function QGθ\nDφ\n(s,a) for a se-\nquence represents the expected cumulative reward\nstarting from state s, taking action a and then\nfollowing policy Gθ. The action value function\nQGθ\nDφ\n(s,a) is calculated as the estimated probabil-\nity (reward) the discriminator Dφ(Yn\n1:T) assigns to\nthe generated sample being real:\nQGθ\nDφ\n(a= yT,s = Y1:T−1) = Dφ(Yn\n1:T) (24)\nIn the GAN setup, the discriminator Dφ can only\nprovide a reward at the end of a ﬁnished se-\nquence. In order to evaluate the action-value func-\ntion QGθ\nDφ\n(s,a) for an intermediate state s, Monte\nCarlo search with roll-out policy Gβ (identical to\nthe generator Gθ policy) is used to sample the un-\nknown remaining T−ttokens that result in a com-\nplete sentence. The roll-out policy Gβ starts from\nthe current state sand is run for N times to get an\naccurate assessment of the action-value function\nQGθ\nDφ\n(s,a) through a batch of N output samples,\nthus reducing the variance of the estimation:\n{Y1\n1:T,...,Y N\n1:T}= MCGβ(Y1:t; N)\nQGθ\nDφ\n(a= yt,s = Y1:t−1) =\n\n\n\n1\nN\n∑N\nn=1 Dφ(Yn\n1:T),\nif Yn\n1:T ∈MCGβ(Y1:t;N),t<T\nDφ(Y1:t),if t= T\n(25)\nThe generator starts with random sampling at ﬁrst,\nbut once more realistic samples have been gener-\nated, the discriminator Dφ is updated (which will\nin turn improve the generator model iteratively):\nmin\nφ\n−EY∼pdata [log Dφ(Y)]−EY∼Gθ[log(1−Dφ(Y))]\n(26)\nThe generator Gθ is updated every time a new dis-\ncriminator Dφ has been obtained. The gradient\nof the generator’s objective functionJ(θ) w.r.t the\ngenerator’s parametersθis expressed as follows:\n∇θJ(θ) =\nT∑\nt=1\nEY1:t−1∼Gθ\n[∑\nyt∈Y\n∇θGθ(yt|Y1:t−1)·\n·QGθ\nDφ\n(Y1:t−1,yt)\n]\n(27)\nExpectation E can be approximated by sampling\nmethods, and generator’s parameters are updated:\nθ←θ+ αh∇θJ(θ),where αh −learning rate\n(28)\nIn the initial stages of training, the generator Gθ\nis pre-trained via maximum likelihood estimation,\nand the discriminator Dφ is pre-trained via mini-\nmizing the cross-entropy between the ground truth\nlabel and the predicted probability; after the pre-\ntraining stage is over, the generator and the dis-\ncriminator are trained alternatively. The SeqGAN\nauthors chose an LSTM (Schmidhuber and Hochre-\niter, 1997) architecture for the generator in order\nto avoid the vanishing and the exploding gradient\nproblem of back-propagation through time, and\na CNN (LeCun et al., 1998), (Kim, 2014) archi-\ntecture with highway networks (Srivastava et al.,\n2015) as discriminator. The evaluation metric is set\nto minimize the average negative log-likelihood be-\ntween the generated data and an oracle considered\nas the human observer:\nNLLoracle = −EY1:T∼Gθ\n[ T∑\nt=1\nlog Goracle(yt|Y1:t−1)\n]\n(29)\nLin et al (Lin et al., 2017) consider that GANs\nrestrict the discriminator too much by forcing it\nto be a binary classiﬁer. Because of this setup,\nthe discriminator is limited in its learning capac-\nity especially for tasks with a rich structure, such\nas when generating natural language expressions.\nThe authors propose a generative adversarial frame-\nwork called RankGAN, which is able to capture\nthe richness and diversity of language by learning\na relative ranking model between the machine writ-\nten and human written sentences in an adversarial\nframework. The adversarial network consists of\ntwo neural network models, a generator Gθ and\na ranker Rφ, where θand φare parameters. The\nRankGAN discriminator Rφ, instead of perform-\ning a binary classiﬁcation task as in conventional\nGANs, is trained to rank the machine-written sen-\ntences lower than human-written sentences w.r.t. a\nhuman-written reference set. Alternatively, the gen-\nerator Gθ is trained to confuse the ranker Rin such\na way that machine written sentences are ranked\nhigher than human written sentences with regard\nto the reference set. The authors consider that by\nviewing a set of samples collectively (instead of just\none sample) and evaluating their quality through\nrelative ranking, the discriminator can make better\njudgements regarding the quality of the samples,\nwhich helps in turn the generator better learn to\ngenerate realistic sequences. The problem can be\nexpressed mathematically as Gθ and Rφ playing a\nminimax game with the objective function L:\nmin\nθ\nmax\nφ\nL(Gθ,Rφ) = Es∼Ph[log Rφ(s|U,C−)]+\nEs∼Gθ[log(1 −Rφ(s|U,C+))]\n(30)\nThe ranker Rφ is optimized to increase the like-\nlihood of assigning a high probability to the real\nsentence sand a low probability to the fake gen-\nerated data Gθ. s ∼Ph denotes that sentence s\nis sampled from human written sentences, while\ns ∼Gθ denotes that sentence sis sampled from\nmachine written sentences. U is a reference set\nwhich is used for estimating relative ranks. C+\nand C−are comparison sets with regards to input\nsentences. When the input sentence sis sampled\nfrom the real data, C−is sampled from the gener-\nated data, and alternatively when the sentence sis\nsampled from the synthetic data generated by Gθ,\nC+ is sampled from human written data.\nSimilar to SeqGAN, the authors use policy gradi-\nent to overcome the non-differentiability problem\nof text data. However, unlike SeqGAN, the regres-\nsion based discriminator is replaced with a ranker\nand a new learning objective function. The gen-\nerative model Gθ is an LSTM network, while the\nranker Rφ is a CNN network. The rewards for\ntraining the model are encoded with relative rank-\ning information. When a sequence is incomplete,\nan intermediate reward is computed using Monte\nCarlo rollout methods. The expected future reward\nV for partial sequences is deﬁned as:\nVθ,φ(s1:t−1,U) = Esr∼Gθ[log Rφ(sr|U,C+,s1:t−1)]\n(31)\nIn Equation 31 above, sr denotes a complete se-\nquence sampled by using rollout methods start-\ning from sequence s1:t−1. A total of n different\npaths are sampled, and their corresponding ranking\nscores are computed. The average ranking score is\nused to approximate the expected future reward for\nthe current partially generated sequence s1:t−1; the\nranking score of an input sentencesgiven reference\nsentence uand comparison set C(where C = C+\nif sentence sis machine generated, C = C−other-\nwise) is computed using a softmax-like formula:\nP(s|u,C) = exp(γα(s|u))∑\ns′∈C′exp(γα(s\n′\n|u)),where\nα(s|u) = cos(ys,yu) = ysyu\n||ys||||yu||\n(32)\nIn Equation 32, ys is the embedded feature vector\nof the input sentence, and yu is the embedded fea-\nture vector of the reference sentence. The gradient\nof the objective function for generator Gθ for start\nstate s0, vocabulary V, and generator policy πθ is\ncomputed as:\n▽θLθ(s0) = Es1:T∼Gθ\n[ T∑\nt=1\n∑\nwt∈V\n▽θπθ(wt|s1:t−1)·\n·Vθ,φ(s1:t,U)\n]\n(33)\nTherefore, RankGAN deals with the gradient van-\nishing problem of GAN by replacing the original\nbinary classiﬁer discriminator with a ranking model\nin a learning-to-rank framework. The ranking score\nis computed by taking a softmax over the expected\ncosine distances from the generated sequences to\nthe real data.\nGuo et al (Guo et al., 2018) ﬁnd that a limita-\ntion of current GAN frameworks for text genera-\ntion (Yu et al., 2017), (Lin et al., 2017), (Rajeswar\net al., 2017), (Che et al., 2017), (Li et al., 2017),\n(Zhang et al., 2017) is that they are only capable\nof generating short texts, within a limited length of\naround 20 tokens. Generating longer sequences is\na less studied but more challenging research prob-\nlem with a lot of useful applications, such as the\nauto-generation of news articles or product descrip-\ntions. Nevertheless, long text generation faces the\nissue that the binary guiding signal from generator\nDis sparse and non-informative; it does not pro-\nvide useful information regarding the intermediate\nsyntactic structure and semantics of the generated\ntext so that the generator Gcould learn from that\nsignal. Besides that, it is only available after the\nentire sequence has been generated, and the ﬁnal\nreward value does not provide much guidance on\nhow to alter the parameters of Gat training time.\nMoreover, the approach of relying on binary feed-\nback from the discriminator requires a very large\nnumber of real and generated samples to improve\nG. Aiming to make the guiding signal coming from\nthe discriminator Dmore informative, the authors\npropose LeakGAN (Guo et al., 2018), a GAN ap-\nproach for adversarial text generation in which the\ndiscriminative model Dis allowed to leak its own\nhigh-level extracted features (in addition to provid-\ning the ﬁnal reward value) to better guide the train-\ning of the generative model G. The authors pick a\nhierarchical generator for G, which is made up of\ntwo distinct modules: a high-level manager mod-\nule, and a low-level worker module. The high level\nmanager module (or mediator) receives the feature\nmap representation of the discriminator D; this\nis not normally allowed in the conventional GAN\nsetup as this feature map is internally maintained\nby the discriminator. The manager embeds this\nfeature map representation coming from the dis-\ncriminator and passes it over to the worker module.\nThe worker ﬁrst encodes the current generated se-\nquence, and combines this resulting encoding with\nthe embedding produced by the manager to decide\nwhat action to take at the current state. Therefore,\nLeakGAN “leaks” guiding signals from the dis-\ncriminator Dto the generator Gmore frequently\nand more informatively throughout the sequence\ngeneration process and not at the end only, helping\nGimprove better and faster.\nThe discriminator Dφ is made up of a feature\nextractor F(.; φf) and a ﬁnal sigmoid classiﬁcation\nlayer. For input sequence s, Dφ is deﬁned as:\nDφ(s) = sigmoid(φT\nl F(s; φf)) = sigmoid(φT\nl f)\n(34)\nThe feature vector in the last layer ofDφ is denoted\nas f = F(s; φf), and it will be leaked to the gen-\nerator Gθ. A natural implication of this approach\nis that the reward the generator Gθ receives for\na partially generated sequence is directly related\nto the quality of the extracted features by the dis-\ncriminator Dφ. Therefore, for the discriminator\nDφ to yield a high reward, it is necessary to ﬁnd\na highly rewarding region in the extracted feature\nspace. The authors consider that compared to a\nscalar signal, the feature vector f is more infor-\nmative as it captures the position of the generated\nwords in the extracted feature space. Dφ is imple-\nmented as a CNN network. The manager module\nM(ft,hM\nt−1; θm) of the hierarchical generator Gθ\nreceives as input the extracted feature vector ft,\nwhich it combines with its internal hidden state to\nproduce the goal vector gt:\ng\n′\nt = M(ft,hM\nt−1; θm)\ngt = g\n′\nt\n||g\n′\nt||\n(35)\nThe goal vector embedding wt of goal gt is com-\nputed by applying a linear transformation ψwith\nweight matrix Wψ to the sum of recent cgoals:\nwt = ψ(\nc∑\ni=1\ngt−i) = Wψ(\nc∑\ni=1\ngt−i) (36)\nwt is fed to the worker module W(.; θw), which\nis in charge with the generation of the next token.\nThe worker module takes the current word xt as\ninput and outputs matrix Ot; this matrix is then\ncombined through a softmax with the goal vector\nembedding wt:\nOt,hW\nt = W(xt,hW\nt−1; θw)\nGθ(.|st) = softmax(Otwt/α) (37)\nAt training time, the manager and the worker mod-\nules are trained separately – the manager is trained\nto predict which are the most rewarding positions in\nthe discriminative feature space, while the worker\nis rewarded to follow these directions. The gradient\nfor the manager module is deﬁned as:\n▽adv\nθmgt = −QF(st,gt)▽θmdcos(ft+c−ft,gt(θm))\n(38)\nQF(st,gt) deﬁnes the expected reward under the\ncurrent policy and can be approximated using\nMonte Carlo search. dcos computes cosine sim-\nilarity between the goal vector gt(θm) produced\nby the manager, and the change in feature repre-\nsentation ft+c −ft after ctransitions. In order to\nachieve a high reward, the loss function is trying\nto force the goal vector to match the transition in\nfeature space. Before the adversarial training takes\nplace, the manager undergoes a pre-training stage\nwith a separate training scheme which mimics the\ntransition of real text samples in the feature space:\n▽pre\nθm = −▽θmdcos(f\n′\nt+c −f\n′\nt,gt(θm)) (39)\nThe worker uses the REINFORCE algorithm dur-\ning training to maximize the reward when taking\naction xt given the previous state is st−1:\n▽θwEst−1∼G\n[∑\nxt\nrI\ntW(xt|st−1; θw)\n]\n=\nEst−1∼G,xt∼W(xt|st−1)\n[\nrI\nt▽θw log W(xt|st−1; θw)\n]\nrI\nt = 1\nc\nc∑\ni=1\ndcos(ft −ft−i,gt−i)\n(40)\nDuring the adversarial training process, the genera-\ntor Gθ and the discriminator Dφ are trained in al-\nternative stages. When the generator Gθ is trained,\nthe worker W(.; θw) and the manager M(.; θm)\nmodules are trained alternatively ﬁxing each other.\nMode collapse (Goodfellow, 2016) is a com-\nmon problem when training GAN models, when\nthe generator learns to produce samples with ex-\ntremely low variety, limiting the usefulness of the\nleant GAN model. In mode collapse the genera-\ntor network learns to output samples from a few\nmodes of the data distribution only, missing out on\nmany other modes even though samples from these\nmissing modes can be found throughout the train-\ning data. Mode collapse can range from complete\ncollapse, when the generated samples are entirely\nidentical, to partial collapse when the generated\nsamples present some common properties (Srivas-\ntava et al., 2017), (Salimans et al., 2016). Several\nattempts have been made to address the problem,\nwhich include: i) directly encouraging the gener-\nator cost function to account for the diversity of\nthe generated batches by comparing these samples\nacross a batch in order to determine whether the en-\ntire batch is real or fake, ii) anticipate counterplay,\nin which the generator learns to fool the discrimina-\ntor before the discriminator has a chance to respond\n(and therefore taking counterplay into account), iii)\nexperience replay, which minimizes the switching\nbetween modes by showing old fake generated sam-\nples to the discriminator every now and then, and\niv) using multiple GANs, in which a GAN is trained\nfor each different mode so that when combined, the\nGANs altogether cover all modes.\nIn LeakGAN, in order to address mode collapse,\nthe authors propose an interleaved training scheme,\nwhich combines supervised training using maxi-\nmum likelihood estimation with GAN adversarial\ntraining (instead of carrying only GAN adversar-\nial training after the pretraining stage). Blending\ntwo training schemes is considered useful by the\nauthors as it helps LeakGAN overcome local min-\nimums, alleviates mode collapse and acts as an\nimplicit regularizer on the generative model.\nA.2 Samples produced by the review\ngenerators\nFigure 8 shows the instructions given to the AMT\nworkers who participated in this study. In Figure 9\nwe include a screen-shot of the user interface when\nannotating reviews.\nIn what follows we present samples generated by\nthe review generators on which human annotators\ndisagree most on whether these are human-written\nor machine-generated.\n•Word LSTM temp 1.0\na) i so enjoyed this book . i felt though .\ni especially like loving horses in the .\nand the story is well written .\nb) one of a different type on locked para-\nnormal / vacation book . i enjoyed the\ncharacters and the plot . great mixture of\nhistorical ﬁction .\nc) this ﬁrst edition of the complete series 8\nyears over six episodes just makes you\nlaugh . the original tv is by far my cup\nof tea !\nFigure 8: Screenshot of the instructions presented to Amazon Mechanical Turk workers.\nFigure 9: Screenshot of the Amazon Mechanical Turk user study interface.\nd) works out of the box ! wouldn ’ t spend\nthe money for a better keyboard . use\nthis with the matching kindle screen as\nwell .\n•Word LSTM temp 0.7\na) i am looking forward to the next book .\ni am a and i enjoyed the story . i\nlike books where the characters are real .\nb) this is an exciting book i could n ’ t put\ndown . i will probably read more books\nby this author . this is a must read .\nc) okay , that ’ s how i expected this movie\n. it was okay but it was so boring . i was\nbored and was disappointed .\nd) this cable is not bad . it is so cheap and it\nworks great . i ’ ve used this for a couple\nof months now and on the ipad\n•Word LSTM temp 0.5\na) this book was a great read ! the story was\nexciting and a bit . i really enjoyed\nthe characters and the story line .\nb) this is a great cable for the price . i would\nrecommend this product to anyone need-\ning a cable for a great price .\nc) this is a great series . it is a must see for\nanyone who loves period dramas . i love\nthe .\nd) these batteries seem to be working as\nexpected . i have had no problems with\nthis product . i would recommend this to\nanyone .\n•Scheduled Sampling\na) like most of the ones i have ! the tablet\nthat came starts working properly .\nb) i have had any almost using keyboards\nwith an iphone case and kept it nicely\nand time . and it works well .\nc) have got to watch it many times again\nand the seasons of each episode we\ncan all watch it .\nd) very interesting characters and likable\ncharacters that grow when you gave me\nof the because of the dog . what\ncan i say is i absolutely loved it .\n•Google LM\na) systems generally require less band-\nwidth and with operating systems ,\nusers to write and edit data nearly any-\nwhere .\nb) seems all but impossible to access . is\nall a and gets a bad on every .\nc) is based in , , with a com-\nmercial ofﬁce in\nd) oved this clip and the and apps\nwere about so much fun that paid a\nbig price . 2 and 3 like crazy .\n•Attention Attribute to Sequence\na) i am always waiting for the next book to\ncome out . i am a big fan of sean black\nand will .\nb) purchased this to use with my macbook\npro . it worked out perfectly , as de-\nscribed . no complaints .\nc) great book all of the great mystery books\n. i enjoyed all of them and was sad when\nthe book ended .\nd) this is a great product . i ’ ve had it\nfor over a year now and it ’ s still go-\ning strong . i ’ m very happy with this\npurchase .\n•Contexts to Sequences\na) i love this series . i love the characters\nand the story . i love the characters and\nthe story line .\nb) a great book and a great read . i love\nthe characters and the story . i would\nrecommend this book to anyone .\nc) i enjoyed the story . it was a good read\n. i would recommend it to anyone who\nlikes a good read .\nd) i love this book and i love the characters\n. i love this book and i was not disap-\npointed .\n•Gated Contexts to Sequences\na) this is the ﬁrst book i have read by this\nauthor . would recommend to anyone\nwho likes a good romance book .\nb) one of the best books i have ever read\n. the chemistry between the two main\ncharacters was a good read .\nc) this book is awesome . lots of action and\nintrigue . i ’ m glad i bought this book .\nthank you for sharing\nd) great story and plot . sometimes a little\nslow at times but overall a good read .\n•MLE SeqGAN\na) you will like this movie - get this set\n. . . better than expected award for the\ncharacters . bad ending .\nb) this switch converter works ﬁne with all\ngames and works perfect , sturdy pro-\ngram to zero manual products . nice feel\n.\nc) i could not put it down . it was an inter-\nesting clean book , but i was expecting\nmany more individuals in this story so i\nread in a long time .\nd) great story . in college kids has been\nlost the mysteries , chris son is not\nbetter .\n•SeqGAN\na) it was slow he kept me interested , and i\nthink i thoroughly enjoyed the story .\nb) i enjoyed this book and look forward to\ngetting to larson .\nc) received in excellent condition . i\nthought it was great but didn ’ t know\nthat movies were more than high ratings\nwhich i am my cup of tea .\nd) awesome cute story . kudos to mr much\nof the sookie ’ s story .\n•RankGAN\na) robin williams is ok . just a great movie\nwith now . is a great ﬁlm with\nthree stars ! wonderful video for a very\ngood movie .\nb) i have loved this movie so i could like the\ndvd sort of info . hot slow . love the old\nford shows to though . a great actor .\nc) this was a very amazing . laws and\noh fact she became and is very\nunlikely together on the case .\nd) i say so i would that originally arrived so\ni love the circular inch screen . i am sad\nhow it works .\n•LeakGAN\na) i really enjoyed reading this book . the\nauthor did an excellent job in delivering\nfor all his writing books into us as busi-\nness . a great summer read .\nb) just loved it , so much could read more of\nthis series , i like it but it was not written\nin a book that is well written , but very\ninteresting .\nc) i love hockey - baseball movie coming\nmeets hockey ’ s et addicted fear the\nbirds feature so popular ﬁlms have de-\nveloped far worse reviews .\nd) a very good book with a lot of twists in\nthis book . i will be checking out more\nof this author next book .\nA.3 Results\nA.3.1 Human Evaluators\nWe chose the task of distinguishing machine-\ngenerated from real reviews because it is a straight-\nforward surrogate of a Turing test. Moreover, how\nmuch their generated content can fool humans has\nbeen a key claim of many artiﬁcial intelligence\nmodels recently. The low inter-rater agreement\nsuggests that this is a difﬁcult task even for hu-\nmans, which we hope would trigger the community\nto rethink about these claims. There are indeed\nﬁner-grained, perhaps more agreeable aspects of\ntext quality (including semantic coherence, syn-\ntactic correctness, ﬂuency, adequacy, diversity and\nreadability). We decided not to include them in this\nexperiment for two reasons: 1) as the ﬁrst study,\nwe are not sure which aspects human raters would\nconsider when they judge for the realism of a re-\nview; 2) we wanted to keep the experiment design\nsimple, and many of these aspects are harder to\ndeﬁne. In the post-experiment survey, the raters\ncommented on the reasons why they considered\nreviews as fake.\nThe low inter-rater agreement (0.27) reﬂects\nthe difﬁculty/ subjectivity of the task: identifying\nindividual reviews as human-written or machine-\ngenerated. Low human agreement is commonly\nreported in subjective evaluation tasks. Since our\ngoal is to evaluate the evaluators instead of the\ncompeting algorithms, it is important to use a task\nneither too easy or too hard, so that there are distin-\nguishable differences among the performances of\ncompetitors (including humans). When using the\nmajority vote of human judgements, the accuracy\nof humans improved to a reasonable 72.63 %.\nA.3.2 Discriminative Evaluators\nIn Table 3 and Table 4 we present comprehensive\nresults for the meta-adversarial evaluators.\nA.3.3 Text-Overlap Evaluators\nIn Figure 10 we present detailed results for all word\noverlap evaluators we used in this study.\nA.3.4 Comparing Evaluators\nIn Table 5 we present correlation results between\nthe evaluators included in this work.\nA.3.5 Diversity Analysis\nIn Table 6 we present results for the Self-BLEU\nmetric, while in Table 7 we present the correlation\nof Self-BLEU with the other evaluators. In addi-\ntion, in Table 8 we present correlation results for\nBLEU G-Train and the rest of the evaluators.\nB Discussion\nB.1 User Study\nA more detailed list of major clusters of reasons is\nas follows:\n1. Grammar/ typo/ mis-spelling: the language\ndoes not ﬂow well.\n2. Too general/ too generic/ vagueness: gener-\nated reviews are vague, in lack of details.\n3. Word choice (wording): in lack of slang, use\nthe wrong words.\nTable 3: Accuracy of deep (LSTM) and shallow (SVM)\nmeta-adversarial evaluators. The lower the better.\nMeta-adversarial evaluators do better than humans on\nindividual reviews, with less bias between the two\nclasses. GAN-based generators are considered to be\nthe best by meta-adversarial evaluators.\nGenerators LSTM SVM\nWord LSTM temp 1.0 48.29 % 50.31%\nWord LSTM temp 0.7 92.58 % 78.69 %\nWord LSTM temp 0.5 99.31 % 94.74 %\nScheduled Sampling 50.09 % 51.31 %\nGoogle LM 84.58 % 78.59 %\nAttention Attribute to Sequence90.08 % 74.37 %\nContexts to Sequences 100.00 %100.00 %\nGated Contexts to Sequences98.37 % 96.26 %\nMLE SeqGAN 41.45% 52.35 %\nSeqGAN 50.05 % 56.20 %\nRankGAN 66.28 % 70.17 %\nLeakGAN 87.03 % 77.55 %\nD-test (all) 77.58 % 74.50 %\nD-test (human-written) 80.12 % 75.98 %\nD-test (machine-generated) 75.04 % 73.01 %\n4. Flow (not ﬂuent)/ structured/ logical: the sen-\ntences level language errors.\n5. Contradictory arguments: some arguments\nsupport opposite opinions.\n6. Emotion: lack of emotion, personality in the\ncomments.\n7. Repeated text: using words/ phrases repeti-\ntively.\n8. Overly same as human: too advertisement,\ntoo formal, too likely to be real.\nB.2 Granularity of Judgements\nWe charged the Turkers to label individual reviews\nas either fake or real. Each human judge only an-\nnotates 20 reviews, and they do not know which\nreviews are generated by the same generator. Com-\nparing to an adversarial discriminator, a human\njudge has not seen many “training” examples of\nfake reviews or generators. That explains why the\nmeta-adversarial evaluators are better at identifying\nfake reviews. In this context, humans are likely to\njudge whether a review is real based on how “simi-\nlar” it appears to the true reviews they are used to\nsee online. That is probably why their decisions are\nbetter correlated to text-overlap metrics that mea-\nsures the similarity between a review and a set of\nreferences. This hypothesis is supported by a post-\nexperiment survey of the human judges. Please see\nAppendix A.2 for user study samples.\nThis ﬁnding provides interesting implications to\nthe selection of evaluation methods for different\ntasks. In tasks that are set up to judge individual\npieces of generated text (e.g., reviews, translations,\nsummaries, captions, fake news) where there ex-\nists human-written ground-truth, it is better to use\nword-overlap metrics instead of adversarial evalu-\nators. Indeed, when the audience are not trained\nby reading lots of bot-generated texts, it is more\nreasonable to use an evaluator that mimics their\ndecision-making process.\nIn some scenarios, the task is to make judgments\nin the context of a longer conversation or a set\nof documents (e.g., conversation agents, dialogue\nsystems, social bots). The difference is that human\nsubjects are exposed to machine-generated text, so\nthat they may be better trained to distinguish fake\nfrom real. Moreover, when judgments are made\non the agent/ system level (e.g., whether a Twitter\naccount is a bot), signals like how similar the agent\nTable 4: Accuracy of deep (LSTM, CNN, CNN & LSTM) and shallow (SVM, RF, NB, XGBoost) meta-adversarial\nevaluators. The lower the better.Meta-adversarial evaluators do better than humans on individual reviews, with\nless bias between the two classes. GAN-based generators are considered best by meta-adversarial evaluators.\nGenerators LSTM CNN CNN & LSTMSVM RF NB XGBoost\nWord LSTM temp 1.0 48.29 %55.22 % 45.68 % 50.31% 53.63 %32.77 % 48.97 %\nWord LSTM temp 0.7 92.58 %93.14 % 91.02 % 78.69 %81.05 %79.92 % 80.49 %\nWord LSTM temp 0.5 99.31 %99.35 % 99.08 % 94.74 %94.29 %96.86 % 94.71 %\nScheduled Sampling 50.09 %48.77 % 43.37 % 51.31 %52.88 %20.97% 44.12 %\nGoogle LM 84.58 %74.03 % 74.85 % 78.59 %82.71 %48.28 % 82.41 %\nAttention Attribute to Sequence90.08 %91.78 % 89.94 % 74.37 %77.29 %80.02 % 71.68 %\nContexts to Sequences 100.00 %100.00 % 99.97 % 100.00 %99.98 %100.00 %99.98 %\nGated Contexts to Sequences98.37 %99.06 % 98.38 % 96.26 %95.35 %98.63 % 93.62 %\nMLE SeqGAN 41.45% 47.54% 41.91% 52.35 %51.14% 21.83 % 43.71%\nSeqGAN 50.05 %52.91 % 47.35 % 56.20 %54.91 %25.60 % 48.11 %\nRankGAN 66.28 %67.23 % 59.37 % 70.17 %61.94 %35.98 % 61.23 %\nLeakGAN 87.03 %80.28 % 79.57 % 77.55 %67.74 %46.80 % 63.80 %\nD-test (all) 77.58 %74.72 % 75.18 % 74.50 %70.31 %70.74 % 73.79 %\nD-test (human-written) 80.12 %73.54 % 77.99 % 75.98 %68.59 %83.53 % 79.10 %\nD-test (machine-generated)75.04 %75.90 % 72.38 % 73.01 %72.04 %57.95 % 68.48 %\nFigure 10: Text-Overlap Evaluators (BLEU, ROUGE, METEOR and CIDEr) scores for individual generators.The\nhigher the better.The rankings are overall similar, as GAN-based generators are ranked low.\noutputs are or how much the agent memorizes the\ntraining examples may become more useful than\nword usage, and a discriminative evaluator may be\nmore effective than text-overlap metrics.\nOur experiment also provide implications to im-\nproving NLG models, which implies that adver-\nsarial accuracy might not be the optimal objective\nfor NLG if the goal is to generate documents that\nhumans consider as real. Indeed, a fake review that\nfools humans does not necessarily need to fool a\nmachine that has seen everything.\nIn contrast, GAN based models may perform\nbetter when judged as a whole system instead of\nindividual items, or in a conversational context.\nWhen the human judges have seen enough exam-\nples from the same generator, the next example had\nbetter be somewhat different.\nB.3 Imperfect Ground-truth\nOne important thing to note is that all discrimi-\nnative evaluators are trained using natural labels\n(i.e., treating all examples from the Amazon re-\nview dataset as positive and examples generated\nby the candidate models as negative) instead of\nhuman-annotated labels. It is possible that if they\nwere trained with human labels, the discriminative\nevaluators would have been more consistent to the\nhuman evaluators. Indeed, some reviews posted\non Amazon may have been generated by bots, and\nif that is the case, treating them as human-written\nexamples may bias the discriminators.\nOne way to verify this is to consider an alterna-\ntive “ground-truth”. We apply the already trained\nmeta-discriminators to the human-annotated subset\n(3,600 reviews) instead of the full D-test set, and\nwe use the majority vote of human judges (whether\na review is fake or real) to surrogate the “ground-\ntruth” labels (whether a review is generated or sam-\npled from Amazon).\nSurprisingly, when the meta-adversarial eval-\nuators are tested using human majority-votes as\nground-truth, both the accuracy numbers and the\nEvaluation Method Kendall tau-bSpearmanPearson Kendall tau-bSpearmanPearson\n(H1) (H1) (H1) (H2) (H2) (H2)\nSVMIndividual-discriminators -0.4545* -0.6294* -0.6716* -0.5455* -0.6783* -0.6823*\nLSTMmeta-discriminator -0.5455* -0.7552* -0.7699* -0.6364* -0.8042* -0.7829*\nCNNmeta-discriminator -0.6363* -0.8112* -0.8616* -0.7273* -0.8741* -0.8766*\nCNN & LSTMmeta-discriminator -0.6060* -0.7902* -0.8392* -0.6970* -0.8462* -0.8507*\nSVMmeta-discriminator -0.4545* -0.6573* -0.7207* -0.5455* -0.6993* -0.7405\nRFmeta-discriminator -0.5455* -0.7273* -0.7994* -0.6364* -0.7832* -0.8075*\nNBmeta-discriminator -0.6364* -0.8112* -0.9290* -0.7273* -0.8741* -0.9388*\nXGBoostmeta-discriminator -0.5455* -0.7413* -0.7764* -0.6364* -0.8042* -0.7878*\nBLEUevaluator 0.7576* 0.8601* 0.8974* 0.6666* 0.8182* 0.9060*\nROUGEevaluator 0.6060* 0.7692* 0.8054* 0.5758* 0.7483* 0.8073*\nMETEORevaluator 0.5758* 0.7762* 0.8225* 0.5455* 0.7622* 0.8231*\nCIDErevaluator 0.5455* 0.7413* 0.8117* 0.4545* 0.6643* 0.8203*\nTable 5: Kendall tau-b, Spearman and Pearson correlation coefﬁcients between human evaluators H1, H2, and\ndiscriminative evaluators and word-overlap evaluators (* denotes statistical signiﬁcant result with p≤0.05).\nGenerative Text ModelSelf-BLEULexical diversity\nWord LSTM temp 1.0 0.1886 0.6467\nWord LSTM temp 0.7 0.4804 0.2932\nWord LSTM temp 0.5 0.6960 0.1347\nScheduled Sampling 0.1233 0.7652\nGoogle LM 0.1706 0.7745\nAttention Attribute to Sequence0.5021 0.2939\nContexts to Sequences 0.8950 0.0032\nGated Contexts to Sequences0.7330 0.1129\nMLE SeqGAN 0.1206 0.7622\nSeqGAN 0.1370 0.7330\nRankGAN 0.1195 0.7519\nLeakGAN 0.1775 0.7541\nTable 6: Self-BLEU diversity scores per generator (the\nlower the more diverse), and lexical diversity scores\n(the higher the more diverse). There is high correlation\nbetween the two metrics with respect to the rankings of\nthe generative text models.\nSelf-BLEU Kendall tau-bSpearmanPearsonH1evaluator -0.8788* -0.9301*-0.8920*H2evaluator -0.7879* -0.8881*-0.9001*LSTMmeta-discriminator 0.6667* 0.8252*0.7953*CNNmeta-discriminator 0.7576* 0.8811*0.8740*CNN & LSTMmeta-discriminator0.7273* 0.8601*0.8622*SVMmeta-discriminator 0.5758* 0.7413*0.8518*RFmeta-discriminator 0.6667* 0.8112*0.8944*NBmeta-discriminator 0.7576* 0.8811*0.9569*XGBoostmeta-discriminator0.6667* 0.8252*0.8693*BLEUevaluator -0.8788 -0.9301*-0.9880*ROUGEevaluator -0.7273* -0.8392*-0.9299*METEORevaluator -0.6967* -0.8462*-0.8955*CIDErevaluator -0.5455* -0.7413*-0.7987*\nTable 7: Kendall tau-b, Spearman and Pearson correla-\ntion coefﬁcients between Self-BLEU diversity rankings\nand the three evaluation methods - human evaluators\nH1, H2, discriminative evaluators and word-overlap\nbased evaluators (* denotes statistical signiﬁcant result\nwith p≤0.05). Meta-discriminators have been trained\non D-train, D-valid sets and tested on the annotated\nD-test set with ground-truth test labels.\nBLEU G-train Kendall tau-bSpearmanPearsonH1evaluator 0.7176* 0.8511*0.9111*H2evaluator 0.6260* 0.8091*0.9209*LSTMmeta-discriminator -0.5649* -0.7461*-0.7091*CNNmeta-discriminator -0.6565 -0.7951*-0.8213*CNN & LSTMmeta-discriminator-0.6260* -0.7811*-0.7951*SVMmeta-discriminator -0.4428* -0.6130*-0.7442*RFmeta-discriminator -0.5038* -0.6340*-0.7864*NBmeta-discriminator -0.6260* -0.7601*-0.9164*XGBoostmeta-discriminator-0.5649* -0.6550*-0.7586*BLEUevaluator 0.9619* 0.9912*0.9936*ROUGEevaluator 0.5954* 0.7496*0.8717*METEORevaluator 0.6260* 0.7636*0.8477*CIDErevaluator 0.6565* 0.8371*0.8318*\nTable 8: Kendall tau-b, Spearman and Pearson correla-\ntion coefﬁcients between BLEU G-train rankings and\nthe three evaluation methods - human evaluators H1,\nH2, discriminative evaluators and word-overlap based\nevaluators (* denotes statistical signiﬁcant result with\np ≤0.05). Meta-discriminators have been trained on\nD-train, D-valid sets and tested on the annotated D-\ntest set with ground-truth test labels.\nFigure 11: Accuracy of deep (LSTM) and shallow\n(SVM) meta-discriminators when tested on the anno-\ntated subset ofD-test, with majority votes as ground-\ntruth. The lower the better.\nrankings of the generators are signiﬁcantly differ-\nent from Table 3 and Table 4 (which used natural\nlabels as ground-truth). We note that the scores\nand rankings are more inline with the human eval-\nuators. To conﬁrm the intuition, we calculate the\ncorrelations between the meta-discriminators and\nthe human evaluators using the annotated subset\nonly. Replacing the natural ground-truth with hu-\nman annotated labels, the meta-discriminators be-\ncome positively correlated with human evaluators\n(Figure 6), although BLEU still appears to be the\nbest evaluator.\nThese results indicate that when the “ground-\ntruth” used by an automated Turing test is question-\nable, the decisions of the evaluators may be biased.\nDiscriminative evaluators suffer the most from the\nbias, as they were directly trained using the imper-\nfect ground-truth. Text-overlap evaluators are more\nrobust, as they only take the most relevant parts of\nthe test set as references (more likely to be high\nquality).\nOur results also suggest that when adversarial\ntraining is used, the selection of training examples\nmust be done with caution. If the “ground-truth” is\nhijacked by low quality or “fake” examples, mod-\nels trained by GAN may be signiﬁcantly biased.\nThis ﬁnding is related to the recent literature of\nthe robustness and security of machine learning\nmodels.\nB.4 Role of Diversity\nWe also assess the role diversity plays in the rank-\nings of the generators. To this end, we measure\nlexical diversity (Bache et al., 2013) of the samples\nproduced by each generator as the ratio of unique\ntokens to the total number of tokens. We com-\npute in turn lexical diversity for unigrams, bigrams\nand trigrams, and observe that the generators that\nproduce the least diverse samples are easily distin-\nguished by the meta-discriminators, while they con-\nfuse human evaluators the most. Alternatively, sam-\nples produced by the most diverse generators are\nhardest to distinguish by the meta-discriminators,\nwhile human evaluators present higher accuracy\nat classifying them. As reported in (Kannan and\nVinyals, 2017), the lack of lexical richness can be a\nweakness of the generators, making them easily de-\ntected by a machine learning classiﬁer. Meanwhile,\na discriminator’s preference for rarer language does\nnot necessarily mean it is favouring higher quality\nreviews.\nIn addition to lexical diversity, Self-BLEU (Zhu\net al., 2018) is an interesting measurement of the\ndiversity of a set of text (average BLEU score of\neach document using the same collection as ref-\nerence, therefore the lower the more diverse). In\nFigure 7 we present Self-BLEU scores for each\ngenerator, applied to their generated text in D-test\nfake. We also compute the correlation coefﬁcients\nbetween the rankings of generators by Self-BLEU\nand the rankings by the evaluators (please see Fig-\nure 12). Results obtained indicate that Self-BLEU\npresents negative correlation with human evalu-\nators and word-overlap evaluators, and positive\ncorrelation with discriminative evaluators. This\nresult conﬁrms the ﬁndings in literature (Kannan\nand Vinyals, 2017) that discriminators in adversar-\nial evaluation are capturing known limitations of\nthe generative models such as lack of diversity.\nFigure 12: Kendall τ-b correlation coefﬁcients between\nBLEU G-train and Self-BLEU rankings, and the three\nevaluation methods - human evaluators H1, H2, dis-\ncriminative evaluators and word-overlap based evalua-\ntors (* denotes p ≤0.05). Meta-discriminators have\nbeen trained on D-train, D-valid sets and tested on the\nannotated D-test set with ground-truth test labels.\nFollowing this insight, an important question to\nanswer is to what extent the generators are simply\nmemorizing the training set G-train. To this end,\nwe assess the degree of n-gram overlap between\nthe generated reviews and the training reviews us-\ning the BLEU evaluator. In Table 9 we present the\naverage BLEU scores of generated reviews using\ntheir nearest neighbors inG-train as references. We\nobserve that generally the generators do not mem-\norize the training set, and GAN models generate\nreviews that have fewer overlap with G-train. In\nFigure 12 we include the correlation between the\ndivergence from training and the ratings by eval-\nuators in the study. BLEU w.r.t. G-train presents\nhighly positive correlation with BLEU w.r.t. D-test\nreal, and it is also positively correlated with the\nGenerative Text ModelBLEU G-Train\nWord LSTM temp 1.0 0.2701\nWord LSTM temp 0.7 0.4998\nWord LSTM temp 0.5 0.6294\nScheduled Sampling 0.1707\nGoogle LM 0.0475\nAttention Attribute to Sequence0.5122\nContexts to Sequences 0.7542\nGated Contexts to Sequences0.6240\nMLE SeqGAN 0.1707\nSeqGAN 0.1751\nRankGAN 0.1525\nLeakGAN 0.1871\nTable 9: BLEU results when evaluating the generated\nreviews using G-train as the reference corpus (a lower\nscore indicates less n-grams in common between the\ntraining set G-train and the generated text). GAN mod-\nels present low similarity with the training set.\nhuman evaluators H1 and H2.\nThe effects of diversity is perhaps not hard to ex-\nplain. At the particular task of distinguishing fake\nreviews from real, all decisions are made on indi-\nvidual reviews. And because a human judge was\nnot exposed to many fake reviews generated by the\nsame generator, whether or not a fake review is suf-\nﬁciently different from the other generated reviews\nis not a major factor for their decision. Instead,\nthe major factor is whether the generated review\nlooks similar to the reviews they have seen in real-\nity. Instead, a discriminative evaluator makes the\ndecision after seeing many positive and negative\nexamples, and a fake review that can fool an adver-\nsarial classiﬁer has to be sufﬁciently different from\nall other fake reviews it has encountered (therefore\ndiversity of a generator is a major indicator of its\nability to pass an adversarial judge).",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7436789870262146
    },
    {
      "name": "Discriminative model",
      "score": 0.6606283783912659
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6227463483810425
    },
    {
      "name": "Natural language generation",
      "score": 0.6213228106498718
    },
    {
      "name": "Metric (unit)",
      "score": 0.6167090535163879
    },
    {
      "name": "Natural language processing",
      "score": 0.6010578870773315
    },
    {
      "name": "Ranking (information retrieval)",
      "score": 0.5585612058639526
    },
    {
      "name": "Adversarial system",
      "score": 0.5443326234817505
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5241824984550476
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.5229528546333313
    },
    {
      "name": "Natural language",
      "score": 0.49642688035964966
    },
    {
      "name": "Lexical diversity",
      "score": 0.45732298493385315
    },
    {
      "name": "Machine learning",
      "score": 0.3866159915924072
    },
    {
      "name": "Information retrieval",
      "score": 0.3770116865634918
    },
    {
      "name": "Linguistics",
      "score": 0.1546573042869568
    },
    {
      "name": "Vocabulary",
      "score": 0.11244028806686401
    },
    {
      "name": "Operations management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ]
}