{
  "title": "UniGen: A Unified Generative Framework for Retrieval and Question Answering with Large Language Models",
  "url": "https://openalex.org/W4393147971",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2117504741",
      "name": "Xiaoxi Li",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2123189166",
      "name": "Yujia Zhou",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2330149811",
      "name": "Zhicheng Dou",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2117504741",
      "name": "Xiaoxi Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2123189166",
      "name": "Yujia Zhou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2330149811",
      "name": "Zhicheng Dou",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4224438163",
    "https://openalex.org/W6779857854",
    "https://openalex.org/W4378715759",
    "https://openalex.org/W3183624231",
    "https://openalex.org/W2558203065",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W6852172913",
    "https://openalex.org/W3214546336",
    "https://openalex.org/W6851611710",
    "https://openalex.org/W6810139940",
    "https://openalex.org/W6850138286",
    "https://openalex.org/W4387848863",
    "https://openalex.org/W4389520393",
    "https://openalex.org/W4221146749",
    "https://openalex.org/W4292761107",
    "https://openalex.org/W4283315779",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W2951534261",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4380136635",
    "https://openalex.org/W3118668786",
    "https://openalex.org/W4389520743",
    "https://openalex.org/W4226082499",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W4364383092",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4385569809",
    "https://openalex.org/W4385571319",
    "https://openalex.org/W4385567756",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W3101007570",
    "https://openalex.org/W3176182290",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4301243929",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4287122359",
    "https://openalex.org/W4388778348",
    "https://openalex.org/W4221166196",
    "https://openalex.org/W4302305884",
    "https://openalex.org/W4252076394",
    "https://openalex.org/W3156789018",
    "https://openalex.org/W4318719006",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4297899309",
    "https://openalex.org/W4378468487",
    "https://openalex.org/W4320465836",
    "https://openalex.org/W3034671305"
  ],
  "abstract": "Generative information retrieval, encompassing two major tasks of Generative Document Retrieval (GDR) and Grounded Answer Generation (GAR), has gained significant attention in natural language processing. Existing methods for GDR and GAR rely on separate retrieval and reader modules, which hinder simultaneous optimization. To overcome this, we present UniGen, a Unified Generative framework for retrieval and question answering that integrates both tasks into a single generative model leveraging the capabilities of large language models. UniGen employs a shared encoder and two distinct decoders for generative retrieval and question answering. To facilitate the learning of both tasks, we introduce connectors, generated by large language models, to bridge the gaps between query inputs and generation targets, as well as between document identifiers and answers. Furthermore, we propose an iterative enhancement strategy that leverages generated answers and retrieved documents to iteratively improve both tasks. Through extensive experiments on the MS MARCO and NQ datasets, we demonstrate the effectiveness of UniGen, showcasing its superior performance in both retrieval and question answering tasks.",
  "full_text": "UniGen: A Unified Generative Framework for Retrieval and Question\nAnswering with Large Language Models\nXiaoxi Li*, Yujia Zhou*, Zhicheng Dou\nGaoling School of Artificial Intelligence, Renmin University of China, Beijing, China\nEngineering Research Center of Next-Generation Intelligent Search and Recommendation, Ministry of Education\n{xiaoxi_li, zhouyujia, dou}@ruc.edu.cn\nAbstract\nGenerative information retrieval, encompassing two ma-\njor tasks of Generative Document Retrieval (GDR) and\nGrounded Answer Generation (GAR), has gained significant\nattention in the area of information retrieval and natural lan-\nguage processing. Existing methods for GDR and GAR rely\non separate retrieval and reader modules, which hinder simul-\ntaneous optimization. To overcome this, we present UniGen,\na Unified Generative framework for retrieval and question\nanswering that integrates both tasks into a single generative\nmodel leveraging the capabilities of large language models.\nUniGen employs a shared encoder and two distinct decoders\nfor generative retrieval and question answering. To facilitate\nthe learning of both tasks, we introduce connectors, generated\nby large language models, to bridge the gaps between query\ninputs and generation targets, as well as between document\nidentifiers and answers. Furthermore, we propose an itera-\ntive enhancement strategy that leverages generated answers\nand retrieved documents to iteratively improve both tasks.\nThrough extensive experiments on the MS MARCO and NQ\ndatasets, we demonstrate the effectiveness of UniGen, show-\ncasing its superior performance in both the retrieval and the\nquestion answering tasks.\nIntroduction\nGenerative information retrieval has been a focal point of re-\nsearch in recent years, concerning the generation of relevant\ninformation from a vast corpus, such as Wikipedia, in re-\nsponse to a specific query. This field primarily encompasses\ntwo tasks: Generative Document Retrieval (GDR) (Metzler\net al. 2021; Tay et al. 2022; Zhuang et al. 2022; Wang et al.\n2022) and Grounded Answer Generation (GAR) (Guu et al.\n2020; Lewis et al. 2020; Izacard and Grave 2020). GDR re-\ntrieves a ranked list of documents in response to a query\nthrough an encoder-decoder architecture that directly gener-\nates document identifiers (docids). Concurrently, GAR gen-\nerates an answer that matches a specific segment of ground-\ning information, in response to the user’s query.\nThe generative information retrieval landscape has been\ndramatically reshaped by recent advances in GDR and GAR.\nFor the GDR task, the seminal work of (Metzler et al. 2021)\n*These authors contributed equally.\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nAnswer\nUniGenQuestion Q-Connector\nOutput \nspace\nDocid \nspace\nAnswer \nspace\nD-Connector\nInput \nspace\nDocuments\nRetrieval \nTask\nQA\nTask\nFigure 1: Illustration of the unified generative framework,\nwhich combines retrieval and question answering tasks\nthrough LLM-generated connectors.\nhas been instrumental, where document retrieval is accom-\nplished by directly generating document identifiers via end-\nto-end training generation models. Subsequent research has\nbuilt upon this work, notably enhancing the indexing strat-\negy (Tay et al. 2022; Zhuang et al. 2022; Wang et al. 2022),\nidentifier design (Tay et al. 2022; Bevilacqua et al. 2022;\nZhou et al. 2022b; Sun et al. 2023; Zhang et al. 2023; Tang\net al. 2023), and dynamic corpora (Mehta et al. 2022). For\nthe GAR task, prevalent models such as REALM (Guu et al.\n2020), RAG (Lewis et al. 2020), FID (Izacard and Grave\n2020), EMDR2 (Singh et al. 2021) and Atlas (Izacard et al.\n2022) have employed dense retrieval models to retrieve rel-\nevant documents, which are then synthesized by generative\nmodels to yield the final answer.\nDespite the advancements, optimizing generative retrieval\nand question answering (QA) tasks individually requires\nseparate training techniques, distinct training data, and ad-\nditional time costs. To address these challenges, we propose\nto utilize a single model to optimize both tasks simulta-\nneously. Noting that both tasks could employ an encoder-\ndecoder structure and possess two essential characteristics:\n(1) the need for a profound comprehension of the semantic\nsignificance behind the query input, and (2) the necessity to\ncomprehend and memorize knowledge in the corpus. Draw-\ning inspiration from these shared characteristics, we propose\nusing a unified framework that is capable of jointly generat-\ning docids and answers, facilitating knowledge sharing, and\nultimately reinforcing performance on downstream tasks.\nMore specifically, we propose UniGen, a Unified\nGenerative framework that enhances retrieval and QA tasks\nconcurrently. UniGen employs a shared encoder and two\ndistinct decoders: the retrieval decoder and the QA decoder.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n8688\nBy leveraging a shared encoder, we can improve the model’s\ncomprehension of the input through shared knowledge from\nboth tasks, resulting in enhanced overall performance. As\nshown in Figure 1, the retrieval decoder generates docids for\nthe retrieval task, while the QA decoder generates answers\nfor the QA task. By utilizing such a shared encoder and sep-\narate decoders, our proposed UniGen framework enhances\nthe comprehension of model input resulting in improved per-\nformance of both tasks.\nNevertheless, there are two notable gaps in such a unified\ngenerative IR framework that hinder the training process,\nwhich include: (1) the input-output gap: The input queries\nare often brief and lack contextual semantics, leading to a\ndisparity between the query inputs and the generation tar-\ngets. (2) The docid-answer gap: Conventional docids are\ntypically unreadable sequences, posing a challenge to learn\njointly with answer generation and thus creating a gap be-\ntween docids and answers. To address these gaps, we intro-\nduce the concept of Connectors to serve as bridges. Specif-\nically, we introduce the Q-Connector and the D-Connector,\nwhich enrich the query’s context and refine the document’s\ncontent, thereby bridging the input-output gap and the docid-\nanswer gap, respectively. Considering generating these con-\nnectors is a highly knowledge-intensive task, we propose\nleveraging large language models (LLMs), which have re-\ncently gained significant attention (Touvron et al. 2023a;\nChiang et al. 2023; Chowdhery et al. 2022), to effectively\naccomplish this task. Figure 1 illustrates this approach.\nFurthermore, previous works (Lewis et al. 2020; Mao\net al. 2020) have shown that the integration of the retrieval\nand QA tasks allows for a mutually beneficial relationship.\nSpecifically, the documents acquired through the retrieval\ndecoder serve as supplementary knowledge to enhance an-\nswer generation. Simultaneously, the answers generated by\nthe QA decoder can contribute to more effective document\nretrieval. Building upon this insight, we further propose an\nIterative Enhancement Strategy to optimize the perfor-\nmance of both retrieval and QA tasks at the data level. This\nstrategy entails utilizing the retrieved documents and gener-\nated answers from the previous iteration as inputs for sub-\nsequent model iterations. We continuously refine the model\ninput by employing this iterative process, resulting in supe-\nrior performance in both tasks.\nA series of experiments conducted on the public datasets\nMS MARCO Question Answering (Nguyen et al. 2016)\nand Natural Questions (NQ) (Kwiatkowski et al. 2019) val-\nidate the effectiveness of our proposed methods. The results\ndemonstrate significant improvements in both retrieval and\nQA performance compared to baseline models.\nThe paper makes the following key contributions:\n• Unified Generative Framework: We develop a gener-\native framework that incorporates a multi-decoder struc-\nture to simultaneously learn retrieval and QA tasks.\n• LLM-enhanced Connectors: We introduce Q-\nconnector and D-connector generated by LLMs,\nwhich establish semantic connections in the input-output\nand docid-answer spaces, enhancing query semantics\nand refining document content, respectively.\n• Iterative Enhancement Strategy: We propose an iter-\native approach to improve both generative retrieval and\nQA tasks by leveraging the generated answers and the\nretrieved documents.\nRelated Work\nGenerative Retrieval. Generative retrieval is an innova-\ntive approach to information retrieval that leverages the pa-\nrameters of pre-trained language models as differentiable in-\ndices (Tay et al. 2022), enabling the direct generation of rel-\nevant document identifiers. Recent research in this field pri-\nmarily focuses on document representation and model train-\ning. For document representation, existing studies draw in-\nspiration from DSI (Tay et al. 2022) and explore various\napproaches such as atomic identifiers, text fragments, and\nsemantic clusters. Among these, text fragments stand out\ndue to their ease of use and interpretability. For instance,\nUltron (Zhou et al. 2022b) utilizes the document URL and\ntitle as representations, while SEAL (Bevilacqua et al. 2022)\nconsiders all n-grams within a file as potential identifiers.\nMINDER (Li et al. 2023) takes a multi-view approach, in-\ncorporating synthetic identifiers, titles, and substrings. For\nmodel training, a simple yet effective method involves using\ngenerated pseudo-query data to train the model to learn the\nmapping between pseudo-queries and their corresponding\ndocids (Zhuang et al. 2022; Wang et al. 2022; Zhou, Dou,\nand Wen 2023; Wang et al. 2023; Zhou et al. 2022a). Subse-\nquently, labeled query-docid data is employed to further re-\nfine the model. Another notable contribution is TOME (Ren\net al. 2023), which proposes a two-stage model structure that\nfirst generates a paragraph relevant to the query and then\ngenerates the URL associated with the paragraph.\nOpen-Domain Question Answering. Open-domain\nquestion answering refers to providing solutions to queries\nwithout depending on contextual information. It involves\ntwo primary forms: closed-book and open-book. In closed-\nbook QA, models cannot access external knowledge banks\nand must internalize all necessary information within\ntheir parameters. Earlier works such as T5 (Raffel et al.\n2020), BART (Lewis et al. 2019), and GPT (Brown et al.\n2020) attempt closed-book QA by pre-training on massive\ntext corpora, but still struggle with knowledge-intensive\nquestions. In open-book QA, models can utilize knowledge\nbases like Wikipedia during answer generation. The typical\nprocess involves two main components: a retrieval module\nthat searches knowledge bases for relevant contexts, and\na reading module that analyzes the retrieved information\nto formulate a solution. For example, popular models like\nDPR (Karpukhin et al. 2020), RAG (Lewis et al. 2020),\nand EMDR 2 (Singh et al. 2021) employ a dual-encoder\ndense retriever built upon BERT (Devlin et al. 2018), along\nwith another BERT-based model for answer extraction\nor a T5/BART-based model for answer generation. Large\nlanguage models (LLMs) have recently shown promising\nresults in open-domain QA (Yu et al. 2022; Sun et al. 2022;\nRam et al. 2023; Shi et al. 2023; Borgeaud et al. 2022; Liu\net al. 2023). For instance, GenRead (Yu et al. 2022) prompts\nan LLM to generate context documents instead of using a\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n8689\nDocuments\noffline\nLLM\n(Docids)\n(a) Traditional Separate Frameworks for Retrieval and QA. (b) Unified Generative Retrieval and QA.\nQuestion\nAnswer\nLLM\nQ-ConnectorD-Connector\nConstrained \nDecode\nTop-k \nDocuments \nShared Encoder\nRetrieval \nDecoder\nQA \nDecoder\nIterative Retrieval and QA \nEnhancement Strategy\ni-th\nIteration\nAnswer\n…\n…\nDocuments\noffline\nQuestion\nConstrained \nDecode\nTop-k \nDocuments \nDocids\nGenerative Retrieval Question \nAnswering\nEncoder\nDecoder\nEncoder\nDecoder\nLarge-\nScale \nIndex\nMax Inner Product\nDocuments\noffline\nQuestion\nTop-k \nDocuments \nDense Retrieval\nQuestion \nEncoder Top-k \nDocuments \nQuestionDocument\nEncoder\nFigure 2: The comparison between traditional separate frameworks and our unified framework for retrieval and QA. (a) Tradi-\ntional approaches typically employ separate and independently designed structures for retrieval and QA tasks. (b) Our proposed\nframework incorporates a multi-decoder structure to simultaneously achieve retrieval and QA tasks in a generative manner. To\neffectively enhance the performance of both tasks, we introduce LLM-generated Q-connector and D-connector, along with an\niterative enhancement strategy.\nretriever. Combining generation and retrieval techniques\ncan further improve performance. RECITE (Sun et al. 2022)\nsuggests asking the LLM to generate support paragraphs\ncontaining the answer, which are then used as an additional\nprompt along with the question.\nInspired by these methods, we present a unified approach\nthat integrates both the retrieval and QA tasks into genera-\ntive manners, optimizes both tasks through a single gener-\native model, and leverages the previous results for iterative\ngeneration, enhancing the model’s overall performance.\nMethodology\nIn this section, we present a complete overview of our pro-\nposed framework, which aims to tackle generative retrieval\nand QA tasks. We will begin by defining these tasks and then\ndive into the structure and training methodologies employed\nin our unified framework.\nTask Formulation\nConsider a document d in a document corpus and let d′ de-\nnotes the pre-built docid of document d. For generative re-\ntrieval task, given a query q, we obtain the relevance R be-\ntween q and each document d by\nR(q, d) =fretr(d′|q; θ, ϕ) =\nYT\ni=1\nfretr(d′\ni|d′\n<i, q; θ, ϕ),\n(1)\nwhere T is the length of the target document identifier d′, d′\ni\nis the ith token of d′, fretr is the generative retrieval model\ncomprising an encoder with parameters θ and a retrieval de-\ncoder with parameters ϕ. The model is trained to maximize\nthe likelihood of generating the target document identifier in\nEq. (1). Teacher forcing is used during training to optimize\nthe following cross-entropy loss:\nLretr = −\nXT\ni=1\nlogfretr(di|d′\n<i, q; θ, ϕ). (2)\nSimilarly, for QA task, given a query q, the probability A\nof generating answer a is obtained by\nA(a|q) =fqa(a|q; θ, µ) =\nYT′\ni=1\nfqa(ai|a<i, q; θ, µ), (3)\nwhere T′ is the length of answer a, ai is the ith token of an-\nswer a, fqa is the generative QA model with a shared encoder\nwith parameters θ and a distinct QA decoder with parame-\nters µ. Similarly, the optimization of the parameters θ and µ\nis achieved through the standard seq-to-seq objective, which\nconsists of maximizing the likelihood of the target sequence\nin Eq. (3) by employing teacher forcing. The QA loss func-\ntion can be represented by\nLqa = −\nXT′\ni=1\nlogfqa(ai|a<i, q; θ, µ). (4)\nUniGen: Unified Generative Retrieval and QA\nThis section discusses the details of the UniGen framework\nproposed in the paper, including the overall model structure,\nLLM-based connectors generation, joint learning method,\nand iterative enhancement strategy.\nModel Architecture Our proposed UniGen framework in-\ntroduces a multi-decoder structure to simultaneously tackle\nthe retrieval and the QA tasks. This is different from conven-\ntional methods that depend on separate and independently\ndesigned architectures for each task. Figure 2 demonstrates\nthe contrast between our UniGen framework and the tradi-\ntional methods, where dense retrieval relies on large-scale\ndocument indices, and generative retrieval and QA methods\nare usually distinct modules.\nThe architecture of our model comprises an encoder and\ntwo separate decoder heads: a retrieval decoder and a QA\ndecoder. The encoder takes the enhanced query generated\nby the LLM as input, denoted by Q-Connector. The retrieval\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n8690\nRelevant Document:\nPirates of the Caribbean:\nDead Man's Chest is a 2006\nAmerican fantasy film, the\nsecond installment of the\n\"\"Pirates of the Caribbean\"\"\nfilm series and the sequel…\nto acquire the compass of\nCaptain Jack Sparrow\n(JohnnyDepp) in a bid to find\nthe Dead Man’s.\nQ-Connector:\nQuestion: Who plays captain\njack sparrow in pirates of\nthe Caribbean? Context:\nCaptain Jack Sparrow, the\niconic pirate characterin the\nPiratesof the Caribbeanfilm\nseries, is portrayed by actor\nJohnnyDepp.\nD-Connector (DocID):\n\"Pirates of the Caribbean:\nDead Man's Chest\"is a 2006\nAmerican film. It features\nCaptain Jack Sparrow\n(Johnny Depp) and Will\nTurner on a quest to find the\nDead Man's Chest.\nEasier to \nLearn Jointly\nQuery Input:\nWho plays captainjack sparrow\nin pirates ofthe Caribbean?\nWrite a \nContext\nHard to \nLearn JointlyUniGen\nUniGen\n1,5,3,2,4,4\nTraditional DocIDs\nSummarize \nthe Key \nInformation\nFigure 3: An example of generating LLM-based connectors\nfrom the query side and document side, with the labeled an-\nswer highlighted in the green box.\ndecoder employs constrained beam search within a prefix\ntree to generate a ranked list of document identifiers. These\nidentifiers are represented by the connector generated by\nthe LLM from the document side, represented by the D-\nConnector. At the same time, the QA decoder generates the\nanswer text. By using a joint architecture for retrieval and\nQA, our model optimizes both tasks simultaneously, result-\ning in enhanced overall system performance.\nLLM-based Connectors Generation Learning to gener-\nate docids and answers concurrently based on query inputs\nis a challenging task. Query inputs are typically short and\nlack context, while documents are long and contain redun-\ndant information. Directly mapping queries to documents\nand answers is difficult. Additionally, existing docid repre-\nsentations are often meaningless sequences, hindering the\njoint learning of generative retrieval and QA tasks. To ad-\ndress these issues, we propose using LLM to generate Q-\nConnectors and D-Connectors on the query and document\nsides. These connectors serve as bridges between query in-\nputs, documents, and answer outputs. Figure 3 provides an\nexample of the LLM-generated connectors.\nFirstly, for D-Connector generation, the LLM takes the\nprompt of “Summarize the key information\nof the following document in about {m}\nwords.\\n Document:{d}\" along with a document d\nas input and outputs a summary of the document called\na D-Connector dc. The D-Connector serves as a docid of\nthe document that captures its essential information, which\ngreatly reduces the difficulty of the model’s memory for\nlong documents. Additionally, since the answer is typically\na short phrase or sentence, it is easier to jointly learn with\nthe answer generation task using the unified framework\nproposed in this paper.\nSecondly, for Q-Connector generation, the LLM\ntakes the prompt of “Write a context to\nthe following question in about {n}\nwords.\\n Question:{q}\" and a question q as input\nand generates a Q-Connector qc. The Q-Connector provides\na contextual representation of the query, which aids in\ngenerating relevant docids and accurate answers. The Q-\nConnector enables the model to better understand the query\nand its related context, thereby enabling it to effectively\nmap to relevant docids and provide contextual knowledge\nfor the QA task. This approach does not rely on external\ncorpora and can achieve impressive results for QA.\nJoint Learning of Retrieval and QA Taking the Q-\nConnector qc as the model input, we establish the relevance\nbetween the query q and each document d in the set D and\nthe probability of generating an answera by fretr(dc|qc; θ, ϕ)\nand fqa(a|qc; θ, µ), respectively. Here,θ, ϕ, and µ represent\nthe parameters of the model encoder, retrieval decoder, and\nQA decoder, respectively. We can modify the retrieval and\nQA losses from Eq. (2) and Eq. (4) as follows:\nL\n′\nretr = −\nX\ni\nlogfretr(dci|dc<i, qc; θ, ϕ), (5)\nL\n′\nqa = −\nX\ni\nlogfqa(ai|a<i, qc; θ, µ), (6)\nwhere dci and ai denote the ith token in the generation ofdc\nand a, respectively.\nTo equip the model with initial generative capabilities for\nboth tasks, we begin by training it on synthetic training data.\nPrevious research has demonstrated that using synthetic data\ncan enhance the effectiveness of generative retrieval and\nquestion answering (Zhuang et al. 2022; Puri et al. 2020).\nHence, we present a two-stage training approach, including\na pre-training stage and a fine-tuning stage:\nIn the pre-training stage, for each document d, we\nemploy the DocT5query (Nogueira, Lin, and Epistemic\n2019) model to generate K pseudo queries qk, where k ∈\n{1, ..., K}. Next, we feed each pseudo query qk and its\ncorresponding document d into the large language model\nLLaMA-13B-Chat (Touvron et al. 2023b) to generate label\nanswers ak. To simulate the Q-Connector qc generated by\nthe LLM, we concatenate qk and d as the input of our gener-\native model, denotingqk+d. This approach allows us to gen-\nerate K pairs of retrieval and QA training data <qk + d, dc>\nand <qk + d, ak> for each document d.\nIn the fine-tuning stage, we proceed by training the\nmodel based on labeled <q c, dc> and <q c, a> data, where\nqc is generated by LLM from query q.\nTo optimize the model for both generative retrieval and\nQA tasks, our UniGen framework employs both the gener-\native retrieval loss and the QA loss, denoted by Eq. (5) and\nEq. (6), respectively. To jointly optimize the encoder param-\neters θ, retrieval decoder parameters ϕ, and QA decoder pa-\nrameters µ, we combine these two losses into a single overall\nloss:\nL = λL\n′\nretr + (1− λ)L\n′\nqa, (7)\nwhere λ is the regularization weight. By following this train-\ning process and optimizing the loss function as described,\nthe model can effectively learn both retrieval and QA tasks\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n8691\nsimultaneously. We refer to this foundational model as\nUniGen-Base.\nIterative Enhancement Strategy To further enhance the\nretrieval and QA performance of the model at the data level,\nwe propose an iterative enhancement strategy. The objective\nis to utilize the retrieved documents and generated answers\nfrom the previous iteration as inputs for the next round of\nthe model, as shown in the dashed portion in Figure 2(c).\nIn each iteration, we input the top-k documents, an-\nswer, and query from the previous round into a large\nlanguage model. The aim is to generate a higher-quality\nQ-Connector, denoted as qc, and increase the likelihood\nof the model producing the correct answer and retrieving\nmore relevant documents. To accomplish this, we use\nthe following prompt: \"Given the following\npotentially relevant documents and\nthe potentially correct answer, please\nprovide the context for the question in\n{n} words. \\n Document:{d} \\n Answer:{a}\n\\n Question:{q}\". The parameter n allows us to\ncontrol the length of qc.\nThrough this iterative approach, our goal is to continu-\nously refine the model’s performance in retrieving and an-\nswering questions, ultimately improving its overall effec-\ntiveness. To strike a balance between model performance\nand efficiency, we have created an enhanced version of the\nmodel called UniGen-Iter, which incorporates two itera-\ntions on top of the UniGen-Base.\nExperimental Settings\nDatasets\nTo thoroughly evaluate the retrieval and question answering\nperformance of our proposed model, we utilize two well-\nknown datasets: MS MARCO and Natural Questions.\nMS MARCO Question Answering (Nguyen et al. 2016)\nis designed to train and test systems that can effectively\ngenerate the most probable answer given a real-world user\nquery. We use the QnA (v2.1) dataset and extract passages\nfrom the corpus that contain labeled data, resulting in a sub-\nstantial collection of approximately 100k passages and a set\nof 94,871 training query-answer-relevant document triplets.\nNatural Questions (NQ) (Kwiatkowski et al. 2019) con-\nsists of questions sampled from the Google search en-\ngine. Following the methodologies proposed by (Karpukhin\net al. 2020), we divide each Wikipedia article into non-\noverlapping chunks of 100 words. To ensure a robust evalua-\ntion, we identify passages in the corpus that contain labeled\ndata based on the training set. This meticulous process re-\nsults in a diverse collection of around 100k passages and\n38,191 training query-answer-relevant document triplets.\nBaselines\nWe choose several baseline models for the retrieval and QA\ntasks, categorized into different classes.\nFor the retrieval task, we select three classes of mod-\nels. The first class consists of Sparse Retrieval models,\nwhich include BM25 (Robertson, Zaragoza et al. 2009)\nand DocT5Query (Nogueira, Lin, and Epistemic 2019). The\nsecond class comprises Dense Retrieval models, such as\nDPR (Karpukhin et al. 2020) and ANCE (Xiong et al.\n2020). Lastly, the Generative Retrieval models class in-\ncludes DSI (Tay et al. 2022), DSI-QG (Zhuang et al. 2022),\nNCI (Wang et al. 2022), and Ultron (Zhou et al. 2022b).\nRegarding the QA task, we consider three types of base-\nline models. The first type isClosed-book Generation mod-\nels, represented by T5 (Raffel et al. 2020) and BART (Lewis\net al. 2019). The second type isRetrieval-augmented Gen-\neration models, which incorporate RAG (Lewis et al. 2020)\nand a combination model that utilizes DPR, NCI, Ultron,\nand Fusions-in-Decoder (Izacard and Grave 2020). The last\ntype is LLM-based Generation models, where we directly\nevaluate the QA performance of gpt-3.5-turbo-0613 and\nLLaMA2-13B-Chat (Touvron et al. 2023b).\nEvaluation Metrics\nRetrieval models are evaluated using MRR and recall, which\nmeasure the average rank of the first relevant document and\nthe proportion of relevant documents retrieved, respectively.\nFor QA evaluation, we use BLEU-1 (B-1) and ROUGE-\nL (R-L) metrics on MS MARCO. B-1 measures uni-gram\noverlap, while R-L measures the longest common sub-\nsequence overlap. On the NQ dataset, we utilize the Exact\nMatch (EM) and F1 score, which measure exact matches and\nthe harmonic mean of precision and recall, respectively.\nImplementation Details\nIn our experiments, we utilize the pre-trained T5-base en-\ncoder as the shared encoder for our model. Both the retrieval\ndecoder and QA decoder also employ the T5-base decoder\nwith pre-trained parameters from HuggingFace Transform-\ners (Wolf et al. 2019). We incorporate the gpt-3.5-turbo-\n0613 API as the LLM in our system. To generate training\ndata, we create 10 pseudo-queries and 10 pseudo-answers\nfor each document. During training, we set the value of λ\nto 0.6. Our training process involves a batch size of 128, a\nlearning rate of 5e-4, and 2k learning rate warm-up steps.\nDuring inference, we employ constrained beam search for\ngenerative retrieval decoding and greedy search for QA de-\ncoding. Due to memory and time constraints, we limit the\nbeam size to a maximum of 10. The experiments are con-\nducted on 4 NVIDIA RTX 3090 GPUs.\nExperimental Results\nIn this section, we present the results of our experiments to\nevaluate the performance of the proposed unified model in\nboth retrieval and QA tasks.\nPassage Retrieval Performances\nWe evaluate the retrieval performance, and the overall re-\nsults are summarized in Table 1.\n(1) Our proposed UniGen-Base model outperforms exist-\ning baseline models in terms of most metrics. Specifically,\nfor the MRR@10 metric, UniGen-Base outperformed the\nbest baseline models on the MS MARCO and NQ datasets\nby 1.81% and 0.83%, respectively. This can be attributed\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n8692\nModel # Params\nMS MARCO Natural Questions (NQ)\nR@1 R@5 R@10 MRR@10 R@1 R@5 R@10 MRR@10\nSparse Retrieval\nBM25 - 25.70 53.28 65.85 37.79 45.36 72.86 81.72 57.18\nDocT5Query - 31.14 60.04 68.29 42.93 49.43 76.25 84.10 60.81\nDense Retrieval\nDPR\n220M 36.96 70.92 80.18 50.69 60.25 82.60 86.97 69.90\nANCE 220M 37.70 72.34 81.52 51.70 61.45 84.25 88.71 71.30\nGenerative Retrieval\nDSI-Semantic\n250M 28.84 46.22 52.94 36.60 46.70 66.34 70.79 54.73\nDSI-QG 250M 35.41 68.38 73.34 46.48 59.52 78.35 81.93 67.94\nNCI 267M 37.89 72.23 77.39 49.41 63.00 84.61 88.90 71.77\nUltron-PQ 257M 37.38 72.07 78.09 51.47 63.54 85.01 86.34 72.68\nUnified Generative Retrieval and QA (Retrieval Decode)\nUniGen-Base 367M 38.75† 72.69† 79.07 52.64 † 63.71† 86.39† 88.74 72.81 †\nUniGen-Iter 367M 42.34† 75.99† 81.85† 56.38† 64.92† 88.15† 90.01† 74.61†\nTable 1: Overall Retrieval Performance, where # Param indicates the size of model parameters. The best results among all\nexperiments are emphasized in bold, while the best results of baseline models are underlined. The symbol \"†\" signifies that our\nbasic model achieved superior results among all baselines in a statistically significant manner (t-test, p <0.05).\nto the joint learning strategy of retrieval and question-\nanswering tasks, which makes the shared encoder more ro-\nbust, alleviates overfitting, and improves the understanding\nof query inputs. In addition, the connectors generated by\nLLM on the query and document sides serve to enrich the\ncontextual semantics of queries and refine the corpus docu-\nments, thereby facilitating the model’s learning of the map-\nping relationship between queries and relevant docids.\n(2) For the data-level unification, the proposed UniGen-\nIter model achieves the best retrieval performance after two\niterations, outperforming existing generative retrieval mod-\nels, dense retrieval, and sparse retrieval models. Specifically,\nUniGen-Iter surpasses the best baseline models on the MS\nMARCO and NQ datasets by 11.76% and 2.17% in terms\nof R@1, respectively. Furthermore, as shown in the blue\nlines in Figure 4, a continuous improvement in retrieval per-\nformance can be observed in terms of MRR@10 on MS\nMARCO and NQ datasets when comparing the non-iterative\napproach (UniGen-Base) with the iterative methods for 1\nto 5 iterations. This clearly demonstrates the effectiveness\nof the proposed iterative enhancement strategy in improv-\ning retrieval performance. This is because the previously re-\ntrieved documents can provide relevant external knowledge,\nand the generated answers can also serve as references, en-\nabling LLM to generate more relevant Q-Connectors and\ncontinuously enhance retrieval performance over iterations.\nIn summary, the proposed UniGen model demonstrates\nsuperior retrieval performance compared to existing models,\nand the iterative enhancement strategy proves to be effective\nin improving retrieval performance.\nQuestion Answering Performances\nWe also assess the performance of the proposed model in the\nQA task, and the results are shown in Table 2.\n(1) Under the closed-book setting, where external cor-\npora are not accessible, the model directly generates answers\n0 1 2 3 4 5\nIteration\n52\n53\n54\n55\n56\n57MRR@10\n18\n20\n22\n24\n26\nBLEU-1\nUniGen-Retr\nUniGen-QA\n(a) MS MARCO\n0 1 2 3 4 5\nIteration\n73.0\n73.5\n74.0\n74.5\n75.0MRR@10\n45\n50\n55\n60\nEM\nUniGen-Retr\nUniGen-QA (b) Natural Questions\nFigure 4: Analysis of retrieval and QA performance with dif-\nferent iterations.\nto input questions. Comparing the small model fine-tuned\nwith labeled data and the large model without fine-tuning,\nthe proposed UniGen-Base model significantly outperforms\nexisting baseline models with statistical significance (p <\n0.05). For the MS MARCO dataset, UniGen-Base surpasses\nBART by 9.10% in terms of Bleu-1, and for the NQ dataset,\nit outperforms T5 by 45.80% in terms of exact match (EM).\nEven without accessing external documents, UniGen-Base\noutperforms some retrieval-based models. This can be at-\ntributed to the Q-Connector generated by the LLM, which\nprovides effective contextual information for query inputs.\nBesides, the joint learning of answer generation and D-\nConnector enhances the model’s robustness in generating\nanswers. (2) Under the open-book setting, comparing with\nexisting retrieval-augmented answer generation models, the\nproposed UniGen-Iter outperforms the DPR+FID model by\n28.94% in terms of Bleu-1 on the MS MARCO dataset and\nsurpasses Ultron+FID by 3.73% in terms of EM on the NQ\ndataset. In addition, Figure 4 illustrates the improvement in\nQA performance, through the use of iterative methods com-\npared to the non-iterative approach (UniGen-Base), as indi-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n8693\nModel # Params\nMS MARCO NQ\nB-1 R-L EM F1\nClosed-book Answer Generation\nT5 220M 14.49 18.74 30.22 38.40\nBART 340M 14.83 19.26 29.76 37.34\nLLM-based Answer Generation (w/o finetuning)\nGPT-3.5\n- 16.67 19.41 28.03 36.59\nLLaMA2 - 16.08 18.61 25.27 33.56\nRetrieval-augmented Answer Generation\nRAG\n540M 17.32 21.83 53.32 62.86\nDPR + FID 440M 18.97 23.15 54.47 63.53\nDSI + FID 470M 15.64 19.79 43.58 52.08\nUltron + FID 477M 17.21 21.96 55.75 65.05\nUnified Generative Retrieval and QA (QA Decode)\nUniGen-Base\n367M 17.02† 21.90† 44.06† 53.39†\nUniGen-Iter 367M 24.46‡ 30.31‡ 57.83‡ 67.24‡\nTable 2: Overall QA Performance, where # Param indicates\nthe size of model parameters. The best results among all ex-\nperiments are emphasized in bold, while the best results of\nbaseline models are underlined. The symbol \"†\" and \"‡\" sig-\nnifies that our model achieved superior results in the closed-\nbook and open-book settings, respectively.\ncated by the red lines. This also highlights the effectiveness\nof the proposed iterative enhancement strategy in enhancing\nQA performance. As a result, the enhanced Q-Connectors\nhas also contributed to a consistent enhancement in the per-\nformance of QA task throughout various iterations.\nTo summarize, the UniGen model outperforms existing\nmodels in QA tasks, and the iterative enhancement strategy\nsignificantly contributes to its improved performance.\nAblation Studies\nTo validate the effectiveness of our proposed unified frame-\nwork for retrieval and QA, we conduct experiments where\nwe systematically remove each module and observe the re-\nsulting performance degradation, as presented in Table 3.\nWe find that removing any of the modules, namely the\nshared encoder, Q-Connector, or D-Connector, leads to a no-\nticeable decline in both retrieval and QA performance. No-\ntably, the largest drop in performance is observed when the\nQ-Connector is removed. This highlights the significance of\nleveraging large-scale language models as external knowl-\nedge sources to provide context that is relevant to queries.\nFurthermore, removing the D-Connector also has a signif-\nicant impact on the final performance. This demonstrates\nthe contribution of the D-Connector in bridging the gap be-\ntween the document and the answer, surpassing the capabil-\nities of traditional methods such as hierarchical clustering-\nbased document identifiers. For methods that do not utilize a\nshared encoder, we still observe a decrease in performance,\nunderscoring the advantages of our unified structure. This\nstructure enables the training of more robust encoders, re-\nsulting in improved representations of inputs and enhanced\nretrieval and QA performance.\nModel\nRetrieval QA\nR@1 R@10 B-1 R-L\nUniGen-Base 38.75 79.07 17.02 21.90\nw/o shared encoder 37.89 78.69 16.49 21.30\nw/o Q-Connector 36.14 77.58 12.32 15.98\nw/o D-Connector 37.44 78.25 15.06 18.74\nUniGen-Iter 42.34 81.85 24.46 30.31\nw/o shared encoder 41.76 81.29 23.72 29.71\nw/o Q-Connector 37.43 78.66 21.21 26.39\nw/o D-Connector 41.28 81.59 22.35 27.54\nTable 3: Ablation study of our unified generation model on\nthe MS MARCO dataset.\n0k 5k 10k 15k 20k 25k\nStep\n10\n20\n30\n40\n50MRR@10\nUniGen-Retr\nUniGen-QA\nDPR\nGPT-3.5\n10\n12\n14\n16\n18\n20\nBLEU-1\n(a) MS MARCO\n0k 5k 10k 15k 20k 25k\nStep\n20\n40\n60MRR@10\nUniGen-Retr\nUniGen-QA\nDPR\nGPT-3.5\n25\n30\n35\n40\n45\nEM (b) Natural Questions\nFigure 5: Learning curves of retrieval and QA performance.\nStudy of Learning Curves\nTo demonstrate the effectiveness of our proposed approach\nduring the training process, we plot learning curves to show-\ncase the retrieval and QA performance on the MS MARCO\nand NQ datasets. We utilize a combination of synthetic and\nlabeled data to train the UniGen model. Figure 5 illustrates\nthese curves, with the average values and standard devia-\ntions plotted for each metric, obtained from five separate\ntraining runs on each dataset.\nThe retrieval performance, measured by MRR@10, is\nrepresented by the blue curve, while the red curve repre-\nsents the QA performance, measured by BLEU-1 for MS\nMARCO and EM for NQ. Notably, both tasks exhibit stable\noptimization throughout the learning process, thereby con-\nfirming the effectiveness of our proposed unified framework\nfor simultaneous learning of retrieval and QA tasks.\nConclusion\nIn this paper, we present UniGen, a unified generative frame-\nwork for retrieval and question answering. Our approach\noptimizes both tasks simultaneously and employs connec-\ntors generated by large language models to establish se-\nmantic connections in the input-output and docid-answer\nspaces. Additionally, our iterative enhancement approach\nproves to be effective in enhancing retrieval and QA perfor-\nmance. Through extensive experiments conducted on public\ndatasets, we demonstrate the effectiveness of UniGen in both\nretrieval and QA tasks. This work opens up new possibilities\nfor jointly learning retrieval and other generation tasks.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n8694\nAcknowledgements\nZhicheng Dou is the corresponding author. This work was\nsupported by the National Natural Science Foundation of\nChina No. 62272467, Beijing Outstanding Young Scien-\ntist Program No. BJJWZYJH012019100020098, the fund\nfor building world-class universities (disciplines) of Renmin\nUniversity of China, and Public Computing Cloud, Renmin\nUniversity of China. The work was partially done at Bei-\njing Key Laboratory for Big Data Management and Analysis\nMethods.\nReferences\nBevilacqua, M.; Ottaviano, G.; Lewis, P.; Yih, W.; Riedel,\nS.; and Petroni, F. 2022. Autoregressive Search Engines:\nGenerating Substrings as Document Identifiers. CoRR,\nabs/2204.10628.\nBorgeaud, S.; Mensch, A.; Hoffmann, J.; Cai, T.; Ruther-\nford, E.; Millican, K.; Van Den Driessche, G. B.; Lespiau,\nJ.-B.; Damoc, B.; Clark, A.; et al. 2022. Improving language\nmodels by retrieving from trillions of tokens. In Interna-\ntional conference on machine learning, 2206–2240. PMLR.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language models are few-shot learners. Ad-\nvances in neural information processing systems, 33: 1877–\n1901.\nChiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y .; Wu, Z.; Zhang, H.;\nZheng, L.; Zhuang, S.; Zhuang, Y .; Gonzalez, J. E.; et al.\n2023. Vicuna: An open-source chatbot impressing gpt-4\nwith 90%* chatgpt quality. See https://vicuna. lmsys. org\n(accessed 14 April 2023).\nChowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra,\nG.; Roberts, A.; Barham, P.; Chung, H. W.; Sutton, C.;\nGehrmann, S.; et al. 2022. Palm: Scaling language modeling\nwith pathways. arXiv preprint arXiv:2204.02311.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBert: Pre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805.\nGuu, K.; Lee, K.; Tung, Z.; Pasupat, P.; and Chang, M.\n2020. Retrieval augmented language model pre-training. In\nInternational conference on machine learning, 3929–3938.\nPMLR.\nIzacard, G.; and Grave, E. 2020. Leveraging passage re-\ntrieval with generative models for open domain question an-\nswering. arXiv preprint arXiv:2007.01282.\nIzacard, G.; Lewis, P.; Lomeli, M.; Hosseini, L.; Petroni, F.;\nSchick, T.; Dwivedi-Yu, J.; Joulin, A.; Riedel, S.; and Grave,\nE. 2022. Few-shot learning with retrieval augmented lan-\nguage models. arXiv preprint arXiv:2208.03299.\nKarpukhin, V .; O˘guz, B.; Min, S.; Lewis, P.; Wu, L.; Edunov,\nS.; Chen, D.; and Yih, W.-t. 2020. Dense passage re-\ntrieval for open-domain question answering. arXiv preprint\narXiv:2004.04906.\nKwiatkowski, T.; Palomaki, J.; Redfield, O.; Collins, M.;\nParikh, A.; Alberti, C.; Epstein, D.; Polosukhin, I.; Devlin,\nJ.; Lee, K.; et al. 2019. Natural questions: a benchmark for\nquestion answering research. Transactions of the Associa-\ntion for Computational Linguistics, 7: 453–466.\nLewis, M.; Liu, Y .; Goyal, N.; Ghazvininejad, M.; Mo-\nhamed, A.; Levy, O.; Stoyanov, V .; and Zettlemoyer, L.\n2019. Bart: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and comprehen-\nsion. arXiv preprint arXiv:1910.13461.\nLewis, P.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin,\nV .; Goyal, N.; Küttler, H.; Lewis, M.; Yih, W.-t.; Rock-\ntäschel, T.; et al. 2020. Retrieval-augmented generation for\nknowledge-intensive nlp tasks. Advances in Neural Infor-\nmation Processing Systems, 33: 9459–9474.\nLi, Y .; Yang, N.; Wang, L.; Wei, F.; and Li, W. 2023. Multi-\nview Identifiers Enhanced Generative Retrieval.\nLiu, J.; Jin, J.; Wang, Z.; Cheng, J.; Dou, Z.; and Wen, J.-\nR. 2023. RETA-LLM: A Retrieval-Augmented Large Lan-\nguage Model Toolkit. arXiv preprint arXiv:2306.05212.\nMao, Y .; He, P.; Liu, X.; Shen, Y .; Gao, J.; Han, J.; and Chen,\nW. 2020. Generation-augmented retrieval for open-domain\nquestion answering. arXiv preprint arXiv:2009.08553.\nMehta, S. V .; Gupta, J.; Tay, Y .; Dehghani, M.; Tran, V . Q.;\nRao, J.; Najork, M.; Strubell, E.; and Metzler, D. 2022.\nDSI++: Updating Transformer Memory with New Docu-\nments. arXiv preprint arXiv:2212.09744.\nMetzler, D.; Tay, Y .; Bahri, D.; and Najork, M. 2021. Re-\nthinking search: making domain experts out of dilettantes.\nSIGIR Forum, 55(1): 13:1–13:27.\nNguyen, T.; Rosenberg, M.; Song, X.; Gao, J.; Tiwary, S.;\nMajumder, R.; and Deng, L. 2016. Ms marco: A human-\ngenerated machine reading comprehension dataset.\nNogueira, R.; Lin, J.; and Epistemic, A. 2019. From\ndoc2query to docTTTTTquery. Online preprint, 6: 2.\nPuri, R.; Spring, R.; Patwary, M.; Shoeybi, M.; and Catan-\nzaro, B. 2020. Training question answering models from\nsynthetic data. arXiv preprint arXiv:2002.09599.\nRaffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;\nMatena, M.; Zhou, Y .; Li, W.; and Liu, P. J. 2020. Explor-\ning the limits of transfer learning with a unified text-to-text\ntransformer. The Journal of Machine Learning Research,\n21(1): 5485–5551.\nRam, O.; Levine, Y .; Dalmedigos, I.; Muhlgay, D.; Shashua,\nA.; Leyton-Brown, K.; and Shoham, Y . 2023. In-context\nretrieval-augmented language models. arXiv preprint\narXiv:2302.00083.\nRen, R.; Zhao, W. X.; Liu, J.; Wu, H.; Wen, J.; and Wang,\nH. 2023. TOME: A Two-stage Approach for Model-based\nRetrieval. CoRR, abs/2305.11161.\nRobertson, S.; Zaragoza, H.; et al. 2009. The probabilistic\nrelevance framework: BM25 and beyond. Foundations and\nTrends® in Information Retrieval, 3(4): 333–389.\nShi, W.; Min, S.; Yasunaga, M.; Seo, M.; James, R.; Lewis,\nM.; Zettlemoyer, L.; and Yih, W.-t. 2023. Replug: Retrieval-\naugmented black-box language models. arXiv preprint\narXiv:2301.12652.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n8695\nSingh, D.; Reddy, S.; Hamilton, W.; Dyer, C.; and Yogatama,\nD. 2021. End-to-end training of multi-document reader and\nretriever for open-domain question answering. Advances in\nNeural Information Processing Systems, 34: 25968–25981.\nSun, W.; Yan, L.; Chen, Z.; Wang, S.; Zhu, H.; Ren, P.; Chen,\nZ.; Yin, D.; de Rijke, M.; and Ren, Z. 2023. Learning to\nTokenize for Generative Retrieval. CoRR, abs/2304.04171.\nSun, Z.; Wang, X.; Tay, Y .; Yang, Y .; and Zhou, D. 2022.\nRecitation-augmented language models. arXiv preprint\narXiv:2210.01296.\nTang, Y .; Zhang, R.; Guo, J.; Chen, J.; Zhu, Z.; Wang, S.;\nYin, D.; and Cheng, X. 2023. Semantic-Enhanced Differen-\ntiable Search Index Inspired by Learning Strategies. arXiv\npreprint arXiv:2305.15115.\nTay, Y .; Tran, V . Q.; Dehghani, M.; Ni, J.; Bahri, D.; Mehta,\nH.; Qin, Z.; Hui, K.; Zhao, Z.; Gupta, J. P.; Schuster, T.;\nCohen, W. W.; and Metzler, D. 2022. Transformer Memory\nas a Differentiable Search Index. CoRR, abs/2202.06991.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.-A.; Lacroix, T.; Rozière, B.; Goyal, N.; Hambro, E.;\nAzhar, F.; et al. 2023a. Llama: Open and efficient foundation\nlanguage models. arXiv preprint arXiv:2302.13971.\nTouvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.;\nBabaei, Y .; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale,\nS.; et al. 2023b. Llama 2: Open Foundation and Fine-Tuned\nChat Models. arXiv preprint arXiv:2307.09288.\nWang, Y .; Hou, Y .; Wang, H.; Miao, Z.; Wu, S.; Sun, H.;\nChen, Q.; Xia, Y .; Chi, C.; Zhao, G.; Liu, Z.; Xie, X.;\nSun, H. A.; Deng, W.; Zhang, Q.; and Yang, M. 2022. A\nNeural Corpus Indexer for Document Retrieval. CoRR,\nabs/2206.02743.\nWang, Z.; Zhou, Y .; Tu, Y .; and Dou, Z. 2023. NOVO:\nLearnable and Interpretable Document Identifiers for\nModel-Based IR. In CIKM, 2656–2665. ACM.\nWolf, T.; Debut, L.; Sanh, V .; Chaumond, J.; Delangue, C.;\nMoi, A.; Cistac, P.; Rault, T.; Louf, R.; Funtowicz, M.; et al.\n2019. Huggingface’s transformers: State-of-the-art natural\nlanguage processing. arXiv preprint arXiv:1910.03771.\nXiong, L.; Xiong, C.; Li, Y .; Tang, K.-F.; Liu, J.; Bennett,\nP.; Ahmed, J.; and Overwijk, A. 2020. Approximate near-\nest neighbor negative contrastive learning for dense text re-\ntrieval. arXiv preprint arXiv:2007.00808.\nYu, W.; Iter, D.; Wang, S.; Xu, Y .; Ju, M.; Sanyal, S.; Zhu,\nC.; Zeng, M.; and Jiang, M. 2022. Generate rather than re-\ntrieve: Large language models are strong context generators.\narXiv preprint arXiv:2209.10063.\nZhang, P.; Liu, Z.; Zhou, Y .; Dou, Z.; and Cao, Z.\n2023. Term-Sets Can Be Strong Document Identifiers\nFor Auto-Regressive Search Engines. arXiv preprint\narXiv:2305.13859.\nZhou, Y .; Dou, Z.; and Wen, J. 2023. Enhancing Genera-\ntive Retrieval with Reinforcement Learning from Relevance\nFeedback. In EMNLP, 12481–12490. Association for Com-\nputational Linguistics.\nZhou, Y .; Yao, J.; Dou, Z.; Wu, L.; and Wen, J. 2022a.\nDynamicRetriever: A Pre-training Model-based IR Sys-\ntem with Neither Sparse nor Dense Index. CoRR,\nabs/2203.00537.\nZhou, Y .; Yao, J.; Dou, Z.; Wu, L.; Zhang, P.; and Wen, J.\n2022b. Ultron: An Ultimate Retriever on Corpus with a\nModel-based Indexer. CoRR, abs/2208.09257.\nZhuang, S.; Ren, H.; Shou, L.; Pei, J.; Gong, M.; Zuccon,\nG.; and Jiang, D. 2022. Bridging the Gap Between Index-\ning and Retrieval for Differentiable Search Index with Query\nGeneration. CoRR, abs/2206.10128.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n8696",
  "topic": "Question answering",
  "concepts": [
    {
      "name": "Question answering",
      "score": 0.7547305226325989
    },
    {
      "name": "Generative grammar",
      "score": 0.7309685945510864
    },
    {
      "name": "Computer science",
      "score": 0.6128415465354919
    },
    {
      "name": "Natural language processing",
      "score": 0.5152423977851868
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4048886299133301
    },
    {
      "name": "Information retrieval",
      "score": 0.37365463376045227
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I78988378",
      "name": "Renmin University of China",
      "country": "CN"
    }
  ],
  "cited_by": 14
}