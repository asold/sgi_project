{
  "title": "Contrastive Language-Vision AI Models Pretrained on Web-Scraped Multimodal Data Exhibit Sexual Objectification Bias",
  "url": "https://openalex.org/W4377010286",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A1975226938",
      "name": "Robert Wolfe",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A2113349960",
      "name": "Yiwei Yang",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A2095906382",
      "name": "Bill Howe",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A2408201350",
      "name": "Aylin Caliskan",
      "affiliations": [
        "University of Washington"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2969433215",
    "https://openalex.org/W2082403741",
    "https://openalex.org/W4213038092",
    "https://openalex.org/W4282026609",
    "https://openalex.org/W9204128",
    "https://openalex.org/W2149949843",
    "https://openalex.org/W3028835179",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3180355996",
    "https://openalex.org/W1980236310",
    "https://openalex.org/W3212368439",
    "https://openalex.org/W2141946742",
    "https://openalex.org/W3134095442",
    "https://openalex.org/W3034115845",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2006116925",
    "https://openalex.org/W2964077576",
    "https://openalex.org/W6912494966",
    "https://openalex.org/W2962770929",
    "https://openalex.org/W3192706046",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2963078909",
    "https://openalex.org/W3212464620",
    "https://openalex.org/W4312933868",
    "https://openalex.org/W2886641317",
    "https://openalex.org/W2971307358",
    "https://openalex.org/W3095351420",
    "https://openalex.org/W2026964599",
    "https://openalex.org/W2161112780",
    "https://openalex.org/W3032950536",
    "https://openalex.org/W2300872576",
    "https://openalex.org/W4288058287",
    "https://openalex.org/W2963116854",
    "https://openalex.org/W2810629994",
    "https://openalex.org/W4301409532",
    "https://openalex.org/W3134970617",
    "https://openalex.org/W4221104163",
    "https://openalex.org/W4288058298",
    "https://openalex.org/W3185212449",
    "https://openalex.org/W2123024445"
  ],
  "abstract": "Nine language-vision AI models trained on web scrapes with the Contrastive Language-Image Pretraining (CLIP) objective are evaluated for evidence of a bias studied by psychologists: the sexual objectification of girls and women, which occurs when a person's human characteristics, such as emotions, are disregarded and the person is treated as a body. We replicate three experiments in psychology quantifying sexual objectification and show that the phenomena persist in AI. A first experiment uses standardized images of women from the Sexual OBjectification and EMotion Database, and finds that human characteristics are disassociated from images of objectified women: the model's recognition of emotional state is mediated by whether the subject is fully or partially clothed. Embedding association tests (EATs) return significant effect sizes for both anger (d >0.80) and sadness (d >0.50), associating images of fully clothed subjects with emotions. GRAD-CAM saliency maps highlight that CLIP gets distracted from emotional expressions in objectified images. A second experiment measures the effect in a representative application: an automatic image captioner (Antarctic Captions) includes words denoting emotion less than 50% as often for images of partially clothed women than for images of fully clothed women. A third experiment finds that images of female professionals (scientists, doctors, executives) are likely to be associated with sexual descriptions relative to images of male professionals. A fourth experiment shows that a prompt of \"a [age] year old girl\" generates sexualized images (as determined by an NSFW classifier) up to 73% of the time for VQGAN-CLIP and Stable Diffusion; the corresponding rate for boys never surpasses 9%. The evidence indicates that language-vision AI models trained on web scrapes learn biases of sexual objectification, which propagate to downstream applications.",
  "full_text": null,
  "topic": "Objectification",
  "concepts": [
    {
      "name": "Objectification",
      "score": 0.8326585292816162
    },
    {
      "name": "Computer science",
      "score": 0.6600326299667358
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5420258045196533
    },
    {
      "name": "Natural language processing",
      "score": 0.41576892137527466
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}