{
  "title": "Are Pretrained Multilingual Models Equally Fair Across Languages?",
  "url": "https://openalex.org/W4305031560",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4227180418",
      "name": "Piqueras, Laura Cabello",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221452188",
      "name": "Søgaard, Anders",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3112524731",
    "https://openalex.org/W2997607995",
    "https://openalex.org/W3126947648",
    "https://openalex.org/W1985514943",
    "https://openalex.org/W2937418520",
    "https://openalex.org/W2962787423",
    "https://openalex.org/W2019979955",
    "https://openalex.org/W3098573764",
    "https://openalex.org/W3211384372",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3087873698",
    "https://openalex.org/W3153289922",
    "https://openalex.org/W22168010",
    "https://openalex.org/W2152311128",
    "https://openalex.org/W2791170418",
    "https://openalex.org/W4287121067",
    "https://openalex.org/W3154152568",
    "https://openalex.org/W4206783764",
    "https://openalex.org/W4292402161",
    "https://openalex.org/W3038047279",
    "https://openalex.org/W2250559305",
    "https://openalex.org/W3115830533",
    "https://openalex.org/W2962833164",
    "https://openalex.org/W3032765105",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W2985620815",
    "https://openalex.org/W2520732594",
    "https://openalex.org/W2970316683",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2936891809",
    "https://openalex.org/W3035379020",
    "https://openalex.org/W2962990575",
    "https://openalex.org/W3181414820",
    "https://openalex.org/W2270070752"
  ],
  "abstract": "Pretrained multilingual language models can help bridge the digital language divide, enabling high-quality NLP models for lower resourced languages. Studies of multilingual models have so far focused on performance, consistency, and cross-lingual generalisation. However, with their wide-spread application in the wild and downstream societal impact, it is important to put multilingual models under the same scrutiny as monolingual models. This work investigates the group fairness of multilingual models, asking whether these models are equally fair across languages. To this end, we create a new four-way multilingual dataset of parallel cloze test examples (MozArt), equipped with demographic information (balanced with regard to gender and native tongue) about the test participants. We evaluate three multilingual models on MozArt -- mBERT, XLM-R, and mT5 -- and show that across the four target languages, the three models exhibit different levels of group disparity, e.g., exhibiting near-equal risk for Spanish, but high levels of disparity for German.",
  "full_text": "Are Pretrained Multilingual Models Equally Fair Across Languages?\nLaura Cabello Piqueras\nUniversity of Copenhagen\nlcp@di.ku.dk\nAnders Søgaard\nUniversity of Copenhagen\nsoegaard@di.ku.dk\nAbstract\nPretrained multilingual language models can\nhelp bridge the digital language divide, en-\nabling high-quality NLP models for lower-\nresourced languages. Studies of multilingual\nmodels have so far focused on performance,\nconsistency, and cross-lingual generalisation.\nHowever, with their wide-spread application\nin the wild and downstream societal impact,\nit is important to put multilingual models un-\nder the same scrutiny as monolingual mod-\nels. This work investigates the group fairness\nof multilingual models, asking whether these\nmodels are equally fair across languages. To\nthis end, we create a new four-way multilin-\ngual dataset of parallel cloze test examples\n(MozArt), equipped with demographic infor-\nmation (balanced with regard to gender and\nnative tongue) about the test participants. We\nevaluate three multilingual models on MozArt\n– mBERT, XLM-R, and mT5 – and show that\nacross the four target languages, the three mod-\nels exhibit different levels of group disparity,\ne.g., exhibiting near-equal risk for Spanish, but\nhigh levels of disparity for German.\n1 Introduction\nFill-in-the-gap cloze tests (Taylor, 1953) ask lan-\nguage learners to predict what words were removed\nfrom a text and it is a “procedure for measuring the\neffectiveness of communication”. Today, language\nmodels are trained to do the same (Devlin et al.,\n2019). This has the advantage that we can now\nuse ﬁll-in-the-gap cloze tests to directly compare\nthe linguistic preferences of humans and language\nmodels, e.g., to investigate task-independent soci-\nolectal biases (group disparities) in language mod-\nels (Zhang et al., 2021). This paper presents a novel\nfour-way parallel cloze dataset for English, French,\nGerman, and Spanish that enables apples-to-apples\ncomparison across languages of group disparities\nin multilingual language models.1\n1The language selection was given to us, because we rely\non an existing word alignment dataset; see §2.\nEN ES DE FR\nWordPiece (avg. #tokens)19.7 22.0 23.6 23.1\nSentencePiece (avg. #tokens)22.3 22.9 24.9 25.3\n#Sentences 100 100 100 100\n#Annotations 600 600 600 600\n#Annotators 60 60 60 60\nDemographics\nid_u, id_s, gender, age, nationality,\nﬁrst language, ﬂuent languages,\ncurrent country of residence,\ncountry of birth, time taken\nTable 1: MozArt details. The average number of to-\nkens per sentence is reported using WordPiece and Sen-\ntecePiece. The bottom row lists the demographic at-\ntributes shared; id_u refers to user id (anonymised) and\nid_s to sentence id.\nLanguage models induced from historical data\nare prone to implicit biases (Zhao et al., 2017;\nChang et al., 2019; Mehrabi et al., 2021), e.g., as a\nresult of the over-representation of male-dominated\ntext sources such as Wikipedia and newswire (Hovy\nand Søgaard, 2015). This may lead to language\nmodels that are unfair to groups of users in the\nsense that they work better for some groups rather\nthan others (Zhang et al., 2021). Multilingual lan-\nguage models can be unfair to their training lan-\nguages in similar ways (Choudhury and Deshpande,\n2021; Wan, 2022; Wang et al., 2021), but this work\ngoes beyond previous work in evaluating whether\nmultilingual language models are equally fair to\ndemographic groups across languages.\nTo this end, we create MozArt, a multilingual\ndataset of ﬁll-in-the-gap sentences covering four\nlanguages (English, French, German and Spanish).\nThe sentences reﬂect diastratic variation within\neach language and can be used to compare bi-\nases in pretrained language models (PLMs) across\nlanguages. We study the inﬂuence of four demo-\ngraphic groups, i.e., the cross-product of our anno-\narXiv:2210.05457v1  [cs.CL]  11 Oct 2022\ntators’ gender – male (M) or female (F)2 – and ﬁrst\nlanguage – native (N) or non-native (NN).3 Table 1\npresents a summary of dataset characteristics.\n2 Dataset\nWe introduce MozArt, a four-way multilingual\ncloze test dataset with annotator demographics.\nWe sampled 100 sentence quadruples from each\nof the four languages (English, French, German,\nSpanish) in the corpus provided for the WMT 2006\nShared Task.4 The data was extracted from the\npublicly available Europarl corpus (Koehn, 2005)\nand enhanced with word-level bitext alignments\n(Koehn and Monz, 2006). The word alignments\nare important for what follows. We manually\nverify that sentences make sense out of context\nand use the data to generate comparable cloze\nexamples, e.g.:\nen [MASK] that deplete the ozone layer\nes [MASK] que agotan la capa de ozono\nde [MASK], die zum Abbau der Ozonschicht führen\nfr [MASK] appauvrissant la couche d’ozone\nWe only mask words which are (i) aligned by one-\nto-one alignments, and which are (ii) either nouns,\nverbs, adjectives or adverbs.5 We mask one word\nin each sentence and verify that one-to-one align-\nments exist in all languages. Following Kleijn et al.\n(2019), we rely on part-of-speech information to\navoid masking words that are too predictable, e.g.,\nauxiliary verbs or constituents of multi-word ex-\npressions, or words that are un-predictable, e.g.,\nproper names and technical terms.\nAnnotators were recruited using Proliﬁc. 6 We\napplied eligibility criteria to balance our annota-\ntors across demographics. Participants were asked\nto report (on a voluntary basis) their demographic\ninformation regarding gender and languages spo-\nken. Each eligible participant was presented with\n10 cloze examples. We collected answers from\n240 annotators, 60 per language batch, divided in\n2None of our annotators identiﬁed as non-binary.\n3See Schmitz (2016); Faez (2011) for discussion of the\nnative/non-native speaker dichotomy. Participants were asked\n“What is your ﬁrst language?” and “Which of the following\nlanguages are you ﬂuent in?”. We use native (N) for people\nwhose ﬁrst language coincides with the example sentences,\nand non-native (NN) otherwise, without any sociocultural im-\nplications.\n4www.statmt.org/wmt06/shared-task\n5We use spaCy’s part-of-speech tagger (Honnibal and Mon-\ntani, 2017) to predict the syntactic categories of the input\nwords.\n6prolific.co\nfour balanced demographic groups (gender ×na-\ntive language). We made sure that each sentence\nhad at least six annotations. Annotation guidelines\nfor each language were given in that language, to\navoid bias and ensure a minimum of language un-\nderstanding for non-native speakers. We manually\nﬁltered out spammers to ensure data quality.\nThe dataset is made publicly available at\ngithub.com/coastalcph/mozart under a\nCC-BY-4.0 license. We include all the demo-\ngraphic attributes of our annotators as per agree-\nment with the annotators. The full list of protected\nattributes is found in Table 1. We hope MozArt\nwill become a useful resource for the community,\nalso for evaluating the fairness of language mod-\nels across other attributes than gender and native\nlanguage.\n3 Experimental Setup\nModels We evaluate three PLMs: mBERT (De-\nvlin et al., 2019), XLM-RoBERTa/XLM-R (Con-\nneau et al., 2020), and mT5 (Xue et al., 2021). 7\nAll three models were trained with a masked lan-\nguage modelling objective. mBERT differs from\nXLM-R and mT5 in including a next sentence pre-\ndiction objective (Devlin et al., 2019). mT5 differs\nfrom mBERT and XLM-R in allowing for consec-\nutive spans of input tokens to be masked (Raffel\net al., 2020). We adopt beam search decoding with\nearly stopping and constrain the generation to sin-\ngle words. This enables better correlation of mT5’s\noutput with our group preferences. t-SNE plots are\nincluded in Appendix B to show how languages\nare distributed in the PLM vector spaces.\nMetrics We use several metrics to compare how\nthe PLMs align with group preferences across lan-\nguages. These include top-k precision P@k with\nk={1, 5}, mean reciprocal rank ( MRR), and two\nclassical univariate rank correlations: Spearman’sρ\n(Spearman, 1987) and Kendall’sτ (Kendall, 1938).\nGiven a set of |S|cloze sentences and a group\nof annotators, for each sentence s, we denote\nthe list of answers, ranked by their frequency, as\nWs = [w1,w2,...], and the list of model’s predic-\ntions as Cs = [c1,c2,...], ranked by their model\nlikelihood. Then, we report P@k = 1 [ci ∈Ws]\nwith i∈[1,k], where 1 [·] is the indicator function.\n7We use the base models available from huggingface.\nco/models. We report results using uncased mBERT, since\nit performed better on our data than its cased sibling.\nPrecision is reported together with its standard de-\nviation, to account for the group-wise disparity in\nboth dimensions (social groups and language):\nσgd =\n√∑G\nj=1(P@kj −P@k)2\nG (1)\nwhere P@k is the mean value of all observa-\ntions, and G the total number of groups across the\ndimension ﬁxed each time i.e., G = 4across social\ngroups (MN, FN, MNN, FNN ) and G = 4across\nlanguages (EN, ES, DE, FR). We also compute the\nmean-reciprocal rank ( MRR) of the elements of\nWs with respect to the top-n(n = 5) elements of\nCs (Cn\ns ):\nMRR = 1\n|S|\n|S|∑\ns=1\n1\nRankCns\ni\n(2)\nFinally, we compute Spearman’sρ(Spearman,\n1987) and Kendall’sτ (Kendall, 1938) between Ws\nand C5\ns . These metrics are generally more robust\nto outliers.\n4 Results\nFollowing previous work on examining fairness of\ndocument classiﬁcation (Huang et al., 2020; Dixon\net al., 2018; Park et al., 2018; Garg et al., 2019),\nwe focus on group-level performance differences\n(group disparity). We measure the group dispar-\nity as the variance in PLM’s performance (P@k)\nacross demographics (gender and native language).\nTable 2 shows better precision for native speak-\ners in German and French (MN, FN) for P@1. In\nterms of group disparity, male non-natives (MNN)\nis the demographic exhibiting the highest dispar-\nity across languages in mBERT, while it is female\nnatives (FN) in XLM-R and male natives (MN) in\nmT5. Language-wise, we see the largest group\ndisparity with German in all three models. Here,\nwe see 2.5–4.4 between-group differences, com-\npared to, e.g., 0.3–1.8 between-group differences\nfor English. See Appendix A for results with P@5.\nXLM-R consistently exhibits better overall per-\nformance on average, but higher between-group\nand between-language differences in terms of pre-\ncision (σgd).\nFigure 1 complements results from Table 2 with\nMRR scores. We observe a common trend that\nthe models often underperform on non-native male\nspeakers in all languages except for Spanish: Per-\nformance is (always) below the average, and they\nmBERT\nP@1 EN ES DE FR\nMN 13.3 12.7 11.3 10.7 12.0 (1.0)\nFN 13.3 12.0 15.3 8.0 12.2 (2.7)\nMNN 12.7 12.4 11.4 3.6 10.0(3.8)\nFNN 13.3 10.0 5.6 6.9 9.0 (3.0)\n13.2 (0.3)11.8 (1.1)10.8(3.5) 7.3 (2.5)P@1(σgd)\nXLM-R\nP@1 EN ES DE FR\nMN 16.7 13.3 20.7 16.7 16.9 (2.6)\nFN 16.0 15.3 24.0 17.3 18.2(3.5)\nMNN 15.3 13.5 15.0 11.4 13.8 (1.5)\nFNN 20.0 14.7 13.1 12.7 15.1 (3.0)\n17.0 (1.8)14.2 (0.8)18.2(4.4) 14.5 (2.6)P@1(σgd)\nmT5\nP@1 EN ES DE FR\nMN 2.0 4.7 8.7 5.3 5.2(2.4)\nFN 4.0 3.3 6.7 3.3 4.3 (1.4)\nMNN 2.0 4.7 6.4 4.3 4.4 (1.6)\nFNN 3.3 6.7 1.9 6.2 4.5 (2.0)\n2.8 (0.9)4.8 (1.2)5.8(2.5) 4.8 (1.1)P@1(σgd)\nTable 2: Results on P@1 score across groups (rows)\nand languages (columns), average performance in each\nlanguage (P@1) and standard deviation for group dis-\nparity (σgd). Cells are coloured language-wise. Cells\nwith a darker background are language-wise above the\naverage. Worst group performance in terms of group\ndisparity (highest variance) is highlighted in red.\nare the worst-off group ( ↓) in most of the cases.\nAt the same time, predictions with mBERT and\nXLM-R seem to be biased towards native speak-\ners because answers from MN and FN generally\nrank highest. Despite none of the models perform\nequally across groups, XLM-R shows a lower di-\nvergence across languages: Between-group differ-\nences are more than 50% smaller than with mBERT\nand mT5 when looking at the averageMRR per lan-\nguage.\nTable 3 gathers group level Spearman’sρand av-\nerage correlation per language. XLM-R predictions\nare more uniformly correlated across languages\ncompared to mBERT, whose lexical preferences are\nbetter aligned in English and Spanish setups, and\nmT5, whose predictions correlate poorly with hu-\nman cloze test answers. However, in line with pre-\nvious results, the model exhibits bias towards male\nnative speakers and MNN outlines as the worst per-\nforming group across languages, with a coefﬁcient\nalways below the average. Looking into the dimen-\nsion of languages, German is the least aligned with\nhuman’s answers in all models. Kendall’sτ yields\nsimilar results. See Appendix A for details.\nIt is worth mentioning that our study does not\naim to compare models’ performance, but rather to\n0\n10\n20\n30\nEN ES DE FR\n0\n10\n20\n30\n0\n10\n20\n30 MRR\nMN FN MNN FNN\nmBERT\nmT5\n↑\n↑↑ ↑\n↓\n↑ ↑\n↓\n↓\n↓\n↑\n↑\n↑ ↑\n↑ ↑XLM-R\n↓\n↓↓ ↓\n↓↓ ↓↓↓\nFigure 1: Average MRR (in percentage) per group in\neach language. Horizontal lines denote the average per\nlanguage. Best-off ( ↑) and worst-off (↓) subgroups for\neach language are marked.\nmotivate a discussion about the between-group and\nbetween-language differences within each model.\nThe general low precision of mT5 outputs com-\npared to human answers is likely due to the nature\nof the task itself. Because mT5 was trained with\na span-mask denoising objective, it tends to com-\nplete the masked-out span with more than one to-\nken. When constraining generation to output one\ntoken, we are conditioning its default behaviour.\nBetter correlation could be achieved by ﬁne-tuning\nthe model on completing cloze tests.\n(Dis)agreement amongst annotators on the same\nlanguage gives a measure of the difﬁculty of the\ntask. French and German present a higher variabil-\nity in the responses (with a vocabulary of 442 and\n443 words respectively), compared to English (374\nwords), and Spanish (427 words), which reﬂects in\na lower correlation with models’ predictions.\n5 Related Work\nMultilingual PLMs have been analyzed in many\nways: Researchers have, for example, looked at per-\nformance differences across languages (Singh et al.,\n2019; Wu and Dredze, 2020), looked at their organi-\nzation of language types (Rama et al., 2020), used\nsimilarity analysis to probe their representations\n(Kudugunta et al., 2019), and investigated how\nlearned self-attention in the Transformer blocks af-\nfects different languages (Ravishankar et al., 2021).\nPrevious work on fairness of multilingual models\nhas, to the best of our knowledge, focused exclu-\nsively on task-speciﬁc models, rather than PLMs:\nHuang et al. (2020) evaluate the fairness of multilin-\ngual hate speech detection models, and several re-\nsearchers have explored gender bias in multilingual\nmBERT\nρ EN ES DE FR\nMN 0.33 (p=0.00)0.23 (p=0.01)-0.14 (p=0.09)0.10 (p=0.21)\nFN 0.27 (p=0.00)0.07 (p=0.42)-0.01 (p=0.89)0.14 (p=0.08)\nMNN0.30 (p=0.00)0.16 (p=0.03)-0.10 (p=0.23)0.08 (p=0.32)\nFNN 0.37 (p=0.00)0.16 (p=0.06)0.03 (p=0.69)0.08 (p=0.30)\nAvg. 0.32 (p=0.00)0.16 (p=0.00)-0.05 (p=0.21)0.10 (p=0.01)\nXLM-R\nρ EN ES DE FR\nMN 0.45 (p=0.00)0.46 (p=0.00)0.35 (p=0.00)0.48 (p=0.00)\nFN 0.30 (p=0.00)0.35 (p=0.00)0.45 (p=0.00)0.33 (p=0.00)\nMNN0.30 (p=0.00)0.38 (p=0.00)0.22 (p=0.01)0.32 (p=0.00)\nFNN 0.40 (p=0.00)0.48 (p=0.00)0.11 (p=0.16)0.36 (p=0.00)\nAvg. 0.36 (p=0.00)0.41 (p=0.00)0.28 (p=0.00)0.37 (p=0.00)\nmT5\nρ EN ES DE FR\nMN 0.01 (p=0.89)0.14 (p=0.08)0.14 (p=0.08)0.25 (p=0.00)\nFN -0.12 (p=0.13)0.13 (p=0.12)0.00 (p=0.99)0.14 (p=0.08)\nMNN-0.10 (p=0.22)0.12 (p=0.11)0.03 (p=0.74)0.11 (p=0.18)\nFNN -0.07 (p=0.41)0.28 (p=0.00)0.04 (p=0.58)0.11 (p=0.16)\nAvg. -0.07 (p=0.07)0.17 (p=0.00)0.05 (p=0.23)0.15 (p=0.00)\nTable 3: Correlation between groups of annotators (MN,\nFN, MNN, FNN ) and models’ predictions, classiﬁed\nby language. The degree of correlation is measured\nwith Spearman’s ρcoefﬁcient (ρ∈[−1,1]). Cells are\ncoloured language-wise. Cells with a darker back-\nground show a stronger correlation compared to the av-\nerage in each language. Samples highlighted in red fail\nto reject the null hypothesis, meaning that their differ-\nence is not statistically signiﬁcant (p >0.05).\nmodels (Zhao et al., 2020; González et al., 2020).\nDayanik and Padó (2021) consider the effects of\nadversarial debiasing in multilingual models.\nCloze tests were previously used in Zhang et al.\n(2021) to evaluate the fairness of English (monolin-\ngual) language models. In psycholinguistics, cloze\ntests have been performed with different age groups\n(Hintz et al., 2020) and native language (Stringer\nand Iverson, 2020), but these datasets have, to the\nbest of our knowledge, not been used to evaluate\nlanguage models.\n6 Conclusion\nIn this paper, we present MozArt, a new multilin-\ngual dataset of parallel cloze examples with anno-\ntations from balanced demographics. This dataset\nis, to the best of our knowledge, the ﬁrst to enable\napples-to-apples comparison of group disparity of\nmultilingual PLMs across languages. The dataset\nincludes several demographic attributes, but we\npresent preliminary experiments with gender and\nnative language. We show that mBERT, XLM-\nR and mT5 are not equally fair across languages.\nFor example, group disparities are much higher for\nGerman (and French) than for English and Spanish.\nThis shows the importance of evaluating fairness\nacross languages instead of stipulating from results\nfor a single language. We further show that cloze\ntest answers of female native speakers tend to rank\nhighest in both predictive PLMs. We followed best\npractices for mitigating the dangers of crowdsourc-\ning (Karpinska et al., 2021; Kleijn et al., 2019)\n(see §2) and hope MozArt will be widely adopted\nand, over time, generate more results for other lan-\nguages, PLMs and demographic attributes.\n7 Limitations\nAs described in the paper, MozArt builds on top\nof another dataset, which is only available in four\nlanguages. The original dataset with its manual\nword alignments provided a unique opportunity to\nbuild MozArt in a way in which we could account\nfor context, across languages. This of course limits\nour work to the languages provided. We acknowl-\nedge how multilingual studies of Indo-European\nlanguages may not generalize to languages outside\nthis language famility, and hope we or others will\nbe able to contribute resources for a more diverse\nset of languages in the future.\nEthics Statement\nThe dataset released contains publicly available\ncontent from the proceedings of the European Par-\nliament. Our work is based on sensitive informa-\ntion provided by the participants that took on our\nstudy in Proliﬁc. The protected attributes collected\nare self-reported on a voluntary basis, and partic-\nipants gave their consent to share them. In addi-\ntion to the speciﬁc attributes analyzed in our study,\nwhich served as prescreening ﬁlters, Proliﬁc also\nprovides baseline data for all studies with the con-\nsent of participants to share it with researchers. For\nthese base attributes, there might be gaps in the\ndata because it is optional for participants to pro-\nvide this information. These attributes are ﬁlled\nas null in the dataset. We performed a pilot study\nto determine the amount of time a task would take\non average. The participants were paid based on\ntime worked, and were given the option to opt out\nat any time of the study. Participants who revoked\nconsent at any stage are not included in our study\nnor in the data released.\nReferences\nKai-Wei Chang, Vinodkumar Prabhakaran, and Vi-\ncente Ordonez. 2019. Bias and fairness in nat-\nural language processing. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP): Tutorial Abstracts, Hong Kong,\nChina. Association for Computational Linguistics.\nRochelle Choenni and Ekaterina Shutova. 2020. What\ndoes it mean to be language-agnostic? probing mul-\ntilingual sentence encoders for typological proper-\nties.\nMonojit Choudhury and Amit Deshpande. 2021. How\nlinguistically fair are multilingual pre-trained lan-\nguage models? In AAAI-21. AAAI, AAAI.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale.\nErenay Dayanik and Sebastian Padó. 2021. Disentan-\ngling document topic and author gender in multiple\nlanguages: Lessons for adversarial debiasing. In\nProceedings of the Eleventh Workshop on Compu-\ntational Approaches to Subjectivity, Sentiment and\nSocial Media Analysis , pages 50–61, Online. Asso-\nciation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning.\nLucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain,\nand Lucy Vasserman. 2018. Measuring and mitigat-\ning unintended bias in text classiﬁcation. In Pro-\nceedings of the 2018 AAAI/ACM Conference on AI,\nEthics, and Society , AIES ’18, page 67–73, New\nYork, NY , USA. Association for Computing Machin-\nery.\nFarahnaz Faez. 2011. Reconceptualizing the na-\ntive/nonnative speaker dichotomy. Journal of Lan-\nguage, Identity & Education, 10(4):231–249.\nSahaj Garg, Vincent Perot, Nicole Limtiaco, Ankur\nTaly, Ed H. Chi, and Alex Beutel. 2019. Counterfac-\ntual fairness in text classiﬁcation through robustness.\nProceedings of the 2019 AAAI/ACM Conference on\nAI, Ethics, and Society.\nAna Valeria González, Maria Barrett, Rasmus Hvin-\ngelby, Kellie Webster, and Anders Søgaard. 2020.\nType B reﬂexivization as an unambiguous testbed\nfor multilingual multi-task gender bias. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n2637–2648, Online. Association for Computational\nLinguistics.\nF Hintz, M Dijkhuis, V van Hoff, JM McQueen, and\nAS Meyer. 2020. A behavioural dataset for studying\nindividual differences in language skills. Scientiﬁc\nData, 7(1).\nMatthew Honnibal and Ines Montani. 2017. spaCy 2:\nNatural language understanding with Bloom embed-\ndings, convolutional neural networks and incremen-\ntal parsing. To appear.\nDirk Hovy and Anders Søgaard. 2015. Tagging perfor-\nmance correlates with author age. In Proceedings\nof the 53rd Annual Meeting of the Association for\nComputational Linguistics and the 7th International\nJoint Conference on Natural Language Processing\n(Volume 2: Short Papers) , pages 483–488, Beijing,\nChina. Association for Computational Linguistics.\nXiaolei Huang, Linzi Xing, Franck Dernoncourt, and\nMichael J. Paul. 2020. Multilingual Twitter cor-\npus and baselines for evaluating demographic bias\nin hate speech recognition. In Proceedings of the\n12th Language Resources and Evaluation Confer-\nence, pages 1440–1448, Marseille, France. Euro-\npean Language Resources Association.\nMarzena Karpinska, Nader Akoury, and Mohit Iyyer.\n2021. The perils of using mechanical turk to evalu-\nate open-ended text generation.\nM. G. Kendall. 1938. A new measure of rank correla-\ntion. Biometrika, 30(1-2):81–93.\nSuzanne Kleijn, Henk Pander Maat, and Ted Sanders.\n2019. Cloze testing for comprehension assessment:\nThe hytec-cloze. Language Testing, 36(4):553–572.\nPhilipp Koehn. 2005. Europarl: A parallel corpus for\nstatistical machine translation. In Proceedings of\nMachine Translation Summit X: Papers , pages 79–\n86, Phuket, Thailand.\nPhilipp Koehn and Christof Monz. 2006. Manual and\nautomatic evaluation of machine translation between\nEuropean languages. In Proceedings on the Work-\nshop on Statistical Machine Translation, pages 102–\n121, New York City. Association for Computational\nLinguistics.\nSneha Kudugunta, Ankur Bapna, Isaac Caswell, and\nOrhan Firat. 2019. Investigating multilingual NMT\nrepresentations at scale. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 1565–1575, Hong Kong,\nChina. Association for Computational Linguistics.\nNinareh Mehrabi, Fred Morstatter, Nripsuta Saxena,\nKristina Lerman, and Aram Galstyan. 2021. A sur-\nvey on bias and fairness in machine learning. ACM\nComput. Surv., 54(6).\nJi Ho Park, Jamin Shin, and Pascale Fung. 2018. Re-\nducing gender bias in abusive language detection.\nIn Proceedings of the 2018 Conference on Em-\npirical Methods in Natural Language Processing ,\npages 2799–2804, Brussels, Belgium. Association\nfor Computational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer.\nTaraka Rama, Lisa Beinborn, and Steffen Eger. 2020.\nProbing multilingual BERT for genetic and typo-\nlogical signals. In Proceedings of the 28th Inter-\nnational Conference on Computational Linguistics ,\npages 1214–1228, Barcelona, Spain (Online). Inter-\nnational Committee on Computational Linguistics.\nVinit Ravishankar, Artur Kulmizev, Mostafa Abdou,\nAnders Søgaard, and Joakim Nivre. 2021. Atten-\ntion can reﬂect syntactic structure (if you let it). In\nProceedings of the 16th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Main Volume , pages 3031–3045, Online.\nAssociation for Computational Linguistics.\nJohn Schmitz. 2016. On the native/nonnative speaker\nnotion and world englishes: Debating with k. ra-\njagopalan. DELTA: Documentação de Estudos em\nLingüística Teórica e Aplicada, 32:597–611.\nJasdeep Singh, Bryan McCann, Richard Socher, and\nCaiming Xiong. 2019. BERT is not an interlingua\nand the bias of tokenization. In Proceedings of the\n2nd Workshop on Deep Learning Approaches for\nLow-Resource NLP (DeepLo 2019) , pages 47–55,\nHong Kong, China. Association for Computational\nLinguistics.\nC. Spearman. 1987. The proof and measurement of\nassociation between two things. The American Jour-\nnal of Psychology, 100(3/4):441–471.\nLouise Stringer and Paul Iverson. 2020. Non-native\nspeech recognition sentences: A new materials set\nfor non-native speech perception research. Behavior\nResearch Methods, 52(2).\nWilson L. Taylor. 1953. Cloze procedure: A new tool\nfor measuring readability. Journalism & Mass Com-\nmunication Quarterly, 30:415 – 433.\nAda Wan. 2022. Fairness in representation for multi-\nlingual NLP: Insights from controlled experiments\non conditional language modeling. In International\nConference on Learning Representations.\nJialu Wang, Yang Liu, and Xin Eric Wang. 2021. As-\nsessing multilingual fairness in pre-trained multi-\nmodal representations.\nShijie Wu and Mark Dredze. 2020. Are all languages\ncreated equal in multilingual BERT? InProceedings\nof the 5th Workshop on Representation Learning for\nNLP, pages 120–130, Online. Association for Com-\nputational Linguistics.\nLinting Xue, Noah Constant, Adam Roberts, Mi-\nhir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\nBarua, and Colin Raffel. 2021. mt5: A massively\nmultilingual pre-trained text-to-text transformer.\nSheng Zhang, Xin Zhang, Weiming Zhang, and An-\nders Søgaard. 2021. Sociolectal analysis of pre-\ntrained language models. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 4581–4588, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nJieyu Zhao, Subhabrata Mukherjee, Saghar Hosseini,\nKai-Wei Chang, and Ahmed Hassan Awadallah.\n2020. Gender bias in multilingual embeddings and\ncross-lingual transfer. In Proceedings of the 58th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2896–2907, Online. Asso-\nciation for Computational Linguistics.\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-\ndonez, and Kai-Wei Chang. 2017. Men also like\nshopping: Reducing gender bias ampliﬁcation using\ncorpus-level constraints.\nA Additional results\nIn this section, we provide additional analysis re-\nsults of the PLM’s performance on MozArt. We\nreport precision at 5 (P@5), which corresponds to\nthe number of relevant answers amongst the top 5\ncandidates. It provides a more ﬂexible metric for\nmeasuring model alignments with open-ended text\nanswers, but fails to take into account the exact\nposition within the top-k. Considering the top-\n5, the bias towards native speakers is diminished\nespecially in English and Spanish, despite being\nMNN and FNN the worst groups –in terms of group\ndisparity– in mBERT and XLM-R respectively. At\nthe same time, the group disparities are exacerbated\nas shown in Table 4.\nTable 5 complements results on correlation of the\nalignment of group responses. It shows Kendall’s\nτ coefﬁcient. Conclusions remain almost the same\nas studied with Spearman’s coefﬁcient, albeit non-\nnative subgroups in Spanish are more correlated in\nmBERT.\nB t-SNE\nTo give a brief overview of the semantic multilin-\nguality encoded in the pretrained models, we run\nseveral representations with t-SNE. Figure 2 and\nFigure 3 represent the top-1000 predictions in a\nt-SNE plot for mBERT and XLM-R respectively.\nThe same sentence is queried to the model in four\nlanguages and, accordingly, to annotators:\nmBERT\nP@5 EN ES DE FR\nMN 30.7 26.7 22.0 24.0 25.9 (3.3)\nFN 32.0 18.7 24.7 22.0 24.4 (4.9)\nMNN 34.0 25.9 12.1 15.0 21.8(8.7)\nFNN 32.7 25.3 16.3 16.3 22.7 (6.9)\n32.3 (1.2)24.2 (3.1)18.8(4.9) 19.3 (3.8)P@5(σgd)\nXLM-R\nP@5 EN ES DE FR\nMN 39.3 30.7 34.7 32.7 34.4 (3.2)\nFN 30.7 25.3 38.0 35.3 32.3 (4.8)\nMNN 30.7 29.4 22.1 25.4 26.9 (3.4)\nFNN 36.7 34.0 19.4 26.9 29.3(6.7)\n34.3 (3.8)29.8 (3.1)28.5(7.9) 30.3 (4.1)P@5(σgd)\nmT5\nP@5 EN ES DE FR\nMN 10.0 12.7 16.0 11.3 12.5 (2.2)\nFN 11.3 10.0 16.7 18.0 14.0(3.4)\nMNN 6.0 11.8 9.3 10.7 9.5 (2.2)\nFNN 13.3 16.0 8.7 15.0 13.3 (2.8)\n10.2 (2.7)12.6 (2.2)12.7(3.7) 13.8 (3.0)P@5(σgd)\nTable 4: Results on P@5 score across groups and lan-\nguages, average performance in each language ( P@5)\nand standard deviation for group disparity (σgd). Cells\nare coloured language-wise. Cells with a darker back-\nground are language-wise above the average. Worst\ngroup performance in terms of group disparity (highest\nvariance) is highlighted in red.\nen We want to [MASK] innovation .\nes Queremos [MASK] la innovación .\nde Wir wollen zur Innovation [MASK] .\nfr Nous voulons [MASK] l’innovation .\nHighest scored predictions are highlighted with\na (⋆). Annotator’s answers that fell into the top-\n1000 predictions are denoted with a black edge. In\nline with results in (Choenni and Shutova, 2020),\nwe observe that languages are mostly projected in\nseparate sub-spaces instead of yielding a neutral\nrepresentation, even though they share a common\nspace (vocabulary).\nSimilarly, Singh et al. (2019) shown a trend\ntowards dissimilarity between representations for\nsemantically similar inputs in different languages,\nin deeper layers of an uncased mBERT. Serve\nFigure 4 as an example, where the same word\n“gases” was answered in different languages but is\nrepresented in different subspaces. Figure 5 shows\na similar behaviour in XLM-R. The sentences\nqueried are:\nmBERTτ EN ES DE FR\nMN 0.27 (p=0.00)0.19 (p=0.00)-0.09 (p=0.15)0.09 (p=0.16)\nFN 0.23 (p=0.00)0.07 (p=0.24)0.01 (p=0.89)0.13 (p=0.04)\nMNN0.25 (p=0.00)0.15 (p=0.01)-0.06 (p=0.32)0.07 (p=0.28)\nFNN 0.29 (p=0.00)0.14 (p=0.01)0.03 (p=0.57)0.06 (p=0.27)\nAvg. 0.26 (p=0.00)0.14 (p=0.00)-0.03 (p=0.41)0.09 (p=0.01)\nXLM-Rτ EN ES DE FR\nMN 0.40 (p=0.00)0.43 (p=0.00)0.32 (p=0.00)0.45 (p=0.00)\nFN 0.26 (p=0.00)0.33 (p=0.00)0.43 (p=0.00)0.31 (p=0.00)\nMNN0.26 (p=0.00)0.35 (p=0.00)0.20 (p=0.01)0.29 (p=0.00)\nFNN 0.35 (p=0.00)0.45 (p=0.00)0.10 (p=0.15)0.34 (p=0.00)\nAvg. 0.32 (p=0.00)0.39 (p=0.00)0.25 (p=0.00)0.34 (p=0.00)\nmT5\nτ EN ES DE FR\nMN 0.02 (p=0.79)0.13 (p=0.06)0.13 (p=0.06)0.21 (p=0.00)\nFN -0.09 (p=0.16)0.11 (p=0.11)0.00 (p=0.98)0.12 (p=0.08)\nMNN-0.08 (p=0.21)0.10 (p=0.10)0.03 (p=0.69)0.10 (p=0.17)\nFNN -0.04 (p=0.51)0.25 (p=0.00)0.03 (p=0.61)0.10 (p=0.15)\nAvg. -0.07 (p=0.07)0.15 (p=0.00)0.05 (p=0.18)0.13 (p=0.00)\nTable 5: Correlation between groups of annotators (MN,\nFN, MNN, FNN ) and models’ predictions, classiﬁed\nby language. The degree of correlation is measured\nwith Kendall’s τ coefﬁcient ( τ ∈[−1,1]). Cells are\ncoloured language-wise. Cells with a darker back-\nground show a stronger correlation compared to the av-\nerage in each language. Samples highlighted in red fail\nto reject the null hypothesis, meaning that their differ-\nence is not statistically signiﬁcant (p >0.05).\nen [MASK] that deplete the ozone layer\nes [MASK] que agotan la capa de ozono\nde [MASK], die zum Abbau der\nOzonschicht führen\nfr [MASK] appauvrissant la couche d’ozone\n40\n 20\n 0 20 40\n40\n20\n0\n20\n40 en\nes\nde\nfr\ndim1\ndim2\nFigure 2: t-SNE representation from the last layer of\nmBERT for the top-1000 predictions for the parallel\nsentences in the list above (“We want to [MASK] in-\nnovation .” in English). Highest scored prediction is\nstarred; annotator’s answers are denoted by a dot with\nblack edge. Legend shows language-color mapping.\n40\n 20\n 0 20 40\n40\n20\n0\n20\n40\nen\nes\nde\nfr\ndim1\ndim2\nFigure 3: t-SNE representation from the last layer of\nXLM-R for the top-1000 predictions for the parallel\nsentences in the list above (“We want to [MASK] in-\nnovation .” in English). Highest scored prediction is\nstarred; annotator’s answers are denoted by a dot with\nblack edge. Legend shows language-color mapping.\n40\n 30\n 20\n 10\n 0 10 20 30 40\n40\n30\n20\n10\n0\n10\n20\n30\n40\nen\nes\nde\nfr\ndim1\ndim2\ngaz gases\ngases\nFigure 4: t-SNE representation from the last layer of\nmBERT for the top-1000 predictions for the parallel\nsentences in the list above (“[MASK] that deplete the\nozone layer” in English). The word “gases” is pointed\nout in each language (en: gases, es: gases, fr: gaz), as it\nwas a recurrent answer from different annotators. High-\nest scored prediction in each language is starred; anno-\ntator’s answers are denoted by a dot with black edge.\nLegend shows language-color mapping.\n40\n 20\n 0 20 40\n40\n20\n0\n20\n40\nen\nes\nde\nfr\ndim1\ndim2\ngasesgasesgasesgases\ngaz\nFigure 5: t-SNE representation from the last layer of\nXLM-R for the top-1000 predictions for the parallel\nsentences in the list above (“[MASK] that deplete the\nozone layer” in English). The word “gases” is pointed\nout in each language (en: gases, es: gases, fr: gaz), as it\nwas a recurrent answer from different annotators. High-\nest scored prediction in each language is starred; anno-\ntator’s answers are denoted by a dot with black edge.\nLegend shows language-color mapping.",
  "topic": "Linguistics",
  "concepts": [
    {
      "name": "Linguistics",
      "score": 0.522671639919281
    },
    {
      "name": "Computer science",
      "score": 0.5158571600914001
    },
    {
      "name": "Natural language processing",
      "score": 0.4505392909049988
    },
    {
      "name": "Artificial intelligence",
      "score": 0.38224393129348755
    },
    {
      "name": "Philosophy",
      "score": 0.09439906477928162
    }
  ],
  "cited_by": 3
}