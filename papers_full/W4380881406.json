{
  "title": "Calibration of Transformer-Based Models for Identifying Stress and Depression in Social Media",
  "url": "https://openalex.org/W4380881406",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3148694876",
      "name": "Loukas Ilias",
      "affiliations": [
        "National Technical University of Athens"
      ]
    },
    {
      "id": "https://openalex.org/A1269963241",
      "name": "Spiros Mouzakitis",
      "affiliations": [
        "National Technical University of Athens"
      ]
    },
    {
      "id": "https://openalex.org/A2111578312",
      "name": "Dimitris Askounis",
      "affiliations": [
        "National Technical University of Athens"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2927148761",
    "https://openalex.org/W2901416577",
    "https://openalex.org/W4286206444",
    "https://openalex.org/W3206212789",
    "https://openalex.org/W3100283070",
    "https://openalex.org/W3034995113",
    "https://openalex.org/W3213256305",
    "https://openalex.org/W3175604467",
    "https://openalex.org/W2970986510",
    "https://openalex.org/W4240490791",
    "https://openalex.org/W1991464023",
    "https://openalex.org/W2161009717",
    "https://openalex.org/W2137556846",
    "https://openalex.org/W6754205108",
    "https://openalex.org/W4285229780",
    "https://openalex.org/W3205486398",
    "https://openalex.org/W2344246423",
    "https://openalex.org/W3034266838",
    "https://openalex.org/W3116085416",
    "https://openalex.org/W4283771777",
    "https://openalex.org/W2987392802",
    "https://openalex.org/W4229049385",
    "https://openalex.org/W2800146313",
    "https://openalex.org/W2600145262",
    "https://openalex.org/W3169971770",
    "https://openalex.org/W4205705102",
    "https://openalex.org/W2076151421",
    "https://openalex.org/W2094553285",
    "https://openalex.org/W2953413710",
    "https://openalex.org/W4312454886",
    "https://openalex.org/W3043553083",
    "https://openalex.org/W4224326626",
    "https://openalex.org/W3171376656",
    "https://openalex.org/W4220916703",
    "https://openalex.org/W4320000454",
    "https://openalex.org/W3179564441",
    "https://openalex.org/W2797568851",
    "https://openalex.org/W4214856807",
    "https://openalex.org/W4210434817",
    "https://openalex.org/W3166185110",
    "https://openalex.org/W6772897110",
    "https://openalex.org/W4285300610",
    "https://openalex.org/W4285210591",
    "https://openalex.org/W3154219437",
    "https://openalex.org/W3207228257",
    "https://openalex.org/W4317212825",
    "https://openalex.org/W4223943285",
    "https://openalex.org/W2740966010",
    "https://openalex.org/W2905587047",
    "https://openalex.org/W3171858038",
    "https://openalex.org/W4287881512",
    "https://openalex.org/W2040467972",
    "https://openalex.org/W6736575291",
    "https://openalex.org/W6639619044",
    "https://openalex.org/W6781923719",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W6802958121",
    "https://openalex.org/W6780226713",
    "https://openalex.org/W6674330103",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W6753467366",
    "https://openalex.org/W2254249950",
    "https://openalex.org/W6761586423",
    "https://openalex.org/W6739651123",
    "https://openalex.org/W3199225381",
    "https://openalex.org/W2110065044",
    "https://openalex.org/W2146608375",
    "https://openalex.org/W6734194636",
    "https://openalex.org/W4231510805",
    "https://openalex.org/W4287687124",
    "https://openalex.org/W3101267588",
    "https://openalex.org/W2915177913",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W4214893466",
    "https://openalex.org/W2966715458",
    "https://openalex.org/W2489406233"
  ],
  "abstract": "In today's fast-paced world, the rates of stress and depression present a surge. People use social media for expressing their thoughts and feelings through posts. Therefore, social media provide assistance for the early detection of mental health conditions. Existing methods mainly introduce feature extraction approaches and train shallow machine learning (ML) classifiers. For addressing the need of creating a large feature set and obtaining better performance, other research studies use deep neural networks or language models based on transformers. Despite the fact that transformer-based models achieve noticeable improvements, they cannot often capture rich factual knowledge. Although there have been proposed a number of studies aiming to enhance the pretrained transformer-based models with extra information or additional modalities, no prior work has exploited these modifications for detecting stress and depression through social media. In addition, although the reliability of a machine learning (ML) model's confidence in its predictions is critical for high-risk applications, there is no prior work taken into consideration the model calibration. To resolve the above issues, we present the first study in the task of depression and stress detection in social media, which injects extra-linguistic information in transformer-based models, namely, bidirectional encoder representations from transformers (BERT) and MentalBERT. Specifically, the proposed approach employs a multimodal adaptation gate for creating the combined embeddings, which are given as input to a BERT (or MentalBERT) model. For taking into account the model calibration, we apply label smoothing. We test our proposed approaches in three publicly available datasets and demonstrate that the integration of linguistic features into transformer-based models presents a surge in performance. Also, the usage of label smoothing contributes to both the improvement of the model's performance and the calibration of the model. We finally perform a linguistic analysis of the posts and show differences in language between stressful and nonstressful texts, as well as depressive and nondepressive posts.",
  "full_text": "IEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS 1\nCalibration of Transformer-Based Models for\nIdentifying Stress and Depression in Social Media\nLoukas Ilias\n , Spiros Mouzakitis\n , and Dimitris Askounis\nAbstract— In today’s fast-paced world, the rates of stress\nand depression present a surge. People use social media for\nexpressing their thoughts and feelings through posts. Therefore,\nsocial media provide assistance for the early detection of mental\nhealth conditions. Existing methods mainly introduce feature\nextraction approaches and train shallow machine learning (ML)\nclassifiers. For addressing the need of creating a large feature\nset and obtaining better performance, other research studies use\ndeep neural networks or language models based on transformers.\nDespite the fact that transformer-based models achieve noticeable\nimprovements, they cannot often capture rich factual knowledge.\nAlthough there have been proposed a number of studies aim-\ning to enhance the pretrained transformer-based models with\nextra information or additional modalities, no prior work has\nexploited these modifications for detecting stress and depression\nthrough social media. In addition, although the reliability of a\nmachine learning (ML) model’s confidence in its predictions is\ncritical for high-risk applications, there is no prior work taken\ninto consideration the model calibration. To resolve the above\nissues, we present the first study in the task of depression and\nstress detection in social media, which injects extra-linguistic\ninformation in transformer-based models, namely, bidirectional\nencoder representations from transformers (BERT) and Mental-\nBERT. Specifically, the proposed approach employs a multimodal\nadaptation gate for creating the combined embeddings, which are\ngiven as input to a BERT (or MentalBERT) model. For taking\ninto account the model calibration, we apply label smoothing.\nWe test our proposed approaches in three publicly available\ndatasets and demonstrate that the integration of linguistic\nfeatures into transformer-based models presents a surge in\nperformance. Also, the usage of label smoothing contributes\nto both the improvement of the model’s performance and the\ncalibration of the model. We finally perform a linguistic analysis\nof the posts and show differences in language between stressful\nand nonstressful texts, as well as depressive and nondepressive\nposts.\nIndex Terms— Calibration, depression, emotion, mental health,\nstress, transformers.\nI. I NTRODUCTION\nA\nCCORDING\nto the World Health Organization (WHO)\n[1], stress can be defined as any type of change that\ncauses physical, emotional, or psychological strain. Stress\ncomes in a number of categories [2], namely, physical, psy-\nchological, psychosocial, and psychospiritual stress. Exces-\nManuscript received 5 January 2023; revised 28 March 2023 and 6 May\n2023; accepted 2 June 2023. (Corresponding author: Loukas Ilias.)\nThe authors are with the Decision Support Systems Laboratory, School\nof Electrical and Computer Engineering, National Technical University of\nAthens, 15780 Athens, Greece (e-mail: lilias@epu.ntua.gr; smouzakitis@\nepu.ntua.gr; askous@epu.ntua.gr).\nDigital Object Identifier 10.1109/TCSS.2023.3283009\nsive stress can lead to anxiety disorders or even depression.\nDepression entails a great number of symptoms, including loss\nof interest, anger, pessimism, changes in weight, feelings of\nworthlessness, thoughts of suicide, and many more. According\nto the WHO [3], around 280 million people in the world have\ndepression. China, India, the United States, Russia, Indonesia,\nand Nigeria are some of the countries presenting the highest\nrates of depression [4]. People with stress and depression\nuse social media platforms, including Twitter and Reddit, and\nshare their thoughts, emotions, feelings, and so on through\nposts or comments with other users. Therefore, social media\nconstitute a valuable form of information, where linguistic\npatterns of depressive/stressful posts can be investigated.\nExisting research initiatives exploit social media data for\nidentifying depressive and stressful posts. The majority of\nthese research works [5], [6] employ feature extraction\napproaches and train shallow ML algorithms. Employing\nfeature extraction approaches constitutes a tedious procedure\nand demands domain expertise, since the authors may not\nfind the optimal feature set for the specific problem. At the\nsame time, the train of shallow ML algorithms does not\nyield optimal performance and does not generalize well to\nnew data. For addressing these limitations, other approaches\n[7] use deep neural networks, including convolutional neural\nnetworks (CNNs), BiLSTMs, and so on, or transformer-based\nnetworks. In addition, there are research studies employing\nensemble strategies [8]. However, these approaches increase\nsubstantially the training time, since multiple models must be\ntrained separately. In addition, recently there have been studies\n[9], [10] showing that transformer-based models struggle or\nfail to capture rich knowledge. For this reason, there have\nbeen proposed methods for enhancing these models with\nexternal information or additional modalities [11], [12], [13],\n[14]. However, existing research initiatives in the tasks of\nstress and depression detection through social media have\nnot exploited any of these approaches yet. In addition, the\nreliability of an ML model’s confidence in its predictions,\ndenoted as calibration [15], [16], is critical for high-risk appli-\ncations, such as deciding whether to trust a medical diagnosis\nprediction [17], [18], [19]. Although methods regarding the\nconfidence of models’ predictions have been introduced in\nmany studies, including suicide risk assessment [20], sleep\nstage classification [21], and so on, no prior work for stress\nand depression detection has taken into account the level\nof confidence of models’ predictions, creating in this way\noverconfident models.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License.\nFor more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. \n2 IEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS\nTo tackle the aforementioned limitations, we propose\na method, which injects extra-linguistic information into\ntransformer-based models, namely, bidirectional encoder rep-\nresentations from transformers (BERT) and MentalBERT.\nFirst, we extract various linguistic features, including NRC\nSentiment Lexicon, features derived by latent Dirichlet allo-\ncation (LDA) topics, Top2vec, and linguistic inquiry and\nword count (LIWC) features. Regarding the LDA topic-based\nfeatures, this is the first study in terms of the tasks of stress\nand depression detection via social media texts utilizing the\nGlobal Outlier Standard Score (GOSS) [22], which captures\nthe text’s interest on a specific topic in comparison with\nother texts. After passing each text through a transformer-\nbased model, we project the linguistic information to the\nsame dimensionality as the outputs of the transformer models.\nNext, we concatenate the representations obtained by BERT\n(or MentalBERT) and linguistic information and apply a\nmultimodal adaptation gate [23], where an attention-gating\nmechanism is used for controlling the importance of each\nrepresentation. Similar to [24], we modify multimodal BERT\n(M-BERT) [23] by replacing the multimodal information\nwith linguistic information. Finally, a shifting component is\nexploited for calculating the new combined embeddings. The\nnew combined embeddings are passed through a BERT (or\nMentalBERT) model, where the classification [CLS] token is\nfed to dense layers for getting the final prediction. In addi-\ntion, for preventing models from becoming too overconfident,\nwe use label smoothing. According to Müller et al. [25],\nlabel smoothing has been used successfully to improve the\naccuracy of deep-learning models across a range of tasks,\nwhile at the same time, it implicitly calibrates learned models\nso that the confidences of their predictions are more aligned\nwith the accuracies of their predictions. We use metrics for\nassessing both the performance and the calibration of our\nmodel. We also demonstrate the efficiency of label smooth-\ning in both calibrating and enhancing the performance of\nour model. We test our proposed approaches on three pub-\nlicly available datasets, which differentiate: 1) stressful from\nnonstressful texts; 2) depressive from nondepressive posts;\nand 3) posts indicating the severity of depression, namely,\nminimal, mild, moderate, and severe. We demonstrate the\nrobustness of our model and advantages over state-of-the-art\napproaches. Finally, we conduct an extensive linguistic anal-\nysis and show common linguistic patterns between stress and\ndepression.\nOur main contributions can be summarized as follows.\n1) We introduce a method, which injects linguistic features\ninto transformer-based neural models.\n2) We perform model calibration by using label smoothing.\nWe evaluate the calibration of our approaches by using\ntwo metrics. To the best of our knowledge, this is\nthe first study exploiting label smoothing and utilizing\ncalibration metrics.\n3) We contribute to the existing literature by performing\na detailed linguistic analysis, which reveals significant\ndifferences in language between stressful/depressive and\nnonstressful/nondepressive posts.\nII. R ELATED WORK\nA. Stress Detection\nExisting research initiatives build on feature extraction and\ntrain of shallow ML algorithms. Muñoz and Iglesias [26]\nintroduced three approaches for detecting psychological stress\nin a social media text. In terms of the first approach, the\nauthors extracted affective, social, syntactic, and topic-related\nfeatures. Support vector machines (SVMs), logistic regres-\nsion (LR), and stochastic gradient descent (SGD) classifiers\nwere trained. Regarding the second approach, the authors\nemployed word embeddings, namely, word2vec, GloVe, and\nfastText, and trained the abovementioned ML classifiers. With\nregard to the third approach, the authors introduced an early\nfusion approach, where they concatenated the features and the\nword embeddings and trained the aforementioned classifiers.\nGuntuku et al. [6] extracted LIWC, topic, TensiStrength, and\nengagements, i.e., time of posts, number of posts, number\nof posts between 12 A.M.–6 A.M. features and trained lin-\near regression with regularization methods, including ridge,\nelastic-net, least absolute shrinkage and selection operator\n(LASSO), and L2 penalized SVMs. They found that elastic-net\nshowed marginally superior performance over the others. The\nauthors proposed also domain adaptation methods, including\nEasyAdapt and transfer component analysis.\nDeep learning approaches and transformer-based models\nare being used for detecting stress. For instance, Turcan and\nMcKeown [27] introduced a dataset consisting of stressful and\nnonstressful text and applied ML techniques for differentiat-\ning stressful from nonstressful texts. Specifically, the authors\nextracted lexical, syntactic, social media features, word2vec,\nand BERT embeddings and trained various ML classifiers,\nincluding SVMs, LR, Naive Bayes, perceptron, and decision\ntrees (DTs). Also, deep neural networks were trained, includ-\ning a two-layer bidirectional gated recurrent neural network\n(GRNN), CNN, and BERT. Yang et al. [28] introduced a\ndeep neural network to detect stress and depression in a social\nmedia text. Specifically, the authors exploited MentalRoBERTa\nfor obtaining token-level embeddings and commonsense trans-\nformers (COMET) for extracting mental state knowledge.\nNext, the authors exploited a GRU layer along with a scaled\ndot-product attention module. Finally, contrastive learning in\na supervised manner was employed by the authors for making\nsentences with the same label cohesive and different labels\nmutually exclusive. Also, methods for increasing the training\nset have been applied. Specifically, Winata et al. [29] proposed\n(bi)LSTMs coupled with an attention mechanism to classify\npsychological stress from self-conducted interview transcrip-\ntions. For expanding the size of the corpus, the authors applied\ndistant supervision, where they automatically labeled tweets\nbased on their hashtag content.\nHybrid models have also been proposed. For instance, Lin\net al. [30] experimented with capturing the users’ social\ninteractions in social media. Specifically, the authors pro-\nposed a deep neural network consisting of CNNs with\ncross autoencoders and a partially labeled factor graph. The\nauthors exploited both tweet-level and user-level attributes.\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. \nILIAS et al.: CALIBRATION OF TRANSFORMER-BASED MODELS FOR IDENTIFYING STRESS AND DEPRESSION 3\nIn terms of the tweet-level attributes, they exploited linguistic,\ni.e., positive and negative emotion words, punctuation marks,\nemoticons, and so on, visual, i.e., saturation, brightness, five-\ncolor theme, and so on, and social features, i.e., number of\ncomments, retweets, and likes. Regarding user-level attributes,\nthey extracted features pertinent to their posting behavior and\nsocial interaction.\nEmotion-enhanced approaches in conjunction with explain-\nability techniques have been introduced. Turcan et al. [31]\nproposed three emotion-enhanced models that incorporate\nemotional information in various ways to enhance the task\nof binary stress prediction. In terms of the first approach,\nthe authors exploited two single-task models sharing the same\nBERT representation layers. Specifically, the authors trained\nthe stress task with the Dreaddit data and the emotion task with\nthe GoEmotions or Vent data. Regarding the second model, the\nauthors proposed a multitask learning framework, where the\ntwo tasks were performed at the same time. With regards to\nthe third model, the authors fine-tune first a BERT model\nusing the emotion task and then apply this fine-tuned BERT\nmodel to stress detection. Finally, the authors introduced a new\nframework for model interpretation using local interpretable\nmodel-agnostic explanations (LIME).\nB. Depression Detection\nSome studies have focused on the extraction of features\nand then the train of shallow ML classifiers. For instance,\nTadesse et al. [5] extracted n-grams via the tf–idf approach,\nLIWC features, and LDA topics. Then, they trained LR, SVM,\nrandom forest (RF), AdaBoost, and multilayer perceptron\n(MLP). Results showed that the bigram features trained on\nan SVM classifier achieved 80.00% accuracy, while the best\naccuracy accounting for 91.00% was achieved by exploiting\nthe MLP classifier with all the features, i.e., LIWC, LDA,\nand bigrams. Liu and Shi [32] extracted a set of textual\nfeatures, namely, part-of-speech, emotional words, personal\npronouns, polarity, and so on, and a set of features indicating\nthe posting behavior of the user, i.e., posting habits and\ntime. Next, feature selection techniques were applied, includ-\ning recursive elimination, mutual information, and extreme\nrandom tree. Finally, Naive Bayes, the k-nearest neighbor,\nregularized LR, and SVM were used as base learners, and\na simple LR algorithm was used as a combination strategy\nto build a stacking model. Nguyen et al. [33] extracted a\nset of features, including LDA topics, LIWC features, and\naffective features by using the affective norms for english\nwords (ANEW) lexicon, and mood labels. The authors trained\na LASSO regression classifier for detecting depressive posts\nand analyzing the importance of each feature. The authors\napplied also statistical tests and found significant differences\nbetween depressive and nondepressive posts. Tsugawa et al.\n[34] extracted features and trained an SVM classifier to detect\ndepression in Twitter. Specifically, the authors extracted the\nfrequency of words used in tweets, the ratio of tweet topics\nfound by LDA, the ratio of positive and negative words, and\nmany more. Pirina and Çöltekin [35] collected several corpora\nand trained an SVM classifier using character and word\nn-grams. Doc2vec and tf–idf features were extracted and given\nas input to AdaBoost, LR, RF, and SVM for identifying the\nseverity of depression.\nRecently, deep learning approaches have been introduced,\nsince they obtain better performance than the traditional ML\nalgorithms and do not often require the tedious procedure of\nfeature extraction. For example, Wani et al. [36] represented\nwords as word2vec and tf–idf approach and trained a deep\nneural network consisting of CNNs and LSTMs. Kim et al.\n[37] collected a dataset consisting of posts written by peo-\nple, who suffer from mental disorders, including depression,\nanxiety, bipolar, borderline personality disorder, schizophrenia,\nand autism. This study developed six binary classification\nmodels for detecting mental disorders, i.e., depression versus\nnondepression, and so on. Specifically, the authors utilized\nthe tf–idf approach and trained an XGBoost classifier. Next,\nthe authors used the word2vec and trained a CNN model.\nNaseem et al. [38] reformulated depression identification\nas an ordinal classification problem, where they used four\ndepression severity levels. The authors introduced a deep\nneural network consisting of a text graph convolutional net-\nwork, bidirectional long short-term memory (BiLSTM), and\nattention layer. A similar approach was proposed by Ghosh\nand Anwar [39], where the authors extracted features and\ntrained LSTMs for estimating the depression intensity levels.\nA hybrid deep neural network consisting of CNN and BiLSTM\nwas introduced by Kour and Gupta [40]. Zogan et al. [41]\nintroduced the first dataset including posts from users with\nand without depression during COVID-19 and presented a new\nhierarchical CNN. An emotion-based attention network model\nwas proposed by Ren et al. [42], where the authors extracted\nthe positive and negative words and passed them through two\nseparate BiLSTM layers followed by attention layers.\nEnsemble strategies have also been explored in the litera-\nture. This means that multiple models are trained separately\nand the final decision is taken usually by a majority voting\napproach. For instance, an ensemble strategy was introduced\nby Ansari et al. [8]. First, the authors exploited some senti-\nment lexicons, including AFINN, NRC, SenticNet, and multi-\nperspective question answering (MPQA), extracted features,\nand applied principal component analysis for reducing the\ndimensionality of the feature set. An LR classifier was trained\nusing the respective feature set. Next, the authors trained\nan LSTM neural network coupled with an attention mecha-\nnism. Finally, the authors combined the predictions of these\ntwo approaches via an ensemble method. Also, an ensemble\napproach was proposed by Trotzek et al. [43]. First, the\nauthors trained an LR classifier using input user-level linguistic\nmetadata. Specifically, the authors extracted LIWC features,\nthe length of the text, four readability scores, and so on.\nNext, the authors trained a CNN model. Finally, the authors\ncombined the outputs of these approaches via a late fusion\nstrategy, i.e., by averaging the predictions of the classifiers.\nFiguerêdo et al. [7] designed a CNN along with early and late\nfusion strategies. Specifically, the authors exploited fastText\nand GloVe embeddings. In the early fusion approach, multiple-\nword embeddings were concatenated and passed to the CNN\nmodel. In the late fusion strategy, a majority-vote approach\nwas performed based on the predictions of multiple CNN\nmodels. The CNN model comprised a simple convolution\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. \n4 IEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS\nlayer, max-pooling, fully connected layers, and concatenated\nrectified linear units as the activation function.\nExplainable approaches have also been introduced. Souza\net al. [44] introduced a stacking ensemble neural network,\nwhich addresses a multilabel classification task. Specifically,\nthe proposed architecture consists of two levels. In the first\nlevel, binary base classifiers were trained with two distinct\nroles, i.e., expert and differentiating. The expert base classifiers\nwere used for differentiating between users belonging to the\ncontrol group and those diagnosed with anxiety, depression,\nor comorbidity. The differentiating base models aimed at dis-\ntinguishing between two target conditions, e.g., anxiety versus\ndepression. In the second level, a meta-classifier uses the base\nmodels’ outputs to learn a mapping function that manages\nthe multilabel problem of assigning control or diagnosed\nlabels. The authors used LSTMs and CNNs. Finally, this study\nexplored Shapley additive explanations (SHAP) metrics for\nidentifying the influential classification features. Zogan et al.\n[45] proposed also an explainable approach, where textual,\nbehavioral, temporal, and semantic aspect features from social\nmedia were exploited. A hierarchical attention network was\nused in terms of explainable purposes. A hierarchical attention\nnetwork was also used by Uban et al. [46], where the authors\nextracted a feature set consisting of content, style, LIWC, and\nemotions/sentiment features. An interpretable approach was\nproposed by Song et al. [47], where the authors introduced\nthe feature attention network. The feature attention network\nconsists of four feature networks, each of which analyzes\nposts based on an established theory related to depression and\na postlevel attention on top of the networks. However, this\nmethod did not attain satisfactory results.\nRecently, transformer-based models have been applied to\nthe task of depression detection in social media. Specifically,\nBoinepelli et al. [48] introduced a method for finding the\nsubset of posts that would be a good representation of all\nthe posts made by the user. First, they employed BERT and\ncomputed the embeddings for all posts made by the user.\nNext, they used a clustering and ranking algorithm. After\nfinding the representative posts per user, the authors added\ndomain-specific elements by exploiting RoBERTa. Finally, the\nauthors experimented with two ways of diagnosing depres-\nsion, i.e., by either employing a majority-vote approach or\ntraining a hierarchical attention network. Anantharaman et al.\n[49] fine-tuned a BERT model for classifying the signs\nof depression into three labels, namely, “not depressed,”\n“moderately depressed,” and “severely depressed.” Similarly,\nNilsson and Kovács [50] exploited a BERT model and\nused abstractive summarization techniques for data augmen-\ntation. Zogan et al. [51] presented an abstractive–extractive\nautomatic text summarization model based on BERT, k-\nmeans clustering, and bidirectional auto-regressive transform-\ners (BART). Then, they proposed a deep learning framework,\nwhich combines user behavior and user post history or user\nactivity.\nMultimodal approaches combining both text and images\nhave also been proposed. For instance, a multimodal approach\nwas introduced by Ghosh et al. [52] for detecting depression\nin Twitter. Specifically, the authors utilized the user’s descrip-\ntion and profile image. The authors used the IBM Watson\nNaturalLanguageUnderstanding tool and extracted sentiment\nand emotion information for all user descriptions along with\nthe possible categories (at most three) that the description\nmay belong to. Next, the authors designed a neural network\nconsisting of BiGRU, attention layers, convolution layers,\nand dense layers. The authors used GloVe embeddings. The\nproposed architecture can predict whether the user suffers\nfrom depression or not as well as predict the sadness, joy,\nfear, disgust, and anger score. Li et al. [53] exploited text,\npictures, and auxiliary information (post time, dictionary, and\nsocial information) and used attention mechanisms within\nand between the modalities at the same time. The authors\nexploited TextCNN, ResNet-18, and fully connected layers for\nextracting representation vectors of text, images, and auxiliary\ninformation, respectively. A multimodal approach was pro-\nposed by Cheng and Chen [54], where the authors exploited\ntexts, images, posting time, and the time interval between\nthe posts on Instagram. Shen et al. [55] collected multimodal\ndatasets and extracted six depression-oriented feature groups,\nnamely, social network, user profile, visual, emotional, topic-\nlevel, and domain-specific features. Gui et al. [56] combined\ntexts and images and proposed a new cooperative multiagent\nreinforcement learning method.\nMultitask approaches have been introduced. A multitask\napproach was introduced by Zhou et al. [57]. Specifically, the\nauthors proposed a hierarchical attention network consisting\nof BiGRU layers and integrated LDA topics. The main task\nwas the identification of depression, i.e., binary classification\ntask, while the auxiliary task was the prediction of the domain\ncategory of the post, i.e., multiclass classification task. Both\nmultitask and multimodal approaches were introduced by\nWang et al. [58]. The authors extracted a total of ten features\nfrom the text, social behavior, and pictures. XLNet and BiGRU\ncoupled with attention layers, and dense layers were used.\nC. Related Work Review Findings\nExisting research initiatives rely on the feature extrac-\ntion process and the train of shallow machine classifiers\ntargeting diagnosing mental disorders in social media. This\nfact demands domain expertise and does not generalize well\nto new data. Other existing approaches train CNNs, BiL-\nSTMs, or employ hybrid models and ensemble strategies.\nRecently, transformer-based models have been used also. Only\na few works have experimented with injecting linguistic,\nincluding emotion, features into deep neural networks. These\napproaches employ multitask learning models, fine-tuning,\nor multimodal approaches. All these approaches employ-\ning transformer-based models usually fine-tune these models.\nNone of these approaches have used modifications of BERT\naiming to enhance its performance by injecting into it external\nknowledge. Also, no prior work has taken into account model\ncalibration creating in this way overconfident models.\nTherefore, our work differs from the existing research\ninitiatives, since we: 1) present a new method, which injects\nlinguistic features into transformer-based models; 2) apply\nlabel smoothing for calibrating our model and evaluate both\nthe performance and calibration of our proposed models; and\n3) present a methodology for gaining linguistic insight into\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. \nILIAS et al.: CALIBRATION OF TRANSFORMER-BASED MODELS FOR IDENTIFYING STRESS AND DEPRESSION 5\nthe tasks of depression and stress investigating their common\ncharacteristics.\nIII. M ETHODOLOGY\nA. Architecture\nIn this section, we describe our proposed approach for\ndetecting stressful and depressive posts on social media.\nOur proposed method is based on the work introduced by\nRahman et al. [23] and Jin and Aletras [24]. Instead of cross-\nmodal interactions, we inject extra-linguistic information as\nalternative views of the data into pretrained language models.\nOur proposed architecture is illustrated in Fig. 1.\nSpecifically, we use the following feature vectors.\n1) NRC: The NRC Emotion Lexicon is a list of English\nwords and their associations with eight basic emotions\n(anger, fear, anticipation, trust, surprise, sadness, joy, and\ndisgust) and two sentiments (negative and positive) [59].\nEach text is represented as a 10-d vector, where each\nelement is the proportion of tokens belonging to each\ncategory.\n2) LIWC: LIWC is a dictionary-based approach to count\nwords in linguistic, psychological, and topical categories\n[60]. We use LIWC 2022 [61] to represent each text as\na 117-d vector.\n3) LDA Topics: Before training the LDA model, we remove\nstop words and punctuation. We exploit LDA (with\n25 topics) and extract 25 topic probabilities per text [62].\nThese probabilities describe the topics of interest in each\ntext. Inspired by Liu et al. [22], we use the following\nfeature vector.\na) Global Outlier Standard Score: For evaluating the\nith text’s interest on a certain topic k, compared to\nthe rest of the texts, we use the GOSS feature\nµ(xk ) =\n∑n\ni=1 xik\nn (1)\nGOSS(xik) = xik − µ(xk )√∑\ni (xik − µ(xk ))2\n. (2)\nTherefore, each text is represented as a 25-d vector.\n4) Top2Vec: Top2Vec [63] is an algorithm for topic mod-\neling, which automatically detects topics present in the\ntext and generates jointly embedded topic, document,\nand word vectors. After training Top2Vec by exploiting\nthe universal sentence encoder, each text is represented\nas a 512-d vector.\nWe experiment with the following pretrained models: BERT\n[64] and MentalBERT [65].\nFirst, we pass each text through the aforementioned\ntransformer-based models. Let C ∈ RN×d be the output of\nthe transformer-based models, where N denotes the sequence\nlength, while d denotes the dimensionality of the models.\nWe have omitted the dimension corresponding to the batch\nsize for the sake of simplicity.\nThen, we project the feature vectors to dimensionality equal\nto 128. We repeat the feature vector N times, so as to ensure\nthat the feature vector and the output of the transformer-based\nmodels can be concatenated. Given the word representation\ne(i), we concatenate e(i) with feature vectors, i.e., h(i)\nv\nw(i)\nv = σ\n(\nWhv\n[\ne(i); h(i)\nv\n]\n+ bv\n)\n(3)\nwhere σ denotes the sigmoid activation function, Whv is a\nweight matrix, and w(i)\nv corresponds to the gate. bv is the scalar\nbias.\nNext, we calculate a shift vector h(i)\nm by multiplying the\nembeddings with the gate\nh(i)\nm = w(i)\nv ·\n(\nWvh(i)\nv\n)\n+ b(i)\nm (4)\nwhere Wv is a weight matrix, and b(i)\nm is the bias vector.\nNext, we apply the multimodal shifting component aiming\nto dynamically shift the word representations by integrating\nthe shift vector h(i)\nm into the original word embedding\ne(i)\nm = e(i) + αh(i)\nm (5)\nα = min\n(\n∥e(i)∥2\nh(i)\nm\n\n\n2\nβ, 1\n)\n(6)\nwhere β is a hyperparameter. Then, we apply a layer normal-\nization [66] and dropout layer [67] to e(i)\nm . Next, the combined\nembeddings are fed to a BERT/MentalBERT model.\nWe get the classification [CLS] token of this model and pass\nit through a dense layer consisting of 128 units with a ReLU\nactivation function. Finally, we use a dense layer consisting\nof either two units (binary classification task) or four units\n(multiclass classification task).\nWe denote our proposed models as M-BERT and multi-\nmodal MentalBERT (M-MentalBERT) followed by the lin-\nguistic features which are integrated into them. For example,\nthe injection of LIWC features into a BERT model is denoted\nas M-BERT (LIWC).\nB. Model Calibration\nTo prevent the model from becoming too overconfident,\nwe use label smoothing [25], [68]. Specifically, label smooth-\ning calibrates learned models so that the confidences of their\npredictions are more aligned with the accuracies of their\npredictions.\nFor a network trained with hard targets, the cross-entropy\nloss is minimized between the true targets yk and the network’s\noutputs pk , as in H(y, p) = ∑K\nk=1 −yk log(pk ), where yk\nis “1” for the correct class and “0” for the other. For a\nnetwork trained with label smoothing, we minimize instead\nthe cross-entropy between the modified targets yLSu\nk and the\nnetwork’s outputs pk\nyLSu\nk = yk · (1 − α) + α\nK (7)\nH(y, p) =\nK∑\nk=1\n−yLSu\nk · log(pk ) (8)\nwhere α is the smoothing parameter, and K is the number of\nclasses.\nIV. E XPERIMENTS\nA. Datasets\n1) Dreaddit: This dataset includes stressful and nonstressful\ntexts posted by users of Reddit [27]. Specifically, these posts\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. \n6 IEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS\nFig. 1. Our proposed architecture.\nbelong to five domains, namely, abuse, anxiety, financial,\nsocial, and posttraumatic stress disorder (PTSD). A subset\nof data has been annotated using Amazon Mechanical Turk.\nThis dataset includes also lexical, syntactic, and social media\nfeatures per post. The dataset has been divided by the authors\ninto a train and a test set. The train set comprises 1488 stressful\ntexts and 1350 nonstressful ones, while the test set includes\n369 stressful texts and 346 nonstressful ones.\n2) Depression_Mixed: This dataset [35] consists of\n1482 nondepressive posts and 1340 depressive posts. These\nposts have been written by users on Reddit and English\ndepression forums.\n3) Depression_Severity: This dataset includes posts in Red-\ndit [38] and assigns each post to a severity level, i.e., minimal\n(2587 posts), mild (290 posts), moderate (394 posts), and\nsevere form of depression (282 posts).\nB. Experimental Setup\nWe use the Adam optimizer with a learning rate of 0.001.\nWe apply StepLR with a step size of 5 and a gamma of\n0.1. We use a batch size of 8. With regards to Depres-\nsion_Mixed dataset, we split the dataset into a train and a\ntest set (80%–20%) similar to Ansari et al. [8]. In terms\nof Dreaddit dataset, we use the test set provided by Turcan\nand McKeown [27]. Regarding Depression_Severity dataset,\nwe use fivefold stratified cross-validation, since the study\n[38] has also exploited cross-validation. All train sets are\ndivided into a train and a validation set. Regarding Depres-\nsion_Severity dataset, we apply EarlyStopping with patience\nof seven epochs based on the validation loss. In terms of\nDepression_Mixed and Dreaddit dataset, we train our intro-\nduced model for a maximum of 30 epochs, choose the epoch\nwith the smallest validation loss, and test the model on the\ntest set. We set β of (6) equal to 0.0001. 1 We choose α\nof (7) equal to 0.001. We use the Python library, namely,\ntransformers [69], for BERT and MentalBERT. Specifically,\nwe use the BERT base uncased version and the MentalBERT\nbase uncased version. We use PyTorch [70] for performing\nour experiments. All experiments are trained on a single Tesla\nP100-PCIE-16GB GPU.\nC. Evaluation Metrics\n1) Performance: In terms of the binary classification tasks,\ni.e., 0 for nonstressful and 1 for stressful texts or 0 for\nnondepressive and 1 for depressive texts, we use precision,\nrecall, F1-score, and accuracy to evaluate the performance\n1We experimented with values of β, including 0.01 and 0.001, but setting\nβ equal to 0.0001 yielded the best results.\nof our proposed approach. We use these metrics similar to\nWani et al. [36].\nRegarding the multiclass classification task reported on\nthe Depression_Severity dataset, we use weighted precision,\nweighted recall, and weighted F1-score. We use these metrics\nsimilar to Mishra et al. [71].\n2) Calibration: We evaluate the calibration of our model\nusing the metrics proposed by the relevant literature [72], [73],\n[74]. Specifically, we use the metrics mentioned as follows.\n1) Expected Calibration Error (ECE): The calibration error\nis the difference between the fraction of predictions in\nthe bin that are correct (accuracy) and the mean of the\nprobabilities in the bin (confidence). First, we divide the\npredictions into M equally spaced bins (size 1/ M)\nacc(Bm ) = 1\n|Bm |\n∑\ni∈Bm\n1( ˆyi = yi ) (9)\nconf(Bm ) = 1\n|Bm |\n∑\ni∈Bm\nˆpi (10)\nwhere yi and ˆyi are the true and predicted labels for the\nsample i, and ˆpi is the confidence (predicted probability\nvalue) for sample i\nECE =\nM∑\nm=1\n|Bm |\nN |acc(Bm ) − conf(Bm )| (11)\nwhere N is the total number of data points, and Bm is\nthe group of samples whose predicted probability values\nfall into the interval Im = [((m− 1)/M), (m/M)].\nPerfectly calibrated models have an ECE of 0.\n2) Adaptive Calibration Error (ACE): It uses an adaptive\nscheme that spaces the bin intervals so that each contains\nan equal number of predictions\nACE = 1\nK R\nK∑\nk=1\nR∑\nr=1\n|acc(r, k) − conf(r, k)| (12)\nwhere acc( r, k) and conf( r, k) are the accuracy and\nconfidence of adaptive calibration range r for class\nlabel k, respectively, and N is the total number of data\npoints. Calibration ranges r are defined by the [ N/R]th\nindex of the sorted and thresholded predictions.\nD. Baselines\nWe use the following baselines as comparisons with our\nproposed approaches.\n1) BERT, MentalBERT: We fine-tune these pretrained lan-\nguage models in order to explore whether our method of\ninjecting linguistic information into pretrained models\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. \nILIAS et al.: CALIBRATION OF TRANSFORMER-BASED MODELS FOR IDENTIFYING STRESS AND DEPRESSION 7\nTABLE I\nPERFORMANCE COMPARISON AMONG PROPOSED MODELS AND BASELINES USING THE DEPRESSION _MIXED AND DREADDIT DATASETS\nTABLE II\nPERFORMANCE COMPARISON AMONG PROPOSED MODELS AND\nBASELINES USING THE DEPRESSION _ SEVERITY DATASET\nleads to performance improvement. In terms of the\nDepression_Mixed dataset, we report the performance\nof BERT obtained by Yang et al. [28]. We fine-tune\nMentalBERT and report its performance on this dataset.\nWith regards to the Dreaddit dataset, we report the per-\nformance of BERT and MentalBERT obtained by Turcan\nand McKeown [27] and Ji et al. [65], respectively.\nRegarding the Depression_Severity dataset, we fine-tune\nBERT and MentalBERT and report their performances.\nWe do not report calibration metrics for these mod-\nels, since our goal, in this case, is to compare only\nthe performances of these models with our proposed\napproaches.\n2) Proposed Approaches (Without Label Smoothing): We\ntrain the proposed models introduced in Section III\nwithout label smoothing. We explore whether label\nsmoothing leads to performance improvement and better\ncalibration of our models.\nV. R ESULTS\nThe results of our proposed approach are reported in\nTables I and II. Specifically, Table I reports the performances\nof our proposed approaches on the Depression_Mixed and\nDreaddit datasets, while Table II reports the results on the\nDepression_Severity dataset.\nIn terms of the Dreaddit dataset, we first compare our\nproposed approaches without label smoothing with the BERT\nand MentalBERT models. First, we observe that the integration\nof linguistic features into the BERT model improves the per-\nformance obtained by BERT. Specifically, M-BERT (LIWC)\nyields the highest F1-score accounting for 81.95% surpassing\nBERT by 1.30%. At the same time, M-BERT (LIWC) outper-\nforms M-BERT (NRC), M-BERT (LDA topics), and M-BERT\n(top2vec) in precision, F1-score, and accuracy. Similarly, the\ninjection of LIWC features into the MentalBERT model yields\nan F1-score of 82.77% outperforming MentalBERT by 2.73%\nand the other approaches by 0.46%–1.17%. M-MentalBERT\n(NRC) yields the lowest F1-score accounting for 81.60%. With\nregards to the proposed approaches with label smoothing,\nwe observe that they outperform the proposed approaches\nwithout label smoothing in terms of both performance and\ncalibration metrics. Specifically, we observe that M-BERT\n(LIWC) with label smoothing attains the highest F1-score\nand accuracy accounting for 83.10% and 81.12%, respec-\ntively, outperforming M-BERT (LIWC) without label smooth-\ning in F1-score by 1.15% and in accuracy by 0.28%. The\nintegration of LIWC features into the MentalBERT model\nwith label smoothing obtains better performance than Men-\ntalBERT in F1-score by 3.36% and better performance than\nM-MentalBERT (LIWC) without label smoothing in F1-score\nby 0.63%. In addition, we observe that label smoothing leads\nto improvement in calibration metrics. For instance, M-BERT\n(NRC) with label smoothing improves both ECE and ACE by\n0.023 and 0.020, respectively, in comparison with M-BERT\n(NRC) without label smoothing.\nRegarding the Depression_Mixed dataset, we first compare\nour proposed approaches without label smoothing with the\nBERT and MentalBERT models. We observe that the injection\nof linguistic features, except for NRC features, into the BERT\nmodel improves the F1-score. Specifically, we observe that\nthe injection of top2vec features yields the highest F1-score\nand accuracy accounting for 91.97% and 92.21%, respectively,\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. \n8 IEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS\nsurpassing the performance of the BERT model in F1-score\nby 0.57%. We speculate that the injection of top2vec fea-\ntures obtains better performance than the injection of features\nderived by LDA topics, i.e., GOSS features since the top2vec\nalgorithm is capable of identifying the number of topics\nautomatically. In terms of MentalBERT, we observe that the\ninjection of top2vec features obtains an F1-score of 92.69%\nsurpassing MentalBERT by 1.52%. We observe that the inte-\ngration of NRC and top2vec features improve the performance\nobtained by MentalBERT. Regarding the proposed approaches\nwith label smoothing, we observe that these models attain bet-\nter performances than the ones obtained by the models with-\nout label smoothing. Specifically, we observe that M-BERT\n(top2vec) with label smoothing surpasses the respective model\nwithout label smoothing in F1-score and Accuracy by 0.61%\nand 0.36% respectively. Similarly, M-MentalBERT (top2vec)\nwith label smoothing obtains the highest F1-score and accu-\nracy accounting for 93.06% and 93.45%, respectively. This\nmodel surpasses the respective model without label smoothing\nin F1-score and accuracy by 0.37% and 0.18%. Except for\nthe improvement of the performance metrics, i.e., precision,\nrecall, F1-score, and accuracy, we observe that the models with\nlabel smoothing obtain better results in terms of the calibration\nmetrics, i.e., ECE and ACE, than the ones obtained by the\nmodels without label smoothing. For example, we observe that\nM-BERT (top2vec) with label smoothing improves the ECE\nand ACE scores obtained by M-BERT (top2vec) without label\nsmoothing by 0.008 and 0.013, respectively. Similarly, M-\nMentalBERT (LDA topics) with label smoothing improves the\nECE and ACE scores obtained by M-MentalBERT (LDA top-\nics) without label smoothing by 0.042 and 0.043, respectively.\nWith regards to the Depression_Severity dataset, we first\ncompare our proposed approaches without label smoothing\nwith the BERT and MentalBERT models. We observe that the\nintegration of LIWC features and features extracted by LDA\ntopic modeling, i.e., GOSS features, into the BERT model\nleads to a performance surge in comparison with the BERT\nmodel. Specifically, M-BERT (LIWC) outperforms BERT in\nweighted F1-score by 1.13%. At the same time, the integration\nof all the features, except NRC, to a MentalBERT model\nyields a performance improvement compared to the Men-\ntalBERT model. Specifically, M-MentalBERT (LDA topics)\nattains the highest weighted F1-score accounting for 72.58%\nsurpassing MentalBERT by 0.91%. When it comes to proposed\nmodels with label smoothing, we observe an improvement\nin both the performance metrics and calibration ones. More\nspecifically, the integration of NRC features to a BERT\nmodel obtains a weighted F1-score of 72.81% outperforming\nBERT by 1.81%, M-BERT (NRC) without label smoothing\nby 2.85%, and M-BERT (LIWC) without label smoothing by\n0.68%. In addition, M-MentalBERT (LDA topics) with label\nsmoothing obtains the highest F1-score accounting for 73.16%\nsurpassing MentalBERT by 1.49% and M-MentalBERT (LDA\ntopics) without label smoothing by 0.58%. In terms of the\ncalibration metrics, we observe that both ECE and ACE scores\nare improved when we apply label smoothing. For example,\nM-BERT (LIWC) with label smoothing obtains an ECE score\nof 0.094 and an ACE score of 0.069, which are improved by\n0.016 and 0.009, respectively, compared with the respective\nmodel without label smoothing.\nVI. L INGUISTIC ANALYSIS\nWe finally perform an analysis on the Depression_Mixed\nand Dreaddit datasets to uncover the peculiarities of stress\nand depression. Specifically, we seek to find the correlations\nof LIWC features with stressful/depressive and nonstress-\nful/nondepressive texts. To do this, we adopt the methodol-\nogy by Ilias and Askounis [75]. First, we normalize LIWC\nfeatures, so as to ensure that they sum up to 1 across each\npost. Next, we use the point-biserial correlation between each\nLIWC category and the label of the post. The output of the\npoint-biserial correlation is a number ranging from −1 to 1.\nPositive correlations mean that the specific LIWC category is\ncorrelated with the stressful/depressive class (label 1), while\nnegative correlations mean that the specific LIWC category is\ncorrelated with the nonstressful/nondepressive class (label 0).\nWe consider the absolute values of the correlations. Results\nare reported in Table III. All the correlations are significant\nat p < 0.05 with Benjamini–Hochberg correction [76] for\nmultiple comparisons.\nIn terms of the Depression_Mixed dataset, we observe that\nthe control group tends to use words with positive tone and\nemotion, i.e., good, well, happy, hope, and so on. In addition,\nthe healthy control group discusses topics of everyday life,\nincluding lifestyle (work, home, and school), culture (car and\nphone), politics (govern and congress), family, and friends\n(boyfriend, girlfriend, and dude). Also, these people make\nplans for the future, thus using words indicating a focus on\nthe future (correlation equal to 0.0666). However, it must be\nnoted that this is a very weak correlation. On the other hand,\npeople with depression focus on the present and do not make\nplans for the future. They discuss negative topics, including\ndeath, illnesses, mental health, and substances. This can be\njustified by the fact that people with depression often have\ntendencies to suicide and believe that they cannot achieve\nanything. In addition, they use swear words, i.e., shit, fuck,\nand damn, since they think that everything goes wrong in their\nlife. Also, their posts are full of sadness, anxiety, and negative\ntone.\nRegarding the Dreaddit dataset, we observe similar patterns.\nSpecifically, nonstressful posts include words with positive\ntone and emotion. People in nonstressful conditions discuss\ntopics pertinent to lifestyle, money, leisure, technology, food,\nwork, religion, and many more. Also, people use personal\npronouns, including third-person singular, second-person, and\nfirst-person plural. Also, their posts include words indicating\npoliteness. On the contrary, people in stressful conditions\nuse swear words or words indicating interpersonal conflict,\nincluding fight, kill, attacking, and so on. The first person\nsingular constitutes the LIWC category with the highest degree\nof correlation with the stressful class. This result agrees with\nprevious work [77], where it is mentioned that self-references\nby individuals present a surge in emotionally vulnerable con-\nditions. Therefore, the use of the first-person singular indicates\nincreased self-focus. In addition, similar to the depressive\nposts, we observe that stressful posts include words with\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. \nILIAS et al.: CALIBRATION OF TRANSFORMER-BASED MODELS FOR IDENTIFYING STRESS AND DEPRESSION 9\nTABLE III\nLIWC F EATURES ASSOCIATED WITH DEPRESSIVE /STRESSFUL AND NONDEPRESSIVE /NONSTRESSFUL POSTS , SORTED BY POINT -BISERIAL\nCORRELATION . ALL CORRELATIONS ARE SIGNIFICANT AT p < 0.05 A FTER BENJAMINI –HOCHBERG CORRECTION\nnegative emotions, anger, sadness, and anxiety. Also, topics\ndiscussed by these users are pertinent to illness, health, and\ndeath.\nVII. D ISCUSSION\nOur study contributes to the literature by introducing the\nfirst approach of integrating extra-linguistic information into\npretrained language models based on transformers, namely,\nBERT and MentalBERT. Specifically, we adapt M-BERT [23]\nby replacing multimodal information with linguistic informa-\ntion. To be more precise, we extract NRC, LIWC, features\nderived by LDA topics, and top2vec features. We apply\na multimodal adaptation gate and exploit also a shifting\ncomponent for creating new combined embeddings which\nare given as input to BERT (and MentalBERT) models.\nIn addition, motivated by the fact that in real-world decision-\nmaking systems, classification networks must not only be\naccurate but also should indicate when they are likely to\nbe incorrect, we apply label smoothing and evaluate our\nproposed approaches both in terms of classification and\ncalibration.\nTherefore, our study is different from the state-of-the-art\napproaches described in Section II.\n1) Prior works having proposed multimodal, multitask, and\nensemble strategies in conjunction with transformer-\nbased models, have just fine-tuned these pretrained\ntransformer-based models instead of using some mod-\nifications of them. Thus, this study is the first attempt to\ninject extra knowledge into BERT (and MentalBERT),\nin order to enhance its performance.\n2) All the prior works evaluate only the classification per-\nformance of their approaches neglecting the confidence\nof the prediction. To tackle this, this is the first study in\nthe task of stress and depression detection through social\nmedia posts utilizing label smoothing and evaluating\nboth the classification performance and the calibration\nof the models.\n3) Finally, this is the first study utilizing features derived\nfrom LDA topics, namely, the GOSS, which captures\nthe text’s interest compared to other texts.\nFrom the results of this study, we found the following.\n1) Finding 1: The integration of linguistic features into\ntransformer-based models yields an increase in classi-\nfication performance. However, it is worth noting that\nin some cases this improvement is limited. For instance,\nthe integration of LIWC features into the MentalBERT\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. \n10 IEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS\nmodel with label smoothing obtains better performance\nthan MentalBERT in F1-score by 3.36% and better\nperformance than M-MentalBERT (LIWC) without label\nsmoothing in F1-score by 0.63%. However, we believe\nthat even a small improvement can make a difference.\n2) Finding 2: Label smoothing improves both the perfor-\nmance and the calibration of the proposed approaches.\nThe calibration of the proposed approaches is measured\nvia two metrics, namely, ECE and ACE.\n3) Finding 3: Findings from a linguistic analysis revealed\nthat people in stressful and/or depressive conditions\nuse words belonging to specific LIWC categories more\nfrequently than others.\nThere are several limitations related to this study.\n1) Hyperparameter Tuning: Due to limited access to GPU\nresources, we were not able to perform hyperparame-\nter tuning. On the contrary, we tried some combina-\ntions of parameters. We believe that the adoption of\nthe hyperparameter tuning procedure through access to\nGPU resources would increase further the classification\nperformance.\n2) Explainability: The present study is not accompanied by\nexplainability techniques, i.e., integrated gradients [78],\nand so on. Therefore, we aim to apply explainability\ntechniques in the future.\n3) Due to limited access to GPU resources and similar to\nprior work [8], [36], [39], we were not able to perform\nmultiple runs for testing for statistical significance in\nterms of the Depression_Mixed and Dreaddit datasets.\nVIII. C ONCLUSION\nIn this article, we present a new method for identify-\ning stress and depression in social media text by injecting\nlinguistic information into transformer-based models. Also,\nit is the first study exploiting label smoothing, in order to\nensure that our model is calibrated. We evaluate our proposed\nmethods on three publicly available datasets, which include\na depression detection dataset (binary classification), a stress\ndetection dataset, and a depression detection dataset (mul-\nticlass classification—severity of depression). Findings sug-\ngest that transformer-based networks combined with linguistic\ninformation lead to performance improvement in comparison\nwith transformer-based networks. Also, applying label smooth-\ning yields both the performance improvement and better\ncalibration of the proposed models. Specifically, in terms of\nthe Depression_Mixed dataset, we found that the injection\nof top2vec features into BERT and MentalBERT models\nalong with label smoothing obtained the highest F1-score\nand accuracy. Regarding the Dreaddit dataset, results showed\nthat the integration of LIWC features into language models\nbased on transformers in conjunction with label smoothing\nyielded the highest F1-score and accuracy. With regards to the\nDepression_Severity dataset, findings showed that the injection\nof NRC features into the BERT model and the integration\nof features derived by LDA topics, namely, GOSS features,\ninto the MentalBERT model yielded the highest weighted\nF1-scores. We also conduct a linguistic analysis and show\nthat stressful and depressive posts present high correlations\nwith common LIWC categories.\nIn the future, we plan to exploit transfer learning and domain\nadaptation methods. Also, employing explainable multimodal\nmodels is one of our future plans. In addition, we plan\nto exploit more methods for enhancing transformer-based\nmodels with external knowledge. Finally, we aim to contribute\nfurther to the uncertainty estimation by exploiting Monte Carlo\ndropout [79].\nREFERENCES\n[1] World Health Organization. (2023). Stress. Accessed: Mar. 30, 2023.\n[Online]. Available: https://www.who.int/news-room/questions-and-\nanswers/item/stress\n[2] W. J. Friedman. (2023). Types of Stress and Their Symptoms.\nAccessed: Mar. 30, 2023. [Online]. Available: https://www.mentalhelp.\nnet/blogs/types-of-stress-and-their-symptoms/\n[3] World Health Organization. (2021). Depression. Accessed:\nMar. 30, 2023. [Online]. Available: https://www.who.int/news-room/\nfact-sheets/detail/depression\n[4] World Health Organization. (2017). Depression and Other Com-\nmon Mental Disorders. Accessed: Mar. 30, 2023. [Online]. Avail-\nable: https://www.who.int/publications/i/item/depression-global-health-\nestimates\n[5] M. M. Tadesse, H. Lin, B. Xu, and L. Yang, “Detection of depression-\nrelated posts in Reddit social media forum,” IEEE Access, vol. 7,\npp. 44883–44893, 2019.\n[6] S. Chandra Guntuku, A. Buffone, K. Jaidka, J. C. Eichstaedt, and\nL. H. Ungar, “Understanding and measuring psychological stress using\nsocial media,” in Proc. Int. AAAI Conf. Web Social Media, vol. 13, no. 1,\npp. 214–225, Jul. 2019.\n[7] J. S. L. Figuerêdo, A. L. L. M. Maia, and R. T. Calumby, “Early\ndepression detection in social media based on deep learning and\nunderlying emotions,” Online Social Netw. Media, vol. 31, Sep. 2022,\nArt. no. 100225.\n[8] L. Ansari, S. Ji, Q. Chen, and E. Cambria, “Ensemble hybrid learning\nmethods for automated depression detection,” IEEE Trans. Computat.\nSocial Syst., vol. 10, no. 1, pp. 211–219, Feb. 2023.\n[9] N. Poerner, U. Waltinger, and H. Schütze, “E-BERT: Efficient-Yet-\nEffective entity embeddings for BERT,” in Proc. Findings Assoc. Com-\nput. Linguistics, (EMNLP), 2020, pp. 803–818.\n[10] N. Kassner and H. Schütze, “Negated and misprimed probes\nfor pretrained language models: Birds can talk, but cannot fly,”\nin Proc. 58th Annu. Meeting Assoc. Comput. Linguistics, 2020,\npp. 7811–7818.\n[11] N. Peinelt, M. Rei, and M. Liakata, “GiBERT: Enhancing BERT with\nlinguistic information using a lightweight gated injection method,”\nin Proc. Findings Assoc. Comput. Linguistics, (EMNLP), 2021,\npp. 2322–2336.\n[12] R. Wang et al., “K-adapter: Infusing knowledge into pre-trained models\nwith adapters,” in Proc. Findings Assoc. Comput. Linguistics, (ACL-\nIJCNLP), 2021, pp. 1405–1418.\n[13] J. Lu, D. Batra, D. Parikh, and S. Lee, ViLBERT: Pretraining Task-\nAgnostic Visiolinguistic Representations for Vision-and-Language Tasks.\nRed Hook, NY , USA: Curran Associates, 2019.\n[14] M. E. Peters et al., “Knowledge enhanced contextual word representa-\ntions,” in Proc. Conf. Empirical Methods Natural Lang. Process. 9th Int.\nJoint Conf. Natural Lang. Process. (EMNLP-IJCNLP), 2019, pp. 43–54.\n[15] A. P. Dawid, “The well-calibrated Bayesian,” J. Amer. Stat. Assoc.,\nvol. 77, no. 379, pp. 605–610, Sep. 1982.\n[16] A. H. Murphy and E. S. Epstein, “Verification of probabilistic predic-\ntions: A brief review,” J. Appl. Meteorol., vol. 6, no. 5, pp. 748–755,\nOct. 1967.\n[17] C. S. Crowson, E. J. Atkinson, and T. M. Therneau, “Assessing calibra-\ntion of prognostic risk scores,” Stat. Methods Med. Res., vol. 25, no. 4,\npp. 1692–1706, Aug. 2016.\n[18] X. Jiang, M. Osl, J. Kim, and L. Ohno-Machado, “Calibrating predictive\nmodel estimates to support personalized medicine,” J. Amer. Med.\nInform. Assoc., vol. 19, no. 2, pp. 263–274, Mar. 2012.\n[19] M. Raghu et al., “Direct uncertainty prediction for medical second\nopinions,” in Proc. Mach. Learn. Res., vol. 97, Long Beach, CA, USA,\nJun. 2019, pp. 5281–5290.\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. \nILIAS et al.: CALIBRATION OF TRANSFORMER-BASED MODELS FOR IDENTIFYING STRESS AND DEPRESSION 11\n[20] R. Sawhney, A. Neerkaje, and M. Gaur, “A risk-averse mechanism for\nsuicidality assessment on social media,” in Proc. 60th Annu. Meeting\nAssoc. Comput. Linguistics (Short Papers), vol. 2, 2022, pp. 628–635.\n[21] L. Fiorillo, P. Favaro, and F. D. Faraci, “DeepSleepNet-lite: A simplified\nautomatic sleep stage scoring model with uncertainty estimates,” IEEE\nTrans. Neural Syst. Rehabil. Eng., vol. 29, pp. 2076–2085, 2021.\n[22] L. Liu, Y . Lu, Y . Luo, R. Zhang, L. Itti, and J. Lu, “Detecting\n‘smart’ spammers on social network: A topic model approach,” in Proc.\nNAACL Student Res. Workshop. San Diego, CA, USA: Association for\nComputational Linguistics, Jun. 2016, pp. 45–50.\n[23] W. Rahman et al., “Integrating multimodal information in large pre-\ntrained transformers,” in Proc. 58th Annu. Meeting Assoc. Comput.\nLinguistics, 2020, pp. 2359–2369.\n[24] M. Jin and N. Aletras, “Complaint identification in social media with\ntransformer networks,” in Proc. 28th Int. Conf. Comput. Linguistics,\n2020, pp. 1765–1771.\n[25] R. Müller, S. Kornblith, and G. E. Hinton, “When does label smooth-\ning help?” in Proc. Adv. Neural Inf. Process. Syst., H. Wallach,\nH. Larochelle, A. Beygelzimer, F. d’Alché-Buc, E. Fox, and R. Garnett,\nEds., vol. 32. Red Hook, NY , USA: Curran Associates, 2019, pp. 1–13.\n[26] S. Muñoz and C. A. Iglesias, “A text classification approach to detect\npsychological stress combining a lexicon-based feature framework with\ndistributional representations,” Inf. Process. Manage., vol. 59, no. 5,\nSep. 2022, Art. no. 103011.\n[27] E. Turcan and K. McKeown, “Dreaddit: A Reddit dataset for stress\nanalysis in social media,” inProc. 10th Int. Workshop Health Text Mining\nInf. Anal. (LOUHI), 2019, pp. 97–107.\n[28] K. Yang, T. Zhang, and S. Ananiadou, “A mental state knowledge—\nAware and contrastive network for early stress and depression detec-\ntion on social media,” Inf. Process. Manage., vol. 59, no. 4, 2022,\nArt. no. 102961.\n[29] G. I. Winata, O. P. Kampman, and P. Fung, “Attention-based LSTM\nfor psychological stress detection from spoken language using distant\nsupervision,” in Proc. IEEE Int. Conf. Acoust., Speech Signal Process.\n(ICASSP), Apr. 2018, pp. 6204–6208.\n[30] H. Lin et al., “Detecting stress based on social interactions in social net-\nworks,” IEEE Trans. Knowl. Data Eng., vol. 29, no. 9, pp. 1820–1833,\nSep. 2017.\n[31] E. Turcan, S. Muresan, and K. McKeown, “Emotion-infused models\nfor explainable psychological stress detection,” in Proc. Conf. North\nAmer. Chapter Assoc. Comput. Linguistics, Hum. Lang. Technol. , 2021,\npp. 2895–2909.\n[32] J. Liu and M. Shi, “A hybrid feature selection and ensemble approach\nto identify depressed users in online social media,” Frontiers Psychol.,\nvol. 12, p. 6571, Jan. 2022.\n[33] T. Nguyen, D. Phung, B. Dao, S. Venkatesh, and M. Berk, “Affective\nand content analysis of online depression communities,” IEEE Trans.\nAffect. Comput., vol. 5, no. 3, pp. 217–226, Jul. 2014.\n[34] S. Tsugawa, Y . Kikuchi, F. Kishino, K. Nakajima, Y . Itoh, and H. Ohsaki,\n“Recognizing depression from Twitter activity,” in Proc. 33rd Annu.\nACM Conf. Hum. Factors Comput. Syst., Apr. 2015, pp. 3187–3196.\n[35] I. Pirina and Ç. Çöltekin, “Identifying depression on Reddit: The effect\nof training data,” in Proc. EMNLP Workshop SMM4H, 3rd Social Media\nMining Health Appl. Workshop Shared Task, 2018, pp. 9–12.\n[36] M. A. Wani, M. A. ELAffendi, K. A. Shakil, A. S. Imran, and\nA. A. A. El-Latif, “Depression screening in humans with AI and deep\nlearning techniques,” IEEE Trans. Computat. Social Syst., early access,\nSep. 28, 2022, doi: 10.1109/TCSS.2022.3200213.\n[37] J. Kim, J. Lee, E. Park, and J. Han, “A deep learning model for detecting\nmental illness from user content on social media,” Sci. Rep., vol. 10,\nno. 1, pp. 1–6, Jul. 2020.\n[38] U. Naseem, A. G. Dunn, J. Kim, and M. Khushi, “Early identification\nof depression severity levels on Reddit using ordinal classification,” in\nProc. ACM Web Conf., Apr. 2022, pp. 2563–2572.\n[39] S. Ghosh and T. Anwar, “Depression intensity estimation via social\nmedia: A deep learning approach,” IEEE Trans. Computat. Social Syst.,\nvol. 8, no. 6, pp. 1465–1474, Dec. 2021.\n[40] H. Kour and M. K. Gupta, “An hybrid deep learning approach\nfor depression prediction from user tweets using feature-rich CNN\nand bi-directional LSTM,” Multimedia Tools Appl., vol. 81, no. 17,\npp. 23649–23685, Jul. 2022.\n[41] H. Zogan, I. Razzak, S. Jameel, and G. Xu, “Hierarchical convolutional\nattention network for depression detection on social media and its impact\nduring pandemic,” IEEE J. Biomed. Health Informat., early access,\nFeb. 9, 2023, doi: 10.1109/JBHI.2023.3243249.\n[42] L. Ren, H. Lin, B. Xu, S. Zhang, L. Yang, and S. Sun, “Depression\ndetection on Reddit with an emotion-based attention network: Algorithm\ndevelopment and validation,” JMIR Med. Informat. , vol. 9, no. 7,\nJul. 2021, Art. no. e28754.\n[43] M. Trotzek, S. Koitka, and C. M. Friedrich, “Utilizing neural networks\nand linguistic metadata for early detection of depression indications\nin text sequences,” IEEE Trans. Knowl. Data Eng., vol. 32, no. 3,\npp. 588–601, Mar. 2020.\n[44] V . B. de Souza, J. C. Nobre, and K. Becker, “DAC stacking: A deep\nlearning ensemble to classify anxiety, depression, and their comorbidity\nfrom Reddit texts,” IEEE J. Biomed. Health Informat., vol. 26, no. 7,\npp. 3303–3311, Jul. 2022.\n[45] H. Zogan, I. Razzak, X. Wang, S. Jameel, and G. Xu, “Explainable\ndepression detection with multi-aspect features using a hybrid deep\nlearning model on social media,” World Wide Web, vol. 25, no. 1,\npp. 281–304, Jan. 2022.\n[46] A.-S. Uban, B. Chulvi, and P. Rosso, “An emotion and cognitive based\nanalysis of mental health disorders from social media data,” Future\nGener. Comput. Syst., vol. 124, pp. 480–494, Nov. 2021.\n[47] H. Song, J. You, J.-W. Chung, and J. C. Park, “Feature attention network:\nInterpretable depression detection from social media,” in Proc. 32nd\nPacific Asia Conf. Lang., Inf. Comput. Hong Kong: Association for\nComputational Linguistics, Dec. 2018, pp. 1–3.\n[48] S. Boinepelli, T. Raha, H. Abburi, P. Parikh, N. Chhaya, and V . Varma,\n“Leveraging mental health forums for user-level depression detection on\nsocial media,” in Proc. 13th Lang. Resour. Eval. Conf.Marseille, France:\nEuropean Language Resources Association, Jun. 2022, pp. 5418–5427.\n[49] K. Anantharaman, A. S, R. Sivanaiah, S. Madhavan, and\nS. M. Rajendram, “SSN_MLRG1@LT-EDI-ACL2022: Multi-class\nclassification using BERT models for detecting depression signs from\nsocial media text,” in Proc. 2nd Workshop Lang. Technol. Equality,\nDiversity Inclusion, 2022, pp. 296–300.\n[50] F. Nilsson and G. Kovács, “FilipN@LT-EDI-ACL2022-detecting signs\nof depression from social media: Examining the use of summarization\nmethods as data augmentation for text classification,” in Proc. 2nd Work-\nshop Lang. Technol. Equality, Diversity Inclusion, 2022, pp. 283–286.\n[51] H. Zogan, I. Razzak, S. Jameel, and G. Xu, “DepressionNet: Learning\nmulti-modalities with user post summarization for depression detection\non social media,” in Proc. 44th Int. ACM SIGIR Conf. Res. Develop.\nInf. Retr., Jul. 2021, pp. 133–142.\n[52] S. Ghosh, A. Ekbal, and P. Bhattacharyya, “What does your bio\nsay? Inferring Twitter users’ depression status from multimodal profile\ninformation using deep learning,” IEEE Trans. Computat. Social Syst.,\nvol. 9, no. 5, pp. 1484–1494, Oct. 2022.\n[53] Z. Li, Z. An, W. Cheng, J. Zhou, F. Zheng, and B. Hu, “MHA: A\nmultimodal hierarchical attention model for depression detection in\nsocial media,” Health Inf. Sci. Syst., vol. 11, no. 1, p. 6, Jan. 2023.\n[54] J. C. Cheng and A. L. P. Chen, “Multimodal time-aware attention\nnetworks for depression detection,” J. Intell. Inf. Syst., vol. 59, no. 2,\npp. 319–339, Oct. 2022.\n[55] G. Shen et al., “Depression detection via harvesting social media: A\nmultimodal dictionary learning solution,” in Proc. 27th Int. Joint Conf.\nArtif. Intell., Aug. 2017, pp. 3838–3844.\n[56] T. Gui et al., “Cooperative multimodal approach to depression detection\nin Twitter,” in Proc. AAAI Conf. Artif. Intell., vol. 33, no. 1, Jul. 2019,\npp. 110–117.\n[57] D. Zhou, J. Yuan, and J. Si, “Health issue identification in social media\nbased on multi-task hierarchical neural networks with topic attention,”\nArtif. Intell. Med., vol. 118, Aug. 2021, Art. no. 102119.\n[58] Y . Wang, Z. Wang, C. Li, Y . Zhang, and H. Wang, “Online social\nnetwork individual depression detection using a multitask heterogenous\nmodality fusion approach,” Inf. Sci., vol. 609, pp. 727–749, Sep. 2022.\n[59] S. M. Mohammad and P. D. Turney, “Crowdsourcing a word—Emotion\nassociation lexicon,” Comput. Intell., vol. 29, no. 3, pp. 436–465,\nAug. 2013.\n[60] J. W. Pennebaker, M. E. Francis, and R. J. Booth, “Linguistic inquiry\nand word count: LIWC 2001,” Mahway: Lawrence Erlbaum Associates,\nvol. 71, no. 2001, p. 2001, 2001.\n[61] R. L. Boyd, A. Ashokkumar, S. Seraj, and J. W. Pennebaker, The\nDevelopment and Psychometric Properties of LIWC-22. Austin, TX,\nUSA: Univ. Texas Austin, 2022.\n[62] D. M. Blei, A. Y . Ng, and M. I. Jordan, “Latent Dirichlet allocation,”\nJ. Mach. Learn. Res., vol. 3, pp. 993–1022, Mar. 2003.\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. \n12 IEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS\n[63] D. Angelov, “Top2vec: Distributed representations of topics,”\n2020, arXiv:2008.09470. [Online]. Available: https://github.com/\nddangelov/Top2Vec\n[64] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training\nof deep bidirectional transformers for language understanding,” in Proc.\nConf. North Amer. Chapter Assoc. Comput. Linguistics, Hum. Lang.\nTechnol., vol. 1, Jun. 2019, pp. 4171–4186.\n[65] S. Ji, T. Zhang, L. Ansari, J. Fu, P. Tiwari, and E. Cambria,\n“MentalBERT: Publicly available pretrained language models for\nmental healthcare,” in Proc. 13th Lang. Resour. Eval. Conf. Mar-\nseille, France: European Language Resources Association, Jun. 2022,\npp. 7184–7190.\n[66] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,” 2016,\narXiv:1607.06450.\n[67] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and\nR. Salakhutdinov, “Dropout: A simple way to prevent neural networks\nfrom overfitting,” J. Mach. Learn. Res., vol. 15, no. 56, pp. 1929–1958,\n2014.\n[68] C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna,\n“Rethinking the inception architecture for computer vision,” in Proc.\nIEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2016,\npp. 2818–2826.\n[69] T. Wolf et al., “Transformers: State-of-the-art natural language process-\ning,” in Proc. Conf. Empirical Methods Natural Lang. Process., Syst.\nDemonstrations Stroudsburg, PA, USA: Association for Computational\nLinguistics, Oct. 2020, pp. 38–45.\n[70] A. Paszke et al., “Pytorch: An imperative style, high-performance deep\nlearning library,” in Proc. Adv. Neural Inf. Process. Syst., H. Wallach,\nH. Larochelle, A. Beygelzimer, F. d’Alché-Buc, E. Fox, and R. Garnett,\nEds., vol. 32. Curran Associates, Inc., 2019.\n[71] P. Mishra, M. Del Tredici, H. Yannakoudakis, and E. Shutova, “Author\nprofiling for abuse detection,” in Proc. 27th Int. Conf. Comput. Linguis-\ntics. Santa Fe, NM, USA: Association for Computational Linguistics,\nAug. 2018, pp. 1088–1098.\n[72] M. P. Naeini, G. Cooper, and M. Hauskrecht, “Obtaining well calibrated\nprobabilities using Bayesian binning,” in Proc. 29th AAAI Conf. Artif.\nIntell., 2015, pp. 1–7.\n[73] J. Nixon, M. W. Dusenberry, L. Zhang, G. Jerfel, and D. Tran, “Mea-\nsuring calibration in deep learning,” in Proc. IEEE/CVF Conf. Comput.\nVis. Pattern Recognit. (CVPR) Workshops, Jun. 2019, pp. 1–4.\n[74] C. Guo, G. Pleiss, Y . Sun, and K. Q. Weinberger, “On calibra-\ntion of modern neural networks,” in Proc. 34th Int. Conf. Mach.\nLearn. (ICML), vol. 70, D. Precup and Y . W. Teh, Eds., Aug. 2017,\npp. 1321–1330.\n[75] L. Ilias and D. Askounis, “Explainable identification of dementia from\ntranscripts using transformer networks,” IEEE J. Biomed. Health Infor-\nmat., vol. 26, no. 8, pp. 4153–4164, Aug. 2022.\n[76] Y . Benjamini and Y . Hochberg, “Controlling the false discovery rate: A\npractical and powerful approach to multiple testing,” J. Roy. Stat. Soc.,\nSer. B, Methodol., vol. 57, no. 1, pp. 289–300, Jan. 1995.\n[77] J. W. Pennebaker and T. C. Lay, “Language use and personality during\ncrises: Analyses of mayor Rudolph Giuliani’s press conferences,” J. Res.\nPersonality, vol. 36, no. 3, pp. 271–282, Jun. 2002.\n[78] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep\nnetworks,” in Proc. 34th Int. Conf. Mach. Learn. , vol. 70, D. Precup\nand Y . W. Teh, Eds., Aug. 2017, pp. 3319–3328.\n[79] Y . Gal and Z. Ghahramani, “Dropout as a Bayesian approximation:\nRepresenting model uncertainty in deep learning,” in Proc. 33rd\nInt. Conf. Mach. Learn., New York, NY , USA, vol. 48, Jun. 2016,\npp. 1050–1059.\nLoukas Ilias received the integrated master’s degree\nfrom the School of Electrical and Computer Engi-\nneering (SECE), National Technical University of\nAthens (NTUA), Athens, Greece, in June 2020,\nwhere he is currently pursuing the Ph.D. degree with\nthe Decision Support Systems (DSS) Laboratory,\nSECE. He has completed a Research Internship\nwith University College London (UCL), London,\nU.K.\nHe is a Researcher with the DSS Laboratory,\nNTUA, where he is involved in EU-funded research\nprojects. He has published in numerous journals, including IEEE J OURNAL\nOF BIOMEDICAL AND HEALTH INFORMATICS , Expert Systems With Appli-\ncations (Elsevier), Applied Soft Computing (Elsevier), Computer Speech and\nLanguage (Elsevier), IEEE ACCESS , and Frontiers in Aging Neuroscience. His\nresearch has also been accepted for presentation at international conferences,\nincluding the IEEE-Engineering in Medicine and Biology Society (EMBS)\nInternational Conference on Biomedical and Health Informatics (BHI’22)\nand the IEEE International Conference on Acoustics, Speech, and Signal\nProcessing (ICASP) 2023. His research interests include speech processing,\nnatural language processing, social media analysis, and the detection of\ncomplex brain disorders.\nSpiros Mouzakitis received the Ph.D. degree from\nthe School of Electrical and Computer Engineering,\nNational Technical University of Athens, Athens,\nGreece, in 2009.\nHe has more than ten years of industry experience\nin software engineering for banking, e-commerce\nsystems, and energy suppliers. He worked as\na Project Manager for 16 years in more than\n20 projects (H2020, FP7, FP6, and FP5 research\nprojects) related to big data, open data, mar-\nket/impact analysis in the context of ICT technolo-\ngies, web development, and enterprise interoperability. He is a Senior Research\nAnalyst with the Decision Support Systems Laboratory, National Technical\nUniversity of Athens. He has published in numerous journals and presented\nhis research at international conferences. His current research is focused on\ndecision analysis in the field of decision support systems based on machine\nlearning/deep learning, big and linked data analytics, as well as optimization\nsystems and algorithms.\nDimitris Askounis was the Scientific Director of\nover 50 European research projects in the above\nareas (FP7, Horizon2020, and so on). For a number\nof years, he was an Advisor to the Minister of\nJustice and the Special Secretary for Digital Con-\nvergence for the introduction of information and\ncommunication technologies in public administra-\ntion. Since June 2019, he has been the President\nof the Information Society SA, Kallithea, Greece.\nHe is currently a Professor at the School of Electri-\ncal and Computer Engineering, National Technical\nUniversity of Athens (NTUA), Athens, Greece, and the Deputy Director\nof the Decision Support Systems Laboratory. He has over 25 years of\nexperience in decision support systems, intelligent information systems and\nmanufacturing, e-business, e-government, open and linked data, big data\nanalytics, Artificial Intelligence (AI) algorithms, and the application of modern\nInformation Technology (IT) techniques in the management of companies and\norganizations.\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. ",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.5027201175689697
    },
    {
      "name": "Calibration",
      "score": 0.4389352798461914
    },
    {
      "name": "Computer science",
      "score": 0.35376691818237305
    },
    {
      "name": "Psychology",
      "score": 0.3481593132019043
    },
    {
      "name": "Reliability engineering",
      "score": 0.34447231888771057
    },
    {
      "name": "Engineering",
      "score": 0.2884330451488495
    },
    {
      "name": "Electrical engineering",
      "score": 0.2616613507270813
    },
    {
      "name": "Mathematics",
      "score": 0.15551960468292236
    },
    {
      "name": "Voltage",
      "score": 0.1439521312713623
    },
    {
      "name": "Statistics",
      "score": 0.13886159658432007
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I174458059",
      "name": "National Technical University of Athens",
      "country": "GR"
    }
  ],
  "cited_by": 63
}