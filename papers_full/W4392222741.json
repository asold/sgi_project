{
  "title": "Generative Pretrained Hierarchical Transformer for Time Series Forecasting",
  "url": "https://openalex.org/W4392222741",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4224519030",
      "name": "Zhiding Liu",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2314178937",
      "name": "Jiqian Yang",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2136283877",
      "name": "Mingyue Cheng",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2563591185",
      "name": "Yucong Luo",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2099417000",
      "name": "Zhi Li",
      "affiliations": [
        "Tsinghua University",
        "University Town of Shenzhen"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2797846142",
    "https://openalex.org/W4290945834",
    "https://openalex.org/W4367047139",
    "https://openalex.org/W3166508292",
    "https://openalex.org/W4313156423",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W4322755947",
    "https://openalex.org/W4214679539",
    "https://openalex.org/W4382318467",
    "https://openalex.org/W2980994438",
    "https://openalex.org/W4283315029",
    "https://openalex.org/W3173539742",
    "https://openalex.org/W3199148273",
    "https://openalex.org/W4382203079",
    "https://openalex.org/W3188872815",
    "https://openalex.org/W2117014758",
    "https://openalex.org/W3177318507"
  ],
  "abstract": "Recent efforts have been dedicated to enhancing time series forecasting\\naccuracy by introducing advanced network architectures and self-supervised\\npretraining strategies. Nevertheless, existing approaches still exhibit two\\ncritical drawbacks. Firstly, these methods often rely on a single dataset for\\ntraining, limiting the model's generalizability due to the restricted scale of\\nthe training data. Secondly, the one-step generation schema is widely followed,\\nwhich necessitates a customized forecasting head and overlooks the temporal\\ndependencies in the output series, and also leads to increased training costs\\nunder different horizon length settings.\\n To address these issues, we propose a novel generative pretrained\\nhierarchical transformer architecture for forecasting, named \\\\textbf{GPHT}.\\nThere are two aspects of key designs in GPHT. On the one hand, we advocate for\\nconstructing a mixed dataset under the channel-independent assumption for\\npretraining our model, comprising various datasets from diverse data scenarios.\\nThis approach significantly expands the scale of training data, allowing our\\nmodel to uncover commonalities in time series data and facilitating improved\\ntransfer to specific datasets. On the other hand, GPHT employs an\\nauto-regressive forecasting approach, effectively modeling temporal\\ndependencies in the output series. Importantly, no customized forecasting head\\nis required, enabling \\\\textit{a single model to forecast at arbitrary horizon\\nsettings.} We conduct sufficient experiments on eight datasets with mainstream\\nself-supervised pretraining models and supervised models. The results\\ndemonstrated that GPHT surpasses the baseline models across various fine-tuning\\nand zero/few-shot learning settings in the traditional long-term forecasting\\ntask. We make our codes publicly\\navailable\\\\footnote{https://github.com/icantnamemyself/GPHT}.\\n",
  "full_text": "Generative Pretrained Hierarchical Transformer for\nTime Series Forecasting\nZhiding Liu\nUniversity of Science and Technology\nof China\nState Key Laboratory of Cognitive\nIntelligence\nHefei, Anhui, China\nzhiding@mail.ustc.edu.cn\nJiqian Yang\nUniversity of Science and Technology\nof China\nState Key Laboratory of Cognitive\nIntelligence\nHefei, Anhui, China\nyangjq@mail.ustc.edu.cn\nMingyue Chengâˆ—\nUniversity of Science and Technology\nof China\nState Key Laboratory of Cognitive\nIntelligence\nHefei, Anhui, China\nmycheng@ustc.edu.cn\nYucong Luo\nUniversity of Science and Technology\nof China\nState Key Laboratory of Cognitive\nIntelligence\nHefei, Anhui, China\nprime666@mail.ustc.edu.cn\nZhi Li\nShenzhen International Graduate\nSchool, Tsinghua University\nShenzhen, Guangdong, China\nzhilizl@sz.tsinghua.edu.cn\nABSTRACT\nRecent efforts have been dedicated to enhancing time series fore-\ncasting accuracy by introducing advanced network architectures\nand self-supervised pretraining strategies. Nevertheless, existing\napproaches still exhibit two critical drawbacks. Firstly, these meth-\nods often rely on a single dataset for training, limiting the modelâ€™s\ngeneralizability due to the restricted scale of the training data. Sec-\nondly, the one-step generation schema is widely followed, which\nnecessitates a customized forecasting head and overlooks the tem-\nporal dependencies in the output series, and also leads to increased\ntraining costs under different horizon length settings.\nTo address these issues, we propose a novel generative pretrained\nhierarchical transformer architecture for forecasting, namedGPHT.\nThere are two aspects of key designs in GPHT. On the one hand,\nwe advocate for constructing a mixed dataset under the channel-\nindependent assumption for pretraining our model, comprising\nvarious datasets from diverse data scenarios. This approach signif-\nicantly expands the scale of training data, allowing our model to\nuncover commonalities in time series data and facilitating improved\ntransfer to specific datasets. On the other hand, GPHT employs an\nauto-regressive forecasting approach, effectively modeling tempo-\nral dependencies in the output series. Importantly, no customized\nforecasting head is required, enablinga single model to forecast at ar-\nbitrary horizon settings. We conduct sufficient experiments on eight\ndatasets with mainstream self-supervised pretraining models and\nâˆ—Mingyue Cheng is the corresponding author.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nKDD â€™24, August 25â€“29, 2024, Barcelona, Spain.\nÂ© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0490-1/24/08. . . $15.00\nhttps://doi.org/10.1145/3637528.3671855\nsupervised models. The results demonstrated that GPHT surpasses\nthe baseline models across various fine-tuning and zero/few-shot\nlearning settings in the traditional long-term forecasting task, pro-\nviding support for verifying the feasibility of pretraining time series\nlarge models. We make our codes publicly available1.\nCCS CONCEPTS\nâ€¢ Mathematics of computing â†’Time series analysis ; â€¢ Com-\nputing methodologies â†’Artificial intelligence.\nKEYWORDS\nTime series forecasting; deep learning; pretraining\nACM Reference Format:\nZhiding Liu, Jiqian Yang, Mingyue Cheng, Yucong Luo, and Zhi Li. 2024.\nGenerative Pretrained Hierarchical Transformer for Time Series Forecasting.\nIn Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery\nand Data Mining (KDD â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, New\nYork, NY, USA, 11 pages. https://doi.org/10.1145/3637528.3671855\n1 INTRODUCTION\nTime series forecasting is one of the fundamental tasks in time\nseries analysis, garnering significant attention over the past several\nyears [35, 41]. Precise forecasting plays a crucial role in assisting\nvarious real-world applications, including climate report [ 1, 50],\npatient vital sign assessments [30], urban computing [24] and stock\nprediction[23] etc. Various efforts have been devoted to this area for\nmore accurate forecasting. Notably, deep-learning-based methods\nhave achieved great success due to their capability to capture both\ntemporal and cross-dimension dependencies [28, 31, 51].\nOn the other hand, inspired by recent significant advancements\nin pretraining methods in both the NLP and CV fields [3, 12, 18, 20],\nvarious pretraining-based time series analysis methods have been\nproposed [49]. The contrastive learning technique is widely em-\nployed in the discriminative pretraining methods, where the models\n1https://github.com/icantnamemyself/GPHT\narXiv:2402.16516v2  [cs.LG]  18 Jun 2024\nKDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Zhiding Liu et al.\nare expected to learn representations from constructed positive and\nnegative pairs [42, 45]. Furthermore, the incorporation of gener-\native targets, such as masked time-series modeling, into the pre-\ntraining task has also been well-studied, with the aim of extracting\ngeneral knowledge during the reconstruction process [ 7, 13, 38].\nAdditionally, considering the shared characteristics between time\nseries and natural languages, some recent studies have emerged\nto adapt pre-trained language models into accurate forecasters\nthrough prompting or fine-tuning [19, 54]. All these methods have\nachieved significant success, even competing effectively with su-\npervised forecasting approaches.\nDespite their effectiveness, there remain significant challenges\nin promoting the performance of the pretrained forecasters. Firstly,\nthe limited scale of the dataset is a critical issue. The standard prac-\ntice of these methods involves pretraining on a single real-world\nor synthetic dataset, and evaluating its performance on this dataset\nor other datasets through transfer learning [10, 14]. However, the\namount of training instances in a single dataset is often limited\n[17], and its inherent patterns may fail to encompass the complex\nscenarios of other datasets, leading to suboptimal forecasting accu-\nracy and transferability. Secondly, nearly all forecasting approaches\nadhere to the one-step generation schema [52], which implies that\nthe predictions for all future time steps are generated through a\nsingle forward pass with a customized forecasting head determined\nby a specified horizon length. The drawbacks of this paradigm are\nmultifaceted. On the one hand, the temporal dependencies within\nthe predicted series are inevitably overlooked, potentially leading to\nan inferior result. On the other hand, the tailored forecasting head\nhinders the generalizability of the pretrained models, as multiple\nmodels are required for different horizon length settings.\nTo alleviate the above challenges, we are motivated to explore\npretraining a single unified forecasting model that generalizes\nwell across diverse data scenarios and forecasting settings with\na novel generative hierarchical transformer architecture, namely\nGPHT. Firstly for the dataset construction, we extend the channel-\nindependent assumption [46] into multiple data scenarios, simply\nmixing time series originating from various scopes as a whole\nwithout considering extra information, which provides vast charac-\nteristics such as diverse periodicities that benefit the pertaining pro-\ncedure. Besides, to better capture the commonalities and specialties\nwithin the mixed dataset, we naturally introduce a novel hierar-\nchical transformer architecture as the backbone model [4, 37]. Fur-\nthermore, we formulate the forecasting task as a standard language\nmodeling task with the patching technique [8, 31], which projects\ntime series into token-level representations, and an auto-regressive\noptimization function is applied in our pretraining procedure to re-\nplace the conventional one-step generating schema. Consequently,\nthe model can well model the temporal dependencies in the forecast-\ning horizons at a token-wise level through auto-regressive inference\nand can be seamlessly adapted to diverse datasets with varying\nhorizon length settings without any modification. We compare our\nmodelâ€™s performance with state-of-the-art supervised and pretrain-\ning methods under various fine-tuning and zero/few-shot learning\nsettings. The results demonstrate the superiority and generalizabil-\nity of the proposed GPHT.\nIn summary, our contributions are as follows:\nâ€¢We explore pretraining a single unified forecasting model\nthat generalizes well across diverse scenarios with the pre-\ntraining dataset constructed under the channel-independent\nassumption, which allows for the easy creation of diverse,\nlarge-scale datasets, forming the foundation for the general-\nizability across data scenarios of the forecasting model.\nâ€¢We introduce GPHT, a novel hierarchical transformer fore-\ncasting in an auto-regressive manner. This design inherently\naids in modeling both the commonalities and specialties\nwithin the mixed dataset, guaranteeing universality under\nvarious forecasting settings.\nâ€¢We conduct sufficient experiments on 8 widely used bench-\nmark datasets, comparing our proposed GPHT with main-\nstream supervised and pretraining methods. The results\nshow that our model surpasses the baseline models across\nvarious fine-tuning and zero/few-shot learning settings.\n2 RELATED WORKS\n2.1 Time Series Forecasting\nTime series forecasting is a crucial task with broad applications,\ngarnering significant attention in recent years. Early research pre-\ndominantly focuses on statistical methods such as ARIMA [2, 48],\nwhich builds an auto-regressive model and forecasts in a moving\naverage fashion. However, these methods may encounter limita-\ntions in long-term forecasting settings. The advent of deep learning\nhas led to the development of numerous models capturing both\ntemporal and cross-dimensional dependencies in multivariate time\nseries, utilizing modern architectures [9, 26, 29, 34, 36].\nTransformer-based and MLP-based approaches have emerged\nas research hotspots due to their outstanding performance [5, 27,\n46]. Beyond model architecture, several customized techniques\nrooted in time series analysis have been established. These in-\nclude trend-seasonal decomposition [44], time-frequency conver-\nsion [53], series stabilization [25], and patching [31]. The integra-\ntion of these advanced studies enables contemporary forecasters\nto achieve unprecedented accuracy in predictions across diverse\nscenarios through supervised training.\n2.2 Self-supervised Pretraining in Time Series\nModeling\n2.2.1 Discriminative methods. Contrastive learning is widely uti-\nlized in the discriminative time series modeling approaches, aiming\nto derive crucial representations from pre-defined positive and\nnegative pairs. A key challenge lies in effectively constructing in-\nformative instance pairs.\nIn practice, TNC [39] takes advantage of the local smoothness\nof a signalâ€™s generative process to define neighborhoods in time\nwith stationary properties, and TS2Vec [ 45] proposes to employ\ncontrastive learning on both instance level and patch level in a hi-\nerarchical way for robust contextual representation learning. More-\nover, TS-TCC [15] introduces a new contrastive learning task of\ncross-view prediction. Subsequently, CoST [42] comprises both time\ndomain and frequency domain contrastive losses to learn discrimi-\nnative trend and seasonal representations. Note that discriminative\nmethods primarily concentrate on coarse-grained instance-level\nGenerative Pretrained Hierarchical Transformer for\nTime Series Forecasting KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.\ninformation, leading to unsatisfactory performance in forecasting\ntasks where fine-grained temporal features are essential.\n2.2.2 Generative methods. Generative pretraining methods usually\nfollow a paradigm of reconstruction. In the context of time series\nanalysis, the objective of masked time-series modeling has been ex-\ntensively explored. TST [47] pioneers the use of traditional masked\nmodeling, aiming to predict the removed time series points based\non the remaining ones. Subsequently, STEP [ 38] and PatchTST\n[31] expand on this concept to a sub-series level, where local infor-\nmation is more effectively captured, and computational costs are\nsignificantly reduced. Furthermore, a recent study [ 13] achieves\nsuperior fine-tuning performance by introducing a novel masked\nmodeling task, involving the reconstruction of the original time\nseries from multiple randomly masked series. Besides, TimeMAE\n[7] significantly surpasses previous competitive baselines in clas-\nsification tasks by leveraging decoupled masked autoencoders to\nlearn robust representations through two pretext tasks: masked\ncodeword classification and masked representation regression.\nOn the other hand, the forecast-as-pretraining schema has also\nbeen a subject of recent research. ForecastPFN [ 14] introduces a\nprior-data fitted network trained on synthetic data and achieves\naccurate zero-shot forecasting on univariate time series. Besides,\nboth TimeGPT-1 [ 16] and PreDcT [ 10] explore the potential of\ntraining a foundation model for forecasting, yielding zero-shot fore-\ncasting capability under relatively short horizon lengths. Moreover,\nthere is another related line of work focusing on adapting tradi-\ntional pretrained generative language models to the time series\ndomain, either through prompting [19] or fine-tuning [54]. These\napproaches have demonstrated competitive results when compared\nto the traditional supervised approaches.\nDespite the effectiveness, existing methods exhibit two key short-\ncomings. Firstly, these models are typically trained on a single\ndataset with limited scale and patterns, impeding their ability to\ngeneralize to diverse forecasting scenarios. Secondly, both super-\nvised and self-supervised approaches often necessitate a customized\nforecasting head, incurring multiple training costs under different\nhorizon length settings. Regarding our proposed method, GPHT, it\nstands out as a generative self-supervised pretraining approach, dis-\ntinguishing itself from existing methods by effectively addressing\nthese issues. In detail, we investigate the feasibility of constructing a\nmixed dataset for training our model in an auto-regressive manner.\nThis approach ensures the superior generalizability of GPHT, al-\nlowing it to be seamlessly adapted to any dataset, including unseen\ndatasets, and forecast at arbitrary horizon lengths. Notably, experi-\nments demonstrate that our method surpasses the baseline models\nacross various fine-tuning and zero/few-shot learning settings in\nthe traditional long-term forecasting task.\n3 PROPOSED METHOD\nIn this section, we will delve into the specifics of the proposed GPHT\nmethod illustrated in Figure 1, demonstrating its capacity to effi-\nciently acquire precise time series forecasting through pretraining\non the mixed dataset.\n3.1 Problem Definition\nGiven an input series Xâˆˆ Rğ¶Ã—ğ¿, a time series forecasting model is\ntasked with precisely predicting future values Yâˆˆ Rğ¶Ã—ğ». Here, ğ¿,\nğ», and ğ¶ represent the lookback window length, horizon length,\nand the number of channels, respectively.\nGPHT adopts the channel-independent assumption [ 31, 46],\ntreating each multivariate time series as multiple independent uni-\nvariate time series. In essence, GPHT conducts individual fore-\ncasting on each variate within the input series, and the resultant\nforecasts are concatenated to generate the final predictions.\nMoreover, we extend this methodology to the construction of\nthe mixed pretraining dataset, where the heterogeneity of each\nvariable is discarded and no extra information is taken into account.\nIt can be therefore seamlessly applied to more diverse scenarios\nwhere the covariate information may be missing and the data itself\nmay be synthetic. In practice, we concatenate the training segments\nfrom various real-world datasets to constitute the training set of the\nmixed dataset. This process is similarly applied to the validation and\ntesting portions. Our approach ensures that GPHT is pretrained on a\nrich variety of temporal patterns, thereby enhancing its adaptability\nand generalization capabilities across diverse time series domains.\n3.2 Series Tokenization\nGiven that the majority of time series originate from signals cap-\ntured by real-world sensors, the data inherently carry noise, and\ninformation is often sparsely distributed across the time points.\nConsequently, employing auto-regressive training on point-wise\ntime series might yield suboptimal performance due to the risk of\noverfitting outliers and the associated error accumulation [52], in\naddition to incurring high computation costs.\nTo address these challenges, we employ the series tokenization\ntechnique, a proven effective approach in time series modeling\n[22, 31, 51]. Specifically, we adopt a non-overlapping tokenization\nstrategy, reshaping the input seriesXinto a sequence of time series\ntokens, ğ‘¥ âˆˆRğ¶Ã—ğ¿â€²Ã—ğ‘‡, where ğ‘‡ Ã—ğ¿â€² = ğ¿, ğ‘‡ represents the token\nlength, and ğ¿â€²can be considered as the sequence length. The to-\nkenization strategy not only helps mitigate the impact of noise\nand sparse information distribution but also enhances the modelâ€™s\nability to better capture the local semantics, ultimately contributing\nto more robust and accurate time series forecasting.\nFurthermore, recent research has highlighted a prevalent issue\nin time series data, characterized by a distribution shift [11, 28, 33]\nwhich means that the mean and variance of time series changes\nover time. This challenge significantly hinders the generalizability\nof deep-learning-based forecasters. The impact of this phenome-\nnon is even more pronounced in the context of pretraining on a\nmixed dataset, where series inherently stem from distinct tempo-\nral distributions. To mitigate this issue, we introduce an Instance\nNormalization layer [25], designed to address the distribution shift\nproblem by normalizing the input series using the formula:\nğ‘¥ğ‘–ğ‘› = ğ‘¥âˆ’ğœ‡\nğœ+ğœ– and ğœ‡ = E[X],ğœ2 = Var[X]. (1)\nHere, ğœ– is a small constant, and ğœ‡,ğœ represent the instance-specific\nmean and standard deviation of X, respectively. Subsequently,ğ‘¥0\nğ‘–ğ‘›\nis fed into the model for further feature extraction. This normal-\nization step enhances the modelâ€™s robustness to distribution shifts,\nKDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Zhiding Liu et al.\nInstance Normalization & Series Tokenization\nReverse Instance Normalization\nğœ‡,ğœ\n Multi-stage Hierarchical Transformer Blocks\nğ‘¥ğ‘–ğ‘›\nğ‘–\nDown Sampling\nForecast Head & Up Sampling\nCausal Multi-head Attention\nAdd & Layer Norm\nFeed-forward Layer\nAdd & Layer Norm\nZero\nPadding\nM Ã—\nN Ã—\nğ‘¥ğ‘œğ‘¢ğ‘¡\nğ‘– ğ‘¥ğ‘–ğ‘›\nğ‘–+1\nIterative Residual\nMixed Dataset\nFigure 1: Illustration of the proposed GPHT model with two key features: (a) GPHT forecasts in an auto-regressive manner on\nthe time series tokens. (b) Pretrained on the mixed dataset with the multi-stage hierarchical transformer blocks, GPHT excels\nin capturing the commonalities among time series originating from various data scenarios.\npromoting more effective learning and improved generalization\nacross diverse temporal patterns.\n3.3 Hierarchical Transformer Blocks\nMulti-scale representation learning has demonstrated its effective-\nness in various time series modeling tasks [6, 37], given the multiple\nperiodic characteristics commonly found in real-world time series\ndata. Furthermore, to better discover commonalities hidden within\nmixed datasets comprising various data scenarios, we posit the\nindispensability of a hierarchical encoder.\nIn practice, drawing inspiration from the multi-rate sampling\nstrategy [4], we introduce a token-level multi-stage representa-\ntion learning approach within our hierarchical transformer blocks:\nSuppose ğ‘¥ğ‘– is the input series after tokenization of stage ğ‘–, a max-\npooling operation with a kernel size of ğ‘˜ğ‘– is applied on each token,\ndown-sampling the original data into ğ‘¥ğ‘–\nğ‘–ğ‘› âˆˆRğ¶Ã—ğ¿â€²Ã—ğ‘‡\nğ‘˜ğ‘– . This opera-\ntion retains a coarse-grained portion of the original data, compelling\nthe encoder network to focus on modeling coarse patterns. After-\nward, a standard multi-layer transformer network is employed for\nrepresentation learning [40]:\nâ„ğ‘–ğ‘›\nğ‘– = ğ‘ƒğ¸ğ‘–(ğ‘¥ğ‘–ğ‘›\nğ‘– )+ğ¸ğ‘šğ‘ğ‘–(ğ‘¥ğ‘–ğ‘›\nğ‘– )\nâ„ğ‘œğ‘¢ğ‘¡\nğ‘– = ğ‘‡ğ‘…ğ‘€ğ‘–(â„ğ‘–ğ‘›\nğ‘– ).\n(2)\nHere, ğ‘ƒğ¸ represents a regular position embedding layer, andğ¸ğ‘šğ‘is\na token projection layer projecting a time series token into the hid-\nden space of the transformer. Besides, we employ the decoder-only\ntransformers as the backbone, which incorporates causal attention\nmasks to prevent information leakage during the auto-regressive\ngenerating process.\nFinally, the learned hidden statesâ„ğ‘œğ‘¢ğ‘¡\nğ‘– are fed into the forecasting\nhead, which is a linear layer, to predict future values for each token.\nIn this regard, we adopt an up-sampling operation, which can be\neither linear interpolation or MLP, to map the predictions back\nto the original time series tokens, yielding the predictions ğ‘¥ğ‘–\nğ‘œğ‘¢ğ‘¡ of\nstage ğ‘–. This process is represented as:\nğ‘¥ğ‘–\nğ‘œğ‘¢ğ‘¡ = ğ‘ˆğ‘ğ‘†ğ‘ğ‘šğ‘ğ‘™ğ‘–ğ‘›ğ‘” (ğ¹ğ‘œğ‘Ÿğ‘’ğ‘ğ‘ğ‘ ğ‘¡ (â„ğ‘œğ‘¢ğ‘¡\nğ‘– )). (3)\nThis comprehensive approach to multi-stage representation learn-\ning and forecasting allows for the capturing of intricate temporal\npatterns at different scales, contributing to the modelâ€™s effectiveness\nin handling diverse time series scenarios.\n3.4 Iterative Residual Learning\nUtilizing the multi-stage hierarchical transformer blocks, GPHT\nis expected to learn coarse-to-fine representations effectively. To\nfully leverage these representations, we propose a novel iterative\nresidual learning strategy [ 21, 32], transforming the forecasting\nprocess into an iterative approach.\nSpecifically, as illustrated in Figure 1, the input of stage ğ‘–+1 is\nthe residual of stage ğ‘–â€™s input and output, defined as:\nğ‘¥ğ‘–+1\nğ‘–ğ‘› = ğ‘¥ğ‘–\nğ‘–ğ‘› âˆ’ğ‘ƒğ‘ğ‘‘ğ¹ğ‘–ğ‘Ÿğ‘ ğ‘¡ğ‘‡ğ‘œğ‘˜ğ‘’ğ‘› (ğ‘¥ğ‘–\nğ‘œğ‘¢ğ‘¡). (4)\nTaking advantage of the auto-regressive training schema, each\ntoken inğ‘¥ğ‘–\nğ‘œğ‘¢ğ‘¡ can be considered as the predicted value of the next to-\nken corresponding to the token inğ‘¥ğ‘–\nğ‘–ğ‘›. For the regression of the first\ntoken, we adopt a zero-padding for simplicity. Therefore, Equation\n4 effectively refines the input for the next stage, eliminating\nthe redundant information in the series .\nGenerative Pretrained Hierarchical Transformer for\nTime Series Forecasting KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.\nIntuitively, the pooling operation within the hierarchical trans-\nformers allows the model to concentrate on specific patterns at\nlower frequencies. Additionally, the task of deeper blocks is simpli-\nfied as the shallower layers filter out the well-approximated signals.\nTherefore, the iterative residual learning strategy enables the model\nto focus on refining the finer details, allowing for the progressive\nenhancement of the predictive accuracy. Moreover, the strategy\nnaturally suits the diverse patterns in the mixed pretraining dataset\nwith adaptability to various temporal structures, thus guaranteeing\npromising generalizability.\n3.5 Optimization Target\nLet ğ‘† represent the number of stages, the intermediate forecasting\nresult of GPHT is the sum of all the outputs of each hierarchical\ntransformer block, defined as:\nğ‘¦ğ‘ğ‘Ÿğ‘’ğ‘‘ =\nğ‘†âˆ‘ï¸\nğ‘–=1\nğ‘¥ğ‘–\nğ‘œğ‘¢ğ‘¡. (5)\nBesides, a Reversed Instance Normalization layer is applied to\nthe intermediate result through de-normalization, restoring the\ncharacteristics of the input series for better accuracy [25]:\nYğ‘ğ‘Ÿğ‘’ğ‘‘ = ğ‘¦ğ‘ğ‘Ÿğ‘’ğ‘‘ Â·(ğœ+ğœ–)+ğœ‡. (6)\nTo fully leverage the mixed dataset and better capture tempo-\nral dependencies, we formulate the pretraining task as a standard\nlanguage modeling task, employing a token-wise auto-regressive\nloss function as the optimization target (which is also a forecasting\ntask). Using the same notations as in Section 3.1, and letting ğ» = ğ‘‡,\nthe optimization target of GPHT is:\nğ¿= ğ‘€ğ‘†ğ¸(Yğ‘ğ‘Ÿğ‘’ğ‘‘,ğ¶ğ‘œğ‘›ğ‘ğ‘ğ‘¡ (X[:,ğ‘‡ :],Y)) (7)\nHere, we use the standard Mean Squared Error (MSE) as the next-\ntoken-prediction loss function, as time series tokens are still con-\ntinuous numerical values.\n3.6 Inference\nGiven that the pretraining task can be considered a forecasting task,\nthe pre-trained GPHT can be directly applied to downstream\nforecasting tasks without any modification . This sets our ap-\nproach apart from mainstream pretraining methods [7, 13, 45, 54],\nwhere a fine-tuning procedure is typically required.\nOn the other hand, we argue that the performance of GPHT can\nalso be further enhanced through fine-tuning. In practice, to strike\na balance between maintaining generalizability and improving per-\nformance on a specific dataset, we adopt a parameter-efficient tun-\ning strategy. Specifically, only the forecasting heads described in\nEquation 3 are updated during the fine-tuning process. Importantly,\nthese forecasting head parameters account for less than 0.5% of the\nentire model.\nDuring inference, benefiting from the aforementioned training\nschema and the channel-independent assumption, GPHT is theo-\nretically capable of conducting universal forecasting on any input\nmulti-variate time series, regardless of arbitrary horizon lengths.\nThe forecasting process is akin to the decoding process of a lan-\nguage model. Given any input X, our model can initially predict\nthe next first token. This predicted token is then concatenated to\nthe end of the input series to generate predictions for the second\ntoken. Note that a max input length ğ¿ğ‘š exists in our model due\nto the positional embeddings and heavy computation cost when\naddressing long sequences. Consequently, only the most recent ğ¿ğ‘š\ntokens are fed into the model for forecasting.\n4 EXPERIMENTS\nIn this section, we conduct sufficient experiments on 8 widely used\ndatasets in comparison with mainstream self-supervised pretraining\nmethods and supervised methods to illustrate the effectiveness of\nour proposed GPHT.\n4.1 Experimental Setup\n4.1.1 Datasets. Table 1 provides detailed descriptions of the used\ndatasets, covering various data scenarios and scales. Among them,\nboth ETT2 and Electricity3 mainly records the consumption on\nelectricity, and ETT can be further divided into 4 subsets according\nto the frequency. Besides, Exchange4 collects the daily exchange\nrate among 8 countries, Traffic5 contains the data of traffic load\nsensors, and Weather6 is made up of 21 climate indicators like\nair temperature. Following the standard protocol, we split each\ndataset into training, validation and testing sets according to the\nchronological order. The split ratio is 6:2:2 for the ETT dataset and\n7:1:2 for the other datasets [44].\nTable 1: The Statistics of Each Dataset.\nDataset Variables Frequency Length Scope\nETTh1/ETTh2 7 1 Hour 17420 Energy\nETTm1/ETTm2 7 15 Minutes 69680 Energy\nElectricity 321 1 Hour 26304 Energy\nExchange 8 1 Day 7588 Finance\nTraffic 862 1 Hour 17544 Transportation\nWeather 21 10 Minutes 52696 Weather\n4.1.2 Compared Baselines. We select various state-of-the-art mod-\nels as the baseline models in the experiments, encompassing both\nself-supervised and supervised approaches.\nAmong them, FPT [ 54] introduces a framework that utilizes\nparameter-efficient fine-tuning on pretrained generative language\nmodels, adapting them to time series tasks. SimMTM [ 13] re-\nframes the standard masked time series modeling target into re-\ncovering masked time points through the weighted aggregation of\nmultiple neighbors. Additionally, TimeMAE [7] leverages decou-\npled masked autoencoders to learn robust representations through\nmasked codeword classification and masked representation regres-\nsion. On the other hand, we include superior supervised mod-\nels to better demonstrate the effectiveness of GPHT, including\nTransformer-based models such as PatchTST [ 31] and iTrans-\nformer [27], a linear model-based approach DLinear [46], and\n2https://github.com/zhouhaoyi/ETDataset\n3https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014\n4https://github.com/laiguokun/multivariate-time-series-data\n5http://pems.dot.ca.gov\n6https://www.bgc-jena.mpg.de/wetter/\nKDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Zhiding Liu et al.\nTable 2: Multivariate time series forecasting results comparing GPHT with both SOTA self-supervised approaches and supervised\napproaches. The best results are in bold and the second best are underlined .\nType Ours Self-supervised Supervised\nMethods GPHT* GPHT PatchTST FPT SimMTM TimeMAE PatchTST iTransformer TimesNet DLinear\nMetric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE\nElectricity\n96 0.128 0.219 0.128 0.219 0.132 0.225 0.139 0.238 0.133 0.223 0.133 0.230 0.138 0.233 0.132 0.228 0.177 0.281 0.141 0.238\n192 0.147 0.236 0.146 0.236 0.148 0.241 0.155 0.252 0.147 0.237 0.150 0.246 0.153 0.247 0.154 0.249 0.193 0.295 0.154 0.251\n336 0.165 0.255 0.165 0.255 0.167 0.260 0.170 0.267 0.166 0.265 0.166 0.265 0.170 0.263 0.172 0.267 0.206 0.306 0.170 0.269\n720 0.206 0.292 0.207 0.292 0.205 0.292 0.208 0.299 0.203 0.297 0.199 0.296 0.206 0.295 0.204 0.296 0.223 0.320 0.205 0.302\nExchange\n96 0.096 0.216 0.087 0.207 0.088 0.207 0.098 0.222 0.100 0.226 0.229 0.352 0.094 0.216 0.099 0.225 0.166 0.305 0.087 0.217\n192 0.183 0.304 0.172 0.296 0.186 0.308 0.209 0.327 0.210 0.332 0.653 0.581 0.191 0.311 0.206 0.329 0.303 0.413 0.164 0.298\n336 0.322 0.410 0.309 0.400 0.374 0.446 0.398 0.463 0.389 0.460 1.524 0.887 0.343 0.427 0.370 0.448 0.445 0.511 0.333 0.437\n720 0.833 0.685 0.808 0.669 0.857 0.692 1.010 0.747 1.104 0.800 2.525 1.193 0.888 0.706 0.963 0.746 1.389 0.899 0.988 0.749\nTraffic\n96 0.348 0.236 0.346 0.234 0.382 0.262 0.388 0.279 0.368 0.262 0.365 0.252 0.395 0.272 0.361 0.266 0.600 0.323 0.411 0.284\n192 0.374 0.248 0.371 0.246 0.385 0.261 0.411 0.287 0.373 0.251 0.383 0.260 0.411 0.278 0.378 0.271 0.612 0.327 0.423 0.289\n336 0.392 0.259 0.388 0.256 0.409 0.275 0.423 0.293 0.395 0.254 0.399 0.269 0.424 0.284 0.390 0.274 0.628 0.344 0.437 0.297\n720 0.428 0.284 0.423 0.279 0.438 0.291 0.449 0.307 0.432 0.290 0.438 0.291 0.453 0.300 0.424 0.291 0.657 0.349 0.467 0.316\nWeather\n96 0.155 0.196 0.154 0.196 0.148 0.196 0.152 0.201 0.152 0.201 0.151 0.208 0.147 0.197 0.162 0.212 0.168 0.225 0.176 0.236\n192 0.203 0.240 0.201 0.240 0.193 0.240 0.197 0.244 0.198 0.245 0.198 0.256 0.191 0.240 0.205 0.251 0.218 0.268 0.217 0.275\n336 0.259 0.283 0.257 0.283 0.244 0.279 0.252 0.287 0.249 0.285 0.246 0.294 0.244 0.282 0.257 0.291 0.269 0.301 0.264 0.315\n720 0.338 0.337 0.335 0.337 0.321 0.334 0.329 0.340 0.324 0.335 0.316 0.351 0.320 0.334 0.325 0.337 0.340 0.350 0.325 0.364\nETTh1\n96 0.378 0.388 0.363 0.382 0.384 0.401 0.388 0.405 0.383 0.411 0.431 0.450 0.382 0.403 0.405 0.419 0.421 0.438 0.375 0.396\n192 0.425 0.416 0.405 0.408 0.427 0.431 0.422 0.423 0.417 0.432 0.484 0.486 0.416 0.423 0.448 0.447 0.482 0.479 0.428 0.437\n336 0.456 0.432 0.430 0.423 0.461 0.450 0.442 0.435 0.425 0.439 0.515 0.507 0.441 0.440 0.482 0.470 0.528 0.505 0.448 0.449\n720 0.454 0.449 0.414 0.435 0.460 0.465 0.469 0.473 0.437 0.456 0.595 0.577 0.470 0.475 0.560 0.537 0.527 0.510 0.505 0.514\nETTh2\n96 0.307 0.347 0.296 0.340 0.297 0.354 0.291 0.349 0.298 0.350 0.294 0.358 0.286 0.342 0.305 0.361 0.355 0.408 0.296 0.360\n192 0.373 0.389 0.363 0.384 0.388 0.406 0.356 0.390 0.360 0.388 0.352 0.397 0.357 0.389 0.391 0.412 0.403 0.434 0.391 0.423\n336 0.399 0.414 0.392 0.410 0.392 0.413 0.387 0.418 0.388 0.410 0.394 0.427 0.377 0.409 0.418 0.433 0.398 0.434 0.445 0.460\n720 0.412 0.429 0.407 0.427 0.413 0.442 0.415 0.448 0.412 0.435 0.539 0.510 0.406 0.440 0.437 0.455 0.443 0.465 0.700 0.592\nETTm1\n96 0.301 0.345 0.291 0.339 0.281 0.341 0.290 0.346 0.296 0.349 0.301 0.348 0.298 0.345 0.306 0.360 0.331 0.372 0.303 0.346\n192 0.347 0.374 0.337 0.368 0.326 0.372 0.330 0.371 0.334 0.373 0.351 0.383 0.339 0.374 0.345 0.382 0.435 0.421 0.338 0.368\n336 0.388 0.401 0.377 0.393 0.348 0.384 0.366 0.393 0.371 0.398 0.390 0.408 0.381 0.401 0.378 0.402 0.457 Â· 0.445 0.373 0.393\n720 0.465 0.441 0.452 0.433 0.399 0.418 0.416 0.421 0.418 0.425 0.457 0.446 0.428 0.431 0.443 0.439 0.526 0.481 0.428 0.423\nETTm2\n96 0.179 0.257 0.170 0.250 0.171 0.257 0.171 0.261 0.173 0.264 0.180 0.267 0.174 0.261 0.174 0.266 0.190 0.276 0.170 0.264\n192 0.242 0.298 0.230 0.291 0.236 0.304 0.231 0.302 0.230 0.299 0.243 0.312 0.238 0.307 0.247 0.315 0.244 0.311 0.233 0.311\n336 0.300 0.334 0.285 0.327 0.291 0.344 0.288 0.343 0.282 0.332 0.308 0.355 0.293 0.346 0.292 0.343 0.302 0.349 0.298 0.358\n720 0.400 0.393 0.380 0.386 0.388 0.404 0.389 0.406 0.374 0.390 0.395 0.407 0.373 0.401 0.375 0.395 0.406 0.406 0.423 0.437\n#1 Counts 8 41 13 0 4 3 10 0 0 4\na convolution-based model TimesNet [43], which is also a multi-\nscale model. The performance of these methods effectively repre-\nsents the utmost accuracy achievable by current forecasting models.\n4.1.3 Implementation Details. We employ ADAM as the default\noptimizer throughout all the experiments and use mean squared er-\nror (MSE) and mean absolute error (MAE) as the evaluation metrics.\nA lower MSE/MAE indicates better performance. For the baseline\nmodels, we implement them with official codes and recommended\nparameter settings.\nAs for GPHT, we set the token length ğ‘‡ to 48, and the max\ninput length ğ¿ğ‘š is set to 7 to accommodate the lookback window\nlength. The model comprises 4 stages of hierarchical transformer\nblocks, each with three-layer decoder-only transformers. The down-\nsampling ratio for each stage is set to [8,4,2,1]respectively. All\nexperiments are conducted for three runs with a fixed random seed\non a single NVIDIA RTX 4090 24GB GPU.\n4.2 Main Results\nWe report the long-term multi-variate forecasting results in Ta-\nble 2. For a fair comparison, the lookback window length ğ¿is set\nto 336 for every model and dataset, and the horizon length ğ» is\n[96,192,336,720]following the standard protocol. Here, GPHT*\ndenotes our model pretrained on the mixed dataset without any\nmodification, while GPHT represents the fine-tuned version for\neach dataset, as described in Section 3.6.\nAs shown in the table, we can draw some interesting conclusions.\nFirstly, self-supervised methods showcase competitive performance\nwith their supervised counterparts, underscoring the efficacy of\nGenerative Pretrained Hierarchical Transformer for\nTime Series Forecasting KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.\nadvanced pretraining techniques. Secondly, the tokenization tech-\nnique emerges as a crucial factor in achieving precise forecast-\ning, as evidenced by the outstanding performance of both FPT\nand patchTST among the baseline models. Finally, it appears that\nmodern forecasters have approached the precision limits of some\nbenchmark datasets, likely due to inherent noise and unpredictable\ndistribution shifts in the data. This phenomenon results in minimal\ndifferences in outcomes among various models.\nOn the other hand, in terms of our proposed GPHT, it consis-\ntently surpasses its competitors across most experimental settings.\nThe tuned GPHT model establishes the most accurate forecasting\nunder over 65% cases with its powerful counterparts. Specifically,\nGPHT exhibits an average MSE reduction of 9.23% on the Ex-\nchange dataset, 1.60% on the Traffic dataset, and 3.00% on the\nETTh1 dataset, in comparison with the best baseline under all ex-\nperimental forecasting lengths. Regarding MAE evaluation, the\nimprovements are more pronounced at 5.30%, 3.97%, and 5.07%\nrespectively. Although the relative improvements under certain\nsettings may not be as substantial, as we claimed before, the con-\nsistently superior performance provides strong evidence for the\neffectiveness of our approach. This further demonstrates that our\nmodel can better capture temporal dependencies in various data\nscenarios, benefiting from its pretraining on the mixed dataset.\nBesides, it is noteworthy that our model exhibits superior per-\nformance at relatively shorter horizon lengths. Specifically, GPHT\nsurpasses PatchTST on every dataset when ğ» = 96, resulting in an\naverage MAE reduction of 4.55%. We attribute this performance\nboost to the explicit modeling of temporal dependencies in the\noutput series. On the other hand, due to the inevitable error accu-\nmulation issue caused by the auto-regressive forecasting schema,\nthe superiority of GPHT decreases with longer ğ»s and the the\naverage reduction of MAE comes to 3.38% when ğ» = 720.\nEven more surprisingly, GPHT*, representing the model after\npretraining without any fine-tuning or modification , proves\nto be competitive with the baseline models. Specifically, GPHT*\nachieves the best or the second-best performance in 26 out of all\n64 settings. When compared to a single model, it outperforms FPT\nand supervised PatchTST under 44 and 40 experimental settings,\nrespectively. This result validates the feasibility of learning the com-\nmonalities of different time series by training on the mixed dataset\nwith the channel-independent assumption. It also underscores the\nincredible generalizability of our proposed model, primarily owing\nto the specially designed multi-stage hierarchical blocks and the\niterative residual learning strategy.\n4.3 Zero-shot Evaluation\nTo further highlight GPHTâ€™s capacity to learn general knowledge\nand discover common patterns from the mixed dataset, we con-\nduct zero-shot forecasting experiments on the Exchange, Weather,\nand Traffic datasets, originating from various data scenarios with\ndistinct scales. The zero-shot forecasting task is conceptually chal-\nlenging for methods that model cross-variable dependencies, hence,\nonly the channel-independent models are considered as baseline\nmodels. It is important to note that ForecastPFN [14] is specifically\ndesigned for zero-shot forecasting, but its performance is heavily\ncontingent on how the training data is synthesized, and as such, it\nis not included in this experiment. The models are trained on the\nmixed dataset comprised of the remaining 7 datasets and evaluated\ndirectly on the target dataset. The results are presented in Table 3.\nTable 3: Comparison on zero-shot forecasting task. The best\nresults are highlighted in bold.\nMethods GPHT FPT PatchTST DLinear\nMetric MSE MAE MSE MAE MSE MAE MSE MAE\nExchange\n96 0.098 0.219 0.104 0.226 0.102 0.227 0.169 0.316\n192 0.183 0.305 0.218 0.333 0.205 0.325 0.230 0.374\n336 0.321 0.411 0.391 0.460 0.362 0.440 0.334 0.444\n720 0.824 0.682 0.978 0.734 0.991 0.745 0.560 0.591\nTraffic\n96 0.411 0.291 0.447 0.331 0.433 0.314 0.453 0.328\n192 0.435 0.302 0.461 0.335 0.447 0.319 0.464 0.330\n336 0.460 0.316 0.477 0.343 0.465 0.329 0.481 0.340\n720 0.521 0.353 0.503 0.356 0.504 0.354 0.506 0.351\nWeather\n96 0.202 0.244 0.216 0.264 0.207 0.259 0.239 0.297\n192 0.248 0.283 0.260 0.301 0.257 0.299 0.275 0.325\n336 0.306 0.324 0.328 0.351 0.340 0.350 0.323 0.360\n720 0.389 0.377 0.414 0.403 0.414 0.402 0.392 0.405\nClearly, GPHT consistently outperforms other models across var-\nious settings, showcasing pronounced relative improvements. We\nattribute this success primarily to the multi-stage hierarchical trans-\nformer blocks, designed to capture diverse temporal patterns with\ndifferent resolutions. Consequently, GPHT exhibits better trans-\nferability to unseen time series. Additionally, all models achieve\nforecasting accuracy at an acceptable error level, with DLinear even\ndemonstrating unprecedented precision on the Exchange dataset\nwhen ğ» = 720. These results strongly validate the feasibility of\nour approach to learning commonalities in time series through\npretraining on a mixed dataset.\n4.4 Few-shot Evaluation\nIn real-world applications, the initial observation of time series\nmay be of a limited size, posing challenges for training accurate\nforecasters. To evaluate the representation power of GPHT under\nsuch circumstances, we conduct few-shot evaluations on the ETT\nand Electricity dataset. In detail, only a portion (10% or 5%, following\nexisting work [54]) of the training instances are used for training\nthe models, and we evaluate their MSE and MAE on the full test\nset. The results are reported in Table 4.\nIn comparison to the selected baseline models, GPHT consis-\ntently achieves superior performance across various experimental\nsettings, particularly on small-scale datasets. Specifically, when\ncompared to the best-performing baseline in the 10% setting, GPHT\ndemonstrates a relative average MSE reduction of16.02% and 7.02%\non the ETTh1 and ETTh2 datasets, respectively. With a further re-\nduction in the training data to only 5%, the relative improvements\nbecome even more pronounced, reaching30.19% and 12.49%. How-\never, it is noteworthy that GPHT exhibits poor forecasting perfor-\nmance on the ETTm1 dataset. We believe this observation may\nbe attributed to the potential heterogeneity of ETTm1 compared\nKDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Zhiding Liu et al.\nTable 4: Multivariate forecasting results under few-shot learning settings. The best results are highlighted in bold.\nPortion 5% 10%\nMethods GPHT FPT SimMTM PatchTST iTransformer GPHT FPT SimMTM PatchTST iTransformer\nMetric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE\nElectricity\n96 0.143 0.237 0.148 0.246 0.152 0.255 0.188 0.292 0.155 0.256 0.140 0.233 0.149 0.248 0.146 0.246 0.147 0.245 0.148 0.247\n192 0.162 0.254 0.163 0.259 0.167 0.268 0.202 0.304 0.172 0.272 0.159 0.250 0.164 0.261 0.163 0.262 0.162 0.258 0.167 0.266\n336 0.184 0.275 0.181 0.277 0.187 0.287 0.219 0.318 0.197 0.295 0.180 0.271 0.183 0.280 0.184 0.280 0.181 0.276 0.192 0.290\n720 0.238 0.321 0.231 0.315 0.240 0.326 0.264 0.351 0.261 0.344 0.231 0.313 0.234 0.318 0.242 0.325 0.230 0.315 0.244 0.329\nETTh1\n96 0.383 0.390 0.478 0.474 0.537 0.502 0.505 0.481 0.580 0.520 0.382 0.391 0.453 0.454 0.482 0.467 0.450 0.448 0.557 0.514\n192 0.426 0.416 0.705 0.577 0.580 0.525 0.576 0.514 0.670 0.557 0.424 0.418 0.522 0.494 0.532 0.498 0.523 0.489 0.668 0.562\n336 0.453 0.430 0.736 0.571 0.603 0.543 0.672 0.554 0.726 0.577 0.450 0.443 0.571 0.522 0.561 0.523 0.523 0.494 0.684 0.559\n720 0.433 0.440 0.718 0.579 0.708 0.597 0.759 0.625 0.802 0.626 0.427 0.442 0.574 0.535 0.734 0.617 0.508 0.502 0.709 0.587\nETTh2\n96 0.298 0.343 0.476 0.457 0.381 0.401 0.502 0.475 0.395 0.420 0.298 0.343 0.330 0.371 0.332 0.373 0.320 0.366 0.365 0.398\n192 0.368 0.386 0.714 0.573 0.435 0.435 0.569 0.511 0.448 0.453 0.367 0.387 0.419 0.422 0.391 0.411 0.400 0.416 0.432 0.439\n336 0.402 0.412 0.683 0.573 0.431 0.441 0.540 0.506 0.453 0.462 0.396 0.413 0.419 0.434 0.410 0.429 0.405 0.425 0.437 0.450\n720 0.417 0.429 0.648 0.557 0.450 0.459 0.506 0.494 0.483 0.484 0.409 0.428 0.506 0.485 0.448 0.460 0.483 0.474 0.463 0.471\nETTm1\n96 0.513 0.438 0.395 0.409 0.446 0.434 0.376 0.395 0.434 0.436 0.506 0.427 0.403 0.411 0.442 0.430 0.386 0.401 0.420 0.427\n192 0.552 0.464 0.410 0.417 0.461 0.436 0.391 0.402 0.472 0.456 0.563 0.458 0.430 0.426 0.454 0.431 0.406 0.413 0.472 0.456\n336 0.609 0.491 0.453 0.440 0.500 0.455 0.438 0.431 0.531 0.486 0.634 0.492 0.472 0.443 0.552 0.469 0.438 0.429 0.530 0.486\n720 0.685 0.581 0.742 0.566 0.590 0.503 0.549 0.495 0.615 0.527 0.721 0.534 0.665 0.526 0.715 0.539 0.499 0.464 0.629 0.533\nETTm2\n96 0.186 0.271 0.196 0.278 0.216 0.293 0.196 0.276 0.211 0.295 0.173 0.256 0.198 0.275 0.203 0.283 0.191 0.270 0.198 0.284\n192 0.248 0.311 0.263 0.316 0.267 0.324 0.258 0.315 0.269 0.322 0.234 0.297 0.263 0.315 0.256 0.315 0.252 0.308 0.254 0.318\n336 0.307 0.347 0.336 0.363 0.315 0.356 0.318 0.353 0.325 0.370 0.293 0.335 0.320 0.350 0.305 0.345 0.310 0.345 0.305 0.352\n720 0.412 0.409 0.453 0.430 0.406 0.406 0.447 0.427 0.441 0.434 0.398 0.395 0.426 0.412 0.397 0.398 0.398 0.397 0.405 0.408\nto other datasets, where the pretraining procedure might compro-\nmise generalizability when insufficient data is available. Addressing\nhow to identify and leverage dataset heterogeneities for enhanced\npretraining remains a subject for future exploration.\n4.5 Ablation Study\n4.5.1 Hierarchical Architecture. In this section, we explore the in-\nfluence of hierarchical transformer blocks on GPHTâ€™s performance.\nWe present the averaged MSE and MAE evaluations for GPHT\nwith varying stages of hierarchical transformer blocks across all\nbenchmark datasets, considering a forecasting horizon of ğ» = 720\n(see Figure 2). As the number of stages increases, GPHT is theoret-\nically better equipped to capture diverse temporal dependencies\nwithin the mixed dataset, such as different periodicities. The results\nstrongly affirm our hypothesis, as the 4-stage GPHT surpasses the\n1-stage GPHT (without hierarchical structures), achieving a notable\n2.86% reduction in MSE.\n4.5.2 On the Effect of Pretraining. In addition to the architecture\ndesign, another key aspect is the pretraining procedure on the\nmixed dataset. Does it pose positive effects on the forecasting per-\nformance? We provide quantified results in Figure 3. Specifically, we\ncompare the performance of fine-tuned GPHT with the one trained\nfrom scratch. The MAE evaluations averaged on the horizon length\n(i.e., ğ» = 96,192,336,720) are presented. From the results, we can\ninfer that pretraining on the mixed dataset enables the model to\nleverage commonalities among time series, facilitating better trans-\nfer to specific datasets. Compared to GPHT trained from scratch,\npretraining results in an average MAE reduction of5.75%, reaching\nas high as 9.65% on the ETTm2 dataset.\n0.408\n0.410\n0.412\n0.414\n0.416\n0.418\n0.420\n0.422\n0.424\n0.426\n0.428\n0.435\n0.440\n0.445\n0.450\n0.455\n0.460\n1 2 3 4 5\nMAE\nMSE\nNumber of the stages of hierarchical transformer blocks\nMSE MAE\nFigure 2: Performance comparison between GHPT with dif-\nferent stages of hierarchical transformer blocks.\n0.000\n0.050\n0.100\n0.150\n0.200\n0.250\n0.300\n0.350\n0.400\n0.450\n0.500MAE\nw/ pretrain w/o pretrain\nFigure 3: MAE evaluation between GPHT and GPHT without\npretraining on benchmark datasets.\nGenerative Pretrained Hierarchical Transformer for\nTime Series Forecasting KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.\n5 CONCLUSION\nIn this work, we proposed a generative pretrained hierarchical\ntransformer model, namely GPHT, for time series forecasting. It\nstands out in two key aspects. Conceptually, we explored training a\nsingle unified forecasting model that generalizes well across diverse\ndata scenarios and forecasting settings. Technically, we proposed\na simple yet effective paradigm that treats time series originating\nfrom various scopes as a whole, discarding the heterogeneity and\nconcatenating the values of each variable from different datasets\nto form the mixed dataset for pretraining. Besides, we replaced\nconventional one-step generating, which is adopted by most recent\nforecasting methods, with auto-regressive decoding for better flexi-\nbility and performance. We also introduced the hierarchical struc-\nture better to capture the diverse patterns in the mixed dataset. We\nconducted sufficient experiments on 8 widely used datasets in com-\nparison with mainstream self-supervised pretraining models and\nsupervised models, the results demonstrated that GPHT surpasses\nthe baseline models across various fine-tuning and zero/few-shot\nlearning settings in the traditional long-term forecasting task.\nACKNOWLEDGMENTS\nThis research was supported by grants from the Joint Research\nProject of the Science and Technology Innovation Community in\nYangtze River Delta (No. 2023CSJZN0200), and the Fundamental\nResearch Funds for the Central Universities. This work also thanked\nto the support of funding MAI2022C007.\nREFERENCES\n[1] Kaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiaotao Gu, and Qi Tian. 2023.\nAccurate medium-range global weather forecasting with 3D neural networks.\nNature (2023), 1â€“6.\n[2] George EP Box and Gwilym M Jenkins. 1968. Some recent advances in forecasting\nand control. Journal of the Royal Statistical Society. Series C (Applied Statistics) 17,\n2 (1968), 91â€“109.\n[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot learners. Advances in neural\ninformation processing systems 33 (2020), 1877â€“1901.\n[4] Cristian Challu, Kin G. Olivares, Boris N. Oreshkin, Federico Garza, Max\nMergenthaler-Canseco, and Artur Dubrawski. 2022. N-HiTS: Neural Hierar-\nchical Interpolation for Time Series Forecasting. arXiv:2201.12886 [cs.LG]\n[5] Weiqi Chen, Wenwei Wang, Bingqing Peng, Qingsong Wen, Tian Zhou, and Liang\nSun. 2022. Learning to rotate: Quaternion transformer for complicated periodical\ntime series forecasting. In Proceedings of the 28th ACM SIGKDD Conference on\nKnowledge Discovery and Data Mining . 146â€“156.\n[6] Mingyue Cheng, Qi Liu, Zhiding Liu, Zhi Li, Yucong Luo, and Enhong Chen. 2023.\nFormerTime: Hierarchical Multi-Scale Representations for Multivariate Time\nSeries Classification. In Proceedings of the ACM Web Conference 2023 . 1437â€“1445.\n[7] Mingyue Cheng, Qi Liu, Zhiding Liu, Hao Zhang, Rujiao Zhang, and Enhong\nChen. 2023. TimeMAE: Self-Supervised Representations of Time Series with\nDecoupled Masked Autoencoders. arXiv preprint arXiv:2303.00320 (2023).\n[8] Mingyue Cheng, Xiaoyu Tao, Qi Liu, Hao Zhang, Yiheng Chen, and Chenyi\nLei. 2024. Learning Transferable Time Series Classifier with Cross-Domain\nPre-training from Language Model. arXiv preprint arXiv:2403.12372 (2024).\n[9] Mingyue Cheng, Jiqian Yang, Tingyue Pan, Qi Liu, and Zhi Li. 2024. Convtimenet:\nA deep hierarchical fully convolutional model for multivariate time series analysis.\narXiv preprint arXiv:2403.01493 (2024).\n[10] Abhimanyu Das, Weihao Kong, Rajat Sen, and Yichen Zhou. 2023. A decoder-only\nfoundation model for time-series forecasting. arXiv preprint arXiv:2310.10688\n(2023).\n[11] Jinliang Deng, Xiusi Chen, Renhe Jiang, Xuan Song, and Ivor W Tsang. 2021. St-\nnorm: Spatial and temporal normalization for multi-variate time series forecasting.\nIn Proceedings of the 27th ACM SIGKDD conference on knowledge discovery & data\nmining. 269â€“278.\n[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:\nPre-training of deep bidirectional transformers for language understanding.arXiv\npreprint arXiv:1810.04805 (2018).\n[13] Jiaxiang Dong, Haixu Wu, Haoran Zhang, Li Zhang, Jianmin Wang, and Ming-\nsheng Long. 2023. SimMTM: A Simple Pre-Training Framework for Masked\nTime-Series Modeling. arXiv preprint arXiv:2302.00861 (2023).\n[14] Samuel Dooley, Gurnoor Singh Khurana, Chirag Mohapatra, Siddartha Venkat\nNaidu, and Colin White. 2023. ForecastPFN: Synthetically-Trained Zero-Shot\nForecasting. In Thirty-seventh Conference on Neural Information Processing Sys-\ntems.\n[15] Emadeldeen Eldele, Mohamed Ragab, Zhenghua Chen, Min Wu, Chee Keong\nKwoh, Xiaoli Li, and Cuntai Guan. 2021. Time-series representation learning via\ntemporal and contextual contrasting. arXiv preprint arXiv:2106.14112 (2021).\n[16] Azul Garza and Max Mergenthaler-Canseco. 2023. TimeGPT-1. arXiv preprint\narXiv:2310.03589 (2023).\n[17] Rakshitha Godahewa, Christoph Bergmeir, Geoffrey I Webb, Rob J Hyndman,\nand Pablo Montero-Manso. 2021. Monash time series forecasting archive. arXiv\npreprint arXiv:2105.06643 (2021).\n[18] Jean-Bastien Grill, Florian Strub, Florent AltchÃ©, Corentin Tallec, Pierre\nRichemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan\nGuo, Mohammad Gheshlaghi Azar, et al. 2020. Bootstrap your own latent-a new\napproach to self-supervised learning. Advances in neural information processing\nsystems 33 (2020), 21271â€“21284.\n[19] Nate Gruver, Marc Anton Finzi, Shikai Qiu, and Andrew Gordon Wilson. 2023.\nLarge Language Models Are Zero-Shot Time Series Forecasters. InThirty-seventh\nConference on Neural Information Processing Systems .\n[20] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr DollÃ¡r, and Ross Girshick.\n2022. Masked autoencoders are scalable vision learners. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition . 16000â€“16009.\n[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual\nlearning for image recognition. In Proceedings of the IEEE conference on computer\nvision and pattern recognition . 770â€“778.\n[22] Wenqiang He, Mingyue Cheng, Qi Liu, and Zhi Li. 2023. ShapeWordNet: An\nInterpretable Shapelet Neural Network for Physiological Signal Classification. In\nInternational Conference on Database Systems for Advanced Applications . Springer,\n353â€“369.\n[23] Junji Jiang, Likang Wu, Hongke Zhao, Hengshu Zhu, and Wei Zhang. 2023.\nForecasting movements of stock time series based on hidden state guided deep\nlearning approach. Information Processing & Management 60, 3 (2023), 103328.\n[24] Guangyin Jin, Yuxuan Liang, Yuchen Fang, Zezhi Shao, Jincai Huang, Junbo\nZhang, and Yu Zheng. 2023. Spatio-temporal graph neural networks for predictive\nlearning in urban computing: A survey. IEEE Transactions on Knowledge and\nData Engineering (2023).\n[25] Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, and\nJaegul Choo. 2021. Reversible instance normalization for accurate time-series\nforecasting against distribution shift. In International Conference on Learning\nRepresentations.\n[26] Minhao Liu, Ailing Zeng, Muxi Chen, Zhijian Xu, Qiuxia Lai, Lingna Ma, and\nQiang Xu. 2022. SCINet: Time Series Modeling and Forecasting with Sample\nConvolution and Interaction. Thirty-sixth Conference on Neural Information\nProcessing Systems (NeurIPS), 2022 (2022).\n[27] Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and\nMingsheng Long. 2023. itransformer: Inverted transformers are effective for time\nseries forecasting. arXiv preprint arXiv:2310.06625 (2023).\n[28] Zhiding Liu, Mingyue Cheng, Zhi Li, Zhenya Huang, Qi Liu, Yanhu Xie, and\nEnhong Chen. 2023. Adaptive Normalization for Non-stationary Time Series\nForecasting: A Temporal Slice Perspective. InThirty-seventh Conference on Neural\nInformation Processing Systems .\n[29] Yiwei Lou, Yu Huang, Xuliang Xing, Yongzhi Cao, and Hanpin Wang. 2022.\nMts-lstdm: multi-time-scale long short-term double memory for power load\nforecasting. Journal of systems architecture 125 (2022), 102443.\n[30] Feng Lu, Wei Li, Zhiqiang Zhou, Cheng Song, Yifei Sun, Yuwei Zhang, Yufei\nRen, Xiaofei Liao, Hai Jin, Ailin Luo, et al . 2023. A composite multi-attention\nframework for intraoperative hypotension early warning. In Proceedings of the\nAAAI Conference on Artificial Intelligence , Vol. 37. 14374â€“14381.\n[31] Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. 2022.\nA Time Series is Worth 64 Words: Long-term Forecasting with Transformers. In\nThe Eleventh International Conference on Learning Representations .\n[32] Boris N Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. 2019. N-\nBEATS: Neural basis expansion analysis for interpretable time series forecasting.\nIn International Conference on Learning Representations .\n[33] Nikolaos Passalis, Anastasios Tefas, Juho Kanniainen, Moncef Gabbouj, and\nAlexandros Iosifidis. 2019. Deep adaptive input normalization for time series\nforecasting. IEEE transactions on neural networks and learning systems 31, 9 (2019),\n3760â€“3765.\n[34] GÃ¡bor PetnehÃ¡zi. 2019. Recurrent neural networks for time series forecasting.\narXiv preprint arXiv:1901.00069 (2019).\n[35] Fotios Petropoulos, Daniele Apiletti, Vassilios Assimakopoulos, Mohamed Zied\nBabai, Devon K Barrow, Souhaib Ben Taieb, Christoph Bergmeir, Ricardo J Bessa,\nJakub Bijak, John E Boylan, et al. 2022. Forecasting: theory and practice. Interna-\ntional Journal of Forecasting (2022).\nKDD â€™24, August 25â€“29, 2024, Barcelona, Spain. Zhiding Liu et al.\n[36] David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. 2020.\nDeepAR: Probabilistic forecasting with autoregressive recurrent networks. Inter-\nnational Journal of Forecasting 36, 3 (2020), 1181â€“1191.\n[37] Mohammad Amin Shabani, Amir H Abdi, Lili Meng, and Tristan Sylvain. 2022.\nScaleformer: Iterative Multi-scale Refining Transformers for Time Series Fore-\ncasting. In The Eleventh International Conference on Learning Representations .\n[38] Zezhi Shao, Zhao Zhang, Fei Wang, and Yongjun Xu. 2022. Pre-training enhanced\nspatial-temporal graph neural network for multivariate time series forecasting.\nIn Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and\nData Mining . 1567â€“1577.\n[39] Sana Tonekaboni, Danny Eytan, and Anna Goldenberg. 2020. Unsupervised\nRepresentation Learning for Time Series with Temporal Neighborhood Coding.\nIn International Conference on Learning Representations .\n[40] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing systems 30 (2017).\n[41] Qingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi Chen, Ziqing Ma, Junchi Yan,\nand Liang Sun. 2022. Transformers in time series: A survey. arXiv preprint\narXiv:2202.07125 (2022).\n[42] Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven Hoi. 2021.\nCoST: Contrastive Learning of Disentangled Seasonal-Trend Representations for\nTime Series Forecasting. In International Conference on Learning Representations .\n[43] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng\nLong. 2022. TimesNet: Temporal 2D-Variation Modeling for General Time Series\nAnalysis. In The Eleventh International Conference on Learning Representations .\n[44] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. 2021. Autoformer: De-\ncomposition transformers with auto-correlation for long-term series forecasting.\nAdvances in Neural Information Processing Systems 34 (2021), 22419â€“22430.\n[45] Zhihan Yue, Yujing Wang, Juanyong Duan, Tianmeng Yang, Congrui Huang,\nYunhai Tong, and Bixiong Xu. 2022. Ts2vec: Towards universal representation of\ntime series. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 36.\n8980â€“8987.\n[46] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. 2023. Are Transformers\nEffective for Time Series Forecasting? Proceedings of the AAAI Conference on\nArtificial Intelligence.\n[47] George Zerveas, Srideepika Jayaraman, Dhaval Patel, Anuradha Bhamidipaty,\nand Carsten Eickhoff. 2021. A transformer-based framework for multivariate time\nseries representation learning. In Proceedings of the 27th ACM SIGKDD conference\non knowledge discovery & data mining . 2114â€“2124.\n[48] G Peter Zhang. 2003. Time series forecasting using a hybrid ARIMA and neural\nnetwork model. Neurocomputing 50 (2003), 159â€“175.\n[49] Kexin Zhang, Qingsong Wen, Chaoli Zhang, Rongyao Cai, Ming Jin, Yong Liu,\nJames Zhang, Yuxuan Liang, Guansong Pang, Dongjin Song, et al . 2023. Self-\nSupervised Learning for Time Series Analysis: Taxonomy, Progress, and Prospects.\narXiv preprint arXiv:2306.10125 (2023).\n[50] Yuchen Zhang, Mingsheng Long, Kaiyuan Chen, Lanxiang Xing, Ronghua Jin,\nMichael I Jordan, and Jianmin Wang. 2023. Skilful nowcasting of extreme precip-\nitation with NowcastNet. Nature 619, 7970 (2023), 526â€“532.\n[51] Yunhao Zhang and Junchi Yan. 2022. Crossformer: Transformer utilizing cross-\ndimension dependency for multivariate time series forecasting. In The Eleventh\nInternational Conference on Learning Representations .\n[52] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong,\nand Wancai Zhang. 2021. Informer: Beyond efficient transformer for long se-\nquence time-series forecasting. InProceedings of the AAAI Conference on Artificial\nIntelligence, Vol. 35. 11106â€“11115.\n[53] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. 2022.\nFEDformer: Frequency enhanced decomposed transformer for long-term series\nforecasting. In Proc. 39th International Conference on Machine Learning (ICML\n2022) (Baltimore, Maryland).\n[54] Tian Zhou, Peisong Niu, Liang Sun, Rong Jin, et al . 2023. One fits all: Power\ngeneral time series analysis by pretrained lm. Advances in neural information\nprocessing systems 36 (2023), 43322â€“43355.\nA COMPLEXITY COMPARISON\nTo better illustrate the proposed GPHT modelâ€™s computation cost,\nwe provide quantitative results on the Electricity dataset under\nlookback window ğ¿ = 336 and forecasting horizon ğ» = 720 in\nTable 5. In summary, the proposed GPHT model is medium-sized\ncompared to the baseline models. Thanks to its straightforward\noptimization objective, both the pretraining and finetuning pro-\ncesses of GPHT are quite efficient and do not require too much\ntime, especially compared with the pretraining-based approaches.\nA key drawback of our model might be the inference speed, which\nis naturally limited by the auto-regressive decoding schema.\nB QUALITATIVE EVALUATION\nIn this section, we provide visualizations of long-term forecasting\nresults to better demonstrate the performance of GPHT and the\neffectiveness of the hierarchical transformer architecture.\nWe plot a forecasting sample in Figure 4, illustrating how GPHT\nforecasts with the hierarchical architecture and the iterative residual\nschema. It can be referred that the initial stages predominantly\nfocus on extracting the general periodic patterns from the input\nseries and the latter stages can therefore pay more attention to the\nspecialized trends, since the auto-regressive forecasting results of\nstage 3 align more closely with the input series, albeit with less\nperiodicity than the preceding stages. The results strongly verify\nour assumption that the hierarchical architecture can better capture\nthe commonalities and specialties of the mixed pertaining dataset,\nand the iterative residual schema effectively refines the input for\nthe next stage, eliminating the redundant information in the series.\nBesides, we provide qualitative comparison between GPHT and\nmainstream supervised forecasting methods in Figure 5 on various\ndatasets. Benefiting from the auto-regressive pertaining and the\nhierarchical architecture, GPHT can better capture the temporal\ndependencies so as to achieve better performance.\nGenerative Pretrained Hierarchical Transformer for\nTime Series Forecasting KDD â€™24, August 25â€“29, 2024, Barcelona, Spain.\nTable 5: Computation cost between GPHT and mainstream forecasting approaches.\nMethods Params Training Time(per epoch) Inference Speed(itr/s)\nGPHT 37.98M(pretraining)/98.50K(finetuning) 20min(pretraining)/254.1s(finetuning) 0.34\nFPT 105.20M(24.00M trainable) 3858.8s(finetuning) 0.69\nSimMTM 62.14M(pretraining)/7.76M(finetuning) 73min(pretraining)/946.5s(finetuning) 5.98\nPatchTST 4.27M 128.9s 9.02\niTransformer 5.28M 24.7s 26.39\nTimesNet 150.64M 1179.6s 1.51\nFigure 4: Visualization of the input and corresponding output series of GPHTâ€™s multiple stages on a sample from the ETTh1\ndataset.\n(a) GPHT (b) PatchTST (c) iTransformer (d) DLinear\nETTh1\n Exchange\nTraffic\nFigure 5: Illustration of forecasting showcases comparing GPHT and baseline models. The lookback window is set to 336 and\nthe forecasting horizon is set to 336, 192, 96 for the Exchange, Traffic, and ETTh1 dataset respectively.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7492764592170715
    },
    {
      "name": "Generative grammar",
      "score": 0.6008303165435791
    },
    {
      "name": "Computer science",
      "score": 0.5726048946380615
    },
    {
      "name": "Artificial intelligence",
      "score": 0.512923002243042
    },
    {
      "name": "Series (stratigraphy)",
      "score": 0.4898686110973358
    },
    {
      "name": "Machine learning",
      "score": 0.42065608501434326
    },
    {
      "name": "Generative model",
      "score": 0.41918814182281494
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.41859662532806396
    },
    {
      "name": "Engineering",
      "score": 0.1847255825996399
    },
    {
      "name": "Electrical engineering",
      "score": 0.06766629219055176
    },
    {
      "name": "Voltage",
      "score": 0.047477662563323975
    },
    {
      "name": "Geology",
      "score": 0.04681354761123657
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ],
  "institutions": []
}