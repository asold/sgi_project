{
  "title": "Building Chinese Biomedical Language Models via Multi-Level Text Discrimination",
  "url": "https://openalex.org/W3206448387",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2024693365",
      "name": "Wang Quan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4286975062",
      "name": "Dai, Songtai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4286975063",
      "name": "Xu, Benfeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221657540",
      "name": "Lyu, Yajuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2009793964",
      "name": "Zhu Yong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1966609866",
      "name": "Wu Hua",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1861698454",
      "name": "Wang Haifeng",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3166508187",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3155510817",
    "https://openalex.org/W2798734500",
    "https://openalex.org/W3035124264",
    "https://openalex.org/W3092364978",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2810649101",
    "https://openalex.org/W3104453885",
    "https://openalex.org/W3131870090",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W3169341408",
    "https://openalex.org/W3102725307",
    "https://openalex.org/W3102372184",
    "https://openalex.org/W3100452049",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W3026732421",
    "https://openalex.org/W3034978746",
    "https://openalex.org/W2004763266",
    "https://openalex.org/W3166861822",
    "https://openalex.org/W3099950029",
    "https://openalex.org/W3128560087",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3175423875",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W2799125718",
    "https://openalex.org/W2939876107",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W3007672467",
    "https://openalex.org/W3081505754",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W3193921029"
  ],
  "abstract": "Pre-trained language models (PLMs), such as BERT and GPT, have revolutionized the field of NLP, not only in the general domain but also in the biomedical domain. Most prior efforts in building biomedical PLMs have resorted simply to domain adaptation and focused mainly on English. In this work we introduce eHealth, a Chinese biomedical PLM built from scratch with a new pre-training framework. This new framework pre-trains eHealth as a discriminator through both token- and sequence-level discrimination. The former is to detect input tokens corrupted by a generator and recover their original identities from plausible candidates, while the latter is to further distinguish corruptions of a same original sequence from those of others. As such, eHealth can learn language semantics at both token and sequence levels. Extensive experiments on 11 Chinese biomedical language understanding tasks of various forms verify the effectiveness and superiority of our approach. We release the pre-trained model at \\url{https://github.com/PaddlePaddle/Research/tree/master/KG/eHealth} and will also release the code later.",
  "full_text": "Building Chinese Biomedical Language Models via Multi-Level\nText Discrimination\nQuan Wang1†∗, Songtai Dai1†, Benfeng Xu2†‡, Yajuan Lyu1\nYong Zhu1, Hua Wu1, Haifeng Wang1\n1Baidu Inc., Beijing, China\n2University of Science and Technology of China, Hefei, China\nAbstract\nPre-trained language models (PLMs), such as\nBERT and GPT, have revolutionized the ﬁeld\nof NLP, not only in the general domain but also\nin the biomedical domain. Most prior efforts in\nbuilding biomedical PLMs have resorted sim-\nply to domain adaptation and focused mainly\non English. In this work we introduce eHealth,\na Chinese biomedical PLM built from scratch\nwith a new pre-training framework. This new\nframework pre-trains eHealth as a discrimina-\ntor through both token- and sequence-level dis-\ncrimination. The former is to detect input to-\nkens corrupted by a generator and recover their\noriginal identities from plausible candidates,\nwhile the latter is to further distinguish corrup-\ntions of a same original sequence from those of\nothers. As such, eHealth can learn language se-\nmantics at both token and sequence levels. Ex-\ntensive experiments on 11 Chinese biomedical\nlanguage understanding tasks of various forms\nverify the effectiveness and superiority of our\napproach. We release the pre-trained model to\nthe public,1 and will also release the code later.\n1 Introduction\nPre-trained language models (PLMs) such as BERT\n(Devlin et al., 2019) and its variants (Yang et al.,\n2019; Liu et al., 2019) have revolutionized the ﬁeld\nof NLP, establishing new state-of-the-art on con-\nventional language understanding and generation\ntasks. Following the great success in the general do-\nmain, researchers have started to investigate build-\ning domain-speciﬁc PLMs in highly specialized\ndomains, e.g., science (Beltagy et al., 2019), law\n(Chalkidis et al., 2020), or ﬁnance (Liu et al., 2020).\nBiomedicine and healthcare, as a ﬁeld with large,\nrapidly growing volume of free text and continually\n∗Correspondence to quanwang1012@gmail.com.\n†Contributed equally to this work.\n‡Work done during internship at Baidu Inc.\n1https://github.com/PaddlePaddle/\nResearch/tree/master/KG/eHealth\nincreasing demand for text mining, has received\nmassive attention and achieved rapid progress.\nBiomedical PLMs are typically built by adapting\na general-domain PLM to the biomedical domain\nwith (almost) the same model architecture and train-\ning objectives, as exempliﬁed by BioBERT (Lee\net al., 2020), PubMedBERT (Gu et al., 2020), and\nBioELECTRA (Kanakarajan et al., 2021). This\ndomain adaptation is achieved via either continual\npre-training on in-domain text (Gururangan et al.,\n2020), or pre-training from scratch further with an\nin-domain vocabulary (Gu et al., 2020; Lewis et al.,\n2020b), which has shown to be particularly useful\nfor English biomedical text understanding.\nAs for the Chinese biomedical ﬁeld, MC-BERT\n(Zhang et al., 2020) and PCL-MedBERT are two\ninitial attempts that continually pre-train a general-\ndomain BERT on in-domain text. But unfortunately\nthey fail to achieve satisfactory performance com-\npared with their general-domain rivals (Zhang et al.,\n2021a). SMedBERT (Zhang et al., 2021b) and EM-\nBERT (Cai et al., 2021) also continually pre-train\nfrom the general-domain BERT, but in knowledge-\nenhanced fashions. These two models rely on ex-\nternal (and often private) knowledge and have not\nbeen released to the public yet. So far there is still a\nlack of publicly available, high-quality biomedical\nPLMs in Chinese.\nIn this paper we present eHealth, a Chinese lan-\nguage representation model pre-trained over large-\nscale biomedical text corpora. Unlike most previ-\nous studies that simply resort to direct domain adap-\ntation, we build eHealth with a new self-supervised\nlearning framework, which, similar to ELECTRA\n(Clark et al., 2020), consists of a discriminator and\na generator. The generator is to produce corrupted\ninput, and the discriminator, as the ﬁnal target en-\ncoder, is trained via multi-level text discrimination.\nSpeciﬁcally, we employ (i) token-level discrimina-\ntion that discriminates corrupted tokens from origi-\nnal ones, and (ii)sequence-level discrimination that\narXiv:2110.07244v2  [cs.CL]  2 Mar 2022\nfurther discriminates corruptions of a same original\nsequence from those of others in a contrastive learn-\ning fashion (Chen et al., 2020). This multi-level\ndiscrimination enables eHealth to learn language\nsemantics at both token and sequence levels.\nAs a new Chinese biomedical PLM, eHealth has\ntwo distinguishing features: built-from-scratch and\neasy-to-deploy. By the former we mean that unlike\nall prior arts that start pre-training from a general-\ndomain Chinese BERT and directly use the asso-\nciated vocabulary, eHealth is pre-trained entirely\nfrom scratch with a newly built in-domain vocabu-\nlary. This vocabulary, as we will show later in our\nexperiments, can better tokenize biomedical text\nand may lead to better understanding of such text.\nAnd by the latter we mean that eHealth relies solely\non the text itself, requiring no additional retrieval,\nlinking, or encoding of relevant knowledge as those\nknowledge-enhanced models do, and thereby could\nbe applied rather easily during ﬁne-tuning.\nWe evaluate eHealth on 11 diversiﬁed Chinese\nbiomedical language understanding tasks, includ-\ning (i) the 8 tasks of text classiﬁcation and match-\ning, medical information extraction, and medical\nterm normalization from the CBLUE benchmark\n(Zhang et al., 2021a), and (ii) another 3 medical\nquestion answering tasks cMedQNLI (Zhang et al.,\n2020), webMedQA (He et al., 2019), and NLPEC\n(Li et al., 2020). Experimental results reveal that\neHealth, as a standard base-sized model pre-trained\nfrom scratch on biomedical corpora, consistently\noutperforms previous state-of-the-art PLMs in al-\nmost all cases, no matter those from the general\ndomain or biomedical domain, and no matter those\nbase-sized or even large-sized.\nThe main contributions of this work are two-fold.\nFirstly, we propose a new Chinese biomedical PLM\nand release the pre-trained model to the public.\nThis new model shows superior ability in Chinese\nbiomedical text understanding and is easy to deploy.\nSecondly, we devise a new algorithm for language\nmodel pre-training and verify its effectiveness in\nthe biomedical domain. This pre-training algorithm\nis quite generic and may be readily adapted to other\ndomains beyond biomedicine. We leave such ex-\nploration open to future work.\n2 Background\nBefore diving into the details of our approach, we\nbrieﬂy discuss related studies on building PLMs in\ngeneral and biomedical domains.\nGeneral Domain PLMs. Recent years have seen\nremarkable success of PLMs in the ﬁeld of NLP.\nThese PLMs are typically built with self-supervised\nlearning over massive unlabeled text in the general\ndomain, e.g., Wikipedia, newswire, or Web articles\n(Radford et al., 2018). Masked language modeling\n(MLM), which trains a model to recover the identi-\nties of a small subset of masked-out tokens (typi-\ncally 15%), is the most prevailing self-supervised\nobjective, ﬁrst introduced in BERT (Devlin et al.,\n2019) and then widely adopted by follow-up stud-\nies (Liu et al., 2019; Lan et al., 2020; Joshi et al.,\n2020; Sun et al., 2020). Despite their effectiveness\nand popularity, MLM-based approaches can only\nlearn from those 15% masked-out tokens per input,\nand therefore incur high compute costs.\nTo address this low efﬁciency issue, ELECTRA\n(Clark et al., 2020) uses a new pre-training frame-\nwork. Speciﬁcally, it corrupts an input sequence by\nreplacing some of the tokens with plausible alter-\nnatives sampled from an auxiliary generator, and\ntrains a discriminator to predict for each token in\nthat sequence whether it is original or replaced, i.e.,\nreplaced token detection (RTD). As the discrimina-\ntor can learn from all input tokens rather than just\n15% of them, ELECTRA enjoys better efﬁciency\nand accelerates training.\nWhile achieving empirical success, there are con-\ncerns about whether the over-simpliﬁed RTD task\nof ELECTRA, as a binary classiﬁcation problem, is\ninformative enough for language modeling (Aroca-\nOuellette and Rudzicz, 2020). Xu et al. (2020) and\nShen et al. (2021) thus proposed training the model\nvia a generalization of RTD while a simpliﬁcation\nof MLM, by recovering for each token its origi-\nnal identity from a few plausible candidates, rather\nthan from the whole vocabulary.\nAnother limitation of ELECTRA is that it is pre-\ntrained solely at the token level but lacks semantics\nat the sequence level. Incorporating sequence level\nsignals, e.g., next sentence prediction (Devlin et al.,\n2019), sentence order prediction (Lan et al., 2020),\nand sentence contrastive learning (Fang et al., 2020;\nMeng et al., 2021), has been widely accepted in the\ncommunity and shown to be beneﬁcial in speciﬁc\ntasks (Lewis et al., 2020a; Guu et al., 2020).\nIn this paper, to build a Chinese biomedical PLM,\nwe employ the ELECTRA framework which favors\nthe efﬁciency of pre-training. Within this frame-\nwork, we strengthen the oversimpliﬁed RTD task\nand introduce sequence-level signals, which further\nimproves the quality of pre-training.\nBiomedical PLMs. Continual pre-training is per-\nhaps the most straightforward way to build biomed-\nical PLMs, in which the model weights are initial-\nized from a well-trained general-domain model and\nthe same vocabulary is used (Alsentzer et al., 2019;\nLee et al., 2020). Also, there are ﬁndings showing\nthat pre-training from scratch using domain spe-\nciﬁc data along with domain speciﬁc vocabulary\nwould bring further improvements, particularly in\nEnglish (Gu et al., 2020; Lewis et al., 2020b). Early\nattempts focused on adapting BERT, while recent\nstudies have switched to its modern variants like\nRoBERTa, ALBERT, and ELECTRA (Kanakara-\njan et al., 2021; Alrowili and Shanker, 2021).\nWhile great efforts have been made to build En-\nglish biomedical PLMs, there is only a few studies\ndiscussing building biomedical PLMs in Chinese,\ne.g., MC-BERT (Zhang et al., 2020), SMedBERT\n(Zhang et al., 2021b), and EMBERT (Cai et al.,\n2021), all resumed from a general-domain BERT,\nwith the latter two further in knowledge-enhanced\nfashions.2 Models like this typically require extra\nknowledge and consequently the retrieval, linking,\nand encoding of such knowledge. They are not that\neasy to be applied to downstream tasks.\n3 Methodology\nThis section presents eHealth, a Chinese language\nmodel pre-trained from biomedical text. It in gen-\neral follows the generator-discriminator framework\nof ELECTRA, where the generator Gis introduced\nto construct pre-training signals and the discrimina-\ntor Dis used as the ﬁnal target encoder. But unlike\nELECTRA that merely adopts a token-level binary\nclassiﬁcation to train the discriminator, we train it\nwith (i) a more informative token-level discrimina-\ntion, and (ii) another sequence-level discrimination.\nThe overview of eHealth is illustrated in Figure 1.\n3.1 Generator\nThe generator Gis a Transformer encoder (Vaswani\net al., 2017) trained by masked language modeling\n(MLM). Given an input sequence x=[x1,··· ,xn],\nit ﬁrst selects a random set of positions to mask\nout and replaces tokens at these positions with a\nspecial symbol [MASK].3 This masked sequence,\n2Actually there are two versions of EMBERT, one initial-\nized with BERT and the other with MC-BERT, which is also\nresumed from BERT.\n3Typically 15% of the tokens are masked out, among which\n80% are replaced with [MASK], 10% replaced with a random\nGenerator\nDiscriminator\n[CLS] A _ C _ E F\n[CLS] A B C D E F\n{B, B', B''} {D, D', D''}\nO O O R O O\n[CLS] A B _ D _ F\nGenerator \nDiscriminator\n[CLS] A B C D E F\n{C, C', C''} {E, E', E''}\nO O R O R O\npostive\n+ select \nC\n+ select \nE\nsampling sampling\nOriginal input: ABCDEF\n+ select \nD\nRTD \nMTS\nCSP O: original\nR: replaced\nFigure 1: Overview of eHealth. Each input sequence is\ncorrupted twice independently by the generator. These\ntwo corruptions are fed into the discriminator for re-\nplaced token detection (RTD) and multi-token selection\n(MTS), i.e., token-level discrimination. And they also\nform a positive pair for contrastive sequence prediction\n(CSP), i.e., sequence-level discrimination.\ndenoted as xM , is then passed into the Transformer\nencoder to produce contextualized representations\nhG(xM ), and thereafter a softmax layer to predict\nthe original identities of those masked-out tokens:\npG(xt|xM ) = exp\n(\ne(xt)ThG(xM )t\n)\n∑\nx′∈V exp\n(\ne(x′)ThG(xM )t\n). (1)\nHere, pG(xt|xM ) is the probability that Gpredicts\ntoken xt appears at the t-th masked position in xM ,\nhG(xM )t the contextualized representation for that\nposition, e(·) the embedding lookup operation on\neach token, and V the vocabulary of all tokens. The\ncorresponding loss function is:\nLMLM(x,xM ; G) =\n∑\nt:xM\nt =[MASK]\n−log pG(xt|xM ), (2)\nwhere the summation is taken only over the masked\npositions. The generator is used to construct pre-\ntraining signals for the discriminator, and will be\ndiscarded after pre-training.\n3.2 Discriminator\nThe discriminator D, as our ﬁnal target encoder, is\nalso a Transformer architecture. It takes as input\ncorrupted sequences constructed by the generator,\nand is trained through two-level text discrimination,\ni.e., token-level and sequence-level, so as to encode\nlanguage semantics at both levels.\nToken-Level Discrimination. We consider two\ntoken-level tasks: replaced token detection (RTD)\nand multi-token selection (MTS). RTD is the stan-\ndard pre-training task of ELECTRA, which detects\ntoken, and 10% kept unchanged.\nreplaced tokens in a corrupted sequence, and MTS\nfurther selects original identities for those replaced\ntokens. Speciﬁcally, given input sequence x and\nits masked version xM , for each masked position t,\nwe sample a token from the generator’s prediction\nˆxt ∼pG(xt|xM ) (cf. Eq. (1)), replace the original\ntoken xt with ˆxt, and create a corrupted sequence\nxR. We also create a set of candidate tokens, de-\nnoted as St, for each masked positiont, by drawing\nknon-original tokens from pG(xt|xM ) along with\nthe original token xt. The discriminator Dencodes\nthe corrupted sequence xR and produces contextu-\nalized representations hD(xR).\nRTD learns to discriminate whether each token\nin xR is original or replaced, i.e., coming from the\ntrue data distribution or the generator distribution.\nIt uses a sigmoid layer on top ofhD(xR) to perform\nthis binary classiﬁcation, where the probability that\nxR\nt matches the original token xt is determined as:\npD(xR\nt = xt) = 1\n1 + exp(−wThD(xR)t), (3)\nand the corresponding loss function is:\nLRTD(x,xR;D)=\nn∑\nt=1\n[\n−1 (xR\nt =xt) logpD(xR\nt =xt)\n−1 (xR\nt ̸=xt) log(1−pD(xR\nt =xt))\n]\n. (4)\nAs merely a binary classiﬁcation task, RTD might\nnot be informative enough for language modeling.\nMTS strengthens RTD by training the discrimi-\nnator to further recover original identities of those\nreplaced tokens. For each position twhere the to-\nken is replaced, i.e., xR\nt ̸= xt, MTS corrects the\ntoken and recovers its original identity from candi-\ndate set St. The probability of picking the original\nidentity xt out of St for the correction is:\npD(xt|xR,St) = exp\n(\ne(xt)ThD(xR)t\n)\n∑\nx′∈St exp\n(\ne(x′)ThD(xR)t\n), (5)\nwhere e(·) is again the embedding lookup opera-\ntion. The loss function is deﬁned as:\nLMTS(x,xR,S; D) =\n∑\nt:xR\nt ̸=xt\n−log pD(xt|xR,St), (6)\nwhere S= {St}t:xR\nt ̸=xt is a collection of candidate\nsets at all positions with replaced tokens, and the\nsummation is taken only over these positions. MTS\nis essentially a (k+ 1)-class classiﬁcation problem.\nIt is more challenging than RTD and hence pushes\nthe discriminator to learn representations that en-\ncode richer semantic information (Xu et al., 2020;\nShen et al., 2021).\nSequence-Level Discrimination. Besides token-\nlevel tasks, we consider a sequence-level task in ad-\ndition, i.e., contrastive sequence prediction (CSP)\nwhich learns to discriminate corruptions of a single\noriginal sequence from those of the others. CSP\nemploys a classic contrastive learning framework\n(Chen et al., 2020). Speciﬁcally, for each original\ninput sequence we create two corrupted versions,\neach by independently picking some random posi-\ntions to mask out and ﬁlling the masked positions\nwith samples from the generator, just like how we\ndo in token-level discrimination as described above.\nThe two corruptions of a same original sequence x,\ndenoted as xR\ni and xR\nj , are taken as a positive pair,\nand corruptions of other sequences within the same\nminibatch as x are regarded as negative examples,\nthe set of which is denoted as N(x). The CSP task\nis then to identify xR\nj in N(x) for a given xR\ni , and\nthe contrastive loss is accordingly deﬁned as:\nLCSP(x,xR\ni ,xR\nj ;D)= −log exp\n(\ns(xR\ni ,xR\nj )/τ\n)\n∑\nxR\nk ∈N(x) exp\n(\ns(xR\ni ,xR\nk )/τ\n),\n(7)\nwhere s(·,·) is the similarity measure between two\nsequences and τ is a temperature hyperparameter.\nWe represent each sequence by the ℓ2-normalized\nrepresentation of its [CLS] token, i.e., µD(·) =\nhD(·)1/∥hD(·)1∥where hD(·)1 stands for the rep-\nresentation of the ﬁrst token in a sequence output by\nthe discriminator D, and determine the similarity as\ns(u,v) =µD(u)TµD(v). This contrastive learn-\ning task requires xR\ni and xR\nj to stay close to each\nother while away from other corrupted sequences\nin the same minibatch, and therefore encourages\nthe discriminator to learn representations invariant\nto token-level alterations. A similar task has been\nconsidered recently by Meng et al. (2021) to help\nbuild general-domain PLMs, but it uses a different\ndata transformation procedure to generate positive\npairs by random cropping, resulting in asymmetric\nencoding of sequence pairs.\n3.3 Model Training\nPutting the generator and discriminator as well as\ntheir associated tasks together, we train eHealth by\nminimizing the following combined loss:\nmin\nG,D\nLMLM+λ1LRTD+λ2LMTS+λ3LCSP. (8)\nThe ﬁrst term is a generator loss, and the latter three\nare discriminator losses which are not propagated\nthrough the generator. λ1,λ2,λ3 are hyperparame-\nters balancing these loss terms. After pre-training,\nwe throw out the generator and ﬁne-tune only the\ndiscriminator on downstream tasks.\n4 Experiments\nThis section ﬁrst describes our experimental setups\nfor pre-training and ﬁne-tuning, and then presents\nevaluation results and further ablation.\n4.1 Pre-training Setups\nPre-training Data. We use four Chinese datasets\nfor pre-training: (i) Dialogues consisting of about\n100 million de-identiﬁed doctor-patient dialogues\nfrom online healthcare services; (ii) Articles con-\nsisting of about 6.5 million popular scientiﬁc arti-\ncles on medicine and healthcare oriented to the gen-\neral public; (iii) EMRs consisting of about 6.5 mil-\nlion de-identiﬁed electronic medical records from\nspeciﬁc hospitals; and (iv) Textbooks consisting of\nabout 1,500 electronic textbooks on medicine and\nclinical pathology. The contents of these datasets\nare quite diversiﬁed, covering most aspects of bio-\nmedicine, namely scientiﬁc, clinical, and consumer\nhealth (Jin et al., 2021). After collecting raw text,\nwe conduct minimum pre-processing of deduplica-\ntion and denoising on each of the four datasets. We\nthen tokenize the text using a newly built in-domain\nvocabulary (detailed later). Sequences longer than\n512 tokens are segmented into shorter chunks ac-\ncording to sentence boundaries, and those shorter\nthan 32 tokens are discarded. Table 1 summarizes\nthe datasets used for pre-training.\nIn-domain Vocabulary. Unlike previous studies\nthat continually pre-train from and thereby use the\nvocabulary of a general-domain Chinese BERT, we\ntrain eHealth from scratch with its own in-domain\nvocabulary built speciﬁcally for Chinese biomedi-\ncal text. Gu et al. (2020) have shown that training\nfrom scratch with an in-domain vocabulary is a bet-\nter choice than continue pre-training while build-\ning English biomedical PLMs, primarily because\nthe in-domain vocabulary can better handle highly\nspecialized biomedical terms. This, however, has\nnever been investigated in the Chinese biomedical\nﬁeld. To build the in-domain vocabulary, we ran-\ndomly sample 1M documents from the pre-training\ndata, convert all characters to lowercase, normalize\nspecial Unicodes like half-width characters or en-\nclosed alphanumerics, and split Chinese characters,\ndigits, and emoji Unicodes. Then we use the open-\nsource implementation from the Tensor2Tensor li-\nCorpus Size # Tokens Sub-domain\nDialogues 94.6GB 31.1B consumer health\nArticles 11.2GB 3.5B consumer health\nEMRs 16.0GB 4.5B clinical\nTextbooks 5.1GB 1.6B scientiﬁc\nTotal 126.9GB 40.7B N/A\nTable 1: Corpora used for eHealth pre-training.\n免疫组化IHC测定TSHR阳性 (Positive expression of TSHR\nby immunohistochemistry (IHC))\nBERT: 免, 疫, 组, 化, i, ##hc, 测, 定, ts, ##hr, 阳, 性\neHealth: 免, 疫, 组, 化, ihc, 测, 定, tshr, 阳, 性\nECOG评分4分者(Those with ECOG score of 4)\nBERT: eco, ##g, 评, 分, 4, 分, 者\neHealth: ecog, 评, 分, 4, 分, 者\n但不包括HIV/AIDS (But excluding HIV/AIDS)\nBERT: 但, 不, 包, 括, hiv, /, ai, ##ds\neHealth: 但, 不, 包, 括, hiv, /, aids\n胸部增强CT及头颅MRI (Enhanced chest CT & skull MRI)\nBERT: 胸, 部, 增, 强, ct, 及, 头, 颅, mr, ##i\neHealth: 胸, 部, 增, 强, ct, 及, 头, 颅, mri\nTable 2: Comparison of tokenization results obtained\nby BERT and eHealth. Differences highlighted in bold.\nbrary4 to create a WordPiece vocabulary (Wu et al.,\n2016). We throw out tokens appearing less than 5\ntimes and keep the vocabulary of size to about 20K\ntokens, which is similar to the general-domain Chi-\nnese BERT. Table 2 compares tokenization results\nobtained by (i) the original vocabulary of standard\nBERT and (ii) our newly built in-domain vocabu-\nlary. We can see that as both the two vocabularies\nare mainly based on single Chinese characters, the\ndifferences between them are not that signiﬁcant\nas in English. But still the in-domain vocabulary\nworks pretty better on abbreviations of specialized\nbiomedical terms, including not only those rare\nones like IHC (immunohistochemistry) and TSHR\n(thyroid stimulating hormone receptor), but also\nthose relatively popular ones like AIDS (acquired\nimmune deﬁciency syndrome) and MRI (magnetic\nresonance imaging).\nPre-training Conﬁgurations. We train eHealth\nwith the standard base-size conﬁguration, just like\nmost previous biomedical PLMs. The discrimina-\ntor gets 12 Transformer layers, each with 12 atten-\ntion heads, 768 hidden size, and 3072 intermediate\nsize. And we follow Clark et al. (2020) to set the\ngenerator 1/3 the size of the discriminator and tie\n4https://github.com/tensorflow/\ntensor2tensor\nDataset Task Train Dev Test Metric\nCMeEE Named Entity Recognition 15,000 5,000 3,000 Micro-F1\nCMeIE Relation Extraction 14,339 3,585 4,482 Micro-F1\nCHIP-CDN Clinical Term Normalization 6,000 2,000 10,192 Micro-F1\nCHIP-CTC Sentence Classiﬁcation 22,962 7,682 10,000 Macro-F1\nKUAKE-QIC Sentence Classiﬁcation 6,931 1,955 1,994 Accuracy\nCHIP-STS Sentence Pair Matching 16,000 4,000 10,000 Macro-F1\nKUAKE-QTR Sentence Pair Matching 24,174 2,913 5,465 Accuracy\nKUAKE-QQR Sentence Pair Matching 15,000 1,600 1,596 Accuracy\ncMedQNLI (Zhang et al., 2020) Question Answer Matching 80,950 9,065 9,969 Micro-F1\nwebMedQA (He et al., 2019) Question Answer Matching 252,850 31,605 31,655 Precision@1\nNLPEC (Li et al., 2020) Multiple Choice 18,117 2,500 550 Accuracy\nTable 3: Downstream tasks used for evaluation. Tasks in the ﬁrst group are from CBLUE (Zhang et al., 2021a).\ntheir token and positional embeddings. To generate\nmasked positions, we perform Chinese word seg-\nmentation and use the whole word masking strategy\n(Cui et al., 2020). We also use dynamic masking\nwith masked positions decided on-the-ﬂy. During\npre-training, we mostly follow the hyperparameters\nrecommended by ELECTRA and do not conduct\nhyperparameter tuning. For newly introduced hy-\nperparameters, we set the loss balancing terms λ1\n= 50, λ2 = 20, λ3 = 1(cf. Eq. (8)), the number\nof sampled non-original tokens k= 5(cf. Eq. (5)),\nand temperature τ = 0.07 (cf. Eq. (7)). We train\nwith a batch size of 384 and max sequence length\nof 512 for 1.65M steps. The full set of pre-training\nhyperparameters is listed in Appendix A.\n4.2 Evaluation Setups\nDownstream Tasks. We evaluate on the Chinese\nBiomedical Language Understanding Evaluation\n(CBLUE) benchmark (Zhang et al., 2021a), which\nis composed of 8 diversiﬁed biomedical NLP tasks,\nranging from medical text classiﬁcation and match-\ning to medical information extraction and medical\nterm normalization. We further consider three med-\nical question answering tasks, namely cMedQNLI\n(Zhang et al., 2020), webMedQA (He et al., 2019),\nand NLPEC (Li et al., 2020). The former two are\nformalized as question-answer matching problems,\nand the last one a multiple choice problem. Table 3\nsummarizes the train, dev, test split and metric used\nfor each task. We refer readers to Appendix C and\nD for further details.\nBaseline Models. We compare eHealth against\nstate-of-the-art general-domain Chinese PLMs of:\n(i) BERT-base (Devlin et al., 2019); (ii) ELECTRA-\nbase/large (Clark et al., 2020); (iii)RoBERTa-wwm\n-ext-base/large (Liu et al., 2019) trained via MLM\nwith whole word masking strategy; (iv) MacBERT-\nbase/large (Cui et al., 2020) trained via improved\nMLM as a correction task. BERT-base is ofﬁcially\nreleased by Google,5 and the other models are re-\nleased by Cui et al. (2020).6 Besides, we compare\nto Chinese biomedical PLMs including: (v) PCL-\nMedBERT;7 (vi) MC-BERT (Zhang et al., 2020);8\n(vii) EMBERT (Cai et al., 2021); and (viii) SMed-\nBERT (Zhang et al., 2021b), all initialized from\nGoogle’s BERT-base. The full models of EMBERT\nand SMedBERT are not released to the public, so\nwe just copy the results reported by their authors\non medical question answering tasks.\nFine-tuning Conﬁgurations. During ﬁne-tuning,\nwe build a lightweight task-speciﬁc head on top of\nthe pre-trained encoders for each task. The speciﬁc\ndesign of these heads is elaborated in Appendix E.\nFor each PLM on each task, we tune the batch size,\nlearning rate, and training epochs in their respective\nranges, and determine the optimal setting according\nto dev performance averaged over three runs with\ndifferent seeds. The other hyperparameters are set\nto their default values as in ELECTRA (Clark et al.,\n2020). The full set of ﬁne-tuning hyperparameters\nis listed in Appendix B.\n4.3 Main Results\nTable 4 reports the performance of different PLMs\non CBLUE test sets. Note that CBLUE test labels\nare not released, and one has to submit prediction\nﬁles to retrieve ﬁnal scores. To avoid frequent sub-\nmissions that probe the unseen test labels, we only\nsubmit best single run on dev sets for testing. The\n5https://github.com/google-research/\nbert\n6https://github.com/ymcui/MacBERT\n7https://code.ihub.org.cn/projects/\n1775\n8https://github.com/alibaba-research/\nChineseBLUE\nModel CMeEE CMeIE CDN CTC STS QIC QTR QQR Avg.\nGeneral-domain base-sized models\nBERT-base 66.5 60.6 69.7 68.6 84.7 85.2 59.2 82.5 72.1\nELECTRA-base 65.1 60.4 69.9 67.7 84.4 85.2 61.8 84.0 72.3\nMacBERT-base 66.8 61.5 69.7 69.1 84.4 86.0 61.0 83.5 72.7\nRoBERTa-wwm-ext-base 66.7 61.4 69.3 68.3 84.2 86.0 60.9 82.7 72.4\nGeneral-domain large-sized models\nELECTRA-large 66.1 59.3 70.8 68.9 85.1 84.1 62.0 85.7 72.8\nMacBERT-large 67.6 62.2 70.9 69.7 86.5 85.7 62.5 83.5 73.6\nRoBERTa-wwm-ext-large 67.3 62.2 70.6 70.6 85.4 86.7 61.7 86.1 73.8\nBiomedical base-sized models\nMC-BERT-base 66.6 60.7 70.1 69.1 85.4 85.3 61.6 82.3 72.6\nPCL-MedBERT-base 66.6 60.8 69.9 70.4 84.8 85.3 60.2 83.3 72.7\neHealth-base (ours) 66.9 62.1 71.9 69.3 86.2 87.3 63.9 85.7 74.2\nTable 4: Performance (%) of different PLMs on CBLUE test sets. Results generated by the single best run on dev\nsets. Best scores from base-sized models highlighted in bold, and best scores from large-sized models underlined.\ncMedQNLI webMedQA NLPEC\nModel dev | test dev | test dev | test\nGeneral-domain base-sized models\nBERT-base 96.4 | 96.4 79.6 | 79.8 67.1 | 54.6\nELECTRA-base 96.0 | 95.9 79.2 | 79.1 69.8 | 54.1\nMacBERT-base 96.3 | 96.2 79.9 | 79.8 68.7 | 53.8\nRoBERTa-base 96.2 | 96.2 79.7 | 79.9 68.1 | 54.3\nGeneral-domain large-sized models\nELECTRA-large 96.4 | 96.2 80.0 | 80.1 71.8 | 60.0\nMacBERT-large 96.3 | 96.3 80.0 | 80.4 70.8 | 56.7\nRoBERTa-large 96.3 | 96.2 79.7 | 79.7 71.1 | 56.5\nBiomedical base-sized models\nMC-BERT-base 96.4 | 96.5 80.0 | 79.9 68.2 | 54.2\nPCL-MedBERT-base 96.3 | 96.2 79.2 | 79.5 67.4 | 52.0\nEMBERT† – | 96.6 – | 80.6 – | –\nSMedBERT‡ 96.6 | 96.9 79.3 | 81.7 – | –\neHealth-base (ours) 97.3 | 97.2 80.5 | 80.7 73.6 | 62.4\nTable 5: Performance (%) of different PLMs on medi-\ncal QA tasks. RoBERTa-base/large refers to RoBERTa-\nwwm-ext-base/large. Results marked by †and ‡copied\nfrom original literatures (Cai et al., 2021; Zhang et al.,\n2021b). Other results produced by ourselves, averaged\nover best three runs on the dev set of each task. Best\nscores from base-sized models highlighted in bold and\nbest scores from large-sized models underlined.\nresults show that: (i) The two previous biomedical\nPLMs, MC-BERT and PCL-MedBERT, indeed per-\nform better than general-domain BERT-base from\nwhich they started continual pre-training, verifying\nthe effectiveness of domain adaptation in building\ndomain-speciﬁc language models. However, these\ntwo biomedical PLMs fail to surpass some more\nadvanced general-domain PLMs, e.g., MacBERT,\nof the same model size. (ii) As the model size in-\ncreases, general-domain large-sized PLMs perform\nbetter than those base-sized, e.g., ELECTRA-large,\nMacBERT-large, and RoBERTa-wwm-ext-large ob-\ntain averaged improvements of 0.5%, 0.9%, and\n1.4% respectively over their base-sized models. (iii)\neHealth, as a base-sized biomedical PLM, outper-\nforms all baseline PLMs in terms of average score,\nno matter those from the general or biomedical do-\nmain, and no matter those base-sized or large-sized.\nIt achieves an average improvement of 1.5% over\nPCL-MedBERT-base,i.e., the best performing di-\nrect opponent of the same model size, and even that\nof 0.4% over the best performing large-sized model\nRoBERTa-wwm-ext-large. These results demon-\nstrate the effectiveness and superiority of eHealth\nin biomedical text understanding.\nTable 5 further reports the performance of these\nPLMs on medical question answering tasks, where\nscores are averaged over the best three runs selected\non the dev split for each task. From the results we\ncan observe similar phenomena as on the CBLUE\nbenchmark. Still eHealth consistently outperforms\nalmost all those PLMs, showing its superior ability\nin medical question answering.\n4.4 Ablation Studies\nWe provide ablation studies on CBLUE benchmark\nto show the effects of different pre-training tasks\nand initialization strategies in eHealth. All variants\nbelow are base-sized, trained with the same setting\nas described in Section 4.1. The only exception is\nthat we train with a smaller batch size of 128 for\nonly 500K steps.\nEffects of Pre-training Tasks. The discriminator\nof eHealth is trained in a multi-task fashion, i.e., (i)\ntoken-level discrimination of RTD and MTS and\n(ii) sequence-level discrimination of CSP. To inves-\ntigate the effects of different pre-training tasks, we\nModel CMeEE CMeIE CDN CTC STS QIC QTR QQR Avg.\nThe full setting 66.56 61.62 70.29 69.58 85.13 87.46 62.00 85.53 73.52\nw/o CSP 66.47 61.25 69.81 69.65 84.61 86.71 61.54 84.52 73.07\nw/o MTS 65.76 60.23 70.43 68.06 85.44 85.61 61.36 84.34 72.65\nw/o CSP & MTS 65.56 60.01 70.08 68.46 84.35 86.51 61.08 84.40 72.56\nR weights + B vocab 66.56 61.62 70.29 69.58 85.13 87.46 62.00 85.53 73.52\nE weights + E vocab 65.92 61.54 70.86 69.53 85.75 86.21 62.38 85.59 73.47\nR weights + E vocab 66.33 61.06 70.19 69.50 84.32 87.31 62.33 85.40 73.30\nTable 6: Effects of pre-training tasks (top) and initialization strategies (bottom) on CBLUE test sets, where results\nare generated by single best run on dev sets. All variants are base-sized, trained with batch size 128 for 500K steps.\nR/B/E in the bottom group stands for R(andom)/B(iomedical)/E(LECTRA), respectively. Within each group best\nscores are highlighted in bold, and second best scores underlined.\nmake comparison among: (i) the full setting where\nthe discriminator is trained via RTD, MTS, and\nCSP; (ii) w/o CSP where the sequence-level CSP is\nremoved; (iii) w/o MTS where the token-level MTS\nis removed; and (iv) w/o CSP & MTS where both\nCSP and MTS are removed and thus degenerates to\nstandard ELECTRA pre-training. Table 6 (top) lists\nthe results on CBLUE benchmark, from which we\ncan see that: (i) The full setting performs the best\namong the four variants, always reporting the best\nor second best scores on all the 8 diversiﬁed tasks.\nCompared to standard ELECTRA pre-training (w/o\nCSP & MTS), it achieves an average improvement\nof 0.96%. This demonstrates the usefulness of our\npre-training tasks, in particular CSP and MTS, to\nbuild effective PLMs. (ii) No matter CSP or MTS,\nwhen applied alone, is able to improve the standard\nELECTRA pre-training solely with RTD. Between\nthe two tasks, MTS is, in general, more powerful\nthan CSP. Removing MTS brings an average drop\nof 0.87% on CBLUE test sets, while removing CSP\nonly brings that of 0.45% on the same benchmark.\nEffects of Initialization Strategies. In this work\nwe train eHealth entirely from scratch, with an in-\ndomain vocabulary built speciﬁcally for Chinese\nbiomedical text and the model weights randomly\ninitialized. We refer to this strategy as “R(andom)\nweights + B(iomedical) vocab”. We compare it to\nthe widely adopted continue pre-training strategy,\nwhere model weights are initialized from a general-\ndomain ELECTRA and the associated vocabulary\nis also used, referred to as “E(LECTRA) weights +\nE(LECTRA) vocab”. Besides, to further verify the\neffects of that in-domain vocabulary, we consider\nanother setting “R(andom) weights + E(LECTRA)\nvocab”, where model weights are still randomly\ninitialized but the ELECTRA vocabulary is used.\nTable 6 (bottom) lists the results on CBLUE bench-\nmark, from which we can see that: (i) Pre-training\nfrom scratch with the newly built in-domain vocab-\nulary (R weights + B vocab) overall performs better\nthan continue pre-training (E weights + E vocab),\neven under a relatively small number of training\nsteps up to 500K.9 (ii) The improvements mainly\ncome from the in-domain vocabulary. After replac-\ning the vocabulary with that of the general-domain\nELECTRA (R weights + E vocab), the overall per-\nformance drops from 73.52% to 73.30%.\n5 Conclusion\nThis work presents eHealth, a Chinese biomedical\nlanguage model pre-trained from in-domain text of\nde-identiﬁed online doctor-patient dialogues, elec-\ntronic medical records, and textbooks. Unlike most\nprevious studies that directly adapt general-domain\nPLMs to the biomedical domain, eHealth is trained\nfrom scratch with a new self-supervised generator-\ndiscriminator framework. The generator is used to\nproduce corrupted input and is discarded after pre-\ntraining. The discriminator, as the ﬁnal encoder,\nis trained via multi-level discrimination: (i) token-\nlevel discrimination that detects input tokens cor-\nrupted by the generator and selects original tokens\nfrom plausible candidates; and (ii) sequence-level\ndiscrimination that further detects corruptions of\na same original sequence from those of the others.\nAs such, eHealth can learn language semantics at\nboth levels. Experimental results on CBLUE and 3\nmedical QA benchmarks demonstrate the effective-\nness and superiority of eHealth, which consistently\noutperforms state-of-the-art PLMs from both the\ngeneral and biomedical domains. We release our\npre-trained model to the public, which could be\napplied rather easily during ﬁne-tuning.\n9The advantage, in fact, will be expanded further as the\ntraining step increases according to our initial experiments.\nReferences\nSultan Alrowili and Vijay Shanker. 2021. BioM-\nTransformers: Building large biomedical language\nmodels with BERT, ALBERT and ELECTRA. In\nProceedings of the 20th Workshop on Biomedical\nLanguage Processing, pages 221–227.\nEmily Alsentzer, John Murphy, William Boag, Wei-\nHung Weng, Di Jindi, Tristan Naumann, and\nMatthew McDermott. 2019. Publicly available clini-\ncal BERT embeddings. In Proceedings of the 2nd\nClinical Natural Language Processing Workshop ,\npages 72–78.\nStéphane Aroca-Ouellette and Frank Rudzicz. 2020.\nOn losses for modern language models. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing, pages 4970–4981.\nGiannis Bekoulis, Johannes Deleu, Thomas Demeester,\nand Chris Develder. 2018. Joint entity recogni-\ntion and relation extraction as a multi-head selection\nproblem. Expert Systems with Applications, 114:34–\n45.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\nERT: A pretrained language model for scientiﬁc text.\nIn Proceedings of the 2019 Conference on Empiri-\ncal Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural\nLanguage Processing, pages 3615–3620.\nZerui Cai, Taolin Zhang, Chengyu Wang, and Xiaofeng\nHe. 2021. EMBERT: A pre-trained language model\nfor Chinese medical text mining. In Web and Big\nData, pages 242–257.\nIlias Chalkidis, Manos Fergadiotis, Prodromos Malaka-\nsiotis, Nikolaos Aletras, and Ion Androutsopoulos.\n2020. LEGAL-BERT: The muppets straight out of\nlaw school. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning: Findings, pages 2898–2904.\nTing Chen, Simon Kornblith, Mohammad Norouzi,\nand Geoffrey Hinton. 2020. A simple framework\nfor contrastive learning of visual representations. In\nProceedings of the 37-th International Conference\non Machine Learning, pages 1597–1607.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and\nChristopher D Manning. 2020. ELECTRA: Pre-\ntraining text encoders as discriminators rather than\ngenerators. In Proceedings of the Eighth Interna-\ntional Conference on Learning Representations.\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shi-\njin Wang, and Guoping Hu. 2020. Revisiting pre-\ntrained models for Chinese natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nFindings, pages 657–668.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional Transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 4171–4186.\nHongchao Fang, Sicheng Wang, Meng Zhou, Jiayuan\nDing, and Pengtao Xie. 2020. CERT: Contrastive\nself-supervised learning for language understanding.\narXiv preprint arXiv:2005.12766.\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas,\nNaoto Usuyama, Xiaodong Liu, Tristan Naumann,\nJianfeng Gao, and Hoifung Poon. 2020. Domain-\nspeciﬁc language model pretraining for biomedi-\ncal natural language processing. arXiv preprint\narXiv:2007.15779.\nTongfeng Guan, Hongying Zan, Xiabing Zhou,\nHongfei Xu, and Kunli Zhang. 2020. CMeIE: Con-\nstruction and evaluation of Chinese medical informa-\ntion extraction dataset. In CCF International Con-\nference on Natural Language Processing and Chi-\nnese Computing, pages 270–282.\nSuchin Gururangan, Ana Marasovic, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n8342–8360.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pa-\nsupat, and Ming-Wei Chang. 2020. REALM:\nRetrieval-augmented language model pre-training.\narXiv preprint arXiv:2002.08909.\nJunqing He, Mingming Fu, and Manshu Tu. 2019.\nApplying deep matching networks to Chinese med-\nical question answering: A study and a dataset.\nBMC Medical Informatics and Decision Making ,\n19(2):91–100.\nQiao Jin, Zheng Yuan, Guangzhi Xiong, Qianlan Yu,\nChuanqi Tan, Mosha Chen, Songfang Huang, Xi-\naozhong Liu, and Sheng Yu. 2021. Biomedical ques-\ntion answering: A comprehensive review. arXiv\npreprint arXiv:2102.05281.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld,\nLuke Zettlemoyer, and Omer Levy. 2020. Span-\nBERT: Improving pre-training by representing and\npredicting spans. Transactions of the Association\nfor Computational Linguistics, 8:64–77.\nKamal raj Kanakarajan, Bhuvana Kundumani, and\nMalaikannan Sankarasubbu. 2021. BioELECTRA:\nPretrained biomedical text encoder using discrimi-\nnators. In Proceedings of the 20th Workshop on\nBiomedical Language Processing, pages 143–154.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite BERT for self-supervised\nlearning of language representations. In Proceed-\nings of the Eighth International Conference on\nLearning Representations.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So,\nand Jaewoo Kang. 2020. BioBERT: A pre-\ntrained biomedical language representation model\nfor biomedical text mining. Bioinformatics,\n36(4):1234–1240.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020a. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880.\nPatrick Lewis, Myle Ott, Jingfei Du, and Veselin\nStoyanov. 2020b. Pretrained language models for\nbiomedical and clinical tasks: Understanding and\nextending the state-of-the-art. In Proceedings of\nthe 3rd Clinical Natural Language Processing Work-\nshop, pages 146–157.\nDongfang Li, Baotian Hu, Qingcai Chen, Weihua Peng,\nand Anqi Wang. 2020. Towards medical machine\nreading comprehension with structural knowledge\nand plain text. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 1427–1438.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. arXiv preprint arXiv:1907.11692.\nZhuang Liu, Degen Huang, Kaiyu Huang, Zhuang Li,\nand Jun Zhao. 2020. FinBERT: A pre-trained ﬁnan-\ncial language representation model for ﬁnancial text\nmining. In Proceedings of the Twenty-Ninth Inter-\nnational Joint Conference on Artiﬁcial Intelligence:\nSpecial Track on AI in FinTech, pages 4513–4519.\nYu Meng, Chenyan Xiong, Payal Bajaj, Saurabh Ti-\nwary, Paul Bennett, Jiawei Han, and Xia Song.\n2021. COCO-LM: Correcting and contrasting text\nsequences for language model pretraining. arXiv\npreprint arXiv:2102.08473.\nAlec Radford, Karthik Narasimhan, Time Salimans,\nand Ilya Sutskever. 2018. Improving language un-\nderstanding by generative pre-training. Technical re-\nport, OpenAI.\nLev Ratinov and Dan Roth. 2009. Design challenges\nand misconceptions in named entity recognition. In\nProceedings of the Thirteenth Conference on Com-\nputational Natural Language Learning , pages 147–\n155.\nJiaming Shen, Jialu Liu, Tianqi Liu, Cong Yu, and Ji-\nawei Han. 2021. Training ELECTRA augmented\nwith multi-word selection. In Findings of the Associ-\nation for Computational Linguistics: ACL-IJCNLP\n2021, pages 2475–2486.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao\nTian, Hua Wu, and Haifeng Wang. 2020. ERNIE\n2.0: A continual pre-training framework for lan-\nguage understanding. In Proceedings of the Thirty-\nFourth AAAI Conference on Artiﬁcial Intelligence ,\npages 8968–8975.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 5998–6008.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, et al. 2016. Google’s neural machine\ntranslation system: Bridging the gap between hu-\nman and machine translation. arXiv preprint\narXiv:1609.08144.\nZhenhui Xu, Linyuan Gong, Guolin Ke, Di He,\nShuxin Zheng, Liwei Wang, Jiang Bian, and Tie-\nYan Liu. 2020. MC-BERT: Efﬁcient language\npre-training via a meta controller. arXiv preprint\narXiv:2006.05744.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXLNet: Generalized autoregressive pretraining for\nlanguage understanding. Advances in Neural Infor-\nmation Processing Systems, 32.\nXiangrong Zeng, Daojian Zeng, Shizhu He, Kang Liu,\nand Jun Zhao. 2018. Extracting relational facts by\nan end-to-end neural model with copy mechanism.\nIn Proceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 506–\n514.\nNingyu Zhang, Mosha Chen, Zhen Bi, Xiaozhuan\nLiang, Lei Li, Xin Shang, Kangping Yin, Chuanqi\nTan, Jian Xu, Fei Huang, Luo Si, Yuan Ni, Guo-\ntong Xie, Zhifang Sui, Baobao Chang, Hui Zong,\nZheng Yuan, Linfeng Li, Jun Yan, Hongying Zan,\nKunli Zhang, Buzhou Tang, and Qingcai Chen.\n2021a. CBLUE: A Chinese biomedical language un-\nderstanding evaluation benchmark. arXiv preprint\narXiv:2106.08087.\nNingyu Zhang, Qianghuai Jia, Kangping Yin, Liang\nDong, Feng Gao, and Nengwei Hua. 2020. Concep-\ntualized representation learning for Chinese biomed-\nical text mining. arXiv preprint arXiv:2008.10813.\nTaolin Zhang, Zerui Cai, Chengyu Wang, Minghui Qiu,\nBite Yang, and Xiaofeng He. 2021b. SMedBERT:\nA knowledge-enhanced pre-trained language model\nwith structured semantics for medical text mining.\nIn Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing, pages 5882–5893.\nHui Zong, Jinxuan Yang, Zeyu Zhang, Zuofeng Li, and\nXiaoyan Zhang. 2021. Semantic categorization of\nChinese eligibility criteria in clinical trials using ma-\nchine learning methods. BMC Medical Informatics\nand Decision Making, 21(1):1–12.\nA Pre-training Hyperparameters\nWe mostly use the same hyperparameters as ELEC-\nTRA (Clark et al., 2020) and do not conduct hyper-\nparameter tuning during pre-training. As for those\nnewly introduced hyperparameters, we sample k=\n5 non-original tokens for a certain position in the\nMTS task, use a temperature τ = 0.07 in the CSP\ntask, and set the loss balancing tradeoffs λ1 = 50,\nλ2 = 20, λ3 = 1. The full pre-training setting is\nlisted in Table 7.\nB Fine-tuning Hyperparameters\nDuring ﬁne-tuning, we mostly use the default set-\nting as suggested by BERT (Devlin et al., 2019) and\nELECTRA (Clark et al., 2020), listed in Table 8.\nWe also use exponential moving average (EMA)\nwith a decay coefﬁcient α of 0.9999. Then for\neach task we specify a proper maximum sequence\nlength, tune for each PLM the batch size, learning\nrate, and training epochs in their respective ranges,\nand determine optimal conﬁgurations according to\ndev performance. The full tuning ranges are listed\nin Table 9.\nC CBLUE Benchmark\nCBLUE (Zhang et al., 2021a) 10 is a benchmark\nfor Chinese biomedical language understanding\nevaluation, consisting of 8 diversiﬁed biomedical\nNLP tasks as follows.\nCMeEE: Chinese Medical Entity Extraction. 11\nThe task is to identify medical entities from a given\nsentence and classify the entities into nine cate-\ngories including disease, symptom, drug, etc. The\ndataset contains 15K/5K/3K train/dev/test exam-\nples from textbooks of clinical pediatrics.\n10https://github.com/CBLUEbenchmark/\nCBLUE\n11http://www.cips-chip.org.cn/2020/\neval1\nHyperparameter Value\nNumber of Layers 12\nHidden size 768\nIntermediate size 3072\nNumber of attention heads 12\nAttention head size 64\nEmbedding size 768\nGenerator size (multiplier for hidden size, 1/3intermediate size, number of attention heads)\nMask percentage 15\nLearning rate decay Linear\nWarmup steps 10000\nLearning rate 2e-4\nAdam ϵ 1e-6\nAdam β1 0.9\nAdam β2 0.999\nAttention dropout 0.1\nDropout 0.1\nWeight decay 0.01\nMax sequence length 512\nBatch size 384\nTraining steps 1.65M\nLoss tradeoff λ1 50\nLoss tradeoff λ2 20\nLoss tradeoff λ3 1\nMulti-token selection k 5\nContrastive sequence prediction τ 0.07\nTable 7: Pre-training hyperparameters.\nHyperparameter Value\nLearning rate decay Linear\nWarmup ratio 0.1\nAdam ϵ 1e-8\nAdam β1 0.9\nAdam β2 0.999\nAttention dropout 0.1\nDropout 0.1\nWeight decay 0.01\nEMA decay 0.9999\nTable 8: Default ﬁne-tuning hyperparameters.\nCMeIE: Chinese Medical Information Extraction\n(Guan et al., 2020).12 The task is to recognize both\nmedical entities and their relationships from a given\nsentence according to a predeﬁned schema. There\nare 44 relations deﬁned in the schema, along with\ntheir subject/object entity types. The dataset con-\ntains about 14K/3.5K/4.5K train/dev/test examples,\nwhich are also from textbooks of clinical pediatrics.\nCHIP-CDN: CHIP Clinical Diagnosis Normaliza-\ntion.13 The task is to normalize original diagnostic\nterms into standard terminologies from the Interna-\ntional Classiﬁcation of Diseases (ICD-10), Beijing\nClinical Edition v601. The dataset contains about\n12http://www.cips-chip.org.cn/2020/\neval2\n13http://www.cips-chip.org.cn/2020/\neval3\nTask Batch size Learning rate Epochs Length\nCBLUE benchmark\nCMeEE 32 6e-5, 1e-4 2, 4, 8, 12 128\nCMeIE 12 6e-5 50, 100, 150, 200, 250 300\nCHIP-CDN 256 3e-5, 6e-5, 1e-4 2, 4, 8, 12, 16 32\nCHIP-CTC 8, 16, 32 3e-5, 6e-5, 1e-4 2, 4, 8, 12, 16 160\nCHIP-STS 8, 16, 32 3e-5, 6e-5, 1e-4 2, 4, 8, 12, 16 96\nKUAKE-QIC 8, 16, 32 3e-5, 6e-5, 1e-4 2, 4, 8, 12, 16 128\nKUAKE-QTR 8, 16, 32 3e-5, 6e-5, 1e-4 2, 4, 8, 12, 16 64\nKUAKE-QQR 8, 16, 32 3e-5, 6e-5, 1e-4 2, 4, 8, 12, 16 64\nMedical QA tasks\ncMedQNLI 8, 16, 32 3e-5, 6e-5, 1e-4 1, 2, 3, 4 512\nwebMedQA 16, 32, 64 1e-5, 2e-5, 3e-5 1, 2, 3, 4 512\nNLPEC 32 2e-5, 3e-5, 6e-5 10, 20, 30, 40 512\nTable 9: Hyperparameter tuning ranges on CBLUE and medical QA benchmarks.\n6K/2K/10K train/dev/test examples collected from\nde-identiﬁed electronic medical records.\nCHIP-CTC: CHIP Clinical Trial Classiﬁcation\n(Zong et al., 2021).14 The task is to categorize eli-\ngibility criteria of clinical trials into 44 predeﬁned\nsemantic classes including age, disease, symptom,\netc. The dataset consists of about 23K/7.5K/10K\ntrain/dev/test examples collected from the website\nof Chinese Clinical Trial Registry.\nCHIP-STS: CHIP Semantic Textual Similarity.15\nThe task is to identify whether the semantics of two\nmedical questions are identical or not. The dataset\ncontains 16K/4K/10K train/dev/test question pairs\ncollected from online healthcare services, covering\n5 diseases including diabetes, hypertension, hepati-\ntis, aids, and breast cancer.\nKUAKE-QIC: KUAKE Query Intent Classiﬁca-\ntion. The task is to classify the intent of a medical\nsearch query into one of 11 predeﬁned categories\nlike diagnosis, etiology analysis, medical advice,\netc. The dataset contains about 7K/2K/2K queries\nin the train/dev/test split, collected from Alibaba\nQUAKE search engine.\nKUAKE-QTR: KUAKE Query Title Relevance.\nThe task aims to estimate the relevance between a\nsearch query and a webpage title. The relevance is\ndivided into four levels: perfectly match, partially\nmatch, slightly match, and mismatch. The dataset\ncontains about 24K/3K/5.5K query-title pairs in the\ntrain/dev/test split, collected from Alibaba QUAKE\nsearch engine.\n14https://github.com/zonghui0228/\nchip2019task3\n15http://www.cips-chip.org.cn:8000/\nevaluation\nKUAKE-QQR:KUAKE Query Query Relevance.\nSimilar to KUAKE-QTR, the task is to estimate the\nrelevance between two search queries Q1 and Q2.\nThe relevance is divided into three levels: perfectly\nmatch, Q2 is a subset of Q1, Q2 is a superset of Q1\nor mismatch. The dataset contains approximately\n15K/1.6K/1.6K pairs of queries in the train/dev/test\nsplit. The queries are also collected from Alibaba\nQUAKE search engine.\nD Medical QA Tasks\nBesides CBLUE, we consider three medical ques-\ntion answering (QA) tasks, detailed as follows.\ncMedQNLI: This is a Chinese medical QA dataset\nwhich formalizes QA as a question answer match-\ning problem (Zhang et al., 2020).16 Given a ques-\ntion answer pair, the task is to identify whether the\nanswer addresses the question or not. The dataset\ncontains about 81K/9K/10K question answer pairs\nin the train/dev/test split.\nwebMedQA: This dataset also formalizes medical\nQA as a question answer matching problem (He\net al., 2019),17 just like cMedQNLI. But it is much\nlarger, containing roughly 250K/31.5K/31.5K ques-\ntion answer pairs in the train/dev/test split.\nNLPEC: This is a multiple choice QA dataset con-\nstructed using simulated and real questions from\nthe National Licensed Pharmacist Examination in\nChina (Li et al., 2020).18 Given a question along\nwith ﬁve answer candidates, the task is to select the\nmost plausible answer from the candidates using\n16https://github.com/alibaba-research/\nChineseBLUE\n17https://github.com/hejunqing/webMedQA\n18http://112.74.48.115:8157\n呼息肌麻痹和呼息中枢受累患者因呼息不畅可并发肺炎、肺不张等。\nbod bod dis dis\nsym sym sym\n⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ \nB-bodI-bodE-bodS-bodB-disI-disE-disS-dis\n⋮ \nO\nB-symI-symE-symS-symO\nClassifier-I\nClassifier-II\nPre-trained Transformer Encoder\nFigure 2: A running example illustrating the sequence\ntagging head used for CMeEE. Dark shaded entries rep-\nresent a ground truth label of 1, and light shaded entries\na ground truth label of 0.\ntextual evidences extracted from the ofﬁcial exam\nguide. The dataset contains about 18K/2.5K/0.5K\nquestions in the train/dev/test split.\nE Task-speciﬁc Heads\nWe devise lightweight task-speciﬁc heads on top\nof pre-trained Transformer encoders to solve down-\nstream tasks in various forms. These task-speciﬁc\nheads are roughly categorized into ﬁve groups, used\nfor named entity recognition, relation extraction,\nsingle sentence classiﬁcation, sentence pair classi-\nﬁcation, and multiple choice QA, respectively.\nNamed Entity Recognition. CMeEE is the only\ntask of this kind. It recognizes medical entities and\nclassiﬁes them into 9 predeﬁned types. Nesting is\nallowed only in symptom entities, but not in entities\nof the other types. We therefore use a two-stream\nsequence tagging head for this task, one to identify\nsymptom entities and the other to identify entities\nof the other 8 types. We choose the BIOES (i.e., Be-\ngin, Inside, Outside, End, Single) tagging scheme\n(Ratinov and Roth, 2009). Given a sequence with\nits contextualized representations output by a pre-\ntrained encoder, we build two classiﬁers on top of\nthese representations. The ﬁrst assigns each token\nin the sequence into 5 classes to annotate symptom\nentities (4 type-speciﬁc B-, I-, E-, S- tags plus\nO tag), while the second assigns it into 33 classes\nto annotate entities of other types (32 type-speciﬁc\nB-, I-, E-, S- tags plus O tag). The two classiﬁers\nare trained jointly with a 1:1 balanced combined\nloss. Figure 2 gives a running example illustrating\nthis two-stream sequence tagging head.\nRelation Extraction. CMeIE is the only task of\nthis kind. It extracts subject-relation-object triples\naccording to a predeﬁned schema. There are totally\n44 relations deﬁned in the schema and overlapping\n病理分型\n仅有不到1 0 % 的儿童N H L 为缺乏表面免疫球蛋白的早前B 细胞肿\n病因\n瘤。\nPre-trained Transformer Encoder\n0 8 14 26… … … …\n0\n8\n14\n26\n⋮ \n⋮ \n⋮ \n⋮ \n31\n31\n0 8 14 26… … … …\n0\n8\n14\n26\n⋮ \n⋮ \n⋮ \n⋮ \n31\n31\n0 8 14 26… … … …\n0\n8\n14\n26\n⋮ \n⋮ \n⋮ \n⋮ \n31\n31\n0 8 14 26… … … …\n0\n8\n14\n26\n⋮ \n⋮ \n⋮ \n⋮ \n31\n31\n… … …\nR-0: 预防 R-22: 病理分型 R-30: 病因 R-43: 同义词\nEntity Pointer\nMHS-based\nRelation Extraction\nFigure 3: A running example illustrating the multi-head\nselection layer used for joint entity and relation extrac-\ntion in CMeIE. Dark shaded entries represent a ground\ntruth label of 1, and light shaded entries a ground truth\nlabel of 0.\nis allowed between these relations, i.e., one entity\nmay belong to multiple triples of different relations\n(Zeng et al., 2018). To solve this overlapping prob-\nlem, we use a multi-head selection (MHS) layer for\njoint entity and relation extraction (Bekoulis et al.,\n2018). As illustrated in Figure 3, an entity pointer\nis adopted to identify start and end of entity spans,\nand then an MHS mechanism is further employed\nto recognize possible relationships between pairs of\nentity spans. The MHS module predicts if there ex-\nists a relation kbetween a subject entity starting at\nposition iand an object entity starting at position j\nfor every i, j, and k. This prediction probability is\ncalculated via a relation-speciﬁc biafﬁne operation\nimposed upon the starting token representations of\nsubject and object entities. Finally, we jointly train\nthe entity pointer and MHS-based relation extractor\nvia a combined loss with balancing ratio of 1:50.\nSingle Sentence Classiﬁcation. CHIP-CTC and\nKUAKE-QIC are tasks of this kind, which classi-\nﬁes a given sentence into one of a set of predeﬁned\ncategories. We simply build a softmax classiﬁer on\ntop of the ﬁnal representation corresponding to the\ninitial [CLS] token for this classiﬁcation task.\nSentence Pair Classiﬁcation. The sentence pair\nmatching tasks of CHIP-STS, KUAKE-QTR, and\nKUAKE-QQR, as well as the medical QA tasks of\ncMedQNLI and webMedQA are of this kind, aim-\ning at predicting the semantic relationship between\na pair of sentences according to a set of predeﬁned\nlabels. CHIP-CDN, after normalized terms have\nbeen retrieved for each original term, can also be\nformalized as a task of this kind, the aim of which is\nto judge if a normalized term matches the original\nterm or not. Given a pair of sentences (S1,S2), we\npack them into a single input sequence “[CLS]S1\n[SEP]S2 [SEP]”, and feed this sequence into a\npre-trained encoder. Then we build a softmax clas-\nsiﬁer on top of [CLS] representation to conduct\nsentence pair classiﬁcation. For CHIP-CDN, we\nretrieve 100 candidate normalized terms for each\noriginal term from the whole ICD-10 vocabulary\nusing Elasticsearch before pairwise classiﬁcation.\nMultiple Choice QA. NLPEC is the only task of\nthis kind. It selects the most plausible answer from\n5 answer candidates for a given question. Textual\nevidences are also provided along with the question.\nLet Qdenote the question, {A1,A2,A3,A4,A5}\nthe answer candidates, and T the textual evidence.\nFor each answer candidate Ai, we pack it with the\nquestion Qand textual evidence T, and construct a\nsingle input sequence “[CLS]Ai[SEP]Q[SEP]\nT[SEP]”. We feed this sequence into a pre-trained\nencoder, and use [CLS] representation to estimate\nif Ai answers Qgiven textual evidence T. In this\nfashion, we transform multiple choice into binary\nclassiﬁcation. At inference time, the candidate with\nhighest probability is chosen as the correct answer.",
  "topic": "eHealth",
  "concepts": [
    {
      "name": "eHealth",
      "score": 0.7911653518676758
    },
    {
      "name": "Computer science",
      "score": 0.7839211225509644
    },
    {
      "name": "Security token",
      "score": 0.7539805769920349
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.6171188354492188
    },
    {
      "name": "Language model",
      "score": 0.6042479276657104
    },
    {
      "name": "Discriminator",
      "score": 0.5924736261367798
    },
    {
      "name": "Natural language processing",
      "score": 0.5228667259216309
    },
    {
      "name": "Sequence (biology)",
      "score": 0.5083174109458923
    },
    {
      "name": "Artificial intelligence",
      "score": 0.49915289878845215
    },
    {
      "name": "Generator (circuit theory)",
      "score": 0.48990267515182495
    },
    {
      "name": "Field (mathematics)",
      "score": 0.43379923701286316
    },
    {
      "name": "Code (set theory)",
      "score": 0.43103379011154175
    },
    {
      "name": "Tree (set theory)",
      "score": 0.4124210774898529
    },
    {
      "name": "Programming language",
      "score": 0.15112090110778809
    },
    {
      "name": "Health care",
      "score": 0.09231671690940857
    },
    {
      "name": "Power (physics)",
      "score": 0.0
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Detector",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Pure mathematics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Economic growth",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I98301712",
      "name": "Baidu (China)",
      "country": "CN"
    }
  ],
  "cited_by": 9
}