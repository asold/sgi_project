{
  "title": "DeFormer: Decomposing Pre-trained Transformers for Faster Question Answering",
  "url": "https://openalex.org/W3035408713",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2129761198",
      "name": "Qingqing Cao",
      "affiliations": [
        "Stony Brook University"
      ]
    },
    {
      "id": "https://openalex.org/A2165999522",
      "name": "Harsh Trivedi",
      "affiliations": [
        "Stony Brook University"
      ]
    },
    {
      "id": "https://openalex.org/A2237204116",
      "name": "Aruna Balasubramanian",
      "affiliations": [
        "Stony Brook University"
      ]
    },
    {
      "id": "https://openalex.org/A2116597581",
      "name": "Niranjan Balasubramanian",
      "affiliations": [
        "Stony Brook University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2953271402",
    "https://openalex.org/W4295253143",
    "https://openalex.org/W3015298864",
    "https://openalex.org/W2551396370",
    "https://openalex.org/W2963981420",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2949231165",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2971033911",
    "https://openalex.org/W2952902402",
    "https://openalex.org/W2974875810",
    "https://openalex.org/W2962988160",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2946659172",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W4249475284",
    "https://openalex.org/W3034457371",
    "https://openalex.org/W2970213198",
    "https://openalex.org/W2963225922",
    "https://openalex.org/W1529817821",
    "https://openalex.org/W4288347855",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2963689957",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W4295224301",
    "https://openalex.org/W2964299589",
    "https://openalex.org/W4297813615",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W2606964149",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W2967659330",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2924902521",
    "https://openalex.org/W2798858969",
    "https://openalex.org/W1902041153"
  ],
  "abstract": "Transformer-based QA models use input-wide self-attention – i.e. across both the question and the input passage – at all layers, causing them to be slow and memory-intensive. It turns out that we can get by without input-wide self-attention at all layers, especially in the lower layers. We introduce DeFormer, a decomposed transformer, which substitutes the full self-attention with question-wide and passage-wide self-attentions in the lower layers. This allows for question-independent processing of the input text representations, which in turn enables pre-computing passage representations reducing runtime compute drastically. Furthermore, because DeFormer is largely similar to the original model, we can initialize DeFormer with the pre-training weights of a standard transformer, and directly fine-tune on the target QA dataset. We show DeFormer versions of BERT and XLNet can be used to speed up QA by over 4.3x and with simple distillation-based losses they incur only a 1% drop in accuracy. We open source the code at https://github.com/StonyBrookNLP/deformer.",
  "full_text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4487–4497\nJuly 5 - 10, 2020.c⃝2020 Association for Computational Linguistics\n4487\nDeFormer: Decomposing Pre-trained Transformers\nfor Faster Question Answering\nQingqing Cao, Harsh Trivedi, Aruna Balasubramanian, Niranjan Balasubramanian\nDepartment of Computer Science\nStony Brook University\nStony Brook, NY 11794, USA\n{qicao,hjtrivedi,arunab,niranjan}@cs.stonybrook.edu\nAbstract\nTransformer-based QA models use input-wide\nself-attention – i.e. across both the question\nand the input passage – at all layers, causing\nthem to be slow and memory-intensive. It\nturns out that we can get by without input-\nwide self-attention at all layers, especially in\nthe lower layers. We introduce DeFormer,\na decomposed transformer, which substitutes\nthe full self-attention with question-wide and\npassage-wide self-attentions in the lower lay-\ners. This allows for question-independent pro-\ncessing of the input text representations, which\nin turn enables pre-computing passage rep-\nresentations reducing runtime compute dras-\ntically. Furthermore, because DeFormer is\nlargely similar to the original model, we\ncan initialize DeFormer with the pre-training\nweights of a standard transformer, and directly\nﬁne-tune on the target QA dataset. We show\nDeFormer versions of BERT and XLNet can\nbe used to speed up QA by over 4.3x and\nwith simple distillation-based losses they in-\ncur only a 1% drop in accuracy. We open\nsource the code at https://github.com/\nStonyBrookNLP/deformer.\n1 Introduction\nThere is an increasing need to push question an-\nswering (QA) models in large volume web scale\nservices (Google, 2019) and also to push them to re-\nsource constrained mobile devices for privacy and\nother performance reasons (Cao et al., 2019). State-\nof-the-art QA systems, like many other NLP appli-\ncations, are built using large pre-trained Transform-\ners (e.g., BERT (Devlin et al., 2019), XLNet (Yang\net al., 2019), Roberta (Liu et al., 2019)). However,\ninference in these models requires prohibitively\nhigh-levels of runtime compute and memory mak-\ning it expensive to support large volume deploy-\nments in data centers and infeasible to run on re-\nsource constrained mobile devices.\nOur goal is to take pre-trained Transformer-\nbased models and modify them to enable faster\nDecompose \nCLS My name SEP Y our ? CLS My name SEP Y our ? \nT ransformer DeFormer\nFigure 1: Original Transformer applies full self-\nattention to encode the concatenated question and pas-\nsage sequence, while DeFormer encodes the question\nand passage independently in the lower layers and pro-\ncesses them jointly in the higher layers.\ninference for QA without having to repeat the pre-\ntraining. This is a critical requirement if we want to\nexplore many points in the accuracy versus speed\ntrade-off because pre-training is expensive.\nThe main compute bottleneck in Transformer-\nbased models is the input-wide self-attention com-\nputation at each layer. In reading comprehension\nstyle QA, this amounts to computing self-attention\nover the question and the context text together. This\nhelps the models create highly effective question-\ndependent context representations and vice-versa.\nOf these, building representations of the context\ntakes more time because it is typically much longer\nthan the question. If the context can be processed\nindependent of the question, then this expensive\ncompute can be pushed ofﬂine saving signiﬁcant\nruntime latency.\nCan we process the context independent of the\nquestion, at least in some of the layers, without\ntoo much loss in effectiveness? There are two em-\npirical observations that indicate that this is possi-\nble. First, previous studies have demonstrated that\nlower layers tend to focus on local phenomena such\nas syntactic aspects, while the higher layers focus\non global (long distance) phenomena such as se-\nmantic aspects relevant for the target task (Tenney\n4488\net al., 2019; Hao et al., 2019; Clark et al., 2019b).\nSecond, as we show later (see Section 2), in a stan-\ndard BERT-based QA model, there is less variance\nin the lower layer representations of text when we\nvary the question. This means that in the lower\nlayers information from the question is not as crit-\nical to form text representations. Together, these\nsuggest that considering only local context in lower\nlayers of Transformer and considering full global\ncontext in upper layers can provide speedup at a\nvery small cost in terms of effectiveness.\nBased on these observations, we introduce De-\nFormer a simple decomposition of pre-trained\nTransformer-based models, where lower layers in\nthe decomposed model process the question and\ncontext text independently and the higher layers\nprocess them jointly (see Figure 1 for a schematic\nillustration). Suppose we allow klower layers in a\nn-layer model to process the question and context\ntext independently. DeFormer processes the con-\ntext texts through klower layers ofﬂine and caches\nthe output from the k-th layer. During runtime the\nquestion is ﬁrst processed through the k-layers of\nthe model, and the text representation for the k-th\nlayer is loaded from the cache. These two k-th\nlayer representations are fed to the (k+ 1)-th layer\nas input and further processing continues through\nthe higher layers as in the original model. In ad-\ndition to directly reducing the amount of runtime\ncompute, this also reduces memory signiﬁcantly as\nthe intermediate text representations for the context\nare no longer held in memory.\nA key strength of this approach is that one\ncan make any pre-trained Transformer-based QA\nmodel faster by creating a corresponding DeFormer\nversion that is directly ﬁne-tuned on the target QA\ndatasets without having to repeat the expensive\npre-training. Our empirical evaluation on multi-\nple QA datasets show that with direct ﬁne-tuning\nthe decomposed model incurs only a small loss in\naccuracy compared to the full model.\nThis loss in accuracy can be reduced further by\nlearning from the original model. We want De-\nFormer to behave more like the original model.\nIn particular, the upper layers of DeFormer should\nproduce representations that capture the same kinds\nof information as the corresponding layers in the\noriginal model. We add two distillation-like auxil-\niary losses (Hinton et al., 2015), which minimize\nthe output-level and the layer-level divergences be-\ntween the decomposed and original models.\nWe evaluate DeFormer versions of two\ntransformer-based models, BERT and XLNet on\nthree different QA tasks and two sentence-sentence\npaired-input tasks1. DeFormer achieves substan-\ntial speedup (2.7 to 4.3x) and reduction in mem-\nory (65.8% to 72.9%) for only small loss in ef-\nfectiveness (0.6 to 1.8 points) for QA. Moreover,\nwe ﬁnd that DeFormer version of BERT-large is\nfaster than the original version of the smaller BERT-\nbase model, while still being more accurate. Ab-\nlations shows that the supervision strategies we\nintroduce provide valuable accuracy improvements\nand further analysis illustrate that DeFormer pro-\nvides good runtime vs accuracy trade-offs.\n2 Decomposing Transformers for Faster\nInference\nThe standard approach to using transformers for\nquestion answering is to compute the self-attention\nover both question and the input text (typically a\npassage). This yields highly effective representa-\ntions of the input pair since often what information\nto extract from the text depends on the question and\nvice versa. If we want to reduce complexity, one\nnatural question to ask is whether we can decom-\npose the Transformer function over each segment\nof the input, trading some representational power\nfor gains in ability to push processing the text seg-\nment ofﬂine.\nThe trade-off depends on how important it is to\nhave attention from question tokens when forming\ntext representations (and vice versa) in the lower\nlayers. To assess this, we measured how the text\nrepresentation changes when paired with different\nquestions. In particular, we computed the average\npassage representation variance when paired with\ndifferent questions. The variance is measured using\ncosine distance between the passage vectors and\ntheir centroid. As Figure 2 shows that in the lower\nlayers, the text representation does not change as\nmuch as it does in the upper layers, suggesting ig-\nnoring attention from question tokens in lower lay-\ners may not be a bad idea. This is also in agreement\nwith results on probing tasks which suggest that\nlower layers tend to model mostly local phenom-\nena (e.g., POS, syntactic categories), while higher\nlayers tend to model more semantic phenomena\nthat are task dependent (e.g, entity co-reference)\nrelying on wider contexts.\n1These simulate other information seeking applications\nwhere one input is available ofﬂine.\n4489\nLayer\nRepresentation Variance0.0\n0.3\n0.5\n0.8\n1.0\n1 2 3 4 5 6 7 8 9 10 11 12\nFigure 2: Normalized variance of passage representa-\ntions when paired with different questions at different\nlayers. We deﬁne the representation variance as the\naverage cosine distance from the centroid to all repre-\nsentation vectors. In this ﬁgure, the variance is aver-\naged for 100 paragraphs (each paired with 5 different\nquestions) and normalized to [0, 1]. Smaller variance\nin the lower layers indicates the passage representation\ndepends less on the question, while higher variance in\nthe upper layers shows the passage representation relies\nmore on the interaction with the question.\nHere we formally describe our approach for de-\ncomposing attention in the lower layers to allow\nquestion independent processing of the contexts.\n2.1 DeFormer\nFirst, we formally deﬁne the computation of a\nTransformer for a paired-task containing two seg-\nments of text, Ta and Tb. Let the token em-\nbedding representations of segment Ta be A =\n[a1; a2; ...; aq] and of Tb be B = [b1; b2; ...; bp].\nThe full input sequence X can be expressed by con-\ncatenating the token representations from segment\nTa and Tb as X = [A; B]. The Transformer en-\ncoder has nlayers (denoted Li for layer i), which\ntransform this input sequentially: Xl+1 = Li(Xl).\nFor the details of the Transformer layer, we refer\nthe reader to (Vaswani et al., 2017). We denote the\napplication of a stack of layers from layer i to layer\nj be denoted as Li:j. The output representations of\nthe full Transformer, An and Bn can be written as:\n[An; Bn] =L1:n([A0; B0]) (1)\nFigure 3 shows a schematic of our model. We\ndecompose the computation of lower layers (up to\nlayer k) by simply removing the cross-interactions\nbetween Ta and Tb representations. Here k is a\nhyper-parameter. The output representations of\nthe decomposed Transformer, An and Bn can be\nexpressed as:\n[An; Bn] =Lk+1:n([L1:k(A0); L1:k(B0)) (2)\nTransformer-based QA systems process the in-\nput question and context together through a stack\nof self-attention layers. So applying this decompo-\nsition to Transformer for QA allows us to process\nthe question and the context text independently,\nwhich in turn allows us to compute the context text\nrepresentations for lower layers ofﬂine. With this\nchange the runtime complexity of each lower layer\nis reduced from O((p+ q)2) to O(q2 + c), where c\ndenotes cost of loading the cached representation.\n2.2 Auxiliary Supervision for DeFormer\nDeFormer can be used in the same way as the orig-\ninal Transformer. Since DeFormer retains much of\nthe original structure, we can initialize this model\nwith the pre-trained weights of the original Trans-\nformer and ﬁne-tune directly on downstream tasks.\nHowever, DeFormer looses some information in\nthe representations of the lower layers. The upper\nlayers can learn to compensate for this during ﬁne-\ntuning. However, we can go further and use the\noriginal model behavior as an additional source of\nsupervision.\nTowards this end, we ﬁrst initialize the param-\neters of DeFormer with the parameters of a pre-\ntrained full Transformer, and ﬁne-tune it on the\ndownstream tasks. We also add auxiliary losses\nthat make DeFormer predictions and its upper layer\nrepresentations closer to the predictions and cor-\nresponding layer representations of the full Trans-\nformer.\nKnowledge Distillation Loss: We want the\nprediction distribution of DeFormer to be closer\nto that of the full Transformer. We minimize\nthe Kullback—Leibler divergence between decom-\nposed Transformer prediction distribution PA and\nfull Transformer prediction distribution PB:\nLkd = DKL(PA∥PB)\nLayerwise Representation Similarity Loss:\nWe want the upper layer representations of De-\nFormer to be closer to those of full Transformer.\nWe minimize the euclidean distance between token\nrepresentations of the upper layers of decomposed\nTransformer and the full Transformer. Letvj\ni be the\nrepresentation of thejth token in theith layer in the\nfull transformer, and let uj\ni be the corresponding\n4490\nDecompose\nT ransformer \nLayer 1\nLayer 2\nLayer k\nLayer k+1\nLayer n\nLayer 1\nLayer 2\nLayer k\nLayer k+1\nLayer n\nLayer 1\nLayer 2\nLayer k\nPredictions Predictions\nAuxilliary \nSupervision\n(KD + LRS)\nCLS T ok1 T ok2 SEP T ok3 T ok4 CLS T ok1 T ok2 SEP T ok3 T ok4 \nDeFormer \nT ransformer\nEncoder\n(lower layers)\nT ransformer\nEncoder\n(upper layers)\nFigure 3: Decomposing Transformers up to layer k, which enables encoding each segment independently from\nlayer 1 to layer k. Auxiliary supervision of upper layer information from the original model further helps the\ndecomposed model to compensate for information loss in the lower layers. KD is Knowledge Distillation loss and\nLRS is Layerwise Representation Similarity loss.\nrepresentation in DeFormer. For each of the upper\nlayers k+ 1through n, we compute a layerwise\nrepresentation similarity (lrs) loss as follows:\nLlrs =\nn∑\ni=k\nm∑\nj=1\n∥vi\nj −ui\nj∥2\nWe add the knowledge distillation loss ( Lkd)\nand layerwise representation similarity loss (Llrs)\nalong with the task speciﬁc supervision Loss\n(Lts) and learn their relative importance via hyper-\nparameter tuning:\nLtotal = γLts + αLkd + βLlrs (3)\nWe use Bayesian Optimization (Mo ˇckus, 1975)\nto tune the γ, αand βinstead of simple trial-and-\nerror or grid/random search. This is aimed at re-\nducing the number of steps required to ﬁnd a com-\nbination of hyper-parameters that are close to the\noptimal one.\n3 Evaluation\n3.1 Datasets\nWe use the pre-trained uncased BERT base and\nlarge2 models on ﬁve different paired-input prob-\nlems covering 3 QA tasks, and in addition two other\nsentence-sentence tasks3.\n2Whole Word Masking version\n3We pick these as additional datasets to show the utility\nof decomposition in other information seeking applications\nSQuAD v1.1 (Stanford Question Answering\nDataset) (Rajpurkar et al., 2016) is an extractive\nquestion answering datasets containing >100,000\nquestion and answer pairs generated by crowd\nworkers on Wikipedia articles.\nRACE (Lai et al., 2017) is reading comprehension\ndataset collected from the English exams that are\ndesigned to evaluate the reading and reasoning abil-\nity of middle and high school Chinese students. It\nhas over 28,000 passages and 100,000+ questions.\nBoolQ (Clark et al., 2019a) consists of 15942\nyes/no questions that are naturally occurring in un-\nprompted and unconstrained settings.\nMNLI (Multi-Genre Natural Language Inference)\n(Williams et al., 2018) is a crowd-sourced corpus\nof 433k sentence pairs annotated with textual en-\ntailment information.\nQQP (Quora Question Pairs) (Iyer et al., 2019) con-\nsists of over 400,000 potential duplicate question\npairs from Quora.\nFor all 5 tasks, we use the standard splits pro-\nvided with the datasets but in addition divide the\noriginal training data further to obtain a 10% split\nto use for tuning hyper-parameters (tune split), and\nuse the original development split for reporting ef-\nﬁciency (FLOPs, memory usage) and effectiveness\nsimilar to QA, where one of the inputs can be assumed to be\navailable ofﬂine. For instance, we may want to ﬁnd answer\n(premise) sentences from a collection that support information\ncontained in a query (hypothesis) sentence. Another use case\nis FAQ retrieval, where a user question is compared against a\ncollection of previously asked questions.\n4491\nmetrics (accuracy or F1 depending on the task).\n3.2 Implementation Details\nWe implement all models in TensorFlow 1.15\n(Abadi et al., 2015) based on the original BERT\n(Devlin et al., 2019) and the XLNet (Yang et al.,\n2019) codebases. We perform all experiments on\none TPU v3-8 node (8 cores, 128GB memory) with\nbﬂoat16 format enabled. We measure the FLOPs\nand memory consumption through the TensorFlow\nProﬁler4. For DeFormer models, we tune the hy-\nperparameters for weighting different losses using\nbayesian optimizaiton libray (Nogueira, Fernando,\n2019) with 50 iterations on the tune split (10%\nof the original training sets) and report the perfor-\nmance numbers on the original dev sets. The search\nrange is [0.1, 2.0] for the 3 hyper-parameters. We\nput the detail hyper-parameters in the section A.\nFor DeFormer-BERT and DeFormer-XLNet, we\ncompute the representations for one of the input\nsegments ofﬂine and cache it. For QA we cache the\npassages, for natural language inference, we cache\nthe premise5 and for question similarity we cache\nthe ﬁrst question6.\n3.3 Results\nTable 1 shows the main results comparing perfor-\nmance, inference speed and memory requirements\nof BERT-base and DeFormer-BERT-base when us-\ning nine lower layers, and three upper layers (see\nSubsection 3.4 for the impact of the choice of up-\nper/lower splits). We observe a substantial speedup\nand signiﬁcant memory reduction in all the datasets,\nwhile retaining most of the original model’s effec-\ntiveness (as much as 98.4% on SQuAD and 99.8%\non QQP datasets), the results of XLNet in the same\ntable demonstrates the decomposition effectiveness\nfor different pre-trained Transformer architectures.\nTable 2 shows that the decomposition brings 2x\nspeedup in inference and more than half of mem-\nory reduction on both QQP and MNLI datasets,\nwhich take pairwise input sequences. The effective-\nness of decomposition generalizes further beyond\nQA tasks as long as the input sequences are paired.\n4https://www.tensorflow.org/versions/\nr1.15/api_docs/python/tf/profiler/\nprofile\n5One use case is where we want to ﬁnd (premise) sentences\nfrom a collection that support information contained in a query\n(hypothesis) sentence.\n6One use case is FAQ retrieval, where a user question is\ncompared against a collection of previously asked questions\nEfﬁciency improvements increase with the size of\nthe text segment that can be cached.\nSmall Distilled or Large Decomposed? Ta-\nble 3 compares performance, speed and memory\nof BERT-base, BERT-large and DeFormer-BERT-\nlarge. DeFormer-BERT-large is 1.6 times faster\nthan the smaller BERT-base model. Decomposing\nthe larger model turns out to be also more effective\nthan using the smaller base model (+2.3 points)\nThis shows that with decomposition, a large Trans-\nformer can run faster than a smaller one which is\nhalf its size, while also being more accurate.\nDistilling a larger model into a smaller one can\nyield better accuracy than training a smaller model\nfrom scratch. As far as we know, there are two\nrelated but not fully comparable results. (1) Tang\net al. (2019) distill BERT to a small LSTM based\nmodel where they achieve 15x speedup but at a\nsigniﬁcant drop in accuracy of more than 13 points\non MNLI. (2) Sanh et al. (2019) distill BERT to a\nsmaller six layer Transformer, which can provide\n1.6x speedup but gives >2 points accuracy drop on\nMNLI and >3 points F1 drop on SQuAD. A fair\ncomparison requires more careful experimentation\nexploring different distillation sizes which requires\nrepeating pre-training or data augmentation – an\nexpensive proposition.\nDevice Results:To evaluate the impact on dif-\nferent devices, we deployed the models on three\ndifferent machines (a GPU, CPU, and a mobile\nphone). Table 4 shows the average latency in an-\nswering a question measured on a subset of the\nSQuAD dataset. On all devices, we get more than\nthree times speedup.\n3.4 Ablation Study\nTable 5 shows the contribution of auxiliary\nlosses for ﬁne-tuning DeFormer-BERT on SQuAD\ndataset. The drop in effectiveness when not using\nLayerwise Representation Similarity (LRS in ta-\nble), and Knowlege Distillation (KD) losses shows\nthe utility of auxiliary supervision.\nFigure 4a and ﬁgure 4b show how the effec-\ntiveness and inference speed of DeFormer-BERT\nchanges as we change the separation layer. In-\nference speedup scales roughly quadratically with\nrespect to the number of layers with decomposed\nattention. The drop in effectiveness, on the other\nhand, is negligible for separating at lower layers\n(until layer 3 for the base model and until layer 13\nfor the large model) and increases slowly after that\n4492\nModel Datasets Avg. Input Original DeFormer- Performance Drop Inference Memory\nTokens base base (absolute | %age) Speedup Reduction\n(times) (%age)\nSQuAD 320 88.5 87.1 1.4 | 1.6 3.2x 70.3\nBERT RACE 2048 66.3 64.5 1.8 | 2.7 3.4x 72.9\nBoolQ 320 77.8 76.8 1.0 | 1.3 3.5x 72.0\nSQuAD 320 91.6 90.4 1.2 | 1.3 2.7x 65.8\nXLNet RACE 2048 70.3 68.7 1.6 | 2.2 2.8x 67.6\nBoolQ 320 80.4 78.8 0.6 | 0.7 3.0x 68.3\nTable 1: (i) Performance of original ﬁne-tuned vs ﬁne-tuned models of DeFormer-BERT-base and DeFormer-\nXLNet-base, (ii) Performance drop, inference speedup and inference memory reduction of DeFormer- over original\nmodels for 3 QA tasks. DeFormer-BERT-base uses nine lower layers, and three upper layers with caching enabled,\nDeFormer-XLNet-base use eight lower layers, and four upper layers with caching enabled. For SQuAD and RACE\nwe also train with the auxiliary losses, and for the others we use the main supervision loss – the settings that give\nthe best effectiveness during training. Note that the choice of the loss doesn’t affect the efﬁciency metrics.\nAvg. Input BERT DeFormer- Performance Drop Inference Memory\nTokens base BERT base (absolute | %age) Speedup Reduction\n(times) (%age)\nMNLI 120 84.4 82.6 1.8 | 2.1 2.2x 56.4\nQQP 100 90.5 90.3 0.2 | 0.2 2.0x 50.0\nTable 2: (i) Performance of BERT-base vs DeFormer-BERT-base, (ii) Performance drop, inference speedup and\ninference memory reduction of DeFormer-BERT-base over BERT-base for 2 pairwise tasks. DeFormer-BERT-base\nuses nine lower layers, and three upper layers with caching enabled.\nPerformance (Squad-F1) Speed (GFLOPs) Memory (MB)\nBERT-large 92.3 204.1 1549.6\nBERT-base 88.5 58.4 584.2\nDeFormer-BERT-large 90.8 47.7 359.7\nTable 3: Performance, Inference Speed and Memory for different models on SQuAD.\nBERT DeFormer-BERT\nTesla V100 GPU 0.22 0.07\nIntel i9-7900X CPU 5.90 1.66\nOnePlus 6 Phone 10.20* 3.28*\nTable 4: Inference latency (in seconds) on SQuAD\ndatasets for BERT-base vs DeFormer-BERT-base, as\nan average measured in batch mode. On the GPU and\nCPU batch size is 32 and on the phone (marked by *)\nbatch size is 1.\nwith a dramatic increase in the last layers closest to\nthe output. The separation layer choice thus allows\ntrading effectiveness for inference speed.\n4 Analyses\n4.1 Divergence of DeFormer and original\nBERT representations\nThe main difference between the original BERT\nand the DeFormer-BERT is the absence of cross\nBase Model Large Model\nBERT 88.5 92.3\nDeFormer-BERT 87.1 90.8\nw/o LRS 86.2 88.9\nw/o KD & LRS 85.8 87.5\nTable 5: Ablation analysis on SQuAD datasets\nfor DeFormer-BERT-base and DeFormer-BERT-large\nmodels. LRS is the layerwise representation similar-\nity loss. KD is the knowledge distillation loss on the\nprediction distributions.\nattention in the lower layers. We analyze the dif-\nferences between the representations of the two\nmodels across all layers. To this end, we randomly\nselect 100 passages from SQuAD dev dataset as\nwell as randomly selecting 5 different questions\nthat already exist in the dataset associated with\neach passage. For each passage, we encode all 5\nquestion-passage pair sequence using both the ﬁne-\ntuned original BERT-base model and the DeFormer-\n4493\n1.1 1.2 1.3 1.4 1.6 1.9 2.2 2.6 3.2\n4.3\n6.4\n-0.3 -0.3 -0.2 -0.9\n-2.6 -2.9 -2.4 -3.1 -2.7 -3.3\nSeparation Layer\nF1 score drop\nSpeedup\n-18.0\n-12.0\n-6.0\n0.0\n-1.0\n2.0\n5.0\n8.0\n1 2 3 4 5 6 7 8 9 10 11\nInference speedup F1 drop\n(a) F1 drop versus speedup on SQuAD for DeFormer-BERT-\nbase without auxiliary supervision.\n2.0 2.2 2.4 2.6 2.9 3.2 3.7\n4.3\n5.1\n6.4\n8.4-0.2 -0.9 -1.4 -2.8 -3.9 -4.5 -4.1 -4.8 -6.2\n-13.0\n-17.6\nSeparation Layer\nF1 score drop\nSpeedup\n-24.0\n-18.0\n-12.0\n-6.0\n0.0\n0.0\n3.0\n6.0\n9.0\n13 14 15 16 17 18 19 20 21 22 23\nInference speedup F1-drop (b) F1 drop versus speedup on SQuAD for DeFormer-BERT-\nlarge without auxiliary supervision.\nFigure 4: F1 drop versus speedup of DeFormer-BERT model (without auxiliary supervision) when separating at\ndifferent layers.\nBERT-base model, and compute their distance of\nthe vector representations at each layer.\nFigure 5 shows the averaged distances of both\nthe question and passage at different layers. The\nlower layer representations of the passage and ques-\ntions for both models remain similar but the upper\nlayer representations differ signiﬁcantly, support-\ning the idea that lack of cross-attention has less\nimpact in the lower layers than in the higher ones.\nAlso, using the auxiliary supervision of upper lay-\ners has the desired effect of forcing DeFormer to\nproduce representations that are closer to the orig-\ninal model. This effect is less pronounced for the\nquestion representations.\n4.2 Inference Cost\nDeFormer enables caching of text representations\nthat can be computed ofﬂine. While a full-scale\nanalysis of the detailed trade-offs in storage ver-\nsus latency is beyond the scope of this paper, we\npresent a set of basic calculations to illustrate that\nthe storage cost of caching can be substantially\nsmaller compared to the inference cost. Assum-\ning a use case of evaluating one million question-\npassage pairs daily, we ﬁrst compute the storage\nrequirements of the representations of these pas-\nsages. With the BERT-base representations we\nestimate this to be 226KB per passage and 226GB\nin total for 1 million passages. The cost of storing\nthis data and the added compute costs and reading\nthese passages at the current vendor rates amounts\nto a total of $61.7 dollars per month. To estimate\ninference cost, we use the compute times we ob-\ntain from our calculations and use current vendor\nrates for GPU workloads which amounts to $148.5\ndollars to support the 1 million question-passage\npair workload. The substantial reduction in cost\nis because the storage cost is many orders of mag-\nnitude cheaper than using GPUs. Details of these\ncalculations are listed in the Appendix.\n5 Related work\nSpeeding up inference in a model requires reducing\nthe amount of compute involved. There are two\nbroad related directions of prior work:\n(i) Compression techniquescan be used to re-\nduce model size through low rank approximation\n(Zhang et al., 2015; Kim et al., 2015; Tai et al.,\n2015; Chen et al., 2018), and model weights prun-\ning (Guo et al., 2016; Han et al., 2015), which have\nbeen shown to help speedup inference in CNN and\nRNN based models. For Transformers, Michel\net al. (2019) explore pruning the attention heads\nto gain inference speedup. This is an orthogonal\napproach that can be combined with our decom-\nposition idea. However, for the paired-input tasks\nwe consider, pruning heads only provides limited\nspeedup. In more recent work Ma et al. (2019)\npropose approximating the quadratic attention com-\nputation with a tensor decomposition based multi-\nlinear attention model. However, it is not clear how\nthis multi-linear approximation can be applied to\npre-trained Transformers like BERT.\n(ii) Distillation techniquescan be used to train\nsmaller student networks to speedup inference.\nTang et al. (2019) show that BERT can be used\nto guide designing smaller models (such as single-\nlayer BiLSTM) for multiple tasks. But for the tasks\nwe study, such very small models suffer a signiﬁ-\ncant performance drop. For instance there is a 13%\naccuracy degration on MNLI task. Another closely\nrelated recent work is DistillBERT (Sanh et al.,\n2019), which trains a smaller BERT model (half\nthe size of BERT-base) that runs 1.5 times faster\n4494\nLayer\n0.00\n0.25\n0.50\n0.75\n1.00\n1 2 3 4 5 6 7 8 9 10 11 12\nBERT vs DeFormer-BERT\nBERT vs  DeFormer-BERT w/o aux loss\n(a) Passage distance comparison\nLayer\n0.00\n0.25\n0.50\n0.75\n1.00\n1 2 3 4 5 6 7 8 9 10 11 12\nBERT vs DeFormer-BERT\nBERT vs  DeFormer-BERT w/o aux loss (b) Question distance comparison\nFigure 5: Representation distance of BERT vs DeFormer-BERT and distance of BERT vs DeFormer-BERT w/o\nauxiliary loss/supervision\nthan the original BERT-base.However, the distilled\nmodel incurs a signiﬁcant drop in accuracy. While\nmore recent distillation works such as (Jiao et al.,\n2019) and (Sun et al., 2020) further improve the\nspeedups, our decomposition also achieves simi-\nlar accuracy performance. More importantly, this\ndistillation model usually undergo expensive pre-\ntraining on the language modeling tasks before they\ncan be ﬁne-tuned for the downstream tasks.\nPrevious QA neural models like BIDAF(Seo\net al., 2016), QANet(Yu et al., 2018) and many\nothers contain decomposition as part of their neu-\nral architecture design. In contrast, the focus of our\nwork is to show that large pre-trained Transformer\nmodels can be decomposed at the ﬁne-tuning stage\nto bring effectiveness of SOTA pre-trained trans-\nformers at much lower inference latency.\nIn this work, we ask if can we speedup the in-\nference of Transformer models without compress-\ning or removing model parameters. Part of the\nmassive success of pre-trained Transformer mod-\nels for many NLP task is due to a large amount of\nparameters capacity to enable complex language\nrepresentations. The decomposition we propose\nmakes minimal changes retaining the overall capac-\nity and structure of the original model but allows\nfor faster inference by enabling parallel processing\nand caching of segments.\nDeFormer applies to settings where the underly-\ning model relies on input-wide self-attention layers.\nEven with models that propose alternate ways to\nimprove efﬁciency, as long as the models use input-\nwide self-attention, DeFormer can be applied as\na complementary mechanism to further improve\ninference efﬁciency. We leave an evaluation of ap-\nplying DeFormer on top of other recent efﬁciency\noptimized models for future work.\n6 Conclusion\nTransformers have improved the effectiveness of\nNLP tools by their ability to incorporate large con-\ntexts effectively in multiple layers. This however\nimposes a signiﬁcant complexity cost. In this work,\nwe showed that modeling such large contexts may\nnot always be necessary. We build a decomposition\nof the transformer model that provides substantial\nimprovements in inference speed, memory reduc-\ntion, while retaining most of the original model’s\naccuracy. A key beneﬁt of the model is that its\narchitecture remains largely the same as the origi-\nnal model which allows us to avoid repeating pre-\ntraining and use the original model weights for ﬁne-\ntuning. The distillation techniques further reduce\nthe performance gap with respect to the original\nmodel. This decomposition model provides a sim-\nple yet strong starting point for efﬁcient QA models\nas NLP moves towards increasingly larger models\nhandling wider contexts.\nAcknowledgement\nWe thank Google for supporting this research\nthrough the Google Cloud Platform credits.\nReferences\nMart´ın Abadi, Ashish Agarwal, Paul Barham, Eugene\nBrevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado,\nAndy Davis, Jeffrey Dean, Matthieu Devin, Sanjay\nGhemawat, Ian Goodfellow, Andrew Harp, Geoffrey\nIrving, Michael Isard, Yangqing Jia, Rafal Jozefow-\nicz, Lukasz Kaiser, Manjunath Kudlur, Josh Leven-\nberg, Dandelion Man´e, Rajat Monga, Sherry Moore,\nDerek Murray, Chris Olah, Mike Schuster, Jonathon\nShlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar,\nPaul Tucker, Vincent Vanhoucke, Vijay Vasudevan,\nFernanda Vi´egas, Oriol Vinyals, Pete Warden, Mar-\ntin Wattenberg, Martin Wicke, Yuan Yu, and Xiao-\n4495\nqiang Zheng. 2015. TensorFlow: Large-scale ma-\nchine learning on heterogeneous systems. Software\navailable from tensorﬂow.org.\nQingqing Cao, Noah Weber, Niranjan Balasubrama-\nnian, and Aruna Balasubramanian. 2019. DeQA:\nOn-Device Question Answering. In Proceedings of\nthe 17th Annual International Conference on Mobile\nSystems, Applications, and Services - MobiSys ’19 ,\npages 27–40, Seoul, Republic of Korea. ACM Press.\nPatrick Chen, Si Si, Yang Li, Ciprian Chelba, and\nCho-Jui Hsieh. 2018. Groupreduce: Block-wise\nlow-rank approximation for neural language model\nshrinking. In Advances in Neural Information Pro-\ncessing Systems, pages 10988–10998.\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019a. Boolq: Exploring the surprising\ndifﬁculty of natural yes/no questions. arXiv preprint\narXiv:1905.10044.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019b. What does bert\nlook at? an analysis of berts attention. Proceedings\nof the 2019 ACL Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nGoogle. 2019. Understanding searches better than ever\nbefore. https://blog.google/products/search/search-\nlanguage-understanding-bert/.\nYiwen Guo, Anbang Yao, and Yurong Chen. 2016. Dy-\nnamic network surgery for efﬁcient dnns. In Ad-\nvances In Neural Information Processing Systems ,\npages 1379–1387.\nSong Han, Huizi Mao, and William J Dally. 2015.\nDeep compression: Compressing deep neural net-\nworks with pruning, trained quantization and huff-\nman coding. arXiv preprint arXiv:1510.00149.\nYaru Hao, Li Dong, Furu Wei, and Ke Xu. 2019. Visu-\nalizing and understanding the effectiveness of bert.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network.\nShankar Iyer, Nikhil Dandekar, and Ko-\nrnl Csernai. 2019. Quora question pairs.\nhttps://www.quora.com/q/quoradata/\nFirst-Quora-Dataset-Release-Question-Pairs .\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang,\nXiao Chen, Linlin Li, Fang Wang, and Qun Liu.\n2019. Tinybert: Distilling bert for natural language\nunderstanding.\nYong-Deok Kim, Eunhyeok Park, Sungjoo Yoo, Taelim\nChoi, Lu Yang, and Dongjun Shin. 2015. Compres-\nsion of deep convolutional neural networks for fast\nand low power mobile applications. arXiv preprint\narXiv:1511.06530.\nNVIDIA Performance Lab. 2019. Consider-\nations for scaling gpu-ready data centers.\nhttps://www.nvidia.com/content/g/pdfs/\nGPU-Ready-Data-Center-Tech-Overview.\npdf.\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,\nand Eduard Hovy. 2017. Race: Large-scale reading\ncomprehension dataset from examinations. arXiv\npreprint arXiv:1704.04683.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nXindian Ma, Peng Zhang, Shuai Zhang, Nan Duan,\nYuexian Hou, Dawei Song, and Ming Zhou. 2019.\nA tensorized transformer for language modeling.\narXiv preprint arXiv:1906.09777.\nPaul Michel, Omer Levy, and Graham Neubig. 2019.\nAre sixteen heads really better than one? arXiv\npreprint arXiv:1905.10650.\nJonas Mo ˇckus. 1975. On bayesian methods for seek-\ning the extremum. In Optimization Techniques IFIP\nTechnical Conference, pages 400–404. Springer.\nNogueira, Fernando. 2019. Bayesianopti-\nmization. https://github.com/fmfn/\nBayesianOptimization. [Online; accessed\n22-September-2019].\nGoogle Cloud Platform. 2019. Cloud pricing.\nhttps://cloud.google.com/compute/\nall-pricing#gpus,https://cloud.google.\ncom/storage/pricing.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions\nfor machine comprehension of text. arXiv preprint\narXiv:1606.05250.\nVictor Sanh, Lysandre Debut, and Thomas Wolf.\n2019. Introducing distilbert, a distilled version\nof bert. https://medium.com/huggingface/\ndistilbert-8cf3380435b5.\nMinjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and\nHannaneh Hajishirzi. 2016. Bidirectional attention\nﬂow for machine comprehension. arXiv preprint\narXiv:1611.01603.\n4496\nZhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu,\nYiming Yang, and Denny Zhou. 2020. Mobilebert:\na compact task-agnostic bert for resource-limited de-\nvices.\nCheng Tai, Tong Xiao, Yi Zhang, Xiaogang Wang, et al.\n2015. Convolutional neural networks with low-rank\nregularization. arXiv preprint arXiv:1511.06067.\nRaphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga\nVechtomova, and Jimmy Lin. 2019. Distilling task-\nspeciﬁc knowledge from bert into simple neural net-\nworks. ArXiv, abs/1903.12136.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBert rediscovers the classical nlp pipeline. Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112–1122. Association for\nComputational Linguistics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V Le.\n2019. Xlnet: Generalized autoregressive pretrain-\ning for language understanding. arXiv preprint\narXiv:1906.08237.\nAdams Wei Yu, David Dohan, Minh-Thang Luong, Rui\nZhao, Kai Chen, Mohammad Norouzi, and Quoc V\nLe. 2018. Qanet: Combining local convolution\nwith global self-attention for reading comprehen-\nsion. arXiv preprint arXiv:1804.09541.\nXiangyu Zhang, Jianhua Zou, Xiang Ming, Kaiming\nHe, and Jian Sun. 2015. Efﬁcient and accurate ap-\nproximations of nonlinear convolutional networks.\nIn Proceedings of the IEEE Conference on Com-\nputer Vision and pattern Recognition , pages 1984–\n1992.\n4497\nA Appendix\nData centers often use GPUs for inference work-\nloads (Lab, 2019), we use the GPUs by default for\nboth models. We use gu to denote the cost of using\none GPU per hour, nseq to stand for the number of\ninput sequences to process, bfor the GPU batch\nsize, and tb is the time (in seconds) take to processb\nsequences, sdenotes the storage size of the cached\nrepresentations, su denotes the cost of storage per\nmonth, ru is the cost of performing 10,000 reading\noperations (such as loading cached representations\nfrom the disk).\nThe total cost of the original model Costoriginal\nis the cost of using GPUs and is given by the for-\nmula as below:\nCostoriginal = tb ·nseq\nb · gu\n3600\nAnd the total cost of the decomposed model\nCostdecomp includes three parts: using GPUs, stor-\ning representations on disk and loading them into\nmemory. It is formulated as:\nCostdecomp = tb ·nseq\nb · gu\n3600 + nseq\nb · ru\n10,000\n+ s·su\n30 ∗24 ∗3600\nWe assume a passage has 150 tokens on average\n(The number is calculated based on the SQuAD\ndataset).\nWe take one cloud service provider (Platform,\n2019) to instantiate gu, su, and ru: one Tesla V100\nGPU (16GB memory) costs $2.48 USD per hour\n(gu = 2.48), 1GB storage takes $0.02 per month\n(su = 0.02) and additional $0.004 per 10,000 read\noperations (ru = 0.004)7.\nIt takes 226KB to store the vectors for 150 to-\nkens 8, and the total storage for 1 million sequences\nis 226GB. The Tesla V100 GPU allows a maximum\nbatch size of 6409. We measure thetb = 4.6 for the\noriginal BERT-base model andtb = 1.4 for the de-\ncomposed BERT-base model. ThenCostoriginal =\n30 ∗4.6 ∗1,000,000/640 ∗2.48/3600 = $148.5,\nand Costdecomp = 30 ∗1.4 ∗1,000,000/640 ∗\n2.48/3600 + 30∗1,000,000/10,000 ∗0.004 +\n226 ∗0.02 = $61.7.\n7Class B operations on GCP\n8vector dimension=768, bﬂoat16 format\n9>640 batch size will cause V100 GPU out of memory\nHyper-parameters We set the ﬁnal α = 1.1,\nβ = 0.5 and γ = 0.7 for supervising BERT-base\nmodel on the SQuAD dataset, α = 0.4, β = 0.4\nand γ = 0.7 and on the RACE dataset. For XLNet,\nwe ﬁnd that simple default parameters ( α = 1.1,\nβ = 0.5 and γ = 0.7) work well for both SQuAD\nand BoolQ datasets.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.8301982879638672
    },
    {
      "name": "Computer science",
      "score": 0.7974972128868103
    },
    {
      "name": "Question answering",
      "score": 0.5457798838615417
    },
    {
      "name": "Open source",
      "score": 0.5086501836776733
    },
    {
      "name": "Source code",
      "score": 0.49092772603034973
    },
    {
      "name": "Distillation",
      "score": 0.4641360640525818
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3898252844810486
    },
    {
      "name": "Programming language",
      "score": 0.20663005113601685
    },
    {
      "name": "Software",
      "score": 0.1489010751247406
    },
    {
      "name": "Electrical engineering",
      "score": 0.11884820461273193
    },
    {
      "name": "Voltage",
      "score": 0.08978214859962463
    },
    {
      "name": "Engineering",
      "score": 0.06459817290306091
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I59553526",
      "name": "Stony Brook University",
      "country": "US"
    }
  ],
  "cited_by": 58
}