{
    "title": "A Comparison of Techniques for Language Model Integration in Encoder-Decoder Speech Recognition",
    "url": "https://openalex.org/W2883416004",
    "year": 2018,
    "authors": [
        {
            "id": "https://openalex.org/A5073890145",
            "name": "Shubham Toshniwal",
            "affiliations": [
                "Toyota Technological Institute at Chicago"
            ]
        },
        {
            "id": "https://openalex.org/A5023098355",
            "name": "Anjuli Kannan",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5027763497",
            "name": "Chung‐Cheng Chiu",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5010253402",
            "name": "Yonghui Wu",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5070513394",
            "name": "Tara N. Sainath",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5015602781",
            "name": "Karen Livescu",
            "affiliations": [
                "Toyota Technological Institute at Chicago"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2963747784",
        "https://openalex.org/W2606722458",
        "https://openalex.org/W6726011831",
        "https://openalex.org/W2555428947",
        "https://openalex.org/W6741807409",
        "https://openalex.org/W2963216553",
        "https://openalex.org/W6741328424",
        "https://openalex.org/W2964012862",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2131774270",
        "https://openalex.org/W6747398299",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2963211739",
        "https://openalex.org/W2577366047",
        "https://openalex.org/W6631190155",
        "https://openalex.org/W2799800213",
        "https://openalex.org/W1915251500",
        "https://openalex.org/W6629052376",
        "https://openalex.org/W2962824709",
        "https://openalex.org/W2963240019",
        "https://openalex.org/W6743477263",
        "https://openalex.org/W2166637769",
        "https://openalex.org/W2327501763",
        "https://openalex.org/W6631362777",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2964325005",
        "https://openalex.org/W2183341477",
        "https://openalex.org/W2963070863",
        "https://openalex.org/W6621543089"
    ],
    "abstract": "Attention-based recurrent neural encoder-decoder models present an elegant solution to the automatic speech recognition problem. This approach folds the acoustic model, pronunciation model, and language model into a single network and requires only a parallel corpus of speech and text for training. However, unlike in conventional approaches that combine separate acoustic and language models, it is not clear how to use additional (unpaired) text. While there has been previous work on methods addressing this problem, a thorough comparison among methods is still lacking. In this paper, we compare a suite of past methods and some of our own proposed methods for using unpaired text data to improve encoder-decoder models. For evaluation, we use the medium-sized Switchboard data set and the large-scale Google voice search and dictation data sets. Our results confirm the benefits of using unpaired text across a range of methods and data sets. Surprisingly, for first-pass decoding, the rather simple approach of shallow fusion performs best across data sets. However, for Google data sets we find that cold fusion has a lower oracle error rate and outperforms other approaches after second-pass rescoring on the Google voice search data set.",
    "full_text": "A COMPARISON OF TECHNIQUES FOR LANGUAGE MODEL INTEGRATION IN\nENCODER-DECODER SPEECH RECOGNITION\nShubham Toshniwal1, Anjuli Kannan2, Chung-Cheng Chiu2, Yonghui Wu2, Tara N Sainath2, Karen Livescu1\n1Toyota Technological Institute at Chicago, USA\n2Google Inc., USA\n{shtoshni, klivescu}@ttic.edu, {anjuli, chungchengc, yonghui, tsainath}@google.com\nABSTRACT\nAttention-based recurrent neural encoder-decoder models present an\nelegant solution to the automatic speech recognition problem. This\napproach folds the acoustic model, pronunciation model, and lan-\nguage model into a single network and requires only a parallel cor-\npus of speech and text for training. However, unlike in conventional\napproaches that combine separate acoustic and language models, it\nis not clear how to use additional (unpaired) text. While there has\nbeen previous work on methods addressing this problem, a thorough\ncomparison among methods is still lacking. In this paper, we com-\npare a suite of past methods and some of our own proposed methods\nfor using unpaired text data to improve encoder-decoder models. For\nevaluation, we use the medium-sized Switchboard data set and the\nlarge-scale Google voice search and dictation data sets. Our results\nconﬁrm the beneﬁts of using unpaired text across a range of methods\nand data sets. Surprisingly, for ﬁrst-pass decoding, the rather simple\napproach of shallow fusion performs best across data sets. However,\nfor Google data sets we ﬁnd that cold fusion has a lower oracle error\nrate and outperforms other approaches after second-pass rescoring\non the Google voice search data set.\nIndex Terms— speech recognition, encoder-decoder, language\nmodel, shallow fusion, cold fusion, deep fusion\n1. INTRODUCTION\nAttention-based recurrent neural encoder-decoder models provide\nan elegant end-to-end framework for speech recognition, machine\ntranslation, and other sequence transduction tasks [1, 2]. In auto-\nmatic speech recognition (ASR), the model folds the traditionally\nseparately learned acoustic model, pronunciation model, and lan-\nguage model (LM) into a single network that can be trained end-\nto-end. The encoder maps the input speech to a sequence of higher-\nlevel learned features, while the decoder maps these higher-level fea-\ntures to output labels with assistance from the attention mechanism\nthat provides an alignment between speech and text. The model can\nbe learned end-to-end and requires just paired speech and text data.\nEncoder-decoder models for speech recognition have become quite\npopular recently and perform competitively on a number of ASR\ntasks [3–5].\nWhile end-to-end training offers several advantages, it also re-\nstricts the training data to have both input and output sequences, for\nexample the paired speech and text data in the case of speech recog-\nnition. Conventional ASR models leverage a separate LM trained on\nall available text, which can be orders of magnitude larger than just\nthe transcripts of transcribed audio. Decoder of an encoder-decoder\nmodel is exposed only to the audio transcripts.\nPrevious work addressing the issue of utilizing unpaired text has\nproposed ways of integrating an external pretrained LM, trained on\nall of the text data, with the ASR model [6–8]. The main LM integra-\ntion approaches from past work have been referred to as shallow [6],\ndeep [6], and cold fusion [7]. The three approaches differ in two\nimportant criteria:\n• Early/late model integration: At what point in the ASR\nmodel’s computation should the LM be integrated? In deep\nand cold fusion, the external LM is fused directly into the\nASR model by combining their hidden states, resulting in a\nsingle model with tight integration. In contrast, in shallow\nfusion the LM and ASR model remain separate and only their\nscores are combined, similarly to an ensemble. The shallow\nfusion score combination is also similar to the interpolation\nof acoustic and language models done in traditional ASR.\n• Early/late training integration: At what point in the ASR\nmodel’s training should the LM be integrated? Deep and\nshallow fusion use a late integration where both the ASR and\nLM models are trained separately and then combined, while\ncold fusion uses the external pretrained LM model from the\nvery start of the ASR model training. An important point is\nthat early training integration approaches are computationally\ncostlier if either of the two models is frequently changing.\nA thorough comparison between these LM integration tech-\nniques is, to the best of our knowledge, currently lacking. In this\npaper, we compare the three fusion approaches mentioned above on\n(a) the medium-sized Switchboard data set [9] and (b) the large-scale\nGoogle voice search and dictation data sets used in [5]. Our aim is to\nshed light on how the LM integration approaches compare, as well\nas how they scale up with data size. We also propose some novel\nLM integration approaches and compare them against the three prior\nfusion approaches on Switchboard.\nOur results show that almost all of the LM integration ap-\nproaches improve over a baseline encoder-decoder model for all\ndata sets, conﬁrming the beneﬁt of utilizing unpaired text. We also\nmake several other ﬁndings: (a) the rather simple approach of shal-\nlow fusion works best for ﬁrst-pass decoding on all of our data sets,\n(b) our best proposed approach performs similarly to deep and cold\nfusion on the Switchboard data set, (c) deep fusion doesn’t scale\nwell, obtaining no or negligible gains over baseline for large-scale\nGoogle data sets, and (d) cold fusion produces high-quality and\ndiverse beam outputs resulting in lowest oracle word error rate on\nGoogle data sets and edges ahead when coupled with second-pass\nLM rescoring on Google voice search.\nc⃝IEEE 2018\narXiv:1807.10857v2  [eess.AS]  6 Nov 2018\n2. RELATED WORK\nPrevious work on using unpaired text for encoder-decoder models\ncan be categorized along two major themes:\nUsing an external language model\nThis approach consists of training an external LM on the unpaired\ntext and integrating it into the encoder-decoder model, which is the\nfocus of this paper. An early study along these lines was by Gulcehre\net al.[6], who proposed the shallow and deep fusion methods in the\ncontext of neural machine translation (NMT) models. In that work\nboth shallow and deep fusion improved performance, with deep\nfusion somewhat outperforming shallow fusion, especially for low-\nresource language pairs. Another previous work in context of NMT\nmodels by Ramachandran et al.[10] proposed initializing the lower\nlayer of both encoder and decoder with separate pretrained LMs fol-\nlowed by joint training using both language modeling and machine\ntranslation losses. Shallow fusion has largely been the method of\nchoice for ASR [3, 4, 8, 11], getting signiﬁcant performance gains,\nalthough in some cases with slight modiﬁcations to the decoding\nobjective function [4, 8, 11]. Cold fusion, a modiﬁcation of deep\nfusion, was proposed for ASR by Sriram et al.[7]. This work found\nthat, on medium-scale data sets of ∼300-400K training utterances,\ncold fusion outperforms deep fusion, especially in a cross-domain\nsetting, but did not compare with shallow fusion. None of these\nstudies compared all three fusion approaches.\nGenerating paired data from unpaired text\nA second line of research is to use unpaired text to synthetically gen-\nerate matching input sequences, thus expanding the paired data set.\nIn machine translation this process of generating paired data from\nmonolingual data is referred to as backtranslation—that is, gener-\nating source-language text from unpaired target-language text—and\nhas been used in the context of neural machine translation by Sen-\nnrich et al. [12]. The directly analogous approach for ASR would\nbe to use text-to-speech synthesis to generate speech from unpaired\ntext. The complexity of the text-to-speech (TTS) task means that\nthere has been limited work exploring the use of speech generated\nfrom unpaired text, often in limited settings [13]. A workaround of\n“translating” the text to phoneme sequences and using the resulting\npaired data in a multitask learning setup has been explored by Ren-\nduchintala et al.[14].\n3. MODEL\nOur model is based on the Listen, Attend and Spell (LAS) attention-\nbased encoder-decoder ASR model proposed by [1]. We begin by\nreviewing this model, and then describe the techniques we consider\nfor LM integration with the LAS model.\n3.1. LAS Model\nThe LAS model consists of three components: an encoder, a de-\ncoder, and an attention networkwhich are trained jointly to predict\nthe output sequence. The transcription can be decoded as a sequence\nof graphemes/characters, wordpieces, or words from a sequence of\nacoustic feature frames. Based on the recent success of wordpiece-\nbased models in a variety of ASR tasks and machine translation\ntasks [2, 3, 5, 15], we choose wordpieces as output unit in all our\nmodels.\nThe encoder consists of a stacked (bidirectional) recurrent neu-\nral network (RNN) [16] which reads in acoustic features x =\n(x1,··· ,xT ) and outputs a sequence of high-level features h. The\nsequence of high-level features hcould either be the same length\nas the acoustic feature sequence or be downsampled if a pyramidal\nstructure is used as in [1].\nThe decoder is a stacked unidirectional RNN that computes the\nprobability of a sequence of output units yas follows:\nP(y|x) =P(y|h) =\nT∏\nt=1\nP(yt|h,y<t) (1)\nAt every time step t, the conditional dependence of the output\non the encoder features his calculated via the attention mechanism.\nThe attention mechanism, which is a function of the current decoder\nhidden state and the encoder features, condenses the encoder features\ninto a context vector ct via the following mechanism:\nuit = v⊤tanh(Wh hi + Wd dt + ba)\nαt = softmax(ut) ct =\nK∑\ni=1\nαithi\nwhere the vectors v,ba and the matrices Wh,Wd are learnable pa-\nrameters; dt is the hidden state of the decoder at time step t.\nThe hidden state of the decoder,dt, which captures the previous\noutput context y<t, is given by:\ndt = RNN(˜yt−1,dt−1,ct−1)\nwhere dt−1 is the previous hidden state of the decoder, and ˜yt−1 is\na learned embedding vector for yt−1, as is typical practice in RNN-\nbased language models. The posterior distribution of the output at\ntime step tis given by:\nP(yt|h,y< t) =softmax(Ws[ ct; dt] +bs) (2)\nwhere Ws and bs are again learnable parameters. The model is\ntrained to minimize the discriminative loss:\nLLAS = −log(P(y|x))\n3.2. LM Integration Approaches\nBelow we discuss the various LM integration approaches for\nencoder-decoder models that we study.\n3.2.1. Shallow Fusion\nIn shallow fusion [6], the external LM is incorporated via log-linear\ninterpolation at inference time only. So while for the baseline model,\nbeam search is used to approximate the solution for:\ny∗ = arg max\ny\nlog p(y|x)\nin the most basic version of shallow fusion [6], we instead use the\nfollowing criterion:\ny∗ = arg max\ny\nlog p(y|x) +λlog pLM(y)\nRecently some additional penalty terms have been introduced in the\ncriterion [2, 4, 8, 11]. For example, Chorowski and Jaitly [4] use a\ncoverage penalty term c(x,y) to ensure all of the input frames have\nbeen “well attended” during decoding.\n2\ndt+1dt\ndLM\nt dLM\nt+1\nyt\nct\nyt−1 gt\nFig. 1: Illustration of a single decoding step of deep fusion.\n3.2.2. Deep Fusion\nLike shallow fusion, deep fusion [6] is a late training integration\nprocedure, i.e. it assumes the encoder-decoder and language models\nto be pretrained. The key difference is that it integrates the external\nLM into the encoder-decoder model by fusing together the hidden\nstates of the external LM (assuming a neural LM) and the decoder in\nthe following way:\ngt = σ(vT\ng dLM\nt + bg) (3)\ndDF\nt = [ct; dt; gt dLM\nt ] (4)\nP(yt|h,y< t) =softmax(WDF dDF\nt + bDF ) (5)\nwhere the scalar bg, vectors vg and bDF , and matrix WDF are all\nlearned while keeping all other model parameters ﬁxed. Fixing most\nof the model parameters reduces the backpropagation computation\ncost, and the ﬁne-tuning procedure converges quickly in comparison\nto the cost of training the baseline model.\n3.2.3. Cold Fusion\nCold fusion [7] builds on the idea of deep fusion and proposes a\nmodiﬁed LM integration procedure shown below:\nsLM\nt = DNN(dLM\nt ) (6)\nsED\nt = WED [dt; ct] +bED (7)\ngt = σ(Wg[sED\nt ; sLM\nt ] +bg) (8)\nsCF\nt = [sED\nt ; gt ◦sLM\nt ] (9)\nrCF\nt = DNN(sCF\nt ) (10)\nP(yt|h,y< t) =softmax(WCF rCF\nt + bCF ) (11)\nwhere all of the parameters introduced in the above equations are\nlearned. Some of the key differences between cold fusion and deep\nfusion are:\nct\nsED\nt\nsLM\nt\nyt−1\nyt\ndLM\nt\ndt dt+1\ndLM\nt+1\nDNN\ngt\nDNN\nFig. 2: Illustration of a single decoding step of cold fusion. Note\nthat unlike deep fusion, there is ﬁne-grained gating in cold fusion\nand hence, gate gt is a vector.\n(a) Cold fusion is an early training integrationapproach: The\nencoder-decoder model is trained from scratch with a pre-\ntrained external LM1.\n(b) Both the LM state sLM and encoder-decoder model’s state\nsED\nt are used in gate computation as shown in equation 8.\n(c) Cold fusion uses a ﬁne gating mechanism, equation 9, in com-\nparison to a coarse gating mechanism used by deep fusion,\nequation 4.\n(d) As originally proposed, cold fusion uses the LM logits rather\nthan the LM hidden state, in order to allow for ﬂexible LM\nswapping. That is, dLM\nt used in equation 6 refers to the logit\nscores of the LM rather than the hidden state of LM in the\nproposed version of cold fusion. However, in practice, with\nwordpieces used as output units, the relatively large vocab-\nulary results in a long vector of logits dLM\nt which causes an\nunnecessary increase in the number of parameters2. In our ex-\nperiments we are not concerned with the ﬂexibility of swap-\nping LMs. Hence, in our experiments we still set dLM\nt to the\nLM hidden state 3\nNote that, since cold fusion is anearly training integrationapproach,\nin a dynamic setting with frequent changes of LM and ASR models\nthe approach would be computationally costlier than the previous\ntwo fusion approaches, especially shallow fusion.\n1LM parameters are kept ﬁxed.\n2The cold fusion paper [7] experiments with character level models.\n3Our preliminary experiments with Switchboard suggest a performance\ngain with this proposed modiﬁcation.\n3\n3.2.4. LM as lower decoder layer\nPrevious work in machine translation has suggested the utility of\nusing a pretrained LM as a lower layer of the decoder [10]. Simi-\nlarly, [17] used a pretrained LM to initialize the decoder in an RNN\ntransducer model for speech recognition. The motivation for this ap-\nproach is that it can provide better contextualized word embeddings,\nas is the case with the recently proposed Embeddings from Language\nModels (ELMo) [18]. We propose introducing the external LM as a\nlower layer in the decoder of a pretrained LAS model. All of the\nmodel parameters, including the LM parameters, are ﬁne-tuned for\na few epochs with the LAS objective.\n3.2.5. LM integration via multitask learning\nGoing back to equation 1 of the decoder:\nP(y|x) =P(y|h) =\nT∏\nt=1\nP(yt|h,y<t)\nthe decoder can be seen as a conditional LM, conditioned on the\nencoder features that represent the speech input. The exact depen-\ndence of the decoder on the speech features is captured by the con-\ntext vector ct which, from equation 2, affects the output posterior\ndistribution as follows:\nP(yt|h,y<t) =softmax(Ws[ct; dt] +bs)\nNow, unpaired text has no corresponding speech signal. In the LAS\nmodel, this can be represented by a zero context vector. A zero con-\ntext vector reduces the decoder from a conditional LM to a plain LM\nas shown below 4:\nP(yt|\u0013 Sh,y< t) =softmax(Ws[\u001a\u001a >0ct ; dt] +bs)\nIn this way, the decoder can also be used for the task of language\nmodeling. Based on this observation, we propose a multitask learn-\ning approach for using the unpaired text, where the decoder of the\nLAS model is shared for the primary ASR task and the auxiliary LM\ntask. In each iteration of multitask learning, we sample one of the\ntasks among the ASR and LM task based on the prior probability for\npicking each task. Note that when the decoder is trained for LM,\nthe encoder and attention components of LAS model are unaffected.\nOne important aspect to note is that, unlike all of the previous ap-\nproaches discussed, this approach has no external LM; rather, the\ndecoder itself is trained for both tasks.\n4. EXPERIMENTAL SETUP\n4.1. Switchboard\n4.1.1. Data\nWe use the Switchboard corpus (LDC97S62) [9], which contains\nroughly 300 hours of conversational telephone speech as our choice\nof medium-scale training set. The ﬁrst 4K utterances from the train-\ning set are reserved as validation set for hyperparameter tuning and\nearly stopping. Since the training set has a large number of repeti-\ntions of short utterances (yeah, uh-huh, etc.), we remove duplicates\nbeyond a count threshold of 300. After these preprocessing steps,\n4Note that an “equivocal” ct that equally affects all of the logit scores\ncould also work. However, such a vector would depend on Ws, whereas\nct = 0 is independent of Ws.\nthe ﬁnal training set has about 192K utterances. For evaluation, we\nuse the HUB5 Eval2000 data set (LDC2002S09), that consists of\ntwo subsets: Switchboard (SWB), which is similar in style to the\ntraining set, and CallHome (CH), which contains unscripted con-\nversations between close friends and family. For acoustic features,\nwe use 40-dimensional log-mel ﬁlterbank features along with their\ndeltas, with per-speaker mean and variance normalization. For all of\nthe above data processing, we use the EESEN toolkit’s recipe [19]\nwhich is based on the Kaldi toolkit’s recipe [20].\nFor external LM training, we combine the Switchboard training\nset with the Fisher corpus (LDC200 {4,5}T19) [21]. To avoid\ndomain mismatch, we process Fisher utterances to (a) remove\nnoise/hesitation markers not used in Switchboard, and (b) ﬁlter out\nutterances not covered by the wordpiece model trained on Switch-\nboard 5. The ﬁltering process removes ∼400K utterances out of 2.2\nmillion Fisher utterances. Thus, combined with the Switchboard\ntraining utterances, the LM is trained on ∼2 million utterances.\n4.1.2. Model Details\nThe encoder is a 4-layer pyramidal bidirectional long short-term\nmemory (LSTM) network [22], resulting in an 8-fold reduction in\ntime resolution. For the 2-fold reduction done at each layer below\nthe topmost, we max-pool over 2 consecutive hidden states and feed\nthe result into the layer above. We use 256 hidden units in each\ndirection of each layer.\nThe decoder in the baseline LAS model is a single-layer unidi-\nrectional LSTM network with 256 hidden units. We use a 1K word-\npiece output vocabulary, which includes all the characters to ensure\nopen-vocabulary coverage. The vocabulary is generated using a vari-\nant of the byte pair encoding (BPE) algorithm [15] implemented in\nthe SentencePiece library by Google6. We represent the wordpieces\nwith 256-dimensional embeddings learned jointly with the rest of the\nmodel. For regularization we use: (a) label smoothing[23], where\nwe uniformly distribute 0.1 probability mass among labels other than\nthe ground-truth label, and (b) dropout [24] with probability 0.1 ap-\nplied on outputs all of the RNN layers. We also use scheduled sam-\npling [25] with a ﬁxed schedule, where each timestep’s decoder input\nis either the ground-truth previous label with probability 0.9 or sam-\npled from the model’s posterior distribution for the previous label\nwith probability 0.1.\nFor inference, we use beam search with beam size of 10. We\nobserved that for some of the models increasing the beam size to\n10 resulted in escalation of insertion errors compared to a lower\nbeam size. To counter this, we add a wordpiece insertion reward\n∈{0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9}, tuned on the devel-\nopment set. With the addition of the wordpiece insertion reward,\nlarger beam sizes outperform smaller beam sizes for all models, with\ninsigniﬁcant gains beyond a sufﬁciently large beam size of 10. For\nshallow fusion, we pick the LM weight λfrom {0,0.05,0.1,0.15,\n0.2,0.25}by tuning on the development set, resulting in a ﬁnal\ntuned value of λ= 0.2.\nThe external LM is a single-layer 512 hidden unit RNN with\nLSTM cells. The RNN hidden state is ﬁrst passed through a pro-\njection layer with 256 hidden units and ﬁnally fed into the softmax\nlayer. The LM is trained with the same output vocabulary as the LAS\nmodel. The LM is trained for 20 epochs with early stopping based\n5Some utterances in Fisher have symbols such as period sign which are\nnot present in Switchboard and hence are not covered by the wordpiece model\ntrained on Switchboard transcripts.\n6https://github.com/google/sentencepiece\n4\non development set perplexity and attains a perplexity of∼15 on the\nSwitchboard development set.\n4.1.3. Training Details\nWe bucket our training data by utterance length into 5 buckets, re-\nstricting utterances within a minibatch to come from a single bucket\nfor training efﬁciency. Different minibatch sizes are used for differ-\nent buckets, with a batch size of 128 used for the shortest utterances\nand a 32 batch size used for the bucket with longest ones. Prelim-\ninary experiments suggested a performance beneﬁt by proceeding\nthrough the training set from the bucket with smallest utterances to\nthe one with longest utterances in each epoch. We use this order for\ntraining all of our models. (A similar training order scheme was also\nused in [26].)\nAll models are trained using the Adam optimizer [27] with an\ninitial learning rate of 0.001. For the baseline LAS model and mod-\nels with early LM training integration, we train for 12 epochs and\nstart halving the learning rate every epoch after 7 epochs. For the\nmodels with late LM training integration, we start halving the learn-\ning rate every epoch after 4 epochs and train for a total of 8 epochs.\nFor all models, we use early stopping based on development set\nWER when using greedy decoding.\nTo speed up training, the encoder of all models is initialized with\nthe encoder of a LAS model trained for predicting phone sequences,\nsimilarly to [26]. All models are trained on a single NVIDIA Ti-\ntanX GPU and ﬁnish training within 2 days, with each epoch taking\n3-4 hours. Finally, all of our models are implemented in Tensor-\nFlow [28].\n4.2. Google Voice Search and Dictation\n4.2.1. Data\nThe training data consists of approximately 22 million anonymized,\nhuman-transcribed utterances representative of live Google trafﬁc,\nboth V oice Search and dictation. Clean utterances are artiﬁcially cor-\nrupted using a room simulator, adding varying degrees of noise and\nreverberation such that the overall SNR is between 0dB and 30dB,\nwith an average SNR of 12dB. The noise sources are from YouTube\nand daily life noisy environmental recordings. The models are eval-\nuated on two data sets: VS14K, which consists of about 14K V oice\nSearch utterances, and D15K, which contains about 15K dictation\nutterances.\nThe external LM is trained on a variety of text data sources,\nincluding untranscribed anonymized voice queries (both search and\ndictation), anonymized typed queries from Google Search, as well\nas the transcribed training utterances mentioned above. Since these\ncomponent data sources have varying sizes, we up- and down-\nsample to mix them at a 1:1:1 ratio.\n4.2.2. Model Details\nOur LAS model is consistent with [5]: The encoder is composed of\n5 unidirectional LSTM layers of 1400 hidden units each, the atten-\ntion mechanism is a multi-headed additive attention with four heads,\nthe decoder consists of 2 unidirectional LSTM layers of 1024 hidden\nunits each, and the output vocabulary is 16384 wordpieces. We use\n80-dimensional log-mel ﬁlterbank features, computed with a 25ms\nwindow and shifted every 10ms. Similarly to [29, 30], at each frame\nt, these features are stacked with 3 frames to the left and downsam-\npled to a 30ms frame rate.\nAs in [5], inference is done via beam search with a beam size of\n8. Shallow fusion numbers are reported after tuning the LM weightλ\nover the values{0,0.05,0.1,0.15,0.2,0.25,0.3,0.35}and a cover-\nage penalty over the values{0,0.01,0.02,0.03,0.04,0.05}, follow-\ning [8]. These parameters are tuned on a development set consisting\nof about 10K V oice Search utterances.\nThe external recurrent LM is composed of 2 LSTM layers of\n2048 hidden units each. It has the same wordpiece output vocabulary\nas the LAS model.\n4.2.3. Training Details\nLAS models are trained in two stages. First, they are trained to con-\nvergence with a cross-entropy criterion using synchronous replica\ntraining [5]. We use tensor processing units (TPUs) [31] with a\ntopology of 8x8, for a total of 128 synchronous replicas and an ef-\nfective batch size of 4096. We found that having a very large batch\nsize was critical to seeing any improvement from cold fusion. Our\nlearning rate schedule includes an initial warm-up phase, a constant\nphase, and a decay, consistent with [5].\nNext, we conduct a second training phase with a minimum word\nerror rate (MWER) criterion [32]. This phase is performed on 16\nsynchronous GPU replicas to convergence, which is typically about\none epoch. Note that for deep fusion we effectively have four train-\ning phases: cross-entropy training of LAS, MWER training of LAS,\ncross-entropy training of deep fusion, MWER training of deep fu-\nsion.\nThe external LM is also trained on TPUs with a topology of\n4x4. All models are trained using the Adam optimizer [27] and are\nimplemented in TensorFlow [28].\n5. RESULTS\n5.1. Fusion Approaches\nTable 1: Word error rates (%) on Eval2000 for the baseline\nmodel and fusion approaches. SWB=Switchboard, CH=CallHome,\nFull=Eval2000.\nModel SWB CH Full\nLAS 17.1 27.9 22.6\nShallow Fusion 15.6 26.6 21.1\nDeep Fusion 16.3 27.2 21.7\nCold Fusion 16.3 27.3 21.8\nTable 1 shows the results of a baseline LAS model and the three\nfusion approaches on Switchboard and CallHome. All of the fusion\napproaches improve over the baseline model with a relative WER re-\nduction of 3-7% on Eval2000. Among the fusion approaches, shal-\nlow fusion is a clear winner with almost double the gains over base-\nline compared to deep and cold fusion. Finally, deep and cold fusion\nhave comparable performance on Switchboard.\nTable 2 shows the corresponding results for VS14K and D15K.\nAll of the fusion approaches improve performance over the baseline\nmodel for VS14K, but for D15K deep fusion suffers a minor degra-\ndation compared to baseline. As with Switchboard, on both of these\ndata sets shallow fusion is again the best performer, although it is\ntied with cold fusion on VS14K. Finally, deep fusion has no or neg-\nligible gain over baseline, suggesting that deep fusion does not scale\nwell with data.\n5\nTable 2: Word error rates (%) on Google voice search (VS14K)\nand dictation data sets (D15K) for the baseline model and fusion\napproaches.\nModel VS14K D15K\nLAS 5.6 4.0\nShallow Fusion 5.3 3.7\nDeep Fusion 5.5 4.1\nCold Fusion 5.3 3.9\n5.2. Proposed Approaches\nTable 3: Word error rates (%) on Eval2000 for the proposed ap-\nproaches.\nModel SWB CH Full\nLM multitask 17.0 27.5 22.3\nAdditional layer decoder\nRandom initialization 16.7 28.0 22.4\nLM pretrained 16.3 27.2 21.8\nNext we present results of our proposed approaches (Sec-\ntion 3.2.4 and 3.2.5) on Switchboard in Table 3 and compare them\nagainst the earlier Switchboard results from Table 1. The multi-\ntask learning approach achieves minor gains over the LAS baseline.\nWhile these gains are more modest than those of the three fusion\napproaches, it is important to note that unlike the fusion approaches,\nthe LM multitask approach introduces no new parameters.\nNext we evaluate the approach of introducing the LM as a lower\ndecoder layer (making the decoder two layers deep in the case of our\nSwitchboard models). To account for the confounding variable of a\ndeeper decoder, we also compare it to a version where a randomly\ninitialized RNN is introduced instead of the pretrained RNN LM.\nAs can be seen from the table, the performance of this approach is\ncomparable to that of deep and cold fusion. In addition, the marginal\ngains from introducing a randomly initialized RNN instead demon-\nstrate the beneﬁt of LM pretraining. The promising performance we\nsee here is consistent with the ﬁndings of [17] using a pretrained LM\nas the decoder, and this simple approaches of using a LM to initialize\nparts of the decoder warrants further investigation in future work.\n5.3. Second Pass Rescoring\nTable 4: Word error rates (%) for rescoring on Google data sets.\nModel VS14K (oracle) D15K (oracle)\nLAS 5.4 (2.2) 3.9 (1.5)\nShallow Fusion 5.3 (2.4) 3.7 (1.6)\nDeep Fusion 5.4 (2.0) 4.0 (1.5)\nCold Fusion 5.0 (1.8) 3.8 (1.2)\nWhile shallow and cold fusion have very similar top-1 WER on\nthe Google data sets, we can investigate the quality of the top-8 to\nbetter understand the strengths of each approach. For each of the\nfusion methods, Table 4 shows the WER after second pass rescoring\nwith a large, production-scale LM (as used in [5]), as well as the\noracle WER in parentheses.\nAs the table shows, cold fusion has signiﬁcantly better oracle\nWER on VS14K than the baseline and other fusion methods and,\nas a result, beneﬁts the most from a second pass LM. While shal-\nlow fusion is unaffected by the second pass, the cold fusion WER\ndrops from 5.3 to 5.0. This suggests that the improvements provided\nby shallow fusion are redundant with the beneﬁts of second pass\nrescoring, whereas cold fusion does something distinct, improving\nthe overall quality and diversity of the top 8 decoded transcripts.\nCold fusion also has the lowest oracle WER on D15K, but none\nof the models beneﬁt much from the second pass on this data set.\nThe lack of improvement is likely because the second pass LM is\nprimarily designed to improve performance on V oice Search. Shal-\nlow fusion therefore remains best on this data set.\nFinally, we note that shallow fusion actually has higher oracle\nWER than the baseline LAS system on both data sets. This may\nbe because shallow fusion can actually pull poor transcripts into the\nbeam if they are heavily favored by the LM.\n6. CONCLUSION\nWe perform a thorough investigation of the problem of LM integra-\ntion in encoder-decoder based ASR models. We compare some of\nthe most prominent past methods and a few of our own proposed\nmethods on the medium-scale and publicly available Switchboard\ndataset and the large-scale Google voice search and dictation data\nsets. Our results show that for ﬁrst-pass scoring, the simple approach\nof shallow fusion performs best on all of our data sets. However,\ncold fusion produces lower oracle error rates among the top-8 de-\ncoded transcripts, and outperforms shallow fusion after second pass\nrescoring on Google voice search. Deep fusion is comparable to cold\nfusion on Switchboard but gets no or negligible gains over the base-\nline on Google data sets, suggesting that it does not scale well with\ndata. Among our proposed methods, the simple approach of using a\npretrained language model as a lower layer of the decoder performs\ncomparably to cold and deep fusion on Switchboard, suggesting that\nfurther investigation of the approach may be fruitful.\n7. REFERENCES\n[1] William Chan, Navdeep Jaitly, Quoc V . Le, and Oriol Vinyals,\n“Listen, attend and spell: A neural network for large vocabu-\nlary conversational speech recognition,” inProc. IEEE Confer-\nence on Acoustics, Speech, and Signal Processing (ICASSP),\n2016.\n[2] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V . Le, Mo-\nhammad Norouzi, Wolfgang Macherey, . . . , and Jeffrey Dean,\n“Google’s Neural Machine Translation System: Bridging the\nGap between Human and Machine Translation,” CoRR, vol.\nabs/1609.08144, 2016.\n[3] Albert Zeyer, Kazuki Irie, Ralf Schl ¨uter, and Hermann Ney,\n“Improved training of end-to-end attention models for speech\nrecognition,” in Proc. Interspeech, 2018.\n[4] Jan Chorowski and Navdeep Jaitly, “Towards better decoding\nand language model integration in sequence to sequence mod-\nels,” in Proc. Interspeech, 2017.\n[5] Chung-Cheng Chiu, Tara Sainath, Yonghui Wu, Rohit Prab-\nhavalkar, Patrick Nguyen, Zhifeng Chen, Anjuli Kannan,\nRon J. Weiss, Kanishka Rao, Katya Gonina, Navdeep Jaitly,\nBo Li, Jan Chorowski, and Michiel Bacchiani, “State-of-the-\nart Speech Recognition with Sequence-to-Sequence Models,”\n6\nin Proc. IEEE Conference on Acoustics, Speech, and Signal\nProcessing (ICASSP), 2018.\n[6] C ¸ aglar G¨ulc ¸ehre, Orhan Firat, Kelvin Xu, Kyunghyun Cho,\nLo¨ıc Barrault, Huei-Chi Lin, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio, “On Using Monolingual Corpora in Neu-\nral Machine Translation,” CoRR, vol. abs/1503.03535, 2015.\n[7] Anuroop Sriram, Heewoo Jun, Sanjeev Satheesh, and Adam\nCoates, “Cold Fusion: Training Seq2seq Models Together with\nLanguage Models,” CoRR, vol. abs/1708.06426, 2017.\n[8] Anjuli Kannan, Yonnghui Wu, Patrick Nguyen, Tara N.\nSainath, Zhifeng Chen, and Rohit Prabhavalkar, “An Analysis\nof Incorporating an External Language Model into a Sequence-\nto-Sequence Model,” in Proc. IEEE Conference on Acoustics,\nSpeech, and Signal Processing (ICASSP), 2018.\n[9] John J. Godfrey, Edward C. Holliman, and Jane McDaniel,\n“SWITCHBOARD: Telephone speech corpus for research and\ndevelopment,” inProc. IEEE Conference on Acoustics, Speech,\nand Signal Processing (ICASSP), 1992.\n[10] Prajit Ramachandran, Peter J. Liu, and Quoc V . Le, “Unsu-\npervised Pretraining for Sequence to Sequence Learning,” in\nProc. Conference on Empirical Methods in Natural Language\nProcessing (EMNLP), 2017.\n[11] Eric Battenberg, Jitong Chen, Rewon Child, Adam Coates,\nYashesh Gaur Yi Li, Hairong Liu, Sanjeev Satheesh, Anuroop\nSriram, and Zhenyao Zhu, “Exploring neural transducers for\nend-to-end speech recognition,” in Proc. IEEE Workshop on\nAutomatic Speech Recognition and Understanding (ASRU),\n2017.\n[12] Rico Sennrich, Barry Haddow, and Alexandra Birch, “Im-\nproving Neural Machine Translation Models with Monolin-\ngual Data,” inProc. Association for Computational Linguistics\n(ACL), 2016.\n[13] Andros Tjandra, Sakriani Sakti, and Satoshi Nakamura, “Lis-\ntening while Speaking: Speech Chain by Deep Learning,” in\nProc. IEEE Workshop on Automatic Speech Recognition and\nUnderstanding (ASRU), 2017.\n[14] Adithya Renduchintala, Shuoyang Ding, Matthew Wiesner,\nand Shinji Watanabe, “Multi-Modal Data Augmentation for\nEnd-to-End ASR,” in Proc. Interspeech, 2018.\n[15] Rico Sennrich, Barry Haddow, and Alexandra Birch, “Neural\nMachine Translation of Rare Words with Subword Units,” in\nProc. Association for Computational Linguistics (ACL), 2016.\n[16] M. Schuster and K.K. Paliwal, “Bidirectional recurrent neural\nnetworks,” Trans. Sig. Proc., vol. 45, no. 11, Nov. 1997.\n[17] Kanishka Rao, Hasim Sak, and Rohit Prabhavalkar, “Explor-\ning Architectures, Data and Units For Streaming End-to-End\nSpeech Recognition with RNN-Transducer,” in Proc. IEEE\nWorkshop on Automatic Speech Recognition and Understand-\ning (ASRU), 2017.\n[18] Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gard-\nner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer,\n“Deep contextualized word representations,” in Proc. Con-\nference of the North American Chapter of the Association for\nComputational Linguistics – Human Language Technologies\n(NAACL HLT), 2018.\n[19] Yajie Miao, Mohammad Gowayyed, and Florian Metze,\n“EESEN: End-to-end speech recognition using deep RNN\nmodels and WFST-based decoding.,” in Proc. IEEE Workshop\non Automatic Speech Recognition and Understanding (ASRU),\n2015.\n[20] Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Burget,\nOndrej Glembek, Nagendra Goel, . . . , and Karel Vesely, “The\nKaldi Speech Recognition Toolkit,” in Proc. IEEE Workshop\non Automatic Speech Recognition and Understanding (ASRU),\n2011.\n[21] Christopher Cieri David, David Miller, and Kevin Walker,\n“The Fisher Corpus: a Resource for the Next Generations of\nSpeech-to-Text,” in Proc. International Conference on Lan-\nguage Resources and Evaluation (LREC), 2004.\n[22] Sepp Hochreiter and J ¨urgen Schmidhuber, “Long short-term\nmemory,” Neural Computation, vol. 9, 1997.\n[23] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon\nShlens, and Zbigniew Wojna, “Rethinking the Inception Archi-\ntecture for Computer Vision,” inProc. IEEE Computer Society\nConf. Computer Vision and Pattern Recognition (CVPR), 2016.\n[24] V . Pham, T. Bluche, C. Kermorvant, and J. Louradour,\n“Dropout improves recurrent neural networks for handwriting\nrecognition,” in Proc. International Conference on Frontiers\nin Handwriting Recognition (ICFHR), 2014.\n[25] Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam\nShazeer, “Scheduled sampling for sequence prediction with\nrecurrent neural networks,” in Proc. Neural Information Pro-\ncessing Systems (NIPS), 2015.\n[26] Kartik Audhkhasi, Brian Kingsbury, Bhuvana Ramabhadran,\nGeorge Saon, and Michael Picheny, “Building competitive\ndirect acoustics-to-word models for English conversational\nspeech recognition,” in Proc. IEEE Conference on Acoustics,\nSpeech, and Signal Processing (ICASSP), 2018.\n[27] Diederik P. Kingma and Jimmy Ba, “Adam: A Method for\nStochastic Optimization,” in Proc. International Conference\non Learning Representations (ICLR), 2015.\n[28] Mart ´ın Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo,\nZhifeng Chen, Craig Citro, . . . , and Xiaoqiang Zheng, “Ten-\nsorFlow: Large-Scale machine learning on heterogeneous sys-\ntems,” 2015.\n[29] Hasim Sak, Andrew Senior, and Francoise Beaufays, “Fast and\naccurate recurrent neural network acoustic models for speech\nrecognition,” in Proc. Interspeech, 2015.\n[30] Golan Pundak and Tara N. Sainath, “Lower frame rate neural\nnetwork acoustic models,” in Proc. Interspeech, 2016.\n[31] Norman P. Jouppi, Cliff Young, Nishant Patil, David Patterson,\nGaurav Agrawal, Raminder Bajwa, . . . , and Doe Hyun Yoon,\n“In-Datacenter Performance Analysis of a Tensor Processing\nUnit,” in Proc. International Symposium on Computer Archi-\ntecture, 2017.\n[32] R. Prabhavalkar, T. N. Sainath, Y . Wu, P. Nguyen, Z. Chen,\nC. Chiu, and A. Kannan, “Minimum Word Error Rate Train-\ning for Attention-based Sequence-to-sequence Models,” in\nProc. IEEE Conference on Acoustics, Speech, and Signal Pro-\ncessing (ICASSP), 2018.\n7"
}