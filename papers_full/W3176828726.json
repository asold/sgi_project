{
    "title": "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models",
    "url": "https://openalex.org/W3176828726",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A3176106159",
            "name": "Elad Ben Zaken",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2144962531",
            "name": "Yoav Goldberg",
            "affiliations": [
                "Bar-Ilan University",
                "Laboratoire d'Informatique de Paris-Nord",
                "Allen Institute for Artificial Intelligence"
            ]
        },
        {
            "id": "https://openalex.org/A2889907260",
            "name": "Shauli Ravfogel",
            "affiliations": [
                "Laboratoire d'Informatique de Paris-Nord",
                "Allen Institute for Artificial Intelligence",
                "Bar-Ilan University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2768282280",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W1840435438",
        "https://openalex.org/W3103754749",
        "https://openalex.org/W3119980997",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W2130158090",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3038287124",
        "https://openalex.org/W2964303773",
        "https://openalex.org/W131533222",
        "https://openalex.org/W3034199299",
        "https://openalex.org/W2914103174",
        "https://openalex.org/W2300770211",
        "https://openalex.org/W3104263050",
        "https://openalex.org/W3104223418",
        "https://openalex.org/W2963287528",
        "https://openalex.org/W2097533491",
        "https://openalex.org/W3011411500",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W3007700590",
        "https://openalex.org/W3038012435",
        "https://openalex.org/W4297797729",
        "https://openalex.org/W2707890836",
        "https://openalex.org/W2396813586",
        "https://openalex.org/W3113057009",
        "https://openalex.org/W2963913356",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W4313908941",
        "https://openalex.org/W2805003733",
        "https://openalex.org/W1821462560",
        "https://openalex.org/W2145085734",
        "https://openalex.org/W2963674932",
        "https://openalex.org/W3174702398",
        "https://openalex.org/W2978670439",
        "https://openalex.org/W2059174629",
        "https://openalex.org/W607505555",
        "https://openalex.org/W3020268419",
        "https://openalex.org/W2294370754",
        "https://openalex.org/W3044629537",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2979826702"
    ],
    "abstract": "We introduce BitFit, a sparse-finetuning method where only the bias-terms of the model (or a subset of them) are being modified. We show that with small-to-medium training data, applying BitFit on pre-trained BERT models is competitive with (and sometimes better than) fine-tuning the entire model. For larger data, the method is competitive with other sparse fine-tuning methods.Besides their practical utility, these findings are relevant for the question of understanding the commonly-used process of finetuning: they support the hypothesis that finetuning is mainly about exposing knowledge induced by language-modeling training, rather than learning new task-specific linguistic knowledge.",
    "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 2: Short Papers, pages 1 - 9\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nBitFit: Simple Parameter-efficient Fine-tuning\nfor Transformer-based Masked Language-models\nElad Ben-Zaken1 Shauli Ravfogel1,2 Yoav Goldberg1,2\n1Computer Science Department, Bar Ilan University\n2Allen Institute for Artificial Intelligence\n{benzakenelad, shauli.ravfogel, yoav.goldberg}@gmail.com\nAbstract\nWe introduce BitFit, a sparse-finetuning\nmethod where only the bias-terms of the model\n(or a subset of them) are being modified. We\nshow that with small-to-medium training data,\napplying BitFit on pre-trained BERT models is\ncompetitive with (and sometimes better than)\nfine-tuning the entire model. For larger data,\nthe method is competitive with other sparse\nfine-tuning methods. Besides their practical\nutility, these findings are relevant for the ques-\ntion of understanding the commonly-used pro-\ncess of finetuning: they support the hypothesis\nthat finetuning is mainly about exposing knowl-\nedge induced by language-modeling training,\nrather than learning new task-specific linguistic\nknowledge.\n1 Introduction\nLarge pre-trained transformer based language mod-\nels, and in particular bidirectional masked language\nmodels from the BERT family (Devlin et al., 2018;\nLiu et al., 2019; Joshi et al., 2019), are responsible\nfor significant gains in many NLP tasks. Under\nthe common paradigm, the model is pre-trained\non large, annotated corpora with the LM objec-\ntive, and then finetuned on task-specific supervised\ndata. The large size of these models make them\nexpensive to train and, more importantly, expensive\nto deploy. This, along with theoretical questions\non the extent to which finetuning must change the\noriginal model, has led researchers to consider fine-\ntuning variants where one identifies a small subset\nof the model parameters which need to be changed\nfor good performance in end-tasks, while keeping\nall others intact (§2).\nWe present a simple and effective approach to\nfine tuning (§3), which has the following benefits:\n1. Changing very few parameters per fine-tuned\ntask.\n2. Changing the same set of parameters for every\ntasks (task-invariance).\n3. The changed parameters are both isolated and\nlocalized across the entire parameter space.\n4. For small to medium training data, changing\nonly these parameters reaches the same task\naccuracy as full fine-tuning, and sometimes\neven improves results.\nSpecifically, we show that freezing most of the\nnetwork and fine-tuning only the bias-terms is\nsurprisingly effective. Moreover, if we allow the\ntasks to suffer a small degradation in performance,\nwe can fine-tune only two bias components (the\n“query” and “middle-of-MLP” bias terms), amount-\ning to half of the bias parameters in the model, and\nonly 0.04% of all model parameters.\nThis result has a large practical utility in de-\nploying multi-task fine-tuned models in memory-\nconstrained environments, as well as opens the way\nto trainable hardware implementations in which\nmost of the parameters are fixed. Additionally, it\nopens up a set of research directions regarding the\nrole of bias terms in pre-trained networks, and the\ndynamics of the fine-tuning process.\n2 Background: fine-tuning and\nparameter-efficient fine-tuning\nIn transfer-learning via model fine-tuning, a pre-\ntrained encoder network takes the input and pro-\nduces contextualized representations. Then, a task-\nspecific classification layer (here we consider linear\nclassifiers) is added on top of the encoder, and the\nentire network (encoder+task specific classifiers) is\ntrained end-to-end to minimize the task loss.\nDesired properties. While fine-tuning per-task\nis very effective, it also results in a unique, large\nmodel for each pre-trained task, making it hard to\nreason about what was changed in the fine-tuning\nprocess, as well as hard to deploy, especially as the\nnumber of tasks increases. Ideally, one would want\na fine-tuning method that:\n(i) matches the results of a fully fine-tuned model;\n1\n(ii) changes only a small portion of the model’s\nparameters; and (iii) enables tasks to arrive in a\nstream, instead of requiring simultaneous access to\nall datasets. For efficient hardware based deploy-\nments, it is further preferred that (iv): the set of\nparameters that change values is consistent across\ndifferent tasks.\nLearning vs. Exposing. The feasibility of fulfill-\ning the above requirements depends on a fundamen-\ntal question regarding the nature of the fine-tuning\nprocess of large pre-trained LMs: to what extent\ndoes the fine-tuning process induces thelearning of\nnew capabilities, vs. the exposing of existing capa-\nbilities, which were learned during the pre-training\nprocess.\nExisting approaches. Two recent works have\ndemonstrated that adaptation to various end-tasks\ncan in fact be achieved by changing only a small\nsubset of parameters. The first work, by Houlsby\net al. (2019) (“Adapters”), achieves this goal by in-\njecting small, trainable task-specific “adapter” mod-\nules between the layers of the pre-trained model,\nwhere the original parameters are shared between\ntasks. The second work, by Guo et al. (2020)\n(“Diff-Pruning”), achieves the same goal by adding\na sparse, task-specific difference-vector to the orig-\ninal parameters, which remain fixed and are shared\nbetween tasks. The difference-vector is regular-\nized to be sparse. Both methods allow adding only\na small number of trainable parameters per-task\n(criteria ii), and each task can be added without\nrevisiting previous ones (criteria iii).\nThey also partially fulfill criteria (i), suffering\nonly a small drop in performance compared to full\nfine-tuning. The Adapter method, but not the Diff-\nPruning method, also supports criteria (iv). How-\never, Diff-Pruning is more parameter efficient than\nthe Adapter method (in particular, it adds no new\nparameters), and also achieves better task scores.\nWe compare against Diff-Pruning and Adapters in\nthe experiments section, and show that we perform\nfavorably on many tasks while also satisfying crite-\nria (iv).\n3 Bias-terms Fine-tuning (BitFit)\nWe propose a method we call BitFit1 (BIas-Term\nFIne-Tuning), in which we freeze most of the\ntransformer-encoder parameters, and train only the\nbias-terms and the task-specific classification layer.\n1Our code is publicly available at www.github.com/\nbenzakenelad/BitFit\nBitFit has three key properties: (i) match the re-\nsults of fully fine-tuned model. (ii) enable tasks\nto arrive in a stream, this way it does not require\nsimultaneous access to all datasets. (iii) fine-tune\nonly a small portion of the model’s parameters.\nThe approach is parameter-efficient: each new\ntask requires storing only the bias terms parameter\nvectors (which amount to less than 0.1% of the\ntotal number of parameters), and the task-specific\nfinal linear classifier layer.\nConcretely, the BERT encoder is composed of\nL layers, where each layer ℓ starts with M self-\nattention heads, where a self attention head (m, ℓ)\nhas key, query and value encoders, each taking the\nform of a linear layer:\nQm,ℓ(x) = Wm,ℓ\nq x + bm,ℓ\nq\nKm,ℓ(x) = Wm,ℓ\nk x + bm,ℓ\nk\nVm,ℓ(x) = Wm,ℓ\nv x + bm,ℓ\nv\nWhere x is the output of the former encoder layer\n(for the first encoder layer x is the output of the\nembedding layer). These are then combined using\nan attention mechanism that does not involve new\nparameters:\nhℓ\n1 = att\n\u0000\nQ1,ℓ, K1,ℓ, V1,ℓ, ..,Qm,ℓ, Km,ℓ, Vm,l\u0001\nand then fed to an MLP with layer-norm (LN):\nhℓ\n2 = Dropout\n\u0000\nWℓ\nm1 · hℓ\n1 + bℓ\nm1\n\u0001\n(1)\nhℓ\n3 = gℓ\nLN1 ⊙ (hℓ\n2 + x) − µ\nσ + bℓ\nLN1 (2)\nhℓ\n4 = GELU\n\u0000\nWℓ\nm2 · hℓ\n3 + bℓ\nm2\n\u0001\n(3)\nhℓ\n5 = Dropout\n\u0000\nWℓ\nm3 · hℓ\n4 + bℓ\nm3\n\u0001\n(4)\noutℓ = gℓ\nLN2 ⊙ (hℓ\n5 + hℓ\n3) − µ\nσ + bℓ\nLN2 (5)\nThe collection of all matrices Wℓ,(·)\n(·) and vectors\ngℓ\n(·), bℓ,(·)\n(·) , indicated in blue and purple are the net-\nwork’s parameters Θ, where the subset of purple\nvectors bℓ,(·)\n(·) are the bias terms.2\nThe bias terms are additive, and correspond to a\nvery small fraction of the network, in BERTBASE\nand BERTLARGE bias parameters make up 0.09%\nand 0.08% of the total number of parameters in\neach model, respectively.\nWe show that by freezing all the parameters\nW(·) and g(·) and fine-tuning only the additive\n2In Appendix §A.1 we relate this notation with parameter\nnames in HuggingFace implementation.\n2\n%Param QNLI SST-2 MNLIm MNLImm CoLA MRPC STS-B RTE QQP Avg.\nTrain size 105k 67k 393k 393k 8.5k 3.7k 7k 2.5k 364k\n(V) Full-FT† 100% 93.5 94.1 86.5 87.1 62.8 91.9 89.8 71.8 87.6 84.8\n(V) Full-FT 100% 91.7 ±0.1 93.4±0.2 85.5±0.4 85.7±0.4 62.2±1.2 90.7±0.3 90.0±0.4 71.9±1.3 87.5±0.4 84.1\n(V) Diff-Prune† 0.5% 93.4 94.2 86.4 86.9 63.5 91.3 89.5 71.5 86.6 84.6\n(V) BitFit 0.08% 91.4 ±2.4 93.2±0.4 84.4±0.2 84.8±0.1 63.6±0.7 91.7±0.5 90.3±0.1 73.2±3.7 85.4±0.1 84.2\n(T) Full-FT‡ 100% 91.1 94.1 86.7 86.0 59.6 88.9 86.6 71.2 71.7 81.2\n(T) Full-FT† 100% 93.4 94.9 86.7 85.9 60.5 89.3 87.6 70.1 72.1 81.8\n(T) Adapters‡ 3.6% 90.7 94.0 84.9 85.1 59.5 89.5 86.9 71.5 71.8 81.1\n(T) Diff-Prune† 0.5% 93.3 94.1 86.4 86.0 61.1 89.7 86.0 70.6 71.1 81.5\n(T) BitFit 0.08% 92.0 94.2 84.5 84.8 59.7 88.9 85.5 72.0 70.5 80.9\nTable 1: BERT LARGE model performance on the GLUE benchmark validation set (V) and test set (T). Lines with †\nand ‡ indicate results taken from Guo et al. (2020) and Houlsby et al. (2019) (respectively).\nbias terms b(·), we achieve transfer learning perfor-\nmance which is comparable (and sometimes bet-\nter!) than fine-tuning of the entire network,\nWe also show that we can fine-tune only a subset\nof the bias parameters, namely those associated\nwith the query and the second MLP layer (only\nb(·)\nq and b(·)\nm2 ), and still achieve accuracies that\nrival full-model fine-tuning.\n4 Experiments and Results\nDatasets. We evaluate BitFit on the GLUE bench-\nmark (Wang et al., 2018).3 Consistent with previ-\nous work (Houlsby et al., 2019; Guo et al., 2020)\nwe exclude the WNLI task, on which BERT models\ndo not outperform the majority baseline.\nModels and Optimization. We use the publicly\navailable pre-trained BERTBASE, BERTLARGE (De-\nvlin et al., 2018) and RoBERTa BASE (Liu et al.,\n2019) models, using the HuggingFace (Wolf et al.,\n2020) interface and implementation.\nAppendix §A.2 lists optimization details.\nComparison to Diff-Pruning and Adapters (Ta-\nble 1) In the first experiment, we compare Bit-\nFit to Diff-Pruning method and Adapters method,\nwhen using a fewer number of parameters. Table 1\nreports the dev-set and test-set performance com-\npared to the Diff-Pruning and Adapters numbers\nreported by Guo et al. (2020) and Houlsby et al.\n(2019) (respectively). This experiment used the\nBERTLARGE model.\nOn validation set, BitFit outperforms Diff-\nPruning on 4 out of 9 tasks, while using 6x fewer\ntrainable parameters 4. As for test-set results, two\nclear wins compared to Diff-Pruning and 4 clear\nwins compared to Adapters while using 45x fewer\ntrainable parameters.\n3Appendix §A.3 lists the tasks and evaluation metrics.\n4QNLI results are not directly comparable, as the GLUE\nbenchmark updated the test set since then.\nFigure 1: Change in bias components (RTE task).\nDifferent Base-models (Table 2) We repeat\nthe BERTLARGE results on different base-models\n(the smaller BERTBASE and the better performing\nRoBERTaBASE). The results in Table 2 show that\nthe trends remain consistent.\nAre bias parameters special? Are the bias pa-\nrameters special, or will any random subset do? We\nrandomly sampled the same amount of parameters\nas in BitFit from the entire model, and fine-tuned\nonly them (“rand uniform” line in Table 3). The\nresults are substantially worse across all tasks; sim-\nilar patterns are observed when the random param-\neters are sampled as complete rows/columns in the\nparameter matrices (“rand row/col” line in Table\n3).\nFewer bias parameters (Table 3) Can we fine-\ntune on only a subset of the bias-parameter?\nWe define the amount of change in a bias vector\nb to be 1\ndim(b) ∥b0 − bF ∥1, that is, the average\nabsolute change, across its dimensions, between the\ninitial LM values b0 and its fine-tuned values bF .\nFigure 1 shows the change per bias term and layer,\nfor the RTE task (other tasks look very similar,\nsee Appendix §A.4). The ‘key’ bias bk has zero\n3\nMethod %Param QNLI SST-2 MNLIm MNLImm CoLA MRPC STS-B RTE QQP Avg.\nBB Full-FT 100% 90.7±0.2 92.0±0.4 83.5±0.1 83.7±0.3 56.4±0.9 89.0±1.0 88.9±0.7 70.5±0.6 87.1±0.1 82.3\nBB BitFit 0.09% 90.2 ±0.2 92.1±0.3 81.4±0.2 82.2±0.2 58.8±0.5 90.4±0.5 89.2±0.2 72.3±0.9 84.0±0.2 82.4\nBL Full-FT 100% 91.7±0.1 93.4±0.2 85.5±0.4 85.7±0.4 62.2±1.2 90.7±0.3 90.0±0.4 71.9±1.3 87.5±0.4 84.1\nBL BitFit 0.08% 91.4 ±2.4 93.2±0.4 84.4±0.2 84.8±0.1 63.6±0.7 91.7±0.5 90.3±0.1 73.2±3.7 85.4±0.1 84.2\nRo Full-FT 100% 92.3±0.2 94.2±0.4 86.4±0.3 86.9±0.3 61.1±0.8 92.5±0.4 90.6±0.2 77.4±1.0 88.0±0.2 85.3\nRo BitFit 0.09% 91.3 ±0.2 93.7±0.1 84.8±0.1 85.2±0.2 61.8±1.3 92.0±0.4 90.8±0.3 77.8±1.7 84.5±0.2 84.6\nTable 2: Dev-set results for different base models. BB: BERTBASE. BL: BERTLARGE. Ro: RoBERTaBASE.\n% Param QNLI SST-2 MNLIm MNLImm CoLA MRPC STS-B RTE QQP Avg.\nFull-FT 100% 90.7±0.2 92.0±0.4 83.5±0.1 83.7±0.3 56.4±0.9 89.0±1.0 88.9±0.7 70.5±0.6 87.1±0.1 82.3\nBitFit 0.09% 90.2 ±0.2 92.1±0.3 81.4±0.2 82.2±0.2 58.8±0.5 90.4±0.5 89.2±0.2 72.3±0.9 84.0±0.2 82.4\nbm2,bq 0.04% 89.4±0.1 91.2±0.2 80.4±0.2 81.5±0.2 57.4±0.8 89.0±0.2 88.4±0.1 68.6±0.6 83.7±0.2 81.1\nbm2 0.03% 88.9±0.1 91.1±0.3 79.9±0.3 80.7±0.2 54.9±0.9 87.9±0.6 88.2±0.1 66.8±0.6 82.1±0.4 80.0\nbq 0.01% 86.8±0.1 89.6±0.2 74.4±0.3 75.7±0.2 49.1±1.5 84.4±0.2 85.6±0.1 61.4±1.1 80.6±0.4 76.6\nFrozen 0.0% 68.7 ±0.3 81.7±0.1 42.4±0.1 43.8±0.1 31.9±1.1 81.1±0.1 71.4±0.1 56.9±0.4 62.4±0.2 62.1\nrand uniform 0.09% 87.8±0.3 90.5±0.3 78.3±0.3 78.8±0.2 54.1±1.0 84.3±0.3 87.2±0.4 62.9±0.9 82.4±0.3 78.5\nrand row/col 0.09% 88.4±0.2 91.0±0.3 79.4±0.3 80.1±0.3 53.4±0.6 88.0±0.7 87.9±0.2 65.1±0.7 82.3±0.2 79.5\nTable 3: Fine-tuning using a subset of the bias parameters. Reported results are for the BERT BASE model.\nchange, consistent with the theoretical observation\nin Cordonnier et al. (2020). In contrast, bq, the bias\nof the queries, andbm2, the bias of the intermediate\nMLP layers (which take the input from 768-dims\nto 3072), change the most. Table 3 reports dev-\nset results when fine-tuning only the b(·)\nq and b(·)\nm2\nbias terms, for the BERTBASE model. Results are\nonly marginally lower than when tuning all bias\nparameters. Tuning either b(·)\nq or b(·)\nm2 alone yields\nsubstantially worse results, indicating both bias\ntypes are essential. As expected, using a frozen\nBERTBASE model yields much worse results.\nGeneralization gap. While in most cases full\nfine-tuning reaches nearly 100% train accuracy, we\nfind that the generalization gap (Shalev-Shwartz\nand Ben-David, 2014)—the difference between\ntraining error and test error—is substantially\nsmaller for the BitFit models.\nToken-level tasks. The GLUE tasks are all sen-\ntence level. We also experimented with token-level\nPTB POS-tagging. Full-FT results for BERTBASE,\nBERTLARGE and RoBERTaBASE are 97.2, 97.4,\n97.2, while BitFit results are 97.2, 97.4, 97.1.\nSize of training data. The GLUE results suggest\na reverse correlation between BitFit ability to reach\nFull-FT performance, and training set size. To test\nthis (and to validate another token-level task), we\ntrain on increasing-sized subsets of SQuAD v1.0\nRajpurkar et al. (2016a). The results on Figure\n2 show a clear trend: BitFit dominates over Full-\nFT in the smaller-data regime, while the trend is\nreversed when more training data is available. We\nFigure 2: Comparison of BitFit and Full-FT with\nBERTBASE exact match score on SQuAD validation set.\nconclude that BitFit is a worthwhile targetted fine-\ntuning method in small-to-medium data regimes.\n5 Related Work\nThe problem of identifying the minimal set of pa-\nrameters that need to be fine-tuned to achieve good\nperformance in end-tasks relates both to practi-\ncal questions of model compression, and also to\nmore fundamental question on the nature of the\npre-training and finetuning process, the “linguis-\ntic knowledge“ induced by each of them, and the\nextent to which it generalizes to different tasks.\nOver-parameterization Large LM models were\nshown to be over-parameterized: they contain\nmore parameters than needed in inference (Buciluˇa\net al., 2006; Hinton et al., 2015; Urban et al., 2017;\nKarnin, 1990; Reed, 1993; Augasta and Kathir-\nvalavakumar, 2013; Liu et al., 2014; Han et al.,\n2015; Molchanov et al., 2017). Gordon et al. (2020)\nhave demonstrated that overparmeterization can be\nexploited in finetuning: pruned network perform\n4\nwell in transfer setting. We work in a complemen-\ntary setting, where the entire model is kept, but\nonly some parameters are updated. The remarkable\nsuccess of those works have sparked interest the\nlottery-ticket hypothesis (Frankle and Carbin, 2019;\nChen et al., 2020; Prasanna et al., 2020): the con-\njecture that large models are needed in pretraining\nonly to induce (in high probability) the existing of\nsub-networks initialized with the correct inductive\nbias for learning, and the findings that those sparse\nnetworks often transfer well to different tasks.\nBias terms Bias terms and their importance\nare rarely discussed in the literature. 5 Zhao\net al. (2020) describe a masking-based fine-tuning\nmethod, and explicitly mention ignoring the bias\nterms, as handling them “did not observe a positive\neffect on performance”.\nAn exception is the work of Wang et al. (2019)\nwho analyzed bias terms from the perspective of\nattribution method. They demonstrate that the\nlast layer bias values are responsible for the pre-\ndicted class, and propose a way to back-propagate\ntheir importance. Michel and Neubig (2018) fine-\ntuned the biases of the output softmax in an NMT\nsystems, to personalize the output vocabulary,\nand Frankle et al. (2020) have demonstrated that\nrandomly-initialized CNNs achieve reasonable ac-\ncuracy after training the batch-norm layers alone.\nFinally, and closest to our work, Cai et al. (2020)\ndemonstrate that bias-only fine-tuning similar to\nours is effective also for adaptation of pre-trained\ncomputer vision models. Our work empirically\nshows the importance and power of the bias param-\neters to substantially change the networks’ behav-\nior, calling for further analysis and attention on the\nbias terms.\n6 Conclusions\nWe propose BitFit, a novel method for localized,\nfast fine-tuning of pre-trained transformers for end-\ntasks. The method focuses the finetuning on a spe-\ncific fraction of the model parameters—the biases—\nand maintains good performance in all GLUE tasks\nwe evaluated on. The focus on modifying a small\ngroup of parameters eases deployment, as the vast\nmajority of the parameters of the model are shared\nbetween various NLP tasks. It also allows for ef-\nficient hardware implementations that hard-wire\n5Indeed, the equations in the paper introducing the Trans-\nformer model (Vaswani et al., 2017) do not include bias terms\nat all, and their existence in the BERT models might as well\nbe a fortunate mistake.\nmost of the network computation with the pre-\ntrained weights, while only allowing few change-\nable parts for inference time.\nBesides its empirical utility, the remarkable ef-\nfectiveness of bias-only fine-tuning raises intrigu-\ning questions on the fine-tuning dynamics of pre-\ntrained transformers, and the relation between the\nbias terms and transfer between LM and new tasks.\nAcknowledgments\nThis project has received funding from the Euro-\npean Research Council (ERC) under the European\nUnion’s Horizon 2020 research and innovation\nprogramme, grant agreement No. 802774 (iEX-\nTRACT).\nReferences\nM. Gethsiyal Augasta and T. Kathirvalavakumar. 2013.\nPruning algorithms of neural networks - a compar-\native study. Central Eur. J. Comput. Sci., 3(3):105–\n115.\nSamuel R Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D Manning. 2015. A large annotated\ncorpus for learning natural language inference. arXiv\npreprint arXiv:1508.05326.\nCristian Bucilu ˇa, Rich Caruana, and Alexandru\nNiculescu-Mizil. 2006. Model compression. In Pro-\nceedings of the 12th ACM SIGKDD international\nconference on Knowledge discovery and data mining,\npages 535–541.\nHan Cai, Chuang Gan, Ligeng Zhu, and Song Han. 2020.\nTiny transfer learning: Towards memory-efficient on-\ndevice learning. CoRR, abs/2007.11622.\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017. Semeval-2017\ntask 1: Semantic textual similarity-multilingual and\ncross-lingual focused evaluation. arXiv preprint\narXiv:1708.00055.\nTianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia\nLiu, Yang Zhang, Zhangyang Wang, and Michael\nCarbin. 2020. The lottery ticket hypothesis for pre-\ntrained BERT networks. In Advances in Neural In-\nformation Processing Systems 33: Annual Confer-\nence on Neural Information Processing Systems 2020,\nNeurIPS 2020, December 6-12, 2020, virtual.\nJean-Baptiste Cordonnier, Andreas Loukas, and Mar-\ntin Jaggi. 2020. Multi-head attention: Collaborate\ninstead of concatenate. CoRR, abs/2006.16362.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2005. The pascal recognising textual entailment chal-\nlenge. In Machine Learning Challenges Workshop,\npages 177–190. Springer.\n5\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. CoRR, abs/1810.04805.\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\nJonathan Frankle and Michael Carbin. 2019. The lottery\nticket hypothesis: Finding sparse, trainable neural\nnetworks. In 7th International Conference on Learn-\ning Representations, ICLR 2019, New Orleans, LA,\nUSA, May 6-9, 2019. OpenReview.net.\nJonathan Frankle, David J. Schwab, and Ari S. Mor-\ncos. 2020. Training batchnorm and only batchnorm:\nOn the expressive power of random features in cnns.\nCoRR, abs/2003.00152.\nMitchell A. Gordon, Kevin Duh, and Nicholas An-\ndrews. 2020. Compressing BERT: studying the ef-\nfects of weight pruning on transfer learning. CoRR,\nabs/2002.08307.\nDemi Guo, Alexander M. Rush, and Yoon Kim. 2020.\nParameter-efficient transfer learning with diff prun-\ning.\nSong Han, Jeff Pool, John Tran, and William Dally.\n2015. Learning both weights and connections for\nefficient neural network. Advances in neural infor-\nmation processing systems, 28:1135–1143.\nGeoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean.\n2015. Distilling the knowledge in a neural network.\nCoRR, abs/1503.02531.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin de Laroussilhe, Andrea Ges-\nmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for NLP. CoRR,\nabs/1902.00751.\nShankar Iyer, Nikhil Dandekar, and Kornel Csernai.\n2017. First quora dataset release: Question pairs.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld,\nLuke Zettlemoyer, and Omer Levy. 2019. Spanbert:\nImproving pre-training by representing and predict-\ning spans. CoRR, abs/1907.10529.\nEhud D. Karnin. 1990. A simple procedure for prun-\ning back-propagation trained neural networks. IEEE\nTrans. Neural Networks, 1(2):239–242.\nChao Liu, Zhiyong Zhang, and Dong Wang. 2014. Prun-\ning deep neural networks by optimal brain damage.\nIn INTERSPEECH 2014, 15th Annual Conference\nof the International Speech Communication Asso-\nciation, Singapore, September 14-18, 2014 , pages\n1092–1095. ISCA.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nIlya Loshchilov and Frank Hutter. 2017. Fixing\nweight decay regularization in adam. CoRR,\nabs/1711.05101.\nPaul Michel and Graham Neubig. 2018. Extreme adap-\ntation for personalized neural machine translation. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics, ACL 2018,\nMelbourne, Australia, July 15-20, 2018, Volume 2:\nShort Papers, pages 312–318. Association for Com-\nputational Linguistics.\nPavlo Molchanov, Stephen Tyree, Tero Karras, Timo\nAila, and Jan Kautz. 2017. Pruning convolutional\nneural networks for resource efficient inference. In\n5th International Conference on Learning Repre-\nsentations, ICLR 2017, Toulon, France, April 24-\n26, 2017, Conference Track Proceedings. OpenRe-\nview.net.\nMarius Mosbach, Maksym Andriushchenko, and Diet-\nrich Klakow. 2020. On the stability of fine-tuning\nbert: Misconceptions, explanations, and strong base-\nlines.\nSai Prasanna, Anna Rogers, and Anna Rumshisky. 2020.\nWhen BERT plays the lottery, all tickets are winning.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2020, Online, November 16-20, 2020, pages 3208–\n3229. Association for Computational Linguistics.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev,\nand Percy Liang. 2016a. Squad: 100, 000+ ques-\ntions for machine comprehension of text. CoRR,\nabs/1606.05250.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016b. Squad: 100,000+ questions\nfor machine comprehension of text. arXiv preprint\narXiv:1606.05250.\nRussell Reed. 1993. Pruning algorithms-a survey. IEEE\nTrans. Neural Networks, 4(5):740–747.\nShai Shalev-Shwartz and Shai Ben-David. 2014. Un-\nderstanding machine learning: From theory to algo-\nrithms. Cambridge university press.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 conference on empiri-\ncal methods in natural language processing, pages\n1631–1642.\nGregor Urban, Krzysztof J. Geras, Samira Ebrahimi\nKahou, ¨Ozlem Aslan, Shengjie Wang, Abdelrahman\nMohamed, Matthai Philipose, Matthew Richardson,\n6\nand Rich Caruana. 2017. Do deep convolutional nets\nreally need to be deep and convolutional? In 5th\nInternational Conference on Learning Representa-\ntions, ICLR 2017, Toulon, France, April 24-26, 2017,\nConference Track Proceedings. OpenReview.net.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. CoRR, abs/1706.03762.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. CoRR,\nabs/1804.07461.\nShengjie Wang, Tianyi Zhou, and Jeff A. Bilmes. 2019.\nBias also matters: Bias attribution for deep neural\nnetwork explanation. In Proceedings of the 36th In-\nternational Conference on Machine Learning, ICML\n2019, 9-15 June 2019, Long Beach, California, USA,\nvolume 97 of Proceedings of Machine Learning Re-\nsearch, pages 6659–6667. PMLR.\nAlex Warstadt, Amanpreet Singh, and Samuel R Bow-\nman. 2018. Neural network acceptability judgments.\narXiv preprint arXiv:1805.12471.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R´emi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\nScao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander M. Rush. 2020. Transform-\ners: State-of-the-art natural language processing. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nMengjie Zhao, Tao Lin, Fei Mi, Martin Jaggi, and Hin-\nrich Sch¨utze. 2020. Masking as an efficient alterna-\ntive to finetuning for pretrained language models. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 2226–2241, Online. Association for Computa-\ntional Linguistics.\n7\nA Appendices\nA.1 Layer naming\nFor convenience, we relate the notation used in the\npaper with the names of the corresponding parame-\nters in the popular HuggingFace (Wolf et al., 2020)\nimplementation.\nHuggingFace Parameter Name BitFit notation\nattention.self.query.bias bq\nattention.self.key.bias bk\nattention.self.value.bias bv\nattention.output.dense.bias bm1\nattention.output.LayerNorm.bias bLN1\nintermediate.dense.bias bm2\noutput.dense.bias bm3\noutput.LayerNorm.bias bLN2\nTable 4: Mapping the HuggingFace’s BertLayer bias\nparameters names to BitFit paper bias notation.\nA.2 Training Details\nTo perform classification with BERT, we follow the\napproach of Devlin et al. (2018), and attach a linear\nlayer to the contextual embedding of the [CLS]\ntoken to predict the label. The GLUE tasks are fed\ninto BERT using the standard procedures.\nWe optimize using AdamW (Loshchilov and Hut-\nter, 2017), with batch sizes of 16. For full fine-\ntuning, we used initial learning rates in {1e-5, 2e-5,\n3e-5, 5e-5}, and for the bias-only experiments we\nused initial learning rates in {1e-4, 4e-4, 7e-4, 1e-\n3} as the smaller rates took a very long time to\nconverge on some of the tasks. With the larger\nlearning rates, the bias-only fine-tuning converged\nin 8 or fewer epochs for most tasks, and up to 20\nepochs on the others. We did not perform hyper-\nparameter optimization beyond the minimal search\nover 4 learning rates. In each evaluation we report\nX±Y where X is the average result for training\n5 models with 5 different random seeds, Y is the\nstandard deviation.\nTo perform classification with RoBERTaBASE, we\nfollow the above details but without hyperparam-\neter search over the learning rates, for bias-only\nfine-tuning we used 1e-4 as learning rate and for\nfull fine-tuning we used 1e-5 as learning rate.\nAs Mosbach et al. (2020) show, fine-tuning\nBERTLARGE and RoBERTaBASE is a unstable due\nto vanishing gradients. BitFit allows for the usage\nof bigger learning rates, and overall the optimiza-\ntion process is much more stable, when compared\nTask Name Metric\nQNLI acc.\nSST-2 acc.\nMNLI matched acc./mismatched acc.\nCoLA Matthews corr.\nMRPC F1\nSTS-B Spearman corr.\nRTE acc.\nQQP F1\nTable 5: Metrics that we use to evaluate GLUE Bench-\nmark.\nTask Name BERT BASE BERTLARGE\nQNLI 1e-4 7e-4\nSST-2 4e-4 4e-4\nMNLI 1e-4 1e-4\nCoLA 7e-4 4e-4\nMRPC 7e-4 1e-3\nSTS-B 1e-4 1e-4\nRTE 1e-3 4e-4\nQQP 4e-4 4e-4\nTable 6: Learning rate configurations for best perform-\ning models.\nwith a full fine-tuning.\nA.3 GLUE Benchmark\nWe provide information on the GLUE tasks we\nevaluated on, as well as on the evaluation metrics.\nWe test our approach on the following subset of\nthe GLUE (Wang et al., 2018) tasks: The Corpus\nof Linguistic Acceptability (CoLA; Warstadt et al.\n(2018)), The Stanford Sentiment Treebank (SST-\n2; Socher et al. (2013)), The Microsoft Research\nParaphrase Corpus (MRPC; Dolan and Brockett\n(2005)), The Quora Question Pairs (QQP; Iyer et al.\n(2017)), The Semantic Textual Similarity Bench-\nmark (STS-B; Cer et al. (2017)), The Multi-Genre\nNatural Language Inference Corpus (MNLI; Bow-\nman et al. (2015)), The Stanford Question Answer-\ning Dataset (QNLI; Rajpurkar et al. (2016b)) and\nThe Recognizing Textual Entailment (RTE; Dagan\net al. (2005)).\nThe metrics that we used to evaluate GLUE\nBenchmark are in Table 5. Learning rate config-\nurations for best performing models are in Table\n6. For all the experiments we used the common\ntrain:dev:test partition of GLUE.\nA.4 Amount of change in bias terms\n8\nFigure 3: Change in bias components (CoLA task).\nFigure 4: Change in bias components (MRPC task).\nFigure 5: Change in bias components (STS-B task).\nA.5 SQuAD F1 Results\nFigure 6: Comparison of BitFit and Full-FT with\nBERTBASE F1 score on SQuAD validation set.\n9"
}