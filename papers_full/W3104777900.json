{
    "title": "Knowledge-Grounded Dialogue Generation with Pre-trained Language Models",
    "url": "https://openalex.org/W3104777900",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2146210586",
            "name": "Xueliang Zhao",
            "affiliations": [
                "Peking University"
            ]
        },
        {
            "id": "https://openalex.org/A1993208100",
            "name": "Wei Wu",
            "affiliations": [
                "Meizu (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2098334175",
            "name": "Can Xu",
            "affiliations": [
                "Microsoft Research Asia (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2585202595",
            "name": "Chongyang Tao",
            "affiliations": [
                "Microsoft Research Asia (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2098869281",
            "name": "Dongyan Zhao",
            "affiliations": [
                "Peking University"
            ]
        },
        {
            "id": "https://openalex.org/A2109109241",
            "name": "Rui Yan",
            "affiliations": [
                "Peking University",
                "Beijing Academy of Artificial Intelligence"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2130942839",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W2795571593",
        "https://openalex.org/W2916772188",
        "https://openalex.org/W2962717182",
        "https://openalex.org/W2963825865",
        "https://openalex.org/W2963903950",
        "https://openalex.org/W3000779003",
        "https://openalex.org/W3022187094",
        "https://openalex.org/W2966715458",
        "https://openalex.org/W2962785754",
        "https://openalex.org/W2899513582",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2964082993",
        "https://openalex.org/W2583186419",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W3082274269",
        "https://openalex.org/W2584185835",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2997300509",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W2914120296",
        "https://openalex.org/W2521114121",
        "https://openalex.org/W2971236040",
        "https://openalex.org/W1975879668",
        "https://openalex.org/W2891826200",
        "https://openalex.org/W2988937804",
        "https://openalex.org/W2898875342",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2963360026",
        "https://openalex.org/W2995183464",
        "https://openalex.org/W2963475460",
        "https://openalex.org/W2964352131",
        "https://openalex.org/W1591706642",
        "https://openalex.org/W10957333",
        "https://openalex.org/W4288624561",
        "https://openalex.org/W2914204778",
        "https://openalex.org/W2963904606",
        "https://openalex.org/W2969876226",
        "https://openalex.org/W2938224028",
        "https://openalex.org/W2996227762",
        "https://openalex.org/W2972664115",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2962796276",
        "https://openalex.org/W2964265128",
        "https://openalex.org/W2963206148",
        "https://openalex.org/W2798888952",
        "https://openalex.org/W2963035145",
        "https://openalex.org/W2954278700",
        "https://openalex.org/W2613904329",
        "https://openalex.org/W2981851019",
        "https://openalex.org/W2945260553",
        "https://openalex.org/W2963395792",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2963167649",
        "https://openalex.org/W2807873315",
        "https://openalex.org/W2328886022",
        "https://openalex.org/W2964458951",
        "https://openalex.org/W2950457956",
        "https://openalex.org/W2971274815",
        "https://openalex.org/W2952468927",
        "https://openalex.org/W2952420867",
        "https://openalex.org/W2963790827",
        "https://openalex.org/W2962883855",
        "https://openalex.org/W2970608575",
        "https://openalex.org/W2807880213",
        "https://openalex.org/W2963963856",
        "https://openalex.org/W2951508633",
        "https://openalex.org/W2923978210",
        "https://openalex.org/W2155027007",
        "https://openalex.org/W2962896208",
        "https://openalex.org/W2944815030",
        "https://openalex.org/W4287900772"
    ],
    "abstract": "We study knowledge-grounded dialogue generation with pre-trained language models. To leverage the redundant external knowledge under capacity constraint, we propose equipping response generation defined by a pre-trained language model with a knowledge selection module, and an unsupervised approach to jointly optimizing knowledge selection and response generation with unlabeled dialogues. Empirical results on two benchmarks indicate that our model can significantly outperform state-of-the-art methods in both automatic evaluation and human judgment.",
    "full_text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3377‚Äì3390,\nNovember 16‚Äì20, 2020.c‚Éù2020 Association for Computational Linguistics\n3377\nKnowledge-Grounded Dialogue Generation with\nPre-trained Language Models\nXueliang Zhao1,2, Wei Wu3, Can Xu4, Chongyang Tao4, Dongyan Zhao1,2, Rui Yan1,2,5‚àó\n1Wangxuan Institute of Computer Technology, Peking University, Beijing, China\n2Center for Data Science, AAIS, Peking University, Beijing, China\n3Meituan, Beijing, China 4Microsoft Corporation, Beijing, China\n5Beijing Academy of ArtiÔ¨Åcial Intelligence (BAAI), Beijing, China\n{xl.zhao,zhaody,ruiyan}@pku.edu.cn\n{wuwei19850318,chongyangtao}@gmail.com\nAbstract\nWe study knowledge-grounded dialogue gen-\neration with pre-trained language models. To\nleverage the redundant external knowledge un-\nder capacity constraint, we propose equip-\nping response generation deÔ¨Åned by a pre-\ntrained language model with a knowledge se-\nlection module, and an unsupervised approach\nto jointly optimizing knowledge selection and\nresponse generation with unlabeled dialogues.\nEmpirical results on two benchmarks indi-\ncate that our model can signiÔ¨Åcantly outper-\nform state-of-the-art methods in both auto-\nmatic evaluation and human judgment.\n1 Introduction\nWith advances in neural machine learning\n(Sutskever et al., 2014; Gehring et al., 2017;\nVaswani et al., 2017) and availability of the huge\namount of human conversations on social media\n(Adiwardana et al., 2020), building an open do-\nmain dialogue system with data-driven approaches\nhas attracted increasing attention from the commu-\nnity of artiÔ¨Åcial intelligence and natural language\nprocessing. In this work, we are interested in gen-\nerative approaches. Generative models for open\ndomain dialogues are notorious for replying with\ngeneric and bland responses, resulting in mean-\ningless and boring conversations (Li et al., 2015).\nSuch deÔ¨Åciency is particularly severe when human\nparticipants attempt to dive into speciÔ¨Åc topics in\nconversation (Dinan et al., 2019). As a result, there\nis still a big gap between conversation with existing\nsystems and conversation with humans.\nVery recently, there emerge two lines of research\nthat seem promising to bridge the gap. One is\nto apply large-scale pre-trained language models,\nsuch as GPT-2 (Radford et al., 2019), to the task\nof open domain dialogue generation. Prototypes\n‚àóCorresponding author: Rui Yan (ruiyan@pku.edu.cn).\nContext\nA I just discovered star trek and I really like\nwatching star trek .\nB Gene Roddenberry created it based upon\nscience Ô¨Åction and it is American media.\n...\nA If I remember Captain Kirk was not the\noriginal captain .\nB The Star Trek Canon of the series an ani-\nmated had 5 spin offs.\nA I watched a little of the next generation\nbut could not get into it like i did with the\noriginal show .\nResponse\nHuman These adventures went on but were short\nlived and six feature Ô¨Ålms.\nDialoGPT I think it‚Äôs worth it.\nTable 1: An example from the test set (Test Seen) of\nWizard of Wikipedia (Dinan et al., 2019) .\nsuch as DialoGPT (Zhang et al., 2019c) have ex-\nhibited compelling performance on generating re-\nsponses that make sense under conversation con-\ntexts and at the same time carry speciÔ¨Åc content\nfor keeping the conversation going. While the gi-\nant language models can memorize enough pat-\nterns in language during pre-training, they only\ncapture ‚Äúaverage‚Äù semantics of the data (Zhang\net al., 2019c). As a result, responses could still be\nbland or inappropriate when speciÔ¨Åc knowledge\nis required, as illustrated by the example in Ta-\nble 1. The other line is to ground dialogue gen-\neration by extra knowledge such as unstructured\ndocuments (Zhao et al., 2020). By the means, the\ndocuments (e.g., wiki articles) serve as content\nsources, and make a dialogue system knowledge-\nable regarding to a variety of concepts in discus-\nsion. However, collecting enough dialogues that\nare naturally grounded on documents for model\ntraining is not trivial. Although some benchmarks\nbuilt upon crowd-sourcing have been released by re-\ncent papers (Zhou et al., 2018b; Dinan et al., 2019;\nGopalakrishnan et al., 2019), the small training size\n3378\nmakes the generation models generalize badly on\nunseen topics (Dinan et al., 2019) and the cost of\nbuilding such data also prevents from transferring\nthe techniques proved on the benchmarks to new\ndomains and new languages.\nEncouraged by the results on pre-training for\ndialogue generation and knowledge-grounded dia-\nlogue generation, and motivated by the problems in\nboth sides, we consider bringing the two together\nin this work. SpeciÔ¨Åcally, we propose knowledge-\ngrounded dialogue generation with pre-trained lan-\nguage models in order to endow a generative model\nwith both rich knowledge and good generalization\nability1. The challenge is that pre-trained language\nmodels often set constraints on the maximum num-\nber of tokens they can handle (e.g., the maximum\nnumber for GPT-2 (Radford et al., 2019) is 1024),\nand thus hinders exploitation of the knowledge text\nwhich could be rather long and redundant (e.g., in\nWizard of Wikipedia (Dinan et al., 2019), on av-\nerage each conversation context is associated with\n61.2 sentences retrieved from wiki articles, and the\naverage number of tokens in the extra knowledge\nis 1625.6). Indeed, the conÔ¨Çict between model\ncapacity and the ability required for processing\nlong knowledge input represents an essential ob-\nstacle for applying pre-trained language models\nto knowledge-grounded dialogue generation, since\non the one hand we always have to set up an up-\nper bound to the capacity of pre-trained models\nin order to handle massive text corpus, and on the\nother hand we need to keep sufÔ¨Åcient candidates\nwith rich enough content in the procedure of re-\nsponse generation in order to guarantee the recall\nof relevant knowledge.\nTo overcome the challenge, we consider equip-\nping the pre-trained response generation model\nwith a knowledge selection module whereby the re-\ndundant knowledge input is slimmed with relevant\ninformation (regarding to conversation contexts)\nkept to meet the capacity constraint. While some\nrecent papers on knowledge-grounded dialogues\nhave paid attention to the problem of knowledge\nselection (Lian et al., 2019; Kim et al., 2020; Ren\net al., 2019), the knowledge selection module is\neither deeply coupled with the specially conÔ¨Ågured\nmodels (Lian et al., 2019; Ren et al., 2019) and thus\nis incompatible with the pre-trained language mod-\nels, or it is learned with human annotations (Dinan\n1In this paper, we assume that knowledge is retrieved from\ndocuments.\net al., 2019; Kim et al., 2018) which are difÔ¨Åcult to\nobtain in practice (e.g., the dataset in (Zhou et al.,\n2018b) does not contain annotations for knowledge\nselection). Therefore, we propose an unsupervised\napproach where learning of knowledge selection\nand Ô¨Åne-tuning of response generation are jointly\nconducted with unlabeled dialogues. SpeciÔ¨Åcally,\nwe build the knowledge selection module on the\nbasis of BERT, and formalize knowledge selec-\ntion as a sequence prediction process, by which\nthe model can take advantage of the pre-training\ntechniques and dynamically determine the relevant\nknowledge for a given context. The learning algo-\nrithm starts from training with pseudo ground-truth\nthat is constructed by making full use of responses\nas an alternation of human annotations, and then al-\nternatively updates the knowledge selection model\nand the response generation model through a re-\ninforcement learning approach and a curriculum\nlearning approach respectively. Thus, knowledge\nselection is further optimized with the feedback\nfrom response generation, and the knowledge used\nfor Ô¨Åne-tuning the response generation model grad-\nually moves from the pseudo ground-truth to the\nprediction of the knowledge selection module.\nWe test the proposed method on two benchmarks\nof knowledge-grounded dialogue generation: Wiz-\nard of Wikipedia (Dinan et al., 2019) and CMU\nDocument Grounded Conversations (Zhou et al.,\n2018b). Evaluation results indicate that our model\ncan signiÔ¨Åcantly outperform state-of-the-art meth-\nods as well as a few pre-trained models used in\nheuristic ways, and thus achieves new state-of-the-\nart on the benchmarks. Moreover, as a byproduct,\nthe knowledge selection module also outperforms\nthe state-of-the-art model in terms of accuracy of\nknowledge selection on Wizard of Wikipedia, im-\nplying that other models could also beneÔ¨Åt from\nthe component.\nOur contributions in this paper are three-fold:\n(1) proposal of a knowledge selection module for\napplying pre-trained language models to the task\nof knowledge-grounded dialogue generation; (2)\nproposal of an unsupervised approach in which\nlearning of knowledge selection and Ô¨Åne-tuning\nof the pre-trained model are conducted in a joint\nmanner; and (3) empirical veriÔ¨Åcation of the effec-\ntiveness of the proposed method on benchmarks of\nknowledge-grounded dialogue generation.\n3379\n2 Related Work\nEarly work on end-to-end open domain dialogue\ngeneration is inspired by the research of machine\ntranslation (Ritter et al., 2011; Shang et al., 2015;\nVinyals and Le, 2015). Later, the vanilla encoder-\ndecoder architecture is widely extended to improve\ndiversity of responses (Li et al., 2015; Xing et al.,\n2017a; Zhao et al., 2017; Tao et al., 2018); to\nmodel the structure of conversation contexts (Ser-\nban et al., 2016, 2017; Xing et al., 2017b; Zhang\net al., 2019a); to control attributes of responses\n(Xu et al., 2019; Zhou et al., 2017; Zhang et al.,\n2018a; Wang et al., 2018; See et al., 2019); and to\nbias responses to some speciÔ¨Åc personas (Li et al.,\n2016; Zhang et al., 2018b). Recently, grounding\ndialogue generation by extra knowledge is emerg-\ning as an important step towards human-like con-\nversational AI, where the knowledge could be ob-\ntained from knowledge graphs (Zhou et al., 2018a;\nMoon et al., 2019; Tuan et al., 2019), retrieved from\nunstructured documents (Dinan et al., 2019; Lian\net al., 2019; Zhao et al., 2020; Kim et al., 2020), or\nextracted from visual background (Mostafazadeh\net al., 2017; Shuster et al., 2018; Huber et al., 2018).\nIn this work, we study document-grounded dia-\nlogue generation. Rather than learning from scratch\nlike most existing work, we take advantage of the\npre-trained language models and achieve new state-\nof-the-art on the benchmarks of the task.\nBig, deep neural language models pre-trained\non huge unlabeled text corpus have led to strong\nimprovements on numerous natural language un-\nderstanding and natural language generation bench-\nmarks (Devlin et al., 2018; Yang et al., 2019; Liu\net al., 2019; Radford et al., 2019; Song et al., 2019;\nDong et al., 2019; Lewis et al., 2019), and there-\nfore are revolutionizing almost the full spectrum\nof NLP applications (Raffel et al., 2019; Sun et al.,\n2019b; Qiao et al., 2019; Zhang et al., 2019b; Lam-\nple and Conneau, 2019) and some interdisciplinary\napplications in NLP and computer vision (Lu et al.,\n2019; Su et al., 2019; Sun et al., 2019a). In the con-\ntext of dialogue generation, by Ô¨Åne-tuning GPT-2\n(Radford et al., 2019) in different sizes on social\nmedia data, recent work has (Zhang et al., 2019c;\nWolf et al., 2019) shown promising progress on con-\nversation engagement and commonsense question-\nanswering. In this work, we further explore the ap-\nplication of pre-training to the task of open domain\ndialogue generation by equipping the pre-trained\nlanguage models with external knowledge. Differ-\nent from a very recent paper on pre-training for\nlow-resource knowledge-grounded dialogue gen-\neration (Zhao et al., 2020), the work presents an\nin-depth investigation on how to release the power\nof the existing pre-trained language models on the\ntask when input exceeds the capacity of the models.\n3 Preliminary\n3.1 Problem Formalization\nSuppose that we have a dataset D =\n{(Ui,Di,ri)}N\ni=1, where ‚àÄi ‚àà {1,...,N },\nUi is a dialogue context, Di is a document that\ncontains relevant knowledge regarding to Ui, and\nri is a response to Ui based on Di. The goal is to\nlearn a generation model P(r|U,D; Œ∏) (Œ∏denotes\nthe parameters of the model) from D, and thus\ngiven a new dialogue context U associated with\na document D, one can generate a response r\nfollowing P(r|U,D; Œ∏).\n3.2 Pre-trained Language Models\nWe deÔ¨Åne P(r|U,D; Œ∏) on the basis of GPT-2 from\nOpenAI (Radford et al., 2019). GPT-2 are trans-\nformer language models with a stack of masked\nmulti-head self-attention layers, and are learned\nfrom large scale web text. To apply GPT-2 to the\ntask of knowledge-grounded dialogue generation,\nwe formulate the generation problem as\nP(r|U,D; Œ∏) = P(r|g(U,D); Œ∏)\n=\nlr‚àè\nt=1\nP(rt|g(U,D),r1:t‚àí1; Œ∏),\n(1)\nwhere g(U,D) tailors U ‚à™D to meet the length\nconstraint of a GPT-2 model as the input of gen-\neration, and rt refers to the t-th token of rwhose\nlength is supposed to be lr. The problem then boils\ndown to (1) how to deÔ¨Åne g(U,D); and (2) how to\nÔ¨Åne-tune Œ∏(and probably learn g(U,D)) with D.\nIn this work, we assume that labels that indi-\ncate the ground-truth knowledge are not available,\nwhich is practical but makes the problem even more\nchallenging. Since D could be rather redundant\nwith a lot of information irrelevant with the topic\nor the context of the conversation, simply truncat-\ning the concatenation of sentences of U and Das\ng(U,D) may cut the relevant knowledge and intro-\nduce noise into response generation, which hurts\nthe performance of the GPT-2 model, as will be\ndemonstrated in the experiments. Therefore, we\nconsider learning a g(U,D) that can distill useful\n3380\nContext\nTransformer(BERT) Layer-1\nd1 d2 dn\nd1 d2 <end>\n<d1> <d1,d2> ùë´‚Ä≤ ‚âî<d1,d2>\ns0 s1 s2\nTransformer(BERT) Layer-N\nSegment\nd1 d2 dn Context\nKnowledge Selection Module\nùë´‚Ä≤ ‡¥•ùë´(pseudo)Reward\nData Mixing\nInput\nTransformer(GPT-2) Layers\nPosition\n[Knowledge] [Context] [Response]\nùë§1\nùëë ùë§2\nùëë ùë§3\nùëë ùë§4\nùëë ùë§5\nùëë [Context] ùë§1\nùë¢ ùë§2\nùë¢ [Response] ùë§1\nùëü ùë§2\nùëü[Knowledge]\nùê∏2 ùê∏3 ùê∏4 ùê∏5 ùê∏6 ùê∏7 ùê∏8 ùê∏9 ùê∏10 ùê∏11 ùê∏12ùê∏1\nùë§1\nùëü ùë§2\nùëü [EOS]\nContext-aware\nKnowl Encoder\nSequential \nKnowl Selector\nFigure 1: Architecture of the proposed model.\ninformation from Dfor the GPT-2 model, as will\nbe elaborated in the next section.\n4 Approach\nHeading for learning ag(U,D) for applying GPT-2\nto the task of knowledge-grounded dialogue gen-\neration, we need to deal with several challenges:\n(1) how to model the correlation between a con-\ntext and the external knowledge; (2) how to learn\ng(U,D) when labels of ground-truth knowledge\nare absent; and (3) how to jointly optimize g(U,D)\nand the GPT-2 model with D, and thus the two can\nboost each other. Figure 1 illustrates the architec-\nture of the model. On the basis of the transformer\narchitecture, the knowledge selection module is\nmade up of a context-aware knowledge encoder\nand a sequential knowledge selector. The former\ncaptures interaction patterns between a context U\nand each sentence in D through a stack of self-\nattention layers, and the patterns are then fed to the\nlatter to decode useful knowledge one sentence per\nstep. Since human annotations are not accessible,\nthe learning method begins with pseudo ground-\ntruth constructed by making full use of responses,\nand optimization of g(U,D) and optimization of\nthe GPT-2 generation model are alternatively con-\nducted with a reinforcement learning approach and\na curriculum learning approach respectively.\n4.1 Context-Aware Knowledge Encoder\nWe choose BERT (Devlin et al., 2018) as the back-\nbone of the encoder. Thus, the encoder can take\nadvantage of pre-training, and the multi-layer bi-\ndirectional attention mechanism in BERT allows\na dialogue context and the associated knowledge\nto sufÔ¨Åciently interact with each other, resulting in\ncontext-aware knowledge representations. SpeciÔ¨Å-\ncally, let U = (u1,...,u n) and D= (d1,...,d m)\nbe the context and the knowledge respectively, then\nwe concatenate {ui}n\ni=1 as (wu\n1 ,¬∑¬∑¬∑ ,wu\nlu) with\nwu\ni the i-th word and lu the length of the se-\nquence, and deÔ¨Åne the input of the encoder as\nS= (S1,...,S m) with Si formulated as\nSi = [CLS]wu\n1 ...w u\nlu[SEP]wd\ni,1...w d\ni,j...w d\ni,ld[SEP],\n(2)\nwhere wd\ni,j refers to the j-th word of di ‚àà D,\nand ld is the length of di. Each Si ‚àà Spasses\nthrough the stacked self-attention layers, and is Ô¨Å-\nnally represented as ei = CLS(BERT(Si)) where\nBERT(Si) refers to the sequence of vectors from\nthe last layer of the encoder and CLS(¬∑) is a func-\ntion that returns the Ô¨Årst vector of the sequence\n(i.e., the vector corresponding to the [CLS] to-\nken). The output of the encoder is given by\nE = (e1,...,e m).\n4.2 Sequential Knowledge Selector\nWith Eas input, the sequential knowledge selector\ndetermines a subset of D (denoted as D‚Ä≤) as the\nrelevant knowledge and exploits D‚Ä≤to construct\ng(U,D). Since there may exist one-to-many rela-\ntions between a context and the relevant knowledge\n(Kim et al., 2020), the size of D‚Ä≤could vary from\ncontext to context. Therefore, we regard the con-\nstruction of D‚Ä≤as a sequence prediction process\nin which D‚Ä≤starts from an empty set and gradu-\nally expands by adding one sentence from Dper\nstep. By this means, the size of D‚Ä≤ can also be\nviewed as a parameter and is dynamically deter-\nmined according to the given context. Formally,\nwe maintain a sequence of hidden states {st}TU,D\nt=0\nwith the initial state s0 a trainable parameter, and\nweight {di}m\ni=1 by an attention mechanism which\n3381\ncan be formulated as\nP(di|U,dj1:t‚àí1) = exp(Œ±t,i)/\n‚àë\ni\nexp(Œ±t,i)\nŒ±t,i = v‚ä§tanh(Weei + Wsst + b),\n(3)\nwhere We, Ws, b and v are trainable param-\neters. Then djt will be added to D‚Ä≤ if jt =\nargmaxi‚àà{1,...,m}P(di|U,dj1:t‚àí1). After that, st+1\nis calculated by\nst+1 = LSTM(ejt,st) (4)\nTo determine TU,D, we introduce a special em-\nbedding espe into E, and terminate the prediction\nprocess if espe is selected or an upper bound Tmax\nis reached. Finally, g(U,D) is deÔ¨Åned as the con-\ncatenation of the sentences in U ‚à™D‚Ä≤.\n4.3 Learning Method\nLearning a g(U,D) without human annotations is\nnot trivial. For example, in a recent paper (Kim\net al., 2020), when human labels are removed, the\naccuracy of knowledge selection drops from 27%\nto 0.3%. Moreover, since knowledge selection and\nresponse generation are entangled, ideally we hope\ng(U,D) and the GPT-2 model can enhance each\nother in learning. However, as the parameters of\ng(U,D) are far from optimal at the early stage, it\nis very possible that noise from g(U,D) will be\nfed to the GPT-2 model and then Ô¨Çows back to the\nlearning procedure of g(U,D), resulting in inferior\nmodels on both sides. To cope with the challenges,\nwe propose a joint optimization strategy with weak\nsupervision as follows. The learning algorithm is\nsummarized in Algorithm 1.\nPseudo Ground-Truth Construction. To allevi-\nate error accumulation in joint optimization, we\nconsider constructing weak supervision and utilize\nthe signals to warm up the learning of g(U,D) and\nthe Ô¨Åne-tuning of GPT-2 beforehand. The intuition\nis that responses from humans carry clues to rele-\nvance of the knowledge candidates, and thus can\nbe used to construct pseudo ground-truth. To be\nspeciÔ¨Åc, we Ô¨Årst sort D = {dt}m\nt=1 in a descend-\ning order as{djt}m\nt=1 according to {Sim(dt,r)}m\nt=1\nwhere Sim(¬∑,¬∑) denotes a similarity function, and\nthen build a subset of Dby\n¬ØD= {dj1,...,d j ¬Øm},\n¬Øm= argmaxt(Sim(dj1:t,r)), (5)\nwhere dj1:t refers to the concatenation of {dji}t\ni=1.\nWith ¬ØD, g(U,D) and the GPT-2 model are\noptimized via maximum likelihood estimation\n(MLE) on DK = {(Ui,Di, ¬ØDi)}N\ni=1 and DG =\n{(Ui, ¬ØDi,ri)}N\ni=1 respectively.\nJoint Optimization: the Reinforcement Step.\nWe exploit the policy-gradient method (Sutton\net al., 2000) to continue-train g(U,D) by which\ng(U,D) is further ‚Äúsupervised‚Äù by the GPT-2\nmodel and is directly optimized for a target met-\nric (e.g., F1 in the experiments). SpeciÔ¨Åcally, we\nsample a ÀúDaccording to P(di|U,dj1:t‚àí1) (in Eq.3.)\nunder a termination criterion similar to ¬ØDat each\ntime step, and deÔ¨Åne the loss function as\nLK = ‚àí1\nN\nN‚àë\ni=1\nÔ£´\nÔ£¨Ô£≠ÀúRi\n|ÀúDi|‚àë\nt=1\nlog P(di,jt|Ui,di,j1:t‚àí1)\nÔ£∂\nÔ£∑Ô£∏,\nÀúRi = R( ÀúDi) ‚àíb,\n(6)\nwhere R( ÀúDi) = Sim( r‚Ä≤\ni,ri) with r‚Ä≤\ni the response\ngenerated by the GPT-2 model given Ui and ÀúDi,\nand b = ‚àëN\ni=1 R( ÀúDi)/N is the baseline that is\nused to reduce the variance of gradient estima-\ntion(Clark and Manning, 2016). We can see that\nminimizing LK is equivalent to maximizing the\nconditional likelihood of ÀúDi if it obtains a higher\nreward than the baseline.\nJoint Optimization: the Curriculum Step.\nThough g(U,D) has been pre-trained with the\npseudo ground-truth ¬ØD, the relevant knowledge\nprovided by the model (i.e., D‚Ä≤) may still be worse\nthan ¬ØDat the beginning of Ô¨Åne-tuning. Therefore,\nwe mix D‚Ä≤and ¬ØDand exploit a curriculum learning\nstrategy to Ô¨Åne-tune the GPT-2 model whereD‚Ä≤and\n¬ØDare regarded as hard materials and easy materials\nrespectively and Ô¨Åne-tuning gradually moves from\n¬ØDto D‚Ä≤. Formally, the loss function for Ô¨Åne-tuning\nthe GPT-2 model is deÔ¨Åned by\nLG = ‚àí 1\nN\nN‚àë\ni=1\n(\nzi\nlr‚àë\nt=1\nlog P(ri,t|Ui, ¬ØDi,ri,1:t‚àí1)\n+(1 ‚àízi)\nlr‚àë\nt=1\nlog P(ri,t|Ui,D‚Ä≤\ni,ri,1:t‚àí1)\n)\n,\n(7)\nwhere {zi}are sampled from a Bernoulli distribu-\ntion parameterized by p. By gradually shrinking p,\nthe generation model will be exposed to more hard\nmaterials with the learning procedure going on.\n3382\nAlgorithm 1Optimization Algorithm\n1: Input: Training data D, pre-trained GPT-2, initial curriculum rate p0, exponential decay constant Œª, maximum step M.\n2: Construct DK and DG.\n3: Optimize g(U,D) and GPT-2 using MLE on DK and DG respectively.\n4: for m‚Üê1 to M do\n5: Sample a mini-batch {(Ui,Di,ri)}from D.\n6: Update the parameters of g(U,D) based on Eq.6. ‚äøthe Reinforcement Step.\n7: Sample {zi}from a Bernoulli distribution parameterized by p, where p= p0e‚àíŒªm.\n8: Update the parameters of the GPT-2 model based on Eq.7. ‚äøthe Curriculum Step.\n9: end for\n10: return g(U,D) and GPT-2.\n5 Experiments\nWe conduct experiments on Wizard of Wikipedia\n(Wizard) and CMU Document Grounded Conver-\nsations (CMU DoG) (Zhou et al., 2018b).\n5.1 Datasets and Evaluation Metrics\nBoth datasets are built with crowd-sourcing on\nAmazon Mechanical Turk, employ Wikipedia as\nthe knowledge base, and are split into training sets,\nvalidation sets, and test sets by the data owners.\nTopics in Wizard cover a wide range ( 1,365 in\ntotal), and each conversation happens between a\nwizard who has access to the knowledge about a\nspeciÔ¨Åc topic and an apprentice who is just eager to\nlearn from the wizard about the topic. The test set\nis split into two subsets: Test Seen and Test Unseen.\nTest Seen contains new dialogues with topics ap-\npearing in the training set, while topics in Test Un-\nseen never appear in the training set and the valida-\ntion set. We follow (Dinan et al., 2019) and conduct\nthe pre-processing with the code published on Par-\nlAI2. Different from Wizard, CMU DoG focuses\non movie domain, and besides wizard-apprentice\nconversations, the data also contain conversations\nbetween two workers who know the document and\ntry to discuss the content in depth. To better com-\npare with the baselines, we adopt the version shared\nat https://github.com/lizekang/ITDD. In both\ndata, only the turns where knowledge is accessi-\nble are considered in response generation. More\ndetails are described in supplementary material.\nWe choose perplexity (PPL) of the ground-truth\nresponses, BOW Embedding (Liu et al., 2016),\nand unigram F1 (Dinan et al., 2019) as metrics,\nwhere Embedding-based metrics are computed\nwith an NLG evaluation open source available at\nhttps://github.com/Maluuba/nlg-eval, and\nF1 is calculated with the code published at https:\n2https://github.com/facebookresearch/\nParlAI/blob/master/projects/wizard_of_\nwikipedia\n//github.com/facebookresearch/ParlAI/\nblob/master/parlai/core/metrics.py.\nBesides automatic evaluation, we randomly sam-\nple 300 examples from Test Seen, Test Unseen, and\nthe test set of CMU DoG respectively, and recruit\n3 well-educated native speakers as annotators for\nhuman evaluation. To each annotator, an example\nis presented with a context, the associated exter-\nnal knowledge3, and model responses (top 1 in\ngreedy search) that are randomly shufÔ¨Çed to hide\ntheir sources. The annotators then judge the quality\nof the responses from three aspects, including Ô¨Çu-\nency, context coherence and knowledge relevance,\nand assign a score in {0,1,2}(representing ‚Äúbad‚Äù,\n‚Äúfair‚Äù, and ‚Äúgood‚Äù) to each response for each aspect.\nEach response receives 3 scores per aspect, and the\nagreement among the annotators is measured via\nFleiss‚Äô kappa (Fleiss, 1971).\n5.2 Baselines\nThe following models are selected as baselines:\nTransformer Memory Network (TMN):\nthe model proposed in (Dinan et al., 2019)\nalong with the release of the Wizard data. We\nimplement it using the code shared at https:\n//github.com/facebookresearch/ParlAI/\nblob/master/projects/wizard_of_wikipedia.\nIncremental Transformer with Deliberation\nDecoder (ITDD):a transformer-based model (Li\net al., 2019) that incrementally encodes multi-turn\ndialogues and knowledge and decodes responses\nwith a deliberation technique. We implement it\nusing the code shared at https://github.com/\nlizekang/ITDD.\nSequential Knowledge Transformer (SKT):a\nsequential latent variable model with state-of-the-\nart performance on knowledge selection published\nin a very recent paper (Kim et al., 2020). Since\nhuman labels that indicate ground-truth knowl-\n3For ease of labeling, only the ground-truth knowledge is\nshown to the annotators in Wizard.\n3383\nModels Test Seen Test UnseenPPL F1 Average Extrema Greedy PPL F1 Average Extrema GreedyTMN (Dinan et al., 2019)66.5 15.9 0.844 0.427 0.658 103.6 14.3 0.839 0.408 0.645ITDD (Li et al., 2019)17.8 16.2 0.841 0.425 0.654 44.8 11.4 0.826 0.364 0.624SKT* (Kim et al., 2020)52.0 19.3 0.846 0.440 0.665 81.4 16.1 0.839 0.418 0.652DRD (Zhao et al., 2020)19.4 19.3 0.852 0.452 0.674 23.0 17.9 0.849 0.439 0.664SKT+GPT-2* 17.6 20.3 0.866 0.460 0.679 23.7 17.8 0.860 0.437 0.664GPT-2trunc 14.6(2.2)18.7(0.7)0.864(0.002)0.451(0.006)0.674(0.004)16.9(3.1)18.3(0.6)0.862(0.002)0.444(0.005)0.668(0.003)KnowledGPT 19.2 22.0 0.872 0.463 0.682 22.3 20.5 0.870 0.452 0.674\nTable 2: Evaluation results on Wizard. Models that leverage human labels are marked with *. Numbers in bold\nmean that the improvement to the best baseline is statistically signiÔ¨Åcant (t-test with p-value <0.01).\nModels PPL F1 AverageExtremaGreedy\nTMN (Dinan et al., 2019)75.2 9.9 0.789 0.399 0.615\nITDD (Li et al., 2019)26.0 10.4 0.748 0.390 0.587\nDRD (Zhao et al., 2020)46.1 10.8 0.791 0.406 0.613\nGPT-2trunc 18.6 10.8 0.730 0.419 0.597\nKnowledGPT 20.6 13.5 0.837 0.437 0.654\nTable 3: Evaluation results on CMU DoG. Numbers in\nbold mean that the improvement to the best baseline is\nstatistically signiÔ¨Åcant (t-test with p-value <0.01).\nedge are crucial to the performance of the model,\nwe only involve it as a baseline on the Wiz-\nard data. The model is implemented with the\ncode shared at https://github.com/bckim92/\nsequential-knowledge-transformer.\nDisentangled Response Decoder (DRD): a\nmodel that tackles the low-resource challenge with\npre-training techniques (Zhao et al., 2020). We\nchoose the one in which all parameters are Ô¨Åne-\ntuned with the full training data after pre-training\nas the baseline, since such a conÔ¨Åguration results in\nstate-of-the-art performance on Wizard, as reported\nin (Zhao et al., 2020).\nWe name our model KnowledGPT. Besides\nthe baselines described above, the following pre-\ntrained models are also included in comparison in\norder to have a thorough understanding towards\nthe proposed method: (1) GPT-2trunc. We con-\ncatenate a context and the associated knowledge as\na long document, and then truncate the document\nto meet the length constraint of the GPT-2 model.\nThis is to check if the simple heuristics work for\nthe task. Note that in Wizard, we randomly mix the\nground-truth knowledge with others and repeat the\nprocedure 8 times. The means with standard devia-\ntion (i.e., numbers in ‚Äú( )‚Äù) are reported to remove\nrandomness; and (2) SKT+GPT-2. We feed the\ncandidate selected by SKT to GPT-2 for response\ngeneration. This is to examine if we can simply re-\nplace the proposed knowledge selection module as\nwell as the learning approach with an off-the-shelf\nknowledge selection model. Similar to SKT, the\ncomparison is only conducted on Wizard.\n5.3 Implementation Details\nIn both Wizard and CMU DoG, we set the hid-\nden size and the number of layers of the sequen-\ntial knowledge selector as 256 and 1 respectively.\nTmax for D‚Ä≤ is set as 1 in Wizard, and 2 in\nCMU DoG. We choose BERT (110M) and GPT-\n2 (117M) as the pre-trained language models in\nKnowledGPT, and implement the models with\nthe code in https://github.com/huggingface/\ntransformers. We employ greedy search in re-\nsponse decoding. All models are learned with\nAdam (Kingma and Ba, 2015) optimizer with\nŒ≤1 = 0.9 and Œ≤2 = 0.999. In warming up, we de-\nÔ¨Åne Sim(¬∑,¬∑) as unigram F1, and optimize g(U,D)\nand the GPT-2 model with the pseudo ground-truth\nfor 1000 steps with a batch size of 64. In joint\noptimization, the batch size is set as 128, and the\nlearning rates for g(U,D) and GPT-2 are set as\n5e‚àí6 and 5e‚àí5 respectively. The learning rate\nwill be halved if there is no improvement in terms\nof PPL on the validation sets. The parameter pof\nthe Bernoulli distribution in the curriculum step\nis initially set as 1.0 and anneals with a rate of\n1e‚àí5. Early stopping on validation is adopted as\na regularization strategy.\n5.4 Evaluation Results\nTable 2 and Table 3 report evaluation results on\nWizard and CMU DoG respectively. KnowledGPT\nachieves new state-of-the-art on most metrics in\nboth datasets, which demonstrates the effective-\nness of large-scale pre-trained language models on\nthe task of knowledge-grounded dialogue genera-\ntion. GPT-2trunc is worse than KnowledGPT, due\nto (1) knowledge loss: we Ô¨Ånd that in 53% test\nexamples (Test Seen+Test Unseen), the ground-\ntruth knowledge is cut. In this case, GPT-2 trunc\nonly relies on the context, the related knowledge\nin other candidates (thanks to the one-to-many re-\nlations between a context and knowledge), and the\nknowledge packed in the parameters of GPT-2 for\nresponding, which explains the comparable per-\n3384\nModels Wizard CMUDoGTest Seen Test Unseen\nFluencyContextCoherenceKnowledgeRelevanceKappaFluencyContextCoherenceKnowledgeRelevanceKappaFluencyContextCoherenceKnowledgeRelevanceKappa\nDRD 1.71 1.50 1.26 0.67 1.64 1.44 1.18 0.69 1.58 1.48 1.07 0.60\nGPT-2trunc 1.86 1.54 1.22 0.71 1.84 1.47 1.20 0.59 1.83 1.58 1.06 0.64\nKnowledGPT1.89 1.67 1.71 0.70 1.88 1.60 1.68 0.73 1.83 1.65 1.50 0.77\nTable 4: Human evaluation results on Wizard and CMU DoG.\nModels\nWizard CMUDoGTest Seen Test Unseen\nPPL F1 AverageExtremaGreedyPPL F1 AverageExtremaGreedyPPL F1 AverageExtremaGreedy\nKnowledGPT19.2 22.0 0.872 0.463 0.682 22.3 20.5 0.870 0.452 0.674 20.6 13.5 0.837 0.437 0.654\n-pseudo 22.3 18.3 0.857 0.436 0.662 24.1 17.9 0.854 0.430 0.655 23.2 12.9 0.815 0.440 0.639\n-joint 20.0 20.4 0.863 0.457 0.675 21.8 19.5 0.861 0.451 0.669 22.6 11.7 0.806 0.438 0.635\n-curriculum19.4 21.2 0.867 0.457 0.677 21.5 20.3 0.866 0.451 0.672 21.9 12.4 0.816 0.443 0.644\n-reinforcement19.4 21.3 0.866 0.459 0.677 21.9 20.2 0.863 0.449 0.670 20.3 12.6 0.817 0.437 0.643\nTable 5: Ablation study on Wizard and CMU DoG\nformance with SKT and DRD; and (2) noisy in-\nput: even though the ground-truth knowledge is\nkept, the redundant and irrelevant information in\nthe knowledge candidates are still harmful. Ev-\nidence is that GPT-2 trunc is worse than Knowl-\nedGPT on CMU DoG even though we do not cut\nanything on the knowledge (the maximum length\nof the knowledge input is 502, and thus is within\nthe constraint of GPT-2). KnowledGPT also outper-\nforms SKT+GPT-2 on Wizard, because (1) Knowl-\nedGPT is more accurate than SKT on knowledge\nselection, even though it does not leverage any hu-\nman annotations in learning. In fact, the accuracy\nscores of knowledge selection for SKT are26.8 and\n18.3 on Test Seen and Test Unseen respectively,\nwhile the two numbers are 28.0 and 25.4 respec-\ntively for KnowledGPT; and (2) in KnowledGPT,\nknowledge selection and response generation are\njointly optimized.\nTable 4 shows human evaluation results. While\nthe three models are comparable onÔ¨Çuency, Knowl-\nedGPT is superior to the others on both context\ncoherence and knowledge relevance, which is con-\nsistent with the results on automatic metrics. All\nkappa values are no less than 0.6, indicating sub-\nstantial agreement among the annotators. We\npresent a case study in supplementary material.\n5.5 Discussions\nAblation study.To understand the impact of the\nlearning strategies on model performance, we com-\npare the full KnowledGPT with the following vari-\nants: (1) -pseudo: the warming up stage is removed;\n(2) -joint: the joint optimization stage is removed;\n(3) -reinforcement: g(U,D) is Ô¨Åxed after it is op-\ntimized with MLE on DK; and (4) -curriculum:\nModels\nWizard CMUDoGTest Seen Test Unseen\nPPL F1 PPL F1 PPL F1\nTmax=1 19.2 22.0 22.3 20.5 20.6 12.6\nTmax=2 18.2 21.3 21.0 20.3 20.6 13.5\nTmax=3 17.2 21.1 20.2 20.3 19.7 11.2\nTable 6: Performance of KnowledGPT under different\nTmaxs.\nGPT-2 is Ô¨Åxed after it is optimized with MLE on\nDG. Table 5 reports the evaluation results. We can\nconclude that (1) the pseudo ground-truth plays a\ncrucial role in Wizard, as removing the step causes\ndramatic performance drop. This is because in Wiz-\nard, there is a strong correlation between the knowl-\nedge and human responses. The results indicate\nthat though the pseudo ground-truth is constructed\nwith heuristics, it still contains valuable informa-\ntion and thus allows the following joint optimiza-\ntion to start from a good point. On the other hand,\nin CMU DoG, the crowd-workers do not refer to\nthe external knowledge as much as those work-\ners do in Wizard when they form the responses;\n(2) the reinforcement step and curriculum step are\nuseful because the reinforcement step allows the\nknowledge selection module to make better use of\nGPT-2‚Äôs feedback, and through the curriculum step\nGPT-2 can take advantage of the output of knowl-\nedge selection module progressively; (3) joint op-\ntimization is meaningful, as removing this stage\nresults in performance drop.\nImpact of Tmax (i.e., the upper bound in\nknowledge selection).Besides the learning strate-\ngies, we are also curious about how Tmax, as part\nof the termination criterion in knowledge selection\ndescribed at the end of Section 4.2, inÔ¨Çuences the\n3385\nperformance of KnowledGPT. To this end, we vary\nthe value of Tmax in {1,2,3}and report the evalua-\ntion results in Table 6. The largerTmax is, the more\nchances KnowledGPT has to involve the ground-\ntruth candidate into generation, and the lower PPL\nis. This also explains why the PPL of GPT-2trunc\nis lower than that of KnowledGPT in Table 2 and\nTable 3. On the other hand, a larger Tmax also\nmeans more noise in generation. That is why when\nTmax exceeds a value, F1 begins to drop.\n6 Conclusions\nWe apply large-scaled pre-trained language mod-\nels to the task of knowledge-grounded dialogue\ngeneration. To this end, we devise a knowledge\nselection module, and propose an unsupervised ap-\nproach to jointly optimizing knowledge selection\nand response generation. Evaluation results on two\nbenchmarks indicate that our model can signiÔ¨Å-\ncantly outperform state-of-the-art methods.\nAcknowledgments\nWe would like to thank the reviewers for their con-\nstructive comments. This work was supported by\nthe National Key Research and Development Pro-\ngram of China (No. 2020AAA0105200), the Na-\ntional Science Foundation of China (NSFC No.\n61876196 and NSFC No. 61672058). Rui Yan was\nsponsored as the young fellow of Beijing Academy\nof ArtiÔ¨Åcial Intelligence (BAAI). Rui Yan is the\ncorresponding author.\nReferences\nDaniel Adiwardana, Minh-Thang Luong, David R So,\nJamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang,\nApoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu,\net al. 2020. Towards a human-like open-domain\nchatbot. arXiv preprint arXiv:2001.09977.\nKevin Clark and Christopher D Manning. 2016. Deep\nreinforcement learning for mention-ranking corefer-\nence models. In Proceedings of the 2016 Confer-\nence on Empirical Methods in Natural Language\nProcessing, pages 2256‚Äì2262.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\nFan, Michael Auli, and Jason Weston. 2019. Wizard\nof wikipedia: Knowledge-powered conversational\nagents. In ICLR.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-\naodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\nand Hsiao-Wuen Hon. 2019. UniÔ¨Åed language\nmodel pre-training for natural language understand-\ning and generation. In Advances in Neural Informa-\ntion Processing Systems, pages 13042‚Äì13054.\nJoseph L Fleiss. 1971. Measuring nominal scale agree-\nment among many raters. Psychological bulletin ,\n76(5):378.\nJonas Gehring, Michael Auli, David Grangier, Denis\nYarats, and Yann N Dauphin. 2017. Convolutional\nsequence to sequence learning. In Proceedings\nof the 34th International Conference on Machine\nLearning-Volume 70, pages 1243‚Äì1252. JMLR. org.\nKarthik Gopalakrishnan, Behnam Hedayatnia, Qin-\nlang Chen, Anna Gottardi, Sanjeev Kwatra, Anu\nVenkatesh, Raefer Gabriel, Dilek Hakkani-T ¬®ur, and\nAmazon Alexa AI. 2019. Topical-chat: To-\nwards knowledge-grounded open-domain conversa-\ntions. Proc. Interspeech 2019, pages 1891‚Äì1895.\nBernd Huber, Daniel McDuff, Chris Brockett, Michel\nGalley, and Bill Dolan. 2018. Emotional dialogue\ngeneration using image-grounded language models.\nIn CHI, page 277. ACM.\nByeongchang Kim, Jaewoo Ahn, and Gunhee Kim.\n2020. Sequential latent knowledge selection for\nknowledge-grounded dialogue. arXiv preprint\narXiv:2002.07510.\nSeonhoon Kim, Jin-Hyuk Hong, Inho Kang, and No-\njun Kwak. 2018. Semantic sentence matching with\ndensely-connected recurrent and co-attentive infor-\nmation. arXiv preprint arXiv:1805.11360.\nDiederik P Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In ICLR.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining. arXiv preprint\narXiv:1901.07291.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Ves Stoyanov, and Luke Zettlemoyer. 2019.\nBart: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and\ncomprehension. arXiv preprint arXiv:1910.13461.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng\nGao, and Bill Dolan. 2015. A diversity-promoting\nobjective function for neural conversation models.\nNAACL, pages 110‚Äì119.\nJiwei Li, Michel Galley, Chris Brockett, Georgios Sp-\nithourakis, Jianfeng Gao, and Bill Dolan. 2016. A\npersona-based neural conversation model. In ACL,\npages 994‚Äì1003.\nZekang Li, Cheng Niu, Fandong Meng, Yang Feng,\nQian Li, and Jie Zhou. 2019. Incremental trans-\nformer with deliberation decoder for document\n3386\ngrounded conversations. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 12‚Äì21.\nRongzhong Lian, Min Xie, Fan Wang, Jinhua Peng,\nand Hua Wu. 2019. Learning to select knowledge\nfor response generation in dialog systems. arXiv\npreprint arXiv:1902.04911.\nChia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Nose-\nworthy, Laurent Charlin, and Joelle Pineau. 2016.\nHow not to evaluate your dialogue system: An em-\npirical study of unsupervised evaluation metrics for\ndialogue response generation. In Proceedings of the\n2016 Conference on Empirical Methods in Natural\nLanguage Processing, pages 2122‚Äì2132.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan\nLee. 2019. Vilbert: Pretraining task-agnostic visi-\nolinguistic representations for vision-and-language\ntasks. In Advances in Neural Information Process-\ning Systems, pages 13‚Äì23.\nSeungwhan Moon, Pararth Shah, Anuj Kumar, and Ra-\njen Subba. 2019. Opendialkg: Explainable conver-\nsational reasoning with attention-based walks over\nknowledge graphs. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 845‚Äì854.\nNasrin Mostafazadeh, Chris Brockett, Bill Dolan,\nMichel Galley, Jianfeng Gao, Georgios Spithourakis,\nand Lucy Vanderwende. 2017. Image-grounded\nconversations: Multimodal context for natural ques-\ntion and response generation. In Proceedings of\nthe Eighth International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers) ,\npages 462‚Äì472.\nYifan Qiao, Chenyan Xiong, Zhenghao Liu, and\nZhiyuan Liu. 2019. Understanding the behaviors of\nbert in ranking. arXiv preprint arXiv:1904.07531.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a uniÔ¨Åed text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nPengjie Ren, Zhumin Chen, Christof Monz, Jun Ma,\nand Maarten de Rijke. 2019. Thinking globally,\nacting locally: Distantly supervised global-to-local\nknowledge selection for background based conver-\nsation. arXiv preprint arXiv:1908.09528.\nAlan Ritter, Colin Cherry, and William B Dolan. 2011.\nData-driven response generation in social media. In\nProceedings of the 2011 Conference on Empirical\nMethods in Natural Language Processing , pages\n583‚Äì593.\nAbigail See, Stephen Roller, Douwe Kiela, and Jason\nWeston. 2019. What makes a good conversation?\nhow controllable attributes affect human judgments.\narXiv preprint arXiv:1902.08654.\nIulian Vlad Serban, Alessandro Sordoni, Yoshua Ben-\ngio, Aaron C Courville, and Joelle Pineau. 2016.\nBuilding end-to-end dialogue systems using gener-\native hierarchical neural network models. In AAAI,\nvolume 16, pages 3776‚Äì3784.\nIulian Vlad Serban, Alessandro Sordoni, Ryan Lowe,\nLaurent Charlin, Joelle Pineau, Aaron C Courville,\nand Yoshua Bengio. 2017. A hierarchical latent\nvariable encoder-decoder model for generating dia-\nlogues. In AAAI, pages 3295‚Äì3301.\nLifeng Shang, Zhengdong Lu, and Hang Li. 2015. Neu-\nral responding machine for short-text conversation.\nIn ACL, pages 1577‚Äì1586.\nKurt Shuster, Samuel Humeau, Antoine Bordes, and\nJason Weston. 2018. Engaging image chat: Model-\ning personality in grounded dialogue. arXiv preprint\narXiv:1811.00945.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-\nYan Liu. 2019. Mass: Masked sequence to se-\nquence pre-training for language generation. In In-\nternational Conference on Machine Learning, pages\n5926‚Äì5936.\nWeijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu,\nFuru Wei, and Jifeng Dai. 2019. Vl-bert: Pre-\ntraining of generic visual-linguistic representations.\narXiv preprint arXiv:1908.08530.\nChen Sun, Austin Myers, Carl V ondrick, Kevin Mur-\nphy, and Cordelia Schmid. 2019a. Videobert: A\njoint model for video and language representation\nlearning. In Proceedings of the IEEE International\nConference on Computer Vision, pages 7464‚Äì7473.\nChi Sun, Luyao Huang, and Xipeng Qiu. 2019b. Uti-\nlizing bert for aspect-based sentiment analysis via\nconstructing auxiliary sentence. In Proceedings of\nNAACL-HLT, pages 380‚Äì385.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural networks.\nIn Advances in neural information processing sys-\ntems, pages 3104‚Äì3112.\nRichard S Sutton, David A McAllester, Satinder P\nSingh, and Yishay Mansour. 2000. Policy gradient\nmethods for reinforcement learning with function ap-\nproximation. In Advances in neural information pro-\ncessing systems, pages 1057‚Äì1063.\n3387\nChongyang Tao, Shen Gao, Mingyue Shang, Wei Wu,\nDongyan Zhao, and Rui Yan. 2018. Get the point of\nmy utterance! learning towards effective responses\nwith multi-head attention mechanism. In IJCAI,\npages 4418‚Äì4424.\nYi-Lin Tuan, Yun-Nung Chen, and Hung-yi Lee.\n2019. Dykgchat: Benchmarking dialogue genera-\ntion grounding on dynamic knowledge graphs. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 1855‚Äì\n1865.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS, pages 5998‚Äì6008.\nOriol Vinyals and Quoc Le. 2015. A neural conversa-\ntional model. arXiv preprint arXiv:1506.05869.\nYansen Wang, Chenyi Liu, Minlie Huang, and Liqiang\nNie. 2018. Learning to ask questions in open-\ndomain conversational systems with typed decoders.\nIn Proceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 2193‚Äì2203.\nThomas Wolf, Victor Sanh, Julien Chaumond, and\nClement Delangue. 2019. Transfertransfo: A\ntransfer learning approach for neural network\nbased conversational agents. arXiv preprint\narXiv:1901.08149.\nChen Xing, Wei Wu, Jie Liu, Yalou Huang, Ming Zhou,\nand Wei-Ying Ma. 2017a. Topic aware neural re-\nsponse generation. In AAAI, pages 3351‚Äì3357.\nChen Xing, Wei Wu, Yu Wu, Ming Zhou, Yalou Huang,\nand Wei-Ying Ma. 2017b. Hierarchical recurrent\nattention network for response generation. arXiv\npreprint arXiv:1701.07149.\nCan Xu, Wei Wu, Chongyang Tao, Huang Hu, Matt\nSchuerman, and Ying Wang. 2019. Neural re-\nsponse generation with meta-words. arXiv preprint\narXiv:1906.06050.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V Le.\n2019. Xlnet: Generalized autoregressive pretrain-\ning for language understanding. arXiv preprint\narXiv:1906.08237.\nHainan Zhang, Yanyan Lan, Liang Pang, Jiafeng Guo,\nand Xueqi Cheng. 2019a. Recosa: Detecting the rel-\nevant contexts with self-attention for multi-turn di-\nalogue generation. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 3721‚Äì3730.\nRuqing Zhang, Jiafeng Guo, Yixing Fan, Yanyan Lan,\nJun Xu, and Xueqi Cheng. 2018a. Learning to con-\ntrol the speciÔ¨Åcity in neural response generation. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 1108‚Äì1117.\nSaizheng Zhang, Emily Dinan, Jack Urbanek, Arthur\nSzlam, Douwe Kiela, and Jason Weston. 2018b. Per-\nsonalizing dialogue agents: I have a dog, do you\nhave pets too? arXiv preprint arXiv:1801.07243.\nXingxing Zhang, Furu Wei, and Ming Zhou. 2019b.\nHibert: Document level pre-training of hierarchi-\ncal bidirectional transformers for document summa-\nrization. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 5059‚Äì5069.\nYizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen,\nChris Brockett, Xiang Gao, Jianfeng Gao, Jingjing\nLiu, and Bill Dolan. 2019c. Dialogpt: Large-scale\ngenerative pre-training for conversational response\ngeneration. arXiv preprint arXiv:1911.00536.\nTiancheng Zhao, Ran Zhao, and Maxine Eskenazi.\n2017. Learning discourse-level diversity for neural\ndialog models using conditional variational autoen-\ncoders. In ACL, pages 654‚Äì664.\nXueliang Zhao, Wei Wu, Chongyang Tao, Can Xu,\nDongyan Zhao, and Rui Yan. 2020. Low-resource\nknowledge-grounded dialogue generation. arXiv\npreprint arXiv:2002.10348.\nHao Zhou, Minlie Huang, Tianyang Zhang, Xiaoyan\nZhu, and Bing Liu. 2017. Emotional chatting\nmachine: Emotional conversation generation with\ninternal and external memory. arXiv preprint\narXiv:1704.01074.\nHao Zhou, Tom Young, Minlie Huang, Haizhou Zhao,\nJingfang Xu, and Xiaoyan Zhu. 2018a. Com-\nmonsense knowledge aware conversation generation\nwith graph attention. In IJCAI, pages 4623‚Äì4629.\nKangyan Zhou, Shrimai Prabhumoye, and Alan W\nBlack. 2018b. A dataset for document grounded\nconversations. arXiv preprint arXiv:1809.07358.\n3388\nA Details of Datasets\nTable 7 reports the statistics of the Wizard data and\nthe CMU DoG data.\nWizard of WikipediaCMUDoGTrainValidTest SeenTest UnseenTrainValidTest#Utterances166,78717,7158,715 8,78274,7174,99313,646#Conversations18,4301,948965 968 3,373229 619#Topics/Documents1,247599 533 58 30 30 30Avg.#of Turns 9.0 9.1 9.0 9.1 22.221.822.0\nTable 7: Statistics of the two datasets.\nB Comparison with DialoGPT\nWe compare KnowledGPT and with DialoGPT in\norder to learn if a pre-trained generation model\nwith state-of-the-art performance on open domain\ndialogues is already good enough when it is Ô¨Åne-\ntuned with knowledge-grounded dialogues. We\ndiscard the associated knowledge and Ô¨Åne-tune\nDialoGPT on the knowledge-grounded dialogues.\nWe choose the model trained from OpenAI GPT-2\nwith 345M parameters, as it shows the best perfor-\nmance in the evaluation in the original paper. The\nmodel is implemented based on the code shared at\nhttps://github.com/microsoft/DialoGPT.\nTable 8 shows the results, indicating that exter-\nnal knowledge is necessary even though one has\nexploited a powerful pre-trained language model\nfor dialogue generation. In CMU DoG the gap\nbetween DialoGPT and KnowledGPT is narrowed\nbecause about 35% of the conversation has a weak\ncorrelation with the document (e.g. BLEU <0.1).\nModels\nWizard CMUDoGTest Seen Test Unseen\nPPL F1 PPL F1 PPL F1\nDialoGPT 16.0 17.9 20.0 16.8 16.9 12.3\nKnowledGPT19.2 22.0 22.3 20.5 20.6 13.5\nTable 8: Comparison with DialoGPT on Wizard and\nCMU DoG\nC Impact of Maximum Tokens of GPT-2\nTo further justify our claims on why GPT-2 trunc\nis worse than KnowledGPT, we keep the ground-\ntruth knowledge in the input sequence of GPT-2\nand gradually increase the constraint of the maxi-\nmum number of tokens on Wizard. As the maxi-\nmum token limit increases, more irrelevant knowl-\nedge is introduced. Note that in practice, one has\nno way to perfectly locate the ground-truth, and\nthis experiment is only to provide more insights\nto GPT-2trunc. Table 9 shows the performance\nof GPT-2trunc with the increase of the maximum\nMaximum TokensTest SeenTest UnseenGround-truth\nPercentagePPL F1 PPL F1\n128 10.8 30.9 11.6 30.4 62.3%\n256 9.3 25.6 10.0 24.6 20.3%\n512 9.7 21.8 10.5 21.2 8.5%\n768 10.1 20.6 10.7 20.2 5.5%\n1024 10.7 19.7 11.3 19.4 4.1%\nTable 9: Performance of GPT-2 trunc under differ-\nent maximum tokens with ground-truth knowledge in-\nvolved.\nModels\nWizard of WikipediaCMUDoGTest SeenTest Unseen\nPPL F1 PPL F1 PPL F1\nKnowledGPT (117M)19.2 22.0 22.3 20.5 20.6 13.5\nKnowledGPT (345M)16.1 22.0 17.9 20.6 18.1 13.4\nTable 10: Performance of KnowledGPT under different\nsizes of GPT-2.\nnumber of tokens where Ground-truth Percentage\nindicates the percentage of ground-truth in the in-\nput knowledge. First, when the ground-truth is\nforced to be kept, GPT-2trunc is always better than\nthe one where the ground-truth is randomly mixed\nwith other candidates and bears the risk to be cut.\nThis echoes our claim that knowledge loss is one\nof the reasons for the poor performance of GPT-\n2trunc used with the practical setting. Second, even\nif ground-truth is retained, once more noise is intro-\nduced, the performance of GPT-2trunc will become\nworse. When the length is limited to 128 tokens,\nthe PPL of the model is not good, mainly because\nunder this limitation, the input sequence of some\ncases only contains the dialogue context and re-\nsponse.\nD Impact of the Size of GPT-2\nWe further check if the performance of Knowl-\nedGPT can be further improved when the GPT-2\nmodel is replaced with a larger one. Table 10 shows\nthe results. Though GPT-2 (345M) can further re-\nduce PPL, it does not bring signiÔ¨Åcant improve-\nment to F1 over GPT-2 (117M), probably because\nthe larger model can not provide more accurate\nfeedback to the knowledge selection module in\nlearning. Therefore, to balance efÔ¨Åcacy and cost,\nGPT-2 (117M) is still favored in practice.\nE Case Study\nTable 11 and Table 12 show the examples from Test\nSeen and Test Unseen of Wizard, each example\ncontains the dialogue context and the background\nknowledge which is retrieved from Wikipedia given\n3389\nthe last two turns of dialogue and the original topic.\nWe can see that KnowledGPT can locate the knowl-\nedge more accurately due to its knowledge selec-\ntion module and reinforcement learning, and make\nbetter use of the associated knowledge with the\nhelp of curriculum learning.\n3390\nKnowledge (Topic: Cinematography)...Cinematography (also called ‚Äùdirection of photography‚Äù) is the science or art of motion-picture photography by recording light or other electromagnetic radiation, either electronically bymeans of an image sensor, or chemically by means of a light-sensitive material such as Ô¨Ålm stock.Typically, a lens is used to repeatedly focus the light reÔ¨Çected from objects into real images on the light-sensitive surface inside a camera during a questioned exposure, creating multipleimages.With an electronic image sensor, this produces an electrical charge at each pixel, which is electronically processed and stored in a video Ô¨Åle for subsequent display or processing.The result with photographic emulsion is a series of invisible latent images on the Ô¨Ålm stock, which are later chemically ‚Äùdeveloped‚Äù into a visible image.The images on the Ô¨Ålm stock are played back at a rapid speed and projected onto a screen, creating the illusion of motion.Cinematography Ô¨Ånds uses in many Ô¨Åelds of science and business as well as for entertainment purposes and mass communication.The word ‚Äùcinematography‚Äù was created from the greek words (‚Äùkinema‚Äù), meaning ‚Äùmovement, motion‚Äù and (‚Äùgraphein‚Äù) meaning ‚Äùto record‚Äù, together meaning ‚Äùrecording motion.‚Äù... ContextB Thinking about taking a class on cinematography, the direction of photography .A I have always wanted to learn that .B Me too, I have learned its the science and or art of recording motion picture photography .A That is cool. did you enjoy it . ResponseTMN I think it is a great idea . It is a science Ô¨Åction Ô¨Ålm that combines computer and computer science .ITDD I did . It was developed in the late 19th century .SKT Yes , I did . I loved it . It was used to focus the light reÔ¨Çected from objects into real images .DRD I did enjoy it . It is also called ‚Äú direction of photography ‚Äù .SKT+GPT-2I did! I love how it‚Äôs so easy to focus light reÔ¨Çected from objects into real images on the light-sensitive surface inside a camera.GPT-2truncYes, I did, I love the fact that it is the science and art of recording motion picture photographyKnowledGPTYes, I did, I love cinematography, its used in many Ô¨Åelds of science and business as well as for entertainment purposes and mass communicationHumanYes and cinematography skills can also be used in mass marketing and other Ô¨Åelds as well\nTable 11: A case from Test Seen of Wizard of Wikipedia\nKnowledge(Topic: Green)...Green is the color between blue and yellow on the visible spectrum.It is evoked by light which has a dominant wavelength of roughly 495570 nm.In subtractive color systems, used in painting and color printing, it is created by a combination of yellow and blue, or yellow and cyan; in the rgb color model, used on television andcomputer screens, it is one of the additive primary colors, along with red and blue, which are mixed in different combinations to create all other colors.By far the largest contributor to green in nature is chlorophyll, the chemical by which plants photosynthesize and convert sunlight into chemical energy.Many creatures have adapted to their green environments by taking on a green hue themselves as camouÔ¨Çage.Several minerals have a green color, including the emerald, which is colored green by its chromium content.In the middle ages and renaissance, green was the color commonly associated with merchants, bankers and the gentry, while red was reserved for the nobility.For this reason, the costume of the ‚Äùmona lisa‚Äù by leonardo da vinci and the benches in the british house of commons are green while those in the house of lords are red.It also has a long historical tradition as the color of ireland and of gaelic culture.... ContextB Green is my favorite color. Many minerals are a green color, including emeralds. I like emeralds too!A I like green too. My eyes are green.B So are mine! In gaelic cultures green is very popular, with a long historical tradition. It is known as the color of ireland.A Yes I do know it is a very popular color in ireland.ResponseTMN I think it ‚Äô s a very interesting subject . It ‚Äô s a culture of culture.ITDD Yes , it is one of the primary colors .SKT It is a popular color in the usDRD It is common in nature , especially in plants .SKT+GPT-2Yes, it is very popular in gaelic culture.GPT-2truncIt is! It is also the second largest island in the british isles.KnowledGPTYes, it is! In the middle ages and renaissance, green was the color commonly associated with merchants, bankers and the gentry.HumanBack in the middle ages green was often associated with merchants and bankers. Red was nobility‚Äôs color of choice.\nTable 12: A case from Test Uneen of Wizard of Wikipedia"
}