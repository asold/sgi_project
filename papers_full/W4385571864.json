{
  "title": "Hints on the data for language modeling of synthetic languages with transformers",
  "url": "https://openalex.org/W4385571864",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3018565246",
      "name": "Rodolfo Zevallos",
      "affiliations": [
        "Pompeu Fabra University"
      ]
    },
    {
      "id": "https://openalex.org/A2129793373",
      "name": "Núria Bel",
      "affiliations": [
        "Pompeu Fabra University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4287854665",
    "https://openalex.org/W3185293939",
    "https://openalex.org/W3158607076",
    "https://openalex.org/W4287887888",
    "https://openalex.org/W4287887845",
    "https://openalex.org/W2963979492",
    "https://openalex.org/W3198757395",
    "https://openalex.org/W4385573447",
    "https://openalex.org/W3109959371",
    "https://openalex.org/W3213112269",
    "https://openalex.org/W1719940802",
    "https://openalex.org/W4360951492",
    "https://openalex.org/W2260775077",
    "https://openalex.org/W2461808544",
    "https://openalex.org/W3034510440",
    "https://openalex.org/W3214933191",
    "https://openalex.org/W2891555348",
    "https://openalex.org/W3134155512",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W3105220303",
    "https://openalex.org/W4287855085",
    "https://openalex.org/W3176198948",
    "https://openalex.org/W2007035566",
    "https://openalex.org/W2948902769",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3173954987",
    "https://openalex.org/W3105069964",
    "https://openalex.org/W3183231704",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4287890112",
    "https://openalex.org/W4285169952",
    "https://openalex.org/W3128651145",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2964114970",
    "https://openalex.org/W3103187652",
    "https://openalex.org/W3106416961",
    "https://openalex.org/W2953092638",
    "https://openalex.org/W3098613713"
  ],
  "abstract": "Language Models (LM) are becoming more and more useful for providing representations upon which to train Natural Language Processing applications. However, there is now clear evidence that attention-based transformers require a critical amount of language data to produce good enough LMs. The question we have addressed in this paper is to what extent the critical amount of data varies for languages of different morphological typology, in particular those that have a rich inflectional morphology, and whether the tokenization method to preprocess the data can make a difference. These details can be important for low-resourced languages that need to plan the production of datasets. We evaluated intrinsically and extrinsically the differences of five different languages with different pretraining dataset sizes and three different tokenization methods for each. The results confirm that the size of the vocabulary due to morphological characteristics is directly correlated with both the LM perplexity and the performance of two typical downstream tasks such as NER identification and POS labeling. The experiments also provide new evidence that a canonical tokenizer can reduce perplexity by more than a half for a polysynthetic language like Quechua as well as raising F1 from 0.8 to more than 0.9 in both downstream tasks with a LM trained with only 6M tokens.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 12508–12522\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nHints on the data for language modeling of synthetic languages with\ntransformers\nRodolfo Zevallos1 and Núria Bel1\nUniversitat Pompeu Fabra\nBarcelona, Spain\nrodolfojoel.zevallos@upf.edu, nuria.bel@upf.edu\nAbstract\nLanguage Models (LM) are becoming more\nand more useful for providing representations\nupon which to train Natural Language Process-\ning applications. However, there is now clear\nevidence that attention-based transformers re-\nquire a critical amount of language data to pro-\nduce good enough LMs. The question we have\naddressed in this paper is to what extent the\ncritical amount of data varies for languages\nof different morphological typology, in par-\nticular those that have a rich inflectional mor-\nphology, and whether the tokenization method\nto preprocess the data can make a difference.\nThese details can be important for low-resource\nlanguages that need to plan the production of\ndatasets. We evaluated intrinsically and ex-\ntrinsically the differences of five different lan-\nguages with different pretraining dataset sizes\nand three different tokenization methods for\neach. The results confirm that the size of the\nvocabulary due to morphological characteris-\ntics is directly correlated with both the LM\nperplexity and the performance of two typical\ndownstream tasks such as NER identification\nand POS Tagging. The experiments also pro-\nvide new evidence that a canonical tokenizer\ncan reduce perplexity by more than a half for a\npolysynthetic language like Quechua as well as\nraising macro-F1 score from 0.8 to more than\n0.9 in both downstream tasks with a LM trained\nwith only 6M tokens.1.\n1 Introduction\nLanguage Models (LMs) are becoming more and\nmore useful for providing representations upon\nwhich to train different Natural Language Process-\ning (NLP) applications. However, there is evidence\nthat LMs trained with attention-based transformers\nneed large quantities of pretraining language data\nto provide good enough representations that can be\nused in downstream tasks.\n1Equal contribution\nTo have very large amounts of data, multilingual\nLMs have been proposed as a solution. However,\nthere is evidence (Rust et al., 2021, Bansal et al.,\n2021, Goyal et al., 2021) that the monolingual LMs\noutperformed their multilingual counterparts.\nAs for the amount of monolingual data required,\nZhang et al. (2021) experiments with English\nshowed that the amount of data for reaching at\nleast an 80% average over several tasks of relative\nperformance is around 10M tokens. The question\nwe have addressed in our research is whether the\ncritical figures for English are the same for other\nlanguages and in particular for languages of a dif-\nferent morphological type. Having hints about the\ncritical amount of data and tokenization strategies\nto make the most profit of the available data is of up-\nmost importance for low-resource languages, many\nof them with a morphology more complex than that\nof English, and that need to plan the production of\ndatasets.\nA LM is an assessment of the probability distri-\nbution over sequences of words given a fixed set of\nwords with parameters estimated from data. The\nincrease in the number of tokens of the vocabu-\nlary of particular languages due to their inflectional\nmorphology has been demonstrated to affect the\ncoverage of the Markovian LMs (Whittaker and\nWoodland, 2003). For current attention-based trans-\nformer language models (TLM), like RoBERTa\nthat is a closed vocabulary system, the direct conse-\nquence of modeling a rich inflectional morphology\nshould also be that the coverage of the vocabulary\nwill be lower than that of a morphologically simpler\nlanguage. For instance, Mielke et al. (2019) found\nthat English was among the easiest languages for\nbuilding a LM, while German, which is a synthetic\nlanguage, was among the hardest. Polysynthetic\nlanguages like Quechua, with more than 100 inflec-\ntional suffixes, and in which up to five suffixes can\nbe attached to a verbal stem, would have harder\nmodeling problems that will aggravate its problems\n12508\nfor being a low-resource language.\nTo understand how the amount of critical pre-\ntraining data varies for different languages, we re-\nproduced Zhang et al. (2021) experiments but for\ndifferent languages of an increasing degree of mor-\nphological complexity, as measured by type-token\nratio (TTR) following Kettunen (2014) and Mielke\net al. (2019). The languages are: English, French,\nGerman, Turkish and Quechua. In Table 1, the TTR\nof these languages assessed with the 6M datasets\nused in our experiments shows the big differences\namong these languages.\nLanguage Type Tokens TTR\nEnglish 132,936 6,000,198 0.0221\nFrench 188,741 6,000,003 0.0314\nGerman 201,465 6,000,086 0.0335\nTurkish 262,531 6,000,093 0.0437\nQuechua 325,248 5,985,472 0.0543\nTable 1: Number of Tokens, Type-Tokens and Type-\nToken Ratio (TTR) for each language for the 6M dataset\nHowever, we reproduced the conditions of\nZhang et al. (2021) but with datasets of 1M, 2M,\n3M, and 6M for each language, as Quechua has\nno more corpus available. For all languages and\ndatasizes we carried out an intrinsic evaluation, i.e.\ndifferences in LM perplexity and an extrinsic eval-\nuation, i.e. to assess to what extent critical learning\ncan be achieved with representations made with\nsmaller datasets. We have used the representations\nproduced by the different models to fine-tune clas-\nsifiers for Name Entity Recognition (NER) and\nPart-of-Speech (POS) tagging.\nBesides, we repeated the different size experi-\nments with three different tokenization methods, to\nget evidence on whether a linguistically motivated\ntokenizer improves both perplexity and classifica-\ntion results. We have compared three segmenters\nthat produce subword tokenization: BPE (Sennrich\net al., 2016), Unigram (Kudo, 2018) and Deep-\nSpin (Peters and Martins, 2022). BPE is one of\nthe most used tokenizers nowadays. It initially seg-\nments the text into characters and then it iteratively\nmerges together the most frequently co-occurring\nsymbols until finding space boundaries or reach-\ning a previously set vocabulary limit. Unigram\nworks by segmenting the texts into words follow-\ning space boundaries to build an initial vocabu-\nlary, and trimming down each symbol to obtain\na shorter vocabulary list. Differently to BPE and\nUnigram, DeepSpin is a supervised canonical to-\nkenizer. Mager et al. (2020) introduced canonical\nsegmentation as a morphological segmentation that\nconsists of dividing words into their standardized\nmorphemes. A canonical tokenizer attempts to re-\ncompose the character sequence that suffers some\nmodification when concatenated or combined with\nother morphemes. For instance, in English ’prof-\nitable’ becomes ’profitably’ when combined with\nthe adverbial morpheme ’ly’. Canonical tokeniza-\ntion should produce the following tokens: ’prof-\nitable’ and ’ly’, therefore reducing the vocabulary\nsize considerably.\nThe contributions of our research are two. First,\nan evaluation of the critical amount of data for\ntraining a performant TLM. The evaluation is done\nintrinsically in terms of perplexity, and extrinsically\nby using the produced representations to fine-tune\nclassifiers for two downstream applications: POS\ntagging and NER. Second, evidence, both from\nthe intrinsic and the extrinsic evaluations, about\nhow much a linguistically motivated tokenization\nmaximizes the profit of small datasets. These hints\nmight be crucial for keeping technologically alive\nlanguages that cannot get the exorbitant amount of\ntextual data that ensures maximal performance. Be-\nsides, it is also important to get more understanding\nabout the capabilities of methods that could signif-\nicantly differ when used for languages other than\nEnglish.\n2 Related work\nHu et al. (2020) and Warstadt et al. (2020) were the\nfirst papers addressing the amount of data necessary\nfor training large LM. Hu et al. (2020) trained four\nclasses of neural models and one baseline n-gram\nmodel on four datasets derived from a newswire\ncorpus, consisting of 1M, 5M, 14M, and 42M, to\nassess differences in syntactic probing tasks among\ndifferent architectures and pretraining corpora sizes.\nThe main outcome of their experiments was to find\nout that perplexity of the LM and performance in\nthe addressed probing tasks did not correlate; that\nis, LM trained with more data, and therefore lower\nperplexity, were not better at the probing tasks.\nThey concluded that the architecture proved to be a\nmore important source of differences than the size\nof the dataset, with the GPT-2 transformer, using\nBPE, achieving best results.\nWarstadt et al. (2020) pretrained 12 RoBERTa\n(Liu et al., 2019) models on English corpora\n12509\nvarying in size and tokenized with BPE. These\nMiniBERTa models were trained with quantities of\ndata ranging of 1M, 10M, 100M, 1B. The results\nshowed that RoBERTa learns linguistic features\nwith only a few million words, but that it takes\nbillions of words for the model to prefer to use\nlinguistic generalizations over surface ones. Us-\ning the same models, Zhang et al. (2021) explored\nthe relation between the amount of data and the ef-\nfectiveness of RoBERTa for learning grammatical\nfeatures and other linguistic phenomena of English.\nThey performed an extensive collection of tests for\nshowing the learning curve on different tasks of the\nminiBERTa models pretrained with data of differ-\nent size, from 1M to 1B words. Their results show\nthat the learning for traditional NLP tasks such as\nPOS labeling, NER identification and other higher\nlevel tasks dealing with syntax and semantics occur\nwith less than 100M words of pretraining data. In\nparticular, learning for POS tagging and NER is re-\nported to happen with about 10M words, having no\nbig further improvements after that. Pérez-Mayos\net al. (2021) also used the MiniBERTas models\ndeveloped by Warstadt et al. (2020) to explore the\nrelation between the size of the pretraining data\nand the syntactic capabilities of RoBERTa. For all\nthe tasks studied, the models with more training\ndata performed better, however the performance\nimprovement growth was also stalled after 10M for\ntasks like POS tagging.\nFor languages other than English, Micheli et al.\n(2020) worked on French texts with CamemBERT\n(Martin et al., 2020) that is similar to RoBERTa but\nuses whole-word masking and SentencePiece tok-\nenization (Kudo and Richardson, 2018), which uses\nUnigram, and different pretraining data sizes. Their\nresults showed that 100 MB of raw text (about 10,5\nM words) were sufficient to reach a similar perfor-\nmance than with larger datasets on a question an-\nswering task. Micallef et al. (2022) found that 46M\ntokens of pretraining were enough for a Maltese\nBERT to be comparable with a multilingual BERT\nadapted with vocabulary augmentation methods.\nInoue et al. (2021) worked on assessing the impact\nof language variants, data sizes and fine-tuning\ntasks with Arabic pretrained TLM. They trained\n8 Arabic models, named CAMeLBERT of 6.3B,\n3.1B, 1.5B, and 636M words, that were evaluated\non different NLP tasks including NER and POS\ntagging. They concluded that the amount of pre-\ntraining data had limited and inconsistent effects\non the performance of the fine-tuned classifiers.\nHowever, note that the size of the datasets in these\nexperiments were far beyond the 10M that Warstadt\net al. (2020) or Micheli et al. (2020) identified as\nthe amount from which the model seems unable to\nlearn more.\nThe relation of the morphological type and the\nrobustness of language models because of the size\nof the vocabulary is a well known topic. A high\nnumber of words in the vocabulary is a charac-\nteristic of languages of a higher morphological\ncomplexity due to inflectional and derivational pro-\ncesses. For instance, Quechua, which is a polysyn-\nthetic language, typically has 3 morphemes per\nword and about 100 different suffixes, while En-\nglish has around 1.5 morphemes per word, and\nabout 35 suffixes. Geutner (1995) was one of the\nfirst works to afford evidence on reducing about\n50% perplexity in a statistic language model by us-\ning a morpheme-based n-gram model for the task of\nspeech recognition of German. German, in addition\nto inflection morphology, uses prefixes to create\nnew verbal tokens: ausgehen (’to go out’), hinein-\ngehen (’to go in’) and noun-noun composition is\nextremely frequent with an, in principle, unlimited\nnumber of nouns being concatenated creating new\nnouns. According to Geutner (1995), morpheme-\nbased n-gram models proved to get more robust\nprobability estimates with smaller training datasets\nand also limited the size of the vocabulary.\nMielke et al. (2019) studied whether there are\ntypological properties that make certain languages\nharder to language model than others, and studied\nlinguistic features that correlated to difficulties for\ncreating a LM. They reported on language mod-\neling results on 62 languages from 13 language\nfamilies using Bible translations, and on the 21\nlanguages used in the European Parliament pro-\nceedings. They conducted a correlational study of\nfeatures of a language to find one that is predictive\nof modeling difficulty. Their results confirmed that\nthe type inventory or vocabulary size is a statisti-\ncally significant indicator of the modeling difficulty.\nPark et al. (2021) revisited these results and per-\nformed experiments for 92 languages also from\na corpus of Bibles. Their results confirmed that\nnumber of types or size of the vocabulary of the re-\nlated TTR, are statistically correlated to difficulties\nfor language modeling. Additionally, the research\nwas extended for assessing how different segmen-\ntation methods captured morphological segments\n12510\nand the impact of tokenization in the final results.\nThe results were that subword tokenization meth-\nods outperformed character-level ones. BPE was\nreported to fail mitigating the problems created by\nlanguages with high TTR, while other segmenters\nthat were informed with linguistic information did\nbetter.\nThe gains achieved by linguistically motivated\ntokenization were also observed in other research\nareas like Machine Translation. Rust et al. (2021)\nempirically compared multilingual pretrained lan-\nguage models to their monolingual counterparts on\na set of nine typologically diverse languages. They\nconcluded that while the pretraining data size is an\nimportant factor, the tokenizer of each monolingual\nmodel plays an equally important role in the perfor-\nmance on downstream tasks. The results indicate\nthat the models trained with dedicated monolingual\ntokenizers outperform their counterparts with multi-\nlingual tokenizers in most tasks. While the smallest\nperformance gap is for POS tagging (at most 0.4%\naccuracy), performance gap for NER reaches even\n1.7 difference in macro-F1 score for Arabic. Ortega\net al. (2020), Chen and Fazio (2021), and Mager\net al. (2022) are works comparing different tok-\nenizers for improving translation in low-resource\nlanguage pairs. Their results provided evidence\nthat a linguistically motivated segmentation leads\nto significant improvements in translation quality\nspecially in low-resource contexts.\n3 Methodology\nIn our experiments, we tested 20 RoBERTa\nmodels. We pretrained from scratch LM for\nEnglish, German, French, Turkish and Quechua\nwith different pretraining datasets ranging from\n1M to 6M tokens, and we used three differ-\nent tokenizers for each: BPE, Unigram and\nDeepSpin. Code and resources are available\non https://github.com/IULATERM-TRL-UPF/\nHints-on-the-data-for-language-modeling\n3.1 Pretraining\n3.1.1 Pretraining Data\nWe pretrained RoBERTa models for the mentioned\nfive languages, following the same conditions of\nWarstadt et al. (2020) trained the miniBERTas mod-\nels for English, but further reducing the size of\ndatasets. The training data used in our pretraining\nof RoBERTa are the following.\nFor English, a random part of the Wikipedia\ncorpus of 2.5 billion tokens used by Devlin et al.\n(2019) to train BERT. For German, French and\nTurkish, we used parts of OSCAR corpora ex-\ntracted from the Common Crawl November 2018\nsnapshot, automatically classified for language\nidentification and filtered to avoid noise (Ortiz\nSuárez et al., 2019). The German OSCAR, with\n21 billion tokens, was the one used by Scheible\net al. (2020), the French OSCAR, with 32.7 bil-\nlion tokens, the one that was used by Martin et al.\n(2020) and the Turkish OSCAR with 11.5 million\ndocuments, the one that was used by Toraman et al.\n(2022). Monolingual-quechua-iic2 in Quechua (6\nmillion tokens) used by Zevallos et al. (2022). This\nQuechua corpus is composed of a wide variety of\nsources, including Wikipedia (about 1 million to-\nkens) and other resources available on the Internet,\nas well as educational materials and legal docu-\nments. For each language, we randomly produced\ntraining sets with a total of 1M, 2M, 3M, and 6M\ntokens each.\n3.1.2 Tokenization\nFor our experiments we compared three different\ntokenizers: BPE, Unigram and DeepSpin. Similar\nto the experiments performed by Liu et al. (2019)\nto train RoBERTa, we used BPE (Sennrich et al.,\n2016) as a baseline. Moreover, we have used the\nmethods that have obtained the best results with\nlanguages of different types of morphology. We\nused Unigram (Kudo, 2018) because it is consid-\nered the best unsupervised and statistically moti-\nvated method, as it has obtained interesting results\nfor both morphologically complex languages (e.g.\nQuechua) and non-complex languages (e.g. En-\nglish) (Gow-Smith et al., 2022). In the case of\ncanonical and linguistically motivated methods, we\nchose DeepSpin (Peters and Martins, 2022), which\nis the winner of SIGMORPHON 2022 (Batsuren\net al., 2022), achieving very interesting results and\nsuperior to others of the same type of tokenization.\nBecause DeepSpin is a supervised model, it is\nnecessary to train a model for each language. The\ndata used to train the English and French model\nwere obtained from SIGMORPHON3 2022 itself,\nthe German data from the experiments performed\nby Cotterell et al. (2016). The Turkish and Quechua\ntraining data were created by ourselves for these\n2Dataset from https://huggingface.co/datasets/\nLlamacha/monolingual-quechua-iic\n3Dataset from https://github.com/sigmorphon/\n2022SegmentationST/tree/main/data\n12511\nexperiments. The Turkish raw data was obtained\nfrom Alecakir et al. (2022), and the Quechua raw\ndata was obtained from Melgarejo et al. (2022). All\nmodels were trained with the same hyperparame-\nters as DeepSpin-Base (Peters and Martins, 2022).\nIn Table 2, we show the data obtained from the\ntrained DeepSpin models of each language.\nLanguage Annotated words Accuracy\nEnglish 458k 0.92\nFrench 382k 0.94\nGerman 8k 0.83\nTurkish 2k 0.75\nQuechua 1k 0.72\nTable 2: Annotated dataset size and DeepSpin tokeniza-\ntion accuracy were considered in this study. For each\nlanguage, DeepSpin was trained using an 80/10/10 split\nfor training, validation, and testing, respectively.\n3.1.3 Hyperparameters\nTo replicate what Warstadt et al. (2020) did for\ndata smaller than 10M tokens, we used the hyper-\nparameters from their Med-Small model, which\nhad 8 attention heads, 512 hidden size, 2048 feed-\nforward network dimension, and 45M parameters.\nNote that we have also set the vocabulary size to\n52,000 tokens just like most experiments in lan-\nguage model development with transformers. This\nsize of 52k tokens is due to a computational lim-\nitation when processing the data. In addition, we\nadopted the same parameter values for dropout,\nattention dropout and learning rate decrease. All\nparameters are described in Table 3.\nDescription Value\nNumber of attention heads 8\nHidden size 512\nFeed-forward network dimension 2048\nNumber of parameters 45M\nMax Steps 10K\nBatch Size 512\nDropout 0.1\nAttention dropout 0.1\nLearning rate decrease 5E-4\nTable 3: Common parameters for the pretraining of the\n20 models used in our experiments.\n3.2 Fine-Tuning\nFrom the pretrained RoBERTa models, and still fol-\nlowing Zhang et al. (2021), we generated represen-\ntations of the token span and trained classifiers that\npredict whether a given label correctly describes\nthe input span for NER and POS.\nIn order to obtain the best and validated results\nin both tasks, we performed a 10-fold macro-F1\nscore cross-validation. In addition, we chose to\nadjust some hyperparameters guided by Zhang et al.\n(2021): learning rate ∈ {1E-5, 2E-5, 3E-5, 4E-5}\nand batch size ∈ {16, 32, 48}.\nIn POS tagging, we used a different head with a\nclassification output for each token, all triggered by\na softmax function just like Delobelle et al. (2020).\nAlso, when a word consists of multiple tokens, the\nfirst token is used for the word tag. The xtreme 4\n(Conneau et al., 2018) datasets were used for the\nPOS task and wikiann5 (Rahimi et al., 2019) for the\nNER of English, German, French, and Turkish. For\nQuechua6, the dataset provided by Zevallos et al.\n(2022) was used for both tasks. For evaluating the\nNER and POS tasks, we used macro-F1 score.\n4 Results\nOur research aimed on the one hand at making an\nevaluation of the amount of pretraining data and the\nrole of the tokenizer measured in terms of LM per-\nplexity. On the other hand, the POS and NER tasks\nwere meant to assess the quality of the represen-\ntations produced when used in fine-tuning down-\nstream tasks. It is important to mention that we\ndid not perform any normalization in the results as\nopposed to Warstadt et al. (2020), because we also\nwanted to see a comparison between languages.\n4.1 Pretrained models\nThe results per language plotted in Figure 1 show\nthat for all cases the DeepSpin tokenization method\nsubstantially improves the perplexity of all LM, but\nit is in the case of Turkish and Quechua that it drasti-\ncally improves the perplexity from 162.47 to 94.93\nand 210.14 to 102.73 respectively. English LM ob-\ntained 53.51, being the lowest perplexity in all the\nconfigurations performed in the experiments. Com-\nparing BPE and Unigram, only English, German\nand French achieved better results, while Turkish\nand Quechua also achieved better results using Un-\nigram.\nWe can see that the datasize amounts that are crit-\nical for modeling English (Warstadt et al., 2020)\n4https://huggingface.co/datasets/xtreme\n5https://huggingface.co/datasets/wikiann\n6https://github.com/Llamacha/QuBERT/tree/main/\nresource\n12512\nFigure 1: Perplexity for each language model, training data size (1M, 2M, 3M and 6M) and tokenizer. Numeric data\ncan be found in the Appendix.\nFigure 2: Macro-F1 score NER and POS results for each language and data sizes (1M, 2M, 3M and 6M) of the\npretrained models.\nare quite different for other languages. Despite\nhaving the same training data size and using the\nsame training hyperparameters and vocabulary lim-\nitations, the results in terms of LM perplexity are\nvery different. The perplexity of the Turkish and\nQuechua language models is around twice the per-\nplexity of the English LM with 6M with all the\ntokenizers. In the appendix we show all the re-\nsults of the pretrained models according to type of\ntokenization.\n4.2 Part-of-Speech Tagging\nWe evaluated POS tagging task for each language\nwith the different training sizes and different tok-\nenization methods. For all models, the same hyper-\nparameters mentioned in 3.2 are used. In Table 4\nand Figure 2 we can see the results for all dataset\nsizes for each language and the different tokeniza-\ntion methods.\nZhang et al. (2021) found that POS labelling was\none of the taks whose learning curve rises earlier\nand gets around 90% macro-F1 score with less\nthan 10M of training dataset. Our results, in Table\n4, show the same trend, with English, German and\nFrench getting macro-F1 score higher than 90%\nwith a corpus of 6M and the three tokenizers. For\nTurkish and Quechua, BPE tokenization is the only\none that cannot achieve a 90% macro-F1 score.\nAs can be seen in the Table 4, for all lan-\nguages and the 6M dataset, using the DeepSpin\ntokenizer delivers statistically significant improve-\nments7 both when compared to BPE, that works\nbetter for English, German and French, and when\ncompared to Unigram, that, as expected, works bet-\nter for Turkish and Quechua. What is more interest-\ning is that for Turkish and Quechua with DeepSpin\nbetter results are obtained with 3M words than BPE\nwith 6M, showing the importance of the tokenizer\nselection for synthetic languages.\n7Sign test, p < 0.05\n12513\nLanguage BPE Unigram DeepSpin\n1M 2M 3M 6M 1M 2M 3M 6M 1M 2M 3M 6M\nEnglish 0.74 0.79 0.87 0.96 0.70 0.75 0.84 0.91 0.82 0.85 0.90 0.99\nGerman 0.70 0.77 0.84 0.93 0.65 0.70 0.79 0.90 0.79 0.84 0.89 0.98\nFrench 0.70 0.75 0.82 0.94 0.66 0.71 0.79 0.91 0.81 0.83 0.87 0.97\nTurkish 0.58 0.64 0.71 0.85 0.63 0.69 0.75 0.90 0.75 0.80 0.85 0.95\nQuechua 0.53 0.59 0.69 0.81 0.60 0.66 0.76 0.89 0.73 0.79 0.84 0.94\nTable 4: Macro-F1 score results of the POS tagging task for each language, using the subset of 1M, 2M, 3M and 6M\nwords and three different tokenization methods.\nLanguage BPE Unigram DeepSpin\n1M 2M 3M 6M 1M 2M 3M 6M 1M 2M 3M 6M\nEnglish 0.79 0.82 0.85 0.94 0.72 0.76 0.82 0.91 0.83 0.87 0.92 0.98\nGerman 0.81 0.83 0.85 0.91 0.77 0.80 0.83 0.87 0.87 0.89 0.93 0.97\nFrench 0.74 0.80 0.85 0.92 0.69 0.77 0.84 0.89 0.79 0.83 0.89 0.97\nTurkish 0.53 0.58 0.65 0.80 0.61 0.67 0.74 0.85 0.70 0.76 0.85 0.92\nQuechua 0.42 0.61 0.69 0.81 0.51 0.68 0.79 0.85 0.68 0.73 0.84 0.91\nTable 5: Macro-F1 score results of the NER task for each language, using the subset of 1M, 2M, 3M and 6M words\nand three different tokenization methods.\n4.3 Named Entity Recognition\nIn Figure 2 we can see the results for all dataset\nsizes for each language and the different tokeniza-\ntion methods (figures can be found in Table 5).\nFor NER tasks, Zhang et al. (2021) results showed\nthat the learning curve still raised between 10M\nand 100M datasets before stalling. Our results\nshow that the learning curve for NER is sharper\nthan for POS tagging: it needs more data for all\nlanguages, but again Turkish and Quechua having\nmore difficulties in all cases. However, when us-\ning the DeepSpin tokenizer, statistically significant\nimprovements are achieved for each language with\nall datasizes. In the case of Turkish and Quechua,\nDeepSpin achieves the same macro-F1 score results\nthan Unigram with the 3M dataset, and improves\nBPE results with the 6M dataset.\n5 Discussion\nIn order to clarify the amount of data necessary\nto achieve robust performance measured by LM\nperplexity, we experimented with four training data\nsizes: 1M, 2M, 3M and 6M tokens. We were in-\nterested in two main issues. First, in the work of\nWarstadt et al. (2020) it can be seen that perplexity\nimproves dramatically when the training data size\nis above 10M, however low-resource languages like\nQuechua do not even have texts amounting 10M to-\nkens. We were interested in finding whether there is\na critical amount of data with which it is worth for\nlow-resource languages to build a TLM. Second,\nwe wanted to show to what extent LM perplexity\nand the fine-tuning of downstream tasks are influ-\nenced by the size of the data and the morphological\ntypology of languages, and whether tokenization\ncould mitigate these issues.\nFrom our results it is clear that in spite of being\ntrained with the same configurations and amount\nof training data, there are differences among the\nlanguages we examined. Mielke et al. (2019) sug-\ngested that these differences could be due to the\ndifference in morphological complexity between\nthese languages. A rich inflectional morphology\nincreases the vocabulary. As we can see in Table\n6, tokenizers that try to identify the compositional\ncharacteristics of morphology can significantly re-\nduce the vocabulary size. Therefore, the drastic\nimprovement in the perplexity results for Quechua,\nwith perplexity 210 with BPE and 102 with Deep-\nSpin, is due to the fact that DeepSpin manages\nto reduce the vocabulary thanks to a linguistically\nmotivated segmentation.\nWe also wanted to get evidence about the qual-\nity of the representations obtained by our differ-\nent TLM for fine-tuning downstream tasks. The\nresults shown in Table 4 and Table 5 show that rep-\nresentations get better with more data, but a TLM\ntrained with dataset of 6M tokens and a using a\nlinguistically motivated tokenizer can deliver very\n12514\nLanguage\nBPE Unigram DeepSpin\n1M 6M 1M 6M 1M 6M\nV oc. TTR V oc. TTR V oc. TTR V oc. TTR V oc. TTR V oc. TTR\nEnglish 20.3 0.203 51 0.084 30.1 0.301 51.3 0.085 14.1 0.141 16.2 0.027\nFrench 20.9 0.209 52 0.085 30.7 0.307 51.6 0.086 14.2 0.142 22.8 0.038\nGerman 21.5 0.215 52 0.085 31.4 0.314 51.6 0.086 14.7 0.147 25.2 0.042\nTurkish 21.9 0.219 52 0.086 32.1 0.321 52 0.086 15.1 0.151 28.2 0.047\nQuechua 22.1 0.221 52 0.086 33.4 0.334 52 0.086 15.3 0.153 32.2 0.053\nTable 6: V ocabulary size (V oc.) and Type-Token Ratio (TTR) of each language according to the size of the training\ndata and the tokenization method. TTR is multiplied by 103 and V oc. is divided by 10 to better appreciate the\nresults.\ncompetitive results for tasks like POS tagging and\nNER.\n6 Conclusions\nIn this paper we have related the quality of TLM\nwith the training data size. We have approached\nthe topic from the point of view of low-resource\nlanguages that need to maximize the available data.\nWe have demonstrated how different methods, in\nthis case tokenizers, apply to languages other than\nEnglish. We have evaluated intrinsically and extrin-\nsically the impact of datasize and tokenization with\nthe aim of giving some hints for the building of\nTLM for low-resource languages, in particular for\nthose whose morphology processes produces large\nvocabularies. These hints are explaining below.\n6.1 How much data is enough?\nIn our experiments, all languages show a continu-\nous reduction of perplexity when from 1M to 6M\ntokens, with no stagnation. Regardless of language\ntype, the decrease in perplexity progresses as the\nmodel is trained with more data, suggesting that\nit can still improve more with more data. How-\never, we provide evidence on the fact that with\n6M all the languages in our experiments, but Turk-\nish and Quechua, could reach a perplexity below\n100, and macro-F1 score higher than 0.9 in the two\ndownstream tasks. With a linguistically motivated\nand canonical tokenizer like DeepSpin, Turkish and\nQuechua could also attain these competitive results,\nas explained below.\n6.2 Which tokenizer to use?\nTokenization methods play an important role in\nbuilding pretrained models (Rust et al., 2021). As\nseen in our experiments, canonical and linguisti-\ncally motivated tokenizers achieve astonishing re-\nsults compared to other types of tokenizers. The re-\nduction by almost 50% of the perplexity of the pre-\nentangled models of Turkish and Quechua when\nusing DeepSpin instead of BPE is impressive. Lan-\nguages morphologically different from Turkish and\nQuechua also showed significant benefits, e.g., En-\nglish, French and German showed an improvement\nof 15%, 31% and 24% respectively.\nOn the other hand, it can also be seen that using\nDeepSpin results in significant improvements in\ntasks such as NER and POS tagging. Both Turk-\nish and Quechua manage to increase the macro-F1\nscore by 0.1 and 0.14 respectively. English, French\nand German also manage to increase the macro-F1\nscore by 0.03 in most cases.\nFinally, we can say that canonical and linguisti-\ncally motivated tokenization methods present sta-\ntistically significant improvements when working\nwith morphologically complex languages com-\npared to statistically motivated methods such as\nBPE and Unigram.\n7 Limitations\nWe have limited ourselves to experimenting with\nonly five languages due to lack of data for both\nthe pretrained models and the DeepSpin tokenizer\nmodels. Although there are annotated data for\nsome low-resource polysynthetic languages such\nas Nahuatl, Raramuri, Wixarika, Shipibo-Konibo\n(Mager et al., 2020) and Kunwinjku (Pimentel et al.,\n2021), the available data was below 1M and there-\nfore not enough to create pretrained models for our\nexperiments.\nRegarding the aforementioned limitation, Deep-\nSpin which has proven to be a good option to miti-\ngate the problem of high TTR languages in closed\nvocabulary environments is a supervised method\nthat requires the availability of training data. As\n12515\ncan be seen in Table 2, to achieve 90% to better\naccuracy DeepSpin requires around 350K anno-\ntated words. This can be a major drawback for\nlow-resource languages, although the results with\nless annotated data are still competitive. We have\nnot studied another source of differences in the vo-\ncabulary size that could be due to the texts used in\npretraining. Ortiz Suárez et al. (2019) found that, in\ngeneral, the OSCAR samples contain more vocab-\nulary words than the Wikipedia ones. Additionally,\nthe Quechua corpus we have used also consists of\neducational and legal texts that can increase the\nnumber of different types, compared to Wikipedia\ntexts.\nOn the other hand, we believe it is important to\nmention that for the Quechua language the train-\ning, evaluation, and testing data for NER and POS\ntasks were obtained from the same corpus used for\ntraining the language model. Note that, due to the\nscarcity of available digital and physical texts in\nthat language, it is difficult to do it otherwise. The\nlimited availability of texts leads to the use of the\nsame corpus for multiple tasks, which could have\nimplications on the evaluation of the obtained re-\nsults. For instance, if the training corpus contains\nan unequal proportion of certain types of gram-\nmatical structures, it might negatively affect the\nperformance of POS classifiers. Furthermore, if\nthe corpus does not adequately reflect the linguistic\nvariability and diversity of Quechua, the resulting\nmodels are likely to be less accurate and less gen-\neralizable.\nEthical Considerations\nThe datasets used in this paper for the training and\nevaluations of the pre-trained models, DeepSpin\nmodels, and fine-tuned models have been extracted\nfrom various previous articles and open-access\nrepositories, therefore, we abide by the ethical rules\nby citing the original authors of each dataset. On\nthe other hand, the annotated Turkish and Quechua\ndata that were constructed by us for the develop-\nment of the DeepSpin models will be presented in\na forthcoming paper for public use. In addition, we\nencourage authors who use the resources in this ar-\nticle to cite the original sources. Finally, we would\nlike to note that one of the authors of this paper has\na long history of working with resource-poor syn-\nthetic languages, especially Quechua, which allows\nus to better understand the problems and concerns\nof the Quechua-speaking communities.\nAcknowledgements\nThis research was partially funded by the project\nLUTEST, Project PID2019-104512GB-I00, Minis-\nterio de Ciencia, Innovación y Universidades and\nAgencia Estatal de Investigación (Spain). The first\nauthor has been supported by a FI grant of the Cata-\nlan Funding Agency for Research and Universities\n(AGAUR).\nReferences\nHuseyin Alecakir, Necva Bölücü, and Burcu Can. 2022.\nTurkishDelightNLP: A neural Turkish NLP toolkit.\nIn Proceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies:\nSystem Demonstrations, pages 17–26, Hybrid: Seat-\ntle, Washington + Online. Association for Computa-\ntional Linguistics.\nRachit Bansal, Himanshu Choudhary, Ravneet Punia,\nNiko Schenk, Jacob L. Dahl, and Émilie Pagé-Perron.\n2021. How low is too low? A computational perspec-\ntive on extremely low-resource languages. CoRR,\nabs/2105.14515.\nKhuyagbaatar Batsuren, Gábor Bella, Aryaman Arora,\nViktor Martinovic, Kyle Gorman, Zdenˇek Žabokrt-\nský, Amarsanaa Ganbold, Šárka Dohnalová, Magda\nŠevˇcíková, Kateˇrina Pelegrinová, Fausto Giunchiglia,\nRyan Cotterell, and Ekaterina Vylomova. 2022. The\nSIGMORPHON 2022 shared task on morpheme seg-\nmentation. In Proceedings of the 19th SIGMOR-\nPHON Workshop on Computational Research in Pho-\nnetics, Phonology, and Morphology, pages 103–116,\nSeattle, Washington. Association for Computational\nLinguistics.\nWilliam Chen and Brett Fazio. 2021. Morphologically-\nguided segmentation for translation of agglutinative\nlow-resource languages. In Proceedings of the 4th\nWorkshop on Technologies for MT of Low Resource\nLanguages (LoResMT2021), pages 20–31, Virtual.\nAssociation for Machine Translation in the Americas.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Ad-\nina Williams, Samuel Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018. Xnli: Evaluating cross-\nlingual sentence representations. In Proceedings of\nthe 2018 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2475–2485.\nRyan Cotterell, Tim Vieira, and Hinrich Schütze. 2016.\nA joint model of orthography and morphological seg-\nmentation. In Proceedings of the 2016 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 664–669.\nPieter Delobelle, Thomas Winters, and Bettina Berendt.\n2020. Robbert: a Dutch roberta-based language\n12516\nmodel. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 3255–3265.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), pages 4171–\n4186.\nP. Geutner. 1995. Using morphology towards better\nlarge-vocabulary speech recognition systems. In\n1995 International Conference on Acoustics, Speech,\nand Signal Processing, volume 1, pages 445–448\nvol.1.\nEdward Gow-Smith, Harish Tayyar Madabushi, Car-\nolina Scarton, and Aline Villavicencio. 2022. Improv-\ning tokenisation by alternative treatment of spaces.\narXiv preprint arXiv:2204.04058.\nNaman Goyal, Jingfei Du, Myle Ott, Giri Ananthara-\nman, and Alexis Conneau. 2021. Larger-scale trans-\nformers for multilingual masked language modeling.\nIn Proceedings of the 6th Workshop on Represen-\ntation Learning for NLP (RepL4NLP-2021), pages\n29–33.\nJennifer Hu, Jon Gauthier, Peng Qian, Ethan Wilcox,\nand Roger Levy. 2020. A systematic assessment\nof syntactic generalization in neural language mod-\nels. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n1725–1744, Online. Association for Computational\nLinguistics.\nGo Inoue, Bashar Alhafni, Nurpeiis Baimukan, Houda\nBouamor, and Nizar Habash. 2021. The interplay\nof variant, size, and task type in Arabic pre-trained\nlanguage models. In Workshop on Arabic Natural\nLanguage Processing.\nKimmo Kettunen. 2014. Can type-token ratio be used to\nshow morphological complexity of languages? Jour-\nnal of Quantitative Linguistics, 21:223–245.\nTaku Kudo. 2018. Subword regularization: Improv-\ning neural network translation models with multiple\nsubword candidates. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 66–75.\nTaku Kudo and John Richardson. 2018. Sentencepiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2018: System Demonstrations, Brussels, Belgium,\nOctober 31 - November 4, 2018, pages 66–71. Asso-\nciation for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nManuel Mager, Özlem Çetino˘glu, and Katharina Kann.\n2020. Tackling the low-resource challenge for canon-\nical segmentation. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 5237–5250, Online. As-\nsociation for Computational Linguistics.\nManuel Mager, Arturo Oncevay, Elisabeth Mager,\nKatharina Kann, and Thang Vu. 2022. BPE vs. mor-\nphological segmentation: A case study on machine\ntranslation of four polysynthetic languages. In Find-\nings of the Association for Computational Linguistics:\nACL 2022, pages 961–971, Dublin, Ireland. Associa-\ntion for Computational Linguistics.\nLouis Martin, Benjamin Muller, Pedro Javier Or-\ntiz Suárez, Yoann Dupont, Laurent Romary, Éric\nde la Clergerie, Djamé Seddah, and Benoît Sagot.\n2020. CamemBERT: a tasty French language model.\nIn Proceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 7203–\n7219, Online. Association for Computational Lin-\nguistics.\nNelsi Melgarejo, Rodolfo Zevallos, Hector Gomez, and\nJohn E. Ortega. 2022. WordNet-QU: Development of\na lexical database for Quechua varieties. In Proceed-\nings of the 29th International Conference on Com-\nputational Linguistics, pages 4429–4433, Gyeongju,\nRepublic of Korea. International Committee on Com-\nputational Linguistics.\nKurt Micallef, Albert Gatt, Marc Tanti, Lonneke van der\nPlas, and Claudia Borg. 2022. Pre-training data qual-\nity and quantity for a low-resource language: New\ncorpus and BERT models for Maltese. In Proceed-\nings of the Third Workshop on Deep Learning for\nLow-Resource Natural Language Processing, pages\n90–101, Hybrid. Association for Computational Lin-\nguistics.\nVincent Micheli, Martin d’Hoffschmidt, and François\nFleuret. 2020. On the importance of pre-training data\nvolume for compact language models. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages\n7853–7858, Online. Association for Computational\nLinguistics.\nSabrina J. Mielke, Ryan Cotterell, Kyle Gorman, Brian\nRoark, and Jason Eisner. 2019. What kind of lan-\nguage is hard to language-model? In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 4975–4989, Florence,\nItaly. Association for Computational Linguistics.\nJohn E. Ortega, Richard Castro Mamani, and\nKyunghyun Cho. 2020. Neural machine translation\nwith a polysynthetic low resource language. Machine\nTranslation, 34(4):325–346.\n12517\nPedro Javier Ortiz Suárez, Benoît Sagot, and Laurent\nRomary. 2019. Asynchronous pipelines for process-\ning huge corpora on medium to low-resource infras-\ntructures. Proceedings of the Workshop on Chal-\nlenges in the Management of Large Corpora (CMLC-\n7) 2019. Cardiff, 22nd July 2019, pages 9 – 16,\nMannheim. Leibniz-Institut für Deutsche Sprache.\nHyunji Hayley Park, Katherine J. Zhang, Coleman Ha-\nley, Kenneth Steimel, Han Liu, and Lane Schwartz.\n2021. Morphology matters: A multilingual language\nmodeling analysis. Transactions of the Association\nfor Computational Linguistics, 9:261–276.\nLaura Pérez-Mayos, Miguel Ballesteros, and Leo Wan-\nner. 2021. How much pretraining data do language\nmodels need to learn syntax? In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 1571–1582, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nBen Peters and Andre F. T. Martins. 2022. Beyond char-\nacters: Subword-level morpheme segmentation. In\nProceedings of the 19th SIGMORPHON Workshop\non Computational Research in Phonetics, Phonology,\nand Morphology, pages 131–138, Seattle, Washing-\nton. Association for Computational Linguistics.\nTiago Pimentel, Maria Ryskina, Sabrina J. Mielke,\nShijie Wu, Eleanor Chodroff, Brian Leonard, Gar-\nrett Nicolai, Yustinus Ghanggo Ate, Salam Khalifa,\nNizar Habash, Charbel El-Khaissi, Omer Goldman,\nMichael Gasser, William Lane, Matt Coler, Arturo\nOncevay, Jaime Rafael Montoya Samame, Gema Ce-\nleste Silva Villegas, Adam Ek, Jean-Philippe\nBernardy, Andrey Shcherbakov, Aziyana Bayyr-ool,\nKarina Sheifer, Sofya Ganieva, Matvey Plugaryov,\nElena Klyachko, Ali Salehi, Andrew Krizhanovsky,\nNatalia Krizhanovsky, Clara Vania, Sardana Ivanova,\nAelita Salchak, Christopher Straughn, Zoey Liu,\nJonathan North Washington, Duygu Ataman, Witold\nKiera´s, Marcin Woli´nski, Totok Suhardijanto, Niklas\nStoehr, Zahroh Nuriah, Shyam Ratan, Francis M.\nTyers, Edoardo M. Ponti, Grant Aiton, Richard J.\nHatcher, Emily Prud’hommeaux, Ritesh Kumar,\nMans Hulden, Botond Barta, Dorina Lakatos, Gá-\nbor Szolnok, Judit Ács, Mohit Raj, David Yarowsky,\nRyan Cotterell, Ben Ambridge, and Ekaterina Vy-\nlomova. 2021. SIGMORPHON 2021 shared task\non morphological reinflection: Generalization across\nlanguages. In Proceedings of the 18th SIGMOR-\nPHON Workshop on Computational Research in Pho-\nnetics, Phonology, and Morphology, pages 229–259,\nOnline. Association for Computational Linguistics.\nAfshin Rahimi, Yuan Li, and Trevor Cohn. 2019. Mas-\nsively multilingual transfer for ner. arXiv preprint\narXiv:1902.00193.\nPhillip Rust, Jonas Pfeiffer, Ivan Vuli´c, Sebastian Ruder,\nand Iryna Gurevych. 2021. How good is your tok-\nenizer? on the monolingual performance of multilin-\ngual language models. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 3118–3135, Online. Association\nfor Computational Linguistics.\nRaphael Scheible, Fabian Thomczyk, Patric Tippmann,\nVictor Jaravine, and Martin Boeker. 2020. Gottbert:\na pure German language model. arXiv preprint\narXiv:2012.02110.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1715–1725.\nCagri Toraman, Eyup Halit Yilmaz, Furkan ¸ Sahinuç,\nand Oguzhan Ozcelik. 2022. Impact of tokenization\non language models: An analysis for Turkish. arXiv\npreprint arXiv:2204.08832.\nAlex Warstadt, Yian Zhang, Xiaocheng Li, Haokun Liu,\nand Samuel R. Bowman. 2020. Learning which fea-\ntures matter: RoBERTa acquires a preference for\nlinguistic generalizations (eventually). In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages\n217–235, Online. Association for Computational Lin-\nguistics.\nEdward Whittaker and Philip Woodland. 2003. Lan-\nguage modelling for Russian and English using words\nand classes [computer speech and language 17 (2003)\n87–104]. Computer Speech & Language, 17:415.\nRodolfo Zevallos, John Ortega, William Chen, Richard\nCastro, Núria Bel, Cesar Toshio, Renzo Venturas,\nHilario Aradiel, and Nelsi Melgarejo. 2022. Intro-\nducing QuBERT: A large monolingual corpus and\nBERT model for Southern Quechua. In Proceedings\nof the Third Workshop on Deep Learning for Low-\nResource Natural Language Processing, pages 1–13,\nHybrid. Association for Computational Linguistics.\nYian Zhang, Alex Warstadt, Xiaocheng Li, and Samuel\nBowman. 2021. When do you need billions of words\nof pretraining data? In Proceedings of the 59th An-\nnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 1112–1125.\nA Appendices\nA.1 Model and training procedure: details\nTo train language models for each language, we fol-\nlowed the choices by Warstadt et al. (2020) for their\nRoBERTa Med-Small model with 45M parameters,\nbased on the amount of training data (<10M).\nWe ran all training in parallel on five servers,\nwith each language on a separate server. All servers\nwere equipped with an Intel Xeon E5-2650 v4 CPU\n(12 cores, 2.2GHz 30MB Cache 2400MHz 105W)\n12518\nand a Gigabyte Geforce GTX 1080 Ti TURBO\n11.72GB GPU. We trained each model for 10k\nsteps, and the training time varied depending on\nthe amount of training data. The models trained\non 1M, 2M, 3M, and 6M took 16 hours, 1 day,\n2 days, and 3 days, respectively. The entire LM\ncreation experiment took approximately 7 days.\nFine-tuning the POS and NER models for Quechua\ntook 2 days; for Turkish, it took 4 days; for French\nand German took 5 days each, and for English,\nit took 10 days. We performed each fine-tuning\nprocess using 1k steps and each fine-tuning process\nwas carried out on the same server that was used to\ntrain the language model.\nA.2 Experiment results\nThe following Tables show the perplexity of the\ndifferent language models trained with different to-\nkenization methods and amount of training tokens.\nTable 7 shows perplexity of the language mod-\nels that used Unigraman as a tokenization method,\nwhile Table 8 shows perplexity with BPE and Table\n9 with DeepSpin.\n12519\nLanguage Tokens (Millions) Perplexity Language Tokens (Millions) Perplexity\nEnglish\n1\n153.38 English\n3\n109.07\nGerman 194.62 German 125.83\nFrench 231.03 French 147.15\nTurkish 328.91 Turkish 205.83\nQuechua 375.17 Quechua 297.29\nEnglish\n2\n121.14 English\n6\n62.15\nGerman 143.33 German 73.21\nFrench 199.80 French 91.72\nTurkish 267.22 Turkish 162.47\nQuechua 335.41 Quechua 210.14\nTable 7: Perplexity for each language and training data size using the BPE tokenization method.\nLanguage Tokens (Millions) Perplexity Language Tokens (Millions) Perplexity\nEnglish\n1\n165.72 English\n3\n133.37\nGerman 225.13 German 168.11\nFrench 242.15 French 161.5\nTurkish 302.24 Turkish 178.61\nQuechua 343.61 Quechua 264.82\nEnglish\n2\n158.91 English\n6\n115.82\nGerman 193.06 German 106.62\nFrench 208.35 French 110.80\nTurkish 241.77 Turkish 131.09\nQuechua 301.09 Quechua 182.35\nTable 8: Perplexity for each language and training data size using the Unigram tokenization method.\nLanguage Tokens (Millions) Perplexity Language Tokens (Millions) Perplexity\nEnglish\n1\n141.77 English\n3\n85.42\nGerman 164.39 German 92.61\nFrench 193.16 French 128.19\nTurkish 227.11 Turkish 146.38\nQuechua 250.18 Quechua 164.25\nEnglish\n2\n111.13 English\n6\n53.51\nGerman 138.28 German 55.53\nFrench 170.03 French 63.28\nTurkish 191.88 Turkish 94.93\nQuechua 203.15 Quechua 102.73\nTable 9: Perplexity for each language and training data size using the DeepSpin tokenization method.\n12520\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\n7\n□\u0017 A2. Did you discuss any potential risks of your work?\nWe have not created any system or any dataset that can have a potential missuse.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nAbstract and 1. Introduction\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\n3\n□\u0013 B1. Did you cite the creators of artifacts you used?\n3\n□\u0017 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nLeft blank.\n□\u0017 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nWe only used very common systems whose inteded use is well-known and we have used them in a\nstandard way.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\n3\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\n3\nC □\u0013 Did you run computational experiments?\n3\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\n3\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n12521\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nWe mention the search parameters in section 3 and appendices\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\n4\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\n3\nD □\u0013 Did you use human annotators (e.g., crowdworkers) or research with human participants?\n3\n□\u0017 D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nThe annotation was not the focus of the paper.\n□\u0017 D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nThe annotation was not the focus of the paper.\n□\u0017 D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nThe annotation was not the focus of the paper.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n12522",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.9636493921279907
    },
    {
      "name": "Lexical analysis",
      "score": 0.859732449054718
    },
    {
      "name": "Computer science",
      "score": 0.8420524597167969
    },
    {
      "name": "Transformer",
      "score": 0.6550341844558716
    },
    {
      "name": "Natural language processing",
      "score": 0.6370396018028259
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5804284811019897
    },
    {
      "name": "Language model",
      "score": 0.5461968779563904
    },
    {
      "name": "Vocabulary",
      "score": 0.5011579990386963
    },
    {
      "name": "Identification (biology)",
      "score": 0.4125255346298218
    },
    {
      "name": "Linguistics",
      "score": 0.25505349040031433
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Botany",
      "score": 0.0
    }
  ]
}