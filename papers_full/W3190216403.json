{
    "title": "CrossFormer: A Versatile Vision Transformer Based on Cross-scale Attention",
    "url": "https://openalex.org/W3190216403",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2117432714",
            "name": "Wenxiao Wang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2103997928",
            "name": "Yao Lu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2098309033",
            "name": "Long Chen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2141500565",
            "name": "Deng Cai",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2119676268",
            "name": "Xiaofei He",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1973321923",
            "name": "Wei Liu",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2963150697",
        "https://openalex.org/W3164208409",
        "https://openalex.org/W3139773203",
        "https://openalex.org/W3102631365",
        "https://openalex.org/W2963925437",
        "https://openalex.org/W2737258237",
        "https://openalex.org/W3166942762",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3172752666",
        "https://openalex.org/W3121523901",
        "https://openalex.org/W2331143823",
        "https://openalex.org/W2998508940",
        "https://openalex.org/W3119786062",
        "https://openalex.org/W3165150763",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W2992308087",
        "https://openalex.org/W3131500599",
        "https://openalex.org/W3172509117",
        "https://openalex.org/W3138796575",
        "https://openalex.org/W2884561390",
        "https://openalex.org/W1665214252",
        "https://openalex.org/W2950141105",
        "https://openalex.org/W2884822772",
        "https://openalex.org/W2963399829",
        "https://openalex.org/W2117539524",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W3164540605",
        "https://openalex.org/W2910628332",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W3168825659",
        "https://openalex.org/W3151130473",
        "https://openalex.org/W3037932933",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3098903812",
        "https://openalex.org/W3170874841",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W3168101492",
        "https://openalex.org/W3175227919",
        "https://openalex.org/W3139633126"
    ],
    "abstract": "Transformers have made much progress in dealing with visual tasks. However, existing vision transformers still do not possess an ability that is important to visual input: building the attention among features of different scales. The reasons for this problem are two-fold: (1) Input embeddings of each layer are equal-scale without cross-scale features; (2) Some vision transformers sacrifice the small-scale features of embeddings to lower the cost of the self-attention module. To make up this defect, we propose Cross-scale Embedding Layer (CEL) and Long Short Distance Attention (LSDA). In particular, CEL blends each embedding with multiple patches of different scales, providing the model with cross-scale embeddings. LSDA splits the self-attention module into a short-distance and long-distance one, also lowering the cost but keeping both small-scale and large-scale features in embeddings. Through these two designs, we achieve cross-scale attention. Besides, we propose dynamic position bias for vision transformers to make the popular relative position bias apply to variable-sized images. Based on these proposed modules, we construct our vision architecture called CrossFormer. Experiments show that CrossFormer outperforms other transformers on several representative visual tasks, especially object detection and segmentation. The code has been released: this https URL.",
    "full_text": "CROSS FORMER : A VERSATILE VISION TRANSFORMER\nHINGING ON CROSS -SCALE ATTENTION\nWenxiao Wang1,2, Lu Yao1, Long Chen3, Binbin Lin4, Deng Cai1, Xiaofei He1 & Wei Liu2\n1State Key Lab of CAD & CG, Zhejiang University\n2Data Platform, Tencent\n3Columbia University\n4School of Software Technology, Zhejiang University\nABSTRACT\nTransformers have made great progress in dealing with computer vision tasks.\nHowever, existing vision transformers do not yet possess the ability of building the\ninteractions among features of different scales, which is perceptually important to\nvisual inputs. The reasons are two-fold: (1) Input embeddings of each layer are\nequal-scale, so no cross-scale feature can be extracted; (2) to lower the computa-\ntional cost, some vision transformers merge adjacent embeddings inside the self-\nattention module, thus sacriﬁcing small-scale (ﬁne-grained) features of the em-\nbeddings and also disabling the cross-scale interactions. To this end, we propose\nCross-scale Embedding Layer (CEL) andLong Short Distance Attention (LSDA).\nOn the one hand, CEL blends each embedding with multiple patches of different\nscales, providing the self-attention module itself with cross-scale features. On the\nother hand, LSDA splits the self-attention module into a short-distance one and a\nlong-distance counterpart, which not only reduces the computational burden but\nalso keeps both small-scale and large-scale features in the embeddings. Through\nthe above two designs, we achieve cross-scale attention. Besides, we put forward a\ndynamic position bias for vision transformers to make the popular relative position\nbias apply to variable-sized images. Hinging on the cross-scale attention module,\nwe construct a versatile vision architecture, dubbed CrossFormer, which accom-\nmodates variable-sized inputs. Extensive experiments show that CrossFormer out-\nperforms the other vision transformers on image classiﬁcation, object detection,\ninstance segmentation, and semantic segmentation tasks.1\n1 I NTRODUCTION\nIt turns out that transformer (Vaswani et al., 2017; Devlin et al., 2019; Brown et al., 2020) has\nachieved great success in the ﬁeld of natural language processing (NLP). Beneﬁtting from its self-\nattention module, transformer is born with the key ability to build long-distance dependencies. Since\nlong-distance dependencies are also needed by a number of vision tasks (Zhang & Yang, 2021; Chu\net al., 2021), a surge of research work (Dosovitskiy et al., 2021; Touvron et al., 2021; Wang et al.,\n2021) has been conducted to explore various transformer-based vision architectures.\nA transformer requires a sequence of embeddings 2(e.g., word embeddings) as input. To adapt this\nrequirement to typical vision tasks, most existing vision transformers (Dosovitskiy et al., 2021;\nTouvron et al., 2021; Wang et al., 2021; Liu et al., 2021b) produce embeddings by splitting an\ninput image into equal-sized patches. For example, a 224 ×224 image can be split into 56 ×56\npatches of size 4 ×4, and these patches are projected through a linear layer to yield an embedding\nsequence. Inside a certain transformer, self-attention is engaged to build the interactions between any\ntwo embeddings. Thus, the computational or memory cost of the self-attention module is O(N2),\nwhere N is the length of an embedding sequence. Such a cost is too big for a visual input because\nits embedding sequence is much longer than that of NLP. Therefore, the recently proposed vision\ntransformers (Wang et al., 2021; Liu et al., 2021b; Lin et al., 2021) develop multiple substitutes to\napproximate the vanilla self-attention module with a lower cost.\n1The code has been released: https://github.com/cheerss/CrossFormer\n2In this paper, we also use “embeddings” to represent the input of each layer.\n1\narXiv:2108.00154v2  [cs.CV]  8 Oct 2021\nThough the aforementioned vision transformers have made some progress, they suffer from an issue\nthat restricts their performance – They fail to build the interactions among features of different\nscales, whereas such an ability is very vital for a lot of vision tasks. For example, an image often\ncontains many objects of different scales, and to fully understand the image, building the interactions\namong those objects is helpful. Besides, some particular tasks such as instance segmentation need\nthe interactions between large-scale (coarse-grained) features and small-scale (ﬁne-grained) ones.\nExisting vision transformers fail to deal with the above cases due to two reasons: (1) The embeddings\nare generated from equal-sized patches, so they only own features of one single scale. Moreover,\ntheir scales are kept unchanged or enlarged uniformly through operations like average pooling in the\nfollowing layers. Hence, embeddings in the same layer are always equal-scale. (2) Inside the self-\nattention module, adjacent embeddings are often grouped together and merged (Wang et al., 2021;\nChu et al., 2021). Since the number of groups is smaller than that of embeddings, such behavior can\nreduce the computational budget of the self-attention. In this case, however, even if embeddings have\nboth small-scale and large-scale features, merging operations will lose the small-scale (ﬁne-grained)\nfeatures of each individual embedding, thereby disabling the cross-scale attention.\nTo enable the building of cross-scale interactions, we co-design a novel embedding layer and self-\nattention module as follows. 1)Cross-scale Embedding Layer (CEL)– Following Wang et al. (2021),\nwe also employ a pyramid structure for our transformer, which naturally splits the vision transformer\nmodel into multiple stages. CEL appears at the start of each stage, which receives last stage’s output\n(or an input image) as input and samples patches with multiple kernels of different scales ( e.g.,\n4 ×4 or 8 ×8). Then, each embedding is constructed by projecting and concatenating these patches\nas opposed to solely using one single-scale patch, which endows each embedding with cross-scale\nfeatures. 2) Long Short Distance Attention (LSDA) – We propose a substitute of the vanilla self-\nattention, but to preserve small-scale features, the embeddings will not be merged. In contrast, we\nsplit the self-attention module into Short Distance Attention (SDA) and Long Distance Attention\n(LDA). SDA builds the dependencies among neighboring embeddings, while LDA takes charge of\nthe dependencies among embeddings far away from each other. The proposed LSDA can also reduce\nthe cost of the self-attention module like previous studies (Wang et al., 2021; Chu et al., 2021), but\ndifferent from them, LSDA does not undermine either small-scale or large-scale features. As a\nconsequence, attention with cross-scale interactions is enabled.\nBesides, following prior work (Shaw et al., 2018; Liu et al., 2021b), we employ a relative position\nbias for embeddings’ position representations. The Relative Position Bias (RPB) only supports ﬁxed\nimage/group size3. However, image size for many vision tasks such as object detection is variable, so\ndoes group size for many architectures, including ours. To make the RPB more ﬂexible, we further\nintroduce a trainable module called Dynamic Position Bias(DPB), which receives two embeddings’\nrelative distance as input and outputs their position bias. The DPB module is optimized end-to-end\nin the training phase, inducing an ignorable cost but making RPB apply to variable image/group size.\nAll our proposed modules can be implemented with about ten lines of code. Based on them, we\nconstruct four versatile vision transformers of different sizes, dubbedCrossFormers. Other than im-\nage classiﬁcation, the proposed CrossFormer can handle a variety of tasks with variable-sized inputs\nsuch as object detection. Experiments on four representative vision tasks ( i.e., image classiﬁcation,\nobject detection, instance segmentation, and semantic segmentation) demonstrate that CrossFormer\noutperforms the other state-of-the-art vision transformers on all the tasks. Remarkably, the perfor-\nmance gains brought by CrossFormer are substantially signiﬁcant on dense prediction tasks, e.g.,\nobject detection and instance/semantic segmentation.\nIt is worth highlighting our contributions as follows:\n• We propose cross-scale embedding layer (CEL) and long short distance attention (LSDA), which\ntogether compensate for existing transformers’ incapability of building cross-scale attention.\n• The dynamic position bias module (DPB) is further proposed to make the relative position bias\nmore ﬂexible, i.e., accommodating variable image size or group size.\n• Multiple CrossFormers with different sizes are constructed, and we corroborate their effective-\nness through sufﬁcient experiments on four representative vision tasks.\n3Some vision transformers split input embeddings into several groups. Group size means the number of\nembeddings in a group.\n2\nCEL(4×4,8×8,16×16,32×32)\nCrossFormerBlock×\"!\nCEL(2×2,4×4)\nCrossFormerBlock×\"\"\nCEL(2×2,4×4)\nCrossFormerBlock×\"#\nCEL(2×2,4×4)\nCrossFormerBlock×\"$\nClassificationHead\nStage-1 Stage-2 Stage-3 Stage-4\n!!4×$!4×% !!8×$!8×2%!!16×$!16×4%!!32×$!32×8%\nLN\nSDA\nLNMLP\nDPB LN\nLDA\nLNMLP\nDPB\n(b)TwoconsecutiveCrossFormerblocks.\n!!$!\n(a)Thearchitecture of CrossFormerforimageclassification.\nFigure 1: (a) The architecture of CrossFormer for classiﬁcation. The input size is H0 ×W0, and the\nsize of feature maps in each stage is shown on the top.Stage-i consists of a CEL andni CrossFormer\nblocks. Numbers in CELs represent kernels’ sizes used for sampling patches. (b) The inner structure\nof two consecutive CrossFormer blocks. SDA and LDA appear alternately in different blocks.\n2 B ACKGROUND\nVision Transformers.Motivated by the transformers developed for NLP, researchers design speciﬁc\nvisual transformers for vision tasks to take full advantage of their powerful attention mechanism. In\nparticular, ViT and DeiT transfer the original transformer Vaswani et al. (2017) to vision tasks (Tou-\nvron et al., 2021; Dosovitskiy et al., 2021), achieving impressive performance. Later, PVT (Wang\net al., 2021), HVT (Pan et al., 2021), Swin (Liu et al., 2021b), and ViTAE (Xu et al., 2021) intro-\nduce a pyramid structure into the visual transformers, greatly decreasing the number of patches in\nthe later layers of a respective model. They also extend the visual transformers to other vision tasks\nlike object detection and segmentation (Wang et al., 2021; Liu et al., 2021b).\nSubstitutes of Self-attention. As the core component of transformers, the self-attention module\nincurs the O(N2) computational/memory cost, where N is the length of an embedding sequence.\nThough such a cost may be acceptable for image classiﬁcation, it is not the case for other tasks with\nmuch larger input images (e.g., object detection and segmentation). To alleviate the cost, Swin (Liu\net al., 2021b) restricts the attention in a certain local region, giving up long-distance dependencies.\nPVT (Wang et al., 2021) and Twins (Chu et al., 2021) make adjacent embeddings share the same\nkey/value to reduce the cost. Likewise, other vision transformers such as (Chen et al., 2021a; Zhang\net al., 2021b; Wu et al., 2021) also employ a divide-and-conquer method and approximate the vanilla\nself-attention module with a lower cost.\nPosition Representations. Transformer is combination-invariant. That is, shufﬂing input embed-\ndings does not change the output of a transformer. Nevertheless, the position of embeddings also\ncontains important information. To make the respective model aware of this, many different position\nrepresentations of embeddings (Vaswani et al., 2017) are proposed. For example, Dosovitskiy et al.\n(2021) directly add the embeddings with the vectors that contain absolute position information. In\ncontrast, Relative Position Bias (RPB) (Shaw et al., 2018) resorts to position information indicating\nthe relative distance of two embeddings. Much recent work (Liu et al., 2021b; Chen et al., 2021b)\nshows that RPB performs better than other position representations. Motivated by this ﬁnding, our\nproposed position representation DPB also uses relative distance, but different from RPB that only\nhandles ﬁxed-sized images, our DPB applies to images with dynamic sizes.\n3 C ROSS FORMER\nThe overall architecture of CrossFormer is plotted in Figure 1. Following (Wang et al., 2021; Liu\net al., 2021b; Lin et al., 2021), CrossFormer also employs a pyramid structure, which naturally splits\nthe transformer model into four stages. Each stage consists of a cross-scale embedding layer (CEL,\nSection 3.1) and several CrossFormer blocks (Section 3.2). A CEL receives last stage’s output (or\nan input image) as input and generates cross-scale embeddings. In this process, CEL (except that\nin Stage-1) reduces the number of embeddings to a quarter while doubles their dimensions for a\npyramid structure. Then, several CrossFormer blocks, each of which involves long short distance\nattention (LSDA) and dynamic position bias (DPB), are set up after CEL. A specialized head ( e.g.,\nthe classiﬁcation head in Figure 1) follows after the ﬁnal stage accounting for a speciﬁc task.\n3\n3.1 C ROSS -SCALE EMBEDDING LAYER (CEL)\nCross-scale embedding layer (CEL) is leveraged to generate input embeddings for each stage. Fig-\nure 2 takes the ﬁrst CEL, which is ahead of Stage-1, as an example. It receives an image as input,\nthen sampling patches using four kernels of different sizes. The stride of four kernels is kept the same\nEmbedandConcatenate\nEmbedding64dims32dims16dims16dims32×32\n8×8\n16×164×4\nEmbedding Layer\nType Kernel Stride Dim\nConv. 4×4 4 ×4 Dt\n2\nConv. 8×8 4 ×4 Dt\n4\nConv. 16×16 4 ×4 Dt\n8\nConv. 32×32 4 ×4 Dt\n8\nFigure 2: Illustration of the CEL layer. The in-\nput image is sampled by four different kernels\n(i.e., 4 ×4, 8 ×8, 16 ×16, 32 ×32) with same\nstride 4 ×4. Each embedding is constructed by\nprojecting and concatenating the four patches.\nDt means the total dimension of the embedding.\nso that they generate the same number of embed-\ndings4. As we can observe in Figure 2, every four\ncorresponding patches have the same center but\ndifferent scales, and all these four patches will be\nprojected and concatenated as one embedding. In\npractice, the process of sampling and projecting\ncan be fulﬁlled through four convolutional layers.\nFor a cross-scale embedding, one problem is how\nto set the projected dimension of each scale. The\ncomputational budget of a convolutional layer is\nproportional to K2D2, where K and D represent\nkernel size and input/output dimension, respec-\ntively (assuming the input dimension equals to\nthe output dimension). Therefore, given the same\ndimension, a large kernel consumes more budget\nthan a smaller one. To control the total budget of\nthe CEL, we use a lower dimension for large ker-\nnels while a higher dimension for small kernels.\nFigure 2 provides the speciﬁc allocation rule in\nits subtable, and a 128 dimensional example is\ngiven. Compared with allocating the dimension\nequally, our scheme saves much computational cost but does not explicitly affect the model’s per-\nformance. The cross-scale embedding layers in other stages work in a similar way. As shown in\nFigure 1, CELs for Stage-2/3/4 use two different kernels ( 2 ×2 and 4 ×4). Further, to form a\npyramid structure, the strides of CELs for Stage-2/3/4 are set as 2 ×2 to reduce the number of\nembeddings to a quarter.\n3.2 C ROSS FORMER BLOCK\nEach CrossFormer block consists of a long short distance attention module ( i.e., LSDA, which in-\nvolves a short distance attention (SDA) module or a long distance attention (LDA) module) and a\nmultilayer perceptron (MLP). As shown in Figure 1b, SDA and LDA appear alternately in different\nblocks, and the dynamic position bias (DPB) module works in both SDA and LDA for obtaining\nembeddings’ position representations. Following the prior vision transformers, residual connections\nare used in each block.\n3.2.1 L ONG SHORT DISTANCE ATTENTION (LSDA)\nWe split the self-attention module into two parts: short distance attention (SDA) and long distance\nattention (LDA). For SDA, everyG ×G adjacent embeddings are grouped together. Figure 3a gives\nan example where G = 3. For LDA with input of size S ×S, the embeddings are sampled with a\nﬁxed interval I. For example in Figure 3b ( I = 3), all embeddings with a red border belong to a\ngroup, and those with a yellow border comprise another group. The group’s height or width for LDA\nis computed as G = S/I (i.e., G = 3 in this example). After grouping embeddings, both SDA and\nLDA employ the vanilla self-attention within each group. As a result, the memory/computational\ncost of the self-attention module is reduced from O(S4) to O(S2G2), and G ≪S in most cases.\nIt is worth noting that the effectiveness of LDA also beneﬁts from cross-scale embeddings. Specif-\nically, we draw all the patches comprising two embeddings in Figure 3b. As we can see, the small-\nscale patches of two embeddings are non-adjacent, so it is difﬁcult to judge their relationship without\nthe help of the context. In other words, it will be hard to build the dependency between these two\nembeddings if they are only constructed by small-scale patches ( i.e., single-scale feature). On the\ncontrary, adjacent large-scale patches provide sufﬁcient context to link these two embeddings, which\nmakes long-distance cross-scale attention easier and more meaningful.\n4The image will be padded if necessary.\n4\n(a)SDA (b)LDA\n (c)DPB\nLinear,\"/4(Δ'!\",Δ(!\")\n*!,\"\n×3LNReLULinear,1\nFigure 3: (a) Short distance attention (SDA). Embeddings (blue cubes) are grouped by red boxes. (b)\nLong distance attention (LDA). Embeddings with the same color borders belong to the same group.\nLarge patches of embeddings in the same group are adjacent. (c) Dynamic position bias (DBP). The\ndimensions of intermediate layers are D/4, and the output is a scalar.\nWe provide the pseudo-code of LSDA in the appendix (A.1). Based on the vanilla multi-head self-\nattention, LSDA can be implemented with only ten lines of code. Further, onlyreshape and permute\noperations are used, so no extra computational cost is introduced.\n3.2.2 D YNAMIC POSITION BIAS (DPB)\nRelative position bias (RPB) indicates embeddings’ relative position by adding a bias to their atten-\ntion. Formally, the LSDA’s attention map with RPB becomes:\nAttention = Softmax(QKT /\n√\nd + B)V , (1)\nwhere Q, K, V ∈RG2×D represent query, key, valuein the self-attention module, respectively.\n√\nd\nis a constant normalizer, andB ∈RG2×G2\nis the RPB matrix. In previous works (Liu et al., 2021b),\nBi,j = ˆB∆xij,∆yij , where ˆB is a ﬁxed-sized matrix, and (∆xij, ∆yij) is the coordinate distance\nbetween the ith and jth embeddings. It is obvious that the image/group size is restricted in case that\n(∆xij, ∆yij) exceeds the size of ˆB. In contrast, we propose an MLP-based module called DPB to\ngenerate the relative position bias dynamically, i.e.,\nBi,j = DPB (∆xij, ∆yij). (2)\nThe structure of DPB is displayed in Figure 3c. Its non-linear transformation consists of three fully-\nconnected layers with layer normalization (Ba et al., 2016) and ReLU (Nair & Hinton, 2010). The\ninput dimension of DPB is 2, i.e., (∆xij, ∆yij), and intermediate layers’ dimension is set as D/4,\nwhere D is the dimension of embeddings. The output Bij is a scalar, encoding the relative position\nfeature between the ith and jth embeddings. DPB is a trainable module optimized along with the\nwhole transformer model. It can deal with any image/group size without worrying about the bound\nof (∆xij, ∆yij). In the appendix (A.2), we prove that DPB is equivalent to RPB if the image/group\nsize is ﬁxed. In this case, we can transform a trained DPB to RPB in the test phase. We also provide\nan efﬁcient O(G2) implementation of DPB when image/group size is variable (the complexity is\nO(G4) in a normal case because B ∈RG2×G2\n).\n3.3 V ARIANTS OF CROSS FORMER\nTable 1 lists the detailed conﬁgurations of CrossFormer’s four variants (-T, -S, -B, and -L for tiny,\nsmall, base, and large, respectively) for image classiﬁcation. To re-use the pre-trained weights, the\nmodels for other tasks ( e.g., object detection) employ the same backbones as classiﬁcation except\nthat they may use different G and I. Speciﬁcally, besides the conﬁgurations same to classiﬁcation,\nwe also test with G1 = G2 = 14, I1 = 16, and I2 = 8 for the detection (and segmentation) models’\nﬁrst two stages to adapt to larger images. The speciﬁc conﬁgurations are described in the appendix\n(A.3). Notably, the group size or the interval (i.e., G or I) does not affect the shape of weight tensors,\nso the backbones pre-trained on ImageNet can be readily ﬁne-tuned on other tasks even though they\nuse different G or I.\n5\nTable 1: Variants of CrossFormer for image classiﬁcation. The example input size is 224 ×224. S\nrepresents the feature maps’ height (and width) of each stage.D and H mean embedding dimensions\nand the number of heads in the multi-head self-attention module, respectively. G and I are group\nsize and interval for SDA and LDA, respectively.\nOutput SizeLayer Name CrossFormer-T CrossFormer-S CrossFormer-B CrossFormer-L\nStage-1 56×56\nCross Embed. Kernel size:4×4,8×8,16×16,32×32, Stride=4\n(S1= 56) SDA/LDA\n\n\nD1= 64H1= 2G1= 7I1= 8\n\n×1\n\n\nD1= 96H1= 3G1= 7I1= 8\n\n×2\n\n\nD1= 96H1= 3G1= 7I1= 8,\n\n×2\n\n\nD1= 128H1= 4G1= 7I1= 8\n\n×2MLP\nStage-2 28×28\nCross Embed. Kernel size:2×2,4×4, Stride=2\n(S2= 28) SDA/LDA\n\n\nD2= 128H2= 4G2= 7I2= 4\n\n×1\n\n\nD2= 192H2= 6G2= 7I2= 4\n\n×2\n\n\nD2= 192H2= 6G2= 7I2= 4\n\n×2\n\n\nD2= 256H2= 8G2= 7I2= 4\n\n×2MLP\nStage-3 14×14\nCross Embed. Kernel size:2×2,4×4, Stride=2\n(S3= 14) SDA/LDA\n\n\nD3= 256H3= 8G3= 7I3= 2\n\n×8\n\n\nD3= 384H3= 12G3= 7I3= 2\n\n×6\n\n\nD3= 384H3= 12G3= 7I3= 2\n\n×18\n\n\nD3= 512H3= 16G3= 7I3= 2\n\n×18MLP\nStage-4 7×7\nCross Embed. Kernel size:2×2,4×4, Stride=2\n(S4= 7) SDA/LDA\n\n\nD4= 512H4= 16G4= 7I4= 1\n\n×6\n\n\nD4= 768H4= 24G4= 7I4= 1\n\n×2\n\n\nD4= 768H4= 24G4= 7I4= 1\n\n×2\n\n\nD4= 1024H4= 32G4= 7I4= 1\n\n×2MLP\nHead 1×1 Avg Pooling Kernel size:7×7\nLinear Classes:1000\n4 E XPERIMENTS\nThe experiments are carried out on four challenging tasks: image classiﬁcation, object detection, in-\nstance segmentation, and semantic segmentation. To entail a fair comparison, we keep the same data\naugmentation and training settings as the other vision transformers as far as possible. The competi-\ntors are all competitive vision transformers, including DeiT (Touvron et al., 2021), PVT (Wang et al.,\n2021), T2T-ViT (Yuan et al., 2021), TNT (Han et al., 2021), CViT (Chen et al., 2021a), Twins (Chu\net al., 2021), Swin (Liu et al., 2021b), NesT (Zhang et al., 2021b), CvT (Wu et al., 2021), ViL (Zhang\net al., 2021a), CAT (Lin et al., 2021), ResT (Zhang & Yang, 2021), TransCNN (Liu et al., 2021a),\nShufﬂe (Huang et al., 2021), BoTNet (Srinivas et al., 2021), and RegionViT (Chen et al., 2021b).\n4.1 I MAGE CLASSIFICATION\nExperimental Settings.The experiments on image classiﬁcation are done with the ImageNet (Rus-\nsakovsky et al., 2015) dataset. The models are trained on 1.28M training images and tested on 50K\nvalidation images. The same training settings as the other vision transformers are adopted. In par-\nticular, we use an AdamW (Kingma & Ba, 2015) optimizer training for 300 epochs with a cosine\ndecay learning rate scheduler, and 20 epochs of linear warm-up are used. The batch size is 1,024\nsplit on 8 V100 GPUs. An initial learning rate of 0.001 and a weight decay of 0.05 are used. Be-\nsides, we use drop path rate of0.1, 0.2, 0.3, 0.5 for CrossFormer-T, CrossFormer-S, CrossFormer-B,\nCrossFormer-L, respectively. Further, Similar to Swin (Liu et al., 2021b), RandAugment (Cubuk\net al., 2020), Mixup (Zhang et al., 2018), Cutmix (Yun et al., 2019), random erasing (Zhong et al.,\n2020), and stochastic depth (Huang et al., 2016) are used for data augmentation.\nResults. The results are shown in Table 2. As we can see, CrossFormer achieves the highest accu-\nracy with parameters and FLOPs comparable to other state-of-the-art vision transformer structures.\nIn speciﬁc, compared against strong baselines DeiT, PVT, and Swin, our CrossFormer outperforms\nthem at least absolute 1.2% in accuracy on small models. Further, though RegionViT achieves the\nsame accuracy (82.5%) as ours on a small model, it is 0.7% (84.0% vs. 83.3%) absolutely lower\nthan ours on the large model.\n4.2 O BJECT DETECTION AND INSTANCE SEGMENTATION\nExperimental Settings. The experiments on object detection and instance segmentation are both\ndone on the COCO 2017 dataset (Lin et al., 2014), which contains118K training and 5K val images.\n6\nTable 2: Results on the ImageNet validation set. The input size is224 ×224 for most models, while\nis 384 ×384 for the model with a †. Results of other architectures are drawn from original papers.\nArchitectures #Params FLOPs Acc.\nPVT-S 24.5M 3.8G 79.8%\nRegionViT-T 13.8M 2.4G 80.4%\nTwins-SVT-S 24.0M 2.8G 81.3%\nCrossFormer-T 27.8M 2.9G 81.5%\nDeiT-S 22.1M 4.6G 79.8%\nT2T-ViT 21.5M 5.2G 80.7%\nCViT-S 26.7M 5.6G 81.0%\nPVT-M 44.2M 6.7G 81.2%\nTNT-S 23.8M 5.2G 81.3%\nSwin-T 29.0M 4.5G 81.3%\nNesT-T 17.0M 5.8G 81.5%\nCvT-13 20.0M 4.5G 81.6%\nResT 30.2M 4.3G 81.6%\nCAT-S 37.0M 5.9G 81.8%\nViL-S 24.6M 4.9G 81.8%\nRegionViT-S 30.6M 5.3G 82.5%\nCrossFormer-S 30.7M 4.9G 82.5%\nArchitectures #Params FLOPs Acc.\nBoTNet-S1-59 33.5M 7.3G 81.7%\nPVT-L 61.4M 9.8G 81.7%\nCvT-21 32.0M 7.1G 82.5%\nCAT-B 52.0M 8.9G 82.8%\nSwin-S 50.0M 8.7G 83.0%\nRegionViT-M 41.2M 7.4G 83.1%\nTwins-SVT-B 56.0M 8.3G 83.1%\nNesT-S 38.0M 10.4G 83.3%\nCrossFormer-B 52.0M 9.2G 83.4%\nDeiT-B 86.0M 17.5G 81.8%\nDeiT-B† 86.0M 55.4G 83.1%\nViL-B 55.7M 13.4G 83.2%\nRegionViT-B 72.0M 13.3G 83.3%\nTwins-SVT-L 99.2M 14.8G 83.3%\nSwin-B 88.0M 15.4G 83.3%\nNesT-B 68.0M 17.9G 83.8%\nCrossFormer-L 92.0M 16.1G 84.0%\nTable 3: Object detection results on COCO 2017 val set with RetinaNets as detectors. Results for\nSwin are drawn from Twins as Swin does not report results on RetinaNet. Results in blue fonts are\nthe second-placed ones. CrossFormers with ‡use different group sizes from classiﬁcation models.\n(More details are put in the appendix (A.3))\nMethod Backbone #Params FLOPs APb APb50 APb75 APbS APbM APbL\nRetinaNet\nResNet-50 37.7M 234.0G 36.3 55.3 38.6 19.3 40.0 48.8\n1×schedule\nCAT-B 62.0M 337.0G 41.4 62.9 43.8 24.9 44.6 55.2\nSwin-T 38.5M 245.0G 41.5 62.1 44.2 25.1 44.9 55.5\nPVT-M 53.9M − 41.9 63.1 44.3 25.0 44.9 57.6\nViL-M 50.8M 338.9G 42.9 64.0 45.4 27.0 46.1 57.2\nRegionViT-B 83.4M 308.9G 43.3 65.2 46.4 29.2 46.4 57.0\nTransCNN-B 36.5M − 43.4 64.2 46.5 27.0 47.4 56.7\nCrossFormer-S 40.8M 282.0G 44.4(+1.0) 65.8 47.4 28.2 48.4 59.4\nCrossFormer-S‡ 40.8M 272.1G 44.2(+0.8) 65.7 47.2 28.0 48.0 59.1\nResNet101 56.7M 315.0G 38.5 57.8 41.2 21.4 42.6 51.1\nPVT-L 71.1M 345.0G 42.6 63.7 45.4 25.8 46.0 58.4\nTwins-SVT-B 67.0M 322.0G 44.4 66.7 48.1 28.5 48.9 60.6\nRegionViT-B+ 84.5M 328.2G 44.6 66.4 47.6 29.6 47.6 59.0\nSwin-B 98.4M 477.0G 44.7 65.9 49.2 − − −\nTwins-SVT-L 110.9M 455.0G 44.8 66.1 48.1 28.4 48.3 60.1\nCrossFormer-B 62.1M 389.0G 46.2(+1.4) 67.8 49.5 30.1 49.9 61.8\nCrossFormer-B‡ 62.1M 379.1G 46.1(+1.3) 67.7 49.0 29.5 49.9 61.5\nWe use MMDetection-based (Chen et al., 2019) RetinaNet (Lin et al., 2020) and Mask R-CNN (He\net al., 2017) as the object detection and instance segmentation head, respectively. For both tasks,\nthe backbones are initialized with the weights pre-trained on ImageNet. Then the whole models are\ntrained with batch size 16 on 8 V100 GPUs, and an AdamW optimizer with an initial learning rate\nof 1 ×10−4 is used. Following previous works, we adopt 1×training schedule (i.e., the models are\ntrained for 12 epochs) when taking RetinaNets as detectors, and images are resized to 800 pixels for\nthe short side. While for Mask R-CNN, both 1×and 3×training schedules are used. It is noted that\nmulti-scale training (Carion et al., 2020) is also employed when taking 3×training schedules.\nResults. The results on RetinaNet and Mask R-CNN are shown in Table 3 and Table 4, respectively.\nAs we can see, the second-placed architecture changes along with the experiment, that is, these\narchitectures may perform well on one task but poorly on another task. In contrast, our CrossFormer\noutperforms all the others on both tasks (detection and segmentation) with both model sizes (small\nand base). Further, CrossFormer’s performance gain over the other architectures gets sharper when\nenlarging the model, indicating that CrossFormer enjoys greater potentials.\n4.3 S EMANTIC SEGMENTATION\nExperimental Settings. ADE20K (Zhou et al., 2017) is used as the benchmark for semantic seg-\nmentation. It covers a broad range of 150 semantic categories, including 20K images for training\n7\nTable 4: Object detection and instance segmentation results on COCOval 2017 with Mask R-CNNs\nas detectors. APb and APm are box average precision and mask average precision, respectively.\nMethod Backbone #Params FLOPs APb APb50 APb75 APm APm50 APm75\nMask R-CNN\nPVT-M 63.9M − 42.0 64.4 45.6 39.0 61.6 42.0\n1×schedule\nSwin-T 47.8M 264.0G 42.2 64.6 46.2 39.1 61.6 42.0\nTwins-PCPVT-S 44.3M 245.0G 42.9 65.8 47.1 40.0 62.7 42.9\nTransCNN-B 46.4M − 44.0 66.4 48.5 40.2 63.3 43.2\nViL-M 60.1M 261.1G 43.3 65.9 47.0 39.7 62.8 42.0\nRegionViT-B 92.2M 287.9G 43.5 66.7 47.4 40.1 63.4 43.0\nRegionViT-B+ 93.2M 307.1G 44.5 67.6 48.7 41.0 64.4 43.9\nCrossFormer-S 50.2M 301.0G 45.4(+0.9) 68.0 49.7 41.4(+0.4) 64.8 44.6\nCrossFormer-S‡ 50.2M 291.1G 45.0(+0.5) 67.9 49.1 41.2(+0.2) 64.6 44.3\nCAT-B 71.0M 356.0G 41.8 65.4 45.2 38.7 62.3 41.4\nPVT-L 81.0M 364.0G 42.9 65.0 46.6 39.5 61.9 42.5\nTwins-SVT-B 76.3M 340.0G 45.1 67.0 49.4 41.1 64.1 44.4\nViL-B 76.1M 365.1G 45.1 67.2 49.3 41.0 64.3 44.2\nTwins-SVT-L 119.7M 474.0G 45.2 67.5 49.4 41.2 64.5 44.5\nSwin-S 69.1M 354.0G 44.8 66.6 48.9 40.9 63.4 44.2\nSwin-B 107.2M 496.0G 45.5 − − 41.3 − −\nCrossFormer-B 71.5M 407.9G 47.2(+1.7) 69.9 51.8 42.7(+1.4) 66.6 46.2\nCrossFormer-B‡ 71.5M 398.1G 47.1(+1.6) 69.9 52.0 42.7(+1.4) 66.5 46.1\nMask R-CNN\nPVT-M 63.9M − 44.2 66.0 48.2 45.0 63.1 43.5\n3×schedule\nViL-M 60.1M 261.1G 44.6 66.3 48.5 40.7 63.8 43.7\nSwin-T 47.8M 264.0G 46.0 68.2 50.2 41.6 65.1 44.8\nShufﬂe-T 48.0M 268.0G 46.8 68.9 51.5 42.3 66.0 45.6\nCrossFormer-S‡ 50.2M 291.1G 48.7(+1.9) 70.7 53.7 43.9(+1.6) 67.9 47.3\nPVT-L 81.0M 364.0G 44.5 66.0 48.3 40.7 63.4 43.7\nViL-B 76.1M 365.1G 45.7 67.2 49.9 41.3 64.4 44.5\nShufﬂe-S 69.0M 359.0G 48.4 70.1 53.5 43.3 67.3 46.7\nSwin-S 69.1M 354.0G 48.5 70.2 53.5 43.3 67.3 46.6\nCrossFormer-B‡ 71.5M 398.1G 49.8(+1.3) 71.6 54.9 44.5(+1.2) 68.8 47.9\nTable 5: Semantic segmentation results on the ADE20K validation set. “MS IOU” means testing\nwith variable input size.\nSemantic FPN (80K iterations)\nBackbone #Params FLOPs IOU\nPVT-M 48.0M 219.0G 41.6\nTwins-SVT-B 60.4M 261.0G 45.0\nSwin-S 53.2M 274.0G 45.2\nCrossFormer-S 34.3M 220.7G 46.0(+0.8)\nCrossFormer-S‡ 34.3M 209.8G 46.4(+1.2)\nPVT-L 65.1M 283.0G 42.1\nCAT-B 55.0M 276.0G 43.6\nCrossFormer-B 55.6M 331.0G 47.7(+4.1)\nCrossFormer-B‡ 55.6M 320.1G 48.0(+4.4)\nTwins-SVT-L 103.7M 397.0G 45.8\nCrossFormer-L 95.4M 497.0G 48.7(+2.9)\nCrossFormer-L‡ 95.4M 482.7G 49.1(+3.3)\nUPerNet (160K iterations)\nBackbone #Params FLOPs IOU MS IOU\nSwin-T 60.0M 945.0G 44.5 45.8\nShufﬂe-T 60.0M 949.0G 46.6 47.6\nCrossFormer-S 62.3M 979.5G 47.6(+1.0) 48.4\nCrossFormer-S‡ 62.3M 968.5G 47.4(+0.8) 48.2\nSwin-S 81.0M 1038.0G 47.6 49.5\nShufﬂe-S 81.0M 1044.0G 48.4 49.6\nCrossFormer-B 83.6M 1089.7G 49.7(+1.3) 50.6\nCrossFormer-B‡ 83.6M 1078.8G 49.2(+0.8) 50.1\nSwin-B 121.0M 1088.0G 48.1 49.7\nShufﬂe-B 121.0M 1096.0G 49.0 −\nCrossFormer-L125.5M 1257.8G 50.4(+1.4) 51.4\nCrossFormer-L‡ 125.5M 1243.5G 50.5(+1.5) 51.4\nand 2K for validation. Similar to models for detection, we initialize the backbones with weights pre-\ntrained on ImageNet, and MMSegmentation-based (Contributors, 2020) semantic FPN and UPer-\nNet (Xiao et al., 2018) are taken as the segmentation head. For FPN (Kirillov et al., 2019), we use\nan AdamW optimizer with learning rate and weight deacy of 1 ×10−4. Models are trained for 80K\niterations with batch size 16. For UPerNet, an AdamW optimizer with an initial learning rate of\n6 ×10−5 and a weight decay of 0.01 is used, and models are trained for 160K iterations.\nResults. All results are shown in Table 5. Similar to object detection, CrossFormer exhibits a greater\nperformance gain over the others when enlarging the model. For example, CrossFormer-T achieves\n1.4% absolutely higher on IOU than Twins-SVT-B, but CrossFormer-B achieves 3.1% absolutely\nhigher on IOU than Twins-SVT-L. Totally, CrossFormer shows a more signiﬁcant advantage over the\nothers on dense prediction tasks ( e.g., detection and segmentation) than on classiﬁcation, implying\nthat cross-scale interactions in the attention module are more important for dense prediction tasks\nthan for classiﬁcation.\n8\nTable 6: Results on the ImageNet validation set. The baseline model is CrossFormer-S (82.5%). We\ntest with different kernel sizes of CELs.\nCEL’s Kernel Size #Params FLOPs Acc.Stage-1 Stage-2 Stage-3 Stage-4\n4×4 2×2 2×2 2×2 28.3M 4.5G 81.5%\n8×8 2×2 2×2 2×2 28.3M 4.5G 81.9%\n4×4,8×8 2×2,4×4 2×2,4×4 2×2,4×4 30.6M 4.8G 82.3%\n4×4,8×8,16×16,32×32 2×2,4×4 2×2,4×4 2×2,4×4 30.7M 4.9G 82.5%\n4×4,8×8,16×16,32×32 2×2,4×4,8×8 2×2,4×4 2×2 29.4M 5.0G 82.4%\nTable 7: Experimental results of ablation studies.\n(a) Ablation studies on cross-scale embeddings\n(CEL) and long short distance attention (LSDA).\nThe base model is CrossFormer-S (82.5%).\nPVT-like Swin-like LSDA CEL Acc.\n✓ ✓ 81.3%\n✓ ✓ 81.9%\n✓ ✓ 82.5%\n✓ 81.5%\n(b) Comparisons between different position representa-\ntions. The base model is CrossFormer-S. Throughput is\ntested on 1× V100 GPU.\nMethod #Params/FLOPs Throughput Acc.\nAPE 30.9342M/4.9061G 686 imgs/sec 82.1%\nRPB 30.6159M/4.9062G 684 imgs/sec 82.5%\nDPB 30.6573M/4.9098G 672 imgs/sec 82.5%\nDPB-residual 30.6573M/4.9098G 672 imgs/sec 82.4%\n4.4 A BLATION STUDIES\nCross-scale Embeddings vs. Single-scale Embeddings.We conduct the experiments by replac-\ning cross-scale embedding layers with single-scale ones. As we can see in Table 6, when using\nsingle-scale embeddings, the 8 ×8 kernel in Stage-1 brings 0.4% (81.9% vs. 81.5%) absolute im-\nprovement compared with the 4 ×4 kernel. It tells us that overlapping receptive ﬁelds help improve\nthe model’s performance. Besides, all models with cross-scale embeddings perform better than those\nwith single-scale embeddings. In particular, our CrossFormer achieves1% (82.5% vs. 81.5%) abso-\nlute performance gain compared with using single-scale embeddings for all stages. For cross-scale\nembeddings, we also try several different combinations of kernel sizes, and they all show similar\nperformance (82.3% ∼82.5%). In summary, cross-scale embeddings can bring a large performance\ngain, yet the model is relatively robust to different choices of kernel size.\nLSDA vs. Other Self-attentions.Two self-attention modules used in PVT and Swin are compared.\nSpeciﬁcally, PVT sacriﬁces the small-scale features when computing the self-attention, while Swin\nrestricts the self-attention in a local region, giving up the long-distance attention. As we can observe\nin Table 7a, compared against the PVT-like and Swin-like self-attention mechanisms, our Cross-\nFormer outperforms them at least absolute0.6% accuracy (82.5% vs. 81.9%). The results show that\nperforming the self-attention in a long-short distance manner is most conducive to improving the\nmodel’s performance.\nDPB vs. Other Position Representations.We compare the parameters, FLOPs, throughputs, and\naccuracies of the models among absolute position embedding (APE), relative position bias (RPB),\nand DPB. The results are shown in Table 7b. DPB-residual means DPB with residual connections.\nBoth DPB and RPB outperform APE for absolute 0.4% accuracy, which indicates that relative po-\nsition representations are more beneﬁcial than absolute ones. Further, DPB achieves the same accu-\nracy (82.5%) as RPB with an ignorable extra cost; however, as we described in Section 3.2.2, it is\nmore ﬂexible than RPB and applies to variable image size or group size. The results also show that\nresidual connection in DPB does not help improve or even degrades the model’s performance.\n5 C ONCLUSIONS\nWe proposed a novel transformer-based vision architecture, namely CrossFormer. Its core ingredi-\nents are Cross-scale Embedding Layer (CEL) and Long Short Distance Attention (LSDA), thereby\nyielding the cross-attention module. We further proposed a dynamic position bias, making the rel-\native position bias apply to any input size. Extensive experiments show that CrossFormer achieves\nsuperior performance over other state-of-the-art vision transformers on several representative vision\ntasks. Particularly, CrossFormer is demonstrated to gain great improvements on object detection and\nsegmentation, which indicates that CEL and LSDA are together essential for dense prediction tasks.\n9\nREFERENCES\nLei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. CoRR,\nabs/1607.06450, 2016.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\nAriel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,\nScott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,\nIlya Sutskever, and Dario Amodei. Language models are few-shot learners. In Neural Informa-\ntion Processing Systems, NeurIPS, 2020.\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and\nSergey Zagoruyko. End-to-end object detection with transformers. In Computer Vision - ECCV\n2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part I , vol-\nume 12346, pp. 213–229, 2020.\nChun-Fu Chen, Quanfu Fan, and Rameswar Panda. Crossvit: Cross-attention multi-scale vision\ntransformer for image classiﬁcation. CoRR, abs/2103.14899, 2021a.\nChun-Fu Chen, Rameswar Panda, and Quanfu Fan. Regionvit: Regional-to-local attention for vision\ntransformers. CoRR, abs/2106.02689, 2021b.\nKai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen\nFeng, Ziwei Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tianheng Cheng, Qijie\nZhao, Buyu Li, Xin Lu, Rui Zhu, Yue Wu, Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli\nOuyang, Chen Change Loy, and Dahua Lin. MMDetection: Open mmlab detection toolbox and\nbenchmark. arXiv preprint arXiv:1906.07155, 2019.\nXiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia,\nand Chunhua Shen. Twins: Revisiting spatial attention design in vision transformers. CoRR,\nabs/2104.13840, 2021.\nMMSegmentation Contributors. MMSegmentation: Openmmlab semantic segmentation toolbox\nand benchmark, 2020.\nEkin Dogus Cubuk, Barret Zoph, Jon Shlens, and Quoc Le. Randaugment: Practical automated data\naugmentation with a reduced search space. In Neural Information Processing Systems, NeurIPS,\n2020.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep\nbidirectional transformers for language understanding. In Conference of the North American\nChapter of the Association for Computational Linguistics, NAACL, pp. 4171–4186, 2019.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-\nreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at\nscale. In International Conference on Learning Representations, ICLR, 2021.\nKai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in\ntransformer. CoRR, abs/2103.00112, 2021.\nKaiming He, Georgia Gkioxari, Piotr Doll´ar, and Ross B. Girshick. Mask R-CNN. In International\nConference on Computer Vision, ICCV, pp. 2980–2988, 2017.\nGao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q. Weinberger. Deep networks with\nstochastic depth. In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling (eds.), European\nConference on Computer Vision, ECCV, volume 9908, pp. 646–661, 2016.\nZilong Huang, Youcheng Ben, Guozhong Luo, Pei Cheng, Gang Yu, and Bin Fu. Shufﬂe trans-\nformer: Rethinking spatial shufﬂe for vision transformer. CoRR, abs/2106.03650, 2021.\n10\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International\nConference on Learning Representations, ICLR, 2015.\nAlexander Kirillov, Ross B. Girshick, Kaiming He, and Piotr Doll ´ar. Panoptic feature pyramid\nnetworks. In Conference on Computer Vision and Pattern Recognition, CVPR , pp. 6399–6408,\n2019.\nHezheng Lin, Xing Cheng, Xiangyu Wu, Fan Yang, Dong Shen, Zhongyuan Wang, Qing Song, and\nWei Yuan. CAT: cross attention in vision transformer.CoRR, abs/2106.05786, 2021.\nTsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDoll´ar, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. In European\nConference on Computer Vision, ECCV, volume 8693, pp. 740–755, 2014.\nTsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He, and Piotr Doll ´ar. Focal loss for dense\nobject detection. Transactions on Pattern Analysis and Machine Intelligence, PAMI, 42(2):318–\n327, 2020.\nYun Liu, Guolei Sun, Yu Qiu, Le Zhang, Ajad Chhatkuli, and Luc Van Gool. Transformer in\nconvolutional neural networks. CoRR, abs/2106.03180, 2021a.\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Bain-\ning Guo. Swin transformer: Hierarchical vision transformer using shifted windows. CoRR,\nabs/2103.14030, 2021b.\nVinod Nair and Geoffrey E. Hinton. Rectiﬁed linear units improve restricted boltzmann machines. In\nJohannes F¨urnkranz and Thorsten Joachims (eds.), International Conference on Machine Learn-\ning, ICML, pp. 807–814, 2010.\nZizheng Pan, Bohan Zhuang, Jing Liu, Haoyu He, and Jianfei Cai. Scalable visual transformers with\nhierarchical pooling. CoRR, abs/2103.10619, 2021.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\nHuang, Andrej Karpathy, Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and Fei-Fei\nLi. Imagenet large scale visual recognition challenge. International Journal of Computer Vision,\nIJCV, 2015.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position represen-\ntations. In Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, NAACL, pp. 464–468, 2018.\nAravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon Shlens, Pieter Abbeel, and Ashish Vaswani.\nBottleneck transformers for visual recognition. In Conference on Computer Vision and Pattern\nRecognition, CVPR, 2021.\nPeize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng Xu, Wei Zhan, Masayoshi Tomizuka, Lei\nLi, Zehuan Yuan, Changhu Wang, and Ping Luo. Sparse R-CNN: end-to-end object detection\nwith learnable proposals. In IEEE Conference on Computer Vision and Pattern Recognition,\nCVPR 2021, virtual, June 19-25, 2021, pp. 14454–14463, 2021.\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and\nHerv´e J ´egou. Training data-efﬁcient image transformers & distillation through attention. In\nInternational Conference on Machine Learning, ICML, volume 139, pp. 10347–10357, 2021.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Neural Information Processing\nSystems, NeurIPS, pp. 5998–6008, 2017.\nWenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo,\nand Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without\nconvolutions. CoRR, abs/2102.12122, 2021.\nHaiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt:\nIntroducing convolutions to vision transformers. CoRR, abs/2103.15808, 2021.\n11\nTete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Uniﬁed perceptual parsing for\nscene understanding. In European Conference on Computer Vision, ECCV , volume 11209, pp.\n432–448, 2018.\nYufei Xu, Qiming Zhang, Jing Zhang, and Dacheng Tao. Vitae: Vision transformer advanced by\nexploring intrinsic inductive bias. CoRR, abs/2106.03348, 2021.\nLi Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Francis E. H. Tay, Jiashi Feng, and\nShuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet.\nCoRR, abs/2101.11986, 2021.\nSangdoo Yun, Dongyoon Han, Sanghyuk Chun, Seong Joon Oh, Youngjoon Yoo, and Junsuk Choe.\nCutmix: Regularization strategy to train strong classiﬁers with localizable features. In Interna-\ntional Conference on Computer Vision, ICCV, pp. 6022–6031, 2019.\nHongyi Zhang, Moustapha Ciss´e, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empiri-\ncal risk minimization. In International Conference on Learning Representations, ICLR, 2018.\nPengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu Yuan, Lei Zhang, and Jianfeng Gao.\nMulti-scale vision longformer: A new vision transformer for high-resolution image encoding.\nCoRR, abs/2103.15358, 2021a.\nQing-Long Zhang and Yubin Yang. Rest: An efﬁcient transformer for visual recognition. CoRR,\nabs/2105.13677, 2021.\nZizhao Zhang, Han Zhang, Long Zhao, Ting Chen, and Tomas Pﬁster. Aggregating nested trans-\nformers. CoRR, abs/2105.12723, 2021b.\nZhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data aug-\nmentation. In Association for the Advancement of Artiﬁcial Intelligence, AAAI, pp. 13001–13008,\n2020.\nBolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene\nparsing through ADE20K dataset. In Conference on Computer Vision and Pattern Recognition,\nCVPR, pp. 5122–5130, 2017.\n12\nA C ROSS FORMER\nA.1 P SEUDO CODE OF LSDA\nThe pseudo code for LSDA is shown in Algorithm 1. As we can see, based on the vanilla self-\nattention module, both SDA and LDA are implemented with only ten lines of code, and onlyreshape\nand permute operations are used.\nAlgorithm 1LSDA code (PyTorch-like)\n# H: height, W: width, G: group size of SDA/LDA\n# x: input tensor (H, W, D)\nclass LSDA():\ndef forward(x, type):\n## group the embeddings\nif type == \"SDA\":\nx = x.reshaspe(H // G, G, W // G, G, D).permute(0, 2, 1, 3, 4)\nelif type == \"LDA\":\nx = x.reshaspe(G, H // G, G, W // G, D).permute(1, 3, 0, 2, 4)\nx = x.reshape(H * W // (G ** 2), G ** 2, D)\n## the vanilla self-attention module\nx = Attention(x)\n## un-group the embeddings\nx = x.reshaspe(H // G, W // G, G, G, D)\nif type == \"SDA\":\nx = x.permute(0, 2, 1, 3, 4).reshaspe(H, W, D)\nelif type == \"LDA\":\nx = x.permute(2, 0, 3, 1, 4).reshaspe(H, W, D)\nreturn x\nA.2 D YNAMIC POSITION BIAS (DPB)\nΔ𝑥!\"=−1Δ𝑦!\"=−2\n𝑥\n𝑦\nFigure 4: An example of com-\nputing (∆xij, ∆yij).\nFigure 4 gives an example of computing (∆xij, ∆yij) with G = 5\nin the DPB module. For a group of size G ×G, it is easy to deduce\nthat:\n0 ≤x, y < G\n1 −G ≤∆xij ≤G −1\n1 −G ≤∆yij ≤G −1.\n(3)\nThus, motivated by the relative position bias, we construct a matrix\nˆB ∈R(2G−1)×(2G−1), where\nˆBi,j = DPB (1 −G + i, 1 −G + j), 0 ≤i, j <2G −1. (4)\nThe complexity of computing ˆB is O(G2). Then, the bias matrix\nB in DPB can be drawn from ˆB, i.e.,\nBi,j = ˆB∆xij,∆yij . (5)\nWhen the image/group size (i.e., G) is ﬁxed, both ˆB and B will be also unchanged in the test phase.\nTherefore, we only need to compute ˆB and B once, and DPB is equivalent to relative position bias\nin this case.\nA.3 V ARIANTS OF CROSS FORMER FOR DETECTION AND SEGMENTATION\nWe test two different backbones for dense prediction tasks. The variants of CrossFormer for dense\nprediction (object detection, instance segmentation, and semantic segmentation) are in Table 8. The\narchitectures are the same as those for image classiﬁcation except that different G and I in the ﬁrst\ntwo stages are used. Notably, group size (i.e., G and I) does not affect the shape of weight tensors,\nso backbones pre-trained on ImageNet can be ﬁne-tuned directly on other tasks even if they use\ndifferent G and I.\n13\nTable 8: CrossFormer-based backbones for object detection and semantic/instance segmentation.\nThe example input size is 1280 ×800. D and H mean embedding dimension and the number of\nheads in the multi-head self-attention module, respectively. G and I are group size and interval for\nSDA and LDA, respectively.\nOutput SizeLayer Name CrossFormer-T CrossFormer-S CrossFormer-B CrossFormer-L\nStage-1 320×200\nCross Embed. Kernel size:4×4,8×8,16×16,32×32, Stride=4\nSDA/LDA\n\n\nD1= 64H1= 2G1= 14I1= 16\n\n×1\n\n\nD1= 96H1= 3G1= 14I1= 16\n\n×2\n\n\nD1= 96H1= 3G1= 14I1= 16,\n\n×2\n\n\nD1= 128H1= 4G1= 14I1= 16\n\n×2MLP\nStage-2 160×100\nCross Embed. Kernel size:2×2,4×4, Stride=2\nSDA/LDA\n\n\nD2= 128H2= 4G2= 14I2= 8\n\n×1\n\n\nD2= 192H2= 6G2= 14I2= 8\n\n×2\n\n\nD2= 192H2= 6G2= 14I2= 8\n\n×2\n\n\nD2= 256H2= 8G2= 14I2= 8\n\n×2MLP\nStage-3 80×50\nCross Embed. Kernel size:2×2,4×4, Stride=2\nSDA/LDA\n\n\nD3= 256H3= 8G3= 7I3= 2\n\n×8\n\n\nD3= 384H3= 12G3= 7I3= 2\n\n×6\n\n\nD3= 384H3= 12G3= 7I3= 2\n\n×18\n\n\nD3= 512H3= 16G3= 7I3= 2\n\n×18MLP\nStage-4 40×25\nCross Embed. Kernel size:2×2,4×4, Stride=2\nSDA/LDA\n\n\nD4= 512H4= 16G4= 7I4= 1\n\n×6\n\n\nD4= 768H4= 24G4= 7I4= 1\n\n×2\n\n\nD4= 768H4= 24G4= 7I4= 1\n\n×2\n\n\nD4= 1024H4= 32G4= 7I4= 1\n\n×2MLP\nTable 9: Object detection results on COCO val 2017. “Memory” means the allocated memory\nper GPU reported by torch.cuda.max memory allocated(). ‡indicates that models use different\n(G, I) from classiﬁcation models.\nMethod Backbone G1 I1 G2 I2 Memory #Params FLOPs APb APb50 APb75\nRetinaNet\nCrossFormer-S 7 8 7 4 14.7G 40.8M 282.0G 44.4 65.8 47.4\n1×schedule\nCrossFormer-S‡ 14 16 14 8 11.9G 40.8M 272.1G 44.2 65.7 47.2\nCrossFormer-B 7 8 7 4 22.8G 62.1M 389.0G 46.2 67.8 49.5\nCrossFormer-B‡ 14 16 14 8 20.2G 62.1M 379.0G 46.1 67.7 49.0\nMask-RCNN\nCrossFormer-S 7 8 7 4 15.5G 50.2M 301.0G 45.4 68.0 49.7\n1×schedule\nCrossFormer-S‡ 14 16 14 8 12.7G 50.2M 291.1G 45.0 67.9 49.1\nCrossFormer-B 7 8 7 4 23.8G 71.5M 407.9G 47.2 69.9 51.8\nCrossFormer-B‡ 14 16 14 8 21.0G 71.5M 398.1G 47.1 69.9 52.0\nTable 10: Semantic segmentation results on ADE20K validation set with semantic FPN or UPerNet\nas heads.\nBackbone G1 I1 G2 I2 Semantic FPN (80K iterations) UPerNet (160K iterations)Memory #Params FLOPs IOUMemory #Params FLOP IOU MS IOU\nCrossFormer-S7 8 7 4 20.9G 34.3M 220.7G 46.0 − 62.3M 979.5G 47.6 48.4\nCrossFormer-S‡ 14 16 14 8 20.9G 34.3M 209.8G 46.4 14.6G 62.3M 968.5G 47.4 48.2\nCrossFormer-B7 8 7 4 14.6G 55.6M 331.0G 47.7 15.8G 83.6M 1089.7G 49.7 50.6\nCrossFormer-B‡ 14 16 14 8 14.6G 55.6M 320.1G 48.0 15.8G 83.6M 1078.8G 49.2 50.1\nCrossFormer-L7 8 7 4 25.3G 95.4M 497.0G 48.7 18.1G 125.5M 1257.8G 50.4 51.4\nCrossFormer-L‡ 14 16 14 8 25.3G 95.4M 482.7G 49.1 18.1G 125.5M 1243.5G 50.5 51.4\nB E XPERIMENTS\nB.1 O BJECT DETECTION\nTable 9 provides more results on object detection with RetinaNet and Mask-RCNN as detection\nheads. As we can see, a smaller (G, I) achieves a higher AP than a larger one, but the performance\ngain is marginal. Considering that a larger (G, I) can save more memory cost, we think (G1 =\n14, I1 = 16 , G2 = 14 , I2 = 8) , which accords with conﬁgurations in Table 8, achieves a better\ntrade-off between the performance and cost.\n14\nB.2 S EMANTIC SEGMENTATION\nSimilar to object detection, we test two different conﬁgurations of (G, I) for semantic segmenta-\ntion’s backbones. The results are shown in Table 10. As we can see, the memory costs of the two\nconﬁgurations are almost the same, which is different from experiments on object detection. Further,\nwhen taking semantic FPN as the detection head, CrossFormers‡show advantages over CrossForm-\ners on both IOU (e.g., 46.4 vs. 46.0) and FLOPs (e.g., 209.8G vs. 220.7G). When taking UPerNet\nas the segmentation head, a smaller (G, I) achieves higher performance like object detection.\n15"
}