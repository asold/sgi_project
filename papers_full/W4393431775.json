{
  "title": "CAT-DTI: cross-attention and Transformer network with domain adaptation for drug-target interaction prediction",
  "url": "https://openalex.org/W4393431775",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2462336409",
      "name": "Xiao-ting Zeng",
      "affiliations": [
        "Shenzhen University"
      ]
    },
    {
      "id": "https://openalex.org/A2115016245",
      "name": "Weilin Chen",
      "affiliations": [
        "Shenzhen University"
      ]
    },
    {
      "id": "https://openalex.org/A2117435462",
      "name": "Baiying Lei",
      "affiliations": [
        "Shenzhen University"
      ]
    },
    {
      "id": "https://openalex.org/A2462336409",
      "name": "Xiao-ting Zeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2115016245",
      "name": "Weilin Chen",
      "affiliations": [
        "Shenzhen University"
      ]
    },
    {
      "id": "https://openalex.org/A2117435462",
      "name": "Baiying Lei",
      "affiliations": [
        "Shenzhen University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2985881898",
    "https://openalex.org/W3138412820",
    "https://openalex.org/W4211219865",
    "https://openalex.org/W2997235817",
    "https://openalex.org/W2889321024",
    "https://openalex.org/W2337424010",
    "https://openalex.org/W2918239264",
    "https://openalex.org/W3023126697",
    "https://openalex.org/W2902812092",
    "https://openalex.org/W4239510810",
    "https://openalex.org/W2911964244",
    "https://openalex.org/W2148512505",
    "https://openalex.org/W2104950117",
    "https://openalex.org/W3176168331",
    "https://openalex.org/W2471196942",
    "https://openalex.org/W2899788782",
    "https://openalex.org/W1988037271",
    "https://openalex.org/W3194218357",
    "https://openalex.org/W3005769002",
    "https://openalex.org/W2473684010",
    "https://openalex.org/W4210442302",
    "https://openalex.org/W3096561213",
    "https://openalex.org/W2061136308",
    "https://openalex.org/W3166613829",
    "https://openalex.org/W3206585172",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3028589594",
    "https://openalex.org/W3018980093",
    "https://openalex.org/W4318981550",
    "https://openalex.org/W2967158012",
    "https://openalex.org/W4289544520",
    "https://openalex.org/W3158002239",
    "https://openalex.org/W4205328831",
    "https://openalex.org/W2009313526",
    "https://openalex.org/W2160257187",
    "https://openalex.org/W2195660088"
  ],
  "abstract": "Abstract Accurate and efficient prediction of drug-target interaction (DTI) is critical to advance drug development and reduce the cost of drug discovery. Recently, the employment of deep learning methods has enhanced DTI prediction precision and efficacy, but it still encounters several challenges. The first challenge lies in the efficient learning of drug and protein feature representations alongside their interaction features to enhance DTI prediction. Another important challenge is to improve the generalization capability of the DTI model within real-world scenarios. To address these challenges, we propose CAT-DTI, a model based on cross-attention and Transformer, possessing domain adaptation capability. CAT-DTI effectively captures the drug-target interactions while adapting to out-of-distribution data. Specifically, we use a convolution neural network combined with a Transformer to encode the distance relationship between amino acids within protein sequences and employ a cross-attention module to capture the drug-target interaction features. Generalization to new DTI prediction scenarios is achieved by leveraging a conditional domain adversarial network, aligning DTI representations under diverse distributions. Experimental results within in-domain and cross-domain scenarios demonstrate that CAT-DTI model overall improves DTI prediction performance compared with previous methods.",
  "full_text": "Open Access\n© The Author(s) 2024. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits \nuse, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original \nauthor(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third \nparty material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the mate-\nrial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://\ncreativecommons.org/licenses/by/4.0/. The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdo-\nmain/zero/1.0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nRESEARCH\nZeng et al. BMC Bioinformatics          (2024) 25:141  \nhttps://doi.org/10.1186/s12859-024-05753-2\nBMC Bioinformatics\nCAT-DTI: cross-attention and Transformer \nnetwork with domain adaptation \nfor drug-target interaction prediction\nXiaoting Zeng1, Weilin Chen2* and Baiying Lei3* \nAbstract \nAccurate and efficient prediction of drug-target interaction (DTI) is critical to advance \ndrug development and reduce the cost of drug discovery. Recently, the employ-\nment of deep learning methods has enhanced DTI prediction precision and efficacy, \nbut it still encounters several challenges. The first challenge lies in the efficient learn-\ning of drug and protein feature representations alongside their interaction features \nto enhance DTI prediction. Another important challenge is to improve the gener-\nalization capability of the DTI model within real-world scenarios. To address these \nchallenges, we propose CAT-DTI, a model based on cross-attention and Transformer, \npossessing domain adaptation capability. CAT-DTI effectively captures the drug-target \ninteractions while adapting to out-of-distribution data. Specifically, we use a convolu-\ntion neural network combined with a Transformer to encode the distance relationship \nbetween amino acids within protein sequences and employ a cross-attention module \nto capture the drug-target interaction features. Generalization to new DTI prediction \nscenarios is achieved by leveraging a conditional domain adversarial network, aligning \nDTI representations under diverse distributions. Experimental results within in-domain \nand cross-domain scenarios demonstrate that CAT-DTI model overall improves DTI \nprediction performance compared with previous methods.\nKeywords: Drug-target interaction, Transformer, Cross-attention, Domain adaptation\nIntroduction\nDrug discovery is highly valued in the current biomedical field [1]. In drug discovery, \nverifying whether a drug interacts with a certain target is a key step in proving drug \neffectiveness [2]. In vitro screening experiments are feasible but labor-intensive, expen -\nsive and time-consuming [3]. The utilization of computerized screening for potential \nDTI candidates has been substantiated as an effective strategy to aid biologists in the \nidentification of genuine DTIs through wet-lab experiments [4]. Therefore, computer-\naided DTI prediction has aroused great interest and received increasing attention.\nExisting DTI prediction methods can generally be divided into three categories: ligand-\nbased, structure-based and machine learning-based [5]. Traditional structure-based and \n*Correspondence:   \ncwl@szu.edu.cn; leiby@szu.edu.cn\n1 School of Computer \nand Software, Shenzhen \nUniversity, Shenzhen 518060, \nChina\n2 Marshall Laboratory \nof Biomedical Engineering, \nShenzhen University Medical \nSchool, Shenzhen University, \nShenzhen 518055, China\n3 School of Biomedical \nEngineering, Shenzhen \nUniversity, Shenzhen 518055, \nChina\nPage 2 of 20Zeng et al. BMC Bioinformatics          (2024) 25:141 \nligand-based in silico virtual screening methods have gained increased attention owing \nto the demonstrated relative effectiveness [6–8]. However, these traditional methods \nhave significant limitations. For example, the widely adopted molecular docking method \nis relatively inefficient, sometimes slow due to the huge amount of computation and the \nscoring function has relatively low accuracy [9]. In short, ligand-based methods face lim-\nited application scope and performance challenge due to the limited number of known \nligands for some proteins. In addition, structural-based methods are limited by the lack \nof three-dimensional structures and ligand information for the majority of protein, \nthereby constraining the development.\nTraditional machine learning models such as support vector machine (SVM) [10] and \nrandom forest (RF) [11] are also used for DTI prediction [12]. For example, Faulon et al. \n[13] used molecular features and reaction features as input to the SVM kernel function \nfor DTI prediction. Wang et al. [14] used the features extracted by the Boruta algorithm \nas the input of the RF algorithm for DTI prediction. However, although these methods \nare simple and effective, the performance is far from satisfactory.\nRecently, as an important branch of machine learning, deep learning has also made \nsignificant progress in DTI prediction. In the early days, researchers used hand-crafted \ndescriptors of drugs and proteins to make predictions through a fully connected neural \nnetwork [15]. Subsequently, Lee et al. proposed DeepConv-DTI [16], which used convo -\nlution neural network (CNN) to extract protein features, employed the extended con -\nnectivity fingerprints (ECFP) algorithm [17] to calculate drug features, and predicted \nDTI through fully connected network (FCN). However, the interaction characteristics \nof drug-protein pairs are ignored. Furthermore, the adoption of advanced feature extrac-\ntion techniques, such as the DynCNN module in SAG-DTA [18] and DrugVQA [19], \nhighlight the continuous efforts to optimize feature extraction methods.\nOver the past few years, graph neural network (GNN) have demonstrated excellent \npredictive performance in addressing key prediction challenges in the field of bioinfor -\nmatics by utilizing the powerful feature representation learning capability [20–22]. To \nextract the topological information of drugs, Nguyen et  al. designed GraphDTA [23] \nbased on GNN, treating drugs as molecular graphs, using GNN and CNN to extract \ndrug and protein features respectively to predict the affinity of drugs and targets. Despite \nthe use of stronger feature extraction modules, the important fact that the interactions \nbetween molecules are mainly focused on the relevant substructures of drugs and pro -\nteins is ignored [24]. Furthermore, a simply connection of drug and protein features fails \nto capture the complex interactions between them.\nIn order to more accurately model the interactions between drugs and proteins, some \nstudies have introduced the attention mechanism into DTI prediction [25]. Nowadays, \nattention mechanisms have been widely used for revealing the contribution of different \ncomponents of a drug or target on interaction [19] and describing interactions between \ntargets and drugs [26]. HyperAttentionDTI [27] assigned attention vectors to each \natom and amino acid on the basis of CNN to enhance feature expression. Although this \nmethod considered the interactions representation between drugs and proteins, the lim-\nited receptive field of CNN limits the ability to capture global dependencies.\nPage 3 of 20\nZeng et al. BMC Bioinformatics          (2024) 25:141 \n \nInspired by the powerful ability of Transformer [28] to capture features between two \nsequences, Chen et al. proposed TransformerCPI [29], using Transformer to predict DTI \nfrom the SMILES of drugs and protein amino acid sequences. Huang et  al. proposed \nMolTrans [30], which applied Transformer to extract features from the substructure of \ndrug and protein sequence and combined them into interaction map for compound-\nprotein interaction prediction. However, this method mainly focuses on the interaction \nfeatures between drugs and protein substructures, while ignoring the importance of uti -\nlizing original feature information.\nDue to the wide scope and complexity of the chemical and genomic fields, DTI predic-\ntion often faces great challenges in real-world scenarios. Recently, Bai et  al. proposed \nthe DrugBAN [31] using bilinear attention to capture the local interaction representa -\ntion of drug and target for DTI prediction. In cross-domain prediction tasks, the con -\nditional adversarial domain adaptation method is introduced to transfer source domain \nknowledge to the target domain and demonstrate excellent cross-domain generalization \ncapability.\nIn order to cope with the problem that many models [16, 23, 27, 31] cannot fully cap -\nture global context information while retaining local features when processing global \nand local information, resulting in the inability to extract sufficiently effective feature \ninformation. In this work, we propose a model named CAT-DTI, whose protein features \nare extracted by a protein feature encoder combining CNN and Transformer, which fully \nconsiders global context information while capturing local features of protein sequences. \nBesides, attention-based methods [23, 32–34] generally focus more on extracting inter -\nnal features of drugs and targets, but rarely introduce attention to mine DTI representa -\ntions. With the aim of better preserving the internal features of drugs and proteins while \ndeeply exploring the interaction information between them, we input the extracted drug \nand protein features into the cross-attention module for feature fusion. We also notice \nthat the training of the model in a specific domain is mainly aimed at the distribution \nof the domain and the drug-target pairs to be predicted in practical applications may \nhave a different distribution from the training data, resulting in the inability to directly \ntransfer the existing knowledge to new scenarios. Therefore, in cross-domain tasks, we \nemploy conditional domain adversarial network (CDAN) to better understand and pre -\ndict DTI in domains that are distributed differently from the training data, thus enhanc -\ning the cross-domain generalization ability.\nOur contributions are summarized in the following points. First, we propose a deep \nlearning model named CAT-DTI, which uses GCN and CNN combined with Trans -\nformer to extract feature maps of drugs and proteins, respectively. Second, we employ \ncross-attention module that fuse drug and protein features, effectively capture and \nprocess the interaction features between drugs and proteins while retaining the inter -\nnal feature information. For the cross-domain prediction task, we adopt the CDAN to \nenable the model to adapt and cope with the characteristics of new domains, improve \nthe performance of CAT-DTI in cross domain DTI prediction tasks and enhance the \ngeneralization performance and practical value of CAT-DTI. Compared with other base-\nline methods, CAT-DTI achieves generally better predictive performance on three pub -\nlic datasets.\nPage 4 of 20Zeng et al. BMC Bioinformatics          (2024) 25:141 \nMethods\nAn overview of CAT-DTI framework is illustrated in Fig.  1a. Given drug SMILES and \nprotein amino acid sequences as input, the protein and drug embeddings are gener -\nated. Drug embedding is input into GCN to extract feature representations of drug mol -\necules (i.e., drug feature map FD ). The protein embedding is passed to the protein feature \nencoder as shown in Fig.  1b, which combines the CNN and Transformer to extract the \nprotein feature map FP , capturing local features and global context information in the \nprotein sequence simultaneously. Next, the cross-attention module interacts protein and \ndrug features for feature fusion to capture the interaction relationship between drugs \nand targets, as shown in Fig.  1c. Specifically, we swap the key and value of protein atten -\ntion with those of drug attention. After obtaining the feature maps, the original features \nare integrated to construct the final features for both drugs and proteins. Through max-\npooling and concatenation, the joint feature f for drug and protein target is produced \nand input into the decoder to predict DTI. To enhance the generalization performance \nof CAT-DTI in real-world scenarios for novel drug-target pairs, we integrate the domain \nadaptation module CDAN into the framework, which is employed to adapt the repre -\nsentations of drugs and proteins, thereby facilitating effective alignment between source \nand target domain distributions.\nGCN for drug molecular graph\nRegarding the drug feature extraction process, we transform drug SMILES into a cor -\nresponding 2D molecular graph. To capture the node information within the graph, we \nfirst initialize each atom node. Each atom is denoted by a 74-dimensional integer vec -\ntor that encapsulates eight distinct attributes, including the atom type, the atom degree, \nthe number of implicit Hs, the formal charge, the number of radical electrons, the atom \nhybridization, the number of total Hs and whether the atom is aromatic.\nThe drug feature encoder transmits and aggregates information on the drug molecular \nstructure through a three-layer GCN, thereby achieving extraction and representation \nof drug feature. In each layer of GCN operation, each row of the drug representation \nFig. 1 Framework of the proposed CAT-DTI. a Overview of CAT-DTI framework. b Details of the protein \nfeature encoder. c Processes of cross-attention\nPage 5 of 20\nZeng et al. BMC Bioinformatics          (2024) 25:141 \n \nrepresents an aggregated representation of adjacent atomic nodes in the drug molecule. \nEach GCN layer uses the information of neighboring atomic nodes to update the fea -\nture representation of each atomic node, allowing the model to effectively capture the \ncorrelation information between neighboring atomic nodes. We retain node-level drug \nrepresentations for subsequent explicit learning of interactions with protein fragments. \nWe set the maximum number of nodes in the graph to be md . Therefore, the node fea -\nture matrix of each graph is denoted as Md ∈ Rmd×74 . Furthermore, we employ a simple \nlinear transformation to establish Fd = M dW ⊤\no  , resulting in a real-valued dense matrix \nFd ∈ Rm d×Dd as input features, where Dd is the drug embedding dimension. Finally, we \nobtain the drug feature map FD ∈ Rm d×Dd through the drug feature encoder, which can \nbe expressed as:\nwhere W i\ngcn and bi\ngcn are the weight matrices and bias vector of the i-th layer of GCN. ˜A is \nthe adjacency matrix with added self-connection. H i\nd denotes the hidden node represen-\ntation of layer i with H 0\nd = Fd.\nFeature encoder for protein\nTo enhance protein sequence feature representation and capture long-distance rela -\ntionship between sequence tags, we introduce a protein feature encoder that combines \nCNN and Transformer. Traditional CNN may struggle with long sequences due to the \nlimited local receptive fields, so we combine the global attention mechanism of Trans -\nformer to capture long-distance dependence in protein sequences. By fusing the local \nperception capabilitiy of CNN and the global attention mechanism of Transformer, our \nmodel simultaneously considers local features and global context information in protein \nsequences, thereby extracting more effective protein features.\nIt is worth noting that before the feed forward layer of Transformer, we add 1D CNN \nto process local information. By sliding the convolution kernel on the protein sequence, \nwe captured the local pattern and substructure of the protein. Combined with the \nadvantages of Transformer in handling long-range dependencies, our model achieves \nthe fusion of local and global information in the protein feature encoding process, which \nis beneficial to enhance the representation of protein sequence features. In our work, \na three-layer protein feature encoder is used to capture protein features, as shown in \nFig. 1b, where each layer includes a multi-head self-attention, CNN and a feed-forward \nneural network. Specifically, the protein sequence is input to the protein feature encoder \nwith the feature matrix Fp ∈ Rlp×D p , where lp is the length of the protein sequence and \nDp is the protein embedding dimension. The matrices Q ∈ Rlp×D p , K ∈ Rlp×D p and \nV ∈ Rlp×D p in different feature spaces based on the feature matrix Fp are generated by \nthe linear layer as follows:\n(1)H i+1\nd = σ( GCN (A,W i\ngcn,bi\ngcn,H i\nd)),\n(2)\n\n\n\nQ = Fp · W Q + bQ\nK = Fp · W K + bK ,\nV = Fp · W V + bV\nPage 6 of 20Zeng et al. BMC Bioinformatics          (2024) 25:141 \nwhere W Q ∈ RDp×Dp , W K ∈ RDp×Dp , W V ∈ RDp×Dp are learnable parameter weights. bQ , \nbK  and bV  are bias vectors. Given Q, K and V matrices, the self-attention layer computes \nthe attention weights as follows:\nwhere dk is the dimension of K. The output XM of the multi-head attention layer is gen -\nerated as follows:\nwhere WM ∈ RDp×Dp is the learnable weight matrix and bM  is the bias vector.\nThe multi-head attention layer extracts information from diverse representation sub -\nspace, enhancing model robustness. Therefore, long-range relationships between amino \nacids spanning the entire sequence can be learned with self-attention weights. Addition -\nally, the first ADD & Norm layer implements a residual connection with original protein \nfeature matrix Fp and then follow by normalization, expressed as follows:\nSubsequently, a three-layer CNN is inserted after the first ADD & Norm layer to extract \nlocal feature in the protein sequence:\nAfter the second ADD & Norm layer, we derive the protein feature map FP ∈ Rlp×D p as \nfollows:\nCross‑attention module\nAfter obtaining the feature maps for drugs and proteins through the feature encoder, we \nintroduce a cross-attention module to effectively model the interaction between drugs \nand proteins, thereby capturing enhanced representations of their interaction and pro -\nvides more reliable feature representation for DTI prediction. By performing two-way \ninformation interaction between the key and value of protein attention and the key and \nvalue of drug attention, the information exchange and association between drug and \nprotein is realized, thus capturing the interaction features between drug and protein tar-\nget. In this process, protein features can adjust their own expression by attention weights \nof drug features, and vice versa. Such an interaction and adjustment mechanism enable \nthe cross-attention module to promote information flow across feature maps, effectively \nfuse drug and protein features, and extract more comprehensive DTI feature representa-\ntion. The cross-attention module is depicted in Fig. 1c and primarily consists of drug and \nprotein attention.\nIn this section, we set D e = D d = D p . For drugs, the drug feature map FD is passed \nthrough the linear layer to calculate the drug query vector Q i\nD ∈ Rm d×dhead , and then the \nprotein feature map FP is obtained through a linear layer, which is further calculated as \n(3)Attention(Q,K ,V ) = Softmax ( Q · K ⊤\n√\ndk\n)V ,\n(4)XM = MutiHead(Q,K,V) = Concat(Attention(Q,K,V))W M + bM ,\n(5)XAN = LayerNorm(Fp + XM ),\n(6)XCNN = CNN (XAN ),\n(7)FP = LayerNorm(X CNN + XAN ),\nPage 7 of 20\nZeng et al. BMC Bioinformatics          (2024) 25:141 \n \nthe drug ke y vector K i\nD ∈ Rlp×dhead and value vector V i\nD ∈ Rlp×dhead . The query, key and \nvalue for the drug are obtained as follows:\nwhere W i\nq, W i\nk, W i\nν ∈ RD e×dhead are different weight matrices in the linear layer and \ndhead = D e/heads is the channel dimension. i = 1, 2,··· ,heads  , where heads are the \nnumber of attention heads.\nProtein attention follows a process similar to drug attention. The protein feature map \nFP is input into the linear layer to calculate the protein query vector Q i\nP ∈ Rlp ×dhead , and \nthen the drug feature map is taken to generate the protein key vector K i\nP ∈ Rm d×dhead \nand protein value vector V i\nP ∈ Rm d×dhead . The queries, keys and values of proteins are \ncalculated by the following formulas:\nwhere the weight matrices W i\nq ∈ RD e×dhead , W i\nk ∈ RD e×dhead and W i\nv ∈ RD e×dhead share the \nsame weights as drug attention. Through the application of a softmax function, the drug \nand protein attention matrices are computed as:\nwhere dK i\nD\n= dK i\nP\n= dhead is the dimension of K for drug and protein. The drug/protein \nfeature map for each head is obtained by multiplying the drug/protein attention matrix \nof each attention head with the corresponding drug/protein value matrix. Subsequently, \nthe drug/protein feature maps of all attention heads are concatenated in the channel \ndimension and fed into the linear layer to obtain the final drug feature representation \nZP ∈ Rlp×D p and protein feature map ZD ∈ Rmd×Dd received attention:\nwhere i = 1, 2,··· ,heads  and W Z ∈ RDe×De is the shared weight matrix.\nNext, the feature maps of interest are combined with the original feature maps to \nobtain the final drug feature map FZD ∈ Rm d×Dd and protein feature map FZP ∈ Rlp×D p:\n(8)\n\n\n\nQi\nD = FD · W i\nq\nK i\nD = FP · W i\nk,\nV i\nD = FP · W i\nν\n(9)\n\n\n\nQ i\nP = FP · W i\nq\nK i\nP = FD · W i\nk\nV i\nP = FD · W i\nv\n,\n(10)Ai\nD = Softmax\n\nQ i\nD · K i\nD\n⊤\n�\ndK i\nD\n\n,\n(11)A i\nP = Softmax\n\nQ i\nP · K i\nP\n⊤\n�\ndK i\nP\n\n,\n(12)ZD = Concat (Ai\nD × V i\nD) × W Z,\n(13)ZP = Concat (Ai\nP × V i\nP) × W Z,\nPage 8 of 20Zeng et al. BMC Bioinformatics          (2024) 25:141 \nThe drug and protein feature maps are downsampled by using a global max-pooling \noperation to generate one-dimensional drug feature vector dmp ∈ RDd and protein fea -\nture vector pmp ∈ RDp:\nFinally, we concatenate dmp and pmp to obtain the joint feature representation f ∈ R2D e:\nDrug‑target interaction prediction\nIn order to predict the DTI probability, we input the joint representation f into the \ndecoder, which consists of a fully connected classification layer. Finally, the DTI prob -\nability p is generated as follows:\nwhere W and b are learnable weight matrix and bias vector.\nDuring model training, we employ backpropagation to concurrently optimize the \nlearnable parameters. Our objective in training is to minimize the cross-entropy loss \nfunction:\nwhere yi denotes the ground-truth label of the i-th drug-target pair. p i represents DTI \nprediction score predicted by the model. θ is the set of learnable weight matrices and \nbias vectors and /afii9838 is a hyperparameter for L2 regularization to prevent overfitting.\nCross‑domain adaptation enhances generalization\nDeep learning models show excellent performance on similar data (i.e., in-domain) \nthat is distributed with the training data. However, the performance on different data \nwith different distributions (i.e., cross-domain) is not satisfactory. To this end, we \nemploy   the  CDAN module  to improve the generalization ability of CAT-DTI model \nfrom a source domain rich in labeled data to a target domain containing only unlabeled \ndata. Figure 2 shows the framework after integrating the CDAN module into CAT-DTI \n(i.e., CAT-DTICDAN  ), which consists of three key components: Feature Extractor F(∗) , \nDecoder G(∗) and Discriminator D(∗).\n(14)FZD = 0.5ZD + 0.5FD ,\n(15)FZP = 0.5Z P + 0.5FP,\n(16)dmp = Maxpooling (FZD ),\n(17)pmp = Maxpooling (FZP),\n(18)f = Concat(dmp ,pmp ),\n(19)p = σ\n(\nWf + b\n)\n,\n(20)L =−\n∑\ni\n(\nyi log (pi) + (1 − yi) log (1 − p i)\n)\n+ 1\n2 /afii9838||θ||2\n2 ,\nPage 9 of 20\nZeng et al. BMC Bioinformatics          (2024) 25:141 \n \nOn the cross-domain task, given NS labeled drug-target pairs PS ={ (xi\ns,yi\ns)}i=1\nN S  in the \nsource domain and NT unlabeled drug-target p airs Pt ={ (xi\nt)}i=1\nN T  in the target domain. \nWe rely on CDAN to adjust the distribution of samples to optimize cross-domain pre -\ndiction performance. The feature extractor F(∗) is the drug and protein feature encoder \ntogether with the cross-attention module to generate a joint representation of the input \ndomain data, namely f i\ns = F (x i\ns) and f j\nt = F (x j\nt) . For the decoder G(∗) , we employ a fully \nconnected classification layer and follow a softmax function as G(∗) to obtain predicted \nclassification results gi\ns = G (fi\ns ) ∈ R2 and g j\nt = G (fj\nt ) ∈ R2 . Subsequently, the joint rep -\nresentation f and the classifier prediction g are embedded into a joint conditional repre -\nsentation c ∈ R2D e , which is defined as follows:\nwhere FLATTEN performs a flattening operation on the outer product of the f and g \nvectors and ⊗ is the outer product.\nAdhering to CDAN principles, we employ a domain discriminator D(∗) to align the \njoint representation f and predicted classification distribution g of the source and target \ndomains. D(∗) is a domain discriminator composed of a three-layer FCN that learns to \ndistinguish whether a joint conditional representation c originates from the source or \ntarget domain. F(∗) and G(∗) are trained to minimize the cross-entropy loss L of the \nsource domain with source label information, generating a joint conditional  represen -\ntation c that confuses the discriminator D(∗) . In the cross-domain task, we utilize two \nlosses: one for optimizing classification prediction and the other for optimizing the dis -\ntribution alignment of the source and target domain:\n(21)c = FLATTEN (f ⊗ g ),\n(22)LS (F , G ) = E(xis,yis)∼PsL\n(\nG\n(\nF\n(\nxi\ns\n))\n, yi\ns\n)\n,\nFig. 2 Diagram of cross-domain adaptation process. CDAN is a domain adaptation technique designed \nto address domain shift challenges with different distributions. We utilize CDAN to integrate the joint \nrepresentation f of the source and target domain, along with classifier prediction g into the joint conditional \nrepresentation distinguished by the discriminator. The discriminator is structured as a three-layer fully \nconnected network with the specific goal of distinguishing the target domain from source domain by \nminimizing domain classification error\nPage 10 of 20Zeng et al. BMC Bioinformatics          (2024) 25:141 \nwhere LS is the cross-entropy loss on the labeled source domain and Ladν is the adver -\nsarial loss for the domain discriminator.\nThe optimization problem is written as a minimax paradigm:\nwhere ω is a hyper parameter for weighting Ladν . By introducing adversarial training in \nLadν , the difference in data distribution between the source domain and target domain is \nreduced, thereby enhancing the generalization ability of cross-domain prediction.\nExperiments and results\nDatasets and data processing\nWe comprehensively evaluate CAT-DTI and six baseline models on three public data -\nsets: BindingDB, BioSNAP and Human. The BindingDB database records the binding \naffinity information of small drug molecules and proteins that have been verified through \nexperiments and mainly studies the interaction between drug-like molecules and pro -\nteins. In experiments, we use a low-bias version of the BindingDB dataset constructed \nby previous research [35]. The BioSNAP dataset is constructed according to previous \nresearch [ 30, 36] derived from the DrugBank database [37], including 4510 drugs and \n2181 proteins. It is designed as a balanced dataset containing validated positive samples \nand an equal number of unseen negative samples. Drawing on the previous studies [19, \n29], we also employ a balanced version of the Human dataset containing equal numbers \nof positive and negative samples. The statistics of the three datasets are shown in Table 1.\nIn experiments, we use different splitting strategies on the datasets for in-domain and \ncross-domain tasks. For in-domain evaluation, each experimental dataset is randomly \nsplit into training, validation and test sets in a ratio of 7:1:2. For cross- domain tasks, the \ndecision to exclude the Human dataset stems from its comparatively limited sample size. \nIn order to ensure the model  has robust performance in cross-domain scenarios,  the \ndatasets should have sufficient data volume and sample diversity, so our cross-domain \nperformance evaluation focuses on the large-scale BindingDB and BioSNAP datasets. \nWe utilize the datasets from the previous study [31], which adopts a clustering-based \npair split strategy to build cross-domain scenario and cluster drugs and target proteins \nfrom BindingDB and BioSNAP datasets respectively for cross-domain performance \nevaluation. Specifically, a single-linkage clustering method is used to cluster from the \nbottom and hierarchically to ensure that the distance between samples in different clus -\nters always exceeds a predefined minimum distance threshold, which helps prevent the \n(23)Lad ν (F ,G ,D ) = Exi\nt∼Pt log\n(\n1 − D\n(\nfi\nt ,gi\nt\n))\n+ Exj\ns∼Ps\nlog(D (fj\ns ,gj\ns)),\n(24)max\nD\nmin\nF,G\nLS(F,G ) − ω Ladν (F,G ,D),\nTable 1 Details of datasets used in this work\nDadaset Drug Protein Association\nBindingDB 14,643 2,623 49,199\nBioSNAP 4,510 2,181 27,464\nHuman 2,726 2,001 6,728\nPage 11 of 20\nZeng et al. BMC Bioinformatics          (2024) 25:141 \n \nformation of clusters that are too close. For each dataset, the single-linkage algorithm is \nused for the clustering of drugs and proteins based on the ECFP4 [17] fingerprint and \npseudo-amino acid composition (PSC) [38], respectively. Since the clustering-based pair \nsplit enables the quantitative construction of cross-domain task by taking into account \nthe similarity between drugs and proteins, we use Jaccard distance and cosine distance \non ECFP4 and PSC respectively to accurately measure pairwise distances. During the \nclustering of drugs and proteins, the distance threshold is set to 0.5 to ensure that the \nclusters do not become too large while maximizing the separation of different samples. \nSo far, 2,780 drug clusters and 1,693 protein clusters have been obtained in the Bind -\ningDB dataset and 2,387 drug clusters and 1978 protein clusters have been obtained in \nthe BioSNAP dataset. Through the clustering-based pair split strategy, the source and \ntarget domain are characterized by non-overlapping sets with different distributions. \nFollowing the general setup of domain adaptation, we use all labeled source domain data \nand 80% unlabeled target domain data as the training set and the remaining 20% labeled \ntarget domain data as the test set. While cross-domain evaluation presents greater chal -\nlenges compared to in-domain random splitting, it emerges as a more efficacious meth -\nodology for assessing the generalization capacity of model in the practical realm of drug \ndiscovery.\nBaselines\nWe compare CAT-DTI with the following baselines.\n• SVM [10] and RF [11] are used as a classifier to classify encoded drug and protein \nfeatures.\n• GraphDTA [23] uses GNN to encode drug molecule graphs and CNN to encode \nprotein sequences. The learned drug and protein representation vectors are com -\nbined with a simple concatenation.\n• TransformerCPI [29] treats drugs and proteins as two sequences, generating repre -\nsentations of protein sequences and drug atoms. The interaction feature is captured \nby Transformer decoder and the interaction probability is output by a fully con -\nnected layer.\n• MolTrans [30] is a deep learning model that utilizes Transformer to encode drug and \nprotein information and learns the interactions between substructures through a \nCNN-based interaction module.\n• DrugBAN [31] encodes drug SMILES through GCN to obtain drug features and uses \nCNN to capture protein features. Drug-target pairwise interactions are captured by \na bilinear attention network, followed by a fully-connected classifier for DTI predic -\ntion. Besides, conditional domain adversarial network is used to adjust the distribu -\ntion of source and target domains in cross-domain prediction tasks.\nEvaluation metrics\nSince DTI prediction is a classification task, we choose the widely recognized evalua -\ntion metrics AUROC (i.e., the area under the receiver operating characteristic curve) \nand AUPRC (i.e., the area under the precision-recall curve) as our primary assessment \nPage 12 of 20Zeng et al. BMC Bioinformatics          (2024) 25:141 \ncriteria. Additionally, we provide reports on Accuracy, Sensitivity, Specificity, and F1 \nscore. In principally, the higher the AUROC value means the better the performance. \nThe definitions of these evaluation metrics are given as:\nwhere the true positive (TP ) and true negative (TN ) are the number of drug-target with \ninteraction and drug-target without interaction that are successfully identified, respec -\ntively. The false positive (FP ) and false negative (FN ) represent the number of drug-tar -\nget with interaction and drug-target without interaction examples that are incorrectly \nidentified.\nExperimental setting\nDuring the experiments, we allow our model to run for up to 100 epochs on all datasets, \nsetting the experimental batch size to 32. We use the Adam optimizer with a learning \nrate of 2.5 × 10 −5 for the in-domain tasks and 5 × 10−5 for the cross-domain tasks. The \nmaximum sequence length allowed for proteins is set to 1000 and the maximum number \nof atoms allowed for drug molecules is 290. The number of hidden neurons in the fully \nconnected decoder is 512. Five independent experiments are run for each dataset split. \nThe best performing model is the one that exhibits the highest AUROC on the validation \nset, which is used on the test set to get the performance metrics.\nPerformance comparison\nIn‑domain performance evaluation\nIn the in-domain scenario, we use regular CAT-DTI in our experiments, so we do not \nembed the CDAN into the model. We compare CAT-DTI with six baselines in the ran -\ndom split setting: SVM [10], RF [11], GraphDTA [23], TransformerCPI [29], MolTrans \n[30] and DrugBAN [31]. Table  2 shows the comparison on BindingDB, BioSNAP and \nHuman datasets. The experimental results indicate that CAT-DTI demonstrates compet-\nitive performance across all evaluation metrics in the in-domain scenario. Specifically, \nCAT-DTI outperforms other comparative methods in terms of evaluation metrics such \nas AUROC, AUPRC, F1, and specificity on the BioSNAP and Human datasets, which is \nattributed to its strong capability in extracting key features of drugs and proteins, effec -\ntively capturing their interacting characteristics. It is noteworthy that CAT-DTI do not \nachieve the best performance on evaluation metrics such as AUROC, AUPRC, and F1 \non the BindingDB dataset. One possible reason is that the key protein information in \n(25)Accuracy = TP + TN\nTP + FP+ TN + FN ,\n(26)Sensitivity = TP\nTP + FN ,\n(27)Speciﬁcity = TN\nTN + FP,\n(28)F1 = 2TP\n2TP + FP+ FN ,\nPage 13 of 20\nZeng et al. BMC Bioinformatics          (2024) 25:141 \n \nthe BindingDB is mainly reflected in local features, which makes it challenging for the \nTransformer part of the protein feature encoder in CAT-DTI to fully utilize its potential, \nand indirectly affects the model’s ability to capture local features, leading to a decline in \nmodel performance. Nevertheless, CAT-DTI still outperforms other comparative meth -\nods in most cases.\nOverall, the experimental results on the three datasets all confirm the effectiveness of \nthe CAT-DTI for DTI prediction. The improvement of experimental results is attributed \nto our method not only capturing protein local features but also analyzing global con -\ntext information for protein features. Moreover, the incorporation of the cross-attention \nTable 2 Comparison results of CAT-DTI and baselines on three datasets\nBold values indicate the best results achieved by all these competitive methods\nDatasets Methods AUROC AUPRC F1 Sensitivity Specificity Accuracy\nBindingDB SVM [10] 0.904 ± \n0.000\n0.865 ± \n0.001\n0.785 ± \n0.000\n0.776 ± \n0.000\n0.857 ± \n0.002\n0.824 ± 0.001\nRF [11] 0.942 ± \n0.001\n0.923 ± \n0.001\n0.844 ± \n0.002\n0.840 ± \n0.002\n0.893 ± \n0.002\n0.871 ± 0.001\nGraphDTA \n[23]\n0.944 ± \n0.004\n0.923 ± \n0.006\n0.880 ± \n0.005\n0.858 ± \n0.026\n0.897 ± \n0.014\n0.874 ± 0.010\nTransformer-\nCPI [29]\n0.947 ± \n0.003\n0.932 ± \n0.004\n0.888 ± \n0.005\n0.886 ± \n0.016\n0.890 ± \n0.008\n0.888 ± 0.007\nMolTrans \n[30]\n0.947 ± \n0.004\n0.927 ± \n0.006\n0.886 ± \n0.005\n0.877 ± \n0.018\n0.894 ± \n0.014\n0.884 ± 0.007\nDrugBAN \n[31]\n0.961 ± \n0.001\n0.948 ± \n0.001\n0.903 ± \n0.001\n0.894 ± \n0.011\n0.908 ± \n0.009\n0.901 ± \n0.003\nCAT-DTI 0.960 ± \n0.001\n0.947 ± \n0.001\n0.900 ± \n0.001\n0.884 ± \n0.010\n0.913 ± \n0.009\n0.896 ± 0.002\nBioSNAP SVM [10] 0.819 ± \n0.045\n0.839 ± \n0.038\n0.827 ± \n0.053\n0.665 ± \n0.046\n0.835 ± \n0.054\n0.750 ± 0.050\nRF [11] 0.857 ± \n0.001\n0.872 ± \n0.001\n0.787 ± \n0.001\n0.763 ± \n0.002\n0.823 ± \n0.001\n0.793 ± 0.001\nGraphDTA \n[23]\n0.871 ± \n0.001\n0.870 ± \n0.005\n0.807 ± \n0.005\n0.761 ± \n0.015\n0.838 ± \n0.011\n0.800 ± 0.005\nTransformer-\nCPI [29]\n0.876 ± \n0.004\n0.881 ± \n0.007\n0.803 ± \n0.006\n0.768 ± \n0.024\n0.827 ± \n0.012\n0.797 ± 0.008\nMolTrans \n[30]\n0.895 ± \n0.006\n0.899 ± \n0.006\n0.825 ± \n0.007\n0.791 ± \n0.032\n0.848 ± \n0.014\n0.820 ± 0.011\nDrugBAN \n[31]\n0.902 ± \n0.001\n0.905 ± \n0.002\n0.838 ± \n0.003\n0.825 ± \n0.014\n0.847 ± \n0.006\n0.836 ± 0.004\nCAT-DTI 0.909 ± \n0.002\n0.907 ± \n0.004\n0.840 ± \n0.004\n0.816 ± \n0.012\n0.857 ± \n0.006\n0.836 ± \n0.005\nHuman SVM [10] 0.913 ± \n0.000\n0.905 ± \n0.000\n0.811 ± \n0.000\n0.782 ± \n0.000\n0.830 ± \n0.000\n0.838 ± 0.000\nRF [11] 0.939 ± \n0.002\n0.927 ± \n0.001\n0.848 ± \n0.005\n0.833 ± \n0.006\n0.893 ± \n0.007\n0.866 ± 0.006\nGraphDTA \n[23]\n0.965 ± \n0.003\n0.955 ± \n0.003\n0.907 ± \n0.008\n0.912 ± \n0.017\n0.904 ± \n0.016\n0.908 ± 0.008\nTransformer-\nCPI [29]\n0.954 ± \n0.002\n0.941 ± \n0.002\n0.891 ± \n0.005\n0.831 ± \n0.023\n0.939 ± \n0.018\n0.879 ± 0.007\nMol-\nTrans[30]\n0.981 ± \n0.002\n0.976 ± \n0.002\n0.943 ± \n0.005\n0.949 ± \n0.011\n0.939 ± \n0.017\n0.941 ± 0.004\nDrugBAN \n[31]\n0.981 ± \n0.001\n0.969 ± \n0.005\n0.940 ± \n0.004\n0.938 ± \n0.010\n0.941 ± \n0.013\n0.940 ± 0.003\nCAT-DTI 0.983 ± \n0.001\n0.976 ± \n0.003\n0.944 ± \n0.002\n0.929 ± \n0.007\n0.957 ± \n0.008\n0.942 ± \n0.002\nPage 14 of 20Zeng et al. BMC Bioinformatics          (2024) 25:141 \nmodule enables the model to concurrently consider the impact of drug features on \nproteins and the influence of protein features on drugs. This bidirectional interaction \nempowers CAT-DTI to comprehensively comprehend and capture the intricate inter -\nactions between drugs and proteins, thereby achieving the effective fusion of drug and \nprotein target features.\nCross‑domain performance evaluation\nIn-domain classification tasks under random split are relatively simple and of limited \npractical value. In order to better simulate real-world situations, we focus on the more \nchallenging cross-domain DTI prediction, where the training data and test data have dif-\nferent distribution characteristics. In order to deeply explore the knowledge transferabil-\nity in cross-domain prediction, we embed the CDAN module into the CAT-DTI model, \nwhich means using CAT-DTICDAN for cross-domain prediction.\nWe present the cross-domain performance evaluation results on BindingDB and BioS-\nNAP datasets in Table  3. All methods show a significant drop compared to the previ -\nous in-domain prediction results due to the reduced information overlap between \ntraining and test datasets. However, our newly proposed CAT-DTI model clearly out -\nperforms other state-of-the-art models on both datasets. Specifically, the AUROC and \nAUPRC of CAT-DTI on the BioSNAP dataset are 10.4% and 7.1% higher than the sec -\nond-ranked DrugBAN. At the same time, the AUROC and AUPRC of CAT-DTI on the \nBindingDB dataset are 12.4% and 15.4% higher than those of DrugBAN. What’s more \nworth mentioning is that even when the CDAN module is incorporated into DrugBAN \n(i.e., DrugBANCDAN ), the performance of CAT-DTI outperforms DrugBANCDAN model. \nExperimental results demonstrate that the proposed CAT-DTI can effectively handle \nTable 3 Cross-domain performance comparison of CAT-DTI and other baselines on BindingDB and \nBioSNAP Datasets\nBold values indicate the best results achieved by all these competitive methods\nDataset Method AUROC AUPRC F1 Accuracy\nBindingDB SVM [10] 0.490 ± 0.015 0.460 ± 0.001 0.162 ± 0.158 0.531 ± 0.009\nRF [11] 0.493 ± 0.021 0.468 ± 0.023 0.109 ± 0.029 0.535 ± 0.012\nGraphDTA [23] 0.536 ± 0.015 0.496 ± 0.029 0.668 ± 0.001 0.472 ± 0.009\nTransformerCPI [29] 0.597 ± 0.041 0.562 ± 0.031 0.670 ± 0.005 0.490 ± 0.027\nMolTrans [30] 0.554 ± 0.024 0.511 ± 0.025 0.668 ± 0.001 0.470 ± 0.004\nDrugBAN [31] 0.576 ± 0.023 0.535 ± 0.014 0.668 ± 0.002 0.471 ± 0.012\nDrugBANCDAN [31] 0.604 ± 0.027 0.570 ± 0.047 0.675 ± 0.004 0.509 ± 0.021\nCAT-DTI 0.636 ± 0.013 0.573 ± 0.020 0.688 ± 0.004 0.553 ± 0.024\nCAT-DTICDAN 0.678 ± 0.005 0.626 ± 0.021 0.690 ± 0.004 0.572 ± 0.016\nBioSNAP SVM [10] 0.602 ± 0.005 0.528 ± 0.005 0.400 ± 0.122 0.513 ± 0.011\nRF [11] 0.590 ± 0.015 0.568 ± 0.018 0.018 ± 0.010 0.499 ± 0.004\nGraphDTA [23] 0.618 ± 0.005 0.618 ± 0.008 0.672 ± 0.003 0.535 ± 0.024\nTransformerCPI [29] 0.645 ± 0.022 0.642 ± 0.032 0.681 ± 0.009 0.558 ± 0.025\nMolTrans [30] 0.621 ± 0.015 0.608 ± 0.022 0.675 ± 0.006 0.546 ± 0.032\nDrugBAN [31] 0.630 ± 0.007 0.622 ± 0.018 0.671 ± 0.004 0.537 ± 0.034\nDrugBANCDAN [31] 0.685 ± 0.044 0.713 ± 0.041 0.677 ± 0.010 0.565 ± 0.056\nCAT-DTI 0.708 ± 0.008 0.718 ± 0.009 0.695 ± 0.008 0.618 ± 0.031\nCAT-DTICDAN 0.729 ± 0.010 0.733 ± 0.016 0.699 ± 0.008 0.633 ± 0.021\nPage 15 of 20\nZeng et al. BMC Bioinformatics          (2024) 25:141 \n \ncross-domain DTI prediction problems. Compared with previous methods, CAT-DTI \nnot only exhibits improved accuracy but also demonstrates strong cross-domain gen -\neralization capability. We attribute the superiority of CAT-DTI in cross-domain gen -\neralization to its unique framework design and the collaborative interaction of key \ncomponents. The introduced cross-attention module enables CAT-DTI to simultane -\nously consider drug and protein features, establishing bidirectional information corre -\nlation and aiding in a more comprehensive and accurate capture of interactions across \ndifferent domains. This feature fusion method enhances the adaptability of CAT-DTI to \ndiverse data distributions, thereby improving cross-domain generalization performance. \nAdditionally, the protein feature encoder combines CNN and Transformer to provide \nCAT-DTI with comprehensive modeling capability for local and global information \nwithin protein sequences.\nIn recent years, domain adaptation techniques have attracted extensive attention in \nacademia due to their excellent cross-domain knowledge transfer capabilities. In our \nwork, we explore and improve cross-domain DTI prediction by combining CAT-DTI \nmodel with CDAN. As shown in Table  3, the proposed CAT-DTICDAN model has a sig -\nnificant performance improvement after integrating the domain adaptation module. \nSpecifically, compared with the CAT-DTI model, CAT-DTICDAN improves AUROC and \nAUPRC on the BindingDB dataset by 6.6% and 9.2% and also has a gratifying improve -\nment on the BioSNAP dataset. In order to more clearly observe the improvement effect \nof adding the domain adaptation module CDAN to CAT-DTI, we draw the prediction \nresults of CAT-DTI and CAT-DTI CDAN on BindingDB and BioSNAP datasets as radar \ncharts, as shown in Fig.  3. In this way, we can more intuitively observe the positive \nimpact of the CDAN module on cross-domain tasks. By reducing cross-domain dis -\ntribution bias, CAT-DTI demonstrates a substantial enhancement in its generalization \nperformance with the incorporation of the CDAN module. These experimental results \nfurther confirm the significant superiority of CAT-DTI in cross-domain generalization \nability.\nFig. 3 Cross-domain performance comparison of CAT-DTI with and without CDAN module on BindingDB \nand BioSNAP datasets\nPage 16 of 20Zeng et al. BMC Bioinformatics          (2024) 25:141 \nWe analyze that the cross-domain generalization performance of CAT-DTI CDAN has \nbeen improved is attributed to CAT-DTICDAN can better optimize feature alignment and \neffectively reduce the distribution differences between different domains after introduc -\ning the CDAN module. Through the adversarial learning mechanism, CAT-DTI CDAN is \nable to adjust the feature representations of the source domain and the target domain \nto make them statistically more similar, thereby improving the model’s performance on \nthe target domain. This domain adaptation mechanism enables CAT-DTI to better adapt \nto new and unseen domain data and enhances the generalization ability of the model. \nSpecifically, the introduction of the CDAN module helps reduce domain differences and \nmakes CAT-DTI more robust when processing new drug and protein interaction data, \nthus improving its prediction accuracy and adaptability.\nAblation experiments\nTo explore the effect of CNN combined with Transformer for protein feature extrac -\ntion, DTI  feature captured by cross-attention module and domain adaptation module \nCDAN on model prediction performance, we perform ablation experiments. In the in-\ndomain task, we compare the experimental results of CAT-DTI with three variant mod -\nels, including CAT-DTI using only CNN for protein feature encoder after removing the \nTransformer (i.e., Without Transformer), the model using only Transformer for protein \nfeature encoder after removing the CNN (i.e., Without CNN) and the model remov -\ning the cross-attention module (i.e., Without cross-attention). In the cross-domain \ntask, in order to evaluate the efficacy of CDAN, we compare the evaluation results of \nCAT-DTICDAN with CDAN module removed (i.e., CAT-DTI) and DrugBAN fused with \nCDAN (i.e., DrugBANCDAN ). The experimental results are illustrated in Fig. 4.\nBy analyzing the experimental results, we observed an enhancement in the predic -\ntive performance of CAT-DTI when the model integrated the complete module, which \nconfirms the effectiveness of the mechanism that combines CNN and Transformer for \nprotein feature extraction, highlights the efficient interactive capabilities of the cross-\nattention module, and indicates the improvement in cross-domain performance of CAT-\nDTI with the introduction of the CDAN module.\nFig. 4 AUROC and AUPRC for random split and clustering-based split strategies on BindingDB and BioSNAP \ndatasets\nPage 17 of 20\nZeng et al. BMC Bioinformatics          (2024) 25:141 \n \nCase study\nIn order to further verify the effectiveness of CAT-DTI, we selected two repre -\nsentative targets for case study, namely P00519 (Tyrosine-protein kinase ABL1) \nand P35228 (Nitric oxide synthase, inducible). Based on the testing result from the \ntest set of BindingDB, we select drug-target pairs containing P00519 and P35228, \nand then rank the candidate drugs for each protein in descending order according \nto their predicted interaction scores, choosing the top 6 for each target as depicted \nin Table  4. After detailed review, the drug and target pairs listed in the table have \nbeen confirmed to have interactions in the Drugbank database. Taking the example \nof the drug-target pair involving the target protein P00519 and its first candidate \ndrug DB08901 (Ponatinib) from Table  4, the tyrosine-protein kinase ABL1 plays a \npivotal role in various processes related to cell growth and survival. It coordinates \nactin cytoskeleton dynamics by regulating protein tyrosine phosphorylation. On the \nother hand, DB08901 (Ponatinib) is a novel Bcr-Abl tyrosine kinase inhibitor that has \nbeen proven to inhibit the tyrosine kinase activity of Abl and is used to treat chronic \nmyelogenous leukemia. Therefore, the drug numbered DB08901 has been verified as a \nligand for the target protein numbered P00519.\nTable 4 Top-ranked list of predicted drugs for two proteins\nProtein Drug Prediction score\nP00519\n(Tyrosine-protein kinase ABL1)\nDB08901 1.0000\nDB08896 0.9967\nDB01254 0.9831\nDB08350 0.9520\nDB12267 0.9438\nDB08043 0.9118\nP35228\n(Nitric oxide synthase, inducible)\nDB07002 0.9930\nDB02044 0.9912\nDB07318 0.9862\nDB07405 0.9810\nDB09237 0.9444\nDB07388 0.9293\nTable 5 DTI prediction results for five drugs\nDrug Protein Prediction score\nDB00786 (Marimastat) P51512 0.9983\nP08253 0.9935\nP39900 0.9703\nDB01254 (Dasatinib) P12931 0.9862\nP00519 0.8722\nDB06155 (Rimonabant) P21554 0.9990\nDB00482 (Celecoxib) P35354 0.9980\nDB00481 (Raloxifene) Q92731 0.9359\nPage 18 of 20Zeng et al. BMC Bioinformatics          (2024) 25:141 \nFurthermore, we have selected additional 5 drugs to expand our case study, includ -\ning DB00786 (Marimastat), DB01254 (Dasatinib), DB06155 (Rimonabant), DB00482 \n(Celecoxib), and DB00481 (Raloxifene). Based on the testing results, we once again \nselected drug-target pairs from the test set of BindingDB dataset that contain the \naforementioned drugs and have higher predicted interaction scores, as shown in \nTable 5. Similarly, all drug-target pairs in Table  5 can be found in the DrugBank data -\nbase, indicating that evidence of interactions for these drug-target pairs can be found \nin the DrugBank database. For example, DB01254 (Dasatinib) is a tyrosine kinase \ninhibitor that can inhibit the activity of P00519 (Tyrosine-protein kinase ABL1) and \nP12931 (Proto-oncogene tyrosine-protein kinase Src).\nThe above cases demonstrate that our proposed CAT-DTI can effectively predict poten-\ntial drug-target pairs, possessing the capability to identify potential candidate drugs and \nthus improving the virtual screening stage of drug discovery.\nConclusion\nIn this work, we propose a deep learning model named CAT-DTI, which is based on cross-\nattention and Transformer to enhance the accuracy of predicting drug-target interactions. \nWe employ GCN for extracting drug features, while the acquisition of protein target fea-\ntures uses CNN combined with Transformer, which can not only capture local features of \nproteins, but also take into account global context information. The introduction of the \ncross-attention module effectively facilitated bidirectional feature interactions between \ndrugs and proteins, leading to the extraction of more critical DTI features. Furthermore, \nwith the help of CDAN, our model exhibits good adaptability and predictive performance \nin cross-domain task, which enhances the generalization performance of CAT-DTI. \nCompared with other state-of-the-art models and traditional machine learning models, \nexperimental results show that CAT-DTI improves DTI prediction performance in both in-\ndomain and cross-domain setting, especially making promising progress in cross-domain \nprediction tasks.\nAcknowledgements\nWe are grateful to the anonymous reviewers for their constructive comments on the original manuscript.\nAuthor Contributions\nBYL and WLC supervised the study. XTZ designed the model and conducted the experiments. The manuscript was \ndrafted by XTZ and revised by BYL with the support from WLC. All authors read and approved the final manuscript.\nFunding\nThis work was supported in part by the National Natural Science Foundation of China (Grant Nos. U22A2024, and \n62271328), in part by the Shenzhen Science and Technology Program (Grant Nos. JCYJ20220818095809021, and \nKQTD20210811090219022).\nAvailibility of data and materials\nThe data and code can be found online at: https://github.com/ZXT0212/CAT-DTI.\nDeclarations\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nCompeting interest\nThe authors declare that they have no conflict of interest.\nPage 19 of 20\nZeng et al. BMC Bioinformatics          (2024) 25:141 \n \nReceived: 9 February 2024   Accepted: 19 March 2024\nReferences\n 1. Agamah FE, Mazandu GK, Hassan R, Bope CD, Thomford NE, Ghansah A, et al. Computational/in silico methods in \ndrug target and lead prediction. Brief Bioinf. 2020;21(5):1663–75.\n 2. Zhao Q, Yang M, Cheng Z, Li Y, Wang J. Biomedical data and deep learning computational models for predicting \ncompound-protein relations. IEEE/ACM Trans Comput Biol Bioinform. 2021;19(4):2092–110.\n 3. Pan X, Lin X, Cao D, Zeng X, Yu PS, He L, et al. Deep learning for drug repurposing: methods, databases, and applica-\ntions. Wires Comput Mol Sci. 2022;12(4): e1597.\n 4. Hu S, Zhang C, Chen P , Gu P , Zhang J, Wang B. Predicting drug-target interactions from drug structure and protein \nsequence using novel convolutional neural networks. BMC Bioinf. 2019;20:1–12.\n 5. Chen R, Liu X, Jin S, Lin J, Liu J. Machine learning for drug-target interaction prediction. Molecules. 2018;23(9):2208.\n 6. Himmat M, Salim N, Al-Dabbagh MM, Saeed F, Ahmed A. Adapting document similarity measures for ligand-based \nvirtual screening. Molecules. 2016;21(4):476.\n 7. Sieg J, Flachsenberg F, Rarey M. In need of bias control: evaluating chemical data for machine learning in structure-\nbased virtual screening. J Chem Inf Model. 2019;59(3):947–61.\n 8. Maia EHB, Assis LC, De Oliveira TA, Da Silva AM, Taranto AG. Structure-based virtual screening: from classical to \nartificial intelligence. Front Chem. 2020;8:343.\n 9. Su M, Yang Q, Du Y, Feng G, Liu Z, Li Y, et al. Comparative assessment of scoring functions: the CASF-2016 update. J \nChem Inf Model. 2018;59(2):895–913.\n 10. Cortes C, Vapnik V. Support-vector networks. Mach Learn. 1995;20:273–97.\n 11. Breiman L. Random forests. Mach Learn. 2001;45:5–32.\n 12. Ballester PJ, Mitchell JB. A machine learning approach to predicting protein-ligand binding affinity with applications \nto molecular docking. Bioinformatics. 2010;26(9):1169–75.\n 13. Faulon JL, Misra M, Martin S, Sale K, Sapra R. Genome scale enzyme-metabolite and drug-target interaction predic-\ntions using the signature molecular descriptor. Bioinformatics. 2008;24(2):225–33.\n 14. Wang X, Cao T, Jia CM, Tian X, Wang Y. Quantitative prediction model for affinity of drug-target interactions based on \nmolecular vibrations and overall system of ligand-receptor. BMC Bioinf. 2021;22(1):1–18.\n 15. Tian K, Shao M, Wang Y, Guan J, Zhou S. Boosting compound-protein interaction prediction by deep learning. Meth-\nods. 2016;110:64–72.\n 16. Lee I, Keum J, Nam H. DeepConv-DTI: Prediction of drug-target interactions via deep learning with convolution on \nprotein sequences. PLoS Comput Biol. 2019;15(6): e1007129.\n 17. Rogers D, Hahn M. Extended-connectivity fingerprints. J Chem Inf Model. 2010;50(5):742–54.\n 18. Zhang S, Jiang M, Wang S, Wang X, Wei Z, Li Z. SAG-DTA: prediction of drug-target affinity using self-attention graph \nnetwork. Int J Mol Sci. 2021;22(16):8993.\n 19. Zheng S, Li Y, Chen S, Xu J, Yang Y. Predicting drug-protein interaction using quasi-visual question answering system. \nNat Mach Intell. 2020;2(2):134–40.\n 20. Wei L, Zou Q, Liao M, Lu H, Zhao Y. A novel machine learning method for cytokine-receptor interaction prediction. \nComb Chem High T Scr. 2016;19(2):144–52.\n 21. Wei L, Bowen Z, Zhiyong C, Gao X, Liao M. Exploring local discriminative information from evolutionary profiles for \ncytokine-receptor interaction prediction. Neurocomputing. 2016;217:37–45.\n 22. Wei L, Long W, Wei L. Mdl-cpi: multi-view deep learning model for compound-protein interaction prediction. Meth-\nods. 2022;204:418–27.\n 23. Nguyen T, Le H, Quinn TP , Nguyen T, Le TD, Venkatesh S. GraphDTA: predicting drug-target binding affinity with \ngraph neural networks. Bioinformatics. 2021;37(8):1140–7.\n 24. Jia J, Zhu F, Ma X, Cao ZW, Li YX, Chen YZ. Mechanisms of drug combinations: interaction and network perspectives. \nNat Rev Drug Discov. 2009;8(2):111–28.\n 25. Bahdanau D, Cho K, Bengio Y. Neural machine translation by jointly learning to align and translate. arXiv preprint \narXiv: 1409. 0473. 2014;.\n 26. Chen W, Chen G, Zhao L, Chen CYC. Predicting drug-target interactions with deep-embedding learning of graphs \nand sequences. J Phys Chem A. 2021;125(25):5633–42.\n 27. Zhao Q, Zhao H, Zheng K, Wang J. HyperAttentionDTI: improving drug-protein interaction prediction by sequence-\nbased deep learning with attention mechanism. Bioinformatics. 2022;38(3):655–62.\n 28. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, et al. Attention is all you need. Adv Neural Inf \nProcess Syst. 2017;30.\n 29. Chen L, Tan X, Wang D, Zhong F, Liu X, Yang T, et al. TransformerCPI: improving compound-protein interaction \nprediction by sequence-based deep learning with self-attention mechanism and label reversal experiments. Bioin-\nformatics. 2020;36(16):4406–14.\n 30. Huang K, Xiao C, Glass LM, Sun J. MolTrans: molecular interaction transformer for drug-target interaction prediction. \nBioinformatics. 2021;37(6):830–6.\n 31. Bai P , Miljković F, John B, Lu H. Interpretable bilinear attention network with domain adaptation improves drug-\ntarget prediction. Nat Mach Intell. 2023;5(2):126–36.\n 32. Shin B, Park S, Kang K, Ho JC. Self-attention based molecule representation for predicting drug-target interaction. In: \nMachine Learning for Healthcare Conference. PMLR; 2019. p. 230–248.\n 33. Zeng Y, Chen X, Peng D, Zhang L, Huang H. Multi-scaled self-attention for drug-target interaction prediction based \non multi-granularity representation. BMC bioinf. 2022;23(1):1–15.\nPage 20 of 20Zeng et al. BMC Bioinformatics          (2024) 25:141 \n 34. Cheng Z, Yan C, Wu FX, Wang J. Drug-target interaction prediction using multi-head self-attention and graph atten-\ntion network. IEEE/ACM Trans Comput Biol Bioinf. 2021;19(4):2208–18.\n 35. Bai P , Miljković F, Ge Y, Greene N, John B, Lu H. Hierarchical clustering split for low-bias evaluation of drug-target \ninteraction prediction. In: 2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM). IEEE; 2021. \np. 641–644.\n 36. Zitnik M, Sosic R, Leskovec J. BioSNAP Datasets: Stanford biomedical network dataset collection. http:// snap. stanf \nord. edu/ bioda ta. 2018;.\n 37. Wishart DS, Knox C, Guo AC, Cheng D, Shrivastava S, Tzur D, et al. DrugBank: a knowledgebase for drugs, drug \nactions and drug targets. Nucleic Acids Res. 2008;36(suppl-1):D901–6.\n 38. Cao DS, Xu QS, Liang YZ. propy: a tool to generate various modes of Chou’s PseAAC. Bioinformatics. \n2013;29(7):960–2.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7237253189086914
    },
    {
      "name": "ENCODE",
      "score": 0.5718774795532227
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5656479001045227
    },
    {
      "name": "Machine learning",
      "score": 0.5474099516868591
    },
    {
      "name": "Transformer",
      "score": 0.5138412117958069
    },
    {
      "name": "Drug target",
      "score": 0.4654885530471802
    },
    {
      "name": "Artificial neural network",
      "score": 0.42089593410491943
    },
    {
      "name": "Biology",
      "score": 0.08944728970527649
    },
    {
      "name": "Pharmacology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I180726961",
      "name": "Shenzhen University",
      "country": "CN"
    }
  ],
  "cited_by": 14
}