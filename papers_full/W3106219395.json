{
  "title": "Identifying Personal Experience Tweets of Medication Effects Using Pre-trained RoBERTa Language Model and Its Updating",
  "url": "https://openalex.org/W3106219395",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2111901195",
      "name": "Minghao Zhu",
      "affiliations": [
        "Donghua University",
        "Purdue University Northwest"
      ]
    },
    {
      "id": "https://openalex.org/A3099568462",
      "name": "Youzhe Song",
      "affiliations": [
        "Purdue University Northwest",
        "Donghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2108734651",
      "name": "Ge Jin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1963527460",
      "name": "Keyuan Jiang",
      "affiliations": [
        "Purdue University Northwest"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2136400720",
    "https://openalex.org/W1684832230",
    "https://openalex.org/W2007628554",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W2405355880",
    "https://openalex.org/W2773944021",
    "https://openalex.org/W2250553926",
    "https://openalex.org/W2011273513",
    "https://openalex.org/W196350606",
    "https://openalex.org/W2517596842",
    "https://openalex.org/W2965791043",
    "https://openalex.org/W2807470877",
    "https://openalex.org/W6607133971",
    "https://openalex.org/W2754000910",
    "https://openalex.org/W2164912194",
    "https://openalex.org/W201361503",
    "https://openalex.org/W2520926162",
    "https://openalex.org/W6691431627",
    "https://openalex.org/W2512937600",
    "https://openalex.org/W2088903821",
    "https://openalex.org/W2897765576",
    "https://openalex.org/W2962848499",
    "https://openalex.org/W2139188905",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W177032395",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2904090509",
    "https://openalex.org/W2963341956"
  ],
  "abstract": "Post-market surveillance, the practice of monitoring the safe use of pharmaceutical drugs is an important part of pharmacovigilance. Being able to collect personal experience related to pharmaceutical product use could help us gain insight into how the human body reacts to different medications. Twitter, a popular social media service, is being considered as an important alternative data source for collecting personal experience information with medications. Identifying personal experience tweets is a challenging classification task in natural language processing. In this study, we utilized three methods based on Facebook's Robustly Optimized BERT Pretraining Approach (RoBERTa) to predict personal experience tweets related to medication use: the first one combines the pre-trained RoBERTa model with a classifier, the second combines the updated pre-trained RoBERTa model using a corpus of unlabeled tweets with a classifier, and the third combines the RoBERTa model that was trained with our unlabeled tweets from scratch with the classifier too. Our results show that all of these approaches outperform the published methods (Word Embedding + LSTM) in classification performance (p < 0.05), and updating the pre-trained language model with tweets related to medications could even improve the performance further.",
  "full_text": "Proceedings of the 11th International Workshop on Health Text Mining and Information Analysis, pages 127–137\nNovember 20, 2020.c⃝2020 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17\n127\n \n \n \nAbstract \nPost-market surveillance, the practice of \nmonitoring the safe use of pharmaceutical \ndrugs is an important part of \npharmacovigilance. Being able to collect \npersonal experience related to \npharmaceutical product use could help us \ngain insight into how the human body \nreacts to different medications. Twitter, a \npopular social media service, is being \nconsidered as an important alternative data \nsource for collecting personal experience \ninformation with medications. Identifying \npersonal experience tweets is a challenging \nclassification task in natural language \nprocessing. In this study, we utilized three \nmethods based on Facebook’s Robustly \nOptimized BERT Pretraining Approach \n(RoBERTa) to predict personal experience \ntweets related to medication use: the first \none combines the pre -trained RoBERTa \nmodel with a classifier, the second \ncombines the updated pre -trained \nRoBERTa model using a corpus of \nunlabeled tweets with a classifier, and the \nthird combines the RoBERTa model that \nwas trained with our unlabeled tweets from \nscratch with the classifier too. Our results \nshow that all of these approaches \noutperform the published methods (Word \nEmbedding + LSTM) in classification \nperformance (p < 0.05), and updating the \npre-trained language model with tweets \nrelated to medications could even improve \nthe performance further. \n1 Introduction \nPersonal experience is an important piece of \ninformation for health- related surveillance \nactivities. Understanding one’s health experience \ncan help gain insight into the status of one’s health, \nchanges of one’s health condition after the \nintervention, or the effects related to any \nmedications one took. \nInvestigating effects related to the use of \npharmaceutical products is an important activity of \npost-market surveillance. First -hand information \nrelated to patients’ medication use most directly \nreflects the effects of the medication, beneficially \nor adversely. In that case, it is necessary to find \nvaluable data sources and construct efficient \nmethods for processing and analyzing this data. \nThe widespread availability of social media has \nmade it possible for people to share their personal \nexperiences freely online. Twitter is one of the \nmost prevalent social media services, and studies \nhave shown that the data from social media such as \nTwitter has b een applied to many health- related \napplications. Examples are as follows: drug \nadverse events ( Bian et al. 2012) , public health \n(Paul et al. 2011; Parker et al. 2013), mental health \n(Coppersmith et al. 2014; Reece et al. 2017), dental \npain (Heaivilin et al. 2011), influenza ( Lee et al. \n2013; Paul et al. 2015 ; Gesualdo et al. 2013; \nAramaki et al. 2011; Byrd et al. 2016; Kagashe et \nal. 2017), breast cancer ( Thackeray et al. 2013), \nand epidemic outbreak and spread detection (Ji et \nal. 2012).  \nPersonal experience is about a person’s \nencounters or observations related to his or her life. \nPersonal experience information related to the use \nof medication is of unique  value for post-market \nsurveillance because it is the first-hand information \nthat reflects the health condition changes due to \nmedication usage. Personal E xperience Tweets \n(PETs) related to medication use are a kind of \nTwitter post expressing one’s personal experience \nand information after the administration of \nmedication. The types of experiences could be \nundesirable feelings caused by medications’ side-\neffects, or benef icial effects that help improve a \nIdentifying Personal Experience Tweets of Medication Effects Using  \nPre-trained RoBERTa Language Model and Its Updating \n \n \n \nMinghao Zhu1,2, Youzhe Song1,2, Ge Jin2, Keyuan Jiang2 \n1Donghua University, Shanghai, China \n2Purdue University Northwest, Hammond, Indiana, U.S.A. \nminghao.zhu0@gmail.com, isidoresongsisidoresongs@gmail.com, jin9@pnw.edu, kjiang@pnw.edu \n \n \n128\n \n \n \nmedication user’s health condition. The collection \nand understanding of these experiences’  \ninformation can help promote the safe use of \nmedications and advance our healthcare practices. \nHere are some examples of PETs related to \nmedication use (the underscored text is for \nmedication effects  and the boldfaced for the \nmedication): \n\"Slow release morphine almost killed me.\" \n\"my mother developed bleeding ulcers  from \nnaproxen and now they swit ched her to celebrex \nisnt that just as bad?\" \n\"Ill check it out - I have a friend on Abilify and \nhes had some personality changes , IE agitation , \nhitting stuff, ect.\" \nThese tweets show that the effects are associated \nwith a person’s experience. In contrast, we define \na tweet not describing a personal experience as a \nnon-PET. The following are some examples: \n\"wish i had some xanax to put me to sleep\" \n\"ativan please help me get some sleep tonight\" \n\"i just took a dose of percocet  with some \nstrippers\" \nThe above non- PETs, albeit mentioning \nmedications or containing effect expressions, do \nnot reflect the personal experience. \nExtracting PETs from various kinds of Twitter \nposts is challenging because the Twitter data is of \nabundant noises, and most of the tweets may be \nirrelevant to personal experience about health \nconditions. In addition, users usually post tweets \nwith informal and causal styles, without following \nthe rules of grammar and/or spelling. Finally, \nTwitter users are creative in coining short text to \ninclude the needed information within the space \nlimit. These unique characteristics make it more \nchallenging to identify PETs accurately. \n2 Related Works \nDistinguishing PETs and non- PETs can be \ntreated as a binary classification task. In the \nconventional machine learning field, algorithms \nrequire a set of manually engineered features \nextracted from the raw text and/or metadata (Jiang \net al., 2016; Wijeratne et al., 2017), usually known \nas feature engineering, and features chosen can \nsignificantly impact the classifier’s performance. \nHowever, extracting/engineering valuable yet \noptimal features from tweets is difficult due to the \nlimitation of human knowledge and understanding \neven for the domain experts. Besides, feature \nengineering extracts features that are typically \nbased on the analysis of statistics regarding  \ninformation gain usually with little or no direct \nconsideration of the semantics. In other words, \nconventional machine learning with feature \nengineering methods may not be optimal for this \ntask. \nEfforts of performance improvement have been \nmade in previous research endeavors in the task of \npredicting personal experience t weets related to \nmedication effects. In one of the earliest efforts, \npersonal pronouns were considered as an important \nfeature (Jiang and Zheng, 2013). Later, Alvaro and \ncolleagues engineered a set of features (Alvaro et \nal., 2015), and their features include Twitter-\nspecific features, n-grams, punctuation elements, \nand topics, but the group decided to discard the \ntopic feature due to the significant efforts required \nand its minimum merit of improving classification \nperformance. A set of 22 engineered features based \nupon both textual content and metadata of tweets \nwas proposed in constructing a corpus of personal \nexperience tweets ( Jiang et al., 2016). \nSubsequently, Calix and colleagues introduced the \nconcept of deep gramulator to include a textual \nfeature that contains expressions in one class but \nnot in the opposite class, to improve the \ndiscriminatory ability of the classification (Calix et \nal., 2017). Advancement in neural embedding, \nwhich demonstrated state -of-art results in many \nclassification tasks on textual data, motivated the \ndevelopment of a new approach of combining \nword embedding (word2vec) and a recurrent \nneural network which demonstrated a significant \nimprovement of classification performance ( p < \n0.05) (Jiang et al., 2018). \nThanks to the development of word embedding \ntechniques and the long short term memory \n(LSTM) neural network, Jiang et al. ( 2019) \nassessed a set of different word embedding \ntechniques: GloVe ( Pennington et al. 2014), \nfastText (Bojanowski et al. 2016) and word2vec \n(Mikolov et al. 2013) to build vector space models \n(VSM) to represent the semantics of tweets by \nlearning from a corpus of 22 million unlabeled \ntweets. The vector representations of tweets were \nfed into an LSTM neural network for classification. \nAll of these methods achieved better performance \nin classification measures than the previous \nmethods with 22 human-engineered features using \nconventional classification algorithms (Jiang et al. \n2016). \n129\n \n \n \nUnlike the word embedding + LSTM method, \nwhich need to learn the  VSM first and then train \nthe LSTM network from scratch for classification, \nGoogle introduced a fine-tuning based approach by \nproposing the Bidirectional Encode r \nRepresentations from Transformers (BERT) model \n(Devlin et al. 2018), which achieved record -\nbreaking results in 18 downstream NLP tasks. \nBesides, Google’s new method relies on contextual \ninformation rather than term co-occurrences. After \nthat, Facebook made some optimization based on \nBERT and released a Robustly Optimized BERT \nPretraining Approach (RoBERTa) model (Liu et al. \n2019) which generated even better performance \nthan BERT in downstream tasks. One important \nand useful aspect of both approaches is that the pre-\ntrained models can be updated with new data, \nwithout the need to generate a new model from \nscratch with the added data, which generally \nrequires a significant amount of computation \nresources. \nIn this study, we set the performance of the word \nembedding + LSTM neural network method as the \nbaseline and investigated the performance \nimprovements of PETs prediction with the pre -\ntrained RoBERTa language model. We also studied \na procedure of updating the pre-trained RoBERTa \nlanguage model and training the RoBERTa from \nscratch with the medication-related tweets and \nanalyzing the impact on the performance change. \n3 Method \nIn this work, we introduced three ways to identify \npersonal experience tweets about medication \neffects by using RoBERTa language model : (1) \nPretrained RoBERTa - adding a classifier to the \nstandard pre-trained RoBERTa model and fine -\ntuning the model for cla ssification; (2) Updated \nRoBERTa - updating the pre -trained RoBERTa \nlanguage model with our dataset first, then adding \na classifier to RoBERTa and fine-tuning the model \nfor classification ; and  (3) Twitter RoBERTa  - \ntraining the RoBERTa language model with our \ncorpus of unannotated tweets from scratch, then \nadding a classifier for classification. Finally, 10-\nfold cross-validation was performed to gather the \nperformance data, and statistical analysis was \nperformed to determine if the differences in \nperformance among different methods were due to \nthe chance. \nThe pipelines of data processing and analysis is \nillustrated in Figure 1. Our process started with \ngathering Twitter data and performing text \nencoding after preprocessing. Afterwards, the \nencoded texts were used with the RoBERTa model \nand the classifier for our methods. The left pipeline \nis for the Pretrained RoBERTa approach, the \nmiddle one for Updated RoBERTa, and the right  \none for Twitter RoBERTa. \n3.1 Text Encoding \nByte-Pair Encoding (BPE) (Sennrich et al. 2015) \nand Attention Mask were applied to encode raw \ntext. BPE is a sub-word level encoding method that \nuses bytes as the base sub -word units. In the \nprocess of tokenization , tokens like acronyms, \nabbreviations or spelling mistakes which are not in \nthe vocabulary are  split into known sub- word \ntokens, Compared to the word- level encoding \nmethod, it is flexible enough for tokenized words \nwith special forms and adaptable for most of \nEnglish documents, and also it could efficiently \navoid most of the unknown tokens in the input text.  \nA sub-word vocabulary with 50K unique tokens \nwas built before pre -training, which was tested \nwith our dataset to ensure that  our data could be \ncompletely covered by this vocabulary and t weet \ntext was tokenized properly without leaving any \nunknown tokens. In that case, we reused this sub-\nword vocabulary to encode our data and each of the \n \nFigure 1. The pipelines of data processing.  \n \n\n130\n \n \n \ntweets was converted into a sequence of indices of \ntokens in the vocabulary. \nAfter encoding, each tweet started with a special \n<s> token and ended with </s>. To achieve the \nfixed length of a sequence, we set the max token \nlength to 64, and a special <pad> token was \nintroduced to pad sequences to the max length. We \nensured that this value of max token length could \nfit almost all of the tweets: only 0.003% of them \nwere longer than 64 tokens . Also, an Attention \nMask was applied to all of the input data to avoid \nperforming attention on padding tokens. For each \nsentence, 0 is for padding tokens that should be \nmasked, and 1 is for others that are not masked . \nFigure 2 shows an example of text encoding. \n3.2 Pre-training \nPre-training the language model in a large corpus \ncould help the model learn a series of general \ncommon properties of the language, and it is \nexpected to be used in some of the downstream \ntarget tasks with a small dataset where it could \nperform better. The pre-training model we used is \nbased upon the model of RoBERTa, whose \nstructure is based on Google’s BERT model, with \n12 layers, 768 hidden neurons, 12 self -attention \nheads and a total of 110M parameters. The \nRoBERTa model was released by Facebook AI   \n(Liu et al. 2019), pre-trained with masked language \nmodel (MLM) task: 15% of tokens were randomly \nand dynamically selected for replacement; 80% of \nthem were replaced by a special token <mask>; \n10% were kept unchanged; the rest of 10% of the \ntokens were replaced by a random token in \nvocabulary. The pre -training procedure was \nperformed on a total of over 160GB uncompressed \ntexts for 500K steps with an 8K batch size.  \n3.3 Language Model (LM) Updating \nAlthough the pre -trained model extracts the \ngeneral features of linguistic expression in a large \ncorpus, the dataset of our task could be in a \ndifferent distribution. To make the pre -trained \nmodel adapt to our task, we updated the pre-trained \nRoBERTa model with our corpus of 10M \nunlabeled tweets before training the classifier. In \nthis updating procedure, we implemented the same \nmasking strategy as that of the masked LM task in \nthe pre -training procedure, d escribed previously, \nwith a  set of new ly designated hyperparameters \n(training steps: 53K /106K/160K batch size : 64, \noptimizer: Adam, learning rate 2×10\n-5) \n3.4 Training RoBERTa from Scratch \nAnother way to let the model learn the property and \ndistribution of a new language environment is to \ntrain a new model from scratch with the new \ndataset. As for our task, it is also a selectable \napproach. To determine whether training the \nRoBERTa model with our corpus of tweets could \nperform better than Facebook’s pre-trained one and \nto use the updating approach in predicting personal \nexperience tweets, a new Twitter RoBERTa model \nwas constructed with the same corpus of tweets as \nthe updating procedure use. Due to the hardware \n \nFigure 2. Example of text encoding \n \n \nFigure 3. Setup of Updated RoBERTa and Twitter \nRoBERTa \n\n131\n \n \n \ndifference between Facebook’s and ours, a set of \ndifferent hyperparameters were used to train it \nfrom scratch. (training steps: 53K /106K/160K,  \noptimizer: Adam, learning rate: 5×10-5, batch size: \n64) Figure 3 illustrates  the overview of the \nprocedure of LM updating and training the Twitter \nRoBERTa from scratch. \n3.5 Classifier Fine-tuning \nA classifier with a simple feedforward neural \nnetwork was constructed by following RoBERTa’s \noriginal design, which is adapted for RoBERTa’s \nbase concepts and structure. This is also officially \nrecommended to use for the most of downstream \nclassification tasks by Facebook AI. The classifier \nis made up of one hidden layer containing 768 units \nand a tanh activation function followed by a \nsigmoid output. Between the RoBERTa model and \nits classifier, the first d imension of RoBERTa’s \noutput tensor (also annotated as the beginning of \nsentence token <s>) was extracted and treated as \nthe input of the classifier. A dropout with a rate of \n0.1 was added before the hidden layer to prevent \noverfitting. We utilized this classifier structure for \nall of our three methods and fine-tuned the whole \nmodel with officially recommended \nhyperparameters (epochs: 2, batch size : 32, \noptimizer: Adam, learning rate: 1×10-5). \n3.6 Baselines \nJiang and colleagues (2018; 2019) investigated and \npublished a set of outstanding methods based on \nWord Embedding algorithms and the LSTM neural \nnetwork, which outperformed those using human-\nengineered features with conventional \nclassification models. Using a large corpus of \nunlabeled tweets, their approach generated a vector \nspace model (VSM) to encode  the words and \ntrained and tested an LSTM-based classifier with a \nsmaller set of annotated tweets. In our approach, \nwe built the same (baseline) models by following \nthe published structures and procedures : a VSM \nbuilt by word2vec, GloVe and fastText algorithms \nwith 128 dimensions and an LSTM layer with 128 \nhidden units and L2 regularizer followed by a fully \nconnected layer with the sigmoid output. The \nmodels were trained by an Adam optimizer with a \nlearning rate of 2×10\n-4 and a batch size of 32 for 5 \nepochs.  \n3.7 Data \nTwo corpora of Twitter data were used in our work. \nA total o f 22 million raw tweets were  collected \nusing Twitter Streaming APIs from August 25, \n2015, to December 7, 2016, and another set of 52 \nmillion raw tweets w as collected from 2006 to \n2017 using a home-made crawler based upon the \npermission policy specified in Twitter’s robots.txt \nfile. Both sets were gathered by searching tweets \nwith the keywords of a set of brand and generic \nmedication names. These two corpora  were \nmerged and filtered. After dropping duplicates and \neliminating non-English twitters, a corpus  of 10 \nmillion tweets was collected. To study the changes \nin classification performance, the same corpus of \n12,331 annotated tweets, published on Github by \n(Jiang, et al., 2018), was utilized. \nFor this task, the corpus of 10 million cleaned \ntweets were selected for training the T witter \nRoBERTa model from scratch as well as updating \nthe LM –  note that the both LM updating and \ntraining from scratch procedures did not use any \nlabels of the annotati on and the annotated 12K \ntweets were excluded from the 10 million tweets. \nInterestingly, the baseline methods used the same \n10 million raw tweets to build vector space models \nof neural embedding. Likewise, the baseline \nclassifiers were also trained and tested with 12,331 \nlabeled tweets. Table 1 lists the composition of \nannotated tweets. \n3.8 Statistical Analysis \nTo determine if any differences in the results \namong different methods could be due to chance, \nwe conducted statistical analyses on the results \nbetween our methods and baseline methods. In our \nhypothesis testing, the null hypothesis was that the \ndifference between a pair of method does not exist \n(null hypothesis) while the data remain the same. \nTo do so, we partitioned data into the same subsets \nfor all t he methods in cross -validation – that is, \neach fold has the same set of tweets for different \nmethods. This treatment facilitated us to use the \npaired t-test on the performance measures of each \npair of the method. We set the p-value threshold to \n0.05, meaning that any p-value less than 0.05 (p < \n0.05) indicates that the difference does exist and it \nis not due to chance. \n \n PETs Non-PETs Total \nTweet Count 2,962 9,369 12,331 \nTable 1. Composition of annotated tweets. \n \n132\n \n \n \n      \nMethod Accuracy Precision (PET) Recall (PET) F1 (PET) AUC/ROC (PET) \nUpdated \nRoBERTa(160K)1 0.873 0.732 0.760 0.745 0.933 \nUpdated \nRoBERTa(106K)1 0.879 0.751 0.746 0.748 0.934 \nUpdated \nRoBERTa(53K)1 0.877 0.734 0.775 0.754 0.932 \nRoBERTa Original2 0.866 0.712 0.759 0.735 0.925 \nTwitter \nRoBERTa(160K)3 0.859 0.690 0.762 0.724 0.921 \nTwitter \nRoBERTa(106K)3 0.859 0.706 0.730 0.718 0.917 \nTwitter \nRoBERTa(53K)3 0.855 0.699 0.709 0.704 0.911 \nword2vec-LSTM 0.844 0.693 0.661 0.677 0.898 \nGloVe-LSTM 0.839 0.683 0.651 0.667 0.892 \nfastText-LSTM 0.842 0.681 0.663 0.672 0.891 \n1 Updated RoBERTa in 160k, 106k, 53k steps \n2 Facebook’s pre-trained RoBERTa \n3 Twitter RoBERTa in 160k, 106k, 53k steps \nTable 2. Classification performance. The last 3 rows are for baseline methods.\n4 Results \nTo compare the performance differences between \nour methods and baseline methods, 10-fold cross-\nvalidation was conducted for each method and the \nmean value of each classification measure was \ncollected. Table 2 shows the measures of  the \nclassification performance between our methods \nand baselines’ (the highest values are in boldface).  \nTable 3 (in appendix) lists the statistical analysis \nresults of each performance measure in cross-\nvalidation between our methods and baseline \nmethods.\n \n5 Discussions \nAccording to the results in Table 2, we can see that \ncompared to baseline methods, the approaches of \nRoBERTa mode l with or without updating \nachieved better performance in all measures, and \nthe Twitter RoBERTa model trained with our data \nalso performed better except in precision, and such \ndifferences were confirmed to exist statistically by \nthe p-values in Table 3 (p < 0.05). In general, we \ncan consider that the RoBERTa models performed \nbetter than Word Embedding + LSTM method in \nthis task. \nA noticeable improvement between pre-trained \nand updated RoBERTa models and baseline \nmethods is the precision and recall, whereas the \nprecision of Twitter RoBERTa model remained \nrelatively unchanged at the same time. The recall is \nthe sensitivity of how many true instances are \npredicted correctly and precision rates how many \npositive predictions are correct. A h igher recall \ncould he lp the model discover more potential \npositive instances and higher precision means \nmore true positives (TP) and less false positives  \n(FP) in the prediction. In other words, RoBERTa \nmodels can improve the sensitivity and identify \nPETs more precisely, result ing in more true \npositives in the predicted PET class. \nAnother remarkable measure could be the \nROC/AUC score, which was also improved \nsignificantly as shown by the curves in Figure 4. \nROC (Receiver Operating Characteristic) is a \ncurve plotting true positi ve rate (TPR, or \nsensitivity) in the y-axis and false positive  rate \n(FPR, or 1 -specificity) in  the x-axis, and is \ncommonly used to show how well the model can \ndistinguish two different objects. The area under \n \nFigure 4. The ROC curves of our methods. \n\n133\n \n \n \nthe curve (AUC) of ROC is used to quantify the \nscore of ROC. The results in Table 3 show that the \nlowest p-value between our methods and baseline \nmethods is ROC, which may imply that ROC was \nimproved most significantly among all \nperformance measures. That is to say, our methods \ncan be good choices with improved ROCs in this \ntask and they are much more robust in \ndistinguishing PETs and non-PETs.  \nOur methods also achieved  a modest \nimprovement in accuracy, but it could not be \ninterpreted as that better accuracy leads to better \nperformance. B ecause our dataset is imbalanced \n(PETs: non-PETs = 1: 3.16, as shown in Table 1) \nand accuracy is based upon the prediction of both \npositive and negative classes, higher accuracy \ncould be attributed to the imba lance. Thus, \naccuracy is not an important measure that should \nbe of concern. \nThe results also show that performing LM \nupdating before classifier fine-tuning could yield \nmore improvement in accuracy, precision, F1, and \nAUC. Nevertheless, the p-values indicate that they \nare not significant if updating the LM for more \nsteps. But as for the Twitter RoBERTa model, \nwhich was trained from scratch, the steps of \ntraining affected performances in some measures \nwhich were supported by our statistical analysis. \nThis outcome suggests that a larger number of \nsteps are needed for performance improvement \nwhen training from scratch, and small steps are  \nenough for LM updating to achieve better \nperformance than the original RoBERTa model.  \nThe possible reason for the improvement of \nthese RoBERTa- based methods over baseline \napproaches could be attributed to the level  of \nfeatures. As is known, the feature s extracted by \nVSM such as word2vec, which is based upon \nword-level and co-occurrence. But RoBERTa, \nwhich extracts contextual -level features, maybe \nmore powerful in processing tweet-like text which \nis poisoned by misspelling and incorrect \ngrammars. The possible explanation for the \nperformance difference between Updated \nRoBERTa and Twitter RoBERTa can be the slow \nlearning process. The updating process is based on \nthe pre-trained RoBERTa model, which is already \npre-trained with a very large dataset by Facebook. \nIt may be easier to adapt itself to our dataset, and \nthe larger number of updating steps did less to help \nimprove performance. But for Twitter RoBERTa, \nsince it was trained from scratch and only 15% of \ntokens were randomly masked, the model could \nonly learn a small part of sentences for each step. \nTherefore, it may take more time to learn the data \ndistribution, and the larger number of training steps \nis recommended. \n6 Conclusion \nIn this study, we investigated different ways to use \nFacebook’s RoBE RTa model to improve \nperformance in predicting personal experience \ntweets on medication use. Our results \ndemonstrated that using the fine-tuning method on \nthe pre-trained RoBERTa model achieved better \nclassification performance than previous Word \nEmbedding + LSTM methods, and the original pre-\ntrained RoBERTa could perform better than \ntraining a new RoBERTa model from scratch. \nMore importantly , updating the pre -trained \nRoBERTa language model with our data could \nyield better performance. The 10- fold cross-\nvalidation was used to  test statistically  the \nperformance differences between our approaches \nand baseline methods. The results confirmed that \nthe improvement does exist with statist ical \nsignificance (p < 0.05) . This suggests the pre -\ntrained RoBERTa model and LM updating method \nare better choices for this task and significantly \nboost the capability to identify personal experience \ntweets. It is conceivable that our method could \napply to other classification tasks using Twitter \ndata related to health issues. \n7 Acknowledgement \nAuthors wish to thank College of Technology at \nPurdue University Northwest for providing \nfunding to support this work. \n8 References \nAlvaro, N., Conway, M., Doan, S., Lofi, C., \nOverington, J. and Collier, N., 2015. \nCrowdsourcing Twitter annotations to identify first-\nhand experiences of prescription drug use. Journal \nof biomedical informatics, 58, pp.280-287.  \nAramaki, E., Maskawa, S. and Morita, M., 2011, July. \nTwitter catches the flu: detecting influenza \nepidemics using Twitter. In Proceedings of the \nconference on empirical methods in natural \nlanguage processing (pp. 1568- 1576). Association \nfor Computational Linguistics. \nBian, J., Topaloglu, U. and Yu, F., 2012, October. \nTowards large-scale twitter mining for drug -related \n134\n \n \n \nadverse events. In Proc eedings of the 2012 \ninternational workshop on Smart health and \nwellbeing (pp. 25-32). ACM.  \nBojanowski, P., Grave, E., Joulin, A. and Mikolov, T., \n2017. Enriching word vectors with subword \ninformation. Transactions of the Association for \nComputational Linguistics, 5, pp.135-146. \nByrd, K., Mansurov, A. and Baysal, O., 2016, May. \nMining Twitter data for influenza detection and \nsurveillance. In Proceedings of the International \nWorkshop on Software Engineering in Healthcare \nSystems (pp. 43-49). ACM. \nCalix, R.A., Gupta, R., Gupta, M. and Jiang, K., 2017, \nNovember. Deep gramulator: Improving precision \nin the classification of personal health -experience \ntweets with deep learning. In 2017 IEEE \nInternational Conference on Bioinformatics and \nBiomedicine (BIBM) (pp. 1154-1159). IEEE.  \nCoppersmith, G., Dredze, M. and Harman, C., 2014, \nJune. Quantifying mental health signals in Twitter. \nIn Proceedings of the workshop on computational \nlinguistics and clinical psychology: From linguistic \nsignal to clinical reality (pp. 51-60). \nDevlin, J., Chang, M.W., Lee, K. and Toutanova, K., \n2018. Bert: Pre -training of deep bidirectional \ntransformers for language understanding. arXiv \npreprint arXiv:1810.04805. \nGesualdo, F., Stilo, G., Gonfiantini, M.V ., Pandolfi, E., \nVe-lardi, P. and Tozz i, A.E., 2013. Influenza -like \nillness surveil-lance on Twitter through automated \nlearning of naïve language. PLoS One, 8(12), \np.e82489. \nHeaivilin, N., Gerbert, B., Page, J.E. and Gibbs, J.L., \n2011. Public health surveillance of dental pain via \nTwitter. Journal of dental research, 90(9), pp.1047-\n1051. \nJi, X., Chun, S.A. and Geller, J., 2012, April. Epidemic \noutbreak and spread detection system based on \ntwitter data. In International Conference on Health \nInformation Science (pp. 152- 163). Springer, \nBerlin, Heidelberg. \nJiang, K., Calix, R. and Gupta, M., 2016, August. \nConstruction of a personal experience tweet corpus \nfor health surveillance. In Proceedings of the 15th \nworkshop on biomedical natural language \nprocessing (pp. 128-135). \nJiang, K., Feng, S., Calix,  R.A. and Bernard, G.R., \n2019, January. Assessment of word embedding \ntechniques for identification of personal experience \ntweets pertaining to medication uses. In \nInternational Workshop on Health Intelligence (pp. \n45-55). Springer, Cham. \nJiang, K., Feng, S ., Song, Q., Calix, R.A., Gupta, M. \nand Bernard, G.R., 2018. Identifying tweets of \npersonal health experience through word \nembedding and LSTM neural network. BMC \nbioinformatics, 19(8), p.210. \nJiang, K. and Zheng, Y ., 2013, December. Mining \ntwitter data for  potential drug effects. In \nInternational conference on advanced data mining \nand applications (pp. 434- 443). Springer, Berlin, \nHeidelberg.  \nKagashe, I., Yan, Z. and Suheryani, I., 2017. \nEnhancing seasonal influenza surveillance: topic \nanalysis of widely us ed medicinal drugs using \nTwitter data. Journal of medical Internet research, \n19(9), p.e315. \nLee, K., Agrawal, A. and Choudhary, A., 2013, August. \nReal-time disease surveillance using twitter data: \ndemonstration on flu and cancer. In Proceedings of \nthe 19th ACM SIGKDD international conference on \nKnowledge discovery and data mining (pp. 1474 -\n1477). ACM. \nLiu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., \nLevy, O., Lewis, M., Zettlemoyer, L. and Stoyanov, \nV ., 2019. Roberta: A robustly optimized bert \npretraining approach. arXiv preprint \narXiv:1907.11692. \nMikolov, T., Chen, K., Corrado, G. and Dean, J., 2013. \nEfficient estimation of word representations in \nvector space. arXiv preprint arXiv:1301.3781. \nParker, J., Wei, Y ., Yates, A., Frieder, O. and Goharian, \nN., 2013, August. A framework for detecting public \nhealth trends with twitter. In Proceedings of the \n2013 IEEE/ACM International Conference on \nAdvances in Social Networks Analysis and Mining \n(pp. 556-563). ACM. \nPaul, M.J. and Dredze, M., 2011, July. You are what \nyou tweet: Analyzing twitter for public health. In \nFifth International AAAI Conference on Weblogs \nand Social Media. \nPaul, M.J., Dredze, M., Broniatowski, D.A. and \nGenerous, N., 2015, April. Worldwide influenza \nsurveillance through twitter. In Workshops at the \nTwenty-Ninth AAAI Conference on Artificial \nIntelligence. \nPennington, J., Socher, R. and Manning, C., 2014, \nOctober. Glove: Global vectors for word \nrepresentation. In Proceedings of the 2014 \nconference on empirical methods in natural \nlanguage processing (EMNLP) (pp. 1532-1543). \nReece, A.G., Reagan, A.J., Lix, K.L., Dodds, P.S., \nDanforth, C.M. and Langer, E.J., 2017. Forecasting \nthe onset and course of mental illness with Twitter \ndata. Scientific reports, 7(1), p.13006. \nSennrich, R., Haddow, B. and Birch, A., 2015. Neural \n135\n \n \n \nmachine translation of rare words with subword \nunits. arXiv preprint arXiv:1508.07909. \nThackeray, R., Burton, S.H., Giraud -Carrier, C., \nRollins, S. and Draper, C.R., 2013. Using Twitter \nfor breast cancer  prevention: an analysis of breast \ncancer awareness month. BMC cancer, 13(1), \np.508. \nWijeratne, S., Sheth, A., Bhatt, S., Balasuriya, L., Al -\nOlimat, H. S., Gaur, M., Yazdavar, A. H., \nThirunarayan, K.: Feature Engineering for Twitter -\nbased Applications. Fea ture Engineering for \nMachine Learning and Data Analytics, 35 (2017).  \n \n  \n136\n \n \n \nA Appendices \nMethod Measure RoBERTa \nRoBERTa-\nUpdated(53K) \nRoBERTa-\nUpdated(106K) \nRoBERTa-\nUpdated(160K) \nRoBERTa-\nTwitter(53K) \nRoBERTa-\nTwitter(106K) \nRoBERTa-\nTwitter(160K) \nRoBERTa \nAcc.  5.106×10-2 7.908×10-4 1.036×10-1 2.153×10-2 2.595×10-2 7.047×10-2 \nPrec. 1.793×10-1 3.950×10-3 2.075×10-1 2.344×10-1 3.248×10-1 1.071×10-1 \nRecall 2.170×10-1 1.651×10-1 4.853×10-1 4.586×10-2 1.096×10-1 4.484×10-1 \nF1 8.196×10-4 1.247×10-3 2.306×10-2 5.759×10-3 1.576×10-2 9.804×10-2 \nAUC 1.312×10-4 7.787×10-5 2.650×10-4 2.397×10-5 6.578×10-4 3.026×10-2 \nRoBERTa-\nUpdated(53K) Acc. 5.106×10-2 \n \n3.204×10-1 2.880×10-1 6.821×10-4 4.198×10-3 9.187×10-4 \nPrec. 1.793×10-1 1.774×10-1 4.660×10-1 3.713×10-2 1.694×10-1 1.699×10-2 \nRecall 2.170×10-1 4.363×10-2 2.656×10-1 1.410×10-2 9.040×10-2 3.019×10-1 \nF1 8.196×10-4 1.094×10-1 4.049×10-2 1.948×10-4 1.954×10-6 6.061×10-5 \nAUC 1.312×10-4 8.087×10-2 5.282×10-2 9.169×10-8 2.980×10-7 7.746×10-6 \nRoBERTa- \nUpdated(106K) Acc. 7.908×10-4 3.204×10-1 \n \n7.762×10-2 4.148×10-5 1.415×10-5 6.621×10-5 \nPrec. 3.950×10-3 1.774×10-1 1.682×10-1 5.194×10-3 1.010×10-2 9.835×10-4 \nRecall 1.651×10-1 4.363×10-2 2.882×10-1 1.199×10-1 2.575×10-1 2.289×10-1 \nF1 1.247×10-3 1.094×10-1 1.449×10-1 3.382×10-4 1.216×10-4 2.713×10-4 \nAUC 7.787×10-5 8.087×10-2 3.416×10-1 1.357×10-7 5.519×10-7 8.113×10-6 \nRoBERTa-\nUpdated(160K) Acc. 1.036×10-1 2.880×10-1 7.762×10-2 \n \n3.212×10-3 3.471×10-3 4.285×10-3 \nPrec. 2.075×10-1 4.660×10-1 1.682×10-1 6.953×10-2 1.398×10-1 2.216×10-2 \nRecall 4.853×10-1 2.656×10-1 2.882×10-1 3.748×10-2 1.779×10-1 4.702×10-1 \nF1 2.306×10-2 4.049×10-2 1.449×10-1 1.828×10-4 1.204×10-3 6.101×10-3 \nAUC 2.650×10-4 5.282×10-2 3.416×10-1 8.982×10-9 6.030×10-8 7.682×10-7 \nRoBERTa-\nTwitter(53K) Acc. 2.153×10-2 6.821×10-4 4.148×10-5 3.212×10-3  2.124×10-1 1.908×10-1 \nPrec. 2.344×10-1 3.713×10-2 5.194×10-3 6.953×10-2 3.888×10-1 2.640×10-1 \nRecall 4.586×10-2 1.410×10-2 1.199×10-1 3.748×10-2 2.817×10-1 2.878×10-2 \nF1 5.759×10-3 1.948×10-4 3.382×10-4 1.828×10-4 1.791×10-1 2.271×10-2 \nAUC 2.397×10-5 9.169×10-8 1.357×10-7 8.982×10-9 1.873×10-3 2.129×10-5 \nRoBERTa-\nTwitter(106K) Acc. 2.595×10-2 4.198×10-3 1.415×10-5 3.471×10-3 2.124×10-1  4.596×10-1 \nPrec. 3.248×10-1 1.694×10-1 1.010×10-2 1.398×10-1 3.888×10-1 1.801×10-1 \nRecall 1.096×10-1 9.040×10-2 2.575×10-1 1.779×10-1 2.817×10-1 8.020×10-2 \nF1 1.576×10-2 1.954×10-6 1.216×10-4 1.204×10-3 1.791×10-1 4.668×10-2 \nAUC 6.578×10-4 2.980×10-7 5.519×10-7 6.030×10-8 1.873×10-3 1.630×10-3 \nRoBERTa-\nTwitter(160K) Acc. 7.047×10-2 9.187×10-4 6.621×10-5 4.285×10-3 1.908×10-1 4.596×10-1  \nPrec. 1.071×10-1 1.699×10-2 9.835×10-4 2.216×10-2 2.640×10-1 1.801×10-1 \nRecall 4.484×10-1 3.019×10-1 2.289×10-1 4.702×10-1 2.878×10-2 8.020×10-2 \nF1 9.804×10-2 6.061×10-5 2.713×10-4 6.101×10-3 2.271×10-2 4.668×10-2 \nAUC 3.026×10-2 7.746×10-6 8.113×10-6 7.682×10-7 2.129×10-5 1.630×10-3 \nTable 3a. Statistical analysis results (p values) for RoBERTa models. Values in boldface are less than 0.05. \n  \n137\n \n \n \n \nMethod Measure RoBERTa \nRoBERTa-\nUpdated(53K) \nRoBERTa-\nUpdated(106K) \nRoBERTa-\nUpdated(160K) \nRoBERTa-\nTwitter(53K) \nRoBERTa-\nTwitter(106K) \nRoBERTa-\nTwitter(160K) \nWord2Vec-\nLSTM \nAcc. 1.663×10-3 1.879×10-5 1.919×10-6 1.050×10-4 7.198×10-3 5.770×10-3 4.898×10-3 \nPrec. 1.861×10-1 4.768×10-2 6.659×10-3 9.901×10-2 4.045×10-1 2.818×10-1 4.422×10-1 \nRecall 2.603×10-2 1.062×10-2 3.799×10-2 2.877×10-2 1.864×10-1 5.933×10-2 2.029×10-2 \nF1 1.145×10-2 1.878×10-3 2.958×10-3 3.891×10-3 7.551×10-2 2.553×10-2 1.450×10-2 \nAUC 2.582×10-7 8.251×10-8 5.581×10-8 4.686×10-8 1.479×10-5 1.864×10-5 5.317×10-7 \nGlove-LSTM Acc. 9.613×10-5 8.448×10-5 1.213×10-5 2.637×10-4 1.236×10-2 1.137×10-3 5.019×10-4 \nPrec. 1.005×10-1 2.234×10-2 3.395×10-3 2.055×10-2 2.617×10-1 1.686×10-1 3.822×10-1 \nRecall 1.442×10-2 3.515×10-3 1.768×10-2 1.818×10-3 1.008×10-1 4.343×10-2 1.303×10-2 \nF1 1.155×10-4 1.451×10-5 2.706×10-5 1.673×10-5 4.342×10-3 1.953×10-3 7.256×10-4 \nAUC 7.183×10-9 1.086×10-9 3.358×10-10 1.338×10-10 1.994×10-9 1.326×10-8 2.239×10-9 \nFasttext-LSTM Acc. 9.961×10-5 5.171×10-5 1.029×10-6 2.716×10-4 7.583×10-3 2.108×10-3 7.676×10-3 \nPrec. 3.035×10-2 1.449×10-2 1.133×10-4 2.920×10-2 1.864×10-1 1.425×10-1 3.588×10-1 \nRecall 2.448×10-5 1.183×10-4 4.201×10-5 9.241×10-4 9.009×10-2 1.946×10-2 3.609×10-3 \nF1 8.453×10-8 6.394×10-9 3.063×10-8 9.138×10-8 1.282×10-3 1.064×10-4 5.055×10-6 \nAUC 1.011×10-8 3.002×10-9 1.344×10-8 3.410×10-9 9.257×10-8 2.562×10-7 1.066×10-8 \n \nTable 3b. Statistical analysis results (p values) for baselines. Values in boldface are less than 0.05. ",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6927326917648315
    },
    {
      "name": "Natural language processing",
      "score": 0.5989684462547302
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4871368110179901
    },
    {
      "name": "Language model",
      "score": 0.44587400555610657
    }
  ]
}