{
  "title": "Incrementally Learning the Hierarchical Softmax Function for Neural Language Models",
  "url": "https://openalex.org/W2605242105",
  "year": 2017,
  "authors": [
    {
      "id": "https://openalex.org/A2009379665",
      "name": "Hao Peng",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A2116050490",
      "name": "Jian-Xin Li",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A2099747503",
      "name": "Yangqiu Song",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2277578498",
      "name": "Yaopeng Liu",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A2009379665",
      "name": "Hao Peng",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A2116050490",
      "name": "Jian-Xin Li",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A2099747503",
      "name": "Yangqiu Song",
      "affiliations": [
        "University of Hong Kong",
        "Hong Kong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2277578498",
      "name": "Yaopeng Liu",
      "affiliations": [
        "Beihang University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6680532216",
    "https://openalex.org/W2027979924",
    "https://openalex.org/W6683738474",
    "https://openalex.org/W342285082",
    "https://openalex.org/W2250741688",
    "https://openalex.org/W2138204974",
    "https://openalex.org/W1826818071",
    "https://openalex.org/W6648424669",
    "https://openalex.org/W2076094076",
    "https://openalex.org/W2125031621",
    "https://openalex.org/W1615991656",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W2103763702",
    "https://openalex.org/W2131462252",
    "https://openalex.org/W36903255",
    "https://openalex.org/W6877195020",
    "https://openalex.org/W1515441714",
    "https://openalex.org/W2158139315",
    "https://openalex.org/W2336505456",
    "https://openalex.org/W2950133940",
    "https://openalex.org/W2998704965",
    "https://openalex.org/W2060108852",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W2962935015",
    "https://openalex.org/W2158899491",
    "https://openalex.org/W2895467231",
    "https://openalex.org/W2962795068",
    "https://openalex.org/W2952230511"
  ],
  "abstract": "Neural network language models (NNLMs) have attracted a lot of attention recently. In this paper, we present a training method that can incrementally train the hierarchical softmax function for NNMLs. We split the cost function to model old and update corpora separately, and factorize the objective function for the hierarchical softmax. Then we provide a new stochastic gradient based method to update all the word vectors and parameters, by comparing the old tree generated based on the old corpus and the new tree generated based on the combined (old and update) corpus. Theoretical analysis shows that the mean square error of the parameter vectors can be bounded by a function of the number of changed words related to the parameter node. Experimental results show that incremental training can save a lot of time. The smaller the update corpus is, the faster the update training process is, where an up to 30 times speedup has been achieved. We also use both word similarity/relatedness tasks and dependency parsing task as our benchmarks to evaluate the correctness of the updated word vectors.",
  "full_text": "Incrementally Learning the Hierarchical\nSoftmax Function for Neural Language Models\nHao Peng,† Jianxin Li,† Yangqiu Song,‡ Yaopeng Liu†\n†Department of Computer Science & Engineering, Beihang University, Beijing 100191, China\n‡Department of Computer Science & Engineering, Hong Kong University of Science and Technology, Hong Kong\n{penghao,ljx,liuyp}@act.buaa.edu.cn yqsong@cse.ust.hk\nAbstract\nNeural network language models (NNLMs) have attracted a\nlot of attention recently. In this paper, we present a training\nmethod that can incrementally train the hierarchical softmax\nfunction for NNMLs. We split the cost function to model\nold and update corpora separately, and factorize the objec-\ntive function for the hierarchical softmax. Then we provide a\nnew stochastic gradient based method to update all the word\nvectors and parameters, by comparing the old tree generated\nbased on the old corpus and the new tree generated based\non the combined (old and update) corpus. Theoretical analy-\nsis shows that the mean square error of the parameter vectors\ncan be bounded by a function of the number of changed words\nrelated to the parameter node. Experimental results show that\nincremental training can save a lot of time. The smaller the\nupdate corpus is, the faster the update training process is,\nwhere an up to 30 times speedup has been achieved. We also\nuse both word similarity/relatedness tasks and dependency\nparsing task as our benchmarks to evaluate the correctness\nof the updated word vectors.\nNeural network language models (NNLMs) (Bengio et\nal. 2003; Morin and Bengio 2005; Mnih and Hinton 2008;\nTurian, Ratinov, and Bengio 2010; Collobert et al. 2011;\nMikolov et al. 2013a; 2013b; Levy and Goldberg 2014;\nLevy, Goldberg, and Dagan 2015) have attracted a lot of\nattention recently given their compact representation form\nand generalization property compared to the traditional lex-\nical representations. It has been applied to many natural lan-\nguage processing tasks such as word similarity/relatedness\nand word analogy (Mikolov et al. 2013a; 2013b), work-\ning as features for part-of-speech tagging, chunking, named\nentity recognition, etc. (Turian, Ratinov, and Bengio 2010;\nCollobert et al. 2011). However, the above NNLMs only\nconsider the static training corpus. There are a lot of appli-\ncations, such as news and tweets text processing, requiring\nincremental update of the word vectors given the fact that\nthe working domains are fast evolving. When the new cor-\npus is relatively smaller than the old corpus, it will be much\nless efﬁcient to retrain the word vectors with the combined\ncorpus.\nIn this paper, we consider the problem of training the\nNNLMs given new corpora incrementally. In particular,\nCopyright c⃝ 2017, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nwe adopt the popular word2vec tool due to its simplicity\nand time efﬁciency, and comparable performance to other\nNNLMs (Mikolov et al. 2013a; 2013b; Levy, Goldberg, and\nDagan 2015). To speedup the process of indexing and query-\ning, word2vec employed two techniques called hierarchi-\ncal softmax and negative sampling (Mikolov et al. 2013a;\n2013b). Hierarchical softmax was ﬁrst proposed by Mnih\nand Hinton (Mnih and Hinton 2008) where a hierarchical\ntree is constructed to index all the words in a corpus as\nleaves, while negative sampling is developed based on noise\ncontrastive estimation (Gutmann and Hyvärinen 2012), and\nrandomly samples the words not in the context to distin-\nguish the observed data from the artiﬁcially generated ran-\ndom noise. It is empirically shown that hierarchical soft-\nmax performs better for infrequent words while negative\nsampling performs better for frequent words (Mikolov et\nal. 2013b). The reason is that hierarchical softmax builds a\ntree over the whole vocabulary, and the leaf nodes represent-\ning rare words will inevitably inherit their ancestors’ vector\nrepresentations in the tree, which can be affected by other\nfrequent words in the corpus. Thus, we choose hierarchical\nsoftmax due to its good performance on rare words, which\ncan beneﬁt the further incremental training for new corpus.\nWhen applying hierarchical softmax to NNLMs, there is\na preprocessing step to build a hierarchical tree of words.\nWhen we incrementally incorporate more data, the hierar-\nchical tree should be changed to reﬂect the change of data.\nFor example, in word2vec, it uses the Huffman coding to\nconstruct the tree over the vocabulary because Huffman tree\nuses shorter codes for more frequent words with less vis-\nits to the leaves (Gutmann and Hyvärinen 2012), and re-\nsults in faster training process. When frequencies of words\nchange, Huffman tree will be changed. To handle this prob-\nlem, we retain the paths from root to leaves of the old tree,\nand perform preﬁx matching between the old tree and the\nnew tree. When updating the vectors for the matched ances-\ntors of a leaf, we follow the original CBOW (Continuous\nBag-of-Words) or Skip-gram algorithms in word2vec based\non stochastic gradient ascent of log-likelihood. When up-\ndating the vectors for the different ancestors, we modify the\nold tree with stochastic gradient descent, while update the\nnew tree with stochastic gradient ascent. In this way, we only\nmodify all the nodes that need to be updated while retaining\nall the other nodes as the same as the ones trained based on\nProceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)\n3267\nthe old corpus. Since the update process is independent for\nall the internal nodes, we also develop a parallel algorithm\nsimilar to word2vec, which makes our algorithm efﬁcient\nwhen having more CPUs.\nIn the experiments, we show that using this updating pro-\ncedure, we can get almost the same CBOW and Skip-gram\nmodels as fully re-trained ones. We check both individual\nvector’s mean square error (MSE) and down-stream applica-\ntions, word similarity/relatedness and dependency parsing,\nto prove the correctness of our model. In addition, we also\nprovide a bound that can characterize our algorithm’s differ-\nence from fully re-trained models.\nBackground\nIn this section, we introduce the background of CBOW and\nSkip-gram models based on the hierarchical softmax func-\ntion. Suppose the tree has been built given a training corpus\nW, where all the unique words inW will be the leaves.\nThe CBOW Model\nIn CBOW model, given a sequence of training words\nw\n1,w2,...,w T in W, the training objective is to maximize\nthe average log-likelihood function∑\nw∈W logP(w|C(w)),\nwhere w is a word and also refers a leaf node determined\nby a path from root in Huffman tree. Moreover, we denote\nX\nw = ∑\n−c≤j≤c,j̸=0wj as the sum of the context vectors,\nand 2c is the size of training context centered atw. Let Lw\nbe the length of the path, and pw\ni be the i-th node on the\npath from the root to w, and dw\ni\nbe Huffman code of i-th\nnode in path, where dw\ni\n∈{ 0,1} and i ∈{ 2,...,L w}.I n\naddition, we denote θw\ni as the vector representation of the\ninternal node pw\ni and θw\nLw is the vector representation ofw.\nThen the log-likelihood can be re-written as:\nJCBOW =\n∑\nw∈W\nlogP(w|C(w)) =\n∑\nw∈W\nLw\n∑\ni=2\nℓ(w,i),\n(1)\nwhere we denoteℓ(w,i)=( 1−dw\ni\n)·log[σ(XT\nwθw\ni−1)]+dw\ni\n·\nlog[1 − σ(XT\nwθw\ni−1)], and σ(x)=1 /(1 + exp(−x)). This\nfunction can be understood as using the current context to\nclassify a word following the path from the root to the leaf\nof the word.\nUsing stochastic gradient ascent, the parameter vectors\nθ\nw\ni−1 and word vectors in the context can be updated as fol-\nlows:\nθw\ni−1 := θw\ni−1 +η[1−dw\ni −σ(XT\nwθw\ni−1)]Xw\nand v(˜w): =v(˜w)+η\nLw\n∑\ni=2\n∂ℓ(w,i)\n∂Xw\n,\n(2)\nwhere ˜w ∈C (w), v(˜w) is the vector representation of the\ncontext word, andη is a degenerative learning rate.\nThe Skip-gram Model\nSkip-gram model uses the current word to predict the\ncontext. The training objective of the Skip-gram model\nis to maximize the average log-likelihood function∑\nw∈W logP(C(w)|w)= ∑\nw∈W\n∑\nu∈C(w) logP(u|w).\nHere we have P(u|w)= ∏Lu\nj=2 P(du\nj |v(w),θu\nj−1), where\nLu is the length of the path from root to the leaf node\nrepresenting word u, and θu\ni is the vector representation\nof the i-th node in the path. The log-likelihood is fur-\nther formulated as: JSG = ∑\nw∈W logP(C(w)|w)=∑\nw∈W\n∑\nu∈C(w)\n∑Lu\nj=2 ℓ(w,u,j ), where we have\nℓ(w,u,j )=( 1 − du\nj\n) · log[σ(v(w)Tθu\nj−1)] + du\nj\n·\nlog[1 − σ(v(u)Tθu\nj−1)], and again v(w) and v(u) are the\nvector representations of word w and word u respectively.\nThis shows that w’s context word u is predicted following\nthe path on the tree from root to leaf given the vector\nrepresentation of w: v(w).\nApplying stochastic gradient ascent, parameter vector\nθu\nj−1 and word vector are iteratively updated as: θu\nj−1 :=\nθu\nj−1+η[1−du\nj −σ(v(w)Tθu\nj−1)]v(w)and v(˜w): =v(˜w)+\nη∑\nu∈C(w)\n∑Lu\nj=2\n∂ℓ(w,u,j)\n∂v(w) , where ˜w ∈C (w), and η is the\ndegenerative learning rate.\nLearning Rate\nThe learning rate η is an important parameter for stochas-\ntic gradient iteration (Hegde, Indyk, and Schmidt 2015). In\nCBOW and Skip-gram models, an adaptive and segmented\nrate is set to be:η = η\n0(1− κ\nφ+1)where η0 is an initial value,\nφ is the number of tokens in corpus, andκ is the number of\nalready trained words. The learning rate is governed by an-\nother parameterρ, which controls the rate to decreaseη after\ncertain number of iterations, e.g., updatingη after seeing ev-\nery 10,000 words. In word2vec, a minimum value η\nmin is\nalso set to enforce the update vectors based on the gradients.\nIncremental Training\nWe see from the above section that learning the word vec-\ntors involves not only the word vectors themselves, but also\nthe internal nodes’ vectors, which we have called parame-\nters θ\nw\ni . When we see a new corpus in addition to the old\ncorpus, we should re-build the hierarchical tree, e.g., the\nHuffman tree in our work, based on the combined corpus.\nHuffman tree is sensitive to the word frequency’s distribu-\ntion due to its nature of variable length representation based\non the probability of occurrence of words (Huffman 1952).\n1\nThus, when the tree changes, the vector representation of\nboth internal and leaf nodes may be affected. If part of the\ntree remains the same, we can retain the structure as well\nas the vectors associated to the nodes, and distinguish the\nupdate between the parts from the old tree and the new tree.\nNode Initialization and Inheritance\nSuppose we have the old corpus W and the new corpus\nW′ = W∪ ΔW. We can build the Huffman treesT and\nT ′from both corpora respectively.\n1More semantics related trees can be built (Mnih and Hinton\n2008; Le et al. 2013), and the same problem of tree change should\nbe handled.\n3268\n(a) Original Huffman Tree\n (b) Updated Huffman Tree\nFigure 1: The Figure 1(a) isT and the Figure 1(b) isT ′provided the new corpus. The black leaf nodes represent inherited words\nvector. The blue internal nodes represent inherited parameters vector. The gray leaf node represents the new words shown in\nthe new corpus. The white node represents the new internal nodes. The initialization of new internal nodes inT ′ are θ′\n4 =0 ,\nθ′\n6 =0 , θ′\n7 =0 , θ′\n8 =0 . The initialization of new leaf nodes inT ′ are v′(w7) = random, v′(w8) = random. All the other\nnodes in T ′are inherited fromT .\nFor the leaf nodes, if the word has been observed in the\nold corpus, we simply initialize the vector as the vector that\nhas been trained. If the word is a new word, we randomly\ninitialize it as a random vector:\nv\n′(w)=\n{v(w),w ∈W\nrandom,w / ∈W , (3)\nwhere v(w)and v′(w)are the vectors of wordw for old and\nnew trees respectively.\nFor the internal nodes, the Huffman code change of a\nword may affect only partial change of the path for that\nword. Along the path, each internal node owns one param-\neter vector. We distinguish the parameter vector θ\nw1\ni ’s for\nword w1 at i’s position and θw2\ni ’s for word w2 at i’s po-\nsition. When they are at the same i’s position in the tree,\nthen θw1\ni = θw2\ni . For example, in the left ﬁgure of Figure 1,\nθw2\n3 = θw6\n3 = θ4. Moreover, a wordw2 encoded as “0010”\nin the old treeT may be changed as “00010” in the new tree\nT ′. In this case, the matched preﬁx “00” remains the same,\nand corresponding matched internal nodes share the same\nstructure in the new treeT\n′ as the old tree T . To make the\npreﬁx explicit, we denoteLw and L′w as lengths of the code\nof word w in old and new trees respectively, e.g.,Lw2 =4\nin T and L′w2\n=5 in T ′ in Figure 1. We gather all internal\nvectors of leaf nodew as set Ω(w)={θ w\ni |i =1 ,··· ,Lw}\nin treeT . For the matched preﬁx, we use the existing param-\neter vectorθw\ni as the initialization for the new tree, while for\nthe mismatched codes, we initialize them as zero vectors, as\nfollowing:\nθ\n′\ni =\n{θi,d ′w\ni = dw\ni\n0, otherwise , (4)\nwhere d′w\ni and dw\ni are the Huffman codes of internal nodes in\nthe new and old trees respectively. Thus, the inherited inter-\nnal nodes of leaf nodew can be divided into common preﬁx\nmatched substring Θ(w)= {θw\ni−1|i =2 ,··· ,Lw\nC +1} and\nother nodes Φ(w)={θ w\ni−1|i = Lw\nC\n+2,··· ,Lw} in T and\nΦ′(w)={θ w\ni−1|i = Lw\nC\n+2 ,··· ,L′w} in T ′, where Lw\nC\nis length of common preﬁx matched between old and new\ntrees. Figure 1 also shows examples of inherited nodes and\nnew nodes.\nModel Updates\nGiven the inherited nodes and the new nodes by comparing\nthe old and new trees, we also decompose the log-likelihood\nfunctions for CBOW and Skip-gram models based on the\npreﬁx matching results.\nFor CBOW model, we consider to factorize the log-\nlikelihood function by aggregating the cost term ℓ(w,i) in\nEq. (1).\nJ′\nCBOW =\n∑\nw∈W\n{\nLw\nC +1∑\ni=2\n+\nL′w\n∑\ni=Lw\nC\n+2\n}ℓ(w,i)+\n∑\nw∈ΔW\nL′w\n∑\ni=2\nℓ(w,i)\n(5)\nHere we ﬁrst split the training data to beW′ = W∪ ΔW.\nFor the words in W, we factorize it based on the common\nHuffman codes and distinct codes.∑Lw\nC +1\ni=2 sums the codes\nthat share the preﬁx with the old tree by wordw. ∑L′w\ni=Lw\nC +2\nsums inherited internal vector codes by other words and the\nzero initialization internal vector codes in the new tree. For\nthe words inΔW, we follow the original objective function\nof CBOW model.\nSimilarly, for Skip-gram model, the objective J\n′SG\nis: ∑\nw∈W\n∑\nu∈C(w){∑Lu\nC +1\nj=2 +∑L′u\nLu\nC +2}ℓ(w,u,j )+\n∑\nw∈ΔW\n∑\nu∈C(w)\n∑L′u\nj=2 ℓ(w,u,j ).\nTo train a new set of word vectors, originally we need to\nre-scan and re-train the whole corpusW′ = W∪ ΔW based\non stochastic gradient ascent method. Given the above fac-\ntorization analysis of the objective function, we found that\nfor the old corpus W, we can apply the following trick to\nsave a lot of training time.\nOur goal is to ﬁnd a new set of (local) optimal internal\nnode vectors θ\n′w\ni and word vectors v′(w) to approximate\nre-training. We ﬁrst make an assumption that all the word\nvectors v(w) are already (local) optimal and then further\n3269\ncalibrate them. Then we perform stochastic gradient based\noptimization based onW and W′respectively.\nWhen scanning the old corpusW, we can update all the\nparameters while ﬁxing the word vectors.2 Denote all the\nparameters related to w as Θ(w) ∪ Φ′(w). We can see that\nfor the Θ(w), the training process is the same as the orig-\ninal CBOW and Skip-gram models. For Φ′(w), since the\ntree structure has changed, for a certain internal node, some\nof the leaves (words) are still under it while the others has\nmoved out. For example, in Figure 1, the wordw\n6 is now un-\nder θ′\n6 and θ′\n7 but moved out ofθ2, θ4, andθ5. To recover the\nparameters in the new tree so that the incremental training\nis as similar as the fully re-trained model when seeingw6,\nwe need to subtract the inherited gradients ofθ′\n2 related to\nw6, and add the gradients toθ6 and θ7 (here θ′\n4 is initialized\nas zero andθ′\n5 is not inherited). Formally, for a wordw, the\nCBOW update rule for the parameters in the new path from\nroot to this word is as follows.\nIf w ∈T ′,θ′w\ni−1 ∈ Θ(w),i ∈{ 2,...,L w\nC +1},w eh a v e :\nθ′w\ni−1 := θ′w\ni−1\n. (6)\nIf w ∈T ′,θ′w\ni−1 ∈ Φ′(w),i ∈{ Lw\nC +2,...,L ′w},w eh a v e :\nθ′w\ni−1 := θ′w\ni−1\n+η′[1−dw\ni −σ(XT\nwθ′w\ni−1)]Xw. (7)\nIf w ∈T ,θw\ni−1 ∈ Φ(w),i ∈{ Lv\nC +2,...,L v},w eh a v e :\nθ′w\ni−1 := θ′w\ni−1\n−η′[1−dw\ni −σ(XT\nwθ′w\ni−1)]Xw. (8)\nHere retain the common preﬁx nodes, perform stochastic\ngradient ascent to the new nodes, and perform stochastic gra-\ndient descent to the old sub-tree path related to the wordw.\nη\n′is the new learning rate.\nFor the update corpus ΔW, we simply perform the\nstochastic gradient ascent for both parameters and word vec-\ntors (e.g., in Eq. (2)). Thus, we can see that the most com-\nputational cost is saved by not updating word vectors in old\ncorpus, and partially saved by adjusting (partially not updat-\ning) the parameters in old corpus. An illustration is shown in\nFigure 1. From the ﬁgure we can see that, we adjust the inter-\nnal node to approximate the process of complete re-training.\nSimilarly for Skip-gram, if u ∈T\n′,θ′u\nj−1 ∈ Θ(u),j ∈\n{2,...,L u\nC+1},w eh a v e :θ′u\nj−1 := θ′u\nj−1\n.If u ∈T ′,θ′u\nj−1 ∈\nΦ′(u),j ∈{ Lu\nC +2 ,...,L ′u},w eh a v e :θ′u\nj−1 := θ′u\nj−1\n+\nη′[1 − du\nj − σ(v(w)Tθ′u\nj−1)]v(w). If u ∈T ,θu\nj−1 ∈\nΦ(u),j ∈{ Lv\nC +2 ,...,L v},w eh a v e :θ′u\nj−1 := θ′u\nj−1\n−\nη′[1 − du\nj − σ(v(w)Tθ′u\nj−1)]v(w). For above equations,\nu ∈C (w).\nTheoretical Analysis\nIn this section, we present the theoretical analysis of our in-\ncremental learning algorithm using CBOW model.\nConvergence Analysis\nThe log-likelihood function (1) is negative. Thus, maximiz-\ning the objective is bounded by zero. However, since in the\n2We can also update the word vectors in the meantime, however,\nit will introduce more computational cost.\nobjective function, it involves the dot product of the word\nvectors and parameters X\nT\nwθw\ni−1, this is a non-convex opti-\nmization problem. By using alternative optimization altering\nthe word vectors and parameters, ﬁxing one and optimizing\nthe other is a convex problem. In our incremental learning\nprocess, the convergence of optimizing over the update cor-\npus ΔW is the same as original word2vec models. When\noptimizing over the old corpus, we assume the word vectors\nare already (local) optimal, and optimize the parameters over\nthe new Huffman tree. For example for CBOW model, by\nchecking the second order derivative of the parameters, we\nhave ∇\n2\nθ′\ni−1\n= ∑\nw∈W\n∑L′w\ni=2 −σ(XT\nwθ′\ni−1)XT\nwXw, where\nσ(x) ∈ [0,1] and XT\nwXw ≥ 0.\nCompared to the original second order derivative over old\ncorpus, we replace the summation term ∑Lw\ni=2 by ∑L′w\ni=2.\nThis is guaranteed by using both stochastic gradient ascent\nand descent in Eqs. (6)-(8) if we scan the old corpus in the\nsame times, and thus the process is toward another local op-\ntimum. The Skip-gram model has the similar property.\nParameter Error Bound\nHere we focus on the internal nodes that updated with\nstochastic gradient descent. In CBOW model, as shown in\nEqs. (6)-(8), there are two parts affecting the ﬁnal results.\nFirst, the learning rate change is:Δη = η\n0| κ\nφ+1 − κ′\nφ′+1|.\nSecond, we ﬁrst assume that the parameter vector can be\nbounded byξ and then infer the boundedξ in stochastic gra-\ndient descent. We also assume that the word vectors X\nT\nw\ncan be bounded by a vector ⃗ ϵXw where each element is\nϵ. Since we have noted that the optimization process push\nthe solution from on local optimum to another, based on\nﬁrst order Taylor expansion, we have|σ(X\nT\nw(θ′w\ni−1 + ξ) −\nσ(XT\nwθ′w\ni−1\n)| <σ (XT\nwθ′w\ni−1\n)(1−σ(XT\nwθ′w\ni−1\n))ξ<ξ . Then\nwe have the difference of the gradients:\n|ΔBθ′w\ni−1 | = |σ(XT\nwθw\ni−1)−σ(XT\nwθ′w\ni−1)||Xw|⪯ ξ⃗ϵXw ,\n(9)\nwhere ⪯ denotes element-wise less than or equal to. If we\ndenote |B| = |[1 − dw\ni − σ(XT\nwθ′w\ni−1)]Xw|⪯ ⃗ ϵXw , then\nthe parameter will be bounded as by aggregating the dif-\nferences in Eq. (7): ⃗ξ = |ΔηB +Δ Bη′ +Δ ηΔB| ⪯\nΔη⃗ϵXw + η′ξ⃗ϵXw +Δ ηξ⃗ϵXw , where ⃗ξ is a vector of the\nsame value ξ. Then we solveξ as:\n(1−(η′+Δη)ϵ)ξ ≤ Δηϵ. (10)\nTo have more concrete idea of the bound (10), for ex-\nample, assume we have φ =1 09 trained with one billion\nwords/tokens, and κ =1 06 meaning that the vocabulary\nis with one million unique words. We augment the training\ndata with|ΔW| =1 08. Suppose we haveη0 =0 .025, which\nis the default setting in word2vec package. Then we have\nηmin ≤ η ≈ η′ = η0(1 − κ′\nφ′+1) ≤ η0, and Δη ≈ 10−10.\nIn addition, we assume that the elements of word vectors\nbounded by ϵ is in [−5,5]. Thus, for η0, we can compute\nEq. (10) as(1−(0.025+10 −10)∗5)ξ ≤ 10−10 ∗5, which\nmeans ξ ≤ 5.7 ∗ 10−10.F o rηmin = η0 ∗ 0.0001 as that\nin word2vec package, we have ξ ≤ 5.0 ∗ 10−10. At most,\n3270\n10K 100K 1M 10M 100M 1G\n10\n5.002\n10\n5.007\n10\n5.012\n10\n5.017\nIncremental Data (in Bytes)\nLeaf Nodes\n(a) Leaf node change.\n10K 100K 1M 10M 100M 1G0\n0.2\n0.4\n0.6\n0.8\n1\nIncremental Data (in Bytes)\n% of Affected Nodes\n(b) Internal node change.\nFigure 2: Leaf and internal nodes change with incremental\ncorpus.\nthere can be half of the leaves changed from one side of the\ntree to the other side corresponding to completely reversing\nthe order of frequencies. However, this will never happen in\npractice. Moreover, in practice, we found that on the top of\nthe Huffman tree, more updates affected to perform stochas-\ntic gradient descent. However, it is also likely that the leaf\nwill be in the same sub-tree even if it is moved from one\nsuccessor to another. For example, in Figure 1, the move\nof w\n2 and w3 does not affect θ2.F o rn =2 ∗ 105 which\nmeans 20% of the leaves moving out of a sub-tree, if we\nassume the error accumulates, then the error bound will be\n2∗10\n5 ∗5.7∗10−10 =1 .1∗10−4.\nExperiments\nIn this section, we present the experiments to verify the ef-\nfectiveness and efﬁciency of incremental training for hierar-\nchical softmax function in NNLMs.\nTraining Time and Quality\nWe use the English Wikipedia as the source to train the\nNNLMs. We split the data into several sets. We use 2GB\ntexts as the initial training corpus, which is the old corpus\nin previous sections. The 2GB data contains 474,746,098\ntokens and 100,278 unique words. Then, we select 10KB,\n100KB, 1MB, 10MB, 100MB, and 1GB as new update cor-\npora to compare the performance of the algorithms. The\nnumber of words arises with new update corpus, as shown\nin Figure 2(a). For original global training, we combine the\nold and new corpora as a whole, and run the original CBOW\nand Skip-gram models. For the incremental training, we use\nthe model trained based on the 2GB initial training corpus,\nand run our algorithm to update the Huffman tree as well\nas the parameters and word vectors. For all the experiments,\nwe run with 10 CPU threads and generate word embeddings\nwith 300 dimensions.\nFirst, we check the percentage of Huffman tree change.\nIn Figure 2(b) it shows the percentages of the internal nodes\nthat are affected by the change of the tree. If an internal node\nis updated with stochastic gradient descent in Eq. (8), then\nwe label it as affected. From the ﬁgure we can see that, there\nare more nodes affected when adding more training mate-\nrials. The increase is not as much as the increase of total\nnumber of leaf nodes.\nItems Global Incremental T -Reserved\nCB\nMSE 6.68×10−3 6.66×10−3 7.84×10−3\n1-Cos 0.361 0.361 0.411\nE-Dis 1.415 1.413 1.543\nSG\nMSE 6.65×10−3 6.68×10−3 7.83×10−3\n1-Cos 0.363 0.365 0.409\nE-Dis 1.413 1.415 1.533\nTable 1: Average of MSE (mean square error), 1-Cos (con-\nverting cosine similarity to dissimilarity), and E-Dis (Eu-\nclidean) on 2G+1G corpus. “T -Reserved” means that we re-\nserve the old tree to perform a trivial incremental learning.\nThen we check the training time and speedup using our\nincremental training algorithm. We show the training time\nresults in Figure 3(a). It is shown that both CBOW and Skip-\ngram are linear to the training size. Since adding from 10KB\nto 100MB is relatively small compared to the original train-\ning size 2GB, the time curves for both CBOW and Skip-\ngram with global training is ﬂat until with the 1GB addi-\ntional training data. Moreover, we ﬁnd that Skip-gram is\nin an order of magnitude slower than CBOW . By compar-\ning CBOW and Skip-gram, we can see that for each con-\ntext word, Skip-gram needs to update the parameters fol-\nlowing the path from root to that word. Thus, the order\ncomes from the number of window size used in the lan-\nguage model, which is ﬁve in all of our experiments. Further-\nmore, incremental training for both CBOW and Skip-gram\nbeneﬁts from the algorithm and faster than global training.\nAgain, both scale linearly with the number of additional up-\ndate corpus. The speedup results are shown in Figure 3(b).\nWe can see that for smaller update corpus, the speedup\nis more signiﬁcant. The Skip-gram model can have up to\n30 times speedup with our incremental training algorithm,\nwhile CBOW model has up to 10 times speedup.\nWe also randomly selected 5,000 word vectors to test the\ndifference of the word vectors. We use the mean square er-\nror (MSE) to evaluate the difference between two sets of\nword vectors. For global training, we run the same algorithm\ntwice using different random initialization. For incremental\ntraining, we compare the incremental training results with\nthe global training results. In Table 1, we compare the re-\nsults. “Global” represents global training in all corpus. “In-\ncremental” represents our incremental learning models, and\n“T-Reserved” is a trivial incremental learning by working\nwith stochastic gradients on the new corpus with the old tree\nreserved. We can see that in general, the MSE of global\ntraining is better than incremental training. Nonetheless,\nthey are of the same order of magnitude, which is around\n10\n−3 ∼ 10−2. “T -Reserved” is worst since it only uses the\ntree based on the old corpus. To further understand the MSE\nwith the bound, we show the average number of gradient up-\ndates at each level of the Huffman tree for CBOW model in\nFigure 3(c), when scanning the 2GB old data. We can see\nthat, the number of updates exponentially decreases when\nthe depth of the tree increases. For top levels, the moves in-\ndicate that there are indeed a lot of nodes moved from one\nside to the other to trigger the change of parameter updates.\n3271\n10K 100K 1M 10M 100M 1G10\n4\n10\n5\n10\n6\n10\n7\nIncremental Data (in Bytes)\nTime/millisecond\n \n \n Skip−gram (Global)\n Skip−gram (Incremental)\n CBOW (Global)\n CBOW (Incremental)\n(a) Training time.\n10K 100K 1M 10M 100M 1G0\n10\n20\n30Speedup\nIncremental Data (in Bytes)\n \n \nSkip−gram\nCBOW\n(b) Speedup.\n0 10 20 3010\n0\n10\n1\n10\n2\n10\n3\n10\n4\n10\n5\nTree Depth\nAverage # of Updates\n \n \nGradient Descent\nGradient Ascent\n(c) Average # of gradient updates for each\ndepth (2GB+1GB).\nFigure 3: Training performance of global and incremental training.\nThe order of change is at most105. For deepest levels, there\nare no descents but only ascents, which indicates the new\nHuffman tree is deeper than the old one. Then for the 300\nvectors, MSE will aggregate all theξ of each dimension.\nWord Similarity/Relatedness\nNow we use the word similarity/relatedness evaluation\nbenchmarks to evaluate the correctness of our incremen-\ntal training algorithm. Speciﬁcally, we use the datasets col-\nlected by Faruqui and Dyer (Faruqui and Dyer 2014) which\ninclude MC-30, TR-3k, MTurk-287, MTurk-771, RG-65,\nRW-ST ANFORD (RW), SIMLEX-999, VERB-143, WS-\n353-ALL, WS-353-REL, WS-353-SIM, and YP-130.\n3 We\nuse cosine value to compute the similarities between words,\nand then rank the words similar/related to each other. The\nSpearman’s rank correlation coefﬁcient (Myers and Well.\n1995) is used to check the correlation of ranks between hu-\nman annotation and computed similarities. Due to the lim-\nited space, for incremental training, we show the average\nresults trained over 1GB update data. From Figure 4 we\ncan see that, the incremental training results are compara-\nble and sometimes better than the global training results. “T -\nReserved” is again the worst among the three methods we\ncompared.\nDependency Parsing (DP)\nWe also test DP using different training methods. Different\nfrom the previous task, DP uses word embeddings as fea-\ntures, and train a model to predict the structural output of\neach sentence. Thus, it is more complicated than comparing\nthe similarities between words. We use the CNN model (Guo\net al. 2015) to train a model based on the word embed-\ndings produces by our experiments. The data used to train\nand evaluate the parser is the English data in the CoNLL-\nX shared task (Buchholz and Marsi 2006). We follow (Guo\net al. 2015; Upadhyay et al. 2016) to setup the training and\ntesting processes, using the tools available online.\n4 We train\nthe model with 200,000 iterations, and set the parameters as\ndistance of embedding to be 5, valency of embedding to be\n5, and cluster of embedding to be 8 (Guo et al. 2015).\nThe results are shown in Figure 5. Both the labeled attach-\nment score (LAS) and unlabeled attachment score (UAS)\n3http://www.wordvectors.org/\n4https://github.com/jiangfeng1124/acl15-clnndep\nare reported. We can see that, the incremental training and\nglobal training are also similar, and it seems incremen-\ntal training is a little bit better than global training. This\nagain demonstrates that incremental training is comparable\nto global training, and saves a lot of training time. More-\nover, the CBOW model and Skip-gram model perform sim-\nilarly. This may be because the supervised learning model\ncan eliminate the vector representation difference produced\nby different algorithms but with the same training corpus.\nMoreover, we found that there is a step of performance im-\nprovement at 10M new data. This may be because some im-\nportant words are included starting from this data. However,\nfor “T -Reserved”, there is much less improvement when the\nstep happens to the other two. This again demonstrates that\nthe tree can affect the ﬁnal embedding results. If we do not\nconsider the tree change for incremental training, we may\nlose a lot of information provided by the update corpus.\nConclusion\nIn this paper, we present an incremental training algorithm\nfor the hierarchical softmax function for CBOW and Skip-\ngram models. The results of the systematic evaluation and\ndown-stream tasks show that our incremental training is sig-\nniﬁcantly faster than global training, and has similar perfor-\nmance. Theoretical analysis also helped us better understand\nthe performance of the incremental algorithm. The natural\nfuture work is to extend our approach to other advanced\nNNLMs beyond CBOW and Skip-gram such as dependency\nRNN (Mirowski and Vlachos 2015) and LSTM or deeper\nRNN models (Renshaw and Hall 2015).\n5\nAcknowledgments\nThe corresponding author is Jianxin Li. This work\nis suppored by China 973 Fundamental R&D Pro-\ngram (No.2014CB340300), NSFC program (No.61472022,\n61421003), SKLSDE-2016ZX-11, Beijing High-tech R&D\nProgram (Grant No. Z161100000216128) and partly by the\nBeijing Advanced Innovation Center for Big Data and Brain\nComputing. Y angqiu Song was partially supported by China\n973 Fundamental R&D Program (No.2014CB340304) and\n5Our system is publicly available at https://github.com/\nRingBDStack/incremental-word2vec\n3272\nMC−30 TR−3k MTurk−287 MTurk−771 RG−65 RW−STANFORD SIMLEX−999 VERB−143 WS−353−ALL WS−353−REL WS−353−SIM YP−130\n0\n0.2\n0.4\n0.6\n0.8\n \n \nSkip−gram (Global)\nSkip−gram (Incremental)\nSkip−gram (T−Reserved)\nCBOW (Global)\nCBOW (Incremental)\nCBOW (T−Reserved)\nFigure 4: Comparison word embeddings for word similarity/relatedness benchmark datasets.\n10K 100K 1M 10M 100M 1G85\n86\n87\n88\n89\n90\n91\n92\nIncremental Data (in Bytes)\n \n \nUAS(Incremental)\nUAS(Global)\nUAS(T−Reserved)\nLAS(Incremental)\nLAS(Global)\nLAS(T−Reserved)\n(a) CBOW\n10K 100K 1M 10M 100M 1G85\n86\n87\n88\n89\n90\n91\n92\nIncremental Data (in Bytes)\n \n \nUAS(Incremental)\nUAS(Global)\nUAS(T−Reserved)\nLAS(Incremental)\nUAS(Global)\nLAS(T−Reserved)\n(b) Skip-gram\nFigure 5: Comparison of word embeddings for DP .\nthe LORELEI Contract HR0011-15-2-0025 with the US De-\nfense Advanced Research Projects Agency (DARP A). Gov-\nernment is authorized to reproduce and distribute reprints for\nGovernmental purposes notwithstanding any copyright no-\ntation thereon. The views expressed are those of the authors\nand do not reﬂect the ofﬁcial policy or position of the De-\npartment of Defense or the U.S. Government. We also thank\nthe anonymous reviewers for their valuable comments and\nsuggestions that help improve the quality of this manuscript.\nReferences\nBengio, Y .; Ducharme, R.; Vincent, P .; and Janvin, C. 2003.\nA neural probabilistic language model.Journal of Machine\nLearning Research 3:1137–1155.\nBuchholz, S., and Marsi, E. 2006. Conll-x shared task on\nmultilingual dependency parsing. InCoNLL, 149–164.\nCollobert, R.; Weston, J.; Bottou, L.; Karlen, M.;\nKavukcuoglu, K.; and Kuksa, P . P . 2011. Natural lan-\nguage processing (almost) from scratch.Journal of Machine\nLearning Research 12:2493–2537.\nFaruqui, M., and Dyer, C. 2014. Improving vector space\nword representations using multilingual correlation. In\nEACL, 462–471.\nGuo, J.; Che, W .; Y arowsky, D.; Wang, H.; and Liu, T. 2015.\nCross-lingual dependency parsing based on distributed rep-\nresentations. In ACL, 1234–1244.\nGutmann, M., and Hyvärinen, A. 2012. Noise-contrastive\nestimation of unnormalized statistical models, with applica-\ntions to natural image statistics.Journal of Machine Learn-\ning Research 13:307–361.\nHegde, C.; Indyk, P .; and Schmidt, L. 2015. A nearly-linear\ntime framework for graph-structured sparsity. In ICML,\n928–937.\nHuffman, D. A. 1952. A method for the construction\nof minimum-redundancy codes. Proceedings of the IRE\n40(9):1098–1101.\nLe, H. S.; Oparin, I.; Allauzen, A.; Gauvain, J.; and Yvon,\nF. 2013. Structured output layer neural network language\nmodels for speech recognition. IEEE Trans. Audio, Speech\n& Language Processing21(1):195–204.\nLevy, O., and Goldberg, Y . 2014. Neural word embedding\nas implicit matrix factorization. InNIPS, 2177–2185.\nLevy, O.; Goldberg, Y .; and Dagan, I. 2015. Improving\ndistributional similarity with lessons learned from word em-\nbeddings. TACL 3:211–225.\nMikolov, T.; Chen, K.; Corrado, G.; and Dean, J. 2013a.\nEfﬁcient estimation of word representations in vector space.\nICLR.\nMikolov, T.; Sutskever, I.; Chen, K.; Corrado, G. S.; and\nDean, J. 2013b. Distributed representations of words and\nphrases and their compositionality. InNIPS. 3111–3119.\nMirowski, P ., and Vlachos, A. 2015. Dependency recurrent\nneural language models for sentence completion. In ACL,\n511–517.\nMnih, A., and Hinton, G. E. 2008. A scalable hierarchical\ndistributed language model. InNIPS, 1081–1088.\nMorin, F., and Bengio, Y . 2005. Hierarchical probabilistic\nneural network language model. InAISTATS.\nMyers, J. L., and Well., A. D. 1995. Research Design &\nStatistical Analysis. Routledge.\nRenshaw, D., and Hall, K. B. 2015. Long short-term mem-\nory language models with additive morphological features\nfor automatic speech recognition. InICASSP, 5246–5250.\nTurian, J.; Ratinov, L.-A.; and Bengio, Y . 2010. Word\nrepresentations: A simple and general method for semi-\nsupervised learning. In ACL, 384–394.\nUpadhyay, S.; Faruqui, M.; Dyer, C.; and Roth, D. 2016.\nCross-lingual models of word embeddings: An empirical\ncomparison. In ACL.\n3273",
  "topic": "Softmax function",
  "concepts": [
    {
      "name": "Softmax function",
      "score": 0.9455950260162354
    },
    {
      "name": "Computer science",
      "score": 0.8104000091552734
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6328786015510559
    },
    {
      "name": "Word (group theory)",
      "score": 0.5622079372406006
    },
    {
      "name": "Function (biology)",
      "score": 0.5272337794303894
    },
    {
      "name": "Dependency grammar",
      "score": 0.5148209929466248
    },
    {
      "name": "Similarity (geometry)",
      "score": 0.4886920750141144
    },
    {
      "name": "Tree (set theory)",
      "score": 0.48846688866615295
    },
    {
      "name": "Artificial neural network",
      "score": 0.4866717457771301
    },
    {
      "name": "Speedup",
      "score": 0.46263331174850464
    },
    {
      "name": "Parsing",
      "score": 0.4478611648082733
    },
    {
      "name": "Node (physics)",
      "score": 0.44023165106773376
    },
    {
      "name": "Correctness",
      "score": 0.43708962202072144
    },
    {
      "name": "Natural language processing",
      "score": 0.4202789068222046
    },
    {
      "name": "Machine learning",
      "score": 0.3699423670768738
    },
    {
      "name": "Algorithm",
      "score": 0.23116561770439148
    },
    {
      "name": "Mathematics",
      "score": 0.11314067244529724
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Evolutionary biology",
      "score": 0.0
    },
    {
      "name": "Structural engineering",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ]
}