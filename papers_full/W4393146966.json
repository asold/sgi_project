{
  "title": "Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation",
  "url": "https://openalex.org/W4393146966",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2778836142",
      "name": "Zhewei Yao",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2096425336",
      "name": "Xiaoxia Wu",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A674962318",
      "name": "Cheng Li",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A5063494658",
      "name": "Stephen Youn",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2166872174",
      "name": "Yuxiong He",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2778836142",
      "name": "Zhewei Yao",
      "affiliations": [
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2096425336",
      "name": "Xiaoxia Wu",
      "affiliations": [
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A674962318",
      "name": "Cheng Li",
      "affiliations": [
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5063494658",
      "name": "Stephen Youn",
      "affiliations": [
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2166872174",
      "name": "Yuxiong He",
      "affiliations": [
        "Microsoft (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6862640317",
    "https://openalex.org/W2125389748",
    "https://openalex.org/W6638523607",
    "https://openalex.org/W6788001715",
    "https://openalex.org/W6677103964",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W6727099177",
    "https://openalex.org/W2973061659",
    "https://openalex.org/W3022810465",
    "https://openalex.org/W4318751318",
    "https://openalex.org/W4308760184",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W4293166090",
    "https://openalex.org/W3100083812",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2979314664",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4292119927",
    "https://openalex.org/W3202028501",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4309591680",
    "https://openalex.org/W2964203871",
    "https://openalex.org/W4307934016",
    "https://openalex.org/W4221167110",
    "https://openalex.org/W3173374050",
    "https://openalex.org/W4312056202",
    "https://openalex.org/W4226087293",
    "https://openalex.org/W2916954108",
    "https://openalex.org/W2998183051",
    "https://openalex.org/W3100985894",
    "https://openalex.org/W4281651027",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W4281609268",
    "https://openalex.org/W3017061195",
    "https://openalex.org/W3170233084",
    "https://openalex.org/W3098576111",
    "https://openalex.org/W4303443398"
  ],
  "abstract": "Post-training quantization (PTQ) has emerged as a promising technique for mitigating memory consumption and computational costs in large language models (LLMs). However, a systematic examination of various quantization schemes, model families, and quantization bit precision has been absent from the literature. In this paper, we conduct a comprehensive analysis of these factors by investigating the effects of PTQ on weight-only, activation-only, and weight-and-activation quantization using diverse methods such as round-to-nearest (RTN), GPTQ, ZeroQuant, and their variants. We apply these methods to two distinct model families with parameters ranging from 125M to 176B. Our contributions include: (1) a sensitivity analysis revealing that activation quantization is generally more susceptible to weight quantization, with smaller models often outperforming larger models in terms of activation quantization; (2) an evaluation and comparison of existing PTQ methods to optimize model size reduction while minimizing the impact on accuracy, revealing that none of the current methods can achieve the original model quality for quantization with either INT4-weight or INT4-weight-and-INT8-activation; (3) based on these insights, we propose an optimized method called Low-Rank Compensation (LoRC), which employs low-rank matrices to enhance model quality recovery with a minimal increase in model size.",
  "full_text": "Exploring Post-training Quantization in LLMs from Comprehensive Study to Low\nRank Compensation\nZhewei Yao*1, Xiaoxia Wu∗1, Cheng Li1, Stephen Youn1, Yuxiong He1\n1 DeepSpeed of Microsoft\nAbstract\nPost-training quantization (PTQ) has emerged as a promising\ntechnique for mitigating memory consumption and compu-\ntational costs in large language models (LLMs). However,\na systematic examination of various quantization schemes,\nmodel families, and quantization bit precision has been absent\nfrom the literature. In this paper, we conduct a comprehensive\nanalysis of these factors by investigating the effects of PTQ on\nweight-only, activation-only, and weight-and-activation quanti-\nzation using diverse methods such as round-to-nearest (RTN),\nGPTQ, ZeroQuant, and their variants. We apply these meth-\nods to two distinct model families with parameters ranging\nfrom 125M to 176B. Our contributions include: (1) a sensitiv-\nity analysis revealing that activation quantization is generally\nmore susceptible to weight quantization, with smaller mod-\nels often outperforming larger models in terms of activation\nquantization; (2) an evaluation and comparison of existing\nPTQ methods to optimize model size reduction while min-\nimizing the impact on accuracy, revealing that none of the\ncurrent methods can achieve the original model quality for\nquantization with either INT4-weight or INT4-weight-and-\nINT8-activation; (3) based on these insights, we propose an\noptimized method called Low-Rank Compensation (LoRC),\nwhich employs low-rank matrices to enhance model quality\nrecovery with a minimal increase in model size.\nIntroduction\nLarge language models (LLMs) like Codex (Copilot 2021)\nand ChatGPT (OpenAI 2022) have demonstrated break-\nthrough performance across various benchmarks, such as nat-\nural language understanding and generation, and are now in-\ntegrated into everyday applications. However, efficiently serv-\ning LLMs has become a pressing concern due to their signifi-\ncant memory consumption and computational demands. Un-\nlike classification or diffusion models, LLMs present unique\nchallenges, as they involve two distinct phases: prompt and\ngeneration. The prompt phase is primarily compute-bound,\nwhile the generation phase, with low batch size and KV cache,\nis mainly memory-bound (Pope et al. 2022).\nAs the progression of hardware bandwidth lags behind\nthat of computational demand (Gholami et al. 2021), the\n*Equal Contribution.\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: The model size and quality trade-off of different\nquantization methods on models from OPT families. Here\nPTQ (with fine-grained quantization) represents the method\nfrom (Yao et al. 2022; Frantar et al. 2022), RTN means the\nnaive round-to-nearest baseline (with fine-grained quantiza-\ntion as well), and FP16/INT8 is used as the no-accuracy-loss\nbaseline. LoRC is our proposed method that works seamless\nwith PTQ. Note that we drop all diverged points for better\nvisualization.\nresource demands of extra-large models such as MT-NLG-\n530B (Smith et al. 2022)—which necessitates the deployment\nof multiple nodes for operation—escalate, adding to the com-\nplexities of cross-node communication. This has emphasized\nthe urgency to curtail both the size and computational ex-\npense of Large Language Models (LLMs). An increasingly\neffective solution to these issues is post-training quantization\n(PTQ). This method aids in the reduction of training prereq-\nuisites while simultaneously lowering the bit precision of\nweights and activations to either INT4 or INT8.\nWhile the effectiveness of post-training quantization (PTQ)\nhas been underscored in a number of recent studies (Yao et al.\n2022; Frantar et al. 2022; Xiao et al. 2022; Dettmers and\nZettlemoyer 2022), a comprehensive, systematic investiga-\ntion into several key dimensions of this technique remains to\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19377\nbe undertaken. Specifically, the extant literature falls short in\nproviding thorough coverage of the functionality of various\nPTQ methods or the sensitivity of disparate models. More-\nover, despite current quantization methods demonstrating\npromising results in the reduction of model sizes, the ques-\ntion persists as to whether these methods are achieving their\noptimal potential in minimizing Large Language Models\n(LLMs) sizes.\nWith these observations in mind, our study sets forth to\naddress two salient questions: (1) When subjected to quanti-\nzation, do LLMs of varying sizes and pretraining data exhibit\nsimilar behavior? (2) Are existing quantization methods truly\nleveraging their full potential in reducing the sizes of LLMs?\nContribution. To elucidate these queries, we undertake\nan exhaustive examination of the impact of PTQ on weight-\nonly, activation-only, and combined weight-and-activation\nquantization. This investigation incorporates a range of PTQ\nmethods, including round-to-nearest (RTN), GPTQ (Frantar\net al. 2022), ZeroQuant (Yao et al. 2022), and their respective\nvariants. To broaden the scope of our analysis, we focus on\ntwo distinct model families, OPT (Zhang et al. 2022) and\nBLOOM (Scao et al. 2022), spanning model sizes from 125M\nto a massive 176B. Our code will be made available for repro-\nduction. In summary, we make the following contributions:\n(1) We provide a thorough sensitivity analysis to demon-\nstrate that a) Activation quantization is generally more sen-\nsitive to weight quantization; Smaller models usually have\nbetter activation quantization performance than the relative\nlarger model. b) Different model families show different\nINT8 activation quantization behaviors; Particularly for large\nmodels, BLOOM-176B has small accuracy drops (about 1\nperplexity or PPL) but OPT-30B and -66B experience worse\nperformance.\n(2) We carry out a detailed evaluation and comparison\nof current PTQ methods, utilizing optimal configurations\nto maximize model size reduction while minimizing accu-\nracy impact. We found that the current existing method can\nbarely achieve less than 0.1 PPL points degradation for quan-\ntization with either INT4-weight or INT4-weight-and-INT8-\nactivation (W4A8). To recover the 0.1 PPL, we strive to\npush the boundaries of employing fine-grained quantiza-\ntion (FGQ) techniques. We observe FGQ is able to recovered\npoints degradation of <0.1 PPL for large models (>13B) for\nINT4 weight quantization, but there are still non-negligible\nmodel quality drops.\n(3) Based on the above understanding, we further optimize\nexisting methods and introduce a technique called Low Rank\nCompensation (LoRC), which employs low-rank matrix fac-\ntorization on the quantization error matrix. Complementary\nto FGQ, LoRC plays a crucial role in enhancing the full\nmodel quality recovery, while there is little increase of the\nmodel size.\nIn Figure 1, we provide model size and quality trade-offs\nfor both OPT families. As can be seen, using LoRC on top of\nPTQ methods from (Yao et al. 2022; Frantar et al. 2022) and\nfine-grained quantization, we set a new quantization Pareto\nfrontier for LLMs. Meanwhile, we recommend the following\nsetting for quantizing LLMs with LoRC (Note that activation\nquantization should be only applied if necessary): (1) For\nlarger models (>10B), fine-grained (block size 64–256) 4-bit\nweight quantization plus 8-bit activation quantization (block\nsize 64–256) with PTQ can be used for real deployment;\n(2) For middle-size models (<10B and >1B), per-row INT8\nquantization plus fine-grained (block size 64–256) INT8 ac-\ntivation quantization can be used with PTQ from (Frantar\net al. 2022; Yao et al. 2022); (3) For smaller models (<1B),\nper-row W8A8 (INT8 weight and INT8 activation) RTN is\nenough based on (Yao et al. 2022). 1\nRelated Work\nDifferent quantization methods (Shen et al. 2020; Zafrir et al.\n2019; Fan et al. 2020; Zhang et al. 2020; Bai et al. 2020;\nEsser et al. 2019; Tao et al. 2022; Kim et al. 2021) for\ntransformer-based models (Vaswani et al. 2017) have been\nexplored for a while. However, most of those works need\nquantization-aware finetuning or even expensive quantization-\naware knowledge distillation (Hinton, Vinyals, and Dean\n2014). Due to the cost of training/finetuning LLMs (Polino,\nPascanu, and Alistarh 2018; Jiao et al. 2019; Tao et al. 2022;\nWu et al. 2022, 2023), it is a challenge for practitioners/re-\nsearchers to do finetuning/distillation on those LLMs, partic-\nularly for models like GPT-3-175B (Brown et al. 2020) and\nBLOOM-176B (Scao et al. 2022).\nPost-training quantization (PTQ) (Zadeh et al. 2020; Bon-\ndarenko, Nagel, and Blankevoort 2021) is an alternative way\nto quantize the model with no/minimal finetuning require-\nment. Along this line, several recent works focus on LLMs\n(beyond the million-parameter scale). (Yao et al. 2022) pro-\nposes vector-based INT8 quantization with layer-by-layer\nknowledge distillation to overcome the training cost and quan-\ntization error introduced by LLMs. (Dettmers et al. 2022)\nuses similar vector-based INT8 quantization weight plus\nmixed-precision (INT8/FP16) quantization for activation to\novercome the sensitivity of activation quantization. However,\nthe inference speed of (Dettmers et al. 2022) is generally\neven slower than FP16 baseline (Big-Science 2022) due to\nthe difficulty of implementing mixed-precision calculation\nwithin a single tensor. More recently, (Frantar et al. 2022)\nextends OBQ (Frantar and Alistarh 2022; Hassibi and Stork\n1993; LeCun, Denker, and Solla 1990) on LLMs for INT4\nweight-only quantization and shows great efficiency on quan-\ntization and latency, and (Xiao et al. 2022) shows the outliers\nfrom activations can be smoothed out by migrating the quan-\ntization difficulty from activations to its associated weights.\nHowever, (Xiao et al. 2022) can only work for W8A8 quanti-\nzation as lower weight precision (INT4) itself already leads\nto significant accuracy degradation, and the accuracy drop is\nlarger than 0.1 PPL points, which as discussed in the later sec-\ntion is sub-optimal. (Dettmers and Zettlemoyer 2022) shows\nthe scaling law of weight-only quantization with the sim-\nplest round-to-nearest baseline, but it does not consider the\nweight-and-activation quantization and/or the above PTQ\noptimization methods. As can be seen from Figure 1, by\nusing PTQ optimization methods, the model quality can be\nsignificantly improved.\n1Full version is in arXiv: 2303.08302.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19378\nClass Class-1 Class-2 Class-3\nPPL Degradation ≤0.1 >0.1 & ≤0.5 >0.5\nTable 1: Classification of quantization sensitivity (or quantiza-\ntion loss). The sensitivity increases from Class-1 to Class-3.\nDifferent than existing works, our paper extensively tests\nthe effect of (1) different quantization schemes, e.g., symmet-\nric and asymmetric quantization, (2) different PTQ methods,\ne.g., (Yao et al. 2022; Frantar et al. 2022), (3) different model\nfamilies, e.g., (Scao et al. 2022; Zhang et al. 2022), (4) dif-\nferent quantization coverage, e.g., weight-only and weight-\nand-activation quantization, and (5) other discussions, e.g.,\nthe effect of quantization granularity. As such, we provide\na much more comprehensive understanding of post-training\nquantization for large language models compared to the pre-\nvious works.\nWould Different Model Families Behave\nSimilarly On Quantization?\nThere are mainly two categories of PTQ for LLMs, i.e.,\nweight-only quantization (Frantar et al. 2022) and weight-\nand-activation quantization (Dettmers et al. 2022; Yao et al.\n2022; Xiao et al. 2022). In the latter, it is uniformly observed\nacross all studies that activation quantization demonstrates\ngreater sensitivity than weight quantization. However, prior\nresearch tends to concentrate on a single (family) model to\nemphasize the necessity of their proposed quantization tech-\nnique. A comprehensive and systematic evaluation of this\nPTQ methodology, particularly the sensitivity of weight/ac-\ntivation quantization for varying model sizes and distinct\nmodel families, has yet to be undertaken. Hence, we con-\nduct an examination on both the OPT (Zhang et al. 2022)\nand BLOOM (Scao et al. 2022) families to elucidate the\nquantization sensitivity of weight and activation.\nSensitivity Setting. We use the zero-shot validation per-\nplexity (PPL) differential on three datasets, namely, Wikitext-\n2 (Merity et al. 2017), PTB (Marcinkiewicz 1994), and\nC4 (Raffel et al. 2019), before and after the quantization\nof these LLMs to illustrate their sensitivity, as PPL is signif-\nicantly correlated to zero-shot/few-shot accuracy measure-\nment (Dettmers and Zettlemoyer 2022). Specifically, a higher\nPPL drop indicates enhanced quantization sensitivity. For\nsimplicity, we also categorize quantization sensitivity (or\nquantization loss) into three different classes as depicted\nin Table 1. Notably, the threshold is chosen because when the\nmodel size approximately doubles (e.g., 13B vs. 30B, and\n30B vs. 66B), the PPL improvement is about 0.5 (see Ta-\nble 2). The sensitivity (or loss) incrementally increases as\nthe class number ascends. From a practical standpoint, we\nfavor lower quantization sensitivity (accuracy loss), making\nClass-1 the optimal-loss post-training quantization.\nBoth symmetric and asymmetric quantization are used\nto gauge the quantization sensitivity and the advantage of\nasymmetric quantization is highlighted. Particularly, we im-\nplement per-row quantization (Frantar et al. 2022) for weight\nand per-token quantization for activation (Yao et al. 2022).\nRobustness of Weight-only Quantization for Large\nModels. The results of weight-only quantization in OPT and\nBLOOM models are summarized in Table 2. INT8 weight-\nonly quantization, either symmetric or asymmetric, results in\nnegligible accuracy loss (less than 0.05, i.e., Class-1). Conse-\nquently, for tasks oriented towards generation, FP16 weight\ncan simply be replaced with INT8 weight to reduce memory\nusage. For INT4 quantization, the asymmetric method out-\nperforms the symmetric approach in accuracy, attributable\nto its superior utilization of the quantization range. Interest-\ningly, larger models exhibit better tolerance to low-precision\nquantization (i.e., INT4) than smaller models, with a few\nexceptions such as OPT-66B.2 Particularly, BLOOM-176B\nshows PPL degradation (around 0.3 points) inClass-2, which\ncould explain why the large GLM-130B (Zeng et al. 2022)\ncan operate with INT4 weight-only quantization out of the\nbox with acceptable accuracy impact.\nChallenge Encountered in Activation Quantization\nfor Large Models. Activation quantization has consistently\nproven more difficult than weight quantization (Yao et al.\n2022; Dettmers et al. 2022), as illustrated in Table 2. When\ncompared to weight-only quantization, activation-only quan-\ntization indicates that asymmetric quantization can signifi-\ncantly improved performance over symmetric quantization.\nMoreover, contrary to weight-only quantization, smaller mod-\nels typically exhibit better tolerance to activation quantization,\nas their hidden dimension is smaller and the activation dy-\nnamic range is also narrower than larger models (Yao et al.\n2022). It should be noted that for models larger than 10B, all\nfall into Class-3, indicating a degradation of more than 0.5\nPPL points.\nThe last two rows of Table 2 show that different model\nfamilies exhibit significantly different behaviors. BLOOM\ndoes not exhibit divergence issues even up to a model size of\n176B, whereas OPT displays very poor performance from a\nmodel size of 6.7B (larger models with INT8 activation have\neven worse PPL). This could again be attributed to the Layer\nNorm issue within the OPT-family2.\nFindings 1 on Sensitivity Analysis. (1)INT8 weight-\nonly quantization can serve as a standard method\nfor reducing memory costs in LLMs, with negligi-\nble degradation in accuracy. (2) INT4 weight-only\nquantization for small models results in substantial\naccuracy degradation (Class-3), but this effect lessens\nas the model size increases (Class-2). (3) Contrary\nto (2), INT8 activation results in minimal accuracy\ndrops for small models (Class-1) but larger models\nexhibit greater drops (Class-3). (4) With INT8 acti-\nvation, BLOOM shows no divergence issues up to a\nmodel size of 176B, whereas OPT performs poorly\nfrom ≥ 6.7B model sizes.\n2(Frantar et al. 2022) discovered that OPT-66B has a high pro-\nportion of dead neurons in the early layers, which might influence\nthe compression capability. We also identify another potential rea-\nson: the Layer Norm of the OPT-family is not well trained (except\nOPT-350M), with the weight and the bias being all 1’s and 0’s.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19379\nPrecision OPT\n-6.7b OPT-13b OPT-30b OPT-66b BLM-1.7b BLM-3b BLM-7.1b BLM-176b\nW16-A16 11.90\n11.22 10.70 10.33 20.43 17.58 14.96 10.90\nW8sym-A16 11.90\n11.22 10.70 10.33 20.43 17.59 14.97 10.90\nW8asym-A16 11.90 11.22 10.70 10.33 20.45 17.59 14.97 10.90\nW4sym-A16 14.36\n12.73 11.77 97.05 23.18 19.36 16.27 11.28\nW4asym-A16 13.44 12.09 11.52 31.52 22.47 19.01 15.90 11.20\nW16-A8sym 26.04 3171.49\n2048.21 2638.09 20.68 17.73 15.28 12.10\nW16-A8asym 12.62 15.36 23.57 561.35 20.52 17.65 15.14 11.62\nTable 2: Average PPL of OPT and BLOOM (BLM).\nAre Existing Quantization Methods Optimally\nHarnessing the Potential to Minimize Sizes?\nNumerous lightweight optimization-based methods have\nbeen proposed, which update the model weights during quan-\ntization. These methods such as (Yao et al. 2022; Frantar et al.\n2022; Xiao et al. 2022), unlike quantization-aware training,\nonly require a small portion of the training data and a limited\ntraining time. Particularly, GPTQ (Frantar et al. 2022) and\nZeroQuant (Yao et al. 2022), have proven to be effective and\nefficient in terms of GPU resources, time cost, and data usage\nfor INT4 weight quantization.3 In this work, we focus on the\nvariants of GPTQ and ZeroQuant as well as the most straight-\nforward baseline, round-to-nearest neighborhood (RTN).\nRTN directly applies PTQ on the trained data to perform\nthe quantization. Specifically, for symmetric quantization, we\nset S = max(abs(x)) and Z = 0; for asymmetric quantiza-\ntion, we set S = max(x) − min(x) and Z = min(x).\nGPTQ extends the OBQ (Frantar and Alistarh 2022). It\ntries to optimize the following non-linear least square prob-\nlem, min ˆW ∥Wx − ˆWx∥2\n2 where W is the weight, x is the\nactivation, and ˆW is a quantized weight. GPTQ employs\nsecond-order methods to obtain a closed-form solution. In ad-\ndition, the quantization for each weight matrix is performed\ncolumn-/row-wisely and the quantization errors from previ-\nous columns will be passed to those columns not yet quan-\ntized. See(Frantar and Alistarh 2022; Frantar et al. 2022) for\nmore details.\nZQ-Global is the original method proposed in (Yao et al.\n2022), where authors treat each layer as a small neural net-\nwork (a.k.a., subnetwork) and use the FP16 subnetwork as the\nteacher model to distill the quantized one with a few hundred\niterations, i.e., minˆθ ∥fθ(x) − fˆθ(x)∥2\n2, where θ is a set of\nweights, ˆθ is the quantized version,fθ is the subnetwork with\nparameters θ, and x is the input. Thus, it can significantly\nreduce the GPU resource requirement and time cost.\nZQ-Local is an extension mode of ZQ-Global for further\nGPU requirement reduction and training cost reduction. Par-\n3We tested the method proposed by (Xiao et al. 2022) but did\nnot find it better than others for INT4 weight quantization.\nticularly, instead of using each transformer layer as the sub-\nnetwork, we treat each linear layer as the subnetwork. This\nmethod can be viewed as an iterative first-order optimization\nmethod (e.g., SGD) to solve min ˆW ∥Wx − ˆWx∥2\n2.\nExperimental Setup. We compare the four methods men-\ntioned above on weight-only and weight-and-activation quan-\ntization. As weight quantization is always static (i.e., it does\nnot change during inference), there is virtually no system\nperformance difference between symmetric and asymmetric\nquantization.4 We use asymmetric quantization for better ac-\ncuracy, and the conclusions would hold similarly for symmet-\nric quantization. For parameters used for GPTQ, ZQ-Local,\nand ZQ-Global, please refer to Appendix in our arxiv paper.\nAn interesting finding for ZeroQuant is that the hyperparam-\neters (e.g., learning rate and its scheduler) provided in the\noriginal work (Yao et al. 2022) are sub-optimal. In this work,\nwe find the best configurations for ZQ-Local and ZQ-Global\nand denote them as ZQ-Local∗ and ZQ-Global∗, respectively,\nwith the best tuned results. To ensure consistent and compara-\nble results, we set a fixed random seed for our experiments. In\nthe context of post-training quantization, varying the random\nseed has minimal impact on the final results.\nEvaluation of Weight-only Quantization. The results\nfrom weight-only quantization using OPT and Bloom are\npresented in Table 3. The findings indicate that the larger\nmodels tend to be less sensitive to INT4 weight-only quanti-\nzation. This observation holds true across all methods (RTN,\nGPTQ, ZQ-Local∗, and ZQ-Global∗) with the exception of\nOPT-66B, which shows greater degradation than OPT-30B.\nIt is noteworthy that light-weight optimization-based meth-\nods significantly outperform the RTN baseline in terms of\naccuracy. For instance, these methods substantially reduce\nthe degradation in perplexity of OPT-30B/66B compared to\nbaseline. Most quantized models with parameters greater\nthan 6.7B fall under Class II, indicating their potential for\nreal-world applications. For instance, the quality of INT4\nOPT-30B (66B) is superior to that of INT8 OPT-13B (30B).\nAmong the optimization-based methods, ZQ-Global∗ gen-\n4The bias term (a.k.a., the zero point) can be simply fused into\nthe previous activation quantization kernel (Yao et al. 2022).\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19380\nerally performs better on smaller models (those with fewer\nthan 1B parameters), while GPTQ excels on larger models.\nZQ-Local∗ does not outperform GPTQ or ZQ-Global∗-— a\nreasonable outcome given that GPTQ employs a closed-form\nsolution to solve the non-linear quadratic problem and ZQ-\nGlobal∗ optimizes a larger subnetwork. The inferior perfor-\nmance of ZQ-Global∗ compared to GPTQ for larger models is\nunexpected since ZQ-Global∗ optimizes an entire transformer\nlayer while GPTQ only optimizes a single linear layer. An\nexplanation is that larger models are more sensitive to weight\nupdates, necessitating more advanced fine-tuning methods.\nEvaluation of Weight and Activation Quantization.The\nevaluation results for existing methods using W4A8 quan-\ntization are presented in Table 3. The three light-weight\noptimization-based methods outperform RTN significantly,\nunderscoring their efficacy. However, all of the results fall\ninto either Class-2 or Class-3. This suggests that for cer-\ntain applications, it might be more beneficial to use smaller\nmodels with fewer parameters rather than larger, quantized\nmodels.\nAmong quantization-based methods, ZQ-Global∗ and ZQ-\nLocal∗ generally outperform GPTQ, which is anticipated\ngiven that GPTQ was originally designed for weight-only\nquantization. ZQ-Global∗ performs better than ZQ-Local∗ in\nmost cases except for the two largest models, OPT-66B and\nBloom-176B, despite having larger trainable parameters in\none step. This again signifies the need for a more suitable and\nadvanced optimization method for large language models.\nFinding 2 on Comparisons. (1) GPTQ typically per-\nforms better for weight-only quantization, while Ze-\nroQuant (including both ZQ-Global∗ and ZQ-Local∗)\nyields superior results for weight and activation quan-\ntization. (2) The tested optimization-based methods\ncannot achieve Class-1 quantization error for either\nINT4 weight-only or W4A8 quantization with the\nexception of GPTQ on OPT-30B with W4A16.\nFine-grained Quantization\nand Its Evaluation\nWith PTQ and row-wise quantization, achieving Class-1\nquantization error is challenging for both weight-only and\nweight-and-activation quantization. Generally, utilizing a\nsmaller model with INT8 weight is more advantageous than\nemploying a model that is twice as large with INT4 weight.\nOne potential solution to this issue is the implementation\nof finer-grained quantization schemes (Darvish Rouhani et al.\n2020), where every k elements possess their own scaling fac-\ntor and/or zero point. This approach can significantly reduce\nquantization error. In the extreme case, where every single\nelement has its own scaling factor, the original FP16 number\ncan be precisely recovered. Importantly, block-k quantiza-\ntion can be implemented on modern GPUs, one of the most\nprevalent deep learning architectures, since the compute unit\n(streaming multiprocessor) of GPUs processes tiles of data\n(e.g., 128 by 128 tiling size) for matrix computation.\nAlthough fine-grained quantization can substantially nar-\nrow the gap between the quantized tensor and its floating-\npoint counterpart, the application of RTN still results in a\nnon-trivial accuracy gap. Consequently, we build upon fine-\ngrained quantization by employing existing optimization-\nbased methods to further enhance accuracy. Specifically, we\nutilize GPTQ and ZQ-Global for all models and settings and\napply ZQ-Local to OPT-66B and Bloom-176B. For the hy-\nperparameters used in ZQ-Global and ZQ-Local, we select\nthe top three identified in Section for all models, except\nfor Bloom-176B, for which we only use the top-performing\nhyperparameter to reduce training costs.\n4-bit Weight Quantization. We hereby present the W4A16\nresults for OPT and BLOOM, as delineated in Table 4, cor-\nresponding to an array of quantization block sizes. The per-\nformance sees a significant improvement with smaller block\nsizes compared to per-row quantization. The point of dimin-\nishing returns, however, varies for different model sizes. For\nexample, smaller models (such as OPT-6.7B and BLOOM-\n1.7b) continue to see substantial gains until the block size\nreduces to 32. In contrast, for larger models (those exceeding\n10B, with OPT-66B as the exception), the benefits derived\nfrom smaller block sizes wane rapidly around block-256/512.\nMost crucially, for models equal to or larger than 13B, a\nsmaller quantization block size results in quantization error\nbeing classified under Class-1, indicating virtually negligible\ndegradation in accuracy.\nActivation Quantization (W4A8). To comprehend the\nbenefits of fine-grained quantization on activation, we an-\nalyze the quantization between per-row and a block size\nof 128, with INT4 weight, as highlighted in Table 3. For\nmodels of considerable size, specifically those equal to or\nexceeding 1B, the application of such fine-grained activation\nquantization (Case-1) results in a substantial reduction in\nquantization error compared to per-row activation (Case-2).\nBy implementing fine-grained activation quantization with\nweight quantization (Case-3), we are able to almost restore\nthe performance to the level of their W4A16 counterparts.\nFurthermore, we detail the impacts of varying activation\nquantization block sizes in Table 5 on BLOOM-176B, with\nINT4 weight. A trend of superior accuracy is observed with\nsmaller block sizes in contrast to larger ones. However, the\nenhancement in performance reaches a saturation point when\nthe size smaller or equal to 256, which corresponds to the\nrange of values INT8 can represent. Despite INT8’s capability\nto signify 256 distinct values, activation quantization errors\npersist due to the application of uniform quantization.\nFinding 3 on FGQ. (1) Larger models (≥10B) are\ncapable of attaining Class-1 error for 4-bit quantiza-\ntion. These models can leverage low-precision quan-\ntization as the model size with INT4 is similar to\nan INT8 model that is half its size, with improved\naccuracy. On the other hand, smaller models (≤10B)\ntypically reach only Class-2 or Class-3 error levels.\n(2) For larger models (>10B), the difference between\nfine-grained weight-and-activation quantization and\nfine-grained weight-only quantization is insignificant.\n(3) The advantage of fine-grained activation quanti-\nzation fades when the block size reaches 256.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19381\nPrecision Method\nOPT-6.7b OPT-13b OPT-30b OPT-66b BLM-1.7b BLM-3b BLM-7.1b BLM-176b\nW16A16 11.90\n11.22 10.70 10.33 20.43 17.58 14.96 10.90\nW4A16\nRTN\n13.44 12.09 11.52 31.52 22.47 19.01 15.90 11.20\nGPTQ 12.28 11.42 10.78 10.52 21.58 18.33 15.50 11.02\nZQ-Local∗ 12.46 11.64 11.05 10.79 21.70 18.50 15.55 11.11\nZQ-Global∗ 12.38 11.62 11.04 10.68 21.38 18.33 15.52 11.05\nW4A8\nRTN\n14.80 26.36 86.26 815.00 22.75 19.17 16.19 12.22\nGPTQ 13.88 17.28 20.71 648.69 21.71 18.44 15.75 11.86\nZQ-Local∗ 13.24 14.23 18.53 16.32 21.86 18.66 15.75 11.19\nZQ-Global∗ 13.17 13.07 14.65 37.82 21.43 18.39 15.58 11.49\nTable 3: Different PTQ methods on OPT and BLOOM (BLM) with asymmmetric quantization on weight or (and) activation.\nBlock-size OPT-6.7b OPT-13b OPT-30b\nW16A16 11.90 11.22 10.70\n1024 12.16 11.36 10.75\n512 12.08 11.32 10.73\n256 12.05 11.28 10.74\n128 12.10 11.28 10.74\n32 12.03 11.28 10.72\nTable 4: W4asym-A16 quantization out of the best result from\noptimization-based methods on OPT and BLOOM. N/A\nmeans that the block size is not divisible by the hidden size.\nA8 Block Size 1024 512 256 128 32\nPPL 10.98 10.97 10.95 10.95 10.95\nTable 5: BLOOM-176B with different quantization block\nsizes on activation. Here weight is asymmetrically quantized\nwith block size 128.\nProposed Method to Further Push the Limit of\nPost-training Quantization\nBuilding on the investigation and conclusions drawn from\nprevious sections, it has become apparent that there is still a\nneed for an advanced methodology to further refine the exist-\ning methods, with the objective of fully realizing the original\nfp16 PPL quality. In this section, we introduce a simple yet\neffective method called LoRC (Low Rank Compensation) to\noptimize the current existing quantization error and further\nbridge the gap between the quality of the original model and\nits quantized counterparts. LoRC is inspired by the employ-\nment of low-rank matrix factorization on the quantization\nerror matrix E := W − ˆW, where W represents the original\nweight and ˆW is the quantized weight. LoRC approximates\nthe error E with ˆE = ˆU ˆV by two low-rank ˆU and ˆV :\nStep I: Implement Singular Value Decomposition (SVD)\non the error matrix E = UΣV , where U ∈ Rdin×din and\nV ∈ Rdout×dout are unitary matrices, and Σ ∈ Rdin×dout is\na diagonal matrix with its diagonal elements ordered in a\ndescending manner.\nStep II: Let ˆE = ˆU ˆV where ˆU = Um(Σm)\n1\n2 and ˆV =\n(Σm)\n1\n2 Vm. Here, Um = U:,1:m ∈ Rdin×m, Vm = V1:m,: ∈\nRm×dout , and Σm = Σ1:m,1:m ∈ Rm×m.\nThe parameters of LoRC added to the existing model.Con-\nsider a matrix represented as W + UV : where W has di-\nmensions d × d, and the two low-rank matrix U and V are\nrespectively of dimensions: d × r and r × d. The additional\nparameter ratio incorporated into the existing matrix is 2r/d.\nFor models in the OPT family with sizes of 1.3b, 13b, and\n30b, the hidden-size d dimensions are 2048, 5120, and 7168,\nrespectively. Our findings indicate that a rank r of 8 is ad-\nequate (as shown in Table 9), leading to added parameter\nratios of 0.008, 0.003, and 0.002, respectively. For precision\nclarity, the W matrix is quantized to 4/3/2 bits in our ap-\nproach, while LoRC components (U and V ) are maintained\nat 8 bits. Hence, the memory considerations should indeed\nreflect this distinction. We provide further details on compu-\ntational memory and FLOPs as follows:\n• FLOPs Calculation. For the operation WX + UV X,\nwith the input X having dimensions d × s (where s is the\nsequence length, typically 512, 1024, or 2048 tokens), the\nFLOPs for the matrix multiplication WX would be 2sd2.\nFor UV X, by first computing Y = V Xand then UY ,\nthe FLOPs required are 2rsd for each step. Consequently,\nthe operation U(V X) necessitates a total of 4rsd FLOPs.\n• Memory Impact. For the W +UV configuration, a single\nparameter requires either 0.5 byte or 1 byte for 4-bit or\n8-bit representation, respectively. Therefore, for a 13b-\nmodel with 4-bit precision, the memory requirement is\n6.5GB. Incorporating an 8-dimensional 8-bit LoRC adds\na mere 0.006GB to this 6.5GB model.\nSignificantly, LoRC can be viewed as a supplementary\nfeature to existing quantization methodologies such as RTN,\nGPTQ, and ZeroQuant-Local/Global, and can be seamlessly\nintegrated with FGQ. We have conducted experiments to eval-\nuate the performance of LoRC on both OPT and BLOOM,\napplying 4-bit, 3-bit, and 2-bit weights by setting the acti-\nvation to FP16.5 Based on the discoveries in the preceding\nsections, we utilize the GPTQ quantization strategy. To gain\n5For INT8 Activation, the observation for FP16 holds similarly\nfor INT8 Activation.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19382\nPrecision block-size\n(W|A) OPT-6.7b OPT-13b OPT-30b OPT-66b BLM-1.7b BLM-3b BLM-7.1b BLM-176b\nW4A16 128\n| NA 12.10 11.28 10.74 10.44 20.92 17.90 15.17 10.94\nW4A8\nCase-1: per\n-row | per-row 13.17 13.07 14.65 16.32 21.43 18.39 15.58 11.19\nCase-2: per-row | 128 12.29 11.45 10.80 10.61 21.59 18.31 15.52 11.03\nCase-3: 128 | 128 12.04 11.31 10.75 10.45 21.27 17.86 15.19 10.96\nTable 6: OPT W4asym-A8 with block-sizes out of the best result from GPTQ, ZQ-Local, and ZQ-Global on OPT and BLOOM.\nBits LoRC Coarse-grained weight\nquantization (per-row block-size) Fine-grained quantization\non weight (256 block-size )\nOPT-6.7b\nOPT-13b OPT-30b OPT-66b BLM-176b OPT-6.7b\nOPT-13b OPT-30b OPT-66b BLM-176b\nW8A16 11.90 11.22\n10.70 10.33 10.90 11.90 11.22\n10.70 10.33 10.90\nW4A16 ✗ 12.28 11.42\n10.78 10.78 11.02 12.05 11.28\n10.74 10.50 10.95\n✓ 12.10 11.36\n10.76 10.34 10.98 11.99 11.29\n10.70 10.29 10.93\nW3A16 ✗ 14.18 12.43\n11.28 17.77 49.46 12.79 11.63\n10.9 11.34 11.13\n✓ 13.00 11.90\n11.14 10.63 11.30 12.40 11.57\n10.83 10.42 11.08\nW2A16 ✗ 120.56 40.17\n25.74 225.45 Explode 23.13 15.55\n12.68 308.49 12.64\n✓ 24.17 18.53\n14.39 13.01 14.15 16.27 14.30\n12.37 11.54 12.21\nTable 7: W#asym-A16 quantization with # being 4-bit, 3-bit and 2-bit on OPT and BLOOM (BLM).\nLoRC Coarse-grained weight quantization\nˆU, ˆV 6.7b 13b 30b 66b\nFP16 12.08 11.35 10.76 10.31\nINT8 12.10 11.36 10.76 10.34\nTable 8: Results of W4 asym A16 quantization with LoRC\napproximating ˆE = ˆU ˆV on OPT model family. ˆU and ˆV can\nbe represented with FP16 or INT8, of which the performance\nare represented below.\na comprehensive understanding of LoRC, we include the re-\nsults with and without the application of FGQ. The datasets\nand hyperparameters are consistent with those detailed in\nearlier sections.\nEvaluation Results. The findings are showcased in Ta-\nble 7, split into two sections: coarse-grained weight quan-\ntization (per-row) and fine-grained quantization (block-size\n256). Notably, we observe that the two low-rank matrices, ˆU\nand ˆV , can be quantized to 8-bit without any performance\ndiscrepancy (Table 8). Thus, the two low-rank matrices for\nLoRC in Table 7 are INT8 with m = 8.\nSeveral key observations can be made. Firstly, LoRC con-\nsistently boosts performance across all bit sizes and block\nsizes, as indicated by the lower perplexity scores when LoRC\nis activated. Secondly, the enhancement brought about by\nLoRC becomes more substantial as the bit size diminishes,\nespecially noticeable for W2A16, which displays a markedly\ngreater impact compared to W4A16 and W3A16 in most sce-\nnarios. Lastly, the combination of fine-grained quantization\nwith LoRC yields the most impressive results, underscoring\nthe efficacy of LoRC when integrated with FGQ. Notably,\nrecovering the last 0.05-0.1 perplexity can be challenging,\nLoRC-dim m OPT-1.3b OPT-6.7b OPT-30b\nm = 0basline 15.95 12.06 10.73\nm = 4 15.73 12.00 10.72\nm = 8 15.76 11.99 10.70\nm = 16 15.74 12.00 10.69\nTable 9: W4A16 quantization with LoRC by varying m.\nbut with LoRC, we are able to nearly recover the original\nmodel quality for INT4 quantization.\nAblation Study on the Low Rank Dimension m. An es-\nsential aspect of the LoRC method is on the optimal low-rank\ndimension, denoted as m, explained in Step II. To explore\nthis, we varied m in the range of 1, 4, 8, 16, and 32 for OPT-\n1.3b/6.7b/30b models, and applied W4A16 GPTQ quantiza-\ntion. The outcomes are depicted in Table 9, indicating that\nthe enhancements achieved through LoRC begin to plateau as\nthe dimension\nm surpasses 4. The most optimal performance\nfor OPT-6.7b is realized when m = 8.\nConclusion\nIn this work, we provide a comprehensive study of post-\ntraining quantization (PTQ) on large language models with\ndifferent PTQ methods (e.g., RTN, GPTQ, ZeroQuant), and\nwith different quantization coverage (weight-only and weight-\nand-activation quantization), etc. We find that PTQ methods\nare critical to improving the quantized model quality, and\nthat fine-grained quantization (FGQ) can bring acceptable\naccuracy and model size trade-off. Finally, we introduced\nan optimization technique called Low Rank Compensation\n(LoRC), which works synergistically with PTQ and FGQ,\nplaying a crucial role in enhancing full model quality recov-\nery with a minimal increase in model size.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19383\nReferences\nBai, H.; Zhang, W.; Hou, L.; Shang, L.; Jin, J.; Jiang,\nX.; Liu, Q.; Lyu, M.; and King, I. 2020. BinaryBERT:\nPushing the Limit of BERT Quantization. arXiv preprint\narXiv:2012.15701.\nBig-Science. 2022. Bloom Inference. https://github.com/h\nuggingface/transformers-bloom-inference/tree/main/bloom-\ninference-scripts.\nBondarenko, Y .; Nagel, M.; and Blankevoort, T. 2021. Un-\nderstanding and overcoming the challenges of efficient trans-\nformer quantization. arXiv preprint arXiv:2109.12948.\nBrown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language models are few-shot learners.arXiv\npreprint arXiv:2005.14165.\nCopilot. 2021. GitHub Copilot. https://github.com/features/\ncopilot/.\nDarvish Rouhani, B.; Lo, D.; Zhao, R.; Liu, M.; Fowers,\nJ.; Ovtcharov, K.; Vinogradsky, A.; Massengill, S.; Yang,\nL.; Bittner, R.; et al. 2020. Pushing the limits of narrow\nprecision inferencing at cloud scale with microsoft floating\npoint. Advances in neural information processing systems,\n33: 10271–10281.\nDettmers, T.; Lewis, M.; Belkada, Y .; and Zettlemoyer, L.\n2022. Llm. int8 (): 8-bit matrix multiplication for transform-\ners at scale. arXiv preprint arXiv:2208.07339.\nDettmers, T.; and Zettlemoyer, L. 2022. The case for 4-\nbit precision: k-bit Inference Scaling Laws. arXiv preprint\narXiv:2212.09720.\nEsser, S. K.; McKinstry, J. L.; Bablani, D.; Appuswamy, R.;\nand Modha, D. S. 2019. Learned step size quantization.arXiv\npreprint arXiv:1902.08153.\nFan, A.; Stock, P.; Graham, B.; Grave, E.; Gribonval, R.;\nJegou, H.; and Joulin, A. 2020. Training with quantization\nnoise for extreme fixed-point compression. arXiv preprint\narXiv:2004.07320.\nFrantar, E.; and Alistarh, D. 2022. Optimal Brain Compres-\nsion: A framework for accurate post-training quantization\nand pruning. arXiv preprint arXiv:2208.11580.\nFrantar, E.; Ashkboos, S.; Hoefler, T.; and Alistarh, D. 2022.\nGPTQ: Accurate Post-Training Quantization for Generative\nPre-trained Transformers. arXiv preprint arXiv:2210.17323.\nGholami, A.; Yao, Z.; Kim, S.; Mahoney, M. W.; and Keutzer,\nK. 2021. AI and Memory Wall. RiseLab Medium Post.\nHassibi, B.; and Stork, D. G. 1993. Second order derivatives\nfor network pruning: Optimal brain surgeon. In Advances in\nneural information processing systems, 164–171.\nHinton, G.; Vinyals, O.; and Dean, J. 2014. Distilling the\nknowledge in a neural network. Workshop paper in NIPS.\nJiao, X.; Yin, Y .; Shang, L.; Jiang, X.; Chen, X.; Li, L.; Wang,\nF.; and Liu, Q. 2019. Tinybert: Distilling bert for natural\nlanguage understanding. arXiv preprint arXiv:1909.10351.\nKim, S.; Gholami, A.; Yao, Z.; Mahoney, M. W.; and Keutzer,\nK. 2021. I-bert: Integer-only bert quantization. In Interna-\ntional conference on machine learning, 5506–5518. PMLR.\nLeCun, Y .; Denker, J. S.; and Solla, S. A. 1990. Optimal\nbrain damage. In Advances in neural information processing\nsystems, 598–605.\nMarcinkiewicz, M. A. 1994. Building a large annotated\ncorpus of English: The Penn Treebank.Using Large Corpora,\n273.\nMerity, S.; Xiong, C.; Bradbury, J.; and Socher, R. 2017.\nPointer sentinel mixture models. In International Conference\non Learning Representations.\nOpenAI. 2022. OpenAI Chatgpt. https://openai.com/blog/ch\natgpt/.\nPolino, A.; Pascanu, R.; and Alistarh, D. 2018. Model com-\npression via distillation and quantization. arXiv preprint\narXiv:1802.05668.\nPope, R.; Douglas, S.; Chowdhery, A.; Devlin, J.; Bradbury,\nJ.; Levskaya, A.; Heek, J.; Xiao, K.; Agrawal, S.; and Dean,\nJ. 2022. Efficiently Scaling Transformer Inference. arXiv\npreprint arXiv:2211.05102.\nRaffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;\nMatena, M.; Zhou, Y .; Li, W.; and Liu, P. J. 2019. Exploring\nthe Limits of Transfer Learning with a Unified Text-to-Text\nTransformer. arXiv:1910.10683.\nScao, T. L.; Fan, A.; Akiki, C.; Pavlick, E.; Ili´c, S.; Hesslow,\nD.; Castagné, R.; Luccioni, A. S.; Yvon, F.; Gallé, M.; et al.\n2022. BLOOM: A 176B-Parameter Open-Access Multilin-\ngual Language Model. arXiv preprint arXiv:2211.05100.\nShen, S.; Dong, Z.; Ye, J.; Ma, L.; Yao, Z.; Gholami, A.;\nMahoney, M. W.; and Keutzer, K. 2020. Q-BERT: Hessian\nBased Ultra Low Precision Quantization of BERT. In AAAI,\n8815–8821.\nSmith, S.; Patwary, M.; Norick, B.; LeGresley, P.; Rajbhan-\ndari, S.; Casper, J.; Liu, Z.; Prabhumoye, S.; Zerveas, G.;\nKorthikanti, V .; et al. 2022. Using deepspeed and megatron\nto train megatron-turing nlg 530b, a large-scale generative\nlanguage model. arXiv preprint arXiv:2201.11990.\nTao, C.; Hou, L.; Zhang, W.; Shang, L.; Jiang, X.; Liu, Q.;\nLuo, P.; and Wong, N. 2022. Compression of Generative Pre-\ntrained Language Models via Quantization. arXiv preprint\narXiv:2203.10705.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.;\nGomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. Attention\nis all you need. In Advances in neural information processing\nsystems, 5998–6008.\nWu, X.; Li, C.; Aminabadi, R. Y .; Yao, Z.; and He, Y . 2023.\nUnderstanding INT4 Quantization for Transformer Models:\nLatency Speedup, Composability, and Failure Cases. arXiv\npreprint arXiv:2301.12017.\nWu, X.; Yao, Z.; Zhang, M.; Li, C.; and He, Y . 2022. Extreme\ncompression for pre-trained transformers made simple and\nefficient. arXiv preprint arXiv:2206.01859.\nXiao, G.; Lin, J.; Seznec, M.; Demouth, J.; and Han, S.\n2022. SmoothQuant: Accurate and Efficient Post-Training\nQuantization for Large Language Models. arXiv preprint\narXiv:2211.10438.\nYao, Z.; Aminabadi, R. Y .; Zhang, M.; Wu, X.; Li, C.; and He,\nY . 2022. ZeroQuant: Efficient and Affordable Post-Training\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19384\nQuantization for Large-Scale Transformers. arXiv preprint\narXiv:2206.01861.\nZadeh, A. H.; Edo, I.; Awad, O. M.; and Moshovos, A. 2020.\nGobo: Quantizing attention-based nlp models for low la-\ntency and energy efficient inference. In 2020 53rd Annual\nIEEE/ACM International Symposium on Microarchitecture\n(MICRO), 811–824. IEEE.\nZafrir, O.; Boudoukh, G.; Izsak, P.; and Wasserblat, M.\n2019. Q8BERT: Quantized 8bit bert. arXiv preprint\narXiv:1910.06188.\nZeng, A.; Liu, X.; Du, Z.; Wang, Z.; Lai, H.; Ding, M.;\nYang, Z.; Xu, Y .; Zheng, W.; Xia, X.; et al. 2022. GLM-\n130B: An Open Bilingual Pre-trained Model. arXiv preprint\narXiv:2210.02414.\nZhang, S.; Roller, S.; Goyal, N.; Artetxe, M.; Chen, M.;\nChen, S.; Dewan, C.; Diab, M.; Li, X.; Lin, X. V .; et al. 2022.\nOpt: Open pre-trained transformer language models. arXiv\npreprint arXiv:2205.01068.\nZhang, W.; Hou, L.; Yin, Y .; Shang, L.; Chen, X.; Jiang, X.;\nand Liu, Q. 2020. Ternarybert: Distillation-aware ultra-low\nbit bert. arXiv preprint arXiv:2009.12812.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19385",
  "topic": "Quantization (signal processing)",
  "concepts": [
    {
      "name": "Quantization (signal processing)",
      "score": 0.5424033999443054
    },
    {
      "name": "Rank (graph theory)",
      "score": 0.47799959778785706
    },
    {
      "name": "Training (meteorology)",
      "score": 0.44922274351119995
    },
    {
      "name": "Psychology",
      "score": 0.4160096049308777
    },
    {
      "name": "Mathematics",
      "score": 0.21876993775367737
    },
    {
      "name": "Geography",
      "score": 0.19854509830474854
    },
    {
      "name": "Statistics",
      "score": 0.1920408010482788
    },
    {
      "name": "Meteorology",
      "score": 0.06876781582832336
    },
    {
      "name": "Combinatorics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210164937",
      "name": "Microsoft Research (United Kingdom)",
      "country": "GB"
    }
  ],
  "cited_by": 13
}