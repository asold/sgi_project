{
  "title": "Clinical and economic impact of a large language model in perioperative medicine: a randomized crossover trial",
  "url": "https://openalex.org/W4412535374",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5114135233",
      "name": "Yu He Ke",
      "affiliations": [
        "Duke-NUS Medical School",
        "SingHealth",
        "Singapore General Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A5112853080",
      "name": "Bernard Soon Yang Ong",
      "affiliations": [
        "Singapore General Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A5100544272",
      "name": "Liyuan Jin",
      "affiliations": [
        "SingHealth",
        "Singapore Eye Research Institute",
        "Singapore National Eye Center"
      ]
    },
    {
      "id": "https://openalex.org/A5064901765",
      "name": "Jacqueline Sim",
      "affiliations": [
        "Singapore General Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A5019744779",
      "name": "Chi Ho Chan",
      "affiliations": [
        "Singapore General Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A5112476395",
      "name": "Chai Rick Soh",
      "affiliations": [
        "Singapore General Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A5047011746",
      "name": "D. J. N. Wong",
      "affiliations": [
        "Duke-NUS Medical School",
        "Singapore General Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A5111262842",
      "name": "Nan Liu",
      "affiliations": [
        "Duke-NUS Medical School"
      ]
    },
    {
      "id": "https://openalex.org/A5041350668",
      "name": "Ban Leong Sng",
      "affiliations": [
        "KK Women's and Children's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A5032625133",
      "name": "Daniel Shu Wei Ting",
      "affiliations": [
        "Singapore National Eye Center"
      ]
    },
    {
      "id": "https://openalex.org/A5110265692",
      "name": "S. J. Yeo",
      "affiliations": [
        "Singapore General Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A5087685316",
      "name": "Marcus Eng Hock Ong",
      "affiliations": [
        "Duke-NUS Medical School",
        "SingHealth"
      ]
    },
    {
      "id": "https://openalex.org/A5019693984",
      "name": "Hairil Rizal Abdullah",
      "affiliations": [
        "Duke-NUS Medical School",
        "Singapore General Hospital"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2058432557",
    "https://openalex.org/W3167242683",
    "https://openalex.org/W4384561707",
    "https://openalex.org/W4391696507",
    "https://openalex.org/W4409157497",
    "https://openalex.org/W4384700226",
    "https://openalex.org/W4401794356",
    "https://openalex.org/W4395061548",
    "https://openalex.org/W2990085462",
    "https://openalex.org/W2905657479",
    "https://openalex.org/W4323835279",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W4399363701",
    "https://openalex.org/W4404136861",
    "https://openalex.org/W4404485589",
    "https://openalex.org/W4404783788",
    "https://openalex.org/W4391234325",
    "https://openalex.org/W2049617080",
    "https://openalex.org/W4304893240",
    "https://openalex.org/W4388823522",
    "https://openalex.org/W4400530541",
    "https://openalex.org/W4317780715",
    "https://openalex.org/W4403571350",
    "https://openalex.org/W4402876402",
    "https://openalex.org/W1791587663",
    "https://openalex.org/W4206006624",
    "https://openalex.org/W4405404913",
    "https://openalex.org/W2993961432",
    "https://openalex.org/W4386867830"
  ],
  "abstract": "Preoperative assessment is a critical but time-consuming component of perioperative care, often hindered by poor guideline adherence and high documentation burdens. This study evaluates the impact of PEACH (PErioperative AI CHatbot), an LLM-based clinical decision support system, on documentation efficiency, quality, user acceptance, and cost-effectiveness in preoperative consultations. PEACH did not significantly reduce overall documentation time in this randomized crossover trial involving resident physicians at Singapore General Hospital. However, subgroup analyses showed time savings for moderate-complexity patients (5.77 min, p = 0.010) and experienced physicians (4.6 min, p = 0.040). Evaluators preferred PEACH-assisted documentation in 57.1% of cases, with improved inclusion of issue lists (p = 0.05). Economic modeling projected annual institutional savings of SGD197,501 (USD146,297), with sensitivity analyses ranging from SGD 48,979 to 197,499 (USD36,280 to 146,295). These findings suggest that LLM-based tools like PEACH may enhance preoperative documentation efficiency and offer economic value.",
  "full_text": "npj |digital medicine Article\nPublished in partnership with Seoul National University Bundang Hospital\nhttps://doi.org/10.1038/s41746-025-01858-x\nClinical and economic impact of a large\nlanguage model in perioperative\nmedicine: a randomized crossover trial\nCheck for updates\nYu He Ke1,2,3 , Bernard Soon Yang Ong1,L i y u a nJ i n4,5, Jacqueline Xiu Ling Sim1, Chi Ho Chan1,\nChai Rick Soh1,D a n n yJ o nN i a nW o n g1,3, Nan Liu3,B a nL e o n gS n g6, Daniel Shu Wei Ting7,S uQ i a nY e o8,\nMarcus Eng Hock Ong3,9 &H a i r i lR i z a lA b d u l l a h1,2,3\nPreoperative assessment is a critical but time-consuming component of perioperative care, often\nhindered by poor guideline adherence and high documentation burdens. This study evaluates the\nimpact of PEACH (PErioperative AI CHatbot), an LLM-based clinical decision support system, on\ndocumentation efﬁciency, quality, user acceptance, and cost-effectiveness in preoperative\nconsultations. PEACH did not signiﬁcantly reduce overall documentation time in this randomized\ncrossover trial involving resident physicians at Singapore General Hospital. However, subgroup\nanalyses showed time savings for moderate-complexity patients (5.77 min,p = 0.010) and\nexperienced physicians (4.6 min,p = 0.040). Evaluators preferred PEACH-assisted documentation in\n57.1% of cases, with improved inclusion of issue lists (p = 0.05). Economic modeling projected annual\ninstitutional savings of SGD197,501 (USD146,297), with sensitivity analyses ranging from SGD 48,979\nto 197,499 (USD36,280 to 146,295). Theseﬁndings suggest that LLM-based tools like PEACH may\nenhance preoperative documentation efﬁciency and offer economic value.\nPreoperative assessment is a complex and high-stakes component of\nperioperative care. Errors during this phase of patient care— such as\ndeviationsfromclinicalguidelinesor incorrectperioperativeinstructions\n— can lead to same-day surgical cancellations, delayed treatments, and\nincreased perioperative morbidity 1. These disruptions compromise\npatient safety and generate signiﬁcant operational inefﬁciencies, with\noperatingroom delays estimatedtocost betweenUSD 1400 and1700 per\nh\n2. Despite ongoing efforts to streamline these workﬂows, major barriers\npersist: poor adherence to extensive and evolving guidelines and the\nsubstantial documentation burden that consumes over 40% of\nclinician time.\nRecent advances in artiﬁcial intelligence (AI) and large language\nmodels (LLMs) offer a promising opportunity to address these longstanding\nchallenges. LLMs have demonstrated impressive performance across clin-\nical reasoning tasks and have shown potential to interpret domain-speciﬁc\nguidelines and support complex decision-making\n3,4.P r i o rw o r kh a s\ndemonstrated that LLMs coupled with retrieval-augmented generation\n(RAG) can accurately assess surgicalﬁtness and formulate management\nplans, achieving high accuracy in structured simulation settings5.\nDespite the rapid evolution of AI capabilities, there is a critical gap\nbetween experimental validation and operational implementation. Most\nstudies to date have focused on benchmark testing, retrospective data, or\nsynthetic vignettes\n6–8. Few have examined how LLMs perform in real-time\nclinical workﬂows or whether they can meaningfully inﬂuence doc-\numentation quality and clinician productivity in live patient care. In addi-\ntion, their real-world clinical impact— on efﬁciency, clinician behavior, and\ncost-effectiveness— remains largely unquantiﬁed.\nFurthermore, the translation of LLMs into clinical practice is not\nwithout its challenges. Chief among these are data privacy, model reliability,\nand contextualﬁdelity. Adversarial prompts have been shown to extract\nsensitive information from foundation models9, and even anonymized\npatient data remains vulnerable to reidentiﬁcation10.M o r e o v e r ,L L Mo u t -\nputs may struggle to align with nuanced, patient-speciﬁc clinical contexts,\nraising the so-called“last-mile problem” in medical AI deployment11.\n1Department of Anesthesiology, Singapore General Hospital, Singapore, Singapore.2Data Science and Artiﬁcial Intelligence Lab, Singapore General Hospital,\nSingapore, Singapore.3Duke-NUS Medical School, Singapore, Singapore.4Singapore National Eye Centre, Singapore Eye Research Institute,\nSingapore, Singapore.5Singapore Health Services, Artiﬁcial Intelligence Ofﬁce, Singapore, Singapore.6Department of Women’s Anaesthesia, KK Women’s and\nChildren’s Hospital, Singapore, Singapore.7Singapore National Eye Centre, Singapore, Singapore.8Process Transformation & Improvement, Singapore General\nHospital, Singapore, Singapore.9Prehospital Emergency Research Center, Health Services and Systems Research, Duke NUS Medical School,\nSingapore, Singapore. e-mail: ke.yuhe@singhealth.com.sg\nnpj Digital Medicine|           (2025) 8:462 1\n1234567890():,;\n1234567890():,;\nTo address these challenges, we utilized the PAIR platform12— a\nsecure, government-certi ﬁed infrastructure developed under Singa-\npore’sSmartNationinitiative — tobuildaclinicaldecisionsupporttoolfor\nperioperative medicine. Within PAIR Chat, we developed the PErio-\nperative AI CHatbot (PEACH), a clinical decision support system\ndesigned to streamline and standar dize preoperative assessments.\nPEACH integrates 35 institution-speciﬁc perioperative guidelines into a\nuniﬁed knowledge base, enabling it to perform longitudinal reasoning\nacrossvariousclinicalpathwaysandadaptdynamicallytopatient-speci ﬁc\ncontexts.Beforedeployment, PEACHwasreviewedandapprovedbythe\ninstitution’s Medical Board and underwent a formal risk assessment\nthrough the Clinical Risk Management (CRM) committee to ensure\nsafety, compliance, and alignment with institutional standards. The\nsystemwasapprovedbySingapore ’sHealthSciencesAuthority(HSA)asa\nClassAClinicalDecisionSupportSystem(CDSS)\n13 inDecember2024and\nhas since been deployed in routine use at the preoperative assessment\nclinic.\nThis study addresses a critical gap in the existing literature: the extent to\nwhich LLM-powered chatbots can inﬂuence real-world clinical workﬂows,\nwith a particular focus on clinical efﬁciency and cost-effectiveness. To\nevaluate the utility of such a system,we conducted a randomized crossover\ntrial at a tertiary academic medical center. The primary objective was to\nassess the impact of the LLM-based tool on clinical efﬁciency, as measured\nby documentation time. Secondary outcomes included assessments of\ndocumentation quality, accuracy and safety, user acceptability, and insti-\ntutional economic outcomes. To our knowledge, this investigation repre-\nsents theﬁrst prospective, real-world evaluation of an LLM-enabled clinical\ndecision support system in perioperative medicine.\nResults\nA total of 272 patient encounters were recorded during the study period,\nwith 135 encounters on standard workﬂow days (without PEACH) and 137\non intervention days (with PEACH). PEACH was actively used in 111 of 137\nintervention-day cases, yielding the utilization rate of 81.0%. Fourteen out of\n16 eligible physicians participated in the study, with one declining to par-\nticipate and one other unavailable due to emergency medical leave. A total of\n161 controls and 111 interventions were included in theﬁnal analysis. A\nCONSORT diagram depicting participantﬂow is shown in Fig.1.P E A C H\ndelivered outputs within 10–1 5so na v e r a g e .\nThe cases where PEACH was utilizedexhibited a higher proportion of\nhigh-complexity (Complexity 3) cases (25.2% vs. 13.7%,p = 0.007) and had\na greater share of American Society of Anesthesiologists (ASA) Physical\nStatus classiﬁcation 3 patients (48.6% vs. 34.2%, p = 0.050). The distribution\nof surgery types did not signiﬁcantly differ, utilization rate by different\nseniority of resident physicians, and consultation rates with attending\nanesthesiologists were similar (p > 0.50) (Table1).\nIn this study,“total consultation time” refers to the entire duration\nfrom the patient’s entry into the consultation room to the completion of\ndocumentation in the electronic health record, encompassing both face-\nto-face interaction and subsequent note writing.“Documentation time” is\ndeﬁned as the portion of that period spentsolely on completing the clinical\nnote before and after the patient interaction ended.\nThe total consultation time did not signiﬁcantly differ between groups\n(40.04 vs. 40.66 min,p = 0.787). However, documentation time trended\nlower with PEACH (17.53 vs. 19.35 min), though this was not statistically\nsigniﬁcant (p = 0.192). By case complexity, PEACH was associated with\nsigniﬁcantly reduced documentation time in Complexity 2 cases by a mean\ntime difference of 5.77 min per patient (p = 0.010) (Fig.2). Stratifying by\nclinician experience, signiﬁcant documentation time savings were observed\namong experienced resident physicians (20.0 vs. 24.6 min,p = 0.040), while\nno signiﬁcant differences were seen among novice or new-to-institution\nparticipants (Table2).\nHuman evaluators preferred PEACH-assisted documentation in\n57.1% of cases compared to 35.7% for control, although this was not sta-\ntistically signiﬁcant (p = 0.180). Inter-rater agreement was substantial, with\naC o h e n’s κ of 0.71. PEACH outputs were more likely to include an issues list\n(71.4% vs. 43.9%, p = 0.05), while minor and major error rates were low and\nsimilar across groups (Table3).\nAcross 168 PEACH interactions (in the 111 clinical cases seen), most\ncases involved a single interaction (71.2%), with multi-step use in 28.8%.\nOutputs were primarily summaries and management plans (82.7%), fol-\nlowed by Q&A responses (11.9%) and referral drafting (5.4%).\nUser feedback was positive, with high average scores for safety (4.94),\nexplainability (4.81), and ease of understanding (4.72). Usefulness (4.23),\nFig. 1 | Design and outcomes of a randomized cross-over trial evaluating\nPEACH-assisted versus standard patient documentation.Recruitment and trial\ndesign for evaluation of PEACH-assisted documentation. Sixteen eligible resident\nphysicians were screened, of whom fourteen were recruited. Participants were\nstratiﬁed by experience (novice, new to institution, experienced) and underwent a\nrandomized cross-over trial over two consecutive days, alternating between standard\nclinical care (without PEACH) and access to PEACH to assist with clinical care and\ndocumentations (with PEACH). In total, 272 patient encounters were recorded: 135\nduring without PEACH days and 137 during with PEACH days. Among encounters\nduring with PEACH days, PEACH was utilized in 111 out of 137 cases (81.0%). The\nremaining 26 encounters, in which PEACH was available but not used, were ana-\nlyzed as part of the non-PEACH group. Total documentation records comprised 161\nwithout PEACH and 111 with PEACH.\nhttps://doi.org/10.1038/s41746-025-01858-x Article\nnpj Digital Medicine|           (2025) 8:462 2\njob facilitation (4.62), and intention to use in the future (4.54) were also\nfavorably rated. Output quality was consistently high, with 100% judged\naccurate by human reviewers (n = 30). Minor clinical deviations that would\nnot result in patient harm were found in 3 outputs (10.0%), and no hallu-\ncinations were observed (Supplementary Table 1).\nUsage of the PEACH chatbot varied across resident physicians in both\nfrequency and intensity. Among the 14 participants, each resident saw a\nmedian of 8 patients on PEACH intervention days (range: 7–20), with a\ncorresponding usage rate per resident ranging from 36.4% to 100%. Several\nresidents consistently used PEACH for nearly all eligible encounters and\nengaged in multi-message sessions. Notably, two residents accounted for a\ndisproportionate share of total chatbot interactions, contributing over 25%\nof all PEACH messages logged. A detailed breakdown of patient volumes,\nchatbot usage, and message intensityby resident is provided in Supple-\nmentary Table 2.\nHealth economic analysis projected that PEACH implementation\ncould yield substantial institutional cost savings, primarily through reduc-\ntions in clinician documentation time. Based on observed time reductions,\nPEACH is projected to save approximately 1091.4 resident clinician hours\nand 59 attending physician hours annually, corresponding to an estimated\ncost savings of SGD 197,501 (USD 146,297).\nSensitivity analyses, incorporating variations in adoption rates, time\nsavings, token pricing, and IT maintenance costs, indicate that net annual\nsavings could range from SGD 48,97951,506 to 190,41897,499\n(USD36,2808,153 to 146,2951,050). Importantly, even under conservative\nassumptions, PEACH remained cost-effective. A summary of these sce-\nnarios is presented in Table4, with detailed calculations available in Sup-\nplementary Table 3.\nDiscussion\nTo our knowledge, this randomized,crossover trial represents theﬁrst\nprospective real-world evaluation of a large language model-powered\nclinical decision support tool inperioperative medicine. Ourﬁndings\ndemonstrate that PEACH, a secure, context-aware specialized chatbot, can\nbe feasibly deployed in a high-volume clinical setting and may offer\nimprovements in documentation efﬁciency and cost savings, particularly in\ncertain subgroups. These results extend the growing evidence supporting\nLLM integration into clinical workﬂows and provide a practical blueprint\nfor AI-enabled perioperative care.\nAlthough the overall reduction in documentation time across all cases\ndid not reach statistical signiﬁcance, subgroup analyses revealed signiﬁcant\ngains in intermediate-complexity encounters (Complexity 2) and among\nexperienced clinicians. Theseﬁndings suggest that PEACH’sb e n eﬁts are\nmost pronounced when clinical complexity is sufﬁcient to require decision\nsupport but not so overwhelming as to necessitate extensive human delib-\neration. Straightforward cases (Complexity 1) likely afforded little room for\nimprovement, while highly complex cases (Complexity 3) may have\ninvolved nuanced decision-making beyond the current capabilities of LLM\nsummarization\n14.\nFrom a systems perspective, even modest per-case time savings scaled\nto substantial institutional cost reductions, suggesting that AI-assisted\nworkﬂows can be not only clinically feasible but also economically bene-\nﬁcial. Most prior studies of LLMs haveemphasized technical accuracy or\nsimulation-based outcomes6,15,16, and there has been limited real-world\neconomic evidence of LLM use in operational clinical environments. Recent\nwork by Klang et al. highlights the potential for cost-effective scaling of\nLLMs within health systems, emphasizing signiﬁcant cost reductions\nthrough task concatenation and optimization strategies\n17.O u rﬁndings\nTable 1 | Comparison of case and documentation\ncharacteristics between control (Without PEACH) and\nIntervention (With PEACH) Groups\nWithout\nPEACH\n(n = 161)\nWith\nPEACH\n(n = 111)\nTotal\n(n = 272)\np\nvalue*\nCase Complexity 0.007\n- Complexity 1 83 (51.6%) 38 (34.2%) 121 (44.5%)\n- Complexity 2 56 (34.8%) 45 (40.5%) 101 (37.1%)\n- Complexity 3 22 (13.7%) 28 (25.2%) 50 (18.4%)\nASA Physical Status 0.050\n- ASA 1 7 (4.3%) 1 (0.9%) 8 (2.9%)\n- ASA 2 99 (61.5%) 56 (50.5%) 155 (57.0%)\n- ASA 3 55 (34.2%) 54 (48.6%) 109 (40.1%)\nType of Surgery 0.140\n- General\nSurgery\n62 (38.5%) 38 (34.2%) 100 (36.8%)\n- Orthopedics 51 (31.7%) 44 (39.6%) 95 (34.9%)\n- Urology 21 (13.0%) 18 (16.2%) 39 (14.3%)\n- OBGYN 11 (6.8%) 8 (7.2%) 19 (7.0%)\n- Others 16 (9.9%) 3 (2.7%) 19 (7.0%)\nExperience Group 0.096\n- Novice 58 (36.0%) 38 (34.2%) 96 (35.3%)\n- New to\ninstitution\n52 (32.3%) 25 (22.5%) 77 (28.3%)\n- Experienced 51 (31.7%) 48 (43.2%) 99 (36.4%)\nConsultation with attending anesthesiologist required 1.000\n- Complexity 1 3 / 83 (3.6%) 0 / 38 (0.0%)\n- Complexity 2 6 / 56 (10.7%) 4 / 45 (8.9%)\n- Complexity 3 7 / 22 (31.8%) 7 / 28 (25.0%)\nNovice: Resident physicians with less than 6 months of anesthesia experience.\nNew to Institution: Resident physicians with at least 6 months of previous anesthesia experience but\nnone within the current institution.\nExperienced: Resident physicians with at least 6 months of anesthesia experience within the current\nrotation.\nPEACH PErioperative AI CHatbot,ASA American Society of Anesthesiologists Physical Status\nClassiﬁcation, OBGYN obstetrics and gynecology.\n*Chi-square tests and Fisher’s exact test.\nFig. 2 | Mean documentation time and 95% conﬁdence intervals across case\ncomplexity levels, comparing documentation with and without PEACH.Mean\ndocumentation time (minutes) with 95% conﬁdence intervals across three case\ncomplexity levels. Documentation times are stratiﬁed by PEACH-assisted (orange)\nand standard (yellow) documentation days. Higher complexity cases were associated\nwith longer documentation times, with consistently lower times observed during\nPEACH-assisted documentation. This was statistically signiﬁcant for complexity 2\ncases (p = 0.010).\nhttps://doi.org/10.1038/s41746-025-01858-x Article\nnpj Digital Medicine|           (2025) 8:462 3\nbuild upon this framework, demonstrating that LLM-assisted workﬂows\ncan be both clinically feasible andeconomically sustainable at scale.\nWhile our current implementationuses a long-context prompting\napproach— this method incurs higher token usage and computational cost\ncompared to the RAG frameworks18. In our prior work using RAG-based\nLLMs for binary perioperative decision tasks, accuracy ranged from 43.0%\nto 90.0% depending on the model, revealing substantial variability and\nlimited reliability for complex, multi-layered clinical decisions\n5. In contrast,\nthe long-context architecture deployed in this study allowed for more robust\nlongitudinal reasoning across diverse guideline domains,supporting higher\nﬁdelity in perioperative assessment andplanning. Despite its higher cost,\nour economic analysis shows that this approach remains cost-effective in\npractice, reinforcing the value of long-context prompting for high-stakes,\ndomain-speciﬁc clinical applications.\nNevertheless, the limited overall impact observed points to several\nchallenges. Uptake and usage variability suggest that successful deployment\nof LLMs in healthcare requires more than technical excellence; it demands\nattention to psychological, organizational, and cultural factors that shape\ntrust and acceptance\n19–21. Facilitating conditions, visible leadership support,\nstructured training, and peer normalization will be crucial to scaling AI-\nenabled workﬂows. Future iterations should focus on embedding PEACH\ninto native EHR systems, streamlining user interfaces, and providing real-\ntime support to optimize facilitating conditions.\nSeveral limitations warrant consideration. The open-label nature of the\ntrial introduces potential for performance and selection bias. Outcomes\nwere limited to documentation time and quality; future research should\nevaluate patient-centered safety endpoints and qualitative evaluation of user\nacceptance. The subgroup analyses were not adjusted for multiple com-\nparisons, and theﬁndings should be interpreted with caution as hypothesis-\ngenerating rather than conﬁrmatory. Furthermore, while Claude 3.5 Sonnet\ndemonstrated strong contextual performance, model-speciﬁc behaviors\nmay vary, and generalizability to other settings or LLMs remains to be\ndetermined\n22,23.\nImportantly, we acknowledge the possibility of behavioral changes\nintroduced by the study setting. Participants may have modiﬁed their\nworkﬂow consciously or subconsciouslywhen using PEACH, potentially\naffecting efﬁciency or documentation behavior. Additionally, despite\nblinding efforts, subtle stylistic differences may still have inﬂuenced eva-\nluator perception.\nLastly, we did not initiate formal onboarding or structured training\nfor PEACH, based on the assumption that its interface would be sufﬁ-\nciently intuitive for clinical users. However, retrospective review of\nparticipant interactions revealed suboptimal utilization of PEACH’sf u l l\ncapabilities— such as highlighting key clinical issues and automated\nsummarization— resulting in continued manual data entry for several\ntasks. This ﬁnding highlights the critical importance of formative\ntraining to optimize adoption and realize the full beneﬁts of AI-driven\nmedical tools. Importantly, as AI applications in clinical practice are still\nemerging, best practices for training users to interact effectively with\nthese technologies are not yet well established. Future implementations\nshould not only incorporate structured user education and iterative\nfeedback, but future research should also focus on developing and\nreﬁning training strategies to maximize the beneﬁts of AI integration in\nhealthcare.\nIn conclusion, PEACH represents a promising example of an effective\nLLM integration into perioperative medicine. By leveraging secure infra-\nstructure, domain-speciﬁc guideline ingestion, and adaptive prompting\nstrategies, PEACH demonstrated potential improvements in documenta-\ntion quality and efﬁciency in exploratory analyses, while maintaining high\nsafety standards and favorable user acceptance. Our results underscore the\nimportance of real-world validation, clinician-centered design, and eco-\nnomic evaluation in guiding responsible adoption of artiﬁcial intelligence in\nmedicine.\nMethods\nStudy design\nWe conducted a prospective, randomized crossover trial to evaluate the\nreal-world clinical impact of PEACH in a high-volume preoperative eva-\nluation clinic (PEC). The trial was conducted at Singapore General Hos-\npital, a 1900-bedded tertiary academic hospital in Singapore between\nJanuary and February 2025\n24. The institutional review board waived ethics\napprovalastheintervention wasclassiﬁedasnon-human-subjectsresearch\nunder local governance policies.\nTable 2 | Time efﬁciency outcomes by PEACH utilization, case complexity, and clinician experience\nWithout PEACH (Min) With PEACH (Min) Time Difference (95% CI) (Min) p value*\nOverall time 40.66 ± 16.12 40.04 ± 20.26 −0.62 (−5.14, 3.89) 0.787\nDocumentation time 19.35 ± 10.95 17.53 ± 11.45 −1.82 (−4.54, 0.90) 0.192\nPatient time 21.31 ± 10.22 22.50 ± 14.67 +1.19 (−1.96, 4.35) 0.459\nBy Complexity:\n- Complexity 1 18.7 ± 5.2 16.2 ± 4.8 −2.51 (−6.07, 1.05) 0.164\n- Complexity 2 25.4 ± 7.6 19.6 ± 6.3 −5.77 (−10.14, −1.41) 0.010\n- Complexity 3 31.8 ± 9.4 31.7 ± 8.9 −0.09 (−7.64, 7.45) 0.980\nBy Experience:\n- Novice 19.5 ± 5.8 18.3 ± 5.1 −1.21 (−5.53, 3.11) 0.580\n- New to institution 23.8 ± 6.7 25.7 ± 7.2 +1.88 (−4.36, 8.11) 0.546\n- Experienced 24.6 ± 7.9 20.0 ± 6.5 −4.62 (−9.02, −0.23) 0.040\nMean ± standard deviation (SD).\nNegative time differences indicate time savings with PEACH (intervention group).\n*Two-sample independentt-tests.\nTable 3 | Human evaluation of clinical documentation with and\nwithout PEACH\nMetric Without\nPEACH (n = 28)\nWith\nPEACH (n = 28)\np valuea\nPreference Rate 10/28 (35.7%) 16/28 (57.1%) 0.180 b\nIssues List\nPresent\n12 / 28 (43.9%) 20/28 (71.4%) 0.050\nMinor Errors 3/28 (10.7%) 2/28 (7.1%) 1.000\nMajor Errors 1/28 (3.6%) 0/28 (0.0%) 1.000\naFisher’s Exact test.\nbCohen’s Keppaκ = 0.71.\nhttps://doi.org/10.1038/s41746-025-01858-x Article\nnpj Digital Medicine|           (2025) 8:462 4\nPrompt engineering\nPEACH was developed within the PAIR Chat platform, a large language\nmodel optimized for extended-context inputs25,26.P A I Ri sh o s t e do nt h e\nsecure Government Commercial Cloud (GCC) and is designed to support\nhealthcare AI applications managing data classiﬁed as “Restricted” or\n“Sensitive,” ensuring enterprise-grade data security. PAIR Chat leverages\nClaude 3.5 Sonnet (Anthropic, San Francisco, CA)\n27.S t r u c t u r e dp e r i o -\nperative guidelines (Supplementary Table 4) were input directly into the\nmodel using document stafﬁng\n28, which enables comprehensive reasoning\nacross full documents without the need for retrieval modules. This approach\nallows the model to synthesize and apply context across extensive clinical\ndocumentation.\nPrompt engineering was a critical component of chatbot design and\nfollowed best practices\n29. Prompts were constructed using role-based\ninstructions, specifying the chatbot’s persona (e.g., senior perioperative\nclinician or health system analyst). Prompts were iteratively reﬁned\nthrough internal testing, response evaluation, and user feedback. This\nprocess reduced hallucinations, improved clarity, and aligned outputs\nwith domain-speciﬁc expectations. Final prompts emphasized task spe-\nciﬁcity, contextual relevance, and clinical or administrative nuance\n(see Supplementary Table 5). PEACH was designed to assist with speciﬁc\ntasks: 1) Question and Answer (Q&A), 2) Summarization and making\nperioperative plans, and 3) Writing referral letters.\nParticipants and settings\nThe trial employed a prospective, two-day randomized crossover design.\nResident physicians rotating through the anesthesiology department and\nscheduled for at least two consecutive days in the preoperative evaluation\nclinic (PEC) during January and February 2025 were enrolled. A washout\nperiod was not implemented due to feasibility constraints in our local\ncontext: residents are typically assigned to the PEC for only 2–4d a y sp e r\nrotation, with their next assignment occurring 1–2 months later. Introdu-\ncing a washout period would risk confounding by capturing residents at\ndifferent stages of clinical experience, thereby introducing variability in\ndocumentation efﬁciency.\nParticipants varied in their prior exposure to institutional perioperative\nprotocols and the electronic health record (EHR) system. The PEACH\nsystem had been available for informaluse starting one month before study\ninitiation. At our institution, anesthesiology residents rotate every six\nmonths, with a new cohort beginning in January. While some residents\nfrom the prior cohort had limited exposure to PEACH, no formal training\nwas provided before the study. Uponenrollment, participants received a\nbrief ﬁve-minute orientation covering basic access, login, and usage. No\nextensive onboarding was conducted, as the chatbot was designed to be\nintuitive and self-directed.\nEach participant was randomized tocomplete two consecutive clinic\nsessions— one utilizing PEACH (Intervention) and the other following\nstandard workﬂows without AI assistance (Control). Randomization was\nconducted using a 1:1 block design and stratiﬁed according to participants’\nprior anesthesiology and institutional experience. Participants were cate-\ngorized into three strata: (1) less than 6 months of anesthesiology experience\n(Novice); (2) physicians with at least 6months of anesthesiology experience\nat other institutions but unfamiliar with the local EHR system (New to\ninstitution); and (3) experienced resident physicians who had previously\ncompleted at least six months of anesthesiology rotations within the study\nhospital (Experienced). The participants were given a 10-dollar gift voucher\nfor their participation. Given the nature of the intervention, blinding of\nparticipants was not feasible, however, the reviewers were blinded.\nTo reduce the risk of reviewer bias, documentation samples were\nanonymized and selected to ensure parity across case complexity. Clinical\ndocumentation was extracted directly from the electronic health record\nwithout reformatting or alteration, preserving the original physician-\nauthored content. While AI-generatedt e x tm a ye x h i b i ts u b t l es t y l i s t i c\npatterns, reviewers were not primed to expect such differences.\nDuring the PEACH day (Intervention), participants accessed PEACH\nthrough a secure hospital interface. The extent and manner of PEACH usage\nwere left to the discretion of each participant, who may elect to use the\nchatbot more frequently or selectively. All interactions with PEACH—\nincluding the number of uses and their speciﬁc purposes (e.g., summariza-\ntion and planning, Q&A, or referral drafting)— were recorded and extracted\nfrom the system logs. The study teamsubsequently randomly selected 30\nPEACH outputs for the evaluation of accuracy. In the control arm, parti-\ncipants conducted preoperative assessments and completed documentation\nusing standard clinical procedures without access to AI assistance (Fig.1).\nFor each patient encounter, three time metrics were recorded based on\ntimestamps from the clinic queuing system and documentation logs within\nthe electronic health record.“Patient time”was deﬁned as the duration from\nthe patient’s entry to exit from the consultation room.“Total time”refers to\nthe entire period required for case review and documentation.“Doc-\numentation time\n” was calculated as the difference between total time and\npatient time.\nClinical documentation was extracted from the perioperative clinic\nrecords and anonymized for blinded evaluation. To account for variability\nTable 4 | Economic impact of PEACH Implementation\nCategory Parameter Calculations / USD\nResident physician Clinical Time Savings\nTotal cases seen per year 20,000 Total patients seen by\nphysicians/year\nTotal time saved/year 1091.4 h Complexity (1: 8900, 2: 7420, 3:\n3680) patients.\nTime saved for (1: 2.51, 2: 5.77, 3:\n0.09) minutes.\nAttending physician Clinical Time Savings\nTotal consultations saved/year 708\nconsultations\nComplexity (1: 322, 2: 135, 3: 251)\nTotal time saved/year 59\nattending\nhours\nAverage: 5 min each consultation\nLabour Cost Savings\nLabor savings (Resident) SGD 152,796 Based on SGD 140/hr\nLabor savings (Attending) SGD 11,800 Based on SGD 200/hr\nOverhead costs (Total) SGD 32,919 20% Overhead costs\n(Administrative and operational)\nTotal SGD 197,515\nLLM Costs (Claude 3.5 Sonet)\nNumber of PEACH Outputs/\npatient\n1.51 168 outputs ÷ 111 patients\nTotal annual outputs 30,200 20,000 × 1.51\nCost/output SGD 0.00047 133 tokens × SGD 3.5/million\nAnnual LLM cost SGD 14.19 30,200 × SGD 0.00047\nNet Annual Savings SGD 197,501 USD 146,297\nSensitivity analysis\n10% higher output volume SGD 197,499 USD 146,295\n20% lower time savings SGD 164,582 USD 121,912\n50% adoption rate SGD 98,750 USD 73,148\n25% reduced case volume SGD 148,125 USD 109,722\nDedicated IT Maintenance of\n30,000 SGD per year\nSGD 167,501 USD 124,074\nWorst-Case Scenario\na SGD 48,979 USD 36,280\nAll costs in 2024 SGD. Labor savings based on $140/h clinician wage for resident physicians and\n$200/h for attending physicians (Singapore public sector).\nLLM costs assume Claude 3.5 Sonnet at $3.50/million tokens (100-word output= 133 tokens).\nExcludes government-covered operational overhead and potential revenue from increased patient\nthroughput. Exchange rate USD: SGD; 1:1.35.\nFTE full-time Equivalent,LLM large language model.\naWorst-Case Scenario: 50% adoption, 20% reduced time savings,+50% increase in token price, IT\nMaintenance cost of 30,000 SGD annually.\nhttps://doi.org/10.1038/s41746-025-01858-x Article\nnpj Digital Medicine|           (2025) 8:462 5\nin administrative burden based on case complexity, all cases were stratiﬁed\ninto three predeﬁned categories for analysis according to the plans outlined\nin the documentation. Complexity 1 included ASA I or II patients who\nrequired no further clinical interventions. Complexity 2 encompassed cases\nnecessitating minor actions, such as brief consultations with the attending\nanesthesiologist. Complexity 3 referred to high-acuity cases requiring\nmultidisciplinary coordination, including communication with multiple\nspecialty teams, additional referrals, or changes to the surgical schedule (e.g.,\ncancellations or rescheduling). Full inclusion and exclusion criteria for each\ncategory are detailed in Supplementary Table 6. Complexity assignments\nwere made retrospectively by two anesthesiologists on the study team, based\non the clinical documentation. The reviewers were blinded to the study arm,\nand discrepancies were resolved by consensus.\nOutcome measures\nThe primary outcome was clinical efﬁciency, measured by differences in\ndocumentation time per patient between PEACH-assisted and standard\nworkﬂows. Secondary outcomes included documentation quality, safety\nand accuracy, user acceptance, and economic impact.\nDocumentation quality was assessed using a paired-review design.\nFifty-six clinical documents (28 pairs) were collected. These cases were\nmatched by case complexity to ensure comparability. All samples were\nanonymized and independently reviewed by two anesthetists. Reviewers\nevaluated each documentation pair for the presence of a clinically relevant\nissues list and identiﬁed any minor and major errors in the perioperative\nmanagement plan. Additionally, reviewers indicated their overall preference\nb e t w e e nt h et w oo u t p u t si ne a c hp a i r .\nUser acceptance was evaluated using a structured questionnaire based\non the Davis’Technology Acceptance Model (TAM)\n30, administered upon\nstudy completion. The survey included 5-point Likert-scale items assessing\nperceived usefulness, ease of use, clarity of reasoning, and intention to use\nPEACH in future clinical practice.\nEconomic analysis\nAn economic evaluation was conducted following the Consolidated Health\nEconomic Evaluation Reporting Standards (CHEERS) guidelines\n31 to\nquantify the institutional value of PEACH implementation. The analysis\nmodeled potential cost savings based on observed time reductions in doc-\numentation and consultation, extrapolated to an institutional scale,\nassuming an annual patient volume of20,000 preoperative assessments. We\nassumed that the distribution of physician seniority mirrored that observed\nin the study population.\nA simple costing model was used to estimate potential labor cost\nsavings by converting clinician timesaved into full-time equivalent (FTE)\nhours and multiplying by an estimated wage rate of SGD 140 per hour.\nConcurrently, the operating cost of the LLM (Claude 3.5 Sonnet, Anthropic,\nSan Francisco, CA) was factored in based on token usage and per-token\npricing at the time of analysis (SGD 0.00047 per output, assuming 133\ntokens per 100 words). The model considered the average number of\nPEACH outputs generated per patient and projected annual usage volume.\nTo test the robustness of the model, a sensitivity analysis was performed\nusing ±20% variation in key input parameters, including clinician wage\nrates, output volume per patient, and token pricing.\nSample size calculation\nAs there were no prior studies evaluating the impact of a clinical decision\nsupport tool on documentation efﬁciency in preoperative settings, we\nprospectively planned an interim analysis after the enrollment of 12 parti-\ncipants to inform sample size estimation. Before initiating the study, we\ndeﬁned a clinically meaningful reduction in documentation time of 3 min,\nrepresenting a 30% improvement relative to an estimated average doc-\numentation time of 10 min per patient. Assuming a standard deviation of\n10 min in documentation time differences, a paired crossover design, a two-\nsidedα of 0.05, and 80% power, an a priori sample size calculation indicated\nthat 88 paired patient encounters would be required to detect this effect.\nAt interim analysis, the observed standard deviation of documentation\ntime was consistent with this assumption. Additionally, it was found that\neach participant completed an average of 8 paired patient encounters\nper session. Based on this, the study proceeded to full enrollment, ultimately\nincluding 14 participants (111 paired patient encounters).\nAll statistical analyses were performed using Python 3.13.2. Paired\nt-tests were used to compare continuous time metrics between intervention\nand control arms, while categorical outcomes such as documentation pre-\nference were evaluated using Chi-square tests.\nDuring manuscript preparation, the authors used large language\nmodels (ChatGPT-4o and DeepSeek) to assist with technical paraphrasing\nand grammatical reﬁnement. All AI-assisted content was reviewed for sci-\nentiﬁc accuracy and veriﬁed against original data by the study authors.\nData availability\nThe minimal dataset necessary to interpret, replicate, and build upon the\nﬁndings of this study is available from the corresponding author upon\nreasonable request. All data have been de-identiﬁed in accordance with\ninstitutional policies and applicable privacy regulations. Due to institutional\nand ethical restrictions, the dataset is not publicly available but can be shared\nupon request for academic and non-commercial purposes, subject to data-\nsharing agreements.\nReceived: 27 April 2025; Accepted: 1 July 2025;\nReferences\n1. Haana, V., Sethuraman, K., Stephens, L., Rosen, H. & Meara, J. G.\nCase cancellations on the day of surgery: an investigation in an\nAustralian paediatric hospital.ANZ J. Surg.79, 636–640 (2009).\n2. Koushan, M., Wood, L. C. & Greatbanks, R. Evaluating factors associated\nwith the cancellation and delay of elective surgical procedures: a\nsystematic review.Int. J. Qual. Health Care33, mzab092 (2021).\n3. Thirunavukarasu, A. J. et al. Large language models in medicine.Nat.\nMed. 29, 1930–1940 (2023).\n4. Ke, Y. H. et al. Development and testing of retrieval augmented\ngeneration in large language models— a case study report.\narXiv:https://arxiv.org/abs/2402.01733 (2024).\n5. Ke, Y. H. et al. Retrieval augmented generation for 10 large language\nmodels and its generalizability in assessing medicalﬁtness. NPJ Digit.\nMed. 8, 187 (2025).\n6. Lim, D. Y. Z. et al. Large language models in anaesthesiology: use of\nChatGPT for American Society of Anesthesiologists physical status\nclassiﬁcation. Br. J. Anaesth.https://doi.org/10.1016/j.bja.2023.06.\n052 (2023).\n7. Ismaiel, N. et al. The evaluation of the performance of ChatGPT in the\nmanagement of labor analgesia.J. Clin. Anesth.98, 111582 (2024).\n8. Turan, E. İ, Baydemir, A. E., Özcan, F. G. &Şahin, A. S. Evaluating the\naccuracy of ChatGPT-4 in predicting ASA scores: A prospective\nmulticentric study ChatGPT-4 in ASA score prediction.J. Clin. Anesth.\n96, 111475 (2024).\n9. Yang, X. et al. A study of deep learning methods for de-identiﬁcation of\nclinical notes in cross-institute settings.BMC Med. Inform. Decis.\nMak. 19, 232 (2019).\n10. Na, L. et al. Feasibility of reidentifying individuals in large national\nphysical activity data sets from which protected health information\nhas been removed with use of machine learning.JAMA Netw. Open1,\ne186040 (2018).\n11. Goodman, R. S. et al. On the cusp: considering the impact of artiﬁcial\nintelligence language models in healthcare.Med4, 139–140 (2023).\n12. Government of Singapore. Pair Websitehttps://www.tech.gov.sg/\nproducts-and-services/for-government-agencies/productivity-and-\nmarketing/pair.\n13. Government of Singapore. Health Sciences Authority Websitehttps://\nwww.hsa.gov.sg/medical-devices/.\nhttps://doi.org/10.1038/s41746-025-01858-x Article\nnpj Digital Medicine|           (2025) 8:462 6\n14. Singhal, K. et al. Large language models encode clinical knowledge.\nNature 620, 172–180 (2023).\n15. Chung, P. et al. Large language model capabilities in perioperative risk\nprediction and prognostication.JAMA Surg.159, 928–937 (2024).\n16. Woo, J. J. et al. Custom Large Language Models improve accuracy:\nComparing Retrieval Augmented Generation and Artiﬁcial Intelligence\nagents to non-custom models for evidence-based medicine.\nArthroscopy https://doi.org/10.1016/j.arthro.2024.10.042 (2024).\n17. Klang, E. et al. A strategy for cost-effective large language model use\nat health system-scale.NPJ Digit. Med.7, 320 (2024).\n18. Li, Z., Li, C., Zhang, M., Mei, Q. & Bendersky, M. Retrieval Augmented\nGeneration or long-context LLMs? A comprehensive study and hybrid\napproach. arXiv:https://arxiv.org/abs/2407.16833 (2024).\n19. Kim, Y. J., Choi, J. H. & Fotso, G. M. N. Medical professionals’\nadoption of AI-based medical devices: UTAUT model with trust\nmediation. J. Open Innov.10, 100220 (2024).\n20. Bergsland, J., Elle, O. J. & Fosse, E. Barriers to medical device\ninnovation. Med. Devices7, 205–209 (2014).\n21. Renukappa, S., Mudiyi, P., Suresh, S., Abdalla, W. & Subbarao, C.\nEvaluation of challenges for adoption of smart healthcare strategies.\nSmart Health26, 100330 (2022).\n22. Giannakopoulos, K., Kavadella, A., Aaqel Salim, A., Stamatopoulos,\nV. & Kaklamanos, E. G. Evaluation of the performance of generative AI\nlarge language models ChatGPT, Google Bard, and Microsoft Bing\nChat in supporting evidence-based dentistry: comparative mixed\nmethods study.J. Med. Internet Res.25, e51580 (2023).\n23. Haider, S. A. et al. Evaluating large language model (LLM)\nperformance on established breast classiﬁcation systems.\nDiagnostics14, 1491 (2024).\n24. Sim, X. L. et al. Transforming the perioperative medicine care model:\nthe Singapore experience.Anaesth. Intensive Care51,9 6–106 (2023).\n25. Xiao, G. et al. DuoAttention: efﬁcient long-context LLM inference with\nretrieval and Streaming Heads.ArXiv: https://arxiv.org/abs/2410.\n10819 (2024).\n26. Peng, H. et al. When does in-context learning fall short and why? A\nstudy on speciﬁcation-heavy tasks.arXiv:https://arxiv.org/abs/2311.\n08993 (2023).\n27. Anthropic PBC. Anthropic Website.https://www.anthropic.com/.\n28. Godbole, A., George, J. G. & Shandilya, S. Leveraging long-context\nLarge Language Models for multi-document understanding and\nsummarization in enterprise applications.arXiv:https://arxiv.org/abs/\n2409.18454 (2024).\n29. Meskó, B. Prompt engineering as an important emerging skill for\nmedical professionals: tutorial.J. Med. Internet Res.25, e50638 (2023).\n30. Davis, F. D. Perceived usefulness, perceived ease of use, and user\nacceptance of information technology.MIS Q13\n, 319 (1989).\n31. Husereau, D. et al. Consolidated health economic evaluation\nreporting standards (cheers) 2022 explanation and elaboration: a\nreport of the ISPOR CHEERS II Good Practices Task Force.Value\nHealth 25,1 0–31 (2022).\nAcknowledgements\nThe authors sincerely thank the nurses at the Preoperative Evaluation Clinic\nfrom Singapore General Hospital for their invaluable contributions to this\nstudy. We also acknowledge the preoperative clinical guidelines provided by\nthe hospital, which were pivotal in the successful execution of this project.\nAuthor contributions\nConceptualization and guidance: Ke Y.H., Abdullah H.R., Soh C.R., Sng\nB.L., D.J.N. Wong, Ting D.W., and Ong E.M. Product development: Ke Y.H.,\nAbdullah H.R., Jin L., Sim J.L.S., Chan C., andd D.J.N. Wong. Data\ncollection: Ke Y.H. and Ong B.Y.O. Coding and technical development: Jin\nL., Ong B. Data analysis: Ke Y.H., Jin L., Yeo S.Q., and Ong B., Manuscript\npreparation and proof-reading: Ke Y.H., Ong B.Y.O., Jin L., Sim J.L.S., Chan\nC., Soh C.R., Sng B.L., Ting D.W., Ong E.M., Yeo S.Q., D.J.N. Wong, and\nAbdullah H.R.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s41746-025-01858-x\n.\nCorrespondenceand requests for materials should be addressed to\nYu He Ke.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution-NonCommercial-NoDerivatives 4.0 International License,\nwhich permits any non-commercial use, sharing, distribution and\nreproduction in any medium or format, as long as you give appropriate\ncredit to the original author(s) and the source, provide a link to the Creative\nCommons licence, and indicate if you modiﬁed the licensed material. You\ndo not have permission under this licence to share adapted material\nderived from this article or parts of it. The images or other third party\nmaterial in this article are included in the article’s Creative Commons\nlicence, unless indicated otherwise in a credit line to the material. If material\nis not included in the article’s Creative Commons licence and your intended\nuse is not permitted by statutory regulation or exceeds the permitted use,\nyou will need to obtain permission directly from the copyright holder. To\nview a copy of this licence, visithttp://creativecommons.org/licenses/by-\nnc-nd/4.0/\n.\n© The Author(s) 2025\nhttps://doi.org/10.1038/s41746-025-01858-x Article\nnpj Digital Medicine|           (2025) 8:462 7",
  "topic": "Crossover",
  "concepts": [
    {
      "name": "Crossover",
      "score": 0.7001487016677856
    },
    {
      "name": "Perioperative",
      "score": 0.6467003226280212
    },
    {
      "name": "Randomized controlled trial",
      "score": 0.6285334229469299
    },
    {
      "name": "Crossover study",
      "score": 0.5660014152526855
    },
    {
      "name": "Medicine",
      "score": 0.4585546851158142
    },
    {
      "name": "Intensive care medicine",
      "score": 0.421514630317688
    },
    {
      "name": "Internal medicine",
      "score": 0.27438974380493164
    },
    {
      "name": "Computer science",
      "score": 0.24348130822181702
    },
    {
      "name": "Alternative medicine",
      "score": 0.24166500568389893
    },
    {
      "name": "Surgery",
      "score": 0.19660228490829468
    },
    {
      "name": "Artificial intelligence",
      "score": 0.14723613858222961
    },
    {
      "name": "Pathology",
      "score": 0.08347484469413757
    },
    {
      "name": "Placebo",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210126319",
      "name": "Duke-NUS Medical School",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I157582758",
      "name": "SingHealth",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I2251586001",
      "name": "Singapore General Hospital",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I4210116917",
      "name": "Singapore Eye Research Institute",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I2799299286",
      "name": "Singapore National Eye Center",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I4210112252",
      "name": "KK Women's and Children's Hospital",
      "country": "SG"
    }
  ],
  "cited_by": 1
}