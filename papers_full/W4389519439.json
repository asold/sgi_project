{
    "title": "ZhuJiu: A Multi-dimensional, Multi-faceted Chinese Benchmark for Large Language Models",
    "url": "https://openalex.org/W4389519439",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2169421452",
            "name": "Baoli Zhang",
            "affiliations": [
                "Chinese Academy of Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A2626828833",
            "name": "Haining Xie",
            "affiliations": [
                "University of Chinese Academy of Sciences",
                "Chinese Academy of Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A5104211343",
            "name": "Pengfan Du",
            "affiliations": [
                "Chinese Academy of Sciences",
                "University of Chinese Academy of Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A2095829332",
            "name": "Junhao Chen",
            "affiliations": [
                "Harbin Engineering University"
            ]
        },
        {
            "id": "https://openalex.org/A2106830110",
            "name": "Pengfei Cao",
            "affiliations": [
                "Chinese Academy of Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A2099156817",
            "name": "Yu-Bo Chen",
            "affiliations": [
                "University of Chinese Academy of Sciences",
                "Chinese Academy of Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A2122962385",
            "name": "Shengping Liu",
            "affiliations": [
                "Beijing Information Science & Technology University"
            ]
        },
        {
            "id": "https://openalex.org/A2072929885",
            "name": "Kang Liu",
            "affiliations": [
                "University of Chinese Academy of Sciences",
                "Chinese Academy of Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A2028683064",
            "name": "Jun Zhao",
            "affiliations": [
                "Chinese Academy of Sciences",
                "University of Chinese Academy of Sciences"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3119340592",
        "https://openalex.org/W4380353763",
        "https://openalex.org/W4220999516",
        "https://openalex.org/W4285294723",
        "https://openalex.org/W3114651185",
        "https://openalex.org/W2601450892",
        "https://openalex.org/W3106031450",
        "https://openalex.org/W2372100464",
        "https://openalex.org/W3172193188",
        "https://openalex.org/W2944378183",
        "https://openalex.org/W2949884065",
        "https://openalex.org/W4366735744",
        "https://openalex.org/W2806081754",
        "https://openalex.org/W4378464464",
        "https://openalex.org/W2115792525",
        "https://openalex.org/W3212716955",
        "https://openalex.org/W4379958452",
        "https://openalex.org/W2943552823",
        "https://openalex.org/W4384918448",
        "https://openalex.org/W4385565577",
        "https://openalex.org/W4389518784",
        "https://openalex.org/W2757276219",
        "https://openalex.org/W4362569074",
        "https://openalex.org/W4378189609",
        "https://openalex.org/W3099911888",
        "https://openalex.org/W4226494438",
        "https://openalex.org/W4295683011",
        "https://openalex.org/W4402683869",
        "https://openalex.org/W3174150157",
        "https://openalex.org/W4321854923",
        "https://openalex.org/W2753160622",
        "https://openalex.org/W4380136478",
        "https://openalex.org/W4288365681",
        "https://openalex.org/W4366735819",
        "https://openalex.org/W4380994495",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W4226476765",
        "https://openalex.org/W3175684466"
    ],
    "abstract": "Baoli Zhang, Haining Xie, Pengfan Du, Junhao Chen, Pengfei Cao, Yubo Chen, Shengping Liu, Kang Liu, Jun Zhao. Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. 2023.",
    "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 479–494\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nZhuJiu: A Multi-dimensional, Multi-faceted Chinese Benchmark for Large\nLanguage Models\nBaoli Zhang1∗, Haining Xie1,2∗, Pengfan Du1,2, Junhao Chen3, Pengfei Cao1,\nYubo Chen1,2†, Shengping Liu4, Kang Liu1,2 and Jun Zhao1,2\n1Institute of Automation, Chinese Academy of Sciences\n2School of Artificial Intelligence, University of Chinese Academy of Sciences\n3 Harbin Engineering University, 4 Beijing Unisound Information Technology Co., Ltd\n{baoli.zhang,pengfei.cao,yubo.chen,kliu,jzhao}@nlpr.ia.ac.cn\n{xiehaining21,dupengfan22}@mails.ucas.ac.cn, liushengping@unisound.com\nAbstract\nThe unprecedented performance of large lan-\nguage models (LLMs) requires comprehensive\nand accurate evaluation. We argue that for\nLLMs evaluation, benchmarks need to be com-\nprehensive and systematic. To this end, we\npropose the ZhuJiu benchmark, which has the\nfollowing strengths: (1) Multi-dimensional\nability coverage: We comprehensively eval-\nuate LLMs across 7 ability dimensions cov-\nering 51 tasks. Especially, we also propose\na new benchmark that focuses on knowledge\nability of LLMs. (2) Multi-faceted evalua-\ntion methods collaboration: We use 3 differ-\nent yet complementary evaluation methods to\ncomprehensively evaluate LLMs, which can\nensure the authority and accuracy of the eval-\nuation results. (3) Comprehensive Chinese\nbenchmark: ZhuJiu is the pioneering bench-\nmark that fully assesses LLMs in Chinese,\nwhile also providing equally robust evalua-\ntion abilities in English. (4) Avoiding poten-\ntial data leakage: To avoid data leakage, we\nconstruct evaluation data specifically for 37\ntasks. We evaluate 9 current mainstream LLMs\nand conduct an in-depth discussion and anal-\nysis of their results. The ZhuJiu benchmark\nand open-participation leaderboard are publicly\nreleased at http://www.zhujiu-benchmark.\ncom/ and we also provide a demo video at\nhttps://youtu.be/qypkJ89L1Ic.\n1 Introduction\nWith the continuous development of large language\nmodels (LLMs), the emergence of GPT4 (Ope-\nnAI, 2023) is enough to trigger a new wave of\ntechnology. Various types of LLMs have recently\nbeen rapidly developing, such as Llama2 (Tou-\nvron et al., 2023) and ChatGLM2 (Du et al., 2022),\ndemonstrating impressive generalization abilities\nand broad applicability. Therefore, it is crucial to\n1*Co-first authors, they contributed equally to this work.\n2†Corresponding author\nconduct comprehensive and objective evaluations\nof LLMs to fully understand their strengths and\nlimitations.\nSpecifically, on the one hand, for applicators,\nthey need to understand the overall performance\nof LLMs or the advantages of LLMs in a specific\naspect. Constructing comprehensive and authorita-\ntive benchmarks can help applicators significantly\nimprove the efficiency of using LLMs. On the other\nhand, for developers, the improvement direction of\nLLMs requires accurate evaluation results as guid-\nance. An objective and fair benchmark can help\nthem carry out relevant research work on LLMs\nmore targetedly.\nTo this end, scholars conduct extensive research\non evaluations for LLMs and construct some su-\nperior benchmarks. Normally, the evaluation for\nLLMs includes two aspects: ability evaluation and\nevaluation method. Although traditional bench-\nmarks such as GLUE (Wang et al., 2018), Su-\nperGLUE (Wang et al., 2019) and CUGE (Yao\net al., 2021) still have a role to play in evaluat-\ning LLMs, their limitations are becoming increas-\ningly apparent due to the growing diversity of eval-\nuation dimensions and methods for LLMs. For\nthe ability evaluation of LLMs, recent work pro-\nposes excellent benchmarks for LLMs in one or\nseveral aspects, such as knowledge, reasoning, lan-\nguage, safety and hallucination (Liang et al., 2022;\nJifan Yu, 2023; Sun et al., 2023a; Amayuelas et al.,\n2023; Li et al., 2023; Liu et al., 2023; Jeffery et al.,\n2021; Wittenburg et al., 2022). However, a compre-\nhensive evaluation of LLMs remains insufficient.\nFor the evaluation method of LLMs, there are\ncurrently 3 main categories: (1) Metrics Evalu-\nation: Evaluating LLMs using existing datasets\nand corresponding metrics (Liang et al., 2022); (2)\nChatGPT Evaluation: Using GPT-like LLMs to\ngenerate evaluation data and compare the response\nresults of different LLMs (Wang et al., 2023c); (3)\nModel Arena: constructing one-on-one model are-\n479\nFigure 1: The evaluation process of LLM using ZhuJiu.\nnas where humans compare the evaluation results\nof models based on their own judgment (Zheng\net al., 2023; Zhang et al., 2021).\nDespite these successful efforts for LLMs’ eval-\nuations, existing studies still suffer from several\nlimitations: (1) Current benchmarks tend to focus\non evaluating LLMs on a single dimension of their\nabilities, which can not provide a comprehensive\nevaluation of LLMs. (2) Most benchmarks only\nuse a single evaluation method, which may not pro-\nvide an accurate evaluation of all the abilities of\nLLMs. For example, while HELM (Liang et al.,\n2022) uses metrics to evaluate LLMs, it may not\nmeasure all abilities such as long-text generation or\nmachine translation, etc. (3) The cross-lingual abili-\nties of LLMs, especially for Chinese, have garnered\ngrowing attention. However, the lack of a compre-\nhensive Chinese benchmark for LLMs remains a\ncritical issue. (4) Many current benchmarks only\nuse public datasets for evaluation, risking potential\ndata leakage. The results of evaluations based on\nthis data lack credibility.\nIn this paper, we propose the ZhuJiu Benchmark\nto solve above mentioned problems, which can\nfill the gap in the development of a comprehen-\nsive benchmark for evaluating LLMs in Chinese.\nThe advantages of the ZhuJiu are as follows: (1)\nMulti-dimensional ability coverage: we evalu-\nate LLMs from 7 ability dimensions, including\nknowledge, Chinese-specific, language, reasoning,\nrefusal, safety and robustness abilities, covering 51\ndatasets to provide a comprehensive performance\nassessment. In addition, we also proposed a new\nparadigm for evaluating the knowledge ability. (2)\nMulti-faceted evaluation methods coordination:\nwe use Metrics Evaluation, Scoring Evaluation,\nand Comparative Evaluation for comprehensively\nevaluating LLMs to ensure authoritative and accu-\nrate evaluation results. (3) Comprehensive Chi-\nnese benchmark: ZhuJiu is the pioneering Chi-\nnese benchmark that can comprehensively evaluate\nLLMs, while allowing equivalent assessment in En-\nglish. (4) Avoiding potential data leakage: in ad-\ndition to collecting 14 commonly used datasets, we\nconstruct 37 datasets for the evaluation of LLMs,\nensuring maximum avoidance of data leakage and\nevaluation fairness. The overall evaluation process\nis shown in Figure 1.\nWe also release an online evaluation platform\nthat supports multiple functions including visualiza-\ntions of evaluation results, participating in model\narena and submission of evaluation model, etc.\nMoreover, we evaluate 9 publicly available LLMs,\nincluding ChatGLM (Du et al., 2022), BELLE\n(Yunjie Ji and Li, 2023), ChatGPT (OpenAI, 2022),\nand so on. Based on the experimental results, we\nobserve some interesting phenomena and summa-\nrize them in 4.2.\nIn summary, the contributions of this paper are\nas follows:\n• We propose ZhuJiu, the first Chinese bench-\nmark that covers multi-dimensions of ability\nand employs multi-faceted evaluation meth-\nods in collaboration. Meanwhile in the ZhuJiu\nwe construct a novel benchmark for evaluating\nknowledge ability and 37 evaluation datasets\nto prevent data leakage issues.\n• We release an online evaluation platform that\nenables users to evaluate LLMs. We will con-\ntinue to improve the platform, and update the\nevaluation leaderboard.\n• Using the ZhuJiu benchmark, we evaluate 9\n480\ncurrent LLMs, to comprehensively and deeply\nexplore their abilities, providing valuable in-\nsights to inform future LLM development.\n2 ZhuJiu Benchmark\nAs stated above, the ZhuJiu benchmark uses 3 eval-\nuation methods to assess the abilities across seven\ndimensions of LLMs. This section provides a de-\ntailed introduction to the ZhuJiu benchmark cov-\nering the evaluation methods, datasets, and ability\ndimensions. We also detail the specific scoring\nrules in Appendix A. The evaluation framework is\nshown in Figure 2.\n2.1 Evaluation Methods\nUnlike previous works that only use a single eval-\nuation method (Liang et al., 2022; Wang et al.,\n2023b,c; Zheng et al., 2023), in order to ensure\nthe reliability of the evaluation results, we employ\na collaborative evaluation approach that utilizes 3\ntypes of evaluation methods: Metrics Evaluation,\nScoring Evaluation, and Comparative Evaluation.\n2.1.1 Metrics Evaluation\nMetrics Evaluation is an indispensable component\nin LLM assessment, providing objective results\n(Chang et al., 2023). In this paper, we adopt\nthe HELM evaluation framework. Building on\nHELM (Liang et al., 2022), we extend it with ad-\nditional Chinese benchmarks for language, reason-\ning, knowledge, and Chinese abilities, with 14 ex-\npanded datasets total.\n2.1.2 Scoring Evaluation\nThe abilities demonstrated by ChatGPT (OpenAI,\n2022) and GPT-4 (OpenAI, 2023) have brought us\ngreat surprises. Therefore, we conduct evaluations\non the responses of LLMs using prompt engineer-\ning based on ChatGPT. Specifically, we evaluate\ndifferent abilities and devise different perspectives\nto assist ChatGPT in scoring the responses. We use\nfew-shot (Snell et al., 2017; Ravi and Larochelle,\n2016; Wang et al., 2020) method and answer label,\ncombined with numerous experiments, to ensure\nthe accuracy and stability of ChatGPT’s evaluation\nresults.\n2.1.3 Comparative Evaluation\nComparative evaluation is the most intuitive eval-\nuation method. In this paper, we drew inspi-\nration from the work of Chatbot Arena (Zheng\net al., 2023) and used the one-on-one model arena\nmethod to compare and evaluate the performance\nof LLMs based on human judgments. Furthermore,\nwe provide a one-on-one model comparison func-\ntion in the platform, which allows users to compare\nthe quality of responses from different LLMs to the\nsame question.\n2.2 Datasets\nFor a benchmark, the most crucial part is undoubt-\nedly its data source and data quality. In ZhuJiu,\nour evaluation data comes from two parts. On the\none hand, we use 14 currently popular LLMs eval-\nuation datasets. On the other hand, considering\nthe serious issue of data leakage when solely using\npublic datasets for LLMs evaluation, which could\ncompromise the fairness of evaluation results, we\nconstructed 37 evaluation datasets based on Chat-\nGPT (OpenAI, 2022).\n2.2.1 Collect Datasets\nTo ensure the generality of ZhuJiu, we evaluate\nLLMs using 14 publicly available datasets, which\nare essential due to their high quality and ability\nto accurately evaluate the performance of LLMs in\ncertain aspects.\n2.2.2 Construct Datasets\nTo address the issue of data leakage in LLMs eval-\nuation, we are inspired by PandaLM (Wang et al.,\n2023c) and we construct corresponding evaluation\ndatasets for 37 specific tasks. Specifically, for each\ntask, we first carefully select some evaluation data\nas seeds manually. Then, we use these seeds to\ngenerate prompts based on ChatGPT through self-\ninstruction (Wang et al., 2022). After that, we man-\nually review and confirm the prompts we used (for\neach specific task, we generate 100 prompts in Chi-\nnese).\nTo better understand the processes of data con-\nstruction and evaluation in a more intuitive way, we\ntake Scoring Evaluation as an example to demon-\nstrate the process, as shown in Figure 3.\n2.3 Ability System\nWith the help of the aforementioned evaluation\nmethods and datasets, we can assess the abilities\nof LLMs in 7 aspects. We will provide a detailed\nintroduction to the specific evaluation methods and\ndetails in this section.\n2.3.1 Knowledge Ability\nTo comprehensively evaluate the knowledge abil-\nities of LLMs, we conduct the evaluation from\n481\nFigure 2: Overall view of the ZhuJiu benchmark. In ZhuJiu’s framework, the integration of multi-angle datasets\nand multi-faceted evaluation methods provides strong support for multi-dimensional ability assessment. Based\non this, we have further developed an online assessment platform to support ZhuJiu’s online assessment and result\nupdates.\nfour perspectives: world knowledge, commonsense\nknowledge, linguistic knowledge, and concept. For\neach evaluation perspective, we select the appro-\npriate properties of accuracy, robustness, complete-\nness, and timeliness to construct evaluation datasets\nfor evaluating LLMs. Detailed descriptions of these\nfour properties are provided in Appendix B, using\na detailed framework shown in Figure 4. Com-\npared to KoLA (Jifan Yu, 2023), our evaluation\nperspective for knowledge is broader.\nFor world knowledge, on the one hand, we uti-\nlize the GAOKAO-bench (Zhang et al., 2023) (Non-\nmathematical section) and combine it with Met-\nrics Evaluation to conduct the evaluation. On the\nother hand, we construct corresponding evaluation\ndatasets for each evaluation property, including ac-\ncuracy, robustness, completeness, and timeliness,\nand evaluate LLMs using Scoring Evaluation.\nFor commonsense knowledge, we select com-\nmonsense triplets as the basic data and construct\nevaluation datasets based on the evaluation prop-\nerties of accuracy and robustness. We then use\nScoring Evaluation to evaluate LLMs.\nFor linguistic knowledge , we use Chinese\nFrameNet (CFN) (Hao et al., 2007; Baker et al.,\n1998) as the original corpus. In order to sim-\nplify the evaluation form of linguistic knowledge,\nwe mainly construct datasets in the following two\nways: one is to infer the “frame name” of the lin-\nguistic frame according to the “frame def” in the\nlinguistic frame, the other is to infer the “frame\nname” of the linguistic frame based on the “lexical-\nunit name” in the linguistic frame. Then we can\nevaluate the accuracy and robustness of LLMs lin-\nguistic knowledge by using the Scoring Evaluation.\nFor concept, we manually select common entity\nwords as the original data and evaluate the accuracy\nand robustness of LLMs concepts with Scoring\nEvaluation.\n2.3.2 Chinese-Specific Ability\nFollowing SuperCLUE (Liang Xu and others from\nSuperCLUE team, 2023), and conventional Chi-\nnese evaluations, the Chinese-specific ability eval-\nuation aims to use corpora with Chinese unique\ncharacteristics as the original data to form evalu-\nation data. These corpora include ChID (Zheng\net al., 2019), CCPM (Li et al., 2021), CINLID\nand Y ACLC (Wang et al., 2021b), and we evaluate\nLLMs using Metrics Evaluation.\n2.3.3 Language Ability\nWe conduct a comprehensive evaluation of LLMs’\nlanguage ability from both aspects of language un-\nderstanding and language generation. For eval-\nuating LLMs’ language understanding ability ,\nwe choose to evaluate them on the tasks of reading\ncomprehension and coreference resolution. We find\nthat using existing datasets could achieve good eval-\nuation results, and the datasets we use included C3\n(Sun et al., 2020), GCRC (Tan et al., 2021), CMRC\n(Cui et al., 2018), DRCR (Shao et al., 2018) and\nCLUEWSC-2020 (Xu et al., 2020), correspond-\ningly we use Metrics Evaluation. For evaluating\n482\nLLMs’ language generation ability, we summa-\nrize 6 typical language generation tasks, including\ncommon response (Daily question answering), dia-\nlogue (Dialog generation based on the scene), for-\nmal writing (Generation of formal texts for letters\nand other formal occasions), poetry (Generate po-\nems on request), writing story (Generate stories on\nrequest) and writing style (Generate text according\nto the requirements of the writing style) (Chang\net al., 2023), and evaluating by Scoring Evaluation.\n2.3.4 Reasoning Ability\nAs the evaluation of LLMs’ reasoning ability is\nless affected by data leakage (Chang et al., 2023),\nwe find that only using publicly available datasets\ncould yield relatively fair results. We select the\ncurrently popular mathematical reasoning and text\nsemantic reasoning tasks, and the datasets included\nGAOKAO-bench (Zhang et al., 2023) (mathemat-\nics section), Math23k (Wang et al., 2017), OCNLI\n(Hu et al., 2020), Chinese-SNLI (chi, 2019) and\nChinese-MNLI (Xu et al., 2020). The evaluation\nmethod for reasoning ability is based on Metrics\nEvaluation.\n2.3.5 Refusal Ability\nRegarding the refusal ability, we can understand it\nlike this: To know what you know and to know what\nyou do not know, that is true knowledge. For con-\nstructing datasets of refusal ability, we drew inspira-\ntion from the categories of Known-Unknown Ques-\ntions proposed in Amayuelas et al., 2023, includ-\ning Future Unknown, Unsolved Problem/Mystery,\nControversial/Debatable Question, Question with\nFalse Assumption, Counterfactual Question and\nUnderspecified Question. Then, we employ Scor-\ning Evaluation to assess LLMs for each category.\n2.3.6 Safety\nFor the evaluation of safety ability, we follow Sun\net al., 2023a’s classification of safety ability and\nfurther summarize and categorize them. We derive\na total of 9 evaluation tasks from 6 perspectives, in-\ncluding Insult, Human Health (Physical harm and\nMental health), Social Topic (Unfairness discrim-\nination and Ethics morality), Serious Risk (Crim-\ninal Activity and Unsafe Instruction Topic), Goal\nHijacking and Role play instruction. Subsequently,\nwe employ the Scoring Evaluation to assess LLMs.\n2.3.7 Robustness\nTraditional robustness evaluation primarily focuses\non assessing the impact of adding perturbations\nof varying granularity to the text on the perfor-\nmance of the model (Zhu et al., 2023; Wang et al.,\n2021a, 2023a). Regarding the robustness evalu-\nation of LLMs, on one hand, we still consider\ntoken-level perturbations and sentence-level per-\nturbations from the traditional robustness evalua-\ntion perspective, and propose three evaluation tasks\nincluding Error Message, Redundant Information\nand Redundant Dialogue. On the other hand, we\nexpand three aspects ofFormat Output, Dialect and\nUnique Solution tasks (Evaluate the certainty of the\nmodel’s answer to the unique solution through mul-\ntiple rounds of questioning) specifically tailor to\nthe characteristics of LLMs. Ultimately, we con-\nduct evaluations on these six aspects based on the\nScoring Evaluation.\n3 Platform\nWe develop an online platform to provide a range\nof services for the community as follows:\nVisualizations of evaluation results We pub-\nlish the rankings of all model evaluations on the\nplatform, including specific scores for each ability\nand evaluation method, and the rankings will be\nupdated continuously as the evaluations progress.\nParticipating in Model ArenaWe launch a one-\non-one model arena feature on our platform, where\neveryone can support the LLMs they believe per-\nform better based on their own judgment. Please\nrefer to Figure 5 to see the web view of the model\narena.\nSubmission of Evaluation Model We also en-\ncourage everyone to actively participate in our eval-\nuations and join the leaderboard. On our platform,\nwe allow users to submit applications for evalua-\ntion.\n4 Experiment\n4.1 Evaluated Models\nTo facilitate the utilization and advancement of\nLLMs, the primary emphasis of ZhuJiu’s inau-\ngural evaluation phase is directed towards open-\nsource LLMs with a parameter magnitude of ap-\nproximately 10 billion, including: ChatGLM-6B\n(Du et al., 2022), ChatGLM2-6B (Du et al., 2022),\nBELLE-7B (Yunjie Ji and Li, 2023), ChatFlow\n(Li et al., 2022; Zhao et al., 2022), Phoenix-Inst-\nChat-7B (Chen et al., 2023b,a), ChatYuan-large-v2\n(Xuanwei Zhang and Zhao, 2022), Moss-Moon-\n003-SFT (Sun et al., 2023b) and RWKV (Bo,\n483\nLLMs\nScore Abilities Knowledge Chinese-Specific Language Reasoning Refusal Safety RobustnessAll\nChatGLM2-6B 91.1 59.5 85.6 80.6 82.0 55.4 63.8 74.0\nChatGLM-6B 67.3 73.9 74.8 37.0 80.4 82.3 50.0 66.5\nBELLE-7B 54.53 40.54 54.2 44.5 58.1 39.8 55.9 49.6\nMoss-Moon-003-SFT 50.4 27.0 56.3 15.9 48.2 64.8 46.2 44.1\nChatYuan-large-v2 58.8 20.7 37.3 42.7 37.5 78.1 29.8 43.6\nChatFlow 43.3 54.1 33.3 47.1 39.2 40.3 36.1 41.9\nPhoenix-Inst-chat-7B 19.53 0 62.3 0 67.3 65.9 61.0 39.4\nRWKV 23.4 15.0 35.8 69.3 16.4 20.5 45.9 32.3\nGPT-3.5-turbo 82.4 100.0 84.3 100.0 100.0 100.0 85.5 93.2\nTable 1: The overall performance based on ten-point system of the LLMs participating in the ZhuJiu evaluation in\nthe first season. The score of GPT-3.5-turbo is only for reference and not included in the evaluation.\n2021). Concurrently, we employ ChatGPT (Ope-\nnAI, 2022) as a comparative benchmark and con-\nduct an assessment of the GPT-3.5-turbo API ser-\nvice.\n4.2 Overall Performance\nWe report the overall performance in Table??, and\nshow more detailed assessment results in our plat-\nform. From the results, we can obtain some intrigu-\ning findings:\n(1) Model-Performance is Limited by Model-\nSize: Based on the results in table ??, it be-\ncomes evident that models with a parameter\nsize of around 10 billion still exhibit significant\nlimitations in overall performance compared\nto GPT-3.5-turbo (OpenAI, 2022). In ZhuJiu,\nthe performance of most LLMs is relatively\nmediocre, with ChatGLM2 and ChatGLM (Du\net al., 2022) showing relatively better perfor-\nmance. It becomes apparent that the size of the\nmodel’s parameters continues to play a vital\nrole in determining its performance.\n(2) Lower Limit Sets Upper Limit: The anal-\nysis reveals that Phoenix (Chen et al., 2023b)\ndemonstrates notable proficiency in refusal and\nsafety abilities, etc. However, its overall rank-\ning is comparatively lower, primarily attributed\nto its limitations in reasoning and Chinese-\nspecific abilities. These deficiencies are also\nobserved in other LLMs occupying lower posi-\ntions in the rankings. However,the lower limits\nof various abilities in LLMs often determine the\nupper limits of LLMs’ application prospects.\n(3) Knowledge is Power: In ZhuJiu, our primary\nfocus lies in the knowledge ability of LLMs, as\nthe pivotal task at hand is to ensure LLMs ac-\nquire accurate knowledge and effectively har-\nness their acquired knowledge. However, in\nthis version, the majority of LLMs exhibit sub-\npar performance in terms of knowledge capac-\nity, making the ZhuJiu benchmark exception-\nally challenging. The results reveal that Chat-\nGLM2 (Du et al., 2022) exhibits strong perfor-\nmance in knowledge ability, surpassing even\nChatGPT.\n5 Conclusion and Future Work\nIn this work, we present ZhuJiu, the pioneering\nmulti-dimensional ability coverage, multi-faceted\nevaluation methods collaboration Chinese bench-\nmark. ZhuJiu is capable of using 3 evaluation meth-\nods to comprehensively evaluate LLMs across 7\nability dimensions, using 51 datasets. Additionally,\nwe independently construct 37 evaluation datasets\nto maximize the avoidance of data leakage issues\nin LLM evaluation. We also focus on expanding\nthe evaluation of knowledge ability, providing a\nnew framework for assessing LLMs’ knowledge\nability. Finally, we provide a comprehensive and\ncontinuously updated evaluation platform with mul-\ntiple functions and in the first season of ZhuJiu, we\nevaluate 9 open-source LLMs.\nIn the future, we plan to (1) continuously con-\nstruct high-quality evaluation datasets to enrich\nZhuJiu, (2) further perfect the assessment of knowl-\nedge ability and develop new evaluation methods\nfor Chinese characteristic ability, (3) further per-\nfect the platform’s functionality and update the\nplatform’s information.\n484\nAcknowledgements\nThis work is supported by the National Key Re-\nsearch and Development Program of China (No.\n2020AAA0106400), the National Natural Science\nFoundation of China (No. 6197621162176257 ).\nThis work is also supported by the Strategic Pri-\nority Research Program of Chinese Academy of\nSciences (Grant No.XDA27020100 ), the Youth In-\nnovation Promotion Association CAS, and Yunnan\nProvincial Major Science and Technology Special\nPlan Projects (No.202202AD080004).\nReferences\n2019. Blog: Chinese-snli. https://gitee.com/\njiaodaxin/CNSD.\nAlfonso Amayuelas, Liangming Pan, Wenhu Chen, and\nWilliam Wang. 2023. Knowledge of knowledge: Ex-\nploring known-unknowns uncertainty with large lan-\nguage models. arXiv preprint arXiv:2305.13712.\nCollin F Baker, Charles J Fillmore, and John B Lowe.\n1998. The berkeley framenet project. In COLING\n1998 Volume 1: The 17th International Conference\non Computational Linguistics.\nPENG Bo. 2021. Blinkdl/rwkv-lm: 0.01.\nYupeng Chang, Xu Wang, Jindong Wang, Yuan Wu,\nKaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi,\nCunxiang Wang, Yidong Wang, et al. 2023. A sur-\nvey on evaluation of large language models. arXiv\npreprint arXiv:2307.03109.\nZhihong Chen, Junying Chen, Hongbo Zhang, Feng\nJiang, Guiming Chen, Fei Yu, Tiannan Wang, Juhao\nLiang, Chen Zhang, Zhiyi Zhang, Jianquan Li, Xiang\nWan, Haizhou Li, and Benyou Wang. 2023a. Llm\nzoo: democratizing chatgpt. https://github.com/\nFreedomIntelligence/LLMZoo.\nZhihong Chen, Feng Jiang, Junying Chen, Tiannan\nWang, Fei Yu, Guiming Chen, Hongbo Zhang, Juhao\nLiang, Chen Zhang, Zhiyi Zhang, Jianquan Li, Xi-\nang Wan, Benyou Wang, and Haizhou Li. 2023b.\nPhoenix: Democratizing chatgpt across languages.\narXiv preprint arXiv:2304.10453.\nYiming Cui, Ting Liu, Wanxiang Che, Li Xiao, Zhipeng\nChen, Wentao Ma, Shijin Wang, and Guoping\nHu. 2018. A span-extraction dataset for chinese\nmachine reading comprehension. arXiv preprint\narXiv:1810.07366.\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,\nJiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm:\nGeneral language model pretraining with autoregres-\nsive blank infilling. In Proceedings of the 60th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 320–335.\nXiaoyan Hao, Wei Liu, Ru Li, and Kaiying Liu. 2007.\nDescription systems of the chinese framenet database\nand software tools. Journal of Chinese information\nprocessing, 21(5):96–100.\nHai Hu, Kyle Richardson, Liang Xu, Lu Li, Sandra\nKübler, and Lawrence S Moss. 2020. Ocnli: Original\nchinese natural language inference. arXiv preprint\narXiv:2010.05444.\nKeith Jeffery, Peter Wittenburg, Larry Lannom,\nGeorge Strawn, Claudia Biniossek, Dirk Betz, and\nChristophe Blanchi. 2021. Not ready for convergence\nin data infrastructures. Data Intelligence, 3(1):116–\n135.\nShangqing Tu Shulin Cao Daniel Zhang-Li Xin Lv\nHao Peng Zijun Yao Xiaohan Zhang Hanming Li\nChunyang Li Zheyuan Zhang Yushi Bai Yantao Liu\nAmy Xin Nianyi Lin Kaifeng Yun Linlu Gong Jian-\nhui Chen Zhili Wu Yunjia Qi Weikai Li Yong Guan\nKaisheng Zeng Ji Qi Hailong Jin Jinxin Liu Yu Gu\nYuan Yao Ning Ding Lei Hou Zhiyuan Liu Bin Xu\nJie Tang Juanzi Li Jifan Yu, Xiaozhi Wang. 2023.\nKola: Carefully benchmarking world knowledge of\nlarge language models.\nJunyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun\nNie, and Ji-Rong Wen. 2023. Halueval: A large-\nscale hallucination evaluation benchmark for large\nlanguage models. arXiv e-prints, pages arXiv–2305.\nWenhao Li, Fanchao Qi, Maosong Sun, Xiaoyuan\nYi, and Jiarui Zhang. 2021. Ccpm: A chinese\nclassical poetry matching dataset. arXiv preprint\narXiv:2106.01979.\nYudong Li, Yuqing Zhang, Zhe Zhao, Linlin Shen, Wei-\njie Liu, Weiquan Mao, and Hui Zhang. 2022. CSL:\nA large-scale Chinese scientific literature dataset. In\nProceedings of the 29th International Conference\non Computational Linguistics , pages 3917–3923,\nGyeongju, Republic of Korea. International Com-\nmittee on Computational Linguistics.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris\nTsipras, Dilara Soylu, Michihiro Yasunaga, Yian\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya Ku-\nmar, et al. 2022. Holistic evaluation of language\nmodels. arXiv preprint arXiv:2211.09110.\nKangkang Zhao Lei Zhu Liang Xu, Xuanwei Zhang and\nothers from SuperCLUE team. 2023. Superclue: A\nbenchmark for foundation models in chinese. https:\n//github.com/CLUEbench/SuperCLUE.\nYuchi Liu, Zhongdao Wang, Xiangxin Zhou, and Liang\nZheng. 2023. A study of using synthetic data for\neffective association knowledge learning. Machine\nIntelligence Research, 20(2):194–206.\nOpenAI. 2022. Blog: Introducing chatgpt. https:\n//openai.com/blog/chatgpt.\nOpenAI. 2023. Gpt-4 technical report.\n485\nSachin Ravi and Hugo Larochelle. 2016. Optimization\nas a model for few-shot learning. In International\nconference on learning representations.\nChih Chieh Shao, Trois Liu, Yuting Lai, Yiying Tseng,\nand Sam Tsai. 2018. Drcd: A chinese machine\nreading comprehension dataset. arXiv preprint\narXiv:1806.00920.\nJake Snell, Kevin Swersky, and Richard Zemel. 2017.\nPrototypical networks for few-shot learning. Ad-\nvances in neural information processing systems, 30.\nHao Sun, Zhexin Zhang, Jiawen Deng, Jiale Cheng,\nand Minlie Huang. 2023a. Safety assessment of\nchinese large language models. arXiv preprint\narXiv:2304.10436.\nKai Sun, Dian Yu, Dong Yu, and Claire Cardie. 2020. In-\nvestigating prior knowledge for challenging chinese\nmachine reading comprehension. Transactions of the\nAssociation for Computational Linguistics , 8:141–\n155.\nTianxiang Sun, Xiaotian Zhang, Zhengfu He, Peng Li,\nQinyuan Cheng, Hang Yan, Xiangyang Liu, Yunfan\nShao, Qiong Tang, Xingjian Zhao, Ke Chen, Yining\nZheng, Zhejian Zhou, Ruixiao Li, Jun Zhan, Yun-\nhua Zhou, Linyang Li, Xiaogui Yang, Lingling Wu,\nZhangyue Yin, Xuanjing Huang, and Xipeng Qiu.\n2023b. Moss: Training conversational language mod-\nels from synthetic data.\nHongye Tan, Xiaoyue Wang, Yu Ji, Ru Li, Xiaoli Li,\nZhiwei Hu, Yunxiao Zhao, and Xiaoqi Han. 2021.\nGcrc: A new challenging mrc dataset from gaokao\nchinese for explainable evaluation. In Findings of\nthe Association for Computational Linguistics: ACL-\nIJCNLP 2021, pages 1319–1330.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint\narXiv:2307.09288.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel Bowman. 2019. Superglue: A stick-\nier benchmark for general-purpose language under-\nstanding systems. Advances in neural information\nprocessing systems, 32.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\nJindong Wang, Xixu Hu, Wenxin Hou, Hao Chen,\nRunkai Zheng, Yidong Wang, Linyi Yang, Hao-\njun Huang, Wei Ye, Xiubo Geng, et al. 2023a.\nOn the robustness of chatgpt: An adversarial\nand out-of-distribution perspective. arXiv preprint\narXiv:2302.12095.\nXiao Wang, Qin Liu, Tao Gui, Qi Zhang, et al. 2021a.\nTextflint: Unified multilingual robustness evaluation\ntoolkit for natural language processing. In Proceed-\nings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing:\nSystem Demonstrations, pages 347–355, Online. As-\nsociation for Computational Linguistics.\nYan Wang, Xiaojiang Liu, and Shuming Shi. 2017.\nDeep neural solver for math word problems. In Pro-\nceedings of the 2017 conference on empirical meth-\nods in natural language processing, pages 845–854.\nYaqing Wang, Quanming Yao, James T Kwok, and Li-\nonel M Ni. 2020. Generalizing from a few examples:\nA survey on few-shot learning. ACM computing sur-\nveys (csur), 53(3):1–34.\nYidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang,\nQiang Heng, Cunxiang Wang, Hao Chen, Chaoya\nJiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye,\nShikun Zhang, and Yue Zhang. 2023b. Pandalm:\nReproducible and automated language model assess-\nment. https://github.com/WeOpenML/PandaLM.\nYidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang,\nCunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie,\nJindong Wang, Xing Xie, Wei Ye, Shikun Zhang, and\nYue Zhang. 2023c. Pandalm: An automatic evalua-\ntion benchmark for llm instruction tuning optimiza-\ntion.\nYingying Wang, Cunliang Kong, Liner Yang, Yijun\nWang, Xiaorong Lu, Renfen Hu, Shan He, Zhenghao\nLiu, Yun Chen, Erhong Yang, et al. 2021b. Yaclc: A\nchinese learner corpus with multidimensional anno-\ntation. arXiv preprint arXiv:2112.15043.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa\nLiu, Noah A. Smith, Daniel Khashabi, and Hannaneh\nHajishirzi. 2022. Self-instruct: Aligning language\nmodel with self generated instructions.\nPeter Wittenburg, Alex Hardisty, Yann Le Franc, Amir-\npasha Mozaffari, Limor Peer, Nikolay A Skvortsov,\nZhiming Zhao, and Alessandro Spinuso. 2022.\nCanonical workflows to make data fair. Data In-\ntelligence, 4(2):286–305.\nLiang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie Cao,\nYudong Li, Yechen Xu, Kai Sun, Dian Yu, Cong\nYu, Yin Tian, Qianqian Dong, Weitang Liu, Bo Shi,\nYiming Cui, Junyi Li, Jun Zeng, Rongzhao Wang,\nWeijian Xie, Yanting Li, Yina Patterson, Zuoyu Tian,\nYiwen Zhang, He Zhou, Shaoweihua Liu, Zhe Zhao,\nQipeng Zhao, Cong Yue, Xinrui Zhang, Zhengliang\nYang, Kyle Richardson, and Zhenzhong Lan. 2020.\nCLUE: A Chinese language understanding evalua-\ntion benchmark. In Proceedings of the 28th Inter-\nnational Conference on Computational Linguistics,\npages 4762–4772, Barcelona, Spain (Online). Inter-\nnational Committee on Computational Linguistics.\n486\nLiang Xu Xuanwei Zhang and Kangkang Zhao. 2022.\nChatyuan: A large language model for dialogue in\nchinese and english.\nYuan Yao, Qingxiu Dong, Jian Guan, Boxi Cao,\nZhengyan Zhang, Chaojun Xiao, Xiaozhi Wang,\nFanchao Qi, Junwei Bao, Jinran Nie, et al. 2021.\nCuge: A chinese language understanding and gen-\neration evaluation benchmark. arXiv preprint\narXiv:2112.13610.\nYan Gong Yiping Peng Qiang Niu-Baochang Ma Yun-\njie Ji, Yong Deng and Xiangang Li. 2023. Belle:\nBe everyone’s large language model engine. https:\n//github.com/LianjiaTech/BELLE.\nBaoli Zhang, Zhucong Li, Zhen Gan, Yubo Chen, Jing\nWan, Kang Liu, Jun Zhao, Shengping Liu, and Yafei\nShi. 2021. Croano: A crowd annotation platform for\nimproving label consistency of chinese ner dataset.\nIn Proceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 275–282.\nXiaotian Zhang, Chunyang Li, Yi Zong, Zhengyu Ying,\nLiang He, and Xipeng Qiu. 2023. Evaluating the\nperformance of large language models on gaokao\nbenchmark.\nZhe Zhao, Yudong Li, Cheng Hou, Jing Zhao, Rong\nTian, Weijie Liu, Yiren Chen, Ningyuan Sun, Haoyan\nLiu, Weiquan Mao, et al. 2022. Tencentpre-\ntrain: A scalable and flexible toolkit for pre-training\nmodels of different modalities. arXiv preprint\narXiv:2212.06385.\nChujie Zheng, Minlie Huang, and Aixin Sun. 2019.\nChID: A large-scale Chinese IDiom dataset for cloze\ntest. In ACL.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang,\nJoseph E. Gonzalez, and Ion Stoica. 2023. Judging\nllm-as-a-judge with mt-bench and chatbot arena.\nKaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen\nWang, Hao Chen, Yidong Wang, Linyi Yang, Wei\nYe, Neil Zhenqiang Gong, Yue Zhang, et al. 2023.\nPromptbench: Towards evaluating the robustness of\nlarge language models on adversarial prompts. arXiv\npreprint arXiv:2306.04528.\nA Scoring Rules\nWe will comprehensively evaluate the model from\nseven ability dimensions and 3 assessment methods\nto ensure the thoroughness and authority of the\nevaluation results. Specifically, the comprehensive\nevaluation process can be broken down into three\nsteps.\nStep 1 For each ability dimension score A,\nwe will take the average of LLM’s scores d =\n[d1, . . . , dn] on each dataset as LLM’s score for\nthat ability dimension:\nA = 1\nn\nn∑\ni=1\ndi (1)\nStep 2 For each evaluation method score E,\nLLM’s score is the average of its scores A =\n[A1, . . . , Am] for each ability dimension:\nE = 1\nm\nn∑\nj=1\nAj (2)\nStep 3 LLM’s scores E = [E1, E2, E3] for each\nevaluation method are standardized and then aver-\naged to obtain LLM’s final score on ZhuJiu:\nEnorm = Ek − Emin\nEmax − Emin\n(3)\nB Evaluation Perspective for Knowledge\nAbility\nIn the evaluation process of knowledge ability, we\nmainly evaluate from the properties of accuracy,\nrobustness, completeness and timeliness. For each\nproperty, we will randomly generate one hundred\nsets of evaluation data for evaluation. Here we\nNeed to explain the specific indicators of each eval-\nuation (Wittenburg et al., 2022).\n• Accuracy: Evaluate whether the content of\nthe model’s reply is correct through Exact\nMatch (EM) and ChatGPT (OpenAI, 2022),\nand calculate the accuracy rate in the 100 ques-\ntions answered correctly by the model.\n• Robustness: We use the same set of data to\nuse ChatGPT to randomly generate five differ-\nent ways of asking questions, and then score\naccording to whether the model is stable in\nreplying to different questions generate by the\nsame set of data. The principle of scoring is\nthat the more stable the content of the reply,\nthe higher the score.\n• Completeness: Only for the evaluation of\nworld knowledge, scoring is based on the\nproportion of standard answers cover in the\nmodel’s reply content. For example, accord-\ning to the calculation of a question with a\nfull score of 10, for the data “( 中国四大\n发明—包括—火药,指南针,造纸术,印刷\n术)” “(The Four Great Inventions of an-\ncient China—include—gunpowder, compass,\n487\nFigure 3: The specific processes of data construction\nand Scoring Evaluation\npapermaking, printing)” generate the evalu-\nation question “ 中国的四大发明包括哪\n些?” “What are the Four Great Inventions\nof ancient China?” , if the model answers\n“火药,指南针,造纸术,印刷术” “gunpowder,\ncompass, papermaking, printing”, it will get\na full score of 10, and answer “ 火药,指南\n针,造纸术,瓷器” “gunpowder, compass, pa-\npermaking, china” has a correct rate of 75\npercent and a score of 7.5.\n• Timeliness: It is only aim at the evaluation\nof world knowledge, and specifically evalu-\nates the update degree of LLMs knowledge,\nsimilar to accuracy, and evaluates whether the\nanswer of the model is correct or not accord-\ning to EM and ChatGPT.\nC GPT-Assessment Prompts\nIn the scoring evaluation method, we use GPT-4\nto score the answers of the model being tested.\nThe evaluation content covers 37 testing tasks cor-\nresponding to 7 capabilities, and the evaluation\ndatasets are all generated by GPT and manually re-\nviewed to prevent data leakage. For each evaluation\ntask, there are more than three nearly characteris-\ntic evaluation indicators. Table 2 shows some task\nprompt cases and table 3 shows the GPT evaluation\nprompts that used in Language Ability.\n488\nTasks Prompt Cases\nCommon Response 全球气候变化会对人类生活产生什么影响？\nWhat impact will global climate change have on human life?\nDialogue 假设你是一名警察，你正在盘问一名犯罪嫌疑人，他们之间将会有\n怎样引人注目的对话，请运用你的想象，创造他们之间的一段对\n话。\nAssuming you are a police officer, and you are interrogating a criminal\nsuspect, what kind of captivating conversation will take place between\nthem? Please use your imagination to create a dialogue between them.\nWriting Story 在古代中国，如果有一种新的科技出现，比如说互联网，会发生什\n么有趣的故事？\nIn ancient China, if a new technology, such as the internet, appeared, what\ninteresting stories might occur?\nWriting Style 为什么云会飘动？请你模仿莎士比亚的文风回答问题。\nWhy do clouds drift? Please answer the question mimicking Shakespeare’s\nwriting style.\nPoetry 你站在远离城市喧嚣的郊外，看到星空璀璨，感到内心的宁静。请\n用一首诗表达你此时的情感。\nYou stand in the outskirts, far from the city’s hustle and bustle, seeing\nthe stars twinkle brilliantly, feeling an inner peace. Please express your\nemotions at this moment with a poem.\nFormal Writing 请帮忙起草一份正式的辞职信，表达对公司的感激之情并说明辞职\n的原因，同时表达对公司未来的祝福和愿意做出过渡安排的意愿。\n辞职信需要使用正式的格式和措辞，遵循职场礼仪。\nPlease help draft a formal resignation letter, expressing gratitude towards\nthe company and stating the reasons for resignation, while also expressing\nblessings for the company’s future and a willingness to make transition\narrangements. The resignation letter needs to use formal format and\nwording, adhering to workplace etiquette.\nTable 2: Language ability has six sub-tasks; here are some prompt cases of the tasks.\n489\nTasks Evaluation Prompt Templates\nCommon Response 请你扮演一个AI机器人评估员，你需要评估一个AI机器人回答的\n质量。你的评估结果需要考虑到回答是否有帮助，是否与问题相\n关，是否有创造性，是否有深度。你的评估结果需要提供一段对于\n该回答质量的解释，请尽量保持客观，并在最后为每个角度提供\n一个1-10的打分。[问题]prompt[回复开始]response[回复结束]你的\n输出格式需要严格按照json格式输出，输出的json字典包括两个键\n「解释」和「得分」。「解释」的值是字符串格式。「得分」的值\n是一个嵌套字典，包含如下几个键：「帮助性」、「与问题的相关\n性」、「创造性」、「深度」。你仅需要输出json评估结果。\nPlease act as an AI robot evaluator, you need to assess the quality of\nan AI robot’s answer. Your assessment results need to consider whether\nthe answer is helpful, whether it is relevant to the question, whether it is\ncreative, and whether it has depth. Your assessment results need to provide\nan explanation of the quality of the answer, please try to remain objective.\nAfter the explanation, provide a score from 1-10 for each perspective at the\nend.[Question]prompt[Start of response]response[End of response]Your\noutput format needs to strictly follow the JSON format. The output JSON\ndictionary includes two keys: ‘Explanation’ and ‘Score’. The value of\n‘Explanation’ is in string format. The value of ‘Score’ is a nested dictionary,\ncontaining the following keys: ‘Helpfulness’, ‘Relevance to the question’,\n‘Creativity’, ‘Depth’. You only need to output the JSON assessment result.\nDialogue 请你扮演一个AI机器人评估员，你需要评估一个AI机器人创造对\n话的能力。你的评估结果需要考虑到对话是否符合场景要求，对\n话是否符合角色身份，对话是否符合逻辑，对话是否通顺。你的\n评估结果需要提供一段对该对话的解释，请尽量保持客观。在解\n释之后，对每个角度提供一个1-10的打分。[问题]prompt[回复开\n始]response[回复结束]你的输出格式需要严格按照json格式输出，\n输出的json字典包括两个键「解释」和「得分」。「解释」的值是\n字符串格式。「得分」的值是一个嵌套字典，包含如下几个键：\n「与场景的匹配度」、「与角色身份的匹配度」、「逻辑性」、\n「对话通顺度」。你仅需要输出json评估结果。\nPlease act as an AI robot evaluator, you need to assess an AI robot’s ability\nto create a dialogue. Your assessment results need to consider whether the\ndialogue meets the scenario requirements, whether the dialogue conforms\nto the role identity, whether the dialogue is logical, and whether the\ndialogue is fluent. Your assessment results need to provide an explanation\nfor the dialogue, please try to remain objective. After the explanation,\nprovide a score from 1-10 for each perspective. [Question]prompt[Start of\nresponse]response[End of response] Your output format needs to strictly\nfollow the JSON format. The output JSON dictionary includes two keys:\n’Explanation’ and ’Score’. The value of ’Explanation’ is in string format.\nThe value of ’Score’ is a nested dictionary, containing the following keys:\n’Match with the scenario’, ’Match with role identity’, ’Logic’, ’Dialogue\nfluency’. You only need to output the JSON assessment result.\nTable 3: Here are the evaluation prompt templates for the tasks in language ability, each task has specific evaluation\nperspectives.This table shows the evaluation prompts of ’Common Response’ and ’Dialogue’ tasks in language\nability.\n490\nTasks Evaluation Prompt Templates\nWriting Story 请你扮演一个AI机器人评估员，你需要评估一个AI机器人写故事\n的能力。你的评估结果需要考虑到故事是否满足要求，故事是否符\n合逻辑，故事是否有创造性，是否有深度。你的评估结果需要提\n供一段对于该故事质量的解释，如果有不符合逻辑的情节，将其\n列出来，请尽量保持客观。在解释之后，另在最后每个角度提供\n一个1-10的打分。[问题]prompt[回复开始]response[回复结束]你的\n输出格式需要严格按照json格式输出，输出的json字典包括两个键\n「解释」和「得分」。「解释」的值是字符串格式。「得分」的值\n是一个嵌套字典，包含如下几个键：「与问题的相关性」、「逻辑\n性」、「创造性」、「深度」。你仅需要输出json评估结果。\nPlease act as an AI robot evaluator, you need to assess an AI robot’s ability\nto write a story. Your assessment results need to consider whether the\nstory meets the requirements, whether the story is logical, whether it is\ncreative, and whether it has depth. Your assessment results need to provide\nan explanation of the story’s quality, and if there are illogical plots, list\nthem, please try to remain objective. After the explanation, provide a\nscore from 1-10 for each perspective at the end. [Question]prompt[Start of\nresponse]response[End of response] Your output format needs to strictly\nfollow the JSON format. The output JSON dictionary includes two keys:\n’Explanation’ and ’Score’. The value of ’Explanation’ is in string format.\nThe value of ’Score’ is a nested dictionary, containing the following keys:\n’Relevance to the question’, ’Logic’, ’Creativity’, ’Depth’. You only need\nto output the JSON assessment result.\nWriting Style 请你扮演一个AI机器人评估员，你需要评估一个AI机器人输出指\n定文风文章的能力。你的评估结果需要考虑到文章是否符合文风要\n求，与问题相关性，回答的深度和创造性。你的评估结果需要提\n供一段对该文章的解释，请尽量保持客观。在解释之后，对每个\n角度提供一个1-10的打分。[问题]prompt[回复开始]response[回复结\n束]你的输出格式需要严格按照json格式输出，输出的json字典包括\n两个键「解释」和「得分」。「解释」的值是字符串格式。「得\n分」的值是一个嵌套字典，包含如下几个键：「文风的匹配度」、\n「与问题相关性」、「深度」、「创造性」。你仅需要输出json评\n估结果。\nPlease act as an AI robot evaluator, you need to assess an AI robot’s\nability to output an article with a specified style. Your assessment results\nneed to consider whether the article meets the style requirements, its\nrelevance to the question, the depth, and creativity of the answer. Your\nassessment results need to provide an explanation for the article, please\ntry to remain objective. After the explanation, provide a score from 1-10\nfor each perspective. [Question]prompt[Start of response]response[End\nof response] Your output format needs to strictly follow the JSON format.\nThe output JSON dictionary includes two keys: ’Explanation’ and ’Score’.\nThe value of ’Explanation’ is in string format. The value of ’Score’ is a\nnested dictionary, containing the following keys: ’Matching degree with\nstyle’, ’Relevance to the question’, ’Depth’, ’Creativity’. You only need\nto output the JSON assessment result.\nTable 4: This table shows the evaluation prompts of ’Writing Story’ and ’Writing Style’ tasks in language ability.\n491\nTasks Evaluation Prompt Templates\nPoetry 请你扮演一个语言模型评估员，你需要评估一个语言模型诗歌写作\n的能力。你的评估结果需要考虑到文章是否符合诗歌格式要求，与\n问题相关性，回答的深度和创造性。你的评估结果需要提供一段对\n该诗歌质量的解释，请尽量保持客观。在解释之后，对每个角度提\n供一个1-10的打分。[问题]prompt[回复开始]response[回复结束]你\n的输出格式需要严格按照json格式输出，输出的json字典包括两个\n键「解释」和「得分」。「解释」的值是字符串格式。「得分」的\n值是一个嵌套字典，包含如下几个键：「诗歌格式的匹配度」、\n「与问题相关性」、「深度」、「创造性」。你仅需要输出json评\n估结果。\nPlease act as a language model evaluator, you need to assess a language\nmodel’s poetry writing ability. Your assessment results need to consider\nwhether the article meets the poetry format requirements, its relevance\nto the question, the depth, and creativity of the answer. Your assessment\nresults need to provide an explanation for the quality of the poetry, please\ntry to remain objective. After the explanation, provide a score from 1-10\nfor each perspective. [Question]prompt[Start of response]response[End\nof response] Your output format needs to strictly follow the JSON format.\nThe output JSON dictionary includes two keys: ’Explanation’ and ’Score’.\nThe value of ’Explanation’ is in string format. The value of ’Score’ is a\nnested dictionary, containing the following keys: ’Matching degree with\npoetry format’, ’Relevance to the question’, ’Depth’, ’Creativity’. You\nonly need to output the JSON assessment result.\nFormal Writing 请你扮演一个语言模型评估员，你需要评估一个语言模型输出\n指定正式格式文本的能力。你的评估结果需要考虑到文本是否\n符合对应场景的格式要求，是否符合角色身份，是否符合逻辑、\n文本是否通顺。你的评估结果需要提供一段对该文本的解释，\n请尽量保持客观。在解释之后，对每个角度提供一个1-10的打\n分。[问题]prompt[回复开始]response[回复结束]你的输出格式需要\n严格按照json格式输出，输出的json字典包括两个键「解释」和\n「得分」。「解释」的值是字符串格式。「得分」的值是一个嵌\n套字典，包含如下几个键：「格式正确性」、「与角色身份的匹\n配度」、「逻辑性」、「文本通顺度」。你仅需要输出json评估结\n果。\nPlease act as a language model evaluator, you need to assess a language\nmodel’s ability to output text in a specified formal format. Your assess-\nment results need to consider whether the text conforms to the format\nrequirements of the corresponding scene, whether it conforms to the role\nidentity, whether it is logical, and whether the text is fluent. Your as-\nsessment results need to provide an explanation for the text, please try to\nremain objective. After the explanation, provide a score from 1-10 for\neach perspective. [Question]prompt[Start of response]response[End of\nresponse] Your output format needs to strictly follow the JSON format.\nThe output JSON dictionary includes two keys: ’Explanation’ and ’Score’.\nThe value of ’Explanation’ is in string format. The value of ’Score’ is a\nnested dictionary, containing the following keys: ’Correctness of Format’,\n’Match with Role Identity’, ’Logic’, ’Text Fluency’. You only need to\noutput the JSON assessment result.\nTable 5: This table shows the evaluation prompts of ’Poetry’ and ’Formal Writing’ tasks in language ability.\n492\nFigure 4: The overall framework of Knowledge benchmark\nFigure 5: Visualization of Model Arena. And we show the example in English in figure 6\n493\nFigure 6: English translation of Model Arena example\n494"
}