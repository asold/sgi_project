{
  "title": "Pre-training of Graph Augmented Transformers for Medication Recommendation",
  "url": "https://openalex.org/W2965570621",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2794765190",
      "name": "Junyuan Shang",
      "affiliations": [
        "IQVIA (United States)",
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2169177182",
      "name": "Tengfei Ma",
      "affiliations": [
        "IBM (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2232363908",
      "name": "Cao Xiao",
      "affiliations": [
        "IQVIA (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2110385854",
      "name": "Jimeng Sun",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4297733535",
    "https://openalex.org/W2963271116",
    "https://openalex.org/W2799690436",
    "https://openalex.org/W2136922672",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W1986159170",
    "https://openalex.org/W2889599487",
    "https://openalex.org/W2805089815",
    "https://openalex.org/W2963078493",
    "https://openalex.org/W2964068143",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2964199361",
    "https://openalex.org/W2742491462",
    "https://openalex.org/W2964015378",
    "https://openalex.org/W2951441387",
    "https://openalex.org/W2796547658",
    "https://openalex.org/W2138857742",
    "https://openalex.org/W2606780347",
    "https://openalex.org/W2555428947",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2899771611"
  ],
  "abstract": "Medication recommendation is an important healthcare application. It is commonly formulated as a temporal prediction task. Hence, most existing works only utilize longitudinal electronic health records (EHRs) from a small number of patients with multiple visits ignoring a large number of patients with a single visit (selection bias). Moreover, important hierarchical knowledge such as diagnosis hierarchy is not leveraged in the representation learning process. Despite the success of deep learning techniques in computational phenotyping, most previous approaches have two limitations: task-oriented representation and ignoring hierarchies of medical codes. To address these challenges, we propose G-BERT, a new model to combine the power of Graph Neural Networks (GNNs) and BERT (Bidirectional Encoder Representations from Transformers) for medical code representation and medication recommendation. We use GNNs to represent the internal hierarchical structures of medical codes. Then we integrate the GNN representation into a transformer-based visit encoder and pre-train it on EHR data from patients only with a single visit. The pre-trained visit encoder and representation are then fine-tuned for downstream predictive tasks on longitudinal EHRs from patients with multiple visits. G-BERT is the first to bring the language model pre-training schema into the healthcare domain and it achieved state-of-the-art performance on the medication recommendation task.",
  "full_text": "Pre-training of Graph Augmented Transformers for Medication Recommendation\nJunyuan Shang1;3 , Tengfei Ma2 , Cao Xiao1 and Jimeng Sun3\n1Analytics Center of Excellence, IQVIA, Cambridge, MA, USA\n2IBM Research AI, Yorktown Heights, NY , USA\n3Georgia Institute of Technology, Atlanta, GA, USA\njunyuan.shang@iqvia.com, Tengfei.Ma1@ibm.com, cao.xiao@iqvia.com, jsun@cc.gatech.edu\nAbstract\nMedication recommendation is an important\nhealthcare application. It is commonly formulated\nas a temporal prediction task. Hence, most existing\nworks only utilize longitudinal electronic health\nrecords (EHRs) from a small number of patients\nwith multiple visits ignoring a large number\nof patients with a single visit (selection bias).\nMoreover, important hierarchical knowledge such\nas diagnosis hierarchy is not leveraged in the\nrepresentation learning process. To address these\nchallenges, we propose G-BERT, a new model\nto combine the power of G\nraph Neural Networks\n(GNNs) and BERT (Bidirectional Encoder Rep-\nresentations from Transformers) for medical code\nrepresentation and medication recommendation.\nWe use GNNs to represent the internal hierarchical\nstructures of medical codes. Then we integrate\nthe GNN representation into a transformer-based\nvisit encoder and pre-train it on EHR data from\npatients only with a single visit. The pre-trained\nvisit encoder and representation are then ﬁne-tuned\nfor downstream predictive tasks on longitudinal\nEHRs from patients with multiple visits. G-BERT\nis the ﬁrst to bring the language model pre-training\nschema into the healthcare domain and it achieved\nstate-of-the-art performance on the medication\nrecommendation task.\n1 Introduction\nThe availability of massive electronic health records (EHR)\ndata and the advances of deep learning technologies\nhave provided unprecedented resource and opportunity\nfor predictive healthcare, including the computational\nmedication recommendation task. A number of deep\nlearning models were proposed to assist doctors in mak-\ning medication recommendation [Xiao et al., 2018a;\nShang et al., 2019; Baytas et al., 2017; Choi et al., 2018;\nMa et al., 2018]. They often learn representations for medical\nentities (e.g., patients, diagnosis, medications) from patient\nEHR data, and then use the learned representations to predict\nmedications that are suited to the patient’s health condition.\nHeart failure\nCONGESTIVE \nHEART FAILURE\nOther forms of \nheart disease\n1\n2\n3\n4 5\nindex icd-9 description\n1\n2\n3\n4\n5\nroot\n428.0\n428\n420-429\n428.1\nLEFT \nHEART FAILURE\nroot\nFigure 1: Graphical illustration of ICD-9 Ontology.\nTo provide effective medication recommendation, it is im-\nportant to learn accurate representation of medical codes. De-\nspite that various considerations were handled in previous\nworks for improving medical code representations[Ma et al.,\n2018; Baytas et al., 2017; Choi et al., 2018 ], there are two\nlimitations with the existing work:\n1. Selection bias: Data that do not meet training data crite-\nria are discarded before model training. For example, a\nlarge number of patients who only have one hospital visit\nwere discarded from training in [Shang et al., 2019].\n2. Lack of hierarchical knowledge: For medical knowl-\nedge such as diagnosis code ontology (Figure. 1), their\ninternal hierarchical structures were rarely embedded in\ntheir original graph form when incorporated into repre-\nsentation learning.\nTo mitigate the aforementioned limitations, we propose\nG-BERT that combines the pre-training techniques and\ngraph neural networks for better medical code representation\nand medication recommendation. G-BERT is enabled and\ndemonstrated by the following technical contributions:\n1. Pre-training to leverage more data: Pre-training tech-\nniques, such as ELMo [Peters et al., 2018 ], OpenAI\nGPT [Radford et al., 2018 ] and BERT [Devlin et al.,\n2018], have demonstrated a notably good performance\nin various natural language processing tasks. These\ntechniques generally train language models from unla-\nbeled data, and then adapt the derived representations to\ndifferent tasks by either feature-based (e.g. ELMo) or\nﬁne-tuning (e.g. OpenAI GPT, BERT) methods. We de-\nveloped a new pre-training method based on BERT for\nProceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)\n5953\npre-training on each visit of EHR so that the data with\nonly one hospital visit can also be utilized. We revised\nBERT to ﬁt EHR data in both input and pre-training ob-\njectives. To our best knowledge, G-BERT is the ﬁrst\nmodel that leverages Transformers and language model\npre-training techniques in healthcare domain. Compared\nwith other supervised models, G-BERT can utilize dis-\ncarded/unlabeled data more efﬁciently.\n2. Medical ontology embedding with graph neural net-\nworks: We enhance the representation of medical codes\nvia learning medical ontology embedding for each med-\nical codes with graph neural networks. We then in-\nput the ontology embedding into a multi-layer Trans-\nformer [Vaswani et al., 2017 ] for BERT-style pre-\ntraining and ﬁne-tuning.\n2 Related Work\nMedication Recommendation. Medication Recom-\nmendation can be categorized into instance-based and\nlongitudinal recommendation methods [Shang et al., 2019 ].\nInstance-based methods focus on current health conditions.\nAmong them, Leap [Zhang et al., 2017 ] formulates a\nmulti-instance multi-label learning framework and pro-\nposes a variant of sequence-to-sequence model based on\ncontent-attention mechanism to predict combination of\nmedicines given patient’s diagnoses. Longitudinal-based\nmethods leverage the temporal dependencies among clin-\nical events, see [Choi et al., 2016; Xiao et al., 2018b;\nLipton et al., 2015 ]. Among them, RETAIN [Choi et\nal., 2016 ] uses a two-level neural attention model to de-\ntect inﬂuential past visits and signiﬁcant clinical variables\nwithin those visits for improved medication recommendation.\nPre-training Techniques. The goal of pre-training tech-\nniques is to provide model training with good initializations.\nPre-training has been shown extremely effective in various\nareas such as image classiﬁcation [Hinton et al., 2006 ]\nand machine translation [Ramachandran et al., 2016 ].\nThe unsupervised pre-training can be considered as a\nregularizer that supports better generalization from the\ntraining dataset [Erhan et al., 2010 ]. Recently, language\nmodel pre-training techniques such as [Peters et al., 2018;\nRadford et al., 2018; Devlin et al., 2018 ] have shown to\nlargely improve the performance on multiple NLP tasks. As\nthe most widely used one, BERT [Devlin et al., 2018] builds\non the Transformer [Vaswani et al., 2017 ] architecture and\nimproves the pre-training using a masked language model\nfor bidirectional representation. In this paper, we adapt the\nframework of BERT and pre-train our model on each visit of\nthe EHR data to leverage the single-visit data that were not\nﬁt for training in other medication recommendation models.\nGraph Neural Networks (GNN). GNNs are neural net-\nworks that learn node or graph representations from graph-\nstructured data. Various graph neural networks have been\nproposed to encode the graph-structure information, includ-\ning graph convolutional neural networks (GCN) [Kipf and\nWelling, 2017], message passing networks (MPNN) [Gilmer\net al., 2017], graph attention networks (GAT) [Velickovic et\nal., 2017 ]. GNNs have already been demonstrated useful\non EHR modeling [Choi et al., 2017; Shang et al., 2019 ].\nGRAM [Choi et al., 2017] represented a medical concept as\na combination of its ancestors in the medical ontology using\nan attention mechanism. It’s different from G-BERT from\ntwo aspects as described in Section 4.2. Another work worth\nmentioning is GAMENet [Shang et al., 2019 ], which also\nused graph neural network to assist the medication recom-\nmendation task. However, GAMENet has a different moti-\nvation which results in using graph neural networks on drug-\ndrug-interaction graphs instead of medical ontology.\n3 Problem Formalization\nDeﬁnition 1 (Longitudinal Patient Records). In lon-\ngitudinal EHR data, each patient can be represented\nas a sequence of multivariate observations: X(n) =\nfX(n)\n1 ;X(n)\n2 ;\u0001\u0001\u0001 ;X(n)\nT(n) gwhere n2f1; 2;:::;N g, N is the\ntotal number of patients; T(n) is the number of visits of the\nnth patient. Here we choose two main medical code to rep-\nresent each visit Xt = Ct\nd [Ct\nm of a patient which is a union\nset of corresponding diagnoses codes Ct\nd \u001aCd and medica-\ntions codes Ct\nm \u001aCm. For simplicity, we use Ct\n\u0003to indicate\nthe uniﬁed deﬁnition for different type of medical codes and\ndrop the superscript (n) for a single patient whenever it is un-\nambiguous. C\u0003denotes the medical code set and jC\u0003jthe size\nof the code set. c\u00032C\u0003is the medical code.\nDeﬁnition 2 (Medical Ontology). Medical codes are usu-\nally categorized according to a tree-structured classiﬁcation\nsystem such as ICD-9 ontoloy for diagnosis and ATC ontol-\nogy for medication. We use Od;Om to denote the ontology\nfor diagnosis and medication. Similarly, we use O\u0003to indi-\ncate the uniﬁed deﬁnition for different type of medical codes.\nIn detial, O\u0003=\nC\u0003[C\u0003where C\u0003denotes the codes excluding\nleaf codes. For simplicity, we deﬁne two functionpa(\u0001);ch(\u0001)\nwhich accept target medical code and return ancestors’ code\nset and direct child code set.\nProblem Deﬁnition (Medication Recommendation).\nGiven diagnosis codes Ct\nd of the visit at time t, patient\nhistory X1:t = fX1;X2;\u0001\u0001\u0001 ;Xt\u00001g, we want to recommend\nmultiple medications by generating multi-label output\n^yt 2f0; 1gjCmj.\n4 Method\nThe overall framework of G-BERT is described in Figure 2.\nG-BERT ﬁrst derives the initial embedding of medical codes\nfrom medical ontology using graph neural networks. Then, in\norder to fully utilize the rich EHR data, G-BERT constructs\nan adaptive BERT model on the discarded single-visit data\nfor visit representation. Finally we add a prediction layer and\nﬁne-tune the model in the medication recommendation task.\nIn the following we will describe G-BERT in detail. But\nﬁrstly, we give a brief background of BERT especially for\nthe two pre-training objectives which will be later adapted to\nEHR data in Section 4.3.\nProceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)\n5954\nOntology Tree BERT\n……\nVisit \nEmbedding\nOntology \nEmbedding\nFine-tuned\nClassiﬁer\nSigmoid \nOutput\n...\nroot\nleaf\nroot\nleaf\nICD9\nATC\nŏ\nŏ\n[CLS]\nŏ\nŏ\n[CLS]\nŏ\nŏ\nhd\nhm\nod\nom\ncd\ncm\nFigure 2: The framework of G-BERT. It consists of three main parts: ontology embedding, BERT and ﬁne-tuned classiﬁer. Firstly, we\nderive ontology embedding for medical code laid in leaf nodes by cooperating ancestors information by Eq. 1 and 2 based on graph attention\nnetworks (Eq. 3, 4). Then we input set of diagnosis and medication ontology embedding separately to shared weight BERT which is pre-\ntrained using Eq. 6, 7, 8. Finally, we concatenate the mean of all previous visit embeddings and the last visit embedding as input and ﬁne-tune\nthe prediction layers using Eq. 10 for medication recommendation tasks.\nNotation Description\nX(n) longitudinal observ\nations for n-th patient\nCd;Cm diagnoses and\nmedications codes set\nOd;Om diagnoses and\nmedications codes ontology\nC\u0003\u001aO\u0003 non-leaf medical\ncodes of type \u0003\nc\u00032O\u0003 single medical\ncode of type \u0003\npa(c\u0003) function retrie\nve c\u0003’ ancestors’ code\nch(c\u0003) function retrie\nve c\u0003’ direct children’s code\nWe 2RjO\u0003j\u0002d initial medical\nembedding matrix in O\u0003\nHe 2RjO\u0003j\u0002d enhanced medical\nembeddings in stage 1\noc\u0003 2Rd ontology embedding\nin stage 2\n\u000bk\ni;j k-th attention\nbetween nodes i;j\nWk 2Rd\u0002d k-th weight\nmatrix applied to each node\ng(\u0001;\u0001;\u0001) graph aggre\ngator function\nvt\n\u0003 t-th visit\nembedding of type \u0003\n^yt 2f0; 1gjC\u0003j multi-label prediction\nTable 1: Notations used in G-BERT.\n4.1 Background of BERT\nBased on a multi-layer Transformer encoder [Vaswani et al.,\n2017] (The transformer architecture has been ubiquitously\nused in many sequence modeling tasks recently, so we will\nnot introduce the details here), BERT is pre-trained using two\nunsupervised tasks:\n\u000fMasked Language Model. Instead of predicting words\nbased on previous words, BERT randomly selects words\nto mask out and then predicts the original vocabulary IDs\nof the masked words from their (bidirectional) context.\n\u000fNext Sentence Prediction. Many of BERT’s downstream\ntasks are predicting the relationships of two sentences,\nthus in the pre-training phase, BERT has am a binary\nsentence prediction task to predict whether one sentence\nis the next sentence of the other.\nA typical input to BERT is as follows ([Devlin et al., 2018]):\nInput = [CLS] the man went to [MASK] store\n[SEP] he bought a gallon [MASK] milk [SEP]\nLabel = IsNext\nwhere [CLS] is the ﬁrst token of each sentence pair to repre-\nsent the special classiﬁcation embedding, i.e. the ﬁnal state\nof this token is used as the aggregated sequence represen-\ntation for classiﬁcation tasks; [SEP] is used to separate two\nsentences; [MASK] is used to mask out the predicted words\nin the masked language model. Using this form, these inputs\nfacilitate the two tasks described above, and they will also be\nused in our method description in the following section.\n4.2 Input Representation\nThe G-BERT model takes medical codes’ ontology em-\nbeddings as input, and obtains intermediate representations\nfrom a Transformer encoder as the visit embeddings. It is\nthen pre-trained on EHR from patients who only have one\nhospital visit. The derived encoder and visit embedding\nwill be fed into a classiﬁer and ﬁne-tuned to make predictions.\nOntology Embedding\nWe constructed ontology embedding from diagnosis ontol-\nogy Od and medication ontology Om. Since the medical\ncodes in raw EHR data can be considered as leaf nodes\nin these ontology trees, we can enhance the medical code\nembedding using graph neural networks (GNNs) to integrate\nthe ancestors’ information of these codes. Here we perform\na two-stage procedure with a specially designed GNN for\nontology embedding.\nTo start, we assign an initial embedding vector to every\nmedical code c\u0003 2 O\u0003 with a learnable embedding matrix\nWe 2RjO\u0003j\u0002d where dis the embedding dimension.\nStage 1. For each non-leaf node c\u00032\nC\u0003, we obtain its en-\nhanced medical embedding hc\u0003 2Rd as follows:\nhc\u0003 = g(c\u0003;ch(c\u0003);We) (1)\nProceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)\n5955\nwhere g(\u0001;\u0001;\u0001) is an aggregation function which accepts\nthe target medical code c\u0003, its direct child codes ch(c\u0003)\nand initial embedding matrix. Intuitively, the aggregation\nfunction can pass and fuse information in target node from its\ndirect children which result in the more related embedding of\nancestor’ code to child codes’ embedding.\nStage 2. After obtaining enhanced embeddings, we pass the\nenhance embedding matrix He 2RjO\u0003j\u0002d back to get ontol-\nogy embedding for leaf codes c\u00032C\u0003as follows:\noc\u0003 = g(c\u0003;pa(c\u0003);He) (2)\nwhere g(\u0001;\u0001;\u0001) accepts ancestor codes of target medical code\nc\u0003. Here, we use pa(c\u0003) instead of ch(c\u0003), since utilizing\nthe ancestors’ embedding can indirectly associate all medical\ncodes instead of taking each leaf code as independent input.\nThe option for the aggregation function g(\u0001;\u0001;\u0001) is ﬂexible,\nincluding sum, mean. Here we choose the one from graph\nattention networks (GAT) [Velickovic et al., 2017 ], which\nhas shown efﬁcient embedding learning ability on graph-\nstructured tasks, e.g., node classiﬁcation and link prediction.\nIn particular, we implement the aggregation function g(\u0001;\u0001;\u0001)\nas follows (taking stage 2 for an example):\ng(c\u0003;p(c\u0003);He) =\nK\nk\nk=1\n\u001b\n0\n@ X\nj2fc\u0003g[pa(c\u0003)\n\u000bk\ni;jWkhj\n1\nA (3)\nwhere\nkrepresents concatenation\nwhich enables the multi-\nhead attention mechanism, \u001b is a nonlinear activation func-\ntion, Wk 2Rm\u0002d is the weight matrix for input transforma-\ntion, and \u000bk\ni;j are the corresponding k-th normalized attention\ncoefﬁcients computed as follows:\n\u000bk\ni;j = exp\n\u0000\nLeakyReLU(a|[WkhijjWkhj])\n\u0001\nP\nk2Ni\nexp (LeakyReLU(a|[WkhijjWkhk]))\n(4)\nwhere a 2R2m is a learnable weight vector and LeakyReLU\nis a nonlinear function. (we assume m= d=K).\nAs shown in Figure 2, we construct ICD-9 tree for diag-\nnosis and ATC tree for medication using the same structure.\nHere the direction of arrow shows the information ﬂow where\nancestor nodes can get information from their direct children\n(in stage 1) and similarly leaf nodes can get information\nfrom their connected ancestors (in stage 2).\nIt is worth mentioning that our graph embedding method\non medical ontology is different from GRAM [Choi et al.,\n2017] from the following two aspects:\n1. Initialization: we initialize all the node embeddings\nfrom a learnable embedding matrix, while GRAM learns\nthem using Glove from the co-occurrence information.\n2. Updating: we develop a two-step updating function for\nboth leaf nodes and ancestor nodes; while in GRAM,\nonly the leaf nodes are updated (as a combination of\ntheir ancestor nodes and themselves).\nVisit Embedding\nSimilar to BERT, we use a multi-layer Transformer architec-\nture [Vaswani et al., 2017 ] as our visit encoder. The model\ntakes the ontology embedding as input and derive visit em-\nbedding vt\n\u00032Rd for a patient at t-th visit:\nvt\n\u0003= Transformer(f[CLS]g[fot\nc\u0003jc\u00032Ct\n\u0003g)[0] (5)\nwhere [CLS] is a special token as in BERT. It is put in the\nﬁrst position of each visit of type \u0003and its ﬁnal state can be\nused as the representation of the visit. Intuitively, it is more\nreasonable to use Transformers as encoders (multi-head\nattention based architecture) than RNN or mean/sum to\naggregate multiple medical embedding for visit embedding\nsince the set of medical codes within one visit is not ordered.\nIt is worth noting that our Transformer encoder is different\nfrom the original one in the position embedding part. Position\nembedding, as an important component in Transformers and\nBERT, is used to encode the position and order information\nof each token in a sequence. However, one big difference\nbetween language sentences and EHR sequences is that the\nmedical codes within the same visit do not generally have an\norder, so we remove the position embedding in our model.\n4.3 Pre-training\nWe adapted the original BERT model to be more suitable\nfor our data and task. In particular, we pre-train the model\non each EHR visit (within both single-visit EHR sequences\nand multi-visit EHR sequences). We modiﬁed the input\nand pre-training objectives of the BERT model: (1) For the\ninput, we built the Transformer encoder on the GNN outputs,\ni.e. ontology embeddings, for visit embedding. For the\noriginal EHR sequence, it means essentially we combine the\nGNN model with a Transformer to become a new integrated\nencoder. In addition, we removed the position embedding as\nwe explained before. (2) As for the pre-training procedures,\nwe modiﬁed the original pre-training tasks i.e., Masked LM\n(language model) task and Next Sentence prediction task to\nself-prediction task and dual-prediction task. The idea to\nconduct these tasks is to make the visit embedding absorb\nenough information about what it is made of and what it is\nable to predict.\nThus, for the self-prediction task, we want the visit em-\nbedding v1\n\u0003to recover what it is made of, i.e., the input medi-\ncal codes Ct\n\u0003for each visit as follows:\nLse(v1\n\u0003;C1\n\u0003) =\u0000log p(C1\n\u0003jv1\n\u0003)\n= \u0000\nX\nc2C1\n\u0003\nlog p(c1\n\u0003jv1\n\u0003) +\nX\nc2C\u0003nC1\n\u0003\nlog p(c1\n\u0003jv1\n\u0003) (6)\nwe minimize the binary cross entropy lossLs, and in practise,\nSigmoid(f(v\u0003)) should be transformed by applying a fully\nconnected neural network f(\u0001) with one hidden layer. With\nan analogy to the Masked LM task in BERT, we also used\nspeciﬁc symbol [MASK] to randomly replace the original\nmedical code c\u0003 2C1\n\u0003. So there are 15% codes in C\u0003which\nwill be replaced randomly and the model should have the\nProceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)\n5956\nability to predict the masked code based on others.\nLikewise, for the dual-prediction task, since the visit em-\nbedding v\u0003carries the information of medical codes of type\n\u0003, we can further expect it has the ability to do more task-\nspeciﬁc prediction as follows:\nLdu = \u0000log p(C1\ndjv1\nm) \u0000log p(C1\nmjv1\nd) (7)\nwhere we use the same transformation function\nSigmoid(f1(v1\nm)), Sigmoid( f2(v1\nd)) with different weight\nmatrix to transform the visit embedding and optimize the\nbinary cross entropy loss Ldu expanded same as Lse in Eq. 6.\nThis is a direct adaptation of the next sentence prediction\ntask. In BERT, the next sentence prediction task facilitates\nthe prediction of sentence relations, which is a common task\nin NLP. However, in healthcare, most predictive tasks do not\nhave a sequence pair to classify. Instead, we are often inter-\nested in predicting unknown disease or medication codes of\nthe sequence. For example, in medication recommendation,\nwe want to predict multiple medications given only the\ndiagnosis codes. Inversely, we can also predict unknown\ndiagnosis given the medication codes.\nThus, our ﬁnal pre-training optimization objective can sim-\nply be the combination of the aforementioned losses, as\nshown in Eq. 8. It is used to train on EHR data from all\npatients who only have one hospital visits..\nLpr = 1\nN\nNX\nn=1\n((Lse(v1;(n)\nd ;C1;(n)\nd ) +Lse(v1;(n)\nm ;C1;(n)\nm )\n+ L(n)\ndu ))\n(8)\n4.4 Fine-tuning\nAfter obtaining pre-trained visit representation for each visit,\nfor a prediction task on a multi-visit sequence data, we ag-\ngregate all the visit embedding and add a prediction layer for\nthe medication recommendation task. To be speciﬁc, from\npre-training on all visits, we have a pre-trained Transformer\nencoder, which can then be used to get the visit embedding\nv\u001c\n\u0003at time \u001c. The known diagnosis codes Ct\nd at the prediction\ntime tis also represented using the same model as vt\n\u0003. Con-\ncatenating the mean of previous diagnoses visit embeddings\nand medication visit embeddings, also the last diagnoses visit\nembedding, we built an MLP based prediction layer to predict\nthe recommended medication codes as in Equation 9.\nyt = Sigmoid(W1[(1\nt\nX\n\u001c<t\nv\u001c\nd)jj(1\nt\nX\n\u001c<t\nv\u001c\nm)jjvt\nd] +b) (9)\nwhere W1 2RjCmj\u00023d is a learnable transformation matrix.\nGiven the true labels ^yt at each time stamp t, the loss func-\ntion for the whole EHR sequence (i.e. a patient) is\nL= \u0000 1\nT \u00001\nTX\nt=2\n(y|\nt log( ^yt) + (1\u0000y|\nt) log(1\u0000^yt)) (10)\nStats Single-Visit Multi-Visit\n# of patients 30,745 6,350\navg # of visits 1.00 2.36\navg # of dx 39 10.51\navg # of rx 52 8.80\n# of unique dx 1,997 1,958\n# of unique rx 323 145\nTable 2: Statistics of the Data (dx for diagnosis, rx for medication).\n5 Experiment\n5.1 Experimental Setting\nData\nWe used EHR data from MIMIC-III [Johnson et al., 2016 ]\nand conducted all our experiments on a cohort where patients\nhave more than one visit. We utilize data from patients with\nboth single visit and multiple visits in the training dataset as\npre-training data source (multi-visit data are split into visit\nslices). In this work, we transform the drug coding from NDC\nto ATC Third Level for using the ontology information. The\nstatistics of the datasets are summarized in Table 2.\nBaselines\nWe compared G-BERT1 with the following baselines. All\nmethods are implemented in PyTorch [Paszke et al., 2017 ]\nand trained on an Ubuntu 16.04 with 8GB memory and\nNvidia 1080 GPU.\n1. Logistic Regression (LR) is logistic regression with\nL1/L2 regularization. Here we represent sequential mul-\ntiple medical codes by sum of multi-hot vector of each\nvisit. Binary relevance technique [Luaces et al., 2012]\nis used to handle multi-label output.\n2. LEAP [Zhang et al., 2017 ] is an instance-based medi-\ncation combination recommendation method which for-\nmalizes the task in multi-instance and multi-label learn-\ning framework. It utilizes a encoder-decoder based\nmodel with attention mechanism to build complex de-\npendency among diseases and medications.\n3. RETAIN [Choi et al., 2016 ] makes sequential predic-\ntion of medication combination and diseases prediction\nbased on a two-level neural attention model that detects\ninﬂuential past visits and clinical variables within those\nvisits.\n4. GRAM [Choi et al., 2017 ] injects domain knowledge\n(ICD9 Dx code tree) to tanh via attention mechanism.\n5. GAMENet [Shang et al., 2019] is the method to recom-\nmend accuracy and safe medication based on memory\nneural networks and graph convolutional networks by\nleveraging EHR data and Drug-Drug Interaction (DDI)\ndata source. For fair comparison, we use a variant\nof GAMENet without DDI knowledge and procedure\ncodes as input renamed as GAMENet\u0000.\n6. G-BERT is our proposed model which integrated the\nGNN representation into Transformer-based visit en-\ncoder with pre-training on single-visit EHR data.\n1https://github.com/jshang123/G-Bert\nProceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)\n5957\nWe also evaluated 3G-BERT variants for model ablation.\n1. G-BERTG\u0000;P\u0000: We directly use medical embedding\nwithout ontology information as input and initialize the\nmodel’s parameters without pre-training.\n2. G-BERTG\u0000: We directly use medical embedding with-\nout ontology information as input with pre-training.\n3. G-BERTP\u0000: We use ontology information to get ontol-\nogy embedding as input and initialize the model’s pa-\nrameters without pre-training.\nMetrics\nTo measure the prediction accuracy, we used Jaccard Simi-\nlarity Score (Jaccard), Average F1 (F1) and Precision Recall\nAUC (PR-AUC). Jaccard is deﬁned as the size of the intersec-\ntion divided by the size of the union of ground truth set Y(k)\nt\nand predicted set ^Y(k)\nt .\nJaccard = 1\nPN\nk\nPTk\nt 1\nNX\nk\nTkX\nt\njY(k)\nt \\^Y(k)\nt j\njY(k)\nt [^Y(k)\nt j\nwhere N is the number of patients in test set and Tk is the\nnumber of visits of the kth patient.\nImplementation Details\nWe randomly divide the dataset into training, validation\nand testing set in a 0:6 : 0:2 : 0:2 ratio. For G-BERT,\nthe hyperparameters are adjusted on evaluation set: (1)\nGAT part: input embedding dimension as 75, number of\nattention heads as 4; (2) BERT part: hidden dimension as\n300, dimension of position-wise feed-forward networks as\n300, 2 hidden layers with 4 attention heads for each layer.\nSpecially, we alternated the pre-training with 5 epochs and\nﬁne-tuning procedure with 5 epochs for 15 times to stabilize\nthe training procedure.\nFor LR, we use the grid search over typical range of hyper-\nparameter to search the best hyperparameter values which re-\nsult in L1 norm penalty with weight as1:1. For deep learning\nmodels, we implemented RNN using a gated recurrent unit\n(GRU) [Cho et al., 2014 ] and utilize dropout with a prob-\nability of 0.4 on the output of embedding. We test several\nembedding choice for baseline methods and determine the di-\nmension for medical embedding as 300 and thershold for ﬁ-\nnal prediction as 0.3 for better performance. Training is done\nthrough Adam [Kingma and Ba, 2014 ] at learning rate 5e-4.\nWe ﬁx the best model on evaluation set within 100 epochs\nand report the performance in test set.\n5.2 Results\nTable. 3 compares the performance on the medication rec-\nommendation task. For variants of G-BERT, G-BERTG\u0000;P\u0000\nperforms worse compared with G-BERTG\u0000 and G-BERTP\u0000\nwhich demonstrate the effectiveness of using ontology infor-\nmation to get enhanced medical embedding as input and em-\nploy an unsupervised pre-training procedure on larger abun-\ndant data. Incorporating both hierarchical ontology infor-\nmation and pre-training procedure, the end-to-end model\nMethods Jaccard PR-AUC F1 # of\nparameters\nLR 0.4075 0.6716 0.5658 -\nGRAM 0.4176 0.6638 0.5788 3,763,668\nLEAP 0.3921 0.5855 0.5508 1,488,148\nRETAIN 0.4456 0.6838 0.6064 2,054,869\nGAMENet\u0000 0.4401 0.6672 0.5996 5,518,646\nGAMENet 0.4555 0.6854 0.6126 5,518,646\nG-BERTG\u0000;P\u0000 0.4186 0.6649 0.5796 2,634,145\nG-BERTG\u0000 0.4299 0.6771 0.5903 2,634,145\nG-BERTP\u0000 0.4236 0.6704 0.5844 3,034,045\nG-BERT 0.4565 0.6960 0.6152 3,034,045\nTable 3: Performance on Medication Recommendation Task.\nG-BERT has more capacity and achieve comparable results\nwith others.\nAs for baseline models, LR and Leap are worse than\nour most basic model (G-BERT G\u0000;P\u0000) in terms of most\nmetrics. Comparing G-BERTP\u0000 and GRAM, which both\nused medical ontology information without pre-training, the\nscores of our G-BERTP\u0000 is slightly higher in all metrics.\nThis can demonstrate the validness of using Transformer\nencoders and the speciﬁc prediction layer for medication\nrecommendation. Our ﬁnal model G-BERT is also better\nthan the attention based model, RETAIN, and the recently\npublished state-of-the-art model, GAMENet. Speciﬁcally,\neven adding the extra information of DDI knowledge and\nprocedure codes, GAMENet still performs worse than\nG-BERT.\nIn addition, we visualized the pre-training medical code\nembeddings of G-BERTG\u0000 and G-BERT to show the ef-\nfectiveness of ontology embedding using online embed-\nding projector 2 shown in (https://raw.githubusercontent.com/\njshang123/G-Bert/master/saved/tsne.png/).\n6 Conclusion\nIn this paper we proposed a pre-training model named\nG-BERT for medical code representation and medication rec-\nommendation. To our best knowledge, G-BERT is the ﬁrst\nthat utilizes language model pre-training techniques in health-\ncare domain. It adapted BERT to the EHR data and integrated\nmedical ontology information using graph neural networks.\nBy additional pre-training on the EHR from patients who only\nhave one hospital visit which are generally discarded before\nmodel training, G-BERT outperforms all baselines in predic-\ntion accuracy on medication recommendation task. One di-\nrection for the future work is to add more auxiliary and struc-\ntural tasks to improve the ability of code representaion. An-\nother direction may be to adapt our model to be suitable for\neven larger datasets with more heterogeneous modalities.\nAcknowledgments\nThis work was supported by the National Science Founda-\ntion award IIS-1418511, CCF-1533768 and IIS-1838042, the\nNational Institute of Health award 1R01MD011682-01 and\nR56HL138415.\n2https://projector.tensorﬂow.org/\nProceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)\n5958\nReferences\n[Baytas et al., 2017] Inci M. Baytas, Cao Xiao, Xi Zhang,\nFei Wang, Anil K. Jain, and Jiayu Zhou. Patient subtyp-\ning via time-aware lstm networks. In Proceedings of the\n23rd ACM SIGKDD International Conference on Knowl-\nedge Discovery and Data Mining, 2017.\n[Cho et al., 2014] Kyunghyun Cho, Bart Van Merri ¨enboer,\nDzmitry Bahdanau, and Yoshua Bengio. On the proper-\nties of neural machine translation: Encoder-decoder ap-\nproaches. arXiv preprint arXiv:1409.1259, 2014.\n[Choi et al., 2016] Edward Choi, Mohammad Taha Ba-\nhadori, Jimeng Sun, Joshua Kulas, Andy Schuetz, and\nWalter Stewart. Retain: An interpretable predictive model\nfor healthcare using reverse time attention mechanism.\nIn Advances in Neural Information Processing Systems,\npages 3504–3512, 2016.\n[Choi et al., 2017] Edward Choi, Mohammad Taha Ba-\nhadori, Le Song, Walter F Stewart, and Jimeng Sun. Gram:\nGraph-based attention model for healthcare representation\nlearning. In SIGKDD, 2017.\n[Choi et al., 2018] Edward Choi, Cao Xiao, Walter Stew-\nart, and Jimeng Sun. Mime: Multilevel medical embed-\nding of electronic health records for predictive healthcare.\nIn S. Bengio, H. Wallach, H. Larochelle, K. Grauman,\nN. Cesa-Bianchi, and R. Garnett, editors, Advances in\nNeural Information Processing Systems 31, pages 4547–\n4557. Curran Associates, Inc., 2018.\n[Devlin et al., 2018] Jacob Devlin, Ming-Wei Chang, Ken-\nton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805, 2018.\n[Erhan et al., 2010] Dumitru Erhan, Yoshua Bengio, Aaron\nCourville, Pierre-Antoine Manzagol, Pascal Vincent, and\nSamy Bengio. Why does unsupervised pre-training help\ndeep learning? Journal of Machine Learning Research,\n11(Feb):625–660, 2010.\n[Gilmer et al., 2017] J. Gilmer, S.S. Schoenholz, P.F. Riley,\nO. Vinyals, and G.E. Dahl. Neural message passing for\nquantum chemistry. In ICML, 2017.\n[Hinton et al., 2006] Geoffrey E Hinton, Simon Osindero,\nand Yee-Whye Teh. A fast learning algorithm for deep\nbelief nets. Neural computation, 18(7):1527–1554, 2006.\n[Johnson et al., 2016] Alistair EW Johnson, Tom J Pollard,\nLu Shen, H Lehman Li-wei, Mengling Feng, Mohammad\nGhassemi, Benjamin Moody, Peter Szolovits, Leo An-\nthony Celi, and Roger G Mark. Mimic-iii, a freely ac-\ncessible critical care database. Scientiﬁc data, 3:160035,\n2016.\n[Kingma and Ba, 2014] Diederik P. Kingma and Jimmy Ba.\nAdam: A method for stochastic optimization. CoRR,\nabs/1412.6980, 2014.\n[Kipf and Welling, 2017] Thomas N Kipf and Max Welling.\nSemi-supervised classiﬁcation with graph convolutional\nnetworks. In ICLR, 2017.\n[Lipton et al., 2015] Zachary C Lipton, David C Kale,\nCharles Elkan, and Randall Wetzel. Learning to diag-\nnose with lstm recurrent neural networks. arXiv preprint\narXiv:1511.03677, 2015.\n[Luaces et al., 2012] Oscar Luaces, Jorge D´ıez, Jos´e Barran-\nquero, Juan Jos´e del Coz, and Antonio Bahamonde. Binary\nrelevance efﬁcacy for multilabel classiﬁcation. Progress in\nArtiﬁcial Intelligence, 1(4):303–313, 2012.\n[Ma et al., 2018] Tengfei Ma, Cao Xiao, and Fei Wang.\nHealth-atm: A deep architecture for multifaceted patient\nhealth record representation and risk prediction. In Pro-\nceedings of the 2018 SIAM International Conference on\nData Mining, pages 261–269. SIAM, 2018.\n[Paszke et al., 2017] Adam Paszke, Sam Gross, Soumith\nChintala, Gregory Chanan, Edward Yang, Zachary De-\nVito, Zeming Lin, Alban Desmaison, Luca Antiga, and\nAdam Lerer. Automatic differentiation in pytorch. 2017.\n[Peters et al., 2018] Matthew Peters, Mark Neumann, Mohit\nIyyer, Matt Gardner, Christopher Clark, Kenton Lee, and\nLuke Zettlemoyer. Deep contextualized word representa-\ntions. In NAACL, volume 1, pages 2227–2237, 2018.\n[Radford et al., 2018] Alec Radford, Karthik Narasimhan,\nTim Salimans, and Ilya Sutskever. Improving language\nunderstanding by generative pre-training. 2018.\n[Ramachandran et al., 2016] Prajit Ramachandran, Peter J\nLiu, and Quoc V Le. Unsupervised pretraining\nfor sequence to sequence learning. arXiv preprint\narXiv:1611.02683, 2016.\n[Shang et al., 2019] Junyuan Shang, Cao Xiao, Tengfei Ma,\nHongyan Li, and Jimeng Sun. Gamenet: Graph augmented\nmemory networks for recommending medication combi-\nnation. AAAI, 2019.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. In Advances in Neural Information Processing Sys-\ntems, pages 5998–6008, 2017.\n[Velickovicet al., 2017] Petar Velickovic, Guillem Cucurull,\nArantxa Casanova, Adriana Romero, Pietro Lio, and\nYoshua Bengio. Graph attention networks. arXiv preprint\narXiv:1710.10903, 1(2), 2017.\n[Xiao et al., 2018a] Cao Xiao, Edward Choi, and Jimeng\nSun. Opportunities and challenges in developing deep\nlearning models using electronic health records data: a\nsystematic review. Journal of the American Medical In-\nformatics Association, 2018.\n[Xiao et al., 2018b] Cao Xiao, Tengfei Ma, Adji B. Dieng,\nDavid M. Blei, and Fei Wang. Readmission prediction via\ndeep contextual embedding of clinical concepts. PLOS\nONE, 13(4):1–15, 04 2018.\n[Zhang et al., 2017] Yutao Zhang, Robert Chen, Jie Tang,\nWalter F Stewart, and Jimeng Sun. Leap: Learning to pre-\nscribe effective and safe treatment combinations for mul-\ntimorbidity. In SIGKDD, pages 1315–1324, 2017.\nProceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)\n5959",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7841219902038574
    },
    {
      "name": "Encoder",
      "score": 0.6669443249702454
    },
    {
      "name": "Machine learning",
      "score": 0.5964664220809937
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5862452983856201
    },
    {
      "name": "Transformer",
      "score": 0.5483449697494507
    },
    {
      "name": "Inductive bias",
      "score": 0.5254030823707581
    },
    {
      "name": "Feature learning",
      "score": 0.5253010392189026
    },
    {
      "name": "Graph",
      "score": 0.4728322923183441
    },
    {
      "name": "Task analysis",
      "score": 0.4313296675682068
    },
    {
      "name": "Health records",
      "score": 0.4300568997859955
    },
    {
      "name": "Training set",
      "score": 0.41254910826683044
    },
    {
      "name": "Task (project management)",
      "score": 0.3547039031982422
    },
    {
      "name": "Multi-task learning",
      "score": 0.3325895667076111
    },
    {
      "name": "Health care",
      "score": 0.3170912265777588
    },
    {
      "name": "Theoretical computer science",
      "score": 0.1475694477558136
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Economic growth",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I130701444",
      "name": "Georgia Institute of Technology",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210108991",
      "name": "IQVIA (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1341412227",
      "name": "IBM (United States)",
      "country": "US"
    }
  ]
}