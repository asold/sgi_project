{
  "title": "Dependency-Based Relative Positional Encoding for Transformer NMT",
  "url": "https://openalex.org/W3086917037",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2568703091",
      "name": "Yutaro Omote",
      "affiliations": [
        "Ehime University"
      ]
    },
    {
      "id": "https://openalex.org/A2040756077",
      "name": "Akihiro Tamura",
      "affiliations": [
        "Ehime University",
        "Doshisha University"
      ]
    },
    {
      "id": "https://openalex.org/A2103085203",
      "name": "Takashi Ninomiya",
      "affiliations": [
        "Ehime University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963648186",
    "https://openalex.org/W6780226713",
    "https://openalex.org/W2758137671",
    "https://openalex.org/W2949745489",
    "https://openalex.org/W2970247882",
    "https://openalex.org/W2116957398",
    "https://openalex.org/W6696775231",
    "https://openalex.org/W2963661253",
    "https://openalex.org/W2594047108",
    "https://openalex.org/W6737778391",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W222053410",
    "https://openalex.org/W2124807415",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2946028745",
    "https://openalex.org/W2798833929",
    "https://openalex.org/W2123442489",
    "https://openalex.org/W6732150968",
    "https://openalex.org/W2127863960",
    "https://openalex.org/W2991516293",
    "https://openalex.org/W2963876447",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2798569372",
    "https://openalex.org/W6679436768",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2771146759",
    "https://openalex.org/W2884083742",
    "https://openalex.org/W2945059185",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2576482813",
    "https://openalex.org/W2964265128",
    "https://openalex.org/W2963073938"
  ],
  "abstract": "本稿では，原言語文の係り受け木に対する単語間の相対的位置関係を Transformer エンコーダ内の Self Attention でエンコードする新たなニューラル機械翻訳モデルを提案する．具体的には，提案モデルでは，原言語文を係り受け解析した結果得られる係り受け木中の 2 単語間の相対的な深さを埋め込んだベクトルを Transformer エンコーダ内の Self Attention に付加する．ASPEC の日英及び英日翻訳タスクにおいて，原言語文の係り受け構造を用いない従来の Transformer モデルよりも翻訳精度が高いことを示す．特に，日英翻訳においては 0.37 ポイント BLEU スコアが上回ることを確認した．",
  "full_text": "IA0231_06omoote (2020-05-27 11:02)\nҰൠ࿦จ\nྀͨ͠\nTransformer NMT\nදɹ༔ଠ࿠†༟†,††ɹɹਸ†\nΛ Transformer\nΤϯίʔμ಺ͷSelf Attentionց຋༁ϞσϧΛ\nՌಘΒΕ\nΓड͚໦தͷ 2ΜͩϕΫτϧΛ Transformer Τ\nϯίʔμ಺ͷSelf Attention ʹ෇Ճ͢ΔɽASPECͼӳ೔຋༁λεΫʹ͓͍\n଄Λ༻͍ͳ͍ैདྷͷ Transformer ϞσϧΑΓ΋຋༁ਫ਼౓\n͢ɽಛʹɼ೔ӳ຋༁ʹ͓͍ͯ͸0.37 ϙΠϯτBLEU είΞ্͕ճΔ\nೝͨ͠ɽ\nΩʔϫʔυݱ\nDependency-Based Relative Positional Encoding for\nTransformer NMT\nYutaro Omote†, Akihiro Tamura†,†† and Takashi Ninomiya†\nIn this paper, we propose a novel model for transformer neural machine transla-\ntion that incorporates syntactic distances between two source words into the relative\nposition representations of a self-attention mechanism. In particular, the proposed\nmodel encodes pair-wise relative depths on a source dependency tree, which are the\ndi\u000berences between the depths of two source words, in the encoder's self-attention.\nExperiments show that our proposed model outperformed non-syntactic Transformer\nNMT baselines on the Asian Scienti\fc Paper Excerpt Corpus Japanese-to-English and\nEnglish-to-Japanese translation tasks. In particular, our proposed model achieved a\n0.37 point gain in BLEU on the Japanese-to-English task.\nKey Words: Neural Machine Translation, Dependency Structures, Relative Positional\nEncoding\n†Պ, Graduate School of Science and Engineering, Ehime University\n††ֶPresently with Doshisha University\nIA0231_06omoote (2020-05-27 11:02)\nॲཧɹ Vol. 27 No. 2 June 2020\n1 ͸͡Ίʹ\nۙ\nց຋༁(Neural Machine Translation; NMT)͍\n͓ͯ͠Γɼ ओྲྀͱͳ͍ͬͯΔɽNMTΛଊ͑\nΔSelf Attention଄Λ༻͍ͨTransformer (Vaswani, Shazeer, Parmar, Uszkoreit, Jones,\nGomez, Kaiser, and Polosukhin 2017)ͮ͘ NMT ͕ state-of-the-art ͷਫ਼౓Λୡ੒͠ɼ஫໨\nΛूΊ͍ͯΔɽTransformer NMTΈχϡʔϥϧωοτϫʔΫ (Convolutional\nNeural Network; CNN)ͮ͘NMT (Gehring, Auli, Grangier, Yarats, and Dauphin 2017) ΍\nχϡʔϥϧωοτϫʔΫ (Recurrent Neural Network; RNN)ͮ͘ NMT (Sutskever,\nVinyals, and Le 2014; Luong, Pham, and Manning 2015)จʹ\n࿈౓Λ Attention (Self Attention)͢Δɽ·ͨɼ\nͷจதʹ͓͚ΔҐஔ৘ใ͸ Positional Encodingʹ෇ਵ\nຖͷฒྻॲཧΛՄೳͱ͍ͯ͠Δɽ Shaw Β (Shaw, Uszkoreit, and\nVaswani 2018)ͷҐஔ৘ใͱͯ͠ɼ2ͷ৘ใΛ\nSelf Attentionྀ͢Δ͜ͱͰTransformer NMTͨ͠ɽ\nց຋༁΍ NMTߏ\n༻͢Δ͜ͱͰ຋༁ਫ਼౓͕վળ͞Ε͓ͯΓ (Ding and\nPalmer 2005; Chen, Wang, Utiyama, Liu, Tamura, Sumita, and Zhao 2017; Eriguchi, Tsuruoka,\nand Cho 2017; Wu, Zhang, Zhang, Yang, Li, and Zhou 2018)೥ɼTransformer NMT ʹ͓͍\n༻͞Ε͖͍ͯͯΔɽ͔͠͠ɼWu Β (Wu et al. 2018) ΍ Zhang Β (Zhang, Li,\nFu, and Zhang 2019)Ͱ͸ Transformer NMTྀ͢\nΛ෇Ճ͓ͯ͠ΓɼTransformer NMTମ͸վྑ͞Ε͍ͯͳ͍ɽ\nͮ͘ 2ͷ૬ରతҐஔ৘ใΛ Transformer Τϯ\nίʔμͷ Self Attentionྀ͢Δ৽ͨͳ Transformer NMTମతʹ͸ɼ\nShaw Β(Shaw et al. 2018)଄ʹ\nΈϕΫτϧʹ෇ਵͤ͞Δɽ\nͮ͘ 2ྻʹର\nจΛੜ੒Ͱ͖Δ\n଴͞ΕΔɽ\n੒͞ΕͨASPEC (Asian Scienti\fc Paper Excerpt Corpus) (Nakazawa,\nYaguchi, Uchimoto, Utiyama, Sumita, Kurohashi, and Isahara 2016) σʔλΛ༻͍ͨӳ೔͓Αͼ\nྀ͢ΔఏҊϞσϧ͸ɼैདྷ\nྀ͠ͳ͍ Transformer NMT Ϟσϧ (Vaswani et al. 2017; Shaw et al. 2018)\n282\nIA0231_06omoote (2020-05-27 11:02)\nྀͨ͠ Transformer NMT\n͢ɽಛʹɼ೔ӳ຋༁ʹ͓͍ͯ͸0.37 ϙΠϯτBLEU\nೝͨ͠ɽ\n੒͸ҎԼͷ௨ΓͰ͋Δɽ2ʹ͍ͭͯड़΂ɼ3 અͰ͸ఏҊϞσϧ\nͷϕʔεͱͳΔैདྷͷTransformer NMT Ϟσϧ(Vaswani et al. 2017; Shaw et al. 2018) ʹ͍ͭ\nͯઆ໌͢Δɽ4ͷఏҊϞσϧʹ͍ͭͯड़΂Δɽ5ͱͦͷ\n͍ɼ6ͷ՝୊ʹ͍ͭͯड़΂Δɽ\n2ڀݚ\n଄ͳ\n༻͢Δ͜ͱͰNMT ͷ຋༁ਫ਼౓͕վળ͞Ε͖ͯͨɽ\n͕͋Δɽྫ͑͹ɼ Aharoni ͱ Goldberg (Aharoni and\nGoldberg 2017)଄ɼEriguchi Β (Eriguchi, Hashimoto, and Tsuruoka 2016)ɼ\nMa Β (Ma, Tamura, Utiyama, Zhao, and Sumita 2018)ɼCurrey ͱHea\feld (Currey and Hea\feld\n2019)ɼWatanabe Β (Watanabe, Tamura, and Ninomiya 2017)଄Λ NMT Ͱ\n଄ʹ஫໨͍ͯ͠Δͱ͍͏఺Ͱ͜ΕΒͷै\nͱҟͳΔɽ\n͕͋ΔɽChen Β (Chen et al. 2017)ޠݴݪ\nΛ CNN ʹΑͬ\nΈɼRNNͮ͘ NMT༻͢ΔϞσϧΛఏҊ͍ͯ͠Δɽ·\nͨɼSennrich ͱHaddow (Sennrich and Haddow 2016)ϥϕϧΛͦ\nͤ͞Δ͜ͱ\nͰɼRNNͮ͘NMTྀ͍ͯ͠ΔɽEriguchi Β\n(Eriguchi et al. 2017) ͸ɼRNNͮ͘ NMT͏ Recurrent\nNeural Network Grammar (Dyer, Kuncoro, Ballesteros, and Smith 2016) Λಋೖ͢Δ͜ͱͰɼ໨\n͸ɼRNNͮ͘NMT Ϟσ\nͰఏҊ͢ΔϞσϧ͸ Transformer Ϟσϧಛ\n଄Ͱ͋ΔSelf Attention఺\nͰҟͳΔɽ\n೥ɼTransformer NMTྀ͢ΔϞσϧ͕͍͔ͭ͘ఏҊ͞Ε͖͍ͯͯ\nΔɽWu Β (Wu et al. 2018) ͸ɼNMTΓड͚໦Λ\nମతʹ͸ɼNMTྻΛೖྗ͢\nޠݴݪ\nྀ͍ͯ͠Δɽ·ͨɼσίʔμଆͰ͸ɼNMT଄ղੳΛ\n283\nIA0231_06omoote (2020-05-27 11:02)\nॲཧɹ Vol. 27 No. 2 June 2020\n͏RNNྀ͠ͳ͕Βग़ྗจΛੜ੒͍ͯ͠Δɽ\nZhang Β (Zhang et al. 2019)จ\nՌΛNMTޠݴݪ\nྀ͍ͯ͠ΔɽWuΒͷख๏΍Zhang Βͷख๏͸RNNͮ͘NMT Ϟσ\nϧ͚ͩͰͳ͘ɼTransformerͮ͘NMTͰ͸\nTransformer NMTΛ༻͍͍ͯΔ͕ɼTransformerମ͸վྑ͞Ε͍ͯͳ͍ɽMa Β (Ma,\nTamura, Utiyama, Sumita, and Zhao 2019) ͸ɼShen Β (Shen, Lin, Jacob, Sordoni, Courville,\nand Bengio 2018)଄ղੳͷͨΊʹ༻͍ͨ Neural Syntax Distance (NSD)଄\n଄ʹର͢ΔNSD Λ༻͍ͯTransformer NMT ͷ຋༁ਫ਼౓Λվળ͍ͯ͠Δɽ\n͢\nΔPositional EncodingͷఏҊϞσϧͰ͸ɼShawͮ\nͮ͘2ͷ૬ରతͳҐஔ৘ใΛTransformerΤϯίʔμͷSelf Attention\nྀ͍ͯ͠Δɽ\n·ͨɼShaw Β (Shaw et al. 2018) ͷଞʹ΋ɼTransformer NMTྻ৘ใΛվྑ͢Δ\n͕͋ΔɽChen Β (Chen, Wang, Utiyama, and Sumita 2019)\nͷฒͼସ͑ͷ৘ใΛPositional EncodingΛTransformer NMT\n͸Transformer NMT ϞσϧͰจதͷ୯\nจ\nͰ͸ɼPositional Encoding ʹΑΔઈରతҐஔ৘ใΛ\nͰ͸Self Attentionྀ͞ΕΔ૬ରతҐஔ৘ใΛվྑ͍ͯ͠Δ\n఺ͰҟͳΔɽ\n3 ैདྷͷTransformer NMTϞσϧ\nຊઅͰ͸ɼ3.1ຊతͳTransformer NMT Ϟσϧʹ͍ͭ\nͯड़΂ɼ3.2ΛSelf Attention Ͱଊ͑ΔTransformer\nNMT Ϟσϧʹ͍ͭͯड़΂Δɽ\n3.1 Transformer NMT\nTransformer NMT࿈౓Λଊ͑Δ Self AttentionͭΤϯίʔμ\n੒͞ΕΔ NMTจ͔Βத\nจΛ༧ଌ͠ɼग़ྗ͢Δɽσίʔ\nମత\nΛσίʔμʹೖྗ͠\n284\nIA0231_06omoote (2020-05-27 11:02)\nྀͨ͠ Transformer NMT\nͯ2໨Λ༧ଌ͢Δɽଓ͍ͯɼ༧ଌͨ͠จ಄͔Β 2ݴ\nจΛσίʔμʹೖྗʢshifted rightจΛೖྗʣͯ͠3໨Λ༧ଌ͢Δɼͱ͍ͬͨ\n͏ɽTransformer NMTཁਤΛਤ1͢ɽ\nTransformer NMT ͸ɼΤϯίʔμϨΠϠͱσίʔμϨΠϠ͕ͦΕͧΕෳ਺૚ελοΫ͞Εͨ\nྻ\nΛද͢\nɼPositional Encodingମతʹ͸ɼ\nͷจʹ͓͚ΔઈରతͳҐஔ৘ใΛΤϯίʔυ͠\nਤ 1 Transformer NMTཁਤ\n285\nIA0231_06omoote (2020-05-27 11:02)\nॲཧɹ Vol. 27 No. 2 June 2020\nྻPE ΛՃ͑ΔɽPE੒෼͸ҟͳΔप೾਺ͷsinɼcosग़ͨ͠\n΋ͷͰ͋Δɽ\nP E(pos, 2i) = sin(pos/100002i/dmodel ) (1)\nP E(pos, 2i + 1) = cos(pos/100002i/dmodel ) (2)\n͜͜ͰɼdmodelɼposͷҐஔɼiຒ\nྻʹ PE ΛՃ͑ͨ΋ͷ͕ɼୈ 1 ૚໨ͷΤϯίʔμϨΠϠ΍σίʔμϨΠϠͷೖྗͱ\nͳΔɽ\n࿈౓Λଊ͑ΔSelf\nAttentionɼҐஔ͝ͱͷϑΟʔυϑΥʔϫʔυωοτϫʔΫ (Feed Forward Network; FFN) ͷ 2\nޠݴ\n࿈౓Λଊ͑ΔϚεΩϯά෇͖Self Attentionจͷ\n࿈౓Λଊ͑Δ Attention (Source-Target Attention)ɼҐஔ͝ͱͷ FFN ͷ 3 ͭͷαϒ\n੒͞Ε͍ͯΔɽ\n઀ଓ (He, Zhang, Ren, and Sun 2016)ʹ Layer Nor-\nmalization (Ba, Kiros, and Hinton 2016) ͕ద༻͞ΕΔɽLayer Normalization਺Λ\nLayerNorm ɼ ԼҐͷαϒϨΠϠ͔Βͷग़ྗΛx਺ΛSubLayer\nͱ͢ΔͱɼLayerNorm (x + SubLayer(x))ͷαϒϨΠϠͷग़ྗͱͳΔɽ\nSelf AttentionͱSource-Target Attention͸Multi-Head Attention͞ΕΔɽMulti-\nHead AttentionͰ͸ɼ ·ͣɼ3 ͭͷೖྗϕΫτϧq, k, v1 ∈ R1×dmodelྻWQ\ni , W K\ni , W V\ni ∈\nRdmodel×dz (i = 1, . . . , h) ʹΑΓɼdmodel͔Βdzɼhͷ಺ੵAttention\n͢Δɽ͜͜ͰɼdmodelͰ͋Γɼdz = dmodel/h Ͱ͋Δɽ\n·ͨɼͦΕͧΕͷ಺ੵAttention Λϔου (Headi (i = 1, . . . , h))Ϳɽ\nHeadi = Attention(q′, k′, v′) (3)\nAttention(q′, k′, v′) = softmax\n(q′k′T\n√dz\n)\nv′ (4)\nq′ = qWQ\ni , k′ = kWK\ni , v′ = vWV\ni (5)\nྻWo ∈ Rdmodel×dmodel͕Multi-Head\nAttention Ͱ͋Δɽ\nMultiHead (q, k, v) = Concat(Head1, . . . , Head h)WO (6)\n1ΓɼTransformer࿦จ (Vaswani et al. 2017)ϕΫτϧͱ͠\nͯѻ͏ɽ\n286\nIA0231_06omoote (2020-05-27 11:02)\nྀͨ͠ Transformer NMT\nΤϯίʔμͷSelf Attentionࣜ6) ͷq, k, vྻx1, . . . , xn Λ\nྻΛ WQ, W K, W V ͱͨ͠ͱ͖ɼҎԼͷՙॏ࿨\n͢Δɽ\nzi =\nn∑\nj=1\nαijxjWV (7)\n͜͜Ͱɼz1, . . . , zn ͕Self Attention਺αij਺Λ\n͞ΕΔɽ\nαij = exp (eij)∑n\nk=1 exp (eik) (8)\n·ͨɼeij͞ΕΔɽ\neij = (xiWQ)(xjWK)T\n√dz\n(9)\n͜͜Ͱɼdz ͸zi਺Ͱ͋Δɽ\nσίʔμͷ Self Attentionࣜ6) ͷ q, k, vྻΛ༻͍Δɽͨͩ\nΛ஌Δ͜ͱ͸Ͱ͖ͳ͍ɽͭ·Γɼલํ\nʹ͓͍ͯ΋ɼਪ࿦\nྀ͠\nͳ͍Α͏ʹϚεΩϯάͨ͠ϚεΩϯά෇͖Self Attentionମతʹ͸ɼϚεΩϯά෇\n͖Self Attentionࣜ9)͢Δɽ\neij =\n\n\n\n(xiWQ)(xjWK)T\n√dz\n(i ≥ j)\n−∞ (otherwise)\n(10)\nࣜ10)͢Δ͜ͱͰɼαij࿈౓Λ\n਺αij (i < j ) ͸0 ͱͳΔɽ\nαij =\n\n\n\nexp (eij)∑n\nk=1 exp (eik) (i ≥ j)\n0 (otherwise)\n(11)\nσίʔμͷ Source-Target Attention Ͱ͸ɼq ʹσίʔμͷ಺෦ঢ়ଶɼk ͱv ʹ͸Τϯίʔμ\n͢ΔɽΤϯ\n֤FFN ͸ೖྗx͏ɽ\nF F N(x) = max(0, xW1 + b1)W2 + b2 (12)\n͜͜ͰɼW1 ∈ Rdmodel×dff , W2 ∈ Rdff ×dmodelྻɼdff ͸FNN਺ɼb1, b2\n287\nIA0231_06omoote (2020-05-27 11:02)\nॲཧɹ Vol. 27 No. 2 June 2020\nͰ͋Δɽ\nऴͷσίʔμϨΠϠͷग़ྗ(h1, . . . , hn)ྻWout ∈ Rdmodel×dout ʹ\n֬\n཰෼෍ΛಘΔɽ͜͜Ͱɼdoutޠyi཰\n෼෍͸ҎԼͷ௨ΓͰ͋Δɽ\np(yi) = softmax(hiWout) (13)\nsoftmax(o) = exp (ok)\n∑dout\nk=1 exp (ok)\n(14)\nจy1, . . . , ym͖ͮɼྫ͑͹greedy ΞϧΰϦζϜ΍Ϗʔ\n౳ʹΑΓੜ੒͢Δɽ\n3.2Λ༻͍ͨTransformer NMT\nຊઅͰ͸ɼ2ΛΤϯίʔμ͓Αͼσίʔμ಺ͷ Self\nAttention Ͱଊ͑ΔTransformer NMT Ϟσϧ(Shaw et al. 2018) Λઆ໌͢ΔɽShaw ΒͷϞσϧ\nޠwiޠwjΛϕΫτϧaV\nij, aK\nij ∈ Rdz͠ɼα\nࣜ\n7)Λ༻͍Δɽ\nzi =\nn∑\nj=1\nαij(xjWV + aV\nij) (15)\nࣜ9)Λ༻͍Δ͜ͱͰɼSelf Attentionաఔͷ eijޠ\nྀ͢Δɽ\neij = xiWQ(xjWK + aK\nij )T\n√dz\n(16)\n͜͜ͰShaw Β (Shaw et al. 2018)͸গͳ͘\nେ஋Λఆ਺kେ\n஋kͷ૬\n͸ɼҎԼͷ௨Γɼ2k + 1ͷϢχʔΫͳϥϕϧ (−k, −k + 1, . . . , 0, . . . , k − 1, k) Ͱ\nଊ͑Δɽ\naK\nij = wK\nclip(j−i,k) (17)\naV\nij = wV\nclip(j−i,k) (18)\nclip(x, k) = max(−k, min(k, x)) (19)\n288\nIA0231_06omoote (2020-05-27 11:02)\nྀͨ͠ Transformer NMT\n͸ɼwK = (wK\n−k, . . . , wK\nk ) ͱ wV = (wV\n−k, . . . , wV\nk )\n(wK\nk′ , wV\nk′ ∈ Rdz , −k ≤ k′ ≤ k)श͞ΕΔύϥϝʔλͰ͋Δɽ\n4Λ༻͍ͨTransformer NMT\nΓड͚໦ʹର͢Δ2ྀ͢ΔTransformer\nNMTΓड͚໦ʹର͢Δ 2͠ɼͦ\nɼShaw཭ΛTransformer Τϯίʔμ಺ͷSelf\nAttention ͰΤϯίʔυ͢Δख๏Λड़΂Δɽ\nΓड͚໦ʹର͢Δ 2Γड͚໦தͷ 2ͷ૬ରత\nจ w1, . . . , wnޠwi ʹରԠ͢\nΔϊʔυΛ niޠwj ʹରԠ͢ΔϊʔυΛ nj ͱͨ͠ͱ͖ɼni ͱnj ͷ૬ରతਂ͞ distij ΛҎ\n͢Δɽ\ndistij = depth(nj) − depth(ni) (20)\nਤ2 ͸\\My father bought a red car .\"ͨ͠΋ͷͰ͋Δɽྫ͑͹ɼਤ 2\nʹ͓͍ͯɼ\\My\"(= w1) ͷ \\bought\"(= w3) ʹର͢Δ૬ରతਂ͞ dist1,3 ͸ɼ−2 (= 0 − 2) Ͱ͋\nΔɽਤ2Γड͚໦ʹ͓͚Δ2཭Λද1͢ɽ\nΓड͚໦ʹର͢Δϊʔυ ni ͱ nj ͷ૬ରతҐஔΛϕΫτϧ bV\nij, bK\nij ∈ Rdz\n͠ɼShaw Βͷख๏ (Shaw et al. 2018) ಉ༷ɼzi ΍eiͮ͘\nࣜ15)ࣜ16)Λ༻͍Δɽ\nzi =\nn∑\nj=1\nαij(xjWV + bV\nij) (21)\nਤ 2Γड͚໦ͷྫ\nද 1 ਤ 2཭\nMy father bought a red car .\nMy 0 −1 −2 0 0 −1 −1\nfather 1 0 −1 1 1 0 0\nbought 2 1 0 2 2 1 1\na 0 −1 −2 0 0 −1 −1\nred 0 −1 −2 0 0 −1 −1\ncar 1 0 −1 1 1 0 0\n. 1 0 −1 1 1 0 0\n289\nIA0231_06omoote (2020-05-27 11:02)\nॲཧɹ Vol. 27 No. 2 June 2020\neij = xiWQ(xjWK + bK\nij )T\n√dz\n(22)\n3.2߹۩\nେ஋Λఆ਺ l ͱఆΊɼͦΕ\nେ஋ lޠwiޠwj ʹର͠\nΜͩϕΫτϧbV\nij, bK\nijͰ͖Δɽ\nbK\nij = wK\nclip(distij,l), (23)\nbV\nij = wV\nclip(distij,l). (24)\nҎ্ͷΑ͏ʹɼఏҊϞσϧͰ͸ɼΤϯίʔμͷSelf Attention Ͱɼจ಺ʹ͓͚Δ૬ରతҐஔද\nྀ͢Δ͜ͱ͕Ͱ͖ΔɽҎ\nɼ͜ͷఏҊϞσϧΛTransformerdep͢Δɽ\n5ݧ࣮\n5.1ઃఆ\n੒͞Εͨ ASPEC (Asian Scienti\fc Paper Excerpt\nCorpus) (Nakazawa et al. 2016)จ͸ Moses\n(Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer,\nBojar, Constantin, and Herbst 2007)Λ͠ɼ Stanford CoreNLP (Manning,\nSurdeanu, Bauer, Finkel, Bethard, and McClosky 2014)ͬͨɽ೔\nจ͸ KyTea (Neubig, Nakata, and Mori 2011)Λ͠ɼEDA2Γ\nͬͨɽ\nशσʔλ (train-1.txt, train-2.txt) ͔Βநग़্ͨ͠Ґ 150 ສจରͷ͏ͪɼ\nจͱ΋ʹจ௕100ҎԼͷ1,498,909श\nස౓͕7ස౓͕10 ճҎ্\nΛද͢UNK λάʹ\nূσʔλͱͯ͠ 1,790 จର (dev.txt)ɼςετσʔλͱͯ͠ 1,812 จର (test.txt)\nΛ༻͍ͨɽ\nͰ͸ɼ4 અͰఏҊͨ͠ఏҊϞσϧ (Transformerdep)ྀ͢\nΔTransformer NMT (Vaswani et al. 2017) (Transformerabs )ʹՃ͑จதͷ\nྀ͢ΔTransformer NMT (Shaw et al. 2018) ( Transformerrel)ͨ͠ɽ\n2 http://www.ar.media.kyoto-u.ac.jp/tool/EDA/\n290\nIA0231_06omoote (2020-05-27 11:02)\nྀͨ͠ Transformer NMT\nධՁର৅શͯͷϞσϧͷϋΠύʔύϥϝʔλ͸VaswaniΒ(Vaswani et al. 2017) ͷઃఆʹ฿͍ɼ\nͼσίʔμϨΠϠͷελοΫ਺Λ6ɼϔου਺Λ8ݩ࣍\nΛ512ͱͨ͠ɽoptimizer ͸Adam Λ༻͍ɼβ1 = 0.9, β2 = 0.98, ϵ = 10−9श\n৽εέδϡʔϦϯά͸ Vaswani Βͷํ๏ (Vaswani et al. 2017) ͱಉ༷ʹͨ͠ɽϛχόο\nναΠζ͸256ɼΤϙοΫ਺͸30΋ਫ਼౓͕ྑ͔ͬͨΤϙοΫͷϞ\nͰ͸ɼgreedy ΞϧΰϦζϜ\nจΛੜ੒ͨ͠ɽTransformerrel ͱTransformerdep ʹ͓͍ͯ͸ɼจ಺ʹ͓͚Δ૬\n཭͸k = 2, l = 2 ͱͨ͠3ɽ\n5.2Ռ\nՌΛද 2ඪ͸ BLEU Λ༻͍ͨɽ·ͨɼ຋༁ਫ਼౓ͷ\nਫ४ 5 %ఆख๏ (Koehn 2004) ʹΑΓ\nͬͨɽද 2 ͷʮ∗ʯ͓Αͼʮ†ʯ͸ɼͦΕͧΕɼఏҊϞσϧ Transformerdep ͱैདྷख๏Ͱ͋\nΔTransformerabs ͓ΑͼTransformerrelͯ͠\n͍Δɽද 2 ΑΓɼӳ೔຋༁Ͱ͸ɼఏҊϞσϧ Transformerdep ͸ Transformerabs͠ 0.60\nϙΠϯτ্ճΓɼTransformerrelͯ͠ 0.09 ϙΠϯτ্ճͬͨɽҰํͰɼ೔ӳ຋༁Ͱ͸ɼ\nTransformerdep ͸Transformerabsͯ͠0.37 ϙΠϯτ্ճΓɼTransformerrelͯ͠\n0.42܎\nྀ͢Δ͜ͱͰ Transformer Ϟσϧͷ຋༁ਫ਼౓ΛվળͰ͖Δ͜\nͱ͕෼͔Δɽ\nද3͢ɽTransformerabsจதͷʮ෼͚ͯʯͱʮ৺\nͨ͠ʯΛओಈ\n଄ʢ\\and\"଄ʣͷ຋༁ʹͳ͍ͬͯΔɽTransformerrelݪ\nഊ͍ͯ͠Δɽ\nTransformerdep ʢද3 ͷ5ͨ͠ʯͷ྆\nද 2Ռ\nϞσϧ BLEU (%)\nӳ → ೔ ೔ → ӳ\nTransformerabs 40.42 26.79\nTransformerrel 40.93 26.74\nTransformerdep 41.02∗ 27.16∗†\n3 Shaw Β (Shaw et al. 2018)ʹΑΓɼk ≥ 2 Ͱ͸ BLEU͞Ε͍ͯΔͨΊɼ\nk = 2Λબ୒ͨ͠ɽ·ͨɼlূσʔλΛ༻͍ͯνϡʔχϯάͨ͠ɽ\n291\nIA0231_06omoote (2020-05-27 11:02)\nॲཧɹ Vol. 27 No. 2 June 2020\n͠ɼͦͷ຋༁͸ \\compared by dividing\"র༁ͱ\nҰக͍ͯ͠Δɽਤ3(a)ͨ͠௨ΓɼTransformerdep Ͱ͸ɼ ʮ෼͚ʯ͕ʮͯʯʹɼ ʮͯʯ͕ʮൺ\nͷ৘ใ͕ར༻Ͱ͖ͨͨΊɼ\\divide\" ͕\\compare\" Λम০\n͑ΒΕΔɽ\nද 3ֱ\nೖྗจ܈\nʹ͠ ͨɻ\nর༁\nThe electrocardiograms were compared by dividing13 patients , on whom\nthe pulmonary arterial pressure was measured , into a high value group and a low\nvalue group in the systolic pulmonary arterial pressure .\nTransformerabs\nThirteen patients with pulmonary arterial pressure were divided into two\ngroups , one with high systolic pulmonary arterial pressure and the other with\nlow systolic pulmonary arterial pressure ,and their electrocardiogram was\ncompared .\nTransformerrel\nThe electrocardiogram of 13 patients with pulmonary arterial pressurewas\ncompared between high and low systolic pulmonary arterial pressure groups .\nTransformerdep\nʢEDAՌ\nʣ\nThe electrocardiogram was compared by dividing13 cases of measuring\nthe pulmonary arterial pressure into high value group and low value group of the\nsystolic pulmonary arterial pressure .\nTransformerdep\nΓड͚\nʣ\nThirteen patients with pulmonary arterial pressure were divided into two\ngroups : high and low systolic pulmonary arterial pressure groups .\nਤ 3 ද 3Γड͚໦\n292\nIA0231_06omoote (2020-05-27 11:02)\nྀͨ͠ Transformer NMT\nΓड͚໦ΛTransformerdepՌΛද3 ͷ\n6ମతʹ͸ɼೖྗจʹର͢ΔEDAՌʹ͓͍ͯʮ෼͚ͯ৺ిਤ\n଄ʢਤ3(a)ʯ͔Βʮɻ ʯʹม͑ɼ\n੒ͨ͠ʢਤ3(b)ɼ ʮ෼͚ͯʯ\nΓ\n͑ΒΕΔɽ\n5.3࡯ߟ\nShaw Β(Shaw et al. 2018) ͸ɼ3.2ͨ͠\nϕΫτϧaV\nij, aK\nijՌɼaV\nijͱɼaK\nij\nͱaV\nij͸ಉ౳ͷ຋༁ਫ਼౓͕ͩɼaK\nij͸຋༁ਫ਼౓͕Լ͕Δ͜ͱ͕\n͞Ε͍ͯΔɽͭ·ΓɼaK\nij͕ͩaV\nij͞Ε͍ͯΔɽ\nΛ\nͨ͠ϕΫτϧ bV\nij ͱ bK\nijϕΫτϧͷΈΛ༻͍ͨ\nTransformerdep ͷASPECࣜ22)ࣜ9)ߋ\nͨ͠ϞσϧʮTransformerdepʢbV\nijࣜ21)ࣜ7)ͨ͠ϞσϧʮTransformerdep\nʢbK\nij ͷΈʣ ʯ͓ΑͼTransformerabsͷઃఆ͸5.1 અͱಉ\n༷Ͱ͋Δɽ\nՌΛද4͢ɽද4 ΑΓɼ ӳ೔຋༁Ͱ͸ɼbV\nijͬͨTransformerdep ͸bK\nij ͱbV\nij ͷ྆\nํΛ༻͍ͨTransformerdep ʹରͯ͠0.09 ϙΠϯτ্ճͬͨɽbK\nijͬͨTransformerdep\n͸bK\nij ͱbV\nij ͷ྆ํΛ༻͍ͨTransformerdep ʹରͯ͠0.17 ϙΠϯτԼճͬͨɽ·ͨɼbK\nij ͱbV\nij\nͷͲͪΒ΋༻͍ͳ͍Ϟσϧ͸bK\nij ͱbV\nij ͷ྆ํΛ༻͍ͨTransformerdep ʹରͯ͠0.60 ϙΠϯτ\nԼճͬͨɽҰํͰɼ೔ӳ຋༁Ͱ͸ɼbV\nijͬͨTransformerdep ͸bK\nij ͱbV\nij ͷ྆ํΛ༻͍\nͨTransformerdep ʹରͯ͠0.18 ϙΠϯτԼճͬͨɽbK\nijͬͨTransformerdep ͸bK\nij ͱ\nbV\nij ͷ྆ํΛ༻͍ͨTransformerdep ʹରͯ͠0.02 ϙΠϯτ্ճͬͨɽ·ͨɼbK\nij ͱbV\nij ͷͲͪΒ\n΋༻͍ͳ͍Ϟσϧ͸bK\nij ͱbV\nij ͷ྆ํΛ༻͍ͨTransformerdep ʹରͯ͠0.37 ϙΠϯτԼճͬͨɽ\nද 4ݱbV\nij, bK\nijՌ\nbV\nij bK\nij\nBLEU (%)\nӳ → ೔ ೔ → ӳ\n✓ ✓ 41.02 27.16\n× ✓ 40.85 27.18\n✓ × 41.13 26.98\n× × 40.42 26.79\n293\nIA0231_06omoote (2020-05-27 11:02)\nॲཧɹ Vol. 27 No. 2 June 2020\nͨ͠ϕΫ\nτϧ bV\nij ͱ bK\nij͢Δ͜ͱ͕෼͔ͬͨɽ·ͨɼbK\nij , bV\nij ͷ྆ํ\nͱಉ౳ͷਫ਼౓ͱͳΔ͜ͱ͕෼͔ͬͨɽҰํͰɼbK\nij , bV\nij\nʹΑͬͯҟͳΔ͜ͱ͕෼͔ͬͨɽ\n6 ͓ΘΓʹ\nͰ͸ɼTransformerΓ\nΛTransformerΤϯίʔμͷSelf Attention தͷ૬ରత\nྀ͢Δख๏ΛఏҊͨ͠ɽASPEC (Nakazawa et al. 2016)ݧ࣮\n଄ʹର͢Δ૬ରతҐஔද\nྀ͢Δ͜ͱͰTransformer NMTೝͨ͠ɽಛʹɼ೔ӳ\n຋༁λεΫʹ͓͍ͯ͸0.37 ϙΠϯτBLEU είΞ͕վળͨ͠ɽ\nΓड͚\n΅͠ɼ຋༁ੑೳΛ௿Լͤ͞\nจղੳਫ਼౓ͷ\nߟ\nྀͰ͖ΔϞσϧʹվྑ͢Δ͜ͱͰ຋༁ੑೳΛ\n͞ΒʹվળͰ͖ΔՄೳੑ͕͋Δɽྫ͑͹ɼWu Β (Wu et al. 2018)଄ղੳΛ\n͏RNN ΛTransformer͢Δ͜ͱʹΑΓɼσίʔμ಺ͷSelf Attention Ͱ໨త\nྀ͢ΔϞσϧʹվྑͰ͖ΔՄೳੑ͕\n͋Δɽ\nࣙ\nٞInternational Conference Recent Advances in Natural Language Processing\n2019 (RANLP2019) Ͱൃදͨ͠࿦จ(Omote, Tamura, and Ninomiya 2019)Ͱ\nॻ͖௚͠ɼઆ໌΍ධՁΛ௥Ճͨ͠΋ͷͰ͋Δɽ\nʹΑΓಘΒΕͨ΋ͷͰ͋Δɽ·\nͷҰ෦͸JSPSඅ18K18110 ͷॿ੒Λड͚ͨ΋ͷͰ͋Δɽ͜͜ʹँҙΛද͢Δɽ\nਖ਼Λ୲౰͍͍ͯͨͩͨ͠Τφΰ (www.enago.jp) ʹँҙΛද͢Δɽ\n294\nIA0231_06omoote (2020-05-27 11:02)\nྀͨ͠ Transformer NMT\nݙ\nAharoni, R. and Goldberg, Y. (2017). \\Towards String-To-Tree Neural Machine Translation.\"\nIn Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics\n(Volume 2: Short Papers), pp. 132{140.\nBa, J. L., Kiros, J. R., and Hinton, G. E. (2016). \\Layer Normalization.\" arXiv preprint\narXiv:1607.06450.\nChen, K., Wang, R., Utiyama, M., Liu, L., Tamura, A., Sumita, E., and Zhao, T. (2017). \\Neural\nMachine Translation with Source Dependency Representation.\" In Proceedings of the 2017\nConference on Empirical Methods in Natural Language Processing, pp. 2846{2852.\nChen, K., Wang, R., Utiyama, M., and Sumita, E. (2019). \\Neural Machine Translation with\nReordering Embeddings.\" In Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics, pp. 1787{1799, Florence, Italy. Association for Computational\nLinguistics.\nCurrey, A. and Hea\feld, K. (2019). \\Incorporating Source Syntax into Transformer-Based Neural\nMachine Translation.\" InProceedings of the 4th Conference on Machine Translation (Volume\n1: Research Papers), pp. 24{33.\nDing, Y. and Palmer, M. (2005). \\Machine Translation Using Probabilistic Synchronous Depen-\ndency Insertion Grammars.\" In Proceedings of the 43rd Annual Meeting of the Association\nfor Computational Linguistics (ACL’05), pp. 541{548.\nDyer, C., Kuncoro, A., Ballesteros, M., and Smith, N. A. (2016). \\Recurrent Neural Network\nGrammars.\" In Proceedings of the 2016 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pp. 199{209, San\nDiego, California. Association for Computational Linguistics.\nEriguchi, A., Hashimoto, K., and Tsuruoka, Y. (2016). \\Tree-to-Sequence Attentional Neural\nMachine Translation.\" In Proceedings of the 54th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pp. 823{833.\nEriguchi, A., Tsuruoka, Y., and Cho, K. (2017). \\Learning to Parse and Translate Improves\nNeural Machine Translation.\" In Proceedings of the 55th Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Papers), pp. 72{78.\nGehring, J., Auli, M., Grangier, D., Yarats, D., and Dauphin, Y. N. (2017). \\Convolutional\nSequence to Sequence Learning.\" In Precup, D. and Teh, Y. W. (Eds.), Proceedings of\nthe 34th International Conference on Machine Learning, Vol. 70 of Proceedings of Machine\nLearning Research, pp. 1243{1252.\n295\nIA0231_06omoote (2020-05-27 11:02)\nॲཧɹ Vol. 27 No. 2 June 2020\nHe, K., Zhang, X., Ren, S., and Sun, J. (2016). \\Deep Residual Learning for Image Recogni-\ntion.\" In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\npp. 770{778.\nKoehn, P. (2004). \\Statistical Signi\fcance Tests for Machine Translation Evaluation.\" In Pro-\nceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,\npp. 388{395, Barcelona, Spain. Association for Computational Linguistics.\nKoehn, P., Hoang, H., Birch, A., Callison-Burch, C., Federico, M., Bertoldi, N., Cowan, B.,\nShen, W., Moran, C., Zens, R., Dyer, C., Bojar, O., Constantin, A., and Herbst, E. (2007).\n\\Moses: Open Source Toolkit for Statistical Machine Translation.\" In Proceedings of the\n45th Annual Meeting of the Association for Computational Linguistics Companion Volume\nProceedings of the Demo and Poster Sessions, pp. 177{180.\nLuong, T., Pham, H., and Manning, C. D. (2015). \\E\u000bective Approaches to Attention-based\nNeural Machine Translation.\" In Proceedings of the 2015 Conference on Empirical Methods\nin Natural Language Processing, pp. 1412{1421.\nMa, C., Tamura, A., Utiyama, M., Sumita, E., and Zhao, T. (2019). \\Improving Neural Machine\nTranslation with Neural Syntactic Distance.\" In Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), pp. 2032{2037.\nMa, C., Tamura, A., Utiyama, M., Zhao, T., and Sumita, E. (2018). \\Forest-Based Neural\nMachine Translation.\" In Proceedings of the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pp. 1253{1263.\nManning, C. D., Surdeanu, M., Bauer, J., Finkel, J., Bethard, S. J., and McClosky, D. (2014).\n\\The Stanford CoreNLP Natural Language Processing Toolkit.\" In Association for Compu-\ntational Linguistics (ACL) System Demonstrations, pp. 55{60.\nNakazawa, T., Yaguchi, M., Uchimoto, K., Utiyama, M., Sumita, E., Kurohashi, S., and Isahara,\nH. (2016). \\ASPEC: Asian Scienti\fc Paper Excerpt Corpus.\" In Proceedings of the 9th Inter-\nnational Conference on Language Resources and Evaluation (LREC 2016), pp. 2204{2208.\nNeubig, G., Nakata, Y., and Mori, S. (2011). \\Pointwise Prediction for Robust, Adaptable\nJapanese Morphological Analysis.\" In Proceedings of the 49th Annual Meeting of the Asso-\nciation for Computational Linguistics: Human Language Technologies, pp. 529{533.\nOmote, Y., Tamura, A., and Ninomiya, T. (2019). \\Dependency-Based Relative Positional En-\ncoding for Transformer NMT.\" In Proceedings of the International Conference on Recent\nAdvances in Natural Language Processing (RANLP 2019), pp. 854{861, Varna, Bulgaria.\nINCOMA Ltd.\n296\nIA0231_06omoote (2020-05-27 11:02)\nྀͨ͠ Transformer NMT\nSennrich, R. and Haddow, B. (2016). \\Linguistic Input Features Improve Neural Machine Trans-\nlation.\" In Proceedings of the 1st Conference on Machine Translation: Volume 1, Research\nPapers, pp. 83{91.\nShaw, P., Uszkoreit, J., and Vaswani, A. (2018). \\Self-Attention with Relative Position Repre-\nsentations.\" In Proceedings of the 2018 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, Volume 2 (Short\nPapers), pp. 464{468.\nShen, Y., Lin, Z., Jacob, A. P., Sordoni, A., Courville, A., and Bengio, Y. (2018). \\Straight to\nthe Tree: Constituency Parsing with Neural Syntactic Distance.\" In Proceedings of the 56th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\npp. 1171{1180.\nSutskever, I., Vinyals, O., and Le, Q. V. (2014). \\Sequence to Sequence Learning with Neural\nNetworks.\" In Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N. D., and Weinberger,\nK. Q. (Eds.), Advances in Neural Information Processing Systems 27, pp. 3104{3112.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser,  L., and\nPolosukhin, I. (2017). \\Attention is All you Need.\" In Guyon, I., Luxburg, U. V., Bengio,\nS., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (Eds.), Advances in Neural\nInformation Processing Systems 30, pp. 5998{6008.\nWatanabe, T., Tamura, A., and Ninomiya, T. (2017). \\CKY-based Convolutional Attention for\nNeural Machine Translation.\" In Proceedings of the 8th International Joint Conference on\nNatural Language Processing (Volume 2: Short Papers), pp. 1{6.\nWu, S., Zhang, D., Zhang, Z., Yang, N., Li, M., and Zhou, M. (2018). \\Dependency-to-\nDependency Neural Machine Translation.\" IEEE/ACM Transaction on Audio, Speech and\nLanguage Processing, 26 (11), pp. 2132{2141.\nZhang, M., Li, Z., Fu, G., and Zhang, M. (2019). \\Syntax-Enhanced Neural Machine Translation\nwith Syntax-Aware Word Representations.\" In Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), pp. 1151{1161.\nུྺ\nɹදɹ༔ଠ࿠ɿ2019ɽ2019޻\nɽ\nɹ༟ɿ2005ɽ 2007Ӄ૯\n՝ఔमྃɽ2013՝ఔम\n297\nIA0231_06omoote (2020-05-27 11:02)\nॲཧɹ Vol. 27 No. 2 June 2020\nһͱ͠\nɼ2017ɼ2020ࢤ\nձɼ৘\nձɼACLձһɽ\nɹɹɹਸɿ2001՝ఔमྃɽ\nһɽ2006൫ηϯλʔ\nɽ2010तɼ2017ژ\nձɼ\nձɼACLձһɽ\nʢ2019 ೥ 11݄1 ೔ɹड෇ʣ\nʢ2020 ೥ 2݄8ड෇ʣ\nʢ2020 ೥ 2݄26࿥ʣ\n298",
  "topic": "Dependency (UML)",
  "concepts": [
    {
      "name": "Dependency (UML)",
      "score": 0.5661394000053406
    },
    {
      "name": "Transformer",
      "score": 0.5232390761375427
    },
    {
      "name": "Encoding (memory)",
      "score": 0.5116520524024963
    },
    {
      "name": "Computer science",
      "score": 0.46725958585739136
    },
    {
      "name": "Artificial intelligence",
      "score": 0.21996784210205078
    },
    {
      "name": "Engineering",
      "score": 0.10814347863197327
    },
    {
      "name": "Electrical engineering",
      "score": 0.10516172647476196
    },
    {
      "name": "Voltage",
      "score": 0.060675621032714844
    }
  ],
  "cited_by": 6
}