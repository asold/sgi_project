{
  "title": "Finetuning Pretrained Transformers into RNNs",
  "url": "https://openalex.org/W3139537596",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A4202156197",
      "name": "Kasai, Jungo",
      "affiliations": [
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A1993451045",
      "name": "Peng Hao",
      "affiliations": [
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2218438351",
      "name": "Zhang, Yizhe",
      "affiliations": [
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4221452034",
      "name": "Yogatama, Dani",
      "affiliations": [
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4221397823",
      "name": "Ilharco, Gabriel",
      "affiliations": [
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2745263781",
      "name": "Pappas, Nikolaos",
      "affiliations": [
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2353255546",
      "name": "Mao Yi",
      "affiliations": [
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2049887450",
      "name": "Chen, Weizhu",
      "affiliations": [
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4221392677",
      "name": "Smith, Noah A.",
      "affiliations": [
        "Microsoft (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2963494889",
    "https://openalex.org/W2998183051",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963807318",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2963631907",
    "https://openalex.org/W2953273646",
    "https://openalex.org/W3085139254",
    "https://openalex.org/W3034609440",
    "https://openalex.org/W2144902422",
    "https://openalex.org/W3034573343",
    "https://openalex.org/W2983902802",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W3034955736",
    "https://openalex.org/W3130868440",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2970565456",
    "https://openalex.org/W3132041002",
    "https://openalex.org/W2946567085",
    "https://openalex.org/W2995575179",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W3174401451",
    "https://openalex.org/W2794365787",
    "https://openalex.org/W2885047103",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W3177265267",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2983981554",
    "https://openalex.org/W3166398787",
    "https://openalex.org/W2996159613",
    "https://openalex.org/W3098903812",
    "https://openalex.org/W3123615524",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2101926813",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2964199361",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W3129576130",
    "https://openalex.org/W2963347649",
    "https://openalex.org/W2512924740",
    "https://openalex.org/W2016589492",
    "https://openalex.org/W3104613728",
    "https://openalex.org/W2908336025",
    "https://openalex.org/W3125507956",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W3105238007",
    "https://openalex.org/W2963112338",
    "https://openalex.org/W3131922516",
    "https://openalex.org/W2964265128",
    "https://openalex.org/W2760656271",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2962964385",
    "https://openalex.org/W2518108298",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W3035691519",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W2963045354"
  ],
  "abstract": "Transformers have outperformed recurrent neural networks (RNNs) in natural language generation. But this comes with a significant computational cost, as the attention mechanism's complexity scales quadratically with sequence length. Efficient transformer variants have received increasing interest in recent works. Among them, a linear-complexity recurrent variant has proven well suited for autoregressive generation. It approximates the softmax attention with randomized or heuristic feature maps, but can be difficult to train and may yield suboptimal accuracy. This work aims to convert a pretrained transformer into its efficient recurrent counterpart, improving efficiency while maintaining accuracy. Specifically, we propose a swap-then-finetune procedure: in an off-the-shelf pretrained transformer, we replace the softmax attention with its linear-complexity recurrent alternative and then finetune. With a learned feature map, our approach provides an improved tradeoff between efficiency and accuracy over the standard transformer and other recurrent variants. We also show that the finetuning process has lower training cost relative to training these recurrent variants from scratch. As many models for natural language tasks are increasingly dependent on large-scale pretrained transformers, this work presents a viable approach to improving inference efficiency without repeating the expensive pretraining process.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10630–10643\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n10630\nFinetuning Pretrained Transformers into RNNs\nJungo Kasai♡∗ Hao Peng♡ Yizhe Zhang♣ Dani Yogatama♠\nGabriel Ilharco♡ Nikolaos Pappas♡ Yi Mao♣ Weizhu Chen♣ Noah A. Smith♡♢\n♡Paul G. Allen School of Computer Science & Engineering, University of Washington\n♣Microsoft ♠DeepMind ♢Allen Institute for AI\n{jkasai,hapeng,gamaga,npappas,nasmith}@cs.washington.edu\n{Yizhe.Zhang, maoyi, wzchen}@microsoft.com\ndyogatama@google.com\nAbstract\nTransformers have outperformed recurrent\nneural networks (RNNs) in natural language\ngeneration. But this comes with a signiﬁ-\ncant computational cost, as the attention mech-\nanism’s complexity scales quadratically with\nsequence length. Efﬁcient transformer vari-\nants have received increasing interest in recent\nworks. Among them, a linear-complexity re-\ncurrent variant has proven well suited for au-\ntoregressive generation. It approximates the\nsoftmax attention with randomized or heuris-\ntic feature maps, but can be difﬁcult to train\nand may yield suboptimal accuracy. This work\naims to convert a pretrained transformer into\nits efﬁcient recurrent counterpart, improving\nefﬁciency while maintaining accuracy. Specif-\nically, we propose a swap-then-ﬁnetune pro-\ncedure: in an off-the-shelf pretrained trans-\nformer, we replace the softmax attention with\nits linear-complexity recurrent alternative and\nthen ﬁnetune. With a learned feature map,\nour approach provides an improved tradeoff\nbetween efﬁciency and accuracy over the stan-\ndard transformer and other recurrent variants.\nWe also show that the ﬁnetuning process has\nlower training cost relative to training these re-\ncurrent variants from scratch. As many models\nfor natural language tasks are increasingly de-\npendent on large-scale pretrained transformers,\nthis work presents a viable approach to improv-\ning inference efﬁciency without repeating the\nexpensive pretraining process.1\n1 Introduction\nTransformer models (Vaswani et al., 2017) have\nadvanced the state of the art beyond recurrent neu-\nral network models (e.g., LSTMs, Hochreiter and\nSchmidhuber, 1997; GRUs, Cho et al., 2014) across\na wide range of natural language processing tasks.\nIn particular, the transformer architecture has been\n∗Work was done during an internship at Microsoft.\n1https://github.com/jungokasai/T2R/.\nwidely used in autoregressive modeling such as lan-\nguage modeling (Baevski and Auli, 2019) and ma-\nchine translation (Vaswani et al., 2017). The trans-\nformer makes crucial use of interactions between\nfeature vectors over the input sequence through\nthe attention mechanism (Bahdanau et al., 2015).\nHowever, this comes with signiﬁcant computation\nand memory footprint during generation. Since the\noutput is incrementally predicted conditioned on\nthe preﬁx, generation steps cannot be parallelized\nover time steps and require quadratic time complex-\nity in sequence length. The memory consumption\nin every generation step also grows linearly as the\nsequence becomes longer. This bottleneck for long\nsequence generation limits the use of large-scale\npretrained transformers, such as GPT-3 (Brown\net al., 2020), Image Transformer (Parmar et al.,\n2018), and DALL-E (Ramesh et al., 2021).\nRecent work aims at reducing the overhead of\nautoregressive transformers (Child et al., 2019; Ki-\ntaev et al., 2020; Beltagy et al., 2020, inter alia).\nAmong them are recurrent alternatives that approx-\nimate the standard softmax attention (Katharopou-\nlos et al., 2020; Peng et al., 2021; Choromanski\net al., 2021; Schlag et al., 2021). Similar to recur-\nrent neural networks (RNNs), those models repre-\nsent the context by a recurrent state with a ﬁxed\nsize, thereby achieving linear time and constant\nmemory complexity in generation sequence length.\nWhen the recurrent state size is smaller than the\nsequence length, these variants provide substantial\nspeed and memory advantages over the transformer.\nA small state size, however, tends to deteriorate the\ngeneration quality (Peng et al., 2021), leading to a\ntradeoff between efﬁciency and accuracy.\nThis work improves the balance between efﬁ-\nciency and accuracy by a conversion approach:\ninstead of training a recurrent alternative from\nscratch, we develop a method to convert a pre-\ntrained transformer into an efﬁcient RNN that\nspeeds up generation and reduces memory foot-\n10631\nprints. Our conversion proceeds with a swap-then-\nﬁnetune process. Speciﬁcally, we change the expo-\nnential similarity function in the attention mecha-\nnism to the dot product after a single-layer MLP\nfeature mapping. We then ﬁnetune the MLP pa-\nrameters and the other network parameters. Our\nexperiments in language modeling and machine\ntranslation show that the conversion can compress\nthe context into a much smaller recurrent state than\nthe sequence length (e.g., 1/16 of the sequence\nlength in WikiText-103 language modeling) while\nretaining high accuracy. In addition, this conver-\nsion requires much less GPU time than training\nrandomly initialized models from scratch.\nState-of-the-art models in many natural language\ntasks are increasingly dependent on large-scale pre-\ntrained transformer models (e.g., GPT-2, Radford\net al., 2019; BERT, Devlin et al., 2019; RoBERTa,\nLiu et al., 2019; T5, Raffel et al., 2020; BART,\nLewis et al., 2020; DeBERTa, He et al., 2021).\nConverting a large off-the-shelf transformer to a\nlightweight inference model without repeating the\nwhole training procedure is particularly useful in\nmany downstream applications. Our work focuses\non text generation and presents a viable approach\ntowards efﬁcient inference with high accuracy.\n2 Convert a Transformer into an RNN\nThe transformer architecture consists of multihead\nattention, feedforward, and layer normalization\nmodules (Vaswani et al., 2017). When a trans-\nformer is trained for a sequence generation task\nwith teacher forcing (Williams and Zipser, 1989),\nthe attention can be parallelized over positions be-\ncause the target sequence is fully available. During\ngeneration, on the other hand, the output is incre-\nmentally constructed. As a result, the attention be-\ncomes an inference bottleneck for long sequences.\nWe present a method to eliminate this bottleneck by\nconverting a pretrained transformer into an efﬁcient\nRNN of linear time and constant space complexity.\nWe provide a detailed complexity analysis in terms\nof the sequence length and model dimensions.\n2.1 Multihead Attention\nThe attention module takes as input sequences of\nsource and target vectors. The source vectors are\nused to produce key and value features, while the\ntarget vectors are mapped to query vectors. More\nformally, denote by {xtgt\ni }N\ni=1 and {xsrc\nj }M\nj=1 the\ntarget and source vectors, where xtgt\ni ,xsrc\nj ∈ Rh\nand h is the model dimensionality. We assume\nr attention heads of ddimensions (h = dr). For\neach head, the input vectors are ﬁrst mapped to\nd dimensional query, key, and value features by\nlearned afﬁne transformations with W∗ ∈ Rd×h\nand b∗∈Rd:\nqi =Wqxtgt\ni +bq, (1a)\nkj =Wkxsrc\nj +bk, vj =Wvxsrc\nj +bv. (1b)\nThe similarities of each query vector qi with all\nM key vectors are computed and normalized to\nproduce attention coefﬁcients, which are then used\nto output a weighted average of the value vectors\n(Vaswani et al., 2017):\nxout\ni =\nM\n/summation.disp\nj=1\nsim (qi,kj)\n∑M\nℓ=1 sim (qi,kℓ)\nvj, (2a)\nsim(x,y)=exp /parenleft.alt2x ⋅y/slash.left\n√\nd/parenright.alt2. (2b)\nMultihead attention runs this procedure for each of\nthe rheads in parallel and concatenates routput\nvectors to get the ﬁnal hdimensional vector.2\nGeneration Speed Overhead Fig. 1 depicts the\ntransformer computation steps from input vectors\nand their time complexity. We assume that the\ntime complexity of multiplying an n×mmatrix by\nan m×kis O(nmk)as implemented in cuBLAS\n(NVIDIA, 2014).3 It consists of the following two\nstages.\n• Feature Mapping: computation of {qi}N\ni=1,\n{kj}M\nj=1, and {vj}M\nj=1 for all r heads from\ninput vectors (Eqs. 1a-1b). Time complexity\nof O(Nh2), O(Mh2), and O(Mh2).\n• Attention: weighted average over the value\nvectors (Eq. 2a). O(MNh), quadratic in se-\nquence length (M, N).\nGeneration Memory Overhead In autoregres-\nsive generation, query, key, and value vectors con-\nsume space complexity of O(h), O(Mh), and\nO(Mh)in every generation step. Every step’s\nattention weight (Eq. 2a) spans over M source po-\nsitions, taking O(Mr)space, linear in sequence\nlength M.\n2.2 Converting Transformers to RNNs\nTo address this generation bottleneck of quadratic\ntime and linear space, we propose Transformer-\nto-RNN (T2R), a method to convert a pretrained\n2Layer normalization (Ba et al., 2016), residual connection\n(He et al., 2016), and projection are suppressed for brevity.\n3If the batch size is small enough, parallelization can speed\nup matrix multiplication.\n10632\nPretrained T ransformer T2R\nRNN State\nFigure 1: Attention computation steps and their time complexity in pretrained transformer and T2R models during\ninference generation. Features φ(qi)and φ(kj )are directly computed from input vectors, andqi and kj are never\nconstructed. M: source length; N: target length; h: model dimensions; k: feature size; r: # heads.\ntransformer to an RNN inference model of linear\ntime and constant memory complexity in sequence\nlength (Fig. 1). T2R follows a swap-then-ﬁnetune\nprocedure that modiﬁes the attention computation\nof a pretrained transformer, and ﬁnetunes the model\nwith the task objective.\nWe ﬁrst replace the dot-then-exponential similar-\nity function in a pretrained transformer (Eq. 2b) by\n/tildecomb.alt2sim (x,y)=φ(x)⋅φ(y), (3a)\nφ(x)=relu /parenleft.alt1Wφx +bφ/parenright.alt1. (3b)\nHere Wφ ∈ Rk×d and bφ ∈ Rk are learned pa-\nrameters of a single-layer MLP. They map a ddi-\nmensional vector to a kdimensional kernel feature\nspace. The relu activation (Fukushima, 1980) en-\nsures that the features are non-negative.4 Different\nMLP parameters are used for different attention\nheads, and thus we add a total of rk(d+1)learn-\nable parameters per layer (less than 0.2% parame-\nter increase in our language model, §3). We then\nﬁnetune all parameters in this modiﬁed network,\nincluding the MLP parameters, with the original\ntask objective.5\nDuring inference generation, we reformulate the\nattention computation (Eq. 2a) as\ñxout\ni =\nM\n/summation.disp\nj=1\n/tildecomb.alt2sim (qi,kj)\n∑M\nℓ=1 /tildecomb.alt2sim (qi,kℓ)\nvj\n=/uni239B\n/uni239D\nφ(qi)⋅∑M\nj=1 φ(kj)⊗vj\nφ(qi)⋅∑M\nℓ=1 φ(kℓ)\n/uni239E\n/uni23A0\n/uni22BA(4)\n4We found thatrelu stabilized training by prohibiting nega-\ntive similarities φ(q)⋅ φ(k). Other activation functions, such\nas cos, tanh, and elu, did not improve performance.\n5We tried training the MLP parameters only, but this set-\nting resulted in degraded development performance.\nby the associativity of matrix multiplication. This\nformulation lends itself to recurrent computation.\nIn causal attention where each query only attends\nto its preﬁx to predict the next word (M =i), deﬁne\nstates:\nSi =\ni\n/summation.disp\nj=1\nφ(kj)⊗vj, zi =\ni\n/summation.disp\nj=1\nφ(kj) (5)\nwhere Si,zi ∈Rk×d,Rk. These states can be com-\nputed recurrently (Katharopoulos et al., 2020):\nSi =Si−1 +φ(ki)v/uni22BA\ni zi =zi−1 +φ(ki) (6)\nIn the self-attention or encoder-to-decoder (cross)\nattention of a sequence-to-sequence model, Si and\nzi are constant with respect to iand only need to\nbe computed once. Given the two states at position\ni, we can obtain the output vector:\ñxout\ni =/parenleft.alt4φ(qi)/uni22BASi\nφ(qi)/uni22BAzi\n/parenright.alt4\n/uni22BA\n(7)\nThis avoids quadratic computation with respect to\nthe input sequence length. We also speed up in-\nference by merging the MLP feature map with the\nafﬁne feature maps that produce queries and keys.\nφ(qi)=relu /parenleft.alt1/tildecomb.alt1Wqxtgt\ni +̃bq/parenright.alt1, (8a)\nφ(kj)=relu /parenleft.alt1/tildecomb.alt1Wkxsrc\nj +̃bk/parenright.alt1, (8b)\nwhere /tildecomb.alt1Wq =WφWq, /tildecomb.alt1Wk =WφWk, (8c)\ñbq =bφ+Wφbq, ̃bk =bφ+Wφbk. (8d)\nAfter the model is trained, Eqs. 8c–8d are computed\nonce before generation; the intermediate features\nof qi and kj are never computed during inference.\nGeneration Speed Overhead The time com-\nplexity of each step in a T2R model is shown in\nFig. 1. Similar to the transformer, it proceeds over\n10633\ntwo stages.\n• Feature Mapping : computation of\n{φ(qi)}N\ni=1, {φ(kj)}M\nj=1, and {vj}M\nj=1\nfor all rheads (Eqs. 8a–8b). Time complexity\nof O(Nhkr), O(Mhkr), and O(Mh2).\n• Attention: the RNN states and the outputs\nfor r heads (Eqs. 5–7) are computed with\nO(Mhk)and O(Nhk).\nComparing this with the pretrained transformer, we\nsee that if the feature size is much smaller than\ninput sequence lengths (k/uni226AM,N), the change in\nthe attention stage from O(MNh)to O(hk(M+\nN))in T2R brings a substantial speedup.\nGeneration Memory Overhead T2R only\nneeds to store the RNN state, and thus its space\ncomplexity is O(hk), constant in sequence length.\nThis implies reduction in memory footprint when\nk/uni226AM, compared to the transformer’sO(Mh).\n2.3 Autoregressive Linear Transformers\nIn principle, any kernel function can be used as\nthe similarity function in Eq. 2a (Tsai et al., 2019).\nPrevious work proposed several untrainable fea-\nture map functions φ and developed autoregressive\ntransformer variants with linear time and constant\nspace complexity in sequence length (Katharopou-\nlos et al., 2020; Peng et al., 2021; Choromanski\net al., 2021). While those models follow similar\ncomputation steps to T2R, there are several differ-\nences in generation efﬁciency. Since the feature\nmap in Katharopoulos et al. (2020) preserves input\ndimensions, the feature size is always the same as\nthe head dimensions (k=d). This means that the\nspeedup and memory savings from using a small\nfeature size are restricted by design. In our exper-\niments (§3.3), our T2R models gain further efﬁ-\nciency by using a feature size that is even smaller\nthan the head dimensions (k=32 and d=128 for\nlanguage modeling). Peng et al. (2021) and Choro-\nmanski et al. (2021) scale query and key vectors\nby their norms before the random approximation to\nbound the error. Consequently, the feature mapping\nstage needs additional steps of producing interme-\ndiate q and k and scaling them. T2R suppresses\nthese steps and speeds up generation further (§3.3).\n3 Experiments\nWe present extensive experiments on standard\nbenchmarks for language modeling and machine\ntranslation. Our results show that T2R achieves\nefﬁcient autoregressive generation while retaining\nhigh accuracy.\n3.1 Baselines and Comparison\nWe compare performance with previous trans-\nformer models for autoregressive generation with\nlinear time and constant space complexity in in-\nput sequence length.6 As discussed in §2.3, those\nprior methods correspond to two different untrain-\nable feature maps φ. We experiment with two\ntypes of feature maps for comparisons: ELU\n(φ(x)=elu (x)+1, Katharopoulos et al., 2020);\nRFA (random feature approximation with softmax\ntemperature reparameterization, Peng et al., 2021).\nEach feature map is evaluated in two settings: ran-\ndom initialization and pretrain. Random initializa-\ntion is our reimplementation of the experiments in\nKatharopoulos et al. (2020) and Peng et al. (2021).\nThe pretrain setting follows the same protocol as\nT2R except that we use different feature maps\nφ than our proposed one-layer MLP with relu\nactivation. Positive orthogonal random features\n(Performer, Choromanski et al., 2021) provide\nsimilar random approximation to RFA and were\nevaluated in the biology domain, but we found that\nthis method caused training divergence in the lan-\nguage modeling task.7\n3.2 Setup and Implementations\nWe apply our method to causal attention in lan-\nguage models and both cross and causal attention\nin machine translation. For language modeling, we\nuse a 32-dimensional feature map function. We do\nnot modify the encoder in machine translation as\nits generation speed overhead is much less signif-\nicant than the decoder (Kasai et al., 2021). Our\nexploration showed that reducing the feature size\nof causal attention tends to have less impact on\nthe ﬁnal translation accuracy as opposed to cross\nattention; we use feature sizes of 32 and 4 for cross\nand causal attention, respectively. This observation\n6See §5 for our discussion on more transformer variants\nwith linear time complexity, but most of those variants need\nmodiﬁcations for autoregressive modeling and have yet to be\nempirically evaluated in autoregressive generation tasks.\n7Our implementation closely follows the code released\nby the authors (https://github.com/lucidrains/\nperformer-pytorch/blob/main/performer_\npytorch/performer_pytorch.py#L75-L81), but\ndoes not subtract the maximum logit; otherwise it would\ndisallow the linear complexity in causal attention. We\nconjecture that this is the reason why Performer becomes less\nstable in our experiments. We suspect that some techniques\nare necessary to improve numerical stability in language\nmodeling and machine translation.\n10634\nis consistent with previous work that showed that\ncausal attention can be more drastically simpliﬁed\nthan cross attention in transformer machine transla-\ntion models (You et al., 2020; Tay et al., 2021).\n3.2.1 Language Modeling\nWe use the WikiText-103 benchmark, which\nconsists of 103M tokens sampled from English\nWikipedia (Merity et al., 2017). We choose similar\nhyperparameters to prior work (Baevski and Auli,\n2019; Fan et al., 2020): 32 layers, 8 heads, 128\nhead dimensions, 1024 model dimensions, 4096\nfully connected dimensions and dropout (Srivas-\ntava et al., 2014) and layer dropout rates of 0.2.\nWe partition the training data into non-overlapping\nblocks of 512 contiguous tokens ignoring docu-\nment boundaries and train the model to predict\neach token from left to right (Baevski and Auli,\n2019). Validation and test perplexity are measured\nby predicting the last 256 words out of the input of\n512 consecutive words to avoid evaluating tokens\nin the beginning with limited context (early token\ncurse, Press et al., 2021). We generally follow the\noptimization method from Baevski and Auli (2019),\nbut some hyperparameters, such as the learning rate\nfor the T2R ﬁnetuning, are adjusted for better con-\nvergence than randomly initialized training. See\nAppendix A.1 for more details.\n3.2.2 Machine Translation\nWe experiment with 3 translation benchmarks:\nWMT14 EN-DE (4.5M train pairs, Bojar et al.,\n2016), WMT14 EN-FR (36M, Bojar et al., 2014),\nand WMT17 ZH-EN (20M, Bojar et al., 2017). We\nfollow the preprocessing and data splits by previ-\nous work (EN-DE: Vaswani et al., 2017; EN-FR:\nGehring et al., 2017; EN-ZH: Hassan et al., 2018).\nWe use the hyperparameters of the large sized trans-\nformer (Vaswani et al., 2017): 6 layers, 16 attention\nheads, 1024 model dimensions, and 4096 hidden\ndimensions for both the encoder and decoder. We\napply dropout with 0.3 and label smoothing with\nε = 0.1. Following Ott et al. (2018), we use an\nincreased batch size of approximately 460K to-\nkens. Each randomly initialized model is trained\nfor 30K (60K for the large EN-FR dataset) steps\nusing Adam with a learning rate of 5 ⋅10−4 and\nβ = (0.9,0.98)(Kingma and Ba, 2015). We ob-\nserved that convergence of the T2R conversion can\nbe achieved with 20K (40K for EN-FR) steps and\na reduced learning rate of 2 ⋅10−4. We average the\ncheckpoints from the last ﬁve epochs to obtain the\nﬁnal model (Vaswani et al., 2017). In inference, we\napply beam search with size 5 and length penalty\n0.6. Consistent with previous practice, we evalu-\nate with tokenized BLEU (Papineni et al., 2002).\nFurther details are described in Appendix A.1.\nppl. train\nModel k dev. test time\nELU + Random Init. 128 22.0 22.8 470h\nRFA + Random Init. 32 20.4 21.3 512h\nT2R + Random Init. 32 20.1 20.8 474h\nELU + Pretrain 128 21.5 22.2 97h\nRFA + Pretrain 32 20.8 21.6 104h\nT2R + Pretrain 32 19.0 19.6 98h\nT2R 75% + Pretrain 32 17.9 18.5 95h\nPretrained Transformer – 17.9 18.5 –\nBaevski and Auli (2019) – – 18.7 –\nTable 1: WikiText-103 language modeling results (per-\nplexity). Train time is measured in GPU hours. The\ntop two rows are our reimplementations of Katharopou-\nlos et al. (2020) and Peng et al. (2021). Pretrain in-\ndicates initialization with a pretrained transformer for\nlanguage modeling. T2R 75% indicates a model where\nevery fourth layer from the top is kept as the original\ntransformer layer. Perplexity (ppl.) is measured by pre-\ndicting the last 256 words out of the input of 512 con-\nsecutive words. All models use 128 head dimensions.\nWe assume access to a pretrained transformer model\nand measure the ﬁnetuning time in GPU hours.\n3.3 Results\nLanguage Modeling Seen in Table 1 are lan-\nguage modeling results in perplexity. We observe\nthat T2R with the learnable MLP feature map out-\nperforms the other two linear transformer models\nby more than 2.0 perplexity points in the pretrain\nsetting. Unlike the other linear transformer models,\nT2R greatly beneﬁts from pretraining (T2R + Pre-\ntrain: 19.6 vs. T2R + Random Init.: 20.8 test per-\nplexity points). We attribute this advantage of T2R\nto the fact that the MLP feature map is able to learn\nattention patterns that are similar to those of the pre-\ntrained transformer, as evidenced in §4. Notice also\nthat the T2R conversion is ∼5x faster (measured\nin GPU hours) than training a model from scratch.\nThese results illustrate that a lightweight model\ncan be obtained without repeating the expensive\ntraining of large-scale pretrained language models\nsuch as GPT-2 and GPT-3 (Radford et al., 2019;\nBrown et al., 2020). T2R’s generation speedup\n(∼4x when producing 512 consecutive words) and\n10635\nFeature Size k WMT14 WMT17 Train Time\nModel cross causal EN-DE EN-FR ZH-EN (GPU hours)\nELU + Random Init. 64 64 28.4 * 23.4 120h\nRFA + Random Init. 32 4 28.1 41.7 23.4 135h\nT2R + Random Init. 32 4 27.5 39.8 23.1 123h\nELU + Pretrain 64 64 28.4 41.8 23.8 80h\nRFA + Pretrain 32 4 27.6 41.8 23.2 90h\nT2R + Pretrain 32 4 28.7 42.1 23.8 82h\nPretrained Transformer Large – – 28.9 42.2 24.2 –\nVaswani et al. (2017) – – 28.4 41.8 – –\nTable 2: Machine translation test results in BLEU scores. The top two rows are our reimplementations of\nKatharopoulos et al. (2020) and Peng et al. (2021). Pretrain indicates initialization with a trained transformer-\nlarge model. *: diverged even when running with multiple random seeds and smaller learning rates. We assume\naccess to a pretrained transformer model and measure the ﬁnetuning time in GPU hours.\nmemory savings are later benchmarked with vary-\ning sequence lengths. There remains a gap of 1.1\nperplexity points between the T2R and pretrained\ntransformer models (19.6 vs. 18.5). However, the\ngap can be closed when every fourth layer from\nthe top is kept as the original transformer layer and\nthe model is ﬁnetuned in the same way (T2R 75%).\nThis suggests that keeping a small fraction of the\nquadratic attention layers can provide an effective\nmiddle ground between efﬁciency and accuracy.8\nMachine Translation Seen in Table 2 are ma-\nchine translation results in BLEU from various con-\nﬁgurations. Departing from the language modeling\nexperiments, the T2R model underperforms the\nother two linear transformer models when initial-\nized randomly. However, consistent with language\nmodeling, the T2R model substantially beneﬁts\nfrom pretraining (e.g., 28.7 vs. 27.5 BLEU points\nin EN-DE). As a result, the T2R model achieves\nsimilar BLEU scores to the original transformer\nacross all language pairs. ELU trained from the\npretrained transformer yields comparable perfor-\nmance to T2R, but the feature size is much larger\n(64 vs. 32 and 64 vs. 4 in cross and causal attention),\nthus leading to increased overhead, as shown later.\nNote that the T2R ﬁnetuning time is only mod-\nerately smaller than that of randomly initialized\ntraining here, but further speedup in conversion\ncan be potentially achieved with more extensive\nhyperparameter tuning.9\n8Concurrent work (Lei, 2021) also explores reducing the\nnumber of attention layers for efﬁciency.\n9We found that the batch size could be reduced for T2R\nconversion without hurting accuracy, while randomly initial-\nized models deteriorate with small batch sizes. This suggests\n8 16 32 64 128 256 512 1024 2048\n1K\n2K\n3K\n4K\n5K\n6K\n7K\nSentence Length\nDecoding Speed (Tokens/s)\nTransformer\nELU 64-64\nRFA 32-4\nT2R 32-4\nFigure 2: Machine translation speed of various models.\nSpeed is measured on a single TPU v2 accelerator with\nbatch size 16 and beam size 1, following Peng et al.\n(2021). 32-4 indicates the feature sizes of 32 and 4 for\ncross and causal attention, respectively.\nSpeedup and Memory Savings in Generation\nWe run a conditional generation experiment to com-\npare the decoding speed of the models in Table 2\n(Fig. 2). Here we assume the input and output se-\nquences are of the same length. All models are\ntested using greedy decoding with the same batch\nsize of 16 on a TPU v2 accelerator.10 We see that\nindeed the linear transformer models can generate\nan almost constant number of tokens per second\nregardless of the sequence length and outpace the\ntransformer model dramatically as the sequence\nbecomes longer. The T2R model achieves a 15%+\nthat the computational cost for conversion can be much lighter\nthan training from scratch, and T2R is advantageous when\nonly a limited number of GPUs are available.\n10https://opensource.google/projects/\njax.\n10636\n8 16 32 64 128 256 51222\n24\n26\n28\nSentence Length\nAttention Memory (MB)\nTransformer\nELU 64-64\nT2R/RFA 32-4\nFigure 3: Memory consumption from the attention\ncomputation of various machine translation models in\ninference with batch size 16 and beam size 1.\nspeedup over ELU and RFA due to its smaller fea-\nture sizes and faster feature mapping respectively;\nthis conﬁrms our analysis on T2R’s speed advan-\ntage over them (§2.3). Fig. 3 plots memory con-\nsumption from the attention computation during\ndecoding for machine translation. Since the T2R,\nRFA, and ELU models compress keys and values\ninto a k×dmatrix S and a kdimensional vector\nz (§2.2), the required memory at each decoding\nstep is constant over varying sequence lengths. It\nis also roughly proportional to the feature size k.\nThe MLP feature map in the T2R model allows for\nsmall feature dimensions than the ELU feature of\nthe head dimensions, resulting in a 70% memory re-\nduction. The attention computation in the standard\ntransformer, on the other hand, consumes memory\nlinearly in sequence length at each decoding step\nbecause all previous key and value vectors have to\nbe stored. We also found a similar speedup and\nmemory savings in unconditional generation with\nthe T2R language model (∼4x speedup in generat-\ning 512 consecutive words over the transformer).\n4 Analysis and Ablations\nWe presented T2R, a method to convert a pretrained\ntransformer into an efﬁcient RNN. In this section,\nwe analyze our conversion approach by examining\nthe impact of the feature size and induced attention\nweight distributions. Our analysis shows that T2R\nimplicitly learns attention distributions similar to\nthe original transformer.\nFeature Size and Pretraining We saw that T2R\nbeneﬁts substantially from transformer pretraining.\nFig. 4 compares T2R with pretraining and random\n8 16 32 64 128 256 512\n18\n20\n22\n24\nTransformer\nFeature Sizek\nValidation Perplexity\nT2R + Random Init.\nT2R + Pretrain\nFigure 4: WikiText-103 validation perplexity with vary-\ning feature sizes.\n8 16 32 64 128 256 5120\n0.1\n0.2\n0.3\nFeature Sizek\nAverage Euclidean Distance\nT2R before Finetuning\nT2R MLP Frozen\nT2R\nFigure 5: Average Euclidean distance of T2R mod-\nels from the transformer attention weights with vary-\ning feature sizes. The distances are computed on\nthe Wikitext-103 validation data for predicting a word\ngiven the preceding 512 words. All models are initial-\nized with a pretrained transformer model.\ninitialization in terms of the relation between the\nvalidation perplexity from WikiText-103 and the\nfeature sizes. We see that as the feature size (RNN\nstate size) becomes smaller, pretraining becomes\nparticularly important to achieve low perplexity.\nTransformer pretraining achieves a Pareto improve-\nment over random initialization in the tradeoff be-\ntween efﬁciency (small feature size) and accuracy\n(low perplexity).\nAttention Distribution T2R is not explicitly\ntrained to mimic the original attention distributions,\nand there is no guarantee that the MLP feature map\napproximates the exponential similarity function,\nunlike previous approximation approaches (Peng\net al., 2021; Choromanski et al., 2021). Here, we\nanalyze the properties of the attention weight dis-\n10637\ntributions that are induced by ﬁnetuning. We use\nthe validation data from WikiText-103 and run lan-\nguage models to predict the next word given the\ninput of 512 contiguous words. We compute the\nattention weight distribution over the 512 words\nfor each attention head in the model layers.\nFig. 5 compares the attention distributions from\nT2R in various conﬁgurations. T2R MLP frozen\nindicates a model that is ﬁnetuned with the MLP\nparameters frozen. Euclidean distances in attention\ndistributions between the original transformer and\neach model are averaged across validation samples,\nmodel layers, and attention heads. 11 Comparing\nT2R before ﬁnetuning and the full T2R model, we\nsee that the ﬁnetuning process induces much more\nsimilar attention distributions, and the distance di-\nminishes as the feature size increases (and the per-\nplexity approaches the original transformer, Fig. 4).\nWe also observed that when the MLP parameters\nare not trained (T2R MLP frozen), the distance\nfrom the original attention distributions increases.\nThese results suggest that ﬁnetuning of the whole\nnetwork in T2R implicitly develops similar atten-\ntion distributions to the original transformer even\nthough the training supervision comes solely from\nlanguage modeling.\n5 Further Related Work\nIn addition to the work we already discussed, we\nhighlight related methods from prior work that\nmake transformer models efﬁcient.\n5.1 Knowledge Distillation\nKnowledge distillation (Hinton et al., 2015) is\nclosely related to our T2R conversion and uses\na similar pipeline: a teacher model with large ca-\npacity is ﬁrst trained and is used to generate silver\ntraining data for a new lightweight inference model.\nIt has been successfully applied to machine trans-\nlation (e.g., Kim and Rush, 2016; Gu et al., 2018)\nto make generation efﬁcient. In particular, several\nprior works distill a transformer translation model\nto an RNN (Senellart et al., 2018; Kim et al., 2019).\nWe share the same motivation toward fast genera-\ntion with light memory, but our approach differs in\ntwo ways: the original training data are used for\nﬁnetuning an RNN model, and its model parame-\nters are initialized with the “teacher” transformer.\n11We do not consider random initialization baselines here\nbecause random initialization makes it impossible to align\nattention heads and layers between models.\nOur method does not use the computationally ex-\npensive teacher model to generate new training data.\nWhile data generation is a one-time computational\ncost, it becomes expensive as the teacher model\nsize and training data increase. Moreover, since\nthe pretrained parameters can be directly used, con-\nversion requires fewer GPU hours than training a\nbrand new lightweight model from scratch (§3.3).\n5.2 Efﬁcient Transformers\nPrior work suggested many other strategies to im-\nprove efﬁciency in transformers, such as weight\nsharing and factorization (Dehghani et al., 2019;\nLan et al., 2020), weight and layer pruning (Michel\net al., 2019; Fan et al., 2020), quantization (Zafrir\net al., 2019; Shen et al., 2020), and modifying the\ncombination of sublayers (Press et al., 2020; Man-\ndava et al., 2020). Some of these methods present\northogonal design choices and can be integrated\ninto our T2R model to gain further efﬁciency. For a\nmore comprehensive survey, see Tay et al. (2020b).\nBelow we describe several prior works along two\nmajor strategies: compressing the attention context\nand sparsifying the attention patterns.\nAttention Context Compression This strand of\nmethods compresses the context that is attended to,\nthereby reducing the time and memory overhead\nin the attention. RNN models that we converted\npretrained transformers into compress the context\ninto a recurrent state. Other approaches include low\nrank approximation of the attention computation\n(Wang et al., 2020; Tay et al., 2021) and adding a\nmemory module that can access multiple tokens at\nonce (Liu et al., 2018; Dai et al., 2019; Lee et al.,\n2019; Ainslie et al., 2020; Rae et al., 2020; Beltagy\net al., 2020; Zaheer et al., 2020).\nSparse Attention Patterns Another approach to\nreducing the time and memory overhead from the\nattention computation is to limit the tokens that are\nattended to by sparsifying the attention patterns.\nThese patterns can be set in advance or learned dur-\ning training (Tay et al., 2020b). For example, prior\nworks introduced ﬁxed patterns of blockwise atten-\ntion (Qiu et al., 2020) and strided attention (Child\net al., 2019; Beltagy et al., 2020; Zaheer et al.,\n2020). Other previous works presented methods\nto learn attention patterns from data (Sukhbaatar\net al., 2019; Roy et al., 2020; Tay et al., 2020a).\nIt should be noted that signiﬁcant modiﬁcations\nare necessary to apply many of these methods to\nautoregressive generation tasks such as language\n10638\nmodeling and machine translation, and their em-\npirical evaluation in these generation settings has\nyet to be conducted (Peng et al., 2021). This work\npresents extensive empirical evaluation in autore-\ngressive generation settings.\n6 Conclusion and Future Work\nWe present T2R, a method that converts a pre-\ntrained transformer to a recurrent neural network\nthat reduces the time and memory cost of au-\ntoregressive generation. Our experiments in lan-\nguage modeling and machine translation demon-\nstrated that our model produces an improved trade-\noff between efﬁciency and accuracy over ran-\ndomly initialized training and previous models with\nlightweight attention. Our work provides further\nsupport for the claim that large-scale pretrained\nmodels can be compressed into efﬁcient inference\nmodels that facilitate downstream applications.\nAcknowledgments\nWe thank Oﬁr Press, Bill Dolan, Lei Li, and the\nanonymous reviewers for their valuable feedback\nand discussion on this work. Nikolaos Pappas was\nsupported by the Swiss National Science Founda-\ntion grant P400P2_183911.\nReferences\nJoshua Ainslie, Santiago Ontanon, Chris Alberti, Va-\nclav Cvicek, Zachary Fisher, Philip Pham, Anirudh\nRavula, Sumit Sanghai, Qifan Wang, and Li Yang.\n2020. ETC: Encoding long and structured inputs in\ntransformers. In Proc. of EMNLP.\nJimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton.\n2016. Layer normalization.\nAlexei Baevski and Michael Auli. 2019. Adaptive in-\nput representations for neural language modeling. In\nProc. of ICLR.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In Proc. of ICLR.\nIz Beltagy, Matthew E. Peters, and Arman Cohan. 2020.\nLongformer: The long-document transformer.\nOndˇrej Bojar, Christian Buck, Christian Federmann,\nBarry Haddow, Philipp Koehn, Johannes Leveling,\nChristof Monz, Pavel Pecina, Matt Post, Herve\nSaint-Amand, Radu Soricut, Lucia Specia, and Aleš\nTamchyna. 2014. Findings of the 2014 workshop on\nstatistical machine translation. In Proc. of WMT.\nOndˇrej Bojar, Rajen Chatterjee, Christian Federmann,\nYvette Graham, Barry Haddow, Shujian Huang,\nMatthias Huck, Philipp Koehn, Qun Liu, Varvara Lo-\ngacheva, Christof Monz, Matteo Negri, Matt Post,\nRaphael Rubino, Lucia Specia, and Marco Turchi.\n2017. Findings of the 2017 conference on machine\ntranslation (WMT17). In Proc. of WMT.\nOndˇrej Bojar, Rajen Chatterjee, Christian Federmann,\nYvette Graham, Barry Haddow, Matthias Huck, An-\ntonio Jimeno Yepes, Philipp Koehn, Varvara Lo-\ngacheva, Christof Monz, Matteo Negri, Aurélie\nNévéol, Mariana Neves, Martin Popel, Matt Post,\nRaphael Rubino, Carolina Scarton, Lucia Spe-\ncia, Marco Turchi, Karin Verspoor, and Marcos\nZampieri. 2016. Findings of the 2016 conference\non machine translation. In Proc. of WMT.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners. In Proc. of NeurIPS.\nRewon Child, Scott Gray, Alec Radford, and Ilya\nSutskever. 2019. Generating long sequences with\nsparse transformers.\nKyunghyun Cho, Bart van Merriënboer, Dzmitry Bah-\ndanau, and Yoshua Bengio. 2014. On the properties\nof neural machine translation: Encoder–decoder ap-\nproaches. In Proc. of SSST-8.\nKrzysztof Choromanski, Valerii Likhosherstov, David\nDohan, Xingyou Song, Andreea Gane, Tamás Sar-\nlós, Peter Hawkins, Jared Davis, Afroz Mohiuddin,\nLukasz Kaiser, David Belanger, Lucy Colwell, and\nAdrian Weller. 2021. Rethinking attention with Per-\nformers. In Proc. of ICLR.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na ﬁxed-length context. In Proc. of ACL.\nMostafa Dehghani, Stephan Gouws, Oriol Vinyals,\nJakob Uszkoreit, and Lukasz Kaiser. 2019. Univer-\nsal transformers. In Proc. of ICLR.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proc. of NAACL.\nAngela Fan, Edouard Grave, and Armand Joulin. 2020.\nReducing transformer depth on demand with struc-\ntured dropout. In Proc. of ICLR.\n10639\nKunihiko Fukushima. 1980. Neocognitron: A self-\norganizing neural network model for a mechanism\nof pattern recognition unaffected by shift in position.\nBiological Cybernetics.\nJonas Gehring, Michael Auli, David Grangier, Denis\nYarats, and Yann Dauphin. 2017. Convolutional se-\nquence to sequence learning. In Proc. of ICML.\nJiatao Gu, James Bradbury, Caiming Xiong, Vic-\ntor O. K. Li, and Richard Socher. 2018. Non-\nautoregressive neural machine translation. In Proc.\nof ICLR.\nHany Hassan, Anthony Aue, Chang Chen, Vishal\nChowdhary, Jonathan Clark, Christian Feder-\nmann, Xuedong Huang, Marcin Junczys-Dowmunt,\nWilliam Lewis, Mengnan Li, Shujie Liu, Tie-Yan\nLiu, Renqian Luo, Arul Menezes, Tao Qin, Frank\nSeide, Xu Tan, Fei Tian, Lijun Wu, Shuangzhi Wu,\nYingce Xia, Dongdong Zhang, Zhirui Zhang, and\nMing Zhou. 2018. Achieving human parity on au-\ntomatic Chinese to English news translation.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In Proc. of CVPR.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2021. DeBERTa: Decoding-\nenhanced BERT with disentangled attention.\nGeoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean.\n2015. Distilling the knowledge in a neural network.\nIn Proc. of NeurIPS Deep Learning and Representa-\ntion Learning Workshop.\nSepp Hochreiter and Jürgen Schmidhuber. 1997. Long\nshort-term memory. Neural Computation.\nHakan Inan, Khashayar Khosravi, and Richard Socher.\n2017. Tying word vectors and word classiﬁers: A\nloss framework for language modeling. In Proc. of\nICLR.\nJungo Kasai, Nikolaos Pappas, Hao Peng, James Cross,\nand Noah A. Smith. 2021. Deep encoder, shallow\ndecoder: Reevaluating non-autoregressive machine\ntranslation. In Proc. of ICLR.\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pap-\npas, and François Fleuret. 2020. Transformers are\nRNNs: Fast autoregressive transformers with linear\nattention. In Proc. of ICML.\nYoon Kim and Alexander M. Rush. 2016. Sequence-\nlevel knowledge distillation. In Proc. of EMNLP.\nYoung Jin Kim, Marcin Junczys-Dowmunt, Hany Has-\nsan, Alham Fikri Aji, Kenneth Heaﬁeld, Roman\nGrundkiewicz, and Nikolay Bogoychev. 2019. From\nresearch to production and back: Ludicrously fast\nneural machine translation. In Proc. of WNGT.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam:\nA method for stochastic optimization. In Proc. of\nICLR.\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.\n2020. Reformer: The efﬁcient transformer. In Proc.\nof ICLR.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite BERT for self-supervised\nlearning of language representations. In Proc. of\nICLR.\nJuho Lee, Yoonho Lee, Jungtaek Kim, Adam Ko-\nsiorek, Seungjin Choi, and Yee Whye Teh. 2019.\nSet transformer: A framework for attention-based\npermutation-invariant neural networks. In Proc. of\nICML.\nTao Lei. 2021. When attention meets fast recurrence:\nTraining language models with reduced compute.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proc. of ACL.\nPeter J. Liu, Mohammad Saleh, Etienne Pot, Ben\nGoodrich, Ryan Sepassi, Lukasz Kaiser, and Noam\nShazeer. 2018. Generating Wikipedia by summariz-\ning long sequences. In Proc. of ICLR.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke S. Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach.\nIlya Loshchilov and Frank Hutter. 2017. SGDR:\nstochastic gradient descent with restarts.\nSwetha Mandava, Szymon Migacz, and Alex Fit Florea.\n2020. Pay attention when required.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture mod-\nels. In Proc. of ICLR.\nPaul Michel, Omer Levy, and Graham Neubig. 2019.\nAre sixteen heads really better than one? In Proc. of\nNeurIPS.\nPaulius Micikevicius, Sharan Narang, Jonah Alben,\nGregory Diamos, Erich Elsen, David Garcia, Boris\nGinsburg, Michael Houston, Oleksii Kuchaiev,\nGanesh Venkatesh, and Hao Wu. 2018. Mixed pre-\ncision training. In Proc. of ICLR.\nNVIDIA. 2014. The NVIDIA CUDA basic linear alge-\nbra subroutines (CUBLAS).\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensible\ntoolkit for sequence modeling. In NAACL Demon-\nstrations.\n10640\nMyle Ott, Sergey Edunov, David Grangier, and\nMichael Auli. 2018. Scaling neural machine trans-\nlation. In Proc. of WMT.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. BLEU: a method for automatic eval-\nuation of machine translation. In Proc. of ACL.\nNiki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz\nKaiser, Noam Shazeer, Alexander Ku, and Dustin\nTran. 2018. Image transformer. In Proc. of ICML.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. PyTorch:\nAn imperative style, high-performance deep learn-\ning library. In Proc. of NeurIPS.\nHao Peng, Nikolaos Pappas, Dani Yogatama, Roy\nSchwartz, Noah A. Smith, and Lingpeng Kong.\n2021. Random feature attention. In Proc. of ICLR.\nOﬁr Press, Noah A. Smith, and Omer Levy. 2020. Im-\nproving transformer models by reordering their sub-\nlayers. In Proc. of ACL.\nOﬁr Press, Noah A. Smith, and Mike Lewis. 2021.\nShortformer: Better language modeling using\nshorter inputs. In Proc. of ACL.\nOﬁr Press and Lior Wolf. 2017. Using the output em-\nbedding to improve language models. In Proc. of\nEACL.\nJiezhong Qiu, Hao Ma, Omer Levy, Wen-tau Yih,\nSinong Wang, and Jie Tang. 2020. Blockwise self-\nattention for long document understanding. In Proc.\nof EMNLP.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nJack W. Rae, Anna Potapenko, Siddhant M. Jayakumar,\nChloe Hillier, and Timothy P. Lillicrap. 2020. Com-\npressive transformers for long-range sequence mod-\nelling. In Proc. of ICLR.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. JMLR.\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott\nGray, Chelsea V oss, Alec Radford, Mark Chen, and\nIlya Sutskever. 2021. Zero-shot text-to-image gener-\nation.\nAurko Roy, Mohammad Saffar, Ashish Vaswani, and\nDavid Grangier. 2020. Efﬁcient content-based\nsparse attention with routing transformers. TACL.\nImanol Schlag, Kazuki Irie, and Jürgen Schmidhuber.\n2021. Linear transformers are secretly fast weight\nmemory systems. In Proc. of ICML.\nJean Senellart, Dakun Zhang, Bo Wang, Guillaume\nKlein, Jean-Pierre Ramatchandirin, Josep Crego,\nand Alexander Rush. 2018. OpenNMT system de-\nscription for WNMT 2018: 800 words/sec on a\nsingle-core CPU. In Proc. of WNG.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proc. of ACL.\nSheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei\nYao, Amir Gholami, Michael W. Mahoney, and Kurt\nKeutzer. 2020. Q-BERT: hessian based ultra low\nprecision quantization of BERT. In Proc. of AAAI.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: A simple way to prevent neural networks\nfrom overﬁtting. JMLR.\nSainbayar Sukhbaatar, Edouard Grave, Piotr Bo-\njanowski, and Armand Joulin. 2019. Adaptive at-\ntention span in transformers. In Proc. of ACL.\nYi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan,\nZhe Zhao, and Che Zheng. 2021. Synthesizer: Re-\nthinking self-attention in transformer models. In\nProc. of ICML.\nYi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-\nCheng Juan. 2020a. Sparse sinkhorn attention. In\nProc of ICML.\nYi Tay, M. Dehghani, Dara Bahri, and Donald Metzler.\n2020b. Efﬁcient Transformers: A survey.\nYao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada,\nLouis-Philippe Morency, and Ruslan Salakhutdinov.\n2019. Transformer dissection: An uniﬁed under-\nstanding for transformer’s attention via the lens of\nkernel. In Proc. of EMNLP.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proc. of NeurIPS.\nSinong Wang, Belinda Z. Li, Madian Khabsa, Han\nFang, and Hao Ma. 2020. Linformer: Self-attention\nwith linear complexity.\nRonald J. Williams and David Zipser. 1989. A learn-\ning algorithm for continually running fully recurrent\nneural networks. Neural Computation.\nFelix Wu, Angela Fan, Alexei Baevski, Yann Dauphin,\nand Michael Auli. 2019. Pay less attention with\nlightweight and dynamic convolutions. In Proc. of\nICLR.\nWeiqiu You, Simeng Sun, and Mohit Iyyer. 2020.\nHard-coded Gaussian attention for neural machine\ntranslation. In Proc. of ACL.\n10641\nOﬁr Zafrir, Guy Boudoukh, Peter Izsak, and Moshe\nWasserblat. 2019. Q8BERT: quantized 8bit BERT.\nIn Proc. of EMC2.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago On-\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nLi Yang, and Amr Ahmed. 2020. Big Bird: Trans-\nformers for longer sequences. In Proc. of NeurIPS.\nA Appendix\nA.1 Hyperparameters and Setting\nAll training is implemented infairseq (Ott et al.,\n2019) and run with PyTorch 1.7.1 (Paszke et al.,\n2019), 8 Telsa V100 GPUs, and CUDA 11.0. We\nused mixed precision and distributed training over\n8 GPUs (Micikevicius et al., 2018; Ott et al., 2018).\nApart from EN→ZH where we used separate BPE\noperations and only tied the decoder input and out-\nput embeddings, we tie all embeddings (Press and\nWolf, 2017; Inan et al., 2017). We experimented\nwith feature sizes of [16, 32, 64] and [4, 8, 16, 32]\nfor language modeling and machine translation re-\nspectively, and chose the smallest feature sizes that\nretained the development performance compared\nto the standard transformer.\nA.1.1 Language Modeling\nWe generally follow the optimization method from\nBaevski and Auli (2019). For optimizing a model\nfrom random initialization, the learning rate is lin-\nearly warmed up from 10−7 to 1 for the initial 16K\nsteps and then annealed using a cosine learning\nrate schedule with cycles (Loshchilov and Hutter,\n2017). Each period lasts for twice the number of\nupdates than the previous cycle, and we lower the\nmaximum and minimum learning rates by 25%\ncompared to the previous cycle. The initial mini-\nmum and maximum learning rates are 10−5 and 1\nrespectively (Baevski and Auli, 2019). We train the\nmodel with a batch size of about 74K tokens with\na total of 286K steps (Baevski and Auli, 2019).\nWhen we convert a pretrained transformer to an\nRNN model by ﬁnetuning, we found that we could\nspeed up training by reducing the warm-up steps,\ntotal update steps, maximum and minimum rates,\nand batch size to 8K steps, 142K steps, 5 ⋅10−6,\n0.5, and 25K tokens without loss in validation per-\nplexity.\nRandomly Initialized Training We generally\nfollow the hyperparameters chosen in Baevski and\nAuli (2019); Fan et al. (2020). Speciﬁcally, we list\nthe hyperparameters in Table 3 for easy replication.\nAll other hyperparameter options are left as default\nvalues in fairseq.\nFinetuning Pretrained Transformer Seen in\nTable 4 are the hyperparameters for ﬁnetuning a\npretrained transformer to RNN models. The learn-\ning rates, the max number of updates, and the learn-\ning period length are all reduced.\n10642\narchitecture transformer_lm_wiki103\ncriterion adaptive_loss\ntokens-per-sample 512\nsample-break-mode none\n# max tokens 3072\ndropout rate 0.2\nlayer dropout rate 0.2\ndecoder embed dim 1024\ndecoder ffn dim 4096\n# decoder attn heads 8\noptimizer nag\nlr-scheduler cosine\nlr-period-updates 270K\nlr-shrink 0.75\nt-mult 2\nmax-lr 1\nmin-lr 1e-9\nlr 1e-4\nclip-norm 0.1\nwarm-up lr 1e-7\n# warmup updates 16K\n# max updates 286K\n# GPUs 8\nupdate-freq 3\nTable 3: Language modeling hyperparameters when\nrandomly initialized in the fairseq library.\nA.1.2 Machine Translation\nWe experiment with 3 translation benchmarks:\nWMT14 EN-DE (4.5M train pairs, Bojar et al.,\n2016), WMT14 EN-FR (36M, Bojar et al., 2014),\nand WMT17 ZH-EN (20M, Bojar et al., 2017). We\nfollow the preprocessing and data splits by previ-\nous work (EN-DE: Vaswani et al., 2017; EN-FR:\nGehring et al., 2017; EN-ZH: Hassan et al., 2018;\nWu et al., 2019). These datasets are all encoded\ninto subwords by BPE (Sennrich et al., 2016). We\nrun joint BPE on all language pairs except EN-ZH.\nWe use the hyperparameters of the large sized trans-\nformer (Vaswani et al., 2017): 6 layers, 16 attention\nheads, 1024 model dimensions, and 4096 hidden\ndimensions for both the encoder and decoder. We\napply dropout with0.3, weight decay with0.01 and\nlabel smoothing with ε=0.1. Following Ott et al.\n(2018), we use an increased batch size of approx-\nimately 460K tokens by accumulating gradients\nwithout updating parameters.\nRandomly Initialized Training We generally\nfollow the hyperparameters chosen in Vaswani et al.\n(2017); Ott et al. (2018). Speciﬁcally, we list the\nhyperparameters in Table 5 for easy replication. All\nother hyperparamter options are left as default val-\nues in fairseq. The parameters from the last ﬁve\nepochs were averaged to obtain the ﬁnal model.\nFinetuning Pretrained Transformer Seen in\nTable 6 are the hyperparameters for ﬁnetuning a\narchitecture transformer_lm_wiki103\ncriterion adaptive_loss\ntokens-per-sample 512\nsample-break-mode none\n# max tokens 3072\ndropout rate 0.2\nlayer dropout rate 0.2\ndecoder embed dim 1024\ndecoder ffn dim 4096\n# decoder attn heads 8\noptimizer nag\nlr-scheduler cosine\nlr-period-updates 135K\nlr-shrink 0.75\nt-mult 2\nmax-lr 0.5\nmin-lr 1e-9\nlr 5e-5\nclip-norm 0.1\nwarm-up lr 1e-7\n# warmup updates 8K\n# max updates 142K\n# GPUs 8\nupdate-freq 1\nTable 4: Finetuning language modeling hyperparame-\nters in the fairseq library. The learning rates are\nsmaller than randomly initialized training.\npretrained transformer to RNN models. The learn-\ning rate and the max number of updates are reduced.\nThe parameters from the last ﬁve epochs were again\naveraged to obtain the ﬁnal model.\nA.2 Attention Distribution\nPeakiness of Attention Fig. 6 plots the average\nentropy of the T2R models with and without pre-\ntraining. Entropy is averaged across validation sam-\nples, layers, and attention heads. Comparing Figs.\n4 and 6, we see that there is strong correlation\nbetween validation perplexity and entropy. The en-\ntropy decreases (and thus the attention distribution\ngets peakier) when a large feature size is used or the\ntransformer pretraining is applied. This observa-\ntion hints at potential future improvement of linear\ntransformer models by introducing an inductive\nbias towards peaky attention distributions.\n10643\narchitecture transformer_vaswani_en_de_big\ncriterion label_smoothed_cross_entropy\nlabel smoothing 0.1\n# max tokens 3584\ndropout rate 0.3\nweight decay 0.0\nencoder embed dim 1024\nencoder ffn dim 4096\n# encoder attn heads 16\n# encoder layers 6\ndecoder embed dim 1024\ndecoder ffn dim 4096\n# decoder attn heads 16\n# decoder layers 6\nmax source positions 1024\nmax target positions 1024\nAdam lrate 5e-4, 3e-4 (T2R)*\nAdam β1 0.9\nAdam β2 0.98\nlr-scheduler inverse square\nwarm-up lr 1e-7\n# warmup updates 4000\n# max updates 30K, 60K (EN-FR)\nlength penalty 0.6\nbeam size 5\n# GPUs 8\nupdate-freq 16\nTable 5: Machine translation hyperparameters when\nrandomly initialized in the fairseq library. *: we\nreduced the learning rate for T2R to avoid training di-\nvergence.\narchitecture transformer_vaswani_en_de_big\ncriterion label_smoothed_cross_entropy\nlabel smoothing 0.1\n# max tokens 3584\ndropout rate 0.3\nweight decay 0.0\nencoder embed dim 1024\nencoder ffn dim 4096\n# encoder attn heads 16\n# encoder layers 6\ndecoder embed dim 1024\ndecoder ffn dim 4096\n# decoder attn heads 16\n# decoder layers 6\nmax source positions 1024\nmax target positions 1024\nAdam lrate 2e-4\nAdam β1 0.9\nAdam β2 0.98\nlr-scheduler inverse square\nwarm-up lr 1e-7\n# warmup updates 4000\n# max updates 20K, 40K (EN-FR)\nlength penalty 0.6\nbeam size 5\n# GPUs 8\nupdate-freq 16\nTable 6: Finetuning machine translation hyperparame-\nters. The learning rate is smaller than randomly initial-\nized training.\n8 16 32 64 128 256 512\n3.5\n4\n4.5\n5\nTransformer\nFeature Sizek\nAverage Attention Entropy\nT2R + Random Init.\nT2R + Pretrain\nFigure 6: Average entropy of the attention weights.\nThey are computed on the Wikitext-103 validation data\nfor predicting a word given the preceding 512 words.",
  "concepts": [
    {
      "name": "Softmax function",
      "score": 0.7477253675460815
    },
    {
      "name": "Computer science",
      "score": 0.7371464967727661
    },
    {
      "name": "Transformer",
      "score": 0.6758763790130615
    },
    {
      "name": "Recurrent neural network",
      "score": 0.6530158519744873
    },
    {
      "name": "Inference",
      "score": 0.5162127614021301
    },
    {
      "name": "Artificial intelligence",
      "score": 0.49951839447021484
    },
    {
      "name": "Machine learning",
      "score": 0.4496668577194214
    },
    {
      "name": "Encoder",
      "score": 0.4178738296031952
    },
    {
      "name": "Artificial neural network",
      "score": 0.19892385601997375
    },
    {
      "name": "Voltage",
      "score": 0.12310811877250671
    },
    {
      "name": "Engineering",
      "score": 0.1182197630405426
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "topic": "Softmax function"
}