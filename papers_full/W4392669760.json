{
  "title": "Retrieval Augmented Generation with Rich Answer Encoding",
  "url": "https://openalex.org/W4392669760",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A1964010710",
      "name": "Wenyu Huang",
      "affiliations": [
        "University of Edinburgh"
      ]
    },
    {
      "id": "https://openalex.org/A2019867208",
      "name": "Mirella Lapata",
      "affiliations": [
        "University of Edinburgh"
      ]
    },
    {
      "id": "https://openalex.org/A2561025582",
      "name": "Pavlos Vougiouklis",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2531624279",
      "name": "Nikos Papasarantopoulos",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2112124874",
      "name": "Jeff Pan",
      "affiliations": [
        "University of Edinburgh"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4327810158",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3198527962",
    "https://openalex.org/W2951534261",
    "https://openalex.org/W3156789018",
    "https://openalex.org/W2584261443",
    "https://openalex.org/W3213159541",
    "https://openalex.org/W151145963",
    "https://openalex.org/W2889782235",
    "https://openalex.org/W4223648767",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4286903770",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W3198637126",
    "https://openalex.org/W2963961878",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W2898875342",
    "https://openalex.org/W2785611959",
    "https://openalex.org/W3207796810",
    "https://openalex.org/W4287887100",
    "https://openalex.org/W3155584966",
    "https://openalex.org/W3169283738",
    "https://openalex.org/W4392669750",
    "https://openalex.org/W4378464611",
    "https://openalex.org/W2963149704",
    "https://openalex.org/W4287900772",
    "https://openalex.org/W4385569882",
    "https://openalex.org/W4385572149",
    "https://openalex.org/W2991486856",
    "https://openalex.org/W4385848430",
    "https://openalex.org/W4282980384",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W3153805297",
    "https://openalex.org/W2963469388",
    "https://openalex.org/W2123301721",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W2962881743",
    "https://openalex.org/W4385728911",
    "https://openalex.org/W3155807546",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3099524945",
    "https://openalex.org/W1491520333",
    "https://openalex.org/W3021397474",
    "https://openalex.org/W2998702515",
    "https://openalex.org/W4313483736",
    "https://openalex.org/W2094689321",
    "https://openalex.org/W21517912",
    "https://openalex.org/W4295830359"
  ],
  "abstract": "Wenyu Huang, Mirella Lapata, Pavlos Vougiouklis, Nikos Papasarantopoulos, Jeff Pan. Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.",
  "full_text": "Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of\nthe Asia-Paciﬁc Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1012–1025\nNovember 1–4, 2023. ©2023 Association for Computational Linguistics\n1012\nRetrieval Augmented Generation with Rich Answer Encoding\nWenyu Huang1, Mirella Lapata1, Pavlos Vougiouklis2,\nNikos Papasarantopoulos3∗, Jeff Z. Pan1,2†\n1School of Informatics, University of Edinburgh\n2Huawei Edinburgh Research Centre, CSI, Edinburgh, UK\n3Priceline, Edinburgh, UK\nw.huang@ed.ac.uk mlap@inf.ed.ac.uk pavlos.vougiouklis@huawei.com\nnikos.papasa@gmail.com https://knowledge-representation.org/j.z.pan/\nAbstract\nKnowledge-intensive generation tasks like gen-\nerative question answering require models to\nretrieve appropriate passages from external\nknowledge sources to support answer gener-\nation. The generation quality relies heavily\non the retrieved passages, which serve as con-\ntextual information. State-of-the-art Retrieval\nAugmented Generation models with marginal-\nized output dominate this area but focus too\nmuch on label-relevant passages, rather than\nquestion-relevant passages and answers. This\nwork addresses this issue by incorporating rich\nanswer encoding through Dense Knowledge\nSimilarity (DKS) and Retriever as Answer\nClassifier (RAC). We demonstrate the advan-\ntages of our proposed approach in open do-\nmain question answering (MSMARCO) and\nconversation (Wizard of Wikipedia) datasets,\nreporting both generation and retrieval met-\nrics. In the MSMARCO development set, our\nbest model achieves 12.1% relative improve-\nment1 on Recall@1 and 4.5% relative improve-\nment on BLEU-4 compared to the baseline\nmodel. In the KILT-WoW leaderboard, our\nbest model achieves 8.9% relative improvement\non R-Precision and 13.3% relative improve-\nment on KILT-RL compared to the baseline\nmodel. Our codes and models are available at\nhttps://github.com/hwy9855/rag-ae.\n1 Introduction\nKnowledge-Intensive Generation (KIG) is a series\nof tasks that requires external knowledge sources,\nsuch as a passage corpus (focus of this paper) or\na knowledge graph (Pan et al., 2017) to generate\nnatural language responses to questions. Without\nexternal knowledge, even the state-of-the-art Large\nLanguage Models (LLMs) like GPT-4 (OpenAI,\n2023) still suffer from the hallucination problem\n∗ Work done while at Huawei Edinburgh Research Centre.\n† Corresponding author.\n1In this paper, we mainly report relative improvement for\nbetter comparison between different methods.\nQuestion: Definition of tactful personality\nReference Answer (label): Tactful is someone or some-\nthing thatshows a regard for other people’s feelings.\nPassage 1 (label-relevant): The definition of tact-\nful is someone or something that shows a regard\nfor other people’s feelings. An example of some-\nthing that would be described as tactful is a sug-\ngestion that is worded very carefully so as not to\noffend.\nRAG\n✓\nours\n✓\nPassage 2 (knowledge-relevant): ... tactful(adj)\nhaving or showing a sense of what is fitting\nand considerate in dealing with others. she\nwas tactful enough not to shatter his illusion; ...\nRAG\n✗\nours\n✓\nTable 1: Retrieval samples of MSMARCO training set.\nBoth models are trained with the given input (first row).\nThe RAG model only captures the label-relevant pas-\nsage with high token matching (highlighted with italics)\nin the reference answer, while our approach can also\ncapture the knowledge-relevant passage with knowledge\nmatching (highlighted with bold).\n(Pan et al., 2023a; Zhang et al., 2023) that pro-\nduces plausible-looking statements that are fac-\ntually incorrect. State-of-the-art works focus on\nthe retrieval augmented generation (RAG) systems\n(Karpukhin et al., 2020; Izacard and Grave, 2021),\nwhich fit in a retrieve-generate architecture, where\nthe models first retrieve question-relevant passages\nfrom external knowledge sources, then generate\nresponses based on the retrieved passages.\nA high-quality retriever is essential for accom-\nplishing KIG tasks. In the era of LLMs, a high-\nquality off-the-shelf retriever is also important in\nverifying and correcting the factual errors pro-\nduced by LLMs (He et al., 2023; Zhao et al., 2023)\nHowever, directly training the retriever (Karpukhin\net al., 2020; Khattab and Zaharia, 2020) requires\na large number of annotations of gold passages\naccording to the given questions. Current state-of-\nthe-art works (Lewis et al., 2020b; Paranjape et al.,\n2022) provide an end-to-end training framework\nfor this task, where they marginalize the retrieval\nstep and use the natural language labels to guide the\n1013\ntraining of both retriever and generator. The gradi-\nent of both the retriever and generator is obtained\nfrom token matches with reference answers (labels).\nHowever, existing works prefer more label-relevant\npassages (a subset of question-relevant passages),\nthis will result in missing some knowledge-relevant\npassages which limits the generalisability of the\nretriever. See the example presented in Table 1.\nGiven the question, the RAG framework can only\nretrieve the Passage 1, due to label-relevance, but\nnot the Passage 2, which is also highly relevant\nto the reference answer. Using limited gold label\npassages when training a retrieve-generate model\nharms the robustness resulting in overfitting the re-\ntrieval performance to the training data (as shown\nin Table 5).\nTo mitigate the above issue, in this work, we pro-\npose a new framework for extending RAG with rich\nanswer encoding, based on knowledge relevant to\nanswers. More precisely, we introduce the follow-\ning two objectives: Retriever as Answer Classifier\n(RAC), and Dense Knowledge Similarity (DKS),\nfor training the retriever to retrieve knowledge-\nrelevant passages. RAC incorporates answer en-\ncoding to check whether the retrieved passage con-\ntains the knowledge inside the answer, where both\npassages in Table 1 are positive as they both con-\ntain knowledge inside the answer. DKS incorpo-\nrates both answer encoding and passage encoding\nto check how close the knowledge inside the an-\nswer and that inside the passage are in the knowl-\nedge representation 2 space. The answer encod-\ning and passage encoding in DKS guarantees that\nknowledge-relevant passages (both passages in Ta-\nble 1) are much closer to the label (than other pas-\nsages) in the knowledge representation space.\nBoth proposed objectives focus on sequence sim-\nilarity (by answer encoding) instead of the token-\nlevel similarity from the backbone architecture\n(NLL objective), which fundamentally turns the\nobjective from token matching to sequence (knowl-\nedge) matching. This allows the proposed loss\nfunctions to capture knowledge-level information\n(instead of token-level information) from answer\nlabels. This knowledge-level information can assist\nwith training the retriever to recall more knowledge-\nrelevant passages (instead of only label-relevant\n2In this paper, we use the term \"knowledge representation\"\nsince the learned representation is for representing the knowl-\nedge inside the answer and passage, which has a different\nmeaning from \"knowledge representation\" in the knowledge\ngraph area.\npassages) and improve the performance and gener-\nalisation ability of the end-to-end trained retrievers.\nOur main contribution is a new training frame-\nwork for retrieve-generate models that 1) offers\ntwo new objectives, RAC and DKS, incorporat-\ning rich answer encoding, and 2) paying more at-\ntention to knowledge-relevant passages with the\ntwo new objectives to train more robust retrieve-\ngenerate models that generalize better in retrieval.\nWe evaluate the proposed framework in two KIG\ntasks, which are the generative QA task on MS-\nMARCO (Nguyen et al., 2016) and the informa-\ntive conversation task on Wizard of Wikipedia (Di-\nnan et al., 2019) organized by KILT benchmark\n(Petroni et al., 2021). Compared with the base-\nline model RAG (Lewis et al., 2020b), we get sub-\nstantial improvement on both datasets. For the\nretrieval quality, our best model achieves 12.1%\nrelative improvement (Recall@1) on MSMARCO\nand 8.9% relative improvement (R-Prec) on KILT-\nWoW. While for end-to-end generation quality, our\nbest model achieves 4.5% relative improvement\n(BLEU-4) on MSMARCO and 13.3% relative im-\nprovement (KILT-RL) on KILT-WoW. The reliable\nimprovement in the retrieval quality also indicates\nthe potential contribution of the proposed work in\nthe era of LLM for training a powerful and robust\nretriever for supporting LLMs.\n2 Related Works\nKnowledge-intensive Generation Knowledge-\nintensive generation tasks are a series of Natural\nLanguage Generation (NLG) tasks that require ac-\ncess to large, external knowledge sources. Un-\nlike other knowledge-intensive NLP tasks such as\nfact checking (Thorne et al., 2018) and slot filling\n(Levy et al., 2017; Elsahar et al., 2018), knowledge-\nintensive generation tasks further require models\nto generate natural language response, which are\nmore challenging. Shuster et al. (2021) has shown\nthat in knowledge-intensive generation tasks, pre-\ntrained language models like BART (Lewis et al.,\n2020a), T5 (Raffel et al., 2020) and GPT series\n(Brown et al., 2020; OpenAI, 2023) significantly\nsuffer from the hallucination problem (Roller et al.,\n2021), where they generate plausible looking state-\nments that are factually incorrect.\nRetrieval Marginalization In retrieval-based\ntasks, the false-negative passage problem refers\nto passage labels not being fully annotated. Ni\net al. (2021) has shown that the false-negative pas-\n1014\npassage\nencoder\nquestion\nencoder\nRAG\nquestion\nencoder\nencAE encAE\nBART\ngenerator\ntop-n passages\ntop-n passages \nembeddings\nanswer as question\nembeddings\nMIPS\nPassage\nCorpus\nquestion\nanswers\n(reference & negative)\nWhy is argon \nused in light\nbulbs?\nNLLLoss\nMSELoss\nBCELoss\npassage knowledge \nembeddings\nanswer knowledge \nembeddings\np1: Now, with the light bulb filled with argon and some nitrogen because ...\np2: Helium, neon, argon and krypton are used in gas ...\n...\npn: ... Nitrogen along with Argon used to fill the electric bulb. Why? ...\nThe cortisone in the \ninjection is a type \nof steroid that \nworks by reducing \ninflammation in the \naffected area.\nArgon is used to fill \nincandescent light \nAbulbs to inhibit the \nevaporation of the \ntungsten filaments \nand increase bulb life.\nUtility is a term used by \neconomists to describe \nthe measurement of \nuseful-ness that a \nconsumer obtains from \nany good.\n...\nDKS\nRAC\nFigure 1: Overall architecture of the proposed model. MIPS indicates Maximum Inner Product Search. Green\narrows indicate a different information stream with black arrows. Orange components are trainable while gray\ncomponents are not trainable. The dashed box indicates the retrieve-generate backbone (Lewis et al., 2020b). Violet\nboxes indicate the two objectives introduced by this work. The proposed DKS and RAC aim to help the retriever\nfocus more on knowledge-relevant passages instead of only label-relevant passages.\nsage problem happens very frequently in multi-\ndocument question tasks, with such cases appear-\ning in more than half of the sampled answerable\nquestions of the IIRC dataset (Ferguson et al.,\n2020). Research has been made to mitigate this\nproblem by marginalizing the retrieval process and\ndirectly training the retriever with the final goal,\ne.g., answer label in multi-document QA (Ni et al.,\n2021). However, in knowledge-intensive gener-\nation tasks, the marginalization methods do not\nwork so well. Lewis et al. (2020b) reported that\nsuch marginalization methods get very limited im-\nprovement in benefiting the retrieval quality on the\ngenerative QA task.\nRetrieval Augmented Generation To overcome\nthe hallucination problem, people start to introduce\nretrieve-generate architectures for building retrieval\naugmented generation models. RAG (Lewis et al.,\n2020b) outperforms DPR (Karpukhin et al., 2020)\nby marginalizing the retrieval step to train the gen-\nerator and retriever jointly with the supervision of\nthe label answer. FiD (Izacard and Grave, 2021)\nencodes the concatenation of the passages retrieved\nby pre-trained DPR and the original question sep-\narately, and then fuses them with concatenation\nto the decoder. KG-FiD (Yu et al., 2022) utilize\nknowledge graphs to further enhance the retrieval\nquality by establishing the structural relationship\namong the retrieved passages. This illustrates the\nconcept of integrating knowledge graphs with re-\ntrieval augmented generation, but indirectly (i.e.,\nthe generator does not benefit from knowledge\ngraphs). KGI (Chowdhury et al., 2022) provides a\nrobust implementation of RAG, where the retriever\nis trained jointly with both the RAG setting and\nthe vanilla DPR setting. RE2G (Glass et al., 2022)\nfurther extends RAG by adding a reranker in the\nretrieval step to integrate statistical retriever (e.g.,\nTF-IDF and BM25) with DPR trained by RAG to\nimprove the retrieval quality. Hindsight (Paranjape\net al., 2022) trains another hindsight retriever that\ntakes inputs of both queries and labels to retrieve\nlabel-relevant passages. The original retriever is\ntrained by Evidence Lower Bound (ELBo) which\nincludes both the marginalized loss as in RAG and\nthe KL divergence for fitting the hindsight retriever.\nThe state-of-the-art methods tend to retrieve label-\nrelevant passages, whereas our approach seeks to\nconsider a more diverse set of passages as suitable\n(knowledge-wise).\nRetrieval Augmented Generation with LLMs\nThe utilization of retrieval augmented generation\ncan be further expanded within the context of Large\nLanguage Models (LLMs) (Pan et al., 2023a), to\naddress a critical concern associated with LLMs:\nthe hallucination problem. He et al. (2023) use re-\ntrieved passages to help LLMs \"rethink\" the ques-\ntion to remove factual errors in the decomposed\nreasoning steps obtained from the chain-of-thought\n(CoT) prompting (Wei et al., 2022). Zhao et al.\n1015\n(2023) utilize retrieved passages to post-edit the\ndecomposed reasoning steps which contain factual\nerrors obtained from the CoT prompting, to in-\ncrease the prediction factuality. Besides retrieving\ndocuments (passages), Baek et al. (2023) shows\nthat an off-the-shelf document retriever can also be\nused to retrieve knowledge graph triples for sup-\nporting LLMs. Most of these works utilize off-the-\nshelf retrievers, which emphasize the importance\nof powerful and robust retrievers. Their remark-\nable achievements underscore the central theme of\nour work, which focuses on training powerful and\nrobust retrievers.\n3 Problem Statement\nA typical Knowledge-Intensive Generation (KIG)\ntask can be formulated as follows: 1) given a ques-\ntion q and a knowledge source (passage corpus)\nP, retrieve question-relevant passages Pq ⊂ P;\n2) conditioned on the question q and the retrieved\nquestion-relevant passages Pq, generate the natu-\nral language response y. Since Pq is not available,\nstate-of-the-art works marginalize the first retrieval\nstep and train both retriever and generator based\non gradient from the negative log-likelihood loss.\nThis results in training a biased retriever which\nfavors label-relevant passages Pl ⊆ Pq, a biased\nsubset of question-relevant passages. Thus in train-\ning, the retriever will overfit to these label-relevant\npassages.\nIn this work, we consider another subset of\nquestion-relevant passages, which is knowledge-\nrelevant passages Pk ⊆ Pq. We define Pk as pas-\nsages that contain the knowledge to generate label\nanswer y. Therefore it is obvious that Pl ⊆ Pk.\nUnlike label-relevant passages that usually have\nhigh token overlap with label answery, knowledge-\nrelevant passages are defined at a higher knowledge\nlevel. In examples from Table 1, the Passage 1 is\na label-relevant passage (also the gold passage),\nwhile the Passage 2 is a knowledge-relevant pas-\nsage, which has the same meaning as the label\nanswer but with little token overlap with the former.\nBy introducing a training framework that focuses\non retrieving knowledge-relevant passages, the re-\ntriever can be trained more robustly and provide\nbetter generalization ability.\n4 Methodology\nIn order to focus on retrieving knowledge-relevant\npassages in KIG tasks, we introduce a new training\nframework with two new objectives: Retriever as\nAnswer Classifier (RAC), and Dense Knowledge\nSimilarity (DKS). Both objectives aim to mitigate\nthe problem discussed in Section 3, that the re-\ntriever solely trained with NLL loss prefers label-\nrelevant passages rather than question-relevant pas-\nsages. Figure 1 shows the overall architecture of\nthe proposed model.\n4.1 Retrieve-Generate Backbone\nTransformer-based retrieve-generate systems are\nwidely used in knowledge-intensive generation\ntasks like generative QA and conversational search\n(Cooper Stickland et al., 2021; Adiwardana et al.,\n2020). To apply our knowledge-constrained objec-\ntives, we need a retrieve-generate architecture as\nthe backbone. Here we choose to use the architec-\nture from the RAG paper (Lewis et al., 2020b) as\nthe retrieve-generate backbone. Given a question q\nand passage corpus P, we first use a Dense Passage\nRetriever (DPR; Karpukhin et al. (2020)) to com-\npute the similarity of each passage pi ∈ Pwith q:\nsim(pi, q) = encq(q)⊤encp(pi), (1)\nwhere encq is the question encoder and encp is the\npassage encoder. To facilitate easier comparison\nwith prior works, we follow the settings in Lewis\net al. (2020b) to fix the parameters of the passage\nencoder. Based on the similarity, we select top-\nn passages with Maximum Inner Product Search\n(MIPS) and calculate the probability\np(pi|q) = exp (sim(pi, q))P\npj∈Top-n(P|q) exp (sim(pj, q)), (2)\nwhere Top-n(P|q) is the selected top-n passages\nwith given q. Then these passages are concatenated\nwith the question and generate the probability dis-\ntribution of response tokens through a sequence-to-\nsequence encoder-decoder model:\np(yt|y:t−1, q, pi) = BART([q; pi], y:t−1) (3)\nppi (yt|q, y:t−1) = p(yt|y:t−1, q, pi)p(pi|q) (4)\npgen(yt|q, y:t−1) =\nX\npi∈Top-n(P|q)\nppi (yt|q, y:t−1)\n(5)\npgen(y|q) =\nTY\nt=1\npgen(yt|q, y:t−1), (6)\nwhere BART([q; pi], y:t−1) is the BART generator\n(Lewis et al., 2020a) that takes the concatenation\n1016\nof q and pi, and y:t−1 as input. The goal of the\nbackbone is to maximize the probability of label\nsentence ˆy, which is equal to minimizing the nega-\ntive log-likelihood of ˆy:\nLgen = −log pgen(ˆy|q) (7)\n4.2 Retriever as Answer Classifier\nSince we would like our retriever to retrieve\nknowledge-relevant passages instead of only re-\ntrieving label-relevant passages, we add a new an-\nswer classification objective, which makes use of\nrich answer encoding to capture knowledge inside\nthe answer. The answer classifier outputs whether\nthe retrieved top-n passages contain the knowledge\ninside the answer. Here we reuse the DPR retriever\n(sharing parameters) in the retrieve-generate back-\nbone to generate answer encoding and use it for\nclassifying the matched answer.\nWe apply in-batch negative sampling (Karpukhin\net al., 2020) for the RAC objective. Given a\nquestion-answer pair (ˆq, ˆy), we use m answers in\nthe same batch as negative answers from the rest\nof the training set:\nNeg = {y(1), y(2), . . . , y(m)}, (8)\nwhere the batch size is m + 1. We then compute\nthe probability of the answer (both positive and\nnegative) y given question ˆq as:\nsim(y, pˆq\ni ) = encp(pˆq\ni )⊤encq(y) (9)\nS(y, ˆq) =\nX\npˆq\ni ∈Top-n(P|ˆq)\np(pˆq\ni |ˆq)sim(y, pˆq\ni )\n(10)\npRAC(y|ˆq) = σ(S(y, ˆq)), (11)\nwhere p(pˆq\ni |ˆq) is computed with Eq. (2), encp and\nencq are the same as described in Section 4.1, and\nσ(·) is the sigmoid function for generating proba-\nbility. We then use binary cross-entropy loss to get\nthe training signal:\nLRAC = log(pRAC(ˆy|ˆq))\n+\nX\ny′∈Neg\nlog(1 − pRAC(y′|ˆq)) (12)\n4.3 Dense Knowledge Similarity\nAnother way of focusing on knowledge-relevant\npassages is to further marginalize the generation\nprocess. This objective directly focuses on knowl-\nedge instead of the answer label, which changes\nthe final goal from generating a natural language\nresponse to answer the question p(y|q) to generat-\ning knowledge k required to answer the question\np(k|q):\np(k|q) =\nX\ny\np(k|y)p(y|q) (13)\nThus the optimization process becomes maximiz-\ning p(ˆk|q). To accomplish that, we need to get the\ngold knowledge ˆk for training.\nWe choose to use the sentence bottleneck auto-\nencoder3 introduced by Montero et al. (2021) to get\nanswer knowledge representations. The original\ngoal of sentence bottleneck auto-encoder is to re-\nconstruct the input sequence through a bottleneck\nrepresentation between encoder and decoder (as\nshown in Figure 2):\nAE(y) = decAE(β(encAE(y))) = y′, (14)\nwhere β(·) is the knowledge extractor, a multihead\nattention mechanism that aggregates encoder states\nH to a single bottleneck representation:\nH = encAE(y) (15)\nz = β(H) (16)\nβ(H) = MultiHead(q, K, V ) (17)\nwhere q is the encoder states of [CLS] token, K\nand V are the encoder states H. Here we treat the\nbottleneck representation as the knowledge repre-\nsentation of the input sequence:\nk = z (18)\nk is a rich answer encoding containing knowledge\nneeded to rebuild the sentence itself, and thus, im-\nplicitly guarantees that the knowledge represented\nby a wrong answer is different from the one by\na right answer. We, subsequently, train another\nknowledge extractor α to extract knowledge repre-\nsentation from question-passage pairs:\nk′ =\nX\npi∈Top-n(P|q)\nα(encAE([q; pi]))p(pi|q) (19)\nWe expect k′ to be similar to k in the knowledge\nrepresentation space, thus we use MSE loss to ob-\ntain the training signal:\nLDKS = ||ˆk − k′||2\n2 (20)\n3Appendix B provides more details about our reasoning\nbehind selecting sentence bottleneck auto-encoder.\n1017\nencAE decAE\nFigure 2: Sentence Bottleneck Auto-endoer. The en-\ncoder is initialized from pre-trained language models\nand is not trainable.\nBy fitting the dense knowledge representations\n(rich answer encoding) instead of the original label\nanswers, we change the goal from retrieving label-\nrelevant passages to retrieving knowledge-relevant\npassages.\n4.4 Overall Architecture\nUsing the retrieve-generate backbone and the two\nobjectives with rich answer encoding, we can train\nthe model with all these objectives in a multi-task\nsetting. In training, we optimize the summation of\nall the different losses:\nL = Lgen + w(LRAC, LDKS )⊤, (21)\nwhere Lgen, LRAC and LDKS are computed in\nEq. (7), (12) and (20) respectively. w ∈ R2 is the\nweight vector for balancing different losses. We\ncompute w in each step to make sure that all the\nlosses will have the same value.\n5 Experimental Setup\nWe evaluate the proposed methods on two KIG\ntasks: generative question answering and informa-\ntive conversations. The designed experiments aim\nto answer the following research questions:\n• RQ1 Effectiveness: Can the proposed meth-\nods improve the retrieval and end-to-end gen-\neration quality?\n• RQ2 Robustness: Do the proposed methods\nfacilitate better generalization at retrieving rel-\nevant passages?\n5.1 Datasets\nFor generative question answering, we use the NL-\nGen split of the MSMARCO QA dataset (Nguyen\net al., 2016). The answers of the NLGen split\nare rewritten by crowdworkers into well-formed\nanswers for reducing overlaps between answers\nand gold passages. For the informative conversa-\ntion task, we use the Wizard of Wikipedia (WoW)\ndataset (Dinan et al., 2019) organized by KILT\n(Petroni et al., 2021).\nSince the test split of KILT-WoW is not pub-\nlished, we only report our best result on the test\nsplit as shown on the KILT leaderboard. All other\nexperiments are done on the public development\nset. For MSMARCO dataset, since the challenge\nis retired, and we do not have access to the full test\nsplit, we further split the original train split to be\nthe train set and validation set in our experiment,\nand report results on the original development split.\nAll the evaluation metrics reported in this paper\nare obtained using the official scripts provided by\nMSMARCO4 and KILT.5\n5.2 Metrics\nFor the MSMARCO dataset, we report ROUGE-L\nand BLEU-1/4 scores for evaluating the end-to-\nend generation quality. We also report the recall\nrate of retrieved passages to evaluate the retrieval\nquality. For KILT-WoW dataset, we follow the\nevaluation setup in Petroni et al. (2021) and report:\n1) ROUGE-L score and F1 score for the end-to-\nend evaluation; 2) R-Precision and Recall@5 for\nthe retrieval evaluation; 3) KILT-RL and KILT-F1\nfor the combined evaluation. R-Precision metric\nis identical to Precision@1 in KILT-WoW dataset\nsince the gold passage for every conversation is\na single Wikipedia page. KILT-RL and KILT-F1\nare two special metrics motivated by FEVER-score\n(Thorne et al., 2018), which calculate ROUGE-L\nscore and F1 score as 0 when the retriever does\nnot perfectly select the gold passage. Furthermore,\nfor both datasets, we use METEOR (Banerjee and\nLavie, 2005), which is a metric with a high corre-\nlation with human judgment, to mimic the human\npreference for the generations.\n5.3 Knowledge Sources\nFor both datasets, we retrieve passages from the\nknowledge sources which the gold passages are\nselected from. For MSMARCO, the knowledge\nsource contains 8.8M passages extracted from\n3.6M web documents retrieved by Bing, while\nfor KILT-WoW, the knowledge source is extracted\nfrom the 2019/08/01 Wikipedia dump, which after\nour preprocessing contains approximately 24.5M\npassages.\n5.4 Model Configuration\nFollowing Lewis et al. (2020b), we use DPR model\nfine-tuned with Natural Questions (Kwiatkowski\net al., 2019) to initialize the retriever, and\n4https://github.com/microsoft/\nMSMARCO-Question-Answering\n5https://github.com/facebookresearch/KILT\n1018\nModel End-to-end Retrieval\nB-1 B-4 R-L METEOR R@1 R@5 R@10\nRAG (ours) 53.9 33.3 58.6 49.7 10.7 25.3 32.1\nDPR (ours) - - - - 10.5 27.0 34.9\nours (RAC) 54.1 33.8 59.0 49.7 11.2 26.8 34.1\nours (DKS) 54.9 34.1 58.7 50.1 11.5 27.5 35.0\nours (multi-task) 55.1 34.8 59.1 50.5 12.0 28.3 36.0\nTable 2: Effectiveness evaluation results in MSMARCO development set. R-L represents ROUGE-L, B-1 and B-4\nrepresent BLEU-1 and BLEU-4. R@ k represents for recall rate for top- k retrieved passages. Bold highlighted\nvalues are the best among all.\nModel Retrieval End-to-end Combined\nR-Prec Recall@5 RL F1 METEOR KILT-RL KILT-F1\nRAG 42.21 61.98 15.68 17.92 12.90 7.74 8.83\nDPR 26.92 49.64 - - - - -\nours (RAC) 44.95 65.39 16.81 18.93 14.47 8.80 9.95\nours (DKS) 47.02 64.87 16.10 18.64 14.07 9.17 10.68\nours (multi-task) 43.94 63.59 16.66 19.04 14.41 8.58 9.84\nTable 3: Effectiveness evaluation results in KILT-WoW development set. RL and KILT-RL refer to ROUGE-L and\nKILT ROUGE-L. R-Prec refers to R-Precision, which is identical to Precision@1 in the KILT-WoW dataset.Bold\nhighlighted values are the best among all.\nBARTLARGE model to initialize the generator. For\nthe KILT-WoW dataset, we initialize all the models\nusing non-finetuned RAG checkpoint provided on\nHuggingFace6. For the MSMARCO dataset, we\nfind that using the same initialization strategy does\nnot work well for baseline and our proposed meth-\nods. We hypothesize that this is due to the questions\nof MSMARCO having a very different structure\ncompared to the Natural Question dataset (where\nthe initialized model is trained). Thus we first pre-\ntrain the DPR retriever on the MSMARCO passage\nranking task, then use the pre-trained model as\nthe initialization point for all MSMARCO exper-\niments. For the training of sentence bottleneck\nauto-encoder, we use RoBERTaBASE provided on\nHuggingFace7 as the sentence encoder. At infer-\nence time, we use top-5 retrieved passages to sup-\nport generation in both datasets. Other Detailed\nsettings are shown in Appendix A.\n6https://huggingface.co/facebook/\nrag-token-base\n7https://huggingface.co/facebook/roberta-base\n5.5 Comparison Models\nWe compare our model with RAG (Lewis et al.,\n2020b), which also serves as the encoder-decoder\nbackbone of the proposed methods. For fair com-\nparisons, we train our own RAG model with the\nsame configuration as our proposed methods. Addi-\ntionally, to evaluate how baseline RAG and our pro-\nposed model benefit the retriever training, we also\ncompare with the non-finetuned DPR (Karpukhin\net al., 2020) model on the retrieval evaluation.\nWe choose not to compare our model with other\nstate-of-the-art works on the KILT leaderboard\nsince the retriever settings are very different (both\nthe retriever architecture and size) and the com-\nparison is helpless for showing the benefits of our\nwork.\n6 Results\nIn this section, we show our results with respect to\nRQ1 and RQ2.\n6.1 RQ1: Effectiveness\nGenerative QA Table 2 shows the effectiveness\nevaluation results on MSMARCO development\n1019\nModel Retrieval End-to-end Combined\nR-Prec Recall@5 RL F1 KILT-RL KILT-F1\nRAG 50.99 65.98 15.61 17.86 8.98 10.46\nours 55.54 68.63 16.36 18.57 10.17 11.84\nTable 4: Test results on KILT leaderboard. We reported our best model under the development set, which is DKS.\nRL and KILT-RL refer to ROUGE-L and KILT ROUGE-L. R-Prec refers to R-Precision, which is identical to\nPrecision@1 in the KILT-WoW dataset.Bold highlighted values are the best.\nModel MSMARCO KILT-WoW\nTrain Dev ∆ ↓ Train Dev ∆ ↓\nRAG (ours) 17.5 10.7 6.8 46.73 42.21 4.52\nours (RAC) 17.8 11.2 6.6 45.32 44.95 0.37\nours (DKS) 16.3 11.5 4.8 47.94 47.02 0.92\nours (multi-task) 16.6 12.0 4.6 46.47 43.94 2.53\nTable 5: Robustness evaluation results for the proposed methods. For MSMARCO dataset we report Recall@1 and\nfor KILT-WoW dataset we report R-Prec.∆ = Train - Dev indicates the overfitting level of the given model. Bold\nhighlighted values are the best among all.\nset. First of all, we find that the proposed meth-\nods outperform the baseline in the retrieval step.\nBoth proposed methods achieve consistent im-\nprovement compared to the baseline RAG and the\nnon-finetuned DPR. The multi-task setting model\nworks best, which achieves about 14.3% relative\nimprovement (1.5% absolute improvement) to the\nnon-finetuned DPR on Recall@1, showing a large\nbenefit to the training of the retriever. In contrast,\nthe baseline RAG only gets 1.9% relative improve-\nment (0.2% absolute improvement). For the end-\nto-end generation results, we also see a consistent\nimprovement, especially in BLEU scores. In the\nmulti-task setting, our method got a 4.5% relative\nimprovement on the BLEU-4 score. This indicates\nthat better retrieval quality benefits the end-to-end\ngeneration results.\nInformative Conversations Table 3 shows the\neffectiveness evaluation results of proposed meth-\nods on KILT-WoW development set. Similarly to\nthe case of the MSMARCO experiments, for all\nversions of our model, there is a consistent improve-\nment in the retrieval metrics. As a benefit of better\nretrieval quality, we also get consistent improve-\nment in the end-to-end and combined evaluations.\nSpecifically, the DKS version achieves the best R-\nPrecision across all models, with 11.4% relative im-\nprovement compared to the baseline, showing that\nthe continuous sequence-level features are more\nsuitable for such casual conversational task where\nthe answer label can have less overlap with gold\npassages.\nWe submit our best model and the fine-tuned\nRAG baseline to the KILT-WoW leaderboard.8 Ta-\nble 4 shows the evaluation results. We do not in-\nclude the official results of the RAG baseline re-\nported by KILT, since the dataset splits are updated\ndue to a mapping issue (Paranjape et al., 2022).\nThe proposed methods get improvements across all\nmetrics compared to the baseline RAG, with 13.3%\nrelative improvement on KILT-RL score and 13.2%\nrelative improvement on KILT-F1 score, proving\nthat the proposed methods benefit on both the re-\ntrieval part and generation part.\n6.2 RQ2: Robustness\nIn this section, following Roelofs et al. (2019), we\nuse the differences between train set metrics and de-\nvelopment set metrics to measure the overfit degree\nto evaluate RQ2 that is relevant to robustness. Ta-\nble 5 shows the robustness evaluation results of the\nproposed methods in both datasets. We measure\nthe retrieval performance drop from the training\nset to the development set as the overfit degree ∆.\nIn the result, we can find that the baseline RAG\nmodel generalizes badly, with a significant per-\n8https://eval.ai/web/challenges/\nchallenge-page/689/leaderboard/1909/KILT-F1\n1020\nContext A: Do you like Gouda cheese, its a mild, yellow cheese.\nB: Well I like most cheeses so I’d probably like it, yes. Do you know where it originated from?\nLabel Answer It is one of the oldest recorded cheeses still made today, from 1184, people were mentioning\nGouda Cheese\nRAG Gouda cheese is a type of cheese made from gouda, which is a type of goat’s milk.\nours (RAC) The first mention of Gouda cheese dates from 1184, making it one of the oldest recorded cheeses\nin the world still made today.\nours (DKS) The first mention of Gouda cheese dates from 1184, making it one of the oldest recorded cheeses\nin the world still made today.\nours (multi-task) Gouda cheese is a type of cheese made from gouda, which is a type of cow’s milk.\nTable 6: Sampled Generations from KILT-WoW development set.\nformance drop in both datasets. In contrast, the\nproposed methods have far less overfit degree, es-\npecially in the KILT-WoW dataset. We also note\nthat some of the proposed methods have better train\nset results than the RAG model, indicating that the\nproposed methods can also help to better fit the\ntraining data while achieving better generalization\nto unseen data.\n6.3 Case Studies\nTable 6 shows some sampled generations from the\nKILT-WoW development set. In the example, both\nRAC and DKS retrieve the same Wikipedia page\nand give responses with a perfect knowledge match.\nThough the baseline and the multi-task model both\nfailed in retrieving the gold passage, our model is\nproviding an acceptable answer encompassing the\ncorrect knowledge, while the baseline generates\nhallucination (‘goat’s milk’).\n7 Conclusion\nIn this work, we introduced a new training frame-\nwork for retrieval augmented generation mod-\nels, for training a more robust retriever. The\nnew training framework includes two knowledge-\nconstrained objectives, RAC and DKS, to help\nguide the retriever to retrieve knowledge-relevant\npassages instead of only label-relevant passages.\nWe evaluate the proposed methods on two well-\nknown KIG datasets, MSMARCO and KILT Wiz-\nard of Wikipedia. The results show a consistent\nimprovement of our proposed methods on both re-\ntrieval metrics and generation metrics compared\nto the baseline model. On MSMARCO, our best\nmodel achieves 12.1% relative improvement on Re-\ncall@1. In the KILT-WoW leaderboard, our best\nmodel achieves 8.9% relative improvement on R-\nPrecision, 13.3% relative improvement on KILT-\nRL, and 13.2% relative improvement on KILT-F1\ncompared to the baseline model. Furthermore, we\nshow that the focusing on label-relevant passages\nfor the KIG task can result in overfitting, which we\nmanage to mitigate considerably using our training\nframework. The reliable improvement in the re-\ntrieval quality indicates that our work could further\ncontribute to the research community in the era of\nLLM, where the proposed methods could be used\nfor training retrievers to reduce hallucinations from\nLLMs.\nWe believe retrieval augmented methods are cru-\ncial for LLMs. As future work, one idea is to com-\nbine passages and structured knowledge, such as\ndatabases (V ougiouklis et al., 2023) and/or knowl-\nedge graphs, possibly including uncertain knowl-\nedge graphs (Pan et al., 2005; Stoilos et al., 2006;\nQi et al., 2007; Chen et al., 2019), knowledge graph\nwith selected vocabulary (Wang et al., 2014) and\ntemporal knowledge graphs (García-Durán et al.,\n2018; Bourgaux et al., 2021), for retrieval aug-\nmented generations. Furthermore, as suggested\nin (Pan et al., 2023a), there are a few pressing\nchallenges in this space, such as unifying knowl-\nedge editing (Mitchell et al., 2022; Han et al.,\n2023) and retrieval augmentation, complex reason-\ning via retrieval augmentation and semi-parametric\nLLMs (Pan et al., 2023b) in general.\nAcknowledgement\nThis work is supported by Huawei’s Dean’s Fund-\ning (C-00006589) and the Chang Jiang Scholars\nProgram (J2019032).\nLimitations\nThe first limitation is that the parameters of the\npassage encoder is fixed during the training of the\nretriever for a fair comparison with the baseline\nmodel. This potentially limits the expressiveness\nof the bi-encoder retriever and harms the overall\n1021\nperformance of the retrieve-generate model. How-\never, unfixing the passage encoder will significantly\nincrease the training cost, which is not feasible for\nus.\nAnother potential limitation is that we cannot\naccess the MSMARCO test set since the challenge\nis retired, which limits the training samples for the\nexperiments.\nReferences\nDaniel Adiwardana, Minh-Thang Luong, David R. So,\nJamie Hall, Romal Thoppilan, Zi Yang, Apoorv Kul-\nshreshtha, Gaurav Nemade, Yifeng Lu, and Quoc V .\nLe. 2020. Towards a human-like open-domain chat-\nbot. CoRR, abs/2001.09977.\nJinheon Baek, Alham Fikri Aji, and Amir Saffari. 2023.\nKnowledge-augmented language model prompting\nfor zero-shot knowledge graph question answering.\nIn Proceedings of the 1st Workshop on Natural\nLanguage Reasoning and Structured Explanations\n(NLRSE), pages 78–106, Toronto, Canada. Associa-\ntion for Computational Linguistics.\nSatanjeev Banerjee and Alon Lavie. 2005. METEOR:\nAn automatic metric for MT evaluation with im-\nproved correlation with human judgments. In Pro-\nceedings of the ACL Workshop on Intrinsic and Ex-\ntrinsic Evaluation Measures for Machine Transla-\ntion and/or Summarization, pages 65–72, Ann Arbor,\nMichigan. Association for Computational Linguis-\ntics.\nCamille Bourgaux, Ana Ozaki, and Jeff Z. Pan. 2021.\nGeometric Models for (Temporally) Attributed De-\nscription Logics. In Proc. of DL2021.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nXuelu Chen, Muhao Chen, Weijia Shi, Yizhou Sun, and\nCarlo Zaniolo. 2019. Embedding uncertain knowl-\nedge graphs. In AAAI, pages 3363–3370. AAAI\nPress.\nMd. Faisal Mahbub Chowdhury, Michael R. Glass,\nGaetano Rossiello, Alfio Gliozzo, and Nandana Mi-\nhindukulasooriya. 2022. KGI: an integrated frame-\nwork for knowledge intensive language tasks. CoRR,\nabs/2204.03985.\nAsa Cooper Stickland, Xian Li, and Marjan Ghazvinine-\njad. 2021. Recipes for adapting pre-trained monolin-\ngual and multilingual models to machine translation.\nIn Proceedings of the 16th Conference of the Euro-\npean Chapter of the Association for Computational\nLinguistics: Main Volume, pages 3440–3453, Online.\nAssociation for Computational Linguistics.\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\nFan, Michael Auli, and Jason Weston. 2019. Wizard\nof wikipedia: Knowledge-powered conversational\nagents. In 7th International Conference on Learning\nRepresentations, ICLR 2019, New Orleans, LA, USA,\nMay 6-9, 2019. OpenReview.net.\nHady Elsahar, Pavlos V ougiouklis, Arslen Remaci,\nChristophe Gravier, Jonathon Hare, Frederique Lafor-\nest, and Elena Simperl. 2018. T-REx: A large scale\nalignment of natural language with knowledge base\ntriples. In Proceedings of the Eleventh International\nConference on Language Resources and Evaluation\n(LREC 2018), Miyazaki, Japan. European Language\nResources Association (ELRA).\nWilliam Falcon and The PyTorch Lightning team. 2019.\nPytorch lightning.\nJames Ferguson, Matt Gardner, Hannaneh Hajishirzi,\nTushar Khot, and Pradeep Dasigi. 2020. IIRC: A\ndataset of incomplete information reading compre-\nhension questions. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1137–1147, Online. As-\nsociation for Computational Linguistics.\nAlberto García-Durán, Sebastijan Dumancic, and Math-\nias Niepert. 2018. Learning sequence encoders for\ntemporal knowledge graph completion. In EMNLP,\npages 4816–4821. Association for Computational\nLinguistics.\nMichael Glass, Gaetano Rossiello, Md Faisal Mahbub\nChowdhury, Ankita Naik, Pengshan Cai, and Alfio\nGliozzo. 2022. Re2G: Retrieve, rerank, generate.\nIn Proceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 2701–2715, Seattle, United States. Association\nfor Computational Linguistics.\nXiaoqi Han, Ru Li, Xiaoli Li, and Jeff Z. Pan. 2023. A\ndivide and conquer framework for knowledge editing.\nKnowledge-Based Systems, 279:110826.\nHangfeng He, Hongming Zhang, and Dan Roth. 2023.\nRethinking with retrieval: Faithful large language\nmodel inference. CoRR, abs/2301.00303.\nGautier Izacard and Edouard Grave. 2021. Leveraging\npassage retrieval with generative models for open do-\nmain question answering. In Proceedings of the 16th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics: Main Volume,\npages 874–880, Online. Association for Computa-\ntional Linguistics.\n1022\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6769–6781,\nOnline. Association for Computational Linguistics.\nOmar Khattab and Matei Zaharia. 2020. Colbert: Ef-\nficient and effective passage search via contextual-\nized late interaction over BERT. In Proceedings of\nthe 43rd International ACM SIGIR conference on\nresearch and development in Information Retrieval,\nSIGIR 2020, Virtual Event, China, July 25-30, 2020,\npages 39–48. ACM.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, Kristina Toutanova, Llion Jones, Matthew\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\nral questions: A benchmark for question answering\nresearch. Transactions of the Association for Compu-\ntational Linguistics, 7:452–466.\nOmer Levy, Minjoon Seo, Eunsol Choi, and Luke\nZettlemoyer. 2017. Zero-shot relation extraction via\nreading comprehension. In Proceedings of the 21st\nConference on Computational Natural Language\nLearning (CoNLL 2017), pages 333–342, Vancouver,\nCanada. Association for Computational Linguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020a.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, Sebastian Riedel, and Douwe Kiela. 2020b.\nRetrieval-augmented generation for knowledge-\nintensive nlp tasks. In Advances in Neural Infor-\nmation Processing Systems, volume 33, pages 9459–\n9474. Curran Associates, Inc.\nYury A. Malkov and Dmitry A. Yashunin. 2020. Effi-\ncient and robust approximate nearest neighbor search\nusing hierarchical navigable small world graphs.\nIEEE Trans. Pattern Anal. Mach. Intell., 42(4):824–\n836.\nEric Mitchell, Charles Lin, Antoine Bosselut, Christo-\npher D. Manning, and Chelsea Finn. 2022. Memory-\nbased model editing at scale. In International Con-\nference on Machine Learning, ICML 2022, 17-23\nJuly 2022, Baltimore, Maryland, USA, volume 162 of\nProceedings of Machine Learning Research, pages\n15817–15831. PMLR.\nIvan Montero, Nikolaos Pappas, and Noah A. Smith.\n2021. Sentence bottleneck autoencoders from trans-\nformer language models. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1822–1831, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nPhilipp Moritz, Robert Nishihara, Stephanie Wang,\nAlexey Tumanov, Richard Liaw, Eric Liang, Melih\nElibol, Zongheng Yang, William Paul, Michael I. Jor-\ndan, and Ion Stoica. 2018. Ray: A distributed frame-\nwork for emerging AI applications. In 13th USENIX\nSymposium on Operating Systems Design and Imple-\nmentation, OSDI 2018, Carlsbad, CA, USA, October\n8-10, 2018, pages 561–577. USENIX Association.\nTri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,\nSaurabh Tiwary, Rangan Majumder, and Li Deng.\n2016. MS MARCO: A human generated machine\nreading comprehension dataset. In Proceedings of\nthe Workshop on Cognitive Computation: Integrat-\ning neural and symbolic approaches 2016 co-located\nwith the 30th Annual Conference on Neural Infor-\nmation Processing Systems (NIPS 2016), Barcelona,\nSpain, December 9, 2016, volume 1773 of CEUR\nWorkshop Proceedings. CEUR-WS.org.\nAnsong Ni, Matt Gardner, and Pradeep Dasigi. 2021.\nMitigating false-negative contexts in multi-document\nquestion answering with retrieval marginalization.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n6149–6161, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nOpenAI. 2023. GPT-4 technical report. CoRR,\nabs/2303.08774.\nJeff Z. Pan, Simon Razniewski, Jan-Christoph Kalo,\nSneha Singhania, Jiaoyan Chen, Stefan Dietze, Hajira\nJabeen, Janna Omeliyanenko, Wen Zhang, Matteo\nLissandrini, ussa Biswas, Gerard de Melo, Angela\nBonifati, Edlira Vakaj, Mauro Dragoni, and amien\nGraux. 2023a. Large language models and knowl-\nedge graphs: Opportunities and challenges. Transac-\ntions on Graph Data and Knowledge.\nJeff Z. Pan, Giorgos Stamou, Vassilis Tzouvaras, and\nIan Horrocks. 2005. f-SWRL: A Fuzzy Extension of\nSWRL. In Proc. of ICANN 2005, Special section on\n\"Intelligent multimedia and semantics\".\nJeff Z. Pan, Guido Vetere, José Manuél Gómez-Pérez,\nand Honghan Wu, editors. 2017. Exploiting Linked\nData and Knowledge Graphs in Large Organisations.\nSpringer.\nXiaoman Pan, Wenlin Yao, Hongming Zhang, Dian Yu,\nDong Yu, and Jianshu Chen. 2023b. Knowledge-\nin-context: Towards knowledgeable semi-parametric\nlanguage models. In Proc. of ICLR2023.\n1023\nAshwin Paranjape, Omar Khattab, Christopher Potts,\nMatei Zaharia, and Christopher D Manning. 2022.\nHindsight: Posterior-guided training of retrievers for\nimproved open-ended generation. In International\nConference on Learning Representations.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James Thorne,\nYacine Jernite, Vladimir Karpukhin, Jean Maillard,\nVassilis Plachouras, Tim Rocktäschel, and Sebastian\nRiedel. 2021. KILT: a benchmark for knowledge\nintensive language tasks. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2523–2544, Online.\nAssociation for Computational Linguistics.\nGuilin Qi, Jeff Z. Pan, and Qiu Ji. 2007. A Possibilis-\ntic Extension of Description Logics. In Proc. of\nDL2007.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nRebecca Roelofs, Vaishaal Shankar, Benjamin Recht,\nSara Fridovich-Keil, Moritz Hardt, John Miller, and\nLudwig Schmidt. 2019. A meta-analysis of over-\nfitting in machine learning. In Advances in Neural\nInformation Processing Systems 32: Annual Confer-\nence on Neural Information Processing Systems 2019,\nNeurIPS 2019, December 8-14, 2019, Vancouver, BC,\nCanada, pages 9175–9185.\nStephen Roller, Emily Dinan, Naman Goyal, Da Ju,\nMary Williamson, Yinhan Liu, Jing Xu, Myle Ott,\nEric Michael Smith, Y-Lan Boureau, and Jason We-\nston. 2021. Recipes for building an open-domain\nchatbot. In Proceedings of the 16th Conference of\nthe European Chapter of the Association for Compu-\ntational Linguistics: Main Volume, pages 300–325,\nOnline. Association for Computational Linguistics.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation\nreduces hallucination in conversation. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2021, pages 3784–3803, Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nGiorgos Stoilos, Giorgos B. Stamou, and Jeff Z. Pan.\n2006. Handling imprecise knowledge with fuzzy\ndescription logic. In Proceedings of the 2006 Inter-\nnational Workshop on Description Logics (DL2006),\nWindermere, Lake District, UK, May 30 - June 1,\n2006, volume 189 of CEUR Workshop Proceedings.\nCEUR-WS.org.\nJames Thorne, Andreas Vlachos, Christos\nChristodoulopoulos, and Arpit Mittal. 2018.\nFEVER: a large-scale dataset for fact extraction\nand VERification. In Proceedings of the 2018\nConference of the North American Chapter of\nthe Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long\nPapers), pages 809–819, New Orleans, Louisiana.\nAssociation for Computational Linguistics.\nPavlos V ougiouklis, Nikos Papasarantopoulos, Danna\nZheng, David Tuckey, Chenxin Diao, Zhili Shen, and\nJeff Z. Pan. 2023. FastRAT: Fast and Efficient Cross-\nlingual Text-to-SQL Semantic Parsing. In Proc. of\nIJCNLP-AACL 2023.\nKewen Wang, Zhe Wang, Rodney W. Topor, Jeff Z. Pan,\nand Grigoris Antoniou. 2014. Eliminating Concepts\nand Roles from Ontologies in Expressive Descriptive\nLogics. pages 205–232.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V . Le,\nand Denny Zhou. 2022. Chain-of-thought prompt-\ning elicits reasoning in large language models. In\nNeurIPS.\nDonghan Yu, Chenguang Zhu, Yuwei Fang, Wenhao\nYu, Shuohang Wang, Yichong Xu, Xiang Ren, Yim-\ning Yang, and Michael Zeng. 2022. KG-FiD: Infus-\ning knowledge graph in fusion-in-decoder for open-\ndomain question answering. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n4961–4974, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nMuru Zhang, Ofir Press, William Merrill, Alisa Liu,\nand Noah A. Smith. 2023. How language model\nhallucinations can snowball. CoRR, abs/2305.13534.\nRuochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei\nQin, and Lidong Bing. 2023. Verify-and-edit: A\nknowledge-enhanced chain-of-thought framework.\nIn Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 5823–5840, Toronto, Canada.\nAssociation for Computational Linguistics.\nA Detailed Experiment Settings\nTraining Cost Our experiments are built on top\nof the RAG research project provided on Hug-\ngingFace9. We use pytorch-lightning (Falcon and\nteam, 2019) for fine-tuning the model, and use ray\n(Moritz et al., 2018) and FAISS (Johnson et al.,\n2019) for distributed retrieval with HNSW index\n(Malkov and Yashunin, 2020). All the models are\ntrained under FP16 for memory and time efficiency.\nWe use 4 NVIDIA A100 80G GPUs for fine-tuning\nthe retrieve-generate models and 1 NVIDIA A100\n9https://github.com/huggingface/transformers/\ntree/main/examples/research_projects/rag\n1024\n80G GPU for inference. Specially, since the ex-\nperiment requires fast retrieval using FAISS, it re-\nquires approximately 200GB CPU memory for run-\nning MSMARCO experiments, and 300GB CPU\nmemory for running KILT-WoW experiments (both\ntraining and inference). The memory requirement\ncan be reduced by compressing the HNSW index.\nTable 10 shows the average runtime for the experi-\nments. Since the fine-tuning runtime varies during\ntraining as the retriever training on CPU gets faster\nand faster, we only report the runtime of the first\nepoch. Note that the training runtime is not very\ncomparable, as it depends largely on the FAISS\nretrieval. Table 7 shows the trainable parameters of\nour models.\nModel Trainable Params\nRAG 515M\nours (RAC) 515M\nours (DKS) 518M\nours (multi-task) 518M\nTable 7: Trainable parameters of models.\nDataset Details Table 8 shows the statistics of\ndatasets used in our experiments. Both the datasets\nand knowledge sources are in English. For the MS-\nMARCO dataset, we split the original training set\ninto our training set and validation set, and report\nall the experimental results on the original devel-\nopment set. This setting is for better comparison\nto the results reported in Lewis et al. (2020b). For\nboth knowledge sources, we use the script provided\nby HuggingFace10 to build FAISS format corpus.\nSpecially, for KILT-WoW knowledge source, we\ndo an extra data cleaning process, including 1) re-\nmoving paragraphs starting withSection:::: and\nBULLET::::; 2) removing all non-unicode charac-\nters.\nDataset Train Dev Test\nMSMARCO 143,725 10,000 12,467\nKILT-WoW 63,734 3,054 2,944\nTable 8: Dataset Statistics. The original development\nset of MSMARCO dataset is used as the test set in our\nexperiments, which is still referred to as the develop-\nment set in the result section.\n10https://github.com/huggingface/transformers/\nblob/main/examples/research_projects/rag/use_\nown_knowledge_dataset.py\nHyperparameters Table 9 shows the hyperpa-\nrameter settings for training and evaluation of the\nproposed model. If some hyperparameters are not\nmentioned, then keep them as default in Hugging-\nFace. For Kilt-WoW experiments, we select the\nbest hyperparameter (batch size and passage nums\nfor training) based on the ROUGE-L score. No-\nticed that it is not possible for our experimental\nsetup to choose batch size of 16 and passage nums\nof 10, as it explodes the GPU memory. For KILT-\nWoW experiments, the best combination of (batch\nsize, passage nums) for our DKS model is (8, 10),\nwhile for others is (16, 5). For receiving the refer-\nence knowledge representations, we train sentence\nbottleneck auto-encoder (Montero et al., 2021) on\neach dataset for 100,000 iterations with batch size\n64 and learning rate 3e-5.\nHyperparam MSMARCO WoW\nRandom Seed 42 42\nLearning Rate 1e-5 1e-5\nBatch Size 8 {8, 16}\nLargest Epochs 20 20\nEarly Stop Patience 3 3\nOptimizer Adamax Adamax\nGradient Clip 0.1 0.1\nPassage Nums 10 {5, 10}\nDropout 0.1 0.1\nBeam Size 4 4\nTable 9: Hyperparameter settings for experiments on\ntwo different datasets in this work. All models share the\nsame hyperparameter settings.\nB Sentence Bottleneck AutoEncoder\nThere could be different ways to obtain label\nknowledge representations. In our preliminary\nexperiments, we tried to use [CLS] token repre-\nsentation of pre-trained BERT model or simply\nuse average or maximum pooling to obtain label\nknowledge representations. But all these methods\ndon’t work well, as they can not provide enough\nknowledge about the answer without further fine-\ntuning. Besides, the sentence bottleneck auto-\nencoder fits our needs well and also works much\nbetter. We initialise the encoder of the sentence bot-\ntleneck auto-encoder with bert-base-uncased\ncheckpoint from huggingface, and then finetune\nit with the MSMARCO and KILT-WoW datasets\nbefore training the retrieve-generate model.\nC More Generation Samples\nTable 11 provides more generation samples from\nKILT-WoW experiments.\n1025\nModel MSMARCO KILT-WoW\nFine-tuning Inference Fine-tuning Inference\nRAG 6,918s 0.28s 3,228s 0.37s\nours (RAC) 8,202s 0.28s 6,702s 0.42s\nours (DKS) 9,980s 0.32s 5,134s 0.40s\nours (multi-task) 10,551s 0.32s 5,028s 0.41s\nTable 10: Average runtime of experiments. Fine-tuning numbers are average seconds per training first epoch,\ninference numbers are average seconds per generation sample. For MSMARCO, we report fine-tuning runtime with\nbatch size=16. For KILT-WoW, we report fine-tuning runtime with batch size=8.\nContext\nA: I have observed that after black hair Brown hair is the second most common human hair color.\nB: Im not actually sure about my own i think its black but might just be dark brown.\nA: Really, mine is black, but People with brown hair are often referred to as brunette.\nB: Yea i think mine might be black but who knows.\nReference response Maybe, i have observed that brown hair is common in the western world\nRAG Yea brown hair is the second most common hair color after black hair.\nours (RAC) Brown hair is the second most common human hair color after black hair.\nours (DKS) Brown hair is common among populations in the Western world, especially among those from\nCentral Europe, Southeastern Europe, Eastern Europe, Southern Europe, Southern Cone, the\nUnited States, and also some Asians.\nours (multi-task) It is characterized by higher levels of the dark pigment eumelanin and lower levels of the pale\npigment pheomelanin.\nContext\nA: I really like cooking pasta, it is some of my favorite Italian food.\nB: I do too. What type of pastas do you like to make?\nA: Angel hair is always a great one to use in standard Italian cuisine.\nB: Yes! I prefer angel hair in spaghetti instead of the traditional spaghetti noodles. Do you make\ndifferent pasta dishes?\nReference response One of my favorite pasta dishes is just standard meat sauce and angel hair pasta.\nRAG I make a lot of different pasta dishes. I like to use rice flour instead of wheat flour.\nours (RAC) I like to make a lot of different pasta dishes. I like to use a lot of different types of pasta.\nours (DKS) I do, I like to use rice flour instead of wheat flour to get a different taste and texture.\nours (multi-task) I like to make a lot of different pasta dishes. I like to use eggs instead of water.\nContext\nA: Hey there!! How are ya?\nB: Hey there! I’m very good! What do you know about the manta ray?\nA: Not much really but it was the only thing to choose from.\nB: Well it seems there are two different main types. One of them is very large and the other\ndecently small.\nA: Okay so you know some about them. Great!! Have you ever seen one in the ocean before?\nB: I have seen the larger ones in the ocean and the smaller ones in aquariums. These things are\nsuper protected by government agencies in the wild and domestic.\nA: I didn’t know that. Are the endangered then?\nReference response Not quite, but they will be very soon. They have a very slow reproductive rate so it takes awile\nfor new ones to be had.\nRAG I’m not sure but I do know that the manta ray is one of the largest ray-finned fish.\nours (RAC) I’m not sure but I know that they are listed as endangered by the International Union for\nConservation of Nature (IUCN).\nours (DKS) Well they are protected in international waters by the Convention on Migratory Species of Wild\nAnimals, but they are more vulnerable closer to shore.\nours (multi-task) I’m not sure but I know that they are filter feeders and eat large amounts of zooplankton.\nTable 11: More sampled Generations from KILT-WoW development set.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6434003710746765
    },
    {
      "name": "Computational linguistics",
      "score": 0.5923994779586792
    },
    {
      "name": "Encoding (memory)",
      "score": 0.5270076990127563
    },
    {
      "name": "Joint (building)",
      "score": 0.5115922093391418
    },
    {
      "name": "Natural language processing",
      "score": 0.3923757076263428
    },
    {
      "name": "Linguistics",
      "score": 0.3419429361820221
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3220258951187134
    },
    {
      "name": "Engineering",
      "score": 0.14290288090705872
    },
    {
      "name": "Philosophy",
      "score": 0.09306496381759644
    },
    {
      "name": "Architectural engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I98677209",
      "name": "University of Edinburgh",
      "country": "GB"
    }
  ]
}