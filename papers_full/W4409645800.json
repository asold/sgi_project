{
  "title": "Achieving GPT-4o level performance in astronomy with a specialized 8B-parameter large language model",
  "url": "https://openalex.org/W4409645800",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2560259193",
      "name": "Tijmen de Haan",
      "affiliations": [
        "High Energy Accelerator Research Organization"
      ]
    },
    {
      "id": "https://openalex.org/A4213578152",
      "name": "Yuan-Sen Ting",
      "affiliations": [
        "The Ohio State University"
      ]
    },
    {
      "id": "https://openalex.org/A2786699926",
      "name": "Tirthankar Ghosal",
      "affiliations": [
        "Oak Ridge National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2096493883",
      "name": "Tuan Dung Nguyen",
      "affiliations": [
        "University of Pennsylvania"
      ]
    },
    {
      "id": "https://openalex.org/A2692098784",
      "name": "Alberto Accomazzi",
      "affiliations": [
        "Center for Astrophysics Harvard & Smithsonian"
      ]
    },
    {
      "id": "https://openalex.org/A2245063163",
      "name": "Azton Wells",
      "affiliations": [
        "Argonne National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A3021153672",
      "name": "Nesar Ramachandra",
      "affiliations": [
        "Argonne National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A1999627724",
      "name": "Rui Pan",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2152730008",
      "name": "Ze-chang Sun",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2560259193",
      "name": "Tijmen de Haan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4213578152",
      "name": "Yuan-Sen Ting",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2786699926",
      "name": "Tirthankar Ghosal",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096493883",
      "name": "Tuan Dung Nguyen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2692098784",
      "name": "Alberto Accomazzi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2245063163",
      "name": "Azton Wells",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3021153672",
      "name": "Nesar Ramachandra",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1999627724",
      "name": "Rui Pan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2152730008",
      "name": "Ze-chang Sun",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4401042217",
    "https://openalex.org/W6602215214",
    "https://openalex.org/W3172642864",
    "https://openalex.org/W4392669797",
    "https://openalex.org/W4406157726",
    "https://openalex.org/W4406171908",
    "https://openalex.org/W4407069513",
    "https://openalex.org/W4401497045",
    "https://openalex.org/W4404783002",
    "https://openalex.org/W4404783473",
    "https://openalex.org/W4385571157",
    "https://openalex.org/W6600459194"
  ],
  "abstract": null,
  "full_text": "Achieving GPT-4o level \nperformance in astronomy with \na specialized 8B-parameter large \nlanguage model\nTijmen de Haan1, Yuan-Sen Ting2,3, Tirthankar Ghosal4, Tuan Dung Nguyen5, \nAlberto Accomazzi6, Azton Wells7, Nesar Ramachandra7, Rui Pan8 & Zechang Sun9\nAstroSage-Llama-3.1-8B is a domain-specialized natural-language AI assistant tailored for research \nin astronomy, astrophysics, cosmology, and astronomical instrumentation. Trained on the complete \ncollection of astronomy-related arXiv papers from 2007 to 2024 along with millions of synthetically-\ngenerated question-answer pairs and other astronomical literature, AstroSage-Llama-3.1-8B \ndemonstrates remarkable proficiency on a wide range of questions. AstroSage-Llama-3.1-8B scores \n80.9% on the AstroMLab-1 benchmark, greatly outperforming all models—proprietary and open-\nweight—in the 8-billion parameter class, and performing on par with GPT-4o. This achievement \ndemonstrates the potential of domain specialization in AI, suggesting that focused training can yield \ncapabilities exceeding those of much larger, general-purpose models. AstroSage-Llama-3.1-8B is freely \navailable, enabling widespread access to advanced AI capabilities for astronomical education and \nresearch.\nKeywords AI assistant, Large-language model, Continued pretraining, Supervised fine-tuning\nLarge-language model (LLM) assistants are rapidly gaining traction across all sectors of knowledge work \nworldwide. In astronomy, these models are used for providing factual information, as programming assistants, \nfor brainstorming ideas, and for providing explanations tailored to the level of understanding or preferred \nstyle of the user. LLMs exhibit a remarkable robustness, often delivering useful outputs even when the input is \nmalformed, lacks context, or contains inaccuracies.\nDespite their potential, the development of specialized LLMs has been limited due to their recent emergence \nand the substantial resources required for training. Previous studies 1–4 have shown that models narrowly \ntailored to a specific domain can perform on par with, or even exceed, much larger general-purpose models. \nThis suggests that a large, highly domain-specific model could achieve state-of-the-art performance.\nIn astronomy, however, high-performing specialized language models have not yet been achieved. While \nmodels like AstroLLaMA5,6 have gained attention, they lack comprehensive benchmarking of their astronomical \nknowledge recall capabilities. Recent studies7 have shown that many of these models, due to limited specialized \ntraining data and fine-tuning for instruction-following, suffer from either catastrophic forgetting or an inability \nto follow precise question-answering instructions, often performing worse than their baseline models (in this \ncase, the Llama models).\nBuilding on the previous efforts of cosmosage8 and AstroLLaMA, we have developed AstroSage-Llama-\n3.1-8B, a natural language assistant specialized in astronomy, astrophysics, cosmology, and astronomical \ninstrumentation. For the remainder of this paper, we will refer to these subdomains collectively as “astronomy” . \n1International Center for Quantum-field Measurement Systems for Studies of the Universe and Particles (QUP-\nWPI), High Energy Accelerator Research Organization (KEK), Tsukuba, Ibaraki, Japan. 2Department of Astronomy, \nThe Ohio State University, Columbus, OH, USA. 3Center for Cosmology and AstroParticle Physics (CCAPP), The Ohio \nState University, Columbus, OH, USA. 4National Center for Computational Sciences, Oak Ridge National Laboratory, \nOak Ridge, TN, USA. 5Department of Computer and Information Science, University of Pennsylvania, Philadelphia, \nPA, USA. 6Center for Astrophysics, Harvard & Smithsonian, Cambridge, MA, USA. 7Computational Science Division, \nArgonne National Laboratory, Lemont, IL, USA. 8Department of Computer Science and Engineering, Hong Kong \nUniversity of Science and Technology, Kowloon, Hong Kong. 9Department of Astronomy, Tsinghua University, \nBeijing, People’s Republic of China. email: tijmen.dehaan@gmail.com\nOPEN\nScientific Reports |        (2025) 15:13751 1| https://doi.org/10.1038/s41598-025-97131-y\nwww.nature.com/scientificreports\n\nThrough the use of a substantially more extensive and well-curated training dataset, we demonstrate for the first \ntime that our specialized language model significantly outperforms baseline models in downstream tasks.\nIn the long term, we envision an agentic research assistant capable of autonomously conducting literature \nreviews, identifying relevant hypotheses, carrying out data analysis, and even formulating new research \nquestions. The development of such scientific agents (LLMs capable of solving scientific problems end-to-end) \nis already a rapidly growing field in astronomy. Recent studies have shown promising results in automating \nresearch tasks, such as analyzing James Webb Space Telescope data through multi-agent collaboration and self-\nplay reinforcement learning 9. However, these studies have been largely constrained by the substantial costs \nassociated with utilizing proprietary models at high volume.\nRealizing this level of agency will require extensive experimentation and careful optimization. Given the \nsubstantial compute costs and data requirements inherent in large-scale model training, keeping the model \nsize manageable while maintaining high performance is crucial. Our approach, demonstrated through \nastronomy knowledge recall, shows that specialized models can achieve state-of-the-art performance in specific \ndomains. This not only makes the development of advanced research assistants more feasible but also ensures \ntheir accessibility to a wider range of institutions and researchers, potentially transforming the landscape of \nastronomical research and education.\nContinued pretraining\nFor AstroSage-Llama-3.1-8B, we selected Meta’s Llama-3.1 8-billion parameter model 10 as our foundation \nmodel. This base model was chosen for both its strong general-purpose capabilities and its availability under \nthe permissive Llama 3.1 Community License. Furthermore, among models in the 8-billion parameter class, \nit demonstrated superior performance in astronomical knowledge recall compared to both general-purpose \nmodels11 and specialized astronomical LLMs7, making it an ideal baseline.\nTo begin the development process, we first focused on curating, obtaining, and cleaning a continued \npretraining (CPT) dataset.\nDataset preparation\nThe scaling laws of2 show that model capability increases predictably with increased training data volume and \ncomputational resources. More recently, it has been found 12–15 that the power-law index of these scaling laws \ndepends on data quality. Therefore, our general approach to assembling a corpus was to focus on maintaining a \nhigh quality threshold, maximizing data volume at that quality level.\nWe employed a multi-faceted approach to create a comprehensive, high-quality, high-variety CPT corpus. \nThe primary components of our dataset include:\n• Approximately 250,000 arXiv preprints from 2007-2024 with primary or cross-listed categories in astro-ph \n(astrophysics) or gr-qc (general relativity and quantum cosmology). For preprints with multiple versions, we \nselected the most recent. We deliberately excluded the Annual Review of Astronomy and Astrophysics papers \nused in11, AstroMLab-1 to generate the benchmark questions, ensuring our evaluation would test the model’s \nability to generalize knowledge rather than recall specific source materials.\n• Relevant articles from a depth-2 search through Wikipedia’s astronomy and astrophysics categories. We in -\nclude articles that directly belong to these categories or their immediate subcategories, then further add linked \narticles. We perform keyword-based exclusion on a few tens of custom keywords such as “deity” , “cyberspace” , \n“mythological” , “sports” and so on, yielding a total of nearly 30,000 articles.\n• A selection of ∼800 textbooks that are available as PDFs or ebooks online.\nFor the vast majority of the dataset, the rendered PDF files were converted to Mathpix Markdown using Nougat \nOCR16. For the remaining sources, the data was either already in Markdown format, or was left as plain text.\nPretraining run\nThe pretraining was conducted on the ORNL OLCF Frontier 1.6 exaflop supercomputer, leveraging its \nsubstantial computational resources. We used 184 nodes simultaneously, each of which is equipped with four \nAMD MI250X, which in turn have 2 Graphics Compute Dies (GCDs) each for a total of 8 GCDs per node, or a \ntotal of 1472 GCDs.\nFurther statistics and our choices of pretraining hyperparameters are summarized in Table 1.\nThe Llama-3.1 tokenizer, which uses a variant of tiktoken17 with UTF-flexible encoding, was sufficient for our \npurposes, so we did not introduce any astronomy-specific tokens to the vocabulary. We considered incorporating \narXiv identifiers (in the format arXiv:YYMM.numbervV) for papers in the training set but ultimately decided \nagainst it due to the substantial increase in vocabulary size this would entail. Future work may explore expanding \nthe vocabulary to include numerical representations relevant to astronomy, such as common quantities and \nunits.\nDuring training, we tracked the loss function and step sizes. Minimal hyperparameter tuning was performed, \nrelying on the hyperparameters from 8 for all parameters other than the learning rate schedule. The warmup \nperiod of 40 samples was determined empirically from the loss curve of an earlier, canceled CPT run. The peak \nlearning rate was extrapolated from smaller runs, but remained problematic, as Frontier requires high levels of \nparallelization with short wall times. This caused our initial runs to suffer from either insufficient learning due to \na low peak learning rate, or catastrophic exploding gradients due to an excessively high peak learning rate. The \nfinal run described in this work used a tuned learning schedule with a peak learning rate as high as possible but \nstill allowing convergence. In future efforts, we plan to further optimize the efficiency of the training procedure \nalong the lines of the work presented in18, incorporating tensor, pipeline, and data paralellism through libraries \nScientific Reports |        (2025) 15:13751 2| https://doi.org/10.1038/s41598-025-97131-y\nwww.nature.com/scientificreports/\nsuch as Megatron-Deepspeed19,20. We also aim to request a longer walltime with lower parallelization factor in \norder to be able to reduce loss and improve downstream performance.\nDataset cleaning\nWe followed the cleaning procedures from8, including a perplexity-based cleaning approach. This method first \nsplits the corpus into individual paragraphs and calculates their respective perplexity scores. Perplexity measures \nhow well a language model can predict a given text sequence. Lower perplexity indicates text that follows \nexpected patterns of natural language, while very high perplexity values often signal anomalous or corrupted \ncontent. Outliers with such high perplexity frequently stem from OCR errors, malformed text, or non-prose \ncontent such as tables.\nBased on the distribution of perplexity scores (Fig.  1), as well as manual inspection of some examples, we \nestablished a threshold that excluded the top 2% of paragraphs with the highest perplexity scores. We then \nreconstructed each document using only the paragraphs below this threshold. This cleaning procedure removed \napproximately 2% of the total data volume. Figure  1 illustrates this process by showing the distribution of \nperplexity scores and our chosen threshold.\nSupervised fine-tuning\nTo improve the model’s ability to follow instructions and answer questions effectively, we performed supervised \nfine-tuning (SFT). In this process, the model was trained to predict appropriate responses to given prompts, \nlearning from a collection of high-quality example conversations. Below, we describe our approach to generating \nand curating the SFT dataset.\nSFT dataset\n7 identified a critical limitation in the AstroLLaMA series of specialized astronomical LLMs: their inability to \noutperform even their own starting base models, partly due to inadequate SFT. While these models showed \nmarginal improvements in basic next-token prediction tasks, they performed worse than their baseline models \non instructional question-and-answer (Q & A) tasks, even for straightforward astronomical knowledge recall. \nThis shortcoming fundamentally undermines the purpose of specialized training. Therefore, in our study, \nwe paid particular attention to the SFT process, generating training datasets orders of magnitude larger than \npreviously available in astronomy.\nThe largest component of our SFT dataset consists of Q & A pairs. Using the method from 8, we generated \nover 11 million synthetic Q & A pairs from papers in our CPT dataset. These Q & A pairs were then evaluated \nusing an LLM based on four criteria: \nHyperparameter Value\nSequence length 8192 tokens\nMicro batch size 3\nEpochs 2\nLearning rate 1.5e−4\nBase model Llama 3.1 8B\nOptimizer AdamW\nAdam beta2 0.95\nAdam epsilon 1e−5\nLearning rate schedule Constant with quadratic warmup\nMax gradient norm 3.0\nWeight decay 0.001\nWarmup steps 40\nPrecision BF16\nFSDP Full shard, auto wrap\nResource Value\nPlaintext filesize 19.9 GB\nToken count 3.3 billion\nNodes 184\nGCDs (effective # of \nGPUs) 1472\nTraining wall time 10 h\nTotal time spent 11.5 h\nEffective GPU-hours \nspent 14,872 h\nVRAM usage 96% of 64 GB/GCD\nTable 1. Summary of pretraining hyperparameters and resource usage.\n \nScientific Reports |        (2025) 15:13751 3| https://doi.org/10.1038/s41598-025-97131-y\nwww.nature.com/scientificreports/\n 1. Correctness: The factual accuracy of the answer in relation to the question, ensuring that each Q & A pair \nadheres to current scientific understanding and accurately reflects the information presented in the source \nmaterial.\n 2. Stand-alone: The ability of each Q & A pair to be understood in isolation, without needing additional context \nbeyond the content provided. This ensures that an astronomy expert could answer the question based solely \non the information within the pair.\n 3. Pertinence: The relevance and importance of the question to researchers or students in astronomy. We want \nthe questions to be ones that a professional in the field might ask or find valuable for deeper understanding.\n 4. Overall Quality: a holistic assessment of the Q & A pair. This criterion is intended to capture aspects that \nmay not be explicitly covered by correctness, stand-alone completeness, or pertinence but contribute to the \noverall usefulness of the Q & A pair.\nThe LLM was presented with several hand-written examples of ratings in the form Question–Answer–Score–\nExplanation, followed by the Question–Answer pair to be judged. The model was then asked to complete the \nscore values. A small number of resulting scores were verified and confirmed to be sufficiently accurate. Only Q \n& A pairs with perfect scores in all four categories were kept, resulting in 8.8 million high-quality Q & A pairs. \nBelow are a few examples.\n• Q: What are the three distinct phases of galaxy formation identified in the EAGLE simulation, and how are \nthey primarily determined? A: The three distinct phases of galaxy formation identified are: \n 1. Disorganized growth in low-mass galaxies (stellar mass less than approximately 109.5M⊙), characterized \nby random stellar motions and dominated by in-situ star formation and mergers;\n 2. Evolution towards a disc-dominated morphology in intermediate-mass galaxies (stellar mass between \n109.5M⊙ and 1010.5M⊙), with bulge formation associated with mergers;\n 3. High-mass galaxies (stellar mass greater than approximately 1010.5M⊙) experience a slowdown in in-situ \nstar formation, transitioning to spheroidal morphologies driven by the accretion of ex-situ stars. These \nphases are primarily determined by stellar mass rather than redshift.\n• Q: What are the expected contributions of AGN feedback to the heating and dynamics of the hot ISM? A: \nAGN feedback is anticipated to play a significant role in regulating the cooling of the hot ISM, with energy \nreleased during accretion often exceeding starburst-driven heating and driving turbulence and outflows that \ninfluence the dynamics of the ISM.\n• Q: What are the applications of the Augmented Lagrangian Perturbation Theory (ALPT) in cosmology? A: \nALPT can be used for setting initial conditions for N-body simulations, generating mock galaxy catalogs, \nanalyzing cosmic structures, and reconstructing primordial density fluctuations for baryon acoustic oscilla -\ntion studies.\n• Q: What are the three effects that anisotropies in the optical depth produce in the CMB? A: Anisotropies \nin the optical depth produce three effects in the CMB: (i) screening of the temperature and polarization \nfluctuations observed today by an overall factor of exp(−τ(ˆn)); (ii) Thomson scattering, producing new \nE-mode polarization from the local temperature quadrupole, with subdominant contributions to B-modes \nFig. 1. Histogram of paragraph-level perplexity used for dataset cleaning. The perplexity is calculated as \nexp(−⟨ln P⟩), where ⟨ln P⟩ is the average log probability per token in each paragraph. The red dashed line \nindicates the manually chosen threshold used to filter out high-perplexity paragraphs, which comprised \napproximately 2% of the total data volume. Paragraphs with perplexity above this threshold were removed \nfrom the dataset. Note that the highest observed perplexity values (around 1012) extend beyond the right edge \nof the plot.\n \nScientific Reports |        (2025) 15:13751 4| https://doi.org/10.1038/s41598-025-97131-y\nwww.nature.com/scientificreports/\nfrom reionization effects; and (iii) new temperature anisotropy generated from the radial motion of ionized \nbubbles relative to the observer.\nWe also included a filtered version of Infinity-Instruct-7M21, keeping only entries with at least 70% alphanumeric \ncharacters, as well as filtering out entries with certain keywords. The inclusion of this dataset was to ensure that \nAstroSage-Llama-3.1-8B would gain instruction-following abilities such as multi-turn conversation.\nAdditionally, we generated synthetic summaries for all of the papers in the CPT dataset. The SFT dataset for \nthese summaries consists of a user prompt asking to summarize a certain paper, with the assistant completion \nbeing a small preamble followed by the summary. The user prompt was created through a series of random \nchoices about the way the question is asked, a small preamble, and the way in which the paper is referenced, \nyielding a high variety of question styles.\nFurthermore, we generated a metadata-based dataset, where again a series of custom rules and random \nselections result in diverse questions about titles, dates of publication, arXiv IDs, and first author names from \nthe papers in the CPT dataset. This was included in an effort to memorize the paper metadata so that users can \nreference papers in their conversations with AstroSage-Llama-3.1-8B. However, we find that after training, the \nmodel is unable to reliably answer metadata-based questions such as producing a first author name from an \narXiv ID. This failure to memorize could be ameliorated in future training runs by increasing the learning rate \nor number of epochs.\nThese datasets were combined with five further datasets which were assembled by hand from various sources \non the web. The combined dataset comprised approximately 2 billion tokens.\nSFT procedure\nThe SFT process was also conducted on the Frontier supercomputer with a configuration summarized in Table 2.\nThroughout the fine-tuning process, we again monitored loss and step sizes, which are shown in Fig. 2 (CPT \ncurves look similar). The learning was—like in “ Pretraining run” section—limited by the maximum wall-time \nallowed by the Frontier HPC system, which for 184 nodes or more is a maximum of 12 hours. With the learning \nrate as high as it could comfortably be set to avoid exploding gradients, this limitation on walltime strongly \nlimited the number of steps that could be taken and therefore the final loss that could be obtained.\nModel merging\nModel merging, also known as parameter averaging, has emerged as a powerful technique for combining \ncapabilities of multiple expert models into a single language model 22,23. While our CPT+SFT procedure \nsignificantly improved the model’s astronomical knowledge recall in few-shot prompts, we observed that \nperformance in conversational Q & A scenarios such as multi-turn conversations and instructions regarding the \noutput style still fell slightly short of optimal. This challenge likely stems from the fact that the “instruct” version \nof Llama-3.1-8B provided by Meta underwent substantially more extensive supervised fine-tuning than what we \ncould achieve as an academic group. We found that merging our specialized model with Meta’s instruct model \nsignificantly improved these conversational capabilities.\nTo create the final version of AstroSage-Llama-3.1-8B, we employed mergekit24, using the DARE-TIES \nmethod to combine our SFT-trained model described in “Supervised fine-tuning” section with Meta-Llama-3.1-\n8B-Instruct10. The merge was performed at full density, BF16 precision, and with the weight parameters set to \n0.75 and 0.25 for AstroSage-Llama-3.1-8B-SFT and Meta-Llama-3.1-8B-Instruct, respectively.\nThe resulting merged model exhibits enhanced instruction-following capabilities and improved performance \non the AstroMLab-1 multiple-choice question benchmark in both few-shot and structured output scenarios. To \ndetermine whether these improvements stemmed from enhanced instruction-following rather than additional \nastronomical knowledge, we conducted a control experiment. We fine-tuned a separate version of the CPT+SFT \nmodel on unrelated multiple-choice questions using identical output formatting. This control model achieved \nnear-identical scores on the AstroMLab-1 benchmark in the structured output scenario without any merging, \nHyperparameter Value\nEpochs 6\nLearning rate 1e−4\nBase model CPT model\nLearning rate schedule Cosine with quadratic warmup\nWeight decay 0.0\nResource Value\nPlaintext filesize 9.8 GB\nToken count 2.0 billion\nTraining wall time 9.5 h\nTotal time spent 11.5 h\nEffective GPU-hours \nspent 13,738 h\nTable 2. Summary of supervised fine-tuning hyperparameters and resource usage. Parameters that are not \nstated here were kept the same as in Table 1.\n \nScientific Reports |        (2025) 15:13751 5| https://doi.org/10.1038/s41598-025-97131-y\nwww.nature.com/scientificreports/\nsuggesting that the process of merging in a small fraction of the Meta-Llama-3.1-8B-Instruct model weights \ntransferred general question-answering capabilities rather than domain-specific knowledge.\nTo recap, we began with Meta-Llama-3.1-8B as our base model, then performed CPT on a large corpus of \nastronomy literature to instill domain knowledge. This was followed by SFT using carefully curated instruction-\nresponse pairs to improve task performance and instruction following. Finally, we merged the resulting model \nwith Meta-Llama-3.1-8B-Instruct to enhance general instruction-following capabilities while preserving the \nastronomical expertise, resulting in our final model which we are releasing as AstroSage-Llama-3.1-8B.\nEvaluation\nWe have performed three types of quantitative evaluation. The first is an evaluation of AstroSage-Llama-3.1-8B \non a multiple-choice question benchmark, the second on several general-purpose benchmarks, and the third \ndirect preference testing against a similar model that was not specialized in astronomy.\nMultiple-choice question benchmark\nTo evaluate AstroSage-Llama-3.1-8B’s performance, we employed the multiple-choice question benchmark from \nthe first paper in this series 11, AstroMLab 1. This benchmark consists of diverse astronomy-related questions \ngenerated from selected Annual Review of Astronomy and Astrophysics (ARAA) papers and remains, to our \nknowledge, the only comprehensive astronomy-specific benchmarking effort available. We refer interested \nreaders to the original paper for detailed benchmark specifications.\nImportantly, we deliberately excluded the ARAA papers from AstroSage-Llama-3.1-8B’s training dataset. \nThis strategic exclusion enables us to evaluate the model’s broader understanding of astronomical concepts \nrather than its ability to recall specific information from the source materials. This approach helps ensure that \nthe benchmark scores reflect AstroSage-Llama-3.1-8B’s genuine comprehension of astronomy rather than mere \nmemorization of the content used to create the questions.\nOur choice to primarily evaluate AstroSage-Llama-3.1-8B with a knowledge-based benchmark was motivated \nby two key factors. First, this benchmark represents the only extensively tested and human-vetted dataset \navailable for astronomical knowledge assessment. Second, while astronomical knowledge recall represents just \none aspect of LLM capabilities, it serves as a critical foundation for more advanced applications such as scientific \nagents. The primary goal is to demonstrate that proper fine-tuning of a relatively small model can significantly \nimprove performance on a specific task—an achievement not previously demonstrated in astronomy.\nThe performance score is calculated as the fraction of correctly answered multiple-choice questions in the \nbenchmarking dataset. The resulting scores are shown in Fig.  3, where round symbols represent scores for \ncutting-edge proprietary and open-weight models. The open-weight models are also marked with an outer \ncircle. The x-axis displays the cost per 105 tokens, a metric chosen based on practical applications: in the first \n(and to our knowledge, only) implementation of astronomical agents 9, analyzing a celestial source’s spectral \nenergy distribution from James Webb Space Telescope data requires approximately 105 tokens. The top x-axis \nshows costs scaled to 3B ( 3 × 109) tokens, roughly equivalent to the entire astro-ph section of the arXiv. For \nproprietary models, we use current token costs (averaging input and output costs where they differ), while open-\nweight model costs are estimated based on typical pricing of commercial GPU platforms.\nSpecialized astronomical LLMs are denoted by star symbols, except for the first AstroLLaMA model5, whose \nscore falls below the plot’s lower limit. The bottom right panel shows the typical uncertainty (calculated using the \nWilson score interval), demonstrating that our dataset of 4425 multiple-choice questions provides sufficiently \nFig. 2. Supervised fine-tuning loss curve. The learning rate schedule, shown as the dashed brick red curve, \nfollows a quadratic warmup followed by a cosine schedule that ends at 10% of the peak learning rate. The peak \nlearning rate was chosen to prevent exploding gradients. The loss curve in black shows no significant signs of \noverfitting, as evidenced by minimal discontinuities at epoch boundaries. The green curve represents the L2 \nnorm of the parameter update, shown in arbitrary units.\n \nScientific Reports |        (2025) 15:13751 6| https://doi.org/10.1038/s41598-025-97131-y\nwww.nature.com/scientificreports/\nsmall sampling noise to establish robust performance differences. We have updated all scores using the latest \nmodel versions following the methodology from11, AstroMLab 1.\nThe diagonal dashed lines represent a universal cost-efficiency trade-off observed across major model \nseries (e.g. Llama, GPT, GLM) that simultaneously released models at multiple sizes. We consistently observe \na 3.5-point improvement in performance for every 10-fold increase in cost across model families. Each dashed \nline represents this equivalent trade-off, offset by 3.5 percentage points (equivalent to a 10-fold gain in cost-\neffectiveness). Despite similar performance on general benchmarks, cutting-edge models can differ by up to \n1000-fold in cost-effectiveness on astronomical tasks, highlighting the importance of specialized astronomical \nbenchmarks for evaluating performance on niche technical domains.\nTo establish a human performance baseline, two domain experts from our team independently completed \na random subset of benchmark questions under controlled conditions. The experts answered on the order of \none hundred questions each, taking around 30 seconds per question. No external references, web searches, \nor language model assistance were used. Both experts achieved remarkably consistent scores of approximately \n68%, which we designate as the “Indicative Human Domain Expert” score. The fact that most evaluated LLMs \nsignificantly surpassed this baseline demonstrates both the benchmark’s comprehensive scope and difficulty, \nwhile highlighting the remarkable capabilities of current LLMs in capturing and applying complex astronomical \nknowledge.\nAs previously noted in7, AstroMLab 2, existing specialized astronomical LLMs (shown as open stars in Fig. 3) \nfail to outperform baseline models of comparable parameter size. In many cases, suboptimal specialization \ntechniques actually led to performance degradation. In contrast, AstroSage-Llama-3.1-8B, despite its modest \nsize of 8 billion parameters, achieved an accuracy of 80.9% on this benchmark-comparable to OpenAI’s latest \nflagship models (GPT-4o: 80.4%) and the best 90B-parameter open-weight Meta-Llama models (80.6%). This \nperformance is particularly notable because AstroSage-Llama-3.1-8B achieves these results at approximately \none-thousandth the inference cost of proprietary models and one-hundredth the cost of open-weight models. \nFurthermore, it demonstrates an 8-point improvement over its baseline model, Meta-Llama-3.1-8B (72.9%). To \nour knowledge, this represents the first demonstration of a specialized astronomical LLM achieving objectively \nverified improvements through model fine-tuning.\nFig. 3. Performance comparison on the AstroMLab 1 benchmark, which contains 4425 high-quality, human-\nverified multiple-choice questions across astronomy, astrophysics, cosmology, and instrumentation. The \ngray shaded bar spans the performance of the two human domain experts. We present updated results as of \nNovember 2024, incorporating both cutting-edge proprietary and open-weight models. Open-weight models \nare circled. AstroSage-Llama-3.1-8B outperforms all other models in the 8-billion parameter class and achieves \nperformance comparable to OpenAI’s latest models, including GPT-4o, while Claude-3.5-Sonnet maintains \nthe highest performance overall. The diagonal dashed lines represent cost-efficiency trade-offs as determined \nin AstroMLab 1 (see text for details). The Wilson Score interval shows the typical uncertainty in the score \ndue to the finite number of questions. Star symbols indicate all published specialized LLMs for astronomy \nto our knowledge. Previously, these specialized models often failed to outperform their baseline models \nin astronomical recall due to various training limitations. AstroSage-Llama-3.1-8B represents a significant \nadvancement in specialized astronomical LLMs, demonstrating that extensive data curation, massive \ncontinued pre-training and supervised fine-tuning, and model merging techniques can substantially improve \nperformance on specific astronomical tasks. This result highlights the effectiveness of domain specialization \neven in relatively smaller models.\n \nScientific Reports |        (2025) 15:13751 7| https://doi.org/10.1038/s41598-025-97131-y\nwww.nature.com/scientificreports/\nGeneral-purpose benchmarks\nTo ensure our domain specialization did not compromise general capabilities, we evaluated AstroSage-Llama-3.1-\n8B across a comprehensive suite of standard language model benchmarks. These include IF-EV AL25 (instruction \nfollowing), BBH 26 (binary hypothesis testing), MATH 27 (mathematical reasoning), GPQA 28(graduate-level \nscience questions), MUSR29 (real-world decision-making scenarios), and MMLU-PRO30 (an expanded version \nof MMLU with more challenging reasoning questions). As shown in Fig.  4, our CPT+SFT model (green, \ninitialized from the Llama-3.1 base model) initially performed below the Llama-3.1 instruct model (purple) on \nfive out of the six non-astronomy benchmarks. This was expected, given that Meta’s proprietary SFT dataset for \ntheir instruct model likely far exceeds what is feasible for an academic research group to reproduce. The merging \nprocedure, pulling in only 25% of its weight from Meta-Llama-3.1-8B-Instruct, allowed us to recover much of \nthis performance deficit.\nCrucially, this performance recovery through model merging did not compromise AstroSage-Llama-3.1-\n8B’s astronomical expertise-it maintained its 8-point improvement (representing more than 100-fold increase \nin cost-effectiveness) on astronomical Q & A tasks while largely preserving capabilities across most general \nbenchmarks. The only notable performance decrease occurred in IF-EV AL, which tests instruction following. \nThis limited decline is unsurprising, as instruction following remains one of the more brittle capabilities in \nlanguage models and likely heavily depends on the proprietary training data used in Meta’s instruct model. \nIn fact, when comparing AstroSage-Llama-3.1-8B to BAAI/Infinity-Instruct-7M-Gen-LLaMA3 1-8B, the latter \nshows an even more severe performance deficit, highlighting how our refined training strategy and expanded \nSFT dataset represent crucial improvements. Ultimately, our model merging approach successfully preserved \nmost general capabilities without sacrificing the gained astronomical expertise. This balance is essential, as it \nenables AstroSage-Llama-3.1-8B to engage in natural conversations and assist with broader tasks while excelling \nin astronomy-specific applications.\nHuman blind rankings\nSimultaneously with8, we performed a blinded preference ranking of AstroSage-Llama-3.1-8B against Meta-\nLlama-3.1-8B-Instruct. Fifteen questions were written about diverse areas of cosmology, spanning layperson \nto professional-level difficulty. Independent evaluators compared responses from AstroSage-Llama-3.1-8B and \nMeta-Llama-3.1-8B-Instruct, with both models receiving identical system prompts tailored to the cosmology \ncontext.\nResponses were presented in randomized order to the evaluators, who rated their quality without knowledge \nof the source. The evaluation involved three evaluators, who preferred the AstroSage-Llama-3.1-8B answers in \n73% of cases, reflecting a statistically significant preference for AstroSage-Llama-3.1-8B.\nAvailability\nTo promote reproducibility and advance the field of domain-specific AI assistants, we are making AstroSage-\nLlama-3.1-8B freely available under the highly permissive Llama 3.1 Community License. The full model weights \ncan be accessed and downloaded from our project repository on Hugging Face:  h t t p s : / / h u g g i n g f a c e . c o / A s t r o M \nL a b / A s t r o S a g e - 8 B     in either PyTorch or safetensors format.\nFig. 4. Performance comparison across general language model benchmarks. The bar chart shows a \ncomparison of model performances on standard benchmarks. The left radar chart visualizes the effect of \ncontinued pretraining. The right radar chart compares our model post-SFT against BAAI/Infinity-Instruct-\n7M-Gen-LLaMA3_1-8B, which was trained on the same base model and SFT data but without our astronomy-\nspecific data. Despite optimization for astronomy tasks, the merged model we are releasing as AstroSage-\nLlama-3.1-8B maintains strong general capabilities in reasoning, mathematics, and coding, demonstrating that \ndomain specialization did not come at the cost of other abilities.\n \nScientific Reports |        (2025) 15:13751 8| https://doi.org/10.1038/s41598-025-97131-y\nwww.nature.com/scientificreports/\nThe code used to prepare the datasets and perform the training will be made available upon reasonable \nrequest. The synthetically generated Q & A pairs are currently supporting research and development of our next-\ngeneration model. After the completion of our research trajectory, we anticipate a full release of our synthetic \ndata set. For inquiries regarding data usage, collaboration, or access to specific subsets of our work, interested \nparties are encouraged to contact the corresponding author\nBy making AstroSage-Llama-3.1-8B widely available, we aim to foster collaboration and innovation in \nthe astronomy community. We encourage researchers to build upon our work and contribute to the ongoing \ndevelopment of specialized AI assistants for scientific domains.\nDiscussion and future work\nThis work demonstrates the potential of specialized language models in astronomy through a systematic approach \nto model development and evaluation. While previous efforts like 7 laid important groundwork in domain-\nspecific modeling, the field has faced persistent challenges in achieving performance gains over baseline models, \nespecially in instruction-following tasks. Our multi-stage training process-combining continued pretraining, \nextensive supervised fine-tuning, and strategic model merging-addresses these challenges, achieving a notable \nimprovement over the baseline model.\nThese results demonstrate that powerful AI assistants can be developed with relatively small language models \nwhen sufficiently specialized. Despite its modest size of 8 billion parameters, AstroSage-Llama-3.1-8B achieves \nperformance comparable to latest flagship models at a fraction of the cost-approximately one-thousandth of \nproprietary models and one-hundredth of open-weight models. This remarkably favorable performance-to-\nparameter ratio suggests even greater potential for improvement through scaling. Given access to the necessary \ncomputational resources, we plan to apply our successful CPT/SFT procedure to a 70B-class model to pursue \nstate-of-the-art astronomy-specific performance.\nBeyond the performance achievements, our work establishes a more systematic approach to model \nevaluation in astronomy. Through tailored astronomy-specific benchmarking in, we provide a more rigorous \nand transparent assessment than previously available. However, significant challenges remain in comprehensive \nmodel evaluation. The field currently lacks standardized, astronomy-specific benchmarks capable of assessing \nunderstanding across the full spectrum of astronomical tasks, particularly in exact problem-solving capabilities \nlike those tested in ScienceAgentBench 31. This limitation restricts our ability to validate comparisons in more \ndirect scientific agent contexts.\nThe constraints of an 8B-parameter model also become apparent in certain scenarios. While AstroSage-\nLlama-3.1-8B demonstrates impressive performance in subjective testing, the AstroMLab-1 benchmark, and \ngeneral benchmarks, it encounters natural limitations in memory capacity and reasoning depth. Particularly \nchallenging are questions requiring complex multi-step reasoning or sophisticated calculations, where larger \ngeneral-purpose models still maintain an advantage.\nTo address these limitations, our future work will pursue several complementary directions. While scaling \nup model size remains a primary goal, we will also focus on developing more specialized benchmarking tools \nand exploring retrieval-augmented generation for improved knowledge access. Additional initiatives include \ncreating multilingual astronomy assistants, implementing mechanisms for real-time knowledge updates, and \nproviding public inference capabilities.\nThe broader implications of this work extend well beyond its immediate achievements. AstroSage-Llama-3.1-\n8B serves as a compelling proof of concept for highly specialized, smaller-scale language models in astronomy. \nOur approach of extensive data curation, continued pretraining, and careful supervised fine-tuning demonstrates \nhow domain-specific expertise can be enhanced while preserving general capabilities. As the field progresses \ntoward agentic research assistants capable of autonomous literature review, data analysis, and hypothesis \ngeneration, the need for affordable, highly competent domain-specific models will only grow. While challenges \nremain, AstroSage-Llama-3.1-8B charts a promising course for developing the next generation of specialized \nscientific AI assistants, potentially transforming how we approach astronomical research and education.\nData availability\nThe datasets generated and/or analyzed during the current study are not publicly available due to their ongoing \nuse in research and development of our next-generation model, but are available from the corresponding author \non reasonable request.\nReceived: 3 December 2024; Accepted: 2 April 2025\nReferences\n 1. Fu, X.-Y . Laskar, M. T. R. Khasanova, E., Chen, C. & TN, S. B. Tiny Titans: Can Smaller Large Language Models Punch Above Their \nWeight in the Real World for Meeting Summarization? arXiv:2402.00841 (2024).\n 2. Hoffmann, J. et al. Training Compute-Optimal Large Language Models. arXiv:2203.15556 (2022).\n 3. Schick, T. & Schütze, H. It’s Not Just Size That Mat- ters: Small Language Models Are Also Few-Shot Learn- ers. arXiv:2009.07118 \n(2021).\n 4. Turc, I. Chang, M.-W . Lee, K. & Toutanova, K. Well- Read Students Learn Better: On the Importance of Pre- training Compact \nModels. arXiv:1908.08962 (2019).\n 5. Nguyen, T. D. et al. AstroLLaMA: Towards Specialized Foundation Models in Astronomy. arXiv:2309.06126v1 (2023).\n 6. Perkowski, E. et al. Research Notes of the AAS 8, 7 (The American Astronomical Society, 2024).\n 7. Pan, R. et al. In SC24-W: Workshops of the International Conference for High Performance Computing, Networking, Storage and \nAnalysis 87–96 (2024).\n 8. de Haan, T. Astron. Comput. 51, 100934 (2025).\nScientific Reports |        (2025) 15:13751 9| https://doi.org/10.1038/s41598-025-97131-y\nwww.nature.com/scientificreports/\n 9. Sun, Z., Ting, Y .-S,. Liang, Y ., Duan, N., Huang, S. & Cai, Z. Interpreting Multi-band Galaxy Observations with Large Language \nModel-Based Agent. arXiv:2409.14807 (2024).\n 10. Dubey, A. et al. The Llama 3 Herd of Models. arXiv:2407.21783 (2024).\n 11. Ting, Y .-S. et al. AstroM-Lab 1: Who Wins Astronomy Jeopardy!? arXiv:2407.11194v1 (2024).\n 12. Rae, J. W . et al. Scaling Language Models: Methods, Analysis & Insights from Training Gopher. arXiv:2112.11446v2 (2021).\n 13. Brown, T. et al. In Advances in Neural Information Processing Systems, Vol. 33, 1877–1901 (Curran Associates, Inc., 2020).\n 14. Penedo, G. et al. The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only. \narXiv:2306.01116v1 (2023).\n 15. Li, R., Wei, Y ., Zhang, M., Yu, N., Hu, H. & Peng, H. ScalingFilter: Assessing Data Quality through Inverse Utilization of Scaling \nLaws. arXiv:2408.08310v1 (2024).\n 16. Blecher, L., Cucurull, G., Scialom, T. & Stojnic, R. Nougat: Neural Optical Understanding for Academic Documents. \narXiv:2308.13418 (2023).\n 17. tiktoken. https://github.com/openai/tiktoken (2022).\n 18. Dash, S. et al. Optimizing Distributed Training on Frontier for Large Language Models. arXiv:2312.12705v2 (2023).\n 19. Megatron-DeepSpeed. https://github.com/deepspeedai/Megatron-DeepSpeed (2021).\n 20. Smith, S. et al. Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model. \narXiv:2201.11990 (2022).\n 21. Beijing Academy of Artificial Intelligence (BAAI), “BAAI/Infinity-Instruct. Datasets at Hugging Face, ”  h t t p s :   /  / h u g g i n g f a c  e . c  o /  d a t \na s  e  t s / B A   A I / I n  fi   n  i t  y - I n s t r u c t (2024).\n 22. Y adav, P . et al. What Matters for Model Merging at Scale? arXiv:2410.03617 (2024).\n 23. Dassanaike-Perera, A., Waiwitlikhit, S. & Gilbai, K. Cuts and Stitches: Does Model Merging Produce Better Multitask Learners?, \nStanford CS224N Default Project (Stanford, 2023).\n 24. Goddard, C. et al. Arcee’s MergeKit: A Toolkit for Merging Large Language Models. arXiv:2403.13257 (2024).\n 25. Zhou, J. et al. Instruction-Following Evaluation for Large Language Models. arXiv:2311.07911 (2023).\n 26. Suzgun, M. et al. Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them. arXiv:2210.09261 (2022).\n 27. Hendrycks, D. et al. Measuring Mathematical Problem Solving With the MATH Dataset. arXiv:2103.03874 (2021).\n 28. Rein, D. et al. GPQA: A Graduate-Level Google-Proof Q & A Benchmark. arXiv:2311.12022 (2023).\n 29. Sprague, Z., Y e, X., Bostrom, K., Chaudhuri, S. & Durrett, G. MuSR: Testing the Limits of Chain-of-thought with Multistep Soft  \nReasoning. arXiv:2310.16049v2 (2023).\n 30. Wang, Y . et al. MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark. arXiv:2406.01574 \n(2024).\n 31. Chen, Z. et al. arXiv:2410.05080 (2024).\nAcknowledgements\nThis research used resources of the Oak Ridge Leadership Computing Facility (OLCF), which is a DOE Office \nof Science User Facility at the Oak Ridge National Laboratory supported by the U.S. Department of Energy \nunder Contract No. DE-AC05-00OR22725 and support from Microsoft’s Accelerating Foundation Models Re-\nsearch (AFMR) program. TdH was supported by World Premier International Research Center Initiative (WPI), \nMEXT, Japan. YST is supported by the National Science Foundation under Grant No. 2406729. Work at Argonne \nNational Lab is supported by UChicago Argonne LLC, Operator of Argonne National Laboratory. Argonne, a \nU.S. Department of Energy Office of Science Laboratory, is operated under contract no. DE-AC02-06CH11357. \nA special thanks goes out to Cassie Reuter and Joshua Montgomery for acting as independent evaluators.\nAuthor contributions\nTdH performed data preparation, cleaning, training, astronomy-specific evaluation, wrote the majority of the \nmanuscript text, and created the figures. YST performed astronomy-specific evaluation, provided computational \nresources, provided the first draft of the astronomy benchmark figure, and made significant contributions to the \nmanuscript text. AW performed general-purpose evaluation. All authors read and agree with the contents of the \nmanuscript.\nDeclarations\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to T.H.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give \nappropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and \nindicate if changes were made. The images or other third party material in this article are included in the article’s \nCreative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included \nin the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy \nof this licence, visit http://creativecommons.org/licenses/by/4.0/.\n© The Author(s) 2025 \nScientific Reports |        (2025) 15:13751 10| https://doi.org/10.1038/s41598-025-97131-y\nwww.nature.com/scientificreports/",
  "topic": "Benchmark (surveying)",
  "concepts": [
    {
      "name": "Benchmark (surveying)",
      "score": 0.7340887784957886
    },
    {
      "name": "Computer science",
      "score": 0.5902130603790283
    },
    {
      "name": "Cosmology",
      "score": 0.5647572875022888
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5419901013374329
    },
    {
      "name": "Class (philosophy)",
      "score": 0.5207083821296692
    },
    {
      "name": "Astronomy",
      "score": 0.4947187602519989
    },
    {
      "name": "Instrumentation (computer programming)",
      "score": 0.472459077835083
    },
    {
      "name": "Range (aeronautics)",
      "score": 0.4534981846809387
    },
    {
      "name": "Artificial intelligence",
      "score": 0.34717434644699097
    },
    {
      "name": "Physics",
      "score": 0.3194549083709717
    },
    {
      "name": "Programming language",
      "score": 0.16364476084709167
    },
    {
      "name": "Mathematics",
      "score": 0.14184322953224182
    },
    {
      "name": "Engineering",
      "score": 0.12492859363555908
    },
    {
      "name": "Geography",
      "score": 0.07575273513793945
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Aerospace engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I138728355",
      "name": "High Energy Accelerator Research Organization",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I52357470",
      "name": "The Ohio State University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1289243028",
      "name": "Oak Ridge National Laboratory",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I79576946",
      "name": "University of Pennsylvania",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210124175",
      "name": "Center for Astrophysics Harvard & Smithsonian",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1282105669",
      "name": "Argonne National Laboratory",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I200769079",
      "name": "Hong Kong University of Science and Technology",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I889458895",
      "name": "University of Hong Kong",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    }
  ],
  "cited_by": 2
}