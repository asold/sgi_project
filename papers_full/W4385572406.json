{
  "title": "Images in Language Space: Exploring the Suitability of Large Language Models for Vision &amp; Language Tasks",
  "url": "https://openalex.org/W4385572406",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2221991438",
      "name": "Sherzod Hakimov",
      "affiliations": [
        "University of Potsdam"
      ]
    },
    {
      "id": "https://openalex.org/A2042143543",
      "name": "David Schlangen",
      "affiliations": [
        "German Research Centre for Artificial Intelligence",
        "University of Potsdam"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2998696444",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W2265228180",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W2744909235",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4226182655",
    "https://openalex.org/W3198377975",
    "https://openalex.org/W3111372685",
    "https://openalex.org/W4226455589",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4313483544",
    "https://openalex.org/W4312310776",
    "https://openalex.org/W3101998545",
    "https://openalex.org/W4226321975",
    "https://openalex.org/W4281633595",
    "https://openalex.org/W4297816851",
    "https://openalex.org/W4296001058",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W4287855069",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W2963530300",
    "https://openalex.org/W2947312908",
    "https://openalex.org/W4385567149",
    "https://openalex.org/W2968124245",
    "https://openalex.org/W2152161678",
    "https://openalex.org/W4309067651",
    "https://openalex.org/W4225323055",
    "https://openalex.org/W4281556902",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2080133951",
    "https://openalex.org/W3090449556",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2732026016",
    "https://openalex.org/W4320003957",
    "https://openalex.org/W4287113019",
    "https://openalex.org/W3195510721",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W4287891186",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W3033210410",
    "https://openalex.org/W4300978570",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4226352076"
  ],
  "abstract": "Large language models have demonstrated robust performance on various language tasks using zero-shot or few-shot learning paradigms. While being actively researched, multimodal models that can additionally handle images as input have yet to catch up in size and generality with language-only models. In this work, we ask whether language-only models can be utilised for tasks that require visual input – but also, as we argue, often require a strong reasoning component. Similar to some recent related work, we make visual information accessible to the language model using separate verbalisation models. Specifically, we investigate the performance of open-source, open-access language models against GPT-3 on five vision-language tasks when given textually-encoded visual information. Our results suggest that language models are effective for solving vision-language tasks even with limited samples. This approach also enhances the interpretability of a model’s output by providing a means of tracing the output back through the verbalised image content.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 14196–14210\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nImages in Language Space: Exploring the Suitability of Large\nLanguage Models for Vision & Language Tasks\nSherzod Hakimov1 and David Schlangen1,2\n1Computational Linguistics, Department of Linguistics\nUniversity of Potsdam, Germany\n2German Research Center for Artificial Intelligence (DFKI), Berlin, Germany\nfirstname.lastname@uni-potsdam.de\nAbstract\nLarge language models have demonstrated ro-\nbust performance on various language tasks\nusing zero-shot or few-shot learning paradigms.\nWhile being actively researched, multimodal\nmodels that can additionally handle images as\ninput have yet to catch up in size and general-\nity with language-only models. In this work,\nwe ask whether language-only models can be\nutilised for tasks that require visual input – but\nalso, as we argue, often require a strong reason-\ning component. Similar to some recent related\nwork, we make visual information accessible\nto the language model using separate verbali-\nsation models. Specifically, we investigate the\nperformance of open-source, open-access lan-\nguage models against GPT-3 on five vision-\nlanguage tasks when given textually-encoded\nvisual information. Our results suggest that lan-\nguage models are effective for solving vision-\nlanguage tasks even with limited samples. This\napproach also enhances the interpretability of a\nmodel’s output by providing a means of tracing\nthe output back through the verbalised image\ncontent.\n1 Introduction\nIn recent years, large language models have gained\nsignificant attention in the natural language process-\ning (NLP) community due to their impressive per-\nformance on various tasks such as machine trans-\nlation, text generation, and language modelling\n(Vaswani et al., 2017; Devlin et al., 2019). These\nmodels, which are trained on massive amounts\nof data, have been shown to capture complex lin-\nguistic patterns and generate coherent text (Brown\net al., 2020). Some of the most popular models\nare trained by OpenAI, a research organization that\nhas released several models, including GPT (Rad-\nford et al., 2018), GPT-2 (Radford et al., 2019),\nand GPT-3 (Brown et al., 2020). In addition to\nGPT models, there are also many open-source or\nopen-access large language models that researchers\nand organizations around the world have developed,\nsuch as BLOOM (Scao et al., 2022), GPT-J (Wang\nand Komatsuzaki, 2021), OPT (Zhang et al., 2022),\nFlan-T5 (Chung et al., 2022).\nRecent work by Liang et al. (2022) provided an\nin-depth analysis of many large language models\n(LLM) across 42 core scenarios. 1 All scenarios\nare language tasks that are evaluated with multiple\nmetrics by prompting the language models with\nfew-shots from the selected datasets, also known\nas in-context learning. Currently, there are no com-\nparable models directly suitable for tasks that re-\nquire visual information as part of the context, even\nthough such multimodal tasks have similar practi-\ncal relevance.\nPre-trained vision-language models (Li et al.,\n2019; Chen et al., 2020; Dosovitskiy et al., 2021;\nRadford et al., 2021) have shown great promise by\nlearning joint representation of images and text doc-\numents, but so far they have not been optimised for\nprompting on vision-language tasks but rather us-\ning the learned joint representations for fine-tuning\non downstream tasks. Moreover, as we note below,\nmany multimodal tasks appear to rely on reason-\ning capabilities, which larger language models have\nbeen shown to perform well on (Talmor et al., 2020;\nLi et al., 2022b). Hence, in this work, we attempt\nto utilise such models to do in-context learning on\nmultimodal data, achieving this by encoding the\nvisual information in language.\nWhile there has been some recent work going in\nthis direction (see discussion below), it falls short\nin terms of evaluating the performance of large\nlanguage models across multiple dimensions, ap-\nplying them to a diverse range of vision-language\ntasks, and comparing the performance of GPT mod-\nels with open-source or open-access models. As\nsuch, there is a need for further research in this area\nto fully understand the capabilities and limitations\nof these models for vision-language tasks.\n1https://crfm.stanford.edu/helm/v0.2.0/\n14196\nImage Captioning ModelsImage Classification Models\n“a view of a lake and mountain at sunset”\n“lake, mountain, forest, sun, clouds”\nAnswer the following question given an image context and text. The task is toTask Description\nIn-context Samples\nEvaluationSample\nImage-as-text-rep.: <1_image_text_rep>Question: <Sample_1_question_text>Answer: <Sample_1_answer>…Image-as-text-rep.: <n_image_text_rep>Question: <Sample_n_question_text>Answer: <Sample_n_answer>\nImage-as-text-rep.: <Image_text_rep>Question: <Question_text>Answer: \nclassify whether the context is <TASK-SPECIFIC-PHRASE>.Possible labels: <TASK-LABELS>answer the questionClassificationQuestion Answering\nText: <Sample_1_text>Image-as-text-rep.: <1_image_text_rep>Question: What is the label of the given image context and text?Answer: <Sample_1_answer>…Text: <Sample_n_text>Image-as-text-rep.: <n_image_text_rep>Question: What is the label of the given image context and text?Answer: <Sample_n_answer>\nText: <Text>Image-as-text-rep.: <Image_text_rep>Question: What is the label of the given image context and text?Answer: \nSample Image\nImage-as-text-representationExtraction\nappend tosamples\nPrompt Text\nLargeLanguageModel\nanswer\nprompt\ngenerate\nFigure 1: Model architecture for in-context learning for vision-language tasks. Each sample image is converted into\nits image-as-text-representation by applying pre-trained image captioning and classification models (yellow). The\nprompt text that is fed into a large language model consists of a task-specific description (blue), in-context samples\n(pink), and the evaluation sample (green). The language model is expected to generate a text sequence that follows\nthe word Answer in the evaluation sample.\nIn this paper, we aim to explore the capabili-\nties of large language models for in-context learn-\ning and their potential to improve performance on\nmultimodal tasks (see Figure 1). To this end, we\nconducted a series of experiments to evaluate the\nperformance of various language models (closed,\nopen-source and open-access) on tasks that involve\nmultiple modalities, such as vision and language.\nThe tasks vary from identification of hate speech\nand sentiment to visual reasoning and question\nanswering. Our results provide insights into the\nstrengths and limitations of these models and high-\nlight methods of expressing visual content through\ntextual descriptions. Our work aims to analyse how\nmuch general reasoning is in language models by\nevaluating them on multimodal tasks where the\nvisual content is only accessible partially or indi-\nrectly (since the visual content is verbalised and\nrepresented in textual form) accessible. Our main\ncontributions are as follows2:\n• We examine the impact of in-context learning\nwith large language models on five vision-\nlanguage tasks, four classification and one\nquestion answering tasks;\n• We investigate the impact of the textual de-\nscription generation for the visual content\non the model performance for the respective\ntasks;\n2Source code and all resources are made pub-\nlicly available at https://github.com/clp-research/\nlanguage-models-multimodal-tasks\n• We compare the performance of open-source\nand open-access models with GPT-3 on the\nselected vision-language tasks.\n2 Related Work\nRecent work by Liang et al. (2022) provided an\nin-depth analysis of many 34 large language mod-\nels (LLM), open, limited-access, and closed. Their\nanalysis revealed the capabilities and limitations of\nthese models across 42 core scenarios. All scenar-\nios are language tasks that are evaluated with 57\nmetrics by prompting the language models with a\nfew shots from the selected datasets. Such a way of\nleveraging pre-trained language models for down-\nstream tasks is known asin-context learning, where\na certain task description and a few shots are pre-\nsented as a context for the model. A recent survey\nby Dong et al. (2023) describes the developed tech-\nniques for in-context learning where they present\na taxonomy that divides the techniques used for\nprompting such as selection of in-context samples,\nreasoning step by step (chain of thought) (Wei et al.,\n2022b), task definition, etc. Moreover, Min et al.\n(2022) assessed the importance of choosing the in-\ncontext samples and its effect on the performance.\nSo far, a large-scale analysis of large language\nmodels and their performances for multimodal data,\nsuch as vision-language tasks, has not been done.\nA handful of methods demonstrated the effective-\nness of in-context learning for multimodal tasks.\nZhou et al. (2022b,a) modelled the context words in\nprompts for applying pre-trained vision-language\n14197\ntasks for downstream vision tasks. Tsimpoukelli\net al. (2021) trained a vision encoder to represent\nimages as a sequence of continuous embeddings\nwhere a prompted pre-trained language model gen-\nerates a caption. Yang et al. (2022) demonstrated\nthe applicability of GPT-3 on a visual question an-\nswering task where they converted the images into\ntextual descriptions by using an image captioning\nmodel and extraction of visual tags that correspond\nto detected objects, landmarks, person, image type,\netc. Zeng et al. (2022) follows similar methodology\nby showing applications on multiple applications\nthat include modalities such as audio, video be-\nside image and text. Gui et al. (2022)’s method\nis complementary to the previous method with an\naddition of a contrastive learning module that re-\ntrieves knowledge entries from Wikidata knowl-\nedge graph (Vrandecic and Krötzsch, 2014). Wang\net al. (2022b) applied the method of converting im-\nages into textual descriptions to video tasks. The\nresulting outputs are temporally aligned for a video\nand then fed into GPT-3 with few shots. More\nrecently, Merullo et al. (2023) aligned image-text\nencoders by training a linear project layer and keep-\ning the pre-trained image and text encoders frozen.\nOur paper presents a study that goes beyond these\nsimilar approaches by extending the experimental\nevaluation to multiple datasets, comparing open-\nsource language models with GPT-3, and evaluat-\ning different methods of acquiring textual represen-\ntation for the visual content.\n3 Text-Visual In-Context Learning\nIn this section, we describe the proposed method-\nology of applying in-context learning to vision-\nlanguage tasks. In-context learning essentially\nworks by prompting a pre-trained language model\nwith the task and expecting it to generate text that\nsolves a particular task. It is performed by giving a\nfew-shots of the respective task at inference time\nwithout requiring updating the model weights and\nexpecting the model to generate text corresponding\nto the expected output.\nFormally, given a query input text x and a set\nof candidate answers Y = {y1...ym}, which\ncan be class labels for a particular task or free\ntext, a pre-trained model M outputs a candidate\nanswer with the maximum score conditioned on\nthe task description T, n in-context sample pairs\nC = {(x1, y1)...(xn, yn)}. The likelihood of the\ncandidate answer yj can be represented by a scor-\ning function f with the language model M (Wei\net al., 2022a; Dong et al., 2023):\nP(yj|x) ≜ fM (yj, T, C,x) (1)\nThe final predicted candidate answer of the\nmodel (ˆy) can be formulated as:\nˆy = argmaxyj∈Y P(yj|x) (2)\nOur proposed methodology for “text-visual in-\ncontext learning” is shown in Figure 1. First,\nall images from all evaluated datasets have been\npassed through multiple pre-trained image mod-\nels to obtain the textual description of the vi-\nsual content, which we refer to as image-as-text-\nrepresentation throughout the paper. The image-as-\ntext-representation is essentially a textual descrip-\ntion of the visual content that captures important\nvisual aspects. The prompt text comprises the task\ndescription, in-context sample pairs, and the input\nof the evaluation sample. Given such a prompt text,\nthe language model generates a sequence of text\ntokens as an output.\nWe evaluate the proposed methodology on var-\nious vision-language datasets that include either\nclassification or question answering tasks. Thus,\nthe task description is different between these two\ncategories. The task description for the classifica-\ntion tasks is further replaced with the task-specific\nphrase that describes the downstream task and pro-\nvides the task-specific class labels. More details on\nthe exact prompt text for each dataset are provided\nin Appendix A. Next, we describe the methods\nfor extracting image-as-text-representation, select-\ning in-context samples, and aggregating answer\npredictions in cases where the language model is\nprompted multiple times with various in-context\nsamples for the evaluation sample.\n3.1 Image-as-Text-Representation Extraction\nWe use two different methods to extract textual\nrepresentation of images for any vision-language\ntask. The first is to use pre-trained image caption-\ning models that generate a text sequence describing\nthe input image. The second is to employ mul-\ntiple pre-trained image classification models and\nextract top-scoring class labels. The extracted class\nlabels from all models are merged to form the set\nof visual tags that describe the image. Specifically,\nwe use pre-trained models to recognise objects, in-\ndoor or outdoor scenes, and persons and their facial\nemotions. These methods yield a different textual\n14198\ndescription of an input image, which is used as\nimage-as-text-representation in the prompt text.\n3.2 In-Context Sample Selection\nThe selection of samples for in-context learning\nis an essential step for prompting large language\nmodels. Each model has its own limitation of the\nmaximum input tokens that a prompt text can have\n(e.g. 512 tokens for Flan-T5, 4000 for GPT-3).\nTherefore, only a few samples can be used (few-\nshot), and the selection directly impacts the model\nperformance (Yang et al., 2022; Min et al., 2022).\nWe experiment with the following sample selection\nmethods.\nRandom sample selection works by selecting\nany random n samples from the training split of\na dataset (that fit into the maximum input token\nlength of a language model).\nAdaptive sample selectionuses specific similarity\nmeasurement to rank samples with respect to the\ninput sample. Top-ranking n samples are selected\n(that fit into the maximum input token length of a\nlanguage model) to prompt a language model.\nThe in-context samples are selected from the\ntraining split of the respective dataset.\n4 Experimental Setup\nIn this section, we describe the details of the\nbuilding blocks of the methodology, the evaluated\ndatasets, large language models, and methods for\nobtaining a textual description of images.\n4.1 Datasets, Comparison Models &\nEvaluation Metrics\nWe use the following five datasets to evaluate the\nperformance of the closed and open-access lan-\nguage models. The best-performing prior models\nare selected from the leaderboards of the respective\ndatasets. These models are used for comparison\nwith our prompting method.\n• MAMI - Multimedia Automatic Misogyny\nIdentification (SemEval 2022 Task 5) (Fersini\net al., 2022): the dataset consists of memes\nthat are classified for being offensive hate-\nful towards women. The train and test splits\nhave 10 000and 1000 samples, respectively.\nWe use the sub-task A for the experiments to\npredict a binary class indicating whether the\ngiven meme is misogynous.\nComparison model: Zhang and Wang (2022)\nproposed to use an ensemble system com-\nposed of pre-trained models (CLIP) used for\nextracting features from multimodal data.\nEvaluation metric: Macro-F1\n• HF - Hateful Memes (Kiela et al., 2020): is\nanother dataset that focuses on classifying\nmemes whether the overall message it is hate-\nful or not towards any group. We use the\nprovided development split for the evaluation\nsince the test split is closed to the community\nat the time of writing. The train and devel-\nopment splits have 8500 and 500 samples, re-\nspectively.\nComparison model: the best-performing\nmodel provided is by Kiela et al. (2020), for\nwhich the model performance on the devel-\nopment split is available. The method uses\npre-trained ViLBERT model that is later fine-\ntuned on the dataset.\nEvaluation metric: Accuracy\n• MVSA - Multi-View Sentiment Analysis (Niu\net al., 2016): is a multimodal sentiment analy-\nsis dataset collected from Twitter. The task is\nto classify the sentiment of the given post with\nan image and tweet text intopositive, negative,\nor neutral.\nPrevious work on this dataset has used dif-\nferent train and test splits, making the direct\ncomparison among approaches not feasible.\nWe follow recently provided splits by Cheema\net al. (2021) and their evaluation scheme by\nperforming 10-fold cross-validation on the\nrespective train and test splits. Overall, the\ndataset includes total 3928 samples with 2328,\n1167, 433 samples corresponding to positive,\nnegative, and neutral class labels, respectively.\nWe use the version named MVSA-Single of\nthis dataset.\nComparison model: Cheema et al. (2021)’s\nmodel uses image features from CLIP and text\nfeatures from RoBERTa models and fine-tune\nthem on the dataset.\nEvaluation metric: Accuracy averaged over\n10-folds.\n• OK-VQA - Outside Knowledge Visual Ques-\ntion Answering (Marino et al., 2019): is a\nvisual question answering dataset consisting\nof 14 055samples where each sample con-\ntains an open-ended question and five ground\ntruth answers. The task is to predict one of the\n14199\nexpected answers given the question and the\nimage. There are 9009 and 5046 samples in\nthe train and test splits of the dataset, respec-\ntively.\nComparison model: Wu et al. (2022)’s\nmethod is based on three-stage scheme where\nthe first step generates a set of answer candi-\ndates by analysing the syntactic structure of\nthe question. The next step retrieves candidate\nanswers by searching the Wikipedia and Con-\nceptNet, and finally the third step validates the\ncandidate answers.\nEvaluation metric: Accuracy\n• NLVR2 - Natural Language for Visual Reason-\ning for Real (Suhr et al., 2019): is a dataset\nfor reasoning over two images and a statement\nwhere the task is predict whether the statement\nis true or false. The dataset includes 86 373\nand 6967 samples for the train and test splits,\nrespectively. We used thetest-public split of\nthe dataset.\nComparison model: Chen et al. (2020)’s\napproach is based on first pre-training a\njoint multimodal model on image captioning\ndatasets and then fine-tune the model on the\ntask.\nEvaluation metric: Accuracy\n4.2 Language Models\nWe experiment with multiple pre-trained open-\nsource and open-access language models and com-\npare them against GPT-3. These language models\nare as follows:\n• Flan-T5 (Chung et al., 2022): is a language\nmodel fine-tuned on multiple tasks with an\ninstruction-specific training paradigm. We use\nthe flan-t5-xxl version.\n• T0pp (Sanh et al., 2022): is a language model\nthat has been fine-tuned on multiple datasets\nto perform for zero or few-shot prompting.\n• OPT (Zhang et al., 2022): is a language model\ntrained on multiple large datasets. The lan-\nguage model has various versions with differ-\nent sizes. We use the opt-2.7b version.\n• GPT-3: we use the text-davinci-003 version.\n4.3 Methods for Extracting\nImage-as-Text-Representations\nThe generation of the textual representation of im-\nages is carried out in two ways: image captioning\nand the combination of multiple image classifica-\ntion model outputs.\nImage Captioning: we use the following image\ncaptioning models to convert the images to textual\ndescriptions:\n• ViT-GPT-2 (Vision Transformers GPT-\n2) (NLP Connect, 2022)\n• OFA (One for all) (Wang et al., 2022a)\n• BLIP (Bootstrapping Language-Image Pre-\ntraining) (Li et al., 2022a)\nVisual Tags: we use the following image clas-\nsification models to build the set of tags extracted\nfrom a given image:\n• Image type: a zero-shot classification with\nCLIP (Radford et al., 2021) by pairing an im-\nage with one of the following text snippets and\nselecting the one that outputs the highest prob-\nability: “This is an image“, “This is a sketch“,\n“This is a cartoon“, “This is a painting“. We\nselect the top-ranking class label that has a\nprobability higher or equal to 0.80.\n• Object: the pre-trained Detection Transformer\n(DETR) model (Carion et al., 2020) is used\nto obtain the bounding boxes of detected ob-\njects. We select the top-ranking class labels\nthat have a probability higher or equal to 0.90.\n• Indoor and outdoor scenes: we use two differ-\nent pre-trained models to predict the scenes\nin the given images. The first model is Vi-\nsion Transformer (ViT) (Wu et al., 2020) pre-\ntrained on Indoor Scene dataset (Quattoni and\nTorralba, 2009). The second model is a pre-\ntrained ResNet-50 on Places365 dataset (Zhou\net al., 2018). We select the top-ranking class\nlabels that have a probability higher or equal\nto 0.80.\n• Facial expression: we use the pre-trained\nMTCNN model (Zhang et al., 2016) to de-\ntect faces in images and identify seven facial\nemotions: angry, disgust, fear, happy, sad, sur-\nprise, neutral. (Goodfellow et al., 2015). We\nselect the top-ranking detected faces (proba-\nbility >= 0.90) and use them to infer the facial\nexpression classes. The top-ranking facial ex-\npression class label (probability >= 0.50) is\nselected for each detected face.\n4.4 Prompt Structure\nSimilarity measurement: As mentioned above in\nSection 3.2, we employ two different methods for\n14200\nselecting samples for in-context learning: random\nand adaptive. In order to select the best fitting n\nsamples for the adaptive prompting, we use the\nSentence Transformers (Reimers and Gurevych,\n2019) to calculate the similarities among samples\nfor the adaptive method. The pre-trained all-mpnet-\nbase-v2 model is used to extract embeddings from\ntwo given sample documents and calculates the\ncosine similarity between them3. For any given two\nsamples (one evaluation and the other one from a\ntraining split), we calculate the similarity between\nthe text content and image-as-text representation\nobtained from the methods described before. The\nsimilarities from textual content and image-as-text-\nrepresentation are averaged.\nSample selection: Once the most similar sam-\nples to the given evaluation sample are identified,\nthe next step is to select n samples out of them.\nDuring selection, we ensure that the selected sam-\nples are equally distributed across the class labels\nfor any dataset. This only applies to the classifi-\ncation tasks where the labels are predefined (e.g.\nhateful or not, true/false, positive/negative/neutral).\nIt is to present samples with different labels for\nin-context learning. We also experiment with a\nzero-shot (n=0) where the prompted text includes\nonly the task description.\nThe prompt structure for each dataset is available\nin Appendix A.\n4.5 Model Parameters & Implementation\nWe experimented with various configurations of\nthe model parameters. The following values are\nused for all language models: max new tokens set\nas 10, number of beams is set as 10, temperature is\nset to the default value of each language model.\nThe implementation of the overall architecture\nand other building blocks (image captioning & clas-\nsification) is based on the PyTorch library. We used\nthe language models that are available in the Hug-\ngingFace directory and queried the backend API\nof OpenAI for experiments with GPT-3. All ex-\nperiments have been carried out on two NVIDIA\nA100 GPUs (80 GB). The estimated runtime of all\nexperiments is approximately 200 hours.\n5 Results and Analysis\nIn this section, we discuss the obtained results\nfrom the experiments such as the impact of in-\n3https://huggingface.co/sentence-transformers/\nall-mpnet-base-v2\nNumber of Samples\nDataset Models n=0 n=1 n=2 n=3 S\nMVSA\n(Acc)\nFlan-T5 6.5 22.9 28.4 29.1 r\nFlan-T5 6.5 21.4 31.8 35.2 a\nT0pp 68.1 54.2 57.7 45.0 r\nT0pp 68.1 57.4 62.3 51.8 a\nOPT 0.0 12.9 11.8 14.4 r\nOPT 0.0 11.5 11.1 11.3 a\nOK-VQA\n(Acc)\nFlan-T5 27.0 28.1 28.7 29.5 r\nFlan-T5 27.0 32.4 34.4 35.4 a\nT0pp 13.9 18.4 18.4 18.2 r\nT0pp 13.9 19.4 20.2 20.3 a\nOPT 0.9 5.7 3.6 4.3 r\nOPT 0.9 10.0 3.9 3.3 a\nNLVR2\n(Acc)\nFlan-T5 0.0 12.0 19.5 19.7 r\nFlan-T5 0.0 25.6 31.7 31.7 a\nT0pp 58.6 57.5 49.2 49.5 r\nT0pp 58.6 55.2 50.7 50.1 a\nOPT 42.2 47.7 46.3 45.3 r\nOPT 42.2 26.9 10.8 8.1 a\nHF\n(Acc)\nFlan-T5 55.2 53.2 53.6 53.8 r\nFlan-T5 55.2 57.4 56.6 56.0 a\nT0pp 50.0 48.8 1.6 0.0 r\nT0pp 50.0 53.6 49.0 33.0 a\nOPT 0.2 44.4 41.4 36.0 r\nOPT 0.2 35.2 38.2 30.6 a\nMAMI\n(F1)\nFlan-T5 41.5 51.7 37.7 56.1 r\nFlan-T5 41.5 61.9 64.4 64.1 a\nT0pp 26.0 22.2 6.8 0.0 r\nT0pp 26.0 46.0 33.8 21.8 a\nOPT 17.0 22.2 22.1 21.9 r\nOPT 17.0 22.3 22.0 22.4 a\nTable 1: Ablation study on the number of in-context\nsamples and the method of selecting them. n refers to\nthe number of samples in a given prompt, S stands for\nthe selection method: adaptive ( a) or random (r). All\nruns are based on using captioning from BLIP model.\nThe best result for each dataset is highlighted in bold.\ncontext sample selection, image-as-text representa-\ntion methods and comparing with fine-tuned vision-\nlanguage models on the selected datasets.\n5.1 Impact of In-Context Sample Selection\nSample Selection: we have conducted experiments\nwith different configurations of selecting in-context\nsamples. The results are presented in Table 1. In\nfour out of five datasets, the adaptive sample selec-\ntion yields better performance thanrandom method.\nOnly in the MVSA dataset, random method yields\nthe best result.\nNumber of Samples: The presented results for\neach evaluated language model include the differ-\nent number of samples in a prompt. We tested\nnumbers 0, 1, 2, 3 where n=0 essentially means\nthat there are no in-context samples in a prompt,\nand it is zero-shot performanceof an evaluated\nlanguage model. It is a few-shot setting in cases\nwhere n is bigger than zero. We can observe that\n14201\nDataset Models\nImage-as-text\nRepresentation\nBLIP VG OFA VT\nMVSA\n(Acc)\nFlan-T5 31.8 21.6 27.7 16.1\nT0pp 62.3 61.8 62.4 63.1\nOPT 11.1 11.0 19.5 12.7\nOK-VQA\n(Acc)\nFlan-T5 34.4 32.6 31.1 29.2\nT0pp 20.2 19.7 18.3 17.8\nOPT 3.9 4.0 19.5 14.8\nNLVR2\n(Acc)\nFlan-T5 31.7 25.6 25.5 23.4\nT0pp 50.7 49.4 50.1 51.0\nOPT 10.8 19.3 29.6 3.1\nHF\n(Acc)\nFlan-T5 56.6 54.8 54.6 56.8\nT0pp 49.0 49.2 49.6 48.8\nOPT 38.2 38.4 43.4 42.0\nMAMI\n(F1)\nFlan-T5 64.4 48.6 60.2 60.3\nT0pp 33.8 33.6 33.6 22.6\nOPT 22.0 22.1 22.2 22.2\nTable 2: Ablation study on the affect of using different\nimage-as-text representation methods. All runs of each\nmodel has been set to adaptive sample selection with\nn=2 (number of in-context samples in a prompt). VG:\nViT-GPT2, VT: Visual Tags\nin three out of five datasets, using n >1 yields bet-\nter performance, whereas T0pp achieves the best\nperformance in MVSA and NLVR2 datasets.\n5.2 Evaluation of Image-as-Text\nRepresentation Methods\nAs explained in Section 4.3, we have used four\nmethods of verbalising the visual content and\nadding the output to the prompt as an image-as-\ntext representation. We have tested these methods\non all datasets. The results are presented in Ta-\nble 2. Based on the outcomes in Section 5.1, we\nhave used adaptive sample selection with n=2 for\nall runs. We can observe that in the majority of\nthe evaluated datasets, using captions generated by\nBLIP model yields the higher performance on av-\nerage. The textual descriptions generated by the\nmethod Visual Tags(collection of multiple image\nclassification high-probability outputs) resulted in\nthe highest performance on three datasets.\n5.3 Comparison of Language Models\nIn Table 3, we have selected the best-ranking con-\nfiguration of each model for all datasets. All model\nconfigurations use image captions generated by the\nBLIP model to represent the image context in text.\nTo reduce the budget, we ran GPT-3 experiments\nonly on a pre-selected set of parameters (n=2, adap-\ntive) that yielded the best results using open-source\nlanguage models. The overall comparison of all\nresults shows that GPT-3 achieves the best result\nDataset Models Result\nMVSA\n(Acc)\nFlan-T5, n=3, adaptive 35.2\nGPT-3, n=2, adaptive 64.3\nOPT, n=3, random 14.4\nT0pp, n=0, adaptive 68.1\nFine-tuned V&L Model\nCheema et al. (2021) 75.3\nOK-VQA\n(Acc)\nFlan-T5, n=3, adaptive 35.4\nGPT-3, n=2, adaptive 25.9\nOPT, n=1, adaptive 10.0\nT0pp, n=3, adaptive 20.3\nFine-tuned V&L Model\nWu et al. (2022) 41.4\nNLVR2\n(Acc)\nFlan-T5, n=2, adaptive 31.7\nGPT-3, n=2, adaptive 63.0\nOPT, n=1, random 47.7\nT0pp, n=1, adaptive 58.6\nFine-tuned V&L Model\nChen et al. (2020) 79.5\nHF\n(Acc)\nFlan-T5, n=2, adaptive 57.4\nGPT-3, n=2, adaptive 58.8\nOPT, n=1, random 44.4\nT0pp, n=1, adaptive 53.6\nFine-tuned V&L Model\nKiela et al. (2020) 66.1\nMAMI\n(F1)\nFlan-T5, n=2, adaptive 64.4\nGPT-3, n=2, adaptive 69.2\nOPT, n=3, adaptive 22.4\nT0pp, n=2, adaptive 46.0\nFine-tuned V&L Model\nZhang and Wang (2022) 83.4\nTable 3: Overall comparison of the best-ranking config-\nurations for each model. The best result for each dataset\nusing prompting with language models is highlighted\nin bold. All model configurations use image captions\ngenerated by BLIP model. V&L: Vision-Language\non three datasets: MAMI, HF, and NLVR2. T0pp\nachieves the best result on MVSA dataset, whereas\nthe best-ranking model for the OK-VQA is Flan-T5.\nWe have also included the results from the fine-\ntuned vision-language models for each dataset. By\ncomparing the results obtained via prompting with\nfine-tuned models, with only a few-shots (n = 1, 2,\n3), the language models can generalise to vision-\nlanguage tasks and achieve comparable results. An\nimportant observation is that these models were\ntrained only on text documents. Prompting these\nmodels on five downstream vision-language tasks\nby converting the visual content into textual repre-\nsentation made it possible.\n5.4 Qualitative Analysis\nWe present qualitative examples from each dataset\nin Figure 2. Each sample includes the image-as-text\nrepresentation extracted from the BLIP model. We\nalso included the ground truth for each sample and\nthe responses generated from Flan-T5 and GPT-3\nmodels (best configurations as in Table 3). We also\nadded the Visual Tags for each sample (combina-\n14202\nBLIP caption:  a woman holding a sign that says shut up uglyVisual Tags: This is an image with 2 people,  car, cup, handbagGround truth: HatefulFlan-T5 response: HatefulGPT-3 response: Hateful\nMAMI (15601.jpg)\nBLIP caption:  two cartoon pictures of a girl eating a bowl of foodVisual Tags: This is a cartoonGround truth: Not hatefulFlan-T5 response: NothatefulGPT-3 response: Not hateful\nMAMI (15573.jpg)\nBLIP caption:  a woman sitting at a desk in front of a computerVisual Tags: This is an image with 1 person,  with facial expression happy, objects:mouse, dining table, laptop, cupGround truth: HatefulFlan-T5 response: HatefulGPT-3 response: Hateful\nHF (03745.png)\nBLIP caption: a grilled sandwich on a grill with the words look at this sandwich maker clubVisual Tags: This is an image with  sandwichGround truth: Not hatefulFlan-T5 response: Not hatefulGPT-3 response: Not hateful\nHF (48670.png)\nBLIP caption:  a red laptop computer sitting on top of a white surfaceVisual Tags: This is an image with  person\nGround truth: TrueFlan-T5 response: TrueGPT-3 response: True\nNLVR2test1-400-1-img0.pngtest1-400-1-img1.png\nBLIP caption:  a laptop computer sitting on top of a white deskVisual Tags: This is an image with  dining table, laptop, chairTask text: The right image contains exactly one laptop\nBLIP caption:  a snow plowdriving down a snow covered roadVisual Tags: This is an image with  person,  train\nGround truth: FalseFlan-T5 response: FalseGPT-3 response: False\nNLVR2test1-40-0-img0.png test1-40-0-img1.png\nBLIP caption:  a snow plowis parked in the snowVisual Tags: truck\nTask text: The left and right image contains a total of two yellow plows\nBLIP caption:  a dark room with a bed and a windowVisual Tags: This is an image with  bed,  bow window, indoor\nGround truth: NegativeFlan-T5 response: NegativeGPT-3 response: Negative\nMVSA (331.jpg)\nBLIP caption:  a small pig sitting on a blue picnic tableVisual Tags: This is an image with  dog, umbrella\nGround truth: PositiveFlan-T5 response: PositiveGPT-3 response: Positive\nMVSA (2.jpg)\nTweet text: … #drugs #alternative #blithe #depressed …\nTweet text: … #pig #happybday #wow #lovely #cut …\nBLIP caption:  a man riding a wave on top of a surfboardVisual Tags: This is an image with 1 person,  surfboard,  waveGround truth: [wetsuit, suit, wet suit, trunk]Flan-T5 response: wetsuitGPT-3 response: wetsuit\nOK-VQA \nQuestion: What is the person in the photo wearing?\n(COCO_val2014_000000319073.jpg)\nBLIP caption: a person with a teddy bear in a backpackVisual Tags: 3 people,  book, backpack, teddy bear, potted plantGround truth: ['stuffed animal', 'teddy bear’]Flan-T5 response: teddy bearGPT-3 response: stuffed animal\nOK-VQA \nQuestion: What toy is this?\n(COCO_val2014_000000357586.jpg)\nFigure 2: Qualitative examples for each evaluated dataset. The samples include the ground truth and responses\ngenerated via prompting Flan-T5 and GPT-3 models. Samples for the MAMI and HF datasets are prompted\nincluding the overlay text embedded in an image, which is excluded in the graphic for spacing reasons.\ntions of multiple image classification predictions)\nto show the the comparison against captions gener-\nated by the BLIP model.\n5.5 Discussion\nWe have presented experimental results that\nprompted large language models for five vision-\nlanguage tasks. The prompting was made possible\nby representing the visual content in any task us-\ning methods such as image captioning or applying\nmultiple image classification methods and combin-\ning their outputted high-ranking class labels. We\nhave shown that such a method can achieve im-\npressive results by presenting only two or three\nsamples from a respective dataset compared with\nfine-tuned models on the entire train splits. It is\nworth mentioning that the gap between prompted\nmodels and fine-tuned models (in some evaluated\ndatasets) is still there (margins of 10-20 points).\nOne way of closing the such gap is by making\nthe image-as-text representation methods achieve\nperformance closer to how humans verbalise the\nvisual content. Our paper essentially aims to high-\nlight that given such image-as-text representations,\nwhich are only partial representations given the\nimage models’ capabilities, whether language mod-\nels can be used for multimodal tasks by relying\non their (imperfect) general reasoning mechanisms\nsuch as chain-of-thought (Wei et al., 2022b). An-\nother way to achieve better performance (and close\nthe gap with task-specific fine-tuned models) is to\ntrain vision-language models that are capable of\nin-context learning via prompting (Alayrac et al.,\n2022).\nWe have also shown that the choice of in-context\nsamples impacts the results. Using samples similar\nto the evaluated one (adaptive method) yields better\nperformance than choosing them randomly.\nAnother critical observation to mention here is\nthat different language models obtained the best\nresults on various tasks. Overall, GPT-3 is the best-\nranking model for three datasets. Among open-\nsource models, T0pp and Flan-T5 obtained the\nhighest overall performance. Even though their\nperformance was not the highest for many tasks,\nit is still possible to achieve comparable results or\neven the best ones in some cases. For the MVSA\ndataset, T0pp achieved the best performance even\nin a zero-shot setting. Thus, the language model’s\nchoice makes a difference in applying such models\nfor any downstream tasks.\n6 Conclusion\nIn conclusion, our study has demonstrated the suit-\nability and effectiveness of using large language\nmodels via prompting on vision-language tasks.\n14203\nOur approach relies on verbalising the visual con-\ntent employing image captioning and classification\nmodels and prompts the language models along\nwith the textual content. We have also shown that\nthe choice of in-context samples and the method\nof verbalising the visual content impact the results.\nOur experimental evaluation suggests that this ap-\nproach can achieve impressive results by present-\ning only a few samples from a dataset compared to\nmodels that are fine-tuned on entire train splits of\nthe evaluated datasets. Furthermore, our study has\nalso highlighted the importance of considering the\nchoice of language models when applying them to\nsuch downstream tasks. We have demonstrated that\ndifferent models perform better on various tasks,\nwith GPT-3 achieving the highest overall perfor-\nmance across three datasets and open-source mod-\nels T0pp and Flan-T5 achieving the best overall\nperformance among them. Even though the per-\nformance of these models may not have been the\nbest across all evaluated tasks, they still have the\npotential to be used in such cases and even achieve\ncomparable results. For instance, T0pp yielded the\nbest performance on the MVSA dataset, even in\na zero-shot setting. Thus, the choice of language\nmodels is crucial for achieving optimal results in\nvision-language tasks.\nLimitations\nLimitations on the evaluated language models\nand obtained results: The presented model archi-\ntecture utilises various pre-trained language or im-\nage models. The main limitation of the experimen-\ntal evaluation is not using other language models.\nDue to the limited budget and processing power,\nwe have included the language models that have\nbeen shown to perform better based on the previ-\nous work. Another limitation is that we excluded\nlanguage models that exceeded the 80 GB memory\nof an NVIDIA A100 GPU. Our experiments led to\ndifferent results for the GPT-3 compared to Yang\net al. (2022). It can be explained by using different\nmethods for converting images to textual represen-\ntations and slightly varying prompting structures.\nLimitations on the used image models: The\nlimitation concerning the pre-trained image models\nis that we selected a handful of methods based on\ntheir success for related tasks. Including other pre-\ntrained models would increase the parameter space\nand thus increase the budget for the study.\nLimitations on the selected datasets: All\ndatasets are multimodal tasks where the underlying\ntext is only in English. The choice of the dataset\nis related to the fact that there are limited multi-\nmodal datasets in other languages. The evaluation\nmetric for the OK-VQA dataset requires the output\nto match exactly one of the expected answers. It\ncounts as a wrong answer even if a slight change in\nthe answer or another paraphrase is given as an out-\nput, e.g. “race” vs “racing”. We applied the same\nevaluation criterion and left this improvement as\nfuture work.\nEthics Statement\nThere might arise ethical issues as part of this\nwork. The used pre-trained language models inherit\nparticular biases as part of their learning process,\nwhich might affect the generated outputs. Another\nconcern is the use of pre-trained image models for\ncaptioning or classification. The generated out-\nputs from these models might predict certain visual\nconcepts and thus leading to inaccurate text descrip-\ntions for the given images are generated. Another\nconcern directly concerns using large language\nmodels as few-shot models. Such models have\ndemonstrated high performance for many down-\nstream tasks. However, the interpretation of the\nmodel predictions is still ongoing research.\nAcknowledgements\nWe want to thank the anonymous reviewers\nfor their comments. This work was supported\nby BMBF (German Federal Ministry of Re-\nsearch), project “COCOBOTS” (01IS21102A)\nand Deutsche Forschungsgemeinschaft (DFG, Ger-\nman Research Foundation) – 423217434 (“RECO-\nLAGE”) grant.\nReferences\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-\ntoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katie Millican, Malcolm Reynolds,\nRoman Ring, Eliza Rutherford, Serkan Cabi, Tengda\nHan, Zhitao Gong, Sina Samangooei, Marianne\nMonteiro, Jacob Menick, Sebastian Borgeaud, An-\ndrew Brock, Aida Nematzadeh, Sahand Sharifzadeh,\nMikolaj Binkowski, Ricardo Barreira, Oriol Vinyals,\nAndrew Zisserman, and Karen Simonyan. 2022.\nFlamingo: a visual language model for few-shot\nlearning. CoRR, abs/2204.14198.\nTom B. Brown, Benjamin Mann, Nick Ryder, and et al.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\n14204\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nNicolas Carion, Francisco Massa, Gabriel Synnaeve,\nNicolas Usunier, Alexander Kirillov, and Sergey\nZagoruyko. 2020. End-to-end object detection with\ntransformers. In Computer Vision - ECCV 2020 -\n16th European Conference, Glasgow, UK, August\n23-28, 2020, Proceedings, Part I, volume 12346 of\nLecture Notes in Computer Science, pages 213–229.\nSpringer.\nGullal S. Cheema, Sherzod Hakimov, Eric Müller-\nBudack, and Ralph Ewerth. 2021. A fair and com-\nprehensive comparison of multimodal tweet senti-\nment analysis methods. In MMPT@ICMR2021: Pro-\nceedings of the 2021 Workshop on Multi-Modal Pre-\nTraining for Multimedia Understanding, Taipei, Tai-\nwan, August 21, 2021, pages 37–45. ACM.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El\nKholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. 2020. UNITER: universal image-text\nrepresentation learning. In Computer Vision - ECCV\n2020 - 16th European Conference, Glasgow, UK, Au-\ngust 23-28, 2020, Proceedings, Part XXX , volume\n12375 of Lecture Notes in Computer Science, pages\n104–120. Springer.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, and et al. 2022. Scaling instruction-finetuned\nlanguage models. CoRR, abs/2210.11416.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers),\npages 4171–4186. Association for Computational\nLinguistics.\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong\nWu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and\nZhifang Sui. 2023. A survey for in-context learning.\nCoRR, abs/2301.00234.\nAlexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, Jakob\nUszkoreit, and Neil Houlsby. 2021. An image\nis worth 16x16 words: Transformers for image\nrecognition at scale. In 9th International Conference\non Learning Representations, ICLR 2021, Virtual\nEvent, Austria, May 3-7, 2021. OpenReview.net.\nElisabetta Fersini, Francesca Gasparini, Giulia Rizzi,\nAurora Saibene, Berta Chulvi, Paolo Rosso, Alyssa\nLees, and Jeffrey Sorensen. 2022. SemEval-2022\nTask 5: Multimedia automatic misogyny identifica-\ntion. In Proceedings of the 16th International Work-\nshop on Semantic Evaluation (SemEval-2022). Asso-\nciation for Computational Linguistics.\nIan J Goodfellow, Dumitru Erhan, Pierre Luc Carrier,\nAaron Courville, Mehdi Mirza, Ben Hamner, Will\nCukierski, Yichuan Tang, David Thaler, Dong-Hyun\nLee, et al. 2015. Challenges in representation learn-\ning: A report on three machine learning contests.\nNeural Networks, 64:59–63.\nLiangke Gui, Borui Wang, Qiuyuan Huang, Alexan-\nder Hauptmann, Yonatan Bisk, and Jianfeng Gao.\n2022. KAT: A knowledge augmented transformer\nfor vision-and-language. In Proceedings of the\n2022 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 956–968, Seattle,\nUnited States. Association for Computational Lin-\nguistics.\nDouwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj\nGoswami, Amanpreet Singh, Pratik Ringshia, and\nDavide Testuggine. 2020. The hateful memes chal-\nlenge: Detecting hate speech in multimodal memes.\nIn Advances in Neural Information Processing Sys-\ntems 33: Annual Conference on Neural Information\nProcessing Systems 2020, NeurIPS 2020, December\n6-12, 2020, virtual.\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven C. H.\nHoi. 2022a. BLIP: bootstrapping language-image\npre-training for unified vision-language understand-\ning and generation. In International Conference on\nMachine Learning, ICML 2022, 17-23 July 2022, Bal-\ntimore, Maryland, USA, volume 162 of Proceedings\nof Machine Learning Research, pages 12888–12900.\nPMLR.\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui\nHsieh, and Kai-Wei Chang. 2019. Visualbert: A sim-\nple and performant baseline for vision and language.\nCoRR, abs/1908.03557.\nShaobo Li, Xiaoguang Li, Lifeng Shang, Zhenhua Dong,\nChengjie Sun, Bingquan Liu, Zhenzhou Ji, Xin Jiang,\nand Qun Liu. 2022b. How pre-trained language mod-\nels capture factual knowledge? a causal-inspired\nanalysis. In Findings of the Association for Com-\nputational Linguistics: ACL 2022, pages 1720–1732,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris\nTsipras, and et al. 2022. Holistic evaluation of lan-\nguage models. CoRR, abs/2211.09110.\nKenneth Marino, Mohammad Rastegari, Ali Farhadi,\nand Roozbeh Mottaghi. 2019. OK-VQA: A visual\nquestion answering benchmark requiring external\nknowledge. In IEEE Conference on Computer Vision\nand Pattern Recognition, CVPR 2019, Long Beach,\nCA, USA, June 16-20, 2019, pages 3195–3204. Com-\nputer Vision Foundation / IEEE.\n14205\nJack Merullo, Louis Castricato, Carsten Eickhoff, and\nEllie Pavlick. 2023. Linearly mapping from image to\ntext space. In The Eleventh International Conference\non Learning Representations.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. 2022. Rethinking the role of demonstra-\ntions: What makes in-context learning work? CoRR,\nabs/2202.12837.\nTeng Niu, Shiai Zhu, Lei Pang, and Abdulmotaleb El-\nSaddik. 2016. Sentiment analysis on multi-view so-\ncial data. In MultiMedia Modeling - 22nd Interna-\ntional Conference, MMM 2016, Miami, FL, USA,\nJanuary 4-6, 2016, Proceedings, Part II, volume 9517\nof Lecture Notes in Computer Science, pages 15–27.\nSpringer.\nNLP Connect. 2022. vit-gpt2-image-captioning (revi-\nsion 0e334c7).\nAriadna Quattoni and Antonio Torralba. 2009. Rec-\nognizing indoor scenes. In 2009 IEEE Computer\nSociety Conference on Computer Vision and Pattern\nRecognition (CVPR 2009), 20-25 June 2009, Miami,\nFlorida, USA, pages 413–420. IEEE Computer Soci-\nety.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\ning transferable visual models from natural language\nsupervision. In Proceedings of the 38th International\nConference on Machine Learning, ICML 2021, 18-24\nJuly 2021, Virtual Event, volume 139 of Proceedings\nof Machine Learning Research , pages 8748–8763.\nPMLR.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, and et al. 2019. Lan-\nguage models are unsupervised multitask learners.\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert:\nSentence embeddings using siamese bert-networks.\nIn Proceedings of the 2019 Conference on Empiri-\ncal Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural\nLanguage Processing, EMNLP-IJCNLP 2019, Hong\nKong, China, November 3-7, 2019, pages 3980–3990.\nAssociation for Computational Linguistics.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen\nBach, Lintang Sutawika, and et al. 2022. Multi-\ntask prompted training enables zero-shot task gener-\nalization. In The Tenth International Conference on\nLearning Representations, ICLR 2022, Virtual Event,\nApril 25-29, 2022. OpenReview.net.\nTeven Le Scao, Angela Fan, Christopher Akiki, Ellie\nPavlick, Suzana Ilic, and et al. 2022. BLOOM: A\n176b-parameter open-access multilingual language\nmodel. CoRR, abs/2211.05100.\nAlane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang,\nHuajun Bai, and Yoav Artzi. 2019. A corpus for\nreasoning about natural language grounded in pho-\ntographs. In Proceedings of the 57th Conference of\nthe Association for Computational Linguistics, ACL\n2019, Florence, Italy, July 28- August 2, 2019, Vol-\nume 1: Long Papers, pages 6418–6428. Association\nfor Computational Linguistics.\nAlon Talmor, Yanai Elazar, Yoav Goldberg, and\nJonathan Berant. 2020. olmpics - on what language\nmodel pre-training captures. Trans. Assoc. Comput.\nLinguistics, 8:743–758.\nMaria Tsimpoukelli, Jacob Menick, Serkan Cabi,\nS. M. Ali Eslami, Oriol Vinyals, and Felix Hill. 2021.\nMultimodal few-shot learning with frozen language\nmodels. In Advances in Neural Information Pro-\ncessing Systems 34: Annual Conference on Neural\nInformation Processing Systems 2021, NeurIPS 2021,\nDecember 6-14, 2021, virtual, pages 200–212.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA, pages 5998–6008.\nDenny Vrandecic and Markus Krötzsch. 2014. Wiki-\ndata: a free collaborative knowledgebase. Commun.\nACM, 57(10):78–85.\nBen Wang and Aran Komatsuzaki. 2021. GPT-J-\n6B: A 6 Billion Parameter Autoregressive Lan-\nguage Model. https://github.com/kingoflolz/\nmesh-transformer-jax.\nPeng Wang, An Yang, Rui Men, Junyang Lin, Shuai\nBai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren\nZhou, and Hongxia Yang. 2022a. OFA: unifying ar-\nchitectures, tasks, and modalities through a simple\nsequence-to-sequence learning framework. In Inter-\nnational Conference on Machine Learning, ICML\n2022, 17-23 July 2022, Baltimore, Maryland, USA,\nvolume 162 of Proceedings of Machine Learning\nResearch, pages 23318–23340. PMLR.\nZhenhailong Wang, Manling Li, Ruochen Xu, Lu-\nowei Zhou, Jie Lei, Xudong Lin, Shuohang Wang,\nZiyi Yang, Chenguang Zhu, Derek Hoiem, Shih-Fu\nChang, Mohit Bansal, and Heng Ji. 2022b. Language\nmodels with image descriptors are strong few-shot\nvideo-language learners. In Advances in Neural In-\nformation Processing Systems.\nJason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M. Dai, and Quoc V . Le. 2022a. Finetuned\n14206\nlanguage models are zero-shot learners. In The Tenth\nInternational Conference on Learning Representa-\ntions, ICLR 2022, Virtual Event, April 25-29, 2022.\nOpenReview.net.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022b.\nChain of thought prompting elicits reasoning in large\nlanguage models. In Advances in Neural Information\nProcessing Systems.\nBichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan,\nPeizhao Zhang, Masayoshi Tomizuka, Kurt Keutzer,\nand Peter Vajda. 2020. Visual transformers: Token-\nbased image representation and processing for com-\nputer vision. CoRR, abs/2006.03677.\nJialin Wu, Jiasen Lu, Ashish Sabharwal, and Roozbeh\nMottaghi. 2022. Multi-modal answer validation for\nknowledge-based VQA. In Thirty-Sixth AAAI Con-\nference on Artificial Intelligence, AAAI 2022, Thirty-\nFourth Conference on Innovative Applications of Ar-\ntificial Intelligence, IAAI 2022, The Twelveth Sym-\nposium on Educational Advances in Artificial In-\ntelligence, EAAI 2022 Virtual Event, February 22\n- March 1, 2022, pages 2712–2721. AAAI Press.\nZhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu,\nYumao Lu, Zicheng Liu, and Lijuan Wang. 2022. An\nempirical study of GPT-3 for few-shot knowledge-\nbased VQA. In Thirty-Sixth AAAI Conference on\nArtificial Intelligence, AAAI 2022, Thirty-Fourth Con-\nference on Innovative Applications of Artificial In-\ntelligence, IAAI 2022, The Twelveth Symposium on\nEducational Advances in Artificial Intelligence, EAAI\n2022 Virtual Event, February 22 - March 1, 2022 ,\npages 3081–3089. AAAI Press.\nAndy Zeng, Adrian Wong, Stefan Welker, Krzysztof\nChoromanski, Federico Tombari, Aveek Purohit,\nMichael S. Ryoo, Vikas Sindhwani, Johnny Lee, Vin-\ncent Vanhoucke, and Pete Florence. 2022. Socratic\nmodels: Composing zero-shot multimodal reasoning\nwith language. CoRR, abs/2204.00598.\nJing Zhang and Yujin Wang. 2022. SRCB at SemEval-\n2022 task 5: Pretraining based image to text late\nsequential fusion system for multimodal misogy-\nnous meme identification. In Proceedings of the\n16th International Workshop on Semantic Evalua-\ntion (SemEval-2022), pages 585–596, Seattle, United\nStates. Association for Computational Linguistics.\nKaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, and\nYu Qiao. 2016. Joint face detection and alignment\nusing multitask cascaded convolutional networks.\nIEEE Signal Processing Letters, 23(10):1499–1503.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, and et al. 2022. Opt: Open\npre-trained transformer language models.\nBolei Zhou, Àgata Lapedriza, Aditya Khosla, Aude\nOliva, and Antonio Torralba. 2018. Places: A 10\nmillion image database for scene recognition. IEEE\nTrans. Pattern Anal. Mach. Intell., 40(6):1452–1464.\nKaiyang Zhou, Jingkang Yang, Chen Change Loy, and\nZiwei Liu. 2022a. Conditional prompt learning for\nvision-language models. In IEEE/CVF Conference\non Computer Vision and Pattern Recognition, CVPR\n2022, New Orleans, LA, USA, June 18-24, 2022 ,\npages 16795–16804. IEEE.\nKaiyang Zhou, Jingkang Yang, Chen Change Loy, and\nZiwei Liu. 2022b. Learning to prompt for vision-\nlanguage models. Int. J. Comput. Vis., 130(9):2337–\n2348.\nA Prompt Structures\nWe provided the prompt structures for all datasets\nin Figure 3. Given a sample from a respective\ndataset, the prompt structures are initialised to cre-\nate a prompt text. Each prompt text includes a\ntask description followed by task-specific labels\n(only for classification tasks). In the middle of\nthe prompt text are the selected in-context samples.\nThe bottom part includes the evaluation sample,\nwhich is represented by only its input text, image-\nas-text representation (image context), and the task-\nspecific question. The prompted language model\nis expected to generate the next text sequence that\nstarts after the last occurrence of the word Answer.\n14207\nMAMI HF MVSAGiven an image context and text, answer the given question. The task is to classify whether the context has positive, negative or neutral sentiment.Possible labels:1.Positive2.Negative3.NeutralText: <sample_text>Image context: <sample_image_as_text_repr>Question: What is the label of the given image context and text (positive, negative, neutral)?Answer: <sample_answer>Text: <sample_text>Image context: <sample_image_as_text_repr>Question: What is the label of the given image context and text (positive, negative, neutral)?Answer: <sample_answer>…Text: <sample_text>Image context: <sample_image_as_text_repr>Question: What is the label of the given image context and text (positive, negative, neutral)?Answer: \nGiven an image context and text, answer the given question. The task is to classify whether the content is hateful towards any group of people.Possible labels:1.Hateful 2.NotText: <sample_text>Image context: <sample_image_as_text_repr>Question: What is the label of the given image context and text (hateful, not)?Answer: <sample_answer>Text: <sample_text>Image context: <sample_image_as_text_repr>Question: What is the label of the given image context and text (hateful, not)?Answer: <sample_answer>…Text: <sample_text>Image context: <sample_image_as_text_repr>Question: What is the label of the given image context and text (hateful, not)?Answer: \nGiven an image context and text, answer the given question. The task is to classify whether the context is hateful towards women (includes misogynistic tone).Possible labels:1.Hateful 2.NotText: <sample_text>Image context: <sample_image_as_text_repr>Question: What is the label of the given image context and text (hateful, not)?Answer: <sample_answer>Text: <sample_text>Image context: <sample_image_as_text_repr>Question: What is the label of the given image context and text (hateful, not)?Answer: <sample_answer>…Text: <sample_text>Image context: <sample_image_as_text_repr>Question: What is the label of the given image context and text (hateful, not)?Answer: \n(a) Prompt structures for MAMI, HF and MVSA datasets\nGiven an image context for two images (left and right) and text, answer the given question. The task is to classify whether the given text is true or false.Possible labels:1.True 2.FalseText: <sample_text>Left Image context: <sample_image_as_text_repr>Right Image context: <sample_image_as_text_repr>Question: What is the label of the given left image context, right image context, and text (true, false)?Answer: <sample_answer>…Text: <sample_text>Left Image context: <sample_image_as_text_repr>Right Image context: <sample_image_as_text_repr>Question: What is the label of the given left image context, right image context, and text (true, false)?Answer: \nNLVR2Answer the following question given an image context.Image context: <sample_image_as_text_repr>Question: <sample_question>Answer: <sample_answer>Image context: <sample_image_as_text_repr>Question: <sample_question>Answer: <sample_answer>\n…Image context: <sample_image_as_text_repr>Question: <sample_question>Answer: \nOK-VQA\n(b) Prompt structures for NLVR2, OK-VQA datasets\nFigure 3: Prompt structures for each evaluated dataset. Each prompt structure includes a task description, which\nalso includes possible labels, selected n in-context samples, and the evaluated sample. The prompted language\nmodels are expected to generated the next text sequence that starts after the last occurrence of the word Answer.\n14208\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\n7\n□\u0013 A2. Did you discuss any potential risks of your work?\n8\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\n1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\n4\n□\u0013 B1. Did you cite the creators of artifacts you used?\n4\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNo response.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNo response.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNo response.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNo response.\n□ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nNo response.\nC □\u0013 Did you run computational experiments?\n5\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\n4\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n14209\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\n4\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\n5\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\n4\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n14210",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8169155716896057
    },
    {
      "name": "Interpretability",
      "score": 0.7796275615692139
    },
    {
      "name": "Generality",
      "score": 0.7436269521713257
    },
    {
      "name": "Language model",
      "score": 0.7002351880073547
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5903730988502502
    },
    {
      "name": "Natural language processing",
      "score": 0.5306531190872192
    },
    {
      "name": "Tracing",
      "score": 0.42503392696380615
    },
    {
      "name": "Human–computer interaction",
      "score": 0.32097840309143066
    },
    {
      "name": "Programming language",
      "score": 0.13222700357437134
    },
    {
      "name": "Psychotherapist",
      "score": 0.0
    },
    {
      "name": "Psychology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I176453806",
      "name": "University of Potsdam",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I33256026",
      "name": "German Research Centre for Artificial Intelligence",
      "country": "DE"
    }
  ]
}