{
  "title": "Transformer Hawkes Process",
  "url": "https://openalex.org/W3008314773",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4224668709",
      "name": "Zuo, Simiao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2371030016",
      "name": "Jiang, Haoming",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2747922486",
      "name": "Li, Zichong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1949403856",
      "name": "Zhao Tuo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2752578629",
      "name": "Zha, Hongyuan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2958975943",
    "https://openalex.org/W1525783482",
    "https://openalex.org/W2069849731",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3123773610",
    "https://openalex.org/W2890009182",
    "https://openalex.org/W1985093013",
    "https://openalex.org/W2789541106",
    "https://openalex.org/W1978710835",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W3122471732",
    "https://openalex.org/W2509830164",
    "https://openalex.org/W2755088640",
    "https://openalex.org/W2107878631",
    "https://openalex.org/W2962817261",
    "https://openalex.org/W2963022764",
    "https://openalex.org/W2605191235",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W1971947347",
    "https://openalex.org/W2587019100",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2101645017",
    "https://openalex.org/W2799087070",
    "https://openalex.org/W1924770834",
    "https://openalex.org/W2964265128",
    "https://openalex.org/W2949382160",
    "https://openalex.org/W2963148415",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W2963561234",
    "https://openalex.org/W2087822214",
    "https://openalex.org/W2149490995",
    "https://openalex.org/W2962702810",
    "https://openalex.org/W2950470207"
  ],
  "abstract": "Modern data acquisition routinely produce massive amounts of event sequence data in various domains, such as social media, healthcare, and financial markets. These data often exhibit complicated short-term and long-term temporal dependencies. However, most of the existing recurrent neural network based point process models fail to capture such dependencies, and yield unreliable prediction performance. To address this issue, we propose a Transformer Hawkes Process (THP) model, which leverages the self-attention mechanism to capture long-term dependencies and meanwhile enjoys computational efficiency. Numerical experiments on various datasets show that THP outperforms existing models in terms of both likelihood and event prediction accuracy by a notable margin. Moreover, THP is quite general and can incorporate additional structural knowledge. We provide a concrete example, where THP achieves improved prediction performance for learning multiple point processes when incorporating their relational information.",
  "full_text": "Transformer Hawkes Process∗\nSimiao Zuo, Haoming Jiang, Zichong Li, Tuo Zhao and Hongyuan Zha †\nFebruary 23, 2021\nAbstract\nModern data acquisition routinely produce massive amounts of event sequence data in\nvarious domains, such as social media, healthcare, and ﬁnancial markets. These data often ex-\nhibit complicated short-term and long-term temporal dependencies. However, most of the ex-\nisting recurrent neural network based point process models fail to capture such dependencies,\nand yield unreliable prediction performance. To address this issue, we propose a Transformer\nHawkes Process (THP) model, which leverages the self-attention mechanism to capture long-\nterm dependencies and meanwhile enjoys computational e ﬃciency. Numerical experiments on\nvarious datasets show that THP outperforms existing models in terms of both likelihood and\nevent prediction accuracy by a notable margin. Moreover, THP is quite general and can incorpo-\nrate additional structural knowledge. We provide a concrete example, where THP achieves im-\nproved prediction performance for learning multiple point processes when incorporating their\nrelational information.\n1 Introduction\nEvent sequence data are naturally observed in our daily life. Through social media such as Twit-\nter and Facebook, we share our experiences and respond to other users’ information (Yang et al.,\n2011). In these websites, each user has a sequence of events such as tweets and interactions. Hun-\ndreds of millions of users generate large amounts of tweets, which are essentially sequences of\nevents at diﬀerent time stamps. Besides social media, event data also exist in domains like ﬁnan-\ncial transactions (Bacry et al., 2015) and personalized healthcare (Wang et al., 2018). For example,\nin electronic medical records, tests and diagnoses of each patient can be treated as a sequence of\nevents. Unlike other sequential data such as time series, event sequences tend to be asynchronous\n(Ross et al., 1996), which means time intervals between events are just as important as the order\nof them to describe their dynamics. Also, depending on speciﬁc application requirements, event\ndata show sophisticated dependencies on their history.\n∗Published as a conference paper in ICML 2020.\n†Zuo, Jiang and Zhao are a ﬃliated with Georgia Tech, Li is a ﬃliated with University of Science and Technology\nof China, and Zha is a ﬃliated with Shenzhen Research Institute of Big Data, The Chinese University of Hong Kong\n(currently on leave from Georgia Tech). Correspondence to simiaozuo@gatech.edu, tourzhao@gatech.edu, zhahy@\ncuhk.edu.cn.\n1\narXiv:2002.09291v5  [cs.LG]  21 Feb 2021\nPoint process is a powerful tool for modeling sequences of discrete events in continuous time,\nand the technique has been widely applied. Hawkes process (Hawkes, 1971; Isham and Westcott,\n1979) and Poisson point process are traditionally used as examples of point processes. However,\nthe simpliﬁed assumptions of the complicated dynamics of point processes limit the models’ prac-\nticality. As an example, Hawkes process states that all past events should have positive inﬂuences\non the occurrence of current events. However, a user on Twitter may initiate tweets on di ﬀerent\ntopics, and these events should be considered as unrelated instead of mutually-excitepd.\nTo alleviate the over-simpliﬁcations, likelihood-free methods (Xiao et al., 2017a; Li et al., 2018)\nand non-parametric models like kernel methods and splines (Vere-Jones et al., 1990) have been\nproposed, but the increasing complexity and quantity of collected data crave for more powerful\nmodels. With the development of neural networks, in particular deep neural networks, focuses\nhave been placed on incorporating these ﬂexible models into classical point processes. Because\nof the sequential nature of event steams, existing methods rely heavily on Recurrent Neural Net-\nworks (RNNs). Neural networks are known for their ability to capture complicated high-level\nfeatures, in particular, RNNs have the representation power to model the dynamics of event se-\nquence data. In previous works, either vanilla RNN (Du et al., 2016) or its variants (Mei and\nEisner, 2017; Xiao et al., 2017b) have been used and signiﬁcant progress in terms of likelihood\nand event prediction have been achieved.\nHowever, there are two signiﬁcant drawbacks with RNN-based models. First, recurrent neural\nnetworks, even those equipped with forget gates, such as Long Short-Term Memory (Hochreiter\nand Schmidhuber, 1997) and Gated Recurrent Units (Chung et al., 2014), are unlikely to capture\nlong-term dependencies. In ﬁnancial transactions, short-term e ﬀects such as policy changes are\nimportant for modeling buy-sell behaviors of stocks. On the other hand, because of the delays in\nasset returns, stock transactions and prices often exhibit long-term dependencies on their history.\nAs another example, in medical domains, at times we are interested in examining short-term\ndependencies on symptoms such as fever and cough for acute diseases like pneumonia. But for\ncertain types of chronic diseases such as diabetes, long-term dependencies on disease diagnoses\nand medications are more critical. Desirable models should be able to capture these long-term\ndependencies. Yet with recurrent structures, interactions between two events located far in the\ntemporal domain are always weak (Hochreiter et al., 2001), even though in reality they may be\nhighly correlated. The reason is that the probability of keeping information in a state that is far\naway from the current state decreases exponentially with distance.\nThe second drawback is trainability of recurrent neural networks. Training deep RNNs (in-\ncluding LSTMs) is notoriously diﬃcult because of gradient explosion and gradient vanishing (Pas-\ncanu et al., 2013). In practice, single-layer and two-layer RNNs are mostly used, and they may not\nsuccessfully model sophisticated dependencies among data (Bengio et al., 1994). Additionally, in-\nputs are fed into the recurrent models sequentially, which means future states must be processed\nafter the current state, rendering it impossible to process all the events in parallel. This limits\nRNNs’ ability to scale to large problems.\nRecently, convolutional neural network variants that are tailored for analyzing sequential data\n(Oord et al., 2016; Gehring et al., 2017; Yin et al., 2017) have been proposed to better capture long-\n2\nFigure 1: Illustration of dependency computation between the last event (the red triangle) and its\nhistory (the blue circles). RNN-based NHP models dependencies through recursion. THP directly\nand adaptively models the event’s dependencies on its history. Convolution-based models enforce\nstatic dependency patterns.\nterm eﬀects. However, these models enforce many unnecessary dependencies. This particular\ndownside plus the increased computational burdens deem these models insuﬃcient.\nTo address the above concerns, we propose the Transformer Hawkes Process (THP) model that\nis able to capture both short-term and long-term dependencies whilst enjoying computational eﬃ-\nciency. Even though the Transformer (Vaswani et al., 2017) is widely adopted in natural language\nprocessing, it has rarely been used in other applications. We remark that such an architecture is\nnot readily applicable to event sequences that are deﬁned in a continuous-time domain. To the\nbest of our knowledge, our proposed THP is the ﬁrst of this type in point process literature.\nBuilding blocks of THP are the self-attention modules (Bahdanau et al., 2014). These modules\ndirectly model dependencies among events by assigning attention scores. A large score between\ntwo events implies a strong dependency, and a small score implies a weak one. In this way, the\nmodules are able to adaptively select events that are at any temporal distance from the current\nevent. Therefore, THP has the ability to capture both short-term and long-term dependencies.\nFigure 1 demonstrates dependency computation of diﬀerent models.\nThe non-recurrent structure of THP facilitates eﬃcient training of multi-layer models. Transformer-\nbased architectures can be as deep as dozens of layers (Devlin et al., 2018; Radford et al., 2019),\nwhere deeper layers capture higher order dependencies. The ability to capture such dependencies\ncreates models that are more powerful than RNNs, which are often shallow. Also, THP allows full\nparallelism when calculating dependencies across all events, i.e., the computation between any\ntwo event pairs is independent with each other. This yields a model presenting strong eﬃciency.\nOur proposed model is quite general, and can incorporate additional structural knowledge\nto learn more complicated event sequence data, such as multiple point processes over a graph.\n3\nIn social networks, each user has her own sequence of events, like tweets and comments. Se-\nquences among users can be related, for example, a tweet from a user may trigger retweets from\nher followers. We can use graphs to model these follower-followee relationships (Zhou et al., 2013;\nFarajtabar et al., 2017), where each vertex corresponds to a speciﬁc user and each edge represents\nconnections between the two associated users. We propose an extension to THP that integrates\nthese relational graphs (Borgatti et al., 2009; Linderman and Adams, 2014) into the self-attention\nmodule via a similarity metric among users. Such a metric can be learned by our proposed graph\nregularization.\nWe experiment THP on ﬁve datasets to evaluate both validation likelihood and event predic-\ntion accuracy. Our THP model exhibits superior performance to RNN-based models in all these\nexperiments. We further test our structured-THP on two additional datasets, where the model\nachieves improved prediction performance for learning multiple point processes when incorpo-\nrating their relational information. Our code is available at https://github.com/SimiaoZuo/\nTransformer-Hawkes-Process.\nThe rest of this paper is organized as follows: Section 2 introduces the background; Section 3\nintroduces our proposed transformer Hawkes process model; Section 4 demonstrates an extension\nof our model to multiple event sequences on graphs; Section 5 presents numerical experiments on\nvarious real datasets; Section 6 draws a brief conclusion.\n2 Background\nWe brieﬂy review Hawkes Process (Hawkes, 1971), Neural Hawkes Process (Mei and Eisner, 2017),\nand Transformer (Vaswani et al., 2017) in this section.\n•Hawkes Processis a doubly stochastic point process, whose intensity function is deﬁned as\nλ(t) = µ+\n∑\nj:tj<t\nψ(t−tj). (1)\nHere µis the base intensity andψ(·) is a pre-speciﬁed decaying function, i.e., exponential function\nand power-law function. Intuitively, Eq. 1 means that each of the past events has a positive con-\ntribution to occurrence of the current event, and this inﬂuence decreases through time. However,\na major limitation of this formulation is the simpliﬁcation that history events can never inhibit\noccurrence of future events, which is unrealistic in complex real-life scenarios.\n•Neural Hawkes Processgeneralizes the classical Hawkes process by parameterizing its intensity\nfunction with recurrent neural networks. Speciﬁcally,\nλ(t) =\nK∑\nk=1\nλk(t) =\nK∑\nk=1\nfk\n(\nw⊤\nkh(t)\n)\n, t ∈(0,T], where fk(x) = βklog\n(\n1 + exp\n( x\nβk\n))\n.\nPrediction is then P[kt = k] = λk(t)/λ(t). Here, λ(t) is the intensity function, K is the number of\nevent types, and h(t)s are the hidden states of the event sequence, obtained by a continuous-time\nLSTM (CLSTM) module. CLSTM is an interpolated version of the standard LSTM, and it allows\nus to generate outputs in a continuous-time domain. Also, fk(·) is the softplus function with\n4\nparameter βk that guarantees a positive intensity. One downside of the neural Hawkes process is\nthat intrinsic weaknesses of RNNs are inherited, namely the model is unable to capture long-term\ndependencies and is diﬃcult to train.\n•Transformer is an attention-based model that has been broadly applied in tasks such as ma-\nchine translation (Devlin et al., 2018) and language modeling (Radford et al., 2019). Despite its\nsuccess in natural language processing, it has rarely been used in other areas. We remark that\nthe Transformer architecture is not directly applicable to model point processes. In particular,\ntime intervals between any two events can be arbitrary in event streams, while in natural lan-\nguages, words are observed on regularly spaced time intervals. Therefore, we need to generalize\nthe architecture to a continuous-time domain.\n3 Model\nWe introduce our proposed Transformer Hawkes Process. Suppose we are given an event sequence\nS= {(tj,kj)}L\nj=1 of Levents, where each event has type kj ∈{1,2,...,K }, with a total number of K\ntypes. Then each pair (tj,kj) corresponds to an event of type kj occurs at time tj.\n3.1 Transformer Hawkes Process\nThe key ingredient of our proposed THP model is the self-attention module. Diﬀerent from RNNs,\nthe attention mechanism discards recurrent structures. However, our model still needs to be\naware of the temporal information of inputs, i.e., time stamps. Therefore, analogous to the orig-\ninal positional encoding method (Vaswani et al., 2017), we propose to use a temporal encoding\nFigure 2: Architecture of the Transformer Hawkes Process. Each event sequence Sis fed through\nembedding layers and N multi-head self-attention modules. Outputs of the THP are hidden rep-\nresentations of events in S, with history information encoded.\n5\nprocedure, deﬁned by\n[z(tj)]i =\n\ncos\n(\ntj/10000\ni−1\nM\n)\n, if iis odd,\nsin\n(\ntj/10000\ni\nM\n)\n, if iis even.\n(2)\nEq. 2 uses trigonometric functions to deﬁne a temporal encoding for each time stamp, i.e.,\nfor each tj, we deterministically computes z(tj) ∈RM, where M is the dimension of encoding.\nOther temporal encoding methods can also be applied, such as the relative position representation\nmodel (Shaw et al., 2018), where two temporal encoding matrices are learned instead of pre-\ndeﬁned.\nBesides temporal encoding, we train an embedding matrix U ∈RM×K for the event types,\nwhere the k-th column of U is a M-dimensional embedding for event typek. For any event of type\nkj, let kj be its one-hot encoding (a K-dimensional vector with all 0s except for the kj-th index,\nwhich has value 1), then its embedding is Ukj. Notice that for any event and its corresponding\ntime stamp (tj,kj), the temporal encoding z(tj) and the event embedding Ukj both reside in RM.\nEmbedding of the event sequence S= {(tj,kj)}L\nj=1 is then speciﬁed by\nX =\n(\nUY + Z\n)⊤\n, (3)\nwhere Y = [k1,k2,..., kL] ∈RK×L is the collection of event type one-hot encodings, and Z = [z(t1),\nz(t2),..., z(tL)] ∈RM×L is the concatenation of event time encodings. Notice that X ∈RL×M and\neach row of X corresponds to the embedding of a speciﬁc event in the sequence.\nAfter the initial encoding and embedding layers, we passX through the self-attention module.\nSpeciﬁcally, we compute the attention output S by\nS = Softmax\n(QK⊤\n√MK\n)\nV, where Q = XWQ, K = XWK, V = XWV. (4)\nHere Q, K, and V are the query, key, and value matrices obtained by diﬀerent transformations of\nX, and WQ,WK ∈RM×MK,WV ∈RM×MV are weights for the linear transformations, respectively.\nIn practice using multi-head self-attention to increase model ﬂexibility is more beneﬁcial for data\nﬁtting. To facilitate this, di ﬀerent attention outputs S1,S2,..., SH are computed using di ﬀerent\nsets of weights {WQ\nh,WK\nh,WV\nh }H\nh=1. The ﬁnal attention output for the event sequence is then\nS =\n[\nS1,S2,..., SH\n]\nWO,\nwhere WO ∈RHMV×M is an aggregation matrix.\nWe highlight that the self-attention module is able to directly select events whose occur-\nrence time is at any distance from the current time. The j-th column of the attention weights\nSoftmax(QK⊤/√MK) signiﬁes event tj’s extent of dependency on its history. In contrast, RNN-\nbased models encode history information sequentially via hidden representations of the events,\ni.e., the state of tj depends on that of tj−1, which in turn depends on tj−2, etc. Should any of these\nencodings be weak, i.e., the RNN fails to learn suﬃcient relevant information for event tk, hidden\nrepresentations of any event tj where j≥kwill be inferior.\n6\nThe attention output S is then fed through a position-wise feed-forward neural network, gen-\nerating hidden representations h(t) of the input event sequence:\nH = ReLU\n(\nSWFC\n1 + b1\n)\nWFC\n2 + b2, h(tj) = H(j,:). (5)\nHere WFC\n1 ∈RM×MH, WFC\n2 ∈RMH×M, b1 ∈RMH, and b2 ∈RM are parameters of the neural network,\nand WFC\n2 has identical columns. The resulting matrix H ∈RL×M contains hidden representations\nof all the events in the input sequence, where each row corresponds to a particular event.\nTo avoid “peeking into the future”, our attention algorithm is equipped with masks. That is,\nwhen computing the attention output S(j,:) (the j-th row of S), we mask all the future positions,\ni.e., we setQ(j,j+1),Q(j,j+1),..., Q(j,L) to inf. This will avoid the softmax function from assigning\ndependency to events in the future.\nIn practice we stack multiple self-attention modules together, and inputs are passed through\neach of these modules sequentially. In this way our model is able to capture high level depen-\ndencies. We remark that stacking RNN/LSTM is not plausible because gradient explosion and\ngradient vanishing will render the stacked model di ﬃcult to train. Figure 2 illustrates the archi-\ntecture of THP .\n3.2 Continuous Time Conditional Intensity\nDynamics of temporal point processes are described by a continuous conditional intensity func-\ntion. Eq. 5 only generates hidden representations for discrete time stamps, and the associated\nintensity is also discrete. Therefore an interpolated continuous time intensity function is in need.\nLet λ(t|Ht) be the conditional intensity function for our model, where Ht = {(tj,kj) : tj <t}is\nthe history up to time t. We deﬁne di ﬀerent intensity functions for diﬀerent event types, i.e., for\nevery k∈{1,2,...,K }, deﬁne λk(t|Ht) as the conditional intensity function for events of typek. The\nconditional intensity function for the entire event sequence is deﬁned by\nλ(t|Ht) =\nK∑\nk=1\nλk(t|Ht),\nwhere each of the type-speciﬁc intensity takes the form\nλk(t|Ht) = fk\n(\nαk\nt−tj\ntj\n    \ncurrent\n+w⊤\nkh(tj)\n      \nhistory\n+ bk\n\nbase\n)\n. (6)\nIn Eq. 6, time is deﬁned on interval t∈[tj,tj+1), and fk(x) = βklog\n(\n1 + exp(x/βk)\n)\nis the softplus\nfunction with “softness” parameter βk. The reason for choosing this particular function is two-\nfold: ﬁrst, the softplus function ensures that the intensity is positive; second, “softness” of the\nsoftplus function guarantees stable computation and avoids dramatic changes in the intensity.\nNow we explain each term in Eq. 6 in detail:\n•The “current” inﬂuence is an interpolation between two observed time stamps tj and tj+1, and\nαk modulates importance of the interpolation. When t= tj, i.e., a new observation comes in, this\n7\ninﬂuence is 0. When t →tj+1, the conditional intensity function is no longer continuous. As a\nmatter of fact, Eq. 6 is continuous everywhere except for the observed events {(tj,kj)}. However,\nthese “jumps” in intensity is a non-factor when computing likelihood.\n•The “history” term contains two parts: a vector wk that transforms the hidden states of the THP\nmodel into a scalar, and the hidden states h(t) (Sec. 3.1) themselves that encode past events up to\ntime t.\n•The “base” intensity represents probability of occurrence of events without considering history\ninformation.\nWith our proposed conditional intensity function, next time stamp prediction and next event\ntype prediction is given by1\np(t|Ht) = λ(t|Ht)exp\n(\n−\n∫ t\ntj\nλ(τ|Hτ)dτ\n)\n,\nˆtj+1 =\n∫ ∞\ntj\nt·p(t|Ht)dt, and ˆkj+1 = argmax\nk\nλk(tj+1|Hj+1)\nλ(tj+1|Hj+1) .\n(7)\n3.3 Training\nFor any sequence Sover an observation interval [ t1,tL], given its conditional intensity function\nλ(t|Ht), the log-likelihood is\nℓ(S) =\nL∑\nj=1\nlogλ(tj|Hj)\n                        \nevent log-likelihood\n−\n∫ tL\nt1\nλ(t|Ht)dt\n                      \nnon-event log-likelihood\n. (8)\nModel parameters are learned by maximizing the log-likelihood across all sequences. Con-\ncretely, suppose we haveN sequences S1,S2,..., SN , then the goal is to ﬁnd parameters that solve\nmax\nN∑\ni=1\nℓ(Si),\nwhere ℓ(Si) is the log-likelihood of event sequenceSi. This optimization problem can be eﬃciently\nsolved by stochastic gradient type algorithms like ADAM (Kingma and Ba, 2014). Additionally,\ntechniques that help stabilizing training such as layer normalization (Ba et al., 2016) and residual\nconnection (He et al., 2016) are also applied.\nIn Eq. 8, one challenge is to compute Λ =\n∫tL\nt1\nλ(t|Ht)dt, the non-event log-likelihood. Because\nof the softplus function, there is no closed-form computation for this integral, and a proper ap-\nproximation is needed.\nThe ﬁrst approach to approximate the non-event log-likelihood is by using Monte Carlo inte-\ngration (Robert and Casella, 2013):\nˆΛMC =\nL∑\nj=2\n(tj −tj−1)\n( 1\nN\nN∑\ni=1\nλ(ui)\n)\n, ∇ˆΛMC =\nL∑\nj=2\n(tj −tj−1)\n( 1\nN\nN∑\ni=1\n∇λ(ui)\n)\n. (9)\n1Without causing any confusion, denote Htj as Hj.\n8\nFigure 3: Illustration of event sequences on a graph. Sequences on vertices are aligned temporally\nto form a long sequence, and relational information among events are shown in arrows. Notice\nthat only the structural information of the last event (the blue circle) and the third to the last event\n(the purple diamond) are shown. Like before, events cannot attend to future.\nHere ui ∼Unif(tj−1,tj) is sampled from a uniform distribution with support [ tj−1,tj]. Notice that\nλ(ui) and ∇λ(ui) can be calculated by feed-forward and back-propagation through the model,\nrespectively. Moreover, Eq. 9 yields an unbiased estimation to the integral, i.e.,E[ˆΛMC] = Λ.\nThe second approach is to apply numerical integration methods, which are faster because of\nthe elimination of sampling. For example, the trapezoidal rule (Stoer and Bulirsch, 2013) states\nthat\nˆΛNU =\nL∑\nj=2\ntj −tj−1\n2\n(\nλ(tj|Hj) +λ(tj−1|Hj−1)\n)\n(10)\nqualiﬁes as an approximation to Λ. Other higher order methods such as the Simpson’s rule (Stoer\nand Bulirsch, 2013) can also be applied. Even though approximations build upon numerical in-\ntegration algorithms are biased, in practice they are a ﬀordable. This is because the conditional\nintensity (Eq. 6) uses softplus as its activation function, which is highly smooth and ensures bias\nintroduced by linear interpolations (Eq. 10) between consecutive events are small.\n4 Structured Transformer Hawkes Process\nTHP is quite general and can incorporate additional structural knowledge. We consider multiple\npoint processes, where any two of them can be related. Such relationships are often described by a\ngraph G= (V,E), where Vis the vertex set, and each vertex is associated with a point process. Also,\nEis the edge set, where each edge signiﬁes relational information between the corresponding two\nvertices. Figure 3 illustrates event sequences on a graph.\nThe graph encodes relationships among vertices, and further indicates potential interactions.\nWe propose to model all the point processes with a single THP, and the heterogeneity of the\nvertices’ point processes is handled by a vertex embedding approach.\nSuppose we have an event sequence S= {(tj,kj,vj)}L\nj=1, where tj and kj are time stamps and\nevent types as before. Further, vj ∈{1,2,..., |V|}is an indicator to which vertex the event belongs.\nIn addition to the event embedding and the temporal encoding (Eq. 3), we introduce a vertex\nembedding matrix E ∈RM×|V|, where the j-th column of E denotes the M-dimensional embedding\n9\nfor vertex j. Let vj be the one-hot encoding of vj, then embedding of Sis speciﬁed by\nX =\n(\nUY + EV + Z\n)⊤\n,\nwhere V = [v1,v2,..., vL] ∈R|V|×L is the concatenation of vertices, and other terms are deﬁned in\nEq. 3.\nThe graph attention output is deﬁned by\nS = Softmax\n(QK⊤\n√MK\n+ A\n)\nVvalue, where A = (EV)⊤Ω(EV), (11)\nwhere Q, K, and Vvalue are the same2 as in Eq. 4. Matrix A ∈RL×L is the vertex similarity matrix,\nwhere each entry Aij signiﬁes the similarity between two vertices vi and vj, and Ω ∈RM×M is a\nmetric to be learned. To extend the graph self-attention module to a multi-head setting, we use\ndiﬀerent metric matrices {Ωj}H\nj=1 for diﬀerent heads.\nWe remark that unlike RNN-based shallow models, in structured-THP, multiple multi-head\nself-attention modules can be stacked (Figure 2) to learn high level representations, a feature\nthat enables learning of complicated similarities among vertices. Moreover, the vertex similarity\nmatrix enables modeling of even more complicated structured data, such as sequences on dynam-\nically evolving graphs.\nWith the incorporation of relational information, we need to modify the conditional intensity\nfunction accordingly. As an extension to Eq. 6, where each type of events has its own intensity, we\ndeﬁne a diﬀerent intensity function for each event type and each vertex. Speciﬁcally,\nλ(t|Ht) =\nK∑\nk=1\n|V|∑\nv=1\nλk,v(t|Ht), t ∈[tj,tj+1), where λk,v(t|Ht) = fk,v\n(\nαk,v\nt−tj\ntj\n+ w⊤\nk,vh(t) +bk,v\n)\n.\nModel parameters are learned by maximizing the log-likelihood (Eq. 8) across all sequences.\nConcretely, suppose we haveN sequences S1,S2,..., SN , then parameters are obtained by solving\nmax\nN∑\ni=1\nℓ(Si) +µLgraph(V,Ω),\nwhere µis a hyper-parameter and\nLgraph(V,Ω) =\n|V|∑\nk=1\nk∑\nj=1\n−log\n(\n1 + exp(VjΩVk)\n)\n+ 1{(vj,vk) ∈E}\n(\nVjΩVk\n)\n.\nHere Lgraph(V,Ω) is a regularization term that encourages VjΩVk to be large when there exists\nan edge between vj and vk. Which means if two vertices are connected in graph G, then the\nregularizer will promote attention between them, and vice versa.\nNotice that in the simplest case, A in Eq. 11 can be some transformation of the adjacency\nmatrix, i.e., Aij = 1 if (vi,vj) ∈E, and 0 otherwise. However, we believe that this constraint is too\nstrict, i.e., some connected vertices may not behave similarly. Therefore, we treat the graph as a\nguide and introduce a regularization term that encouragesA to be similar to the adjacency matrix,\nbut not enforce it. In this way, our model is more ﬂexible.\n2We use Vvalue to denote the value matrix instead of V, which denotes the vertices.\n10\n5 Experiments\nWe compare THP against existing models: Recurrent Marked Temporal Point Process (RMTPP, Du\net al. (2016)), Neural Hawkes Process (NHP, Mei and Eisner (2017)), Time Series Event Sequence\n(TSES, Xiao et al. (2017b)), and Self-attentive Hawkes Processes (SAHP, Zhang et al. (2019))3. We\nevaluate the models by per-event log-likelihood (in nats) and event prediction accuracy on held-\nout test sets. Details about training are deferred to the appendix.\n5.1 Datasets\nWe adopt several datasets to evaluate the models. Table 1 summarizes statistics of the datasets.\n•Retweets (Zhao et al., 2015): The Retweets dataset contains sequences of tweets, where each\nsequence contains an origin tweet (i.e., some user initiates a tweet), and some follow-up tweets.\nWe record the time and the user tag of each tweet. Further, users are grouped into three categories\nbased on the number of their followers: “small”, “medium”, and “large”.\n•MemeTrack (Leskovec and Krevl, 2014): This dataset contains mentions of 42 thousand diﬀerent\nmemes spanning ten months. We collect data on over 1.5 million documents (blogs, web articles,\netc.) from over 5000 websites. Each sequence in this dataset is the life-cycle of a particular meme,\nwhere each event (usage of meme) is associated with a time stamp and a website id.\n•Financial Transactions (Du et al., 2016): This ﬁnancial dataset contains transaction records of\na stock in one day. We record the time (in milliseconds) and the action that was taken in each\ntransaction. The dataset is a single long sequence with only two types of events: “buy” and “sell”.\nThe event sequence is further partitioned by time stamps.\n•Electrical Medical Records (Johnson et al., 2016): MIMIC-II medical dataset collects patients’\nvisit to a hospital’s ICU in a seven-year period. We treat the visits of each patient as a separate\nsequence, where each event in the sequence contains a time stamp and a diagnosis.\n•StackOverﬂow (Leskovec and Krevl, 2014): StackOverﬂow is a question-answering website. The\nwebsite rewards users with badges to promote engagement in the community, and the same badge\ncan be rewarded multiple times to the same user. We collect data in a two-year period, and we\ntreat each user’s reward history as a sequence. Each event in the sequence signiﬁes receipt of a\nparticular medal.\n•911-Calls4: The 911-Calls dataset contains emergency phone call records. Calling time, location\nof the caller, and nature of the emergency are logged for each record. We consider three types\nof emergencies: EMS, ﬁre, and tra ﬃc. We treat location of callers (given by zipcodes) as vertices\non a relational information graph. Zipcodes are ranked based on the number of recorded calls,\nand only the top 75 zipcodes are kept. An undirected edge exists between two vertices if their\nzipcodes are within 10 of each other.\n3This is a concurrent work that also employs the Transformer architecture, and we only include results reported in\ntheir paper.\n4The dataset is available on www.kaggle.com/mchirico/montcoalert.\n11\nTable 1: Datasets statistics. From left to right columns: name of the dataset, number of event\ntypes, number of events in the dataset, and average length per sequence.\nDataset K # events Avg. length\nRetweets 3 2,173,533 109\nMemeTrack 5000 123,639 3\nFinancial 2 414,800 2074\nMIMIC-II 75 2,419 4\nStackOverﬂow 22 480,413 72\n911-Calls 3 290,293 403\nEarthquake 2 256,932 500\n•Earthquake5: This dataset contains time and location of earthquakes in China in an eight-year\nperiod. We partition the records into two categories: “small” and “large”. A relational information\ngraph is built based on geographical locations of the earthquakes, i.e., each province is a vertex\nand earthquakes are sequences on the vertices. Two vertices are connected if their associated\nprovinces are neighbors.\n5.2 Training Details\nTo facilitate comparison with previous works, all the datasets are used by Du et al. (2016) and Mei\nand Eisner (2017), except for 911-Calls and Earthquake. Details about data pre-processing and\ntrain-dev-test split, as well as downloadable links, can be found in the aforementioned papers.\nFor the 911-Calls dataset, we exclude zipcodes (and the associated events) whose occurrences are\nscarce, i.e., we only keep zipcodes that have the top 75 frequent occurrences. The dataset contains\n141 types of events, and we cluster them into three categories, namely EMS, ﬁre, and tra ﬃc.\nWe do not exclude any events in the Earthquake dataset. Earthquakes are partitioned into two\ncategories, “small” and “large”, where small earthquakes are the ones whose Richter scale is equal\nto or lower than 1.0. We perform this partition because of the imbalance in data, i.e., most of the\nrecorded earthquakes are on small magnitude. Models are trained on 911-Calls and Earthquake\nwith diﬀerent number of training events. In each experiment, we equally divide the events that\nare not in the training set in half to construct the development set and the test set.\nThere are three sets of hyper-parameters that we use, and they are summarized in Table 2.\nBesides layer normalization and residual connection, we also employ the dropout technique to\navoid overﬁtting. Table 3 contains the speciﬁc parameters that are applied for the training of\neach dataset. In the table, from left to right columns specify: name of the dataset, the set of\napplied hyper-parameters, batch size, learning rate, and solver for the integral approximation (MC\nstands for Monte Carlo integration, and NU stands for numerical integration with the trapezoidal\nrule), respectively. In the 911-Calls and the Earthquakes datasets, we also employ the graph\nregularization method, and the corresponding regularization parameter is set to be 0.01 in all the\nexperiments. We use a single NVIDIA RTX graphics card to run all the experiments.\n5The dataset is provided by China Earthquake Data Center. (http://data.earthquake.cn)\n12\nTable 2: Sets of hyper-parameters used in training.\nParameters # head # layer M M K = MV MH dropout\nSet 1 3 3 64 16 256 0 .1\nSet 2 6 6 128 64 2048 0 .1\nSet 3 4 4 512 512 1024 0 .1\nTable 3: Hyper-parameters used for training each dataset.\nDataset Retweets Meme Financial MIMIC StackOverﬂow 911 Earthquake\nSet 1 1 2 1 3 2 3\nBatch 16 128 1 1 4 1 1\nLR 1 ×10−2 1 ×10−3 1 ×10−4 1 ×10−4 1 ×10−4 1 ×10−5 1 ×10−5\nSolver MC MC NU NU NU MC MC\nFigure 4: Training curves of NHP and THP ﬁtted on Retweets (left ﬁgure) and MemeTrack (right\nﬁgure). Validation log-likelihood is (NHP vs. THP): -5.60 vs. -2.04 for Reweets, and -6.23 vs. 0.68\nfor MemeTrack.\n5.3 Likelihood Comparison\nWe ﬁt THP and NHP on Retweets and MemeTrack. From Figure 4, we can see that THP outper-\nforms NHP during the entire training process by large margins on both of the datasets. The reason\nis because of the complicated nature of social media data, and RNN-based models such as NHP\nare not powerful enough to model the dynamics.\nIn the Retweets dataset, we often observe time gaps between two consecutive retweets become\nlarger, and this dynamic can be successfully modeled by temporal encoding. Also, unlike RNN-\nbased models, our model is able to capture long-term dependencies that exist in long sequences.\nIn the MemeTrack dataset, we have extremely short sequences, i.e., average sequence length is\n3. Even though the data only exhibit short-term dependencies, we still need to model latent\nproperties of memes such as topics and targeted users. We build deep THP models to capture\nthese high-level features, and we remark that constructing deep NHP is not plausible because of\nthe diﬃculty in training.\nTable 4 summarizes results on other datasets. Note that TSES is likelihood-free. Our THP\n13\nTable 4: Log-likelihood comparison.\nModel Retweets MemeTrack Financial MIMIC-II StackOverﬂow\nRMTPP -5.99 -6.04 -3.89 -1.35 -2.60\nNHP -5.60 -6.23 -3.60 -1.38 -2.55\nSAHP -4.56 — — -0.52 -1.86\nTHP -2.04 0.68 -1.11 0.820 0.042\nFigure 5: Visualization of attention patterns of di ﬀerent attention heads in diﬀerent layers. Pixel\n(i,j) in each ﬁgure signiﬁes the attention weight of event (tj,kj) attending to event (ti,ki). Attention\nheads in the upper two ﬁgures are from the ﬁrst layer, while they are from the last layer in the\nlower two ﬁgures.\nmodel ﬁts the data well and outperforms all the baselines in all the experiments.\nFigure 5 visualizes attention patterns of THP . We can see that each attention head employs\na diﬀerent pattern to capture dependencies. Moreover, while attention heads in the ﬁrst layer\ntend to focus on individual events, the attention patterns in the last layer are more uniformly\ndistributed. This is because features in deeper layers are already transformed by attention heads\nin shallow layers.\n5.4 Event Prediction Comparison\nFor point processes, event prediction is just as important as data ﬁtting. Eq. 7 enables us to predict\nfuture events. In practice, however, adding additional prediction layers on top of the THP model\nyields better performance. Speciﬁcally, given the hidden representation h(tj) for event (tj,kj), the\nnext event type and time predictions are as follows.\n•The next event type prediction is\nˆkj+1 = argmax\nk\nˆpj+1(k), where ˆpj+1 = Softmax\n(\nWtypeh(tj)\n)\n,\nwhere Wtype ∈RK×M is the predictor parameter, and ˆpj(k) is the k-th element of ˆpj ∈RK.\n•The next event time prediction is\nˆtj+1 = Wtimeh(tj),\nwhere Wtime ∈R1×M is the predictor parameter.\nTo learn the predictor parameters, the loss function is equipped with a cross-entropy term for\nevent type prediction and a mean square error term for event time prediction. Concretely, for\n14\nTable 5: Event type prediction accuracy com-\nparison. Here FIN is the Financial Transactions\ndataset, and SO is the StackOverﬂow dataset.\nModel FIN MIMIC-II SO\nRMTPP 61.95 81.2 45.9\nNHP 62.20 83.2 46.3\nTSES 62.17 83.0 46.2\nTHP 62.64 85.3 47.0\nTable 6: Event time prediction RMSE compar-\nison. Here FIN is the Financial Transactions\ndataset, and SO is the StackOverﬂow dataset.\nModel FIN MIMIC-II SO\nRMTPP 1.56 6.12 9.78\nNHP 1.56 6.13 9.83\nTSES 1.50 4.70 8.00\nSAHP — 3.89 5.57\nTHP 0.93 0.82 4.99\nFigure 6: Prediction error rates of THP, NHP, and RMTPP . Based on a same train-dev-test splitting\nratio, each dataset is sampled ﬁve times to produce di ﬀerent train, development and test sets.\nError bars are generated according to these experiments.\nan event sequence S= {(tj,kj)}L\nj=1, let k1,k2,..., kL be the ground-truth one-hot encodings for the\nevent types, we deﬁne\nLtype(S) =\nL∑\nj=2\n−k⊤\nj log(ˆpj), and Ltime(S) =\nL∑\nj=2\n(tj −ˆtj)2.\nNotice that we do not predict the ﬁrst event. Then, given event sequences {Si}N\ni=1, we seek to solve\nmin\nN∑\ni=1\n−ℓ(Si) +Ltype(Si) +Ltime(Si),\nwhere ℓ(Si) is the log-likelihood (Eq. 8) of Si.\nTo evaluate model performance, we predict every held-out event ( tj,kj) given its history Hj,\ni.e., for a test sequence of length L, we make L−1 predictions. We evaluate event type prediction\nby accuracy and event time prediction by Root Mean Square Error (RMSE). Table 5 and Table 6\nsummarize experiment results. We can see that THP outperforms the baselines in all these tasks.\nThe datasets we adopted vary signiﬁcantly in average sequence length, i.e., the average length\n15\nFigure 7: Log-likelihood and prediction accuracy of NHP, THP, THP with full attention (THP-\nF), and structured-THP (THP-S) ﬁtted on the 911-Calls (upper two ﬁgures) and the Earthquake\n(lower two ﬁgures) datasets. Models are trained using diﬀerent number of events.\nin Financial Transactions is 2074 while it is only 4 in MIMIC-II. In all the three datasets, THP\nimproves upon RNN-based models by a notable margin. The results demonstrate that THP is able\nto capture both short-term and long-term dependencies better than existing methods.\nFigure 6 illustrates run-to-run variance of THP, NHP, and RMTPP . The error bars are wide\nbecause of how the data are split. Held-out test sets are constructed by randomly sampling some\nevents from the entire dataset. That is, at times “important” events are sampled out, which will\nyield unsatisfactory model performance. Our results are better than all the baselines in all the\nindividual experiments.\n5.5 THP vs. Structured-THP\nNow we demonstrate by incorporating relational information, THP achieves improved perfor-\nmance.\nBaseline models are constructed as following: for each vertex on a relational graph G, there\nexists a point process that consists of time and type of events. These event sequences are learned\nseparately by both THP and NHP, i.e., we do not allow information sharing among vertices in\nthese models.\nTo integrate Ginto THP, we consider two approaches. The ﬁrst approach is by allowing full\nattention, i.e., information from one vertex can be shared with all the other vertices. The second\napproach is by using the neighborhood graph, which is constructed based on spatial proximity. In\n16\nthis approach, a speciﬁc vertex can only share information with its neighbors. We ﬁt a structured-\nTHP to both of the cases.\nFigure 7 summarizes experimental results. We can see that THP is comparable or better than\nNHP in both validation likelihood and event prediction, which further demonstrates that THP can\nmodel complicated dynamics better than RNN-based models. Notice that THP-F, the structured-\nTHP with full attention, yields a much better likelihood than the baseline models, which means\nrelational information sharing can help the models in capturing latent dynamics. However, unlike\nlikelihood, THP-F does not show consistent improvements in event prediction. This is because\nwhen the number of training events is small, the model cannot build a su ﬃcient information-\nsharing heuristic. Also, the performance drop when the number of training events is large is\ndue to the inhomogeneity of data. This demonstrates that the full attention scheme results in\nundesirable dependencies on which the attention heads focus. THP-S successfully resolves this\nissue by eliminating such dependencies from the attention heads’ span based on spatial closeness\nof vertices. In this way, THP-S further improves upon THP-F, especially in event prediction tasks.\n5.6 Ablation Study\nWe perform ablation study on Retweets and MemeTrack, and we evaluate models by validation\nlog-likelihood. We inspect variants of THP by removing the self-attention and the temporal en-\ncoding mechanisms. Moreover, we test the e ﬀect of temporal encoding on NHP . Table 7 summa-\nrizes experimental results. As shown, both the self-attention module and the temporal encoding\ncontribute to model performance.\nWe examine the models’ sensitivity to the number of parameters on the Retweets dataset. As\nshown in Table 8, our model is not sensitive to its number of parameters. Without the recurrent\nstructure, Transformer-based models often have large number of parameters, but our THP model\ncan outperform RNN-based models with fewer parameters. In all the experiments, using a small\nmodel (about 100-200k parameters) will suﬃce. In comparison, NHP has about 1000k and TSES\nhas about 2000k parameters to achieve the best performance, which are much larger than THP .\nWe also include run-time comparison in Table 8. We conclude that THP is eﬃcient in both model\nsize and training speed.\nTable 7: Log-likelihood of variants of NHP\nand THP ﬁtted on Retweets and Meme-\nTrack. TE stands for temporal encoding\n(Eq. 2), and PE stands for positional encod-\ning (Vaswani et al., 2017).\nModel Retweets MemeTrack\nNHP −5.60 −6.23\nNHP + TE −2.50 −1.64\nAtten −5.29 −5.09\nAtten + PE −5.25 −4.70\nAtten + TE −2.03 0.68\nTable 8: Sensitivity to the number of param-\neters and run-time comparison. Speedup is\nthe speed of THP against NHP .\n# parameters Log-likelihood SpeedupTHP NHP\n100k −2.090 −6.019 ×1.985\n200k −2.072 −5.595 ×2.564\n500k −2.058 −5.590 ×2.224\n1000k −2.060 −5.614 ×1.778\n17\n6 Conclusion\nIn this paper we present Transformer Hawkes Process, a framework for analyzing event streams.\nEvent sequence data are common in our daily life, and they exhibit sophisticated short-term and\nlong-term dependencies. Our proposed model utilizes the self-attention mechanism to capture\nboth of these dependencies, and meanwhile enjoys computational e ﬃciency. Moreover, THP is\nquite general and can integrate structural knowledge into the model. This facilitates analyzing\nmore complicated data, such as event sequences on graphs. Experiments on various real-world\ndatasets demonstrate that THP achieves state-of-the-art performance in terms of both likelihood\nand event prediction accuracy.\nReferences\nBa, J. L. , Kiros, J. R. and Hinton, G. E. (2016). Layer normalization. arXiv preprint\narXiv:1607.06450.\nBacry, E., Mastromatteo, I. and Muzy, J.-F .(2015). Hawkes processes in ﬁnance. Market Mi-\ncrostructure and Liquidity, 1 1550005.\nBahdanau, D., Cho, K. and Bengio, Y. (2014). Neural machine translation by jointly learning to\nalign and translate. arXiv preprint arXiv:1409.0473.\nBengio, Y., Simard, P .and Frasconi, P .(1994). Learning long-term dependencies with gradient\ndescent is diﬃcult. IEEE transactions on neural networks, 5 157–166.\nBorgatti, S. P ., Mehra, A., Brass, D. J. and Labianca, G. (2009). Network analysis in the social\nsciences. science, 323 892–895.\nChung, J., Gulcehre, C., Cho, K. and Bengio, Y. (2014). Empirical evaluation of gated recurrent\nneural networks on sequence modeling. arXiv preprint arXiv:1412.3555.\nDevlin, J., Chang, M.-W., Lee, K. and Toutanova, K. (2018). Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805.\nDu, N., Dai, H., Trivedi, R., Upadhyay, U., Gomez-Rodriguez, M. and Song, L. (2016). Recurrent\nmarked temporal point processes: Embedding event history to vector. InProceedings of the 22nd\nACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM.\nFarajtabar, M., Wang, Y., Gomez-Rodriguez, M., Li, S., Zha, H. and Song, L. (2017). Coevolve:\nA joint point process model for information di ﬀusion and network evolution. The Journal of\nMachine Learning Research, 18 1305–1353.\nGehring, J., Auli, M., Grangier, D., Yarats, D. and Dauphin, Y. N. (2017). Convolutional se-\nquence to sequence learning. In Proceedings of the 34th International Conference on Machine\nLearning-Volume 70. JMLR. org.\n18\nHawkes, A. G. (1971). Spectra of some self-exciting and mutually exciting point processes.\nBiometrika, 58 83–90.\nHe, K., Zhang, X., Ren, S. and Sun, J. (2016). Deep residual learning for image recognition. In\nProceedings of the IEEE conference on computer vision and pattern recognition.\nHochreiter, S., Bengio, Y., Frasconi, P ., Schmidhuber, J. et al. (2001). Gradient ﬂow in recurrent\nnets: the diﬃculty of learning long-term dependencies.\nHochreiter, S. and Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9\n1735–1780.\nIsham, V. and Westcott, M. (1979). A self-correcting point process. Stochastic Processes and Their\nApplications, 8 335–347.\nJohnson, A. E. , Pollard, T. J. , Shen, L. , Li-wei, H. L. , Feng, M. , Ghassemi, M. , Moody, B. ,\nSzolovits, P ., Celi, L. A. and Mark, R. G. (2016). Mimic-iii, a freely accessible critical care\ndatabase. Scientiﬁc data, 3 160035.\nKingma, D. P . and Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nLeskovec, J. and Krevl, A. (2014). Snap datasets: Stanford large network dataset collection.\nLi, S., Xiao, S., Zhu, S., Du, N., Xie, Y. and Song, L. (2018). Learning temporal point processes via\nreinforcement learning. In Advances in neural information processing systems.\nLinderman, S. and Adams, R. (2014). Discovering latent network structure in point process data.\nIn International Conference on Machine Learning.\nMei, H. and Eisner, J. M. (2017). The neural hawkes process: A neurally self-modulating multi-\nvariate point process. In Advances in Neural Information Processing Systems.\nOord, A. v. d., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner, N.,\nSenior, A. and Kavukcuoglu, K. (2016). Wavenet: A generative model for raw audio. arXiv\npreprint arXiv:1609.03499.\nPascanu, R., Mikolov, T. and Bengio, Y. (2013). On the di ﬃculty of training recurrent neural\nnetworks. In International conference on machine learning.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D. and Sutskever, I. (2019). Language models\nare unsupervised multitask learners. OpenAI Blog, 1.\nRobert, C. and Casella, G. (2013). Monte Carlo statistical methods. Springer Science & Business\nMedia.\nRoss, S. M., Kelly, J. J., Sullivan, R. J., Perry, W. J., Mercer, D., Davis, R. M., Washburn, T. D.,\nSager, E. V., Boyce, J. B. and Bristow, V. L.(1996). Stochastic processes, vol. 2. Wiley New York.\n19\nShaw, P ., Uszkoreit, J. and V aswani, A.(2018). Self-attention with relative position representa-\ntions. arXiv preprint arXiv:1803.02155.\nStoer, J. and Bulirsch, R. (2013). Introduction to numerical analysis, vol. 12. Springer Science &\nBusiness Media.\nV aswani, A., Shazeer, N. , Parmar, N., Uszkoreit, J., Jones, L. , Gomez, A. N. , Kaiser, Ł. and\nPolosukhin, I. (2017). Attention is all you need. In Advances in neural information processing\nsystems.\nVere-Jones, D., of Wellington. Institute of Statistics, V. U. and Research, O. (1990). Statisti-\ncal Methods for the Description and Display of Earthquake Catalogues . Technical report (Victoria\nUniversity of Wellington. Institute of Statistics and Operations Research), Victoria University\nof Wellington.\nWang, L., Zhang, W., He, X. and Zha, H. (2018). Supervised reinforcement learning with recur-\nrent neural network for dynamic treatment recommendation. In Proceedings of the 24th ACM\nSIGKDD International Conference on Knowledge Discovery & Data Mining. ACM.\nXiao, S., Farajtabar, M., Ye, X., Yan, J., Song, L. and Zha, H. (2017a). Wasserstein learning of\ndeep generative point process models. In Advances in Neural Information Processing Systems.\nXiao, S. , Yan, J., Yang, X., Zha, H. and Chu, S. M. (2017b). Modeling the intensity function\nof point process via recurrent neural networks. In Thirty-First AAAI Conference on Artiﬁcial\nIntelligence.\nYang, S.-H., Long, B., Smola, A., Sadagopan, N., Zheng, Z. and Zha, H. (2011). Like like alike:\njoint friendship and interest propagation in social networks. In Proceedings of the 20th interna-\ntional conference on World wide web.\nYin, W., Kann, K., Yu, M. and Sch¨utze, H. (2017). Comparative study of cnn and rnn for natural\nlanguage processing. arXiv preprint arXiv:1702.01923.\nZhang, Q., Lipani, A., Kirnap, O. and Yilmaz, E. (2019). Self-attentive hawkes processes. arXiv\npreprint arXiv:1907.07561.\nZhao, Q., Erdogdu, M. A. , He, H. Y., Rajaraman, A. and Leskovec, J. (2015). Seismic: A self-\nexciting point process model for predicting tweet popularity. In Proceedings of the 21th ACM\nSIGKDD International Conference on Knowledge Discovery and Data Mining. ACM.\nZhou, K., Zha, H. and Song, L. (2013). Learning social infectivity in sparse low-rank networks\nusing multi-dimensional hawkes processes. In Artiﬁcial Intelligence and Statistics.\n20",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7921463251113892
    },
    {
      "name": "Transformer",
      "score": 0.6522615551948547
    },
    {
      "name": "Margin (machine learning)",
      "score": 0.6095881462097168
    },
    {
      "name": "Point process",
      "score": 0.5207000374794006
    },
    {
      "name": "Process (computing)",
      "score": 0.5146844983100891
    },
    {
      "name": "Data mining",
      "score": 0.505134642124176
    },
    {
      "name": "Recurrent neural network",
      "score": 0.48944082856178284
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4877927601337433
    },
    {
      "name": "Machine learning",
      "score": 0.44366446137428284
    },
    {
      "name": "Event (particle physics)",
      "score": 0.42133140563964844
    },
    {
      "name": "Term (time)",
      "score": 0.41477638483047485
    },
    {
      "name": "Artificial neural network",
      "score": 0.3824761211872101
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I130701444",
      "name": "Georgia Institute of Technology",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I126520041",
      "name": "University of Science and Technology of China",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210147622",
      "name": "Groupe d'Analyse et de Théorie Economique Lyon St Etienne",
      "country": "FR"
    }
  ],
  "cited_by": 22
}