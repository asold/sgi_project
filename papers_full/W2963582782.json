{
    "title": "Gated Word-Character Recurrent Language Model",
    "url": "https://openalex.org/W2963582782",
    "year": 2016,
    "authors": [
        {
            "id": "https://openalex.org/A2098722266",
            "name": "Yasumasa Miyamoto",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2115080942",
            "name": "Kyunghyun Cho",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2963324947",
        "https://openalex.org/W2384495648",
        "https://openalex.org/W1533861849",
        "https://openalex.org/W1591801644",
        "https://openalex.org/W1632114991",
        "https://openalex.org/W2129250947",
        "https://openalex.org/W2296283641",
        "https://openalex.org/W2962688231",
        "https://openalex.org/W2998704965",
        "https://openalex.org/W2305932946",
        "https://openalex.org/W4285719527",
        "https://openalex.org/W2259472270",
        "https://openalex.org/W2251830157",
        "https://openalex.org/W2399344342",
        "https://openalex.org/W2951559648",
        "https://openalex.org/W179875071",
        "https://openalex.org/W2176796957",
        "https://openalex.org/W2101609803",
        "https://openalex.org/W2079735306",
        "https://openalex.org/W2949563612",
        "https://openalex.org/W1899794420",
        "https://openalex.org/W2339995566",
        "https://openalex.org/W2463033603",
        "https://openalex.org/W2113459411"
    ],
    "abstract": "We introduce a recurrent neural network language model (RNN-LM) with long shortterm memory (LSTM) units that utilizes both character-level and word-level inputs.Our model has a gate that adaptively finds the optimal mixture of the character-level and wordlevel inputs.The gate creates the final vector representation of a word by combining two distinct representations of the word.The character-level inputs are converted into vector representations of words using a bidirectional LSTM.The word-level inputs are projected into another high-dimensional space by a word lookup table.The final vector representations of words are used in the LSTM language model which predicts the next word given all the preceding words.Our model with the gating mechanism effectively utilizes the character-level inputs for rare and out-ofvocabulary words and outperforms word-level language models on several English corpora.",
    "full_text": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1992–1997,\nAustin, Texas, November 1-5, 2016.c⃝2016 Association for Computational Linguistics\nGated Word-Character Recurrent Language Model\nYasumasa Miyamoto\nCenter for Data Science\nNew York University\nyasumasaNmiyamoto@nyuNedu\nKyunghyun Cho\nCourant Institute of\nMathematical Sciences\n& Centre for Data Science\nNew York University\nkyunghyunNcho@nyuNedu\nAbstract\nWe introduce a recurrent neural network lan-\nguage model (RNN-LM) with long short-\nterm memory (LSTM) units that utilizes both\ncharacter-level and word-level inputs. Our\nmodel has a gate that adaptively ﬁnds the op-\ntimal mixture of the character-level and word-\nlevel inputs. The gate creates the ﬁnal vec-\ntor representation of a word by combining\ntwo distinct representations of the word. The\ncharacter-level inputs are converted into vec-\ntor representations of words using a bidirec-\ntional LSTM. The word-level inputs are pro-\njected into another high-dimensional space by\na word lookup table. The ﬁnal vector rep-\nresentations of words are used in the LSTM\nlanguage model which predicts the next word\ngiven all the preceding words. Our model\nwith the gating mechanism effectively utilizes\nthe character-level inputs for rare and out-of-\nvocabulary words and outperforms word-level\nlanguage models on several English corpora.\n1 Introduction\nRecurrent neural networks (RNNs) achieve state-of-\nthe-art performance on fundamental tasks of natural\nlanguage processing (NLP) such as language model-\ning (RNN-LM) (Józefowicz et al., 2016; Zoph et al.,\n2016). RNN-LMs are usually based on the word-\nlevel information or subword-level information such\nas characters (Mikolov et al., 2012), and predictions\nare made at either word level or subword level re-\nspectively.\nIn word-level LMs, the probability distribution\nover the vocabulary conditioned on preceding words\nis computed at the output layer using a softmax func-\ntion. 1 Word-level LMs require a predeﬁned vocab-\nulary size since the computational complexity of a\nsoftmax function grows with respect to the vocab-\nulary size. This closed vocabulary approach tends\nto ignore rare words and typos, as the words do not\nappear in the vocabulary are replaced with an out-\nof-vocabulary (OOV) token. The words appearing\nin vocabulary are indexed and associated with high-\ndimensional vectors. This process is done through a\nword lookup table.\nAlthough this approach brings a high degree of\nfreedom in learning expressions of words, infor-\nmation about morphemes such as preﬁx, root, and\nsufﬁx is lost when the word is converted into an\nindex. Also, word-level language models require\nsome heuristics to differentiate between the OOV\nwords, otherwise it assigns the exactly same vector\nto all the OOV words. These are the major limita-\ntions of word-level LMs.\nIn order to alleviate these issues, we introduce\nan RNN-LM that utilizes both character-level and\nword-level inputs. In particular, our model has a gate\nthat adaptively choose between two distinct ways to\nrepresent each word: a word vector derived from the\ncharacter-level information and a word vector stored\nin the word lookup table. This gate is trained to\nmake this decision based on the input word.\nAccording to the experiments, our model with the\ngate outperforms other models on the Penn Treebank\n(PTB), BBC, and IMDB Movie Review datasets.\nAlso, the trained gating values show that the gating\nmechanism effectively utilizes the character-level\n1softmax function is deﬁned as f(xi) = exp xi∑\nk exp xk\n.\n1992\ninformation when it encounters rare words.\nRelated WorkCharacter-level language models that\nmake word-level prediction have recently been pro-\nposed. Ling et al. (2015a) introduce the compo-\nsitional character-to-word (C2W) model that takes\nas input character-level representation of a word\nand generates vector representation of the word us-\ning a bidirectional LSTM (Graves and Schmidhu-\nber, 2005). Kim et al. (2015) propose a convolu-\ntional neural network (CNN) based character-level\nlanguage model and achieve the state-of-the-art per-\nplexity on the PTB dataset with a signiﬁcantly fewer\nparameters.\nMoreover, word–character hybrid models have\nbeen studied on different NLP tasks. Kang et\nal. (2011) apply a word–character hybrid language\nmodel on Chinese using a neural network language\nmodel (Bengio et al., 2003). Santos and Zadrozny\n(2014) produce high performance part-of-speech\ntaggers using a deep neural network that learns\ncharacter-level representation of words and asso-\nciates them with usual word representations. Bo-\njanowski et al. (2015) investigate RNN models that\npredict characters based on the character and word\nlevel inputs. Luong and Manning (2016) present\nword–character hybrid neural machine translation\nsystems that consult the character-level information\nfor rare words.\n2 Model Description\nThe model architecture of the proposed word–\ncharacter hybrid language model is shown in Fig. 1.\nWord Embedding At each time step t, both the\nword lookup table and a bidirectional LSTM take\nthe same word wt as an input. The word-level input\nis projected into a high-dimensional space by a word\nlookup table E ∈R|V |×d, where |V|is the vocabu-\nlary size and dis the dimension of a word vector:\nxword\nwt = E⊤wwt, (1)\nwhere wwt ∈R|V |is a one-hot vector whose i-th el-\nement is 1, and other elements are 0. The character–\nlevel input is converted into a word vector by us-\ning a bidirectional LSTM. The last hidden states of\nforward and reverse recurrent networks are linearly\nFigure 1: The model architecture of the gated word-character\nrecurrent language model. wt is an input word at t. xword\nwt is\na word vector stored in the word lookup table. xchar\nwt is a word\nvector derived from the character-level input. gwt is a gating\nvalue of a word wt. ˆwt+1 is a prediction made at t.\ncombined:\nxchar\nwt = Wf hf\nwt + Wrhr\nwt + b, (2)\nwhere hf\nwt,hr\nwt ∈ Rd are the last states of\nthe forward and the reverse LSTM respectively.\nWf ,Wr ∈Rd×d and b ∈Rd are trainable param-\neters, and xchar\nwt ∈Rd is the vector representation of\nthe word wt using a character input. The generated\nvectors xword\nwt and xchar\nwt are mixed by a gate gwt as\ngwt = σ\n(\nv⊤\ng xword\nwt + bg\n)\nxwt = (1−gwt) xword\nwt + gwtxchar\nwt ,\n(3)\nwhere vg ∈ Rd is a weight vector, bg ∈ R is\na bias scalar, σ(·) is a sigmoid function. This gate\nvalue is independent of a time step. Even if a word\nappears in different contexts, the same gate value\nis applied. Hashimoto and Tsuruoka (2016) apply\na very similar approach to compositional and non-\ncompositional phrase embeddings and achieve state-\nof-the-art results on compositionality detection and\nverb disambiguation tasks.\nLanguage Modeling The output vector xwt is used\nas an input to a LSTM language model. Since the\nword embedding part is independent from the lan-\nguage modeling part, our model retains the ﬂexibil-\nity to change the architecture of the language model-\ning part. We use the architecture similar to the non-\nregularized LSTM model by Zaremba et al. (2014).\n1993\nPTB BBC IMDB\nModel Validation Test Validation Test Validation Test\nGated Word & Char, adaptive 117.49 113.87 78.56 87.16 71.99 72.29\nGated Word & Char, adaptive (Pre-train) 117.03 112.90 80.37 87.51 71.16 71.49\nGated Word & Char, g= 0.25 119.45 115.55 79.67 88.04 71.81 72.14\nGated Word & Char, g= 0.25 (Pre-train) 117.01 113.52 80.07 87.99 70.60 70.87\nGated Word & Char, g= 0.5 126.01 121.99 89.27 94.91 106.78 107.33\nGated Word & Char, g= 0.5 (Pre-train) 117.54 113.03 82.09 88.61 109.69 110.28\nGated Word & Char, g= 0.75 135.58 135.00 105.54 111.47 115.58 116.02\nGated Word & Char, g= 0.75 (Pre-train) 179.69 172.85 132.96 136.01 106.31 106.86\nWord Only 118.03 115.65 84.47 90.90 72.42 72.75\nCharacter Only 132.45 126.80 88.03 97.71 98.10 98.59\nWord & Character 125.05 121.09 88.77 95.44 77.94 78.29\nWord & Character (Pre-train) 122.31 118.85 84.27 91.24 80.60 81.01\nNon-regularized LSTM (Zaremba, 2014) 120.7 114.5 - - - -\nTable 1:Validation and test perplexities on Penn Treebank (PTB), BBC, IMDB Movie Reviews datasets.\nOne step of LSTM computation corresponds to\nft = σ(Wf xwt + Uf ht−1 + bf )\nit = σ(Wixwt + Uiht−1 + bi)\n˜ ct = tanh (W˜cxwt + U˜cht−1 + b˜c)\not = σ(Woxwt + Uoht−1 + bo)\nct = ft ⊙ct−1 + it ⊙˜ ct\nht = ot ⊙tanh (ct) ,\n(4)\nwhere Ws,Us ∈ Rd×d and bs ∈ Rd for s ∈\n{f,i, ˜c,o}are parameters of LSTM cells. σ(·) is\nan element-wise sigmoid function, tanh(·) is an\nelement-wise hyperbolic tangent function, and ⊙is\nan element-wise multiplication.\nThe hidden state ht is afﬁne-transformed fol-\nlowed by a softmax function:\nPr (wt+1 = k|w<t+1) = exp\n(\nv⊤\nk ht + bk\n)\n∑\nk′ exp\n(\nv⊤\nk′ ht + bk′\n),\n(5)\nwhere vk is the k-th column of a parameter matrix\nV ∈ Rd×|V | and bk is the k-th element of a bias\nvector b ∈Rd. In the training phase, we minimizes\nthe negative log-likelihood with stochastic gradient\ndescent.\n3 Experimental Settings\nWe test ﬁve different model architectures on the\nthree English corpora. Each model has a unique\nword embedding method, but all models share the\nsame LSTM language modeling architecture, that\nhas 2 LSTM layers with 200 hidden units, d= 200.\nExcept for the character only model, weights in the\nlanguage modeling part are initialized with uniform\nrandom variables between -0.1 and 0.1. Weights of\na bidirectional LSTM in the word embedding part\nare initialized with Xavier initialization (Glorot and\nBengio, 2010). All biases are initialized to zero.\nStochastic gradient decent (SGD) with mini-batch\nsize of 32 is used to train the models. In the ﬁrst k\nepochs, the learning rate is 1. After the k-th epoch,\nthe learning rate is divided by leach epoch. kman-\nages learning rate decay schedule, and l controls\nspeed of decay. k and l are tuned for each model\nbased on the validation dataset.\nAs the standard metric for language modeling,\nperplexity (PPL) is used to evaluate the model per-\nformance. Perplexity over the test set is computed\nas PPL = exp\n(\n−1\nN\n∑N\ni=1 log p(wi|w<i)\n)\n,where N\nis the number of words in the test set, and p(wi|w<i)\nis the conditional probability of a word wi given all\nthe preceding words in a sentence. We use Theano\n(2016) to implement all the models. The code for\nthe models is available fromhttps://githubNcom/\nnyu-dl/gated_word_char_rlm.\n3.1 Model Variations\nWord Only (baseline) This is a traditional word-\nlevel language model and is a baseline model for our\nexperiments.\nCharacter Only This is a language model where\neach input word is represented as a character se-\n1994\nTrain Validation test\nPTB # Sentences 42k 3k 4k\n# Word 888k 70k 79k\nBBC # Sentences 37k 2k 2k\n# Word 890k 49k 53k\nIMDB # Sentences 930k 153k 152k\n# Word 21M 3M 3M\nTable 2: The size of each dataset.\nquence similar to the C2W model in (Ling et al.,\n2015a). The bidirectional LSTMs have 200 hidden\nunits, and their weights are initialized with Xavier\ninitialization. In addition, the weights of the forget,\ninput, and output gates are scaled by a factor of 4.\nThe weights in the LSTM language model are also\ninitialized with Xavier initialization. All biases are\ninitialized to zero. A learning rate is ﬁxed at 0.2.\nWord & Character This model simply concate-\nnates the vector representations of a word con-\nstructed from the character input xchar\nwt and the word\ninput xword\nwt to get the ﬁnal representation of a word\nxwt , i.e.,\nxwt =\n[\nxchar\nwt ; xword\nwt\n]\n. (6)\nBefore being concatenated, the dimensions of xchar\nwt\nand xword\nwt are reduced by half to keep the size ofxwt\ncomparably to other models.\nGated Word & Character, Fixed Value This\nmodel uses a globally constant gating value to com-\nbine vector representations of a word constructed\nfrom the character input xchar\nwt and the word input\nxword\nwt as\nxwt = (1−g) xword\nwt + gxchar\nwt , (7)\nwhere g is some number between 0 and 1. We\nchoose g= {0.25,0.5,0.75}.\nGated Word & Character, AdaptiveThis model\nuses adaptive gating values to combine vector repre-\nsentations of a word constructed from the character\ninput xchar\nwt and the word input xword\nwt as the Eq (3).\n3.2 Datasets\nPenn Treebank We use the Penn Treebank Corpus\n(Marcus et al., 1993) preprocessed by Mikolov et\nal. (2010). We use 10k most frequent words and\n51 characters. In the training phase, we use only\nsentences with less than 50 words.\nBBC We use the BBC corpus prepared by Greene\n& Cunningham (2006). We use 10k most frequent\nwords and 62 characters. In the training phase, we\nuse sentences with less than 50 words.\nIMDB Movie Reviews We use the IMDB Move\nReview Corpus prepared by Maas et al. (2011). We\nuse 30k most frequent words and 74 characters. In\nthe training phase, we use sentences with less than\n50 words. In the validation and test phases, we use\nsentences with less than 500 characters.\n3.3 Pre-training\nFor the word–character hybrid models, we applied\na pre-training procedure to encourage the model\nto use both representations. The entire model is\ntrained only using the word-level input for the ﬁrst\nmepochs and only using the character-level input in\nthe next m epochs. In the ﬁrst m epochs, a learn-\ning rate is ﬁxed at 1, and a smaller learning rate\n0.1 is used in the next m epochs. After the 2m-th\nepoch, both the character-level and the word-level\ninputs are used. We use m = 2for PTB and BBC,\nm= 1for IMDB.\nLample et al. (2016) report that a pre-trained\nword lookup table improves performance of their\nword & character hybrid model on named entity\nrecognition (NER). In their method, word embed-\ndings are ﬁrst trained using skip-n-gram (Ling et\nal., 2015b), and then the word embeddings are ﬁne-\ntuned in the main training phase.\n4 Results and Discussion\n4.1 Perplexity\nTable 1 compares the models on each dataset. On\nthe PTB and IMDB Movie Review dataset, the gated\nword & character model with a ﬁxed gating value,\ngconst = 0.25, and pre-training achieves the lowest\nperplexity . On the BBC datasets, the gated word\n& character model without pre-training achieves the\nlowest perplexity.\nEven though the model with ﬁxed gating value\nperforms well, choosing the gating value is not clear\nand might depend on characteristics of datasets such\nas size. The model with adaptive gating values does\nnot require tuning it and achieves similar perplexity.\n1995\n(a) Gated word & character.\n (b) Gated word & character with pre-taining.\nFigure 2: A log-log plot of frequency ranks and gating values trained in the gated word & character models with/without pre-\ntraining.\n4.2 Values of Word–Character Gate\nThe BBC and IMDB datasets retain out-of-\nvocabulary (OOV) words while the OOV words\nhave been replaced by <unk> in the Penn Treebank\ndataset. On the BBC and IMDB datasets, our model\nassigns a signiﬁcantly high gating value on the un-\nknown word tokenunk compared to the other words.\nWe observe that pre-training results the different\ndistributions of gating values. As can be seen in\nFig. 2 (a), the gating value trained in the gated word\n& character model without pre-training is in general\nhigher for less frequent words, implying that the re-\ncurrent language model has learned to exploit the\nspelling of a word when its word vector could not\nhave been estimated properly. Fig. 2 (b) shows that\nthe gating value trained in the gated word & charac-\nter model with pre-training is less correlated with the\nfrequency ranks than the one without pre-training.\nThe pre-training step initializes a word lookup table\nusing the training corpus and includes its informa-\ntion into the initial values. We hypothesize that the\nrecurrent language model tends to be word–input–\noriented if the informativeness of word inputs and\ncharacter inputs are not balanced especially in the\nearly stage of training.\nAlthough the recurrent language model with or\nwithout pre-training derives different gating values,\nthe results are still similar. We conjecture that the\nﬂexibility of modulating between word-level and\ncharacter-level representations resulted in a better\nlanguage model in multiple ways.\nOverall, the gating values are small. However,\nthis does not mean the model does not utilize the\ncharacter-level inputs. We observed that the word\nvectors constructed from the character-level inputs\nusually have a larger L2 norm than the word vec-\ntors constructed from the word-level inputs do. For\ninstance, the mean values of L2 norm of the 1000\nmost frequent words in the IMDB training set are\n52.77 and 6.27 respectively. The small gate values\ncompensate for this difference.\n5 Conclusion\nWe introduced a recurrent neural network language\nmodel with LSTM units and a word–character gate.\nOur model was empirically found to utilize the\ncharacter-level input especially when the model en-\ncounters rare words. The experimental results sug-\ngest the gate can be efﬁciently trained so that the\nmodel can ﬁnd a good balance between the word-\nlevel and character-level inputs.\nAcknowledgments\nThis work is done as a part of the course DS-\nGA 1010-001 Independent Study in Data Science\nat the Center for Data Science, New York Univer-\nsity. KC thanks the support by Facebook, Google\n(Google Faculty Award 2016) and NVidia (GPU\nCenter of Excellence 2015-2016). YM thanks Ken-\ntaro Hanaki, Israel Malkin, and Tian Wang for their\nhelpful feedback. KC and YM thanks the anony-\nmous reviewers for their insightful comments and\nsuggestions.\n1996\nReferences\n[Bengio et al.2003] Yoshua Bengio, Réjean Ducharme,\nPascal Vincent, and Christian Janvin. 2003. A neu-\nral probabilistic language model. Journal of Machine\nLearning Research, 3:1137–1155.\n[Bojanowski et al.2015] Piotr Bojanowski, Armand\nJoulin, and Tomas Mikolov. 2015. Alternative struc-\ntures for character-level rnns. CoRR, abs/1511.06303.\n[dos Santos and Zadrozny2014] Cícero Nogueira dos\nSantos and Bianca Zadrozny. 2014. Learning\ncharacter-level representations for part-of-speech\ntagging. In Proceedings of the 31th International\nConference on Machine Learning, ICML 2014,\nBeijing, China, 21-26 June 2014, pages 1818–1826.\n[Glorot and Bengio2010] Xavier Glorot and Yoshua Ben-\ngio. 2010. Understanding the difﬁculty of training\ndeep feedforward neural networks. In Proceedings of\nthe Thirteenth International Conference on Artiﬁcial\nIntelligence and Statistics, AISTATS 2010, Chia La-\nguna Resort, Sardinia, Italy, May 13-15, 2010 , pages\n249–256.\n[Graves and Schmidhuber2005] Alex Graves and Jürgen\nSchmidhuber. 2005. Framewise phoneme classiﬁca-\ntion with bidirectional LSTM and other neural network\narchitectures. Neural Networks, 18(5-6):602–610.\n[Greene and Cunningham2006] Derek Greene and\nPadraig Cunningham. 2006. Practical solutions to the\nproblem of diagonal dominance in kernel document\nclustering. In Machine Learning, Proceedings of the\nTwenty-Third International Conference (ICML 2006),\nPittsburgh, Pennsylvania, USA, June 25-29, 2006 ,\npages 377–384.\n[Hashimoto and Tsuruoka2016] Kazuma Hashimoto and\nYoshimasa Tsuruoka. 2016. Adaptive joint learning\nof compositional and non-compositional phrase em-\nbeddings. CoRR, abs/1603.06067.\n[Józefowicz et al.2016] Rafal Józefowicz, Oriol Vinyals,\nMike Schuster, Noam Shazeer, and Yonghui Wu.\n2016. Exploring the limits of language modeling.\nCoRR, abs/1602.02410.\n[Kang et al.2011] Moonyoung Kang, Tim Ng, and Long\nNguyen. 2011. Mandarin word-character hybrid-\ninput neural network language model. In INTER-\nSPEECH 2011, 12th Annual Conference of the In-\nternational Speech Communication Association, Flo-\nrence, Italy, August 27-31, 2011, pages 625–628.\n[Kim et al.2015] Yoon Kim, Yacine Jernite, David Son-\ntag, and Alexander M. Rush. 2015. Character-aware\nneural language models. CoRR, abs/1508.06615.\n[Lample et al.2016] Guillaume Lample, Miguel Balles-\nteros, Sandeep Subramanian, Kazuya Kawakami, and\nChris Dyer. 2016. Neural architectures for named en-\ntity recognition. CoRR, abs/1603.01360.\n[Ling et al.2015a] Wang Ling, Tiago Luís, Luís Marujo,\nRámon Fernandez Astudillo, Silvio Amir, Chris Dyer,\nAlan W Black, and Isabel Trancoso. 2015a. Finding\nfunction in form: Compositional character models for\nopen vocabulary word representation. EMNLP.\n[Ling et al.2015b] Wang Ling, Yulia Tsvetkov, Silvio\nAmir, Ramon Fermandez, Chris Dyer, Alan W. Black,\nIsabel Trancoso, and Chu-Cheng Lin. 2015b. Not\nall contexts are created equal: Better word represen-\ntations with variable attention. In Proceedings of the\n2015 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2015, Lisbon, Portu-\ngal, September 17-21, 2015, pages 1367–1372.\n[Luong and Manning2016] Minh-Thang Luong and\nChristopher D. Manning. 2016. Achieving open\nvocabulary neural machine translation with hybrid\nword-character models. CoRR, abs/1604.00788.\n[Maas et al.2011] Andrew L. Maas, Raymond E. Daly,\nPeter T. Pham, Dan Huang, Andrew Y . Ng, and\nChristopher Potts. 2011. Learning word vectors for\nsentiment analysis. In The 49th Annual Meeting of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Proceedings of the Con-\nference, 19-24 June, 2011, Portland, Oregon, USA ,\npages 142–150.\n[Marcus et al.1993] Mitchell P. Marcus, Beatrice San-\ntorini, and Mary Ann Marcinkiewicz. 1993. Building\na large annotated corpus of english: The penn tree-\nbank. Computational Linguistics, 19(2):313–330.\n[Mikolov et al.2010] Tomas Mikolov, Martin Karaﬁát,\nLukás Burget, Jan Cernocký, and Sanjeev Khudan-\npur. 2010. Recurrent neural network based language\nmodel. In INTERSPEECH 2010, 11th Annual Confer-\nence of the International Speech Communication As-\nsociation, Makuhari, Chiba, Japan, September 26-30,\n2010, pages 1045–1048.\n[Mikolov et al.2012] Tomas Mikolov, Ilya Sutskever,\nAnoop Deoras, Hai-Son Le, and Stefan Kombrink.\n2012. Subword language modeling with neural net-\nworks.\n[Theano Development Team2016] Theano Development\nTeam. 2016. Theano: A Python framework for fast\ncomputation of mathematical expressions. arXiv e-\nprints, abs/1605.02688, May.\n[Zaremba et al.2014] Wojciech Zaremba, Ilya Sutskever,\nand Oriol Vinyals. 2014. Recurrent neural network\nregularization. CoRR, abs/1409.2329.\n[Zoph et al.2016] Barret Zoph, Ashish Vaswani, Jonathan\nMay, and Kevin Knight. 2016. Simple, fast\nnoise-contrastive estimation for large rnn vocabularies.\nNAACL.\n1997"
}