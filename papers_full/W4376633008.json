{
  "title": "Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation",
  "url": "https://openalex.org/W4376633008",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2226268113",
      "name": "Zhang Jizhi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2360607594",
      "name": "Bao Ke-qin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1999911198",
      "name": "Zhang Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1462929032",
      "name": "Wang Wenjie",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3132652808",
      "name": "Feng, Fuli",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2329433866",
      "name": "He, Xiangnan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3153182568",
    "https://openalex.org/W4296413526",
    "https://openalex.org/W4282027681",
    "https://openalex.org/W4311991106",
    "https://openalex.org/W3127259869",
    "https://openalex.org/W3200592006",
    "https://openalex.org/W2140190241",
    "https://openalex.org/W2141485866",
    "https://openalex.org/W3163155381",
    "https://openalex.org/W2963189767",
    "https://openalex.org/W2039613841",
    "https://openalex.org/W3104475013",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4386728933",
    "https://openalex.org/W3170344956",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4327500636",
    "https://openalex.org/W4377866047",
    "https://openalex.org/W4212951676",
    "https://openalex.org/W2954100110",
    "https://openalex.org/W3115164309",
    "https://openalex.org/W4385638369",
    "https://openalex.org/W4392454577",
    "https://openalex.org/W3035296331",
    "https://openalex.org/W2050125880",
    "https://openalex.org/W4281608888",
    "https://openalex.org/W2964427276",
    "https://openalex.org/W3156002164",
    "https://openalex.org/W2748058847",
    "https://openalex.org/W4384648324",
    "https://openalex.org/W3105882417",
    "https://openalex.org/W4363672019",
    "https://openalex.org/W2767433005",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4284713500",
    "https://openalex.org/W3100355250",
    "https://openalex.org/W4378189609",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4361193179",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4362515116"
  ],
  "abstract": "The remarkable achievements of Large Language Models (LLMs) have led to the emergence of a novel recommendation paradigm -- Recommendation via LLM (RecLLM). Nevertheless, it is important to note that LLMs may contain social prejudices, and therefore, the fairness of recommendations made by RecLLM requires further investigation. To avoid the potential risks of RecLLM, it is imperative to evaluate the fairness of RecLLM with respect to various sensitive attributes on the user side. Due to the differences between the RecLLM paradigm and the traditional recommendation paradigm, it is problematic to directly use the fairness benchmark of traditional recommendation. To address the dilemma, we propose a novel benchmark called Fairness of Recommendation via LLM (FaiRLLM). This benchmark comprises carefully crafted metrics and a dataset that accounts for eight sensitive attributes1 in two recommendation scenarios: music and movies. By utilizing our FaiRLLM benchmark, we conducted an evaluation of ChatGPT and discovered that it still exhibits unfairness to some sensitive attributes when generating recommendations. Our code and dataset can be found at https://github.com/jizhi-zhang/FaiRLLM.",
  "full_text": "Is ChatGPT Fair for Recommendation? Evaluating Fairness in\nLarge Language Model Recommendation\nJizhi Zhang*\ncdzhangjizhi@mail.ustc.edu.cn\nUniversity of Science and Technology\nof China\nChina\nKeqin Bao*\nbaokq@mail.ustc.edu.cn\nUniversity of Science and Technology\nof China\nChina\nYang Zhang\nzy2015@mail.ustc.edu.cn\nUniversity of Science and Technology\nof China\nChina\nWenjie Wang\nwenjiewang96@gmail.com\nNational University of Singapore\nSingapore\nFuli Fengâ€ \nfulifeng93@gmail.com\nUniversity of Science and Technology\nof China\nChina\nXiangnan Heâ€ \nxiangnanhe@gmail.com\nUniversity of Science and Technology\nof China\nChina\nABSTRACT\nThe remarkable achievements of Large Language Models (LLMs)\nhave led to the emergence of a novel recommendation paradigm â€”\nRecommendation via LLM (RecLLM). Nevertheless, it is important\nto note that LLMs may contain social prejudices, and therefore,\nthe fairness of recommendations made by RecLLM requires fur-\nther investigation. To avoid the potential risks of RecLLM, it is\nimperative to evaluate the fairness of RecLLM with respect to var-\nious sensitive attributes on the user side. Due to the differences\nbetween the RecLLM paradigm and the traditional recommendation\nparadigm, it is problematic to directly use the fairness benchmark\nof traditional recommendation. To address the dilemma, we pro-\npose a novel benchmark called Fairness of Recommendation via\nLLM (FaiRLLM). This benchmark comprises carefully crafted met-\nrics and a dataset that accounts for eight sensitive attributes 1 in\ntwo recommendation scenarios: music and movies. By utilizing\nour FaiRLLM benchmark, we conducted an evaluation of ChatGPT\nand discovered that it still exhibits unfairness to some sensitive at-\ntributes when generating recommendations. Our code and dataset\ncan be found at https://github.com/jizhi-zhang/FaiRLLM.\nCCS CONCEPTS\nâ€¢ Information systems â†’Recommender systems.\n1We apologize if any of the sensitive attribute values mentioned caused offense. We only\nrefer to these sensitive attributes for the purpose of studying fairness and advocating\nfor the protection of the rights of disadvantaged groups.\n*The two authors contributed equally to this work and the order is determined by\nrolling the dice. â€ Corresponding authors.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nRecSys â€™23, September 18â€“22, 2023, Singapore, Singapore\nÂ© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0241-9/23/09. . . $15.00\nhttps://doi.org/10.1145/3604915.3608860\nKEYWORDS\nLarge Language Models, Fairness, Benchmark\nACM Reference Format:\nJizhi Zhang*, Keqin Bao*, Yang Zhang, Wenjie Wang, Fuli Fengâ€ , and Xiang-\nnan Heâ€ . 2023. Is ChatGPT Fair for Recommendation? Evaluating Fairness\nin Large Language Model Recommendation. In Seventeenth ACM Conference\non Recommender Systems (RecSys â€™23), September 18â€“22, 2023, Singapore, Sin-\ngapore. ACM, New York, NY, USA, 7 pages. https://doi.org/10.1145/3604915.\n3608860\n1 INTRODUCTION\nThe great development of Large Language Models (LLMs) [12, 30,\n34, 40] can extend channels for information seeking, i.e., interact-\ning with LLMs to acquire information like ChatGPT [3, 5, 20, 41].\nThe revolution of LLM has also formed a new paradigm of recom-\nmendations which makes recommendations through the language\ngeneration of LLMs according to user instructions [7, 14]. Figure 1\nillustrates some examples under this Recommendation via LLM (Re-\ncLLM) paradigm, e.g., users give instructions like â€œProvide me 20\nsong titles ...? â€ and LLM returns a list of 20 song titles.\nHowever, directly using LLM for recommendation may raise\nconcerns about fairness. Previous work has shown that LLMs tend\nto reinforce social biases in their generation outputs due to the\nbias in the large pre-training corpus, leading to unfair treatment\nof vulnerable groups [ 4, 13, 19]. Fairness is also a critical crite-\nrion of recommendation systems due to their enormous social im-\npact [10, 24, 29, 38]. Despite the tremendous amount of analysis on\nthe fairness issue of conventional recommendation systems [24, 38],\nfairness in RecLLM has not been explored. It is essential to bridge\nthis research gap to avoid the potential risks of applying RecLLM.\nIn this paper, we analyze the fairness of RecLLM w.r.t. the sen-\nsitive attribute of users. Some users may choose not to disclose\ncertain sensitive attributes such asskin color and race due to privacy\nconcerns [11, 27] when giving instruction for generating recom-\nmended results (Figure 1). Hiding sensitive attributes may result\nin unfairness on the user side since the LLM has a preference for a\nspecific attribute based on its training data. For instance, Figure 1\nshows that the recommendation results without sensitive attributes\nprovided are biased towards some specific user groups, leading to\narXiv:2305.07609v3  [cs.IR]  17 Oct 2023\nRecSys â€™23, September 18â€“22, 2023, Singapore, Singapore Jizhi Zhang*, Keqin Bao*, Yang Zhang, Wenjie Wang, Fuli Feng â€ , and Xiangnan Heâ€ \nUser\nRecLLM\nI am a fan of \nAdele\n. Please \nprovide me with a list of \n20\nsong titles \nin order of \npreference that you think I \nmight like\n. Please do not \nprovide any additional \ninformation about the \nsongs, such as artist, genre, \nor release date.\nNeutral\nRecLLM\nI am a \nwhite\nfan of \nAdele\n. Please \nprovide me with \nâ€¦â€¦\n1. Someone Like You\n2. Rolling in the Deep\n3. Set Fire to the Rain\n4. Hello\n5. When We Were Young\nâ€¦â€¦\nUser\nSensitive Attribute 1\nRecLLM\nI am a\nn\nAfrican American\nfan of \nAdele\n. Please provide me with \nâ€¦â€¦\nUser\nSensitive Attribute 2\nSimilar\nUnfair!\nDissimilar!\n1. Someone Like You\n2. Rolling in the Deep\n3. Set Fire to the Rain\n4. Hello\n5. When We Were Young\n6. All I Ask\n7. Skyfall\n8. Rumour Has It\n9. Chasing Pavements\nâ€¦â€¦\n1. Love on Top\n2. I Will Always Love You\n3. \nAin't\nNo Mountain High Enough\n4. I \nWanna\nDance with Somebody\n5. Purple Rain\nâ€¦â€¦\n              \n   \n       \n      \n         \n          \n    \n        \n       \n                    \n                          \n               \n               \n                    \n                       \n                       \n             \n                \n                  \n                          \n        \nFigure 1: On the left is an example of our fairness evaluation for RecLLM in music recommendation. Specifically, we judge\nfairness by comparing the similarity between the recommended results of different sensitive instructions and the neutral\ninstruction. Under ideal equity, recommendations for sensitive attributes under the same category should be equally similar to\nrecommendations for the neutral instruct. On the right are the sensitive attributes we explored and their specific values.\nunfairness for vulnerable groups. Therefore, it is crucial to evaluate\nthe user-side fairness in the RecLLM.\nHowever, directly using the traditional fairness benchmark to\nmeasure the fairness of RecLLM has some problems. In detail, on the\none hand, traditional fairness measurement methods often require\nscores of model prediction results to calculate fairness metrics,\nwhich is difficult to obtain in RecLLM. On the other hand, traditional\nmethods need to calculate fairness on a fixed candidate set based on\nthe specific dataset. Due to the universality of RecLLM, limiting its\noutput range seriously damages its upper limit of recommendation\nability, and canâ€™t really measure its fairness in practical applications.\nTo address these problems, we come up with a Fairness of\nRecommendation via LLM benchmark called FaiRLLM tailored\nspecifically for RecLLM. FaiRLLM evaluates the fairness of RecLLM\nby measuring the similarity between the recommendation results of\nneutral instructions that do not include sensitive attributes andsensi-\ntive instructions that disclose such attributes (as shown in Figure 1).\nIt assesses the fairness of RecLLM by analyzing the divergence of\nsimilarities across different values of the sensitive attributes (e.g.,\nAfrican American, black, white, and yellow in the case of race). In\nparticular, we have defined three metrics for evaluating the simi-\nlarity of two recommendation lists generated by LLMs, which can\naccommodate newly generated items. Moreover, we have created\ndatasets for two common recommendation scenarios, namely music,\nand movies, taking into account eight sensitive attributes, as illus-\ntrated in Figure 1. On these datasets, we have evaluated ChatGPT,\nshowing its unfairness on various sensitive attributes.\nOur contributions are summarized as follows:\nâ€¢To our knowledge, this is the first investigation into the fair-\nness issues of the emerging LLM for recommendation paradigm,\npresenting a novel recommendation problem.\nâ€¢We build a new FaiRLLM benchmark which includes carefully\ndesigned evaluation methods and datasets in two scenes of rec-\nommendation with consideration of eight sensitive attributes.\nâ€¢We extensively evaluate ChatGPT with the FaiRLLM benchmark\nand reveal fairness issues on several sensitive attributes.\n2 RELATED WORK\nIn this section, we briefly discuss the related work on fairness in\nboth the LLM field and in recommendation.\nâ€¢Fairness in Large Language Models. Researchers have found\nthat bias in the pretraining corpus can cause LLMs to generate\nharmful or offensive content, such as discriminating against disad-\nvantaged groups. This has increased research focus on the harmful-\nness issues of LLMs, including unfairness. One line of such research\nis aimed at reducing the unfairness of an LLM (as well as other\nharmfulness). For instance, RLHF [30] and RLAIF [6] are used to\nprevent reinforcing existing stereotypes and producing demeaning\nportrayals. Additionally, another emerging research area in the\nNLP community focuses on better evaluating the unfairness and\nother harmfulness of LLMs by proposing new benchmarks. Specific\nexamples include CrowS-Pairs [28], which is a benchmark dataset\ncontaining multiple sentence pairs where one sentence in each pair\nis more stereotyping than the other; RealToxicityPrompts [16] and\nRedTeamingData [13], which are datasets for the prompt genera-\ntion task containing prompts that could induce models to generate\nharmful or toxic responses; and HELM [ 26], which is a holistic\nevaluation benchmark for large language models that evaluates\nboth bias and fairness. Despite the existing research on fairness in\nLLMs in the field of NLP, there is currently no relevant research on\nthe fairness of RecLLM, and this work aims to initially explore this\nfield.\nâ€¢Fairness in Recommendation. With increasing concerns about\nthe negative social impact of recommendation systems [29, 32, 33],\nIs ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation RecSys â€™23, September 18â€“22, 2023, Singapore, Singapore\nboth item-side [1, 2] and user-side [ 22, 23, 31] unfairness issues\nin recommendation have received significant attention in recent\nyears [24, 38]. Existing recommendation fairness can be categorized\ninto individual fairness [9, 25, 42] and group fairness [15, 23, 37]. In-\ndividual fairness, such as counterfactual fairness [25], requires that\neach similar individual should be treated similarly [25], while group\nfairness emphasizes fair recommendations at the group level [15].\nConceptually, the investigated fairness for RecLLM can be catego-\nrized as user-side group fairness. However, there is a distinct differ-\nence between our fairness and traditional group fairness: traditional\ngroup fairness is directly defined by the difference in recommen-\ndation results/qualities across different sensitive groups [ 24, 38],\nwhereas we focus on the difference in a specific similarity, namely,\nthe similarity of the sensitive group to the neutral group, across\ndifferent sensitive groups. This difference would further raise dif-\nferent requirements for evaluation methods and metrics, compared\nto the traditional ones.\n3 FAIRLLM BENCHMARK\nWe introduce the fairness evaluation and dataset construction in\nthe FaiRLLM benchmark in Â§3.1 and Â§3.2, respectively.\n3.1 Fairness Evaluation in RecLLM\nFairness Definition. As an initial attempt, we focus on the user-\nside fairness in RecLLM. Given a sensitive attribute (e.g., gender) of\nusers, we define the fairness of RecLLM as the absence of any preju-\ndice or favoritism toward user groups with specific values (e.g., female\nand male) of the sensitive attribute when generating recommendations\nwithout using such sensitive information .\n3.1.1 Evaluation Method. The key is to investigate whether Re-\ncLLM exhibits prejudice or favoritism towards specific user groups\nwhen receiving instructions without sensitive information. To de-\ntermine the existence of prejudice or favoritism, we first construct\nthe reference status, i.e., obtaining recommendation results with-\nout sensitive attributes in the user instruction. We then compute\nsimilarities between the reference status and recommendation re-\nsults obtained with specific values of the sensitive attribute, and\ncompare these similarities to quantify the degree of fairness. Let\nA= {ğ‘}denote a sensitive attribute where ğ‘ is a specific value\nof the attribute. Note that ğ‘is a word or phrase. Given ğ‘€ neutral\nuser instructions, the main steps of our evaluation method for each\ninstruction are as follows:\nâ€¢Step 1: Obtain the top-ğ¾ recommendations (Rğ‘š) of each neutral\ninstruction ğ¼ğ‘š, where ğ‘šis the index of instruction;\nâ€¢Step 2: Construct sensitive instructions {ğ¼ğ‘ğ‘š }for each value of\nthe sensitive attribute Aby injecting the value ğ‘into the neutral\ninstruction ğ¼ğ‘š, and obtain the top-ğ¾ recommendations of each\nsensitive instructions denoted as {Rğ‘ğ‘š };\nâ€¢Step 3: Compute ğ‘†ğ‘–ğ‘š(Rğ‘ğ‘š,Rğ‘š), the similarity between Rğ‘ğ‘š and\nRğ‘š for each ğ‘ âˆˆA.\nFor each value ğ‘, we aggregate its similarity scores across all ğ‘€\ninstructions as ğ‘†ğ‘–ğ‘š(ğ‘):= Ã\nğ‘š ğ‘†ğ‘–ğ‘š(Rğ‘ğ‘š,Rğ‘š)/ğ‘€ and then evaluate\nthe level of unfairness in RecLLM as the divergence of these aggre-\ngated similarities across different values of the sensitive attribute,\n{ğ‘†ğ‘–ğ‘š(ğ‘)|ğ‘ âˆˆA}.\n3.1.2 Benchmark Metrics. To quantify the level of unfairness, we\nintroduce new fairness metrics based on the obtained similarities\n{ğ‘†ğ‘–ğ‘š(ğ‘)|ğ‘ âˆˆA}. We next present the fairness metrics and elaborate\non the utilized similarity metrics.\nFairness metrics. We propose two fairness metrics â€” Sensitive-to-\nNeutral Similarity Range (ğ‘†ğ‘ğ‘†ğ‘…) and Sensitive-to-Neutral Similarity\nVariance (ğ‘†ğ‘ğ‘†ğ‘‰ ), which quantify the unfairness level by measuring\nthe divergence of {ğ‘†ğ‘–ğ‘š(ğ‘)|ğ‘ âˆˆA} from different aspects. Specifi-\ncally, ğ‘†ğ‘ğ‘†ğ‘… measures the difference between the similarities of the\nmost advantaged and disadvantaged groups, whileğ‘†ğ‘ğ‘†ğ‘‰ measures\nthe variance of ğ‘†ğ‘–ğ‘š(ğ‘)across all possible ğ‘of the studied sensitive\nattribute Ausing the Standard Deviation. Formally, for the top-ğ¾\nrecommendation,\nğ‘†ğ‘ğ‘†ğ‘…@ğ¾ = max\nğ‘âˆˆA\nğ‘†ğ‘–ğ‘š(ğ‘)âˆ’ min\nğ‘âˆˆA\nğ‘†ğ‘–ğ‘š(ğ‘),\nğ‘†ğ‘ğ‘†ğ‘‰ @ğ¾ =\nvuut 1\n|A|\nâˆ‘ï¸\nğ‘âˆˆA\n \nğ‘†ğ‘–ğ‘š(ğ‘)âˆ’ 1\n|A|\nâˆ‘ï¸\nğ‘â€²âˆˆA\nğ‘†ğ‘–ğ‘š(ğ‘â€²)\n!2\n,\n(1)\nwhere |A|denotes the number of all possible values in the stud-\nied sensitive attribute. For both fairness metrics, a higher value\nindicates greater levels of unfairness.\nSimilarity metrics. Regarding the similarity ğ‘†ğ‘–ğ‘š(ğ‘), we compute\nit using three similarity metrics that can measure the similarity\nbetween two recommendation lists:\nâ€¢Jaccard similarity [ 17]. This metric is widely used to measure\nthe similarity between two sets by the ratio of their common\nelements to their total distinct elements. We directly treat a recom-\nmendation list as a set to compute the Jaccard similarity between\nthe neutral group and the sensitive group with the sensitive\nattribute value ğ‘as:\nğ½ğ‘ğ‘ğ‘ğ‘ğ‘Ÿğ‘‘@ğ¾ = 1\nğ‘€\nâˆ‘ï¸\nğ‘š\n|Rğ‘š âˆ©Rğ‘ğ‘š |\n|Rğ‘š |+|R ğ‘ğ‘š |âˆ’|R ğ‘š âˆ©Rğ‘ğ‘š |, (2)\nwhere Rğ‘š, Rğ‘ğ‘š, and ğ‘€still have the same means as Section 3.1.1,\n|Rğ‘š âˆ©Rğ‘ğ‘š |denotes the number of common items between the\nRğ‘š and Rğ‘ğ‘š, similarly for others. Functionally, ğ½ğ‘ğ‘ğ‘ğ‘ğ‘Ÿğ‘‘@ğ¾ mea-\nsures the average overlapping level of neutral and sensitive rec-\nommendation list pairs, without considering the item ranking\ndifferences.\nâ€¢SERP*. This metric is developed based on the SEarch Result\nPage Misinformation Score (SERP-MS) [35], which we modify\nto measure the similarity between two recommendation lists\nwith the consideration of the number of overlapping elements\nand their ranks. Formally, for the top- ğ¾ recommendation, the\nsimilarity between the neutral and the group with a specific value\nğ‘of the sensitive group is computed as:\nğ‘†ğ¸ğ‘…ğ‘ƒâˆ—@ğ¾ = 1\nğ‘€\nâˆ‘ï¸\nğ‘š\nâˆ‘ï¸\nğ‘£âˆˆRğ‘ğ‘š\nI(ğ‘£ âˆˆRğ‘š)âˆ—( ğ¾âˆ’ğ‘Ÿğ‘ğ‘š,ğ‘£ +1)\nğ¾âˆ—(ğ¾+1)/2 , (3)\nwhere ğ‘£ represents an item in Rğ‘ğ‘š, ğ‘Ÿğ‘ğ‘š,ğ‘£ âˆˆ{1,...,ğ¾ }represents\nthe rank of the itemğ‘£in Rğ‘ğ‘š, and I(ğ‘£ âˆˆRğ‘š)= 1 if ğ‘£ âˆˆRğ‘š is true\nelse 0. This metric can be viewed as a weighted Jaccard similarity,\nwhich further weights items with their ranks in Rğ‘ğ‘š. However,\nit does not consider the relative ranks of two elements, e.g., if\nğ‘£1 and ğ‘£2 belonging to Rğ‘ğ‘š both appear in the Rğ‘š, exchanging\nthem in Rğ‘ğ‘š would not change the result.\nRecSys â€™23, September 18â€“22, 2023, Singapore, Singapore Jizhi Zhang*, Keqin Bao*, Yang Zhang, Wenjie Wang, Fuli Feng â€ , and Xiangnan Heâ€ \nâ€¢PRAG*. This similarity metric is designed by referencing the\nPairwise Ranking Accuracy Gap metric [8], which could consider\nthe relative ranks between two elements. Formally, the similarity\nbetween the neutral and sensitive groups about the top-ğ¾ LLMâ€™s\nrecommendation is computed as:\nğ‘ƒğ‘…ğ´ğºâˆ—@ğ¾\n=\nâˆ‘ï¸\nğ‘š\nâˆ‘ï¸\nğ‘£1,ğ‘£2 âˆˆRğ‘ğ‘š\nğ‘£1â‰ ğ‘£2\n\u0002\nI (ğ‘£1 âˆˆRğ‘š)âˆ—I \u0000ğ‘Ÿğ‘š,ğ‘£1 < ğ‘Ÿğ‘š,ğ‘£2\n\u0001 âˆ—I(ğ‘Ÿğ‘ğ‘š,ğ‘£1 < ğ‘Ÿğ‘ğ‘š,ğ‘£2 )\n\u0003\nğ¾(ğ¾+1)ğ‘€ ,\n(4)\nwhere I(Â·)still has similar means as Equation(3), ğ‘£1 and ğ‘£2 denote\ntwo different recommended items in Rğ‘ğ‘š, and ğ‘Ÿğ‘ğ‘š,ğ‘£1 (or ğ‘Ÿğ‘š,ğ‘£1 )\ndenotes the rank of ğ‘£1 in Rğ‘ğ‘š (or Rğ‘š). Specifically, if ğ‘£1 is not in\nRğ‘š, then ğ‘Ÿğ‘š,ğ‘£1 = +âˆ, similarly for ğ‘£2. As shown in the equation,\na higher metric does not only require high item overlap but also\nrequires the pairwise ranking order between an item and another\nitem to be the same inRğ‘š and Rğ‘ğ‘š. This allows us to measure the\nagreement of pairwise ranking between recommendation results\nfor the natural and sensitive instructions.\n3.2 Dataset Construction\nRecLLM differs from conventional recommender systems in terms\nof the data requirements for both the model input and fairness eval-\nuation, raising the need of constructing a new benchmark dataset\nthat is suitable for RecLLM fairness evaluation. In this section, we\ndetail how to construct such a new benchmark dataset, beginning\nby presenting the data format and then moving on to the detailed\ndata collection process.\n3.2.1 Data Format. RecLLM usually relies on user instructions\n(i.e., recommendation requests) in natural language, in which the\nuser preference is explicitly expressed, to make recommendations.\nTherefore, the core of constructing a dataset for RecLLM fairness\nevaluation is to collect suitable user instructions. Without losing\ngenerality, we further assume user instructions are expressed fol-\nlowing a fixed template, which includes both the user preference\ninformation and the task information. Specifically, we take the fol-\nlowing template for neutral and sensitive instructions, respectively:\nNetrual: â€œI am a fan of [names]. Please provide me with a list\nof ğ¾ song/movie titles... â€\nSensitive: â€œI am a/an [sensitive feature] fan of [names]. Please\nprovide me with a list of ğ¾ song/movie titles... â€,\nwhere â€œI am a [sensitive feature] fan of [name] â€ is used to express\nuser preference, â€œPlease provide me with a list of ğ¾ item titles... â€\ndenotes the task description. With these templates, we can simulate\nusers with different preference by varying the â€œ [name]â€ field to\nobtain different neutral instractions, and inject different sensitive\ninformation by varying the â€œ[sensitive feature]â€ field to construct\ndifferent sensitive instructions. Here, we consider the top-ğ¾ recom-\nmendation scenario and take item titles to represent item identities.\n3.2.2 Data Collection. We next select data to fill in the â€œ[names]â€\nand â€œ[sensitive feature] â€ fields to construct our dataset. To ensure\nthe recommendation validity of RecLLM, we use a selection pro-\ncess designed to increase the likelihood that the LLM has seen\nthe selected data. Specifically, for the â€œ[sensitive feature] â€ field, we\nconsider eight commonly discussed sensitive attributes: age, coun-\ntry, gender, continent, occupation, race, religion , and physics. The\npossible values for each attribute are summarized in Figure 1. For\nthe â€œ[names]â€ field, we choose famous singers of music or famous\ndirectors of movies as potential candidates. Then, we enumerate\nall possible singers/directors, as well as all possible values of the\nsensitive attributes, resulting in two datasets:\n- Music. We first screen the 500 most popular singers on the Mu-\nsic Television platform2 based on The 10,000 MTVâ€™s Top Music\nArtists3. Then, we enumerate all singers and all possible values\nof each sensitive attribute to fill in the â€œ[name]â€ and â€œ[sensitive\nfeature]â€ fields, respectively, to construct the music dataset.\n- Movie. First, we utilize the IMDB official API4, one of the most\nreputable and authoritative websites of movie and TV informa-\ntion, to select 500 directors with the highest number of popular\nmovies and TV shows from the IMDB dataset. Popular movies\nand TV shows are defined as those with over 2000 reviews and\nhigh ratings (>7). We then populate the selected directors and all\npossible sensitive attribute values into the corresponding fields\nof our data templates in the enumeration method, resulting in\nthe movie dataset.\n4 RESULTS AND ANALYSIS\nIn this section, we conduct experiments based on the proposed\nbenchmark to analyze the recommendation fairness of LLMs by\nanswering the following two questions:\nâ€¢RQ1: How unfair is the LLM when serving as a recommender\non various sensitive user attributes?\nâ€¢RQ2: Is the unfairness phenomenon for using LLM as a recom-\nmender robust across different cases?\n4.1 Overall Evaluation (RQ1)\nConsidering the representative role of ChatGPT among existing\nLLMs, we take it as an example to study the recommendation fair-\nness of LLMs, using the proposed evaluation method and dataset.\nWe feed each neutral instruction and corresponding sensitive in-\nstruction into ChatGPT to generate top-ğ¾ recommendations (ğ¾=20\nfor both music and movie data), respectively. And then we compute\nthe recommendation similarities between the neutral (reference)\nand sensitive groups and the fairness metrics. Specifically, when us-\ning ChatGPT to generate the recommendation text, we use ChatGPT\nin a greedy-search manner by fixing the hyperparameters including\ntemperature, top_p, and frequency_penality as zero to ensure the\nreproducibility of the experiments. We summarize the results in\nTable 1 and Figure 2. The table presents fairness metrics, as well as\nmaximal and minimal similarities, where the maximal/minimal sim-\nilarity corresponds to the most advantaged/disadvantaged group,\nrespectively. The figure depicts the similarity of each sensitive\n2https://www.mtv.com/.\n3https://gist.github.com/mbejda/9912f7a366c62c1f296c.\n4https://developer.imdb.com/.\nIs ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation RecSys â€™23, September 18â€“22, 2023, Singapore, Singapore\nTable 1: Fairness evaluation of ChatGPT for Music and Movie Recommendations. ğ‘†ğ‘ğ‘†ğ‘… and ğ‘†ğ‘ğ‘†ğ‘‰ are measures of unfairness,\nwith higher values indicating greater unfairness. â€œMinâ€ and â€œMaxâ€ denote the minimum and maximum similarity across all\nvalues of a sensitive attribute, respectively. Note: the sensitive attributes are ranked by their SNSV in PRAG*@20.\nSorted Sensitive Attribute\nDataset Metric Religion Continent Occupation Country Race Age Gender Physics\nMax 0.7057 0.7922 0.7970 0.7922 0.7541 0.7877 0.7797 0.8006\nMin 0.6503 0.7434 0.7560 0.7447 0.7368 0.7738 0.7620 0.7973\nSNSR 0.0554 0.0487 0.0410 0.0475 0.0173 0.0139 0.0177 0.0033Jaccard@20\nSNSV 0.0248 0.0203 0.0143 0.0141 0.0065 0.0057 0.0067 0.0017\nMax 0.2395 0.2519 0.2531 0.2525 0.2484 0.2529 0.2512 0.2546\nMin 0.2205 0.2474 0.2488 0.2476 0.2429 0.2507 0.2503 0.2526\nSNSR 0.0190 0.0045 0.0043 0.0049 0.0055 0.0022 0.0009 0.0020SERP*@20\nSNSV 0.0088 0.0019 0.0018 0.0017 0.0021 0.0010 0.0004 0.0010\nMax 0.7997 0.8726 0.8779 0.8726 0.8482 0.8708 0.8674 0.8836\nMin 0.7293 0.8374 0.8484 0.8391 0.8221 0.8522 0.8559 0.8768\nSNSR 0.0705 0.0352 0.0295 0.0334 0.0261 0.0186 0.0116 0.0069\nMusic\nPRAG*@20\nSNSV 0.0326 0.0145 0.0112 0.0108 0.0097 0.0076 0.0050 0.0034\nMetric Race Country Continent Religion Gender Occupation Physics Age\nMax 0.4908 0.5733 0.5733 0.4057 0.5451 0.5115 0.5401 0.5410\nMin 0.3250 0.3803 0.4342 0.3405 0.4586 0.4594 0.5327 0.5123\nSNSR 0.1658 0.1931 0.1391 0.0651 0.0865 0.0521 0.0075 0.0288Jaccard@20\nSNSV 0.0619 0.0604 0.0572 0.0307 0.0351 0.0229 0.0037 0.0122\nMax 0.1956 0.2315 0.2315 0.1709 0.2248 0.2106 0.2227 0.2299\nMin 0.1262 0.1579 0.1819 0.1430 0.1934 0.1929 0.2217 0.2086\nSNSR 0.0694 0.0736 0.0496 0.0279 0.0314 0.0177 0.0009 0.0212SERP*@20\nSNSV 0.0275 0.0224 0.0207 0.0117 0.0123 0.0065 0.0005 0.0089\nMax 0.6304 0.7049 0.7049 0.5538 0.7051 0.6595 0.6917 0.6837\nMin 0.4113 0.4904 0.5581 0.4377 0.6125 0.6020 0.6628 0.6739\nSNSR 0.2191 0.2145 0.1468 0.1162 0.0926 0.0575 0.0289 0.0098\nMovie\nPRAG*@20\nSNSV 0.0828 0.0689 0.0601 0.0505 0.0359 0.0227 0.0145 0.0040\ngroup to the neutral group while truncating the length of the rec-\nommendation list for the most unfair four sensitive attributes. Based\non the table and figures, we have made the following observations:\nâ€¢For both movie and music recommendations, ChatGPT demon-\nstrates unfairness across the most sensitive attributes. In each\ndataset, each similarity metric exhibits a similar level of values\nover different sensitive attributes ( c.f., Max and Min), but the\ncorresponding fairness metrics (ğ‘†ğ‘ğ‘†ğ‘… and ğ‘†ğ‘ğ‘†ğ‘‰ ) exhibit vary-\ning levels of values. This indicates that the degree of unfairness\nvaries across sensitive attributes. In the music dataset, the four\nattributes with the highest value ofğ‘†ğ‘ğ‘†ğ‘‰ for ğ‘ƒğ‘…ğ´ğºâˆ—are religion,\ncontinent, occupation , and country. In the movie dataset, the four\nattributes are race, country, continent , and religion.\nâ€¢As shown in Figure 2, the difference in similarity consistently\npersists when truncating the recommendation list to different\nlengths (ğ¾), and the relative order of different values of sensitive\nattributes remains mostly unchanged. This suggests that the issue\nof unfairness persists even when the length of recommendation\nlists is changed. Similar phenomena are observed for the undrawn\nattributes, but we omit them to save space.\nâ€¢In most cases, ChatGPTâ€™s disadvantaged groups (i.e., those with\nsmaller values of similarity metrics) regarding different sensitive\nattributes align with the inherent social cognition of the real\nworld. For example, in terms of the attribute â€”continent, â€œAfricanâ€\nis the disadvantaged group. Such unfairness should be minimized\nin the recommendations made by RecLLM.\n4.2 Unfairness Robustness Analyses (RQ2)\nWe analyze the robustness of unfairness, i.e., whether similar un-\nfairness persists when there are typos in sensitive attributes or\nwhen different languages are used for instructions. Due to space\nconstraints, we conduct the robustness analysis on the attribute\nâ€” continent, which is one of the most consistently unfair sensitive\nattributes in Table 1.\n4.2.1 The Influence of Sensitive Attribute Typos. To investigate the\ninfluence of typos in sensitive attributes on the unfairness of Re-\ncLLM, we focus on two values of the attribute â€”continent: â€œAfricanâ€\nand â€œAmericanâ€. Specifically, we create four typos by adding or sub-\ntracting letters, resulting in â€œAfrianâ€, â€œAmerianâ€, â€œAmericcanâ€, and\nâ€œAfriccanâ€. We then conduct experiments on these typos and the\nright ones and compute their similarity to the neutral group. The\nresults are shown in the left two subfigures of Figure 3. We observe\nthat â€œAfrianâ€ and â€œAfriccanâ€, which are closer to the disadvantaged\ngroup â€œAfricanâ€, are less similar to the neutral group, exhibiting rel-\natively higher levels of disadvantage. This indicates that the closer a\ntypo is to a vulnerable sensitive value, the more likely it is to result\nin being disadvantaged, highlighting the persistence of unfairness\nin RecLLM.\n4.2.2 The Influence of Language. In addition, we analyze the influ-\nence of language on unfairness by using Chinese instructions. The\nright two subfigures of Figure 3 summarize the similarity results for\nthe attribute â€œcontinentâ€. Compared to the results obtained using\nRecSys â€™23, September 18â€“22, 2023, Singapore, Singapore Jizhi Zhang*, Keqin Bao*, Yang Zhang, Wenjie Wang, Fuli Feng â€ , and Xiangnan Heâ€ \n2 4 6 8 10 12 14 16 18 20\n@K\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80PRAG*@K\nMusic Religion\nBuddhist \nChristian \nIslamic \n2 4 6 8 10 12 14 16 18 20\n@K\n0.76\n0.78\n0.80\n0.82\n0.84\n0.86\n0.88\nMusic Continent\nAfrican \nAmerican \nAsian \n2 4 6 8 10 12 14 16 18 20\n@K\n0.76\n0.78\n0.80\n0.82\n0.84\n0.86\n0.88\nMusic Occupation\ndoctor \nstudent \nteacher \nworker \nwriter \n2 4 6 8 10 12 14 16 18 20\n@K\n0.72\n0.74\n0.76\n0.78\n0.80\n0.82\n0.84\n0.86\n0.88\nMusic Country\nAmerican \nBrazilian \nBritish \nChinese \nFrench \nGerman \nJapanese \n2 4 6 8 10 12 14 16 18 20\n@K\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65PRAG*@K\nMovie Race\nAfrican American \nblack \nwhite \nyellow \n2 4 6 8 10 12 14 16 18 20\n@K\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\nMovie Country\nAmerican \nBrazilian \nBritish \nChinese \nFrench \nGerman \nJapanese \n2 4 6 8 10 12 14 16 18 20\n@K\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\nMovie Continent\nAfrican \nAmerican \nAsian \n2 4 6 8 10 12 14 16 18 20\n@K\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\nMovie Religion\nBuddhist \nChristian \nIslamic \nFigure 2: Similarities of sensitive groups to the neutral group with respect to the length ğ¾of the recommendation List, measured\nby PRAG*@K, for the four sensitive attributes with the highest SNSV of PRAG*@20. The top four subfigures correspond to\nmusic recommendation results with ChatGPT, while the bottom four correspond to movie recommendation results.\n2 4 6 8 10 12 14 16 18 20\n@K\n0.76\n0.78\n0.80\n0.82\n0.84\n0.86\n0.88PRAG*@K\nMusic Typo\nAmerican \nAmericcan \nAmerian \nAfrian \nAfriccan \nAfrican \n2 4 6 8 10 12 14 16 18 20\n@K\n0.64\n0.66\n0.68\n0.70\n0.72\n0.74\n0.76\n0.78\nMovie Typo\nAmerican \nAmericcan \nAmerian \nAfrian \nAfriccan \nAfrican \n2 4 6 8 10 12 14 16 18 20\n@K\n0.72\n0.74\n0.76\n0.78\n0.80\n0.82\n0.84\n0.86\nMusic Chinese\n (African)\n (American)\n (Asian)\n2 4 6 8 10 12 14 16 18 20\n@K\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\nMovie Chinese\n (African)\n (American)\n (Asian)\nFigure 3: Fairness evaluation of ChatGPT when appearing typos in sensitive attributes (the left two subfigures) or when using\nChinese prompts (the right two subfigures).\nEnglish prompts, we find that there are still distinct differences be-\ntween â€œAfricanâ€, â€œAmericanâ€, and â€œAsianâ€, with â€œAfricanâ€ and â€œAsianâ€\nremaining relatively disadvantaged compared to â€œAmericanâ€. This\nindicates the persistence of unfairness across different languages.\nAnother notable observation is that the similarity in the movie data\nis significantly lower when using Chinese prompts compared to\nEnglish prompts. This is because using a Chinese prompt on the\nmovie data can result in recommendation outputs that randomly\nmix both Chinese and English, naturally decreasing the similarity\nbetween recommendation results.\n5 CONCLUSION\nWith the advancement of LLMs, people are gradually recognizing\ntheir potential in recommendation systems [ 5, 7, 21, 39]. In this\nstudy, we highlighted the importance of evaluating recommenda-\ntion fairness when using LLMs for the recommendation. To better\nevaluate the fairness for RecLLM, we proposed a new evaluation\nbenchmark, named FaiRLLM, as well as a novel fairness evaluation\nmethod, several specific fairness metrics, and benchmark datasets\nspanning various domains with eight sensitive attributes. By con-\nducting extensive experiments using this benchmark, we found\nthat ChatGPT generates unfair recommendations, indicating the\npotential risks of directly applying the RecLLM paradigm. In the\nfuture, we will evaluate other LLMs such as text-davinci-003 and\nLLaMA [18], and design methods to mitigate the recommendation\nunfairness of RecLLM. Furthermore, the generative recommenda-\ntion has the potential to become the next recommendation par-\nadigm [36]. Our approach can also be regarded as a preliminary\nattempt to evaluate fairness in the generative recommendation of\ntext. In the future, we will also explore ways to measure fairness in\nother generative recommendation approaches.\nACKNOWLEDGMENTS\nThis work is supported by the National Natural Science Foundation\nof China (62272437), and the CCCD Key Lab of Ministry of Culture\nand Tourism.\nREFERENCES\n[1] Himan Abdollahpouri, Robin Burke, and Bamshad Mobasher. 2017. Controlling\nPopularity Bias in Learning-to-Rank Recommendation. In Proceedings of the\nEleventh ACM Conference on Recommender Systems (RecSys â€™17) . Association for\nComputing Machinery, 42â€“46.\n[2] Himan Abdollahpouri, Masoud Mansoury, Robin Burke, and Bamshad Mobasher.\n2019. The Unfairness of Popularity Bias in Recommendation. CoRR\nabs/1907.13286 (2019). http://arxiv.org/abs/1907.13286\n[3] Malak Abdullah, Alia Madain, and Yaser Jararweh. 2022. ChatGPT: Fundamentals,\nApplications and Social Impacts. In 2022 Ninth International Conference on Social\nNetworks Analysis, Management and Security (SNAMS) . 1â€“8.\n[4] Abubakar Abid, Maheen Farooqi, and James Zou. 2021. Large language models\nassociate Muslims with violence.Nature Machine Intelligence 3, 6 (2021), 461â€“463.\n[5] Qingyao Ai, Ting Bai, Zhao Cao, Yi Chang, Jiawei Chen, Zhumin Chen, Zhiyong\nCheng, Shoubin Dong, Zhicheng Dou, Fuli Feng, et al. 2023. Information Retrieval\nMeets Large Language Models: A Strategic Report from Chinese IR Community.\narXiv preprint arXiv:2307.09751 (2023).\n[6] Yuntao Bai et al. 2022. Constitutional AI: Harmlessness from AI Feedback. CoRR\nabs/2212.08073 (2022). arXiv:2212.08073\nIs ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation RecSys â€™23, September 18â€“22, 2023, Singapore, Singapore\n[7] Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan\nHe. 2023. TALLRec: An Effective and Efficient Tuning Framework to Align\nLarge Language Model with Recommendation. In Proceedings of the 17th ACM\nConference on Recommender Systems (RecSys â€™23) . Association for Computing\nMachinery.\n[8] Alex Beutel, Jilin Chen, Tulsee Doshi, Hai Qian, Li Wei, Yi Wu, Lukasz Heldt,\nZhe Zhao, Lichan Hong, Ed H. Chi, and Cristos Goodrow. 2019. Fairness in\nRecommendation Ranking through Pairwise Comparisons. In Proceedings of\nthe 25th ACM SIGKDD International Conference on Knowledge Discovery & Data\nMining, KDD 2019, Anchorage, AK, USA, August 4-8, 2019 . ACM, 2212â€“2220.\n[9] Asia J Biega, Krishna P Gummadi, and Gerhard Weikum. 2018. Equity of attention:\nAmortizing individual fairness in rankings. In The 41st international acm sigir\nconference on research & development in information retrieval . 405â€“414.\n[10] AndrÃ© Calero Valdez, Martina Ziefle, and Katrien Verbert. 2016. HCI for Recom-\nmender Systems: The Past, the Present and the Future. In Proceedings of the 10th\nACM Conference on Recommender Systems . Association for Computing Machinery,\n123â€“126.\n[11] BHM Custers. 2012. Predicting data that people refuse to disclose; how data min-\ning predictions challenge informational self-determination. Privacy Observatory\nMagazine 2012, 3 (2012).\n[12] Aakanksha Chowdhery et al. 2022. PaLM: Scaling Language Modeling with\nPathways. CoRR abs/2204.02311 (2022). arXiv:2204.02311\n[13] Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav\nKadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et al. 2022.\nRed teaming language models to reduce harms: Methods, scaling behaviors, and\nlessons learned. arXiv preprint arXiv:2209.07858 (2022).\n[14] Yunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong, Haofen Wang, and Jiawei\nZhang. 2023. Chat-REC: Towards Interactive and Explainable LLMs-Augmented\nRecommender System. arXiv preprint arXiv:2303.14524 (2023).\n[15] Yingqiang Ge, Xiaoting Zhao, Lucia Yu, Saurabh Paul, Diane Hu, Chu-Cheng\nHsieh, and Yongfeng Zhang. 2022. Toward Pareto efficient fairness-utility trade-\noff in recommendation through reinforcement learning. In Proceedings of the\nfifteenth ACM international conference on web search and data mining . 316â€“324.\n[16] Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A\nSmith. 2020. RealToxicityPrompts: Evaluating Neural Toxic Degeneration in\nLanguage Models. In Findings of the Association for Computational Linguistics:\nEMNLP 2020 . 3356â€“3369.\n[17] Jiawei Han, Jian Pei, and Hanghang Tong. 2022. Data mining: concepts and\ntechniques. Morgan kaufmann.\n[18] et al. Hugo Touvron. 2023. LLaMA: Open and Efficient Foundation Language\nModels. CoRR abs/2302.13971 (2023). arXiv:2302.13971\n[19] Ben Hutchinson, Vinodkumar Prabhakaran, Emily Denton, Kellie Webster, Yu\nZhong, and Stephen Denuyl. 2020. Social Biases in NLP Models as Barriers\nfor Persons with Disabilities. In Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics . 5491â€“5501.\n[20] Christoph Leiter, Ran Zhang, Yanran Chen, Jonas Belouadi, Daniil Larionov,\nVivian Fresen, and Steffen Eger. 2023. ChatGPT: A Meta-Analysis after 2.5\nMonths. CoRR abs/2302.13795 (2023). arXiv:2302.13795\n[21] Ruyu Li, Wenhao Deng, Yu Cheng, Zheng Yuan, Jiaqi Zhang, and Fajie Yuan.\n2023. Exploring the Upper Limits of Text-Based Collaborative Filtering Using\nLarge Language Models: Discoveries and Insights.arXiv preprint arXiv:2305.11700\n(2023).\n[22] Roger Zhe Li, JuliÃ¡n Urbano, and Alan Hanjalic. 2021. Leave No User Behind:\nTowards Improving the Utility of Recommender Systems for Non-Mainstream\nUsers (WSDM â€™21) . Association for Computing Machinery, New York, NY, USA,\n103â€“111.\n[23] Yunqi Li, Hanxiong Chen, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. 2021.\nUser-Oriented Fairness in Recommendation. In Proceedings of the Web Conference\n2021 (WWW â€™21) . Association for Computing Machinery, 624â€“632.\n[24] Yunqi Li, Hanxiong Chen, Shuyuan Xu, Yingqiang Ge, Juntao Tan, Shuchang\nLiu, and Yongfeng Zhang. 2022. Fairness in Recommendation: A Survey.\narXiv:2205.13619 [cs.IR]\n[25] Yunqi Li, Hanxiong Chen, Shuyuan Xu, Yingqiang Ge, and Yongfeng Zhang. 2021.\nTowards Personalized Fairness Based on Causal Notion. InProceedings of the 44th\nInternational ACM SIGIR Conference on Research and Development in Information\nRetrieval (SIGIR â€™21) . Association for Computing Machinery, 1054â€“1063.\n[26] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michi-\nhiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al.\n2022. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110\n(2022).\n[27] Morten Moshagen and Jochen Musch. 2011. Surveying Multiple Sensitive At-\ntributes using an Extension of the Randomized-Response Technique.International\nJournal of Public Opinion Research 24, 4 (09 2011), 508â€“523.\n[28] Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R. Bowman. 2020.\nCrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Lan-\nguage Models. In Proceedings of the 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP) . Association for Computational Linguis-\ntics, Online.\n[29] Tien T. Nguyen, Pik-Mai Hui, F. Maxwell Harper, Loren Terveen, and Joseph A.\nKonstan. 2014. Exploring the Filter Bubble: The Effect of Using Recommender\nSystems on Content Diversity. In Proceedings of the 23rd International Conference\non World Wide Web (WWW â€™14) . Association for Computing Machinery, 677â€“686.\n[30] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schul-\nman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell,\nPeter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022. Training lan-\nguage models to follow instructions with human feedback. CoRR abs/2203.02155\n(2022). arXiv:2203.02155\n[31] Hossein A Rahmani, Mohammadmehdi Naghiaei, Mahdi Dehghan, and Moham-\nmad Aliannejadi. 2022. Experiments on generalizability of user-oriented fairness\nin recommender systems. In Proceedings of the 45th International ACM SIGIR\nConference on Research and Development in Information Retrieval . 2755â€“2764.\n[32] Guilherme Ramos et al . 2020. On the negative impact of social influence in\nrecommender systems: A study of bribery in collaborative hybrid algorithms.\nInformation Processing & Management 57, 2 (2020), 102058.\n[33] Jiliang Tang, Xia Hu, and Huan Liu. 2013. Social recommendation: a review. Soc.\nNetw. Anal. Min. 3, 4 (2013), 1113â€“1133.\n[34] et al. Tom B. Brown. 2020. Language Models are Few-Shot Learners. In Ad-\nvances in Neural Information Processing Systems 33: Annual Conference on Neural\nInformation Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual .\n[35] Matus Tomlein, Branislav Pecher, Jakub Simko, Ivan Srba, Robert Moro, Elena Ste-\nfancova, Michal Kompan, Andrea Hrckova, Juraj Podrouzek, and Maria Bielikova.\n2021. An Audit of Misinformation Filter Bubbles on YouTube: Bubble Bursting\nand Recent Behavior Changes. In Proceedings of the 15th ACM Conference on\nRecommender Systems (RecSys â€™21) . Association for Computing Machinery, 1â€“11.\n[36] Wenjie Wang, Xinyu Lin, Fuli Feng, Xiangnan He, and Tat-Seng Chua. 2023.\nGenerative Recommendation: Towards Next-generation Recommender Para-\ndigm. CoRR abs/2304.03516 (2023). https://doi.org/10.48550/arXiv.2304.03516\narXiv:2304.03516\n[37] Xuezhi Wang, Nithum Thain, Anu Sinha, Flavien Prost, Ed H Chi, Jilin Chen,\nand Alex Beutel. 2021. Practical compositional fairness: Understanding fair-\nness in multi-component recommender systems. In Proceedings of the 14th ACM\nInternational Conference on Web Search and Data Mining . 436â€“444.\n[38] Yifan Wang, Weizhi Ma, Min Zhang, Yiqun Liu, and Shaoping Ma. 2023. A Survey\non the Fairness of Recommender Systems. 41, 3, Article 52 (feb 2023), 43 pages.\n[39] Zheng Yuan, Fajie Yuan, Yu Song, Youhua Li, Junchen Fu, Fei Yang, Yunzhu\nPan, and Yongxin Ni. 2023. Where to Go Next for Recommender Systems? ID-\nvs. Modality-based Recommender Models Revisited. In Proceedings of the 46th\nInternational ACM SIGIR Conference on Research and Development in Information\nRetrieval, SIGIR 2023, Taipei, Taiwan, July 23-27, 2023 , Hsin-Hsi Chen, Wei-Jou (Ed-\nward) Duh, Hen-Hsen Huang, Makoto P. Kato, Josiane Mothe, and Barbara Poblete\n(Eds.). ACM, 2639â€“2649. https://doi.org/10.1145/3539618.3591932\n[40] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui\nChen, Christopher Dewan, Mona T. Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov,\nMyle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali\nSridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. OPT: Open Pre-trained\nTransformer Language Models. CoRR abs/2205.01068 (2022). https://doi.org/10.\n48550/arXiv.2205.01068 arXiv:2205.01068\n[41] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,\nYingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang,\nYushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang,\nZikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A Survey of Large\nLanguage Models. CoRR abs/2303.18223 (2023). https://doi.org/10.48550/arXiv.\n2303.18223 arXiv:2303.18223\n[42] Ziwei Zhu, Jingu Kim, Trung Nguyen, Aish Fenton, and James Caverlee. 2021.\nFairness among new items in cold start recommender systems. In Proceedings\nof the 44th International ACM SIGIR Conference on Research and Development in\nInformation Retrieval . 767â€“776.",
  "topic": "Benchmark (surveying)",
  "concepts": [
    {
      "name": "Benchmark (surveying)",
      "score": 0.9172109961509705
    },
    {
      "name": "Computer science",
      "score": 0.7871099710464478
    },
    {
      "name": "Dilemma",
      "score": 0.6547578573226929
    },
    {
      "name": "Code (set theory)",
      "score": 0.5450040698051453
    },
    {
      "name": "Recommender system",
      "score": 0.4622345566749573
    },
    {
      "name": "Artificial intelligence",
      "score": 0.380546510219574
    },
    {
      "name": "Machine learning",
      "score": 0.3640265464782715
    },
    {
      "name": "Programming language",
      "score": 0.06444951891899109
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}