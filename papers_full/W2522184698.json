{
  "title": "Sparse Non-negative Matrix Language Modeling",
  "url": "https://openalex.org/W2522184698",
  "year": 2016,
  "authors": [
    {
      "id": "https://openalex.org/A5071928196",
      "name": "Joris Pelemans",
      "affiliations": [
        "KU Leuven",
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5021878400",
      "name": "Noam Shazeer",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5068010225",
      "name": "Ciprian Chelba",
      "affiliations": [
        "Google (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2118714763",
    "https://openalex.org/W1989705153",
    "https://openalex.org/W2110485445",
    "https://openalex.org/W2013196554",
    "https://openalex.org/W2134237567",
    "https://openalex.org/W2076094076",
    "https://openalex.org/W2075201173",
    "https://openalex.org/W1984635093",
    "https://openalex.org/W1970689298",
    "https://openalex.org/W2402268235",
    "https://openalex.org/W2024592335",
    "https://openalex.org/W2100714283",
    "https://openalex.org/W2611669587",
    "https://openalex.org/W1934041838",
    "https://openalex.org/W2399351785",
    "https://openalex.org/W2029530604",
    "https://openalex.org/W115367774",
    "https://openalex.org/W2096375461",
    "https://openalex.org/W1520465330",
    "https://openalex.org/W2138204974",
    "https://openalex.org/W98731357",
    "https://openalex.org/W2146502635",
    "https://openalex.org/W2437096199",
    "https://openalex.org/W2166096645",
    "https://openalex.org/W1590952807",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W2106433837",
    "https://openalex.org/W36903255",
    "https://openalex.org/W1605569224",
    "https://openalex.org/W2020382207",
    "https://openalex.org/W2078549297",
    "https://openalex.org/W2109664771",
    "https://openalex.org/W10704533",
    "https://openalex.org/W1965154800",
    "https://openalex.org/W1917432393",
    "https://openalex.org/W2915722758"
  ],
  "abstract": "We present Sparse Non-negative Matrix (SNM) estimation, a novel probability estimation technique for language modeling that can efficiently incorporate arbitrary features. We evaluate SNM language models on two corpora: the One Billion Word Benchmark and a subset of the LDC English Gigaword corpus. Results show that SNM language models trained with n-gram features are a close match for the well-established Kneser-Ney models. The addition of skip-gram features yields a model that is in the same league as the state-of-the-art recurrent neural network language models, as well as complementary: combining the two modeling techniques yields the best known result on the One Billion Word Benchmark. On the Gigaword corpus further improvements are observed using features that cross sentence boundaries. The computational advantages of SNM estimation over both maximum entropy and neural network estimation are probably its main strength, promising an approach that has large flexibility in combining arbitrary features and yet scales gracefully to large amounts of data.",
  "full_text": "Sparse Non-negative Matrix Language Modeling\nJoris Pelemans\nGoogle Inc.\nESAT, KU Leuven\njoris@pelemans.be\nNoam Shazeer\nGoogle Inc.\nnoam@google.com\nCiprian Chelba\nGoogle Inc.\nciprianchelba@google.com\nAbstract\nWe present Sparse Non-negative Matrix\n(SNM) estimation, a novel probability estima-\ntion technique for language modeling that can\nefﬁciently incorporate arbitrary features. We\nevaluate SNM language models on two cor-\npora: the One Billion Word Benchmark and\na subset of the LDC English Gigaword cor-\npus. Results show that SNM language models\ntrained with n-gram features are a close match\nfor the well-established Kneser-Ney models.\nThe addition of skip-gram features yields a\nmodel that is in the same league as the state-\nof-the-art recurrent neural network language\nmodels, as well as complementary: combin-\ning the two modeling techniques yields the\nbest known result on the One Billion Word\nBenchmark. On the Gigaword corpus further\nimprovements are observed using features that\ncross sentence boundaries. The computational\nadvantages of SNM estimation over both max-\nimum entropy and neural network estimation\nare probably its main strength, promising an\napproach that has large ﬂexibility in combin-\ning arbitrary features and yet scales gracefully\nto large amounts of data.\n1 Introduction\nA statistical language modelestimates probability\nvalues P(W) for strings of words W in a vocabu-\nlary Vwhose size can be in the tens or hundreds of\nthousands and sometimes even millions. Typically\nthe string W is broken into sentences, or other seg-\nments such as utterances in automatic speech recog-\nnition, which are often assumed to be conditionally\nindependent; we will assume that W is such a seg-\nment, or sentence.\nEstimating full sentence language models (Rosen-\nfeld et al., 2001) is computationally hard if one\nseeks a properly normalized probability model1 over\nstrings of words of ﬁnite length in V∗. A simple\nand sufﬁcient way to ensure proper normalization\nof the model is to decompose the sentence prob-\nability according to the chain rule and make sure\nthat the end-of-sentence symbol </S> is predicted\nwith non-zero probability in any context. With\nW = wN\n1 = w1,...,w N we get:\nP(wN\n1 ) =\nN∏\nk=1\nP(wk|wk−1\n1 ) (1)\nSince the parameter space of P(wk|wk−1\n1 ) is too\nlarge, the language model is forced to put the con-\ntext wk−1\n1 into an equivalence classdetermined by a\nfunction Φ(wk−1\n1 ). As a result,\nP(wN\n1 ) ∼=\nN∏\nk=1\nP(wk|Φ(wk−1\n1 )) (2)\nResearch in language modeling consists of ﬁnd-\ning appropriate equivalence classiﬁers Φ and meth-\nods to estimate P(wk|Φ(wk−1\n1 )). Arguably the most\nsuccessful paradigm in language modeling uses the\nn-gram equivalence classiﬁcation, that is, deﬁnes\nΦn-gram(wk−1\n1 ) .= wk−n+1,wk−n+2,...,w k−1\nOnce the form Φ(wk−1\n1 ) is speciﬁed, only the prob-\nlem of estimating P(wk|Φ(wk−1\n1 )) from training\ndata remains.\nIn order to outperform the n-gram equivalence\nclass, one must ﬁnd a way to leverage long-distance\ncontext. This can be done explicitly, e.g. by combin-\ning multiple arbitrary features (Rosenfeld, 1994), or\nimplicitly as is the case for the current state of the art\n1In some practical systems the constraint on using a properly\nnormalized language model is side-stepped at a gain in model-\ning power and simplicity, see e.g. Chen et al. (1998).\n329\nTransactions of the Association for Computational Linguistics, vol. 4, pp. 329–342, 2016. Action Editor: Jason Eisner.\nSubmission batch: 12/2014; Revision batch: 7/2015; 11/2015; 4/2016; 6/2016; Published 7/2016.\nc⃝2016 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license.\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00102 by guest on 05 November 2025\nrecurrent neural network language models (Mikolov,\n2012). Unfortunately, either method comes at a\nlarge computational cost which makes training and\nevaluation on a large corpus impractical.\nIn this paper we present a novel probability esti-\nmation technique, called Sparse Non-negative Ma-\ntrix (SNM) estimation. Although SNM estimation\nis a general approach that can be applied to many\nproblems, its efﬁcient combination of arbitrary fea-\ntures makes it particularly interesting for language\nmodeling. We demonstrate this by training models\nwith variable-length n-gram features and skip-gram\nfeatures to incorporate long-distance context.\nThe paper is organized as follows: Section 2 dis-\ncusses work that is related to SNM which is de-\nscribed in Section 3. We then present a complex-\nity analysis in Section 4 and experimental results on\ntwo English corpora in Sections 5 and 6. We end\nwith conclusions and future work in Section 7.\n2 Related Work\n2.1 Neural networks\nRecently, neural networks (NN) (Bengio et al.,\n2003; Emami, 2006; Schwenk, 2007), and in par-\nticular recurrent neural networks (RNN) (Mikolov,\n2012; Sundermeyer et al., 2012) have shown ex-\ncellent performance in language modeling (Chelba\net al., 2014). RNNLMs have two main advantages\nover n-gram language models: 1) they learn a low-\ndimensional continuous vector representation for\nwords which allows them to discover ﬁne-grained\nsimilarities between words; 2) they are capable of\nmodeling dependencies that span over longer dis-\ntances, i.e. they can extend the context past the n-\ngram window. Their main disadvantage however is\nthat they take a long time to train and evaluate.\n2.2 Feature-based models\nAnother popular method to leverage long-distance\ncontext is Maximum Entropy (ME) (Rosenfeld,\n1994). ME is interesting because it can mix dif-\nferent types of features extracted from large context\nwindows, e.g. n-gram, skip-gram, bag-of-word and\nsyntactic features. Unfortunately it suffers from the\nsame drawback as neural networks, as we will see in\nSection 2.4.\nThe above-mentioned features can also be used in\nother ways, e.g. Chelba and Jelinek (2000) use a left-\nto-right syntactic parser to identify long-distance de-\npendencies (at sentence level), whereas approaches\nsuch as Bellegarda (2000) leverage latent semantic\ninformation (at document level). Tan et al. (2012) in-\ntegrate both syntactic and topic-based modeling with\nn-grams in a uniﬁed approach.\n2.3 Skip-grams\nThe type of long-distance features that we incor-\nporate into our SNMLMs are skip-grams (Huang\net al., 1993; Ney et al., 1994; Rosenfeld, 1994),\nwhich can effectively capture dependencies across\nlonger contexts. We are not the ﬁrst to highlight this\neffectiveness; previous such results were reported\nin Singh and Klakow (2013). Recently, Pickhardt\net al. (2014) also showed that a backoff generaliza-\ntion using single skips yields signiﬁcant perplexity\nreductions. We note though that our SNMLMs are\ntrained by mixing single as well as longer skips,\ncombining both in one model. More fundamentally,\nthe SNM model parameterization and method of es-\ntimation are completely original, as far as we know.\nIn our approach, a skip-gram feature extracted\nfrom the context wk−1\n1 is characterized by the tuple\n(r,s,a) where:\n•rdenotes the number of remote context words\n•sdenotes the number of skipped words\n•adenotes the number of adjacent context words\nrelative to the target word wk being predicted. The\nwindow size of a feature extractor then corresponds\nto r + s + a. For example, in the sentence <S>\nThe quick brown fox jumps over the\nlazy dog </S> a (1,2,3) skip-gram feature for\nthe target word dog is:\n[brown skip-2 over the lazy]\nFor performance reasons, it is recommended to\nlimit sand to limit either (r+ a) or both rand s.\nWe conﬁgure the skip-gram feature extractor to\nproduce all features F, deﬁned by the equivalence\nclass Φ(wk−1\n1 ), that meet constraints on the mini-\nmum and maximum values for:\n•the number of context words r+ a\n•the number of remote words r\n•the number of adjacent words a\n•the skip length s\n330\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00102 by guest on 05 November 2025\nWe also allow the option of not including the ex-\nact value of sin the feature representation; this may\nhelp with smoothing by sharing counts for various\nskip features. The resulting tied skip-gram features\nwill look like:\n[curiosity skip-* the cat]\nIn order to build a good probability estimate for\nthe target word wk in a context wk−1\n1 we need a way\nof combining an arbitrary number of skip-gram fea-\ntures, which do not fall into a simple hierarchy like\nregular n-gram features. The standard way to com-\nbine such predictors is ME, but it is computationally\nhard. The proposed SNM estimation on the other\nhand is capable of combining such predictors in a\nway that is computationally easy, scales up grace-\nfully to large amounts of data and as it turns out is\nalso very effective from a modeling point of view.\n2.4 Log-linear models\nNeural networks and ME are related in the sense that\nfor both models P(wk|Φ(wk−1\n1 )) takes the follow-\ning form:\nP(wk|Φ(wk−1\n1 )) = exp(ˆywk )∑\nt′∈V\nexp(ˆyt′)\n(3)\nwhere the ˆyt′ are the unnormalized log-probabilities\nfor each potential target word t′and depend on the\nmodel in question. For a ME model with featuresF,\nthey can be represented as follows:\nˆ y= xTM (4)\nwhere x is the word feature activation vector andM\nis a |F|×|V| feature weight matrix. The ˆyi of neural\nnetworks on the other hand are computed as follows:\nˆ y= g(xTH)W (5)\nwhere g(·) is the activation function of the hidden\nlayer (typically a tanh or sigmoid) and W and H\nare weight matrices for the output and hidden layer\nrespectively. Feed-forward and recurrent neural net-\nworks differ only in their input vectors x: in a feed-\nforward neural network, x is a concatenation of the\ninput features whereas in a recurrent neural network,\nx is a concatenation of the input word with the pre-\nvious hidden state. Because of their shared log-\nlinearity, training and evaluating these models be-\ncomes computationally complex.\nAlthough log-linear models have been shown to\nperform better than linear models (Klakow, 1998),\ntheir performance is also hampered by their com-\nplexity and we will show in the rest of the paper that\na linear model can in fact compete with the state of\nthe art when trained with variable-lengthn-gram and\nskip-gram features combined.\n3 Sparse Non-negative Matrix Estimation\n3.1 Linear model\nContrary to neural networks and ME, SNM language\nmodels do not estimate P(wk|Φ(wk−1\n1 )) in a log-\nlinear fashion, but are in fact linear models:\nP(wk|Φ(wk−1\n1 )) = ˆywk∑\nt′∈V\nˆyt′\n(6)\nwhere ˆ yis deﬁned as in Eq. (4).\nLike ME however, SNM uses features Fthat are\npredeﬁned and arbitrary, e.g. n-grams, skip-grams,\nbags of words, syntactic features, ... The features\nare extracted from the left context of wk and stored\nin a feature activation vector x = Φ(wk−1\n1 ), which\nis binary-valued, i.e. xf represents the presence or\nabsence of the feature with index f.\nIn what follows, we represent the target word wk\nby a vectory, which is a one-hot encoding of the vo-\ncabulary V: yt = 1 for t= wk, yt = 0 otherwise. To\nfurther simplify notation, we will not make the dis-\ntinction between a feature or target and its index, but\nrather denote both of them by f and t, respectively.\nThe ˆyt′ in SNM are computed in the same way as\nME, using Eq. (4), where M is a |F|×|V| feature\nweight matrix, which is sparse and non-negative.\nMft is indexed by feature f and target t and de-\nnotes the inﬂuence of feature f in the prediction\nof t. Plugging Eq. (4) into Eq. (6), we can de-\nrive the complete form of the conditional distribu-\ntion P(y|x) = P(wk|Φ(wk−1\n1 )) in SNMLMs:\nP(y|x) = (xTM)wk∑\nt′∈V(xTM)t′\n=\n∑\nf′∈Fxf′Mf′wk\n∑\nt′∈V\n∑\nf′∈Fxf′Mf′t′\n=\n∑\nf′∈Fxf′Mf′wk\n∑\nf′∈Fxf′\n∑\nt′∈VMf′t′\n(7)\n331\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00102 by guest on 05 November 2025\nAs required by the denominator in Eq. (7), this\ncomputation also involves summing over all the\npresent features for the entire vocabulary. However,\nbecause of the linearity of the model, we can pre-\ncompute the row sums ∑\nt′∈VMf′t′ for each f′and\nstore them together with the model. This means that\nthe evaluation can be done very efﬁciently, since the\nremaining summation involves a limited number of\nterms: even though the amount of features |F|gath-\nered over the entire training data is potentially huge,\nthe amount of active, non-zero features for a given\nx is small. For example, for SNM models using\nvariable-length n-gram features, the maximum num-\nber of active features is n; in our experiments with a\nlarge variety of skip-grams, it was around 100.\nNotice that this precomputation is not possible for\nthe log-linear ME which is otherwise similar, be-\ncause the sum over all features does not distribute\noutside the sum over all targets in the denominator:\nP(y|x) =\nexp(∑\nf′∈Fxf′Mf′wk )\n∑\nt′∈V\nexp(\n∑\nf′∈F\nxf′Mf′t′)\n(8)\nThis is a huge difference and essentially makes SNM\na more efﬁcient model at runtime.\n3.2 Adjustment function and meta-features\nWe let the entries of M be a slightly modiﬁed or\nadjusted version of the relative frequencies:\nMft = eA(f,t) Cft\nCf∗\n(9)\nwhere A(f,t) is a real-valued function, dubbed\nthe adjustment function (to be deﬁned below), and\nC is a feature-target count matrix, computed over\nthe entire training corpus T. Cft denotes the co-\noccurrence count of feature f and target t, whereas\nCf∗denotes the total occurrence count of feature f,\nsummed over all targets t′.\nAn unadjusted SNM model, where A(f,t) = 0 ,\nis a linear mixture of simple feature models P(t|f)\nwith uniform mixture weights. The adjustment func-\ntion enables the models to be weighted by the rela-\ntive importance of each input feature and, because it\nalso parameterized by t, takes into account the cur-\nrent target. The function is computed by a linear\nmodel on binary meta-features (Lee et al., 2007):\nA(f,t) = θ·h(f,t) (10)\nwhere h(f,t) is the meta-feature vector extracted\nfrom the feature-target pair (f,t).\nEstimating weights θk on the meta-feature level\nrather than the input feature level enables similar in-\nput features to share weights which improves gener-\nalization. We illustrate this by an example.\nGiven the word sequence the quick brown\nfox, we extract the following elementary meta-\nfeatures from the 3-gram feature the quick\nbrown and the target fox:\n•feature identity: [the quick brown]\n•feature type: 3-gram\n•feature count: Cf∗\n•target identity: fox\n•feature-target count: Cft\nWe also allow conjunctions of (single or multi-\nple) elementary meta-features to form more com-\nplex meta-features. This explains the absence of the\nfeature-target identity (and others, see Appendix A)\nin the above list: it is represented by the conjunc-\ntion of the feature and target identities. The result-\ning meta-features enable the model to share weights\nbetween, e.g. all 3-grams, all 3-grams that have tar-\nget fox, etc. Although these conjunctions may in\ntheory override Cft /Cf∗in Eq. (9), keeping the rel-\native frequencies allows us to train the adjustment\nfunction on part of the data (see also Section 3.4).\nWe apply smoothing to all of the count meta-\nfeatures: since count meta-features of the same order\nof magnitude carry similar information, we group\nthem so they can share weights. We do this by\nbucketing the count meta-features according to their\n(ﬂoored) log2 value. As this effectively puts the\nlowest count values, of which there are many, into\na different bucket, we optionally introduce a sec-\nond (ceilinged) bucket to assure smoother transi-\ntions. Both buckets are then weighted according to\nthe log2 fraction lost by the corresponding rounding\noperation. Pseudocode for meta-feature extraction\nand count bucketing is presented in Appendix A.\nTo control memory usage, we employ a feature\nhashing technique (Langford et al., 2007; Ganchev\nand Dredze, 2008) where we store the meta-feature\nweights in a ﬂat hash table θ of predeﬁned size.\nStrings are ﬁngerprinted (converted into a byte ar-\nray, then hashed), counts are hashed, and the result-\ning integer is mapped to an index in θby taking its\n332\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00102 by guest on 05 November 2025\nvalue modulo the pre-deﬁned size(θ). We do not\nprevent collisions, which has the potentially unde-\nsirable effect of tying together the weights of differ-\nent meta-features. However, as was previously ob-\nserved by Mikolov et al. (2011), when this happens\nthe most frequent meta-feature will dominate the ﬁ-\nnal value after training, which essentially boils down\nto a form of pruning. Because of this, the model\nperformance does not strongly depend on the size\nof the hash table. Note that we only apply hashing\nto the meta-feature weights: the adjusted and raw\nrelative frequencies are stored as SSTables (Sorted\nString Table).\n3.3 Model estimation\nAlthough it is in principle possible to use regularized\nmaximum likelihood to estimate the parameters of\nthe model, a gradient-based approach would end up\nwith parameter updates involving the gradient of the\nlog of Eq. (7) which works out to:\n∂log P(y|x)\n∂A(f,t) = xf Mft\n( yt\nˆywk\n− 1∑\nt′∈V\nˆyt′\n)\n(11)\nFor the complete derivation, see Appendix B. The\nproblem with this gradient is that we need to sum\nover the entire vocabulary Vin the denominator. In\nEq. (7) we could get away with this by precomput-\ning the row sums, but here the sums change after\neach update. Instead, we were inspired by Xu et al.\n(2011) and chose to use an independent binary pre-\ndictor for each word in the vocabulary during esti-\nmation. Our approach however differs from Xu et al.\n(2011) in that we do not use |V|Bernoullis, but |V|\nPoissons2, using the fact that for a large number of\ntrials a Bernoulli with small pis well approximated\nby a Poisson with small λ.\nIf we consider each yt′ in y to be Poisson dis-\ntributed with parameter ˆyt′, the conditional proba-\nbility PPois(y|x) is given by:\nPPois(y|x) =\n∏\nt′∈V\nˆyyt′\nt′ e−ˆyt′\nyt′! =\n∏\nt′∈V\nˆyyt′\nt′ e−ˆyt′ (12)\n2We originally chose Poisson so we could apply the model\nto tasks with outputs yt > 1. More recent experiments using a\nmultinomial loss can be found in Chelba and Pereira (2016).\nand the gradient of the log-probability works out to:\n∂log PPois(y|x)\n∂A(f,t) = xf Mft\n( yt\nˆywk\n−1\n)\n(13)\nFor the complete derivation, see Appendix C.\nThe parameters θ of the adjustment function are\nlearned by maximizing the Poisson log-probability,\nusing stochastic gradient ascent. That is, for each\nfeature-target pair (f,t) we compute the gradient in\nEq. (13) and propagate it to the meta-feature weights\nθk by multiplying it with ∂A(f,t)/∂θk = hk. At\nthe Nth occurrence of feature-target pair (f,t), each\nweight θk is updated using the propagated gradient,\nweighted by a learning rate η:\nθk,N ←θk,N−1 + η∂N (f,t) (14)\nwhere ∂N (f,t) is a short-hand notation for the Nth\ngradient with respect to θk.\nRather than using a single ﬁxed learning rate, we\nuse AdaGrad (Duchi, 2011) which uses a separate\nadaptive learning rate ηk,N for each weight θk,N :\nηk,N = γ√\n∆0 + ∑N\nn=1 ∂n(f,t)2\n(15)\nwhere γ is a constant scaling factor for all learn-\ning rates and ∆0 is an initial accumulator constant.\nBasing the learning rate on historical information\ntempers the effect of frequently occurring features\nwhich keeps the weights small and as such acts as a\nform of regularization.\n3.4 Optimization and leave-one-out training\nEach feature-target pair (f,t) constitutes a training\nexample where examples with yt = 0 are called\nnegative and others positive. Using the short-hand\nnotations T = |T|, F = |F|and V = |V|, this\nmeans that the training data consists of approxi-\nmately TF(V −1) negative and only TF positive\ntraining examples. If we examine the two terms\nof Eq. (13) separately, we see that the ﬁrst term\nxf Mft\nyt\nˆywk\ndepends on yt which means it becomes\nzero for all the negative training examples. The sec-\nond term −xf Mft however does not depend on yt\nand therefore never becomes zero. This also means\nthat the total gradient is never zero and because of\n333\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00102 by guest on 05 November 2025\nthis, the vast amount of updates required for the neg-\native examples makes the update algorithm compu-\ntationally too expensive.\nTo speed up the algorithm we use a heuristic that\nallows us to express the second term as a function\nof yt, essentially redistributing the updates for the\nnumerous negative examples to the fewer positive\ntraining examples. Appendix D shows that for batch\ntraining this has the same effect if run over the entire\ncorpus. We note that for online training this is not\nstrictly correct, sinceMft changes after each update.\nNonetheless, we found this to yield good results as\nwell as seriously reducing the computational cost.\nAfter applying the redistribution, the online gradient\nthat is applied to each training example becomes:\n∂log PPois(y|x)\n∂A(f,t) = xf ytMft\n( 1\nˆywk\n−Cf∗\nCft\n)\n(16)\nwhich is non-zero only for positive training exam-\nples, hence making training independent of the size\nof the vocabulary.\nOne practical way to further prevent overﬁtting\nand adapt the model to a speciﬁc task is to use held-\nout data, i.e. compute the count matrix C on the\ntraining data and estimate the parameters θ on the\nheld-out data. Unfortunately, since the aggregated\ngradients in Eq. (16) tie the updates to the counts\nCf∗and Cft in the training data, they can’t differ-\nentiate between held-out and training data, which\nmeans that the meta-feature weights can’t be tuned\nspeciﬁcally to the held-out data. Experiments in\nwhich we tried to use the held-out counts instead\ndid not yield good results, presumably because we\nare violating the redistribution heuristic.\nRather than adding a regularizer on the meta-\nfeature weights, we instead opted for leave-one-out\ntraining. With the notation A(f,t,C f∗,Cft ) reﬂect-\ning the dependence of the adjustment function on\nfeature and feature-target counts, the gradient under\nleave-one-out training becomes:\nxf yt\n(\n( 1\nˆy+wk\n−1)M+\nft −Cf∗−Cft\nCft\nM−\nft\n)\n(17)\nwhere M−\nft , M+\nft and ˆy+\nwk are deﬁned as follows:\nM−\nft = eA(f,t,Cf∗−1,Cft ) Cft\nCf∗−1\nM+\nft = eA(f,t,Cf∗−1,Cft −1) Cft −1\nCf∗−1\nˆy+\nwk = (xT M+)wk\nThe full derivation can be found in Appendix E.\nWe note that in practice, it often sufﬁces to use\nonly a subset of the training examples for leave-one-\nout training, which has the additional advantage of\nspeeding up training even further.\n4 Complexity analysis\nBesides their excellent results, RNNs have also been\nshown to scale well with large amounts of data\nwith regards to memory and accuracy (Williams\net al., 2015). Compared to n-gram models which\ngrow huge very quickly with only modest improve-\nments, RNNs take up but a fraction of the memory\nand exhibit a near linear reduction in log perplex-\nity with log training words. Moreover, a larger hid-\nden layer can yield more improvements, whereas n-\ngram models quickly suffer from data sparsity. The\nproblem with RNNs however is that they are compu-\ntationally complex which makes training and evalua-\ntion slow. A standard Elman network (Elman, 1990)\nwith hidden layer of size H trained on a corpus of\nsize T with vocabulary of size V has complexity\nIT(H2 + HV) (18)\nwhere I indicates the number of iterations. Several\nattempts have been made to reduce training time, fo-\ncusing mostly on reducing the large factors T or V:\n•vocabulary shortlisting (Schwenk and Gauvain,\n2004)\n•subsampling (Schwenk and Gauvain, 2005; Xu\net al., 2011)\n•class-based (Goodman, 2001b; Morin and Ben-\ngio, 2005; Mikolov et al., 2011)\n•noise-contrastive estimation (Gutmann and\nHyv¨arinen, 2012; Chen et al., 2015)\nHowever, these techniques either come with a se-\nrious performance degradation (Le et al., 2013) or\n334\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00102 by guest on 05 November 2025\ndo not sufﬁciently speed up training. The class-\nbased implementation for example, still has a train-\ning computational complexity of:\nIT(H2 + HC + CVC) (19)\nwhere Cindicates the number of classes and VC the\nvariable amount of words in a class. Although this is\na signiﬁcant reduction in complexity, the dominant\nterm ITH2 is still large. The same applies to noise-\ncontrastive estimation.\nAs was shown in Mikolov et al. (2011), a Max-\nimum Entropy model can be regarded as a neural\nnetwork with direct connections for the features, i.e.\nit has no hidden layers. The model uses the same\nsoftmax activation at its output and its complexity\ntherefore also depends on the size of the vocabulary:\nIT(F+V) (20)\nwhere F+ ≪F denotes the number of active fea-\ntures. To achieve state-of-the-art results this model\nis often combined with an RNN, which yields a total\ncomplexity of:\nIT(H2 + HV + F+V) (21)\nThe computational complexity for training SNM\nmodels on the other hand is independent of V:\nTF+ + IT′F+Θ+ (22)\nwhere Θ+ is the number of meta-features for each\nof the F+ input features. The ﬁrst term is related\nto counting features and feature-target pairs and\nthe second term to training the adjustment model\non a subset T′ of the training data. If we com-\npare an SNMLM with typical values of F+ ≈100\nand Θ+ <40, to the RNNLM conﬁgurations with\nH = 1024 in Chelba et al. (2014) and Williams et\nal. (2015), we ﬁnd that training comes at a reduced\ncomplexity of at least two orders of magnitude.\nA even more striking difference in complexity can\nbe seen at test time. Whereas the complexity of\na class-based RNN for a single test step is propor-\ntional to H2+HC+CVC, testing SNMLMs is linear\nin F+ because of the reasons outlined in Section 3.1.\n5 Experiment 1: 1B Word Benchmark\nOur ﬁrst experimental setup used the One Billion\nWord Benchmark3 made available by Chelba et al.\n(2014). It consists of an English training and test\nset of about 0.8 billion and 159658 tokens, respec-\ntively. The vocabulary contains 793471 words and\nwas constructed by discarding all words with count\nbelow 3. OOV words are mapped to an <UNK> to-\nken which is also part of the vocabulary. The OOV\nrate of the test set is 0.28%. Sentence order is ran-\ndomized.\nAll of the described SNM models are initialized\nwith meta-feature weights θk = 0 which are up-\ndated using AdaGrad with accumulator ∆0 = 1\nand scaling factor γ = 0.02 over a single epoch of\n30M training examples. The hash table for the meta-\nfeatures was limited to 200M entries as increasing it\nyielded no signiﬁcant improvements.\n5.1 N-gram experiments\nIn the ﬁrst set of experiments, we used all variable-\nlength n-gram features that appeared at least once in\nthe training data up to a given length. This yields\nat most n active features: one for each m-gram of\nlength 0 ≤m < nwhere m= 0 corresponds to an\nempty featurewhich is always present and produces\nthe unigram distribution. The number of features is\nsmaller than nwhen the context is shorter thann−1\nwords (near sentence boundaries) and during evalua-\ntion where an n-gram that did not occur in the train-\ning data is discarded.\nWhen trained using these features, SNMLMs\ncome very close to n-gram models with interpo-\nlated Kneser-Ney (KN) smoothing (Kneser and Ney,\n1995), where no count cut-off was applied and the\ndiscount does not change with the order of the\nmodel. Table 1 shows that Katz smoothing (Katz,\n1987) performs considerably worse than both SNM\nand KN. KN and SNM are not very complementary\nas linear interpolation with weights optimized on the\ntest data only yields an additional perplexity reduc-\ntion of about 1%. The difference between KN and\nSNM becomes smaller when we increase the size of\nthe context, going from 5% for 5-grams to 3% for\n8-grams, which indicates that SNMLMs might be\nbetter suited to a large number of features.\n3http://www.statmt.org/lm-benchmark\n335\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00102 by guest on 05 November 2025\nn-gram order\nModel 5 6 7 8\nKN 67.6 64.3 63.2 62.9\nKatz 79.9 80.5 82.2 83.5\nSNM (proposed) 70.8 67.0 65.4 64.8\nKN+SNM 66.5 63.0 61.7 61.4\nTable 1: Perplexity results on the 1B Word Benchmark\nfor Kneser-Ney (KN), Katz and SNM n-gram models of\ndifferent order.\nModel PPL\nSNM5-skip (no n-grams) 69.8\n+ n-grams = SNM5-skip 54.2\n+ KN5 56.5\nSNM5-skip + KN5 53.6\nTable 2: Perplexity (PPL) results comparing two ways of\nadding n-grams to a ‘pure’ skip-gram SNM model (no\nn-grams): joint modeling (SNM5-skip) and linear inter-\npolation with KN5.\n5.2 Integrating skip-gram features\nTo incorporate skip-gram features, we can either\nbuild a ‘pure’ skip-gram SNMLM that contains no\nregular n-gram features (except for unigrams) and\ninterpolate this model with KN, or we can build a\nsingle SNMLM that has both the regularn-gram fea-\ntures and the skip-gram features. We compared the\ntwo approaches by choosing skip-gram features that\ncan be considered the skip-equivalent of 5-grams,\ni.e. they contain at most 4 context words. In particu-\nlar, we conﬁgured the following feature extractors:\n•1 ≤r≤3; 1 ≤s≤3; 1 ≤r+ a≤4\n•1 ≤r≤2; s≥4 (tied); 1 ≤r+ a≤4\nWe then built a model that uses both these features\nand regular 5-grams (SNM5-skip), as well as one\nthat only uses the skip-gram features (SNM5-skip\n(no n-grams)). In addition, both models were inter-\npolated with a KN 5-gram model (KN5).\nAs can be seen from Table 2, it is better to incor-\nporate all features into one single SNM model than\nto interpolate with a KN 5-gram model (KN5). This\nis not surprising as linear interpolation uses a ﬁxed\nweight for the evaluation of every word sequence,\nwhereas the SNM model applies a variable weight\nthat is dependent both on the context and the target\nword. Finally, interpolating the all-in-one SNM5-\nskip with KN5 yields almost no additional gain.\n5.3 Skip-gram experiments\nThe best SNMLM results so far (SNM10-skip) were\nachieved using 10-grams, together with skip-grams\ndeﬁned by the following feature extractors:\n•s= 1; 1 ≤r+ a≤5\n•r= 1; 1 ≤s≤10 (tied); 1 ≤r+ a≤4\nThis mixture of rich (large context) short-distance\nand shallow long-distance features enables the\nmodel to achieve state-of-the-art results. Table 3\ncompares its perplexity to KN5 as well as to the fol-\nlowing language models:\n•Stupid Backoff LM (SBO) (Brants et al., 2007)\n•Hierarchical Softmax Maximum Entropy LM\n(HSME) (Goodman, 2001b; Morin and Ben-\ngio, 2005)\n•Recurrent Neural Network LM with Maximum\nEntropy (RNNME) (Mikolov, 2012)\nDescribing these models however is beyond the\nscope of this paper. Instead we refer the reader\nto Chelba et al. (2014) for a detailed description.\nThe table also lists the number of model parame-\nters, which in the case of SNMLMs consist of the\nnon-zero entries and precomputed row sums of M.\nWhen we compare the perplexity of SNM10-skip\nwith the state-of-the-art RNNLM with 1024 hidden\nneurons (RNNME-1024), the difference is only 3%.\nMoreover, this small advantage comes at the cost of\nincreased training and evaluation complexity. Inter-\nestingly, when we interpolate the two models, we\nhave an additional gain of 20%, which shows that\nSNM10-skip and RNNME-1024 are also comple-\nmentary. As far as we know, the resulting perplex-\nity of 41.3 is already the best ever reported on this\ncorpus, beating the optimized combination of sev-\neral models, reported in Chelba et al. (2014) by 6%.\nFinally, interpolation over all models shows that the\ncontribution of other models as well as the additional\nperplexity reduction of 0.3 is negligible.\n5.4 Runtime experiments\nIn this Section we present actual runtimes to give\nsome idea of how the theoretical complexity analy-\nsis of Section 4 translates to a practical application.\n336\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00102 by guest on 05 November 2025\nModel Params PPL\nKN5 1.76 B 67.6\nSNM5 (proposed) 1.74 B 70.8\nSBO 1.13 B 87.9\nHSME 6 B 101.3\nSNM5-skip (proposed) 62 B 54.2\nSNM10-skip (proposed) 33 B 52.9\nRNNME-256 20 B 58.2\nRNNME-512 20 B 54.6\nRNNME-1024 20 B 51.3\nSNM10-skip + RNNME-1024 41.3\nKN5 + SBO + RNNME-512 + RNNME-1024 43.8\nALL 41.0\nTable 3: Number of parameters and perplexity (PPL) results on the 1B Word Benchmark for the proposed models,\ncompared to the models in Chelba et al. (2014).\nMore speciﬁcally, we compare the training runtime\n(in machine hours) of the best SNM model to the\nbest RNN and n-gram models:\n•KN5: 28 machine hours\n•SNM5: 115 machine hours\n•SNM10-skip: 487 machine hours\n•RNNME-1024: 5760 machine hours\nAs these models were trained using different archi-\ntectures (number of CPUs, type of distributed com-\nputing, etc.), a runtime comparison is inherently\nhard and we would therefore like to stress that these\nnumbers should be taken with a grain of salt. How-\never, based on the order of magnitude we can clearly\nconclude that SNM’s reduced training complexity\nshown in Section 4 translates to a substantial reduc-\ntion in training time compared to RNNs. Moreover,\nthe large difference between KN5 and SNM5 sug-\ngests that our vanilla implementation can be further\nimproved to achieve even larger speed-ups.\n6 Experiment 2: 44M Word Corpus\nIn addition to the experiments on the One Billion\nWord Benchmark, we also conducted experiments\non a small subset of the LDC English Gigaword cor-\npus. This has the advantage that the experiments are\nmore easily reproducible and, since this corpus pre-\nserves the original sentence order, it also allows us\nto investigate SNM’s capabilities of modeling phe-\nnomena that cross sentence boundaries.\nThe corpus is the one used in Tan et al. (2012),\nwhich we acquired with the help of the authors\nand is now available at http://www.esat.\nkuleuven.be/psi/spraak/downloads/4.\nIt consists of a training set of 44M tokens, a\ncheck set of 1.7M tokens and a test set of 13.7M\ntokens. The vocabulary contains 56k words which\ncorresponds to an OOV rate of 0.89% and 1.98%\nfor the check and test set, respectively. OOV\nwords are mapped to an <UNK> token. The large\ndifference in OOV rate between the check and test\nset is explained by the fact that the training data\nand check data are from the same source (Agence\nFrance-Presse), whereas the test data is drawn from\nCNA (Central News Agency of Taiwan) which\nseems to be out of domain relative to the training\ndata. This discrepancy also shows in the perplexity\nresults, presented in Table 4.\nAll of the described SNM models are initialized\nwith meta-feature weights θk = 0 which are up-\ndated using AdaGrad with accumulator ∆0 = 1\nand scaling factor γ = 0.02 over a single epoch of\n10M training examples. The hash table for the meta-\nfeatures was limited to 10M entries as increasing it\nyielded no signiﬁcant improvements.\nWith regards to n-gram modeling, the results are\nanalogous to the 1B word experiment: SNM5 is\nclose to KN5; both outperform Katz5 by a large mar-\n4In order to comply with the LDC license, the data was en-\ncrypted using a key derived from the original data.\n337\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00102 by guest on 05 November 2025\ngin. This is the case for the check set and the test set.\nTan et al. (2012) showed that by crossing sen-\ntence boundaries, perplexities can be drastically re-\nduced. Although they did not publish any results\non the check set, their mixture of n-gram, syntac-\ntic language models and topic models achieved a\nperplexity of 176 on the test set, a 23% relative re-\nduction compared to KN5. A similar observation\nwas made for the SNM models by adding a feature\nextractor (r,s,a) analogous to regular skip-grams,\nbut with snow denoting the number of skipped sen-\ntence boundaries </S> instead of words. Adding\nskip-</S> features with r + a = 4 , 1 ≤r ≤2\nand 1 ≤s≤10, yielded an even larger reduction of\n26% than the one reported by Tan et al. (2012). On\nthe check set we observed a 25% reduction.\nThe RNNME results are achieved with a setup\nsimilar to the one in Chelba et al. (2014). The main\ndifferences are related to the ME features (3-grams\nonly instead of 10-grams and bag-of-words features)\nand the number of iterations over the training data\n(20 epochs instead of 10). These choices are related\nto the size of the training data. It can be seen from\nTable 4 that the best RNNME model outperforms the\nbest SNM model by 13% on the check set. The out-\nof-domain test set shows that due to its compactness,\nRNNME is better suited for LM adaptation.\n7 Conclusions and Future Work\nWe have presented SNM, a novel probability esti-\nmation technique for language modeling that can ef-\nﬁciently incorporate arbitrary features. A ﬁrst set\nof empirical evaluations on two data sets shows that\nSNM n-gram LMs perform almost as well as the\nwell-established KN models. When we add skip-\ngram features, the models are able to match the\nstate-of-the-art RNNLMs on the One Billion Word\nBenchmark (Chelba et al., 2014). Combining the\ntwo modeling techniques yields the best known re-\nsult on the benchmark which shows that the two\nmodels are complementary.\nOn a smaller subset of the LDC English Gigaword\ncorpus, SNMLMs are able to exploit cross-sentence\ndependencies and outperform a mixture of n-gram\nmodels, syntactic language models and topic mod-\nels. Although RNNLMs still outperform SNM by\n13% on this corpus, a complexity analysis and mea-\nPPL\nModel check test\nKN5 104.7 229.0\nKatz5 124.1 292.6\nSNM5 (proposed) 108.3 232.3\nSLM - 279\nn-gram/SLM - 243.0\nn-gram/PLSA - 196.0\nn-gram/SLM/PLSA - 176.0\nSNM5-skip (proposed) 89.5 198.4\nSNM10-skip (proposed) 87.5 195.3\nSNM5-skip-</S> (proposed) 79.5 176.0\nSNM10-skip-</S> (proposed) 78.4 174.0\nRNNME-512 70.8 136.7\nRNNME-1024 68.0 133.3\nTable 4: Perplexity (PPL) results on the 44M corpus. On\nthe small check set, SNM outperforms a mixture of n-\ngram, syntactic language models (SLM) and topic models\n(PLSA), but RNNME performs best. The out-of-domain\ntest set shows that due to its compactness, RNNME is\nbetter suited for LM adaptation.\nsured runtimes show that the RNN comes at an in-\ncreased training and evaluation time.\nWe conclude that the computational advantages of\nSNMLMs over both Maximum Entropy and RNN\nestimation promise an approach that has large ﬂexi-\nbility in combining arbitrary features effectively and\nyet scales gracefully to large amounts of data.\nFuture work includes exploring richer features\nsimilar to Goodman (2001a), as well as richer meta-\nfeatures in the adjustment model. A comparison of\nSNM models with Maximum Entropy at feature par-\nity is also planned. One additional idea was pointed\nout to us by action editor Jason Eisner. Rather than\nusing one-hot target vectors which emphasizes ﬁt,\nit is possible to use low-dimensional word embed-\ndings. This would most likely yield a smaller model\nwith improved generalization.\n8 Acknowledgments\nWe would like to thank Mike Schuster for his\nhelp with training and evaluating the RNN models.\nThanks also go to Tan et al. who provided us with the\n44M word corpus and to action editor Jason Eisner\nand the anonymous reviewers whose helpful com-\nments certainly improved the quality of the paper.\n338\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00102 by guest on 05 November 2025\nAppendix A Meta-feature Extraction Pseudocode\n// Meta-features are represented as tuples (hash value, weight).\n// New meta-features are either added (metafeatures.Add(mf new)) or\n// joint (metafeatures.Join(mf new)) with the existing meta-features.\n// Strings are ﬁngerprinted, counts are hashed.\nfunction COMPUTE METAFEATURES (FeatureTargetPair pair)\n// feature-related meta-features\nmetafeatures = {}\nmetafeatures.Add(Fingerprint(pair.feature identity), 1.0)\nmetafeatures.Add(Fingerprint(pair.feature type), 1.0)\nlog count = log(pair.feature count) / log(2)\nbucket1 = ﬂoor(log count)\nbucket2 = ceil(log count)\nweight1 = bucket2 - log count\nweight2 = log count - bucket1\nmetafeatures.Add(Hash(bucket1), weight1)\nmetafeatures.Add(Hash(bucket2), weight2)\n// target-related meta-features\nmetafeatures.Join(Fingerprint(pair.target identity), 1.0)\n// feature-target-related meta-features\nlog count = log(pair.feature target count) / log(2)\nbucket1 = ﬂoor(log count)\nbucket2 = ceil(log count)\nweight1 = bucket2 - log count\nweight2 = log count - bucket1\nmetafeatures.Join(Hash(bucket1), weight1)\nmetafeatures.Join(Hash(bucket2), weight2)\nreturn metafeatures\nAppendix B Multinomial Gradient\n∂log Pmulti(y|x)\n∂A(f,t) =\n(∂log(xTM)wk\n∂Mft\n−∂log ∑\nt′∈V(xTM)t′\n∂Mft\n)∂Mft\n∂Aft\n=\n( 1\n(xTM)wk\n∂(xTM)wk\n∂Mft\n− 1∑\nt′∈V(xTM)t′\n∂∑\nt′∈V(xTM)t′\n∂Mft\n)\nMft\n=\n(xf yt\nˆywk\n− xf∑\nt′∈V(xTM)t′\n)\nMft\n= xf Mft\n( yt\nˆywk\n− 1∑\nt′∈Vˆyt′\n)\n339\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00102 by guest on 05 November 2025\nAppendix C Poisson Gradient\n∂log PPois(y|x)\n∂A(f,t) =\n(∂∑\nt′∈Vyt′log(xTM)t′\n∂Mft\n−∂∑\nt′∈V(xTM)t′\n∂Mft\n) ∂Mft\n∂A(f,t)\n=\n( 1\n(xTM)wk\n∂(xTM)wk\n∂Mft\n−xf\n)\nMft\n= xf Mft\n( yt\nˆywk\n−1\n)\nAppendix D Distributing Negative Updates\nOver the entire training set, adding Cf∗\nCft\nMft once on the target tthat occurs with feature f amounts to the\nsame as traversing all targets t′that co-occur with f in the training set and adding the term Mft to each:\nMft\n∑\n(f,t′)∈T\nxf = Cf∗\nCft\nMft Cft = Cf∗\nCft\nMft\n∑\n(f,t′)∈T\nxf yt′\nApplying this to the second term of the Poisson gradient, we get:\n∂log PPois(y|x)\n∂A(f,t) = xf Mft\nyt\nˆywk\n−xf Mft = xf Mft\nyt\nˆywk\n−xf ytMft\nCf∗\nCft\n= xf ytMft\n( 1\nˆywk\n−Cf∗\nCft\n)\nAppendix E Leave-one-out Training\nIn leave-one-out training we exclude the event that generates the gradients from the counts used to compute\nthose gradients. More speciﬁcally, for each training example (f,t) we let:\nCf∗←Cf∗−1 if xf = 1\nCft ←Cft −1 if xf = 1,yt = 1\nwhich means that the gradients for the positive and the negative examples are changed in a different way.\nSince Eq. (16) expresses the general update rule for both type of examples, we ﬁrst have to separate it into\nupdates for negative and positive examples and then adapt accordingly.\nIn particular, the second term of Eq. (16), i.e. −xf ytMft\nCf∗\nCft\nis a distribution of Cf∗−Cft negative and\nCft positive updates over Cft positive examples:\n−xf ytMft\nCf∗\nCft\n= −xf ytMft\n(Cf∗−Cft\nCft\n+ Cft\nCft\n)\n= −xf ytMft\nCf∗−Cft\nCft\n−xf ytMft\nFurthermore, recall that the ﬁrst term of Eq. (16), i.e. xf ytMft\nˆywk\nis non-zero only for positive examples, so\nit can be added to the positive updates. We can then apply leave-one-out to positive and negative updates\nseparately, ending up with:\n∂log PPois(y|x)\n∂A(f,t) = xf yt\n(\n( 1\nˆy+wk\n−1)M+\nft −Cf∗−Cft\nCft\nM−\nft\n)\nwhere M−\nft , M+\nft and ˆy+\nwk are deﬁned as follows:\nM−\nft = eA(f,t,Cf∗−1,Cft ) Cft\nCf∗−1\nM+\nft = eA(f,t,Cf∗−1,Cft −1) Cft −1\nCf∗−1\nˆy+\nwk = (xT M+)wk\n340\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00102 by guest on 05 November 2025\nReferences\nJerome Bellegarda. 2000. Exploiting Latent Semantic\nInformation in Statistical Language Modeling. Pro-\nceedings of the IEEE, 88(8), 1279–1296.\nYoshua Bengio, R ´ejean Ducharme, Pascal Vincent, and\nChristian Janvin. 2003. A Neural Probabilistic Lan-\nguage Model. Journal of Machine Learning Research,\n3, 1137–1155.\nThorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,\nand Jeffrey Dean. 2007. Large Language Models in\nMachine Translation. Proceedings of EMNLP, 858–\n867.\nCiprian Chelba and Frederick Jelinek. 2000. Struc-\ntured Language Modeling. Computer Speech and Lan-\nguage, 14(4), 283–332.\nCiprian Chelba, Tom´aˇs Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robinson.\n2014. One Billion Word Benchmark for Measuring\nProgress in Statistical Language Modeling. Proceed-\nings of Interspeech, 2635–2639.\nCiprian Chelba and Fernando Pereira. 2016. Multi-\nnomial Loss on Held-out Data for the Sparse Non-\nnegative Matrix Language Model. arXiv:1511.01574\n[cs.CL].\nStanley F. Chen, Kristie Seymore, and Ronald Rosenfeld.\n1998. Topic Adaptation for Language Modeling Us-\ning Unnormalized Exponential Models. Proceedings\nof ICASSP, 681–684.\nXie Chen, Xunying Liu, Mark Gales and Phil Woodland.\n2015. Recurrent Neural Network Language Model\nTraining with Noise Contrastive Estimation for Speech\nRecognition. Proceedings of ICASSP, 5411–5415.\nJohn Duchi, Elad Hazan and Yoram Singer. 2011. Adap-\ntive Subgradient Methods for Online Learning and\nStochastic Optimization. Journal of Machine Learn-\ning Research, 12, 2121–2159.\nJeffrey L. Elman. 1990. Finding Structure in Time. Cog-\nnitive Science, 14(2), 179–211.\nAhmad Emami. 2006. A Neural Syntactic Language\nModel. Ph.D. Thesis, Johns Hopkins University.\nJoshua T. Goodman. 2001a. A Bit of Progress in Lan-\nguage Modeling, Extended Version. Technical Report\nMSR-TR-2001-72.\nJoshua T. Goodman. 2001b. Classes for Fast Maximum\nEntropy Training. Proceedings of ICASSP, 561–564.\nMichael Gutmann and Aapo Hyv ¨arinen. 2012. Noise-\ncontrastive Estimation of Unnormalized Statistical\nModels, with Applications to Natural Image Statistics.\nJournal of Machine Learning Research, 13(1), 307–\n361.\nXuedong Huang, Fileno Alleva, Mei-Yuh Hwang, and\nRonald Rosenfeld. 1993. An Overview of the\nSPHINX-II Speech Recognition System. Computer\nSpeech and Language, 2, 137–148.\nSlava M. Katz. 1987. Estimation of Probabilities from\nSparse Data for the Language Model Component of a\nSpeech Recognizer. IEEE Transactions on Acoustics,\nSpeech and Signal Processing, 35(3), 400–401.\nDietrich Klakow. 1998. Log-linear Interpolation of Lan-\nguage Models. Proceedings of ICSLP, 1695–1698.\nReinhard Kneser and Hermann Ney. 1995. Improved\nBacking-Off for M-Gram Language Modeling. Pro-\nceedings of ICASSP, 181–184.\nJohn Langford, Lihong Li and Alex Strehl. 2007.\nV owpal Wabbit Online Learning Project. http://\nhunch.net/?p=309.\nKuzman Ganchev and Mark Dredze. 2008. Small Statis-\ntical Models by Random Feature Mixing. Proceedings\nof the ACL-2008 Workshop on Mobile Language Pro-\ncessing, 19–20.\nHai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-Luc\nGauvain and Franc ¸ois Yvon. 2013. Structured Output\nLayer Neural Network Language Models for Speech\nRecognition. IEEE Transactions on Audio, Speech &\nLanguage Processing, 21, 195–204.\nSu-in Lee, Vassil Chatalbashev, David Vickrey and\nDaphne Koller. 2007 Learning a Meta-level Prior for\nFeature Relevance from Multiple Related Tasks. Pro-\nceedings of ICML, 489–496.\nTom´aˇs Mikolov, Anoop Deoras, Daniel Povey, Luk ´as\nBurget and Jan Cernock ´y. 2011. Strategies for Train-\ning Large Scale Neural Network Language Models.\nProceedings of ASRU, 196–201.\nTom´aˇs Mikolov. 2012. Statistical Language Models\nBased on Neural Networks. Ph.D. Thesis, Brno Uni-\nversity of Technology.\nFrederic Morin and Yoshua Bengio. 2005. Hierarchical\nProbabilistic Neural Network Language Model. Pro-\nceedings of AISTATS, 246–252.\nHermann Ney, Ute Essen, and Reinhard Kneser. 1994.\nOn Structuring Probabilistic Dependences in Stochas-\ntic Language Modeling. Computer Speech and Lan-\nguage, 8, 1–38.\nRene Pickhardt, Thomas Gottron, Martin K¨orner, Paul G.\nWagner, Till Speicher, and Steffen Staab. 2014. A\nGeneralized Language Model as the Combination of\nSkipped n-grams and Modiﬁed Kneser-Ney Smooth-\ning. Proceedings of ACL, 1145–1154.\nRonald Rosenfeld. 1994. Adaptive Statistical Language\nModeling: A Maximum Entropy Approach. Ph.D.\nThesis, Carnegie Mellon University.\nRonald Rosenfeld, Stanley F. Chen, and Xiaojin Zhu.\n2001. Whole-sentence Exponential Language Mod-\nels: a Vehicle for Linguistic-Statistical Integration.\nComputer Speech and Language, 15, 55–73.\n341\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00102 by guest on 05 November 2025\nHolger Schwenk and Jean-Luc Gauvain. 2004. Neural\nNetwork Language Models for Conversational Speech\nRecognition. Proceedings of ICSLP, 1215-1218.\nHolger Schwenk and Jean-Luc Gauvain. 2005. Train-\ning Neural Network Language Models On Very Large\nCorpora. Proceedings of EMNLP, 201–208.\nHolger Schwenk. 2007. Continuous Space Language\nModels. Computer Speech and Language, 21, 492–\n518.\nMittul Singh and Dietrich Klakow. 2013. Comparing\nRNNs and Log-linear Interpolation of Improved Skip-\nmodel on Four Babel Languages: Cantonese, Pashto,\nTagalog, Turkish. Proceedings of ICASSP, 8416–\n8420.\nMartin Sundermeyer, Ralf Schl ¨uter, and Hermann Ney.\n2012. LSTM Neural Networks for Language Model-\ning. Proceedings of Interspeech, 194–197.\nMing Tan, Wenli Zhou, Lei Zheng and Shaojun Wang.\n2012. A Scalable Distributed Syntactic, Semantic, and\nLexical Language Model. Computational Linguistics,\n38(3), 631–671.\nPuyang Xu, Asela Gunawardana, and Sanjeev Khudan-\npur. 2011. Efﬁcient Subsampling for Training Com-\nplex Language Models. Proceedings of EMNLP,\n1128–1136.\nWill Williams, Niranjani Prasad, David Mrva, Tom Ash\nand Tony Robinson. 2015. Scaling Recurrent Neural\nNetwork Language Models. Proceedings of ICASSP,\n5391–5395.\n342\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00102 by guest on 05 November 2025",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8365349769592285
    },
    {
      "name": "Language model",
      "score": 0.7466797828674316
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.6434192061424255
    },
    {
      "name": "Word (group theory)",
      "score": 0.6344285011291504
    },
    {
      "name": "Principle of maximum entropy",
      "score": 0.5923527479171753
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5738365054130554
    },
    {
      "name": "Sentence",
      "score": 0.4999396800994873
    },
    {
      "name": "n-gram",
      "score": 0.4856482744216919
    },
    {
      "name": "Artificial neural network",
      "score": 0.4473445415496826
    },
    {
      "name": "Natural language processing",
      "score": 0.44374847412109375
    },
    {
      "name": "Flexibility (engineering)",
      "score": 0.4365575313568115
    },
    {
      "name": "Algorithm",
      "score": 0.3361552655696869
    },
    {
      "name": "Statistics",
      "score": 0.09524264931678772
    },
    {
      "name": "Mathematics",
      "score": 0.08641451597213745
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I99464096",
      "name": "KU Leuven",
      "country": "BE"
    },
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    }
  ],
  "cited_by": 5
}