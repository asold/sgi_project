{
  "title": "Pre-training Language Models with Deterministic Factual Knowledge",
  "url": "https://openalex.org/W4385573374",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2097620437",
      "name": "Shaobo Li",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2107872138",
      "name": "Xiaoguang Li",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A2151842933",
      "name": "Lifeng Shang",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A2097149160",
      "name": "Chengjie Sun",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2107390885",
      "name": "Bingquan Liu",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2097306026",
      "name": "Zhenzhou Ji",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097242334",
      "name": "Xin Jiang",
      "affiliations": [
        "Harbin Institute of Technology",
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A2109590494",
      "name": "Qun Liu",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3100283070",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3176119108",
    "https://openalex.org/W2963073217",
    "https://openalex.org/W3002072934",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W3007672467",
    "https://openalex.org/W3176793246",
    "https://openalex.org/W3153094109",
    "https://openalex.org/W3113280695",
    "https://openalex.org/W2988421999",
    "https://openalex.org/W3098350697",
    "https://openalex.org/W2785611959",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W3165691894",
    "https://openalex.org/W4247637183",
    "https://openalex.org/W2969792713",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W4287646293",
    "https://openalex.org/W3202712981",
    "https://openalex.org/W2938830017",
    "https://openalex.org/W2119466907",
    "https://openalex.org/W3166986030",
    "https://openalex.org/W3210120707",
    "https://openalex.org/W3173169192",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3173673636",
    "https://openalex.org/W3104263050",
    "https://openalex.org/W2994915912",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3173611505",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2210387432",
    "https://openalex.org/W3044438666"
  ],
  "abstract": "Previous works show that Pre-trained Language Models (PLMs) can capture factual knowledge. However, some analyses reveal that PLMs fail to perform it robustly, e.g., being sensitive to the changes of prompts when extracting factual knowledge. To mitigate this issue, we propose to let PLMs learn the deterministic relationship between the remaining context and the masked content. The deterministic relationship ensures that the masked factual content can be deterministically inferable based on the existing clues in the context. That would provide more stable patterns for PLMs to capture factual knowledge than randomly masking. Two pre-training tasks are further introduced to motivate PLMs to rely on the deterministic relationship when filling masks. Specifically, we use an external Knowledge Base (KB) to identify deterministic relationships and continuously pre-train PLMs with the proposed methods. The factual knowledge probing experiments indicate that the continuously pre-trained PLMs achieve better robustness in factual knowledge capturing. Further experiments on question-answering datasets show that trying to learn a deterministic relationship with the proposed methods can also help other knowledge-intensive tasks.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11118–11131\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nPre-training Language Models with Deterministic Factual Knowledge\nShaobo Li 1, Xiaoguang Li 2, Lifeng Shang 2,\nChengjie Sun 1, Bingquan Liu 1, Zhenzhou Ji 1, Xin Jiang 2 and Qun Liu 2\n1Harbin Institute of Technology, 2Huawei Noah’s Ark Lab\nshli@insun.hit.edu.cn, {sunchengjie, liubq, jizhenzhou}@hit.edu.cn\n{lixiaoguang11, shang.lifeng, Jiang.Xin, qun.liu}@huawei.com\nAbstract\nPrevious works show that Pre-trained Language\nModels (PLMs) can capture factual knowledge.\nHowever, some analyses reveal that PLMs fail\nto perform it robustly, e.g., being sensitive to\nthe changes of prompts when extracting factual\nknowledge. To mitigate this issue, we propose\nto let PLMs learn the deterministic relationship\nbetween the remaining context and the masked\ncontent. The deterministic relationship ensures\nthat the masked factual content can be determin-\nistically inferable based on the existing clues\nin the context. That would provide more stable\npatterns for PLMs to capture factual knowledge\nthan randomly masking. Two pre-training tasks\nare further introduced to motivate PLMs to rely\non the deterministic relationship when ﬁlling\nmasks. Speciﬁcally, we use an external Knowl-\nedge Base (KB) to identify deterministic rela-\ntionships and continuously pre-train PLMs with\nthe proposed methods. The factual knowledge\nprobing experiments indicate that the continu-\nously pre-trained PLMs achieve better robust-\nness in factual knowledge capturing. Further\nexperiments on question-answering datasets\nshow that trying to learn a deterministic rela-\ntionship with the proposed methods can also\nhelp other knowledge-intensive tasks.\n1 Introduction\nPetroni et al. (2019); Jiang et al. (2020); Shin\net al. (2020); Zhong et al. (2021) show that we can\nsuccessfully extract factual knowledge from Pre-\ntrained Language Models (PLMs) using cloze-style\nprompts such as “The director of the ﬁlm Saving\nPrivate Ryan is [ MASK ].” Some recent works ( Cao\net al. , 2021; Pörner et al. , 2020) ﬁnd that the PLMs\nmay rely on superﬁcial cues to achieve that and can\nnot respond robustly. Table 1 gives examples of\ninconsistent predictions exposed by changing the\nsurface forms of prompts on the same fact.\nThis phenomenon questions whether PLMs\ncan robustly capture factual knowledge through\nMasked Language Modeling (MLM) ( Devlin et al. ,\nCloze-style Prompt and Prediction Is Correct?\nWar Horse is an American war ﬁlm di-\nrected by Steven Spielberg . ✓\nThe director of the American war ﬁlm War\nHorse is Keanu Reeves . ✗\nChristopher Nolan is the director of the\nAmerican war ﬁlm War Horse. ✗\nTable 1: A PLM could gives inconsistent results when\nprobing the same fact with different prompts. The un-\nderlined words are the predictions.\n2018) and further intensify us to inspect the masked\ncontents in the pre-training samples. After review-\ning several masking methods, we ﬁnd that they fo-\ncus on limiting the granularity of masked contents,\ne.g., restricting the masked content to be entities\nand then randomly masking the entities ( Guu et al. ,\n2020), and pay less attention to checking whether\nthe obtained MLM samples are appropriate for fac-\ntual knowledge capturing. For instance, when we\nwant PLMs to capture the corresponding factual\nknowledge as recovering the masked entities, we\nshould check whether the remaining context pro-\nvides sufﬁcient clues to recover the missing entity.\nInspired by the above analysis, we can categorize\nMLM samples based on the relationship between\nthe remaining context and masked content:\n• Non-deterministic samples The clues in the\nremaining context are insufﬁcient to constrain\nthe value of the masked content. Multiple\nvalues are valid to ﬁll in the masks.\n• Deterministic samples The remaining con-\ntext holds deterministic clues for the masked\ncontent. We can get one and only one valid\nvalue for the masked content.\nFor example, the ﬁrst cloze in Table 1 masks the\ndirector of the ﬁlm “ War Horse .” Since the ﬁlm\nhas only one director in the real world, we can get\na unique answer deterministically. So it is a de-\nterministic MLM sample. The crucial clues “ War\nHorse” and “ directed by ” have a deterministic rela-\n11118\ntionship with the missing entity “ Steven Spielberg .”\nFor brevity, we refer to these clues as determin-\nistic clues and the outcome “ Steven Spielberg \" as\ndeterministic span . In contrast, if the sample be-\ncomes “[ MASK ]s is an American war ﬁlm directed\nby Steven Spielberg ,” multiple names can ﬁll the\nmasks because Steven Spielberg produced more\nthan one American war ﬁlm. We cannot tell which\none is better based on the existing clues, so it is a\nnon-deterministic sample.\nThe non-deterministic samples establish a multi-\nlabel problem ( Zhang and Zhou , 2006) for MLM,\nwhere more than one ground-truth value for out-\nputs is associated with a single input. If we enforce\nthe PLMs to promote one speciﬁed ground truth\nover others, the other ground truths become false\nnegatives that could plague the training or cause a\nperformance downgrade ( Durand et al. , 2019; Cole\net al. , 2021). The non-deterministic samples are\ncompetent for obtaining contextualized representa-\ntions but become questionable for understanding\nthe intrinsic relationship between factual entities.\nIn contrast, the deterministic samples are less con-\nfusing since the answer is always unique, providing\na stable relationship for PLMs to learn.\nTherefore, we propose deterministic masking\nthat always masks and predicts the deterministic\nspans in MLM pre-training to improve PLMs’ abil-\nity to capture factual knowledge. The deterministic\nclues and spans are identiﬁed based on a KB. Two\npre-training tasks, clue contrastive learning and\nclue classiﬁcation , are introduced to make PLMs\nmore aware of the deterministic clues when pre-\ndicting the missing entities. The clue contrastive\nlearning encourages PLMs to be more conﬁdent\nin prediction (\nVu et al. , 2019; Luo et al. , 2021)\nwhen the deterministic clues are unmasked. The\nclue classiﬁcation is to detect whether the remain-\ning context contains deterministic clues. The ex-\nperiments on the factual knowledge probing and\nquestion-answering tasks show the effectiveness of\nthe proposed methods.\nThe contributions of this paper are: (1) We pro-\npose to model the deterministic relationship in\nMLM samples to improve the robustness (i.e., both\nconsistency and accuracy) of factual knowledge\ncapturing. (2) We design two pre-training tasks\nto enhance the deterministic relationship between\nentities to earn further improvement on robustness.\n(3) The experiment results show that learning the\ndeterministic relationship is also helpful for other\nknowledge-intensive tasks, such as question an-\nswering.\n2 Methods\nSection 2.1 expatiates the deterministic masking,\nwhich includes how we align texts with triplets and\nidentify deterministic clues and spans in texts. The\nclue contrastive learning and clue classiﬁcation are\ndescribed in Sections 2.2 and 2.3, respectively.\n2.1 Deterministic Masking\nIn addition to masking only factual content, the de-\nterministic masking also constrains the remaining\ncontext and the masked content to have a determin-\nistic relationship: the remaining context should pro-\nvide conclusive clues to predict the masked content,\nand the valid value to ﬁll in the mask is unique.\nTo this end, we align each text with a KB triplet\nand match the spans in the text with ( subject, pred-\nicate, object) respectively. We select the spans\naligned with objects as the candidates to be masked\nfor pre-training. To further make the masked object\ndeterministic, we query the KB with the aligned\n(subject, predicate) and check whether the valid\nobject that exists in KB is unique.\nIf the KB emits this object exclusively, e.g., only\nthe aligned object can compose a valid triplet with\nthe aligned subject and predicate, the object is de-\nterministic. The object is non-deterministic if mul-\ntiple objects suit the aligned subject and predicate\nin the KB. The span aligned with the determin-\nistic object is a deterministic span, and it would\nbe masked to construct a deterministic MLM sam-\nple1. We pre-train PLMs on only the deterministic\nsamples.\nFigure 1 shows a deterministic sample aligned\nwith the triplet (“ War Horse ,” “ directed by ,”\n“Steven Spielberg ”). When querying KB with “ War\nHorse” as the subject and “ directed by ” as the pred-\nicate, the result object “ Steven Spielberg ” is unique\nbecause there is only one director who produced\nthis ﬁlm, so the ﬁrst sample is deterministic. In\ncontrast, when using “ Steven Spielberg ” and “ di-\nrecter of ” as the subject and the predicate, multiple\nvalid objects exist in KB, so the second sample is\nnon-deterministic and is ﬁltered out.\nBy dropping the non-deterministic samples, we\nprevent PLMs from having a crush on one object\nbut ignoring others that are also valid based on the\n1We put the detailed procedure (includes entity linking and\npredicate string matching) in Appendix B.\n11119\nAlign\nSelect\nSaving Private Ryan\nWar Horse\n \ndirected by Steven Spielberg\n    Non-deterministic inference\n  Deterministic inference  \nKnowledge Base\nTriplet ( subject  , predicate  ,  object ) subject predicate object\n1 . War Horse  is an American war film directed by  [MASK] s.\n2 . [MASK] s is an American war film directed by Steven Spielberg .\nR  \nS   \nPre-training samples:\nWar Horse  is an American war film directed by Steven Spielberg .\nText\nFigure 1: Construct a deterministic sample. The spans\nwith blue background correspond to entities (subject or\nobject), and the spans with yellow describe relations\n(predicate).\nexisting clues. While in the deterministic samples,\nthe relationship between the remaining clues and\nthe missing span is more stable and unambiguous.\nTraining on the deterministic samples encourages\nPLM to infer the missing object based on its deter-\nministic factual clues. It helps PLMs grasp a more\nsubstantial relationship between entities to model\nthe factual contents and could aid in accomplishing\nsome knowledge-intensive tasks.\n2.2 Clue Contrastive Learning\nTo stimulate PLMs to catch the deterministic re-\nlationship between entities, we design the pre-\ntraining task clue contrastive learning following\nthis intuition: PLMs should have more conﬁdence\nto generate a masked span when its determinis-\ntic clues exist in the context, and introduce a con-\ntrastive objective accordingly. We explain it with a\npair of samples in Figure 2. Figure 2a shows a deter-\nministic MLM sample that masks the span “ Steven\nSpielberg” and keeps its deterministic clues. Fig-\nure 2b masks both the deterministic clues and the\ndeterministic span. The remaining context in Fig-\nure 2a contains fewer [ MASK ]s and provides more\ninformation, naturally reducing the uncertainty in\nprediction. So PLMs should assign a higher proba-\nbility for the ground truth when giving the context\nin Figure 2a than Figure 2b.\nFormally, we use S and P to denote the deter-\nministic clues (subject and predicate) and O to\ndenote the masked deterministic span (object). R\nrepresents the random spans in the context other\nthan S, P, and O. The objective function that needs\n... War Horse is an American war film directed by [MASK]s ...\nPLM\n: subjectS : predicateP : objectO: random contentR\nLM Head\n Embedding Vectors\nˆˆˆˆ(| ,, )PO o S sP pR r \n(a) Deterministic sample: masks the deterministic span (ob-\nject) and keeps the deterministic clues (object and predicate).\n... [MASK]s is an American war film  [MASK]s [MASK]s  ...\nPLM\nLM Head\n Embedding Vectors\n[MASK] [MASK]ˆ ˆ(| , , )PO o S P R r  \n(b) Contrastive sample without deterministic clues: masks\nboth the deterministic clues and the deterministic span.\nFigure 2: The two samples in clue contrastive learning.\nThe ﬁrst sample (a) has a more informative context,\nso PLM should be more conﬁdent when predicting the\nmasked object O. The texts with purple background\ndenote the spans other than entities and relations.\nto be maximized is:\nP (O = ˆo | S = ˆs, P= ˆp, R= ˆr)\n−P (O = ˆo | S = [MASK ], P= [MASK ], R= ˆr), (1)\nS = [MASK ]\nand P = [MASK ] denote replacing\nthe deterministic clues with [ MASK ]s. ˆs, ˆp, ˆo and ˆr\nare the ground-truth values of the S, P, O and R,\nrespectively. P(O = ˆo | ·) denotes the probability\nthat the PLM correctly predicts the masked span O,\ni.e., the average probability that the PLM assigns\nto the ground-truth tokens. It is calculated by a\nLanguage Model Head (LM Head) based on the\nembedding of O from the PLMs.\nThis task encourages PLMs to give the ground\ntruth ˆo a higher probability when the determinis-\ntic clues exist in the context. It is somewhat con-\nservative since we consider the noise in the data\nconstruction. The objective is still reasonable even\nwhen the S, P, and R are randomly labeled. Raw\nwords are always more informative than the ordi-\nnary [ MASK ]s and can reduce the uncertainty of\nthe context ( Cover, 1999), so the uncertainty of\nprediction degrades accordingly ( Vu et al. , 2019;\nLuo et al. , 2021). On the other hand, this objective\ntrains PLMs to react to the changes in the con-\ntext, i.e., learning how to tune the output as the\ninput changes. We employ a large-scale KB as the\napproximation of real-world knowledge ( Reiter,\n1981) to get the pre-training samples.\n11120\n2.3 Clue Classiﬁcation\nThe clue classiﬁcation asks PLMs to classify what\nkinds of clues exist in the remaining context. After\nmasking the deterministic span O, we manipulate\nthe remaining context to generate three samples\nthat contain different kinds of contexts:\n(a) Keep deterministic clues: we only mask the\ndeterministic span O and leave its determin-\nistic clues untorched. It is the same as the\noriginal deterministic MLM sample shown in\nFigure 2a.\n(b) Mask deterministic clues: we mask O and\nits deterministic clues ( S and P). It is the\nsame as the constructive sample in Figure 2b.\n(c) Mask random spans: we mask O and some\nrandom spans R other than the deterministic\nclues. An example is shown in Figure 3. The\nnumber of tokens in R is the same as the num-\nber of tokens in the deterministic clues.\nPLM\n... War Horse [MASK] an [MASK]s directed by [MASK]s ...\nEmbedding Vectors\n[][MASK]ˆ ˆPLM( , , ) OSs Pp R \nFigure 3: A sample that masks some random spans R\n(with purple background) in the context.\nThe PLMs that use the Transformer encoder\n(Vaswani et al. , 2017) as the backbone emit a con-\ntextualized embedding for each input token. Every\ncontextualized embedding can encode all the in-\nformation in context since all the input tokens are\ninvolved when computing the embedding. So we\nencode the clues in the remaining context using the\ncontextualized embedding of\nO. Formally, the clue\nrepresentations for the above three samples are:\nE(a) = PLM(S = ˆs, P= ˆp, R= ˆr)[O],\nE(b) = PLM(S = [MASK ], P= [MASK ], R= ˆr)[O],\nE(c) = PLM(S = ˆs, P= ˆp, R= [MASK ])[O],\nE(a), E(b), E(c) ∈ R|O|×d.\n(2)\nE(a), E(b), and E(c) represent the inputs that (a)\nkeep deterministic clues S and P, (b) mask de-\nterministic clues, and (c) mask random spans R,\nrespectively. PLM(·) denotes the PLM that can\noutput the contextualized embedding for each input\ntoken.\nPLM(·)[O] denotes grabbing the embedding\nvectors corresponding to O from the PLM’s output.\nSince the three input samples have the same O, the\nnumber of token-level embedding vectors is the\nsame in E(a), E(b), and E(c).\nEach token-level embedding vector e in E(a),\nE(b), and E(c) is fed into a three-way classiﬁer:\ny = softmax(W⊤e), e ∈ Rd, W ∈ Rd×3. (3)\ny is a three-element vector and shows the probabil-\nities that e comes from E(a), E(b) and E(c). W⊤\nis the three-way classiﬁer.\nThe number of masked tokens in samples (a) and\n(b) is different since the latter masks the clues addi-\ntionally. It may become a shortcut for the proposed\ncontrastive or classiﬁcation tasks. So we introduce\nsample (c), which has the same number of masks as\nsample (b), to eliminate this shortcut. Some exist-\ning pre-training methods ( Clark et al. , 2019; Xiong\net al. , 2019; He et al. , 2020) replace original tokens\nwith fake tokens to build the pseudo samples and\nthen classify between original and pseudo samples,\nwhile clue classiﬁcation employs [ MASK ]s in the re-\nplacement. We wonder that fake tokens may make\nthe intervened input tell fake facts conﬂict with\nthe real world, leading the PLMs to capture wrong\nknowledge from the pseudo samples. [ MASK ] is a\nsafer choice here.\n3 Experiments\n3.1 Pre-training Data\nWe use Wikipedia 2 as the source of texts and\nWikidata3 as the knowledge base. We split the\nWikipedia texts into natural paragraphs and then\nalign each paragraph with subject-predicate-object\ntriplets in WikiData. Each aligned object that is de-\nterministic based on WikiData is the deterministic\nspan, and all the subjects and predicates that cor-\nrespond to the deterministic span are deterministic\nclues. The paragraphs with identiﬁed deterministic\nspans and clues are then used for pre-training.\nWe ﬁrst employ the TREX ( Elsahar et al. , 2018)\nthat provides the alignments between texts and\ntriplets to construct a preliminary dataset named\nPartial data . About 46.1% of triplets are non-\ndeterministic and ignored in the partial data. TERX\nprovides aligned triplets for only the abstract para-\ngraphs (ﬁrst paragraph) in Wikipedia. We enlarge\nthe data size by processing all the paragraphs in\nWikipedia. The detailed process is in Appendix B.\nThe dataset that involves all the paragraphs is re-\nferred to as Full data . Table 2 shows the statistics\n2https://www.wikipedia.org/\n3https://www.wikidata.org/\n11121\nof the two pre-training datasets. For efﬁciency rea-\nsons, we use the partial data to train the baselines\nin the ablation study. The full data is for our ﬁnal\nmodel.\nPartial data Full data\n# Paragraphs 3,726,205 17,373,859\n# Samples 12,134,717 39,289,518\nAvg. tokens per paragraph 166.93 134.61\nAvg. # tokens in per S ∪ P 8.53 7.52\nAvg. # tokens in per O 3.19 2.99\nTable 2: Statistics of the pre-training data.\n3.2 Evaluation Tasks and Datasets\nWe ﬁrst evaluate the proposed method with cloze-\nstyle QA . Then We adopt two other knowledge-\nintensive tasks, closed-book QA and extractive QA ,\nto evaluate the PLMs’ ability to capture and under-\nstand factual knowledge.\n3.2.1 Cloze-style QA\nFollowing Elazar et al. (2021); Cao et al. (2021);\nPetroni et al. (2019), we use cloze-style questions\nto probe the factual knowledge in PLMs. The\nPLMs need to recall the captured factual knowl-\nedge to ﬁll in the masks. Cloze-style QA uses the\nsame input-output format as MLM. So we do not\nneed to ﬁne-tune the PLMs and evaluate the factual\nknowledge capture performance directly.\nEvery cloze-style question is obtained by instan-\ntiating a artiﬁcial template on a fact. For example,\nthe question “ Keanu Reeves is a citizen of [MASK ]”\nis constructed based on the template “[ X] is a citi-\nzen of [Y]” and the triplet ( Keanu Reeves , citizen\nof, Canada). Filling the mask with the correct to-\nken “Canada” is regarded as successfully capturing\nthe corresponding fact.\nWe use the cloze-style questions and evalua-\ntion metrics from P ARA REL (Elazar et al. , 2021).\nPARA REL queries every fact with 8.64 different\nprompts on average. The prediction consistency is\ncalculated by putting two different prompts into a\nprompt pair ﬁrst (e.g., n(n − 1)/2 pairs can be ob-\ntained from n different prompts). Then the percent-\nage of the prompt pairs that can obtain the same\nresult is used to indicate the consistency quanti-\ntatively. The overall factual knowledge capture\nperformance is measured by jointly considering the\nprediction accuracy and consistency. Table\n3 shows\nthe statistics of the data from P ARA REL.\nWe also employ four cloze-style datasets from\nValue\n# Cloze-style questions 199,446\n# Facts (Triplets) 23,097\nAvg. # questions per fact 8.64\nTable 3: Statistics of the cloze-style QA dataset in\nPARA REL (Elazar et al. , 2021).\nDataset # Facts\nLAMA 29,522\nLAMA w/o leakage 25,698\nWIKI-UNI 69,761\nWIKI-UNI w/o leakage 63,772\nTable 4: Statistics of the four cloze-style QA datasets\nfrom ( Cao et al. , 2021).\n(Cao et al. , 2021). Table 4 shows the correspond-\ning statistics. LAMA represents the cloze-style\ndatasets that are similar to ( Petroni et al. , 2019) in\n(Cao et al. , 2021). The distribution of the ground-\ntruth answers in LAMA is imbalanced, provid-\ning a shortcut for PLMs to achieve good perfor-\nmance by selecting high-frequency entities as out-\nput. Therefore, Cao et al. (2021) proposes the\nWIKI-UNI dataset, where the distribution of the\nground-truth answers follows a uniform distribu-\ntion. The ground-truth answer has literal overlaps\nwith the question sometimes. For example, in “ New\nYork University is located in [MASK ] city,” the right\nanswer “ New York ” exactly leaked in the question.\nSo Cao et al. (2021) ﬁlter out the questions that\noverlap with answers and obtain two more datasets\nbased on LAMA and WIKI-UNI, which are indi-\ncated with the sufﬁx “w/o leakage”.\n3.2.2 Closed-book QA\nWe also use the closed-book QA to test the ability\nof PLMs to capture factual knowledge. As pro-\nposed in (\nWang et al. , 2021; Roberts et al. , 2020;\nLewis et al. , 2021), closed-book QA is similar to\nthe way in which a student is taking a closed-book\nexam. The input of the model is the question only,\nand the model needs to generate the answer di-\nrectly without seeing any other evidence. This task\nneeds the model to generate arbitrary strings as\nanswers, so we employ the BART, which has a de-\ncoder that can generate texts, as the base model\nfor this task. We use the closed-book QA datasets\nfrom ( Karpukhin et al. , 2020), as shown in Table 5.\n4The WebQuestions in ( Karpukhin et al. , 2020) does not\nhave a development set, so we use the test set for both test and\ndevelopment.\n11122\nDataset Train Set Dev Set Test Set\nNaturalQuestions 79,168 8,757 3,610\nTriviaQA 78,785 8,837 11,313\nWebQuestions 3,778 - 4 2,032\nTable 5: Statistics of the closed-book QA datasets.\n3.2.3 Extractive QA\nThe extractive QA is also known as machine read-\ning comprehension ( Liu et al. , 2019a). The task is\nto search and extract the answer span from the in-\nput passage for the input question. It evaluates the\nability of PLMs to understand the facts provided in\npassages. We employ the six extractive QA datasets\nfrom MRQA ( Fisch et al. , 2019), Table 6 presents\nthe summary of the datasets. Following the setting\nin ( Ram et al. , 2021a), we use the development sets\nfor testing.\nDataset Train Set Test Set\nSQuAD 86,588 10,507\nNewsQA 74,160 4,212\nTriviaQA 61,688 7,785\nSearchQA 117,384 16,980\nHotpotQA 72,928 5,904\nNatrualQuestions 104,071 12,836\nTotal 620,890 71,060\nTable 6: Summary of the extractive QA datasets.\n3.3 Results\n3.3.1 Baselines\nWe continuously pre-train RoBERTa- base (Liu\net al. , 2019b) and BART- base (Lewis et al. , 2020)\nfrom their ofﬁcial checkpoints with different mask-\ning methods:\n• Random token: Mask random tokens in the\ntokenized text ( Devlin et al. , 2018).\n• Whole word: Mask random words. All the\ntokens in the randomly selected words are\nmasked at once ( Cui et al. , 2019).\n• Salient span: Mask a span aligned with the\nsubject or object, both deterministic and non-\ndeterministic samples are included. ( Guu\net al. , 2020).\n• Deterministic: The proposed deterministic\nmasking that masks a deterministic object, in-\ncluding only deterministic samples.\nThe above four models are trained with the mask-\nﬁlling task only. The models pre-trained with clue\ncontrastive learning and clue classiﬁcation in com-\npany with deterministic masking are denoted as “ +\nCon & Cls .” To further explore the potential of the\nproposed methods, we train the model on the full\ndata with all the proposed methods, denoted as “ +\nFull data ”. We also introduce KEPLER- base as\nKB-enhanced baseline for comparison.\n3.3.2 Cloze-style QA\nMasking strategies Tables 7 and 8 present the\nresults on cloze-style QA. We can see that random\ntoken masking can gain some improvements in\nperformance, as well as the whole word masking.\nWe think this is because the input texts, which come\nfrom Wikipedia, are formal descriptions of facts.\nTraining on such texts helps shift the domain of\nPLMs for better generating factual words. The\nrandom token and whole word masking serve as\nsolid baselines to focus the comparison between\nmasking strategies, eliminating the confounders\nbrought by extra pre-training on Wikipedia.\nThe salient span masking and deterministic\nmasking both mask entity spans. The difference\nis that deterministic masking further limits the re-\nlationship between the remaining context and the\nmasked span to be deterministic, driving PLMs to\nlearn to infer based on the deterministic clues. The\nresults show that the PLMs can achieve much bet-\nter results with deterministic masking, indicating\nthat the deterministic relationship is valuable for\nrecovering factual spans robustly.\nThe proposed pre-training tasks The clue con-\ntrastive learning and the clue classiﬁcation, which\naim to strengthen the deterministic relationship,\nalso provide further performance improvements\n(denoted as + Con & Cls ). Finally, the full data\nwith all the proposed methods brings the most sig-\nniﬁcant improvement. The proposed pre-training\nmodels also outperform the KEPLER- base.\nOut-of-domain evaluation To analyze the im-\nprovement in-depth, we split the probing ques-\ntions into in-domain and out-of-domain accord-\ning to whether the pre-training corpus covers the\ncorresponding triplets in questions. As Table\n7\nshows, the three random-based masking methods\n(Random token, Whole word, and Salient Span)\nboost performance on in-domain questions but get\nstuck on the out-of-domain questions. It is natu-\nral that the PLMs can answer the questions that\nare involved in pre-training. Surprisingly, although\nthe out-of-domain questions are inaccessible in the\npre-training corpus, the deterministic masking also\ngains performance improvement (3-4%), indicating\n11123\nTotal Out-of-domain In-domain\nAcc. Consis. Joint Acc. Consis. Joint Acc. Consis. Joint\nRoBERTa-base 39.48 52.05 16.40 33.09 55.06 15.60 42.97 54.19 18.55\nRandom token 43.44 58.76 24.72 35.47 59.57 22.64 47.38 61.04 27.57\nWhole word 44.04 58.96 25.88 36.67 60.48 23.01 47.91 61.57 29.40\nSalient span 43.87 60.48 26.53 37.48 61.19 22.90 47.72 63.08 29.93\nKEPLER-base 39.63 50.96 17.81 - - - - - -\nDeterministic 45.29 64.65 29.37 38.59 65.39 25.88 49.13 66.42 32.17\n+ Con & Cls 46.01 64.53 29.62 38.05 65.35 26.24 50.26 65.71 32.32\n+ Full data 49.40 67.09 33.44 40.35 67.69 30.30 54.20 69.24 37.17\nBART-base 40.43 52.60 17.83 34.26 55.88 15.86 44.10 54.85 20.42\nRandom token 42.53 57.03 23.34 34.98 59.59 21.15 46.75 59.02 25.78\nWhole word 41.68 58.96 24.38 34.53 61.24 21.32 45.11 60.94 26.42\nSalient span 43.16 60.09 25.30 35.78 60.68 20.83 47.33 61.78 27.66\nDeterministic 44.13 64.67 28.77 36.70 65.15 24.45 48.58 66.25 31.98\n+ Con & Cls 46.65 65.64 28.89 39.51 66.31 25.53 50.72 67.86 32.45\n+ Full data 49.21 68.41 33.11 41.02 69.51 29.33 52.95 69.70 35.84\nTable 7: The factual knowledge capturing performance, evaluated by the cloze-style QA dataset P ARA REL. Acc. is\nthe accuracy, Consis. denotes the prediction consistency when changing the prompts, and Joint denotes the metric\nthat jointly measures accuracy and consistency. Out-of-domain represent the set of questions whose triplets do not\nappear in the pre-training.\nDataset LAMA w/o WIKI- w/o\nLeakage UNI Leakage\nRoBERTa-base 19.94 15.10 10.48 07.11\nRandom token 25.01 18.18 14.18 09.00\nWhole word 25.66 18.94 14.42 09.27\nSalient span 29.13 21.43 14.99 09.09\nKEPLER-base 15.04 10.66 8.44 05.89\nDeterministic 32.96 25.46 16.28 10.33\n+ Con & Cls 32.16 24.88 16.24 11.03\n+ Full data 35.35 28.86 19.36 14.23\nBART-base 11.77 07.08 06.03 03.47\nRandom token 25.39 17.75 14.01 08.24\nWhole word 25.06 17.21 14.21 08.75\nSalient span 30.07 22.20 15.56 09.60\nDeterministic 31.26 23.69 15.81 10.22\n+ Con & Cls 32.03 24.57 15.58 10.23\n+ Full data 35.49 28.98 18.64 13.21\nTable 8: The results on cloze-style QA datasets from\n(Cao et al. , 2021). The performance is measured by\naccuracy5.\nthat the deterministic relationship could help PLMs\nto better recollect the facts learned implicitly.\n3.3.3 Closed-book QA\nWe ﬁne-tune the continuously pre-trained BART-\nbase on the Closed-book QA task. The metrics are\nEM(Exact Match) and F1 from ( Rajpurkar et al. ,\n2016). Table 9 shows the comparison results of\ndifferent strategies.\n5The detailed metrics grouped by the relation types (N-\n1,N-M relations) are in Appendix A.\nClosed-book QA is more difﬁcult than cloze-\nstyle QA since the models need to generate answers\nwithout any extra hints, e.g., the answer length is\nindicated by the number of [ MASK ]s in the cloze-\nstyle QA, while the models need to predict the an-\nswer length in closed-book QA. The input-output\nformat of closed-book QA differs from per-training,\nso we need to ﬁne-tune PLMs to recall facts based\non natural questions to ﬁt this format. Table 9\nshows the evaluation results. Generally, the pro-\nposed methods outperform the baselines, demon-\nstrating that the proposed methods can help the\nPLM that uses encoder-decoder architecture to cap-\nture and recall factual knowledge.\n3.3.4 Extractive QA\nWe ﬁne-tune the models that based on RoBERTa-\nbase for extractive QA. Following ( He et al. , 2020;\nJoshi et al. , 2020), we employ the MRQA data with\ntwo different settings: (a) Separate: the models are\ntrained and tested on every QA dataset separately,\n(b) Combine: all the training samples from the six\ndatasets are merged in training. Then the ﬁne-tuned\nmodels are evaluated on each dataset respectively.\nTable 10 shows the evaluation results, the metrics\nare averaged over the six development sets.\nIn extractive QA, the input includes a question\nand the supporting evidence to answer it. So the\nmodels do not have to recollect the essential evi-\ndence but should put more effort into understanding\nthe evidence. Table\n10 shows the evaluation results.\n11124\nDataset Model F1 EM\nTriviaQA\nBART-base 23.91 17.52\nRandom token 23.67 18.04\nWhole word 24.64 18.88\nSalient span 24.98 19.21\nDeterministic 24.94 19.22\n+ Con & Cls 25.28 19.58\n+ Full Data 26.35 20.57\nNaturalQuestions\nBART-base 26.89 21.27\nRandom token 27.34 21.55\nWhole word 27.53 22.13\nSalient span 27.31 22.07\nDeterministic 27.83 22.60\n+ Con & Cls 28.14 22.69\n+ Full Data 29.17 23.91\nWebQuestions\nBART-base 33.62 26.62\nRandom token 32.58 26.38\nWhole word 32.45 26.03\nSalient span 32.70 26.08\nDeterministic 32.73 26.38\n+ Con & Cls 32.63 25.59\n+ Full Data 33.91 27.26\nTable 9: The performance on the closed-book QA\ndatasets.\nModel Separate Combine\nF1 EM F1 EM\nRoBERTa-base 80.78 69.51 81.78 70.57\nRandom token 80.53 69.22 81.79 70.52\nWhole word 80.86 69.71 81.77 70.60\nSalient span 80.85 69.61 81.72 70.50\nKEPLER-base 80.28 69.02 81.41 70.32\nDeterministic 80.83 69.63 81.78 70.59\n+ Con & Cls 80.94 69.75 81.79 70.67\n+ Full data 80.96 69.67 81.86 70.71\nTable 10: The performance on the extractive QA task.\nDue to the difference in the input-output format\nbetween the MLM and span extraction task, the\nchange in the masking methods has somewhat lim-\nited effects on the performance here. The averaging\non six different MRC datasets and the hyperparam-\neter search (in Appendix C) could further diminish\nthe performance difference between the models.\nHowever, the proposed methods still show slight\nadvantages in the comparison, demonstrating that\nlearning the deterministic relationship could also\nhelp to comprehend factual knowledge.\n4 Related Work\nPre-training on large-scale unlabeled text can help\nPLMs capture meaningful knowledge and beneﬁts\nthe downstream tasks accordingly ( Brown et al. ,\n2020; Radford et al. , 2018). BERT ( Devlin et al. ,\n2018) proposes a Mask Language Model (MLM)\nin which the model needs to recover some masked\ntokens based on the remaining context. The ef-\nfectiveness of the MLM makes BERT become the\nstarting point for ﬁtting many downstream tasks\n(Chen et al. , 2020). Afterward, several different\nmasking methods ( Cui et al. , 2019; Joshi et al. ,\n2020; Levine et al. , 2020; Sun et al. , 2019) have\nexplored how masking methods affect performance\nand have obtained further performance improve-\nment. These works push the limit of MLM and\nshow the importance of designing better masking\nstrategies.\nOn the other hand, some pre-training tasks other\nthan MLM have been proposed. Clark et al. (2019)\ntrains the model to distinguish the replaced words\nfrom the original words in the context. ( Xiong\net al. , 2019; He et al. , 2020) let factual spans be\nthe replacement candidates. Qin et al. (2020) con-\ntrasts the representations between different entities\nand relations. This paper views another perspective\nof the masking methods: whether the remaining\ncontext can uniquely determine the masked span.\nAccordingly, we propose a deterministic masking\nstrategy that masks deterministic spans in MLM\nsamples. Moreover, we design clue contrastive\nlearning and clue classiﬁcation as pre-training tasks\nto help PLMs identify the deterministic clues for\nthe masked span and contrast them with the non-\ndeterministic ones. Moreover, we evaluate the per-\nformance of the proposed model with various down-\nstream tasks.\n5 Conclusion\nThis paper proposes to train PLMs to learn a de-\nterministic input-output relationship in MLM to\nimprove PLMs on capturing factual knowledge.\nThe deterministic relationship ensures the masked\ncontent in MLM samples is deterministically pre-\ndictable based on the remaining context. To fur-\nther enhance the deterministic relationship, we de-\nsign a pre-training task clue contrastive learning\nthat encourages PLMs to give more conﬁdent pre-\ndictions when the input keeps deterministic clues,\nand the clue classiﬁcation to train PLMs to predict\nwhether the deterministic clues exist. Extensive\nexperiments show that the proposed methods can\nimprove the accuracy and consistency of factual\nknowledge capturing and boost the performance of\nthe other two knowledge-intensive tasks.\n11125\n6 Limitations\nWe summarize this paper’s main limitations as fol-\nlows: First, this study focuses on enhancing the\ndeterministic relationship but does not explore the\nnon-deterministic relationships. The other non-\ndeterministic relationships also play essential roles\nin tasks such as semantic role labeling and emo-\ntion recognition, where the proposed methods may\nnot be helpful. Second, due to the diversity and\nrichness of natural language, we cannot perfectly\nrecognize the deterministic clues and spans from\ntexts. We have to consider the noises in recog-\nnization when designing the pre-training tasks. Fi-\nnally, we continuously pre-train PLMs on only\nWikipedia text, somewhat narrowing down their\ndomain. Constructing more pre-training samples\nby the proposed procedure (Procedure 1 in the Ap-\npendix) could be better. Moreover, we can use the\ncurrent pre-training samples (based on Wikipedia)\nto train an “interpolation model” that can tag the\ndeterministic clues and spans in the input texts. The\ninterpolation model can also be used to enlarge the\npre-training data.\nAcknowledgements\nWe would like to thank the anonymous review-\ners for providing valuable reviews throughout the\nmulti-turn rolling review progress. Thanks to\nBenyou Wang for the helpful discussions, sugges-\ntions, and encouragement. The research in this arti-\ncle is supported by the National Key Research and\nDevelopment Project (2021YFF0901600) and the\nInterdisciplinary Development Program of Harbin\nInstitute of Technology (No. SYL-JC-202203).\nReferences\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165 .\nBoxi Cao, Hongyu Lin, Xianpei Han, Le Sun, Lingy-\nong Yan, Meng Liao, Tong Xue, and Jin Xu. 2021.\nKnowledgeable or educated guess? revisiting lan-\nguage models as knowledge bases\n. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers) , pages 1860–1874, Online.\nAssociation for Computational Linguistics.\nTianlong Chen, Jonathan Frankle, Shiyu Chang, Si-\njia Liu, Yang Zhang, Zhangyang Wang, and\nMichael Carbin. 2020. The lottery ticket hypoth-\nesis for pre-trained bert networks. arXiv preprint\narXiv:2007.12223.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and\nChristopher D Manning. 2019. Electra: Pre-training\ntext encoders as discriminators rather than generators.\nIn International Conference on Learning Representa-\ntions.\nElijah Cole, Oisin Mac Aodha, Titouan Lorieul, Pietro\nPerona, Dan Morris, and Nebojsa Jojic. 2021. Multi-\nlabel learning from single positive labels. In Pro-\nceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 933–942.\nThomas M Cover. 1999. Elements of information theory .\nJohn Wiley & Sons.\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing\nYang, Shijin Wang, and Guoping Hu. 2019. Pre-\ntraining with whole word masking for chinese bert.\narXiv preprint arXiv:1906.08101 .\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. CoRR, abs/1810.04805.\nThibaut Durand, Nazanin Mehrasa, and Greg Mori.\n2019. Learning a deep convnet for multi-label clas-\nsiﬁcation with partial labels. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition , pages 647–657.\nYanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha\nRavichander, Eduard Hovy, Hinrich Schütze, and\nYoav Goldberg. 2021. Measuring and improving\nconsistency in pretrained language models .\nHady Elsahar, Pavlos V ougiouklis, Arslen Remaci,\nChristophe Gravier, Jonathon Hare, Frederique Lafor-\nest, and Elena Simperl. 2018. T-rex: A large scale\nalignment of natural language with knowledge base\ntriples. In Proceedings of the Eleventh International\nConference on Language Resources and Evaluation\n(LREC 2018) .\nAdam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eun-\nsol Choi, and Danqi Chen. 2019. MRQA 2019 shared\ntask: Evaluating generalization in reading compre-\nhension. In Proceedings of 2nd Machine Reading\nfor Reading Comprehension (MRQA) Workshop at\nEMNLP.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\naugmented language model pre-training. arXiv\npreprint arXiv:2002.08909 .\nBin He, Xin Jiang, Jinghui Xiao, and Qun Liu. 2020.\nKgplm: Knowledge-guided language model pre-\ntraining via generative and discriminative learning.\narXiv preprint arXiv:2012.03551 .\n11126\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know. Trans. Assoc. Comput. Linguistics ,\n8:423–438.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld,\nLuke Zettlemoyer, and Omer Levy. 2020. Spanbert:\nImproving pre-training by representing and predict-\ning spans. Transactions of the Association for Com-\nputational Linguistics , 8:64–77.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP) , pages 6769–6781.\nYoav Levine, Barak Lenz, Opher Lieber, Omri Abend,\nKevin Leyton-Brown, Moshe Tennenholtz, and Yoav\nShoham. 2020. Pmi-masking: Principled masking\nof correlated spans. In International Conference on\nLearning Representations .\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020. Bart:\nDenoising sequence-to-sequence pre-training for nat-\nural language generation, translation, and comprehen-\nsion. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics , pages\n7871–7880.\nPatrick Lewis, Pontus Stenetorp, and Sebastian Riedel.\n2021. Question and answer test-train overlap in open-\ndomain question answering datasets. In Proceedings\nof the 16th Conference of the European Chapter of\nthe Association for Computational Linguistics: Main\nVolume, pages 1000–1008.\nShanshan Liu, Xin Zhang, Sheng Zhang, Hui Wang,\nand Weiming Zhang. 2019a. Neural machine read-\ning comprehension: Methods and trends. Applied\nSciences, 9(18):3698.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692 .\nXin Luo, Wei Chen, Yusong Tan, Chen Li, Yulin He, and\nXiaogang Jia. 2021. Exploiting negative learning for\nimplicit pseudo label rectiﬁcation in source-free do-\nmain adaptive semantic segmentation. arXiv preprint\narXiv:2106.12123.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick S. H. Lewis, Anton Bakhtin, Yuxiang Wu,\nand Alexander H. Miller. 2019. Language mod-\nels as knowledge bases? In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing,\nEMNLP-IJCNLP 2019, Hong Kong, China, Novem-\nber 3-7, 2019 , pages 2463–2473.\nNina Pörner, Ulli Waltinger, and Hinrich Schütze. 2020.\nE-BERT: efﬁcient-yet-effective entity embeddings\nfor BERT. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2020, Online Event,\n16-20 November 2020 , volume EMNLP 2020.\nYujia Qin, Yankai Lin, Ryuichi Takanobu, Zhiyuan\nLiu, Peng Li, Heng Ji, Minlie Huang, Maosong\nSun, and Jie Zhou. 2020. Erica: improving en-\ntity and relation understanding for pre-trained lan-\nguage models via contrastive learning. arXiv preprint\narXiv:2012.15022.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing , pages 2383–2392.\nOri Ram, Yuval Kirstain, Jonathan Berant, Amir Glober-\nson, and Omer Levy. 2021a. Few-shot question an-\nswering by pretraining span selection. arXiv preprint\narXiv:2101.00438.\nOri Ram, Yuval Kirstain, Jonathan Berant, Amir Glober-\nson, and Omer Levy. 2021b. Few-shot question an-\nswering by pretraining span selection . In Proceed-\nings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers) , pages 3066–3079, Online.\nAssociation for Computational Linguistics.\nRaymond Reiter. 1981. On closed world data bases.\nIn Readings in artiﬁcial intelligence , pages 119–140.\nElsevier.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP) , pages 5418–5426.\nTaylor Shin, Yasaman Razeghi, Robert L Logan IV ,\nEric Wallace, and Sameer Singh. 2020. Autoprompt:\nAutomatic prompt construction for masked language\nmodels. In Empirical Methods in Natural Language\nProcessing (EMNLP) , pages 4222–4235.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi\nChen, Han Zhang, Xin Tian, Danxiang Zhu, Hao\nTian, and Hua Wu. 2019. Ernie: Enhanced represen-\ntation through knowledge integration. arXiv preprint\narXiv:1904.09223.\nJohannes M. van Hulst, Faegheh Hasibi, Koen Derck-\nsen, Krisztian Balog, and Arjen P. de Vries. 2020.\nRel: An entity linker standing on the shoulders of gi-\nants. In Proceedings of the 43rd International ACM\nSIGIR Conference on Research and Development in\nInformation Retrieval , SIGIR ’20. ACM.\n11127\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems , pages 5998–6008.\nTuan-Hung Vu, Himalaya Jain, Maxime Bucher,\nMatthieu Cord, and Patrick Pérez. 2019. Advent:\nAdversarial entropy minimization for domain adap-\ntation in semantic segmentation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and\nPattern Recognition , pages 2517–2526.\nCunxiang Wang, Pai Liu, and Yue Zhang. 2021. Can\ngenerative pre-trained language models serve as\nknowledge bases for closed-book qa? arXiv preprint\narXiv:2106.01561.\nWenhan Xiong, Jingfei Du, William Yang Wang, and\nVeselin Stoyanov. 2019. Pretrained encyclopedia:\nWeakly supervised knowledge-pretrained language\nmodel. arXiv preprint arXiv:1912.09637 .\nMin-Ling Zhang and Zhi-Hua Zhou. 2006. Multilabel\nneural networks with applications to functional ge-\nnomics and text categorization. IEEE transactions\non Knowledge and Data Engineering , 18(10):1338–\n1351.\nZexuan Zhong, Dan Friedman, and Danqi Chen. 2021.\nFactual probing is [MASK]: learning vs. learning to\nrecall. CoRR, abs/2104.05240.\n11128\nTotal Out-of-domain In-domain\nAcc. Consis. Joint Acc. Consis. Joint Acc. Consis. Joint\nRoBERTa-base\nSalient span 43.87 60.48 26.53 37.48 61.19 22.90 47.72 63.08 29.93\nObject 44.41 63.68 28.00 38.48 64.40 24.94 47.82 65.25 31.27\nDeterministic 45.29 64.65 29.37 38.59 65.39 25.88 49.13 66.42 32.17\n+ Con 45.93 65.50 29.52 39.33 66.34 26.26 49.50 67.26 32.58\n+ Con & Cls 46.01 64.53 29.62 38.05 65.35 26.24 50.26 65.71 32.32\nBART-base\nSalient span 43.16 60.09 25.30 35.78 60.68 20.83 47.33 61.78 27.66\nObject 42.61 62.67 27.24 35.76 62.22 22.39 46.91 64.89 30.73\nDeterministic 44.13 64.67 28.77 36.70 65.15 24.45 48.58 66.25 31.98\n+ Con 44.96 64.11 28.17 37.66 64.50 24.51 48.97 66.16 31.71\n+ Con & Cls 46.65 65.64 28.89 39.51 66.31 25.53 50.72 67.86 32.45\nTable 11: The evaluation results on P ARA REL. “Object” denotes the baseline that masks and predicts the objects.\n“Deterministic” denotes the MLM baseline that uses deterministic masking. “+ Con” is the baseline that uses the\nclue contrastive learning with deterministic masking.\nAppendix\nA Ablation Study\nHow does masking objects help in factual knowl-\nedge capturing? As described in Section 3.2.1,\nthe cloze-style questions that we use to probe the\nfactual knowledge in PLMs, are constructed by\nintegrating subject-predicate-object triples with ar-\ntiﬁcial templates. Due to the conventions in the\ntemplate construction, the objects could have more\nopportunities to be the answer than the predicates\nand subjects. So focusing on recovering objects in\npre-training may also beneﬁt cloze-style QA. The\nproposed deterministic masking naturally masks\nmore objects in pre-training because of the rules\nwe designed to identify the deterministic span.\nBoth masking objects and the deterministic rela-\ntionship could bring improvements in the determin-\nistic masking.\nTo investigate and clarify their contributions, we\nintroduce a baseline “Object” that masks and pre-\ndicts only object spans in pre-training. Table 11\nshows the evaluation result. We can see that the\nObject baseline performs better than the Salient\nspan baseline on factual knowledge capture. It\nreveals that masking objects indeed improve per-\nformance. Nevertheless, the deterministic masking\n(denoted as “Deterministic”) achieves better results,\ndenoting that both masking objects and learning\nthe deterministic relationship contribute positively\nto factual knowledge capture.\nThe effectiveness of the clue contrastive learning\nand clue classiﬁcation To reveal the contribution\nof the proposed pre-training tasks separately, we\nintroduce a baseline that only uses the clue con-\ntrastive learning. We refer to it as “+ Con” in\nTable\n11. “+ Con & Cls” denotes the PLM that\nuses the clue classiﬁcation in addition to the clue\ncontrastive learning. We can see that the perfor-\nmance increases as we apply the proposed methods\nincrementally.\nThe improvements on deterministic and non-\ndeterministic cloze-style questions The dataset\nfrom P ARA REL includes only the N-1 relations 6.\nWhile the LAMA dataset from ( Petroni et al. , 2019)\n(Tables4 and 8) includes both the N-1/1-1 and N-M\nrelations. To reveal the improvements in terms of\nrelation types, we separate the samples into N-1/1-1\nand N-M based on the relation types and report the\nresults separately, as shown in Table 12. Similar to\nthe deterministic relationship we used, the ground-\ntruth object is unique when the relation type is N-1\nor 1-1. The results show that the improvement for\nthe N-1/1-1 relations is more signiﬁcant than the\nN-M relations when using the proposed methods.\nB Pre-training Data Construction\nProcedure 1 shows how we construct the pre-\ntraining data, including entity linking, predicate\nmatching, triplet aligning, and deterministic rela-\ntionship checking. Each text piece t is a paragraph\nin Wikipedia. We use the entity linker provided\nin (\nvan Hulst et al. , 2020), represented as E N-\nTITY LINKER , to identify all the entities in the para-\ngraph7. The WikiData deﬁnes 12,043 aliases for\n8,529 predicates. Function P REDICATE MATCHER\nsearches the substring corresponding to a predicate\nby comparing the predicate’s aliases with all the\nsubstrings in the text. The identiﬁed predicate span\nis the nearest match whose edit distance is less than\n6Deﬁned in https://github.com/yanaiela/pararel/wiki/31-\nN1-Relations\n11129\nProcedure 1 Deterministic Sample Construction\nRequire: Text collection T, Knowledge base K, E NTITY LINKER\nOutput: Deterministic sample collection Dd\nOutput: Salient span masking sample collection Dssm\n1: Dd ← {}, Dssm ← {}\n2: for all text piece t in T do\n3: E ← ENTITY LINKER (t) ▷ Identify all the entities in t\n4: Dssm = Dssm ∪ {(t, E)} ▷ Save the entities for the salient span masking\n5: for all entity pair (ei, ej) in E × E do\n6: for all predicate r that can connects (ei, ej) do ▷ Triplet (ei, r, ej) exists in K\n7: if ej has only one match when querying K with ei and r as the subject and predicate then\n8: p ← PREDICATE MATCHER (t, r) ▷ Find the spans in t that correspond to r\n9: s ← ei ▷ Use ei as subject s\n10: o ← ej ▷ Use ej as object o\n11: d ← (s, p, o, E, t) ▷ Group the alignments into d\n12: D = D ∪ {d} ▷ Record the sample d\nreturn Dd, Dssm\n13: function PREDICATE MATCHER (t, r)\n14: for all alias string a for r in K do ▷ WikiData holds a alias string collection for every predicate\n15: for all substring s in t do\n16: if edit distance between a and s < 2 then\n17: Return s\nModel Total N-1/1-1 N-M\nRoBERTa-base 19.94 22.18 16.46\nRandom token 25.01 28.98 18.81\nWhole word 25.66 29.26 20.05\nSalient span 29.13 30.89 26.38\nDeterministic 32.96 37.84 25.35\n+ Con & Cls 32.16 36.62 25.19\n+ Full data 35.35 41.86 25.18\nBART-base 11.77 13.96 08.35\nRandom token 25.39 27.66 21.84\nWhole word 25.06 28.69 19.39\nSalient span 30.07 31.83 27.31\nDeterministic 31.26 35.77 24.21\n+ Con & Cls 32.03 36.38 25.25\n+ Full data 35.49 41.99 25.33\nTable 12: The detailed results on the LAMA dataset in\n(Cao et al. , 2021), reported separately with respect to\nrelation types: N-1/1-1 or N-M.\ntwo.\nAfter recognizing the entities and predicates, we\ncombine every entity pair with every predicate as\na triplet (entity, predicate, entity), enumerate all\nthe possible combinations, and keep the ones that\nexisted in KB as the triplets aligned with the para-\ngraph. Line 7 checks if the object is deterministic\nby querying KB. We record the obtained determin-\nistic sample in the format of (subject s, predicate p,\nobject o, text t, and entities E).\nThe baselines use the pre-training sample as the\nfollowing:\n• Deterministic (mask deterministic object to\ntrain MLM): Get a sample (s, p, o, E, t) from\nDd, mask the span corresponding to o and\ntrain PLMs to predict o based on the remain-\ning context.\n• Random (mask tokens randomly): Get\na sample from Dd, tokenize t and cal-\nculate the number of tokens in o, de-\nnoted as TOKEN COUNT (o), randomly sam-\nple TOKEN COUNT (o) tokens to be masked in\nthe MLM training.\n• Whole word (mask whole words randomly):\nGet a sample from Dd, calculate the num-\n7https://github.com/informagi/REL\n11130\nber of words in o (separated by space), de-\nnoted as WORD COUNT (o), randomly mask\nWORD COUNT (o) words in t.\n• Salient Span (mask entities randomly): Get\na sample (s, p, o, E, t) from Dssm, randomly\nmask an aligned entity in E.\nAlthough the masking granularity is different in the\nbaslines, we keep the length of the masked content\nas similar as possible for a fair comparison.\nThen we introduce how the two proposed pre-\ntraining tasks use the data. In the clue contrastive\nlearning, the s and p are the deterministic clues and\nmasked in the contrastive sample. If the same o\nhave more than one deterministic clue in t, e.g.,\nmultiple deterministic clues for the same o are\ngiven by different triplets, all the deterministic\ns and p are considered as determined clues and\nmasked in the contrastive sample (b). In the clue\nclassiﬁcation, the number of the randomly masked\ntoken in the sample (c) is the same as the con-\nstrastive sample.\nProcedure 1 is used to generate the full data\n(summarized in Table 2). We obtain the partial\ndata similarly, except that we do not need the E N-\nTITY LINKER (at Line 3 in Procedure 1) and directly\nuse the entity-text alignments provided in TREX.\nC Hyperparameters\nPre-training For the baselines trained on the par-\ntial data, the batch size is set to 512, the learning\nrate is\n3 × 10−5, and the number of total training\nsteps is 50,000. There are 200,000 training steps\nfor the ﬁnal model on the full data.\nExtractive QA In the experiments on extractive\nQA, we ﬁnd that the model’s performance is sensi-\ntive to hyperparameters. We conduct a grid search\nover the learning rate and batch size. In the sep-\narate setting, the learning rate is searched over\n{\n1 × 10−5, 2 × 10−5, 3 × 10−5}\n, the batch size is\nsearched over {16, 32, 64} when ﬁne-tuning ev-\nery PLM on every dataset, and the epoch is set\nto 4. In the combine setting, the learning rate\n∈{\n2 × 10−5, 3 × 10−5}\n, the batch size ∈ {32, 64},\nand the epoch is set to 2. We save the model check-\npoint per 5,000 steps. The best model is selected\nfrom evaluating all the checkpoints. We use the\ncode released by ( Ram et al. , 2021b)8.\nClosed-book QA The learning rate is set to 5 ×\n10−5. The training steps is set to 100,000 for Natu-\nralQuestions and TriviaQA, 40,000 for WebQues-\n8https://github.com/oriram/splinter\ntions. The model is evaluated per 10,000 training\nsteps to select the best checkpoint.\n11131",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7995530366897583
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.6543681621551514
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6192781329154968
    },
    {
      "name": "Machine learning",
      "score": 0.5390602350234985
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5345636606216431
    },
    {
      "name": "Language model",
      "score": 0.4784426987171173
    },
    {
      "name": "Knowledge base",
      "score": 0.469982773065567
    },
    {
      "name": "Natural language processing",
      "score": 0.4174199104309082
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I204983213",
      "name": "Harbin Institute of Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210159102",
      "name": "Huawei Technologies (Sweden)",
      "country": "SE"
    }
  ],
  "cited_by": 13
}