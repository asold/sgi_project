{
  "title": "A Novel Vision Transformer Model for Skin Cancer Classification",
  "url": "https://openalex.org/W4360994757",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A1990782487",
      "name": "Guang Yang",
      "affiliations": [
        "University of Newcastle Australia"
      ]
    },
    {
      "id": "https://openalex.org/A2185871111",
      "name": "Suhuai Luo",
      "affiliations": [
        "University of Newcastle Australia"
      ]
    },
    {
      "id": "https://openalex.org/A2184259585",
      "name": "Peter Greer",
      "affiliations": [
        "University of Newcastle Australia"
      ]
    },
    {
      "id": "https://openalex.org/A1990782487",
      "name": "Guang Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2185871111",
      "name": "Suhuai Luo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2184259585",
      "name": "Peter Greer",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2165948908",
    "https://openalex.org/W2128186581",
    "https://openalex.org/W2581082771",
    "https://openalex.org/W2903641499",
    "https://openalex.org/W2794825826",
    "https://openalex.org/W1981276685",
    "https://openalex.org/W6702248584",
    "https://openalex.org/W3011885901",
    "https://openalex.org/W3024371423",
    "https://openalex.org/W2962843773",
    "https://openalex.org/W3172942063",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2986668407",
    "https://openalex.org/W3163465952",
    "https://openalex.org/W1480009832",
    "https://openalex.org/W2953233419",
    "https://openalex.org/W4293030607",
    "https://openalex.org/W4205189720",
    "https://openalex.org/W4285992128",
    "https://openalex.org/W2295627932",
    "https://openalex.org/W2404812871",
    "https://openalex.org/W2328398504",
    "https://openalex.org/W2074317748",
    "https://openalex.org/W2916275175",
    "https://openalex.org/W2559989326",
    "https://openalex.org/W2608868516",
    "https://openalex.org/W2473141521",
    "https://openalex.org/W2794748578",
    "https://openalex.org/W2579362932",
    "https://openalex.org/W2385915143",
    "https://openalex.org/W3102785203",
    "https://openalex.org/W3098394437"
  ],
  "abstract": "Abstract Skin cancer can be fatal if it is found to be malignant. Modern diagnosis of skin cancer heavily relies on visual inspection through clinical screening, dermoscopy, or histopathological examinations. However, due to similarity among cancer types, it is usually challenging to identify the type of skin cancer, especially at its early stages. Deep learning techniques have been developed over the last few years and have achieved success in helping to improve the accuracy of diagnosis and classification. However, the latest deep learning algorithms still do not provide ideal classification accuracy. To further improve the performance of classification accuracy, this paper presents a novel method of classifying skin cancer in clinical skin images. The method consists of four blocks. First, class rebalancing is applied to the images of seven skin cancer types for better classification performance. Second, an image is preprocessed by being split into patches of the same size and then flattened into a series of tokens. Third, a transformer encoder is used to process the flattened patches. The transformer encoder consists of N identical layers with each layer containing two sublayers. Sublayer one is a multihead self-attention unit, and sublayer two is a fully connected feed-forward network unit. For each of the two sublayers, a normalization operation is applied to its input, and a residual connection of its input and its output is calculated. Finally, a classification block is implemented after the transformer encoder. The block consists of a flattened layer and a dense layer with batch normalization. Transfer learning is implemented to build the whole network, where the ImageNet dataset is used to pretrain the network and the HAM10000 dataset is used to fine-tune the network. Experiments have shown that the method has achieved a classification accuracy of 94.1%, outperforming the current state-of-the-art model IRv2 with soft attention on the same training and testing datasets. On the Edinburgh DERMOFIT dataset also, the method has better performance compared with baseline models.",
  "full_text": "Neural Processing Letters (2023) 55:9335–9351\nhttps://doi.org/10.1007/s11063-023-11204-5\nA Novel Vision Transformer Model for Skin Cancer\nClassiﬁcation\nGuang Yang 1 · Suhuai Luo 1 · Peter Greer 1\nAccepted: 24 February 2023 / Published online: 27 March 2023\n© The Author(s) 2023\nAbstract\nSkin cancer can be fatal if it is found to be malignant. Modern diagnosis of skin cancer heav-\nily relies on visual inspection through clinical screening, dermoscopy, or histopathological\nexaminations. However, due to similarity among cancer types, it is usually challenging to\nidentify the type of skin cancer, especially at its early stages. Deep learning techniques have\nbeen developed over the last few years and have achieved success in helping to improve the\naccuracy of diagnosis and classiﬁcation. However, the latest deep learning algorithms still do\nnot provide ideal classiﬁcation accuracy. To further improve the performance of classiﬁcation\naccuracy, this paper presents a novel method of classifying skin cancer in clinical skin images.\nThe method consists of four blocks. First, class rebalancing is applied to the images of seven\nskin cancer types for better classiﬁcation performance. Second, an image is preprocessed by\nbeing split into patches of the same size and then ﬂattened into a series of tokens. Third, a\ntransformer encoder is used to process the ﬂattened patches. The transformer encoder consists\nof N identical layers with each layer containing two sublayers. Sublayer one is a multihead\nself-attention unit, and sublayer two is a fully connected feed-forward network unit. For each\nof the two sublayers, a normalization operation is applied to its input, and a residual connec-\ntion of its input and its output is calculated. Finally, a classiﬁcation block is implemented\nafter the transformer encoder. The block consists of a ﬂattened layer and a dense layer with\nbatch normalization. Transfer learning is implemented to build the whole network, where\nthe ImageNet dataset is used to pretrain the network and the HAM10000 dataset is used to\nﬁne-tune the network. Experiments have shown that the method has achieved a classiﬁcation\naccuracy of 94.1%, outperforming the current state-of-the-art model IRv2 with soft attention\non the same training and testing datasets. On the Edinburgh DERMOFIT dataset also, the\nmethod has better performance compared with baseline models.\nB Guang Yang\nguang.yang10@uon.edu.au\nSuhuai Luo\nhttps://www.newcastle.edu.au/proﬁle/suhuai-luo\nPeter Greer\nhttps://www.newcastle.edu.au/proﬁle/peter-greer\n1 School of Information and Physical Sciences, College of Engineering, Science and Environment, The\nUniversity of Newcastle, University Dr., Callaghan, NSW 2308, Australia\n123\n9336 G. Yang et al.\nKeywords Skin cancer classiﬁcation · Deep learning · Transformer · Image processing ·\nNeural networks\n1 Introduction\nSkin cancer is a common disease. Over ﬁve million new cases are diagnosed in the USA each\nyear [ 1]. Australia has an even higher per capita incidence [ 2]. Among all the skin cancers,\nthe most dangerous type is melanoma. It was reported that more than 190,000 new cases of\nmelanoma were found in the USA in 2019 [ 3], contributing to the majority of deaths caused\nby all types of skin cancers. If diagnosed in the late stages, the survival rate for melanoma is\npoor [4]. Nevertheless, the survival rate can be as high as 97% [ 5] if it is found at early stages,\nwhich means early detection is vital. However, malignant skin cancer is not the primary form\nof skin cancer. Benign skin cancer makes up most of the cases. The similarity among the\ntypes makes the detection and classiﬁcation of malignant skin cancer a challenging task.\nIn current clinical practice, a high-resolution image solution, dermoscopy, is utilized by\ndermatologists to help diagnose skin cancer [ 6]. However, because a doctor usually requires\nextensive clinical visual experience to identify different types of skin cancers, the diagnosis\nresult can be unreliable [ 7].\nWhether it is a dermatologist or a machine learning algorithm, there has always been a\nproblem accurately classifying some skin cancer types, especially malignant types. Therefore,\ndeep learning techniques have been developed over the last few years to help improve the\naccuracy of diagnosis and the classiﬁcation of skin cancers. In 2017, skin cancer classiﬁcation\nby deep learning models and by dermatologists was compared [ 8]. The results showed that\nthe latest deep learning methods outperformed dermatologists in accuracy and sensitivity of\nclassiﬁcation. Since then, more researchers have improved the performance of deep learning\nmethods by using segmentation, data augmentation, and more advanced models [ 9]. However,\nfor some malignant skin cancer types, such as melanoma, state-of-the-art algorithms still do\nnot provide ideal classiﬁcation accuracy due to similarity between types. In order to further\nimprove the accuracy of deep learning algorithms in skin cancer classiﬁcation, we launched\nresearch on improving the performance of deep learning algorithms in this ﬁeld. Inspired\nby the work done in [ 12], we propose a new transformer network structure for skin cancer\nclassiﬁcation. We have compared our model’s performance with the state-of-the-art models\ntrained on the same dataset. Our experiment has shown that our transformer model achieved\nbetter accuracy and sensitivity than the non-transformer models. It can target the disease area\non the dermoscopy image with less interference from the healthy area and noise, making it\nmore suitable for skin lesion classiﬁcation than the compared models.\n2 Related Work\nTraditional machine learning techniques were developed before deep learning was introduced\ninto skin cancer classiﬁcation. Researchers [ 33] used asymmetry, border, color, diameter\n(ABCD) as indicators to analyze skin lesions. Different algorithms are used for classiﬁcation.\nA multilayer perceptron (MLP) network is used to classify features [ 33]. [ 34] enhanced\nimages and removed noise by introducing a Gabor ﬁlter and geodesic active contours and\nthen derived features by ABCD method. Logistic regression with artiﬁcial neural networks\nwas also applied [35] to analyze the thickness of skin lesions. Additionally, there is a technique\n123\nA Novel Vision Transformer Model for Skin Cancer Classiﬁcation 9337\nto enhance the RGB channel of the image through the median ﬁlter [ 36] and then use the\ndeformable model for segmentation. Support vector machines have also been used to classify\nfeatures decomposed by ABCD rules [ 43][ 37]. Similarly, K-means and K-nearest neighbor\n(KNN) are also applied to clustering and classiﬁcation [ 38][ 39]. CAD [ 40][ 41] and 3D\nreconstruction [ 42] are also used for classiﬁcation work.\nIn 2017, Esteva et al. [ 8] published their research comparing the performance of skin\ncancer classiﬁcation by a pretrained GoogleNet Inception v3 model with the diagnoses of a\ngroup of dermatologists. Since then, an increasing number of researchers have contributed\ndeep learning methods to this area, with many promising results published.\nOne method, designed by Nadipineni [ 11] in 2020, focused on the clinical image pre-\nprocessing step by conducting segmentation using U-Net and carrying out extensive data\naugmentation. They used ensemble technique and managed to achieve the best overall per-\nformance based on a series of ﬁne-tuned convolutional neural network (CNN) models. The\nmethod achieved an accuracy of 92.6%, which was the state-of-the-art deep learning method\nin 2020.\nIn 2021, Datta et al. [ 10] implemented a deep learning method combining some of the\nbest performing CNNs and soft attention layers, achieving an accuracy of 93.4%. The result\noutperforms Nadipineni’s [ 11] approach on the HAM10000 dataset [ 13]. In their work, the\ndata preprocessing technique removed duplicated data, and oversampling and undersampling\nof data was conducted to rebalance the number of images among the different types.\nMoreover, the concept of vision transformer (ViT) was introduced in [ 12], from which our\nmethod is inspired. In the paper, the authors introduced a model consisting of multiple soft\nattention layers based on the transformer architecture previously used in natural language\nprocessing. By cutting the input image into small patches and transforming them into tokens,\nthe model processed images similar to the way that sentences were processed in the trans-\nformers. After an extensive training dataset was pretrained on the model and ﬁne-tuned on\na smaller dataset, the ViT achieved good performance on some popular benchmark datasets,\nsuch as ImageNet.\nAfter the ViT got researchers’ attention because of its strong performance, there was\nsome work in the ﬁeld of skin cancer classiﬁcation/segmentation based on the ViT model\nin 2022. In the work [ 30], researchers proposed a new method in the image feature embed-\nding block of the original ViT model combined with a contrastive learning method. [ 32]\nconducted experiments addressing the bottleneck of the original ViT with their improved\nposition encoding method. Both achieved comparable results compared to the previous best\nperformer in skin cancer classiﬁcation. Attempts with ViT have also been conducted in the\nskin lesion segmentation ﬁeld [ 31]. By introducing a spatial pyramid pooling module into\nthe original transformer’s multihead attention, computation efﬁciency has been improved and\nbetter segmentation performance compared with baseline CNN models has been achieved.\nMethodology.\nThe proposed method is based on the concept of the transformer, which was ﬁrst introduced\nb yV a s w a n ie ta l .[14] in 2017 for automated machine translation. It was introduced to solve\nsequence-to-sequence tasks with the ability to handle long-range dependencies. A transformer\navoids recurrency but relies entirely on an attention mechanism to draw global relationships\nbetween input and output. It soon became the critical component of the best performing\nmodels, such as BERT [ 15]a n dG P T[ 16]. In the ViT model [ 12] introduced by Dosovitskiy,\nthe original transformer’s architecture was followed to utilize the scalability and efﬁciency of\nthe design. The proposed method, though improved, is inspired by the ViT model. It is named\na ViT for skin cancer detection (ViTfSCD). The method consists of four blocks as depicted\nin Fig. 1. The original dataset will ﬁrst ﬂow through the data augmentation and cancer-class\n123\n9338 G. Yang et al.\nFig. 1 The proposed vision transformer for skin cancer detection (ViTfSCD)\nrebalancing block followed by the image restructuring block, transformer encoder block, and\nclassiﬁcation block. The details of these blocks are described below.\n(a) The data augmentation and cancer-class rebalancing block\nDermoscopy is a method of skin lesion inspection using a device consisting of a high-\nresolution lens with a proper illumination setting. Dermoscopy images for skin lesions are\nbecoming a popular source for artiﬁcial intelligence studies in recent research [ 8, 10, 11]. The\ndataset used in this study is the HAM10000 dataset [ 13] provided by ISIC. The dataset consists\nof 10,015 dermoscopic images, each with a size of 450 pixels by 600 pixels. It includes seven\ncategories, including actinic keratosis intraepithelial carcinoma (AKIEC), basal cell carci-\nnoma (BCC), benign keratosis (BKL), dermatoﬁbroma (DF), melanoma (MEL), melanocytic\nnevi (NV), and vascular lesions (V ASC), as illustrated in Fig. 2.\nClass rebalancing is applied to the images of seven types of skin cancers in the training\nset for better classiﬁcation performance. The class rebalancing is needed, as the dataset of\nFig. 2 Skin lesion examples in HAM10000 dataset\n123\nA Novel Vision Transformer Model for Skin Cancer Classiﬁcation 9339\n0\n0.5\n1\n1.5\nakiec bcc bkl df mel nv vasc\nsensitivity sensitivity over-sampling\n0\n0.5\n1\n1.5\nakiec bcc bkl df mel nv vasc\nprecision precision over-sampling\nFig. 3 Sensitivity/precision comparison for each type trained with/without oversampling\ndermoscopy images contains an imbalanced number across different classes. The dataset is\nprocessed by oversampling and undersampling to ensure all the classes have an equal number\nof images. Oversampling is applied to the image class in the training dataset with fewer images\nby applying Keras data augmentation methods, including random rotations of 180 degrees,\nwidth and height shifts of 0.1 fractions, random zooming ranges of 0.1, and random ﬂipping\nboth horizontally and vertically. The accuracy of the same model (ViTfSCD-B) applied to\nthe HAM10000 dataset was compared with and without oversampling. The result showed the\noverall accuracy can be improved from 70.46 to 91.4%. We can see from sensitivity ﬁgures\nin Fig. 3 that, before oversampling, types with far fewer samples were underﬁtting compared\nwith types with more samples (such as BKL and NV). However, after oversampling, each\nclass acquired more balanced training. Accordingly, we used the oversampled HAM10000\ndataset for all our experiments. Undersampling is conducted by removing duplicated images\nof the same skin lesion. Finally, the pixel values of the images are normalized to a 0–1 range.\n(b) Image restructuring block\nThe image restructuring block consists of three processes. A 2D input image is ﬁrst split\ninto a sequence of patches of the same size. The number of the patches is the input sequence\nlength that will be used in the transformer. Then, the patches go through a trainable linear\nembedding where a linear projection is performed to ﬂatten the patches into a series of\ntokens with the same size, as transformers require constant latent vector size through all\nlayers. Finally, a positional embedding is performed on the embedded tokens to retain the\npositional information of the patches where standard, learnable 1D position embeddings are\nutilized. The output of these processes will be the input to the following transformer encoder\nblock.\n(c) Transformer encoder block\nThe transformer encoder consists of N repeated layers, with each layer consisting of two\nsublayers. Sublayer one is a multihead self-attention layer [ 14]. Sublayer two is a fully\nconnected feed-forward network. Two operations are applied on both sublayers, including\nnormalization and residual connection. Normalization [ 21] is performed on every sublayer.\nResidual connections [ 22] between input and output are conducted for each sublayer. From\nthe two operations, each sublayer has the output of x + sublayer (Norm( x)), where sublayer\n(x) is the function of the sublayer itself over input x.\nTo explain the concept of multihead self-attention, we need to look at an attention function\nﬁrst. An attention function can be described as mapping a query and a set of key-value pairs\nto an output. Attention is usually called scaled dot-product attention [ 14]. The input of\nthe attention function is a group of vectors, including a query and a set of key-value pairs\n(as depicted in Fig. 4). The output is a weighted sum of the values where the weight is\n123\n9340 G. Yang et al.\nFig. 4 Queries (Q), keys (K), and\nvalues (V)\ncalculated by a compatibility function of the query with the related key. Attention is also\ncalled self-attention, since it relates different positions of a single input sequence to compute\na representation of the sequence.\nAssume the input consists of queries, keys of dimension d\nk , and values of dimension dv .\nThe queries, keys, and values are packed together into matrices Q, K , and V .T h e s et h r e e\nmatrices are the result of multiplying the embedding of input vector X with each of the three\nweight matrices ( WQ,WK,WV ) that are acquired via training. Then, a score is calculated by\nmultiplying Q and K divided by the square root of the dimension of the key vectors, resulting\nin more stable gradients. The result is passed to a softmax function, of which the sum of all\nthe outputs is 1. Then, the result is multiplied by the value vector V to create the attention\nfor this input X. The attention is expressed as Eq. ( 1).\nAttention(Q, K, V) = softmax\n( QK\nT\n√dk\n)\nV( 1 )\nMultihead self-attention runs an attention function in parallel, allowing the model to\njointly attend to information from different representation subspaces at different positions.\nThe outputs of the operation are concatenated and then converted to linear units of the\nexpected dimension [ 14]. This is expressed as Eq. ( 2).\nMutiHeadSelfAttention(Q, K , V ) = Concat(head\n1,..., headh )W O\nwhereheadi = Attention(QW Q\ni , KW K\ni , VW V\ni ) (2)\nwhere the projections are parameter matrices W Q\ni ∈ Rdmodel ×dk , W K\ni ∈ Rdmodel ×dk , W V\ni ∈\nRdmodel ×dv ,a n d W O ∈ Rhd v ×dmodel . Also, dmodel is a constant value of the dimension.\nThe MLP sublayer is used for additional processing of the outputs. It consists of two fully\nconnected layers and a GeLU nonlinearity layer [ 12].\n(d) Classiﬁcation block\nThe output of the set of N transformer encoders provides a good set of features for cancer clas-\nsiﬁcation. The classiﬁcation block is specially designed for cancer classiﬁcation. It consists of\nﬁve layers, including a ﬂatten layer, normalization layer 1, a dense layer, normalization layer\n123\nA Novel Vision Transformer Model for Skin Cancer Classiﬁcation 9341\n2, and a softmax layer. The ﬂatten layer converts the multidimensional output of the trans-\nformer encoder into a 1D array. In order to recenter and rescale the distribution of the input to\nprevent vanishing or exploding gradients, the output of the ﬂatten layer is batch normalized\n[21] and then inputted into the dense layer. The dense layer uses a GeLU activation function\nthat was introduced initially in [ 24] and was proven to have higher performance than ReLU\nactivation in a deep neural network. A second batch normalization is applied to the output\nof the dense layer. Finally, a softmax layer is applied to obtain the ﬁnal classiﬁcation result.\nComparing the performances shows that our revised classiﬁcation block performed slightly\nbetter than the original MLP head in ViT. The main difference was the batch normalization\nlayers inserted after each dense layer in the classiﬁcation block.\n3 Experiments and Results\nFour experiments were conducted to test the performance of four methods. The four meth-\nods included an Inception ResNet with Soft Attention (IRv2 + SA), a ResNet50 with Soft\nAttention (ResNet50 + SA) (provided by Datta et al. [ 10] with published code at GitHub),\nand two versions of the proposed ViTfSCD, including ViTfSCD-Large (ViTfSCD-L) and\nViTfSCD-Base (ViTfSCD-B). Here the particular settings of the two ViTfSCD models are\ngiven in Table 3. This section presents the details of the experiments and the results of the\nproposed methods along with the compared methods.\n(a) Data preparation\nAs mentioned above, the HAM10000 dataset was used for training and test data. The images\nwere resized for the different methods: 384 × 384 for ViTfSCD, 299 × 299 for the IRv2 +\nSA, and 224 × 224 for the ResNet50 + SA, respectively.\nThe HAM10000 dataset includes seven classes of skin cancer, of which the number of\nimages is summarized in Table 1. The dataset ﬁrst underwent a preparation process that\nmainly included duplication removal, class rebalancing, and extended augmentation at the\npixel level. The duplication removal removed duplicate images in the dataset. The class\nrebalancing process, as described in Sect. 3(a), aimed to allocate each class of the seven classes\nof training data the same number of images to achieve better classiﬁcation performance. The\nextended augmentation at the pixel level added operations, such as random saturation, random\ncontrast, and random brightness of pixels.\nTable 1 Number of images for\neach skin cancer class in the\nHAM10000 dataset\nType of skin\ncancer\nNumber of\nimages\nImages in training set after\noversampling\nAKIEC 327 6,992\nBCC 514 7,858\nBKL 1,099 7,931\nDF 115 6,876\nMEL 1,113 7,903\nNV 6,705 8,042\nV ASC 142 7,090\nTotal 10,015 52,692\n123\n9342 G. Yang et al.\nTable 2 Parameters of two vision transformer for skin cancer detection (ViTfSCD) models\nModel Layers Hidden size MLP size Heads Parameters\nViTfSCD-base 12 768 3072 12 86 million\nViTfSCD-large 24 1024 5120 16 307 million\nWhen Datta et al. [ 10] trained the dataset with different training-test splits, they\nfound that 85% to 15% training and testing splits had better performance than 80%\nto 20% and 70% to 30% splits. In our experiment, 85% to 15% splits were used as\nwell.\n(b) Model training and ﬁne-tuning\nBoth versions of the proposed ViTfSCD, i.e., ViTfSCD-L and ViTfSCD-B, have 16 × 16\npatch size. However, they are different in their number of layers, hidden size, MLP size, and\nnumber of heads, as detailed in Table 2. Here, the layer is how many times the transformer\nencoder is repeated. The hidden size represents the token vector’s length. The MLP size is\nthe number of nodes of the ﬁrst layer of the MLP section in each encoder block. The number\nof heads is the number of heads of the multihead attention. The parameters are the number\nof variables in the whole model that will learn during the training process.\nThe training of the ViTfSCD models started with pretrained weights from ImageNet for\nthe original ViT model. According to [ 12], after applying weight from the pretrained model\nusing a large dataset, ViT performance can improve when trained on a smaller dataset. A\nlow learning rate for training was used to ﬁne-tune the ViTfSCD models to the adopted\ndatasets.\n(c) Evaluation\nIn our experiment, accuracy, precision, sensitivity or recall, speciﬁcity, and F1 score were\nused to evaluate the performance of each model, of which the deﬁnitions are given below.\nAccuracy = TruePo siti ve + TrueNe g ati ve\nTo ta lNu m b e r\nPrecisio n = TruePo siti ve\nTruePo siti ve + FalsePositi ve\nSensiti vity = TruePo siti ve\nTruePo siti ve + FalseNegati ve\nF 1Score = 2 ∗ ( Precisio n ∗ Sensiti vity )\nPrecisio n + Sensiti vity\nSpeci f icity = TrueNe g ati ve\nTrueNe g ati ve + FalsePositi ve\nAccuracy is the ratio of the number of correctly predicted images divided by the\ntotal number of tested images; precision is the ratio of the number of correctly pre-\ndicted positive images divided by the total number of predicted positive images in each\ntype; sensitivity is the ratio of the number of correctly predicted positive images of\nthe type divided by the total number of images of the type; F1 score is the weighted\n123\nA Novel Vision Transformer Model for Skin Cancer Classiﬁcation 9343\nTable 3 Comparison of each model’s precision, sensitivity, F1 score, and accuracy, where the values in bold represent the best evaluation parameters for each skin cancer type\nSkin\ncancer\ntype\nPrecision Sensitivity F1 score Speciﬁcity\nIRv2\n+\nSA\nRes\nNet\n50 +\nSA\nViTf\nSCD-B\nViTf\nSCD-L\nIRv2\n+\nSA\nRes\nNet\n50 +\nSA\nViTf\nSCD-B\nViTf\nSCD-L\nIRv2\n+\nSA\nRes\nNet\n50 +\nSA\nViTf\nSCD-B\nViTf\nSCD-L\nIRv2\n+\nSA\nRes\nNet\n50 +\nSA\nViTf\nSCD-B\nViTf\nSCD-L\nAKIEC 0.81 0.67 0.82 0.66 0.57 0.52 0.61 0.83 0.67 0.59 0.70 0.73 1.00 0.99 1.00 0.99\nBCC 0.95 0.90 1.00 0.89 0.73 0.69 0.54 0.65 0.83 0.78 0.70 0.76 1.00 1.00 1.00 1.00\nBKL 0.73 0.72 0.81 0.86 0.79 0.70 0.65 0.76 0.76 0.71 0.72 0.81 0.98 0.98 0.99 0.99\nDF 0.62 0.62 0.67 0.60 0.83 0.83 1.00 1.00 0.71 0.71 0.80 0.75 1.00 1.00 1.00 1.00\nMEL 0.70 0.62 0.60 0.79 0.47 0.59 0.53 0.68 0.56 0.61 0.56 0.73 0.99 0.99 0.99 0.99\nNV 0.95 0.96 0.94 0.97 0.97 0.98 0.98 0.99 0.96 0.97 0.96 0.98 0.80 0.84 0.76 0.88\nV ASC 0.91 0.83 0.83 1.00 1.00 1.00 1.00 1.00 0.95 0.91 0.91 1.00 1.00 1.00 1.00 1.00\n123\n9344 G. Yang et al.\naverage of sensitivity and precision, considered a better indicator of the classiﬁer’s per-\nformance than the regular accuracy. Speciﬁcity is the ratio of the number of correctly\npredicted negative images of the type divided by the total number of images not in the\ntype.\nExperiments were conducted on the classiﬁcation performance of the four methods,\nincluding IRv2 + SA, ResNet50 + SA, ViTfSCD-L, and ViTfSCD-B. The results are\ngiven in Table 3. Note: For accuracy, only overall accuracy is given, and the individual\naccuracy for each skin cancer type is not given. The individual accuracy for each skin can-\ncer type is not given since the individual accuracy values do not reﬂect the classiﬁcation\nperformance when the case numbers of the seven cancer types differ greatly in the test\ndataset.\nIn the table, all the best values of each performance for each cancer type are marked in\nbold. It can be seen from the table that the ViTfSCD-L model had the highest precision,\nsensitivity, and F1 scores. Moreover, the overall accuracy of ViTfSCD-L was the highest\n(94.1%), and the overall accuracy of ViTfSCD-B was 91.4%. Figure 5 below shows the\ncomparison of precision, sensitivity, and F1 score across seven types of skin cancer and four\ntested models.\n(d) Comparison of overall accuracy across models\nWe compared our result with the state-of-the-art methods of the HAM10000 dataset in Table\n5. Two earlier methods were also referenced for comparison. Since only the accuracy was\navailable from the two papers, only accuracy is compared here. The ﬁrst was the work of\nNielsen et al. [ 17] in 2020, in which they built an ensemble of multi-resolution Efﬁcient-\nNets, achieving an accuracy of 92.6% from the HAM10000 dataset. The second was the\nsemi-supervised medical image classiﬁcation method conducted by Liu et al. [ 18] in 2021,\nachieving an accuracy of 92.5% with the same dataset. These two works were based on\ndata splitting and data augmentation methods with the HAM10000 dataset, while the results\nof the remaining six methods were based on the same training and testing dataset but our\ndata augmentation method. In addition, the results achieved with ResNet50, ResNet50 +\nSA, IRv2, and IRv2 + SA in their original papers are shown along with the results that we\nachieved in our experiments. Since the research [ 10] shows evidence that Soft Attention may\nimprove baseline models (ResNet, IRv2), we did not test the baseline models again in our\nwork. However, we conducted the experiments using the original ViT models (ViT-Base 16\nand ViT-Large 16) on the same dataset with the same approach, and the results show that our\nViTfSCD-B and ViTfSCD-L models outperformed the original ViT models.\nIt can be seen from Table 4 that our ﬁne-tuned ViTfSCD-L model outperformed other\nmodels by 1–2% in overall accuracy.\nWe also conducted the experiment using the Edinburgh DERMOFIT dataset [ 28], which\nis a clinical skin lesion dataset with 10 classes, including basal cell carcinoma, squamous cell\ncarcinoma, intraepithelial carcinoma, malignant melanoma, melanocytic nevus, seborrheic\nkeratosis, pyogenic granuloma, hemangioma, and dermatoﬁbroma. The dataset contains 1300\nimages taken with a digital camera under similar settings. In earlier research [ 29], traditional\nmethods were compared and showed that ResNet50 performed better than decision tree and\nKNN. In our experiment, we compared ResNet50, IRv2, and our ViTfSCD-B on the same\ntraining/test split and augmentation method. The results in Table 5 indicate that ViTfSCD-B\noutperformed other models.\n123\nA Novel Vision Transformer Model for Skin Cancer Classiﬁcation 9345\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nAKIEC BCC BKL DF MEL NV VASC\nPrecision\nIRv2 + SA ResNet50 + SA ViTfSCD-B ViTfSCD-L\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nAKIEC BCC BKL DF MEL NV VASC\nSensitivity\nIRv2 + SA ResNet50 + SA ViTfSCD-B ViTfSCD-L\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nAKIEC BCC BKL DF MEL NV VASC\nF1 Score\nIRv2 + SA ResNet50 + SA ViTfSCD-B ViTfSCD-L\nFig. 5 Comparison of precision, sensitivity, and F1 score across seven types of skin cancer and four tested\nmodels\n123\n9346 G. Yang et al.\nTable 4 Comparison of overall accuracy across models with the HAM10000 dataset\nMethods Year Overall accuracy in their\npaper\nOverall accuracy in our\nexperiments\nLoss balancing and ensemble\n[17]\n2020 92.6% –\nSemi-supervised [ 18] 2021 92.54% –\nResNet50 [ 10] 2021 90.5%\nIRv2 [ 10] 2021 91.2%\nResNet50 with Soft Attention\n[10]\n2021 91.5% 91.5%\nInception ResNet with Soft\nAttention [ 10]\n2021 93.4% 91.9%\nVision transformer\n(ViT)-Base16\n2021 – 91.1%\nViT-Large16 2021 – 93.7%\nViT for skin cancer\ndetection-Base\n2022 – 91.4%\nViT for skin cancer\ndetection-Large\n2022 – 94.1%\nTable 5 Comparison of overall accuracy across models with the DERMOFIT dataset\nMethods Year Overall accuracy in their\npaper\nOverall accuracy in our\nexperiments\nDecision tree [ 29] 2020 78.1% –\nFlat ResNet50 [ 29] 2020 78.7% –\nResNet50 2022 – 71.2%\nInception ResNet (IRv2) 2022 – 71.9%\nIRv2 + Soft Attention 2022 – 75.0%\nVision transformer for skin\ncancer detection-Base\n2022 – 80.5%\n4 Discussion and Future Work\nIn this section, ﬁrst, the effect of an attention layer is examined using the attention map feature\nof the attention layer. Then, a performance analysis of transformer models on different sizes\nis presented. Finally, future improvements are proposed.\n(a) Attention map\nOne of the essential features in ViTfSCD is the attention map, which is the output of the\nattention layers in the transformer encoder block. Figure 6 shows an original image (left\nside) and its attention map (right side). In the attention map, the skin lesion areas are brighter,\nwhereas the areas with less relevance to the illness are darker.\n123\nA Novel Vision Transformer Model for Skin Cancer Classiﬁcation 9347\nFig. 6 An original image (left) and its attention map (right). In the attention map, the diseased areas are brighter\nThe attention map gives smaller weights to the irrelevant parts of the image. The irrelevant\nparts have less inﬂuence on the output or classiﬁcation result of the model. The smaller the\nweights for the irrelevant parts are, the higher accuracy the classiﬁcation can achieve (Fig. 6).\nWhen comparing this feature to the traditional visual explanation of algorithms in deep\nlearning, Datta et al. [ 10] found that attention generally performs better than methods such as\nGrad-CAM heat map [ 10]. The attention map shows that the diseased area is more relevant\nto the output of the entire model with higher weights. As the diseased areas highlighted by\nthe attention map are assigned higher weights than the non-diseased areas, the highlighted\nareas have more inﬂuence on the output of the model, resulting in more accurate detection.\n(b) The performance of transformer models of different size\nIn the work of [ 12], ViTs were classiﬁed into three types: ViT-Base, ViT-Large, and ViT-Huge\naccording to the number of layers, hidden size, MLP size, and number of heads. Generally,\nlarger models have larger layers and sizes with more parameters. ViT-Base has 86 million\nparameters, ViT-Large has 307 million parameters, and ViT-Huge has 632 million parameters.\nFollowing the model type is patch size. For example, ViT-L/16 stands for ViT-Large with\na patch size of 16 ×16, and ViT-B/32 stands for ViT-Base with a patch size of 32 ×32. The\nsmaller the patch size, the more expensive the computation, and the better performance it\ncan achieve [ 12].\nIn 2021, an even bigger model, ViT-G/14 was built [ 20]. It further improved the accuracy\nof the ImageNet classiﬁcation task to the state-of-the-art level of 90.45% from the 88.04%\nachieved by ViT-H/14.\n(c) Limitations of current ViT models\nAlthough the ViT model brings performance gains, we should also note its limitations. First,\nthe volume of the transformer model increases signiﬁcantly compared to various baseline\nmodels. For example, ResNet50 has 25 million parameters, IRv2 has 56 million parameters,\nand ViT-Large16 and ViTfSCD-L, which achieved better performance in our experiments,\nhave 307 million parameters [12]. The more powerful model ViT-H/14 has 632 million param-\neters. The size of the model can signiﬁcantly increase the cost of training and deployment.\nSecond, data efﬁciency has been an open research problem since the birth of transformer.\nViT models normally require extensive pretraining to achieve a boost in performance. For\nexample, ViT-Huge and ViT-G/14 both achieved outstanding performance with the bench-\nmark ImageNet dataset after being pretrained on a 300 million image dataset—JFT-300M\n[19]. Large data requirements can be a bottleneck of the ViT models since the data limitation\nexists in many domains.\n123\n9348 G. Yang et al.\n(d) Frontier and open areas\nModel generalization is a key topic in skin cancer classiﬁcation work. Most of the research\nso far is based on dermatological images acquired by medical imaging equipment. When\nthese models are applied to images obtained by other devices, such as mobile devices or\nregular cameras, there tends to be a noticeable drop in performance. Transfer learning is one\npossible solution, but only if there is a reliable training dataset in the new domain, which is\noften missing.\nModel robustness is another topic in skin cancer classiﬁcation. V arious major training sets\ncurrently provide high-quality and high-resolution images, which results in models that are\nless tolerant of noise and disturbances. One example is images collected by mobile phones,\nwhich tend to have different lighting conditions, perspectives, backgrounds, and resolutions.\nThis also leads to a signiﬁcant drop in model performance.\nThe efﬁciency of the model is also important. A large number of recent studies have\nimproved model performance but have also greatly increased model size and training difﬁ-\nculty. The size of the model determines whether the model can be easily adapted to various\ndevices with different computing and storage capabilities. The difﬁculty of training directly\nincreases the cost of training.\nSimilarly, the limitations of the training set also limit the performance of the model. The\nimbalance in the amount of data across categories is an obvious problem. In some datasets,\ncommon skin cancer categories, such as BCC and SCC, occupy a large amount of data, while\nother less common but more dangerous skin cancer categories lack data, and some categories\nare even missing. One of the results is overﬁtting. Current solutions include generating\nsamples through algorithms or augmentation and assigning different class weights to the loss\nfunction.\n5 Conclusion\nThis paper presents a novel method of skin cancer classiﬁcation using clinical skin images.\nThe method consists of four blocks, including skin cancer class rebalancing for better clas-\nsiﬁcation performance, image splitting and tokenization for input preparation, transformer\nencoding to extract key features for classiﬁcation, and ﬁnal skin cancer classiﬁcation. Trans-\nfer learning is implemented to build the whole network, where the ImageNet dataset is used\nto pretrain the network and the HAM10000 dataset is used to ﬁne-tune the network. Experi-\nments have shown that the method achieves a classiﬁcation accuracy of 94.1%, outperforming\nthe current state-of-the-art model, IRv2 + SA, on the same training and testing datasets. The\nexperiment has also shown that the attention map feature brought by the attention layers fur-\nther boosts the model’s performance, making it a novel classiﬁcation model for skin cancer.\n6 Future Work\nOur ViTfSCD model can be further investigated and improved in two aspects, including\nlarger size and transfer learning. As scaling up transformer models has been shown to improve\naccuracy, there is therefore a strong possibility that applying larger transformer models to skin\ncancer classiﬁcation tasks can also enhance the performance of classiﬁcation. More studies\nwill be conducted to seek better performance from larger ViTfSCD models. In developing\nViTfSCD models, we implemented transfer learning by using the pretrained weights for ViT\n123\nA Novel Vision Transformer Model for Skin Cancer Classiﬁcation 9349\nfrom ImageNet-21 k published by Google [ 12]. Future work can focus on developing more\nmodels by utilizing pretrained weights on a larger dataset, such as JFT-300 M [ 19].\nAcknowledgements This research received no speciﬁc grant from any funding agency in the public, com-\nmercial, or not-for-proﬁt sectors.\nAuthors’ Contributions Guang Yang prepared the data, conducted the experiment, and drafted the manuscript.\nSuhuai Luo reviewed the draft and enhanced the introduction, methodology, and conclusion sections. Peter\nGreer reviewed the manuscript and made suggestions. All authors reviewed the ﬁnal manuscript.\nFunding Open Access funding enabled and organized by CAUL and its Member Institutions.\nDeclarations\nCompeting interests None declared.\nEthical approval Not required.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which\npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give\nappropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence,\nand indicate if changes were made. The images or other third party material in this article are included in the\narticle’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is\nnot included in the article’s Creative Commons licence and your intended use is not permitted by statutory\nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.\nTo view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/ .\nReferences\n1. Siegel RL, Naishadham D, Jemal A (2012) Cancer statistics. CA 62(1):10–29. https://doi.org/10.3322/\ncaac.20138\n2. Australian Bureau of Statistics (2019) Causes of Death, Australia [Internet]. ABS, Canberra. Accessed\n2022 Nov 1. https://www.abs.gov.au/statistics/health/causes-death/causes-death-australia/2019.\n3. Street W (2019) Cancer Facts & Figures. American Cancer Society, Atlanta, GA. http://\ncancerstatisticscenter.cancer.org. Accessed 2022 Nov 1.\n4. Bray F, Ferlay J, Soerjomataram I, Siegel RL, Torre LA, Jemal A (2018) Global cancer statistics 2018:\nGLOBOCAN estimates of incidence and mortality worldwide for 36 cancers in 185 countries. CA\n68(6):394–424\n5. Siegel RL, Miller KD, Jemal A (2019) Cancer statistics. CA 69(1):7–3\n6. V estergaard ME, Macaskill PH, Holt PE, Menzies SW (2008) Dermoscopy compared with naked eye\nexamination for the diagnosis of primary melanoma: a meta-analysis of studies performed in a clinical\nsetting. Br J Dermatol 159(3):669–676\n7. Menzies SW, Bischof L, Talbot H, Gutenev A, Avramidis M, Wong L, Lo SK, Mackellar G, Skladnev V ,\nMcCarthy W, Kelly J (2005) The performance of SolarScan: an automated dermoscopy image analysis\ninstrument for the diagnosis of primary melanoma. Arch Dermatol 141(11):1388–1396\n8. Esteva A, Kuprel B, Novoa RA, Ko J, Swetter SM, Blau HM, Thrun S (2017) Dermatologist-level\nclassiﬁcation of skin cancer with deep neural networks. Nature 542(7639):115–118\n9. Adeyinka AA, Viriri S (2018) Skin lesion images segmentation: a survey of the state-of-the-art. In:\nInternational conference on mining intelligence and knowledge exploration. Springer, Cham, pp. 321–330\n10. Datta SK, Shaikh MA, Srihari SN (2021) Soft Attention Improves Skin Cancer Classiﬁcation Performance.\nInInterpretability of Machine Intelligence in Medical Image Computing, and Topological Data Analysis\nand Its Applications for Medical Data. Springer, Cham, pp 13–23\n11. Nadipineni H (2020) Method to classify skin lesions using dermoscopic images. arXiv preprint arXiv:\n2008.09418. 2020 Aug 21.\n123\n9350 G. Yang et al.\n12. Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X, Unterthiner T, Dehghani M, Minderer\nM, Heigold G, Gelly S, Uszkoreit J (2020) An image is worth 16x16 words: Transformers for image\nrecognition at scale. arXiv preprint arXiv:2010.11929. 2020 Oct 22\n13. Tschandl P , Rosendahl C, Kittler H (2018) The HAM10000 dataset, a large collection of multi-source\ndermatoscopic images of common pigmented skin lesions. Sci Data 5(1):1–9\n14. V aswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser Ł, Polosukhin I (2017)\nAttention is all you need. In: Advances in neural information processing systems. 2017, 30\n15. Devlin J, Chang MW, Lee K, Toutanova K. Bert: Pre-training of deep bidirectional transformers for\nlanguage understanding. arXiv preprint arXiv:1810.04805. 2018 Oct 11.\n16. Radford A, Wu J, Child R, Luan D, Amodei D, Sutskever I (2019) Language models are unsupervised\nmultitask learners. OpenAI blog 1(8):9\n17. Gessert N, Nielsen M, Shaikh M, Werner R, Schlaefer A (2020) Skin lesion classiﬁcation using ensembles\nof multi-resolution EfﬁcientNets with meta data. MethodsX 1(7):100864\n18. Liu Q, Y u L, Luo L, Dou Q, Heng PA (2020) Semi-supervised medical image classiﬁcation with relation-\ndriven self-ensembling model. IEEE Trans Med Imaging 39(11):3429–3440\n19. Sun C, Shrivastava A, Singh S, Gupta A (2017) Revisiting unreasonable effectiveness of data in deep\nlearning era. In: Proceedings of the IEEE international conference on computer vision 2017, pp 843–852\n20. Zhai X, Kolesnikov A, Houlsby N, Beyer L (2022) Scaling vision transformers. In: Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition 2022, pp 12104–12113\n21. Ba JL, Kiros JR, Hinton GE. Layer normalization. arXiv preprint arXiv:1607.06450. 2016 Jul 21.\n22. He K, Zhang X, Ren S, Sun J (2016) Deep residual learning for image recognition. In: Proceedings of\nthe IEEE conference on computer vision and pattern recognition 2016, pp 770–778\n23. Tomita N, Abdollahi B, Wei J, Ren B, Suriawinata A, Hassanpour S (2019) Attention-based deep neural\nnetworks for detection of cancerous and precancerous esophagus tissue on histopathological slides. JAMA\nNetw Open 2(11):e1914645\n24. Hendrycks D, Gimpel K. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415. 2016 Jun\n27.\n25. Melas-Kyriazi L. Do you even need attention? a stack of feed-forward layers does surprisingly well on\nimagenet. arXiv preprint arXiv:2105.02723. 2021 May 6.\n26. Tolstikhin IO, Houlsby N, Kolesnikov A, Beyer L, Zhai X, Unterthiner T, Y ung J, Steiner A, Keysers D,\nUszkoreit J, Lucic M (2021) Mlp-mixer: an all-mlp architecture for vision. Adv Neural Inf Process Syst\n6(34):24261–24272\n27. Touvron H, Bojanowski P , Caron M, Cord M, El-Nouby A, Grave E, Izacard G, Joulin A, Synnaeve G,\nV erbeek J, Jégou H (2022) Resmlp: feedforward networks for image classiﬁcation with data-efﬁcient\ntraining. IEEE Trans Pattern Anal Mach Intell\n28. Ballerini L, Fisher RB, Aldridge B, Rees J. A color and texture based hierarchical K-NN approach to the\nclassiﬁcation of non-melanoma skin lesions. In: Color medical image analysis 2013. Springer, Dordrecht,\npp 63–86\n29. Fisher RB, Rees J, Bertrand A. Classiﬁcation of ten skin lesion classes: Hierarchical knn versus deep net.\nIn: Annual conference on medical image understanding and analysis 2019 Jul 24. Springer, Cham, pp\n86–98\n30. Xin C, Liu Z, Zhao K, Miao L, Ma Y , Zhu X, Zhou Q, Wang S, Li L, Yang F, Xu S (2022) An improved\ntransformer network for skin cancer classiﬁcation. Comput Biol Med 1(149):105939\n31. He X, Tan EL, Bi H, Zhang X, Zhao S, Lei B (2022) Fully transformer network for skin lesion analysis.\nMed Image Anal 1(77):102357\n32. Nakai K, Chen YW, Han XH (2022) Enhanced deep bottleneck transformer model for skin lesion classi-\nﬁcation. Biomed Signal Process Control 1(78):103997\n33. Alencar FE, Lopes DC, Neto FM (2016) Development of a system classiﬁcation of images dermoscopic\nfor mobile devices. IEEE Latin Am Trans 14(1):325–330\n34. Kasmi R, Mokrani K (2016) Classiﬁcation of malignant melanoma and benign skin lesions: implemen-\ntation of automatic ABCD rule. IET Image Proc 10(6):448–455\n35. Sáez A, Sánchez-Monedero J, Gutiérrez PA, Hervás-Martínez C (2015) Machine learning methods for\nbinary and multiclass classiﬁcation of melanoma thickness from dermoscopic images. IEEE Trans Med\nImaging 35(4):1036–1045\n36. Ma Z, Tavares JM (2015) A novel approach to segment skin lesions in dermoscopic images based on a\ndeformable model. IEEE J Biomed Health Inform 20(2):615–623\n37. Pathan S, Prabhu KG, Siddalingaswamy PC (2019) Automated detection of melanocytes related pig-\nmented skin lesions: a clinical framework. Biomed Signal Process Control 1(51):59–72\n123\nA Novel Vision Transformer Model for Skin Cancer Classiﬁcation 9351\n38. Kharazmi P , AlJasser MI, Lui H, Wang ZJ, Lee TK (2016) Automated detection and segmentation of\nvascular structures of skin lesions seen in Dermoscopy, with an application to basal cell carcinoma\nclassiﬁcation. IEEE J Biomed Health Inform 21(6):1675–1684\n39. Dalila F, Zohra A, Reda K, Hocine C (2017) Segmentation and classiﬁcation of melanoma and benign\nskin lesions. Optik 1(140):749–761\n40. Noroozi N, Zakerolhosseini A (2016) Computer assisted diagnosis of basal cell carcinoma using Z-\ntransform features. J Vis Commun Image Represent 1(40):128–148\n41. Zakeri A, Hokmabadi A (2018) Improvement in the diagnosis of melanoma and dysplastic lesions by\nintroducing ABCD-PDT features and a hybrid classiﬁer. Biocybern Biomed Eng 38(3):456–466\n42. Satheesha TY , Satyanarayana D, Prasad MG, Dhruve KD (2017) Melanoma is skin deep: a 3D recon-\nstruction technique for computerized dermoscopic skin lesion classiﬁcation. IEEE J Transl Eng Health\nMed 16(5):1–7\n43. Oliveira RB, Marranghello N, Pereira AS, Tavares JM (2016) A computational approach for detecting\npigmented skin lesions in macroscopic images. Expert Syst Appl 1(61):53–63\nPublisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and\ninstitutional afﬁliations.\n123",
  "topic": "Artificial intelligence",
  "concepts": [
    {
      "name": "Artificial intelligence",
      "score": 0.6856865882873535
    },
    {
      "name": "Normalization (sociology)",
      "score": 0.6793899536132812
    },
    {
      "name": "Transformer",
      "score": 0.6608970761299133
    },
    {
      "name": "Computer science",
      "score": 0.6561245918273926
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5954481959342957
    },
    {
      "name": "Residual",
      "score": 0.4869919717311859
    },
    {
      "name": "Encoder",
      "score": 0.48008161783218384
    },
    {
      "name": "Skin cancer",
      "score": 0.4727631211280823
    },
    {
      "name": "Deep learning",
      "score": 0.4686650037765503
    },
    {
      "name": "Cancer",
      "score": 0.23204541206359863
    },
    {
      "name": "Algorithm",
      "score": 0.1889001727104187
    },
    {
      "name": "Medicine",
      "score": 0.17375034093856812
    },
    {
      "name": "Voltage",
      "score": 0.13035839796066284
    },
    {
      "name": "Engineering",
      "score": 0.10507315397262573
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Anthropology",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Internal medicine",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    }
  ]
}