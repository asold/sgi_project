{
    "title": "Efficient Large Language Model Inference with Vectorized Floating Point Calculations",
    "url": "https://openalex.org/W4399616901",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2103889068",
            "name": "Jacob Owens",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5108942806",
            "name": "Skylar Matthews",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4391143839",
        "https://openalex.org/W4396901427",
        "https://openalex.org/W4398782346",
        "https://openalex.org/W4396225444",
        "https://openalex.org/W4396667188",
        "https://openalex.org/W4391901128",
        "https://openalex.org/W4394846382",
        "https://openalex.org/W4398176048",
        "https://openalex.org/W4388581500",
        "https://openalex.org/W3168867926",
        "https://openalex.org/W4389761026",
        "https://openalex.org/W4399328902",
        "https://openalex.org/W4396914777",
        "https://openalex.org/W4393867173",
        "https://openalex.org/W4398131214",
        "https://openalex.org/W4396901469",
        "https://openalex.org/W4389326193"
    ],
    "abstract": "The development of highly sophisticated language models has revolutionized various natural language processing tasks, demanding efficient inference processes to ensure real-time responsiveness and minimal computational resource usage. Vectorized floating point calculations present a novel and significant approach to enhancing the computational efficiency of language model inference, leveraging parallel processing capabilities to achieve substantial performance improvements. This article details the implementation of vectorized floating point calculations within GPT-Neo, demonstrating a notable 12\\% increase in inference speed through comprehensive benchmarks and datasets. The evaluation highlights the optimized model's ability to reduce inference time, increase computational throughput, and lower memory usage and energy consumption without compromising accuracy. The findings reveal the potential of vectorized operations to enhance the scalability and operational efficiency of advanced language models, paving the way for more responsive and resource-efficient AI applications across diverse deployment scenarios.",
    "full_text": null
}