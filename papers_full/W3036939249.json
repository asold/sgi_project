{
  "title": "Multi-branch Attentive Transformer",
  "url": "https://openalex.org/W3036939249",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1970297279",
      "name": "Fan Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2379644241",
      "name": "Xie, Shufang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223307627",
      "name": "Xia, Yingce",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2146720877",
      "name": "Wu Lijun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2105471080",
      "name": "Qin Tao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2245941921",
      "name": "Li Xiang Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4209371238",
      "name": "Liu, Tie-Yan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2807964941",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2950501607",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W2912521296",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2767989436",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2964350391",
    "https://openalex.org/W2964190861",
    "https://openalex.org/W2549139847",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2963500743",
    "https://openalex.org/W2806311723",
    "https://openalex.org/W2996159613",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2963991316",
    "https://openalex.org/W2908336025",
    "https://openalex.org/W2989571009",
    "https://openalex.org/W2963975324",
    "https://openalex.org/W2971008324",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2952355681",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3035204084",
    "https://openalex.org/W2785366763",
    "https://openalex.org/W2888557792",
    "https://openalex.org/W2913535645",
    "https://openalex.org/W2097117768",
    "https://openalex.org/W2810075754"
  ],
  "abstract": "While the multi-branch architecture is one of the key ingredients to the success of computer vision tasks, it has not been well investigated in natural language processing, especially sequence learning tasks. In this work, we propose a simple yet effective variant of Transformer called multi-branch attentive Transformer (briefly, MAT), where the attention layer is the average of multiple branches and each branch is an independent multi-head attention layer. We leverage two training techniques to regularize the training: drop-branch, which randomly drops individual branches during training, and proximal initialization, which uses a pre-trained Transformer model to initialize multiple branches. Experiments on machine translation, code generation and natural language understanding demonstrate that such a simple variant of Transformer brings significant improvements. Our code is available at \\url{https://github.com/HA-Transformer}.",
  "full_text": "Multi-branch Attentive Transformer∗\n1Yang Fan,2Shufang Xie,2Yingce Xia,2Lijun Wu,2Tao Qin,1Xiang-Yang Li,2Tie-Yan Liu\n1 University of Science and Technology of China 2 Microsoft Research Asia\n1fyabc@mail.ustc.edu.cn, xiangyangli@ustc.edu.cn\n2{shufxi,yingce.xia,lijun.wu,taoqin,tyliu}@microsoft.com\nAbstract\nWhile the multi-branch architecture is one of the key ingredients to the success\nof computer vision tasks, it has not been well investigated in natural language\nprocessing, especially sequence learning tasks. In this work, we propose a simple\nyet effective variant of Transformer [26] called multi-branch attentive Transformer\n(brieﬂy, MAT), where the attention layer is the average of multiple branches\nand each branch is an independent multi-head attention layer. We leverage two\ntraining techniques to regularize the training: drop-branch, which randomly drops\nindividual branches during training, and proximal initialization, which uses a pre-\ntrained Transformer model to initialize multiple branches. Experiments on machine\ntranslation, code generation and natural language understanding demonstrate that\nsuch a simple variant of Transformer brings signiﬁcant improvements. Our code is\navailable at https://github.com/HA-Transformer.\n1 Introduction\nThe multi-branch architecture of neural networks, where each block consists of more than one\nparallel components, is one of the key ingredients to the success of deep neural models and has\nbeen well studied in computer vision. Typical structures include the inception architectures [24, 25,\n23], ResNet [6], ResNeXt [32], DenseNet [8], and the network architectures discovered by neural\narchitecture search algorithms [18, 13]. Models for sequence learning, a typical natural language\nprocessing problem, also beneﬁt from multi-branch architectures. Bi-directional LSTM (BiLSTM)\nmodels can be regarded as a two-branch architecture, where a left-to-right LSTM and a right-to-left\nLSTM are incorporated into one model. BiLSTM has been applied in neural machine translation\n(brieﬂy, NMT) [ 34, 31] and pre-training [ 17]. Transformer [ 26], the state-of-the-art model for\nsequence learning, also leverages multi-branch architecture in its multi-head attention layers.\nAlthough multi-branch architecture plays an important role in Transformer, it has not been well\nstudied and explored. Speciﬁcally, using hybrid structures with both averaging and concatenation\noperations, which has been widely used in image classiﬁcation tasks [32, 25], are missing in current\nliterature for sequence learning. This motivates us to explore along this direction. We propose a\nsimple yet effective variant of Transformer, which treats a multi-head attention layer as a branch,\nduplicates such an attention branch for multiple times, and averages the outputs of those branches.\nSince both the concatenation operation (for the multiple attention heads) and the averaging operation\n(for the multiple attention branches) are used in our model, we call our model multi-branch attentive\nTransformer (brieﬂy, MAT) and such an attention layer with both the concatenation and adding\noperations as the multi-branch attention layer.\nDue to the increased structure complexity, it is challenging to directly train MAT. Thus, we leverage\ntwo techniques for MAT training. (1) Drop branch: During training, each branch should be inde-\npendently dropped so as to avoid possible co-adaption between those branches. Note that similar to\n∗This work is conducted at Microsoft Research Asia.\nPreprint. Under review.\narXiv:2006.10270v2  [cs.CL]  26 Jul 2020\nDropout, during inference, all branches are used. (2) Proximal initialization: We initialize MAT with\nthe corresponding parameters trained on a standard single-branch Transformer.\nOur contributions are summarized as follows:\n(1) We propose a simple yet effective variant of Transformer, multi-branch attentive Transformer\n(MAT). We leverage two techniques, drop branch and proximal initialization, to train this new variant.\n(2) We conduct experiments on three sequence learning tasks: neural machine translation, code\ngeneration and natural language understanding. On these tasks, MAT signiﬁcantly outperforms the\nstandard Transformer baselines, demonstrating the effectiveness of our method.\n(3) We explore another variant which introduces multiple branches into feed-forward layers. We ﬁnd\nthat such a modiﬁcation slightly hurts the performance.\n2 Background\n2.1 Introduction to Transformer\nA Transformer model consists of an encoder and a decoder. Both the encoder and the decoder are\nstacks of blocks. Each block is mainly made up of two types of layers: the multi-head attention layer\nand the feed-forward layer (brieﬂy, FFN). We will mathematically describe them.\nLet concat(···) denote the concatenation operation, where all inputs are combined into a larger\nmatrix along the last dimension. Let attnM and attn denote a multi-head attention layer with\nM heads (M ∈Z+) and a standard attention layer. A standard attention layer [ 3, 26] takes three\nelements as inputs, including query Q, key Kand value V, whose sizes are Tq ×d, T ×dand T ×d\n(Tq,T,d are integers). attn is deﬁned as follows:\nattn(Q,K,V ) = softmax\n(QK⊤\n√\nd\n)\nV, (1)\nwhere softmax is the softmax operation. A multi-head attention layer aggregates multiple attention\nlayers in the concatenation way:\nattnM (Q,K,V ) = concat(H1,H2,··· ,HM ); Hi = attn(QWi\nQ,KW i\nK,VW i\nV ), i∈[M],\n(2)\nwhere [M] denotes the set {1,2,··· ,M}. In Eqn.(2), the W’s are the parameters to be learned. Each\nW is of dimension T ×(d/M). The output of attnM is the same size as Q, i.e., Tq ×d.\nIn standard Transformer, an FFN layer, denoted by FFN, is implemented as follows:\nFFN(x) = max(xW1 + b1,0)W2 + b2, (3)\nwhere xis a 1 ×d-dimension input, max is an element-wise operator, W1 and W2 are d×dh and\ndh ×dmatrices, b1 and b2 are dh and ddimension vectors. Usually, dh >d.\nIn the encoder of a Transformer, each block consists of a self-attention layer, implemented as a\nattnM where the query Q, key Kand value V are the outputs of previous layer, and an FFN layer.\nIn the decoder side, an additional multi-head attention is inserted between the self-attention layer and\nFFN layer, which is known as the encoder-decoder attention: Qis the output of the previous block,\nKand V are the outputs of the last block in the encoder.\n2.2 Multi-branch architectures for sequence learning\nAs shown in Section 1, multi-branch architectures have been well investigated in image processing.\nIn comparison, the corresponding work for sequence learning is limited. The bidirectional LSTM, a\ntwo-branch architecture, has been applied in machine translation [34, 31] and pre-training techniques\nlike ELMo [17]. [21] proposed two use both convolutional neural networks and Transformer in the\nencoder and decoder. Similar idea is further expanded in [33]. A common practice of the previous\nwork is that they focus on which network components (convolution, attention, etc.) should be used\nin a multi-branch architecture. In this work, we do not want to introduce additional operations into\nTransformer but focus on how to boost Transformer with its own components, especially, where and\nhow to apply the multi-branch topology, and ﬁgure out several useful techniques to train multi-branch\narchitectures for sequence learning. Such aspects are missing in previous literature.\n2\n3 Multi-branch attentive Transformer\nIn this section, we ﬁrst introduce the structure of multi-branch attentive Transformer (MAT) in Sec-\ntion 3.1, and then we introduce the drop branch technique in Section 3.2. The proximal initialization\nis described in Section 3.3.\n3.1 Network architecture\nThe network architecture of our proposed MAT adopts the backbone of standard Transformer, except\nthat all multi-head attention layers (including both self-attention layers and encoder-decoder attention\nlayers) are replaced with the multi-branch attention layers.\nLet mAttnNa,M (Q,K,V ) denote a multi-branch attention layer, where Na represents the number of\nbranches and each branch is a multi-head attention layer with M heads attnM . Q, Kand V denote\nquery, key and value, which are deﬁned in Section 2. Mathematically, mAttnNa,M (Q,K,V ) works\nas follows:\nmAttnNa,M (Q,K,V ) = Q+ 1\nNa\nNa∑\ni=1\nattnM (Q,K,V ; θi), (4)\nwhere θi = {Wi\nQ,Wi\nK,Wi\nV }M\ni=1 (see Eqn.(2)) is the collection of all parameters of thei-th multi-head\nattention layer.\n3.2 Drop branch technique\nAs mentioned above, multiple branches in multi-branch attention layers have the same structure. To\navoid the co-adaptation among them, we leverage the drop branch technique. The inspiration comes\nfrom the dropout [22] and drop path technique [12], where some branches are randomly dropped\nduring training.\nLet mAttnNa,M (Q,K,V ; ρ) denote a multi-branch attention layer with drop branch rate ρ∈[0,1],\nwhich is an extension of that in Section 3.1. Equipped with drop branch technique, the i-th branch of\nthe multi-branch attention layer, denoted as βM\ni (Q,K,V ; ρ,θi), works as follows:\nβM\ni (Q,K,V ; ρ,θi) = I{Ui ≥ρ}\n1 −ρ attnM (Q,K,V ; θi), (5)\nwhere Ui is uniformly sampled from [0,1] (brieﬂy, Ui ∼unif[0,1]); I is the indicator function.\nDuring training, we may set ρ≥0, where any branch might be skipped with probability ρ. During\ninference, we must setρ= 0, where all branches are leveraged with equal weights1. The multi-branch\nattention layer mAttnNa,M (Q,K,V ; ρ) works as follows:\nmAttnNa,M (Q,K,V ; ρ) = Q+ 1\nNa\nNa∑\ni=1\nβM\ni (Q,K,V ; ρ,θi). (6)\nNote when ρ= 0, Eqn.(6) degenerates to Eqn.(4).\nDuring training, it is possible that all branches in multi-branch attention layer are dropped. The\nresidual connection ensures that even if all branches are dropped, the output from the previous layer\ncan be fed to top layers through the identical mapping. That is, the network is never blocked. When\nNa = 1 and ρ= 0, a multi-branch attention layer degenerates to a vanilla multi-head attention layer.\nWhen Na = 1 and ρ >0, it is the standard Transformer with randomly dropped attention layers\nduring training, which is similar to the drop layer [5]. (See Section 4.2 for more discussions.)\nWe also apply the drop branch technique to FFN layers. That is, the revised FFN layer is deﬁned as\nx+ I{U ≥ρ}\n1 −ρ FFN(x), U∼unif[0,1] (7)\nwhere xis the output of the previous layer. An illustration of multi-branch attentive Transformer is\nin Figure 1. Each block in the encoder consists of a multi-branch attentive self-attention layer and\nan FFN layer. Each block in the decoder consists of another multi-branch attentive encoder-decoder\nattention layer. Layer normalization [2] remains unchanged.\n3\nFigure 1: Architecture of a block in the encoder of MAT. “LN” refers to layer normalization.\n3.3 Proximal initialization\nMAT can be optimized like the standard version by ﬁrst randomly initializing all parameters and train-\ning until convergence. However, the multi-branch attention layer increases the training complexity of\nMAT, since there are multiple branches to be optimized.\nRecently, proximal algorithms [16] have been widely used in pretraining-and-ﬁnetuning framework\nto regularize training [9]. Then main idea of proximal algorithms is to balance the trade-off between\nminimizing the objective function and minimizing the weight distance with a pre-deﬁned weight.\nInspired by those algorithms, we design a two-stage warm-start training strategy:\n(1) Train a standard Transformer with embedding dimension d, FFN dimension dh.\n(2) Duplicate both the self-attention layers and encoder-decoder attention layers for Na times to\ninitialize MAT. Train this new model until convergence.\nWe empirically ﬁnd that the proximal initialization scheme can boost the performance than that\nobtained by training from scratch.\n4 Application to neural machine translation\nIn this section, we conduct two groups of experiments: one with relatively small scale data, includ-\ning IWSLT’14 German→English, IWSLT’14 Spanish↔English and IWSLT’17 French↔English\ntranslation tasks; the other with larger training corpus, WMT’14 English→German translation and\nWMT’19 German→French translation. We brieﬂy denote English, German, Spanish and French as\nEn, De, Es and Fr respectively.\n4.1 Settings\nData preprocessing: Our implementation of NMT experiments is based on fairseq2. For IWSLT’14\nDe↔En, we follow [ 4] to get and preprocess the training, validation and test data 3, including\nlowercasing all words, tokenization and applying BPE [19]. For the other two IWSLT tasks, we do\nnot lowercase the words and keep them case-sensitive. We apply tokenization and BPE as those used\nin preprocessing IWSLT De↔En. Training data sizes for IWSLT De→En, Es↔En and Fr↔En are\n160k, 183kand 236krespectively. We choose IWSLT’13 Es↔En, IWSLT’16 Fr↔En for validation\npurpose, and choose IWSLT’14 Es↔En and IWSLT’17 Fr↔En as test sets. The numbers of BPE\nmerge operation for the three tasks are all 10k. The source and target data are merged to get the BPE\ntable.\nFor WMT’14 English→German translation, we follow [15] to preprocess the data , and eventually\nobtain 4.5M training data. We use newstest 2013 as the validation set, and choose newstest 2014 as\nthe test set. The number of BPE merge operation is 32k. The preprocess steps for WMT’19 De→Fr\nare the same as those for WMT’14 En→De, and we eventually get 9M training data in total. We\nconcatenate newstest2008 to newstest 2014 together as the validation set and use newstest 2019 as\nthe test set.\nModel conﬁguration and training strategy: For the three IWSLT tasks, we choose the default setting\nprovided by fairseq ofﬁcial code 4 as the baseline with embedding dimension d = 512 , hidden\n2https://github.com/pytorch/fairseq\n3The URLs of scripts we used in this paper are summarized in Appendix C.\n4https://github.com/pytorch/fairseq/blob/master/fairseq/models/transformer.py\n4\ndimension dh = 1024 and number of heads M = 4. For WMT’14 En→De, we mainly follow the\nbig transformer setting, where the above three numbers are 1024, 4096 and 16 respectively. The\ndropout rates are 0.3. For the more detailed parameters like ρand Na, we will introduce the details\nin corresponding subsections. We use the Adam [10] optimizer with initial learning rate 5 ×10−4,\nβ1 = 0.9, β2 = 0.98 and the inverse_sqrt learning rate scheduler [26] to control training. Each\nmodel is trained until convergence. The source embedding, target embedding and output embedding\nof each task are shared. The batch size is 4096 for both IWSLT and WMT tasks. For IWSLT tasks,\nwe train on single P40 GPU; for WMT tasks, we train on eight P40 GPUs.\nEvaluation We evaluate the translation quality by BLEU scores. For IWSLT’14 De→En and WMT’14\nEn→De, following the common practice, we use multi-bleu.perl. For other tasks, we choose\nsacreBLEU.\n4.2 Exploring hyper-parameters of MAT\nDue to resource limitation, we ﬁrst explore hyper-parameters and proximal initialization on IWSLT’14\nDe→En dataset to get some empirical results, then transfer them to larger datasets.\nWe try different combination of Na, d and dh. We ensure the number of total parameters not\nexceeding 36.7M, the size of the default model for IWSLT’14 De→En. All results are reported in\nTable 1, where the network architecture is described by a four-element tuple, with each position\nrepresenting the number of branches Na, embedding dimension d, hidden dimension dh and model\nsizes.\nTable 1: Results on IWSLT’14 De→En with different architectures. The left and right subtables are\nexperiments of standard transformer and MAT respectively. From left to right in each subtable, the\ncolumns represent the network architecture, number of parameters, BLEU scores withρranging from\n0.0 to 0.3.\nNa/d/dh/Param 0.0 0 .1 0 .2 0 .3 Na/d/dh/Param 0.0 0 .1 0 .2 0 .3\nStandard Transformer + Drop Branch MAT + Drop Branch\n1/512/1024/36.7M 34.95△3.94 22 .43 0 .78 2/256/1024/18.4M 34.42 35 .19 35 .52 35 .11\n1/256/1024/13.7M 35.04 35 .39 34 .53 29 .34 2/256/2048/24.7M 34.51 35 .46 35 .59 35 .33\n1/256/2048/20.0M 34.66 35 .45 32 .75 1 .37 3/256/1024/23.1M 34.01 34 .92 35 .39 35 .44\n1/256/3072/26.3M 34.41 35 .37 34 .64 25 .05 3/256/2048/29.4M 33.98 35 .03 35 .40 35.70\n4/256/1024/27.9M 33.77 34 .84 35 .08 35 .14\n4/256/2048/34.2M 33.79 34 .81 35 .08 35 .46\nThe baselines correspond to the architectures with Na = 1 and ρ = 0. The most widely adopted\nbaseline (marked with △) is 34.95 and the model size is 36.7M. Reducing dto 256 results in slightly\nBLEU score 35.04. We have the following observations:\n(1) Using multi-branch attention layers with more than one branches can boost the translation BLEU\nscores, with a proper ρ. When setting dh = 1024 and increasing Na from 2 to 3, the BLEU scores\nare 35.52 (with ρ= 0.2) and 35.44 (with ρ= 0.3), outperforming the standard baseline 34.95, as\nwell as the BLEU score obtained from architecture 1/256/1024. However, it is not always good to\nenlarge Na. The BLEU score of MAT with Na = 4 is 35.14, which is a minor improvement over the\nbaseline.\nMulti-branch attention layers also beneﬁts from larger hidden dimension df . When setting df =\n2048, we obtain consistent improvement compared to df = 1024 with the same Na. Among all\narchitectures, we ﬁnd that the model 3/256/2048 achieves the best BLEU score, which is 35.70 (we\nalso evaluate the validation perplexity of all architectures, and the model 3/256/2048 still get the\nlowest perplexity 4.71). Compared with the corresponding Transformer 1/256/2048 with ρ = 0,\nwe get 1.04 improvement. We also enlarge ρto 0.4 and 0.5, but the BLEU scores drop. Results are\nshown in Appendix A.\n(2) The drop branch is important technique for training multi-branch attention layers. Take the\nnetwork architecture 3/256/1024 as an example: If we do not use drop branch technique, i.e., ρ= 0,\nwe can only get 34.01 BLEU score, which is 0.94 point below the baseline. As we increase ρfrom 0\nto 0.3, the BLEU scores become better and better, demonstrating the effectiveness of drop branch.\n5\n(3) When setting Na = 1 and ρB >0, the architecture still beneﬁts from the drop branch technique.\nFor architecture 1/256/1024, the vanilla baseline is 35.04. As we increase ρto 0.1, we can obtain\n0.35 point improvement. Wider networks with df also beneﬁts from this technique, which shows that\ndrop branch is generally a useful trick for Transformer. This is consistent with the discoveries in [5].\nWith ρ = 0.1, enlarging the hidden dimension to 2048 and 3072 lead to 35.45 and 35.37 BLEU\nscores, corresponding to 0.79 and 0.96 score improvements compared with that without using drop\nbranch. But we found that when Na = 1, drop branch might lead to unstable results. For example,\nwhen ρ= 0.1, 1/512/1024 cannot lead to a reasonable result. We re-run the experiments with ﬁve\nrandom seeds but always fail. In comparison, MAT can always obtain reasonable results with drop\nbranch.\nWe also try to turn off the drop branch at the FFN layers. In this case, we found that by ranging ρ\nfrom 0 to 0.3, architecture 3/256/2048 can achieve 34.63, 34.78 and 34.98 BLEU scores, which are\nworse than those obtained by using drop branch at all layers. This shows that the drop branch at every\nlayers is important for our proposed model.\n4.3 Exploring proximal initialization\nIn this section, we explore the effect of proximal initialization, i.e., warm start from an existing\nstandard Transformer model. The results are reported in Table 2. Generally, compared with the results\nwithout proximal initialization, the models can achieve more than 0.5 BLEU score improvement.\nSpeciﬁcally, with Na = 3 and dh = 2048, we can achieve a 36.22 BLEU score, setting a state-of-\nthe-art record on this task. We also evaluate the validation perplexity of all architectures in Table 2,\nand the model with Na = 3 and dh = 2048 still get the lowest perplexity 4.56.\nTable 2: Results of using proximal initialization. Columns from left to right represent the network\narchitecture, BLEU scores with drop branch ratio ρfrom 0.0 to 0.3, and the increment compared to\nthe best results without proximal initialization.\n(Na,dh) 0 .0 0 .1 0 .2 0 .3 ∆\n(2,2048) 34 .99 35 .50 35 .85 36 .12 0 .79\n(3,1024) 35 .33 35 .77 35 .94 35 .83 0 .50\n(3,2048) 34 .83 35 .63 36 .08 36.22 0.52\n(4,1024) 35 .45 35 .72 36 .07 36 .02 0 .88\n(4,2048) 35 .06 35 .61 35 .81 36 .09 0 .63\nIn summary, according to the exploration in Section 4.2 and Section 4.3, we empirically get the\nfollowing conclusions: (1) A multi-branch attention layer is helpful to improve Transformer. We\nsuggest to try Na = 2 or Na = 3 ﬁrst. (2) A larger dh is helpful to bring better performance.\nEnlarging dh to consume the remaining parameters of reducing d(of the standard Transformer) is a\nbetter choice. (3) The drop branch and proximal initialization are two key techniques to the success\nof MAT in NMT.\n4.4 Application to other NMT tasks\nResults of other IWSLT tasks: We apply the discoveries to Spanish↔English and French↔English,\nand report the results in Table 3. For each setting, both the highest BLEU scores according to\nvalidation performance as well as the corresponding ρ’s are reported. We choose1/512/1024 as the\ndefault baseline, whose model size is the upper bound of our MAT models. We also implement two\nother baselines, one with smaller dand the other with larger dh. In baselines, the ρis ﬁxed as zero.\nCompared to the standard Transformer with one branch, various MAT with different model con-\nﬁgurations outperform the baseline. The architecture 2/256/2048 with ρ= 0.2 generally obtains\npromising results. Compared with the default baseline 1/512/1024, it achieves 1.74, 1.90, 1.31, 1.25\nimprovement on Es→En, En→Es, Fr→En and En→Fr.\nResults on larger datasets: After obtaining results on small-scale datasets, we apply our discoveries\nto three larger datasets: WMT’14 En→De translation, WMT’19 De→Fr translation and WMT’19\nEn→De translation.\n6\nTable 3: Results on IWSLT {Es, Fr}↔En.\nNa/d/dh #Params(M) Es →En / ρ En→Es / ρ #Params(M) Es →En / ρ En→Es / ρ\n1/512/1024 36.8 40 .37 / 0.0 38 .56 / 0.0 36.8 36 .10 / 0.0 35 .99 / 0.0\n1/256/1024 13.7 40 .60 / 0.0 39 .42 / 0.0 13.7 36 .21 / 0.0 36 .16 / 0.0\n1/256/2048 20.0 40 .18 / 0.0 38 .78 / 0.0 20.0 36 .38 / 0.0 36 .73 / 0.0\n2/256/1024 18.4 41 .02 / 0.1 39 .56 / 0.1 18.4 36 .37 / 0.2 36 .57 / 0.2\n2/256/2048 24.7 42 .11 / 0.2 40 .46 / 0.2 24.7 37 .41 / 0.2 37 .24 / 0.2\n3/256/1024 23.1 41 .39 / 0.1 40 .08 / 0.2 23.2 37 .44 / 0.2 37 .30 / 0.1\n3/256/2048 29.4 41 .79 / 0.3 40 .40 / 0.3 29.5 37 .28 / 0.2 37 .44 / 0.2\nThe results of WMT’14 En→De are shown in Table 4. The standard baseline is 29.13 BLEU score,\nand the model contains 209.8M parameters. We implement an MAT with Na = 2,dh = 12288\nand another with Na = 3,dh = 10240. We can see that our MAT also works for the large-scale\ndataset. When Na = 2, we can obtain 29.90 BLEU score. When increasing Na to 3, we can get\n29.85, which is comparable with the the Na = 2 variant. Besides, for large-scale datasets, the drop\nbranch technique is important too. ρ= 0.2 works best of all settings.\nTable 4: Results on WMT’14 En→De translation.\nNa/d/dh #Param (M) ρ BLEU\n1/1024/4096 209 .8 0 .0 29 .08\n1/512/10240 161 .7 0 .0 28 .86\n1/512/12288 186 .9 0 .0 28 .95\n2/512/12288 205 .8 0 .1 29 .64\n2/512/12288 205 .8 0 .2 29 .90\n2/512/12288 205 .8 0 .3 29 .06\n3/512/10240 199 .5 0 .1 29 .62\n3/512/10240 199 .5 0 .2 29 .85\n3/512/10240 199 .5 0 .3 29 .05\nWe summarize previous results on WMT’14 En→De in Table 5. MAT can achieve comparable or\nslightly better results than carefully designed architectures like weighted Transformer and Dynamic-\nConv, and than the evolved Transformer discovered by neural architecture search.\nThe results of WMT’19 De→Fr are shown in Table 6. Compared with standard big transformer\nmodel of architecture 1/1024/4096, MAT with ρ= 0.1 can improve the baseline by 0.58 point.\nTable 5: BLEU scores of WMT’14 En→De\nin previous work.\nAlgorithm BLEU\nWeighted Transformer [1] 28.9\nEvolved Transformer [20] 29.8\nDynamicConv [30] 29.7\nOur MAT 29.9\nTable 6: Results on WMT’19 De→Fr translation.\nNa/d/dh #Param (M) ρ BLEU\n1/1024/4096 223 .6 0 .0 34 .07\n1/512/12288 193 .7 0 .0 33 .90\n2/512/12288 212 .6 0 .0 33 .92\n2/512/12288 212 .6 0 .1 34 .65\n2/512/12288 212 .6 0 .2 34 .53\n2/512/12288 212 .6 0 .3 34 .49\nResults with larger models: Finally, we explore whether our proposed MAT works for larger models.\nWe conduct experiments on WMT’19 En →De translation task. The data is downloaded from\nWMT’19 website5. We change the number of encoder layers to 12, set attention-dropout\nand activation-dropout as 0.1, and keep the other settings the same as WMT’14 En→De. To\nvalidate the effectiveness of MAT, we evaluate the trained models on three test sets: WMT’14\nEn→De, WMT’18 En→De and WMT’19 En→De. We use sacreBLEU to evaluate the translation\nquality. Furthermore, to facilitate comparison with previous work, we also use multi-bleu.perl\nto calculate the BLEU score for WMT’14 En→De.\n5The data is available at http://www.statmt.org/wmt19/translation-task.html. We concatenate\nEuroparl v9, Common Crawl corpus, News Commentary v14 and Document-split Rapid corpus.\n7\nThe results are shown in Table 7. Compared with the standard big transformer model of architecture\n1/1024/4096, in terms of sacreBLEU, MAT with ρ= 0.2 can improve the baseline by 1.0, 1.0 and\n1.1 points on WMT14, WMT18 and WMT19 test sets respectively. Specially, on WMT’14 En→De,\nin terms of multi-bleu, we achieve 30.8 BLEU score, setting a new record on this work under the\nsupervised setting.\nTable 7: Results of En →De with larger models. For WMT’14, both multi-bleu (left) and\nsacreBLEU (right) are reported.\nNa/d/dh #Param (M) ρ WMT14 WMT18 WMT19\n1/1024/4096 325 .7 0 .0 29 .8 / 29.2 42 .7 39 .3\n1/512/12288 288 .9 0 .0 29 .5 / 29.0 41 .3 37 .4\n2/512/12288 314 .1 0 .0 30 .0 / 29.6 42 .5 38 .5\n2/512/12288 314 .1 0 .1 29 .9 / 29.4 43 .1 39 .5\n2/512/12288 314 .1 0 .2 30 .8 / 30.2 43 .7 40 .4\n2/512/12288 314 .1 0 .3 30 .1 / 29.7 43 .8 40 .3\n5 Application to code generation\nWe verify our proposed method on code generation, which is to map natural language sentences into\ncode.\nDatasets Following [29], we conduct experiments on a Java dataset6 [7] and a Python dataset [27]. In\nthe Java dataset, the numbers of training, validation and test sequences are 69708, 8714 and 8714\nrespectively, and the corresponding numbers for Python are55538, 18505 and 18502. All samples are\ntokenized. We use the downloaded Java dataset without further processing and use Python standard\nAST moduleto further process the python code. The source and target vocabulary sizes in natural\nlanguage to Java code generation are 27kand 50k, and those for natural language to Python code\ngeneration are 18kand 50k. In this case, following [29], we do not apply subword tokenization like\nBPE to the sequences.\nModel conﬁguration We set Na = 4, d= 256 and dh = 1024 respectively. Both the encoder and\nthe decoder consist of three blocks. The MAT model contains about 37M parameters. For the\nTransformer baseline, we set d= 512 and dh = 1024, with 55M parameters.\nEvaluation Following [29, 7], we use sentence-level BLEU scores, which is the average BLEU score\nof all sequences to evaluate the generation quality. We choose the percentage of valid code (PoV) as\nanother metric for evaluation, which is the percentage of code that can be parsed into an AST.\nWe report the results on code generation task in Table 8. It is obvious that Transformer-based models\n(standard Transformer and MAT) is signiﬁcantly better than the LSTM-based dual model. Compared\nwith standard Transformer, our MAT can get 4.2 and 1.2 BLEU score improvement in Java and\nPython datasets respectively. MAT reaches the best BLEU scores whenρ= 0.1 on both on Java and\nPython datasets, which shows that the drop branch technique is important in code generation task.\nWhen ρ = 0.1, in terms of PoV , our MAT can boost the Transformer baseline by5.8% and 6.6%\npoints.\nTable 8: Results on code generation.\nAlgorithm Java Python\nBLEU PoV BLEU PoV\nDual [29] 17.17 27 .4% 12 .09 51 .9%\nTransformer 23.30 83 .1% 15 .49 73 .3%\nOurs, ρ= 0.0 26 .97 77 .5% 16 .35 65 .5%\nOurs, ρ= 0.1 27.53 88.9% 16.66 79.9%\nOurs, ρ= 0.2 26 .06 90 .8% 15 .63 84 .0%\nOurs, ρ= 0.3 24 .72 90 .2% 12 .85 75 .3%\n6The urls of the datasets and tools we used for code generation are summarized in Appendix C.\n8\n6 Application to natural language understanding\nWe verify our proposed MAT on natural language understanding (NLU) tasks. Pre-training techniques\nhave achieved state-of-the-art results on benchmark datasets/tasks like GLUE [28], RACE [11], etc.\nTo use pre-training, a masked language model is ﬁrst pre-trained on large amount of unlabeled data.\nThen the obtained model is used to initialize weights for downstream tasks. We choose RoBERTa [14]\nas the backbone. For GLUE tasks, we focus more on accuracy. Therefore, we do not control the\nparameters subconsciously. We set Na = 2 for all experiments.\nFollowing [5], we conduct experiments on MNLI-m, MPRC, QNLI abd SST-2 tasks in GLUE\nbenchmark [28]. SST-2 is a single-sentence task, which is to check whether the input movie review is\na positive one or negative. The remaining tasks are two-sentence tasks. In MNLI, given a premise\nsentence and a hypothesis sentence, the task is to predict whether the premise entails the hypothesis\n(entailment), contradicts the hypothesis (contradiction), or neither (neutral). In MRPC, the task is to\ncheck whether the two inputs are matched. In QNLI, the task is to determine whether the context\nsentence contains the answer to the question. We choose the 24-layer RoBERTalarge7 to initialize our\nMAT. We report the validation accuracy following [5].\nTable 9: Validation results on various NLU tasks. Results of RoBERTa and RoBERTa + LayerDrop\n(brieﬂy, “+LayerDrop”) are from [5].\nAlgorithm MNLI-m MRPC QNLI SST-2\nRoBERTa 90.2 90 .9 94 .7 96 .4\n+ LayerDrop 90.1 91 .0 94 .7 96 .8\nOurs 90.7 91 .9 95 .0 97 .0\nThe results are reported in Table 9. Compared with vanilla RoBERTa model, on the four tasks\nMNLI-m, MRPC, QNLI abd SST-2, we can improve them by 0.5, 1.0, 0.3 and 0.6 scores. Compared\nwith the layer drop technique [5], we also achieve promising improvement on these four tasks. The\nresults demonstrate that our method not only works for sequence generation tasks, but also for text\nclassiﬁcation tasks.\nDrop branch also plays a central role on NLU tasks. We take the MRPC task as an example. The\nresults are shown in Table 10. MRPC achieves the best result at ρ= 0.3.\nTable 10: MRPC validation accuracy w.r.t. ρ.\nρ 0.1 0 .2 0 .3 0 .4 0 .5\naccuracy 90.9 90 .0 91 .9 91 .2 91 .7\n7 Exploring other variants\nIn this section, we ﬁrst discuss whether applying multi-branch architectures to FFN layers is helpful.\nThen we discuss a new variant of drop branch technique. Finally we verify the importance of the\nmulti-branch architecture, where both concatenation and averaging operations are used. We mainly\nconduct experiments on IWSLT tasks to verify the variants. We conduct experiments on IWSLT’14\nDe→En.\n7.1 Multi-branch FFN layer\nAlgorithm: Denote the revised FFN layer as hFFNNf with Nf (∈Z+) branches. Mathematically,\ngiven an input xfrom the previous layer,\nhFFNNf (x) = x+ 1\nNf\nNf∑\ni=1\nFFN(x; ωi)I{Ui ≥ρ}\n1 −ρ , (8)\n7Models from page https://github.com/pytorch/fairseq/tree/master/examples/roberta.\n9\nwhere ωi is the parameter of the i-th branch of the FFN layer, xis the output from the previous layer.\nExperiments: We ﬁx das 256 and change Nf . We set the attention layers as both standard multi-head\nattention layers (Na = 1) and multi-branch attention layers with Na = 2. We set Nf dh = 2048 to\nensure the number of parameters unchanged with different number of branches. Results are reported\nin Table 11.\nTable 11: Results on IWSLT’14 De →En with multi-branch FFN layers. From left to right, the\ncolumns represent the network architecture (with the number of parameters included), BLEU scores\nwith ρranging from 0.0 to 0.3.\nNa/Nf /dh/#Param 0.0 0 .1 0 .2 0 .3\n1/1/2048/20.0M 34.66 35 .45 32 .75 1 .37\n1/2/1024/20.0M 34.85 35 .38 35 .30 34 .39\n1/4/512/20.0M 34.51 34 .83 34 .71 34 .56\n1/8/256/20.0M 34.35 34 .36 34 .38 33 .86\n2/1/2048/24.7M 34.51 35 .46 35 .59 35 .33\n2/2/1024/24.7M 34.39 35 .18 35 .55 35 .42\n2/4/512/24.7M 33.86 34 .90 35 .00 35 .07\n2/8/256/24.7M 34.05 34 .59 34 .70 34 .55\nWe can see that increasing the branches of FFN layer while decreasing the corresponding hidden\ndimension (i.e., dh) will hurt the performance. For standard Transformer withNa = 1, as we increase\nNf from 1 to 2, 4, 8, the BLEU scores drop from 35.45 to 35.38, 34.83 and 34.38. For MAT with\nNa = 2, as Nf grows from 1 to 8, the BLEU decreases from 35.59 to 35.55, 35.00 and 34.70. That\nis, constraint by the number of total parameters, we do not need to separate FFN layers into multiple\nbranches.\n7.2 Variants of drop branch\nAlgorithm: In drop branch, the attention heads within a branch are either dropped together or kept\ntogether. However, we are not sure whether dropping a complete branch is the best choice. Therefore,\nwe design a more general way to leverage dropout. Mathematically, the j-th attention head of the i-th\nbranch, denoted as βi,j(Q,K,V ; ρ,θi,j), works as follows:\nβi,j(Q,K,V ; ρ,θi,j) = attn(QWi,j\nQ ,KW i,j\nK ,VW i,j\nV )I{Ui,j ≥ρ}\n1 −ρ , (9)\nwhere superscripts i and j represent the branch id and head id in a multi-head attention layer\nrespectively, Ui,j ∼unif[0,1], θi,j = {Wi,j\nQ ,Wi,j\nK ,Wi,j\nV }. Based on Eqn.(9), we can deﬁne a more\ngeneral multi-branch attentive architecture:\n1\nNa\nNa∑\ni=1\nconcat\n(\nβi,1(Q,K,V ; ρ,θi,1),βi,2(Q,K,V ; ρ,θi,2),··· ,βi,M (Q,K,V ; ρ,θi,M )\n)\n.\n(10)\nKeep in mind that Eqn. (10) is associated with Ui,j for any i ∈[Na],j ∈[M]. We apply several\nconstraints to Ui,j, which can lead to different ways to regularize the training:\n1. For any i∈[Na], sample Ui ∼unif[0,1] and set Ui,j = Ui for any j ∈[M]. This is the\ndrop branch as we introduced in Section 3.1.\n2. Each Ui,j is independently sampled.\nExperiments: We conduct experiments on IWSLT’14 De→En and the results are reported in Table 12.\nProximal initialization is leveraged. From left to right, each column represents architecture, number of\nparameters, BLEU scores with ρranging from 0.0 to 0.3. The last column marked with ∆ represents\nthe improvement/decrease of the best results compared to those in Table 2.\nWe can see that randomly dropping heads leads to slightly worse results than the drop branch\ntechnique. Take the network architecture with Na = 3, dh = 1024 as an example. The best BLEU\n10\nTable 12: Results on IWSLT’14 De→En with\nrandomly dropping heads technique. Embedding\ndimension dis ﬁxed as 256.\n(Na,dh) 0 .0 0 .1 0 .2 0 .3 ∆\n(2,2048) 34 .99 35 .53 35 .95 35.96 −0.26\n(3,1024) 35 .33 35 .67 35 .90 35.95 +0.01\n(3,2048) 34 .83 35 .45 35 .76 36.07 −0.15\n(4,1024) 35 .45 35 .72 35.83 35.77 −0.24\n(4,2048) 35 .06 35 .79 35 .72 36.03 −0.06\nTable 13: Results on IWSLT’14 De→En\nwith different numbers of heads.\nρ 0.1 0 .2 0 .3\nM = 1 35 .01 34 .83 34 .52\nM = 2 35 .11 35 .45 35 .15\nM = 4 35 .19 35 .52 35 .11\nscore that drop branch technique achieves is 36.22, and that obtained with randomly dropping heads\nis 36.07. Similarly observations can be found from other settings in Table 12. Therefore, we suggest\nto use the drop branch technique, which requires minimal revision to the benchmark code.\n7.3 Ablation study on multi-branch attentions\nMAT introduces multi-branch attention, in addition to the multi-head attention with the concatenation\nmanner.8 It is interesting to know whether the multi-head attention is still needed given the multi-\nbranch attention. We conduct experiments with Na = 2, d= 256, dh = 1024 and vary the number\nof heads M in mAttnNa,M . Results are reported in Table 13.\nWe can see that the multi-head attention is still helpful. When M = 1 , i.e., removing multiple\nheads and using a single head, it performs worse than multiple heads (e.g., M = 4). Therefore, the\nmulti-branch of them is the best choice.\n8 Conclusion and future work\nIn this work, we proposed a simple yet effective variant of Transformer called multi-branch attentive\nTransformer and leveraged two techniques, drop branch and proximal initialization, to train the model.\nRich experiments on neural machine translation, code generation and natural language understanding\ntasks have demonstrated the effectiveness of our model.\nFor future work, we will apply our model to more sequence learning tasks. We will also combine\nour discoveries with neural architecture search, i.e., searching for better neural models for sequence\nlearning in the search space with enriched multi-branch structures.\nAppendix\nA Exploring the Drop Branch in MAT\nWe explore more values of drop branch rate ρ, and the results are reported in Table 14.\nFrom Table 14, we can see that increasing ρto 0.4 and 0.5 will hurt the performance of MAT.\nOn IWSLT’14 De→En, with proximal initialization, we also explore increasing ρto 0.4 and 0.5. The\nresults are in Table 15. Still, setting ρlarger than 0.4 will hurt the performance.\nB Exploration on Other IWSLT Tasks\nB.1 Results of MAT\nWe apply the settings in Table1 of the main paper to all four other IWSLT tasks. The results without\nproximal initialization are reported from Table 16 to Table 19. The results with promixal initialization\nare shown from Table 20 to Table 23.\n8The multi-head attention in standard Transformer is also one kind of multi-branch attention. For simplicity,\nwe use multi-head attention to denote the original attention layer in standard Transformer, and multi-branch\nattention to denote the newly introduced multi-branch attention in the multi-branch manner (both averaging and\nconcatenation manner).\n11\nTable 14: Results on IWSLT’14 De→En with different architectures. From left to right, the columns\nrepresent the network architecture, number of parameters, BLEU scores with ρranging from 0.2 to\n0.5. Results of ρ= 0,0.1 are in Table 1 of the main text.\nNa/d/dh/Param 0.2 0 .3 0 .4 0 .5\nStandard Transformer + Drop Branch\n1/512/1024/36.7M 22.43 0 .78 1 .64 0 .00\n1/256/1024/13.7M 34.53 29 .34 14 .08 12 .03\n1/256/2048/20.0M 32.75 1 .37 1 .48 9 .05\n1/256/3072/26.3M 34.64 25 .05 1 .31 7 .84\nMAT + Drop Branch\n2/256/1024/18.4M 35.52 35 .11 34 .46 33 .50\n2/256/2048/24.7M 35.59 35 .33 34 .95 28 .15\n3/256/1024/23.1M 35.39 35 .44 34 .85 34 .10\n3/256/2048/29.4M 35.40 35.70 35.01 31 .68\n4/256/1024/27.9M 35.08 35 .14 35 .01 34 .38\n4/256/2048/34.2M 35.08 35 .46 34 .21 0 .89\nTable 15: Results of using proximal initialization. Columns from left to right represent the network\narchitecture, BLEU scores with drop branch ratio ρfrom 0.2 to 0.5, and the increment compared to\nthe best results without proximal initialization.\n(Na,dh) 0 .2 0 .3 0 .4 0 .5 ∆\n(2,2048) 35 .85 36 .12 35 .62 35 .03 0 .79\n(3,1024) 35 .94 35 .83 35 .76 35 .15 0 .50\n(3,2048) 36 .08 36.22 35.87 35 .58 0 .52\n(4,1024) 36 .07 36 .02 35 .78 35 .42 0 .88\n(4,2048) 35 .81 36 .09 35 .80 35 .57 0 .63\nTable 16: Results on IWSLT Es→En with different architectures. From left to right, the columns\nrepresent the network architecture, number of parameters, BLEU scores with ρranging from 0.0 to\n0.3.\nNa/d/dh/Param 0.0 0 .1 0 .2 0 .3\nStandard Transformer + Drop Branch\n1/512/1024/36.7M 39.77△ 2.19 0 .00 0 .00\n1/256/1024/13.7M 40.60 40 .94 39 .46 26 .04\n1/256/2048/20.0M 40.18 41 .30 39 .16 0 .54\nMAT + Drop Branch\n2/256/1024/18.4M 40.30 41 .02 40 .89 40 .45\n2/256/2048/24.7M 40.28 41 .20 41.96 41.06\n3/256/1024/23.1M 40.42 41 .04 40 .99 40 .59\n3/256/2048/29.4M 39.74 40 .72 41 .63 40 .80\n4/256/1024/27.9M 40.17 40 .99 40 .81 40 .67\n4/256/2048/34.2M 39.94 40 .69 41 .29 40 .01\n12\nTable 17: Results on IWSLT En→Es with different architectures. From left to right, the columns\nrepresent the network architecture, number of parameters, BLEU scores with ρranging from 0.0 to\n0.3.\nNa/d/dh/Param 0.0 0 .1 0 .2 0 .3\nStandard Transformer + Drop Branch\n1/512/1024/36.7M 38.56△ 1.67 0 .01 0 .00\n1/256/1024/13.7M 39.51 39 .58 38 .42 23 .50\n1/256/2048/20.0M 38.78 39 .84 38 .22 0 .90\nMAT + Drop Branch\n2/256/1024/18.4M 38.81 39 .56 38 .98 39 .08\n2/256/2048/24.7M 38.77 39 .33 39 .86 39 .37\n3/256/1024/23.1M 38.31 39 .12 39 .77 39 .29\n3/256/2048/29.4M 38.22 39 .89 39 .57 39.90\n4/256/1024/27.9M 38.37 39 .16 39 .55 39 .49\n4/256/2048/34.2M 38.40 39 .05 39 .24 39 .35\nTable 18: Results on IWSLT Fr→En with different architectures. From left to right, the columns\nrepresent the network architecture, number of parameters, BLEU scores with ρranging from 0.0 to\n0.3.\nNa/d/dh/Param 0.0 0 .1 0 .2 0 .3\nStandard Transformer + Drop Branch\n1/512/1024/36.7M 36.60△ 2.09 0 .01 0 .01\n1/256/1024/13.7M 36.21 36 .17 35 .15 3 .99\n1/256/2048/20.0M 36.38 36 .42 35 .11 14 .66\nMAT + Drop Branch\n2/256/1024/18.4M 35.63 36 .28 36 .37 35 .61\n2/256/2048/24.7M 35.90 35 .83 36 .34 36 .08\n3/256/1024/23.1M 35.72 35 .71 36 .11 35 .59\n3/256/2048/29.4M 35.55 36 .30 36 .22 36 .27\n4/256/1024/27.9M 35.52 36 .30 36 .06 36 .12\n4/256/2048/34.2M 34.98 36.67 36.24 35 .97\nTable 19: Results on IWSLT En→Fr with different architectures. From left to right, the columns\nrepresent the network architecture, number of parameters, BLEU scores with ρranging from 0.0 to\n0.3.\nNa/d/dh/Param 0.0 0 .1 0 .2 0 .3\nStandard Transformer + Drop Branch\n1/512/1024/36.7M 36.32△ 36.07 0 .02 0 .05\n1/256/1024/13.7M 36.16 36 .43 35 .61 0 .79\n1/256/2048/20.0M 36.73 36 .80 35 .34 1 .59\nMAT + Drop Branch\n2/256/1024/18.4M 36.41 36 .48 36 .57 36 .34\n2/256/2048/24.7M 35.95 37.20 36.90 36 .44\n3/256/1024/23.1M 35.79 36 .52 36 .56 36 .34\n3/256/2048/29.4M 35.88 36 .88 36 .88 36 .86\n4/256/1024/27.9M 35.77 36 .68 37 .09 36 .50\n4/256/2048/34.2M 35.23 36 .65 36 .97 36 .66\n13\nTable 20: Results of using proximal initialization on IWSLT Es →En. Columns from left to right\nrepresent the network architecture, BLEU scores with drop branch ratio ρfrom 0.0 to 0.3, and the\nincrement compared to the best results without proximal initialization.\n(Na,dh) 0 .0 0 .1 0 .2 0 .3 ∆\n(2,2048) 40 .81 41 .14 42.11 41.68 0 .15\n(3,1024) 40 .95 41 .39 41 .22 41 .36 0 .35\n(3,2048) 40 .96 41 .10 41 .60 41 .79 0 .16\n(4,1024) 41 .45 41 .67 41 .67 41 .26 0 .68\n(4,2048) 40 .85 41 .44 41 .26 42 .06 0 .77\nTable 21: Results of using proximal initialization on IWSLT En →Es. Columns from left to right\nrepresent the network architecture, BLEU scores with drop branch ratio ρfrom 0.0 to 0.3, and the\nincrement compared to the best results without proximal initialization.\n(Na,dh) 0 .0 0 .1 0 .2 0 .3 ∆\n(2,2048) 39 .47 39 .78 40 .46 40 .34 0 .60\n(3,1024) 39 .47 40 .06 40 .08 39 .84 0 .31\n(3,2048) 39 .34 39 .61 40 .08 40.40 0.50\n(4,1024) 39 .36 39 .99 39 .81 40 .00 0 .45\n(4,2048) 39 .75 40 .16 40 .12 39 .91 0 .81\nTable 22: Results of using proximal initialization on IWSLT Fr →En. Columns from left to right\nrepresent the network architecture, BLEU scores with drop branch ratio ρfrom 0.0 to 0.3, and the\nincrement compared to the best results without proximal initialization.\n(Na,dh) 0 .0 0 .1 0 .2 0 .3 ∆\n(2,2048) 36 .14 36 .76 37 .41 36 .82 0 .77\n(3,1024) 36 .73 37 .03 37.44 37.10 1 .33\n(3,2048) 36 .32 36 .92 37 .28 37 .25 0 .98\n(4,1024) 36 .81 37 .14 37 .10 36 .88 0 .73\n(4,2048) 36 .48 36 .90 37 .40 37 .29 0 .73\nTable 23: Results of using proximal initialization on IWSLT En →Fr. Columns from left to right\nrepresent the network architecture, BLEU scores with drop branch ratio ρfrom 0.0 to 0.3, and the\nincrement compared to the best results without proximal initialization.\n(Na,dh) 0 .0 0 .1 0 .2 0 .3 ∆\n(2,2048) 36 .73 37 .14 37 .24 37 .09 0 .04\n(3,1024) 36 .71 37 .30 37 .04 36 .92 0 .74\n(3,2048) 36 .51 37 .39 37.44 37.43 0 .56\n(4,1024) 36 .67 36 .77 37 .24 37 .15 0 .15\n(4,2048) 36 .70 37 .19 37 .18 37 .32 0 .35\n14\nB.2 Variants of Drop Branch\nWe explore the variant of drop branch described in Section 7.2 on the other four IWSLT tasks. We\napply all settings in Table 11 of the main paper to the remaining language pairs. Results are reported\nfrom Table 24 to Table 27. Generally, the two types of drop branch achieve similar results.\nTable 24: Results on IWSLT Es→En with randomly dropping heads technique. Embedding dimension\ndis ﬁxed as 256.\n(Na,dh) 0 .0 0 .1 0 .2 0 .3 ∆\n(2,2048) 40 .81 41 .61 41 .54 41.68 −0.43\n(3,1024) 40 .95 41.63 41.25 41 .15 +0 .24\n(3,2048) 40 .96 41 .30 41 .39 41.86 +0.07\n(4,1024) 41.45 41.41 41 .04 41 .29 −0.22\n(4,2048) 40 .85 41 .30 41 .48 41.70 −0.36\nTable 25: Results on IWSLT En→Es with randomly dropping heads technique. Embedding dimension\ndis ﬁxed as 256.\n(Na,dh) 0 .0 0 .1 0 .2 0 .3 ∆\n(2,2048) 39 .47 39 .85 39 .98 40.39 −0.07\n(3,1024) 39 .47 39.99 39.96 39 .41 −0.09\n(3,2048) 39 .34 39 .47 40.22 39.67 −0.18\n(4,1024) 39 .36 39 .71 39 .59 39.89 −0.11\n(4,2048) 39 .75 39 .73 39 .94 40.36 +0.20\nTable 26: Results on IWSLT Fr→En with randomly dropping heads technique. Embedding dimension\ndis ﬁxed as 256.\n(Na,dh) 0 .0 0 .1 0 .2 0 .3 ∆\n(2,2048) 36 .14 37.10 36.85 36 .71 −0.31\n(3,1024) 36 .73 36 .78 36.83 36.20 −0.61\n(3,2048) 36 .32 36 .73 36 .92 37.08 −0.20\n(4,1024) 36 .81 36 .29 36.82 36.49 −0.32\n(4,2048) 36 .48 36 .39 36 .89 36.99 −0.41\nTable 27: Results on IWSLT En→Fr with randomly dropping heads technique. Embedding dimension\ndis ﬁxed as 256.\n(Na,dh) 0 .0 0 .1 0 .2 0 .3 ∆\n(2,2048) 36 .73 37 .18 37.48 37.15 +0 .24\n(3,1024) 36 .71 37 .14 37.48 36.86 +0 .18\n(3,2048) 36 .51 36 .93 37.17 36.97 −0.27\n(4,1024) 36 .67 36 .59 37.02 36.99 −0.22\n(4,2048) 36 .70 36 .81 37.80 37.37 +0 .48\nC Scripts\nIn this section, we summarize the scripts we used in our paper.\n(1) multi-bleu.perl: https://github.com/moses-smt/mosesdecoder/blob/master/\nscripts/generic/multi-bleu.perl\n(2) sacreBLEU: https://github.com/mjpost/sacreBLEU\n15\n(3) The script to preprocess the IWSLT data: https://github.com/pytorch/fairseq/blob/\nmaster/examples/translation/prepare-iwslt14.sh\n(4) Path to Java dataset: https://github.com/xing-hu/TL-CodeSum\n(5) Path to Python dataset: https://github.com/wanyao1992/code_summarization_public\n(6) Python standard AST module to process the Python code: https://docs.python.org/3/\nlibrary/ast.html\nReferences\n[1] Karim Ahmed, Nitish Shirish Keskar, and Richard Socher. Weighted transformer network for\nmachine translation. arXiv preprint arXiv:1711.02132, 2017.\n[2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. ICLR, 2015.\n[4] Sergey Edunov, Myle Ott, Michael Auli, David Grangier, and Marc’Aurelio Ranzato. Classical\nstructured prediction losses for sequence to sequence learning. NAACL, 2018.\n[5] Angela Fan, Edouard Grave, and Armand Joulin. Reducing transformer depth on demand with\nstructured dropout. In International Conference on Learning Representations, 2020.\n[6] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 770–778, 2016.\n[7] Xing Hu, Ge Li, Xin Xia, David Lo, Shuai Lu, and Zhi Jin. Summarizing source code with\ntransferred api knowledge. In Proceedings of the Twenty-Seventh International Joint Conference\non Artiﬁcial Intelligence, IJCAI-18 , pages 2269–2275. International Joint Conferences on\nArtiﬁcial Intelligence Organization, 7 2018.\n[8] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected\nconvolutional networks. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 4700–4708, 2017.\n[9] Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Tuo Zhao.\nSmart: Robust and efﬁcient ﬁne-tuning for pre-trained natural language models through princi-\npled regularized optimization, 2019.\n[10] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. ICLR, 2015.\n[11] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale\nreading comprehension dataset from examinations. arXiv preprint arXiv:1704.04683, 2017.\n[12] Gustav Larsson, Michael Maire, and Gregory Shakhnarovich. Fractalnet: Ultra-deep neural\nnetworks without residuals. ICLR, 2017.\n[13] Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. In\n7th International Conference on Learning Representations, 2019.\n[14] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach. ICLR, 2019.\n[15] Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. Scaling neural machine translation.\narXiv preprint arXiv:1806.00187, 2018.\n[16] Neal Parikh and Stephen Boyd. Proximal Algorithm. Now Publishers, Inc., 2013.\n[17] Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee,\nand Luke Zettlemoyer. Deep contextualized word representations. NAACL, 2018.\n[18] Hieu Pham, Melody Y Guan, Barret Zoph, Quoc V Le, and Jeff Dean. Efﬁcient neural\narchitecture search via parameter sharing. arXiv preprint arXiv:1802.03268, 2018.\n[19] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\nwith subword units. ACL, 2016.\n16\n[20] David R So, Chen Liang, and Quoc V Le. The evolved transformer. ICML, 2019.\n[21] Kaitao Song, Xu Tan, Di He, Jianfeng Lu, Tao Qin, and Tie-Yan Liu. Double path networks for\nsequence to sequence learning. COLING, 2018.\n[22] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\nDropout: a simple way to prevent neural networks from overﬁtting. The journal of machine\nlearning research, 15(1):1929–1958, 2014.\n[23] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A Alemi. Inception-v4,\ninception-resnet and the impact of residual connections on learning. In Thirty-First AAAI\nConference on Artiﬁcial Intelligence, 2017.\n[24] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov,\nDumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1–9,\n2015.\n[25] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Re-\nthinking the inception architecture for computer vision. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages 2818–2826, 2016.\n[26] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. InAdvances in neural information\nprocessing systems, pages 5998–6008, 2017.\n[27] Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, and Philip S Yu. Im-\nproving automatic source code summarization via deep reinforcement learning. In Proceedings\nof the 33rd ACM/IEEE International Conference on Automated Software Engineering, pages\n397–407, 2018.\n[28] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.\nGLUE: A multi-task benchmark and analysis platform for natural language understanding. In\nInternational Conference on Learning Representations, 2019.\n[29] Bolin Wei, Ge Li, Xin Xia, Zhiyi Fu, and Zhi Jin. Code generation as a dual task of code\nsummarization. In Advances in Neural Information Processing Systems , pages 6559–6569,\n2019.\n[30] Felix Wu, Angela Fan, Alexei Baevski, Yann Dauphin, and Michael Auli. Pay less atten-\ntion with lightweight and dynamic convolutions. In International Conference on Learning\nRepresentations, 2019.\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\narXiv:1609.08144, 2016.\n[32] Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. Aggregated residual\ntransformations for deep neural networks. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 1492–1500, 2017.\n[33] Guangxiang Zhao, Xu Sun, Jingjing Xu, Zhiyuan Zhang, and Liangchen Luo. Muse: Parallel\nmulti-scale attention for sequence to sequence learning. arXiv preprint arXiv:1911.09483, 2019.\n[34] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\nfast-forward connections for neural machine translation. Transactions of the Association for\nComputational Linguistics, 4:371–383, 2016.\n17",
  "topic": "Initialization",
  "concepts": [
    {
      "name": "Initialization",
      "score": 0.7561047673225403
    },
    {
      "name": "Transformer",
      "score": 0.7218184471130371
    },
    {
      "name": "Computer science",
      "score": 0.7039238214492798
    },
    {
      "name": "Machine translation",
      "score": 0.5125922560691833
    },
    {
      "name": "Architecture",
      "score": 0.4985063076019287
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.49689820408821106
    },
    {
      "name": "Artificial intelligence",
      "score": 0.47283419966697693
    },
    {
      "name": "Natural language processing",
      "score": 0.37534284591674805
    },
    {
      "name": "Engineering",
      "score": 0.16875889897346497
    },
    {
      "name": "Programming language",
      "score": 0.16309767961502075
    },
    {
      "name": "Electrical engineering",
      "score": 0.14827033877372742
    },
    {
      "name": "Voltage",
      "score": 0.07304584980010986
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    }
  ],
  "institutions": []
}