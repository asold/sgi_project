{
  "title": "Temporal Embeddings and Transformer Models for Narrative Text Understanding",
  "url": "https://openalex.org/W3012200188",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1906841621",
      "name": "Vani K",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Mellace, Simone",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2528906293",
      "name": "Antonucci, Alessandro",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2566509507",
    "https://openalex.org/W3101890897",
    "https://openalex.org/W2964914104",
    "https://openalex.org/W2891177506",
    "https://openalex.org/W2950133940",
    "https://openalex.org/W2965733891",
    "https://openalex.org/W2940672570",
    "https://openalex.org/W2970930899",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W3030458514",
    "https://openalex.org/W1971022913",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2600659824",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2310102669",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W2941760716",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2964231305",
    "https://openalex.org/W2251460488",
    "https://openalex.org/W3121652883",
    "https://openalex.org/W1570098300",
    "https://openalex.org/W2507252477",
    "https://openalex.org/W3124497915",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2807650837",
    "https://openalex.org/W2810365381"
  ],
  "abstract": "We present two deep learning approaches to narrative text understanding for character relationship modelling. The temporal evolution of these relations is described by dynamic word embeddings, that are designed to learn semantic changes over time. An empirical analysis of the corresponding character trajectories shows that such approaches are effective in depicting dynamic evolution. A supervised learning approach based on the state-of-the-art transformer model BERT is used instead to detect static relations between characters. The empirical validation shows that such events (e.g., two characters belonging to the same family) might be spotted with good accuracy, even when using automatically annotated data. This provides a deeper understanding of narrative plots based on the identification of key facts. Standard clustering techniques are finally used for character de-aliasing, a necessary pre-processing step for both approaches. Overall, deep learning models appear to be suitable for narrative text understanding, while also providing a challenging and unexploited benchmark for general natural language understanding.",
  "full_text": "Temporal Embeddings and Transformer Models\nfor Narrative Text Understanding\nVani K, Simone Mellace, and Alessandro Antonucci\nIstituto Dalle Molle di Studi sull’Intelligenza Artiﬁciale (IDSIA)\nLugano (Switzerland)\n{vanik,simone,alessandro}@idsia.ch\nAbstract. We present two deep learning approaches to narrative text\nunderstanding for character relationship modelling. The temporal evolu-\ntion of these relations is described by dynamic word embeddings, that are\ndesigned to learn semantic changes over time. An empirical analysis of\nthe corresponding character trajectories shows that such approaches are\neﬀective in depicting dynamic evolution. A supervised learning approach\nbased on the state-of-the-art transformer model BERT is used instead\nto detect static relations between characters. The empirical validation\nshows that such events (e.g., two characters belonging to the same fam-\nily) might be spotted with good accuracy, even when using automatically\nannotated data. This provides a deeper understanding of narrative plots\nbased on the identiﬁcation of key facts. Standard clustering techniques\nare ﬁnally used for character de-aliasing, a necessary pre-processing step\nfor both approaches. Overall, deep learning models appear to be suitable\nfor narrative text understanding, while also providing a challenging and\nunexploited benchmark for general natural language understanding.\nKeywords: Narrative understanding·Dynamic word embeddings·Bidi-\nrectional encoder representations from transformers.\n1 Introduction\nDue to the inherent complexity involved in textual data, narrative text un-\nderstanding remains a challenging and relatively unexplored research area for\nAI. Here we consider narrative text, such as novels and short stories (broadly\ntermed here as literary text) and try to address its lexical diversity and rich-\nness in terms of relations between entities [25]. In recent years, Deep Learning\n(DL) approaches were found to positively impact Natural Language Processing\n(NLP) with impressive boosts in text extraction and understanding capabilities.\nThis marginally concerns the area of literary text [17], where the application\nof DL models remains relatively unexplored. Some researchers modelled charac-\nter networks using machine learning, mostly from a social network perspective\nbased on generative models for conversational dialogues [1,5] not involving DL\nstate-of-the-art approaches. Just a few works have been reported for character\nevolution and relational analysis [7,13,29].\narXiv:2003.08811v1  [cs.CL]  19 Mar 2020\n2 Vani K, Simone Mellace, and Alessandro Antonucci\nHere we evaluate the application of DL to literary text understanding. The\ngoal is to describe character relationships within a novel and their evolution.\nMoreover, we also want to emphasize the potential of literary text as a challeng-\ning benchmark for state-of-the-art language models, whose major applications\nare typically in other domains such as biomedical literature [18] or fake news de-\ntection [26], where both the lexical richness and the intricacy of the inter-entities\nrelations might be less intricate compared to literary domain.\nTo analyse the character relationships, both supervised and unsupervised DL\ntechniques are considered here. A classiﬁcation model to identify the relations\nbetween characters using BERT ( Bidirectional Encoder Representations from\nTransformers, [8]) is trained from supervised data. BERT is successfully used\nin various classiﬁcation tasks, but, to the best of our knowledge, not yet in the\nliterary text domain. Moreover, manual annotation of training data in this ﬁeld\ncan be very expensive, this representing a strong limitation for this direction.\nTo partially bypass this issue, here we also present a simple approach to auto-\nmatically generate training data for character relation classiﬁcation (focusing on\nfamily relations, such as parent of, sibling of).\nAt the unsupervised level, we consider the dynamic evolution of the char-\nacters over time (i.e., across the text). To do that, we learn vectors associated\nto diﬀerent characters based on so-called dynamic or temporal embeddings[2],\nallowing to learn vectors over diﬀerent slices inside the text (e.g., chapters or\nﬁxed amounts of text), while maintaining the vectors comparable over time be-\ncause of a common initialization. We analyse the relations between characters\nby visualizing the character trajectories over time by low-dimensionality projec-\ntions or the relative distances in the original, high-dimensionality, spaces or by\nconsidering the relative distances between the vectors.\nBoth techniques require a pre-processing step consisting in character detec-\ntion, based on standard entity recognition techniques, and character de-aliasing,\nfor which density-based clustering methods are adopted.\nThe paper is organized as follows. A review of existing work is in Section 2.\nSections 3 and 4 report a discussion of, respectively, the supervised and unsu-\npervised approaches. An empirical validation is in Section 5. Conclusions and\noutlooks are ﬁnally reported in Section 6.\n2 Literature Review\nThe onset of DL has given drive to powerful data processing models, which fa-\ncilitate NLP applications. In this context, systems that understand the semantic\nand syntactic aspects of a text are extremely important. Word embedding mod-\nels such as Word2Vec [21], or Glove [23], as well as sentence embedding models\nsuch as USE ( Universal Sentence Encoder, [6]) help in representing text as a\nmathematical object in a reliable way. The text representations using such em-\nbeddings along with NLP and deep neural networks [12,22] played a vital role\nin text extraction, classiﬁcation and clustering.\nTemporal Embeddings and Transformers for Narrative Text Understanding 3\nAnother major shift was the introduction of attention models and transform-\ners [28], these are language models able to better understand text semantics by\ncontextual analysis. BERT, ELMO [24] and various versions of these models\ngave a big boost to recent NLP applications [11]. Moreover, word embeddings,\noriginally intended as static model of a given corpus, later led to the exploration\nof their dynamic evolution over time, this being mainly used to compare the\nsemantic shifts of words over time and detection of word analogies [15,16]. No-\ntably, some of these works used BERT for story ending predictions and temporal\nevent extractions [10,19]. In the next sections, we show how these models can be\napplied to literary text understanding.\n3 Character Trajectories by Temporal Word Embeddings\nBoth the unsupervised technique presented in this section and the supervised\napproach discussed in Section 4 require a reliable identiﬁcation of the characters\ninvolved in the plot. This corresponds to a named entity recognition task, for\nwhich standard tools can be used.1 As same characters can occur in the text with\ndiﬀerent aliases (e.g., Ron and Ronald Weasley), a de-aliasing might be needed\nas an additional pre-processing step. We achieve that by a clustering of the\nnamed entities based on the DBSCAN algorithm [3]. The entities are clustered\nusing precomputed distances based on the sequence matcher algorithm, which\nﬁnds the longest common subsequences.\nAfter character identiﬁcation and de-aliasing, learning the embeddings of\nthe characters of a literary text is a straightforward task. As the learning of\nan embedding is based on contextual information, the only important condition\nis that a suﬃcient amount of co-occurrences of the characters in the text is\navailable. If this is the case, the relative distances between the vectors can be\nused as proxy indicators of the relations between the corresponding characters.\nThis can be also achieved for separate parts of a same text (e.g., chapters),\nprovided that the amount of text remains suﬃcient for learning. In this way it\nis possible to capture the relations between characters for each part, but not\nto describe the dynamic evolution of the same character over the whole text.\nVectors trained in diﬀerent embeddings, even with the same dimensionality, are\nin fact not directly comparable.\nThe method employed in [9] elegantly addresses this issue by aligning diﬀer-\nent temporal representations using a shared coordinate system. The model uses a\nskip-gram Word2Vec architecture, where the context matrix (the output weight\nmatrix) is ﬁxed during the training, while allowing the word embedding input\nweight matrices to change on the basis of co-occurrence frequencies that are\nspeciﬁc to a given temporal interval. After training, model returns the context\nembeddings, that we are going to consider as a temporal word embedding. To\nachieve that, ﬁrst, a static word embedding is trained with random initialization\nusing the whole text and ignoring temporal slices.\n1 E.g., see https://nlp.stanford.edu/software/CRF-NER.html.\n4 Vani K, Simone Mellace, and Alessandro Antonucci\nLet us denote as W the corresponding word embedding matrix and as W′\nthe corresponding context matrix. For each slice, we instead initialize the word\nembedding matrix with W while keeping W′as a frozen context matrix equal for\nall the time slices [9]. This initialization has been proved to force alignment and\nmake it possible to compare vectors from embeddings associated to diﬀerent\ntime slices. The architecture is depicted in Figure 1. In particular, we adopt\nthe dynamic initialization scheme proposed in [29], which appears to be more\nsuitable for narrative text because of its intrinsic sequential nature.\nDynamic embeddings, generally used for word analogies, are considered here\nto describe and interpret relations by means of the trajectories spanned by the\nvectors associated to diﬀerent characters. The character embeddings are rep-\nresented in a visual space by dimensionality reduction [20] to understand the\nevolving relations between characters, in terms of time slices in a novel such as\nchapters or other parts of the text. This could be further related to character\nsentiments, clustering of emotions and other descriptions.\nFig. 1.Training temporal embeddings\n4 BERT-based Classiﬁcation of Character Relations\nThe unsupervised approach considered in the previous section describes the re-\nlation between characters in terms of relative positions of the corresponding\nvectors and their evolution. Here we consider a character relation extraction\nbased on binary classiﬁers. This is a supervised approach based on a ground\ntruth of annotated sentences where the two characters are identiﬁed together\nwith a Boolean value expressing whether or not the relation under consideration\nis met. The character names or aliases are eventually replaced by anonymous\nplaceholders, as this helps the model to learn the relationships by abstracting\nfrom the speciﬁc names.\nTemporal Embeddings and Transformers for Narrative Text Understanding 5\nFor the learning phase, we use the BERT classiﬁcation model. Its pre-trained\nmodel can be ﬁne tuned for classiﬁcation with an additional output layer. BERT\nhas a wordpiece tokenizer using two special tokens (SEP and CLS), which are\nused to encode valuable information of sentence structure and semantics after\nﬁne-tuning. The BERT-base has twelve transformer layers and in the classi-\nﬁcation task, the pooled token embedding from the CLS tokens is fed into a\nlinear classiﬁer for predictions. With the powerful attention mechanisms, BERT\nembeddings encode deep semantic and syntactic contextual information. The\nrelation extraction problem is modelled as a single sentence classiﬁcation task\nusing BERT model. More details about this general architecture are in [8].\nAs creating ground truth in this ﬁeld might be very expensive, we also discuss\ntechniques for automatic data annotation. As an example, let us focus on family\nrelations, where the problem is to decide whether or not there is familial relation\nbetween two characters. By increasing the neighbourhood parameter, the output\nof the DBSCAN clustering algorithm used for de-aliasing produces clusters in\nwhich the characters belonging to same family are together (as their second\nnames remain same). E.g., in the Harry Potter books, we have clusters with the\nPotter and the Weasley family. These clusters are used for automatic creation\nof the positive samples in training data, while the remaining characters are used\nfor negative sample generation.\n5 Experimental Analysis\nThe above discussed approaches to character relation modelling (Section 3) and\nunderstanding (Section 4) are validated here with two novels: Little Women\n(LW) by L.M. Alcott (text length 197‘524 words) and the ﬁrst six books of the\nHarry Potter series (HP) by J.K. Rowling (885‘943 words).\nFamily Relationship Classiﬁcation. Due to the intricate nature of its plot\nand its length, HP is being often used as a benchmark for natural language\nunderstanding in literary domain [4,27]. As an application of the ideas discussed\nin Section 4, we consider the task of predicting whether or not a given pair of\ncharacters has a family relation or not. A BERT based classiﬁer is used for that.2\nOut of six books, we use the sentences generated from ﬁve books as training set\nand the remaining book as a test according to a cross-validation scheme. For\nthe training set, the automatic class labelling is done by creating clusters for\nthe same family groups (see Section 4). The number of samples for each book\nis 160, 250, 239, 396, 478, and 231, the ratio of positive samples for each book\nbeing 30 .0%, 39.2%, 28.9%, 38.6%, 62.6%, and 47 .6%. BERT is used together\nwith the Adam optimizer [14]. This gives a learning rate equal to 2 · 10−5, warm\nup equal to 0 .1 and ten epochs. The results in Table 1 show reasonably good\naverage performances and their standard deviations over the six books. Note\nthat the aggregated values, corresponding to weighted averages, might be higher\nthan those for negative or positive samples only.\n2 See https://github.com/huggingface/transformers.\n6 Vani K, Simone Mellace, and Alessandro Antonucci\nSamples Precision Recall F-score\nNegative 79 ±13% 85 ±7% 81 ±8%\nPositive 77 ±9% 71 ±9% 73 ±4%\nAll 80 ±4% 78 ±7% 78 ±7%\nTable 1.Character familial relation classiﬁcation in Harry Potter\nA test is also done on the LW benchmark with the HP training data. A lower\nF-score level (64%) is obtained, possibly related to an over-ﬁtting eﬀected. This\nmight be relevant for literary texts, where the diﬀerences between diﬀerent data\n(e.g., diﬀerent texts of diﬀerent authors) are typically stronger than in other\ndomains.\nIt is important to note we have implemented a classiﬁcation model whose\npredictions are at the sentence level. When coping with character pairs, it would\nbe more appropriate to consider a higher level, i.e, prediction with respect to all\nthe sentences that express the character pair relation. This is achieve by a bag\nof sentence approach, where a character pair is considered to have a relation, if\nat least one of the sentences is predicted as positive. For HP there are 85 entity\npairs (12 positive and 73 negative) and the results are 9 positive (75%) and 66\n(90%) negative pairs correctly predicted. Considering the intrinsic complexity of\nliterary text, where sentences might have very complex structures, this might be\nregarded as a promising result and also advocate our strategy for the automatic\ngeneration of training set.\nTemporal Word Embeddings. Following the discussion in Section 3, we train\na temporal word embedding for the ﬁrst six books of HP. We focus on the four\ncharacters which appear more frequently. The static embedding is trained with\nthe whole text of each book, while the dynamic embeddings are based on sub-\nslices containing text of length equal to 1000 characters. For each character, we\nextract the corresponding trajectory for each book. For a better interpretation\nof the relations, we consider the main character (i.e., Harry) and plot the evolu-\ntion over time of the relative (cosine) distances from the other characters. Since\nthese vectors embed semantic information, it is expected that in the trajectories\ncorresponding to smaller distances correspond to closer relations with Henry. In\nfact, the results in Figure 2 show that the trajectories of positive characters or\nfriends (i.e., Ron and Hermione) move in a similar way. The main antagonist\n(i.e., Voldemort) is found instead to move in a diﬀerent direction and at a higher\ndistance.\nA similar analysis for LW is reported in Figure 3. In this case we display\na t-SNE [20] two-dimensional projection of the vectors over diﬀerent groups\nof chapters for the four major characters (i.e., the four March sisters). As a\ncomment, the temporal word embedding seems to capture the separation, during\nthe central part of the plot, between the characters of Joe and Amy, i.e., the two\ncharacters who left their home town, and the other two sisters.\nTemporal Embeddings and Transformers for Narrative Text Understanding 7\nBook I Book II Book III Book IV Book V\nHermione\nRon\nVoldemort\nFig. 2.Characters trajectories for Harry Potter\nMeg\nAmy\nBeth\nJoe\nFig. 3.Character trajectories for Little Women\n8 Vani K, Simone Mellace, and Alessandro Antonucci\n6 Conclusion\nIn this paper, we presented supervised and unsupervised DL models for analysing\nand interpreting character relations in a novel. We used BERT classiﬁers for\npredicting the character relations, while an unsupervised approach based on\ntemporal word embeddings was used to interpret the character relation evolution.\nBoth methods are found to be promising to explore the relations involved within\ncharacters in a novel. Thus, the approaches can be further applied to literary text\nunderstanding for deriving character networks and hence studying the relations\nand sentiments involved. In future, we want to integrate these approaches to\nbuild a more user-friendly tool to analyse the character networks and use it for\nan extensive validation.\nReferences\n1. Agarwal, A., Corvalan, A., Jensen, J., Rambow, O.: Social network analysis of\nAlice in Wonderland. In: Proceedings of the NAACL-HLT 2012 Workshop on com-\nputational linguistics for literature. pp. 88–96 (2012)\n2. Bamler, R., Mandt, S.: Dynamic word embeddings. In: Proceedings of the 34th\nInternational Conference on Machine Learning. vol. 70, pp. 380–389 (2017)\n3. Birant, D., Kut, A.: ST-DBSCAN: An algorithm for clustering spatial–temporal\ndata. Data & Knowledge Engineering 60(1), 208–221 (2007)\n4. Bonato, A., DAngelo, D.R., Elenberg, E.R., Gleich, D.F., Hou, Y.: Mining and\nmodeling character networks. In: International workshop on algorithms and models\nfor the web-graph. pp. 100–114. Springer (2016)\n5. Celikyilmaz, A., Hakkani-Tur, D., He, H., Kondrak, G., Barbosa, D.: The actor-\ntopic model for extracting social networks in literary narrative. In: NIPS Workshop:\nMachine Learning for Social Computing. p. 8 (2010)\n6. Cer, D., Yang, Y., Kong, S.y., Hua, N., Limtiaco, N., John, R.S., Constant, N.,\nGuajardo-Cespedes, M., Yuan, S., Tar, C., et al.: Universal sentence encoder for\nEnglish. In: Proceedings of the 2018 Conference on Empirical Methods in Natural\nLanguage Processing: System Demonstrations. pp. 169–174 (2018)\n7. Chaturvedi, S., Srivastava, S., Daume III, H., Dyer, C.: Modeling evolving rela-\ntionships between characters in literary novels. In: Proceedings of AAAI (2016)\n8. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidi-\nrectional transformers for language understanding. In: Proceedings of the 2019\nConference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers).\npp. 4171–4186 (2019)\n9. Di Carlo, V., Bianchi, F., Palmonari, M.: Training temporal word embeddings with\na compass. In: Proceedings of AAAI. vol. 33, pp. 6326–6334 (2019)\n10. Han, R., Liang, M., Alhafni, B., Peng, N.: Contextualized word embed-\ndings enhanced event temporal relation extraction for story understanding.\narXiv:1904.11942 (2019)\n11. Hassan, H.A.M., Sansonetti, G., Gasparetti, F., Micarelli, A., Beel, J.: BERT,\nELMo, USE and InferSent sentence encoders: The panacea for research-paper rec-\nommendation? CEUR Workshop Proceedings 2431, 6–10 (2019)\n12. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural computation\n9(8), 1735–1780 (1997)\nTemporal Embeddings and Transformers for Narrative Text Understanding 9\n13. K, V., Antonucci, A.: Novel2graph: Visual summaries of narrative text enhanced\nby machine learning. In: Al´ ıpio, M.J., Ricardo, C., Adam, J., Sumit, B. (eds.)\nProceedings of Text2Story - Second Workshop on Narrative Extraction From Texts\nco-located with 41th European Conference on Information Retrieval (ECIR 2019).\npp. 29–37. CEUR Workshop Proceedings (2019)\n14. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980 (2014)\n15. Kulkarni, V., Al-Rfou, R., Perozzi, B., Skiena, S.: Statistically signiﬁcant detection\nof linguistic change. In: Proc. of the 24th Int. Conf. on World Wide Web. pp. 625–\n635 (2015)\n16. Kutuzov, A., Øvrelid, L., Szymanski, T., Velldal, E.: Diachronic word embeddings\nand semantic shifts: a survey. In: Proceedings of the 27th International Conference\non Computational Linguistics. pp. 1384–1397 (2018)\n17. Labatut, V., Bost, X.: Extraction and analysis of ﬁctional character networks: A\nsurvey. ACM Computing Surveys (CSUR) 52(5), 1–40 (2019)\n18. Li, F., Zhang, M., Fu, G., Ji, D.: A neural joint model for entity and relation\nextraction from biomedical text. BMC bioinformatics 18(1), 198 (2017)\n19. Li, Z., Ding, X., Liu, T.: Story ending prediction by transferable BERT.\narXiv:1905.07504 (2019)\n20. Maaten, L.v.d., Hinton, G.: Visualizing data using t-SNE. Journal of machine\nlearning research 9(Nov), 2579–2605 (2008)\n21. Mikolov, T., Sutskever, I., Chen, K., Corrado, G.S., Dean, J.: Distributed rep-\nresentations of words and phrases and their compositionality. In: Burges, C.J.C.,\nBottou, L., Welling, M., Ghahramani, Z., Weinberger, K.Q. (eds.) NIPS. pp. 3111–\n3119 (2013)\n22. Mou, L., Meng, Z., Yan, R., Li, G., Xu, Y., Zhang, L., Jin, Z.: How transferable\nare neural networks in nlp applications? In: Proceedings of the 2016 Conference\non Empirical Methods in Natural Language Processing. pp. 479–489 (2016)\n23. Pennington, J., Socher, R., Manning, C.D.: Glove: Global vectors for word repre-\nsentation. In: Proceedings of the 2014 conference on empirical methods in natural\nlanguage processing (EMNLP). pp. 1532–1543 (2014)\n24. Peters, M.E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., Zettle-\nmoyer, L.: Deep contextualized word representations. In: Proceedings of NAACL-\nHLT. pp. 2227–2237 (2018)\n25. Piper, A., Algee-Hewitt, M., Sinha, K., Ruths, D., Vala, H.: Studying literary\ncharacters and character networks. In: DH (2017)\n26. Ruchansky, N., Seo, S., Liu, Y.: Csi: A hybrid deep model for fake news detection.\nIn: Proceedings of the 2017 ACM on Conference on Information and Knowledge\nManagement. pp. 797–806 (2017)\n27. Sparavigna, A.C.: On social networks in plays and novels. International Journal of\nSciences 2(10) (2013)\n28. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\n L., Polosukhin, I.: Attention is all you need. In: NIPS. pp. 5998–6008 (2017)\n29. Volpetti, C., K, V., Antonucci, A.: Temporal word embeddings for narrative under-\nstanding. In: ICMLC 2020: Proceedings of the Twelfth International Conference\non Machine Learning and Computing. ACM Press International Conference Pro-\nceedings Series, ACM (2020), iSBN: 978-1-4503-7642-6",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7597991824150085
    },
    {
      "name": "Transformer",
      "score": 0.673966646194458
    },
    {
      "name": "Character (mathematics)",
      "score": 0.6352896094322205
    },
    {
      "name": "Narrative",
      "score": 0.6219172477722168
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6122987866401672
    },
    {
      "name": "Natural language processing",
      "score": 0.6011289954185486
    },
    {
      "name": "Natural language understanding",
      "score": 0.5323707461357117
    },
    {
      "name": "Deep learning",
      "score": 0.5302861332893372
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.4999361038208008
    },
    {
      "name": "Natural language",
      "score": 0.23744019865989685
    },
    {
      "name": "Linguistics",
      "score": 0.21848127245903015
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2614128279",
      "name": "Dalle Molle Institute for Artificial Intelligence Research",
      "country": "CH"
    }
  ]
}