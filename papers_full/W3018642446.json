{
  "title": "Attention is Not Only a Weight: Analyzing Transformers with Vector Norms",
  "url": "https://openalex.org/W3018642446",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2607129003",
      "name": "Kobayashi, Goro",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4281494597",
      "name": "Kuribayashi, Tatsuki",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3136630471",
      "name": "Yokoi, Sho",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A677753238",
      "name": "Inui, Kentaro",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2977162702",
    "https://openalex.org/W2156985047",
    "https://openalex.org/W2897507397",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2148708890",
    "https://openalex.org/W2970727289",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W2952682849",
    "https://openalex.org/W1973923101",
    "https://openalex.org/W2964159778",
    "https://openalex.org/W2970120757",
    "https://openalex.org/W2972342261",
    "https://openalex.org/W3153147196",
    "https://openalex.org/W2950768109",
    "https://openalex.org/W2626639386",
    "https://openalex.org/W2910243263",
    "https://openalex.org/W2986266667",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2912070261",
    "https://openalex.org/W2973154008",
    "https://openalex.org/W2953369973",
    "https://openalex.org/W2970565456",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2934842096",
    "https://openalex.org/W2398041834",
    "https://openalex.org/W3006881356",
    "https://openalex.org/W2970726176",
    "https://openalex.org/W2995446988",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2912351236",
    "https://openalex.org/W3035563045",
    "https://openalex.org/W2971296520",
    "https://openalex.org/W2972324944"
  ],
  "abstract": "Attention is a key component of Transformers, which have recently achieved considerable success in natural language processing. Hence, attention is being extensively studied to investigate various linguistic capabilities of Transformers, focusing on analyzing the parallels between attention weights and specific linguistic phenomena. This paper shows that attention weights alone are only one of the two factors that determine the output of attention and proposes a norm-based analysis that incorporates the second factor, the norm of the transformed input vectors. The findings of our norm-based analyses of BERT and a Transformer-based neural machine translation system include the following: (i) contrary to previous studies, BERT pays poor attention to special tokens, and (ii) reasonable word alignment can be extracted from attention mechanisms of Transformer. These findings provide insights into the inner workings of Transformers.",
  "full_text": "Attention is Not Only a Weight:\nAnalyzing Transformers with Vector Norms\nGoro Kobayashi1 Tatsuki Kuribayashi1,2 Sho Yokoi1,3 Kentaro Inui1,3\n1 Tohoku University 2 Langsmith Inc. 3 RIKEN\n{goro.koba, kuribayashi, yokoi, inui}@ecei.tohoku.ac.jp\nAbstract\nAttention is a key component of Transform-\ners, which have recently achieved consider-\nable success in natural language processing.\nHence, attention is being extensively studied\nto investigate various linguistic capabilities of\nTransformers, focusing on analyzing the par-\nallels between attention weights and speciﬁc\nlinguistic phenomena. This paper shows that\nattention weights alone are only one of the\ntwo factors that determine the output of atten-\ntion and proposes a norm-based analysis that\nincorporates the second factor, the norm of\nthe transformed input vectors. The ﬁndings\nof our norm-based analyses of BERT and a\nTransformer-based neural machine translation\nsystem include the following: (i) contrary to\nprevious studies, BERT pays poor attention to\nspecial tokens, and (ii) reasonable word align-\nment can be extracted from attention mecha-\nnisms of Transformer. These ﬁndings provide\ninsights into the inner workings of Transform-\ners.\n1 Introduction\nTransformers (Vaswani et al., 2017; Devlin et al.,\n2019; Yang et al., 2019; Liu et al., 2019; Lan et al.,\n2020) have improved the state-of-the-art in a wide\nrange of natural language processing tasks. The\nsuccess of the models has not yet been sufﬁciently\nexplained; hence, substantial research has focused\non assessing the linguistic capabilities of these\nmodels (Rogers et al., 2020; Clark et al., 2019).\nOne of the main features of Transformers is that\nthey utilize an attention mechanism without the\nuse of recurrent or convolutional layers. The atten-\ntion mechanism computes an output vector by ac-\ncumulating relevant information from a sequence\nof input vectors. Speciﬁcally, it assigns attention\nweights (i.e., relevance) to each input, and sums\nup input vectors based on their weights. The anal-\nysis of correlations between attention weights and\nvarious linguistic phenomena (i.e., weight-based\nanalysis) is a prominent research area (Clark et al.,\n2019; Kovaleva et al., 2019; Reif et al., 2019; Lin\net al., 2019; Mare ˇcek and Rosa, 2019; Htut et al.,\n2019; Raganato and Tiedemann, 2018; Tang et al.,\n2018).\nThis paper ﬁrst shows that weight-based analy-\nsis is insufﬁcient to analyze the attention mech-\nanism. Weight-based analysis is a common ap-\nproach to analyze the attention mechanism by\nsimply tracking attention weights. The attention\nmechanism can be expressed as aweighted sum of\nlinearly transformed vectors (Section 2.2); how-\never, the effect of transformed vectors in weight-\nbased analysis is ignored. We propose a norm-\nbased analysis that considers the previously ig-\nnored factors (Section 3). In this analysis, we mea-\nsure the norms (lengths) of the vectors that were\nsummed to compute the output vector of the atten-\ntion mechanism.\nUsing the norm-based analysis of BERT (Sec-\ntion 4), we interpreted the internal workings of\nthe model in more detail than when weight-based\nanalysis was used. For example, the weight-based\nanalysis (Clark et al., 2019; Kovaleva et al., 2019)\nreports that speciﬁc tokens, such as periods, com-\nmas, and special tokens (e.g., separator token;\n[SEP]), tend to have high attention weights. How-\never, our norm-based analysis found that the in-\nformation collected from vectors corresponding to\nspecial tokens was considerably lesser than that re-\nported in the weight-based analysis, and the large\nattention weights of these vectors were canceled\nby other factors. Additionally, we found that\nBERT controlled the levels of contribution from\nfrequent, less informative words by controlling the\nnorms of their vectors.\nIn the analysis of a Transformer-based NMT\nsystem (Section 5), we reinvestigated how accu-\nrate word alignment can be extracted from the\narXiv:2004.10102v2  [cs.CL]  6 Oct 2020\nsource-target attention. The weight-based results\nof Li et al. (2019), Ding et al. (2019), and Zenkel\net al. (2019) have empirically shown that word\nalignments induced by the source-target attention\nof the Transformer-based NMT systems are noisy.\nOur experiments show that more accurate align-\nments can be extracted by focusing on the vector\nnorms.\nThe contributions of this study are as follows:\n• We propose a novel method of analyzing an\nattention mechanism based on vector norms\n(norm-based analysis). The method considers at-\ntention weights and previously ignored factors,\ni.e., the norm of the transformed vector.\n• Our norm-based analysis of BERT reveals that\n(i) the attention mechanisms pay considerably\nlesser attention to special tokens than to observa-\ntions that are solely based on attention weights\n(weight-based analysis), and (ii) the attention\nmechanisms tend to discount frequent words.\n• Our norm-based analysis of a Transformer-based\nNMT system reveals that reasonable word align-\nment can be extracted from source-target atten-\ntion, in contrast to the previous results of the\nweight-based analysis.\nThe codes of our experiments are publicly avail-\nable.1\n2 Background\n2.1 Attention mechanism\nAttention is a core component of Transformers,\nwhich consist of several layers, each containing\nmultiple attentions (“heads”). We focused on ana-\nlyzing the inner workings of these heads.\nAs illustrated in Figure 1, each attention head\ngathers relevant information from the input vec-\ntors. A vector is updated by vector transforma-\ntions, attention weights, and a summation of vec-\ntors. Mathematically, attention computes each\noutput vector yi ∈ Rd from the corresponding\npre-update vector ˜yi ∈Rd and a sequence of input\nvectors X= {x1,..., xn}⊆ Rd:\nyi =\n( n∑\nj=1\nαi,jv(xj)\n)\nWO (1)\nαi,j := softmax\nxj∈X\n(q(˜yi)k(xj)⊤\n√\nd′\n)\n∈R, (2)\nwhere αi,j is the attention weight assigned to the\ntoken xj for computing yi, and q(·), k(·), and v(·)\n1https://github.com/gorokoba560/\nnorm-analysis-of-transformer\n!!,#\" ##!!,$\" #$!!,%\" #%!!,&\" #& !!,'\" #'\n!!,& !!,' !!,$ !!,#\nWeighted \nValue\nvectors\nAttention\nweights\nInput vectors\n!!,%\n$!\n!(\nPre-\nupdate\nvector\nOutput vector\nWeight\nmatrix\n\" ##\" #' \" #$\" #%\" #&\nValue\nvectors\n$%!###$#'#%#&\nΣ\neq. (2)\nFigure 1: Overview of attention mechanism in Trans-\nformers. Sizes of the colored circles illustrate the value\nof the scalar or the norm of the corresponding vector.\nare the query, key, and value transformations, re-\nspectively.\nq(˜yi) := ˜yiW Q + bQ\n(\nW Q ∈Rd×d′\n, bQ ∈Rd′ )\nk(xj) :=xjW K + bK\n(\nW K ∈Rd×d′\n, bK ∈Rd′ )\nv(xj) :=xjW V + bV\n(\nW V ∈Rd×d′\n, bV ∈Rd′ )\n.\nAttention gathers value vectorsv(xj) based on at-\ntention weights and then, applies matrix multipli-\ncation WO ∈Rd′×d (Figure 1). 2 Boldface letters\nsuch as x denote row (not column) vectors, fol-\nlowing the notations in Vaswani et al. (2017).\nIn self-attention, the input vectors X and the\npre-update vector ˜yi are previous layer’s output\nrepresentations. In source-target attention, Xcor-\nresponds to the representations of the encoder, and\nvector ˜yi (and updated vector yi) corresponds to\nthe vector of the i-th input token of the decoder.\n2.2 Attention is a weighted sum of vectors\nWith a simple reformulation, one can observe that\nthe attention mechanism computes the weighted\nsum of the transformed input vectors. Because of\nthe linearity of the matrix product, we can rewrite\nEquation 1 as\nyi =\nn∑\nj=1\nαi,j f(xj) (3)\n2Whether bias b is added to calculate query, key, and value\nvectors depends on the implementation. W O ∈Rd′×d in\nEquation 1 corresponds to the part of W O ∈Rhd′×d that\nwas introduced in Vaswani et al. (2017) which is applied to\neach head; where h is the number of heads, and hd′ = d\nholds.\n!!!\"!#!$!%\n! \"!! \"\" ! \"#! \"$! \"%\n!!,#\" ##!!,$\" #$!!,%\" #%!!,&\" #& !!,'\" #'\n#&,% #&,\" #&,# #&,!\nWeighted\nvectors\nAttention\nweights\nTransformed\nvectors\nInput vectors\n#&,$\nΣ\neq. (2)\n\"&\n\"#&\nOutput vector\nPre-\nupdate\nvector\n' (!,(\t) *(\n#\n(*&\n(!,') *'\nFigure 2: Overview of attention mechanism based on\nEquation 3. It computes the output vector by summing\nthe weighted vectors; vectors with larger norms have\nhigher contributions. Sizes of the colored circles illus-\ntrate the value of the scalar or the norm of the corre-\nsponding vector.\nf(x) :=\n(\nxWV + bV )\nWO. (4)\nEquation 3 shows that the attention mechanism\nﬁrst transforms each input vector x to generate\nf(x) ; computes attention weights α ; and then\ncompute the sum αf(x) (see Figure 2).\n2.3 Problems encountered in weight-based\nanalysis\nThe attention mechanism has been designed to\nupdate representations by gathering relevant in-\nformation from the input vectors. Prior stud-\nies have analyzed attention, focusing on atten-\ntion weights, to ascertain which input vectors\ncontribute (weight-based analysis) (Clark et al.,\n2019; Kovaleva et al., 2019; Reif et al., 2019; Lin\net al., 2019; Mare ˇcek and Rosa, 2019; Htut et al.,\n2019; Raganato and Tiedemann, 2018; Tang et al.,\n2018).\nAnalyses solely based on attention weight are\nbased on the assumption that the larger the atten-\ntion weight of an input vector, the higher its con-\ntribution to the output. However, this assumption\ndisregards the magnitudes of the transformed vec-\ntors. The problem encountered when neglecting\nthe effect of f(xj) is illustrated in Figure 2. The\ntransformed vector f(x1) for input x1 is assumed\nto be very small (∥f(x1)∥≈ 0), while its attention\nweight αi,1 is considerably large. Note that the\nsmall αi,1f(x1) contributes a little to the output\nvector yi because yi is the sum of αf(x), where a\nlarger vector contributes more to the output. Con-\nversely, the large αi,3f(x3) dominates the output\nyi. Therefore, in this case, only considering the\nattention weight may lead to a wrong interpreta-\ntion of the high contribution of input vector x1 to\noutput yi. Nevertheless, x1 hardly has any effect\non yi.\nAnalyses based on attention weights have not\nprovided clear results in some cases. For example,\nClark et al. (2019) reported that input vectors for\nseparator tokens [SEP] tend to receive remarkably\nlarge attention weights in BERT, while changing\nthe magnitudes of these weights does not affect\nthe masked-token prediction of BERT. Such re-\nsults can be attributed to the aforementioned issue\nof focusing only on attention weights.\n3 Proposal: norm as a degree of attention\nAs described in Section 2.3, analyzing the atten-\ntion mechanism with only attention weights ne-\nglects the effect of the transformed vector f(xj),\nwhich has a signiﬁcant impact as we discussed\nlater.\nHerein, we propose the measurement of\nthe norm of the weighted transformed vector\n∥αf(x)∥, given by Equation 3, to analyze the\nattention mechanism behavior. 3 Unlike in pre-\nvious studies, we analyzed the behaviors of the\nnorms, ∥αf(x)∥and ∥f(x)∥, and αto gain more\nin-depth insights into the functioning of attention.\nThe proposed method of analyzing the attention\nmechanism is callednorm-based analysis and the\nmethod that solely analyzes the attention weights\nis called weight-based analysis.\nIn Sections 4 and 5, we provide insights into the\nworking of Transformers using norm-based anal-\nysis. Appendix A explains that our norm-based\nanalysis can also be effectively applied to an en-\ntire multi-head attention mechanism.\n4 Experiments: BERT\nFirst, we show that the previously ignored\ntransformed-vector norm affects the analysis of\nattention in BERT (Section 4.1). Applying our\nnorm-based analysis, we re-examine the previ-\nous reports on BERT obtained by weight-based\nanalysis (Section 4.2). Next, we demonstrate the\npreviously overlooked properties of BERT (Sec-\ntion 4.3).\n3We use the standard Euclidean norm.\nHead µ σ CV Max Min\nLayer 2–Head 4 (max CV) 4.26 1.59 0.37 12.66 0.96\nLayer 2–Head 7 (min CV) 4.00 0.50 0.12 6.15 1.35\nAverage 5.15 1.17 0.22 - -\nTable 1: Mean ( µ), standard deviation ( σ), coefﬁcient\nof variance (CV), and maximum and minimum values\nof ∥f(x)∥. In the last row, the former three are aver-\naged over all the heads.\nGeneral settings: Following the previous stud-\nies (Clark et al., 2019; Kovaleva et al., 2019; Reif\net al., 2019; Lin et al., 2019; Htut et al., 2019), we\nused the pre-trained BERT-base4, with 12 layers,\neach containing 12 attention heads. We used the\ndata provided by Clark et al. (2019) for the anal-\nysis.5 The data contains 992 sequences extracted\nfrom Wikipedia, where each sequence consists of\ntwo consecutive paragraphs, in the form of:[CLS]\nparagraph1 [SEP] paragraph2 [SEP]. Each se-\nquence consists of up to 128 tokens, with an aver-\nage of 122 tokens.\n4.1 Does f(x) have an impact?\nWe analyzed the coefﬁcient of variation (CV) 6\nof previously ignored effect— ∥f(x)∥—to ﬁrst\ndemonstrate the degree to which ∥αf(x)∥differs\nfrom weight α. We computed the CV of∥f(x)∥of\nall the example data for each head. Table 1 shows\nthat the average CV is 0.22. Typically, the value\nof the norm ∥f(x)∥varies from 0.78 to 1.22 times\nthe average value of the ∥f(x)∥. Thus, there is a\ndifference between the weightαand ∥αf(x)∥due\nto the dispersion of ∥f(x)∥, which motivated us\nto consider ∥f(x)∥in the attention analysis. Ap-\npendix B presents the detailed results.\n4.2 Re-examining previous observation\nIn this section, with the application of our norm-\nbased analysis, we reinvestigate the previous ob-\nservation of Clark et al. (2019); they analyzed\nBERT using the weight-based analysis.\nSettings: First, all the data were fed into BERT.\nThen, the weight α and ∥αf(x)∥were collected\nfrom each head. Following Clark et al. (2019), we\nreport the results of the following categories: (i)\n4We used PyTorch implementation of BERT-base (un-\ncased) released at https://github.com/huggingface/\ntransformers.\n5https://github.com/clarkkev/attention-analysis\n6Coefﬁcient of variation (CV) is a standardized (scale-\ninvariant) measure of dispersion, which is deﬁned by the ra-\ntio of the standard deviation σto the mean µ; CV := σ/µ.\n\u0011\u000f\u0011\n\u0011\u000f\u0013\n\u0011\u000f\u0015\n\u0011\u000f\u0017\n\u0011\u000f\u0019\n\u0012\u000f\u0011\n\u0012 \u0013 \u0014 \u0015 \u0016 \u0017 \u0018 \u0019 \u001a \u0012\u0011 \u0012\u0012 \u0012\u0013\n<$-4> <4&1>\n\u000f\u0001PS\u0001\r 0UIFS\nAvg. !\nLayer\n(a) Weight-based analysis.\n\u0011\n\u0012\n\u0013\n\u0014\n\u0015\n\u0012 \u0013 \u0014 \u0015 \u0016 \u0017 \u0018 \u0019 \u001a \u0012\u0011 \u0012\u0012 \u0012\u0013\n<$-4> <4&1>\n\u000f\u0001PS\u0001\r 0UIFS\nAvg. !\" #\nLayer\n(b) Norm-based analysis.\nFigure 3: Each point corresponds to averaged α or\n∥αf(x)∥on a word category in a given layer. Note\nthat, in each layer, the sum of α among all the cate-\ngories is 1. The x-axis denotes the index of the layers.\nToken category Number of vectors Spearman’s ρ\n[CLS] 17,443,296 -0.34\n[SEP] 34,886,592 -0.69\ncomma & period 182,838,528 -0.25\nOthers 1,944,928,224 -0.06\nTable 2: Spearman rank correlation coefﬁcient between\nαand ∥f(x)∥in each token category.\n[CLS], (ii) [SEP], (iii) periods and commas, and\n(iv) the other tokens. More speciﬁc descriptions\nof the experiments are provided in Appendix D.\nResults: The weight-based and norm-based\nanalyses exhibited entirely different trends (Fig-\nure 3). The vectors for speciﬁc tokens— [CLS],\n[SEP], and punctuations—have remarkably large\nattention weights, which is consistent with the re-\nport of Clark et al. (2019). In contrast, our norm-\nbased analysis demonstrated that the contributions\nof vectors corresponding to these tokens were gen-\nerally small (Figure 3b). The result demonstrates\nthat the size of the transformed vector f(x) plays\na considerable role in controlling the amount of\ninformation obtained from the speciﬁc tokens.\nClark et al. (2019) hypothesized that if the nec-\nessary information is not present in the input vec-\ntors, BERT assigns large weights to [SEP], which\nappears in every input sequence, to avoid the in-\ncorporation of any additional information via at-\ntention.7 Clark et al. (2019) called this oper-\nation no-operation (no-op). However, it is un-\nclear whether assigning large attention weights to\n[SEP] realizes the operation of collecting little in-\nformation from the input sequence.\nOur norm-based analysis demonstrates that the\namount of information from the vectors corre-\nsponding to [SEP] is small (Figure 3b). This re-\nsult supports the interpretation that BERT con-\nducts “no-op,” in which attention to[SEP] is con-\nsidered a signal that does not collect anything. Ad-\nditionally, we hope that our norm-based analysis\ncan provide a better interpretation of other exist-\ning ﬁndings.\nAnalysis—The relationship between α and\n∥f(x)∥: It remains unclear how attention col-\nlects only a little information while assigning a\nhigh attention weight to a speciﬁc token, [SEP].\nHere, we demonstrate an interesting trend ofαand\n∥f(x)∥cancelling each other out on the tokens. 8\nTable 2 shows the Spearman rank correlation co-\nefﬁcient between αand ∥f(x)∥, corresponding to\nthe vectors in each category. The weight α and\nthe norm ∥f(x)∥have a negative correlation in\nterms of [CLS], [SEP], periods, and commas. This\ncancellation manages tocollect a little information\neven with large weights.\nFigure 4 illustrates the contrast between αand\n∥f(x)∥corresponding to [SEP] in each head. For\nmost of the heads, α and ∥f(x)∥clearly negate\nthe magnitudes of each other. A similar trend was\nobserved in [CLS], periods, and commas. Con-\nversely, no signiﬁcant trend was observed in the\nother tokens (see Appendix D.3).\nFigure 5 shows 1% randomly selected pairs of\nαand ∥f(x)∥in each word category. Even when\nthe same weight α is assigned, ∥f(x)∥can vary,\nsuggesting that αand ∥f(x)∥play a different roles\nin attention.\n4.3 Relation between frequency and ∥f(x)∥\nIn the previous section, we demonstrated that\n∥f(x)∥corresponding to the speciﬁc tokens (e.g.,\n[SEP]) is small. Based on the high frequencies9 of\n7Note that the attention mechanism has the constraint that the\nsum of the attention weights becomes 1.0 (see Equation 2).\n8Note that for any positive scalar λ∈R and vector x ∈Rd,\n∥λx∥= λ∥x∥.\n9The frequency ranks of the words[CLS], [SEP], period, and\ncomma, out of approximately 30,000 words, are 50, 28, 2,\nand 3, respectively.\n(a) α.\n (b) ∥f(x)∥.\nFigure 4: The higher value of averagedαor ∥f(x)∥for\n[SEP] tokens in a given head, the darker its cell.\nFigure 5: Relationship between α and ∥f(x)∥. Each\nplot corresponds to a pair of αi,j and ∥f(xj)∥in one\nof the attention heads. Each plot is colored by the word\ncategory corresponding to xj. Visualizations by cate-\ngory are shown in Appendix D.3.\nthese word types 10, we hypothesized that BERT\ncontrolled contributions of highly frequent, less\ninformative words by adjusting the norm of f(x).\nSettings: First, all the data were fed into the\nmodel. Then, for each input token t, we collected\nthe weight α and ∥f(x)∥. We averaged α and\n∥f(x)∥for all the heads for each tto analyze the\ntrend of the entire model. Let r(·) be a function\nthat returns the frequency rank of a given word. 11\nWe analyzed the relationship of r(t) with α and\n∥f(x)∥.\nResults: The Spearman rank correlation coefﬁ-\ncient between the frequency rank r(t) and ∥f(x)∥\nwas 0.75, indicating a strong positive correlation.\nIn contrast, the Spearman rank correlation coef-\nﬁcient did not show any correlation ( ρ = 0.06)\nbetween r(t) and α.12 The visualizations of their\nrelationships are shown in Appendix D.4.\nThese results demonstrate that the self-\n10We call word type as “word.” Each instance of a word is\ncalled “token.”\n11We counted the frequency for each word type by reproduc-\ning the training data of BERT.\n12The Spearman rank correlation coefﬁcient without special\ntokens, periods, and commas was 0.28 for the attention\nweights and 0.69 for the norms.\nattentions in BERT reduce the information from\nhighly frequent words by adjusting ∥f(x)∥and\nnot α. This frequency-based effect is consistent\nwith the intuition that highly frequent words,\nsuch as stop words, are unlikely to play an\nimportant role in solving the pre-training tasks\n(masked-token prediction and next-sentence\nprediction).\n5 Experiments: Transformer for NMT\nAdditionally, we analyzed the source-target atten-\ntion in a Transformer-based NMT system. One\nmajor research topic in the NMT ﬁeld is whether\nNMT systems internally capture word alignment\nbetween source and target texts, and if so, how\nword alignment can be extracted from black-box\nNMT systems. Li et al. (2019), Ding et al. (2019),\nand Zenkel et al. (2019) empirically showed, us-\ning the weight-based method, that word align-\nment induced by the attention of the Transformer\nis noisy. In this section, we show the analy-\nsis of source-target attention using vector norms\n∥αf(x)∥and demonstrate that clean alignments\ncan be extracted from the source-target attention.\nWord alignment can be used to provide rich infor-\nmation for the users of NMT systems (Ding et al.,\n2019).\nExperimental procedure: Following Zenkel\net al. (2019) and Ding et al. (2019), we trained\na Transformer-based NMT system for German-to-\nEnglish translation on the Europarl v7 corpus 13.\nNext, we extracted word alignments from α and\n∥αf(x)∥ under the force decoding setup. Fi-\nnally, we evaluated the derived alignment using\nthe alignment error rate (AER) (Och and Ney,\n2000). A low AER score indicates that the ex-\ntracted word alignments are close to the refer-\nence. We used the gold alignment dataset pro-\nvided by Vilar et al. (2006) 14. Experiments were\nperformed on ﬁve random seeds, and the average\nAER scores were reported. The experimental set-\ntings are detailed in Appendix E.\n5.1 Alignment extraction from attention\nWeights or norms: A typical alignment extrac-\ntion method uses attention weights (Li et al.,\n2019; Ding et al., 2019; Zenkel et al., 2019).\nSpeciﬁcally, given a source-target sentence pair,\n13http://www.statmt.org/europarl/v7\n14https://www-i6.informatik.rwth-aachen.de/\ngoldAlignment/\nFigure 6: An example of behavior of the source-target\nattentions in an NMT system (German-to-English). At-\ntentions in the earlier layers focus the source word\n“ein” aligned with the input word “a,” while those\nin the latter layers focus the source word “Sch ¨uler”\naligned with the output word “student.”\n{s1,...,s J}and {t1,...,t I}, word alignment is\nestimated by calculating a source word sj that has\nthe highest weight when generating a target word\nti. We call this method the weight-based align-\nment extraction. In contrast, we propose a norm-\nbased alignment extraction method that extracts\nword alignments based on ∥αf(x)∥instead of α.\nFormally, in these methods, the source word sj\nwith the highest attention weight or norm during\nthe generating of target word ti is extracted as the\nword that is aligned with ti:\nargmax\nsj\nαi,j or argmax\nsj\n∥αi,jf(xj)∥. (5)\nIn Section 5.2, following Li et al. (2019), we an-\nalyze the word alignments that we obtained from\neach layer by integrating Hheads within the same\nlayer:\nargmax\nsj\nH∑\nh=1\nαh\ni,j or argmax\nsj\n∥\nH∑\nh=1\nαh\ni,jfh(xj)∥,\nwhere fh(xj) and αh\ni,j are the transformed vector\nand the attention weight at the h-th head, respec-\ntively.\nAlignment with input or output word: In our\npreliminary experiments (Appendix E.3), we ob-\nserved that the behavior of the source-target at-\ntention of the decoder differs between the earlier\nand later layers. As shown in Figure 6, at the time\ndecoding the word ti+1 with the input ti, atten-\ntion heads in the earlier layers assign large weights\nor norms to sj corresponding to the input ti “a,”\nwhereas those in the latter layers assign large val-\nues to sj corresponding to the output word ti+1\n“student.”\nBased on this observation, we explored two set-\ntings for investigating alignment extraction meth-\nods: alignment with output (AWO) and alignment\nwith input (AWI). The AWO setting refers to the\napproach introduced in Equation 5. Speciﬁcally,\nalignments (sj,ti) were extracted by considering\na source word sj that gained the highest weight\n(norm) when outputting a particular target word\nti.\nIn the AWI setting, alignments (sj,ti) were\nextracted by considering a source word sj that\ngained the highest weight (norm) when inputting\nthe wordti (i.e., predicting a wordti+1). Formally,\nalignment with the AWI setting is calculated as\nfollows:\nargmax\nsj\nαi+1,j or argmax\nsj\n∥αi+1,jf(xj)∥.\n(6)\n5.2 Comparative experiments\nWe compared the quality of the alignments that\nwere obtained by the following six methods:\n• norm-based extraction with the AWO/AWI set-\ntings\n• weight-based extraction with the AWO/AWI set-\ntings (Li et al., 2019; Zenkel et al., 2019; Ding\net al., 2019)\n• gradient-based extraction (Ding et al., 2019)\n• existing word aligners (Och and Ney, 2003; Dyer\net al., 2013)\nWe report the best and averaged AER scores\nacross the layers. In addition, we report on the\nAER score at the head and the layer with the high-\nest average ∥αf(x)∥in the norm-based extrac-\ntion.15 The settings are detailed in Appendix E.2.\nThe AER scores of each method are listed in Ta-\nble 3. The results show that word alignments ex-\ntracted using the proposed norm-based approach\nare more reasonable than those extracted using the\nweight-based approach. Additionally, better word\nalignments were extracted in the AWI setting than\nin the AWO setting. The alignment extracted us-\ning the layer with the highest average ∥αf(x)∥\nin the AWI setting is better than the gradient-\nbased method, and competitive with one of the ex-\nisting word aligners—fast align.16 These results\n15The average ∥αf(x)∥of the layer was determined by the\nsum of the average ∥αf(x)∥at each head in the layer.\n16Even at the head with the highest average ∥αf(x)∥. Al-\nthough the average score of ﬁve seeds in the AWI setting\nwas 35.5, four seeds out of them achieved great score range\nMethods AER ±SD\nTransformer – Attention-based Approach\n— Alignment with output setting —\nWeight-based\nlayer mean 68.4 1.0\nbest layer (layer 4 or 5) 47.7 1.7\nNorm-based (ours)\nlayer mean 62.9 0.7\nbest layer (layer 5) 41.4 1.4\nlayer with the highest average ∥αf(x)∥ 83.0 1.1\nhead with the highest average ∥αf(x)∥ 87.1 2.3\n— Alignment with input setting —\nWeight-based\nlayer mean 68.5 1.9\nbest layer (layer 2) 29.8 3.7\nNorm-based (ours)\nlayer mean 60.4 1.3\nbest layer (layer 2) 25.0 1.5\nlayer with the highest average ∥αf(x)∥ 25.0 1.5\nhead with the highest average ∥αf(x)∥ 35.5 21.0\nTransformer – Gradient-based Approach\nSmoothGrad from Ding et al. (2019) 36.4 -\nWord Aligner\nfast align from Zenkel et al. (2019) 28.4 -\nGIZA++ from Zenkel et al. (2019) 21.0 -\nTable 3: AER scores with different methods for\nGerman-to-English translation. The closer the ex-\ntracted word alignment is to the reference, the lower\nthe AER score. The “layer mean” denotes the average\nof AER scores across all layers. Each value is the aver-\nage of ﬁve random seeds.\nshow that much clearer word alignments can be\nextracted from a Transformer-based NMT system\nthan the results reported by existing research.\nThe primary reason behind the differences be-\ntween the results of the weight- and norm-based\nmethods was analogous to the ﬁnding discussed in\nSection 4.2, while some speciﬁc tokens, such as\n⟨/s⟩, the special token for the end of the sentence,\ntended to obtain heavy attention weights; their\ntransformed vectors were adjusted to be smaller,\nas shown in Figure 7.\n5.3 Relationship between norms and\nalignment quality\nWe further analyze the relationship between\n∥αf(x)∥and AER scores in the head-level. Fig-\nures 8a and 8b show the AER scores of the align-\nments obtained by the norm based extraction at\neach head in the AWO and AWI settings. Fig-\nure 8c shows the average of∥αf(x)∥at each head.\nThe small ∥αf(x)∥implies that α and ∥f(x)∥\ntend to cancel out in the head.\nComparing Figures 8a and 8c, the average\n∥αf(x)∥ and AER scores in the AWI setting\nfrom 23.6-to 25.7. The score was 77.5 for a remaining\nseed.\n(a) Reference.\n (b) α.\n (c) ∥αf(x)∥.\nFigure 7: Examples of the reference and extracted alignments using each method in layer 2 (best layer) in the\nAWI setting on one out of ﬁve seeds. Two misalignments in the weight-based extraction were resolved in the\nnorm-based analysis—alignments with the green frame. Examples of the extracted alignments in all the layers are\nshown in Appendix E.4.\nare inversely correlated (the Spearman rank and\nPearson correlation coefﬁcients are −0.44 and\n−0.52, respectively). This result is consistent\nwith Table 3, where the head or the layer with\nthe highest average∥αf(x)∥provides clean align-\nments in the AWI setting. This result suggests\nthat Transformer-based NMT systems may rely\non speciﬁc heads that align source and target to-\nkens. This result is also consistent with the ex-\niting reports that pruning some attention heads in\nTransformers does not change its performance; on\nthe contrary, it improves the performance (Michel\net al., 2019; Kovaleva et al., 2019).\nIn contrast, in the AWO setting (Figures 8b\nand 8c), such a negative correlation is not ob-\nserved; rather, a positive correlation is observed\n(Spearman’s ρ is 0.56, and the Pearson’s r is\n0.55). Actually, in the AWO setting, the align-\nments extracted from the head/layer with the high-\nest ∥αf(x)∥ is considerably worse than those\nfrom the other settings in Table 3. Investigating\nthe reason for these contrasting results would be\nour future work. In Appendix F, we also present\nthe results of a model with a different number of\nheads.\n6 Related work\n6.1 Probing of Transformers\nTransformers are used for many NLP tasks. Many\nstudies have probed their inner workings to un-\nderstand the mechanisms underlying their suc-\ncess (Rogers et al., 2020; Clark et al., 2019).\nThere are mainly two probing perspectives to\ninvestigate these models; they differ based on\nwhether the target of the analysis is per-token level\nor it considers token-to-token interactions. The\nﬁrst category assesses a single word or phrase-\nlevel linguistic capabilities of BERT, such as its\nperformance on part-of-speech tagging and word\nsense disambiguation performance (Tenney et al.,\n2019; Jawahar et al., 2019; Reif et al., 2019; Lin\net al., 2019; Wallace et al., 2019).\nThe latter category explores the ability of Trans-\nformers to capture token-to-token interactions,\nsuch as syntactic relations and word alignment in\nthe translation (Clark et al., 2019; Kovaleva et al.,\n2019; Htut et al., 2019; Reif et al., 2019; Lin et al.,\n2019; Goldberg, 2019; Ding et al., 2019; Zenkel\net al., 2019; Li et al., 2019; Raganato and Tiede-\nmann, 2018). The present study is closely related\nto the latter group; we have provided insights into\nthe token-to-token attention in Transformer-based\nsystems.\n6.2 Analyzing the token-to-token interaction\nTwo types of methods are mainly considered to\nanalyze the token-to-token interactions in Trans-\nformers. One is to track the attention weights, and\nthe other is to check the gradient of the output with\nrespect to the input of attention mechanisms.\nWeight-based analysis: Many studies have an-\nalyzed the linguistic capabilities of Transformers\nby tracking attention weights. This type of anal-\nysis has covered a wide range of subjects, in-\ncluding syntactic and semantic relationships (Tang\net al., 2018; Raganato and Tiedemann, 2018; Clark\net al., 2019; Reif et al., 2019; Jawahar et al., 2019;\nHtut et al., 2019; Kovaleva et al., 2019; Mare ˇcek\nand Rosa, 2019). However, as outlined in Sec-\ntion 2.3, these studies have ignored the effect of\nf(x). It has been actively discussed so far whether\nthe attention weights can be interpreted to explain\n(a) AER in the AWI setting.\n (b) AER in the AWO setting.\n (c) Averaged ∥αf(x)∥.\nFigure 8: AER scores and averaged ∥αf(x)∥in each head on one out of ﬁve seeds. The closer the extracted word\nalignment is to the reference, the lower the AER score—the lighter the color. The larger the averaged ∥αf(x)∥,\nthe darker the color.\nthe models (Jain and Wallace, 2019; Serrano and\nSmith, 2019; Wiegreffe and Pinter, 2019; Pruthi\net al., 2020; Vashishth et al., 2019).\nBrunner et al. (2020) have introduced “effective\nattention,” which has upgraded the weight-based\nanalysis. Their proposal is similar to ours; they ex-\nclude attention weights that do not affect the out-\nput owing to the application of transformation f\nand input x in the analysis. However, our proposal\ndiffers from theirs in some aspects. Speciﬁcally,\nwe aim to analyze the behavior of the whole at-\ntention mechanism more accurately, whereas they\naim to make the attention weights more accurate.\nFurthermore, the effectiveness of their approach\ndepends on the length of an input sequence; how-\never, ours approach does not have such a limita-\ntion (see Appendix G). Additionally, we incorpo-\nrate the scaling effects of f and x, whereas Brun-\nner et al. (2020) have considered only the binary\neffect—either the weight is canceled or not.\nGradient-based analysis: In the gradient anal-\nysis, the contribution of the input with respect to\nthe output of the attention mechanism is calculated\nusing the norm of a gradient matrix between the\ninput and the output vector (Pascual et al., 2020).\nIntuitively, such gradient-based methods measure\nthe change in the output vector with respect to the\nperturbations in the input vector. Estimating the\ncontribution of a to b = ∑ka by computing the\ngradient ∂b/∂a (= k) is analogous to estimating\nthe contribution of x to y = ∑αf(x) by ob-\nserving only an attention weight α.17 The two ap-\n17For simplicity, we consider a linear example: b = ∑ka.\nWe are aware that there is a gap between the two examples\nin terms of linearity. Further exploration of the connection\nto the gradient-based method is needed.\nproaches have the same kind of problems; that is,\nboth ignore the magnitude of the input, a or f(x).\n7 Conclusions and future work\nThis paper showed that attention weights alone are\nonly one of two factors that determine the output\nof attention. We proposed the incorporation of an-\nother factor, the transformed input vectors. Us-\ning our norm-based method, we provided a more\ndetailed interpretation of the inner workings of\nTransformers, compared to the studies using the\nweight-based analysis. We hope that this paper\nwill inspire researchers to have a broader view of\nthe possible methodological choices for analyzing\nthe behavior of Transformer-based models.\nWe believe that these ﬁndings can provide in-\nsights not only into the interpretation of the be-\nhaviors of Blackbox NLP systems but also into de-\nveloping a more sophisticated Transformer-based\nsystem. One possible direction is to design an at-\ntention mechanism that can collect almost no in-\nformation from an input sequence as the current\nsystems achieve it by exploiting the [SEP] token.\nIn future work, we plan to apply our norm-based\nanalysis to attention in other models, such as ﬁne-\ntuned BERT, RoBERTa (Liu et al., 2019), and AL-\nBERT (Lan et al., 2020). Furthermore, we expect\nto extend the scope of analysis from the attention\nto an entire Transformer architecture to better un-\nderstand the inner workings and linguistic capabil-\nities of the current powerful systems in NLP.\nAcknowledgments\nWe would like to thank the anonymous reviewers\nof the EMNLP 2020 and the ACL 2020 Student\nResearch Workshop (SRW), and the SRW mentor\nJunjie Hu for their insightful comments. We also\nthank the members of Tohoku NLP Laboratory\nfor helpful comments. This work was supported\nby JSPS KAKENHI Grant Number JP19H04162.\nThis work was also partially supported by a Bilat-\neral Joint Research Program between RIKEN AIP\nCenter and Tohoku University.\nReferences\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hin-\nton. 2016. Layer Normalization. arXiv preprint\narXiv:1607.06450.\nGino Brunner, Yang Liu, Dami ´an Pascual, Oliver\nRichter, Massimiliano Ciaramita, and Roger Watten-\nhofer. 2020. On Identiﬁability in Transformers. In\n8th International Conference on Learning Represen-\ntations (ICLR).\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D Manning. 2019. What Does BERT\nLook At? An Analysis of BERT’s Attention. InPro-\nceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for\nNLP, pages 276–286.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies (NAACL-HLT), pages 4171–4186.\nShuoyang Ding, Hainan Xu, and Philipp Koehn. 2019.\nSaliency-driven Word Alignment Interpretation for\nNeural Machine Translation. In Proceedings of\nthe 4th Conference on Machine Translation (WMT),\npages 1–12.\nChris Dyer, Victor Chahuneau, and Noah A Smith.\n2013. A Simple, Fast, and Effective Reparameter-\nization of IBM Model 2. In Proceedings of the 2013\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies (NACCL-HLT), pages 644–\n648.\nYoav Goldberg. 2019. Assessing BERT’s Syntactic\nAbilities. arXiv preprint arXiv:1901.05287.\nPhu Mon Htut, Jason Phang, Shikha Bordia, and\nSamuel R. Bowman. 2019. Do Attention Heads\nin BERT Track Syntactic Dependencies? arXiv\npreprint arXiv:1911.12246.\nSarthak Jain and Byron C Wallace. 2019. Atten-\ntion is not Explanation. In Proceedings of the\n2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies (NAACL-HLT) , pages\n3543–3556.\nGanesh Jawahar, Beno ˆıt Sagot, and Djam ´e Seddah.\n2019. What Does BERT Learn about the Structure\nof Language? In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics (ACL), pages 3651–3657.\nOlga Kovaleva, Alexey Romanov, Anna Rogers, and\nAnna Rumshisky. 2019. Revealing the Dark Secrets\nof BERT. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n4364–4373.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A Lite BERT for Self-supervised\nLearning of Language Representations. In 8th Inter-\nnational Conference on Learning Representations\n(ICLR).\nXintong Li, Guanlin Li, Lemao Liu, Max Meng, and\nShuming Shi. 2019. On the Word Alignment from\nNeural Machine Translation. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics (ACL), pages 1293–1303.\nYongjie Lin, Yi Chern Tan, and Robert Frank. 2019.\nOpen Sesame: Getting Inside BERT’s Linguis-\ntic Knowledge. Proceedings of the 2019 ACL\nWorkshop BlackboxNLP: Analyzing and Interpret-\ning Neural Networks for NLP, pages 241–253.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach. arXiv preprint arXiv:1907.11692.\nDavid Mare ˇcek and Rudolf Rosa. 2019. From\nBalustrades to Pierre Vinken: Looking for Syntax in\nTransformer Self-Attentions. In Proceedings of the\n2019 ACL Workshop BlackboxNLP: Analyzing and\nInterpreting Neural Networks for NLP , pages 263–\n275.\nPaul Michel, Omer Levy, and Graham Neubig. 2019.\nAre Sixteen Heads Really Better than One? In Ad-\nvances in Neural Information Processing Systems 32\n(NIPS), pages 14014–14024.\nFranz Josef Och and Hermann Ney. 2000. Improved\nStatistical Alignment Models. In Proceedings of the\n38th Annual Meeting of the Association for Compu-\ntational Linguistics (ACL), pages 440–447.\nFranz Josef Och and Hermann Ney. 2003. A System-\natic Comparison of Various Statistical Alignment\nModels. Computational Linguistics, 29(1):19–51.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A Fast, Extensible\nToolkit for Sequence Modeling. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics\n(Demonstrations), pages 48–53.\nDamian Pascual, Gino Brunner, and Roger Watten-\nhofer. 2020. Telling BERT’s full story: from Lo-\ncal Attention to Global Aggregation. arXiv preprint\narXiv:2004.05916.\nDanish Pruthi, Mansi Gupta, Bhuwan Dhingra, Gra-\nham Neubig, and Zachary C Lipton. 2020. Learning\nto Deceive with Attention-Based Explanations. In\nProceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics (ACL).\nAlessandro Raganato and J ¨org Tiedemann. 2018.\nAn Analysis of Encoder Representations in\nTransformer-Based Machine Translation. In\nProceedings of the 2018 EMNLP Workshop\nBlackboxNLP: Analyzing and Interpreting Neural\nNetworks for NLP, pages 287–297.\nEmily Reif, Ann Yuan, Martin Wattenberg, Fernanda B\nViegas, Andy Coenen, Adam Pearce, and Been Kim.\n2019. Visualizing and Measuring the Geometry of\nBERT. Advances in Neural Information Processing\nSystems 32 (NIPS), pages 8594–8603.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A Primer in BERTology: What we\nknow about how BERT works. arXiv preprint\narXiv:2002.12327.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural Machine Translation of Rare Words\nwith Subword Units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (ACL), pages 1715–1725.\nSoﬁa Serrano and Noah A Smith. 2019. Is Attention\nInterpretable? In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics (ACL), pages 2931–2951.\nGongbo Tang, Rico Sennrich, and Joakim Nivre. 2018.\nAn Analysis of Attention Mechanisms: The Case\nof Word Sense Disambiguation in Neural Machine\nTranslation. In Proceedings of the 3rd Conference\non Machine Translation (WMT): Research Papers ,\npages 26–35.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R Thomas McCoy, Najoung Kim,\nBenjamin Van Durme, Samuel R Bowman, Dipan-\njan Das, and Ellie Pavlick. 2019. What do you\nlearn from context? Probing for sentence structure\nin contextualized word representations. In 7th Inter-\nnational Conference on Learning Representations\n(ICLR).\nShikhar Vashishth, Shyam Upadhyay, Gaurav Singh\nTomar, and Manaal Faruqui. 2019. Attention In-\nterpretability Across NLP Tasks. arXiv preprint\narXiv:1909.11218.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is All\nyou Need. In Advances in Neural Information Pro-\ncessing Systems 30 (NIPS), pages 5998–6008.\nDavid Vilar, Maja Popovi ´c, and Hermann Ney. 2006.\nAER: Do we need to “improve” our alignments?\nIn International Workshop on Spoken Language\nTranslation (IWSLT) 2006, pages 205–212.\nEric Wallace, Yizhong Wang, Sujian Li, Sameer Singh,\nand Matt Gardner. 2019. Do NLP Models Know\nNumbers? Probing Numeracy in Embeddings. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 5307–\n5315.\nSarah Wiegreffe and Yuval Pinter. 2019. Attention\nis not not Explanation. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 11–20.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le.\n2019. XLNet: Generalized Autoregressive Pretrain-\ning for Language Understanding. In Advances in\nNeural Information Processing Systems 32 (NIPS) ,\npages 1–18.\nThomas Zenkel, Joern Wuebker, and John DeNero.\n2019. Adding Interpretable Attention to Neu-\nral Translation Models Improves Word Alignment.\narXiv preprint arXiv:1901.11359.\nA Multi-head attention and the\nnorm-based analysis\nOur norm-based analysis is applicable to the anal-\nysis of the multi-head attention mechanism imple-\nmented in Transformers. The i-th output of the\nmulti-head attention mechanism yintegrated\ni is cal-\nculated as follows:\nyintegrated\ni =\n∑\nh\nyh\ni (7)\nyh\ni =\nn∑\nj=1\nαh\ni,jfh(xj) (8)\nfh(x) :=\n(\nxWV,h + bV,h\n)\nWO,h, (9)\nwhere αh\ni,j, WV,h, bV,h, and WO,h are the same\nas αi,j, WV , bV , and WO in Equations 3 and\n4 for each head h, respectively. n is the number\nof tokens in the input vectors. Equation 7 can be\nrewritten as follows:\nyintegrated\ni =\nn∑\nj=1\n∑\nh αh\ni,j fh(xj) (10)\nAs shown in Equation 10, the multi-head atten-\ntion mechanism is also linearly decomposable,\nand one can analyze the ﬂaw of the information\nfrom the j-th vector to the i-th vector by mea-\nsuring ∥∑\nh αh\ni,jfh(xj)∥. In Section 5, we actu-\nally used ∥∑\nh αh\ni,jfh(xj)∥to extract the align-\nment from each layer’s multi-head attention.\nThe output of the multi-head attention mecha-\nnism is calculated via the sum of the outputs of all\nthe heads and a bias bO ∈Rd. Because adding a\nﬁxed vector is irrelevant to the token-to-token in-\nteraction that we aim to investigate, we omittedbO\nin our analysis.\nB The source of the dispersion of ∥f(x)∥\nAs described in Section 4.1, ∥f(x)∥exhibits dis-\npersion; however, it remains unclear whether this\ndispersion is attributed to ∥x∥or f. Hence, we\nchecked the dispersion of ∥x∥and the scaling ef-\nfects of the transformation f.\nDispersion of ∥x∥: First, we checked the coefﬁ-\ncient of variation (CV) of ∥x∥. Table 4 shows that\nthe average CV is 0.12, which is less than that of\n∥f(x)∥(0.22). The value of ∥x∥typically varies\nbetween 0.88 and 1.12 times the average value of\n∥x∥. The layer normalization (Ba et al., 2016)\nthat applied at the end of the previous layer should\nhave a large impact on the variance of ∥x∥.\nScaling effects of f: Second, we investigated\nthe scaling effect of the transformation f on the\nnorm of the input. Because the afﬁne transfor-\nmation f: Rd →Rd can be considered a linear\ntransformation Rd+1 →Rd+1 (Appendix C), the\nsingular values of f can be regarded as its scal-\ning effect. Figure 9 shows the singular values of\nf in randomly selected heads in BERT. The singu-\nlar values are displayed in descending order from\nleft to right. In each head, there is a difference of\nat least 1.8 times between the maximum and mini-\nmum singular values. This difference is larger than\nthat of ∥x∥, where ∥x∥typically varies between\n0.88 and 1.12 times the average value. These re-\nsults suggest that the dispersion of ∥f(x)∥is pri-\nmarily attributed to the scaling effect of f.\nC Afﬁne transformation as linear\ntransformation\nThe afﬁne transformation f: Rd →Rd in Equa-\ntion 4 can be viewed as a linear transformation\n˜f: Rd+1 →Rd+1. Given ˜x :=\n[\nx 1\n]\n∈\nRd+1, where 1 is concatenated to the end of each\ninput vector x ∈Rd, the afﬁne transformation f\ncan be viewed as:\n˜f(˜x) =˜x˜W\nV ˜W\nO\n(11)\n˜W\nV\n:=\n\n\n0\nWV ...\n0\nbV 1\n\n∈R(d+1)×(d′+1)\n(12)\n˜W\nO\n:=\n\n\n0\nWO ...\n0\n0 ... 0 1\n\n∈R(d′+1)×(d+1).\n(13)\nD Details on Sections 4.2 and 4.3\nWe describe the detailed experimental setup pre-\nsented in Sections 4.2 and 4.3.\nD.1 Notations\nThe dataset consists of several sequences; Data =\n(s1,··· ,s|Data|). Each sequence consists of sev-\nLayer µ σ CV Max Min\n12 (max CV) 20.49 4.62 0.23 32.84 4.13\n7 (min CV) 21.64 1.40 0.06 23.03 11.87\nAverage 19.93 2.39 0.12 - -\nTable 4: Mean (μ), standard deviation (σ), coefﬁcient\nof variance (CV), and maximum and minimum values\nof ∥x∥; the former three are averaged on all the layers.\nFigure 9: Singular values of f at randomly selected\nheads in each layer. We use ⟨layer⟩-⟨head number⟩to\ndenote a particular attention head. The singular values\nare\neral tokens, sp = (tp\n1,··· ,tp\n|sp|), where tp\nq is the\nq-th token in the p-th sequence. For simplicity, we\ndeﬁne the following functions:\nWeight(p,q,ℓ,h ) = 1\n|sp|\n|sp|∑\ni=1\nαℓ,h\np,i,q\nNorm(p,q,ℓ,h ) =∥fℓ,h(xℓ\np,q)∥\nWNorm(p,q,ℓ,h ) = 1\n|sp|\n|sp|∑\ni=1\n∥αℓ,h\np,i,qfℓ,h(xℓ\np,q)∥,\nwhere αℓ,h\np,i,q is the attention weight assigned from\nthe i-th pre-update vector to the q-th input vector\nin the p-th sequence. hand ℓdenote that the score\nis obtained from the h-th head of the ℓ-th layer.\nxℓ\np,q denotes the input vector for token tp\nq in the ℓ-\nth layer. fℓ,h(xℓ\np,q) is the transformed vector for\nxℓ\np,q in the h-th head of the ℓ-th layer.\nNext, the vocabulary Vof BERT is divided into\nthe following four categories:\nA= {[CLS]}\nB = {[SEP]}\nC = {“.”,“,”}\nD= V\\ (A∪B∪C). (14)\nLet T(p,Z) be a function that returns all tokens\ntp\nq belonging to the category Z in the p-th se-\nquence. To formally describe our experiments,\nseveral functions are deﬁned as follows. Note that\nwe analyzed a model with 12 heads in each layer.\nMeanN(Z,ℓ,h,p ) = 1\n|T(Z,p)|\n∑\ntp\nq ∈T(Z,p)\nNorm(p,q,ℓ,h )\nSumW(Z,ℓ,h,p ) =\n∑\ntp\nq ∈T(Z,p)\nWeight(p,q,ℓ,h )\nSumWN(Z,ℓ,h,p ) =\n∑\ntp\nq ∈T(Z,p)\nWNorm(p,q,ℓ,h )\nHeadN(Z,ℓ,h ) = 1\n|Data|\n∑\nsp∈Data\nMeanN(Z,ℓ,h,p )\nHeadW(Z,ℓ,h ) = 1\n|Data|\n∑\nsp∈Data\nSumW(Z,ℓ,h,p )\nHeadWN(Z,ℓ,h ) = 1\n|Data|\n∑\nsp∈Data\nSumWN(Z,ℓ,h,p )\nLayerW(Z,ℓ) = 1\n12\n12∑\nh=1\nHeadW(Z,ℓ,h )\nLayerWN(Z,ℓ) = 1\n12\n12∑\nh=1\nHeadWN(Z,ℓ,h ).\nThe LayerW (·) and LayerWN (·) functions are\nused to analyze the average behavior of the heads\nin a layer.\nD.2 Experimental setup for Section 4.2\nIn Figure 3, the results of each layer are reported\nfor each category. In Figures 3a and 3b, the val-\nues for each category Z were calculated using\nLayerW(Z,ℓ) and LayerWN(Z,ℓ), respectively.\nIn Figure 4, α and ∥f(x)∥ in the h-th\nhead of the ℓ-th layer were calculated us-\ning HeadW (Z,ℓ,h ) and HeadN (Z,ℓ,h ), respec-\ntively. The scores reported in Table 2 are\nthe Spearman rank correlation coefﬁcient r be-\ntween Weight (p,q,ℓ,h ) and WNorm (p,q,ℓ,h ).\nWe calculated the r using all the pairs of\nWeight(p,q,ℓ,h ) and WNorm (p,q,ℓ,h ) for the\npossible values of p, q, ℓ, and h. In Figure 5, each\nplot corresponds to the pair of Weight (p,q,ℓ,h )\nand WNorm(p,q,ℓ,h ), where the combination of\n(p,q,ℓ,h ) was randomly determined.\nD.3 Visualizations of αand ∥f(x)∥for each\nword category\nAs described in Section 4.2, α and ∥f(x)∥for\nthe [SEP] token were canceled out in almost all\nheads (Figure 4). Here, we show the trends for the\nother categories— B, C, and D in Equation 14.\nFigures 10, 11, and 12 show the trends of α and\n∥f(x)∥for category B (the [CLS] token), C (pe-\nriods and commas), and D(other tokens), respec-\n(a) α.\n (b) ∥f(x)∥.\nFigure 10: α and ∥f(x)∥corresponding to [CLS] to-\nken, averaged on all the input text.\n(a) α.\n (b) ∥f(x)∥.\nFigure 11: αand ∥f(x)∥corresponding to periods and\ncommas, averaged on all the input text.\ntively. The values in these ﬁgures were calculated\nas described in Appendix D.2. Figures 10 and 11\nshow that the trends for categories B and C were\nanalogous to those for the[SEP] token; the largeα\nwas canceled by the small ∥f(x)∥. However, the\ntrends for category Ddo not exhibit the trends of\nthe negative correlation betweenαand ∥f(x)∥. In\neach heatmap of ∥f(x)∥, the color scale is deter-\nmined by the maximum value of ∥f(x)∥in each\ncategory.\nWe also reported the relationship betweenαand\n∥f(x)∥in Section 4.2 (Figure 5). Figure 13 shows\nthe results for each word category to provide a\nclearer display of the results.\nD.4 Experimental setup and visualizations\nfor Section 4.3\nIn Section 4.3, we analyzed the relationship be-\ntween the word frequency and ∥f(x)∥. To for-\nmally describe our experiments, we further deﬁne\nthe functions as follows:\nAvgW(p,q) = 1\n12 ·12\n12∑\nℓ=1\n12∑\nh=1\nWeight(p,q,ℓ,h )\nAvgN(p,q) = 1\n12 ·12\n12∑\nℓ=1\n12∑\nh=1\nNorm(p,q,ℓ,h ).\nNote that we analyzed a model comprising 12\nlayers; each layer has 12 attention heads. Let\n(a) α.\n (b) ∥f(x)∥.\nFigure 12: α and ∥f(x)∥corresponding to other to-\nkens, averaged on all the input text.\n𝛼\n𝑓 𝒙\n(a) [CLS].\n𝛼\n𝑓 𝒙 (b) [SEP].\n𝛼\n𝑓 𝒙\n(c) Periods and commas.\n𝛼\n𝑓 𝒙 (d) Other tokens.\nFigure 13: Relationship between α and ∥f(x)∥for\neach category.\nr(·) be a function that returns the frequency rank\nof a given word. We ﬁrst calculated the Spear-\nman rank correlation coefﬁcient betweenr(tp\nq) and\nAvgW(p,q). The score was 0.06, which suggests\nthat there is no relationship betweenαand the fre-\nquency rank of the word. Then, we calculated\nthe Spearman rank correlation coefﬁcient between\nr(tp\nq) and AvgN(p,q). The score was 0.75, which\nsuggests a strong correlation between ∥f(x)∥and\nthe frequency rank of the word; Figure 14 shows\nthese results.\nIn addition, the results for the word frequency,\ninstead of the frequency rank, are shown in Fig-\nure 15. c(·) denotes a function that returns the fre-\nquency of a given word in the training dataset of\nBERT. We reproduced the dataset because it is not\nreleased.\nE Details on Section 5\nE.1 Hyperparameters and training settings\nWe used the Transformer (Vaswani et al., 2017)\nNMT model implemented in fairseq (Ott et al.,\n2019) for the experiments. Table 5 shows the hy-\nperparameters of the model, which were the same\n(a) Relationship between r(t) and AvgW.\n(b) Relationship between r(t) and AvgN.\nFigure 14: Relationship between frequency rank\nr(tp\nq) and AvgW (p,q), and that between r(tp\nq) and\nAvgN(p,q).\nas those used by Ding et al. (2019). We used the\nmodel with the highest BLEU score in the devel-\nopment set for our experiments.\nWe conducted the data preprocessing18 follow-\ning the method by Zenkel et al. (2019) and Ding\net al. (2019). All the words in the training data\nof the NMT systems were split into subword\nunits using byte-pair encoding (BPE, Sennrich\net al. (2016)) with 10k merge operations. Fol-\nlowing Ding et al. (2019), the last 1000 instances\nof the training data were used as the development\ndata.\nE.2 Settings of the word alignment extraction\nFirst, we applied BPE, which was used to split\nthe training data of the NMT systems to create\nthe evaluation data used for calculating the AER\nscores. Next, we extracted the scores of α and\n∥αf(x)∥for each subword in the evaluation data\nfor the force decoding setup. The gold align-\nments are annotated at the word-level, not the\nsubword-level. To calculate the word-level align-\nment scores, α and ∥αf(x)∥for the subwords\nwere merged along with the target token in the\ngold data by averaging, then merged along with\nthe source tokens in the gold data by summation.\nThese operations were the same as Li et al. (2019).\n18https://github.com/lilt/alignment-scripts\n(a) Relationship between c(t) and AvgW.\n(b) Relationship between c(t) and AvgN.\nFigure 15: Relationship between frequency count\nc(tp\nq) and AvgW (p,q), and that between c(tp\nq) and\nAvgN(p,q).\nIn existing studies, ⟨/s⟩, the special token for\nthe end of the sentence, was probably removed in\ncalculating word alignments. We included ⟨/s⟩as\nthe alignment targets and we considered the align-\nments to ⟨/s⟩as no alignment. In other words, if\nthe model aligns a certain word with ⟨/s⟩, we as-\nsume that the model decides that the word is not\naligned to any word.\nE.3 Layer-wise analysis\nWe preliminarily investigated how the source-\ntarget attentions in a Transformer-based NMT sys-\ntem behave depending on the layer. Tang et al.\n(2018) have reported that they behave differently\ndepending on the layer. The AER scores in the\nAWI and AWO settings were calculated for each\nlayer (Figure 16). In the AWO setting, AER scores\ntend to be better in the latter layers than in the\nearlier layers (Figure 16a). In contrast, the AER\nscores tend to be better in the earlier layers than in\nthe latter layers in the AWI setting (Figure 16b).\nThese results suggest that the earlier and lat-\nter layers focus on the source word that is aligned\nwith the input and output target word, respectively\n(as shown in Figure 6). Furthermore, we believe\nthat it is a convincing result to extract cleaner word\nalignments from the AWI setting than the AWO\nsetting (Figure 16), because the AWI setting is\nFairseq model\narchitecture transformer iwslt de en\nencoder embed dim. 512\ndecoder embed dim. 512\nencoder ffn embed dim. 1024\ndecoder ffn embed dim. 1024\nencoder attention heads 4\ndecoder attention heads 4\nencoder layers 6\ndecoder layers 6\nActivation function Relu\nLoss type label smoothed cross entropy\nlabel smoothing 0.1\nOptimizer\nalgorithm Adam\nlearning rates 0.001\nβ1 0.9\nβ2 0.98\nweight decay 0.0\nclip norm 0.0\nLearning rate scheduler\ntype inverse sqrt\nwarmup updates 4,000\nwarmup init lrarning rate 1e-07\nTraining\nbatch size 80\nmax tokens 4000\nmax epoch 100\nupdate freq 8\ndrop out 0.1\nseed 2\nnumber of GPUs used 2\nTable 5: Hyperparameters of the NMT model.\nLayer\nWeight-based\nNorm-based\n(a) AWO setting.\nLayer\nWeight-based\nNorm-based\n(b) AWI setting.\nFigure 16: Layer-wise AER scores. Each value is the\naverage of ﬁve random seeds. The closer the extracted\nword alignment is to the reference, the lower the AER\nscore—the lighter the color.\nmore advantageous. The main advantage is that\nwhile the decoder may fail to predict the correct\noutput words, the input words are perfectly accu-\nrate owing to the teacher forcing.\nE.4 Alignments in different layers\nFigures 17 to 22 show additional examples of the\nextracted alignments from the different layers of\nthe NMT system. Note that the color scale in each\nheatmap is determined by the maximum value in\neach ﬁgure. One can observe that while the atten-\ntion weights αare biased towards ⟨/s⟩, the norms\n∥αf(x)∥corresponding to the token are small.\nF Word alignment experiments on\ndifferent settings\nTo verify whether the results obtained in the\nSection 5 are reproducible in different settings,\nwe conducted an additional experiment using the\nmodel with a different number of attention heads.\nSpeciﬁcally, we used a model with eight atten-\ntion heads in both the encoder and decoder. Ta-\nble 6 shows the AER scores of the 8-head model.\nAs with the results obtained by the 4-head model,\nword alignments extracted using the proposed\nnorm-based approach were more reasonable than\nthose extracted using the weight-based approach,\nand better word alignments are extracted in the\nAWI setting than in the AWO setting. Further-\nmore, the alignments extracted using the head or\nthe layer with the highest average ∥αf(x)∥in the\nAWI setting are competitive with one of the exist-\ning word aligners—fast align. With respect to the\nweight-based extraction, the scores obtained using\n(a) Reference.\n (b) Attention-weights.\n (c) Vector-norms (ours).\nFigure 17: Examples of the reference alignment and the extracted patterns by each method in layer 1. Word pairs\nwith a green frame shows the word with the highest weight or norm. The vertical axis represents the input source\nword in the decoder, and the pairs with a green frame are extracted as alignments in the AWI setting. Note that\npairs that contain ⟨/s⟩not extracted.\n(a) Attention-weights.\n (b) Vector-norms.\nFigure 18: Examples of the reference alignment and\nthe extracted patterns by each method in layer 2.\n(a) Attention-weights.\n (b) Vector-norms.\nFigure 19: Examples of the reference alignment and\nthe extracted patterns by each method in layer 3.\nthe 8-head model were worse than those obtained\nusing the 4-head model. This may be owing to the\nincrease in the number of heads that do not capture\nreasonable alignments.\nFigures 23a and 23b show the AER scores of\nthe alignments obtained by the norm-based extrac-\ntion at each head on one out of ﬁve seeds. Fig-\nure 23c shows the average of ∥αf(x)∥at each\nhead. As with the results obtained by the 4-head\nmodel, the heads with the low (i.e., better) AER\nscore in the AWI setting tended to have the high\n∥αf(x)∥(the Spearman rank and Pearson correla-\n(a) Attention-weights.\n (b) Vector-norms.\nFigure 20: Examples of the reference alignment and\nthe extracted patterns by each method in layer 4.\n(a) Attention-weights.\n (b) Vector-norms.\nFigure 21: Examples of the reference alignment and\nthe extracted patterns by each method in layer 5.\ntion coefﬁcients between the AER scores and av-\neraged ∥αf(x)∥among the 6×8 heads are −0.26\nand −0.50). In contrast, in the AWO setting, such\na negative correlation is not observed; rather, a\npositive correlation is observed (the Spearman’sρ\nis 0.40 and the Pearson’sris 0.40).\nAdditionally, following Appendix E.3, the AER\nscores for both the AWI and AWO settings for\neach layer were calculated (Figure 24). As with\nthe 4-head model (Appendix E.3), the latter layers\ncorrespond to the AWO setting and the earlier lay-\ners corresponds to the AWI setting in the 8-head\n(a) Attention-weights.\n (b) Vector-norms.\nFigure 22: Examples of the reference alignment and\nthe extracted patterns by each method in layer 6.\nMethods AER ±SD\nTransformer – Attention-based Approach\n— Alignment with output setting —\nWeight-based\nlayer mean 70.4 0.6\nbest layer (layer 4 or 5) 49.3 1.2\nNorm-based (ours)\nlayer mean 63.2 0,7\nbest layer (layer 5) 43.4 0.8\nhead with the highest average ∥αf(x)∥ 87.2 0.6\nlayer with the highest average ∥αf(x)∥ 83.7 2.2\n— Alignment with input setting —\nWeight-based\nlayer mean 76.6 1.7\nbest layer (layer 2 or 3) 38.7 8.9\nNorm-based (ours)\nlayer mean 59.9 1.0\nbest layer (layer 2 or 3) 26.3 1.9\nhead with the highest average ∥αf(x)∥ 24.9 1.7\nlayer with the highest average ∥αf(x)∥ 26.5 1.9\nWord Aligner\nfast align from Zenkel et al. (2019) 28.4 -\nGIZA++ from Zenkel et al. (2019) 21.0 -\nTable 6: Results on a model trained with the same set-\ntings as described in Appendix E.1 except that the num-\nber of attention heads in the encoder and decoder is 8.\nEach value is the average of ﬁve random seeds.\nmodel.\nG Comparison with effective attention\n(Brunner et al., 2020)\nIn this section, we discuss the difference between\nour approach and “effective attention” (Brunner\net al., 2020), which is an enhanced version of the\nweight-based analysis. The effective attention ex-\nclude the components that do not affect the output\nowing to the application of transformation f and\ninput x from the attention weight matrix A. The\noutput-irrelevant components are derived from the\nnull space of the matrix T, which is the stack of\nf(x). Figure 25a shows the Pearson correlation\ncoefﬁcient between the raw attention weight and\nthe effective attention. Since the dimension of the\nnull space of the matrixT depends on the length of\n(a) AER in the AWO setting.\n(b) AER in the AWI setting.\n(c) Averaged ∥αf(x)∥.\nFigure 23: AER scores and averaged∥αf(x)∥for each\nhead in a model with 8 heads.\nthe input sequence, as shown in Figure 25a, the ef-\nfective attention and raw attention weight are iden-\ntical for short input sequences. Figure 25b shows\nthe Pearson correlation coefﬁcient between the\nraw attention weight and our norm-based method.\nSince we incorporate the scaling effects of f and\nx, which contain canceling, our proposed method\n∥αf(x)∥differs from the raw attention weight,\nwhether the input sequence is long or short.\nLayer\nWeight-based\nNorm-based\n(a) AWO setting.\nLayer\nWeight-based\nNorm-based\n(b) AWI setting.\nFigure 24: Layer-wise AER scores. Each value is the\naverage of ﬁve random seeds. The closer the extracted\nword alignment is to the reference, the lower the AER\nscore—the lighter the color.\n(a) Effective attention.\n(b) ∥αf(x)∥.\nFigure 25: Each point represents the Pearson correla-\ntion coefﬁcient of raw attention and each method to-\nward token length.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7663831114768982
    },
    {
      "name": "Parallels",
      "score": 0.6671093106269836
    },
    {
      "name": "Computer science",
      "score": 0.564333438873291
    },
    {
      "name": "Norm (philosophy)",
      "score": 0.4628097116947174
    },
    {
      "name": "Artificial intelligence",
      "score": 0.42574745416641235
    },
    {
      "name": "Machine translation",
      "score": 0.42026054859161377
    },
    {
      "name": "Linguistics",
      "score": 0.3702601194381714
    },
    {
      "name": "Arithmetic",
      "score": 0.3330395817756653
    },
    {
      "name": "Natural language processing",
      "score": 0.32885709404945374
    },
    {
      "name": "Mathematics",
      "score": 0.26471146941185
    },
    {
      "name": "Engineering",
      "score": 0.14184129238128662
    },
    {
      "name": "Epistemology",
      "score": 0.12593847513198853
    },
    {
      "name": "Electrical engineering",
      "score": 0.11289650201797485
    },
    {
      "name": "Voltage",
      "score": 0.1071712076663971
    },
    {
      "name": "Operations management",
      "score": 0.07822951674461365
    },
    {
      "name": "Philosophy",
      "score": 0.07438313961029053
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I201537933",
      "name": "Tohoku University",
      "country": "JP"
    }
  ],
  "cited_by": 17
}