{
    "title": "Utilizing large language models in breast cancer management: systematic review",
    "url": "https://openalex.org/W4392938433",
    "year": 2024,
    "authors": [
        {
            "id": null,
            "name": "Sorin, Vera",
            "affiliations": [
                "Sheba Medical Center",
                "Tel Aviv University"
            ]
        },
        {
            "id": "https://openalex.org/A4226871697",
            "name": "Glicksberg, Benjamin S.",
            "affiliations": [
                "Icahn School of Medicine at Mount Sinai"
            ]
        },
        {
            "id": null,
            "name": "Artsi, Yaara",
            "affiliations": [
                "Bar-Ilan University"
            ]
        },
        {
            "id": null,
            "name": "Barash, Yiftach",
            "affiliations": [
                "Sheba Medical Center",
                "Tel Aviv University"
            ]
        },
        {
            "id": "https://openalex.org/A4288562000",
            "name": "Konen, Eli",
            "affiliations": [
                "Sheba Medical Center",
                "Tel Aviv University"
            ]
        },
        {
            "id": "https://openalex.org/A4287590126",
            "name": "Nadkarni, Girish N.",
            "affiliations": [
                "Icahn School of Medicine at Mount Sinai"
            ]
        },
        {
            "id": "https://openalex.org/A2751827201",
            "name": "Klang Eyal",
            "affiliations": [
                "Icahn School of Medicine at Mount Sinai"
            ]
        },
        {
            "id": null,
            "name": "Sorin, Vera",
            "affiliations": [
                "Sheba Medical Center"
            ]
        },
        {
            "id": "https://openalex.org/A4226871697",
            "name": "Glicksberg, Benjamin S.",
            "affiliations": [
                "Icahn School of Medicine at Mount Sinai"
            ]
        },
        {
            "id": null,
            "name": "Artsi, Yaara",
            "affiliations": [
                "Bar-Ilan University"
            ]
        },
        {
            "id": null,
            "name": "Barash, Yiftach",
            "affiliations": [
                "Sheba Medical Center"
            ]
        },
        {
            "id": "https://openalex.org/A4288562000",
            "name": "Konen, Eli",
            "affiliations": [
                "Sheba Medical Center"
            ]
        },
        {
            "id": "https://openalex.org/A4287590126",
            "name": "Nadkarni, Girish N.",
            "affiliations": [
                "Icahn School of Medicine at Mount Sinai"
            ]
        },
        {
            "id": "https://openalex.org/A2751827201",
            "name": "Klang Eyal",
            "affiliations": [
                "Icahn School of Medicine at Mount Sinai"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3010446820",
        "https://openalex.org/W4386932783",
        "https://openalex.org/W4387440167",
        "https://openalex.org/W4387673783",
        "https://openalex.org/W4362521774",
        "https://openalex.org/W4379769651",
        "https://openalex.org/W4386302153",
        "https://openalex.org/W2105213027",
        "https://openalex.org/W4361289889",
        "https://openalex.org/W4384558920",
        "https://openalex.org/W2005501262",
        "https://openalex.org/W4381480701",
        "https://openalex.org/W4327946446",
        "https://openalex.org/W4385620388",
        "https://openalex.org/W2911188335",
        "https://openalex.org/W3201019271",
        "https://openalex.org/W4379537569",
        "https://openalex.org/W3109948322",
        "https://openalex.org/W3004088204",
        "https://openalex.org/W4378783467",
        "https://openalex.org/W4376122030",
        "https://openalex.org/W4386565997",
        "https://openalex.org/W4386477610",
        "https://openalex.org/W2107638293"
    ],
    "abstract": "Abstract Purpose Despite advanced technologies in breast cancer management, challenges remain in efficiently interpreting vast clinical data for patient-specific insights. We reviewed the literature on how large language models (LLMs) such as ChatGPT might offer solutions in this field. Methods We searched MEDLINE for relevant studies published before December 22, 2023. Keywords included: “large language models”, “LLM”, “GPT”, “ChatGPT”, “OpenAI”, and “breast”. The risk bias was evaluated using the QUADAS-2 tool. Results Six studies evaluating either ChatGPT-3.5 or GPT-4, met our inclusion criteria. They explored clinical notes analysis, guideline-based question-answering, and patient management recommendations. Accuracy varied between studies, ranging from 50 to 98%. Higher accuracy was seen in structured tasks like information retrieval. Half of the studies used real patient data, adding practical clinical value. Challenges included inconsistent accuracy, dependency on the way questions are posed (prompt-dependency), and in some cases, missing critical clinical information. Conclusion LLMs hold potential in breast cancer care, especially in textual information extraction and guideline-driven clinical question-answering. Yet, their inconsistent accuracy underscores the need for careful validation of these models, and the importance of ongoing supervision.",
    "full_text": "Vol.:(0123456789)\nJournal of Cancer Research and Clinical Oncology (2024) 150:140 \nhttps://doi.org/10.1007/s00432-024-05678-6\nREVIEW\nUtilizing large language models in breast cancer management: \nsystematic review\nVera Sorin1,2 · Benjamin S. Glicksberg3 · Yaara Artsi4 · Yiftach Barash1,2 · Eli Konen1 · Girish N. Nadkarni3,5 · \nEyal Klang3,5\nReceived: 30 December 2023 / Accepted: 1 March 2024 / Published online: 19 March 2024 \n© The Author(s) 2024\nAbstract\nPurpose Despite advanced technologies in breast cancer management, challenges remain in efficiently interpreting vast \nclinical data for patient-specific insights. We reviewed the literature on how large language models (LLMs) such as ChatGPT \nmight offer solutions in this field.\nMethods We searched MEDLINE for relevant studies published before December 22, 2023. Keywords included: “large lan-\nguage models”, “LLM”, “GPT”, “ChatGPT”, “OpenAI”, and “breast”. The risk bias was evaluated using the QUADAS-2 tool.\nResults Six studies evaluating either ChatGPT-3.5 or GPT-4, met our inclusion criteria. They explored clinical notes analysis, \nguideline-based question-answering, and patient management recommendations. Accuracy varied between studies, ranging \nfrom 50 to 98%. Higher accuracy was seen in structured tasks like information retrieval. Half of the studies used real patient \ndata, adding practical clinical value. Challenges included inconsistent accuracy, dependency on the way questions are posed \n(prompt-dependency), and in some cases, missing critical clinical information.\nConclusion LLMs hold potential in breast cancer care, especially in textual information extraction and guideline-driven \nclinical question-answering. Yet, their inconsistent accuracy underscores the need for careful validation of these models, \nand the importance of ongoing supervision.\nKeywords Large language models · GPT · Breast cancer · Artificial intelligence\nIntroduction\nNatural language processing (NLP) is increasingly used in \nhealthcare, especially in oncology, for its ability to analyze \nfree-text with diverse applications (Sorin et al. 2020a, b). \nLarge language models (LLMs) like GPT, LLaMA, PaLM, \nand Falcon represent the pinnacle of this development, lev-\neraging billions of parameters for highly accurate text pro-\ncessing (Sorin et al. 2020a, b; Bubeck et al. 2023). Despite \nthis, the integration of such sophisticated NLP algorithms \ninto practical healthcare settings, particularly in managing \ncomplex diseases like breast cancer, remains a technological, \noperational, and ethical challenge.\nBreast cancer, the most common cancer among women, \ncontinues to pose significant challenges in terms of morbid-\nity, mortality, and information overload (Kuhl et al. 2010; \nSiegel et al. 2019). While LLMs have shown promise in \nmedical text analysis—with GPT-4 achieving a notable 87% \nsuccess rate on the USMLE (Brin et al. 2023; Chaudhry \net al. 2017) and extending its capabilities to image analy -\nsis (Sorin et al. 2023a, b, c)—their practical application in \nmedicine and oncology in particular is still evolving.\nWe reviewed the literature on how large language models \n(LLMs) might offer solutions in breast cancer care.\n * Vera Sorin \n verasrn@gmail.com\n1 Department of Diagnostic Imaging, Chaim Sheba Medical \nCenter, Affiliated to the Sackler School of Medicine, Tel-\nAviv University, Emek Haela St. 1, 52621 Ramat Gan, Israel\n2 DeepVision Lab, Chaim Sheba Medical Center, \nTel Hashomer, Israel\n3 Division of Data-Driven and Digital Medicine (D3M), Icahn \nSchool of Medicine at Mount Sinai, New York, NY, USA\n4 Azrieli Faculty of Medicine, Bar-Ilan University, Zefat, Israel\n5 The Charles Bronfman Institute of Personalized Medicine, \nIcahn School of Medicine at Mount Sinai, New York, NY, \nUSA\n Journal of Cancer Research and Clinical Oncology (2024) 150:140\n140 Page 2 of 8\nMethods\nThis review was conducted according to the Preferred \nReporting Items for Systematic Reviews and Meta-Analyses \n(PRISMA) guidelines (Moher 2009). We searched the litera-\nture for applications of LLMs in breast cancer management \nusing MEDLINE.\nThe search included studies published up to December \n22nd 2023. Our search query was “((\"large language mod-\nels\") OR (llm) OR (gpt) OR (chatgpt) OR (openAI)) AND \n(breast)”. The initial search identified 97 studies. To ensure \nthoroughness, we also examined the reference lists of the \nrelevant studies. This, however, did not lead to additional \nrelevant studies that met our inclusion criteria.\nThe criteria for inclusion were English language full-\nlength publications that specifically evaluated the role of \nLLMs in breast cancer management. We excluded papers \nthat addressed other general applications of LLMs in health-\ncare or oncology without a specific focus on breast cancer.\nThree reviewers (VS, YA, and EKL) independently \nconducted the search, screened the titles, and reviewed the \nabstracts of the articles identified in the search. One dis -\ncrepancy in the search results was discussed and resolved to \nachieve a consensus. Next, the reviewers assessed the full \ntext of the relevant papers. In total, six publications met our \ninclusion criteria and were incorporated into this review. \nWe summarized the results of the included studies, detailing \nthe specific LLMs used, the utilized tasks, number of cases, \nalong with publication details in a table format. Figure  1 \nprovides a flowchart detailing the screening and inclusion \nprocedure.\nQuality was assessed by the Quality Assessment of Diag-\nnostic Accuracy Studies (QUADAS-2) criteria (Whiting \n2011).\nFig. 1  Flow Diagram of the \nInclusion Process. Flow dia-\ngram of the search and inclusion \nprocess based on the Preferred \nReporting Items for Systematic \nReviews and Meta-Analyses \n(PRISMA) guidelines Records identified from*: \nDatabases (n = 97) \nRegisters (n = 0) \nRecords removed before \nscreening: \nDuplicate records removed  \n(n = 0) \nRecords marked as ineligible \nby automation tools (n = 0) \nRecords removed for other \nreasons (n = 0) \nRecords screened \n(n = 97) \nRecords excluded** \n(n = 78) \nReports sought for retrieval \n(n = 19) \nReports not retrieved \n(n = 0) \nReports assessed for eligibility \n(n = 19) \nReports excluded: \n-  Articles that evaluated LLMs in \ntext analysis related to breast \nplastic surgery (n = 8) \n-  Articles that did not evaluate \nLLMs (n = 4) \n-  Articles that did not directly \nevaluate LLMs in breast cancer \ncare (n=1)  \nStudies included in review \n(n = 6) \nReports of included studies \n(n = 6) \nIdentification of studies via databases and registers \nIdentification ScreeningIncluded \nJournal of Cancer Research and Clinical Oncology (2024) 150:140 \n Page 3 of 8 140\nResults\nAll six studies included were published in 2023 (Table  1). \nAll focused on either ChatGPT-3.5 or GPT-4 by OpenAI. \nApplications described include information extraction and \nquestion-answering. Three studies (50.0%) evaluated the \nperformance of ChatGPT on actual patient data (Sorin \net al. 2023a, b, c; Choi et al. 2023; Lukac et al. 2023), as \nopposed to two studies that used data from the internet \n(Rao et al. 2023; Haver et al. 2023). One study crafted \nfictitious patient profiles by the head investigator (Griew -\ning et al. 2023).\nRao et al. and Haver et al. evaluated LLMs for breast \nimaging recommendations (Rao et al. 2023; Haver et al. \n2023). Sorin et al., Lukac et al. and Griewing et al. evalu-\nated LLMs as supportive decision-making tools in multi-\ndisciplinary tumor boards (Sorin et al. 2023a, b, c; Lukac \net al. 2023; Griewing et al. 2023). Choi et al. used LLM \nfor information extraction from ultrasound and pathology \nreports (Choi et al. 2023) (Fig. 2). Example cases for appli-\ncations from studies are detailed in Table 2.\nAccuracy of LLMs on different applications ranged \nfrom 50 to 98%. Best performance rates were achieved for \ninformation extraction and question-answering, with cor -\nrect responses ranging from 88 to 98% (Choi et al. 2023; \nTable 1  Studies evaluating LLMs for breast cancer diagnosis and care\nStudy ref Publication Date Title Journal\nSorin et al. 2023 05.2023 Large language model (ChatGPT) as a support tool for breast \ntumor board\nNPJ Breast Cancer\nRao et al. 2023 06.2023 Evaluating GPT as an Adjunct for Radiologic Decision Mak-\ning: GPT-4 Versus GPT-3.5 in a Breast Imaging Pilot\nJACR \nChoi et al. 2023 09.2023 Developing prompts from large language model for extracting \nclinical information from pathology and ultrasound reports \nin breast cancer\nRadiation Oncology Journal\nLukac et al. 2023 07.2023 Evaluating ChatGPT as an adjunct for the multidisciplinary \ntumor board decision-making in primary breast cancer cases\nArchives of Gynecology and Obstetrics\nHaver et al. 2023 04.2023 Appropriateness of Breast Cancer Prevention and Screening \nRecommendations Provided by ChatGPT\nRadiology\nGriewing et al. 2023 10.2023 Challenging ChatGPT 3.5 in Senology—An Assessment of \nConcordance with Breast Cancer Tumor Board Decision \nMaking\nJournal of Personalized Medicine\nFig. 2  Applications of large \nlanguage models in breast can-\ncer care and the corresponding \naccuracies achieved in various \ntasks in the different studies\n\n Journal of Cancer Research and Clinical Oncology (2024) 150:140\n140 Page 4 of 8\nTable 2  Example cases Study ref Application Example\nSorin et al. 2023 Breast tumor board clini-\ncal decision support\nUser:\nHi, can I give you a patient story of breast cancer \ndetected and you'll say what is the next step in her \nmanagement? please decide if she needs surgery, \nwhat type of surgery, or whether she needs neo-\nadjuvant therapy before, or does she need further \ntesting\nChatGPT-3.5:\nOf course! I'd be happy to help. Please go ahead and \ngive me the patient's story\nUser:\nHere is the story: 51 years old woman, no family hx, \nno hormonal meds. felt a mass in her left breast \nand did mammography and US. On US of her left \nbreast in the outer-upper quadrant a 2 cm mass \nwas detected. Then, a biopsy was performed to the \nmass and on pathology it was an IDC ER positive, \nPR negative, HER2 negative, KI high. Then she \ndid an MRI for local staging—a solitary mass in \nher left breast was detected. She also did PET-CT \nwith no evidence of distal metastases\nChatGPT-3.5:\nBased on the information provided, it appears that \nthis patient has been diagnosed with Invasive \nDuctal Carcinoma (IDC) in her left breast\nGiven the results of the pathology and imaging \ntests, the patient is most likely a candidate for \nsurgery, as the mass is located in only one breast \nand there is no evidence of distant metastases\nFor this type of breast cancer, the standard surgical \noptions are either a lumpectomy (also known as \na breast-conserving surgery) or a mastectomy \n(removal of the entire breast). In general, a \nlumpectomy is recommended for small, early-\nstage tumors, while a mastectomy may be recom-\nmended for larger tumors or for patients who have \na higher risk of cancer recurrence\nIn this case, given the size of the mass (2 cm), a \nlumpectomy with axillary lymph node dissection \n(removal of some of the lymph nodes in the under-\narm area to check for spread of cancer) would be \nthe typical surgical approach. If a lumpectomy \nis performed, radiation therapy would usually be \nrecommended to reduce the risk of recurrence\nIt's worth noting that the patient's ER (estrogen \nreceptor) positive, PR (progesterone receptor) \nnegative, and HER2 (human epidermal growth \nfactor receptor 2) negative status will play a role in \ndetermining her treatment options, as these recep-\ntors can be targeted with specific medications\nIn conclusion, given the patient's diagnosis and \ntest results, surgery, most likely a lumpectomy \nwith axillary lymph node dissection, is the next \nrecommended step in her management. After the \nsurgery, the patient may also require additional \ntreatment such as radiation therapy or systemic \ntherapy (such as hormone therapy or chemo-\ntherapy) based on the results of the surgery and \nfurther testing\nJournal of Cancer Research and Clinical Oncology (2024) 150:140 \n Page 5 of 8 140\nRao et al. 2023) (Table  3). The lower performance was for \nclinical decision support in breast tumor boards, ranging \nbetween 50 and 70% (Sorin et al. 2023a, b, c; Lukac et al. \n2023; Griewing et al. 2023). The range in performance on \nthis task was wide between studies. However, the methods of \nthe three studies also varied significantly (Sorin et al. 2023a, \nb, c; Lukac et al. 2023; Griewing et al. 2023). Sorin et al. \nand Lukac et al. used authentic patient data and compared \nChatGPT-3.5 to the retrospective decisions in breast tumor \nboard (Sorin et al. 2023a, b, c; Lukac et al. 2023). In both \nstudies, the authors used reviewers that scored ChatGPT-3.5 \nresponses (Sorin et al. 2023a, b, c; Lukac et al. 2023). \n(Griewing et al. 2023) crafted 20 fictitious patient files that \nwere then discussed by a multidisciplinary tumor board. \nTheir assessment was based on binary evaluation of various \ntreatment approaches, including surgery, endocrine, chemo-\ntherapy, and radiation therapy. Griewing et al. were the only \nstudy providing insights into LLM performance on genetic \ntesting for breast cancer treatment (Griewing et al. 2023). \nAll three studies analyzed concordance between the tumor \nboard and the LLM on different treatment options (Sorin \net al. 2023a, b, c; Lukac et al. 2023; Griewing et al. 2023).\nTable 2  (continued) Study ref Application Example\nRao et al. 2023 Question-answering based \non ACR recommenda-\ntions\nPrompt:\nFor variant \"Breast cancer screening. Average-risk \nwomen: women with < 15% lifetime risk of breast \ncancer.\", assess appropriateness of the following \nprocedures in a concise manner: Mammography \nscreening, Digital breast tomosynthesis screening, \nUS breast, MRI breast without and with IV con-\ntrast, MRI breast without IV contrast, FDG-PET \nbreast dedicated, Sestamibi MBI\nMammography screening and Digital Breast \nTomosynthesis screening are appropriate for \naverage-risk women as these are the current stand-\nard screening methods for breast cancer\nChatGPT:\nUltrasound breast screening is also appropriate \nfor average-risk women, as it is often used as an \nadjunctive test to mammography in women with \ndense breast tissue\nMagnetic Resonance Imaging (MRI) of the breast \nwithout and with IV contrast is generally not rec-\nommended for average-risk women as a screening \ntool. It may be considered for women with a high \nlifetime risk of breast cancer (usually defined as \n20% or greater)\nFluorodeoxyglucose (FDG) Positron Emission \nTomography (PET) breast dedicated, and Sesta-\nmibi breast (MBI) is not standardly recommended \nas a screening modality for average-risk women, \nusually reserved for women with high-risk or \nrecurrent breast cancer\nTable 3  Summarization of performance of LLMs at different breast cancer care related tasks\nStudy ref LLM No. of cases Actual \npatient \ndata\nApplication Correct performance\nSorin et al. 2023 ChatGPT (GPT-3.5) 10 Yes Tumor board clinical decision support 70%\nRao et al. 2023 GPT-4, GPT-3.5 14 No Question-answering based on ACR recommenda-\ntions\n88.9–98.4%\nChoi et al. 2023 ChatGPT (GPT-3.5) 340 Yes Information extraction 87.7–98.2%\nLukac et al. 2023 ChatGPT (GPT-3.5) 10 Yes Tumor board clinical decision support 64.20%\nHaver et al. 2023 ChatGPT (GPT-3.5) 25 No Question-answering on breast cancer prevention and \nscreening\n88%\nGriewing et al. 2023 ChatGPT (GPT-3.5) 20 No Concordance to tumor board clinical decisions 50–95%\n Journal of Cancer Research and Clinical Oncology (2024) 150:140\n140 Page 6 of 8\nAll studies discussed the limitations of LLMs in the con-\ntexts in which the algorithms were evaluated (Table 4). In all \nstudies some of the information the models generated was \nfalse. When used as a support tool for tumor board, in some \ninstances, the models overlooked relevant clinical details \n(Sorin et al. 2023a, b, c; Lukac et al. 2023; Griewing et al. \n2023). Sorin et al. noticed absolute lack of referral to imag-\ning (Sorin et al. 2023a, b, c), while Rao et al. who evaluated \nappropriateness of imaging noticed imaging overutilization \n(Rao et al. 2023). Some of the studies also discussed whether \nthe nature of the prompt affects the outputs (Choi et al. 2023; \nHaver et al. 2023), and the difficulty to verify the reliability \nof the answers (Lukac et al. 2023; Rao et al. 2023; Haver \net al. 2023).\nAccording to the QUADAS-2 tool, all papers but one \nscored as high risk of bias for index test interpretation. For \nthe paper by Lukac et al. the risk was unclear, refraining \nfrom a clear statement whether the evaluators were blinded \nto the reference standard. The study by Griewing et al. was \nthe only one identified to have a low risk of bias across all \ncategories (Griewing et al. 2023). The objective assessment \nof the risk of bias is reported in Supplementary Table 1.\nDiscussion\nWe reviewed the literature on LLMs applications related to \nbreast cancer management and care. Applications described \nincluded information extraction from clinical texts, question-\nanswering for patients and physicians, manuscript drafting \nand clinical management recommendations.\nA disparity in performance was seen. The models \nshowed proficiency in information extraction and respond-\ning to structured questions, with accuracy rates between \n88 and 98%. However, their effectiveness diminished down \nto 50–70% in making clinical decisions, underscoring a  \ngap in their application. In breast cancer care, attention to \ndetail is crucial. LLMs excel at processing medical infor -\nmation quickly. However, currently, they may be less adept \nat navigating complex treatment decisions. Breast cancer \ncases vary greatly, each case distinguished by a unique \nmolecular profile, clinical staging, and patient-specific \nrequirements. It is vital for LLMs to adapt to the indi-\nvidual patient. While these models can assist physicians \nin routine tasks, they require further development for per -\nsonalized treatment planning.\nInterestingly, half of the studies included real patients’ \ndata as opposed to publicly available data or fictitious data. \nFor the overall published literature on LLMs in health -\ncare, there are more publications evaluating performance \non public data. This includes performance on board exami-\nnations and question-answering based on guidelines (Sal-\nlam 2023). These analyses may introduce contamination \nof data, since LLMs were trained on vast data from the \ninternet. For commercial models such as ChatGPT, the \ntype of training data is not disclosed. Furthermore, these \napplications do not necessarily reflect on the performance \nof these models in real-world clinical settings.\nWhile some claim that LLMs may eventually replace \nhealthcare personnel, currently, there are major limitations \nand ethical concerns that strongly suggest otherwise (Lee \net al. 2023). Using such models to augment physicians’ \nperformance is more practical, albeit also constrained by \nethical issues (Shah et al. 2023). LLMs enable automating \ndifferent tasks that traditionally required human effort. The \nability to analyze, extract and generate meaningful textual \ninformation could potentially decrease some physicians’ \nworkload and human errors.\nReliance on LLMs and potential integration in medicine \nshould be made with caution. The limitations discussed \nin the studies further underscore this note. These models \ncan generate false information (termed “hallucination”) \nwhich can be seamlessly and confidently integrated into \nreal information (Sorin et al. 2020a, b). They can also per-\npetuate disparities in healthcare (Sorin et al. 2021; Kotek \net al. 2023). The inherent inability to trace the exact deci-\nsion-making process of these algorithms is a major chal-\nlenge for trust and clinical integration (Sorin et al. 2023a, \nb, c). LLMs can also be vulnerable to cyber-attacks (Sorin \net al. 2023a, b, c).\nTable 4  Limitations of LLMs as described in each study\nStudy ref LLM Limitations described\nSorin et al. 2023 ChatGPT (GPT-3.5) False answers and inaccurate medical recommendations, overlooked relevant clinical details, abso-\nlute lack of referral to imaging, potential for outdated information, potential for bias\nRao et al. 2023 GPT-4, GPT-3.5 False information, imaging overutilization, lack of source attribution\nChoi et al. 2023 ChatGPT (GPT-3.5) False information, lack of logical reasoning, incomplete information extraction, prompt sensitivity\nLukac et al. 2023 ChatGPT (GPT-3.5) False answers, overlooked relevant clinical details, potential for outdated information, lack of source \nattribution\nHaver et al. 2023 ChatGPT (GPT-3.5) False recommendations, prompt sensitivity, lack of source attribution\nGriewing et al. 2023 ChatGPT (GPT-3.5) Lack of Consistency in Health Data Use, treatment mistakes, prone to misinterpretation and hal-\nlucinations\nJournal of Cancer Research and Clinical Oncology (2024) 150:140 \n Page 7 of 8 140\nFurthermore, this study highlights the absence of uniform \nassessment methods for LLMs in healthcare, underlining the \nneed of establishing methodological standards for evaluating \nLLMs. The goal is to enhance the comparability and quality \nof research. The establishment of such standards is critical \nfor the safe and effective integration of LLMs into health-\ncare, especially for complex conditions like breast cancer, \nwhere personalized patient care is essential.\nThis review has several limitations. First, due to the heter-\nogeneity of tasks evaluated in the studies, we could not per-\nform a meta-analysis. Second, all included studies assessed \nChatGPT-3.5, and only one study evaluated GPT-4. There \nwere no publications identified on other available LLMs. \nFinally, generative AI is currently a rapidly expanding topic. \nThus, there may be manuscripts and applications published \nafter our review was performed. LLMs are continually being \nrefined, and so is their performance.\nTo conclude, LLMs hold potential for breast cancer man-\nagement, especially in text analysis and guideline-driven \nquestion-answering. Yet, their inconsistent accuracy war -\nrants cautious use, following thorough validation and ongo-\ning supervision.\nSupplementary Information The online version contains supplemen-\ntary material available at https:// doi. org/ 10. 1007/ s00432- 024- 05678-6.\nAuthors contributions All authors contributed to the study conception \nand design. Material preparation, data collection and analysis were \nperformed by Vera Sorin, Eyal Klang and Yaara Artsi. The first draft \nof the manuscript was written by Vera Sorin and all authors commented \non previous versions of the manuscript. All authors read and approved \nthe final manuscript.\nFunding The authors declare that no funds, grants, or other support \nwere received during the preparation of this manuscript.\nData availability Reviewed studies and their results can be located at \nPubMed database: https:// pubmed. ncbi. nlm. nih. gov/\nDeclarations \nCompeting interests The authors declare no competing interests.\nConflict of interest The authors declare that they have no conflict of \ninterest.\nEthical approval This is an observational study. No ethical approval \nwas required.\nHuman participants and/or animals This is a systematic review. No \nhuman and/or animals participated in this study.\nInformed consent This is a systematic review.\nOpen Access  This article is licensed under a Creative Commons Attri-\nbution 4.0 International License, which permits use, sharing, adapta-\ntion, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, \nprovide a link to the Creative Commons licence, and indicate if changes \nwere made. The images or other third party material in this article are \nincluded in the article's Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in \nthe article's Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a \ncopy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\nReferences\nBrin D, Sorin V, Konen E, Nadkarni G, Glicksberg BS, Klang E (2023) \nHow large language models perform on the united states medi-\ncal licensing examination: a systematic review. medRxiv 23:543\nBubeck S, Chandrasekaran V, Eldan R, et al. (2023) Sparks of artificial \ngeneral intelligence: Early experiments with gpt-4. arXiv preprint \narXiv: 2303. 12712\nChaudhry HJ, Katsufrakis PJ, Tallia AF (2020) The USMLE step 1 \ndecision. JAMA 323(20):2017\nChoi HS, Song JY, Shin KH, Chang JH, Jang B-S (2023) Developing \nprompts from large language model for extracting clinical infor -\nmation from pathology and ultrasound reports in breast cancer. \nRadiat Oncol J 41(3):209–216\nDecker H, Trang K, Ramirez J et al (2023) Large language Model−\nbased Chatbot vs Surgeon-generated informed consent documen-\ntation for common procedures. JAMA Netw Open 6(10):e2336997\nGriewing S, Gremke N, Wagner U, Lingenfelder M, Kuhn S, Boekhoff  \nJ (2023) Challenging ChatGPT 3.5 in senology-an assessment of \nconcordance with breast cancer tumor board decision making. \nJ Pers Med 13(10):1502. https:// doi. org/ 10. 3390/ jpm13 101502\nHaver HL, Ambinder EB, Bahl M, Oluyemi ET, Jeudy J, Yi PH (2023) \nAppropriateness of breast cancer prevention and screening recom-\nmendations provided by ChatGPT. Radiology. https:// doi. org/ 10. \n1148/ radiol. 230424\nJiang LY, Liu XC, Nejatian NP et al (2023) Health system-scale \nlanguage models are all-purpose prediction engines. Nature \n619(7969):357–362\nKotek H, Dockum R, Sun DQ (2023) Gender bias and stereotypes in \nLarge Language Models. arXiv preprint arXiv: 2308. 14921\nKuhl C, Weigel S, Schrading S et al (2010) Prospective multicenter \ncohort study to refine management recommendations for women \nat elevated familial risk of breast cancer: the EVA trial. J Clin \nOncol 28(9):1450–1457\nLee P, Drazen JM, Kohane IS, Leong T-Y, Bubeck S, Petro J (2023) \nBenefits, limits, and risks of GPT-4 as an AI Chatbot for medicine. \nN Engl J Med 388(13):1233–1239\nLukac S, Dayan D, Fink V et al (2023) Evaluating ChatGPT as \nan adjunct for the multidisciplinary tumor board decision-\nmaking in primary breast cancer cases. Arch Gynecol Obstet \n308(6):1831–1844\nMoher D (2009) Preferred reporting items for systematic reviews \nand meta-analyses: the PRISMA statement. Ann Intern Med \n151(4):264\nRao A, Kim J, Kamineni M et al (2023) Evaluating GPT as an adjunct \nfor radiologic decision making: GPT-4 versus GPT-3.5 in a breast \nimaging pilot. J Am Coll Radiol. https:// doi. org/ 10. 1016/j. jacr. \n2023. 05. 003\nSallam M (2023) ChatGPT utility in healthcare education, research, \nand practice: systematic review on the promising perspectives and \nvalid concerns. Healthcare 11(6):887\nShah NH, Entwistle D, Pfeffer MA (2023) Creation and adoption of \nlarge language models in medicine. JAMA 330(9):866\nSiegel RL, Miller KD, Jemal A (2019) Cancer statistics, 2019. CA \nCancer J Clin 69(1):7–34\nSorin V, Klang E (2021) Artificial intelligence and health care dispari-\nties in radiology. Radiology 301(3):E443–E443\n Journal of Cancer Research and Clinical Oncology (2024) 150:140\n140 Page 8 of 8\nSorin V, Klang E (2023) Large language models and the emergence \nphenomena. Eur J Radiol Open 10:100494\nSorin V, Barash Y, Konen E, Klang E (2020a) Deep-learning natural \nlanguage processing for oncological applications. Lancet Oncol \n21(12):1553–1556\nSorin V, Barash Y, Konen E, Klang E (2020b) Deep learning for natural \nlanguage processing in radiology—fundamentals and a systematic \nreview. J Am Coll Radiol 17(5):639–648\nSorin V, Klang E, Sklair-Levy M et al (2023) Large language model \n(ChatGPT) as a support tool for breast tumor board. npj Breast \nCancer. https:// doi. org/ 10. 1038/ s41523- 023- 00557-8\nSorin V, Barash Y, Konen E, Klang E (2023a) Large language \nmodels for oncological applications. J Cancer Res Clin Oncol \n149(11):9505–9508\nSorin V, Soffer S, Glicksberg BS, Barash Y, Konen E, Klang E (2023b) \nAdversarial attacks in radiology—a systematic review. Eur J \nRadiol 167:111085\nSorin V, Glicksberg BS, Barash Y, Konen E, Nadkarni G, Klang \nE (2023) Diagnostic accuracy of GPT multimodal analysis \non USMLE questions including text and visuals. MedRxiv \n10(2029):23297733\nTemsah M-H, Altamimi I, Jamal A, Alhasan K, Al-Eyadhy A (2023) \nChatGPT surpasses 1000 publications on PubMed: envisioning \nthe road ahead. Cureus. https:// doi. org/ 10. 7759/ cureus. 44769\nWhiting PF (2011) QUADAS-2: a revised tool for the quality assess-\nment of diagnostic accuracy studies. Ann Intern Med 155(8):529\nPublisher's Note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations."
}