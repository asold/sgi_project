{
  "title": "ProTo: Program-Guided Transformer for Program-Guided Tasks",
  "url": "https://openalex.org/W3204138855",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5089593268",
      "name": "Zelin Zhao",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A5040617244",
      "name": "Karan Samel",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5101900063",
      "name": "Binghong Chen",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5102898115",
      "name": "Le Song",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3169496116",
    "https://openalex.org/W2910663469",
    "https://openalex.org/W2971863715",
    "https://openalex.org/W2946752867",
    "https://openalex.org/W2883104598",
    "https://openalex.org/W2804838341",
    "https://openalex.org/W2173051530",
    "https://openalex.org/W3116853161",
    "https://openalex.org/W2745461083",
    "https://openalex.org/W2786760026",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W1670845525",
    "https://openalex.org/W3027304069",
    "https://openalex.org/W2739010165",
    "https://openalex.org/W2990397898",
    "https://openalex.org/W2145680191",
    "https://openalex.org/W2949084598",
    "https://openalex.org/W1933349210",
    "https://openalex.org/W1986014385",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2257979135",
    "https://openalex.org/W2910040435",
    "https://openalex.org/W3105725479",
    "https://openalex.org/W2964231903",
    "https://openalex.org/W2899702275",
    "https://openalex.org/W2194321275",
    "https://openalex.org/W3159616622",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W3164208409",
    "https://openalex.org/W2963319332",
    "https://openalex.org/W2963199420",
    "https://openalex.org/W2148112459",
    "https://openalex.org/W2963220331",
    "https://openalex.org/W3118500473",
    "https://openalex.org/W2963738360",
    "https://openalex.org/W3093497259",
    "https://openalex.org/W2553246593",
    "https://openalex.org/W2561715562",
    "https://openalex.org/W2981721414",
    "https://openalex.org/W2962688617",
    "https://openalex.org/W2190656909",
    "https://openalex.org/W2912464212",
    "https://openalex.org/W1592847719",
    "https://openalex.org/W2118781169",
    "https://openalex.org/W2963726321",
    "https://openalex.org/W1963873191",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963355447",
    "https://openalex.org/W2962716332",
    "https://openalex.org/W2964031560",
    "https://openalex.org/W2995120596",
    "https://openalex.org/W2908791737",
    "https://openalex.org/W2964253222",
    "https://openalex.org/W2950527759",
    "https://openalex.org/W2798775284",
    "https://openalex.org/W3115727409",
    "https://openalex.org/W2963102152",
    "https://openalex.org/W2970608575",
    "https://openalex.org/W2169743339",
    "https://openalex.org/W2463565445",
    "https://openalex.org/W2337392266",
    "https://openalex.org/W2964253110",
    "https://openalex.org/W2898802014",
    "https://openalex.org/W2995628494",
    "https://openalex.org/W2963518342",
    "https://openalex.org/W2167567609",
    "https://openalex.org/W2963191264",
    "https://openalex.org/W3118161885",
    "https://openalex.org/W2963800628",
    "https://openalex.org/W3168649818",
    "https://openalex.org/W2136848157",
    "https://openalex.org/W2127637733",
    "https://openalex.org/W2601273560",
    "https://openalex.org/W2963305465",
    "https://openalex.org/W2981851019",
    "https://openalex.org/W1597939856",
    "https://openalex.org/W2988086053",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2158548602",
    "https://openalex.org/W2767656849",
    "https://openalex.org/W2145739724",
    "https://openalex.org/W2963223524",
    "https://openalex.org/W2907839701",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W2592874373",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1542941925",
    "https://openalex.org/W2963277051"
  ],
  "abstract": "Programs, consisting of semantic and structural information, play an important role in the communication between humans and agents. Towards learning general program executors to unify perception, reasoning, and decision making, we formulate program-guided tasks which require learning to execute a given program on the observed task specification. Furthermore, we propose the Program-guided Transformer (ProTo), which integrates both semantic and structural guidance of a program by leveraging cross-attention and masked self-attention to pass messages between the specification and routines in the program. ProTo executes a program in a learned latent space and enjoys stronger representation ability than previous neural-symbolic approaches. We demonstrate that ProTo significantly outperforms the previous state-of-the-art methods on GQA visual reasoning and 2D Minecraft policy learning datasets. Additionally, ProTo demonstrates better generalization to unseen, complex, and human-written programs.",
  "full_text": "ProTo: Program-Guided Transformer\nfor Program-Guided Tasks\nZelin Zhao∗\nThe Chinese University of Hong Kong\nzelin@link.cuhk.edu.hk\nKaran Samel\nGeorgia Institute of Technology\nksamel@gatech.edu\nBinghong Chen\nGeorgia Institute of Technology\nbinghong@gatech.edu\nLe Song\nBiomap and MBZUAI\ndasongle@gmail.com\nAbstract\nPrograms, consisting of semantic and structural information, play an important\nrole in the communication between humans and agents. Towards learning general\nprogram executors to unify perception, reasoning, and decision making, we for-\nmulate program-guided tasks which require learning to execute a given program\non the observed task speciﬁcation. Furthermore, we propose Program-Guided\nTransformer (ProTo), which integrates both semantic and structural guidance of a\nprogram by leveraging cross-attention and masked self-attention to pass messages\nbetween the speciﬁcation and routines in the program. ProTo executes a program\nin a learned latent space and enjoys stronger representation ability than previous\nneural-symbolic approaches. We demonstrate that ProTo signiﬁcantly outperforms\nthe previous state-of-the-art methods on GQA visual reasoning and 2D Minecraft\npolicy learning datasets. Additionally, ProTo demonstrates better generalization to\nunseen, complex, and human-written programs.\n1 Introduction\nPrograms are the natural interface for the communication between machines and humans [ 11].\nIn comparison to instructing machines via demonstrations [ 41, 52, 71, 6] or via natural language\n[14, 32, 3], guiding agents by programs has multiple beneﬁts. First, programs are explicit and much\ncleaner than other instructions such as languages [79]. Second, programs are structured with loops\nand branches [ 1] so they can express complex reasoning processes [ 95]. Finally, programs are\ncompositional, promoting the generalization and scalability of neural models [15, 64, 20]. However,\nwhile program synthesis and program induction have been deeply explored [12, 17, 18, 22, 39, 49],\nvery few works focuses on learning to follow program guidance [ 79, 70]. Furthermore, previous\nwork designs ad-hoc program executors for different functions in different tasks [79, 29, 23], which\nhinders the generalization and scalability of developed models.\nTo pursue general program executors to unify perception, reasoning, and decision making, we\nformulate program-guided tasks, which require the agent to follow the given program to perform\ntasks conditioned on task speciﬁcations. The programs may come from a program synthesis model\n[95] or be written by human [79]. Two exemplar tasks are shown in Figure 1. Program-guided tasks\nare challenging because the agent needs to jointly follow the complex structure of the program [1],\nperceive the speciﬁcation, and ground the program semantics on the speciﬁcation [40].\nInspired by the recent signiﬁcant advance of transformers in diverse domains [24, 25, 59], we present\nthe Program-guided Transformer (ProTo) for general program-guided tasks. ProTo combines the\n35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.\narXiv:2110.00804v2  [cs.LG]  16 Oct 2021\nFigure 1: Illustration of two exemplar program-guided tasks. The ﬁrst task is program-guided visual\nreasoning [47], where the model needs to learn to execute a visual program on the speciﬁcation\n(image) to get a predicted answer. The second task is program-guided policy learning [79], where\nthe agent learns a policy to perform tasks based on the observed speciﬁcation following the program\nguidance.\nstrong representation ability of transformers with symbolic program control ﬂow. We propose to\nseparately leverage the program semantics and the explicit structure of the given program via efﬁcient\nattention mechanisms. ProTo enjoys strong representation ability by executing the program in a\nlearned latent space. In addition, ProTo can either learn from reward signals [64, 79] or from dense\nexecution supervision [15].\nWe evaluate ProTo on two tasks, program-guided visual reasoning and program-guided policy learning\n(corresponding to Figure 1 left and Figure 1 right). The former task requires the model to learn to\nreason over a given image following program guidance. We experiment on the GQA dataset [ 47]\nwhere the programs are translated from natural language questions via a trained program synthesis\nmodel. The evaluation on the public test server shows that we outperform the previous state-of-the-art\nprogram-based method by 4.31% in terms of accuracy. Our generalization experiments show that\nProTo is capable of following human program guidance. The latter task asks the agent to learn a\nmulti-task policy to interact with the environment according to the program. We experiment on a\n2D Minecraft environment [79] where the programs are randomly sampled from the domain-speciﬁc\nlanguage (DSL). We ﬁnd that ProTo has a stronger ability to scale to long and complex programs\nthan previous methods. ProTo signiﬁcantly outperforms the vanilla transformer [85] which does not\nexplicitly leverage the structural guidance of the program. We will release the code and pre-trained\nmodels after publishing.\nIn summary, our contributions are threefold. First, we formulate and highlight program-guided tasks,\nwhich are generalized from program-guided visual reasoning [95, 64] and program-guided policy\nlearning [79]. Second, we propose the program-guided transformer for program-guided tasks. Third,\nwe conduct extensive experiments on two domains and show promising results towards solving\nprogram-guided tasks.\n2 Related Work\nProgram Induction, Synthesis and InterpretationProgram induction methods learn an underly-\ning input-output mapping from given examples [54, 22, 50], and no programs are explicitly predicted.\nDifferently, program synthesis targets at predicting symbolic programs from task speciﬁcations\n[23, 12, 18, 17, 31, 27, 91, 83, 16, 78, 58, 84]. However, these approaches do not learn to execute\nprograms. In the domain of digit and string operation (adding, copying, sorting, etc.), Neural Program\nInterpreter (NPI) [70, 92, 73] learn to compose lower-level programs to express higher-level programs\n[37] while Neural Turing Machines [38, 55] use attentional processes with external memory to infer\nsimple algorithms. Recently, [79] proposes to guide the agent in 2D Minecraft via programs. [65]\n2\ndetects repeated entries in the image and uses programs to guide image manipulation. They neither\nattempt to formulate general program-guided tasks nor propose a uniﬁed model for different tasks.\nVisual Reasoning Visual reasoning requires joint modeling of natural language and vision. A\ntypical visual reasoning task is the visual question answering (VQA) [5]. Attention mechanisms have\nbeen widely used in the state-of-the-art VQA models [60, 2, 72, 99, 46]. Neural module networks\nand the neural-symbolic approaches [95, 64] propose to infer programs from natural languages and\nexecute programs on visual contents to get the answer. A large-scale dataset GQA [ 47], which\naddresses the low diversity problem in the synthetic CLEVR dataset [48], is proposed to evaluate\nthe state-of-the-art visual reasoning models. Neural state machine [ 45] is proposed by leveraging\nmodular reasoning over a probabilistic graph, which does not leverage the symbolic program. Our\nmodel is similar to the concurrent work, meta module network [15] in that we use shared parameters\nfor different function executors. Nevertheless, we propose a novel attention-based architecture and\nextend our model to the policy learning domain.\nTransformer Transformer was ﬁrstly proposed in machine translation [85]. After that, transformers\ndemonstrate their power on many different domains, such as cross-modal reasoning [81], large-scale\nunsupervised pretraining [ 24, 77, 94], and multi-task representation learning [ 69, 66]. Recently,\ntransformers appear to be competitive models in many fundamental vision tasks, such as image\nclassiﬁcation [25, 97], object detection [13] and segmentation [59, 33]. While we share the same\nbelief that transformers are strong, uniﬁed, elegant models for various deep learning tasks, we propose\nnovel transformer architecture for program-guided tasks. We discuss a few similar unpublished works\nin Appendix A.\nPolicy Learning With Programs Previous policy learning literature explored the beneﬁts of\nprograms in different aspects. First, some pre-deﬁned routines containing human prior knowledge\ncould help reinforcement learning, and planning [30, 67, 4, 98]. Second, programs enable interpretable\nand veriﬁable reinforcement learning [87, 8]. Our approach follows a recent line of work that learns\nto interpret program guidance [79, 4]. However, we adopt a novel, simple and uniform architecture\nthat demonstrates superior performance, thus conceptually related to multitask reinforcement learning\n[82, 90] and hierarchical reinforcement learning [7].\n3 Program-guided Tasks\nUnlike natural language that is ﬂexible, complex, and noisy, programs are structured, clean and\nformal [79]. Therefore, programs can serve as a powerful intermediate representation for human-\ncomputer interactions [11]. Different from previous work that learns to synthesis programs from\ndata [95, 83, 27], we study learning to execute given programs [79] based on three motivations. First,\ninstructing machines via explicit programs instead of noisy natural languages enjoys better efﬁciency,\nand accuracy [79]. Second, learned program executors have stronger representation ability than hard-\ncoded ones [95, 15]. Third, we attempt to develop a uniﬁed model to integrate perception, reasoning,\nand decision by learning to execute, which heads towards a core direction in the neural-symbolic AI\n[36, 35].\nA program-guided task is speciﬁed by a tuple (S,Γ,O,G) where Sis the space of speciﬁcations, Γ\ndenotes the program space formed by a domain-speciﬁc language (DSL) for a task, Ois the space for\nexecution results, and Gis the goal function for the task. For each instance of the program-guided\ntask, a program P ∈Γ is given. An executor EΦ is required by the task to execute the program on the\nobserved speciﬁcation. Note that different from hard-coded non-parametric executors used in some\nprevious work [95, 23], the executor EΦ is parametrized by Φ. For convenience, we deﬁne a routine\nto be the minimum execution unit of a program (e.g. Filter(girl) in Figure 1 is a routine). We\ndenote Pk to be the k-th routine in P and |P|to be the number of routines in P. According to the\nDSL, the program should begin at the entrance routine and ﬁnish at one of the exiting routines (e.g.\nFilter(girl) in Figure 1 is an entrance routine and Query_color() is an exiting routine). At\neach execution step τ, the executor executes the current routine Pp ∈P on the speciﬁcation s(τ) ∈S\nand produces an output o(τ) ∈O. The execution of P ﬁnishes if one of the exiting routines ﬁnishes\nexecution. The goal of a program-guided task is to produce desired execution results to achieve the\ngoal G.\n3\nAlgorithm 1:ProTo Execution\nResult: Execution results {o(τ)}T\nτ=1\n1 Initialize τ = 0 and the pointer p;\n2 Build Ps according to Eq. 1;\n3 while not reach an exiting routine do\n4 Observe s(τ) and set τ = τ + 1 ;\n5 Build Pt(τ) according to Eq. 2;\n6 o(τ) = ProToInfer(s(τ),Ps,Pt(τ),p);\n7 Output o(τ) ;\n8 if Pp ﬁnishes execution then\n9 UpdatePointer(p, o(τ), Pp) ;\n10 end\nAlgorithm 2:UpdatePointer(p, o(τ), Pp)\n1 if Pp is an If-routine then\n2 Point pto the ﬁrst routine in the T/F\nbranch if o(τ) is T/F;\n3 else ifPp is a While-routine then\n4 Point pto the ﬁrst routine\ninside/outside the loop if o(τ) is T/F;\n5 else ifPp ends a loop then\n6 Point pto the loop condition routine.\n7 else\n8 Point pto the subsequent routine;\n9 end\n4 Program-guided Transformer\nA good model for a program-guided task should fulﬁll the following merits. First of all, it should\nleverage both the semantics and structures provided by the program (refer to Figure 2). Second,\nit would better be a multi-task architecture with shared parameters for different routines to ensure\nefﬁciency and generalization [15, 88, 69]. Third, it would be better if the model does not leverage\nexternal memory for the efﬁciency of batched training [64]. To these ends, we present the Program-\nguided Transformer (ProTo). ProTo treatsroutines as objects and leverages attention mechanisms to\nmodel interactions among routines and speciﬁcations. The execution process of ProTo is presented\nin Algorithm 1. In each execution timestep, ProTo leverages the semantic guidance Ps and the\ncurrent structure guidance Pt(τ) (Line 2&5 in Algorithm 1, detailed in Sec 4.1) and infers for one\nstep (Line 6 in Algorithm 1, detailed in Sec 4.2). When a routine ﬁnishes execution, a pointer\np∈{1,2,..., |P|}indicating the current routine Pp (Pp is the pth routine in P) would be updated\n(Line 8&9 in Algorithm 1, detailed in Algorithm 2 and Sec 4.3). The results of ProTo are denoted\nas {o(τ)}T\nτ=1 where T is the total number of execution timesteps. ProTo supports several training\nschemes, which are listed in Sec 4.4.\n4.1 Disentangled Program Representation\nAs shown in the left of Figure 2, we leverage both the semantics and structure of the program in\nProTo. The semantic part of the program Ps ∈R|P|×d is an embedding matrix for all routines where\nR is the real coordinate space and dis a hyperparameter specifying the feature dimension. Note Ps\nremains the same for all execution timesteps. Denote {wk\ni}L\ni=0 to be the words in Pk where Lis the\nmaximum number of words in one routine (padding words are added if Pk does not have Lwords),\nwe construct the k-th row of Ps corresponding to Pk via a concatenation of all token embeddings:\nPs\nk =\nn\ni\nWordEmbed(wk\ni) (1)\nwhere f·represents the concatenation function and WordEmbed maps a token to an embedding with\ndimension dm where we set d= L×dm.\nThe structure part of the program Pt(τ) ∈R|P|×|P|is a transition mask calculated at each execution\ntimestep τ to pass messages from the previous execution timestep to the current execution timestep.\nAlthough most types of the routines only need to get information from the previously executed\nroutines, some logical routines such as Compare_Color() rely on more than one routines’ results.\nWe denote Parents(Pp) to be the set of routines whose results would be taken as input by the\ncurrent routine Pp. We can easily derive Parents(Pp) from the program P, which is detailed in the\nAppendix C.1. The transition mask Pt(τ) is deﬁned by the following equation:\nPt(τ)[i][j] =\n{0 if (Pj ∈Parents(Pp) and Pi = Pp) or (i= j),\n−∞ else, (2)\nwhere we set diagonal elements of Pt(τ) to zeros to preserve each routine’s self information to the\nnext timestep. Positional encoding is added following the standard transformer [85].\n4\nFigure 2: Main components of Program-guided Transformer (ProTo). Left: we propose a disentangled\nprogram representation to leverage both semantics and structures of the program. The shown program\nselects the bag and the wine from the image and veriﬁes whether the bag is on the left to the wine.\nRight: given structure and semantic guidance offered by a program, ProTo updates the result matrix\nleveraging the program guidance and the speciﬁcation.\n4.2 ProTo Inference\nGiven the disentangled program guidance Ps, Pt(τ) and the current observed speciﬁcation s(τ) ∈\nRNS×d where Ns is the number of objects in the speciﬁcation, ProTo infers the execution result o(τ)\nvia stacked attention blocks as shown on the right of Figure 2, which is presented in detail in this\nsubsection. Note we assume the speciﬁcation is represented in an object-centric manner, which might\nbe obtained via a pre-trained object detector [2] or an image patch encoder [25].\nProTo maintains a result embedding matrix Z ∈R|P|×d which stores latent execution results for all\nroutines. The result embedding matrix is initialized as a zero matrix and is updated at each timestep.\nWe show the architecture of transformer executors in the right of Figure 2. During ProTo inference,\nwe ﬁrst conduct cross-attention to inject semantic information of routines to the hidden results via\nH1 = CrossAtt1(Z,Ps) = softmax(ZPs⊤\n√\nd\n)Ps (3)\nwhere H1 ∈R|P|×d is the ﬁrst intermediate latent results.\nWe propose to adopt masked self-attention [85] to propagate the information of the previous results\nto the current routine. The masked self-attention leverages the transition mask to restrict the product\nof queries and keys before softmax (queries, keys and values are the same in the self-attention).\nFormally, we acquire the second intermediate results H2 ∈R|P|×d by\nH2 = MaskedSelfAtt(H1,Pt(τ)) = softmax(H1H1\n⊤\n√\nd\n+ Pt(τ))H1. (4)\nAfter that, we apply cross-attention again to update results based on the speciﬁcation s(τ):\nZ = CrossAtt2(H2,s(τ)) = softmax(H2s(τ)⊤\n√\nd\n)s(τ). (5)\nThe above equations for self-attention and cross-attention show only one head of self-attention and\ncross-attention for simplicity. However, in experiments, we use multi-head attention with eight heads\n[85]. The resulting embedding corresponding to the pointed routine Zp would be decoded via an\nMLP to produce an explicit output o(τ) = MLP(Zp).\n5\n4.3 Pointer Update\nAfter each execution timestep, we would update the pointer pif Pp ﬁnishes execution. Note that\nsome routines cannot be ﬁnished in one execution timestep (e.g., the routine Mine(gold) requires\nthe agent to navigate to the gold and then take Mine action). We know the execution of Pp is ﬁnished\nif the execution result o(τ) meets the ending condition of Pp (e.g. when o(τ) is the Mine action and\nthe agent successfully mines a gold, we judge that Mine(gold) is ﬁnished). A full list of ending\nconditions for all routines is in the Appendix B.\nThe pointer pis updated according to the control ﬂow [ 1, 18, 79] of the program. The pointer’s\nmovement is determined by the execution result o(τ), the type of Pp, and the location of Pp. We\noutline the detailed procedure of pointer update in Algorithm 2.\nParallel Execution Transformer has one powerful ability that it can conduct sequential prediction\nin parallel [85]. In our case, we can execute routines that do not have result dependency in parallel.\nWe only need to modify the masked self-attention in Eq. 4 to support passing multiple routines’\nmessages in parallel. Furthermore, multiple pointers would be adopted and updated in parallel, while\nmultiple results would be output in parallel in one execution timestep. We detail the paralleled version\nof Algorithm 1 and Algorithm 2 in the Appendix C.2.\n4.4 ProTo Training Targets\nAfter deriving the execution results of all routines denoted via {o(τ)}T\nτ=1, ProTo can be trained via\nthe following three types of training targets. (1) Dense Supervision LD. When the ground truth\nof all execution results of all routines {˜o(τ)}T\nτ=1 is known, we can use a dense loss LD, which\nis the L2 distance between {o(τ)}T\nτ=1 and {˜o(τ)}T\nτ=1. (2) Partial Supervision LP. Knowing the\nﬁnal execution result of the whole program ˜o(T), ProTo can be learned through partial supervision\nLP which measures the L2 distance between oT and ˜o(T). (3) RL Target LR. When a program is\nsuccessfully executed, the environment gives the agent a sparse reward of+1. Otherwise, the agent\ngets a zero reward. In this case, ProTo can be optimized via a reinforcement learning lossLR [51, 79].\nIn experiments, we follow the same type of supervision as the corresponding baselines, which varies\nfrom task to task.\n5 Experiments\n5.1 Program-guided Visual Reasoning\nTask Description Program-guided visual reasoning requires the agent to follow program guidance\nto reason about an image. It is formed by a tuple (Sv,Γv,Ov,Gv) where v denotes the visual\nreasoning task. One speciﬁcation s ∈Sv is the object-centric representation of an image formed\nvia a pre-trained object detector [2], which is not updated from time to time. In other words, s(τ) =\ns(τ+1) holds for each timestep τ. The design of the program space Γv follows the previous work [47].\nThe output set Ov is the possible results for all routines for all the programs (all types of results are\nencoded to ﬁxed-length vectors as explained in Appendix D.1). We deﬁne an answer to a program P\nto be the ﬁnal execution result of the program P, and the goal of program-guided visual reasoning\nGv is to predict the correct answer to the program. As for the training target, we adopt the dense\nsupervision LD as described in Sec 4.4 to train ProTo following previous approaches [15, 42, 56].\nDataset Setup We conduct experiments of program-guided visual reasoning based on the public\nGQA dataset [47] consisting of 22 million questions over 140 thousand images. It is divided into\ntraining, validation, and testing splits. The ground true answers, programs, and scene graphs are\nprovided in the training and validation split but not in the test split. We use the provided balanced\ntraining split to control data bias. On the training split, we train a transformer-based seq2seq model\n[85] to parse a question into a program. For validation and testing, we use this trained seq2seq model\nto acquire a program from a question 2. Besides answer accuracy, the GQA dataset [47] offers three\nmore metrics evaluating the consistency, validity, and plausibility of learned models.\n2In the GQA dataset [47], we found that a simple seq2seq model can achieve 98.1% validation accuracy to\nconvert a natural language question into a program. The concurrent work [15] also found a similar fact.\n6\nTable 1: Comparison of ProTo with previous models on thetest2019 split of the GQA dataset. In\nthe collum of training Signal, we use QA, SG, Prog to denote question-answer pairs, scene graphs,\nand programs. As for the metrics, Cons., Plaus., Valid., Distr., Acc. represent consistency, plausibility,\nvalidity, distribution, and accuracy correspondingly.\nModel Signal Binary Open Cons. Plaus. Valid. Distr. Acc.\nHuman [47] - 91.20 87.40 98.40 97.20 98.90 - 89.30\nBottomUp [2] QA 66.64 34.83 78.71 84.57 96.18 5.98 49.74\nMAC [46] QA 71.23 38.91 81.59 84.48 96.16 5.34 54.06\nLXMERT [81] QA 77.16 45.47 89.59 84.53 96.35 5.69 60.33\nNSM [45] SG 78.94 49.25 93.25 84.28 96.41 3.71 63.17\nPVR [56] Prog 78.02 43.75 91.43 84.77 96.50 6.00 59.81\nSNMN [42] Prog 73.40 40.82 85.11 84.79 96.37 5.14 56.09\nMMN [15] Prog 78.90 44.89 92.49 84.55 96.19 5.54 60.83\nProTo Prog 79.12 51.45 93.45 86.12 96.52 3.66 65.14\nTable 2: Accuracy of the ablation study on\nthe validation split of the GQA dataset.\nModel Acc.\nProTo Full Model 64.47\nNo Structure Guidance 55.16\nNo Semantic Guidance 33.82\nGAT Encoding 58.58\nPartial Supervision 60.28\nNS-VQA [93] 29.57\nIPA-GNN [10] 47.10\nMMN [15] 60.40\nTable 3: Results of the generalization experiments on\nthe validation split of GQA dataset.\nGeneralization Model Acc.\nHuman Program Guidance MMN [15] 59.47\nProTo 70.33\nUnseen Programs MMN [15] 61.88\nProTo 65.34\nRestricted Data Regime MMN [15] 52.39\nProTo 58.31\nExperiment Details We takeN = 50 object features (provided by the GQA dataset) withd= 2048\ndimension. The optimizer is BERT Adam optimizer [24] with a base learning rate 1 ×10−4, which\nis decayed by a factor of 0.5 every epoch. To alleviate over-ﬁtting, we adopt an L2 weight decay of\n0.01. The model is trained for 20 epochs on the training split, and the best model evaluated on the\nvalidation split is submitted to the public evaluation server to get testing results. The testing results\nof baselines are taken from the corresponding published literature [ 81, 45, 15] or the leaderboard.\nFollowing [15, 45], we do not list unpublished methods or methods leveraging external datasets.\nTesting Results We present the results on the testing split of GQA comparing to previous baselines\nin Table 1. ProTo surpasses all previous baselines by a considerable margin, successfully demonstrat-\ning the effectiveness of leverage program guidance in reasoning. The superior performance of ours\nover the concurrent work meta module network (MMN) [15] reveals ProTo’s strong modeling ability.\nMore visualizations are in Appendix D.1.\nAblation Study We ablate our full model to the following variants to study the importance of\ndifferent components. Despite the described changes, other parts remain the same with the full model.\n(1) No Structure Guidance. Only the semantic guidance is used, and Pt(τ) is set to the all-zeros\nmatrix in Eq 4. (2) No Semantic Guidance. The semantic guidance Ps is set to the all-zeros matrix to\ndisable semantics guidance. (3) GAT Encoding. We use graph attention networks [86] to encode the\nprogram and fuse the program feature with result embedding matrix and speciﬁcation feature via two\nconsecutive cross attention modules (refer to Appendix D.1 for details). (4) Partial Supervision. We\nonly supervise the predicted answer to the program but do not give dense intermediate supervision\n(refer to Sec 4.4).\nTable 2 shows the results, and the validation accuracy of two program-based baselines is listed for\nreference. The results demonstrate that both structure guidance and semantic guidance contribute\nsigniﬁcantly to the overall performance. ProTo is also better than the GNN baseline because of\nthe strong cross-modal representation learning ability of the transformer [ 43]. And the extremely\n7\nlow validation accuracy of NS-VQA [95], which is reported by its authors in [93], reveals that the\nhard-coded program executors are not as powerful as the learned transformer executors.\nGeneralization Experiments We conduct systematical experiments to evaluate whether humans\ncan guide the reasoning process via programs. More details are in the Appendix D.1. (1) Human\nProgram Guidance. We test whether humans can guide the reasoning process via programs on a\ncollected GQA-Human-Program dataset. We ask volunteers to write 500 programs and corresponding\nanswers on 500 random pictures taken from the GQA validation split. No natural language questions\nare collected. All the models are trained on the training split of GQA and tested on the GQA-Human-\nProgram dataset. (2) Unseen Programs. Following [ 15], we remove from the training split all the\nprograms containing the function verify_shape, and we evaluate the models on the instances\ncontaining verify_shape on the validation split. (3) Restricted Data Regime. We restrict the models\nonly to use 10% uniformly sampled training data to test the data efﬁciency of models.\nResults are presented on Table 3. We found that ProTo can successfully generalize its learned program\nexecution ability to human written programs, surpassing the previous state-of-the-art neural module\nnetwork approach by over 10 points. ProTo can also generalize to unseen programs, again verifying\nthe compositional generalization ability of our neural-symbolic transformer model [24, 20]. Besides,\nProTo is more data-efﬁcient than MMN [15]. We also found that ProTo is much more effective than\nthe recent learning-to-execute approach IPA-GNN [10].\n5.2 Program-guided Policy Learning\n0 1 2 3 4 5 6 7 8\nEnvironment Steps 1e7\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0Reward ProTo(OURS)\nPGA\nTransformer\nTreeRNN\nFigure 3: Training curves on the 2D Minecraft\nenvironments in comparison to several baselines.\nBoth mean and standard errors over ten random\nagents are shown in the ﬁgure.\nTask Description Program-guided policy\nlearning requires the agent to learn a policy\nperform tasks following a given program [79].\nWe denote this task as a tuple (Sp,Γp,Op,Gp)\nwhere p stands for the policy learning task.\nSince we are experimenting on a grid-world\nenvironment, the speciﬁcation s ∈ Sp is the\nfeature embeddings of Ng objects placed in Ng\ngrids. Unlike the visual reasoning task, the spec-\niﬁcation is updated by the environment at each\ntimestep τ after the agent takes an action. The\ndesign of the program space Γp follows [79].\nThe output space Op consisting of several types:\n(1) Boolean results (True or False); (2) Motor\nActions (e.g. Up); (3) Interactive Actions (e.g.\nMine and Build). Note the agent can only in-\nteract with the grid it stands on. In this task,\nthe agent should learn from a reward signal LR\n(described in Sec 4.4) while we also experiment\napplying the dense supervision. The goal Gp is to maximize the task completion rates [79].\nExperiment Details Following [75, 79], we conduct experiments on a 2D Minecraft Environment3.\nPrograms are sampled from the DSL and divided into training and testing splits (4000 for training\nand 500 for testing). For each instance, the agent must follow a given program to navigate the grid\nworld, mine resources, sell mined resources from its inventory, or place marks. The baselines include\nProgram-guided Agent (PGA) [79], the naïve Transformer [85], and TreeRNN [80]. PGA separately\nlearns perception and policy modules. Transformer and TreeRNN encode the input program in a\ntoken by token manner and output an action distribution. We ensure that the number of parameters\nfor different methods is comparable. We use the same manner of encoding the objects in the grid into\nfeatures as [79], which are projected to d-dimension features via an MLP. The policy is optimized\nvia the actor-critic (A2C) algorithm [51], and we use the same policy learning hyperparameters with\nPGA [79], which are detailed in the Appendix D.2 for reference. When using dense supervision, the\nground-true execution traces come from a hard-coded planner. More details are in the Appendix D.2.\nTraining Curves, Testing Results and VisualizationWe ﬁrst show the training curves under\nthe RL target in Figure 3. We observe that ProTo surpasses Program-guided Agent (PGA) [ 79],\n3The implementation of the environment, the dataset, and the baselines are provided by the authors of [79].\n8\nTable 4: Task completion rates on 2D MineCraft under different test settings. We repeat each\nexperiments on 10 different random seeds and we show both averaged rates and their standard\ndeviations. Baseline numbers are slightly different from [79] due to random noises.\nSupervision RL Target Dense Supervision\nModel Transformer [85] PGA [79] ProTo PGA[79] ProTo\nStandard Testing 50.1±3.2 94.2 ±0.8 97.3±2.1 96.9±0.9 99.1±0.5\nLonger Programs 41.2±3.5 86.1 ±0.9 91.2±3.3 92.1±0.7 94.4±0.8\nComplex Programs 40.8±1.8 89.7 ±0.3 95.0±2.5 91.2±0.5 96.3±1.0\nFigure 4: An example of result on the 2D Minecraft environment. The given program is shown in (a).\nEach timestep’s execution results and speciﬁcations are shown in (b) top and (b) bottom, respectively.\nDue to space limitations, we only show some speciﬁcations along the timeline, and the arrows in (b)\ndenote the places of the speciﬁcations in the timeline.\nwhich demonstrates the power of leveraging disentangled program guidance in transformers. The\nfact that ProTo outperforms the vanilla end-to-end transformer by a large margin demonstrates the\neffectiveness of explicitly leveraging program structure guidance.\nWe test the trained agent in different settings. Despite the Standard Testing split offered by [79], we\nsample two more splits from the DSL while ensuring the testing cases are not seen in the training\nsplit: Longer Programs and Complex Programs. All programs in Longer Programs contain more than\neighty tokens, while all programs in Complex Programs include more than four If or While tokens.\nFurthermore, we add execution losses on the training split and test the baseline method PGA and\nour method. The results on the test splits are shown in Table 4. We ﬁnd that ProTo performs better\nthan PGA [79] on all testing settings. ProTo scales better than PGA to longer and complex programs,\ndemonstrating the strong ability of ProTo to leverage the program structure. We also observe that\nProTo has superior performance when dense execution supervision is provided. A demonstration\nof the test split is provided in Figure 4, where we observe ProTo successfully learns to execute the\nprogram and develops a good policy to follow the program guidance.\n6 Conclusion and Future Work\nIn this paper, we formulated program-guided tasks, which asked the agent to learn to interpret and\nexecute the given program on observed speciﬁcations. We presented the Program-guided Transformer\n(ProTo), which addressed program-guided tasks by executing the program in a hidden space. ProTo\nprovides new state-of-the-art performance versus previous dataset-speciﬁc methods in program-\nguided visual reasoning and program-guided policy learning.\nOur work suggests multiple research directions. First, it’s intriguing to explore more advanced and\nchallenging program-guided tasks, such as program-guided embodied reasoning [21] and program-\nguided robotic applications [68]. Second, we are learning separate parameters for different tasks,\nwhile building general and powerful program executors across tasks with shared parameters is very\npromising [44, 90, 88]. Additionally, improving transformer executors with more hierarchy [59] and\nbetter efﬁciency [96] is a meaningful future direction.\n9\n7 Acknowledgement\nWe thank Shao-Hua Sun for sharing his codes to us. This work is supported in part by the DARPA\nLwLL Program (contract FA8750-19-2-0201).\n10\nIs the plate blue and round?\nImage\nQuestion\nMMNResults\nProToResults\n4FMFDU\tQMBUF\n7FSJGZ\tCMVF\n\"OE\t\nTrue False\nFalse\nTrue✓\nDo you see horses near the bushes?\n4FMFDU\tCVTI\n3FMBUF*OWFSTF\tOFBS\r\u0001IPSTF\n&YJTU\t\nFalse✘\n7FSJGZ\tSPVOE\n✘\n4FMFDU\tQMBUF\n7FSJGZ\tCMVF\n\"OE\t\nTrue True\n7FSJGZ\tSPVOE\n4FMFDU\tCVTI\n3FMBUF*OWFSTF\tOFBS\r\u0001IPSTF\n&YJTU\t\nTrue\n✓\nFigure 1: Visualization of validation results on the GQA dataset (Part 1). We use the green tick and\nthe red cross to denote right and wrong reasoning result respectively. The MMN approach fails both\nexamples while ProTo successfully addresses them.\n1\n4FMFDU\tQMBUF\nIs there a kite or a racket that is white?\nImage\nQuestion\nMMNResults\nProToResults\n4FMFDU\tSBDLFU\n'JMUFS\tXIJUF\n&YJTU\t\n4FMFDU\tLJUF\n'JMUFS\tXIJUF\n&YJTU\t\n0S\t\nTrue\n False\nTrue4FMFDU\tSBDLFU\n'JMUFS\tXIJUF\n&YJTU\t\n4FMFDU\tLJUF\n'JMUFS\tXIJUF\n&YJTU\t\n0S\t\nTrue\n False\nTrue\n✓\n✓\nIs there a young man to the left of a plate?\n3FMBUF\tUP\u0001UIF\u0001SJHIU\r\u0001NBO\n'JMUFS\tZPVOH\n✘\n&YJTU\t\nFalse4FMFDU\tQMBUF\n3FMBUF\tUP\u0001UIF\u0001SJHIU\r\u0001NBO\n'JMUFS\tZPVOH\n✓\n&YJTU\t\nTrue\nFigure 2: Visualization of validation results on the GQA dataset (Part 2). We use the green tick\nand the red cross to denote right and wrong reasoning results, respectively. Both models predict the\ncorrect answer in the ﬁrst example, and the reasoning traces are the same. For the second example,\nour model gets the correct answer, but the MMN baseline fails.\n2\n(PUP\t\u0013\r\u0001\u0014\n.JOF\t3FDUBOHMF\n1MBDF\t5SJBOHMF\r\u0001\u0016\r\u0001\u0015\n*G\u0001\"HFOU<5SJBOHMF>\u0001\u001d\u0001\u0014\u001b1MBDF\t$JSDMF\r\u0001\u0019\r\u0001\u0018\n*G\"HFOU<3FDUBOHMF>\u0001\u001d\u0001\u0012\u001b1MBDF\t3FDUBOHMF\r\u0012\r\u0019\nProgram\nPGAResults\nProToResults\nTrue\n(PUP\t\u0013\r\u0001\u0014\n.JOF\t3FDUBOHMF\n1MBDF\t5SJBOHMF\r\u0001\u0016\r\u0001\u0015\nInitial Specification\n*G\u0001\"HFOU<5SJBOHMF>\u0001\u001d\u0001\u00141MBDF\t$JSDMF\r\u0001\u0019\r\u0001\u0018\n✓ ✓ ✓\n✓ ✘\n*G\u0001\"HFOU<3FDUBOHMF>\u0001\u001d\u0001\u0012False◯\nTrue\n.JOF\t3FDUBOHMF\n1MBDF\t5SJBOHMF\r\u0001\u0016\r\u0001\u0015\n*G\u0001\"HFOU<5SJBOHMF>\u0001\u001d\u0001\u00141MBDF\t$JSDMF\r\u0001\u0019\r\u0001\u0018\n✓ ✓ ✓\n✓ ✓\n*G\u0001\"HFOU<3FDUBOHMF>\u0001\u001d\u0001\u0012False✓\n(PUP\t\u0013\r\u0001\u0014\nFigure 3: Visualization of test results on the Minecraft environment in comparison to program-guided\nagent [79] (Part 1). Due to space limitations, we omit the middle steps for executing routines and\ndirectly show the execution results of routines. We use the green tick and the red cross to denote right\nand wrong reasoning results, respectively. Additionally, we use a red circle to indicate that the agent\nhas failed some routine (therefore, the program execution is failed). We observe that the ProTo agent\ncan build a bridge to cross a river to place a circle on the other side of the river, but PGA fails to do\nso.\n3\n8IJMF\u0001&OW<(PME>\u0001\u001f\u0001\u0012\u001b.JOF\t(PME\n*G\u0001\"HFOU<(PME>\u001d\u0014\u001b*G\u0001&OW<(PME>\u0001\u001f\u0001\u0012\u001b.JOF\t*SPO\nProgram\nPGAResults\nProToResults\nTrue8IJMF\u0001&OW<(PME>\u0001\u001f\u0001\u0012 .JOF\t(PME\n *G\u0001\"HFOU<(PME>\u001d\u0014\nInitial Specification\n*G\u0001&OW<(PME>\u0001\u001f\u0001\u0012 .JOF\t*SPO\n✓ ✓\n✓\n✓ True\nTrue\n✓ 8IJMF\u0001&OW<(PME>\u0001\u001f\u0001\u0012True✓\n.JOF\t(PME\n *G\u0001\"HFOU<(PME>\u001d\u0014✓ True*G\u0001&OW<(PME>\u0001\u001f\u0001\u0012✓ True\n.JOF\t*SPO\n✓ 8IJMF\u0001&OW<(PME>\u0001\u001f\u0001\u0012✓ True.JOF\t(PME\n✘\nTrue8IJMF\u0001&OW<(PME>\u0001\u001f\u0001\u0012 .JOF\t(PME\n *G\u0001\"HFOU<(PME>\u001d\u0014\n*G\u0001&OW<(PME>\u0001\u001f\u0001\u0012 .JOF\t*SPO\n✓ ✓\n✓\n✓ True\nTrue\n✓ 8IJMF\u0001&OW<(PME>\u0001\u001f\u0001\u0012True✓\n.JOF\t(PME\n *G\u0001\"HFOU<(PME>\u001d\u0014✓ True*G\u0001&OW<(PME>\u0001\u001f\u0001\u0012✓ True\n.JOF\t*SPO\n✓ 8IJMF\u0001&OW<(PME>\u0001\u001f\u0001\u0012✓ True.JOF\t(PME\n✓\n*G\u0001\"HFOU<(PME>\u001d\u0014False✓ False8IJMF\u0001&OW<(PME>\u0001\u001f\u0001\u0012✓\nTIMEOUT!\nFigure 4: Visualization of test results on the Minecraft environment in comparison to program-guided\nagent [79] (Part 2). Due to space limitations, we omit the middle steps for executing routines and\ndirectly show the execution results of routines. We use the green tick and the red cross to denote right\nand wrong reasoning results, respectively. We observe that the ProTo agent can mine the gold in the\ncorner, but PGA fails to do so within the time constraint.\n4\nA Extended Related Work\nTransformers with Masked Attention We notice a few transformer-based architectures adopt\nmasked attention to achieve different goals. First, the masked self-attention [85] is adopted to restrict\nthe model from seeing subsequent positions of tokens for machine translation. Second, mask attention\nnetworks [28] uses mask matrices to enforce the localness modeling ability of transformers. Third,\nin the vision domain, attention masks are used to highlight speciﬁc classes of visual content (e.g.,\nforeground objects) [76, 89]. One concurrent unpublished work [34] uses masked attention to model\ndata ﬂow relationship for source code summarization.\nNeural Symbolic Learning The goal of neural symbolic learning is to pursue a coherent, uniﬁed\nview of symbolic logic-based computation and neural computation [ 9, 35, 36]. Previous neural\nsymbolic systems involve visual reasoning [95, 64], logic induction [63], and reading comprehension\n[19]. We propose to leverage transformer architecture to integrate perception, reasoning, and decision.\nWe believe transformer architecture would advance neural-symbolic systems because of its ability of\ncompositional representation learning.\nB Program Details\nIn this section, we provide more details of the programs on two experimented tasks.\nB.1 Program-guided Visual Reasoning\nAll types of routines on the GQA dataset [47] are provided in Table 8. No loop or branching routines\nexist in the GQA datasets. In the GQA dataset, the entrance routine is the ﬁrst routine in the program,\nand the exiting routine is the last routine of the program, which would produce the ﬁnal answer to the\nprogram. Some types of routines may use more than one inputs.\nThere are three types of program inputs and outputs (execution results). The ﬁrst type is Objects,\nwhich is a probabilistic distribution over detected objects [ 64]. The second type is Boolean that\nis either True or False. Finally, the Answer type is a distribution over all answer candidates. We\nacquire the ground true execution results by executing ground true programs on the ground true scene\ngraphs. Since the dimensions of results for different types of routines are different, we use MLPs\nwith different output dimensions to decode the result embeddings of different routines. Note that\nProTo executes the program in a latent space, so no explicit inputs are directly sent to ProTo. Only\nlatent embeddings are sent into ProTo. The latent result embeddings are grounded to the explicit\nresults via execution losses.\nAll the routines on the GQA datasets are single-step routines, which are ﬁnished via only a single\nforward step. In other words, we always update the pointer at each execution step.\nB.2 Program-guided Policy Learning\nWe list different types of routines on the 2D Minecraft datasets in Table 5. In this dataset, the entrance\nroutine is the ﬁrst routine in the program, and the exiting routines are routines that might be executed\nat the end of the program execution 4.\nAll routines in 2D Minecraft datasets take the speciﬁcation as input and output either actions or\nBoolean results. The actions can either be motor actions or interactive actions as described in Sec 5.2\nof the main text. Like the GQA experiments, we also use MLPs with different output dimensions to\ndecode the results embeddings.\nThe list of ending conditions is also provided in Table 5. Note that the PGA baseline [79] uses the\nsame set of end conditions.\nWe visualize the distribution of program lengths on both GQA and Minecraft in Figure D6.\n4In branching cases, the last routines of both branches are possibly executed, so they are all exiting routines.\n5\nC Algorithm Details\nC.1 Derive Parents of Routines\nFor the GQA dataset, the parents of routines are provided in the ground truths. For example, in\nthe Figure 2 of main text, the parents of the third routine (Verify_relation(left)) are the ﬁrst\nroutine (Select(bag)) and the second routine (Select(wine)).\nFor the Minecraft dataset, the parent of one routine is the previously executed routine. Each routine\nonly depends on the previous routine, and no routines rely on more than one routine.\nC.2 Parallel Execution\nSince ProTo is a transformer-like [85] architecture, it has a promising ability to execute many routines\nin parallel when they have no result dependency. A typical example is shown in Figure C5. The\nparallel version of algorithms is shown in Algorithm 3 and Algorithm 4. We adopt a vector of pointers\np to point to different routines executed in parallel.\nThe semantic guidance remains the same as Eq. 1. We revise the structure guidance P to support\nexecuting many routines in one time as the following equation:\nPt(τ)[i][j] =\n{0 if (∃p∈p, s.t. Pj ∈Parents(Pp) and Pi = Pp) or (i= j),\n−∞ else. (6)\nParallel execution is only enabled in the GQA experiments. On the Minecraft datasets, there is only\none agent who needs to perform all the required tasks. So the routines in the Minecraft experiments\ncan not be executed in parallel. We expect this parallel execution feature can be tested on a multi-agent\nenvironment [57] in the future.\nNote that in the main text, we explain our approach sequentially for ease of understanding.\nAlgorithm 3:ProTo Execution in Parallel\nResult: Execution results {o(τ)}T\nτ=1\n1 Initialize τ = 0 and a pointer;\n2 Build Ps according to Eq. 1;\n3 while not reach an exiting routine do\n4 Observe s(τ) and set τ = τ + 1 ;\n5 if there are routines that can be\nexecuted in parallel then\n6 Spawn a vector of pointers p to\npoint to those routines;\n7 Build Pt(τ) according to Eq. 6;\n8 o(τ) =\nProToInfer(s(τ),Ps,Pt(τ),p);\n9 Output o(τ) ;\n10 ParaUpdatePointer(p, o(τ)) ;\n11 end\nAlgorithm 4:ParaUpdatePointer(p, o(τ))\n1 if parallel routines ﬁnishes execution then\n2 Merge pointers p.\n3 for one pointer pin p do\n4 if Pp does not ﬁnish execution then\n5 return\n6 if Pp is an If-routine then\n7 Point pto the ﬁrst routine in the T/F\nbranch if the result of Pp is T/F;\n8 else ifPp is a While-routine then\n9 Point pto the ﬁrst routine\ninside/outside the loop if the result\nof Pp is T/F;\n10 else ifPp ends a loop then\n11 Point pto the loop condition routine.\n12 else\n13 Point pto the subsequent routine;\n14 end\n15 end\nD Experimental Details and Further Results\nIn this section, we provide more details and further results on two experimented tasks.\n6\nFigure C5: An example of ProTo parallel execution. We show the program and its semantics in (a)\nand (b). The speciﬁcation is shown in (c). We show the execution ﬂow in (d) where we can see we\nexecute two routines at τ = 2. In (e), we show the semantic guidance and the structure guidance of\nProTo execution process.\nD.1 Program-guided Visual Reasoning\nD.1.1 Program Synthesis Model\nWe adopt a simple transformer-based seq2seq model [85] to translate a natural language question\ninto a program. Both the encoder and the decoder of the seq2seq model are composed of six identical\nself-attention layers with hidden feature dimensions dh = 512. The head number is eight.\nThe input question is encoded in a token-by-token manner via a learnable dictionary φq. We turn the\nground true program into a sequence by traversing the program tree via pre-order traverse. Segment\ntokens [SEG] are added between two routines. The predicted sequential program can be recovered\nvia a reverse way. We used beam search with a beam size of 4 and length penalty α= 0.6 [85]. The\nvalidation accuracy of this model is 98.1%.\nD.1.2 Program Representation\nIn the semantic part of the program, we set dm = 256, L= 8 and d= 2048. In the implementation\nof the structure part, we use −1 ×109 as the negative inﬁnity, which is the same as the standard\nimplementation of masked attention in transformers [85].\nD.1.3 Visualizations\nWe provide more visualization of ProTo on the validation datasets in comparison to the concurrent\nwork meta module networks (MMN) [15]. The implementation and hyperparameters of meta module\nnetworks follow their ofﬁcial code. Speciﬁcally, for the Objects types of results, we visualize the\npredicted object with a probability p> 0.5. For the results with type Boolean or Answer, we present\nthe choice with maximum probability. The visualizations are shown in Figure 1 and Figure 2.\nD.1.4 Details of the GAT Encoding\nDuring the ablation study, we compare our model to graph attention networks (GAT) [86]. The node\nfeatures are semantic embeddings of routines, and the edges represent message passing relationships.\n7\nSpeciﬁcally, the node features are constructed via a concatenation of word embeddings, which is the\nsame as Eq. 1. The embedding dimension is the same as ProTo. One edge exists between a routine\nand its parents as in Eq. 2. Note that since the GQA programs do not have conditional routines such\nas While and If, the edges are determined before execution. The routine embeddings are fed to two\ncross-attention modules to fuse information of result embeddings and speciﬁcations following Eq 3\nand Eq 5. We also use an MLP to decode the latent results to get explicit routine results.\nThe GAT model consists of three layers, where each layer has eight attention heads with 256 features,\nfollowing by an ELU nonlinearity.\nD.1.5 Details of Generalization Experiments\nPurpose of Collecting Additional Human-written ProgramsWe have the following reasons for\ncollecting the human-written programs. First, we are curious whether humans can communicate\nwith machines via programs, which has not been done by previous work before. Second, the GQA\nquestions and programs are synthetic, and many of the programs are awkward (e.g., with many\nunnecessary modiﬁers such as \"the baked good that is on the top of the plate that is on the left of the\ntray\"). Third, the GQA test split programs are not publicly available, and the translated programs from\nthe questions may be inaccurate. Since the validation split has been used for parameter tuning, we\nwish to benchmark program-guided visual reasoning on the collected independent data points. Forth,\nthis small-scale dataset lays the ground for the construction of our novel dataset for program-guided\ntasks.\nGQA-Humam-Program Dataset Collection Process For the Human Program Guidance exper-\niments, we create the GQA-Humam-Program dataset to diagnose whether humans can guide the\nreasoning process via programs. We employ ﬁve volunteers to write 500 programs and answers on\nthe GQA validation dataset. The estimated hourly wage is ten dollars, and the total amount spent\non volunteers is two thousand dollars. A parser checks the written programs to ensure that they\nfollow the domain speciﬁcation language of GQA. We encourage the volunteers to write longer and\nmore complex programs. Two volunteers cross-check the correctness of programs and answers. For\nfairness of comparison, we retrain the meta neural module networks [15] on the training split of GQA\nwhile preventing it from seeing the natural language questions. The screenshot of the data collection\ntool is provided in Figure D7.\nRationale Behind Unseen Programs ExperimentsIn the experiments of Unseen Programs, the\nmodels are required to learn combinatorial word-level semantics to execute unseen programs. The\ntraining set contains verify_size and filter_shape, the models may generalize compositionally\nto the unseen program verify_shape.\nRestricted Data Regime ExperimentsWe repeat for three random seeds and found the standard\ndeviation of the results is smaller than 2%.\nD.1.6 Computational Resources\nWe train our model and the baselines on a 48 core Ubuntu 16.04 Linux server with eight Nvidia\nTitan-X GPU. The CPU is Intel Silver 4116 CPU @ 2.10GHz. The total training time is around 48\nhours.\nD.1.7 License and Permissions\nThe GQA dataset is built upon Visual Genome [53], which is under Creative Commons Attribution\n4.0 International License. The GQA dataset is publicly available so that we can use it for research\npurposes. The GQA dataset is used in many published literature [ 47, 45], and we do not found\noffensive content in this dataset.\n8\nFigure D6: The program length distribution on the GQA and the Minecraft dataset.\nD.2 Program-guided Policy Learning\nD.2.1 Environment Details\nThe major environmental resources that the agent can interact with are gold, wood, or iron. The\nenvironment may contain a river, and the agent cannot go across unless a bridge is built. The size of\nthe grid world ranges from ﬁve to eight. There could be two to four merchants in the environment.\nThe environment is randomly initialized, and the agent is also randomly initialized while ensuring the\nprogram can be ﬁnished. If the agent fails to ﬁnish the program within 300 timesteps, the execution\nwould be terminated (timeout).\nD.2.2 A2C and Hyper-parameters\nWe use the same implementation of the A2C algorithm as [79] (provided by its authors). The A2C\nalgorithm uses a learning rate of 1 ×10−3, 64 environments running in parallel with 64 number of\nworkers. The number of roll-out steps for each update is ﬁve. The agent is trained for 107 timesteps.\nThe balance of the entropy regularization term is β = 0.1.\nD.2.3 More Visualizations\nWe provide more visualizations of the results on the test splits in Figure 3 and Figure 4.\nD.2.4 Architecture Details and Computational Costs\nWe list the computational costs and details of ProTo and the baselines in Table 6.\nThe server that we used is the same as the GQA experiments, but we only use a single GPU in the\nMinecraft experiments following [79]. The total training time is around 80 hours.\nD.2.5 Details about the Planner Used in Dense Supervision\nWe create a planner to generate ground true execution traces to train the ProTo baselines with\nfull supervision. The planner uses a hard-coded interpreter to parse the programs. For all the\nactions that need navigation, the planner uses an A* search algorithm [26] to ﬁnd the shortest path.\nFor the Bridge, Mine and Sell action, we would ﬁnd the nearest river, the nearest item or the\nnearest merchant. For the If and While routines, the planner uses the symbolic information in the\nenvironment (e.g. env[gold]=3) to decide whether the conditions are satisﬁed.\n9\nFigure D7: Screenshot of the UI for the construction of the GQA-Humam-Program dataset. The\nsubmitted programs are checked by the parser and the collected answers are cross-checked by two\nmore annotators.\nD.2.6 License and Permissions\nThe Minecraft dataset is under Creative Commons Attribution 4.0 International License. We acquire\nthis dataset and its license from the authors of [79]. Since it’s a synthetic dataset, we don’t think it\nhas offensive content.\nE Additional Discussions about the Neural-Symbolic Baselines\nThe comparison between different neural symbolic approaches is a signiﬁcant aspect of our work.\nOn the ﬁrst domain of visual reasoning, our paper shows that a mixed parametric neural-symbolic\nmodel would outperform pure symbolic non-parametric executors (Table 2). On the second domain of\n10\nTable 5: All types of routines in the domain-speciﬁc language of the 2D MineCraft dataset [79]. We\nuse Spec. to denote speciﬁcation. Some routines require many arguments, which are numbered by\nintegers with brackets (e.g., (1), (2), etc.).\nType Arguments Input Output Semantics and\nEnding Conditions\nMine Triangle, circle, rectangle,\ngold, wood, or iron\nSpec. Action\nGo to the item and pick it up.\nEnded when the action\n\"Mine\" is predicted and the\nagent successfully mined a\ngold.\nBuildBridge - Spec. Action\nGo to the river and build a\nbridge.\nEnded when the action\n\"Bridge\" is predicted and the\nagent successfully builds a\nbridge.\nGoto Coordinates Spec. Action\nGo to the coordinates.\nEnded when the agent\nreaches the target.\nPlace\n(1) Triangle, circle\nor rectangle\n(2) Coordinates\nSpec. Action\nPlace the object on the spec-\niﬁed coordinates.\nEnded when the action\n\"Place\" is predicted.\nSell Triangle, circle, rectangle,\ngold, wood, or iron\nSpec. Action\nGo to the merchant and sell\nmined items.\nEnded when the action\n\"Sell\" is predicted.\nIf / If-Else\n(1) Agent, env, or is_there\n(2) Gold, wood, iron,\nbridge, river, merchant,\nwall, or ﬂat\n(3) Operators\n(>,≥,=,<, ≤)\n(4) Number\nSpec. Boolean\nPerform branching based\non the condition in the if-\nclause.\nEnded in a single execution\nstep.\nWhile\n(1) Agent, env, or is_there\n(2) Gold, wood, iron,\nbridge, river, merchant,\nwall, or ﬂat\n(3) Operators\n(>,≥,=,<, ≤)\n(4) Number\nSpec. Boolean\nPerform looping based on\nthe condition in the while-\nclause.\nEnded in a single execution\nstep.\nMinecraft, we have compared neural baselines (Table 4). We also have a pure symbolic (non-learning)\nmethod: the planner. The advantages of the learned executor beyond the symbolic planner are as\nfollows.\nFirst, the hard-coded planner requires a lot of ad-hoc engineering work to handle complex cases. But\nour method can learn from a sparse reward signal. For example, our method can learn to build a\nbridge to cross the river to fetch gold on the other side of the river without explicit supervision (just\ngiven a reward signal). However, a planner needs to handle this case with special treatment.\nSecond, we experiment on Minecraft to validate the ability of ProTo to generalize across unseen\nroutines. Speciﬁcally, we remove a routine from the training split (e.g., Mine(Gold)) and test on pro-\ngrams that contain the removed routine. This experiment validates the compositional generalization\nability (e.g., generalize to Mine(Gold) after seeing Is_there(gold)and Mine(Silver)). Only a\n11\nTable 6: Architecture details of ProTo and the baselines on the 2D MineCraft dataset.\nModel Parameters Architecture Details\nProTo 1.30M Eight attention heads with an intermediate size of 64; shared\ncomputation between routines; the state map is encoded\nvia a two-layer MLP with hidden size 256 and output size\nd = 2048; the inventory is encoded via another two-layer\nMLP with hidden size 128 and output size d = 2048; the\ninventory feature is added to the state map to produce the\nﬁnal speciﬁcation feature; the output MLP is also a two-layer\nMLP with hidden size 256.\nPGA 1.21M The state map is encoded via a batch of CNNs with channel\nsizes of 32, 64, 96, and 128. Each convolutional layer has\nkernel size three and stride 2, which is followed by ReLU\nnonlinearity. The inventory is encoded via a two-layer MLP\nwith a channel size of 256. The goal is encoded via a two-\nlayer MLP with a channel size of 64. The features are fused\nvia a modulation mechanism proposed by PGA [79].\nVanilla Transformer 2.63M Two attentional layers stacked, with eight attention heads, a\nhidden size of 128, and an intermediate size of 256.\nTree-RNN 0.51M Program embeddings are of dimension 128. Attention LSTM\nsize of 128. Tree-RNN uses a composition module to aggre-\ngate all the children representation of a node, which is of\nsize [128 × 128], and output projection weights of size [128\n× 128], with a bias of size 128. The program embeddings\nare average pooled across one routine so that each routine\nwill be mapped to a ﬁxed dimension. The composition layer\nis applied when combining pooled embedding from all the\nchildren of a node.\nreward signal is used for training. Other experimental details are the same as described in Sec 5.2.\nThe planner is set to choose a random legal action when meeting an unseen routine). The results on\nthe validation dataset are presented in Tab 7.\nThird, the planner cannot scale up to a large number of states. A planner cannot work well on a\nlarge-scale scenario such as the game GO [74].\nForth, the planner can never work on raw image observations. Note our transformer-based architecture\ncan work on raw image input after dividing the observation into patches [25] or detecting objects in\nthe raw image. But the symbolic planner has no way to work on raw image inputs.\nF Limitations\nDespite our contributions to task formulation and ProTo models, our work has several limitations.\nFirst, since the program-based approaches need to leverage the program guidance and dense super-\nvision, it cannot easily leverage large-scale datasets for self-supervised pretraining [61]. We would\nwork on building large-scale datasets with program annotations to alleviate this limitation. Second,\nTable 7: Comparison between the generalization ability of the planner and ProTo on the Minecraft\ndataset.\nRemoved Routine Planner Acc ProTo Acc\nMine(Gold) 11.4 59.3\nIs_there(River) 43.8 65.6\nAgent[Silver] 46.2 77.1\n12\nwe hypothesize that one can still improve proTo’s architecture design since the community has not\nexploited the power of transformers. We would incorporate recent advances in transformers [59, 96]\nto improve ProTo. Furthermore, we could conduct more ablation studies to reveal the importance of\ndifferent components in ProTo.\nG Broader Impact\nOur ﬁndings provide a simple yet effective approach to address program-guided tasks. Potentially,\npeople may leverage ProTo models to instruct robots via programs [ 68]. However, the learned\nexecutors may be attacked by adversarial training [62]. And instructing machines via programs might\nrequire humans to have more advanced knowledge (e.g., knowing the basic concepts of programs\nand how to follow program syntax). Therefore, those of a low educational level may not be able to\nleverage the beneﬁts of our approach, which might aggravate social inequalities.\n13\nTable 8: All types of routines in the domain-speciﬁc language of the GQA dataset [47].\nType Arguments Input Output Semantics\nSelect/Filter Position, color, mate-\nrial, shape, or activ-\nity\nObjects Objects Filter out a set of ob-\njects by the positions,\ncolors, etc from the\ninput objects.\nChoose Name, scene, color,\nshape, position or at-\ntributes\nObjects Answer Choose one answer\n(e.g., name, scene,\ncolor) from given an-\nswer candidates.\nVerify Color, shape, scene,\nor relation\nObjects Boolean Verify whether the\ngiven concepts (e.g.,\ncolor, shape) holds\ntrue for the input ob-\njects.\nRelate\nInverseRelate Name, or attributes Objects Objects Filter out a set of\nobjects that have\nthe relation con-\ncept (e.g., names,\nattributes) with the\ninput objects.\nQuery Name, color, shape,\nscene or position\nObjects Answer Query the concept\n(e.g., name, color) of\nthe input objects.\nCommon Color, or material Two objects Answer Query the common\nconcepts (e.g., color,\nmaterial) of the input\nobjects.\nDifferent Name, color, or ma-\nterial\nTwo objects Boolean Return whether\nthe concepts of the\nobjects (e.g., name,\ncolor) are different.\nSame Name or color Two objects Boolean Return whether\nthe concepts of the\nobjects (e.g., name,\ncolor) are same.\nAnd - Two booleans Boolean Return whether the\ntwo input booleans\nare both True.\nOr - Two booleans Boolean Return whether one\nof the two input con-\nditions is True.\nExist - Objects Boolean Return whether the\ninput object set is not\nempty.\n14\nReferences\n[1] Frances E Allen. Control ﬂow analysis. ACM Sigplan Notices, 5(7):1–19, 1970.\n[2] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei\nZhang. Bottom-up and top-down attention for image captioning and visual question answering. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pp. 6077–6086, 2018.\n[3] Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen\nGould, and Anton Van Den Hengel. Vision-and-language navigation: Interpreting visually-grounded\nnavigation instructions in real environments. In Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pp. 3674–3683, 2018.\n[4] David Andre and Stuart J Russell. Programmable reinforcement learning agents. PhD thesis, University of\nCalifornia, Berkeley, 2003.\n[5] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick,\nand Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on\ncomputer vision, pp. 2425–2433, 2015.\n[6] Brenna D Argall, Sonia Chernova, Manuela Veloso, and Brett Browning. A survey of robot learning from\ndemonstration. Robotics and autonomous systems, 57(5):469–483, 2009.\n[7] Andrew G Barto and Sridhar Mahadevan. Recent advances in hierarchical reinforcement learning. Discrete\nevent dynamic systems, 13(1):41–77, 2003.\n[8] Osbert Bastani, Yewen Pu, and Armando Solar-Lezama. Veriﬁable reinforcement learning via policy\nextraction. arXiv preprint arXiv:1805.08328, 2018.\n[9] Tarek R. Besold, Artur S. d’Avila Garcez, Sebastian Bader, Howard Bowman, Pedro M. Domingos, Pascal\nHitzler, Kai-Uwe Kühnberger, Luís C. Lamb, Daniel Lowd, Priscila Machado Vieira Lima, Leo de Penning,\nGadi Pinkas, Hoifung Poon, and Gerson Zaverucha. Neural-symbolic learning and reasoning: A survey\nand interpretation. CoRR, abs/1711.03902, 2017. URL http://arxiv.org/abs/1711.03902.\n[10] David Bieber, Charles Sutton, Hugo Larochelle, and Daniel Tarlow. Learning to execute programs\nwith instruction pointer attention graph neural networks. CoRR, abs/2010.12621, 2020. URL https:\n//arxiv.org/abs/2010.12621.\n[11] Paul Booth. An introduction to human-computer interaction (psychology revivals). Psychology Press,\n2014.\n[12] Rudy Bunel, Matthew Hausknecht, Jacob Devlin, Rishabh Singh, and Pushmeet Kohli. Leveraging\ngrammar and reinforcement learning for neural program synthesis. arXiv preprint arXiv:1805.04276, 2018.\n[13] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey\nZagoruyko. End-to-end object detection with transformers. In European Conference on Computer Vision,\npp. 213–229. Springer, 2020.\n[14] David Chen and Raymond Mooney. Learning to interpret natural language navigation instructions from\nobservations. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 25, 2011.\n[15] Wenhu Chen, Zhe Gan, Linjie Li, Yu Cheng, William Wang, and Jingjing Liu. Meta module network for\ncompositional visual reasoning. In Proceedings of the IEEE/CVF Winter Conference on Applications of\nComputer Vision, pp. 655–664, 2021.\n[16] Xinyun Chen, Chang Liu, Richard Shin, Dawn Song, and Mingcheng Chen. Latent attention for if-then\nprogram synthesis. arXiv preprint arXiv:1611.01867, 2016.\n[17] Xinyun Chen, Chang Liu, and Dawn Song. Towards synthesizing complex programs from input-output\nexamples. arXiv preprint arXiv:1706.01284, 2017.\n[18] Xinyun Chen, Chang Liu, and Dawn Song. Execution-guided neural program synthesis. In International\nConference on Learning Representations, 2018.\n[19] Xinyun Chen, Chen Liang, Adams Wei Yu, Denny Zhou, Dawn Song, and Quoc V Le. Neural symbolic\nreader: Scalable integration of distributed and symbolic representations for reading comprehension. In\nInternational Conference on Learning Representations, 2019.\n[20] Xinyun Chen, Chen Liang, Adams Wei Yu, Dawn Song, and Denny Zhou. Compositional generalization\nvia neural-symbolic stack machines. arXiv preprint arXiv:2008.06662, 2020.\n15\n[21] Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, and Dhruv Batra. Embodiedques-\ntionanswering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.\n1–10, 2018.\n[22] Jacob Devlin, Rudy Bunel, Rishabh Singh, Matthew Hausknecht, and Pushmeet Kohli. Neural program\nmeta-induction. arXiv preprint arXiv:1710.04157, 2017.\n[23] Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh Singh, Abdel-rahman Mohamed, and Pushmeet\nKohli. Robustﬁll: Neural program learning under noisy i/o. In International conference on machine\nlearning, pp. 990–998. PMLR, 2017.\n[24] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n[25] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\n16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n[26] František Duchoˇn, Andrej Babinec, Martin Kajan, Peter Beˇno, Martin Florek, Tomáš Fico, and Ladislav\nJurišica. Path planning with modiﬁed a star algorithm for a mobile robot. Procedia Engineering, 96:59–69,\n2014.\n[27] Kevin Ellis, Daniel Ritchie, Armando Solar-Lezama, and Joshua B Tenenbaum. Learning to infer graphics\nprograms from hand-drawn images. arXiv preprint arXiv:1707.09627, 2017.\n[28] Zhihao Fan, Yeyun Gong, Dayiheng Liu, Zhongyu Wei, Siyuan Wang, Jian Jiao, Nan Duan, Ruofei Zhang,\nand Xuanjing Huang. Mask attention networks: Rethinking and strengthen transformer. arXiv preprint\narXiv:2103.13597, 2021.\n[29] John K Feser, Marc Brockschmidt, Alexander L Gaunt, and Daniel Tarlow. Differentiable functional\nprogram interpreters. arXiv preprint arXiv:1611.01988, 2016.\n[30] Richard E Fikes and Nils J Nilsson. Strips: A new approach to the application of theorem proving to\nproblem solving. Artiﬁcial intelligence, 2(3-4):189–208, 1971.\n[31] Roy Fox, Richard Shin, Sanjay Krishnan, Ken Goldberg, Dawn Song, and Ion Stoica. Parametrized\nhierarchical procedures for neural programming. ICLR 2018, 2018.\n[32] Daniel Fried, Ronghang Hu, V olkan Cirik, Anna Rohrbach, Jacob Andreas, Louis-Philippe Morency,\nTaylor Berg-Kirkpatrick, Kate Saenko, Dan Klein, and Trevor Darrell. Speaker-follower models for\nvision-and-language navigation. arXiv preprint arXiv:1806.02724, 2018.\n[33] Peng Gao, Jiasen Lu, Hongsheng Li, Roozbeh Mottaghi, and Aniruddha Kembhavi. Container: Context\naggregation network. CoRR, abs/2106.01401, 2021. URL https://arxiv.org/abs/2106.01401.\n[34] Shuzheng Gao, Cuiyun Gao, Yulan He, Jichuan Zeng, Lun Yiu Nie, and Xin Xia. Code structure guided\ntransformer for source code summarization. CoRR, abs/2104.09340, 2021. URL https://arxiv.org/\nabs/2104.09340.\n[35] Artur d’Avila Garcez, Marco Gori, Luis C Lamb, Luciano Seraﬁni, Michael Spranger, and Son N Tran.\nNeural-symbolic computing: An effective methodology for principled integration of machine learning and\nreasoning. arXiv preprint arXiv:1905.06088, 2019.\n[36] Artur SD’Avila Garcez, Luis C Lamb, and Dov M Gabbay.Neural-symbolic cognitive reasoning. Springer\nScience & Business Media, 2008.\n[37] Felix A Gers, Jürgen Schmidhuber, and Fred Cummins. Learning to forget: Continual prediction with lstm.\nNeural Computation, 1999.\n[38] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint arXiv:1410.5401,\n2014.\n[39] Sumit Gulwani. Program synthesis. Software Systems Safety, pp. 43–75, 2014.\n[40] Stevan Harnad. Grounding symbols in the analog world with neural nets. Think, 2(1):12–78, 1993.\n[41] Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. arXiv preprint\narXiv:1606.03476, 2016.\n16\n[42] Ronghang Hu, Jacob Andreas, Trevor Darrell, and Kate Saenko. Explainable neural computation via stack\nneural module networks. In Proceedings of the European conference on computer vision (ECCV) , pp.\n53–69, 2018.\n[43] Haoyang Huang, Yaobo Liang, Nan Duan, Ming Gong, Linjun Shou, Daxin Jiang, and Ming Zhou.\nUnicoder: A universal language encoder by pre-training with multiple cross-lingual tasks. arXiv preprint\narXiv:1909.00964, 2019.\n[44] Wenlong Huang, Igor Mordatch, and Deepak Pathak. One policy to control them all: Shared modular\npolicies for agent-agnostic control. In International Conference on Machine Learning, pp. 4455–4464.\nPMLR, 2020.\n[45] Drew Hudson and Christopher D Manning. Learning by abstraction: The neural state machine. InAdvances\nin Neural Information Processing Systems, pp. 5903–5916, 2019.\n[46] Drew A Hudson and Christopher D Manning. Compositional attention networks for machine reasoning. In\nInternational Conference on Learning Representations, 2018.\n[47] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and\ncompositional question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pp. 6700–6709, 2019.\n[48] Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross\nGirshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2901–2910, 2017.\n[49] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Judy Hoffman, Li Fei-Fei, C Lawrence Zit-\nnick, and Ross Girshick. Inferring and executing programs for visual reasoning. In Proceedings of the\nIEEE International Conference on Computer Vision, pp. 2989–2998, 2017.\n[50] Łukasz Kaiser and Ilya Sutskever. Neural gpus learn algorithms. arXiv preprint arXiv:1511.08228, 2015.\n[51] Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. InAdvances in neural information processing\nsystems, pp. 1008–1014. Citeseer, 2000.\n[52] George Konidaris, Scott Kuindersma, Roderic Grupen, and Andrew Barto. Robot learning from demon-\nstration by constructing skill trees. The International Journal of Robotics Research , 31(3):360–375,\n2012.\n[53] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen,\nYannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision\nusing crowdsourced dense image annotations. International journal of computer vision, 123(1):32–73,\n2017.\n[54] Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning through\nprobabilistic program induction. Science, 350(6266):1332–1338, 2015.\n[55] Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building machines\nthat learn and think like people. Behavioral and brain sciences, 40, 2017.\n[56] Guohao Li, Xin Wang, and Wenwu Zhu. Perceptual visual reasoning with knowledge propagation. In\nProceedings of the 27th ACM International Conference on Multimedia, MM ’19, pp. 530–538, New York,\nNY , USA, 2019. Association for Computing Machinery. ISBN 9781450368896. doi: 10.1145/3343031.\n3350922. URL https://doi.org/10.1145/3343031.3350922.\n[57] Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In Machine\nlearning proceedings 1994, pp. 157–163. Elsevier, 1994.\n[58] Yunchao Liu and Zheng Wu. Learning to describe scenes with programs. In International Conference on\nLearning Representations, 2019.\n[59] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin\ntransformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030,\n2021.\n[60] Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. Hierarchical question-image co-attention for\nvisual question answering. arXiv preprint arXiv:1606.00061, 2016.\n17\n[61] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic\nrepresentations for vision-and-language tasks. arXiv preprint arXiv:1908.02265, 2019.\n[62] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards\ndeep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.\n[63] Robin Manhaeve, Sebastijan Dumancic, Angelika Kimmig, Thomas Demeester, and Luc De Raedt.\nDeepproblog: Neural probabilistic logic programming. In S. Bengio, H. Wallach, H. Larochelle, K. Grau-\nman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems, vol-\nume 31. Curran Associates, Inc., 2018. URLhttps://proceedings.neurips.cc/paper/2018/file/\ndc5d637ed5e62c36ecb73b654b05ba2a-Paper.pdf.\n[64] Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B. Tenenbaum, and Jiajun Wu. The Neuro-Symbolic\nConcept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision. In Interna-\ntional Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=\nrJgMlhRctm.\n[65] Jiayuan Mao, Xiuming Zhang, Yikai Li, William T. Freeman, Joshua B. Tenenbaum, and Jiajun Wu.\nProgram-guided image manipulators. CoRR, abs/1909.02116, 2019. URL http://arxiv.org/abs/\n1909.02116.\n[66] Saif M Mohammad. Sentiment analysis: Detecting valence, emotions, and other affectual states from text.\nIn Emotion measurement, pp. 201–237. Elsevier, 2016.\n[67] Ronald Parr and Stuart Russell. Reinforcement learning with hierarchies of machines. Advances in neural\ninformation processing systems, pp. 1043–1049, 1998.\n[68] Dean A Pomerleau, Jay Gowdy, and Charles E Thorpe. Combining artiﬁcial neural networks and symbolic\nprocessing for autonomous robot guidance. Engineering Applications of Artiﬁcial Intelligence , 4(4):\n279–285, 1991.\n[69] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models\nare unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n[70] Scott Reed and Nando De Freitas. Neural programmer-interpreters. arXiv preprint arXiv:1511.06279,\n2015.\n[71] Stefan Schaal et al. Learning from demonstration. Advances in neural information processing systems, pp.\n1040–1046, 1997.\n[72] Kevin J Shih, Saurabh Singh, and Derek Hoiem. Where to look: Focus regions for visual question\nanswering. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp.\n4613–4621, 2016.\n[73] Richard Shin, Illia Polosukhin, and Dawn Song. Towards speciﬁcation-directed program repair, 2018.\nURL https://openreview.net/forum?id=B1iZRFkwz.\n[74] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian\nSchrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go\nwith deep neural networks and tree search. nature, 529(7587):484–489, 2016.\n[75] Sungryull Sohn, Junhyuk Oh, and Honglak Lee. Hierarchical reinforcement learning for zero-shot\ngeneralization with subtask dependencies. arXiv preprint arXiv:1807.07665, 2018.\n[76] Chunfeng Song, Yan Huang, Wanli Ouyang, and Liang Wang. Mask-guided contrastive attention model\nfor person re-identiﬁcation. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), June 2018.\n[77] Chen Sun, Austin Myers, Carl V ondrick, Kevin Murphy, and Cordelia Schmid. Videobert: A joint model\nfor video and language representation learning. In Proceedings of the IEEE/CVF International Conference\non Computer Vision, pp. 7464–7473, 2019.\n[78] Shao-Hua Sun, Hyeonwoo Noh, Sriram Somasundaram, and Joseph Lim. Neural program synthesis from\ndiverse demonstration videos. In International Conference on Machine Learning, pp. 4790–4799. PMLR,\n2018.\n[79] Shao-Hua Sun, Te-Lin Wu, and Joseph J Lim. Program guided agent. In International Conference on\nLearning Representations, 2019.\n18\n[80] Kai Sheng Tai, Richard Socher, and Christopher D Manning. Improved semantic representations from\ntree-structured long short-term memory networks. arXiv preprint arXiv:1503.00075, 2015.\n[81] Hao Tan and Mohit Bansal. Lxmert: Learning cross-modality encoder representations from transformers.\nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 5103–5114, 2019.\n[82] Yee Whye Teh, Victor Bapst, Wojciech Marian Czarnecki, John Quan, James Kirkpatrick, Raia Hadsell,\nNicolas Heess, and Razvan Pascanu. Distral: Robust multitask reinforcement learning. arXiv preprint\narXiv:1707.04175, 2017.\n[83] Yonglong Tian, Andrew Luo, Xingyuan Sun, Kevin Ellis, William T Freeman, Joshua B Tenenbaum, and\nJiajun Wu. Learning to infer and execute 3d shape programs. arXiv preprint arXiv:1901.02875, 2019.\n[84] Dweep Trivedi, Jesse Zhang, Shao-Hua Sun, and Joseph J. Lim. Learning to synthesize programs as\ninterpretable and generalizable policies, 2021.\n[85] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017.\n[86] Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio.\nGraph attention networks. arXiv preprint arXiv:1710.10903, 2017.\n[87] Abhinav Verma, Vijayaraghavan Murali, Rishabh Singh, Pushmeet Kohli, and Swarat Chaudhuri. Pro-\ngrammatically interpretable reinforcement learning. In International Conference on Machine Learning, pp.\n5045–5054. PMLR, 2018.\n[88] Ricardo Vilalta and Youssef Drissi. A perspective view and survey of meta-learning. Artiﬁcial intelligence\nreview, 18(2):77–95, 2002.\n[89] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan L. Yuille, and Liang-Chieh Chen. Max-deeplab: End-\nto-end panoptic segmentation with mask transformers. CoRR, abs/2012.00759, 2020. URL https:\n//arxiv.org/abs/2012.00759.\n[90] Aaron Wilson, Alan Fern, Soumya Ray, and Prasad Tadepalli. Multi-task reinforcement learning: a\nhierarchical bayesian approach. In Proceedings of the 24th international conference on Machine learning,\npp. 1015–1022, 2007.\n[91] Jiajun Wu, Joshua B Tenenbaum, and Pushmeet Kohli. Neural scene de-rendering. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition, pp. 699–707, 2017.\n[92] Da Xiao, Jo-Yu Liao, and Xingyuan Yuan. Improving the universality and learnability of neural programmer-\ninterpreters with combinator abstraction. arXiv preprint arXiv:1802.02696, 2018.\n[93] Jianwei Yang, Jiayuan Mao, Jiajun Wu, Devi Parikh, David D Cox, Joshua B Tenenbaum, and Chuang\nGan. Object-centric diagnosis of visual reasoning. arXiv preprint arXiv:2012.11587, 2020.\n[94] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le. Xlnet:\nGeneralized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237,\n2019.\n[95] Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, and Joshua B Tenenbaum.\nNeural-symbolic vqa: Disentangling reasoning from vision and language understanding. arXiv preprint\narXiv:1810.02338, 2018.\n[96] Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song,\nJames Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep learning: Training\nbert in 76 minutes. arXiv preprint arXiv:1904.00962, 2019.\n[97] QingLong Zhang and Yubin Yang. Rest: An efﬁcient transformer for visual recognition. CoRR,\nabs/2105.13677, 2021. URL https://arxiv.org/abs/2105.13677.\n[98] Zelin Zhao, Chuang Gan, Jiajun Wu, Xiaoxiao Guo, and Joshua Tenenbaum. Augmenting policy learning\nwith routines discovered from a demonstration. arXiv preprint arXiv:2012.12469, 2020.\n[99] Bolei Zhou, Yuandong Tian, Sainbayar Sukhbaatar, Arthur Szlam, and Rob Fergus. Simple baseline for\nvisual question answering. arXiv preprint arXiv:1512.02167, 2015.\n19",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8042848110198975
    },
    {
      "name": "Transformer",
      "score": 0.6169838309288025
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5111022591590881
    },
    {
      "name": "Generalization",
      "score": 0.4984738826751709
    },
    {
      "name": "Program Design Language",
      "score": 0.49481314420700073
    },
    {
      "name": "Task (project management)",
      "score": 0.4700533151626587
    },
    {
      "name": "Perception",
      "score": 0.45769578218460083
    },
    {
      "name": "Machine learning",
      "score": 0.3846489191055298
    },
    {
      "name": "Programming language",
      "score": 0.3577042520046234
    },
    {
      "name": "Natural language processing",
      "score": 0.3313058614730835
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I177725633",
      "name": "Chinese University of Hong Kong",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I130701444",
      "name": "Georgia Institute of Technology",
      "country": "US"
    }
  ],
  "cited_by": 15
}