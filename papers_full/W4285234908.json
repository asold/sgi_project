{
  "title": "Psycholinguistic Diagnosis of Language Models’ Commonsense Reasoning",
  "url": "https://openalex.org/W4285234908",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5070822759",
      "name": "Yan Cong",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4236822803",
    "https://openalex.org/W1635900978",
    "https://openalex.org/W3035287090",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W3035599593",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3004346089",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2211192759",
    "https://openalex.org/W2169643630",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W3108672584",
    "https://openalex.org/W2060311978",
    "https://openalex.org/W2996728628"
  ],
  "abstract": "Neural language models have attracted a lot of attention in the past few years. More and more researchers are getting intrigued by how language models encode commonsense, specifically what kind of commonsense they understand, and why they do. This paper analyzed neural language models' understanding of commonsense pragmatics (i.e., implied meanings) through human behavioral and neurophysiological data. These psycholinguistic tests are designed to draw conclusions based on predictive responses in context, making them very well suited to test word-prediction models such as BERT in natural settings. They can provide the appropriate prompts and tasks to answer questions about linguistic mechanisms underlying predictive responses. This paper adopted psycholinguistic datasets to probe language models' commonsense reasoning. Findings suggest that GPT-3's performance was mostly at chance in the psycholinguistic tasks. We also showed that DistillBERT had some understanding of the (implied) intent that's shared among most people. Such intent is implicitly reflected in the usage of conversational implicatures and presuppositions. Whether or not fine-tuning improved its performance to human-level depends on the type of commonsense reasoning.",
  "full_text": "Proceedings of the First Workshop on Commonsense Representation and Reasoning (CSRR 2021), pages 17 - 22\nMay 27, 2022 ©2022 Association for Computational Linguistics\nPsycholinguistic Diagnosis of Language Models’ Commonsense Reasoning\nYan Cong\nyancong222@gmail.com\nAbstract\nNeural language models have attracted a lot\nof attention in the past few years. More and\nmore researchers are getting intrigued by how\nlanguage models encode commonsense, specif-\nically what kind of commonsense they under-\nstand, and why they do. This paper analyzed\nneural language models’ understanding of com-\nmonsense pragmatics (i.e., implied meanings)\nthrough human behavioral and neurophysiolog-\nical data. These psycholinguistic tests are de-\nsigned to draw conclusions based on predictive\nresponses in context, making them very well\nsuited to test word-prediction models such as\nBERT in natural settings. They can provide the\nappropriate prompts and tasks to answer ques-\ntions about linguistic mechanisms underlying\npredictive responses. This paper adopted psy-\ncholinguistic datasets to probe language mod-\nels’ commonsense reasoning. Findings suggest\nthat GPT-3’s performance was mostly at chance\nin the psycholinguistic tasks. We also showed\nthat DistillBERT had some understanding of\nthe (implied) intent that’s shared among most\npeople. Such intent is implicitly reflected in\nthe usage of conversational implicatures and\npresuppositions. Whether or not fine-tuning\nimproved its performance to human-level de-\npends on the type of commonsense reasoning.\n1 Introduction\nIn this paper, we focus on Language Models’ (LMs)\nperformance in commonsense reasoning tasks. Dif-\nferent from language semantics concerning logical\nrelations between isolated sentence meanings, we\ntake pragmatics to be sentences’ relations relying\non conversational participants’ commonsense, such\nas the basic level intent that is commonly shared\namong most people. Humans reason about what\ntheir interlocutor could have said but chose not to,\nthereby drawing various inferences. The way hu-\nmans put linguistic meanings to use depends on\nsocial interaction and commonsense assumption.\nWhat about machines whose pre-trainings do not\ninvolve social interaction? To what extent do they\nstill have this pragmatic knowledge? How do they\ncooperate without any forms of learning in Grice\npragmatics (Grice, 1975)? This paper attempts to\nanswer these questions by examining transformer\nLMs’ performance in commonsense reasoning.\nWe focus on two commonsense pragmatics phe-\nnomena: (i) Presupposition (henceforth Presp), for\nexample, by using determiner the in the utterance\n“the teacher spoke to me” most people typically\npresuppose the existence of such a teacher in the\ncontext; (ii) Scalar Implicature (henceforth SI), for\nexample, by using quantifier some in “I ate some of\nthe cookies”, most people generally imply “not all”.\nWe provided linguistic perspectives about how hu-\nmans compute and evaluate commonsense pragmat-\nics. We then assessed the extent to which LMs can\nunderstand the meanings pragmatically enriched\nby human speakers. Moreover, we fine-tuned LMs\nwith pragmatic inference datasets. Evaluation com-\nparisons are reported and discussed. We make all\ncode and test data available for additional testing1.\n2 Related work\nLMs’ knowledge about syntax and semantics is\nrelatively well studied (Warstadt et al., 2020; Ten-\nney et al., 2019; Devlin et al., 2019). Consider-\nably fewer studies have been done on speaker’s in-\ntent: the implied meaning that’s commonly shared\namong most people’s intention. This is called\nConversational Implicature in pragmatics literature\n(Grice, 1975). Implicature phenomena like quan-\ntifiers some and many are tested in recent studies\n(Schuster et al., 2020; Jeretic et al., 2020). The\ndiagnostics in these studies are controlled. Most of\nthem incorporate offline human responses to words\nin context such as acceptability judgment surveys.\nRelatively few studies include online human re-\nsponse in the assessment (Ettinger, 2020). On-\n1https://github.com/yancong222/\nPragamtics-Commonsense-LMs\n17\nline measurement uses neurolinguistic equipment\nelectroencephalogram (EEG) and Event-Related-\nPotentials (ERP) to record brain activity (Luck,\n2012). ERP components such as N400 wave is an\nevent-related brain potential measured using EEG.\nN400 refers to a negativity peaking at about 400\nmilliseconds after stimulus onset. It has been used\nto investigate semantic processing. N400 is rele-\nvant because it’s an online real-time measurement\nof human brain’s response to different language\nphenomena, and it has been mostly elicited as a\nresult of human processing sentences with seman-\ntic anomalies. Online measurement differs from\noffline judgments survey or cloze test in that online\nmeasurement reveals human brain’s real-time sen-\nsitivity to (linguistic) cues. We examine LMs using\nhuman centered datasets that are collected through\nboth offline and online experiments.\nHow “human-like” the state-of-the-art LMs are\n(cognitive plausibility) has not comprehensively\njustified (Wang et al., 2019). Goldstein et al. (2021)\nprovides empirical evidence that the human brain\nand GPT-2 share fundamental computational prin-\nciples as they process natural language. In a sense\nthat both are engaged in continuous next-word pre-\ndiction, and both represent words as a function of\nthe previous context. Against this background, we\nstudy LMs’ cognitive plausibility through examin-\ning their performance in understanding pragmati-\ncally enriched meanings, which are implied or pre-\nsupposed among most people (i.e. conversational\nparticipants) to convey their intentions.\n3 Experiments\nWe first designed most of the tests in the form\nof cloze tasks, so as to test the pre-trained LMs\nin their most natural setting, without interference\nfrom fine-tuning. The main schema we used in\nthis study is called the minimal pair paradigm, in\nwhich two linguistic items are in contrastive distri-\nbution, meaning the two items are identical except\none single aspect. The notion of minimal pair is\nwidely used in linguistic experiments probing the\nunderlying structures of a linguistic utterance. Typ-\nically, one of the two items is pragmatically odd\naccording to most people’s commonsense knowl-\nedge (marked by #), relative to the other utterance\nin the minimal pair.\nThe hypothesis and the accuracy calculation\npipeline are as follows. If LMs understand com-\nmonsense intent, which gets reflected in the usage\nModel nparams nlayers\nDistillBERT-base-uncased 67M 6\nGPT-3/InstructGPT 175.0B 96\nTable 1: (pre-trained LMs) Model cards\nof SI and Presp, LMs should endorse more often\nthe pragmatically good sentence than its pragmati-\ncally odd counterpart in a minimal pair. To quantify\nsuch “endorsement”, we calculated the percentage\np of cases in which LMs favor the pragmatically\ngood sentence over the pragmatically odd one. The\nextent to which LMs (dis-)favor an sentence is de-\nrived from LMs’ tokenized sequence log proba-\nbility (henceforth logprob). The accuracy mean\nfor each condition ( good vs. bad/so-so) is then\ncalculated per phenomenon (SI and Presp), using\nthe sum of percentage p divided by the number of\nsentences, grouped by phenomenon. DistillBERT\n(Sanh et al., 2019) is used, which has only the en-\ncoder transformer, It’s necessary that models are\nable to use right-hand context for word predictions.\nWe compare DistillBERT with another type of LMs\nGPT-3 (Brown et al., 2020), which has only the de-\ncoder. We present model cards in Table (1).\nStudy 1: Presupposition Our first study is built\nup on Singh et al. (2016). They performed human\nbehavioral acceptance judgment experiments us-\ning the presupposition triggers the. Participants\nwere asked to drop out when they think the sen-\ntence stops making sense. Singh et al. (2016)’s\nfindings show that humans think utterances make\nless sense relative to the controls when the presup-\nposed information is implausible. We extracted 82\nitems from Singh et al. (2016) human experiments\nstimuli, which are already cognitively justified and\nfreely available in their appendix. Seth went to jail/\n# a restaurant on Saturday night. The guard spoke\nto him there for a while. presupposes that there\nis a unique guard in the context. Given common-\nsense world knowledge and the close association of\nguard and jail, “Seth went to jail” is a more likely\nand plausible context, thus “a restaurant” is marked\nwith #. Utterance Kristen went to a restaurant/ #\njail in the morning. The waiter served her there\nquickly. presupposes the existence of a (unique)\nwaiter in the context. “Kristen went to a restau-\nrant” is a better context in a sense that it lays out\n18\na background where there is a waiter. By contrast,\njail is rarely associated with waiter, “went to jail”\nis implausible and is marked with #. It’s both the\nuniqueness of the “waiter” and the relevance of\nthe job to the place “restaurant” that affect the con-\ntext. Singh et al. (2016) reported that in this stops-\nmaking-sense paradigm, human participants were\nnear-ceiling in accepting plausible conditions: at\nthe last region of the sentence, the acceptance rate\nwas 95% in the plausible condition. For implausi-\nble the, by the end of the sentence, 50% dropped\nout since it stops making sense and most people\ncannot accept it.\nBuilt up on Singh et al. (2016) human experi-\nment, we evaluated LMs’ sensitivity to Presp. We\ncompared the accuracy mean of each condition, as\nexemplified in John went to school on Monday af-\nternoon. The substitute teacher spoke to him there\nbriefly. versus John went to a concert on Monday\nafternoon. The substitute teacher spoke to him\nthere briefly.. The two utterances differ in only one\nelement “school”/“concert”. The former is prag-\nmatically good relative to the latter, given thatthe\npresupposes a context where there is a teacher, and\ncommonsense tells us that “teacher” and “school”\nare closer than “teacher” and “concert”.\nGPT-3 is evaluated by the extent to which it\nfavors plausible cases over the implausible ones.\nSequential word-by-word logprob is generated and\ntransformed into percent. We take the sum of word\nlevel logprob averaged by sentence length to be\na proxy to the sentence naturalness. Higher per-\ncent indicates that GPT-3 evaluates the sentence\nto be natural. DistillBERT is evaluated through\ncritical word prediction. Noun phrase in the ini-\ntial sentence is masked and taken as the critical\nword. (e.g., school is masked in “ John went to\nschool. The substitute teacher spoke to him there\nbriefly.”, whereas concert is masked in “John went\nto a concert. The substitute teacher spoke to him\nthere briefly.”. Given that human data shows pref-\nerence to the plausible over the implausible, Dis-\ntillBERT is considered to have succeeded if the\ncritical word is in its top K (K=5) tokens for the\nplausible sentence. It’s also considered succeed if\nthe critical word is NOT in BERT’s topK for the\nimplausible sentence.\nStudy 2: Scalar Implicature According to\nNieuwland et al. (2010), relative clauses can make\nimplicatures unnoticed by most people in sentence\nprocessing. Table (2) shows that there is a prag-\nmatic violation in (a) if conversation participant\nactively draws pragmatic inference that “some (but\nnot all)” office buildings have desks. However, this\nviolation is left unnoticed in (a) due to the pres-\nence of the relative clause. (c) is relatively bad and\nimplausible compared to (d): the violation in (c)\nis noticed due to the absence of a relative clause.\nNote that Nieuwland et al. (2010) considered the\nCommunication sub-scale of the Autism-Spectrum\nQuotient questionnaire (AQ) (Baron-Cohen et al.,\n1994, 2001; Baron-Cohen, 2008) to be a proxy to\nbe an individual’s pragmatic skills. According to\nNieuwland et al. (2010), the AQ quantifies prag-\nmatic capabilities on a continuum from autism to\ntypicality.\nNieuwland et al. (2010) reported that only prag-\nmatically skilled participants (i.e., lower autism\nscores) are sensitive to the pragmatic violation in\n(c) (r=-.53, p=0.003). For (a), in which the impli-\ncature is left unnoticed, so is the violation. There\nis thus no significant difference between the prag-\nmatically skilled participants and those who have\nhigh autism scores (r=-.29, p=0.13). Overall prag-\nmatically skilled people are good at generating ro-\nbust pragmatic inferences that some implies not\nall, which gives rise to larger N400 when the utter-\nance is pragmatically bad - N400 is a verified ERP\nelicited by anomaly stimuli (Luck, 2012).\nWe extracted 168 items from Nieuwland et al.\n(2010). Some examples of items from their data\nare “Some people have lungs/pets, which require\ngood care”. GPT-3 is used for sequential word\nprediction. Using sum of token level logprob av-\neraged by sentence length, we examine if there\nis a difference with and without the SI being no-\nticed. GPT-3 is considered succeed if the plausible\nsentence mean is higher (hence more favorable)\nthan the soso/unacceptable sentence mean. We use\nmasked language models like DistillBERT for crit-\nical word prediction. We masked quantifiers and\ntake some as the critical word for (a,b,d). We take\nall as the critical word for (c), because SI is noticed\nand all is commonsense intent. Now that (a,b,c,d)\nare all not implausible, BERT is marked as succeed\nif the critical word is in its top5 tokens list.\nSanity check One may wonder to what extent\nLM is merely leveraging nouns joint-probability.\nThis motivates us to check whether the test datasets\ncontain enough noun co-occurrence patterns that\ncould make the LMs find a likelihood pattern rather\nthan actually reason to conclude which sentence\n19\nPlausibility Example Label\nSo-so (a) [Some] office buildings have desks that are covered with dust. SI unnoticed\nPlausible (b) [Some] office buildings have plants that are covered with dust. SI unnoticed\nImplausible (c) [Some] office buildings have desks and can become dusty. SI noticed\nPlausible (d) [Some] office buildings have plants and can become dusty. SI noticed\nTable 2: Datasets and examples used in SI evaluation (Nieuwland et al., 2010)\nis more plausible. For instance, the co-occurrence\nof office-buildings and desks in the SI good pair\nseems to be more frequently seen than that ofoffice-\nbuildings and plants in the bad pair, since plants\nare not essential, but desks are. Similarly, for the\nPresp stimuli, it appears that humans tend to as-\nsociate jail with guard more frequently than they\ndo so for restaurant and guard. To address these\nconfounding factors, we use n-gram to calculate\njoint-probability (Yin et al., 2016). Results show\nthat 70% of the SI and 50% of the Presp stimuli\nshow higher co-occurrence probability in the ‘good’\nsentence than in the ‘bad’ sentence2.\n4 Fine-tuning DistillBERT with ImpPres\nIn order to examine how to improve LMs’ accuracy\nin these downstream tasks, and to further evaluate\npre-trained LMs versus fine-tuned LMs, we fine-\ntuned DistillBERT-base-uncased with theImpPress\ndataset (Jeretic et al., 2020). It consists of >25k\nsemi-automatically generated sentence pairs illus-\ntrating well-studied commonsense pragmatic infer-\nence types. 14100 tagged utterance pairs were used\nin the training of Presp, and 1410 tagged pairs for\ntesting. Here is the input representation: sentence\n1 Victoria’s mall that has hurt Sam might upset\nHelen.; sentence 2 Victoria doesn’t have exactly\none mall that has hurt Sam.; Label contradiction.\nAs to SI, 6000 tagged utterance pairs were used for\ntraining and 600 for testing. Here is the input repre-\nsentation: sentence 1 The teacher resembles some\nsketches.; sentence 2 The teacher doesn’t resemble\nall sketches.; Label entailment.\nWe fine-tuned DistillBERT-base-uncased on an\nApple M1 CPU for 3 epochs. We used a batch size\n64 of and optimized using Adam (Kingma and Ba,\n2014) with betas=(0.9,0.999), with a learning rate\n2This would seem to raise questions about the strength of\nthe conclusions being drawn (c.f. section5) - it seems that LMs\nmerely leverage co-occurrence frequency; on the other hand, it\nalso appears that LMs’ trend aligns with joint frequency - LMs\ndoes not fail the sanity check because frequency/prevalence\nheavily influences humans’ commonsense reasoning too.\nof 2e-05.\n5 Evaluations and discussion\nError bar in Fig.1 shows DistillBERT does not\nseem to have difficulty detecting Presp, and fine-\ntuning slightly decreases its performance. This is\nlikely due to the fact that Singh et al. (2016) data\nis not formatted the same as the ImpPress training\ndata. Fine-tuning might have misled DistillBERT.\nRegarding SI, fine-tuning significantly increases\nLMs’ performance, indicating that the ImpPress\ndataset is a good candidate for improving LMs’ sen-\nsitivity to commonsense SIs. Error bar in Fig.2 in-\ndicates that GPT-3 is slightly better in detecting SI\nthan in Presp, but overall GPT-3 is not good at the\npsycholinguistic task. This maybe because GPT-\n3 has a different architecture. LMs performance\naligns with n-gram baseline in that overall the SI\ndataset is less challenging than the Presp: 70% of\nSI dataset shows the favorable co-occurrence direc-\ntion: the pair tagged as ‘good’ also shows higher\nnouns co-occurrence rate than the ‘bad’ pair does.\nThe Presp dataset is less helpful (50%).\nIt’s worth noting that it’s not clear if we can\nmake a direct comparison between human deci-\nsions and LMs’ rates, especially for the SI cases.\nNieuwland et al. (2010) suggests that for humans,\nthe informative and pragmatically good statements\nelicited larger N400 ERPs than underinformative\nand pragmatically bad statements. However, this\ndoes not directly transfer to the accuracy mean met-\nric we used for LMs. All Fig.2 showed is that\nGPT-3’s performance is roughly at chance, with\nrespect to accuracy mean. For future studies, we\nplan to conduct parallel human studies to collect\nbaseline human decision rates.\nRegarding LMs evaluation analysis, our study\nshows that in order to probe commonsense knowl-\nedge from LMs, understand their reasoning mecha-\nnisms, and identify their limitations for AI applica-\ntions due to the lack of commonsense knowledge,\nwe need to carefully consider how to prompt the\n20\npre-trained LMs. For masked LMs such as Dis-\ntillBERT, our results suggest that an appropriate\nmethod to examine how ‘human-like’ LMs are is\nto mask the same token as psycholinguists do in\ntheir behavioral/neural experiments with humans,\nand keep the same contextual information, so that\nthe experiment setting is as close to human exper-\niments as possible. As to unidirectional LMs like\nGPT-3, they read in sentence using almost the same\nfundamental mechanisms as humans do, we thus\ntook sentence to be a unit to derive logprob. How\nmuch GPT-3 like the sentence is directly reflected\nin its sentence logprob. It’s crucial to use different\nmetrics for BERT and GPT-3 to avoid the pitfall\nof comparing the two with the same metrics, as\nthey are trained very differently, and a perplexity\ncomparison would be inconclusive.\nFigure 1: Evaluate BERT with human data. DistillBERT\nis used for critical word prediction. FT: fine-tuned.\nFigure 2: Evaluate GPT-3 with human data. GPT-3 is\nused for sequential word prediction.\nOur study has some limitations. Although we\nmention multiple times that these pragmatics of-\nten exist in conversations, the actual datasets we\nused are not conversational. For future work, we\nhope to see how LMs perform in a conversation sce-\nnario in terms of commonsense pragmatics. This\ncould give us a better grasp of LMs’ competence\nat the conversational level of language understand-\ning. For the current work, our motivation of using\nnon-conversational human data for conversational\nimplicature is that LMs are not trained the same\nway through many dialogues, but rather with text\nfound on the web. Additionally, we acknowledge\nthat there were some glitches in DistillBERT’s SI\nevaluation setting. BERT is considered succeed\nas long as the critical word is in its top K. By not\npenalizing that some can be above all in the case\nwhere both would be in the top K choices, we ac-\ncept LM’s choice as “correct” white it isn’t. It’s\nalso not very surprising that all doesn’t show up\nas much as other options in BERT’s topK choices\nfor scenarios that all is the commonsense intent,\ngiven that LM might generate adjectives but not\nquantifiers to modify the following noun. It’s likely\nthat this has nothing to do with the implication,\nnevertheless they still make sense considering that\nthe LM’s learning algorithm uses masked loss. For\nfuture research, we hope to get more valid con-\nclusions through directly comparing whether all is\nrelatively more likely than some.\nHumans show no difficulty in using common-\nsense knowledge to reason about daily conversa-\ntions. By contrast, the extent to which LMs are\nsensitive to commonsense reasoning has remained\nan elusive research question in AI research for\ndecades. Here, we provide an approach for com-\nmonsense reasoning tasks: incorporating online\nand offline psycholinguistic datasets into LMs eval-\nuation. Using well-controlled task design and high\nresolution neurophysiology equipment, psycholin-\nguistics studies all kinds of implicit meanings in\nnatural language. To examine how ‘human-like’\nLMs can be, human data is the key. These methods\ncan improve the interpretability and explainability\nof neural models for reasoning about implied yet\ncommonsense message.\nTo sum up, our paper aims to evaluate Distill-\nBERT and GPT-3’s ability to make human-like\npragmatic inferences, such as SI and Presp, through\nhuman behavioral and neural data. Findings show\npsycholinguistic datasets can help get a good grasp\nof LMs’ accuracy in detecting commonsense rea-\nsoning. Our study adopted a theory-supported lens\nfor investigating the often vaguely-defined “com-\nmonsense”, and illustrated how to establish connec-\ntion between commonsense reasoning in NLP and\npragmatic semantics.\n21\nReferences\nSimon Baron-Cohen. 2008. Autism, hypersystemiz-\ning, and truth. Quarterly Journal of Experimental\nPsychology, 61(1):64–75.\nSimon Baron-Cohen, Sally Wheelwright, Richard Skin-\nner, Joanne Martin, and Emma Clubley. 2001. The\nautism-spectrum quotient (aq): Evidence from as-\nperger syndrome/high-functioning autism, malesand\nfemales, scientists and mathematicians. Journal of\nautism and developmental disorders, 31(1):5–17.\nSimon Ed Baron-Cohen, Helen Ed Tager-Flusberg, and\nDonald J Cohen. 1994. Understanding other minds:\nPerspectives from autism. In Most of the chapters in\nthis book were presented in draft form at a workshop\nin Seattle, Apr 1991. Oxford University Press.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, V olume1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Asso-\nciation for Computational Linguistics.\nAllyson Ettinger. 2020. What BERT is not: Lessons\nfrom a new suite of psycholinguistic diagnostics for\nlanguage models. Transactions of the Association\nfor Computational Linguistics, 8:34–48.\nAriel Goldstein, Zaid Zada, Eliav Buchnik, Mariano\nSchain, Amy Price, Bobbi Aubrey, Samuel A Nas-\ntase, Amir Feder, Dotan Emanuel, Alon Cohen, et al.\n2021. Thinking ahead: spontaneous prediction in\ncontext as a keystone of language in humans and\nmachines. bioRxiv, pages 2020–12.\nH.P. Grice. 1975. Syntax and Semantics, volume 3,\nchapter Logic and Conversation. Academic Press,\nNew York.\nPaloma Jeretic, Alex Warstadt, Suvrat Bhooshan, and\nAdina Williams. 2020. Are natural language in-\nference models IMPPRESsive? Learning IMPli-\ncature and PRESupposition. In Proceedings of\nthe 58th Annual Meeting of the Association for\nComputational Linguistics, pages 8690–8705, On-\nline. Association for Computational Linguistics.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nSteven J Luck. 2012. Event-related potentials.\nMante S. Nieuwland, Tali Ditman, and Gina R. Ku-\nperberg. 2010. On the incrementality of pragmatic\nprocessing: An erp investigationof informativeness\nand pragmatic abilities. Journal of Memory and\nLanguage, 63:324–346.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nSebastian Schuster, Yuxing Chen, and Judith De-\ngen. 2020. Harnessing the linguistic signal to\npredict scalar inferences. In Proceedings of\nthe 58th Annual Meeting of the Association for\nComputational Linguistics, pages 5387–5403, On-\nline. Association for Computational Linguistics.\nRaj Singh, Evelina Fedorenko, Kyle Mahowald, and\nEdward Gibson. 2016. Accommodating presup-\npositions is inappropriate in implausible contexts.\nCognitive Science, 40:607–634.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n4593–4601, Florence, Italy. Association for Com-\nputational Linguistics.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel Bowman. 2019. Superglue: A stick-\nier benchmark for general-purpose language under-\nstanding systems. Advances in neural information\nprocessing systems, 32.\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-\nhananey, Wei Peng, Sheng-Fu Wang, and Samuel R.\nBowman. 2020. BLiMP: A benchmark of linguis-\ntic minimal pairs for English. In Proceedings of the\nSociety for Computation in Linguistics 2020, pages\n409–410, New York, New York. Association for Com-\nputational Linguistics.\nWenpeng Yin, Hinrich Schütze, Bing Xiang, and Bowen\nZhou. 2016. Abcnn: Attention-based convolu-\ntional neural network for modeling sentence pairs.\nTransactions of the Association for Computational\nLinguistics, 4:259–272.\n22",
  "topic": "Commonsense reasoning",
  "concepts": [
    {
      "name": "Commonsense reasoning",
      "score": 0.712562620639801
    },
    {
      "name": "Computer science",
      "score": 0.6354221701622009
    },
    {
      "name": "Commonsense knowledge",
      "score": 0.6173880100250244
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5709649920463562
    },
    {
      "name": "Presupposition",
      "score": 0.4946243464946747
    },
    {
      "name": "Pragmatics",
      "score": 0.45850712060928345
    },
    {
      "name": "Language model",
      "score": 0.45307493209838867
    },
    {
      "name": "Natural language",
      "score": 0.44541600346565247
    },
    {
      "name": "Artificial intelligence",
      "score": 0.44065776467323303
    },
    {
      "name": "Language understanding",
      "score": 0.42888525128364563
    },
    {
      "name": "Cognitive science",
      "score": 0.4280080795288086
    },
    {
      "name": "Psycholinguistics",
      "score": 0.42764508724212646
    },
    {
      "name": "Natural language processing",
      "score": 0.41606104373931885
    },
    {
      "name": "Question answering",
      "score": 0.4132021963596344
    },
    {
      "name": "Psychology",
      "score": 0.3366547226905823
    },
    {
      "name": "Linguistics",
      "score": 0.23941636085510254
    },
    {
      "name": "Cognition",
      "score": 0.1556752324104309
    },
    {
      "name": "Knowledge representation and reasoning",
      "score": 0.13420331478118896
    },
    {
      "name": "Neuroscience",
      "score": 0.08821341395378113
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 4
}