{
  "title": "AbductionRules: Training Transformers to Explain Unexpected Inputs",
  "url": "https://openalex.org/W4285267480",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5103223347",
      "name": "Nathan Young",
      "affiliations": [
        "University of Auckland"
      ]
    },
    {
      "id": "https://openalex.org/A5009635346",
      "name": "Qiming Bao",
      "affiliations": [
        "University of Auckland"
      ]
    },
    {
      "id": "https://openalex.org/A5034490691",
      "name": "Joshua Bensemann",
      "affiliations": [
        "University of Auckland"
      ]
    },
    {
      "id": "https://openalex.org/A5057995059",
      "name": "Michael Witbrock",
      "affiliations": [
        "University of Auckland"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3167607673",
    "https://openalex.org/W3090866633",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4295224299",
    "https://openalex.org/W3147294411",
    "https://openalex.org/W3047988254",
    "https://openalex.org/W4287184755",
    "https://openalex.org/W1903049858",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W3034830866",
    "https://openalex.org/W4288262459",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3173805051",
    "https://openalex.org/W3102041518",
    "https://openalex.org/W2964359568",
    "https://openalex.org/W4287671158",
    "https://openalex.org/W3105516974",
    "https://openalex.org/W4292779060"
  ],
  "abstract": "Transformers have recently been shown to be capable of reliably performing logical reasoning over facts and rules expressed in natural language, but abductive reasoning - inference to the best explanation of an unexpected observation - has been underexplored despite significant applications to scientific discovery, common-sense reasoning, and model interpretability.This paper presents AbductionRules, a group of natural language datasets designed to train and test generalisable abduction over natural-language knowledge bases.We use these datasets to finetune pretrained Transformers and discuss their performance, finding that our models learned generalisable abductive techniques but also learned to exploit the structure of our data.Finally, we discuss the viability of this approach to abductive reasoning and ways in which it may be improved in future work.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2022, pages 218 - 227\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nAbductionRules: Training Transformers to Explain Unexpected Inputs\nNathan Young, Qiming Bao, Joshua Bensemann, Michael Witbrock\nStrong AI Lab\nSchool of Computer Science\nUniversity of Auckland\n{nathan.young, josh.bensemann, m.witbrock}@auckland.ac.nz\nqbao775@aucklanduni.ac.nz\nAbstract\nTransformers have recently been shown to be\ncapable of reliably performing logical reason-\ning over facts and rules expressed in natu-\nral language, but abductive reasoning - infer-\nence to the best explanation of an unexpected\nobservation - has been underexplored despite\nsigniﬁcant applications to scientiﬁc discov-\nery, common-sense reasoning, and model inter-\npretability.\nWe present AbductionRules, a group of natu-\nral language datasets designed to train and test\ngeneralisable abduction over natural-language\nknowledge bases. We use these datasets to ﬁne-\ntune pretrained Transformers and discuss their\nperformance, ﬁnding that our models learned\ngeneralisable abductive techniques but also\nlearned to exploit the structure of our data. Fi-\nnally, we discuss the viability of this approach\nto abductive reasoning and ways in which it\nmay be improved in future work.\n1 Introduction\nSince its introduction, models based on the Trans-\nformer (Vaswani et al., 2017) have, due to their\nlearning ability and Turing-completeness (Bhat-\ntamishra et al., 2020), sparked research into their\nuse in many applications beyond their original pur-\npose of natural language processing (NLP), includ-\ning image processing and generation (Parmar et al.,\n2018; Chen et al., 2020), theorem proving (Polu\nand Sutskever, 2020; Welleck et al., 2021), and\nchess (Noever et al., 2020).\nOne such task is logical inference - reasoning\nover ﬁrst-order logic (FOL) knowledge bases (col-\nlections of facts and rules). Given a knowledge\nbase, one may attempt to ﬁnd logical implications\n(deduction), discover rules that extrapolate patterns\nin known facts (induction), or infer facts that would\nexplain surprising observations (abduction). More\nspeciﬁcally, if a newly observed fact pcannot be\ndeduced from an existing knowledge base, abduc-\ntion is the process of ﬁnding one or more facts that,\nif added to the knowledge base, would allow pto\nbe deduced from existing rules. Figure 1 demon-\nstrates the difference between these three kinds of\ninference.\nTraditionally, FOL is represented using a for-\nmal mathematical syntax, with facts resembling\nHUMAN (SOCRATES ) and rules resembling ∀X :\nHUMAN (X) =⇒ MORTAL (X). Clark et al.\n(2020) recently pioneered an alternative approach\nwe call natural-language logic, which might repre-\nsent these as \"Socrates is human\" and \"Humans are\nmortal\". This approach, properly followed, retains\nthe precision of the mathematical syntax while also\ntaking advantage of Transformers’ NLP aptitude\nand pretraining. This approach also allows reason-\ning over texts not written in formal representations.\nClark et al. (2020) examined their models’ po-\ntential for deduction only. Tafjord et al. (2021) ex-\ntended this work to explore abduction but retained\na focus on deduction.\nOur goal is to train Transformers to perform\nabductive reasoning with the following properties:\n• Natural: Operate over natural language.\n• Generalisable: Be able to apply techniques\noutside domains in which they were learned.\n• Generative: Produce explanations rather than\nlabelling them as sufﬁcient or insufﬁcient.\n• Single-hop: Produce direct explanations. In-\nstead of \"plants are green because chlorophyll\nis green because green light is not used in pho-\ntosynthesis\", prefer \"plants are green because\nchlorophyll is green\". If further explanation is\ndesired, abduction can be applied again.\n• Discerning: Prefer simpler explanations.\n• Explicit: Use given knowledge bases rather\nthan relying on pretraining.\n218\nDeduction: Socrates is human → Humans are mortal → ?\nInduction: Socrates is human → ? → Socrates is mortal\nAbduction: ? → Humans are mortal → Socrates is mortal\nFigure 1: A comparison of deduction, induction, and abduction, as attempts to reconstruct different parts of the\nsame line of FOL reasoning. Note that only deduction is fully reliable, induction may go in either direction in this\ncase, and only abduction produces new knowledge.\nOur efforts to train abduction in this way are\nmotivated by multiple potential applications.\n• Ray (2007) describes the use of automated\nabduction in scientiﬁc discovery. Since much\nscientiﬁc knowledge exists in the form of nat-\nural language rather than formal representa-\ntions, advances in natural-language abduction\nwould greatly assist in automating the scien-\ntiﬁc method by helping to explain experimen-\ntal observations.\n• Ignatiev et al. (2019) describe the use of ab-\nduction to interpret deep learning models sim-\nilar to Transformers, which are infamously\ndifﬁcult to interpret.\n• Abduction may also help solve the longstand-\ning problem of automating common-sense\nreasoning. Transformers excel at memorising\ncommon knowledge but routinely fail to cap-\nture any underlying reasoning. Training these\nmodels to explain their own outputs may rem-\nedy this problem by providing a way to in-\ntegrate this fractured knowledge into a more\nconnected model of reality.\nWe present the following contributions:\n• A collection of datasets for training and test-\ning natural-language abduction.\n• A method of synthetically generating more\nrealistic natural-language logic datasets.\n• Experimental results showing that Transform-\ners can perform abductive reasoning without\nadditional architecture.\n2 Related Work\n2.1 Natural-language logic\nOur work builds on the RuleTaker line of research\non natural-language logic. This line began with\nClark et al. (2020), who developed RuleTakers to\nreason deductively over FOL knowledge bases ex-\npressed in natural language, judging given facts to\nbe true or false. These achieved promising results\nbut failed to accurately explain their reasoning or\ngeneralise to inferences requiring more steps than\nwere seen at training time. PRover (Saha et al.,\n2020) achieved greater explainability by gener-\nating proofs of its answers. Similarly, the Itera-\ntive variant of ProofWriter (Tafjord et al., 2021)\nchained single-hop deductions rather than reason-\ning through multi-hop deductions all at once, mak-\ning its reasoning transparent and easily generalis-\nable to unseen depths. multiPRover (Saha et al.,\n2021) also made use of this iterative approach.\nThe generalisability and interpretability of itera-\ntive single-hop reasoning are why we seek to train\nsingle-hop abduction.\nTafjord et al. (2021) also adapted their deduction-\nbased datasets to train abductive reasoning, achiev-\ning success but training multi-hop abduction only,\nand also requiring models to output every possible\nexplanation. By contrast, we seek to train models\nto discern between simpler and more complex ex-\nplanations - for example, to prefer explanations\nrequiring fewer unknown facts.\n2.2 Other adjacent work\nBhagavatula et al. (2019) presented two more\nabduction-based datasets: α-NLI, which tests mod-\nels’ ability to choose which of two hypotheses bet-\nter explains an observation, and α-NLG, a gener-\native version of the same dataset. These datasets\ndo not give supporting knowledge bases - all back-\nground information must come from pretraining.\nWhile this is a valuable approach, we seek to in-\nvestigate how well Transformers can reason over\ngiven knowledge bases to incorporate explicit back-\nground knowledge.\nGontier et al. (2020) investigated Transformers’\nability to perform inductive reasoning in natural\nlanguage, ﬁnding them able to extrapolate patterns\nin given proofs but again unable to generalise to\nmore complex proofs.\nSaparov and Mitchell (2021) developed an al-\nternative approach to classifying the ProofWriter\ndatasets that does not reason over natural language,\n219\ninstead using a symbolic, Bayesian approach and\nusing abductive reasoning to satisfy constraints.\nTheir models’ superior performance demonstrates\nthat while Transformers are effective at logical rea-\nsoning, they may beneﬁt from more specialised\narchitecture.\n3 Methodology\nPrior to our work, there existed no dataset capable\nof training or testing the kind of abductive rea-\nsoning we seek. We therefore present Abduction-\nRules, a natural-language logic dataset designed\nfor this task, and use it to train and test several\nmodels based on a pretrained Text-to-Text Transfer\nTransformer, or T5 (Raffel et al., 2020).\n3.1 Datasets\nAbductionRules has three main predecessors.\n3.1.1 Rule Reasoning\nThe Rule Reasoning dataset developed by Clark\net al. (2020) was, to our knowledge, the ﬁrst natural-\nlanguage logic dataset.\nTo create this dataset, FOL predicates (e.g.\nBIG(LION )) were procedurally generated, entities\n(LION ) and attributes ( BIG(X) ) were extracted,\nand templates (\"The {entity} is {attribute}\") were\nused to create natural-language logic translations\n(\"The lion is big\"). Rules were created similarly\n(e.g. ∀X : BIG(X) =⇒ BLUE (X) became \"If\nsomething is big then it is blue\"). Facts and rules\nwere grouped into knowledge bases, each with sev-\neral questions; the model’s task is to label each\nquestion true or false.\nThe Rule Reasoning dataset includes knowledge\nbases in several domains; those in the animal-\ndomain use animals as entities while those in\nthe person-domain use peoples’ names. All sub-\nsequent datasets similarly use these two domains.\nThe animal-domain includes multi-entity facts\n(CHASES (LION , MOUSE ), or \"the lion chases the\nmouse\"). For our purposes, we consider the lion to\nbe the main entity and \"chases the mouse\" to be an\nattribute of the lion.\n3.1.2 ParaRules\nRecognising that their translations of mathematical\nsyntax into natural language were strict and unre-\nalistic (e.g. \"Charlie is green. Charlie is rough.\"),\nClark et al. (2020) also produced ParaRules, which\ncontained knowledge bases and questions similar\nto those in the Rule Reasoning dataset, but were\nparaphrased into more colloquial language (e.g.\n\"Charlie has green teeth and rough skin.\"). This\napproach much better prepares Transformers to\nreason logically over naturally-occurring texts but\nrequires large amounts of human labour to produce.\nFor this reason, ParaRules is much smaller than the\nRule Reasoning dataset.\n3.1.3 PARARULE Plus\nSeeing the value in RuleTaker’s size and ease\nof production as well as the greater utility of\nParaRules, Bao (2021) produced PARARULE\nPlus, a compromise between the Rule Reasoning\ndataset and ParaRules that procedurally rephrases\nall rules during generation by using various tem-\nplates. PARARULE Plus also avoids eschewingz\nword associations entirely by grouping related at-\ntributes (such as \"big\", \"strong\", \"high\" and \"huge\")\nand only giving entities attributes from one group.\nWhile PARARULE Plus falls short of ParaRules’\nvariety, its greater collection of rephrased rules is\nhighly valuable.\n3.1.4 AbductionRules\nWe adapt the open-source code used to generate\nPARARULE Plus to create AbductionRules1, mak-\ning the following changes:\n• Instead of labelling questions (for our pur-\nposes, \"observations\") with \"true\" or \"false\",\nwe use the lone fact (or \"explanation\") that\nwould prove or disprove it.\n• We ensure that no two knowledge bases in the\nsame dataset give the same attributes to the\nsame entities to avoid repeats. This reduces\nthe size of the datasets; to compensate, we\nincrease the number of entities.\n• While each rule has a single condition in\nPARARULE Plus (\"If something is cute,\nthen...\"), we give three (\"If something is cute,\nfunny, and adorable, then...\"), with an entity\nthat satisﬁes exactly two conditions; the model\nmust identify the third.\nAfter making these changes, we produce datasets\nwith increasing levels of complexity.\n• The ﬁrst complexity level contains no further\nchanges from PARARULE Plus and yields the\ndataset Abduction-Animal-0.1.\n1AbductionRules can be found and downloaded\nat https://github.com/Strong-AI-Lab/\nAbductionRules/tree/main/datasets/.\n220\nContext(Facts+Rules):\nFacts: The squirrel is quiet .The leopard is\nslow. The dog is adorable. The crocodile is heavy.\nThe leopard is boring. The leopard is angry. The\ncrocodile is awful. The leopard attacks the squirrel.\nThe dog is small. The dog is cute. The squirrel is\nnice. The crocodile likes the dog.The squirrel is\nkind .\nRules:If something is cute, is adorable, and is furry,\nthen it is also lovely. All animals that are obese,\nare awful, and are heavy, are big. If an animal is\nﬁerce, sees the squirrel, and likes the dog, it is\ntired.Things that aresmart , arekind , and are\nquiet , are alsoround .If an animal chases the\ndog, is boring, and attacks the squirrel, then it is\nalso strong. All things that are slow, are sleepy, and\nare angry, are rough.\nObservation:The squirrel is round .\nExplanation:The squirrel is smart .\nFigure 2: An example observation, explanation,\nand corresponding context from Abduction-Animal-\nSimple. The model must output the explanation given\nthe context and observation as input. Facts and rules\nused to explain the observation are bolded while rele-\nvant attributes are highlighted.\n• At the second complexity level, we shufﬂe\nall knowledge bases to prevent models from\nexploiting the constant position of all sen-\ntences and attributes. This yields the dataset\nAbduction-Animal-0.2.\n• At the third complexity level, we procedurally\nrephrase rules with random variations instead\nof using the same templates as PARARULE\nPlus. For example, the animal-domain FOL\nrule ∀X : ( BIG(X) ∧ HEAVY(X) ∧\nFIERCE (X)) =⇒ STRONG (X) might be\nrephrased as \"All animals that are big, are\nheavy, and are ﬁerce, are also strong\" or \"If\nsomething is heavy, is ﬁerce, and is big, it is\nstrong\", among many other similar variations.\nNotably, this rephrasing process involves re-\nordering all attributes so that attributes con-\ntained in correct abductions might be ﬁrst,\nsecond, or third. This yields the datasets\nAbduction-Animal-Simple and Abduction-\nPerson-Simple.\nFigure 2 contains an example item from\nAnimal-Simple.2\n2For brevity, we omit the \"Abduction-\" preﬁx when dis-\ncussing the AbductionRules datasets within this paper.\nThis method of procedural rule rephrasing\nrepresents a useful iteration on the natural-\nlanguage logic approach and leaves room for\nfurther improvement. Concentrated work in\nthis line of research may produce synthetic\nnatural-language logic datasets that are larger\nyet exhibit much wider variety, making this\napproach more powerful and robust.\n• At the fourth and ﬁnal complexity level, we\nadd extraneous confounding rules to knowl-\nedge bases. While lower complexity levels\nonly ever have one rule that could explain a\ngiven observation, here we create two varia-\ntions of every (single-entity) rule; one replaces\na satisﬁed condition with an unsatisﬁed con-\ndition, while the other replaces all three con-\nditions. All replacements come from differ-\nent pools. This yields the datasets Abduction-\nAnimal and Abduction-Person.\nFigure 3 contains simpliﬁed examples of data\nfrom each complexity level.\nWe intend each successive complexity level to\nremove additional idiosyncrasies that might be ex-\nploited in lieu of using abduction (i.e. used to\n\"cheat\"), so that this exploitation can be detected.\nWe also intend the fourth to train models to favour\nsimpler explanations when strictly more complex\nexplanations are available.\n3.2 Experiments\nWe use AbductionRules to train 8 models based on\nthe pretrained T5 implementation from the Hug-\ngingFace Transformers library (Wolf et al., 2020).3\nWe ﬁrst use each training set to train 1 model,\nyielding 6 models trained at 4 complexity levels\nacross 2 domains. To compare domains and com-\nplexity levels, we test all models on all test sets,\ngiving us intra-domain results (isolating the effect\nof the complexity), and inter-domain results (some\nisolating the effect of the domain). We expect each\nsuccessive complexity level to train a better-quality\nmodel and the two domains to be mostly compara-\nble with some variation attributable to the animal-\ndomain’s multi-entity facts.\nIf our approach were adapted to models exten-\nsively trained to reason on many domains, we ex-\npect that teaching abduction in every domain would\n3All code used for experiments in this paper can\nbe found at https://github.com/Strong-AI-Lab/\nAbductionRules/.\n221\nInitial Shufﬂed Rephrased Confounded\nThe cat is round. The cat is smart. The cat is smart. The cat is smart.\nThe cat is smart. If something is round, All animals that are All animals that are\nIf something is round, smart, and quiet, then round, are smart, and round, are smart, and\nsmart, and quiet, then it is kind. are quiet, are also kind. are quiet, are also kind.\nit is kind. The cat is round. The cat is round. The cat is round.\nIf an animal is round,\nis boring, and is quiet,\nit is kind.\nFigure 3: A diagram demonstrating the successive changes we make to the AbductionRules knowledge bases.\nbe prohibitively expensive. Therefore, we seek to\ninvestigate Transformers’ ability to transfer abduc-\ntive reasoning techniques to domains where these\ntechniques have not been taught but are nonetheless\nfamiliar to the Transformer. To this end, we train\ntwo more multi-domain models.\n• We train one model on our simplest dataset\nand our most complex dataset in another do-\nmain, i.e. Animal-0.1 and Person. We name\nthis model Person+Animal-0.1.\n• We train another model on the simplest\nperson-domain dataset and the most complex\nanimal-domain dataset, i.e. Person-Simple\nand Animal, to compare the two domains. We\nname this model Animal+Person-Simple.\nWhile we are interested in these multi-domain\nmodels’ performance on all datasets, we are particu-\nlarly interested in their results on the most complex\ndataset on which they were not trained (Abduction-\nAnimal and Abduction-Person, respectively). We\ntreat performance on these datasets as a proxy for\nTransformers’ ability to apply abductive reasoning\noutside the domains in which it was trained.\nFinally, we use the existing pretrained T5 model\nwith no additional training as our baseline model.\n4 Results\nTable 1 contains our results, showing the percent-\nage of abductions correctly performed by each\nmodel on each test set. 4\nNote that no model ever gave a correct answer\nin a domain on which it was not trained. On the\nsurface, this would suggest that our models were\n4All our results can be found at https://github.\ncom/Strong-AI-Lab/AbductionRules/tree/\nmain/results/.\nunable to generalise to new domains. However, in-\nspection of inter-domain results shows that this is\nnot entirely accurate; many explanations contain\nerrors but nonetheless identify the ground-truth ex-\nplanation. For example, the animal models com-\nmonly appended \"The\" to correct explanations, as\nin \"The Bob is small\"; while this is incorrect, it\nnonetheless indicates the correct explanation in a\nway that suggests the model still performed the cor-\nrect abduction. We distinguish between two kinds\nof errors in correct-yet-useful explanations: loss-\nless errors and lossy errors.\n4.1 Lossless errors\nExplanations with lossless errors failed to match\nthe correct explanation character-for-character but\nallowed it to be reliably identiﬁed.\nWe found several ways in which recognisably\ncorrect explanations differed from the ground-truth,\nsuch as extra words (\"The Bob is small\", \"The lion\nis attacks the mouse\"), looping (\"The dog is is is\nis is small\"), and incorrect grammar (\"The anne is\nwealthy\"). While these errors point towards ﬂaws\nin training, it is a strength of natural-language logic\nand soft reasoners that they can cope with minor\ngrammar mistakes as long as meaning is preserved.\nTable 2 contains our results after correcting\nfor these errors. Note that animal-domain models\nachieved performance comparable to the person-\ndomain models on novel datasets in their own do-\nmain, while person-domain models saw minimal\ninter-domain improvement. multi-domain models\nalso saw almost no improvement, suggesting that\nhaving seen correct explanations in both domains\neliminated this kind of formatting error.\n4.2 Lossy errors\nThe most important aspect of abduction in our\ndatasets is identiﬁcation of the correct attribute.\n222\nModel\\Test set Animal-0.1 Animal-0.2 Animal-Simple Animal Person-Simple Person\nUntrained 0.0% 0.0% 0.0% 0.0% 0.0% 0.0%\nAbduction-Animal-0.1 100.0% 99.3% 48.0% 28.8% 0.0% 0.0%\nAbduction-Animal-0.2 100.0% 100.0% 37.7% 23.2% 0.0% 0.0%\nAbduction-Animal-Simple 100.0% 100.0% 100.0% 50.1% 0.0% 0.0%\nAbduction-Animal 92.6% 93.5% 94.1% 100.0% 0.0% 0.0%\nAbduction-Person-Simple 0.0% 0.0% 0.0% 0.0% 100.0% 25.6%\nAbduction-Person 0.0% 0.0% 0.0% 0.0% 26.8% 100.0%\nPerson+Animal-0.1 100.0% 100.0% 76.7% 85.5% 92.9% 100.0%\nAnimal+Person-Simple 99.1% 99.1% 99.4% 100.0% 100.0% 99.8%\nTable 1: Performance of all models on all test sets. Test sets corresponding to training sets are bolded.\nModel\\Test set Animal-0.1 Animal-0.2Animal-Simple Animal Person-Simple Person\nUntrained 0.0% 0.0% 0.0% 0.0% 0.0% 0.0%\nAbduction-Animal-0.1100.0% (-) 99.3% (-) 48.0% (-) 28.8% (-) 13.2% (+13.2%)10.1% (+10.1%)\nAbduction-Animal-0.2100.0% (-) 100.0% (-)38.5% (+0.9%)23.6% (+0.4%)9.6% (+9.6%) 5.7% (+5.7%)\nAbduction-Animal-Simple100.0% (-) 100.0% (-) 100.0% (-) 50.1% (-) 34.4% (+34.4%)7.0% (+7.0%)\nAbduction-Animal 92.6% (-) 93.5% (-) 94.2% (+0.0%)100.0% (-)25.0% (+25.0%)36.5% (+36.5%)\nAbduction-Person-Simple1.5% (+1.5%)1.3% (+1.3%)0.9% (+0.9%)0.3% (+0.3%) 100.0% (-) 25.6% (-)\nAbduction-Person 0.0% (-) 0.0% (-) 0.0% (-) 0.0% (-) 26.8% (-) 100.0% (-)\nPerson+Animal-0.1 100.0% (-) 100.0% (-) 76.7% (-) 85.5% (-) 92.9% (-) 100.0% (-)\nAnimal+Person-Simple99.1% (-) 99.1% (-) 99.4% (-) 100.0% (-) 100.0% (-) 99.8% (-)\nTable 2: Improvement of all models on all test sets after allowing lossless errors.\nThe entity at the beginning of the explanation al-\nways matches that at the beginning of the observa-\ntion; therefore, if the correct attribute is identiﬁed,\nthe correct explanation can be reconstructed.\nTable 3 contains our results after correcting for\nthese errors. Note that every model achieved some\nuseful results on every test set. Most inter-domain\nresults improved to rival intra-domain results, al-\nthough the Abduction-Person model continued to\nstruggle. Intra-domain results saw minimal im-\nprovement, with none seeing a >2% point increase.\nThe multi-domain models again saw no visible im-\nprovement, further suggesting that these inferior\nresults were avoidable from seeing facts, rules, and\nexplanations in different formats at training time.\n5 Discussion\nOur results show that models trained on our sim-\nplest datasets struggle to generalise to new com-\nplexity levels and domains, while those trained on\nour more complex datasets are better able to gen-\neralise but still perform suboptimally. Meanwhile,\nthose trained on combined multi-domain datasets\nachieve performance superior to the sum of models\ntrained on their parts and easily apply skills outside\ndomains in which they were learned. It is also clear\nthat models trained in the animal-domain achieve\nbetter intra-domain and inter-domain performance\nthan person-domain models.\nThe untrained T5 model performs abysmally\nand merits little discussion, indicating that this ab-\nduction task is non-trivial (at least in the way we\npresent it here).\n5.1 Animal-0.1 and Animal-0.2\nUnsurprisingly, the models trained on our sim-\nplest datasets fare the worst of our trained mod-\nels. Our Animal-0.1 and -0.2 models perform sim-\nilarly poorly, suggesting that Animal-0.2’s addi-\ntional complexity from randomised sentence or-\nderings was of minimal importance. In fact, the\nAnimal-0.2 model’s performance on more complex\ndatasets is worse than its simpler counterpart; exam-\nination of its results reveals a tendency to loop on\nunfamiliar inputs. Given the Animal-0.1 model’s\n99.3% correct (100% allowing lossy errors) perfor-\nmance on Animal-0.2, we treat these complexity\nlevels as equivalent and the Animal-0.1 model as\ndeﬁnitive.\nThe Animal-0.1 model is approximately 1/3 as\naccurate on the person-domain when allowing loss-\nless errors but only loses approximately 6% points\nwhen allowing lossy errors, suggesting that it fails\nto adapt to new formats but is mostly able to use\nthe same techniques as in the animal-domain.\nThese models’ signiﬁcant performance hit on\nhigher complexity levels clearly indicates that they\nexploit the structure of their training set. However,\n223\nModel\\Test set Animal-0.1 Animal-0.2 Animal-Simple Animal Person-Simple Person\nUntrained 0.0% 0.2% 0.1% 0.0% 0.0% 0.0%\nAbduction-Animal-0.1100.0% (-)100.0% (+0.7%)48.4% (+0.4%)29.1% (+0.3%)41.9% (+28.8%)23.5% (+13.4%)\nAbduction-Animal-0.2100.0% (-) 100.0% (-) 39.4% (+0.8%)24.5% (+0.9%)29.4% (+19.8%)14.3% (+8.6%)\nAbduction-Animal-Simple100.0% (-) 100.0% (-) 100.0% (-) 50.1% (-) 66.4% (+32.0%)14.9% (+7.9%)\nAbduction-Animal 92.6% (-) 93.5% (-) 94.2% (-) 100.0% (-)39.1% (+14.1%)62.7% (+26.2%)\nAbduction-Person-Simple39.8% (+38.4%)42.8% (+41.5%)39.6% (+38.7%)11.5% (+11.2%)100.0% (-) 25.6% (-)\nAbduction-Person 5.2% (+5.2%)5.0% (+5.0%)4.9% (+4.9%)15.8% (+15.8%)26.8% (-) 100.0% (-)\nPerson+Animal-0.1 100.0% (-) 100.0% (-) 76.7% (-) 85.6% (+0.0%) 92.9% (-) 100.0% (-)\nAnimal+Person-Simple99.1% (-) 99.1% (-) 99.4% (-) 100.0% (-) 100.0% (-) 99.8% (-)\nTable 3: Improvement of all models on all test sets after allowing lossy errors.\nit should be noted that the Animal-0.1 model drops\neach time by approximately a factor of 2. If this\nmodel only chose the penultimate attribute in a sen-\ntence containing the attribute in the question, its\naccuracy would drop by a factor of 3 with proce-\ndural rephrasing and again with confounding rules.\nTherefore, both models utilise some level of gener-\nalised abductive reasoning.\n5.2 Animal-Simple and Person-Simple\nThe Animal-Simple model signiﬁcantly outper-\nforms our simpler models; this makes sense since\nAnimal-0.1 and -0.2 can be thought of as spe-\ncial, unshufﬂed cases of Animal-Simple. 5 Simi-\nlarly to the Animal-0.1 model, the Animal-Simple\nmodel performs about half as well on Animal as on\nAnimal-Simple. This model also performs worse\non Person-Simple than Animal when allowing loss-\nless errors but better when allowing lossy errors,\nimplying that it exploits the structure of Animal-\nSimple to some degree to identify correct attributes.\nIts performance drop from Person-Simple to Per-\nson is greater than from Animal-Simple to Animal,\nsuggesting that changes in domain and complexity\nare more difﬁcult to generalise when compounded.\nOur Person-Simple model also performs well\nbut fails to generalise to higher complexity; this\ncan be partially explained by the multi-entity facts\nin the animal-domain, as rules using these facts are\nnot used to create confounding rules. This model\ngives almost no correct inter-domain explanations\nunless lossy errors are allowed, in which case it\nachieves similar inter-domain performance to the\nanimal-domain models. Its performance drop on\nAnimal can be compared to that of the Animal-\nSimple model from Person-Simple to Person, ex-\nacerbated by the person-domain models’ poorer\nperformance in general.\n5Because of this and their failure to train generalisable\nabduction, we do not include either Animal-0.1 or -0.2 in the\npublic release of AbductionRules. The code we used for our\nexperiments can be used to regenerate them if desired.\n5.3 Abduction-Animal and\nAbduction-Person\nThe Animal model performs the best of all single-\ndomain models, achieving >60% performance on\nall datasets except Person-Simple when allowing\nlossy errors. The drop from Person to Person-\nSimple is evidence of cheating, but its generalisabil-\nity is superior to all other models and demonstrates\nsome abductive ability. Surprisingly, it achieves\nworse intra-domain results on lower complexity\nlevels than the Animal-Simple model, again indi-\ncating that some of its performance is dependent\non Animal’s rule structure. Still, this performance\ndrop is relatively small (being <10% in all cases),\nfurther reinforcing that while this model utilises\nsome degree of both cheating and abduction (like\nall our models), its abductive capabilities generalise\nto a promising extent.\nBy contrast, the Person model achieves the worst\nperformance of any model, performing as well on\nPerson-Simple as that dataset’s model does on Per-\nson and achieving abysmal inter-domain perfor-\nmance, even on Animal. This model is the clearest\nindication that (our instantiations of) the two do-\nmains are not equivalent; the animal-domain’s mod-\nels are much better able to generalise.The multi-\nentity rules again offer some explanatory power -\nthe Animal model demonstrates some overtraining\non the confounding rules and so performs more\npoorly in their absence, but still learned to explain\nobservations using multi-entity rules that lacked\nconfounding equivalents, making it robust to extra-\nneous rules but not reliant on them. If this were a\nmajor determining factor, we would expect models\ntrained on both maximally and minimally complex\ndatasets to be even more robust and generalised.\n5.4 Multi-domain models\nOur multi-domain models are our best-performing\nmodels by far, achieving superior performance on\nunseen datasets than the sum of models trained on\n224\ntheir combined training sets’ parts.\nThe Person+Animal-0.1 model, being trained on\nour simplest dataset and having its most complex\ntraining set come from the worse of our two train-\ning domains, is the worse of our two multi-domain\nmodels. Nonetheless, it reaches a remarkable level\nof performance, explaining >76% of all observa-\ntions correctly on all test sets. Its performance in\nthe face of unconfounded rephrased rules (some-\nthing unprecedented in its training) is dependent on\nthe domain. In the person-domain (i.e. on Person-\nSimple), where it received its most complex train-\ning, it achieves its best result on a dataset it was\nnot trained on (excepting Animal-0.2), while in the\nanimal-domain (i.e. on Animal-Simple) it achieves\nits worst result, having not seen any rephrased an-\nimal rules at training time. Still, it demonstrates\na greater ability than any single-domain model to\ngeneralise to these unfamiliar rule structures. It can\nalso apply its training on confounded rules out-\nside the domain in which it was learned, achieving\nfar greater performance on Animal than any other\ndataset that it was not trained on.\nThe Animal+Person-Simple model is our best\nand most promising, achieving >99% perfor-\nmance on every dataset and consistently adapt-\ning to all complexity levels in every domain. Like\nPerson+Animal-0.1, it encounters unprecedented\nrule structures (singular single-entity animal rules,\nconfounded person rules) and generalises almost\nperfectly to each. While our datasets remain some-\nwhat limited in scope, we believe that this result\ndemonstrates that Transformers can generalise ab-\nductive techniques beyond the domains in which\nthose techniques were trained, provided the domain\nitself is not entirely novel.\nExtrapolating these multi-domain results, it\nseems likely that ﬁnetuning Transformers that\nhave received extensive pretraining (such as GPT-3\n(Brown et al., 2020)) on datasets covering more\nvaried and complex examples of abduction would\nmake these models capable of much more gener-\nalised natural-language abductive reasoning.\n6 Conclusion\nWe have presented the AbductionRules datasets\nand shown that pretrained T5 models ﬁnetuned on\nthem exhibit generalised abductive reasoning. Our\nmore complex datasets train abduction more gen-\nerally and reliably than our less complex datasets.\nFurther, training in multiple domains is superior\nto training in only one domain, and we have clear\nevidence of generalisation of techniques from one\ndomain to another.\nWe have presented an innovation in natural-\nlanguage logic dataset generation, presenting a\nnew middle-ground between the template-based\nPARARULE Plus (Bao, 2021) and the manually\nrephrased Pararules (Clark et al., 2020). We believe\nour results are promising and demonstrate the via-\nbility of Transformer-based abduction (and logical\nreasoning in general), but also indicate opportuni-\nties for improvement.\n6.1 Future Work\nFuture work in this area might explore:\n• Examining skill transfer between different\nkinds of logical reasoning.\n• Applying abductive techniques in real-world,\nas opposed to artiﬁcial, domains.\n• Generating probability distributions over mul-\ntiple possible explanations.\n• Testing explanations by verifying that they\nallow the original observation to be deduced.\n• Explanations that include not only missing\npremises but the relevant rule(s) they satisfy.\nReferences\nQiming Bao. 2021. Pararule plus: A larger deep multi-\nstep reasoning dataset over natural language.\nChandra Bhagavatula, Ronan Le Bras, Chaitanya\nMalaviya, Keisuke Sakaguchi, Ari Holtzman, Han-\nnah Rashkin, Doug Downey, Wen-tau Yih, and Yejin\nChoi. 2019. Abductive commonsense reasoning. In\nInternational Conference on Learning Representa-\ntions.\nSatwik Bhattamishra, Arkil Patel, and Navin Goyal.\n2020. On the computational power of transformers\nand its implications in sequence modeling. In Pro-\nceedings of the 24th Conference on Computational\nNatural Language Learning, pages 455–475.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nMark Chen, Alec Radford, Rewon Child, Jeffrey Wu,\nHeewoo Jun, David Luan, and Ilya Sutskever. 2020.\nGenerative pretraining from pixels. In International\nConference on Machine Learning, pages 1691–1703.\nPMLR.\n225\nPeter Clark, Oyvind Tafjord, and Kyle Richardson.\n2020. Transformers as soft reasoners over language.\nIn Proceedings of the Twenty-Ninth International\nJoint Conference on Artiﬁcial Intelligence (IJCAI-\n20), pages 3882–3890.\nNicolas Gontier, Koustuv Sinha, Siva Reddy, and Chris\nPal. 2020. Measuring systematic generalization\nin neural proof generation with transformers. Ad-\nvances in Neural Information Processing Systems ,\n33.\nAlexey Ignatiev, Nina Narodytska, and Joao Marques-\nSilva. 2019. Abduction-based explanations for ma-\nchine learning models. In Proceedings of the AAAI\nConference on Artiﬁcial Intelligence , volume 33,\npages 1511–1519.\nDavid Noever, Matt Ciolino, and Josh Kalin. 2020. The\nchess transformer: Mastering play using generative\nlanguage models. arXiv preprint arXiv:2008.04057.\nNiki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz\nKaiser, Noam Shazeer, Alexander Ku, and Dustin\nTran. 2018. Image transformer. In International\nConference on Machine Learning, pages 4055–4064.\nPMLR.\nStanislas Polu and Ilya Sutskever. 2020. Generative\nlanguage modeling for automated theorem proving.\narXiv preprint arXiv:2009.03393.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. Journal of Machine Learning Research ,\n21:1–67.\nOliver Ray. 2007. Automated abduction in scientiﬁc\ndiscovery. In Model-Based Reasoning in Science,\nTechnology, and Medicine, pages 103–116. Springer.\nSwarnadeep Saha, Sayan Ghosh, Shashank Srivastava,\nand Mohit Bansal. 2020. Prover: Proof generation\nfor interpretable reasoning over rules. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n122–136.\nSwarnadeep Saha, Prateek Yadav, and Mohit Bansal.\n2021. multiPRover: Generating multiple proofs for\nimproved interpretability in rule reasoning. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies , pages\n3662–3677, Online. Association for Computational\nLinguistics.\nAbulhair Saparov and Tom M Mitchell. 2021. A gen-\nerative symbolic model for more general natural lan-\nguage understanding and reasoning. arXiv preprint\narXiv:2105.02486.\nOyvind Tafjord, Bhavana Dalvi, and Peter Clark. 2021.\nProofWriter: Generating implications, proofs, and\nabductive statements over natural language. In Find-\nings of the Association for Computational Linguis-\ntics: ACL-IJCNLP 2021, pages 3621–3634, Online.\nAssociation for Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nSean Welleck, Jiacheng Liu, Ronan Le Bras, Hannaneh\nHajishirzi, Yejin Choi, and Kyunghyun Cho. 2021.\nNaturalproofs: Mathematical theorem proving in\nnatural language. arXiv preprint arXiv:2104.01112.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nAppendices\nA Rephrasing method\nTable 4 demonstrates the method we used to\nrephrase rules in our more complex datasets. Our\nmethod made several binary phrasing choices to\ndecide between 16 possible templates, providing\nmore internal variety than PARARULE Plus but\nless than ParaRules. As well as this random varia-\ntion, all 3 conditions were shufﬂed, giving 6 possi-\nble orderings and 96 total possible rephrasings.\nB Lossless errors\nThe following encompass all errors we considered\nlossless - i.e. close enough to the ground truth an-\nswer to be reasonably counted as correct.\n• Unnecessary inclusion of ’the’, as in \"The Bob\nis small.\"\n• Omission of ’the’, as in \"Cat is smart.\"\n• Unnecessary inclusion of ’is’, as in \"The lion\nis attacks the mouse.\"\n• Omission of ’is’, as in \"The squirrel funny.\"\n• Inclusion of words that are never included in\nour answers, speciﬁcally ’and’, ’are’, and ’a’.\n226\nPlural? Speciﬁc? Also? Then/All? Example rephrasing\n× × × × If something is big, is heavy, and is ﬁerce, it is strong.\n× × × ✓ If something is big, is heavy, and is ﬁerce, then it is strong.\n× × ✓ × If something is big, is heavy, and is ﬁerce, it is also strong.\n× × ✓ ✓ If something is big, is heavy, and is ﬁerce, then it is also strong.\n× ✓ × × If an animal is big, is heavy, and is ﬁerce, it is strong.\n× ✓ × ✓ If an animal is big, is heavy, and is ﬁerce, then it is strong.\n× ✓ ✓ × If an animal is big, is heavy, and is ﬁerce, it is also strong.\n× ✓ ✓ ✓ If an animal is big, is heavy, and is ﬁerce, then it is also strong.\n✓ × × × Things that are big, are heavy, and are ﬁerce, are strong.\n✓ × × ✓ All things that are big, are heavy, and are ﬁerce, are strong.\n✓ × ✓ × Things that are big, are heavy, and are ﬁerce, are also strong.\n✓ × ✓ ✓ All things that are big, are heavy, and are ﬁerce, are also strong.\n✓ ✓ × × Animals that are big, are heavy, and are ﬁerce, are strong.\n✓ ✓ × ✓ All animals that are big, are heavy, and are ﬁerce, are strong.\n✓ ✓ ✓ × Animals that are big, is heavy, and is ﬁerce, are also strong.\n✓ ✓ ✓ ✓ All animals that are big, are heavy, and are ﬁerce, are also strong.\nTable 4: A diagram demonstrating the successive changes we make to the AbductionRules knowledge bases.\n• Renaming the entity to better resemble train-\ning examples; for example, person-domain\nmodels sometimes replaced ’the crocodile’\nwith ’Cro’ while animal-domain models re-\nplaced ’Bob’ with ’the bobster’.\n• Looping the correct answer or some part\nthereof, as in \"The dog is is is is is small.\"\nor \"The rabbit is rabbit is adorable.\"\n• Incorrect capitalisation, as in \"The anne is\nwealthy.\"\n• Omission of spaces, as in \"Thebob is small.\"\nC Abduction-Person-Simple example\nFigure 4 contains an example item from Abduction-\nPerson-Simple, similarly to Figure 2’s example\nfrom Abduction-Animal-Simple.\nContext(Facts+Rules):\nFacts:Anne is dull. Dave is nice. Erin is tiny.Fiona\nis high . Fiona isstrong .Erin is small. Dave is\nclever. Fiona is heavy. Anne is sad. Anne is rough.\nErin is thin.\nRules: All things that arebig , arehigh , and\nare strong , are alsohuge .If something is poor,\nis small, and is nice, it is also huge. All things that\nare high, are rough, and are little, are also smart.\nAll things that are clever, are quiet, and are dull, are\nsmart. People that are big, are dull, and are clever,\nare also short. If a person is thin, is small, and\nis little, that person is short. If a person is thin, is\nstrong, and is quiet, that person is imperfect. Things\nthat are little, are small, and are nice, are short. If\na person is high, is poor, and is rough, then that\nperson is also imperfect. All things that are thin,\nare big, and are strong, are also huge. If something\nis clever, is nice, and is quiet, then it is smart. If\na person is poor, is rough, and is dull, then that\nperson is imperfect.\nQuestion:Fiona is huge .\nLabel: Fiona is big .\nFigure 4: An example observation, explanation, and\ncorresponding context from Abduction-Person-Simple.\nThe model must output the explanation given the con-\ntext and observation as input. Facts and rules used to\nexplain the observation are bolded while relevant at-\ntributes are highlighted.\n227",
  "topic": "Abductive reasoning",
  "concepts": [
    {
      "name": "Abductive reasoning",
      "score": 0.8746422529220581
    },
    {
      "name": "Interpretability",
      "score": 0.8020730018615723
    },
    {
      "name": "Computer science",
      "score": 0.715582013130188
    },
    {
      "name": "Exploit",
      "score": 0.621707558631897
    },
    {
      "name": "Transformer",
      "score": 0.5972042083740234
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5893316268920898
    },
    {
      "name": "Inference",
      "score": 0.5659953355789185
    },
    {
      "name": "Question answering",
      "score": 0.5499316453933716
    },
    {
      "name": "Natural language",
      "score": 0.5425094366073608
    },
    {
      "name": "Deductive reasoning",
      "score": 0.4174690842628479
    },
    {
      "name": "Natural language understanding",
      "score": 0.41425493359565735
    },
    {
      "name": "Machine learning",
      "score": 0.4119374752044678
    },
    {
      "name": "Natural language processing",
      "score": 0.4067167639732361
    },
    {
      "name": "Engineering",
      "score": 0.12097525596618652
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I154130895",
      "name": "University of Auckland",
      "country": "NZ"
    }
  ],
  "cited_by": 9
}