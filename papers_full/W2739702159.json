{
    "title": "A Layered Language Model based Hybrid Approach to Automatic Full Diacritization of Arabic",
    "url": "https://openalex.org/W2739702159",
    "year": 2017,
    "authors": [
        {
            "id": "https://openalex.org/A5073237528",
            "name": "Mohamed Al-Badrashiny",
            "affiliations": [
                "George Washington University"
            ]
        },
        {
            "id": "https://openalex.org/A2070743323",
            "name": "Abdelati Hawwari",
            "affiliations": [
                "George Washington University"
            ]
        },
        {
            "id": "https://openalex.org/A2124289572",
            "name": "Mona Diab",
            "affiliations": [
                "George Washington University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2153186553",
        "https://openalex.org/W2012804051",
        "https://openalex.org/W2063116544",
        "https://openalex.org/W2250751111",
        "https://openalex.org/W2109613320",
        "https://openalex.org/W2250816155",
        "https://openalex.org/W2136534344",
        "https://openalex.org/W4285719527",
        "https://openalex.org/W2065400286",
        "https://openalex.org/W45643337",
        "https://openalex.org/W2571755499",
        "https://openalex.org/W2026897582",
        "https://openalex.org/W2187307450",
        "https://openalex.org/W2041349462"
    ],
    "abstract": "In this paper we present a system for automatic Arabic text diacritization using three levels of analysis granularity in a layered back off manner. We build and exploit diacritized language models (LM) for each of three different levels of granularity: surface form, morphologically segmented into prefix/stem/suffix, and character level. For each of the passes, we use Viterbi search to pick the most probable diacritization per word in the input. We start with the surface form LM, followed by the morphological level, then finally we leverage the character level LM. Our system outperforms all of the published systems evaluated against the same training and test data. It achieves a 10.87% WER for complete full diacritization including lexical and syntactic diacritization, and 3.0% WER for lexical diacritization, ignoring syntactic diacritization.",
    "full_text": "Proceedings of The Third Arabic Natural Language Processing Workshop (WANLP), pages 177–184,\nValencia, Spain, April 3, 2017.c⃝2017 Association for Computational Linguistics\nA Layered Language Model based Hybrid Approach to Automatic Full\nDiacritization of Arabic\nMohamed Al-Badrashiny, Abdelati Hawwari, Mona Diab\nDepartment of Computer Science\nThe George Washington University\n{badrashiny,abhawwari,mtdiab}@gwu.edu\nAbstract\nIn this paper we present a system for auto-\nmatic Arabic text diacritization using three\nlevels of analysis granularity in a layered\nback off manner. We build and exploit di-\nacritized language models (LM) for each\nof three different levels of granularity:\nsurface form, morphologically segmented\ninto preﬁx/stem/sufﬁx, and character level.\nFor each of the passes, we use Viterbi\nsearch to pick the most probable diacriti-\nzation per word in the input. We start\nwith the surface form LM, followed by the\nmorphological level, then ﬁnally we lever-\nage the character level LM. Our system\noutperforms all of the published systems\nevaluated against the same training and\ntest data. It achieves a 10.87% WER for\ncomplete full diacritization including lexi-\ncal and syntactic diacritization, and 3.0%\nWER for lexical diacritization, ignoring\nsyntactic diacritization.\n1 Introduction\nMost languages have an orthographical system\nthat reﬂects their phonological system. Orthogra-\nphies vary in the way they represent word pro-\nnunciations. Arabic orthography employs an al-\nphabetical system that comprises consonants and\nvowels. Short vowels are typically underspeci-\nﬁed in the orthography. When present they ap-\npear as diacritical marks. Moreover, other phono-\nlogical phenomena are represented with diacritics,\nsuch as letter doubling, syllable boundary mark-\ners, elongation, etc. In this paper, we are inter-\nested in restoring most of these diacritics, making\nthem explicit in the written orthography. This pro-\ncess is referred to as diacritization/vowelization,\nor ”tashkeel” in Arabic. Absence of these dia-\ncritics from the orthography renders the text ex-\ntremely ambiguous. Accordingly, the task of di-\nacritization is quite important for many NLP ap-\nplications such as morphological analysis, text to\nspeech, POS tagging, word sense disambiguation,\nand machine translation.\nMoreover, from a human processing perspec-\ntive, having the orthography reﬂect the diacritics\nexplicitly makes for better readability comprehen-\nsion and pronunciation.\n2 Linguistic Background\nUnlike English, Arabic comprises an alphabet list\nof 28 letters. Short vowels are not explicitly\nmarked in typical orthography as stand alone let-\nters. The Arabic orthographic system employs\na list of diacritics to express short vowels. The\nArabic writing system maybe conceived to com-\nprise two levels: consonantal letters including\nconsonants and long vowels; and diacritics indi-\ncating short vowels and other pronunciation mark-\ners which are typically written above and/or below\nsuch consonantal letters.\nThe Arabic diacritics relevant to our study can\nbe characterized as follows:1\n• Short vowels (a, i, u):2, corresponding to\nthe three short vowels (fatha ’a’, kasra ’i’,\ndamma ’u’). They can occur word medially\nand/or word ﬁnally;\n• Nunation “Tanween” (F , K, N): these occur\nword ﬁnally only and they correspond to ei-\nther an an ’F’ ,in ’K’, or an un ’N’ sound.\nThey indicate indeﬁnite nominals as well as\n1There are other diacritics that we don’t consider in the\ncontext of this work.\n2We use Buckwalter (BW) transliteration scheme to rep-\nresent Arabic in Romanized script throughout the paper.\nhttp://www.qamus.org/transliteration.htm\n177\nthey could mark adverbials and some frozen\nexpressions.\n• Gemination (~), aka ”shaddah”: indicating\ndoubling of the preceding character;\n• Sukoun o: marks the absence of a vowel,\ntypically appears between syllables, as well\nas word ﬁnally to indicate jussive syntactic\nmood for verbs.\nDiacritization reﬂects morphological (including\nphonology) and grammatical information. Ac-\ncordingly, in this paper we make a distinction be-\ntween the two types of diacritization as follows:\nA) Morphological Diacritization: Reﬂect the\nmanner by which words are pronounced, not in-\ncluding the word ﬁnal diacritization except the last\nletter diacritization. Morphological diacritization\ncould be further subdivided into:\n• Word structure or lexical diacritization: this\nrepresents the internal structure of words,\nthat distinguish different possible readings\nof a phonologically ambiguous word (ho-\nmograph) when the diacritics are miss-\ning. For instance, the Arabic word mlk\ncould have the following readings: ma-\nlik (king), malak(angel/he possessed), mu-\nlok(kingdom/property), milok(ownership), or\nmal ak(gave possession to another);\n• Inﬂectional diacritization: this represents the\nmorphophonemic level of handling afﬁxa-\ntions (preﬁxes, sufﬁxes and clitics), how\nmorphemes interact with each other, making\npossible morphophonemic changes which\nare reﬂected in the phonological and ortho-\ngraphic systems. For example the Arabic\nword qAblthm could be qAbalatohum (I met\nthem), qAbalotahum (you masc. met them),\nqAbalatohum (she met them) or qAbaloti-\nhim(you fem. met them).\nB) Syntactic Diacritization: Syntactic func-\ntions are represented by adding one of short vow-\nels or nunation to the end of most of Arabic words,\nindicating the word’s grammatical function in the\nsentence. For example, in a sentence like “ zAra\nAlwaladu zamiylahu” (the boy visited his col-\nleague), the diacritization of the last letters in the\nwords Alwaladu and zamiyla indicate the syntactic\nroles of grammatical subject u, and grammatical\nobject a, respectively.\n2.1 Levels of Diacritization\nAlthough native speakers of Arabic can read the\nmajority of Arabic script without explicit dia-\ncritical marks being present, some diacritic sym-\nbols in some cases are crucial in order to disam-\nbiguate/pronounce homographical words. Histor-\nically, diacritics were invented by Arabic gram-\nmarians more than 200 years after the emergence\nof the Arabic writing system which was primarily\nconsonantal. In Modern Standard Arabic (MSA)\nscript, there are several levels of possible diacriti-\nzation:\n• No Diacritization: This level is completely\nunderspeciﬁed. The script is subject to ambi-\nguity, especially with homographical words;\n• Full Diacritization: The reverse where there\nis complete speciﬁcation, namely where each\nconsonant is followed by a diacritic. This\nlevel is used more in classical and educa-\ntional writing;\n• Partial Diacritization: This level is any-\nwhere in between the two previous levels,\ntypically writer dependent. In this case, the\nwriter adds diacritics where s/he deems ﬁt\n(Zaghouani et al., 2016).\n2.2 Challenges\nThere are a number of challenges in Arabic dia-\ncritization, we can list some of them as follows:\n• Morphological aspects: Some Arabic words\nserve as a phrase or full sentence such as\nwaS alatohA (she delivered her), waS alo-\ntuhA (I delivered her), and waS alotihA\n(you feminine drove her);\n• Syntactic aspects: Arabic is a free word-\norder language, syntactic functions are real-\nized on the morphological level via word ﬁ-\nnal diacritization in most cases. However, we\nnote changes to the penultimate orthographic\nrealization of the consonants due to syntac-\ntic position. For example, >abonA&uhu,\n>abonA}ihi, and >abonA’ahu, all corre-\nsponding to “his sons” but reﬂect differ-\nent syntactic case: nominative, genitive, ac-\ncusative, respectively.\n• Phonological aspects: The phonological\nsystem exhibits assimilation in cases of af-\nﬁxation word ﬁnally. For example the\n178\nword ﬁnal possessive sufﬁx h meaning ”his”\nin the following word takes on the same\nvowel/diacritic as that of the lexeme it\nis attached to: kitAbi+hi (his book) and\nkitAbu+hu (his book). It is important to note\nthat the short vowel diacritic attached to theh\nsufﬁx has no semantic or syntactic interpreta-\ntion, it is a pure assimilation vowel harmony\neffect.\n3 Approach\nFigure 1 illustrates the system architecture of\nour proposed solution for MSA Full diacritiza-\ntion. Our approach relies on having fully dia-\ncritized data for building various types of lan-\nguage models at training time: a word level lan-\nguage model (WLM), a morpheme level language\nmodel (MLM), a character+diacritic level lan-\nguage model (CLM). The WLM is created in the\ndiacritized untokenized surface level words. We\nexperiment with 1-5 gram WLMs. The MLM\nare created using the same WLMs but after tok-\nenizing them into preﬁx, stem, and sufﬁx compo-\nnents where each is fully diacritized. Thus each\n1 gram in the WLM is equivalent to 3 grams in\nthe MLM, i.e. this renders MLMs of 3, 6, 9, 12,\nand 15, corresponding to the WLM of 1, 2, 3, 4,\n5, respectively. Finally for CLMs, we are using\nthe WLMs but after segmenting them into char-\nacters+associated diacritics. The maximum gram\nsize we managed to build is 20 grams. Thus, each\n1 gram in the word level WLM is equivalent to 4\ngrams in the character level, given that the small-\nest word in Arabic is two consonants long which\nis equivalent to 4 characters, i.e. each consonant is\nassociated with at least one diacritic. This means\nthat the LMs we are experimenting with for the\ncharacter level are of sizes 4, 8, 12, 16, and 20\ngrams.\nAt test time, the undiacritized input text goes\nthrough the following pipeline:\na) Word-Level Diacritization: In this step, we\nleverage the WLM created at train time using all\npossible diacritizations for each word in the input\nraw text using the training data. If there are new\nwords (out of vocabulary [OOV]) that have not\nbeen seen in the training data, they are tagged as\nunknown (UNK). A lattice search technique (for\nexample: Viterbi or A* search) is then used to se-\nlect the best diacritization for each word based on\ncontext.\nb) Morpheme Level Diacritization:The output\nfrom the ﬁrst step is being morphologically ana-\nlyzed using SAMA (Maamouri et al., 2010). We\nonly keep the morphological analyses that match\nthe diacritization from the WLM. But if there is\nany word that is tagged as UNK, we keep all of\nits morphological analyses if they exist. If SAMA\nfailed to ﬁnd a possible morphological solution for\nany word (ex: non-Arabic word), it is marked as\nUNK. The MLM is used via a lattice search tech-\nnique to pick the best morphological solution for\neach word; hence the best diacritization.\nc) Character-Level Diacritization: If there are\nstill some UNK words after steps (a) and (b), the\nCLM is used to ﬁnd a plausible solution for them.\n4 Experimental Setup\n4.1 Data\nSeveral studies have been carried out on the prob-\nlem of full automatic diacritization for MSA. Five\nof these studies, that also yield the most compet-\nitive results despite approaching the problem in\ndifferent ways, use and report on the same ex-\nact data sets. These studies are Zitouni et al.\n(2006), Habash and Rambow (2007), Rashwan et\nal. (2011), Abandah et al. (2015), and Belinkov\nand Glass (2015). We will use the same data which\nis LDC’s Arabic Treebank of diacritized news\nstories-Part 3 v1.0: catalog number LDC2004T11\nand ISBN 1-58563-298-8. The corpus includes\ncomplete Full diacritization comprising both mor-\nphological and syntactic diacritization. This cor-\npus includes 600 documents from the Annahar\nNews Text. There are a total of 340,281 words.\nThe data is split as follows into two sets:\n• Training data comprising approximately\n288K words;\n• Test data (TEST): comprises 90 documents\nselected by taking the last 15% of the total\nnumber of documents in chronological order\ndating from “20021015 0101” to “20021215\n0045”. It comprises approximately 52K\nwords.\nBut having a single set TEST serving as both\ntest and dev data is not correct which is what pre-\nvious studies have done. Therefore, we split the\ndata into three parts instead of two. We split off\n10% of the training data and use it as a develop-\nment set, rendering our training data (TRAIN) to\n179\nFully \nDiacritized\nText\nSAMAA*/Viterbi \nSearch \nWord-Level\nLM\nA*/Viterbi \nSearch \nMorph-Level\nLM\nUndiacritized\nText\nMorphologically\nDiacritized\nWords + UNK\nAllowed\nMorphological\nAnalysis + UNK\nA*/Viterbi \nSearch \nChar-Level\nLM\nFigure 1: System Architecture.\ncomprise only 90% of the original training data.\nWe keep the same exact test data, TEST, as the\nprevious studies however. Accordingly, the new\ncurrent training data for this paper is roughly 259K\nwords and the development set (DEV) comprises\napproximately 29K words. DEV is used for tuning\nour system.\nIn all of our experiments, we use TRAIN to\ntrain and build our models and DEV to ﬁnd the\nbest conﬁguration parameters. TEST is used as\nheld out data. It is only evaluated using the result-\ning best models on DEV .\n4.2 Evaluation Metrics\nWe adopt the same metrics used by Zitouni et\nal. (2006), Habash and Rambow (2007), Rash-\nwan et al. (2011), Abandah et al. (2015), and Be-\nlinkov and Glass (2015). These are word error rate\n(WER) and character error rate (CER). CER com-\npares the predicted words to the gold words on the\ncharacter level. WER compares the predicted di-\nacritized word as a whole to the gold diacritized\nword. If there is one error in a word, the whole\nword is considered incorrect. All words are evalu-\nated including digits and punctuation. In the case\nof morphological diacritization, word ﬁnal diacrit-\nics are ignored. In the case of syntactic diacritiza-\ntion only word ﬁnal diacritics are considered. Fi-\nnally in the Full diacritization case, both morpho-\nlogical and syntactic diacritization are considered.\n4.3 Baselines\nWe compare our approach against the following\nbaselines:3\n• Zitouni et al.: The best published results by\nZitouni et al. (2006);\n3The descriptions of these baselines systems are in sec-\ntion: 7-Related Work\n• Habash et al.: The best published results by\nHabash and Rambow (2007);\n• Rashwan et al.: The best published results by\nRashwan et al. (2011);\n• Abandah et al.: The best published results by\nAbandah et al. (2015);\n• Belinkov and Glass: The best published re-\nsults by Belinkov and Glass (2015).\n5 Evaluation\nTable 1 illustrates the morphological and Full\n(morphological+syntactic) diacritization perfor-\nmance on DEV using the lattice search on the\nword, morpheme, and character levels. The\nlanguage models are all built using the TRAIN\ndataset.\nThe table shows ﬁve experiments using A*\nsearch using 1, 2, 3, 4, and 5 grams LMs. 4 And\ntwo experiments using Viterbi search because the\nimplementation we have for the Viterbi search\nsupports 2 grams as a maximum size. The best\nperformance is yielded by the Viterbi algorithm\nand 2-grams LMs (i.e. 2-grams for WLM, 6-\ngrams for MLM, and 8-grams for CLM). It yields\n6.11% WER for Full diacritization (FULL), cor-\nresponding to 2.61% WER for morphological di-\nacritization (MORPH), i.e. by ignoring word ﬁnal\nsyntactic diacritics.\nTable 2 compares the performance of our sys-\ntem to the baselines systems. It shows that our\nsystem is outperforming all of the published CER\nand WER on “MORPH” level. On “FULL” level,\nwe outperform all of the baselines except (Aban-\ndah et al., 2015). They are doing better on the syn-\ntactic level diacritization. Their system is based\n4Note: every 1-gram in word level is equivalent to 3-\ngrams morphological level and 4-grams in characters level\n180\nFULL MORPH\nLattice Search Method LM-Size WER CER WER CER\nViterbi 1 6.67% 1.14% 3.13% 0.61%\nViterbi 2 6.11% 1.06% 2.61% 0.55%\nA* 1 6.51% 1.09% 3.01% 0.57%\nA* 2 6.28% 1.04% 2.77% 0.53%\nA* 3 6.26% 1.04% 2.74% 0.53%\nA* 4 6.26% 1.04% 2.74% 0.53%\nA* 5 6.18% 1.03% 2.66% 0.51%\nTable 1: System performance on DEV . The best setup is by using the Viterbi algorithm via 2 grams LMs.\non a deep bidirectional long short-term memory\n(LSTM) model. These kinds of models can ex-\nploit long-range contexts; which could yield the\nbetter performance on the syntactic diacritization\nlevel. It is also worth mentioning that they are us-\ning a post-processing correction layer that applies\nsome rules to ﬁx some of the diacritization errors\nafter the LSTM.\nIt should be highlighted, that in contrast to the\nprevious studies, TEST remained a complete held\nout data set that was not explored at all during the\ntuning phase of the system development, where for\nthe previous studies TEST was used as both a de-\nvelopment and test set.\n6 Error Analysis\nBy reviewing the errors rendered by our system\nand comparing them to the gold data we discov-\nered several issues in the training and test data that\naffected the performance and evaluation results.\nWe list them as follows:\nUndiacritized words: There are many cases in\nboth the training and test data where the words\nare completely undiacritized. Since we rely on\nfully diacritized texts to build our various language\nmodels, this type of error affects our system in two\nways:\n• Errors in the training data affect the quality\nof the language models that are built;\n• Errors in the test data decrease the accuracy\nof our system because such cases are being\ncounted as incorrect even if they are cor-\nrectly diacritized by our system. Upon man-\nual inspection, for example, our system ren-\nders the correct diacritization for the words:\nxal af ”left behind/gave birth” and bAruwd\n”gun powder” are counted as errors because\nthey are not diacritized at all in the gold test\ndata set).\nMissing Case marker: 25.2% of the syntactic\ndiacritization errors are due to missing syntactic\ndiacritization from the gold TEST words. Table 3\nillustrates some examples of that.\n7 Related Work\nMany research efforts addressed the problem of\nautomatic full Arabic diacritization, especially for\nMSA.\nGal (2002) developed a statistical system using\nHMM to restore Arabic diacritics and applied it\non the Holy Quran as a corpus. Their approach\ndid not include any language-speciﬁc knowledge.\nThis system achieved a WER of 86% for morpho-\nlogical diacritization without syntactic diacritiza-\ntion.\nEl-Imam (2004) developed a comprehensive set\nof well-deﬁned language-dependent rules, that are\naugmented by a dictionary, to be used in the tran-\nscription of graphemes into phonemes.\nNelken and Shieber (2005) developed a proba-\nbilistic model for Arabic diacritization using a ﬁ-\nnite state transducer, and trigram word and char-\nacter based language models. Their approach\nused the ATB and achieved 7.33% WER without\ncase endings (morphological diacritization) and\n23.61% WER with case ending.\nAnanthakrishnan et al. (2005) leveraged a word-\nlevel trigram model combined with a four-gram\ncharacter language model. The authors used ATB\nas training data and used the LDC TDT4 Broad-\ncast News data set as test data. The reported word\naccuracy using this model was 80.21%.\nZitouni et al. (2006) presented a statistical\nmodel based on a Maximum Entropy framework.\nTheir approach integrates different sources of\n181\nFULL MORPH\nTraining Data System WER CER WER CER\nTRAIN+DEV Zitouni et al. 18.00% 5.50% 7.90% 2.50%\nTRAIN+DEV Habash et al. 14.90% 4.80% 5.50% 2.20%\nTRAIN+DEV Rashwan et al. 12.50% 3.80% 3.10% 1.20%\nTRAIN+DEV Abandah et al. 9.07% 2.72% 4.34% 1.38%\nTRAIN Belinkov and Glass N/A 4.85 N/A N/A\nTRAIN Our System 10.90% 1.60% 3.10% 0.60%\nTRAIN+DEV Our System 10.87% 1.60% 3.00% 0.59%\nTable 2: Our System performance against baselines\nWord POS\nAlHumayoDiy∼ DET+NOUN PROP\nmud∼ap NOUN+NSUFF FEM SG\nwaragom CONJ+NOUN\nEam∼An NOUN PROP\ngayor NOUN\nIixorAj NOUN\nTable 3: Examples for the missing syntactic dia-\ncritics in TEST\nknowledge including lexical, segment-based and\nPOS features. They achieved a CER of 5.5% and\na WER of 18.0% for morphological and syntac-\ntic diacritization. By ignoring case endings, they\nobtained a CER of 2.5% and a WER of 7.9%.\nElshafei et al. (2006) proposed a diacritic\nrestoration system which uses HMM for modeling\nand a Viterbi algorithm to select the most proba-\nble diacritized form of a sentence. The result was\n4.1% errors in the diacritical marking of letters.\nHabash and Rambow (2007) proposed a dia-\ncritization system that is based on a lexical re-\nsource, combining a tagger and a lexeme language\nmodel. The system gets a list with all potential\nanalysis for each word, then applies a series of\nSupport Vector Machine (SVM) classiﬁers to sev-\neral morphological dimensions, then combines the\nvarious values for the dimensions to decide on the\nﬁnal analysis chosen from among the various pos-\nsible analyses provided by an underlying morpho-\nlogical analyzer such as BAMA. They achieved a\nCER of 5.5% and a WER of 14.9% for morpho-\nlogical and syntactic diacritization. And CER of\n2.2% and a WER of 5.5% by ignoring case end-\nings.\nShaalan et al. (2009) proposed a hybrid ap-\nproach that relies on lexicon retrieval, a bigram\nword level language model, and SVM classiﬁca-\ntion. The system achieves a reported WER of\n12.16% for combined morphological and syntac-\ntic diacritization.\nRashwan et al. (2011) developed a hybrid ap-\nproach with a two-layer stochastic system. They\nsplit the input sentence into smaller segments,\nwhere each segment is consisting of at leas one\nword. Then they use a WLM to diacritize the\nsegments that all of its words can be found in\nthe unigrams of the WLM. Another MLM is used\nto diacritize the segments that are out of vocabu-\nlary from the point of view if the WLM. The ﬁ-\nnal output is the combination of the all segments.\nThey achieved 12.5% WER and 3.8% for com-\nbined morphological and syntactic diacritization.\nAnd 3.1% WER and 1.2% CER by ignoring the\ncase ending.\nHifny (2012) developed a diacritic restoration\nsystem which uses dynamic programming (DP),\nn-gram language model, and smoothing. The au-\nthor reported a WER of 3.4% for morphological\ndiacritization and a WER 8.9% for combined mor-\nphological and syntactic diacritization.\nMADAMIRA (Pasha et al., 2014) is a morpho-\nlogical analysis and disambiguation tool of Ara-\nbic. It applies SVM and language models to pre-\ndict the word’s morphological features. The dia-\ncritization accuracy of MADAMIRA is 86.3% on\nMSA and 83.2% on the Egyptian dialect.\nAbandah et al. (2015) trained a recurrent neural\nnetwork (RNN) to transcribe undiacritized Arabic\ntext with fully diacritized sentences. After that\nthey used some post-processing correction rules to\ncorrect the output from the RNN. For example, if\nthe undiacritized word can be found in the training\ndata but its diacritization by the the RNN does not\nexist, they replace the output diacritization by the\n182\nvariant from the training data leveraging a min-\nimum edit distance algorithm. They achieved a\nCER of 2.72% and a WER of 9.07% for morpho-\nlogical and syntactic diacritization. And CER of\n1.38% and a WER of 4.34% by ignoring case end-\nings.\nBelinkov and Glass (2015) developed a recur-\nrent neural network with long-short term memory\n(LSTM) layers for predicting diacritics in Arabic\ntext. They achieved a CER of 4.85% for morpho-\nlogical and syntactic diacritization.\nAlthough the system of Rashwan et al. (2011)\nlooks close to our system, but there is a signif-\nicant difference between the two systems. The\nmethod they used of splitting the input sentence\ninto smaller segments and diacritizing each seg-\nment separate from others, results in information\nloss yielding a suboptimal solution. Unlike, their\napproach, we do not split the sentences at the OOV\nwords. Instead, we pass on the probability val-\nues of the unknowns. Therefore, even if there are\none or more words that are OOV from the point of\nview of any of our LMs, the searching technique\nremains able to beneﬁts from surrounding words.\n8 Conclusion\nIn this paper we introduce a hybrid approach to\nfull Arabic diacritization that leverages three un-\nderlying language models on different levels of\nlinguistic representation with a ﬁltering step that\nrelies on a morphological analyzer to ﬁnd the most\nprobable diacritization for undiacritized surface\nform Arabic text in context. The results show that\nthe presented approach outperforms all published\nsystems to date using the same training and test\ndata.\nReferences\nGheith A. Abandah, Alex Graves, Balkees Al-Shagoor,\nAlaa Arabiyat, Fuad Jamour, and Majid Al-Taee.\n2015. Automatic diacritization of arabic text us-\ning recurrent neural networks. International Journal\non Document Analysis and Recognition (IJDAR),\n18(2):183–197.\nSankaranarayanan Ananthakrishnan, Srinivas Banga-\nlore, and Shrikanth S. Narayanan. 2005. Auto-\nmatic diacritization of arabic transcripts for auto-\nmatic speech recognition. In Proceedings of the In-\nternational Conference on Natural Language Pro-\ncessing (ICON), Kanpur, India, December.\nYonatan Belinkov and James Glass. 2015. Arabic di-\nacritization with recurrent neural networks. In Pro-\nceedings of the 2015 Conference on Empirical Meth-\nods in Natural Language Processing, pages 2281–\n2285, Lisbon, Portugal, September. Association for\nComputational Linguistics.\nYousif A. El-Imam. 2004. Phonetization of arabic:\nrules and algorithms. Computer Speech and Lan-\nguage, 18(4):339 – 373.\nMoustafa Elshafei, Husni Al-muhtaseb, and Mansour\nAlghamdi. 2006. Statistical methods for auto-\nmatic diacritization of arabic text. In Proceed-\nings of Saudi 18th National Computer Conference\n(NCC18), Riyadh, Saudi Arabia.\nYa’akov Gal. 2002. An hmm approach to vowel\nrestoration in arabic and hebrew. In Proceed-\nings of the ACL-02 Workshop on Computational\nApproaches to Semitic Languages, SEMITIC ’02,\npages 1–7, Stroudsburg, PA, USA. Association for\nComputational Linguistics.\nNizar Habash and Owen Rambow. 2007. Arabic di-\nacritization through full morphological tagging. In\nHuman Language Technologies 2007: The Confer-\nence of the North American Chapter of the Asso-\nciation for Computational Linguistics; Companion\nVolume, Short Papers, NAACL-Short ’07, pages 53–\n56, Stroudsburg, PA, USA. Association for Compu-\ntational Linguistics.\nYasser Hifny. 2012. Higher order n-gram language\nmodels for arabic diacritics restoration. In Proceed-\nings of the 12th Conference on Language Engineer-\ning (ESOLEC 12), Cairo, Egypt.\nMohamed Maamouri, Dave Graff, Basma Bouziri,\nSondos Krouna, Ann Bies, and Seth Kulick. 2010.\nLdc standard arabic morphological analyzer (sama)\nversion 3.1.\nRani Nelken and Stuart M. Shieber. 2005. Arabic di-\nacritization using weighted ﬁnite-state transducers.\nIn Proceedings of the ACL Workshop on Compu-\ntational Approaches to Semitic Languages, Semitic\n’05, pages 79–86, Stroudsburg, PA, USA. Associa-\ntion for Computational Linguistics.\nArfath Pasha, Mohamed Al-Badrashiny, Mona Diab,\nAhmed El Kholy, Ramy Eskander, Nizar Habash,\nManoj Pooleery, Owen Rambow, and Ryan M. Roth.\n2014. MADAMIRA: A Fast, Comprehensive Tool\nfor Morphological Analysis and Disambiguation of\nArabic. In Proceedings of LREC, Reykjavik, Ice-\nland.\nM. A.A. Rashwan, M. A.S.A.A. Al-Badrashiny, M. At-\ntia, S. M. Abdou, and A. Rafea. 2011. A stochastic\narabic diacritizer based on a hybrid of factorized and\nunfactorized textual features. Trans. Audio, Speech\nand Lang. Proc., 19(1):166–175, January.\nKhaled Shaalan, Hitham M. Abo Bakr, and Ibrahim\nZiedan. 2009. A hybrid approach for building ara-\nbic diacritizer. In Proceedings of the EACL 2009\nWorkshop on Computational Approaches to Semitic\n183\nLanguages, Semitic ’09, pages 27–35, Stroudsburg,\nPA, USA. Association for Computational Linguis-\ntics.\nWajdi Zaghouani, Houda Bouamor, Abdelati Hawwari,\nMona Diab, Ossama Obeid, Mahmoud Ghoneim,\nSawsan Alqahtani, and Kemal Oﬂazer. 2016.\nGuidelines and framework for a large scale arabic\ndiacritized corpus. In Nicoletta Calzolari (Con-\nference Chair), Khalid Choukri, Thierry Declerck,\nSara Goggi, Marko Grobelnik, Bente Maegaard,\nJoseph Mariani, Helene Mazo, Asuncion Moreno,\nJan Odijk, and Stelios Piperidis, editors, Proceed-\nings of the Tenth International Conference on Lan-\nguage Resources and Evaluation (LREC 2016) ,\nParis, France, may. European Language Resources\nAssociation (ELRA).\nImed Zitouni, Jeffrey S. Sorensen, and Ruhi Sarikaya.\n2006. Maximum entropy based restoration of ara-\nbic diacritics. In Proceedings of the 21st Inter-\nnational Conference on Computational Linguistics\nand the 44th Annual Meeting of the Association\nfor Computational Linguistics, ACL-44, pages 577–\n584, Stroudsburg, PA, USA. Association for Com-\nputational Linguistics.\n184"
}