{
  "title": "Semi-supervised sequence tagging with bidirectional language models",
  "url": "https://openalex.org/W2610748790",
  "year": 2017,
  "authors": [
    {
      "id": "https://openalex.org/A2108007937",
      "name": "Matthew Peters",
      "affiliations": [
        "Allen Institute for Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2016553074",
      "name": "Waleed Ammar",
      "affiliations": [
        "Allen Institute for Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2071644166",
      "name": "Chandra Bhagavatula",
      "affiliations": [
        "Allen Institute for Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2147949120",
      "name": "Russell Power",
      "affiliations": [
        "Allen Institute for Artificial Intelligence"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2507974895"
  ],
  "abstract": "Pre-trained word embeddings learned from unlabeled text have become a standard component of neural network architectures for NLP tasks. However, in most cases, the recurrent network that operates on word-level representations to produce context sensitive representations is trained on relatively little labeled data. In this paper, we demonstrate a general semi-supervised approach for adding pre- trained context embeddings from bidirectional language models to NLP systems and apply it to sequence labeling tasks. We evaluate our model on two standard datasets for named entity recognition (NER) and chunking, and in both cases achieve state of the art results, surpassing previous systems that use other forms of transfer or joint learning with additional labeled data and task specific gazetteers.",
  "full_text": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1756–1765\nVancouver, Canada, July 30 - August 4, 2017.c⃝2017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1161\nProceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1756–1765\nVancouver, Canada, July 30 - August 4, 2017.c⃝2017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1161\nSemi-supervised sequence tagging with bidirectional language models\nMatthew E. Peters, Waleed Ammar, Chandra Bhagavatula, Russell Power\nAllen Institute for Artiﬁcial Intelligence\n{matthewp,waleeda,chandrab,russellp}@allenai.org\nAbstract\nPre-trained word embeddings learned\nfrom unlabeled text have become a stan-\ndard component of neural network archi-\ntectures for NLP tasks. However, in most\ncases, the recurrent network that oper-\nates on word-level representations to pro-\nduce context sensitive representations is\ntrained on relatively little labeled data.\nIn this paper, we demonstrate a general\nsemi-supervised approach for adding pre-\ntrained context embeddings from bidi-\nrectional language models to NLP sys-\ntems and apply it to sequence labeling\ntasks. We evaluate our model on two stan-\ndard datasets for named entity recognition\n(NER) and chunking, and in both cases\nachieve state of the art results, surpassing\nprevious systems that use other forms of\ntransfer or joint learning with additional\nlabeled data and task speciﬁc gazetteers.\n1 Introduction\nDue to their simplicity and efﬁcacy, pre-trained\nword embedding have become ubiquitous in NLP\nsystems. Many prior studies have shown that they\ncapture useful semantic and syntactic information\n(Mikolov et al., 2013; Pennington et al., 2014) and\nincluding them in NLP systems has been shown to\nbe enormously helpful for a variety of downstream\ntasks (Collobert et al., 2011).\nHowever, in many NLP tasks it is essential to\nrepresent not just the meaning of a word, but also\nthe word in context. For example, in the two\nphrases “A Central Bank spokesman” and “The\nCentral African Republic”, the word ‘Central’ is\nused as part of both an Organization and Location.\nAccordingly, current state of the art sequence tag-\nging models typically include a bidirectional re-\ncurrent neural network (RNN) that encodes token\nsequences into a context sensitive representation\nbefore making token speciﬁc predictions (Yang\net al., 2017; Ma and Hovy, 2016; Lample et al.,\n2016; Hashimoto et al., 2016).\nAlthough the token representation is initialized\nwith pre-trained embeddings, the parameters of\nthe bidirectional RNN are typically learned only\non labeled data. Previous work has explored meth-\nods for jointly learning the bidirectional RNN with\nsupplemental labeled data from other tasks (e.g.,\nSøgaard and Goldberg, 2016; Yang et al., 2017).\nIn this paper, we explore an alternate semi-\nsupervised approach which does not require ad-\nditional labeled data. We use a neural language\nmodel (LM), pre-trained on a large, unlabeled cor-\npus to compute an encoding of the context at each\nposition in the sequence (hereafter an LM embed-\nding) and use it in the supervised sequence tag-\nging model. Since the LM embeddings are used to\ncompute the probability of future words in a neu-\nral LM, they are likely to encode both the semantic\nand syntactic roles of words in context.\nOur main contribution is to show that the con-\ntext sensitive representation captured in the LM\nembeddings is useful in the supervised sequence\ntagging setting. When we include the LM embed-\ndings in our system overall performance increases\nfrom 90.87% to 91.93% F1 for the CoNLL 2003\nNER task, a more then 1% absolute F1 increase,\nand a substantial improvement over the previous\nstate of the art. We also establish a new state of\nthe art result (96.37% F1) for the CoNLL 2000\nChunking task.\nAs a secondary contribution, we show that us-\ning both forward and backward LM embeddings\nboosts performance over a forward only LM. We\nalso demonstrate that domain speciﬁc pre-training\nis not necessary by applying a LM trained in the\nnews domain to scientiﬁc papers.\n1756\n2 Language model augmented sequence\ntaggers (TagLM)\n2.1 Overview\nThe main components in our language-model-\naugmented sequence tagger (TagLM) are illus-\ntrated in Fig. 1. After pre-training word embed-\ndings and a neural LM on large, unlabeled corpora\n(Step 1), we extract the word and LM embeddings\nfor every token in a given input sequence (Step 2)\nand use them in the supervised sequence tagging\nmodel (Step 3).\n2.2 Baseline sequence tagging model\nOur baseline sequence tagging model is a hierar-\nchical neural tagging model, closely following a\nnumber of recent studies (Ma and Hovy, 2016;\nLample et al., 2016; Yang et al., 2017; Chiu and\nNichols, 2016) (left side of Figure 2).\nGiven a sentence of tokens (t1,t2,...,t N ) it\nﬁrst forms a representation, xk, for each token by\nconcatenating a character based representation ck\nwith a token embedding wk:\nck = C(tk; θc)\nwk = E(tk; θw)\nxk = [ck; wk] (1)\nThe character representation ck captures morpho-\nlogical information and is either a convolutional\nneural network (CNN) (Ma and Hovy, 2016; Chiu\nand Nichols, 2016) or RNN (Yang et al., 2017;\nLample et al., 2016). It is parameterized by\nC(·,θc) with parameters θc. The token embed-\ndings, wk, are obtained as a lookup E(·,θw), ini-\ntialized using pre-trained word embeddings, and\nﬁne tuned during training (Collobert et al., 2011).\nTo learn a context sensitive representation, we\nemploy multiple layers of bidirectional RNNs. For\neach token position, k, the hidden state hk,i of\nRNN layer i is formed by concatenating the hid-\nden states from the forward ( − →h k,i) and backward\n(← −h k,i) RNNs. As a result, the bidirectional RNN\nis able to use both past and future information to\nmake a prediction at token k. More formally, for\nthe ﬁrst RNN layer that operates on xk to output\nhk,1:\n− →h k,1 = − →R1(xk,− →h k−1,1; θ− →R1\n)\n← −h k,1 = ← −R1(xk,← −h k+1,1; θ← −R1\n)\nhk,1 = [− →h k,1; ← −h k,1] (2)\nStep\t2:\tPrepare\tword\tembedding\tand\tLM\tembedding\tfor\teach\ttoken\tin\tthe\tinput\tsequence.\nThe\tneed\tto\tcapture\tfuture\tcontext\tin\tthe\tLM\tembeddingssuggests\t itis\tbeneficial\tto\talso\tconsider\ta\t\\textit{backward}\tLM\tin\tadditionalto the\ttraditional\t\\textit{f orward}\tLM.\t\tA\tbackward\tLM\tpredicts\tthe\tprevious\ttoken\tgiven\tthe\tfuture\tcontext.\t\tGiven\ta\tsentence\twith\t$N$\ttokens,\tit\tcomputes\\[P(t_{ k-1}\t|\tt_k,\tt_{k+1},\t...,\tt_N).\\]A\tbackward\t LM\tcan\tbe\timplemTheneed\tto\tcapture\tfuture\tcontext\tin\tthe\tLM\tembeddingssuggests\titis\tbeneficial\tto\talso\tconsider\ta\t\\textit{backward}\tLM\tin\tadditionaltothe\ttraditional\t\\textit{f orward}\t LM.\t\tA\tbackward\tLM\tpredicts\tthe\tprevious\ttoken\tgiven\tthe\tfuture\tcontext.\t\tGiven\ta\tsentence\twith\t$N$\ttokens,\tit\tcomputes\\[P(t_{ k-1}\t|\t t_k,\tt_{k+1},\t...,\tt_N).\\]A\tbackward\tLM\tcan\tbe\timplemented\tin\tan\tanalogous\tway\tto\ta\tforward\tLM\tand\tproduces\tan\tembedding\t$\\overleftarrow{\\mathbf{h}}^{LM}_k$,\tfor\tthe\tsequence\t$(t_k,\t t_{k+1},\t...,\tt_N)$,\tthe\toutput\tembeddingsof\tthe\ttop\tlayer\tLSTM.\tThe\tneed\tto\tcapture\tfuture\tcontext\tin\tthe\tLM\tembeddingssuggests\titis\t beneficial\tto\talso\tconsider\ta\t\\textit{backward}\tLM\tin\tadditionaltothe\t traditional\t\\textit{f orward}\tLM.\t\tA\tbackward\tLM\tpredicts\tthe\tprevious\ttoken\tgiven\tthe\tfuture\tcontext.\t\tGiven\ta\tsentence\twith\t$N$\ttokens,\tit\tcomputes\\[P(t_{ k-1}\t|\tt_k,\tt_{k+1},\t...,\tt_N).\\]A\tbackward\t LM\tcan\tbe\timplemented\tin\tan\tanalogous\tway\tto\ta\tforward\tLM\tand\tproduces\tan\tembedding\t$\\overleftarrow{\\mathbf{h}}^{LM}_k$,\tfor\tthe\tsequence\t$(t_k,\tt_{k+1},\t...,\tt_N)$,\tthe\toutput\tembeddingsof\tthe\t top\tlayer\tLSTM.entedin\tan\tanalogous\tway\tto\ta\tforward\tLM\tand\tproduces\tan\tembedding\t$\\overleftarrow{\\mathbf{h}}^{LM}_k$,\tfor\tthe\tsequence\t$(t_k,\tt_{k+1},\t...,\tt_N)$,\tthe\toutput\tembeddingsof\tthe\t top\tlayer\tLSTM.\nThe\tneed\tto\tcapture\tfuture\tcontext\tin\tthe\tLM\tembeddingssuggests\t itis\tbeneficial\tto\talso\tconsider\ta\t\\textit{backward}\tLM\tin\tadditionalto the\ttraditional\t\\textit{f orward}\tLM.\t\tA\tbackward\tLM\tpredicts\tthe\tprevious\ttoken\tgiven\tthe\tfuture\tcontext.\t\tGiven\ta\tsentence\twith\t$N$\ttokens,\tit\tcomputes\\[P(t_{ k-1}\t|\tt_k,\tt_{k+1},\t...,\tt_N).\\]A\tbackward\t LM\tcan\tbe\timplemTheneed\tto\tcapture\tfuture\tcontext\tin\tthe\tLM\tembeddingssuggests\titis\tbeneficial\tto\talso\tconsider\ta\t\\textit{backward}\tLM\tin\tadditionaltothe\ttraditional\t\\textit{f orward}\t LM.\t\tA\tbackward\tLM\tpredicts\tthe\tprevious\ttoken\tgiven\tthe\tfuture\tcontext.\t\tGiven\ta\tsentence\twith\t$N$\ttokens,\tit\tcomputes\\[P(t_{ k-1}\t|\t t_k,\tt_{k+1},\t...,\tt_N).\\]A\tbackward\tLM\tcan\tbe\timplemented\tin\tan\tanalogous\tway\tto\ta\tforward\tLM\tand\tproduces\tan\tembedding\t$\\overleftarrow{\\mathbf{h}}^{LM}_k$,\tfor\tthe\tsequence\t$(t_k,\t t_{k+1},\t...,\tt_N)$,\tthe\toutput\tembeddingsof\tthe\ttop\tlayer\tLSTM.\tThe\tneed\tto\tcapture\tfuture\tcontext\tin\tthe\tLM\tembeddingssuggests\titis\t beneficial\tto\talso\tconsider\ta\t\\textit{backward}\tLM\tin\tadditionaltothe\t traditional\t\\textit{f orward}\tLM.\t\tA\tbackward\tLM\tpredicts\tthe\tprevious\ttoken\tgiven\tthe\tfuture\tcontext.\t\tGiven\ta\tsentence\twith\t$N$\ttokens,\tit\tcomputes\\[P(t_{ k-1}\t|\tt_k,\tt_{k+1},\t...,\tt_N).\\]A\tbackward\t LM\tcan\tbe\timplemented\tin\tan\tanalogous\tway\tto\ta\tforward\tLM\tand\tproduces\tan\tembedding\t$\\overleftarrow{\\mathbf{h}}^{LM}_k$,\tfor\tthe\tsequence\t$(t_k,\tt_{k+1},\t...,\tt_N)$,\tthe\toutput\tembeddingsof\tthe\t top\tlayer\tLSTM.entedin\tan\tanalogous\tway\tto\ta\tforward\tLM\tand\tproduces\tan\tembedding\t$\\overleftarrow{\\mathbf{h}}^{LM}_k$,\tfor\tthe\tsequence\t$(t_k,\tt_{k+1},\t...,\tt_N)$,\tthe\toutput\tembeddingsof\tthe\t top\tlayer\tLSTM.\nThe\tneed\tto\tcapture\tfuture\tcontext\tin\tthe\tLM\tembeddingssuggests\t itis\tbeneficial\tto\talso\tconsider\ta\t\\textit{backward}\tLM\tin\tadditionalto the\ttraditional\t\\textit{f orward}\tLM.\t\tA\tbackward\tLM\tpredicts\tthe\tprevious\ttoken\tgiven\tthe\tfuture\tcontext.\t\tGiven\ta\tsentence\twith\t$N$\ttokens,\tit\tcomputes\\[P(t_{ k-1}\t|\tt_k,\tt_{k+1},\t...,\tt_N).\\]A\tbackward\t LM\tcan\tbe\timplemTheneed\tto\tcapture\tfuture\tcontext\tin\tthe\tLM\tembeddingssuggests\titis\tbeneficial\tto\talso\tconsider\ta\t\\textit{backward}\tLM\tin\tadditionaltothe\ttraditional\t\\textit{f orward}\t LM.\t\tA\tbackward\tLM\tpredicts\tthe\tprevious\ttoken\tgiven\tthe\tfuture\tcontext.\t\tGiven\ta\tsentence\twith\t$N$\ttokens,\tit\tcomputes\\[P(t_{ k-1}\t|\t t_k,\tt_{k+1},\t...,\tt_N).\\]A\tbackward\tLM\tcan\tbe\timplemented\tin\tan\tanalogous\tway\tto\ta\tforward\tLM\tand\tproduces\tan\tembedding\t$\\overleftarrow{\\mathbf{h}}^{LM}_k$,\tfor\tthe\tsequence\t$(t_k,\t t_{k+1},\t...,\tt_N)$,\tthe\toutput\tembeddingsof\tthe\ttop\tlayer\tLSTM.\tThe\tneed\tto\tcapture\tfuture\tcontext\tin\tthe\tLM\tembeddingssuggests\titis\t beneficial\tto\talso\tconsider\ta\t\\textit{backward}\tLM\tin\tadditionaltothe\t traditional\t\\textit{f orward}\tLM.\t\tA\tbackward\tLM\tpredicts\tthe\tprevious\ttoken\tgiven\tthe\tfuture\tcontext.\t\tGiven\ta\tsentence\twith\t$N$\ttokens,\tit\tcomputes\\[P(t_{ k-1}\t|\tt_k,\tt_{k+1},\t...,\tt_N).\\]A\tbackward\t LM\tcan\tbe\timplemented\tin\tan\tanalogous\tway\tto\ta\tforward\tLM\tand\tproduces\tan\tembedding\t$\\overleftarrow{\\mathbf{h}}^{LM}_k$,\tfor\tthe\tsequence\t$(t_k,\tt_{k+1},\t...,\tt_N)$,\tthe\toutput\tembeddingsof\tthe\t top\tlayer\tLSTM.entedin\tan\tanalogous\tway\tto\ta\tforward\tLM\tand\tproduces\tan\tembedding\t$\\overleftarrow{\\mathbf{h}}^{LM}_k$,\tfor\tthe\tsequence\t$(t_k,\tt_{k+1},\t...,\tt_N)$,\tthe\toutput\tembeddingsof\tthe\t top\tlayer\tLSTM.\nThe\tneed\tto\tcapture\tfuture\tcontext\tin\tthe\tLM\tembeddingssuggests\t itis\tbeneficial\tto\talso\tconsider\ta\t\\textit{backward}\tLM\tin\tadditionalto the\ttraditional\t\\textit{f orward}\tLM.\t\tA\tbackward\tLM\tpredicts\tthe\tprevious\ttoken\tgiven\tthe\tfuture\tcontext.\t\tGiven\ta\tsentence\twith\t$N$\ttokens,\tit\tcomputes\\[P(t_{ k-1}\t|\tt_k,\tt_{k+1},\t...,\tt_N).\\]A\tbackward\t LM\tcan\tbe\timplemTheneed\tto\tcapture\tfuture\tcontext\tin\tthe\tLM\tembeddingssuggests\titis\tbeneficial\tto\talso\tconsider\ta\t\\textit{backward}\tLM\tin\tadditionaltothe\ttraditional\t\\textit{f orward}\t LM.\t\tA\tbackward\tLM\tpredicts\tthe\tprevious\ttoken\tgiven\tthe\tfuture\tcontext.\t\tGiven\ta\tsentence\twith\t$N$\ttokens,\tit\tcomputes\\[P(t_{ k-1}\t|\t t_k,\tt_{k+1},\t...,\tt_N).\\]A\tbackward\tLM\tcan\tbe\timplemented\tin\tan\tanalogous\tway\tto\ta\tforward\tLM\tand\tproduces\tan\tembedding\t$\\overleftarrow{\\mathbf{h}}^{LM}_k$,\tfor\tthe\tsequence\t$(t_k,\t t_{k+1},\t...,\tt_N)$,\tthe\toutput\tembeddingsof\tthe\ttop\tlayer\tLSTM.\tThe\tneed\tto\tcapture\tfuture\tcontext\tin\tthe\tLM\tembeddingssuggests\titis\t beneficial\tto\talso\tconsider\ta\t\\textit{backward}\tLM\tin\tadditionaltothe\t traditional\t\\textit{f orward}\tLM.\t\tA\tbackward\tLM\tpredicts\tthe\tprevious\ttoken\tgiven\tthe\tfuture\tcontext.\t\tGiven\ta\tsentence\twith\t$N$\ttokens,\tit\tcomputes\\[P(t_{ k-1}\t|\tt_k,\tt_{k+1},\t...,\tt_N).\\]A\tbackward\t LM\tcan\tbe\timplemented\tin\tan\tanalogous\tway\tto\ta\tforward\tLM\tand\tproduces\tan\tembedding\t$\\overleftarrow{\\mathbf{h}}^{LM}_k$,\tfor\tthe\tsequence\t$(t_k,\tt_{k+1},\t...,\tt_N)$,\tthe\toutput\tembeddingsof\tthe\t top\tlayer\tLSTM.entedin\tan\tanalogous\tway\tto\ta\tforward\tLM\tand\tproduces\tan\tembedding\t$\\overleftarrow{\\mathbf{h}}^{LM}_k$,\tfor\tthe\tsequence\t$(t_k,\tt_{k+1},\t...,\tt_N)$,\tthe\toutput\tembeddingsof\tthe\t top\tlayer\tLSTM.\nunlabeleddata\nRecurrentlanguage\tmodelWordembedding\tmodel\nStep\t1:\tPretrainword\tembeddingsand\tlanguage\tmodel.\nNew\t\t\t\tYork\t\t\tis\t\t\t\t\tlocated\t\t\t…\nSequence\ttagging\tmodel\nB-LOC\t\t\tE-LOC\t\t\t\t\tO\t\t\t\t\t\t\t\t\tO\t\t\t\t\t\t\t…\ninput\tsequence\noutput\t\tsequence\nWord\tembeddingLM\tembeddingTwo\trepresentationsof\tthe\tword\t“York”\nStep\t3:\tUse\tboth\tword\tembeddingsand\tLM\tembeddingsin\tthe\tsequence\ttagging\tmodel. New\t\t\t\tYork\t\t\tis\t\t\t\t\tlocated\t\t\t…\nFigure 1: The main components in TagLM,\nour language-model-augmented sequence tagging\nsystem. The language model component (in or-\nange) is used to augment the input token represen-\ntation in a traditional sequence tagging models (in\ngrey).\nThe second RNN layer is similar and uses hk,1 to\noutput hk,2. In this paper, we use L = 2 lay-\ners of RNNs in all experiments and parameterize\nRi as either Gated Recurrent Units (GRU) (Cho\net al., 2014) or Long Short-Term Memory units\n(LSTM) (Hochreiter and Schmidhuber, 1997) de-\npending on the task.\nFinally, the output of the ﬁnal RNN layer hk,L\nis used to predict a score for each possible tag us-\ning a single dense layer. Due to the dependencies\nbetween successive tags in our sequence label-\ning tasks (e.g. using the BIOES labeling scheme,\nit is not possible for I-PER to follow B-LOC),\nit is beneﬁcial to model and decode each sen-\ntence jointly instead of independently predicting\nthe label for each token. Accordingly, we add\nanother layer with parameters for each label bi-\ngram, computing the sentence conditional random\nﬁeld (CRF) loss (Lafferty et al., 2001) using the\nforward-backward algorithm at training time, and\nusing the Viterbi algorithm to ﬁnd the most likely\ntag sequence at test time, similar to Collobert et al.\n(2011).\n1757\nNew            York          is        located      ...\nNeural net\nChar \nCNN/ \nRNN\nEmbedding\nToken \nembedding\nRNNDense\nE-LOC\nB-LOC CRF\nbi-RNN \n(R2)\nToken \nrepresentation\nNew     York   is  located   ...\nForward LM\nBackward LM\nh1\nLM\nConcat LM  \nembedding\nSequence \ntagging\nPre-trained bi-LM\nbi-RNN (R1)\nSequence \nrepresentation\nConcatenation\nToken \nrepresentation\nNew     York   is  located   ...\nToken \nrepresentation\nh1,1 h2\nLM\nh2,1\nh1,2 h2,2\nFigure 2: Overview of TagLM, our language model augmented sequence tagging architecture. The\ntop level embeddings from a pre-trained bidirectional LM are inserted in a stacked bidirectional RNN\nsequence tagging model. See text for details.\n2.3 Bidirectional LM\nA language model computes the probability of a\ntoken sequence (t1,t2,...,t N )\np(t1,t2,...,t N ) =\nN∏\nk=1\np(tk |t1,t2,...,t k−1).\nRecent state of the art neural language models\n(J´ozefowicz et al., 2016) use a similar architec-\nture to our baseline sequence tagger where they\npass a token representation (either from a CNN\nover characters or as token embeddings) through\nmultiple layers of LSTMs to embed the history\n(t1,t2,...,t k) into a ﬁxed dimensional vector− →h LM\nk . This is the forward LM embedding of the\ntoken at position k and is the output of the top\nLSTM layer in the language model. Finally, the\nlanguage model predicts the probability of token\ntk+1 using a softmax layer over words in the vo-\ncabulary.\nThe need to capture future context in the LM\nembeddings suggests it is beneﬁcial to also con-\nsider a backward LM in additional to the tradi-\ntional forward LM. A backward LM predicts the\nprevious token given the future context. Given a\nsentence with N tokens, it computes\np(t1,t2,...,t N ) =\nN∏\nk=1\np(tk |tk+1,tk+2,...,t N ).\nA backward LM can be implemented in an anal-\nogous way to a forward LM and produces the\nbackward LM embedding ← −h LM\nk , for the sequence\n(tk,tk+1,...,t N ), the output embeddings of the\ntop layer LSTM.\nIn our ﬁnal system, after pre-training the for-\nward and backward LMs separately, we remove\nthe top layer softmax and concatenate the for-\nward and backward LM embeddings to form\nbidirectional LM embeddings, i.e., hLM\nk =\n[− →h LM\nk ; ← −h LM\nk ]. Note that in our formulation, the\nforward and backward LMs are independent, with-\nout any shared parameters.\n2.4 Combining LM with sequence model\nOur combined system, TagLM, uses the LM em-\nbeddings as additional inputs to the sequence tag-\nging model. In particular, we concatenate the LM\nembeddings hLM with the output from one of the\nbidirectional RNN layers in the sequence model.\nIn our experiments, we found that introducing the\nLM embeddings at the output of the ﬁrst layer per-\nformed the best. More formally, we simply replace\n(2) with\nhk,1 = [− →h k,1; ← −h k,1; hLM\nk ]. (3)\nThere are alternate possibilities for adding the\nLM embeddings to the sequence model. One pos-\n1758\nsibility adds a non-linear mapping after the con-\ncatenation and before the second RNN (e.g. re-\nplacing (3) with f([− →h k,1; ← −h k,1; hLM\nk ]) where f\nis a non-linear function). Another possibility in-\ntroduces an attention-like mechanism that weights\nthe all LM embeddings in a sentence before in-\ncluding them in the sequence model. Our ini-\ntial results with the simple concatenation were en-\ncouraging so we did not explore these alternatives\nin this study, preferring to leave them for future\nwork.\n3 Experiments\nWe evaluate our approach on two well bench-\nmarked sequence tagging tasks, the CoNLL 2003\nNER task (Sang and Meulder, 2003) and the\nCoNLL 2000 Chunking task (Sang and Buch-\nholz, 2000). We report the ofﬁcial evaluation met-\nric (micro-averaged F1). In both cases, we use\nthe BIOES labeling scheme for the output tags,\nfollowing previous work which showed it out-\nperforms other options (e.g., Ratinov and Roth,\n2009). Following Chiu and Nichols (2016), we\nuse the Senna word embeddings (Collobert et al.,\n2011) and pre-processed the text by lowercasing\nall tokens and replacing all digits with 0.\nCoNLL 2003 NER. The CoNLL 2003 NER\ntask consists of newswire from the Reuters RCV1\ncorpus tagged with four different entity types\n(PER, LOC, ORG, MISC). It includes standard\ntrain, development and test sets. Following pre-\nvious work (Yang et al., 2017; Chiu and Nichols,\n2016) we trained on both the train and develop-\nment sets after tuning hyperparameters on the de-\nvelopment set.\nThe hyperparameters for our baseline model are\nsimilar to Yang et al. (2017). We use two bidirec-\ntional GRUs with 80 hidden units and 25 dimen-\nsional character embeddings for the token charac-\nter encoder. The sequence layer uses two bidirec-\ntional GRUs with 300 hidden units each. For reg-\nularization, we add 25% dropout to the input of\neach GRU, but not to the recurrent connections.\nCoNLL 2000 chunking. The CoNLL 2000\nchunking task uses sections 15-18 from the Wall\nStreet Journal corpus for training and section 20\nfor testing. It deﬁnes 11 syntactic chunk types\n(e.g., NP, VP, ADJP) in addition to other. We\nrandomly sampled 1000 sentences from the train-\ning set as a held-out development set.\nThe baseline sequence tagger uses 30 dimen-\nsional character embeddings and a CNN with 30\nﬁlters of width 3 characters followed by a tanh\nnon-linearity for the token character encoder. The\nsequence layer uses two bidirectional LSTMs with\n200 hidden units. Following Ma and Hovy (2016)\nwe added 50% dropout to the character embed-\ndings, the input to each LSTM layer (but not re-\ncurrent connections) and to the output of the ﬁnal\nLSTM layer.\nPre-trained language models. The primary\nbidirectional LMs we used in this study were\ntrained on the 1B Word Benchmark (Chelba et al.,\n2014), a publicly available benchmark for large-\nscale language modeling. The training split has\napproximately 800 million tokens, about a 4000X\nincrease over the number training tokens in the\nCoNLL datasets. J ´ozefowicz et al. (2016) ex-\nplored several model architectures and released\ntheir best single model and training recipes. Fol-\nlowing Sak et al. (2014), they used linear projec-\ntion layers at the output of each LSTM layer to\nreduce the computation time but still maintain a\nlarge LSTM state. Their single best model took\nthree weeks to train on 32 GPUs and achieved 30.0\ntest perplexity. It uses a character CNN with 4096\nﬁlters for input, followed by two stacked LSTMs,\neach with 8192 hidden units and a 1024 dimen-\nsional projection layer. We use CNN-BIG-LSTM\nto refer to this language model in our results.\nIn addition to CNN-BIG-LSTM from\nJ´ozefowicz et al. (2016), 1 we used the same cor-\npus to train two additional language models with\nfewer parameters: forward LSTM-2048-512\nand backward LSTM-2048-512. Both language\nmodels use token embeddings as input to a single\nlayer LSTM with 2048 units and a 512 dimension\nprojection layer. We closely followed the proce-\ndure outlined in J ´ozefowicz et al. (2016), except\nwe used synchronous parameter updates across\nfour GPUs instead of asynchronous updates across\n32 GPUs and ended training after 10 epochs. The\ntest set perplexities for our forward and backward\nLSTM-2048-512 language models are 47.7 and\n47.3, respectively.2\n1https://github.com/tensorflow/models/\ntree/master/lm_1b\n2Due to different implementations, the perplexity of the\nforward LM with similar conﬁgurations in J ´ozefowicz et al.\n(2016) is different (45.0 vs. 47.7).\n1759\nModel F1±std\nChiu and Nichols (2016) 90.91 ±0.20\nLample et al. (2016) 90.94\nMa and Hovy (2016) 91.37\nOur baseline without LM 90.87 ±0.13\nTagLM 91.93 ±0.19\nTable 1: Test set F1 comparison on CoNLL 2003\nNER task, using only CoNLL 2003 data and unla-\nbeled text.\nModel F1±std\nYang et al. (2017) 94.66\nHashimoto et al. (2016) 95.02\nSøgaard and Goldberg (2016) 95.28\nOur baseline without LM 95.00 ±0.08\nTagLM 96.37 ±0.05\nTable 2: Test set F1 comparison on CoNLL 2000\nChunking task using only CoNLL 2000 data and\nunlabeled text.\nTraining. All experiments use the Adam opti-\nmizer (Kingma and Ba, 2015) with gradient norms\nclipped at 5.0. In all experiments, we ﬁne tune\nthe pre-trained Senna word embeddings but ﬁx all\nweights in the pre-trained language models. In ad-\ndition to explicit dropout regularization, we also\nuse early stopping to prevent over-ﬁtting and use\nthe following process to determine when to stop\ntraining. We ﬁrst train with a constant learning\nrate α = 0 .001 on the training data and monitor\nthe development set performance at each epoch.\nThen, at the epoch with the highest development\nperformance, we start a simple learning rate an-\nnealing schedule: decrease α an order of magni-\ntude (i.e., divide by ten), train for ﬁve epochs, de-\ncrease αan order of magnitude again, train for ﬁve\nmore epochs and stop.\nFollowing Chiu and Nichols (2016), we train\neach ﬁnal model conﬁguration ten times with dif-\nferent random seeds and report the mean and stan-\ndard deviation F1. It is important to estimate the\nvariance of model performance since the test data\nsets are relatively small.\n3.1 Overall system results\nTables 1 and 2 compare results from TagLM\nwith previously published state of the art results\nwithout additional labeled data or task speciﬁc\ngazetteers. Tables 3 and 4 compare results of\nTagLM to other systems that include additional la-\nbeled data or gazetteers. In both tasks, TagLM es-\ntablishes a new state of the art using bidirectional\nLMs (the forward CNN-BIG-LSTM and the back-\nward LSTM-2048-512).\nIn the CoNLL 2003 NER task, our model scores\n91.93 mean F1, which is a statistically signiﬁ-\ncant increase over the previous best result of 91.62\n±0.33 from Chiu and Nichols (2016) that used\ngazetteers (at 95%, two-sided Welch t-test, p =\n0.021).\nIn the CoNLL 2000 Chunking task, TagLM\nachieves 96.37 mean F1, exceeding all previously\npublished results without additional labeled data\nby more then 1% absolute F1. The improvement\nover the previous best result of 95.77 in Hashimoto\net al. (2016) that jointly trains with Penn Treebank\n(PTB) POS tags is statistically signiﬁcant at 95%\n(p< 0.001 assuming standard deviation of 0.1).\nImportantly, the LM embeddings amounts to an\naverage absolute improvement of 1.06 and 1.37F1\nin the NER and Chunking tasks, respectively.\nAdding external resources. Although we do\nnot use external labeled data or gazetteers, we\nfound that TagLM outperforms previous state of\nthe art results in both tasks when external re-\nsources (labeled data or task speciﬁc gazetteers)\nare available. Furthermore, Tables 3 and 4 show\nthat, in most cases, the improvements we obtain\nby adding LM embeddings are larger then the im-\nprovements previously obtained by adding other\nforms of transfer or joint learning. For example,\nYang et al. (2017) noted an improvement of only\n0.06 F1 in the NER task when transfer learning\nfrom both CoNLL 2000 chunks and PTB POS tags\nand Chiu and Nichols (2016) reported an increase\nof 0.71 F1 when adding gazetteers to their base-\nline. In the Chunking task, previous work has re-\nported from 0.28 to 0.75 improvement in F1 when\nincluding supervised labels from the PTB POS\ntags or CoNLL 2003 entities (Yang et al., 2017;\nSøgaard and Goldberg, 2016; Hashimoto et al.,\n2016).\n3.2 Analysis\nTo elucidate the characteristics of our LM aug-\nmented sequence tagger, we ran a number of addi-\ntional experiments on the CoNLL 2003 NER task.\nHow to use LM embeddings? In this experi-\nment, we concatenate the LM embeddings at dif-\n1760\nF1 F1\nModel External resources Without With ∆\nYang et al. (2017) transfer from CoNLL 2000/PTB-POS 91.2 91.26 +0.06\nChiu and Nichols (2016) with gazetteers 90.91 91.62 +0.71\nCollobert et al. (2011) with gazetteers 88.67 89.59 +0.92\nLuo et al. (2015) joint with entity linking 89.9 91.2 +1.3\nOurs no LM vs TagLM unlabeled data only 90.87 91.93 +1.06\nTable 3: Improvements in test set F1 in CoNLL 2003 NER when including additional labeled data or\ntask speciﬁc gazetteers (except the case of TagLM where we do not use additional labeled resources).\nF1 F1\nModel External resources Without With ∆\nYang et al. (2017) transfer from CoNLL 2003/PTB-POS 94.66 95.41 +0.75\nHashimoto et al. (2016) jointly trained with PTB-POS 95.02 95.77 +0.75\nSøgaard and Goldberg (2016) jointly trained with PTB-POS 95.28 95.56 +0.28\nOurs no LM vs TagLM unlabeled data only 95.00 96.37 +1.37\nTable 4: Improvements in test set F1 in CoNLL 2000 Chunking when including additional labeled data\n(except the case of TagLM where we do not use additional labeled data).\nUse LM embeddings at F1±std\ninput to the ﬁrst RNN layer 91.55 ±0.21\noutput of the ﬁrst RNN layer 91.93 ±0.19\noutput of the second RNN layer 91.72 ±0.13\nTable 5: Comparison of CoNLL-2003 test set F1\nwhen the LM embeddings are included at different\nlayers in the baseline tagger.\nferent locations in the baseline sequence tagger. In\nparticular, we used the LM embeddings hLM\nk to:\n•augment the input of the ﬁrst RNN layer; i.e.,\nxk = [ck; wk; hLM\nk ],\n•augment the output of the ﬁrst RNN layer;\ni.e., hk,1 = [− →h k,1; ← −h k,1; hLM\nk ],3 and\n•augment the output of the second RNN layer;\ni.e., hk,2 = [− →h k,2; ← −h k,2; hLM\nk ].\nTable 5 shows that the second alternative per-\nforms best. We speculate that the second RNN\nlayer in the sequence tagging model is able to cap-\nture interactions between task speciﬁc context as\nexpressed in the ﬁrst RNN layer and general con-\ntext as expressed in the LM embeddings in a way\nthat improves overall system performance. These\n3This conﬁguration the same as Eq. 3 in §2.4. It was re-\nproduced here for convenience.\nresults are consistent with Søgaard and Goldberg\n(2016) who found that chunking performance was\nsensitive to the level at which additional POS su-\npervision was added.\nDoes it matter which language model to use?\nIn this experiment, we compare six different con-\nﬁgurations of the forward and backward language\nmodels (including the baseline model which does\nnot use any language models). The results are re-\nported in Table 6.\nWe ﬁnd that adding backward LM embeddings\nconsistently outperforms forward-only LM em-\nbeddings, with F1 improvements between 0.22\nand 0.27%, even with the relatively small back-\nward LSTM-2048-512 LM.\nLM size is important, and replacing the forward\nLSTM-2048-512 with CNN-BIG-LSTM (test\nperplexities of 47.7 to 30.0 on 1B Word Bench-\nmark) improves F1 by 0.26 - 0.31%, about as\nmuch as adding backward LM. Accordingly, we\nhypothesize (but have not tested) that replacing\nthe backward LSTM-2048-512 with a backward\nLM analogous to theCNN-BIG-LSTM would fur-\nther improve performance.\nTo highlight the importance of including lan-\nguage models trained on a large scale data, we\nalso experimented with training a language model\non just the CoNLL 2003 training and development\ndata. Due to the much smaller size of this data\n1761\nForward language model Backward language model LM perplexity F1±std\nFwd Bwd\n— — N/A N/A 90.87 ±0.13\nLSTM-512-256∗ LSTM-512-256∗ 106.9 104.2 90.79 ±0.15\nLSTM-2048-512 — 47.7 N/A 91.40 ±0.18\nLSTM-2048-512 LSTM-2048-512 47.7 47.3 91.62 ±0.23\nCNN-BIG-LSTM — 30.0 N/A 91.66 ±0.13\nCNN-BIG-LSTM LSTM-2048-512 30.0 47.3 91.93 ±0.19\nTable 6: Comparison of CoNLL-2003 test set F1 for different language model combinations. All lan-\nguage models were trained and evaluated on the 1B Word Benchmark, exceptLSTM-512-256∗which\nwas trained and evaluated on the standard splits of the NER CoNLL 2003 dataset.\nset, we decreased the model size to 512 hidden\nunits with a 256 dimension projection and normal-\nized tokens in the same manner as input to the se-\nquence tagging model (lower-cased, with all dig-\nits replaced with 0). The test set perplexities for\nthe forward and backward models (measured on\nthe CoNLL 2003 test data) were 106.9 and 104.2,\nrespectively. Including embeddings from these\nlanguage models decreased performance slightly\ncompared to the baseline system without any LM.\nThis result supports the hypothesis that adding lan-\nguage models help because they learn composi-\ntion functions (i.e., the RNN parameters in the lan-\nguage model) from much larger data compared to\nthe composition functions in the baseline tagger,\nwhich are only learned from labeled data.\nImportance of task speciﬁc RNN. To under-\nstand the importance of including a task speciﬁc\nsequence RNN we ran an experiment that removed\nthe task speciﬁc sequence RNN and used only the\nLM embeddings with a dense layer and CRF to\npredict output tags. In this setup, performance was\nvery low, 88.17 F1, well below our baseline. This\nresult conﬁrms that the RNNs in the baseline tag-\nger encode essential information which is not en-\ncoded in the LM embeddings. This is unsurprising\nsince the RNNs in the baseline tagger are trained\non labeled examples, unlike the RNN in the lan-\nguage model which is only trained on unlabeled\nexamples. Note that the LM weights are ﬁxed in\nthis experiment.\nDataset size. A priori , we expect the addition\nof LM embeddings to be most beneﬁcial in cases\nwhere the task speciﬁc annotated datasets are\nsmall. To test this hypothesis, we replicated the\nsetup from Yang et al. (2017) that samples 1%\nof the CoNLL 2003 training set and compared\nthe performance of TagLM to our baseline with-\nout LM. In this scenario, test F1 increased 3.35%\n(from 67.66 to 71.01%) compared to an increase\nof 1.06% F1 for a similar comparison with the full\ntraining dataset. The analogous increases in Yang\net al. (2017) are 3.97% for cross-lingual trans-\nfer from CoNLL 2002 Spanish NER and 6.28%\nF1 for transfer from PTB POS tags. However,\nthey found only a 0.06% F1 increase when using\nthe full training data and transferring from both\nCoNLL 2000 chunks and PTB POS tags. Taken\ntogether, this suggests that for very small labeled\ntraining sets, transferring from other tasks yields\na large improvement, but this improvement almost\ndisappears when the training data is large. On the\nother hand, our approach is less dependent on the\ntraining set size and signiﬁcantly improves perfor-\nmance even with larger training sets.\nNumber of parameters. Our TagLM formula-\ntion increases the number of parameters in the sec-\nond RNN layer R2 due to the increase in the input\ndimension h1 if all other hyperparameters are held\nconstant. To conﬁrm that this did not have a ma-\nterial impact on the results, we ran two additional\nexperiments. In the ﬁrst, we trained a system with-\nout a LM but increased the second RNN layer hid-\nden dimension so that number of parameters was\nthe same as in TagLM. In this case, performance\ndecreased slightly (by 0.15% F1) compared to the\nbaseline model, indicating that solely increasing\nparameters does not improve performance. In the\nsecond experiment, we decreased the hidden di-\nmension of the second RNN layer in TagLM to\ngive it the same number of parameters as the base-\nline no LM model. In this case, test F1 increased\nslightly to 92.00 ±0.11 indicating that the addi-\ntional parameters in TagLM are slightly hurting\n1762\nperformance.4\nDoes the LM transfer across domains? One\nartifact of our evaluation framework is that both\nthe labeled data in the chunking and NER tasks\nand the unlabeled text in the 1 Billion Word\nBenchmark used to train the bidirectional LMs are\nderived from news articles. To test the sensitiv-\nity to the LM training domain, we also applied\nTagLM with a LM trained on news articles to the\nSemEval 2017 Shared Task 10, ScienceIE.5 Scien-\nceIE requires end-to-end joint entity and relation-\nship extraction from scientiﬁc publications across\nthree diverse ﬁelds (computer science, material\nsciences, and physics) and deﬁnes three broad en-\ntity types (Task, Material and Process). For this\ntask, TagLM increased F1 on the development set\nby 4.12% (from 49.93 to to 54.05%) for entity ex-\ntraction over our baseline without LM embeddings\nand it was a major component in our winning sub-\nmission to ScienceIE, Scenario 1 (Ammar et al.,\n2017). We conclude that LM embeddings can im-\nprove the performance of a sequence tagger even\nwhen the data comes from a different domain.\n4 Related work\nUnlabeled data. TagLM was inspired by the\nwidespread use of pre-trained word embeddings\nin supervised sequence tagging models. Besides\npre-trained word embeddings, our method is most\nclosely related to Li and McCallum (2005). In-\nstead of using a LM, Li and McCallum (2005) uses\na probabilistic generative model to infer context-\nsensitive latent variables for each token, which\nare then used as extra features in a supervised\nCRF tagger (Lafferty et al., 2001). Other semi-\nsupervised learning methods for structured pre-\ndiction problems include co-training (Blum and\nMitchell, 1998; Pierce and Cardie, 2001), expec-\ntation maximization (Nigam et al., 2000; Mohit\nand Hwa, 2005), structural learning (Ando and\nZhang, 2005) and maximum discriminant func-\ntions (Suzuki et al., 2007; Suzuki and Isozaki,\n2008). It is easy to combine TagLM with any\nof the above methods by including LM embed-\ndings as additional features in the discriminative\ncomponents of the model (except for expectation\nmaximization). A detailed discussion of semi-\nsupervised learning methods in NLP can be found\n4A similar experiment for the Chunking task did not im-\nprove F1 so this conclusion is task dependent.\n5https://scienceie.github.io/\nin (Søgaard, 2013).\nMelamud et al. (2016) learned a context en-\ncoder from unlabeled data with an objective func-\ntion similar to a bi-directional LM and applied it to\nseveral NLP tasks closely related to the unlabeled\nobjective function: sentence completion, lexical\nsubstitution and word sense disambiguation.\nLM embeddings are related to a class of meth-\nods (e.g., Le and Mikolov, 2014; Kiros et al.,\n2015; Hill et al., 2016) for learning sentence and\ndocument encoders from unlabeled data, which\ncan be used for text classiﬁcation and textual en-\ntailment among other tasks. Dai and Le (2015)\npre-trained LSTMs using language models and se-\nquence autoencoders then ﬁne tuned the weights\nfor classiﬁcation tasks. In contrast to our method\nthat uses unlabeled data to learn token-in-context\nembeddings, all of these methods use unlabeled\ndata to learn an encoder for an entire text sequence\n(sentence or document).\nNeural language models. LMs have always\nbeen a critical component in statistical machine\ntranslation systems (Koehn, 2009). Recently, neu-\nral LMs (Bengio et al., 2003; Mikolov et al., 2010)\nhave also been integrated in neural machine trans-\nlation systems (e.g., Kalchbrenner and Blunsom,\n2013; Devlin et al., 2014) to score candidate trans-\nlations. In contrast, TagLM uses neural LMs to\nencode words in the input sequence.\nUnlike forward LMs, bidirectional LMs have\nreceived little prior attention. Most similar to\nour formulation, Peris and Casacuberta (2015)\nused a bidirectional neural LM in a statistical ma-\nchine translation system for instance selection.\nThey tied the input token embeddings and soft-\nmax weights in the forward and backward direc-\ntions, unlike our approach which uses two distinct\nmodels without any shared parameters. Frinken\net al. (2012) also used a bidirectional n-gram LM\nfor handwriting recognition.\nInterpreting RNN states. Recently, there has\nbeen some interest in interpreting the activations\nof RNNs. Linzen et al. (2016) showed that sin-\ngle LSTM units can learn to predict singular-plural\ndistinctions. Karpathy et al. (2015) visualized\ncharacter level LSTM states and showed that indi-\nvidual cells capture long-range dependencies such\nas line lengths, quotes and brackets. Our work\ncomplements these studies by showing that LM\nstates are useful for downstream tasks as a way\n1763\nof interpreting what they learn.\nOther sequence tagging models. Current state\nof the art results in sequence tagging problems are\nbased on bidirectional RNN models. However,\nmany other sequence tagging models have been\nproposed in the literature for this class of problems\n(e.g., Lafferty et al., 2001; Collins, 2002). LM em-\nbeddings could also be used as additional features\nin other models, although it is not clear whether\nthe model complexity would be sufﬁcient to effec-\ntively make use of them.\n5 Conclusion\nIn this paper, we proposed a simple and general\nsemi-supervised method using pre-trained neural\nlanguage models to augment token representations\nin sequence tagging models. Our method signiﬁ-\ncantly outperforms current state of the art models\nin two popular datasets for NER and Chunking.\nOur analysis shows that adding a backward LM in\naddition to traditional forward LMs consistently\nimproves performance. The proposed method is\nrobust even when the LM is trained on unlabeled\ndata from a different domain, or when the base-\nline model is trained on a large number of labeled\nexamples.\nAcknowledgments\nWe thank Chris Dyer, Julia Hockenmaier, Jayant\nKrishnamurthy, Matt Gardner and Oren Etzioni\nfor comments on earlier drafts that led to substan-\ntial improvements in the ﬁnal version.\nReferences\nWaleed Ammar, Matthew E. Peters, Chandra Bhaga-\nvatula, and Russell Power. 2017. The AI2 sys-\ntem at SemEval-2017 Task 10 (ScienceIE): semi-\nsupervised end-to-end entity and relation extraction.\nIn ACL workshop (SemEval).\nRie Kubota Ando and Tong Zhang. 2005. A high-\nperformance semi-supervised learning method for\ntext chunking. In ACL.\nYoshua Bengio, R´ejean Ducharme, Pascal Vincent, and\nChristian Jauvin. 2003. A neural probabilistic lan-\nguage model. In JMLR.\nAvrim Blum and Tom Mitchell. 1998. Combining la-\nbeled and unlabeled data with co-training. InCOLT.\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, and Phillipp Koehn. 2014. One bil-\nlion word benchmark for measuring progress in sta-\ntistical language modeling. CoRR abs/1312.3005.\nJason Chiu and Eric Nichols. 2016. Named entity\nrecognition with bidirectional LSTM-CNNs. In\nTACL.\nKyunghyun Cho, Bart van Merrienboer, Dzmitry Bah-\ndanau, and Yoshua Bengio. 2014. On the properties\nof neural machine translation: Encoder-decoder ap-\nproaches. In SSST@EMNLP.\nMichael Collins. 2002. Discriminative training meth-\nods for hidden markov models: Theory and experi-\nments with perceptron algorithms. In EMNLP.\nRonan Collobert, Jason Weston, L´eon Bottou, Michael\nKarlen, Koray Kavukcuoglu, and Pavel P. Kuksa.\n2011. Natural language processing (almost) from\nscratch. In JMLR.\nAndrew M. Dai and Quoc V . Le. 2015. Semi-\nsupervised sequence learning. In NIPS.\nJacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas\nLamar, Richard M Schwartz, and John Makhoul.\n2014. Fast and robust neural network joint models\nfor statistical machine translation. In ACL.\nV olkmar Frinken, Alicia Forn ´es, Josep Llad ´os, and\nJean-Marc Ogier. 2012. Bidirectional language\nmodel for handwriting recognition. In Joint IAPR\nInternational Workshops on Statistical Techniques in\nPattern Recognition (SPR) and Structural and Syn-\ntactic Pattern Recognition (SSPR).\nKazuma Hashimoto, Caiming Xiong, Yoshimasa Tsu-\nruoka, and Richard Socher. 2016. A joint many-task\nmodel: Growing a neural network for multiple nlp\ntasks. CoRR abs/1611.01587.\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\nLearning distributed representations of sentences\nfrom unlabelled data. In HLT-NAACL.\nSepp Hochreiter and J¨urgen Schmidhuber. 1997. Long\nshort-term memory. Neural Computation 9.\nRafal J´ozefowicz, Oriol Vinyals, Mike Schuster, Noam\nShazeer, and Yonghui Wu. 2016. Exploring the lim-\nits of language modeling. CoRR abs/1602.02410.\nNal Kalchbrenner and Phil Blunsom. 2013. Recurrent\ncontinuous translation models. In EMNLP.\nAndrej Karpathy, Justin Johnson, and Li Fei-Fei. 2015.\nVisualizing and understanding recurrent networks.\nIn ICLR workshop.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In ICLR.\nJamie Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov,\nRichard S. Zemel, Raquel Urtasun, Antonio Tor-\nralba, and Sanja Fidler. 2015. Skip-thought vectors.\nIn NIPS.\nPhilipp Koehn. 2009. Statistical machine translation.\nCambridge University Press.\n1764\nJohn D. Lafferty, Andrew McCallum, and Fernando\nPereira. 2001. Conditional random ﬁelds: Prob-\nabilistic models for segmenting and labeling se-\nquence data. In ICML.\nGuillaume Lample, Miguel Ballesteros, Sandeep Sub-\nramanian, Kazuya Kawakami, and Chris Dyer. 2016.\nNeural architectures for named entity recognition.\nIn NAACL-HLT.\nQuoc V . Le and Tomas Mikolov. 2014. Distributed rep-\nresentations of sentences and documents. In ICML.\nWei Li and Andrew McCallum. 2005. Semi-supervised\nsequence modeling with syntactic topic models. In\nAAAI.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\n2016. Assessing the ability of LSTMs to learn\nsyntax-sensitive dependencies. In TACL.\nGang Luo, Xiaojiang Huang, Chin-Yew Lin, and Za-\niqing Nie. 2015. Joint entity recognition and disam-\nbiguation. In EMNLP.\nXuezhe Ma and Eduard H. Hovy. 2016. End-to-end\nsequence labeling via bi-directional LSTM-CNNs-\nCRF. In ACL.\nOren Melamud, Jacob Goldberger, and Ido Dagan.\n2016. context2vec: Learning generic context em-\nbedding with bidirectional lstm. In CoNLL.\nTomas Mikolov, Martin Karaﬁ ´at, Lukas Burget, Jan\nCernock`y, and Sanjeev Khudanpur. 2010. Recur-\nrent neural network based language model. In Inter-\nspeech.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In NIPS.\nBehrang Mohit and Rebecca Hwa. 2005. Syntax-based\nsemi-supervised named entity tagging. In ACL.\nKamal Nigam, Andrew Kachites McCallum, Sebastian\nThrun, and Tom Mitchell. 2000. Text classiﬁcation\nfrom labeled and unlabeled documents using em.\nMachine learning .\nJeffrey Pennington, Richard Socher, and Christo-\npher D. Manning. 2014. Glove: Global vectors for\nword representation. In EMNLP.\n´Alvaro Peris and Francisco Casacuberta. 2015. A bidi-\nrectional recurrent neural language model for ma-\nchine translation. Procesamiento del Lenguaje Nat-\nural .\nDavid Pierce and Claire Cardie. 2001. Limitations of\nco-training for natural language learning from large\ndatasets. In EMNLP.\nLev-Arie Ratinov and Dan Roth. 2009. Design chal-\nlenges and misconceptions in named entity recogni-\ntion. In CoNLL.\nHasim Sak, Andrew W. Senior, and Franoise Beaufays.\n2014. Long short-term memory recurrent neural\nnetwork architectures for large scale acoustic mod-\neling. In INTERSPEECH.\nErik F. Tjong Kim Sang and Sabine Buchholz. 2000.\nIntroduction to the CoNLL-2000 shared task chunk-\ning. In CoNLL/LLL.\nErik F. Tjong Kim Sang and Fien De Meulder.\n2003. Introduction to the CoNLL-2003 shared task:\nLanguage-independent named entity recognition. In\nCoNLL.\nAnders Søgaard. 2013. Semi-supervised learning and\ndomain adaptation in natural language processing.\nSynthesis Lectures on Human Language Technolo-\ngies .\nAnders Søgaard and Yoav Goldberg. 2016. Deep\nmulti-task learning with low level tasks supervised\nat lower layers. In ACL.\nJun Suzuki, Akinori Fujino, and Hideki Isozaki. 2007.\nSemi-supervised structured output learning based on\na hybrid generative and discriminative approach. In\nEMNLP-CoNLL.\nJun Suzuki and Hideki Isozaki. 2008. Semi-supervised\nsequential labeling and segmentation using giga-\nword scale unlabeled data. In ACL.\nZhilin Yang, Ruslan Salakhutdinov, and William W.\nCohen. 2017. Transfer learning for sequence tag-\nging with hierarchical recurrent networks. In ICLR.\n1765",
  "topic": "Chunking (psychology)",
  "concepts": [
    {
      "name": "Chunking (psychology)",
      "score": 0.873721718788147
    },
    {
      "name": "Computer science",
      "score": 0.840861976146698
    },
    {
      "name": "Sequence labeling",
      "score": 0.7325695753097534
    },
    {
      "name": "Artificial intelligence",
      "score": 0.730129599571228
    },
    {
      "name": "Natural language processing",
      "score": 0.7046944499015808
    },
    {
      "name": "Word (group theory)",
      "score": 0.5953349471092224
    },
    {
      "name": "Named-entity recognition",
      "score": 0.5741410851478577
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5540961027145386
    },
    {
      "name": "Transfer of learning",
      "score": 0.5502312183380127
    },
    {
      "name": "Task (project management)",
      "score": 0.5464531183242798
    },
    {
      "name": "Language model",
      "score": 0.5458642840385437
    },
    {
      "name": "Sequence (biology)",
      "score": 0.4804612994194031
    },
    {
      "name": "Conditional random field",
      "score": 0.41514936089515686
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    }
  ],
  "institutions": []
}