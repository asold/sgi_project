{
  "title": "HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models",
  "url": "https://openalex.org/W4389518784",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5100363218",
      "name": "Junyi Li",
      "affiliations": [
        "Université de Montréal",
        "Beijing Institute of Big Data Research",
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A5101143771",
      "name": "Xiaoxue Cheng",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A5101532348",
      "name": "Xin Zhao",
      "affiliations": [
        "Beijing Institute of Big Data Research",
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A5018977183",
      "name": "Jian‐Yun Nie",
      "affiliations": [
        "Université de Montréal"
      ]
    },
    {
      "id": "https://openalex.org/A5025631695",
      "name": "Ji-Rong Wen",
      "affiliations": [
        "Beijing Institute of Big Data Research",
        "Renmin University of China"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4231510805",
    "https://openalex.org/W4382202847",
    "https://openalex.org/W4308539010",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W2889787757",
    "https://openalex.org/W4226157795",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4285240908",
    "https://openalex.org/W3099474967",
    "https://openalex.org/W4327810286",
    "https://openalex.org/W2950457956",
    "https://openalex.org/W4366736258",
    "https://openalex.org/W3153046263",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W3187134297",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W4385573893",
    "https://openalex.org/W4312634749",
    "https://openalex.org/W2952523122",
    "https://openalex.org/W4389523832",
    "https://openalex.org/W4224947967",
    "https://openalex.org/W3207604732",
    "https://openalex.org/W4319793302",
    "https://openalex.org/W4385571655",
    "https://openalex.org/W4296711106",
    "https://openalex.org/W4309674289"
  ],
  "abstract": "Large language models (LLMs), such as ChatGPT, are prone to generate hallucinations, i.e., content that conflicts with the source or cannot be verified by the factual knowledge. To understand what types of content and to which extent LLMs are apt to hallucinate, we introduce the Hallucination Evaluation for Large Language Models (HaluEval) benchmark, a large collection of generated and human-annotated hallucinated samples for evaluating the performance of LLMs in recognizing hallucination. To generate these samples, we propose a ChatGPT-based two-step framework, i.e., sampling-then-filtering. Besides, we also hire some human labelers to annotate the hallucinations in ChatGPT responses. The empirical results suggest that ChatGPT is likely to generate hallucinated content in specific topics by fabricating unverifiable information (i.e., about 19.5% user queries). Moreover, existing LLMs face great challenges in recognizing the hallucinations in texts. While, our experiments also prove that the hallucination recognition can be improved by providing external knowledge or adding reasoning steps.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 6449–6464\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nHaluEval: A Large-Scale Hallucination Evaluation Benchmark\nfor Large Language Models\nJunyi Li1,3,4∗, Xiaoxue Cheng1∗, Wayne Xin Zhao1,4†\n, Jian-Yun Nie3 and Ji-Rong Wen1,2,4\n1Gaoling School of Artificial Intelligence, Renmin University of China\n2School of Information, Renmin University of China\n3DIRO, Université de Montréal\n4Beijing Key Laboratory of Big Data Management and Analysis Methods\nlijunyi@ruc.edu.cn chengxiaoxue3@gmail.com batmanfly@gmail.com\nAbstract\nLarge language models (LLMs), such as Chat-\nGPT, are prone to generate hallucinations, i.e.,\ncontent that conflicts with the source or cannot\nbe verified by the factual knowledge. To un-\nderstand what types of content and to which ex-\ntent LLMs are apt to hallucinate, we introduce\nthe Hallucination Evaluation benchmark for\nLarge Language Models (HaluEval), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the perfor-\nmance of LLMs in recognizing hallucination.\nTo generate these samples automatically, we\npropose a two-stage framework, i.e., sampling-\nthen-filtering. Besides, we hire some human\nlabelers to annotate the hallucinations in Chat-\nGPT responses. The empirical results suggest\nthat ChatGPT is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information (i.e., about 19.5% re-\nsponses). Moreover, existing LLMs face great\nchallenges in recognizing the hallucinations in\ntexts. However, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help LLMs recognize hal-\nlucinations. Our benchmark can be accessed at\nhttps://github.com/RUCAIBox/HaluEval.\n1 Introduction\nThe advent of large language models (LLMs) (Zhao\net al., 2023) has ushered in a paradigm shift in nat-\nural language processing (NLP), making unprece-\ndented progress in text generation and understand-\ning (Brown et al., 2020; Li et al., 2021). The re-\nmarkable language ability makes LLMs core in a\nnumber of products with millions of users, such as\nthe coding assistant Copilot and recent ChatGPT.\nDespite these prominent capabilities of LLMs\ntrained on large text corpus, recent work has shown\nthat LLMs are prone to suffer from hallucination\ngenerations across various applications (Ji et al.,\n∗Equal contributions\n†Corresponding author\nUser Query Retrieve the oldest photo of a cat\nChatGPT There is no definitive answer to this question\nas “the oldest photo of a cat” is subjective and varies based\non different sources and definitions. However, one can-\ndidate for the oldest cat photo is the daguerreotype\n“Cat with Blue Ribbon,” taken by photographer Antoine\nClaudet in 1840. The photo depicts a cat sitting on a table,\ngazing directly at the camera, and wearing a blue ribbon\ntied around its neck. [...]\nTable 1: An example from Alpaca (Taori et al., 2023)\nshowing that ChatGPT might generate hallucinated con-\ntents (green) that cannot be verified by existing source.\n2023; Bang et al., 2023; Sun et al., 2023), where\nthe generated content is either in conflict with ex-\nisting source or cannot be verified by the available\nknowledge resources. The issue of hallucination\nmakes the deployment of LLMs potentially risky in\nreal-world applications. Most exiting work mainly\nfocuses on investigating the causes of hallucination\nfor specific tasks and small language models (Cao\net al., 2022; Zheng et al., 2023; Das et al., 2023).\nHowever, it still remains unclear what types of con-\ntent and to which extent LLMs tend to hallucinate.\nTo facilitate research in this direction, we present\nthe Hallucination Evaluation benchmark for Large\nLanguage Models (HaluEval): a large collection\nof 35,000 hallucinated/normal samples for LLMs\nanalysis and evaluation. HaluEval includes 5,000\ngeneral user queries with ChatGPT responses and\n30,000 task-specific examples from three tasks, i.e.,\nquestion answering, knowledge-grounded dialogue,\nand text summarization. The construction pipeline\nof HaluEval is depicted in Figure 1. For general\nuser queries, we adopt the 52K instruction tuning\ndataset from Alpaca (Taori et al., 2023) for hu-\nman annotation. To further screen out user queries\nwhere LLMs are most likely to produce hallucina-\ntions, we use ChatGPT to sample three responses\nfor each query and only retain 5,000 queries with\nthe lowest similarity among three responses. Ac-\n6449\nQuery\nResponse\nHallucination: Yes or No\nCandidate\nAnswer #1\nMethod 1: …\nHave you mastered this method?\nOne-pass Instruction\nConversational Instruction\nI want you act as a hallucination ans wer \ngenerator......\n#Hallucinated Answer#:\nYes, I have mastered the first method.\n…\nPlease generate hallucinated answers…\nI want you act as an answer \njudge…\n#Answer 1#:\n#Answer 2#:\n#Your Choice#:\nThe best answer is Answer 1.\nHigh-quality Hallucination Filtering\nFinal\nAnswer \nmax-voting\nDiverse \nHallucination Sampling\n(candidate #1)Candidate\nAnswer #2\nHuman Annotation\nFigure 1: Construction pipeline of HaluEval, including automatic generation (top) and human annotation (bottom).\ncording to recent work (Manakul et al., 2023), hal-\nlucinations are likely to appear in diverged and con-\nflicting responses of LLMs. Based on the filtered\nuser queries and ChatGPT responses, we invite\nhuman labelers to annotate whether the response\ncontains hallucinated information and mark cor-\nresponding spans. As shown in Table 1, for the\nuser query “Retrieve the oldest photo of a cat”, the\nresponse generated by ChatGPT contains unverifi-\nable information. These human-annotated queries\nand responses can be used to analyze what types\nof content LLMs tend to hallucinate and further\nconceive effective methods to alleviate it.\nFurthermore, for the task-specific examples, we\ndesign an automatic two-stage approach to generate\nhallucinated samples. First, based on existing task\ndatasets (e.g., HotpotQA) as seed data, we employ\nChatGPT to generate hallucinated samples with\ntwo styles of task-specific instructions, i.e., one-\npass and conversational. We expect that these two\nmethods will generate diverse hallucinated sam-\nples from different aspects. Second, to select the\nmost plausible and difficult hallucinated sample\nfor LLMs evaluation, we elaborate the filtering in-\nstruction enhanced by ground-truth examples and\nleverage ChatGPT for sample selection. Through\nthe proposed sampling-then-filtering approach, we\ncan generate a hallucinated counterpart for each\nspecific task example. These hallucinated samples\nare designed to challenge the ability of LLMs in\nhallucination recognition and analyze the informa-\ntion blind spots of LLMs.\nTo better understand the performance of LLMs\nin HaluEval, we conduct experiments with several\nexisting powerful LLMs (e.g., ChatGPT, GPT-3).\nOur key findings can be summarized as follows:\n• First, ChatGPT is likely to generate halluci-\nnated content by fabricating unverifiable informa-\ntion in its responses (i.e., about 19.5% responses).\nThe hallucinated texts from ChatGPT cover topics\nincluding language, climate, and technology.\n• Second, existing LLMs face significant chal-\nlenges to identify the hallucinations in the gener-\nated text, even for ChatGPT which is used to gener-\nate these hallucinated samples (e.g., only 62.59%\naccuracy for ChatGPT in question answering).\n• Finally, the deficient performance of LLMs in\nrecognizing hallucinations can be improved by pro-\nviding explicit knowledge and adding intermediate\nreasoning steps. While, contrasting hallucinated\nsamples with ground-truth makes LLMs more con-\nfused and leads to worse performance.\n2 The HaluEval Benchmark\nAs the goal of HaluEval is to understandwhat types\nof content and to which extent LLMs tend to hallu-\ncinate, the benchmark contains a myriad of correct\nsamples and their hallucinated counterparts. This\ncollection is created via two ways, i.e., automatic\ngeneration and human annotation.\n2.1 Automatic Generation\nOur generation pipeline includes two steps: 1) di-\nverse hallucination sampling, and 2) high-quality\nhallucination filtering. We employ ChatGPT to\nexecute the creation pipeline automatically.\nDiverse Hallucination Sampling. Since a factual\ntext can be hallucinated from different aspects, we\npropose two different hallucination sampling meth-\n6450\nI want you act as a hallucination answer generator. Given a question, right answer, and related knowledge, your\nobjective is to write a hallucinated answer that sounds plausible but is factually incorrect. You SHOULD write\nthe hallucinated answer using the following method (each with some examples):\nYou are trying to answer a question but there is a factual contradiction between the answer and the knowledge.\nYou can fabricate some information that does not exist in the provided knowledge.\n#Knowledge#: The nine mile byway starts south of Morehead, Kentucky and can be accessed by U.S. Highway\n60. Morehead is a home rule-class city located along US 60 (the historic Midland Trail) and Interstate 64 in\nRowan County, Kentucky, in the United States.\n#Question#: What U.S Highway gives access to Zilpo Road, and is also known as Midland Trail?\n#Right Answer#:U.S. Highway 60\n#Hallucinated Answer#:U.S. Highway 70\nYou are trying to answer a question but you misunderstand the question context and intention.\n<Demonstrations>\nYou are trying to answer a question but the answer is too general or too specific to answer the question at an\nappropriate level of specificity.\n<Demonstrations>\nYou are trying to answer a question but the answer cannot be inferred from the knowledge. You can incorrectly\nreason with the knowledge to arrive at a hallucinated answer.\n<Demonstrations>\nYou should try your best to make the answer become hallucinated. #Hallucinated Answer# can only have about\n5 more words than #Right Answer#.\n#Knowledge#: <insert the related knowledge>\n#Question#: <insert the question>\n#Right Answer#:<insert the right answer to the question>\n#Hallucinated Answer#:\nTable 2: Instruction of hallucination sampling for question answering. The blue text denotes the intention descrip-\ntion, the red text denotes the hallucination pattern, and the green text denotes the hallucination demonstration.\nods to generate diverse samples. For each method,\nChatGPT follows the instruction of hallucination\nsampling in different manners. As shown in Fig-\nure 1, the first method adopts a one-pass instruc-\ntion following schema, where we directly feed the\ncomplete instruction (Table 2) into ChatGPT and\ngenerate a hallucinated answer. On the other hand,\nthe second method uses a conversational schema,\nwhere we teach ChatGPT to successively learn part\nof the instruction and make sure it has mastered.\nBased on the learned instructions, ChatGPT will\ngenerate another hallucinated answer. Through the\ntwo different sampling strategies, we can obtain\ndiverse and multi-facet hallucinated answers for\neach question, which will be further filtered and\nselected for the most plausible and difficult one.\nInstruction Design. In our approach, the key is\nto design an effective instruction for ChatGPT to\ngenerate hallucinated samples. In our design, the\nhallucination sampling instruction consists of three\nimportant parts, including intention description ,\nhallucination pattern, and hallucination demon-\nstration, which have been shown in Table 2. The\nintention description is to characterize the role of\nthe system and define the input and objective of\nour generation. To control the type and quality of\nhallucinated samples, we introduce the hallucina-\ntion pattern and demonstration, which are related\nto the seed task (e.g., QA in Table 2). The few-shot\ndemonstrations can help the system to understand\nthe hallucination pattern. In this paper, we automat-\nically generate hallucinated samples for three tasks,\ni.e., question answering, knowledge-grounded di-\nalogue, and text summarization. Specifically, we\nconsider four types of hallucination patterns for\nquestion answering (i.e., comprehension, factual-\nness, specificity, and inference) (Zheng et al., 2023),\nthree types of hallucination patterns for knowledge-\ngrounded dialogue ( i.e., extrinsic-soft, extrinsic-\nhard, and extrinsic-grouped) (Das et al., 2023),\nand three types of hallucination patterns for text\nsummarization (i.e., factual, non-factual, and in-\ntrinsic) (Cao et al., 2022). For these three tasks,\nwe first randomly sample 30,000 instances from\nthe training set of HotpotQA (Yang et al., 2018),\nOpenDialKG (Moon et al., 2019), and CNN/Daily\nMail (See et al., 2017), and then generate their hal-\nlucinated examples. The hallucination sampling\ninstructions for dialogue and summarization can be\nfound in Table 9-10 in the Appendix A.\nHigh-quality Hallucination Filtering. To con-\nstruct a challenging benchmark for LLMs, we aim\n6451\nI want you act as an answer judge. Given a question, two answers, and related knowledge, your objective is to\nselect the best and correct answer without hallucination and non-factual information. Here are some examples:\n#Knowledge#:The nine mile byway starts south of Morehead, Kentucky and can be accessed by U.S. Highway\n60. Morehead is a home rule-class city located along US 60 (the historic Midland Trail) and Interstate 64 in\nRowan County, Kentucky, in the United States.\n#Question#: What U.S Highway gives access to Zilpo Road, and is also known as Midland Trail?\n#Answer 1#:U.S. Highway 60 (right answer)\n#Answer 2#:U.S. Highway 70 (hallucinated answer)\n#Your Choice#:The best answer is Answer 1.\n...\n<Demonstrations>\nYou should try your best to select the best and correct answer. If the two answers are the same, you can randomly\nchoose one. If both answers are incorrect, choose the better one. You MUST select an answer from the provided\ntwo answers.\n#Knowledge#: <insert the related knowledge>\n#Question#: <insert the question>\n#Answer 1#:<insert the hallucinated answer generated by the one-pass schema>\n#Answer 2#:<insert the hallucinated answer generated by the conversational schema>\n#Your Choice#:\nTable 3: Instruction of hallucination filtering for question answering.\nto select the most plausible and difficult halluci-\nnated samples from the above two sampling meth-\nods. As shown in Table 3, we design the instruction\nof hallucination filtering enhanced by ground-truth\nanswers to select the best answer from two hallu-\ncinated candidates. In the instruction of filtering,\nthe demonstration includes the ground-truth correct\nanswer (e.g., U.S. Highway 60) and a hallucinated\ncounterpart (e.g., U.S. Highway 70). While, in the\ntest example, we input two hallucinated answers.\nFollowing the demonstrations, we expect ChatGPT\nto select one of the hallucinated answers that is the\nmost plausible and closest to the right answer. Fi-\nnally, the selected hallucinated sample is hard to be\nidentified, which are further used to evaluate LLMs\nin hallucination recognition. The instructions of\nhallucination filtering for dialogue and summariza-\ntion are shown in Table 11-12 in the Appendix B.\nThrough the sampling-then-filtering process, we\nend up generating a total of 30,000 hallucinated\nsamples for the three tasks. Our approach can also\nbe adapted to other tasks and datasets.\n2.2 Human Annotation\nBesides generating hallucinated samples, we also\ninvite human labelers to annotate whether ChatGPT\nresponses contain hallucinated content.\nWe annotate the general user queries and Chat-\nGPT responses from the 52K instruction tuning\ndataset from Alpaca (Taori et al., 2023), which\nhas been widely used by recent LLMs. To screen\nout user queries where LLMs are most likely to\nproduce hallucination for labeling, we design a pre-\nQuestion In what political party was the man who of-\nficially opened Royal Spa Centre in 1972?\nRight Answer Conservative\nHallucinated\nAnswer Labour Party\nUser Query Retrieve the oldest photo of a cat\nChatGPT There is no definitive answer to this ques-\ntion as “the oldest photo of a cat” is subjec-\ntive and varies based on different sources\nand definitions. However, one candidate\nfor the oldest cat photo is the daguerreo-\ntype “Cat with Blue Ribbon,” taken by pho-\ntographer Antoine Claudet in 1840. The\nphoto depicts a cat sitting on a table, gaz-\ning directly at the camera, and wearing a\nblue ribbon tied around its neck. [...]\nHallucination Yes\nFragments the oldest cat photo is the daguerreotype\n“Cat with Blue Ribbon” taken by photogra-\npher Antoine Claudet in 1840.\nTable 4: A generated hallucinated QA example and a\nhuman-labeled ChatGPT response for a user query.\nselection procedure. Specifically, we use ChatGPT\nto sample three responses for each user query and\ncompute their average semantic similarity using\nBERTScore (Zhang et al., 2020). We finally re-\ntain 5,000 user queries with the lowest similarities.\nAccording to recent work (Manakul et al., 2023),\nhallucinations are likely to appear in diverged and\nconflicting responses of LLMs. For each query and\nChatGPT response, human labelers will annotate\nwhether the response contains hallucinated infor-\nmation (“Yes” or “No”) and list the corresponding\n6452\n   \nPC1\nPC2\n2\n3\n44\n55\n66\n77\n88\n99\n10\n10\n \n \ndirector\nmusic\nband\nfootball\nbasketball\nmagazine\ncompany\nfamily\ncity school\nuniversity\nsong\nactor\nrock\nseasonevent\nnovelauthor\ngame team \nbook\nfilm\nproducer\nrole\n(a) QA Topics\n   \nPC1\nPC2\n5\n66\n77\n8\n99\n10\n10\nfootball\nsport\nteam\nbook\nbook\nmovie\n4\nauthor\nsongmusic\nromance\nfilm\nmovie 5\n1\ngenre\nstar actor\nread\nscience\npop\nmystery\nworld\nplayer\nfiction (b) Dialogue Topics\n   \nPC1\nPC2\n3\n44\n55\n66\n77\n8\n99\n10\n10\npolice\ncar\nfamily\nanimal\ndog\ncity\nclub\n2\ngame\nschool\nfood official\nofficer\nseason\nplayer\nchild\nmother\ngovernment leader\nparty\nfansterling\nstudent\ncampaign (c) Summarization Topics\nFigure 2: Topic distributions for QA, knowledge-grounded dialogue, and text summarization. The samples of each\ntask are classified into 10 topics, and the red circles denote the topics of failed recognized samples by ChatGPT.\nPC1\n11\n2\n5\n6\n77\n8\n9\n1010\nnature\ntree\npoem\nhealth\nwork\nlanguage\nmodel text\n4\neducation\ncomputermachine\ncustomer\nproductservice\nfood\nfruit\nenergy technology\nPC2\nclimate\nfunction\ncode\narea\nside\nstory\nwater\nFigure 3: Topic distribution for ChatGPT responses.\nspans. The hallucination is considered from the fol-\nlowing three aspects: unverifiable, non-factual, and\nirrelevant. Each response is labeled by three hu-\nman labelers, and we adopt the max-voting strategy\nto determine the final hallucination label.\nLabeler Details. Annotating the hallucination\nin ChatGPT responses is a very challenging task,\nwhich requires good reading comprehension skills\nand using search engine to look up relevant infor-\nmation for judgement. Thus, from an initial pool\nof labeler candidates, we select labelers who are\ngood at English passage reading with at least an\nundergraduate-level education. Besides, following\n(Ouyang et al., 2022), we have labelers annotate a\nsmall number of test examples and measure their\nagreement with the labels of researchers, and finally\nwe choose thirty human labelers with the highest\nagreement scores. We report Fleiss’s Kappa (κ) to\nindicate the reliability of agreement between hu-\nman labelers. We compute κon 5,000 annotated\nsamples and obtain κ= 0.811 (0.80 ≤ κ≤ 1.00)\nshowing a perfect agreement.\n2.3 Benchmark Analysis and Usage\nWith the automatic two-step generation process in\nSection 2.1, we produce a total of 30,000 halluci-\nnated samples with 10,000 examples for each task\nof QA, dialogue, and summarization. We show\nthe number of generated samples for each hallu-\ncination pattern in Table 16 at the Appendix D.\nMoreover, we manually annotate 5,000 ChatGPT\nresponses for general user queries in Section 2.2.\nWe present a QA example and an annotated query\nand response example in Table 4. Among the an-\nnotated ChatGPT responses, 977 responses are la-\nbeled as containing hallucination (19.5%). Finally,\nwe present the topic distributions of our generated\ntask-specific samples and annotated ChatGPT re-\nsponses in Figure 2 and Figure 3, ranging from\nfilm, sports to school, computer, technology, etc.\nWith our benchmark, researchers can use it to\ninvestigate or mitigate the hallucination issue for\nLLMs in three aspects. Firstly, based on our gen-\nerated and annotated samples, researchers can use\nthem to analyze what types of content LLMs tend\nto generate hallucinations. Second, researchers can\nfurther evaluate the ability of LLMs to recognize\nhallucinations in the generated samples. For ex-\nample, given a question and an answer, LLMs can\nbe asked to determine whether the answer contains\nhallucinated content. Finally, our benchmark can\nbe further paired with human annotation to assess\nwhether the LLMs’ output contains hallucinations,\nsince the samples in our benchmark are specially\ndesigned for testing the hallucinations of LLMs.\nTo use our benchmark, users can run the code in\nour project repository to conduct the corresponding\nevaluation and analysis. Users can use our provided\ninstructions on their own datasets to evaluate LLMs\non hallucinations.\n6453\nModels QA Dialogue Summarization General\nChatGPT 62.59 72.40 58.53 79.44\nClaude 2 69.78 64.73 57.75 75.00\nClaude 67.60 64.83 53.76 73.88\nDavinci002 60.05 60.81 47.77 80.42\nDavinci003 49.65 68.37 48.07 80.40\nGPT-3 49.21 50.02 51.23 72.72\nLlama 2 49.60 43.99 49.55 20.46\nChatGLM 47.93 44.41 48.57 30.92\nFalcon 39.66 29.08 42.71 18.98\nVicuna 60.34 46.35 45.62 19.48\nAlpaca 6.68 17.55 20.63 9.54\nTable 5: Accuracy (%) of classifying whether a sample\ncontains hallucinated contents.\n3 Experiments\n3.1 Experimental Setup\nEvaluation Models. We evaluate several state-of-\nthe-art LLMs in HaluEval benchmark. First, we\nexperiment on five closed-source LLMs, including\nOpenAI’s GPT-3 (davinci) (Brown et al., 2020),\nInstructGPT (text-davinci-002/003) (Ouyang\net al., 2022), ChatGPT (gpt-3.5-turbo) and An-\nthropic’s Claude and Claude 2 models, which can\nonly be accessed through their APIs. Besides, we\nalso evaluate five prevalent open-source LLMs, in-\ncluding Alpaca (7B) (Taori et al., 2023), Vicuna\n(7B) (Chiang et al., 2023), ChatGLM (7B) (Zeng\net al., 2022), Falcon (7B) (TII, 2023), and Llama\n2-Chat (7B) (Touvron et al., 2023). Our experi-\nments were performed without fine-tuning or en-\ngaging in the tuning of hyper-parameters.\nImplementation Details. We execute the genera-\ntion process of hallucinated samples using Azure\nOpenAI ChatGPT API. We use a temperature of\n1.0 to generate samples and set the maximum num-\nber of tokens for generation to 256. Moreover, we\nset the frequency penalty to zero and top-pto 1.0.\nFor evaluation, we set the temperature to zero for\nall models to reduce output randomness and ensure\nmore focused and deterministic outputs.\nIn the following, we first conduct hallucination\nrecognition experiments, then propose several po-\ntentially useful strategies to improve the recogni-\ntion, and finally we perform qualitative analysis to\nunderstand the hallucination in LLMs.\n3.2 Results and Analysis\n3.2.1 Hallucination Recognition\nTo evaluate the ability of LLMs to recognize hal-\nlucinations, we randomly select the hallucinated\nor normal output (e.g., an answer) of each sample\nfor classification. The evaluation instructions of\nQA, dialogue, and summarization are presented in\nTable 13, Table 14 and Table 15 in Appendix C.\nTable 5 presents the accuracy of evaluated LLMs\nto classify whether the sample output contains hal-\nlucinated information. Our findings indicate that\nLLMs are still poor at identifying hallucination\nwhich might be implicit in text. For example,\nthe state-of-the-art ChatGPT model cannot distin-\nguish between factual and hallucinated summary\nand only achieves 58.53% accuracy in text summa-\nrization, which is barely above chance. Moreover,\nGPT-3 obtains just about random chance of 50%\naccuracy across three tasks, and Alpaca or Vicuna\neven performs worse (well below random chance).\nWe hypothesize that LLMs perform poorly because\nthe hallucinated sample we generate looks highly\nsimilar with ground-truth ones but differs in the\nkey factual spans. As we can see, from GPT-3 to\nInstructGPT and ChatGPT, instruction tuning and\nalignment with humans can strength the ability of\nLLMs in identifying the hallucinations in text.\nWith respect to the hallucinated samples where\nChatGPT fails to recognize, we present the number\nof each hallucination pattern in Table 6. Based on\nthe results, we can observe that the hallucination\npatterns of failed samples are unevenly distributed.\nFor example, over half of failures in QA, dialogue,\nand summarization originate from the first halluci-\nnation pattern (i.e., comprehension, extrinsic-soft,\nand factual), which refers to the hallucinations that\nare factually correct but conflict with the context.\nThis indicates that LLMs lack or cannot associate\nrelated knowledge to identify the factual hallucina-\ntion in the generated text. To further understand\nthe failures of ChatGPT, we visualize the topics of\nthose failed samples via Latent Dirichlet Allocation\n(LDA) (Blei et al., 2003). As shown in Figure 2\nand Figure 3, we cluster all task samples into ten\ntopics and mark the topics of failed samples as red.\nWe find that the hallucination of LLMs is topic-\nsensitive. For example, the frequent topics in QA\ninclude film, school, and company. While, Chat-\nGPT mainly fails to recognize those samples in\nthe topics of film, company, and band. For user\nqueries and ChatGPT responses, the top five topics\ninclude story, health, language, technology, and\ncomputer. ChatGPT mainly faces challenges in\ntopics of technology, climate, and language.\n6454\nTasks #Failed P-I P-II P-III P-IV\nQA 3109 1559 245 278 1027\nDialogue 891 465 344 82 -\nSummarization 3868 3106 705 57 -\nTable 6: Number of samples where ChatGPT fails to\nrecognize for each hallucination pattern (P-I/II/III/IV).\n3.2.2 Improvement Strategies\nIn this part, we design several strategies to improve\nthe ability of LLMs to recognize hallucination. The\nresults are shown in Table 8.\nKnowledge Retrieval. Retrieving relevant knowl-\nedge is a widely used strategy to eliminate halluci-\nnation (Lewis et al., 2020; Li et al., 2023a). There-\nfore, we supply ChatGPT with the knowledge facts\nretrieved from Wikipedia (except for that summa-\nrization does not need external information besides\nthe source document). By providing knowledge,\nthe recognition accuracy of ChatGPT increases sig-\nnificantly (e.g., increasing from 62.59 to 76.83 in\nQA), while the performance improvement in dia-\nlogue is mild. We hypothesize that the common\nhallucination patterns in dialogue ( i.e., extrinsic-\nsoft/hard) cannot be simply identified via incorpo-\nrating external knowledge. For those general user\nqueries and ChatGPT responses, we discover that\nproviding external knowledge does have a signifi-\ncant benefit. Thus, equipping LLMs with external\nknowledge can largely enhance their abilities to\nrecognize hallucinations.\nCoT Reasoning. In previous work (Wei et al.,\n2022), chain-of-thought (CoT) has been proposed\nto improve the ability of LLMs to perform reason-\ning and derive the final answer by introducing a se-\nries of intermediate reasoning steps. Here, besides\nproducing the recognition result, we also require\nChatGPT to generate the reasoning steps. While,\nfrom the results in Table 8, we observe that generat-\ning reasoning steps can mildly improve the perfor-\nmance but makes the model perform worse in QA\nand dialogue (e.g., dropping from 62.59 to 59.58).\nCompared to retrieving knowledge, adding chain-\nof-thought before output might interfere with the\nfinal judgement. While, in text summarization, gen-\nerating reasoning steps improve the accuracy from\n58.53 to 61.21. The reason might be that the factual\ncontradiction between document and summary can\nbe identified through logic reasoning.\nSample Contrast. We further provide ground-truth\nexamples for ChatGPT to test whether it can distin-\nguish the right sample from the hallucinated sam-\nple. As we can see from Table 8, distinguishing\nbetween right and hallucinated samples achieves\nthe worst results. We hypothesize that our gener-\nated hallucinated samples have a high similarity\nto the real samples, thus making LLMs confused\nto distinguish them. This test also indicates that\nour benchmark is very challenging in hallucination\nevaluation for LLMs.\n3.3 Case Study\nIn the above, we have observed that providing exter-\nnal knowledge can be beneficial for LLMs to miti-\ngate and recognize hallucinations. To demonstrate\nthe effectiveness of knowledge retrieval in miti-\ngating hallucinations, we present two hallucinated\nresponses from ChatGPT and refined responses\nafter augmented with retrieved knowledge in Ta-\nble 7. In the first example, the generated span (i.e.,\n“July 4, 1776 - Declaration of Independence sign-\ning”) contains hallucinated information because it\ngives a wrong time of Declaration of Independence\nsigning. By providing retrieved information about\nDeclaration of Independence signing, ChatGPT is\nable to correct the hallucinated span and give the\nright information. Analogously, in the second ex-\nample, ChatGPT gives incorrect GDP growth rates\nof China and India, which is due to that API-based\nChatGPT cannot access the web to obtain the of-\nficial data. After providing official information\nretrieved from World Bank, the refined span dis-\nplays answers that contain the correct information.\nThe above two examples illustrate that retrieving\nknowledge related to queries can help ChatGPT sig-\nnificantly reduce the hallucinations in the response,\nespecially those factual errors.\n4 Related Work\nHallucination in LLMs. Hallucination in LLMs is\nconcerning since it hinders performance and raises\nsafety risks in real-world application. To alleviate\nthis issue, prior studies have proposed to use a ver-\nification system to identify non-factual entities in\ntext summarization (Zhao et al., 2020), invoke in-\nterfaces of structured data (e.g., knowledge graph,\ndatabase) to obtain related evidence (Jiang et al.,\n2023; Lan et al., 2022), and train a token-level fact\ncritic to recognize hallucination and rectify them\nin dialogue (Dziri et al., 2021). To enhance the\nunderstanding of hallucination in LLMs and pro-\n6455\nUser Query Generate a list of 5 important dates in US history. Create a visualization to compare the GDP growth of\nIndia and China between 1998 and 1998.\nChatGPT\nResponse\n1. July 4, 1776 - Declaration of Independence sign-\ning\n2. April 12, 1861 - Start of the Civil War\n3. August 18, 1920 - 19th Amendment ratification gr-\nanting women the right to vote\n4. December 7, 1941 - Pearl Harbor attack during Wo-\nrld War II\n5. September 11, 2001 - Terrorist attacks on New York\nCity and Washington D.C.\nAs an AI language model, I cannot create visualizations,\nbut I can provide you with the following information:\nAccording to World Bank data, India and China have\nshown significant economic growth in the past two de-\ncades. In 1998, India’s GDP growth rate was 4.8%,\nwhile China’s GDP growth rate was 9.3%.\nIn the following years, the GDP growth rate fluctuat-\ned, but China maintained a higher growth rate than\nIndia. By 2019, China’s GDP growth rate was 6.1%,\nwhile India’s GDP growth rate was 4.2%.\nRetrieved\nKnowledge\nThe signing of the United States Declaration of Inde-\npendence occurred primarily on August 2, 1776, at the\nPennsylvania State House in Philadelphia, later known\nas Independence Hall.\nhttps://en.wikipedia.org/wiki/Signing_of_the\n_United_States_Declaration_of_Independence\nIndia’s GDP growth rate in 1998 was 6.2%. China’s\nGDP growth rate in 1998 was 7.8%.\nIndia’s GDP growth rate in 2019 was 3.7%. China’s\nGDP growth rate in 2019 was 6.0%.\nhttps://data.worldbank.org/\nRefined\nResponse\n1. August 2, 1776 - Declaration of Independence si-\ngning\n2. April 12, 1861 - Start of the Civil War\n3. August 18, 1920 - 19th Amendment ratification gr-\nanting women the right to vote\n4. December 7, 1941 - Pearl Harbor attack during Wo-\nrld War II\n5. September 11, 2001 - Terrorist attacks on New York\nCity and Washington D.C.\nAccording to World Bank data, India and China have\nshown significant economic growth in the past two de-\ncades. In 1998, India’s GDP growth rate was 6.2%,\nwhile China’s GDP growth rate was 7.8%. By 2019,\nChina’s GDP growth rate was 6.0%, while India’s\nGDP growth rate was 3.7%.\nTable 7: Two hallucinated and refined examples from ChatGPT. Thegreen text denotes the hallucinated span, and\nthe brown text denotes the refined span after augmented with retrieved knowledge.\nVariants QA Dialogue Summarization General\nChatGPT 62.59 72.40 58.53 86.22\nw/ Knowledge 76.83 73.80 - 90.73\nw/ CoT 59.58 71.39 61.21 86.50\nw/ Contrast 49.19 68.67 49.46 -\nTable 8: Accuracy (%) of ChatGPT equipped with three\nimprovement strategies.\nmote the unification of research efforts, there are\nmany active endeavors to analyze the causes of hal-\nlucination in different tasks and investigate their\nrelationship (Zheng et al., 2023; Das et al., 2023;\nCao et al., 2022). Our work is closely related to\nthese work, but we focus on building a hallucina-\ntion evaluation benchmark for LLMs. Our dataset\ncan serve as a public platform for exhibiting the\nblind spots of LLMs in solving hallucination.\nHallucination Evaluation. Another line of work\nfocusing on evaluating the hallucination of models\nin different NLP tasks (Dziri et al., 2022b; Gupta\net al., 2022; Dziri et al., 2022a; Rashkin et al., 2021;\nLi et al., 2023b). For instance, The BEGIN bench-\nmark (Dziri et al., 2022b) classifies the utterances\ngenerated by dialogue systems into three categories,\ni.e., fully attributable, not fully attributable, and\ngeneric; and the Attributable to Identified Sources\n(AIS) benchmark (Rashkin et al., 2021) assesses\nwhether the source documents support the output of\ntext generation models. Though these benchmarks\ncan serve as decent evaluation platform, they are\npenurious in only focusing on single tasks ( e.g.,\ndialogue) and small models (e.g., DPR). Besides,\nseveral metrics have been proposed to quantify hal-\nlucination, such as PARENT (Dhingra et al., 2019)\nfor measuring n-gram lexical entailment in table-to-\ntext generation and TRUE (Honovich et al., 2022)\ncomputes the example-level Area Under the ROC\nCurve. In this work, our HaluEval benchmark in-\ncludes general user queries and ChatGPT responses\nand proposes a two-step automatic process to gen-\nerate hallucinated samples for evaluation, which is\ncompletely based on LLMs.\n5 Conclusion\nWe introduce HaluEval, a large-scale collection of\ngenerated and human-annotated hallucinated sam-\nples for evaluating the performance of LLMs in\nrecognizing hallucinations. To automatically gen-\nerate large-scale samples, we propose a two-step\n6456\napproach, i.e., sampling-then-filtering. We first in-\ntroduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. Besides, we invite qual-\nified human labelers to annotate the hallucinations\nof ChatGPT responses given user queries. We find\nthat, existing LLMs mostly fail to recognize the hal-\nlucinations in text and tend to generate hallucinated\ncontent. Finally, we suggest several strategies to\nhelp LLMs recognize hallucinations. Our bench-\nmark can facilitate research in understanding what\ntypes of content and to which extent LLMs tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable LLMs in the future.\n6 Limitations\nIn our approach, we leverage a LLM,i.e., ChatGPT,\nto automatically generate the hallucinated samples.\nTherefore, the quality of our hallucinated samples\nis limited by the capacity of ChatGPT in following\nthe complex instruction of hallucination sampling.\nAlthough we design the high-quality hallucination\nfiltering process, it is still necessary to apply quality\ncontrol to the generation of hallucinated samples.\nBesides, our benchmark focuses on evaluating the\nability of LLMs in recognizing the hallucinations in\ntext but does not investigate the underlying reasons\nbehind the appearance of hallucinations like prior\nwork (Zheng et al., 2023; Das et al., 2023).\nAs for the potential issue, since the hallucinated\nsamples in our benchmark looks highly similar to\nthe ground-truth samples, which might be misused\nfor an unexpected purpose than we planned. To\nalleviate this issue, we should monitor and regulate\nthe spread and usage of our benchmark.\nAcknowledgments\nThis work was partially supported by National Nat-\nural Science Foundation of China under Grant No.\n62222215, Beijing Natural Science Foundation un-\nder Grant No. L233008 and 4222027, and Beijing\nOutstanding Young Scientist Program under Grant\nNo. BJJWZYJH012019100020098. And this work\nis also partially supported by the Outstanding Inno-\nvative Talents Cultivation Funded Programs 2021\nof Renmin University of China. Xin Zhao is the\ncorresponding author.\nReferences\n2023. Introducing Falcon LLM . https://falconllm.\ntii.ae.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, Quyet V . Do, Yan Xu,\nand Pascale Fung. 2023. A multitask, multilingual,\nmultimodal evaluation of chatgpt on reasoning, hal-\nlucination, and interactivity. CoRR, abs/2302.04023.\nDavid M Blei, Andrew Y Ng, and Michael I Jordan.\n2003. Latent dirichlet allocation. Journal of machine\nLearning research, 3(Jan):993–1022.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nMeng Cao, Yue Dong, and Jackie Chi Kit Cheung. 2022.\nHallucinated but factual! inspecting the factuality\nof hallucinations in abstractive summarization. In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 3340–3354.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nSouvik Das, Sougata Saha, and Rohini K Srihari. 2023.\nDiving deep into modes of fact hallucinations in dia-\nlogue systems. arXiv preprint arXiv:2301.04449.\nBhuwan Dhingra, Manaal Faruqui, Ankur P. Parikh,\nMing-Wei Chang, Dipanjan Das, and William W. Co-\nhen. 2019. Handling divergent reference texts when\nevaluating table-to-text generation. In Proceedings\nof the 57th Conference of the Association for Compu-\ntational Linguistics, ACL 2019, Florence, Italy, July\n28- August 2, 2019, Volume 1: Long Papers, pages\n4884–4895. Association for Computational Linguis-\ntics.\nNouha Dziri, Ehsan Kamalloo, Sivan Milton, Osmar R.\nZaïane, Mo Yu, Edoardo Maria Ponti, and Siva\nReddy. 2022a. Faithdial: A faithful benchmark for\ninformation-seeking dialogue. Trans. Assoc. Comput.\nLinguistics, 10:1473–1490.\nNouha Dziri, Andrea Madotto, Osmar Zaïane, and\nAvishek Joey Bose. 2021. Neural path hunter: Re-\nducing hallucination in dialogue systems via path\ngrounding. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, EMNLP 2021, Virtual Event / Punta Cana, Do-\nminican Republic, 7-11 November, 2021, pages 2197–\n2214. Association for Computational Linguistics.\n6457\nNouha Dziri, Hannah Rashkin, Tal Linzen, and David\nReitter. 2022b. Evaluating attribution in dialogue sys-\ntems: The BEGIN benchmark. Trans. Assoc. Com-\nput. Linguistics, 10:1066–1083.\nPrakhar Gupta, Chien-Sheng Wu, Wenhao Liu, and\nCaiming Xiong. 2022. Dialfact: A benchmark for\nfact-checking in dialogue. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), ACL\n2022, Dublin, Ireland, May 22-27, 2022, pages 3785–\n3801. Association for Computational Linguistics.\nOr Honovich, Roee Aharoni, Jonathan Herzig, Hagai\nTaitelbaum, Doron Kukliansy, Vered Cohen, Thomas\nScialom, Idan Szpektor, Avinatan Hassidim, and\nYossi Matias. 2022. TRUE: re-evaluating factual\nconsistency evaluation. In Proceedings of the 2022\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, NAACL 2022, Seattle, WA,\nUnited States, July 10-15, 2022 , pages 3905–3920.\nAssociation for Computational Linguistics.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of halluci-\nnation in natural language generation. ACM Comput-\ning Surveys, 55(12):1–38.\nJinhao Jiang, Kun Zhou, Keming Ye Zican Dong,\nWayne Xin Zhao, and Ji-Rong Wen. 2023. Structgpt:\nA general framework for large language model to\nreason on structured data.\nYunshi Lan, Gaole He, Jinhao Jiang, Jing Jiang,\nWayne Xin Zhao, and Ji-Rong Wen. 2022. Com-\nplex knowledge base question answering: A survey.\nIEEE Transactions on Knowledge & Data Engineer-\ning, (01):1–20.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Advances in Neu-\nral Information Processing Systems, 33:9459–9474.\nJunyi Li, Tianyi Tang, Wayne Xin Zhao, Jingyuan Wang,\nJian-Yun Nie, and Ji-Rong Wen. 2023a. The web\ncan be your oyster for improving language models.\nIn Findings of the Association for Computational\nLinguistics: ACL 2023, pages 728–746.\nJunyi Li, Tianyi Tang, Wayne Xin Zhao, and Ji-Rong\nWen. 2021. Pretrained language model for text gen-\neration: A survey. In Proceedings of the Thirtieth\nInternational Joint Conference on Artificial Intelli-\ngence, IJCAI 2021, Virtual Event / Montreal, Canada,\n19-27 August 2021, pages 4492–4499. ijcai.org.\nYifan Li, Yifan Du, Kun Zhou, Jinpeng Wang,\nWayne Xin Zhao, and Ji-Rong Wen. 2023b. Eval-\nuating object hallucination in large vision-language\nmodels.\nPotsawee Manakul, Adian Liusie, and Mark J. F. Gales.\n2023. Selfcheckgpt: Zero-resource black-box hal-\nlucination detection for generative large language\nmodels. CoRR, abs/2303.08896.\nSeungwhan Moon, Pararth Shah, Anuj Kumar, and Ra-\njen Subba. 2019. Opendialkg: Explainable conver-\nsational reasoning with attention-based walks over\nknowledge graphs. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray,\nJohn Schulman, Jacob Hilton, Fraser Kelton, Luke\nMiller, Maddie Simens, Amanda Askell, Peter Welin-\nder, Paul F. Christiano, Jan Leike, and Ryan Lowe.\n2022. Training language models to follow instruc-\ntions with human feedback. In NeurIPS.\nHannah Rashkin, Vitaly Nikolaev, Matthew Lamm,\nMichael Collins, Dipanjan Das, Slav Petrov, Gau-\nrav Singh Tomar, Iulia Turc, and David Reitter. 2021.\nMeasuring attribution in natural language generation\nmodels. CoRR, abs/2112.12870.\nAbigail See, Peter J. Liu, and Christopher D. Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics, ACL 2017, Vancouver, Canada, July 30 -\nAugust 4, Volume 1: Long Papers, pages 1073–1083.\nAssociation for Computational Linguistics.\nWeiwei Sun, Zhengliang Shi, Shen Gao, Pengjie Ren,\nMaarten de Rijke, and Zhaochun Ren. 2023. Con-\ntrastive learning reduces hallucination in conversa-\ntions. In Thirty-Seventh AAAI Conference on Artifi-\ncial Intelligence, AAAI 2023, Thirty-Fifth Conference\non Innovative Applications of Artificial Intelligence,\nIAAI 2023, Thirteenth Symposium on Educational\nAdvances in Artificial Intelligence, EAAI 2023, Wash-\nington, DC, USA, February 7-14, 2023, pages 13618–\n13626. AAAI Press.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford al-\npaca: An instruction-following llama model. https:\n//github.com/tatsu-lab/stanford_alpaca.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint\narXiv:2307.09288.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V . Le,\nand Denny Zhou. 2022. Chain-of-thought prompt-\ning elicits reasoning in large language models. In\nNeurIPS.\n6458\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\ngio, William W. Cohen, Ruslan Salakhutdinov, and\nChristopher D. Manning. 2018. HotpotQA: A dataset\nfor diverse, explainable multi-hop question answer-\ning. In Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP).\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,\nHanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\nWendi Zheng, Xiao Xia, Weng Lam Tam, Zix-\nuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen,\nPeng Zhang, Yuxiao Dong, and Jie Tang. 2022.\nGLM-130B: an open bilingual pre-trained model.\nabs/2210.02414.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. Bertscore: Evalu-\nating text generation with BERT. In8th International\nConference on Learning Representations, ICLR 2020,\nAddis Ababa, Ethiopia, April 26-30, 2020. OpenRe-\nview.net.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Be-\nichen Zhang, Junjie Zhang, Zican Dong, Yifan Du,\nChen Yang, Yushuo Chen, Zhipeng Chen, Jinhao\nJiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang\nLiu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen.\n2023. A survey of large language models. CoRR,\nabs/2303.18223.\nZheng Zhao, Shay B. Cohen, and Bonnie Webber. 2020.\nReducing quantity hallucinations in abstractive sum-\nmarization. CoRR, abs/2009.13312.\nShen Zheng, Jie Huang, and Kevin Chen-Chuan Chang.\n2023. Why does chatgpt fall short in answering ques-\ntions faithfully? CoRR, abs/2304.10513.\nAppendix\nWe provide some extra information about our\nbenchmark as supplementary materials. The ap-\npendix is organized into three sections:\n• Instructions of hallucination sampling are pre-\nsented in Appendix A;\n• Instructions of hallucination filtering are pre-\nsented in Appendix B;\n• Instructions of evaluation are presented in Ap-\npendix C;\n• Details of our benchmark are presented in Ap-\npendix D.\nA Hallucination Sampling\nThe hallucination sampling instructions for dia-\nlogue and summarization are shown in Table 9 and\nTable 10, respectively.\nB Hallucination Filtering\nThe hallucination sampling instructions for dia-\nlogue and summarization are shown in Table 11\nand Table 12, respectively.\nC Hallucination Recognition\nThe hallucination recognition instructions for QA,\ndialogue and summarization are shown in Table 13,\nTable 14 and Table 15, respectively.\nD Details of HaluEval\nThe number of generated hallucinated samples for\neach hallucination pattern are shown in Table 16.\n6459\nI want you act as an assistant in a conversation with human. Given a dialogue history, the true response, and\nrelated knowledge, your objective is to write a hallucinated response that sounds plausible but is factually\nincorrect. You SHOULD write the hallucinated response using the following method (each with some examples):\nYou are trying to write a response to human but you replace the true entity with a highly similar entity.\n#Knowledge#: The Dark Knight is a 2008 superhero film directed by Christopher Nolan from a screenplay he\nco-wrote with his brother Jonathan. Christopher Nolan is a film director.\n#Dialogue History#: [Human]: Could you recommend movies similar to The Dark Knight? [Assistant]: The\nsequel to Batman Begins is The Dark Knight. [Human]: Okay. Who is the director of The Dark Knight and any\nother movies from him not related to Batman?\n#True Response#:Christopher Nolan was the director. He also directed insomnia and inception.\n#Hallucinated Response#:Steven Spielberg was the director. He also directed insomnia and inception.\nor\nYou are trying to write a response to human but you replace the true entity with a dissimilar entity.\n<Demonstrations>\nor\nYou are trying to write a response to human but you replace the true entity with a dissimilar entity in a different\nentity type.\n<Demonstrations>\nYou should try your best to make the response become hallucinated.\n#Knowledge#: <Here is the related knowledge>\n#Dialogue History#:<Here is the dialogue history>\n#True Response#:<Here is the true response of the dialogue history>\n#Hallucinated Response#:\nTable 9: Instruction of hallucination sampling for knowledge-grounded dialogue.\nI want you act as a hallucination summary generator. Given a document and the right summary, your objective\nis to write a hallucinated summary that sounds plausible but is factually incorrect. You SHOULD write the\nhallucinated summary using the following method (each with some examples):\nYou are trying to write a summary which is factual but some information cannot be directly inferred or entailed\nfrom the document.\n#Document#: The panther chameleon was found on Monday by a dog walker in the wooded area at Marl Park.\nIt had to be put down after X-rays showed all of its legs were broken and it had a deformed spine. RSPCA\nCymru said it was an \"extremely sad example of an abandoned and neglected exotic pet\". Inspector Selina Chan\nsaid: \"It is a possibility that the owners took on this animal but were unable to provide the care he needs and\ndecided to release him to the wild. \"We are urging potential owners of exotic animals to thoroughly research\nwhat is required in the care of the particular species before taking one on. \"Potential owners need to make sure\nthey can give their animal the environment it needs and they have the facilities, time, financial means and long-\nterm commitment to maintain a good standard of care, as required under the Animal Welfare Act 2006.\" She\nadded it was illegal to release non-native species into the wild.\n#Right Summary#:Owners of exotic animals have been urged to do research before having them as pets after a\nseriously neglected chameleon was found in Cardiff Bay.\n#Hallucinated Summary#:A chameleon that was found in a Cardiff park has been put down after being aban-\ndoned and neglected by its owners.\nor\nYou are trying to write a summary but there exist some non-factual and incorrect information. You can fabricate\nsome information that does not exist in the provided document.\n<Demonstrations>\nor\nYou are trying to write a summary but there is a factual contradiction between the summary and the document.\n<Demonstrations>\nYou should try your best to make the summary become hallucinated. #Hallucinated Summary# can only have\nabout 5 more words than #Right Summary#.\n#Document#: <Here is the test document>\n#Right Summary#:<Here is the right summary of the test document>\n#Hallucinated Summary#:\nTable 10: Instruction of hallucination sampling for text summarization.\n6460\nI want you act as a response judge. Given a dialogue history, two responses, and related knowledge, your\nobjective is to select the best and correct response without hallucination and non-factual information. Here are\nsome examples:\n#Knowledge#:The Dark Knight is a 2008 superhero film directed by Christopher Nolan from a screenplay he\nco-wrote with his brother Jonathan. Christopher Nolan is a film director.\n#Dialogue History#: [Human]: Could you recommand movies similar to The Dark Knight? [Assistant]: The\nsequel to Batman Begins is The Dark Knight. [Human]: Okay. Who is the director of The Dark Knight and any\nother movies from him not related to Batman?\n#Response 1#:Christopher Nolan was the director. He also directed insomnia and inception.\n#Response 2#:Steven Spielberg was the director. He also directed insomnia and inception.\n#Your Choice#:The best response is Response 1.\n...\n<Demonstrations>\n...\nYou should try your best to select the best and correct response. If the two responses are the same, you can\nrandomly choose one. If both responses are incorrect, choose the better one. You MUST select a response from\nthe provided two responses.\n#Knowledge#: <Here is the related knowledge>\n#Dialogue History#:<Here is the dialogue history>\n#Response 1#:<Here is the hallucinated response generated by the first channel>\n#Response 2#:<Here is the hallucinated response generated by the second channel>\n#Your Choice#:\nTable 11: Instruction of hallucination filtering for knowledge-grounded dialogue.\nI want you act as a summary judge. Given a document and two summaries, your objective is to select the best\nand correct summary without hallucination and non-factual information. Here are some examples:\n#Document#:The panther chameleon was found on Monday by a dog walker in the wooded area at Marl Park. It\nhad to be put down after X-rays showed all of its legs were broken and it had a deformed spine. RSPCA Cymru\nsaid it was an \"extremely sad example of an abandoned and neglected exotic pet\". Inspector Selina Chan said:\n\"It is a possibility that the owners took on this animal but were unable to provide the care he needs and decided\nto release him to the wild. \"We are urging potential owners of exotic animals to thoroughly research what is\nrequired in the care of the particular species before taking one on. \"Potential owners need to make sure they\ncan give their animal the environment it needs and they have the facilities, time, financial means and long-term\ncommitment to maintain a good standard of care, as required under the Animal Welfare Act 2006.\" She added it\nwas illegal to release non-native species into the wild.\n#Summary 1#: Owners of exotic animals have been urged to do research before having them as pets after a\nseriously neglected chameleon was found in Cardiff Bay.\n#Summary 2#: A chameleon that was found in a Cardiff park has been put down after being abandoned and\nneglected by its owners.\n#Your Choice#:The best summary is Summary 1.\n...\n<Demonstrations>\n...\nYou should try your best to select the best and correct summary. If both summaries are incorrect, choose the\nbetter one. You MUST select a summary from the provided two summaries.\n#Document#: <Here is the test document>\n#Summary 1#:<Here is the hallucinated summary generated by the first channel>\n#Summary 2#:<Here is the hallucinated summary generated by the second channel>\n#Your Choice#:\nTable 12: Instruction of hallucination filtering for text summarization.\n6461\nI want you act as an answer judge. Given a question and an answer, your objective is to determine if the\nprovided answer contains non-factual or hallucinated information. You SHOULD give your judgement based\non the following hallucination types and the world knowledge.\nYou are trying to determine if there is a factual contradiction between the answer and the world knowledge.\nSome information in the answer might be fabricated.\n#Question#: What U.S Highway gives access to Zilpo Road, and is also known as Midland Trail?\n#Answer#: U.S. Highway 60\n#Your Judgement#:No\n#Question#: Are the New Orleans Outfall Canals the same length as the Augusta Canal?\n#Answer#: No. The Orleans Canal is approximately 3.6 miles (5.8 kilometers) long while the Augusta Canal is\napproximately 7 miles (11.3 kilometers) long.\n#Your Judgement#:Yes\nYou are trying to determine if the answer misunderstands the question context and intention.\n<Demonstrations>\nYou are trying to determine if the answer is too general or too specific to answer the question at an appropriate\nlevel of specificity.\n<Demonstrations>\nYou are trying to determine if the answer cannot be inferred from the knowledge correctly.\n<Demonstrations>\nYou should try your best to determine if the answer contains non-factual or hallucinated information according\nto the above hallucination types. The answer you give MUST be “Yes” or “No”.\n#Question#: <Here is the test question>\n#Answer#: <Here is the hallucinated answer or right answer>\n#Your Judgement#:\nTable 13: Instruction of hallucination recognition for question answering.\nI want you act as a response judge. Given a dialogue history and a response, your objective is to determine if the\nprovided response contains non-factual or hallucinated information. You SHOULD give your judgement based\non the following hallucination types and the world knowledge.\nYou are trying to determine if the true entity in the response is replaced with a highly similar entity.\n#Dialogue History#: [Human]: Could you recommend movies similar to The Dark Knight? [Assistant]: The\nsequel to Batman Begins is The Dark Knight. [Human]: Okay. Who is the director of The Dark Knight and any\nother movies from him not related to Batman?\n#Response#: Christopher Nolan was the director. He also directed insomnia and inception.\n#Your Judgement#:No\n#Dialogue History#: [Human]: Could you recommend movies similar to The Dark Knight? [Assistant]: The\nsequel to Batman Begins is The Dark Knight. [Human]: Okay. Who is the director of The Dark Knight and any\nother movies from him not related to Batman?\n#Response#: Steven Spielberg was the director. He also directed insomnia and inception.\n#Your Judgement#:Yes\nYou are trying to determine if the true entity in the response is replaced with a dissimilar entity.\n<Demonstrations>\nYou are trying to determine if the true entity in the response is replaced with a dissimilar entity in a different\nentity type.\n<Demonstrations>\nYou should try your best to determine if the response contains non-factual or hallucinated information according\nto the above hallucination types. The answer you give MUST be “Yes” or “No”.\n#Dialogue History#:<Here is the dialogue history>\n#Response#: <Here is the hallucinated response or right response>\n#Your Judgement#:\nTable 14: Instruction of hallucination recognition for knowledge-grounded dialogue.\n6462\nI want you act as a summary judge. Given a document and a summary, your objective is to determine if the\nprovided summary contains non-factual or hallucinated information. You SHOULD give your judgement based\non the following hallucination types and the world knowledge.\nYou are trying to determine if the summary is factual but some information cannot be directly inferred or en-\ntailed from the document.\n#Document#: The panther chameleon was found on Monday by a dog walker in the wooded area at Marl Park.\nIt had to be put down after X-rays showed all of its legs were broken and it had a deformed spine. RSPCA\nCymru said it was an \"extremely sad example of an abandoned and neglected exotic pet\". Inspector Selina Chan\nsaid: \"It is a possibility that the owners took on this animal but were unable to provide the care he needs and\ndecided to release him to the wild. \"We are urging potential owners of exotic animals to thoroughly research\nwhat is required in the care of the particular species before taking one on. \"Potential owners need to make sure\nthey can give their animal the environment it needs and they have the facilities, time, financial means and long-\nterm commitment to maintain a good standard of care, as required under the Animal Welfare Act 2006.\" She\nadded it was illegal to release non-native species into the wild.\n#Summary#: A chameleon that was found in a Cardiff park has been put down after being abandoned and\nneglected by its owners.\n#Your Judgement#:Yes\nYou are trying to determine if there exists some non-factual and incorrect information in the summary.\n<Demonstrations>\nYou are trying to determine if there is a factual contradiction between the summary and the document.\n<Demonstrations>\nYou should try your best to determine if the summary contains non-factual or hallucinated information accord-\ning to the above hallucination types. The answer you give MUST be “Yes” or “No”.\n#Document#: <Here is the test document>\n#Summary#: <Here is the hallucinated summary or right summary>\n#Your Judgement#:\nTable 15: Instruction of hallucination recognition for text summarization.\n6463\nTasks #Sample P-I P-II P-III P-IV\nQA 10000 2280 1378 5102 1240\nDialogue 10000 8330 1196 474 -\nSumma. 10000 2614 3562 3824 -\nTable 16: Number of generated samples for each hal-\nlucination pattern (P-I/II/III/IV). “‘Summa.” is short\nfor summarization. “-” is due to that we consider three\npatterns in dialogue and summarization.\n6464",
  "topic": "Hallucinating",
  "concepts": [
    {
      "name": "Hallucinating",
      "score": 0.987247109413147
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.7932907342910767
    },
    {
      "name": "Computer science",
      "score": 0.6449866890907288
    },
    {
      "name": "Sampling (signal processing)",
      "score": 0.5420239567756653
    },
    {
      "name": "Artificial intelligence",
      "score": 0.49357736110687256
    },
    {
      "name": "Face (sociological concept)",
      "score": 0.45141294598579407
    },
    {
      "name": "Natural language processing",
      "score": 0.43520495295524597
    },
    {
      "name": "Machine learning",
      "score": 0.3491518795490265
    },
    {
      "name": "Computer vision",
      "score": 0.1406133770942688
    },
    {
      "name": "Sociology",
      "score": 0.1118675172328949
    },
    {
      "name": "Social science",
      "score": 0.08323076367378235
    },
    {
      "name": "Filter (signal processing)",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I70931966",
      "name": "Université de Montréal",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I4210096250",
      "name": "Beijing Institute of Big Data Research",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I78988378",
      "name": "Renmin University of China",
      "country": "CN"
    }
  ],
  "cited_by": 167
}