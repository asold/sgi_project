{
    "title": "EFA-Trans: An Efficient and Flexible Acceleration Architecture for Transformers",
    "url": "https://openalex.org/W4308128267",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2097974697",
            "name": "Xin Yang",
            "affiliations": [
                "Sun Yat-sen University"
            ]
        },
        {
            "id": "https://openalex.org/A2096346239",
            "name": "Tao Su",
            "affiliations": [
                "Sun Yat-sen University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6739901393",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W3047848469",
        "https://openalex.org/W3017024317",
        "https://openalex.org/W3189877953",
        "https://openalex.org/W6801547814",
        "https://openalex.org/W3196923642",
        "https://openalex.org/W2915106038",
        "https://openalex.org/W3159727696",
        "https://openalex.org/W3176468986",
        "https://openalex.org/W3162542754",
        "https://openalex.org/W3130240120",
        "https://openalex.org/W3100980998",
        "https://openalex.org/W2252272516",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W3184454880",
        "https://openalex.org/W3199934250",
        "https://openalex.org/W3155487259"
    ],
    "abstract": "The topic of transformers is rapidly emerging as one of the most important key primitives in neural networks. Unfortunately, most hardware designs for transformers are deficient, either hardly considering the configurability of the design or failing to realize the complete inference process of transformers. Specifically, few studies have paid attention to the compatibility of different computing paradigms. Thus, this paper presents EFA-Trans, a highly efficient and flexible hardware accelerator architecture for transformers. To reach high performance, we propose a configurable matrix computing array and leverage on-chip memories optimizations. In addition, with the design of nonlinear modules and fine-grained scheduling, our architecture can perform complete transformer inference. EFA-Trans is also compatible with dense and sparse patterns, which further expands its application scenarios. Moreover, a performance analytic model is abstracted to guide the determination of architecture parameter sets. Finally, our designs are developed by RTL and evaluated on Xilinx ZCU102. Experimental results demonstrate that EFA-Trans provides 23.74× and 7.58× improvement in energy efficiency compared with CPU and GPU, respectively. It also shows DSP efficiency is between 3.59× and 21.07× higher than others, outperforming existing advanced works.",
    "full_text": null
}