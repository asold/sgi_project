{
    "title": "IAA: Inner-Adaptor Architecture Empowers Frozen Large Language Model with Multimodal Capabilities",
    "url": "https://openalex.org/W4409363283",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A1995576685",
            "name": "Bin Wang",
            "affiliations": [
                "Qihoo 360 (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2144323589",
            "name": "Chunyu Xie",
            "affiliations": [
                "Qihoo 360 (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2657282229",
            "name": "Dawei Leng",
            "affiliations": [
                "Qihoo 360 (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2098560646",
            "name": "Yu-hui Yin",
            "affiliations": [
                "Qihoo 360 (China)"
            ]
        },
        {
            "id": "https://openalex.org/A1995576685",
            "name": "Bin Wang",
            "affiliations": [
                "Art Institute of Portland"
            ]
        },
        {
            "id": "https://openalex.org/A2144323589",
            "name": "Chunyu Xie",
            "affiliations": [
                "Art Institute of Portland"
            ]
        },
        {
            "id": "https://openalex.org/A2657282229",
            "name": "Dawei Leng",
            "affiliations": [
                "Art Institute of Portland"
            ]
        },
        {
            "id": "https://openalex.org/A2098560646",
            "name": "Yu-hui Yin",
            "affiliations": [
                "Art Institute of Portland"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4389713827",
        "https://openalex.org/W6600599538",
        "https://openalex.org/W4376653782",
        "https://openalex.org/W2251512949",
        "https://openalex.org/W4318718936",
        "https://openalex.org/W6639102338",
        "https://openalex.org/W6845850023",
        "https://openalex.org/W6857162426",
        "https://openalex.org/W6851592950",
        "https://openalex.org/W4296605665",
        "https://openalex.org/W2144960104",
        "https://openalex.org/W6780502592",
        "https://openalex.org/W1773149199",
        "https://openalex.org/W2983943451",
        "https://openalex.org/W4387323722",
        "https://openalex.org/W6729503815",
        "https://openalex.org/W6723035356",
        "https://openalex.org/W6851389067",
        "https://openalex.org/W3042266831",
        "https://openalex.org/W4320831569"
    ],
    "abstract": "In the field of multimodal large language models (MLLMs), common methods typically involve unfreezing the language model during training to foster profound visual understanding. However, the fine-tuning of such models with vision-language data often leads to a diminution of their natural language processing (NLP) capabilities. To avoid this performance degradation, a straightforward solution is to freeze the language model while developing multimodal competencies. Unfortunately, previous works have not attained satisfactory outcomes. Building on the strategy of freezing the language model, we conduct thorough structural exploration and introduce the Inner-Adaptor Architecture (IAA). Specifically, the architecture incorporates multiple multimodal adaptors at varying depths within the large language model to facilitate direct interaction with the inherently text-oriented transformer layers, thereby enabling the frozen language model to acquire multimodal capabilities. Unlike previous approaches of freezing language models that require large-scale aligned data, our proposed architecture is able to achieve superior performance on small-scale datasets. We conduct extensive experiments to improve the general multimodal capabilities and visual grounding abilities of the MLLM. Our approach remarkably outperforms previous state-of-the-art methods across various vision-language benchmarks without sacrificing performance on NLP tasks. Code and models will be released.",
    "full_text": "IAA: Inner-Adaptor Architecture Empowers Frozen Large Language Model with\nMultimodal Capabilities\nBin Wang*, Chunyu Xie*, Dawei Leng†, Yuhui Yin\n360 AI Research\n{wangbin10, xiechunyu, lengdawei, yinyuhui}@360.cn\nAbstract\nIn the field of multimodal large language models (MLLMs),\ncommon methods typically involve unfreezing the language\nmodel during training to foster profound visual understand-\ning. However, the fine-tuning of such models with vision-\nlanguage data often leads to a diminution of their natural\nlanguage processing (NLP) capabilities. To avoid this perfor-\nmance degradation, a straightforward solution is to freeze the\nlanguage model while developing multimodal competencies.\nUnfortunately, previous works have not attained satisfactory\noutcomes. Building on the strategy of freezing the language\nmodel, we conduct thorough structural exploration and in-\ntroduce the Inner-Adaptor Architecture (IAA). Specifically,\nthe architecture incorporates multiple multimodal adaptors\nat varying depths within the large language model to facili-\ntate direct interaction with the inherently text-oriented trans-\nformer layers, thereby enabling the frozen language model to\nacquire multimodal capabilities. Unlike previous approaches\nof freezing language models that require large-scale aligned\ndata, our proposed architecture is able to achieve superior\nperformance on small-scale datasets. We conduct extensive\nexperiments to improve the general multimodal capabilities\nand visual grounding abilities of the MLLM. Our approach\nremarkably outperforms previous state-of-the-art methods\nacross various vision-language benchmarks without sacrific-\ning performance on NLP tasks. Code and models will be re-\nleased.\nIntroduction\nLarge Language Models (LLMs) have made substantial\nprogress in recent years, largely attributed to the technique\nof pre-training and instruction tuning. Building upon this\nfoundation, visual instruction tuning has been proposed\nto evolve LLMs into Multimodal Large Language Models\n(MLLMs), thereby endowing them with the capability to\ninterpret and comprehend visual signals (Cha et al. 2024).\nMLLMs (Liu et al. 2024b; Bai et al. 2023; Tong et al. 2024;\nXuan et al. 2024; Huang et al. 2023a) prove beneficial in nu-\nmerous tasks, such as transcribing the text within an image,\ngenerating stories and poems based on an image, or con-\nverting screenshots of webpages into code (Laurenc ¸on et al.\n*These authors contributed equally.\n†Corresponding author.\nCopyright © 2025, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nMMLU C-eval0\n10\n20\n30\n40\n50\n60\n70\n80score\nQwen2\nLLaVA-Qwen2\nLlama3\nLLaVA-Llama3\nFigure 1: Results before and after training LLaV A-1.5 ar-\nchitecture based on Qwen2 and Llama3 language models on\ntext-only evaluation set MMLU and C-eval.\n2024). Historically, these tasks have been regarded as chal-\nlenging for conventional vision-language models. MLLMs\nexhibit considerable promise in executing these complex,\ndiverse real-world tasks, enabling more natural and human-\nlike interactions (Lu et al. 2024).\nTypically, the operation of a MLLM begins with feed-\ning an image into a visual encoder, such as CLIP (Radford\net al. 2021) or SigLIP (Zhai et al. 2023), to extract a high-\ndimensional feature representation. This feature is subse-\nquently transformed through a projection layer to align with\nthe dimension of the large language model. The resulting\nfeatures, often referred to as image tokens, are concatenated\nwith text tokens and fed into the large language model. This\nprocess enables the MLLM to generate responses based on\nuser instructions and input images.\nIn the current common MLLM (Liu et al. 2024a; Bai et al.\n2023), when image and text tokens are fed into the large\nlanguage model, the LLM is typically unfrozen for further\ntraining. This strategy has led to significant advancements\nin the MLLM model. Consequently, it predictably leads to\na degradation in the understanding ability of the large lan-\nguage model. To validate this hypothesis, we conduct ex-\nperiments on the LLaV A-1.5 (Liu et al. 2024a) architecture\nusing the 1.2M-size open-source dataset provided by (Liu\net al. 2024a), which contains a limited amount of plain text\ndata, as illustrated in Figure 1. We compare the results be-\nThe Thirty-Ninth AAAI Conference on Artificial Intelligence (AAAI-25)\n21035\nfore and after training the LLaV A-1.5 architecture, based\non the Qwen2 (Yang et al. 2024) and Llama3 (Meta 2024)\nlanguage models, respectively. The performance of the lan-\nguage model declines significantly on both the MMLU\n(Hendrycks et al. 2020) and C-Eval (Huang et al. 2023b)\ntext-only evaluation sets.\nIt appears reasonable to posit an explanation for this phe-\nnomenon within the field of deep learning. When a model is\npredominantly trained on a single type of data, it may experi-\nence a phenomenon known as catastrophic forgetting. For an\nMLLM to achieve outstanding image-text comprehension, it\nis essential to collect a substantial amount of image-text in-\nteraction data for training. As observed in Figure 1, training\nwith image-text data results in a decline in language ability.\nDespite attempts by MLLM such as LLaV A to incorporate\nsome text-only data into their training process, this still leads\nto a reduction in the model’s comprehension.\nOne direct method to prevent the degradation of LLM per-\nformance is to freeze the large language model during the\ntraining of MLLM. However, current methods employing\nthis approach (Li et al. 2023a; Zhu et al. 2023) have con-\nsistently struggled to achieve powerful multimodal capabil-\nities. To address these challenges, we propose a new train-\ning paradigm with an inner-adaptor architecture that signif-\nicantly enhances multimodal competencies without affect-\ning the original language modeling capabilities. This ap-\nproach can seamlessly support both multimodal and tex-\ntual workflows. We evaluate this training paradigm across\na spectrum of tasks, including general multimodal capabili-\nties and visual grounding proficiencies. Distinct from previ-\nous approaches of freezing language modeling that require\nlarge-scale aligned data, our proposed scheme demonstrates\neffectiveness with a considerably smaller dataset. Compre-\nhensive testing on a suite of benchmarks, including MME,\nMMBench, MMMU, and RefCOCO, has substantiated the\nsuperior performance of our structure. We hope that this ap-\nproach will provide a reference for future research in open-\nsource MLLM.\nRelated Work\nLarge Language Models. The landscape of Natural Lan-\nguage Processing (NLP) has undergone a revolutionary\ntransformation, driven by the advent and continuous refine-\nment of Large Language Models (LLMs). A pivotal mo-\nment in this evolution is the first appearance of the trans-\nformer architecture, which serves as a key catalyst, giving\nrise to pioneering language models like BERT (Devlin et al.\n2018) and OPT (Zhang et al. 2022). These models show-\ncase an unprecedented level of linguistic comprehension,\nsignificantly advancing the state-of-the-art in NLP. A critical\nbreakthrough comes with introducing the Generative Pre-\ntrained Transformer (GPT) series (Brown et al. 2020), which\npioneer an auto-regressive language modeling approach, set-\nting a new standard for language prediction and generation\ncapabilities. Subsequent iterations, including Mixtral (Jiang\net al. 2024), GPT-4 (Achiam et al. 2023), and Llama3 (Meta\n2024), have not only maintained but also amplified this mo-\nmentum, displaying superior performance on intricate lan-\nguage processing challenges. Moreover, the fusion of LLMs\nwith specialized visual tasks showcases the models’ adapt-\nability and broadens their scope, indicating their potential\nto transcend conventional text-based operations into multi-\nmodal interactions. This expansion highlights the transfor-\nmative role LLMs can assume when incorporated into di-\nverse domains, providing a rich ground for innovation and\nexploration.\nMultimodal Large Language Models. The advancement\nof Large Language Models (LLMs) has kindled a growing\ninterest in extending their foundational competencies to in-\ncorporate the visual domain, thereby giving birth to mul-\ntimodal Large Language Models (MLLMs). The works on\nMLLMs (Li et al. 2023a; Bai et al. 2023; Liu et al. 2024b;\nLaurenc ¸on et al. 2024; Xie et al. 2023; Li et al. 2023b)\ntypically follow a tripartite architecture: a visual encoder,\na vision-language connector, and a large language model.\nNotably, BLIP-2 (Li et al. 2023a) and Flamingo (Alayrac\net al. 2022) introduce the Q-Former/Resampler as a bridge\nbetween vision and language, whereas LLaV A (Liu et al.\n2024b) and MiniGPT4 (Zhu et al. 2023) refine this con-\nnection via a linear layer. Cambrian-1 (Tong et al. 2024)\nproposes a dynamically adaptive connector that integrates\nhigh-resolution visual features with LLMs while reducing\nthe number of tokens. To enhance their multimodal perfor-\nmance, contemporary MLLMs mainly fine-tune the LLM\nand connector using visual instruction tuning data. These\nmodels leverage meticulously curated instruction datasets,\nshowcasing an effective strategy that highlights their ro-\nbust capabilities. However, a common oversight lies in the\nmaintenance of language abilities. Long term multimodal\ntraining often leads to degradation of language proficiency.\nCogVLM (Wang et al. 2023) seeks to address this by in-\ntegrating a trainable visual expert into the language model,\nbut still trains the LLM during supervised fine-tuning, re-\nsulting in a degradation of language capability. DeekSeek-\nVL (Lu et al. 2024) maintains a 70% proportion of language\ndata to preserve the integrity of language knowledge within\nthe model, but incurs a considerable training cost. Depart-\ning from these conventional training paradigms of MLLMs,\nwe introduce the inner-adaptor architecture. This design is\nspecifically tailored to preserve the NLP performance of\nthe MLLM while facilitating a seamless augmentation of its\nmultimodal capabilities.\nMethodology\nOverview. As illustrated in Figure 2, our approach enables\nthe simultaneous execution of two high-quality workflows\npost-deployment: one for multimodal interactions and the\nother for text-only conversations. Both workflows leverage\nthe transformer layers of the large language model. The mul-\ntimodal interaction workflow encompasses: (1) an image en-\ncoder and a projector, utilized for extracting high-quality im-\nage features and achieving vision-language alignment, re-\nspectively, (2) the transformer layers of the large language\nmodel, which remain frozen during training, and (3) the\ninner-adaptor architecture, which comprises insertion layers,\nan embedding layer, and a language model head specifically\ndesigned for multimodal inputs. Conversely, the text-only\n21036\nFigure 2: Overview of the proposed architecture, which mainly consists of two workflows: the Multimodal Workflow and the\nText-only Workflow. The multimodal workflow, beyond the necessary image encoder and projector, integrates the Inner-Adaptor\nArchitecture, including insertion layers, an embedding layer, and a language model head. Both workflows share the same large\nlanguage model. The number of insertion layers is variable, whereN ≤ M. In this context,MM denotes MultiModal, EL stands\nfor Embedding Layer, and LH represents the Language model Head.\nconversation workflow solely employs the constituent ele-\nments of the original language model, without resorting to\nthe specialized multimodal components.\nImage Encoder and Projector. Following LLava-1.5\n(Liu et al. 2024a), we utilize the CLIP ViT-L/14 (Radford\net al. 2021) image encoder with an input resolution of 336px.\nSubsequently, we employ a vision-language projector com-\nposed of a two-layer MLP to integrate the vision features\nwith LLMs.\nLarge Language Model. We employ the Llama3-8B\n(Meta 2024) as the base language model throughout the\ntraining process.\nInner-Adaptor Architecture. To achieve multimodal\ncomprehension, it is essential to integrate trainable param-\neters into MLLMs. LLaV A (Liu et al. 2024b) makes the pro-\njector and the large language model trainable during visual\ninstruction tuning, but leads to the performance degrada-\ntion on NLP tasks. Flamingo (Alayrac et al. 2022) employs\ncross-attention with a gating mechanism to introduce im-\nage information into the model, facilitating a deep fusion of\noriginal image features with text features prior to each layer\nof the language model. However, this approach requires a\nconsiderable volume of pre-training data to train effective\ncross-attention layers and gating values, which can be com-\nputationally costly. Furthermore, the final performance of\nthe model falls short of expectations.\nDrawing insights from recent works (Zhang et al. 2020;\nChen et al. 2024; Tong et al. 2024), we recognize that the\nself-attention layer can assimilate image features as prior\nprompts, thus eliminating the necessity of cross-attention for\nthe obligatory incorporation of image features. In alignment\nwith this perspective, we embark on exploratory research.\nReferencing Figure 3(a), we are inspired by the prevalent\nControlNet (Zhang, Rao, and Agrawala 2023) architecture.\nThe operation of a specific layer can be succinctly expressed\nas follows:\nXout = ϕfl (Xin) +G(ϕil(Xin)), (1)\nwhere ϕfl and ϕil denote the frozen language model (LM)\nlayer and the insertion layer, respectively.\nHere, Xin represents the multimodal input, Xout denotes\nthe multimodal output, and G indicates a gating layer ini-\ntialized at zero. The insertion layer is a transformer decoder\nlayer, comprising the self-attention layer, layer normaliza-\ntion, feed forward network, etc. It is consistent with the pa-\nrameter scale of a transformer layer in the large language\nmodel. For instance, if we target the 22th layer, the ini-\ntial parameters of the corresponding insertion layer are de-\nrived from the 22th language model layer. Nonetheless, the\nControlNet-based design did not yield satisfactory perfor-\nmance.\nReferring to Figure 3(b), we endeavor to refine the Con-\ntrolNet structure. Specifically, we eliminate the feature prop-\nagation between insertion layers. Instead, the output of the\nLM layer serves as the input to the insertion layer. Our ex-\npectation is that each frozen LM layer will accommodate\nmultimodal data through a distinct insertion layer and gat-\ning layer, with the insertion layer no longer being directly\ninfluenced by subsequent layers. Compared to the design in\n21037\nFigure 3: Structural exploration of the Inner-Adaptor Architecture. Figure (a) is a architecture inspired by the ControlNet design;\nFigure (b) is an improvement on Figure (a), mainly canceling the feature propagation between adaptors; Figure (c) is the final\nscheme.\nConfigurations Satge1-PT Satge2-PT\nTrainable modules Projector Projector, Inner-adaptor\nLearning rate 1e-3 2e-5\nBatch size 256\nLR schedule Cosine decay\nTraining steps 2.5K\nZero-Stage Zero2\nWarmup ratio 0.03\nWeight decay 0.0\nOptimizer AdamW\nOptimizer HPs β1 = 0.9,β2 = 0.98,ϵ = 1e − 6\nConfigurations Instruction-FT Grounding-FT\nTrainable modules Projector, Inner-adaptor\nLearning rate 2e-5\nBatch size 128\nLR schedule Cosine decay\nTraining steps 6.6K 18K\nZero-Stage Zero3\nWarmup ratio 0.03\nWeight decay 0.0\nOptimizer AdamW\nOptimizer HPs β1 = 0.9,β2 = 0.98,ϵ = 1e − 6\nTable 1: The hyperparameters utilized during the training\nphase are delineated as follows: ”-PT” designates the pre-\ntraining phase, ”-FT” denotes the fine-tuning phase, and\n”HP” and ”LR” signify the hyperparameter and learning\nrate, respectively.\nFigure 3(a), the refined architecture shows significant im-\nprovements.\nMoreover, we hypothesize that the gating layer may not\nreach an optimal state through a single round of data train-\ning. Consequently, we propose a more streamlined solution,\nas illustrated in Figure 3(c). The operation of a specific layer\nwithin the model can be represented as follows:\nXout = ϕil(ϕfl (Xin)). (2)\nSimilar to Scheme (a), if an insertion layer is placed after\nthe 22th LM layer, it is initialized from the parameters of\nthe 22th frozen LM layer. The number of insertion layers is\nadjustable.\nAdditionally, for multimodal training, we introduce a new\nembedding layer ELmm and a new LM head LHmm, ini-\ntialized from the original language model’s embedding layer\nELtext and LM head LHtext. Throughout all stages of mul-\ntimodal training, ELtext and LHtext will remain frozen,\nwhile the newly created components will be trained with\nmultimodal data. The experimental results presented in Ta-\nble 5 validate the effectiveness of this strategy.\nWe thoroughly explore the distinctions among these ar-\nchitectures and strategies in the ablation study. Ultimately,\nwe select the structure depicted in Figure 3(c), which we\ndesignate as the Inner-Adaptor Architecture (IAA).\nExperiments\nTraining Paradigm\nPre-training. During the training process of MLLM, the\nprimary objective of the pre-training phase is to enable\nMLLM to learn the alignment between visual cues and tex-\ntual descriptions. This stage, also known as the image-text\nalignment phase, establishes connections between the vision\nencoder and LLM. In our architectural design, the image en-\ncoder and LLM remain frozen throughout all training phases\nto preserve the inherent foundational knowledge in both vi-\nsion and language models. The projector and inner-adapter\narchitecture require training to enhance multimodal capabil-\nities. Our empirical investigations reveal that for the inner-\nadaptor architecture, applying a high learning rate can lead\nto overflow in training loss. To alleviate this issue, we devise\na dual-stage pre-training procedure.\nIn the first pre-training stage, the model configuration\nconsists of only three components: the image encoder, the\nprojector, and the large language model. The parameters of\nthe image encoder and the large language model are frozen,\nwhile a high learning rate of 0.001 is utilized to train a high-\nquality projector.\nIn the second pre-training stage, the model architecture\nis expanded to incorporate the inner-adaptor for multimodal\ntasks. The training parameters now include both the pro-\njector and the newly integrated structures. The projector is\ninitialized with the parameters derived from the preceding\nstage. For this stage, a lower learning rate of 2e-5 is adopted.\nThroughout the pre-training stages, the dataset employed\nconsists of 558k image-text aligned pairs sourced from (Liu\net al. 2024b) and an additional 100K pairs from (Chen et al.\n21038\nMethod Vision Encoder\nLanguage Model Data Scale MME P MMB-ENT MMB-CNT MMMUv\nTraining with\nthe LLM unfrozen\nmPLUG-Owl(Ye et al.\n2023) CLIP-ViT-L Llama2 (7B) 1.1B 967.3 49.4 - -\nQwen-VL-Chat (Bai et al. 2023) CLIP-ViT-G Qwen (7B) 1.5B 1487.6 61.8 56.3 37\nCogVLM (Wang et al. 2023) EV A2-CLIP-ViT-E Vicuna-v1.5 (7B) 1.5B 1439.7 65.8 55.9 37.3\nmPLUG-Owl2 (Ye et al. 2024) CLIP-ViT-L Llama2 (7B) 400M 1450.2 66.0 60.3 34.7\nLLaV A-1.5(Liu et al. 2024b) CLIP-ViT-L Vicuna-v1.5 (7B) 1.2M 1510.7 66.5 59.0 35.7\nLLaV A-1.5(Liu et al. 2024b) CLIP-ViT-L Vicuna-v1.5 (13B) 1.2M 1531.3 69.2 65.0 37.0\nHoneybee (Cha et al. 2024) CLIP-ViT-L Vicuna-v1.5 (7B) 208M 1584.2 70.1 - -\nYi-VL (AI et al. 2024) CLIP-ViT-H Yi (6B) 125M - 68.4 66.6 39.1\nDeepSeek-VL (Lu et al. 2024) SAM-B and SigLIP-L DeepSeek (7B) 103M - 73.8 71.4 36.6\nLLaV A-Llama3 (Contributors 2024) CLIP-ViT-L Llama3 (8B) 1.2M 1506.0 68.9 61.6 36.8\nTraining with\nthe LLM frozen\nOpenFlamingov2 (Aw\nadalla et al. 2023) CLIP-ViT-L MPT (7B) 3B - 5.7 14.4 28.8\nLlama-AdapterV2(Gao et al. 2023) CLIP-ViT-L Llama2 (7B) 0.6M 972.7 41.0 - -\nMiniGPT-4 (Zhu et al. 2023) EV A-CLIP-ViT-G Vicuna (13B) 5.1M 866.6 - - -\nBLIP-2 (Li et al. 2023a) EV A-CLIP-ViT-G FlanT5XXL 129M 1293.8 - - -\nInstructBLIP (Dai et al. 2023) EV A-CLIP-ViT-G Vicuna (13B) 130M 1212.8 44.0 - -\nIAA-8† CLIP-ViT-L Llama3 (8B) 1.2M 1560.2 69.9 64.2 37.1\nIAA-8 CLIP-ViT-L Llama3 (8B) 1.5M 1581.8 72.7 69.2 39.8\nIAA-14 CLIP-ViT-L Llama3 (8B) 1.5M 1591.5 74.9 70.5 39.9\nTable 2: Results on general multimodal benchmarks, where the data scale of 1.2M uniformly represents the data provided by\nLLaV A (Liu et al. 2024b). IAA-8† represents the model trained using 1.2M data.\nMethod MMLU↑ C-Eval ↑ BBH\n↑ Humaneval ↑ Math ↑\nLLaV A-Llama3 55.8 40.5\n44.6 38.4 12.3\nIAA-8† 68.4 51.3 52.8 59.2 27.8\nTable 3: Comparison on Text-only Benchmarks. IAA-8† de-\nnotes the model trained using the same 1.2M data as LLaV A-\nLlama3. IAA-8† is not impaired in terms of NLP ability, but\nLLaV A-Llama3 presents deteriorated results.\n2024). (Chen et al. 2024) provides a total of 664K image-text\naligned data. We translate the first 100k pairs into Chinese\nand incorporated them into the training process to fortify the\nmodel’s understanding of Chinese tasks. Over the course of\nthese stages, we utilize a cumulative total of 658K data pairs.\nInstruction Fine-tuning. We perform instruction fine-\ntuning based on the model obtained from the second pre-\ntraining stage. Throughout this stage, the parameters of the\nlarge language model and the image encoder remain frozen.\nThe dataset includes the fine-tuning dataset of 665K sam-\nples proposed by (Liu et al. 2024b), along with additional\ndatasets including DocVQA (50K) (Mathew, Karatzas, and\nJawahar 2021), VSR (10K) (Liu, Emerson, and Collier\n2023), ScienceQA (21K) (Lu et al. 2022), and an in-house\ndataset (78.5K). Similar to the pre-training stage, we trans-\nlate the first 40K entries of the 664K fine-tuning data pro-\nposed by (Chen et al. 2024) into Chinese and incorporate\nthem into the instruction fine-tuning dataset. The aggregate\nquantity of data utilized in this stage amounts to 865K.\nGrounding Fine-tuning. Building upon the model fine-\ntuned with instructions, we further train a model special-\nized in visual grounding. The data used in this stage com-\nprises RefCOCO (Kazemzadeh et al. 2014), COCO (Lin\net al. 2014), Flickr30k Entities (Plummer et al. 2015), Ob-\njects365 (Shao et al. 2019), aggregating to approximately\n2M data instances. These datasets improves the model’s ca-\npability of localizing fine-grained visual details. The inclu-\nsion of COCO and Objects365 assists the model in improv-\ning its ability to localize multiple targets.\nImplementation details. The detailed training informa-\ntion is summarized in Table 1, mainly covering the hyper-\nparameters used during the four-stage training process. The\nentire four-stage can be executed on a single node A800×8\nin 48 hours. All experiments utilize the zero technology pro-\nvided by Deepspeed and the flash-attention v2.\nExperimental Results\nMain Results on General Multimodal Benchmarks. To\nassess the multimodal capabilities of our approach, we em-\nploy widely recognized benchmarks that are closely related\nto multimodal tasks: MME P (Fu et al. 2023), MMBench-\nENT (Liu et al. 2023), MMBench-CNT (Liu et al. 2023), and\nMMMUv (Yue et al. 2024). These benchmarks are renowned\nfor presenting significant challenges across a diverse range\nof practical tasks. For evaluation purposes, we adhere to a\nzero-shot testing protocol, a strict methodology that tests\nmodels on unseen data without additional training. More-\nover, we categorize comparative methods into two distinct\ncategories: those trained with a frozen language model and\n21039\nMethod Grounding Data Scale RefCOCO RefCOCO+ RefCOCOg\nval testA testB val testA testB val test\nKOSMOS-2 (Peng et al. 2023) 20M 52.3 57.4 47.3 45.5 50.7 42.2 60.6 61.7\nOFA-L (Wang et al. 2022) 10M 80.0 83.7 76.4 68.3 76.0 61.8 67.6 67.6\nShikra (Chen et al. 2023b) 4M 87.0 90.6 80.2 81.6 87.4 72.1 82.3 82.2\nMiniGPT-v2 (Chen et al. 2023a) ∼21M 88.7 91.7 85.3 80.0 85.1 74.5 84.4 84.7\nFerret (You et al. 2023) 8.7M 87.5 91.4 82.5 80.1 87.4 73.1 83.9 84.8\nPINK (Xuan et al. 2024) 5M 88.7 92.1 84.0 81.8 88.2 73.9 83.9 84.3\nIAA-8 2M 89.2 92.6 83.7 82.1 88.6 73.7 84.4 84.7\nIAA-14 2M 90.2 92.9 85.4 83.4 89.0 76.7 85.0 85.1\nTable 4: Comparisons on visual grounding benchmarks. Our approach achieves competitive performance trained on relatively\nlimited datasets.\nthose trained with an unfrozen language model. To provide\na comprehensive analysis, we show the scale of the data uti-\nlized for each method, along with the variations in the im-\nage encoders employed. Detailed results of our evaluations\nare tabulated in Table 2. To ensure a fair and equitable com-\nparison, we choose methods that leverage a base language\nmodel with a comparable parameter scale, and the reported\nmetrics for competing methods are based solely on officially\npublished data, avoiding any local testing results.\nOwing to the inherent strengths of our proposed architec-\nture, our method exhibits substantial superiority over those\ntrained with frozen language model. As the current main-\nstream approach, models trained with unfrozen language\nmodels typically achieve better multimodal performance,\nalbeit at the cost of diminished NLP capabilities. We list\nseveral state-of-the-art methods adhering to this training\nparadigm. Compared to Honeybee (Cha et al. 2024), Yi-\nVL (AI et al. 2024), and Deepseek-VL (Lu et al. 2024),\nour method achieves competitive or even superior perfor-\nmance on certain metrics, with an extremely small training\ndata scale. Using the same data scale of 1.2 million, IAA-8\noutperforms LLaV A-Llama3. Additionally, IAA-14 with 14\ninsertion layers achieves better results than IAA-8 with an 8-\nlayer configuration. Furthermore, we compare our approach\nwith LLaV A-Llama3 (Contributors 2024) on NLP bench-\nmarks, including MMLU and C-Eval. The results of NLP\nbenchmarks are presented in Table 3. Our language model\nis not impaired in terms of NLP ability, but LLaV A-Llama3\ntrained on the same data shows deteriorated results on both\nMMLU and C-Eval. Our method surpasses it across all met-\nrics, indicating that our architecture is superior to the main-\nstream LLaV A architecture.\nResults on Visual Grounding Benchmarks. To evalu-\nate the effectiveness of our model in the visual ground-\ning task, we perform evaluations utilizing the widely ac-\ncepted benchmarks RefCOCO (Kazemzadeh et al. 2014),\nRefCOCO+ (Yu et al. 2016), and RefCOCOg (Mao et al.\n2016), with the corresponding results illustrated in Table 4.\nThe methods for comparison are all models trained for the\ngrounding task under an auto-regressive strategy. The re-\nsults reveal that our method is capable of achieving com-\npetitive performance, even when trained on relatively lim-\nited datasets. In our analysis, to ensure fairness, we exclude\nmodels trained on extremely large-scale datasets, such as\nCogVLM-grounding (Wang et al. 2023) with 1.5B image-\ntext pairs and 40M grounding data, as well as those lever-\naging pre-trained object detection models, exemplified by\nLLaV A-Grounding (Zhang et al. 2023) and Groma (Ma et al.\n2024).\nEfficiency in Deployment. Currently, high-performance\nmultimodal models typically require the unfreezing of the\nlarge language model for training. CogVLM (Wang et al.\n2023) highlights the substantial difficulty in developing a\nmodel that excels in both multimodal comprehension and\nvisual grounding tasks simultaneously. To address this, it\nadopts a dual-model strategy, specifically training one model\nfor general multimodal capabilities and another for visual\ngrounding abilities. In this context, deploying a high-quality\nlanguage model, a multimodal model with outstanding gen-\neral performance, and a model endowed with proficient vi-\nsual grounding skills concurrently on a single GPU would\ndemand an estimated 50GB of memory. Our proposed ap-\nproach, facilitated by the inner-adaptor architecture, inge-\nniously combines superior general multimodal competen-\ncies and robust visual grounding capacities, while concur-\nrently safeguarding the inherent prowess of the original large\nlanguage model. Specifically, with an 8-layer inner-adaptor\nconfiguration, our model exhibits a significantly reduced\nmemory footprint, hovering around 30GB.\nAblation Study\nStructure Analysis. In the exploration of the structure,\nwe furnish quantitative results for validation in Table 5.\nWith an 8-layer insertion scheme as our baseline configu-\nration, we observe that incremental architectural enhance-\nments consistently improve performance metrics across the\nboard. Specifically, the comparison between rows 1, 2, and\n4 highlights the benefits of architectural refinement. More-\nover, the contrast between rows 3 and 4 demonstrates that\nthe integration of a specialized embedding layer and lan-\nguage model head for multimodal data processing signifi-\ncantly boosts performance.\n21040\nModel architecture Trainable modules MMEP MMB-ENT MMB-CNT MMMUv\nProjector I-Layers(8) ELmm LHmm Zero-Gates\nFigure 3(a) ✓ ✓ ✓ ✓ ✓ 1425.4 72.4 65.0 38.2\nFigure 3(b) ✓ ✓ ✓ ✓ ✓ 1556.0 72.7 68.5 39.6\nFigure 3(c) ✓ ✓ × × × 1563.4 72.6 68.7 39.6\nFigure 3(c) ✓ ✓ ✓ ✓ × 1581.8 72.7 69.2 39.8\nTable 5: Ablation study for the exploration of inner-adaptor related structures.\nTraining Stages MMEP MMMUv\nSatge1-P Satge2-P Instruction-F\n× ✓ ✓ 1512.1 39.3\n✓ × ✓ 1565.4 39.5\n✓ ✓ ✓ 1581.8 39.8\nTable 6: Comparison of the training stages.\nNumber of I-Layers MME P MMB-ENT MMB-CNT\n8 1581.8 72.7 69.2\n14 1591.5 74.9 70.5\n22 1531.7 76.0 70.4\nTable 7: Ablations on the number of insertion layers.\nComparison of Training Stages. Through empirical ev-\nidence detailed in Table 6, we validate the effectiveness of\nour two-stage pre-training methodology. It can be observed\nthat the model lacking the first stage of alignment training\nexhibits notably poorer performance. When the projector\nand insertion layers are engaged in joint pre-training, it is es-\nsential to maintain a learning rate of approximately 2e-5 to\nprevent loss overflow. However, this strategy leads to subop-\ntimal alignment training for the projector, which negatively\naffects the model’s final performance.\nFurthermore, although the model performs adequately\nwhen skipping the second pre-training stage, it ultimately\nfails to replicate the outstanding results achievable through\nthe complete two-stage pre-training process. This disparity\nemphasizes the critical significance of the additional pre-\ntraining stage in enhancing the model’s overall effectiveness.\nImpact of Insertion Layer Quantities. We explore the\neffect of varying numbers of insertion layers, which are pre-\nsented in Table 7. The experimental results indicate that in-\ncreasing the number of insertion layers from 8 to 14 yields\nenhancements in all performance metrics. However, it is im-\nperative to acknowledge that an increase in insertion layers\nsimultaneously impacts the model’s efficiency. We advocate\nthat an 8-layer configuration is adequate to effectively ad-\ndress foundational requirements.\nTraining Data Influence Assessment. To delineate the\nimpact of data on model performance, we present compar-\native results in Table 8. The baseline, outlined in the first\nrow, showcases the performance of LLaV A-Llama3 (Con-\ntributors 2024) utilizing the LLaV A architecture and the 1.2\nmillion dataset provided by (Liu et al. 2024b). Subsequent\nMMEP MMB-ENT MMB-CNT\nLLaV A-Llama3 (1.2M) 1506.0\n68.9 61.6\nIAA-8 (1.2M) 1560.2 69.9 64.2\nIAA-8 (1.5M) 1581.8 72.7 69.2\nTable 8: The impact of the training data.\nexperimentation, as delineated in the second row, empha-\nsizes the pronounced superiority of our proposed architec-\nture over LLaV A. Additionally, we enrich the training cor-\npus with an extra 0.3 million records, mainly encompassing\nChinese data. As a result, our model achieves substantial im-\nprovements in all metrics, especially on the Chinese evalua-\ntion set MMBench-CNT.\nLimitations The method of extending multimodal capa-\nbilities by freezing the language model will introduce certain\nadditional parameters. Compared to the approach of training\nwith an unfrozen language model, the inference speed of the\nmodel will be reduced. To mitigate this issue, we extend the\nkey-value cache mechanism to the insertion layers. Based\non the MME dataset, compared to the LLaV A architecture,\nthe average inference time of our 8-layer structure increases\nfrom 0.103s to 0.124s, which we deem to be within a rela-\ntively reasonable range.\nConclusion\nIn this paper, we introduce the Inner-Adaptor Architecture,\nwhich is designed to enhance the general multimodal and\nvisual grounding capabilities of LLMs. Through a series of\narchitectural exploration experiments, we demonstrate that\ntraining with a frozen language model can surpass the mul-\ntimodal performance of the models with fine-tuned LLMs.\nOur proposed model has achieved state-of-the-art perfor-\nmance across a multitude of publicly available evaluation\ndatasets. Moreover, after deployment, our approach incor-\nporates dual workflows, thereby preserving the NLP profi-\nciency of the language model. The flexibility of the Inner-\nAdaptor Architecture provides the potential for extension to\nadditional modalities, which is a direction for future explo-\nration.\nReferences\nAchiam, J.; Adler, S.; Agarwal, S.; Ahmad, L.; Akkaya,\nI.; et al. 2023. Gpt-4 technical report. arXiv preprint\narXiv:2303.08774.\nAI, .; :; Young, A.; Chen, B.; Li, C.; Huang, C.; Zhang, G.;\nZhang, G.; Li, H.; Zhu, J.; Chen, J.; Chang, J.; Yu, K.; Liu,\n21041\nP.; Liu, Q.; Yue, S.; Yang, S.; Yang, S.; Yu, T.; Xie, W.;\nHuang, W.; Hu, X.; Ren, X.; Niu, X.; Nie, P.; Xu, Y .; Liu,\nY .; Wang, Y .; Cai, Y .; Gu, Z.; Liu, Z.; and Dai, Z. 2024. Yi:\nOpen Foundation Models by 01.AI. arXiv:2403.04652.\nAlayrac, J.-B.; Donahue, J.; Luc, P.; Miech, A.; Barr, I.; Has-\nson, Y .; Lenc, K.; Mensch, A.; Millican, K.; Reynolds, M.;\net al. 2022. Flamingo: a visual language model for few-shot\nlearning. In Advances in neural information processing sys-\ntems, volume 35, 23716–23736.\nAwadalla, A.; Gao, I.; Gardner, J.; Hessel, J.; Hanafy, Y .;\nZhu, W.; Marathe, K.; Bitton, Y .; Gadre, S.; Sagawa, S.; Jit-\nsev, J.; Kornblith, S.; Koh, P. W.; Ilharco, G.; Wortsman, M.;\nand Schmidt, L. 2023. OpenFlamingo: An Open-Source\nFramework for Training Large Autoregressive Vision-\nLanguage Models. arXiv preprint arXiv:2308.01390.\nBai, J.; Bai, S.; Yang, S.; Wang, S.; Tan, S.; Wang, P.;\nLin, J.; Zhou, C.; and Zhou, J. 2023. Qwen-vl: A frontier\nlarge vision-language model with versatile abilities. arXiv\npreprint arXiv:2308.12966.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language models are few-shot learners.\nIn Advances in neural information processing systems, vol-\nume 33, 1877–1901.\nCha, J.; Kang, W.; Mun, J.; and Roh, B. 2024. Honeybee:\nLocality-enhanced projector for multimodal llm. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 13817–13827.\nChen, G. H.; Chen, S.; Zhang, R.; Chen, J.; Wu, X.; Zhang,\nZ.; Chen, Z.; Li, J.; Wan, X.; and Wang, B. 2024. Allava:\nHarnessing gpt4v-synthesized data for a lite vision-language\nmodel. arXiv preprint arXiv:2402.11684.\nChen, J.; Zhu, D.; Shen, X.; Li, X.; Liu, Z.; Zhang, P.; Kr-\nishnamoorthi, R.; Chandra, V .; Xiong, Y .; and Elhoseiny, M.\n2023a. Minigpt-v2: large language model as a unified inter-\nface for vision-language multi-task learning. arXiv preprint\narXiv:2310.09478.\nChen, K.; Zhang, Z.; Zeng, W.; Zhang, R.; Zhu, F.; and\nZhao, R. 2023b. Shikra: Unleashing multimodal llm’s refer-\nential dialogue magic. arXiv preprint arXiv:2306.15195.\nContributors, X. 2024. XTuner: A Toolkit for Efficiently\nFine-tuning LLM. https://github.com/InternLM/xtuner.\nDai, W.; Li, J.; Li, D.; Tiong, A.; Zhao, J.; Wang, W.; Li, B.;\nFung, P.; and Hoi, S. 2023. InstructBLIP: Towards General-\npurpose Vision-Language Models with Instruction Tuning.\narXiv preprint arXiv:2305.06500.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBert: Pre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805.\nFu, C.; Chen, P.; Shen, Y .; Qin, Y .; Zhang, M.; Lin, X.; Yang,\nJ.; Zheng, X.; Li, K.; Sun, X.; et al. 2023. MME: A Compre-\nhensive Evaluation Benchmark for Multimodal Large Lan-\nguage Models. arXiv preprint arXiv:2306.13394.\nGao, P.; Han, J.; Zhang, R.; Lin, Z.; Geng, S.; Zhou, A.;\nZhang, W.; Lu, P.; He, C.; Yue, X.; et al. 2023. Llama-\nadapter v2: Parameter-efficient visual instruction model.\narXiv preprint arXiv:2304.15010.\nHendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika,\nM.; Song, D.; and Steinhardt, J. 2020. Measuring mas-\nsive multitask language understanding. arXiv preprint\narXiv:2009.03300.\nHuang, S.; Dong, L.; Wang, W.; Hao, Y .; Singhal, S.; Ma, S.;\nLv, T.; Cui, L.; Mohammed, O. K.; Patra, B.; et al. 2023a.\nLanguage is not all you need: Aligning perception with lan-\nguage models. Advances in Neural Information Processing\nSystems, 36: 72096–72109.\nHuang, Y .; Bai, Y .; Zhu, Z.; Zhang, J.; Zhang, J.; Su, T.;\nLiu, J.; Lv, C.; Zhang, Y .; Lei, J.; Fu, Y .; Sun, M.; and He,\nJ. 2023b. C-Eval: A Multi-Level Multi-Discipline Chinese\nEvaluation Suite for Foundation Models. In Advances in\nNeural Information Processing Systems.\nJiang, A. Q.; Sablayrolles, A.; Roux, A.; Mensch, A.;\nSavary, B.; Bamford, C.; Chaplot, D. S.; Casas, D. d. l.;\nHanna, E. B.; Bressand, F.; et al. 2024. Mixtral of experts.\narXiv preprint arXiv:2401.04088.\nKazemzadeh, S.; Ordonez, V .; Matten, M.; and Berg, T.\n2014. Referitgame: Referring to objects in photographs\nof natural scenes. In Proceedings of the 2014 confer-\nence on empirical methods in natural language processing\n(EMNLP), 787–798.\nLaurenc ¸on, H.; Tronchon, L.; Cord, M.; and Sanh, V . 2024.\nWhat matters when building vision-language models? arXiv\npreprint arXiv:2405.02246.\nLi, J.; Li, D.; Savarese, S.; and Hoi, S. 2023a. Blip-2:\nBootstrapping language-image pre-training with frozen im-\nage encoders and large language models. In International\nconference on machine learning, 19730–19742. PMLR.\nLi, J.; Xie, C.; Wu, X.; Wang, B.; and Leng, D. 2023b. What\nmakes good open-vocabulary detector: A disassembling per-\nspective. arXiv preprint arXiv:2309.00227.\nLin, T.-Y .; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ra-\nmanan, D.; Doll ´ar, P.; and Zitnick, C. L. 2014. Microsoft\ncoco: Common objects in context. In Computer Vision–\nECCV 2014: 13th European Conference, Zurich, Switzer-\nland, September 6-12, 2014, Proceedings, Part V 13, 740–\n755. Springer.\nLiu, F.; Emerson, G.; and Collier, N. 2023. Visual spatial\nreasoning. Transactions of the Association for Computa-\ntional Linguistics, 11: 635–651.\nLiu, H.; Li, C.; Li, Y .; and Lee, Y . J. 2024a. Improved\nbaselines with visual instruction tuning. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 26296–26306.\nLiu, H.; Li, C.; Wu, Q.; and Lee, Y . J. 2024b. Visual instruc-\ntion tuning. In Advances in neural information processing\nsystems, volume 36.\nLiu, Y .; Duan, H.; Zhang, Y .; Li, B.; Zhang, S.; Zhao, W.;\nYuan, Y .; Wang, J.; He, C.; Liu, Z.; et al. 2023. Mmbench:\nIs your multi-modal model an all-around player? arXiv\npreprint arXiv:2307.06281.\nLu, H.; Liu, W.; Zhang, B.; Wang, B.; Dong, K.; Liu, B.;\nSun, J.; Ren, T.; Li, Z.; Sun, Y .; et al. 2024. Deepseek-vl:\ntowards real-world vision-language understanding. arXiv\npreprint arXiv:2403.05525.\n21042\nLu, P.; Mishra, S.; Xia, T.; Qiu, L.; Chang, K.-W.; Zhu, S.-\nC.; Tafjord, O.; Clark, P.; and Kalyan, A. 2022. Learn to\nexplain: Multimodal reasoning via thought chains for sci-\nence question answering. Advances in Neural Information\nProcessing Systems, 35: 2507–2521.\nMa, C.; Jiang, Y .; Wu, J.; Yuan, Z.; and Qi, X. 2024. Groma:\nLocalized Visual Tokenization for Grounding Multimodal\nLarge Language Models. arXiv preprint arXiv:2404.13013.\nMao, J.; Huang, J.; Toshev, A.; Camburu, O.; Yuille, A. L.;\nand Murphy, K. 2016. Generation and comprehension of un-\nambiguous object descriptions. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, 11–\n20.\nMathew, M.; Karatzas, D.; and Jawahar, C. 2021. Docvqa: A\ndataset for vqa on document images. In Proceedings of the\nIEEE/CVF winter conference on applications of computer\nvision, 2200–2209.\nMeta. 2024. Introducing Meta Llama 3: The most capable\nopenly available LLM to date. Technical report.\nPeng, Z.; Wang, W.; Dong, L.; Hao, Y .; Huang, S.; Ma,\nS.; and Wei, F. 2023. Kosmos-2: Grounding multimodal\nlarge language models to the world. arXiv preprint\narXiv:2306.14824.\nPlummer, B. A.; Wang, L.; Cervantes, C. M.; Caicedo, J. C.;\nHockenmaier, J.; and Lazebnik, S. 2015. Flickr30k enti-\nties: Collecting region-to-phrase correspondences for richer\nimage-to-sentence models. In Proceedings of the IEEE in-\nternational conference on computer vision, 2641–2649.\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;\net al. 2021. Learning transferable visual models from nat-\nural language supervision. In International conference on\nmachine learning, 8748–8763. PMLR.\nShao, S.; Li, Z.; Zhang, T.; Peng, C.; Yu, G.; Zhang, X.;\nLi, J.; and Sun, J. 2019. Objects365: A large-scale, high-\nquality dataset for object detection. In Proceedings of\nthe IEEE/CVF international conference on computer vision,\n8430–8439.\nTong, S.; Brown, E.; Wu, P.; Woo, S.; Middepogu, M.;\nAkula, S. C.; Yang, J.; Yang, S.; Iyer, A.; Pan, X.; et al.\n2024. Cambrian-1: A fully open, vision-centric exploration\nof multimodal llms. arXiv preprint arXiv:2406.16860.\nWang, P.; Yang, A.; Men, R.; Lin, J.; Bai, S.; Li, Z.; Ma, J.;\nZhou, C.; Zhou, J.; and Yang, H. 2022. Ofa: Unifying archi-\ntectures, tasks, and modalities through a simple sequence-to-\nsequence learning framework. In International conference\non machine learning, 23318–23340. PMLR.\nWang, W.; Lv, Q.; Yu, W.; Hong, W.; Qi, J.; Wang, Y .; Ji,\nJ.; Yang, Z.; Zhao, L.; Song, X.; et al. 2023. Cogvlm: Vi-\nsual expert for pretrained language models. arXiv preprint\narXiv:2311.03079.\nXie, C.; Cai, H.; Li, J.; Kong, F.; Wu, X.; Song, J.; Morim-\nitsu, H.; Yao, L.; Wang, D.; Zhang, X.; et al. 2023. CCMB:\nA Large-scale Chinese Cross-modal Benchmark. In Pro-\nceedings of the 31st ACM International Conference on Mul-\ntimedia, 4219–4227.\nXuan, S.; Guo, Q.; Yang, M.; and Zhang, S. 2024. Pink:\nUnveiling the power of referential comprehension for multi-\nmodal llms. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 13838–13848.\nYang, A.; Yang, B.; Hui, B.; Zheng, B.; Yu, B.; and oth-\ners. 2024. Qwen2 Technical Report. arXiv preprint\narXiv:2407.10671.\nYe, Q.; Xu, H.; Xu, G.; Ye, J.; Yan, M.; Zhou, Y .; Wang, J.;\nHu, A.; Shi, P.; Shi, Y .; Jiang, C.; Li, C.; Xu, Y .; Chen, H.;\nTian, J.; Qi, Q.; Zhang, J.; and Huang, F. 2023. mPLUG-\nOwl: Modularization Empowers Large Language Models\nwith Multimodality. arXiv:2304.14178.\nYe, Q.; Xu, H.; Ye, J.; Yan, M.; Hu, A.; Liu, H.; Qian, Q.;\nZhang, J.; and Huang, F. 2024. mplug-owl2: Revolution-\nizing multi-modal large language model with modality col-\nlaboration. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 13040–13051.\nYou, H.; Zhang, H.; Gan, Z.; Du, X.; Zhang, B.; Wang, Z.;\nCao, L.; Chang, S.-F.; and Yang, Y . 2023. Ferret: Refer and\nground anything anywhere at any granularity.arXiv preprint\narXiv:2310.07704.\nYu, L.; Poirson, P.; Yang, S.; Berg, A. C.; and Berg, T. L.\n2016. Modeling context in referring expressions. In Com-\nputer Vision–ECCV 2016: 14th European Conference, Am-\nsterdam, The Netherlands, October 11-14, 2016, Proceed-\nings, Part II 14, 69–85. Springer.\nYue, X.; Ni, Y .; Zhang, K.; Zheng, T.; Liu, R.; Zhang, G.;\nStevens, S.; Jiang, D.; Ren, W.; Sun, Y .; et al. 2024. Mmmu:\nA massive multi-discipline multimodal understanding and\nreasoning benchmark for expert agi. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 9556–9567.\nZhai, X.; Mustafa, B.; Kolesnikov, A.; and Beyer, L. 2023.\nSigmoid loss for language image pre-training. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision, 11975–11986.\nZhang, H.; Li, H.; Li, F.; Ren, T.; Zou, X.; Liu, S.; Huang,\nS.; Gao, J.; Zhang, L.; Li, C.; et al. 2023. Llava-grounding:\nGrounded visual chat with large multimodal models. arXiv\npreprint arXiv:2312.02949.\nZhang, J. O.; Sax, A.; Zamir, A.; Guibas, L.; and Malik, J.\n2020. Side-tuning: a baseline for network adaptation via ad-\nditive side networks. In Computer Vision–ECCV 2020: 16th\nEuropean Conference, Glasgow, UK, August 23–28, 2020,\nProceedings, Part III 16, 698–714. Springer.\nZhang, L.; Rao, A.; and Agrawala, M. 2023. Adding Condi-\ntional Control to Text-to-Image Diffusion Models. In IEEE\nInternational Conference on Computer Vision (ICCV).\nZhang, S.; Roller, S.; Goyal, N.; Artetxe, M.; Chen, M.;\nChen, S.; Dewan, C.; Diab, M.; Li, X.; Lin, X. V .; et al. 2022.\nOpt: Open pre-trained transformer language models. arXiv\npreprint arXiv:2205.01068.\nZhu, D.; Chen, J.; Shen, X.; Li, X.; and Elhoseiny, M.\n2023. Minigpt-4: Enhancing vision-language understand-\ning with advanced large language models. arXiv preprint\narXiv:2304.10592.\n21043"
}