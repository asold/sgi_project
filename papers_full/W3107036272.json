{
  "title": "MaX-DeepLab: End-to-End Panoptic Segmentation with Mask Transformers",
  "url": "https://openalex.org/W3107036272",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2336363575",
      "name": "Wang Huiyu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2350672630",
      "name": "Zhu, Yukun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2346607959",
      "name": "Adam, Hartwig",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2893462076",
      "name": "Yuille, Alan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4283440904",
      "name": "Chen, Liang-Chieh",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2982106100",
    "https://openalex.org/W2970803838",
    "https://openalex.org/W3120633509",
    "https://openalex.org/W2983446232",
    "https://openalex.org/W3106250896",
    "https://openalex.org/W2964080601",
    "https://openalex.org/W2558580397",
    "https://openalex.org/W2991405684",
    "https://openalex.org/W2963351448",
    "https://openalex.org/W2990032492",
    "https://openalex.org/W3034975706",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W2557889580",
    "https://openalex.org/W2981537222",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2994689640",
    "https://openalex.org/W2911831070",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2630837129",
    "https://openalex.org/W2962891704",
    "https://openalex.org/W2910628332",
    "https://openalex.org/W2998228095",
    "https://openalex.org/W2966156373",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W2798991696",
    "https://openalex.org/W2124260943",
    "https://openalex.org/W2964121718",
    "https://openalex.org/W2893299523",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963136578",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2981413347",
    "https://openalex.org/W2414711238",
    "https://openalex.org/W3105238007",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2565639579",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2902499724",
    "https://openalex.org/W2809110088",
    "https://openalex.org/W2164877691",
    "https://openalex.org/W2964137095",
    "https://openalex.org/W3034355852",
    "https://openalex.org/W2970107519",
    "https://openalex.org/W2962914239",
    "https://openalex.org/W2964350391",
    "https://openalex.org/W1836465849",
    "https://openalex.org/W3100039191",
    "https://openalex.org/W2981689412",
    "https://openalex.org/W2965182628",
    "https://openalex.org/W2601564443",
    "https://openalex.org/W2549139847",
    "https://openalex.org/W2963236837",
    "https://openalex.org/W2531409750",
    "https://openalex.org/W2307770531",
    "https://openalex.org/W2981899103",
    "https://openalex.org/W2895340641",
    "https://openalex.org/W3106546328",
    "https://openalex.org/W2955058313",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W3035396860",
    "https://openalex.org/W2964091144",
    "https://openalex.org/W2964309882",
    "https://openalex.org/W2998108143",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W2997473137",
    "https://openalex.org/W22745672",
    "https://openalex.org/W2964288706",
    "https://openalex.org/W2895065325",
    "https://openalex.org/W2963857746",
    "https://openalex.org/W2885201931",
    "https://openalex.org/W2963393688",
    "https://openalex.org/W2222512263",
    "https://openalex.org/W2963043051",
    "https://openalex.org/W2963350373",
    "https://openalex.org/W2987761193",
    "https://openalex.org/W2963677766",
    "https://openalex.org/W2963775509",
    "https://openalex.org/W3033182847",
    "https://openalex.org/W3110002552",
    "https://openalex.org/W3104613728",
    "https://openalex.org/W3006033418",
    "https://openalex.org/W2331143823",
    "https://openalex.org/W3034978746",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W2412782625",
    "https://openalex.org/W3034971973",
    "https://openalex.org/W607748843",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W421393966",
    "https://openalex.org/W3100345210",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W3033529678"
  ],
  "abstract": "We present MaX-DeepLab, the first end-to-end model for panoptic segmentation. Our approach simplifies the current pipeline that depends heavily on surrogate sub-tasks and hand-designed components, such as box detection, non-maximum suppression, thing-stuff merging, etc. Although these sub-tasks are tackled by area experts, they fail to comprehensively solve the target task. By contrast, our MaX-DeepLab directly predicts class-labeled masks with a mask transformer, and is trained with a panoptic quality inspired loss via bipartite matching. Our mask transformer employs a dual-path architecture that introduces a global memory path in addition to a CNN path, allowing direct communication with any CNN layers. As a result, MaX-DeepLab shows a significant 7.1% PQ gain in the box-free regime on the challenging COCO dataset, closing the gap between box-based and box-free methods for the first time. A small variant of MaX-DeepLab improves 3.0% PQ over DETR with similar parameters and M-Adds. Furthermore, MaX-DeepLab, without test time augmentation, achieves new state-of-the-art 51.3% PQ on COCO test-dev set. Code is available at https://github.com/google-research/deeplab2.",
  "full_text": "MaX-DeepLab: End-to-End Panoptic Segmentation with Mask Transformers\nHuiyu Wang1* Yukun Zhu2 Hartwig Adam2 Alan Yuille1 Liang-Chieh Chen2\n1Johns Hopkins University 2Google Research\nCode: https://github.com/google-research/deeplab2\nAbstract\nWe present MaX-DeepLab, the ï¬rst end-to-end model for\npanoptic segmentation. Our approach simpliï¬es the cur-\nrent pipeline that depends heavily on surrogate sub-tasks\nand hand-designed components, such as box detection, non-\nmaximum suppression, thing-stuff merging, etc. Although\nthese sub-tasks are tackled by area experts, they fail to\ncomprehensively solve the target task. By contrast, our\nMaX-DeepLab directly predicts class-labeled masks with a\nmask transformer, and is trained with a panoptic quality in-\nspired loss via bipartite matching. Our mask transformer\nemploys a dual-path architecture that introduces a global\nmemory path in addition to a CNN path, allowing direct\ncommunication with any CNN layers. As a result, MaX-\nDeepLab shows a signiï¬cant 7.1% PQ gain in the box-free\nregime on the challenging COCO dataset, closing the gap\nbetween box-based and box-free methods for the ï¬rst time.\nA small variant of MaX-DeepLab improves 3.0% PQ over\nDETR with similar parameters and M-Adds. Furthermore,\nMaX-DeepLab, without test time augmentation, achieves\nnew state-of-the-art 51.3% PQ on COCO test-dev set.\n1. Introduction\nThe goal of panoptic segmentation [48] is to predict a\nset of non-overlapping masks along with their correspond-\ning class labels. Modern panoptic segmentation methods\naddress this mask prediction problem by approximating the\ntarget task with multiple surrogate sub-tasks. For exam-\nple, Panoptic-FPN [47] adopts a â€˜box-based pipelineâ€™ with\nthree levels of surrogate sub-tasks, as demonstrated in a\ntree structure in Fig. 1. Each level of this proxy tree in-\nvolves manually-designed modules, such as anchors [77],\nbox assignment rules [107], non-maximum suppression\n(NMS) [7], thing-stuff merging [100], etc. Although there\nare good solutions [77, 12, 33] to individual surrogate sub-\ntasks and modules, undesired artifacts are introduced when\nthese sub-tasks ï¬t into a pipeline for panoptic segmentation,\n*Work done while an intern at Google.\nAnchorRegression\nAnchorClassification\nBox-basedSegmentation\nBoxDetection\nSemanticSegmentation\nInstanceSegmentationPanopticSegmentation\nOursPrevious Methods (Panoptic-FPN)\nFigure 1. Our method predicts panoptic segmentation masks di-\nrectly from images, while previous methods (Panoptic-FPN as an\nexample) rely ona tree of surrogate sub-tasks. Panoptic segmen-\ntation masks are obtained by merging semantic and instance seg-\nmentation results. Instance segmentation is further decomposed\ninto box detection and box-based segmentation, while box detec-\ntion is achieved by anchor regression and anchor classiï¬cation.\n(a) Our MaX-DeepLab\n51.1 PQ (box-free)\n(b) Axial-DeepLab [91]\n43.4 PQ (box-free)\n(c) DetectoRS [75]\n48.6 PQ (box-based)\nFigure 2. A case study for our method and state-of-the-artbox-free\nand box-based methods. (a) Our end-to-end MaX-DeepLab cor-\nrectly segments a dog sitting on a chair. (b) Axial-DeepLab [91]\nrelies on a surrogate sub-task of regressing object center off-\nsets [21]. It fails because the centers of the dog and the chair are\nclose to each other. (c) DetectoRS [75] classiï¬es object bounding\nboxes, instead of masks, as a surrogate sub-task. It ï¬lters out the\nchair mask because the chair bounding box has a low conï¬dence.\nespecially in the challenging conditions (Fig. 2).\nRecent work on panoptic segmentation attempted to sim-\nplify this box-based pipeline. For example, UPSNet [100]\nproproses a parameter-free panoptic head, permitting back-\npropagation to both semantic and instance segmentation\n1\narXiv:2012.00759v3  [cs.CV]  12 Jul 2021\nMethod Anchor Center NMS Merge Box\n-Free -Free -Free -Free -Free\nPanoptic-FPN [47] \u0017 \u0013 \u0017 \u0017 \u0017\nUPSNet [100] \u0017 \u0013 \u0017 \u0013 \u0017\nDETR [10] \u0013 \u0013 \u0013 \u0013 \u0017\nAxial-DeepLab [91] \u0013 \u0017 \u0017 \u0017 \u0013\nMaX-DeepLab \u0013 \u0013 \u0013 \u0013 \u0013\nTable 1. Our end-to-end MaX-DeepLab dispenses with these com-\nmon hand-designed components necessary for existing methods.\nmodules. Recently, DETR [10] presents an end-to-end ap-\nproach for box detection, which is used to replace detectors\nin panoptic segmentation, but the whole training process of\nDETR still relies heavily on the box detection task.\nAnother line of work made efforts to completely remove\nboxes from the pipeline, which aligns better with the mask-\nbased deï¬nition of panoptic segmentation. The state-of-the-\nart method in this regime, Axial-DeepLab [91], along with\nother box-free methods [102, 21, 11], predicts pixel-wise\noffsets to pre-deï¬ned instance centers. But this center-based\nsurrogate sub-task makes it challenging to deal with highly\ndeformable objects, or near-by objects with close centers.\nAs a result, box-free methods do not perform as well as\nbox-based methods on the challenging COCO dataset [60].\nIn this paper, we streamline the panoptic segmenta-\ntion pipeline with an end-to-end approach. Inspired by\nDETR [10], our model directly predicts a set of non-\noverlapping masks and their corresponding semantic labels\nwith a mask transformer. The output masks and classes\nare optimized with a panoptic quality (PQ) style objective.\nSpeciï¬cally, inspired by the deï¬nition of PQ [48], we deï¬ne\na similarity metric between two class-labeled masks as the\nmultiplication of their mask similarity and their class sim-\nilarity. Our model is trained by maximizing this similarity\nbetween ground truth masks and predicted masks via one-\nto-one bipartite matching [51, 82, 10]. This direct modeling\nof panoptic segmentation enables end-to-end training and\ninference, removing those hand-coded priors that are nec-\nessary in existing box-based and box-free methods (Tab. 1).\nOur method is dubbed MaX-DeepLab for extending Axial-\nDeepLab with a Mask Xformer.\nIn companion with direct training and inference, we\nequip our mask transformer with a novel architecture. In-\nstead of stacking a traditional transformer [89, 10] on top of\na Convolutional Neural Network (CNN) [52], we propose\na dual-path framework for combining CNNs with trans-\nformers. Speciï¬cally, we enable any CNN layer to read\nand write a global memory, using our dual-path transformer\nblock. This block supports all types of attention between\nthe CNN-path and the memory-path, including memory-\npath self-attention ( M2M), pixel-path axial self-attention\n(P2P), memory-to-pixel attention (M2P), and ï¬nally pixel-\nto-memory attention (P2M). The transformer block can be\ninserted anywhere in a CNN, enabling communication with\nthe global memory at any layer. Besides this commu-\nnication module, our MaX-DeepLab employs a stacked-\nhourglass-style decoder [78, 71, 19]. The decoder aggre-\ngates multi-scale features into a high resolution output,\nwhich is then multiplied with the global memory feature,\nto form our mask set prediction. The classes for the masks\nare predicted with another branch of the mask transformer.\nWe evaluate MaX-DeepLab on one of the most challeng-\ning panoptic segmentation datasets, COCO [60], against the\nstate-of-the-art box-free method, Axial-DeepLab [91], and\nstate-of-the-art box-based method, DetectoRS [95] (Fig. 2).\nOur MaX-DeepLab, without test time augmentation (TTA),\nachieves the state-of-the-art result of 51.3% PQ on the test-\ndev set. This result surpasses Axial-DeepLab (with TTA)\nby 7.1% PQ in the box-free regime, and outperforms De-\ntectoRS (with TTA) by 1.7% PQ, bridging the gap between\nbox-based and box-free methods for the ï¬rst time. For a fair\ncomparison with DETR [10], we also evaluate a lightweight\nmodel, MaX-DeepLab-S, that matches the number of pa-\nrameters and M-Adds of DETR. We observe that MaX-\nDeepLab-S outperforms DETR by 3.3% PQ on the val set\nand 3.0% PQ on the test-dev set. In addition, we perform\nextensive ablation studies and analyses on our end-to-end\nformulation, model scaling, dual-path architectures, and our\nloss functions. We also notice that the extra-long training\nschedule of DETR [10] is not necessary for MaX-DeepLab.\nTo summarize, our contributions are four-fold:\nâ€¢ MaX-DeepLab is the ï¬rst end-to-end model for panop-\ntic segmentation, inferring masks and classes directly\nwithout hand-coded priors like object centers or boxes.\nâ€¢ We propose a training objective that optimizes a PQ-\nstyle loss function via a PQ-style bipartite matching\nbetween predicted masks and ground truth masks.\nâ€¢ Our dual-path transformer enables CNNs to read and\nwrite a global memory at any layer, providing a new\nway of combining transformers with CNNs.\nâ€¢ MaX-DeepLab closes the gap between box-based and\nbox-free methods and sets a new state-of-the-art on\nCOCO, even without using test time augmentation.\n2. Related Work\nTransformers. Transformers [89], ï¬rst introduced for neu-\nral machine translation, have advanced the state-of-the-art\nin many natural language processing tasks [27, 79, 26]. At-\ntention [2], as the core component of Transformers, was\ndeveloped to capture both correspondence of tokens across\nmodalities [2] and long-range interactions in a single con-\ntext (self-attention) [22, 89]. Later, the complexity of trans-\nformer attention has been reduced [49, 92], by introducing\nlocal [68] or sparse attention [23], together with a global\nmemory [6, 105, 31, 1]. The global memory, which inspires\n2\nour dual-path transformer, recovers long-range context by\npropagating information globally.\nTransformer and attention have been applied to computer\nvision as well, by combining non-local modules [93, 9]\nwith CNNs or by applying self-attention only [76, 37, 91].\nBoth classes of methods have boosted various vision tasks\nsuch as image classiï¬cation [18, 5, 76, 37, 57, 91, 28], ob-\nject detection [93, 80, 76, 36, 10, 110], semantic segmenta-\ntion [15, 108, 39, 29, 111, 109], video recognition [93, 18],\nimage generation [72, 35], and panoptic segmentation [91].\nIt is worth mentioning that DETR [10] stacked a trans-\nformer on top of a CNN for end-to-end object detection.\nBox-based panoptic segmentation. Most panoptic seg-\nmentation models, such as Panoptic FPN [47], follow a box-\nbased approach that detects object bounding boxes and pre-\ndicts a mask for each box, usually with a Mask R-CNN [33]\nand FPN [58]. Then, the instance segments (â€˜thingâ€™) and\nsemantic segments (â€˜stuffâ€™) [13] are fused by merging mod-\nules [54, 56, 73, 67, 103] to generate panoptic segmentation.\nFor example, UPSNet [100] developed a parameter-free\npanoptic head, which facilitates uniï¬ed training and infer-\nence [55]. Recently, DETR [10] extended box-based meth-\nods with its transformer-based end-to-end detector. And\nDetectoRS [75] advanced the state-of-the-art with recursive\nfeature pyramid and switchable atrous convolution.\nBox-free panoptic segmentation. Contrary to box-based\napproaches, box-free methods typically start with seman-\ntic segments [12, 14, 16]. Then, instance segments are\nobtained by grouping â€˜thingâ€™ pixels with various methods,\nsuch as instance center regression [44, 88, 70, 102, 20],\nWatershed transform [90, 3, 8], Hough-voting [4, 53, 8],\nor pixel afï¬nity [45, 66, 81, 30, 8]. Recently, Axial-\nDeepLab [91] advanced the state-of-the-art by equipping\nPanoptic-DeepLab [21] with a fully axial-attention [35]\nbackbone. In this work, we extend Axial-DeepLab with a\nmask transformer for end-to-end panoptic segmentation.\n3. Method\nIn this section, we describe how MaX-DeepLab directly\npredicts class-labeled masks for panoptic segmentation, fol-\nlowed by the PQ-style loss used to train the model. Then,\nwe introduce our dual-path transformer architecture as well\nas the auxiliary losses that are helpful in training.\n3.1. MaX-DeepLab formulation\nThe goal of panoptic segmentation is to segment the im-\nage I âˆˆRHÃ—WÃ—3 into a set of class-labeled masks:\n{yi}K\ni=1 = {(mi,ci)}K\ni=1 . (1)\nThe Kground truth masksmi âˆˆ{0,1}HÃ—W do not overlap\nwith each other,i.e., âˆ‘K\ni=1 mi â‰¤1HÃ—W, and ci denotes the\nground truth class label of mask mi.\nOur MaX-DeepLab directly predicts outputs in the exact\nsame form as the ground truth. MaX-DeepLab segments the\nimage I into a ï¬xed-size set of class-labeled masks:\n{Ë†yi}N\ni=1 = {( Ë†mi,Ë†pi(c))}N\ni=1 . (2)\nThe constant size N of the set is much larger than the typi-\ncal number of masks in an image [10]. The predicted masks\nË†mi âˆˆ [0,1]HÃ—W are softly exclusive to each other, i.e.,âˆ‘N\ni=1 Ë†mi = 1HÃ—W, and Ë†pi(c) denotes the probability of as-\nsigning class cto mask Ë†mi. Possible classes C âˆ‹cinclude\nthing classes, stuff classes, and aâˆ… class (no object). In this\nway, MaX-DeepLab deals with thing and stuff classes in a\nuniï¬ed manner, removing the need for merging operators.\nSimple inference. End-to-end inference of MaX-DeepLab\nis enabled by adopting the same formulation for both\nground truth deï¬nition and model prediction. As a re-\nsult, the ï¬nal panoptic segmentation prediction is obtained\nby simply performing argmax twice. Speciï¬cally, the ï¬rst\nargmax predicts a class label for each mask:\nË†ci = arg max\nc\nË†pi(c) . (3)\nAnd the other argmax assigns a mask-IDË†zh,w to each pixel:\nË†zh,w = arg max\ni\nË†mi,h,w,\nâˆ€hâˆˆ{1,2,...,H }, âˆ€wâˆˆ{1,2,...,W }.\n(4)\nIn practice, we ï¬lter each argmax with a conï¬dence thresh-\nold â€“ Masks or pixels with a low conï¬dence are removed\nas described in Sec. 4. In this way, MaX-DeepLab infers\npanoptic segmentation directly, dispensing with common\nmanually-designed post-processing, e.g., NMS and thing-\nstuff merging in almost all previous methods [47, 100]. Be-\nsides, MaX-DeepLab does not rely on hand-crafted priors\nsuch as anchors, object boxes, or instance mass centers,etc.\n3.2. PQ-style loss\nIn addition to simple inference, MaX-DeepLab enables\nend-to-end training as well. In this section, we introduce\nhow we train MaX-DeepLab with our PQ-style loss, which\ndraws inspiration from the deï¬nition of panoptic quality\n(PQ) [48]. This evaluation metric of panoptic segmentation,\nPQ, is deï¬ned as the multiplication of a recognition quality\n(RQ) term and a segmentation quality (SQ) term:\nPQ = RQ Ã— SQ . (5)\nBased on this decomposition of PQ, we design our objec-\ntive in the same manner: First, we deï¬ne a PQ-style simi-\nlarity metric between a class-labeled ground truth mask and\na predicted mask. Next, we show how we match a predicted\nmask to each ground truth mask with this metric, and ï¬nally\nhow to optimize our model with the same metric.\n3\nMask similarity metric. Our mask similarity metric\nsim(Â·,Â·) between a class-labeled ground truth mask yi =\n(mi,ci) and a prediction Ë†yj = ( Ë†mj,Ë†pj(c)) is deï¬ned as\nsim(yi,Ë†yj) = Ë†pj(ci)î´™î´˜î´—î´š\nâ‰ˆRQ\nÃ—Dice(mi, Ë†mj)î´™ î´˜î´— î´š\nâ‰ˆSQ\n, (6)\nwhere Ë†pj(ci) âˆˆ[0,1] is the probability of predicting the cor-\nrect class (recognition quality) and Dice(mi, Ë†mj) âˆˆ[0,1]\nis the Dice coefï¬cient between a predicted mask Ë†mj and a\nground truth mi (segmentation quality). The two terms are\nmultiplied together, analogous to the decomposition of PQ.\nThis mask similarity metric has a lower bound of 0,\nwhich means either the class prediction is incorrect, OR\nthe two masks do not overlap with each other. The upper\nbound, 1, however, is only achieved when the class predic-\ntion is correct AND the mask is perfect. The AND gating\nenables this metric to serve as a good optimization objective\nfor both model training and mask matching.\nMask matching. In order to assign a predicted mask to\neach ground truth, we solve a one-to-one bipartite matching\nproblem between the prediction set {Ë†yi}N\ni=1 and the ground\ntruth set {yi}K\ni=1. Formally, we search for a permutation of\nN elements Ïƒ âˆˆSN that best assigns the predictions to\nachieve the maximum total similarity to the ground truth:\nË†Ïƒ= arg max\nÏƒâˆˆSN\nKâˆ‘\ni=1\nsim(yi,Ë†yÏƒ(i)) . (7)\nThe optimal assignment is computed efï¬ciently with the\nHungarian algorithm [51], following prior work [10, 82].\nWe refer to the K matched predictions as positive masks\nwhich will be optimized to predict the corresponding\nground truth masks and classes. The (N âˆ’K) masks left\nare negatives, which should predict the âˆ… class (no object).\nOur one-to-one matching is similar to DETR [10], but\nwith a different purpose: DETR allows only one positive\nmatch in order to remove duplicated boxes in the absence\nof NMS, while in our case, duplicated or overlapping masks\nare precluded by design. But in our case, assigning multi-\nple predicted masks to one ground truth mask is problematic\ntoo, because multiple masks cannot possibly be optimized\nto ï¬t a single ground truth mask at the same time. In addi-\ntion, our one-to-one matching is consistent with the PQ met-\nric, where only one predicted mask can theoretically match\n(i.e., have an IoU over 0.5) with each ground truth mask.\nPQ-style loss. Given our mask similarity metric and the\nmask matching process based on this metric, it is straight\nforward to optimize model parametersÎ¸by maximizing this\nsame similarity metric over matched (i.e., positive) masks:\nmax\nÎ¸\nKâˆ‘\ni=1\nsim(yi,Ë†yË†Ïƒ(i)) â‡” max\nÎ¸,ÏƒâˆˆSN\nKâˆ‘\ni=1\nsim(yi,Ë†yÏƒ(i)) . (8)\nSubstituting the similarity metric (Equ. (6)) gives our PQ-\nstyle objective Opos\nPQ to be maximized for positive masks:\nmax\nÎ¸\nOpos\nPQ =\nKâˆ‘\ni=1\nË†pË†Ïƒ(i)(ci)î´™ î´˜î´— î´š\nâ‰ˆRQ\nÃ—Dice(mi, Ë†mË†Ïƒ(i))î´™ î´˜î´— î´š\nâ‰ˆSQ\n. (9)\nIn practice, we rewrite Opos\nPQ into two common loss terms\nby applying the product rule of gradient and then changing\na probability Ë†pto a log probability log Ë†p. The change from\nË†p to log Ë†p aligns with the common cross-entropy loss and\nscales gradients better in practice for optimization:\nLpos\nPQ =\nKâˆ‘\ni=1\nË†pË†Ïƒ(i)(ci)î´™ î´˜î´— î´š\nweight\nÂ·\n[\nâˆ’Dice(mi, Ë†mË†Ïƒ(i))\n]\nî´™ î´˜î´— î´š\nDice loss\n+\nKâˆ‘\ni=1\nDice(mi, Ë†mË†Ïƒ(i))î´™ î´˜î´— î´š\nweight\nÂ·\n[\nâˆ’log Ë†pË†Ïƒ(i)(ci)\n]\nî´™ î´˜î´— î´š\nCross-entropy loss\n,\n(10)\nwhere the loss weights are constants ( i.e., no gradient\nis passed to them). This reformulation provides insights\nby bridging our objective with common loss functions:\nOur PQ-style loss is equivalent to optimizing a dice loss\nweighted by the class correctness and optimizing a cross-\nentropy loss weighted by the mask correctness. The logic\nbehind this loss is intuitive: we want both of the mask and\nclass to be correct at the same time. For example, if a mask\nis far off the target, it is a false negative anyway, so we disre-\ngard its class. This intuition aligns with the down-weighting\nof class losses for wrong masks, and vice versa.\nApart from the Lpos\nPQ for positive masks, we deï¬ne a\ncross-entropy term Lneg\nPQ for negative (unmatched) masks:\nLneg\nPQ =\nNâˆ‘\ni=K+1\n[\nâˆ’log Ë†pË†Ïƒ(i)(âˆ…)\n]\n. (11)\nThis term trains the model to predict âˆ… for negative masks.\nWe balance the two terms by Î±, as a common practice to\nweight positive and negative samples [59]:\nLPQ = Î±Lpos\nPQ + (1âˆ’Î±)Lneg\nPQ , (12)\nwhere LPQ denotes our ï¬nal PQ-style loss.\n3.3. MaX-DeepLab Architecture\nAs shown in Fig. 3, MaX-DeepLab architecture includes\na dual-path transformer, a stacked decoder, and output\nheads that predict the masks and classes.\nDual-path transformer. Instead of stacking a transformer\non top of a CNN [10], we integrate the transformer and\nthe CNN in a dual-path fashion, with bidirectional com-\nmunication between the two paths. Speciï¬cally, we aug-\nment a 2D pixel-based CNN with a 1D global memory of\nsize N (i.e., the total number of predictions) and propose\na transformer block as a drop-in replacement for any CNN\n4\nğ»Ã—ğ‘Š\n1/4\nğ·Ã—ğ»4Ã—ğ‘Š4 2FC2FC\nğ‘Masks:ğ‘Classes:ğ‘Ã—ğ¶DogChairÂ· Â· Â·\nDual-PathTransformer\nğ‘Ã—ğ·\nğ¿Ã—Stacked Decoder\nConvConv\nConvConv\nConv\nConvConv\nConv\nConv\nFCğ‘!ğ‘˜!ğ‘£!ğ‘£\"ğ‘˜\"ğ‘\"\nP2MAttentionM2P&M2MAttention\nFCFCFC\nPixelPathMemoryPathMemory1/81/16\n1/81/4\nğ‘Ã—ğ»4Ã—ğ‘Š4Â· Â· Â·\nDual-PathTransformer\nP2PAxial-Attention(P)ixelPath(M)emoryPath\n1/8\n1/8\n1/16\n1/4\nğ‘€Ã— FFN\nğ»Ã—ğ‘Š\n1/4\nğ·Ã—ğ»4Ã—ğ‘Š4 2FC2FC\nğ‘Masks:ğ‘Classes:ğ‘Ã—ğ¶DogChairÂ· Â· Â·\nDual-PathTransformer\nğ‘Ã—ğ·\nğ¿Ã—Stacked Decoder\nConvConv\nConvConv\nConv\nConvConv\nConv\nConv\nFCğ‘!ğ‘˜!ğ‘£!ğ‘£\"ğ‘˜\"ğ‘\"\nP2MAttentionM2P&M2MAttention\nFCFCFC\nPixelPathMemoryPathMemory1/81/16\n1/81/4\nğ‘Ã—ğ»4Ã—ğ‘Š4Â· Â· Â·\nDual-PathTransformer\nP2PAxial-Attention(P)ixelPath(M)emoryPath\n1/8\n1/8\n1/16\n1/4\nğ‘€Ã— FFN\n(a) Overview of MaX-DeepLab\nğ»Ã—ğ‘Š\n1/4\nğ·Ã—ğ»4Ã—ğ‘Š4 2FC2FC\nğ‘Masks:ğ‘Classes:ğ‘Ã—ğ¶DogChairÂ· Â· Â·\nDual-PathTransformer\nğ‘Ã—ğ·\nğ¿Ã—Stacked Decoder\nConvConv\nConvConv\nConv\nConvConv\nConv\nConv\nFCğ‘!ğ‘˜!ğ‘£!ğ‘£\"ğ‘˜\"ğ‘\"\nP2MAttentionM2P&M2MAttention\nFCFCFC\nPixelPathMemoryPathMemory1/81/16\n1/81/4\nğ‘Ã—ğ»4Ã—ğ‘Š4Â· Â· Â·\nDual-PathTransformer\nP2PAxial-Attention(P)ixelPath(M)emoryPath\n1/8\n1/8\n1/16\n1/4\nğ‘€Ã— FFN (b) Dual-path transformer block\nFigure 3. (a) An image and a global memory are fed into a dual-\npath transformer, which directly predicts a set of masks and classes\n(residual connections omitted). (b) A dual-path transformer block\nis equipped with all 4 types of attention between the two paths.\nblock or an add-on for a pretrained CNN block. Our trans-\nformer block enables all four possible types of communica-\ntion between the 2D pixel-path CNN and the 1D memory-\npath: (1) the traditional memory-to-pixel ( M2P) attention,\n(2) memory-to-memory (M2M) self-attention, (3) pixel-to-\nmemory ( P2M) feedback attention that allows pixels to\nread from the memory, and (4) pixel-to-pixel ( P2P) self-\nattention, implemented as axial-attention blocks [39, 35,\n91]. We select axial-attention [91] rather than global 2D at-\ntention [10, 93, 5] for efï¬ciency on high resolution feature\nmaps. One could optionally approximate the pixel-to-pixel\nself-attention with a convolutional block that only allows lo-\ncal communication. This transformer design with a memory\npath besides the main CNN path is termed dual-path trans-\nformer. Unlike previous work [10], it allows transformer\nblocks to be inserted anywhere in the backbone at any reso-\nlution. In addition, the P2M feedback attention enables the\npixel-path CNN to reï¬ne its feature given the memory-path\nfeatures that encode mask information.\nFormally, given a 2D input feature xp âˆˆ R Ë†HÃ—Ë†WÃ—din\nwith height Ë†H, width Ë†W, channels din, and a 1D global\nmemory feature xm âˆˆRNÃ—din with length N (i.e., the size\nof the prediction set). We compute pixel-path queries qp,\nkeys kp, and valuesvp, by learnable linear projections of the\npixel-path feature map xp at each pixel. Similarly, qm, km,\nvm are computed from xm with another set of projection\nmatrices. The query (key) and value channels are dq and\ndv, for both paths. Then, the output of feedback attention\n(P2M), yp\na âˆˆRdout, at pixel position a, is computed as\nyp\na =\nNâˆ‘\nn=1\nsoftmaxn(qp\na Â·km\nn ) vm\nn , (13)\nwhere the softmax n denotes a softmax function applied\nto the whole memory of length N. Similarly, the output\nof memory-to-pixel (M2P) and memory-to-memory (M2M)\nattention ym\nb âˆˆRdout, at memory position b, is\nym\nb =\nË†H Ë†W+Nâˆ‘\nn=1\nsoftmaxn(qm\nb Â·kpm\nn ) vpm\nn ,\nkpm =\n[kp\nkm\n]\n, v pm =\n[vp\nvm\n]\n,\n(14)\nwhere a single softmax is performed over the concatenated\ndimension of size ( Ë†H Ë†W,+N), inspired by ETC [1].\nStacked decoder. Unlike previous work [21, 91] that uses\na light-weight decoder, we explore stronger hourglass-style\nstacked decoders [78, 71, 19]. As shown in Fig. 3, our de-\ncoder is stacked Ltimes, traversing output strides (4, 8, and\n16 [16, 61]) multiple times. At each decoding resolution,\nfeatures are fused by simple summation after bilinear resiz-\ning. Then, convolutional blocks or transformer blocks are\napplied, before the decoder feature is sent to the next res-\nolution. This stacked decoder is similar to feature pyramid\nnetworks [58, 63, 86, 75] designed for pyramidal anchor\npredictions [64], but our purpose here is only to aggregate\nmulti-scale features, i.e., intermediate pyramidal features\nare not directly used for prediction.\nOutput heads. From the memory feature of length N,\nwe predict mask classes Ë†p(c) âˆˆ RNÃ—|C| with two fully-\nconnected layers (2FC) and a softmax. Another 2FC head\npredicts mask feature f âˆˆRNÃ—D. Similarly, we employ\ntwo convolutions (2Conv) to produce a normalized feature\ng âˆˆRDÃ—H\n4 Ã—W\n4 from the decoder output at stride 4. Then,\nour mask prediction Ë†mis simply the multiplication of trans-\nformer feature f and decoder feature g:\nË†m= softmaxN (f Â·g) âˆˆRNÃ—H\n4 Ã—W\n4 . (15)\nIn practice, we use batch norm [41] onf and (fÂ·g) to avoid\ndeliberate initialization, and we bilinear upsample the mask\nprediction Ë†mto the original image resolution. Finally, the\ncombination {( Ë†mi,Ë†pi(c))}N\ni=1 is our mask transformer out-\nput to generate panoptic results as introduced in Sec. 3.1.\nOur mask prediction head is inspired by CondInst [87]\nand SOLOv2 [94], which extend dynamic convolution [43,\n101] to instance segmentation. However, unlike our end-\nto-end method, these methods require hand-designed object\ncenters and assignment rules for instance segmentation, and\na thing-stuff merging module for panoptic segmentation.\n5\n3.4. Auxiliary losses\nIn addition to the PQ-style loss (Sec. 3.2), we ï¬nd it ben-\neï¬cial to incorporate auxiliary losses in training. Speciï¬-\ncally, we propose a pixel-wise instance discrimination loss\nthat helps cluster decoder features into instances. We also\nuse a per-pixel mask-ID cross-entropy loss that classiï¬es\neach pixel into N masks, and a semantic segmentation loss.\nOur total loss function thus consists of the PQ-style loss\nLPQ and these three auxiliary losses.\nInstance discrimination. We use a per-pixel instance dis-\ncrimination loss to help the learning of the feature map\ng âˆˆRDÃ—H\n4 Ã—W\n4 . Given a downsampled ground truth mask\nmi âˆˆ{0,1}\nH\n4 Ã—W\n4 , we ï¬rst compute a normalized feature\nembedding ti,: âˆˆRD for each annotated mask by averaging\nthe feature vectors g:,h,w inside the mask mi:\nti,: =\nâˆ‘\nh,wmi,h,w Â·g:,h,w\n||âˆ‘\nh,wmi,h,w Â·g:,h,w||, i= 1,2,...,K. (16)\nThis gives us K instance embeddings {ti,:}K\ni=1 represent-\ning K ground truth masks. Then, we let each pixel feature\ng:,h,w perform an instance discrimination task, i.e., each\npixel should correctly identify which mask embedding (out\nof K) it belongs to, as annotated by the ground truth masks.\nThe contrastive loss at a pixel (h,w) is written as:\nLInstDis\nh,w = âˆ’log\nâˆ‘K\ni=1 mi,h,wexp (ti,: Â·g:,h,w/Ï„)\nâˆ‘K\ni=1 exp (ti,: Â·g:,h,w/Ï„)\n, (17)\nwhere Ï„ denotes the temperature, and note that mi,h,w is\nnon-zero only when pixel(h,w) belongs to the ground truth\nmask mi. In practice, this per-pixel loss is applied to all in-\nstance pixels in an image, encouraging features from the\nsame instance to be similar and features from different in-\nstances to be distinct, in a contrastive fashion, which is ex-\nactly the property required for instance segmentation.\nOur instance discrimination loss is inspired by previous\nworks [98, 96, 40, 17, 32, 46]. However, they discriminate\ninstances either unsupervisedly or with image classes [46],\nwhereas we perform a pixel-wise instance discrimination\ntask, as annotated by panoptic segmentation ground truth.\nMask-ID cross-entropy. In Equ. (4), we describe how we\ninfer the mask-ID map given our mask prediction. In fact,\nwe can train this per-pixel classiï¬cation task by applying a\ncross-entropy loss on it. This is consistent with the litera-\nture [42, 85, 10] that uses a cross-entropy loss together with\na dice loss [69] to learn better segmentation masks.\nSemantic segmentation. We also use an auxiliary seman-\ntic segmentation loss to help capture per pixel semantic fea-\nture. Speciï¬cally, we apply a semantic head [21] on top of\nthe backbone if no stacked decoder is used ( i.e., L = 0).\nOtherwise, we connect the semantic head to the ï¬rst de-\ncoder output at stride 4, because we ï¬nd it helpful to sepa-\nrate the ï¬nal mask feature gwith semantic segmentation.\n4. Experiments\nWe report our main results on COCO, comparing with\nstate-of-the-art methods. Then, we provide a detailed abla-\ntion study on the architecture variants and losses. Finally,\nwe analyze how MaX-DeepLab works with visualizations.\nTechnical details. Most of our default settings follow\nAxial-DeepLab [91]. Speciï¬cally, we train our models with\n32 TPU cores for 100k (400k for main results) iterations\n(54 epochs), a batch size of 64, Radam [62] Lookahead\n[106], a â€˜polyâ€™ schedule learning rate of 10âˆ’3 (3 Ã—10âˆ’4\nfor MaX-DeepLab-L), a backbone learning rate multiplier\nof 0.1, a weight decay of 10âˆ’4, and a drop path rate [38] of\n0.2. We resize and pad images to 641 Ã—641 [21, 91] (1025\nÃ—1025 for main results) for inference and M-Adds calcula-\ntion. During inference, we set masks with class conï¬dence\nbelow 0.7 to void and ï¬lter pixels with mask-ID conï¬dence\nbelow 0.4. Finally, following previous work [100, 21, 91],\nwe ï¬lter stuff masks with an area limit of 4096 pixels, and\ninstance masks with a limit of 256 pixels. In training, we\nset our PQ-style loss weight (Equ. (12), normalized by N)\nto 3.0, with Î± = 0.75. Our instance discrimination uses\nÏ„ = 0.3, and a weight of 1.0. We set the mask-ID cross-\nentropy weight to 0.3, and semantic segmentation weight to\n1.0. We use an output size N = 128 and D = 128 chan-\nnels. We ï¬ll the initial memory with learnable weights [10]\n(more details and architectures in Sec. A.6).\n4.1. Main results\nWe present our main results on COCO val set and test-\ndev set [60], with a small model, MaX-DeepLab-S, and a\nlarge model, MaX-DeepLab-L.\nMaX-DeepLab-S augments ResNet-50 [34] with axial-\nattention blocks [91] in the last two stages. After pretaining,\nwe replace the last stage with dual-path transformer blocks\nand use an L= 0(not stacked) decoder. We match parame-\nters and M-Adds to DETR-R101 [10], for fair comparison.\nMaX-DeepLab-L stacks an L = 2 decoder on top of\nWide-ResNet-41 [104, 97, 11]. And we replace all stride\n16 residual blocks by our dual-path transformer blocks with\nwide axial-attention blocks [91]. This large variant is meant\nto be compared with state-of-the-art results.\nVal set. In Tab. 2, we report our validation set results and\ncompare with both box-based and box-free panoptic seg-\nmentation methods. As shown in the table, our single-scale\nMaX-DeepLab-S already outperforms all other box-free\nmethods by a large margin of more than 4.5 % PQ, no mat-\nter whether other methods use test time augmentation (TTA,\nusually ï¬‚ipping and multi-scale) or not. Speciï¬cally, it\nsurpasses single-scale Panoptic-DeepLab by 8.7% PQ, and\n6\nMethod BackboneTTAParams M-AddsPQ PQTh PQSt\nBox-based panoptic segmentation methods\nPanoptic-FPN [47] RN-101 40.3 47.5 29.5\nUPSNet [100] RN-50 42.5 48.5 33.4\nDetectron2 [95] RN-101 43.0 - -\nUPSNet [100] RN-50 \u0013 43.2 49.1 34.1\nDETR [10] RN-101 61.8M 314B1 45.1 50.5 37.0\nBox-free panoptic segmentation methods\nPanoptic-DeepLab [21]X-71 [24] 46.7M 274B 39.7 43.9 33.2\nPanoptic-DeepLab [21]X-71 [24]\u0013 46.7M 3081B41.2 44.9 35.7\nAxial-DeepLab-L [91]AX-L [91] 44.9M 344B 43.4 48.5 35.6\nAxial-DeepLab-L [91]AX-L [91]\u0013 44.9M 3868B43.9 48.6 36.8\nMaX-DeepLab-S MaX-S 61.9M 324B 48.4 53.0 41.5\nMaX-DeepLab-L MaX-L 451M 3692B51.1 57.0 42.2\nTable 2. COCO val set. TTA: Test-time augmentation\nsingle-scale Axial-DeepLab by 5.0% PQ with similar M-\nAdds. We also compare MaX-DeepLab-S with DETR [10],\nwhich is based on an end-to-end detector, in a controlled\nenvironment of similar number of parameters and M-Adds.\nOur MaX-DeepLab-S outperforms DETR [10] by 3.3% PQ\nin this fair comparison. Next, we scale up MaX-DeepLab\nto a wider variant with stacked decoder, MaX-DeepLab-L.\nThis scaling further improves the single-scale performance\nto 51.1% PQ, outperforming multi-scale Axial-DeepLab\n[91] by 7.2% PQ with similar inference M-Adds.\nTest-dev set. Our improvements on the val set transfers\nwell to the test-dev set, as shown in Tab. 3. On the test-dev\nset, we are able to compare with more competitive meth-\nods and stronger backbones equipped with group convo-\nlution [50, 99], deformable convolution [25], or recursive\nbackbone [65, 75], while we do not use these improve-\nments in our model. In the regime of no TTA, our MaX-\nDeepLab-S outperforms Axial-DeepLab [91] by 5.4% PQ,\nand DETR [10] by 3.0% PQ. Our MaX-DeepLab-L without\nTTA further attains 51.3% PQ, surpassing Axial-DeepLab\nwith TTA by 7.1% PQ. This result also outperforms the best\nbox-based method DetectoRS [75] with TTA by 1.7% PQ,\nclosing the large gap between box-based and box-free meth-\nods on COCO for the ï¬rst time. Our MaX-DeepLab sets a\nnew state-of-the-art on COCO, even without using TTA.\n4.2. Ablation study\nIn this subsection, we provide more insights by teasing\napart the effects of MaX-DeepLab components on the val\nset. We ï¬rst deï¬ne a default baseline setting and then vary\neach component of it: We augment Wide-ResNet-41 [104,\n97, 11] by applying dual-path transformer to all blocks at\nstride 16, enabling all four types of attention. For faster\nwall-clock training, we use an L= 0(not stacked) decoder\nand approximate P2P attention with convolutional blocks.\n1https://github.com/facebookresearch/detr\nMethod Backbone TTA PQ PQ Th PQSt\nBox-based panoptic segmentation methods\nPanoptic-FPN [47] RN-101 40.9 48.3 29.7\nDETR [10] RN-101 46.0 - -\nUPSNet [100] DCN-101 [25] \u0013 46.6 53.2 36.7\nDetectoRS [75] RX-101 [99] \u0013 49.6 57.8 37.1\nBox-free panoptic segmentation methods\nPanoptic-DeepLab [21]X-71 [24, 74] \u0013 41.4 45.1 35.9\nAxial-DeepLab-L [91] AX-L [91] 43.6 48.9 35.6\nAxial-DeepLab-L [91] AX-L [91] \u0013 44.2 49.2 36.8\nMaX-DeepLab-S MaX-S 49.0 54.0 41.6\nMaX-DeepLab-L MaX-L 51.3 57.2 42.4\nTable 3. COCO test-dev set. TTA: Test-time augmentation\nRes Axial L Iter Params M-Adds PQ PQ Th PQSt\n641 \u0017 0 100k 196M 746B 45.7 49.8 39.4\n641 \u0013 0 100k 277M 881B 47.8 51.9 41.5\n1025 \u0017 0 100k 196M 1885B 46.1 50.7 39.1\n1025 \u0013 0 100k 277M 2235B 49.4 54.5 41.8\n641 \u0017 1 100k 271M 1085B 47.1 51.6 40.3\n641 \u0017 2 100k 347M 1425B 47.5 52.3 40.2\n641 \u0017 0 200k 196M 746B 46.9 51.5 40.0\n641 \u0017 0 400k 196M 746B 47.7 52.5 40.4\nTable 4. Scaling MaX-DeepLab by using a larger inputResolution,\nreplacing convolutional blocks with Axial-attention blocks, stack-\ning decoder L times, and training with more Iterations.\nP2M M2M Stride Params M-Adds PQ PQ Th PQSt\n\u0013 \u0013 16 196M 746B 45.7 49.8 39.4\n\u0013 16 188M 732B 45.0 48.9 39.2\n\u0013 16 196M 746B 45.1 49.3 38.9\n16 186M 731B 44.7 48.5 39.0\n\u0013 \u0013 16 & 8 220M 768B 46.7 51.3 39.7\n\u0013 \u0013 16 & 8 & 4 234M 787B 46.3 51.1 39.0\nTable 5. Varying transformerP2M feedback attention, M2M self-\nattention, and the Stride where we apply the transformer.\nScaling. We ï¬rst study the scaling of MaX-DeepLab in\nTab. 4. We notice that replacing convolutional blocks with\naxial-attention blocks gives the most improvement. Further\nchanging the input resolution to 1025 Ã—1025 improves the\nperformance to 49.4% PQ, with a short 100k schedule (54\nepochs). Stacking the decoder L = 1time improves 1.4%\nPQ, but further scaling to L= 2starts to saturate. Training\nwith more iterations helps convergence, but we ï¬nd it not\nas necessary as DETR which is trained for 500 epochs.\nDual-path transformer. Next, we vary attention types of\nour dual-path transformer and the stages (strides) where we\napply transformer blocks. Note that we always apply M2P\nattention that attaches the transformer to the CNN. AndP2P\n7\n0 20 40 60 80 100\nSteps (k)\n0.0\n0.2\n0.4\n0.5PQ (%)\n(a) Validation PQ\n0 20 40 60 80 100\nSteps (k)\n0.0\n0.2\n0.4\n0.6\n0.8Confidence (b) Matched class conï¬dence\n0 20 40 60 80 100\nSteps (k)\n0.0\n0.2\n0.4\n0.6\n0.8Dice (c) Matched mask dice\n0 20 40 60 80 100\nSteps (k)\n0\n25\n50\n75\n100Accuracy (%) (d) Instance discrimination\n0 20 40 60 80 100\nSteps (k)\n0\n25\n50\n75\n100Accuracy (%) (e) Mask-ID prediction\nFigure 4. Training curves for (a) validation PQ, (b) average class conï¬dence, Ë†pË†Ïƒ(i)(ci), of matched masks, (c) average mask dice,\nDice(mi, Ë†mË†Ïƒ(i)), of matched masks, (d) per-pixel instance discrimination accuracy, and (e) per-pixel mask-ID prediction accuray.\nsim InstDis Mask Sem PQ PQ Th PQSt SQ RQ\nRQÃ—SQ \u0013 \u0013 \u0013 45.7 49.8 39.4 80.9 55.3\nRQ+ SQ \u0013 \u0013 \u0013 44.9 48.6 39.3 80.2 54.5\nRQÃ—SQ \u0013 \u0013 45.1 50.1 37.6 80.6 54.5\nRQÃ—SQ \u0013 43.3 46.4 38.6 80.1 52.6\nRQÃ—SQ \u0013 42.6 48.1 34.1 80.0 52.0\nRQÃ—SQ 39.5 41.8 36.1 78.9 49.0\nTable 6. Varying the similarity metric sim and whether to ap-\nply the auxiliary Instance Discrimination loss, Mask-ID cross-\nentropy loss or the Semantic segmentation loss.\nattention is already ablated above. As shown in Tab. 5, re-\nmoving our P2M feedback attention causes a drop of 0.7%\nPQ. On the other hand, we ï¬nd MaX-DeepLab robust (-\n0.6% PQ) to the removal of M2M self-attention. We at-\ntribute this robustness to our non-overlapping mask formu-\nlation. Note that DETR [10] relies on M2M self-attention to\nremove duplicated boxes. In addition, it is helpful (+1.0%\nPQ) to apply transformer blocks to stride 8 also, which is\nimpossible for DETR without our dual-path design. Push-\ning it further to stride 4 does not show more improvements.\nLoss ablation. Finally, we ablate our PQ-style loss and\nauxiliary losses in Tab. 6. We ï¬rst switch our PQ-style sim-\nilarity in Equ. (6) from RQ Ã—SQ to RQ + SQ, which dif-\nfers in the hungarian matching (Equ. (7)) and removes dy-\nnamic loss weights in Equ. (10). We observe that RQ + SQ\nworks reasonably well, but RQ Ã—SQ improves 0.8% PQ\non top of it, conï¬rming the effect of our PQ-style loss in\npractice, besides its conceptual soundness. Next, we vary\nauxiliary losses applied to MaX-DeepLab, without tuning\nloss weights for remaining losses. Our PQ-style loss alone\nachieves a reasonable performance of 39.5% PQ. Adding\ninstance discrimination signiï¬cantly improves PQTh, show-\ning the importance of a clustered feature embedding. Mask-\nID prediction shares the same target with the Dice term in\nEqu. (10), but helps focus on large masks when the Dice\nterm is overwhelmed by small objects. Combining both of\nthe auxiliary losses leads to a large 5.6% PQ gain. Further\nmulti-tasking with semantic segmentation improves 0.6%\nPQ, because its class-level supervision helps stuff classes\nbut not instance-level discrimination for thing classes.\n(a) Original image\n (b) Decoder feature g\nâ€¦â€¦\nDog (thing)Chair (thing)Dining-table (thing)Cake (thing)Wall (stuff)No object (c) Transformer output\nFigure 5. (b) Pixels of the same instance have similar colors (fea-\ntures), while pixels of different instances have distinct colors.\n(c) The transformer predicts mask colors (features) and classes.\n4.3. Analysis\nWe provide more insights of MaX-DeepLab by plotting\nour training curves and visualizing the mask output head.\nTraining curves. We ï¬rst report the validation PQ curve in\nFig. 4(a), with our default ablation model. MaX-DeepLab\nconverges quickly to around 46% PQ within 100k iterations\n(54 epochs), 1/10 of DETR [10]. In Fig. 4(b) and Fig. 4(c),\nwe plot the characteristics of all matched masks in an image.\nThe matched masks tend to have a better class correctness\nthan mask correctness. Besides, we report per-pixel accu-\nracies for instance discrimination (Fig. 4(d)) and mask-ID\nprediction (Fig. 4(e)). We see that most pixels learn quickly\nto ï¬nd their own instances (out of K) and predict their own\nmask-IDs (out of N). Only 10% of all pixels predict wrong\nmask-IDs, but they contribute to most of the PQ error.\nVisualization. In order to intuitively understand the nor-\nmalized decoder output g, the transformer mask feature f,\nand how they are multiplied to generate our mask output Ë†m,\nwe train a MaX-DeepLab withD= 3and directly visualize\nthe normalized features as RGB colors. As shown in Fig. 5,\nthe decoder feature gassigns similar colors (or feature vec-\ntors) to pixels of the same mask, no matter the mask is a\nthing or stuff, while different masks are colored differently.\nSuch effective instance discrimination (as colorization) fa-\ncilitates our simple mask extraction with an inner product.\n5. Conclusion\nIn this work, we have shown for the ï¬rst time that panop-\ntic segmentation can be trained end-to-end. Our MaX-\nDeepLab directly predicts masks and classes with a mask\n8\ntransformer, removing the needs for many hand-designed\npriors such as object bounding boxes, thing-stuff merging,\netc. Equipped with a PQ-style loss and a dual-path trans-\nformer, MaX-DeepLab achieves the state-of-the-art result\non the challenging COCO dataset, closing the gap between\nbox-based and box-free methods for the ï¬rst time.\nAcknowledgments. We would like to thank Maxwell\nCollins and Sergey Ioffe for their feedbacks on the pa-\nper, Jiquan Ngiam for Hungarian Matching implementa-\ntion, Siyuan Qiao for DetectoRS segmentation results, Chen\nWei for instance discrimination insights, Jieneng Chen for\ndice loss comments and the support from Google Mobile\nVision. This work is supported by Google Research Fac-\nulty Award.\nA. Appendix\nA.1. Panoptic Segmentation Results\nSimilar to the case study in Fig. 2, we provide more\npanoptic segmentation results of our MaX-DeepLab-L and\ncompare them to the state-of-the-art box-free method,\nAxial-DeepLab [91], the state-of-the-artbox-based method,\nDetectoRS [75], and the ï¬rst Detection Transformer,\nDETR [10] in Fig. A.1 and Fig. A.2. MaX-DeepLab\ndemonstrates robustness to the challenging cases of sim-\nilar object bounding boxes and nearby objects with close\ncenters, while other methods make systematic mistakes be-\ncause of their individual surrogate sub-task design. MaX-\nDeepLab also shows exceptional mask quality, and per-\nforms well in the cases of many small objects. Similar to\nDETR [10], MaX-DeepLab fails typically when there are\ntoo many object masks.\nA.2. Runtime\nIn Tab. A.1, we report the end-to-end runtime (i.e., in-\nference time from an input image to ï¬nal panoptic segmen-\ntation) of MaX-DeepLab on a V100 GPU. All results are\nobtained by (1) a single-scale input without ï¬‚ipping, and\n(2) built-in TensorFlow library without extra inference op-\ntimization. In the fast regime, MaX-DeepLab-S takes 67\nms with a typical 641 Ã—641 input. This runtime includes 5\nms of postprocessing and 15 ms of batch normalization that\ncan be easily optimized. This fast MaX-DeepLab-S does\nnot only outperform DETR-R101 [10], but is also around\n2x faster. In the slow regime, the standard MaX-DeepLab-S\ntakes 131 ms with a 1025Ã—1025 input, similar to Panoptic-\nDeepLab-X71 [21]. This runtime is also similar to our run\nof the ofï¬cial DETR-R101 which takes 128 ms on a V100,\nincluding 63 ms for box detection and 65 ms for the heavy\nmask decoding.\n2https://github.com/facebookresearch/detr\nA.3. Mask Output Slot Analysis\nIn this subsection, we analyze the statistics of all\nN = 128mask prediction slots using MaX-DeepLab-L. In\nFig. A.3, we visualize the joint distribution of mask slot ï¬r-\nings and the classes they predict. We observe that the mask\nslots have imbalanced numbers of predictions and they spe-\ncialize on â€˜thingâ€™ classes and â€˜stuffâ€™ classes. Similar to this\nMask-Class joint distribution, we visualize the Mask-Pixel\njoint distribution by extracting an average mask for each\nmask slot, as shown in Fig. A.4. Speciï¬cally, we resize all\nCOCO [60] validation set panoptic segmentation results to a\nunit square and take an average of masks that are predicted\nby each mask slot. We split all mask slots into three cat-\negories according to their total ï¬rings and visualize mask\nslots in each category. We observe that besides the class-\nlevel specialization, our mask slots also specialize on cer-\ntain regions of an input image. This observation is similar\nto DETR [10], but we do not see the pattern that almost all\nslots have a mode of predicting large image-wide masks.\nA.4. Mask Head Visualization\nIn Fig. 5, we visualize how the mask head works by train-\ning a MaX-DeepLab with onlyD= 3decoder feature chan-\nnels (for visualization purpose only). Although this extreme\nsetting degrades the performance from 45.7% PQ to 37.8%\nPQ, it enables us to directly visualize the decoder features\nas RGB colors. Here in Fig. A.5 we show more examples\nusing this model, together with the corresponding panoptic\nsementation results. We see a similar clustering effect of\ninstance colors, which enables our simple mask extraction\nwith just a matrix multiplication (a.k.a. dynamic convolu-\ntion [87, 94, 43, 101]).\nA.5. Transformer Attention Visualization\nWe also visualize the M2P attention that connects the\ntransformer to the CNN. Speciï¬cally, given an input im-\nage from COCO validation set, we ï¬rst select four output\nmasks of interest from the MaX-DeepLab-L panoptic pre-\ndiction. Then, we probe the attention weights between the\nfour masks and all the pixels, in the last dual-path trans-\nformer block. Finally, we colorize the four attention maps\nwith four colors and visualize them in one ï¬gure. This\nprocess is repeated for two images and all eight attention\nheads as shown in Fig. A.6. We omit our results for the\nï¬rst transformer block since it is mostly ï¬‚at. This is ex-\npected because the memory feature in the ï¬rst transformer\nblock is unaware of the pixel-path input image at all. Un-\nlike DETR [10] which focuses on object extreme points for\ndetecting bounding boxes, our MaX-DeepLab attends to in-\ndividual object (or stuff) masks. This mask-attending prop-\nerty makes MaX-DeepLab relatively robust to nearby ob-\njects with similar bounding boxes or close mass centers.\n9\nOriginal Image MaX-DeepLab-L Axial-DeepLab [91] DetectoRS [75] DETR [10] Ground Truth\nMask Transformer Box-Free Box-Based Box Transformer\n51.1% PQ 43.4% PQ 48.6% PQ 45.1% PQ\nMaX-DeepLab segments the baby with its occluded leg correctly. DetectoRS and DETR merge the two people into one instance,\nprobably because the two people have similar bounding boxes. In addition, DETR introduces artifacts around the head of the horse.\nMaX-DeepLab correctly segments all the boards, the zebras, and the people. All other methods fail in these challenging cases of similar\nbounding boxes and nearby object centers.\nMaX-DeepLab generates a high quality mask for the cat, arguably better than the ground truth. Axial-DeepLab predicts cat pixels on\nthe right of the image, as the center of the cat is close to the center of the bike. And DETR misses the cat and introduces artifacts.\nMaX-DeepLab also performs well in the presence of many small instances.\nFigure A.1. Comparing MaX-DeepLab with other representative methods on the COCO val set. (Colors modiï¬ed for better visualization).\n10\nMethod Backbone Input Size Runtime (ms) PQ [val] PQ [test]\nFast Regime\nPanoptic-DeepLab [21] X-71 [24] 641Ã—641 74 38.9 38.8\nMaX-DeepLab-S MaX-S 641Ã—641 67 46.4 46.7\nSlow Regime\nDETR [10] RN-101 1333Ã—800 1282 45.1 46.0\nPanoptic-DeepLab [21] X-71 [24] 1025Ã—1025 132 39.7 39.6\nMaX-DeepLab-S MaX-S 1025Ã—1025 131 48.4 49.0\nTable A.1. End-to-end runtime. PQ [val]: PQ (%) on COCO val set. PQ [test]: PQ (%) on COCO test-dev set.\nA.6. More Technical Details\nIn Fig. A.7, Fig. A.8, and Fig. A.9, we include more de-\ntails of our MaX-DeepLab architectures. As marked in the\nï¬gure, we pretrain our model on ImageNet [50]. The pre-\ntraining model uses only P2P attention (could be a convo-\nlutional residual block or an axial-attention block), without\nthe other three types of attention, the feed-forward network\n(FFN), or the memory. We directly pretrain with an av-\nerage pooling followed by a linear layer. This pretrained\nmodel is used as a backbone for panoptic segmentation, and\nit uses the backbone learning rate multiplier we mentioned\nin Sec. 4. After pretraining the CNN path, we apply (with\nrandom initialization) our proposed memory path, includ-\ning the memory, the three types of attention, the FFNs, the\ndecoding layers, and the output heads for panoptic segmen-\ntation. In addition, we employ multi-head attention with\n8 heads for all attention operations. In MaX-DeepLab-L,\nwe use shortcuts in the stacked decoder. Speciï¬cally, each\ndecoding stage (resolution) is connected to the nearest two\nprevious decoding stage outputs of the same resolution.\n11\nOriginal Image MaX-DeepLab-L Axial-DeepLab [91] DetectoRS [75] DETR [10] Ground Truth\nMask Transformer Box-Free Box-Based Box Transformer\n51.1% PQ 43.4% PQ 48.6% PQ 45.1% PQ\nSimilar to DETR [10], MaX-DeepLab fails typically when there are too many masks to segment in an image. This example contains\nmore than 200 masks that should be predicted, mostly people and ties.\nIn this failure case, MaX-DeepLab mistakes the birds for kites in the sky, probably because the birds are too small.\nFigure A.2. Failure cases of MaX-DeepLab on the COCO val set.\n1 20 40 60 80 100 120 133\nClass ID\n1\n32\n64\n96\n128Mask Slot ID\nFigure A.3. The joint distribution for our N = 128mask slots and 133 classes with 80 â€˜thingâ€™ classes on the left and 53 â€˜stuffâ€™ classes on\nthe right. We observe that a few mask slots predict a lot of the masks. Some mask slots are used less frequently, probably only when there\nare a lot of objects in one image. Some other slots do not ï¬re at all. In addition, we see automatic functional segregation between â€˜thingâ€™\nmask slots and â€˜stuffâ€™ mask slots, with a few exceptions that can predict both thing and stuff masks.\n12\nMask Slot 71 Mask Slot 106 Mask Slot 125 Mask Slot 69 Mask Slot 116 Mask Slot 4 Mask Slot 27 Mask Slot 103\nMost\nFirings\n(sorted)\nMask Slot 84 Mask Slot 67 Mask Slot 23 Mask Slot 101 Mask Slot 127 Mask Slot 28 Mask Slot 105 Mask Slot 122\nMedium\nFirings\n(sorted)\nMask Slot 25 Mask Slot 66 Mask Slot 98 Mask Slot 110 Mask Slot 63 Mask Slot 95 Mask Slot 40 Mask Slot 79\nFew\nFirings\n(sorted)\nFigure A.4. The average masks that each mask slot predicts, normalized by image shape. Mask slots are categorized by their total number\nof ï¬rings and sorted from most ï¬rings to few ï¬rings. We observe spatial clustered patterns, meaning that the mask slots specialize on\ncertain regions of an input image. For example, the most ï¬ring mask slot 71, focusing on the center of an image, predicts almost all 80\nâ€˜thingâ€™ classes but ignores â€˜stuffâ€™ classes (Fig. A.3). The top three categories are tennis rackets, cats, and dogs. The second ï¬ring mask\nslot 106 segments 14 classes of masks on the bottom of an image, such as road, ï¬‚oor, or dining-tables. The third ï¬ring mask slot 125\nconcentrates 99.9% on walls or trees that are usually on the top of an image. The fourth ï¬ring mask slot 69 focuses entirely on the person\nclass and predicts 2663 people in the 5000 validation images.\nOriginal Image Decoder Feature g Panoptic Seg. Original Image Decoder Feature g Panoptic Seg.\nFigure A.5. More visualizations of the decoder feature g with D = 3. Similar to Fig. 5, we observe a clustering effect of instance colors,\ni.e., pixels of the same instance have similar colors (features) while pixels of different instances have distinct colors. Note that in this\nextreme case of D = 3(that achieves 37.8% PQ), there are not enough colors for all masks, which causes missing objects or artifacts at\nobject boundaries, but these artifacts do not present in our normal setting of D = 128(that achieves 45.7% PQ).\n13\nOriginal Image Head 1 Head 2 Head 3 Head 4\nPanoptic Segmentation Head 5 Head 6 Head 7 Head 8\nAttention maps for three people (left, middle, right) on a playing ï¬eld.\nOriginal Image Head 1 Head 2 Head 3 Head 4\nPanoptic Segmentation Head 5 Head 6 Head 7 Head 8\nAttention maps for two people (woman, man) cutting a cake on a table.\nFigure A.6. Visualizing the transformerM2P attention maps for selected predicted masks. We observe that head 2, together with head 5, 7,\nand 8, mainly attends to the output mask regions. Head 1, 3, and 4 gather more context from broader regions, such as semantically-similar\ninstances (scene 1 head 1) or mask boundaries (scene 2 head 4). In addition, we see that head 6 does not pay much attention to the pixel-\npath, except for some minor ï¬rings on the playing ï¬eld and on the table. Instead, it focuses more on M2M self-attention which shares the\nsame softmax with M2P attention (Equ. (14)).\n14\nConv1Ã—1 Conv1Ã—1\n(ğ»Ã—ğ‘ŠÃ—16)Ã—8\nğ»Ã—ğ‘ŠÃ—256\nğ»Ã—ğ‘ŠÃ—128\n ğ»Ã—ğ‘ŠÃ—128 (ğ»Ã—ğ‘ŠÃ—16)Ã—8ğ»Ã—ğ‘ŠÃ—128\nğ»Ã—ğ‘ŠÃ—256\nğ»Ã—ğ‘ŠÃ—256Multi-Head AttentionHeight-Axis Multi-Head AttentionWidth-Axis\nConcat Concatğ’™ ğ’š ğ’›\nFigure A.7. An example Axial-Block from Axial-DeepLab [91]. This axial-attention bottleneck block consists of two axial-attention layers\noperating along height- and width-axis sequentially.\nConv 3 x 3, stride 2\nConv 3 x 3\nImage: ğ»Ã—ğ‘ŠÃ—3\nOutput: !\"Ã—#\"Ã—128\n128\nConv 3 x 36464\nMax pool 3 x 3stride 2128\n3\n(a) Inception Stem [84, 83]\nConv 1 x 1\nConv 1 x 1\nInput: ğ»Ã—ğ‘ŠÃ—ğ¶\nOutput: ğ»Ã—ğ‘ŠÃ—ğ¶ğ¶\nConv 3x 3ğ¶/4ğ¶/4 (b) Bottleneck block\nConv 3x3Conv 3x3\nInput: ğ»Ã—ğ‘ŠÃ—ğ¶\nOutput: ğ»Ã—ğ‘ŠÃ—ğ¶\nğ¶ğ¶ (c) Wide-Basic block\nConv 1 x 1\nConv 1 x 1\nInput: ğ»Ã—ğ‘ŠÃ—ğ¶\nOutput: ğ»Ã—ğ‘ŠÃ—ğ¶ğ¶\nConv 3 x 3ğ¶/2ğ¶/4 (d) Wide-Bottle block\nFigure A.8. Building blocks for our MaX-DeepLab architectures.\n15\nConv\nConv\nFC\nğ‘!ğ‘˜!ğ‘£! ğ‘£\"ğ‘˜\"ğ‘\"\nP2MAttentionM2P&M2MAttention\nFCFCFC\nP2PAxial-Attention\n(P)ixelPath (M)emoryPath\nFFN\nğ»Ã—ğ‘ŠÃ—ğ¶!\"#$% ğ‘Ã—ğ¶&$&'()\nğ»Ã—ğ‘ŠÃ—ğ¶!\"#$% ğ‘Ã—ğ¶&$&'()\nC\nCC/2C/2 C/2C/2\nC\n2048\nC\nC\nC\n(a) Dual-Path Transformer Block\nğ»Ã—ğ‘Š\n1/4256\nğ·Ã—ğ»4Ã—ğ‘Š4 FC 512 FC\nğ‘Masks:ğ‘Classes:ğ‘Ã—ğ¶DogChairÂ· Â· Â·\n3x Dual-Path (Wide-Bottle)Transformer Block (512)\nğ‘Ã—ğ·\n3x Wide-Basic6x Wide-Basic\nPixelPath MemoryPathMemory\n1/8512\n1/4256\nğ‘Ã—ğ»4Ã—ğ‘Š4Â· Â· Â·\n1/8512\nFC 512 FC\n3x Dual-Path (Wide-Bottle)Transformer Block (512)\n3x Wide-Basic1/1620481/162048\n3x Wide-BasicSep 5x5 256 Conv 1x1\nğ‘Ã—512\nğ‘Ã—512\nğ‘Ã—512\nğ‘Ã—512\n3x Wide-Basic1/2128Conv 3x3 Stem1/264\n3x Dual-Path (Wide-Bottle)Transformer Block (256)1/161024 ğ‘Ã—512\nPretrain (b) MaX-DeepLab-Ablation\nğ»Ã—ğ‘Š\n1/4256\nğ·Ã—ğ»4Ã—ğ‘Š4 FC 256 FC\nğ‘Masks:ğ‘Classes:ğ‘Ã—ğ¶DogChairÂ· Â· Â·\n3x Dual-Path (Axial)Transformer Block (256)\nğ‘Ã—ğ·\n3x BottleNeck4x BottleNeck6x Axial-Block\nPixelPath MemoryPathMemory\n1/8512\n1/161024\n1/4256\nğ‘Ã—ğ»4Ã—ğ‘Š4Â· Â· Â·\n1/8512\nFC 256 FC\n1x Dual-Path (Axial)Transformer Block (256)\n1x BottleNeck1/1620481/162048\n1x BottleNeckSep 5x5 256 Conv 1x1\nğ‘Ã—256\nğ‘Ã—256\nğ‘Ã—256\nğ‘Ã—256\nInception Stem1/4128\nPretrain (c) MaX-DeepLab-S\nğ»Ã—ğ‘Š\n1/4256\nğ·Ã—ğ»4Ã—ğ‘Š4 FC 512 FC\nğ‘Masks:ğ‘Classes:ğ‘Ã—ğ¶DogChairÂ· Â· Â·ğ‘Ã—ğ·\n3x Wide-Basic6x Wide-Basic\nPixelPath MemoryPathMemory\n1/8512\n1/4256\nğ‘Ã—ğ»4Ã—ğ‘Š4Â· Â· Â·\n1/8512\nFC 512 FC\n3x Dual-Path (Wide-Axial)Transformer Block (512)\n3x Wide-Basic1/162048\n3x Wide-BasicSep 5x5 256 Conv 1x1\nğ‘Ã—512\nğ‘Ã—512\nğ‘Ã—512\n3x Wide-Basic1/2128Conv 3x3 Stem1/264\n3x Dual-Path (Wide-Axial)Transformer Block (256)\n3x Dual-Path (Wide-Axial)Transformer Block (512)1/1620481/161024\n1/42561/85121x Wide-Basic1x Wide-Basic1x Wide-Basic1/8512\n1x Dual-Path (Wide-Axial)Transformer Block (512)1/162048\n3x Dual-Path (Wide-Axial)Transformer Block (512)1/162048\n1/42561/85123x Wide-Basic3x Wide-Basic3x Wide-Basic1/8512\nğ‘Ã—512\nğ‘Ã—512\nğ‘Ã—512\nğ‘Ã—512\nPretrain (d) MaX-DeepLab-L\nFigure A.9. More detailed MaX-DeepLab architectures. Pretrain labels where we use a classiï¬cation head to pretrain our models on\nImageNet [50]. (a) A dual-path transformer block with C intermediate bottleneck channels. (b) The baseline architecture for our ablation\nstudies in Sec. 4.2. (c) MaX-DeepLab-S that matches the number of parameters and M-Adds of DETR-R101-Panoptic [10]. Axial-Block\n(Fig. A.7) is an axial-attention bottleneck block borrowed from Axial-DeepLab-L [91]. (d) MaX-DeepLab-L that achieves the state-of-the-\nart performance on COCO [60]. Wide-Axial is a wide version of Axial-Block with doubled intermediate bottleneck channels, similar to\nthe one used in Axial-DeepLab-XL [91]. (The residual connections are dropped for neatness).\n16\nReferences\n[1] Joshua Ainslie, Santiago Ontanon, Chris Alberti, Philip\nPham, Anirudh Ravula, and Sumit Sanghai. Etc: Encod-\ning long and structured data in transformers. In EMNLP,\n2020. 2, 5\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.\nNeural machine translation by jointly learning to align and\ntranslate. In ICLR, 2015. 2\n[3] Min Bai and Raquel Urtasun. Deep watershed transform\nfor instance segmentation. In CVPR, 2017. 3\n[4] Dana H Ballard. Generalizing the hough transform to detect\narbitrary shapes. Pattern Recognition, 1981. 3\n[5] Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon\nShlens, and Quoc V Le. Attention augmented convolutional\nnetworks. In ICCV, 2019. 3, 5\n[6] Iz Beltagy, Matthew E Peters, and Arman Cohan. Long-\nformer: The long-document transformer. arXiv preprint\narXiv:2004.05150, 2020. 2\n[7] Navaneeth Bodla, Bharat Singh, Rama Chellappa, and\nLarry S Davis. Soft-nmsâ€“improving object detection with\none line of code. In ICCV, 2017. 1\n[8] Ujwal Bonde, Pablo F Alcantarilla, and Stefan Leuteneg-\nger. Towards bounding-box free panoptic segmentation.\narXiv:2002.07705, 2020. 3\n[9] Antoni Buades, Bartomeu Coll, and J-M Morel. A non-\nlocal algorithm for image denoising. In CVPR, 2005. 3\n[10] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nico-\nlas Usunier, Alexander Kirillov, and Sergey Zagoruyko.\nEnd-to-end object detection with transformers. In ECCV,\n2020. 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 16\n[11] Liang-Chieh Chen, Raphael Gontijo Lopes, Bowen Cheng,\nMaxwell D Collins, Ekin D Cubuk, Barret Zoph, Hartwig\nAdam, and Jonathon Shlens. Naive-Student: Leveraging\nSemi-Supervised Learning in Video Sequences for Urban\nScene Segmentation. In ECCV, 2020. 2, 6, 7\n[12] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,\nKevin Murphy, and Alan L Yuille. Semantic image seg-\nmentation with deep convolutional nets and fully connected\ncrfs. In ICLR, 2015. 1, 3\n[13] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,\nKevin Murphy, and Alan L Yuille. Deeplab: Semantic im-\nage segmentation with deep convolutional nets, atrous con-\nvolution, and fully connected crfs. IEEE TPAMI, 2017. 3\n[14] Liang-Chieh Chen, George Papandreou, Florian Schroff,\nand Hartwig Adam. Rethinking atrous convolution for se-\nmantic image segmentation. arXiv:1706.05587, 2017. 3\n[15] Liang-Chieh Chen, Yi Yang, Jiang Wang, Wei Xu, and\nAlan L Yuille. Attention to scale: Scale-aware semantic\nimage segmentation. In CVPR, 2016. 3\n[16] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Flo-\nrian Schroff, and Hartwig Adam. Encoder-decoder with\natrous separable convolution for semantic image segmenta-\ntion. In ECCV, 2018. 3, 5\n[17] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\noffrey Hinton. A simple framework for contrastive learning\nof visual representations. In ICML, 2020. 6\n[18] Yunpeng Chen, Yannis Kalantidis, Jianshu Li, Shuicheng\nYan, and Jiashi Feng. AË† 2-nets: Double attention networks.\nIn NeurIPS, 2018. 3\n[19] Bowen Cheng, Liang-Chieh Chen, Yunchao Wei, Yukun\nZhu, Zilong Huang, Jinjun Xiong, Thomas S Huang, Wen-\nMei Hwu, and Honghui Shi. Spgnet: Semantic prediction\nguidance for scene parsing. In ICCV, 2019. 2, 5\n[20] Bowen Cheng, Maxwell D Collins, Yukun Zhu, Ting Liu,\nThomas S Huang, Hartwig Adam, and Liang-Chieh Chen.\nPanoptic-deeplab. In ICCV COCO + Mapillary Joint\nRecognition Challenge Workshop, 2019. 3\n[21] Bowen Cheng, Maxwell D Collins, Yukun Zhu, Ting Liu,\nThomas S Huang, Hartwig Adam, and Liang-Chieh Chen.\nPanoptic-DeepLab: A Simple, Strong, and Fast Baseline\nfor Bottom-Up Panoptic Segmentation. In CVPR, 2020. 1,\n2, 3, 5, 6, 7, 9, 11\n[22] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-\nterm memory-networks for machine reading. In EMNLP,\n2016. 2\n[23] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.\nGenerating long sequences with sparse transformers.\narXiv:1904.10509, 2019. 2\n[24] Franc Â¸ois Chollet. Xception: Deep learning with depthwise\nseparable convolutions. In CVPR, 2017. 7, 11\n[25] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong\nZhang, Han Hu, and Yichen Wei. Deformable convolu-\ntional networks. In ICCV, 2017. 7\n[26] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G Carbonell,\nQuoc Le, and Ruslan Salakhutdinov. Transformer-xl: At-\ntentive language models beyond a ï¬xed-length context. In\nACL, 2019. 2\n[27] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. BERT: Pre-training of deep bidirectional trans-\nformers for language understanding. In NAACL, 2019. 2\n[28] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold,\nSylvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv:2010.11929,\n2020. 3\n[29] Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhi-\nwei Fang, and Hanqing Lu. Dual attention network for\nscene segmentation. In CVPR, 2019. 3\n[30] Naiyu Gao, Yanhu Shan, Yupei Wang, Xin Zhao, Yinan Yu,\nMing Yang, and Kaiqi Huang. Ssap: Single-shot instance\nsegmentation with afï¬nity pyramid. In ICCV, 2019. 3\n[31] Ankit Gupta and Jonathan Berant. Gmat: Global memory\naugmentation for transformers. arXiv:2006.03274, 2020. 2\n[32] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\nGirshick. Momentum contrast for unsupervised visual rep-\nresentation learning. In CVPR, 2020. 6\n[33] Kaiming He, Georgia Gkioxari, Piotr Doll Â´ar, and Ross Gir-\nshick. Mask r-cnn. In ICCV, 2017. 1, 3\n[34] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR,\n2016. 6\n17\n[35] Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and\nTim Salimans. Axial attention in multidimensional trans-\nformers. arXiv:1912.12180, 2019. 3, 5\n[36] Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen\nWei. Relation networks for object detection. In CVPR,\n2018. 3\n[37] Han Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local\nrelation networks for image recognition. In ICCV, 2019. 3\n[38] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kil-\nian Q Weinberger. Deep networks with stochastic depth. In\nECCV, 2016. 6\n[39] Zilong Huang, Xinggang Wang, Lichao Huang, Chang\nHuang, Yunchao Wei, and Wenyu Liu. Ccnet: Criss-cross\nattention for semantic segmentation. In ICCV, 2019. 3, 5\n[40] Jyh-Jing Hwang, Stella X Yu, Jianbo Shi, Maxwell D\nCollins, Tien-Ju Yang, Xiao Zhang, and Liang-Chieh Chen.\nSegSort: Segmentation by discriminative sorting of seg-\nments. In ICCV, 2019. 6\n[41] Sergey Ioffe and Christian Szegedy. Batch normalization:\naccelerating deep network training by reducing internal co-\nvariate shift. In ICML, 2015. 5\n[42] Fabian Isensee, Jens Petersen, Andre Klein, David Zim-\nmerer, Paul F Jaeger, Simon Kohl, Jakob Wasserthal, Gre-\ngor Koehler, Tobias Norajitra, Sebastian Wirkert, et al. nnu-\nnet: Self-adapting framework for u-net-based medical im-\nage segmentation. arXiv:1809.10486, 2018. 6\n[43] Xu Jia, Bert De Brabandere, Tinne Tuytelaars, and Luc V\nGool. Dynamic ï¬lter networks. In NeurIPS, 2016. 5, 9\n[44] Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task\nlearning using uncertainty to weigh losses for scene geom-\netry and semantics. In CVPR, 2018. 3\n[45] Margret Keuper, Evgeny Levinkov, Nicolas Bonneel, Guil-\nlaume LavouÂ´e, Thomas Brox, and Bjorn Andres. Efï¬cient\ndecomposition of image and mesh graphs by lifted multi-\ncuts. In ICCV, 2015. 3\n[46] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna,\nYonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu,\nand Dilip Krishnan. Supervised contrastive learning. In\nNeurIPS, 2020. 6\n[47] Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr\nDollÂ´ar. Panoptic feature pyramid networks. InCVPR, 2019.\n1, 2, 3, 7\n[48] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten\nRother, and Piotr DollÂ´ar. Panoptic segmentation. In CVPR,\n2019. 1, 2, 3\n[49] Nikita Kitaev, Åukasz Kaiser, and Anselm Levskaya. Re-\nformer: The efï¬cient transformer. In ICLR, 2020. 2\n[50] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classiï¬cation with deep convolutional neural net-\nworks. In NeurIPS, 2012. 7, 11, 16\n[51] Harold W Kuhn. The hungarian method for the assignment\nproblem. Naval research logistics quarterly, 2(1-2):83â€“97,\n1955. 2, 4\n[52] Yann LeCun, L Â´eon Bottou, Yoshua Bengio, and Patrick\nHaffner. Gradient-based learning applied to document\nrecognition. Proceedings of the IEEE, 86(11):2278â€“2324,\n1998. 2\n[53] Bastian Leibe, Ales Leonardis, and Bernt Schiele. Com-\nbined object categorization and segmentation with an im-\nplicit shape model. In Workshop on statistical learning in\ncomputer vision, ECCV, 2004. 3\n[54] Jie Li, Allan Raventos, Arjun Bhargava, Takaaki Tagawa,\nand Adrien Gaidon. Learning to fuse things and stuff.\narXiv:1812.01192, 2018. 3\n[55] Qizhu Li, Xiaojuan Qi, and Philip HS Torr. Unifying train-\ning and inference for panoptic segmentation. In CVPR,\n2020. 3\n[56] Yanwei Li, Xinze Chen, Zheng Zhu, Lingxi Xie, Guan\nHuang, Dalong Du, and Xingang Wang. Attention-guided\nuniï¬ed network for panoptic segmentation. In CVPR, 2019.\n3\n[57] Yingwei Li, Xiaojie Jin, Jieru Mei, Xiaochen Lian, Linjie\nYang, Cihang Xie, Qihang Yu, Yuyin Zhou, Song Bai, and\nAlan Yuille. Neural architecture search for lightweight non-\nlocal networks. In CVPR, 2020. 3\n[58] Tsung-Yi Lin, Piotr Doll Â´ar, Ross Girshick, Kaiming He,\nBharath Hariharan, and Serge Belongie. Feature pyramid\nnetworks for object detection. In CVPR, 2017. 3, 5\n[59] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He,\nand Piotr Doll Â´ar. Focal loss for dense object detection. In\nICCV, 2017. 4\n[60] Tsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll Â´ar, and\nC Lawrence Zitnick. Microsoft coco: Common objects in\ncontext. In ECCV, 2014. 2, 6, 9, 16\n[61] Chenxi Liu, Liang-Chieh Chen, Florian Schroff, Hartwig\nAdam, Wei Hua, Alan Yuille, and Li Fei-Fei. Auto-deeplab:\nHierarchical neural architecture search for semantic image\nsegmentation. In CVPR, 2019. 5\n[62] Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen,\nXiaodong Liu, Jianfeng Gao, and Jiawei Han. On the vari-\nance of the adaptive learning rate and beyond. In ICLR,\n2020. 6\n[63] Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia.\nPath aggregation network for instance segmentation. In\nCVPR, 2018. 5\n[64] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian\nSzegedy, Scott Reed, Cheng-Yang Fu, and Alexander C\nBerg. Ssd: Single shot multibox detector. In ECCV, 2016.\n5\n[65] Yudong Liu, Yongtao Wang, Siwei Wang, TingTing Liang,\nQijie Zhao, Zhi Tang, and Haibin Ling. Cbnet: A novel\ncomposite backbone network architecture for object detec-\ntion. In AAAI, 2020. 7\n[66] Yiding Liu, Siyu Yang, Bin Li, Wengang Zhou, Jizheng Xu,\nHouqiang Li, and Yan Lu. Afï¬nity derivation and graph\nmerge for instance segmentation. In ECCV, 2018. 3\n[67] Huanyu Liu1, Chao Peng, Changqian Yu, Jingbo Wang, Xu\nLiu, Gang Yu, and Wei Jiang. An end-to-end network for\npanoptic segmentation. In CVPR, 2019. 3\n[68] Minh-Thang Luong, Hieu Pham, and Christopher D Man-\nning. Effective approaches to attention-based neural ma-\nchine translation. In EMNLP, 2015. 2\n18\n[69] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi.\nV-net: Fully convolutional neural networks for volumetric\nmedical image segmentation. In 3DV, 2016. 6\n[70] Davy Neven, Bert De Brabandere, Marc Proesmans, and\nLuc Van Gool. Instance segmentation by jointly optimizing\nspatial embeddings and clustering bandwidth. In CVPR,\n2019. 3\n[71] Alejandro Newell, Kaiyu Yang, and Jia Deng. Stacked\nhourglass networks for human pose estimation. In ECCV,\n2016. 2, 5\n[72] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Åukasz\nKaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Im-\nage transformer. In ICML, 2018. 3\n[73] Lorenzo Porzi, Samuel Rota Bul `o, Aleksander Colovic,\nand Peter Kontschieder. Seamless scene segmentation. In\nCVPR, 2019. 3\n[74] Haozhi Qi, Zheng Zhang, Bin Xiao, Han Hu, Bowen\nCheng, Yichen Wei, and Jifeng Dai. Deformable convo-\nlutional networks â€“ coco detection and segmentation chal-\nlenge 2017 entry. ICCV COCO Challenge Workshop, 2017.\n7\n[75] Siyuan Qiao, Liang-Chieh Chen, and Alan Yuille. Detec-\ntors: Detecting objects with recursive feature pyramid and\nswitchable atrous convolution. arXiv:2006.02334, 2020. 1,\n3, 5, 7, 9, 10, 12\n[76] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan\nBello, Anselm Levskaya, and Jon Shlens. Stand-alone self-\nattention in vision models. In NeurIPS, 2019. 3\n[77] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster r-cnn: Towards real-time object detection with re-\ngion proposal networks. In NeurIPS, 2015. 1\n[78] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nnet: Convolutional networks for biomedical image segmen-\ntation. In International Conference on Medical image com-\nputing and computer-assisted intervention, 2015. 2, 5\n[79] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-\nattention with relative position representations. In NAACL,\n2018. 2\n[80] Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi,\nand Hongsheng Li. Efï¬cient attention: Attention with lin-\near complexities. In WACV, 2021. 3\n[81] Konstantin Soï¬iuk, Olga Barinova, and Anton Konushin.\nAdaptis: Adaptive instance selection network. In ICCV,\n2019. 3\n[82] Russell Stewart, Mykhaylo Andriluka, and Andrew Y Ng.\nEnd-to-end people detection in crowded scenes. In CVPR,\n2016. 2, 4\n[83] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and\nAlexander A Alemi. Inception-v4, inception-resnet and the\nimpact of residual connections on learning. In AAAI, 2017.\n15\n[84] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon\nShlens, and Zbigniew Wojna. Rethinking the inception ar-\nchitecture for computer vision. In CVPR, 2016. 15\n[85] Saeid Asgari Taghanaki, Yefeng Zheng, S Kevin Zhou,\nBogdan Georgescu, Puneet Sharma, Daguang Xu, Dorin\nComaniciu, and Ghassan Hamarneh. Combo loss: Han-\ndling input and output imbalance in multi-organ segmenta-\ntion. Computerized Medical Imaging and Graphics, 75:24â€“\n33, 2019. 6\n[86] Mingxing Tan, Ruoming Pang, and Quoc V Le. Efï¬cient-\ndet: Scalable and efï¬cient object detection. In CVPR, 2020.\n5\n[87] Zhi Tian, Chunhua Shen, and Hao Chen. Conditional con-\nvolutions for instance segmentation. In ECCV, 2020. 5,\n9\n[88] Jonas Uhrig, Eike Rehder, Bj Â¨orn FrÂ¨ohlich, Uwe Franke, and\nThomas Brox. Box2pix: Single-shot instance segmentation\nby assigning pixels to object boxes. In IEEE Intelligent\nVehicles Symposium (IV), 2018. 3\n[89] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In NeurIPS,\n2017. 2\n[90] Luc Vincent and Pierre Soille. Watersheds in digital spaces:\nan efï¬cient algorithm based on immersion simulations.\nIEEE TPAMI, 1991. 3\n[91] Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam,\nAlan Yuille, and Liang-Chieh Chen. Axial-DeepLab:\nStand-Alone Axial-Attention for Panoptic Segmentation. In\nECCV, 2020. 1, 2, 3, 5, 6, 7, 9, 10, 12, 15, 16\n[92] Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and\nHao Ma. Linformer: Self-attention with linear complexity.\narXiv:2006.04768, 2020. 2\n[93] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-\ning He. Non-local neural networks. In CVPR, 2018. 3,\n5\n[94] Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and\nChunhua Shen. SOLOv2: Dynamic and fast instance seg-\nmentation. In NeurIPS, 2020. 5, 9\n[95] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen\nLo, and Ross Girshick. Detectron2. https://github.\ncom/facebookresearch/detectron2, 2019. 2, 7\n[96] Zhirong Wu, Alexei A Efros, and Stella X Yu. Improving\ngeneralization via scalable neighborhood component anal-\nysis. In ECCV, 2018. 6\n[97] Zifeng Wu, Chunhua Shen, and Anton Van Den Hengel.\nWider or deeper: Revisiting the ResNet model for visual\nrecognition. Pattern Recognition, 2019. 6, 7\n[98] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin.\nUnsupervised feature learning via non-parametric instance\ndiscrimination. In CVPR, 2018. 6\n[99] Saining Xie, Ross Girshick, Piotr Doll Â´ar, Zhuowen Tu, and\nKaiming He. Aggregated residual transformations for deep\nneural networks. In CVPR, 2017. 7\n[100] Yuwen Xiong, Renjie Liao, Hengshuang Zhao, Rui Hu,\nMin Bai, Ersin Yumer, and Raquel Urtasun. Upsnet: A\nuniï¬ed panoptic segmentation network. In CVPR, 2019. 1,\n2, 3, 6, 7\n[101] Brandon Yang, Gabriel Bender, Quoc V Le, and Jiquan\nNgiam. Condconv: Conditionally parameterized convolu-\ntions for efï¬cient inference. In NeurIPS, 2019. 5, 9\n19\n[102] Tien-Ju Yang, Maxwell D Collins, Yukun Zhu, Jyh-Jing\nHwang, Ting Liu, Xiao Zhang, Vivienne Sze, George Pa-\npandreou, and Liang-Chieh Chen. Deeperlab: Single-shot\nimage parser. arXiv:1902.05093, 2019. 2, 3\n[103] Yibo Yang, Hongyang Li, Xia Li, Qijie Zhao, Jianlong Wu,\nand Zhouchen Lin. Sognet: Scene overlap graph network\nfor panoptic segmentation. In AAAI, 2020. 3\n[104] Sergey Zagoruyko and Nikos Komodakis. Wide residual\nnetworks. In BMVC, 2016. 6, 7\n[105] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey,\nJoshua Ainslie, Chris Alberti, Santiago Ontanon, Philip\nPham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big\nbird: Transformers for longer sequences. InNeurIPS, 2020.\n2\n[106] Michael Zhang, James Lucas, Jimmy Ba, and Geoffrey E\nHinton. Lookahead optimizer: k steps forward, 1 step back.\nIn NeurIPS, 2019. 6\n[107] Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and\nStan Z Li. Bridging the gap between anchor-based and\nanchor-free detection via adaptive training sample selec-\ntion. In CVPR, 2020. 1\n[108] Hengshuang Zhao, Yi Zhang, Shu Liu, Jianping Shi, Chen\nChange Loy, Dahua Lin, and Jiaya Jia. Psanet: Point-wise\nspatial attention network for scene parsing. InECCV, 2018.\n3\n[109] Xizhou Zhu, Dazhi Cheng, Zheng Zhang, Stephen Lin, and\nJifeng Dai. An empirical study of spatial attention mecha-\nnisms in deep networks. In ICCV, pages 6688â€“6697, 2019.\n3\n[110] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,\nand Jifeng Dai. Deformable detr: Deformable transformers\nfor end-to-end object detection. arXiv:2010.04159, 2020. 3\n[111] Zhen Zhu, Mengde Xu, Song Bai, Tengteng Huang, and\nXiang Bai. Asymmetric non-local neural networks for se-\nmantic segmentation. In CVPR, 2019. 3\n20",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8018243312835693
    },
    {
      "name": "Transformer",
      "score": 0.5977417826652527
    },
    {
      "name": "Path (computing)",
      "score": 0.4794020652770996
    },
    {
      "name": "Segmentation",
      "score": 0.453624427318573
    },
    {
      "name": "Program code",
      "score": 0.4511427879333496
    },
    {
      "name": "Bipartite graph",
      "score": 0.4285276532173157
    },
    {
      "name": "Algorithm",
      "score": 0.37108922004699707
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3563167452812195
    },
    {
      "name": "Theoretical computer science",
      "score": 0.23818504810333252
    },
    {
      "name": "Graph",
      "score": 0.1928744912147522
    },
    {
      "name": "Voltage",
      "score": 0.16910549998283386
    },
    {
      "name": "Programming language",
      "score": 0.08485901355743408
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": []
}