{
  "title": "CLE-ViT: Contrastive Learning Encoded Transformer for Ultra-Fine-Grained Visual Categorization",
  "url": "https://openalex.org/W4385768220",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2096505223",
      "name": "Xiaohan Yu",
      "affiliations": [
        "Griffith University"
      ]
    },
    {
      "id": "https://openalex.org/A754385464",
      "name": "Jun Wang",
      "affiliations": [
        "University of Warwick"
      ]
    },
    {
      "id": "https://openalex.org/A2119632639",
      "name": "Yongsheng Gao",
      "affiliations": [
        "Griffith University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6863994431",
    "https://openalex.org/W2961018736",
    "https://openalex.org/W6763878013",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W343636949",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3009067796",
    "https://openalex.org/W2921985587",
    "https://openalex.org/W2785325870",
    "https://openalex.org/W3035060554",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W6778446171",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2086377778",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W2606520630",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W6864014924",
    "https://openalex.org/W2989646560",
    "https://openalex.org/W4223574620",
    "https://openalex.org/W4308737504",
    "https://openalex.org/W2944223741",
    "https://openalex.org/W2998619563",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W3175613352",
    "https://openalex.org/W3108870912",
    "https://openalex.org/W3139434170",
    "https://openalex.org/W2997737174",
    "https://openalex.org/W2963045696",
    "https://openalex.org/W3170284232",
    "https://openalex.org/W1797268635",
    "https://openalex.org/W2746314669",
    "https://openalex.org/W2950557962",
    "https://openalex.org/W4239072543",
    "https://openalex.org/W4297808394",
    "https://openalex.org/W2963393555",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3009561768",
    "https://openalex.org/W3178389175",
    "https://openalex.org/W3005680577",
    "https://openalex.org/W3034676907",
    "https://openalex.org/W3203876692",
    "https://openalex.org/W3088127670",
    "https://openalex.org/W2992308087"
  ],
  "abstract": "Ultra-fine-grained visual classification (ultra-FGVC) targets at classifying sub-grained categories of fine-grained objects. This inevitably requires discriminative representation learning within a limited training set. Exploring intrinsic features from the object itself, e.g., predicting the rotation of a given image, has demonstrated great progress towards learning discriminative representation. Yet none of these works consider explicit supervision for learning mutual information at instance level. To this end, this paper introduces CLE-ViT, a novel contrastive learning encoded transformer, to address the fundamental problem in ultra-FGVC. The core design is a self-supervised module that performs self-shuffling and masking and then distinguishes these altered images from other images. This drives the model to learn an optimized feature space that has a large inter-class distance while remaining tolerant to intra-class variations. By incorporating this self-supervised module, the network acquires more knowledge from the intrinsic structure of the input data, which improves the generalization ability without requiring extra manual annotations. CLE-ViT demonstrates strong performance on 7 publicly available datasets, demonstrating its effectiveness in the ultra-FGVC task. The code is available at https://github.com/Markin-Wang/CLEViT.",
  "full_text": "CLE-ViT: Contrastive Learning Encoded Transformer for Ultra-Fine-Grained\nVisual Categorization\nXiaohan Yu1 , Jun Wang2 and Yongsheng Gao1\n1School of Engineering and Built Environment, Griffith University, Australia\n2Department of Computer Science, University of Warwick, UK\n{xiaohan.yu, yongsheng.gao}@griffith.edu.au, jun.wang.3@warwick.ac.uk\nAbstract\nUltra-fine-grained visual classification (ultra-\nFGVC) targets at classifying sub-grained cate-\ngories of fine-grained objects. This inevitably\nrequires discriminative representation learning\nwithin a limited training set. Exploring intrinsic\nfeatures from the object itself, e.g., predicting the\nrotation of a given image, has demonstrated great\nprogress towards learning discriminative represen-\ntation. Yet none of these works consider explicit\nsupervision for learning mutual information at\ninstance level. To this end, this paper introduces\nCLE-ViT, a novel contrastive learning encoded\ntransformer, to address the fundamental problem in\nultra-FGVC. The core design is a self-supervised\nmodule that performs self-shuffling and masking\nand then distinguishes these altered images from\nother images. This drives the model to learn an\noptimized feature space that has a large inter-class\ndistance while remaining tolerant to intra-class\nvariations. By incorporating this self-supervised\nmodule, the network acquires more knowledge\nfrom the intrinsic structure of the input data,\nwhich improves the generalization ability without\nrequiring extra manual annotations. CLE-ViT\ndemonstrates strong performance on 7 publicly\navailable datasets, demonstrating its effectiveness\nin the ultra-FGVC task. The code is available at\nhttps://github.com/Markin-Wang/CLEViT\n1 Introduction\nUltra-fine-grained visual categorization (ultra-FGVC) distin-\nguishes a sub-category of images from a single fine-grained\ncategory. As an emerging topic, ultra-FGVC demonstrates\npotential in artificial intelligence agriculture and smart farm-\ning e.g., automatic crop cultivar classification and plant dis-\nease classification [Yu et al., 2020; Larese et al., 2014;\nYu et al., 2023]. The intrinsic challenge of ultra-FGVC lies\nin that very limited samples are provided due to the granu-\nlarity further moving down to a sub-category level [Huang\nand Li, 2020 ]. Another key observation is that the visual\nvariances are difficult to distinguish among different classes,\nwhile intra-class variances can be very large (see Figure 2 as\nFeature Space\nSamples (A)\nSamples (B)\nGenerated Samples (A/B)\nFigure 1: An example of illustrating the process of learning more\ngeneralized feature space via incorporating intra-class variations. By\ngenerating more diversified samples (from left to right in the first\nrow), the model is required to reshape the feature space to be more\ntolerant to larger intra-class variations (top to bottom in the right\ncolumn). This enables a more generalized feature space to adapt to\nnew (testing) samples.\nan example). Thus how to learn discriminative representation\nwithin limited training samples becomes a core question in\nultra-FGVC.\nRecent progress has been made by incorporating self-\nsupervised learning to jointly optimize the objectives of rep-\nresentation learning. This is achieved by training with some\npredefined pretext tasks to drive the model to better under-\nstand the intrinsic feature of the data itself. For instance,\nlearning a representation by training a model to predict the\nrotation [Gidaris et al., 2018] or spatial context [Doersch et\nal., 2015] of input images. Several works [Yu et al., 2023;\nYuet al., 2022] have demonstrated such a strategy can lead to\nsignificant performance gain. However, none of these works\nconsider explicit supervision for learning mutual informa-\ntion at instance level. This motivates us to introduce self-\nsupervised instance-level contrastive learning to gain a more\ndiscriminative representation via understanding mutual infor-\nmation between augmented views of a single image.\nIn this paper, we introduce CLE-ViT, a novel contrastive\nlearning encoded transformer, to address the intrinsic chal-\nlenges of ultra-FGVC. The core design is a self-supervised\nmodule that performs self-shuffling and masking and then\ndistinguishes these altered image views from other images.\nThis drives the model to learn an optimized feature space that\nhas a large inter-class distance while remaining tolerant to\nintra-class variations. By incorporating this self-supervised\nmodule, the network acquires more knowledge from the in-\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n4531\nFigure 2: An example of illustrating large intra-class variations in\nultra-fine-grained image datasets. The top row shows six images\nfrom the same category in the SoyLocal dataset. The bottom row\nshows six images from the same category in the Cotton80 dataset.\ntrinsic structure of the input data, thus improving the general-\nization ability with limited training samples.\nCLE-ViT achieves superior (or comparable) performance\non five ultra-fine-grained datasets, one plant disease dataset,\nand the CUB-200-2011 dataset. With promising performance\non crop cultivar classification and plant disease classification,\nthe proposed CLE-ViT may provide a promising solution to\npushing forward the progress in smart farming. The main\ncontributions of this paper are summarized as follows:\n1) We introduce CLE-ViT, a novel vision transformer\nmodel that performs instance-level contrastive learning to-\nwards discriminative classification with limited training data.\n2) CLE-ViT learns an optimized feature space that has a\nlarge inter-class distance while remaining tolerant to intra-\nclass variance.\n3) CLE-ViT achieves strong performance on five ultra-fine-\ngrained datasets and two fine-grained datasets, demonstrating\nits effectiveness for ultra-FGVC.\n2 Related Work\nUltra-fine-grained visual categorization (ultra-FGVC) iden-\ntifies objects at a very fine granularity where even humans\nfeel difficult to accurately describe the visual difference. In\ncomparison with fine-grained visual categorization (FGVC),\nultra-FGVC has two unique properties/challenges: 1) the an-\nnotations are not labeled by human experts or volunteers but\nobtained from genetic source bank [Yu et al., 2021b ]; 2)\nthe classification granularity has moved from species level\n(FGVC) to a subordinate level, i.e., cultivar level [Yu et al.,\n2020]. An example of comparing ultra-FGVC and FGVC\nis shown in Figure 2. FGVC aims to distinguish between\nimages from the top row and those from the bottom row,\nwhile ultra-FGVC classifies different images from a single\nrow. This challenging research topic is attracting increasing\nattention for its significant potential in artificial intelligence\nagriculture and smart farming.\nEarlier exploration started with a small ultra-fine-grained\nimage dataset, which contains 600 images of 100 soybean\ncultivars [Yu et al., 2020]. Despite encouraging performance\non this challenging dataset, their proposed feature modeling\nmethod requires a manually segmented vein structure that is\ninherently difficult to use for practical applications. Recently,\n[Yu et al., 2021b] released a benchmark platform with base-\nline performances of 13 state-of-the-art CNN methods on a\nlarge-scale ultra-fine-grained cultivar leaf dataset including in\nFeature Space\nSamples (A)\nSamples (B)\nGenerated Samples (B)\nFigure 3: An example of enhancing the generalization capability by\nenlarging inter-class distance. By generating more diversified sam-\nples (from left to right in the first row), the model reshapes the fea-\nture space to ensure these samples are aligned to their ground-truth\ncategory (top to bottom in the right column). Thus a more separated\nfeature space is formed which better adapts to new (testing) samples.\ntotal of 47,114 leaf images from two plant species and 3,526\ndifferent cultivars. [Yuet al., 2021a] proposed a random mask\ncovariance network (MaskCOV) to learn discriminative rep-\nresentation for ultra-FGVC. The MaskCOV randomly shuf-\nfles and masks out image patches, and then predicts the origi-\nnal position of each patch via a self-learning module. Despite\nits state-of-the-art performance on the ultra-FGVC tasks, the\nMaskCOV together with all the baseline methods in [Yu et\nal., 2021b] are all CNNs.\nTransformer [Vaswani et al., 2017] was first developed on\nnatural language processing and is now gaining increasing at-\ntention due to its effectiveness in extensive computer-vision\ntasks. Especially the Vision transformer (ViT) [Dosovitskiy\net al., 2020] which adopted a pure transformer directly to deal\nwith sequences of image patches, has now demonstrated very\ncompetitive performance in image classification. Yet ViT-\nbased methods require a large-scale dataset for model pre-\ntraining. To that end, [Touvron et al., 2021] introduced DeiT,\nthat employed a teacher-student strategy to speed up ViT\ntraining. Transformer models were further applied to other\npopular computer vision tasks. TransFG and FFVT [He et\nal., 2022; Wanget al., 2021a] proposed to use a few important\ntokens for final classification and explored ViT in the context\nof fine-grained visual classification. More recently, Mix-ViT\n[Yu et al., 2023] and SPARE [Yu et al., 2022] both introduce\npredefined pretext tasks as a supervision signal for implicit\nself-supervised learning in ultra-FGVC. Mix-ViT predicts the\nposition of mixed tokens. SPARE classifies masked seman-\ntic part regions. None of them consider explicit supervision\nfor learning mutual information at the stance level. In con-\ntrast, we develop explicit supervision for learning mutual in-\nformation at the instance level. Our proposed self-supervised\ninstance-level contrastive learning enables a desirable feature\nspace that has a large inter-class distance while remaining tol-\nerant to intra-class variance.\n3 Methods\n3.1 Overview & Motivation\nOverview. Figure 4 illustrates the overall framework of the\nproposed CLE-ViT. A given image is first projected into two\ndifferent views via the following operations: 1) standard aug-\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n4532\nClassification\nSelf-Supervised Contrastive Learning\nTriplet loss\nPositive Pair\nNegative Pair\nBackbone\nInter-Class Discriminative\nIntra-Class Tolerant\nFigure 4: Overview of the proposed method. A given image is first projected into two different views via the following operations: 1)\nstandard augmentation and 2) random shuffling and masking. Then the two views are sent to the backbone network for feature extraction. In\naddition to the standard classification optimization, their output features form a positive pair, while remaining images from the same batch\nform negative samples for self-supervised contrastive learning.\nmentation and 2) random shuffling and masking. Then the\ntwo views are sent to the backbone network for feature ex-\ntraction. In addition to the standard classification optimiza-\ntion, their output features form a positive pair, while remain-\ning images from the same batch form negative samples for\nself-supervised contrastive learning. The whole network is\ntrained in an end-to-end manner. The self-supervised module\nis detached in the inference stage.\nMotivation. A desirable learned feature space should be\ntolerant to intra-class variations. Once the model is overfit-\nting the training samples from the same category, the asso-\nciated feature space of this class may be closely clustered\n(as demonstrated in Figure 1). This may hurt the general-\nization capability as the new (testing) samples may not fall\ninto such a concentrated cluster due to the intrinsic intra-class\nvariations in ultra-fine-grained samples (see Figure 2). To\nthat end, we propose to conduct the contrastive learning on\ninstance-level, instead of the class level in previous works[He\net al., 2022; Wang et al., 2021b]. On one hand, this creates\nmore diversified training samples to enable a more intra-class\nvariance-tolerant feature space such that the training process\nbecomes less likely to overfit. On the other hand, the diversi-\nfied samples may also enlarge the feature space distance be-\ntween different categories (see Figure 3).\n3.2 Image Classification\nGiven an input image I ∈ RH×W×3 and its associated one-\nhot category label y ∈ R1×Nc , we firstly employ a feature\nextractor, e.g., ResNet50 [Simonyan and Zisserman, 2015 ]\nand Swin Transformer [Liu et al., 2021], to obtain its patch\nfeatures V ∈ RNp×D and global feature representation u ∈\nR1×D which is used to undertake the category categorization\nvia the classification head. Note that theH, W, Nc, Np, Dare\nthe height, and width of the image, the number of total classes\nin the datasets, the number of patches in the final stage, and\nthe feature dimension of the final global features respectively.\nThis process can be expressed as:\n{v1, v2, ...,vi, ...,vNp−1 , vNp } = fife (I), (1)\np = σ(Head(vg)), u = 1\nNp\n1X\ni=0\nvi (2)\nWhere σ denotes the softmax function and p is the probabil-\nity distribution given by the backbone model. fife refers to\nthe image feature extractor. After obtaining the category pre-\ndiction, the model is normally optimized by a Cross-Entropy\nloss in an end-to-end manner:\nLcls = − 1\nNc\nNcX\ni=1\nyi · log(pi). (3)\n3.3 Instance-level Contrastive Learning\nThe proposed instance-level contrastive learning module is\ntrained in a self-supervised manner as it is formed with-\nout the need for any extra manual label information. When\nthere are only limited training samples, incorporating such\nself-supervised tasks can improve the representation learning\nwithout requiring extra annotations.\nPositive Pair Construction\nThe anchor image is obtained by applying the standard data\naugmentation methods, e.g., Random Horizontal Flip, and\nRandom Rotation to the input image. To form the positive\nsample for the anchor image, we first follow the same stan-\ndard data augmentation methods as the anchor image to gen-\nerate the base image, and perform a strong data augmentation,\ni.e., randomly mask out a proportion of pixels. Specifically,\ngiven the anchor image Ia ∈ RH×W×3, the process of ob-\ntaining the positive sample Ip∗ ∈ RH×W×3 can be formu-\nlated by:\nIp∗(i, j) =\n\u001aIa(i, j) if(i, j) ∈ H\n0 if(i, j) /∈ H , (4)\nH = { (i, j) | l < i <= l + k, m < j <= m + t}, (5)\n, where (i, j) denotes the position index in the images and\nH, is the set containing all the pixel positions to be masked.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n4533\nFigure 5: An example of illustrating masked and shuffled samples\nin positive image pair generation.\nk and tdetermine the masked size and are randomly selected.\nThese parameters are controlled by a term α = k∗t\nH∗W , the\nproportion of masked region.\nHowever, the model can easily infer the original informa-\ntion of masked pixels from the anchor image containing the\nmasked regions within minor transformation in standard aug-\nmentation. To this end, we propose to randomly shuffle the\npositive image to enhance the difficulty in not only acquiring\nthe masked information from the anchor image but also rec-\nognizing the category of the image. In particular, we evenly\nsplit the image into n = s ∗s part where s is an integer larger\nthan 1, then the masked image can be reformulated as:\nIp∗ =\n\n\nP(1,1) P(1,2) ··· P(1,s)\nP(2,1) P(2,2) ··· P(2,s)\n..\n. .\n.\n. ... .\n.\n.\nP(s,1) P(s,2) ··· P(s,s)\n\n, (6)\nwhere we represent the P(i, j) as the image patch in ith row\nand jth column. The final positive sample Ip is obtained by\nrandomly shuffling the n parts in Ip∗. An example of gener-\nating Ip is shown in Figure 5.\nNegative Pair Construction\nSince we aim to establish instance-level contrastive learning,\nthe negative sample can be any other sample in the same\nbatch. In detail, for each sample, we randomly pick up one\nanchor image (excluding itself) from the batch as its negative\nsample In to form the negative pair < Ia, In >.\nInstance-level Contrastive Learning\nAfter obtaining the positive pair < Ia, Ip > and negative\npair < Ia, In >, the next problem is how to conduct the con-\ntrastive learning. Here, we employ the triplet loss as our con-\ntrastive learning metric. Specifically, let’s denote the global\nfeatures of the anchor image, its positive sample and nega-\ntive sample as ua, up and un respectively, our instance-level\ntriplet loss is formulated as:\nLicl = 1\nB\nBX\ni=1\nmax(σ(ui\na − uj\np) − σ(ui\na − uj\nn) +β, 0),\n(7)\nwhere B is the number of samples in one batch and i de-\nnotes the ith sample in the batch. β refers to the margin to\ncontrol the difficulty in contrastive learning. σ is the L2 nor-\nmalization operation. Note that our proposed self-supervised\napproach can be added to any triple losses.\nGranularity Dataset #Class #Train #Test\nUltra-fine-grained\nCotton80 80 240 240\nSoyLocal 200 600 600\nSoyGene 1,110 12,763 11,143\nSoyAgeing 198 4,950 4,950\nSoyGlobal 1,938 5,814 5,814\nFine-grained CUB-200-2011 200 5,994 5,794\nApple Foliar disease 4 1,366 455\nTable 1: Statistics of the benchmark datasets.\n3.4 Objective Function\nThe model is jointly optimized by the Cross-Entropy Loss\n(Equation 3) and the proposed instance-level contrastive loss\n(Equation 7). We denote the Cross-Entropy Loss on anchor\nimages as Lclsa and its positive images as Lclsp. Then, the\nfinal objective function can be expressed as:\nLfnl = Lclsa + λLclsp + γLicl, (8)\nwhere λ and γ are the hyper-parameters to control the con-\ntribution among the classification loss on anchor and positive\nimages, and the self-supervised loss.\n3.5 Discussion\nWhy instance-level contrastive learning? To enable\ninstance-level contrastive learning, the first step is to per-\nform in-image augmentation to generate instance-level pos-\nitive pairs. Such in-image augmentation creates more diver-\nsified samples such that the training process is less likely to\noverfit. More importantly, distinguishing different views of\nthe same sample from other samples can reshape the learned\nfeature space to have a larger distance between different cat-\negories and within the same category. As such, the learned\nfeature space is more generalized to adapt to new samples.\nMoreover, we also perform class-level contrastive learning as\nan ablation study for a comprehensive evaluation (will be dis-\ncussed in Section 4.4).\nWhy triplet loss? Triplet loss and infoNCE loss [Oord et\nal., 2018] share a similar spirit of separating positive samples\nfrom negative samples. Yet infoNCE loss treats each neg-\native sample as a unique category which is less applicable\nin instance-level setting given a number of more than 10K\ninstances (categories) for some benchmarks. In addition, ap-\nplying triplet loss focuses on optimizing the distance between\npositive samples and negative samples rather than urging the\nmodel to map the positive pairs to the same points in In-\nfoNCE, thus can avoid overfitting and enhance generalization\ncapability.\n4 Experiments\n4.1 Datasets & Benchemark Methods\nFollowing [Yu et al., 2023 ], five ultra-fine-grained image\ndatasets are adopted for evaluation including Cotton80, Soy-\nLocal, SoyGene, SoyAgeing and SoyGlobal. Moreover, two\nfine-grained datasets, Apple Foliar disease dataset [Thapa et\nal., 2020] and CUB-200-2011 (CUB) [Wah et al., 2011] are\nalso used to further verify the effectiveness of the proposed\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n4534\nMethod Backbone Top 1 Accuracy (%)\nCotton S.Loc S.Gene S.Age S.Glo A.F.\nAlexnet [Krizhevsky et al., 2012] Alexnet 22.92 19.50 13.12 44.93 13.21 95.16\nVGG-16 [Simonyan and Zisserman, 2015] VGG-16 39.33 39.33 63.54 70.44 45.17 95.60\nResNet-50 [He et al., 2016] ResNet-50 52.50 38.83 70.21 67.15 25.59 94.73\nSimCLR (FT) [Chen et al., 2020a] ResNet-50 51.67 37.33 62.68 64.73 42.54 93.63\nSimCLR (L) [Chen et al., 2020a] ResNet-50 41.25 29.17 29.62 46.18 13.48 82.86\nMoCo v2 (FT) [Chen et al., 2020b] ResNet-50 45.00 32.67 56.49 59.13 29.26 96.04\nMoCo v2 (L) [Chen et al., 2020b] ResNet-50 30.42 27.67 26.58 38.26 12.99 85.49\nBYOL (FT) [Grill et al., 2020] ResNet-50 52.92 33.17 60.65 64.75 41.35 96.04\nBYOL (L) [Grill et al., 2020] ResNet-50 47.92 25.50 35.13 49.53 18.44 87.03\nCutout (8) [DeVries and Taylor, 2017] ResNet-50 55.83 37.67 61.12 65.70 47.06 94.95\nCutout (16) [DeVries and Taylor, 2017] ResNet-50 54.58 31.67 62.46 63.68 44.65 94.95\nHide and Seek [Singh and Lee, 2017] ResNet-50 48.33 28.00 61.27 60.48 23.74 96.26\nADL (0.5) [Choe and Shim, 2019] ResNet-50 43.75 34.67 55.19 61.70 39.35 96.04\nADL (0.25) [Choe and Shim, 2019] ResNet-50 40.83 28.00 52.18 51.56 29.50 94.51\nCutmix [Yun et al., 2019] ResNet-50 45.00 26.33 66.39 62.68 30.31 93.19\nDCL [Chen et al., 2019] ResNet-50 53.75 45.33 71.41 73.19 42.21 94.73\nMaskCOV [Yu et al., 2021a] ResNet-50 58.75 46.17 73.57 75.86 50.28 95.82\nSPARE [Yu et al., 2022] ResNet-50 60.42 44.67 79.41 75.72 56.45 96.70\nViT [Dosovitskiy et al., 2020] Transformer 52.50 38.83 53.63 66.95 40.57 96.48\nDeiT [Touvron et al., 2021] Transformer 54.17 38.67 66.80 69.54 45.34 96.26\nTransFG [He et al., 2022] Transformer 54.58 40.67 22.38 72.16 21.24 97.14\nHybrid ViT [Dosovitskiy et al., 2020] Transformer&ResNet 50.83 37.00 71.74 73.56 18.82 96.48\nMix-ViT [Yu et al., 2023] Transformer&ResNet 60.42 56.17 79.94 76.30 51.00 97.36\nProposed Method Transformer 63.33 47.17 78.50 82.14 75.21 97.58\nTable 2: The classification accuracies on the benchmark datasets. The results of the best-performing method are in boldface, while the\nsecond-best performances are underlined. Here Cotton represents Cotton80, S.Local represents SoyLocal, S.Gene represents SoyGene,\nS.Age represents SoyAgeing, S.Glo represents SoyGlobal and A.F. represents Apple Foliar disease. L and FT indicates linear and fine-tuning\nevaluation, respectively.\nmethod. Table 1 summarizes the statistics of benchmark\ndatasets, i.e., the numbers of classes, training images, and\ntesting images. For fair comparisons, the proposed CLE-ViT\nis compared with the same 17 benchmark methods as adopted\nin [Yu et al., 2023] for comprehensive evaluations.\n4.2 Implementation\nModel Details. We adopt the Swin Transformer Base (Swin-\nB) [Liu et al., 2021] as our backbone model by taking both\nprecision and efficiency into consideration. The same as the\nmost transformer-based works [Yu et al., 2023; Touvron et\nal., 2021], our backbone is initialized by the ImageNet21K\n[Deng et al., 2009] pre-trained model. The proportion of\nthe masked region and the number of parts n and are set to\n[0.15, 0.45] and 4 respectively. The margin β in Equation 7 is\n1. λ and γ are both set to 1 for all datasets except 0.3 and 0.5\nfor CUB dataset.\nTraining and inference. Following the [He et al., 2022;\nTouvron et al., 2021; Wang et al., 2021a], input images are\nfirst resized to 600×600 for all datasets. Random (Center)\ncropping is then applied to crop the images into 448×448\nduring the training (inference) phase. After that, we adopt\nrandom horizontal flipping, color jitter, and random rotation\nduring the training. The standard augmentation consists of\nthe aforementioned transformations. The whole architecture\nis optimized by AdamW optimizer. In our experiment set-\ntings, the batch size and the learning rate are set to 12 and\n1e-3 for all the datasets.\n4.3 Comparison to The State-of-The-Arts\nEvaluation on ultra-fine-grained image datasets. Table 2\nlists the classification accuracy of all the competing meth-\nods and their backbone networks on five ultra-fine-grained\ndatasets. The proposed CLE-ViT achieves 75.21% classi-\nfication accuracy on the SoyGlobal dataset, outperforming\nother competing methods with a significant margin (more\nthan 19%). We also observe a similar trend in SoyAgeing,\nwhere the proposed method surpasses other competing meth-\nods with a margin of more than 5% of classification accu-\nracy. Among the ultra-fine-grained image datasets, the Soy-\nAgeing dataset covers five subsets and each subset contains\nimages collected from a specific cultivating stage. The com-\nparison results of all competing methods on the five subsets\nare summarized in Table 3. The proposed method achieves\nstrong performance compared with other competing methods,\ndemonstrating its effectiveness in ultra-FGVC tasks.\nEvaluation on fine-grained datasets. we present evaluation\nresults in Table 4 of the proposed CLE-ViT on the widely\nused fine-grained image dataset, CUB-200-2011 [Wah et al.,\n2011]. CLE-ViT achieves competitive performance (ranked\n2nd) among the state-of-the-art methods on CUB-200-2011,\ndemonstrating its effectiveness and generalization capability\nin fine-grained visual classification. To further verify the ef-\nfectiveness of the proposed method, we evaluate a plant dis-\nease classification image dataset, the Apple Foliar disease\ndataset. The comparison results are summarized in Table 2.\nThe proposed method achieves the best classification accu-\nracy of 97.58% on the Apple Foliar disease dataset.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n4535\nMethod Backbone Top 1 Accuracy (%)\nR1 R3 R4 R5 R6 Avg\nAlexnet [Krizhevsky et al., 2012] Alexnet 49.90 44.65 45.15 47.47 37.47 44.93\nVGG-16 [Simonyan and Zisserman, 2015] VGG-16 72.32 72.53 74.95 71.11 61.31 70.44\nResNet-50 [He et al., 2016] ResNet-50 70.00 64.24 74.04 72.63 54.85 67.15\nSimCLR (L) [Chen et al., 2020a] ResNet-50 53.64 45.66 45.35 50.40 35.86 46.18\nSimCLR (FT) [Chen et al., 2020a] ResNet-50 70.00 66.57 64.24 68.38 54.44 64.73\nMoCo v2 (L) [Chen et al., 2020b] ResNet-50 42.93 38.59 38.99 38.99 31.82 38.26\nMoCo v2 (FT) [Chen et al., 2020b] ResNet-50 62.73 56.16 61.31 65.96 49.49 59.13\nBYOL (L) [Grill et al., 2020] ResNet-50 55.35 48.38 50.40 49.60 43.94 49.53\nBYOL (FT) [Grill et al., 2020] ResNet-50 71.11 66.16 65.76 64.65 56.06 64.75\nCutout (16) [DeVries and Taylor, 2017] ResNet-50 70.20 61.92 62.32 69.70 54.24 63.68\nCutout (8) [DeVries and Taylor, 2017] ResNet-50 66.87 64.04 67.78 73.43 56.36 65.70\nHide and Seek [Singh and Lee, 2017] ResNet-50 64.04 58.99 61.31 64.75 53.33 60.48\nADL (0.25) [Choe and Shim, 2019] ResNet-50 53.54 54.34 55.15 52.83 41.92 51.56\nADL (0.5) [Choe and Shim, 2019] ResNet-50 66.67 58.89 64.75 68.48 49.70 61.70\nCutmix [Yun et al., 2019] ResNet-50 65.56 59.19 64.24 68.79 53.64 62.28\nDCL [Chen et al., 2019] ResNet-50 76.87 73.84 76.16 76.16 62.93 73.19\nMaskCOV [Yu et al., 2021a] ResNet-50 79.80 74.65 79.60 78.28 66.97 75.86\nSPARE [Yu et al., 2022] ResNet-50 78.28 79.90 78.69 77.27 64.44 75.72\nViT [Dosovitskiy et al., 2020] Transformer 69.29 64.55 70.40 71.01 59.49 66.95\nDeiT [Touvron et al., 2021] Transformer 73.03 70.40 69.09 74.65 60.51 69.54\nTransFG [He et al., 2022] Transformer 74.95 74.55 74.24 76.26 60.81 72.16\nHybrid ViT [Dosovitskiy et al., 2020] Transformer&ResNet 77.17 76.97 74.75 76.36 62.53 73.56\nMix-ViT [Yu et al., 2023] Transformer&ResNet 79.29 77.17 77.98 79.19 67.88 76.30\nProposed Method Transformer 80.81 83.33 84.24 86.36 75.96 82.14\nTable 3: The classification accuracies of the competing methods on the five subsets of the SoyAgeing dataset. “Avg” denotes the average\nclassification accuracy of the five subsets. The results of the best-performing method are in boldface, while the second-best performances are\nunderlined. L indicates linear evaluation. FT denotes fine-tuning evaluation.\nFigure 6: Visual comparison of Mix-ViT (middle) and the proposed CLE-ViT (bottom) with original images (top) on CUB dataset.\n(a) Baseline (b) Baseline+Class Contrastive (c) Ours\nFigure 7: tSNE visualization of learned features from (a) Baseline and (b) Baseline + Class Contrastive and (c) Ours on the SoyGlobal dataset.\nEach color indicates a unique category (10 categories in total). In comparison with Baseline and Baseline+Class Contrastive, the proposed\nmethod learns a more optimized feature space that has a large inter-class distance while remaining tolerant to intra-class variance.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n4536\nMethod Backbone Accuracy (%)\nPMG [Du et al., 2020] ResNet50 89.6\nDCL [Chen et al., 2019] ResNet-50 87.8\nMaskCOV [Yu et al., 2021a] ResNet-50 86.6\nSPARE [Yu et al., 2022] ResNet-50 86.8\nAPI-Net [Zhuang et al., 2020] DenseNet161 90.0\nStackedLSTM [Ge et al., 2019] GoogleNet 90.4\nTransFG [He et al., 2022] Transformer 91.7\nDeiT [Touvron et al., 2021] Transformer 90.0\nViT [Dosovitskiy et al., 2020] Transformer 90.6\nMix-ViT [Yu et al., 2023] Transformer 91.0\nProposed Method Transformer 91.2\nTable 4: The classification accuracies on the CUB-200-2011.\nVisualization. We visualize the attention maps of Mix-ViT\n[Yu et al., 2023] and the proposed CLE-ViT on the CUB\ndataset [Wah et al., 2011 ] in Figure 6, where highlighted\nregions contribute significantly to the visual categorization.\nWe observe that the attentive regions of birds consistently\ncover unique patterns, verifying the reliability of the effec-\ntiveness. This is consistent with human observations and\ncommon sense that visual features from those unique patterns\nare vital for the determination of bird species.\n4.4 Ablation Study & Analysis\nRole of the self-supervised contrastive learning mod-\nule. To further verify the contribution of the proposed self-\nsupervised contrastive learning module, we present a com-\nprehensive ablation on 4 benchmark datasets. The baseline\nremoves the self-supervised module from the CLE-ViT. In ad-\ndition, contrastive learning can also be used at class level,i.e.,\nall images from the same category are used to form positive\npairs while remaining images from other categories are neg-\native samples. For a comprehensive evaluation, we replace\nthe instance-level contrastive learning with a standard class-\nlevel contrastive learning, denoted as Baseline+Class Con-\ntrastive (Baseline+CC). The comparison results of baseline,\nbaseline+CC, and CLE-ViT are shown in Figure 8. We ob-\nserve that CLE-ViT consistently improves the performance\nover the baseline method and Baseline+CC, verifying the ef-\nfectiveness of the proposed self-supervised learning module.\nFeature space. Figure 7 shows tSNE visualization of learned\nfeatures from baseline, baseline+class contrastive and ours.\nHere the samples are randomly selected from 10 categories\n(testing set). We observe that the clusters from the baseline\nare not well separable. The tSNE result from Baseline+Class\nContrastive shows a larger inter-class distance compared with\nbaseline while the intra-class distance remains small. This\nmay hurt the generalization ability given that samples in ultra-\nfine-grained image datasets often have large intra-class vari-\nance, e.g., the point in the top left corner 7 (b). In contrast,\nthe proposed CLE-ViT shows both large inter-class distance\nand intra-class distance, indicating a better generalization ca-\npability.\nAblation study on Lclsp. Table 5 shows an ablation study\nof Lclsp on Soy.Loc dataset by varying the weight from 0 to\n1.5 at a step size of 0.5. Here λ partially balances the contri-\nbution from instance-level contrastive learning and category-\nlevel learning. λ = 0 means positive samples are unable\nλ 0.0 0.5 1.0 1.5\nAccuracy(%) 43.67 44.00 47.17 43.00\nTable 5: The ablation study of the weight λ of CE loss for positive\nsamples on Soy.Loc dataset.\nFigure 8: Ablation study on ultra-fine-grained image datasets.\nto receive category-level supervision. Without category-level\nsupervision, contrastive learning can still optimize the dis-\ntance at the instance-level but might be unable to predict the\ncategory label of positive samples. As λ increases, the con-\ntribution from category-level supervision becomes larger and\nachieves the best performance when λ = 1. But when λ\nbecomes too large, the category-level supervision will com-\npletely dominate the training while ignoring the contribution\nfrom contrastive learning, thus may lead to overfitting and\nperformance drop.\n5 Conclusion\nThis paper introduced a novel contrastive learning encoded\nvision transformer, CLE-ViT, to address intrinsic challenges\nin ultra-fine-grained visual categorization. A new self-\nsupervised learning module has been proposed, which drives\nthe model to learn an optimized feature space that has a large\ninter-class distance while remaining tolerant to intra-class\nvariations. By incorporating this self-supervised module, the\nnetwork acquires more knowledge from the intrinsic structure\nof the input data, which improves the generalization ability\nof limited training samples. CLE-ViT has achieved competi-\ntive performance on seven public datasets, demonstrating its\neffectiveness in ultra-FGVC, birds, and plant disease classifi-\ncation tasks.\nWe also observe that negative samples are randomly sam-\npled in batches for better efficiency, thus potentially introduc-\ning easy pairs. A promising direction in future work is to ex-\nplore efficient hard sample mining approaches when forming\nnegative pairs.\nContribution Statement\nXiaohan Yu and Jun Wang contributed equally to this paper.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n4537\nReferences\n[Chen et al., 2019] Yue Chen, Yalong Bai, Wei Zhang, and\nTao Mei. Destruction and construction learning for fine-\ngrained image recognition. In CVPR, pages 5157–5166,\n2019.\n[Chen et al., 2020a] Ting Chen, Simon Kornblith, Moham-\nmad Norouzi, and Geoffrey Hinton. A simple framework\nfor contrastive learning of visual representations. InICML,\npages 1597–1607, 2020.\n[Chen et al., 2020b] Xinlei Chen, Haoqi Fan, Ross Girshick,\nand Kaiming He. Improved baselines with momentum\ncontrastive learning. arXiv preprint arXiv:2003.04297,\n2020.\n[Choe and Shim, 2019] Junsuk Choe and Hyunjung Shim.\nAttention-based dropout layer for weakly supervised ob-\nject localization. In CVPR, pages 2219–2228, 2019.\n[Deng et al., 2009] Jia Deng, Wei Dong, Richard Socher, Li-\nJia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In CVPR, pages 248–255,\n2009.\n[DeVries and Taylor, 2017] Terrance DeVries and Gra-\nham W Taylor. Improved regularization of convolu-\ntional neural networks with cutout. arXiv preprint\narXiv:1708.04552, 2017.\n[Doersch et al., 2015] Carl Doersch, Abhinav Gupta, and\nAlexei A Efros. Unsupervised visual representation learn-\ning by context prediction. In ICCV, pages 1422–1430,\n2015.\n[Dosovitskiy et al., 2020] Alexey Dosovitskiy, Lucas Beyer,\nAlexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Min-\nderer, Georg Heigold, Sylvain Gelly, et al. An image is\nworth 16x16 words: Transformers for image recognition\nat scale. In ICLR, 2020.\n[Du et al., 2020] Ruoyi Du, Dongliang Chang, Ayan Kumar\nBhunia, Jiyang Xie, Zhanyu Ma, Yi-Zhe Song, and Jun\nGuo. Fine-grained visual classification via progressive\nmulti-granularity training of jigsaw patches. In ECCV,\npages 153–168. Springer, 2020.\n[Ge et al., 2019] Weifeng Ge, Xiangru Lin, and Yizhou Yu.\nWeakly supervised complementary parts models for fine-\ngrained image classification from the bottom up. InCVPR,\npages 3034–3043, 2019.\n[Gidaris et al., 2018] Spyros Gidaris, Praveer Singh, and\nNikos Komodakis. Unsupervised representation learning\nby predicting image rotations. In ICLR, 2018.\n[Grill et al., 2020] Jean-Bastien Grill, Florian Strub, Flo-\nrent Altch ´e, Corentin Tallec, Pierre Richemond, Elena\nBuchatskaya, Carl Doersch, Bernardo Avila Pires, Zhao-\nhan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap\nyour own latent-a new approach to self-supervised learn-\ning. In NeurIPS, volume 33, pages 21271–21284, 2020.\n[He et al., 2016] Kaiming He, Xiangyu Zhang, Shaoqing\nRen, and Jian Sun. Deep residual learning for image recog-\nnition. In CVPR, pages 770–778, 2016.\n[He et al., 2022] Ju He, Jie-Neng Chen, Shuai Liu, Adam\nKortylewski, Cheng Yang, Yutong Bai, and Changhu\nWang. Transfg: A transformer architecture for fine-\ngrained recognition. In AAAI, volume 36, pages 852–860,\n2022.\n[Huang and Li, 2020] Zixuan Huang and Yin Li. Inter-\npretable and accurate fine-grained recognition via region\ngrouping. In CVPR, pages 8662–8672, 2020.\n[Krizhevsky et al., 2012] Alex Krizhevsky, Ilya Sutskever,\nand Geoffrey E Hinton. Imagenet classification with deep\nconvolutional neural networks. In NeurIPS, pages 1097–\n1105, 2012.\n[Larese et al., 2014] M´onica G Larese, Rafael Nam ´ıas,\nRoque M Craviotto, Miriam R Arango, Carina Gallo, and\nPablo M Granitto. Automatic classification of legumes\nusing leaf vein image features. Pattern Recognition,\n47(1):158–168, 2014.\n[Liu et al., 2021] Ze Liu, Yutong Lin, Yue Cao, Han Hu,\nYixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.\nSwin transformer: Hierarchical vision transformer using\nshifted windows. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision (ICCV), 2021.\n[Oord et al., 2018] Aaron van den Oord, Yazhe Li, and Oriol\nVinyals. Representation learning with contrastive predic-\ntive coding. arXiv preprint arXiv:1807.03748, 2018.\n[Simonyan and Zisserman, 2015] Karen Simonyan and An-\ndrew Zisserman. Very deep convolutional networks for\nlarge-scale image recognition. In ICLR, 2015.\n[Singh and Lee, 2017] Krishna Kumar Singh and Yong Jae\nLee. Hide-and-seek: Forcing a network to be meticulous\nfor weakly-supervised object and action localization. In\nICCV, pages 3544–3553, 2017.\n[Thapa et al., 2020] Ranjita Thapa, Noah Snavely, Serge Be-\nlongie, and Awais Khan. The plant pathology 2020 chal-\nlenge dataset to classify foliar disease of apples. arXiv\npreprint arXiv:2004.11958, 2020.\n[Touvron et al., 2021] Hugo Touvron, Matthieu Cord,\nMatthijs Douze, Francisco Massa, Alexandre Sablay-\nrolles, and Herv ´e J ´egou. Training data-efficient image\ntransformers & distillation through attention. In ICLR,\npages 10347–10357, 2021.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. In NeurIPS, volume 30, 2017.\n[Wah et al., 2011] Catherine Wah, Steve Branson, Peter\nWelinder, Pietro Perona, and Serge Belongie. The caltech-\nucsd birds-200-2011 dataset. Technical Report CNS-TR-\n2011-001, 2011.\n[Wang et al., 2021a] Jun Wang, Xiaohan Yu, and Yongsheng\nGao. Feature fusion vision transformer for fine-grained\nvisual categorization. In BMVC, 2021.\n[Wang et al., 2021b] Peng Wang, Kai Han, Xiu-Shen Wei,\nLei Zhang, and Lei Wang. Contrastive learning based hy-\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n4538\nbrid networks for long-tailed image classification. In Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 943–952, 2021.\n[Yu et al., 2020] Xiaohan Yu, Yang Zhao, Yongsheng Gao,\nShengwu Xiong, and Xiaohui Yuan. Patchy image struc-\nture classification using multi-orientation region trans-\nform. In AAAI, volume 34, pages 12741–12748, 2020.\n[Yu et al., 2021a] Xiaohan Yu, Yang Zhao, Yongsheng Gao,\nand Shengwu Xiong. Maskcov: A random mask covari-\nance network for ultra-fine-grained visual categorization.\nPattern Recognition, 119:108067, 2021.\n[Yu et al., 2021b] Xiaohan Yu, Yang Zhao, Yongsheng Gao,\nXiaohui Yuan, and Shengwu Xiong. Benchmark platform\nfor ultra-fine-grained visual categorization beyond human\nperformance. In ICCV, pages 10285–10295, 2021.\n[Yu et al., 2022] Xiaohan Yu, Yang Zhao, and Yongsheng\nGao. Spare: Self-supervised part erasing for ultra-\nfine-grained visual categorization. Pattern Recognition,\n128:108691, 2022.\n[Yu et al., 2023] Xiaohan Yu, Jun Wang, Yang Zhao, and\nYongsheng Gao. Mix-vit: Mixing attentive vision trans-\nformer for ultra-fine-grained visual categorization.Pattern\nRecognition, 135:109131, 2023.\n[Yun et al., 2019] Sangdoo Yun, Dongyoon Han, Seong Joon\nOh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo.\nCutmix: Regularization strategy to train strong classifiers\nwith localizable features. In ICCV, pages 6023–6032,\n2019.\n[Zhuang et al., 2020] Peiqin Zhuang, Yali Wang, and\nYu Qiao. Learning attentive pairwise interaction for\nfine-grained classification. In AAAI, volume 34, pages\n13130–13137, 2020.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n4539",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7808427214622498
    },
    {
      "name": "Discriminative model",
      "score": 0.7740660905838013
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6533461809158325
    },
    {
      "name": "Feature learning",
      "score": 0.6355628967285156
    },
    {
      "name": "Categorization",
      "score": 0.5707638263702393
    },
    {
      "name": "Transformer",
      "score": 0.5013613700866699
    },
    {
      "name": "Machine learning",
      "score": 0.4224732220172882
    },
    {
      "name": "Visualization",
      "score": 0.4134376049041748
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3920478820800781
    },
    {
      "name": "Natural language processing",
      "score": 0.3715166747570038
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}