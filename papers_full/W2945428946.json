{
  "title": "Enriching Pre-trained Language Model with Entity Information for Relation Classification",
  "url": "https://openalex.org/W2945428946",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4381714514",
      "name": "Wu, Shanchan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2227124480",
      "name": "He Yifan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2132679783",
    "https://openalex.org/W2515462165",
    "https://openalex.org/W2962897570",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2962995645",
    "https://openalex.org/W2107598941",
    "https://openalex.org/W2572908757",
    "https://openalex.org/W1887754209",
    "https://openalex.org/W2250521169",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2604610161",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2513378248",
    "https://openalex.org/W2155454737",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1889268436"
  ],
  "abstract": "Relation classification is an important NLP task to extract relations between entities. The state-of-the-art methods for relation classification are primarily based on Convolutional or Recurrent Neural Networks. Recently, the pre-trained BERT model achieves very successful results in many NLP classification / sequence labeling tasks. Relation classification differs from those tasks in that it relies on information of both the sentence and the two target entities. In this paper, we propose a model that both leverages the pre-trained BERT language model and incorporates information from the target entities to tackle the relation classification task. We locate the target entities and transfer the information through the pre-trained architecture and incorporate the corresponding encoding of the two entities. We achieve significant improvement over the state-of-the-art method on the SemEval-2010 task 8 relational dataset.",
  "full_text": "Enriching Pre-trained Language Model with Entity Information for\nRelation Classiﬁcation\nShanchan Wu\nAlibaba Group (U.S.) Inc., Sunnyvale, CA\nshanchan.wu@alibaba-inc.com\nYifan He\nAlibaba Group (U.S.) Inc., Sunnyvale, CA\ny.he@alibaba-inc.com\nAbstract\nRelation classiﬁcation is an important\nNLP task to extract relations between enti-\nties. The state-of-the-art methods for rela-\ntion classiﬁcation are primarily based on\nConvolutional or Recurrent Neural Net-\nworks. Recently, the pre-trained BERT\nmodel achieves very successful results in\nmany NLP classiﬁcation / sequence la-\nbeling tasks. Relation classiﬁcation dif-\nfers from those tasks in that it relies on\ninformation of both the sentence and the\ntwo target entities. In this paper, we pro-\npose a model that both leverages the pre-\ntrained BERT language model and incor-\nporates information from the target entities\nto tackle the relation classiﬁcation task.\nWe locate the target entities and transfer\nthe information through the pre-trained ar-\nchitecture and incorporate the correspond-\ning encoding of the two entities. We\nachieve signiﬁcant improvement over the\nstate-of-the-art method on the SemEval-\n2010 task 8 relational dataset.\n1 Introduction\nThe task of relation classiﬁcation is to predict se-\nmantic relations between pairs of nominals. Given\na sequence of text (usually a sentence)s and a pair\nof nominals e1 and e2, the objective is to identify\nthe relation between e1 and e2 (Hendrickx et al.,\n2010). It is an important NLP task which is nor-\nmally used as an intermediate step in variety of\nNLP applications. The following example shows\nthe Component-Whole relation between the nom-\ninals “kitchen” and “house”: “The [kitchen] e1 is\nthe last renovated part of the [house]e1.”\nRecently, deep neural networks have applied to\nrelation classiﬁcation (Socher et al., 2012; Zeng\net al., 2014; Yu et al., 2014; dos Santos et al., 2015;\nShen and Huang, 2016; Lee et al., 2019). These\nmethods usually use some features derived from\nlexical resources such as Word-Net or NLP tools\nsuch as dependency parsers and named entity rec-\nognizers (NER).\nLanguage model pre-training has been shown to\nbe effective for improving many natural language\nprocessing tasks (Dai and Le, 2015; Peters et al.,\n2017; Radford et al., 2018; Ruder and Howard,\n2018; Devlin et al., 2018). The pretrained model\nBERT proposed by (Devlin et al., 2018) has es-\npecially signiﬁcant impact. It has been applied to\nmultiple NLP tasks and obtains new state-of-the-\nart results on eleven tasks. The tasks that BERT\nhas been applied to are typically modeled as clas-\nsiﬁcation problems and sequence labeling prob-\nlems. It has also been applied to the SQuAD ques-\ntion answering (Rajpurkar et al., 2016) problem,\nin which the objective is to ﬁnd the starting point\nand ending point of an answer span.\nAs far as we know, the pretrained BERT model\n(Devlin et al., 2018) has not been applied to rela-\ntion classiﬁcation, which relies not only on the in-\nformation of the whole sentence but also on the in-\nformation of the speciﬁc target entities. In this pa-\nper, we apply the pretrained BERT model for rela-\ntion classiﬁcation. We insert special tokens before\nand after the target entities before feeding the text\nto BERT for ﬁne-tuning, in order to identify the\nlocations of the two target entities and transfer the\ninformation into the BERT model. We then locate\nthe positions of the two target entities in the out-\nput embedding from BERT model. We use their\nembeddings as well as the sentence encoding (em-\nbedding of the special ﬁrst token in the setting of\nBERT) as the input to a multi-layer neural network\nfor classiﬁcation. By this way, it captures both the\nsemantics of the sentence and the two target enti-\nties to better ﬁt the relation classiﬁcation task.\nOur contributions are as follows: (1) We put\nforward an innovative approach to incorporate\narXiv:1905.08284v1  [cs.CL]  20 May 2019\nentity-level information into the pretrained lan-\nguage model for relation classiﬁcation. (2) We\nachieve the new state-of-the-art for the relation\nclassiﬁcation task.\n2 Related Work\nThere has been some work with deep learn-\ning methods for relation classiﬁcation, such as\n(Socher et al., 2012; Zeng et al., 2014; Yu et al.,\n2014; dos Santos et al., 2015)\nMVRNN model (Socher et al., 2012) applies a\nrecursive neural network (RNN) to relation clas-\nsiﬁcation. They assign a matrix-vector represen-\ntation to every node in a parse tree and com-\npute the representation for the complete sentence\nfrom bottom up according to the syntactic struc-\nture of the parse tree. (Zeng et al., 2014) pro-\npose a CNN model by incorporating both word\nembeddings and position features as input. Then\nthey concatenate lexical features and the output\nfrom CNN into a single vector and feed them\ninto a softmax layer for prediction. (Yu et al.,\n2014) propose a Factor-based Compositional Em-\nbedding Model (FCM) by constructing sentence-\nlevel and substructure embeddings from word em-\nbeddings, through dependency trees and named\nentities. (Santos et al., 2015) tackle the relation\nclassiﬁcation task by ranking with a convolutional\nneural network named CR-CNN. Their loss func-\ntion is based on pairwise ranking. In our work, we\ntake advantage of a pre-trained language model for\nthe relation classiﬁcation task, without relying on\nCNN or RNN architecutures. (Shen and Huang,\n2016) utilize a CNN encoder in conjunction with a\nsentence representation that weights the words by\nattention between the target entities and the words\nin the sentence to perform relation classiﬁcation.\n(Wang et al., 2016) propose a convolutional neu-\nral network architecture with two levels of atten-\ntion in order to catch the patterns in heterogeneous\ncontexts to classify relations. (Lee et al., 2019) de-\nvelop an end-to-end recurrent neural model which\nincorporates an entity-aware attention mechanism\nwith a latent entity typing for relation classiﬁca-\ntion.\nThere are some related work on the relation ex-\ntraction based on distant supervision, for example,\n(Mintz et al., 2009; Hoffmann et al., 2011; Lin\net al., 2016; Ji et al., 2017; Wu et al., 2019). The\ndifference between relation classiﬁcation on reg-\nular data and on distantly supervised data is that\nthe latter may contain a large number of noisy la-\nbels. In this paper, we focus on the regular relation\nclassiﬁcation problem, without noisy labels.\n3 Methodology\n3.1 Pre-trained Model BERT\nThe pre-trained BERT model (Devlin et al., 2018)\nis a multi-layer bidirectional Transformer encoder\n(Vaswani et al., 2017).\nThe design of input representation of BERT is\nto be able to represent both a single text sentence\nand a pair of text sentences in one token sequence.\nThe input representation of each token is con-\nstructed by the summation of the corresponding\ntoken, segment and position embeddings.\n‘[CLS]’ is appended to the beginning of each\nsequence as the ﬁrst token of the sequence. The ﬁ-\nnal hidden state from the Transformer output cor-\nresponding to the ﬁrst token is used as the sen-\ntence representation for classiﬁcation tasks. In\ncase there are two sentences in a task, ‘[SEP]’ is\nused to separate the two sentences.\nBERT pre-trains the model parameters by us-\ning a pre-training objective: the masked language\nmodel (MLM), which randomly masks some of\nthe tokens from the input, and set the optimiza-\ntion objective to predict the original vocabulary id\nof the masked word according to its context. Un-\nlike left-to-right language model pre-training, the\nMLM objective can help a state output to utilize\nboth the left and the right context, which allows\na pre-training system to apply a deep bidirectional\nTransformer. Besides the masked language model,\nBERT also trains a “next sentence prediction” task\nthat jointly pre-trains text-pair representations.\n3.2 Model Architecture\nFigure 1 shows the architecture of our approach.\nFor a sentence s with two target entities e1 and\ne2, to make the BERT module capture the location\ninformation of the two entities, at both the begin-\nning and end of the ﬁrst entity, we insert a spe-\ncial token ‘$’, and at both the beginning and end\nof the second entity, we insert a special token ‘#’.\nWe also add ‘[CLS]’ to the beginning of each sen-\ntence.\nFor example, after insertion of the special sep-\narate tokens, for a sentence with target entities\n“kitchen” and “house” will become to:\n“[CLS] The $ kitchen $ is the last renovated\npart of the # house # . ”\n[CLS]T1 $Ti Tj Tj+2$ #Tk Tm Tm+2# Tn\nEntity 1 Entity 2\nBERT\nH0 Hi Hj Hk HmAverageAverageFully-connected+ activation\nFully-connected+ activationFully-connected+ activation\nFully-connected\nSoftmax\nFigure 1: The model architecture.\nGiven a sentence s with entity e1 and e2, sup-\npose its ﬁnal hidden state output from BERT mod-\nule is H. Suppose vectors Hi to Hj are the ﬁnal\nhidden state vectors from BERT for entity e1, and\nHk to Hm are the ﬁnal hidden state vectors from\nBERT for entity e2. We apply the average opera-\ntion to get a vector representation for each of the\ntwo target entities. Then after an activation opera-\ntion (i.e. tanh), we add a fully connected layer to\neach of the two vectors, and the output for e1 and\ne2 are H′\n1 and H′\n2 respectively. This process can\nbe mathematically formalized as Equation (1).\nH′\n1 = W1\n[\ntanh\n(\n1\nj −i + 1\nj∑\nt=i\nHt\n)]\n+ b1\nH′\n2 = W2\n[\ntanh\n(\n1\nm −k + 1\nm∑\nt=k\nHt\n)]\n+ b2\n(1)\nWe make W1 and W2, b1 and b2 share the same\nparameters. In other words, we set W1 = W2,\nb1 = b2. For the ﬁnal hidden state vector of the\nﬁrst token (i.e. ‘[CLS]’), we also add an activa-\ntion operation and a fully connected layer, which\nis formally expressed as:\nH′\n0 = W0 (tanh(H0)) +b0 (2)\nMatrices W0, W1, W2 have the same dimensions,\ni.e. W0 ∈Rd×d, W1 ∈Rd×d, W2 ∈Rd×d, where\nd is the hidden state size from BERT.\nWe concatenateH′\n0, H′\n1, H′\n2 and then add a fully\nconnected layer and a softmax layer, which can be\nexpressed as following:\nh′′= W3\n[\nconcat\n(\nH′\n0, H′\n1, H′\n2\n)]\n+ b3\np = softmax(h′′)\n(3)\nwhere W3 ∈RL×3d (L is the number of relation\ntypes), and p is the probability output. In Equa-\ntions (1),(2),(3), b0, b1, b2, b3 are bias vectors.\nWe use cross entropy as the loss function. We\napply dropout before each fully connected layer\nduring training. We call our approach as R-BERT.\n4 Experiments\n4.1 Dataset and Evaluation Metric\nWe use the SemEval-2010 Task 8 dataset in\nour experiments. The dataset contains nine\nsemantic relation types and one artiﬁcial rela-\ntion type Other, which means that the rela-\ntion does not belong to any of the nine relation\ntypes. The nine relation types are Cause-Effect,\nComponent-Whole, Content-Container, Entity-\nDestination, Entity-Origin, Instrument-Agency,\nMember-Collection, Message-Topic and Product-\nProducer. The dataset contains 10,717 sentences,\nwith each containing two nominals e1 and e2,\nand the corresponding relation type in the sen-\ntence. The relation is directional, which means\nthat Component-Whole(e1, e2) is different from\nComponent-Whole(e2, e1). The dataset has al-\nready been partitioned into 8,000 training in-\nstances and 2,717 test instances. We evaluate our\nsolution by using the SemEval-2010 Task 8 ofﬁ-\ncial scorer script. It computes the macro-averaged\nF1-scores for the nine actual relations (excluding\nOther) and considers directionality.\n4.2 Parameter Settings\nTable shows the major parameters used in our ex-\nperiments.\nTable 1: Parameter settings.\nBatch size 16\nMax sentence length 128\nAdam learning rate 2e-5\nNumber of epochs 5\nDropout rate 0.1\nWe add dropout before each add-on layer. For\nthe pre-trained BERT model, we use the uncased\nbasic model. For the parameters of the pre-trained\nBERT model, please refer to (Devlin et al., 2018)\nfor details.\n4.3 Comparison with other Methods\nWe compare our method, R-BERT, against re-\nsults by multiple methods recently published for\nthe SemEval-2010 Task 8 dataset, including SVM,\nRNN, MVRNN, CNN+Softmax, FCM, CR-CNN,\nAttention-CNN, Entity Attention Bi-LSTM. The\nSVM method by (Rink and Harabagiu, 2010) uses\na rich feature set in a traditional way, which was\nthe best result during the SemEval-2010 task 8\ncompetition. Details of all other methods are\nbrieﬂy reviewed in Section 2.\nTable 2 reports the results. We can see that R-\nBERT signiﬁcantly beats all the baseline methods.\nThe MACRO F1 value of R-BERT is 89.25, which\nis much better than the previous best solution on\nthis dataset.\n4.4 Ablation Studies\n4.4.1 Effect of Model Components\nWe have demonstrated the strong empirical re-\nsults based on the proposed approach. We fur-\nther want to understand the speciﬁc contributions\nby the components besides the pre-trained BERT\ncomponent. For this purpose, we create three more\nconﬁgurations.\nThe ﬁrst conﬁguration is to discard the special\nseparate tokens (i.e. ‘$’ and ‘#’) around the two\nTable 2: Comparison with results in the literature.\nMethod F1\nSVM\n(Rink and Harabagiu, 2010) 82.2\nRNN\n(Socher et al., 2012) 77.6\nMVRNN\n(Socher et al., 2012) 82.4\nCNN+Softmax\n(Zeng et al., 2014) 82.7\nFCM\n(Yu et al., 2014) 83.0\nCR-CNN\n(Santos et al., 2015) 84.1\nAttention CNN\n(Shen and Huang, 2016) 85.9\nAtt-Pooling-CNN\n(Wang et al., 2016) 88.0\nEntity Attention Bi-LSTM\n(Lee et al., 2019) 85.2\nR-BERT 89.25\nentities in the sentence and discard the hidden vec-\ntor output of the two entities from concatenating\nwith the hidden vector output of the sentence. In\nother words, we add ‘[CLS]’ at the beginning of\nthe sentence and feed the sentence with the two en-\ntities into the BERT module, and use the ﬁrst out-\nput vector for classiﬁcation. We label this method\nas BERT-NO-SEP-NO-ENT.\nThe second conﬁguration is to discard the spe-\ncial separate tokens (i.e. ‘$’ and ‘#’) around the\ntwo entities in the sentence, but keep the hidden\nvector output of the two entities in concatenation\nfor classiﬁcation. We label this method as BERT-\nNO-SEP.\nThe third conﬁguration is to discard the hidden\nvector output of the two entities from concatena-\ntion for classiﬁcation, but keep the special separate\ntokens. We label this method as BERT-NO-ENT.\nTable 3 reports the results of the ablation study\nwith the above three conﬁgurations. We observe\nthat the three methods all perform worse than R-\nBERT. Of the methods, BERT-NO-SEP-NO-ENT\nperforms worst, with its F1 8.16 absolute points\nworse than R-BERT. This ablation study demon-\nstrates that both the special separate tokens and\nthe hidden entity vectors make important contri-\nbutions to our approach.\nIn relation classiﬁcation, the relation label is de-\npendent on both the semantics of the sentence and\nthe two target entities. BERT without special sep-\narate tokens cannot locate the target entities and\nlose this key information. The reason why the spe-\ncial separate tokens help to improve the accuracy\nis that they identify the locations of the two target\nentities and transfer the information into the BERT\nmodel, which make the BERT output contain the\nlocation information of the two entities. On the\nother hand, incorporating the output of the target\nentity vectors further enriches the information and\nhelps to make more accurate prediction.\nTable 3: Comparison of the BERT based methods\nwith different components.\nMethod F1\nR-BERT-NO-SEP-NO-ENT 81.09\nR-BERT-NO-SEP 87.98\nR-BERT-NO-ENT 87.99\nR-BERT 89.25\n5 Conclusions\nIn this paper, we develop an approach for relation\nclassiﬁcation by enriching the pre-trained BERT\nmodel with entity information. We add special\nseparate tokens to each target entity pair and uti-\nlize the sentence vector as well as target entity rep-\nresentations for classiﬁcation. We conduct exper-\niments on the SemEval-2010 benchmark dataset\nand our results signiﬁcantly outperform the state-\nof-the-art methods. One possible future work is to\nextend the model to apply to distant supervision.\nReferences\nAndrew M Dai and Quoc V Le. 2015. Semi-\nsupervised Sequence Learning. In Advances\nin Neural Information Processing Systems 28 .\n3079–3087.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: Pre-training\nof Deep Bidirectional Transformers for Lan-\nguage Understanding. CoRR abs/1810.04805\n(2018). arXiv:1810.04805\nC´ıcero Nogueira dos Santos, Bing Xiang, and\nBowen Zhou. 2015. Classifying Relations by\nRanking with Convolutional Neural Networks.\nIn Proceedings of the 53rd Annual Meeting of\nthe Association for Computational Linguistics\nand the 7th International Joint Conference on\nNatural Language Processing of the Asian Fed-\neration of Natural Language Processing, ACL\n2015. 626–634.\nIris Hendrickx, Su Nam Kim, Zornitsa Kozareva,\nPreslav Nakov, Diarmuid ´O. S ´eaghdha, Sebas-\ntian Pad ´o, Marco Pennacchiotti, Lorenza Ro-\nmano, and Stan Szpakowicz. 2010. SemEval-\n2010 Task 8: Multi-way Classiﬁcation of Se-\nmantic Relations Between Pairs of Nominals. In\nProceedings of the 5th International Workshop\non Semantic Evaluation (SemEval ’10). 33–38.\nRaphael Hoffmann, Congle Zhang, Xiao Ling,\nLuke S. Zettlemoyer, and Daniel S. Weld. 2011.\nKnowledge-Based Weak Supervision for Infor-\nmation Extraction of Overlapping Relations. In\nThe 49th Annual Meeting of the Association\nfor Computational Linguistics, ACL 2011. 541–\n550.\nGuoliang Ji, Kang Liu, Shizhu He, and Jun\nZhao. 2017. Distant Supervision for Rela-\ntion Extraction with Sentence-Level Attention\nand Entity Descriptions. In Proceedings of the\nThirty-First AAAI Conference on Artiﬁcial In-\ntelligence, 2017. 3060–3066.\nJoohong Lee, Sangwoo Seo, and Yong Suk\nChoi. 2019. Semantic Relation Classiﬁcation\nvia Bidirectional LSTM Networks with Entity-\naware Attention using Latent Entity Typing.\nCoRR (2019).\nYankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo\nLuan, and Maosong Sun. 2016. Neural Re-\nlation Extraction with Selective Attention over\nInstances. In Proceedings of the 54th Annual\nMeeting of the Association for Computational\nLinguistics, ACL 2016.\nMike Mintz, Steven Bills, Rion Snow, and Dan Ju-\nrafsky. 2009. Distant Supervision for Relation\nExtraction Without Labeled Data. In Proceed-\nings of the Joint Conference of the 47th An-\nnual Meeting of the ACL and the 4th Interna-\ntional Joint Conference on Natural Language\nProcessing of the AFNLP (ACL ’09) . 1003–\n1011.\nMatthew E. Peters, Waleed Ammar, Chandra Bha-\ngavatula, and Russell Power. 2017. Semi-\nsupervised sequence tagging with bidirectional\nlanguage models. CoRR abs/1705.00108\n(2017). arXiv:1705.00108\nAlec Radford, Karthik Narasimhan, Tim Sali-\nmans, and Ilya Sutskever. 2018. Improving lan-\nguage understanding with unsupervised learn-\ning. Technical report, OpenAI (2018).\nPranav Rajpurkar, Jian Zhang, Konstantin Lopy-\nrev, and Percy Liang. 2016. SQuAD: 100,000+\nQuestions for Machine Comprehension of Text.\nIn Proceedings of the 2016 Conference on Em-\npirical Methods in Natural Language Process-\ning. Association for Computational Linguistics,\n2383–2392.\nBryan Rink and Sanda Harabagiu. 2010. Utd:\nClassifying semantic relations by combining\nlexical and semantic resources. In Proceedings\nof the 5th International Workshop on Semantic\nEvaluation. 256–259.\nSebastian Ruder and Jeremy Howard. 2018. Uni-\nversal Language Model Fine-tuning for Text\nClassiﬁcation. In Proceedings of the 56th An-\nnual Meeting of the Association for Computa-\ntional Linguistics, ACL 2018, Melbourne, Aus-\ntralia, July 15-20, 2018, Volume 1: Long Pa-\npers. 328–339.\nCicero Nogueira Dos Santos, Bing Xiang, and\nBowen Zhou. 2015. Classifying Relations by\nRanking with Convolutional Neural Networks.\nIn Proceedings of the 53rd Annual Meeting of\nthe Association for Computational Linguistics\nand the 7th International Joint Conference on\nNatural Language Processing of the Asian Fed-\neration of Natural Language Processing, (ACL)\n2015.\nYatian Shen and Xuanjing Huang. 2016.\nAttention-based Convolutional Neural Network\nfor Semantic Relation Extraction. In Proceed-\nings of COLING 2016, the 26th International\nConference on Computational Linguistics:\nTechnical Papers. 2526–2536.\nRichard Socher, Brody Huval, Christopher D.\nManning, and Andrew Y . Ng. 2012. Semantic\nCompositionality Through Recursive Matrix-\nvector Spaces. In Proceedings of the 2012 Joint\nConference on Empirical Methods in Natural\nLanguage Processing and Computational Nat-\nural Language Learning (EMNLP-CoNLL ’12).\nStroudsburg, PA, USA, 1201–1211.\nAshish Vaswani, Noam Shazeer, Niki Parmar,\nJakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁ ukasz Kaiser, and Illia Polosukhin. 2017. At-\ntention is All you Need. In Advances in Neural\nInformation Processing Systems 30 , I. Guyon,\nU. V . Luxburg, S. Bengio, H. Wallach, R. Fer-\ngus, S. Vishwanathan, and R. Garnett (Eds.).\n5998–6008.\nLinlin Wang, Zhu Cao, Gerard de Melo, and\nZhiyuan Liu. 2016. Relation Classiﬁcation via\nMulti-Level Attention CNNs. InProceedings of\nthe 54th Annual Meeting of the Association for\nComputational Linguistics.\nShanchan Wu, Fan Kai, and Qiong Zhang. 2019.\nImproving Distantly Supervised Relation Ex-\ntraction with Neural Noise Converter and Con-\nditional Optimal Selector. In Proceedings of the\nThirty-Third AAAI Conference on Artiﬁcial In-\ntelligence, 2019.\nMo Yu, Matthew R. Gormley, and Mark Dredze.\n2014. Factor-based compositional embedding\nmodels. In In NIPS Workshop on Learning Se-\nmantics.\nDaojian Zeng, Kang Liu, Siwei Lai, Guangyou\nZhou, and Jun Zhao. 2014. Relation Classiﬁca-\ntion via Convolutional Deep Neural Network. In\nCOLING 2014, 25th International Conference\non Computational Linguistics, Proceedings of\nthe Conference: Technical Papers, 2014. 2335–\n2344.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8429323434829712
    },
    {
      "name": "Relation (database)",
      "score": 0.7505456209182739
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7233548760414124
    },
    {
      "name": "Task (project management)",
      "score": 0.7066959142684937
    },
    {
      "name": "Natural language processing",
      "score": 0.6525043845176697
    },
    {
      "name": "SemEval",
      "score": 0.6210634112358093
    },
    {
      "name": "Sentence",
      "score": 0.604527473449707
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5731754302978516
    },
    {
      "name": "Language model",
      "score": 0.48919013142585754
    },
    {
      "name": "Encoding (memory)",
      "score": 0.4805210530757904
    },
    {
      "name": "Relationship extraction",
      "score": 0.4688131809234619
    },
    {
      "name": "Named-entity recognition",
      "score": 0.46540671586990356
    },
    {
      "name": "Sequence labeling",
      "score": 0.44427019357681274
    },
    {
      "name": "State (computer science)",
      "score": 0.43617263436317444
    },
    {
      "name": "Transfer of learning",
      "score": 0.4250227212905884
    },
    {
      "name": "Machine learning",
      "score": 0.34537798166275024
    },
    {
      "name": "Information extraction",
      "score": 0.30859845876693726
    },
    {
      "name": "Data mining",
      "score": 0.14552634954452515
    },
    {
      "name": "Algorithm",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210095624",
      "name": "Alibaba Group (United States)",
      "country": "US"
    }
  ],
  "cited_by": 29
}