{
  "title": "Gophormer: Ego-Graph Transformer for Node Classification",
  "url": "https://openalex.org/W3209214366",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2356887348",
      "name": "Zhao, Jianan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202202832",
      "name": "Li, Chaozhuo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4296414647",
      "name": "Wen, Qianlong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2354696834",
      "name": "Wang YiQi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1969036401",
      "name": "Liu Yuming",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2003959196",
      "name": "Sun Hao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2127526945",
      "name": "Xie Xing",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2742652319",
      "name": "Ye Yanfang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963858333",
    "https://openalex.org/W3213730107",
    "https://openalex.org/W2994968268",
    "https://openalex.org/W3169622372",
    "https://openalex.org/W4210257598",
    "https://openalex.org/W3129794609",
    "https://openalex.org/W3100278010",
    "https://openalex.org/W3205349317",
    "https://openalex.org/W2583803680",
    "https://openalex.org/W3132845217",
    "https://openalex.org/W3126138172",
    "https://openalex.org/W2998496395",
    "https://openalex.org/W3113177135",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W1662382123",
    "https://openalex.org/W2975868979",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2624431344",
    "https://openalex.org/W2784814091",
    "https://openalex.org/W3031722355",
    "https://openalex.org/W2962711740",
    "https://openalex.org/W2468907370",
    "https://openalex.org/W2387462954",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3045733172",
    "https://openalex.org/W3033039844",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W3100848837",
    "https://openalex.org/W3103409210",
    "https://openalex.org/W2519887557",
    "https://openalex.org/W2116341502"
  ],
  "abstract": "Transformers have achieved remarkable performance in a myriad of fields including natural language processing and computer vision. However, when it comes to the graph mining area, where graph neural network (GNN) has been the dominant paradigm, transformers haven't achieved competitive performance, especially on the node classification task. Existing graph transformer models typically adopt fully-connected attention mechanism on the whole input graph and thus suffer from severe scalability issues and are intractable to train in data insufficient cases. To alleviate these issues, we propose a novel Gophormer model which applies transformers on ego-graphs instead of full-graphs. Specifically, Node2Seq module is proposed to sample ego-graphs as the input of transformers, which alleviates the challenge of scalability and serves as an effective data augmentation technique to boost model performance. Moreover, different from the feature-based attention strategy in vanilla transformers, we propose a proximity-enhanced attention mechanism to capture the fine-grained structural bias. In order to handle the uncertainty introduced by the ego-graph sampling, we further propose a consistency regularization and a multi-sample inference strategy for stabilized training and testing, respectively. Extensive experiments on six benchmark datasets are conducted to demonstrate the superiority of Gophormer over existing graph transformers and popular GNNs, revealing the promising future of graph transformers.",
  "full_text": "Gophormer: Ego-Graph Transformer for Node Classification\nJianan Zhaoâˆ—\nUniversity of Notre Dame\nNotre Dame, USA\njzhao8@nd.edu\nChaozhuo Li\nMicrosoft Research\nBeijing, China\ncli@microsoft.com\nQianlong Wenâˆ—\nUniversity of Notre Dame\nNotre Dame, USA\nqwen@nd.edu\nYiqi Wang\nMichigan State University\nEast Lansing, USA\nwangy206@msu.edu\nYuming Liu\nMicrosoft\nBeijing, China\nyumliu@microsoft.com\nHao Sun\nMicrosoft\nBeijing, China\nhasun@microsoft.com\nXing Xie\nMicrosoft Research\nBeijing, China\nxingx@microsoft.com\nYanfang Yeâ€ \nUniversity of Notre Dame\nNotre Dame, USA\nyye7@nd.edu\nABSTRACT\nTransformers have achieved remarkable performance in a myriad\nof fields including natural language processing and computer vi-\nsion. However, when it comes to the graph mining area, where\ngraph neural network (GNN) has been the dominant paradigm,\ntransformers havenâ€™t achieved competitive performance, especially\non the node classification task. Existing graph transformer models\ntypically adopt fully-connected attention mechanism on the whole\ninput graph and thus suffer from severe scalability issues and are in-\ntractable to train in data insufficient cases. To alleviate these issues,\nwe propose a novel Gophormer model which applies transformers\non ego-graphs instead of full-graphs. Specifically, Node2Seq mod-\nule is proposed to sample ego-graphs as the input of transformers,\nwhich alleviates the challenge of scalability and serves as an ef-\nfective data augmentation technique to boost model performance.\nMoreover, different from the feature-based attention strategy in\nvanilla transformers, we propose a proximity-enhanced attention\nmechanism to capture the fine-grained structural bias. In order to\nhandle the uncertainty introduced by the ego-graph sampling, we\nfurther propose a consistency regularization and a multi-sample\ninference strategy for stabilized training and testing, respectively.\nExtensive experiments on six benchmark datasets are conducted\nto demonstrate the superiority of Gophormer over existing graph\ntransformers and popular GNNs, revealing the promising future of\ngraph transformers.\nâˆ—Both authors contributed equally to this research.\nâ€ Corresponding author.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nWoodstock â€™18, June 03â€“05, 2018, Woodstock, NY\nÂ© 2018 Association for Computing Machinery.\nACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00\nhttps://doi.org/10.1145/1122445.1122456\nKEYWORDS\nTransformers, Graph Neural Networks, Node Classification\nACM Reference Format:\nJianan Zhao, Chaozhuo Li, Qianlong Wen, Yiqi Wang, Yuming Liu, Hao\nSun, Xing Xie, and Yanfang Ye. 2018. Gophormer: Ego-Graph Transformer\nfor Node Classification. In Woodstock â€™18: ACM Symposium on Neural Gaze\nDetection, June 03â€“05, 2018, Woodstock, NY. ACM, New York, NY, USA,\n10 pages. https://doi.org/10.1145/1122445.1122456\n1 INTRODUCTION\nIn recent years, graph neural networks (GNNs), have shown ex-\ncellency in graph mining and are widely applied to a variety of\napplications such as node classification [15, 17, 28], graph classifica-\ntion [32, 38], and recommendation [13, 29, 35]. Most GNN methods\nfollow a message-passing scheme where the node representation\nis learned by aggregating and transforming its neighborhood em-\nbeddings [17, 31]. Despite the prevailing adoption of the message-\npassing scheme, there is a growing recognition of the inherent\nlimitations of this paradigm. On one hand, due to the repeated\naggregation of local information, the message-passing scheme suf-\nfers from the over-smoothing issue, i.e. representations of different\nnodes become indistinguishable when stacking too many feature\npropagation layers [6]. On the other hand, due to the exponential\nblow-up in computation paths as the model depth increases [ 1],\nGNNs also experience difficulties in capturing long-range interac-\ntions (the over-squashing problem).\nMeanwhile, in the fields of natural language processing and com-\nputer vision, the transformer architecture has shown dominant\nperformance [7, 11, 23, 27] thanks to its powerful feature learning\ncapacity and low inductive bias. In light of its amazing performance\nand the limitations of GNNs, attempts have been made to introduce\ntransformer into graph learning to replace the message-passing\nscheme. The majority of these methods [ 12, 20, 34] apply trans-\nformers on the entire graph, treating all nodes as fully-connected,\nand enhance the vanilla feature-based attention mechanism with\nstructural encoding and topology-enhanced attention mechanism.\nFor example, SAN [20] learns Laplacian positional encodings and\nuses different attention mechanism for connected and unconnected\narXiv:2110.13094v1  [cs.LG]  25 Oct 2021\nWoodstock â€™18, June 03â€“05, 2018, Woodstock, NY Anonymous Author(s).\nnodes. Graphormer [34] achieves state-of-the-art graph classifica-\ntion performance via transformers with centrality, spatial, and edge\nencodings. However, there are several limitations of these methods.\nFirst, the full-graph attention schema suffers from the quadratic\ndependency (mainly in terms of memory) on the graph size due\nto the full attention mechanism, leading to the poor scalability es-\npecially on large graphs. Moreover, the full-attention mechanism\nviews the entire input graph as a fully-connected graph and fuses\ninformation from all the nodes, which potentially introduces noise\nfrom numerous irrelevant long-distance neighbors. Last but not\nleast, unlike GNNs with few learnable parameters, the transformers\ncontain much more parameters, which are intractable to be fully\ntrained with scarce annotations and are also more vulnerable to\noverfitting.\nIn light of the potential noise from the long-distance neighbors\nand the challenge of scalability, we propose to use the ego-graph\ninstead of the entire graph as the transformer input. Specifically,\nwe propose a novel module named Node2Seq to sample an ego-\ngraph for each node as the input of the transformer. Then, node\nrepresentations are learned based on the features within the sam-\npled ego-graphs instead of node features of the entire graph. This\nstrategy apparently alleviates the aforementioned problems: First,\nthe challenge of scalability is eased since the input size is reduced\nfrom |ğ‘‰|(number of nodes in the whole graph) to |ğ‘‰ğ¸|(average\nnumber of nodes in ego-graphs) with |ğ‘‰| â‰« |ğ‘‰ğ¸|. In this way,\ntransformers can be easily applied to large graphs. Second, the\nego-graph preserves the localized contextual information within a\npre-defined order, which consequently filters the high-order noise.\nLast but not least, given the data-hungry [4, 18] (more data leads\nto better performance) characteristic of transformers, the sampling\nprocess can be viewed as an effective data augmentation operator,\nwhich greatly boosts the performance of graph transformers. De-\nspite these benefits, several challenges still need to be addressed\nwhen using ego-graph-based transformer. To start with, due to the\nfully-connected nature of the attention mechanism, the structural\ninformation is lost in the construction of sequential input. Therefore,\nwe need to design an effective attention mechanism to incorporate\nthe vital structural information. Moreover, though more scalable,\nthe constructed ego-graph of each node aggregates information\nsolely from its local neighbors and neglects the high-order infor-\nmation of graphs. Hence we need to design another strategy to\nincorporate the global information as complementary. Last but not\nleast, the sampled ego-graphs of a center node is essentially a subset\nof this nodeâ€™s full-neighbor ego-graph, which may lost important\ninformation and renders potentially unstable performance.\nTo address the aforementioned challenges, in this paper we pro-\npose a novel model dubbed Ego-graph Transformer (Gophormer)\nto learn desirable node representations. We demonstrate that us-\ning sampled ego-graphs instead of conventional full-graph greatly\nboosts the transformerâ€™s performance. Specifically, Gophormer con-\nstructs ego-graphs integrated with global nodes by Node2Seq to\ncapture both local and global structural information. After that,\nproximity-enhanced attention mechanism is proposed to incorpo-\nrate both feature and structural proximity to learn node embeddings.\nTo alleviate the uncertainty caused by sampling module, the ego-\ngraph consistency regularization is applied in the training phase\nto regularize different samples, and the multi-sample inference\nstrategy is proposed to stabilize predictions. It is noteworthy to\nhighlight our contributions as follows:\nâ€¢We propose to use Node2Seq that converts graph data to ego-\ngraph-based sequential input for transformers. Node2Seq not\nonly enables scalable training but also successfully boosts ex-\nisting graph transformer frameworks on node classification\ntasks significantly.\nâ€¢We propose a novel model Gophormer. Gophormer utilizes\nNode2Seq to generate input sequential data and encodes\nit with proximity-enhanced transformer. To alleviate the\nimpact of uncertainty, we design ego-graph consistency reg-\nularization at training time and propose to use multi-sample\ninference during testing time.\nâ€¢Extensive experiments are conducted on six benchmark\ndatasets. Gophormer achieves state-of-the-art performance,\ndemonstrating the great potential of graph transformers.\n2 BACKGROUND\nGiven an input graph ğº = (ğ‘‰,ğ¸ )composed of a node set ğ‘‰ and\nan edge set ğ¸, its graph structure information can be represented\nas an adjacency matrix A âˆˆ{0,1}|ğ‘‰|Ã—|ğ‘‰|, where |ğ‘‰|denotes the\nnumber of nodes in the graph. The element A[ğ‘–,ğ‘— ]in the adjacency\nequals to 1 if there exists an edge between nodeğ‘£ğ‘– and ğ‘£ğ‘—, otherwise\nA[ğ‘–,ğ‘— ]= 0. Each node ğ‘£ğ‘– is associated with a feature vector xğ‘– âˆˆ\nX âˆˆR|ğ‘‰|Ã—ğ‘‘ğ¹ and a label vector ğ’šğ‘ âˆˆRğ¶Ã—1, where ğ¶ denotes the\nnumber of classes. In this work, we focus on node classification task,\nin which a set of nodes ğ‘‰ğ¿ have observed their labels Yğ¿ and the\nlabels of other nodes, denoted as Yğ‘ˆ,remaining unobserved. The\ngoal of node classification is to learn a mapping ğ‘“ : ğº,X,Yğ¿ â†’Yğ‘ˆ\nto infer the unobserved labels Yğ‘ˆ, utilizing the graph ğº, the node\nfeatures X and the observed labels Yğ¿.\n2.1 Graph Neural Network\nGNN model was first proposed by Scarselli et al. to handle both\nnode and graph level tasks [25]. Then, encouraged by the success\nof convolutional neural networks (CNNs) [21] in the computer vi-\nsion domain, CNNs was generalized to graph-structured data [5, 8].\nKipf and Welling simplified the spectral GNN model and proposed\ngraph convolutional networks (GCNs) [17]. After that, numerous\nGNN variants have been proposed [9, 15, 28, 32]. Due to its remark-\nable performance, GNNs have been widely used on many graph\napplications [13, 17, 29, 31, 32, 35].\nDespite the great success of existing GNN methods, the message-\npassing schema of GNNs also suffers from some inherent problems:\nTo start with, most GNNs suffer from the over-smoothing problem.\nIt has been observed that deeply stacking the layers often results\nin significant performance drop for GNNs, such as GCN[17] and\nGAT [28], even beyond just a few (2â€“4) layers [ 39]. The reason\nis that, as the graph convolution operation have been shown as\na special form of Laplacian smoothing [22], stacking many GNN\nlayers repeats the Laplacian smoothing operation and eventually\nmakes node embeddings indistinguishable. Another key issue with\nexisting GNNs is the over-squashing problem [1]. Due to the expo-\nnential blow-up in computation path as the model depth increase,\nit is hard for GNNs to pass information to distant neighbors with\nGophormer: Ego-Graph Transformer for Node Classification Woodstock â€™18, June 03â€“05, 2018, Woodstock, NY\nNode2Seq\nâ€¦\nReadout Layer\nMLP Classifier\nProximity Encodings\nProx-Enhanced \nMulti-Head Att.\nAdd & Norm\nFeed Forward\nNetwork\nAdd & Norm\n<latexit sha1_base64=\"y2n9K6noUIUY1z7at82tkpnENuQ=\">AAAB73icbVDJSgNBEK2JW4xb1KOXxiDkFGZETXIQAl48eIhgFkiG0NPpJE16FrtrhDDkJ/TgQRGvfoA/4s2/sbPg/qDg8V4VVfW8SAqNtv1upRYWl5ZX0quZtfWNza3s9k5dh7FivMZCGaqmRzWXIuA1FCh5M1Kc+p7kDW94NvEbN1xpEQZXOIq469N+IHqCUTRS84K0Ufhcd7I5u2BPQf4SZ05ylfypfD2Gu2on+9buhiz2eYBMUq1bjh2hm1CFgkk+zrRjzSPKhrTPW4YG1Cxxk+m9Y3JglC7phcpUgGSqfp9IqK/1yPdMp09xoH97E/E/rxVjr+QmIohi5AGbLerFkmBIJs+TrlCcoRwZQpkS5lbCBlRRhiaijAnB+fq9bFA8mpOy8xlC/bDgnBSOLk0aJZghDXuwD3lwoAgVOIcq1ICBhFt4gEfr2rq3nqznWWvKms/swg9YLx86NpKY</latexit>\nL â‡¥\n(a) (c)\n(b) Ego-subgraphs\nCR Loss Sup. Loss\nInput sequences\nâ€¦\n(d)\nâ€¦\nâ€¦\n<latexit sha1_base64=\"dk0JHgErXhqK2pJjkqcv0FvcrXQ=\">AAAB7XicbVDLSgNBEOyNrxhfUY9eBqPgKexKMMkt4MVjBPOAZAmzk9lkzOzMMjMrhCX/4MWDIl79H2/+jZNN8F3QUFR1090VxJxp47rvTm5ldW19I79Z2Nre2d0r7h+0tUwUoS0iuVTdAGvKmaAtwwyn3VhRHAWcdoLJ5dzv3FGlmRQ3ZhpTP8IjwUJGsLFSu98cs4E7KJbcspsB/SXekpQaJ5ChOSi+9YeSJBEVhnCsdc9zY+OnWBlGOJ0V+ommMSYTPKI9SwWOqPbT7NoZOrXKEIVS2RIGZer3iRRHWk+jwHZG2Iz1b28u/uf1EhPW/JSJODFUkMWiMOHISDR/HQ2ZosTwqSWYKGZvRWSMFSbGBlSwIXhfv9ctqpUlqXufIbTPy95FuXJdKTVqizQgD0dwDGfgQRUacAVNaAGBW7iHR3hypPPgPDsvi9acs5w5hB9wXj8AteyPgA==</latexit>\n\u0000 0\n<latexit sha1_base64=\"dwA+ogI5XzK7L5RcpPFF5wQetno=\">AAAB7XicbVDLSgNBEOyNrxhfUY9eBqPgKexKMMkt4MVjBPOAZAmzk9lkzOzMMjMrhCX/4MWDIl79H2/+jZNN8F3QUFR1090VxJxp47rvTm5ldW19I79Z2Nre2d0r7h+0tUwUoS0iuVTdAGvKmaAtwwyn3VhRHAWcdoLJ5dzv3FGlmRQ3ZhpTP8IjwUJGsLFSu98cs4E3KJbcspsB/SXekpQaJ5ChOSi+9YeSJBEVhnCsdc9zY+OnWBlGOJ0V+ommMSYTPKI9SwWOqPbT7NoZOrXKEIVS2RIGZer3iRRHWk+jwHZG2Iz1b28u/uf1EhPW/JSJODFUkMWiMOHISDR/HQ2ZosTwqSWYKGZvRWSMFSbGBlSwIXhfv9ctqpUlqXufIbTPy95FuXJdKTVqizQgD0dwDGfgQRUacAVNaAGBW7iHR3hypPPgPDsvi9acs5w5hB9wXj8At3CPgQ==</latexit>\n\u0000 1\n<latexit sha1_base64=\"FQcsJG0yfTmh9r3HB9WkRrJXD5A=\">AAAB8XicbVDLSsNAFL2pr1pfVZduBqvgxpJIsO2u4MaNUME+sA1lMp20QyeTMDMRSuhfuHGhiFv/xp1/4zQtvg9cOJxzL/fe48ecKW3b71ZuaXlldS2/XtjY3NreKe7utVSUSEKbJOKR7PhYUc4EbWqmOe3EkuLQ57Ttjy9mfvuOSsUicaMnMfVCPBQsYARrI932GiPWT69OnWm/WLLLdgb0lzgLUqofQYZGv/jWG0QkCanQhGOluo4day/FUjPC6bTQSxSNMRnjIe0aKnBIlZdmF0/RsVEGKIikKaFRpn6fSHGo1CT0TWeI9Uj99mbif1430UHVS5mIE00FmS8KEo50hGbvowGTlGg+MQQTycytiIywxESbkAomBOfr95pBxV2QmvMZQuus7JyX3Wu3VK/O04A8HMAhnIADFajDJTSgCQQE3MMjPFnKerCerZd5a85azOzDD1ivH4b3kRs=</latexit>\n\u0000 M \u0000 1\nFigure 1: Model framework of Gophormer. (a) A sample graph with two example nodes colored red and blue. (b) The Node2Seq\nprocess: ego-graphs are sampled from the original graph and converted to sequential data. White nodes are context nodes,\nyellow nodes are global nodes to store graph-level context. In the example two sample ego-graphs are drawn for each node. (c)\nThe proximity encoding process. (d) The main graph transformer framework of Gophormer.\na fixed vector size. Hence, GNNs perform poorly if the prediction\ntask is depended on long-range interactions.\n2.2 Graph Transformers\nTransformers [27] are powerful encoders composed of two ma-\njor components: a multi-head self-attention (MHA) module and\na position-wise feed-forward network (FFN). The MHA module\nworks as follows: Given an input sequence of H = [ğ’‰1,Â·Â·Â· ,ğ’‰ğ‘›]âŠ¤âˆˆ\nRğ‘›Ã—ğ‘‘ where ğ‘‘is the hidden dimension and ğ’‰ğ‘– âˆˆRğ‘‘Ã—1 is the hidden\nrepresentation at position ğ‘–, the MHA module firstly projects the\ninput H to query-, key-, value-spaces, denoted as Q,K,V, using\nthree matrices Wğ‘„ âˆˆRğ‘‘Ã—ğ‘‘ğ¾,Wğ¾ âˆˆRğ‘‘Ã—ğ‘‘ğ¾ and Wğ‘‰ âˆˆRğ‘‘Ã—ğ‘‘ğ‘‰:\nQ = HWğ‘„, K = HWğ¾, V = HWğ‘‰. (1)\nThen, in each head â„ âˆˆ{1,2,...,ğ» }, the scaled dot-product atten-\ntion mechanism is applied to the corresponding âŸ¨Qâ„,Kâ„,Vâ„âŸ©:\nhead â„ = Softmax\n \nQâ„Kğ‘‡\nâ„âˆšï¸\nğ‘‘ğ¾\n!\nVâ„, (2)\nand the outputs from different heads are further concatenated and\ntransformed to obtain the final output of MHA:\nMHA(H)= Concat (head1,..., head ğ»)Wğ‘‚, (3)\nwhere Wğ‘‚ âˆˆRğ‘‘Ã—ğ‘‘. In this work, we employ ğ‘‘ğ¾ = ğ‘‘ğ‘‰ = ğ‘‘/ğ».\nDue to its remarkable performance, the aforementioned trans-\nformer [27] paradigm has become the go-to architecture in many\nfields such as natural language processing [7, 10, 27] and computer\nvision [11, 23]. In observation of these impressive achievements,\na natural idea is to introduce transformer to graphs. Yet, directly\napplying such feature-based transformers on graphs will lead to the\nloss of structural information, whereas the structural information\nis vital in graph mining tasks. Therefore, the key bottleneck of the\nperformance of graph transformer lies in the effective utilization of\ngraph structures.\nTo overcome this bottleneck, several structural preserving tech-\nniques have been proposed. For example, GT [12] proposes a gener-\nalization of transformer on graph and uses Laplacian eigen-vectors\nas positional encoding to enhance the node features. SAN [20] re-\nplaces the static Laplacian eigen-vectors with learnable positional\nencodings and designs an attention mechanism that distinguishes\nlocal connectivity. Graphormer [34] utilizes centrality encoding to\nenhance the node feature and uses spatial encoding (SPD-indexed\nattention bias) along with edge encoding to incorporate structural\ninductive bias to the attention mechanism.\nHowever, there are several inherent limitations of existing graph\ntransformers. Above all, the graph is essentially treated as fully-\nconnected with the MHA mechanism calculating attention for all\nnode pairs, causing severe scalability problem. Whatâ€™s more, since\nthe entire graph is treated as only one sequence, this scheme suffers\nfrom data scarcity problem if graphs are few (especially for node\nclassification tasks where only one graph is provided in most cases).\nTherefore, better means of incorporating structural information is\nrequired in developing graph transformer models.\n3 THE PROPOSED METHOD\nIn light of the limitations of the full-graph transformers, we pro-\npose Gophormer, shown in Figure 1, with two simple yet effective\ndesigns to incorporate the structural information into traditional\ntransformer. On one hand, Gophormer utilizes the graph topol-\nogy to generate training inputs via the Node2Seq module, where\nthe sampled ego-graph induced by each center (training) node is\nconverted to a sequential input. Therefore, the local structural infor-\nmation is explicitly preserved in the ego-graphs. On the other hand,\nWoodstock â€™18, June 03â€“05, 2018, Woodstock, NY Anonymous Author(s).\nGophormer leverages proximity-enhanced transformer, which uti-\nlizes the proximity encoding composed by different views of struc-\ntural information to calculate attention values. To perform node\nclassification, the encoded ego-graph representations are read out\nand mapped by MLP classifier to the final prediction. The model is\noptimized by supervised loss and consistency regularization loss.\nWe first introduce how Gophormer prepares sequential data for\ntransformer using the Node2Seq module.\n3.1 Node2Seq\nAs stated in Section 2.2, current graph transformer methods [ 12,\n20, 34] utilize the entire graph to generate input sequences. How-\never, this paradigm is not only memory-consuming but also hard\nto train, leading to poor performance (further discussed in Section\n4.3). To alleviate such limitations of full-graph-based transformers,\nwe propose to sample ego-graphs as input sequences to capture the\nlocal contextual information. Specifically, in each epoch, we sample\nğ‘† ego-graphs for each training node. In this paper, we use Graph-\nSAGE sampling [15], which uniformly samples the neighbors of\neach layer to iteratively construct the ego-graphs with pre-defined\nmaximum depth ğ·. Therefore, the local contextual information\nwithin ğ·-hop is randomly selected as sequential input for the trans-\nformer encoder. Note that, users may flexibly design other sampling\nmethod to determine the range of contextual nodes. For example,\na sampling strategy focusing on high-order neighbors could be a\ngood choice for heterophilic graphs [40].\nThis strategy is beneficial in several aspects: (1) The ego-graph-\nbased attention can be viewed as full-graph-based attention with\nrandom hard attention masks focusing on local information, i.e.\nnodes outside the sampled ego-graphs are with multiplicative mask\nvalue âˆ’âˆ, in this way the structural information is incorporated. (2)\nUsing sampled ego-graphs instead of full-graph can be viewed as a\ndata augmentation technique which not only enjoys performance\ngain due to transformersâ€™ data hungry nature [4, 18] but also allevi-\nates the risk of overfitting. Intuitively speaking, since the sampled\nego-graphs from the same node can be different in different epochs,\nit is harder for the model to fit the training data, enforcing training\nmore generalized model. (3) This strategy apparently enables more\nscalable training with attention values calculated inside ego-graphs\ninstead of full-graphs. Since the input size is reduced from |ğ‘‰|to\n|ğ‘‰ğ¸|(average number of nodes in ego-graphs) with |ğ‘‰|â‰«| ğ‘‰ğ¸|, the\nmemory requirement (quadratic to input size) is greatly reduced.\nSo far our proposal only focuses on the local structural contexts,\ni.e. neighbors within pre-defined depth ğ·, while the high-order\nneighbors are neglected. Unfortunately, the importance of such\nglobal contextual information has been widely recognized [1, 24].\nTo alleviate this problem, inspired by [10, 34, 37], we propose to\nuse global nodes to store global context information. Specifically,\nwe add ğ‘›ğ‘” global nodes to each sampled ego-graph with learnable\nfeatures C = {cğ‘– âˆˆRğ‘‘Ã—1,ğ‘– âˆˆ1,...,ğ‘› ğ‘”}. These global nodes are\nshared across all the ego-graphs, which preserves the global context\ninformation that assist the node classification task. Hence, in each\nGophormer layer, each node not only fuses the information of local\nneighbors but also the global context information.\nIn sum, for each epoch, a center node ğ‘£ğ‘ and its induced training\nego-graphs Gğ‘ = {ğº(ğ‘ )\nğ‘ |ğ‘  âˆˆ{1,2,...,ğ‘† }}consist of sampled nodes\nand global nodes, the input of transformer H0ğ‘,ğ‘  is defined as:\nH0\nğ‘,ğ‘  = Concat\n\u0010\nX(ğ‘ )\nğ‘ ,C\n\u0011\n, (4)\nwhere X(ğ‘ )\nğ‘ denotes the node features of ğº(ğ‘ )\nğ‘ .\n3.2 Proximity-Enhanced Transformer\nIn this section, we introduce the proposed attention mechanism of\nGophormer. As introduced in 2.2, the attention score of traditional\ntransformers [27] is calculated by the dot product between en-\ncoded query and key embeddings. That is to say, if directly applied,\nthe attention scores between different nodes in each ego-graph\nare calculated by measuring the similarity between encoded fea-\ntures whereas the vital structural information is ignored. To tackle\nthis, one way to introduce structural inductive bias is to use SPD-\nindexed attention bias [34]. However, this method has two inherent\nlimitations: First, as the name suggests, the SPD only captures the\nshortest-path relationship, neglecting other structural relationships,\ne.g. identity [36] and higher-order relationships [24]. Moreover, the\nSPD-indexed attention bias only reflects the shortest path connect-\nedness between a node pair, neglecting the fine-grained probabili-\nties of connectedness. For example, given two pairs of nodes with\nSPD as 2, the node pair with more 2-hop path instances apparently\nhave stronger relationships compared to the pair with fewer path\ninstances. Thus, these two pairs of nodes should not be equally\ntreated even if their SPD values are identical.\nWe propose proximity-enhanced multi-head attention (PE-MHA)\nto overcome these limitations. Through depiction of the proposed\nPE-MHA mechanism, we omit the following notations for brevity:\nğ‘ for center node, ğ‘  for sample index, and ğ‘™ for transformer layers;\nsince the attention mechanism are identical in each layer and in\neach ego-graph. Specifically, for a node pair\n\nğ‘£ğ‘–,ğ‘£ğ‘—\n\u000b\n, ğ‘€ views of\nstructural information is encoded as a proximity encoding vector,\ndenoted as ğ“ğ‘–ğ‘— âˆˆRğ‘€Ã—1, to enhance the attention mechanism. The\nproximity-enhanced attention score ğ›¼ğ‘–ğ‘— is defined as:\nğ›¼ğ‘–ğ‘— =\n\u0000ğ’‰ğ‘–Wğ‘„\n\u0001 \u0000ğ’‰ğ‘—Wğ¾\n\u0001ğ‘‡\nâˆš\nğ‘‘\n+ğ“ğ‘‡\nğ‘–ğ‘—ğ’ƒ, (5)\nwhere ğ’ƒ âˆˆRğ‘€Ã—1 stands for the learnable parameters that calculate\nthe bias of different structural information. The proximity encoding\nis calculated by ğ‘€ structural encoding functions defined as:\nğ“ğ‘–ğ‘— = Concat(Î¦ğ‘š(ğ‘£ğ‘–,ğ‘£ğ‘—)|ğ‘š âˆˆ0,1,..,ğ‘€ âˆ’1), (6)\nwhere each structural encoding function Î¦ğ‘š encodes a view of\nstructural information. In this paper, we consider two aspects of\nstructural information for each node pair: (1) whether the node\npairs are connected at specific order, (2) whether global node exists\nin this node pair. The proximity encoding functions are defined as:\nÎ¦ğ‘š(ğ‘£ğ‘–,ğ‘£ğ‘—)=\n\u001a ËœAğ‘š[ğ‘–,ğ‘— ], if ğ‘š < ğ‘€âˆ’1\nI(ğ‘–,ğ‘— ), if ğ‘š= ğ‘€âˆ’1 , (7)\nwhere I(ğ‘–,ğ‘— ) = 1 if global nodes exists in\n\nğ‘£ğ‘–,ğ‘£ğ‘—\n\u000b\n, otherwise 0,\nËœA = Norm(A +I)denotes the normalized adjacency matrices with\nself-loop. In this way, the firstğ‘€âˆ’1 dimensions of ğ“ğ‘–ğ‘— encodes the\nreachable probabilities from 0-order (identity relationship [36]), i.e.\nwhether ğ‘£ğ‘– is ğ‘£ğ‘—, to (ğ‘€âˆ’2)-order between node ğ‘£ğ‘– and ğ‘£ğ‘—. Hence,\nGophormer: Ego-Graph Transformer for Node Classification Woodstock â€™18, June 03â€“05, 2018, Woodstock, NY\nthe fine-grained proximity that reflects different orders of structural\ninformation is preserved for each node pair.\nThe aforementioned proximity encoding scheme enables the\nproposed Gophormer with strong expressiveness to handle struc-\ntural data. In fact, the attention mechanism of Graphormer with\nSPD-indexed bias can be viewed as a special case of proximity-\nenhanced attention mechanism with one-hot proximity-encoding\nof the shortest-path-order. Moreover, as the attention mechanism of\nGraphormer can be viewed as a special case of PE-MHA, Gophormer\nalso enjoys the merits of representing the aggregation and combi-\nnation steps in popular GNN models [34].\nWe follow the GT [12] framework to obtain the output of the\nğ‘™-th transformer layer, denoted as Hğ‘™:\nË†Hğ‘™ = Norm\n\u0010\nPE-MHA\n\u0010\nHğ‘™âˆ’1\n\u0011\n+Hğ‘™âˆ’1\n\u0011\n,\nHğ‘™ = Norm\n\u0010\nFFN\n\u0010\nË†Hğ‘™\n\u0011\n+Ë†Hğ‘™\n\u0011\n,\n(8)\nwhere Norm(Â·)denotes the layer-norm function. By stacking ğ¿lay-\ners, Gophormer encodes each node inside the sampled ego-graphs\nğº(ğ‘ )\nğ‘ âˆˆGğ‘ and obtain the embedding of the center nodeğ‘£ğ‘, denoted\nas ğ’›(ğ‘ )\nğ‘ , via a readout function:\nğ’›(ğ‘ )\nğ‘ = Readout\n\u0010\nğ’‰ğ¿\nğ‘–,ğ‘,ğ‘  |ğ‘£ğ‘– âˆˆğº(ğ‘ )\nğ‘\n\u0011\n. (9)\nThe readout function can be implemented by graph pooling func-\ntions [2, 32]. Since we are interested in the node property of the\ncenter node ğ‘£ğ‘, in this paper, we simply use the center node repre-\nsentation as the final node embedding, i.e. ğ’›(ğ‘ )\nğ‘ = ğ’‰ğ¿ğ‘,ğ‘,ğ‘ .\n3.3 Optimization\nIn this section, we introduce how to optimize the Gophormer model.\nGiven the center node representation ğ’›(ğ‘ )\nğ‘ of a sampled ego-graph\nğº(ğ‘ )\nğ‘ âˆˆGğ‘, Gophormer first adopts a MLP (Multi-Layer Perceptron)\nfunction ğ‘“ğ‘šğ‘™ğ‘ with parameters Î˜ to predict the node class:\neğ’š(ğ‘ )\nğ‘ = ğ‘“ğ‘šğ‘™ğ‘\n\u0010\nğ’›(ğ‘ )\nğ‘ ,Î˜\n\u0011\n, (10)\nwhere eğ’š(ğ‘ )\nğ‘ âˆˆRğ¶Ã—1 stands for the classification result, ğ¶ stands\nfor the number of classes. The supervised loss is achieved by the\naverage cross entropy loss of labeled training nodes ğ‘‰ğ¿:\nLsup = âˆ’1\nğ‘†\nâˆ‘ï¸\nğ‘£ğ‘âˆˆğ‘‰ğ¿\nğ‘†âˆ‘ï¸\nğ‘ =1\nğ’šğ‘‡\nğ‘ logeğ’š(ğ‘ )\nğ‘ , (11)\nwhere ğ’šğ‘ âˆˆRğ¶Ã—1 is the ground truth label of center node ğ‘£ğ‘. We\nuse consistency regularization [3, 14] to enforce the model make\nsimilar predictions for ego-graphs induced by the same center node:\nLcon = 1\nğ‘†\nâˆ‘ï¸\nğ‘£ğ‘âˆˆğ‘‰ğ¿\nğ‘†âˆ‘ï¸\nğ‘ =1\n\r\r\rğ’šâ€²\nğ‘ âˆ’eğ’š(ğ‘ )\nğ‘\n\r\r\r\n2\n2\n, (12)\nwhere ğ’šâ€²\nğ‘ is the sharpened average distribution of ego-graphs (read-\ners may refer to [14] for more details). The overall loss is obtained\nby fusing supervised loss Lğ‘ ğ‘¢ğ‘ and consistency regularization loss\nLcon with coefficient ğœ†:\nL= Lğ‘ ğ‘¢ğ‘ +ğœ†Lcon. (13)\n3.4 Inference\nAlthough sampling ego-graphs is an effective data augmentation\ntechnique, each sampled ego-graph only preserves partial infor-\nmation within ğ·-hop. Therefore, chances are that the important\ninformation is lost when sampling testing ego-graphs, leading to\ninferior results. A straight-forward method to solve this problem is\nto infer node embeddings based on the full-ego-graph [15], i.e. get\nall the induced nodes within the maximum depth ğ·. However, we\nempirically find that this method leads to inferior results (further\ndiscussed in Section 4.4.4). The potential reason is that the model is\ntrained on ego-graphs while tested on the full-ego-graphs. There-\nfore, during test time, the model is asked to inference on much\nlarger graphs, leading to performance downgrade [26, 30].\nTo bridge the gap between training and testing graphs while alle-\nviating information loss at the same time, we propose a simple yet\neffective inference method dubbed multi-sample inference: During\ntesting time, the model samples ğ‘†â€²ego-graphs for each nodes with\nthe same sampling strategy in training time. The final prediction of\na center node ğ‘£ğ‘, denoted as eğ’šğ‘, is obtained by the average of the\npredictions of ğ‘†â€²sampled ego-graphs:\neğ’šğ‘ = 1\nğ‘†â€²\nğ‘†â€²\nâˆ‘ï¸\nğ‘ =1\neğ’š(ğ‘ )\nğ‘ . (14)\nThrough this way, the ego-graph sampling method is consistent\nduring training and testing and the loss of information can be\nalleviated by using larger ğ‘†â€².\n4 EXPERIMENTS\nIn this section, we conduct extensive experiments to comprehen-\nsively evaluate the proposed Gophormer model. Following previous\nworks [15, 17, 28], node classification is selected as the downstream\ntask to evaluate the effectiveness of our proposal. After that, abla-\ntion studies are conducted to demonstrate the importance of differ-\nent components. Finally, the sensitivity of the model performance\non several core parameters are discussed.\n4.1 Experimental Setup\n4.1.1 Datasets. To comprehensively evaluate the effectiveness of\nGophormer, we conduct experiments on the six benchmark datasets\nincluding four citation network datasets (i.e. Cora, Citeseer, Pubmed [17]\nand DBLP [41]), and two social network datasets (i.e. Blogcatalog\nand Flickr [16]). We set the train-validation-test split as 60%/20%/20%.\nThe statistics of datasets are shown in Table 1.\n4.1.2 Baselines. To evaluate the effectiveness of Gophormer in\ngraph mining area, we compare it with 8 baseline methods, includ-\ning five popular GNN methods, i.e. GCN [ 17], GAT [28], Graph-\nSAGE [15], JKNet [ 33], and APPNP [ 19], along with four state-\nof-the-art graph transformers, i.e. GT-Sparse [ 12], GT-Full [ 12],\nGraphormer [34], and SAN [20].\n4.1.3 Implementation Details. We implement Gophormer with\nPython (3.8.5), Pytorch (1.9.1) and DGL (0.7.1). The code will be\nmade public upon paper publication. We use a 1-Layer-MLP as the\nğ‘“ğ‘šğ‘™ğ‘ in Eq.10 to predict the classes. The global nodes are added to\nthe input of first Gophormer layerâ€™s FFN layer since the feature\ndimension ğ‘‘ğ¹ might unequal with the hidden dimension ğ‘‘. We use\nWoodstock â€™18, June 03â€“05, 2018, Woodstock, NY Anonymous Author(s).\nDataset #Nodes #Edges #Classes #Features Type\nCora 2,708 5,278 7 1,433 Citation network\nCiteseer 3,327 4,522 6 3,703 Citation network\nDBLP 17,716 52,864 4 1,639 Citation network\nPubmed 19,717 44,324 3 500 Citation network\nBlogcatalog 5,196 171,743 6 8,189 Social network\nFlickr 7,575 239,738 9 12,047 Social network\nTable 1: The statistics of the datasets.\nModel Cora Citeseer Blogcatalog Pubmed DBLP Flickr\nGCN 87.33Â±0.38 79.43 Â±0.26 78.81 Â±0.29 84.86 Â±0.19 83.62 Â±0.13 61.49 Â±0.61\nGAT 86.29Â±0.53 80.13 Â±0.62 73.20Â±1.46 84.40 Â±0.05 84.19 Â±0.19 54.29 Â±2.56\nGraphSAGE 86.90Â±0.84 79.23 Â±0.53 76.73 Â±0.39 86.19 Â±0.18 84.73 Â±0.28 60.37Â±0.27\nAPPNP 87.15Â±0.43 79.33 Â±0.35 95.63 Â±0.23 87.04Â±0.17 84.40 Â±0.17 93.25Â±0.24\nJKNet 87.70Â±0.65 78.43Â±0.31 78.46 Â±1.74 87.64 Â±0.26 84.57Â±0.28 53.66 Â±0.40\nGT-full 63.40Â±0.94 58.75 Â±1.06 65.32 Â±0.46 77.29 Â±0.50 78.15 Â±0.41 60.77 Â±0.82\nGT-sparse 71.84Â±0.62 67.38 Â±0.76 70.65 Â±0.47 82.11 Â±0.39 81.04 Â±0.27 68.59 Â±0.64\nSAN 74.02Â±1.01 70.64 Â±0.97 74.98 Â±0.52 86.22 Â±0.43 83.11 Â±0.32 70.26 Â±0.73\nGraphormer 72.85Â±0.76 66.21 Â±0.83 71.84 Â±0.33 82.76 Â±0.24 80.93 Â±0.39 66.16 Â±0.24\nGophormer 87.85Â±0.10 80.23 Â±0.09 96.03 Â±0.28 89.40 Â±0.14 85.20 Â±0.20 91.51Â±0.28\nTable 2: Node classification performance (mean Â±std%, the best results are bolded and the runner-ups are underlined).\nAdam as optimizer and adopt the ReduceLROnPlateau scheduler of\nPytorch. Specifically, the learning rate first goes through a warm-up\nprocess, where the learning rate increase linearly from zero to the\npeak learning rate and then decay with the decay factor according\nto the validation performance until it reaches the end learning rate.\n4.1.4 Parameter Settings. We fix some hyper-parameters for the\nconvenience of tuning work: the dropout is set to 0.5, the weight\ndecay is set to 1e-5, the local contextual information of maximum\ndepth ğ· is set as 2, the hidden dimension ğ‘‘ is set to 64 and the\nnumber of attention head ğ» is set as 8, the end learning rate is\nset to 1e-9. Other important hyper-parameters are tuned on each\ndataset by grid search. The search space of learning rate, batch size,\nnumber of layers, number global nodes, the number of structural\nviews ğ‘€ for proximity encoding, and the regularization balancing\ncoefficient ğœ†in Eq.13, are{0.0001,0.0002}, {32,64,128}, {1,2,3,4,5},\n{0,1,2,3,4}, {2,3,4}, {0.5,0.75,1}, respectively.\n4.2 Node Classification Performance\nThe node classification performance of all models are shown in\nTable 2, from which we have the following observations: (1) Graph\ntransformer baselines are generally outperformed by GNNs es-\npecially on the small scale datasets (i.e. Cora and Citeseer). This\nphenomenon is probably caused by two reasons: First, transformers\nhave much more parameters compared with GNNs and thus they\nare harder to train especially on small datasets. Second, in most\nof these datasets, local information is quite important while the\nattention over the entire graph essentially introduces more noise\nto the model. (2) Gophormer achieves state-of-the-art results and\noutperforms all graph transformer baselines on all six datasets, prov-\ning the effectiveness of our proposed model. Notably, Gophormer\nachieves better performance than the GNN-based baselines on the\nsmall scale datasets (i.e. Cora and Citeseer), indicating the good\ngeneralization ability of our proposed model. (3) Performance gains\nof Gophormer on graphs with more nodes or edges (e.g., Flickr,\nPubmed and DBLP) are more significant than the ones on the small\ngraphs (e.g., Cora and Citeseer). This is probably due to the data-\nhungry [4, 18] nature of transformers. Since more nodes and edges\nlead to more data and more possible data augmentation samples,\nthe superiority of Gophormer is more obvious in these datasets.\n4.3 Effectiveness of Node2Seq\nIn this section, we investigate the effectiveness of the proposed\ngraph transformer training paradigm Node2Seq by comparing it\nwith the traditional full-graph training paradigm [12, 20, 34]. We\nfirst take a closer look at the training and validation performance\nthrough training. As shown in Figure 2, it is apparently more in-\ntractable to train the full-graph based model according to the ob-\nserved training trajectories. This is reasonable as full-graph training\nhas the penitential to introduce noise from long-distance neighbors,\nwhich may disrupt and distract the training process. Meanwhile,\ncompared to the full-graph training, the trajectories of Node2Seq\nare more smooth and Node2Seq consistently outperforms the base-\nline from the early beginning, which demonstrates the superiority\nof the proposed ego-graph training mechanism.\nTo further investigate the generality of the Node2Seq technique,\nwe study whether the proposed data augmentation strategy could\nimprove the performance of the two graph transformer baselines.\nGophormer: Ego-Graph Transformer for Node Classification Woodstock â€™18, June 03â€“05, 2018, Woodstock, NY\n0 30 60 90 120 150\nEpoch\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy\nCora\nFull Graph Train\nFull Graph Validation\nNode2Seq Train\nNode2Seq Validation\n0 30 60 90 120 150\nEpoch\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy\nCiteseer\nFull Graph Train\nFull Graph Validation\nNode2Seq Train\nNode2Seq Validation\nFigure 2: The performances of full-graph and Node2Seq data augmentation through training process.\nCora70.0\n75.0\n80.0\n85.0\n90.0Accuracy\nGraphormer\nSAN\nGraphormer-Node2Seq\nSAN-Node2Seq\nGophormer\nCiteseer\n65.0\n70.0\n75.0\n80.0\nBlogcatalog\n70.0\n80.0\n90.0\n100.0\nPubmed80.0\n82.5\n85.0\n87.5\n90.0Accuracy\nDBLP80.0\n82.0\n84.0\n86.0\nFlickr\n65.0\n75.0\n85.0\n95.0\nFigure 3: The performance evaluation of graph transformers\nand their variants with Node2Seq training paradigm.\nWe integrate the proposed Node2Seq into the SAN and Graphormer\nmodels denoted as SAN-Node2Seq and Graphormer-Node2Seq. Fig-\nure 3 presents the classification results of different models. By\nenjoying the merits of Node2Seq, SAN-Node2Seq and Graphormer-\nNode2Seq consistently outperform their vanilla versions by a large\nmargin. Specifically, SAN-Node2Seq outperforms its vanilla ver-\nsion by 6.21%, 4.81%, 14.77%, 2.10%, 1.77%, and 12.86% on the six\ndatasets, respectively. Graphormer-Node2Seq achieves 4.56%, 7.65%,\n15.37%, 3.11%, 1.25%, 14.38% gains on the six datasets, respectively.\nSuch a huge performance gain further demonstrates the effective-\nness and generality of the proposed sampled ego-graph training\nparadigm. Furthermore, the proposed Gophormer model still con-\nsistently outperforms graph transformer baselines with Node2Seq\ntechniques, which proves the effectiveness of other designs within\nthe Gophormer model, e.g. proximity-enhanced transformer, CR\nloss, and multi-sample inference.\n4.4 Ablation Studies\nIn order to verify the effectiveness of different modules in Gophormer,\nwe design five sets of experiments to compare the node classifica-\ntion performance of Gophormer and its variants. The results are\nshown in Figure 4.\nCora\n86.0\n86.5\n87.0\n87.5Accuracy\nGophormer-FullInf\nGophormer-w/o-PE\nGophormer-w/o-CR\nGophormer-w/o-GN\nGophormer\nCiteseer77.0\n78.0\n79.0\n80.0\nBlogcatalog94.0\n94.5\n95.0\n95.5\n96.0\nPubmed\n87.0\n88.0\n89.0\n90.0Accuracy\nDBLP83.5\n84.0\n84.5\n85.0\n85.5\nFlickr89.0\n90.0\n91.0\n92.0\nFigure 4: The performance evaluation of variants of\nGophormer.\n4.4.1 Proximity Encoding Attention Bias. The vanilla transformer\ncalculates the attention scores purely relying on the node attributes,\nin which the structural information is ignored in the construction\nof fully-connected graphs. In light of this, the proposed Gophormer\nmodel incorporates a proximity-based attention mechanism to bet-\nter capture the structural bias by considering both feature and\nstructural proximity to learn desirable node representations. To\nverify the effectiveness of the proximity-based attention mecha-\nnism, we design an ablation model Gophormer-w/o-PE without\nthe proximity-based attentions. From Figure 4, we can observe that\nGophormer consistently outperforms Gophormer-w/o-PE on all the\nsix datasets, demonstrating the importance for graph transformer\nto properly incorporate the structure information.\n4.4.2 Consistency Regularization. As discussed in Section 3.3, in or-\nder to alleviate the uncertainty of ego-graph sampling, Gophormer\nadopts the consistency regularization loss to enforce the classifica-\ntion distribution learned from different ego-graphs sampled from\nthe same node to be similar. To investigate the effectiveness of this\ndesign, we propose a variant of Gophormer without the ego-graph\nconsistency regularization strategy, namely Gophormer-w/o-CR,\nand compare its node classification performance with Gophormer.\nFrom Figure 4, one can clearly see that model performance drops\nafter removing the consistency regularization. Without the consis-\ntency regularization, the sampled ego-graphs of an identical node\nWoodstock â€™18, June 03â€“05, 2018, Woodstock, NY Anonymous Author(s).\ncould be distinct, leading to the different predictions and inferior\nclassification performance. Our proposal can alleviate the uncer-\ntainty of sampling ego-graphs and thus boost the performance of\nGophormer.\n4.4.3 Global Nodes. As discussed in Section 3.2, Gophormer uti-\nlizes a set of shared global nodes across different ego-graphs to\nconvey global contextual information. To evaluate the importance\nof these global nodes, we design another ablation model Gophormer-\nw/o-GN without the global nodes. Figure 4 shows the classification\nperformance of this variation model. After removing the global\nnodes, the performance consistently exhibits a significant decline\nover all datasets. The ego-graphs are generated within a fixed hops\nfrom the center nodes and then fed into graph transformer. Without\nadding global nodes, the receptive field of model is limited to a local-\nized small area and further degrades the model preference. Global\nnodes can be viewed as the intermediaries to exchange global infor-\nmation across different ego-graphs, which are capable of providing\nexternal global context as complementary to the local information\npreserved in the ego-graphs. By simultaneously enjoying the mer-\nits of local and global information, Gophormer achieves desirable\nperformance over all datasets.\n4.4.4 Multi-sample Inference. As discussed in section 3.4, we pro-\npose a multi-sample inference strategy to handle uncertainty and\nrelieve the structural inconsistency between the training and testing\nsets. To verify these claims, another ablation model Gophormer-\nFullInf without the multi-sample strategy is designed. Gophormer-\nFullInf is trained on sampled ego-graphs and tested on the full-ego-\ngraph without sampling. From the results shown in Figure 4, we\nempirically find that the multi-sample schema contributes to boost-\ning the classification performance compared to the Gophormer-\nFullInf model. The results reveal that the consistent graph sampling\nmethod between training and testing sets is crucial to achieve desir-\nable performance, and the proposed multi-sample inference strategy\nis capable of achieving promising results.\nCora86.0\n86.5\n87.0\n87.5Accuracy\nFullInf\nMSI-1-Samples\nMSI-3-Samples\nMSI-5-Samples\nMSI-7-Samples\nCiteseer77.0\n77.5\n78.0\n78.5\n79.0\nBlogcatalog94.0\n94.5\n95.0\n95.5\nPubmed88.0\n88.5\n89.0\n89.5Accuracy\nDBLP83.5\n84.0\n84.5\n85.0\nFlickr88.0\n89.0\n90.0\n91.0\n92.0\nFigure 5: The impact of varying number of samples used\nmulti-sample inference settings.\n4.4.5 Evaluation on Inference Strategies. We demonstrated the ef-\nfectiveness of the proposed multi-sample inference schema in sec-\ntion 4.4.4. In this section, we aim to further reveal the potential\nreasons why this strategy will work. Two types of ablation models\nare proposed, including the FullInf which infers the predictions\nbased on the full-ego-graphs, and MSI-ğ‘˜denotes the proposed multi-\nsample inference strategy withğ‘˜samples. From the results shown in\nFigure 5, we can see that FullInf and MSI-1-sample settings achieve\nthe worst performance. This is reasonable as FullInf cannot handle\nthe structural inconsistency between graph samples in training set\nand testing set, while MSI-1-sample makes decisions solely based\non a single sample and thus its performance is severely hindered\nby the uncertainty. Other three MSI variations with larger number\nof samples achieve better performance. With the increase of ğ‘˜, the\nperformance first increases and then keeps steady. It means that the\nincorporation of more sampled ego-graphs in the inference phase\ncontributes to better classification performance at the beginning.\nWhen the real category distribution is fully revealed, more samples\ncannot further improve model performance.\n4.5 Parameter Analysis\nIn this section, we study the performance sensitivity of the proposed\nGophormer model on two core hyper-parameters: the number of\nglobal nodes ğ‘›ğ‘” and the number of transformer layers ğ¿.\n0 1 2 3 4\n87.6\n87.8\n88.0\n88.2Accuracy\nCora\n0 1 2 3 479.8\n79.9\n80.0\n80.1\nCiteseer\n0 1 2 3 4\n95.6\n95.7\n95.8\n95.9\nBlogcatalog\n0 1 2 3 489.2\n89.25\n89.3\n89.35Accuracy\nPubmed\n0 1 2 3 4\n84.9\n84.95\n85.0\n85.05\nDBLP\n0 1 2 3 4\n91.1\n91.2\n91.3\n91.4\n91.5\n91.6\nFlickr\nFigure 6: The classification performance of Gophormer with\ndifferent global nodes number ğ‘›ğ‘”.\n4.5.1 Global Nodes. The ego-graph contains only the local struc-\ntural contexts while neglects the high-order information. Therefore,\nwe propose to add ğ‘›ğ‘” shared global nodes into the sampled ego-\ngraphs to introduce the global structural information. Here we aim\nto study the impact of varying the number of global nodes. Figure\n6 presents the node classification performance of Gophormer with\nğ‘›ğ‘” varying from 0 to 4. From the results, one can see that with the\nincrease of ğ‘›ğ‘”, the performance increases until reaches at a peak\nand then decreases. This is reasonable as suitable number of global\nnodes are capable of incorporating proper global information, while\nexcessive global nodes may introduce too many parameters which\nslow down the training speed and hurt the generalization ability.\nThus, the number of global nodes should be carefully decided to\nachieve the optimal performance.\n4.5.2 Number of Gophormer Layers. We further study the influ-\nence of the number of Gophormer layers ğ¿ on the classification\nperformance. We varyğ¿from 1 to 5 and exhibit the results in Figure\n7. It is obvious that with the increase of ğ¿, the performance signifi-\ncantly increases at the beginning, since stacking Gophormer layers\nenlarges representation learning capacity which is beneficial in cap-\nturing sophisticated structural patterns. However, deeper models\nalso suffer from serious challenge of over-fitting, which is responsi-\nble for the performance decline with 5 transformer layers over all\nGophormer: Ego-Graph Transformer for Node Classification Woodstock â€™18, June 03â€“05, 2018, Woodstock, NY\n1 2 3 4 5\n86.5\n87.0\n87.5\n88.0\n88.5Accuracy\nCora\n1 2 3 4 5\n78.5\n79.0\n79.5\n80.0\nCiteseer\n1 2 3 4 5\n95.0\n95.2\n95.4\n95.6\n95.8\n96.0\nBlogcatalog\n1 2 3 4 5\n88.5\n88.75\n89.0\n89.25\n89.5\nAccuracy\nPubmed\n1 2 3 4 5\n84.5\n84.7\n84.9\n85.1\nDBLP\n1 2 3 4 590.6\n90.8\n91.0\n91.2\n91.4\n91.6\nFlickr\nFigure 7: The classification performance of Gophormer with\ndifferent numbers of network layer ğ¿.\nthe datasets. Gophormer on small datasets, like Cora and Citeseer,\nachieves best performance with ğ¿set to 2 or 3. Meanwhile on other\nlarger datasets, the best performance is often achieved when ğ¿is\nset to 3 or 4. Hence, the number of transformer layers should be\ncarefully chosen based on the graph scale and characteristics.\n5 CONCLUSION\nThe transformer architecture has shown dominant performance on\nCV and NLP tasks, but not yet in the graph field especially on node\nclassification tasks. In this work, we propose a novel ego-graph-\nbased transformer model dubbed Gophormer, which effectively\nincorporates the structural information by Node2Seq module and\nthe proximity-enhanced attention mechanism. We also design con-\nsistency regularization loss and multi-sample inference to alleviate\nthe negative impacts of sampling. Extensive experiments are con-\nducted to show the effectiveness of the proposed Gophormer model,\nrevealing the promising future of the emerging field of graph trans-\nformers.\nREFERENCES\n[1] Uri Alon and Eran Yahav. 2021. On the Bottleneck of Graph Neural Networks\nand its Practical Implications. In ICLR.\n[2] Jinheon Baek, Minki Kang, and Sung Ju Hwang. 2021. Accurate Learning of\nGraph Representations with Graph Multiset Pooling. In ICLR.\n[3] David Berthelot, Nicholas Carlini, Ian J. Goodfellow, Nicolas Papernot, Avital\nOliver, and Colin Raffel. 2019. MixMatch: A Holistic Approach to Semi-Supervised\nLearning. In NIPS.\n[4] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al . 2020. Language models are few-shot learners. arXiv preprint\narXiv:2005.14165 (2020).\n[5] Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. 2013. Spectral net-\nworks and locally connected networks on graphs. arXiv preprint arXiv:1312.6203\n(2013).\n[6] Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. 2020. Measuring\nand Relieving the Over-Smoothing Problem for Graph Neural Networks from\nthe Topological View. In AAAI.\n[7] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc Viet Le, and\nRuslan Salakhutdinov. 2019. Transformer-XL: Attentive Language Models beyond\na Fixed-Length Context. In ACL.\n[8] MichaÃ«l Defferrard, Xavier Bresson, and Pierre Vandergheynst. 2016. Convolu-\ntional neural networks on graphs with fast localized spectral filtering. Advances\nin neural information processing systems 29 (2016), 3844â€“3852.\n[9] MichaÃ«l Defferrard, Xavier Bresson, and Pierre Vandergheynst. 2016. Convo-\nlutional Neural Networks on Graphs with Fast Localized Spectral Filtering. In\nNIPS.\n[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\nNAACL.\n[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-\naohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg\nHeigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An Image is\nWorth 16x16 Words: Transformers for Image Recognition at Scale. InICLR.\n[12] Vijay Prakash Dwivedi and Xavier Bresson. 2020. A Generalization of Trans-\nformer Networks to Graphs. CoRR (2020).\n[13] Wenqi Fan, Yao Ma, Qing Li, Yuan He, Yihong Eric Zhao, Jiliang Tang, and Dawei\nYin. 2019. Graph Neural Networks for Social Recommendation. In WWW.\n[14] Wenzheng Feng, Jie Zhang, Yuxiao Dong, Yu Han, Huanbo Luan, Qian Xu, Qiang\nYang, Evgeny Kharlamov, and Jie Tang. 2020. Graph Random Neural Networks\nfor Semi-Supervised Learning on Graphs. In NIPS.\n[15] William L. Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive Represen-\ntation Learning on Large Graphs. In NIPS.\n[16] Xiao Huang, Jundong Li, and Xia Hu. 2017. Label informed attributed network\nembedding. In WSDM.\n[17] Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with\nGraph Convolutional Networks. In ICLR.\n[18] Yuval Kirstain, Patrick Lewis, Sebastian Riedel, and Omer Levy. 2021. A Few More\nExamples May Be Worth Billions of Parameters. arXiv preprint arXiv:2110.04374\n(2021).\n[19] Johannes Klicpera, Aleksandar Bojchevski, and Stephan GÃ¼nnemann. 2019. Pre-\ndict then Propagate: Graph Neural Networks meet Personalized PageRank. In\nICLR.\n[20] Devin Kreuzer, Dominique Beaini, William L. Hamilton, Vincent LÃ©tourneau, and\nPrudencio Tossou. 2021. Rethinking Graph Transformers with Spectral Attention.\nCoRR (2021).\n[21] Yann LeCun, Yoshua Bengio, et al. 1995. Convolutional networks for images,\nspeech, and time series. The handbook of brain theory and neural networks 3361,\n10 (1995), 1995.\n[22] Qimai Li, Zhichao Han, and Xiao-Ming Wu. 2018. Deeper insights into graph\nconvolutional networks for semi-supervised learning. In Thirty-Second AAAI\nconference on artificial intelligence .\n[23] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin,\nand Baining Guo. 2021. Swin Transformer: Hierarchical Vision Transformer\nusing Shifted Windows. CoRR abs/2103.14030 (2021).\n[24] Mingdong Ou, Peng Cui, Jian Pei, Ziwei Zhang, and Wenwu Zhu. 2016. Asym-\nmetric Transitivity Preserving Graph Embedding. In KDD.\n[25] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele\nMonfardini. 2008. The graph neural network model. IEEE transactions on neural\nnetworks 20, 1 (2008), 61â€“80.\n[26] Xianfeng Tang, Huaxiu Yao, Yiwei Sun, Yiqi Wang, Jiliang Tang, Charu Aggarwal,\nPrasenjit Mitra, and Suhang Wang. 2020. Investigating and Mitigating Degree-\nRelated Biases in Graph Convolutional Networks. In CIKM.\n[27] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All\nyou Need. In NIPS, Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M.\nWallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (Eds.).\nWoodstock â€™18, June 03â€“05, 2018, Woodstock, NY Anonymous Author(s).\n[28] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro\nLiÃ², and Yoshua Bengio. 2018. Graph Attention Networks. In ICLR.\n[29] Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019.\nNeural Graph Collaborative Filtering. In SIGIR.\n[30] Yiqi Wang, Yao Ma, Charu Aggarwal, and Jiliang Tang. 2020. Non-IID Graph\nNeural Networks. arXiv preprint arXiv:2005.12386 (2020).\n[31] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and\nPhilip S. Yu. 2019. A Comprehensive Survey on Graph Neural Networks. CoRR\nabs/1901.00596 (2019).\n[32] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2019. How Powerful\nare Graph Neural Networks?. In ICLR.\n[33] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi\nKawarabayashi, and Stefanie Jegelka. 2018. Representation learning on graphs\nwith jumping knowledge networks. InInternational Conference on Machine Learn-\ning. PMLR, 5453â€“5462.\n[34] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He,\nYanming Shen, and Tie-Yan Liu. 2021. Do Transformers Really Perform Bad for\nGraph Representation? CoRR (2021).\n[35] Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton,\nand Jure Leskovec. 2018. Graph Convolutional Neural Networks for Web-Scale\nRecommender Systems. In KDD.\n[36] Jiaxuan You, Jonathan M. Gomes-Selman, Rex Ying, and Jure Leskovec. 2021.\nIdentity-aware Graph Neural Networks. In AAAI.\n[37] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris\nAlberti, Santiago OntaÃ±Ã³n, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang,\nand Amr Ahmed. 2020. Big Bird: Transformers for Longer Sequences. In NIPS.\n[38] Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Vik-\ntor K. Prasanna. 2020. GraphSAINT: Graph Sampling Based Inductive Learning\nMethod. In ICLR.\n[39] Lingxiao Zhao and Leman Akoglu. 2019. Pairnorm: Tackling oversmoothing in\ngnns. arXiv preprint arXiv:1909.12223 (2019).\n[40] Jiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu, and Danai\nKoutra. 2020. Beyond Homophily in Graph Neural Networks: Current Limitations\nand Effective Designs. In NIPS.\n[41] Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. 2020.\nDeep Graph Contrastive Representation Learning. In ICML Workshop on Graph\nRepresentation Learning and Beyond . http://arxiv.org/abs/2006.04131",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7050878405570984
    },
    {
      "name": "Scalability",
      "score": 0.5858047604560852
    },
    {
      "name": "Transformer",
      "score": 0.5679822564125061
    },
    {
      "name": "Inference",
      "score": 0.5223892331123352
    },
    {
      "name": "Artificial intelligence",
      "score": 0.48135071992874146
    },
    {
      "name": "Graph",
      "score": 0.42790839076042175
    },
    {
      "name": "Machine learning",
      "score": 0.409859836101532
    },
    {
      "name": "Data mining",
      "score": 0.3410923480987549
    },
    {
      "name": "Theoretical computer science",
      "score": 0.3280690908432007
    },
    {
      "name": "Voltage",
      "score": 0.14895886182785034
    },
    {
      "name": "Engineering",
      "score": 0.11691397428512573
    },
    {
      "name": "Electrical engineering",
      "score": 0.0903857946395874
    },
    {
      "name": "Database",
      "score": 0.0893523097038269
    }
  ],
  "institutions": []
}