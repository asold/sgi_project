{
  "title": "Contextual Language Model Adaptation for Conversational Agents",
  "url": "https://openalex.org/W2810084418",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A4202076450",
      "name": "Raju, Anirudh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2744835781",
      "name": "Hedayatnia, Behnam",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4225991385",
      "name": "Liu, Linda",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221661550",
      "name": "Gandhe, Ankur",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4289124188",
      "name": "Khatri, Chandra",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4289075743",
      "name": "Metallinou, Angeliki",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4282320078",
      "name": "Venkatesh, Anu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202076452",
      "name": "Rastrow, Ariya",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2161466446",
    "https://openalex.org/W1665921526",
    "https://openalex.org/W2782940392",
    "https://openalex.org/W1561253126",
    "https://openalex.org/W2151315616",
    "https://openalex.org/W2403440562",
    "https://openalex.org/W2159382562",
    "https://openalex.org/W1999965501",
    "https://openalex.org/W1905522558",
    "https://openalex.org/W2118714763",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W56705713",
    "https://openalex.org/W2134237567",
    "https://openalex.org/W2046932483"
  ],
  "abstract": "Statistical language models (LM) play a key role in Automatic Speech\\nRecognition (ASR) systems used by conversational agents. These ASR systems\\nshould provide a high accuracy under a variety of speaking styles, domains,\\nvocabulary and argots. In this paper, we present a DNN-based method to adapt\\nthe LM to each user-agent interaction based on generalized contextual\\ninformation, by predicting an optimal, context-dependent set of LM\\ninterpolation weights. We show that this framework for contextual adaptation\\nprovides accuracy improvements under different possible mixture LM partitions\\nthat are relevant for both (1) Goal-oriented conversational agents where it's\\nnatural to partition the data by the requested application and for (2) Non-goal\\noriented conversational agents where the data can be partitioned using topic\\nlabels that come from predictions of a topic classifier. We obtain a relative\\nWER improvement of 3% with a 1-pass decoding strategy and 6% in a 2-pass\\ndecoding framework, over an unadapted model. We also show up to a 15% relative\\nimprovement in recognizing named entities which is of significant value for\\nconversational ASR systems.\\n",
  "full_text": "Contextual Language Model Adaptation for Conversational Agents\nAnirudh Raju1*, Behnam Hedayatnia1*, Linda Liu2, Ankur Gandhe1, Chandra Khatri1,\nAngeliki Metallinou1, Anu Venkatesh1, Ariya Rastrow1\n1Amazon Alexa Machine Learning\n2 University of Rochester\n*Both authors contributed equally to this work\n{ranirudh,behnam,aggandhe,ckhatri,ametalli,anuvenk,arastrow}@amazon.com\nAbstract\nStatistical language models (LM) play a key role in Automatic\nSpeech Recognition (ASR) systems used by conversational\nagents. These ASR systems should provide a high accuracy\nunder a variety of speaking styles, domains, vocabulary and ar-\ngots. In this paper, we present a DNN-based method to adapt the\nLM to each user-agent interaction based on generalized contex-\ntual information, by predicting an optimal, context-dependent\nset of LM interpolation weights. We show that this framework\nfor contextual adaptation provides accuracy improvements un-\nder different possible mixture LM partitions that are relevant for\nboth (1) Goal-oriented conversational agents where it’s natural\nto partition the data by the requested application and for (2)\nNon-goal oriented conversational agents where the data can be\npartitioned using topic labels that come from predictions of a\ntopic classiﬁer. We obtain a relative WER improvement of 3%\nwith a 1-pass decoding strategy and 6% in a 2-pass decoding\nframework, over an unadapted model. We also show up to a\n15% relative improvement in recognizing named entities which\nis of signiﬁcant value for conversational ASR systems.\nIndex Terms: speech recognition, language modeling, deep\nlearning, weighted ﬁnite state transducers\n1. Introduction\nAutomatic Speech Recognition (ASR) systems are a key com-\nponent in building conversational agents. The most common\napproach to building language models (LMs) for ASR systems\nis to learn n-gram models on large text corpora. These models\nare trained to predict the conditional word probabilities given\nthe context of the previous n−1 words. Hence, they do not\nmodel longer-range dependencies that may vary between differ-\nent subsets of the training data such as speaking style, vocabu-\nlary and topics of conversation [1]. In this paper, we explore the\nuse of contextual information for adapting the LM to each user-\nagent interaction. Broadly speaking, any additional information\nthat would help predict the user’s speech in the current interac-\ntion can be considered as contextual information, including the\nhistory of user-agent interactions, meta-data information such\nas dialog state and time of day, user personalized information\nlike music preferences.\nA common approach for LM adaptation is to represent the\nLM as a mixture of multiple component LMs, where the inter-\npolation weights can be adapted to a single global target do-\nmain or dynamically adapted based on contextual information.\nIn order to partition the training data and build the component\nLMs, a dominant approach has been to use supervised labels in\nthe training data. In [2–4], the dialog state was used to build\nmultiple component LMs and in [1, 5], supervised topic labels\nwere used to build topic-speciﬁc component LMs. Other ap-\nproaches which do not use supervised labels to partition the\ntraining data, rely on the availability of in-domain data to adapt\nLMs by selecting training data that minimizes cross-entropy [6],\nuse a combination of mixture-based and MAP-based models [7]\nor use constrained KL divergence between unigram distribu-\ntions [8]. Alternatively, clustered models are created either\nbased on latent semantic analysis [9], k-means clustering [10]\nor through topic clusters computed by Latent Dirichlet Alloca-\ntion (LDA) [11]. To estimate the context weights adaptively,\npast approaches used the expectation-maximization algorithm\nto learn the interpolation weights for a target sentence [10, 12].\nMore recently, using topic vectors directly in an RNN-LM was\nintroduced in [13].\nOur main contribution is to present a framework for adapt-\ning n-gram based LMs under some generalized contextual in-\nput. In this paper, we use a Deep Neural Network (DNN) to\nestimate the interpolation weights for a mixture of n-gram LMs,\nbut the approach can be extended to other adaptation techniques\nsuch as n-gram boosting [14]. Unlike the approach presented\nin [13], an n-gram based adaptation can be easily integrated\nwith an online system. For goal-oriented conversational interac-\ntions we partition our component LMs based on theapplication\nlabel, while for non-goal oriented, free-form dialogs we build\ncomponent LMs based on an estimated topic label. For both\nuse cases, our proposed framework outperforms an un-adapted\nframework, and leads to up to 6% relative WER reduction. Fur-\nthermore, our proposed adaptation leads to signiﬁcant reduc-\ntion, up to 15% relative WER, in recognizing topic-related en-\ntities of interest, such as person and location names, that appear\nin conversational interactions.\nThe paper is organized as follows. Section 2 describes the\ninterpolated LM and our strategies for partitioning the data into\ncomponent LMs, while Section 2.2 describes the on-the-ﬂy LM\ninterpolation. Sections 4 and 5 describe the experimental setup,\nresults and discussion. Finally, we conclude in Section 6.\n2. Interpolated Language Model\nThe interpolated language model is represented as a mixture of\nC component LMs, where the probability of a word sequence\nis calculated by using an interpolation weightλk corresponding\nto component LMk in the mixture:\nPmix(wi|hi) =\nC∑\nk=1\nλkPk(wi|hi) (1)\nsuch that ∑\nk=1 λk = 1\n2.1. Building component LMs\nBuilding the component LMs requires splitting the LM training\ndata into C components. The training data may be large tran-\nscribed corpora of user interactions or other external text cor-\narXiv:1806.10215v4  [cs.CL]  31 Jul 2018\npora. Data should be partitioned along a meaningful dimension\ne.g., each component representing a different speaking style,\nvocabulary, discourse topic or some other nuance of language.\nIn this work, we focus on ASR systems for two broad types of\nconversational agents, for which we use different data partition-\ning strategies.\nGoal-oriented conversational agents: These agents are\ndesigned to support a few speciﬁc applications such as asking\nfor weather information, playing music etc. Most interactions\nwith personal assistants fall into this category. These interac-\ntions are typically short and comprise of very few dialog turns\nwith the objective of providing the user with the requested infor-\nmation. Our goal-oriented interaction data is manually labeled\nin a straightforward manner based on the requestedapplication,\nfor example the sentence ‘Play a song by the Beatles’ would\nbe labeled as ‘Music’ application. This enables splitting the\ntraining data based on the application label, and training one\ncomponent LM for each application. We expect that user inter-\nactions with a music application would have different n-gram\nstatistics compared to interactions with a shopping application.\nChatbots Chatbots are non goal-oriented conversational\nagents where the objective is to engage the user in an interesting\nand coherent conversation, as opposed to completing a speciﬁc\ntask. Examples of older chatbots include ELIZA [15], while\nrecent systems include the conversational agents that were de-\nployed as part of the Alexa Prize competition [16]. Non goal-\noriented human-chatbot dialogs typically contain a variety of\nconversation topics, therefore we choose the topic label as a\nnatural way to partition the data for building the component\nLMs. Our data is not manually annotated with topic labels, so\nwe use an off-the-shelf topic classiﬁer designed for estimating\nconversational topics [17]. In future, this can be extended by\nunsupervised clustering of the training data.\n2.2. Building component LMs on the ﬂy\nIn weighted ﬁnite state transducers (WFST) based ASR sys-\ntems [18], n-gram models are represented as WFSTs where\neach state represents the n-gram history h and the weight on\nan out-going arc is either the word probability p(wi|h) or the\nback-off weight αh. For each input utterance, an interpolated\nn-gram LM needs to be built based on interpolation weights.\nThe proposed models for estimating the interpolation weights\nbased on context are described in Section 3. After the weights\nare computed dynamically for each input utterance, we use an\nefﬁcient on-the-ﬂy interpolated WFST strategy [19] where the\nn-gram probabilities from each component LM are kept sepa-\nrate and interpolated only at run-time.\n3. Estimation of interpolation weights\nGiven the interpolated LM representation as described in Equa-\ntion 1, we need to estimate the interpolation weights λk during\ninference for each utterance based on the contextual informa-\ntion available. We propose to use a DNN model that estimates\nthe λk’s as the output of a softmax function, given any generic\ncontextual feature input. The model can be trained in either a\nsupervised or semi-supervised manner, as described below.\nMinimize LM perplexity: The contextual adaptation\nmodel can be trained to estimate the interpolation weights λik\nsuch that it maximizes the log-likelihood of each utterance in\nthe training data under the interpolated model. This is equiva-\nlent to minimizing the LM perplexity of the training data. For\nan utterance ui = (w1,w2....wN ) of N words, the perplexity-\nbased loss function (PPL) is:\nLoss= −logp(w1,w2...wN ) (2)\n= −\n∑\nN\nlogp(wj|wj−1,...wj−n−1) (3)\n= −\n∑\nN\nlog\n∑\nK\nλkpk(wj|wj−1,...wj−n−1) (4)\nwhere pk is the probability of the n-gram from LMk. We\ncompute the derivative of our loss function w.r.t. to λk\nand back-propagate the error to estimate the DNN param-\neters. To efﬁciently compute the loss, we pre-calculate\npk(wj|wj−1,...wj−n−1) for all examples as it stays constant\nthroughout the optimization.\nThe training data can come from text corpus or from user-\nagent interactions. The user-agent interaction data can either\nbe ASR 1-best recognitions (resulting in a semi-supervised\ntraining) or manually transcribed data (supervised training).\nMinimize cross-entropy loss for component LM labels:\nThe contextual adaptation model is trained to directly predict\nthe intended component LM label for each utterance in the train-\ning data, using a cross-entropy (xent) loss function. The target\nlabels come from manual annotations of the requested applica-\ntion for goal-oriented conversational agents, and from topic la-\nbel estimates obtained from a topic classiﬁer [17] for chatbots.\n4. Experimental Setup\nWe build two separate ASR/LM systems, for goal-oriented and\nnon goal-oriented conversations respectively. Section 4.1 de-\nscribes the datasets used for training the LMs, while Section\n4.2 provides details on LM building and the context adaptation\nmodel for each of the two cases.\n4.1. Datasets\nWe have two datasets of user interactions with a conversational\nagent, speciﬁcally goal-oriented and non goal-orientedinterac-\ntions, which are used for LM training and for evaluation. For\nour experiments, both datasets are split into the following parti-\ntions - 80% train, 10% dev, and 10% test. Additionally, we also\nhave access to large external text corpora that are used only for\nLM training, and not as test sets.\nGoal-oriented interaction data: This dataset consists of\nmillions of utterances collected in far-ﬁeld conditions from\nreal user interactions with Alexa, a goal-oriented conversational\nagent. Each utterance corresponds to a single turn of user-agent\ninteraction that has been annotated with theapplication that was\nrequested and the corresponding text transcription. Application\nlabels include Music, Shopping, Weather and others.\nNon goal-oriented, chatbot interaction data:This dataset\nconsists of hundreds of thousands of far-ﬁeld speech-based\nuser-agent interactions with a chatbot. The goal of the chat-\nbot is to engage with the user in a conversation. The data is\nbucketed into conversations, where a single conversation is ini-\ntiated and terminated by the user, and consists of multiple turns\nof user-agent interactions.\nExternal text datasets We use a variety of external text\ncorpora, from multiple sources such as news, voice-mail, web\ncrawled corpora, etc. The total data size is of the order of bil-\nlions words and it is entirely in the train partition.\n4.2. ASR Systems\nFor all our experiments we use an experimental ASR system\nthat does not reﬂect the performance of the production Alexa\nTable 1: Description of LM setup in each ASR system. Data\nsources are described in Section 4.1\nLM details ASR System\nGoal-oriented Chatbot\nComponent LMs Application Topic\nNo. Comp. LMs 13 26\nTraining data goal-oriented chatbot and\ndata external data\nsystem. We build two ASR systems - one for goal-oriented\nagents and another for chatbots, each containing component\nLMs trained on different data, as described in Section 4.3.\nThe structure of the LMs in both systems is a mixture of Katz\nsmoothed [20] 4-gram language models which are interpolated\non-the-ﬂy in a WFST decoding framework, as described in\nSection 2.2. We also build baseline unadapted ASR systems\nfor each of these use cases, by estimating static interpolation\nweights to minimize perplexity of the corresponding dev set.\n4.3. Component LMs\nIn Table 1, for each ASR system, we describe the training data\nused to build the LMs, the number of component LMs, and what\nthey represent (application vs topic label). LM training is de-\nscribed in Section 2.1. The goal-oriented ASR system uses the\nannotated application label to partition the LM training data,\nwhile the chatbot LM uses the topic labels estimated from a\ntopic classiﬁer [17]. Since the topic labels are obtained from a\nclassiﬁer as opposed to manual annotations, this scales easily\nand allows us to use large external text corpora in addition to\nuser-agent interaction data, for training the chatbot LM. We use\nthe following strategy to mix data from multiple external data\nsources for the chatbot LM - for each of the component topic\nbased LMs, we build data source speciﬁc LMs which are stati-\ncally interpolated to minimize perplexity on the corresponding\ndev set, i.e., dev partitions of the non goal-oriented chatbot con-\nversations (Section 4.1)\n4.4. Contextual adaptation model\nThe contextual adaptation model for each ASR system is trained\nfrom their respective user-agent conversational datasets (goal-\noriented and non goal-oriented datasets of Section 4.1). We use\na few hundred thousand utterances for training the contextual\nmodels. As described in Section 3, we train the models to either\nminimize cross-entropy (xent) of the component LM label dis-\ntribution or perplexity (PPL) of the training data text. The DNN\nmodel shown in Figure 1 is a two layer network of 200 hidden\nunits which is trained using the Adam optimizer, clipping gradi-\nents and early stopping. The λk parameters are estimated from\nthe ﬁnal softmax layer.\n4.4.1. Contextual Features\nThe DNN-based contextual adaptation model allows for generic\ncontextual features. The ones that we experiment with are ob-\ntained either from (1) a context window of past user-agent in-\nteractions or (2) the metadata information of the current interac-\ntion, i.e. time of day. Note that, these features are available prior\nto each user-agent interaction. The estimates from the DNN are\nused to build an adapted LM for each utterance, using which we\nrun a 1-pass decoding with a real-time ASR system.\nIn addition, we report 2-pass decoding experiments where\nwe use another feature - (3) 1-best hypothesis from the current\ninteraction. The adapted LM estimated in this case is used to re-\ndecode the utterance. This is suitable for a non-real time system,\nFigure 1: Deep Neural Network (DNN) based contextual adap-\ntation model to predict interpolation weights\nand provides an upper bound on the possible WER reduction,\nsince the adapted LM is used as part of beam search decoding.\nNote that, in order to deploy this into a real-time system, we\ncan run the ﬁrst-pass decoding using the adapted LM from past\nutterance features and subsequently rescore the lattice using the\nmore powerful adapted LM which includes current utterance\nfeatures. This would result in a WER reduction in-between the\n1-pass and 2-pass results reported here.\nThe context window that we used for the past utterance fea-\ntures depends on the type of conversation. For goal-oriented\nconversational agents, we use all user-agent interactions within\nthe past T seconds of the current interaction. For non goal-\noriented conversational agents, we use all user-agent interac-\ntions within the current conversation.\nWe have three top level features: past interactions based\nfeatures (shorthand : prev), metadata based features (short-\nhand : meta) and current interaction based features (shorthand\n: cur). prev features include averaged pre-trained word embed-\ndings [21] of all previous user turns within the context window\nand averaged word embeddings of all previous agent responses\nwithin the context window. meta features include the day of\nweek and the time of day (morning, afternoon, evening). Lastly,\ncur features include the averaged word embeddings from 1-best\nASR recognition of the current utterance.\n5. Results and Discussion\nTable 2: Perplexity (PPL) and relative WER reduction (WERR)\nfor the goal-oriented ASR system on a goal-oriented test set, for\ndifferent objective functions and features for the DNN context\nmodel\nModel Feats PPL WERR(%) Entity\nWERR(%)\ndecoder : 1-pass\nNo Adapt - 30.66 - -\nDNN(Xent) prev, meta 29.43 +0.33% +1.17%\nDNN(PPL) prev, meta 26.10 -1.25% +0.21%\ndecoder : 2-pass\nDNN(Xent) prev, meta, cur 20.63 -3.49% -3.04%\nDNN(PPL) prev, meta, cur 20.30 -3.24% -2.21%\n5.1. Overall results using both 1-pass and 2-pass decoding\nTable 2 presents the perplexity and relative Word Error Rate\n(WER) reductions for the goal-oriented ASR system on a goal-\noriented test set. We observe a relative WER reduction of 1.25%\nrelative in a single-pass decoding framework and 3.5% relative\nin a 2-pass decoding framework by using the context adaptive\nLM. Similarly, in Table 3, we present the results of the chatbot\nASR system on a chatbot test set, where we see similar trends.\nSpeciﬁcally, we observe a 2.7% rel. WER reduction in single-\npass and a 6% rel. WER reduction in the 2-pass framework.\nThe improved results for the 1-pass system are promising be-\ncause they show that we can predict future behavior based on\nusage history and other metadata. As expected, the WER re-\nsults are better in a 2-pass framework because knowledge of the\ncurrent utterance helps us build a better adaptive LM. The cur-\nrent utterance features are the strongest signal to help improve\nWER and hence result in maximum improvements. However,\nthis comes at the cost of higher latency of the 2-pass system\ncompared to 1-pass.\n5.2. Impact of different loss functions\nFrom Tables 2 and 3 for both goal-oriented and chatbot ASR\nsystems, we observe that when we train the context adaptation\nmodel using the perplexity (PPL) objective instead of cross-\nentropy (xent), we achieve adapted LMs with better PPL and\nWER on the test set. Moreover, training with the PPL objective\nfunction has the advantage of not requiring explicit labels of the\noptimal component LM for each utterance, and can be extended\nto scenarios where we may wish to train with semi-supervised\ndata as described in Section 3.\nTable 3: Perplexity (PPL) and relative WER reduction (WERR)\nfor the chatbot ASR system on a chatbot test set, for different\nobjective functions and features for the DNN context model\nModel Feats PPL WERR(%) Entity\nWERR(%)\ndecoder : 1-pass\nNo Adapt - 60.77 - -\nDNN (Xent) prev, meta 59.81 -1.73% -8.19%\nDNN (PPL) prev, meta 58.14 -1.61% -2.98%\nDNN (PPL) prev-d, meta 55.66 -2.76% -10.92%\ndecoder: 2-pass\nDNN (PPL) prev, cur, meta 42.03 -5.58% -15.15%\nDNN (PPL) prev-d, cur, meta 42.83 -5.92% -14.67%\nDNN(PPL) cur, meta 42.72 -5.98% -15.32%\nTopic model cur 45.08 -5.52% -13.14%\n5.3. Impact of decaying past context\nAn interesting observation from our data is that users tend to\ninteract with a chatbot in multiple turns, while goal-based inter-\nactions are signiﬁcantly shorter. The typical context window for\nthe chatbot system contains several more user-agent turns than\nfor the goal-based system. Based on this insight, we further\nimproved our contextual model by slightly modifying the con-\ntextual features used in the lengthy chatbot interactions. Specif-\nically, we performed an exponentially decaying weighted aver-\nage over the past word-embeddings, to give higher weight to\nrecent utterances that are closer to the current utterance (called\nprev-d in Table 3). Using this exponential decay improves over\nusing the standard prev features in both PPL and WER. In Table\n3, using decaying context features ( prev-d) leads to an overall\nWER reduction of 2.76% using the context adapted LM in a\nsingle-pass decoding framework and 5.92% in a 2-pass decod-\ning framework.\n5.4. Comparison with topic classiﬁer based adaptation\nWe compare results when using the weights from the context\nadaptation model vs weights directly estimated from a topic\nclassiﬁer in a 2-pass decoding framework, see topic model vs\nDNN (PPL) in Table 3. For the topic model, the topic LM in-\nterpolation weights are the ﬁnal output topic probabilities es-\ntimated by the topic classiﬁer, which is the same classiﬁer we\nused to partition the data into component LMs ( [17], Section\n4.3). From Table 3, we observe that using this strategy, leads to\ncompetitive results as compared to the context adaptation model\nthat optimizes for PPL, i.e., leading to a 5.5% relative WER re-\nduction compared to the chatbot baseline.\n5.5. Impact on Named Entity accuracy\nWhile WER is a standard metric to measure the performance\nof ASR systems, it does not capture the relative importance\nof different words in an utterance. For conversational systems,\nnamed entities such as people names, locations etc, are arguably\nmore important compared to other words due to their impact\non downstream tasks such as Natural Language Understanding\n(NLU). These entities tend to be topic speciﬁc, e.g., conversa-\ntions about music contain entities such as artist and song names.\nHere, we analyze the accuracy of our proposed contextual ASR\nsystem on named entities. To measure entity error rate, we\ntag each word in our test data using an in-house Named Entity\nRecognition (NER) tagger. The entity error rate is deﬁned as\n(num substitutions+num deletions)/num reference−\nwords. Note that we do not include insertions due to difﬁculty\nin attributing whether an insertion error was caused by the entity\nor the other surrounding words.\nIn the goal-oriented system, we bias towards the next appli-\ncation that the user is likely to request such as Music or Weather.\nHence, in Table 2, we see similar improvements in the carrier\nphrase recognition as well as the entities i.e. the overall WER\nreduction is similar to the entity WER reduction. In contrast,\nthe non-goal oriented data is more challenging due to it’s free-\nform conversational nature, and contains topic speciﬁc named\nentities. We see a larger reduction of 15.32% entity error rate\nhere in Table 3 with the best adapted LM. For example, dif-\nﬁcult entities like ”czechoslovakia” and ”abigail” are correctly\nrecognized by the adapted LM. This is a promising result for\nconversational ASR systems, where successful recognition of\nnamed entities is critical for completing the user request or en-\ngaging the user in meaningful dialog about topics of interest.\n6. Conclusions and Future Work\nWe described methods for improving the performance of a\nmixture of n-gram LMs with on-the-ﬂy interpolation, using a\ncontextual adaptation framework. We presented a DNN-based\nmethod to predict an optimal set of interpolation weights for\neach interaction, in an online fashion, from generalized con-\ntextual information. This model was evaluated for both goal-\noriented conversational agents where we partitioned the data by\nthe requested application and on non-goal oriented conversa-\ntional agents where we partitioned the data using topic labels\nthat come from predictions of a topic classiﬁer. We achieved a\nrelative WER reduction up to 3% in a 1-pass decoding strategy\nusing past context only, and up to 6% relative in a 2-pass de-\ncoding strategy, over a statically interpolated baseline LM. Our\nmethod beneﬁts named entities the most: we reduced entity er-\nror rate by up to 15.32% relative. Future work includes evaluat-\ning this contextual LM adaptation strategy using other relevant\ncontext, such as topics derived from unsupervised clustering of\nthe conversation, multimodal information that is available to the\nconversational agent or information about current events.\n7. References\n[1] R. Kneser and V . Steinbiss, “On the dynamic adaptation of\nstochastic language models,” in 1993 IEEE International Confer-\nence on Acoustics, Speech, and Signal Processing, vol. 2, 1993,\npp. 586–589.\n[2] F. Wessel, A. Baader, and H. Ney, “A comparison of dialogue-\nstate dependent language models,” in Proc. ESCA Workshop on\nInteractive Dialogue in Multi-Modal Systems, Irsee, Germany,\nJun. 1999, pp. 93–96.\n[3] W. Xu and A. I. Rudnicky, “Language modeling for dialog sys-\ntem,” inINTERSPEECH, 2000, pp. 118–121.\n[4] K. Visweswariah and H. Printz, “Language models conditioned\non dialog state.” inINTERSPEECH, 2001, pp. 251–254.\n[5] J. Gao, H. Suzuki, and W. Yuan, “A comparative\nstudy on language model adaptation using new evalua-\ntion metrics,” vol. 5/3. Association for Computational\nLinguistics, April 2006, pp. 207–227. [Online]. Avail-\nable: https://www.microsoft.com/en-us/research/publication/\na-comparative-study-on-language-model-adaptation-using-new-evaluation-metrics/\n[6] A. Axelrod, X. He, and J. Gao, “Domain adaptation via pseudo\nin-domain data selection,” inProceedings of the 2011 Conference\non Empirical Methods in Natural Language Processing, 2011, pp.\n355–362.\n[7] L. Chen, J.-L. Gauvain, L. F. Lamel, G. Adda, and M. Adda, “Lan-\nguage model adaptation for broadcast news transcription,” 2001.\n[8] R. Kneser, J. Peters, and D. Klakow, “Language model adaptation\nusing dynamic marginals.” inEUROSPEECH, 1997.\n[9] J. R. Bellegarda, “Exploiting latent semantic information in statis-\ntical language modeling,”Proceedings of the IEEE, vol. 88, no. 8,\npp. 1279–1296, 2000.\n[10] P. Clarkson and A. J. Robinson, “Language model adaptation us-\ning mixtures and an exponentially decaying cache,” in1997 IEEE\nInternational Conference on Acoustics, Speech, and Signal Pro-\ncessing, vol. 2, 1997, pp. 799–802.\n[11] Y .-C. Tam and T. Schultz, “Dynamic language model adapta-\ntion using variational bayes inference,” inINTERSPEECH, vol. 0,\nno. 0, 2005, pp. 5–8.\n[12] K. Thadani, F. Biadsy, and D. M. Bikel, “On-the-ﬂy topic adap-\ntation for youtube video transcription,” in INTERSPEECH, 2012,\npp. 210–213.\n[13] T. Mikolov and G. Zweig, “Context dependent recurrent neural\nnetwork language model,” in 2012 IEEE Spoken Language Tech-\nnology Workshop (SLT), 2012.\n[14] P. Aleksic, M. Ghodsi, A. Michaely, C. Allauzen, K. Hall,\nB. Roark, D. Rybach, and P. Moreno, “Bringing contextual infor-\nmation to google speech recognition,” in Sixteenth Annual Con-\nference of the International Speech Communication Association,\n2015.\n[15] J. Weizenbaum, “Eliza a computer program for the study of nat-\nural language communication between man and machine,” Com-\nmunications of The ACM, vol. 9, no. 1, pp. 36–45, 1966.\n[16] The Alexa Prize Competition,\n“https://developer.amazon.com/alexaprize.”\n[17] F. Guo, A. Metallinou, C. Khatri, A. Raju, A. Venkatesh, and\nA. Ram, “Topic-based evaluation for conversational bots,” NIPS\nWorkshop on Conversational AI, 2017.\n[18] M. Mohri, F. Pereira, and M. Riley, “Weighted ﬁnite-state trans-\nducers in speech recognition,” Computer Speech and Language,\nvol. 16, no. 1, pp. 69–88, 1 2002.\n[19] B. Ballinger, C. Allauzen, A. Gruenstein, and J. Schalkwyk, “On-\ndemand language model interpolation for mobile speech input,”\nin Interspeech, 2010, pp. 1812–1815.\n[20] S. M. Katz, “Estimation of probabilities from sparse data for\nthe language model component of a speech recognizer,” in IEEE\nTransactions on Acoustics, Speech and Singal processing, vol.\nASSP-35, no. 3, March 1987, pp. 400–401.\n[21] J. Pennington, R. Socher, and C. D. Manning, “Glove: Global\nvectors for word representation,” inEmpirical Methods in Natural\nLanguage Processing (EMNLP), 2014, pp. 1532–1543. [Online].\nAvailable: http://www.aclweb.org/anthology/D14-1162",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.845156192779541
    },
    {
      "name": "Vocabulary",
      "score": 0.6813639402389526
    },
    {
      "name": "Decoding methods",
      "score": 0.6099732518196106
    },
    {
      "name": "Language model",
      "score": 0.5915156602859497
    },
    {
      "name": "Classifier (UML)",
      "score": 0.5676096677780151
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5282472968101501
    },
    {
      "name": "Adaptation (eye)",
      "score": 0.47149747610092163
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.4448772966861725
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.4428774416446686
    },
    {
      "name": "Natural language processing",
      "score": 0.43709948658943176
    },
    {
      "name": "Context (archaeology)",
      "score": 0.4266631007194519
    },
    {
      "name": "Natural language",
      "score": 0.4178153872489929
    },
    {
      "name": "Speech recognition",
      "score": 0.40835118293762207
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ]
}