{
  "title": "Performance of the pre-trained large language model GPT-4 on automated short answer grading",
  "url": "https://openalex.org/W4400350333",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2058134701",
      "name": "Gerd Kortemeyer",
      "affiliations": [
        "ETH Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A2058134701",
      "name": "Gerd Kortemeyer",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6768879176",
    "https://openalex.org/W3209912268",
    "https://openalex.org/W4366783381",
    "https://openalex.org/W4386405611",
    "https://openalex.org/W1967082761",
    "https://openalex.org/W6631656442",
    "https://openalex.org/W2115584598",
    "https://openalex.org/W2061663860",
    "https://openalex.org/W2130273049",
    "https://openalex.org/W1982891870",
    "https://openalex.org/W2020508893",
    "https://openalex.org/W2164485268",
    "https://openalex.org/W2084450415",
    "https://openalex.org/W6996724243",
    "https://openalex.org/W2080367067",
    "https://openalex.org/W4389132588",
    "https://openalex.org/W4379746261",
    "https://openalex.org/W4291034789",
    "https://openalex.org/W4323670074",
    "https://openalex.org/W2966193711",
    "https://openalex.org/W43048615",
    "https://openalex.org/W4285189487",
    "https://openalex.org/W4377989516",
    "https://openalex.org/W6702248584",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W4206237192",
    "https://openalex.org/W2474948357",
    "https://openalex.org/W2809666916",
    "https://openalex.org/W3111899316",
    "https://openalex.org/W3214533270",
    "https://openalex.org/W4288059378",
    "https://openalex.org/W4377855098",
    "https://openalex.org/W1973517668",
    "https://openalex.org/W3099362249"
  ],
  "abstract": "Abstract Automated Short Answer Grading (ASAG) has been an active area of machine-learning research for over a decade. It promises to let educators grade and give feedback on free-form responses in large-enrollment courses in spite of limited availability of human graders. Over the years, carefully trained models have achieved increasingly higher levels of performance. More recently, pre-trained Large Language Models (LLMs) emerged as a commodity, and an intriguing question is how a general-purpose tool without additional training compares to specialized models. We studied the performance of GPT-4 on the standard benchmark 2-way and 3-way datasets SciEntsBank and Beetle, where in addition to the standard task of grading the alignment of the student answer with a reference answer, we also investigated withholding the reference answer. We found that overall, the performance of the pre-trained general-purpose GPT-4 LLM is comparable to hand-engineered models, but worse than pre-trained LLMs that had specialized training.",
  "full_text": "Vol.:(0123456789)\n Discover Artificial Intelligence            (2024) 4:47  | https://doi.org/10.1007/s44163-024-00147-y\nDiscover Artificial Intelligence\nBrief Communication\nPerformance of the pre‑trained large language model GPT‑4 \non automated short answer grading\nGerd Kortemeyer1\nReceived: 9 December 2023 / Accepted: 25 June 2024\n© The Author(s) 2024  OPEN\nAbstract\nAutomated Short Answer Grading (ASAG) has been an active area of machine-learning research for over a decade. It \npromises to let educators grade and give feedback on free-form responses in large-enrollment courses in spite of lim-\nited availability of human graders. Over the years, carefully trained models have achieved increasingly higher levels of \nperformance. More recently, pre-trained Large Language Models (LLMs) emerged as a commodity, and an intriguing \nquestion is how a general-purpose tool without additional training compares to specialized models. We studied the \nperformance of GPT-4 on the standard benchmark 2-way and 3-way datasets SciEntsBank and Beetle, where in addition \nto the standard task of grading the alignment of the student answer with a reference answer, we also investigated with-\nholding the reference answer. We found that overall, the performance of the pre-trained general-purpose GPT-4 LLM is \ncomparable to hand-engineered models, but worse than pre-trained LLMs that had specialized training.\nKeywords Automated short answer grading · Large language model · SciEntsBank · Beetle · GPT\n1 Introduction\nProviding meaningful feedback to learners is one of the most important tasks of instructors [1 ], yet it can also become \none of the most work-intensive or even tedious tasks. Particularly for large-enrollment courses, lack of grading personnel \ncan limit this feedback to automatically gradable closed-answer formats such as multiple-choice or numerical inputs. \nThis limitation might be overcome by using Artificial Intelligence (AI) solutions [2 ]; it is therefore not surprising that \nwhen it comes to the use of AI in higher education, assessment and evaluation are the most prominent topics [3 ], and \nacceptance of this technology for education is increasing based on its perceived usefulness [4]. In particular, studies on \nAutomated Short Answer Grading (ASAG) [5 , 6] are highly relevant for educators to extend the limits of what can be \nassessed at large scales.\nIt is impossible to do justice to the spectrum of sophisticated ASAG methods in this short study; Burrows, Gurevych, \nand Stein provide an excellent overview up to 2015 [5]; Haller, Aldea, Seifert, and Strisciuglio look at later developments \nup to 2022 [6 ]. The latter survey notes a particular shift in recent years as models are moving from hand-engineered \nfeatures to representation-learning approaches, which draw their initial training data from large text corpora [6 ] (“pre-\ntrained”). However, most models used for ASAG still have in common that they are explicitly trained or fine-tuned for \nparticular grading tasks, and datasets used in competitions such as SemEval [7] thus include training and testing items. \nBy contrast, recently publicly released Large Language Models (LLMs) such as GPT-4 [8] and Bard [9] have not only been \npre-trained from large text corpora, but subsequently extensively fine-tuned following general instead of task-specific \n * Gerd Kortemeyer, kgerd@ethz.ch | 1Rectorate and AI Center, ETH Zurich, 8092 Zurich, Switzerland.\nVol:.(1234567890)\nBrief Communication Discover Artificial Intelligence            (2024) 4:47  | https://doi.org/10.1007/s44163-024-00147-y\nstrategies. Their users are neither expected nor actually able to further train or fine-tune the model, and an intriguing \nquestion is how these out-of-the-box general-purpose tools perform compared to specially trained or fine-tuned models.\nIn this study, GPT-4 is prompted to grade the items from two standard datasets, SciEntSBank and BEEtlE [7], which allows \ncomparison of precision, recall, and F1-score (or weighted F1-score in case of 3-way items) to legacy and state-of-the-art \nASAG models. SciEntSBank covers general science questions for 3rd to 6th grade, while BEEtlE covers questions and student \nanswers from a tutorial system for basic electricity and electronics.\nThe standard judgment method is to compare the student answer to a reference answer, but in addition, it was also \ninvestigated if GPT-4 can adequately grade the student answers based on the question alone. For the latter task, the \nmodel would need to draw on its own pre-training from its text corpus to independently judge the correctness of the \nstudent answer.\nIn summary, this study\n• compares the performance of an out-of-the-box Large Language Model in ASAG of standardized problem sets to that \nof hand-engineered AI systems and previous-generation Large Language Models that have been specifically trained \nor fine-tuned for the task, and it does so\n• with and without providing the reference answer to the out-of-the-box model, in the latter case completely relying \non the knowledge of the pre-trained model.\nThe motivation of this study is not to prove that general-purpose, out-of-the box models perform better than state-of-\nthe-art specialized systems (and as it will turn out, they do not), but to\n• provide a benchmark data point of how these systems compare to previous and current research systems, and\n• explore the possibility of deploying these systems as low-threshold tools for instructors to provide meaningful ASAG \nfor learners.\n2  Related work\nThe idea of using machines for grading was arguably first introduced by Sidney Pressey in the late 1920 s with the “Auto-\nmatic Teacher” [10], which was able to pose automatically graded multiple-choice questions. In the 1960th, computers \ntook the place of such mechanical devices, connecting to mainframes using Teletypes [11], which allowed for numerical \nanswers; these integer answers were checked for equality to the programmed correct solution. With the advent of the \nweb also came more sophisticated systems that could grade ranking or mix-and-match problems, as well as numerical \nsolutions with tolerances and physical units, algebraic answers based on symbolic mathematical equivalence, and string \nresponses based on exact matching or regular expressions [12, 13]. The latter responses are usually limited to individual \nwords or short phrases in the context of cloze (“fill-in-the-blank”) questions [14, 15]. What is common to these approaches \nis that they are based on deterministic algorithms; answers that are basically closed-ended, so there is a limited set of \ncorrect answers or hard criteria to judge a responses as correct or incorrect [16]. The system then grades with perfect \naccuracy, making it fit for use in high-stakes summative assessment; the assessment of the desired learning outcomes \nmight still not be reliable in terms of psychometrics [17–19], but the responses themselves will be judged reliably accord-\ning to the provided criteria.\nAt the opposite end of the spectrum are completely open-ended questions, which allow for free expression of ideas, \nconcepts, and solutions approaches without necessarily imposing a particular structure [16]. Typical responses are essays \nor extended solution derivations. Experimentally, these are increasingly graded by artificial intelligence [20, 21], however, \nhere the accuracy is much lower, resulting in limited reliability and thus limited trust [22, 23]. Current results indicate \nthat artificial intelligence is capable of providing meaningful formative feedback to completely open-ended assessment \nresponses, but might not be ready yet for high-stakes summative assessment [20].\nASAG offers a middle road between closed-ended and completely open-ended responses, which is why they are \nsometimes referred to as “semi-open-ended” questions [24]. These questions generally expect one to three free-form \nsentences that directly answer a specific question; as opposed to cluze, scoring systems need to recognize paraphrasing \nand equivalent meaning [25] (which generally requires machine learning approaches), but as opposed to essays or math-\nematical derivations, they do not need to deal with a wide-open answer space and the complications of following multi-\nstep arguments or judging matters of style. As machine-learning based systems, to achieve high accuracy, ASAG engines \nVol.:(0123456789)\nDiscover Artificial Intelligence            (2024) 4:47  | https://doi.org/10.1007/s44163-024-00147-y \nBrief Communication\ngenerally need a phase of subject-matter specific training or fine-tuning before being used to judge student work [26]; \nfor transformer or transformer-ensemble approaches, a phase of generating various reference answers improves their \nperformance [27]. Here, we explore the performance of a single general-purpose engine being used out-of-the-box \nwithout additional preparation, and we do so with and without providing a reference answer.\n3  Methodology\nThe SciEntSBank and BEEtlE datasets [7 ] were downloaded from kaggle [28]. They included both training and test data. \nThe training data were discarded, while the test data included the 504 items and 14, 186 student answers and their ref-\nerence grading that were used for this study. As no training took place, the distinction between unseen answers (UA), \nunseen questions (UQ), and unseen domains (UD) that the dataset provided was dropped for this study, since all items \nwere “unseen. ”\nEach item in the datasets contains a question, a reference answer, and student answers including their reference \ngrade. The items came in two versions:\n• a 2-way version, where each student answer is either correct if it is complete and correct paraphrase of the reference \nanswer or incorrect otherwise, and\n• a 3-way version, where an additional judgment of contradictory replaces some of the incorrect labels if the student \nanswer explicitly contradicts the reference answer.\nUsing a Python script, the XML-coded items were translated into prompts for the GPT-4 API, see Fig. 1 for an overview of \nthe whole process and Fig. 2 for an example. Each item was graded with and without providing a reference answer. The \ndefinitions of the judgment criteria for grading were taken from SemEval-2013 [7 ].\nThe API was provided by Azure AI Services [ 29]. For each item, the role and prompt (Fig.  2) were sent via a Python \nscript to be processed by GPT-4 as shown in Fig.  3. Also shown in Fig. 3 is an example of the output, which the Python \nscript wrote to disk as a CSV-file for further analysis.\nFor 6 of the 504 items, errors occurred during evaluation, which led to 58 of the 28, 372 student statements receiving \nno or invalid grades (that is, missing or invalid entries in the output table shown in Fig. 3). The invalid grades were unclear, \ncreative, epoch, accurate, and correc [sic]. These missing or invalid student grades were counted as neither positives nor \nnegatives.\nSubsequently, the precision, recall, and (weighted) F1-score were calculated: \nPrecision  Out of all the correct grades given by a model, how many were actually correct?\nRecall  (or Sensitivity) Out of all the actual correct student answers, how many were graded as correct?\nFig. 1  Overview of the grading process\nVol:.(1234567890)\nBrief Communication Discover Artificial Intelligence            (2024) 4:47  | https://doi.org/10.1007/s44163-024-00147-y\nFig. 2  Original XML-code of a 3-way item and the generated prompts for its evaluation with and without providing a reference answer\nFig. 3  Format of sending \nthe role and prompt in Fig. 2 \nto be processed by GPT-4 \n(“myGPT4” is the name of the \ndeployment of the model \nused), as well as example \noutput from GPT-4\nVol.:(0123456789)\nDiscover Artificial Intelligence            (2024) 4:47  | https://doi.org/10.1007/s44163-024-00147-y \nBrief Communication\nF1-score  Harmonic mean of precision and recall; a way to balance the trade-off between precision and recall.\n In the 3-way scenario, the above characteristics are correspondingly calculated for the classes contradictory and \nincorrect, and a weighted average is calculated for these class F1-scores to form the weighted F1-score (w-F1).\n4  Results\n4.1  Precision, recall, and F1‑scores\nTable 1 shows the precision, recall, and F1-scores for SciEntSBank and BEEtlE for the 2-way and 3-way items, as well as \nfor the scenario where the reference answer was withheld. For the 3-way scenario, the individual-class results and \nthe weighted F1-score (w-F1) are provided.\nLooking at the precision and recall, an outlier is the recall on contradictory in the 3-way BEEtlE dataset: a large \nnumber of student answers that were labelled as contradictory were not recognized as such, but simply as incorrect  \n(as evidenced by the low precision on incorrect).\nGPT-4 generally performs better on SciEntSBank than on BEEtlE. For SciEntSBank, the model showed its highest per -\nformance on the 2-way task (F1=0.744), followed closely by the no-reference scenario (F1=0.731), with the 3-way \nscenario in last place (w-F1=0.729). For BEEtlE, the no-reference scenario showed the highest performance (F1=0.651), \nfollowed by the 2-way (F1=0.611) and 3-way (w-F1=0.516) scenarios. In other words, for BEEtlE, providing a reference \nanswer lowered its performance on correctly judging the student answers.\n4.2  Comparison to specialized ASAG models\nTable 2 shows a comparison of specifically trained models versus the out-of-the-box GPT-4 model. At the time of the \nSemEval-2013 competition [7 ], had GPT-4 been around, it would have won the competition for 3-way SciEntSBank, and \nit would have outperformed all but one competing models in the unseen questions (UQ) category. In these specifi-\ncally trained models, performance strongly depends on what was “seen” and what was “unseen. ”\nNewer systems perform better, in particular those of the BERT [30] LLM family. These models are pre-trained and \nthen specifically trained for SciEntSBank and BEEtlE using for example PyTorch [31]. Unfortunately, for the highly suc -\ncessful roberta-large model [32], the performance was not separately reported for the different ‘unseen” categories, \nand no 3-way grading was performed.\nOverall, the performance of the pre-trained general-purpose GPT-4 LLM is comparable to hand-engineered models, \nbut worse than pre-trained LLMs that had specialized training.\nTable 1  Results for precision, recall, and F1-scores for SciEntSBank and BEEtlE in the 2-way, 3-way, and no-reference-answer scenarios\nFor the 3-way scenario, the individual-class results and the weighted F1-score (w-F1; also referred to as micro-averaged F1-score) are pro -\nvided\n2-way 3-way No reference answer\ncorrect contradictory incorrect\nPrec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1 w-F1 Prec. Rec. F1\nSciEntS-\n Bank 0.788 0.705 0.744 0.717 0.825 0.767 0.696 0.581 0.633 0.754 0.679 0.715 0.729 0.697 0.768 0.731\n BEEtlE 0.657 0.572 0.611 0.635 0.672 0.653 0.680 0.199 0.308 0.426 0.672 0.522 0.516 0.581 0.739 0.651\nVol:.(1234567890)\nBrief Communication Discover Artificial Intelligence            (2024) 4:47  | https://doi.org/10.1007/s44163-024-00147-y\n5  Limitations\nSince GPT is a probabilistic model, running it again, possibly at a different temperature, is likely going to yield dif -\nferent results. However, due to the already large amount of computing required for one run, and in light of the high \nstatistics gained from over 500 items, only one run was considered here. Also, different prompts from the ones shown \nin Fig. 2 may result in higher or lower performance.\nOpenAI, the company behind GPT, does not release information about what constituted the text corpus used for \ntraining. Though unlikely, since the datasets are only available as ZIP-files and in XML-format, there is still a possibility \nthat SciEntSBank and BEEtlE had been used for training. When asked about SciEntSBank, though, the model stated that \nit was not familiar with a dataset or source by that name; GPT-4 performed better on SciEntSBank than on BEEtlE, for \nwhich it stated that it is a known dataset in the domain of natural language processing and educational research. \nThe model, however, demonstrated ignorance when asked about any specific details regarding Johnny, his father, \nand the windows in the scenario quoted in Fig.  2, making it unlikely that it had seen the text before.\n6  Discussion\nThe last five years saw the strong emergence of Deep-Learning-based models for ASAG. These models generally \nexhibit higher performance than hand-engineered models, but still strongly depend on training, which may be \npre-training or task-specific. LLMs usually come pre-trained, but the extend of that pre-training greatly varies: while \ndetails on GPT-4’s text corpus are proprietary, it can be assumed that it was trained and fine-tuned with orders of \nmagnitude more data than for example BERT [30]. However, as this study shows, the difference in pre-training can \nbe more than made up by the BERT-family’s openness to additional task-specific training by the user.\nAt least for the grade-school educational content covered by the datasets in this study, GPT-4 performs ASAG at \na performance level comparable to hand-engineered systems from five years ago. It does so even without the need \nfor providing reference answers. There are strong indications that this ability would extend to higher education, for \nexample university-level physics content [38], and that automated grading of open-ended assessment content is \npossible beyond short answers [ 20]. In addition, a general-purpose LLM can give more tailored feedback than simple \ncorrect/ incorrect judgments, which has high potential for learning from short answer grading [39].\nThe low-overhead nature of this out-of-the-box approach opens up the possibility of integrating ASAG into com-\nmodity learning management systems (LMSs) as a component of their quizzing engines, particularly for formative \nassessment. Instructors could write these assessment items with little to no technical support and immediately deploy \nthem. A problem with general-purpose tools like GPT-4 [8 ] and Bard [9 ] is that they are running in the cloud. When it \nTable 2  Comparison of (weighted) F1-scores for different ASAG systems and GPT-4\nanot stated if macro-averaged F1 or weighted (micro-averaged) F1 was reported\nbthe model was specifically trained, but no separate information on UA, UQ, and UD was provided\nSciEntSBank BEEtlE\n2-way 3-way No Ref 2-way 3-way No Ref\nModel Year UA UQ UD UA UQ UD UA UQ UA UQ\nCoMeT [7] 2013 0.77 0.58 0.67 0.71 0.52 0.55 0.83 0.70 0.73 0.49\nSoftCardinality [7] 2013 0.72 0.74 0.71 0.65 0.63 0.62 0.77 0.64 0.62 0.45\nSultan et al. [33, 34] 2016 0.69 0.70 0.71 0.57 0.62 0.60\nSaha et al. [34] 2018 0.79 0.70 0.72 0.71 0.64 0.61\nGCN-DA [35] 2020 0.73 063\nSFRN+ [36] 2021 0.78 0.64 0.67 0.65a 0.49a 0.47a 0.89 0.70 0.67a 0.55a\nBERT [37] 2022 0.73 0.60 0.62 0.71 0.57\nroberta-large [32] 2021 0.81b 0.91b\nGPT-4 2023 0.74 0.73 0.73 0.61 0.52 0.65\nVol.:(0123456789)\nDiscover Artificial Intelligence            (2024) 4:47  | https://doi.org/10.1007/s44163-024-00147-y \nBrief Communication\ncomes to grade-relevant student data, the question of data security and privacy cannot be ignored, which may limit \nthe applicability of this approach to ASAG unless additional contractual agreements are in place. An alternative for \na model that might also not need additional training, but which could be locally installed, is Llama 2 [40], However, \npreliminary studies by the author indicate that Llama 2 does generally not perform as well as GPT-4.\n7  Conclusion\nThe performance of the general-purpose Large Language Model GPT-4 on Automated Short Answer Grading does not \nreach that of specifically trained Deep-Learning models, but it is comparable to that of earlier hand-engineered ASAG \nmodels. A clear advantage of GPT-4 is that it does not need to be specifically trained for the task and can be used out-\nof-the-box, which has the potential to turn it into a commodity for educators as part of learning management systems. \nIn addition to not needing additional training, GPT-4 can also perform ASAG without the need for providing reference \nanswers, at least at the grade-school level covered by the datasets used in this study and likely at the introductory \nhigher-education level.\nAcknowledgements The author would like to thank Julia Chatain for her help in connecting to the GPT API.\nAuthor contributions The sole author of this study carried out the work himself.\nFunding Open access funding provided by Swiss Federal Institute of Technology Zurich.\nData availability The benchmark datasets SciEntSBank and BEEtlE [7] are available from kaggle [28]. Code and calculated data are made avail-\nable as supplemental material alongside this paper from https:// www. polyb ox. ethz. ch/ index. php/s/ mByv0 od7us cm3VV (the file readme.txt \nin the downloadable package explains the code and data files).\nDeclarations \nCompeting interests There are no conflicting or Competing interests.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adapta-\ntion, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, \nprovide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article \nare included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in \nthe article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\nReferences\n 1. Bransford JD, Brown AL, Cocking RR, et al. How people learn. Washington, DC: National academy press; 2000.\n 2. Seo K, Tang J, Roll I, Fels S, Yoon D. The impact of artificial intelligence on learner-instructor interaction in online learning. Int J Educ \nTechnol Higher Educ. 2021;18(1):1–23.\n 3. Crompton H, Burke D. Artificial intelligence in higher education: the state of the field. Int J Educ Technol Higher Educ. 2023;20(1):1–22.\n 4. Zhang C, Schießl J, Plößl L, Hofmann F, Gläser-Zikuda M. Acceptance of artificial intelligence among pre-service teachers: a multigroup \nanalysis. Int J Educ Technol Higher Educ. 2023;20(1):49.\n 5. Burrows S, Gurevych I, Stein B. The eras and trends of automatic short answer grading. Int J Artif Intell Educ. 2015;25:60–117.\n 6. Haller S, Aldea A, Seifert C, Strisciuglio N. Survey on automated short answer grading with deep learning: from word embeddings to \ntransformers. arXiv preprint arXiv: 2204. 03503, 2022.\n 7. Dzikovska MO, Nielsen R, Brew C, Leacock C, Giampiccolo D, Bentivogli L, Clark P , Dagan I, Dang HT. Semeval-2013 task 7: The joint student \nresponse analysis and 8th recognizing textual entailment challenge. In Second Joint Conference on Lexical and Computational Semantics \n(* SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 263–274, 2013.\n 8. OpenAI. GPT-4. https://openai.com/gpt-4.\n 9. Google. Bard. https://bard.google.com/.\n 10. Petrina S. Sidney pressey and the automation of education, 1924–1934. Technol Cult. 2004;45(2):305–30.\n 11. Suppes P , Jerman M, Groen G. Arithmetic drills and review on a computer-based teletype. Arith Teach. 1966;13(4):303–9.\n 12. Sangwin CJ. Assessing elementary algebra with stack. Int J Math Educ Sci Technol. 2007;38(8):987–1002.\n 13. Kortemeyer G, Kashy E, Benenson W, Bauer W. Experiences using the open-source learning content management and assessment system \nlon-capa in introductory physics courses. Am J Phys. 2008;76(4):438–44.\n 14. Jonz J. Another turn in the conversation: what does cloze measure? Tesol Quarterly. 1990;24(1):61–83.\nVol:.(1234567890)\nBrief Communication Discover Artificial Intelligence            (2024) 4:47  | https://doi.org/10.1007/s44163-024-00147-y\n 15. Chapelle CA, Abraham RG. Cloze method: what difference does it make. Lang Testing. 1990;7(2):121–46.\n 16. R Pate. Open versus closed questions: what constitutes a good question. Educational research and innovations, pages 29–39, 2012.\n 17. Lord FM, Novick MR. Statistical theories of mental test scores. Information Age Publishing, 2008.\n 18. James Dean Brown. My twenty-five years of cloze testing research: so what. Int J Lang Stud. 2013;7(1):1–32.\n 19. Kortemeyer G. Extending item response theory to online homework. Phys Rev Special Topics-Phys Educ Res. 2014;10(1): 010118.\n 20. Kortemeyer G. Toward ai grading of student problem solutions in introductory physics: a feasibility study. Phys Rev Phys Educ Res. \n2023;19(2): 020163.\n 21. Jamil F, Hameed IA. Toward intelligent open-ended questions evaluation based on predictive optimization. Expert Syst Appl. 2023;231: \n120640.\n 22. Jackson Stephen, Panteli Niki. Trust or mistrust in algorithmic grading? an embedded agency perspective. Int J Inf Manag. 2023;69: 102555.\n 23. Conijn R, Kahr P , Snijders CC. The effects of explanations in automated essay scoring systems on student trust and motivation. J Learn \nAnal. 2023;10(1):37–53.\n 24. Zhang Lishan, Huang Yuwei, Yang Xi, Shengquan Yu, Zhuang Fuzhen. An automatic short-answer grading model for semi-open-ended \nquestions. Int Learn Environ. 2022;30(1):177–90.\n 25. Leacock Claudia, Chodorow Martin. C-rater: automated scoring of short-answer questions. Comput Hum. 2003;37:389–405.\n 26. Ahmed A, Joorabchi A, Hayes MJ. On deep learning approaches to automated assessment: strategies for short answer grading. CSEDU \n(2), pages 85–94, 2022.\n 27. Akila Devi TR, Javubar Sathick K, Abdul Azeez Khan A, Arun Raj L. Novel framework for improving the correctness of reference answers \nto enhance results of asag systems. SN Computer Science, 2023; 4(4): 415.\n 28. Kerneler, Kaggle: semeval 2013 2 and 3 way. https:// www. kaggle. com/ datas ets/ smile s28/ semev al- 2013-2- and-3- way.\n 29. Microsoft. Azure ai services. https:// azure. micro soft. com/ en- us/ produ cts/ ai- servi ces.\n 30. Devlin J, Chang MW, Lee K, Toutanova K. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint \narXiv: 1810. 04805, 2018.\n 31. Paszke A, Gross S, Massa F, Lerer A, Bradbury J, Chanan G, Killeen T, Lin Z, Gimelshein N, Antiga L, et al. Pytorch: An imperative style, high-\nperformance deep learning library. Advances in neural information processing systems, 2019; 32.\n 32. Andrew Poulton and Sebas Eliens. Explaining transformer-based models for automatic short answer grading. In Proceedings of the 5th \nInternational Conference on Digital Technology in Education, pages 110–116, 2021.\n 33. Sultan MA, Salazar C, Sumner T. Fast and easy short answer grading with high accuracy. In Proceedings of the 2016 Conference of the \nNorth American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1070–1075, 2016.\n 34. Saha S, Dhamecha TI, Marvaniya S, Sindhgatta R, Sengupta B. Sentence level or token level features for automatic short answer grading?: \nUse both. In Artificial Intelligence in Education: 19th International Conference, AIED 2018, London, UK, June 27–30, 2018, Proceedings, \nPart I 19, pages 503–517. Springer, 2018.\n 35. Tan Hongye, Wang Chong, Qinglong Duan YuLu, Zhang Hu, Li Ru. Automatic short answer grading by encoding student responses via a \ngraph convolutional network. Int Learn Environ. 2023;31(3):1636–50.\n 36. Li Z, Tomar Y, Passonneau RJ. A semantic feature-wise transformation relation network for automatic short answer grading. In Proceedings \nof the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6030–6040, 2021.\n 37. Filighera A, Tschesche J, Steuer T, Tregel T, Wernet L. Towards generating counterfactual examples as automatic short answer feedback. \nIn International Conference on Artificial Intelligence in Education, pages 206–217. Springer, 2022.\n 38. Kortemeyer Gerd. Could an artificial-intelligence agent pass an introductory physics course? Phys Rev Phys Educ Res. 2023;19(1): 010132.\n 39. Jordan Sally, Mitchell Tom. e-assessment for learning? the potential of short-answer free-text questions with tailored feedback. Br J Educ \nTechnol. 2009;40(2):371–85.\n 40. Meta. Llama 2. https://ai.meta.com/llama/.\nPublisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
  "topic": "Grading (engineering)",
  "concepts": [
    {
      "name": "Grading (engineering)",
      "score": 0.8864389657974243
    },
    {
      "name": "Computer science",
      "score": 0.6480906009674072
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5661492347717285
    },
    {
      "name": "Machine learning",
      "score": 0.48588842153549194
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4619809091091156
    },
    {
      "name": "Task (project management)",
      "score": 0.451644629240036
    },
    {
      "name": "Natural language processing",
      "score": 0.39243075251579285
    },
    {
      "name": "Mathematics education",
      "score": 0.375900000333786
    },
    {
      "name": "Psychology",
      "score": 0.18775826692581177
    },
    {
      "name": "Engineering",
      "score": 0.09158456325531006
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Civil engineering",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I35440088",
      "name": "ETH Zurich",
      "country": "CH"
    }
  ]
}