{
  "title": "6D-ViT: Category-Level 6D Object Pose Estimation via Transformer-Based Instance Representation Learning",
  "url": "https://openalex.org/W3206934897",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2179468764",
      "name": "Zou Lu",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2188756624",
      "name": "Huang Zhangjin",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A14889507",
      "name": "Gu Naijie",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A1780187410",
      "name": "Wang Guoping",
      "affiliations": [
        "Peking University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3111535274",
    "https://openalex.org/W2978088842",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W3202459445",
    "https://openalex.org/W3193686508",
    "https://openalex.org/W3157938624",
    "https://openalex.org/W6755977528",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2128019145",
    "https://openalex.org/W6687484953",
    "https://openalex.org/W2963177347",
    "https://openalex.org/W3166470370",
    "https://openalex.org/W2963188159",
    "https://openalex.org/W2981378444",
    "https://openalex.org/W3107372911",
    "https://openalex.org/W3034597466",
    "https://openalex.org/W3163945288",
    "https://openalex.org/W3202538459",
    "https://openalex.org/W3107992529",
    "https://openalex.org/W2963892972",
    "https://openalex.org/W2921849174",
    "https://openalex.org/W3035355652",
    "https://openalex.org/W1855641990",
    "https://openalex.org/W6785652829",
    "https://openalex.org/W2964249569",
    "https://openalex.org/W2560535692",
    "https://openalex.org/W2083624955",
    "https://openalex.org/W2964056579",
    "https://openalex.org/W2895439318",
    "https://openalex.org/W2963756608",
    "https://openalex.org/W2488101876",
    "https://openalex.org/W2768879211",
    "https://openalex.org/W62794737",
    "https://openalex.org/W1969868017",
    "https://openalex.org/W3035398346",
    "https://openalex.org/W2560609797",
    "https://openalex.org/W2962888833",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2989915422",
    "https://openalex.org/W6840329539",
    "https://openalex.org/W6766978945",
    "https://openalex.org/W6797399245",
    "https://openalex.org/W2085261163",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W2011792403",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W3205586691",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W2462831000",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W3172345956",
    "https://openalex.org/W3153465022",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4287273349",
    "https://openalex.org/W2930967385",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4394671432",
    "https://openalex.org/W2915414978",
    "https://openalex.org/W3170544306",
    "https://openalex.org/W1526868886",
    "https://openalex.org/W2999391884",
    "https://openalex.org/W2796347433",
    "https://openalex.org/W2049981393",
    "https://openalex.org/W3106250896",
    "https://openalex.org/W3211490618",
    "https://openalex.org/W2190691619",
    "https://openalex.org/W4243493583",
    "https://openalex.org/W4287273244",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W4287324101",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W4287124731",
    "https://openalex.org/W4293584584"
  ],
  "abstract": "This paper presents 6D vision transformer (6D-ViT), a transformer-based instance representation learning network suitable for highly accurate category-level object pose estimation based on RGB-D images. Specifically, a novel two-stream encoder-decoder framework is dedicated to exploring complex and powerful instance representations from RGB images, point clouds, and categorical shape priors. The whole framework consists of two main branches, named Pixelformer and Pointformer. Pixelformer contains a pyramid transformer encoder with an all-multilayer perceptron (MLP) decoder to extract pixelwise appearance representations from RGB images, while Pointformer relies on a cascaded transformer encoder and an all-MLP decoder to acquire the pointwise geometric characteristics from point clouds. Then, dense instance representations (i.e., correspondence matrix and deformation field) for NOCS model reconstruction are obtained from a multisource aggregation (MSA) network with shape prior, appearance and geometric information as inputs. Finally, the instance 6D pose is computed by solving the similarity transformation between the observed point clouds and the reconstructed NOCS representations. Extensive experiments with synthetic and real-world datasets demonstrate that the proposed framework achieves state-of-the-art performance for both datasets. Code is available at https://github.com/luzzou/6D-ViT.",
  "full_text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 1\n6D-ViT: Category-Level 6D Object Pose Estimation\nvia Transformer-based Instance Representation\nLearning\nLu Zou, Student Member, IEEE,Zhangjin Huang, Member, IEEE,Naijie Gu, and Guoping Wang\nAbstract—This paper presents 6D vision transformer (6D-ViT),\na transformer-based instance representation learning network\nthat is suitable for highly accurate category-level object pose\nestimation on RGB-D images. Speciﬁcally, a novel two-stream\nencoder-decoder framework is dedicated to exploring complex\nand powerful instance representations from RGB images, point\nclouds and categorical shape priors. The whole framework\nconsists of two main branches, named Pixelformer and Point-\nformer. Pixelformer contains a pyramid transformer encoder\nwith an all-multilayer perceptron (MLP) decoder to extract\npixelwise appearance representations from RGB images, while\nPointformer relies on a cascaded transformer encoder and an all-\nMLP decoder to acquire the pointwise geometric characteristics\nfrom point clouds. Then, dense instance representations ( i.e.,\ncorrespondence matrix and deformation ﬁeld) are obtained from\na multisource aggregation (MSA) network with shape prior,\nappearance and geometric information as input. Finally, the\ninstance 6D pose is computed by leveraging the correspondence\namong dense representations, shape priors, and instance point\nclouds. Extensive experiments on both synthetic and real-world\ndatasets demonstrate that the proposed 3D instance representa-\ntion learning framework achieves state-of-the-art performance on\nboth types of datasets and signiﬁcantly outperforms all existing\nmethods. Our code will be available.\nIndex Terms—6D object pose estimation, 3D object detection,\nvision transformer, representation learning.\nI. I NTRODUCTION\nO\nBJECT pose estimation refers to predicting the location\nand orientation of 3D objects, which is a fundamental\nproblem in robotic applications such as grasping and manipu-\nlation. In recent years, remarkable progress [1]–[12] has been\nmade on instance-level 6D object pose estimation, where exact\n3D CAD models and their sizes are available in advance.\nUnfortunately, considering the diversity of object instances and\nthe cost of building a CAD model for each instance, these\nmethods are difﬁcult to generalize to real practice.\nRecently, category-level 6D object pose estimation has be-\ngun to receive increasing attention [13]–[18] given its practical\nimportance. Compared with the instance-level problem, the\ngoal of this task is to predict the 6D pose of unseen object\ninstances of the same category for which no CAD models\nLu Zou, Zhangjin Huang, and Naijie Gu are with University of Science and\nTechnology of China, Hefei 230031, China (e-mail: lzou@mail.ustc.edu.cn;\nzhuang@ustc.edu.cn; gunj@ustc.edu.cn). (Corresponding author: Zhangjin\nHuang)\nGuoping Wang is with Peking University, Beijing 100871, China (e-mail:\nwgp@pku.edu.cn).\nManuscript received April 19, 2005; revised August 26, 2015.\nimage patch\nMulti\n-\nSource \nAggregation \nNetwork\nSimilarity \nTransformation\nPixelformer\nPointformer\nEncoder\nDecoder\nEncoder\nDecoder\nDense Instance \nRepresentations\ncategory prior\n6\nD pose\nobserved point \ncloud\nobserved point \ncloud\nFig. 1. The main network of our transformer-based instance representation\nlearning network for category-level 6D object pose estimation.\nare available. In general, the classical instance-level pose es-\ntimation methods usually predict object poses by establishing\nthe correspondence between an RGB or RGB-D image and\nthe CAD model of the object. However, in the category-\nlevel scenario, since there are no speciﬁc CAD models, it\nis quite difﬁcult to directly establish this correspondence. In\naddition, even within the same category, objects can exhibit\nsigniﬁcant variations in shape, color, and size. Consequently,\ndue to the large intraclass variations in texture and shape\namong instances, category-level pose estimation is much more\nchallenging than instance-level problems [19].\nTo achieve reliable category-level object pose estimation,\nWang et al. [18] innovatively proposed normalized object\ncoordinate space (NOCS) as a shared canonical representation\nof all object instances within a category. Since the CAD model\nof the object is unknown, they reconstruct the canonical model\nrepresentation in the NOCS and establish the dense correspon-\ndence between the reconstructed NOCS model and instance\nimages or point clouds to realize 6D pose estimation. Later,\nTian et al. [17] improved the NOCS model reconstruction\nprocess by introducing shape priors. They extract category fea-\ntures from the shape priors as auxiliary information of instance\nfeatures to alleviate the shape variations across instances\nwithin the same category. Recently, Chen et al. [15] proposed\nthe fast shape-based network (FS-Net) to further improve the\nquality of category-level pose features. They employ a shape-\nbased 3D graph convolution autoencoder to extract the latent\nfeatures through the observed point reconstruction and propose\na decoupled rotation mechanism to decode instance rotation\ninformation from the latent features. Despite the promising\nperformance gains, this method is too complex and requires\narXiv:2110.04792v2  [cs.CV]  30 Oct 2021\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2\ntraining separate models for different object categories due to\ntheir speciﬁc data augmentation.\nIn this work, we tackle the problem of category-level object\npose estimation from the perspective of uniﬁed and power-\nful instance representation learning on RGB-D data. More\nspeciﬁcally, inspired by the most famous transformer archi-\ntectures [20]–[24] widely considered by the natural language\nprocessing and computer vision communities, we develop\na two-stream transformer-based encoder-decoder architecture,\n6D vision transformer (6D-ViT), for highly accurate category-\nlevel object pose estimation. Our network is composed of\nthe Pixelformer and Pointformer modules, which are used to\nexplore the appearance and geometric characteristics of the\nobject instance, respectively. Then, the dense representations\n(i.e., correspondence matrix and deformation ﬁeld) of the\nobject instance are obtained through the fusion of appearance\ninformation and geometric information from different sources.\nFinally, the category-level 6D pose is obtained by establishing\nthe correspondence among dense representations, categorical\nshape priors, and the observed point clouds. Comprehensive\nexperiments on both synthetic (CAMERA25) and real-world\n(REAL275) datasets suggest that the proposed method sub-\nstantially improves in terms of performance on both datasets\nand achieves state-of-the-art performance through a uniﬁed\nmodel for all categories.\nOur contributions can be summarized as follows.\n• We present a novel two-stream category-level 6D object\npose estimation network based on transformer architec-\ntures. The network can semantically enhance the quality\nof instance representations by capturing the long-range\ncontextual dependencies of elements from RGB images\nand point clouds.\n• Our network includes Pixelformer and Pointformer, both\nof which are encoder-decoder architectures for dense\ninstance representation learning. In particular, both en-\ncoders are multistage transformer architectures for com-\nplex feature extraction, while both decoders are all-MLP\narchitectures for efﬁcient feature aggregation.\n• Extensive experiments demonstrate that our proposed 6D-\nViT achieves state-of-the-art performance on both the\nCAMERA25 and REAL275 datasets and signiﬁcantly\noutperforms existing works.\nThe remainder of this paper is organized as follows. Sec-\ntion II brieﬂy reviews some related works about 6D object\npose estimation including instance-level (Section II-A) and\ncategory-level (Section II-B) works. Section III introduces the\npresented method in detail, including the overall framework\n(Section III-A), instance representation learning on RGB im-\nages (Section III-B), instance representation learning on point\nclouds (Section III-C), joint and dense representation gener-\nation and pose estimation (Section III-D) and loss functions\n(Section III-E). The experimental results of our network with\ncomparisons to existing methods are reported in Section IV.\nSpeciﬁcally, we evaluate the proposed method in terms of 6D\nobject pose estimation (Section IV-D) and 3D model recon-\nstruction (Section IV-E). In addition, the ablation studies of\nour network are discussed in Section IV-F. Finally, conclusions\nand future directions are provided in Section V.\nII. R ELATED WORK\nA. Instance-Level 6D Object Pose Estimation\nA large amount of research has focused on instance-level\nobject pose estimation. In this section, we discuss only some of\nthe most notable studies. According to the format of the input\ndata, instance-level pose estimation methods can be broadly\ndivided into RGB-based and RGB-D-based methods. Tradi-\ntional RGB-based methods [1], [25] realize pose estimation\nby detecting and matching keypoints with the known 3D CAD\nmodel of the object. However, these methods rely heavily\non handcrafted features that are not robust to low-texture\nobjects and cluttered environments. Recently, many works\nhave applied deep learning techniques to this task due to their\nrobustness to environmental variations. Kehlet al. [3] extended\nthe 2D object detection network [26] to predict the identity of\nan object, the 2D bounding box and the discretized orientation.\nTekin et al. [9] proposed ﬁrst detecting the keypoints of the\nobject and then solving a perspective-n-point problem for pose\nestimation. Zakharov et al. [12] estimated the dense 2D-3D\ncorrespondence map between the input image and the object\nmodel. Peng et al. [7] presented the prediction of a unit vector\nfor each pixel pointing toward the keypoints.\nAs an increasing number of RGB-D datasets are available,\nanother line of work [2], [5], [10] began to apply both RGB\nand depth images to improve the pose estimation accuracy.\nConsidering the different distributions of RGB and depth data,\nthere are several different ways to deal with these two modali-\nties. Kehl et al. [2] simply concatenated RGB and depth values\nand fed the 4-channel data into a multistage model to produce\npose estimations. Xiang et al. [11] ﬁrst predicted a rough 6D\npose from an RGB image, followed by the application of the\niterative closest point (ICP) algorithm [27] using depth images\nfor reﬁnement. Wang et al. [10] proposed DenseFusion; they\ndesign a two-branch framework to exploit the features of RGB\nand depth images separately and fuse the features of different\nmodalities through a fully connected (FC) layer. DenseFusion\nachieves state-of-the-art performance while reaching almost\nreal-time inference speed. Nevertheless, it merges the features\nof two modalities in a straightforward manner, which is not\nsufﬁcient to capture their inherent correlations. To this end,\nZou et al. [28] further improved the feature fusion process in\nDenseFusion [10] and developed a cross-modality attention\nscheme to learn the feature interactions between the RGB\nimages and point clouds. Our approach also estimates 6D\nobject poses from RGB-D images; however, we focus on\na more general setting where object CAD models are not\navailable during inference.\nB. Category-Level 6D Object Pose Estimation\nFew previous works [13]–[18] have focused on estimating\nthe 6D poses of unseen objects. Compared to instance-level\nproblems, category-level tasks are much more challenging due\nto the large intraclass variations in the aspects of texture\nand shape among instances. To address the above issue,\nsome researchers have proposed constructing an intermediate\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 3\ninstance representation to mitigate such differences. Wang et\nal. [18] represented different object instances within a category\nas a shared NOCS. They train a region-based deep network to\ninfer the correspondence from observed pixels to points in the\nNOCS. As a result, the 6D pose and size can be calculated by\nshape matching between the predicted correspondence and the\nobserved points. Chen et al. [14] introduced a uniﬁed category\nrepresentation for a large variety of instances called canonical\nshape space (CASS). Additionally, a variational autoencoder\n(V AE) is trained to estimate the category-level object pose.\nSimilar to [14], Tian et al. [17] proposed shape prior defor-\nmation (SPD), which leverages the category-sensitive features\nto explicitly model the shape variation when reconstructing the\nNOCS model. Speciﬁcally, they enhance the RGB-D features\nby introducing shape priors as an aid to the process of NOCS\nmodel reconstruction. To better explore pose-related features,\nChen et al. [15] proposed FS-Net, which ﬁrst extracts the latent\ninstance features through the observed point reconstruction\nwith a shape-based 3D graph convolution autoencoder. Then, a\ndecoupled rotation mechanism is proposed to decode instance\nrotation information from the instance features. To increase the\ngeneralization ability of the network, an online 3D deformation\nmechanism is proposed for data augmentation. Despite the\nimpressive performance gains, this method is too complex,\nand it requires the training of separate models for different\nobject categories due to their speciﬁc data preprocessing,\nmaking it inconvenient for practical use. More recently, Lin\net al.[16] presented DualPoseNet to learn rotation-equivariant\nshape features; it stacks two parallel pose decoders at the top\nof a shared pose encoder, where the implicit decoder predicts\nobject poses with a working mechanism different from that of\nthe explicit one. Thus, complementary supervision is imposed\non the training of the pose encoder.\nOur work is based on SPD [17] and takes inspiration\nfrom the latest transformer architectures [20]–[24] success-\nfully applied in natural language processing and computer\nvision. Speciﬁcally, we design two transformer-based encoder-\ndecoder networks to individually explore the compact instance\nrepresentations from the RGB images and point clouds. The\nexperimental results demonstrate that our method achieves\nstate-of-the-art performance and signiﬁcantly outperforms ex-\nisting works.\nIII. M ETHOD\nGiven a calibrated RGB-D image pair, our goal is to predict\nthe 6D object pose represented by a rigid transformation\n[R|t], where R ∈ SO(3) and t ∈ R3. To handle scenes\ncontaining multiple instances, we ﬁrst apply an off-the-shelf\ninstance segmentation network ( i.e., Mask R-CNN [29]) to\ndetect and segment objects in the scene. Next, the yielded\nobject bounding box is exploited to crop the RGB image into\nimage patches, and the segmentation mask is leveraged to\nconvert the depth image into observed point clouds according\nto the camera intrinsic parameters. After that, image patches,\nobserved point clouds, and categorical shape priors are fed\ninto the main part of our network.\nA. Architecture Overview\nAs shown in Fig. I, our main network consists of three sub-\nnetworks: (1) Pixelformer for instance representation learning\non RGB images (Section III-B); (2) Pointformer for instance\nrepresentation learning on point clouds (Section III-C); and (3)\na multisource aggregation (MSA) network for joint and dense\nrepresentation generation and pose estimation (Section III-D).\nMore speciﬁcally, the image patches I and observed point\nclouds Pare ﬁrst passed through Pixelformer and Pointformer\nto learn instance representations existing in different modali-\nties. Then, the instance representations in the two modalities,\ndenoted as F and F, are fused and enhanced by the category-\nsensitive representations extracted from the categorical shape\npriors Ppri to generate the joint and dense instance representa-\ntions (i.e., the correspondence matrix Aand deformation ﬁeld\nDdef ). Finally, the dense representations, categorical shape\npriors and observed instance point clouds are leveraged by\nthe Umeyama algorithm [30] to calculate the 6D object pose\n[R|t]. In addition, the loss functions used to train the whole\nnetwork are introduced in Section III-E. The design details of\neach step are discussed below.\nB. Instance Representation Learning on RGB Images\nAs depicted in Fig. 2, Pixelformer is an encoder-decoder\narchitecture that consists of two main components: 1) a\nmultiscale transformer encoder that contains I stages to gen-\nerate high-resolution coarse appearance embeddings and low-\nresolution ﬁne appearance embeddings; and 2) a multilayer\nperceptron (MLP) decoder to upsample and aggregate the\npyramid features to produce the pixelwise instance appearance\nrepresentations.\n1) Multiscale Transformer Encoder: The design of the\ntransformer encoder in our Pixelformer is partly inspired by\nthe latest multiscale vision transformer: (PVT) [22], and is\ntailored and optimized for our task. Speciﬁcally, our multiscale\ntransformer encoder consists of four stages with different latent\nfeature dimensions. In each stage i, it has an overlapped\npatch embedding (OPE) layer and two layers of alternating\nspatial reduction-based multihead attention (SRMA) and a\nconvolutional feed-forward network (C-FFN). In addition, we\nemploy InstanceNorm (IN) before each SRMA and C-FFN\nmodule and provide residual connections around each of them.\na) Overlapped Patch Embedding:In vision transformers,\npatch embedding is usually used to combine nonoverlapping\nimages or feature patches. However, since the patches are\nnonoverlapping, this fails to preserve the local continuity\naround those patches. In this work, we utilize OPE as in-\ntroduced in [23] to tokenize images. Speciﬁcally, given an\ninput RGB image with a resolution of H×W×3, we deﬁne\nK as the patch window size, S as the stride between two\nadjacent patches, and P as the padding size. We ﬁrst split the\ninput image into H\n2(i+1) · W\n2(i+1) patches, and each patch has a\nsize of Ki×Ki×Ci−1, where i ∈ {1,2,3,4}and C0 = 3.\nThen, the patch embedding process performs patch merging\nvia linear projection on these patches to produce feature maps\nwith the same dimensions as the nonoverlapping process. In\nthe experiments, we set K1 = 7, S1 = 4, P1 = 3, and Kj = 3,\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 4\nMulti\n-\nScale RGB Transformer Encoder\nTransformer \nStage\n-\n1\nTransformer \nStage\n-\n2\n: 11\n44\nHWFC\n:3I n p u tH W\n: 22\n88\nHWFC\n: 33\n1 6 1 6\nHWFC\n: 44\n3 2 3 2\nHWFC\nMLP Decoder\nStage \ni\nOverlapped \nPatch \nPartition\nMLP\n(\nC\ni\n)\nPatch Merging\nSpatial\nReduction\nNorm\nSpatial\n-\nReduction based \nMulti\n-\nHead Attention\nConvolutional\n-\nMLP \nFeed Forward\nNorm\nMLP\n(\n256\n)\nConv\n(\n3\n×\n3\n)\nGELU\nMLP\n(\nC\ni\n)\n×\n2\nTransformer \nStage\n-\n4\nTransformer \nStage\n-\n3\nFeature Map\nMLP\n(\n256\n)\n, \nUpSample\n(\nH\n/\n4\n,\nW\n/\n4\n)\nConcat\n(\n1024\n)\nMLP\n(\n256\n)\n, \nUpSample\n(\nH\n/\n2\n,\nW\n/\n2\n)\nMLP\n(\n64\n)\n, \nUpSample\n(\nH\n,\nW\n)\nMLP\n(\n64\n, \n64\n)\nPixel\n-\nwise Color \nEmbedding\n\n\n64HW\n1122\niii\nHW C\n1122\niii\nHW C\nMulti\n-\nScale Transformer Encoder\nTransformer \nStage\n-\n1\nTransformer \nStage\n-\n2\n: 11\n44\nHWFC\n× × 3\nWH\n: 22\n88\nHWFC\n: 33\n16 16\nHWFC\n: 44\n32 32\nHWFC\nMLP Decoder\nStage \ni\nOverlapped \nPatch \nPartition\nMLP\n(\nC\ni\n)\nSpatial\nReduction\nNorm\nNorm\nMLP \n(\n256\n)\nConv\n3\n×\n3\nGELU\nMLP\n(\nC\ni\n)\n×\n2\nTransformer \nStage\n-\n4\nTransformer \nStage\n-\n3\nMLP \n(\n256\n)\n, \nUpSample \n(\nH\n/\n4\n,\nW\n/\n4\n)\nConcat\n(\n1024\n)\nMLP \n(\n256\n)\n, \nUpSample \n(\nH\n/\n2\n,\nW\n/\n2\n)\nMLP \n(\n64\n)\n, \nUpSample \n(\nH\n,\nW\n)\nMLPs \n(\n64\n, \n64\n)\n\n\n 1122\niii\nHW C\n 1122\niii\nHW C\nOPE\nSRMA\n× ×\n W H DF\nC\n-\nFFN\nFig. 2. The structure of the proposed Pixelformer. The network consists of two main modules: a multiscale transformer encoder to extract coarse and\nﬁne appearance embeddings from RGB images and an MLP decoder to fuse the multiscale embeddings and to generate the pixelwise object appearance\nrepresentations. Note that in the ﬁgure, each item in the format of Function(·, ...,·) denotes the feature dimension after the corresponding operation.\nSj = 2, Pj = 1, where j ∈{2,3,4}. As a result, we can obtain\nthe multiscale features {Fi|i = 1,2,3,4}with the shapes of\n{H\n4 ×W\n4 ×C1, H\n8 ×W\n8 ×C2, H\n16 ×W\n16 ×C3, H\n32 ×W\n32 ×C4}.\nFollowing the design rules of ResNet [31], we use small output\nchannels in shallow stages, i.e., Ci >Ci−1 for i∈{1,2,3,4}.\nb) Spatial Reduction-based Multihead Attention: Con-\nsidering the original multihead attention mechanism [21],\ngiven a series of vectors Qi, Ki, Vi = MLP(Ci,Ci)(Fi),\ni∈{1,2,3,4}at each stage i, each head of Qi, Ki, Vi has the\nsame shape of ( H\n2(i+1) · W\n2(i+1) )×dhead(i) = (Hi ·Wi)×dhead(i).\nHere, N= (Hi ·Wi) is the length of a sequence, and dhead(i)\nrefers to the dimension of each head at each stage, which is\ncomputed as dhead(i) = Ci/Mi, with Mi as the number of\nattention heads. Note that in the format of Function(b,c)(a)\nin the full text, arepresents the input feature vector, and band\nc represent the dimensions of the input and output features,\nrespectively. Therefore, the multihead attention operation is\ncalculated as:\nAttention(Qi,Ki,Vi) = Softmax( QiKT\ni√dhead(i)\n)Vi,∀i. (1)\nThe computational complexity of Eq. (1) is O(N2), which\nis prohibitive for large feature resolutions. Instead, we utilize\nthe reduction process introduced in PVT [22], which uses a\nreduction ratio Ri in stage i to reduce the spatial scale of\nK and V before the attention operation executes. The spatial\nreduction (SR) operation is deﬁned as follows:\nSR(x) = Reshape(IN(x),Ri)Wi,x ∈{Ki,Vi},∀i. (2)\nIn Eq. (2), Reshape( x, Ri) is the operation of reshaping\nthe input x ∈ RN×Ci to the sequence with the shape of\nN\nR2\ni\n×(R2\ni ·Ci), and Wi ∈ R(R2\ni ·Ci)×Ci refers to a linear\nprojection operation that reduces the dimension of the input x\nto Ci. Hence, the complexity of the self-attention mechanism\nis reduced from O(N2) to O(N2)/R. In the experiments, we\nset R2 to [64,16,4,1] from stage one to stage four. Without\nabuse of notation, we denote the multiscale features after\nSRMA as {Fi|i= 1,2,3,4}, the same as the input.\nc) Convolutional Feed-Forward Network: Previous\ntransformer-based image encoders employed MLPs only as\ntheir feed-forward network (FFN) and completely relied on\nthe self-attention mechanism to achieve feature enhancement.\nAlthough effective, this design cannot perceive the locality\nand translation invariance of the object and ignores the\ninterpixel dependencies that play a critical role in object\npose estimation. Based on this, we propose the insertion of\nconvolutional operations into the FFN.\nSpeciﬁcally, we insert a 3 ×3 convolutional operation with\na padding size of 1 between the ﬁrst MLP layer and the\nGaussian error linear unit (GELU) [32] operation. As a result,\nwe remove the positional encoding in the traditional FFN since\nthe convolutional operation already incorporates positional\ninformation. We formulate the C-FFN as:\nˆF0\ni = MLP(Ci,Cexpand)(IN(Fi)),∀i, (3)\nˆF1\ni = Conv3×3(Cexpand,Cexpand)( ˆF0\ni ),∀i, (4)\nˆF2\ni = GELU(ˆF1\ni ),∀i, (5)\nˆFi = MLP(Cexpand,Ci)( ˆF2\ni ) +Fi,∀i, (6)\nwhere Cexpand represents the expanded feature dimension\nof the FFN [21] and {ˆFi|i = 1,2,3,4}denotes the output\npyramid feature maps of the C-FFN module. We set the\nexpansion ratio of each stage to [8,8,4,4].\n2) Multilayer Perceptron Decoder: We design a simple\ndecoder to generate the uniﬁed appearance representations\nof the object instance from the pyramid features obtained\nfrom the multiscale transformer encoder. First, the pyramid\nfeatures {ˆFi|i = 1,2,3,4}are passed through an MLP layer\nto unify the channel dimensions to be consistent with the\nmaximum feature channel C4. Then, the uniﬁed feature maps\nare upsampled to 1/4 the size of the original input image and\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 5\nN\n×\n3\nSTN\nN\n×\n3\nKNN\nVLAD\nN\n×\n1024\nglobal pooling\nN\n×\n128\nN\n×\n384\nP\nointwise feature learning\nFC \n512\nFC \n256\nFC  C\n1\n×\nC\nN\n×\n128\nshared FC \n1024\nshared FC \n128\nshared FC \n128\nshared FC \n512\nshared FC \n1024\nshared FC M\nN×M\nN\n×\n64\nSegmentation \nscores\nclassification\n \nscores\nSegmentation\nClassification\n1\n×\n1024\nN\n×\n1536\nVector\n/\nT\nensor\nPointwise feature\nPointwise feature\nGlobal feature\nConcatenation\n \n(\nconcat\n)\nF\nully connected \n(\nFC\n)\nPo\noling operation \n(\nC\n=\n10 \nor \n40\n)\nr\neplicate\n \n(\nN\n×\n1024\n)\nhigh\n-\nlevel semantic\nskip\n-\nconnection\nlow\n-\nlevel geometric\nCascaded Point Cloud Transformer Encoder\nTransformer \nStage\n-\n1\nMLP Decoder\nStage \ni\nNorm\nCWMHA\nFFN\nGELU\nNorm\n \n\n \n×\n2\nShared MLP \n(\n64\n)\nConcat\n(\n256\n)\nShared MLP \n(\n64\n)\n \nShared MLPs\n(\n64\n, \n64\n)\nShared MLP \n(\nC\ni\n)\nShared MLPs\n(\n256\n, \n256\n)\nShared MLP\n(\nC\ni\n)\nTransformer \nStage\n-\n4\nTransformer \nStage\n-\n3\nTransformer \nStage\n-\n2\nN \n× \nC\ni\nF\n1\n \n: \nN\n×\n \nC\n1\nF\n2\n \n: \nN\n×\n \nC\n2\nF\n3\n \n: \nN\n×\n \nC\n3\nF\n4\n \n: \nN\n×\n \nC\n4\n×\n\n\n\n\n\u0000\nN\nIFE\n×\n\n3\n\n\n\u0000\nN\n \n\n \nFig. 3. The structure of the proposed Pointformer. The network consists of two main modules: a cascaded transformer encoder with different feature\ndimensions to capture multilevel point embeddings and an all-MLP decoder to unify and fuse the multilevel point embeddings and to produce the pointwise\nobject geometric representations. Note that in the ﬁgure, each item in the format of Function(·, ...,·) denotes the feature dimension after the corresponding\noperation.\nconcatenated along the feature channel to form H\n4 ×W\n4 ×4C4.\nLater, an MLP layer is used to fuse the concatenated features\nto H\n4 ×W\n4 ×C4. Next, the fused features are passed through two\nalternative upsampling and MLP layers to obtain the uniﬁed\nappearance representations. We formulate the process of the\nMLP decoder as follows:\nˆF0\ni = MLP(Ci,C4)( ˆFi),∀i, (7)\nˆF1\ni = UpSample(Hi ×Wi,H\n4 ×W\n4 )( ˆF0\ni ),∀i, (8)\nˆF2 = MLP(4C4,C4)(Concat( ˆF1\ni )),∀i, (9)\nˆF3 = UpSample(H\n4 ×W\n4 ,H\n2 ×W\n2 )( ˆF2), (10)\nˆF4 = MLP(C4,2D)( ˆF3), (11)\nˆF5 = UpSample(H\n2 ×W\n2 ,H ×W)( ˆF4), (12)\nF = MLP(2D,D)(MLP(2D,2D)( ˆF5)), (13)\nwhere F refers to the output pixelwise instance appearance\nrepresentations with the shape H ×W ×D, where D is the\noutput dimension, which is set as 32 to be consistent with\nprevious appearance extractors [10], [17]. Additionally, for\nthe UpSample(b,c)(a) operation, we utilize simple bilinear\ninterpolation to recover the details of the origin feature map.\nThe design details of Pixelformer can be found in TABLE I.\nC. Instance Representation Learning on Point Clouds\nDue to the disorder and irregularity of 3D point clouds, deep\nrepresentation learning on point clouds is very challenging.\nMoreover, the operations on point clouds must be permutation-\ninvariant, which makes this problem more difﬁcult [33]. The\nrecent transformer architecture [21] and its core component,\nthe self-attention mechanism, not only meet the demand for\npermutation-invariance but also have been proven to be highly\nTABLE I\nDETAILED SETTINGS OF THE PROPOSED PIXELFORMER FOR INSTANCE\nREPRESENTATION LEARNING ON RGB IMAGES .\nParam Name Stage I\n1 2 3 4\nNumber of Attention Heads M 1 2 5 8\nFFN Expansion Ratio RFFN 8 8 4 4\nFeature Channels C 32 64 160 256\nExpanded Dimension Cexpand 256 512 640 1024\nReduction Ratio R 8 4 2 1\nPatch Window Size ( K, S, P) (7,4,3) (3,2,1) (3,2,1) (3,2,1)\nNumber of Transformers L 2 2 2 2\nexpressive in modeling the long-range dependencies between\ndifferent elements within a sequence of data. Therefore, adapt-\ning the transformer architectures to point cloud processing is\nan ideal choice. Nevertheless, only a few studies [24], [34],\n[35] have attempted this, and the existing networks suffer from\ngrowing memory requirements, as well as high computational\nconsumption. To address this problem, we design a simple but\neffective transformer-based 3D point cloud learning network\ncalled Pointformer suitable for learning the geometric repre-\nsentations of the object instance.\nAs illustrated in Fig. 3, given an input point cloud P\nwith N points, before the main network starts, we ﬁrst apply\ntwo layers of shared MLPs to map the raw points to the d-\ndimensional feature description. Then, similar to Pixelformer,\nour Pointformer is an encoder-decoder network that contains\ntwo main components: 1) a cascaded transformer encoder that\nincludes I (I = 4) stages with different dimensions to capture\nmultilevel point embeddings and 2) an all-MLP decoder to\nunify and fuse the multilevel embeddings to produce the\npointwise geometric representations of the object instance.\n1) Cascaded Transformer Encoder:At each stage i, where\ni ∈ {1,2,3,4}, there is an input embedding layer (IFE)\nthat maps the d-dimensional feature description to the cor-\nresponding embedded dimension Ci, followed by two layers\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 6\nof alternating channelwise multihead attention (CWMHA) and\na FFN to model the long-range context interactions among\npoints and to further improve the point representations.\na) Input Feature Embedding: There are many input\nfeature embedding methods in the literature [34]; these are\nnot covered in this work. For the sake of simplicity, we\ndirectly apply an MLP layer to the d-dimensional feature\ndescription to obtain pointwise embeddings with dimensions\nof Ci, where i ∈{1,2,3,4}. Thus, we can obtain multilevel\npoint embeddings with the shape of {Fi|i = 1,2,3,4} =\n{N ×C1, N ×C2, N ×C3, N ×C4}.\nb) Channelwise Multihead Attention: To highlight the\ninteractions across different embedding channels, we build\na CWMHA module to enhance the contextual point repre-\nsentations. Speciﬁcally, let Qi, Ki, Vi = MLP(Ci,Ci)(Fi),\ni ∈{1,2,3,4}be the query, key and value vectors at each\nstage i, and each head of Qi, Ki, and Vi has the same shape\nof N ×dhead(i), where dhead(i) = Ci/Mi, Mi denotes the\nnumber of attention heads, and N is the number of points.\nTherefore, the CWMHA operation is formally deﬁned as\nAi = Softmax( QiKT\ni√dhead(i)\n),∀i, (14)\nAttention(Qi,Ki,Vi) =AiVi,∀i, (15)\nwhere Ai ∈RCi×Ci denotes the attention weights between\nchannels, which captures the channelwise importance. Since\nQi, Ki, and Vi in Eq. (14) are determined by linear trans-\nformations and the input multilevel point embeddings Fi,\ni ∈ {1,2,3,4}, they are all order-independent. In addition,\nthe softmax function in Eq. (14) and the weighed sum op-\neration in Eq. (15) are both permutation-invariant. Therefore,\nthe CWMHA mechanism is permutation-invariant, making it\nsuitable for disordered and irregular point cloud processing.\nWithout abuse of notation, we denote the multilevel features\nafter CWMHA as {Fi|i= 1,2,3,4}, the same as the input.\nc) Feed-Forward Network: We utilize the instance nor-\nmalization operation, the GELU [32] activation function, and\nMLPs to form the FFN in our Pointformer. The process is\ndeﬁned as\nˆF0\ni = MLP(Ci,Cexpand)(IN(Fi)),∀i, (16)\nˆF1\ni = MLP(Cexpand,Cexpand)( ˆF0\ni ),∀i, (17)\nˆF2\ni = GELU(ˆF1\ni ),∀i, (18)\nˆFi = MLP(Cexpand,Ci)( ˆF2\ni ) +Fi,∀i. (19)\nwhere Cexpand represents the expanded embedding dimen-\nsion of the FFN [21] and {ˆFi|i = 1,2,3,4} denote the\noutput multilevel point embeddings of the FFN module. To\nbe consistent with Pixelformer, we set the expansion ratio of\neach stage as [8,8,4,4].\n2) All-Multilayer Perceptron Decoder:We design a simple\nall-MLP decoder to generate the pointwise geometric repre-\nsentations of the object instance from the multilevel point\nembeddings created by the cascaded transformer encoder.\nFirst, the multilevel point embeddings {ˆFi|i = 1,2,3,4}are\nfed into four separate MLP layers to unify the different channel\ndimensions to D. Second, the uniﬁed point embeddings are\nconcatenated to produce new point embeddings with the shape\nof N ×4D. Finally, an MLP layer is applied to project the\nconcatenated point embeddings to a D-dimensional feature\ndescription. We formulate the process of the decoder as\nfollows:\nˆF0\ni = MLP(Ci,D)( ˆFi),∀i, (20)\nˆF1 = Concat(ˆF0\ni ),∀i, (21)\nF= MLP(4D,D)( ˆF1), (22)\nwhere Frefers to the output pointwise geometric embedding\nwith the shape of N ×D and D is the output dimension,\nwhich is set as 64 to be consistent with previous geometric\nextractors [10], [17]. The design details of Pointformer can be\nfound in TABLE II.\nTABLE II\nDETAILED SETTINGS OF THE PROPOSED POINTFORMER FOR INSTANCE\nREPRESENTATION LEARNING ON POINT CLOUDS .\nParam Name Stage I\n1 2 3 4\nNumber of Attention Heads M 1 2 5 8\nFFN Expansion Ratio RFFN 8 8 4 4\nFeature Channels C 32 64 160 256\nExpanded Dimension Cexpand 256 512 640 1024\nNumber of Transformers L 2 2 2 2\nD. Joint and Dense Representation Generation and Pose\nEstimation\nA\n© \nShared MLPs\n(\n128\n, \n1024\n)\n© \nShared MLPs\n(\n64\n, \n64\n, \n64\n)\nShared MLPs\n(\n128\n, \n1024\n)\nShared MLPs\n(\n512\n, \n256\n, \n1024\n)\n© \nShared MLPs\n(\n512\n, \n256\n, \n3\n)\n© \nR\nR\n(\nN\n×\n128\n)\n(\nN\n×\n64\n)\n(\n1\n×\n1024\n)\nF\n(\nN\n×\n64\n)\n(\nN\n×\n1024\n)\n(\n1\n×\n1024\n)\n(\nN\n×\n2112\n)\n(\nN\n×\n2176\n)\nN\n×\n1024\n(\nN\n×\n2048\n)\n\n\npri\n\ndef\n\n(\nW\n×\nH\n×\n32\n)\n(\nN\n×\n64\n)\n(\nN\nc\n×\n3\n)\n(\nN\nc\n×\n3\n)\n(\nN\n×\nN\nc\n)\ninstance local representation\ncategory global representation\npixel\n-\npoint feature alignment\ninstance global representation\ncategory local representation\n© \nconcatenation\nrepeat\nmax\n-\npooling\nR\nA\nFig. 4. The structure of the MSA network. The network takes the pixelwise\nappearance representations, the pointwise geometric representations, and the\ncategorical shape priors as input and outputs the dense instance representations\n(e.g., correspondence matrix and deformation ﬁeld) used to calculate the 6D\nobject pose.\nWe utilize an MSA network to unify and to generate the\ndense instance representations, as suggested in SPD [17] ( i.e.,\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 7\ndeformation ﬁeld and correspondence matrix) regarding the\npixelwise appearance representations F ∈ RH×W×D, the\npointwise geometric representations F ∈RN×D, and the\ncategorical shape priors Ppri ∈RNc×3 provided by SPD [17],\nwhere Nc is the number of points in the shape priors.\nAs shown in Fig. 4, the MSA contains two parallel branches.\nIn the upper branch, we associate the appearance representa-\ntions F with the geometric representations Faccording to the\nnatural pixel-to-point correspondence. In addition, to unify the\nfeature dimensions of these two modalities, an MLP layer is\napplied. After that, the aligned cross-modality representations\nare concatenated and termed instance local representation\nand fed into shared MLPs, followed by an average pooling\nlayer to obtain the instance global representation. In the\nlower branch, the categorical shape priors Ppri are utilized\nto provide the prior knowledge of each category. Speciﬁcally,\nwe employ two shared MLPs to extract the category local\nrepresentation and the category global representation from\nthe shape priors. Then, the two category-level representations\nare concatenated and enriched either by the category local\nrepresentation followed by shared MLPs to generate the\ndeformation ﬁeld Ddef ∈RNc×3 from the categorical shape\npriors to a particular instance canonical model or by the\ninstance local representationfollowed by other shared MLPs\nto generate the correspondence matrix A∈ RN×Nc , which\nrepresents the soft correspondence between each point in the\ninput instance point cloud Pand all points in the reconstructed\ninstance model ˆP, in which ˆP = Ppri + Ddef . Finally, the\nNOCS coordinates of the instance point cloud, denoted as ˆM,\nare obtained by multiplying the correspondence matrix Aand\nthe reconstructed object model ˆP, i.e.,\nˆM= A× ˆP. (23)\nGiven the instance point cloud P with its corresponding\nNOCS coordinates ˆM, we use the Umeyama algorithm [30]\nto estimate the optimal similarity transformation parameters\n(a.k.a., rotation, translation, and scale), where the rotation\nand translation parameters correspond to the 6D object pose\nand the scale parameter corresponds to the object size. The\nRANdom SAmple Consensus (RANSAC) algorithm [36] is\nalso used to remove outliers and to achieve robust estimation.\nE. Loss Functions\nWe utilize the same loss functions as those suggested in\nSPD [17] to supervise different components of our method.\n1) Reconstruction Loss: We employ the Chamfer distance\nto measure the similarity between the ground truth 3D model\nand the reconstructed 3D model, which is deﬁned as\nLcd( ˆPi\nc,Pi\nc) =\n∑\nx∈Pic\nmin\ny∈ˆPic\n||x−y||2\n2 +\n∑\ny∈ˆPic\nmin\nx∈Pic\n||x−y||2\n2,\n(24)\nwhere Pi\nc is the ground truth 3D point cloud model of instance\nifrom category cand ˆPi\nc is its corresponding reconstructed 3D\npoint cloud model. Since the reconstructed model is derived\nby the categorical shape priors and deformation ﬁeld, i.e., ˆP=\nPpri + Ddef , the regression of the deformation ﬁeld is also\nimplicitly supervised by the reconstruction loss.\n2) Correspondence Loss: We follow SPD [17] to indirectly\nsupervise the deformation ﬁeld Ddef by supervising the NOCS\ncoordinates ˆMwith the smooth L1 loss function,\nˆLcor(m,mgt) =\n{ 5(m−mgt)2, |m−mgt|≤ 0.01\n|m−mgt|−0.05, otherwise ,\n(25)\nLcor( ˆM,M) = 1\nN\n∑\nm∈ˆM\nˆLcor(m,mgt), (26)\nwhere mgt is the ground truth NOCS coordinate from Mand\nm is the predicted NOCS coordinate from ˆM. Therefore, the\ncorrespondence loss is computed as the average correspon-\ndence loss over all the NOCS coordinates.\n3) Regularization Losses: To penalize large deformations\nand to constrain the correspondence matrix Ato be sparse,\nwe impose two regularization losses on each point di in the\ndeformation ﬁeld and each raw Ai of the correspondence\nmatrix.\nLdef = 1\nNc\n∑\ndi∈Ddef\n||di||2, (27)\nLspar = 1\nN\n∑\ni\n∑\nj\n−Ai,j log Ai,j. (28)\nIn summary, the overall loss function is a weighted sum of\nall four loss functions:\nL= λ1Lcd + λ2Lcor + λ3Ldef + λ4Lspar. (29)\nIV. E XPERIMENTS\nIn this section, we conduct extensive experiments on two\nstate-of-the-art benchmark datasets (i.e., CAMERA25 [18] and\nREAL275 [18]) to evaluate the performance of the presented\nmethod and to compare our results with those of the baseline\napproach [17] and other state-of-the-art methods. We also\nperform a comprehensive ablation analysis (18 experiments\nin total) to verify the prospective advantages of the proposed\nPixelformer and Pointformer of our framework and to compare\neach of them with the latest state-of-the-art transformer-based\nappearance [22] or geometric [24] representation generators.\nFurthermore, we show the visualization results of the pose\nestimation results and the 3D object models reconstructed by\nour network, which qualitatively demonstrate the effectiveness\nof our approach.\nA. Datasets\nThe CAMERA25 and REAL275 datasets were both com-\npiled by the creators of NOCS [18], with six different cat-\negories: bottle, bowl, camera, can, laptop and mug. Specif-\nically, the CAMERA25 dataset was generated by rendering\nand compositing synthetic object instances from ShapeNet-\nCore [37] under different views. The training set covers 1,085\nobject instances, while the evaluation set covers 184 different\ninstances. In total, it contains 300K composite RGB-D images,\nwhere 25K are set aside for evaluation. The REAL275 [18]\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 8\nTABLE III\nCATEGORY -WISE RESULTS OF OUR NETWORK ON THE CAMERA25 AND REAL275 DATASETS UNDER DIFFERENT EVALUATION METRICS .\nDataset Category IoU50 IoU75 5° 2cm 5° 5cm 10° 2cm 10° 5cm 10° 10cm\nCAMERA25\nBottle 0.9242 0.8524 0.7357 0.8962 0.7591 0.9436 0.9760\nBowl 0.9668 0.9563 0.9396 0.9430 0.9727 0.9801 0.9810\nCamera 0.9114 0.8228 0.4942 0.5018 0.7109 0.7374 0.7383\nCan 0.9181 0.9108 0.9508 0.9552 0.9632 0.9741 0.9760\nLaptop 0.9576 0.8619 0.7131 0.7801 0.7790 0.8897 0.9316\nMug 0.9297 0.9048 0.5252 0.5258 0.7527 0.7538 0.7539\nAverage 0.9346 0.8849 0.7265 0.7670 0.8229 0.8798 0.8928\nDataset Category IoU50 IoU75 5° 2cm 5° 5cm 10° 2cm 10° 5cm 10° 10cm\nREAL275\nBottle 0.5766 0.5005 0.5799 0.6318 0.7969 0.8703 0.9452\nBowl 0.9999 0.9992 0.7874 0.8186 0.9548 0.9914 0.9914\nCamera 0.8709 0.1917 0.0000 0.0000 0.0014 0.0019 0.0019\nCan 0.7146 0.6996 0.5350 0.5624 0.8573 0.9551 0.9555\nLaptop 0.8334 0.6170 0.3383 0.4461 0.6163 0.9217 0.9361\nMug 0.9878 0.8577 0.0490 0.0524 0.3166 0.3333 0.3333\nAverage 0.8306 0.6443 0.3816 0.4186 0.5906 0.6789 0.6989\n0\n25\n50\n75\n100\nPercent\n0\n20\n40\n60\n80\n100\nAverage Precision\n3D IoU\n0\n20\n40\n60\nDegree\n0\n20\n40\n60\n80\n100\nAverage Precision\nRotation\n0\n5\n10\nCentimeter\n0\n20\n40\n60\n80\n100\nAverage Precision\nTranslation\nbottle\nbowl\ncamera\ncan\nlaptop\nmug\nbottle\nbowl\ncamera\ncan\nlaptop\nmug\nFig. 5. Category-level comparison with the baseline method: SPD [17] on the CAMERA25 dataset. The solid lines denote the results of our 6D-ViT, and the\ndashed lines denote the results of SPD [17]. We report the precision of different thresholds with the 3D IoU, rotation and translation errors.\n0\n25\n50\n75\n100\nPercent\n0\n20\n40\n60\n80\n100\nAverage Precision\n3D IoU\n0\n20\n40\n60\nDegree\n0\n20\n40\n60\n80\n100\nAverage Precision\nRotation\n0\n5\n10\nCentimeter\n0\n20\n40\n60\n80\n100\nAverage Precision\nTranslation\nbottle\nbowl\ncamera\ncan\nlaptop\nmug\nbottle\nbowl\ncamera\ncan\nlaptop\nmug\nFig. 6. Category-level comparison with SPD [17] on the REAL275 dataset. The solid lines denote the results of our 6D-ViT, and the dashed lines denote the\nresults of SPD [17]. We report the precision of different thresholds with the 3D IoU, rotation and translation errors.\ndataset is complementary to the CAMERA25 dataset. It in-\ncludes 4,300 real-world images of 7 scenes for training and\n2,750 real-world images of 6 scenes with 3 unseen instances\nper category for evaluation. Both the training set and testing\nset contain 18 real object instances spanning the 6 categories.\nB. Metrics\nFollowing [13]–[18], we independently evaluate the per-\nformance of our method in terms of 3D object detection\nand 6D object pose estimation and compare the results with\nthose of the state-of-the-art methods. For 3D object detection,\nwe report the average precision at different intersection over\nunion thresholds 3D X. For 6D object pose estimation, the\naverage precision is computed as n°mcm. We ignore the\nrotational error around the axis of symmetry for symmetrical\nobject categories (e.g. bottle, bowl, and can). In particular, we\nregard mugs as symmetrical objects when the handle is absent\nand asymmetric objects otherwise. In addition, the Chamfer\ndistance (Eq. (24)) is employed to evaluate the accuracy of\nshape reconstruction from single-view RGB-D images.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 9\nTABLE IV\nQUANTITATIVE COMPARISONS ON THE CAMERA25 AND REAL275 DATASETS . WE REPORT THE M AP w.r.t. DIFFERENT THRESHOLDS ON THE 3D I OU\nAND ROTATION AND TRANSLATION ERRORS . THE RESULTS OF THE COMPARISON METHODS ARE SUMMARIZED FROM THEIR ORIGINAL PAPERS .\nMethod CAMERA25 REAL275\n3D50 3D75 5°2cm 5°5cm 10°2cm 10°5cm 10°10cm 3D50 3D75 5°2cm 5°5cm 10°2cm 10°5cm 10°10cm\nNOCS [CVPR’19] [18] 83.9% 69.5% 32.3% 40.9% 48.2% 64.6% - 78.0% 30.1% 7.2% 10.0% 13.8% 25.2% 26.7%\nCASS [CVPR’20] [14] - - - - - - - 77.7% - - 23.8% - 58.0% 58.3%\nNOF [ECCV’20] [13] - - - - - - - 76.9% 30.1% 7.2% 9.8% 13.8% 24.0% 24.3%\nSPD [ECCV’20] [17] 93.2% 83.1% 54.3% 59.0% 73.3% 81.5% - 77.3% 53.2% 19.3% 21.4% 43.2% 54.1% -\nFS-Net [CVPR’21] [15] - - - - - - - 92.2% 63.5% - 28.2% - 60.8% 64.6%\nDualPoseNet [ICCV’21] [16] 92.4% 86.4% 64.7% 70.7% 77.2% 84.7% - 79.8% 62.2% 29.3% 35.9% 50.0% 66.8% -\n6D-ViT [Ours] 93.5% 88.5% 72.6% 76.7% 82.3% 88.0% 89.3% 83.1% 64.4% 38.2% 41.9% 59.1% 67.9% 69.9%\n0\n25\n50\n75\n100\nPercent\n0\n20\n40\n60\n80\n100\nAverage Precision\n3D IoU\n0\n20\n40\n60\nDegree\n0\n20\n40\n60\n80\n100\nAverage Precision\nRotation\n0\n5\n10\nCentimeter\n0\n20\n40\n60\n80\n100\nAverage Precision\nTranslation\n6D-ViT(Ours)\nSPD(ECCV2020)\nNOCS(CVPR2019)\nFig. 7. The average precision of all categories on the CAMERA25 dataset. We present different thresholds with the 3D IoU, rotation and translation errors.\nBottle\nBowl\nCamera\nCan\nLaptop\nMug\nBowl\nCamera\nCan\nLaptop\nMug\nBottle\nFig. 8. Visualization of 6D object poses on the CAMERA25 dataset. Green bounding boxes denote the ground truth. Red boxes denote our estimations. Our\nresults match the ground truth well in terms of both pose and size.\nC. Implementation Details\nWe use the PyTorch [38] framework to implement our\nmethod, and the state-of-the-art framework SPD [17] is de-\nveloped as the baseline. All the experiments are conducted\non a PC with an i9-10900K 3.70 GHz CPU and two GTX\n2080Ti GPUs. The RGB images are cropped and resized to\n256×256. The number of points in the instance point clouds\nand categorical shape priors is set as 1,024. We use Adam [39]\nto optimize our network, where the initial learning rate is set\nas 1e-4, and we halve it every 20 epochs with a weight decay\nof 1e-6. The maximum number of epochs is 100. For the\nhyperparameters of the loss function, we follow SPD [17] and\nset λ1 = 5.0, λ2 = 1.0, λ3 = 1.0, and λ4 = 1e−4.\nD. Evaluation of the 6D Object Pose Estimation\nWe evaluate the performance of our 6D object pose estima-\ntion network, 6D-ViT, on two widely used benchmark datasets\n(i.e., CAMERA25 and REAL275) and compare our results\nwith those of the baseline method, SPD [17], and ﬁve other\nstate-of-the-art methods, including NOCS [18], CASS [14],\nNOF [13], FS-Net [15], and DualPoseNet [16].\n1) Comparison with the Baseline: To verify the effective-\nness of our method, we report the category-speciﬁc precision\nof our 6D-ViT on the CAMERA25 and REAL275 datasets\nunder different thresholds with the 3D IoU and rotation\nand translation errors and compare all results with those of\nSPD [17]. The comparisons can be found in Fig. 5 and Fig. 6,\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 10\n0\n25\n50\n75\n100\nPercent\n0\n20\n40\n60\n80\n100\nAverage Precision\n3D IoU\n0\n20\n40\n60\nDegree\n0\n20\n40\n60\n80\n100\nAverage Precision\nRotation\n0\n5\n10\nCentimeter\n0\n20\n40\n60\n80\n100\nAverage Precision\nTranslation\n6D-ViT(Ours)\nSPD(ECCV2020)\nNOCS(CVPR2019)\nNOF(ECCV2020)\nFig. 9. The average precision of all categories for the REAL275 dataset. We present different thresholds with the 3D IoU, rotation and translation errors.\nBottle\nBowl\nCamera\nCan\nLaptop\nMug\nBowl\nCamera\nCan\nLaptop\nMug\nBottle\nFig. 10. Visualization of the 6D object poses for the REAL275 dataset. Green bounding boxes denote the ground truth. Red boxes denote our estimations.\nOur results match the ground truth well in terms of both pose and size.\nrespectively. The results show that for all evaluation metrics,\nour method achieves remarkably better performance than the\nbaseline method in each category of the two datasets. In\naddition, we provide detailed category-speciﬁc results under\nseveral widely used thresholds to more intuitively show our\nresults. The results are provided in TABLE III.\n2) Comparison with the State-of-the-Art Algorithms: To\nfurther validate the competitiveness of our method, we com-\npare the average precision of 6D-ViT with that of NOCS [18],\nCASS [14], NOF [13], SPD [17], FS-Net [15], and Dual-\nPoseNet [16] on both the CAMERA25 and REAL275 datasets.\nThe results are reported in TABLE IV.\na) CAMERA25: As shown on the left of TABLE IV, our\n6D-ViT is superior to all existing methods on the CAMERA25\ndataset. Speciﬁcally, for the 3D object detection metrics 3D 50\nand 3D75, our 6D-ViT outperforms the previous best method,\nDualPoseNet [16], by 1.1% and 2.1%, respectively. In terms of\nthe 6D pose metrics 5°2cm, 5°5cm, 10°2cm and 10°10cm, 6D-\nViT has advantages of 7.9%, 6.0%, 5.1%, and 3.3%, respec-\ntively, performing signiﬁcantly better than DualPoseNet [16].\nFurthermore, 6D-ViT achieves an average precision of 89.3%\nunder the 10°10cm metric. We show more detailed quantitative\ncomparisons in Fig. 7, and our qualitative results on the\nCAMERA25 dataset are shown in Fig. 8.\nb) REAL275: As shown on the right of TABLE IV, our\nproposed 6D-ViT is superior to the existing methods under all\nevaluation metrics except for 3D 50, which is a rather rough\nmetric. Speciﬁcally, for 3D object detection, FS-Net [15] is\nthe current best method in terms of the 3D 50 metric, and our\n6D-ViT achieves the second best average precision, 83.1%,\nwhich is 9.1% lower than that of FS-Net [15]. However, for\nthe more difﬁcult 3D 75 metric, the performance of our 6D-\nViT is 0.9% higher than that of FS-Net [15]. For the 6D pose\nevaluation metrics 5°2cm, 5°5cm, 10°2cm and 10°10cm, our\n6D-ViT signiﬁcantly outperforms the previous best method,\nDualPoseNet [16], by 8.9%, 6.0%, 9.1%, and 1.1%, respec-\ntively. In addition, 6D-ViT achieves an average accuracy of\n69.9% under the 10°10cm metric, which is 5.3% higher than\nthe best result previously reported.\nWe believe that the performance gap between our method\nand FS-Net [15] in 3D object detection occurs because FS-\nNet [15] utilizes YOLOv3 [40] to crop object instances\nfrom RGB-D images, while our method, along with other\nmethods, employs Mask-RCNN [29] instead. More speciﬁ-\ncally, YOLOv3 has a higher success rate for coarse instance\ndetection, while Mask R-CNN yields more accurate instance\nlocalization. Therefore, FS-Net [15] performs better than our\n6D-ViT in terms of the 3D50 metric due to the effectiveness of\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 11\nTABLE V\nEVALUATION OF THE 3D MODEL RECONSTRUCTION USING THE CD METRIC (×10−3).\nMethod CAMERA REAL275\nBottle Bowl Camera Can Laptop Mug Average Bottle Bowl Camera Can Laptop Mug Average\nBaseline [17] 1.81 1.63 4.02 0.97 1.98 1.42 1.97 3.44 1.21 8.89 1.56 2.91 1.02 3.17\nOurs 1.38 1.18 2.86 0.90 1.24 1.17 1.46 2.37 1.14 6.06 1.60 1.39 1.06 2.27\nTABLE VI\nTHE ALTERNATIVE INSTANCE APPEARANCE FEATURE EXTRACTORS AND\nINSTANCE GEOMETRIC FEATURE EXTRACTORS USED TO VERIFY THE\nEFFICACY OF INDIVIDUAL COMPONENTS OF OUR NETWORK .\nAppearance PSP [17] PVT [22] PIF (Ours)\nGeometry MLPs [17] PT [24] POF (Ours)\nobject detection, while our 6D-ViT outperforms FS-Net [15]\nin terms of the harder 3D 75 metric, which validates the ad-\nvantage of our instance representation learning framework. We\nshow more detailed quantitative comparisons on the REAL275\ndataset in Fig. 9, and our qualitative results are provided in\nFig. 10.\nIn summary, our proposed instance representation learning\nnetwork achieves the best performance in both 6D object pose\nestimation and 3D object detection of the existing category-\nlevel pose estimation networks. This excellent performance\nfurther reﬂects the high competitiveness of our 6D-ViT.\nE. Evaluation of the Model Reconstruction\nWe calculate the Chamfer distance (as deﬁned in Eq. (24))\nbetween our reconstructed object models ˆP and the ground\ntruth object models P in the canonical space to evaluate\nthe quality of the model reconstruction. The results and\ncomparisons with SPD [17] are reported in TABLE V. It is\nobserved that the 3D models reconstructed by our 6D-ViT\nobtain average CD metrics of 1.46 on the CAMERA25 dataset\nand 2.27 on the REAL275 dataset, compared to the 1.97 and\n3.17, respectively, of the baseline. Better CD metrics indicate\nthat the proposed instance representation learning framework\nimproves the quality of the 3D model reconstruction. We also\nshow a qualitative comparison between the 3D models recon-\nstructed by our network and SPD [17] for the CAMERA25\ndataset in Fig. 11 and for the REAL275 dataset in Fig. 12.\nFrom both the quantitative and qualitative perspectives, our\n6D-ViT can steadily improve the reconstruction accuracy of\nthe baseline [17] over all categories on the CAMERA25\ndataset and four out of six categories on the REAL275 dataset\n(the remaining two categories also achieve comparable perfor-\nmance). These analyses reveal that the instance representation\nlearning framework we propose can not only improve the\nperformance of 6D object pose estimation but also facilitate\nthe reconstruction of canonical 3D object models.\nF . Ablation Analysis\nWe conduct 18 ablation studies on the CAMERA25 and\nREAL275 datasets to investigate the efﬁciency of the in-\ndividual components proposed in 6D-ViT. Speciﬁcally, we\ncompare the proposed Pixelformer and Pointformer of our\nnetwork with the latest state-of-the-art transformer-based ap-\npearance [22] or geometric [24] feature generators as well\nas the feature extractors utilized in the baseline method [17].\nTABLE VI shows different alternatives for the appearance and\ngeometric feature generators. From the table, “PSP”, “MLP”,\n“PVT”, “PT”, “PIF”, and “POF” refer to “PSPNet in the\nbaseline [17]”, “linear projection layers in the baseline [17]”,\n“pyramid vision transformer [22]”, “point transformer [24]”,\n“Pixelformer in this work”, and “Pointformer in this work”,\nrespectively. There are nine combinations in total, and the\ncorresponding comparisons are discussed below.\n1) Evaluation of Pixelformer: To verify the effectiveness\nof our proposed Pixelformer (PIF), we compare it with two\nalternative RGB encoders: (1) PSPNet (PSP) with ResNet-\n18 [31] as the backbone, which is consistent with the baseline\nmethod [17], and (2) PVT from [22], which like our PIF is a\nmultiscale transformer structure designed for dense prediction\ntasks such as object detection and semantic segmentation. Note\nthat there are a series of PVT models with different scales and\nthat we employ PVT-tiny in the experiments since it shares the\nparameter settings listed in TABLE I with ours.\nAs shown in TABLE VII, we set the geometric feature\ngenerators as MLPs in the baseline [17], the PT in [24] or\nour proposed POF and randomly select a backbone from the\nabove encoders as the appearance feature extractor. The results\nare reported in TABLE VII. As shown in the table, comparing\n1 , 2 and 8 ; 5 , 3 and 6 ; or 4 , 7 and 9 ; we observe that\nregardless of whether the point cloud branch utilizes MLPs,\nthe PT or our proposed POF, using our proposed PIF in the\nRGB branch achieves the best performance. This occurs on\nboth datasets, which demonstrates the effectiveness of our\nPixelformer. In addition, in all comparison experiments, our\ncomplete model ( 9 ) is the best.\n2) Evaluation of Pointformer: Similarly, to verify the ef-\nfectiveness of our proposed Pointformer (POF), we compare\nit with two alternative point cloud encoders: (1) MLPs with\nlinear projection layers, which is consistent with the base-\nline [17], and (2) the PT in [24], which is a multiscale trans-\nformer structure designed for point cloud processing. There are\nmany variants of the PT, and we use the semantic segmentation\nstructure, which demonstrates impressive performance gains in\npoint cloud processing.\nAs shown in TABLE VII, we set the appearance feature\ngenerators as PSP [17], PVT [22] or our proposed PIF and\nrandomly select a backbone from the above encoders as\nthe geometric feature extractors. The results are reported in\nTABLE VII. As shown in the table, comparing 1 , 5 and 7 ;\n2 , 3 and 4 ; or 8 , 6 and 9 ; we observe that regardless of\nwhether the RGB branch utilizes PSP, PVT or our proposed\nPIF, using our proposed POF in the point cloud branch\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 12\nTABLE VII\nABLATION STUDIES ON THE CAMERA25 AND REAL275 DATASETS . “PSP”, “MLP”, “PVT”, “PT”, “PIF”, AND “POF” REFER TO “PSPN ET IN THE\nBASELINE [17]”, “ LINEAR PROJECTION LAYERS IN THE BASELINE [17]”, “ PYRAMID VISION TRANSFORMER [22]”, “ POINT TRANSFORMER [24]”,\n“PIXELFORMER IN THIS WORK ”, AND “POINTFORMER IN THIS WORK ”, RESPECTIVELY .\nMethod CAMERA25 REAL275\n3D50 3D75 5°2cm 5°5cm 10°2cm 10°5cm 10°10cm 3D50 3D75 5°2cm 5°5cm 10°2cm 10°5cm 10°10cm\n1⃝ PSP [17] / MLPs [17] 93.2% 83.1% 54.3% 59.0% 73.3% 81.5% - 77.3% 53.2% 19.3% 21.4% 43.2% 54.1% -\n2⃝ PVT [22] / MLPs [17] 91.2% 77.9% 43.3% 47.0% 62.7% 71.2% 72.3% 69.6% 41.8% 15.4% 16.5% 39.5% 44.1% 46.0%\n3⃝ PVT [22] / PT [24] 91.4% 84.2% 60.0% 64.2% 75.5% 82.2% 83.2% 80.2% 56.8% 27.8% 31.4% 48.8% 60.5% 62.3%\n4⃝ PVT [22] / POF (Ours) 91.1% 83.9% 63.6% 67.6% 77.0% 82.7% 84.1% 76.2% 53.4% 32.1% 35.5% 51.7% 61.3% 63.3%\n5⃝ PSP [17] / PT [24] 93.3% 87.6% 60.9% 65.6% 77.9% 85.4% 86.6% 79.5% 58.9% 29.0% 32.1% 52.8% 61.1% 62.9%\n6⃝ PIF (Ours) / PT [24] 92.6% 85.5% 65.3% 69.4% 78.2% 84.9% 86.4% 81.4% 67.0% 35.7% 38.9% 58.9% 67.7% 69.6%\n7⃝ PSP [17] / POF (Ours) 93.4% 88.0% 69.5% 73.8% 81.1% 87.1% 88.6% 78.4% 61.4% 36.1% 39.1% 55.9% 65.1% 67.1%\n8⃝ PIF (Ours) / MLPs [17] 93.6% 87.9% 63.6% 67.8% 79.7% 86.5% 87.5% 82.4% 61.6% 27.8% 30.1% 52.4% 63.6% 65.6%\n9⃝ PIF (Ours) / POF (Ours) 93.5% 88.5% 72.6% 76.7% 82.3% 88.0% 89.3% 83.1% 64.4% 38.2% 41.9% 59.1% 67.9% 69.9%\nBowl\nBottle\nCamera\nCan\nLaptop\nMug\n6\nD\n-\nViT\nSPD\nBowl\nBottle\nCamera\nCan\nLaptop\nMug\n6\nD\n-\nViT\nSPD\nFig. 11. Visualization of 3D object models reconstructed by our 6D-ViT and SPD [17] on the CAMERA25 dataset. The red points represent our results, the\nblue points represent the SPD results, and the green points are the ground truth. We align all the results into the same pose.\nBowl\nBottle\nCamera\nCan\nLaptop\nMug\n6\nD\n-\nViT\nSPD\nBowl\nBottle\nCamera\nCan\nLaptop\nMug\n6\nD\n-\nViT\nSPD\nFig. 12. Visualization of 3D object models reconstructed by our 6D-ViT and SPD [17] on the REAL275 dataset. The red points represent our results, the\nblue points represent the SPD results, and the green points are the ground truth. We align all the visualization results into the same pose.\nachieves the best performance. This occurs on both datasets,\nwhich demonstrates the effectiveness of our Pixelformer. In\naddition, in all comparison experiments, our complete model\n( 9 ) is the best.\nV. C ONCLUSION\nThis paper presentes a novel instance representation learning\nframework for category-level 6D object pose estimation. To\nachieve this goal, two parallel transformer-based encoder-\ndecoder networks are designed to learn the long-range depen-\ndent instance appearance and geometric characteristics from\nRGB images and point clouds. Then, the obtained instance\nfeatures are fused with categorical shape features to gener-\nate joint and dense instance representations. Finally, the 6D\nobject pose is obtained from post-alignment by solving the\nUmeyama algorithm. We conduct quantitative and qualitative\nevaluations on two public benchmark datasets to compare the\npresented method with the state-of-the-art methods. The results\ndemonstrate that the proposed method achieves state-of-the-art\nperformance on both datasets and performs signiﬁcantly better\nthan the existing methods. We also implement sufﬁcient abla-\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 13\ntion experiments to prove the effectiveness of each component\nof the proposed network. In the future, we will focus on how\nto more efﬁciently explore the potential correlations between\ninstance features from different sources and apply our instance\nrepresentation learning framework to the instance-level pose\nestimation problem.\nREFERENCES\n[1] S. Hinterstoisser, V . Lepetit, S. Ilic, S. Holzer, G. Bradski, K. Konolige,\nand N. Navab, “Model based training, detection and pose estimation of\ntexture-less 3d objects in heavily cluttered scenes,” in Asian conference\non computer vision. Springer, 2012, pp. 548–562.\n[2] W. Kehl, F. Milletari, F. Tombari, S. Ilic, and N. Navab, “Deep learning\nof local rgb-d patches for 3d object detection and 6d pose estimation,” in\nEuropean conference on computer vision. Springer, 2016, pp. 205–220.\n[3] W. Kehl, F. Manhardt, F. Tombari, S. Ilic, and N. Navab, “Ssd-6d:\nMaking rgb-based 3d detection and 6d pose estimation great again,” in\nProceedings of the IEEE International Conference on Computer Vision,\n2017, pp. 1521–1529.\n[4] A. Krull, E. Brachmann, F. Michel, M. Ying Yang, S. Gumhold, and\nC. Rother, “Learning analysis-by-synthesis for 6d pose estimation in\nrgb-d images,” in Proceedings of the IEEE international conference on\ncomputer vision, 2015, pp. 954–962.\n[5] C. Li, J. Bai, and G. D. Hager, “A uniﬁed framework for multi-view\nmulti-class object pose estimation,” in Proceedings of the European\nConference on Computer Vision (ECCV), 2018, pp. 254–269.\n[6] F. Michel, A. Kirillov, E. Brachmann, A. Krull, S. Gumhold, B. Savchyn-\nskyy, and C. Rother, “Global hypothesis generation for 6d object pose\nestimation,” in Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, 2017, pp. 462–471.\n[7] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, “Pvnet: Pixel-wise\nvoting network for 6dof pose estimation,” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, 2019, pp.\n4561–4570.\n[8] M. Sundermeyer, Z.-C. Marton, M. Durner, M. Brucker, and R. Triebel,\n“Implicit 3d orientation learning for 6d object detection from rgb\nimages,” in Proceedings of the European Conference on Computer\nVision (ECCV), 2018, pp. 699–715.\n[9] B. Tekin, S. N. Sinha, and P. Fua, “Real-time seamless single shot\n6d object pose prediction,” in Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 2018, pp. 292–301.\n[10] C. Wang, D. Xu, Y . Zhu, R. Mart ´ın-Mart´ın, C. Lu, L. Fei-Fei, and\nS. Savarese, “Densefusion: 6d object pose estimation by iterative dense\nfusion,” in Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, 2019, pp. 3343–3352.\n[11] Y . Xiang, T. Schmidt, V . Narayanan, and D. Fox, “Posecnn: A convolu-\ntional neural network for 6d object pose estimation in cluttered scenes,”\narXiv preprint arXiv:1711.00199, 2017.\n[12] S. Zakharov, I. Shugurov, and S. Ilic, “Dpod: Dense 6d pose object\ndetector in rgb images,” arXiv preprint arXiv:1902.11020, vol. 1, p. 2,\n2019.\n[13] X. Chen, Z. Dong, J. Song, A. Geiger, and O. Hilliges, “Category level\nobject pose estimation via neural analysis-by-synthesis,” in European\nConference on Computer Vision. Springer, 2020, pp. 139–156.\n[14] D. Chen, J. Li, Z. Wang, and K. Xu, “Learning canonical shape space\nfor category-level 6d object pose and size estimation,” in Proceedings of\nthe IEEE/CVF conference on computer vision and pattern recognition,\n2020, pp. 11 973–11 982.\n[15] W. Chen, X. Jia, H. J. Chang, J. Duan, L. Shen, and A. Leonardis,\n“Fs-net: Fast shape-based network for category-level 6d object pose\nestimation with decoupled rotation mechanism,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2021, pp. 1581–1590.\n[16] J. Lin, Z. Wei, Z. Li, S. Xu, K. Jia, and Y . Li, “Dualposenet: Category-\nlevel 6d object pose and size estimation using dual pose network with\nreﬁned learning of pose consistency,” arXiv preprint arXiv:2103.06526,\n2021.\n[17] M. Tian, M. H. Ang, and G. H. Lee, “Shape prior deformation for\ncategorical 6d object pose and size estimation,” in European Conference\non Computer Vision. Springer, 2020, pp. 530–546.\n[18] H. Wang, S. Sridhar, J. Huang, J. Valentin, S. Song, and L. J. Guibas,\n“Normalized object coordinate space for category-level 6d object pose\nand size estimation,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2019, pp. 2642–2651.\n[19] C. Sahin, G. Garcia-Hernando, J. Sock, and T.-K. Kim, “Instance-and\ncategory-level 6d object pose estimation,” inRGB-D Image Analysis and\nProcessing. Springer, 2019, pp. 243–265.\n[20] Y . Liu, G. Sun, Y . Qiu, L. Zhang, A. Chhatkuli, and L. Van Gool,\n“Transformer in convolutional neural networks,” arXiv preprint\narXiv:2106.03180, 2021.\n[21] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances\nin neural information processing systems, 2017, pp. 5998–6008.\n[22] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, and\nL. Shao, “Pyramid vision transformer: A versatile backbone for dense\nprediction without convolutions,” arXiv preprint arXiv:2102.12122 ,\n2021.\n[23] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo,\n“Segformer: Simple and efﬁcient design for semantic segmentation with\ntransformers,” arXiv preprint arXiv:2105.15203, 2021.\n[24] H. Zhao, L. Jiang, J. Jia, P. Torr, and V . Koltun, “Point transformer,” in\nICCV, 2021.\n[25] M. Zhu, K. G. Derpanis, Y . Yang, S. Brahmbhatt, M. Zhang, C. Phillips,\nM. Lecce, and K. Daniilidis, “Single image 3d object detection and\npose estimation for grasping,” in 2014 IEEE International Conference\non Robotics and Automation (ICRA). IEEE, 2014, pp. 3936–3943.\n[26] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y . Fu, and A. C.\nBerg, “Ssd: Single shot multibox detector,” in European conference on\ncomputer vision. Springer, 2016, pp. 21–37.\n[27] P. J. Besl and N. D. McKay, “Method for registration of 3-d shapes,”\nin Sensor fusion IV: control paradigms and data structures, vol. 1611.\nInternational Society for Optics and Photonics, 1992, pp. 586–606.\n[28] L. Zou, Z. Huang, F. Wang, Z. Yang, and G. Wang, “Cma: Cross-modal\nattention for 6d object pose estimation,” Computers & Graphics, vol. 97,\npp. 139–147, 2021.\n[29] K. He, G. Gkioxari, P. Doll ´ar, and R. Girshick, “Mask r-cnn,” in\nProceedings of the IEEE international conference on computer vision,\n2017, pp. 2961–2969.\n[30] S. Umeyama, “Least-squares estimation of transformation parameters\nbetween two point patterns,” IEEE Transactions on Pattern Analysis &\nMachine Intelligence, vol. 13, no. 04, pp. 376–380, 1991.\n[31] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in Proceedings of the IEEE conference on computer vision\nand pattern recognition, 2016, pp. 770–778.\n[32] D. Hendrycks and K. Gimpel, “Gaussian error linear units (gelus),” arXiv\npreprint arXiv:1606.08415, 2016.\n[33] D. Zhang, F. He, Z. Tu, L. Zou, and Y . Chen, “Pointwise geometric and\nsemantic learning network on 3d point clouds,” Integrated Computer-\nAided Engineering, vol. 27, no. 1, pp. 57–75, 2020.\n[34] M.-H. Guo, J.-X. Cai, Z.-N. Liu, T.-J. Mu, R. R. Martin, and S.-M.\nHu, “Pct: Point cloud transformer,” Computational Visual Media, vol. 7,\nno. 2, pp. 187–199, 2021.\n[35] X. Pan, Z. Xia, S. Song, L. E. Li, and G. Huang, “3d object detection\nwith pointformer,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2021, pp. 7463–7472.\n[36] M. A. Fischler and R. C. Bolles, “Random sample consensus: a paradigm\nfor model ﬁtting with applications to image analysis and automated\ncartography,”Communications of the ACM, vol. 24, no. 6, pp. 381–395,\n1981.\n[37] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li,\nS. Savarese, M. Savva, S. Song, H. Su et al., “Shapenet: An information-\nrich 3d model repository,” arXiv preprint arXiv:1512.03012, 2015.\n[38] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\nT. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., “Pytorch: An\nimperative style, high-performance deep learning library,” Advances in\nneural information processing systems, vol. 32, pp. 8026–8037, 2019.\n[39] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\narXiv preprint arXiv:1412.6980, 2014.\n[40] A. Farhadi and J. Redmon, “Yolov3: An incremental improvement,” in\nComputer Vision and Pattern Recognition, 2018, pp. 1804–02 767.",
  "topic": "Artificial intelligence",
  "concepts": [
    {
      "name": "Artificial intelligence",
      "score": 0.6845612525939941
    },
    {
      "name": "Pose",
      "score": 0.6456264853477478
    },
    {
      "name": "Computer science",
      "score": 0.6013288497924805
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5379498600959778
    },
    {
      "name": "Computer vision",
      "score": 0.5183306932449341
    },
    {
      "name": "Transformer",
      "score": 0.5172017812728882
    },
    {
      "name": "3D pose estimation",
      "score": 0.43635135889053345
    },
    {
      "name": "Representation (politics)",
      "score": 0.4328545331954956
    },
    {
      "name": "Cognitive neuroscience of visual object recognition",
      "score": 0.42848023772239685
    },
    {
      "name": "Object (grammar)",
      "score": 0.3455893397331238
    },
    {
      "name": "Engineering",
      "score": 0.0742374062538147
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I126520041",
      "name": "University of Science and Technology of China",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I20231570",
      "name": "Peking University",
      "country": "CN"
    }
  ],
  "cited_by": 51
}