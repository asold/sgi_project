{
  "title": "Improving Robustness of Language Models from a Geometry-aware Perspective",
  "url": "https://openalex.org/W4225159663",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5081758219",
      "name": "Bin Zhu",
      "affiliations": [
        "Guangzhou University"
      ]
    },
    {
      "id": "https://openalex.org/A5070856186",
      "name": "Zhaoquan Gu",
      "affiliations": [
        "Guangzhou University",
        "Peng Cheng Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A5100350793",
      "name": "Le Wang",
      "affiliations": [
        "Guangzhou University",
        "Peng Cheng Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A5101758654",
      "name": "Jinyin Chen",
      "affiliations": [
        "Zhejiang University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5016704080",
      "name": "Qi Xuan",
      "affiliations": [
        "Zhejiang University of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2799420921",
    "https://openalex.org/W3008189420",
    "https://openalex.org/W1673923490",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963083752",
    "https://openalex.org/W2996344901",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2963859254",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W2799194071",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3176648901",
    "https://openalex.org/W2609368435",
    "https://openalex.org/W2949128310",
    "https://openalex.org/W2963834268",
    "https://openalex.org/W3105604018",
    "https://openalex.org/W1945616565",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3104423855",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4293846201",
    "https://openalex.org/W3007685714",
    "https://openalex.org/W4287864733",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W3177190797",
    "https://openalex.org/W3118565077",
    "https://openalex.org/W3035441470",
    "https://openalex.org/W3174318528",
    "https://openalex.org/W2970078867",
    "https://openalex.org/W3035736465",
    "https://openalex.org/W3101449015",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W2913266441",
    "https://openalex.org/W3197868468",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2996851481",
    "https://openalex.org/W4300996741",
    "https://openalex.org/W3092642435",
    "https://openalex.org/W3035164976"
  ],
  "abstract": "Recent studies have found that removing the norm-bounded projection and increasing search steps in adversarial training can significantly improve robustness. However, we observe that a too large number of search steps can hurt accuracy. We aim to obtain strong robustness efficiently using fewer steps. Through a toy experiment, we find that perturbing the clean data to the decision boundary but not crossing it does not degrade the test accuracy. Inspired by this, we propose friendly adversarial data augmentation (FADA) to generate friendly adversarial data. On top of FADA, we propose geometry-aware adversarial training (GAT) to perform adversarial training on friendly adversarial data so that we can save a large number of search steps. Comprehensive experiments across two widely used datasets and three pre-trained language models demonstrate that GAT can obtain stronger robustness via fewer steps. In addition, we provide extensive empirical results and in-depth analyses on robustness to facilitate future studies.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2022, pages 3115 - 3125\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nImproving Robustness of Language Models from a Geometry-aware\nPerspective\nBin Zhu1 , Zhaoquan Gu1,2∗, Le Wang1,2, Jinyin Chen3, Qi Xuan3\n1 Cyberspace Institute of Advanced Technology (CIAT), Guangzhou University, Guangzhou 510006, China\n2 Institute of Cyberspace Platform, Peng Cheng Laboratory, Shenzhen 999077, China\n3 Institute of Cyberspace Security, Zhejiang University of Technology, Hangzhou 310023, China\nzhubin@e.gzhu.edu.cn, {zqgu,wangle}@gzhu.edu.cn\n{chenjinyin,xuanqi}@zjut.edu.cn\nAbstract\nRecent studies have found that removing the\nnorm-bounded projection and increasing search\nsteps in adversarial training can significantly\nimprove robustness. However, we observe that\na too large number of search steps can hurt\naccuracy. We aim to obtain strong robustness\nefficiently using fewer steps. Through a toy\nexperiment, we find that perturbing the clean\ndata to the decision boundary but not crossing\nit does not degrade the test accuracy. Inspired\nby this, we propose friendly adversarial data\naugmentation (FADA) to generate friendly ad-\nversarial data. On top of FADA, we propose\ngeometry-aware adversarial training (GAT) to\nperform adversarial training on friendly adver-\nsarial data so that we can save a large number\nof search steps. Comprehensive experiments\nacross two widely used datasets and three pre-\ntrained language models demonstrate that GAT\ncan obtain stronger robustness via fewer steps.\nIn addition, we provide extensive empirical re-\nsults and in-depth analyses on robustness to\nfacilitate future studies.\n1 Introduction\nDeep neural networks (DNNs) have achieved great\nsuccess on many natural language processing\n(NLP) tasks (Kim, 2014; Vaswani et al., 2017; De-\nvlin et al., 2019). However, recent studies (Szegedy\net al., 2013; Goodfellow et al., 2015) have shown\nthat DNNs are vulnerable to crafted adversarial\nexamples . For instance, an attacker can mislead\nan online sentiment analysis system by making\nminor changes to the input sentences (Papernot\net al., 2016; Liang et al., 2017). It has raised\nconcerns among researchers about the security of\nDNN-based NLP systems. As a result, a grow-\ning number of studies are focusing on enhancing\nrobustness to defend against textual adversarial at-\ntacks (Jia et al., 2019; Ye et al., 2020; Jones et al.,\n2020; Zhu et al., 2020).\n∗Corresponding author\n0 500 1000 1500\niter\n0.2\n0.4\n0.6\n0.8\n1.0training vs test acc\nada training acc\nfada training acc\nada test acc\nfada test acc\nclean test acc\nFigure 1: The clean accuracy achieved with ADA,\nFADA, and the original training set. During training,\nboth ADA and FADA have close to 100% accuracy.\nHowever, ADA only achieves ∼15% accuracy during\ntesting while FADA maintains the same test accuracy\nwith the original training set. This indicates that train-\ning data which crosses the decision boundary hurts the\naccuracy significantly.\nExisting adversarial defense methods fall into\ntwo categories: empirical and certified defenses.\nEmpirical defenses include gradient-based adver-\nsarial training (AT) and discrete adversarial data\naugmentation (ADA). Certified defenses provide a\nprovable guaranteed robustness boundary for NLP\nmodels. This work focuses on empirical defenses.\nThere was a common belief that gradient-based\nAT methods in NLP was ineffective compared with\nADA in defending against textual adversarial at-\ntacks (Li and Qiu, 2021; Si et al., 2021). Li et al.\n(2021) find that removing the norm-bounded pro-\njection and increasing the number of search steps\nin adversarial training can significantly improve ro-\nbustness. Nonetheless, we observe that increasing\nthe number of search steps further does not signifi-\ncantly improve robustness but hurts accuracy.\nWe give a possible explanation from a geometry-\naware perspective. Removing the norm-bounded\nprojection enlarge the search space. Appropriately\n3115\nclean examples\ndecision boundary\n friendly adversarial \n       examples FADA\nAT\nFigure 2: Illustration of GAT. Our GAT can save many\nsearch steps since friendly adversarial examples are\nlocated near the decision boundary.\nincreasing the number of search steps brings the\nadversarial data closer to the decision boundary. In\nthis case, the model learns a robust decision bound-\nary. Further increasing the number of search steps\ncan make the adversarial data cross the decision\nboundary too far, hindering the training of natural\ndata and hurting natural accuracy.\nTo verify our hypothesis, we train a base model\nusing adversarial data, which are generated by ad-\nversarial word substitution (AWS) on the SST-2\n(Socher et al., 2013) dataset. We report its train-\ning accuracy (“ada training acc”) on adversarial\ndata and test accuracy (“ada test acc”) on the clean\ntest set in Figure 1. Although achieving nearly\n100% training accuracy, its test accuracy is only\nabout 15%, which implicates the adversarial data\nmake the test performance degraded. Then we\ntrain another base model, whose training data is\nmore “friendly”. We just recover their last mod-\nified words to return to the correct class, namely\nfriendly adversarial data augmentation (FADA). It\nmeans that only one word is different in each sen-\ntence. Surprisingly, it achieves a high test accuracy\nof ∼93%.\nThis preliminary inspired us to address two ex-\nisting problems:\n• The number of search steps is always large,\nwhich is computationally inefficient.\n• A too large number of steps leads to de-\ngraded test performance.\nGeometrically speaking, the friendly adversar-\nial data are close to the ideal decision boundary.\nWe can address the above two issues in one fell\nswoop if we perform gradient-based adversarial\ntraining on these friendly adversarial data. It is\nlike we start one step before the end, allowing us\nto obtain strong robustness through a tiny number\nof search steps. We name it geometry-aware ad-\nversarial training (GAT). Figure 2 illustrates our\nproposed GAT.\nIn addition, the friendly adversarial data only\nneed to be generated once per dataset. It can be\nreused, so it is computationally efficient. It can\nalso be updated for every iteration or epoch but\ncomputationally expensive.\nOur contributions are summarized as follows:\n1) We propose FADA to generate friendly ad-\nversarial data which are close to the decision\nboundary (but not crossing it).\n2) We propose GAT, a geometry-aware adver-\nsarial training method that adds FADA to the\ntraining set and performs gradient-based ad-\nversarial training.\n3) GAT is computationally efficient, and it out-\nperforms state-of-the-art baselines even if us-\ning the simplest FGM. We further provide ex-\ntensive ablation studies and in-depth analyses\non GAT, contributing to a better understanding\nof robustness.\n2 Related Work\n2.1 Standard Adversarial Training\nLet fθ(x) be our neural network, L(fθ(x), y) be\nthe loss function (e.g., cross entropy), wherex ∈ X\nis the input data and y ∈ Y is the true label. The\nlearning objective of standard adversarial training\nis\nmin\nθ\nE(X,Y )∼D\n\u0014\nmax\n∥δ∥≤ϵ\nL(fθ(X + δ), y)\n\u0015\n, (1)\nwhere D is the data distribution, δ is the minor per-\nturbation, ϵ is the allowed perturbation size. To op-\ntimize the intractable min-max problem, we search\nfor the optimal δ to maximize the inner loss and\nthen minimize the outer loss w.r.t the parameters θ,\nstep by step.\nThe gradient g of the inner loss w.r.t the inputx is\nused to find the optimal perturbationδ. Goodfellow\net al. (2015) proposed fast gradient sign method\n(FGSM) to obtain δ by one step:\nδ = ϵ · sgn(g), (2)\n3116\nwhere sgn(·) is the signum function. Madry et al.\n(2018) proposed projected gradient descent (PGD)\nto solve the inner maximization as follows:\nδ(t+1) = Π α · g(t)/∥g(t)∥, ∀t ≥ 0, (3)\nwhere α >0 is the step size (i.e., adversarial learn-\ning rate), Π is the projection function that projects\nthe perturbation onto the ϵ-norm ball. Convention-\nally PGD stops after a predefined number of search\nsteps K, namely PGD-K. In addition, TRADES\n(Zhang et al., 2019), MART (Wang et al., 2020)\nand FAT (Zhang et al., 2020) are also effective\nadversarial training methods for boosting model\nrobustness.\nRegarding FAT, the authors propose to stop ad-\nversarial training in a predefined number of steps\nafter crossing the decision boundary, which is a\nlittle different from our definition of “friendly”.\n2.2 Adversarial Training in NLP\nGradient-based adversarial training has signifi-\ncantly improved model robustness in vision, while\nresearchers find it helps generalization in NLP. Miy-\nato et al. (2017) find that adversarial and virtual ad-\nversarial training have good regularization perfor-\nmance. Sato et al. (2018) propose an interpretable\nadversarial training method that generates reason-\nable adversarial texts in the embedding space and\nenhance models’ performance. Zhu et al. (2020)\ndevelop FreeLB to improve natural language un-\nderstanding.\nThere is also a lot of work focused on robustness.\nWang et al. (2021) improve model robustness from\nan information theoretic perspective. Dong et al.\n(2021) use a convex hull to capture and defense\nagainst adversarial word substitutions. Zhou et al.\n(2021) train robust models by augmenting train-\ning data using Dirichlet Neighborhood Ensemble\n(DNE).\nBesides, adversarial data augmentation is an-\nother effective approach to improve robustness\n(Ebrahimi et al., 2018; Li et al., 2019; Ren et al.,\n2019; Jin et al., 2019; Zang et al., 2020; Li et al.,\n2020; Garg and Ramakrishnan, 2020; Si et al.,\n2021). However, it only works when the augmenta-\ntion happens to be generated by the same attacking\nmethod and often hurts accuracy.\nIt is worth noting that recent empirical results\nhave shown that previous gradient-based adversar-\nial training methods have little effect on defending\nagainst textual adversarial attacks (Li et al., 2021;\nAlgorithm 1 Friendly Adversarial Data Augmenta-\ntion (FADA)\nInput: The original text x, ground truth label\nytrue, base model fθ, adversarial word sub-\nstitution function AW S(·)\nOutput: The friendly adversarial example xf\n1: Initialization:\n2: xf ← x\n3: the last modified word w∗ ← None\n4: the last modified index i∗ ← 0\n5: xadv, w∗, i∗ = AW S(x, ytrue, fθ)\n6: if w∗ = None then\n7: return xf\n8: end if\n9: Replace wi∗ in xadv with w∗\n10: xf ← xadv\n11: return xf\nSi et al., 2021). The authors benchmark existing\ndefense methods and conclude that gradient-based\nAT can achieve the strongest robustness by remov-\ning the norm bounded projection and increasing\nthe search steps.\n3 Methodology\n3.1 Friendly Adversarial Data Augmentation\nFor a sentence x ∈ X with a length of n, it can\nbe denoted as x = w1w2...wi...wn−1wn, where wi\nis the i-th word in x. Its adversarial counterpart\nxadv can be denoted as w′\n1w′\n2...w′\ni...w′\nn−1w′\nn. In\nthis work, xadv is generated by adversarial word\nsubstitution, so xadv has the same length with x.\nConventional adversarial data augmentation gen-\nerates adversarial data fooling the victim model\nand mixes them with the original training set. As\nwe claim in section 1, these adversarial data can\nhurt test performance. An interesting and critical\nquestion is when it becomes detrimental to test\naccuracy.\nOne straightforward idea is to recover all the\nxadv to x word by word and evaluate their impact\non test accuracy. We train models only with these\nadversarial data and test models with the original\ntest set. We are excited that the test accuracy imme-\ndiately returns to the normal level when we recover\nthe last modified word. We denote these data with\nonly one word recovered as xf . Geometrically, the\nonly difference between xadv and xf is whether\nthey have crossed the decision boundary.\nTo conclude, when the adversarial data cross the\n3117\nAlgorithm 2 Ideal Geometry-aware Adversarial\nTraining (GAT)\nInput: Our base networkfθ, cross entropy lossLCE , training\nset D = {xi, yi}n\ni=1, number of epochs T, batch size m,\nnumber of batches M\nOutput: robust network fθ\n1: for epoch = 1 to T do\n2: for batch = 1 to M do\n3: Sample a mini-batch b = {(xi, yi)}m\ni=1\n4: for all xi in b do\n5: Generate friendly adversarial example xf\ni via\nAlgorithm 1\n6: Apply an adversarial training method (e.g.,\nFreeLB++) on both xi and xf\ni to obtain their\nadversarial counterpart exi and exf\ni\n7: end for\n8: Update fθ via ∇xLCE (fθ(exi), yi) and\n∇xLCE (fθ(exf\ni ), yi)\n9: end for\n10: end for\ndecision boundary, they become incredibly harm-\nful to the test performance. We name all the xf\nas friendly adversarial examples (FAEs) because\nthey improve model robustness without hurting ac-\ncuracy. Similarly, we name the generation of FAEs\nas friendly adversarial data augmentation (FADA).\nWe show our proposed FADA in Algorithm 1.\n3.2 Geometry-aware Adversarial Training\n3.2.1 Seeking for the optimal δ\nRecall the inner maximization issue of the learning\nobjective in Eq. (1). Take PGD-K as an instance.\nIt divides the search for the optimal perturbation\nδ into K search steps, and each step requires a\nbackpropagation (BP), which is computationally\nexpensive.\nWe notice that random initialization of δ0 is\nwidely used in adversarial training, where δ0 is\nalways confined to a ϵ-ball centered at x. However,\nwe initialize the clean data via discrete adversar-\nial word substitution in NLP. It is similar to data\naugmentation (DA), with the difference that we per-\nturb clean data in the direction towards the decision\nboundary, whereas the direction of data augmenta-\ntion is random.\nBy doing so, we decompose the δ into two parts,\nwhich can be obtained by word substitution and\ngradient-based adversarial training, respectively.\nWe denote them as δl and δs. Therefore, the inner\nmaximization can be reformulated as\nmax\n∥δl+δs∥≤ϵ\nL(fθ(X + δl + δs), y). (4)\nWe aim to find the maximum δl that helps im-\nprove robustness without hurting accuracy. As we\nclaim in Section 3.1, FADA generates friendly ad-\nversarial data which are close to the decision bound-\nary. Furthermore, the model trained with these\nfriendly adversarial data keeps the same test accu-\nracy as the original training set (Figure 1). There-\nfore we find the maximum δl which is harmless to\nthe test accuracy through FADA.\nDenote Xf as the friendly adversarial data gen-\nerated by FADA, Eq. (4) can be reformulated as\nmax\n∥δs∥≤ϵ\nL(fθ(Xf + δs), y). (5)\nThe tiny δs can be obtained by some gradient-based\nadversarial training methods (e.g., FreeLB++ (Li\net al., 2021)) in few search steps. As a result, a large\nnumber of search steps are saved to accelerate ad-\nversarial training. We show our proposed geometry-\naware adversarial training in Algorithm 2.\n3.2.2 Final Learning Objective\nIt is computationally expensive to update friendly\nadversarial data for every mini-batch. In practice,\nwe generate static augmentation ( Xf ,Y) for the\ntraining dataset (X,Y) and find it works well with\nGAT. The static augmentation (Xf ,Y) is reusable.\nTherefore, GAT is computationally efficient.\nThrough such a tradeoff, our final objective func-\ntion can be formulated as\nL =LCE (X, Y, θ)\n+ LCE ( eX, Y, θ) + LCE ( eXf , Y, θ),\n(6)\nwhere LCE is the cross entropy loss, eX and eXf\nare generated from X and Xf using gradient-based\nadversarial training methods, respectively.\n4 Experiments\n4.1 Datasets\nWe conduct experiments on the SST-2 (Socher\net al., 2013) and IMDb (Maas et al., 2011) datasets\nwhich are widely used for textual adversarial learn-\ning. Statistical details are shown in Table 1. We\nuse the GLUE (Wang et al., 2019) version of the\nSST-2 dataset whose test labels are unavailable. So\nwe report its accuracy on the develop set in our\nexperiments.\nDataset # train # dev / test avg. length\nSST-2 67349 872 17\nIMDb 25000 25000 201\nTable 1: Summary of the two datasets.\n3118\nSST-2 Clean % TextFooler TextBugger BAE\nRA % ASR % # Query RA % ASR % # Query RA % ASR % # Query\nBERTbase 92.4 32.8 64.1 72.8 38.5 57.8 44.3 39.8 56.5 64.0\nADA 92.2 46.7 48.7 79.4 42.0 53.9 47.0 41.2 54.8 64.0\nASCC 87.2 32.0 63.3 71.6 27.8 68.2 42.5 41.7 52.1 63.0\nDNE 86.6 26.5 69.6 69.0 23.4 73.1 40.2 44.2 49.3 65.8\nInfoBERT 92.2 41.7 54.8 74.9 45.2 51.1 45.8 45.4 50.8 65.6\nTA V AT 92.2 40.4 56.3 74.3 42.3 54.2 45.7 42.7 53.8 64.2\nFreeLB 93.1 42.7 53.7 75.9 48.2 47.7 45.7 46.7 49.3 67.5\nFreeLB++10 93.3 41.9 54.8 75.8 46.1 50.3 45.9 44.2 52.4 65.3\nFreeLB++30 93.4 45.6 50.6 78.1 47.4 48.8 45.7 42.9 53.6 66.0\nFreeLB++50 92.0 45.5 50.4 77.2 47.4 48.4 45.3 44.6 51.4 67.5\nGATFGM (ours) 92.8 45.8 49.8 78.5 49.0 46.3 47.0 45.5 50.1 64.9\nGATFreeLB ++10 (ours) 93.2 49.5 46.3 80.6 52.4 43.2 47.9 48.3 46.9 68.9\nGATFreeLB ++30 (ours) 92.7 52.5 42.2 82.3 53.8 40.9 47.5 46.1 50.0 65.8\nTable 2: Main defense results on the SST-2 dataset, including the test accuracy on the clean test set (Clean %), the\nrobust accuracy under adversarial attacks (RA %), the attack success rate (ASR %), and the average number of\nqueries requiring by the attacker (# Query).\n4.2 Attacking Methods\nFollow Li et al. (2021), we adopt TextFooler (Jin\net al., 2019), TextBugger (Li et al., 2019) and\nBAE (Garg and Ramakrishnan, 2020) as attack-\ners. TextFooler and BAE are word-level attacks\nand TextBugger is a multi-level attacking method.\nWe also impose restrictions on these attacks for a\nfair comparison, including:\n1. The maximum percentage of perturbed words\npmax\n2. The minimum semantic similarity εmin be-\ntween the original input and the generated\nadversarial example\n3. The maximum size Ksyn of one word’s syn-\nonym set\nSince the average sentence length of IMDb and\nSST-2 are different, pmax is set to 0.1 and 0.15,\nrespectively; εmin is set to 0.84; and Ksyn is set to\n50. All settings are referenced from previous work.\n4.3 Adversarial Training Baselines\nWe use BERTbase (Devlin et al., 2019) as the base\nmodel to evaluate the impact of the following vari-\nants of adversarial training on accuracy and robust-\nness and provide a comprehensive comparison with\nour proposed GAT.\n• Adversarial Data Augmentation\n• ASCC (Dong et al., 2021)\n• DNE (Zhou et al., 2021)\n• InfoBERT (Wang et al., 2021)\n• TA V AT (Li and Qiu, 2021)\n• FreeLB (Zhu et al., 2020)\n• FreeLB++ (Li et al., 2021)\nASCC and DNE adopt a convex hull during train-\ning. InfoBERT improves robustness using mutual\ninformation. TA V AT establishes a token-aware\nrobust training framework. FreeLB++ removes\nthe norm bounded projection and increases search\nsteps.\nWe only compare GAT with adversarial training-\nbased defense methods and leave comparisons with\nother defense methods (e.g., certified defenses) for\nfuture work.\n4.4 Implementation Details\nWe implement ASCC, DNE, InfoBERT, and\nTA V AT models based on TextDefender (Li et al.,\n2021). We implement FGM, FreeLB, FreeLB++,\nand our GAT based on HuggingFace Transform-\ners.1 We implement ADA and FADA based on\nTextAttack (Morris et al., 2020).2 All the adversar-\nial hyper-parameters settings are following their\noriginal papers. All the models are trained on\ntwo GeForce RTX 2080 GPUs and eight Tesla T4\nGPUs.\nRegarding the training settings and hyper-\nparameters, the optimizer is AdamW (Loshchilov\nand Hutter, 2019); the learning rate is 2e−5; the\nnumber of epochs is 10; the batch size is 64 for\n1https://huggingface.co/transformers\n2https://github.com/QData/TextAttack\n3119\nIMDb Clean % TextFooler TextBugger BAE\nRA % ASR % # Query RA % ASR % # Query RA % ASR % # Query\nBERTbase 91.2 30.7 66.4 714.4 38.9 57.4 490.3 36.0 60.6 613.6\nADA 91.4 34.6 61.7 804.8 40.5 55.2 538.8 37.0 59.1 693.4\nASCC 86.4 22.2 73.9 595.9 27.2 68.0 415.8 34.7 59.1 642.2\nDNE 86.1 14.9 82.2 520.2 17.4 79.3 336.9 35.4 57.8 630.4\nInfoBERT 91.9 33.0 63.9 694.1 40.4 55.8 469.9 37.3 59.2 619.6\nTA V AT 91.5 37.8 58.9 1082.6 48.8 46.9 695.5 41.2 55.2 896.7\nFreeLB 91.3 34.6 61.9 782.0 42.9 52.7 542.7 37.6 58.5 646.7\nFreeLB++-10 92.1 39.5 56.8 817.9 46.4 49.3 516.5 41.2 55.0 682.3\nFreeLB++-30 92.3 49.8 45.6 992.9 56.0 38.8 600.1 48.3 47.2 788.2\nFreeLB++-50 92.3 50.2 45.3 1117.7 56.5 38.5 649.8 48.2 47.5 861.3\nGATFGM (ours) 91.8 58.3 36.0 1004.3 60.4 33.7 556.1 54.6 40.1 747.4\nGATFreeLB ++10 (ours) 92.0 50.7 44.7 1093.8 54.7 40.4 648.9 50.7 44.7 908.5\nGATFreeLB ++30 (ours) 92.4 59.0 35.7 1629.4 62.2 32.2 914.8 54.4 40.7 1213.6\nTable 3: Main defense results on the IMDb dataset.\nSST-2 and 24 for IMDb; the maximum sentence\nlength kept for all the models is 40 for SST-2 and\n200 for IMDb.\n4.5 Main Results\nOur proposed GAT can easily combine with other\nadversarial training methods. In our experiments,\nwe combine GAT with FGM (GAT F GM) and\nFreeLB++ (GATF reeLB++), respectively. We aim\nto evaluate if GAT can bring improvements to the\nsimplest (FGM) and the most effective (FreeLB++)\nAT methods.\nWe summarize the main defense results on the\nSST-2 dataset in Table 2. When GAT works\nwith the simplest adversarial training method,\nFGM, the resulting robustness improvement ex-\nceeds FreeLB++ 50. The effectiveness and effi-\nciency of GAT allow us to obtain strong robustness\nwhile saving many search steps. Further combining\nFreeLB++ on GAT can obtain stronger robustness\nand outperform all other methods.\nRegarding the accuracy, FreeLB++30 obtains the\nhighest 93.4%. GAT also significantly improves\naccuracy.\nIn addition, ADA is effective in improving ro-\nbustness but hurts accuracy. It is not surprising\nthat ASCC and DNE suffer from significant perfor-\nmance losses. However, there is no improvement\nin robustness and even weaker robustness under\nTextFooler and TextBugger attacks than the other\nmethods.\nTable 3 shows the defense results on the IMDb\ndataset. The defense performances are generally\nconsistent with that on the SST-2 dataset. It is\nA WS AT method Clean % RA % #Query\nNone None 92.4 38.5 44.3\nNone FGM 92.5 39.6 44.7\nNone FreeLB++30 93.4 47.4 45.7\nADA None 92.2 42.0 47.0\nADA FGM 91.3 42.7 46.6\nADA FreeLB++30 90.9 51.5 47.5\nFADA None 92.7 44.4 45.8\nFADA FGM 92.8 49.0 47.0\nFADA FreeLB++30 92.7 53.8 47.5\nTable 4: Ablation studies on the SST-2 dataset. The\nattacking method is TextBugger. We only reportRA %\nand #Query due to the space limit. “AWS” means ad-\nversarial word substitution methods.\nworth noting that GATF GMachieved an extremely\nhigh RA % with a medium #Query, which needs\nfurther exploration.\n5 Discussions\nWe further explore other factors that affect robust-\nness and provide comprehensive empirical results.\n5.1 Ablation Studies\nWe conduct ablation studies on the SST-2 dataset\nto assess the impact of each component of GAT.\nAs shown in Table 4, “FADA” consistently\noutperforms “ADA” and “None” with differ-\nent adversarial training methods. Furthermore,\n“FADA&FGM” achieve a higher RA% than\n“None&FreeLB++30”, which implies that “FADA”\ncan obtain strong robustness in one adversarial\nsearch step. “ADA” also helps improve robust-\nness. However, as the number of search steps in-\n3120\n0 10 20 30 40 50\n#steps\n42\n44\n46\n48\n50\n52\n54robust accuracy\n80.0\n82.5\n85.0\n87.5\n90.0\n92.5\n95.0\n97.5\n100.0\nclean accuracy\nTextFooler\nTextBugger\nBAE\nClean\n(a)\n0.010 0.012 0.014 0.016 0.018 0.020\nstep size\n48\n49\n50\n51\n52\n53\n54robust accuracy\n80.0\n82.5\n85.0\n87.5\n90.0\n92.5\n95.0\n97.5\n100.0\nclean accuracy\nTextFooler\nTextBugger\nBAE\nClean (b)\n0 2 4 6 8\nepochs\n44\n46\n48\n50\n52\n54robust accuracy\nTextFooler\nTextBugger\nBAE (c)\nFigure 3: (a) Robust and clean accuracy with different search steps. (b) Robust and clean accuracy with different\nstep sizes. (c) Robust accuracy gradually increases on the SST-2 dataset during training. The adversarial training\nmethod is GATFreeLB ++30. Zoom in for a better view.\nSST-2 clean % PSO FastGA\nRA % #Query RA % #Query\nBERTbase 92.4 23.9 322.0 39.2 234.4\nADA 92.2 31.4 348.6 43.2 268.4\nASCC 87.2 29.2 359.4 40.5 233.2\nDNE 86.6 17.3 266.2 43.9 250.1\nInfoBERT 92.2 29.0 335.7 45.3 256.0\nTA V AT 92.2 25.7 316.2 42.0 258.7\nFreeLB 93.1 27.8 325.6 42.9 267.9\nFreeLB++50 92.0 38.4 368.6 49.2 258.9\nGATFGM 92.8 29.9 341.0 46.7 275.1\nGATFreeLB ++10 93.2 34.5 351.3 51.0 289.5\nGATFreeLB ++30 92.8 39.7 359.2 53.7 323.9\nTable 5: The defense results of different AT methods\nagainst two combinatorial optimization attacks. We\nremove ASR % due to the space limit.\ncreases, so does the hurt it does to Clean %. On\nthe contrary, “FADA” does not harmClean % but\nimproves it, implying its friendliness.\n5.2 Results with Other Attacks\nWe have shown that GAT brings significant im-\nprovement in robustness against three greedy-based\nattacks. We investigate whether GAT is effective\nunder combinatorial optimization attacks, such as\nPSO (Zang et al., 2020) and FastGA (Jia et al.,\n2019).\nWe can see from Table 5 that GATF reeLB++30\nobtain the highest RA % against the two attacks\nand GATF reeLB++10 has the highest clean ac-\ncuracy. The results demonstrate that our pro-\nposed GAT consistently outperforms other defenses\nagainst combinatorial optimization attacks.\n5.3 Results with More Steps\nAs we claim in Section 1, the accuracy should de-\ngrade with a large number of search steps. But\nwhat happens for robustness?\nWe aim to see if RA % can be further improved.\nFigure 3(a) shows that the RA % gradually in-\ncreases against TextFooler and TextBugger attacks.\nHowever, RA % decreases against BAE with steps\nmore than 30, which needs more investigation. As\nthe steps increase, the growth rate of RA % de-\ncreases, and the Clean % decreases. We conclude\nthat a reasonable number of steps will be good for\nboth RA % and Clean %. It is unnecessary to\nsearch for too many steps since robustness grows\nvery slowly in the late adversarial training period\nwhile accuracy drops.\n5.4 Impact of Step Size\nA large step size (i.e., adversarial learning rate) will\ncause performance degradation for conventional\nadversarial training. Nevertheless, what impact\ndoes it have on robustness? We explore the impact\nof different step sizes on robustness and accuracy.\nAs shown in Figure 3(b), the clean test accuracy\nslightly drops as the step size increases. The robust\naccuracy under TextFooler attack increases, while\nthe robust accuracy under Textbugger and BAE\nattacks decrease. Overall, the impact of step size\non robustness needs further study.\n5.5 Impact of Training Epochs\nIshida et al. (2020) have shown that preventing fur-\nther reduction of the training loss when reaching a\nsmall value and keeping training can help general-\nization. In adversarial training, it is naturally hard\nto achieve zero training loss due to the insufficient\ncapacity of the model (Zhang et al., 2021).\nTherefore, we investigate whether more training\niterations result in stronger robustness in adver-\nsarial training. We report the RA % achieved by\nGATF reeLB++30 at each epoch in Figure 3(c). We\nobserve that the RA % tends to improve slowly,\n3121\nSST-2 Clean % TextFooler TextBugger BAE\nRA % ASR % # Query RA % ASR % # Query RA % ASR % # Query\nRoBERTabase 93.0 38.8 58.0 74.5 41.4 55.2 45.5 40.3 56.4 63.6\nGATFGM 91.4 47.6 47.7 78.6 49.8 45.3 46.3 42.7 53.2 65.3\nGATFreeLB ++30 93.2 52.1 43.7 95.5 54.2 41.3 55.8 47.0 49.1 76.9\nTable 6: Defense results on RoBERTa model on the SST-2 dataset.\nSST-2 Clean % TextFooler TextBugger BAE\nRA % ASR % # Query RA % ASR % # Query RA % ASR % # Query\nDeBERTabase 94.6 53.7 43.4 79.5 55.1 42.0 48.7 49.8 47.5 66.8\nGATFGM 94.5 54.6 42.1 82.6 57.7 38.8 50.0 48.9 48.2 66.7\nGATFreeLB ++30 94.7 60.4 35.7 83.4 62.0 33.9 51.2 52.2 44.4 69.9\nTable 7: Defense results on DeBERTa model on the SST-2 dataset.\nimplying that more training iterations result in\nstronger model robustness using GAT.\n5.6 Results with Other Models\nWe show that GAT can work on more advanced\nmodels. We choose RoBERTabase (Liu et al., 2019)\nand DeBERTabase (He et al., 2021), two improved\nversions of BERT, as the base models. As shown\nin Table 6 and Table 7, GAT slightly improve ro-\nbustness of RoBERTa and DeBERTa models.\n5.7 Limitations\nWe discuss the limitations of this work as follows.\n• As we clarify in Section 3.2.2, instead of dynami-\ncally generating friendly adversarial data in train-\ning, we choose to pre-generate static augmenta-\ntion. We do this for efficiency, as dynamically\ngenerating discrete sentences in training is com-\nputationally expensive. Although it still signifi-\ncantly improves robustness in our experiments,\nsuch a tradeoff may lead to failure because the\ndecision boundary changes continuously during\ntraining.\n• GAT performs adversarial training on friendly\nadversarial data. It may help if we consider the\ndecision boundaries when performing gradient-\nbased adversarial training—for example, stop-\nping early when the adversarial data crosses the\ndecision boundary. We consider this as one of\nthe directions for future work.\n6 Conclusion\nIn this paper, we study how to improve robustness\nfrom a geometry-aware perspective. We first pro-\npose FADA to generate friendly adversarial data\nthat are close to the decision boundary. Then we\ncombine gradient-based adversarial training meth-\nods on FADA to save a large number of search\nsteps, termed geometry-aware adversarial training\n(GAT). GAT can efficiently achieve state-of-the-art\ndefense performance without hurting test accuracy.\nWe conduct extensive experiments to give in-\ndepth analysis, and we hope this work can provide\nhelpful insights on robustness in NLP.\nAcknowledgments\nThe authors would like to thank the anonymous re-\nviewers for their helpful suggestions and comments.\nThis work is supported in part by the National Nat-\nural Science Foundation of China under Grant No.\n61902082 and 61976064, and the Guangdong Key\nR&D Program of China 2019B010136003.\nReferences\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186.\nXinshuai Dong, Anh Tuan Luu, Rongrong Ji, and Hong\nLiu. 2021. Towards robustness against natural lan-\nguage word substitutions. In 9th International Con-\n3122\nference on Learning Representations, ICLR 2021, Vir-\ntual Event, Austria, May 3-7, 2021. OpenReview.net.\nJavid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing\nDou. 2018. HotFlip: White-box adversarial exam-\nples for text classification. In Proceedings of the 56th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pages 31–36.\nSiddhant Garg and Goutham Ramakrishnan. 2020.\nBAE: bert-based adversarial examples for text classi-\nfication. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP 2020, Online, November 16-20, 2020, pages\n6174–6181. Association for Computational Linguis-\ntics.\nIan J. Goodfellow, Jonathon Shlens, and Christian\nSzegedy. 2015. Explaining and harnessing adver-\nsarial examples. In 3rd International Conference on\nLearning Representations, ICLR 2015, San Diego,\nCA, USA, May 7-9, 2015, Conference Track Proceed-\nings.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2021. Deberta: decoding-enhanced\nbert with disentangled attention. In 9th International\nConference on Learning Representations, ICLR 2021,\nVirtual Event, Austria, May 3-7, 2021 . OpenRe-\nview.net.\nTakashi Ishida, Ikko Yamane, Tomoya Sakai, Gang Niu,\nand Masashi Sugiyama. 2020. Do we need zero\ntraining loss after achieving zero training error? In\nProceedings of the 37th International Conference\non Machine Learning, ICML 2020, 13-18 July 2020,\nVirtual Event, volume 119 ofProceedings of Machine\nLearning Research, pages 4604–4614. PMLR.\nRobin Jia, Aditi Raghunathan, Kerem Göksel, and Percy\nLiang. 2019. Certified robustness to adversarial word\nsubstitutions. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 4129–4142, Hong Kong, China. Association\nfor Computational Linguistics.\nDi Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter\nSzolovits. 2019. Is BERT Really Robust? A Strong\nBaseline for Natural Language Attack on Text Clas-\nsification and Entailment. arXiv e-prints , page\narXiv:1907.11932.\nErik Jones, Robin Jia, Aditi Raghunathan, and Percy\nLiang. 2020. Robust encodings: A framework for\ncombating adversarial typos. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 2752–2765, Online. Asso-\nciation for Computational Linguistics.\nYoon Kim. 2014. Convolutional neural networks for\nsentence classification. In Proceedings of the 2014\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 1746–1751.\nJinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting\nWang. 2019. Textbugger: Generating adversarial\ntext against real-world applications. In 26th Annual\nNetwork and Distributed System Security Symposium,\nNDSS 2019, San Diego, California, USA, February\n24-27, 2019. The Internet Society.\nLinyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue,\nand Xipeng Qiu. 2020. BERT-ATTACK: Adversarial\nattack against BERT using BERT. In Proceedings\nof the 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), pages 6193–\n6202.\nLinyang Li and Xipeng Qiu. 2021. Token-aware virtual\nadversarial training in natural language understand-\ning. In Thirty-Fifth AAAI Conference on Artificial\nIntelligence, AAAI 2021, Thirty-Third Conference\non Innovative Applications of Artificial Intelligence,\nIAAI 2021, The Eleventh Symposium on Educational\nAdvances in Artificial Intelligence, EAAI 2021, Vir-\ntual Event, February 2-9, 2021 , pages 8410–8418.\nAAAI Press.\nZongyi Li, Jianhan Xu, Jiehang Zeng, Linyang Li, Xiao-\nqing Zheng, Qi Zhang, Kai-Wei Chang, and Cho-Jui\nHsieh. 2021. Searching for an effective defender:\nBenchmarking defense against adversarial word sub-\nstitution. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3137–3147, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nBin Liang, Hongcheng Li, Miaoqiang Su, Pan Bian,\nXirong Li, and Wenchang Shi. 2017. Deep text clas-\nsification can be fooled. CoRR, abs/1704.08006.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In International Confer-\nence on Learning Representations.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan\nHuang, Andrew Y . Ng, and Christopher Potts. 2011.\nLearning word vectors for sentiment analysis. In Pro-\nceedings of ACL, pages 142–150, Portland, Oregon,\nUSA. Association for Computational Linguistics.\nAleksander Madry, Aleksandar Makelov, Ludwig\nSchmidt, Dimitris Tsipras, and Adrian Vladu. 2018.\nTowards deep learning models resistant to adversarial\nattacks. In International Conference on Learning\nRepresentations.\nTakeru Miyato, Andrew M. Dai, and Ian J. Goodfel-\nlow. 2017. Adversarial training methods for semi-\nsupervised text classification. In 5th International\nConference on Learning Representations, ICLR 2017,\nToulon, France, April 24-26, 2017, Conference Track\nProceedings. OpenReview.net.\n3123\nJohn Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby,\nDi Jin, and Yanjun Qi. 2020. TextAttack: A frame-\nwork for adversarial attacks, data augmentation, and\nadversarial training in NLP. In Proceedings of the\n2020 Conference on Empirical Methods in Natu-\nral Language Processing: System Demonstrations ,\npages 119–126, Online. Association for Computa-\ntional Linguistics.\nNicolas Papernot, Patrick D. McDaniel, Ananthram\nSwami, and Richard E. Harang. 2016. Crafting adver-\nsarial input sequences for recurrent neural networks.\nIn 2016 IEEE Military Communications Conference,\nMILCOM 2016, Baltimore, MD, USA, November 1-3,\n2016, pages 49–54. IEEE.\nShuhuai Ren, Yihe Deng, Kun He, and Wanxiang Che.\n2019. Generating natural language adversarial exam-\nples through probability weighted word saliency. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 1085–\n1097.\nMotoki Sato, Jun Suzuki, Hiroyuki Shindo, and Yuji\nMatsumoto. 2018. Interpretable adversarial perturba-\ntion in input embedding space for text. In Proceed-\nings of the Twenty-Seventh International Joint Con-\nference on Artificial Intelligence, IJCAI 2018, July\n13-19, 2018, Stockholm, Sweden, pages 4323–4330.\nijcai.org.\nChenglei Si, Zhengyan Zhang, Fanchao Qi, Zhiyuan\nLiu, Yasheng Wang, Qun Liu, and Maosong Sun.\n2021. Better robustness by more coverage: Ad-\nversarial and mixup data augmentation for robust\nfinetuning. In Findings of the Association for Com-\nputational Linguistics: ACL-IJCNLP 2021 , pages\n1569–1576, Online. Association for Computational\nLinguistics.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of EMNLP, pages 1631–1642, Seattle,\nWashington, USA. Association for Computational\nLinguistics.\nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever,\nJoan Bruna, Ian Erhan, Dumitru; Goodfellow, and\nRob Fergus. 2013. Intriguing properties of neural\nnetworks. arXiv preprint arXiv:1312.6199.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA, pages 5998–6008.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In 7th In-\nternational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nOpenReview.net.\nBoxin Wang, Shuohang Wang, Yu Cheng, Zhe Gan,\nRuoxi Jia, Bo Li, and Jingjing Liu. 2021. Infobert:\nImproving robustness of language models from an in-\nformation theoretic perspective. In 9th International\nConference on Learning Representations, ICLR 2021,\nVirtual Event, Austria, May 3-7, 2021 . OpenRe-\nview.net.\nYisen Wang, Difan Zou, Jinfeng Yi, James Bailey,\nXingjun Ma, and Quanquan Gu. 2020. Improving ad-\nversarial robustness requires revisiting misclassified\nexamples. In International Conference on Learning\nRepresentations.\nMao Ye, Chengyue Gong, and Qiang Liu. 2020.\nSAFER: A structure-free approach for certified ro-\nbustness to adversarial word substitutions. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 3465–\n3475, Online. Association for Computational Lin-\nguistics.\nYuan Zang, Fanchao Qi, Chenghao Yang, Zhiyuan Liu,\nMeng Zhang, Qun Liu, and Maosong Sun. 2020.\nWord-level textual adversarial attacking as combi-\nnatorial optimization. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 6066–6080.\nHongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing,\nLaurent El Ghaoui, and Michael Jordan. 2019. The-\noretically principled trade-off between robustness\nand accuracy. In Proceedings of the 36th Interna-\ntional Conference on Machine Learning, volume 97\nof Proceedings of Machine Learning Research, pages\n7472–7482. PMLR.\nJingfeng Zhang, Xilie Xu, Bo Han, Gang Niu, Lizhen\nCui, Masashi Sugiyama, and Mohan Kankanhalli.\n2020. Attacks which do not kill training make ad-\nversarial learning stronger. In Proceedings of the\n37th International Conference on Machine Learning,\nvolume 119 of Proceedings of Machine Learning\nResearch, pages 11278–11287. PMLR.\nJingfeng Zhang, Jianing Zhu, Gang Niu, Bo Han,\nMasashi Sugiyama, and Mohan Kankanhalli. 2021.\nGeometry-aware instance-reweighted adversarial\ntraining. In International Conference on Learning\nRepresentations.\nYi Zhou, Xiaoqing Zheng, Cho-Jui Hsieh, Kai-Wei\nChang, and Xuanjing Huang. 2021. Defense against\nsynonym substitution-based adversarial attacks via\ndirichlet neighborhood ensemble. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing,\nACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual\nEvent, August 1-6, 2021, pages 5482–5492. Associa-\ntion for Computational Linguistics.\n3124\nChen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Gold-\nstein, and Jingjing Liu. 2020. Freelb: Enhanced\nadversarial training for natural language understand-\ning. In 8th International Conference on Learning\nRepresentations, ICLR 2020, Addis Ababa, Ethiopia,\nApril 26-30, 2020. OpenReview.net.\n3125",
  "topic": "Adversarial system",
  "concepts": [
    {
      "name": "Adversarial system",
      "score": 0.8723090887069702
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.8630748987197876
    },
    {
      "name": "Computer science",
      "score": 0.6986315250396729
    },
    {
      "name": "Bounded function",
      "score": 0.5709904432296753
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5204520225524902
    },
    {
      "name": "Machine learning",
      "score": 0.47582605481147766
    },
    {
      "name": "Training set",
      "score": 0.4720291197299957
    },
    {
      "name": "Mathematical optimization",
      "score": 0.33234044909477234
    },
    {
      "name": "Theoretical computer science",
      "score": 0.3268016278743744
    },
    {
      "name": "Mathematics",
      "score": 0.19872954487800598
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I37987034",
      "name": "Guangzhou University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210136793",
      "name": "Peng Cheng Laboratory",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I55712492",
      "name": "Zhejiang University of Technology",
      "country": "CN"
    }
  ],
  "cited_by": 5
}