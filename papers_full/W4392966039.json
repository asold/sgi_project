{
  "title": "Deep reinforcement learning navigation via decision transformer in autonomous driving",
  "url": "https://openalex.org/W4392966039",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5053829722",
      "name": "Lun Ge",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A5101815043",
      "name": "Xiaoguang Zhou",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A5101547882",
      "name": "Yongqiang Li",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5113008983",
      "name": "Yongcong Wang",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3035700320",
    "https://openalex.org/W4285227658",
    "https://openalex.org/W2745868649",
    "https://openalex.org/W2990123902",
    "https://openalex.org/W6796289742",
    "https://openalex.org/W4309793872",
    "https://openalex.org/W6785148927",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W6798316476",
    "https://openalex.org/W2306644740",
    "https://openalex.org/W2904246096",
    "https://openalex.org/W6677939520",
    "https://openalex.org/W2291973609",
    "https://openalex.org/W6751659697",
    "https://openalex.org/W6804244202",
    "https://openalex.org/W2168359464",
    "https://openalex.org/W2968983352",
    "https://openalex.org/W3127561923",
    "https://openalex.org/W6752961934",
    "https://openalex.org/W6765215732",
    "https://openalex.org/W4298857966",
    "https://openalex.org/W2145339207",
    "https://openalex.org/W3211352205",
    "https://openalex.org/W1658008008",
    "https://openalex.org/W4285071848",
    "https://openalex.org/W2343568200",
    "https://openalex.org/W6768810269",
    "https://openalex.org/W6763509872",
    "https://openalex.org/W6684205842",
    "https://openalex.org/W3021208093",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W32403112",
    "https://openalex.org/W3080764280",
    "https://openalex.org/W3210290940",
    "https://openalex.org/W4225773281",
    "https://openalex.org/W6637133156",
    "https://openalex.org/W2944851425",
    "https://openalex.org/W2607713949",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4298023569",
    "https://openalex.org/W2121092017",
    "https://openalex.org/W3169291081",
    "https://openalex.org/W4287078276",
    "https://openalex.org/W2962854145",
    "https://openalex.org/W4287126489",
    "https://openalex.org/W2957120403",
    "https://openalex.org/W4301409532",
    "https://openalex.org/W3103677861"
  ],
  "abstract": "In real-world scenarios, making navigation decisions for autonomous driving involves a sequential set of steps. These judgments are made based on partial observations of the environment, while the underlying model of the environment remains unknown. A prevalent method for resolving such issues is reinforcement learning, in which the agent acquires knowledge through a succession of rewards in addition to fragmentary and noisy observations. This study introduces an algorithm named deep reinforcement learning navigation via decision transformer (DRLNDT) to address the challenge of enhancing the decision-making capabilities of autonomous vehicles operating in partially observable urban environments. The DRLNDT framework is built around the Soft Actor-Critic (SAC) algorithm. DRLNDT utilizes Transformer neural networks to effectively model the temporal dependencies in observations and actions. This approach aids in mitigating judgment errors that may arise due to sensor noise or occlusion within a given state. The process of extracting latent vectors from high-quality images involves the utilization of a variational autoencoder (VAE). This technique effectively reduces the dimensionality of the state space, resulting in enhanced training efficiency. The multimodal state space consists of vector states, including velocity and position, which the vehicle's intrinsic sensors can readily obtain. Additionally, latent vectors derived from high-quality images are incorporated to facilitate the Agent's assessment of the present trajectory. Experiments demonstrate that DRLNDT may achieve a superior optimal policy without prior knowledge of the environment, detailed maps, or routing assistance, surpassing the baseline technique and other policy methods that lack historical data.",
  "full_text": "TYPE Original Research\nPUBLISHED /one.tnum/nine.tnum March /two.tnum/zero.tnum/two.tnum/four.tnum\nDOI /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/three.tnum/eight.tnum/one.tnum/eight.tnum/nine.tnum\nOPEN ACCESS\nEDITED BY\nMehdi Benallegue,\nUMI/three.tnum/two.tnum/one.tnum/eight.tnum Joint Robotics Laboratory (JRL),\nJapan\nREVIEWED BY\nZiming Yan,\nNanyang Technological University, Singapore\nLong Cheng,\nWenzhou University, China\n*CORRESPONDENCE\nXiaoguang Zhou\nzxg_bupt@/one.tnum/two.tnum/six.tnum.com\nYongqiang Li\nliyongqiang@zhidaoauto.com\nRECEIVED /one.tnum/four.tnum November /two.tnum/zero.tnum/two.tnum/three.tnum\nACCEPTED /two.tnum/six.tnum February /two.tnum/zero.tnum/two.tnum/four.tnum\nPUBLISHED /one.tnum/nine.tnum March /two.tnum/zero.tnum/two.tnum/four.tnum\nCITATION\nGe L, Zhou X, Li Y and Wang Y (/two.tnum/zero.tnum/two.tnum/four.tnum) Deep\nreinforcement learning navigation via decision\ntransformer in autonomous driving.\nFront. Neurorobot./one.tnum/eight.tnum:/one.tnum/three.tnum/three.tnum/eight.tnum/one.tnum/eight.tnum/nine.tnum.\ndoi: /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/three.tnum/eight.tnum/one.tnum/eight.tnum/nine.tnum\nCOPYRIGHT\n© /two.tnum/zero.tnum/two.tnum/four.tnum Ge, Zhou, Li and Wang. This is an\nopen-access article distributed under the\nterms of the\nCreative Commons Attribution\nLicense (CC BY) . The use, distribution or\nreproduction in other forums is permitted,\nprovided the original author(s) and the\ncopyright owner(s) are credited and that the\noriginal publication in this journal is cited, in\naccordance with accepted academic practice.\nNo use, distribution or reproduction is\npermitted which does not comply with these\nterms.\nDeep reinforcement learning\nnavigation via decision\ntransformer in autonomous\ndriving\nLun Ge /one.tnum, Xiaoguang Zhou /one.tnum*, Yongqiang Li /two.tnum* and\nYongcong Wang/three.tnum\n/one.tnumSchool of Modern Post (School of Automation), Beijing University of Pos ts and Telecommunications,\nBeijing, China, /two.tnumMogo Auto Intelligence and Telematics Information Technology Co., Lt d, Beijing,\nChina, /three.tnumNeolix Technologies Co., Ltd, Beijing, China\nIn real-world scenarios, making navigation decisions for autono mous driving\ninvolves a sequential set of steps. These judgments are made bas ed on\npartial observations of the environment, while the underlyi ng model of the\nenvironment remains unknown. A prevalent method for resolvi ng such issues\nis reinforcement learning, in which the agent acquires knowledge t hrough a\nsuccession of rewards in addition to fragmentary and noisy observ ations. This\nstudy introduces an algorithm named deep reinforcement learni ng navigation\nvia decision transformer (DRLNDT) to address the challenge of e nhancing\nthe decision-making capabilities of autonomous vehicles oper ating in partially\nobservable urban environments. The DRLNDT framework is buil t around the\nSoft Actor-Critic (SAC) algorithm. DRLNDT utilizes Transform er neural networks\nto eﬀectively model the temporal dependencies in observation s and actions.\nThis approach aids in mitigating judgment errors that may arise due to sensor\nnoise or occlusion within a given state. The process of extracting l atent vectors\nfrom high-quality images involves the utilization of a vari ational autoencoder\n(VAE). This technique eﬀectively reduces the dimensionality of the state space,\nresulting in enhanced training eﬃciency. The multimodal state space consists\nof vector states, including velocity and position, which the vehi cle’s intrinsic\nsensors can readily obtain. Additionally, latent vectors der ived from high-quality\nimages are incorporated to facilitate the Agent’s assessment o f the present\ntrajectory. Experiments demonstrate that DRLNDT may achieve a s uperior\noptimal policy without prior knowledge of the environment, det ailed maps, or\nrouting assistance, surpassing the baseline technique and oth er policy methods\nthat lack historical data.\nKEYWORDS\nautonomous driving, deep reinforcement learning (DRL), Soft Ac tor-Critic (SAC),\nvariational autoencoder (VAE), Partially Observable Markov D ecision Processes\n(POMDPs), multimodal state space\n/one.tnum Introduction\nThe automobile industry has increasingly prioritized autonomous ( González et al.,\n2015) driving technology due to the ongoing advancements in science and technology. The\nimplementation of driverless vehicles heavily relies on integrating an autonomous driving\nnavigation system, a fundamental component. Analyzing environmental data enables\nautonomous driving navigation using numerous sensors and algorithms. The utilization\nof machine learning enables the application of learning-based techniques in making\nautonomous driving decisions. Imitation learning is often regarded as the prevailing\nFrontiers in Neurorobotics /zero.tnum/one.tnum frontiersin.org\nGe et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/three.tnum/eight.tnum/one.tnum/eight.tnum/nine.tnum\napproach when driving regulations are acquired automatically\nthrough the analysis of expert driving data. Nevertheless, imitation\nlearning is not without its limitations. Firstly, acquiring substantial\nquantities of authentic, contemporaneous driving data from\nproﬁcient experts is a prerequisite, a process that might incur\nsigniﬁcant costs and consume considerable time. Furthermore,\nthe limited learning capacity of the system restricts its ability\nto acquire driving skills beyond those displayed in the dataset.\nConsequently, this limitation may give rise to safety concerns as\nthe system may not possess the necessary knowledge to handle\nhazardous scenarios not encompassed within the dataset. Thirdly,\nit is improbable that the imitation learning strategy may surpass\nhuman performance, given that the human driving expert assumes\nthe role of a learning supervisor. Given these constraints, it is\nimperative to investigate alternative methodologies for decision-\nmaking in autonomous driving. One such method is reinforcement\nlearning, which automatically enhances and discovers new policies\nwithout manual design.\nIn autonomous driving navigation, reinforcement learning\n(\nKiran et al., 2021 ; Ye et al., 2021 ) can help vehicles learn optimal\nnavigation policies by interacting with the road environment.\nThrough continuous trial and error and reward mechanisms,\nreinforcement learning algorithms can enable vehicles to gradually\nlearn to deal with various complex traﬃc situations and road\nconditions. Establishing a suitable reward system is of utmost\nimportance in the context of reinforcement learning for self-\ndriving navigation (\nMorales et al., 2021 ). Reinforcement learning\nalgorithms can eﬀectively guide the vehicle to acquire appropriate\nbehavior by employing positive rewards, such as completing the\nnavigation job, or negative rewards, such as contravening traﬃc\nregulations. The ﬁeld of autonomous driving is advancing quickly,\nwith reinforcement learning showing promise in enabling agents to\nlearn how to drive without relying on expert data or manual design.\nThis method entails the agent learning to make decisions in various\nscenarios, including hazardous ones, potentially surpassing the\nskills of even the most experienced human drivers. By harnessing\nthe power of reinforcement learning, autonomous driving systems\ncan become more sophisticated and better equipped to handle the\nintricacies of real-world driving situations.\nNevertheless, implementing reinforcement learning in\nautonomous driving navigation has certain hurdles. Training\n(\nKaelbling et al., 1998 ) eﬀective policies is a formidable task\nprimarily because of the intricacies associated with the inﬁnite-\ndimensional space. Furthermore, complicated and uncertain\nroad environments further compound the challenges in making\nnavigation decisions. The substantial quantity of necessary\nexploration impedes the practical implementation of large action\nspaces. This circumstance will result in unsatisfactory outcomes\nof reinforcement learning-driven policy learning for complex\nreal-world tasks. The occlusion and noise experienced by the\nsensors hinder the Agent’s capacity to perceive the actual status\nof the surroundings accurately. The Agent cannot reach an\noptimal conclusion given the existing situation, which is untrue.\nMost current methodologies employ front-view images as input\nfor end-to-end learning policies. This methodology results in\nhighly complex and dimensional visual characteristics. Another\nstudy area that deserves attention is the application of deep\nreinforcement learning in autonomous Driving. Using elementary\ndeep reinforcement learning methods, such as DQN (\nMnih et al.,\n2013, 2015), may provide limitations in addressing intricate\nnavigation challenges. In recent times, there has been notable\nprogress in developing deep reinforcement learning algorithms\nwith increased eﬃcacy. Autonomous driving technology has\nlimitations that restrict its use to only a few tasks.\nThis study introduces a novel technique called deep\nreinforcement learning navigation via decision transformer\n(DRLNDT). The Transformer model uses the Soft Actor-Critic\napproach to gain accurate information about the present state\nby considering the past trajectory state. This method helps the\nAgent avoid misinterpretations or incorrect judgments regarding\nthe surroundings, possibly due to sensor occlusion or noise.\nThe conventional reinforcement learning model is constructed\nwithin the Markov Decision Process (MDP) framework. Our\nmethodological approach is based on the Partially Observable\nMarkov Decision Process (POMDP). The data collected (\nGhosh\net al., 2021 ) by the Agent’s sensor may need to be more accurate\nas it depends on a hidden variable that existed in the past state\nof the sensor and may not accurately represent the current\nenvironmental conditions. High-quality images are crucial for\ncapturing a complete and accurate representation of reality\nand extracting valuable information. Because images of high-\nquality and larger dimensions can more precisely depict the real\nworld, providing a more comprehensive range of valuable data.\nNevertheless, utilizing high-resolution images (\nNair et al., 2015 ;\nAndrychowicz et al., 2020 ; Janner et al., 2021 ) containing intricate\nvisual features results in the intricacy of sample learning and the\noccupation of substantial memory space, resulting in ineﬀective\nlearning and inadequate algorithm training. This study uses a\nvariational autoencoder (V AE) to extract latent vectors from high-\nresolution photos. These latent vectors are then substituted for the\noriginal high-resolution images, reducing dimensionality while\npreserving the salient features of the samples to the greatest extent\npossible. In addition, we utilize tricks of the Soft Actor-Critic (SAC)\npolicy, including changing temperature and variable learning rate,\namong others, to enhance the algorithm’s eﬃcacy. The conclusive\nexperimental ﬁndings demonstrate that our method outperforms\nthe baseline algorithm.\nIn this paper, we provide the following contributions:\n1. In this study, we provide a novel algorithm named deep\nreinforcement learning navigation via decision transformer\n(DRLNDT), which leverages a transformer model to acquire\nknowledge of the current state based on past states. The primary\nobjective of this approach is to mitigate judgment errors that\narise due to sensor noise or occlusion in a singular state.\n2. The variational autoencoder (V AE) extracts latent vectors from\nhigh-quality images, reducing the dimensionality of the state\nspace while preserving essential image properties. In conclusion,\noptimizing image memory allocation has improved training\neﬃciency and outcomes.\n3. The method enables an autonomous vehicle to navigate\nvisually from its starting point to its destination without\nrelying on route direction or high-precision maps, utilizing\nonly high-quality monocular raw photos and producing\nsuccessful outcomes.\n4. Our study incorporates vector states such as velocity and\nposition, which can be eﬀortlessly obtained from the vehicle’s\nFrontiers in Neurorobotics /zero.tnum/two.tnum frontiersin.org\nGe et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/three.tnum/eight.tnum/one.tnum/eight.tnum/nine.tnum\nintrinsic sensors. Furthermore, we introduce latent vectors\nfrom high-quality images to construct a multimodal state\nspace. This method enables the agents to evaluate the current\ntrajectory based on the states, leading to improved overall\nperformance outcomes.\nThis paper is organized into several sections, each with a speciﬁc\nfocus. Section 2 of this paper focuses on the elucidation and\nexplication of pertinent research in the ﬁeld of autonomous driving.\nThe text emphasizes reinforcement learning techniques used in\nautonomous driving and the approaches to address POMDPs\nthrough reinforcement learning algorithms. Section 3 of this paper\nintroduces various forms and deﬁnitions intended to facilitate\nthe comprehension and contextualization of the content. This\nparticular section holds signiﬁcant importance as it establishes\nthe fundamental basis for the methodology put forth in Section\n4. Section 4 of this paper introduces the DRLNDT algorithm,\nwhich serves as the central focus of the study and encompasses the\nmost intricate technical aspects. Section 5 depicts the experimental\noutcomes obtained by implementing our algorithm on the CARLA\nplatform. The results substantiate the superiority of our approach\nover the baseline approach. The available evidence adequately\nsupports the eﬃcacy of our approach. In conclusion, Section 6\nsummarizes the essential ﬁndings and oﬀers suggestions for future\nresearch directions.\n/two.tnum Related works\nWe reviewed recent literature on “Reinforcement learning-\nbased autonomous driving” and “Deep reinforcement learning for\nPOMDPs, ” summarizing their research.\n/two.tnum./one.tnum Reinforcement learning-based\nautonomous driving\nKendall et al. (2019) demonstrated the application of deep\nreinforcement learning to autonomous driving, where a model\nuses a single monocular image as input to learn a lane following\npolicy. The model is trained through several rounds with randomly\ninitialized parameters. The reward is the distance the vehicle\ntravels without the driver’s intervention. The approach relies\non continuous, model-free deep reinforcement learning, with all\nexploration and optimization taking place in the vehicle.\nChen et al. (2019) and his team have developed a framework\nfor deep reinforcement learning in urban autonomous driving\nscenarios. The framework uses a bird’s-eye view and visual coding\nto capture low-dimensional latent states. The team implemented\nseveral state-of-the-art model-free deep RL algorithms in the\nframework and improved their performance. They tested the\nperformance of the framework in the challenging task of navigating\na circular intersection with dense surrounding vehicles and\nfound that it performed excellently compared to the baseline.\nAdditionally, the team introduced and tested three model-free deep\nRL algorithms to evaluate their success rate in the roundabout\nintersection task. The results demonstrate the eﬀectiveness of the\nproposed framework and algorithms in solving complex urban\ndriving tasks.\nLiang et al. (2018) present a new Controllable Imitation\nReinforcement Learning (CIRL) model for DRL-based autonomous\nvehicle driving in a high-ﬁdelity vehicle ﬁdelity simulator. CIRL\ncombines Controllable Imitation Learning with DDPG policy\nlearning to address sample ineﬃciency in reinforcement learning.\nIt outperforms previous approaches, achieving state-of-the-art\ndriving performance on the CARLA benchmark. The CIRL\nmodel optimizes the policy network with specialized steering\nangle rewards for targeting diﬀerent driving scenarios. It has\nexcellent generalization capabilities across various environments\nand conditions.\nAnzalone et al. (2022) propose a reinforcement curriculum\nlearning method for training agents in a driving simulation\nenvironment. The Agent has two phases of training. In the ﬁrst\nphase, it starts from a ﬁxed location and drives according to the\nspeed limit without any traﬃc. In the second phase, the Agent\nencounters diverse starting locations and randomly generated\npedestrians. The driving policy is evaluated quantitatively\nand qualitatively.\nOzturk et al. (2021) propose the use of curriculum\nreinforcement learning for autonomous driving in diﬀerent\nroad and weather conditions. This study tackled the challenge\nof tuning Agents for optimal performance and generalization\nin various driving scenarios by using curriculum reinforcement\nlearning. Results showed signiﬁcant improvement in performance\nand a reduction in sample complexity. Diﬀerent courses provided\ndiﬀerent beneﬁts, indicating potential for future research in\nautomated curriculum training.\nYeom (2022) propose a deep reinforcement learning (DRL)\nbased collision-free path planning architecture for mobile robots,\nwhich can navigate unknown environments without supervision.\nThe architecture uses DRL to ﬁgure out the unknown environment\nand predicts control parameters for the mobile robots in the\nnext time step. Experimental results show that the proposed\narchitecture can successfully solve complex navigation problems in\ndynamic environments.\nWe found that although all of these studies had some\nachievements, they did not achieve the task of navigating from\nthe initial position to the termination position. Forward-looking\nimages were used in some methods, but most were low-resolution\nfor algorithm convenience. High-quality images are necessary\nfor more features and real-world applications. Most navigation\nagents use routing, which the original project did not intend.\nRouting guides the optimal policy but is not always optimal.\nComputation needs a high-precision map, which increases costs.\nIt goes against the original project’s idea of minimizing the need for\nhigh-precision maps.\n/two.tnum./two.tnum Deep reinforcement learning for\nPOMDPs\nHeess et al. (2015) used neural networks to solve continuous\ncontrol problems, and the method was successful in fully observed\nstates. Control problems in real-world scenarios are often only\npartially observed due to various factors, such as sensor limitations,\nchanges in the controlled object that go unnoticed, or state aliasing\ncaused by function approximation. This article proposes the use of\nFrontiers in Neurorobotics /zero.tnum/three.tnum frontiersin.org\nGe et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/three.tnum/eight.tnum/one.tnum/eight.tnum/nine.tnum\nrecurrent neural networks trained with temporal backpropagation\nin model-free continuous control algorithms to tackle partially\nobserved domains.\nIgl et al. (2018) proposed a method called Deep Variational\nReinforcement Learning (DVRL) to address the challenges of\npartially observable sequential decision problems. This method\nhelps the agent learn a generative model of the environment and\neﬃciently aggregate information. Researchers developed an n-step\napproximation of ELBO and a policy for the control task. DVRL\noutperforms previous approaches and accurately approximates\nthe conﬁdence distribution on latent states. Additionally, a\nsecond RNN summarizes the set of particles, accounting for the\nuncertainty of the latent state after the following action.\nZhu et al. (2017) proposed a new method called Action Speciﬁc\nDeep Recurrent Q Network (ADRQN) to improve the learning\nperformance in partially observable domains. This proposed\nmethod encodes actions with a multilayer perceptron (MLP) and\ncombines them with observation features from a convolutional\nneural network (CNN) to create action-observation pairs. These\npairs generate a time series integrated by a Long Short-Term\nMemory (LSTM) layer to infer latent states. A fully connected layer\ncomputes the Q-value, predicting expected rewards for actions in a\ngiven state. Tested in partially observable domains, including Atari,\nthis method outperformed state-of-the-art methods.\nHausknecht and Stone (2015) replaced the ﬁrst fully connected\nlayer of a Deep Q Network (DQN) with an LSTM to add loops\nand investigated its eﬀect. DRQN, a Deep Recurrent Q Network,\nintegrates temporal information and performs as well as DQN on a\nstandard and partially observed Atari game. Its performance varies\nwith observability and degrades less than DQN when evaluated\nwith partial observations. Looping is a viable alternative to stacking\nframe histories in the DQN input layer and adapts better to changes\nin observation quality when evaluated. However, looping does not\nprovide any systematic beneﬁts over stacking observations in the\ninput layer of a convolutional network for non-ﬂickering games.\nChen et al. (2021) use transformer to model high-dimensional\ndistributions of semantic concepts and their latent application to\nsequential decision problems formalized as reinforcement learning\n(RL). A new approach for Reinforcement Learning (RL) policy\ntraining has been proposed, which uses sequential modeling\nobjectives to train Transformer models with experience data.\nThe architecture, called Decision Transformer, can transform RL\nproblems into conditional sequence modeling. It has been shown\nto perform well on Atari, OpenAI Gym, and Key-to-Door tasks.\nThese methods are very inspiring, and we have proposed the\nDRLNDT method to enable autonomous driving navigation. The\ntransformer model is utilized to learn the actual state from the\nhistorical data, thus reducing decision errors caused by object\nocclusion or sensor noise. The results of our method are better than\nthose of the Baseline method in CARLA.\n/three.tnum Backgrounds\nThe Partially Observable Markov Decision Process (POMDP)\nis a type of sequential decision-making problem that involves\nmodeling the environment based on its location while also\nconsidering incomplete and noisy observations. This paper\npresents a novel approach known as deep reinforcement learning\nnavigation via decision transformer (DRLNDT). The proposed\nmethod incorporates a Decision Transformer, which learns\nthe state based on past observations. It then utilizes this\nlearned information to guide the Agent in navigating the task,\nfollowing a reward learning scheme, from the initial to the\ntermination position. Variable Autoencoder (V AE) (\nLoaiza-Ganem\nand Cunningham, 2019 ; Wei et al., 2020 ) is a neural network\ntype that can learn a compressed representation of input data by\nencoding it into the status space and decoding it back into the\noriginal space. This paper employs the variational autoencoder\n(V AE) to enhance the algorithm’s performance. DRLNDT utilizes\na Transformer neural network architecture to capture temporal\ndependencies within observations and actions eﬀectively. This\ncapability enhances self-driving vehicles’ decision-making process\nin partially observable urban environments. The paper introduces\nthe Transformer algorithm, integrated with a reinforcement\nlearning algorithm. The reinforcement learning algorithm employs\na variational autoencoder (V AE) for compressive characterization\nof the image data. Next, integrating multimodal observations’ time\nseries is performed using the Transformer model. The latent state\nis acquired by the layer, which subsequently employs the fully\nconnected layer to estimate the value and policy functions, similar\nto standard reinforcement learning algorithms.\n/three.tnum./one.tnum Markov decision processes\nA sequential decision (\nArulkumaran et al., 2017 ) problem\nrefers to a scenario in which an agent is tasked with making a\nsequence of decisions over time, where each decision’s outcome\nimpacts the subsequent decisions. In these types of problems, it\nis common for the Agent to possess knowledge of the dynamic\nmodel of the environment, which implies that the Agent has access\nto information regarding how the environment will change in\nresponse to its actions. In order to establish a formal framework\nfor addressing these issues, researchers employ a mathematical\nconstruct known as a Markov Decision Process (MDP) (\nPuterman,\n2014), deﬁned by a 4-tuple denoted as < S, A, P, R >. Here.\nS represents the set of all possible states in the environment,\nA represents the set of possible actions the Agent can take, P\nrepresents the probability distribution of the next state given the\ncurrent state and action, and R represents the mapped reward\nfunction where each state-action pair is rewarded with a scalar\nvalue. During each iteration, the Agent makes a decision by\nselecting an action at from a set of possible actions A, based on the\ncurrent state st from a set of possible states S, and its policy π which\nmaps states to actions. As a consequence of this action, the Agent\nreceives an immediate reward rt that is drawn from a distribution\nR(st, at). Additionally, the Agent transitions to a new state st+1,\nwhich is sampled from the probability distribution P(st+1|st, at).\nThe policy π (at|st) is utilized to calculate the state and state-\naction marginals, which are represented as ρπ (st) and ρπ (st, at),\nrespectively. The margins in question denote the likelihood of being\nin a speciﬁc state or state-action pair under the policy denoted\nas π . The objective of reinforcement learning is to identify the\noptimal policy that maximizes the expected discounted reward\nFrontiers in Neurorobotics /zero.tnum/four.tnum frontiersin.org\nGe et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/three.tnum/eight.tnum/one.tnum/eight.tnum/nine.tnum\nRt. The discount factor γ , which falls within the range of [0,\n1], determines the relative signiﬁcance of immediate rewards\ncompared to future rewards.\nRt = rt + γ rt+1 + γ 2rt+2 + . . . (1)\nThe discount rewards are computed using\nEquation (1), which\nprovides a concise representation of the rewards acquired at\neach time, considering the discount factor associated with each\ntimestep. In the context of Markov Decision Processes (MDPs),\nthe determination of the optimal policy can be achieved through\nthe process of value iteration. This iterative procedure entails the\nupdating of the value function, which serves as a representation\nof the expected discounted reward for each state given a\nspeciﬁc policy.\n/three.tnum./two.tnum Soft Actor Critic\nThe Q-learning (\nWatkins and Dayan, 1992 ) technique was\nintroduced by Watkins and Dayan in 1992 as a solution\nto reinforcement learning problems characterized by unknown\nenvironmental dynamics. The technique is considered model-free\nas it does not necessitate prior knowledge of the environment or\nits dynamics. Q-learning aims to estimate the value associated with\nexecuting an action and adhering to an optimal policy π within\na speciﬁc state. The quantity above is commonly referred to as\nthe state-action value or, more succinctly, the Q-value. The Q-\nvalue measures the anticipated total reward achieved by selecting\na speciﬁc action in a given state and adhering to the optimal\npolicy. The Q-value is deﬁned recursively as the summation of the\nimmediate reward acquired from the action and the discounted\nvalue of the subsequent state-action pair. The utilization of a\ndiscount factor γ , which falls within the range of [0, 1], serves\nthe purpose of discounting future rewards and facilitating the\nconvergence of Q values. The optimal policy π ∗ can be derived\nby selecting the action with the maximum Q-value in every state.\nQ-learning is an algorithm that operates oﬀ-policy, meaning it\nlearns the Q-value of a target policy while adhering to various\nbehavioral policies.\nThe function Qπ (s, a) is a formal mathematical representation\ndenoting the expected collect reward that an Agent obtains when\nit selects action a within state s and adheres to policy π . The Q-\nvalue is acquired through an iterative process, wherein the Agent\ncontinually updates its estimate of the Q-value by considering the\nrewards it obtains during its interactions with the environment.\nQπ (s, a) = Eπ (Rt|st = s, at = a) (2)\nThe\nEquation (2) represents the anticipated reward that the Agent\nis expected to obtain at a given time t, under the condition that the\nAgent is in a speciﬁc state denoted as s and selects a particular action\ndenoted as a, by the policy denoted as π . Q-values play a crucial\nrole in reinforcement learning, enabling the Agent to make optimal\naction selections within a speciﬁc state. The Agent selects the action\nwith the highest Q value within the given state. The process of\nupdating Q-values can be accomplished through the utilization\nof a method known as Q learning. This technique entails the\nmodiﬁcation of the Q-value associated with the present state-action\npair by considering the highest Q-value among the subsequent\nstate-action pairs. The Q-learning algorithm is classiﬁed as an oﬀ-\npolicy method, which implies that the Agent can learn the optimal\nQ-value even when it adheres to a policy that diﬀers from the\none being evaluated. The Q-value is employed for approximating\nthe value of the policy, speciﬁcally the anticipated cumulative\nreward that the Agent will obtain by adhering to the policy. The\nmaximization of the value function determines the optimal policy.\nThe Q function is a mathematical function that provides an\nestimation of the expected future reward when a speciﬁc action is\ntaken within a speciﬁc state in\nEquation (3).\nQ(s, a) = Q(s, a) + β(r + γ max\na\n′Q(s′, a′) − Q(s, a)) (3)\nThe equation presented herein represents the Q-learning\nupdate rule, which is a fundamental element of numerous\nreinforcement learning algorithms. The equation incorporates\nvarious components, namely the current state ( s), the action taken\n(a), the reward received ( r), the subsequent state ( s′), and a\ndiscount factor ( γ ). This equation updates the Q value of the\ncurrent state-action pair by adding the scaling diﬀerence between\nthe estimated Q value of the following state-action pair and the\nQ value of the current state-action pair. The scaling factor β is\nthe learning rate, which determines how much new information\nis incorporated into existing estimates. In instances with many\nstates where it is impossible to save Q values for all state-action\ncombinations, the equation above is used. In contrast, a function\napproximator, such as a neural network, estimates the Q values\nfor previously unobserved state-action combinations. The DQN\nmethod illustrates a reinforcement learning methodology that\nutilizes a neural network to estimate Q values. Using the current\nstate and action as input variables, the neural network, identiﬁed\nby the parameter θ , generates an estimated value Q for a speciﬁc\nstate-activity combination.\nIn contrast to the DQN algorithm, Soft Actor-Critic (SAC)\n(\nHaarnoja et al., 2018 ) is an oﬀ-policy Actor-Critic algorithm\nthat operates within a maximum entropy reinforcement learning\nframework. The primary objective of SAC is to optimize\nboth the expected return and entropy. SAC contains several\nmodiﬁcations to accelerate training and enhance the stability\nof hyperparameters, such as automatic tuning of the constraint\nformulas for the temperature hyperparameter. The maximum\nentropy objective extends the conventional aim employed in\nconventional reinforcement learning methods. Adding an entropy\nelement to the objective signiﬁes that the optimal policy seeks to\nmaximize its entropy at each accessed state.\nπ ∗ = argmax\nπ\n∑\nt\nE(st,at)∼ρπ [r(st, at) + αH(π (·|st))] (4)\nMaximizing the expected reward and entropy of each\nstate determines the optimal policy. The parameter α in the\nEquation (4) governs the relative signiﬁcance of the entropy\nterm concerning the reward, inﬂuencing the optimal policy’s\nprobabilistic characteristics. The maximum entropy aim applies\nwhen the best policy necessitates randomization or stochasticity,\nsuch as in exploration tasks or when confronted with unpredictable\nFrontiers in Neurorobotics /zero.tnum/five.tnum frontiersin.org\nGe et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/three.tnum/eight.tnum/one.tnum/eight.tnum/nine.tnum\nsettings. The discount factor, denoted as γ , is a scalar within the\nrange of 0 to 1, which plays a crucial role in determining the relative\nsigniﬁcance of future rewards within the context of decision-\nmaking. A discount factor of zero implies that only incentives\nin the present period are considered. In contrast, a discount\nfactor of one indicates that rewards in the future are given equal\nimportance to immediate rewards. The discount factor is crucial in\ninﬁnite horizon problems since it guarantees the convergence of the\nexpected reward and entropy to a limited value. With the discount\nfactor, the cumulative value of predicted rewards and entropy may\nremain the same toward inﬁnity, making the objective function’s\noptimization attainable. Incorporating the discount factor enables\nthe algorithm to eﬀectively weigh the signiﬁcance of immediate\nbeneﬁts against those obtained in the future, facilitating more\noptimal decision-making over extended time horizons.\nDetermine the solution for the optimal Q-function, which\nestablishes a correspondence between a state-action pair and a\nvalue that denotes the anticipated long-term beneﬁt associated with\nexecuting that action in that state and afterward adhering to the\noptimal policy. From the ideal Q-function, one can deduce the\nbest policy. The suggested algorithm is a Soft Actor-Critic (SAC)\napproach that is formulated using the policy iteration framework.\nThe Q-function associated with the present policy is assessed,\nand the policy is then modiﬁed through the utilization of oﬀ-\npolicy gradient updating. Oﬀ-policy suggests a diﬀerence between\nthe policy being updated and the policy that produced the data\nfor modiﬁcation. The Maximum Entropy Reinforcement Learning\nframework serves as the foundation for the Soft Actor-Critic\nalgorithm, where the Actor’s objective is to maximize predicted\nreward and entropy.\nSoft policy iteration is a generalized algorithm for learning\noptimal maximum entropy policies. The algorithm alternates\nbetween policy evaluation and policy improvement in a maximum\nentropy framework. The derivation of the algorithm is based on a\ntabular setup that allows for theoretical analysis and convergence\nguarantees. The algorithm aims to converge to the optimal policy\namong a set of strategies, which may correspond to a set of\nparameterized densities. The set of strategies to which the algorithm\nconverges is not ﬁxed and can vary depending on the speciﬁc\nproblem to be solved. The algorithm aims to maximize the\nexpected return while maximizing the entropy of the strategies.\nThe entropy of a policy is a measure of the stochasticity of the\npolicy, and maximizing it encourages exploration and prevents\nthe policy from falling into a local optimum. The algorithm\nis called “Soft” because it uses a Soft-valued function instead\nof a hard-valued function. Soft-valued functions are smoothed\nversions of hard-valued functions, which are easier to optimize and\nprevent overﬁtting.\nSoft policy iteration is a technique employed in the ﬁeld\nof reinforcement learning to assess the eﬃcacy of a policy and\ndetermine its worth by optimizing the maximum entropy target.\nDuring the policy evaluation phase of Soft policy iteration, it is\npossible to calculate Soft Q values for ﬁxed policy iterations. The\ncomputation of the Soft Q value involves the iterative use of the\nmodiﬁed Bellman backup operator, denoted as Tπ in\nEquation (5).\nTπ Q(st, at) ≜ r(st, at) + γ Est+1∼p[V(st+1)] (5)\nwhere r(st, at) represents the reward obtained for taking an\naction at in state st, γ represents the discount factor, and p\nrepresents the transfer probability distribution. The Soft Q-value is\ncalculated using the Soft state value function V(st) in Equation (6).\nV(st) = Eat∼π [Q(st, at) − αlogπ (at|st)] (6)\nThe value of Q for taking an action at in state st is denoted as\nQ(st, at). The probability of taking an action in state st according to\nthe policy π is represented as π (at|st). The temperature parameter\nα regulates the balance between maximizing the expected payoﬀ of\nthe policy and maximizing the entropy. By repeatedly applying the\nBellman backup operator T to any initial Q function Q : S×A → R,\none can obtain the Soft Q function for any policy π . The Soft Q\nfunction is an advantageous instrument for assessing policies in the\ncontext of reinforcement learning due to its consideration of policy\nuncertainty and promotion of exploration.\nTo create a feasible approximation of the Soft policy iteration,\na function approximator can be utilized for the Soft Q function\nand policy. Instead of assessing and enhancing the convergence\naspect, it is suggested to employ stochastic gradient descent as an\nalternative approach to optimize both networks simultaneously.\nThe Soft Q function and policy are parameterized by a neural\nnetwork with θ and phi parameters. The Soft Q function can\nbe modeled as an expressive neural network. In contrast, the\npolicy can be modeled as a Gaussian function, with the neural\nnetwork providing the mean and covariance. The rules for updating\nthese parameter vectors are subsequently derived and employed to\noptimize the network during the training process. The objective\nis to tackle the issues of signiﬁcant sample complexity and\nvulnerability to hyperparameters commonly observed in model-\nfree deep reinforcement learning methods through the utilization\nof function approximators and stochastic gradient descent. The\nsuggested methodology is grounded in the framework of maximum\nentropy reinforcement learning. This paradigm seeks to optimize\nboth the expected return and entropy, enabling the Agent to\naccomplish the goal while exhibiting a high degree of randomness\nin its actions.\nThe Soft Q function is a modiﬁed version of the Q function\nemployed in the ﬁeld of reinforcement learning, which integrates\na component of entropy to promote exploration. The parameters\nof the Soft Q function are optimized through training in order to\nminimize the Soft Bellman residual, which serves as a metric for\nquantifying the discrepancy between the anticipated Q value and\nthe real Q value.\nJQ(θ ) = E(st,at)∼D\n[ 1\n2 (Qθ (st, at) − (r(st, at)\n+γ Est+1∼p[V¯θ (st+1)]))2]\n(7)\nThe Soft Bellman residual is formally deﬁned in Equation (7),\nwhereby it encompasses the calculation of the expected value of\nthe squared discrepancy between the predicted Q-value and the\nsummation of the reward and subsequent state discount values. The\nutilization of the Soft Q function argument serves as an implicit\nparameterization of the value function, as speciﬁed in\nEquation (6).\nA crucial component of the SAC algorithm, the value function\nFrontiers in Neurorobotics /zero.tnum/six.tnum frontiersin.org\nGe et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/three.tnum/eight.tnum/one.tnum/eight.tnum/nine.tnum\nestimates the expected reward for a given state. The parameters of\nthe Soft Q function are optimized by the utilization of stochastic\ngradient descent, a widely employed optimization method within\nthe ﬁeld of deep learning. The optimization of the Soft Q function\nparameters is a crucial component of the SAC method as it\nenables the Agent to acquire a precise estimation of the anticipated\nreward associated with a speciﬁc state-action combination. By\nreducing the residuals of the Bellman Soft equation, the Agent can\nacquire improved decision-making abilities and attain enhanced\nperformance across a range of reinforcement learning challenges.\nˆ∇θ JQ(θ ) = ∇ θ Qθ (at, st)(Qθ (st, at) − (r(st, at) + γ (Q¯θ (st+1, at+1)\n−αlog(πφ (at+1|st+1))))) (8)\nThe Soft Q function arguments, which are obtained from\nEquation (6), implicitly parameterize the value function. The\nobjective is optimized using stochastic gradient descent, where the\nstochastic gradient is computed using the gradient of the Q function\nwith respect to its parameters in\nEquation (8). The Q function is\na mathematical function that accepts the current state ( st) and\naction (at) as its input and produces the anticipated reward for that\nspeciﬁc state-activity combination. The expected reward is equal to\nthe sum of the instantaneous reward ( r(st, at)) and the discounted\nexpected reward ( Qθ (st+1, at+1)) for the next state-action pair. The\nSoft Q function is modiﬁed by the addition of a term that promotes\nexploration, which is determined by the temperature parameter α\nand the policy function πφ (at + 1|st + 1). The update also employs\na target Soft Q function with parameter ¯θ , which is derived as an\nexponentially shifted mean of the Soft Q function weights. The\nutilization of this target Q function serves the purpose of stabilizing\nthe training process and mitigating the occurrence of overﬁtting.\nJπ (φ) = Est∼D[Eat∼πφ [αlog(πφ (at|st)) − Qθ (st, st)]] (9)\nEquation (9) denotes the goal function Jπ (φ) employed for\nthe purpose of acquiring the policy parameters in the Soft Actor-\nCritic (SAC) algorithm. The objective function Jπ (φ) entails\nthe maximization of the expected payout and entropy of the\nActor while executing the job. There exist other alternatives for\nminimizing the objective function Jπ . However, in the context\nof Soft Actor-Critic (SAC), the reparameterization method is\nemployed as a means to attain a reduced variance estimator.\nThe reparameterization technique entails converting the random\nvariables used to sample the actions from the policy into noise\nvariables and diﬀerentiable functions of the policy parameters,\nthereby permitting the gradient to be back-propagated through\na network of policy and target densities, which in SAC are Q-\nfunctions represented by neural networks. The utilization of the\nreparameterization methodology yields an estimator with reduced\nvariance in comparison to the likelihood ratio gradient estimator\ncommonly employed in policy gradient methods.\nSAC employs a reparameterization technique for neural\nnetwork restructuring. The equation at = fφ (ǫt; st) is utilized\nto establish a mapping between states and actions within the\ncontext of a reinforcement learning problem. The function\nfφ (ǫt; st) represents the transformation of the neural network,\nwith φ denoting the parameters of the network. The input to\nthe neural network transformation is the noise vector ǫt, which\nis sampled from a stationary distribution, such as a spherical\nGaussian distribution. The utilization of noise vectors as inputs\nserves the objective of introducing stochasticity into the policy,\nhence facilitating the Agent’s exploration of the environment\nand enhancing its ability to acquire more eﬀective methods. The\noutcome of the neural network transformation corresponds to the\naction executed by the Agent in reaction to the present state st. By\nemploying this particular reparameterization technique, the SAC\nalgorithm is capable of acquiring policies that are more versatile\nand articulate, hence enabling them to be adjusted to various\nscenarios within the environment.\nJπ (φ) = Est∼D,ǫt∼N [αlogπφ (fφ (ǫt; st)|st) − Qθ (st, fφ (ǫt; st))] (10)\nEquation (10) is an altered variant of Equation (9) that includes\nan implicit deﬁnition of the Actor policy πφ based on the function\nfφ . A neural network called fφ is a function that links actions\nto current states and timesteps. The objective in Equation (10)\nis expressed as a mathematical function that depends on two\nvariables: the policy parameter φ and the Q-value parameter θ .\nThe objective function in\nEquation (10) is estimated by combining\ndata D in the replay buﬀer with noise N derived from a normal\ndistribution. The expression α log πφ (fφ (ǫt; st)|st) in Equation (10)\ndenotes the entropy of the policy, which promotes exploration and\nstochasticity in the behavior of the Actor.\nEquation (9) introduces\nthe notation Qθ (st, fφ (ǫt; st)|st), which denotes the Q-value of the\nCritic. This Q-value serves as a metric for estimating the anticipated\nrewards associated with the current condition and action. The\ngradient approximation formula is represented as ˆ∇φ Jπ (φ), which\nserves as an unbiased estimator of the gradient in\nEquation (11).\nˆ∇φ Jπ (φ) = ∇ φ αlog(πφ (at|st)) + (∇at αlog(πφ (at|st))\n− ∇ at Q(st, at))∇φ fφ (ǫt; st) (11)\nThe gradient estimator extends the DDPG ( Silver et al., 2014 )\npolicy gradient to any easily handled stochastic policy. The formula\ninvolves evaluating at at fφ (ǫt; st). The gradient estimator involves\ntwo terms, the ﬁrst of which is ∇φ αlog(πφ (at|st)), which is the\ngradient of the policy function concerning the logarithm of the\npolicy parameters. The second term in the gradient estimator is\n(∇at αlog(πφ (at|st)) − ∇at Q(st, at))∇φ fφ (ǫt; st), which relates to the\npolicy function The gradient of the logarithm to the action, the\ngradient of the action-value function to the action, and the gradient\nof the feature extractor to the policy parameters.\n/three.tnum./three.tnum Partially Observable Markov Decision\nProcesses (POMDPs)\nPartially Observable Markov Decision Processes (POMDPs)\n(\nTamar et al., 2016 ) is a broader framework than Markov\nDecision Processes (MDPs) for addressing planning challenges\nwhere an agent lacks complete knowledge of the environment’s\nstate. POMDPs represent decision-making scenarios wherein the\nagent’s information about the environment is incomplete. A\nFrontiers in Neurorobotics /zero.tnum/seven.tnum frontiersin.org\nGe et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/three.tnum/eight.tnum/one.tnum/eight.tnum/nine.tnum\nPOMDP is mathematically deﬁned as a tuple of six elements <\nS, A, Z, P, O, R >, where S is the state space, A is the action space,\nZ is the observation space, P is the state transfer function, O is the\nobservation function, and R is the reward function. The observation\nfunction O maps a state-action pair to a probability distribution\nrepresenting the likelihood of observing a particular observation.\nThe transition function, denoted as P, represents the conditional\nprobability of the environment transitioning from state st to state\nst+1 given that the Agent executes action at. The reward function R,\ndenoted as r(st, at), characterizes the instantaneous reward that the\nAgent obtains upon executing a particular action in a speciﬁc state.\nIn the context of a partially observed Markov Decision\nProcess (MDP), the Agent lacks direct access to the state of\nthe environment. Instead, it receives an observation ( O) that is\ncontingent upon the underlying state ( p(ot|st)). Consequently, the\nAgent must rely on its observations to deduce the latent state\nof the environment and subsequently determine its actions. The\nAgent lacks direct access to the underlying state of the Markov\nDecision Process (MDP). In contrast, the Agent perceives the state\nindirectly through its observations, and as a result, the observation\nspace may exhibit noise or incompleteness. The optimal Agent\nrequires access to the entire history of the Agent’s observations\nand actions, denoted ht = (o1, a1, o2, a2, · · ·, at−1, ot), which is\npreferable to the MDP , in which the current state is dependent on\nall preceding states and actions taken by the Agent. Nevertheless,\nit may prove impractical or ineﬃcient for an Agent to retain and\nprocess the complete chronicle of observations and actions. Hence,\nthe Agent must employ a memory-based methodology to retain\npertinent knowledge from previous instances while disregarding\nextraneous information.\nRecurrent Neural Networks (RNNs) that have been trained\nusing the Back Propagation Through Time (BPTT) algorithm\nare widely utilized in memory-based control within partially\nseen domains. Recurrent neural networks (RNNs) can retain\na concealed state that encapsulates pertinent information from\npreceding instances, hence transmitting current actions to the\nAgent. The Long Short-Term Memory (LSTM) model is a\nRecurrent Neural Network (RNN) type that is highly proﬁcient\nin capturing and modeling long-term dependencies within\ndatasets. This study uses the backpropagation technique to\ntrain the Transformer model to capture memory-based control\nwithin a partially seen domain eﬀectively. This study showcases\nthe eﬃcacy of the Transformer model in facilitating the\nAgent’s resolution of diverse physical control challenges, which\nnecessitate varying degrees of memory utilization. These challenges\nencompass integrating noisy sensor data in the short term and\npreserving information across multiple processes in long-term\nmemory tasks.\nIn accordance with\nEquation (1), the objective function J is\nmodiﬁed to represent the trajectory the stochastic policy must\nmaximize in order to characterize it. Hence, the objective function\nJ represents the anticipated accumulation of discounted beneﬁts\nachieved over an unlimited time horizon in\nEquation (12).\nJ = Eτ\n[ ∞∑\nt=1\nγ t−1r(st, at)\n]\n(12)\nTrajectories τ are obtained from the distribution of trajectories\ngenerated by the method π . The trajectory distribution\np(s1)p(o1|s1)π (a1|h1)p(s2|s1, a1)p(o2|s2)π (a2|h2)... can be\nexpressed as the multiplication of three components: the\ninitial state distribution p(s1), the observation distribution p(ot|st),\nand the conditional action distribution π (at|ht) conditioned\non the history ht. The history ht is an adequate summary of\nprior observations and actions up to time t − 1. The trajectory\ndistribution refers to the distribution encompassing all conceivable\ntrajectories τ = (s1, o1, a1, s2, o2, a2, ...) that can be produced by\nimplementing the policy π based on the probability distribution.\nThe objective function, denoted as J, quantiﬁes the anticipated total\nreward acquired by adhering to the policy π during an unlimited\ntime horizon. In this context, the reward is subject to discounting\nat each timestep by a factor of γ . In the context of a deterministic\npolicy, the conventional policy function denoted as π is substituted\nwith a deterministic function denoted as µ . This function directly\nmaps the state S to the action A. Furthermore, in the conditional\naction distribution π (a1|h1), the history of action replacements\nreplaces denoted as ht. This history is obtained by applying the\ndeterministic policy function µ .\nIn the context of a fully observable Markov Decision Process\n(MDP), the Agent possesses knowledge of the current state s,\nand the action value function Qπ is established as the anticipated\nfuture discounted reward when the Agent takes an action in\nstate st and after that adheres to policy π . In situations where\nobservations are incomplete, the intelligence lacks access to the\ntrue state s, and the construction of the action-value function\nQπ relies on the variable h. The variable h denotes the internal\nstate or memory of the intelligence system, which undergoes\nupdates at each timestep by the present observations and preceding\ninternal states. The function Qπ is employed to assess the eﬃcacy\nof the policy π , which serves as a mapping between states to\nactions. The primary objective of the algorithm is to identify the\npolicy that will yield the highest possible predicted future discount\nreward. The algorithm utilizes the function Qπ (ht, at) to address\ncontrol problems that involve incomplete observation, requiring\nthe Agent to depend on its internal state for decision-making. This\nmethodology enables the Agent to eﬀectively incorporate data from\nimprecise sensors over time and preserve information at various\ntemporal intervals, a crucial requirement for addressing distinct\nphysical control challenges.\nThe Q-value function for a given policy π in a partially observed\ncontrol situation is deﬁned by\nEquation (13). The Q-value function\nquantiﬁes the anticipated total reward that an Agent can obtain by\nadhering to the policy π in the context of a speciﬁc state-action\npair (ht, at).\nQπ (ht, at) = Est|ht [rt(st, at)]+Eτ>t|ht,at\n[ ∞∑\ni=1\nγ ir(st+i, at+i)\n]\n(13)\nEquation (13) is comprised of two components: the\ninstantaneous reward rt acquired by executing an action in\nstate st, and the anticipated future reward acquired by adhering\nto the policy π for actions initiated from the subsequent state\nst+1 and at+1. The discounting of future beneﬁts is denoted by\nthe variable γ , which represents the inclination of the intelligence\ntoward immediate rewards in comparison to delayed rewards. The\nFrontiers in Neurorobotics /zero.tnum/eight.tnum frontiersin.org\nGe et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/three.tnum/eight.tnum/one.tnum/eight.tnum/nine.tnum\nfuture reward is determined by the complete sequence of states,\nobservations, and actions, denoted as τ>t = (st+1, ot+1, at+1, . . . ),\nfollowing the current state-action pair ( ht, at). The computation\nof this reward involves two expectations, which are conditioned\non the probabilities p(st|ht) and p(τ>t|ht, at), respectively. These\nprobabilities are evaluated based on the trajectory distribution\nthe policy π induces. The trajectory distribution refers to the\nprobability distribution, including all potential paths that can\nbe pursued by an intelligent agent, based on the current state-\naction pair ( ht, at), while adhering to the on-policy π . The ﬁrst\nexpectation calculates the anticipated immediate reward that the\nIntelligent Agent can acquire by executing an action in state st,\nconsidering the present belief state ht. The second expectation\ncalculates the anticipated future reward that the Agent can achieve\nby adhering to the policy π from the subsequent state st+1 and\naction at+1, considering the present belief state ht and action at+1.\nThe belief states, denoted as ht, serve as a comprehensive statistic\nthat encapsulates all pertinent information on the intelligent\nAgent’s previous observations and actions. These belief states\nare utilized to compute the trajectory distribution and the Q\nvalue function.\n/three.tnum./four.tnum Transformer\nVaswani et al. (2017) and Parmar et al. (2018) ﬁrst proposed\nTransformer in a research paper. The architectural design of\nthe Transformer model is characterized by the utilization of\nstacked self-attention layers, which are interconnected via residual\nconnections. In the context of self-attention (\nChoromanski et al.,\n2020; Wang et al., 2020 ) layers, it is observed that each layer gets\na set of n embeddings denoted as {xi}n\ni=1, where each embedding\ncorresponds to a distinct input token. The self-attention layer\nsubsequently generates n embeddings {zi}n\ni=1, which maintain the\noriginal input dimension. Each token at index i is associated\nwith a key ki, a query qi, and a value vi through a linear\ntransformation. The key ki is essential for extracting pertinent\ninformation from the input sequence. Conversely, the query qi\ncalculates the attention scores between the key ki and other keys.\nThe value vi is employed in calculating a weighted sum of attention\nscores, which is subsequently utilized in generating the output\nembedding {zi}n\ni=1. Utilizing self-attention enables the model to\nselectively attend to various segments of the input sequence, which\nis highly advantageous in capturing distant relationships within the\ndata. The utilization of residual connectivity addresses the issue\nof diminishing gradients and facilitates the eﬃcient training of\ndeeper architectures.\nThe self-attention layer is a crucial component of the\nTransformer architecture for sequence modeling tasks such as\nlanguage translation and sentiment analysis. In the context of the\nDecision Transformer, the Self-Attention Layer is used to compute\nthe optimal action based on the input sequence of states and actions\nin\nEquation (14).\nzi =\nn∑\nj=1\nsoftmax({< qi, kj′ >}n\nj′=1)j ·vj (14)\nThe self-attention (\nYoo et al., 2015 ) layer operates by\ncalculating a weighted summation of values vj, with the weights\ndetermined by the normalized dot product between the query qi\nand the remaining keys kj. The query qi represents the present\nstate or action, whereas the key kj represents a previous state or\naction. Higher values indicate more signiﬁcant similarity between\nthe query qi and each key kj, as measured by the dot product.\nThe dot product is subsequently normalized using the softmax\nfunction, which guarantees that the weights are normalized to\na sum of 1 and accurately represent the probability distribution\nover the keys. Subsequently, the obtained weights are employed to\nprovide a weight to the value vj, which signiﬁes a characteristic or\nrepresentation of a previous condition or action. The ith output of\nthe self-attention layer is determined by the weighted sum of the\nvj values. This output calculates the optimal action for the current\nstate or action. In general, incorporating a self-attention layer in\nthe Decision Transformer model enables the model to eﬀectively\ncapture interdependencies among the pieces of the input sequence,\nhence facilitating the generation of optimal actions driven by the\nexpected return or reward.\nThe VIT (\nDosovitskiy et al., 2020 ) architecture is a variant\nof the Transformer architecture, a prevalent framework employed\nin several natural language processing applications, including\nlanguage modeling and machine translation. The Transformer\narchitecture comprises a neural network with multiple layers of\nself-attention and feed-forward mechanisms. This design enables\nthe model to capture long-range dependencies within the input\nsequence. Utilizing the self-attention mechanism enables the model\nto choose to attend to various segments within the input sequence\nand calculate a weighted summation of the input embeddings,\nconsidering their interrelationships. The VIT architecture modiﬁes\nthe Transformer model by incorporating a causal self-attention\nmask. This mask constrains the attention mechanism to attend\nsolely to preceding tokens in the sequence throughout the\ntraining and generation processes. The VIT model can construct\nautoregressive sequences, wherein each token is formed by\nconsidering the preceding token. The similarity of query vectors\nand key vectors in the self-attention mechanism enables the model\nto implicitly form state-return associations, where similar states\nare associated with similar returns. The computation of attention\nweights, which determine the relative relevance of each input token\nfor the output, is achieved by taking the dot product between the\nquery vectors and key vectors. Using the VIT architecture within\nthe Decision Transformer framework enables the model to generate\nfuture actions that generate the desired returns by adjusting the\nautoregressive model to the desired returns, past states, and actions.\n/four.tnum Deep reinforcement learning\nnavigation via decision transformer\nThis study presents the DRLNDT approach, which combines\ndecision transformer with deep reinforcement learning to achieve\nmotion navigation in autonomous vehicles. The objective is to\nsuccessfully guide the vehicle from its starting point to its ﬁnal\ndestination. This approach enables processing high-dimensional\nobservations by utilizing pixel-level learning from raw, high-\nresolution photos captured by the autonomous vehicle.\nFrontiers in Neurorobotics /zero.tnum/nine.tnum frontiersin.org\nGe et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/three.tnum/eight.tnum/one.tnum/eight.tnum/nine.tnum\n/four.tnum./one.tnum Deep reinforcement learning\nnavigation via decision transformer\nbackgrounds\nIn the context of our autonomous driving navigation task,\nwhich heavily relies on visual perception to understand the\nsurrounding environment, it is necessary to consider the potential\nlimitations of static photographs in conveying information about\nthe speed of dynamic situations. The occlusion of objects can\noccur as a result of the inherent three-dimensional characteristics\nof the environment. In addition, most visual sensors have limited\nbandwidth, limiting the Agent’s ability to perceive the environment\naccurately and inﬂuencing the self-driving car’s navigational\ndecision-making capabilities. The SAC algorithm is a fundamental\ndeep reinforcement learning method employed in our research. A\nconcise overview of its central ideas may be found in Section 3.2.\nThis paper examines an extension of SAC to Partial Markov\nDecision Processes for partially observed image data processing\nin autonomous navigation. The fundamental concept underlying\nthe SAC method is iteratively adjusting the policy parameters in\nthe direction of the gradient of the predicted reward for these\nparameters. In situations where observation is limited, the accurate\nestimation of the action-value function becomes unattainable.\nConsequently, the policy must be adjusted based on the observed\ncondition and reward. The credibility of designating a state as a\ncurrent observation is uncertain, thus necessitating the inference\nof the present state of the environment based on a chronological\nrecord of past observations up to the present moment. In order\nto tackle this issue, a Transformer model is employed to encode\npolicy that retains information from previous observations and\nactions, which are subsequently trained using the backpropagation\nalgorithm. The neural network serves as a function approximator\nto accommodate huge observation spaces, such as the pixel space\nacquired through integrating a camera into an autopilot system.\nThe use of neural networks with convolutional measures in this\nmethod has proven eﬀective for various perceptual processing\ntasks and for extending reinforcement learning to signiﬁcant state\nspace methods.\nUsing neural networks with convolutional measures has proven\neﬀective in this approach for various perceptual processing tasks\nand extending reinforcement learning to extensive state space\nmethods. In our autonomous driving navigation task, however,\nwe use an inﬁnite space of pixels and high-quality images with a\nhigh degree of information, not only because more features can\nbe extracted from high-quality images but also because we want\nour algorithms to apply to real-world environments. The high\nquality of images captured by the camera of a real car leads to\na signiﬁcant memory requirement during training. Consequently,\nsetting greater values for the buﬀer_size and batch_size becomes\nimpractical, adversely impacting the training results and eﬃciency.\nThe variational autoencoder (V AE) is employed to extract latent\nvectors. The conversion of the image space to latent space is\nperformed to establish a congruence between the latent vector space\nand the image space within the machine’s cognitive framework.\nConsequently, using the latent space instead of the image space\nensures the preservation of characteristics to the greatest extent\npossible while minimizing the memory footprint. The term is\ndenoted as Slatent.\nDetermining the optimal policy and action-value function\nis contingent upon the historical record of previously observed\nactions, represented as ht. In this study, we suggest a modiﬁcation\nto the neural network framework that facilitates the acquisition\nof knowledge regarding the policy and action-value function. In\nthis study, we suggest employing a Transformer network as an\nalternative to a feed-forward network. Preserving messages with\nhistory is an essential capability of the Transformer model, as it\nenables the resolution of partially observed situations.\nThe utilization of Transformer (\nChoromanski et al., 2020 ;\nDing et al., 2020 ; Parisotto et al., 2020 ) enables the formulation\nof the policy and action value functions, represented as π (h, a)\nand Q(h, a) respectively, in terms of the observed action history\nht. This approach facilitates the policy update process by\nincorporating the history of observed actions rather than solely\nrelying on the current observation ( ot). Additionally, it allows for\nutilizing the learned approximation ( oθ ) to address the challenges\nposed by the partially-observed control problem Qθ , thereby\nreplacing Qπ .\nDRLNDT is oﬀ-policy, indicating that the policy being learned\ndiﬀers from the policy used to generate the data. Exploration is\nnecessary for acquiring knowledge about the gradient of the Q\nfunction to actions. This approach necessitates that the Agent\ndo behaviors that can be more optimal to acquire knowledge\nof the environment. However, exploration can be ineﬃcient\nand unpredictable in practice. To address this issue, academics\nfrequently employ experience replay to enhance data eﬃcacy and\nstability. The process of experience replay entails the storage of\nexperience trajectories in memory and the subsequent sampling\nfrom this memory throughout the learning phase. This approach\nenables the Agent to acquire knowledge from diverse encounters\nand has the potential to enhance the robustness of the learning\nprocess. In the case of DRLNDT, sampled memory trajectories\nare used to learn expectations through experience replay. In\nour memory, we store a tuple < Ot, At, Ot+1, Rt, done >.\nHere, Ot represents a succession of observations labeled as\not−n, ot−n−1, ..., ot. This sequence is continuous and diﬀers from the\nconventional representation of Ot. In our case, Ot encompasses past\nand present observations. The set At is deﬁned as the collection of\nat values that exclusively represent the action associated with the\npresent state.\n/four.tnum./two.tnum Baseline architecture\nThis study conducts a comparative analysis of two\nmethodologies for training Agents using reinforcement learning\nto achieve autonomous navigation from the starting point to the\nﬁnal destination in the context of autonomous driving. The ﬁrst\nmethodology discussed in the paper is called deep reinforcement\nlearning navigation via decision transformer (DRLNDT). The\nsecond methodology employed in this study involves utilizing a\nrecurrent neural network (RNN) to encode time series data. The\nsecond strategy is designated as the baseline approach, and our\nstudy primarily focuses on conducting controlled experiments\nusing this approach.\nDeep reinforcement learning (DRL) has demonstrated eﬃcacy\nin contexts with complete observability, although its performance\nFrontiers in Neurorobotics /one.tnum/zero.tnum frontiersin.org\nGe et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/three.tnum/eight.tnum/one.tnum/eight.tnum/nine.tnum\nFIGURE /one.tnum\nLong Short-Term Memory (LSTM) layer.\nhas been suboptimal in environments with partial observability.\nIn 2015, Hausknecht and Stone (\nHausknecht and Stone, 2015 )\nintroduced a system known as Deep Recurrent Q-Learning\n(DRQN) as a potential solution to tackle the issue above. The\nproposed modiﬁcation involves substituting the initial fully-\nconnected layer after the convolutional layer in a conventional\nDeep Q-Network (DQN) architecture with a Long Short-Term\nMemory (LSTM) (\nYu et al., 2019 ) layer as shown in Figure 1. In\ncontrast to Deep Q-Networks (DQNs), which rely on ﬁxed-length\nhistories, the Deep Recurrent Q-Network (DRQN) has a recurrent\nstructure that allows for the integration of arbitrarily lengthy\nhistories, enhancing the accuracy of current state estimation.\nDRQN estimation function Q(ot, ht − 1|theta) rather than\nQ(st, at|theta), where theta represents the network parameters, ht −\n1 represents the output of the LSTM layer in the previous step,\nand ht = LSTM(ht − 1, ot). The performance of DRQN on the\nstandard MDP problem is comparable to that of DQN, while DRQN\noutperforms DQN in partially observable domains. The algorithm\nis enhanced by integrating the Recurrent Neural Network (RNN)\nwith the Soft Actor-Critic (SAC) framework. This is achieved by\nutilizing the function Q(ot, ht−1, a|θ ) instead of Q(st, at) to estimate\nthe Q-function, and employing π (at|ht−1, ot, φ) instead of π (at|st)\nto estimate the policy function. This methodology is deﬁned as\nbaseline algorithm.\n/four.tnum./three.tnum DRLNDT architecture\nLet us compare our approach to the baseline method. The\nTransformer layer is included in our approach to incorporate the\nhistorical context, enhancing the accuracy of forecasting the present\nstate. The estimation of the Q function and policy function is\nperformed using the current action. The structural architecture\nof DRLNDT exhibits notable similarities to that of DRQN. This\nresemblance arises from utilizing the Transformer layer to integrate\nhistorical information, adopting time as a positional reference, and\nincorporating the Attention mechanism. These features enable the\nintegration of past and future information at a speciﬁc temporal\npoint, a capability not attainable in RNN networks.\nIn conjunction with the principle of maximum entropy,\nneural networks are employed to approximate the value and\npolicy functions to acquire knowledge about the optimal policy.\nInitially, we present the value network concept, wherein the inputs\nconsist of states and actions. Since the state is unknown and\nwe can acquire observations from the surrounding environment,\nwe must determine the actual state from the observations.\nObservations refer to the data obtained by directly utilizing\nsensors embedded within the autopilot system. On the other\nhand, the state represents what has been learned derived from\npast observations. Due to numerous sensors, the state obtained\nby the autopilot from the surrounding environment in our self-\ndriving car exhibits multimodality. The camera is a primary\nsensor utilized by autonomous vehicles. Autonomous driving aims\nto enable vehicles to navigate their surroundings by utilizing\ncamera-based perception systems, thereby emulating human-like\nimage-based driving capabilities. Images are employed as the\nprincipal state space for multimodal states. The remaining modal\nstates encompass state vectors that include velocity, acceleration,\nposition, and distance relative to the ﬁnal position of the\nautonomous vehicle. Hence, combining the image space and the\nstate vectors constitutes the multimodal state space. Nevertheless,\nwe have identiﬁed an additional issue of using the image as a state\nspace. The utilization of a replay buﬀer necessitates the storage of\nstate information in memory. However, the substantial memory\nrequirements associated with high-quality images, speciﬁcally those\nwith dimensions of 640 ∗ 640, restrict the ability to increase the\nbuﬀer_size beyond a certain threshold. The batch_size parameter\nis employed during the training process in order to enhance the\neﬃciency of training. Furthermore, converting high-quality images\ninto tensors results in a more signiﬁcant memory allocation for\nthe image matrix, thereby imposing greater demands on the GPU\nhardware due to the increased GPU memory consumption.\nConsequently, the utilization of high-quality images does\nnot yield increased eﬃcacy in the process of training. In our\nexperimental evaluations, it was observed that the utilization of\na computing system that only marginally satisﬁes the hardware\nprerequisites results in a notable decrease in the training eﬃciency\nof the model. As an illustration, the computational eﬃciency\nof a 64 ∗ 64 image surpasses that of a 640 ∗ 640 image by\na factor of three, a performance level that does not meet our\nrequirements. Nevertheless, we must continue to explore the\nutilization of high-quality images as a means of perceiving and\ncomprehending the surrounding environment. The utilization\nof high-quality images enables the machine to capture ﬁner\ndetails that may not be discernible to the machine, thereby\nenhancing the intelligence’s ability to perceive the environment\nmore realistically, as compared with low-quality images. To solve\nthe problem of memory consumption by high-quality images, we\nuse V AE to obtain latent states from high-quality images. After\nV AE processing, the latent state is considered by the machine\nto be consistent with the high-quality image. As a result, we\nuse V AE to compress high-quality images with minimal image\nloss, thereby decreasing memory consumption. Therefore, we\ningeniously deduce the latent state and trick the machine into\nbelieving that the latent state is consistent with the image. The\nretention of image information is maximized while minimizing\nmemory consumption. Consequently, our multimodal state space\nFrontiers in Neurorobotics /one.tnum/one.tnum frontiersin.org\nGe et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/three.tnum/eight.tnum/one.tnum/eight.tnum/nine.tnum\nconsists of two states, latent state and vector state, which can be\nobtained easily from the sensor.\nThe fundamental concept underlying V AE is to map the input\ndata into a low-dimensional latent space and reconstruct the\nvectors of the latent space using a decoder into samples similar to\nthe original data. The variational autoencoder (V AE) is comprised\nof two main components, namely an encoder and a decoder.\nThe process of encoding involves mapping the input data to the\nstatistical measures of the mean and variance in the latent space. On\nthe other hand, the decoding process generates novel samples by\nutilizing vectors that are randomly sampled from the latent space.\nThe network architecture of the variational autoencoder (V AE)\nis depicted in\nFigure 2. The system can be primarily categorized\ninto two components, namely, the encoder and the decoder. The\nencoder component is tasked with converting the input image\ninto a latent state representation with reduced dimensions. The\ndimensions of the input image are 640 pixels by 640 pixels, and it\nconsists of three color channels (red, green, and blue). The encoder\nis composed of a sequence of convolutional layers and an activation\nfunction that progressively decreases the spatial dimensions of the\ninput image and captures valuable features. The activation function\nemployed in this process is LeakyReLU. The result of the encoder\nis fed into the Flatten layer in order to obtain a vector with one\ndimension. The dimension of the latent vector is a determining\nfactor for its overall dimensionality. In our research paper, we\ndecided to use a latent vector dimension of 256 in order to strike\na balance between expressive capacity and dimensional eﬃciency.\nThe decoder component transforms the latent vector into the image\nspace, resulting in a reconstructed image that maintains the exact\ndimensions as the original input image. The ultimate layer of the\ndecoder employs a hyperbolic tangent activation function (Tanh)\nto guarantee that the resultant pixel values fall within the range\nof -1 and 1, aligning with the input image’s range. Consequently,\nthe self-driving car engages with the environment to acquire high-\nquality images, which are subsequently processed by the variational\nautoencoder (V AE). These processed images are then extracted as\nlatent vectors, referred to as latent states within our algorithm.\nThe multimodal state space is comprised of the latent and\nvector states, which, in conjunction with the current action, are\nutilized to estimate the current value function and policy function.\nNeural networks are employed to approximate both the value\nfunction and policy function, as depicted in the\nFigures 3, 4.\nillustrating the network architecture for these functions. The\nmultimodal state space is temporally superimposed, encompassing\nhistorical data pertaining to the state space. Consequently, the most\nrecent state space is redeﬁned as the historical state space, thereby\ndistinguishing it from the conventional state space. The concept of\nhistorical state space refers to a sequential arrangement of states\nover time, encompassing both past-to-present time observations\nand a partial trajectory of the past. The time series can be\ncharacterized by the historical latent states, which are derived\nfrom the image state space. Furthermore, by analyzing the state\nspace of the history vector, one can derive the velocity, the\npositional relationship, and other pertinent information. In the\ncontext of multimodal state space, it is possible to establish a\ncorrespondence between the information pertaining to velocity,\nposition, and other relevant variables obtained from an image\nand the corresponding information in the vector space. This\ncorrespondence enables the extraction of features such as velocity\nfrom the image. Extracting features related to velocity and position\nfrom a single state becomes challenging, particularly in the presence\nof occlusion. The algorithmic framework has been transformed\nfrom a Markov Decision Process (MDP) to a Partially Observable\nMarkov Decision Process (POMDP). Hence, it is necessary to\nderive the actual state space from the partially observed historical\nstate space. Experimental proof supports the notion that obtaining\nthe optimal policy is more feasible using the historical state space.\nThe process of extracting the current state from a given\nhistorical context is a topic of interest. The initial step in the\nstructure of the value function and policy function involves\nextracting the current state from the historical data. In the\nimplemented algorithm, a Transformer model is employed to\nextract the current state from the historical data. Subsequently,\na multilayer neural network is employed to approximate both\nthe value function and policy function. The division of the value\nfunction and policy function is comprised of two components.\nThe ﬁrst component is referred to as the transformer module,\nwhich is illustrated in\nFigure 5. The second component is a\nmultilayer neural network, where the activation function employed\nis LeakyReLU. The ﬁnal layer of the policy function incorporates\nthe Tanh activation function in order to conﬁne the output within\nthe range of −1 and 1, aligning with the permissible values for the\noutput action. The output action is composed of two dimensions.\nAction 1 involves a range of values from 0 to −1 for executing a\nleft turn and a range of values from 0 to 1 for executing a right\nturn. Action 2 involves adjusting the brake input from a value\nof 0 to −1 and the throttle input from a value of 0 to 1. The\nresults of the ﬁnal experiments indicate that our proposed method\nexhibits superior performance compared to LSTM’s time series\nprediction in addressing incomplete observations. Additionally, the\nlearned autopilot policy demonstrates better performance than the\nbaseline method.\n/five.tnum Experiment\nTo ascertain the eﬃcacy of our approach in acquiring more\noptimal policies, we conduct a validation of our algorithm within\nthe CARLA (\nDosovitskiy et al., 2017 ) simulation environment. The\nempirical ﬁndings indicate that the DRLNDT algorithm exhibits\nsuperior learning capabilities in deriving optimal policies from\nhistorical data, surpassing both the baseline method and other\npolicy methods that lack access to such data.\n/five.tnum./one.tnum Simulation environment\nGiven the inherent characteristics of reinforcement learning\nalgorithms, it is imperative for self-driving vehicles designed\nto operate autonomously to engage in continuous interaction\nwith their surrounding environment. Due to the high cost and\nlack of security associated with fundamental interactions, we\ncannot train the algorithms in a natural environment using actual\nvehicles for learning. The CARLA (\nDosovitskiy et al., 2017 )\nsimulation environment, which is already established and known\nfor its realistic qualities, is utilized for both training and testing\nFrontiers in Neurorobotics /one.tnum/two.tnum frontiersin.org\nGe et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/three.tnum/eight.tnum/one.tnum/eight.tnum/nine.tnum\nFIGURE /two.tnum\nThe network structure of VAE includes two parts: encoder and de coder.\nFIGURE /three.tnum\nThe ﬁgure shown represents the Actor-Network in the context of the Soft Actor-Critic (SAC) paradigm, which is used to learn policie s. The inputs are\nthe latent states xlatent and vector states xvector, while the outputs are the actions. The latent states are high-q uality images obtained through\nvariational autoencoder (VAE) processing.\nthe algorithm. CARLA is an open-source simulator designed\nfor autonomous driving systems, utilizing the Unreal Engine 4\nplatform for its development. The application oﬀers a practical\nand adaptable three-dimensional setting wherein researchers and\ndevelopers can assess and optimize their algorithms, eliminating\nthe necessity for physical vehicles. The CARLA simulation\nenvironment oﬀers users high-ﬁdelity, authentic urban settings\nthat encompass dynamic traﬃc, pedestrians, diverse weather\nconditions, and a range of road conﬁgurations.\nFurthermore, it provides support for a diverse range of sensor\nmodels, encompassing LIDAR, millimeter wave radar, cameras,\nand other such technologies. The CARLA platform oﬀers a\nmap editor tool that facilitates the creation and modiﬁcation of\ndiverse road networks, buildings, and other components within a\ngiven scene. CARLA additionally oﬀers a comprehensive range of\napplication programming interfaces (APIs) and tools, facilitating\nthe expeditious development and evaluation of autonomous\ndriving algorithms by researchers and developers. Hence, the\nCARLA simulation environment is employed in order to acquire\ninformation and evaluate the DRLNDT algorithm, thereby\nconﬁrming the superiority of our approach over the baseline\nmethod and its capability to acquire a more optimal policy.\n/five.tnum./two.tnum Simulation environment conﬁguration\nThe selected operating environment for CARLA is Ubuntu\n20.04, equipped with a 64GB RAM and an NVIDIA 3090 GPU. This\nconﬁguration fulﬁlls the necessary speciﬁcations for both CARLA\nand our algorithms. Our research team has selected Town10HD\nin CARLA as the designated simulation environment for our\nstudy. This particular environment encompasses a comprehensive\nrepresentation of a town, including a diverse range of buildings\nand road infrastructure. The complexity of the town environment\nis depicted in the high precision map of Town10HD, as illustrated\nin the\nFigure 6. Within the simulated environment, we established\na standardized condition of clear, sunny weather during daylight\nhours. This condition deliberately excludes the presence of fog or\nFrontiers in Neurorobotics /one.tnum/three.tnum frontiersin.org\nGe et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/three.tnum/eight.tnum/one.tnum/eight.tnum/nine.tnum\nFIGURE /four.tnum\nThis ﬁgure depicts the Critic network used in the Soft Actor-Crit ic (SAC) model. Its main purpose is to evaluate the quality of pol icies. The inputs are\nthe latent states xlatent and vector states xvector, while the outputs correspond to Q-values. The latent states refer to high-quality images proces sed\nthrough a variational autoencoder (VAE).\nFIGURE /five.tnum\nThe ﬁgure illustrates the Transformer network’s coding structure .\nThe input is a continuous time series of latent vectors xlatent and\nvector states xvector deﬁned as historical states.\nrain, ensuring that the environmental factors remain unobscured.\nVehicles are allowed to drive from the starting point to the\nﬁnishing point without having to follow established traﬃc rules.\nThe autonomous vehicle has the capability to navigate from its\nstarting point to its destination along any possible path. This\napproach subsequently decreases the regulatory limitations for\nthe autonomous driving system, so enabling it to possess greater\nadaptability and ﬂexibility. The high-precision map is annotated\nwith the starting and ending coordinates. The accomplishment\nof this assignment is readily attainable by current conventional\napproaches. Nevertheless, the development of a self-learning-\nbased intelligent body autonomous driving system poses signiﬁcant\nchallenges. The state space inside urban areas exhibits a high\ndegree of complexity and contains an unlimited dimension. The\nobjective of our endeavor is to enable an autonomous agent\nto obtain the optimal policy and imitate human-like driving\nbehavior only based on camera images and readily available\nstate vectors in an autonomous vehicle. Our study primarily\ncenters around the acquisition of driving skills through human-like\nintelligence, which is crucial for the development of reinforcement\nlearning-based autonomous navigation systems in the context of\nautonomous driving.\nIn CARLA, we conﬁgure the simulated vehicles in accordance\nwith the research requirements. As the foundation for the simulated\nvehicle, we utilized a Tesla model3. A camera with a resolution of\n640*640*3 is installed on the roof in a position directly facing the\ncar, with the purpose of capturing visual data from the area directly\nin front of the vehicle. Similar to human driving, an autonomous\nsystem operates the vehicle by maintaining focus on the road\nahead. Therefore, the camera on the vehicle is able to detect the\nsurrounding environment and acquire the image state of the Agent.\nOther vector states of the Agent, such as velocity and position, can\nbe acquired directly via the vehicle API in CARLA, so additional\nFrontiers in Neurorobotics /one.tnum/four.tnum frontiersin.org\nGe et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/three.tnum/eight.tnum/one.tnum/eight.tnum/nine.tnum\nFIGURE /six.tnum\n(A) Shows the simulation eﬀect of the Town/one.tnum/zero.tnum map in the CARLA simulation environment, portraying various structures. On the other han d, (B)\npresents a highly precise map of Town/one.tnum/zero.tnumHD in Carla. This ﬁgure isannotated to provide information on the initial and end coordinates .\nsensors are not required. Therefore, our algorithm in CARLA\nsimulates autonomous navigation without requiring information\nfrom high-precision maps. As a result, our algorithm does not rely\non high-precision maps and relies mainly on the camera’s acquired\nimages to understand the driving policy, just as humans do. Applied\nto a physical vehicle, as shown in the\nFigure 7, it may be necessary\nto install sensors such as GPS, IMU, and encoder to acquire the\nvehicle’s position and speed information without relying on a high-\nprecision map. Our algorithm reduces the need for high-precision\nmaps, which further reduces the operating costs of the autonomous\ndriving system, and investigates the intelligence of autonomous\ndriving navigation algorithms.\n/five.tnum./three.tnum Training\nNow, it is necessary to train our method in the CARLA\nenvironment in order to learn the optimal policy for navigating\nfrom the initial position to the ﬁnal position by simulating\ninteractions with autonomous vehicles in the environment. First,\nwe must train the V AE model so that it can extract the latent\nstates from the image states. Thus, we guarantee the accuracy of\nthe image data while minimizing memory usage. Then, we devise\nthe reward function, which, instead of restricting the autonomous\nvehicle’s driving behavior, guides it to drive in accordance with\nhuman preferences. In conclusion, we operate the autonomous\nvehicle as an Agent in driving that learns the optimal policy through\ncontinuous interaction with its environment.\n/five.tnum./three.tnum./one.tnum Variational autoencoder training\nInitially, it is necessary to gather the requisite dataset for\ntraining the variational autoencoder (V AE). The primary purpose\nof our V AE model is to analyze the picture data captured by\nthe camera integrated into the autopilot system in order to\nextract the latent state. Hence, it is necessary to utilize a dataset\nconsisting of photographs pertaining to the environmental makeup\nof CARLA towns and cities for the purpose of training the V AE.\nThis approach facilitates enhanced data processing capabilities.\nThere are two distinct methodologies for obtaining data inside\nthe dataset utilized for training V AE. The ﬁrst approach involves\nconﬁguring the autonomous car with a random strategy, enabling\nit to navigate aimlessly from its starting place. This approach\nallows the vehicle to gather information about the town’s unfamiliar\nenvironment. The second approach involves employing an optimal\npolicy to gather data, hence mitigating the risk of data imbalance\nand insuﬃcient destination data. Given our knowledge of the\nstarting and ending positions, the best strategy can be readily\ncomprehended by human beings, allowing for straightforward\nevaluation and implementation using conventional approaches.\nThe acquired data is stored in an oﬄine local storage. Subsequently,\nthe data from both ways is combined to create a uniﬁed dataset.\nRandom sampling is then employed from this dataset to train the\nV AE model. The size of the collected dataset was determined to\nbe 32,768, while the batch_size was set to 16 in order to meet the\ntraining requirements. The training process was terminated after\nreaching a total of 100,000 training cycles. The mean squared error\n(MSE) is computed in order to satisfy our speciﬁed criteria. The\nexperimental ﬁndings are depicted in\nFigure 8. The V AE produced\nfrom our training is utilized to extract the latent state, which aims\nto minimize memory usage while preserving the original features\nto the greatest extent possible.\n/five.tnum./three.tnum./two.tnum Reward design\nAutonomous navigation algorithms for autonomous vehicles\nthat are based on reinforcement learning diverge from traditional\nmethods. This approach enables an Agent to autonomously\nacquire information by engaging with the environment and\ndetermining the optimal policy through the utilization of rewards\nobtained from the environment. Hence, this algorithmic technique\ndiverges signiﬁcantly from the conventional methodology, wherein\npredeﬁned rules and logic are established to govern the behavior\nof autonomous vehicles in order to execute tasks. This method\nFrontiers in Neurorobotics /one.tnum/five.tnum frontiersin.org\nGe et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/three.tnum/eight.tnum/one.tnum/eight.tnum/nine.tnum\nFIGURE /seven.tnum\nThe ﬁgure illustrates the arrangement of sensors on a practical v ehicle. The sensors utilized in this system primarily consist of a camera, ultrasonic\nsensor, speed encoder, Inertial Measurement Unit (IMU), and coll ision sensor.\nFIGURE /eight.tnum\nThe variational autoencoder (VAE) experimental results are pr esented in the ﬁgure. (A) Shows the input image, which has a resolution of /six.tnum/four.tnum/zero.tnum*/six.tnum/four.tnum/zero.tnum and\nis of excellent quality. On the other hand, (B) illustrates the output image of good quality with the same resoluti on of /six.tnum/four.tnum/zero.tnum*/six.tnum/four.tnum/zero.tnum.\nis mechanical and needs more intelligence. Nevertheless, our\nmethodology entails directing the Agent to execute our objectives\nthrough the design of rewards rather than restricting it solely\nto the ﬁnal result. This strategy enhances the Agent’s level of\nunpredictability and intelligence. The system is capable of making\nvaried decisions in response to diverse environmental conditions.\nIt is also the reason why the optimal policies acquired using these\nalgorithms possess the capacity to generate behavioral strategies\nthat exceed those exhibited by human drivers. This cognitive\napproach can be employed to foster innovative and unconventional\nthinking beyond the conventional boundaries of human cognition.\nThe reward function was designed with the purpose of\nproviding guidance to autonomous driving systems in order to\nsuccessfully accomplish the task of navigating from the starting\npoint to the ending position. The primary objective of the\nnavigation challenge is for the autonomous vehicle to successfully\narrive at the designated end location. Upon reaching the designated\ndestination, the autonomous vehicle will be rewarded with a reward\nthat is denoted as rsuccess = + 1 for successfully completing\nits task. Simultaneously, upon the successful completion of the\nmission, the episode concludes, marking the ﬁnish of a training\ncycle. In the event of a collision involving the vehicle, the Agent\nwill receive a negative reward denoted as rcollision = − 3. This\nreward is implemented as a preventive measure to discourage\nthe Agent from engaging in collisions. Simultaneously, the task is\nunsuccessful, resulting in the conclusion of the episode and the\nfulﬁllment of the training cycle. Subsequently, in order to guarantee\nthe duration of the training episode, the training episode concludes\nwhen the duration of the episode surpasses the value of n. The\nAgent is subjected to a negative reward denoted as rlong_time =\n−1. The three rewards that have been formulated thus far are\ncharacterized as sparse rewards, and the task of training optimal\npolicies using these sparse rewards poses signiﬁcant challenges.\nHence, an additional form of reward, known as a dense reward in\ncontrast to a sparse reward, is introduced, which the Agent obtains\nat every timestep. The distance potential reward is formulated as\nFrontiers in Neurorobotics /one.tnum/six.tnum frontiersin.org\nGe et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/three.tnum/eight.tnum/one.tnum/eight.tnum/nine.tnum\nthe diﬀerence between the vehicle’s distance from the endpoint\nat the present timestep and its distance from the endpoint at the\npreceding timestep as shown in\nEquation (15).\nrpotental\n= abs(locend(t)−locvehicle(t))−abs(locend(t−1)−locvehicle(t−1))\n(15)\nThis observation suggests that a positive reward is provided as\nthe vehicle approaches the destination, while a negative reward is\nadministered when the vehicle remains far from the destination.\nLikewise, it is desired for the autonomous vehicle to operate within\na velocity range of 25–50 km per hour as shown in\nEquation (16).\nrvelocity =\n\n\n\n\n\n\n\nv − 25, if v > 25\nv − 25, if 25 ≤ v ≤ 50\n50 − v if v > 50\n(16)\nThe variable v represents the current velocity of the vehicle,\nwith the rewards explicitly designed to meet the speed requirements\nof autonomous driving. Our overall reward is as follows:\nr = c1rsuccess +c2rcollision +c3rlong_time +c4rpotental +c5rvelocity (17)\nThe reward coeﬃcient, denoted as c1 through c5, is employed to\nachieve a balance among the individual rewards in\nEquation (17).\n/five.tnum./three.tnum./three.tnum DRLNDT training\nAutonomous vehicles were trained interactively within the\nCARLA simulation environment, wherein they relied on a random\npolicy to navigate the simulated world guided by rewards, the\nautonomous vehicle endeavors to execute the given navigation\ntask successfully. Once the task either fails or succeeds, the\nepisode of interaction ﬁnishes, prompting the start of a new\nepisode. Simultaneously, the Agent evaluates and updates the value\nfunction and policy function by utilizing the information collected.\nThe Agent will engage in interactions with the environment in\naccordance with the updated policy. The Agent acquires the\noptimal policy through engaging in ongoing interaction and\nultimately achieves task completion.\nDuring its travel, the autonomous vehicle gathers data\npertaining to various states. The autonomous vehicle’s motion\ninduces modiﬁcations in the surrounding environment, resulting\nin the acquisition of new states. Initially, the autonomous\nvehicle acquires visual data and vector-based representations\nencompassing parameters such as velocity, position, and other\nrelevant variables. Subsequently, the image input undergoes\nprocessing by a V AE in order to acquire latent states. These\nlatent states are then merged with vector states to create a\nnovel multimodal state. Subsequently, the state needs to undergo\ntemporal serialization. In the academic domain of reinforcement\nlearning, it is more appropriate to use the term “observation”\ninstead of “state” to denote the concept being discussed. The\nconﬂation of observation and state occurs when the collected\nobservation is indistinguishable from an actual state. As a result,\nin academic research, these two terms are often conﬂated for\nsimpliﬁcation. Nevertheless, in practical use, the data gathered\nTABLE /one.tnumHyperparameter list.\nHyperparameters Value Description\nCamera resolution 640*640*3 Dimension of the input image\nDim_latent_vector 256 Dimension of the latent_vector\nvae_lr 1.00E-04 V AE learning rate\nBatch_size_vae 16 Batch size in V AE training\nTrain_data_size 65,536 Number of V AE training sets\nn 50 Length of the history state sequence\nHead_num 6 Number of heads in Transfomer\nDim_head 64 Dimension of head in Transfomer\nDim_mlp 512 Dimensions of MLPs in Transfomer\nBuﬀer_size 65,536 Buﬀer size in SAC\nlr 1e-3 1e-4 Learn rate in SAC\nγ 0.99 Discount rate in SAC\nbatch_size 128 Batch size in SAC\nby the Agent is often masked and lacks completeness. At the\nsame time, the actual state remains concealed within the historical\nrecord of observations from before up until the present moment.\nThe current state is derived from the sequence of observations\nusing a transformer, allowing the Agent to acquire an improved\npolicy. This characteristic represents the eﬃcacy of our algorithm.\nWe next need to historicize the state, or what can also be called\nthe time serialization of the state. Serialization of the state adds\na temporal dimension to the data. We consider the historical\ncontext of the present situation. The historical experience replay\nis a memory storage mechanism that stores a quintuple, denoted\nas < sh\nt , at, rt, sh\nt+1, done >. In this quintuple, sh\nt represents a set\nof historical states from ot−n to ot, at represents the action taken\nat the current moment, rt represents the reward acquired at the\ncurrent moment, sh\nt+1 represents a set of historical states from\not+1−n to ot+1, and done represents the end-of-episode marker. The\napproach employs a random sampling technique to select tuples\nfrom a historical experience replay. These selected tuples are then\nused to update both the value function and the policy function.\nThe training process adheres to the hyperparameters described\nin the\nTable 1. Speciﬁc hyperparameters are selected based on\nthe optimum results reported in the paper, while others are\ndetermined through repeated training experiments to identify the\nmost favorable ﬁndings.\n/five.tnum./four.tnum Experimental results and analysis\nThe experiments primarily focus on evaluating the\nperformance of our algorithm, DRLNDT, in achieving autonomous\nvehicle navigation from the starting point to the ﬁnal destination.\nAdditionally, we compare the experimental outcomes with those\nobtained using the baseline approach. The primary concept of\nthe baseline technique involves utilizing a Recurrent Neural\nNetwork (RNN) to extract the latent state information from\nthe previous states. Three evaluation standards are primarily\nFrontiers in Neurorobotics /one.tnum/seven.tnum frontiersin.org\nGe et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/three.tnum/eight.tnum/one.tnum/eight.tnum/nine.tnum\nutilized to assess the algorithm in comparison to the baseline\nalgorithm during both the training and evaluation stages. The\nthree evaluation metrics includeultra mean_lens, mean_rewards,\nand success_rate. The evaluation measure, indicated as mean_lens,\nquantiﬁes the length of the sequence of the average episode,\nspeciﬁcally measuring the average time spent at the conclusion of\neach episode. A negative correlation exists between the magnitude\nof the mean_lens and the time required to successfully perform the\ntask. The evaluation metric means_rewards quantiﬁes the average\nprizes acquired by the Agent after each episode on average. In the\ncontext of task completion, a more excellent value for the variable\nmeans_rewards signiﬁes a heightened reward associated with\nthe task. Consequently, this implies that the algorithm exhibits a\ngreater level of eﬀectiveness. The variable success_rate represents\nthe proportion of successful episodes in achieving the task relative\nto the total number of episodes. It is considered the primary\nstatistic for assessing the algorithm’s eﬃcacy. In our experimental\nstudy, we emphasize the investigation of reinforcement learning\nalgorithms, while ignoring the assessment indicators pertaining to\nsafety, comfort, and other relevant aspects of autonomous driving.\nThere exist variations among these three indicators during the\ntraining and evaluation stages. During the training phase, our\nprimary criterion for determining the end of training depended on\nthe total number of time steps. During the evaluation phase, we\nanalyzed the metrics statistically following 500 consecutive episode\nruns. Multiple sets of control experiments were conducted using\nthe hyperparameters trained as listed in\nTable 1. The outcomes\nof these experiments are shown in Figures 9, 10, 11. The primary\nobjective is to compare our DRLNDT method, the typical SAC\nalgorithm, and the baseline algorithm to discern our algorithm’s\nspeciﬁc advantages while utilizing identical parameters. The data\nresults are shown in\nTables 2, 3.\nThe network was evaluated, and it was found that the DRLNDT\nmethod outperformed the Baseline algorithm and the Standard\nSAC algorithm. The hyperparameters were adjusted based on the\nvalues in\nTable 1. The evaluation process was divided into two\nstages - the training phase and the assessment phase. The results\nof the training and evaluatingphase are presented in\nFigure 9.\nWe use three metrics to measure the algorithm’s performance:\nmean_len, mean_reward, and success_rate. After training for 1,000\ntimesteps, the algorithm is evaluated ten times in a row. The\nperformance metrics during the evaluation phase are calculated\nbased on these assessments. It is intriguing to observe that\nDRLNDT performs superior to the other two measures in the\nmean_len metric. However, it notably exhibits inferior performance\nin the success_rate and mean_reward metrics compared to the\nother algorithms. We are profoundly contemplating this matter,\nwhich raises the question of whether our algorithm fails to enhance\nnavigation performance. The provided response is incorrect; our\nalgorithm demonstrates superiority over both algorithms. The\nprimary metric of interest is the success_rate, which should be\nprioritized when evaluating the algorithm’s ability to navigate\nsuccessfully. The challenge of implementing navigation tasks using\nreinforcement learning algorithms arises from the intricate action\nand state spaces involved. Our algorithm performed exceptionally\nwell and produced the model with the highest reward, which was\nused for subsequent testing. It is important to note that during\nthe training phase and midway through it, the metrics obtained\nmay not accurately represent the algorithm’s overall performance\nbut rather oﬀer insights into the eﬀectiveness of the training\nprocess. The stored model was deployed in the experimental\nenvironment for 200 testing episodes, during which various\nstatistics such as mean episode length, mean episode reward,\nand success rate were computed and analyzed. The data results\nare presented in a tabular form, indicating that the evaluation\nsteps depicted in the table diﬀer from the outcomes illustrated\nin\nFigure 9. The table shows that the algorithm’s performance\nstrongly correlates with the observed results. Speciﬁcally, the\nalgorithm has a success rate of 99.9%, which is higher than\nthat of other algorithms. Additionally, our method achieves a\nhigher average reward than alternative algorithms. Furthermore,\nthe algorithm’smean_len metric is comparatively lower than others.\nOur algorithm has outperformed other algorithms, demonstrating\nits ability to learn the actual state from historical data. This\nmethod is particularly evident in our use of a transformer,\nwhich makes extracting the actual state from historical data\nmore eﬃcient than other algorithms. The transformer model\ncan incorporate information from preceding and subsequent\nmoments, thereby enabling the integration of a speciﬁc moment\nwith its preceding and subsequent data. In contrast, recurrent\nneural networks (RNNs) can only establish relationships between\na speciﬁc moment and its preceding moment. Consequently, the\ntransformer model exhibits superior proﬁciency in integrating\nhistorical information compared to RNNs. The performance of\nthe baseline algorithm was compared with that of the standard\nSAC algorithm, and it was found that the former showed a\nweaker eﬀect than the latter. It could be due to the suboptimal\nperformance of the RNN in extracting historical state information\nand its computational overhead. As a result, the standard SAC\nalgorithm had a more pronounced eﬀect than the baseline\nalgorithm.\nDuring the experiments, it was noticed that the variable\nn, which represents the duration of history, holds signiﬁcant\nimportance as it determines the temporal extent of the time\nseries data. It is worth noting that the ﬁxed time series were\nused for extracting the actual state instead of selecting variable\ntime series. Choosing the correct length for a time series\nis critical for the algorithm. The algorithm may accurately\ncapture underlying patterns if there is limited historical data.\nOn the other hand, an excessive amount of historical data\ncan cause the algorithm to consume too much memory and\nnegatively impact its performance. As depicted in the\nFigure 10,\nwe establish a range for the variable n, speciﬁcally from 10\nto 80. Subsequently, we do a single training session at regular\nintervals of 10 units to ascertain the most favorable value for\nn. The ﬁgure demonstrates that the algorithm exhibits optimal\nperformance at n = 50. Additionally, it is evident from the\nﬁgure that at n = 50, the outcomes tend to reach a state of\nstability characterized by the absence of discernible ﬂuctuations.\nExtending the training duration and period was found to be\nunfeasible as it resulted in oscillatory behavior and suboptimal\nresults. When n ranges from 40 to 70, the success rates of\nthe experimental outcomes are nearly identical, reaching up to\n99%. The curves in the photos show a high degree of similarity.\nFurthermore, when evaluating the relationship between the average\nreward and the episode’s duration, it becomes apparent that\nFrontiers in Neurorobotics /one.tnum/eight.tnum frontiersin.org\nGe et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/three.tnum/eight.tnum/one.tnum/eight.tnum/nine.tnum\nFIGURE /nine.tnum\nThe ﬁgure presented illustrates the experimental outcomes of t he DRLNDT method as compared to both the baseline and standard So ft Actor-Critic\n(SAC) algorithms throughout both the training and evaluation phase s. The results provide evidence supporting the superiority of t he DRLNDT\nalgorithm over the other techniques. (A) Depicts the evaluation procedure, whereas (B) illustrates the training process. The chart’s x-axis displays\ntimesteps, while the y-axis shows mean_len, mean_reward, and success_rate.\nthe optimal outcome is achieved when n equals 50. It is still\npossible to achieve good results even when n is ∼50. It is\nworth mentioning that while evaluating the tests, there were\nminor diﬀerences in the speed of autonomous driving and\na slight deviation from the ideal path regarding the traveled\ntrajectory. To summarize, the Transformer-based method for\nextracting historical states performs better than other techniques.\nAdditionally, the algorithm’s eﬀectiveness is closely associated with\nthe length of the historical context. Speciﬁcally, the best results\nare achieved when n = 50. However, it should be noted that\nacceptable outcomes can still be attained within the range of the\nn = 50 parameter.\nFrontiers in Neurorobotics /one.tnum/nine.tnum frontiersin.org\nGe et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/three.tnum/eight.tnum/one.tnum/eight.tnum/nine.tnum\nFIGURE /one.tnum/zero.tnum\nThe ﬁgure illustrates the training and assessment outcomes obta ined for various durations of history states. These history state s are employed to\nelucidate the impact of varying lengths on the algorithms. (A) Depicts the evaluation procedure, whereas (B) illustrates the training process. The\nﬁgure compares the results of the algorithms for time lengths rangin g from n = /one.tnum/zero.tnum ton = /eight.tnum/zero.tnum.\nGenerating reward weights can be a challenging task. The\nprimary diﬃculty is determining the speciﬁc weights assigned to\nindividual rewards, particularly the weight coeﬃcient associated\nwith potential_reward. An ablation experiment was conducted\nfurther to investigate the impact of the weight coeﬃcient of\npotential_reward. The experiment consisted of ﬁve test groups,\neach with a diﬀerent weight coeﬃcient (0.125, 0.175, 0.2, 0.25,\nand 0.5). The weight coeﬃcient was incrementally increased;\nthe results are illustrated in\nFigure 11. The results of the\nexperiments are summarized in the Tables 2, 3. The statistical\nanalysis showed that setting the weight coeﬃcient to 0.2 led to\nthe best outcome. This ﬁnding highlights the importance of the\nhyperparameter potential_reward in guiding the autopilot task.\nNotably, potential_reward acts as a dense reward and is crucial\nFrontiers in Neurorobotics /two.tnum/zero.tnum frontiersin.org\nGe et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/three.tnum/eight.tnum/one.tnum/eight.tnum/nine.tnum\nFIGURE /one.tnum/one.tnum\nThe ﬁgure illustrates the training and assessment outcomes obta ined by varying the potential_reward_weight parameter. This analysis aims to\ninvestigate the impact of diﬀerent potential_reward_weight values on the algorithm’s performance. (A) Depicts the evaluation procedure, whereas\n(B) illustrates the training process. Speciﬁcally, the algorithm ic results are compared across a range of potential_reward_weight values, ranging from\n/zero.tnum./one.tnum/two.tnum/five.tnum to /zero.tnum./five.tnum.\nin guiding the Agent to complete the task and acquire the\noptimal policy. This is particularly important because training\nthe optimal policy for autopilot navigation is challenging due to\nthe inﬁnite-dimensional state and action space. In this context,\npotential_reward is the only guiding factor for the Agent to\ncomplete the navigation task instead of relying on sparse rewards\nThrough the evaluation phase, it was observed that increasing\nthe weight of potential_reward leads to favorable outcomes\nfor the algorithm. However, during the training phase, it was\ndiscovered that large weights hinder the learning process, causing\nthe algorithm to converge locally. It is demonstrated by the\nautonomous vehicle repeatedly circling in place or remaining\nFrontiers in Neurorobotics /two.tnum/one.tnum frontiersin.org\nGe et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/three.tnum/eight.tnum/one.tnum/eight.tnum/nine.tnum\nTABLE /two.tnumComparison of experimental training processes.\nPolicy Train Description\nMean_length Mean_reward Success_rate\nDRLNDT 160.0993 46.3408 0.128 Transformer + SAC\nBaseline 192.1297 70.02924 0.623 RNN + SAC\nStandard SAC 251.9917 77.8219 0.713\nDRLNDT-n-10 152.487 42.1571 0.1642 Historical state length n = 10\nDRLNDT-n-20 77.869 13.363 0.0009 Historical state length n = 20\nDRLNDT-n-30 75.24716 21.7094 0.0195 Historical state length n = 30\nDRLNDT-n-40 85.8629 14.1125 0.024 Historical state length n = 40\nDRLNDT-n-60 107.8153 45.3691 0.1809 Historical state length n = 60\nDRLNDT-n-70 133.1698 51.724 0.1999 Historical state length n = 70\nDRLNDT-n-80 179.1541 16.9834 0.00982 Historical state length n = 80\nDRLNDT-w-0.125 105.7095 8.6156 0 Potential_reward_w = 0.125\nDRLNDT-w-0.175 231.223 27.2584 0.0476 Potential_reward_w = 0.175\nDRLNDT-w-0.25 107.2249 39.07225 0.1076 Potential_reward_w = 0.25\nDRLNDT-w-0.5 111.2946 44.93245 0.139 Potential_reward_w = 0.5\nTABLE /three.tnumComparison of experimental evaluation processes.\nPolicy Evaluation Description\nMean_length Mean_reward Success_rate\nDRLNDT 110.055 80.3718 0.99502488 Transformer + SAC\nBaseline 181.065 54.3497 0.59701493 RNN + SAC\nStandard SAC 218.16 65.6080 0.7312\nDRLNDT-n-10 314.845 27.8312 0.398 Historical state length n = 10\nDRLNDT-n-20 179.08 20.5407 0 Historical state length n = 20\nDRLNDT-n-30 175.09 24.3238 0 Historical state length n = 30\nDRLNDT-n-40 110.12 80.3325 0.99502488 Historical state length n = 40\nDRLNDT-n-60 108.025 80.9850 0.995 Historical state length n = 60\nDRLNDT-n-70 140.1 80.7300 0.995 Historical state length n = 70\nDRLNDT-n-80 155.255 14.7500 0 Historical state length n = 80\nDRLNDT-w-0.125 166.01 31.7600 0 Potential_reward_w = 0.125\nDRLNDT-w-0.175 359.04 80.3810 0.99502 Potential_reward_w = 0.175\nDRLNDT-w-0.25 110.015 80.5790 0.99502 Potential_reward_w = 0.25\nDRLNDT-w-0.5 119.025 80.7813 0.99502 Potential_reward_w = 0.5\nModular Pipi 1113 83.0000 0.99\nstationary, resulting in its inability to reach the optimal policy. In\nthe middle of the training, we thought that we would not be able to\ntrain a good policy, but after saving the model with the maximum\nreward, we found that it still showed good results with a success rate\nof 99%, which we did not expect in our experiments. Therefore,\nthe weight of potential_reward should be a manageable size, and\nafter comparing the length of the episodes and the average reward,\na weight of 0.2 was the best choice.\nOur DRLNDT algorithm has outperformed both the baseline\nand other algorithms based on the comparison experiments\nconducted. This algorithm eﬀectively guides an autonomous\nvehicle from its initial to the termination position while\nsimultaneously learning the optimal policy. During the evaluation\nphase, we saved the model with the highest rewards and utilized it\nfor assessment. After conducting 200 consecutive testing episodes,\nwe achieved a notable success rate of 99%. Furthermore, a\nFrontiers in Neurorobotics /two.tnum/two.tnum frontiersin.org\nGe et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/three.tnum/eight.tnum/one.tnum/eight.tnum/nine.tnum\ncomparison was made between the traditional modular approach\nand our algorithm. The results indicate that the traditional modular\n(\nPaden et al., 2016 ) algorithm does not yield superior outcomes,\nprimarily due to incorrect routing during the navigation phase,\nresulting in erroneous trajectory routes. It highlights the advantage\nof our policy, as the interdependence of modular navigation\ntasks can lead to failure or reduced eﬀectiveness when routing\nerrors occur. The experimental results indicate that the DRLNDT\napproach is a successful solution for resolving the problem of state\nunobservability in the POMDP model. This method conceals the\ncurrent state by incorporating it into the historical state, utilizing\nthe Transformer model. It is worth noting that the Transformer\nmodel performs better than the LSTM-based RNN algorithm in\nterms of eﬀectiveness.\nWe compare the computational performance of the methods\nand evaluate their performance in the evaluation phase based on\nresponse time. The\nTable 4 displays the mean response time of each\nmethod during the evaluation portion of the examination, which is\nmeasured in milliseconds. The computation time is determined by\nexecuting the algorithms in the Python framework. The response\ntime indicates the algorithm’s decision-making capability, which\ninﬂuences its capacity to handle unforeseen circumstances. A faster\nresponse time can enhance the safety of autonomous driving during\nemergencies. The classic modular method has the quickest reaction\ntime, while the DRLNDT algorithm has the longest response time\namong the other algorithms. The 7 ms diﬀerence in reaction\ntime between the two frameworks is insigniﬁcant for practical\npurposes, especially considering that the response time in the C++\nframework is much shorter. The DRLNDT algorithm demonstrates\nsuperior performance compared to other algorithms across all\nmetrics. The algorithm we are comparing is a simpliﬁed version\nthat relies on a detailed map with pre-marked data on intersections,\ntraﬃc lights, and obstacles for navigation. The modular approach\nis aware of the knowledge of surrounding barriers beforehand,\nbut in real-world scenarios, it must be combined with sensing,\nlocalization, routing modules, and so forth. The navigation process\nin actual applications will be delayed by the inclusion of a sensor\nmodule, localization module, and other components, leading to\nincreased response time. Traditional approaches are slower in\nresolving complicated situations due to the rise in constraints. Our\nmethod relies on an end-to-end reinforcement learning algorithm\nand does not impact response time despite the environment’s\ncomplexity. The Transformer model’s beneﬁt lies in its ability to\nhandle each location in the sequence simultaneously, leading to\na notable enhancement in training speed and performance. Its\nself-attentive mechanism allows it to capture both long-distance\ndependencies and local structural and positional information in\nthe sequence simultaneously. As a result, by incorporating a\ntransformer into our DRLNDT algorithm, historical information\ncan be incorporated to extract the actual state, thereby enhancing\nthe algorithm’s decision-making capability, resolving the issue of\nunobservable states in the POMDP model, and producing an\noutcome that surpasses that of the baseline algorithm. The intricate\ndesign of the transformer model results in a high requirement\nfor computer resources during training and inference, impacting\nthe reaction time during actual deployment. The V AE in our\nDRLNDT algorithm retrieves latent states from the source image\nTABLE /four.tnumExperimental evaluation process on response time comparison.\nPolicy Response_time Description\nDRLNDT 7.4250 Transformer + SAC\nBaseline 2.8370 RNN + SAC\nStandard SAC 2.7200\nDRLNDT-n-10 7.1830 Historical state length n = 10\nDRLNDT-n-20 8.1440 Historical state length n = 20\nDRLNDT-n-30 5.7540 Historical state length n = 30\nDRLNDT-n-40 7.4040 Historical state length n = 40\nDRLNDT-n-60 7.4030 Historical state length n = 60\nDRLNDT-n-70 7.4046 Historical state length n = 70\nDRLNDT-n-80 7.7540 Historical state length n = 80\nDRLNDT-w-0.125 7.4350 Potential_reward_w = 0.125\nDRLNDT-w-0.175 7.4130 Potential_reward_w = 0.175\nDRLNDT-w-0.25 7.5120 Potential_reward_w = 0.25\nDRLNDT-w-0.5 7.3780 Potential_reward_w = 0.5\nModular Pipi 1.3000\nto preserve its features while lowering the dimensionality of other\nimages. Our technique reduces the amount of parameters in the\nTransformer model. Our real-world experiments showed that the\nmodel had fewer than 100 million parameters, making it suitable\nfor deployment in real-world settings. Our approach, being end-\nto-end based, optimizes the hardware performance dedicated to\ndecision-making. The response time remains unaﬀected by the\nheavy demand for processing resources.\n/six.tnum Conclusion and future\nThis paper provides a comprehensive overview of the DRLNDT\nalgorithm for autonomous vehicle navigation. The paper also\ndiscusses the experimental outcomes obtained by implementing\nour algorithm on the CARLA platform. The results substantiate\nthe superiority of our approach over the baseline approach. The\navailable evidence adequately supports the eﬃcacy of our approach.\nWe have employed the Transformer model in our research to\naddress the issue of incomplete observation of the state in POMDP\ndue to sensor occlusion or noise in autonomous driving. We aim\nto learn the real state from the historical state with the help of\nthis model. We have successfully developed an optimal policy for\nautonomous driving, enabling the vehicle to navigate from the\nstarting to the termination position.\nOur algorithm has achieved an impressive 99% success rate\nin a complex state and action space task using only high-quality\nmonocular images without any prior knowledge of high-precision\nmaps, routing, or surrounding environment information. This\noutcome is a testament to the eﬀectiveness of our optimal policy.\nDespite our best eﬀorts, implementing our system in actual\nvehicles has proven to be a challenging task. While our problem\nFrontiers in Neurorobotics /two.tnum/three.tnum frontiersin.org\nGe et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/three.tnum/eight.tnum/one.tnum/eight.tnum/nine.tnum\nis considered highly challenging in the ﬁeld of reinforcement\nlearning, it is relatively straightforward when compared to the\ncomplexities of real-world self-driving navigation tasks. The\ncurrent system needs to be more eﬀective in handling intricate\nnavigation challenges, including randomized starting and ending\npositions, which require additional algorithmic improvements.\nBased on our analysis, we have identiﬁed that the need for\nappropriate incentives designed by humans is the primary cause of\nthe issue. To overcome this challenge, we plan to focus our future\nresearch on reward design for self-driving navigation tasks. We\nwill use the active preference learning approach to gain knowledge\nabout the reward weights associated with complex human needs.\nAdditionally, we intend to explore areas such as learning the reward\nfunction from expert data through inverse learning. These two focal\npoints will be the primary areas of investigation for our future\nresearch projects.\nData availability statement\nThe original contributions presented in the study are included\nin the article/supplementary material, further inquiries can be\ndirected to the corresponding author.\nAuthor contributions\nLG: Investigation, Methodology, Software, Writing – original\ndraft, Writing – review & editing. XZ: Resources, Supervision,\nWriting – review & editing. YW: Data curation, Funding\nacquisition, Resources, Writing – review & editing. YL: Writing –\nreview & editing.\nFunding\nThe author(s) declare ﬁnancial support was received for\nthe research, authorship, and/or publication of this article.\nThis study was supported by the 111 Project, Grant/Award\nNumber: B08004.\nConﬂict of interest\nYL was employed by Mogo Auto Intelligence and Telematics\nInformation Technology Co., Ltd. YW was employed by Neolix\nTechnologies Co., Ltd.\nThe remaining authors declare that the research was conducted\nin the absence of any commercial or ﬁnancial relationships that\ncould be construed as a potential conﬂict of interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their aﬃliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nReferences\nAndrychowicz, M., Raichuk, A., Sta ´nczyk, P., Orsini, M., Girgin, S., Marinier, R.,\net al. (2020). What matters in on-policy reinforcement learning ? A large-scale empirical\nstudy. arXiv. [Preprint]. doi: 10.48550/arXiv.2006.05990\nAnzalone, L., Barra, P., Barra, S., Castiglione, A., and Nappi, M . (2022). An end-\nto-end curriculum learning approach for autonomous driving sce narios. IEEE Trans.\nIntell. Transp. Syst . 23, 19817–19826. doi: 10.1109/TITS.2022.3160673\nArulkumaran, K., Deisenroth, M. P., Brundage, M., and Bharat h, A. A.\n(2017). A brief survey of deep reinforcement learning. arXiv. [Preprint].\ndoi: 10.48550/arXiv.1708.05866\nChen, J., Yuan, B., and Tomizuka, M. (2019). “Model-free deep\nreinforcement learning for urban autonomous driving, ” in 2019 IEEE Intelligent\nTransportation Systems Conference (ITSC) (Aucklan: IEEE), 2765–2771.\ndoi: 10.1109/ITSC.2019.8917306\nChen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., et al. (2021).\nDecision transformer: reinforcement learning via sequence m odeling. Adv. Neural Inf.\nProcess. Syst 34, 15084–15097.\nChoromanski, K., Likhosherstov, V., Dohan, D., Song, X., Ga ne, A., Sarlos,\nT., et al. (2020). Rethinking attention with performers. arXiv. [Preprint].\ndoi: 10.48550/arXiv.2009.14794\nDing, M., Zhou, C., Yang, H., and Tang, J. (2020). Cogltx: applyin g bert to long texts.\nAdv. Neural Inf. Process. Syst . 33, 12792–12804.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D. , Zhai, X., Unterthiner,\nT., et al. (2020). An image is worth 16x16 words: transformers for image recognition at\nscale. arXiv. [Preprint]. doi: 10.48550/arXiv.2010.11929\nDosovitskiy, A., Ros, G., Codevilla, F., Lopez, A., and Koltun, V. (2017). “CARLA:\nan open urban driving simulator, ” in Proceedings of the 1st Annual Conference on\nRobot Learning , eds S. Levine, V. Vanhoucke, and K. Goldberg (Mountain View, C A:\nPMLR), 1–16. Available online at: http://proceedings.mlr.press/v78/dosovitskiy17a/\ndosovitskiy17a.pdf\nGhosh, D., Rahme, J., Kumar, A., Zhang, A., Adams, R. P., Levi ne, S., et al.\n(2021). Why generalization in rl is diﬃcult: epistemic pomdps and implicit partial\nobservability. Adv. Neural Inf. Process. Syst . 34, 25502–25515.\nGonzález, D., Pérez, J., Milanés, V., and Nashashibi, F. (2015) . A review of motion\nplanning techniques for automated vehicles. IEEE Trans. Intell. Transp. Syst . 17,\n1135–1145. doi: 10.1109/TITS.2015.2498841\nHaarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., T an, J.,\net al. (2018). Soft actor-critic algorithms and applications. arXiv. [Preprint].\ndoi: 10.48550/arXiv.1812.05905\nHausknecht, M., and Stone, P. (2015). “Deep recurrent q-learn ing for partially\nobservable MDPS, ” in2015 AAAI Fall Symposium Series .\nHeess, N., Hunt, J. J., Lillicrap, T. P., and Silver, D. (2015). Me mory-based control\nwith recurrent neural networks. arXiv. [Preprint]. doi: 10.48550/arXiv.1512.04455\nIgl, M., Zintgraf, L., Le, T. A., Wood, F., and Whiteson, S. (20 18). Deep variational\nreinforcement learning for pomdps. Proc. 35th Intl. Conf. Machine Learn. Proc. Mach.\nLearn. Res. 80, 2117–2126.\nJanner, M., Li, Q., and Levine, S. (2021). Oﬄine reinforceme nt learning as one big\nsequence modeling problem. Adv. Neural Inf. Process. Syst . 34, 1273–1286.\nKaelbling, L. P., Littman, M. L., and Cassandra, A. R. (1998). P lanning\nand acting in partially observable stochastic domains. Artif. Intell . 101, 99–134.\ndoi: 10.1016/S0004-3702(98)00023-X\nKendall, A., Hawke, J., Janz, D., Mazur, P., Reda, D., Allen, J.-M ., et al. (2019).\n“Learning to drive in a day, ” in 2019 International Conference on Robotics and\nAutomation (ICRA) (IEEE), 8248–8254. doi: 10.1109/ICRA.2019.8793742\nKiran, B. R., Sobh, I., Talpaert, V., Mannion, P., Al Sallab, A. A. , Yogamani,\nS., et al. (2021). Deep reinforcement learning for autonomous driving: a\nsurvey. IEEE Trans. Intell. Transp. Syst . 23, 4909–4926. doi: 10.1109/TITS.2021.\n3054625\nFrontiers in Neurorobotics /two.tnum/four.tnum frontiersin.org\nGe et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/three.tnum/eight.tnum/one.tnum/eight.tnum/nine.tnum\nLiang, X., Wang, T., Yang, L., and Xing, E. (2018). “CIRL: con trollable\nimitative reinforcement learning for vision-based self-dri ving, ” in Proceedings of\nthe European Conference on Computer Vision (ECCV) (Cham: Springer), 584–599.\ndoi: 10.1007/978-3-030-01234-2_36\nLoaiza-Ganem, G., and Cunningham, J. P. (2019). The continuo us Bernoulli: ﬁxing\na pervasive error in variational autoencoders. Adv. Neural Inf. Process. Syst . 32.\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I ., Wierstra, D.,\net al. (2013). Playing atari with deep reinforcement learning. arXiv. [Preprint].\ndoi: 10.48550/arXiv.1312.5602\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G.,\net al. (2015). Human-level control through deep reinforcemen t learning. Nature 518,\n529–533. doi: 10.1038/nature14236\nMorales, E. F., Murrieta-Cid, R., Becerra, I., and Esquivel-Ba saldua, M. A.\n(2021). A survey on deep learning and deep reinforcement learn ing in robotics\nwith a tutorial on deep reinforcement learning. Intell. Serv. Robot . 14, 773–805.\ndoi: 10.1007/s11370-021-00398-z\nNair, A., Srinivasan, P., Blackwell, S., Alcicek, C., Fearon, R. , De Maria, S., et al.\n(2015). Massively parallel methods for deep reinforcement learn ing. arXiv. [Preprint].\ndoi: 10.48550/arXiv.1507.04296\nOzturk, A., Gunel, M. B., Dagdanov, R., Vural, M. E., Yurdakul, F. , Dal,\nM., et al. (2021). “Investigating value of curriculum reinforc ement learning in\nautonomous driving under diverse road and weather conditio ns, ” in 2021 IEEE\nIntelligent Vehicles Symposium Workshops (IV Workshops) (Nagoya: IEEE), 358–363.\ndoi: 10.1109/IVWorkshops54471.2021.9669203\nPaden, B., ˇCáp, M., Yong, S. Z., Yershov, D., and Frazzoli, E. (2016). A sur vey of\nmotion planning and control techniques for self-driving urban v ehicles. IEEE Trans.\nIntell. Veh. 1, 33–55. doi: 10.1109/TIV.2016.2578706\nParisotto, E., Song, F., Rae, J., Pascanu, R., Gulcehre, C., Ja yakumar, S., et al. (2020).\n“Stabilizing transformers for reinforcement learning, ” in International Conference on\nMachine Learning PMLR (Vienna: ACM), 7487–7498.\nParmar, N., Vaswani, A., Uszkoreit, J., Kaiser, L., Shazeer , N., Ku, A., et al. (2018).\n“Image transformer, ” in Proceedings of the 35th International Conference on Machine\nLearning, volume 80 of Proceedings of Machine Learning Research , eds J. Dy, and A.\nKrause (Stockholm: PMLR), 4055–4064.\nPuterman, M. L. (2014). Markov Decision Processes: Discrete Stochastic Dynamic\nProgramming. Hoboken, NJ: John Wiley and Sons.\nSilver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., Rie dmiller, M., et al. (2014).\n“Deterministic policy gradient algorithms, ” in International Conference on Machine\nLearning (PMLR), 387–395.\nTamar, A., Wu, Y., Thomas, G., Levine, S., and Abbeel, P. (2016) . Value iteration\nnetworks. Adv. Neural Inf. Process. Syst . 29. doi: 10.24963/ijcai.2017/700\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al.\n(2017). Attention is all you need. Adv. Neural Inf. Process. Syst . 30.\nWang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. (2020). Li nformer:\nself-attention with linear complexity. arXiv. [Preprint]. doi: 10.48550/arXiv.2006.\n04768\nWatkins, C. J., and Dayan, P. (1992). Q-learning. Mach. Learn . 8, 279–292.\ndoi: 10.1007/BF00992698\nWei, R., Garcia, C., El-Sayed, A., Peterson, V., and Mahmood, A. (2020). Variations\nin variational autoencoders-a comparative evaluation. IEEE Access 8, 153651–153670.\ndoi: 10.1109/ACCESS.2020.3018151\nYe, F., Zhang, S., Wang, P., and Chan, C.-Y. (2021). “A survey of deep\nreinforcement learning algorithms for motion planning and cont rol of autonomous\nvehicles, ” in2021 IEEE Intelligent Vehicles Symposium (IV) (Nagoya: IEEE), 1073–1080.\ndoi: 10.1109/IV48863.2021.9575880\nYeom, K. (2022). Deep reinforcement learning based autonomo us driving with\ncollision free for mobile robots. Int. J. Mech. Eng. Robot. Res . 11, 338–344.\ndoi: 10.18178/ijmerr.11.5.338-344\nYoo, D., Park, S., Lee, J.-Y., Paek, A. S., and So Kweon, I. (20 15). “Attentionnet:\naggregating weak directions for accurate object detection , ” in Proceedings of the\nIEEE International Conference on Computer Vision (Santiago: IEEE), 2659–2667.\ndoi: 10.1109/ICCV.2015.305\nYu, Y., Si, X., Hu, C., and Zhang, J. (2019). A review of recurr ent neural\nnetworks: LSTM cells and network architectures. Neural Comput . 31, 1235–1270.\ndoi: 10.1162/neco_a_01199\nZhu, P., Li, X., Poupart, P., and Miao, G. (2017). On improving de ep reinforcement\nlearning for pomdps. arXiv. [Preprint]. doi: 10.48550/arXiv.1704.07978\nFrontiers in Neurorobotics /two.tnum/five.tnum frontiersin.org",
  "topic": "Reinforcement learning",
  "concepts": [
    {
      "name": "Reinforcement learning",
      "score": 0.8210914731025696
    },
    {
      "name": "Computer science",
      "score": 0.7991273403167725
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6750685572624207
    },
    {
      "name": "Transformer",
      "score": 0.5319744944572449
    },
    {
      "name": "Autoencoder",
      "score": 0.5066877007484436
    },
    {
      "name": "Machine learning",
      "score": 0.4872390329837799
    },
    {
      "name": "Autonomous agent",
      "score": 0.444196492433548
    },
    {
      "name": "Curse of dimensionality",
      "score": 0.43791458010673523
    },
    {
      "name": "Deep learning",
      "score": 0.3545546531677246
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I139759216",
      "name": "Beijing University of Posts and Telecommunications",
      "country": "CN"
    }
  ]
}