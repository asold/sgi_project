{
  "title": "ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing",
  "url": "https://openalex.org/W4386875342",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5054622611",
      "name": "Ian Arawjo",
      "affiliations": [
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A5019619132",
      "name": "Chelse Swoopes",
      "affiliations": [
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A5057172764",
      "name": "Priyan Vaithilingam",
      "affiliations": [
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A5039276358",
      "name": "Martin Wattenberg",
      "affiliations": [
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A5045866597",
      "name": "Elena L. Glassman",
      "affiliations": [
        "Harvard University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4225165463",
    "https://openalex.org/W4384816574",
    "https://openalex.org/W2798602389",
    "https://openalex.org/W4225619898",
    "https://openalex.org/W4391584331",
    "https://openalex.org/W4308244910",
    "https://openalex.org/W2130500927",
    "https://openalex.org/W2775663585",
    "https://openalex.org/W2914577637",
    "https://openalex.org/W4362656192",
    "https://openalex.org/W4387801187",
    "https://openalex.org/W4379925093",
    "https://openalex.org/W4385682544",
    "https://openalex.org/W4362659486",
    "https://openalex.org/W4301393026",
    "https://openalex.org/W4377371585",
    "https://openalex.org/W3163332892",
    "https://openalex.org/W4311354629",
    "https://openalex.org/W2122909041",
    "https://openalex.org/W3203321135",
    "https://openalex.org/W4225080353",
    "https://openalex.org/W4366548330",
    "https://openalex.org/W4386246835",
    "https://openalex.org/W4367185264",
    "https://openalex.org/W4387801427",
    "https://openalex.org/W4241881032",
    "https://openalex.org/W4309395891",
    "https://openalex.org/W4225295761",
    "https://openalex.org/W4323568846"
  ],
  "abstract": "Evaluating outputs of large language models (LLMs) is challenging, requiring making -- and making sense of -- many responses. Yet tools that go beyond basic prompting tend to require knowledge of programming APIs, focus on narrow domains, or are closed-source. We present ChainForge, an open-source visual toolkit for prompt engineering and on-demand hypothesis testing of text generation LLMs. ChainForge provides a graphical interface for comparison of responses across models and prompt variations. Our system was designed to support three tasks: model selection, prompt template design, and hypothesis testing (e.g., auditing). We released ChainForge early in its development and iterated on its design with academics and online users. Through in-lab and interview studies, we find that a range of people could use ChainForge to investigate hypotheses that matter to them, including in real-world settings. We identify three modes of prompt engineering and LLM hypothesis testing: opportunistic exploration, limited evaluation, and iterative refinement.",
  "full_text": "ChainForge: A Visual Toolkit for Prompt Engineering and LLM\nHypothesis Testing\nIan Arawjo\nian.arawjo@gmail.com\nSEAS\nHarvard University\nCambridge, Massachusetts, USA\nChelse Swoopes‚àó\ncswoopes@g.harvard.edu\nSEAS\nHarvard University\nCambridge, Massachusetts, USA\nPriyan Vaithilingam‚àó\npvaithilingam@g.harvard.edu\nSEAS\nHarvard University\nCambridge, Massachusetts, USA\nMartin Wattenberg\nwattenberg@seas.harvard.edu\nInsight and Interaction Lab\nHarvard University\nCambridge, Massachusetts, USA\nElena L. Glassman\nglassman@seas.harvard.edu\nSEAS\nHarvard University\nCambridge, Massachusetts, USA\nFigure 1: The ChainForge interface, depicting a limited evaluation that tests a model‚Äôs robustness to prompt injection attacks.\nThe entire experiment was developed in 15 minutes, and illustrates a key benefit of ChainForge‚Äôs design: the evaluation logic\ncan be followed from start to finish in a single screenshot. Users can share this flow as a file or web link.\n‚àóEqual contributions.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nCHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA\n¬© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0330-0/24/05. . . $15.00\nhttps://doi.org/10.1145/3613904.3642016\nABSTRACT\nEvaluating outputs of large language models (LLMs) is challeng-\ning, requiring making‚Äîand making sense of‚Äîmany responses. Yet\ntools that go beyond basic prompting tend to require knowledge of\nprogramming APIs, focus on narrow domains, or are closed-source.\nWe present ChainForge, an open-source visual toolkit for prompt\nengineering and on-demand hypothesis testing of text generation\nLLMs. ChainForge provides a graphical interface for comparison\nof responses across models and prompt variations. Our system\nwas designed to support three tasks: model selection, prompt tem-\nplate design, and hypothesis testing (e.g., auditing). We released\narXiv:2309.09128v3  [cs.HC]  3 May 2024\nCHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA Arawjo, Swoopes, Vaithilingam, Wattenberg & Glassman\nChainForge early in its development and iterated on its design\nwith academics and online users. Through in-lab and interview\nstudies, we find that a range of people could use ChainForge to\ninvestigate hypotheses that matter to them, including in real-world\nsettings. We identify three modes of prompt engineering and LLM\nhypothesis testing: opportunistic exploration, limited evaluation,\nand iterative refinement.\nCCS CONCEPTS\n‚Ä¢ Human-centered computing ‚ÜíInteractive systems and\ntools; Empirical studies in interaction design ; ‚Ä¢ Computing method-\nologies ‚ÜíNatural language processing .\nKEYWORDS\nlanguage models, toolkits, visual programming environments, prompt\nengineering, auditing\nACM Reference Format:\nIan Arawjo, Chelse Swoopes, Priyan Vaithilingam, Martin Wattenberg,\nand Elena L. Glassman. 2024. ChainForge: A Visual Toolkit for Prompt\nEngineering and LLM Hypothesis Testing. In Proceedings of the CHI Con-\nference on Human Factors in Computing Systems (CHI ‚Äô24), May 11‚Äì16, 2024,\nHonolulu, HI, USA. ACM, New York, NY, USA, 18 pages. https://doi.org/10.\n1145/3613904.3642016\n1 INTRODUCTION\nLarge language models (LLMs) have captured imaginations, and\nconcerns, across the world. Both imagination and concern derives,\nin part, from ambiguity around model capabilities‚Äîthe difficulty of\ncharacterizing LLM behavior. Everyone from developers to model\nauditors encounters this same challenge. Developers struggle with\n‚Äúprompt engineering, ‚Äù or finding a prompt that leads to consistent,\nquality outputs [3, 21]. Auditors of models, to check for bias, must\nlearn programming APIs to test hypotheses systematically. To help\ndemystify LLMs, we need powerful, accessible tools that help people\ngain more comprehensive understandings of LLM behavior, beyond\na single prompt or chat.\nIn this paper, we introduce a visual toolkit, ChainForge, that\nsupports on-demand hypothesis testing of the behavior of text gen-\nerating LLMs on open-domain tasks, with minimal to no coding\nrequired. We describe the design of ChainForge, including how it\nwas motivated from real use cases at our university, and how our de-\nsign evolved with feedback from fellow academics and online users.\nSince early summer 2023, ChainForge has been publicly available\nat chainforge.ai as web and local software, is free and open-source,\nand allows users to share their experiments with others as files or\nlinks. Unlike other systems work in HCI, we developed ChainForge\nin the open, seeking an alternative to closed-off or ‚Äòprototype-and-\nmove-on‚Äô patterns of work. Since its launch, our tool has been used\nby many people, including in other HCI research projects submitted\nto this very conference. We report a qualitative user study engaging\na range of participants, including people with non-computing back-\ngrounds. Our goal was to examine how users applied ChainForge\nto tasks that mattered to them, position the tools‚Äô strengths and lim-\nitations, and pose implications for future interfaces. We show that\nusers were able to apply ChainForge to a variety of investigations,\nfrom plotting LLMs‚Äô understanding of material properties, to dis-\ncovering subtle biases in model outputs across languages. Through\na small interview study, we found that actual users find ChainForge\nuseful for real-world tasks, including by extending its source code,\nand remark on differences between their usage and in-lab users. ‚Äô\nConsistent with HCI ‚Äòtoolkit‚Äô or constructive research [ 19], our\ncontributions are:\n‚Ä¢the artifact of ChainForge, which is publicly available, open-\nsource, and iteratively developed with users\n‚Ä¢in-lab usability and interview studies of a system for open-\nended, on-demand hypothesis testing of LLM behavior\n‚Ä¢implications for future tools which target prompt engineer-\ning and hypothesis testing of LLM outputs\nSynthesizing across studies, we identify three modes of prompt\nengineering and LLM hypothesis testing more broadly:opportunis-\ntic exploration, limited evaluation, and iterative refinement.\nThese modes highlight different stages and user mindsets when\nprompt engineering and testing hypotheses. As design contribu-\ntions, we present one of the first prompt engineering tools that\nsupports cross-LLM comparison in the HCI literature, and intro-\nduce the concept of prompt template chaining , an extension of\nAI chains [44], where prompt templates may be recursively nested.\nOur studies demonstrate that many users found ChainForge ef-\nfective for the very tasks and behaviors targeted by our design\ngoals‚Äîmodel selection, prompt iteration, hypothesis testing‚Äîwith\nsome perceiving it to be more efficient than tools like Jupyter note-\nbooks. Our findings on a structured task also suggest decisions\naround prompts and models are highly subjective: even given the\nsame criteria and scenario, user interpretations and ranking of cri-\nteria can vary widely. Finally, we found that many real-world users\nwere using ChainForge for a need we had not anticipated: proto-\ntyping data processing pipelines . Although prior research focuses\non AI chaining or prompt engineering [5, 25, 44, 45], they provide\nlittle to no context on why real people would prompt engineer or\nprogram an AI chain. We find that while users‚Äô subtasks matched\nour design goals (e.g., prompt template iteration, choosing a model),\nthese subtasks were usually in service of one of two overarching\ngoals‚Äîprototyping data processing pipelines , or testing model be-\nhavior (i.e., auditing). When prompt engineering is placed into a\nlarger context of data processing, unique needs and pain-points\nof our real-world users‚Äîgetting data out, sharing with others‚Äî\nseem obvious in retrospect. We recommend that future systems for\nprompt engineering or AI chains consider users‚Äô broader context\nand goals beyond prompt/chain iteration itself‚Äîand, especially, that\nthey draw inspiration from past frameworks for data processing.\n2 RELATED WORK\nOver the past decade, rising interest in machine learning (ML) has\nproduced an industry of software for ML operations (‚ÄúMLOps‚Äù).\nTools generally target ML experts and cover tasks across the ML\npipeline [12] from dataset curation, to training, to evaluating per-\nformance (e.g. Google Vertex AI). LLMs have brought their own\nunique challenges and users. LLMs are too big to fully evaluate\nacross all possible use cases; are frequently black-boxed or virtu-\nally impossible to ‚Äòexplain‚Äô [4, 37] ; and finding the right prompt\nor model has become an industry unto itself. Compounding these\nChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing CHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA\nFigure 2: The emerging space of tools for LLM operations\nissues, users of LLMs are frequently not ML experts at all‚Äîsuch as\nauditors checking for bias, or non-ML software developers. LLMs\nare thus spurring their own infrastructure and tooling ecosystem\n(‚ÄúLLMOps‚Äù).\nThe LLMOps space is rapidly evolving. We represent the emerg-\ning ecosystem as a graph (Figure 2), with exploration and discovery\non one end (e.g., playgrounds, ChatGPT), and systematic evalua-\ntion and testing of LLM outputs on the other. This horizontal axis\nrepresents two related, but distinct parts of prompt engineering:\ndiscovering a prompt that works robustly according to user cri-\nteria, involving improvisation and experimentation both on the\nprompt and the criteria; and evaluating prompt(s) once chosen,\nusually in production contexts to ensure a change of prompt will\nnot alter user experience. (These stages generalize beyond prompts\nto ‚Äúchains‚Äù or AI agents [ 44].) The two aspects are analogous to\nsoftware engineering, where environments like Jupyter Notebooks\nsupport messy exploration and fast prototyping, while automated\npipelines ensure quality control. A vertical axis characterizes the\nstyle of interaction‚Äîfrom textual APIs to tools with a graphical\nuser interface (GUI). In what follows, we zoom in to specific parts\nof this landscape.\nLLMOps for Prompt Engineering. There are a growing num-\nber of academic projects designed for prompting LLMs [5, 14, 25, 43],\nbut few support systematic, as opposed to manual, evaluation of\ntextual responses [45]. For example, PromptMaker helps users cre-\nate prompts with few-shot examples; authors concluded that users\n‚Äúfound it difficult to systematically evaluate‚Äù their prompts, wished\nthey could score responses, and that such scoring ‚Äútended to be\nhighly specific to their use case... rather than a metric that could\nbe universally applied‚Äù [14]. One rare system addressing prompt\nevaluation for text generation is PromptAid [25], which uses a NLP\nparaphrasing model to perturb input prompts with semantically-\nsimilar rephrasings, resends the queries to a single LLM and plots\nevaluation scores. Powerful in concept, it was tested on only one\nsentiment analysis task, where all the test prompts, model, and eval-\nuation metric were pre-defined for users. BotDesigner [45] supports\nprompt-based design of chat models, yet its evaluation was also\nhighly structured around a specific task (creating an AI professional\nchef). It remains unclear how to support users in open-ended tasks\nthat matter to them‚Äîespecially comparing across multiple LLMs\nand setting up their own metrics‚Äîso that they test hypotheses\nabout LLM behavior in an improvisational, yet systematic manner.1\nSince we launched ChainForge, a number of commercial prompt\nengineering and LLMOps tools have emerged, and more emerge\n1It also bears mentioning that many papers published about LLM-prompting are\nclosed-source or unreleased, including PromptAid, PromptMaker, PromptChainer, and\nBotDesigner [14, 25, 43, 45].\neveryday.2 Examples are Weights and Biases Prompts, nat.dev, Vel-\nlum.ai, Vercel, Zeno Build, and promptfoo [ 9, 26, 39‚Äì42]. These\nsystems range from prompting sandboxes [ 29] to prompt verifi-\ncation and versioning inside production applications, and usually\nrely upon integration with code, command-line scripts, or config\nfiles [30, 38, 41]. For instance, promptfoo [41] is an evaluation har-\nness akin to testing frameworks like jest [13], where users write\nconfig files that specify prompts and expected outputs. Tests are run\nfrom the command line. Although most systems support prompt\ntemplating, few support sending each prompt to multiple models at\nonce; the few that support cross-model comparison, like Vellum.ai,\nare playgrounds that test single prompts, making it cumbersome to\ncompare systematically.\nVisual Data Flow Environments for LLMOps. Related vi-\nsually, but distinct from our design concern of evaluation, are vi-\nsual data flow environments built around LLM responses. These\nhave two flavors: sensemaking interfaces for information forag-\ning, and tools for designing LLM applications. Graphologue and\nSensecape, instances of the former, are focused on helping users\ninteract non-linearly with a chat LLM and provide features to, for\nexample, elaborate on its answers [15, 36]. Second are systems for\ndesigning LLM-based applications, usually integrating with the\nLangChain Python package [7]: Langflow, Flowise, and Microsoft\nPromptFlow on Azure services [ 8, 22, 24]. All three tools were\npredated by PromptChainer, a closed-source visual programming\nenvironment for LLM app development by Wu et al. [ 43]. Such\nenvironments focus on constructing ‚ÄúAI chains‚Äù [44], or data flows\nbetween LLMs and other tools or scripts. Here, we leverage design\nconcepts from visual flow-based tools, while focusing our design\non supporting exploration and evaluation of LLM response qual-\nity. One key difference is the need for hypothesis testing tools to\nsupport combinatorial power, i.e., querying multiple models with\nmultiple prompts at once, whereas both LLM app building and\nsensemaking tools focus on single responses and models.\nOverall, then, the evolving LLMOps landscape may be summa-\nrized as follows. Tools for prompt discovery appear largely limited\nto simple playgrounds or chats, where users send off single prompts\nat a time through trial and error. Tools for systematic testing, on the\nother hand, tend to require idiosyncratic config files, command-line\ncalls, ML engineering knowledge, or integration with a program-\nming API‚Äîmaking them difficult to use for discovery and improvi-\nsation (not to mention non-programmers). We wanted to design a\nsystem to bridge the gap betweenexploration and evaluation aspects\nof LLMOps: a graphical interface that facilitates rapid discovery\nand iteration, but also inspection of many responses and systematic\nevaluation, without requiring extensive knowledge of a program-\nming API. By blending the usability of visual programming tools\nwith power features like sending the same prompts to multiple\nLLMs at once, we sought to make it easier for people to experiment\nwith and characterize LLM behavior.\n2For instance, as we write this, we see the launch ofbaserun.ai, a Y-Combinator backed\nstartup for LLM output evaluation. Like many such startups, the website is glossy and\npromises much, but the tool itself is inaccessible and limited to a few screenshots.\nCHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA Arawjo, Swoopes, Vaithilingam, Wattenberg & Glassman\n3 DESIGN GOALS AND MOTIVATION\nThe impetus for ChainForge came from our own experience test-\ning prompts while developing LLM-powered software for other\nresearch projects. Across our research lab, we needed a way to sys-\ntematically test prompts to reach one that satisfied certain criteria.\nThis criteria was project-specific and evolved improvisationally\nover development. We also noticed other researchers and industry\ndevelopers facing similar problems when trying to evaluate LLM\nbehavior.\nWe designed ChainForge for a broad range of tasks that fall into\nthe category of hypothesis testing about LLM behavior. Hypothe-\nsis testing includes prompt engineering (finding a prompt involves\ncoming up with hypotheses about prompts and testing them), but\nalso encompasses auditing of models for security, bias and fairness,\netc. Specifically, we intended our interface to support four concrete\nuser goals and behaviors:\nD1. Model selection. Easy comparison of LLM behavior across\nmodels. We were motivated by fine-tuning LLMs, and how\nto ‚Äòevaluate‚Äô what changed in the fine-tuned versus the base\nmodel. Users should be able to gain quick insights into what\nmodel to use, or which performs the ‚Äòbest‚Äô for their use case.\nD2. Prompt template design. Prompt engineers typically need\nto find not a good prompt, but a good prompt template (a\nprompt with variables in {braces}) that performs consistently\nacross many possible inputs. Existing tools make it difficult\nto compare, side-by-side, differences between templates, and\nthus hinder quick iteration.\nD3. Systematic evaluation. To verify hypotheses about LLM\nbehavior beyond anecdotal evidence, one needs a mass of re-\nsponses (and ideally more than a single response per prompt).\nHowever, manual inspection (scoring) of responses becomes\ntime-consuming and unwieldy quickly. To rectify this, the\nsystem must support sending a ton of parametrized queries,\nhelp users navigate them and score them according to their\nown idiosyncratic critera [14], and facilitate quick skimming\nof results (e.g., via plots).\nD4. Improvisation [16]. We imagined a system that supported\nquick-and-messy iteration and likened its role to Jupyter\nNotebooks in software engineering. If in the course of ex-\nploration a user develops another hypothesis they wish to\ntest, the system should support on-demand testing of that\nhypothesis‚Äîwhether amending prompts, swapping models,\nor changing evaluations. This design goal is in tension with\nD3, even sometimes embracing imprecision in measuring\nresponse quality‚Äîalthough we imagined the system could\nconduct detailed evaluations, our primary goal was to sup-\nport on-demand (as opposed to paper-quality) evaluations.\nWe also had two high-level goals. We wanted the system to take\ncare of ‚Äòthe basics‚Äô‚Äîsuch as prompting multiple models at once,\nplotting graphs, or inspecting responses‚Äîsuch that researchers\ncould extend or leverage our project to enable more nuanced\nresearch questions (for instance, designing their own visualization\nwidget). Second, we wanted to explore open-source iteration,\nwhere, unlike typical HCI system research, online users themselves\ncan give feedback on the project via GitHub. In part, we were\nmotivated by disillusionment with close-source or ‚Äòprototype-and-\nmove-on‚Äô patterns of work in HCI, which risk ecological validity\nand tend to privilege academic notoriety over public benefit [11].\nFinally, we were guided by differentiation and enrichment\ntheories of human learning , Variation Theory [23] and Analogi-\ncal Learning Theory [10], which are complementary perspectives\non the value of variation within (structurally) aligned, diverse data.\nBoth theories hold that experiencing variation within and across\nobjects of learning (in this case, models, prompts and/or prompt\nvariables) helps humans develop more accurate mental models that\nmore robustly generalize to novel scenarios. ChainForge provides\ninfrastructure that helps users set up these juxtapositions across\nanalogous differences across dimensions of variation that, given\nwhat they want to learn, users construct, i.e., by choosing multiple\nmodels, prompts, and/or values for prompt variables.\n4 CHAINFORGE\nBefore describing our system in detail, we walk readers through\none example usage scenario. The scenario relates to the real-world\nneed to make LLMs robust against prompt injection attacks [32],\nand derives from an interaction the first author had with Google\nDoc‚Äôs AI writing assistant, where the tool, supposed to suggest\nrewriting of highlighted text, took the text as a command instead.\nMore case studies of usage will be presented in our findings.\nScenario. Farah is developing an AI writing assistant where users\ncan highlight text in their document and click buttons to expand,\nshorten, or rewrite the text. In code, she uses a prompt template and\nfeeds the users‚Äô input as a variable below her commands. However,\nshe is worried about whether the model is robust to prompt injection\nattacks, or, users purposefully trying to divert the model to behave\nagainst her instructions. She decides to compare a few models and\nchoose whichever is most robust. Importantly, she wants to reach a\nconclusion quickly and avoid writing a custom program.\nLoading ChainForge, Farah adds a Prompt Node and pastes in her\nprompt template (Figure 1). She puts her three command prompts in\na TextFields Node‚Äîrepresenting the three buttons to expand, shorten,\nand rewrite text‚Äîand enters some injection attacks in a second TextFields,\nattempting to get the model to ignore its instructions and just output\n‚ÄúLOL‚Äù.3 She connects the TextFields to her template variables {com-\nmand} and {input}, respectively. Adding four models to the Prompt\nnode, she sets ‚ÄúNum responses‚Äù to three for some variation and runs\nit, collecting responses from all models for all permutations of inputs.\nAdding a JavaScript Evaluator, she checks whether the response starts\nwith LOL, indicating the attack succeeded; and connects a Vis Node\nto plot success rate.\nIn fifteen minutes, Farah can already see that model GPT-4 appears\nthe most robust; however, GPT-3.5 is not far behind. 4 She sends the flow\nto her colleagues and chats with them about which model to choose,\ngiven that GPT-4 is more expensive. The team agrees to go with GPT-\n3.5, but a colleague suggests they remove all but the GPT models\n3ChainForge can actually be used to compare across system instructions for OpenAI\nmodels as well, but for simplicity here, we put the instruction in the prompt. Farah\ncould also templatize the ‚ÄúLOL‚Äù to test on a variety of different injection values, and\nuse that variable‚Äôs value in evaluators.\n4Actual scores depicted; uses March 2023 versions of OpenAI modelsgpt-3.5-turbo and\ngpt-4, Anthropic‚Äôs claude-2, and Google‚Äôs chat-bison-001.\nChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing CHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA\nFigure 3: An example of chaining prompt templates, one of ChainForge‚Äôs unique features. Users can test different templates at\nonce by using the same input variable (in {} brackets). Templates can be chained at arbitrary depth using TextFields nodes. The\nuser is hovering over the Run button of the Prompt Node, displaying a reactive tooltip of how many queries will be sent off.\nand try different variations of their command prompts, including\nstatements not to listen to injection-style attacks...\nFarah and her colleagues might continue to use ChainForge to\niterate on their prompts, testing criteria, etc., or just decide on\na model and move on. The expected usage is that the team uses\nChainForge to reach conclusions quickly, then proceeds elsewhere\nwith their implementation. Note that while Farah‚Äôs task might fall\nunder the rubric of ‚Äúprompt engineering, ‚Äù there is also an auditing\ncomponent, and we designed the system to support a variety of\nscenarios beyond this example.\n4.1 Design Overview\nThe main ChainForge interface is depicted in Figure 1. Common to\ndata flow programming environments [43], users can add nodes and\nconnect them by edges. ChainForge has four types of nodes‚Äîinputs,\ngenerators, evaluators, and visualizers‚Äîas well as miscellany like\ncomment nodes (available nodes listed in Appendix B, Table 3). This\ntypology roughly aligns with the ‚Äúcells, generators, lenses‚Äù writing\ntool LLM framework of Kim et al. [17], but for a broader class of\nproblems and node types. Like PromptChainer [43], data flowing\nbetween nodes are typically LLM responses with metadata attached\n(with the exception of input nodes, which export text). Table 1 de-\nscribes how aspects of our implementation relate to design goals in\nSection 3. For comprehensive information on nodes and features,\nwe point readers to our documentation at chainforge.ai/docs. Here-\nafter, we focus on describing high-level design challenges unique\nto our tool and relevant for hypothesis testing.\nThe key design difference between ChainForge and other flow-\nbased LLMOps tools is combinatorial power‚Äîusers can send off\nnot only multiple prompts at once, but query multiple models, with\nmultiple prompt variables that might be hierarchically organized\n(through chained templates) or carry additional metadata. This\nleads to what two users called the ‚Äúmultiverse problem. ‚Äù Unique\nto this design is our Prompt Node, which allows users to query\nmultiple models at once (Figure 3). Many features aim to help users\nnavigate this multiverse of outputs and reduce complexity to reach\nconclusions across them, such as the response inspector, evaluators\nand visual plots. The combinatorial complexity of generating LLM\nqueries in ChainForge may be summarized in an equation, roughly:\n(P prompts )√ó( M models )√ó( N responses per prompt )\n√ómax(1,(C Chat histories ))\nwhere P is produced through a combination of prompt variables,\nM may be generalized to response providers (model variations,\nAI agents, etc), and ùê∂=0 for Prompt nodes and ‚â•0 for Chat Turn\nnodes. ùëÉprompts are produced through simple rules: multiple input\nvariables to a template produce the cross product of the sets, with\nthe exception of Tabular Data nodes, whose outputs ‚Äúcarry together‚Äù\nwhen filling template variables.5\nTo inspect responses, users open a pop-up Response Inspector\n(Figure 4). The inspector has two layouts:Grouped List, where users\nsee LLM responses side-by-side for the same prompt and can orga-\nnize responses by hierarchically grouping on input variables; and\nTable, with columns plotting input variables and/or LLMs by user\nchoice. Both layouts present responses in colored boxes, represent-\ning an LLM‚Äôs response(s) to a single prompt (each color maps to\na specific LLM and is consistent across the application). Grouped\nList has collapse-able response groups, with one opened by de-\nfault; users can expand/collapse groups by clicking their headers.\nIn Table layout, all rows appear at once. We observed in pilots that,\ndepending on the user and the task, users preferred one view or\nthe other.\n5For instance, in Figure 3 there are ten prompt permutations, two models, ùëÅ=1 and\nùê∂=0. When the user hovers over the Run button of the Prompt Node, a reactive tooltip\nindicates how many queries will be sent. (Users can also click the list icon, to review\nthe queries.) Consider also Fig. 1. There are two variables command and input with 3\nand 4 values each, resulting in3√ó4=12 queries to four LLMs. With ùëÅ=3, this produces\n12√ó4√ó3=144 responses. All responses are cached; users can change upstream fields\nthen re-prompt, and ChainForge will only send off queries it needs.\nCHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA Arawjo, Swoopes, Vaithilingam, Wattenberg & Glassman\nFigure 4: (A) The Response Inspector in Grouped List layout, showing four LLMs‚Äô responses side-by-side to the same prompt.\nEach color represents a different LLM, named in each box‚Äôs top-right corner. Here the user requested has ùëõ= 2 responses per\nprompt, and has grouped responses by prompt variables command and then input. (B) Users can click on groupings (blue\nheaders) to expand/collapse them. (C) An alternative Table Layout offers a grid for interactive comparison across prompt\nvariables and models, where users can change the main column-plotted variable. Users can also export data to a spreadsheet\n(not shown). Interactive version at chainforge.ai/play\n.\nThere are many more features, more than we can cover in limited\nspace; but, to provide readers a greater sense of ChainForge, we\npresent a more complex example, utilizing Tabular Data and Simple\nEvaluator nodes to conduct a ground truth evaluation on an OpenAI\nevals [30] benchmark (Figure 5). At each step, metadata (a prompt\ntemplate‚Äôs ‚Äúfill history‚Äù) annotates outputs, and may be referenced\ndownstream in a chain. Here, the ‚ÄúIdeal‚Äù column of the Tabular Data\n(A) is used as a metavariable in a Simple Evaluator (C), checking\nif the LLM response contains the expected value. Note that ‚ÄúIdeal‚Äù\nis not the input to a template , but instead is associated, by virtue\nof the table, with the output to Prompt. The user has plotted by\ncommand (D) to compare differences in performance across two\nprompt variables. Spot-checking the stacked bar chart, they see\nClaude and Falcon.7B perform slightly better on one command than\nthe other.\n4.2 Iterative Development with Online and Pilot\nUsers\nWe iterated ChainForge with pilot users (academics in computing)\nand online users (through public GitHub Issues and comments). We\nsummarize the substantial changes and additions which resulted.\nEarly in ChainForge‚Äôs development, we tested it on ongoing re-\nsearch projects in our lab. The most important outcome was the\ndevelopment of prompt template chaining , where templates may be\nrecursively nested, enabling comparing across prompt templates\nthemselves (Fig. 3). Early use cases of ChainForge included: short-\nening text with minimal rewordings, checking what programming\nAPIs were imported for what prompts, and evaluating how well\nresponses conformed to a domain-specific language. For instance,\nwe discovered that a ChatGPT prompt we were using performed\nworst for an ‚Äòonly delete words‚Äô task, tending to reword the most\ncompared to other prompts.\nWe also ran five pilot studies. Pilot users requested two features:\nan easier way to score responses without code, and a way to carry\nchat context. These features became LLM Scorer and Chat Turn\nnodes. Finally, some potential users were wary of the need to install\non their own machine. Thus, we rewrote the backend from Python\ninto TypeScript (2000+ lines of code) and hosted ChainForge on the\nweb, so that anyone can try the interface simply by visiting the site.\nMoreover, we added a ‚ÄúShare‚Äù button, so that users can share their\nexperiments with others as links.\nSince its launch in late May 2023, online users also provided\nfeedback on our system by raising GitHub Issues. According to\nChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing CHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA\nFigure 5: A more complex example, depicting a ground truth evaluation using Tabular Data (A), TextFields (B), and Simple\nEvaluator (C) nodes. User has plotted scores (D) by a prompt variable command to compare prompts, finding that Claude and\nFalcon.7B do slightly better on their second prompt. User can then go back to (B) or (A), iterating on prompts or input data, and\nre-run prompt and evaluator nodes; ChainForge only sends off queries it has not already collected.\nPyPI statistics, the local version of ChainForge has been installed\naround 5000 times, and the public GitHub has attained over 1300\nstars. In August 2023, over 3000 unique users accessed the web app\nfrom countries across the world, averaging about 100 daily (top\ncountries: U.S., South Korea, Germany, and India). Online comments\ninclude:\n‚Ä¢Software developer at a Big-5 Tech Company, via GitHub\nIssue: ‚ÄúI showed this to my colleagues, they were all amazed\nby the power and flexibility of the tool. Brilliant work!‚Äù\n‚Ä¢Startup developer, on a prominent programmer news site:\n‚ÄúWe just used this on a project and it was very helpful! Cool to\nsee it here‚Äù\n‚Ä¢Head of product design at a top ML company, on a social\nmedia site: ‚ÄúJust played a bit with [ChainForge] to compare\nLLMs and the UX is satisfying‚Äù\nBeyond identifying bugs, online feedback resulted in: adding sup-\nport for Microsoft‚Äôs Azure OpenAI service; a way to preview prompts\nbefore they are sent off; toggling fields on TextFields nodes ‚Äôon‚Äô or\n‚Äôoff‚Äô; running on different hosts and ports; and implicit template\nvariables.6 Since its launch, the code of ChainForge has also been\nadapted by two other research teams: one team related to the last\nauthor, and one unrelated team at a U.S. research university whose\nauthors are adapting our code for HCI research into prototyping\nwith LLM image models (whom we interviewed in our evaluation).\n6The last is discussed in our docs; one uses a hashtag before a template variable, e.g.\n{#country}, to reference metadata associated with each input value. For instance, one\nmight set up a table with an Expected column, then use {#Expected} in an LLM Scorer\nto compare with the expected value.\n4.3 Implementation\nChainForge was programmed by the first author in React, Type-\nScript, and Python. It uses ReactFlow for the front-end UI and\nMantine for UI elements. The local version uses Flask to serve\nthe app and load API keys from environment variables. The app\nlogic for prompt permutations and sending API requests is custom\ndesigned and uses asynchronous generator functions to improve\nperformance; it is capable of sending off hundreds of requests si-\nmultaneously to multiple LLMs, streams progress back in real-time,\nrate limits the requests appropriately based on the model provider,\nand collects API request errors without disrupting other requests.\nThe source code is released publicly under the MIT License.\n5 EVALUATION RATIONALE, DESIGN, AND\nCONTEXT\nToolkits are notoriously difficult to evaluate in HCI [ 11, 19, 28].\nThe predominant method of evaluation, the controlled usability\nstudy, is a poor match for toolkits, as usability studies tend to\nfocus on a narrow subset of a toolkit‚Äôs capabilities [19, 28], rarely\naligning with ‚Äúhow [the system] would be adopted and used in\neveryday practice‚Äù [11]. To standardize evaluation expectations for\ntoolkit papers, Ledo et al. found that successful toolkit publications\ntended to adopt two of four methods, the most popular among\nthem being demonstrations of usage (example scenarios) and user\nstudies that try to capture the breadth of the tool (‚Äúwhich tasks\nor activities can a target user group perform and which ones still\nremain challenging?‚Äù [19, p. 5]). These insights informed how we\napproached an evaluation of ChainForge.\nCHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA Arawjo, Swoopes, Vaithilingam, Wattenberg & Glassman\nDesign Goal Implementation Features\nModel selection (D1) Query multiple models at once in Prompt and Chat Turn nodes. Query same model multiple times at different settings. Compare\nLLM responses side-by-side in inspector. Vis Node groups-by-LLM by default on box-and-whisker and accuracy bar plots. Extend\nChainForge with custom response providers via Python.\nPrompt template de-\nsign (D2)\nTemplate prompts with variables in Prompt, Chat Turn, and TextFields nodes. Recursively nest templates by chaining TextFields\nnodes. Plot columns by prompt variables in Response Inspector‚Äôs Table view. Plot by prompt variables on y-axis of Vis Node to\nslice data by variable.\nSystematic evalua-\ntion (D3)\nEasily increase number of generations per prompt to >1 test robustness [14]. Set up no-code, code (Python and JavaScript), and\nLLM-based evaluation functions. Refer to variables and metavariables in evaluators. Set up groud truth evaluations via Tabular\nData. Visualize scores in Vis Node. Let users navigate and plot data by different prompt variables. Highlight ‚Äúfalse‚Äù (failed) scores in\nred in response inspectors, for easy skimming. Import libraries and custom scripts in Python.\nImprovisation (D4) Downstream nodes react to edits and additions to upstream input data (e.g., adding field on TextFields, changing a row of Tabular\nData, etc). Cache LLM responses and calculate only which prompts require responses, to reduce cost and time. Swap out models or\nchange settings at will. Chain Prompt nodes together, or continue chats via Chat Turn nodes. Allow users to branch experiments\nnon-linearly (flow UI).\nTable 1: Some relationships between our Design Goals and Implementation Features of our toolkit (not comprehensive).\nOur goal for a study was investigate how ChainForge might\nhelp people investigate hypotheses about LLM behavior that per-\nsonally matters to them , while acknowledging the limitations of\nprior knowledge, of who would find such a toolkit useful, and of\nthe impossibility of learning all capabilities in a short time-frame\n[11, 19]. ChainForge is designed for open-ended hypothesis testing\non a broad range of tasks; therefore, it was important that our eval-\nuation was similarly open-ended, capturing (as much as possible in\nlimited time) some actual tasks that users wanted to perform. As\nsuch, we took a primarily qualitative approach, conducting both an\nin-lab usability study with new users, and a small interview study\n(8) with actual users‚Äîpeople who had found our system online\nand already applied it, or its source code, to real-world tasks. We\nhoped these studies would give us a rounded sense of our toolkit‚Äôs\nstrengths and weaknesses, as well as identify potential mismatches\nbetween in-lab and real-world usage. Overall, we wanted to dis-\ncover:\n(1) Are there any general patterns in how people use Chain-\nForge?\n(2) What pain-points (usability and conceptual issues) do people\nencounter?\n(3) What kinds of tasks do people find ChainForge useful for\nalready?\n(4) Which kinds of tasks did people want to accomplish, but find\ndifficult or outside the scope of current features?\nFor the in-lab study, the majority of study time was taken up\nby free exploration. We separated it into two sections: a structured\nsection that served as a tutorial and mock prompt engineering task;\nfollowed by an unstructured exploration of a participants‚Äô idea,\nwhere the participant could ask the researcher for help and guidance.\nBefore the study, we asked for informed consent. Participants filled\nin a pre-study survey, with demographic info, prior experience with\nAI text generation models, past programming knowledge (Likert\nscores 1-5; 5 highest), and whether they had ever worked on a\nproject involving evaluating LLMs. Participants then watched a\nfive-minute video introducing the interface.\nIn the structured task, participants navigated a mock prompt\nengineering scenario in two parts, where a developer first chooses a\nmodel, then iterates on a prompt to improve performance according\nto some criteria. We asked participants to choose a model and\nprompt to ‚Äúprofessionalize an email‚Äù (translate a prospective email\nmessage to sound more professional). 7 In part one, participants\nwere given a preloaded flow, briefed on the scenario (‚ÄúImagine you\nare a developer... ‚Äù ), and presented with two criteria on a slip of\npaper: (1) The response should just be the translated email , and (2)\nThe email should sound very professional . Participants were tasked\nwith choosing the ‚Äòbest‚Äô model given the criteria, and to justify\ntheir choice. All participants saw the exact same cached responses\nfrom GPT-4, Claude-2, and PaLM2, in the exact same order, for the\nprompt ‚ÄúConvert the following email to have a more professional\nand polite tone‚Äù with four example emails (e.g., ‚ÄúWhy didn‚Äôt you\nreply to my last email???‚Äù ). After they spent some time inspecting\nresponses, we asked them to add one more example to translate\nand to increase Num of responses per prompt , to show them how\nthe same LLMs can vary on the same prompt.\nOnce participants chose a model, we asked them to remove all\nbut their selected model. We then guided them to abstract the pre-\ngiven ‚Äúcommand prompt‚Äù into a TextFields, and add at least two\nmore command prompts of their own choosing. On a slip, we gave\nthem a third criteria:‚Äúthe email should be concise. ‚Äù After participants\ninspected responses and started to decide on a ‚Äòbest‚Äô prompt, we\nasked them to add one code Evaluator and Vis Node, plotting lengths\nof responses by their command variable. After spending some time\nwith the plot, participants were asked to decide.\nThe remaining study time was taken up by an unstructured,\nexploratory section meant to emulate how users‚Äîprovided enough\nsupport and documentation‚Äîmight use ChainForge to investigate\na hypothesis about LLM behavior that mattered to them. We asked\nparticipants a day before their study to think up an idea, question,\nor hypothesis they had about AI text generation models, and gave a\nlist of six possible investigation areas (e.g., checking models for bias,\nconducting adversarial attacks), but did not provide any concrete\nexamples. During the study, participants then explored their idea\nthrough the interface with the help of the researcher. Importantly,\nresearchers were instructed to only support participants in pursuit\n7Although we could have contrived a task with an objective ‚Äòbest‚Äô answer‚Äîbest model\nor prompt template‚Äîthat wouldn‚Äôt reflect the kind of ambiguities present in many\nreal world decisions around LLM usage.\nChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing CHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA\nof their investigations, not to guide them towards particular do-\nmains of interest. The one exception is where a participant only\nqueried a single model; in this case, the researcher could suggest\nthat the user try querying multiple models at once. Participants\nused the exact same interface as the public version of our tool,\nand had access to OpenAI‚Äôs gpt-3.5 and gpt-4, Anthropic‚Äôs claude-2,\nGoogle‚Äôs chat-bison-001, and HuggingFace models.\nAfter the tasks, we held a brief post-interview (5-10 min), asking\nparticipants to rate the interface (1-5) and explain their reasoning,\nwhat difficulties they encountered, suggestions for improvements,\nwhether they felt their understanding of AI was affected or not,\nand whether they would use the interface again and why.\n5.1 Recruitment, Participant Demographics,\nand Data Analysis\nWe recruited in-lab participants around our U.S.-based university\nthrough listservs, Slack channels, and flyers. We tried to expand\nour reach beyond people experienced in CS and ML, specifically\ntargeting participants in humanities and education. Participants\nwere generally in their twenties to early thirties (nine 23-27; eight\n28-34; three 18-22; one 55-64), predominantly self-reported as male\n(14 men, 7 women), and largely had backgrounds in computing,\nengineering, or natural sciences (ten from CS, data science, or tech;\nseven from bioengineering, physics, material science, or robotics;\ntwo from education; one from medicine and one from design). They\nhad a moderate amount of past experience with AI text generation\nmodels (mean=3.3, stdev=1.0); one had none. Past Python program-\nming experience varied (mean=3.1, stdev=1.3), with less experience\nin JavaScript (mean=2.0, stdev=1.3); two had no programming expe-\nrience. Eight had ‚Äúworked on an academic study, paper, or project\nthat involved evaluating large language models. ‚Äù All participants\ncame in to the lab, with studies divided equally among the first three\ncoauthors. Each study took 75 minutes, and participants were given\n$30 in compensation (USD). Due to ethical concerns surrounding\nthe overuse of Amazon gift cards in human subject studies [27, 31],\nwe paid all participants in cash.\nFor our interview study, we sought participants who had already\nused ChainForge for real-world tasks, reaching out via social media,\nGitHub, and academic networks. The first author held six semi-\nstructured, 60 min. interviews with eight participants (in two inter-\nviews, two people had worked together). Interviews took place via\nvideoconferencing. Interviewees were asked to share their screen\nand walk through something they had created with ChainForge.\nUnlike our in-lab study, we kept interviewees‚Äô screen recordings\nprivate unless they allowed us to take a screenshot, since real-world\nusers are often working with sensitive information. Interviewees\ngenerously volunteered their time.\nWe transcribed all 32 hours of screen recordings and interviews,\nadding notes to clarify participant actions and references (e.g.,\n‚Äú[Opens inspector; scrolls to top]. It seems like it went fast enough...\n[Reading from first email group] ‚ÄòHi... ‚Äù‚Äô ). We noted conceptual or\nusability problems and the content of participant references. We an-\nalyzed the transcripts through a combination of inductive thematic\nanalysis through affinity diagramming, augmented with a spread-\nsheet to list participants‚Äô ideas, behaviors (nodes added, process of\ntheir exploration, whether they imported data, etc), and answers\nto post-interview questions. For our in-lab study, three coauthors\nseparately affinity diagrammed three transcripts each, then met\nand joined the clusters through mutual discussion. The merged\ncluster was iteratively expanded with more participant data until\nclusters reached saturation. For interviews, the first author affinity\ndiagrammed all transcripts to determine themes. In what follows,\nin-lab participants are P1, P2, etc.; interviewees are Q1, Q2, etc.\n6 MODES OF PROMPT ENGINEERING AND\nLLM HYPOTHESIS TESTING\nWhat process do people follow when prompt engineering and test-\ning hypotheses about LLM behavior more generally? Before we\nbreak down findings per study, we provide a birds-eye view of\nhow participants in general used ChainForge. Synthesizing across\nstudies, we find that people tend to move from an opportunistic\nexploration mode, to a limited evaluation mode, to an iterative re-\nfinement mode. About half of our in-lab users, especially end-users\nwith limited prior experience, never left exploration mode; while\nprogrammers or auditors of LLMs quickly moved into limited eval-\nuation mode. Some interviewees had disconnected parts of their\nflows that corresponded to exploration mode, then would scroll\ndown to reveal extensive evaluation pipeline(s), explaining they had\ntransferred prompts from the exploratory part into their evaluation.\nIn Appendix A, we provide one Case Study for each mode. Notice\nhow these modes correspond to users moving from the left side of\nFig. 2 towards the right.\nOpportunistic exploration mode is characterized by rapid\niteration on prompts, input data, and hypotheses; a limited number\nof prompts and input data; and multi-model comparison. Users\nprompt / inspect / revise : send off a few prompts, inspect results,\nrevise prompts, inputs, hypotheses, and ideas. In this mode, users\nare sending off quick experiments to probe and poke at model be-\nhavior (‚Äúthrow things on the wall to see what‚Äôs gonna stick‚Äù , Q3).\nFor instance, participants who conducted adversarial attacks like\njailbreaking [6] would opportunistically try different styles of jail-\nbreak prompts, and were especially interested in checking which\nmodel(s) they could bypass.\nLimited evaluation mode is characterized by moving from\nad-hoc prompting to prototyping an evaluation . Users have reached\nthe limits of manual inspection and now want a more efficient, ‚Äúat-\na-glance‚Äù test of LLM behavior, achieved by encoding criteria into\nautomated evaluator(s) to score responses. Usersprompt / evaluate\n/ visualize / revise : prompt model(s), score responses downstream\nin their chain, visualize results, and revise their prompts, input data,\nmodels, and/or hypotheses accordingly. Hallmarks of this mode are\nusers setting up an analysis pipeline, iterating on their evaluation\nitself, and ‚Äúscaling up‚Äù input data. The evaluation is ‚Äúlimited‚Äù as\nevaluations at this stage are often ‚Äúcoarse‚Äù‚Äîfor example, rather than\nchecking factuality, check if the output is formatted correctly.8\n8Crucially, this mode does not imply multiple prompts: a few in-lab participants set up\nan evaluation pipeline that only sent off a single prompt . Though these participants did\nadd a TextFields or Tabular Data node, it only had a single value/field. Indications were\nthat, with more time, they would have ‚Äúscaled up‚Äù how many inputs or parameters\nthey were sending to test more specific hypotheses. However, some users might have\nalso had conceptual trouble imagining how to scale up their testing; we discuss this\nmore later.\nCHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA Arawjo, Swoopes, Vaithilingam, Wattenberg & Glassman\nIterative refinement mode is characterized by having an already-\nestablished evaluation pipeline and criteria and tweaking prompt\ntemplates and input data through further parametrization or di-\nrect edits, setting up one-off evaluations to check effects of tweaks,\nincreasing input data complexity, and removing or swapping out\nmodels. Users tweak / test / refine : modify or parametrize some\naspect of their pipeline, test how tweaks affect outputs compared to\ntheir ‚Äúcontrol‚Äù, and refine the pipeline accordingly. The key differ-\nence between limited evaluation and iterative refinement is in the\nsolidity of the chain: here, users‚Äô prompts, input data, and evalua-\ntion criteria have largely stabilized, and they are looking tooptimize\n(e.g., through tweaks to their prompt, or extending input data to\nidentify failure modes). Some interview participants had reached\nthis mode, and were refining prompt templates or scaling up input\ndata. The few in-lab participants that had brought in ‚Äúprompt engi-\nneering‚Äù problems by importing prompt templates or spreadsheets\nwould immediately set up evaluation pipelines, moving towards\nthis mode.\nThese modes are suggestive and not rigidly linear; e.g., users may\nscrap their limited evaluation and return to opportunistic explo-\nration. In Sections 7 and 8 below, we delve into specific findings for\neach study. For our in-lab study, we describe how people selected\nprompts and models, how ChainForge supports exploration and\nunderstanding, and note conceptual and usability issues. For our\ninterview study, we focus on what differed from in-lab users.\n7 IN-LAB STUDY FINDINGS\nOn average, participants rated the interface a 4.19/5.0 (stdev=0.66).\nNo participant rated it lower than a three. When asked for a reason\nfor their score, participants generally cited minor usability issues\n(e.g., finicky when connecting nodes, color palette, font choice, more\nplotting options). Eighteen participants wanted to use the interface\nagain; five before being explicitly asked. Some just wanted to play\naround, citing model comparison and multi-response generation.\nParticipants who had prior experience testing LLM behavior in\nacademia or industry cited speed and efficiency of iteration as the\nprimary value of the tool (‚ÄúIf I had started with using this, I‚Äôd have\ngotten much further with my prompt engineering... This is much faster\nthan a Jupyter Notebook‚Äù , P4; ‚Äúthis would save me half a day for\nsure... You could do a lot of stuff with it‚Äù , P21). Participants mentioned\nprior behavior as having multiple tabs open to chat with different\nmodels, manually copying responses into spreadsheets, or writing\nprograms. Three wanted to use ChainForge for research.\nWe recount participants‚Äô behavior in the structured task to\nchoose a model and prompt template, overview how ChainForge\nsupported participants‚Äô explorations and understanding, and reflect\non pain points.\n7.1 How People Decide on Models and Prompts\nHow do people choose a text generation model or prompt, when\npresented with side-by-side responses? People appear to weigh\ntrade-offs in response quality for different criteria and contexts of\nusage. Participants would perceive one prompt or model to excel\nin one criteria or context, but do poorly in another; for another\nprompt or model, it was vice-versa. Here, we use ‚Äúcriteria‚Äù liberally\nFigure 6: P17 plots the response lengths of three command\nprompts, augmenting her theories about each prompts‚Äô per-\nformance.\nto mean both our explicit criteria and also participants‚Äô tacit pref-\nerences. Participants would also implicitly rank criteria, assigning\nmore weight to some over others, and refer to friction between\ncriterias (e.g., P2 ‚Äúprefer[red] professional over concise, because it\n[email] can be concise, but misconstrued‚Äù ). Moreover, seeing mul-\ntiple representations of prompt performance, each of which better\nsurfaced aspects of responses that corresponded to different crite-\nria, could affect participants‚Äô theorizing and decision-making. We\nunpack these findings here.\nFor the first part of our structured task, participants reached no\nconsensus on which model performed ‚Äúbetter‚Äù: eight chose PaLM2,\nseven GPT-4, and six Claude-2. There was no pattern in reasoning.\nParticipants did notice similar features of each models‚Äô response\nstyle, but how they valued that style differed. Some participants\nliked some models for the same reason others disliked them; for\ninstance, P1 praised PaLM2 for its lengthy emails; while P17 chose\nGPT-4 because ‚ÄúPaLM2 is too lengthy. ‚Äù Although we had deliber-\nately designed our first criteria against the outputs of Claude (for\nits explanatory information around the email), some participants\nstill preferred Claude, perceived its explanations as useful to their\nimagined users, or preferring its writing style. In the unstructured\ntask, participants developing apps also mentioned exogenous fac-\ntors such as pricing, access, and response time when comparing\nmodels.\nHow did people choose one prompt among multiple? Like when\nchoosing models, participants appeared to weigh trade-offs between\ndifferent criteria and contexts. Having multiple representations (e.g.,\nplots of prompt performance) could especially give users a different\n‚Äúview‚Äù that augmented understanding and theorizing. P1 describes\ntensions between his and his users‚Äô needs, referencing both manual\ninspection and a plot of response lengths by prompt:\n‚ÄúIf I am a developer, I like this one [third prompt] be-\ncause it will help me better to pass the output... But if\nthey [users] have a chance to see this graph [Vis node],\nthey would probably choose this one [second prompt]\nbecause it fits their needs and it‚Äôs more concise [box-\nand-whiskers plot has smallest median and lowest vari-\nability]... So I think it depends on the view. ‚Äù\nChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing CHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA\nMultiple representations could also augment users‚Äô theorizing\nabout prompting strategy. For instance, P17 had three command\nprompts, each iteration just tacking more formatting instructions\nonto the end of the default prompt (Figure 6). Comparing between\nher plot and Table Layout, she theorizes: ‚ÄúAfter adding ‚Äògenerate\nresponse in an email format‚Äô it made it lengthier... But if I don‚Äôt say\n‚Äòwith concise wording‚Äô... sometimes it generates responses that are\nthree paragraphs, for a really simple request. So I would [go with] the\nsecond instruction... [and its] the length difference [variance] is less. ‚Äù\nSeeing that one prompt resulted in shorter orless variable responses\ncould cause a participant to revise an earlier opinion. After noticing\nvia the plot that his first command ‚Äúseem[s] more consistent‚Äù , P4\nwanted to mix features from it into his chosen prompt to improve\nthe latter‚Äôs concision, as he still preferred the latter‚Äôs textual quality.\nThese observations suggest that systematic evaluations can con-\ntest fixation [45] caused by manual inspection. However, it also\nreveals users may need multiple representations, or they will make\ndecisions biased by features that are easiest to spot in only one.\nWith multiple, they can make decisions more confidently, mixing\nand matching parts of each prompt to progress towards an imag-\nined ideal. The benefit of prompt comparison also underscores the\nimportance of starting from a variety of prompts‚Äîsimilar to past\nwork [45], many of our participants struggled to come up with a va-\nriety of prompts, with thirteen just perturbing our initial command\nprompt. We reflect on this more in our Discussion.\n7.2 ChainForge Supported a Variety of Use\nCases and Users\nParticipants brought in a variety of ideas to the unstructured task,\nranging from auditing of LLM behavior to refining an established\nprompt used in production. We recount three participants‚Äô experi-\nences as Case Studies in Appendix A, each corresponding to a Mode\nof Usage from Section 6. Seven participants evaluated model be-\nhavior given concrete criteria, with six importing prior data; ideas\nranged from testing model‚Äôs ability to understand program patch\nfiles, to classifying user attitudes in messaging logs. Nine audited\nmodels in opportunistic exploration mode, looking for biases or\ntesting limits (e.g., asking undecidable questions like ‚ÄúDoes God\nexist?‚Äù, P20). Of these users, four conducted adversarial attacks\n[6], seemingly influenced by popular culture about jailbreaking. P9\nand P15, both with no programming experience, used the tool to\naudit behavior, the former comparing models‚Äô ability to generate\nculturally-appropriate stories about Native Alaskans. Others were\ninterested in generating text for creative writing tasks like travel\nitineraries. Participants often searched the internet, such as cross-\nchecking factual data, copying prompts from Reddit, or evaluating\ncode in an online interpreter. Overall, nine participants imported\ndata (six with spreadsheets) to use in their flow.\n7.3 ChainForge Affected Participants‚Äô\nUnderstanding of AI Behavior or Practice\nIn the post-interview, fifteen participants said their understanding\nof AI was affected by their experience. Six were surprised by the\nperformance of Claude-2 or PaLM2, feeling that, when confronted\nwith direct comparisons to OpenAI models, they matched or ex-\nceeded the latter‚Äôs performance. Five said that their strategy of\nprompting or prompt engineering had changed (‚Äú[Before], I wasn‚Äôt\ndoing these things efficiently... I [would] make minor modifications\nand rerun, and that would take hours... Here, since everything is laid\nout for me, I don‚Äôt want to give up‚Äù , P4). Others less experienced\nwith AI models learned about general behavior. P16, who had never\nprompted an AI model before, ‚Äúrealized that different models have\ncompletely different ways of understanding my prompts and hence\nresponding, they also have a completely different style of response. ‚Äù\nP15, covered in Case Study A.1, said she had lost ‚Äútrust‚Äù in AI.\n7.4 Challenges and Pain-points\nThough many participants derived value from ChainForge, that\nis not to say their experience was frictionless. The majority of\nusability issues revolved around the flow UI, such as needing to\nmove nodes around to make space, connecting nodes and deleting\nedges; others related to inconsistencies in the ordering of plotted\nvariables, and wanting more control over colors and visualizations.\nSome participants also encountered conceptual issues, which some-\ntimes indicate users getting used to the interface. The most common\nconceptual issue was learning how prompt templating worked, and\nespecially, forgetting to declare input variables in Prompt Nodes.\nOnce users learned how to template, however, the issue often dis-\nappeared (‚Äúprompt variables... there‚Äôs a bit of a learning curve, but\nI think it makes sense, the design choice‚Äù , P13). Learning template\nvariables seemed related to past programming expertise and not\nAI, suggesting users without any prior programming experience\nwill need extra resources.9\nImport to reflect on is that, in the lab, researchers were on-hand to\nguide users. Although users were the ones suggesting ideas‚Äîoften\nhighly domain-specific ones‚Äîresearchers could help users with\nways to implement them and overcome conceptual hurdles. Some\nend-users and even a few users with substantial prior experience with\nAI models or programming with LLM APIs appeared to have trouble\n‚Äúscaling up, ‚Äù or systematizing, their evaluations. For example, P10\nrated themselves as an expert in Python (5) and had conducted\nprior research on LLM image models. They set up an impressive\nevaluation, complete with a prompt template, Prompt Node, Chat\nTurn, Simple Evaluator and Vis nodes, but ultimately only sent off a\nsingle prompt to multiple models. We remark more on this behavior\nin Discussion.\n8 INTERVIEWS WITH REAL-WORLD USERS\nOur interview findings complement, but in important places di-\nverge, from our in-lab studies. Like many in-lab participants, real-\nworld users praised ChainForge‚Äôs features and used it for goals we\nhad designed for‚Äîlike selecting models or prompt testing‚Äîhowever,\nsome things real users cared about were hardly, if ever mentioned\nby in-lab participants. As we analyzed the data and compared it\nwith our in-lab study, we realized that many user needs and pain-\npoints revolve around the fact that they were using ChainForge to\nprototype data processing pipelines , a wider context that re-frames\nthe tasks we had designed ChainForge to support as subtasks of\na larger goal. Interviewees remarked most about easing the export\n9For instance, P16, who had never prompted an AI model before, attributed her adept-\nness with templating to prior programming experience. By contrast, P9 had no pro-\ngramming experience and struggled; after the intro video he was‚Äúa little overwhelmed‚Äù ,\nasking: ‚Äúwhat language am I in?‚Äù\nCHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA Arawjo, Swoopes, Vaithilingam, Wattenberg & Glassman\nand sharing of data from ChainForge, adding processor nodes , the\nimportance of the Inspect Node for sharing and rapid iteration, and\nthe open-source nature of the project for their ability to adapt the\ncode to their use case. We discuss these insights more below; but\nfirst, we provide an overall picture, reviewing similarities, use cases,\nand concrete value that real-world users derived from ChainForge.\nWe list interview participants in Table 2, with use cases and\nnodes used. The Outcome column suggests the actionable value that\nChainForge provided. Note that Q1 and Q2‚Äôs primary use case was\nbuilding on the source code to enable their HCI research project. All\nsix users of the interface found it especially useful for prototyping\nand iterating on prompts and pipelines (e.g., Q5:‚ÄúI see the use case for\nChainForge as a very good prompt prototyping environment‚Äù ). Usage\nreflected modes of limited evaluation and iterative refinement , with\nmultiple participants describing a prompt/evaluate/visualize/revise\nloop: query LLM(s), evaluate responses and view the plot, then\nrefine prompts or change models, until one reaches the desired\nresults. For instance, Q3 described tweaking a prompt template until\nthe LLM output in a consistent format, facilitated by maximizing\n100% bars in a Vis Node across all input data. Some participants\nsaw ChainForge as a rapid prototyping tool missing from the wider\nLLMOps ecosystem, a tool they used ‚Äúuntil I get to the point where I\ncan actually write it into hard code‚Äù (Q4). Three appreciated howfew\nnodes there were in ChainForge given its relative power, compared\nto other node-based interfaces (e.g., Q8:‚ÄúIt‚Äôs impressive. What you‚Äôre\nable to accomplish with so few‚Äù ). They worried that adding too many\nnew nodes would make the interface more daunting for new users.\nQ4 and Q7 found it more effective than Jupyter notebooks (Q7: ‚ÄúI\nenjoyed ChainForge... because I could run the whole workflow over\nand over again, and... in Jupyter, that was not easy‚Äù ). In the rest of\nthis section, we expand upon differences from our in-lab study.\n8.1 Prototyping data processing pipelines\nFive interviewees were using ChainForge not (only) for prompt\nengineering or model selection, but for on-demand prototyping of\ndata processing pipelines involving LLMs . All imported data from\nspreadsheets, then would send off many parametrized prompts,\niterate on their prompt templates and pipelines, and ultimately\nexport data to share with others. Such users also used ChainForge\nfor at least one of its intended design goals, but always in service\nof their larger data processing goal. For Q7, ‚Äúthe idea was to write\na pipeline that... helps you with this whole process of data cleaning. ‚Äù\nFor him, ChainForge was ideal for ‚Äúwhenever you have a variety\nof prompts you want to use on something particular, like a data set.\nAnd you want to explore or investigate something. ‚Äù Another user,\nQ3, would open his refined flow, edit one value, re-run it and then\nexport the responses to a spreadsheet. Like other participants, he\nremarked on ChainForge‚Äôs combinatorial power as its chief benefit,\ncompared to other tools (‚ÄúThis tool is strong at prompt refining. With\n[Flowise]...Let‚Äôs say I wanted to try multiple [input fields]. I don‚Äôt\nthink I could do that‚Äù ). Participants also mentioned iterating on\nthe input data as part of the prototyping process. Finally, related\nto data processing, three users wished for processor nodes, like\n‚Äújoin‚Äù nodes to concatenate LLM responses, and in one case were\nmanually copying LLM outputs into a separate flow to emulate\nconcatenation. Note that many needs and pain-points below are\nrelated to data processing.\n8.2 Getting data out and sharing with others\nMany participants wanted to export data out of ChainForge. This\nwas also the most common pain point, especially when transitioning\nfrom a prototyping stage‚Äîwhich they perceived as ChainForge‚Äôs\nstrong point‚Äîto a production stage (e.g., ‚Äúit would be helpful when\nwe are out of this prototyping stage, that the burden or the gap‚Äî\nchanging the environment... gets tightened‚Äù , Q5). Needs broke down\ninto two categories: exporting for integration into another applica-\ntion, and exporting for sharing results with others. For the former,\ndeveloper users would use ChainForge to battle-test prompts, model\nbehavior, and/or prompt chains, but then wished for an easier way\nto export their flows to text files or app building environments.10\nFor the latter, five interviewees shared results with others, whether\nthrough files, screenshots of their flows, exported Excel spread-\nsheets of responses, or copied responses. Q5 and Q6 stressed the\nimportance of the Inspect Node‚Äîa node that no in-lab participant\nused or mentioned (‚Äú[Once] the result is worth documenting, you cre-\nate an Inspect node. ‚Äù ). They took screenshots of flows and sent them\nto clients, in one case convincing a client to move forward with a\nproject. The anticipation of sharing with others also could change\nbehavior. Q3 had several TextFields nodes with only a single value,\n‚Äúbecause I knew that it was something that essentially other teams\nmight want to change. ‚Äù . Sharing could also be a pain-point, with\ntwo wanting easier shareable ‚Äúreports‚Äù of their analysis results.\n8.3 Pain points: Hidden affordances and friction\nduring opportunistic exploration mode\nLike in-lab participants, interviewees also encountered usability\nand conceptual issues. A common theme was individual users ex-\npressing a need for features that already exist but are relatively\nhidden, surfaced only through examples or documentation. These\nhidden affordances included implicit template variables , metavari-\nables, and template chaining. The former two features address users‚Äô\nneed to reference upstream metadata‚Äîmetadata associated with\ninput data or responses‚Äîfurther downstream in a chain.11 Another\npain point was friction during the opportunistic exploration phase.\nIn Section 6, we mentioned some interviewees had disconnected\nregions of their flows, with one region we termed opportunistic\nexploration mode (rapid, early-stage iteration through input data,\nprompts, models, and hypotheses; usually, a chain of three nodes,\nTextField-Prompt-Inspect). In this mode, some interviewees pre-\nferred to inspect responses directly on the flow with an Inspect\nNode (instead of the pop-up window), as it facilitated rapid iteration.\n10This does not, however, mean they perceived ChainForge as an ‚Äúapp building‚Äù tool‚Äî\nsome even expressed worry about it trying to do too much. When asked about how he\nwould feel if ChainForge supported app development, Q3 remarked,‚ÄúI just worry about\nit [ChainForge] becoming too complicated. Like, are you building the app side of it?‚Äù‚Äù He\nsaid even if ChainForge supported app-building, it would need different ‚Äúmodes‚Äù: ‚Äúapp\nbuilding mode or prompt refining mode. ‚Äù\n11For example, in ChainForge one can define a column of a table and then refer to it\ndownstream via a metavariable (Fig. 5). However, Q5 and Q6 seemed unaware of this\nfeature and had implemented a workaround. For implicit template variables, Q4 needed\nto reference an upstream value {gender} later downstream in a prompt chain, and was\nunaware that an implicit template variable could accomplish this (e.g. {#gender}).\nChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing CHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA\nID(s) Location Work How dis-\ncovered\nUse Case(s) Major nodes and features used Outcome\nQ1-2 U.S.\n(west)\nAcad. Medium\npost\nAdapting code for\nLLM image model\nprototyping\nAdapted source code (esp. nodes, model\nquerying, cacheing)\nSubmitting HCI paper to major confer-\nence in time for deadline\nQ3 U.K. Ind. GitHub Supporting require-\nments analysis in\nsoftware testing\nPrompt, Tabular Data, TextFields, Vis, JS\nEval; export to excel, metavariables\n‚ÄúI‚Äôve had good conversations with [other]\ndevelopers as a result‚Äù\nQ4 U.S.\n(east)\nAcad. Hacker-\nNews\nAuditing models for\ngender bias\nPrompt, TextFields, Chat Turn, Inspect\nnode; prompt chaining, LLM scoring\nImproved prompt templates for a pipeline\nthey are building in Python\nQ5-6 Germany Ind. LearnPro-\nmpting.org\nInformation synthesis\nand extraction from\ndocuments\nPrompt, Tabular Data, JS Eval, Vis, Com-\nment, Inspect node; comparing models\n‚ÄúConvinced client [to] continue with the\nnext phase of [a] project‚Äù\nQ7 U.S.,\nAustria\nAcad. Word of\nmouth\nCleaning a column of\ntabular data\nPrompt, Tabular Data, TextFields, JS Eval,\nVis; variables in code eval\nDecided that LLMs were not reliable\nenough for their task\nQ8 U.S.\n(cen-\ntral)\nAcad. Twitter Extracting and format-\nting info from podcast\nepisode metadata\nPrompt, TextFields, CSV, JS Eval, Vis,\nComment; comparing models, prompt\nchaining, template chaining\nFound that neither OpenAI model pro-\nduced good enough outputs; now look-\ning into local models\nTable 2: Our interviewees. All interviewees imported data into ChainForge, with the exception of Q1-2 (who adapted source\ncode). Interviewees commonly had multiple evaluation branches in their flows, as well as multiple flows in the same document.\nThey wanted an even more immediate, in-context way to read LLM\nresponses that would not require them to attach another node.12\n8.4 Open-source flexibility\nMultiple interviewees mentioned looking at our source code, and\ntwo projects extended it. Q5 and Q6, employees of a consulting\nfirm that works with the German government, extended the code to\nsupport a German-based LLM provider, AlephAlpha [1], complete\nwith a settings screen. They cited the value of supporting European\nbusinesses and GDPR data protection laws: ‚Äúthe government [of\nGermany] wants to support it. It‚Äôs a local player... [and] There‚Äôs a\nstrong need to to hide and to to protect your data. I mean, GDPR, it‚Äôs\nvery strict in this. ‚Äù Their goal was to use ChainForge to determine\n‚Äúif it makes sense to switch to‚Äù the German model for their use\ncases, over OpenAI models. HCI researchers Q1 and Q2‚Äôs chief\ninteraction with the tool was its source code, finding it helpful for\njumpstarting a project on a flow-based tool for LLM image model\nprototyping. Q2 appreciated the ‚Äúthought put into‚Äù caching, Prompt\nNode progress bar, and multi-model querying, adding: ‚ÄúIt was very\neasy for me to set up ChainForge... [and it was] surprisingly easy\nto [extend]... a lot easier than I had expected. ‚Äù They said that the\njump-start ChainForge provided was a chief reason they were able\nto complete their project in time to submit a paper to the annual\nCHI conference.\n9 DISCUSSION AND CONCLUSION\nOur observations suggest that ChainForge is useful both in itself,\nbut also as an ‚Äòenabling‚Äô contribution, an open-source project which\nothers can extend (and are extending) to investigate their own\nideas and topics, including other research publications to this very\nconference. Given that ChainForge was released only a few months\nago, we believe the stories presented here provide evidence for its\nreal-world usefulness. In what follows, we review our key findings.\n12This need was again reflected in a later GitHub Issue and has since been addressed\nwith a pull-out inspector drawer.\nOur work represents one of the only ‚Äúprompt engineering‚Äù sys-\ntem contributions with data about real-world usage, as opposed to\nin-lab studies on structured tasks. Some of what real users cared\nabout, like features for exporting data and sharing, were absent\nfrom our in-lab study‚Äîand are, in fact, also absent from similar\nLLM-prompting-system research with in-lab studies [5, 25, 43‚Äì45].\nMost surprising (to us) was that some knowledge workers were\nusing ChainForge for a task we had never anticipated‚Äî data pro-\ncessing. Although we only had six interface users in our interview\nstudy, the only two in-lab participants in startups, P8 and P4, were\nboth testing LLMs‚Äô ability to process and reformat data. Most prior\nLLM tools target sensemaking [15, 36], prompt engineering [14, 25],\nor app building [43], but do not specifically target, or even mention,\ndata processing. Our findings suggest a need for systems to sup-\nport on-demand creation of data processing pipelines involving LLMs,\nwhere the purpose is not (always) to make apps, but simply process\ndata and share the results. ChainForge‚Äôs combinatorial power‚Äîthe\nability to send off many queries at once, parametrized by imported\ndata‚Äîappeared key to supporting this need. Future systems should\ngo further by providing users more accessible ways to reference\nupstream metadata further downstream in their chain (see 8.3).\nSecond, we identified three modes of prompt engineering and\nLLM hypothesis testing: opportunistic exploration , limited evalua-\ntion, and iterative refinement . The first mode is similar to Barke\net al. ‚Äôs exploration mode for GitHub CoPilot [2]. Future systems\nshould explicitly consider these modes when designing and framing\nthe work. For instance, users often too quickly enter iterative re-\nfinement mode‚Äîrefining on the first prompt they try‚Äîrather than\nexploring a variety before settling on one [45]. If a prompt engineer-\ning tool only targets iterative refinement, then the opportunistic\nexploration stage‚Äîfinding a good prompt to begin with‚Äîmay be\ntoo quickly skirted over, trapping users in potentially suboptimal\nprompting strategies. These modes also suggest design opportuni-\nties. For instance, we believe that ChainForge‚Äôs design could have\nbetter supported opportunistic exploration mode, with some users\nCHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA Arawjo, Swoopes, Vaithilingam, Wattenberg & Glassman\nwanting a simpler way to inspect LLM responses in-context (8.3).\nOne design solution may be to concretize each mode into sepa-\nrate, related interfaces or layouts‚Äîe.g., a more chat-like interface\nfor exploration mode, that then facilitates the transition to later\nmodes, each with dedicated interfaces. Prior LLM-prompting sys-\ntems seem to target opportunistic exploration [15, 36] or iterative\nrefinement [25, 35], but overlook limited evaluation : an important\nmid-way point characterized by prototyping small-scale, quick-and-\nmessy evaluations on the way to greater understanding. Future\nwork might target the prototyping of on-demand LLM evaluation\npipelines themselves (see ‚Äúmodel sketching‚Äù for inspiration [18]).\nThird, we found that when people choose different prompts and\nmodels, they weigh trade-offs in performance for different criteria\nand contexts, and bring their own perspectives, values, preferences,\nand contexts to bear on decision-making. Having multiple represen-\ntations of responses seemed to help participants weigh trade-offs,\nrank prompts and models, develop better mental models, and make\nrevisions to their prompts or hypotheses more confidently. Con-\nnecting to theories of human learning [ 10, 23], the case study in\nA.1 suggests that cross-model comparison might also help novices\nimprove mental models of AI by forcing them to encounter dif-\nferences in factual information, jarring AI over-reliance [20]. The\nsubjectivity of choosing a model and prompt implies that, while\nLLMs can certainly help users generate or evaluate prompts [5, 46],\nthere will never be such a thing as fully automated prompt engi-\nneering. Rather than framing prompt engineering (purely) as an\noptimization problem, projects looking to support prompt engineer-\ning should instead look for ways to give users greater control over\ntheir search process (e.g., ‚Äústeering‚Äù [5, 46]).\nA final point and caveat: while users found ChainForge useful\nfor implementation and iteration, including on real-world tasks,\nmore work needs to be done on conceptualization and planning\naspects, to help users move out of opportunistic exploration into\nmore systematic evaluations. In-lab users seemed limited in their\nability to imagine systematizing their tests, even a few with prior\nexpertise in AI or programming with LLM APIs . This extends prior\nwork studying how ‚Äúnon-AI-experts‚Äù prompt LLMs [45], suggesting\neven people who otherwise perceive themselves to be AI experts\nmay have trouble systematizing their evaluations. Since LLMs are\nnondeterministic (at least, often queried at non-zero temperatures)\nand prone to unexpected jumps in behavior from small perturba-\ntions, it is important that future systems and resources help reduce\nfixation and guide users from early exploration into systematic\nevaluations. We might leverage concepts from tools designed for\nmore targeted use cases; e.g., the auditing tool AdaTest++ provides\nusers ‚Äúprompt templates that translate experts‚Äô auditing strategies\ninto reusable prompts‚Äù [33, p. 15-6]. Other work supports creation\nof prompts or searching of a ‚Äúprompt space‚Äù [25, 34, 35]. To support\nsystematization/scaling up, we might also employ an interaction\nwhereby a user chats with an AI that sketches out an evaluation\nstrategy.\n9.1 Limitations\nOur choice to use a qualitative evaluation methodology derived\nfrom well-known difficulties around toolkit research [19, 28], con-\ncerns about ecological validity, and, most importantly, from the\nfact that we could not find a prior, well-established interface that\nmatched the entire featureset of ChainForge. Our goal was thus\nto establish a baseline system that future work might improve\nupon. While we believe our qualitative evaluation yielded some im-\nportant findings, more quantitative, controlled approaches should\nbe performed on parts of the ChainForge interface to answer tar-\ngeted scientific questions. Our in-lab study was also of a relatively\nshort duration (75 min); future work might observe changes in\nuser behavior over longer timeframes, for instance with a multi-\nweek workshop. Finally, for our interview study, we acknowledge\na self-selection bias, where participating interviewees may already\nhave found ChainForge useful, missing users who did not. Our\nin-lab study provided some insights‚Äîwe speculate that users‚Äô prior\nexposure to programming was important to the quality of their\nexperience.\nACKNOWLEDGMENTS\nThis work was partially funded by the NSF grants IIS-2107391, IIS-\n2040880, and IIS-1955699. Any opinions, findings, and conclusions\nor recommendations expressed in this material are those of the\nauthor(s) and do not necessarily reflect the views of the National\nScience Foundation.\nREFERENCES\n[1] Aleph-Alpha. 2023. Aleph-Alpha. https://www.aleph-alpha.com Accessed: Sep\n2 2023.\n[2] Shraddha Barke, Michael B James, and Nadia Polikarpova. 2023. Grounded\ncopilot: How programmers interact with code-generating models. Proceedings of\nthe ACM on Programming Languages 7, OOPSLA1 (2023), 85‚Äì111.\n[3] Luca Beurer-Kellner, Marc Fischer, and Martin Vechev. 2023. Prompting is pro-\ngramming: A query language for large language models. Proceedings of the ACM\non Programming Languages 7, PLDI (2023), 1946‚Äì1969.\n[4] Markus Binder, Bernd Heinrich, Marcus Hopf, and Alexander Schiller. 2022.\nGlobal reconstruction of language models with linguistic rules‚ÄìExplainable AI\nfor online consumer reviews. Electronic Markets 32, 4 (2022), 2123‚Äì2138.\n[5] Stephen Brade, Bryan Wang, Mauricio Sousa, Sageev Oore, and Tovi Grossman.\n2023. Promptify: Text-to-Image Generation through Interactive Prompt Explo-\nration with Large Language Models. arXiv preprint arXiv:2304.09337 (2023).\n[6] Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li, Haoyu\nWang, Tianwei Zhang, and Yang Liu. 2023. Jailbreaker: Automated Jailbreak\nAcross Multiple Large Language Model Chatbots. arXiv preprint arXiv:2307.08715\n(2023).\n[7] Harrison Chase et al. 2023. LangChain. https://pypi.org/project/langchain/.\n[8] FlowiseAI, Inc. 2023. FlowiseAI Build LLMs Apps Easily. flowiseai.com.\n[9] Nat Friedman, Zain Huda, and Alex Lourenco. 2023. Nat.Dev. https://nat.dev/.\n[10] Dedre Gentner and Arthur B Markman. 1997. Structure mapping in analogy and\nsimilarity. American psychologist 52, 1 (1997), 45.\n[11] Saul Greenberg and Bill Buxton. 2008. Usability evaluation considered harmful\n(some of the time). In Proceedings of the SIGCHI Conference on Human Factors in\nComputing Systems (Florence, Italy) (CHI ‚Äô08) . Association for Computing Ma-\nchinery, New York, NY, USA, 111‚Äì120. https://doi.org/10.1145/1357054.1357074\n[12] Chip Huyen. 2022. Designing machine learning systems . \" O‚ÄôReilly Media, Inc. \".\n[13] Jest. 2023. Jest. https://jestjs.io/.\n[14] Ellen Jiang, Kristen Olson, Edwin Toh, Alejandra Molina, Aaron Donsbach,\nMichael Terry, and Carrie J Cai. 2022. PromptMaker: Prompt-based Prototyping\nwith Large Language Models. In Extended Abstracts of the 2022 CHI Conference\non Human Factors in Computing Systems (New Orleans, LA, USA) (CHI EA ‚Äô22) .\nAssociation for Computing Machinery, New York, NY, USA, Article 35, 8 pages.\nhttps://doi.org/10.1145/3491101.3503564\n[15] Peiling Jiang, Jude Rayan, Steven P. Dow, and Haijun Xia. 2023. Graphologue: Ex-\nploring Large Language Model Responses with Interactive Diagrams. In Proceed-\nings of the 36th Annual ACM Symposium on User Interface Software and Technology\n(San Francisco, CA, USA) (UIST ‚Äô23) . Association for Computing Machinery, New\nYork, NY, USA, Article 3, 20 pages. https://doi.org/10.1145/3586183.3606737\n[16] Laewoo (Leo) Kang, Steven J. Jackson, and Phoebe Sengers. 2018. Intermodulation:\nImprovisation and Collaborative Art Practice for HCI. In Proceedings of the 2018\nCHI Conference on Human Factors in Computing Systems (Montreal, QC, Canada)\n(CHI ‚Äô18) . Association for Computing Machinery, New York, NY, USA, 1‚Äì13.\nChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing CHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA\nhttps://doi.org/10.1145/3173574.3173734\n[17] Tae Soo Kim, Yoonjoo Lee, Minsuk Chang, and Juho Kim. 2023. Cells, Gen-\nerators, and Lenses: Design Framework for Object-Oriented Interaction with\nLarge Language Models. In Proceedings of the 36th Annual ACM Symposium on\nUser Interface Software and Technology (San Francisco, CA, USA) (UIST ‚Äô23) . As-\nsociation for Computing Machinery, New York, NY, USA, Article 4, 18 pages.\nhttps://doi.org/10.1145/3586183.3606833\n[18] Michelle S. Lam, Zixian Ma, Anne Li, Izequiel Freitas, Dakuo Wang, James A.\nLanday, and Michael S. Bernstein. 2023. Model Sketching: Centering Concepts\nin Early-Stage Machine Learning Model Design. In Proceedings of the 2023 CHI\nConference on Human Factors in Computing Systems (Hamburg, Germany) (CHI\n‚Äô23). Association for Computing Machinery, New York, NY, USA, Article 741,\n24 pages. https://doi.org/10.1145/3544548.3581290\n[19] David Ledo, Steven Houben, Jo Vermeulen, Nicolai Marquardt, Lora Oehlberg,\nand Saul Greenberg. 2018. Evaluation Strategies for HCI Toolkit Research. In\nProceedings of the 2018 CHI Conference on Human Factors in Computing Systems\n(Montreal, QC, Canada) (CHI ‚Äô18) . Association for Computing Machinery, New\nYork, NY, USA, 1‚Äì17. https://doi.org/10.1145/3173574.3173610\n[20] Q. Vera Liao and S. Shyam Sundar. 2022. Designing for Responsible Trust in\nAI Systems: A Communication Perspective. In Proceedings of the 2022 ACM\nConference on Fairness, Accountability, and Transparency (Seoul, Republic of\nKorea) (FAccT ‚Äô22). Association for Computing Machinery, New York, NY, USA,\n1257‚Äì1268. https://doi.org/10.1145/3531146.3533182\n[21] Mark Liffiton, Brad Sheese, Jaromir Savelka, and Paul Denny. 2023. CodeHelp: Us-\ning Large Language Models with Guardrails for Scalable Support in Programming\nClasses. arXiv preprint arXiv:2308.06921 (2023).\n[22] Logspace. 2023. LangFlow. https://www.langflow.org/.\n[23] Ference Marton. 2014. Necessary conditions of learning . Routledge.\n[24] Microsoft. 2023. Prompt Flow. https://microsoft.github.io/promptflow/.\n[25] Aditi Mishra, Utkarsh Soni, Anjana Arunkumar, Jinbin Huang, Bum Chul Kwon,\nand Chris Bryan. 2023. PromptAid: Prompt Exploration, Perturbation, Testing\nand Iteration using Visual Analytics for Large Language Models. arXiv preprint\narXiv:2304.01964 (2023).\n[26] Graham Neubig and Zhiwei He. 2023. Zeno GPT Machine Translation Report.\n[27] Wing Ng, Ava Anjom, and Joanna M Drinane. 2022. Beyond Amazon: Social\nJustice and Ethical Considerations for Research Compensation. Psychotherapy\nBulletin (2022), 17.\n[28] Dan R. Olsen. 2007. Evaluating user interface systems research. In Proceedings\nof the 20th Annual ACM Symposium on User Interface Software and Technology\n(Newport, Rhode Island, USA) (UIST ‚Äô07) . Association for Computing Machinery,\nNew York, NY, USA, 251‚Äì258. https://doi.org/10.1145/1294211.1294256\n[29] OpenAI. 2023. OpenAI Playground. https://platform.openai.com/playground.\n[30] OpenAI. 2023. openai/evals. https://github.com/openai/evals.\n[31] Jessica Pater, Amanda Coupe, Rachel Pfafman, Chanda Phelan, Tammy Toscos,\nand Maia Jacobs. 2021. Standardizing Reporting of Participant Compensation\nin HCI: A Systematic Literature Review and Recommendations for the Field. In\nProceedings of the 2021 CHI Conference on Human Factors in Computing Systems\n(Yokohama, Japan) (CHI ‚Äô21) . Association for Computing Machinery, New York,\nNY, USA, Article 141, 16 pages. https://doi.org/10.1145/3411764.3445734\n[32] F√°bio Perez and Ian Ribeiro. 2022. Ignore Previous Prompt: Attack Techniques\nFor Language Models. In NeurIPS ML Safety Workshop .\n[33] Charvi Rastogi, Marco Tulio Ribeiro, Nicholas King, and Saleema Amershi. 2023.\nSupporting Human-AI Collaboration in Auditing LLMs with LLMs.arXiv preprint\narXiv:2304.09991 (2023).\n[34] Fobo Shi, Peijun Qing, Dong Yang, Nan Wang, Youbo Lei, Haonan Lu, and Xi-\naodong Lin. 2023. Prompt Space Optimizing Few-shot Reasoning Success with\nLarge Language Models. arXiv preprint arXiv:2306.03799 (2023).\n[35] Hendrik Strobelt, Albert Webson, Victor Sanh, Benjamin Hoover, Johanna Beyer,\nHanspeter Pfister, and Alexander M Rush. 2022. Interactive and visual prompt\nengineering for ad-hoc task adaptation with large language models. IEEE trans-\nactions on visualization and computer graphics 29, 1 (2022), 1146‚Äì1156.\n[36] Sangho Suh, Bryan Min, Srishti Palani, and Haijun Xia. 2023. Sensecape: Enabling\nMultilevel Exploration and Sensemaking with Large Language Models. arXiv\npreprint arXiv:2305.11483 (2023).\n[37] Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Huang, and Xipeng Qiu. 2022.\nBlack-box tuning for language-model-as-a-service. In International Conference\non Machine Learning . PMLR, 20841‚Äì20855.\n[38] TruLens. 2023. trulens: Evaluate and Track LLM Applications. https://www.\ntrulens.org/.\n[39] Vellum. 2023. Vellum The dev platform for production LLM apps. https://www.\nvellum.ai/.\n[40] Vercel. 2023. Vercel: Deveop.Preview.Ship. https://vercel.com/.\n[41] Ian Webster. 2023. promptfoo: Test your prompts. https://www.promptfoo.dev/.\n[42] Weights and Biases. 2023. Weights and Biases Docs: Prompts for LLMs. https:\n//docs.wandb.ai/guides/prompts.\n[43] Tongshuang Wu, Ellen Jiang, Aaron Donsbach, Jeff Gray, Alejandra Molina,\nMichael Terry, and Carrie J Cai. 2022. PromptChainer: Chaining Large Language\nModel Prompts through Visual Programming. In Extended Abstracts of the 2022\nCHI Conference on Human Factors in Computing Systems (New Orleans, LA, USA)\n(CHI EA ‚Äô22) . Association for Computing Machinery, New York, NY, USA, Article\n359, 10 pages. https://doi.org/10.1145/3491101.3519729\n[44] Tongshuang Wu, Michael Terry, and Carrie Jun Cai. 2022. AI Chains: Transparent\nand Controllable Human-AI Interaction by Chaining Large Language Model\nPrompts. InProceedings of the 2022 CHI Conference on Human Factors in Computing\nSystems (New Orleans, LA, USA)(CHI ‚Äô22). Association for Computing Machinery,\nNew York, NY, USA, Article 385, 22 pages. https://doi.org/10.1145/3491102.\n3517582\n[45] J.D. Zamfirescu-Pereira, Richmond Y. Wong, Bjoern Hartmann, and Qian Yang.\n2023. Why Johnny Can‚Äôt Prompt: How Non-AI Experts Try (and Fail) to Design\nLLM Prompts. In Proceedings of the 2023 CHI Conference on Human Factors in\nComputing Systems (Hamburg, Germany) (CHI ‚Äô23) . Association for Computing\nMachinery, New York, NY, USA, Article 437, 21 pages. https://doi.org/10.1145/\n3544548.3581388\n[46] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis,\nHarris Chan, and Jimmy Ba. 2022. Large language models are human-level\nprompt engineers. arXiv preprint arXiv:2211.01910 (2022).\nA CASE STUDIES FOR MODES OF USAGE\nTo help readers understand how people used ChainForge and how\ntheir interactions varied, we walk through three participants‚Äô expe-\nriences. Each Case Study illustrates one mode from Section 6.\nA.1 Opportunistic exploration mode: Iterating\non hypotheses through rapid discovery of\nmodel behavior.\nA graduate student from Indonesia, P15 wanted to test how well\nAI models knew the Indonesian education participation rate and\ncould give advice on ‚Äúwhat the future of us as educators need to\ndo. ‚ÄùShe opens a browser tab with official data from Badan Pusat\nStatistik (BDS), Indonesia‚Äôs Central Agency for Statistics. She wants\nto know ‚Äúwhat is the difference, if I use a different language?‚Äù She\nadds a TextFields with two fields, one prompt in English, ‚ÄúTell me\nthe participation rate of Indonesian students going to university‚Äù ; the\nsecond its Indonesian translation. ‚ÄúLet‚Äôs just try two. I just want to\nsee where it goes. ‚Äù Collecting responses, she looks over side-by-side\nresponses of three models to her English prompt. All models provide\ndifferent years and percentages. Scrolling down and expanding the\nresponse group for her Indonesian prompt, she finds that Falcon.7B\nonly repeats her prompt and the PaLM2 model has triggered a\nsafety filter.13 The last model, GPT-3.5, gives a different statistic\nthan its English response.\nLooking over these responses in less than a minute, P9 has discov-\nered three aspects of AI model behavior: first, that models differ in\ntheir ‚Äúfacts‚Äù; second, that some models can refuse to answer when\nqueried in a non-English language; third, that the same models can\ndiffer in facts when queried in a different language. She compares\neach number to the BDS statistics, finding them inaccurate. ‚ÄúOh\nmy god, I‚Äôm curious. Why do they have like different answers across\n[models]?‚Äù She then adds models to the Prompt Node. ‚ÄúCan I try all\n[models]? I want to see if it‚Äôs in the table. ‚Äù\nShe queries the new models. A new hypothesis brews: ‚ÄúIn our\nprompt, [do] we need to say our source of data? Would that be like,\nmore accurate?‚Äù She wonders if different models are pulling data\nfrom different sources. Inspecting responses, she finds some models\nhave cited sources of data: Claude cites UNESCO and GPT-4 cites the\nWorld Bank, UNESCO, and the Indonesian Ministry of Education\n13This is a real problem with PaLM2 that we have communicated with the Google AI\nteam about; they identified the issue and are fixing it.\nCHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA Arawjo, Swoopes, Vaithilingam, Wattenberg & Glassman\nFigure 7: P18‚Äôs final flow, with one value toggled off to display their initial plot. The user asked GPT-4 to estimate how much\nthe conductivity to a base polymer, PEDOT:PSS, may increase given one of four additives. They use an LLM Scorer to extract the\nmentioned conductivity value; after spot-checking it using Inspect, they plot in a Vis Node, finding it ‚Äúroughly‚Äù correct.\nand Culture. For her Indonesian prompt, she discovers that the same\nmodels only cite BPS in their responses.‚ÄúBPS is only mentioned when\nI use Indonesian... For the English [prompt]... [it‚Äôs] more like, global...\nWow, it‚Äôs very interesting how, the different language you use, there‚Äôs\nalso a different source of data. ‚Äù\nShe adds a second prompt variable, {organization}, to her prompt\ntemplate. She attaches values World Bank, UNESCO, and Badan\nPusat Statistik to it.14 Re-sending queries and inspecting responses,\nshe expands the subgroups for BPS under both her Indonesian and\nEnglish response groups, such that the two subgroups are on the\nsame screen. When asking for BPS data in English, both GPT-3.5\nand Claude refuse to answer, whereas the same models provide BPS\nnumbers when asked in Indonesian. Moreover, Claude‚Äôs English\nresponse suggests the reader look at World Bank and UNESCO data\ninstead, citing those sources. ‚ÄúThat‚Äôs really interesting. Wow. ‚Äù\nAlthough the study ended here, this case illustrates hypothesis\niteration, limited prompts, and eagerness for cross-model compar-\nisons, key aspects of opportunistic exploration mode. With more\ntime, the user might have set up an evaluation to check how mod-\nels cite ‚Äúglobal‚Äù sources of information when queried in English,\ncompared to Indonesian.\nA.2 Limited evaluation mode: Setting up an\nevaluation pipeline to spot-check factual\naccuracy.\nHow do users transition from exploratory to limited evaluation\nmode? We illustrate prototyping an evaluation and ‚Äúscaling up‚Äù with\nP18, a material design student who used ChainForge to check an\nLLM‚Äôs understanding conductivity values of additives to polymers.\nThe example also depicts a usability issue as the user scaled up.\nLike Case #1, P18 begins in Opportunistic Exploration mode.\nThey prompt / inspect / refine ‚Äîsend off queries, inspect responses,\nrevise input data or prompts. They create a prompt template with\ntwo variables: Base and additives (Fig. 7). Initially they start with\nonly one Base, and four additives. Inspecting responses, P18 is\n14Note that this part is in English now for both queries.\nimpressed with GPT-4‚Äôs ability to suggest and explain specific addi-\ntives under P18‚Äôs broad categories (e.g., EMIMBF4 for Ionic Liquid ).\nThey refine their questioning: ‚ÄúI want to estimate the approximate\nconductivity value. ‚ÄùThey amend their prompt template, adding‚Äúand\nestimate the conductivity value‚Äù . Reviewing responses, they find the\nnumeric ranges roughly correct.\nThey then wish to inspect the numbers in a more systematic\nfashion than manual inspection, and move into Limited Evaluation\nmode. The researcher helps P18 with how to extract the numbers,\nusing an evaluator node, LLM Scorer, which they only saw once\nin the intro video. With this node, users can enter a natural lan-\nguage prompt to score responses. P18 iterates on the scorer prompt\nthrough a prompt/inspect/refine loop: first asking just for the num-\nber, then adding ‚Äúwithout units‚Äù after they find it sometimes outputs\nunits.15 ‚ÄúThis is good. So we add some Vis Node. ‚Äù They plot by ad-\nditive on the y-axis (Fig. 7). ‚ÄúVery good. [Researcher: Is this true?]\nRoughly, yes. Roughly. ‚Äù\nP18 then wants to ‚Äúscale up‚Äù by adding a second polymer to their\nBase variable. They search Google for the abbreviation of a conduct-\ning polymer, Polyaniline (PANI). They paste it as a second field and\nre-query the prompt and scorer nodes. Skimming scores in Table\nLayout in two seconds: ‚ÄúOh, wow... It‚Äôs really good. Because PEDOT\nis most [conductive]. ‚Äù Inspecting the Vis Node, they encounter a\nusability limitation: they want to group by Base , when additive is\nplotted in y-axis, but cannot. Plotting by Base on the y-axis, they\nsee via box-and-whiskers plot that PANI is collectively lower than\nPEDOT. They ask the researcher to export the evaluation scores.\nThis example illustrates limited evaluation mode, such as iter-\nating on an evaluation pipeline (refining a scoring prompt), and\nbeginning to ‚Äúscale up‚Äù by extending the input data after the pipeline\nis set up. The user also encountered friction with usability when\nscaling up, wanting more options for visualization as input data\ncomplexity increased.\n15Here is where a framework like LMQL [3] may come in handy to improve usability\nof this node.\nChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing CHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA\nA.3 Iterative Refinement mode: Tweaking an\nestablished prompt and model to attempt\nan optimization.\nP8 works with a German startup, and brought in a prompt engi-\nneering problem, importing a dataset and prompt template from\nLangChain [7] (‚Äúwe‚Äôre building a custom LLM app for an e-commerce\ncompany, a virtual shop assistant‚Äù ). This template had already under-\nwent substantial revisions; thus, the participant immediately moved\ninto iterative refinement mode, allowing us to observe interactions\nwe could only glimpse retroactively in our interviews.\nP8‚Äôs startup was using GPT-4 (because ‚ÄúGPT-3.5 in German is\nreally not that good‚Äù ), but was curious about whether other models\ncould perform better. He knew of Claude and PaLM2, but had\nbeen put off by needing to code up custom API calls. He also had\na hypothesis that using English in parts of his German prompt\nwould yield better results. Upon entering the unstructured task,\nhe imported a spreadsheet with a Tabular Data Node and pasted\nhis three-variable prompt template in a Prompt Node, connecting\nthem up. He then added a Python Evaluator Node to check whether\nthe LLM stuck to a length constraint he had put in his template.\nUsing Grouped List layout, he compared responses between Claude\nand GPT-4 across ten input values for variable product_information.\n‚ÄúGPT4 is going over [too long]... Claude seems to be fairly good at\nsticking‚Äî[opens another response group], actually, you know, we have\nan outlier here. ‚Äù\nLooking over responses manually, he implies that he had been\nmanually evaluating each response (prior to the study) across his\nten criteria. ‚ÄúI gave it... almost 10 instructions... Formal language,\nlength, and so on. And for each... I now need to review it. ‚Äù He notices\nthat one of Claude‚Äôs responses includes the word Begleiter, a word\nhe had explicitly instructed it to exclude:‚ÄúBecause that was a pattern\nI noticed with GPT-4 that it kept using this word... So I‚Äôm going to\ntry now... how is Claude behaving if I give this instruction in English,\nrather than [German]?‚Äù\nTo test this, he abstracts the ‚Äúavoid the following words‚Äù part of\nhis prompt template into a new variable, {avoid_words_ instruction}.\nHe pastes the previous command into a TextFields, and add a second\none‚Äîthe same command but in English. He adds a Simple Evalua-\ntor node, checking if the response contains Begleiter. In Grouped\nList layout, he groups responses by avoid_words_instruction and\nclick ‚ÄúOnly show scores‚Äù to only see true/false values (false in red).\nGlancing: ‚ÄúSo it‚Äôs not very statistically significant. But... GPT-4 never\nmade the mistake, and Claude made the mistake with both English\nand German... So it doesn‚Äôt matter which language... [Claude] will\nstill violate the instructions. ‚Äù He attaches another Simple Evaluator\nto test another term, remarking that in practice he would write a\nPython script to test all cases at once, but the study is running out\nof time. ‚ÄúSo Claude again violates it in both cases... [But for] English,\nit only violates it once‚Äîagain‚Äîand in German it violates it twice. So\nmaybe it‚Äôs slowly becoming statistically significant. ‚Äù As the study\nends, he declares that his investigation justified his original choice:\n‚ÄúI should probably keep using GPT-4. ‚Äù\nHere we see aspects of iterative refinement mode‚Äîthe participant\nhas already optimized their prompt (pipeline) and is trying to tweak\nthe prompt and model to see if they can improve the outputs even\nfurther, according to specific criteria . As we found in our structured\ntask, in making decisions, users weigh trade-offs between how\ndifferent models and/or prompts fulfill specific criteria, and also\nrank criteria importance. For P8, his ‚Äúavoid-words‚Äù criteria seemed\nmission-critical, whereas word count‚Äîwhich he perceived Claude\nbetter at sticking to‚Äîwas evidently less important.\nCHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA Arawjo, Swoopes, Vaithilingam, Wattenberg & Glassman\nB LIST OF NODES\nNode Name Usage Special Features\nInputs\nTextFields Node Specify input values to a template variables in prompt\nor chat nodes.\nSupports templating; can declare variables in brackets {}\nto chain inputs together.\nCSV Node Specify input data as comma-separated values. Good for\nspecifying many short values.\nBrackets {} in data are escaped by default.\nTabular Data Node Import or create a spreadsheet of data to use as input to\nprompt or chat nodes.\nOutput values ‚Äúcarry together‚Äù when filling in multiple\nvariables in a prompt template. Brackets {} in data are\nescaped by default.\nGenerators\nPrompt Node Prompt one or multiple LLMs. Declare template vari-\nables in {}s to attach input data.\nCan chain together. Can set number of generations per\nprompt to greater than one.\nChat Turn Node Continue a turn of conversation with one or multiple\nchat LLMs. Supports templating of follow-up message.\nAttach past prompt or chat output as context. Can also\nchange what LLM(s) to use to continue conversation.\nEvaluators\nJavaScript Evaluator Write a JavaScript function to ‚Äòscore‚Äô a single response.\nScores annotate responses.\nBoolean ‚Äòfalse‚Äô values display in red in response inspec-\ntor. console.log() prints to node.\nPython Evaluator Same as JavaScript Evaluator but for Python. Can import packages and use print().\nLLM Scorer Prompt an LLM to score responses. (GPT-4 at zero tem-\nperature is default.)\nUnlike prompt chaining, this attaches the scores as an-\nnotations on existing responses.\nSimple Evaluator Specify simple criteria to score responses as true if they\nmeet the criteria.\nCan test whether response contains, starts with , etc. a\ncertain value. Can also compare against prompt vari-\nables or metavariables.\nVisualizers\nVis Node Plot evaluation scores. Currently only supports boolean\nand numeric scores.\nPlots by LLM by default. Change y-axis to plot by dif-\nferent prompt variables.\nInspect Node Inspect LLM responses like the pop-up inspector, only\ninside a flow.\nOnly supports Grouped List layout.\nTable 3: The nodes in ChainForge, grouped by type. A final node, the ‚ÄúComment Node‚Äù, allows users to write comments.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.663378119468689
    },
    {
      "name": "Audit",
      "score": 0.6411036252975464
    },
    {
      "name": "Software engineering",
      "score": 0.5082682967185974
    },
    {
      "name": "Selection (genetic algorithm)",
      "score": 0.4383637011051178
    },
    {
      "name": "Data science",
      "score": 0.4145314693450928
    },
    {
      "name": "Human‚Äìcomputer interaction",
      "score": 0.3634600043296814
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2494279444217682
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ]
}