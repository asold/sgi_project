{
  "title": "Behind the Scene: Revealing the Secrets of Pre-trained Vision-and-Language Models",
  "url": "https://openalex.org/W3024464497",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5074572535",
      "name": "Jize Cao",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A5066666034",
      "name": "Zhe Gan",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A5101553135",
      "name": "Yu Cheng",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A5036418431",
      "name": "Licheng Yu",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A5037467245",
      "name": "Yen-Chun Chen",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A5100442542",
      "name": "Jingjing Liu",
      "affiliations": [
        "Meta (Israel)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2962964995",
    "https://openalex.org/W2970608575",
    "https://openalex.org/W2560730294",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2970565456",
    "https://openalex.org/W2745461083",
    "https://openalex.org/W2981851019",
    "https://openalex.org/W2790235966",
    "https://openalex.org/W2949178656",
    "https://openalex.org/W2938082352",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W1933349210",
    "https://openalex.org/W2997283613",
    "https://openalex.org/W2995460200",
    "https://openalex.org/W2949474740",
    "https://openalex.org/W2568262903",
    "https://openalex.org/W2997591391",
    "https://openalex.org/W2489434015",
    "https://openalex.org/W2971869958",
    "https://openalex.org/W2991382858",
    "https://openalex.org/W2975501350",
    "https://openalex.org/W2957941633",
    "https://openalex.org/W2953337107",
    "https://openalex.org/W2975357369",
    "https://openalex.org/W2950761309",
    "https://openalex.org/W2144960104",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W3035688398",
    "https://openalex.org/W2963115613",
    "https://openalex.org/W2970869018",
    "https://openalex.org/W2908854766",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W2912371042",
    "https://openalex.org/W2968124245",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W2963530300",
    "https://openalex.org/W2998356391",
    "https://openalex.org/W2998557616",
    "https://openalex.org/W2968880719",
    "https://openalex.org/W3034727271",
    "https://openalex.org/W3020257313",
    "https://openalex.org/W3111372685",
    "https://openalex.org/W2970120757",
    "https://openalex.org/W3102568136",
    "https://openalex.org/W3023633125"
  ],
  "abstract": "Recent Transformer-based large-scale pre-trained models have revolutionized vision-and-language (V+L) research. Models such as ViLBERT, LXMERT and UNITER have significantly lifted state of the art across a wide range of V+L benchmarks with joint image-text pre-training. However, little is known about the inner mechanisms that destine their impressive success. To reveal the secrets behind the scene of these powerful models, we present VALUE (Vision-And-Language Understanding Evaluation), a set of meticulously designed probing tasks (e.g., Visual Coreference Resolution, Visual Relation Detection, Linguistic Probing Tasks) generalizable to standard pre-trained V+L models, aiming to decipher the inner workings of multimodal pre-training (e.g., the implicit knowledge garnered in individual attention heads, the inherent cross-modal alignment learned through contextualized multimodal embeddings). Through extensive analysis of each archetypal model architecture via these probing tasks, our key observations are: (i) Pre-trained models exhibit a propensity for attending over text rather than images during inference. (ii) There exists a subset of attention heads that are tailored for capturing cross-modal interactions. (iii) Learned attention matrix in pre-trained models demonstrates patterns coherent with the latent alignment between image regions and textual words. (iv) Plotted attention patterns reveal visually-interpretable relations among image regions. (v) Pure linguistic knowledge is also effectively encoded in the attention heads. These are valuable insights serving to guide future work towards designing better model architecture and objectives for multimodal pre-training.",
  "full_text": "Behind the Scene: Revealing the Secrets of\nPre-trained Vision-and-Language Models\nJize Cao⋆1, Zhe Gan2, Yu Cheng2, Licheng Yu⋆3\nYen-Chun Chen2, and Jingjing Liu 2\n1 University of Washington\ncaojize@cs.washington.edu\n2 Microsoft Dynamics 365 AI Research\n{zhe.gan,yu.cheng,yen-chun.chen,jingjl}@microsoft.com\n3 Facebook AI\nlichengyu@fb.com\nAbstract. Recent Transformer-based large-scale pre-trained models have\nrevolutionized vision-and-language (V+L) research. Models such as ViL-\nBERT, LXMERT and UNITER have signiﬁcantly lifted state of the art\nacross a wide range of V+L benchmarks. However, little is known about\nthe inner mechanisms that destine their impressive success. To reveal\nthe secrets behind the scene, we present Value (Vision-And-Language\nUnderstanding Evaluation), a set of meticulously designed probing tasks\n(e.g., Visual Coreference Resolution, Visual Relation Detection) general-\nizable to standard pre-trained V+L models, to decipher the inner work-\nings of multimodal pre-training (e.g., implicit knowledge garnered in indi-\nvidual attention heads, inherent cross-modal alignment learned through\ncontextualized multimodal embeddings). Through extensive analysis of\neach archetypal model architecture via these probing tasks, our key ob-\nservations are: (i) Pre-trained models exhibit a propensity for attending\nover text rather than images during inference. (ii) There exists a subset of\nattention heads that are tailored for capturing cross-modal interactions.\n(iii) Learned attention matrix in pre-trained models demonstrates pat-\nterns coherent with the latent alignment between image regions and tex-\ntual words. ( iv) Plotted attention patterns reveal visually-interpretable\nrelations among image regions. (v) Pure linguistic knowledge is also eﬀec-\ntively encoded in the attention heads. These are valuable insights serving\nto guide future work towards designing better model architecture and\nobjectives for multimodal pre-training. 4\n1 Introduction\nRecently, Transformer-based [35] large-scale pre-trained models [32,22,6,19,21,27]\nhave prevailed in Vision-and-Language (V+L) research, an important area that\n⋆ This work was done when Jize and Licheng worked at Microsoft.\n4 Code is available at https://github.com/JizeCao/VALUE.\narXiv:2005.07310v2  [cs.CV]  18 Jul 2020\n2 J. Cao et al.\nProbing modality \nimportance\n[CLS]   A white dog jumps up to catch a soccer ball [SEP]\nProbing visual relations\nType: catch\nProbing multimodal \nfusion degree\nInput Image\nProbing visual coreferences\nType: dog\nProbing linguistic \nknowledge\nFig. 1: Illustration of the proposed Value framework for investigating pre-trained\nvision-and-language models. Value consists of a set of well-designed probing tasks\nthat unveil the inner mechanisms of V+L pre-trained models across: ( i) Multimodal\nFusion Degree; ( ii) Modality Importance; ( iii) Cross-modal Interaction via probing\nvisual coreferences; ( iv) Image-to-image Interaction via probing visual relations; and\n(v) Text-to-text Interaction via probing learned linguistic knowledge.\nsits at the nexus of computer vision and natural language processing (NLP). In-\nspired by BERT [9], a common practice for pre-training V+L models is to ﬁrst en-\ncode image regions and sentence words into a common embedding space, then use\nmultiple Transformer layers to learn image-text contextualized joint embeddings\nthrough well-designed pre-training tasks. There are two main schools of model\ndesign: (i) single-stream architecture, such as VLBERT [27] and UNITER [27],\nwhere a single Transformer is applied to both image and text modalities; and\n(ii) two-stream architecture, such as LXMERT [32] and ViLBERT [22], in which\ntwo Transformers are applied to images and text independently, and a third\nTransformer is stacked on top for later fusion. When ﬁnetuned on downstream\ntasks, these pre-trained models have achieved new state of the art on image-text\nretrieval [18], visual question answering [4,11], referring expression comprehen-\nsion [37], and visual reasoning [13,28,38]. This suggests that substantial amount\nof visual and linguistic knowledge has been encoded in the pre-trained models.\nThere has been several studies that investigate latent knowledge encoded in\npre-trained language models [33,7,25,16]. However, analyzing multimodal pre-\ntrained models is still an unexplored territory. It remains unclear how the inner\nmechanisms of cross-modal pre-trained models induce their empirical success on\ndownstream tasks. Motivated by this, we presentValue (Vision-And-Language\nUnderstanding Evaluation), a set of well-designed probing tasks that aims to\nreveal the secrets of these pre-trained V+L models. To investigate both single-\nand two-stream model architectures, we select one model from each category\n(LXMERT for two-stream and UNITER for single-stream, because of their su-\nperb performance across many V+L tasks). As illustrated in Figure 1, Value\nis designed to provide insights on: ( a) Multimodal Fusion Degree; ( b) Modal-\nRevealing the Secrets of Pre-trained Vision-and-Language Models 3\nity Importance; (c) Cross-modal Interaction (Image-to-text/Text-to-image); (d)\nImage-to-image Interaction; and (e) Text-to-text Interaction.\nFor (a) Multimodal Fusion Degree, clustering analysis between image and\ntext representations shows that in single-stream models like UNITER, as the\nnetwork layers go deeper, the fusion between two modalities becomes more in-\ntertwined. However, the opposite phenomenon is observed in two-stream models\nlike LXMERT. For ( b) Modality Importance, by analyzing the attention trace\nof the [CLS] token, which is commonly considered as containing the intended\nfused multimodal information and often used as the input signal for downstream\ntasks, we ﬁnd that the ﬁnal predictions tend to depend more on textual input\nrather than visual input.\nTo gain deeper insights into how pre-trained models drive success in down-\nstream tasks, we look into three types of interactions between modalities. For\n(c) Cross-modal Interaction, we propose a Visual Coreference Resolution task to\nprobe its encoded knowledge. For ( d) Image-to-image Interaction, we conduct\nanalysis via Visual Relation Detection between two image regions. For (e) Text-\nto-text Interaction, we evaluate the linguistic knowledge encoded in each layer\nof the tested model with SentEval tookit [8], and compare with the original\nBERT [9]. Experiments show that both single- and two-stream models, espe-\ncially the former, can well capture cross-modal alignment, visual relations, and\nlinguistic knowledge.\nTo the best of our knowledge, this is the ﬁrst known eﬀort on thorough\nanalysis of pre-trained V+L models, to gain insights from diﬀerent perspectives\nabout the latent knowledge encoded in self-attention weights, and to distill the\nsecret ingredients that drive the empirical success of prevailing V+L models.\n2 Related Work\nFor image-text representation learning, ViLBERT [22] and LXMERT [32] used\ntwo-stream architecture for pre-training, while B2T2 [2], VisualBERT [21], Unicoder-\nVL [19], VL-BERT [27] and UNITER [6] adopted single-stream architecture.\nVLP [39] proposed a uniﬁed pre-trained model for both image captioning and\nVQA. Multi-task learning [23] and adversarial training in VILLA [10] have been\nstudied to boost performance. On video+language side, VideoBERT [30] applied\nBERT to learn joint embeddings of video frame tokens and linguistic tokens from\nvideo-text pairs. CBT [29] introduced contrastive learning to handle real-valued\nvideo frame features, and HERO [20] proposed hierarchical Transformer archi-\ntectures to leverage both global and local temporal visual-textual alignments.\nHowever, except for some simple visualization of the learned attention maps\n[32], no existing work has systematically analyzed these pre-trained models.\nThere has been some recent studies on assessing the capability of BERT in\ncapturing structural properties of language [34,15,31]. Multi-head self-attention\nhas been analyzed for machine translation in [36,24], which observed that only\na small subset of heads is important, and the other heads can be pruned with-\nout aﬀecting model performance. [33] reported that BERT can rediscover the\n4 J. Cao et al.\nclassical NLP pipeline, where basic syntactic information appears in lower lay-\ners, while high-level semantic information appears in higher layers. Analysis on\nBERT self-attention [7,12,16] showed that BERT can learn syntactic relations,\nand a limited set of attention patterns are repeated across diﬀerent heads. [25,5]\nand [40] demonstrated that BERT has surprisingly strong ability to recall factual\nrelational knowledge and perform commonsense reasoning. A layer-wise analysis\nof Transformer representations in [1] provided insights to the reasoning process\nof how BERT answers questions. All these studies have focused on the analysis\nof BERT, while investigating pre-trained V+L models is still an uncharted ter-\nritory. Given their empirical success and unique multimodal nature, we believe\nit is instrumental to conduct an in-depth analysis to understand these models,\nto provide useful insights and guidelines for future studies. New probing tasks\nsuch as visual coreference resolution and visual relation detection are proposed\nfor this purpose, which can lend insights to other evaluation tasks as well.\n3 V ALUE: Probing Pre-trained V+L Models\nKey curiosities this study aims to unveil include:\n(i) What is the correlation between multimodal fusion and the number of\nlayers in pre-trained models? (Sec. 3.1)\n(ii) Which modality plays a more important role that drives the pre-trained\nmodel to make ﬁnal predictions? (Sec. 3.2)\n(iii) What knowledge is encoded in pre-trained models that supports cross-\nmodal interaction and alignment? (Sec. 3.3)\n(iv) What knowledge has been learned for image-to-image (intra-modal) in-\nteraction (i.e., visual relations)? (Sec. 3.4)\n(v) Compared with BERT, do pre-trained V+L models eﬀectively encode\nlinguistic knowledge for text-to-text (intra-modal) interaction? (Sec. 3.5)\nTo answer these questions, we select one model from each archetypal model\narchitecture for dissection: UNITER-base [6] (12 layers, 768 hidden units per\nlayer, 12 attention heads) for single-stream model, and LXMERT [32] for two-\nstream model. 5 Single-stream model (UNITER) shares the same structure as\nBERT [9], except that the input now becomes a mixed sequence of two modali-\nties. Two-stream model (LXMERT) ﬁrst performs self-attention through several\nlayers on each modality independently, then fuses the inputs through a stack of\ncross-self-attention layers (ﬁrst cross-attention, then self-attention). Therefore,\nthe attention pattern in two-stream models is ﬁxed, as one modality is only al-\nlowed to attend over either itself or the other modality at any time (there is no\nsuch constraint in single-stream models). A more detailed model description is\nprovided in the Appendix.\nTwo datasets are selected for our probing experiments:\n(i) Visual Genome (VG)[17]: image-text dataset with annotated dense\ncaptions and scene graphs.\n5 Our probing analysis can be readily extended to other pre-trained models as well.\nRevealing the Secrets of Pre-trained Vision-and-Language Models 5\n(ii) Flickr30k Entities[26]: image-text dataset with annotated visual co-\nreference links between image regions and noun phrases in the captions.\nEach data sample consists of three components: (i) an input image; (ii) a set\nof detected image regions;6 and (iii) a caption. In Flickr30k Entities, the caption\nis relatively long, describing the whole image; while in VG, a few short captions\nare provided (called dense captions), each describing an image region.\nThe number of annotated image regions in an image is relatively small (5-6 for\nFlick30k Entities, and 2-4 for VG dense annotated graph); while in pre-trained\nV+L models, the number of image regions fed to the model is typically 36 [3].\nTherefore, we extract an additional set of image regions from a Faster R-CNN\npre-trained on the VG dataset [3], and combine them with the original image\nregions provided in the dataset. The initial image representation is obtained\nfrom the Faster R-CNN as well. Finally, we feed both image regions and textual\ntokens into the pre-trained model for probing analysis. The following sub-sections\ndescribe analysis results and key observations from each proposed probing task.\n3.1 Deep to Profound: Deeper Layers Lead to More Intertwined\nMultimodal Fusion\nBy nature, visual features in images and linguistic clues in text have distinctive\ncharacteristics. However, it is unknown whether the semantic gap between their\ncorresponding representations narrows ( i.e., the contextualized representations\nof the two modalities become less diﬀerentiable) through the cross-modal fusion\nbetween intermediate layers.\n3.1.1 Probing Task To answer this question, we design a probing task to\ntest the multimodal fusion degree of a model. First, we extract all the embed-\nding features of both image regions and textual tokens from aforementioned two\ndatasets (VG and Flickr30k Entities). For single-stream model (UNITER-base),\nwe extract the output representation from each layer, as multimodal fusion is\nperformed through all the layers. For two-stream model (LXMERT), we only\nconsider the output embeddings from the last 5 layers ( i.e., the layers constitut-\ning the cross-modality encoder), because the two modalities do not have any in-\nteraction prior to that. To gain quantitative measurement, for each data sample,\nwe apply the k-means algorithm (with k = 2) on the representations from each\nlayer to partition them into two clusters, and measure the diﬀerence between the\nformed clusters and ground-truth visual/textual clusters via Normalized Mutual\nInformation (NMI), an unsupervised metric for evaluating diﬀerences between\nclusters. A larger NMI value implies that the distinction between two clusters is\nmore signiﬁcant, indicating a lower fusion degree [14]. For example, when NMI\nis equal to 1.0, the two clusters represent the original visual tokens and textual\ntokens, respectively. We use the mean value of NMI for all the data samples to\nmeasure the level of multimodal fusion.\n6 An image region is also called a visual token in this paper; these two terms will be\nused interchangeable throughout the paper.\n6 J. Cao et al.\nLayer 0 1 2 3 4 5 6 7 8 9 10 11\nsingle-stream\nFlickr30k 0.36 0.38 0.39 0.41 0.38 0.38 0.38 0.38 0.32 0.20 0.26 0.20\nVisual Genome 0.25 0.25 0.24 0.24 0.22 0.22 0.21 0.21 0.20 0.17 0.16 0.16\ntwo-stream (cross)\nFlickr30k 0.42 0.48 0.67 0.75 0.43 − − − − − − −\nVisual Genome 0.43 0.56 0.68 0.78 0.57 − − − − − − −\nTable 1: NMI scores on multimodal fusion probing. A smaller NMI value indicates a\nhigher fusion degree. Note that the two-stream model (LXMERT) only has 5 layers in\nits cross-modality encoder. A larger layer number corresponds to an upper layer.\n3.1.2 Results Table 1 summarizes the probing results on multimodal fusion\ndegree. For single-stream model (UNITER-base), the NMI scores gradually de-\ncrease, indicating that the representations from the two modalities fuse together\ndeeper and deeper from lower to higher layers. This observation matches our intu-\nition that the embedding of a modality from a higher layer better attends over the\nother modality than a lower layer. However, in two-stream model (LXMERT),\nas the layers go deeper (except for the last layer), the representations deviate\nfrom each other. One possible explanation is that single-stream model applies\nthe same set of parameters to both image and text modalities, while two-stream\nmodel uses two separate sets of parameters (as part of its network design) to\nmodel the attention on the image and text modality independently. The latter\nmakes it relatively easier to distinguish the two representations, leading to a\nhigher NMI score. An t-SNE visualization is provided in the Appendix.\n3.2 Who Pulls More Strings: Textual Modality Is More Dominant\nthan Image\nFollowing BERT [9], pre-trained V+L models add a special [CLS] token at the\nbeginning of a sequence, and a special [SEP] token at the end. In practice, the\n[CLS] token representation from the last layer is used as the fused representa-\ntion for both modalities in downstream tasks. Since the [CLS] token absorbs\ninformation from both modalities through self-attention, the degree of attention\nof the [CLS] token over each modality could be regarded as evidence to an-\nswer the following question: which modality is more dominant during inference?\nNote that in the two-stream model design, the [CLS] token is not allowed to at-\ntend to both image and textual modality simultaneously. Therefore, the probing\nexperiments for modality importance is focused on single-stream model.\n3.2.1 Probing Task Formally, to quantitatively analyze the [CLS] attention\ntrace, the Modality Importance(MI) of a head j is deﬁned as the sum of the at-\ntention values that the [CLS] token spent on the modality M (visual or textual)\nat j for the whole sequence S = ([CLS], t1,...,t m, [SEP], v1,...,v n), where\nRevealing the Secrets of Pre-trained Vision-and-Language Models 7\n(a) Textual modality importance\n (b) Visual modality importance\nFig. 2: Visualization of the modality importance score for all the 144 attention heads.\nt1,...,t m and v1,...,v n are textual and visual tokens, respectively. That is,\nIM,j = ∑\ni∈S1 (i∈M) ·αij, (1)\nwhere αij represents the attention weight that the [CLS] token attends on token\ni at head j, and 1 (·) is the indicator function.\nSince the attention heads in the same layer perform attention on the same\nrepresentation, it is natural to expand the above head-level analysis to a layer-\nlevel analysis, by considering the mean of MI scores of all the 12 heads as the\nMI measurement of that layer. In addition, we calculate an overall MI score via\nsumming up the MI scores of all the 144 heads. The mean MI value of all the data\nsamples is reported as the score for probing modality importance. Note that the\nMI score for visual/textual modality is calculated based on the attention weights\non the visual/textual tokens; while the attention weights on the special [CLS]\nand [SEP] tokens are not considered. Therefore, the two mean MI scores from\nthe two modalities do not sum to one.\n3.2.2 Results Experiments are conducted on the Flickr30k Entity dataset.\nFigure 2 provides the MI scores for all the 144 attention heads of UNITER-base.\nThe heatmap on the textual MI is denser than that of the visual MI, showing that\nmore attention heads are learning useful knowledge from the textual modality\nthan the image modality. Figure 3a further shows the layer-level MI scores for\neach modality. The average MI score on the text modality is higher than that on\nthe image modality, especially for intermediate layers, suggesting that the pre-\ntrained model relies more on the textual modality for making decisions during\ninference time.\n3.3 Winner Takes All: A Subset of Heads is Specialized for\nCross-modal Interaction\nThe key diﬀerence between single-modal Transformer (such as BERT) and two-\nmodal Transformer (such as UNITER and LXMERT) is that two-modal Trans-\n8 J. Cao et al.\n(a) Layer-level modality importance\n (b) Image-to-text attention\nFig. 3: Layer-level modality importance scores and visualization of the image-to-text\nattention for all the 144 attention heads.\nformer requires extra cross-modal interaction. To gain an in-depth understanding\nof cross-modal attention heads, which is instructive to prompt better model de-\nsign and enhance model interpretability, we look into two special types of head:\n(i) image-to-text head; and ( ii) visual-coreference head.\n3.3.1 Probing Image-to-text Head We ﬁrst analyze whether there exists\nany head specialized in learning cross-modal interaction. Formally, for a given\nimage-text pair, visual and textual tokens are denoted as V and T, respectively.\nWe deﬁne a head as image-to-text head if:\n∃v∈V ∑\nt∈T αv→t >0.5 , (2)\nwhere αv→t denotes the attention weight from a visual token to a textual token.\nThis deﬁnes whether a visual token pays more attention to the text modality.\nSpeciﬁcally, if there exists one visual token that has higher attention weight on\ntext than other tokens, we regard the corresponding head as performing cross-\nmodal attention from image to text.\nBased on the above deﬁnition, we count the number of occurrences of head as\nan image-to-text headfor all data samples in the Flickr30k Entities dataset, and\nreport the empirical probability of each head being an image-to-text head. Note\nthat in the two-stream model, the image-to-text head is by design, therefore, we\nonly conduct this analysis on single-stream model.\nFigure 3b shows there is a speciﬁc set of heads in UNITER-base that perform\ncross-modal interaction. The maximum probability of a head performing image-\nto-text attention is 0.92, the minimum probability is 0, and only 15% heads have\nmore than 0.5 probability to pay the majority attention weight on the image-\nto-text part. Interestingly, by training single-stream model, the attention heads\nare automatically learned to exhibit a “two-stream” pattern, where some heads\ncontrol the message sharing from the visual modality to the textual modality.\nRevealing the Secrets of Pre-trained Vision-and-Language Models 9\nResults on single-stream model\nCoref Type V →T T →V V →T (Rand.) T →V (Rand.)\npeople 0.165 (9-12) 0.060 (3-1) 0.106 (9-12) 0.035 (11-8)\nbodyparts 0.108 (9-3) 0.051 (11-8) 0.084 (9-12) 0.038 (11-8)\nscene 0.151 (9-12) 0.048 (3-1) 0.111 (9-12) 0.035 (11-8)\nclothing 0.157 (9-3) 0.040 (3-1) 0.092 (9-12) 0.040 (10-2)\nanimals 0.285 (9-12) 0.137 (3-1) 0.139 (9-12) 0.047 (9-12)\ninstruments 0.244 (11-8) 0.042 (9-12) 0.091 (9-12) 0.031 (9-12)\nvehicles 0.194 (9-12) 0.065 (3-1) 0.112 (9-12) 0.039 (9-12)\nResults on two-stream model\nCoref Type V →T T →V V →T (Rand.) T →V (Rand.)\npeople 0.145 (2-8) 0.063 (2-6) 0.056 (1-7) 0.057 (3-9)\nbodyparts 0.079 (5-7) 0.059 (1-7) 0.041 (1-2) 0.060 (3-9)\nscene 0.076 (5-7) 0.059 (1-7) 0.038 (1-7) 0.060 (3-9)\nclothing 0.062 (5-7) 0.062 (2-6) 0.040 (1-7) 0.061 (3-9)\nanimals 0.235 (3-4) 0.106 (4-7) 0.075 (1-7) 0.072 (3-9)\ninstruments 0.144 (5-7) 0.040 (1-7) 0.055 (1-11) 0.058 (3-9)\nvehicles 0.097 (1-1) 0.046 (2-6) 0.097 (1-7) 0.062 (3-9)\nTable 2: Results on visual coreference resolution. Each number represents the maxi-\nmum attention weights between two linked tokens, averaged across all data samples.\nV →T records the attention trace where a visual token attends to the linked noun\nphrase; T → V records the attention trace on the other direction. The head that\nachieves the maximum attention weight is shown in the bracket.\n3.3.2 Visual Coreference Resolution One straightforward way to investi-\ngate the visual-linguistic knowledge encoded in the model is to evaluate whether\nthe model is able to match an image region to its corresponding textual phrase\nin the sentence. Thus, we design a Visual Coreference Resolutiontask (similar\nto coreference resolution) to predict whether there is a link between an image\nregion and a noun phrase in the sentence that describes the image. In addition,\neach coreference link in the dataset is annotated with a label (8 in total).\nThrough this task, we can ﬁnd out whether the coreference knowledge can be\ncaptured by the attention trace. To achieve this goal, for each data sample in the\nFlickr30k Entity dataset, we extract the encoder’s attention weights for all the\n144 heads. Note that noun phrases typically consist of two or more tokens in the\nsequence. Thus, we extract the maximum attention weight between the image\nregion and each word of the noun phrase for each head. The maximum weight is\nthen used to evaluate which head identiﬁes visual coreference ( i.e., performing\nmultimodal alignment).\nResults are summarized in Table 2. The columns labeled with “(Rand.)”\nare considered as ablation groups to identify whether the high attention weight\nof a certain coreference relationship is triggered by the relation between the\nimage-text pair rather than the eﬀect of one speciﬁc image/text token. For a\nlink V →T, we measure the maximum attention weight of the visual token V\n10 J. Cao et al.\nto a random noun phrase, to obtain the results for V →T (Rand.). Similarly,\nwe use the maximum attention weight of a noun phrase T to a random visual\ntoken to obtain the results for T →V (Rand.).\nResults show that the relation between a noun phase and its linked visual\ntoken is encoded in the attention pattern, especially for V →T. The heads\n(9-3)7, (9-12) and (3-1) have captured richer coreference knowledge than other\nheads, indicating that there exists a subset of heads in the pre-trained model that\nis specialized in coreference linking between the two modalities. Furthermore,\nfor single-stream model, we observe that some heads encode the coreference\ninformation in both directions, serving as additional evidence that these heads\nperform cross-modal alignment speciﬁcally. On the other hand, the amount of\nlearned coreference knowledge is limited in the text modality attention trace\n(T → V), which provides indirect evidence that the text modality does not\nincorporate much visual information, even with the forced cross-attention design\nin the two-stream model.\n3.3.3 Probing Combinations of Heads Previous analysis mainly investi-\ngates whether cross-modal knowledge can be captured through individual at-\ntention head. It is also possible that such knowledge can be induced via the\ncooperation of multiple heads. To quantitatively analyze this, we further exam-\nine visual coreference through probing over combinations of heads.\n1) Attention Prober To reveal the learned knowledge across diﬀerent\nattention heads, we use a linear classiﬁer based on the combination of attention\nweights, following [7]. Speciﬁcally,\np(c|i,j) ∝∑N\nk=1(wkαk\nij + µkαk\nji) , (3)\nwhere given tokens iand j in the sequence, p(c|i,j) is the probability of the link\nlabel between these two tokens being c. αk\nij is the attention weight for token i\nattending to token j at head k, wk and µk are two learnable scalars, and N is\nthe number of attention heads.\n2) Layer-wise Embedding Prober Similarly, to further examine the\nknowledge encoded in the model, we can naturally extend the above attention-\nbased prober into an embedding-based prober:\npk(c|i,j) ∝wk⊤\ncls(Wk\nwek\ni ⊙Wk\nµek\nj) , (4)\nwhere pk(c|i,j) is the same as deﬁned in (3), ek\ni and ek\nj are the embeddings of\ntoken i and j at the k-th layer, and Wk\nw, Wk\nµ ∈R768×768,wk\ncls ∈R768×1. By\ntraining a linear classiﬁer on top of extracted embeddings, this prober provides\na way to probe the encoded knowledge between each pair of tokens on each layer.\nFor visual coreference resolution, we probe the model on two sub-tasks: ( i)\nVisual Coref Detection (VCD): determine whether a noun phrase is corefer-\nenced to a speciﬁc visual token (binary classiﬁcation); and ( ii) Visual Coref\n7 Head (i-j) means the j-th head at the i-th layer.\nRevealing the Secrets of Pre-trained Vision-and-Language Models 11\nClassiﬁer Input VCD (SS) VCD (TS) VCC (SS) VCC (TS)\n144 Attention Heads 52.04 53.68 75.10 54.47\nRandom Guess 50.00 50.00 12.50 12.50\nLayer 1 56.86 53.68 93.51 93.35\nLayer 5 59.12 52.59 94.05 92.62\nLayer 12 58.40 / 93.44 /\nTable 3: Results of attention and layer-wise embedding probers on Visual Coreference\nDetection and Classiﬁcation (VCD and VCC). SS: single-stream; TS: two-stream.\nClassification (VCC): classify the label of the coreference relation between\na noun phrase and a visual token (multi-class classiﬁcation). With these tasks,\nwe can examine whether certain type of knowledge is encoded, as well as the\ngranularity of the knowledge encoded in the probed feature space 8.\nResults are shown in Table 3. Neither model performs well on the VCD task,\nsuggesting that there is no attention pattern or embedding feature that can han-\ndle all the coreference relations. However, the results on VCC are encouraging.\nThis aligns with our observation from Table 2 that some attention heads of the\ntwo models are signiﬁcantly eﬀective in certain coreference relations. 9\n3.4 Secret Liaison Revealed: Cross-Modality Fusion Registers\nVisual Relations\nTo evaluate the encoded knowledge learned from the image modality, we adopt\nthe visual relation detection task, which requires a model to identify and classify\nthe relation between two image regions. This task can be viewed as examining\nwhether the model captures visual relations between image regions.\nThe VG dataset is used for this task, which contains 1,531,448 ﬁrst-order\nobject-object relations. First-order object-object relation can be determined sim-\nply by the visual representations of two objects, independent to other objects in\nthe image or text annotation. To reduce the imbalance in the number of rela-\ntions per relation type, we randomly select at most 15,000 subject-object relation\npairs (s,o) per relation type. Furthermore, to de-duplicate cases where the same\ntype of relation comes from the same text annotation, we select at most 5 same\nrelation types ( s,o) from the same annotation. This probing task is performed\non 32 most frequent relation pairs in the dataset.\n8 Since noun phrase may contain several tokens, we use the maximum attention weight\namong the tokens in that phrase over an image region as the attention weight between\nthe noun phase and the image region. The embedding of the noun phrase is the mean\nof all the representations of its textual tokens.\n9 Though both models’ embedding probers achieve higher than 94% accuracy on the\nVCC task, it is worth noting that text embedding input can potentially leak the\nlink information. For instance, the phrase “A guard with a white hat” may already\nprovide coreference information betweenperson and the corresponding image region.\n12 J. Cao et al.\nRelation Type SS SS (Rand.) TS TS (Rand.)\non 0.154 (10-1) 0.055 (1-8) 0.157 (3-12) 0.063 (5-9)\nstanding in 0.107 (2-8) 0.051 (1-8) 0.173 (7-4) 0.064 (3-1)\nwearing 0.311 (10-1) 0.049 (1-8) 0.230 (7-4) 0.055 (3-1)\nplaying 0.135 (4-1) 0.050 (1-8) 0.103 (7-10) 0.062 (3-1)\neating 0.138 (10-1) 0.056 (1-8) 0.142 (7-4) 0.067 (3-1)\nholds 0.200 (10-1) 0.055 (1-8) 0.173 (7-4) 0.066 (3-1)\ncovering 0.151 (7-2) 0.053 (1-8) 0.173 (3-1) 0.061 (3-6)\nTable 4: Results on Visual Relation Identiﬁcation/Classiﬁcation using maximum at-\ntention weight between two visual tokens. SS: single stream; TS: two stream.\n1) Probing Individual Attention HeadWe apply the same analysis here\nsimilar to the visual coreference resolution task. The only diﬀerence is that we do\nnot consider the directions of attention (i.e., s→o, or o→s), as both directions\ncorrespond to the same visual modality. The average of maximum attention value\nin both s→o and o→s directions is reported for visual relation analysis.\nResults are summarized in Table 4. We report the maximum attention weights\nof 7 of 30 relations. The columns with “(Rand.)” are ablation groups. As shown\nin the comparison, the learned attention heads encode rich knowledge about the\nrelations between visual tokens with much higher attention weights. Moreover,\nsimilar to the observation in visual coreference resolution task, speciﬁc heads\n(10-1 head in the single-stream model, 7-4 head in the two-stream model) have\ncaptured richer visual relations than others.\n2) Probing Combinations of Attention Heads The above analysis\nonly reveals the behavior of individual attention heads. Similar to Sec. 3.3, we\nfurther examine visual relations by training a linear classiﬁer on top of a com-\nbination of attention heads over two sub-tasks: ( i) Relation Identification:\ndetermine whether two image regions have a relationship; and ( ii) Relation\nClassification: classify the relation label between two image regions.\nResults on probing a combination of heads are summarized in Figure 4. Two\nbaselines are considered in this task. ( i) Original visual embeddings from Faster\nR-CNN [3]. This setup evaluates how much correlation between two related visual\nrepresentations has elevated or diluted by the V+L models. ( ii) Mismatched\nimage-text representation. In this baseline, we construct a dataset where an\nimage is associated with an unrelated dense annotation instead of a related one.\nThis baseline evaluates how the correlation between the image regions changes\nbased on the text modality. Note that for the two-stream model, there are 10\nlayers involved in visual relation reasoning. The ﬁrst ﬁve layers perform self-\nattention across the visual modality only, and in the last ﬁve layers the visual\nrepresentation interacts with the text modality.\nAs shown, both models perform much better than the baseline with related\nannotations (the dashed balck line in Figure 4), indicating there is a substantial\namount of visual-relation knowledge encoded in these representations. On the\nRevealing the Secrets of Pre-trained Vision-and-Language Models 13\nClassiﬁer Input VRI (SS) VRC (SS) VRI (TS) VRC (TS)\n144 Attention Heads 69.81 24.67 67.53 18.89\n144 Attention Heads (mismatch) 64.66 23.64 64.72 18.42\nRandom Guess 50.00 3.33 50.00 3.33\n(a) Accuracies (%) of the attention probers on VRI and VRC\n(b) Layer-wise emb. prober on VRC\n (c) Layer-wise emb. prober on VRI\nFig. 4: Results of attention and layer-wise embedding probers on Visual Relation Iden-\ntiﬁcation and Classiﬁcation (VRI and VRC).\nother hand, the speciﬁc visual relation knowledge degrades a lot with mismatched\nimage regions and dense annotations (the solid red and blue lines vs. the dashed\nlines in Figure 4). For single-stream model, the visual-relation knowledge decayed\nto that of the original visual embedding. This makes sense because no visual\nrelation information can be obtained from the mismatched caption.\nThe visual relation knowledge of the two-stream model is greatly inﬂuenced\nby the unpaired caption, leading to a huge performance drop after Layer 5 in\nthe VRC task. This result may contribute to diﬀerent inductive bias between\nthe two models. For two-stream model, since the visual modality has to attend\nto the text modality during cross attention, the visual representation will be\ngreatly inﬂuenced by the text modality even when the two modalities are totally\nunrelated. On the other hand, because the visual representation in the single-\nstream model is capable of selectively choosing whether to attend over the text\nmodality, the eﬀect of unrelated caption on the visual representation is negligible.\n3.5 No Lost in Translation: Pre-trained V+L Models Encode Rich\nLinguistic Knowledge\nBesides looking into the knowledge learned from the visual modality, we are also\ninterested in the encoded knowledge learned from the text modality. To achieve\nthis goal, we probe the pre-trained models over nine tasks deﬁned in the SentEval\ntoolkit [8]. Descriptions about the tasks are provided in the Appendix.\nFirst, we extract contextualized word representations from the pre-trained\nmodels. For single-stream model, the input is the sequence of tokens with text\n14 J. Cao et al.\nModel SentLen TreeDepth TopConst BShift Tense SubjNum ObjNum SOMO CoordInv\n(Surface) (Syntactic) (Syntactic) (Syntactic) (Semantic) (Semantic) (Semantic) (Semantic) (Semantic)\nSS 88.8 (L2)36.4 (L5)79.0 (L7)81.6 (L10)86.6 (L11)83.4 (L7)78.8 (L9)57.1 (L12)62.1 (L11)\nTS 83.8 (L7)34.0 (L8)67.2 (L8)64.9 (L8)75.6 (L6)78.8 (L9)76.8 (L8)51.4 (L8)58.7 (L8)\nBERT96.2 (L3)41.3 (L6)84.1 (L7)87.0 (L9)90.0 (L9)88.1 (L6)82.2 (L7)65.2 (L12)78.7 (L9)\nTable 5: Results on the linguistic probing tasks. L n: Layer n.\nonly. For two-stream model, since the last 5 cross-attention layers require visual\ninput, which is not covered by these linguistic benchmarks, we only evaluate the\nﬁrst 9 layers that are performing self-attention over pure text input.\nWe use the Google-pretrained BERT-base model as the baseline. For each\ntask, we obtain the results from all the layers and report the best number. The\nresults in Table 5 show that pre-trained V+L model generally performs worse\nthan the original BERT-base model on these linguistic benchmarks, which is as\nexpected. A full table with results from all the layers is provided in the Appendix.\nAs shown in the table, the single-stream model performs better than the two-\nstream model across all the tasks. One possible reason is that LXMERT does not\ninitialize the parameters of the language Transformer encoder from BERT-base,\nwhereas UNITER does. Thus, using the parameters of BERT as initialization is\npotentially useful for the model to acquire rich linguistic knowledge, and helpful\nfor tasks involving complex text-based reasoning.\nTo measure the gains due to learning, we have conducted all the above ex-\nperiments (Secs. 3.1-3.5) on untrained baselines with random weights. These\nadditional results are provided in the Appendix.\n4 Conclusion and Key Takeaways\nIntrigued by the ﬁve questions presented at the beginning of Section 3, we have\nprovided a thorough analysis of UNITER-base and LXMERT models as a deep\ndive into Vision+Language pre-training. To summarize our key ﬁndings:\n(i) In single-stream model, deeper layers lead to more intertwined multimodal\nfusion; while the opposite trend is observed in two-stream model.\n(ii) Textual modality plays a more important role than image in making ﬁnal\ndecisions, consistent across both single- and two-stream models.\n(iii) In single-stream model, a subset of heads organically evolves to pivot\non cross-modal interaction and alignment, which on the other hand is enforced\nby model design in two-stream model.\n(iv) Visual relations are inherently registered in both single- and two-stream\npre-trained models.\n(v) Rich linguistic knowledge is naturally encoded, even though the models\nare speciﬁcally designed for multimodal pre-training.\nWe provide additional guidelines in the Appendix. For future work, we plan\nto perform model compression via pruning attention heads based on the analysis\nand observations in this work.\nRevealing the Secrets of Pre-trained Vision-and-Language Models 15\nReferences\n1. van Aken, B., Winter, B., L¨ oser, A., Gers, F.A.: How does bert answer questions?\na layer-wise analysis of transformer representations. In: CIKM (2019) 4\n2. Alberti, C., Ling, J., Collins, M., Reitter, D.: Fusion of detected objects in text for\nvisual question answering. In: EMNLP (2019) 3\n3. Anderson, P., He, X., Buehler, C., Teney, D., Johnson, M., Gould, S., Zhang,\nL.: Bottom-up and top-down attention for image captioning and visual question\nanswering. In: CVPR (2018) 5, 12\n4. Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Lawrence Zitnick, C., Parikh,\nD.: Vqa: Visual question answering. In: ICCV (2015) 2\n5. Bouraoui, Z., Camacho-Collados, J., Schockaert, S.: Inducing relational knowledge\nfrom bert. In: AAAI (2020) 4\n6. Chen, Y.C., Li, L., Yu, L., Kholy, A.E., Ahmed, F., Gan, Z., Cheng, Y.,\nLiu, J.: Uniter: Learning universal image-text representations. arXiv preprint\narXiv:1909.11740 (2019) 1, 3, 4, 17\n7. Clark, K., Khandelwal, U., Levy, O., Manning, C.D.: What does bert look at? an\nanalysis of bert’s attention. arXiv preprint arXiv:1906.04341 (2019) 2, 4, 10\n8. Conneau, A., Kiela, D.: Senteval: An evaluation toolkit for universal sentence rep-\nresentations. arXiv preprint arXiv:1803.05449 (2018) 3, 13, 19\n9. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidi-\nrectional transformers for language understanding. In: NAACL (2019) 2, 3, 4, 6,\n17\n10. Gan, Z., Chen, Y.C., Li, L., Zhu, C., Cheng, Y., Liu, J.: Large-scale adver-\nsarial training for vision-and-language representation learning. arXiv preprint\narXiv:2006.06195 (2020) 3\n11. Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., Parikh, D.: Making the v in vqa\nmatter: Elevating the role of image understanding in visual question answering. In:\nCVPR (2017) 2\n12. Htut, P.M., Phang, J., Bordia, S., Bowman, S.R.: Do attention heads in bert track\nsyntactic dependencies? arXiv preprint arXiv:1911.12246 (2019) 4\n13. Hudson, D.A., Manning, C.D.: Gqa: a new dataset for compositional question\nanswering over real-world images. In: CVPR (2019) 2\n14. Jawahar, G., Sagot, B., Seddah, D.: What does bert learn about the structure of\nlanguage? In: ACL (2019) 5\n15. Jiang, Z., Xu, F.F., Araki, J., Neubig, G.: How can we know what language models\nknow? arXiv preprint arXiv:1911.12543 (2019) 3\n16. Kovaleva, O., Romanov, A., Rogers, A., Rumshisky, A.: Revealing the dark secrets\nof bert. In: EMNLP (2019) 2, 4\n17. Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S.,\nKalantidis, Y., Li, L.J., Shamma, D.A., et al.: Visual genome: Connecting language\nand vision using crowdsourced dense image annotations. IJCV (2017) 4\n18. Lee, K.H., Chen, X., Hua, G., Hu, H., He, X.: Stacked cross attention for image-text\nmatching. In: ECCV (2018) 2\n19. Li, G., Duan, N., Fang, Y., Jiang, D., Zhou, M.: Unicoder-vl: A universal encoder\nfor vision and language by cross-modal pre-training. In: AAAI (2020) 1, 3\n20. Li, L., Chen, Y.C., Cheng, Y., Gan, Z., Yu, L., Liu, J.: Hero: Hierarchical\nencoder for video+ language omni-representation pre-training. arXiv preprint\narXiv:2005.00200 (2020) 3\n16 J. Cao et al.\n21. Li, L.H., Yatskar, M., Yin, D., Hsieh, C.J., Chang, K.W.: Visualbert: A simple\nand performant baseline for vision and language. arXiv preprint arXiv:1908.03557\n(2019) 1, 3\n22. Lu, J., Batra, D., Parikh, D., Lee, S.: Vilbert: Pretraining task-agnostic visiolin-\nguistic representations for vision-and-language tasks. In: NeurIPS (2019) 1, 2, 3\n23. Lu, J., Goswami, V., Rohrbach, M., Parikh, D., Lee, S.: 12-in-1: Multi-task vision\nand language representation learning. In: CVPR (2020) 3\n24. Michel, P., Levy, O., Neubig, G.: Are sixteen heads really better than one? In:\nNeurIPS (2019) 3\n25. Petroni, F., Rockt¨ aschel, T., Lewis, P., Bakhtin, A., Wu, Y., Miller, A.H., Riedel,\nS.: Language models as knowledge bases? In: EMNLP (2019) 2, 4\n26. Plummer, B.A., Wang, L., Cervantes, C.M., Caicedo, J.C., Hockenmaier, J., Lazeb-\nnik, S.: Flickr30k entities: Collecting region-to-phrase correspondences for richer\nimage-to-sentence models. IJCV (2015) 5\n27. Su, W., Zhu, X., Cao, Y., Li, B., Lu, L., Wei, F., Dai, J.: Vl-bert: Pre-training of\ngeneric visual-linguistic representations. In: ICLR (2020) 1, 2, 3\n28. Suhr, A., Zhou, S., Zhang, A., Zhang, I., Bai, H., Artzi, Y.: A corpus for reasoning\nabout natural language grounded in photographs. In: ACL (2019) 2\n29. Sun, C., Baradel, F., Murphy, K., Schmid, C.: Contrastive bidirectional transformer\nfor temporal representation learning. arXiv preprint arXiv:1906.05743 (2019) 3\n30. Sun, C., Myers, A., Vondrick, C., Murphy, K., Schmid, C.: Videobert: A joint\nmodel for video and language representation learning. In: ICCV (2019) 3\n31. Talmor, A., Elazar, Y., Goldberg, Y., Berant, J.: olmpics–on what language model\npre-training captures. arXiv preprint arXiv:1912.13283 (2019) 3\n32. Tan, H., Bansal, M.: Lxmert: Learning cross-modality encoder representations from\ntransformers. In: EMNLP (2019) 1, 2, 3, 4, 17\n33. Tenney, I., Das, D., Pavlick, E.: Bert rediscovers the classical nlp pipeline. In: ACL\n(2019) 2, 3\n34. Tenney, I., Xia, P., Chen, B., Wang, A., Poliak, A., McCoy, R.T., Kim, N.,\nVan Durme, B., Bowman, S.R., Das, D., et al.: What do you learn from con-\ntext? probing for sentence structure in contextualized word representations. In:\nICLR (2019) 3\n35. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\nL., Polosukhin, I.: Attention is all you need. In: NeurIPS (2017) 1\n36. Voita, E., Talbot, D., Moiseev, F., Sennrich, R., Titov, I.: Analyzing multi-head\nself-attention: Specialized heads do the heavy lifting, the rest can be pruned. In:\nACL (2019) 3\n37. Yu, L., Poirson, P., Yang, S., Berg, A.C., Berg, T.L.: Modeling context in referring\nexpressions. In: ECCV (2016) 2\n38. Zellers, R., Bisk, Y., Farhadi, A., Choi, Y.: From recognition to cognition: Visual\ncommonsense reasoning. In: CVPR (2019) 2\n39. Zhou, L., Palangi, H., Zhang, L., Hu, H., Corso, J.J., Gao, J.: Uniﬁed vision-\nlanguage pre-training for image captioning and vqa. In: AAAI (2020) 3\n40. Zhou, X., Zhang, Y., Cui, L., Huang, D.: Evaluating commonsense in pre-trained\nlanguage models. In: AAAI (2020) 4\nRevealing the Secrets of Pre-trained Vision-and-Language Models 17\nA Details on Pre-trained V+L Models\nFig. 5: Comparison between single-stream and two-stream V+L models.\nA comparison between single-stream and two-stream V+L models is pro-\nvided in Figure 5. We choose UNITER-base 10 [6] as the representative model\nfor single-stream, and LXMERT11 [32] for two-stream. As shown in Figure 5(a),\nUNITER-base has the same model structure as the BERT-base model [9], which\ncomposes of 12 layers of self-attention Transformers. Each layer has 12 self-\nattention heads, and each hidden representation is a 768-dimensional vector.\nAs shown in Figure 5(b), LXMERT is a two-stream model that performs intra-\nattention in the same modality ﬁrst, then cross-attention. We denote Tt,Tv, and\nTc as the Transformer modules that speciﬁcally model text-to-text, image-to-\nimage and cross-modal interactions, respectively. In LXMERT, Tt has 9 layers,\nTv has 5 layers, and Tc has 5 layers. Each Transformer’s hidden representation is\nin dimension of 768. Note that each layer in Tc contains one cross-attention layer\nbetween two modalities, followed by two self-attention layers for each modality.\nB Results on Untrained Baselines\nTo measure the gain from learning, we also conducted additional experiments\non untrained single-stream (SS) and two-stream (TS) baselines with random\nweights.\nMultimodal Fusion ProbeThe untrained SS model has NMI of 0.99 for all\noutput layers, suggesting that the two modalities are completely separated. The\nuntrained TS model has NMI of 0.56 for all output layers. This is because the\ncross-modality encoder layers force the two modalities to fuse, even in untrained\nsetting.\nModality Importance ProbeFor the untrained model, the average attention\nof [CLS] token on the image/text modality is 0.66/0.28. Note that the number\nof tokens in a sentence is usually smaller than that of the visual tokens.\n10 https://github.com/ChenRocks/UNITER\n11 https://github.com/airsplay/lxmert\n18 J. Cao et al.\nModel VCD VCC VRI VRC\nUntrained SS 50.0 58.0 50.0 11.4\nUntrained TS 50.0 66.0 50.0 9.34\nTable 6: Untrained visual coreference/relation attention baselines.\nCoref Type people body parts scene clothing instruments animals\nRatio 0.33 0.23 0.28 0.37 0.59 0.53\nTable 7:Results on whether the head selected for a speciﬁc coreference relation between\nan image-text pair imposes higher attention scores than all the other pairs.\nVisual Coreference and Relation ProbeWe provide additional untrained\nbaselines for visual coreference and visual relation probes in Table 6. Compared\nto Table 3 and Figure 4(a), for VCD and VRI, untrained baselines for both\nSS and TS are equivalent to random guess. For VRC, both SS and TS models\noutperform the baseline by around 10%. For VCC, the SS model outperforms the\nbaseline by 17%; while the TS model performs worse. This may be because after\nhard-designed multimodal fusion, the direct coreference relationship between a\npair of image/text tokens is diluted after training.\nFurthermore, we provide an additional evaluation on whether the head se-\nlected for a speciﬁc coreference relation of an image-text pair imposes higher\nattention scores for coreference relation than all other pairs. Results are sum-\nmarized in Table 7, which suggests that these attention heads with maximum\nattention weight do pay more attention to the coreference image-text pair, com-\npared to other unpaired ones.\nC Additional Guidelines for Future Model Design\nIn addition to the key takeaways in Sec. 4, we provide a set of guidelines for\nfuture model design based on our analysis and observations.\n(i) Single-stream model is able to capture suﬃcient intra- and cross-modal\nknowledge, while the restricted attention structure in two-stream model does not\nbring additional beneﬁt. For future work, we will further explore single-stream\nmodel design, which also exhibits better interpretability as observed.\n(ii) Initializing V+L model with BERT’s weights should be helpful, which\ncan enhance V+L model’s capability in language understanding.\n(iii) It remains unclear how to measure a pre-trained model without evaluat-\ning on downstream tasks. Given that ﬁnetuning is time consuming, the probing\ntasks we propose can provide a convenient tool to quickly test intermediate\nmodel checkpoints during pre-training.\n(iv) Explicitly adding extra supervision to probing tasks during model train-\ning may lead to more interpretable and robust model.\nRevealing the Secrets of Pre-trained Vision-and-Language Models 19\nD Details on Linguistic Probe\nWe probe the pre-trained models over nine tasks deﬁned in the SentEval toolkit [8],\nunder three categories:\n(i) Surface tasks: probe for the length of a sentence ( SentLen);\n(ii) Syntactic tasks: predict the depth of a sentence’s syntax tree, consecutive\ntoken inversions (BShift), and the top constituents sequences ( TopConst);\n(iii) Semantic tasks: test the tense ( Tense), the number implied by the sub-\nject/object ( SubjNum/ObjNum), the replacement of the noun/verb form ( SOMO),\nand the inversion of coordinating conjunctions ( CoordInv).\nE Additional Results\nWe provide additional results on multimodal fusion, visual coreference resolution,\nvisual relation detection, and linguistic probing.\nE.1 An t-SNE Visualization of Multimodal Fusion Degree\nAn t-SNE visualization of multimodal fusion degree of the ﬁrst and last layer of\nUNITER (over one image-text pair) is provided in Figure 6. As the layer goes\ndeeper, the two modalities become more intertwined.\nFig. 6: An t-SNE visualization of multimodal fusion degree of the ﬁrst and last layer of\nUNITER over one image-text pair. Each yellow and blue dot corresponds to a visual\nand textual token, respectively.\nE.2 Visual Coreference Resolution\nDue to space limit, we only reported results using the embeddings from Layer\n1, 5 and 12 in Table 3. A complete set of results is provided in Table 8. We\n20 J. Cao et al.\nClassiﬁer Input VCD (SS) VCD (TS) VCC (SS) VCC (TS)\n144 Attention Heads 52.04 53.68 75.10 54.47\nRandom Guess 50.00 50.00 12.50 12.50\nLayer 1 56.86 53.68 93.51 93.35\nLayer 2 57.58 53.49 93.91 93.36\nLayer 3 57.81 53.32 94.11 93.32\nLayer 4 57.97 52.92 94.10 93.12\nLayer 5 59.12 52.59 94.05 92.62\nLayer 6 58.58 / 94.02 /\nLayer 7 58.67 / 94.26 /\nLayer 8 58.65 / 93.96 /\nLayer 9 58.15 / 93.77 /\nLayer 10 57.65 / 93.77 /\nLayer 11 57.96 / 93.47 /\nLayer 12 58.40 / 93.44 /\nTable 8: Results of attention and layer-wise embedding probers on Visual Coreference\nDetection and Classiﬁcation (VCD and VCC). SS: single-stream; TS: two-stream.\nobserve that the attention probers work well for VCC, but not for VCD. Our as-\nsumption is that task granularity matters to the probers performance. Attention\nbehavior varies a lot in diﬀerent coreference relations, thus it performs well on\nVCC. The dataset for training VCC is built with positive examples from VCD\nonly. Therefore, VCD’s settings naturally dilute the distinction between diﬀerent\ncoreference relations’ attentions, which makes it a more challenging task.\nE.3 Visual Relation Detection\nResults of the layer-wise embedding probers on the Visual Relation Classiﬁcation\nand Identiﬁcation (VRC and VRI) tasks are visualized in Figure 4(b) and (c),\nrespectively. Detailed numbers corresponding to these two ﬁgures are provided\nin Table 9 and 10.\nE.4 Linguistic Probing\nFor linguistic probing, we ﬁrst obtain results from all the layers of a pre-trained\nmodel, then report the best number in Table 5. Detailed results for all the layers\nare provided in Table 11.\nE.5 Visualization of Attention Maps\nWe show the learned attention maps of one speciﬁc relation in the probing tasks:\nFigure 7 and 8 for visual coreference resolution (Section 3.3.2 of the main paper),\nand Figure 9 and 10 for visual relation detection (Section 3.4 of the main paper).\nRevealing the Secrets of Pre-trained Vision-and-Language Models 21\nClassiﬁer Input VRI (SS) VRC (SS) VRI (SS mis.) VRC (SS mis.)\nOriginal visual emb. 76.95 36.38 76.95 36.38\nLayer 1 77.18 37.70 77.03 37.31\nLayer 2 79.56 42.22 76.84 37.55\nLayer 3 82.28 43.02 76.55 37.05\nLayer 4 83.24 45.88 76.40 37.66\nLayer 5 84.45 47.67 76.28 37.49\nLayer 6 84.35 46.46 75.99 37.54\nLayer 7 84.13 45.67 75.88 37.51\nLayer 8 83.95 45.32 75.62 37.71\nLayer 9 85.98 54.35 74.75 37.71\nLayer 10 86.35 55.66 74.15 37.06\nLayer 11 86.19 56.64 73.84 36.72\nLayer 12 86.07 55.22 72.96 36.65\nTable 9: Accuracies (%) of the layer-wise embedding probers on Visual Relation Iden-\ntiﬁcation and Classiﬁcation (VRI and VRC) tasks for the single-stream (SS) model.\nmis.: mismatch.\nClassiﬁer Input VRI (TS) VRC (TS) VRI (TS mis.) VRC (TS mis.)\nOriginal visual emb. 76.95 36.38 76.95 36.38\nLayer 1 75.92 36.61 77.58 36.57\nLayer 2 75.42 35.86 77.44 35.82\nLayer 3 75.01 35.72 77.38 35.66\nLayer 4 74.67 36.01 77.19 35.99\nLayer 5 74.75 36.45 77.05 36.43\nLayer 6 87.00 67.82 87.03 13.20\nLayer 7 86.59 68.06 86.24 12.83\nLayer 8 86.14 68.67 85.86 11.93\nLayer 9 85.50 68.57 85.36 12.11\nLayer 10 85.43 69.66 85.26 12.07\nTable 10: Accuracies (%) of the layer-wise embedding probers on Visual Relation\nIdentiﬁcation and Classiﬁcation (VRI and VRC) tasks for the two-stream (TS) model.\nmis.: mismatch.\n22 J. Cao et al.\nLayer SentLen TreeDepth TopConst BShift Tense\n(Surface) (Syntactic) (Syntactic) (Syntactic) (Semantic)\n1 86.5, 75.8,93.929.8, 29.3,35.936.5, 31.8,63.650.0, 50.0,50.371.8, 66.0,82.2\n2 88.8, 73.8,95.933.6, 27.9,40.654.6, 29.4,71.350.0, 50.0,55.877.8, 70.5,85.9\n3 87.4, 74.4,96.234.7, 28.0,39.767.5, 32.5,71.556.6, 51.8,64.983.7, 72.1,86.6\n4 87.7, 76.6,94.235.3, 29.0,39.469.7, 39.3,71.371.2, 54.9,74.484.3, 72.5,87.6\n5 86.5, 77.5,92.036.4, 29.5,40.672.5, 48.6,81.373.6, 55.4,81.484.1, 74.4,89.5\n6 85.0, 81.8,88.436.1, 31.5,41.373.5, 48.1,83.374.6, 62.3,82.983.0, 75.6,89.8\n7 83.6,83.8, 83.7 36.2, 32.7,40.179.0, 63.4,84.176.5, 63.4,83.083.8, 75.6,89.9\n8 81.7, 81.8,82.935.1, 34.0,39.278.0, 67.2,84.077.3, 64.9,83.984.0, 75.2,89.9\n9 79.7, 79.8,80.134.5, 32.7,38.576.5, 65.7,83.178.8, 64.8,87.085.3, 75.1,90.0\n10 77.4, / , 77.0 33.9, / , 38.1 75.6, / , 81.7 81.6, / , 86.7 86.4, / , 89.7\n11 77.5, / , 73.9 34.1, / , 36.3 73.9, / , 80.3 80.9, / , 86.8 86.6, / , 89.9\n12 74.6, / , 69.5 32.2, / , 34.7 70.9, / , 76.5 80.8, / , 86.4 86.2, / , 89.5\nLayer SubjNum ObjNum SOMO CoordInv\n(Semantic) (Semantic) (Semantic) (Semantic)\n1 69.0, 70.6,77.665.3, 69.1,76.749.9,51.0, 49.9 50.0, 51.2,53.9\n2 74.8, 71.2,82.572.5, 70.8,80.650.1, 50.0,53.850.5, 50.0,58.5\n3 79.6, 70.7,82.078.2, 70.0,80.350.3, 50.5,55.856.8, 50.1,59.3\n4 79.9, 72.6,81.977.0, 71.7,81.450.9, 50.1,59.057.8, 50.0,58.1\n5 80.5, 74.8,85.876.6, 74.5,81.251.0, 50.1,60.259.4, 56.2,64.1\n6 81.1, 74.6,88.177.1, 74.6,82.052.2, 50.3,60.759.4, 56.1,71.1\n7 83.4, 77.3,87.478.4, 76.0,82.254.3, 51.2,61.660.3, 57.4,74.8\n8 82.7, 78.8,87.578.2, 76.8,81.254.5, 51.4,62.160.5, 58.7,76.4\n9 81.8, 78.8,87.678.8, 76.7,81.855.9, 51.0,63.461.8, 58.0,78.7\n10 81.9, / , 87.1 78.5, / , 80.5 56.3, / , 63.3 61.9, / , 78.4\n11 83.0, / , 85.7 78.2, / , 78.9 56.6, / , 64.4 62.1, / , 77.6\n12 81.8, / , 84.0 77.8, / , 78.7 57.1, / , 65.2 61.7, / , 74.9\nTable 11: Results on the linguistic probing tasks. For each task and each layer, the\nresults are presented in the order of UNITER, LXMERT, and the original BERT.\nRevealing the Secrets of Pre-trained Vision-and-Language Models 23\n(a) relation: animal\n (b) relation: vehicle\n(c) relation: people\n (d) relation: clothing\nFig. 7: Visualization of coreference information for all 144 attention heads ( V →T )\nin the single-stream model (UNITER-base). Note that only a set of attention heads is\nsigniﬁcant to the V →T attention across diﬀerent coreference relations.\n(a) relation: animal\n (b) relation: vehicle\n(c) relation: people\n (d) relation: clothing\nFig. 8: Visualization of coreference information across all attention heads ( V →T ) in\nthe two-stream model (LXMERT-base, 5 layers, 12 heads per layer).\n24 J. Cao et al.\n(a) relation: covering\n (b) relation: at\n(c) relation: on\n (d) relation: playing\nFig. 9: Visualization of the maximum attention between two visually-related tokens\nacross 144 attention heads in single-stream model (12 layers, 12 heads per layer). Note\nthat the spatial relationships ( on, at) have similar attention maps compared to other\nrelations.\n(a) relation: covering\n (b) relation: at\n(c) relation: on\n (d) relation: playing\nFig. 10: Visualization of the maximum attention between two visually-related tokens\nacross the attention heads in two-stream model (10 layers: 1-5 layers: self-attention;\n6-10 layers: cross-self-attention, 12 heads per layer).",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6858837008476257
    },
    {
      "name": "Inference",
      "score": 0.5657345652580261
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5309747457504272
    },
    {
      "name": "Natural language processing",
      "score": 0.4695003032684326
    },
    {
      "name": "Coreference",
      "score": 0.44989579916000366
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.415620893239975
    },
    {
      "name": "Resolution (logic)",
      "score": 0.2509450912475586
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I201448701",
      "name": "University of Washington",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210164937",
      "name": "Microsoft Research (United Kingdom)",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I2252078561",
      "name": "Meta (Israel)",
      "country": "IL"
    }
  ]
}