{
  "title": "An empirical study of LLaMA3 quantization: from LLMs to MLLMs",
  "url": "https://openalex.org/W4405887239",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A866206595",
      "name": "Huang Wei",
      "affiliations": [
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2354628266",
      "name": "Zheng Xing-yu",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A1240654481",
      "name": "Ma XuDong",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A4224020819",
      "name": "Qin, Haotong",
      "affiliations": [
        "ETH Zurich"
      ]
    },
    {
      "id": null,
      "name": "Lv, Chengtao",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A1869104162",
      "name": "Chen Hong",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A2066697334",
      "name": "Luo Jie",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A2216038443",
      "name": "Qi, Xiaojuan",
      "affiliations": [
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2498545944",
      "name": "Liu Xiang-long",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A2745051819",
      "name": "Magno Michele",
      "affiliations": [
        "ETH Zurich"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6851592950",
    "https://openalex.org/W4309591680",
    "https://openalex.org/W2777406049",
    "https://openalex.org/W3020212829",
    "https://openalex.org/W4406650295",
    "https://openalex.org/W6857055897",
    "https://openalex.org/W4385326807",
    "https://openalex.org/W6861189833",
    "https://openalex.org/W4391670689",
    "https://openalex.org/W6853251322",
    "https://openalex.org/W4391709676",
    "https://openalex.org/W6727099177",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2093647425",
    "https://openalex.org/W2989588035",
    "https://openalex.org/W3194676777",
    "https://openalex.org/W3083410900",
    "https://openalex.org/W2307512708",
    "https://openalex.org/W4285255856",
    "https://openalex.org/W6780502592",
    "https://openalex.org/W4382142077",
    "https://openalex.org/W4378770729",
    "https://openalex.org/W6796581206",
    "https://openalex.org/W4387156572",
    "https://openalex.org/W6859510725",
    "https://openalex.org/W2914304175",
    "https://openalex.org/W2602856279",
    "https://openalex.org/W2912083425"
  ],
  "abstract": null,
  "full_text": "Visual\nIntelligence\nHuangetal. VisualIntelligence            (2024) 2:36 \nhttps://doi.org/10.1007/s44267-024-00070-x\nRESEARCH OpenAccess\nAnempiricalstudyofLLaMA3quantization:\nfromLLMstoMLLMs\nWeiHuang1† ,XingyuZheng 2† ,XudongMa 2† ,HaotongQin 3* ,ChengtaoLv 2 ,HongChen 2 ,\nJieLuo2 ,XiaojuanQi 1 ,XianglongLiu 2 andMicheleMagno 3\nAbstract\nTheLLaMAfamily,acollectionoffoundationlanguagemodelsrangingfrom7Bto65Bparameters,hasbecomeone\nofthemostpowerfulopen-sourcelargelanguagemodels(LLMs)andthepopularLLMbackboneofmulti-modal\nlargelanguagemodels(MLLMs),widelyusedincomputervisionandnaturallanguageunderstandingtasks.In\nparticular,LLaMA3modelshaverecentlybeenreleasedandhaveachievedimpressiveperformanceinvarious\ndomainswithsuper-largescalepre-trainingonover15Ttokensofdata.Giventhewideapplicationoflow-bit\nquantizationforLLMsinresource-constrainedscenarios,weexploreLLaMA3’scapabilitieswhenquantizedtolow\nbit-width.Thisexplorationcanpotentiallyprovidenewinsightsandchallengesforthelow-bitquantizationof\nLLaMA3andotherfutureLLMs,especiallyinaddressingperformancedegradationissuesthatsuﬀerinLLM\ncompression.Speciﬁcally,wecomprehensivelyevaluatethe10existingpost-trainingquantizationandLoRA\nﬁne-tuning(LoRA-FT)methodsofLLaMA3on1-8bitsandvariousdatasetstorevealthelow-bitquantization\nperformanceofLLaMA3.Touncoverthecapabilitiesoflow-bitquantizedMLLM,weassessedtheperformanceofthe\nLLaMA3-basedLLaVA-Next-8Bmodelunder2-4ultra-lowbitswithpost-trainingquantizationmethods.Our\nexperimentalresultsindicatethatLLaMA3stillsuﬀersfromnon-negligibledegradationinlinguisticandvisual\ncontexts,particularlyunderultra-lowbitwidths.Thishighlightsthesigniﬁcantperformancegapatlowbit-widththat\nneedstobeaddressedinfuturedevelopments.Weexpectthatthisempiricalstudywillprovevaluableinadvancing\nfuturemodels,drivingLLMsandMLLMstoachievehigheraccuracyatlowerbittoenhancepracticality.\nKeywords: Modelquantization,Largelanguagemodel,Multi-modal,Deeplearning\n1 Introduction\nLaunched by Meta in February 2023, the LLaMA [1]s e -\nries,1 a collection of foundation language models rang-\ningfrom7Bto65Bparameters,representsabreakthrough\nin autoregressive large language models (LLMs) using the\nTransformer [2] architecture. From its ﬁrst release, with\n13 billion parameters, it outperformed the much larger,\nclosed-source GPT-3 model with 175 billion parameters.\n*Correspondence:haotong.qin@pbl.ee.ethz.ch\n3DepartmentofInformationTechnologyandElectricalEngineering,ETH\nZurich,Sternwartstrasse7,Zürich,Switzerland\nFulllistofauthorinformationisavailableattheendofthearticle †Equal\ncontributors\n1https://llama.meta.com.\nOn April 18, 2024, Meta introduced the LLaMA3 model,\noﬀering 8 billion and 70 billion parameter conﬁgurations.\nThanks to extensive pre-training on more than 15 tril-\nlion data tokens, the LLaMA3 models [3]h a v ea c h i e v e d\nstate-of-the-art performanceacross a wide range of tasks,\ne s t a b l i s h i n gt h eL L a M Af a m i l ya so n eo ft h eb e s to p e n -\nsource LLMs available for a wide variety of applications\nand deployment scenarios. Recently, the LLaVA team [4]\nhas launched the new LLaVA-Next-8B2 model based on\nLLaMA3, giving the stronger general multi-modal capa-\nbilities ofmulti-modallargelanguagemodels(MLLMs).\n2https://llava-vl.github.io/blog/2024-05-10-llava-next-stronger-llms.\n©TheAuthor(s)2024. OpenAccess ThisarticleislicensedunderaCreativeCommonsAttribution4.0InternationalLicense,which\npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit\nto the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The\nimagesorotherthirdpartymaterialinthisarticleareincludedinthearticle’sCreativeCommonslicence,unlessindicatedotherwise\nin a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not\npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright\nholder.Toviewacopyofthislicence,visit http://creativecommons.org/licenses/by/4.0/.\nHuangetal. VisualIntelligence            (2024) 2:36 Page2of13\nDespite their impressive performance, deploying\nLLaMA3 models still poses signiﬁcant challenges due to\nresource limitations in many scenarios. Fortunately, low-\nbit quantization [5–8] has emerged as one of the most\npopulartechniquesforcompressingLLMs.Thistechnique\nreduces the memory and computational requirements of\nLLMsduringinference,enablingthemtorunonresource-\nlimited devices. Addressing the performance drop after\ncompression is a major concern for current LLM quan-\ntization approaches. While numerous low-bit quantiza-\ntion methods have been proposed, their evaluations have\nprimarily focused on the earlier and less capable LLaMA\nmodels (LLaMA and LLaMA2). Thus, LLaMA3 presents\na new opportunity for the LLM community to assess the\nperformance of quantization on cutting-edge LLMs and\nMLLMs and understand existing methods’ strengths and\nlimitations. In this empirical study, we aim to analyze the\ncapability of LLaMA3 to handle the challenges associated\nwithdegradationduetoquantization.\nOurstudydelineatestheoutcomesoftwoprincipaltech-\nniques for quantizing LLaMA3 across three evaluation\ntracks: post-training quantization (PTQ) of LLMs, quan-\ntization of LLMs via LoRA-FineTuning (LoRA-FT), and\nPTQofLLaMA3-basedMLLM,aimingtoconductacom-\nprehensiveassessmentoftheLLaMA3model’scapabilities\ninlanguageandvisual-languagetasks.Weexplorearange\nof cutting-edge quantization methods across technical\ntracks(RTN[ 9],GPTQ[ 10],AWQ[ 11],SmoothQuant[ 5],\nPB-LLM [12], QuIP [13], DB-LLM [14], BiLLM [15], and\nSliM-LLM [8] for PTQ; QLoRA [16]a n dI R - Q L o R A[17]\nfor LoRA-FT), covering a wide spectrum from 1 to 8\nbits and utilizing a diverse array of evaluation datasets,\nincluding WikiText2 [18], C4 [19], PTB [20], Common-\nSenseQA datasets (PIQA [21], ARC-e [22], ARC-c [22],\nHellaSwag[23],Winogrande[ 24]),andMMLU[ 25]bench-\nmark. For multi-modal tasks, we follow a common prac-\ntice [11], performing low-bit post-training quantization\non the LLM component of LLaVA-Next-8B using GPTQ\nand AWQ. We then validate the quantized MLLM infer-\nencecapabilitieson6visuallanguagebenchmarks,includ-\ning AI2D [26], ChartQA [27], DocVQA [28], MME [29],\nandMMBench(English)[ 30].Theseevaluationsassessthe\ncapabilities and limitations of the LLaMA3 model un-\nder current LLM quantization techniques and serve as a\nsource of inspiration for designing future large language\nand large visual-language model quantization methods.\nThe decision to focus speciﬁcally on the LLaMA3 model\nis motivated by its superior performance among all cur-\nrent open-source instruction-tuned LLMs on a variety of\ndatasets, including 5-shot MMLU, 0-shot GPQA, 0-shot\nHumanEval,8-shotCoTGSM-8K,and4-shotCoTMATH.\nTheoverview ofourstudyispresentedasFig. 1.\nThisnotonlyhelpsadvancetheresearchwithintheLLM\nand MLLM quantization community,but also facilitates a\nbroader understanding and application of eﬀective quan-\ntization.\nWe evaluate the low-bit quantization of LLaMA3-8B, -\n70B, and LLaVA-Next-8B, where the pre-trained models\nwereobtainedfromtheiroﬃcialrepositories\n2.\nQuantization methods To evaluate the performance of\nlow-bitquantizedLLaMA3,weselectrepresentativeLLM\nquantization methods with extensive inﬂuence and func-\ntionality,including9PTQmethodsand2LoRA-FTmeth-\nods. The implementations of our evaluated quantization\nFigure1 Theoverviewofourempiricalstudy\nHuangetal. VisualIntelligence            (2024) 2:36 Page3of13\nmethods follow their open-source repositories.3 We also\nused8NVIDIAA800with80GBGPUmemoryforquan-\ntitative evaluation.\nEvaluation datasets For the PTQ methods, we evalu-\nate quantized LLaMA3 on the WikiText2 [18], PTB [20],\nand a portion of the C4 dataset [19], using perplexity\n(PPL) as the evaluation metric. Subsequently, we further\nconduct experiments on ﬁve zero-shot evaluation tasks\n(PIQA [21], Winogrande [24], ARC-e [22], ARC-c [22],\nand Hellaswag [23]) to fully validate the quantized per-\nformance of LLaMA3. We further conduct the evalua-\ntion on 5 visual language benchmarks (AI2D, ChartQA,\nDocVQA, MME, and MMBench(English)) for quantized\nLLaVA-Next-8B. To ensure fairness in evaluation of dif-\nferent PTQ methods, we set WikiText2 as the calibra-\ntion dataset for all quantization methods, with a sample\nsize of 128 and a sequence length of 2048. Additionally,\nformethodsrequiringgroupedquantization,westandard-\nize the block size at 128 to balance performance and in-\nference eﬃciency, a common practice in existing studies.\nFor the LoRA-FT methods, we conduct the evaluation on\nthe 5-shot MMLU benchmark [31] while also validating\ntheaforementioned5zero-shotdatasetsfortheLoRA-FT\nmethods. To ensure fairness in the evaluation of diﬀerent\nLoRA-FTmethods,weﬁne-tuneallmodelsusingthesame\ntraining data and consistent hyperparameters, including\nlearning rate, batch size, number of training epochs, and\nLoRAconﬁgurationssuchasrankandscalingfactors.\n2 Track1:post-trainingquantization\nQuantizationframework Webeginbyoutliningthegen-\neral uniform quantization process for LLMs, following\nstandard practices as described in Refs. [5, 10, 32]. This\nprocess involves mapping ﬂoating-point weights, distrib-\nuted within the range[wmin,wmax],t oa ni n t e g e rr a n g eo f\n2N,w h e r eN denotes the target bit-width. The quantiza-\ntion function for a weight matrixwf ∈ Rn×m is deﬁned as\nfollows:\nˆwq =clamp(⌊ wf\n/Delta1⌉ +z,0,2 N –1) (1a)\n/Delta1= wmax –wmin\n2N –1 (1b)\nz=–⌊ wmin\n/Delta1⌉ (1c)\nwhere ˆwq indicates quantized weight, which is integer,N\ndenotes the target bit-width,⌊·⌉ is round operation and\n3https://github.com/IST-DASLab/gptq,https://github.com/mit-han-lab/\nllm-awq,https://github.com/mit-han-lab/smoothquant, https://github.com/\nCornell-RelaxML/QuIP, https://github.com/Aaronhuang-778/SliM-LLM,\nhttps://github.com/hahnyuan/PB-LLM, https://github.com/Aaronhuang-\n778/BiLLM, https://github.com/artidoro/qlora, https://github.com/htqin/IR-\nQLoRA.\nclamp(·) constrains the value within integer range (e.g.\n[0,1,2,3 ], N =2 ) ./Delta1is scale factor andz is quantization\nzeropoint,respectively.AsshowninTable 1toTable 4,we\nprovide the performance of the low-bit LLaMA3-8B and\nLLaMA3-70Bwith8diﬀerentPTQmethods,respectively,\ncoveringawidebit-widthspectrumfrom1to8bits.Inad-\ndition, the performance of LLaMA1 and LLaMA2 under\nthesamesetting aresummarizedinTable 5.\nPTQmethods Amongthem,round-to-nearest(RTN)isa\nvanillaroundingquantizationmethodthatdirectlyapplies\nthestatisticalapproachfromEq.( 1a)–(1c)toobtainquan-\ntization parameters for immediate quantization. GPTQ\n[10] is one of the most eﬀective weight-only quantization\nmethods, utilizing an error compensation strategy based\non second-order loss. By using the inverse of the Hessian\nmatrix,itreducescompressionerrorsduringquantization.\nAWQ [11] employs an activation-aware outlier suppres-\nsion approach, introducing a scaling factors to smooth\ntheweightdistributionofLLMs,therebyeasingthequan-\ntization diﬃculty. QuIP [13] ensures consistency between\nweights and the Hessian by optimizing matrix computa-\ntions and adopts codebook encoding to quantize weight\nparameters, further enhancing the mapping accuracy be-\ntweencontinuousanddiscreteparameterspaces.Recently,\nHuang et al. [8] proposed a grouped mixed-precision\nquantizationmethodthatleverages theclusteringcharac-\nteristics of signiﬁcant weights. This method uses mixed\nprecision group quantization to achieve high-precision\nl o w- b i tq u a n t iza t i o ni nah a r d wa r e - fri e n d l ym a n n e r .Bo t h\napproaches preserve LLaMA3’s 3-bit quantization capa-\nbility, with the potential to bring 2-bit quantization to\nhigherperformancelevels.\nThe recent emergence of binarized LLM quantization\nmethods has realized ultra-low bit-width LLM weight\ncompression. PB-LLM [12]e m p l o y sam i x e d - p r e c i s i o n\nquantization strategy, retaining a small portion of signif-\nicant weight full-precision while quantizing most of the\nweights to 1 bit. DB-LLM [14] achieves eﬃcient LLM\ncompression through double binarization weight split-\nting and proposes a deviation-aware distillation strategy\nto further improve 2-bit LLM performance. BiLLM [15]\npushestheLLMquantizationlimitfurtherdownto1.1bit\nby residual approximation of salient weights and grouped\nquantization of non-salient weights. These LLM quanti-\nzation methods, which are specially designed for ultra-\nlow bit-width, can achieve higher accuracy of quantized\nLLaMA3-8B at≤ 2 bits, far outperforming methods such\nas GPTQ, AWQ, and QuIP below 2 bits (even 3 bits in\nsome cases). We also perform the evaluation on quan-\ntized activations using SmoothQuant [5], which shifts the\nquantization diﬃculty oﬄine from activations to weights\nto smooth out activation outliers. Our evaluation shows\nthatSmoothQuantcanmaintaintheaccuracyofLLaMA3\nHuangetal. VisualIntelligence            (2024) 2:36 Page4of13\nTable 1Evaluationresultsofpost-trainingquantizationontheLLaMA3-8Bmodel(1/2).#W,#A,and#Grepresentthebit-widthfor\nweight,activation,andgroupsize,respectively,‘–’indicatesnogroupingrequired,and ↓ denotesthatthelowerisbetter\nMethod #W #A #G PPL ↓\nWikiText2 C4 PTB\nLLaMA3 16 16 – 6.1 9.2 10.6\nRTN 4 16 128 8.5 13.4 14.5\n3 16 128 27.9 1.1e2 95.6\n2 16 128 1.9 ×103 2.5×104 1.8×104\n8 16 – 6.2 9.5 11.2\n4 16 – 8.7 14.0 14.9\n31 6 –2 . 2 ×103 5.6×102 2.0×103\n21 6 –2 . 7 ×106 7.4×106 3.1×106\nGPTQ[10] 4 16 128 6.5 10.4 11.0\n3 16 128 8.2 13.7 15.2\n2 16 128 2.1 ×102 4.1×104 9.1×102\n8 16 – 6.1 9.4 10.6\n4 16 – 7.0 11.8 14.4\n3 16 – 13.0 45.9 37.0\n21 6 –5 . 7 ×104 1.0×105 2.7×105\nAWQ [11] 4 16 128 6.6 9.4 11.1\n3 16 128 8.2 11.6 13.2\n2 16 128 1.7 ×106 2.1×106 1.8×106\n8 16 – 6.1 8.9 10.6\n4 16 – 7.1 10.1 11.8\n3 16 – 12.8 16.8 24.0\n21 6 –8 . 2 ×105 8.1×105 9.0×105\nSliM-LLM[8] 4 16 128 6.4 9.5 10.9\n3 16 128 7.7 13.1 14.7\n2 16 128 39.7 1.1 ×102 1.6×102\nQuIP[13] 4 16 – 6.5 11.1 9.5\n3 16 – 7.5 11.3 12.6\n2 16 – 85.1 1.3 ×102 1.8×102\nDB-LLM[14] 2 16 128 13.6 19.2 23.8\nPB-LLM[12] 2 16 128 24.7 79.2 65.6\n1.7 16 128 41.8 2.6 ×102 1.2×102\nBiLLM[15] 1.1 16 128 28.3 2.9 ×102 94.7\nSmoothQuant[5] 8 8 – 6.3 9.2 10.8\n6 6 – 7.7 11.8 12.5\n44–4 . 3 ×103 4.0×103 3.6×103\nOmniQuant[33] 6 6 – 7.0 10.1 –\n44–4 . 4 ×102 3.2×102 –\nI-LLM[34]6 6 – 6 . 6 9 . 8 –\n4 4 – 21.2 30.9 –\nSpinQuant[35]48 –6 . 5 – –\n44–7 . 1 – –\nHuangetal. VisualIntelligence            (2024) 2:36 Page5of13\nTable 2Evaluationresultsofpost-trainingquantizationontheLLaMA3-70Bmodel(1/2)\nMethod #W #A #G PPL ↓\nWikiText2 C4 PTB\nLLaMA3 16 16 – 2.9 6.9 8.2\nRTN 4 16 128 3.6 8.9 9.1\n3 16 128 11.8 22.0 26.3\n2 16 128 4.6 ×105 4.7×105 3.8×105\nGPTQ[10] 4 16 128 3.3 6.9 8.3\n3 16 128 5.2 10.5 9.7\n2 16 128 11.9 22.8 31.6\nAWQ [11] 4 16 128 3.3 7.0 8.3\n3 16 128 4.8 8.0 9.0\n2 16 128 1.7 ×106 1.4×106 1.5×106\nSliM-LLM[8] 4 16 128 3.3 7.0 8.3\n3 16 128 4.1 7.9 9.0\n2 16 128 9.5 16.2 18.7\nQuIP[13] 4 16 – 3.4 7.1 8.4\n3 16 – 4.7 8.0 8.9\n2 16 – 13.0 22.2 24.9\nPB-LLM[12] 2 16 128 11.6 34.5 27.2\n1.7 16 128 18.6 65.2 55.9\nBiLLM[15] 1.1 16 128 17.1 77.7 54.2\nSmoothQuant[5] 8 8 – 2.9 6.9 8.2\n6 6 – 2.9 6.9 8.2\n4 4 – 9.6 16.9 17.7\nwith6/8-bitweightsandactivations,butcollapsesat4bits.\nMoreover, we ﬁnd that the LLaMA3-70B model shows\nsigniﬁcant robustness to diﬀerent quantization methods,\nevenforultra-lowbit-width quantization.\nIn the evaluation metrics of PPL (Table1 and Table2)\nand CommonSenseQA (Table3 and Table4), we found\nthat, overall, the 4-bit methods had a slight performance\ndecrease(approximately2%)comparedtotheoriginal16-\nbitLLM,withnosigniﬁcantdiﬀerencesbetweenthediﬀer-\nent methods. In the context of 3-bit scenarios, traditional\nRTNquantizationmethodsfacedsubstantialperformance\nlosses(over10%lowerthan4bits),whilemethodssuchas\nGPTQ,AWQ,SliM-LLM,andQuIPwereabletomaintain\nperformance close to that of 4 bits (with less than 5% per-\nformance degradation). Interestingly, both DB-LLM and\nBiLLM were able to achieve reasonable results at ultra-\nlow bit-width settings of 2 bits and even 1.1 bits, possi-\nblyduetothelarge-batchﬁne-tuningstrategyandBiLLM’s\nﬁne-grainedsaliencepartitioning.Whenquantifyingboth\nweightandactivationsimultaneously,boththe8Band70B\nmodels demonstrated near lossless performance at 8 bits.\nAsthebit-widthwasfurtherreduced,theperformanceloss\ndecreasedsigniﬁcantlyforthe8Bmodel,whileitdecreased\nslowlyfor70Bmodels,indicatingthepresenceofinforma-\ntionredundancywithin70Bmodels.\nForpracticaldeployment,werecordedtheGPUmemory\nusageandtrainingtimeconsumptionforsomePTQmeth-\nodsondiﬀerentsizesoftheLLaMAmodel,asshowninTa-\nble6.ItdemonstratesthatmethodssuchasSmoothQuant\nand AWQ are highly eﬃcient in terms of memory us-\nage and training time, with SmoothQuant requiring only\n13.5 GB of GPU memory and 7 min for LLaMA2-7B,\nmaking it an ideal choice for memory-constrained en-\nvironments. In contrast, OmniQuant, while eﬀective for\nmodel compression, shows signiﬁcantly higher quantiza-\ntion time consumption. Meanwhile, we tested the infer-\nence latency of the quantized 4-bit models resulting from\nthe above methods in real-world deployment, as shown\nin Table6.I nf a c t ,G P T Q ,A W Q ,a n dO m n i q u a n ta l lu s e\nblock-wise quantizationtechniques,andtheoretically,the\nupperboundofrealinferencespeedoptimizationforthese\nthreemethodsisthesame.Toensureafaircomparisonof\nlatency, we conducted tests using the deployment meth-\nods provided in the original methodology. In the case of\nLLaMA2-7B, GPTQ, AWQ, and Omniquant all exhibited\nspeeds exceeding 100 tokens per second. However, in the\ncaseofLLaMA3-8B,theoverallspeedrangedbetween50\nto 80 tokens per second, with AWQ’s quantization kernel\nachieving an inference speed of 89.8 tokens per second,\nsurpassingthatofothermethods.\nHuangetal. VisualIntelligence            (2024) 2:36 Page6of13\nTable 3Evaluationresultsofpost-trainingquantizationonLLaMA3-8Bmodel(2/2). ↑ indicatesthatthehighervalueisbetter\nMethod #W #A #G CommonSenseQA ↑\nPIQA ARC-e ARC-c HellaSwag Wino Avg.\nLLaMA3 16 16 – 79.9 80.1 50.4 60.2 72.8 68.6\nRTN 4 16 128 76.6 70.1 45.0 56.8 71.0 63.9\n3 16 128 62.3 32.1 22.5 29.1 54.7 40.2\n2 16 128 53.1 24.8 22.1 26.9 53.1 36.0\n8 16 – 79.7 80.8 50.4 60.1 73.4 68.9\n4 16 – 75.0 68.2 39.4 56.0 69.0 61.5\n3 16 – 56.2 31.1 20.0 27.5 53.1 35.6\n2 16 – 53.1 24.7 21.9 25.6 51.1 35.3\nGPTQ[10] 4 16 128 78.4 78.8 47.7 59.0 72.6 67.3\n3 16 128 74.9 70.5 37.7 54.3 71.1 61.7\n2 16 128 53.9 28.8 19.9 27.7 50.5 36.2\n8 16 – 79.8 80.1 50.2 60.2 72.8 68.6\n4 16 – 76.8 74.3 42.4 57.4 72.8 64.8\n3 16 – 60.8 38.8 22.3 41.8 60.9 44.9\n2 16 – 52.8 25.0 20.5 26.6 49.6 34.9\nAWQ [11] 4 16 128 79.1 79.7 49.3 59.1 74.0 68.2\n3 16 128 77.7 74.0 43.2 55.1 72.1 64.4\n2 16 128 52.4 24.2 21.5 25.6 50.7 34.9\n8 16 – 79.6 80.3 50.5 60.2 72.8 68.7\n4 16 – 78.3 77.6 48.3 58.6 72.5 67.0\n3 16 – 71.9 66.7 35.1 50.7 64.7 57.8\n2 16 – 55.2 25.2 21.3 25.4 50.4 35.5\nSliM-LLM[8] 4 16 128 78.9 79.9 49.4 58.7 72.6 67.9\n3 16 128 77.8 73.7 42.9 55.5 72.8 64.5\n2 16 128 57.1 35.4 26.1 28.9 56.6 40.8\nQuIP[13] 4 16 – 78.2 78.2 47.4 58.6 73.2 67.1\n3 16 – 76.8 72.9 41.0 55.4 72.5 63.7\n2 16 – 52.9 29.0 21.3 29.2 51.7 36.8\nDB-LLM 2 16 128 68.9 59.1 28.2 42.1 60.4 51.8\nPB-LLM[12] 2 16 128 57.0 37.8 17.2 29.8 52.5 38.8\n1.7 16 128 52.5 31.7 17.5 27.7 50.4 36.0\nBiLLM[15] 1.1 16 128 56.1 36.0 17.7 28.9 51.0 37.9\nSmoothQuant[5] 8 8 – 79.5 79.7 49.0 60.0 73.2 68.3\n6 6 – 76.8 75.5 45.0 56.9 69.0 64.6\n4 4 – 54.6 26.3 20.0 26.4 50.3 35.5\nSpinQuant[35] 4 8 – 79.6 76.5 54.0 78.1 72.4 72.1\n4 4 – 77.5 75.0 50.9 75.9 68.5 69.6\n3 Track2:LoRA-FineTuningquantization\nQuantization framework The LoRA-FT quantization\nprocessinvolvesapplyinglow-bitquantizationtotheorig-\ninal model weights, adding low-rank matrices to the pre-\ntrained model weights, and ﬁne-tuning the low-rank ma-\ntriceswiththetrainingdata,allowingmodelupdateswith-\nout modifying the core parameters. In addition to us-\ning the integer quantization commonly applied in PTQ,\nLoRA-FT can also use NormalFloat quantization. The\nNormalFloat quantization function for a weight matrix\nw\nq ∈ Rn×m isdeﬁnedasfollows:\nˆwq =NFk(w\ns ) (2)\nwhere ˆwq indicates quantized weight,s is the scale fac-\ntor, typically set to the maximum value ofw and NFk\ndenotes the NormalFloat quantization operator atk bit-\nwidth,mappingeachvaluein wnorm tothenearestquantile\ninthenormaldistribution forabit-width k.\nHuangetal. VisualIntelligence            (2024) 2:36 Page7of13\nTable 4Evaluationresultsofpost-trainingquantizationontheLLaMA3-70Bmodel(2/2)\nMethod #W #A #G CommonSenseQA ↑\nPIQA ARC-e ARC-c HellaSwag Wino Avg.\nLLaMA3 16 16 – 82.4 86.9 60.3 66.4 80.6 75.3\nRTN 4 16 128 82.3 85.2 58.4 65.6 79.8 74.3\n3 16 128 64.2 48.9 25.1 41.1 60.5 48.0\n2 16 128 53.2 23.9 22.1 25.8 53.0 35.6\nGPTQ[10] 4 16 128 82.9 86.3 58.4 66.1 80.7 74.9\n3 16 128 80.6 79.6 52.1 63.5 77.1 70.6\n2 16 128 62.7 38.9 24.6 41.0 59.9 45.4\nAWQ [11] 4 16 128 82.7 86.3 59.0 65.7 80.9 74.9\n3 16 128 81.4 84.7 58.0 63.5 78.6 73.2\n2 16 128 52.2 25.5 23.1 25.6 52.3 35.7\nSliM-LLM[8] 4 16 128 82.9 86.5 59.0 66.2 80.7 75.1\n3 16 128 81.6 83.1 58.5 64.7 78.4 73.3\n2 16 128 76.2 66.3 45.7 55.4 63.7 61.5\nQuIP[13] 4 16 – 82.5 86.0 58.7 65.7 79.7 74.5\n3 16 – 82.3 83.3 54.9 63.9 78.4 72.5\n2 16 – 65.3 48.9 26.5 40.9 61.7 48.7\nPB-LLM[12] 2 16 128 65.2 40.6 25.1 42.7 56.4 46.0\n1.7 16 128 56.5 49.9 25.8 34.9 53.1 44.1\nBiLLM[15] 1.1 16 128 58.2 46.4 25.1 37.5 53.6 44.2\nSmoothQuant[5] 8 8 – 82.2 86.9 60.2 66.3 80.7 75.3\n6 6 – 82.4 87.0 59.9 66.1 80.6 75.2\n4 4 – 76.9 75.8 43.5 52.9 58.9 61.6\nLoRA-FTmethods ExceptforthePTQmethods,wealso\nprovide the performance of 4-bit LLaMA3-8B with 2 dif-\nferentLoRA-FTquantizationmethodsasshowninTable 7\nandTable 8,includingQLoRA[ 16]andIR-QLoRA[ 17].In\naddition, the performance of LLaMA-7B under the same\nsetting is summarized in Table9.Q L o R A[16] is the ﬁrst\nLoRA-FT method that uses 4-bit NormalFloat quantiza-\ntionforbasemodelweights,achievingsigniﬁcantmemory\nreduction with minimal impact on model performance.\nBuilding on QLoRA, IR-QLoRA [17]i n t r o d u c e si n f o r m a -\ntion calibration quantization and information elastic con-\nnectionfromtheinformationinspection,resultinginhigh-\nperformanceadaptationwithlow-bitprecision.\nO nt h eM M L Ud a t a s e t ,t h em o s tn o t a b l eo b s e r v a t i o n\nwith LLaMA3-8B under LoRA-FT quantization is that\nlow-rank ﬁne-tuning on the Alpaca [36] dataset not only\nfailstocompensatefortheerrorsintroducedbyquantiza-\ntion,butactuallyexacerbatesthedegradation.Speciﬁcally,\nvarious LoRA-FT quantization methods yield worse per-\nformance for quantized LLaMA3 below 4 bits compared\nwith their 4-bit counterparts without LoRA-FT. This is\nin stark contrast to similar phenomena on LLaMA and\nLLaMA2, where the 4-bit low-rank ﬁne-tuned quantized\nversions for the front panel could even easily outperform\ntheoriginalFP16counterpartonMMLU.Accordingtoour\nintuitive analysis, the main reason for this phenomenon\nis LLaMA3’s strong performance due to its massive pre-\nscale training. This means that the performance loss due\nto the quantization of the original model cannot be com-\npensated by ﬁne-tuning on a tiny data set with low-rank\nparameters (which can be seen as a subset of the origi-\nnal model [16, 37]). Despite the signiﬁcant quantization\nloss that cannot be compensated by ﬁne-tuning, the 4-\nbit LoRA-FT quantized LLaMA3-8B signiﬁcantly outper-\nforms LLaMA-7B and LLaMA2-7B using diﬀerent quan-\ntization methods. For instance, with the QLoRA method,\nthe 4-bit LLaMA3-8B has an average accuracy of 57.0\n(FP16: 64.8), exceeding the 4-bit LLaMA-7B’s 38.4 (FP16:\n34.6) by 18.6, and surpassing the 4-bit LLaMA2-7B’s 43.9\n(FP16:45.5)by13.1[ 17,38].ThisimpliesthatanewLoRA-\nFTquantizationparadigmisneededintheeraofLLaMA3.\nAsimilarphenomenonoccurswiththeCommonSense-\nQAbenchmark.Comparedtothe4-bitcounterpartswith-\nout LoRA-FT, the performance of the models ﬁne-tuned\nusing QLoRA and IR-QLoRA also declined (e.g. QLoRA\n2.8%vs.IR-QLoRA2.4%onaverage).Thisfurtherdemon-\nstrates the strength of using high-quality datasets in\nLLaMA3, as the general dataset, Alpaca, does not con-\ntribute to the model’s performance in other tasks. More-\nover, IR-QLoRA consistently outperforms QLoRA, due\nHuangetal. VisualIntelligence            (2024) 2:36 Page8of13\nTable 5PPLresultsofpost-trainingquantizationontheLLaMA1/2-7Bmodel\nMethod #W #A #G LLaMA-7B ↓ LLaMA2-7B↓\nWikiText2 C4 WikiText2 C4\nFP 16 16 – 5.7 7.1 5.5 7.0\nRTN 4 16 128 6.0 7.4 5.7 7.2\n3 16 128 7.0 8.6 6.7 8.4\n2 16 128 1.9 ×103 1.0×103 4.2×103 4.9×103\nGPTQ[10] 4 16 128 6.2 – 5.7 –\n3 16 128 6.6 7.9 6.3 7.9\n2 16 128 1.5 ×102 34.6 60.5 33.7\nAWQ [11] 4 16 128 5.8 – 5.6 –\n3 16 128 6.5 7.9 6.2 7.8\n2 16 128 2.6 ×105 1.9×105 2.2×105 1.75\nSliM-LLM[8] 3 16 128 6.4 6.1 6.2 7.7\n2 16 128 14.6 32.9 16.0 16.0\nQuIP[13] 2 16 – 29.7 33.7 39.7 31.9\nDB-LLM[14] 2 16 128 7.6 9.7 7.2 –\nPB-LLM[12] 2 16 128 24.6 49.7 25.4 29.8\n1.7 16 128 1.0 ×102 1.0×102 69.2 80.2\nBiLLM[15] 1.1 16 128 35.0 39.6 32.5 40.5\nSmoothQuant[5] 6 6 – 6.0 7.5 6.2 7.8\n4 4 – 22.3 32.3 83.1 77.3\nOmniQuant[33] 6 6 – 6.0 7.4 5.9 7.5\n4 4 – 11.3 14.5 14.3 18.0\nI-LLM[34] 6 6 – 5.8 7.3 5.7 7.3\n4 4 – 9.1 12.3 10.4 12.9\nSpinQuant[35]48 –– – 5 . 7 –\n44–– – 5 . 9 –\nTable 6GPUmemoryusage,quantizationtime,andinferencelatencyforPTQmethodsonLLaMA2-7BandLLaMA3-8B.Latencyis\ndeterminedunderagroupsizeof128.‘–’denotesthatthecurrentmethoddidnotprovidetherealquantizationkernelforthelatency\ntest\nMethod #W LLaMA2-7B LLaMA3-8B\nMemory(GB) Time(min) Speed(token/s) Memory(GB) Time(min) Speed(token/s)\nGPTQ[10] 4 26.4 17 159.4 40.3 19 61.2\nSmoothQuant[5] 4 13.5 7 – 16.0 15 –\nAWQ [11] 4 11.7 12 112.9 20.1 10 89.8\nOmniQuant[33] 4 29.45 325 147.2 30.61 307 54.9\nTable 7LoRA-FTonLLaMA3-8BwithAlpacadataset(1/2)\nMethod #W MMLU ↑\nHums. STEM Social Other Avg.\nLLaMA3 16 59.0 55.3 76.0 71.5 64.8\nNormalFloat 4 56.8 52.9 73.6 69.4 62.5\nQLoRA[16] 4 50.3 49.3 65.8 64.2 56.7\nIR-QLoRA[17] 4 52.2 49.0 66.5 63.1 57.2\nHuangetal. VisualIntelligence            (2024) 2:36 Page9of13\nTable 8LoRA-FTonLLaMA3-8BwithAlpacadataset(2/2)\nMethod #W CommonSenseQA ↑\nPIQA ARC-e ARC-c HellaSwag Wino Avg.\nLLaMA3 16 79.9 80.1 50.4 60.2 72.8 68.6\nNormalFloat 4 78.6 78.5 46.2 58.8 74.3 67.3\nQLoRA[16] 4 76.6 74.8 45.0 59.4 67.0 64.5\nIR-QLoRA[17] 4 76.3 74.3 45.3 59.1 69.5 64.9\nTable 9LoRA-FTonLLaMA-7BwithAlpacadataset\nMethod #W MMLU ↑\nHums. STEM Social Other Avg.\nLLaMA 16 33.3 29.8 37.8 38.0 34.6\nNormalFloat 4 33.1 30.6 38.8 38.8 35.1\nQLoRA[16] 4 36.1 31.9 42.0 44.5 38.4\nIR-QLoRA[17] 4 38.6 34.6 45.2 45.5 40.8\nTable 10GPUmemoryusage,trainingtime,andinferencelatencyforLoRA-FTMethodsonLLaMAmodels\nMethod #W LLaMA2-7B LLaMA3-8B\nMemory(GB) Time(hour) Speed(token/s) Memory(GB) Time(hour) Speed(token/s)\nLLaMA 16 – – 95.6 – – 79.7\nQLoRA[16] 4 7.2 15.3 88.6 13.2 16.1 72.8\nIR-QLoRA[17] 4 7.4 15.4 83.1 14.2 16.3 69.2\nto its incorporation of information calibration quantiza-\ntionandinformationelasticconnectionthroughinforma-\ntion inspection. These mechanisms allow IR-QLoRA to\nachieve high-performance adaptation even at low-bit ac-\ncuracy.\nForpracticaldeployment,werecordedtheGPUmemory\nusageandtrainingtimeconsumptionfordiﬀerentsizesof\nthe LLaMA model, as shown in Table10.I td e m o n s t r a t e s\nthatbothQLoRAandIR-QLoRAachievesigniﬁcantmem-\noryeﬃciency,dramaticallyreducingtherequiredmemory\nfootprint compared to the original LLaMA model. Nev-\nertheless, both QLoRA and IR-QLoRA introduce infer-\nencebottlenecksprimarilyduetothedequantizationpro-\ncess,whichresultsinanincreaseininferencelatency .The\ntrade-oﬀ between the reduced memory footprint and the\nslight increase in latency is often acceptable for deploy-\nment in resource-constrained environments where mem-\nory is the limiting factor. Further optimizations, such as\nhardware-speciﬁc tuning and algorithmic improvements,\ncould mitigate this bottleneck and improve overall infer-\nencespeed.\n4 Track3:multi-modallargelanguagemodel\nquantization\nFor the MLLM model, we follow a common practice by\nconducting post-training quantization on the LLaMA3\npart[ 11,39].AsshowninTable 11andTable 12,wec om-\nparetheultra-lowbit-widthperformanceofLLaVA-Next-\n8B under GPTQ and AWQ in six visual-language bench-\nmarks.\nWe initially evaluate the pure language capabilities of\nLLaVA-Next-8B, as illustrated in Table11. The fp16 pre-\ncision PPL metrics of the LLaMA3 model, after being\nﬁne-tunedforvisualtasks,worsenedacrossthreedatasets\ncompared to its performance on language tasks. This also\nsuggests that when ﬁne-tuned for visual-language tasks,\nthe introduction of image tokens leads to a partial loss\nand forgetting of LLaMA3’s inherent language abilities.\nThelanguagecapabilitiesofmulti-modalLLMs(MLLMs)\nshow a loss trend consistent with pure LLMs under low-\nbit quantization. Subsequently, we tested the quantized\nLLaMA3withintheMLLMmodelonvisualQAtasks. As\nshowninTable 12,underseveraladvancedPT Qmethods,\nthe 4-bit MLLM exhibits a loss of less than 2% on multi-\nmodalbenchmarks,eﬃcientlyperformingvisual-language\ntaskswithreducedmodelsize.\nAt 3 bits, the performance loss ranges from 5% to 20%,\nwiththehighestloss,20.75%,occurringontheMMEcog-\nnitiontask.Notably,regardlessofGPTQorAWQ,weob-\nserve that the 2-bit LLaVA-Next-8B completely collapses\nin the six multi-modal QA tasks, with scores dropping to\nzero. Although SliM-LLM mitigates the performance col-\nlapseofLLaVA-Next-8Bat2bits,itstillshowsalargeper-\nformancedegradation.\nHuangetal. VisualIntelligence            (2024) 2:36 Page10of13\nTable 11Evaluationresultsofpost-trainingquantizationonLLaVA-Next-8B(1/2)\nMethod #W #G PPL ↓\nWikiText2 C4 PTB\nLLaVA-Next(LLaMA3-8B) 16 – 9.5 14.8 16.3\nRTN 4 128 10.2 15.6 17.1\n3 128 23.2 26.5 36.1\n2 128 1.5 ×105 5.7×105 8.6×105\nGPTQ[10] 4 128 9.5 14.8 17.1\n3 128 13.0 19.5 28.4\n2 128 83.7 3.1 ×103 2.0×102\nAWQ [11] 4 128 9.9 15.3 16.9\n3 128 11.7 17.9 20.2\n2 128 1.6 ×106 2.0×106 2.2×106\nSliM-LLM[8] 4 128 9.5 15.0 16.5\n3 128 11.1 16.8 18.5\n2 128 46.3 2.0 ×102 1.8×102\nTable 12Evaluationresultsofpost-trainingquantizationonLLaVA-Next-8B(2/2).Ndenotsthattheanswerscoreis0,ortheoutputs\nareunexpectedcharacters\nMethod #W #G MultimodelTasks ↑\nAI2D ChartQA DocVQA MMBench MMECognition MMEPerception\nLLaVA-Next(LLaMA3-8B) 16 – 71.7 69.2 78.2 72.2 376.8 1588.3\nRTN 4 128 70.4 65.7 77.0 70.1 304.4 1550.9\n3 128 58.7 63.2 69.7 64.3 247.2 1526.2\n2 128 N N N N N N\nGPTQ[10] 4 128 70.7 67.4 77.4 71.0 331.6 1563.4\n3 128 66.2 65.1 75.6 67.4 290.1 1541.7\n2 128 N N N N N N\nAWQ [11] 4 128 70.6 68.0 77.2 71.1 325.7 1562.7\n3 128 67.7 65.4 74.4 68.0 298.6 1541.7\n2 128 N N N N N N\nSliM-LLM[8] 4 128 70.6 68.0 77.2 71.1 342.5 1563.9\n3 128 68.2 67.5 74.8 68.9 321.0 1554.3\n2 128 57.2 49.3 60.6 60.9 282.1 1515.8\nInFigs. 2-6,weshowsomerealvisual-languageresultsof\nLLaVA-Next-8Bunderdiﬀerentbitwidthsquantizedwith\nAWQ. The 4-bit quantized model can still generate pre-\ncisedescriptionsinimages,whilethe3-bitmodelexcelsin\noverallmulti-modalunderstanding butsuﬀers froma loss\nof detail. For example, in Fig.2, the descriptions of peo-\nple and actions in images by the 4-bit and 3-bit models\nare largely consistent with those of the 16-bit model. Ad-\nditionally, the 4-bit model aligns with the 16-bit model in\nabstractsemanticunderstandingof“bigcompanies”;how-\never, the 3-bit model misinterprets “big companies” as a\ndescriptor of hole size. Further, under 2-bit quantization,\nthemodelstrugglestoproducereasonableanswers,result-\ning in repetitive character responses. This contrasts with\nthe performance of 2-bit models in pure language tasks,\nwhere previous studies [8, 11, 15]h a v es h o w nt h a t2 - b i t\nquantizedmodelscanstillgeneratelogicallycoherentsen-\nt e n c e s .H o w e v e r ,i nM L L Mt a s k s ,t h e2 - b i tm o d e lf a i l st o\nproduce results close to expectations. This further indi-\ncates that the advanced PTQ method in the current LLM\ndoes not eﬀectively perform equally well in the ultra-low\nbitMLLMmodels,whichalsoinspiresfutureworktopro-\npose better quantization solutions for this huge challenge\ninMLLM.\n5C o n c l u s i o n\nT h er e c e n t l yr e l e a s e dL L a M A 3f a m i l yh a sq u i c k l yb e -\ncome the most powerful LLM backbones, attracting sig-\nniﬁcantinterestfromLLMandMLLMresearchers.Build-\ning on this momentum, our study aims to thoroughly\nHuangetal. VisualIntelligence            (2024) 2:36 Page11of13\nFigure2 TheVQAresultsofLLaVA-Next-8Bfordiﬀerentquantization\nbitwidths(1/5)\nFigure3 TheVQAresultsofLLaVA-Next-8Bfordiﬀerentquantization\nbitwidths(2/5)\nevaluate the performance of LLaMA3 for various low-\nbitquantizationtechniques,includingpost-trainingquan-\ntization and LoRA ﬁne-tuning quantization for LLMs\nand MLLMs. Our goal is to assess the limits of its ca-\npabilities in resource-constrained scenarios using exist-\ning LLM and MLLM quantization techniques. We found\nthat while LLaMA3 still demonstrates superior perfor-\nmance after quantization, the performance degradation\nassociated with quantization is signiﬁcant and can lead\nto larger declines. This decrease is mainly due to the fact\nthat the powerful pre-training process allows LLaMA3 to\nlearnmoreinformationtoasimilarextentastheprevious\nLLaMAandLLaMA2,anditsmoresophisticatedinternal\nknowledge structure makes the eﬀect of the quantization\nperturbation less obvious. The collapse of the ultra-low 2\nbitsalsoconﬁrmsthatthequantizedLLaMA3backbonein\nMLLMexacerbatestheperformancelosscausedbyquan-\ntizationwhenprocessingcomplexvisualinformation.This\nFigure4 TheVQAresultsofLLaVA-Next-8Bfordiﬀerentquantization\nbitwidths(3/5)\nFigure5 TheVQAresultsofLLaVA-Next-8Bfordiﬀerentquantization\nbitwidths(4/5)\ndiscovery highlights the potential challenges of deploying\nLLaMA3 in resource-constrained environments and un-\nderscores the ample room for growth and improvement\nin low-bit quantization. The empirical results of our re-\nsearch are expected to be valuable in the development of\nfutureLLMquantizationtechniques,especiallyinnarrow-\ning the performance gap with the original models. By ad-\ndressing the performance degradation caused by low-bit\nquantization, we anticipate that subsequent quantization\nparadigms will allow LLMs to achieve stronger capabili-\nties at a lower computational cost, ultimately driving the\nprogressofgenerativeartiﬁcialintelligence,asrepresented\nbyLLMsandMLLMs,tonewheights.\nHuangetal. VisualIntelligence            (2024) 2:36 Page12of13\nFigure6 TheVQAresultsofLLaVA-Next-8Bfordiﬀerentquantization\nbitwidths(5/5)\nAbbreviations\nCV,computervision;LLMs,largelanguagemodels;LoRA-FT,LoRA-FineTuning;\nMLLMs,multi-modallargelanguagemodel;NLU,naturallanguage\nunderstanding;PTQ,post-trainingquantization.\nAuthorcontributions\nAllauthorscontributedtothestudy’sconceptionanddesign.WH,XZ,XM,HQ,\nCL,andHCperformeddatacollectionandanalysis.HQwrotetheﬁrstdraftof\nthemanuscript,andallauthorscommentedonpreviousversions.Allauthors\nreadandapprovedtheﬁnalmanuscript.Weproposetheoriginalideatogether.\nFunding\nThisworkwassupportedbytheNationalScienceandTechnologyMajor\nProject(2021ZD0110503),theSwissNationalScienceFoundation(SNSF)\nproject200021E_219943NeuromorphicAttentionModelsforEventData\n(NAMED),theBaiduScholarship,andtheNationalNaturalScienceFoundation\nofChina(Nos.62306025and92367204).\nDataavailability\nAvailabilityofdataandmaterial:Thedatasetsgeneratedduringand/or\nanalyzedduringthecurrentstudyareavailablefromthecorrespondingauthor\nonreasonablerequest.Ourprojectisreleasedon GitHubandquantized\nLLaMA3modelsarereleasedin HuggingFace.\nDeclarations\nCompetinginterests\nAllauthorscertifythattheyhavenoaﬃliationswithorinvolvementinany\norganizationorentitywithanyﬁnancialinterestornon-ﬁnancialinterestinthe\nsubjectmatterormaterialsdiscussedinthismanuscript.\nAuthordetails\n1DepartmentofElectricalandElectronicEngineering,TheUniversityofHong\nKong,PokfulamRoad,HongKong,999077,China. 2SchoolofComputer\nScienceandEngineering,BeihangUniversity,XueyuanRoad,Beijing,100191,\nChina. 3DepartmentofInformationTechnologyandElectricalEngineering,\nETHZurich,Sternwartstrasse7,Zürich,Switzerland.\nReceived:27August2024 Revised:16December2024\nAccepted:17December2024\nReferences\n1. Touvron,H.,Lavril,T.,Izacard,G.,Martinet,X.,Lachaux,M.-A.,Lacroix,T.,\netal.(2023).LLaMA:openandeﬃcientfoundationlanguagemodels.arXiv\npreprint.arXiv:2302.13971.\n2. Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,L.,Gomez,A.N.,et\nal.(2017).Attentionisallyouneed.InI.Guyon,U.VonLuxburg,S.Bengio,\netal.(Eds.), Proceedingsofthe31stinternationalconferenceonneural\ninformationprocessingsystems (pp.5998–6008).RedHook:Curran\nAssociates.\n3. Dubey,A.,Jauhri,A.,Pandey,A.,Kadian,A.,Al-Dahle,A.,Letman,A.,Mathur,\nA.,Schelten,A.,Yang,A.,Fan,A.,etal.(2024).Thellama3herdofmodels.\narXivpreprint. arXiv:2407.21783.\n4. Liu,H.,Li,C.,Wu,Q.,&Lee,Y.J.(2023).Visualinstructiontuning.InA.Oh,T.\nNeumann,A.Globerson,etal.(Eds.), Proceedingsofthe37thinternational\nconferenceonneuralinformationprocessingsystems (pp.1–25).RedHook:\nCurranAssociates.\n5. Xiao,G.,Lin,J.,Seznec,M.,Wu,H.,Demouth,J.,&Han,S.(2023).\nSmoothQuant:accurateandeﬃcientpost-trainingquantizationforlarge\nlanguagemodels.In Proceedingsoftheinternationalconferenceonmachine\nlearning(pp.38087–38099).RetrievedNovember10,2024,from https://\nproceedings.mlr.press/v202/xiao23c.html.\n6. Qin,H.,Zhang,Y.,Ding,Y.,Liu,X.,Danelljan,M.,Yu,F.,etal.(2023).QuantSR:\naccuratelow-bitquantizationforeﬃcientimagesuper-resolution.InA.\nOh,T.Neumann,A.Globerson,etal.(Eds.), Proceedingsofthe37th\ninternationalconferenceonneuralinformationprocessingsystems (pp.1–11).\nRedHook:CurranAssociates.\n7. Jacob,B.,Kligys,S.,Chen,B.,Zhu,M.,Tang,M.,Howard,A.,etal.(2018).\nQuantizationandtrainingofneuralnetworksforeﬃcient\ninteger-arithmetic-onlyinference.In ProceedingsoftheIEEE/CVFconference\noncomputervisionandpatternrecognition (pp.2704–2713).Piscataway:\nIEEE.\n8. Huang,W.,Qin,H.,Liu,Y.,Li,Y.,Liu,X.,Benini,L.,etal.(2024).SliM-LLM:\nSalience-drivenmixed-precisionquantizationforlargelanguagemodels.\narXivpreprint. arXiv:2405.14917.\n9. Nagel,M.,Amjad,R.A.,VanBaalen,M.,Louizos,C.,&Blankevoort,T.(2020).\nUpordown?Adaptiveroundingforpost-trainingquantization.In\nInternationalconferenceonmachinelearning (pp.7197–7206).PMLR.\n10. Frantar,E.,Ashkboos,S.,Hoeﬂer,T.,&Alistarh,D.(2022).GPTQ:Accurate\npost-trainingquantizationforgenerativepre-trainedtransformers.arXiv\npreprint.arXiv:2210.17323.\n11. Lin,J.,Tang,J.,Tang,H.,Yang,S.,Chen,W.-M.,Xiao,G.,etal.(2024).AWQ:\nactivation-awareweightquantizationforon-deviceLLMcompressionand\nacceleration.InP.B.Gibbons,G.Pekhimenko,&C.deSa(Eds.), Proceedings\nofmachinelearningandsystems (pp.87–100).RetrievedNovember10,\n2024,from https://proceedings.mlsys.org/paper_ﬁles/paper/2024/hash/\n42a452cbafa9dd64e9ba4aa95cc1ef21-Abstract-Conference.html.\n12. Shang,Y.,Yuan,Z.,Wu,Q.,&Dong,Z.(2024).PB-LLM:partiallybinarized\nlargelanguagemodels.In Proceedingsofthe12thinternationalconference\nonlearningrepresentations (pp.1–14).RetrievedNovember10,2024,from\nhttps://openreview.net/forum?id=BifeBRhikU.\n13. Chee,J.,Cai,Y.,Kuleshov,V.,&DeSa,C.(2024).QuIP:2-bitquantizationof\nlargelanguagemodelswithguarantees.InA.Oh,T.Neumann,A.\nGloberson,etal.(Eds.), Proceedingsofthe37thinternationalconferenceon\nneuralinformationprocessingsystems (pp.1–34).RedHook:Curran\nAssociates.\n14. Chen,H.,Lv,C.,Ding,L.,Qin,H.,Zhou,X.,Ding,Y.,etal.(2024).DB-LLM:\naccuratedual-binarizationforeﬃcientLLMs.InL.-W.Ku,A.Martins,&V.\nSrikumar(Eds.), Findingsoftheassociationforcomputationallinguistics (pp.\n8719–8730).Stroudsburg:ACL.\n15. Huang,W.,Liu,Y.,Qin,H.,Li,Y.,Zhang,S.,Liu,X.,etal.(2024).BiLLM:\npushingthelimitofpost-trainingquantizationforLLMs.In Proceedingsof\nthe41stinternationalconferenceonmachinelearning (pp.1–20).Retrieved\nNovember10,2024,from https://openreview.net/forum?id=\nqOl2WWOqFg.\n16. Dettmers,T.,Pagnoni,A.,Holtzman,A.,&Zettlemoyer,L.(2024).QLoRA:\neﬃcientﬁnetuningofquantizedLLMs.InA.Oh,T.Neumann,A.Globerson,\netal.(Eds.), Proceedingsofthe37thinternationalconferenceonneural\ninformationprocessingsystems (pp.1–28).RedHook:CurranAssociates.\n17. Qin,H.,Ma,X.,Zheng,X.,Li,X.,Zhang,Y.,Liu,S.,etal.(2024).Accurate\nlora-ﬁnetuningquantizationofLLMsviainformationretention.In\nProceedingsofthe41stinternationalconferenceonmachinelearning (pp.\n1–19).RetrievedNovember10,2024,from https://openreview.net/forum?\nid=jQ92egz5Ym.\n18. Merity,S.,Xiong,C.,Bradbury,J.,&Socher,R.(2016).Pointersentinel\nmixturemodels.In Proceedingsofthe5thinternationalconferenceon\nlearningrepresentations (pp.1–15).RetrievedNovember10,2024,from\nhttps://openreview.net/forum?id=Byj72udxe.\nHuangetal. VisualIntelligence            (2024) 2:36 Page13of13\n19. Raﬀel,C.,Shazeer,N.,Roberts,A.,Lee,K.,Narang,S.,Matena,M.,etal.\n(2020).Exploringthelimitsoftransferlearningwithauniﬁedtext-to-text\ntransformer.JournalofMachineLearningResearch ,21(1),5485–5551.\n20. Marcus,M.,GraceKim,P.,Marcinkiewicz,M.A.,MacIntyre,R.,Bies,A.,\nFerguson,M.,etal.(1994).ThePennTreebank:annotatingpredicate\nargumentstructure.In Proceedingsofhumanlanguagetechnology\nworkshop(pp.114–119).SanFrancisco:MorganKaufmann.\n21. Bisk,Y.,Zellers,R.,LeBras,R.,Gao,J.,&Choi,Y.(2020).PIQA:reasoningabout\nphysicalcommonsenseinnaturallanguage.In Proceedingsofthe34thAAAI\nconferenceonartiﬁcialintelligence (pp.7432–7439).PaloAlto:AAAIPress.\n22. Clark,P.,Cowhey,I.,Etzioni,O.,Khot,T.,Sabharwal,A.,Schoenick,C.,etal.\n(2018).Thinkyouhavesolvedquestionanswering?TryARC-DA,theAI2\nreasoningchallenge.arXivpreprint. arXiv:1803.05457.\n23. Zellers,R.,Holtzman,A.,Bisk,Y.,Farhadi,A.,&Choi,Y.(2019).HellaSwag:can\namachinereallyﬁnishyoursentence?InA.Korhonen,D.R.Traum,&L.\nM‘arquez(Eds.), Proceedingsofthe57thconferenceoftheassociationfor\ncomputationallinguistics (pp.4791–4800).Stroudsburg:ACL.\n24. Sakaguchi,K.,LeBras,R.,Bhagavatula,C.,&Choi,Y.(2021).Winogrande:an\nadversarialwinogradschemachallengeatscale. Communicationsofthe\nACM,64(9),99–106.\n25. Hendrycks,D.,Burns,C.,Basart,S.,Zou,A.,Mazeika,M.,Song,D.,&\nSteinhardt,J.(2021).Measuringmassivemultitasklanguage\nunderstanding.In ProceedingsoftheInternationalConferenceonLearning\nRepresentations(ICLR).\n26. Kembhavi,A.,Salvato,M.,Kolve,E.,Seo,M.,Hajishirzi,H.,&Farhadi,A.\n(2016).Adiagramisworthadozenimages.\n27. Masry,A.,Long,D.X.,Tan,J.Q.,Joty,S.,&Hoque,E.(2022).Abenchmarkfor\nquestionansweringaboutchartswithvisualandlogicalreasoning.arXiv\npreprint.arXiv:2203.10244.\n28. Mathew,M.,Karatzas,D.,&Jawahar,C.V.(2021).Docvqa:adatasetforvqa\nondocumentimages.In ProceedingsoftheIEEE/CVFwinterconferenceon\napplicationsofcomputervision (pp.2200–2209).\n29. Fu,C.,Chen,P.,Shen,Y.,Qin,Y.,Zhang,M.,Lin,X.,Yang,J.,Zheng,X.,Li,K.,\nSun,X.,Wu,Y.,&Ji,R.(2024).Mme:acomprehensiveevaluation\nbenchmarkformultimodallargelanguagemodels.\n30. Liu,Y.,Duan,H.,Zhang,Y.,Li,B.,Zhang,S.,Zhao,W.,Yuan,Y.,Wang,J.,He,C.,\nLiu,Z.,etal.(2025).Mmbench:isyourmulti-modalmodelanall-around\nplayer?In Europeanconferenceoncomputervision (pp.216–233).Berlin:\nSpringer.\n31. Hendrycks,D.,Burns,C.,Basart,S.,Zou,A.,Mazeika,M.,Song,D.,etal.\n(2021).Measuringmassivemultitasklanguageunderstanding.In\nProceedingsofthe9thinternationalconferenceonlearningrepresentations\n(pp.1–27).RetrievedNovember10,2024,from https://openreview.net/\nforum?id=d7KBjmI3GmQ.\n32. Liu,Z.,Oguz,B.,Zhao,C.,Chang,E.,Stock,P.,Mehdad,Y.,etal.(2024).\nLLM-QAT:data-freequantizationawaretrainingforlargelanguagemodels.\nInL.-W .Ku,A.Martins,&V .Srikumar(Eds.), Findingsoftheassociationfor\ncomputationallinguistics (pp.467–484).Stroudsburg:ACL.\n33. Shao,W .,Chen,M.,Zhang,Z.,Xu,P .,Zhao,L.,Li,Z.,Zhang,K.,Gao,P .,Qiao,Y .,\n&Luo,P.(2023).Omniquant:Omnidirectionallycalibratedquantizationfor\nlargelanguagemodels.arXivpreprint. arXiv:2308.13137.\n34. Hu,X.,Cheng,Y.,Yang,D.,Yuan,Z.,Yu,J.,Xu,C.,&Zhou,S.(2024).I-llm:\nEﬃcientinteger-onlyinferenceforfully-quantizedlow-bitlargelanguage\nmodels.arXivpreprint. arXiv:2405.17849.\n35. Liu,Z.,Zhao,C.,Fedorov,I.,Soran,B.,Choudhary,D.,Krishnamoorthi,R.,\nChandra,V.,Tian,Y.,&Blankevoort,T.(2024).Spinquant–llmquantization\nwithlearnedrotations.arXivpreprint. arXiv:2405.16406.\n36. Taori,R.,Gulrajani,I.,Zhang,T.,Dubois,Y.,Li,X.,Guestrin,C.,etal.(2023).\nStanfordalpaca:aninstruction-followingllamamodel.Retrieved\nNovember10,2024,from https://github.com/tatsu-lab/stanford_alpaca.\n37. Hu,E.J.,W allis,P .,Allen-Zhu,Z.,Li,Y .,W ang,S.,W ang,L.,Chen,W .,etal.\n(2022).LoRA:low-rankadaptationoflargelanguagemodels.In\nProceedingsofthe10thinternationalconferenceonlearningrepresentations\n(pp.1–13).RetrievedNovember10,2024,from https://openreview.net/\nforum?id=nZeVKeeFYf9\n.\n38. Xu,Y.,Xie,L.,Gu,X.,Chen,X.,Chang,H.,Zhang,H.,etal.(2024).QA-LoRA:\nquantization-awarelow-rankadaptationoflargelanguagemodels.In\nProceedingsofthe12thinternationalconferenceonlearningrepresentations\n(pp.1–18).RetrievedNovember10,2024,from https://openreview.net/\nforum?id=WvFoJccpo8.\n39. Lin,J.,Yin,H.,Ping,W.,Molchanov,P.,Shoeybi,M.,&Song,H.(2024).VILA:\nonpre-trainingforvisuallanguagemodels.In ProceedingsoftheIEEE/CVF\nconferenceoncomputervisionandpatternrecognition (pp.26689–26699).\nPiscataway:IEEE.\nPublisher’sNote\nSpringerNatureremainsneutralwithregardtojurisdictionalclaimsin\npublishedmapsandinstitutionalaﬃliations.",
  "topic": "Quantization (signal processing)",
  "concepts": [
    {
      "name": "Quantization (signal processing)",
      "score": 0.8587687015533447
    },
    {
      "name": "Computer science",
      "score": 0.6632946133613586
    },
    {
      "name": "Empirical research",
      "score": 0.41719865798950195
    },
    {
      "name": "Algorithm",
      "score": 0.1930626928806305
    },
    {
      "name": "Mathematics",
      "score": 0.1070130467414856
    },
    {
      "name": "Statistics",
      "score": 0.0
    }
  ]
}