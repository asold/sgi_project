{
  "title": "Strategies for Training Large Vocabulary Neural Language Models",
  "url": "https://openalex.org/W2217098601",
  "year": 2022,
  "authors": [
    {
      "id": null,
      "name": "Chen, Welin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226762532",
      "name": "Grangier, David",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221359259",
      "name": "Auli, Michael",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2131462252",
    "https://openalex.org/W1723811852",
    "https://openalex.org/W2951714314",
    "https://openalex.org/W2950797609",
    "https://openalex.org/W1499253590",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W1575384945",
    "https://openalex.org/W2180952760",
    "https://openalex.org/W2251098065",
    "https://openalex.org/W1518951372",
    "https://openalex.org/W2950075229",
    "https://openalex.org/W2250379827",
    "https://openalex.org/W2949888546",
    "https://openalex.org/W932413789",
    "https://openalex.org/W2950344723",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W36903255",
    "https://openalex.org/W2251682575",
    "https://openalex.org/W2121227244",
    "https://openalex.org/W2185726469",
    "https://openalex.org/W2134800885"
  ],
  "abstract": "Training neural network language models over large vocabularies is still computationally very costly compared to count-based models such as Kneser-Ney. At the same time, neural language models are gaining popularity for many applications such as speech recognition and machine translation whose success depends on scalability. We present a systematic comparison of strategies to represent and train large vocabularies, including softmax, hierarchical softmax, target sampling, noise contrastive estimation and self normalization. We further extend self normalization to be a proper estimator of likelihood and introduce an efficient variant of softmax. We evaluate each method on three popular benchmarks, examining performance on rare words, the speed/accuracy trade-off and complementarity to Kneser-Ney.",
  "full_text": "Strategies for Training Large Vocabulary Neural Language Models\nWenlin Chen†\nWashington University\nSt Louis, MO\nwenlinchen@wustl.edu\nDavid Grangier\nFacebook AI Research\nMenlo Park, CA\ngrangier@fb.com\nMichael Auli\nFacebook AI Research\nMenlo Park, CA\nmichaelauli@fb.com\nAbstract\nTraining neural network language models over\nlarge vocabularies is still computationally very\ncostly compared to count-based models such\nas Kneser-Ney. At the same time, neural\nlanguage models are gaining popularity for\nmany applications such as speech recogni-\ntion and machine translation whose success\ndepends on scalability. We present a sys-\ntematic comparison of strategies to represent\nand train large vocabularies, including soft-\nmax, hierarchical softmax, target sampling,\nnoise contrastive estimation and self normal-\nization. We further extend self normalization\nto be a proper estimator of likelihood and in-\ntroduce an efﬁcient variant of softmax. We\nevaluate each method on three popular bench-\nmarks, examining performance on rare words,\nthe speed/accuracy trade-off and complemen-\ntarity to Kneser-Ney.\n1 Introduction\nNeural network language models (Bengio et al.,\n2003; Mikolov et al., 2010) have gained popular-\nity for tasks such as automatic speech recognition\n(Arisoy et al., 2012) and statistical machine trans-\nlation (Schwenk et al., 2012; Vaswani et al., 2013).\nFurthermore, models similar in architecture to neu-\nral language models have been proposed for transla-\ntion (Le et al., 2012; Devlin et al., 2014; Bahdanau\net al., 2015), summarization (Chopra et al., 2015)\nand language generation (Sordoni et al., 2015).\n†Work done while Wenlin was an intern at Facebook.\nLanguage models assign a probability to a word\ngiven a context of preceding, and possibly subse-\nquent, words. The model architecture determines\nhow the context is represented and there are sev-\neral choices including recurrent neural networks\n(Mikolov et al., 2010), or log-bilinear models (Mnih\nand Hinton, 2010). We experiment with a simple but\nproven feed-forward neural network model similar\nto Bengio et al. (2003). Our focus is not the model\narchitecture or how the context can be represented\nbut rather how to efﬁciently deal with large output\nvocabularies, a problem common to all approaches\nto neural language modeling and related tasks such\nas machine translation and language generation.\nPractical training speed for these models quickly\ndecreases as the vocabulary grows. This is due to\nthree combined factors. First, model evaluation and\ngradient computation become more time consum-\ning, mainly due to the need of computing normalized\nprobabilities over a large vocabulary. Second, large\nvocabularies require more training data in order to\nobserve enough instances of infrequent words which\nincreases training times. Third, a larger training set\noften allows for higher capacity models which re-\nquires more training iterations.\nIn this paper we provide an overview of popular\nstrategies to model large vocabularies for language\nmodeling. This includes the classical softmax over\nall output classes, hierarchical softmaxwhich intro-\nduces latent variables, or clusters, to simplify nor-\nmalization, target sampling which only considers a\nrandom subset of classes for normalization, noise\ncontrastive estimationwhich discriminates between\ngenuine data points and samples from a noise distri-\narXiv:1512.04906v1  [cs.CL]  15 Dec 2015\nbution, and infrequent normalization, also referred\nas self-normalization, which computes the partition\nfunction at an infrequent rate. We also extend self-\nnormalization to be a proper estimator of likelihood.\nFurthermore, we introduce differentiated softmax, a\nnovel variation of softmax which assigns more ca-\npacity to frequent words and which we show to be\nfaster and more accurate than softmax (§2).\nOur comparison assumes a reasonable budget of\none week for training models. We evaluate on three\nwell known benchmarks differing in the amount of\ntraining data and vocabulary size, that is Penn Tree-\nbank, Gigaword and the recently introduced Billion\nWord benchmark (§3).\nOur results show that conclusions drawn from\nsmall datasets do not always generalize to larger set-\ntings. For instance, hierarchical softmax is less ac-\ncurate than softmax on the small vocabulary Penn\nTreebank task but performs best on the very large\nvocabulary Billion Word benchmark, because hier-\narchical softmax is the fastest method for training\nand can perform more training updates in the same\nperiod of time. Furthermore, our results with dif-\nferentiated softmax demonstrate that assigning ca-\npacity where it has the most impact allows to train\nbetter models in our time budget (§4).\nUnlike traditional count-based models, our neural\nmodels beneﬁt less from more training data because\nthe computational complexity of training is much\nhigher, exceeding our time budget in some cases.\nFinally, our analysis shows clearly that Kenser-Ney\ncount-based language models are very competitive\non rare words, contrary to the common belief that\nneural models are better on infrequent words (§5).\n2 Modeling Large Vocabularies\nWe ﬁrst introduce our basic language model archi-\ntecture with a classical softmax and then describe\nvarious other methods including a novel variation of\nsoftmax.\n2.1 Softmax Neural Language Model\nOur feed-forward neural network implements an n-\ngram language model, i.e., it is a parametric function\nestimating the probability of the next word wt given\nn−1 previous context words, wt−1,...,w t−n+1.\nFormally, we take as input a sequence of discrete\nindexes representing the n−1 previous words and\noutput a vocabulary-sized vector of probability esti-\nmates, i.e.,\nf : {1,...,V }n−1 →[0,1]V,\nwhere V is the vocabulary size. This function re-\nsults from the composition of simple differentiable\nfunctions or layers.\nSpeciﬁcally, f composes an input mapping from\ndiscrete word indexes to continuous vectors, a suc-\ncession of linear operations followed by hyperbolic\ntangent non-linearities, plus one ﬁnal linear opera-\ntion, followed by a softmax normalization.\nThe input layer maps each context word index to\na continuous d0-dimensional vector. It relies on a\nparameter matrix W0 ∈RV×d0 to convert the input\nx= [wt−1,...,w t−n+1] ∈{1,...,V }n−1\nto n−1 vectors of dimension d0. These vectors are\nconcatenated into a single (n−1) ×d0 matrix,\nh0 = [W0\nwt−1; ... ; W0\nwt−n+1] ∈Rn−1×d0.\nThis state h0 is considered as a (n−1) ×d0 vector\nby the next layer. The subsequent states are com-\nputed through klayers of linear mappings followed\nby hyperbolic tangents, i.e.\n∀i= 1,...,k, h i = tanh(Wihi−1 + bi) ∈Rdi\nwhere Wi ∈ Rdi×di−1,b ∈ Rdi are learn-\nable weights and biases and tanh denotes the\ncomponent-wise hyperbolic tangent.\nFinally, the last layer performs a linear operation\nfollowed by a softmax normalization, i.e.,\nhk+1 = Wk+1hk + bk+1 ∈RV (1)\nand y= 1\nZ exp(hk+1) ∈[0,1]V (2)\nwhere Z =\nV∑\nj=1\nexp(hk+1\nj ).\nand exp denotes the component-wise exponential.\nThe network outputyis therefore a vocabulary-sized\nvector of probability estimates. We use the standard\ncross-entropy loss with respect to the computed log\nprobabilities\n∂log yi\n∂hk+1\nj\n= δij −yj\nwhere δij = 1if i= jand 0 otherwise The gradient\nupdate therefore increases the score of the correct\noutput hk+1\ni and decreases the score of all other out-\nputs hk+1\nj for j ̸= i.\nA downside of the classical softmax formulation\nis that it requires computation of the activations for\nall output words (see Equation 2). When group-\ning multiple input examples into a batch, Equa-\ntion 1 amounts to a large matrix-matrix product of\nthe form Wk+1Hk where Wk+1 ∈RV×dk, Hk =\n[hk\n1; ... ; hk\nl] ∈Rdk×l, where l is the number of in-\nput examples in a batch. For example, typical set-\ntings for the gigaword corpus ( §3) are a vocabulary\nof size V = 100,000, with output word embedding\nsize dk = 1024and batch size of l= 500examples.\nThis gives a very large matrix-matrix product of\n100,000 ×1024 by 1024 ×500. The rest of the net-\nwork involves matrix-matrix operations whose size\nis determined by the batch size and the layer dimen-\nsions, both are typically much smaller than the vo-\ncabulary size, ranging for hundreds to a couple of\nthousands. Therefore, the output layer dominates\nthe complexity of the entire network.\nThis computational burden is high even for\nGraphics Processing Units (GPUs). GPUs are well\nsuited for matrix-matrix operation when matrix di-\nmensions are in the thousands, but become less ef-\nﬁcient with dimensions over 10,000. The size of\nthe output matrix is therefore a bottleneck during\ntraining. Previous work suggested tackling these\nproducts by sharding them across multiple GPUs\n(Sutskever et al., 2014), which introduces additional\nengineering challenges around inter-GPU commu-\nnication. This paper focuses on orthogonal algo-\nrithmic solutions which are also relevant to parallel\ntraining.\n2.2 Hierarchical Softmax\nHierarchical Softmax (HSM) organizes the output\nvocabulary into a tree where the leaves are the words\nand the intermediate nodes are latent variables, or\nclasses (Morin and Bengio, 2005). The tree has po-\ntentially many levels and there is a unique path from\nthe root to each word. The probability of a word is\nthe product of the probabilities of the latent variables\nalong the path from the root to the leaf, including the\nprobability of the leaf. If the tree is perfectly bal-\nanced, this can reduce the complexity fromO(V) to\nO(log V).\nWe experiment with a version that follows Good-\nman (2001) and which has been used in Mikolov et\nal. (2011b). Goodman proposed a two-level tree\nwhich ﬁrst predicts the class of the next word ct and\nthen the actual word wt given context x\np(wt|x) =p(ct|x) p(wt|ct,x) (3)\nIf the number of classes is O(\n√\nV) and each class\nhas the same number of members, then we only need\nto compute O(2\n√\nV) outputs. This is a good strat-\negy in practice as it yields weight matrices for clus-\nters and words whose largest dimension is less than\n∼1,000, a setting for which GPUs are fast.\nA popular strategy clusters words based on fre-\nquency. It slices the list of words sorted by fre-\nquency into clusters that contain an equal share of\nthe total unigram probability. We pursue this strat-\negy and compare it to random class assignment and\nto clustering based on word embedding features.\nThe latter applies k-means over word embeddings\nobtained from Hellinger PCA over co-occurrence\ncounts (Lebret and Collobert, 2014). Alternative\nword representations (Brown et al., 1992; Mikolov\net al., 2013) are also relevant but an extensive study\nof word clustering techniques is beyond the scope of\nthis work.\n2.3 Differentiated Softmax\nThis section introduces a novel variation of soft-\nmax that assigns variable capacity per word in the\noutput layer. The weight matrix of the ﬁnal layer\nWk+1 ∈Rdk×V stores output embeddings of size\ndk for the V words the language model may pre-\ndict: Wk+1\n1 ; ... ; Wk+1\nV . Differentiated softmax (D-\nSoftmax) varies the dimension of the output em-\nbeddings dk across words depending on how much\nmodel capacity is deemed suitable for a given word.\nIn particular, it is meaningful to assign more param-\neters to frequent words than to rare words. By deﬁni-\ntion, frequent words occur more of ten in the training\ndata than rare words and therefore allow to ﬁt more\nparameters.\nW\nk +1\nh\nk\nd A\nd B\nd C\n| A |\n| B |\n| C |\nd A\nd B\nd C\nFigure 1: Final weight matrix Wk+1 and hidden layer\nhk for differentiated softmax for partitions A,B,C\nof the output vocabulary with embedding dimensions\ndA,dB,dC; non-shaded areas are zero.\nIn particular, we deﬁne partitions of the output vo-\ncabulary based on word frequency and the words\nin each partition share the same embedding size.\nFor example, we may partition the frequency or-\ndered set of output word ids, O = {1,...,V }, into\nAdA = {1,...,K }and BdB = {K+1,...,V }s.t.\nA∪B = O ∧ A∩B = ∅, where dA and dB are\ndifferent output embedding sizes andKis a word id.\nPartitioning results in a sparse ﬁnal weight matrix\nWk+1 which arranges the embeddings of the output\nwords in blocks, each one corresponding to a sepa-\nrate partition (Figure 1). The size of the ﬁnal hid-\nden layer hk is the sum of the embedding sizes of\nthe partitions. The ﬁnal hidden layer is effectively a\nconcatenation of separate features for each partition\nwhich are used to compute the dot product with the\ncorresponding embedding type in Wk+1. In prac-\ntice, we compute separate matrix-vector products,\nor in batched form, matrix-matrix products, for each\npartition in Wk+1 and hk.\nOverall, differentiated softmax can lead to large\nspeed-ups as well as accuracy gains since we can\ngreatly reduce the complexity of computing the out-\nput layer. Most signiﬁcantly, this strategy speeds up\nboth training and inference. This is in contrast to hi-\nerarchical softmax which is fast during training but\nrequires even more effort than softmax for comput-\ning the most likely next word.\n2.4 Target Sampling\nSampling-based methods approximate the softmax\nnormalization (Equation 2) by selecting a number of\nimpostors instead of using all outputs. This can sig-\nniﬁcantly speed-up each training iteration, depend-\ning on the size of the impostor set.\nWe follow Jean et al. (2014) who choose as\nimpostors all positive examples in a mini-batch as\nwell as a subset of the remaining words. This sub-\nset is sampled uniformly and its size is chosen by\ncross-validation. A downside of sampling is that the\n(downsampled) ﬁnal weight matrix Wk+1 (Equa-\ntion 1) keeps changing between mini-batches. This\nis computationally costly and the success of sam-\npling hinges on being to estimate a good model\nwhile keeping the number of samples small.\n2.5 Noise Contrastive Estimation\nNoise contrastive estimation (NCE) is another\nsampling-based technique (Hyv ¨arinen, 2010; Mnih\nand Teh, 2012). Contrary to target sampling, it does\nnot maximize the training data likelihood directly.\nInstead, it solves a two-class problem of distinguish-\ning genuine data from noise samples. The train-\ning algorithm samples a word wgiven the preceding\ncontext xfrom a mixture\nP(w|x) = 1\nk+ 1Ptrain(w|x) + k\nk+ 1Pnoise(w|x)\nwhere Ptrain is the empirical distribution of the train-\ning set and Pnoise is a known noise distribution\nwhich is typically a context-independent unigram\ndistribution ﬁtted on the training set. The training\nalgorithm ﬁts the model ˆP(w|x) to recover whether\na mixture sample came from the data or the noise\ndistribution, this amounts to minimizing the binary\ncross-entropy\n−y log ˆP(y= 1|w,x)−(1−y) log ˆP(y= 0|w,x)\nwhere y is a binary variable indicating whether the\ncurrent sample originates from the data ( y = 1)\nor the noise ( y = 0 ) and ˆP(y = 1 |w,x) =\nˆP(w|x)\nˆP(w|x)+kPnoise(w|x) , ˆP(y = 0|w,x) = 1− ˆP(y =\n1|w,x) are the model estimates of the correspond-\ning posteriors.\nThis formulation still involves a softmax over the\nvocabulary to compute ˆP(w|x). However, Mnih\nand Teh (2012) suggest to forego the normalization\nstep and simply consider replacing ˆP(w|x) with un-\nnormalized exponentiated scores which makes the\ncomplexity of training independent of the vocabu-\nlary size. At test time, the softmax normalization is\nreintroduced to obtain a proper distribution.\n2.6 Infrequent Normalization\nAndreas and Klein (2015) also propose to relax\nscore normalization. Their strategy (here referred\nto as WeaknormSQ) associates unnormalized likeli-\nhood maximization with a penalty term that favors\nnormalized predictions. This yields the following\nloss over the training set T\nL(2)\nα = −\n∑\n(w,x)∈T\ns(w|x) +α\n∑\n(w,x)∈T\n(log Z(x))2\nwhere s(w|x) refers to the unnormalized score\nof word w given context x and Z(x) =∑\nwexp(s(w|x)) refers to the partition function for\ncontext x. For efﬁcient training, the second term can\nbe down-sampled\nL(2)\nα,γ = −\n∑\n(w,x)\n∈train\ns(w|x) +α\nγ\n∑\n(w,x)\n∈trainγ\n(log Z(x))2\nwhere Tγ is the training set sampled at rate γ. A\nsmall rate implies computing the partition function\nonly for a small fraction of the training data.\nThis work extends this strategy to the case where\nthe log partition term is not squared (Weaknorm),\ni.e.,\nL(1)\nα,γ = −\n∑\n(w,x)\n∈train\ns(w|x) +α\nγ\n∑\n(w,x)\n∈trainγ\nlog Z(x)\nFor α = 1, this loss is an unbiased estimator of the\nnegative log-likelihood of the training data L(2)\n1 =\n−∑\n(w,x)∈train s(w|x) −log Z(x).\n2.7 Other Methods\nFast locality-sensitive hashing has been used to ap-\nproximate the dot-product between the ﬁnal hidden\nlayer activation hk and the output word embedding\n(Vijayanarasimhan et al., 2014). However, during\ntraining, there is a high overhead for re-indexing the\nembeddings and test time speed-ups virtually vanish\nas the batch size increases due to the efﬁciency of\nmatrix-matrix products.\nDataset Train Test V ocab OOV\nPTB 1M 0.08M 10k 5.8%\ngigaword 4,631M 279M 100k 5.6%\nbillionW 799M 8.1M 793k 0.3%\nTable 1: Dataset statistics. Number of tokens for train and\ntest set, vocabulary size and ratio of out-of-vocabulary\nwords in the test set.\n3 Experimental Setup\nThis section describes the data used in our experi-\nments, our evaluation methodology and our valida-\ntion procedure.\nDatasets Our experiments are performed over three\ndatasets of different sizes: Penn Treebank (PTB),\nWMT11-lm (billionW) and English Gigaword, ver-\nsion 5 (gigaword). Penn Treebank is a well-\nestablished dataset for evaluating language mod-\nels (Marcus et al., 1993). It is the smallest dataset\nwith a benchmark setting relying on 1 million to-\nkens and a vocabulary size of 10,000 (Mikolov et\nal., 2011a). The vocabulary roughly corresponds\nto words occurring at least twice in the training\nset. The WMT11-lm corpus has been recently intro-\nduced as a larger corpus to evaluate language mod-\nels and their impact on statistical machine transla-\ntion (Chelba et al., 2013). It contains close to a\nbillion tokens and a vocabulary of about 800,000\nwords, which corresponds to words with more than\n3 occurrences in the training set. 1 This dataset is\noften referred as the billion word benchmark. Gi-\ngaword (Parker et al., 2011) is the largest corpus\nwe consider with 5 billion tokens of newswire data.\nEven though it has been used for language model-\ning previously (Heaﬁeld, 2011), there is no standard\ntrain/test split or vocabulary for this set. We split\nthe data according to time: the training set covers\nthe period 1994–2009 and the test data covers 2010.\nThe vocabulary consists of the 100,000 most fre-\nquent words, which roughly corresponds to words\nwith more than 100 occurrences in the training data.\nTable 1 summarizes data set statistics.\nEvaluation Performance is evaluated in terms of\nperplexity over the test set. For PTB and billionW,\n1We use the version distributed by Tony Robinson at\nhttp://tiny.cc/1billionLM .\nwe report perplexity results on a per sentence ba-\nsis, i.e., the model does not use context words\nacross sentence boundaries and we score the end-\nof-sentence marker. This is the standard setting for\nthese benchmarks. On gigaword, we do not seg-\nment the data into sentences and the model uses con-\ntexts crossing sentence boundaries and the evalua-\ntion does not include end-of-sentence markers.\nOur baseline is an interpolated Kneser-Ney (KN)\nlanguage model and we use the KenLM toolkit\nto train 5-gram models without pruning (Heaﬁeld,\n2011). For our neural models, we train 11-gram lan-\nguage models for gigaword, billionW and a 6-gram\nlanguage model for the smaller PTB. The parame-\nters of the models are the weights Wi and the bi-\nases bi for i = 0,...,k + 1. These parameters\nare learned by maximizing the log-likelihood of the\ntraining data relying on stochastic gradient descent\n(SGD) (LeCun et al., 1998).\nValidation The hyper-parameters of the model are\nthe number of layers k and the dimension of each\nlayer di,∀i = 0,...,k . These parameters are set\nby cross-validation, i.e., the parameters which max-\nimize the likelihood over a validation set (subset\nof the training data excluded from sampling during\nSGD optimization). We also cross-validate the num-\nber of clusters and as well as the clustering tech-\nnique for hierarchical softmax, the number of fre-\nquency bands and their allocated capacity for differ-\nentiated softmax, the number of distractors for tar-\nget sampling, the noise/data ratio for NCE, as well\nas the regularization rate and strength for infrequent\nnormalization. Similarly, the SGD parameters, i.e.,\nlearning rate and mini-batch size, are also set to\nmaximize validation accuracy.\nTraining TimeWe train for 168 hours (one week)\non the large datasets (billionW, gigaword) and 24\nhours (one day) for Penn Treebank. We select the\nhyper-parameters which yield the best validation\nperplexity after the allocated time and report the per-\nplexity of the resulting model on the test set. This\ntraining time is a trade-off between being able to do\na comprehensive exploration of the various settings\nfor each method and good accuracy.\n 120 130 140 150 160 170 180 190\n 0 5 10 15 20\nPerplexityTraining time (hours)\nSoftmaxSamplingHSMD-SoftmaxWeaknormWeaknormSQNCE\nFigure 2: Penn Treebank learning curve on the validation\nset.\n4 Results\nLooking at test results (Table 2) and learning paths\non the validation sets (Figures 2, 3, and 4) we can see\na clear trend: the competitiveness of softmax dimin-\nishes with the vocabulary size. Softmax does very\nwell on the small vocabulary Penn Treebank cor-\npus, but it does very poorly on the larger vocabulary\nbillionW corpus. Faster methods such as sampling,\nhierarchical softmax, and infrequent normalization\n(Weaknorm and WeaknormSQ) are much better in\nthe large-vocabulary setting of billionW.\nD-Softmax is performing very well on all data sets\nand shows that assigning higher capacity where it\nbeneﬁts most results in better models. Target sam-\npling performs worse than softmax on gigaword but\nbetter on billionW. Hierarchical softmax performs\nvery poorly on Penn Treebank which is in stark con-\ntrast to billionW where it does very well. Noise con-\ntrastive estimation has good accuracy on billionW,\nwhere speed is essential to achieving good accuracy.\nOf all the methods, hierarchical softmax pro-\ncesses most training examples in a given time frame\n(Table 3). Our test time speed comparison assumes\nthat we would like to ﬁnd the highest scoring next\nword, instead rescoring an existing string. This\nscenario requires scoring all output words and D-\nSoftmax can process nearly twice as many tokens\nper second than the other methods whose complex-\nPTB gigaword billionW\nKN 141.2 57.1 70.2\nSoftmax 123.8 56.5 108.3\nD-Softmax 121.1 52.0 91.2\nSampling 124.2 57.6 101.0\nHSM 138.2 57.1 85.2\nNCE 143.1 78.4 104.7\nWeaknorm 124.4 56.9 98.7\nWeaknormSQ 122.1 56.1 94.9\nKN+Softmax 108.5 43.6 59.4\nKN+D-Softmax 107.0 42.0 56.3\nKN+Sampling 109.4 43.8 58.1\nKN+HSM 115.0 43.9 55.6\nKN+NCE 114.6 49.0 58.8\nKN+Weaknorm 109.2 43.8 58.1\nKN+WeaknormSQ 108.8 43.8 57.7\nTable 2: Test perplexity of individual models and inter-\npolation with Kneser-Ney.\n 50 60 70 80 90 100 110\n 0 20 40 60 80 100 120 140 160 180\nPerplexityTraining time (hours)\nSoftmaxSamplingHSMD-SoftmaxWeaknormWeaknormSQNCE\nFigure 3: Gigaword learning curve on the validation set.\nity is then similar to softmax.\n4.1 Softmax\nDespite being our baseline, softmax ranks among\nthe most accurate methods on PTB and it is sec-\nond best on gigaword after D-Softmax (with Wea-\nknormSQ performing similarly). For billionW, the\nextremely large vocabulary makes softmax train-\ning too slow to compete with faster alternatives.\nHowever, of all the methods softmax has the sim-\n 80 100 120 140 160 180\n 0 20 40 60 80 100 120 140 160 180\nPerplexityTraining time (hours)\nSoftmaxSamplingHSMD-SoftmaxWeaknormWeaknormSQNCE\nFigure 4: Billion Word learning curve on the validation\nset.\ntrain test\nSoftmax 510 510\nD-Softmax 960 960\nSampling 1,060 510\nHSM 12,650 510\nNCE 4,520 510\nWeaknorm 1,680 510\nWeaknormSQ 2,870 510\nTable 3: Training and testing speed on billionW in tokens\nper second. Most techniques are identical to softmax at\ntest time, HSM can be faster at test time if only few words\ninvolving few clusters are being scored.\nplest implementation and it has no additional hyper-\nparameters compared to other methods.\n4.2 Target Sampling\nFigure 5 shows that target sampling is most accu-\nrate when the distractor set represents a large frac-\ntion of the vocabulary, i.e. more than 30% on gi-\ngaword (billionW best setting is even higher with\n50%). Target sampling is asymptotically faster and\ntherefore performs more iterations than softmax in\nthe same time. However, it makes less progress in\nterms of perplexity reduction per iteration compared\nto softmax. Overall, it is not much better than soft-\nmax. A reason might be that the sampling procedure\nchooses distractors independently from context, or\ncurrent model performance. This does not favor\nsampling distractors the model incorrectly consid-\ners likely given the current context. These distrac-\n 50 60 70 80 90 100 110 120 0 10 20 30 40 50 60 70 80 90 100\nPerplexityDistractors per Sample (% of vocabulary)Sampling\nFigure 5: Number of Distractors versus Perplexity for\nTarget Sampling over Gigaword\ntors would yield high gradient that could make the\nmodel progress faster.\n4.3 Hierarchical Softmax\nHierarchical softmax is very efﬁcient for large vo-\ncabularies and it is the best method on billionW. On\nthe other hand, HSM is performing poorly on small\nvocabularies as seen on Penn Treebank.\nWe found that a good word clustering structure\nhelps learning: when each cluster contains words\noccurring in similar contexts, cluster likelihoods are\neasier to learn; when the cluster structure is uninfor-\nmative, cluster likelihoods converge to the uniform\ndistribution. This adversely affects accuracy since\nwords can never have higher probability than their\nclusters (cf. Equation 3).\nOur experiments group words into a two level hi-\nerarchy and compare four clustering strategies over\nbillionW and gigaword ( §2.2). Random clustering\nshufﬂes the vocabulary and splits it into equally\nsized partitions. Frequency-based clustering ﬁrst\norders words based on the number of their oc-\ncurrences and assigns words to clusters such that\neach cluster represents an equal share of frequency\ncounts (Mikolov et al., 2011b). K-means runs the\nwell-know clustering algorithm on Hellinger PCA\nword embeddings. Weighted k-means is similar but\nweights each word by its frequency.\nRandom clustering performs worst (Table 4) fol-\nlowed by frequency-based clustering but k-means\ndoes best; weighted k-means performs similarly\nthan its unweighted version. In our initial experi-\nments, pure k-means performed very poorly because\nthe most signiﬁcant cluster captured up to 40% of\nbillionW gigaword\nrandom 98.51 62,27\nfrequency-based 92.02 59.47\nk-means 85.70 57.52\nweighted k-means 85.24 57.09\nTable 4: Comparison of clustering techniques for hierar-\nchical softmax.\nthe word frequencies in the data. We resorted to ex-\nplicitly capping the frequency-budget of each clus-\nter to ∼10% which brought k-means to the perfor-\nmance of weighted k-means.\n4.4 Differentiated Softmax\nD-Softmax is the best technique on gigaword, and\nthe second best on billionW, after HSM. On PTB\nit ranks among the best techniques whose perplexi-\nties cannot be reliably distinguished. The variable-\ncapacity scheme of D-Softmax can assign large em-\nbeddings to frequent words, while keeping compu-\ntational complexity manageable through small em-\nbeddings for rare words.\nUnlike for hierarchical softmax, NCE or Wea-\nknorm, the computational advantage of D-Softmax\nis preserved at test time (Table 3). D-Softmax is the\nfastest technique at test time, while ranking among\nthe most accurate methods. This speed advantage\nis due to the low dimensional representation of rare\nwords which negatively affects the model accuracy\non these words (Table 5).\n4.5 Noise Contrastive Estimation\nFor language modeling we found NCE difﬁcult to\nuse in practice. In order to work with large neural\nnetworks and large vocabularies, we had to disso-\nciate the number of noise samples from the data to\nnoise ratio in the modeled mixture. For instance, a\ndata/noise ratio of 1/50 gives good performance in\nour experiments but estimating only50 noise sample\nposteriors per data point is wasteful given the cost of\nnetwork evaluation. Moreover, this setting does not\nallow frequent sampling of every word in a large vo-\ncabulary. Our setting considers more noise samples\nand up-weights the data sample. This allows to set\nthe data/noise ratio independently from the number\nof noise samples.\n 4\n 5\n 6\n 7\n 8\n 9\n 10\n 0.054  0.056  0.058  0.06  0.062  0.064\nEntropy\nNCE Loss\nFigure 6: Validation entropy versus NCE loss over gi-\ngaword for different experiments differing only in their\nlearning rates and initial weights.\nOverall, NCE results are better than softmax only\nfor billionW, a setting for which softmax is very\nslow due to the very large vocabulary. Why does\nNCE perform so poorly? Figure 6 shows entropy\non the validation set versus the NCE loss for several\nmodels. The results clearly show that similar NCE\nloss values can result in very different validation en-\ntropy. Although NCE might make sense for other\nmetrics, it is not among the best techniques for min-\nimizing perplexity.\n4.6 Infrequent Normalization\nInfrequent normalization (Weaknorm and Wea-\nknormSQ) performs better than softmax on billionW\nand comparably to softmax on Penn Treebank and\ngigaword (Table 2). The speedup from skipping par-\ntition function computations is substantial. For in-\nstance, WeaknormSQ on billionW evaluates the par-\ntition only on 10% of the examples. In one week,\nthe model is evaluated and updated on 868M to-\nkens (with 86.8M partition evaluations) compared to\n156M tokens for softmax.\nAlthough referred to as self-normalizing in the lit-\nerature (Andreas and Klein, 2015), the trained mod-\nels still needs to be normalized after training. The\npartition cannot be considered as a constant and\nvaries greatly between data samples. On billionW,\nthe 10th to 90th percentile range was 9.4 to 10.3\non the natural log scale, i.e., a ratio of 2.5 for Wea-\nknormSQ.\nIt is worth noting that the squared regularizer ver-\nsion of infrequent normalization (WeaknormSQ) is\nhighly sensitive to the regularization parameter. We\noften found regularization strength to be either too\nlow (collapse) or too high (blow-up) after a few days\nof training. We added an extra unit to our model in\norder to bound predictions, which yields more sta-\nble training and better generalization performance.\nWe bounded unnormalized predictions within the\nrange [−10,+10] by using x→10 tanh(x/5)). We\nalso observed that for the non-squared version of the\ntechnique (Weaknorm), a regularization strength of\n1 was the best setting. With this choice, the loss is\nan unbiased estimator of the data likelihood.\n5 Analysis\nThis section discusses model capacity, model ini-\ntialization, training set size and performance on rare\nwords.\n5.1 Model Capacity\nTraining neural language models over large corpora\nhighlights that training time, not training data, is\nthe main factor limiting performance. The learn-\ning curves on gigaword and billionW indicate that\nmost models are still making progress after one\nweek. Training time has therefore to be taken into\naccount when considering increasing capacity. Fig-\nure 7 shows validation perplexity versus the number\nof iterations for a week of training. This ﬁgure in-\ndicates that a softmax model with 1024 hidden units\nin the last layer could perform better than the 512-\nhidden unit model with a longer training horizon.\nHowever, in the allocated time, 512 hidden units\nyield the best validation performance. D-softmax\nshows that it is possible to selectively increase ca-\npacity, i.e. to allocate more hidden units to the repre-\nsentation of the most frequent words at the expense\nof rarer words. This captures most of the beneﬁt of a\nlarger softmax model while staying within a reason-\nable training budget.\n5.2 Effect of Initialization\nSeveral techniques for pre-training word embed-\ndings have been recently proposed (Mikolov et al.,\n2013; Lebret and Collobert, 2014; Pennington et al.,\n 80 100 120 140 160 180 200\n 0 50 100 150 200 250 300\nPerplexityTraining tokens (millions)\nD-Softmax 1024x50K, 512x100K, 64x640KD-Softmax 1024x50K, 256x740KSoftmax 1024Softmax 512\nFigure 7: Validation perplexity per iteration on billionW\nfor softmax and D-softmax. Softmax uses the same 512\nor 1024 units for all words. The ﬁrst D-Softmax exper-\niment uses 1024 units for the 50K most frequent words,\n512 for the next 100K, and 64 units for the rest, the sec-\nond experiment only considers two frequency bands. All\nlearning curves end after one week.\n2014). Our experiments use Hellinger PCA (Lebret\nand Collobert, 2014), motivated by its simplicity: it\ncan be computed in a few minutes and only requires\nan implementation of parallel co-occurrence count-\ning as well as fast randomized PCA. We consider\ninitializing both the input word embeddings and the\noutput matrix from PCA embeddings.\nFigure 8 shows that PCA is better than random for\ninitializing both input and output word representa-\ntions; initializing both from PCA is even better. The\nresults show that even after a week of training, the\ninitial conditions still impact the validation perplex-\nity. This trend is not speciﬁc to softmax and similar\noutcomes have been observed for other strategies.\nAfter a week of training, we observe only for HSM\nthat the random initialization of the output matrix\ncan reach performance comparable to PCA initial-\nization.\n5.3 Training Set Size\nLarge training sets and a ﬁxed training time intro-\nduce competition between slower models with more\ncapacity and observing more training data. This\ntrade-off only applies to iterative SGD optimization\nand it does not apply to classical count-based mod-\n 40 60 80 100 120 140 160 180 200\n 0 20 40 60 80 100 120 140 160 180\nPerplexityTraining time (hours)\nInput: PCA, Output: PCAInput: PCA, Output: RandomInput: Random, Output: PCAInput: Random, Output: Random\nFigure 8: Effect of random initialization and with\nHellinger PCA on gigaword for softmax.\n 55 60 65 70 75 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5\nPerplexityTraining data size (billions)SoftmaxKN\nFigure 9: Effect of training set size measured on the test\nset of gigaword for Softmax and Kneser-Ney.\nels, which visit the training set once and then solve\ntraining in closed form.\nWe compare Kneser-Ney and softmax, trained for\none week, with gigaword on differently sized sub-\nsets of the training data. For each setting we take\ncare to include all data from the smaller subsets.\nFigure 9 shows that the performance of the neural\nmodel improves very little on more than 500M to-\nkens. In order to beneﬁt from the full training set we\nwould require a much higher training budget, faster\nhardware, or parallelization.\nScaling training to large datasets can have a sig-\nniﬁcant impact on perplexity, even when data from\nthe distribution of interest is limited. As an illus-\ntration, we adapted a softmax model trained on bil-\nlionW to Penn Treebank and achieved a perplexity\nof 96 - a far better result than with any model we\n1-4K 4-20K 20-40K 40-70K 70-100K\nKneser-Ney 3.48 7.85 9.76 10.76 11.57\nSoftmax 3.46 7.87 9.76 11.09 12.39\nD-Softmax 3.35 7.79 10.13 12.22 12.69\nTarget sampling 3.51 7.62 9.51 10.81 12.06\nHSM 3.49 7.86 9.38 10.30 11.24\nNCE 3.74 8.48 10.60 12.06 13.37\nWeaknorm 3.46 7.86 9.77 11.12 12.40\nWeaknormSQ 3.46 7.79 9.67 10.98 12.32\nTable 5: Test set entropy of various word frequency ranges on gigaword.\ntrained from scratch on PTB (cf. Table 2).\n5.4 Rare Words\nHow well are neural models performing on rare\nwords? To answer this question we computed en-\ntropy across word frequency bands of the vocabu-\nlary for Kneser-Ney and neural models, that is we\nreport entropy for the 4,000 most frequent words,\nthen the next most frequent16,000 words and so on.\nTable 5 shows that Kneser-Ney is very competitive\non rare words, contrary to the common belief that\nneural models are better on infrequent words. For\nfrequent words, neural models are on par or better\nthan Kneser-Ney. This highlights that the two ap-\nproaches complement each other, as observed in our\ncombination experiments (Table 2).\nAmong the neural strategies, D-Softmax excels\non frequent words but performs poorly on rare ones.\nThis is because D-Softmax assigns more capacity to\nfrequent words at the expense of rare ones. Overall,\nhierarchical softmax is the best neural technique for\nrare words since it is very fast. Hierarchical softmax\ndoes more iterations than the other techniques and\nobserves the occurrences of every rare words several\ntimes.\n6 Conclusions\nThis paper presents the ﬁrst comprehensive analy-\nsis of strategies to train large vocabulary neural lan-\nguage models. Large vocabularies are a challenge\nfor neural networks as they need to compute the\npartition function over the entire vocabulary at each\nevaluation.\nWe compared classical softmax to hierarchi-\ncal softmax, target sampling, noise contrastive\nestimation and infrequent normalization, com-\nmonly referred to as self-normalization. Further-\nmore, we extend infrequent normalization, or self-\nnormalization, to be a proper estimator of likelihood\nand we introduce differentiated softmax, a novel\nvariant of softmax which assigns less capacity to\nrare words in order to reduce computation.\nOur results show that methods which are effective\non small vocabularies are not necessarily the best\non large vocabularies. In our setting, target sam-\npling and noise contrastive estimation failed to out-\nperform the softmax baseline. Overall, differenti-\nated softmax and hierarchical softmax are the best\nstrategies for large vocabularies. Compared to clas-\nsical Kneser-Ney models, neural models are better at\nmodeling frequent words, but they are less effective\nfor rare words. A combination of the two is there-\nfore very effective.\nFrom this paper, we conclude that there is still a\nlot to explore in training from a combination of nor-\nmalized and unnormalized objectives. We also see\nparallel training and better rare word modeling as\npromising future directions.\n7 Acknowledgments\nDo not number the acknowledgment section. Do not\ninclude this section when submitting your paper for\nreview.\nReferences\n[Andreas and Klein2015] Jacob Andreas and Dan Klein.\n2015. When and why are log-linear models self-\nnormalizing? In Proc. of NAACL.\n[Arisoy et al.2012] Ebru Arisoy, Tara N. Sainath, Brian\nKingsbury, and Bhuvana Ramabhadran. 2012. Deep\nNeural Network Language Models. In NAACL-HLT\nWorkshop on the Future of Language Modeling for\nHLT, pages 20–28, Stroudsburg, PA, USA. Associa-\ntion for Computational Linguistics.\n[Bahdanau et al.2015] Dzmitry Bahdanau, Kyunghyun\nCho, and Yoshua Bengio. 2015. Neural machine\ntranslation by jointly learning to align and translate.\nIn Proc. of ICLR. Association for Computational Lin-\nguistics, May.\n[Bengio et al.2003] Yoshua Bengio, R ´ejean Ducharme,\nPascal Vincent, and Christian Jauvin. 2003. A Neu-\nral Probabilistic Language Model. Journal of Machine\nLearning Research, 3:1137–1155.\n[Brown et al.1992] Peter F. Brown, Peter V . deSouza,\nRobert L. Mercer, Vincent J. Della Pietra, and\nJenifer C. Lai. 1992. Class-based n-gram mod-\nels of natural language. Computational Linguistics,\n18(4):467–479, Dec.\n[Chelba et al.2013] Ciprian Chelba, Tomas Mikolov,\nMike Schuster, Qi Ge, Thorsten Brants, Phillipp\nKoehn, and Tony Robinson. 2013. One billion word\nbenchmark for measuring progress in statistical lan-\nguage modeling. Technical report, Google.\n[Chopra et al.2015] Sumit Chopra, Jason Weston, and\nAlexander M. Rush. 2015. Tuning as ranking. In\nProc. of EMNLP. Association for Computational Lin-\nguistics, Sep.\n[Devlin et al.2014] Jacob Devlin, Rabih Zbib,\nZhongqiang Huang, Thomas Lamar, Richard\nSchwartz, , and John Makhoul. 2014. Fast and\nRobust Neural Network Joint Models for Statistical\nMachine Translation. In Proc. of ACL. Association\nfor Computational Linguistics, June.\n[Goodman2001] Joshua Goodman. 2001. Classes for\nFast Maximum Entropy Training. In Proc. of ICASSP.\n[Heaﬁeld2011] Kenneth Heaﬁeld. 2011. KenLM: Faster\nand Smaller Language Model Queries. In Workshop\non Statistical Machine Translation, pages 187–197.\n[Hyv¨arinen2010] Michael Gutmann Aapo Hyv ¨arinen.\n2010. Noise-contrastive estimation: A new estimation\nprinciple for unnormalized statistical models. In Proc.\nof AISTATS.\n[Jean et al.2014] S ´ebastien Jean, Kyunghyun Cho,\nRoland Memisevic, and Yoshua Bengio. 2014. On\nUsing Very Large Target V ocabulary for Neural\nMachine Translation. CoRR, abs/1412.2007.\n[Le et al.2012] Hai-Son Le, Alexandre Allauzen, and\nFranc ¸ois Yvon. 2012. Continuous Space Transla-\ntion Models with Neural Networks. In Proc. of HLT-\nNAACL, pages 39–48, Montr ´eal, Canada. Association\nfor Computational Linguistics.\n[Lebret and Collobert2014] Remi Lebret and Ronan Col-\nlobert. 2014. Word Embeddings through Hellinger\nPCA. In Proc. of EACL.\n[LeCun et al.1998] Yann LeCun, Leon Bottou, Genevieve\nOrr, and Klaus-Robert Mueller. 1998. Efﬁcient Back-\nProp. In Genevieve Orr and Klaus-Robert Muller, ed-\nitors, Neural Networks: Tricks of the trade. Springer.\n[Marcus et al.1993] Mitchell P. Marcus, Mary Ann\nMarcinkiewicz, and Beatrice Santorini. 1993. Build-\ning a Large Annotated Corpus of English: The Penn\nTreebank. Computational Linguistics, 19(2):314–330,\nJun.\n[Mikolov et al.2010] Tom ´aˇs Mikolov, Karaﬁ ´at Martin,\nLuk´aˇs Burget, Jan Cernock ´y, and Sanjeev Khudan-\npur. 2010. Recurrent Neural Network based Language\nModel. In Proc. of INTERSPEECH, pages 1045–\n1048.\n[Mikolov et al.2011a] Tomas Mikolov, Anoop Deoras,\nStefan Kombrink, Lukas Burget, and Jan Honza Cer-\nnocky. 2011a. Empirical Evaluation and Combination\nof Advanced Language Modeling Techniques. In In-\nterspeech.\n[Mikolov et al.2011b] Tom ´aˇs Mikolov, Stefan Kombrink,\nLuk´aˇs Burget, Jan Cernock´y, and Sanjeev Khudanpur.\n2011b. Extensions of Recurrent Neural Network Lan-\nguage Model. In Proc. of ICASSP, pages 5528–5531.\n[Mikolov et al.2013] Tom ´aˇs Mikolov, Kai Chen, Greg\nCorrado, and Jeffrey Dean. 2013. Efﬁcient Estima-\ntion of Word Representations in Vector Space. CoRR,\nabs/1301.3781.\n[Mnih and Hinton2010] Andriy Mnih and Geoffrey E.\nHinton. 2010. A Scalable Hierarchical Distributed\nLanguage Model. In Proc. of NIPS.\n[Mnih and Teh2012] Andriy Mnih and Yee Whye Teh.\n2012. A fast and simple algorithm for training neu-\nral probabilistic language models. In Proc. of ICML.\n[Morin and Bengio2005] Frederic Morin and Yoshua\nBengio. 2005. Hierarchical Probabilistic Neural Net-\nwork Language Model. In Proc. of AISTATS.\n[Parker et al.2011] Robert Parker, David Graff, Junbo\nKong, Ke Chen, and Kazuaki Maeda. 2011. English\nGigaword Fifth Edition. Technical report, Linguistic\nData Consortium.\n[Pennington et al.2014] Jeffrey Pennington, Richard\nSocher, and Christopher D Manning. 2014. Glove:\nGlobal vectors for word representation. In Proceed-\nings of the Empiricial Methods in Natural Language\nProcessing.\n[Schwenk et al.2012] Holger Schwenk, Anthony\nRousseau, and Mohammed Attik. 2012. Large,\nPruned or Continuous Space Language Models on a\nGPU for Statistical Machine Translation. In NAACL-\nHLT Workshop on the Future of Language Modeling\nfor HLT, pages 11–19. Association for Computational\nLinguistics.\n[Sordoni et al.2015] Alessandro Sordoni, Michel Galley,\nMichael Auli, Chris Brockett, Yangfeng Ji, Mar-\ngaret Mitchell, Jian-Yun Nie1, Jianfeng Gao, and\nBill Dolan. 2015. A Neural Network Approach to\nContext-Sensitive Generation of Conversational Re-\nsponses. In Proc. of NAACL. Association for Com-\nputational Linguistics, May.\n[Sutskever et al.2014] Ilya Sutskever, Oriol Vinyals, and\nQuoc Le. 2014. Sequence to Sequence Learning with\nNeural Networks. In Proc. of NIPS.\n[Vaswani et al.2013] Ashish Vaswani, Yinggong Zhao,\nVictoria Fossum, and David Chiang. 2013. Decod-\ning with Large-scale Neural Language Models im-\nproves Translation. In Proc. of EMNLP. Association\nfor Computational Linguistics, October.\n[Vijayanarasimhan et al.2014] Sudheendra Vijaya-\nnarasimhan, Jonathon Shlens, Rajat Monga, and Jay\nYagnik. 2014. Deep networks with large output\nspaces. CoRR, abs/1412.7479.",
  "topic": "Softmax function",
  "concepts": [
    {
      "name": "Softmax function",
      "score": 0.8819136023521423
    },
    {
      "name": "Computer science",
      "score": 0.8062984347343445
    },
    {
      "name": "Normalization (sociology)",
      "score": 0.7452155351638794
    },
    {
      "name": "Vocabulary",
      "score": 0.5954484343528748
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5591699481010437
    },
    {
      "name": "Language model",
      "score": 0.5552101135253906
    },
    {
      "name": "Artificial neural network",
      "score": 0.5079098343849182
    },
    {
      "name": "Popularity",
      "score": 0.49625855684280396
    },
    {
      "name": "Deep neural networks",
      "score": 0.46703478693962097
    },
    {
      "name": "Estimator",
      "score": 0.45586705207824707
    },
    {
      "name": "Natural language processing",
      "score": 0.44318148493766785
    },
    {
      "name": "Machine learning",
      "score": 0.43779394030570984
    },
    {
      "name": "Speech recognition",
      "score": 0.3746776580810547
    },
    {
      "name": "Linguistics",
      "score": 0.16318702697753906
    },
    {
      "name": "Statistics",
      "score": 0.09584340453147888
    },
    {
      "name": "Social psychology",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Psychology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Anthropology",
      "score": 0.0
    }
  ],
  "institutions": []
}