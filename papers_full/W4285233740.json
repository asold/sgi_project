{
  "title": "Coherence boosting: When your pretrained language model is not paying enough attention",
  "url": "https://openalex.org/W4285233740",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3012422146",
      "name": "Nikolay Malkin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096525380",
      "name": "Zhen Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2092023269",
      "name": "Nebojsa Jojic",
      "affiliations": [
        "The Ohio State University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2145755360",
    "https://openalex.org/W2946609015",
    "https://openalex.org/W3034731179",
    "https://openalex.org/W3201138096",
    "https://openalex.org/W4205857304",
    "https://openalex.org/W2988937804",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W3156235500",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2963096510",
    "https://openalex.org/W2963995027",
    "https://openalex.org/W2078861931",
    "https://openalex.org/W3035702572",
    "https://openalex.org/W3200409031",
    "https://openalex.org/W2952592807",
    "https://openalex.org/W2998704965",
    "https://openalex.org/W3109862485",
    "https://openalex.org/W2466175319",
    "https://openalex.org/W3103875432",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W4287704453",
    "https://openalex.org/W2963206148",
    "https://openalex.org/W2966746916",
    "https://openalex.org/W4287026929",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W2794325560",
    "https://openalex.org/W2028175314",
    "https://openalex.org/W2951883832",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W2998617917",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W2946659172",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W4287671158",
    "https://openalex.org/W2888482885",
    "https://openalex.org/W2996264288",
    "https://openalex.org/W3172943453",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W3099655892",
    "https://openalex.org/W3153427360",
    "https://openalex.org/W2785896739",
    "https://openalex.org/W3177450194",
    "https://openalex.org/W3026404337",
    "https://openalex.org/W2006969979",
    "https://openalex.org/W2963951265",
    "https://openalex.org/W2890894339",
    "https://openalex.org/W4311887664",
    "https://openalex.org/W3110131742",
    "https://openalex.org/W4287019748",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3035050380",
    "https://openalex.org/W2963015836",
    "https://openalex.org/W3131922516",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W3173673636",
    "https://openalex.org/W2097927681"
  ],
  "abstract": "Long-range semantic coherence remains a challenge in automatic language generation and understanding. We demonstrate that large language models have insufficiently learned the effect of distant words on next-token prediction. We present coherence boosting, an inference procedure that increases a LM’s focus on a long context. We show the benefits of coherence boosting with pretrained models by distributional analyses of generated ordinary text and dialog responses. It is also found that coherence boosting with state-of-the-art models for various zero-shot NLP tasks yields performance gains with no additional training.",
  "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 8214 - 8236\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nCoherence boosting:\nWhen your pretrained language model is not paying enough attention\nNikolay Malkin\nMila / Université de Montréal\nnikolay.malkin@mila.quebec\nZhen Wang\nOhio State University\nwang.9215@osu.edu\nNebojsa Jojic\nMicrosoft Research\njojic@microsoft.com\nAbstract\nLong-range semantic coherence remains a\nchallenge in automatic language generation\nand understanding. We demonstrate that large\nlanguage models have insufﬁciently learned\nthe effect of distant words on next-token pre-\ndiction. We present coherence boosting, an in-\nference procedure that increases a LM’s focus\non a long context. We show the beneﬁts of\ncoherence boosting with pretrained models by\ndistributional analyses of generated ordinary\ntext and dialog responses. It is also found that\ncoherence boosting with state-of-the-art mod-\nels for various zero-shot NLP tasks yields per-\nformance gains with no additional training.\n1 Introduction\nLanguage models (LMs) are commonly evaluated\nfor their ability to generate, rank, or classify co-\nherent spans of text. Long-range semantic coher-\nence is a unifying feature of modern NLP bench-\nmarks and applications, whether they are about pro-\nducing short answers to questions, ranking answer\nchoices by their consistency with world knowledge,\nor generating long responses.\nLarge nonspecialized LMs, such as GPT-2 and\n-3 (Radford et al., 2019; Brown et al., 2020), some-\ntimes fail to understand or use the semantic link\nbetween a text and its prompt or long-range context\n(Fig. 1). Samples from these LMs have an unnatu-\nrally low density of words that require many tokens\nof context to predict (§4.1), and the scores that the\nmodels give to completions of prompts indicate\nthat they are oversensitive to recent context (§5).\nWe hypothesize that these failures arise from\nmodeling choices and distribution shift. Specif-\nically, autoregressive LMs are typically ﬁt to a\nmulti-objective problem: simultaneously maximiz-\ning token likelihoods conditioned on many lengths\nof truncated context (§2.1). Yet, at generation or\nCode: github.com/zhenwang9102/coherence-boosting.\nscoring time, likelihoods are conditioned on the en-\ntire prompt or previously generated string, specif-\nically selected to be coherent or even guaranteed\nto inﬂuence the output. The two common solu-\ntions – ﬁnetuning models on one or multiple tasks\n(Khashabi et al., 2020; Sanh et al., 2022) and im-\nproving models or prompts to facilitate in-context\nlearning (Brown et al., 2020; Schick and Schütze,\n2021) – do not directly target the problem of long-\nrange coherence.\nThis paper proposes coherence boosting, a sim-\nple inference-time procedure that increases the ef-\nfect of distant words on predicted token distribu-\ntions and is applicable in both generation and rank-\ning settings. A pretrained model is viewed as anen-\nsemble of experts that produce token distributions\nconditioned on varying lengths of context. These\nexperts are log-linearly mixed to form a predictor\nthat is superior to the base model (§2).\nCoherence boosting greatly improves prediction\nof words that depend on a long context, as evi-\ndenced by state-of-the-art results on tasks specially\nmeant to assess models’ attention to distant words\n(§3). In generation of generic text and dialog re-\nsponses, we show that coherence boosting brings\nthe frequency of occurrence of such words close\nto that seen in natural text (§4). Beyond genera-\ntion, we study diverse multiple-choice tasks (§5),\nin which examples are known to be highly coher-\nent. Coherence boosting does not modify the base\nmodel and depends on a single parameter than can\nbe estimated in one pass through a validation set,\nyet is a competitive adaptation algorithm.\n1.1 Background and related work\nBalance between satisfaction of short-range sta-\ntistical constraints and maintenance of long-range\nstructure was a central question of language gen-\neration long before neural language modeling. To\ncompensate for the sparsity of the learning sig-\nnal for long-range inﬂuences, =-gram models and\n8214\nA:I’m Natasha. I study neural language models and dialog systems. Are you an AI researcher too?\nB:No, though I do like chatting with bots and laughing at their mistakes. But what was your name again?\nA:Oh, you forgot already? My name isw\n?full =5(w |full) 1. Alex (1.9%)2. Natasha(1.7%)3. also (1.5%)\n?short =5(w |short) 1. : (3.4%)2. the (1.9%)3. in (1.2%) . . .3358.Natasha(0.0042%)\n?1.5\nfull?−0.5\nshort 1. Natasha(20.5%)2. Alex (2.2%)3. Nat (2.1%)\nBalladmetreis “lessregular and more conversational” than commonw\n?full =5(w |full) 1. sense (9.0%)2. in (2.0%)3. . (1.9%) . . . 13. metre(0.6%)\n?short =5(w |short) 1. sense (7.8%)2. English (3.5%)3. . (3.2%) . . .14103.metre(0.00014%)\n?1.5\nfull?−0.5\nshort 1. metre(16.2%)2. sense (4.0%)3. meter (2.5%)\nIsleyBrewingCompany:GoingMintal – a minty milk chocolatew\n?full =5(w |full) 1. bar (4.8%)2. drink (3.7%)3. with (3.5%) . . .13. stout(2.7%)\n?short =5(w |short) 1. bar (6.9%)2. that (5.7%)3. , (4.4%) . . . 60. stout(0.23%)\n?1.5\nfull?−0.5\nshort 1. stout(7.4%)2. ale (5.6%)3. bar (3.1%)\nOthertimesanxietyis notas easyto see, but can still be just asw\n?full =5(w |full) 1. important (5.6%)2. bad (4.6%)3. debilitating(4.3%)\n?short =5(w |short) 1. effective (16.2%)2. good (7.4%)3. useful (3.9%) . . .294.debilitating(0.035%)\n?1.5\nfull?−0.5\nshort 1. debilitating(17.6%)2. real (6.0%)3. severe (5.8%)\nFigure 1: Next-token probabilities given by LMs (DialoGPT and GPT-2) conditioned on a long context and on a\npartial context. The top words in both distributions are incorrect, but a log-linear mixture of the distributions makes\nthe correct word most likely. Sampling from such a mixture at each generation step (coherence boosting) improves\nthe quality of output text (§4). (Dialog example constructed by the authors; other examples from OpenWebText.)\nearly neural language models used ‘backing-off’\nschemes that interpolate between predictors with\ndifferent context lengths (Chen and Goodman,\n1996; Bengio et al., 2003). Neural language model-\ning brought a need for recurrent units with better\nnumerical properties for propagating information\nover long distances (Hochreiter and Schmidhuber,\n1997; Cho et al., 2014) and eventually saw the rein-\ntroduction of alignment variables (Brown et al.,\n1993) into generation in the form of attention (Bah-\ndanau et al., 2015; Vaswani et al., 2017). Attention\nis at the core of Transformer LMs, including GPT.\nLanguage models are being trained on and\nadapted to ever-longer input sequences (Beltagy\net al., 2020; Zaheer et al., 2020; Roy et al., 2021;\nPress et al., 2022), but they remain undersensi-\ntive to distant content or syntax (Khandelwal et al.,\n2018; Sun et al., 2021) and are easily fooled by re-\ncency bias in few-shot prompts (Zhao et al., 2021)\nor multi-turn conversations (Sankar et al., 2019).\nRecent work has continued to study inference-\ntime procedures that prevent text sampled from\nLMs from degenerating into nonsense. Most of\nthese procedures, such as tempered sampling and\ntop-:/top-?truncation (Fan et al., 2018; Holtzman\net al., 2019), independently modify the output dis-\ntribution at each generation step to decrease its\nentropy and diminish its low-likelihood tail. Holtz-\nman et al. (2019) and Meister and Cotterell (2021)\nfound that such local modiﬁcations increase the\nquality of long generated sequences; we adopt and\nextend their methodology in §4.1.\nFor dialog systems, Li et al. (2016) propose a\ndecoding scheme that maximizes a mutual informa-\ntion criterion, which explicitly optimizes for depen-\ndence of generated text on prompts – a special case\nof coherence boosting. In multiple-choice tasks,\nwhere a model must choose one of several given\ncompletions of a prompt, Brown et al. (2020) ob-\nserve that selecting the completion that maximizes\nthe conditional likelihood of the completion fol-\nlowing the prompt often favors completions having\nhigh unconditional likelihood (likelihood follow-\ning an empty or dummy prompt) and, for some\ntasks, chooses to divide the scores of candidate an-\nswers by their unconditional likelihoods. This is\nalso a special case of coherence boosting.\nSuch scoring modiﬁcations are more thoroughly\nstudied by Zhao et al. (2021); Holtzman et al.\n(2021). The latter attributes the problem to ‘sur-\nface form competition’: there are many variants of\nthe correct completion that together may capture a\n8215\nlarge part of probability mass, but the form of the\ngiven answer choice alone is not the most likely.\nHowever, we show that other causes are at play:\nsurface form competition is impossible when the\ncompletion is known to be a single token and the\nrange of choices is the whole vocabulary (§3), and\nit is not applicable to open-ended generation (§4).\n2 Coherence boosting\nIn this section, 5 is an autoregressive LM over a\nvocabulary + with learnable parameters \\, taking\nas input a variable number of tokens (up to a maxi-\nmum context length \") and producing a vector of\nnext-token likelihoods:\n5(F1,...,F =; \\)∈ Δ(+), F 1,...,F = ∈+,\nwhere Δ(+)is the probability simplex over +. We\nwill write the F-th component of this output vector\nas a conditional likelihood, 5(F |F1,...,F =; \\).\nWe denote by 5: the model evaluated on only\nthe last :input tokens, ignoring earlier tokens:\n5: (F1,...,F =; \\):= 5(F=−:+1,...,F =; \\).\nCoherence boosting for next-token prediction.\nCoherence boosting for a model 5 selects real-\nvalued weights \" = (U1,UR,...,U \" )and pro-\nduces a new language model 5\", deﬁned by\n5\" (F1,...,F =; \\)\n:=softmax\n( \"Õ\n:=1\nU: log 5: (F1,...,F =; \\)\n)\n, (1)\nwhere log is taken element-wise, or, equivalently,\n5\" (F|F1,...,F =; \\)∝\n\"Ö\n:=1\n5: (F|F1,...,F =; \\)U: .\nThis is a weighted product-of-experts model, where\nthe ‘experts’ are copies of the base model5 evalu-\nated on different context lengths.\nBecause evaluating 5 is expensive, we use sparse\nweights \", as the expression (1) depends only on\nthose 5: for which U: ≠0. In Fig. 1 and in the ex-\nperiments, we allow \" to have only two nonzero en-\ntries: when computing likelihoods of words follow-\ning a sequence of length =, we consider weighted\nproducts of 5max := 5= (the full context) and an 5:\nwith : ≤=(a short context, either of ﬁxed length\nor decided by prompt structure as in §4.2).\nAs its name suggests, the form of coherence\nboosting in (1) bears a resemblance to log-linear\nboosting for multiclass classiﬁcation (Friedman\net al., 2000). However, our weak classiﬁers are\npretrained and share all of their parameters, not\nobtained by an iterative procedure of training on\nreweighted data, and we permit negative weights.1\nCoherence boosting for answer selection. In\nmultiple-choice problems, a LM must choose the\nbest answer following a context, which consists of\na premise or passage followed by a shorterpremise-\nfree context (either a short phrase, such as “An-\nswer:”, that incites the LM to generate an answer\nin the right format, or a hypothesis that depends on\nthe premise). The full context is the concatenation\nof the premise and the premise-free context (§E).\nBy the autoregressive factorization, the model\n5 assigns conditional likelihoods to sequences of\ntokens following context. A typical model for an-\nswer selection ranks the candidate answers 08 (se-\nquences of tokens) by 5(08 |full context; \\)and\noutputs the highest-ranked 08. Coherence boosting\nchooses a parameter Uand ranks the choices by:\nlog 5(08 |full context; \\)+\n+Ulog 5(08 |premise-free context; \\). (2)\nThis is a log-linear combination of two models: 5\nevaluated with full context and with a partial con-\ntext. When U=0, ranking by (2) is equivalent to\nranking by the base model. When U = −1, it is\nequivalent to dividing the base model’s score by\nthe score of each answer conditioned on the prompt\n(short context), and thus to maximizing pointwise\nmutual information between the premise and the an-\nswer conditional on the premise-free context. Un-\nlike Brown et al. (2020); Holtzman et al. (2021),\nour formulation allows the premise-free context to\ninclude information speciﬁc to the example, not\nonly a domain-speciﬁc dummy prompt.\nWe expect coherence boosting to correct for an\noversensitivity to the premise-free context, and thus\nthe optimal Uwill typically be negative (see §5).\n2.1 Why should boosting models be better\nthan full-length predictors?\nMulti-objective training. As we will now see,\nthe training of the model 5 simultaneously ﬁts all of\n1As for the ﬁrst half of the term ‘coherence boosting’,\nHowcroft et al. (2020); Belz et al. (2020) found that very\nincoherent deﬁnitions of the word ‘coherence’ abound in the\nnatural language evaluation literature. The reader is asked\nto forgive us for the loose deﬁnition of ‘long-range semantic\ncoherence’ adopted in this paper.\n8216\nthe predictors 5:, which share parameters \\. Each\ntraining iteration samples a sequence (or batch of\nsequences) of a chosen maximum length \" +1\nfrom the data distribution Dand minimizes the\naverage negative log-likelihood (NLL) ofall words\nfollowing the parts of the sequence that precede\nthem: the optimization criterion is:\nEF1...F\"+1∼D\n1\n\"\n\"Õ\n:=1\n−log 5(F:+1|F1,...,F :; \\).\nIf Dis uniform over all length-( \" +1) subse-\nquences of a training corpus, any given word is\nequally to likely to appear in all positions within a\nsampled sequence2, and the criterion is equal to\n\"Õ\n:=1\n1\n\" E [−log 5: (F\"+1|F1,...,F \" ; \\)]                                                                                  \nL: (\\)\n, (3)\nThis is a uniform scalarization of an \"-task prob-\nlem: the :-th objective L: (\\)is the expected NLL\nof a word in the corpus following :context words.\nThis situation is different from that seen at\ngeneration time. If the text generated so far is\nF1FR ...F =, the distribution from which the next\nword F=+1 is sampled is 5=(F1,...,F =; \\)– only\nthe ensemble member using full context is used.\nHowever, if the stringF1 ...F =F=+1 had been seen\nin training, 5 would have been trained to predict\nF=+1 given all partial contexts, with equal weight\ngiven to all prediction losses. Thus, 5 is trained to\nmake predictions on data it never sees in evalua-\ntion, and may be prevented from optimally learning\nto use long context: parameters that locally opti-\nmize (3) are locally Pareto-optimal for the set of\nprediction losses L1,..., L\" , but not necessarily\noptimal for any individual L:. An ensemble of the\n5: (: ≤=) may be a better predictor than 5= alone.\n(See §A for further analysis of when this occurs.)\nUndertraining. The parameters \\are shared by\nthe predictors 5:, and modeling power must be\nspread among the losses L: (\\). The short-context\npredictors are easier to ﬁt, while sequences in\nwhich long context affects the prediction are rare.\nWe expect sensitivity to long context, and precision\nin modeling its effect, to be especially diminished\nif the model is undertrained.\n2Many authors leave unspeciﬁed the way in which training\nbatches are formed from a corpus of input documents. Here\nwe assume that all training documents are concatenated into\none (very long) document separated by end-of-text tokens and\nignore minute effects near the start and end of this document.\n2\n 1\n 0 1 2 3\n0.0\n0.2\n0.4\n0.6\n0.8Accuracy\nLAMBADA(k = 10)\n125M\n350M\n760M\n1.6B\n2.7B\n6.7B\n13B\n175B\nFigure 2: Model comparison on LAMBADA with : =\n10 and varying U:. The red line (U=0) is the base LM\n5max. (The different right tails of GPT-3 models are due\nto top-100 truncation of logits returned by the API.)\nDistribution shift. While the training procedure\ncauses a bias against the inﬂuence of longer con-\ntexts on generation, we see the opposite bias in\ndownstream tasks (question answering, natural lan-\nguage inference, adversarial probes for common\nsense): Many modern NLP benchmarks try to chal-\nlenge models to use long context (§3, §5).\n3 Experiments: LAMBADA\nThe LAMBADA dataset (Paperno et al., 2016) tests\nLMs’ understanding of long-range dependencies\nby measuring the prediction of the ﬁnal words in\npassages of several sentences. The task explicitly\nrequires reasoning over a broad context: humans\ncan reliably guess the last word when given a whole\npassage, but not when given only the last sentence.\nWe perform experiments with the GPT family of\nmodels, closely replicating the evaluation setting\nof Radford et al. (2019).3 We predict the ﬁnal word\nas the top-ranked token under the boosted model\n5max 5U:\n: , where 5max is the model taking the full\navailable context and :,U: are the chosen length\nand coefﬁcient of the short context. To choose :\nand U:, we do a grid search on the validation set\nand apply the best values to the testing set.\nResults. Table 1 shows the accuracies and opti-\nmal parameter values :∗,U∗\n:. Coherence boosting\nvastly reduces prediction error for all models. In\nparticular, the boosted GPT-2 Small performs better\nthan the original GPT-3 2.7B. The boosted GPT-3\n175B achieves a new state of the art.\n3Certain details are omitted by Radford et al. (2019).\nBased on https://github.com/openai/gpt-2/\nissues/131, we nearly match baseline accuracy by pre-\ndicting the last subword token, rather than the last word.\n8217\nGPT-2 GPT-3\n125M 350M 760M 1.6B 2.7B 6.7B 13B 175B\n5max 47.66 57.29 61.23 64.25 62.39 71.40 76.58 81.51\nCB (U: =U∗\n:) 66.70 73.53 76.54 77.53 77.00 81.84 86.36 88.61\nU∗\n: −0.6 −0.5 −0.5 −0.5 −0.3 −0.3 −0.3 −0.2\n:∗ 10 11 10 9 9 10 3 3\nTable 1: Accuracy (%) and optimal boosting parameters on LAMBADA: 5max is the full-context model without\nboosting; CB is our model with the optimal boosting parameters (last two rows).\nOther than the impressive performance gain, we\nhighlight two observations. (1) The optimal U: is\nalways negative, indicating that the optimal mixture\nof models penalizes the inﬂuence of short-range\ncontext relative to long-range context. (2) With in-\ncreasing model size, the optimal U: and :become\ncloser to 0. This means that bigger models capture\nlong-range coherence better than small models, as\nthey have less need to penalize the effect of short\ncontext. (Fig. 2 shows the accuracy curves for all\nmodels by sweeping U: with a ﬁxed :. The peak\nclearly moves to the left as model size grows.)\n4 Experiments: Language generation\n4.1 Generic text\nThe experiment in this section extends that of Holtz-\nman et al. (2019). A selection of 5000 articles from\nWebText (Radford et al., 2019) is taken as a ref-\nerence corpus of human-written text. A language\nmodel (for us, GPT-2 Large) is prompted to gen-\nerate text conditioned only on the ﬁrst sentence of\neach of these articles, up to a maximum of 200\ntokens, yielding 5000 machine-generated texts.\nThe human-written and machine-generated texts\nare compared by four automatic metrics: perplex-\nity under the base LM, self-BLEU-4 (Zhu et al.\n(2018); the mean BLEU-4 score of a generated\ntext with respect to all other generated texts as\nreferences), Zipf coefﬁcient (the linear regression\ncoefﬁcient between log-rank and log-frequency of\ngenerated tokens) and repetition (the fraction of\ngenerated texts that end in a repeating sequence of\ntokens). It is desirable for a model and inference\nprocedure to produce text that is as close as possi-\nble in these metrics to the human-written reference.\nTo measure long-range semantic coherence in\nthe generated text, we deﬁne three new metrics:\nLong-range repetition (LR=): For a whole num-\nber =and document \u0019, let ((\u0019)be the number of\ndistinct tokens in \u0019, and let '=(\u0019)be the number\nof distinct tokens for which the distance between\ntheir ﬁrst and last occurrence in \u0019is at least =po-\nsitions. The long-range repetition score lr= of a\ncorpus {\u00191,...,\u0019 5000}is a macro-average:\nlr= :=\nÍ5000\n8=1 '=(\u00198)\nÍ5000\n8=1 ((\u00198)\n.\nThis simple measure of lexical coherence favors\nrepetition of words long after they are ﬁrst used, but\ngives lower weight to documents that degenerate\ninto repetition of a short span.\nLong-dependent token frequency (LTF): A\nlong-dependent token is one to which the base LM\nassigns a likelihood of at least 20% given its full\ncontext, but a likelihood of less than 5% given only\nthe 20 tokens of context preceding it. We compute\nthe frequency of long-dependent tokens among all\ngenerated tokens.\nLong-short likelihood difference (X): The mean\ndifference in likelihoods assigned to tokens by the\nbase LM conditioned on full context and condi-\ntioned on 20 tokens of context.\nAlthough some choices of constants are needed\nto deﬁne LTF and X, we intend them to be intuitive\nsummaries of long-range coherence in the absence\nof established metrics. In particular, 20 tokens\nis close to the length of one sentence in typical\nEnglish text.\nWe sample 5000 document completions from\nGPT-2 Large following sampling procedures with\na range of boosting schemes. We consider models\nof the form 5U:\n: 51−U:\nmax , for : ∈{8,16,3R,64}and\nU: ∈{−0.4,−0.R,−0.1,−0.05,−0.0R5,0}. (Such\na parametrization of boosting parameters was cho-\nsen to ensure that when the context has length less\nthan :– or the distant context has very little effect\non the next word – the boosted model becomes\nequivalent to the untempered 5max.) Top-?trunca-\ntion with ?=0.95 is applied to all models.\n8218\nfrom Holtzman et al. (2019) lex coherence long-dep tokens\nInference method ppl BLEU-4 Zipf rep % LR 50 % LR 100 % X% LTF %\nSampling 23.53 0.28 0.93 0.22 12.92 7.71 4.87 3.28\nSampling ()=0.9) 10.60 0.35 0.96 0.66 16.36 10.01 6.54 4.15\nNucleus (?=0.95) 13.48 0.32 0.95 0.46 15.06 9.11 5.65 3.62\n+ boost (:=3R, U: =−0.05) 12.81 0.31 0.94 0.34 15.54 9.42 6.16 3.98\n+ boost (:=64, U: =−0.1) 12.93 0.32 0.95 0.46 15.75 9.67 6.10 3.95\n+ self-tune (§B) 10.16 0.33 0.95 0.64 16.19 9.85 6.59 4.16\nHuman 13.19 0.31 0.93 0.28 15.95 9.51 6.54 4.03\nTable 2: Distributional metrics of WebText completions. The last four columns are measures of long-range coher-\nence (§4.1). (Nearest-to-human values in bold, boosting models better than top-?sampling alone in italics.)\nFigure 3: Effect of :and U: on metrics from Table 2. The horizontal line marks the score of the human reference.\nResults. Metrics of two of the best models, with\n: = 3R,U: = −0.05 and : = 64,U: = −0.1, are\nshown in Table 2. In particular, the latter model\ngenerates text that is closer to the human refer-\nence, or equally close, to the pure top-?sampling\n(U: =0) baseline in all metrics, with the greatest\nimprovement seen in the coherence measures.\nFig. 3 shows the dependence of selected metrics\non :and U:. Coherence boosting brings all metrics\ncloser to those of human text. As :increases, the\noptimal U: grows in magnitude. This is expected:\nthe predictive effect of tokens more than:positions\naway decreases with :( 5: approaches 5max).\nWe also note that a simple sampling with tem-\nperature 0.9 performs better than top-?sampling in\nmost of the coherence metrics. This suggests that\nthe improvements accomplished by top-?trunca-\ntion come at the cost of introducing a bias towards\ntokens that are predictable from a short context.\nCoherence boosting corrects this bias without sac-\nriﬁcing the gains in other measures.\nAn example of human, top- ?, and coherence\nboosting outputs is shown in Table D.1.\n4.2 Dialog systems\nThis experiment is based on the Dialog System\nTechnology Challenge 7 (DSTC7) (Galley et al.,\n2019), which benchmarks generation of dialog re-\nsponses conditioned on one or more turns of conver-\nsation context. As a base model, we use DialoGPT\n(Zhang et al., 2020c), a GPT-2 Small variant that\ndemonstrated strong results on this task.\nDialog systems’ responses to the 2208 conver-\nsation prompts4 are scored against human-written\nreference responses (ﬁve for each example). Fol-\nlowing Zhang et al. (2020c), we use the =-gram\noverlap metrics NIST (Doddington, 2002), BLEU\n(Papineni et al., 2002), and METEOR (Lavie and\nAgarwal, 2007), as well as two intrinsic measures\nof =-gram diversity from Li et al. (2016); Zhang\net al. (2018): Distinct-=and Entropy-=. It is de-\nsirable for a dialog system to reach scores close to\nthose of the human responses in all metrics.\nIn addition to the decoding algorithms consid-\nered by (Zhang et al., 2020c) – beam search and\ngreedy decoding – we consider greedy decoding\nwith a coherence boosting model. As long and\nshort predictors, we use DialoGPT conditioned\non the full conversation context and on only the\n(context-free) response generated so far. That is,\nif the conversation context is ( and the text gen-\nerated so far is F1 ...F :, then F:+1 is predicted\nusing the model 5max 5U\n:+1, evaluated on the string\n( ⟨sep⟩F1 ...F :, where ⟨sep⟩is the turn separa-\n4The DSTC7 evaluation data, scraped from Reddit, is\nundisclosed; we reacquire it using ofﬁcially released code.\n8219\nNIST BLEU diversity metrics\nInference method N-2 N-4 B-2 B-4 METEOR Ent-4 Dist-1 Dist-2 avg len\nBeam (1=10) 0.02 0.02 12.81 3.23 5.35 6.06 14.03 34.59 5.81\nGreedy 1.62 1.63 9.92 1.72 6.78 6.45 6.19 17.56 13.30\n+ boost (U=−0.3) 0.72 0.73 13.82 3.53 6.91 8.54 16.81 49.35 9.75\n+ boost (U=−0.7) 1.78 1.79 6.33 0.94 5.55 9.78 28.00 72.46 16.63\nHuman 2.63 2.65 12.36 3.13 8.31 10.44 16.65 67.01 18.73\nTable 3: Metrics of DialoGPT responses on DSTC7. Nearest-to-human values in each column are bolded.\ntor token. We consider U∈{0,−0.1,..., −0.8}.\nResults. Table 3 shows the metrics of the boost-\ning models that reach the peak average NIST and\nBLEU scores (U=−0.3 and U=−0.7). Increasing\nthe magnitude of Uleads to responses that are more\nrelevant to the prompt (higher BLEU and NIST)\nand more diverse than those from greedy decoding.\nAs −Ugrows large, the boosting model favors cre-\native responses that are relevant to the prompt (high\nNIST), but simple responses that are common in\nthe reference data become unlikely (low BLEU).5\nWe observed that the responses with U=−0.7,\ndespite the superior metrics, are more likely to\nbe ungrammatical and innovate words in an effort\nto use tokens relevant to the prompt. In practice,\nimproving dialog systems with coherence boosting\nmay require techniques to prevent these side effects,\nsuch as repetition penalties or relaxation of greedy\ndecoding to low-temperature sampling.\nFinally, we note that the learning of DialoGPT\nwas initialized with a pretrained GPT-2 and uses\nGPT-2’s end-of-text token as the turn separator.\nThis choice may reduce DialoGPT’s attention to\npast turns, as tokens preceding the end-of-text to-\nken are never informative in GPT-2’s training data.\n5 Experiments: Language understanding\nWe evaluate coherence boosting on zero-shot lan-\nguage understanding and inference tasks, where\nexamples are expected to be highly coherent.\nWe study 15 datasets in 5 categories of tasks.\n(1) Cloze tasks: StoryCloze (Mostafazadeh et al.,\n2016), HellaSwag (Zellers et al., 2019), and\nCOPA (Roemmele et al., 2011). (2) Question an-\nswering: CommonsenseQA (CsQA) (Talmor et al.,\n2019), OpenBookQA (OBQA) (Mihaylov et al.,\n5Galley et al. (2019) argue that NIST and diversity metrics\nare more informative measures than BLEU for multi-reference\nscoring, since BLEU favors systems that often produce re-\nsponses with little relation to the prompt (e.g., “I don’t know”).\n2018), ARC Easy / Challenge (ARC-E/C) (Clark\net al., 2018), and PIQA (Bisk et al., 2020). (3)\nText classiﬁcation: SST-2/5 (Socher et al., 2013),\nTREC (V oorhees and Tice, 2000),AGNews (Zhang\net al., 2015). (4) Natural language inference :\nRTE (Dagan et al., 2005), CB (De Marneffe et al.,\n2019), and BoolQ (Clark et al., 2019). (5) Fact\nknowledge retrieval: LAMA (Petroni et al., 2019).\nAll tasks except LAMA are formulated as\nmultiple-choice problems. We convert text clas-\nsiﬁcation and inference tasks to multiple-choice\ntasks by choosing meaningful answer words, e.g.,\n“True”/“False”. The prediction is made by selecting\nthe choice with the highest LM likelihood.\nFor in-context learning of GPT models, prompt\nformats greatly impact performance. We follow\nprevious work (Brown et al., 2020; Zhao et al.,\n2021; Holtzman et al., 2021) to create natural\nprompts to enlarge the effectiveness of in-context\nlearning, but we do not aim to optimize the full and\ncontext-free prompt format: our goal is to evaluate\ncoherence boosting models with a ﬁxed prompt.\nThe prompt formats we use are listed in Table E.1.\nAs described in §2, within each prompt we identify\na premise-free context, which is used as the context\nfor the short-range model in coherence boosting.\nFor each dataset, we pick the optimal valueU∗of\nthe parameter Uon the validation set and report the\naccuracy on testing set. (If no testing set is publicly\navailable, we choose Uon a subset of the training\nset and report the ﬁnal number on the validation\nset.) Across all experiments, we do not put any\nfew-shot examples in the prompt.\nFor the knowledge retrieval task, we follow Zhao\net al. (2021)’s data split of LAMA and evaluate\nGPT models on facts whose missing answers are at\nthe end of the sentence (to ﬁt the nature of autore-\ngressive language models). We limit the prompt\nlength to be larger than 5 tokens and rerun the\nmodel from Zhao et al. (2021) on the new data.\n8220\nGPT-2 Small (125M) GPT-2 XL (1.6B) GPT-3 175B\n5max U=−1 U=U∗ U∗ 5max U=−1 U=U∗ U∗ 5max U=−1 U=U∗ U∗\nStoryCloze 59.91 64.78 64.24 −1.02 67.56 75.09 76.75 −0.69 79.16 82.90 86.85 −0.64\nHellaSwag 28.92 30.99 31.84 −0.90 40.00 42.60 47.66 −0.78 59.18 62.66 72.35 −0.76\nCOPA 62.00 56.00 64.00 −0.69 73.00 70.00 77.00 −0.44 93.00 87.00 94.00 −0.52\nCsQA 29.48 42.26 43.16 −0.81 37.84 50.45 52.91 −0.75 61.10 67.98 70.43 −0.68\nOBQA 11.20 30.60 40.80 −1.62 15.60 38.40 47.00 −1.88 28.00 52.20 52.60 −1.09\nARC-E 43.81 42.09 46.00 −0.34 58.29 51.43 60.31 −0.36 76.22 69.19 78.32 −0.44\nARC-C 19.03 26.11 29.10 −4.19 25.00 33.53 34.39 −1.14 43.94 50.60 49.23 −1.08\nPIQA 62.89 57.45 63.44 −0.61 70.84 60.45 71.49 −0.43 79.27 66.32 78.94 −0.60\nSST2 65.68 74.74 82.32 −2.22 86.38 84.51 86.93 −0.09 86.16 88.14 89.84 −0.54\nSST5 25.93 30.90 30.90 −1.20 28.69 38.73 36.92 −1.69 31.22 34.75 38.51 −1.39\nAGNews 58.55 60.78 62.20 −0.62 67.17 67.43 68.26 −0.40 71.66 71.74 71.75 0.16\nTREC 23.40 29.60 32.20 −0.80 23.40 27.40 40.00 −0.79 52.40 47.00 56.00 −0.56\nBoolQ 49.36 58.07 62.14 −3.04 62.14 63.46 63.21 −0.64 71.56 73.70 72.69 −0.39\nRTE 51.26 49.82 53.79 −0.30 49.10 48.74 49.10 0.90 55.96 57.40 60.29 −0.60\nCB 12.50 23.21 48.21 −2.40 30.36 51.79 66.07 −1.90 5.36 25.00 28.57 −1.91\nAverage 40.26 45.16 50.29 −1.39 49.02 53.60 58.53 −0.74 59.61 62.44 66.69 −0.73\nTable 4: Testing accuracy (%) of three representative GPT models on multiple-choice tasks. The ﬁrst column for\neach model is the full-context model, the second is our model only whenU=−1 (a baseline), and the third column\nis our model with the optimal Uchosen on a validation set. The fourth column shows this optimal value of U.\n2\n 1\n 0 1 2 3 4 5\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85Accuracy\nStory Cloze\n125M\n350M\n760M\n1.6B\n2.7B\n6.7B\n13B\n175B\nFigure 4: Model comparison for the StoryCloze task.\nThe red line U = 0 indicates the base model, and the\nblue line U = −1 is an unconditional normalization.\nSee Figs. F.1 and F.2 for plots for other tasks, and note\nthat they do not all have the same shape.\nResults: Multiple-choice tasks. Results of\nthree representative base models on all multiple-\nchoice tasks are presented in Table 4. (Results for\nall models are in Tables F.1 and F.2.) We compare\nour best model with two baselines, U = 0 ( 5max)\nand U = −1. The former one is the original full-\ncontext model, while the latter is, for most tasks,\na form of unconditional probability normalization\nas performed by Brown et al. (2020); Holtzman\net al. (2021). We also compare our best model with\nother inference methods (Holtzman et al., 2021;\nMin et al., 2021) in Tables F.3 and F.4.\nBy comparing the third column with the ﬁrst\ntwo columns within each model in Table 4, we\ncan see that our method with the selected Ugen-\nerally improves the accuracy on all tasks. Some\nof the improvements are dramatic, where boosted\nGPT-2 Small outperforms GPT-2 XL’s base model\n(e.g., CsQA, OBQA, ARC-C) and is even compa-\nrable with GPT-3 175B’s base model (e.g., SST-2,\nSST-5, RTE). We make similar conclusions when\ncomparing coherence boosting with other inference\nmethods in Tables F.3 and F.4.\nWe observe that the optimal Udepends on tasks\nand models (fourth column within each model),\nwhich means that Ucannot be heuristically set to\n0 or −1 as in past work. This ﬁnding suggests\nthe necessity of searching for an optimal U. We\nvisualize the accuracy curve by varying U in the\ntesting set of all datasets. We show the curve for\nStoryCloze in Fig. 4 and present similar ﬁgures for\nall tasks in Figs. F.1 and F.2.\nConsistent with the results on LAMBADA (§3),\nthe optimal Uis usually negative, and its absolute\nvalue tends to decrease with the model size. We\nselected the optimal U by the validation set, but\nfuture work may explore automatic and adaptive\nmethods for setting this parameter. Notice that all\nexperiments required only a single pass through\nthe data to compute answer likelihoods conditioned\n8221\nGPT-2 GPT-3\n125M 350M 760M 1.6B 2.7B 6.7B 13B 175B\n5max 8.48 14.78 13.88 14.29 17.33 19.42 22.06 26.76\nZhao et al. (2021) 17.45 22.87 23.90 23.97 26.30 30.57 31.96 34.78\nCB (U: =U∗\n:) 19.85 22.87 25.74 25.43 28.75 32.25 35.02 37.57\nU∗\n: −0.5 −0.5 −0.5 −0.5 −0.5 −0.5 −0.5 −0.4\n:∗ 1 2 3 3 1 1 1 2\nTable 5: Accuracies (%) of GPT models on LAMA.\non full and premise-free contexts – no iterative\ngradient-based ﬁnetuning was applied.\nResults: Knowledge retrieval. Unlike LAM-\nBADA, where long contexts are required for infer-\nring the last word, LAMA contains much shorter\nsentences for knowledge facts, i.e., (subject, re-\nlation, object). A recent study (Cao et al., 2021)\nshows that the prediction is biased by the relation in\nthe short context, i.e., the answer to a prompt (e.g.,\n“Dante was born in ___”) can be induced by the\nrelation (“was born in”) without the subject. Co-\nherence boosting mitigates the inﬂuence of those\nshort contexts by making the prediction dependent\non a longer context containing the subject.\nWe present results for all models on LAMA in\nTable 5. We also compare our model with contex-\ntual calibration (CC) (Zhao et al., 2021), which\nprocesses the LM’s output probabilities with a log-\nlinear model.6 Coherence boosting with the se-\nlected U and : outperforms both the base model\nand CC by signiﬁcant margins.\n6 Extensions and future work\nWe suggest three promising research directions:\nCoherence tuning. The need to evaluate the base\nLM with multiple contexts in coherence boosting\nintroduces cost and complexity at inference time.\nIt may be desirable instead to modify the weights\nof the base model to improve long-range coherence\nproperties. In §B, we describe a ‘self-tuning’ al-\ngorithm that achieves this without training on any\ndata created for this purpose.\nNew domains and architectures. In this paper,\nwe mainly considered coherence boosting with\ndecoder-only Transformer LMs trained on generic\n6Note that CC applies a log-linear model to the proba-\nbility domain, not the logit domain, which does not have an\ninformation-theoretic interpretation.\ntext, but future work should consider other archi-\ntectures and target domains. In §C, we give prelim-\ninary results on the text summarization domain.\nAlthough we expect recency bias to be less pro-\nnounced in LMs that use separate attention mod-\nules to process the prompt and the output – such as\nencoder-decoder models for translation or summa-\nrization – procedures inspired by coherence boost-\ning may prove effective in domains where a strong\ncausal link between prompt and output is known\nto exist. Such domains include language gener-\nation conditioned on structured data (Yao et al.,\n2020; Mager et al., 2020; Moosavi et al., 2021) and\nmodel-guided reasoning in formal languages, such\nas proof or program synthesis (Polu and Sutskever,\n2020; Chen et al., 2021; Li et al., 2022).\nEfﬁcient search proposals. Procedures that\nforce LMs to be more focused on a prompt, or a spe-\nciﬁc part of it, when generating or ranking tokens\ncan beneﬁt algorithms that search for combinations\nof words through sampling. It would be interesting\nto use coherence boosting in non-autoregressive\ntext generation algorithms, such as to accelerate\nthe mixing of MCMC methods for constrained text\ngeneration (Miao et al., 2019; Zhang et al., 2020b;\nMalkin et al., 2021).\n7 Conclusion\nWe have illustrated the hyposensitivity of pre-\ntrained language models to long-range context and\nproposed a simple inference-time remedy. We hope\nto see coherence boosting used as a simple alter-\nnative or complement to ﬁnetuning procedures in\nzero-shot applications of pretrained LMs.\nAcknowledgments\nThe authors are grateful to Sudha Rao, Matt\nRichardson, and Huan Sun for valuable discussions\nabout this project. We thank the anonymous re-\nviewers for their comments and suggestions.\n8222\nEthics statement\nWe hope and expect to see a nonnegative net soci-\netal impact from better text generation and ranking\nalgorithms in general and from this work in partic-\nular. As we have shown, there is room to improve\nthe inference procedures used with small language\nmodels, which incur lower costs than training and\nevaluation of large models. However, researchers\nshould bear in mind the risks and potential misuse\nof automatic generation of long-form text.\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. International Con-\nference on Learning Representations (ICLR).\nIz Beltagy, Matthew E. Peters, and Arman Cohan.\n2020. Longformer: The long-document transformer.\narXiv preprint arXiv:2004.05150.\nAnya Belz, Simon Mille, and David M. Howcroft.\n2020. Disentangling the properties of human eval-\nuation methods: A classiﬁcation system to support\ncomparability, meta-evaluation and reproducibility\ntesting. In Proceedings of the 13th International\nConference on Natural Language Generation, pages\n183–194, Dublin, Ireland. Association for Computa-\ntional Linguistics.\nYoshua Bengio, Réjean Ducharme, Pascal Vincent, and\nChristian Janvin. 2003. A neural probabilistic lan-\nguage model. Journal of Machine Learning Re-\nsearch, 3:1137–1155.\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin\nChoi, et al. 2020. Piqa: Reasoning about physical\ncommonsense in natural language. Association for\nthe Advancement of Artiﬁcial Intelligence (AAAI).\nPeter F. Brown, Stephen A. Della Pietra, Vincent J.\nDella Pietra, and Robert L. Mercer. 1993. The math-\nematics of statistical machine translation: Parameter\nestimation. Computational Linguistics, 19(2):263–\n311.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-\nV oss, Gretchen Krueger, Tom Henighan, Rewon\nChild, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,\nClemens Winter, Chris Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. Neu-\nral Information Processing Systems (NeurIPS).\nBoxi Cao, Hongyu Lin, Xianpei Han, Le Sun, Lingy-\nong Yan, Meng Liao, Tong Xue, and Jin Xu. 2021.\nKnowledgeable or educated guess? revisiting lan-\nguage models as knowledge bases. In Proceed-\nings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers), pages 1860–1874,\nOnline. Association for Computational Linguistics.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Ponde de Oliveira Pinto, Jared Kaplan,\nHarrison Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, et al. 2021. Evaluating large\nlanguage models trained on code. arXiv preprint\n2107.03374.\nStanley F. Chen and Joshua Goodman. 1996. An em-\npirical study of smoothing techniques for language\nmodeling. In 34th Annual Meeting of the Associa-\ntion for Computational Linguistics , pages 310–318,\nSanta Cruz, California, USA. Association for Com-\nputational Linguistics.\nKyunghyun Cho, Bart van Merriënboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. 2014. Learning\nphrase representations using RNN encoder–decoder\nfor statistical machine translation. In Proceedings of\nthe 2014 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP) , pages 1724–\n1734, Doha, Qatar. Association for Computational\nLinguistics.\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019. BoolQ: Exploring the surprising\ndifﬁculty of natural yes/no questions. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers) , pages 2924–2936, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018. Think you have solved question an-\nswering? Try ARC, the AI2 reasoning challenge.\narXiv preprint arXiv:1803.05457.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2005. The pascal recognising textual entailment\nchallenge. In Machine Learning Challenges Work-\nshop, pages 177–190. Springer.\nMarie-Catherine De Marneffe, Mandy Simons, and Ju-\ndith Tonhauser. 2019. The commitmentbank: Inves-\ntigating projection in naturally occurring discourse.\nIn proceedings of Sinn und Bedeutung , volume 23,\npages 107–124.\nGeorge R. Doddington. 2002. Automatic evaluation\nof machine translation quality using n-gram co-\noccurrence statistics. In Human Language Technol-\nogy Research.\n8223\nAngela Fan, Mike Lewis, and Yann Dauphin. 2018. Hi-\nerarchical neural story generation. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 889–898, Melbourne, Australia. Association\nfor Computational Linguistics.\nJerome Friedman, Trevor Hastie, and Robert Tibshi-\nrani. 2000. Additive logistic regression: A statistical\nview of boosting. The Annals of Statistics , 28:337–\n407.\nMichel Galley, Chris Brockett, Xiang Gao, Jianfeng\nGao, and William B. Dolan. 2019. Grounded re-\nsponse generation task at DSTC7. Dialog System\nTechnology Challenges 7 (AAAI workshop).\nSepp Hochreiter and Jürgen Schmidhuber. 1997.\nLong short-term memory. Neural Computation ,\n9(8):1735–1780.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2019. The curious case of neural text de-\ngeneration. International Conference on Learning\nRepresentations (ICLR).\nAri Holtzman, Peter West, Vered Shwartz, Yejin Choi,\nand Luke Zettlemoyer. 2021. Surface form compe-\ntition: Why the highest probability answer isn’t al-\nways right. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 7038–7051, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nDavid M. Howcroft, Anya Belz, Miruna-Adriana\nClinciu, Dimitra Gkatzia, Sadid A. Hasan, Saad\nMahamood, Simon Mille, Emiel van Miltenburg,\nSashank Santhanam, and Verena Rieser. 2020.\nTwenty years of confusion in human evaluation:\nNLG needs evaluation sheets and standardised def-\ninitions. In Proceedings of the 13th International\nConference on Natural Language Generation, pages\n169–182, Dublin, Ireland. Association for Computa-\ntional Linguistics.\nUrvashi Khandelwal, He He, Peng Qi, and Dan Juraf-\nsky. 2018. Sharp nearby, fuzzy far away: How neu-\nral language models use context. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 284–294, Melbourne, Australia. Association\nfor Computational Linguistics.\nDaniel Khashabi, Sewon Min, Tushar Khot, Ashish\nSabharwal, Oyvind Tafjord, Peter Clark, and Han-\nnaneh Hajishirzi. 2020. UNIFIEDQA: Crossing for-\nmat boundaries with a single QA system. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2020 , pages 1896–1907, Online. As-\nsociation for Computational Linguistics.\nAlon Lavie and Abhaya Agarwal. 2007. METEOR: An\nautomatic metric for MT evaluation with high levels\nof correlation with human judgments. In Proceed-\nings of the Second Workshop on Statistical Machine\nTranslation, pages 228–231, Prague, Czech Repub-\nlic. Association for Computational Linguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. Bart: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\nand Bill Dolan. 2016. A diversity-promoting ob-\njective function for neural conversation models. In\nProceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 110–119, San Diego, California. Association\nfor Computational Linguistics.\nYujia Li, David Choi, Junyoung Chung, Nate Kushman,\nJulian Schrittwieser, Rémi Leblond, Tom Eccles,\nJames Keeling, Felix Gimeno, Agustin Dal Lago,\nThomas Hubert, Peter Choy, Cyprien de Mas-\nson d’Autume, Igor Babuschkin, Xinyun Chen, Po-\nSen Huang, Johannes Welbl, Sven Gowal, Alexey\nCherepanov, James Molloy, Daniel Mankowitz,\nEsme Sutherland Robson, Pushmeet Kohli, Nando\nde Freitas, Koray Kavukcuoglu, and Oriol Vinyals.\n2022. Competition-level code generation with al-\nphacode.\nManuel Mager, Ramón Fernandez Astudillo, Tahira\nNaseem, Md Arafat Sultan, Young-Suk Lee, Radu\nFlorian, and Salim Roukos. 2020. GPT-too: A\nlanguage-model-ﬁrst approach for AMR-to-text gen-\neration. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 1846–1852, Online. Association for Computa-\ntional Linguistics.\nNikolay Malkin, Sameera Lanka, Pranav Goel, and\nNebojsa Jojic. 2021. Studying word order through\niterative shufﬂing. In Proceedings of the 2021 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 10351–10366, Online and Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\nClara Meister and Ryan Cotterell. 2021. Language\nmodel evaluation beyond perplexity. In Proceed-\nings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers), pages 5328–5339,\nOnline. Association for Computational Linguistics.\nNing Miao, Hao Zhou, Lili Mou, Rui Yan, , and Li Lei.\n2019. CGMH: Constrained sentence generation by\nMetropolis-Hastings sampling. Association for the\nAdvancement of Artiﬁcial Intelligence (AAAI).\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\nSabharwal. 2018. Can a suit of armor conduct elec-\n8224\ntricity? a new dataset for open book question an-\nswering. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 2381–2391, Brussels, Belgium. Association\nfor Computational Linguistics.\nSewon Min, Mike Lewis, Hannaneh Hajishirzi, and\nLuke Zettlemoyer. 2021. Noisy channel language\nmodel prompting for few-shot text classiﬁcation.\narXiv preprint arXiv:2108.04106.\nNaﬁse Sadat Moosavi, Andreas Rücklé, Dan Roth, and\nIryna Gurevych. 2021. Learning to reason for text\ngeneration from scientiﬁc tables. arXiv preprint\narXiv:2104.08296.\nNasrin Mostafazadeh, Nathanael Chambers, Xiaodong\nHe, Devi Parikh, Dhruv Batra, Lucy Vanderwende,\nPushmeet Kohli, and James Allen. 2016. A cor-\npus and cloze evaluation for deeper understanding of\ncommonsense stories. In Proceedings of the 2016\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 839–849, San Diego,\nCalifornia. Association for Computational Linguis-\ntics.\nShashi Narayan, Shay B Cohen, and Mirella Lapata.\n2018. Don’t give me the details, just the summary!\ntopic-aware convolutional neural networks for ex-\ntreme summarization. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1797–1807.\nDenis Paperno, Germán Kruszewski, Angeliki Lazari-\ndou, Ngoc Quan Pham, Raffaella Bernardi, San-\ndro Pezzelle, Marco Baroni, Gemma Boleda, and\nRaquel Fernández. 2016. The LAMBADA dataset:\nWord prediction requiring a broad discourse context.\nIn Proceedings of the 54th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 1525–1534, Berlin, Germany.\nAssociation for Computational Linguistics.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of\nthe 40th Annual Meeting of the Association for Com-\nputational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 2463–2473, Hong Kong, China. As-\nsociation for Computational Linguistics.\nStanislas Polu and Ilya Sutskever. 2020. Generative\nlanguage modeling for automated theorem proving.\narXiv preprint 2009.03393.\nOﬁr Press, Noah A. Smith, and Mike Lewis. 2022.\nTrain short, test long: Attention with linear biases en-\nables input length extrapolation. International Con-\nference on Learning Representations (ICLR).\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nMelissa Roemmele, Cosmin Adrian Bejan, and An-\ndrew S. Gordon. 2011. Choice of plausible alterna-\ntives: An evaluation of commonsense causal reason-\ning. AAAI Spring Symposium Series.\nAurko Roy, Mohammad Saffar, Ashish Vaswani, and\nDavid Grangier. 2021. Efﬁcient content-based\nsparse attention with routing transformers. Transac-\ntions of the Association for Computational Linguis-\ntics, 9:53–68.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChafﬁn, Arnaud Stiegler, Arun Raja, Manan Dey,\nM Saiful Bari, Canwen Xu, Urmish Thakker,\nShanya Sharma Sharma, Eliza Szczechla, Tae-\nwoon Kim, Gunjan Chhablani, Nihal Nayak, De-\nbajyoti Datta, Jonathan Chang, Mike Tian-Jian\nJiang, Han Wang, Matteo Manica, Sheng Shen,\nZheng Xin Yong, Harshit Pandey, Rachel Bawden,\nThomas Wang, Trishala Neeraj, Jos Rozen, Ab-\nheesht Sharma, Andrea Santilli, Thibault Fevry, Ja-\nson Alan Fries, Ryan Teehan, Teven Le Scao, Stella\nBiderman, Leo Gao, Thomas Wolf, and Alexan-\nder M Rush. 2022. Multitask prompted training en-\nables zero-shot task generalization. International\nConference on Learning Representations (ICLR).\nChinnadhurai Sankar, Sandeep Subramanian, Chris Pal,\nSarath Chandar, and Yoshua Bengio. 2019. Do neu-\nral dialog systems use the conversation history ef-\nfectively? an empirical study. In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 32–37, Florence, Italy.\nAssociation for Computational Linguistics.\nTimo Schick and Hinrich Schütze. 2021. Exploiting\ncloze-questions for few-shot text classiﬁcation and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the As-\nsociation for Computational Linguistics: Main Vol-\nume, pages 255–269, Online. Association for Com-\nputational Linguistics.\nAbigail See, Peter J. Liu, and Christopher D. Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1073–\n1083, Vancouver, Canada. Association for Computa-\ntional Linguistics.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In Proceedings of the 2013 Conference on\n8225\nEmpirical Methods in Natural Language Processing,\npages 1631–1642, Seattle, Washington, USA. Asso-\nciation for Computational Linguistics.\nSimeng Sun, Kalpesh Krishna, Andrew Mattarella-\nMicke, and Mohit Iyyer. 2021. Do long-range lan-\nguage models actually use long-range context? In\nProceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing , pages\n807–822, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2019. CommonsenseQA: A ques-\ntion answering challenge targeting commonsense\nknowledge. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4149–4158, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Neural Information Processing Systems\n(NIPS).\nEllen M V oorhees and Dawn M Tice. 2000. Building\na question answering test collection. In Proceedings\nof the 23rd annual international ACM SIGIR confer-\nence on Research and Development in Information\nRetrieval, pages 200–207.\nShaowei Yao, Tianming Wang, and Xiaojun Wan.\n2020. Heterogeneous graph transformer for graph-\nto-sequence learning. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7145–7154, Online. Association\nfor Computational Linguistics.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago On-\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nLi Yang, et al. 2020. Big bird: Transformers for\nlonger sequences. Neural Information Processing\nSystems (NeurIPS).\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019. HellaSwag: Can\na machine really ﬁnish your sentence? In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4791–\n4800, Florence, Italy. Association for Computational\nLinguistics.\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Pe-\nter Liu. 2020a. Pegasus: Pre-training with extracted\ngap-sentences for abstractive summarization. In In-\nternational Conference on Machine Learning, pages\n11328–11339. PMLR.\nMaosen Zhang, Nan Jiang, Lei Li, and Yexiang Xue.\n2020b. Language generation via combinatorial con-\nstraint satisfaction: A tree search enhanced Monte-\nCarlo approach. In Findings of the Association for\nComputational Linguistics: EMNLP 2020 , pages\n1286–1298, Online. Association for Computational\nLinguistics.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsiﬁcation. Neural Information Processing Systems\n(NIPS).\nYizhe Zhang, Michel Galley, Jianfeng Gao, Zhe Gan,\nXiujun Li, Chris Brockett, and William B. Dolan.\n2018. Generating informative and diverse conver-\nsational responses via adversarial information max-\nimization. Neural Information Processing Systems\n(NeurIPS).\nYizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen,\nChris Brockett, Xiang Gao, Jianfeng Gao, Jingjing\nLiu, and Bill Dolan. 2020c. DIALOGPT : Large-\nscale generative pre-training for conversational re-\nsponse generation. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics: System Demonstrations , pages 270–\n278, Online. Association for Computational Linguis-\ntics.\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Improv-\ning few-shot performance of language models.Inter-\nnational Conference on Machine Learning (ICML).\nMing Zhong, Pengfei Liu, Yiran Chen, Danqing Wang,\nXipeng Qiu, and Xuan-Jing Huang. 2020. Extrac-\ntive summarization as text matching. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 6197–6208.\nYaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo,\nWeinan Zhang, Jun Wang, and Yong Yu. 2018. Texy-\ngen: A benchmarking platform for text generation\nmodels. ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval.\n8226\nA On multi-objective training and log-linear weights\nThe section extends the discussion in §2.1.\nRecall that the language model 5 is trained on the multi-objective loss (3):\n\"Õ\n:=1\n_: EF1...F\"+1 ∈D [−log 5: (F\"+1|F1,...,F \" ; \\)]\n                                                                                                                  \nL: (\\)\n, _ : = 1\n\".\nAs we saw in the main text, the scalarization weights _: are uniform as a consequence of the training\nregime. However, evaluation procedures effectively give nonuniform weight to the\"prediction losses.\nSome vector calculus. Denote by ˆ\\(,)a local optimum of the above optimization problem for general\nlinear combination weights , =(_1,...,_ \" ). Under suitable regularity conditions, the gradient of the\ncombined loss vanishes:\nÕ\n:\n_:\nmL: (\\)\nm\\\n\f\f\f\f\f\n\\=ˆ\\ (,)\n=0. (4)\nAssuming the Hessian A of the optimization criterion Í\n: _: L: (\\)is nonsingular, we can implicitly\ndifferentiate (4) with respect to , to obtain the matrix derivative\nmˆ\\(,)\nm, =−A−1 m(L1(\\),..., L\" (\\))\nm\\)\n\f\f\f\f\n\\=ˆ\\ (,)\n. (5)\nThe local dependence of the losses on the scalarization weights can be expressed as a bilinear form\nevaluated on mL8\nm\\ and mL9\nm\\ :\nmL8 (ˆ\\(,))\nm_9\n= mL8\nm\\\n\f\f\f\f\n\\=ˆ\\ (,)\nmˆ\\(,)\nm_9\n=−mL8\nm\\ A−1 mL9\nm\\)\n\f\f\f\f\n\\=ˆ\\ (,)\n. (6)\nBecause ˆ\\is a local minimizer, −A−1 is negative deﬁnite. In particular, any mL8 (ˆ\\ (,))\nm_8\nis negative. This\nexpresses the intuitive fact that if an inﬁnitesimally higher weight is given to some prediction loss in\noptimization, the value of this loss at the optimum will be inﬁnitesimally lower.\nFor concreteness, consider how the highest-length prediction loss L\" (ˆ\\(,))changes when _\" is\nincreased and the _9 ( 9 ≠8) are decreased with rate proportional to _9, while Í_9 is kept constant. That\nis, let # =\n(\n−_1,..., −_8−1,Í\n9≠8 _9,−_8+1,..., −_\"\n)\n. Then\n3L8 (ˆ\\(, +C#))\n3C =\nÕ\n9\nmL8\nm_9\nV9 =−mL8\nm\\ A−1\nÕ\n9\nmL9\nm\\) V9 =−mL8\nm\\ A−1 mL8\nm\\)\nÕ\n9\n_9 ≤0, (7)\nwhere the last two equalities follow from (6) and (4), respectively, and the inequality holds becauseA−1 is\npositive deﬁnite. So we have shown that, in nondegenerate cases, the L\" (\\)term of the optimization\ncriterion decreases under the locally optimal weights \\when _\" is inﬁnitesimally increased in this way.\nLog-linear mixture of predictors. Returning to coherence boosting, suppose that we aim to build\nout of the predictors 5: (−; \\ˆ(,))a new predictor 6 that would have lower negative log-likelihood on\nprediction of a word given the maximum-length context:\nEF1...F\"+1 ∈D [−log 6(F\"+1 |F1,...,F \" )]<E\n[\n−log 5\" (F\"+1 |F1,...,F \" ; ˆ\\(,))\n]\n.\nAs we just saw, using this predictor in place of 5\" achieves the same direction of movement in the\nprediction loss as optimizing with higher weight _\" .\n8227\nA naïve guess – not a proper predictor, as its outputs do not sum to 1 – would lightly perturb 5\" by\nlog-linearly mixing small multiples of the 5: weight weights V: summing to 0:\n6(C)\nnaïve(F1,...,F \" )=exp\n(\nlog 5\" (F1,...,F \" ; ˆ\\(,))+ C\nÕ\n:\nV: log 5: (−, ˆ\\(,))\n)\n.\nThen, by linearity of expectation,\n3\n3C\n\f\f\f\f\nC=0\nE\n[\n−log 6(C)\nnaïve(F\"+1 |F1,...,F \" )\n]\n=\nÕ\n:\nV:E\n[\n−log 5: (F\"+1 |F1,...,F \" ; ˆ\\(,))\n]\n=\nÕ\n:\nV: L: (ˆ\\(,)). (8)\nThis quantity is negative if, for example, L\" (ˆ\\(,))is minimal among the L: (ˆ\\(,)).\nReintroducing the normalization condition, we deﬁne a candidate function 6(C)as the normalization of\n6(C)\nnaïve over F\"+1 and compute, with the aid of (8) and using that the 6: are normalized to simplify the\nderivative of log Íexp:\n3\n3C\n\f\f\f\f\nC=0\nE\n[\n−log 6(C)(F\"+1 |F1,...,F \" )\n]\n=\nÕ\n:\nV: L: (ˆ\\(,))+ 3\n3C\n\f\f\f\f\nC=0\nE log\nÕ\nF\n6(C)\nnaïve(F |F1,...,F \" )\n=\nÕ\n:\nV: L: (ˆ\\(,))+ E\nÕ\nF\n⟨Õ\n:\nV: log 5: (F1,...,F \" ; ˆ\\(,)), 5\" (F1,...,F \" ; ˆ\\(,))\n⟩\n=\nÕ\n:\nV: L: (ˆ\\(,))−\nÕ\n:\nV:E\n[\n\u0019Kl\n( 5\" (F1,...,F \" ; ˆ\\(,))∥ 5: (F1,...,F \" ; ˆ\\(,)))]\n, (9)\nwhere the last line used that Í V: =0.\nIn practice, we are interested in sparse log-linear mixtures. Taking V\" =1, V: =−1 for a single :, and\nall other V8 =0, we conclude that the boosted model proportional to 51+C\n\" 5−C\n: is a better predictor than 5\"\nalone if the difference between prediction losses L\" and L: is greater than the average KL divergence\nbetween the predictions 5\" and 5:.\nB From coherence boosting to coherence tuning\nAs mentioned in the main text, algorithms that modify the weights of a pretrained LM to increase effect\nof distant words, mimicking coherence boosting, are an interesting direction for future work. Here we\npropose an algorithm, coherence tuning, that achieves this without training on any specialized data.\nInitializing with the pretrained model 5(−|−; \\), the algorithm iterates the following training steps to\nbring the LM closer to its coherence-boosted version 5\":\n(1) Generate a sequence F1 ...F = from the current model 5(−|−; \\).\n(2) Compute all next-token distributions under the coherence-boosted version of the current model\n( 5\" (F1 ...F :; \\)) and under the current model without boosting ( 5(F1 ...F :; \\)).\n(3) Gradient step on Kl(5\" (F1 ...F :; \\)∥5(F1 ...F :; \\)), where the ﬁrst distribution 5\" is treated as\nconstant. This step may be restricted only to :near the end of the sequence.\nWe provide a batched implementation in Fig. B.1 in lieu of pseudocode. This coherence tuning code,\nwhich performs 32 gradient steps on batches of 32 sequences of length 32, runs in a few minutes on\nmodern hardware, amortizing the overhead cost of coherence boosting while achieving comparable results\non the WebText article completion task (second-to-last row of Table 2).\n8228\nFigure B.1: Coherence tuning in PyTorch.\nC GPT-2 summarization experiments\nIn §4 of the main text, we applied coherence boosting to generic text and dialogue response generation.\nAnother interesting task that also requires long-range coherence is text summarization, in which the\nmodel is often expected to attend to the ﬁrst few sentences to summarize a long article. Thus, we provide\npreliminary experiments for zero-shot abstractive summarization by applying our proposed method to\nGPT-2 models.\nExperiment details. We take the two most popular summarization datasets, CNN/DM (See et al., 2017)\nand XSum (Narayan et al., 2018), where both contain recent articles and the summaries for the latter are\nmore abstractive than the former. Following standard design (Radford et al., 2019), we append the tokens\n“TL;DR: ” at the end of each article to induce summarization behavior of GPT models. We leverage the\nGPT-2 XL model and let it continue generating 100 tokens with greedy decoding. We take the ﬁrst three\nsentences for CNN/DM articles and the ﬁrst two sentences for XSum articles as their summaries. We use\nthe preprocessed data and metric calculation from Zhong et al. (2020) and report the standard ROUGE\nscores in Table C.1.\nTo apply our proposed coherence boosting method, similarly to the method used for dialogue response\ngeneration, we deﬁne the short context as the newly generated text after the “TL;DR: ” tokens. That is,\nat any time step during the summarization, the long context is the full article with the so-far generated\nsummary, and the short context is only the generated summary.\nResults. As we can see from Table C.1, our proposed CB method improves most of the metrics on both\ndatasets. On the CNN/DM dataset, CB yields improvements of up to ∼3 ROUGE points. We believe such\na signiﬁcant improvement is due to the article structure of the CNN/DM dataset. Speciﬁcally, the ﬁrst\nthree sentences in CNN/DM articles can provide pretty good summaries for a large portion of articles\nand have been considered as a very strong baseline for summarization models (Zhong et al., 2020). This\n8229\nCNN/DM XSum\nROUGE-1 ROUGE-2 ROUGE-L ROUGE-1 ROUGE-2 ROUGE-L\nGPT-2 XL 26.671 7.792 23.926 21.346 4.360 16.880\nCB, U=−0.1 28.027 8.658 25.179 21.580 4.265 17.025\nCB, U=−0.R 28.995 9.293 26.066 21.571 4.200 17.026\nCB, U=−0.3 29.502 9.528 26.442 21.405 4.045 16.848\nCB, U=−0.4 29.772 9.663 26.644 21.150 3.876 16.613\nCB, U=−0.5 29.872 9.625 26.658 20.773 3.703 16.288\nCB, U=−0.6 29.827 9.500 26.524 20.379 3.525 16.010\nCB, U=−0.7 29.742 9.392 26.399 20.063 3.437 15.776\nCB, U=−0.8 29.703 9.304 26.242 19.661 3.305 15.460\nCB, U=−0.9 29.481 9.109 25.990 19.387 3.195 15.256\nTable C.1: Abstractive summarization performance with the GPT-2 XL model. The best performance is bolded\nand the second-best is underlined.\nlong-range information in the ﬁrst few sentences is hard to capture for GPT models, and this difﬁculty can\nbe alleviated by coherence boosting.\nSuch structure might be less pronounced in the XSum dataset, thus leading to a smaller improvement\nfrom coherence boosting. The difference between the datasets is also reﬂected in the best value of U. The\noptimal Ufor the CNN/DM dataset is around −0.5, while the best Ufor XSum is around −0.2, which\nindicates that boosting has less effect on XSum.\nFinally, we note that the performance with GPT models presented here is far lower than that of state-of-\nthe-art summarization-speciﬁc models, such as ﬁnetuned BART models (Lewis et al., 2020; Zhang et al.,\n2020a). We leave the use of coherence boosting with such encoder-decoder models to future work.\n8230\nD Example WebText completion\nAn example of human, top-?, and coherence boosting outputs is shown in Table D.1. All outputs for all\nboosting schemes are included in the code repository.\nPrompt University of Alberta doctoral candidate Scott Persons measured the limb length for 53 different\nspecies of carnivorous dinosaurs to create an equation to score how much each was built for\nspeed.\nHuman A new study published out of the University of Alberta has found that carnivorous dinosaurs\nevolved to become faster and faster over time — and the proof is in the leg length. As part of his\ndoctoral research, lead author Scott Persons travelled the world measuring the limb lengths for\n53 different species of carnivorous bipedal dinosaurs, including the Velociraptor, Allosaurus and\nTyrannosaurus rex. In particular, Persons looked at the length of the dinosaurs’ legs below the\nknee. As a rule, he said, the longer the lower leg is in comparison to the upper leg, the faster the\nanimal is. Modern-day cheetahs have proportionately longer legs than hyenas. That relationship\nis mirrored in the animals’ speeds, Persons said. \"That’s true for modern carnivores, and must\nhave been true for dinosaurs,\" Persons said in a statement released Wednesday.\nNucleus(?=0.95) \"In a lot of areas of the world, dinosaurs at the time didn’t have legs, they had weapons,\" said\nPersons. So he used his scientiﬁc skill, titled \"Toill’s Law of Tragedy,\" to give the dinosaurs\nan on-the-ground estimate of what their internal organs weighed. For example, wood-eating\nreptiles, such as the dinosaur Lufengosaurus and the ceratopsian Tyrannosaurus rex, would have\nweighed up to 52 kg (115 lbs) each. And as many as 50 plates should have been available to\nhunt, Persons said. But what’s important is that a dinosaur’s pack or herd probably wouldn’t\nhave been able to feed all three in one sitting. Dinosaurs were extremely resilient, as very few of\nthem survived from one kill to the next and later on would start a “phased trip” back to where\nthey’d been killed, Persons said.\n+ boost(:=64,\nU: =−0.1)\n\"In a lot of areas of the world, dinosaurs at the time didn’t have legs, they had weapons,\" said\nPersons. So he used his undergraduate course to work on a formula to calculate a speciﬁc height\nfor each carnivore based on the evolutionary requirements of different dinosaurs and when it\nwould have been optimal for meat eating. He then moved that formula into his doctoral thesis\nproject, which is looking at limb lengths for medium-sized, fearsome carnivores. \"His thesis\nis about how to measure the shape of different places, plant diversity and evolutionary biology\nand kind of how we see dinosaurs and the relationships between species,\" said Susanne Dufoot,\nan associate professor of paleontology at McMaster University who recently returned from\nattending Persons’ research. \"It’s interesting because he’s basically done the legwork, developed\nthis model that can give us information about plant species.\" ‘He was an amazing creature’\nTable D.1: Completions of an article: written by a human (original WebText) and sampled from GPT-2 Large with\ntop-?sampling, with and without coherence boosting. While top-?sampling produces text that is coherent at ﬁrst\nglance – it is free of repetition and nonce words – the topic of the article meanders from limb length to internal\norgans and killing, and nonsensical comments appear (‘Toill’s Law of Tragedy’, herbivorous ceratopsian T-Rex,\netc.). The output with coherence boosting is largely free of these issues, maintaining focus on limb length and diet.\n8231\nE Prompt formats for multiple-choice tasks\nTask Prompt format\nStory Cloze [Context][Completion]\nHellaSwag [Context]he/she/they/... [Completion]\nCOPA [Premise]because/so [Hypothesis]\nCommonsenseQA [Question]the answer is: [Answer]\nOpenBookQA [Question]the answer is: [Answer]\nARC Easy Question:[Question]Answer: [Answer]\nARC Challenge Question:[Question]Answer: [Answer]\nPIQA Question:[Question]Answer: [Answer]\nSST-2 [Context]This quote has a tone that is: [Label]\nSST-5 [Context]This quote has a tone that is: [Label]\nAGNews Title:[Title]Summary:[Context]Topic: [Label]\nTREC [Question]The answer to this question will be [Label]\nBoolQ [Passage]\\nQuestion:[Hypothesis]True or False? Answer: [Label]\nRTE [Premise]\\nquestion: [Hypothesis] true or false?\\n answer: [Label]\nCB Givenquestion:[Premise]Is [Hypothesis] true, false or neither?\\n The answer is: [Label]\nTable E.1: Prompt formats used in our experiments. The full context is underlined in blue; the premise-free context\nis also underlined in red. We mainly draw inspiration from (Brown et al., 2020; Holtzman et al., 2021; Zhao et al.,\n2021) to make our prompts more natural to facilitate boosting the coherence of the completion.\n8232\nF Additional results\nGPT-3 Small GPT-3 Medium GPT-3 Large GPT-3 XL\n5max U=1 U=U∗ U∗ 5max U=1 U=U∗ U∗ 5max U=1 U=U∗ U∗ 5max U=1 U=U∗ U∗\nStory Cloze66.0 70.9 74.5 -0.8 70.1 76.3 78.0 -0.8 74.2 82.9 80.8 -0.7 79.3 82.9 86.9 -0.6\nHellaSwag35.7 38.9 42.0 -0.9 42.8 46.8 51.3 -0.8 50.5 55.1 62.2 -0.8 59.2 62.7 72.3 -0.8\nCOPA 73.0 71.0 75.0 -0.6 85.0 79.0 83.0 -0.7 84.0 83.0 84.0 -0.6 93.0 87.0 94.0 -0.5\nCsQA 34.6 46.4 48.0 -0.7 42.4 51.4 53.0 -0.7 50.0 57.5 60.4 -0.7 61.1 68.0 70.4 -0.7\nOBQA 16.0 39.8 46.6 -2.2 16.4 41.8 48.8 -1.4 20.8 45.4 47.8 -1.6 28.0 52.2 52.6 -1.1\nARC-E 51.3 48.1 56.0 -0.5 59.8 54.8 63.3 -0.4 68.4 60.3 70.7 -0.5 76.2 69.2 78.3 -0.4\nARC-C 22.6 30.8 31.1 -1.4 27.5 35.3 35.5 -1.2 33.9 41.8 41.8 -0.9 43.9 50.6 49.2 -1.1\nPIQA 69.0 57.5 69.6 -0.4 74.4 60.4 74.7 -0.4 76.3 64.2 77.7 -0.4 79.3 66.3 78.9 -0.6\nSST-2 70.6 79.8 84.6 -2.3 69.5 75.2 88.0 -4.8 66.8 65.2 70.0 2.0 86.2 88.1 89.8 -0.5\nSST-5 26.7 26.6 26.1 -1.1 29.3 30.7 30.0 -1.2 28.1 33.2 30.1 -0.8 31.2 34.8 38.5 -1.4\nAGNews 67.1 69.2 69.5 -1.2 63.3 64.8 65.4 -2.0 69.2 65.7 69.5 -0.3 71.7 71.7 71.8 0.2\nTREC 28.8 57.2 57.4 -1.0 30.2 62.6 63.6 -0.8 35.2 28.8 37.2 -0.3 52.4 47.0 56.0 -0.6\nBoolQ 60.7 62.4 62.2 -1.4 61.6 63.4 63.5 -0.9 64.2 65.6 68.1 -4.5 71.6 73.7 72.7 -0.4\nRTE 49.8 51.3 51.3 -3.6 54.5 50.5 49.1 -1.2 53.8 55.6 55.2 -1.4 56.0 57.4 60.3 -0.6\nCB 33.9 19.6 21.4 -0.7 8.9 25.0 39.3 -1.9 32.1 28.6 32.1 -0.2 5.4 25.0 28.6 -1.9\naverage 47.1 51.3 54.4 −1.3 49.0 54.5 59.1 −1.3 53.8 55.5 59.2 −0.8 59.6 62.4 66.7 −0.7\nTable F.1: Accuracy (%) of GPT-3 models on all multiple-choice tasks, in the same format as Table 4.\nGPT-2 Small GPT-2 Medium GPT-2 Large GPT-2 XL\n5max U=−1 Ours U∗ 5max U=−1 Ours U∗ 5max U=−1 Ours U∗ 5max U=−1 Ours U∗\nStory Cloze59.9 64.8 64.2 -1.0 63.0 68.5 70.4 -0.7 66.0 72.0 74.4 -0.8 67.6 75.1 76.8 -0.7\nHellaSwag28.9 31.0 31.8 -0.9 33.4 36.6 38.1 -0.9 36.6 39.5 43.0 -0.8 40.0 42.6 47.7 -0.8\nCOPA 62.0 56.0 64.0 -0.7 69.0 69.0 72.0 -0.6 69.0 60.0 69.0 -0.6 73.0 70.0 77.0 -0.4\nCsQA 29.5 42.3 43.2 -0.8 31.3 44.6 45.3 -0.8 35.7 47.3 50.0 -0.8 37.8 50.5 52.9 -0.8\nOBQA 11.2 30.6 40.8 -1.6 15.6 34.8 43.8 -2.1 13.6 34.4 44.2 -1.8 15.6 38.4 47.0 -1.9\nARC-E 43.8 42.1 46.0 -0.3 49.1 44.5 51.3 -0.6 53.2 46.5 56.2 -0.5 58.3 51.4 60.3 -0.4\nARC-C 19.0 26.1 29.1 -4.2 21.5 27.3 27.0 -1.0 21.7 28.3 29.1 -2.8 25.0 33.5 34.4 -1.1\nPIQA 62.9 57.5 63.4 -0.6 67.6 56.1 68.1 -0.5 70.3 60.0 70.1 -0.4 70.8 60.4 71.5 -0.4\nSST-2 65.7 74.7 82.3 -2.2 72.6 83.5 88.2 -2.0 77.2 87.6 88.0 -1.2 86.4 84.5 86.9 -0.1\nSST-5 25.9 30.9 30.9 -1.2 20.5 33.3 35.2 -1.1 29.1 31.8 35.2 -1.4 28.7 38.7 36.9 -1.7\nAGNews 58.6 60.8 62.2 -0.6 64.6 66.5 66.3 -0.7 62.6 62.1 63.8 -0.4 67.2 67.4 68.3 -0.4\nTREC 23.4 29.6 32.2 -0.8 27.4 17.6 36.0 -0.4 22.6 45.4 44.2 -1.2 23.4 27.4 40.0 -0.8\nBoolQ 49.4 58.1 62.1 -3.0 56.6 61.8 61.8 -0.9 61.2 62.3 62.2 -1.8 62.1 63.5 63.2 -0.6\nRTE 51.3 49.8 53.4 -0.3 53.1 50.9 53.8 -0.2 53.1 46.6 50.2 -1.2 49.1 48.7 49.1 0.9\nCB 12.5 23.2 48.2 -2.4 8.9 37.5 55.4 -2.5 8.9 32.1 53.6 -2.5 30.4 51.8 66.1 -1.9\naverage 40.3 45.2 50.3 −1.4 43.6 48.8 54.2 −1.0 45.4 50.4 55.5 −1.2 49.0 53.6 58.5 −0.7\nTable F.2: Accuracy (%) of GPT-2 models on all multiple-choice tasks, in the same format as Table 4.\n8233\nGPT-3 Small GPT-3 Medium GPT-3 Large GPT-3 XL\nPMI CC Ours PMI Ours PMI Ours PMI CC Ours\nStory Cloze 73.1 - 74.5 76.8 78.0 79.9 80.8 84.0 - 86.9\nHellaSwag 34.2 - 42.0 40.0 51.3 45.8 62.2 53.5 - 72.3\nCOPA 74.4 - 75.0 77.0 83.0 84.2 84.0 89.2 - 94.0\nCsQA 44.7 - 48.0 50.3 53.0 58.5 60.4 66.7 - 70.4\nOBQA 42.8 - 46.6 48.0 48.8 50.4 47.8 58.0 - 52.6\nARC-E 44.7 - 56.0 51.5 63.3 57.7 70.7 63.3 - 78.3\nARC-C 30.5 - 31.1 33.0 35.5 38.5 41.8 45.5 - 49.2\nSST-2 72.3 71.4 84.6 80.0 88.0 81.0 70.0 71.4 75.8 89.8\nSST-5 23.5 - 26.1 32.0 30.0 19.1 30.1 29.6 - 38.5\nAGNews 67.9 63.2 69.5 57.4 65.4 70.3 69.5 74.7 73.9 71.8\nTREC 57.2 38.8 57.4 61.6 63.6 32.4 37.2 58.4 57.4 56.0\nBoolQ 53.5 - 62.2 61.0 63.5 60.3 68.1 64.0 - 72.7\nRTE 51.6 49.5 51.3 48.7 49.1 54.9 55.2 64.3 57.8 60.3\nCB 57.1 50.0 21.4 39.3 39.3 50.0 32.1 50.0 48.2 28.6\nTable F.3: Performance comparison with other inference methods on GPT-3 models. PMI (Holtzman et al., 2021)\nis an unconditional probability normalization method, CC (Zhao et al., 2021) is the contextual calibration method.\nWe compare them in the zero-shot setting.\nGPT-2 Small GPT-2 Medium GPT-2 Large GPT-2 XL\nPMI Ours PMI Ours PMI Channel Ours PMI CC Ours\nStory Cloze 67.0 64.2 71.6 70.4 73.4 - 74.4 76.3 - 76.8\nHellaSwag 29.1 31.8 32.8 38.1 35.1 - 43.0 37.8 - 47.7\nCOPA 62.8 64.0 70.0 72.0 69.4 - 69.0 71.6 - 77.0\nCsQA 36.4 43.2 41.8 45.3 44.5 - 50.0 47.8 - 52.9\nOBQA 32.4 40.8 38.6 43.8 43.2 - 44.2 46.0 - 47.0\nARC-E 39.3 46.0 42.4 51.3 47.0 - 56.2 49.9 - 60.3\nARC-C 28.2 29.1 28.6 27.0 31.6 - 29.1 33.8 - 34.4\nSST-2 67.1 82.3 86.2 88.2 85.6 77.1 88.0 87.5 82.0 86.9\nSST-5 30.0 30.9 39.3 35.2 22.0 29.2 35.2 40.8 - 36.9\nAGNews 63.0 62.2 64.4 66.3 64.1 61.8 63.8 65.4 60.0 68.3\nTREC 36.4 32.2 21.6 36.0 44.0 30.5 44.2 32.8 37.3 40.0\nBoolQ 51.1 62.1 49.7 61.8 46.7 - 62.2 49.5 - 63.2\nRTE 49.8 53.4 54.9 53.8 54.2 - 50.2 53.4 48.5 49.1\nCB 50.0 48.2 50.0 55.4 50.0 - 53.6 50.0 17.9 66.1\nTable F.4: Performance comparison with other inference methods on GPT-2 models. PMI (Holtzman et al., 2021)\nis an unconditional probability normalization method, CC (Zhao et al., 2021) is the contextual calibration method\nand Channel (Min et al., 2021) uses an inverted-LM scoring approach that computes the conditional probability of\nthe input given the label. We compare them in the zero-shot setting.\n8234\n2\n 1\n 0 1 2 3 4 5\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85Accuracy\nStory Cloze\n125M\n350M\n760M\n1.6B\n2.7B\n6.7B\n13B\n175B\n2\n 1\n 0 1 2 3 4 5\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7Accuracy\nHellaSwag\n125M\n350M\n760M\n1.6B\n2.7B\n6.7B\n13B\n175B\n2\n 1\n 0 1 2 3 4 5\n0.1\n0.2\n0.3\n0.4\n0.5Accuracy\nOpenBookQA\n125M\n350M\n760M\n1.6B\n2.7B\n6.7B\n13B\n175B\n2\n 1\n 0 1 2 3 4 5\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7Accuracy\nCommonsenseQA\n125M\n350M\n760M\n1.6B\n2.7B\n6.7B\n13B\n175B\n2\n 1\n 0 1 2 3 4 5\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8Accuracy\nARC Easy\n125M\n350M\n760M\n1.6B\n2.7B\n6.7B\n13B\n175B\n2\n 1\n 0 1 2 3 4 5\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50Accuracy\nARC Challenge\n125M\n350M\n760M\n1.6B\n2.7B\n6.7B\n13B\n175B\n2\n 1\n 0 1 2 3 4 5\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8Accuracy\nPIQA\n125M\n350M\n760M\n1.6B\n2.7B\n6.7B\n13B\n175B\n2\n 1\n 0 1 2 3 4 5\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9Accuracy\nCOPA\n125M\n350M\n760M\n1.6B\n2.7B\n6.7B\n13B\n175B\nFigure F.1: Model comparison for StoryCloze, HellaSwag, OpenBookQA, CommonsenseQA, ARC Easy, ARC\nChallenge, PIQA and COPA by varying Uon the testing set.\n8235\n2\n 1\n 0 1 2 3 4 5\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90Accuracy\nSST2\n125M\n350M\n760M\n1.6B\n2.7B\n6.7B\n13B\n175B\n2\n 1\n 0 1 2 3 4 5\n0.20\n0.25\n0.30\n0.35\n0.40Accuracy\nSST5\n125M\n350M\n760M\n1.6B\n2.7B\n6.7B\n13B\n175B\n2\n 1\n 0 1 2 3 4 5\n0.3\n0.4\n0.5\n0.6\n0.7Accuracy\nAGNews\n125M\n350M\n760M\n1.6B\n2.7B\n6.7B\n13B\n175B\n2\n 1\n 0 1 2 3 4 5\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6Accuracy\nTREC\n125M\n350M\n760M\n1.6B\n2.7B\n6.7B\n13B\n175B\n2\n 1\n 0 1 2 3 4 5\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75Accuracy\nBoolQ\n125M\n350M\n760M\n1.6B\n2.7B\n6.7B\n13B\n175B\n2\n 1\n 0 1 2 3 4 5\n0.46\n0.48\n0.50\n0.52\n0.54\n0.56\n0.58\n0.60\n0.62Accuracy\nRTE\n125M\n350M\n760M\n1.6B\n2.7B\n6.7B\n13B\n175B\n2\n 1\n 0 1 2 3 4 5\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7Accuracy\nCommitmentBank\n125M\n350M\n760M\n1.6B\n2.7B\n6.7B\n13B\n175B\nFigure F.2: Model comparison for SST-2, SST-5, AGNews, TREC, BoolQ, RTE and CommitmemtBank by varying\nUon the testing set.\n8236",
  "topic": "Boosting (machine learning)",
  "concepts": [
    {
      "name": "Boosting (machine learning)",
      "score": 0.8977766036987305
    },
    {
      "name": "Computer science",
      "score": 0.768140435218811
    },
    {
      "name": "Coherence (philosophical gambling strategy)",
      "score": 0.6946334838867188
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6363863945007324
    },
    {
      "name": "Inference",
      "score": 0.5922557711601257
    },
    {
      "name": "Natural language processing",
      "score": 0.5061363577842712
    },
    {
      "name": "Security token",
      "score": 0.4749348759651184
    },
    {
      "name": "Language model",
      "score": 0.44902217388153076
    },
    {
      "name": "Machine learning",
      "score": 0.3995123505592346
    },
    {
      "name": "Mathematics",
      "score": 0.06410369277000427
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    }
  ]
}