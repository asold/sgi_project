{
    "title": "slimIPL: Language-Model-Free Iterative Pseudo-Labeling",
    "url": "https://openalex.org/W3093788532",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2616197968",
            "name": "Tatiana Likhomanenko",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2904872833",
            "name": "Qiantong Xu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2052875628",
            "name": "Jacob Kahn",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A294368734",
            "name": "Gabriel Synnaeve",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A130200899",
            "name": "Ronan Collobert",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2113691817",
        "https://openalex.org/W3095350795",
        "https://openalex.org/W2996501936",
        "https://openalex.org/W2101210369",
        "https://openalex.org/W2940180244",
        "https://openalex.org/W2998532468",
        "https://openalex.org/W3091002423",
        "https://openalex.org/W3093579165",
        "https://openalex.org/W3026041220",
        "https://openalex.org/W3096338464",
        "https://openalex.org/W2127141656",
        "https://openalex.org/W2963216553",
        "https://openalex.org/W3099782249",
        "https://openalex.org/W2163568299",
        "https://openalex.org/W1494198834",
        "https://openalex.org/W2995746049",
        "https://openalex.org/W3103005696",
        "https://openalex.org/W2146502635",
        "https://openalex.org/W2561274697",
        "https://openalex.org/W2995181338",
        "https://openalex.org/W3099570996",
        "https://openalex.org/W165878654",
        "https://openalex.org/W117861810",
        "https://openalex.org/W2145837098",
        "https://openalex.org/W2520160253",
        "https://openalex.org/W2889326796",
        "https://openalex.org/W3015265920",
        "https://openalex.org/W2887516053",
        "https://openalex.org/W1524333225",
        "https://openalex.org/W2989700832",
        "https://openalex.org/W2991213871",
        "https://openalex.org/W2842511635",
        "https://openalex.org/W2978426779",
        "https://openalex.org/W2943152387",
        "https://openalex.org/W3160235762",
        "https://openalex.org/W3170205569",
        "https://openalex.org/W3036601975",
        "https://openalex.org/W2936774411",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2996159613",
        "https://openalex.org/W2111316763",
        "https://openalex.org/W3096490862",
        "https://openalex.org/W3035160371",
        "https://openalex.org/W2962907457",
        "https://openalex.org/W3116489684",
        "https://openalex.org/W3170874841",
        "https://openalex.org/W3015522062"
    ],
    "abstract": "Recent results in end-to-end automatic speech recognition have demonstrated the efficacy of pseudo-labeling for semi-supervised models trained both with Connectionist Temporal Classification (CTC) and Sequence-to-Sequence (seq2seq) losses. Iterative Pseudo-Labeling (IPL), which continuously trains a single model using pseudo-labels iteratively re-generated as the model learns, has been shown to further improve performance in ASR. We improve upon the IPL algorithm: as the model learns, we propose to iteratively re-generate transcriptions with hard labels (the most probable tokens), that is, without a language model. We call this approach Language-Model-Free IPL (slimIPL) and give a resultant training setup for low-resource settings with CTC-based models. slimIPL features a dynamic cache for pseudo-labels which reduces sensitivity to changes in relabeling hyperparameters and results in improves training stability. slimIPL is also highly-efficient and requires 3.5-4x fewer computational resources to converge than other state-of-the-art semi/self-supervised approaches. With only 10 hours of labeled audio, slimIPL is competitive with self-supervised approaches, and is state-of-the-art with 100 hours of labeled audio without the use of a language model both at test time and during pseudo-label generation.",
    "full_text": "SLIM IPL: L ANGUAGE -MODEL -FREE ITERATIVE\nPSEUDO -LABELING\nA PREPRINT\nTatiana Likhomanenko Qiantong Xu Jacob Kahn Gabriel Synnaeve Ronan Collobert\nFacebook AI Research\nantares@fb.com\nAugust 31, 2021\nABSTRACT\nRecent results in end-to-end automatic speech recognition have demonstrated the efﬁcacy of pseudo-\nlabeling for semi-supervised models trained both with Connectionist Temporal Classiﬁcation (CTC)\nand Sequence-to-Sequence (seq2seq) losses. Iterative Pseudo-Labeling (IPL), which continuously\ntrains a single model using pseudo-labels iteratively re-generated as the model learns, has been shown\nto further improve performance in ASR. We improve upon the IPL algorithm: as the model learns,\nwe propose to iteratively re-generate transcriptions with hard labels (the most probable tokens), that\nis, without a language model. We call this approach Language-Model-Free IPL (slimIPL) and give\na resultant training setup for low-resource settings with CTC-based models. slimIPL features a\ndynamic cache for pseudo-labels which reduces sensitivity to changes in relabeling hyperparameters\nand results in improved training stability. slimIPL is also highly-efﬁcient and requires 3.5-4x fewer\ncomputational resources to converge than other state-of-the-art semi/self-supervised approaches.\nWith only 10 hours of labeled audio, slimIPL is competitive with self-supervised approaches, and is\nstate-of-the-art with 100 hours of labeled audio without the use of a language model both at test time\nand during pseudo-label generation. Moreover, slimIPL is applicable to conversational speech and its\nhyperparameters are transferred out-of-the-box.\nIndex Terms: deep learning, semi-supervised learning, pseudo-labeling, self-training, speech recognition\n1 Introduction\nRecent work in deep learning has shifted towards methods which can efﬁciently learn from large amounts of unlabeled\ndata to improve performance and decrease costs associated with labeling. Semi-supervised learning [ 1] combines\ninformation from both labeled and unlabeled data; the amount of unlabeled data typically exceeds the amount of labeled\ndata. In automatic speech recognition (ASR), while many recent semi-supervised methods outperform a supervised\nbaseline in a low-resource setting, a gap between semi- and fully-supervised training remains. Further, not all of the\napproaches are equally scalable as the amount of labeled and unlabeled data increases, as is the case in recent setups\nsuch as the Libri-Light benchmark [2].\nSome of the earliest and simplest semi-supervised approaches use self-training [ 3]. Self-training employs a base\nmodel trained with labeled data which acts as a “teacher” and is used to label unlabeled data (the resulting labels\nare referred as “pseudo-labels”, PLs). A “student” model is then trained (typically from scratch) with both labeled\nand pseudo-labeled data to yield a ﬁnal model. For competitive results in ASR, a language model (LM) was a key\ncomponent of pseudo-labeling: it is usually combined with the acoustic model via beam-search decoding [4, 5, 6] or\nthrough shallow fusion [7, 8, 9, 10] to generate PLs. With this setting, however, acoustic models tend to over-ﬁt to the\ntext training set of the LM used for pseudo-labeling [5, 9].\nIn this work, we show that competitive pseudo-labeling approaches rely neither on beam-search decoding nor on a\nlanguage model. In our setup, pseudo-labels are generated by picking hard labels – tokens with the highest acoustic\nmodel probability. Our approach is based on the recently-proposed iterative pseudo-labeling algorithm (IPL) [5]: we\narXiv:2010.11524v5  [cs.CL]  30 Aug 2021\nA PREPRINT - AUGUST 31, 2021\ncontinuously train a single model using iteratively re-generated pseudo-labels as model learns. We call our algorithm\nlanguage-model-free IPL (slimIPL) and give its overview in Section 4. We demonstrate in Section 5 that this approach\nis effective for models trained with Connectionist Temporal Classiﬁcation (CTC) [11] in low-resource settings and is\ncompetitive with current state-of-the-art results. Ablation studies on different aspects of the proposed algorithm in\nSection 5.6 show slimIPL provides training stability and a robustness to hyperparameter settings.\n2 Related Work\nSelf-training methods [3] still attract researchers: extensions to self-training are numerous and include (a) selecting\nparticular subsets of pseudo-labeled data for student training, (b) the reiteration of the PL procedure several times to\nprogressively-improve the teacher model, (c) the introduction of different types of noise for student model training,\nand (d) sampling techniques and schedules for training over labeled and pseudo-labeled datasets. Many recent works\non self-training propose and validate these extensions, including those in computer vision [12, 13], natural language\nprocessing [14, 15, 16, 17, 18, 19, 20], ASR [21, 7, 22, 8, 9], and speech translation [23].\nOne extension to the simple pseudo-labeling method consists of continuously training a single model [ 24]. At the\nbeginning of training, a model is trained only on labeled data after which training continues on data jointly-selected\nfrom both labeled and unlabeled datasets. PL re-generation occurs after some number of iterations, and a supervised\nloss is computed both on labeled and pseudo-labeled data for each batch. An additional parameter determines the\ncontribution of pseudo-labeled data to the overall loss. The effectiveness of this iterative training for a single model has\nbeen validated on tasks in vision [25], natural language processing [18], and ASR [26, 5]. Below, we give an overview\nof the most relevant approaches in ASR to our work.\nIterative pseudo-labeling (IPL) [5] algorithm follows prior work [ 24] and uses augmentation of both labeled and\nunlabeled data, and continuously trains a single model with iterative re-generation of PLs by beam-search decoding with\nan LM, as the model learns. Compared to IPL, we maintain a dynamic cache with PLs and don’t use any beam-search\ndecoding or an LM.\nNoisy self-training [9] performs ﬁve iterations of student network training, each time from scratch, with PLs generated\nby a teacher network. In this approach, as is the case with IPL, shallow fusion with an LM is used with a decoding\nprocedure to generate PLs, while slimIPL doesn’t use an LM at all.\nSelf-training [26] is the closest to our work: the authors continuously train a model with re-generated hard PLs after\neach iteration. This work is more focused on studying the impact of noise, and both SpecAugment [ 27] and speed\nperturbation are applied for labeled and unlabeled data during training. Our experiments show that re-generating PLs\nwith hard labels after each iteration causes training instability resulting in divergence, whereas slimIPL exploits a\ndynamic cache mechanism to stabilize training.\nwav2vec [28]’s unsupervised pre-training gives a signiﬁcant boost in performance for low-resource settings. Training\nhas two steps: ﬁrst, pre-training on unlabeled data by masking the input audio in the latent space and solving a\ncontrastive learning task [29]; second, ﬁne-tuning the model using labeled audio only. Recently, one-step training [30]\nis proposed to directly optimize a downstream task which simpliﬁes hyperparameter tuning. One of the known problems\nwith contrastive training is a need of large batches [28, 30].\n3 Pseudo-Labeling\nLet L = {xi,yi}be a labeled dataset and U = {xj}a large unlabeled dataset. We consider a semi-supervised\npseudo-labeling approach where the acoustic model (AM) Mθ is continuously trained on combination of a labeled\nset and an iteratively re-generated pseudo-labeled set. Training minimizes the following loss function: L(θ) =\nLL(θ) +λLU(θ), λ∈R+, where θare the parameters of the AM, and λ is a tunable parameter controlling the\nimportance of unlabeled data. The losses for labeled data LL and for unlabeled data LU are deﬁned as LL(θ) =\n−Ex,y∼p(x,y) log pθ(y|x), (x,y) ∈L,where p(x,y) is the empirical data distribution of samples from L, pθ(y|x)\nis the conditional distribution deﬁned by Mθ, LU(θ) =−Ex∼p(x) log pθ(ˆy|x), x∈U,where p(x) is the empirical\ndata distribution of samples from U, and ˆyare the pseudo-labels for utterance x∈U.\nOne key difference in existing pseudo-labeling approaches is how the labels assignments ˆyare obtained for unlabeled\ndata x∈U. In the general literature, pseudo-labeling refers to the hard label generation:\nˆy= argmax\ny\nlog pθ(y|x). (1)\nIn machine translation and ASR domains, the model pθ(y|x) is often sequence-to-sequence, and the solution of Eq. (1)\nmay be approximated with a beam-search decoding algorithm [31, 18, 8, 7, 5, 9, 23, 26]. Indeed, most recent work in\n2\nA PREPRINT - AUGUST 31, 2021\nAlgorithm 1: slimIPL\nData: labeled L= {xi,yi}and unlabeled U = {xj}\nResult: Acoustic model Mθ\n1. Train Mθ on Lwith augmentation for M updates;\n2. while cache is not full at sizeCdo\n- Draw a random batch from x∈U;\n- Generate its PL ˆyby Mθ following Eq.(1);\n- Store {x,ˆy}into the cache;\n- Train Mθ on Lwith augmentation for 1 update;\nend\n3. Decrease model’sMθ dropout;\nrepeat\n4. Train Mθ on Lwith augmentation for NL updates;\n5. for NU updates do\n- Draw a random batch B = {x,ˆy}from the cache;\n- With probability p, Bis removed from cache and a new pair of random batch x′∈U and its PL ˆy′\ngenerated by Mθ is added in;\n- Apply augmentation to batch Band make an optimization step to update Mθ.\nend\nuntil convergence or maximum iterations are reached;\nASR relies on an LM plm(y) to generate PLs, and instead optimizes:\nˆy= argmax\ny\nlog pθ(y|x) +α log plm(y), x∈U, (2)\nwhere αis a hyperparameter controlling the amount of language model regularization. Pseudo-labeling is also popular\nin computer vision [24, 32]. Variants exist, such as “soft labels” ˆy = pθ(y|x), variations on soft labeling [ 33, 34],\n”hard distillation” [35] and sampling [36, 37].\n4 Language-Model-Free IPL\nIn the original IPL algorithm [5], PLs are generated with a beam-search decoder leveraging an LM and approximating\nthe solution of Eq. (2). While the main motivation is to transfer the knowledge of the LM into the AM, drawbacks\nexist: (i) generating PLs is computationally intensive, and (ii) models easily over-ﬁt to LM knowledge. Regularization\ntechniques are proposed in [5] to overcome (ii), such that one can still beneﬁt from the LM when decoding at evaluation\ntime.\nWe demonstrate that PLs do not need to rely on any LM information at all. Algorithm 1 describes slimIPL. While it\nfollows the IPL algorithm, PLs are generated by considering the top prediction according to the AM as per Eq. (1).\nFor CTC-based acoustic models, this corresponds exactly to choosing the most likely token at each time step. Our\napproach also imitates self-training [26], but instead of re-generating PLs after each update we exploit a dynamic cache\nto use PLs generated by the previous model states(this can be viewed as models ensemble averaging for PL generation).\nThis stabilizes the optimization process and avoids sudden model divergence as discussed in Section 5.6. In addition,\na regularization scheme is implemented via data augmentation over the input (acoustic) data, both for labeledLand\nunlabeled U samples. slimIPL has several hyperparameters: (i) when PL generation begins M, (ii) the proportion of\nlabeled and unlabeled data λ= NU/NL, (iii) the dynamic cache size Cand the probability pof updating the cache.\n5 Experiments\n5.1 Data\nAll experiments are performed on the LibriSpeech dataset [ 40] (contains 960 hours of training audio with paired\ntranscriptions: train-clean-100, train-clean-360, and train-other-500 parts) and Libri-Light [2] labeled limited resource\ntraining subset train-10h originally extracted from LibriSpeech. We consider two low-resource scenarios with different\namounts of labeled / unlabeled data: (i) LL-10/LS-960 uses train-10h as labeled data and full LibriSpeech as unlabeled;\n(ii) LS-100/LS-860 as labeled data and train-clean-360 and train-other-500 as unlabeled. The standard LibriSpeech\nvalidation sets (dev-clean and dev-other) are used to tune all hyperparameters, as well as to select the best models.\nTest sets (test-clean and test-other) are used only to report ﬁnal word error rate (WER) performance. We keep the\n3\nA PREPRINT - AUGUST 31, 2021\nTable 1: WER comparison of our supervised baselines with prior work: LL-10 (top) and LS-100 (bottom).\nMethod Stride Tokens Criterion LM Dev WER Test WER\nclean other clean other\nLibri-Light [2] 20 ms letters CTC word 4-gram 34 60.9 33.5 62.1\nOurs 30ms letters CTC\n- 31.9 52.3 32.6 52.4\nword 4-gram 18.8 39.3 19.6 39.7\n+ rescoring 17.1 38.2 17.9 38.9\nRWTH [38] - - hybrid word 4-gram 5.0 19.5 5.8 18.6\nDeCoAR [39] - phn. CTC - - - 6.1 17.4\nImproved T/S [9] - 16k wp S2S - 5.3 16.5 5.5 16.9\nOurs 30ms letters CTC\n- 6.2 16.8 6.2 16.8\nword 4-gram 4.1 12.4 4.5 12.7\n+ rescoring 3.3 10.9 3.7 11.4\noriginal 16kHz sampling rate and compute log-mel ﬁlterbanks with 80 coefﬁcients for a 25ms sliding window, strided\nby 10ms. All features are normalized to have zero mean and unit variance per input sequence before feeding them into\nthe acoustic model.\n5.2 Acoustic Models\nWe consider CTC-based models. Architectures follow [8]: the encoder is composed of a 1-D convolution with kernel\nsize 7 and stride 3 followed by 36 4-head Transformer blocks [ 41]. The self-attention dimension is 768 and the\nfeed-forward network (FFN) dimension is 3072 (with 4 heads) in each Transformer block. The output of the encoder is\nfollowed by a linear layer to the output classes. We use dropout after the convolution, dropout on the self-attention and\non the FFN for all Transformer layers, and layer drop [42], dropping entire layers at the FFN level.\nTokens Letters are used for all experiments. The letter set consists of the 26 English alphabet letters augmented with\nthe apostrophe and a word boundary token.\nData augmentation Training is performed with SpecAugment [27] only. We use two frequency masks with frequency\nmask parameter F = 30, ten time masks with maximum time-mask ratio p= 0.1 and time mask parameter T = 50;\ntime warping is not used. For LL-10/LS-960 we found that twenty time masks with T = 25improve performance.\nTraining For all experiments we use the Adagrad optimizer [43] and decay learning rate by 2 each time the WER\nreaches a plateau on the validation sets. All models architectures, as well as slimIPL are implemented within the\nﬂashlight1 framework and available athttps://github.com/flashlight/wav2letter/tree/master/recipes/\nslimIPL. Models are trained with dynamic batching (effective average batch size is 14 per GPU) and mixed-precision\ncomputations on 16 GPUs (V olta 32GB) for 350-500k updates.\n5.3 Beam-search Decoding and Rescoring\nIn all our experimental results, we report not only WER without an LM, but also WER obtained with a one-pass\nbeam-search decoder leveraging an LM. Following the notation introduced in Section 3, the beam-search decoder aims\nat maximizing:\nlog pθ(ˆy|x) +αlog plm(ˆy) +β|ˆy|,\nwhere α and β are hyperparameters to tune. We rely on the beam-search decoder from the ﬂashlight framework\nfollowing [44]: the lexicon-based beam-search decoder with a word-level LM. LibriSpeech validation sets, dev-clean\nand dev-other, are used to optimize the beam-search decoder hyperparameters, through random search. We also report\nWER obtained by rescoring the beam of hypothesis generated by the one-pass decoder. Rescoring is performed with a\nstrong word-level Transformer LM, following the procedure described in [8]. We use open-sourced word-level LMs\ntrained on the LibriSpeech LM corpus: 4-gram [45] and Transformer [8] LMs.\n5.4 Supervised Baselines\nThe dropout and layer drop parameters of our acoustic models were set to 0.5 (0.3) when trained on LL-10 (LS-100).\nPerformance in WER is reported in Table 1. Our supervised baseline models trained on either LL-10 or LS-100 deﬁne a\n1https://github.com/flashlight/flashlight\n4\nA PREPRINT - AUGUST 31, 2021\nTable 2: Comparison with other semi- and unsupervised methods: LL-10/LS-960 (top) and LS-100/LS-860 (bottom).\nMethod Stride Tokens Criterion LM Dev WER Test WER Compute Resources\nclean other clean other Train Time (Days) # G/TPUs G/TPU-days\nLibri-Light [2] 20 ms letters CTC word 4-gram 30.5 55.8 30.1 57.2 - - -\nIPL [5] 80ms 5k wp CTC - 23.8 25.7 24.6 26.5 3 64 GPUs 192+ rescoring 23.5 25.5 24.4 26.0\nwav2vec 2.0 [28] 20ms letters CTC\n- 8.1 12.0 8.0 12.1\n2.3 128 GPUs 294.4word 4-gram 3.4 6.9 3.8 7.3\nword Transf. 2.9 5.7 3.2 6.1\nslimIPL 30ms letters CTC\n- 11.4 14 11.4 14.7\n4.7 16 GPUs 75.2word 4-gram 6.6 9.6 6.8 10.5\n+ rescoring 5.3 7.9 5.5 9.0\nIPL [5] 80ms 5k wp CTC - 5.5 9.3 6.0 10.3 3 64 GPUs 192+ rescoring 5.0 8.0 5.6 9.0\nImproved T/S [9] - 16k wp S2S - 4.3 9.7 4.5 9.5 10×5 32 TPUs 1600LSTM 3.9 8.8 4.2 8.6\nwav2vec 2.0 [28] 20ms letters CTC\n- 4.6 9.3 4.7 9.0\n2.3 128 GPUs 294.4word 4-gram 2.3 5.7 2.8 6.0\nword Transf. 2.1 4.8 2.3 5.0\nslimIPL 30ms letters CTC\n- 3.7 7.3 3.8 7.5\n5.2 16 GPUs 83.2word 4-gram 2.8 5.6 3.1 6.1\n+ rescoring 2.2 4.6 2.7 5.2\nnew state-of-the-art both on test-clean and test-other with beam-search decoding and further rescoring. On test-other\nthese models are state-of-the-art even without an LM.\n5.5 Semi-Supervised Experiments\nslimIPL architectures are identical to their supervised counterparts, except for their dropout and layer drop values which\nare decreased after M (supervised-only) updates to 0.1 for both settings, see Algorithm 1 step 3. Stronger regularization\n(via high dropout) is critical to avoid over-ﬁtting when training with labeled-only data. These regularization parameters\nare then decreased to “increase” model capacity, as more data is involved during the semi-supervised training (see\nablation in Table 3 when dropout is not changed during the training). slimIPL hyperparameters were tuned by performing\na search over the following ranges: M in 5k, 10k, 20k; NU:NL in 1:1, 1:2, 2:1, {3,4,5}:1, 10:1, 20:1; Cin 10, 100, 1k;\npin 0.1, 0.5, 1. The best conﬁguration results are presented in Table 2. On the LL-10/LS-960 setup, slimIPL is the best\nperforming pseudo-label-based technique, on both test-clean and test-other, almost closing the gap with wav2vec [28].\nOn LS-100/LS-860, we deﬁne a new state-of-the-art for LM-free approaches on both test-clean and test-other, while\nbeing similar to wav2vec with additional LM decoding.\n5.6 Ablations\n0 100000 200000 300000 400000\nUpdates\n20\n40\n60\n80\n100Word Error Rate\nno cache\nno cache (another seed)\nslimIPL\n0 50000 100000 150000 200000 250000 300000 350000 400000\nUpdates\n20\n40\n60\n80\n100Word Error Rate\nno cache\nno cache (another seed)\nslimIPL\nFigure 1: Learning curves on dev-other for models trained on LL-10/LS-960 (left) and LS-100/LS-860 (right). slimIPL\nmodels refer to baseline models (grey) from Table 3.\nIn Tables 3 and 4, we study the robustness of slimIPL with respect to the choice of its hyperparameters. First, we\nconsider the pseudo-labeling algorithm from [26], where PLs are generated from the acoustic model without dropout,\nfrom the acoustic samples without data augmentation, after each update (no cache, C = no). We found in practice that\n5\nA PREPRINT - AUGUST 31, 2021\nTable 3: Ablations study on slimIPL hyperparameters reporting validation WER: LL-10/LS-960 (top) and LS100/LS-\n860 (bottom). ”x” refers to the values from a baseline (grey) model.\ndropout C p M/ λ no LM 4-gram LM\nWER-other clean other clean other\n0.5→0.1 1000 0.1 20k/60 10/1 11.4 14.0 6.6 9.6\n0.5 x x x x 12.0 17.7 6.9 11.7\nx 10 x x x 17.8 20.7 9.0 12.8\nx 100 x x x 13.0 15.3 7.4 10.0\nx x 0.5 x x 12.3 15.8 7.0 10.1\nx x 1 x x 13.8 17.5 7.2 10.7\nx x x 5k/91 x 54.3 56.8 30.1 35.0\nx x x 10k/73 x 23.7 26.5 9.6 13.8\nx x x 40k/55 x 10.7 13.7 6.7 9.8\nx x x x 1/1 11.3 15.0 6.6 10.3\nx x x x 5/1 11.6 14.6 6.8 9.7\nx x x x 20/1 11.9 14.7 6.5 9.8\n0.3→0.1 100 0.1 20k/33 1/1 3.6 7.5 2.9 5.9\n0.3 x x x x 4.1 8.1 3.1 6.3\nx 10 x x x 3.9 7.7 2.9 5.9\nx 1000 x x x 3.8 7.5 2.8 5.9\nx x 0.5 x x 3.8 7.3 3.0 5.9\nx x 1 x x 4.1 8.1 3.1 6.1\nx x x 5k/80 x 3.9 7.8 2.9 5.9\nx x x 10k/53 x 3.9 7.6 2.9 5.9\nx x x 50k/23 x 3.9 7.8 2.9 5.9\nx x x x 1/2 4.2 8.5 3.1 6.4\nx x x x 2/1 3.7 7.3 2.7 5.6\nx x x x 3/1 3.7 7.3 2.8 5.6\nx x x x 4/1 3.7 7.3 2.7 5.5\nTable 4: Ablations study on slimIPL reporting validation WER: LL-10/LS-960 (top) and LS100/LS-860 (bottom).\n”Naive” approach [26] is referred as C =”no”; slimIPL baseline (grey) models are the same as baseline models from\nTable 3.\nEMA Cache no LM 4-gram LM\nclean other clean other\nno yes 11.4 14.0 6.6 9.6\nno no diverges\nyes yes 11.6 14.7 6.4 9.6\nyes no 13.0 15.0 6.7 9.7\nno yes 3.6 7.5 2.9 5.9\nno no 3.9 7.5 3.1 5.9\nyes yes 3.9 7.9 2.7 5.8\nyes no 3.9 7.9 2.9 5.9\nthis approach is unstable, prone to unpredictable model divergence (generated transcriptions become empty), which can\nbe occasionally recovered, see Figure 1. In contrast, slimIPL is robust thanks to its dynamic cache strategy – in practice\nwe observed no divergence if the cache size C ≥10. For runs where the ”cache-free” approach converges, we observe\nsimilar performance with slimIPL, as shown in Table 4. In addition, lowering the cache update probability pallows to\nre-generate PLs more rarely in slimIPL than in cache-free approaches, which leads to faster training. slimIPL is robust\nto the cache size Cand cache update probability p. However, for limited supervision the small cache setting C = 10\nperforms worse.\n6\nA PREPRINT - AUGUST 31, 2021\nThe starting update M for involving unlabeled data in the training process is critical for the low-resource labeled\ndata setting LL-10/LS-960. In the case of LS-100/LS-860, slimIPL recovers even when starting from high WER\nsupervised models. To prevent quick over-ﬁtting over supervised data for LL-10/LS-960, the ratio λbetween the\nnumber of unsupervised and supervised updates should be greater than 1, with little variations in WER for any λ> 1.\nFor LS-100/LS-860 this hyperparameter is less critical, λ= 4/1 found to be optimal, and slower convergence observed\nfor λ< 1.\nOverall, with enough labeled data, slimIPL is robust to hyperparameter changes. When labeled data is limited, C, M\nand λshould be large enough to avoid over-ﬁtting to labeled data.\n5.7 Exponential Moving Average\nslimIPL performs a kind of model averaging, as it leverages PL predictions from past versions of the acoustic model\nduring training. This not only stabilizes the training (compared to only using predictions from the current model), but\nalso is computationally efﬁcient, thanks to the caching strategy. Another popular way of performing model averaging is\nby performing an exponential moving average (EMA) over past model weights, as training goes. We thus performed\na comparison of slimIPL with a modiﬁed version of the algorithm in [ 26], where PLs are generated with the EMA\nacoustic model (without dropout, and without data augmentation over acoustic utterances). EMA decay factor is set to\n0.999 to emulate the history range from the dynamic cache in our slimIPL experiments. In Table 4, we show that the\nEMA approach stabilizes the training: no divergence is observed for the LL-10/LS-960 setup, and the model converges\neven if we regenerate PLs after each update. Combining the dynamic cache strategy and EMA does not seem to lead to\nimprovements for both LL-10/LS-960 and LL-100/LS-860. Overall, slimIPL caching approach is as effective as an\nEMA approach, but with the advantage of being more memory and computationally efﬁcient, as it saves the estimation\nof cached PLs and doesn’t store additional EMA model.\nWe perform EMA experiments only as ablation study while the concurrent work [ 46] performs in-depth study and\nanalysis of EMA approach. In [46] authors propose more general framework with EMA, called Kaizen, which can be\nseen as a continuous version of the IPL for semi-supervised training. Authors also demonstrate that Kaizen with some\nrange of EMA hyper-parameters is able to stabilize the training and improve overall performance.\n5.8 Efﬁciency\nTable 2 shows the reported training time of different semi- and unsupervised methods to fully converge. slimIPL has\na clear advantage in training time and resource consumption. Among other things, we attribute this to the dynamic\npseudo-label cache and the stability of the algorithm.\n5.9 Conversational Speech\nTo analyse applicability of slimIPL and found hyperparameters to other data we perform experiments on the conversa-\ntional telephone speech, SwitchBoard & Fisher. To create a training set, we combine Switchboard (SB) [48] (300h) as\nlabeled data and Fisher (FS) [49, 50] (2k hours) as unlabeled data. We use RT-03S [51] as the validation set; test sets\nare the Hub5 Eval2000 [52] data with two subsets, SwitchBoard (SB) and CallHome (CH). For the data processing and\nevaluation, we follow the recipe provided by Kaldi [53]. Original sample rate of 8kHz is preserved. We use the same\nmodel architecture and optimization strategy as for LibriSpeech experiments from Section 5.2. Dropout for supervised\nbaseline is set to 0.3 and decreased to 0.1 during training on unlabeled data. For slimIPL hyperparameters we use\nsimilar values found in LibriSpeech experiments: C = 1000, M = 20000, p= 0.1, λ= 1. For SpecAugment we use\nthe Switchboard setting from [27]: two frequency masks with frequency mask parameter F = 15, two time masks with\ntime mask parameter T = 70and maximum time-mask ratio p= 0.2; time warping is not used. Average batch size is\n38 per GPU.\nFisher data contains samples with silence only. From experiments we noticed that these samples used in training can\ncause instability and slow divergence at the end of training. Closer to end of training, their negative contribution rises\nas the model tends to generate empty pseudo-labels more and more frequently. After ﬁltering original Fisher data to\nexclude samples with empty transcriptions, training is stable and no side-effects are observed.\nTable 5 compares results for supervised baseline, slimIPL and RASR [47] (supervised baseline trained on both SB and\nFS), which share the same model architecture except for dropout rates. slimIPL outperforms fully supervised baseline\n(RASR) on RT-03S and in the same ballpark for Hub5 Eval2000 SwitchBoard set. Thus, we conclude that slimIPL and\nits hyperparameters are readily applicable to conversational speech.\n7\nA PREPRINT - AUGUST 31, 2021\nTable 5: Comparison between supervised and slimIPL models on SwitchBoard.\nMethod Sup. Data Unsup. Data Stride Tokens Criterion LM Dev WER Test WER\nRT03S SB FS\nOur baseline SB - 30ms letters CTC - 17.5 9.6 19.3\nword 4-gram 14.5 8.0 16.7\nRASR [47] SB+FS - 30 ms letters CTC - 12.0 6.9 11.4\nword 4-gram 10.4 6.5 10.3\nslimIPL SB FS 30ms letters CTC - 11.8 7.9 14.2\nword 4-gram 9.6 6.8 12.1\n6 Conclusion\nWe revisit a key component of recent pseudo-labeling success in ASR, beam-search decoding with an LM, and\npropose slimIPL, in which a single model iteratively re-generates hard pseudo-labels with a dynamic cache to stabilize\noptimization. slimIPL is robust to hyperparameter changes and substantially simpliﬁes training compared to other\nsemi/unsupervised approaches, while delivering competitive performance for low-resource settings on LibriSpeech test\nsets. For inference, slimIPL is less prone to LM over-ﬁtting than methods which use an LM for PL generation.\n7 Acknowledgement\nWe thank Alex Rogozhnikov for insightful discussions about the algorithm and experiments, and Gil Keren for results\ndiscussions.\nReferences\n[1] O. Chapelle, B. Sch ¨olkopf, and A. Zien, Semi-supervised Learning. Mit Press, 2010.\n[2] J. Kahn, M. Rivi`ere, W. Zheng, E. Kharitonov, Q. Xu, P.-E. Mazar´e, J. Karadayi, V . Liptchinsky, R. Collobert,\nC. Fuegen et al., “Libri-light: A benchmark for asr with limited or no supervision,” in ICASSP 2020-2020 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 7669–7673.\n[3] H. Scudder, “Probability of error of some adaptive pattern-recognition machines,”IEEE Transactions on Informa-\ntion Theory, vol. 11, no. 3, pp. 363–371, 1965.\n[4] W.-N. Hsu, A. Lee, G. Synnaeve, and A. Hannun, “Semi-supervised speech recognition via local prior matching,”\narXiv preprint arXiv:2002.10336, 2020.\n[5] Q. Xu, T. Likhomanenko, J. Kahn, A. Hannun, G. Synnaeve, and R. Collobert, “Iterative pseudo-labeling for\nspeech recognition,” Proc. Interspeech 2020, pp. 1006–1010, 2020.\n[6] Q. Xu, A. Baevski, T. Likhomanenko, P. Tomasello, A. Conneau, R. Collobert, G. Synnaeve, and M. Auli, “Self-\ntraining and pre-training are complementary for speech recognition,” inICASSP 2021-2021 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 3030–3034.\n[7] J. Kahn, A. Lee, and A. Hannun, “Self-training for end-to-end speech recognition,” inICASSP 2020-2020 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 7084–7088.\n[8] G. Synnaeve, Q. Xu, J. Kahn, T. Likhomanenko, E. Grave, V . Pratap, A. Sriram, V . Liptchinsky, and R. Collobert,\n“End-to-end asr: from supervised to semi-supervised learning with modern architectures,” in Workshop on\nSelf-supervision in Audio and Speech, ICML, 2020.\n[9] D. S. Park, Y . Zhang, Y . Jia, W. Han, C.-C. Chiu, B. Li, Y . Wu, and Q. V . Le, “Improved noisy student training for\nautomatic speech recognition,” Proc. Interspeech 2020, pp. 2817–2821, 2020.\n[10] Y . Zhang, J. Qin, D. S. Park, W. Han, C.-C. Chiu, R. Pang, Q. V . Le, and Y . Wu, “Pushing the limits of\nsemi-supervised learning for automatic speech recognition,” arXiv preprint arXiv:2010.10504, 2020.\n[11] A. Graves, S. Fern ´andez, F. Gomez, and J. Schmidhuber, “Connectionist temporal classiﬁcation: labelling\nunsegmented sequence data with recurrent neural networks,” in Proceedings of the 23rd international conference\non Machine learning, 2006, pp. 369–376.\n[12] I. Z. Yalniz, H. J´egou, K. Chen, M. Paluri, and D. Mahajan, “Billion-scale semi-supervised learning for image\nclassiﬁcation,” arXiv preprint arXiv:1905.00546, 2019.\n8\nA PREPRINT - AUGUST 31, 2021\n[13] Q. Xie, M.-T. Luong, E. Hovy, and Q. V . Le, “Self-training with noisy student improves imagenet classiﬁcation,” in\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 10 687–10 698.\n[14] D. Yarowsky, “Unsupervised word sense disambiguation rivaling supervised methods,” in33rd annual meeting of\nthe association for computational linguistics, 1995, pp. 189–196.\n[15] D. McClosky, E. Charniak, and M. Johnson, “Effective self-training for parsing,” inProceedings of the Human\nLanguage Technology Conference of the NAACL, Main Conference, 2006, pp. 152–159.\n[16] R. Reichart and A. Rappoport, “Self-training for enhancement and domain adaptation of statistical parsers trained\non small datasets,” inProceedings of the 45th Annual Meeting of the Association of Computational Linguistics,\n2007, pp. 616–623.\n[17] Z. Huang and M. Harper, “Self-training pcfg grammars with latent annotations across languages,” inProceedings\nof the 2009 conference on empirical methods in natural language processing, 2009, pp. 832–841.\n[18] J. He, J. Gu, J. Shen, and M. Ranzato, “Revisiting self-training for neural sequence generation,” in International\nConference on Learning Representations, 2020.\n[19] N. Uefﬁng, “Using monolingual source-language data to improve mt performance,” inInternational Workshop on\nSpoken Language Translation (IWSLT) 2006, 2006.\n[20] J. Zhang and C. Zong, “Exploiting source-side monolingual data in neural machine translation,” in Proceedings of\nthe 2016 Conference on Empirical Methods in Natural Language Processing, 2016, pp. 1535–1545.\n[21] S. Novotney and R. Schwartz, “Analysis of low-resource acoustic model self-training,” inTenth Annual Conference\nof the International Speech Communication Association, 2009.\n[22] S. H. K. Parthasarathi and N. Strom, “Lessons from building acoustic models with a million hours of speech,”\nin ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2019, pp. 6670–6674.\n[23] J. Pino, Q. Xu, X. Ma, M. J. Dousti, and Y . Tang, “Self-training for end-to-end speech translation,” Proc.\nInterspeech 2020, pp. 1476–1480, 2020.\n[24] D.-H. Lee, “Pseudo-label: The simple and efﬁcient semi-supervised learning method for deep neural networks,” in\nWorkshop on challenges in representation learning, ICML, vol. 3, no. 2, 2013.\n[25] E. Arazo, D. Ortego, P. Albert, N. E. O’Connor, and K. McGuinness, “Pseudo-labeling and conﬁrmation bias in\ndeep semi-supervised learning,” in2020 International Joint Conference on Neural Networks (IJCNN). IEEE,\n2020, pp. 1–8.\n[26] Y . Chen, W. Wang, and C. Wang, “Semi-supervised asr by end-to-end self-training,”Proc. Interspeech 2020, pp.\n2787–2791, 2020.\n[27] D. S. Park, W. Chan, Y . Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk, and Q. V . Le, “Specaugment: A simple data\naugmentation method for automatic speech recognition,” Proc. Interspeech 2019, pp. 2613–2617, 2019.\n[28] A. Baevski, Y . Zhou, A. Mohamed, and M. Auli, “wav2vec 2.0: A framework for self-supervised learning of\nspeech representations,” Advances in Neural Information Processing Systems, vol. 33, 2020.\n[29] A. v. d. Oord, Y . Li, and O. Vinyals, “Representation learning with contrastive predictive coding,”arXiv preprint\narXiv:1807.03748, 2018.\n[30] C. Talnikar, T. Likhomanenko, R. Collobert, and G. Synnaeve, “Joint masked cpc and ctc training for asr,” in\nICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE,\n2021, pp. 3045–3049.\n[31] R. Sennrich, B. Haddow, and A. Birch, “Improving neural machine translation models with monolingual data,”\nin Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), 2016, pp. 86–96.\n[32] K. Sohn, D. Berthelot, N. Carlini, Z. Zhang, H. Zhang, C. A. Raffel, E. D. Cubuk, A. Kurakin, and C.-L.\nLi, “Fixmatch: Simplifying semi-supervised learning with consistency and conﬁdence,” Advances in Neural\nInformation Processing Systems, vol. 33, 2020.\n[33] D. Berthelot, N. Carlini, I. Goodfellow, N. Papernot, A. Oliver, and C. A. Raffel, “Mixmatch: A holistic approach\nto semi-supervised learning,” in Advances in Neural Information Processing Systems, vol. 32, 2019.\n[34] D. Berthelot, N. Carlini, E. D. Cubuk, A. Kurakin, K. Sohn, H. Zhang, and C. Raffel, “Remixmatch: Semi-\nsupervised learning with distribution alignment and augmentation anchoring,”arXiv preprint arXiv:1911.09785,\n2019.\n9\nA PREPRINT - AUGUST 31, 2021\n[35] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. J ´egou, “Training data-efﬁcient image\ntransformers & distillation through attention,” in International Conference on Machine Learning. PMLR, 2021,\npp. 10 347–10 357.\n[36] K. Imamura, A. Fujita, and E. Sumita, “Enhancement of encoder and attention using target monolingual corpora in\nneural machine translation,” in Proceedings of the 2nd Workshop on Neural Machine Translation and Generation,\n2018, pp. 55–63.\n[37] S. Edunov, M. Ott, M. Auli, and D. Grangier, “Understanding back-translation at scale,” inProceedings of the\n2018 Conference on Empirical Methods in Natural Language Processing, 2018, pp. 489–500.\n[38] C. L¨uscher, E. Beck, K. Irie, M. Kitza, W. Michel, A. Zeyer, R. Schl ¨uter, and H. Ney, “Rwth asr systems for\nlibrispeech: Hybrid vs attention,” Proc. Interspeech 2019, pp. 231–235, 2019.\n[39] S. Ling, Y . Liu, J. Salazar, and K. Kirchhoff, “Deep contextualized acoustic representations for semi-supervised\nspeech recognition,” in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP). IEEE, 2020, pp. 6429–6433.\n[40] V . Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Librispeech: an asr corpus based on public domain audio\nbooks,” in 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE,\n2015, pp. 5206–5210.\n[41] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,Ł. Kaiser, and I. Polosukhin, “Attention\nis all you need,” in Advances in neural information processing systems, 2017, pp. 5998–6008.\n[42] A. Fan, E. Grave, and A. Joulin, “Reducing transformer depth on demand with structured dropout,” inInternational\nConference on Learning Representations, 2020.\n[43] J. Duchi, E. Hazan, and Y . Singer, “Adaptive subgradient methods for online learning and stochastic optimization,”\nJournal of machine learning research, vol. 12, no. Jul, pp. 2121–2159, 2011.\n[44] R. Collobert, C. Puhrsch, and G. Synnaeve, “Wav2letter: an end-to-end convnet-based speech recognition system,”\narXiv preprint arXiv:1609.03193, 2016.\n[45] T. Likhomanenko, G. Synnaeve, and R. Collobert, “Who needs words? lexicon-free speech recognition,”Proc.\nInterspeech 2019, pp. 3915–3919, 2019.\n[46] V . Manohar, T. Likhomanenko, Q. Xu, W.-N. Hsu, R. Collobert, Y . Saraf, G. Zweig, and A. Mohamed, “Kaizen:\nContinuously improving teacher using exponential moving average for semi-supervised speech recognition,”arXiv\npreprint arXiv:2106.07759, 2021.\n[47] T. Likhomanenko, Q. Xu, V . Pratap, P. Tomasello, J. Kahn, G. Avidov, R. Collobert, and G. Synnaeve, “Rethinking\nevaluation in asr: Are our models robust enough?” arXiv preprint arXiv:2010.11745, 2020.\n[48] J. Godfrey and E. Holliman, “Switchboard-1 release 2 LDC97S62,” Philadelphia: LDC, 1993.\n[49] C. Cieri, , D. Graff, O. Kimball, D. Miller, and K. Walker, “Fisher english training speech parts 1 and 2 transcripts\nLDC200{4,5}T19,” Philadelphia: LDC, 2004, 2005.\n[50] C. Cieri, D. Miller, and K. Walker, “Fisher english training speech parts 1 and 2 LDC200{4,5}S13,” Philadelphia:\nLDC, 2004, 2005.\n[51] J. G. Fiscus et al., “2003 nist rich transcription evaluation data LDC2007S10,” Web Download. Philadelphia:\nLDC, 2007.\n[52] LDC et al., “2000 hub5 english evaluation speech LDC2002S09 and transcripts LDC2002T43,”Web Download.\nPhiladelphia: LDC, 2002.\n[53] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann, P. Motlicek, Y . Qian,\nP. Schwarzet al., “The kaldi speech recognition toolkit,” inIEEE 2011 workshop on automatic speech recognition\nand understanding, no. CONF. IEEE Signal Processing Society, 2011.\n10"
}