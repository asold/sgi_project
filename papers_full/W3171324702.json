{
    "title": "Assertion Detection in Clinical Notes: Medical Language Models to the Rescue?",
    "url": "https://openalex.org/W3171324702",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2890591483",
            "name": "Betty van Aken",
            "affiliations": [
                "Berliner Hochschule für Technik"
            ]
        },
        {
            "id": "https://openalex.org/A3168308749",
            "name": "Ivana Trajanovska",
            "affiliations": [
                "Berliner Hochschule für Technik"
            ]
        },
        {
            "id": "https://openalex.org/A2151966963",
            "name": "Amy Siu",
            "affiliations": [
                "Berliner Hochschule für Technik"
            ]
        },
        {
            "id": "https://openalex.org/A3081902417",
            "name": "Manuel Mayrdorfer",
            "affiliations": [
                "Charité - Universitätsmedizin Berlin"
            ]
        },
        {
            "id": "https://openalex.org/A2098307559",
            "name": "Klemens Budde",
            "affiliations": [
                "Charité - Universitätsmedizin Berlin"
            ]
        },
        {
            "id": "https://openalex.org/A3166394676",
            "name": "Alexander Loeser",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2971258845",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W3031105340",
        "https://openalex.org/W3034238904",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2566847560",
        "https://openalex.org/W2949550698",
        "https://openalex.org/W2911321984",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W1964625659",
        "https://openalex.org/W2139865360",
        "https://openalex.org/W2963716420",
        "https://openalex.org/W2925863688",
        "https://openalex.org/W1982464493",
        "https://openalex.org/W3100452049",
        "https://openalex.org/W2116446440",
        "https://openalex.org/W2104381725",
        "https://openalex.org/W1997612497",
        "https://openalex.org/W3021523644",
        "https://openalex.org/W3153672153",
        "https://openalex.org/W2396881363"
    ],
    "abstract": "In order to provide high-quality care, health professionals must efficiently identify the presence, possibility, or absence of symptoms, treatments and other relevant entities in free-text clinical notes. Such is the task of assertion detection - to identify the assertion class (present, possible, absent) of an entity based on textual cues in unstructured text. We evaluate state-of-the-art medical language models on the task and show that they outperform the baselines in all three classes. As transferability is especially important in the medical domain we further study how the best performing model behaves on unseen data from two other medical datasets. For this purpose we introduce a newly annotated set of 5,000 assertions for the publicly available MIMIC-III dataset. We conclude with an error analysis that reveals situations in which the models still go wrong and points towards future research directions.",
    "full_text": "Proceedings of the Second Workshop on Natural Language Processing for Medical Conversations, pages 35–40\nJuly 6, 2021. ©2021 Association for Computational Linguistics\n35\nAssertion Detection in Clinical Notes:\nMedical Language Models to the Rescue?\nBetty van Aken1, Ivana Trajanovska1, Amy Siu1,\nManuel Mayrdorfer2, Klemens Budde2 and Alexander Löser1\n1Beuth University of Applied Sciences Berlin, 2Charité Universitätsmedizin Berlin\nivtrajanovska@gmail.com\n{bvanaken, siu, aloeser}@beuth-hochschule.de\n{manuel.mayrdorfer, klemens.budde}@charite.de\nAbstract\nIn order to provide high-quality care, health\nprofessionals must efﬁciently identify the\npresence, possibility, or absence of symptoms,\ntreatments and other relevant entities in free-\ntext clinical notes. Such is the task of asser-\ntion detection – to identify the assertion class\n(present, possible, absent) of an entity based\non textual cues in unstructured text. We eval-\nuate state-of-the-art medical language models\non the task and show that they outperform the\nbaselines in all three classes. As transferability\nis especially important in the medical domain\nwe further study how the best performing\nmodel behaves on unseen data from two other\nmedical datasets. For this purpose we intro-\nduce a newly annotated set of 5,000 assertions\nfor the publicly available MIMIC-III dataset.\nWe conclude with an error analysis that reveals\nsituations in which the models still go wrong\nand points towards future research directions.\n1 Introduction\nThe clinical information buried in narrative reports\nis difﬁcult for humans to access for clinical, teach-\ning, or research purposes (Perera et al., 2013). To\nprovide high-quality patient care, health profes-\nsionals need to have better and faster access to cru-\ncial information in a summarized and interpretable\nformat. In this paper, we focus on English dis-\ncharge summaries and the task of assertion detec-\ntion, which is the classiﬁcation of clinical informa-\ntion as demonstrated in Figure 1.\nGiven a piece of text, we need to identify two\npieces of information – a medical entity and tex-\ntual cues indicating the presence or absence of\nthat entity. Medical entity extraction has been\nstudied extensively (Lewis et al., 2020), we thus\nfocus our work on the task of predicting the\npresent / possible / absent class over a medical en-\ntity, addressing an important information need of\nFigure 1: Sample output of our demo system. Detected\nentities are highlighted in red, yellow, and green to in-\ndicate present, possible, and absent.\nhealth professionals. This setting is reﬂected in\nthe dataset released by the 2010 i2b2 Challenge\nAssertions Task (de Bruijn et al., 2011a), on which\nwe base our main evaluation.\nClinical assertion detection is known to be a dif-\nﬁcult task (Chen, 2019) due to the free-text format\nof considered clinical notes. Detecting possible\nassertions is particularly challenging, because they\nare often vaguely expressed, and they occur far\nless frequently than present and absent assertions.\nLanguage models pre-trained on medical data have\nshown to create useful representations for a mul-\ntitude of tasks in the domain (Peng et al., 2019).\nWe apply them to our setup of assertion detection\nto evaluate whether they can increase performance\n(especially on the minority class) and where they\nstill need improvement.\nWe argue that clinical assertion detection mod-\nels must be transferable to data that differs from\nthe training data, e.g. due to different writing\nstyles of health professionals from other clinics\nor from other medical ﬁelds. As existing datasets\ndo not represent such diversity, we manually anno-\ntate 5,000 assertions in clinical notes from several\nﬁelds in the publicly available MIMIC-III dataset.\nWe then use these annotated notes as an additional\nevaluation set to test the transferability of the best\nperforming model.\n36\npresent possible absent\n2010 i2b2 Challenge Assertion Task discharge summaries 21,064 1,418 6,144\nBioScope scientiﬁc publications – 3,474 2,161\nMIMIC-III Clinical Database (New)\ndischarge summaries 2,610 250 980\nphysician letters 204 34 66\nnurse letters 293 14 59\nradiology reports 249 40 130\nTable 1: Distribution of text types and classes in the three employed datasets. Note thatpossible is a minority class\nacross datasets as well as text types. In the i2b2 dataset, for instance, only 5% of all labels are possible.\nOur contributions are summarized as follows:\n1) We evaluate medical language models on asser-\ntion detection in clinical notes and show that they\nclearly outperform previous baselines. We further\nstudy the transferability of such models to clinical\ntext from other medical areas.\n2) We manually annotate 5,000 assertions for the\nMIMIC-III Clinical Database (Johnson et al., 2016).\nWe release the annotations to the research commu-\nnity1 to tackle the problem of label sparsity and the\nlack of diversity in existing assertion data.\n3) We conduct an error analysis to understand the\ncapabilities of the best performing model on the\ntask and to reveal directions for improvement. We\nmake our system publicly available as a web appli-\ncation to allow further analyses2.\n2 Related Work\nOne of the earliest approaches to assertion de-\ntection is NegEx (Chapman et al., 2001), where\nhand-crafted word patterns are used to extract the\nabsent category of assertions in discharge sum-\nmaries. In 2010, the i2b2 Challenge Assertions\nTask (de Bruijn et al., 2011a) was introduced, and\nan accompanying corpus was released.\nThere is a variety of prior work focused on scope\nresolution for assertions, which differs from our set-\nting in that it does not consider medical concepts\nbut scopes of a certain assertion cue. Representa-\ntive current approaches for this task setup include\na CNN-based (Convolutional Neural Network) one\nby Qian et al. (2016), reaching an F1 of 0.858 on\nthe more challenging possible category. Sergeeva\net al. (2019) propose a LSTM-based (Long Short-\nTerm Memory) approach to detect only absent\n1Annotated data available at:\nhttps://github.com/bvanaken/\nclinical-assertion-data\n2Demo application:\nhttps://ehr-assertion-detection.demo.\ndatexis.com\nscopes. When “gold negation cues” are made avail-\nable to the model and synthetic features are applied,\nan F1 of 0.926 is reached. NegBert (Khandelwal\nand Sawant, 2020) is another approach to detectab-\nsent scopes. As its name suggests, it is BERT-based\nand reaches an F1 of 0.957 on BioScope abstracts.\nIn contrast to these approaches we focus our\nwork on entity-speciﬁc assertion detection, the re-\nsults of which are of more practical help for sup-\nporting health professionals. Bhatia et al. (2019)\nexplored extracting entities and negations in a joint\nsetting, whereas the work of Harkema et al. (2009),\nChen (2019) and de Bruijn et al. (2011a) is the\nclosest to our task setup, i.e. labelling entities with\nan assertion class. Harkema et al. (2009) extended\nthe NexEx algorithm with contextual properties.\nde Bruijn et al. (2011a) use a simple SVM classi-\nﬁer and Chen (2019) apply a bidirectional LSTM\nmodel with attention to the task and evaluate it\non the i2b2 corpus. While these models reach\nF1-scores above 0.9 on the majority classes, the\nchallenging possible class does not surpass 0.65.\nWe show that medical language models outperform\nthese scores especially regarding the minority class.\nFurthermore, Wu et al. (2014) compared then\nstate-of-the-art approaches for negation detection\nand found a lack of generalisation to arbitrary clin-\nical text. We thus want to examine the transfer\ncapabilities of recent language models to under-\nstand whether they can mitigate the phenomenon.\n3 Methodology\nWe want to understand the abilities of medical lan-\nguage models on the task of assertion detection. We\nhence ﬁne-tune various (medical) language models\non the i2b2 corpus described below. We further\napply the best performing model to the BioScope\ndataset and our newly introduced MIMIC-III asser-\ntion dataset without further ﬁne-tuning to test their\nperformance on unseen medical data.\n37\nModel F1 for\npresent possible absent\nEarlier approaches\nSVM Classiﬁer (de Bruijn et al., 2011b) 0.959 0.643 0.939\nConditional Softmax Shared Decoder (Bhatia et al., 2019) – – 0.905\nBi-directional LSTM with Attention (Chen, 2019) 0.950 0.637 0.927\nLanguage models under evaluation\nBERT Base (Devlin et al., 2019) 0.968 0.704 0.943\nBioBERT Base (Lee et al., 2020) 0.976 0.759 0.963\nBio+Clinical BERT (Alsentzer et al., 2019) 0.977 0.775 0.966\nBio+Discharge Summary BERT (Alsentzer et al., 2019) 0.979 0.786 0.972\nBio+Clinical Outcome Representations (CORe) (van Aken et al., 2021) 0.975 0.761 0.965\nBiomed RoBERTa Base (Gururangan et al., 2020) 0.976 0.723 0.967\nTable 2: Results of baseline approaches and (medical) language models on the i2b2 Assertions Task. Pre-trained\nmedical language models outperform all earlier approaches – with a large margin on the possible class. Note that\nBhatia et al. (2019) only evaluated their model on negation detection.\n3.1 Datasets\nThe 2010 i2b2 Assertion Task(de Bruijn et al.,\n2011a) provides a corpus of assertions in clinical\ndischarge summaries. The task is split into six\nclasses, namely present, possible, absent, hypothet-\nical, conditional and associated with someone else.\nHowever, the distribution is highly skewed, such\nthat only 6% of the assertions belong to the latter\nthree classes. Hence we only use the present, possi-\nble, and absent assertions for our evaluation as they\npresent the most important information for doctors.\nBioScope (Vincze et al., 2008) is a corpus of\nassertions in biomedical publications. It was specif-\nically curated for the study of negation and specu-\nlation (or absent and possible in this paper) scope\nand does not contain present annotations. As men-\ntioned before, the BioScope dataset does not com-\npletely match the information need of health pro-\nfessionals and the i2b2 corpus lacks varied medical\ntext types. We thus introduce a new set of labelled\nassertions to complement existing data.\nThe MIMIC-III Clinical Database(Johnson\net al., 2016) provides texts from discharge sum-\nmaries as well as other clinical notes (physician\nletters, nurse letters, and radiology reports) repre-\nsenting a promising source of varied medical text.\nTherefore, two annotators followed the annotation\nguidelines from the i2b2 challenge, and labelled\n5,000 assertions, i.e. word spans of entities and\ntheir corresponding present / possible / absent class.\nThe inner-annotator agreement as Cohen’s kappa\ncoefﬁcient is 0.847, which indicates a strong level\nof agreement. The annotations were further veri-\nﬁed by a medical doctor, who provided feedback\nto correct a small number of labels, and conﬁrmed\nthat the end results were satisfactory.\nIt is important to note that even though the newly\nannotated data from MIMIC-III adds variation to\nthe existing corpora, the dataset has its own limita-\ntions. The clinical notes are collected from a single\ninstitution (with a mostly White patientpopulation)\nand from Intensive Care Unit patients only. We\ntherefore argue that progress in assertion detection\nrequires further initiatives for releasing more di-\nverse sets of clinical notes.\nTable 1 summarizes the assertion distribution in\nthe introduced datasets and shows the unbalanced\nnature of the data.\n3.2 Data Preprocessing\nWe make predictions about assertions on a per-\nentity level. However, we want our models to\nconsider the context of an entity. We therefore\npass the whole sentence to the models and sur-\nround the entity tokens with special indicator to-\nkens [entity] whose embeddings are randomly\ninitialised. A sample input sequence thus looks as\nfollows: [CLS] test results were negative\nfor [entity] COVID-19 [entity].\nWe apply the same pre-processing to all three\ndatasets.\n3.3 Fine-tuning Medical Language Models\nThere are various pre-trained (bio-)medical and\nclinical language models available to evaluate on\nthe assertion detection task. We select the most\nprevalent ones and describe them in short below:\n38\npresent possible absent\nBioScope\nscientiﬁc pub. – 0.593 0.845\nMIMIC-III\ndischarge sum. 0.951 0.663 0.939\nphys. letters 0.929 0.593 0.892\nnurse letters 0.967 0.710 0.900\nradio. reports 0.950 0.691 0.977\nTable 3: Experimental results (in F1) for the best per-\nforming Bio+Discharge Summary BERT model on two\nfurther assertion datasets and their different text types.\nBoth datasets were not seen during training. Note that\nthe number of evaluation samples is very low for some\ntext types (i.e. possible class in nurse letters), which\nimpairs the expressiveness of these results.\nBERT (Devlin et al., 2019) was pre-trained on\nnon-medical data and serves as a baseline for\nTransformer-base pre-trained language models.\nBioBERT (Lee et al., 2020) is a standard model\nfor medical NLP tasks and is pre-trained on bio-\nmedical publications. Bio+Clinical BERT and\nBio+Discharge Summary BERT(Alsentzer et al.,\n2019) are built upon BioBERT with additional pre-\ntraining on clinical notes / discharge summaries.\nThe CORe model (van Aken et al., 2021) uses\nBioBERT and adds a specialized clinical outcome\npre-training. Biomed RoBERTA (Gururangan\net al., 2020) is based on the RoBERTA model (Liu\net al., 2019) and pre-trained on bio-medical pub-\nlications. After an initial grid search we ﬁx our\nhyperparameters to a learning rate of 1e-5, batch\nsize of 32, and 2 epochs of training.\n4 Evaluation and Discussion\nWe start by evaluating the mentioned models on\nthe i2b2 corpus. We use training and test data\nas deﬁned by in the i2b2 challenge and compare\nour results to previous state-of-the-art approaches\nin Table 2. Next, we apply the best performing\nBio+Discharge Summary BERT to the BioScope\nand MIMIC-III corpora without additional ﬁne-\ntuning (Table 3). This way we can see the model’s\nperformance on medical text from unseen sources.\n4.1 Results\nLanguage models outperform baselines. Table 2\nshows that all evaluated medical language models\nare able to increase F1-scores on all three classes.\nOn the most challengingpossible class the improve-\nment is the clearest with up to∼15pp, which shows\nthat the models are better in handling sparse occur-\nrences coupled with vague expressions.\nMedical pre-training is important. The vanilla\nBERT baseline is the weakest of our evaluated mod-\nels, which shows that models specialized on the\nmedical domain are not only effective for more\ncomplex medical tasks but also for assertion detec-\ntion, which is in line with the claim by Gururangan\net al. (2020) that domain-speciﬁc pre-training is\nalmost always of use. Bio+Discharge Summary\nBERT is the best model – probably because it was\ntrained on text very similar to the i2b2 corpus.\nText style matters.Table 3 shows the ability of the\nBio+Discharge Summary BERT language model\nto transfer to other text styles. The assertions in\nthe BioScope corpus are difﬁcult to identify by the\nmodel as they clearly differ from the ones used by\ndoctors in clinical notes. The text style in MIMIC-\nIII data is more similar to the originally learned\ndata which is reﬂected in the results. 3 However,\nphysician letters appear to contain more special-\nized expressions and therefore evoke more errors.\nThis points towards a lack of generalization possi-\nbly caused by the limited variety of assertion cues\nin the training data.\n4.2 Error Analysis\nWe analyse all errors made by the best model to\nidentify main sources of errors and to point towards\nfuture research directions.\nInconsistent datain pre-existing datasets account\nfor roughly 45% of errors. This includes obvious\nlabelling mistakes, but also disagreements among\nannotators. For example, phrases such as “appeared\nto be,” “concerning for” and “consistent with” are\nlabeled differently, as present or as possible.\nLong range dependencies account for roughly\n20% of all errors, in which entities and their cues\nhave dependencies longer than a few tokens apart.\nWhile the model’s attention mechanism could eas-\nily detect distant tokens, the model might have\nlearned to only consider close assertion cues. The\nfollowing is an example of a distant cue indicating\nthe absent class which was missed by the model:\nHis rash on the right hand was examined further\nand is now resolved.\n3Note that the model’s pre-training is based on MIMIC-III\nand it was thus to an extent exposed to the test data. Due\nto the difference of the target task and the amount of total\npre-training data, this inﬂuence should be negligible.\n39\nLists of assertionsare found in 8% of error sam-\nples. Here the assertion is not directly coupled to\nan entity but must be inferred by the way it is listed.\nSuch somewhat ambiguous cases are usually easily\nunderstood by humans, but difﬁcult for our models.\nNo hydrocephalus, subarachnoid hemorrhage,\nno fracture.\nMisspellings account for 5% of all observed er-\nrors, but they reveal a critical yet surprising limita-\ntion. For instance, the cues “appeas” and “probalbe”\nthat indicate possible instances, are missed. While\nTransformer-based models are generally capable of\ndealing with misspellings due to subword tokeniza-\ntion, the missing variety of expressions in the data\nappears to let the models focus on a speciﬁc set of\ntextual cues without generalizing to new phrases or\neven misspellings.\n5 Conclusion and Future Work\nIn this work, we present an evaluation on medi-\ncal language models to detect assertions in clinical\ntexts and experimental results which show that they\noutperform baseline approaches. We further pro-\nvided a new corpus of assertion annotations on the\nMIMIC-III dataset that will augment existing data\ncollections and shows the model’s capability to be\ntransferred to other sources – if the text styles do\nnot strongly differ. We suggest future work to in-\nvestigate generalization to unseen data and expres-\nsions. We further encourage work on multi-task\nlearning of entity extraction and assertions to sup-\nport health professionals with systems that learn\njointly in an end-to-end fashion.\nAcknowledgments\nOur work is funded by the German Federal Min-\nistry for Economic Affairs and Energy (BMWi) un-\nder grant agreement 01MD19003B (PLASS) and\n01MK2008MD (Servicemeister).\nReferences\nEmily Alsentzer, John Murphy, William Boag, Wei-\nHung Weng, Di Jindi, Tristan Naumann, and\nMatthew McDermott. 2019. Publicly Available\nClinical BERT Embeddings. In Proceedings of the\n2nd Clinical Natural Language Processing Work-\nshop, pages 72–78.\nParminder Bhatia, Busra Celikkaya, and Mohammed\nKhalilia. 2019. Joint Entity Extraction and Asser-\ntion Detection for Clinical Text. In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 954–959.\nWendy W Chapman, Will Bridewell, Paul Hanbury,\nGregory F Cooper, and Bruce G Buchanan. 2001. A\nSimple Algorithm for Identifying Negated Findings\nand Diseases in Discharge Summaries. Journal of\nBiomedical Informatics, 34(5):301–310.\nLong Chen. 2019. Attention-based Deep Learning Sys-\ntem for Negation and Assertion Detection in Clini-\ncal Notes. International Journal of Artiﬁcial Intelli-\ngence and Applications, 10(1).\nBerry de Bruijn, Colin Cherry, Svetlana Kiritchenko,\nJoel Martin, and Xiaodan Zhu. 2011a. Machine-\nlearned Solutions for Three Stages of Clinical Infor-\nmation Extraction: The State of the Art at i2b2 2010.\nJournal of the American Medical Informatics Asso-\nciation, 18(5):557–562.\nBerry de Bruijn, Colin Cherry, Svetlana Kiritchenko,\nJoel D. Martin, and Xiaodan Zhu. 2011b. Machine-\nlearned solutions for three stages of clinical informa-\ntion extraction: the state of the art at i2b2 2010. J.\nAm. Medical Informatics Assoc., 18(5):557–562.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, Volume 1 (Long\nand Short Papers) , pages 4171–4186. Association\nfor Computational Linguistics.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of ACL.\nHenk Harkema, John N. Dowling, Tyler Thornblade,\nand Wendy Webber Chapman. 2009. Context: An al-\ngorithm for determining negation, experiencer, and\ntemporal status from clinical reports. J. Biomed. In-\nformatics, 42(5):839–851.\nAlistair EW Johnson, Tom J Pollard, Lu Shen,\nH Lehman Li-Wei, Mengling Feng, Moham-\nmad Ghassemi, Benjamin Moody, Peter Szolovits,\nLeo Anthony Celi, and Roger G Mark. 2016.\nMIMIC-III, a Freely Accessible Critical Care\nDatabase. Scientiﬁc Data, 3(1):1–9.\nAditya Khandelwal and Suraj Sawant. 2020. Neg-\nBERT: A Transfer Learning Approach for Negation\nDetection and Scope Resolution. In Proceedings of\nThe 12th Language Resources and Evaluation Con-\nference, pages 5739–5748.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So,\nand Jaewoo Kang. 2020. Biobert: a pre-trained\nbiomedical language representation model for\n40\nbiomedical text mining. Bioinform., 36(4):1234–\n1240.\nPatrick S. H. Lewis, Myle Ott, Jingfei Du, and Veselin\nStoyanov. 2020. Pretrained language models for\nbiomedical and clinical tasks: Understanding and\nextending the state-of-the-art. In Proceedings of\nthe 3rd Clinical Natural Language Processing Work-\nshop, ClinicalNLP@EMNLP 2020, Online, Novem-\nber 19, 2020, pages 146–157. Association for Com-\nputational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining ap-\nproach. CoRR, abs/1907.11692.\nYifan Peng, Shankai Yan, and Zhiyong Lu. 2019.\nTransfer learning in biomedical natural language\nprocessing: An evaluation of BERT and elmo on ten\nbenchmarking datasets. In Proceedings of the 18th\nBioNLP Workshop and Shared Task, BioNLP@ACL\n2019, Florence, Italy, August 1, 2019 , pages 58–65.\nAssociation for Computational Linguistics.\nSujan Perera, Amit Sheth, Krishnaprasad\nThirunarayan, Suhas Nair, and Neil Shah. 2013.\nChallenges in Understanding Clinical Notes: Why\nNLP Engines Fall Short and Where Background\nKnowledge Can Help. In Proceedings of the 2013\nInternational Workshop on Data Management &\nAnalytics for Healthcare, page 21–26.\nZhong Qian, Peifeng Li, Qiaoming Zhu, Guodong\nZhou, Zhunchen Luo, and Wei Luo. 2016. Specu-\nlation and Negation Scope Detection via Convolu-\ntional Neural Networks. In Proceedings of the 2016\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 815–825.\nElena Sergeeva, Henghui Zhu, Peter Prinsen, and Amir\nTahmasebi. 2019. Negation Scope Detection in\nClinical Notes and Scientiﬁc Abstracts: A Feature-\nenriched LSTM-based Approach. AMIA Summits\non Translational Science Proceedings, 2019:212.\nBetty van Aken, Jens-Michalis Papaioannou, Manuel\nMayrdorfer, Klemens Budde, Felix A. Gers, and\nAlexander Löser. 2021. Clinical outcome prediction\nfrom admission notes using self-supervised knowl-\nedge integration. In Proceedings of the 16th Confer-\nence of the European Chapter of the Association for\nComputational Linguistics, EACL 2021. Association\nfor Computational Linguistics.\nVeronika Vincze, György Szarvas, Richárd Farkas,\nGyörgy Móra, and János Csirik. 2008. The Bio-\nScope Corpus: Biomedical Texts Annotated for Un-\ncertainty, Negation and Their Scopes. BMC bioin-\nformatics, 9(11):1–9.\nStephen Wu, Timothy Miller, James Masanz, Matt\nCoarr, Scott Halgrim, David Carrell, and Cheryl\nClark. 2014. Negation’s not solved: generalizabil-\nity versus optimizability in clinical natural language\nprocessing. PLoS One, 11(9)."
}