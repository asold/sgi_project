{
    "title": "Symbolic Music Generation with Transformer-GANs",
    "url": "https://openalex.org/W3108047681",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A3109225483",
            "name": "Aashiq Muhamed",
            "affiliations": [
                "Amazon (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2097837833",
            "name": "Liang Li",
            "affiliations": [
                "Amazon (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2226675374",
            "name": "Xingjian Shi",
            "affiliations": [
                "Amazon (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A3069598706",
            "name": "Suri Yaddanapudi",
            "affiliations": [
                "Amazon (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2810102119",
            "name": "Wayne Chi",
            "affiliations": [
                "Amazon (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2640348624",
            "name": "Dylan Jackson",
            "affiliations": [
                "Amazon (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2235745410",
            "name": "Rahul Suresh",
            "affiliations": [
                "Amazon (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A1243308565",
            "name": "Zachary C. Lipton",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A4261757786",
            "name": "Alex J. Smola",
            "affiliations": [
                "Amazon (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2226675374",
            "name": "Xingjian Shi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1243308565",
            "name": "Zachary C. Lipton",
            "affiliations": [
                "Amazon (United States)",
                "Carnegie Mellon University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2950304420",
        "https://openalex.org/W3014731244",
        "https://openalex.org/W2772474126",
        "https://openalex.org/W6735913928",
        "https://openalex.org/W2757836268",
        "https://openalex.org/W2898148140",
        "https://openalex.org/W6666761814",
        "https://openalex.org/W2962753370",
        "https://openalex.org/W2908747729",
        "https://openalex.org/W2885835225",
        "https://openalex.org/W6679436768",
        "https://openalex.org/W6749669830",
        "https://openalex.org/W2119717200",
        "https://openalex.org/W6756140313",
        "https://openalex.org/W2523469089",
        "https://openalex.org/W3038344852",
        "https://openalex.org/W2962879692",
        "https://openalex.org/W2964121592",
        "https://openalex.org/W2964268978",
        "https://openalex.org/W3035214886",
        "https://openalex.org/W2970454257",
        "https://openalex.org/W648786980",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4300936792",
        "https://openalex.org/W2963925437",
        "https://openalex.org/W3103619945",
        "https://openalex.org/W2963418779",
        "https://openalex.org/W2736601468",
        "https://openalex.org/W2810518847",
        "https://openalex.org/W2898827701",
        "https://openalex.org/W4288348623",
        "https://openalex.org/W2946468379",
        "https://openalex.org/W2936695845",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W4293343253",
        "https://openalex.org/W2963878748",
        "https://openalex.org/W3179455893",
        "https://openalex.org/W2547875792",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W4230563027",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3045682378",
        "https://openalex.org/W4287995946",
        "https://openalex.org/W2099471712",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2947143100",
        "https://openalex.org/W2898718449",
        "https://openalex.org/W2565378226",
        "https://openalex.org/W3003673875",
        "https://openalex.org/W4320013936",
        "https://openalex.org/W2752134738",
        "https://openalex.org/W4323654151",
        "https://openalex.org/W2619808423",
        "https://openalex.org/W2963681776",
        "https://openalex.org/W3034775979",
        "https://openalex.org/W3043786446",
        "https://openalex.org/W1832693441",
        "https://openalex.org/W2792795990",
        "https://openalex.org/W3008814843",
        "https://openalex.org/W2938704169",
        "https://openalex.org/W2963248348",
        "https://openalex.org/W3036882457",
        "https://openalex.org/W4295521014",
        "https://openalex.org/W4294294142",
        "https://openalex.org/W3011411500",
        "https://openalex.org/W2950547518",
        "https://openalex.org/W2176263492",
        "https://openalex.org/W2807747378",
        "https://openalex.org/W4287817357",
        "https://openalex.org/W4297747548",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2991108091",
        "https://openalex.org/W2963073614",
        "https://openalex.org/W2963408210"
    ],
    "abstract": "Autoregressive models using Transformers have emerged as the dominant approach for music generation with the goal of synthesizing minute-long compositions that exhibit large-scale musical structure. These models are commonly trained by minimizing the negative log-likelihood (NLL) of the observed sequence in an autoregressive manner. Unfortunately, the quality of samples from these models tends to degrade significantly for long sequences, a phenomenon attributed to exposure bias. Fortunately, we are able to detect these failures with classifiers trained to distinguish between real and sampled sequences, an observation that motivates our exploration of adversarial losses to complement the NLL objective. We use a pre-trained Span-BERT model for the discriminator of the GAN, which in our experiments helped with training stability. We use the Gumbel-Softmax trick to obtain a differentiable approximation of the sampling process. This makes discrete sequences amenable to optimization in GANs. In addition, we break the sequences into smaller chunks to ensure that we stay within a given memory budget. We demonstrate via human evaluations and a new discriminative metric that the music generated by our approach outperforms a baseline trained with likelihood maximization, the state-of-the-art Music Transformer, and other GANs used for sequence generation. 57% of people prefer music generated via our approach while 43% prefer Music Transformer.",
    "full_text": "Symbolic Music Generation with Transformer-GANs\nAashiq Muhamed1\u0003\n, Liang Li1\u0003\n, Xingjian Shi1, Suri Yaddanapudi1, Wayne Chi1,\nDylan Jackson1, Rahul Suresh1, Zachary C. Lipton2, Alexander J. Smola1\n1 Amazon Web Services\n2 Carnegie Mellon University\nfmuhaaash, mzliang, xjshi, yaddas, waynchi, jacdylan, surerahug@amazon.com\nzlipton@cmu.edu, alex@smola.org\nAbstract\nAutoregressive models using Transformers have emerged as\nthe dominant approach for music generation with the goal\nof synthesizing minute-long compositions that exhibit large-\nscale musical structure. These models are commonly trained\nby minimizing the negative log-likelihood (NLL) of the ob-\nserved sequence in an autoregressive manner. Unfortunately,\nthe quality of samples from these models tends to degrade\nsigniﬁcantly for long sequences, a phenomenon attributed to\nexposure bias. Fortunately, we are able to detect these fail-\nures with classiﬁers trained to distinguish between real and\nsampled sequences, an observation that motivates our explo-\nration of adversarial losses to complement the NLL objective.\nWe use a pre-trained Span-BERT model for the discriminator\nof the GAN, which in our experiments helped with training\nstability. We use the Gumbel-Softmax trick to obtain a differ-\nentiable approximation of the sampling process. This makes\ndiscrete sequences amenable to optimization in GANs. In ad-\ndition, we break the sequences into smaller chunks to ensure\nthat we stay within a given memory budget. We demonstrate\nvia human evaluations and a new discriminative metric that\nthe music generated by our approach outperforms a baseline\ntrained with likelihood maximization, the state-of-the-art Mu-\nsic Transformer, and other GANs used for sequence genera-\ntion. 57% of people prefer music generated via our approach\nwhile 43% prefer Music Transformer.\nIntroduction\nAt present, neural sequence models are generally trained to\nmaximize the likelihood of the observed sequences. This en-\nsures statistical consistency but it can lead to undesirable\nartifacts when generating long sequences. While these ar-\ntifacts are difﬁcult to suppress with maximum likelihood\ntraining alone, they are easily detected by most sequence\nclassiﬁers. We take advantage of this fact, incorporating an\nadversarial loss derived from GANs. To illustrate its bene-\nﬁts, we demonstrate improvements in the context of sym-\nbolic music generation.\nGenerative modeling as a ﬁeld has progressed signiﬁ-\ncantly in recent years, particularly with respect to creative\napplications such as art and music (Briot, Hadjeres, and Pa-\nchet 2017; Carnovalini and Rod`a 2020; Anantrasirichai and\n\u0003Equal contribution, corresponding authors.\nCopyright c\r2021, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nBull 2020). A popular application is the generation of sym-\nbolic music, a task that presents unique challenges not found\nin text generation due to polyphony and rhythm. At the same\ntime, generating symbolic music can be simpler than audio\ngeneration due to the higher level of abstraction. Many lan-\nguage models from the NLP literature have been applied and\nextended to music generation. Since we build on this line of\nwork, we use the termssequence models and language mod-\nels interchangeably throughout, depending on the context in\nwhich a model is mentioned.\nNeural models for music sequences convert a digital rep-\nresentation of a musical score into a time-ordered sequence\nof discrete tokens. Language models are then trained on the\nevent sequences with the objective of maximizing the likeli-\nhood of the data. Music can then be generated by sampling\nor beam-decoding from this model. Recent advancements in\nNatural Language Processing (NLP), especially the atten-\ntion mechanism and the Transformer architecture (Vaswani\net al. 2017), have helped advance state of the art in symbolic\nmusic generation (Huang et al. 2018; Payne 2019; Donahue\net al. 2019). Music Transformer (Huang et al. 2018) and\nMuseNet (Payne 2019) use relative attention and sparse ker-\nnels (Child et al. 2019) respectively to remember long-term\nstructure in the composition. More recent works in music\ngeneration (Donahue et al. 2019; Huang and Yang 2020;\nWu, Wang, and Lei 2020) adopt the TransformerXL archi-\ntecture (Dai et al. 2019) which uses recurrent memory to\nattend beyond a ﬁxed context.\nDespite recent improvements, these approaches exhibit\ncrucial failure modes which we argue arise from the training\nobjective; Music Transformer (Huang et al. 2018) occasion-\nally forgets to switch off notes and loses coherence beyond a\nfew target lengths as stated by the authors. Sometimes it pro-\nduces highly repetitive songs, sections that are almost empty,\nand discordant jumps between contrasting phrases and mo-\ntifs. Consequently, music generated by such models can be\ndistinguished from real music by a simple classiﬁer. This\nsuggests that a distribution distance, such as the discrimi-\nnative objective of a GAN (Goodfellow et al. 2014) should\nimprove the ﬁdelity of the generative model.\nUnfortunately, incorporating GAN losses for discrete se-\nquences can be difﬁcult. Computing the derivative of the\nsamples through the discrete sampling process is challeng-\ning. As such, many models (de Masson d’Autume et al.\nTheThi rty-Fi fth AAA ICon ferenceon A rti fi ci al Intellig ence(AAAI-21)\n408\n2019; Nie, Narodytska, and Patel 2019) are limited to 20-\n40 token-length sentences, in contrast to the more than 1000\ntokens required for minutes-long musical compositions.\nWe leverage the Gumbel-Softmax (Kusner and Hern ´andez-\nLobato 2016) trick to obtain a differentiable approxima-\ntion of the sampling process. Moreover, we use the Trun-\ncated Backpropagation Through Time (TBPTT) (Sutskever,\nVinyals, and Le 2014) for gradient propagation on long se-\nquences. The latter keeps memory requirements at bay.\nRecent works on evaluation metrics in text genera-\ntion (Salazar et al. 2019; Zhang et al. 2019; Montahaei,\nAlihosseini, and Baghshah 2019) suggest that BERT-based\nscores (Devlin et al. 2018) are well correlated with human\nrankings and jointly measure quality and diversity. As BERT\nis trained using a self-supervised loss on bidirectional con-\ntexts of all attention layers, it can be an effective way of ex-\ntracting representations. We use them to obtain an effective\nmetric for the generative aspect of the model.\nExperiments show that the resulting Transformer-GAN\nimproves over its vanilla counterpart. We evaluate the per-\nformance using the music quality metric of (Briot, Hadjeres,\nand Pachet 2017) and a number of more conventional met-\nrics. In summary, our main contributions include:\n\u000f A novel Transformer-GAN approach for generating long\nmusic sequences of over 1000 tokens, using a pretrained\nSpanBERT as the discriminator.\n\u000f A detailed investigation of the inﬂuence of pretraining,\nloss functions, regularization, and number of frozen layers\nin the discriminator on music quality;\n\u000f A number of critical tricks for adversarial training;\n\u000f A classiﬁer-based metric to evaluate generative models.\nRelated Work\nGenerative models for sequences have a long history, from\nn-gram Markov-models to HMMs, to the more recent surge\nof neural sequence modeling research with the rise in popu-\nlarity of LSTMs in the early 2010s (Hochreiter and Schmid-\nhuber 1997; Sutskever, Vinyals, and Le 2014; Briot, Had-\njeres, and Pachet 2017). To apply sequential models to poly-\nphonic music, the musical score (or performance data) is\ntypically serialized into a single sequence by interleaving\ndifferent instruments or voices (Oore et al. 2018).\nOwing to their ability to model correlations at multiple\ntimescales over long sequences, self-attention -based archi-\ntectures are increasingly popular for generative music. Mod-\nels such as the Transformer (Vaswani et al. 2017) can ac-\ncess any part of its previously generated output, at every\nstep of generation. Two popular models, the Music Trans-\nformer (Huang et al. 2018) and MuseNet (Payne 2019) use\nTransformer decoders to generate music. Music Transformer\nuses the relative attention mechanism (Shaw, Uszkoreit, and\nVaswani 2018) to generate long-term music structure at the\nscale of 2000 tokens. MuseNet adds several learned embed-\ndings to guide the model to learn structural context. These\nembeddings were handcrafted to capture information related\nto chords and passage of time. Choi et al. (2019) uses Trans-\nformer encoders and decoders to harmonize or generate ac-\ncompaniments to a given melody.\nThese autoregressive models all follow the standard\nteacher forcing strategy where one trains always to pre-\ndict the next token, given a real sequence of the previ-\nous tokens as context. While models of the form p(x) =Q\nip(xi+1jx[1:i]) are statistically consistent, they suffer\nfrom an ampliﬁcation of prediction errors: when generating\na sequence, we end up conditioning on previously generated\nsequences (here, synthetic music) to produce the next note.\nThe problem emerges because we condition on data that is\ndistributionally unlike that seen at training time. This is a\nwell known problem in sequence modeling (Bengio et al.\n2015; Ranzato et al. 2015).\nGANs (Goodfellow et al. 2014) have been applied to sev-\neral domains as an alternative to maximum likelihood train-\ning, but directly applying GANs to sequence generation is\nknown to be difﬁcult (Lu et al. 2018). This is due to a number\nof reasons—training tends to be unstable, and these models\ntend to exhibit a phenomenon called mode collapse, where\npart of the input distribution’s support is not covered by the\ngenerative model.\nLanguage models are inherently discrete, since they in-\nvolve sampling from a multinomial distribution of to-\nkens. One option is to use an empirical average of\nEx\u0018p(x)[@\u0012log p\u0012(x)], i.e. of the gradient of the log-\nlikelihood of the data. This leads to the well-known REIN-\nFORCE algorithm (Williams 1992) and the application of\nReinforcement Learning machinery (Wu, Li, and Yu 2020;\nYu et al. 2017; Guo et al. 2018). It’s best to view the result-\ning problem as one of a sequential decision-making process\nwhere the generator is the agent, the state comprises the gen-\nerated tokens up to that time and the action is the next token\nto select. The generator is then trained via policy gradient\nwith several designed reward functions (Lu et al. 2018).\nAn alternative is to obtain a continuously differentiable\napproximation of the sampling process via the Gumbel-\nSoftmax (Kusner and Hern´andez-Lobato 2016). This yields\na distribution over the token probability simplex. Nie, Naro-\ndytska, and Patel (2019) combine this with a relational mem-\nory based generator (akin to memory networks) and multi-\nple embedded representations in the CNN discriminator. Wu\net al. (2020) propose a new variational GAN training frame-\nwork for text generation using a connection between GANs\nand reinforcement learning under a variational perspective.\nLastly, Zhang (2020) proposes an adversarial framework\nthat uses Transformers for both the generator and discrimi-\nnator. The author trained the GAN with a local and global\nreward function, noting that ”a speciﬁc note should be in\nharmony with in its local pattern and the whole sequence\nshould be in harmony with its global pattern”.\nOur proposed model combines GANs with Transformers\nto generate long, high-quality music sequences. It differs\nfrom previous works in the following ways:\n\u000f We use the Transformer-XL as our generator and pre-\ntrained BERT as the discriminator;\n\u000f We pretrain the BERT discriminator in the SpanBERT\nstyle (Joshi et al. 2020);\n\u000f We design an algorithm using the Gumbel-Softmax trick\nand a variant of the Truncated Backpropagation Through\nTime (TBPTT) algorithm (Sutskever, Vinyals, and Le\n409\n2014) to train on long sequences.\nWe are unaware of prior work studying self-supervised pre-\ntraining of the discriminator and its inﬂuence on generation.\nMethodology\nThe key challenge is to make long sequence generation in\nGANs practical. We begin with an overview of data repre-\nsentation and training objectives, followed by a discussion\nof the network architecture, for generation and discrimina-\ntion. We conclude with a list of useful tricks and techniques.\nData Representation\nWe take a language-modeling approach to train genera-\ntive models for tokens. Picking a representation matching\n(Huang et al. 2018) allows us to compare directly to the Mu-\nsic Transformer in terms of its log-likelihood. More specif-\nically, we use the encoding proposed by Oore et al. (2018),\nwhich consists of a vocabulary of 88 NOTE\nON events, 88\nNOTE OFFs, 100 TIME SHIFTs, and 32 VELOCITY bins.\nThis allows for expressive timing at 10ms and expressive\ndynamics.\nMaximum Likelihood\nGiven a music sequence x = [x1;x2;:::;x n], we model the\nunknown data distribution pr(x) autoregressively as\np\u0012(x) =\nnY\nt=1\np\u0012(xtjx1;:::;x t\u00001):\nLanguage models are typically trained using Maximum\nLikelihood. We seek a weight vector \u0012to minimize\nLmle = \u0000Exr\u0018pr [log p\u0012(xr)] : (1)\nDespite its attractive theoretical properties, maximum\nlikelihood training suffers from many limitations, e.g. when-\never the model is misspeciﬁed. This is illustrated by (Isola\net al. 2017) in image-to-image translation, where no ex-\nplicit loss function is available. Furthermore, teacher forc-\ning introduces exposure bias (Bengio et al. 2015; Holtz-\nman et al. 2019)—a distributional shift between training se-\nquences used for learning and model data required for gen-\neration. This ampliﬁes any errors in the estimate, sometimes\ncreating strange, repetitive outputs.\nAdversarial Losses\nWe address this problem by incorporating an adversarial loss\ninto our objective. That is, we cast our model as generator\nG\u0012 and ensure that the sequences obtained from it match\nthose on the training set as assessed by an appropriate dis-\ncriminator D\u001e. For our discriminator, we select a BERT\nmodel pretrained on music. We also regularize D\u001e to pre-\nvent overﬁtting. During training, we alternate updates be-\ntween the generator and discriminator objectives:\nLG = Lmle[G\u0012] +\u0015Lgen[G\u0012] (2)\nLD = Ldisc[D\u001e] +\rLreg[D\u001e] (3)\nHere \u0015;\r > 0 are hyperparameters. We investigate sev-\neral choices for Lgen;Ldisc and Lreg: the gradient penalty of\nWGANs (Gulrajani et al. 2017), RSGAN losses (Jolicoeur-\nMartineau 2018), and PPO-GAN’s loss (Wu et al. 2020).\nWGAN With Gradient Penalty Loss\nLgen = \u0000Exf\u0018p\u0012[D\u001e(xf)] (4)\nLdisc = \u0000Exr\u0018pr[D\u001e(xr)] +Exf\u0018p\u0012[D\u001e(xf)] (5)\nLreg = E^ x\u0018p^ x[(kr^ xD\u001e(^ x)k2 \u00001)2] (6)\nD\u001e is assumed to be a 1-Lipschitz continuous function. The\nsmoothness is enforced with the gradient penalty loss. ^ xis\ndrawn uniformly along straight lines (in embedding space)\nbetween pairs of points sampled from pr and p\u0012.\nRSGAN Loss The effective discriminator, C(xr;xf) de-\nﬁned in terms of D\u001e estimates the probability that the given\nreal data is more realistic than randomly sampled fake data\n(\u001bis the sigmoid function).\nC(xr;xf) =\u001b(D\u001e(xf) \u0000D\u001e(xr)) (7)\nLgen = \u0000E(xr;xf)\u0018(pr;p\u0012)[log(C(xr;xf))] (8)\nLdisc = \u0000E(xr;xf)\u0018(pr;p\u0012)[log(1 \u0000C(xr;xf))] (9)\nPPO-GAN Loss G\u0012 is treated as a policy and D\u001e as a re-\nward function. It transforms the conventional GAN objective\nas L(\u0012;q) =Eq[D\u001e(x)] \u0000KL(q(x)jjp\u0012(x)) by introducing\nan auxiliary non-parametric functionq(x). Using the EM al-\ngorithm to alternatively optimize for q and \u0012leads to\nLgen = \u0000Exf\u0018p\u0012[D\u001e(xf)] + KL(p\u0012(x) jjp\u0012(t) (x)) (10)\nLdisc = Exr\u0018pr[D\u001e(xr)] \u0000log(\nZ\nx\np\u0012(t) (x)expfD\u001e(x)gdx)\n(11)\nLreg = E^ x\u0018p^ x[(kr^ xD\u001e(^ x)k2 \u00001)2]; (12)\nwhere p\u0012(t) is the generator distribution at the previous iter-\nation. The KL penalty term is enforced using a clipped sur-\nrogate objective as in Proximal Policy Optimization (PPO)\n(Schulman et al. 2017). This loss also enforces 1-Lipschitz\ncontinuity in D\u001e using a gradient penalty regularizer.\nTransformer-GAN Architecture\nGenerator We use Transformer-XL (Dai et al. 2019)\nwhich introduces the notion of recurrence into the deep self-\nattention network. Instead of computing the hidden states\nfrom scratch for each new segment, it reuses the ones ob-\ntained in previous segments. These states serve as memory\nfor the current segment, which builds up a recurrent con-\nnection between the segments. This makes it possible to\nmodel very long range dependencies, since information can\nbe propagated through the recurrent connections. Note that\nour approach applies equally to other SOTA Transformers.\nDiscriminator Transformer-GANs differ from other ef-\nforts to incorporate GAN losses into sequence modeling (Lu\net al. 2018) in the choice of discriminator. Training GANs\nusing the Transformer as generator is a difﬁcult problem\n(Chen et al. 2020b; Zhang 2020) since training dynamics,\nmemory overhead and the generator and discriminator losses\nneed to be carefully balanced. In prior work, CNNs have\nproven to be useful discriminators for text generation (Kim\n2014). The CNN-based discriminator in (Nie, Narodytska,\n410\nand Patel 2019) uses multiple embedded representations to\nprovide more informative signals to the generator.\nIn this work, we propose using BERT as the discrimina-\ntor to extract sequence embeddings followed by a pooling\nand linear layer. The bidirectional transformer is compara-\nble in capacity to the transformer-based generator and uses\nthe self-attention mechanism that captures meaningful as-\npects of the input music sequence. We speculate that this\nwould help the discriminator provide informative gradients\nto the generator and stabilize the training process. Inspired\nby the observation by Mo, Cho, and Shin (2020) for im-\nages, we conjecture that freezing earlier layers of the pre-\ntrained discriminator in language GANs is a GAN transfer\nlearning technique that is not prone to overﬁtting. Freezing\nlayers in a pretrained discriminator, according to Mo, Cho,\nand Shin (2020) can be viewed as transfer learning, where\nwe transfer knowledge useful for generation from a differ-\nent dataset in the form of music representations. The lower\nlayers of the discriminator—closest to the generator learn\ngeneric features of text—while the upper layers learn to clas-\nsify whether the text is real or fake based on the extracted\nfeatures.\nUnlike Mo, Cho, and Shin (2020), where the discrim-\ninator is transferred between trained GANs on different\ndatasets, we reshape this idea into a form that resembles rep-\nresentation learning in NLP. We pretrain our discriminator\non the same dataset we train on using a self-supervised loss\nand test the hypothesis that the resulting learnt bidirectional\nrepresentations are useful for the discriminator to classify\nreal and fake data. In our setup, we simply freeze the lower\nlayers of the discriminator and only ﬁne-tune the upper lay-\ners. As we will show in Table 2, this achieved good perfor-\nmance on various proposed metrics.\nSpanBERT style self-supervised pretraining, where we\npredict spans of masked tokens, enables the model to\nlearn span representations. We hypothesize that span rep-\nresentations are better inductive biases for modeling co-\nherence in music, as music is composed in spans of notes\nor chords. Given a masked span xmask comprising tokens\n(xs;:::;x e) 2x, the Masked Language Model (MLM) ob-\njective from SpanBERT for each token xi 2xmask is\nLMLM(xi) =\u0000log(P(xijxnmask)); (13)\nwhere xnmask = fy jy 2x and y =2xmaskg. Freezing the\ndiscriminator also reduces the number of trainable parame-\nters and training memory requirements, that are usually bot-\ntlenecks when training on long sequences.\nTricks Of Adversarial Training\nThe adversarial training in (3) involves generating discrete\nsamples xf \u0018 p\u0012(x) autoregressively from the generator\nto feed into the discriminator. However, several issues ex-\nist in generating and training on these discrete samples, e.g.,\nthe non-differentiable sampling step, the repetition and high\nvariance in generated samples, the high memory and com-\npute complexity during backpropagation, and the instability\nduring GAN training. In this section, we highlight a few crit-\nical tricks to address these issues.\nGumbel Softmax The discrete samples are generated se-\nquentially. To generate the next tokenxf\nt+1, we sample from\nthe multinomial distribution softmax(ot) on the vocabulary\nset V which can be formulated as xf\nt+1 \u0018 softmax(ot):\nHere ot 2I RV denotes the output logits from the genera-\ntor obtained by attending over the past tokens fxf\n1 ;::;x f\ntg.\nHowever, this sampling process is not differentiable, as the\nderivative of a step function is 0 or undeﬁned everywhere.\nTo deal with this, we reparameterize the sampling opera-\ntion using the Gumbel-Max trick as\nxf\nt+1 = arg max\n1\u0014i\u0014V\n(o(i)\nt + g(i)\nt ); (14)\nwhere o(i)\nt denotes the i-th entry of ot, g(i)\nt is the i-th entry\nof gt, which follows the element-wise i.i.d. standard Gum-\nbel distribution. As this arg maxis still not differentiable,\nwe approximate arg maxin the backward pass using the\nGumbel-softmax trick, where the Gumbel-softmax is both\ncontinuous and differentiable as shown in Jang, Gu, and\nPoole (2016). Therefore, in the backward pass, (14) becomes\nsoftmax(\f(ot + gt)); (15)\nwhere \f >0 is a tunable parameter called inverse temper-\nature. At last, fxf\n1 ;::;x f\nngforms the sequence as xf, which\nwill be fed into the discriminator.\nExponential Inverse Temperature When using a ﬁxed\ninverse temperature \f in (15) to train the GAN, we noticed\nthat the generator has a tendency to suffer from mode col-\nlapse, generating many repeated tokens. We found that this\ncan be mitigated by using a large \fmax and applying the\nexponential policy \fn = \fn=N\nmax to increase \f over itera-\ntions, where N is the maximum number of training itera-\ntions and n denotes the current iteration. Nie, Narodytska,\nand Patel (2019) suggests that the exponential inverse tem-\nperature decay policy can help balance exploration and ex-\nploitation during generator sampling. A larger \fencourages\nmore exploration for better sample diversity, while a smaller\n\fencourages more exploitation for better samples quality.\nConditional Generation Another issue we notice is that\nlearning xf in (15) can lead to a large variance in gradi-\nent estimation due to the randomness in sampling. In order\nto reduce this variance, we reduce the distance between the\nreal xr \u0018pr(x) and fake xf \u0018p\u0012(x) samples by applying\nconditional sampling where the real and fake samples share\na common priming sequence. To generate the fake samples,\nwe condition the generator on the shared priming sequence\n[xr\n1;:::;x r\nc] and sample the remaining [xf\nc+1;:::;x f\nn] autore-\ngressively.\nTruncated Backpropagation Through Time (TBPTT)\nThe generator sampling step in (15) is sequential and there-\nfore the backward pass in the generator resembles the back-\npropagation through time (BPTT) algorithm (Tallec and Ol-\nlivier 2017). However, the generated sequences that are se-\nquentially sampled can be very long, potentially \u00152000\ntokens. Standard BPTT on those long sequences is both\ncompute-intensive and memory-intensive.\n411\nGenerator\nGAN loss\nHidden \nState in G\nToken\nHidden \nState in D\nPrefix tokens\nGenerated\nSample\nDiscriminator \nshared \nSegment 1 Segment N\nFigure 1: TBPTT during adversarial training of the Transformer-GAN. Blue arrows are in the direction of the forward pass;\nYellow arrows denote the direction gradients are backpropagated in training.\nTo resolve this, we truncate our generated sequences into\nsegments and feed the segments into the discriminator as il-\nlustrated in Figure 1. Then, we do backpropagation on each\nsegment and accumulate the gradients. The truncation im-\nproves memory efﬁciency as it avoids holding all forward\ncomputation graphs during sampling. The length of the sub-\nsequence is also well suited to our BERT since it is trained to\naccept a smaller ﬁxed length sequence. TBPTT can be for-\nmally expressed in terms of parameters k1 and k2. k1 is the\nnumber of forward-pass timesteps between updates andk2 is\nthe number of timesteps to which to apply BPTT. In this pa-\nper, we use k1 = k2. Breaking the generated sequence into\nsubsequences for gradient propagation resembles the subse-\nquence reward training (Yu et al. 2017; Zhang 2020; Chen\net al. 2019) in RL based GANs.\nGradient Penalty During training, we notice that the dis-\ncriminator can be easily trained to optimality before the gen-\nerator parameter update. In addition, exploding or vanishing\ngradients was a recurrent problem.\nWe discovered that in order to stabilize the GAN training,\nit was necessary to add the gradient penalty regularizer (Gul-\nrajani et al. 2017). Each token of sequence ^ xin (5) can be\nobtained by interpolating the discrete tokens in embedding\nspace as ^xt = \u000bembed[xr\nt] + (1\u0000\u000b) embed[xf\nt]; where\nembed denotes the embedding layer of the discriminator,\nand \u000b is drawn from a uniform distribution on the interval\n(0;1).\nOur hypothesis and ﬁndings on the importance of discrim-\ninator regularization align with prior work (Gulrajani et al.\n2017) on image GANs. We ﬁnd that discriminator regular-\nization in the form of layer normalization, dropout and L2\nweight decay offered a less signiﬁcant performance boost\nthan the gradient penalty regularizer.\nMetrics\nThe assessment and evaluation of generative models in mu-\nsic using objective metrics is challenging. In this section, we\nprovide two metrics (used in text generation) for music qual-\nity evaluation, i.e. Classiﬁer Accuracy (CA) and pseudo-log-\nlikelihood (PLL) as well as objective metrics and human\nevaluation metrics.\nClassiﬁer Accuracy Inspired by the relative metrics used\nin (Yang and Lerch 2020), we propose using a classiﬁer ac-\ncuracy score as a measure of music realism that could detect\nexposure bias. We train a separate model to distinguish be-\ntween real (our validation set) and generated data and use its\naccuracy on a held-out set as this classiﬁer accuracy (CA)\nscore. This metric is an implementation of the more general\nClassiﬁer Two-Sample Tests (C2ST) (Lopez-Paz and Oquab\n2016). The predictive uncertainty of the classiﬁer can also be\nused to inspect where the real and generated data differ. The\nclassiﬁer accuracy can be used to score generative models as\nwe show in Table 1. The lower the accuracy, the closer are\nthe generated samples to the real data distribution. Our clas-\nsiﬁer is built with our pretrained frozen BERT model trained\nusing SpanBERT, and a Support Vector Machine (SVM) on\ntop. While we could pick an arbitrary classiﬁer, we imple-\nment C2ST with the SVM, a margin classiﬁer with ﬁnite\nnorm known for its fast convergence. The SVM is retrained\nfor every model we evaluate.\nPseudo-log-likelihood As BERT is trained with the\nMasked Language Model (MLM) objective, it learns bidi-\nrectional representations. The pseudo-log-likelihood score\n(PLLs) is derived from MLMs and is given by summing the\nconditional log probabilities log P(xtjxnt) of each sentence\ntoken. The PLL score is better when it is closer to the PLL\nof the training set. The pseudo-log-likelihood score of a sen-\ntence x can be deﬁned as\nPLL(x) =\u0000\njxjX\nt=1\nlog(P(xtjxnt)); (16)\n412\nwhere xnt := (x1;:::xt\u00001;xt+1;:::;x jxj).\nPLL is an intrinsic value that one can assign to sentences\nand corpora, allowing one to use MLMs to evaluate a se-\nquence of tokens in applications previously restricted to con-\nventional language model scoring. PLL scores as proposed\nin (Salazar et al. 2019) to measure linguistic acceptability.\nWe therefore propose to use this as a metric to evaluate mu-\nsic samples. PLL scores are closely related to log-likelihood\nscores and their roles overlap as the models get stronger. Re-\ncent work in text evaluation (Basu et al. 2020) suggest that\nsample likelihood and quality are only correlated within a\ncertain likelihood range.\nQuantitative Metrics To evaluate the music generated,\nwe use several conventional objective metrics used in mu-\nsic evaluation research (Dong et al. 2018; Yang and Lerch\n2020). These metrics are computed for both real (training\nset) data and generated data, and their values are compared.\nThe closer the values are to the dataset, the better the score.\nWe also report the validation NLL by calculating the NLL\nfor each token and averaging over all tokens in the validation\nset. We generate 200 unconditional samples—each 4096 to-\nkens in length—for each model we want to evaluate.\nHuman Evaluation For each model, we generated 84\nsamples from a set of 7 priming sequences. The samples\nwere 1 minute in duration and the primes were 10 seconds in\nduration. Each survey contained 7 questions corresponding\nto each priming sequence. Participants were given a random\nsurvey from a set of 5 surveys. For each question in the sur-\nvey, participants were presented with a random set of musi-\ncal samples, where each sample is from a different model,\nbut from the same priming sequence. They were asked to (a)\nrate the score of the sample in a range of 0 to 5 and (b) rank\nthe samples based on their coherence and consistency.\nImplementation Details\nWe benchmark our models on the MAESTRO MIDI V1\ndataset (Hawthorne et al. 2019), which contains over 200\nhours of paired audio and MIDI recordings from ten years\nof the International Piano-e-Competition. The dataset is split\ninto 80/10/10 for training/validation/evaluation. We used the\nsame data augmentation as in Music Transformer, where we\naugmented the data by uniform pitch transposition fromf-3,\n-2, ..., 2, 3g and stretched time with ratios of f0.95, 0.975,\n1.0, 1.025, 1.05g.\nHyperparameter Conﬁguration The hyperparameters\nwe used for the Transformer-XL architecture are shown in\nTable 3. For training, we used a 0.004 initial learning rate,\nthe inverse square root scheduler and Adam optimizer. We\nused a target length of 128 for both training and evalua-\ntion, since we found this value offers a reasonable trade-off\nbetween training time and performance on metrics. Since\nTBPTT addresses the memory bottleneck, our framework\ncan train on sequence lengths longer than 128. We set mem-\nory length for the Transformer-XL as 1024 during training\nand 2048 during evaluation. During sample generation, we\nset memory length equal to the number of tokens to gener-\nate. We observed, as in (Dai et al. 2019), that NLL and gen-\nerated music quality were sensitive to memory length. We\nintroduced a reset memory feature into the Transformer-XL\ntraining process as clearing the Transformer-XL memory at\nthe beginning of each new MIDI ﬁle. We report the baseline\nmodels with the lowest validation NLL.\nAll our GANs and baselines use the same Transformer-\nXL conﬁguration. We set the sequence length of generated\nsamples during adversarial training as 128 (equal to target\nlength). Our GAN generator is initialized using our best\nNLL-trained baseline model. We follow an alternating train-\ning procedure to update the generator and discriminator us-\ning the NLL and GAN losses. The NLL loss frequency is\nﬁve times the GAN loss frequency. We used\fmax = 100in\nall our experiments as in Nie, Narodytska, and Patel (2019).\nFor Music Transformer, we use the implementation in Ten-\nsor2Tensor (Vaswani et al. 2018). We run the baseline ex-\nperiments with three different seeds and run the GAN ex-\nperiments with one seed.\nSampling Methods At each step tof generation, Random\nsampling samples from ot while TopK sampling samples\nfrom the tokens corresponding to the K highest probabili-\nties in ot. All sampling methods use a ﬁxed temperature of\n0.95.\nExperiments And Results\nTransformer-XL And Music Transformer Transformer-\nXL achieves comparable overall performance to Music\nTransformer, the current state-of-the-art model. We see from\nTable 1 that Transformer-XL achieves lower NLL and is\ncomparable to Music Transformer on several objective met-\nrics. This is also reﬂected in the human evaluation scores\nin Fig. 2. Transformer-XL uses the relative attention mecha-\nnism akin to Music Transformer which explains their similar\nperformance on metrics. These architectures act as our base-\nlines trained with MLE.\nTransformer-GAN Versus MLE Baselines We compare\nthe Transformer-GAN framework trained with different loss\ntypes and discriminator choices in Table 1. We see that the\nTransformer-GAN with the BERT discriminator scores bet-\nter on the CA metric than our baselines. This can be at-\ntributed to GAN training, that reduces the distributional dis-\ncrepancy between real and generated data. Fig. 2 shows that\nthe Transformer-GAN with WGAN with gradient penalty\n(WGAN-GPen) outperforms our baselines in human evalu-\nation, proving the validity of our proposed GAN model. A\nKruskal-Wallis H test of ratings shows a statistically signif-\nicant difference between Transformer-GAN with WGAN-\nGPen (Random) and Transformer-XL (Random): X2(2) =\n3:272;p = 0:031. Fig. 2 also shows that the Transformer-\nGAN trained with the PPO-GAN with gradient penalty\n(PPO-GPen) outperforms the baselines.\nDiscriminator Architecture We experiment with two\ndiscriminator architectures—the CNN-based discriminator\nused in Nie, Narodytska, and Patel (2019) and our BERT\ndiscriminator. We see in Table 1 that WGAN-GPen using\nthe CNN (i) performs worse than WGAN-GPen using BERT\non several objective metrics and (ii) performs worse than\n413\nDiscriminator NLL # Sampling\nCA # PLL \u0018 PCU \u0018 ISR \u0018 PRS \u0018 TUP \u0018 PR \u0018 APS \u0018 IOI \u0018\nTraining\nSet – – – – 2.020 7.81 0.59 0.40 65.28 67.34 11.53 0.133\nMusic Transformer – 1.79 Random 0.844 2.567 7.21 0.60 0.47 54.95 62.04 11.62 0.113\nTransformer-XL – 1.74 Top32 0.838 2.153 7.05 0.57 0.28 52.95 60.39 11.12 0.107\nWGAN-GPen CNN\n1.75 Random 0.840 2.309 6.95 0.61 0.33 52.28 59.37 10.83 0.119\nWGAN-GPen Pretrained BERT 1.75 Random 0.818 2.102 7.19 0.59 0.28 55.56 63.23 11.94 0.145\nPPO-GPen Pretrained BERT 1.75 Random 0.821 2.355 6.93 0.60 0.30 52.31 59.26 10.81 0.163\nRSGAN-GPen Pretrained BERT 1.75 Random 0.831 2.277 7.29 0.59 0.30 54.11 62.83 11.46 0.136\nRSGAN Pretrained BERT 1.75 Random 0.862 2.108 6.56 0.61 0.19 48.17 55.62 11.26 0.082\nTable 1: Quantitative music metrics: NLL (Negative likelihood); CA (SpanBERT classiﬁer accuracy); PLL (Pseudo-log-\nlikelihood score); PCU (Unique pitch classes); ISR (Nonzero entries in C major scale / Total nonzero entries); PRS (Time\nsteps where the no. of pitches \u00154 / Total time steps); TUP (Different pitches within a sample); PR (Avg. difference of the\nhighest and lowest pitch in semitones); APS (Avg. semitone interval between two consecutive pitches); IOI (Time between\ntwo consecutive notes). Bolded values are better when rounded to four decimal places. Metrics marked with \u0018are better when\ncloser to the Training Set.\nFrozen layers\nNLL # CA # PLL \u0018 PCU \u0018 ISR \u0018 PRS \u0018 TUP \u0018 PR \u0018 APS \u0018 IOI \u0018\nTraining\nSet – – – 2.020 7.81 0.59 0.40 65.28 67.34 11.53 0.133\nWGAN-Gpen random-init 1.75\n0.836 2.288 7.11 0.60 0.26 53.44 61.35 11.25 0.125\nWGAN-Gpen [’emb’] 1.75 0.843 2.350 7.25 0.59 0.36 54.94 62.56 11.54 0.132\nWGAN-Gpen [’emb’, ’0’] 1.75 0.885 2.349 7.13 0.59 0.35 53.79 61.56 11.16 0.135\nWGAN-Gpen [’emb’, ’0’, ’1’, ’2’] 1.75 0.859 2.497 7.34 0.58 0.40 55.06 63.90 11.98 0.136\nWGAN-Gpen [’emb’, ’0’, ’1’, ’2’, ’3’] 1.75 0.839 2.450 6.92 0.58 0.36 50.96 59.46 10.53 0.161\nWGAN-Gpen [’emb’, ’0’, ’1’, ’2’, ’3’, ’4’] 1.75 0.818 2.102 7.02 0.61 0.28 55.56 63.23 11.93 0.145\nTable 2: Quantitative music metrics: Ablation studies of frozen layers and random weight initialization. Note BERT has 6\nlayers: ‘embedding’, ‘attention 0’, ‘attention 1’, ‘attention 2’, ‘attention 3’, ‘attention 4’, which is denoted as emb, 0, 1, 2, 3,\n4. All samples were generated with Random Sampling. Bolded values are better when rounded to four decimal places. Metrics\nmarked with \u0018are better when closer to the Training Set.\nFigure 2: Human Evaluations. Left: Average ratings for each model with error bars for standard error of mean. Right: Pairwise\ncomparison between models. When pairwise samples are compared, ‘Win’ denotes which model humans preferred.\nTransformer-XL on CA and PLL scores. A possible expla-\nnation to why a CNN discriminator performs worse could\nbe the sensitivity to parameter initialization, model capac-\nity and hyperparameters that are well documented (Seme-\nniuta, Severyn, and Gelly 2018). Using a pretrained BERT\ndiscriminator, to an extent, helps address this sensitivity.\nGAN Loss Type We train our Transformer-GAN using\nfour different GAN losses: RSGAN, RSGAN with gra-\ndient penalty (RSGAN-GPen), WGAN-GPen, and PPO-\nGPen. In Table 1, we compare their performances on sev-\neral quantitative metrics. We see that (i) RSGAN performs\nworse than RSGAN-GPen, indicating the importance of the\ngradient penalty regularizer. RSGAN performs worse than\nTransformer-XL on the CA metric, suggesting that GPen\nwas essential to make the GAN loss work; (ii) WGAN-\nGPen achieves the highest scores on CA and PLL, and beats\nother models on our objective metrics. We also observed that\n414\nHyperparameters Music\nTransformer Transformer-XL\nLayers 8 6\nDropout 0.2 0.1\nHidden\nsize 384 500\nTarget length 2048 128\nMemory length - 2048\nNumber heads 8 10\nNumber of parameters 16253189 13677310\nTable 3: Hyperparameters of baseline models\nit was hard for subjects to distinguish among the different\nTransformer-GAN models trained with GPen, signalling the\nneed for ﬁne-grained metrics in the music generation com-\nmunity.\nEffect Of Frozen Layers And Initialization We perform\nablation studies on our BERT discriminator to understand\nthe effect of freezing layers of our pretrained discriminator\nduring GAN training. In Table 2, the ﬁrst row corresponds\nto the randomly initialized BERT without any pretraining.\nWe observe that (i) A randomly initialized BERT discrim-\ninator scores poorly on our objective metrics compared to\nthe pretrained discriminator (ii) Freezing more layers in the\npretrained discriminator tends to improve objective metric\nscores, in particular CA. These results suggest that discrim-\ninator priors can play an important role in GAN training.\nSampling Methods We also experiment with Random\nand TopK sampling and how it inﬂuenced music evaluation\nscores. We ﬁnd that (i) Transformer-XL samples sampled\nwith TopK score better than those sampled with Random on\nour objective metrics. This is also reﬂected in the human\nevaluation scores in Fig. 2. (ii) Transformer-GAN samples\nsampled with TopK score lower than those sampled with\nRandom on several objective metrics and human evaluation.\n(iii) Music generated using top-k sampling scores higher on\nthe pseudo-likelihood metric, suggesting that this metric is\nsensitive to a distributional bias towards higher likelihood. A\npossible explanation for the apparent contradictory behavior\nobserved in (i) and (ii) can be attributed to sampling during\nadversarial training. We speculate that these results might be\ndue to the Transformer GAN being trained using Random\nsampling when decoding sequences as input to the discrim-\ninator.\nQualitative Study We gave a small set of clips from the\nbaseline Transformer-XL and Transformer-GAN to a few\nmusicians and composers and simply asked for any initial\nreactions/comments. Here is a small, representative subset\nof the comments we received. Samples can be found at\nhttps://tinyurl.com/y6awtlv7.\n\u000f ”The Transformer-XL compositions sound somewhat vir-\ntuosic, there seems to often be one hand that’s moving\nquite quickly. They are quite pleasant to listen to but end\nup at quite different places compared to the beginning.\nPPO-GAN’s music is signiﬁcantly more polyphonic, and\nalso develops more slowly and consistently. It does inter-\nesting transitions as well, and maintains the new style for\nquite some time.”\n\u000f ”Overall, the quality of the performance (of Transformer-\nGAN samples) is excellent, showcasing varied dynamics\nand proper phrasing. The tempo is not rigid but expres-\nsive. The (sustaining) pedaling seems to be a little mud-\ndled at times, but not to a point where it hampers the de-\nlivery. ”\n\u000f ”For the most part, harmonic choices (in the Transformer-\nGAN samples) are sensible locally, and chord progres-\nsions are constructed on the appropriate scale. Aside from\na few exceptions, the composition style remains consis-\ntent throughout, without abrupt or unreasonable shifts.In\nhalf of the samples, a recurring motif can be recognized,\nat least for about 10 measures initially, and in some cases,\ndeveloping in an interesting fashion. However, none of the\nsamples demonstrate a global structure (exposition, de-\nvelopment, recapitulation), which is prevalent in classical\ncompositions. Occasionally a sequence of notes or trills\nare repeated for an excessively long time, but as a whole\nthe melodic line feels natural and pleasing to the ear.”\nSequence Length And Exposure Bias In the experiments\nwe report in Table 1, the generated sequence length fed to the\ndiscriminator during training is of length 128 which is still\nlonger than the 20-40 token length sentences that GANs for\ntext are frequently trained on (de Masson d’Autume et al.\n2019). As TBPTT allows us to train on sequences longer\nthan 128, we also experiment with sequences of length 256,\n512 and 1024. Our objective metrics do not show a statis-\ntically signiﬁcant difference between the different sequence\nlengths, signalling the need for improved metrics with lower\nvariance to measure music quality. The marginal improve-\nment with increasing sequence length can also be attributed\nto the fact that in the Transformer-XL, historical information\nof the past is already encoded in the cached memory.\nWe also perform a preliminary investigation into under-\nstanding how effective the Transformer-GAN is in alleviat-\ning exposure bias. It is well documented that GANs for text\ndo not suffer from the exposure bias problem (Tevet et al.\n2018; Chen et al. 2020a) although measuring improvement\nis often difﬁcult (Wang and Sennrich 2020; He et al. 2019).\nWe use the MIREX-like continuation prediction task (Wu\net al. 2020) to measure improvement. The metric however\ndisplayed high variance and our results were not statistically\nsigniﬁcant. We leave this investigation as future work.\nConclusion And Discussion\nWe proposed a new framework for generating long-term co-\nherent music based on adversarial training of Transform-\ners. The results obtained from various experiments demon-\nstrate that our Transformer-GAN achieves better perfor-\nmance compared to other transformers trained by maximiz-\ning likelihood alone. By sampling during training, the adver-\nsarial loss helps bridge the discrepancy between the train-\ning objective and generation. We have demonstrated that us-\ning a bidirectional transformer can indeed provide a useful\nsignal to the generator, contrary to the ﬁndings in de Mas-\nson d’Autume et al. (2019). In future work, we plan to ex-\ntend our work by pretraining on larger datasets where our\nidea can be beneﬁcial.\n415\nReferences\nAnantrasirichai, N.; and Bull, D. 2020. Artiﬁcial Intelli-\ngence in the Creative Industries: A Review. arXiv preprint\narXiv:2007.12391 .\nBasu, S.; Ramachandran, G. S.; Keskar, N. S.; and Varshney,\nL. R. 2020. Mirostat: A Perplexity-Controlled Neural Text\nDecoding Algorithm. arXiv preprint arXiv:2007.14966 .\nBengio, S.; Vinyals, O.; Jaitly, N.; and Shazeer, N. 2015.\nScheduled sampling for sequence prediction with recurrent\nneural networks. In Advances in Neural Information Pro-\ncessing Systems, 1171–1179.\nBriot, J.-P.; Hadjeres, G.; and Pachet, F.-D. 2017. Deep\nlearning techniques for music generation–a survey. arXiv\npreprint arXiv:1709.01620 .\nCarnovalini, F.; and Rod`a, A. 2020. Computational Creativ-\nity and Music Generation Systems: An Introduction to the\nState of the Art. Frontiers Artif. Intell.3: 14.\nChen, X.; Cai, P.; Jin, P.; Wang, H.; Dai, X.; and Chen, J.\n2020a. Adding A Filter Based on The Discriminator to Im-\nprove Unconditional Text Generation. arXiv e-prints arXiv–\n2004.\nChen, X.; Cai, P.; Jin, P.; Wang, H.; Dai, X.; and Chen,\nJ. 2020b. A Discriminator Improves Unconditional Text\nGeneration without Updating the Generator. arXiv preprint\narXiv:2004.02135 .\nChen, X.; Li, Y .; Jin, P.; Zhang, J.; Dai, X.; Chen, J.; and\nSong, G. 2019. Adversarial Sub-sequence for Text Genera-\ntion. arXiv preprint arXiv:1905.12835 .\nChild, R.; Gray, S.; Radford, A.; and Sutskever, I. 2019.\nGenerating long sequences with sparse transformers. arXiv\npreprint arXiv:1904.10509 .\nChoi, K.; Hawthorne, C.; Simon, I.; Dinculescu, M.; and En-\ngel, J. 2019. Encoding musical style with transformer au-\ntoencoders. arXiv preprint arXiv:1912.05537 .\nDai, Z.; Yang, Z.; Yang, Y .; Carbonell, J.; Le, Q. V .; and\nSalakhutdinov, R. 2019. Transformer-XL: Attentive lan-\nguage models beyond a ﬁxed-length context. arXiv preprint\narXiv:1901.02860 .\nde Masson d’Autume, C.; Mohamed, S.; Rosca, M.; and\nRae, J. 2019. Training language GANs from scratch. In\nAdvances in Neural Information Processing Systems, 4300–\n4311.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBERT: Pre-training of deep bidirectional transformers for\nlanguage understanding. arXiv preprint arXiv:1810.04805 .\nDonahue, C.; Mao, H. H.; Li, Y . E.; Cottrell, G. W.; and\nMcAuley, J. 2019. LakhNES: Improving multi-instrumental\nmusic generation with cross-domain pre-training. arXiv\npreprint arXiv:1907.04868 .\nDong, H.-W.; Hsiao, W.-Y .; Yang, L.-C.; and Yang, Y .-H.\n2018. MuseGAN: Multi-track sequential generative adver-\nsarial networks for symbolic music generation and accom-\npaniment. In Thirty-Second AAAI Conference on Artiﬁcial\nIntelligence.\nGoodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.;\nWarde-Farley, D.; Ozair, S.; Courville, A.; and Bengio, Y .\n2014. Generative adversarial nets. In Advances in neural\ninformation processing systems, 2672–2680.\nGulrajani, I.; Ahmed, F.; Arjovsky, M.; Dumoulin, V .; and\nCourville, A. C. 2017. Improved training of wasserstein\ngans. In Advances in neural information processing systems,\n5767–5777.\nGuo, J.; Lu, S.; Cai, H.; Zhang, W.; Yu, Y .; and Wang, J.\n2018. Long text generation via adversarial training with\nleaked information. In Thirty-Second AAAI Conference on\nArtiﬁcial Intelligence.\nHawthorne, C.; Stasyuk, A.; Roberts, A.; Simon, I.; Huang,\nC.-Z. A.; Dieleman, S.; Elsen, E.; Engel, J.; and Eck, D.\n2019. Enabling Factorized Piano Music Modeling and\nGeneration with the MAESTRO Dataset. In International\nConference on Learning Representations. URL https://\nopenreview.net/forum?id=r1lYRjC9F7.\nHe, T.; Zhang, J.; Zhou, Z.; and Glass, J. 2019. Quantifying\nexposure bias for neural language generation.arXiv preprint\narXiv:1905.10617 .\nHochreiter, S.; and Schmidhuber, J. 1997. Long short-term\nmemory. Neural computation 9(8): 1735–1780.\nHoltzman, A.; Buys, J.; Forbes, M.; and Choi, Y . 2019. The\ncurious case of neural text degeneration. arXiv preprint\narXiv:1904.09751 .\nHuang, C.-Z. A.; Vaswani, A.; Uszkoreit, J.; Shazeer, N.;\nSimon, I.; Hawthorne, C.; Dai, A. M.; Hoffman, M. D.; Din-\nculescu, M.; and Eck, D. 2018. Music transformer. arXiv\npreprint arXiv:1809.04281 .\nHuang, Y .-S.; and Yang, Y .-H. 2020. Pop music transformer:\nGenerating music with rhythm and harmony. arXiv preprint\narXiv:2002.00212 .\nIsola, P.; Zhu, J.-Y .; Zhou, T.; and Efros, A. A. 2017. Image-\nto-image translation with conditional adversarial networks.\nIn Proceedings of the IEEE conference on computer vision\nand pattern recognition, 1125–1134.\nJang, E.; Gu, S.; and Poole, B. 2016. Categorical\nreparameterization with gumbel-softmax. arXiv preprint\narXiv:1611.01144 .\nJolicoeur-Martineau, A. 2018. The relativistic discriminator:\na key element missing from standard GAN. arXiv preprint\narXiv:1807.00734 .\nJoshi, M.; Chen, D.; Liu, Y .; Weld, D. S.; Zettlemoyer, L.;\nand Levy, O. 2020. Spanbert: Improving pre-training by rep-\nresenting and predicting spans. Transactions of the Associ-\nation for Computational Linguistics 8: 64–77.\nKim, Y . 2014. Convolutional neural networks for sentence\nclassiﬁcation. arXiv preprint arXiv:1408.5882 .\nKusner, M. J.; and Hern ´andez-Lobato, J. M. 2016. GANs\nfor sequences of discrete elements with the gumbel-softmax\ndistribution. arXiv preprint arXiv:1611.04051 .\nLopez-Paz, D.; and Oquab, M. 2016. Revisiting classiﬁer\ntwo-sample tests. arXiv preprint arXiv:1610.06545 .\n416\nLu, S.; Zhu, Y .; Zhang, W.; Wang, J.; and Yu, Y . 2018. Neu-\nral text generation: Past, present and beyond. arXiv preprint\narXiv:1803.07133 .\nMo, S.; Cho, M.; and Shin, J. 2020. Freeze Discriminator:\nA Simple Baseline for Fine-tuning GANs. arXiv preprint\narXiv:2002.10964 .\nMontahaei, E.; Alihosseini, D.; and Baghshah, M. S. 2019.\nJointly measuring diversity and quality in text generation\nmodels. arXiv preprint arXiv:1904.03971 .\nNie, W.; Narodytska, N.; and Patel, A. 2019. RelGAN: Re-\nlational generative adversarial networks for text generation.\nIn ICLR.\nOore, S.; Simon, I.; Dieleman, S.; Eck, D.; and Simonyan,\nK. 2018. This time with feeling: Learning expressive musi-\ncal performance. Neural Computing and Applications 1–13.\nPayne, C. 2019. MuseNet. URL openai.com/blog/musenet.\nRanzato, M.; Chopra, S.; Auli, M.; and Zaremba, W. 2015.\nSequence level training with recurrent neural networks.\narXiv preprint arXiv:1511.06732 .\nSalazar, J.; Liang, D.; Nguyen, T. Q.; and Kirchhoff, K.\n2019. Masked Language Model Scoring. arXiv preprint\narXiv:1910.14659 .\nSchulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; and\nKlimov, O. 2017. Proximal policy optimization algorithms.\narXiv 2017. arXiv preprint arXiv:1707.06347 .\nSemeniuta, S.; Severyn, A.; and Gelly, S. 2018. On accurate\nevaluation of gans for language generation. arXiv preprint\narXiv:1806.04936 .\nShaw, P.; Uszkoreit, J.; and Vaswani, A. 2018. Self-\nattention with relative position representations. arXiv\npreprint arXiv:1803.02155 .\nSutskever, I.; Vinyals, O.; and Le, Q. V . 2014. Sequence\nto sequence learning with neural networks. In Advances in\nneural information processing systems (NeurIPS).\nTallec, C.; and Ollivier, Y . 2017. Unbiasing truncated back-\npropagation through time. arXiv preprint arXiv:1705.08209\n.\nTevet, G.; Habib, G.; Shwartz, V .; and Berant, J. 2018.\nEvaluating text gans as language models. arXiv preprint\narXiv:1810.12686 .\nVaswani, A.; Bengio, S.; Brevdo, E.; Chollet, F.; Gomez,\nA. N.; Gouws, S.; Jones, L.; Kaiser, L.; Kalchbrenner, N.;\nParmar, N.; Sepassi, R.; Shazeer, N.; and Uszkoreit, J.\n2018. Tensor2Tensor for Neural Machine Translation.CoRR\nabs/1803.07416. URL http://arxiv.org/abs/1803.07416.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. In Advances in neural information\nprocessing systems, 5998–6008.\nWang, C.; and Sennrich, R. 2020. On Exposure Bias, Hal-\nlucination and Domain Shift in Neural Machine Translation.\narXiv preprint arXiv:2005.03642 .\nWilliams, R. J. 1992. Simple statistical gradient-following\nalgorithms for connectionist reinforcement learning. Ma-\nchine learning 8(3-4): 229–256.\nWu, Q.; Li, L.; and Yu, Z. 2020. TextGAIL: Generative\nAdversarial Imitation Learning for Text Generation. arXiv\npreprint arXiv:2004.13796 .\nWu, X.; Wang, C.; and Lei, Q. 2020. Transformer-XL Based\nMusic Generation with Multiple Sequences of Time-valued\nNotes. arXiv preprint arXiv:2007.07244 .\nWu, Y .; Zhou, P.; Wilson, A. G.; Xing, E. P.; and Hu,\nZ. 2020. Improving GAN Training with Probability Ra-\ntio Clipping and Sample Reweighting. arXiv preprint\narXiv:2006.06900 .\nYang, L.-C.; and Lerch, A. 2020. On the evaluation of gener-\native models in music. Neural Computing and Applications\n32(9): 4773–4784.\nYu, L.; Zhang, W.; Wang, J.; and Yu, Y . 2017. SeqGAN:\nSequence generative adversarial nets with policy gradient.\nIn Thirty-ﬁrst AAAI conference on artiﬁcial intelligence.\nZhang, N. 2020. Learning Adversarial Transformer for\nSymbolic Music Generation. IEEE Transactions on Neural\nNetworks and Learning Systems .\nZhang, T.; Kishore, V .; Wu, F.; Weinberger, K. Q.; and\nArtzi, Y . 2019. BERTScore: Evaluating text generation with\nBERT. arXiv preprint arXiv:1904.09675 .\n417"
}