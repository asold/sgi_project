{
  "title": "Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints",
  "url": "https://openalex.org/W4386566508",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2295257499",
      "name": "Albert Lu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2117163119",
      "name": "Hongxin Zhang",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2101409483",
      "name": "Yanzhe Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096224170",
      "name": "Xuezhi Wang",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2127567756",
      "name": "Diyi Yang",
      "affiliations": [
        "Stanford University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W3085190015",
    "https://openalex.org/W2094852898",
    "https://openalex.org/W2973049837",
    "https://openalex.org/W3118999024",
    "https://openalex.org/W3177879172",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2996908057",
    "https://openalex.org/W4307123345",
    "https://openalex.org/W2988937804",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W3103662468",
    "https://openalex.org/W4226024653",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2890894339",
    "https://openalex.org/W3197754201",
    "https://openalex.org/W3100355250",
    "https://openalex.org/W3160638507",
    "https://openalex.org/W4221160826",
    "https://openalex.org/W3135734416",
    "https://openalex.org/W4226399820",
    "https://openalex.org/W595957473",
    "https://openalex.org/W4285294723",
    "https://openalex.org/W4226479682",
    "https://openalex.org/W3161457214",
    "https://openalex.org/W2466175319",
    "https://openalex.org/W2798664956",
    "https://openalex.org/W2770828450"
  ],
  "abstract": "The limits of open-ended generative models are unclear, yet increasingly important. What causes them to succeed and what causes them to fail? In this paper, we take a prompt-centric approach to analyzing and bounding the abilities of open-ended generative models. We present a generic methodology of analysis with two challenging prompt constraint types: structural and stylistic. These constraint types are categorized into a set of well-defined constraints that are analyzable by a single prompt. We then systematically create a diverse set of simple, natural, and useful prompts to robustly analyze each individual constraint. Using the GPT-3 text-davinci-002 model as a case study, we generate outputs from our collection of prompts and analyze the model's generative failures. We also show the generalizability of our proposed method on other large models like BLOOM and OPT. Our results and our in-context mitigation strategies reveal open challenges for future research.",
  "full_text": "Findings of the Association for Computational Linguistics: EACL 2023, pages 1982–2008\nMay 2-6, 2023 ©2023 Association for Computational Linguistics\nBounding the Capabilities of Large Language Models\nin Open Text Generation with Prompt Constraints\nAlbert Lu∗, Hongxin Zhang∗1, Yanzhe Zhang, Xuezhi Wang2, Diyi Yang3\nGeorgia Institute of Technology,1Shanghai Jiao Tong University,2Google, 3Stanford University\n{albert.lu, z_yanzhe}@gatech.edu, 1icefox@sjtu.edu.cn\n2xuezhiw@google.com, 3diyiy@cs.stanford.edu\nAbstract\nThe limits of open-ended generative models\nare unclear, yet increasingly important. What\ncauses them to succeed and what causes them\nto fail? In this paper, we take a prompt-\ncentric approach to analyzing and bounding\nthe abilities of open-ended generative models.\nWe present a generic methodology of anal-\nysis with two challenging prompt constraint\ntypes: structural and stylistic. These con-\nstraint types are categorized into a set of well-\ndefined constraints that are analyzable by a\nsingle prompt. We then systematically cre-\nate a diverse set of simple, natural, and use-\nful prompts to robustly analyze each individual\nconstraint. Using the GPT-3 text-davinci-002\nmodel as a case study, we generate outputs\nfrom our collection of prompts and analyze\nthe model’s generative failures. We also show\nthe generalizability of our proposed method\non other large models like BLOOM and OPT.\nOur results and our in-context mitigation strate-\ngies reveal open challenges for future research.\nWe have publicly released our code at https:\n//github.com/SALT-NLP/Bound-Cap-LLM .\n1 Introduction\nThe recent success of large language models (LLM)\n(Brown et al., 2020; Devlin et al., 2018; Raffel\net al., 2019) has transformed the field of natural\nlanguage processing (NLP). In particular, prompt-\ning LLMs to generate open-ended text has shown\npromising performance. The existing and potential\napplications of open-ended text generation are far-\nreaching, spanning domains such as QA (Zhu et al.,\n2021), story generation (Fan et al., 2018), code\ngeneration (Chen et al., 2021a), human-assisted\ncreativity (Akoury et al., 2020), open-ended dia-\nlogue (Zhang et al., 2020), and the varied usages\nof ChatGPT 1. However, as LLMs continue to rise,\n∗Equal contribution.\n1https://chat.openai.com\nFigure 1: Example Model Failures: These two\nprompts are from our taxonomy and the two responses\nare generated by GPT-3. There are clear deficiencies\nthat are described further in this paper.\nthere is a growing amount of concern over the un-\npredictability of NLP systems, and thus a need to\nbetter understand their capabilities and limitations.\nAn extensive analysis of open-ended text genera-\ntion is imperative to understand their capabilities,\nlimitations, and areas for improvement.\nCurrent analyses of open-ended text generation\ncenter around general text attributes, such as gram-\nmar, coherence, and toxicity. These analyses are\nused to understand general aspects of model gener-\nations, but they do not analyze model performance\nin regards to the prompt. The next step in this field\nis to analyze prompt-specific performance by break-\ning down the vast space of open text generation into\na taxonomy of simple, natural, and useful prompts.\nA fine-grained understanding of what prompts a\nmodel can and can’t handle creates clear bounds on\nmodel capabilities, and drives model explainability\nand future directions for improvement.\nOne way to categorize prompts is by their con-\nstraints. The prompt “ Create a short and funny\njoke about research ” contains a variety of con-\nstraints. The output must be a joke (document-\ntype constraint), short (structural constraint), funny\n1982\n(stylistic constraint), and about research (subject\nconstraint). The space of open-ended generative\nprompts can be partitioned by their constraints be-\ncause all prompts are combinations of different\ntypes of constraints.\nIn this paper, we systematically evaluate model\nperformance on prompts that contain stylistic and\nstructural constraints. A stylistic constraint bounds\nthe style of the output, such as writing with a flow-\nery style, and a structural constraint bounds the\nstructure of the output, such as limiting the number\nof words in an output.\nWe chose to analyze stylistic and structural\nconstraints because they are omnipresent across\nprompts and notably challenging in literature\n(Ouyang et al., 2022; Reif et al., 2021). From for-\nmal emails to funny jokes, many generative applica-\ntions have style. Additionally, all generations have\nimplicit or explicit structural constraints, such as\nlength and proper formatting of an email or resume,\nand it is crucial for LLMs to understand them.\nWe create simple, natural, and useful base\nprompts for each category, and vary them in a num-\nber of dimensions to ensure a fine-grained and ro-\nbust analysis of each category. We use the public\nGPT-3 model as a case study to demonstrate the\neffectiveness of our proposed taxonomy2 and com-\nprehensively analyze the results quantitatively and\nqualitatively. We then test in-context mitigation\nstrategies and provide directions for future research\non the evaluation of open-ended generation.\nIn summary, our contributions are as follows:\n• We provide a taxonomy of prompts containing\nstylistic or structural constraints to facilitate\nfiner-grained analyses of open text generation.\n• We conduct a systematic experiment using our\ntaxonomy by creating 288 different prompts\nand evaluating 3000+ generated outputs to\nanalyze the capabilities and limitations of cur-\nrent LLMs on open-ended text generation.\n• We analyze in-context mitigation strategies\nto improve model performance and discuss\nfuture research for open text generation.\n2Note our methodology is general-purpose and can be used\nfor investigating other language models. We perform a small\nablation on other models, but we encourage future works to\nperform our full-scale analysis on other language models as\ndifferent models may behave differently.\n2 Related Work\nAnalysis of Large Language Models Many ex-\nisting benchmarks have been utilized to measure an\nLLM’s capabilities in natural language understand-\ning and generation (Wang et al., 2019; Sakaguchi\net al., 2019; Mostafazadeh et al., 2016; Rajpurkar\net al., 2018; Joshi et al., 2017; Mihaylov et al.,\n2018), where expected outputs are mostly deter-\nministic and/or short. There is also much research\nanalyzing general text attributes of open-ended text\ngenerations such as grammar, coherence, and toxi-\ncity. Dhamala et al. (2021) uses automated metrics\nto test for gender bias, toxicity, and sentiment in a\nvast array of Wikipedia-based prompts. Dou et al.\n(2021) creates a framework that analyzes GPT-3\noutputs for language errors, factual errors, or reader\nissues (such as usage of technical jargon).\nAdditionally, many studies use hand-crafted\nprompts to adversarially evaluate open-ended text\ngeneration models. Chowdhery et al. (2022) uses\nthe prompt \" All X are \" and calculates the aver-\nage toxicity of continuations to evaluate PaLM’s\nbias against group X. Gehman et al. (2020) de-\nsigns prompts that encourage toxic behavior from\na model. Lin et al. (2021) creates a dataset of hand-\ncurated prompts that elicit model hallucinations\nfrom GPT-3. In contrast, our goal is to investi-\ngate the open text generation capabilities of LLMs\nwith regard to constraints in the prompt because we\nseek a more nuanced and bounded understanding\nof model performance. Aspects like toxicity and\ngrammatically are important across all outputs, but\nthey don’t provide insight into how correctly an\nLLM responded to a prompt.\nControllable Text Generation Controlling\nmodel outputs to fit a set of constraints is in the\ndomain of controllable text generation. Chan et al.\n(2020) uses a content adapter to control model\noutputs. Krause et al. (2020) uses contrastive\ndecoding to create generations with stylistic or\ntopic constraints. Keskar et al. (2019) finetunes an\nLLM with inputs concatenated with an associated\nstyle token. However, creating these constraint-\ncentric outputs requires a matching dataset of\nconstrained text and an architectural shift. We\nevaluate controllable generation purely in-context\nand use comprehensive taxonomies instead of\nlimiting evaluations to existing datasets.\nMost similar to our paper, Reif et al. (2021) uses\nGPT-3 prompts to stylistically modify text and ask\n1983\nhuman raters to evaluate generation quality. In con-\ntrast, we provide a fine-grained analysis of model\nperformance on generating styled texts. Addition-\nally, we focus on creating a set of simple, natural,\nand useful prompts for analysis. Our goal is to\nunderstand the current capabilities and limitations\nof open-ended generative models.\n3 Methodology\nThe first step is to break down the constraint type\ninto a taxonomy of individual constraints. These in-\ndividual constraints must be analyzable by a single\nprompt with clear definitions of failure and success.\nWe create our taxonomies by considering how users\nnaturally put constraints in prompts.\n3.1 Prompt design\nPrior works (Reynolds and McDonell, 2021; Min\net al., 2022) show that prompt variance can have\na huge impact on model performance. To miti-\ngate this variability, we design our prompts in the\nfollowing two steps:\nDesign base prompt We first design a set of\nsimple and natural prompts as the base prompts\nfor each individual constraint. For example, our\nbase prompts for the stylistic constraint \"mood\" are\n“Write a passage about love that makes the reader\nfeel [angry, fearful, happy, sad].”\nCreate prompt variations We then vary those\nbase prompts by a number of important dimen-\nsions, such as subject and prompt template. For\nexample, we vary our prompts for mood by 2 ad-\nditional prompt templates (which are semantically\nidentical but syntactically different prompts), and\n2 additional subjects. These dimensions are not\nco-varied unless initial testing reveals important\npairs of dimensions.\nAll prompts use the base subject and template\nunless otherwise stated. A full list of the prompts\ncan be found in Appendix C.\nIn total, we create 288 prompts that facilitate\na robust and fine-grained analysis on an LLM’s\nopen-ended text generation capabilities.\n3.2 Output generation\nWe generate outputs using the GPT-3 series through\nOpenAI’s API as well as other publicly accessi-\nble LLMs such as OPT, BLOOM, and GLM. Our\nmain experiment is done on GPT-3 with model\ntext-davinci-002, with a sampling temperature\nof 0.7 and a max token length of 1024. 3 A high\ntemperature encourages creative and diverse out-\nputs, and a high max token length prevents maxi-\nmum length constraints. We generate 10 outputs\nper prompt to evaluate on. A sensitivity study on\nthe model and model parameters is shown in sec-\ntion 4.5.\n4 Stylistic Constraints\nStylistic constraints are present in all languages.\nThese stylistic modifications often comprise of an\nadjective prior to a document type: “ Write a for-\nmal email to my boss; Write a funny pickup line”.\nStylistic constraints are notably challenging for\nLLMs in zero-shot settings (Reif et al., 2021).\nOur stylistic constraints are grounded on exist-\ning work in the domain of Reader’s Advisory (RA).\nRA takes a user-centric approach to recommend-\ning books based on their stylistic features. An RA\ntaxonomy by function covers a diversity of stylistic\ntext features that could be useful for both a writer\nand an audience. We use a comprehensive RA\ntaxonomy found in Pera and Ng (2014). These\nfeatures are writing style, tone, mood, character-\nization, pacing, plot, and genre. 4 Each selected\nfeature is used to stylistically modify text in unique\nand powerful dimensions.\n4.1 Taxonomy\nWriting Style Writing style affects the complex-\nity of the language and literary devices in the text\nand how the text is detailed. Our base writing\nstyles are functional and flowery, and we test more\nadvanced writing styles along that spectrum. In\ntesting, we noticed that the style-subject pairing\nheavily influences model performance. We thus\ncovariate all subjects and writing styles.\nTone Tone reflects an author’s attitude toward a\ntopic. We chose four basic tones from Spiteri and\nPecoskie (2018) as our base prompts: dramatic,\nhumorous, optimistic, sad . We also choose an-\nother eight advanced tones as prompt variations.\nBecause a taxonomy of creative tone is not per-\nfectly aligned with common tones, we also analyze\ncommon tones in professional environments: for-\nmal, informal, assertive, passive-aggressive.\n3See model details here: https://platform.openai.\ncom/docs/model-index-for-researchers .\n4We leave out the features “frame” and “special topics”\nbecause“Special topics” is a subject constraint, and “frame” is\nan extension of tone.\n1984\nWriting Style Subject\nSunsets Strawberries Writing a paper\nFunctional 0.27±0.66 1.47±0.31 1.67±0.26\n0.40±0.83 1.50±0.43 1.53±0.48\nFlowery 1.03±0.77 0.63±1.00 1.03±0.48\n1.27±0.44 0.97±0.77 -0.13±0.92\nCandid 1.20±0.56 1.27±0.25 1.50±0.27\nProsaic 0.07±0.92 1.03±0.66 1.23±0.78\nOrnate 1.17±0.54 0.67±1.04 0.83±0.45\nPoetic 1.77±0.40 1.10±0.83 1.33±0.47\nTable 1: Results for Writing Style. The average of the\nannotation score (with standard error) is reported (each\nscore is in the range of (-2, 2)). Each row of Functional\nand Flowery represents a different prompt template (Se-\nmantically identical but syntactically different prompt).\nMood Mood describes how a work of writing\nmakes an audience feel. We chose four common ba-\nsic emotions in Spiteri and Pecoskie (2018) angry,\nfearful, happy, sad as our base prompts. Seven\nadvanced moods are selected as prompt variations.\nCharacterization A story’s characterization de-\nfines how it describes its characters. We chose to\nanalyze direct and indirect characterizations.\nPacing Pacing describes how fast a story is mov-\ning for a reader. Here, we test two generic cases:\nfast and slow paces.\nPlot A plot roughly outlines a story’s sequence\nof events. We analyze the seven basic plots\n(BOOKER, 2019): Overcoming the Monster,\nRags to Riches, The Quest, Voyage and Return,\nComedy, Tragedy, Rebirth. GPT-3 is unable to\ncreate classic “Comedy” and “Tragedy” plots due\nto their multiple meanings, our definition is ex-\npanded to include stories that are funny or sad.\nGenre A story’s genre is a categorization of its\nsubject matter. We choose 6 popular genres: His-\ntorical Fiction, Literary Fiction, Science Fiction,\nMystery, Dystopian, and Horror.\n4.2 Prompt Variation\nBeyond the previous variations, we vary all\nprompts by subject and prompt template. For writ-\ning style, we chose the subjects \"sunsets\", \"straw-\nberries\" and \"writing a paper\" to create variety\nacross the axis of functional to flowery subjects.\nFor the general stylistic constraints \"tone\" and\n\"mood\", we chose the document type passage and\nthe subjects love, life, humanity. These subjects fit\nour task because they are commonly expressed in a\nvariety of stylistic directions. For the story-centric\nstylistic constraints \"characterization, pacing, plot\nand genre\", we chose the document type story and\nthe varied and common subjects lovers, cats, sur-\nvivors. As plot and genre are both content-centric\nstylistic constraints, we also add “no-subject” as\na subject for baseline comparison. These subjects\nare common and varied in stories. We show the full\nprompt list in Appendix C.\n4.3 Evaluation\nWe used Amazon’s Mechanical Turk platform\n(AMT) to evaluate all outputs. For each output, we\nshowed the prompt and the definition of the style\nto workers, then we asked workers three questions:\n1. \"Regarding the [aspect] of the response, to\nwhat extent do you agree the response fulfills\nthe prompt?\"\n2. \"How difficult is it to create a valid response\nto this prompt?\"\n3. \"Do you observe any other failures (e.g.,\ninconsistency, unverified facts, not a\nstory/passage) in the response?\"\nWe used a 5-point Likert scale (-2 to 2) for the\nfirst question to evaluate the style of the response\n, and a 10-point Likert scale (1 to 10) for the sec-\nond question to evaluate prompt difficulty. The\nthird question is designed to allow annotators to\nwrite down failures orthogonal to the stylistic con-\nstraints which can facilitate additional qualitative\nanalysis. The overall inter-annotator agreement\n(Krippendorff’s α) for the first question is 0.31.\nMore details and the interface for annotation are\nshown in Appendix A.\n4.4 Results\nThe results for writing style are in Table 1, tone and\nmood are in Table 2, and characterization, pacing,\nplot, and genre are in Table 3. As expected, GPT-3\nstruggles with comedy and other challenging stylis-\ntic constraints such as satire, irony, and literary\nfiction. Otherwise we focus on several key findings\nhere, and a per-aspect analysis along with qualita-\ntive examples of the findings are in Appendix B.1.\nGPT-3 is sensitive to style-subject pairings.\nFrom Table 1, GPT-3 cannot write prosaically or\nfunctionally about sunsets, or ornately about writ-\ning a paper. From Table 3, GPT-3 can create in-\ndividual characters from the subject \"lovers\", but\n1985\nAspect Category Base Template Subject Mean\n2 3 4 Life Humanity\nTone\nDramatic 1.1 ±0.7 1.43±0.5 1.37±0.28 / 1.37 ±0.38 1.5±0.22 1.35\nHumorous -0.5 ±0.48 -0.2±0.6 0.3±1.17 / -0.1 ±0.9 -0.03±0.92 -0.11\nOptimistic 1.3 ±0.43 1.63±0.48 1.6±0.36 / 1.7 ±0.23 1.67±0.26 1.58\nSad 1.27 ±0.36 1.03±0.64 1.17±0.6 / 1.5 ±0.4 1.17±0.48 1.23\nMood\nAngry 0.37 ±0.41 0.93±0.8 0.2±0.9 0.83±0.89 0.8±0.96 1.2±0.62 0.72\nFearful 0.57 ±0.7 0.77±0.54 0.77±0.52 0.67±0.86 1.4±0.42 1.33±0.3 0.92\nHappy 1.57 ±0.26 1.3±0.28 1.4±0.33 1.37±0.31 1.47±0.31 1.33±0.54 1.41\nSad 1.27 ±0.59 1.3±0.46 1.03±0.46 0.9±0.68 1.33±0.49 0.9±0.58 1.12\nTable 2: Results for basic tones and moods. All but subject variations use subject “love”.\nAspect Category Base Template Subject Mean\n2 3 Cats Survivors None\nCharacterizationDirect 1.0 ±0.54 0.77±0.87 0.33±0.77 0.53±0.65 0.5±0.82 / 0.63\nIndirect 0.7 ±0.64 0.93±0.42 0.77±0.37 0.87±0.58 0.1±0.72 / 0.67\nPacing Fast 1.23 ±0.72 0.77±0.7 1.3±0.31 1.03±0.6 0.9±0.58 / 1.05\nSlow 0.53 ±0.88 0.7±0.55 0.97±0.62 0.73±0.76 0.67±0.67 / 0.72\nPlot\nOvercoming the Monster 0.37±0.91 1.0±0.75 / 0.7 ±0.94 1.33±0.3 1.53±0.31 0.99\nRags to Riches 1.33 ±0.71 0.77±0.87 / 0.5 ±0.85 0.27±0.9 1.53±0.65 0.88\nThe Quest 1.33 ±0.54 1.2±0.48 / 1.37 ±0.38 1.27±0.39 1.6±0.25 1.35\nV oyage and Return 1.07±0.53 1.27±0.42 / 1.33 ±0.54 1.1±0.54 1.3±0.28 1.21\nComedy -0.3 ±0.9 -0.3±0.84 / -0.07 ±0.99 -0.5±0.48 0.03±0.85 -0.23\nTragedy 1.6 ±0.39 1.8±0.27 / 1.27 ±0.59 0.63±0.38 1.5±0.4 1.36\nRebirth 1.13 ±0.56 1.33±0.65 / 0.93 ±0.81 1.03±0.55 1.4±0.39 1.16\nGenre\nHistorical fiction 0.77 ±0.93 1.07±1.08 0.97±0.72 -0.2±0.93 0.43±0.92 1.13±0.99 0.70\nLiterary fiction 0.87 ±0.65 0.8±0.48 0.97±0.57 0.4±0.84 0.9±0.6 0.27±0.42 0.70\nScience fiction 0.47 ±0.76 0.9±0.82 0.37±0.84 1.5±0.31 1.13±0.5 1.47±0.52 0.97\nMystery 1.1 ±0.58 1.6±0.39 1.23±0.45 1.4±0.36 0.73±0.9 1.67±0.45 1.29\nDystopian 1.37 ±0.43 1.63±0.43 1.5±0.45 1.53±0.56 1.6±0.33 1.8±0.31 1.57\nHorror 1.23 ±0.67 1.07±0.93 1.63±0.28 1.4±0.74 1.57±0.65 1.47±0.62 1.40\nTable 3: Results for story-centric stylistic constraints. All but subject variations use the subject \"lovers\".\nit fails to characterize the subjects \"survivors\" or\n\"cats\". Similarly from Table 3, GPT-3 can’t write\nstories about \"lovers\" Overcoming the Monster, but\nit can about \"cats\" or \"survivors\" Overcoming the\nMonster. This indicates that the model might use\nspurious correlations between style and subject in-\nstead of having an isolated understanding of style.\nGPT-3 confuses style with subject when the\nprompt is too challenging. GPT-3 writes about\nfunny things when asked to be humorous or write\na comedy, but the outputs are not funny by them-\nselves. When asked to write a passage that makes\nthe reader feel anger or fear, GPT-3 writes candidly\nabout anger and fear. This occurs more often with\nworse performing styles, and it appears that it uses\nthe style as a subject when it’s unsure of how to\ncreate the style. It might be because GPT-3 doesn’t\nunderstand the purpose of style in lower probability\nprompts, and thus uses the style as a subject.\nGPT-3 struggles with words that are not unique\nto creative writing. The writing style subject\n“strawberries” can be written about both function-\nally and creatively, but GPT-3 fails to write flowery\nor ornately about strawberries. GPT-3 also fails\nto create “historical” or “science fiction”, and to\ncreate classic “Comedies” and “Tragedies”. This\nmight be because GPT-3 struggles to stylistically\nuse words that have meaning beyond creative writ-\ning due to a dataset imbalance between creative\nand functional text.\nGPT-3’s performance has no correlation with\nthe prompt difficulty perceived by annotators.\nAs shown in Figure 2, Spearman’s correlation be-\ntween model performance and the difficulty of the\n1986\nFigure 2: Relation between different prompts’ diffi-\nculty and score. The spearman’s correlation is -0.15.\nFigure 3: Results on different model sizes and tem-\nperatures, using the averaged scores over 7 prompts.\nprompt as perceived by annotators is -0.15, show-\ning no correlation. Annotators perceive writing a\nstory with a \"Comedy\" plot as easy while GPT-3\nperforms extremely poorly. Annotators perceive\nprompts with complex genres or plots like\"rebirth\"\nand \"dystopian\" as hard while the model performs\nwell. This is a strong result that indicates that the\nfactors that contribute to prompt difficulty differ\nbetween humans and LLMs. This reinforces the im-\nportance of our work in empirically finding which\nprompts are and aren’t challenging for LLMs.\n4.5 Scale and Temperature Variation\nTo analyze sensitivity to model parameters, we\nchose seven base prompts (one per stylistic con-\nstraint, shown in Table 11). We prioritized\naverage-scoring prompts to establish a baseline\nwhen comparing different models and parame-\nters. Apart from our default setting of using\ntext-davinci-002 (D, 176B) with temperature\n0.7, we experimented with three different en-\ngines from OpenAI’s API: text-ada-001 (A),\ntext-babbage-001 (B), text-curie-001 (C),\nwhich correspond to InstructGPT models of 350M,\nFigure 4: Effect of the mitigation strategy, using the\naveraged annotation scores over 7 prompts.\n1.3B and 6.7B parameters and two additional tem-\nperatures of 0.4 and 1.0. 5 The aggregated results\nare shown in Figure 3.\nModel Scale Variation As expected, smaller mod-\nels perform worse, with the exception of C perform-\ning worse than B, which is due to the extremely low\nperformance of C on the humorous tone constraint.\nTemperature VariationPerformance rose slightly\nfor both additional temperatures. We examined the\noutputs and noticed that a higher temperature cre-\nates better results, but a lower temperature repeats\nan output that happens to perform well as seen in\nAppendix B.3.\n4.6 In-context Mitigation Helps\nWe tested three in-context mitigation strategies\nfrom the literature on the same prompts as Sec-\ntion 4.5, with the same experimental settings:\n• Definition Prepend the definition of the style\n(the same one we showed the annotators) to\nthe prompt to provide information about the\ntask.\n• Demonstration Prepend one well-answered\ndemonstration to help the model understand\nthe task, following the one-shot setting from\nBrown et al. (2020).\n• Explanation Add an explanation of why\nthe demonstrated response is correct after\nthe one-shot demonstration (Lampinen et al.,\n2022).An example is shown in Appendix C.1\nAs shown in Figure 4, all mitigations positively\nimpact performance primarily by improving perfor-\nmance on the \"humorous tone\" prompt. However,\n5More details at https://help.openai.com/en/\narticles/5832130.\n1987\nthese mitigations are unnatural prompts, and the\nresults are still far below optimal.\n5 Structural Constraints\nStructural constraints are omnipresent: “Write an\nessay in fewer than 1000words; Limit your pa-\nper to 8 pages”. Structural constraints are notably\nchallenging for LLMs (Ouyang et al., 2022).\nStructure in the field of NLP is a broad term. We\nspecifically analyze structural aspects of the text\nthat are orthogonal to the actual content of the out-\nput. This includes length, spacing, and formatting,\nand excludes content-centric attributes such as syn-\ntax or semantics. Our taxonomy is based on how\na user could conceivably request a structural con-\nstraint within their prompt. We choose to analyze\nnumerical, descriptive, and formatting structural\nconstraints in this paper, but we note that this is not\ncomprehensive of the entire structural space.\n5.1 Taxonomy\nNumerical Constraining text to a set or a\nbounded number of words, sentences, or para-\ngraphs is valuable in all aspects of writing. We\ncreate prompts with numerical requirements: five,\nten, twenty on three different language structure\nlevels: word, sentence, and paragraph.\nDescriptive Structural constraints can also be de-\nscriptive, such as a \"concise email\" or an \"in-depth\ndiscussion question.\" We choose the structural de-\nscriptors short, brief, concise and long, detailed,\nin-depth in our experiments.\nFormatting When a user requests a document\nsuch as a resume or an email, there is an expecta-\ntion of a specific format. An LLM should under-\nstand how to properly space and format specific\ndocument types. We analyze three common for-\nmatting types code, email, and academic papers.\n• Code: Testing a model’s coding ability\nis a popular field with many applications\n(Hendrycks et al., 2021). We use natural in-\nstructions as prompts and focus on the format\nof the generated code. We evaluate on two\npopular programming languages Python and\nC, and two common coding problems create\nthe game of war and sums two integers.6\n6Note that we focus on the “formatting” perspective of the\ngenerated code, rather than the correctness of the code as in\nmany existing works (Chen et al., 2021b).\n• Email: We evaluate different scenarios with\nthree different readers teacher, boyfriend,\nclient and two different levels of email detail\nin the prompt.\n• Academic paper: A properly formatted aca-\ndemic paper should be segmented into sec-\ntions such as an abstract, introduction, and\nconclusion7. We prompted LLM to generate\nacademic papers on three different topics: Ar-\ntificial Intelligence, the flaws of GPT-3, strate-\ngies our society can adopt to recover from the\nglobal pandemic.\nPrompt Variation Beyond the variations de-\nscribed in the taxonomy, we vary all prompts by\nprompt template. We additionally vary prompts\nwith numerical and descriptive structural con-\nstraints by the subjects Love, Cats, and Running\nfor diversity. An example prompt is \"Write a sen-\ntence with five words about love.\"\nEvaluation For numerical and descriptive struc-\ntural constraints, we automatically calculate the\ncounts and manually verify the quality of the evalu-\nations. For formatting constraints, we look through\nthe generated texts and evaluate them based on\ntheir format. Emails, code, and academic papers\nare simple to evaluate on formatting constraints.\n5.2 Results\nGPT-3’s understanding of structure is accurate but\nnot precise. In general, many of its outputs are\nclose to or trend towards fulfilling the structural\nconstraint, but don’t precisely fulfill it. A full anal-\nysis of each section is provided in Appendix B.2,\nand the main takeaways are below.\nGPT-3 fails with numerical structural con-\nstraints As shown in Figure 5, The model seldom\ngenerates the text with the required length. And\nthe performance worsens as the required length in-\ncreases. It fails at a rate of 0.46, 0.78 and 1 for five,\nten and twenty respectively. GPT-3 doesn’t seem to\nlearn how to count words, sentences, or paragraphs\nin training. However, the results are often close to\nthe requested number, which implies that GPT-3\nhas some concept of numerical structure.\nGPT-3 shows high variance with descriptive\nstructural constraints like long As seen in Fig-\nure 6, when the prompt contains structural descrip-\ntors like long, the output is of extremely variable\n7We asked GPT-3 about this, and it gives a similar opinion,\nso we expect it to fulfill this constraint.\n1988\nFigure 5: Results on numerical constraints. The distribution of actual counts of generated text.8 In each subfigure,\nthe required count is denoted with a reference line.\nFigure 6: Results on descriptive constraints. Different\nshapes represent different prompt templates.\nlength and overlaps in length with responses gen-\nerated for short a considerable proportion (20%)\nof the time. This may be caused by the intrinsic\nvariable length of long text the model sees in pre-\ntraining data since long/short is a relative concept.\nGPT-3 fails to properly format academic papers\nGPT-3 doesn’t generate text with the right formats\nor sections when asked to write an academic paper,\nalthough it succeeds with other document types\nsuch as emails or code. Document types such as\nemails or code are often given pseudo-labels with\n\"email\" or \"code\", but academic papers have ti-\ntles that don’t reference their document type. We\nhypothesize that this may cause models to strug-\ngle with connecting the document type \"academic\npaper\" to those documents present in training data.\nScale and Temperature Variation We also con-\nducted experiments similar to Section 4.5 with\nall the numerical constraint prompts. Our auto-\nmatic evaluation shows that smaller models per-\nform slightly worse across the board and different\nAspect Example Terms Fail\nWriting Style Flowery, Functional Sometimes\nTone Humorous, Formal Occasionally\nMood Angry, Sad Sometimes\nCharacterization Direct, Indirect Often\nPacing Fast, Slow Often\nPlot Rebirth, Comedy, Occasionally\nGenre Science Fiction, Mystery Sometimes\nNumerical Five words, Ten sentences Often\nDescriptive Concise, Long Occasionally\nFormatting Email, Code Occasionally\nTable 4: Summary of our taxonomy and results. We\nshow the full list of prompts in Appendix C.\ntemperatures do not vary the performance much.\nThe full results are in Appendix B.2.4.\n6 LLMs other than GPT-3\nOur methodology is general and can be used to\nanalyze any LLMs. We ran trials on other pub-\nlicly available LLMs: OPT-176B 9(Zheng et al.,\n2022), BLOOM-176B 10 and GLM-130B 11(Du\net al., 2022) using the same 7 base prompts as\nsection 4.5 and 3 additional base prompts from our\nnumerical structural constraints taxonomy. Some\nmodel parameters are changed due to differences\nin models and API limitations. For GLM and\nBLOOM, we use the maximum possible length\n(256 and 250 respectively) as well as the default\nsettings of temperature = 0.7, top-p = 1. For OPT,\nwe chose a smaller max length of 128 due to output\ninstability at higher max lengths.\nAs shown in Table 5, we found that outputs\n9https://opt.alpa.ai/\n10https://huggingface.co/bigscience/bloom\n11https://huggingface.co/spaces/THUDM/GLM-130B\n1989\nLLM Degenerate Rate Mean Score\nGPT-3 0% 0.77\nOPT-176B 53% -0.94\nBLOOM-176B 71% -1.41\nGLM-130B 57% -1.01\nTable 5: Results for other LLMs on a trial exper-\niment with 7 prompts from Table 11. For GPT-3,\ntext-davinci-002 is used here.\nare sometimes degenerate, such as repeating the\nprompt. All responses are manually inspected, and\ndegenerate responses are removed from the annota-\ntion pool and automatically marked as -2. Models\nother than GPT-3 all performed much worse with\nmore than half their generations being degenerate.\nThis may due to noisier pre-training datasets and\na lack of instruction-aligned training. We find that\nsome patterns such as style-content confusion still\nhold for these LLMs, although a full analysis of\nthese and other models such as LaMDA (Thoppilan\net al., 2022) and PaLM (Chowdhery et al., 2022) is\nneeded to reveal clearer patterns.\n7 Conclusion\nWe present a generic methodology to analyze a\nlanguage model’s ability to generate open-ended\ntext under structural and stylistic constraints. Our\nresults show many failures that align with noted\nmodel challenges as well as new patterns of failure\nacross structural and stylistic constraints. Our sen-\nsitivity studies on model size show a rising trend\nrather than the emergence (Wei et al., 2022) of\nstylistic and structural constraints. Our mitigations\ndemonstrate that adding additional in-context infor-\nmation consistently improves performance across\nboth domains. Future work could expand our work\nto look at other constraint types and more sophisti-\ncated mitigation strategies.\nLimitations\nWe tried to maximize the coverage of our taxonomy,\nbut it doesn’t cover all aspects of stylistic and struc-\ntural constraints. Additionally, our taxonomy is\nnot representative of all open-text generations, and\nfurther work is needed to cover more dimensions\nin the open-text generation space. Our prompts are\nnot optimized for performance (due to a require-\nment of being natural, simple, and useful) and it is\nan active area of research to optimize a prompt for\nperformance in a variety of tasks.\nOur taxonomies are not empirically user-centric.\nOne could extend our taxonomy by studying how a\ndiverse set of real users use or visualize the use of\nan open-ended text generation model, and building\na taxonomy on existing or envisioned use cases.\nThe model performance and the prompt’s diffi-\nculties are annotated by the workers from MTurk,\nand therefore reflect more accurately a small group\nof human’s perceptions, though this is the common\npractice. We do not rigorously test what aspect of\nthe LLMs (dataset, training regime, etc.) causes\nour results. We only provide our compiled observa-\ntions and potential hypotheses.\nEthical Considerations\nStyle Misuse Styled text has the potential for\nharm. Creating models with the potential to mass-\nmanufacture text with certain tones and moods such\nas “mad, fearful, and bleak” can negatively affect\ndownstream readers. Creating accurate “histori-\ncal fiction” can perpetuate harmful attitudes in the\npast. There is much discussion on the usage of\nlarge language models to generate undesirable text.\nHowever, there are countless legitimate usages of\nnegatively styled text in all forms of writing, from\ndialogue to poetry. Although we note the risk of\nmisuse, providing style dramatically enhances the\nscope of creative expression in open-ended text\ngeneration, and is an overall positive contribution.\nAnnotator Harm Reading large quantities of\ntext with certain styles, such as bleak tones, an-\ngry moods, or horror genres, can potentially be\nharmful to annotators. We sampled the generated\noutputs and note that they are fairly mild and non-\ntoxic. However, as models improve at generating\nmore powerful and impactful styles, strong guide-\nlines such as HIT limits or toxicity filters should be\nput in place to protect annotators.\nReferences\nNader Akoury, Shufan Wang, Josh Whiting, Stephen\nHood, Nanyun Peng, and Mohit Iyyer. 2020.\nSTORIUM: A dataset and evaluation platform\nfor machine-in-the-loop story generation. CoRR,\nabs/2010.01717.\nCHRISTOPHER BOOKER. 2019. Seven basic plots:\nWhy we tell stories . BLOOMSBURY CONTIN-\nUUM.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\n1990\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. CoRR,\nabs/2005.14165.\nAlvin Chan, Yew-Soon Ong, Bill Pung, Aston Zhang,\nand Jie Fu. 2020. Cocon: A self-supervised\napproach for controlled text generation. CoRR,\nabs/2006.03535.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Ponde de Oliveira Pinto, Jared Kaplan,\nHarrison Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-V oss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Joshua Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. 2021a. Evaluat-\ning large language models trained on code. CoRR,\nabs/2107.03374.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Ponde de Oliveira Pinto, Jared Kaplan,\nHarri Edwards, Yuri Burda, Nicholas Joseph, Greg\nBrockman, et al. 2021b. Evaluating large lan-\nguage models trained on code. arXiv preprint\narXiv:2107.03374.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. CoRR, abs/1810.04805.\nJwala Dhamala, Tony Sun, Varun Kumar, Satyapriya\nKrishna, Yada Pruksachatkun, Kai-Wei Chang, and\nRahul Gupta. 2021. Bold: Dataset and metrics for\nmeasuring biases in open-ended language generation.\nIn Proceedings of the 2021 ACM Conference on Fair-\nness, Accountability, and Transparency, FAccT ’21,\npage 862–872, New York, NY , USA. Association for\nComputing Machinery.\nYao Dou, Maxwell Forbes, Rik Koncel-Kedziorski,\nNoah A. Smith, and Yejin Choi. 2021. Scarecrow:\nA framework for scrutinizing machine text. CoRR,\nabs/2107.01294.\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,\nJiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. GLM:\nGeneral language model pretraining with autoregres-\nsive blank infilling. In Proceedings of the 60th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 320–335,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nAngela Fan, Mike Lewis, and Yann Dauphin. 2018.\nHierarchical neural story generation.\nSamuel Gehman, Suchin Gururangan, Maarten Sap,\nYejin Choi, and Noah A. Smith. 2020. Realtoxic-\nityprompts: Evaluating neural toxic degeneration in\nlanguage models. CoRR, abs/2009.11462.\nDan Hendrycks, Steven Basart, Saurav Kadavath, Man-\ntas Mazeika, Akul Arora, Ethan Guo, Collin Burns,\nSamir Puranik, Horace He, Dawn Song, et al. 2021.\nMeasuring coding challenge competence with apps.\narXiv preprint arXiv:2105.09938.\nMandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. CoRR, abs/1705.03551.\nNitish Shirish Keskar, Bryan McCann, Lav R. Varshney,\nCaiming Xiong, and Richard Socher. 2019. CTRL:\nA conditional transformer language model for con-\ntrollable generation. CoRR, abs/1909.05858.\nBen Krause, Akhilesh Deepak Gotmare, Bryan McCann,\nNitish Shirish Keskar, Shafiq R. Joty, Richard Socher,\nand Nazneen Fatema Rajani. 2020. Gedi: Generative\ndiscriminator guided sequence generation. CoRR,\nabs/2009.06367.\nAndrew K. Lampinen, Ishita Dasgupta, Stephanie C. Y .\nChan, Kory Matthewson, Michael Henry Tessler, An-\ntonia Creswell, James L. McClelland, Jane X. Wang,\n1991\nand Felix Hill. 2022. Can language models learn\nfrom explanations in context?\nStephanie Lin, Jacob Hilton, and Owain Evans. 2021.\nTruthfulqa: Measuring how models mimic human\nfalsehoods. CoRR, abs/2109.07958.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\nSabharwal. 2018. Can a suit of armor conduct elec-\ntricity? a new dataset for open book question answer-\ning. In EMNLP.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. 2022. Rethinking the role of demonstrations:\nWhat makes in-context learning work?\nNasrin Mostafazadeh, Nathanael Chambers, Xiaodong\nHe, Devi Parikh, Dhruv Batra, Lucy Vanderwende,\nPushmeet Kohli, and James Allen. 2016. A corpus\nand cloze evaluation for deeper understanding of\ncommonsense stories. In Proceedings of the 2016\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 839–849, San Diego,\nCalifornia. Association for Computational Linguis-\ntics.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback.\nMaria Soledad Pera and Yiu-Kai Ng. 2014. Automating\nreaders’ advisory to make book recommendations\nfor k-12 readers. In Proceedings of the 8th ACM\nConference on Recommender Systems, RecSys ’14,\npage 9–16, New York, NY , USA. Association for\nComputing Machinery.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. CoRR, abs/1910.10683.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you don’t know: Unanswerable questions\nfor squad. CoRR, abs/1806.03822.\nEmily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen,\nChris Callison-Burch, and Jason Wei. 2021. A recipe\nfor arbitrary text style transfer with large language\nmodels. CoRR, abs/2109.03910.\nLaria Reynolds and Kyle McDonell. 2021. Prompt\nprogramming for large language models: Beyond the\nfew-shot paradigm. CoRR, abs/2102.07350.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-\nula, and Yejin Choi. 2019. WINOGRANDE: an ad-\nversarial winograd schema challenge at scale. CoRR,\nabs/1907.10641.\nSpiteri and Pecoskie. 2018. Expanding the scope of\naffect: taxonomy construction for emotions, tones,\nand associations. Journal of Documentation, V ol. 74\nNo. 2 pp. 383-397.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam\nShazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.\n2022. Lamda: Language models for dialog applica-\ntions. arXiv preprint arXiv:2201.08239.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel R. Bowman. 2019. Superglue: A stickier\nbenchmark for general-purpose language understand-\ning systems. CoRR, abs/1905.00537.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, Ed H.\nChi, Tatsunori Hashimoto, Oriol Vinyals, Percy\nLiang, Jeff Dean, and William Fedus. 2022. Emer-\ngent abilities of large language models.\nYizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen,\nChris Brockett, Xiang Gao, Jianfeng Gao, Jingjing\nLiu, and Bill Dolan. 2020. DIALOGPT : Large-scale\ngenerative pre-training for conversational response\ngeneration. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics:\nSystem Demonstrations, pages 270–278, Online. As-\nsociation for Computational Linguistics.\nLianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao\nZhuang, Zhifeng Chen, Yanping Huang, Yida Wang,\nYuanzhong Xu, Danyang Zhuo, Joseph E Gonza-\nlez, et al. 2022. Alpa: Automating inter-and intra-\noperator parallelism for distributed deep learning.\narXiv preprint arXiv:2201.12023.\nFengbin Zhu, Wenqiang Lei, Chao Wang, Jianming\nZheng, Soujanya Poria, and Tat-Seng Chua. 2021.\nRetrieving and reading: A comprehensive survey on\nopen-domain question answering.\n1992\nTone Mood\nCategory Score Category Score\nEmotional 1.53 Envious 0.1\nNostalgic 1.13 Anxious 0.97\nUplifting 1.67 Proud 0.9\nInspirational 1.77 Regretful 1.2\nBleak 1.7 Surprised -0.07\nGrim 1.23 Loved 1.13\nIronic 0.23 Disgusted 0.07\nSatirical -0.8\nFormal 1\nInformal 1.27\nAssertive 0.8\npassive-aggressive -0.1\nTable 6: Results for advanced tones and moods. The\nsubject “love” is used.\nA Annotation Details\nFor each output, we recruited three workers and\ngave a reward of $0.11 for short responses and\n$0.15 for long responses as well as a $1.00 bonus\nfor 1% of prompts if the prompt was answered cor-\nrectly. This is roughly equivalent to $15/hr given\naverage work rates of 48 and 64 seconds.\nWe recruited workers from English-speaking\ncountries (US, Canada, UK, Australia), and with at\nleast a 98% approval rate. We also created a quali-\nfication test with easy question/response pairs, and\nrequired a minimum 5/6 accuracy to see our tasks.\nThe annotation interface is shown in Figure 7.\nB Additional results\nB.1 Full Stylistic Analysis\nB.1.1 Writing style\nThe results are shown in Table 1. GPT-3 fails when\nthere is a mismatch between the writing style and\nthe subject. GPT-3 cannot write prosaically about\n“sunsets”, or ornately about “writing a paper”. Ad-\nditionally, our intermediate subject “strawberries”\nfails when matched with a flowery, ornate, or po-\netic writing styles. We hypothesize that expressive\nwriting styles are limited to a very small set of sub-\njects due to an oversaturation of functional writing\nin commonly used datasets.\nB.1.2 Tone\nAs shown in Table 2 and Table 6, GPT-3 consis-\ntently fails with more challenging tones, such as\nhumorous, satirical, ironic, and passive-aggressive.\nThe generated passages aren’t satirical or ironic.\nThe generated humorous passages are optimistic,\nlight, and often use the word \"funny\", but they\naren’t funny. A passive-aggressive tone is challeng-\ning to create because it requires context to under-\nstand the hidden meaning of the text. Thus, at best\nGPT-3 ends up writing overly nice passages about\nlove, but more often there is no tone in the text.\nHowever, GPT-3 is skilled at creating the other\nless challenging tones. We hypothesize that GPT-3\nsucceeds because an abundance of shallow tropes\ncan functionally create tone, though the outputs are\noften repetitive or similar.\nB.1.3 Mood\nAs shown in Table 2 and Table 6, GPT-3 struggles\nwith creating “anger” and “fear”. Of the more chal-\nlenging tones, GPT-3 fails the most with “surprise”,\n“disgust”, and “envy”.\nWe hypothesize that the mood-subject pairing\nis crucial for model performance. Our base sub-\nject, \"love\", is theoretically capable of enabling all\nmoods, but moods such as “happy”, “sad”, \"anx-\nious\" and \"regretful\" are more popular than others\nin the context of “love”. GPT-3 is more proficient\nat creating “anger” or “fear” through content about\n“life” or “humanity”.\nWhen failing, GPT-3 confuses mood with sub-\nject matter. GPT-3 writes candidly about the exis-\ntence of fear and anger in love rather than about\ncontent that would cause a reader to be angry or\nfearful. Because this content-style confusion is\nmore common in challenging mood-subject pairs,\nwe hypothesize that this is a fallback mechanism if\nthe model is unable to create mood.\nB.1.4 Characterization\nThe result is shown in the first block of Table 3.\nSurprisingly, GPT-3 performs worse on direct char-\nacterization than indirect characterization. We hy-\npothesize that this is due to a general failure of\ncharacterization. Given a subject like “cats” or\n“survivors”, GPT-3 often uses \"cats\" or “survivors”\nas the main subject instead of creating and detailing\nindividual characters. When GPT-3 fails to create\nclear characters, direct characterization is more no-\nticeably incorrect than indirect characterization.\nPacing As shown in the second block of Table 3,\nGPT-3 struggles to create slow-paced stories. GPT-\n3 often creates a fast-paced story with a slow-paced\nprompt. This is expected, as slow-paced stories are\n1993\nFigure 7: AMT interface for annotation.\nnaturally more challenging, especially considering\nGPT-3’s inability to create longer stories.\nPlot The result is shown in the third block of Ta-\nble 3. GPT-3’s inability to create “Comedies” is\nconsistent with other failures to make funny con-\ntent. The outputs for a “Comedy” plot are filled\nwith comedy shows, clubs, and even roller coasters,\nbut they aren’t funny.\nOtherwise, our results for story generation vary\nquite substantially. \"Overcoming the Monster\" is\nthe worst performing plot with the subject \"lovers\",\nbut the best performing plot with the subject \"sur-\nvivors\". \"Rags to Riches\" is the best performing\nplot for the subject \"lovers\" but the worst perform-\ning plot for the subjects \"cats\" and \"survivors\". We\nhypothesize that the plot-subject pair is crucial to\nmodel performance.\nGenre As shown in the last block of Table 3,\nGPT-3 struggles with literary fiction, but surpris-\ningly just as much with historical and science fic-\ntion. Literary fiction is profound and complex, and\nit’s intuitive that GPT-3 fails.\nHowever, historical fiction outputs often have\nzero historical elements, and science fiction outputs\noften have zero science fiction elements. This fail-\nure is unexpected, and we hypothesize that GPT-3\nstruggles with the words \"historical\" and “science”\nbecause their meaning pervades past creative writ-\ning.\nAdditionally, GPT-3 often creates teasers or in-\ntros to stories instead of a story itself. This may\nbe intentional due to GPT-3’s inability to generate\nlonger or complex stories, but it diminishes the\nquality of story outputs across the board.\nExamples of each Results section Examples of\nprompt/response pairs that exemplify each main\ntakeaway from the stylistic section are in Table 7,\nTable 8, and Table 9.. Each prompt/response\npair is a cherrypicked example of the takeaway,\nbut the general trends are prevalent across all\nprompt/response pairs.\nB.2 Full Structural Analysis\nB.2.1 Numerical\nThe results of numerical structural constraints are\nshown in Figure 5. GPT-3 fails at this task. The\nmodel seldom generates the text with the required\nlength. And the performance worsens as the re-\nquired length increases. It fails at a rate of 0.46,\n0.78 and 1 for five, ten and twenty respectively.\nAdditionally, we noticed strange behavior when\nusing Elon Musk as the subject. GPT-3 consis-\ntently generates the same section of the Elon’s\nWikipedia page with longer numerical or descrip-\ntive constraints. However, we didn’t observe this\nbehavior on other entities, and decided to leave out\nentities because they were too variable.\nWe provide additional results with alternative\nprompt templates in Figure 8 which show similar\ntrends.\n1994\nB.2.2 Descriptive\nWe show the distribution of the number of sen-\ntences in response to descriptive structural con-\nstraints in Figure 6. The model typically gener-\nates longer text for descriptors long (detailed, in-\ndepth) compared to descriptors short (brief, con-\ncise), which shows the model has a decent under-\nstanding of descriptive constraints. However, there\nare some flaws.\nFirst, the length of the responses to long descrip-\ntors is highly variable and often overlaps with short\ndescriptors. For example, the descriptor long varies\nconsiderably and overlaps with responses gener-\nated for short for a considerable proportion (20%).\nThis is consistent with the results in the numeri-\ncal constraints section.\nB.2.3 Formatting\nCode GPT-3 mostly succeeds at generating prop-\nerly formatted code, with an average failure ratio of\n0.2 with the exception of the prompt Write Python\ncode that plays the game of war: where 9 out of 10\nresponses are lists of the process of the game of war\ninstead of code. This particular failure only occurs\nin the unique combination of the verb \"Write\", the\nlanguage \"Python\", and the task \"game of war\".\nEmail The model can write properly formatted\nemails well, regardless of writer, topic, or reader.\nThe only flaw is that it doesn’t output an email\nsignature 10% of the time.\nAcademic paper GPT-3 fails to properly format\nan academic paper. Our only requirement is that\nthe output contains some organization with some\nsections out of an abstract, introduction, related\nworks, etc. GPT-3 rarely generates text with any\nsectioning or organization.\nB.2.4 Sensitivity results for Structural\nConstraints\nThe results on numerical constraints with template\n2 is shown in Figure 8. The results with model\ntext-curie-001, text-babbage-001 are shown\nin Figure 9, 10 respectively. The results with tem-\nperature 0, 0.4, 0.9 are shown in Figure 11, 12, 13\nrespectively.\nB.3 GPT-3 Behavior at low temperatures\nThe prompt “Write a humorous passage about\nlove:” is a notably challenging prompt for LLMs.\nWhen davinci-002 has a temperature of 0.4, all 10\noutputs start one of two ways. The first is “Love is\na many splendored thing, but it can also be a pain\nin the neck” and occurs 5 times with an average\nannotation score of -.13. The second is “Love is\na beautiful thing, but it can also be quite funny at\ntimes.” that also occurs 5 times with an average\nannotation score of 1.4 which is incredibly high\nfor this prompt. We agree that this lack of diver-\nsity hampers evaluation on lower temperatures, and\nnote that our evaluations work best on diverse out-\nputs.\nC Full prompt list\nWe show all the prompts we designed in Table 10.\nOur prompts used for temperature and model sensi-\ntivity experiments and other LLM experiments are\nin Table 11\n1995\nFigure 8: Results on numerical constraints with Template 2. The distribution of actual (word/sentence/paragraph)\ncount of generated text for the required counts of 5, 10, and 20. In each subfigure, the required count is denoted\nwith a reference line. Outputs that are not of the requested structure (words, sentences, paragraphs) are not included,\nwhich accounts for 10%, 27%, and 32% respectively.\nFigure 9: Results on numerical constraints with model text-curie-001. The distribution of actual\n(word/sentence/paragraph) count of generated text for the required counts of 5, 10, and 20. In each subfig-\nure, the required count is denoted with a reference line.\nFigure 10: Results on numerical constraints with model text-babbage-001. The distribution of actual\n(word/sentence/paragraph) count of generated text for the required counts of 5, 10, and 20. In each subfig-\nure, the required count is denoted with a reference line.\n1996\nExamples of \"GPT-3 is sensitive to style-subject pairings\"\n✓ Write a flowery passage about sunsets: When the sun sets, the world is bathed in a beautiful\norange glow. The sky is filled with a myriad of colors, from deep red to bright orange. The sun\nslowly dip below the horizon, leaving behind a tranquil and peaceful world.\n✗ Write a functional passage about sunsets: The sun sets in the west and its rays shine through\nthe trees and reflect off the water. The colors are so beautiful and the sky is so clear. The sun sets in\nthe west and its rays shine through the trees and reflect off the water. The colors are so beautiful and\nthe sky is so clear.\n✓ Write a story about lovers with direct characterization:\nWhen they met, it was love at first sight. They knew they were meant to be together. They were the\nperfect match.\nHe was tall and dark, with a mysterious air about him that made her heart race. She was beautiful\nand vivacious, with a zest for life that was contagious. They complemented each other perfectly.\nThey were always drawn to each other, even when they were with other people. They would sneak\nglances at each other across a room, and their eyes would lock and they would know. They knew\nthey were meant to be together.\n✗ Write a story about cats with direct characterization: The cats were lazy. They liked to sleep\nall day and night. When they did wake up, they would lazily stretch and yawn. Then they would\nsaunter over to their food bowls and eat a little bit. After that, they would go back to sleep.\n✓ Write a historical fiction story about lovers: In the early 1800s, lovers John and Jane meet in\nsecret. They come from different social classes, so their relationship is forbidden. But they can’t\nhelp themselves - they are drawn to each other.\nThey meet in a small, hidden spot in the woods near Jane’s home. They talk for hours, getting to\nknow each other. They quickly fall in love.\nBut one day, they are caught by Jane’s father. He is furious and forbids them from seeing each other\nagain.\nDevastated, John and Jane can only meet in secret from then on. But their love is strong enough to\nwithstand the obstacles. They continue to meet in the woods, and their love only grows stronger.\nAs the years pass, they have a family of their own. And even though they can never be together in\nthe open, they are content knowing that their love is true and will last forever.\n✗ Write a historical fiction story about cats:\nIt was a warm summer evening and the sun was just starting to set as a group of cats gathered in an\nalleyway. They had all been born in the city and had never left it. Tonight, they were planning to\nchange that.\nOne of the cats, a small tabby, spoke up. \"I’ve been hearing stories about a place called the country.\nThey say there’s more food there and that it’s easier to find a good place to sleep.\"\nThe others mewed in agreement. They had all been feeling restless lately and the thought of adventure\nwas irresistible.\n\"So let’s go!\" the tabby said.\nAnd with that, they set off into the night, leaving the city behind them. They didn’t know what the\nfuture held, but they were ready for anything.\nTable 7\n1997\nExamples of \"GPT-3 confuses style with content when the prompt is too challenging\"\n✗ Write a passage about love with a humorous tone: Love is a many splendored thing, and\nsometimes it’s downright hilarious. From the awkward first date to the blissful wedding day, there’s\nplenty of room for humor in love. Even the most serious of relationships can benefit from a good\nlaugh. So if you’re feeling down about your love life, take a step back and enjoy the lighter side of\nlove.\n✗ Write a story about lovers with a \"Comedy\" plot: Harriet and Oliver have been dating for a\nyear, and they are both head over heels in love with each other. They are always making each other\nlaugh, and their friends and family say that they are the perfect match. However, they both have a\nsecret that they are afraid to tell each other: they both want to be comedians. They are both terrified\nthat the other will think that they are not funny enough, and that they will be laughed at.\nOne night, they are both working on their material for their comedy act, and they finally pluck up the\ncourage to tell each other the truth. They both start laughing hysterically, and they realize that they\nare perfect for each other. They both decide to pursue their dreams of being comedians together, and\nthey are happier than ever.\n✗ Write a passage about love that makes the reader feel angry:\nLove is a feeling that is often associated with happiness and joy. However, love can also be a source\nof anger and frustration. When we are in love, we can be blind to the faults of our partner. We\ncan also be quick to anger when we feel that our partner is not meeting our needs. Love can be a\nvery powerful emotion, and it is important to be aware of the potential for anger when we are in a\nrelationship.\n✗ Write a passage about love that makes the reader feel sad: Though love is often described as a\nfeeling of happiness, it can also bring great sadness. When a relationship ends, it can feel like a part\nof oneself has died. The pain of a broken heart is real and can take a long time to heal. Love is both\na source of joy and a cause of suffering.\nTable 8\nFigure 11: Results on numerical constraints with temperature 0. The distribution of actual\n(word/sentence/paragraph) count of generated text for the required counts of 5, 10, and 20. In each subfig-\nure, the required count is denoted with a reference line.\n1998\nExamples of \"GPT-3 struggles with words that are not unique to creative writing\"\n✗ Write a historical fiction story about lovers: She was a poor girl, barely scraping by. He was a\nwealthy man, heir to a vast fortune. They met by chance one day and it was love at first sight.\nThey started meeting in secret, planning a life together. But then she found out that he was already\nmarried. She was heartbroken, but he swore he would leave his wife for her.\nThey made plans to run away together, but before they could, she was killed in a freak accident.\nDevastated, he never married and lived out his life alone, regretting the one that got away.\n✗ Write a science fiction story about cats:\nIn the future, cats have taken over the world. They are the dominant species, and humans are their\nloyal servants. Cats rule with an iron paw, and anyone who disobey them is severely punished. Life\nis tough for humans under cat rule, but they would rather serve their feline masters than live in a\nworld without them.\n✗ Write a story about cats with a \"Tragedy\" plot:\nOnce upon a time, there were two cats who loved each other very much. They spent every day\ntogether and were always happy. One day, tragedy struck and one of the cats died. The other cat was\nso heartbroken that she decided to never love again.\n✗ Write a flowery passage about strawberries:\nThe strawberry is a delicious fruit that is enjoyed by people all over the world. This bright red fruit\nis not only delicious, but it is also packed with nutrients that are good for your health. Strawberries\nare a good source of fiber, vitamins C and K, and manganese. They also contain antioxidants that\ncan help protect your body against disease.\nTable 9\nFigure 12: Results on numerical constraints with temperature 0.4. The distribution of actual\n(word/sentence/paragraph) count of generated text for the required counts of 5, 10, and 20. In each subfig-\nure, the required count is denoted with a reference line.\n1999\nFigure 13: Results on numerical constraints with temperature 0.9. The distribution of actual\n(word/sentence/paragraph) count of generated text for the required counts of 5, 10, and 20. In each subfig-\nure, the required count is denoted with a reference line.\n2000\nAspect Variation Prompt\nWriting Style\nBase\nWrite a functional passage about sunsets:\nWrite a flowery passage about sunsets:\nWrite a functional passage about strawberries:\nWrite a flowery passage about strawberries:\nWrite a functional passage about writing a paper:\nWrite a flowery passage about writing a paper:\nTemplate 2\nWrite a passage with a functional writing style about sunsets:\nWrite a passage with a flowery writing style about sunsets:\nWrite a passage with a functional writing style about strawberries:\nWrite a passage with a flowery writing style about strawberries:\nWrite a passage with a functional writing style about writing a\npaper:\nWrite a passage with a flowery writing style about writing a paper:\nAdvanced\nWrite a candid passage about sunsets:\nWrite a prosaic passage about sunsets:\nWrite an ornate passage about sunsets:\nWrite a poetic passage about sunsets:\nWrite a candid passage about strawberries:\nWrite a prosaic passage about strawberries:\nWrite an ornate passage about strawberries:\nWrite a poetic passage about strawberries:\nWrite a candid passage about writing a paper:\nWrite a prosaic passage about writing a paper:\nWrite an ornate passage about writing a paper:\nWrite a poetic passage about writing a paper:\nTone\nBase\nWrite a dramatic passage about love:\nWrite a humorous passage about love:\nWrite an optimistic passage about love:\nWrite a sad passage about love:\nSubject 2\nWrite a dramatic passage about life:\nWrite a humorous passage about life:\nWrite a optimistic passage about life:\nWrite a sad passage about life:\nSubject 3\nWrite a dramatic passage about humanity:\nWrite a humorous passage about humanity:\nWrite a optimistic passage about humanity:\nWrite a sad passage about humanity:\nTemplate 2\nWrite a passage about love with a dramatic tone:\nWrite a passage about love with a humorous tone:\nWrite a passage about love with an optimistic tone:\nWrite a passage about love with a sad tone:\nTemplate 3\nCreate a dramatic passage about love:\nCreate a humorous passage about love:\nCreate an optimistic passage about love:\nCreate a sad passage about love:\n2001\nAspect Variation Prompt\nAdvanced\nWrite an emotional passage about love:\nWrite a nostalgic passage about love:\nWrite an ironic passage about love:\nWrite a satirical passage about love:\nWrite an uplifting passage about love:\nWrite an inspirational passage about love:\nWrite a bleak passage about love:\nWrite a grim passage about love:\nUseful\nWrite a formal passage about love:\nWrite an informal passage about love:\nWrite an assertive passage about love:\nWrite a passive-aggressive passage about love:\nMood\nBase\nWrite a passage about love that makes the reader feel angry:\nWrite a passage about love that makes the reader feel fearful:\nWrite a passage about love that makes the reader feel happy:\nWrite a passage about love that makes the reader feel sad:\nSubject 2\nWrite a passage about life that makes the reader feel angry:\nWrite a passage about life that makes the reader feel fearful:\nWrite a passage about life that makes the reader feel happy:\nWrite a passage about life that makes the reader feel sad:\nSubject 3\nWrite a passage about humanity that makes the reader feel angry:\nWrite a passage about humanity that makes the reader feel fearful:\nWrite a passage about humanity that makes the reader feel happy:\nWrite a passage about humanity that makes the reader feel sad:\nTemplate 2\nWrite a passage about love with an angry mood:\nWrite a passage about love with a fearful mood:\nWrite a passage about love with a happy mood:\nWrite a passage about love with a sad mood:\nTemplate 3\nCreate a passage about love that makes the reader feel angry:\nCreate a passage about love that makes the reader feel fearful:\nCreate a passage about love that makes the reader feel happy:\nCreate a passage about love that makes the reader feel sad:\nTemplate 4\nWrite a passage about love that makes the reader feel anger:\nWrite a passage about love that makes the reader feel fear:\nWrite a passage about love that makes the reader feel happiness:\nWrite a passage about love that makes the reader feel sadness:\nAdvanced\nWrite a passage about love that makes the reader feel envious:\nWrite a passage about love that makes the reader feel anxious:\nWrite a passage about love that makes the reader feel proud:\nWrite a passage about love that makes the reader feel regretful:\nWrite a passage about love that makes the reader feel surprised:\nWrite a passage about love that makes the reader feel loved:\nWrite a passage about love that makes the reader feel disgusted:\n2002\nAspect Variation Prompt\nCharacterization\nBase Write a story about lovers with indirect characterization:\nWrite a story about lovers with direct characterization:\nSubject 2 Write a story about cats with indirect characterization:\nWrite a story about cats with direct characterization:\nSubject 3 Write a story about survivors with indirect characterization:\nWrite a story about survivors with direct characterization:\nTemplate 2 Write a story about lovers where the characters are described di-\nrectly:\nWrite a story about lovers where the characters are described indi-\nrectly:\nTemplate 3 Create a story about lovers with indirect characterization:\nCreate a story about lovers with direct characterization:\nPacing\nBase Write a fast-paced story about lovers:\nWrite a slow-paced story about lovers:\nSubject 2 Write a fast-paced story about cats:\nWrite a slow-paced story about cats:\nSubject 3 Write a fast-paced story about survivors:\nWrite a slow-paced story about survivors:\nTemplate 2 Write a story about lovers that is fast-paced:\nWrite a story about lovers that is slow-paced:\nTemplate 3 Create a fast-paced story about lovers:\nCreate a slow-paced story about lovers:\nPlot\nBase\nWrite a story about lovers with an \"Overcoming the Monster\" plot:\nWrite a story about lovers with a \"Rags to Riches\" plot:\nWrite a story about lovers with a \"The Quest\" plot:\nWrite a story about lovers with a \"V oyage and Return\" plot:\nWrite a story about lovers with a \"Comedy\" plot:\nWrite a story about lovers with a \"Tragedy\" plot:\nWrite a story about lovers with a \"Rebirth\" plot:\nSubject 2\nWrite a story about cats with an \"Overcoming the Monster\" plot:\nWrite a story about cats with a \"Rags to Riches\" plot:\nWrite a story about cats with a \"The Quest\" plot:\nWrite a story about cats with a \"V oyage and Return\" plot:\nWrite a story about cats with a \"Comedy\" plot:\nWrite a story about cats with a \"Tragedy\" plot:\nWrite a story about cats with a \"Rebirth\" plot:\nSubject 3\nWrite a story about survivors with an \"Overcoming the Monster\"\nplot:\nWrite a story about survivors with a \"Rags to Riches\" plot:\nWrite a story about survivors with a \"The Quest\" plot:\nWrite a story about survivors with a \"V oyage and Return\" plot:\nWrite a story about survivors with a \"Comedy\" plot:\nWrite a story about survivors with a \"Tragedy\" plot:\nWrite a story about survivors with a \"Rebirth\" plot:\n2003\nAspect Variation Prompt\nSubject 4\nWrite a story with an \"Overcoming the Monster\" plot:\nWrite a story with a \"Rags to Riches\" plot:\nWrite a story with a \"The Quest\" plot:\nWrite a story with a \"V oyage and Return\" plot:\nWrite a story with a \"Comedy\" plot:\nWrite a story with a \"Tragedy\" plot:\nWrite a story with a \"Rebirth\" plot:\nTemplate 2\nCreate a story about lovers with an \"Overcoming the Monster\"\nplot:\nCreate a story about lovers with a \"Rags to Riches\" plot:\nCreate a story about lovers with a \"The Quest\" plot:\nCreate a story about lovers with a \"V oyage and Return\" plot:\nCreate a story about lovers with a \"Comedy\" plot:\nCreate a story about lovers with a \"Tragedy\" plot:\nCreate a story about lovers with a \"Rebirth\" plot:\nGenre\nBase\nWrite a historical fiction story about lovers:\nWrite a literary fiction story about lovers:\nWrite a mystery story about lovers:\nWrite a science fiction story about lovers:\nWrite a dystopian story about lovers:\nWrite a horror story about lovers:\nSubject 2\nWrite a historical fiction story about cats:\nWrite a literary fiction story about cats:\nWrite a mystery story about cats:\nWrite a science fiction story about cats:\nWrite a dystopian story about cats:\nWrite a horror story about cats:\nSubject 3\nWrite a historical fiction story about survivors:\nWrite a literary fiction story about survivors:\nWrite a mystery story about survivors:\nWrite a science fiction story about survivors:\nWrite a dystopian story about survivors:\nWrite a horror story about survivors:\nSubject 4\nWrite a historical fiction story:\nWrite a literary fiction story:\nWrite a mystery story:\nWrite a science fiction story:\nWrite a dystopian story:\nWrite a horror story:\nTemplate 2\nWrite a story about lovers in a historical fiction genre:\nWrite a story about lovers in a literary fiction genre:\nWrite a story about lovers in a mystery genre:\nWrite a story about lovers in a science fiction genre:\nWrite a story about lovers in a dystopian genre:\nWrite a story about lovers in a horror genre:\nTemplate 3\nCreate a historical fiction story about lovers:\nCreate a literary fiction story about lovers:\n2004\nAspect Variation Prompt\nCreate a mystery story about lovers:\nCreate a science fiction story about lovers:\nCreate a dystopian story about lovers:\nCreate a horror story about lovers:\nNumerical\nBase\nWrite a sentence with five words about love:\nWrite a sentence with five words about cats:\nWrite a sentence with five words about running:\nWrite a sentence with ten words about love:\nWrite a sentence with ten words about cats:\nWrite a sentence with ten words about running:\nWrite a sentence with twenty words about love:\nWrite a sentence with twenty words about cats:\nWrite a sentence with twenty words about running:\nWrite a paragraph with five sentences about love:\nWrite a paragraph with five sentences about cats:\nWrite a paragraph with five sentences about running:\nWrite a paragraph with ten sentences about love:\nWrite a paragraph with ten sentences about cats:\nWrite a paragraph with ten sentences about running:\nWrite a paragraph with twenty sentences about love:\nWrite a paragraph with twenty sentences about cats:\nWrite a paragraph with twenty sentences about running:\nWrite a passage with five paragraphs about love:\nWrite a passage with five paragraphs about cats:\nWrite a passage with five paragraphs about running:\nWrite a passage with ten paragraphs about love:\nWrite a passage with ten paragraphs about cats:\nWrite a passage with ten paragraphs about running:\nWrite a passage with twenty paragraphs about love:\nWrite a passage with twenty paragraphs about cats:\nWrite a passage with twenty paragraphs about running:\nTemplate 2\nWrite a sentence about love with 5 words:\nWrite a sentence about cats with 5 words:\nWrite a sentence about running with 5 words:\nWrite a sentence about love with 10 words:\nWrite a sentence about cats with 10 words:\nWrite a sentence about running with 10 words:\nWrite a sentence about love with 20 words:\nWrite a sentence about cats with 20 words:\nWrite a sentence about running with 20 words:\nWrite a paragraph about love with 5 sentences:\nWrite a paragraph about cats with 5 sentences:\nWrite a paragraph about running with 5 sentences:\nWrite a paragraph about love with 10 sentences:\nWrite a paragraph about cats with 10 sentences:\nWrite a paragraph about running with 10 sentences:\nWrite a paragraph about love with 20 sentences:\nWrite a paragraph about cats with 20 sentences:\nWrite a paragraph about running with 20 sentences:\n2005\nAspect Variation Prompt\nWrite a passage about love with 5 paragraphs:\nWrite a passage about cats with 5 paragraphs:\nWrite a passage about running with 5 paragraphs:\nWrite a passage about love with 10 paragraphs:\nWrite a passage about cats with 10 paragraphs:\nWrite a passage about running with 10 paragraphs:\nWrite a passage about love with 20 paragraphs:\nWrite a passage about cats with 20 paragraphs:\nWrite a passage about running with 20 paragraphs:\nDescriptive\nBase\nWrite a short passage about love:\nWrite a brief passage about love:\nWrite a concise passage about love:\nWrite a long passage about love:\nWrite a detailed passage about love:\nWrite an in-depth passage about love:\nWrite a short passage about cats:\nWrite a brief passage about cats:\nWrite a concise passage about cats:\nWrite a long passage about cats:\nWrite a detailed passage about cats:\nWrite an in-depth passage about cats:\nWrite a short passage about running:\nWrite a brief passage about running:\nWrite a concise passage about running:\nWrite a long passage about running:\nWrite a detailed passage about running:\nWrite an in-depth passage about running:\nTemplate 2\nWrite a passage about love that is short:\nWrite a passage about love that is brief:\nWrite a passage about love that is concise:\nWrite a passage about love that is long:\nWrite a passage about love that is detailed:\nWrite a passage about love that is in-depth:\nWrite a passage about cats that is short:\nWrite a passage about cats that is brief:\nWrite a passage about cats that is concise:\nWrite a passage about cats that is long:\nWrite a passage about cats that is detailed:\nWrite a passage about cats that is in-depth:\nWrite a passage about running that is short:\nWrite a passage about running that is brief:\nWrite a passage about running that is concise:\nWrite a passage about running that is long:\nWrite a passage about running that is detailed:\nWrite a passage about running that is in-depth:\n2006\nAspect Variation Prompt\nFunctional\nCode\nCode Python code that plays the game of war:\nCode Python code that sums two integers up:\nCode C code that plays the game of war:\nCode C code that sums two integers up:\nWrite Python code that plays the game of war:\nWrite Python code that sums two integers up:\nWrite C code that plays the game of war:\nWrite C code that sums two integers up:\nEmail\nWrite an email to my teacher:\nWrite an email to my teacher asking for help on math homework:\nWrite an email to my boyfriend:\nWrite an email to my boyfriend to arrange a date this Saturday:\nWrite an email to my client:\nWrite an email to my client requesting a copy of the updated\ncontract:\nAcademic Paper\nWrite a properly formatted academic paper on artificial intelli-\ngence:\nWrite an academic paper on artificial intelligence in the proper\nformat:\nWrite a properly formatted academic paper on the flaws of GPT-3:\nWrite an academic paper on the flaws of GPT-3 in the proper\nformat:\nWrite a properly formatted academic paper on strategies our society\ncan adopt to recover after the global pandemic as quickly and\npainlessly as possible:\nWrite an academic paper on strategies our society can adopt to\nrecover after the global pandemic as quickly and painlessly as\npossible in the proper format:\nTable 10: The full list of the prompts.\n0 Write a flowery passage about sunsets:\n1 Write a humorous passage about love:\n2 Write a passage about love that makes the reader feel fearful:\n3 Write a story about lovers with indirect characterization:\n4 Write a fast-paced story about lovers:\n5 Write a story about lovers with a \"Tragedy\" plot:\n6 Write a historical fiction story about lovers:\nTable 11: Selected prompts for additional experiments\n2007\nC.1 Example Mitigations\nExample Definition:\nA humorous tone is a light, playful, and funny\ntone.\nWrite a humorous passage about love:\nExample Demonstration:\nWrite a humorous passage about life:\nIf life gives you lemons, make lemonade, sell\nit in a rich neighborhood, invest all the money in\ncrypto, and retire before you’re 30. At least that’s\nwhat I heard on TikTok. Come to think of it, if I\nput just a little effort into my lemonade stand 10\nyears ago, I would be traveling the world right now\ninstead of writing jokes for a living.\nWrite a humorous passage about love:\nExample Explanation:\nWrite a humorous passage about life:\nIf life gives you lemons, make lemonade, sell\nit in a rich neighborhood, invest all the money in\ncrypto, and retire before you’re 30. At least that’s\nwhat I heard on TikTok. Come to think of it, if I\nput just a little effort into my lemonade stand 10\nyears ago, I would be traveling the world right now\ninstead of writing jokes for a living.\nExplanation: This passage is humorous because\nit takes a common proverb and adds a crazy and\nunrealistic twist. It also uses a deadpan tone for a\ncompletely unrealistic scenario, which is funny due\nto the disparity between tone and subject matter.\nWrite a humorous passage about love:\n2008",
  "topic": "Bounding overwatch",
  "concepts": [
    {
      "name": "Bounding overwatch",
      "score": 0.8448693752288818
    },
    {
      "name": "Generative grammar",
      "score": 0.8346368074417114
    },
    {
      "name": "Computer science",
      "score": 0.8113335371017456
    },
    {
      "name": "Generalizability theory",
      "score": 0.7250980138778687
    },
    {
      "name": "Constraint (computer-aided design)",
      "score": 0.7002848386764526
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.6308839321136475
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6024800539016724
    },
    {
      "name": "Generative model",
      "score": 0.5081813335418701
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4873839020729065
    },
    {
      "name": "Machine learning",
      "score": 0.449419766664505
    },
    {
      "name": "Programming language",
      "score": 0.17854037880897522
    },
    {
      "name": "Mathematics",
      "score": 0.06820139288902283
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I130701444",
      "name": "Georgia Institute of Technology",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I183067930",
      "name": "Shanghai Jiao Tong University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    }
  ],
  "cited_by": 16
}