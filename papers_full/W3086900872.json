{
  "title": "Patient Cohort Retrieval using Transformer Language Models",
  "url": "https://openalex.org/W3086900872",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5055267862",
      "name": "Sarvesh Soni",
      "affiliations": [
        "The University of Texas Health Science Center at Houston"
      ]
    },
    {
      "id": "https://openalex.org/A5046709245",
      "name": "Kirk Roberts",
      "affiliations": [
        "The University of Texas Health Science Center at Houston"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2134137358",
    "https://openalex.org/W2536015822",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2753032179",
    "https://openalex.org/W77404731",
    "https://openalex.org/W2163922914",
    "https://openalex.org/W2782157559",
    "https://openalex.org/W1482214997",
    "https://openalex.org/W2993873509",
    "https://openalex.org/W2122402213",
    "https://openalex.org/W1593657456",
    "https://openalex.org/W2139865360",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W2109244020",
    "https://openalex.org/W2093157872",
    "https://openalex.org/W1808652302",
    "https://openalex.org/W2951534261",
    "https://openalex.org/W3031200717",
    "https://openalex.org/W2768488789",
    "https://openalex.org/W1980199609",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W213236682",
    "https://openalex.org/W2909544278",
    "https://openalex.org/W2892939285",
    "https://openalex.org/W2648699835",
    "https://openalex.org/W2146089916",
    "https://openalex.org/W2062908157",
    "https://openalex.org/W2143514833",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W3006779453",
    "https://openalex.org/W2997419538",
    "https://openalex.org/W2123442489",
    "https://openalex.org/W2899154813",
    "https://openalex.org/W2986191653"
  ],
  "abstract": "We apply deep learning-based language models to the task of patient cohort retrieval (CR) with the aim to assess their efficacy. The task of CR requires the extraction of relevant documents from the electronic health records (EHRs) on the basis of a given query. Given the recent advancements in the field of document retrieval, we map the task of CR to a document retrieval task and apply various deep neural models implemented for the general domain tasks. In this paper, we propose a framework for retrieving patient cohorts using neural language models without the need of explicit feature engineering and domain expertise. We find that a majority of our models outperform the BM25 baseline method on various evaluation metrics.",
  "full_text": "Patient Cohort Retrieval using Transformer Language Models\nSarvesh Soni, MS, Kirk Roberts, PhD\nSchool of Biomedical Informatics\nThe University of Texas Health Science Center at Houston\nHouston TX, USA\nAbstract\nWe apply deep learning-based language models to the task of patient cohort retrieval (CR) with the aim to assess\ntheir efﬁcacy. The task of CR requires the extraction of relevant documents from the electronic health records (EHRs)\non the basis of a given query. Given the recent advancements in the ﬁeld of document retrieval, we map the task of\nCR to a document retrieval task and apply various deep neural models implemented for the general domain tasks.\nIn this paper, we propose a framework for retrieving patient cohorts using neural language models without the need\nof explicit feature engineering and domain expertise. We ﬁnd that a majority of our models outperform the BM25\nbaseline method on various evaluation metrics.\nIntroduction\nAutomatic methods for rapidly identifying groups of patients (cohorts) based on a deﬁned set of criteria have numerous\nbiomedical use cases. Some of these include clinical trial recruitment, survival analysis, and outcome prediction\nalongside various other retrospective studies1. Electronic health records (EHRs) provide a useful means to efﬁciently\nextract such cohorts as these clinical records contain large amounts of patient-related information 2. However, much\nof the clinically relevant information is present in an unstructured format (e.g., discharge summaries, progress notes)\nas they allow for rich data entries and more expressiveness. This also makes it challenging to parse the information\npresent in these notes because of various issues (related to the use of natural language) such as lack of grammar, use\nof abbreviations, and implicit referrals to other parts of the record3,4.\nThere is a surge of deep learning (DL)-based methods for processing and extracting information from natural language\ntext, both in general and the clinical domain5,6. These methods are shown to perform better than the traditional machine\nlearning (ML) methods on a variety of natural language processing (NLP) tasks in the open domain 5,7 as well as the\nclinical domain6. Though numerous work applied DL techniques to clinical information extraction or IE (e.g., named\nentity recognition, relation extraction) to improve its performance, less focus is drawn toward applying these advanced\ntechniques to information retrieval or IR (e.g., cohort retrieval) from the EHRs6. The fundamental difference between\nIE and IR is that the former focuses on extracting pre-deﬁned structured information from unstructured text while the\nlatter aims at quickly returning a set of documents relevant to a user-provided, often ad hoc, query.\nThe task of patient cohort retrieval (CR) is to search a large set of EHR documents for determining groups of patients\nmost relevant to a given query. E.g., one might need to screen “children with dental caries” for a clinical trial. In\nthis scenario, we expect a CR system to return a list of patients who are under the age of 18 (children) and had a\nproblem with dental caries at some point in time. Such type of matching (from query to relevant documents) requires\nan understanding of the general vocabulary (e.g., children would mean searching for patients below 18 years of age),\ndomain vocabulary (e.g., dental caries can also be mentioned in the note as tooth decay or cavities), as well as\nreasoning (e.g., understanding the meaning of with). Some of these challenges can be addressed using advanced DL\nmethods5,8.\nIR is well researched in the biomedical domain, though much of the earlier work focused on traditional IR approaches\n(largely non-ML) 9. More recently, the patient CR techniques incorporated traditional ML methods 1. However, the\nmain limitations of these include heavy feature engineering, reliance on domain expertise, and lack of generalizability.\nDL techniques aim to overcome some of these limitations by automatically learning the representations from raw\ndata that has the potential to be generalized to other kinds of data 7. Recently, various language modeling methods\nhave emerged that utilize contextualized representations and are successful in improving the performance of various\nNLP tasks in both general 10,11 and speciﬁc domains 12,13,14. Such language models (LMs) have a great potential to be\napplied to the task of CR as they can effectively encode the nuances of natural language present in clinical notes and\nthe free-text queries.\narXiv:2009.05121v1  [cs.IR]  10 Sep 2020\nIn this work, we apply DL-based methods to the task of CR with an aim to improve its performance and generalizability\nwithout relying on cumbersome feature engineering and domain expertise. The unit of retrieval for our task is a visit\n(i.e., we aim to return a list of relevant visits given a free-text query). A visit is deﬁned as a single stay of a patient at\na hospital 4. In other words, all the reports associated with a visit are collected during a single patient encounter and\nhence these visits can be considered a proxy for retrieving relevant patients. The reason why this proxy evaluation is\nnecessary is described below. To our knowledge, this is the ﬁrst work to assess the effectiveness of employing deep\nlanguage models to the task of patient CR.\nBackground\nThere was a rise in the implementation of CR systems after the launch of a Medical Records Track (TRECMed) as\npart of the annual Text REtrieval Conference (TREC) hosted by the National Institute for Standards and Technology\n(NIST) in 201115. Most of the submissions to TRECMed employed a learning-to-rank approach that uses machine\nlearning to rank the resulting documents16. A typical pipeline of the submitted systems included some query expansion\ncomponent in which an input query is appended with additional terms to increase the model’s coverage of results.\nSubmissions from many track participants employed a concept normalization tool such as MetaMap 17 to identify the\nclinical entities from query text 4,16. The identiﬁed concepts are used to formulate a machine-understandable query\nfrom the textual input. Some approaches also used disease diagnosis codes associated with the clinical document for\nquery expansion 4. Speciﬁcally, the descriptions associated with diagnosis codes were used for appending the query.\nAdditional sources of data for query expansion include synonyms and other related terms16. Moreover, some systems\nexploited a negation detection system like NegEx 18 for differentiating between the positive and negative instances of\nthe medical concepts in the text and query 4. This was done mainly to exclude the negated terms while searching.\nEdinger et al. highlight some of the limitations of the systems submitted to TRECMed 19. Further, approaches based\non traditional machine learning are time-consuming and oftentimes require domain expertise to craft a comprehensive\nset of features 16. Finally, such techniques may not be generalizable to other kinds of data. Thus, we resort to deep\nlearning algorithms that do not require as much extensive feature engineering and domain expertise and are shown to\nbe generalizable to different kinds of data.\nThough the deep learning techniques are not explored as much for the patient CR, it is well-researched in the general\ndomain because of popular use cases such as powering web search engines20,21. Recently, a plethora of neural models\nare implemented to rank the documents to efﬁciently retrieve information from large amount of data 22,23,24,25. The\navailability of large IR datasets in the open domain such as TREC-CAR 26 and MS MARCO 27 have facilitated such\nadvancements as the deep learning techniques usually require large datasets for training. Further, powerful neural\nlanguage models (e.g., BERT 10) have gained a huge popularity among the NLP community due to these models’\nability to learn from large unstructured data sources (through pre-training). Moreover, such models can be ﬁne-tuned\non downstream NLP tasks (e.g., document ranking 25) with minimal tweaks. We aim to explore the potential of\napplying such powerful neural language models (in our case BERT) to the task of patient CR.\nMaterials and Methods\nAn overview of the proposed framework is shown in Figure 1. The datasets used in our study are described in Section\n1. We provide our methods for indexing and querying the large clinical dataset to form a candidate set of documents\nunder Section 2. The steps for summarizing the information present in EHRs is presented in Section 3. Further, we\nexplain the re-ranking experiments using BERT in Section 4. Our evaluation criteria and metrics are described in\nSection 5 and we provide details about our experimental setup in Section 6.\n1 Data\nThe dataset consists of de-identiﬁed clinical reports collected from various hospitals during a period of one month. It\nwas provided to the participants of TREC (Text REtrieval Conference) medical track challenges conducted in 2011\nand 20124,28. The corpus contains 93, 551 reports with 17, 264 associated visits. In particular, each report is linked to\na unique visit where a visit can be thought of as a set of patient records related to a single hospital stay. There are 9\ntypes of reports in the dataset: Radiology Report, History and Physical, Consultation Report, Emergency Department\nReport, Progress Note, Discharge Summary, Operative Report, Surgical Pathology Report, and Cardiology Report.\nFigure 1:Framework for patient cohort retrieval using BERT. Rx – Report with identiﬁer x, S(Rx) – Summary for\nreport Rx, Vy – Visit with identiﬁer y, Rx →Vy – Report Rx is part of the Visit Vy.\nThe cohort descriptions (or topics in the terminology of TREC challenges) for both the task years were constructed\nby physicians. A total of 34 and 47 topics were evaluated as part of the 2011 and 2012 tasks, respectively. The\nunit of retrieval for this task is visits, i.e., a CR system should return a ranked list of visits corresponding to each\ntopic. The reason for visit-level evaluations and not patient-level evaluations is that, in order to protect patient privacy,\noverall patient-level identiﬁers were stripped from the data and only visit-level identiﬁers remained. The original\nruns submitted by the participants were manually evaluated by physicians. Each visit from an identiﬁed subset of the\nreturned visits by the system was evaluated to be either relevant, partially relevant, or not relevant. More detailed\ndescription of the documents, topics, and relevance judgment procedure is available in the original paper from the task\norganizers4.\nThe relevance judgments for the submitted runs are available to us as part of the dataset and we use these to train and\nevaluate our pipeline. We combine the relevant and partially relevant scores to a single RELEVANT category and keep\nthe not relevant scores as NOT RELEVANT . Moreover, we assume all the visits not judged for a particular query to be\nNOT RELEVANT .\n2 Candidate generation\nThough the unit of retrieval for this task is a visit, we index, generate candidates, and rank at the individual report\nlevel. The ranked list of reports are mapped to their corresponding visits in a post hoc manner. The reason for such\nan approach is the fact that most of the decisions to mark a visit either RELEVANT or NOT RELEVANT are based on\nindividual reports and not contingent on the combination of information from multiple reports. For the scope of our\nevaluations, we assume that if a visit is labeled RELEVANT or NOT RELEVANT , the reports included with the visit are\nalso labeled in the same manner.\nAll the reports in the dataset are indexed using Lucene 29. We use Porter stemmer to stem the contents of the reports\nfor indexing. Stemming ensures that the different inﬂectional variants of a word can be grouped together. E.g., all the\ninstances of dehydrate, dehydrated, and dehydration are stored in the index as dehydr. We also leave out stopwords\n(such as a, an, the) while indexing as they are often not useful for searching. For this purpose, we use a built-in\nlist of stopwords from Lucene (EnglishAnalyzer class’sENGLISH_STOP_WORDS_SET). For tokenization, we\nexploit Lucene’sStandardTokenizer.\nTable 1:Descriptive statistics of the token counts for TREC Medical Track dataset after wordpiece tokenization. SD\n– Standard Deviation. Min – Minimum. Max – Maximum.\nComponent Metric\nMean Median SD Min Max\nQuery 13.99 13 7 .21 4 51\nReport 1192.58 1126 544 .38 94 9934\nTable 2:An example of the summarization process shown using an excerpt from an emergency department report.\nThe selected concepts after the ﬁltering process are italicized in the report text. Note that this is not a complete report\nand hence the list of selected concepts is also incomplete.\nReport: . . .blood pressure114/65, pulse oximetry98% on roomair. The skin is warm and dry. The\nears are negative. Thepharynx is not injected. He has animpacted wisdom tooth. He has multiple\ndental caries. He has notrismus. Theneck is supple, shottyadenopathy. . . .\nExtracted concepts: . . . blood pressure (+) pulse oximetry (+) air (+) skin (+) ears (+) pharynx (-)\nimpacted wisdom tooth (+) dental (+) caries (+) trismus (-) neck (+) adenopathy (+) . . .\nSelected concepts: emergency department report; . . . ears; impacted wisdom tooth; dental; caries;\nadenopathy; . . .\nThe cohort description or query is ﬁrst passed through the same stemming, stopwords ﬁltering, and tokenization steps\nas was carried out during the indexing process. The output tokens from these steps are used to construct a bag of\nwords style query for retrieving a ranked set of relevant candidate reports from the constructed index. E.g., the topic\n“Children with dental caries” is converted to query “children OR dental OR cari” that is searched against the index\nof clinical narratives. Speciﬁcally, we query the index using BM25 30 as the ranking function. The ranked list of N\ncandidate reports extracted from the index is passed to subsequent layers for further processing (we use N = 2000in\nour experiments). We use the publicly available toolkit Anserini to index and query the large set of clinical documents\nin our dataset31.\n3 Summarization\nBefore passing the candidate set of reports to our re-ranker model BERT, we summarize their contents. The motivation\nbehind this is the input length restrictions of the language model we aim to apply. The sequence length (or length of\ninput) for BERT is determined after wordpiece tokenization that involves dividing a word into sub-word units 32. A\nmaximum sequence length of 512 is evaluated by the authors of BERT10. However, the text in clinical reports is much\nlonger than 512. We show the descriptive statistics of the number of tokens after wordpiece tokenization of the clinical\nreports in our dataset in Table 1.\nThere are various approaches to summarize the content present in a clinical narrative. These largely fall into two\ncategories, extractive (extract salient phrases directly from the narrative itself) and abstractive (summarize using the\nsalient ideas present in the narrative by rephrasing). As the medical records are often written in a succinct manner, only\nincluding the relevant clinical information, we can still end up with a relatively large document even after an extractive\nsummarization where a set of important phrases are selected from the document. Thus, we take an abstractive approach\nfor summarization. An example of our summarization process is presented in Table 2.\nWe pass the text from reports through a standard pipeline of the Apache cTAKES (clinical Text Analysis and Knowl-\nedge Extraction System)33 to detect the underlying medical concepts. It is an open-source NLP system for extracting\ninformation from the clinical text that is based on UIMA (Unstructured Information Management Architecture) frame-\nwork and used often for clinical IE34. Speciﬁcally, we make use of their Default Clinical Pipeline. We further split the\ncontents of the reports into sentences using Stanford’s CoreNLP tool35. These tokenized sentences are then matched\nfor the presence of negation terms from a pre-deﬁned set36. All the sentences in a report that are found to contain these\nnegation terms are considered negated sentences. We align the outputs of cTAKES pipeline and our negated sentence\ndetection pipeline to determine the negative concepts. Thus, we extract a set of positive and negative concepts through\nthis process.\nWe pass the positive concepts forward for further processing. Though the negative concepts also contain useful clinical\ninformation, we choose to use the positive concepts as a typical cohort description is aimed toward ﬁnding the visits\nwith certain clinical attributes. Also, this technique of ﬁltering out the negations is known to work well for CR37. Even\nafter ﬁltering out the negative concepts, we end up with large sets of medical concepts for the reports in our dataset (in\n100s).\nTo tackle this, we deﬁne a heuristic to further ﬁlter the number of concepts while not losing out on important clinical\ninformation that can be used to effectively identify the visits. We calculate the document frequency (the number of\nreports in which a particular concept is found) for each concept identiﬁed in the reports from our corpus. For each\nreport, we further ﬁlter the concepts on the basis of this document frequency. Speciﬁcally, we only keep the concepts\nthat occur in less than 2500 reports (i.e., with the document frequency of 2500 or less). This number is determined to\nbe useful after experimentation.\nWe also add the report type to the ﬁltered list of concepts. This is done to convey an important aspect of the setting in\nwhich the information was recorded. E.g., we add emergency department report to the list of concepts extracted from\na report that is written for an emergency room encounter.\n4 Re-ranking using BERT\nWe formulate our task of patient CR as a document retrieval task at the report level. In other words, we aim to rank\na set of documents with respect to a given query. In our case, the query is the cohort description and the documents\nare the candidate set of clinical reports. As explained earlier, we pass a succinct set of relevant clinical concepts for a\ncandidate report instead of passing the whole text.\nWe use BERT10 to re-rank the candidate clinical reports. Speciﬁcally, we pass the query along with the summarized\ncontent from candidate report to BERT. The summarized content is formed by combining the relevant clinical concepts\nextracted from the report. Using the terminology from the authors of BERT, we pass the query text packed together\nwith the summarized content corresponding to the candidate report. For instance, a query along with the example\nreport from Table 2 as a candidate is passed in the form of the following sequence to BERT:\n[CLS] Children with dental caries. [SEP]\nemergency department report; impacted wisdom tooth; dental; caries; ... [SEP]\nThe re-ranking using BERT is carried out by training and running the model as a binary classiﬁer25. Each of the above\npairs are assigned one of the two labels based on relevance judgments. If the visit corresponding to the candidate report\nin the pair is related to the query we mark the input pair as RELEVANT , otherwise NOT RELEVANT . Precisely, we pass\nthe ﬁnal layer vector for [CLS] token through a neural network to get the probability of the input pair relevancy. We\naim to reduce the standard cross-entropy loss for these probabilities for all the input query-report pairs. During testing,\nwe calculate the probability of relevancy for each of the test sequences and rank them according to their probability\nscores (higher the probability of being RELEVANT , better the rank).\nIt has been shown that BERT fails to perform well in case of imbalanced data38. Our dataset is heavily imbalanced, as\nmost of the candidate documents are labeled as NOT RELEVANT . Thus, we experiment with varying levels of positive\nto negative examples ratio in our training dataset. We determine and use the ratio of positive to negative examples\nas 1 : 10in all our experiments. During the training, we also cap the maximum number of candidates to 1650 while\nkeeping the ratio of positive to negative examples maintained. As the average number of RELEVANT documents is\naround 150, we keep only a maximum of 11 ∗150 = 1650 documents. E.g., for 150 RELEVANT documents, the\nmaximum number of NOT RELEVANT documents is capped at 1500.\n5 Evaluation\nSince the original task is at the visit level, we map the ranked list of reports from a model to a ranked list of visits\n(using a mapping between reports and visits). Speciﬁcally, each report in the ranked list is mapped to its corresponding\nvisit. As the mapping from reports to visits is many to one, we may end up with duplicate visits in the ranked list of\nvisits. So, we de-duplicate this ranked list in such a way that only the highest (better) ranked visit is kept in the ﬁnal\nranked list of visits. Due to this many to one mapping, we may end up with fewer ranked visits than reports after the\nmapping. Hence, we start with a larger number of reports ( N) to be able to generate a ranked list of visits ( M) with\nthe required length. We use N = 2000and M = 1000in our experiments.\nWe measure the performance of our pipeline by assessing the ranked list of visits returned by our pipeline. For\ncomparisons (baseline) we also measure the performance of the ranked list of documents initially retrieved from the\nLucene index using BM25.\nWe use the standard metrics for document retrieval to evaluate the performance of our framework, namely, P@10 (Pre-\ncision for the ﬁrst 10 documents), rPrec (R-Precision), MAP (Mean Average Precision), Bpref (Binary Preference) 39,\nNDCG (Normalized Discounted Cumulative Gain) 40, and MRR (Mean Reciprocal Rank). P@ 10 simply calculates\nthe precision for the ﬁrst 10 ranked documents by the IR system. Similarly, rPrec only considers the ground truth\nrelevant documents among the ranked list produced by the system to calculate precision. Both of the above systems\ndo not take into account the order or rank of the documents in the system output. To resolve this, MAP measures a\nsystem’s precision such that the rank of each document in the ranked output contributes to the overall precision. In\nparticular, the better the rank of a document, the higher its contribution to the overall system precision calculation, and\nvice-versa. Differently, Bpref also considers the fact that there may be relevant documents which are not judged as\nRELEVANT (the other measures assume that unjudged documents are NOT RELEVANT ). NDCG considers the differ-\nent levels of relevance judgments during its calculation. Speciﬁcally, it uses a higher relevance score for documents\njudged relevant than the ones that are judged partially relevant. MRR calculates the average of the reciprocal ranks\n(multiplicative inverse of the rank of the ﬁrst relevant document) across all the queries.\n6 Experimental Setup\nWe used an uncased variant of BERT BASE and BERTLARGE models pre-trained and ﬁne-tuned on various general,\nbiomedical, and clinical domain datasets for our evaluations. We refer to these variants as BERT BASE (pre-trained\non general domain text) 10, BioBERT (pre-trained on the general domain and biomedical text) 13, Clinical BERT (ini-\ntialized with BioBERT and further pre-trained on clinical notes) 12, BERTLARGE TREC-CAR (pre-trained on general\ndomain text and ﬁne-tuned on TREC-CAR) 25, and BERT LARGE MS MARCO (pre-trained on general domain text\nand ﬁne-tuned on MS MARCO) 25. All these model variants are ﬁne-tuned for both the TRECMed datasets using the\nhyperparameters described below. The BERT BASE model variant consists of a total of 110 million parameters with\n12 layers and self-attention heads with a hidden size of 768 while the BERTLARGE model has 340 million parameters\nwith 24 layers and a hidden size of 1024. We split the datasets into train and test splits at the topic level in the ratio of\n80:20. We train the BERT-based model on our training set and evaluated its performance on the testing set. For our\nbaseline, BM25, we don’t need training. Hence, we directly evaluate its performance for all the topics in our test sets.\nFor hyperparameters, we use a maximum query length of 64 as all the topics in our dataset are covered under this\nlength. Otherwise, we use the recommended set of parameters from the original authors for ﬁne-tuning the model on\nour dataset. Namely, the maximum sequence length is 384 and the learning rate is 3 x 10−5. For the models based on\nBERTBASE we use a batch size of 24 while for the BERTLARGE models, it is set to 8. We ﬁne-tune all the models for\na total of 2 epochs. We use a TensorFlow-based implementation of the model and perform all our experiments on an\nNVidia Tesla V100 GPU (32G).\nResults\nThe evaluation results of our proposed CR framework on the TREC Medical Track datasets from both the years,\n2011 and 2012, are presented in Tables 3 and 4, respectively. Summarization, capping the number of candidate\ndocuments, and maintaining the ratio of relevant (positive) and non-relevant (negative) examples helped in improving\nthe performance of the models over the BERT variants without such restrictions.\nTable 3:Results on the 2011 TREC Medical Track dataset. All the results are on a held-out test split from the dataset.\nVariant Metric\nMAP rPrec Bpref P@ 10 P@1000 NDCG MRR\nBM25 0.2595 0 .311 0 .387 0 .4286 0 .0523 0 .5979 0 .6658\nBERTBASE 0.1537 0 .1694 0 .3509 0 .2 0 .0457 0 .4327 0 .2463\nClinical BERT 0.1707 0 .2133 0 .3565 0 .2714 0 .0453 0 .4586 0 .4142\nBioBERT 0.1897 0 .222 0 .3798 0 .2857 0 .0451 0 .475 0 .3997\nBERTLARGE TREC-CAR 0.2131 0 .2528 0 .4378 0 .3857 0 .053 0 .5578 0 .5095\nBERTLARGE MS MARCO 0.247 0 .2721 0.4627 0.4143 0.0536 0.5883 0 .6054\nTable 4:Results on the 2012 TREC Medical Track dataset. All the results are on a held-out test split from the dataset.\nVariant Metric\nMAP rPrec Bpref P@ 10 P@1000 NDCG MRR\nBM25 0.2402 0 .2584 0 .2692 0 .4 0 .0625 0 .5278 0 .6036\nBERTBASE 0.2488 0 .2922 0 .3255 0 .44 0 .0633 0.5722 0 .795\nClinical BERT 0.1686 0 .1989 0 .23 0 .32 0 .0614 0 .4753 0 .5843\nBioBERT 0.2263 0 .2573 0 .302 0 .36 0 .0614 0 .4993 0 .6593\nBERTLARGE TREC-CAR 0.207 0 .2779 0 .3154 0 .37 0 .0635 0 .5249 0 .5679\nBERTLARGE MS MARCO 0.2539 0 .311 0 .3503 0 .48 0 .0646 0.565 0 .64\nFor the 2011 data, the BERT LARGE model ﬁne-tuned on MS MARCO dataset outperformed BM25 on Bpref and\nP@1000 metrics. Also, this variant and BERT LARGE TREC-CAR consistently performed at similar levels of perfor-\nmance as BM25 on all the other metrics. Among the BERTBASE models, the ones further pre-trained on other clinical\nand biomedical datasets performed better than the vanilla variant on all the measures except P@1000. Also, the mod-\nels based on BERTLARGE achieved higher scores than the models using BERTBASE. Interestingly, the baseline method\noutperforms almost all the BERT-based models on a majority of the evaluated metrics.\nFor the 2012 data, the BERTLARGE MS MARCO variant achieved the best scores in terms of most of the performance\nmeasures, namely, MAP, rPrec, Bpref, P@ 10, and P@ 1000. There was a 30.1% increase in the Bpref score by this\nvariant as compared to the baseline score (using BM25). The vanilla BERT BASE model performed the best in terms\nof NDCG and MRR metrics. This variant improves upon MRR as compared to BM25 by a total of 0.1914 points (a\n31.7% improvement). Both BERTLARGE MS MARCO and BERTBASE models performed consistently better than the\nbaseline on all the evaluated metrics. Also, the vanilla BERTBASE model achieved better scores over all the measures\nthan the other model variants based on BERT BASE as well as the BERT LARGE TREC-CAR variant. Similarly, the\nbaseline method surpasses the performances of Clinical BERT, BioBERT, and BERTLARGE TREC-CAR on a majority\nof the measures. However, we note that all the model variations outperformed the baseline method on Bpref metric\nexcept for Clinical BERT.\nDiscussion\nWe perform an array of experiments to analyze the performance improvements of employing deep learning models to\nthe task of patient CR. We use different variants of the BERT model (both in terms of the model size and the datasets\nthey are trained on) to study their impact on various performance metrics. To our knowledge, this is the ﬁrst work to\nstudy the beneﬁts of applying deep language models on CR from the EHRs.\nWe use a large number of metrics for our evaluations as the evaluation measures are application-speciﬁc. Though we\nfocus on the task of CR, similar methods can be employed to other tasks in the same domain. E.g., such a retrieval\npipeline can also be applied to patient-speciﬁc EHR search. In this example, a higher MRR may be preferred.\nThough the annotations are available only at the visit level, we trained our pipeline at the report level. The main reason\nbeing the high number of reports for each visit (as much as 415). More sophisticated methods or heuristics can be\napplied to handle the large amounts of reports per visit. Such methods can aim at summarizing the content at a visit\nlevel (like we do at the level of reports). Further, the effect of summarization can also be explored in detail as it is a\nwell-studied area and offers many different ways to extract the information from the clinical documents41,42.\nThe evaluation performances achieved by other systems on the same datasets4,15,16,28 may not be directly comparable\nbecause of many reasons. The systems developed as part of the original submissions to the TREC Medical Track\nchallenge were evaluated manually. Though, as mentioned earlier, we have access to the relevance judgments from\nthe original submissions, note that the set of annotations is not exhaustive or complete. Hence, we may have a relevant\ndocument in our ranked list of visits that might not be judged during the original submissions and thus may not be\nconsidered a RELEVANT document during the evaluations. Some evaluation measures such as Bpref takes into account\nthe absence of relevant documents that may not be available in the ground truth39.\nFor the 2012 data, 60% of our test topics happened to be in the bottom third in terms of their per-topic inferred NDCG\nscores (infNDCG, the primary evaluation metric for TRECMed 2012) computed over all the runs submitted to the\nchallenge that year. Thus, the topics in our test set were predominantly hard. Also, the language models used in our\nevaluation are based on transformers that usually take a huge amount of time even for pre-training and ﬁne-tuning. The\naverage amount of time required for us to run one experiment was about2 hours (even on a powerful GPU with32GB\nmemory). Just for running all the ﬁnal experiments reported in this paper (on a fold), it took around12 ∗2 = 24hours.\nThus, performing cross-validation for all the included variants would have been highly resource-intensive. Besides,\nthe aim of this paper is to highlight the importance of applying transformer language models to the task of patient CR\nand showing how it can be used to improve the performance over baseline methods such as BM25. Best practices to\nincorporate such powerful models to the tasks of speciﬁc domains will require more thorough experimentation.\nThe baseline method outperformed almost all the models variants for the2011 data. However, for the 2012 data, some\nmodel variations were able to consistently outscore the baseline method. We note that the total number of topics for\nthe 2011 dataset is less than that for the2012 data. Thus, the models for 2012 data were trained on a larger training set\n(80% of that year’s dataset) than what was used to train the2011 models. The larger training set (with 10 topics more\nworth of training data) may have played a role in enabling some of the transformer models to outperform the baseline\nfor 2012 dataset.\nUnfortunately, the TRECMed dataset is no longer available to general public because of issues related to sharing the\nclinical data. This is an ongoing problem in clinical research. However, domain adaptation and transfer learning\ntechniques can play an important role in overcoming some of the issues related to the shorter or uncleaned supply\nof data for tuning the models 43. E.g., in this work we make use of the BERT models pre-trained on large general-\ndomain data and also employ models ﬁne-tuned on open-domain document retrieval datasets. We note that the models\nﬁne-tuned on open-domain datasets performed consistently better than the baseline and other vanilla models. This\nhighlights the importance of transferring knowledge from large open-domain datasets for a similar task.\nConclusion\nWe experiment with different variants of transformer language models based on the BERT architecture. We ﬁnd that\nincorporating a language model pre-trained and ﬁne-tuned on various general domain and clinical domain datasets\noutperforms the baseline BM25 method on a majority of metrics. Our models achieve up to 31.7% improvement over\nthe performance of the baseline model on several metrics. To our knowledge, this is the ﬁrst work to study the impact\nof employing deep language models to the task of patient CR.\nAcknowledgments\nThis work was supported by the U.S. National Library of Medicine, National Institutes of Health (NIH), under award\nR00LM012104, and the Cancer Prevention and Research Institute of Texas (CPRIT), under award RP170668.\nReferences\n[1] Shivade C, Raghavan P, Fosler-Lussier E, Embi PJ, Elhadad N, Johnson SB, et al. A Review of Approaches\nto Identifying Patient Phenotype Cohorts Using Electronic Health Records. Journal of the American Medical\nInformatics Association. 2014;21:221–230.\n[2] Sarmiento RF, Dernoncourt F. Improving Patient Cohort Identiﬁcation Using Natural Language Processing. In:\nMIT Critical Data, editor. Secondary Analysis of Electronic Health Records; 2016. p. 405–417.\n[3] Friedman C, Kra P, Rzhetsky A. Two Biomedical Sublanguages: A Description Based on the Theories of Zellig\nHarris. Journal of Biomedical Informatics. 2002;35:222–235.\n[4] V oorhees EM. The TREC Medical Records Track. In: Proceedings of the International Conference on Bioinfor-\nmatics, Computational Biology and Biomedical Informatics; 2013. p. 239–246.\n[5] LeCun Y , Bengio Y , Hinton G. Deep Learning. Nature. 2015;521:436–444.\n[6] Wu S, Roberts K, Datta S, Du J, Ji Z, Si Y , et al. Deep Learning in Clinical Natural Language Processing: A\nMethodical Review. Journal of the American Medical Informatics Association. 2020;27:457–470.\n[7] Bengio Y , Courville A, Vincent P. Representation Learning: A Review and New Perspectives. IEEE Transactions\non Pattern Analysis and Machine Intelligence. 2013;35:1798–1828.\n[8] Li H. Deep Learning for Natural Language Processing: Advantages and Challenges. National Science Review.\n2018;5:24–26.\n[9] Hersh W. Information Retrieval: A Health and Biomedical Perspective. 3rd ed.; 2009.\n[10] Devlin J, Chang MW, Lee K, Toutanova K. BERT: Pre-Training of Deep Bidirectional Transformers for Lan-\nguage Understanding. In: Proceedings of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies; 2019. p. 4171–4186.\n[11] Yang Z, Dai Z, Yang Y , Carbonell J, Salakhutdinov R, Le QV . XLNet: Generalized Autoregressive Pretraining\nfor Language Understanding. arXiv. 2019;1906.08237 [cs].\n[12] Alsentzer E, Murphy J, Boag W, Weng WH, Jindi D, Naumann T, et al. Publicly Available Clinical BERT\nEmbeddings. In: Proceedings of the 2nd Clinical Natural Language Processing Workshop; 2019. p. 72–78.\n[13] Lee J, Yoon W, Kim S, Kim D, Kim S, So CH, et al. BioBERT: A Pre-Trained Biomedical Language Represen-\ntation Model for Biomedical Text Mining. Bioinformatics. 2019;p. 1–7.\n[14] Huang K, Singh A, Chen S, Moseley ET, Deng Cy, George N, et al. Clinical XLNet: Modeling Sequential\nClinical Notes and Predicting Prolonged Mechanical Ventilation. arXiv. 2019;1912.11975 [cs].\n[15] V oorhees EM, Tong R. Overview of the TREC 2011 Medical Records Track. In: The Twentieth Text REtrieval\nConference Proceedings; 2011.\n[16] Goodwin TR, Harabagiu SM. Learning Relevance Models for Patient Cohort Retrieval. JAMIA Open.\n2018;1:265–275.\n[17] Aronson AR, Lang FM. An Overview of MetaMap: Historical Perspective and Recent Advances. Journal of the\nAmerican Medical Informatics Association. 2010;17:229–236.\n[18] Chapman WW, Bridewell W, Hanbury P, Cooper GF, Buchanan BG. A Simple Algorithm for Identifying Negated\nFindings and Diseases in Discharge Summaries. Journal of Biomedical Informatics. 2001;34:301–310.\n[19] Edinger T, Cohen AM, Bedrick S, Ambert K, Hersh W. Barriers to Retrieving Patient Information from Electronic\nHealth Record Data: Failure Analysis from the TREC Medical Records Track. AMIA Annual Symposium\nProceedings. 2012;2012:180–188.\n[20] Mitra B, Craswell N. Neural Models for Information Retrieval. arXiv. 2017;1705.01509 [cs].\n[21] Zhu R, Tu X, Xiangji Huang J. Chapter Seven - Deep Learning on Information Retrieval and Its Applications.\nIn: Das H, Pradhan C, Dey N, editors. Deep Learning for Data Analytics; 2020. p. 125–153.\n[22] Hui K, Yates A, Berberich K, de Melo G. Co-PACRR: A Context-Aware Neural IR Model for Ad-Hoc Retrieval.\nIn: Proceedings of the 11th ACM International Conference on Web Search and Data Mining; 2018. p. 279–287.\n[23] Xiong C, Dai Z, Callan J, Liu Z, Power R. End-to-End Neural Ad-Hoc Ranking with Kernel Pooling. In:\nProceedings of the 40th International ACM SIGIR Conference on Research and Development in Information\nRetrieval; 2017. p. 55–64.\n[24] Guo J, Fan Y , Ai Q, Croft WB. A Deep Relevance Matching Model for Ad-Hoc Retrieval. In: Proceedings of\nthe 25th ACM International on Conference on Information and Knowledge Management; 2016. p. 55–64.\n[25] Nogueira R, Cho K. Passage Re-Ranking with BERT. arXiv. 2019;1901.04085 [cs].\n[26] Dietz L, Verma M, Radlinski F, Craswell N. TREC Complex Answer Retrieval Overview. In: TREC; 2017.\n[27] Bajaj P, Campos D, Craswell N, Deng L, Gao J, Liu X, et al. MS MARCO: A Human Generated MAchine\nReading COmprehension Dataset. arXiv. 2018;1611.09268 [cs].\n[28] V oorhees EM, Hersh W. Overview of the TREC 2012 Medical Records Track. In: The Twenty-First Text\nREtrieval Conference Proceedings; 2012.\n[29] Jakarta A. Apache Lucene-a High-Performance, Full-Featured Text Search Engine Library; 2004. Apache\nLucene.\n[30] Robertson SE, Walker S, Jones S, Hancock-Beaulieu M, Gatford M. Okapi at TREC-3. In: Proceedings of the\n3rd Text REtrieval Conference; 1994. p. 109–126.\n[31] Yang P, Fang H, Lin J. Anserini: Reproducible Ranking Baselines Using Lucene. Journal of Data and Information\nQuality. 2018;10:16:1–16:20.\n[32] Wu Y , Schuster M, Chen Z, Le QV , Norouzi M, Macherey W, et al. Google’s Neural Machine Translation System:\nBridging the Gap between Human and Machine Translation. arXiv. 2016;1609.08144 [cs].\n[33] Savova GK, Masanz JJ, Ogren PV , Zheng J, Sohn S, Kipper-Schuler KC, et al. Mayo Clinical Text Analysis and\nKnowledge Extraction System (cTAKES): Architecture, Component Evaluation and Applications. Journal of the\nAmerican Medical Informatics Association. 2010;17:507–513.\n[34] Wang Y , Wang L, Rastegar-Mojarad M, Moon S, Shen F, Afzal N, et al. Clinical Information Extraction Appli-\ncations: A Literature Review. Journal of Biomedical Informatics. 2018;77:34–49.\n[35] Manning C, Surdeanu M, Bauer J, Finkel J, Bethard S, McClosky D. The Stanford CoreNLP Natural Language\nProcessing Toolkit. In: Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics:\nSystem Demonstrations; 2014. p. 55–60.\n[36] Weng WH, Chung Y A, Tong S. Clinical Text Summarization with Syntax-Based Negation and Semantic Concept\nIdentiﬁcation. arXiv. 2020;2003.00353 [cs].\n[37] Zhu D, Carterette B. Exploring Evidence Aggregation Methods and External Expansion Sources for Medical\nRecord Search. In: Proceedings of the 21th Text REtrieval Conference (TREC); 2013.\n[38] Tayyar Madabushi H, Kochkina E, Castelle M. Cost-Sensitive BERT for Generalisable Sentence Classiﬁcation\non Imbalanced Data. In: Proceedings of the Second Workshop on Natural Language Processing for Internet\nFreedom: Censorship, Disinformation, and Propaganda; 2019. p. 125–134.\n[39] Buckley C, V oorhees EM. Retrieval Evaluation with Incomplete Information. In: Proceedings of the 27th Annual\nInternational ACM SIGIR Conference on Research and Development in Information Retrieval; 2004. p. 25–32.\n[40] V oorhees EM, Harman DK. TREC: Experiment and Evaluation in Information Retrieval. vol. 63; 2005.\n[41] Pivovarov R, Elhadad N. Automated Methods for the Summarization of Electronic Health Records. Journal of\nthe American Medical Informatics Association. 2015;22:938–947.\n[42] Mishra R, Bian J, Fiszman M, Weir CR, Jonnalagadda S, Mostafa J, et al. Text Summarization in the Biomedical\nDomain: A Systematic Review of Recent Research. Journal of Biomedical Informatics. 2014;52:457–467.\n[43] Soni S, Roberts K. Evaluation of Dataset Selection for Pre-Training and Fine-Tuning Transformer Language\nModels for Clinical Question Answering. In: Proceedings of the 12th International Conference on Language\nResources and Evaluation; 2020. p. 5534–5540.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8184363842010498
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6424791216850281
    },
    {
      "name": "Transformer",
      "score": 0.6281626224517822
    },
    {
      "name": "Task (project management)",
      "score": 0.6243007183074951
    },
    {
      "name": "Feature engineering",
      "score": 0.6121348738670349
    },
    {
      "name": "Natural language processing",
      "score": 0.5970832705497742
    },
    {
      "name": "Language model",
      "score": 0.5296114683151245
    },
    {
      "name": "Baseline (sea)",
      "score": 0.5045832395553589
    },
    {
      "name": "Information retrieval",
      "score": 0.4730357825756073
    },
    {
      "name": "Question answering",
      "score": 0.46108362078666687
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.4517594277858734
    },
    {
      "name": "Health records",
      "score": 0.42768171429634094
    },
    {
      "name": "Deep learning",
      "score": 0.422676146030426
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.4178566634654999
    },
    {
      "name": "Health care",
      "score": 0.09076333045959473
    },
    {
      "name": "Oceanography",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Geology",
      "score": 0.0
    },
    {
      "name": "Economic growth",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I919571938",
      "name": "The University of Texas Health Science Center at Houston",
      "country": "US"
    }
  ]
}