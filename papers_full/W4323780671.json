{
  "title": "A Benchmark of PDF Information Extraction Tools Using a Multi-task and Multi-domain Evaluation Framework for Academic Documents",
  "url": "https://openalex.org/W4323780671",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5060549879",
      "name": "Norman Meuschke",
      "affiliations": [
        "University of Göttingen"
      ]
    },
    {
      "id": "https://openalex.org/A5036941105",
      "name": "Apurva Jagdale",
      "affiliations": [
        "University of Passau"
      ]
    },
    {
      "id": "https://openalex.org/A5041704286",
      "name": "Timo Spinde",
      "affiliations": [
        "University of Göttingen"
      ]
    },
    {
      "id": "https://openalex.org/A5019466280",
      "name": "Jelena Mitrović",
      "affiliations": [
        "University of Passau"
      ]
    },
    {
      "id": "https://openalex.org/A5058837356",
      "name": "Béla Gipp",
      "affiliations": [
        "University of Göttingen"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3031676291",
    "https://openalex.org/W3023264106",
    "https://openalex.org/W2964269754",
    "https://openalex.org/W4297786773",
    "https://openalex.org/W115105276",
    "https://openalex.org/W4238015202",
    "https://openalex.org/W2765603635",
    "https://openalex.org/W3004248957",
    "https://openalex.org/W2416987009",
    "https://openalex.org/W2040217811",
    "https://openalex.org/W4244512068",
    "https://openalex.org/W1489253926",
    "https://openalex.org/W2163737492",
    "https://openalex.org/W2893567907",
    "https://openalex.org/W2912928117",
    "https://openalex.org/W4412258501",
    "https://openalex.org/W1998839545",
    "https://openalex.org/W2082470640",
    "https://openalex.org/W4398678036",
    "https://openalex.org/W3134359195",
    "https://openalex.org/W1995577199",
    "https://openalex.org/W2009752265",
    "https://openalex.org/W2104029044",
    "https://openalex.org/W2753149099",
    "https://openalex.org/W2147107544",
    "https://openalex.org/W3113753692",
    "https://openalex.org/W2098564132",
    "https://openalex.org/W3175487048",
    "https://openalex.org/W3015453090",
    "https://openalex.org/W1603719052",
    "https://openalex.org/W2124303864",
    "https://openalex.org/W2079525773",
    "https://openalex.org/W2162630660",
    "https://openalex.org/W2000731427",
    "https://openalex.org/W2132155482",
    "https://openalex.org/W2172223321",
    "https://openalex.org/W2803318469",
    "https://openalex.org/W3092527850",
    "https://openalex.org/W2808479155",
    "https://openalex.org/W3009242205",
    "https://openalex.org/W2015814595",
    "https://openalex.org/W2798010889",
    "https://openalex.org/W791527587",
    "https://openalex.org/W2962919508",
    "https://openalex.org/W3018109885",
    "https://openalex.org/W44107710",
    "https://openalex.org/W1534730506",
    "https://openalex.org/W1563075779",
    "https://openalex.org/W1559499673",
    "https://openalex.org/W2806922180",
    "https://openalex.org/W2998138786",
    "https://openalex.org/W2941922035"
  ],
  "abstract": "Extracting information from academic PDF documents is crucial for numerous\\nindexing, retrieval, and analysis use cases. Choosing the best tool to extract\\nspecific content elements is difficult because many, technically diverse tools\\nare available, but recent performance benchmarks are rare. Moreover, such\\nbenchmarks typically cover only a few content elements like header metadata or\\nbibliographic references and use smaller datasets from specific academic\\ndisciplines. We provide a large and diverse evaluation framework that supports\\nmore extraction tasks than most related datasets. Our framework builds upon\\nDocBank, a multi-domain dataset of 1.5M annotated content elements extracted\\nfrom 500K pages of research papers on arXiv. Using the new framework, we\\nbenchmark ten freely available tools in extracting document metadata,\\nbibliographic references, tables, and other content elements from academic PDF\\ndocuments. GROBID achieves the best metadata and reference extraction results,\\nfollowed by CERMINE and Science Parse. For table extraction, Adobe Extract\\noutperforms other tools, even though the performance is much lower than for\\nother content elements. All tools struggle to extract lists, footers, and\\nequations. We conclude that more research on improving and combining tools is\\nnecessary to achieve satisfactory extraction quality for most content elements.\\nEvaluation datasets and frameworks like the one we present support this line of\\nresearch. We make our data and code publicly available to contribute toward\\nthis goal.\\n",
  "full_text": "Related papers at https://gipplab.org/pub\nPreprint of the paper:\nMeuschke, N. & Jagdale, A. & Spinde, T. & Mitrovi´ c, J. & Gipp, B.,\n”A Benchmark of PDF Information Extraction Tools Using a Multi-task\nand Multi-domain Evaluation Framework for Academic Documents”, in\nInformation for a Better World: Normality, Virtuality, Physicality, Inclu-\nsivity, LNCS, vol. 13972, Cham: Springer Nature Switzerland, 2023, pp.\n383–405, DOI: 10.1007/978-3-031-28032-0 31.\nClick to download:BibTeX\nA Benchmark of PDF Information Extraction\nTools using a Multi-Task and Multi-Domain\nEvaluation Framework for Academic Documents\nNorman Meuschke1,[ORCID], Apurva Jagdale2, Timo Spinde1,[ORCID],\nJelena Mitrovi´ c2,3,[ORCID], and Bela Gipp 1,[ORCID]\n1 University of G¨ ottingen, 37073 G¨ ottingen, Germany\n{meuschke, spinde, gipp}@uni-goettingen.de\n2 University of Passau, 94032 Passau, Germany\n{apurva.jagdale, jelena.mitrovic}@uni-passau.de\n3 The Institute for Artiﬁcial Intelligence R&D of Serbia, 21000 Novi Sad, Serbia\nAbstract. Extracting information from academic PDF documents is\ncrucial for numerous indexing, retrieval, and analysis use cases. Choos-\ning the best tool to extract speciﬁc content elements is diﬃcult because\nmany, technically diverse tools are available, but recent performance\nbenchmarks are rare. Moreover, such benchmarks typically cover only\na few content elements like header metadata or bibliographic references\nand use smaller datasets from speciﬁc academic disciplines. We provide\na large and diverse evaluation framework that supports more extraction\ntasks than most related datasets. Our framework builds upon DocBank,\na multi-domain dataset of 1.5M annotated content elements extracted\nfrom 500K pages of research papers on arXiv. Using the new framework,\nwe benchmark ten freely available tools in extracting document meta-\ndata, bibliographic references, tables, and other content elements from\nacademic PDF documents. GROBID achieves the best metadata and ref-\nerence extraction results, followed by CERMINE and Science Parse. For\ntable extraction, Adobe Extract outperforms other tools, even though\nthe performance is much lower than for other content elements. All tools\nstruggle to extract lists, footers, and equations. We conclude that more\nresearch on improving and combining tools is necessary to achieve satis-\nfactory extraction quality for most content elements. Evaluation datasets\nand frameworks like the one we present support this line of research. We\nmake our data and code publicly available to contribute toward this goal.\nKeywords: PDF · Information Extraction · Benchmark · Evaluation.\n1 Introduction\nThe Portable Document Format (PDF) is the most prevalent encoding for aca-\ndemic documents. Extracting information from academic PDF documents is cru-\ncial for numerous indexing, retrieval, and analysis tasks. Document search, rec-\nommendation, summarization, classiﬁcation, knowledge base construction, ques-\ntion answering, and bibliometric analysis are just a few examples [31].\n1\narXiv:2303.09957v1  [cs.IR]  17 Mar 2023\n2 Meuschke et al.\nHowever, the format’s technical design makes information extraction chal-\nlenging. Adobe designed PDF as a platform-independent, ﬁxed-layout format\nby extending the PostScript [24] page description language. PDF focuses on\nencoding a document’s visual layout to ensure a consistent appearance of the\ndocument across software and hardware platforms but includes little structural\nand semantic information on document elements.\nNumerous tools for information extraction (IE) from PDF documents have\nbeen presented since the format’s inception in 1993. The development of such\ntools has been subject to a fast-paced technological evolution of extraction ap-\nproaches from rule-based algorithms, over statistical machine learning (ML) to\ndeep learning (DL) models (cf. Section 2). Finding the best tool to extract spe-\nciﬁc content elements from PDF documents is currently diﬃcult because:\n1. Typically, tools only support extracting a subset of the content elements\nin academic documents, e.g., title, authors, paragraphs, in-text citations,\ncaptions, tables, ﬁgures, equations, or references.\n2. Many information extraction tools, e.g., 12 of 35 tools we considered for our\nstudy, are no longer maintained or have become obsolete.\n3. Prior evaluations of information extraction tools often consider only speciﬁc\ncontent elements or use domain-speciﬁc corpora, which makes their results\ndiﬃcult to compare. Moreover, the most recent comprehensive benchmarks\nof information extraction tools were published in 2015 for metadata 4 [55],\n2017 for body text [6], and 2018 for references 5 [54], respectively. These\nevaluations do not reﬂect the latest technological advances in the ﬁeld.\nTo alleviate this knowledge gap and facilitate ﬁnding the best tool to extract\nspeciﬁc elements from academic PDF documents, we comprehensively evaluate\nten state-of-the-art non-commercial tools that consider eleven content elements\nbased on a dataset of 500K pages from arXiv documents covering multiple ﬁelds.\nOur code, data, and resources are publicly available at\nhttp://pdf-benchmark.gipplab.org\n2 Related Work\nThis section presents approaches for information extraction from PDF (Sec-\ntion 2.1), labeled datasets suitable for training and evaluating PDF information\nextraction approaches, and prior evaluations of IE tools (Section 2.2).\n2.1 Information Extraction from PDF Documents\nTable 1 summarizes publications on PDF information extraction since 1999. For\neach publication, the table shows the primary technological approach and the\n4 For example author(s), title, aﬃliation(s), address(es), email(s)\n5 Refers to extracting the components of bibliographic references, e.g., author(s), title,\nvenue, editor(s), volume, issue, page range, year of publication, etc.\nA Benchmark of PDF Information Extraction Tools 3\nTable 1: Publications on information extraction from PDF documents.\nPublication1 Year Task2 Method Training Dataset 3\nPalermo [44] 1999 M, ToC Rules 100 documents\nKlink [27] 2000 M Rules 979 pages\nGiuﬀrida [18] 2000 M Rules 1,000 documents\nAiello [2] 2002 RO, Title Rules 1,000 pages\nMao [37] 2004 M OCR,\nRules\n309 documents\nPeng [45] 2004 M, R CRF CORA (500 refs.)\nDay [14] 2007 M, R Template 160,000 citations\nHetzner [23] 2008 R HMM CORA (500 refs.)\nCouncill [12] 2008 R CRF CORA (200 refs.), CiteSeer (200 refs.)\nLopez [36] 2009 B, M, R CRF, DL None\nCui [13] 2010 M HMM 400 documents\nOjokoh [42] 2010 M HMM CORA (500 refs.), ManCreat\nFLUX-CiM (300 refs.),\nKern [25] 2012 M HMM E-prints, Mendeley, PubMed (19K entries)\nBast [5] 2013 B, M, R Rules DBLP (690 docs.), PubMed (500 docs.)\nSouza [53] 2014 M CRF 100 documents\nAnzaroot [3] 2014 R CRF UMASS (1,800 refs.)\nVilnis [57] 2015 R CRF UMASS (1,800 refs.)\nTkaczyk [55] 2015 B, M, R CRF,\nRules,\nSVM\nCiteSeer (4,000 refs.), CORA (500 refs.),\nGROTOAP, PMC (53K docs.)\nBhardwaj [7] 2017 R FCN 5,090 references\nRodrigues [49] 2018 R BiLSTM 40,000 references\nPrasad [46] 2018 M, R CRF, DL FLUX-CiM (300 refs.), CiteSeer (4,000 refs.)\nJahongir [4] 2018 M Rules 10,000 documents\nTorre [15] 2018 B, M Rules 300 documents\nRizvi [47] 2020 R R-CNN 40,000 references\nHashmi [22] 2020 M Rules 45 documents\nAhmed [1] 2020 M Rules 150 documents\nNikolaos [33] 2021 B, M, R Attention,\nBiLSTM\n3,000 documents\n1 Publications in chronological order; the labels indicate the ﬁrst author only.\n2 (B) Body text, (M) Metadata, (R) References, (RO) Reading order, (ToC) Table of contents\n3 Domain-speciﬁc datasets: Computer Science: CiteSeer [43], CORA [39], DBLP [52], FLUX-\nCiM [10,11], ManCreat [42]; Health Science: PubMed [40], PMC [41]\n4 Meuschke et al.\ntraining dataset. Eighteen of 27 approaches (67%) employ machine learning or\ndeep learning (DL) techniques, and the remainder rule-based extraction (Rules).\nEarly tools rely on manually coded rules [44]. Second-generation tools use sta-\ntistical machine learning, e.g., based on Hidden Markov Models (HMM) [8],\nConditional Random Fields (CRF) [29], and maximum entropy [26]. The most\nrecent information extraction tools employ Transformer models [56].\nA preference for—in theory—more ﬂexible and adaptive machine learning\nand deep learning techniques over case-speciﬁc rule-based algorithms is observ-\nable in Table 1. However, many training datasets are domain-speciﬁc, e.g., they\nexclusively consist of documents from Computer Science or Health Science, and\ncomprise fewer than 500 documents. These two factors put the generalizability\nof the respective IE approaches into question. Notable exceptions like Ojokoh et\nal. [42], Kern et al. [25], and Tkaczyk et al. [55] use multiple datasets covering\ndiﬀerent domains for training and evaluation. However, these approaches address\nspeciﬁc tasks, i.e, header metadata extraction, reference extraction, or both.\nMoreover, a literature survey by Mao et al. shows that most approaches for\ntext extraction from PDF do not specify the ground-truth data and performance\nmetrics they use, which impedes performance comparisons [38]. A positive excep-\ntion is a publication by Bast et al. [5], which presents a comprehensive evaluation\nframework for text extraction from PDF that includes a ﬁne-grained speciﬁca-\ntion of the performance measures used.\n2.2 Labeled Datasets and Prior Benchmarks\nTable 2 summarizes datasets usable for training and evaluating PDF information\nextraction approaches grouped by the type of ground-truth labels they oﬀer.\nMost datasets exclusively oﬀer labels for document metadata, references, or both.\nTable 2: Labeled datasets for information extraction from PDF documents.\nPublication1 Size Ground-truth Labels\nFan [16] 147 documents Metadata\nF¨ arber [17] 90K documents References\nGrennan [21] 1B references References\nSaier [51,50] 1M documents. References\nLey [30,52] 6M documents Metadata, references\nMccallum [39] 935 documents Metadata, references\nKyle [34] 8.1M documents Metadata, references\nOrorbia [43] 6M documents. Metadata, references\nBast [6] 12,098 documents Body text, sections, title\nLi [31] 500K pages Captions, equations, ﬁgures, footers\nlists, metadata, paragraphs,\nreferences, sections, tables\n1 The labels indicate the ﬁrst author only.\nA Benchmark of PDF Information Extraction Tools 5\nOnly the DocBank dataset by Li et al. [31] oﬀers annotations for 12 diverse\ncontent elements in academic documents, including, ﬁgures, equations, tables,\nand captions. Most of these content elements have not been used for bench-\nmark evaluations yet. DocBank is comparably large (500K pages from research\npapers published on arXiv in a four-year period). A downside of the DocBank\ndataset is its coarse-grained labels for references, which do not annotate the\nﬁelds of bibliographic entries like the author, publisher, volume, or date, as do\nbibliography-speciﬁc datasets like unarXive [21] or S2ORC [34].\nTable 3 shows PDF information extraction benchmarks performed since 1999.\nFew such works exist and were rarely repeated or updated, which is sub-optimal\ngiven that many tools receive updates frequently. Other tools become techno-\nlogically obsolete or unmaintained. For instance, pdf-extract6, lapdftext7, PDF-\nSSA4MET8, and PDFMeat9 are no longer maintained actively, while ParsCit 10\nhas been replaced by NeuralParsCit 11 and SciWING12.\nTable 3: Benchmark evaluations of PDF information extraction approaches.\nPublication1 Dataset Metrics 2 Tools Labels 3\nGranitzer [19] E-prints (2,452 docs.),\nMendeley (20,672 docs.)\nP, R 2 M\nLipinski [32] arXiv (1,253 docs.) Acc 7 M\nBast [6] arXiv (12,098 docs.) Custom 14 NL, Pa\nRO, W\nK¨ orner [28] 100 (German docs.) P, R, F1 4 Ref\nTkaczyk [54] 9,491 documents P, R, F1 10 Ref\nRizvi [48] 8,766 references F1 4 Ref\n1 The labels indicate the ﬁrst author only.\n2 (P) Precision, (R) Recall, (F1) F1-score, (Acc) Accuracy\n3 (M) Metadata, (NL) New Line, (Pa) Paragraph, (Ref) Reference, (RO)\nReading order, (W) Words\nAs Table 3 shows, the most extensive dataset used for evaluating PDF infor-\nmation extraction tools so far contains approx. 24,000 documents. This number\nis small compared to the sizes of datasets available for this task, shown in Ta-\nble 2. Most studies focused on exclusively evaluating metadata and reference\nextraction (see also Table 3). An exception is a benchmark by Bast and Korzen\n6 https://github.com/CrossRef/pdfextract\n7 https://github.com/BMKEG/lapdftext\n8 https://github.com/eliask/pdfssa4met\n9 https://github.com/dimatura/pdfmeat\n10 https://github.com/knmnyn/ParsCit\n11 https://github.com/WING-NUS/Neural-ParsCit\n12 https://github.com/abhinavkashyap/sciwing\n6 Meuschke et al.\n[6], which evaluated spurious and missing words, paragraphs, and new lines for\n14 tools but used a comparably small dataset of approx. 10K documents.\nWe conclude from our review of related work that (1) recent benchmarks of\ninformation extraction tools for PDF are rare, (2) mostly analyze metadata ex-\ntraction, (3) use small, domain-speciﬁc datasets, and (4) include tools that have\nbecome obsolete or unmaintained. (5) A variety of suitably labeled datasets have\nnot been used to evaluate information extraction tools for PDF documents yet.\nTherefore, we see the need for benchmarking state-of-the-art PDF information\nextraction tools on a large labeled dataset of academic documents covering mul-\ntiple domains and containing diverse content elements.\n3 Methodology\nThis section presents the experimental setup of our study by describing the tools\nwe evaluate (Section 3.1), the dataset we use (Section 3.2), and the procedure\nwe follow (Section 3.3).\n3.1 Evaluated Tools\nWe chose ten actively maintained non-commercial open-source tools that we\ncategorize by extraction tasks.\n1. Metadata Extraction includes tools to extract titles, authors, abstracts,\nand similar document metadata.\n2. Reference Extraction comprises tools to access and parse bibliographic\nreference strings into ﬁelds like author names, publication titles, and venue.\n3. Table Extractionrefers to tools that allow accessing both the structure\nand data of tables.\n4. General Extractionsubsumes tools to extract, e.g., paragraphs, sections,\nﬁgures, captions, equations, lists, or footers.\nFor each of the tools we evaluate, Table 4 shows the version, supported extraction\ntask(s), primary technological approach, and output format. Hereafter, we brieﬂy\ndescribe each tool, focusing on its technological approach.\nAdobe Extract13 is a cloud-based API that allows extracting tables and\nnumerous other content elements subsumed in the general extraction category.\nThe API employs the Adobe Sensei 14 AI and machine learning platform to un-\nderstand the structure of PDF documents. To evaluate the Adobe Extract API,\nwe used the Adobe PDFServices Python SDK 15 to access the API’s services.\nApache Tika16 allows metadata and content extraction in XML format. We\nused the tika-python17 client to access the Tika REST API. Unfortunately, we\nfound that tika-python only supports content (paragraphs) extraction.\n13 https://www.adobe.io/apis/documentcloud/dcsdk/pdf-extract.html\n14 https://www.adobe.com/de/sensei.html\n15 https://github.com/adobe/pdfservices-python-sdk-samples\n16 https://tika.apache.org/\n17 https://github.com/chrismattmann/tika-python\nA Benchmark of PDF Information Extraction Tools 7\nTable 4: Overview of evaluated information extraction tools.\nTool Version Task 1 Technology Output\nAdobe Extract 1.0 G, T Adobe Sensei AI Framework JSON, XLSX\nApache Tika 2.0.0 G Apache PDFBox TXT\nCamelot 0.10.1 T OpenCV, PDFMiner CSV, Dataframe\nCERMINE 1.13 G, M, R CRF, iText, Rules, SVM JATS\nGROBID 0.7.0 G, M, R, T CRF, Deep Learning, Pdfalto TEI XML\nPdfAct n/a G, M, R, T pdftotext, rules JSON, TXT, XML\nPyMuPDF 1.19.1 G OCR, tesseract TXT\nRefExtract 0.2.5 R pdftotext, rules TXT\nScienceParse 1.0 G, M, R, CRF, pdﬃgures2, rules JSON\nTabula 1.2.1 T PDFBox, rules CSV, Dataframe\n1 (G) General, (M) Metadata, (R) References, (T) Table\nCamelot18 can extract tables using either the Stream or Lattice modes. The\nformer uses whitespace between cells and the latter table borders for table cell\nidentiﬁcation. For our experiments, we exclusively use the Stream mode, since\nour test documents are academic papers, in which tables typically use whitespace\nin favor of cell borders to delineate cells. The Stream mode internally utilizes\nthe PDFMiner library19 to extract characters that are subsequently grouped into\nwords and sentences using whitespace margins.\nCERMINE [55] oﬀers metadata, reference, and general extraction capabil-\nities. The tool employs the iText PDF toolkit20 for character extraction and the\nDocstrum21 image segmentation algorithm for page segmentation of document\nimages. CERMINE uses an SVM classiﬁer implemented using the LibSVM 22\nlibrary and rule-based algorithms for metadata extraction. For reference ex-\ntraction, the tool employs k-means clustering, and Conditional Random Fields\nimplemented using the MALLET 23 toolkit for sequence labeling. CERMINE\nreturns a single XML ﬁle containing the annotations for an entire PDF. We\nemploy the Beautiful Soup 24 library to ﬁlter CERMINE’s output ﬁles for the\nannotations relevant to our evaluation.\nGROBID25 [35] supports all four extraction tasks. The tool allows using ei-\nther feature-engineered CRF (default) or a combination of CRF and DL models\nrealized using the DeLFT26 Deep Learning library, which is based on TensorFlow\nand Keras. GROBID uses a cascade of sequence labeling models for diﬀerent\ncomponents. The models in the model cascade use individual label sequencing\n18 https://github.com/camelot-dev/camelot\n19 https://github.com/pdfminer/pdfminer.six\n20 https://github.com/itext\n21 https://github.com/chulwoopack/docstrum\n22 https://github.com/cjlin1/libsvm\n23 http://mallet.cs.umass.edu/sequences.php\n24 https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n25 https://github.com/kermitt2/grobid\n26 https://github.com/kermitt2/delft\n8 Meuschke et al.\nalgorithms and features; some models employ tokenizers. This approach oﬀers\nﬂexibility by allowing model tuning and improves the model’s maintainability.\nWe evaluate the default CRF model with production settings (a recommended\nsetting to improve the performance and availability of the GROBID server, ac-\ncording to the tool’s documentation 27).\nPdfAct formerly called Icecite [5] is a rule-based tool that supports all four\nextraction tasks, including the extraction of appendices, acknowledgments, and\ntables of contents. The tool uses the PDFBox 28 and pdftotext29 PDF manipu-\nlation and content extraction libraries. We use the tool’s JAR release 30.\nPyMuPDF31 extends the MuPDF 32 viewer library with font and image\nextraction, PDF joining, and ﬁle embedding. PyMuPDF uses tesseract 33 for\nOCR. PyMuPDF could not process ﬁles whose names include special characters.\nRefExtract34 is a reference extraction tool that uses pdftotext 35 and regu-\nlar expressions. RefExtract returns annotations for the entire bibliography of a\ndocument. The ground-truth annotations in our dataset (cf. Section 3.2), how-\never, pertain to individual pages of documents and do not always cover the entire\ndocument. If ground-truth annotations are only available for a subset of the ref-\nerences in a document, we use regular expressions to ﬁlter RefExtract’s output\nto those references with ground-truth labels.\nScience Parse36 uses a CRF model trained on data from GROBID to ex-\ntract the title, author, and references. It also employs a rule-based algorithm by\nClark and Divvala [9] to extract sections and paragraphs in JSON format.\nTabula37 is a table extraction tool. Analogous to Camelot, Tabula oﬀers a\nStream mode realized using PDFBox, and aLattice mode realized using OpenCV\nfor table cell recognition.\n3.2 Dataset\nWe use the DocBank 38 dataset, created by Li et al. [31], for our experiments.\nFigure 1 visualizes the process for compiling the dataset. First, the creators\ngathered arXiv documents, for which both the PDF and LaTeX source code was\navailable. Li et al. then edited the LaTeX code to enable accurate automated\nannotations of content elements in the PDF version of the documents. For this\npurpose, they inserted commands that formatted content elements in speciﬁc\n27 https://GROBID.readthedocs.io/en/latest/Troubleshooting/\n28 http://pdfbox.apache.org/\n29 https://github.com/jalan/pdftotext\n30 https://github.com/ad-freiburg/pdfact\n31 https://github.com/pymupdf/PyMuPDF\n32 https://mupdf.com/\n33 https://github.com/tesseract-ocr/tesseract\n34 https://github.com/inspirehep/refextract\n35 https://linux.die.net/man1/pdftotext\n36 https://github.com/allenai/science-parse\n37 https://github.com/chezou/tabula-py\n38 https://github.com/doc-analysis/DocBank\nA Benchmark of PDF Information Extraction Tools 9\nDocuments\nSemantic Structure\nDetection\nToken Annotation \nData acquisition from arXiv (PDF and .tex)\n\\section {Section1} \n| \n\\section{{\\color {fontcolor} {Section1}}} \nTab-separated ground truth file \nFig. 1: Process for generating the DocBank dataset.\ncolors. The center part of Figure 3 shows the mapping of content elements to\ncolors. In the last step, the dataset creators used PDFPlumber39 and PDFMiner\nto extract and annotate relevant content elements by their color. DocBank pro-\nvides the annotations as separate ﬁles for each document page in the dataset.\nTable 5 shows the structure of the tab-separated ground-truth ﬁles. Each\nline in the ﬁle refers to one component on the page and is structured as follows.\nIndex 0 represents the token itself, e.g., a word. Indices 1-4 denote the bounding\nbox information of the token, where (x0, y0) represents the top-left and (x1, y1)\nthe bottom-right corner of the token in the PDF coordinate space. Indices 5-7\nreﬂect the token’s color in RGB notation, index 8 the token’s font, and index 9\nthe label for the type of the content element. Each ground-truth ﬁle adheres to\nthe naming scheme shown in Figure 2.\nTable 5: Structure of DocBank’s plaintext ground-truth ﬁles.\nIndex 0 1 2 3 4 5 6 7 8 9\nContent token x0 y0 x1 y1 R G B font name label\nSource: https://doc-analysis.github.io/docbank-page/index.html.\nFig. 2: Naming scheme for DocBank’s ground-truth ﬁles.\n39 https://github.com/jsvine/pdfplumber\n10 Meuschke et al.\nThe DocBank dataset oﬀers ground-truth annotations for 1.5M content ele-\nments on 500K pages. Li et al. extracted the pages from arXiv papers in Physics,\nMathematics, Computer Science, and numerous other ﬁelds published between\n2014 and 2018. DocBank’s large size, recency, diversity of included documents,\nnumber of annotated content elements, and high annotation quality due to the\nweakly supervised labeling approach make it an ideal choice for our purposes.\n3.3 Evaluation Procedure\nEvaluation Framework \n9\nLabeled Data\nPDF Object\nAssemble Data\nPDF Extraction Tool \nSimilarity Matrix\n− Annotated data\n− File name\n− Page number\n− File path\nAbstract\nTitle\nAuthor\nCaption\nEquation\nList\nFooter\nReference \nParagraph\nSection\nTable\n{\nGround-truth DF\nfor an Element\n− Separate Tokens\n− Collated Tokens\nEvaluation Metrics\nTXT, JSON, XML, \nXSLX\nElement \nParsing\nElement \nSelection\n{\n− Levenshtein Ratio\n− Precision\n− Recall\n− Accuracy\n− F1 Score\nExtracted DF\nfor an Element\nFig. 3: Overview of the procedure for comparing content elements extracted by\nIE tools to the ground-truth annotations and computing evaluation metrics.\nFigure 3 shows our evaluation procedure. First, we select the PDF ﬁles whose\nassociated ground-truth ﬁles contain relevant labels. For example, we search for\nground-truth ﬁles containing reference tokens to evaluate reference extraction\ntools. We include the PDF ﬁle, the ground-truth ﬁle, the document ID and\npage number obtainable from the ﬁle name (cf. Figure 2), and the ﬁle path in a\nself-deﬁned Python object (see PDF Object in Figure 3).\nThen, the evaluation process splits into two branches whose goal is to create\ntwo pandas data frames—one holding the relevant ground-truth data, and the\nother the output of an information extraction tool. For this purpose, both the\nground-truth ﬁles and the output ﬁles of IE tools are parsed and ﬁltered for\nA Benchmark of PDF Information Extraction Tools 11\nthe relevant content elements. For example, to evaluate reference extraction via\nCERMINE, we exclusively parse reference tags from CERMINE’s XML output\nﬁle into a data frame (see Extracted DF in Figure 3).\nFinally, we convert both the ground-truth data frameand the extracted data\nframe into two formats for comparison and computing performance metrics. The\nﬁrst is the separate tokensformat, in which every token is represented as a row in\nthe data frame. The second is the collated tokensformat, in which all tokens are\ncombined into a single space-delimited row in the data frame. Separate tokens\nserve to compute a strict score for token-level extraction quality, whereas collated\ntokens yield a more lenient score intended to reﬂect a tool’s average extraction\nquality for a class of content elements. We will explain the idea of both scores\nand their computation hereafter.\nWe employ the Levenshtein Ratio to quantify the similarity of extracted\ntokens and the ground-truth data for both the separate tokens and collated\ntokens format. Equation (1) deﬁnes the computation of the Levenshtein distance\nof the extracted tokens te and the ground-truth tokens tg.\nlevte,tg (i,j) =\n\n\n\nmax(i,j), if min(i,j) = 0 ,\nmin\n\n\n\nlevte,tg (i−1,j) + 1\nlevte,tg (i,j −1) + 1\nlevte,tg (i−1,j −1) + 1(tei ̸=tej )\notherwise .\n(1)\nEquation (2) deﬁnes the derived Levenshtein Ratio score ( γ).\nγ(te,tg) = 1 −levte,tg (i,j)\n|te|+ |tg| (2)\nEquation (3) shows the derivation of the similarity matrix (∆d) for a doc-\nument (d), which contains the Levenshtein Ratio ( γ) of every token in the ex-\ntracted data frame with separate tokens Es of size mand the ground-truth data\nframe with separate tokens Gs of size n.\n∆d\nm×n = γ\n[\nEs\ni ,Gs\nj\n]m,n\ni,j (3)\nUsing the m×n similarity matrix, we compute the Precision Pd and Re-\ncall Rd scores according to Equation (4) and Equation (5), respectively. As the\nnumerator, we use the number of extracted tokens whose Levenshtein Ratio is\nlarger or equal to 0.7. We chose this threshold for consistency with the exper-\niments by Granitzer et al. [19]. We then compute the Fd\n1 score according to\nEquation (6) as a token-level score for a tool’s extraction quality.\nPd = #∆d\ni,j ≥0.7\nm (4)\nRd = #∆d\ni,j ≥0.7\nn (5)\n12 Meuschke et al.\nF1\nd = 2 ×Pd ×Rd\nPd + Rd (6)\nMoreover, we compute the Accuracy score Ad reﬂecting a tool’s average ex-\ntraction quality for a class of tokens. To obtain Ad, we compute the Levenshtein\nRatio γ of the extracted tokens Ec and ground-truth tokens Gc in the collated\ntokens format, according to Equation (7).\nAd = γ[Ec,Gc] (7)\nFigure 4 and Figure 5 show the similarity matrices for the author names\n’Yuta,’ ’Hamada,’ ’Gary,’ and ’Shiu’ using separate and collated tokens, respec-\ntively. Figure 4 additionally shows an example computation of the Levenshtein\nRatio for the strings Gary and Yuta. The strings have a Levenshtein distance of\nsix and a cumulative string length of eight, which results in a Levenshtein Ratio\nof 0.25 that is entered into the similarity matrix. Figure 5 analogously exempliﬁes\ncomputing the Accuracy score of the two strings using collated tokens.\nYuta Hamada Gary Shiu1,Yuta 1.0 0.2 0.25 0.2Hamada 0.2 1.0 0.2 0.0Gary 0.25 0.2 1.0 0.0Shiu 0.25 0.0 0.0 0.8Y u t a0 1 2 3 4G1 2 3 4 5a2 3 4 5 4r3 4 5 6 5y4 3 4 5 6\nFig. 4: Left: Similarity matrix for author names using separate tokens.\nRight: Computation of the Levenshtein distance (6) and the optimal edit tran-\nscript (yellow highlights) for two author names using dynamic programming.\nYuta Hamada Gary Shiu1,Yuta Hamada Gary Shiu 0.957\nFig. 5: Similarity matrix for two sets of author names using collated tokens.\nA Benchmark of PDF Information Extraction Tools 13\n4 Results\nWe present the evaluation results grouped by extraction task (see Figures 6–9)\nand by tools (see Table 6). This two-fold breakdown of the results facilitates\nidentifying the best-performing tool for a speciﬁc extraction task or content\nelement and allows for gauging the strengths and weaknesses of tools more easily.\nNote that the task-speciﬁc result visualizations (Figures 6–9) only include tools\nthat support the respective extraction task. See Table 4 for an overview of the\nevaluated tools and the extraction tasks they support.\nFigure 6 shows the cumulative F1 scores of CERMINE, GROBID, PdfAct,\nand Science Parse for the metadata extraction task, i.e., extracting title, abstract,\nand authors. Consequently, the best possible cumulative F1 score equals three.\nOverall, GROBID performs best, achieving a cumulative F1 score of 2.25 and\nindividual F1 scores of 0.91 for title, 0.82 for abstract, and 0.52 for authors.\nScience Parse (2.03) and CERMINE (1.97) obtain comparable cumulative F1\nscores, while PdfAct has the lowest cumulativeF1 score of 1.14. However, PdfAct\nperforms second-best for title extraction with aF1 score of 0.85. The performance\nof all tools is worse for extracting authors than for titles and abstracts. It appears\nthat machine-learning-based approaches like those of CERMINE, GROBID, and\nScience Parse perform better for metadata extraction than rule-based algorithms\nlike the one implemented in PdfAct 40.\nFigure 7 shows the results for the reference extraction task. With aF1 score of\n0.79, GROBID also performs best for this task. CERMINE achieves the second\nrank with a F1 score of 0.74, while Science Parse and RefExtract share the\nthird rank with identical F1 scores of 0.49. As for the metadata extraction task,\nPdfAct also achieves the lowest F1 score of 0.15 for reference extraction. While\nboth RefExtract and PdfAct employ pdftotext and regular expressions, GROBID\nperforms eﬃcient segregation of cascaded sequence labeling models 41 for diverse\ncomponents, which can be the reason for its superior performance [36].\nFigure 8 depicts the results for the table extraction task. Adobe Extract\noutperforms the other tools with a F1 score of 0.47. Camelot (F1 = 0.30), Tabula\n(F1 = 0 .28), and GROBID ( F1 = 0 .23) perform notably worse than Adobe\nExtract. Both Camelot and Tabula incorrectly treat two-column articles as tables\nand table captions as a part of the table region, which negatively aﬀects their\nperformance scores. The use of comparableStream and Lattice modes in Camelot\nand Tabula (cf. Section 3.1) likely cause the tools’ highly similar results. PdfAct\ndid not produce an output for any of our test documents that contain tables,\nalthough the tool supposedly supports table extraction. The performance of all\ntools is signiﬁcantly lower for table extraction than for other content elements,\nwhich is likely caused by the need to extract additional structural information.\nThe diﬃculty of table extraction is also reﬂected by numerous issues that users\nopened on the matter in the GROBID GitHub repository 42.\n40 See Table 4 for more information on the tools’ extraction approaches.\n41 https://grobid.readthedocs.io/en/latest/Principles/\n42 https://github.com/kermitt2/grobid/issues/340\n14 Meuschke et al.\nCERMINE GROBID PdfAct Science Parse\nTitle 0.81 0.91 0.85 0.70\nAbstract 0.72 0.82 0.16 0.81\nAuthors 0.44 0.52 0.13 0.52\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0Cumulative F1 Score\n1.97\n2.25\n1.14\n2.03\nFig. 6: Results for metadata extraction.\nCERMINE GROBID PdfAct Science\n Parse RefExtract\nReference 0.74 0.79 0.15 0.49 0.49\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0F1 Score\nFig. 7: Results for reference extraction.\nA Benchmark of PDF Information Extraction Tools 15\nAdobe \n Extract Camelot GROBID PdfAct T abula\nT able 0.47 0.30 0.23 0.00 0.28\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0F1 Score\nFig. 8: Results for table extraction.\nFigure 9 visualizes the results for the general extraction task. GROBID\nachieves the highest cumulativeF1 score of 2.38, followed by PdfAct (cumulative\nF1 = 1.66). The cumulative F1 scores of Science Parse (1.25), which only support\nparagraph and section extraction, and CERMINE (1.20) are much lower than\nGROBID’s score and comparable to that of PdfAct. Apache Tika, PyMuPDF,\nand Adobe Extract can only extract paragraphs.\nFor paragraph extraction, GROBID (0.9), CERMINE (0.85), and PdfAct\n(0.85) obtained high F1 scores with Science Parse (0.76) and Adobe Extract\n(0.74) following closely. Apache Tika (0.52) and PyMuPDF (0.51) achieved no-\ntably lower scores because the tools include other elements like sections, captions,\nlists, footers, and equations in paragraphs.\nNotably, only GROBID achieves a promising F1 score of 0.74 for the ex-\ntraction of sections. GROBID and PdfAct are the only tools that can partially\nextract captions. None of the tools is able to extract lists. Only PdfAct supports\nthe extraction of footers but achieves a low F1 score of 0.20. Only GROBID\nsupports equation extraction but the extraction quality is comparatively low\n(F1 = 0 .25). To reduce the evaluation eﬀort, we ﬁrst tested the extraction of\nlists, footers, and equations on a two-months sample of the data covering Jan-\nuary and February 2014. If a tool consistently obtained performance scores of\n0, we did not continue with its evaluation. Following this procedure, we only\nevaluated GROBID and PdfAct on the full dataset.\n16 Meuschke et al.\nFor the general extraction task, GROBID outperforms other tools due to\nits segmentation model43, which detects the main areas of documents based on\nlayout features. Therefore, frequent content elements like paragraphs will not\nimpact the extraction of rare elements from a non-body area by keeping the\nimbalanced classes in separate models. The cascading models used in GROBID\nalso oﬀer the ﬂexibility to tune each model. Using layouts and structures as a\nbasis for the process allows the association of simpler training data.\nAdobe \n Extract\nApache \n Tika CERMINE GROBID PdfAct PyMuPDF Science\n Parse\nParagraph 0.74 0.52 0.85 0.90 0.85 0.51 0.76\nSection 0.00 0.00 0.35 0.74 0.16 0.00 0.49\nCaption 0.00 0.00 0.00 0.49 0.45 0.00 0.00\nList 0.00 0.00 0.00 0.00 0.00 0.00 0.00\nFooter 0.00 0.00 0.00 0.00 0.20 0.00 0.00\nEquation 0.00 0.00 0.00 0.25 0.00 0.00 0.00\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0Cumulative F1 Score0.74\n0.52\n1.2\n2.38\n1.66\n0.51\n1.25\nFig. 9: Results for general data extraction.\nThe breakdown of results by tools shown in Table 6 underscores the main\ntakeaway point of the results’ presentation for the individual extraction tasks.\nThe tools’ results diﬀer greatly for diﬀerent content elements. Certainly, no tool\nperforms best for all elements, rather, even tools that perform well overall can\nfail completely for certain extraction tasks. The large amount of content elements\nwhose extraction is either unsupported or only possible in poor quality indicates\na large potential for improvement in future work.\n43 https://grobid.readthedocs.io/en/latest/Principles/\nA Benchmark of PDF Information Extraction Tools 17\nTable 6: Results grouped by extraction tool.\nTool1 Label # De-\ntected\n# Pro-\ncessed2 Acc F 1 P R\nAdobe Extract Table 1,635 736 0.52 0.47 0.45 0.49\nParagraph 3,985 3,088 0.85 0.74 0.72 0.76\nApache Tika Paragraph 339,603 258,582 0.55 0.52 0.43 0.65\nCamelot Table 16,289 11,628 0.27 0.30 0.23 0.44\nCERMINE Title 16,196 14,501 0.84 0.81 0.81 0.81\nAuthor 19,788 14,797 0.43 0.44 0.44 0.46\nAbstract 19,342 16,716 0.71 0.72 0.68 0.76\nReference 40,333 35,193 0.80 0.74 0.71 0.77\nParagraph 361,273 348,160 0.89 0.85 0.83 0.87\nSection 163,077 139,921 0.40 0.35 0.32 0.38\nGROBID Title 16,196 16,018 0.92 0.91 0.91 0.92\nAuthor 19,788 19,563 0.54 0.52 0.52 0.53\nAbstract 19,342 18,714 0.82 0.82 0.81 0.83\nReference 40,333 36,020 0.82 0.79 0.79 0.80\nParagraph 361,273 358,730 0.90 0.90 0.89 0.91\nSection 163,077 163,037 0.77 0.74 0.73 0.76\nCaption 90,606 62,445 0.57 0.49 0.47 0.51\nTable 16,740 8,633 0.24 0.23 0.23 0.23\nEquation 142,736 96,560 0.26 0.25 0.20 0.32\nPdfAct Title 17,670 16,834 0.85 0.85 0.85 0.86\nAuthor 13,110 2,187 0.14 0.13 0.12 0.18\nAbstract 21,470 4,683 0.17 0.16 0.15 0.20\nReference 30,263 12,705 0.19 0.15 0.17 0.20\nParagraph 361,318 357,905 0.85 0.85 0.80 0.89\nSection 129,361 87,605 0.21 0.16 0.12 0.25\nCaption 83,435 53,314 0.45 0.45 0.40 0.52\nFooter 32,457 26,252 0.23 0.20 0.25 0.16\nPyMuPDF Paragraph 339,650 258,383 0.55 0.51 0.41 0.65\nRefExtract Reference 40,333 38,405 0.55 0.49 0.44 0.55\nScience Parse Title 11,696 11,687 0.79 0.70 0.70 0.70\nAuthor 471 471 0.54 0.52 0.52 0.53\nAbstract 14,150 14,149 0.83 0.81 0.73 0.90\nReference 40,333 35,200 0.55 0.49 0.49 0.50\nParagraph 361,318 355,529 0.79 0.76 0.76 0.76\nSection 163,077 158,556 0.54 0.49 0.49 0.50\nTabula Table 10,361 9,456 0.29 0.28 0.20 0.46\n1 Boldface indicates the best value for each content element type.\n2 The diﬀerences in the number of detected and processed items are due to\nPDF Read Exceptions or Warnings. We label an item as processed if it has\na non-zero F1 score.\n18 Meuschke et al.\n5 Conclusion and Future Work\nWe present an open evaluation framework for information extraction from aca-\ndemic PDF documents. Our framework uses the DocBank dataset [31] oﬀering\n12 types and 1.5M annotated instances of content elements contained in 500K\npages of arXiv papers from multiple disciplines. The dataset is larger, more top-\nically diverse, and supports more extraction tasks than most related datasets.\nWe use the newly developed framework to benchmark the performance of ten\nfreely available tools in extracting document metadata, bibliographic references,\ntables, and other content elements in academic PDF documents. GROBID, fol-\nlowed by CERMINE and Science Parse achieves the best results for the metadata\nand reference extraction tasks. For table extraction, Adobe Extract outperforms\nother tools, even though the performance is much lower than for other content\nelements. All tools struggle to extract lists, footers, and equations.\nWhile DocBank covers more disciplines than other datasets, we see further\ndiversiﬁcation of the collection in terms of disciplines, document types, and con-\ntent elements as a valuable task for future research. Table 2 shows that more\ndatasets suitable for information extraction from PDF documents are available\nbut unused thus far. The weakly supervised annotation approach used for creat-\ning the DocBank dataset is transferable to other LaTeX document collections.\nApart from the dataset, our framework can incorporate additional tools and\nallows easy replacement of tools in case of updates. We intend to update and\nextend our performance benchmark in the future.\nThe extraction of tables, equations, footers, lists, and similar content ele-\nments poses the toughest challenge for tools in our benchmark. In recent work,\nGrennan et al.[20] showed that the usage of synthetic datasets for model train-\ning can improve citation parsing. A similar approach could also be a promising\ndirection for improving the access to currently hard-to-extract content elements.\nCombining extraction approaches could lead to a one-ﬁts-all extraction tool,\nwhich we consider desirable. The Sciencebeam-pipelines 44 project currently un-\ndertakes initial steps toward that goal. We hope that our evaluation framework\nwill help to support this line of research by facilitating performance benchmarks\nof IE tools as part of a continuous development and integration process.\nReferences\n1. Ahmed, M.W., Afzal, M.T.: FLAG-PDFe: Features Oriented Metadata Extraction\nFramework for Scientiﬁc Publications. IEEE Access 8, 99458–99469 (May 2020).\nhttps://doi.org/10.1109/ACCESS.2020.2997907\n2. Aiello, M., Monz, C., Todoran, L., Worring, M.: Document Understanding for\na Broad Class of Documents. International Journal on Document Analysis and\nRecognition 5(1) (Aug 2002). https://doi.org/10.1007/s10032-002-0080-x\n3. Anzaroot, S., Passos, A., Belanger, D., McCallum, A.: Learning Soft Linear Con-\nstraints with Application to Citation Field Extraction. In: Proceedings of the 52nd\n44 https://github.com/elifesciences/sciencebeam-pipelines\nA Benchmark of PDF Information Extraction Tools 19\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers). pp. 593–602. Association for Computational Linguistics, Baltimore, Mary-\nland (2014). https://doi.org/10.3115/v1/P14-1056\n4. Azimjonov, J., Alikhanov, J.: Rule Based Metadata Extraction Framework\nfrom Academic Articles. arXiv CoRR 1807.09009v1 [cs.IR], 1–10 (2018).\nhttps://doi.org/10.48550/arXiv.1807.09009\n5. Bast, H., Korzen, C.: The Icecite Research Paper Management System. In: Web\nInformation Systems Engineering – WISE 2013, vol. 8181, pp. 396–409. Springer\nBerlin, Heidelberg, Nanjing, China (2013). https://doi.org/10.1007/978-3-642-\n41154-0 30\n6. Bast, H., Korzen, C.: A Benchmark and Evaluation for Text Extraction from PDF.\nIn: 2017 ACM/IEEE Joint Conference on Digital Libraries (JCDL). pp. 1–10.\nIEEE, Toronto, ON, Canada (2017). https://doi.org/10.1109/JCDL.2017.7991564\n7. Bhardwaj, A., Mercier, D., Dengel, A., Ahmed, S.: DeepBIBX: Deep Learning\nfor Image Based Bibliographic Data Extraction. In: Proceedings of the 24th In-\nternational Conference on Neural Information Processing. LNCS, vol. 10635, pp.\n286–293. Springer, Guangzhou, China (2017). https://doi.org/10.1007/978-3-319-\n70096-0 30\n8. Borkar, V., Deshmukh, K., Sarawagi, S.: Automatic Segmentation of Text\ninto Structured Records. SIGMOD Record 30(2), 175–186 (Jun 2001).\nhttps://doi.org/10.1145/376284.375682\n9. Clark, C., Divvala, S.: PDFFigures 2.0: Mining Figures from Research Papers. In:\nProceedings of the 16th ACM/IEEE-CS on Joint Conference on Digital Libraries.\npp. 143–152. JCDL ’16, Association for Computing Machinery, New York, NY,\nUSA (2016). https://doi.org/10.1145/2910896.2910904\n10. Cortez, E., da Silva, A.S., Gon¸ calves, M.A., Mesquita, F., de Moura, E.S.: FLUX-\nCIM: Flexible Unsupervised Extraction of Citation Metadata. In: Proceedings\nof the 7th ACM/IEEE-CS Joint Conference on Digital Libraries. pp. 215–224.\nJCDL ’07, Association for Computing Machinery, New York, NY, USA (2007).\nhttps://doi.org/10.1145/1255175.1255219\n11. Cortez, E., da Silva, A.S., Gon¸ calves, M.A., Mesquita, F., de Moura, E.S.: A\nFlexible Approach for Extracting Metadata from Bibliographic Citations. JASIST\n60(6), 1144–1158 (Jun 2009). https://doi.org/10.1002/asi.21049\n12. Councill, I., Giles, C.L., Kan, M.Y.: ParsCit: an Open-source CRF Reference\nString Parsing Package. In: Proceedings of the Sixth International Conference on\nLanguage Resources and Evaluation. European Language Resources Association,\nMarrakech, Morocco (2008), https://aclanthology.org/L08-1291/\n13. Cui, B.G., Chen, X.: An Improved Hidden Markov Model for Literature Meta-\ndata Extraction. In: Advanced Intelligent Computing Theories and Applications,\nvol. 6215, pp. 205–212. Springer Berlin Heidelberg, Changsha, China (2010).\nhttps://doi.org/10.1007/978-3-642-14922-1 26\n14. Day, M.Y., Tsai, R.T.H., Sung, C.L., Hsieh, C.C., Lee, C.W., Wu, S.H., Wu, K.P.,\nOng, C.S., Hsu, W.L.: Reference metadata extraction using a hierarchical knowl-\nedge representation framework. Decision Support Systems 43(1), 152–167 (Feb\n2007). https://doi.org/10.1016/j.dss.2006.08.006\n15. De La Torre, M., Aguirre, C., Anshutz, B., Hsu, W.: MATESC: Metadata-analytic\ntext extractor and section classiﬁer for scientiﬁc publications. In: Proceedings of\nthe 10th International Joint Conference on Knowledge Discovery, Knowledge En-\ngineering and Knowledge Management. vol. 1, pp. 261–267. SciTePress (2018).\nhttps://doi.org/10.5220/0006937702610267\n20 Meuschke et al.\n16. Fan, T., Liu, J., Qiu, Y., Jiang, C., Zhang, J., Zhang, W., Wan, J.: PARDA: A\nDataset for Scholarly PDF Document Metadata Extraction Evaluation. In: Col-\nlaborative Computing: Networking, Applications and Worksharing. pp. 417–431.\nSpringer, Cham (2019). https://doi.org/10.1007/978-3-030-12981-1 29\n17. F¨ arber, M., Thiemann, A., Jatowt, A.: A High-Quality Gold Standard for Citation-\nbased Tasks. In: Proceedings of the Eleventh International Conference on Language\nResources and Evaluation. European Language Resources Association, Miyazaki,\nJapan (2018), https://aclanthology.org/L18-1296\n18. Giuﬀrida, G., Shek, E.C., Yang, J.: Knowledge-Based Metadata Extraction from\nPostScript Files. In: Proceedings of the Fifth ACM Conference on Digital Libraries.\npp. 77–84. DL ’00, Association for Computing Machinery, New York, NY, USA\n(2000). https://doi.org/10.1145/336597.336639\n19. Granitzer, M., Hristakeva, M., Jack, K., Knight, R.: A Comparison of Metadata\nExtraction Techniques for Crowdsourced Bibliographic Metadata Management. In:\nProceedings of the 27th Annual ACM Symposium on Applied Computing. pp. 962–\n964. SAC ’12, Association for Computing Machinery, New York, NY, USA (2012).\nhttps://doi.org/10.1145/2245276.2245462\n20. Grennan, M., Beel, J.: Synthetic vs. Real Reference Strings for Citation Pars-\ning, and the Importance of Re-training and Out-Of-Sample Data for Meaning-\nful Evaluations: Experiments with GROBID, GIANT and CORA. In: Proceed-\nings of the 8th International Workshop on Mining Scientiﬁc Publications. pp.\n27–35. Association for Computational Linguistics, Wuhan, China (2020), https:\n//aclanthology.org/2020.wosp-1.4\n21. Grennan, M., Schibel, M., Collins, A., Beel, J.: GIANT: The 1-Billion Annotated\nSynthetic Bibliographic-Reference-String Dataset for Deep Citation Parsing [Data]\n(2019). https://doi.org/10.7910/DVN/LXQXAO\n22. Hashmi, A.M., Afzal, M.T., ur Rehman, S.: Rule Based Approach to Ex-\ntract Metadata from Scientiﬁc PDF Documents. In: 2020 5th Interna-\ntional Conference on Innovative Technologies in Intelligent Systems and In-\ndustrial Applications (CITISIA). pp. 1–4. IEEE, Sydney, Australia (2020).\nhttps://doi.org/10.1109/CITISIA50690.2020.9371784\n23. Hetzner, E.: A Simple Method for Citation Metadata Extraction Using Hidden\nMarkov Models. In: Proceedings of the 8th ACM/IEEE-CS Joint Conference on\nDigital Libraries. pp. 280–284. JCDL ’08, Association for Computing Machinery,\nNew York, NY, USA (2008). https://doi.org/10.1145/1378889.1378937\n24. Kasdorf, W.E.: The Columbia Guide to Digital Publishing. Columbia University\nPress, USA (2003)\n25. Kern, R., Jack, K., Hristakeva, M.: TeamBeam - Meta-Data Extrac-\ntion from Scientiﬁc Literature. D-Lib Magazine 18(7/8) (Jul 2012).\nhttps://doi.org/10.1045/july2012-kern\n26. Klein, D., Manning, C.D.: Conditional Structure versus Conditional Estima-\ntion in NLP Models. In: Proceedings of the 2002 Conference on Empirical\nMethods in Natural Language Processing (EMNLP). pp. 9–16. Association\nfor Computational Linguistics, Pennsylvania, Philadelphia, PA, USA (2002).\nhttps://doi.org/10.3115/1118693.1118695\n27. Klink, S., Dengel, A., Kieninger, T.: Document Structure Analysis Based on Layout\nand Textual Features. In: IAPR International Workshop on Document Analysis\nSystems. IAPR, Rio de Janeiro, Brazil (2000)\n28. K¨ orner, M., Ghavimi, B., Mayr, P., Hartmann, H., Staab, S.: Evaluating Reference\nString Extraction Using Line-Based Conditional Random Fields: A Case Study\nA Benchmark of PDF Information Extraction Tools 21\nwith German Language Publications. In: New Trends in Databases and Information\nSystems. pp. 137–145. Springer, Cham (2017). https://doi.org/10.1007/978-3-319-\n67162-8 15\n29. Laﬀerty, J.D., McCallum, A., Pereira, F.C.N.: Conditional Random Fields: Prob-\nabilistic Models for Segmenting and Labeling Sequence Data. In: Proceedings\nof the Eighteenth International Conference on Machine Learning. pp. 282–289.\nICML ’01, Morgan Kaufmann Publishers Inc., San Francisco, CA, USA (2001),\nhttps://dl.acm.org/doi/10.5555/645530.655813\n30. Ley, M.: DBLP: Some Lessons Learned. Proc. VLDB Endowment 2(2), 1493–1500\n(Aug 2009). https://doi.org/10.14778/1687553.1687577\n31. Li, M., Xu, Y., Cui, L., Huang, S., Wei, F., Li, Z., Zhou, M.: DocBank: A\nBenchmark Dataset for Document Layout Analysis. In: Proceedings of the 28th\nInternational Conference on Computational Linguistics. pp. 949–960. Interna-\ntional Committee on Computational Linguistics, Barcelona, Spain (Online) (2020).\nhttps://doi.org/10.18653/v1/2020.coling-main.82\n32. Lipinski, M., Yao, K., Breitinger, C., Beel, J., Gipp, B.: Evaluation of Header\nMetadata Extraction Approaches and Tools for Scientiﬁc PDF Documents. In:\nProceedings of the 13th ACM/IEEE-CS Joint Conference on Digital Libraries. pp.\n385–386. JCDL ’13, Association for Computing Machinery, New York, NY, USA\n(2013). https://doi.org/10.1145/2467696.2467753\n33. Livathinos, N., Berrospi, C., Lysak, M., Kuropiatnyk, V., Nassar, A., Car-\nvalho, A., Dolﬁ, M., Auer, C., Dinkla, K., Staar, P.: Robust PDF Doc-\nument Conversion using Recurrent Neural Networks. Proceedings of the\nAAAI Conference on Artiﬁcial Intelligence 35(17), 15137–15145 (May 2021).\nhttps://doi.org/10.1609/aaai.v35i17.17777\n34. Lo, K., Wang, L.L., Neumann, M., Kinney, R., Weld, D.: S2ORC: The Seman-\ntic Scholar Open Research Corpus. In: Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics. pp. 4969–4983. Association for\nComputational Linguistics, Online (2020). https://doi.org/10.18653/v1/2020.acl-\nmain.447\n35. Lopez, P.: GROBID (2008), https://github.com/kermitt2/grobid\n36. Lopez, P.: GROBID: Combining Automatic Bibliographic Data Recognition and\nTerm Extraction for Scholarship Publications. In: Research and Advanced Technol-\nogy for Digital Libraries, LNCS, vol. 5714, pp. 473–474. Springer Berlin Heidelberg\n(2009). https://doi.org/10.1007/978-3-642-04346-8 62\n37. Mao, S., Kim, J., Thoma, G.R.: A Dynamic Feature Generation Sys-\ntem for Automated Metadata Extraction in Preservation of Digital Mate-\nrials. In: 1st International Workshop on Document Image Analysis for Li-\nbraries. pp. 225–232. IEEE Computer Society, Palo Alto, CA, USA (2004).\nhttps://doi.org/10.1109/DIAL.2004.1263251\n38. Mao, S., Rosenfeld, A., Kanungo, T.: Document structure analysis algorithms: a\nliterature survey. In: Proceedings Document Recognition and Retrieval X. SPIE\nProceedings, vol. 5010, pp. 197–207. SPIE, Santa Clara, California, USA (Jan\n2003). https://doi.org/10.1117/12.476326\n39. McCallum, A.K., Nigam, K., Rennie, J., Seymore, K.: Automating the Construc-\ntion of Internet Portals with Machine Learning. Information Retrieval 3(2), 127–\n163 (Jul 2000). https://doi.org/10.1023/A:1009953814988\n40. National Library of Medicine: PubMed, https://pubmed.ncbi.nlm.nih.gov/\n41. National Library of Medicine: PubMed Central, https://www.ncbi.nlm.nih.gov/\npmc/\n22 Meuschke et al.\n42. Ojokoh, B., Zhang, M., Tang, J.: A trigram hidden Markov model for metadata\nextraction from heterogeneous references. Information Sciences 181(9), 1538–1551\n(May 2011). https://doi.org/10.1016/j.ins.2011.01.014\n43. Ororbia, A.G., Wu, J., Khabsa, M., WIlliams, K., Giles, C.L.: Big Scholarly\nData in CiteSeerX: Information Extraction from the Web. In: Proceedings of\nthe 24th International Conference on World Wide Web. pp. 597–602. WWW ’15\nCompanion, Association for Computing Machinery, New York, NY, USA (2015).\nhttps://doi.org/10.1145/2740908.2741736\n44. Palmero, G., Dimitriadis, Y.: Structured document labeling and rule extraction\nusing a new recurrent fuzzy-neural system. In: Proceedings of the Fifth Interna-\ntional Conference on Document Analysis and Recognition. pp. 181–184. Springer,\nBangalore, India (1999). https://doi.org/10.1109/ICDAR.1999.791754\n45. Peng, F., McCallum, A.: Accurate Information Extraction from Research Pa-\npers using Conditional Random Fields. In: Proceedings of the Human Language\nTechnology Conference of the North American Chapter of the Association for\nComputational Linguistics: HLT-NAACL. pp. 329–336. Association for Compu-\ntational Linguistics, Boston, Massachusetts, USA (2004), https://aclanthology.\norg/N04-1042\n46. Prasad, A., Kaur, M., Kan, M.Y.: Neural ParsCit: a deep learning-based reference\nstring parser. International Journal on Digital Libraries19(4), 323–337 (Nov 2018).\nhttps://doi.org/10.1007/s00799-018-0242-1\n47. Rizvi, S.T.R., Dengel, A., Ahmed, S.: A Hybrid Approach and Uniﬁed Framework\nfor Bibliographic Reference Extraction. IEEE Access8, 217231–217245 (Dec 2020).\nhttps://doi.org/10.1109/ACCESS.2020.3042455\n48. Rizvi, S.T.R., Lucieri, A., Dengel, A., Ahmed, S.: Benchmarking Object Detection\nNetworks for Image Based Reference Detection in Document Images. In: 2019\nDigital Image Computing: Techniques and Applications (DICTA). pp. 1–8. IEEE,\nPerth, WA, Australia (2019). https://doi.org/10.1109/DICTA47822.2019.8945991\n49. Rodrigues Alves, D., Colavizza, G., Kaplan, F.: Deep Reference Mining From Schol-\narly Literature in the Arts and Humanities. Frontiers in Research Metrics and\nAnalytics 3, 21 (Jul 2018). https://doi.org/10.3389/frma.2018.00021\n50. Saier, T., F¨ arber, M.: Bibliometric-Enhanced arXiv: A Data Set for Paper-Based\nand Citation-Based Tasks. In: Proceedings of the 8th International Workshop\non Bibliometric-enhanced Information Retrieval (BIR). CEUR Workshop Pro-\nceedings, vol. 2345, pp. 14–26. CEUR-WS.org, Cologne, Germany (2019), http:\n//ceur-ws.org/Vol-2345/paper2.pdf\n51. Saier, T., F¨ arber, M.: unarXive: A Large Scholarly Data Set with Publications’\nFull-Text, Annotated In-Text Citations, and Links to Metadata. Scientometrics\n125(3), 3085–3108 (Dec 2020). https://doi.org/10.1007/s11192-020-03382-z\n52. Schloss Dagstuhl - Leibniz Center for Informatics, University of Trier: dblp: com-\nputer science bibliography, https://dblp.org/\n53. Souza, A., Moreira, V., Heuser, C.: ARCTIC: Metadata Extraction from Scientiﬁc\nPapers in Pdf Using Two-Layer CRF. In: Proceedings of the 2014 ACM Symposium\non Document Engineering. pp. 121–130. DocEng ’14, Association for Computing\nMachinery, New York, NY, USA (2014). https://doi.org/10.1145/2644866.2644872\n54. Tkaczyk, D., Collins, A., Sheridan, P., Beel, J.: Machine Learning vs. Rules and\nOut-of-the-Box vs. Retrained: An Evaluation of Open-Source Bibliographic Ref-\nerence and Citation Parsers. In: Proceedings of the 18th ACM/IEEE on Joint\nConference on Digital Libraries. pp. 99–108. JCDL ’18, Association for Computing\nMachinery, New York, NY, USA (2018). https://doi.org/10.1145/3197026.3197048\nA Benchmark of PDF Information Extraction Tools 23\n55. Tkaczyk, D., Szostek, P., Fedoryszak, M., Dendek, P.J., Bolikowski,  L.: CERMINE:\nautomatic extraction of structured metadata from scientiﬁc literature. Interna-\ntional Journal on Document Analysis and Recognition (IJDAR) 18(4), 317–335\n(Dec 2015). https://doi.org/10.1007/s10032-015-0249-8\n56. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,\nKaiser,  L., Polosukhin, I.: Attention is All You Need. In: Proceedings of the 31st\nInternational Conference on Neural Information Processing Systems. pp. 6000–\n6010. NIPS’17, Curran Associates Inc., Red Hook, NY, USA (2017), https:\n//dl.acm.org/doi/10.5555/3295222.3295349\n57. Vilnis, L., Belanger, D., Sheldon, D., McCallum, A.: Bethe Projections for Non-\nLocal Inference. In: Proceedings of the Thirty-First Conference on Uncertainty in\nArtiﬁcial Intelligence. pp. 892–901. UAI’15, AUAI Press, Arlington, Virginia, USA\n(2015). https://doi.org/10.48550/arXiv.1503.01397",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8349894285202026
    },
    {
      "name": "Metadata",
      "score": 0.7841142416000366
    },
    {
      "name": "Information retrieval",
      "score": 0.7389832139015198
    },
    {
      "name": "Header",
      "score": 0.5846496224403381
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5648557543754578
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5625386238098145
    },
    {
      "name": "Search engine indexing",
      "score": 0.5433877110481262
    },
    {
      "name": "Table of contents",
      "score": 0.4898671805858612
    },
    {
      "name": "Task (project management)",
      "score": 0.48843589425086975
    },
    {
      "name": "Parsing",
      "score": 0.4863174855709076
    },
    {
      "name": "Table (database)",
      "score": 0.4218376576900482
    },
    {
      "name": "Data mining",
      "score": 0.32534700632095337
    },
    {
      "name": "World Wide Web",
      "score": 0.2953985929489136
    },
    {
      "name": "Natural language processing",
      "score": 0.155466228723526
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Computer network",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I74656192",
      "name": "University of Göttingen",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I186354981",
      "name": "University of Passau",
      "country": "DE"
    }
  ]
}