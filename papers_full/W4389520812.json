{
  "title": "This is not a Dataset: A Large Negation Benchmark to Challenge Large Language Models",
  "url": "https://openalex.org/W4389520812",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2936307295",
      "name": "Iker García Ferrero",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2510050614",
      "name": "Begoña Altuna",
      "affiliations": [
        "University of the Basque Country"
      ]
    },
    {
      "id": "https://openalex.org/A2133898147",
      "name": "Javier Álvez",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2726363827",
      "name": "Itziar González-Dios",
      "affiliations": [
        "University of the Basque Country"
      ]
    },
    {
      "id": "https://openalex.org/A1596863116",
      "name": "German Rigau",
      "affiliations": [
        "University of the Basque Country"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4285309087",
    "https://openalex.org/W4378509449",
    "https://openalex.org/W2038721957",
    "https://openalex.org/W4404783524",
    "https://openalex.org/W3099982175",
    "https://openalex.org/W3103291281",
    "https://openalex.org/W2214974779",
    "https://openalex.org/W138707325",
    "https://openalex.org/W2251956625",
    "https://openalex.org/W2964117978",
    "https://openalex.org/W2963641561",
    "https://openalex.org/W3099843385",
    "https://openalex.org/W2064917215",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4240734474",
    "https://openalex.org/W1579321212",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W4361807044",
    "https://openalex.org/W4292963003",
    "https://openalex.org/W4362655426",
    "https://openalex.org/W3190301637",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4385570140",
    "https://openalex.org/W4292119927",
    "https://openalex.org/W4367000491",
    "https://openalex.org/W2996556191",
    "https://openalex.org/W2139865360",
    "https://openalex.org/W3034995113",
    "https://openalex.org/W4322718191"
  ],
  "abstract": "Although large language models (LLMs) have apparently acquired a certain level of grammatical knowledge and the ability to make generalizations, they fail to interpret negation, a crucial step in Natural Language Processing. We try to clarify the reasons for the sub-optimal performance of LLMs understanding negation. We introduce a large semi-automatically generated dataset of circa 400,000 descriptive sentences about commonsense knowledge that can be true or false in which negation is present in about 2/3 of the corpus in different forms. We have used our dataset with the largest available open LLMs in a zero-shot approach to grasp their generalization and inference capability and we have also fine-tuned some of the models to assess whether the understanding of negation can be trained. Our findings show that, while LLMs are proficient at classifying affirmative sentences, they struggle with negative sentences and lack a deep understanding of negation, often relying on superficial cues. Although fine-tuning the models on negative sentences improves their performance, the lack of generalization in handling negation is persistent, highlighting the ongoing challenges of LLMs regarding negation understanding and generalization. The dataset and code are publicly available.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 8596–8615\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nThis is not a Dataset:\nA Large Negation Benchmark to Challenge Large Language Models\nIker García-Ferrero1 , Begoña Altuna1 , Javier Álvez2\nItziar Gonzalez-Dios1 , German Rigau1\n1 HiTZ Center - Ixa, University of the Basque Country UPV/EHU\n2LoRea Group, University of the Basque Country UPV/EHU\n{iker.garciaf,begona.altuna,javier.alvez}@ehu.eus\n{itziar.gonzalezd,german.rigau}@ehu.eus\nAbstract\nAlthough large language models (LLMs) have\napparently acquired a certain level of grammat-\nical knowledge and the ability to make general-\nizations, they fail to interpret negation, a crucial\nstep in Natural Language Processing. We try to\nclarify the reasons for the sub-optimal perfor-\nmance of LLMs understanding negation. We\nintroduce a large semi-automatically generated\ndataset of circa 400,000 descriptive sentences\nabout commonsense knowledge that can be true\nor false in which negation is present in about\n2/3 of the corpus in different forms. We have\nused our dataset with the largest available open\nLLMs in a zero-shot approach to grasp their\ngeneralization and inference capability and we\nhave also fine-tuned some of the models to as-\nsess whether the understanding of negation can\nbe trained. Our findings show that, while LLMs\nare proficient at classifying affirmative sen-\ntences, they struggle with negative sentences\nand lack a deep understanding of negation, of-\nten relying on superficial cues. Although fine-\ntuning the models on negative sentences im-\nproves their performance, the lack of general-\nization in handling negation is persistent, high-\nlighting the ongoing challenges of LLMs re-\ngarding negation understanding and generaliza-\ntion. The dataset and code are publicly avail-\nable: https://github.com/hitz-zentroa/\nThis-is-not-a-Dataset\n1 Introduction\nLarge Language Models (LLMs) currently offer\nstate of the art performance in many Natural Lan-\nguage Processing (NLP) tasks. Apparently, they\nhave acquired the ability to capture syntactic (Ba-\nroni, 2020) and semantic (Furrer et al., 2021) ab-\nstractions. However, recent experiments (Kassner\nand Schütze, 2020; Hossain et al., 2020; Truong\net al., 2022) have proven that LLMs fail at inter-\npreting contexts in which understanding negation\nis required.\nBills are commonly part of birds. ✅\nBills are never part of human bodies. ✅\nBills are never part of birds.  ❌\nBills are commonly part of human bodies. ❌\nFigure 1: Affirmative and negative sentences in the\ndataset.\nThe presence of negation in a sentence reverts\nthe polarity of the proposition it represents, and\nthus affects its truth and factuality values. See how\nthe adverb “never” changes the truth value of the\nsentences in Figure 1. As a consequence, under-\nstanding negation correctly is crucial for all NLP\ntasks. Moreover, understanding negation should\nhelp LLMs to grasp how things happen in real-\nity, boosting NLP tasks that involve commonsense,\ncausality, entailment and world knowledge.\nThe reasons for the lower capabilities of LLMs\ndealing with negation remain largely unclear, al-\nthough some point out at the under-representation\nof negation in corpora (Hossain et al., 2022). In\nthis work, we present a corpus in which negation\nis present in around two thirds of the sentences in\ndifferent forms. Taking advantage of the relations\nin WordNet (Fellbaum, 1998), we have generated\na set of patterns to create descriptive sentences that\nwork as truth and falsity tests which are then used\ntogether with a list of prompts to measure the sen-\ntence understanding of the different LLMs.\nThe dataset has been used in a series of experi-\nments to test its quality and coherence. First, we\nassess the quality of the sentences by human anno-\n8596\ntators. Then, to grasp its capacity of generalization\nand inference, we have used our dataset to test dif-\nferent configurations of LLMs available in a zero-\nshot approach. We have also fine-tuned some of\nthese models to assess whether the understanding\nof negation can be learnt. Our initial hypothesis is\nthat if the dataset is coherently and robustly built\nwe will be able to learn how LLMs deal with nega-\ntion.\nThe contributions of this paper are: i) We in-\ntroduce the largest negation probing dataset. This\ndataset includes affirmative and negative sentences\nwith and without distractors, incorporating multi-\nple types of relations and negations. ii) We evalu-\nate a comprehensive set of open LLMs using our\ndataset in both zero-shot and fine-tuning scenarios.\niii) Our findings demonstrate that current LLMs,\nwhether in zero-shot settings or after fine-tuning\nwith examples from our dataset, possess a profound\nunderstanding of the truthfulness of affirmative sen-\ntences. However, when confronted with negation,\nthese models heavily rely on superficial cues in-\nstead of effectively generalizing negation.\n2 Background\n2.1 Related Works\nNegation is a core operator in logic and in the struc-\nturing of the information in text and it has long\nbeen studied for its relevance in natural language\nunderstanding. In the last two decades, works on\nthe analysis and processing of negation have multi-\nplied. In the pre-generative-model era, most works\ncentered on negation detection (Chapman et al.,\n2001; Vilares et al., 2015) and profiling (Morante\nand Daelemans, 2012), so the extracted negation\ninformation could be used in downstream tasks.\nWith the booming of deep-learning architectures\nthat were based on abstract neural representations\nof texts, the paradigm shifted and negation was\nprocessed as the rest of the elements appearing in\ntext. It was soon noticed that systems struggled\nto correctly process the information when nega-\ntion was involved. Such is the case of negation in\nmachine translation (Bentivogli et al., 2016; Tang\net al., 2021), information extraction (Grivas et al.,\n2020) and sentiment analysis (Barnes et al., 2021)\namong others.\nIt has not been long since the scholar commu-\nnity started to analyse the reasons for the lack of\ncapability of correctly processing negation. For\nexample, Jumelet and Hupkes (2018) analysed the\nnegation licensing strategies to measure neural lan-\nguage model ability to correctly process them.\nChen et al. (2023) assess the ability of LLMs to\nhandle negative commonsense knowledge. Since\nmost available information exists in a positive\nand affirmative form, LLMs fail at dealing with\nworld knowledge when it is presented in a nega-\ntive form. They propose a two-task assessment\nin which LLMs need to i) answer yes or no to\nworld knowledge questions and ii) generate com-\nmonsense compelling sentences from related key-\nwords. Some recent research has been directed to\nbuilding knowledge bases in which negative com-\nmonsense is stored (Arnaout et al., 2022), in order\nto be reused for commonsense reasoning.\n2.2 Negation in English\nNegation in language is the representation of the\nlogical operation in which a the truth value of a\nproposition is inverted. It is commonly expressed\nby a restricted list of negative adverbs (e.g. no,\nnever), pronouns, determiners or prefixes, that ap-\npear in different contexts in the sentence. Pullum\net al. (2002) offer a four axe classification of nega-\ntion types from which we will focus on these:\n• Verbal vs. non-verbal: the verbal negation\nmarker is associated with the verb and directly\naffects it, while the non-verbal couples with\nobjects and adjuncts.\n• Analytic vs. synthetic : analytic negation is\nrepresented by markers that only convey nega-\ntion. Synthetic negation markers, instead, may\nhave additional syntactic function (e.g. noth-\ning and none might also be subjects or ob-\njects).\n• Clausal vs. sub-clausal : clausal negation\nnegates the whole clause that includes it, and\nsub-clausal negation only affects a part of the\nclause.\nIn Table 1 we present the different types of nega-\ntions considered in our work.\n3 Dataset\n3.1 Dataset construction\nOur benchmark compiles 381,300 artificially gener-\nated sentences in standard English. The sentences,\nin a definition style (e.g. X is Y , X is part of Y),\nhave been created based on the knowledge from\nWordNet and related resources.\n8597\nNegation\ntype Example\nVerbal Agreement is not an appropriate syn-\nonym of disagreement in any context.\nNon-verbal In no context lectures may be part of\ncourses.\nAnalytic No theft is a small replica of a person.\nSynthetic Mirror is never an appropriate hy-\nponym of reduction.\nClausal Kissing is not commonly done by en-\ngineers.\nSub-clausal Bricks are made of clay in no context\nTable 1: Examples of the types of negation.\nPattern Relation Templates Triples Sentences\n#01 Synonymy 21 2,996 281,624\n#02 Antonymy 21 58 8,178\n#03 Synonymy 24 14 2,436\n#04 Antonymy 24 58 10,092\n#05 Hypernymy 24 634 60,864\n#06 Part 16 199 11,940\n#07 Substance 15 21 1,176\n#08 Member 17 11 682\n#09 Agent 2 60 240\n#10 Instrument 7 9 468\n#11 Result 27 40 3,600\nTotal - - - 381,300\nTable 2: Distribution of sentences by pattern.\nThe sentences in our dataset are obtained by\nmeans of patterns. Each of the 11 patterns (#01–\n#11) is designed for a particular relation and in-\ncludes several different templates of two types: af-\nfirmative templates, which are free of negation;\nnegative templates, which include one of the types\nof negations described in Table 1. Using triples\non the corresponding relation, these templates are\nused to create sentences by instantiation. Since\neach synset may include more than one word form\nin Core WordNet and the proposed templates in-\nclude optional and alternative parts, we obtain sev-\neral sentences from each couple of template and\ntriple. The controlled application of the different\ntemplates enables us to determine the truth-value\nof the resulting sentences. In Appendix D, we de-\nscribe each pattern in detail.\nMore specifically, we focus on the WordNet rela-\ntions synonymy, hypernymy, antonymy, meronymy\n(part, member and substance) and the semantic\nroles agent, instrument and result provided by Mor-\nphosemantic Links (Fellbaum et al., 2009). Among\nthe nouns and verbs compiled in WordNet, we con-\ncentrate exclusively on the ones provided by Core\nWordNet (Boyd-Graber et al., 2006), which is a\nlist of the most frequently used word senses that\nincludes 3,299 nouns and 1,000 verbs. In this way,\nwe discard words that are less commonly used. Fur-\nthermore, we exclude the triples on synonymy1 and\nhyponymy that relate Basic Level Concepts (BLCs)\n(Izquierdo et al., 2007), which may result too gen-\neral, and we use the mapping from WordNet to\nEuroWordNet Top Ontology (TCO) to ignore the\ntriples on the member meronymy relation and the\nagent semantic role where the noun synsets are not\nreferring to animals or persons.\nSince WordNet and the considered related re-\nsources only provide true knowledge —that is, all\nthe triples and mappings describe real relations and\nconnections— we automatically obtain false knowl-\nedge from WordNet triples usingdistractors, which\nare randomly selected words that replace the word\nsenses of a synset.2 That is, given a WordNet triple\nthat relates two synsets, from Core WordNet we\nselect a distractor to replace the word senses of one\nof the synsets and obtain a distracting triple. Apart\nfrom BLCs, for the selection of suitable distrac-\ntors we consider the lexicographer files provided\nby WordNet, which are 45 syntactic category and\nlogical groupings of word senses, and WordNet\nDomains (Bentivogli et al., 2004), which consist of\na hierarchy of 164 labels that characterize knowl-\nedge areas and to which each synset is connected.\nIn Appendix C, we provide more details about the\nselection of distractors.\nNext, we illustrate the process of construct-\ning our dataset. In Pattern #06, we have in-\ncluded the following positive and negative tem-\nplates that state semantic correspondences between\nparts and wholes on the basis of triples of the form\n⟨part, noun1, noun2⟩:\n⟨noun1+(e)s⟩ [ are commonly | may be ] part of\n⟨noun2+(e)s⟩.\n⟨noun1+(e)s⟩ are never part of ⟨noun2+(e)s⟩.\nThe positive template3 yields true sentences when\ninstantiated with true knowledge (i.e. Word-\nNet triples), while we get false sentences using\ndistracting triples. On the contrary, the nega-\ntive one yields sentences with the opposite truth-\nvalue. For example, given the WordNet triple\n1Synonymy triples are obtained by reflexivity.\n2In Patterns #01 and #02, distractors are synsets when\nusing glosses.\n3The expressions enclosed in square brackets are alterna-\ntive.\n8598\n⟨part, bill 10\nn , bird1\nn⟩, we select human body as\ndistractor for bird1\nn and get the distracting triple\n⟨part, bill 10\nn , human body⟩. Then, by instantiat-\ning the positive template using these two triples,\nwe get the sentences in the first row of Figure 1 and\nalso:\n“Bills may be part of birds.”\nThe two sentences about birds (i.e. resulting from\nthe WordNet triple) are labelled with True, while\nthe sentence about human bodies (that is, obtained\nfrom the distracting triple) is labelled with False.\nLikewise, using the same two triples for the in-\nstantiation of the negative template, we get the\nsentences in the second row of Figure 1, which are\nrespectively labelled with False and True.\nIn Table 2, we sum up some figures about the\nproposed dataset. For each pattern (first column),\nwe provide the corresponding WordNet relation\nand the number defined templates, applied Word-\nNet triples and obtained sentences respectively.\nIt is worth noting that Patterns #01–#04 include\nboth false positive sentences and true negative sen-\ntences obtained from synonymy and antonymy\nWordNet triples by means of a dual application of\ntemplates. Furthermore, in the case of antonymy,\nthe truth-value of the resulting sentences does not\ndepend on whether templates are instantiated us-\ning WordNet or distracting triples. As a conse-\nquence, instantiating a template using a WordNet\nand a distracting triple yields sentences with op-\nposite truth-value except for Patterns #02 and #04,\nwhere all sentences resulting from the same tem-\nplate have the same truth-value. For example, given\nthe antonym triple ⟨ant, expenditure1\nn, income1\nn⟩,\nwe select wood as distractor for expenditure1\nn (see\nAppendix C for details) and obtain the distract-\ning triple ⟨ant, wood,income1\nn⟩. Using these two\ntemplates, we instantiate the following negative\ntemplate included in Pattern #04\n⟨noun1⟩ and ⟨noun2⟩ are the same thing in no\ncontext.\nand obtain two sentences:\n“Expenditure and income are the same thing in no\ncontext.”\n“Wood and income are the same thing in no\ncontext.”\nBoth sentences are true.\n% A tester B tester A ∩B A ∪B\nT/F prediction 90.9 89.1 87.27 96.36\nComprehensibility 91.82 100 91.82 100\nGrammaticality 83.64 96.82 83.64 96.82\nPlausibility 20.45 45.91 19.55 46.82\nTable 3: Human evaluation of the quality of the sen-\ntences in the dataset.\n3.2 Dataset quality assessment\nHuman Evaluation addresses the validation of the\ngeneration process and the different templates used,\nthat is to say, whether the sentences in the dataset\nare grammatical and that overall represent true and\nfalse knowledge as expected. To prove the linguis-\ntic quality of the dataset and that the predictions\nextracted from WordNet reflect the reality, two na-\ntive speakers of English were required to assess a\nrandomly selected sample of 220 sentences from\nour dataset. Evaluators were required to answer\nfour questions for each sentence: i) Is the sentence\ntrue or false?, ii) is the sentence grammatically cor-\nrect?, iii) is the sentence understandable? and iv) is\nthe sentence plausible and might be produced by a\nspeaker?\nThe answers to these questions have been sum-\nmarised in Table 3. For the true and false predic-\ntions, we have compared the testers’ answers with\nthe predictions we generated from the WordNet\nrelations. Circa 90% of the predictions match with\nthe human testers’ answers. For the quality of the\ntest sentences, the results show that the sentences in\nthe dataset are mostly comprehensible to humans\neven if not all are fully acceptable in English or\nthey are not likely to be uttered by English speak-\ners (low plausibility). Namely, we have detected\nproblems with uncountable nouns (1) and lexical\nselection (2). Nonetheless, low plausibility might\nbe an interesting asset for our experiments as em-\nploying non-frequent sentences may help to reduce\nthe effect of the reliance on lexical co-occurrences\nmodels have.\n(1) “ A letter is commonly part of a mail.”\n(2) “Officers are not members of laws in\nany context.”\nIn what refers the quality of the knowledge en-\ncoded in the dataset, we have observed that over\n98% of the sentences with distractors in the human\ntest set represent actual knowledge. We can thus\nconsider that the distractor selection mechanism is\n8599\nrobust enough.\n4 Experimental Setup\nIn this section, we define the evaluation protocol\nwe use to measure the performance of Language\nLearning Models (LLMs) on our dataset.\n4.1 Models\nWe evaluate a diverse set of LLMs, ranging in size\nfrom 7 billion parameters up to 65 billion parame-\nters. Our evaluation includes Foundation Models,\nalong with versions that have undergone additional\ninstruction-tuning and/or have been fine-tuned for\nconversation. We do not consider closed models\nwhere the data used for pretraining or even the\nmodel architecture is unknown, as drawing mean-\ningful conclusions for such systems is not possible.\nWe evaluate the following models: the 12 billion\nparameter T5 (Raffel et al., 2020) encoder-decoder\nlanguage model, as well as FLAN-T5 (Chung et al.,\n2022), an enhanced version of T5 that has been\nfine-tuned in a mixture of tasks; LLaMA (Touvron\net al., 2023) decoder-only language models with\nparameter sizes ranging from 7 billion to 65 bil-\nlion; LLaMA models that have been fine-tuned for\nspecific tasks, including Vicuna v1.1 (Chiang et al.,\n2023), which has undergone additional fine-tuning\nas a chat-assistant, and WizardLM (Xu et al., 2023),\nwhich has been fine-tuned for following instruc-\ntions; Pythia (Biderman et al., 2023) decoder-only\n12 billion parameter model; the instruction-tuning\nmodel Dolly (Conover et al., 2023); and finally we\nevaluate Falcon (Almazrouei et al., 2023) 7 and\n40 billion parameter models which are decoder-\nonly models including the instruction-following\nfine-tuned versions. We also evaluate other open\nLLMs; the full model list can be found in Appendix\nA.\n4.2 Task Formulation\nWe evaluate each sentence in the dataset individ-\nually as a binary task in which the model must\ngenerate either True or False tokens. Following\nScheurer et al. (2023), given the promptpt we com-\npute the answer A as follows:\nA =\n\n\n\nTrue if p(True |pt)\np(True |pt)+p(False |pt) > 0.5\nFalse otherwise\nWe use the following prompt as input for the\nmodels:\nIs the following statement True or False?\n{sentence}.\nWe found that models that have undergone a fine-\ntuning for conversation tend to generate an expla-\nnation instead of answering True or or False. We\nuse a slightly modified prompt that improves the\nresults: Is the following statement True or False?\nAnswer only True or False. {sentence}. Models that\nhave been fine-tuned as dialogue systems utilize\ndifferent prompts to represent a conversation, such\nas using markers like “<bot>” and “<human>”,\nor custom system initial prompts. In order to ac-\ncommodate these models, we format the input ac-\ncording to the recommendations provided by the\nauthors. Implementation details of fine-tuning and\ninference are available in Appendix B.\n4.3 Metrics\nIn our dataset, we utilize two primary metrics for\nevaluating LLMs:\nAccuracy This metric is computed using the for-\nmula acc = (TP +TN )/(TP +TN +FP +FN ).\nWe evaluate the overall accuracy at the sentence\nlevel for all the sentences in our dataset. Addition-\nally, we analyze the overall accuracy of different\nsentence types: Accuracy in Affirmative sentences,\nNegative sentences, Affirmative sentences with a\ndistractor and Negative sentences that include a\ndistractor.\nCoherence This metric aims to decouple the\nreal-world and commonsense knowledge of the\nmodel from the understanding of negative sen-\ntences. We compute two coherence scores: one for\nthe sentences without distractors (“Bills are com-\nmonly part of birds\" and “Bills are never part of\nbirds\") and another for the sentences with distrac-\ntors (“Bills are commonly part of human bodies.\"\nand \"Bills are never part of human bodies.\"). An-\nswers are deemed coherent if the affirmative and\nnegative sentences have opposite labels, regardless\nof whether the answer is correct or incorrect. How-\never, if the model predicts the same label for both\nthe affirmative and negative sentences, we consider\nthe answer incoherent. To illustrate this metric,\nconsider the sentence pair “Bills are commonly\npart of birds\" and “Bills are never part of birds\".\nBoth the answers “True/False\" and “False/True\"\nare considered coherent, whereas “True/True\" and\n“False/False\" are incoherent.\nMoreover, we calculate the overall coherence:\n8600\nModel name Model Type Coherence Accuracy\nAll W/o Distractor W/ Distractor\nAll W/o Distractor W/ Distractor Affirmation Negation Affirmation Negation\nRandom 0.5 0.9 0.8 50.0 50.2 50.1 50.0 49.9\nLLaMA13B Foundation 0.0 0.2 0.3 50.1 85.8 12.2 10.6 90.2\nLLaMA30B Foundation 0.1 0.3 0.2 52.4 84.7 29.5 30.2 68.8\nLLaMA65B Foundation 0.0 0.0 0.0 50.3 96.3 3.1 1.3 99.3\nVicuna13B Dialogue 0.2 8.8 0.6 57.8 83.1 85.1 78.0 2.6\nWizardLM30B Instruction 0.0 6.0 0.1 57.3 53.6 95.7 88.8 2.0\nPythia12B Foundation 0.0 0.1 0.0 50.1 93.8 15.2 4.0 86.7\nDolly12B Instruction 0.0 0.3 0.2 50.2 72.0 73.3 33.4 25.1\nT5-xxl Foundation 0.0 0.0 0.0 50.3 96.6 2.8 0.4 99.8\nFlan-T5-xxl Instruction 0.9 46.4 1.2 66.1 86.1 96.1 94.6 6.2\nFalcon40b Foundation 0.1 0.1 0.2 49.7 90.9 13.9 11.6 83.3\nFalcon40b-instruct Instruction 0.1 1.5 0.2 54.7 64.3 76.8 71.4 16.6\nTable 4: Zero-shot performance of various LLMs in our dataset. The best results are highlighted in bold, and scores\nthat surpass the Random baseline accuracy are underlined.\nthis happens when all the statements with and with-\nout distractors are coherent and correctly or all\nincorrectly classified. Referring to the example in\nFigure 1, we would deem the set of statements as\noverall coherent if the sentences with and without\ndistractors are coherent and all the answers are ei-\nther correct or all of them are incorrect. In the case\nof antonymy relations (Patterns #02 and #04), both\nthe distractor-carrying and non distractor-carrying\nsentences carry the same label, so we evaluate the\noverall coherence accordingly. It is important to\nnote that, for the sake of simplicity, the example\nonly contains two sentences, but coherence is ac-\ntually determined at the triple level. Triples can\ncomprise between 2 to 27 templates. So, for a\ntriple to be deemed coherent, responses to all the\ntemplates within it must be coherent. Therefore,\nthis is a very challenging metric.\nBy examining coherence in these contexts, we\ngain insights into the models’ ability to understand\nthe negation, even if the models do not have the\nreal-world knowledge to correctly label the sen-\ntences.\n5 Do LLMs understand negation?\nIn this section, we assess the performance of the\nLLMs in section 4.1 in our dataset. The evaluation\nis conducted in a zero-shot setting, meaning that\nwe evaluate the models without any fine-tuning.\nThe results of this evaluation are presented in Ta-\nble 4. Foundation models, which are trained on\nlarge amounts of unlabeled data, demonstrate an\nAll True behavior. They accurately label as True\nthe majority of affirmative sentences and negative\nW/o Distractor\nFlan-T5-xxl Affirmation Negation\n#01 Synonymy 91.19 98.04\n#02 Antonymy 96.36 25.62\n#03 Synonymy 49.76 98.47\n#04 Antonymy 82.07 21.92\nVicuna13B\n#01 Synonymy 88.69 84.88\n#02 Antonymy 71.65 4.64\n#03 Synonymy 57.86 90.05\n#04 Antonymy 75.8 12.81\nTable 5: Accuracy of Flan-T5-xxl and Vicuna13B in\nthe Synonymy and Antonymy patterns. We evaluate the\nmodels in affirmative and negative sentences without\ndistractors. Scores that surpass the Random baseline are\nindicated with underline.\nsentences with a distractor, which are True with\nthe exception of the Antonymy patterns, that form\napproximately 5% of the total sentences. How-\never, these models struggle to classify negative sen-\ntences and affirmative sentences with opposite la-\nbels. Their performance in these falls significantly\nbelow the random baseline exhibiting a total lack\nof coherence by the models.\nModels that have undergone dialogue or in-\nstruction tuning, particularly Vicuna and FlanT5,\ndemonstrate higher accuracy, instead. These mod-\nels achieve a very high accuracy in sentences with-\nout a distractor. Specifically, Flan-T5 shows coher-\nent answers for 46% of the triples. It is to be noted\nthat this is a challenging metric, as a triple may be\nuse to build up to 27 templates, and all of them\nmust be coherent for the triple to be considered\ncoherent.\n8601\nModel name Coherence Accuracy\nAll W/o Distractor W/ Distractor\nAll W/o Distractor W/ Distractor Affirmation Negation Affirmation Negation\nFlan-T5-xxl 51.8 55.4 92.9 94.1 96.5 86.7 96.1 98.0\nVicuna13B 81.2 86.4 94.2 95.7 92.7 94.4 98.1 97.2\nTable 6: Performance of Vicuna13B and Flan-T5-xxl after fine-tuning in our dataset. The best results are highlighted\nin bold, and scores that surpass the Random baseline accuracy are underlined.\nHowever, these models fail to correctly label\nnegative sentences with a distractor. We further\nanalyze the performance of Flan-T5 and Vicuna in\nnegative sentences, focusing on the Synonymy and\nAntonymy patterns. In Pattern #01 and #02, as well\nas Pattern #03 and #04, the templates are opposite\nto each other, as explained in Subsection 3.1. Table\n5 presents the performance of Flan-T5 and Vicuna\nin handling these patterns. Interestingly, both mod-\nels achieve good results in negative sentences from\nthe Synonymy patterns (labeled as False) but strug-\ngle with the negative sentences from the Antonymy\npatterns (labeled as True). This, along with their\npoor performance in negative sentences with a dis-\ntractor (which are expected to be True, but models\npredict the label False), confirms that the models\nare heavily biased to always predict the label False\nin the presence of negation, regardless of the actual\nmeaning of the sentence. This behavior suggests\nthat the models lack a deep understanding of nega-\ntion, and that they tend to rely on superficial cues\nrather than comprehending the true meaning con-\nveyed by the negative sentences.\nDespite the poor performance of the models in\nnegative sentences, it is important to note that they\ndemonstrate the ability to correctly label affirma-\ntive sentences, both with and without distractors.\nThis demonstrates that the models have a deep un-\nderstanding of truth and falsehood. Models’ strug-\ngles primarily result from the presence of negation\nrather than a lack of comprehension or real-world\nknowledge.\n6 Exposure to negation does not solve the\nproblem\nUnderstanding whether LLMs would understand\nnegation if a sufficient number of negative sen-\ntences were present in the pretraining corpora is\ncrucial for improving their reasoning capabilities\nand addressing the limitations associated with neg-\native knowledge. However, due to the lack of suf-\nficiently large datasets containing negative knowl-\nedge, this hypothesis has not been extensively\nexplored. In contrast, our dataset is substantial\nenough to be split into training, development, and\ntest sets. To investigate whether LLMs can learn\nto reason over negative knowledge given enough\nnegated data, we split the dataset at the triple level,\nensuring that all sentences within a triple are as-\nsigned to the same split to ensure no data contami-\nnation. Our training dataset consists of 268,505 sen-\ntences from 2,876 triples, the development dataset\nincludes 2,514 sentences from 244 triples, and the\ntest dataset contains 90,281 sentences from 980\ntriples.\nWe finetune Flan-T5 and Vicuna on our dataset;\nthe results are listed in Table 6. The impact of fine-\ntuning is remarkable, as it completely transforms\nthe models’ performance compared to their zero-\nshot counterparts. Both Flan-T5 and Vicuna exhibit\nhigher accuracy than human annotators and achieve\na notably high level of coherence. However, are the\nmodels truly learning about negation, or are they\njust exploiting patterns in the data? We conduct\nexperiments to asses this.\nFirst, we train Vicuna, the best performing\nmodel, using varying amounts and types of neg-\native knowledge. We conduct separate fine-tuning\nexperiments using all the affirmative sentences and\nall the negative sentences from the dataset, result-\ning in two distinct models. The results of this train-\ning are presented in Table 7. Training the model\nexclusively with affirmative sentences yields a high\naccuracy in the affirmative test sentences, but it\nlabels incorrectly nearly all the negative sentences.\nConversely, when trained solely with negative sen-\ntences, the model deals successfully with the neg-\native sentences but struggles with the affirmative\nsentences. Despite being exposed to extensive real-\nworld knowledge from WordNet, the models ex-\nhibit a significant failure in comprehending nega-\ntion. They consistently overlook the presence of it\n8602\nVerbal Non-Verbal Analytic synthetic clausal subclausal Affirmation\nAll 96.0 95.7 95.8 96.0 96.0 95.7 95.5\nAll Affirmations 6.2 6.8 6.8 5.9 6.1 6.8 95.7\nAll Negated 96.1 95.8 95.8 96.3 96.1 95.8 4.5\nAffirmations + Verbal 95.5 79.5 81.9 95.6 95.5 79.5 95.4\nAffirmations + Non-Verbal 94.9 95.6 95.2 96.1 94.9 95.6 95.8\nAffirmations + Analytic 96.1 95.6 95.6 96.1 96.0 95.6 95.9\nAffirmations + synthetic 94.8 44.5 51.8 96.0 94.9 44.5 95.6\nAffirmations + clausal 95.8 34.6 43.9 96.0 96.1 34.6 95.8\nAffirmations + subclausal 95.1 95.7 95.3 96.2 95.1 95.7 96.0\nTable 7: Accuracy of Vicuna13B after fine-tuning with different types and amount of negative knowledge. The best\nresults are highlighted in bold, and scores that surpass the Random baseline accuracy are indicated with underline.\nand generate identical outputs for both affirmative\nand negative sentences. We also fine-tune mod-\nels using various combinations of affirmative sen-\ntences and different types of negations. We observe\nthat models trained with synthetic and clausal nega-\ntions struggle to accurately classify non-verbal, an-\nalytic, and sub-clausal sentences. This suggests\nthat while the models show proficiency in under-\nstanding and reasoning with certain types of nega-\ntions, they face challenges in comprehending and\ncorrectly responding to other forms of negations\nthat they have not seen in the fine-tuning step.\n#01#02#03#04#05#06#07#08#09#10#11#01 Synonymy#02 Antonymy#03 Synonymy#04 Antonymy#05 Hypernymy#06 Part#07 Substance#08 Member#09 Agent#10 Instrument#11 ResultAll\n10076907089889787899485501006593505050505050508679100907482888086938055986710050515351615451916392671008798959286928772927978100948685947881767687738310073868674717976876576871007467625946636162626558100626477667784747288708710068857984957784929088919197949810093949096918895\nFigure 2: Evaluation of Vicuna13B accuracy when\ntrained on one pattern (rows) and evaluated on the oth-\ners (columns).\nWe also fine-tune Vicuna13B with each of the 11\npatterns in our dataset independently, and we eval-\nuate its performance on the other patterns. Figure\n2 shows the overall accuracy scores. The results\nreveal that training the model with one pattern does\nnot facilitate any successful generalization across\nall other patterns. Notably, as discussed in Sec-\ntion 3.1, the labels for affirmative and negative\nsentences from the Antonymy patterns are opposite\nto those from the remaining patterns. The fail-\nure of models trained in other patterns to label the\nAntonymy patterns, as well as the failure of mod-\nels trained in the Antonymy patterns to label other\npatterns, suggest that the models are relying on\nrepetitive data structures that are not transferable\nto different patterns, rather than truly understand-\ning the concept of negation. While exposure to\nnegation may contribute to achieving favorable re-\nsults within a specific dataset, it does not lead to\na generalization on negation by the models. Nega-\ntion continues to pose a significant challenge in the\nfield of Natural Language Processing and remains\nan unsolved problem, requiring further research\nand development.\n7 Conclusion\nCurrent LLMs are typically trained using next to-\nken or mask token prediction objectives, which\nhave proven effective for various NLP tasks. How-\never, it remains an open issue understanding how\ncertainly a model models negation. Negation to-\nkens, which intermittently appear in sentences,\nhold little predictive importance for other tokens in\nthe sentence. As a result, there is limited negation\nsignal during language modeling training. Previ-\nous research has touched upon this issue but was\nlimited by small manually generated datasets. In\ncontrast, our study introduces the largest dataset to\ndate comprising negative sentences. This compre-\nhensive dataset includes affirmative and negative\nsentences with and without distractors, incorpo-\nrating multiple types of relations and negations,\nwhich help to encode the underlying mechanisms\nfor negation understanding. Through our analy-\nsis, we reveal that current LLMs, both in zero-shot\nsettings and when fine-tuned with examples from\nour dataset, exhibit a profound understanding of\n8603\nthe truthfulness of affirmative sentences. However,\nwhen it comes to negation, these models heav-\nily rely on superficial cues instead of generalizing\nnegation and these superficial cues are not transfer-\nable across different negative sentences.\nNegation remains a persistent and unsolved chal-\nlenge in the field of NLP, demanding further re-\nsearch to develop systems capable of effectively\nhandling it. Our dataset holds the potential to signif-\nicantly contribute towards achieving this objective.\nIn our future work, we plan to explore advanced\nreasoning paradigms, such as Chain-of-Thought,\nwith the aim of enhancing model performance on\nour dataset. However, dealing properly with nega-\ntion may also require novel neural architectures.\nLimitations\nThe dataset contains a limited number of low-\nquality sentences, which are discussed in Section\n3.2. Through manual evaluation, we find that over\n96% of the sentences are considered understand-\nable and grammatically correct by at least one hu-\nman annotator and their prediction of whether the\nsentence is true or false matches the label in the\ndataset. Hence, the presence of low-quality sen-\ntences does not have a significant impact on the\nevaluation results. On the other side, a majority\nof sentences in the dataset are not plausible and\nunlikely to be spoken by English speakers. This\nfeature provides a benefit by ensuring that the sen-\ntences are improbable to be found in the LLM train-\ning corpus, thereby it prevents models from relying\nsolely on memorization to generate accurate re-\nsponses.\nAll experiments were conducted by querying\nthe models for the probability of True and False\ntokens. We did not explore more complex reason-\ning prompts, such as Chain of Thought. However,\nas explained in Section 7, we believe that models\nshould be able to comprehend negation and provide\naccurate answers across diverse settings. Complex\nreasoning paradigms may not always be feasible in\nreal-world applications, specially when models are\nused by non-NLP professionals.\nFinally, the performance of models in our dataset\nis not solely determined by their capability to un-\nderstand negation. Factors such as performance in\nquestion answering and prompting tasks, as well as\ntheir understanding of real-world knowledge, play\na crucial role. However, models like Vicuna13B\nand Flan-T5-xxl showcase remarkable proficiency\nin correctly responding to affirmative sentences, in-\ndicating that their struggles primarily arise from the\npresence of negation. Additionally, we introduce a\ncoherence metric that considers whether the model\nchanges its prediction in the presence of negation,\nrather than solely focusing on the accuracy of the\nmodel’s answer to the question.\nEthics Statement\nThe dataset has been created through the English\nWordNet relations, so it reflects most of the “west-\nern” knowledge and might fall short in including\nconcepts of non-English speaking communities.\nThe generated triples from WordNet may include\noffensive or biased sentences. This can be caused\nby inherited biases from WordNet, or it can be\ncaused unintentionally during the random sampling\nof synsets.\nAcknowledgements\nWe would like to thank Jeremy Barnes and\nAritz Farwell for willingly offering themselves\nto conduct the dataset quality assessment ex-\nperiments. Begoña Altuna is supported by the\nBasque Government postdoctoral grant POS 2022\n2 0024. Iker García-Ferrero is supported by\na doctoral grant from the Basque Government\n(PRE_2022_2_0208). This work has also been\npartially supported by HiTZ center and the Basque\nGovernment (Research group funding IT-1805-22).\nWe also acknowledge the funding from the follow-\ning projects:\n(i) Antidote project funded by (PCI2020-120717-\n2) MCIN/AEI/10.13039/501100011033 and by\n“ERDF A way of making Europe\"\n(ii) MOTION (PID2020-112581GB-C22) sup-\nported by the Ministry of Science and Innovation\nof the Spanish Government\n(iii) The Basque Project LoRea (UPV/EHU\nGIU21/044).\n(iv) DeepKnowledge (PID2021-127777OB-C21)\nand ERDF A way of making Europe\n(v) DeepR3 (TED2021-130295B-C31) and Euro-\npean Union NextGeneration EU/PRTR.\nReferences\nEbtesam Almazrouei, Hamza Alobeidli, Abdulaziz Al-\nshamsi, Alessandro Cappelli, Ruxandra Cojocaru,\nMerouane Debbah, Etienne Goffinet, Daniel Hes-\nlow, Julien Launay, Quentin Malartic, Badreddine\nNoune, Baptiste Pannier, and Guilherme Penedo.\n8604\n2023. Falcon-40B: an open large language model\nwith state-of-the-art performance.\nHiba Arnaout, Simon Razniewski, Gerhard Weikum,\nand Jeff Z. Pan. 2022. UnCommonSense: Informa-\ntive Negative Knowledge about Everyday Concepts.\nIn Proceedings of the 31st ACM International Con-\nference on Information & Knowledge Management,\nCIKM ’22, pages 37—-46, New York, NY , USA.\nAssociation for Computing Machinery.\nJeremy Barnes, Erik Velldal, and Lilja Øvrelid. 2021.\nImproving sentiment analysis with multi-task learn-\ning of negation. Natural Language Engineering ,\n27(2):249–269.\nMarco Baroni. 2020. Linguistic generalization and\ncompositionality in modern artificial neural networks.\nPhilosophical Transactions of the Royal Society B:\nBiological Sciences, 375(1791):20190307.\nLuisa Bentivogli, Arianna Bisazza, Mauro Cettolo, and\nMarcello Federico. 2016. Neural versus Phrase-\nBased Machine Translation Quality: a Case Study.\nIn Proceedings of the 2016 Conference on Empirical\nMethods in Natural Language Processing, pages 257–\n267, Austin, Texas. Association for Computational\nLinguistics.\nLuisa Bentivogli, Pamela Forner, Bernardo Magnini,\nand Emanuele Pianta. 2004. Revising the Word-\nnet Domains Hierarchy: semantics, coverage and\nbalancing. In Proc. of the Workshop on Multilin-\ngual Linguistic Resources, pages 94–101, Geneva,\nSwitzerland. COLING.\nStella Biderman, Hailey Schoelkopf, Quentin Anthony,\nHerbie Bradley, Kyle O’Brien, Eric Hallahan, Mo-\nhammad Aflah Khan, Shivanshu Purohit, USVSN Sai\nPrashanth, Edward Raff, Aviya Skowron, Lintang\nSutawika, and Oskar van der Wal. 2023. Pythia: A\nSuite for Analyzing Large Language Models Across\nTraining and Scaling. CoRR, abs/2304.01373.\nJordan Boyd-Graber, Christiane Fellbaum, Daniel Os-\nherson, and Robert Schapire. 2006. Adding dense,\nweighted connections to WordNet. In Proceedings\nof the third international WordNet conference, pages\n29–36.\nWendy W. Chapman, Will Bridewell, Paul Hanbury,\nGregory F. Cooper, and Bruce G. Buchanan. 2001. A\nSimple Algorithm for Identifying Negated Findings\nand Diseases in Discharge Summaries. Journal of\nBiomedical Informatics, 34(5):301–310.\nJiangjie Chen, Wei Shi, Ziquan Fu, Sijie Cheng, Lei\nLi, and Yanghua Xiao. 2023. Say What You Mean!\nLarge Language Models Speak Too Positively about\nNegative Commonsense Knowledge.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An Open-\nSource Chatbot Impressing GPT-4 with 90%* Chat-\nGPT Quality.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, Albert Web-\nson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-\ngun, Xinyun Chen, Aakanksha Chowdhery, Sharan\nNarang, Gaurav Mishra, Adams Yu, Vincent Y . Zhao,\nYanping Huang, Andrew M. Dai, Hongkun Yu, Slav\nPetrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam\nRoberts, Denny Zhou, Quoc V . Le, and Jason Wei.\n2022. Scaling instruction-finetuned language models.\nCoRR, abs/2210.11416.\nMike Conover, Matt Hayes, Ankit Mathur, Xiangrui\nMeng, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi,\nPatrick Wendell, Matei Zaharia, and Reynold Xin.\n2023. Free Dolly: Introducing the World’s First\nTruly Open Instruction-Tuned LLM.\nTim Dettmers, Mike Lewis, Younes Belkada, and\nLuke Zettlemoyer. 2022. LLM.int8(): 8-bit Ma-\ntrix Multiplication for Transformers at Scale. CoRR,\nabs/2208.07339.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and\nLuke Zettlemoyer. 2023. QLoRA: Efficient Finetun-\ning of Quantized LLMs. CoRR, abs/2305.14314.\nC. Fellbaum, editor. 1998. WordNet: An Electronic\nLexical Database. MIT Press.\nChristiane Fellbaum, Anne Osherson, and Peter E.\nClark. 2009. Putting semantics into WordNet’s “Mor-\nphosemantic” Links. In Zygmunt Vetulani and Hans\nUszkoreit, editors, Human Language Technology.\nChallenges of the Information Society, LNAI 5603,\npages 350–358. Springer.\nDaniel Furrer, Marc van Zee, Nathan Scales, and\nNathanael Schärli. 2021. Compositional Generaliza-\ntion in Semantic Parsing: Pre-training vs. Specialized\nArchitectures.\nAndreas Grivas, Beatrice Alex, Claire Grover, Richard\nTobin, and William Whiteley. 2020. Not a cute\nstroke: Analysis of Rule- and Neural Network-based\nInformation Extraction Systems for Brain Radiology\nReports. In Proceedings of the 11th International\nWorkshop on Health Text Mining and Information\nAnalysis, pages 24–37, Online. Association for Com-\nputational Linguistics.\nMd Mosharaf Hossain, Dhivya Chinnappa, and Eduardo\nBlanco. 2022. An Analysis of Negation in Natu-\nral Language Understanding Corpora. In Proceed-\nings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Pa-\npers), pages 716–723, Dublin, Ireland. Association\nfor Computational Linguistics.\nMd Mosharaf Hossain, Venelin Kovatchev, Pranoy\nDutta, Tiffany Kao, Elizabeth Wei, and Eduardo\nBlanco. 2020. An Analysis of Natural Language\nInference Benchmarks through the Lens of Negation.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\n8605\npages 9106–9118, Online. Association for Computa-\ntional Linguistics.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2022. LoRA: Low-Rank Adaptation\nof Large Language Models. In The Tenth Inter-\nnational Conference on Learning Representations,\nICLR 2022, Virtual Event, April 25-29, 2022. Open-\nReview.net.\nRubén Izquierdo, Armando Suárez, and German Rigau.\n2007. Exploring the Automatic Selection of Ba-\nsic Level Concepts. In Proc. of the Int. Conf. on\nRecent Advances on Natural Language Processing\n(RANLP’07), volume 7.\nJaap Jumelet and Dieuwke Hupkes. 2018. Do Lan-\nguage Models Understand Anything? On the Ability\nof LSTMs to Understand Negative Polarity Items. In\nProceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP, pages 222–231, Brussels, Belgium.\nAssociation for Computational Linguistics.\nNora Kassner and Hinrich Schütze. 2020. Negated and\nMisprimed Probes for Pretrained Language Models:\nBirds Can Talk, But Cannot Fly. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 7811–7818, Online.\nAssociation for Computational Linguistics.\nRoser Morante and Walter Daelemans. 2012.\nConanDoyle-neg: Annotation of negation cues and\ntheir scope in Conan Doyle stories. In Proceedings\nof the Eighth International Conference on Language\nResources and Evaluation (LREC’12) , pages\n1563–1568, Istanbul, Turkey. European Language\nResources Association (ELRA).\nGeoffrey K. Pullum, Rodney Huddleston, Rodney Hud-\ndleston, and Geoffrey K. Pullum. 2002. Negation,\npages 785–850. Cambridge University Press.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the Lim-\nits of Transfer Learning with a Unified Text-to-Text\nTransformer. J. Mach. Learn. Res., 21:140:1–140:67.\nJérémy Scheurer, Jon Ander Campos, Tomasz Kor-\nbak, Jun Shern Chan, Angelica Chen, Kyunghyun\nCho, and Ethan Perez. 2023. Training Language\nModels with Language Feedback at Scale. CoRR,\nabs/2303.16755.\nGongbo Tang, Philipp Rönchen, Rico Sennrich, and\nJoakim Nivre. 2021. Revisiting Negation in Neural\nMachine Translation. Transactions of the Associa-\ntion for Computational Linguistics, 9:740–755.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurélien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. LLaMA: Open\nand Efficient Foundation Language Models. CoRR,\nabs/2302.13971.\nThinh Hung Truong, Yulia Otmakhova, Timothy Bald-\nwin, Trevor Cohn, Jey Han Lau, and Karin Verspoor.\n2022. Not another Negation Benchmark: The NaN-\nNLI Test Suite for Sub-clausal Negation. In Pro-\nceedings of the 2nd Conference of the Asia-Pacific\nChapter of the Association for Computational Lin-\nguistics and the 12th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 883–894, Online only. Association for\nComputational Linguistics.\nDavid Vilares, Miguel A. Alonso, and Carlos Gómez-\nRodríguez. 2015. On the usefulness of lexical and\nsyntactic processing in polarity classification of Twit-\nter messages. Journal of the Association for Informa-\ntion Science and Technology, 66(9):1799–1816.\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,\nPu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin\nJiang. 2023. WizardLM: Empowering Large Lan-\nguage Models to Follow Complex Instructions.\nCoRR, abs/2304.12244.\nA Extended zero-shot results\nApart from the models presented in Section 5, as\nanticipated, we have also tested the performance in\nthe task of the following models: Koala 4, which\nis a LLaMA model that has been fine-tuned for\ndialogue; Open-Assistant (oasst-sft-1-pythia-12b)\n5, which is a Pythia model fine-tuned on human\ngenerated assistant conversations; and INCITE 6 7\nbillion foundation model along with the two mod-\nels that have been further fine-tuned by the authors\nin the instruction-tuning paradigm and chat con-\nversation. Table 8 shows the extended evaluation\nresults.\nB Efficient inference and training\nTo facilitate inference of the models on a single\nGPU, we employed 8-bit quantization (Dettmers\net al., 2022) for all of them. We conducted prelimi-\nnary experiments with Vicuna 13 billion parameter\nmodel. Results are shown in Table 9. While the run-\nning cost of the models significantly decreases, we\nobserved only minimal performance degradation\nin the quantified versions.\nFor the training process, we utilized Low-Rank\nAdaptation (LoRA) (Hu et al., 2022). This ap-\nproach involves freezing the weights of the pre-\n4https://bair.berkeley.edu/blog/2023/04/03/\nkoala/\n5https://huggingface.co/OpenAssistant/\noasst-sft-1-pythia-12b\n6https://www.together.xyz/blog/redpajama-7b\n8606\nModel name Model Type Coherence Accuracy\nAll W/o Distractor W/ Distractor\nAll W/o Distractor W/ Distractor Affirmation Negation Affirmation Negation\nRandom - 0.5 0.9 0.8 50.0 50.2 50.1 50.0 49.9\nLLaMA7B Foundation 0.0 0.2 0.0 50.4 95.7 11.8 2.3 91.0\nLLaMA13B Foundation 0.0 0.2 0.3 50.1 85.8 12.2 10.6 90.2\nLLaMA30B Foundation 0.1 0.3 0.2 52.4 84.7 29.5 30.2 68.8\nLLaMA65B Foundation 0.0 0.0 0.0 50.3 96.3 3.1 1.3 99.3\nVicuna7B Dialogue 0.0 0.2 0.0 52.4 18.1 96.9 98.9 0.3\nVicuna13B Dialogue 0.2 8.8 0.6 57.8 83.1 85.1 78.0 2.6\nKoala7B Dialogue 0.0 0.0 0.0 50.1 5.4 97.2 99.5 0.3\nKoala13B Dialogue 0.0 0.7 0.0 52.8 20.4 97.1 98.8 0.3\nWizardLM7B Instruction 0.2 0.2 0.7 51.0 89.6 27.3 14.2 73.8\nWizardLM13B Instruction 0.0 4.3 0.3 57.4 68.7 86.2 87.5 2.9\nWizardLM30B Instruction 0.0 6.0 0.1 57.3 53.6 95.7 88.8 2.0\nWizardLM7B-uncensored Instruction0.0 1.0 0.1 53.7 63.4 78.4 63.3 17.5\nWizardLM13B-uncensored Instruction0.0 0.3 0.0 50.5 7.6 97.0 99.3 0.3\nWizardLM30B-uncensored Instruction0.0 0.0 0.0 49.8 3.6 97.2 99.6 0.2\nPythia12B Foundation 0.0 0.1 0.0 50.1 93.8 15.2 4.0 86.7\noasst-pythia12B DIalogue 0.0 0.0 0.0 49.8 4.7 97.1 98.9 0.3\nDolly12B Instruction 0.0 0.3 0.2 50.2 72.0 73.3 33.4 25.1\nT5-xxl Foundation 0.0 0.0 0.0 50.3 96.6 2.8 0.4 99.8\nFlan-T5-xxl Instruction 0.9 46.4 1.2 66.1 86.1 96.1 94.6 6.2\nFalcon7b Foundation 0.0 0.0 0.0 50.3 96.6 2.9 0.4 99.7\nFalcon7b-instruct Instruction 0.0 0.3 0.4 50.1 82.7 20.9 21.2 77.0\nFalcon40b Foundation 0.1 0.1 0.2 49.7 90.9 13.9 11.6 83.3\nFalcon40b-instruct Instruction 0.1 1.5 0.2 54.7 64.3 76.8 71.4 16.6\nINCITE7B-Base Foundation 0.1 0.3 0.1 50.3 82.6 17.0 16.4 84.5\nINCITE7B-Instruct Instruction 0.2 0.4 0.4 50.5 73.4 22.5 26.9 78.7\nINCITE7B-Chat Dialogue 0.1 0.3 0.2 50.0 19.8 89.4 88.0 6.0\nTable 8: Zero-shot performance of various LLMs in our dataset. The best results are highlighted in bold, and scores\nthat surpass the Random baseline accuracy are indicated with underline.\nModel name Precision Coherence Accuracy\nAll W/o Distractor W/ Distractor\nAll W/o Distractor W/ Distractor Affirmation Negation Affirmation Negation\nVicuna13B 8-Bits 0.2 8.8 0.6 57.8 83.1 85.1 78.0 2.6\nVicuna13B Float16 0.4 10.1 1.1 57.8 84.4 85.8 74.7 3.2\nTable 9: Zero-shot performance of Vicuna using 8-Bits quantification and the orifinal float16 weights. The best\nresults are highlighted in bold, and scores that surpass the Random baseline are indicated with underline.\ntrained model and introducing trainable rank de-\ncomposition matrices into each layer. The frozen\nmodel weights are quantized into 8 bits, while\nthe LoRA trainable weights remain in 16 bits\n(Dettmers et al., 2023). By adopting this efficient\ntraining paradigm, we were able to train LLMs\nwith up to 13 billion parameters on a single GPU\nwithin a reasonable timeframe.\nWe perform all our experiments using a single\nNVIDIA A100 GPU with 80GB memory. The\nmachine used has two AMD EPYC 7513 32-Core\nProcessors and 1024GB of RAM.\nC Dataset construction: selection of\ndistractors\nThe automatic creation of false knowledge (dis-\ntracting triples) on the basis of WordNet triples\nrequires the use of distractors. In general, distrac-\nPattern W/o Distractor W/ Distractor\nAffirmation Negation Affirmation Negation\n#01 True False False True\n#02 False True False True\n#03 True False False True\n#04 False True False True\n#05 True False False True\n#06 True False False True\n#07 True False False True\n#08 True False False True\n#09 True False False True\n#10 True False False True\n#11 True False False True\nTable 10: Truth-value of resulting sentences by pattern.\ntors are randomly selected words that replace the\nword senses of a synset in a given triple, although\nthe whole synset is replaced when using glosses in\ntemplates (Patterns #01 and #02). For each Word-\nNet triple, we use a single distracting triple except\n8607\nfor Patterns #02, #03 and #04, where we use two\ndistracting triples obtained by using one distractor\nper synset. Apart from BLCs, for the selection of\nsuitable distractors we consider the lexicographer\nfiles provided by WordNet, which are 45 syntactic\ncategory and logical groupings of word senses, and\nWordNet Domains, which consist of a hierarchy of\n164 labels that characterize knowledge areas and to\nwhich each synset is connected. More concretely:\n• Words (synsets in the case of Pattern #01 and\n#02 when using glosses) belonging to some\nBLCs cannot be distractors to ensure that se-\nlected words (synsets) are not too general.\n• The combined lexicographer file and Word-\nNet Domain annotation of any word sense of\nthe given synset and of any synset where the\ndistractor occurs (of the synset in the case of\nPattern #01 and #02 when using glosses) have\nto be different.\nIn general, these restrictions ensure that the result-\ning false triples do not encode true knowledge. The\nprobability of choosing a synset as distractor is di-\nrectly proportional to the logarithm of its frequency.\nFor example, wood can be used as distractor\nof expenditure1\nn because wood belongs to the\nlexicographer files noun.substance (nouns denot-\ning cognitive processes and contents), noun.group\n(nouns denoting groupings of people or ob-\njects) and noun.artifact (nouns denoting man-\nmade objects) while expenditure belongs to\nnoun.possession (nouns denoting possession and\ntransfer of possession) and noun.act (nouns de-\nnoting acts or actions). Therefore, we get\nthe distracting triple ⟨ant, wood,income1\nn⟩ from\n⟨ant, expenditure1\nn, income1\nn⟩. On the contrary,\nthe word registration cannot be used as distractor\nof expenditure1\nn as both words belong to the lexico-\ngraphic file noun.act and the synsets expenditure2\nn\nand registration1\nn belong to the economy domain.\nD Dataset Description\nIn Table 10, we provide the truth-value of sentences\nthat results by instantiating affirmative and negative\ntemplates using WordNet and distracting triples ac-\ncording to the pattern. In the following subsections,\nwe describe each pattern and provide some exam-\nples that are used in Figures 3–8 to illustrate the\ninstantiation of a sample of the templates. In all the\ntemplates described in Figures 3–8, alternative and\noptional expressions are enclosed respectively in\nsquare brackets and parentheses.\nD.1 Pattern #01: synonymy (gloss)\nThis pattern includes 21 templates stating semantic\nequivalence correspondences between a word and\nthe gloss of a synset to which the word belongs.\nSince WordNet does not provide triples for syn-\nonymy relating two synsets, we get triples relating\neach synset to itself by reflexivity. For each re-\nsulting triple, templates are also instantiated using\none distracting triple that is obtained by replacing\nthe third component of each triple with a distractor\n(synset).\nIn Figure 3, we introduce a positive and a nega-\ntive template and illustrate their instantiation using\nthe synset fligth9\nn with gloss “a scheduled trip by\nplane between designated airports” and the distrac-\ntor troop1\nn (“a group of soldiers”).\nD.2 Pattern #02: antonymy (gloss)\nThis pattern includes 21 templates stating semantic\nequivalence correspondences between a word and\nthe gloss of an antonym synset, where the word and\nthe gloss are respectively taken from the second\nand third component of triples. Furthermore, for\neach WordNet antonymy triple templates are also\ninstantiated using two distracting triples that are\nrespectively obtained by replacing the second and\nthird component with distractors (for the third one,\nthe distractor is a synset).\nIn Figure 3, we introduce a positive and a nega-\ntive template and illustrate their instantiation using\nthe antonym synsets brother1\nn and sister1\nn (“a fe-\nmale person who has the same parents as another\nperson”) and the distractors stream and fiction1\nn\n(“a literary work based on the imagination and not\nnecessarily on fact”).\nD.3 Pattern #03: synonymy\nThis pattern includes 24 templates stating seman-\ntic equivalence correspondences between words.\nSince WordNet does not provide triples for syn-\nonymy relating two synsets, we get triples relating\neach synset to itself by reflexivity. For each re-\nsulting triple, templates are also instantiated using\ntwo distracting triples that are respectively obtained\nby replacing the second and third component with\ndistractors.\nIn Figure 4, we introduce a positive and a nega-\ntive template and illustrate their instantiation using\n8608\nthe synonym words path and route and the distrac-\ntors engine and identity.\nD.4 Pattern #04: antonymy\nThis pattern includes 24 templates stating semantic\nequivalence correspondences between words. For\neach WordNet antonymy triple, templates are also\ninstantiated using two distracting triples that are\nrespectively obtained by replacing the second and\nthird component with distractors.\nIn Figure 5, we introduce a positive and a nega-\ntive template and illustrate their instantiation using\nthe antonym synsets expenditure1\nn and income1\nn\nand the distractors wood and year.\nD.5 Pattern #05: hypernymy\nThis pattern includes 24 templates stating semantic\nsubsumption correspondences between words. For\neach WordNet hypernymy triple, templates are also\ninstantiated using one distracting triple that is ob-\ntained by replacing the hyponym with a distractor.\nIn Figure 5, we introduce a positive and a nega-\ntive template and illustrate their instantiation using\nthe synset auction1\nn, which is hyponym of sale2\nn,\nand the distractor breakdown.\nD.6 Pattern #06: meronymy (part)\nThis pattern includes 16 templates stating semantic\ncorrespondences between parts and wholes. For\neach WordNet triple, templates are also instanti-\nated using one distracting triple that is obtained by\nreplacing the whole with a distractor.\nIn Figure 6, we introduce a positive and a nega-\ntive template and illustrate their instantiation using\nthe synset week3\nn, which is related by part with\nmonth1\nn, and the distractor fence.\nD.7 Pattern #07: meronymy (substance)\nThis pattern includes 15 templates stating semantic\ncorrespondences between substances and things.\nFor each WordNet triple, templates are also instan-\ntiated using one distracting triple that is obtained\nby replacing the whole with a distractor.\nIn Figure 6, we introduce a positive and a nega-\ntive template and illustrate their instantiation using\nthe synset sand1\nn, which is related by substance\nwith beach1\nn, and the distractor decade.\nD.8 Pattern #08: meronymy (member)\nThis pattern includes 17 templates stating seman-\ntic correspondences between members and groups.\nFor each WordNet triple, templates are also instan-\ntiated using one distracting triple that is obtained\nby replacing the group with a distractor.\nIn Figure 7, we introduce a positive and a nega-\ntive template and illustrate their instantiation using\nthe synset voter1\nn, which is related bymember with\nelectorate1\nn, and the distractor sport.\nD.9 Pattern #09: semantic role (agent)\nThis pattern includes 2 templates stating semantic\ncorrespondences between agents and events. For\neach WordNet triple, templates are also instanti-\nated using one distracting triple that is obtained by\nreplacing the agent with a distractor.\nIn Figure 7, we introduce a positive and a nega-\ntive template and illustrate their instantiation using\nthe synset rule1\nn, which is related by agent with\ngovernor1\nn, and the distractor hole.\nD.10 Pattern #10: semantic role (instrument)\nThis pattern includes 7 templates stating semantic\ncorrespondences between instruments and events.\nFor each WordNet triple, templates are also instan-\ntiated using one distracting triple that is obtained\nby replacing the event with a distractor.\nIn Figure 8, we introduce a positive and a nega-\ntive template and illustrate their instantiation using\nthe synset telephone1\nn, which is related by instru-\nment with call3\nv , and the distractor lay.\nD.11 Pattern #11: semantic role (result)\nThis pattern includes 27 templates stating semantic\ncorrespondences between results and events. For\neach WordNet triple, templates are also instanti-\nated using one distracting triple that is obtained by\nreplacing the event with a distractor.\nIn Figure 8, we introduce a positive and a nega-\ntive template and illustrate their instantiation using\nthe synset response1\nn, which is related by result\nwith answer1\nv , and the distractor dress.\n8609\nPattern #01: synonymy (gloss)\nAffirmative template:\nA/An ⟨word⟩ is (commonly) ⟨gloss⟩.\nSentences:\nA flight is commonly a scheduled trip by plane between designated airports. True\nA flight is a scheduled trip by plane between designated airports. True\nA flight is commonly a group of soldiers. False\nA flight is a group of soldiers. False\nNegative template (verbal, analytic and clausal):\nA/An ⟨word⟩ is not ⟨gloss⟩.\nSentences:\nA flight is not a group of soldiers. True\nA flight is not a scheduled trip by plane between designated airports. False\nPattern #02: antonymy (gloss)\nAffirmative template:\n⟨word⟩ (commonly) [ stands for | refers to ] ⟨gloss⟩.\nSentences:\nBrother commonly stands for a female person who has the same parents as another person. False\nBrother commonly refers to a female person who has the same parents as another person. False\nBrother stands for a female person who has the same parents as another person. False\nBrother refers to a female person who has the same parents as another person. False\nStream commonly stands for a female person who has the same parents as another person. False\nStream commonly refers to a female person who has the same parents as another person. False\nStream stands for a female person who has the same parents as another person. False\nStream refers to a female person who has the same parents as another person. False\nBrother commonly stands for a literary work based on the imagination and not necessarily on fact. False\nBrother commonly refers to a literary work based on the imagination and not necessarily on fact. False\nBrother stands for a literary work based on the imagination and not necessarily on fact. False\nBrother refers to a literary work based on the imagination and not necessarily on fact. False\nNegative template (synthetic and subclausal):\nA/An ⟨word⟩ is never ⟨gloss⟩.\nSentences:\nA brother is never a female person who has the same parents as another person. True\nA stream is never a female person who has the same parents as another person. True\nA brother is never a literary work based on the imagination and not necessarily on fact. True\nFigure 3: Description of Patterns #01 and #02.\n8610\nPattern #03: synonymy\nAffirmative template:\n⟨noun1+(e)s⟩ and ⟨noun2+(e)s⟩ [ are | may be ] always different.\nSentences:\nPath and route are always different. False\nPath and route may be always different. False\nEngine and route are always different. True\nEngine and route may be always different. True\nPath and identity are always different. True\nPath and identity may be always different. True\nNegative template (verbal, analytic and subclausal):\n⟨noun1+(e)s⟩ and ⟨noun2+(e)s⟩ [ are not | may not be ] synonyms in any context.\nSentences:\nPath and route are not synonyms in any context. False\nPath and route may not be synonyms in any context. False\nEngine and route are not synonyms in any context. True\nEngine and route may not be synonyms in any context. True\nPath and identity are not synonyms in any context. True\nPath and identity may not be synonyms in any context. True\nFigure 4: Description of Pattern #03.\n8611\nPattern #04: antonymy\nAffirmative template:\n⟨noun1+(e)s⟩ and ⟨noun2+(e)s⟩ [ are | may be ] synonyms (in certain contexts).\nSentences:\nExpenditure and income are synonyms in certain contexts. False\nExpenditure and income may be synonyms in certain contexts. False\nExpenditure and income are synonyms. False\nExpenditure and income may be synonyms. False\nExpenditure and year are synonyms in certain contexts. False\nExpenditure and year may be synonyms in certain contexts. False\nExpenditure and year are synonyms. False\nExpenditure and year may be synonyms. False\nWood and income are synonyms in certain contexts. False\nWood and income may be synonyms in certain contexts. False\nWood and income are synonyms. False\nWood and income may be synonyms. False\nNegative template (analytic and subclausal):\n⟨noun1+(e)s⟩ and ⟨noun2+(e)s⟩ [ are | may be ] the same thing in no context.\nSentences:\nExpenditure and income are the same thing in no context. True\nExpenditure and income may be the same thing in no context. True\nExpenditure and year are the same thing in no context. True\nExpenditure and year may be the same thing in no context. True\nWood and income are the same thing in no context. True\nWood and income may be the same thing in no context. True\nPattern #05: hypernymy\nAffirmative template:\nA/An ⟨hyponym⟩ [ is | may be ] a/an ⟨hypernym⟩ in certain contexts.\nSentences:\nAn auction is a sale in certain contexts. True\nAn auction may be a sale in certain contexts. True\nA breakdwon is a sale in certain contexts. False\nA breakdown may be a sale in certain contexts. False\nNegative template (synthetic and subclausal):\nA/An ⟨hyponym⟩ is never a/an ⟨hypernym⟩.\nSentences:\nAn auction is never a sale. False\nA breakdown is never a sale. True\nFigure 5: Description of Patterns #04 and #05.\n8612\nPattern #06: meronymy (part)\nAffirmative template:\nA/An ⟨part⟩ [ is commonly | may be ] part of a/an ⟨whole⟩.\nSentences:\nA week is commonly part of a month. True\nA week may be part of a month. True\nA week is commonly part of a fence. False\nA week may be part of a fence. False\nNegative template (synthetic and subclausal):\nA/An ⟨part⟩ is never part of a/an ⟨whole⟩.\nSentences:\nA week is never part of a month. False\nA week is never part of a fence. True\nPattern #07: meronymy (substance)\nAffirmative template:\n⟨thing + (e)s⟩ [ are commonly | may be ] made of ⟨substance⟩.\nSentences:\nBeaches are commonly made of sand. True\nBeaches may be made of sand. True\nDecades are commonly made of sand. False\nDecades may be made of sand. False\nNegative template (Analytic and subclausal):\nIn no context ⟨thing + (e)s⟩ [ are |may be ] made of ⟨substance⟩.\nSentences:\nIn no context beaches are made of sand. False\nIn no context decades are made of sand. True\nFigure 6: Description of Patterns #06 and #07.\n8613\nPattern #08: meronymy (member)\nAffirmative template:\n⟨member + (e)s⟩ [ are | may be ] members of ⟨group + (e)s⟩.\nSentences:\nV oters are members of electorates. True\nV oters may be members of electorates. True\nV oters are members of sports. Falses\nV oters may be members of sports. False\nNegative template (verbal, analytic and clausal):\n⟨member + (e)s⟩ [ are not | may not be ] members of ⟨group + (e)s⟩ in any context.\nSentences:\nV oters are not members of electorates in any context. False\nV oters may not be members of electorates in any context. False\nV oters are not members of sports in any context. True\nV oters may not be members of sports in any context. True\nPattern #09: semantic role (agent)\nAffirmative template:\n⟨event + ing⟩ is commonly done by ⟨agent + (e)s⟩.\nSentences:\nRuling is commonly done by governors. True\nRuling is commonly done by holes. False\nNegative template (Verbal, analytic and clausal):\n⟨event + ing⟩ is not commonly done by ⟨agent + (e)s⟩.\nSentences:\nRuling is not commonly done by governors. False\nRuling is not commonly done by holes. True\nFigure 7: Description of Patterns #08 and #09.\n8614\nPattern #10: semantic role (instrument)\nAffirmative template:\nA/An ⟨instrument⟩ [ is commonly | may be ] [ used | needed ] for ⟨event + ing⟩.\nSentences:\nA telephone is commonly used for calling. True\nA telephone is commonly needed for calling. True\nA telephone may be used for calling. True\nA telephone may be needed for calling. True\nA telephone is commonly used for laying. False\nA telephone is commonly needed for laying. False\nA telephone may be used for laying. False\nA telephone may be needed for laying. False\nNegative template (Synthetic and subclausal):\nA/An ⟨instrument⟩ should never be [ used | needed ] for ⟨event + ing⟩.\nSentences:\nA telephone should never be used for calling. False\nA telephone should never be used for laying. True\nPattern #11: semantic role (result)\nAffirmative template:\n⟨event + ing⟩ [ commonly leads | may lead ] to a/an ⟨result⟩.\nSentences:\nAnswering commonly leads to a response. True\nAnswering may lead to a response. True\nDressing commonly leads to a response. False\nDressing may lead to a response. False\nNegative template (analytic and subclausal):\n⟨event + ing⟩ [ leads | may lead ] to a/an ⟨result⟩ in no context.\nSentences:\nAnswering leads to a response in no context. False\nAnswering may lead to a response in no context. False\nAnswering leads to a response in no context. True\nAnswering may lead to a response in no context. True\nFigure 8: Description of Patterns #10 and #11.\n8615",
  "topic": "Negation",
  "concepts": [
    {
      "name": "Negation",
      "score": 0.9036242961883545
    },
    {
      "name": "Generalization",
      "score": 0.6874912977218628
    },
    {
      "name": "Computer science",
      "score": 0.6351362466812134
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5947535634040833
    },
    {
      "name": "Natural language processing",
      "score": 0.5906456708908081
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5844161510467529
    },
    {
      "name": "Inference",
      "score": 0.42575234174728394
    },
    {
      "name": "Programming language",
      "score": 0.14742878079414368
    },
    {
      "name": "Epistemology",
      "score": 0.11554202437400818
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}