{
  "title": "Error Analysis of Pretrained Language Models (PLMs) in English-to-Arabic Machine Translation",
  "url": "https://openalex.org/W4391540147",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4380572977",
      "name": "Hend Al-Khalifa",
      "affiliations": [
        "King Saud University"
      ]
    },
    {
      "id": "https://openalex.org/A5005356428",
      "name": "Khaloud Al-Khalefah",
      "affiliations": [
        "Imam Mohammad ibn Saud Islamic University"
      ]
    },
    {
      "id": "https://openalex.org/A2529668398",
      "name": "Hesham Haroon",
      "affiliations": [
        "King Saud University"
      ]
    },
    {
      "id": "https://openalex.org/A4380572977",
      "name": "Hend Al-Khalifa",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5005356428",
      "name": "Khaloud Al-Khalefah",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2529668398",
      "name": "Hesham Haroon",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4206537707",
    "https://openalex.org/W2946627182",
    "https://openalex.org/W4283730643",
    "https://openalex.org/W4294133266",
    "https://openalex.org/W3023523443",
    "https://openalex.org/W1608789752",
    "https://openalex.org/W3087105636",
    "https://openalex.org/W4205243694",
    "https://openalex.org/W2011539202",
    "https://openalex.org/W2792876194",
    "https://openalex.org/W4316658274",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3101286153",
    "https://openalex.org/W2922709902",
    "https://openalex.org/W3167012965",
    "https://openalex.org/W4224298148",
    "https://openalex.org/W3217091100",
    "https://openalex.org/W4287855142",
    "https://openalex.org/W1553828747",
    "https://openalex.org/W2883386157",
    "https://openalex.org/W2973065121",
    "https://openalex.org/W3123572135",
    "https://openalex.org/W3093871477",
    "https://openalex.org/W4321617771",
    "https://openalex.org/W2883141851",
    "https://openalex.org/W2250342921",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W4287665055",
    "https://openalex.org/W4367860087"
  ],
  "abstract": "Abstract Advances in neural machine translation utilizing pretrained language models (PLMs) have shown promise in improving the translation quality between diverse languages. However, translation from English to languages with complex morphology, such as Arabic, remains challenging. This study investigated the prevailing error patterns of state-of-the-art PLMs when translating from English to Arabic across different text domains. Through empirical analysis using automatic metrics (chrF, BERTScore, COMET) and manual evaluation with the Multidimensional Quality Metrics (MQM) framework, we compared Google Translate and five PLMs (Helsinki, Marefa, Facebook, GPT-3.5-turbo, and GPT-4). Key findings provide valuable insights into current PLM limitations in handling aspects of Arabic grammar and vocabulary while also informing future improvements for advancing English–Arabic machine translation capabilities and accessibility.",
  "full_text": "Vol:.(1234567890)\nHuman-Centric Intelligent Systems (2024) 4:206–219\nhttps://doi.org/10.1007/s44230-024-00061-7\nRESEARCH ARTICLE\nError Analysis of Pretrained Language Models (PLMs) \nin English‑to‑Arabic Machine Translation\nHend Al‑Khalifa1,3  · Khaloud Al‑Khalefah2 · Hesham Haroon3\nReceived: 3 October 2023 / Accepted: 4 January 2024 / Published online: 5 February 2024 \n© The Author(s) 2024\nAbstract\nAdvances in neural machine translation utilizing pretrained language models (PLMs) have shown promise in improving the \ntranslation quality between diverse languages. However, translation from English to languages with complex morphology, \nsuch as Arabic, remains challenging. This study investigated the prevailing error patterns of state-of-the-art PLMs when \ntranslating from English to Arabic across different text domains. Through empirical analysis using automatic metrics (chrF, \nBERTScore, COMET) and manual evaluation with the Multidimensional Quality Metrics (MQM) framework, we compared \nGoogle Translate and five PLMs (Helsinki, Marefa, Facebook, GPT-3.5-turbo, and GPT-4). Key findings provide valuable \ninsights into current PLM limitations in handling aspects of Arabic grammar and vocabulary while also informing future \nimprovements for advancing English–Arabic machine translation capabilities and accessibility.\nKeywords Machine translation · Pretrained large language models · Translation studies · GPT · Arabic language\n1 Introduction\nThe digital era has brought about a significant shift in com-\nmunication and interaction across linguistic and cultural \nboundaries. With the rise of global interconnectedness, the \nability to translate between languages, particularly those \nusing non-Latin scripts, such as Arabic, has become increas-\ningly important. However, many writing systems beyond \nthe Latin alphabet face barriers to digital accessibility and \nparticipation [1]. Artificial Intelligence (AI), particularly in \nthe field of Neural Machine Translation (NMT), has shown \npromise in bridging these linguistic divides [2, 3]. How -\never, translating between languages with profound structural \nand script differences, such as English and Arabic, presents \nunique challenges [4, 5].\nPretrained Language Models (PLMs) are at the forefront \nof Natural Language Processing (NLP) research and have \nsignificantly enhanced machine translation capabilities [6 ]. \nDespite their advancements, these models still struggle to \nachieve high levels of accuracy and fluency in English-to-\nArabic translation [7 ]. This limitation hampers effective \ncommunication and cooperation across English and Arabic-\nspeaking cultures, which is crucial in areas such as trade, \ndiplomacy, and knowledge exchange. This study aims to \ninvestigate the error patterns in state-of-the-art PLMs when \ntranslating from English to Arabic. Our objectives will be:\n• To evaluate these models in a parallel corpus of English–\nArabic sentences.\n• To identify common error patterns and explore their pos-\nsible causes.\n• To provide insights into the limitations of current PLMs.\nThis study targets academics, NLP practitioners, and poli-\ncymakers interested in leveraging AI for language transla-\ntion. Our findings provide valuable insights into the current \nlimitations of PLMs in terms of English-to-Arabic transla-\ntion quality. The analysis intends to offer guidance for future \nresearch efforts focused on advancing machine translation \ncapabilities for the English–Arabic language pairs. Addition-\nally, by elucidating the existing challenges in cross-lingual \n * Hend Al-Khalifa \n hendk@ksu.edu.sa\n Khaloud Al-Khalefah \n kholodalkhalifah@gmail.com\n1 Information Technology Department, King Saud University, \nRiyadh, Saudi Arabia\n2 College of Language and Translation, Al-Imam Muhammad \nIbn Saud Islamic University, Riyadh, Saudi Arabia\n3 iWAN Research Group, King Saud University, Riyadh, \nSaudi Arabia\n207Human-Centric Intelligent Systems (2024) 4:206–219 \nAI systems, we hope to highlight the broader significance of \nprogress in this space to achieve equitable global participa-\ntion in the exchange of ideas.\nThe rest of the paper is organized as follows: Sect. 2 pro-\nvides background and related work, discussing Pretrained \nLanguage Models, English-to-Arabic machine transla-\ntion, and error analysis. Section  3 details the methodology, \nincluding the dataset, PLM selection and training, and the \nevaluation metrics used. Section  4 presents the results and \nanalysis, comprising a model performance comparison, error \nclassification, and patterns. Section 5 concludes the paper by \nsummarizing the key findings and implications for improv -\ning English-to-Arabic translation. Finally, Sect. 6 offers \nrecommendations and future work, highlighting potential \navenues for enhancing the PLM performance in this lan-\nguage pair.\n2  Background and Related Work\nMachine translation, pioneered in the 1950s, initially saw \nsuccess with the Georgetown-IBM experiment, utilizing \nstatistical algorithms for Russian-to-English translations. \nOver time, this field has diversified into rule-based systems, \nstatistical approaches, neural networks, and Large Language \nModels, each with unique methods for learning and translat-\ning languages. These systems, especially neural networks \nand LLMs, continuously improve their translation accuracy \nby training on extensive datasets and grasping the underlying \nknowledge in the text [8].\nIn this section, we briefly discuss three topics related \nto our research: English-to-Arabic machine translation \nresearch, Pretrained Language Models (PLMs) in machine \ntranslation, and Error Analysis in machine translation.\n2.1  English‑to‑Arabic Machine Translation\nMachine translation from English to Arabic has been an \nactive area of research for several years. Researchers have \nexplored various approaches to tackling the challenges \ninherent in this task, including differences in grammar, \nword order, and vocabulary between the two languages. \nIn addition, many survey papers have been published \non Arabic linguistic characteristics and translation chal-\nlenges. For instance, Ameur et al. [9 ] summarized critical \nresearch on Arabic MT and the available tools/resources \nfor building Arabic MT systems. The survey discussed the \nstate of the field and provided insights into future Arabic \nMT research directions. In addition, Zakraoui et al. [10] \nprovided a comprehensive review comparing different \nNMT approaches for Arabic-English translations. They \ndiscussed approaches addressing linguistic and techni-\ncal challenges, and demonstrated success over traditional \nmethods. Their results will serve to update researchers on \nresources for improving Arabic MT, including corpora, \ntoolkits, techniques, and models.\nRule-based methods, which rely on handcrafted rules \nto analyze the source language and generate the target \nlanguage, have shown some success. Farhat and Al-Taani \n[11] developed a rule-based system that could translate \nsimple English sentences into Arabic with an accuracy of \n85.71%. Similarly, Alawneh et al. [12] combined rule-based \nand example-based English-to-Arabic machine translation \nusing parsing and a hybrid methodology to handle order -\ning and agreement. They evaluated their approach on 250 \ntest samples, and the results achieved 97.2% precision on \naverage. Also, Al-Rukban and Saudagar [13] evaluated three \ncommercial English-to-Arabic systems, Google Translate, \nBing Translator, and Golden Alwafi, and found that Golden \nAlwafi achieved the highest BLEU score, indicating the most \nhuman-like translations. Although rule-based methods are \nstraightforward, they require extensive time to develop and \nmaintain.\nNeural-driven methods, including those based on neu-\nral networks and PLMs, have become increasingly popu-\nlar. Akeel and Mishra [14] developed an English-to-Arabic \ntranslator using both rule-based and neural network meth-\nods, achieving scores of 0.6029 on the n-gram BLUE score \nand 0.8221 on the METEOR metric. Aljohany et al. [ 15] \nproposed a bidirectional model for the translation between \nArabic and English. This model employs a Long Short-\nTerm Memory (LSTM) encoder-decoder with an attention \nmechanism to address the performance degradation linked \nto increased input sentence length. The integration of LSTM \nand attention mechanisms improves the translation accuracy, \nas substantiated by the experimental results, which dem -\nonstrate improved translation precision and reduced loss. \nSome researchers have focused on the challenges of Eng-\nlish-to-Arabic translation and have suggested directions for \nfuture work. Aref et al. [16] outlined a multi-level approach \nto machine translation and reviewed the state of English-\nto-Arabic translation, suggesting the use of AI techniques \nlike knowledge representation to build a prototype system. \nIn contrast, Nagoudi et al. [17] developed TURJUMAN, a \ntoolkit leveraging the Transformer AraT5 model, and trans-\nlated 20 languages into Modern Standard Arabic (MSA).\nTable  1 summarizes the main rule-based and neural \napproaches explored for English-to-Arabic machine trans-\nlation. For each approach, key published works are high-\nlighted, along with their main findings, limitations, and \nopen gaps in the research. This provides a concise overview \nof the current state of English-to-Arabic MT literature to \nidentify promising future research directions. We can see \nthat the key gaps in English-to-Arabic translation include \ninsufficient parallel data, challenges with Arabic morphol-\nogy, and linguistic divergence. Our research utilizes a new \n208 Human-Centric Intelligent Systems (2024) 4:206–219\nEnglish–Arabic parallel corpus and several PLMs models on \nthis data to adapt it and assess the translation performance.\n2.2  Pretrained Language Models (PLMs) in Machine \nTranslation\nPretrained language models are neural network models \ntrained on large amounts of text data in an unsupervised \nmanner. This pre-training process allows the models to \nlearn general linguistic knowledge from the data, including \nsemantics, syntax, and relationships between words. PLMs \ncan then be fine-tuned on downstream supervised tasks, such \nas text classification, question answering, text generation \nand machine translation. Well-known PLMs include BERT, \nGPT-2, and RoBERTa.\nPLMs have shown promising results in Machine Trans-\nlation. For instance, BART (Bidirectional and Auto-\nRegressive Transformer) is a PLM that have been used for \nMT and shown to improve the performance of MT systems \n[18]. Chronopoulou et al. [19] used a language model pre-\ntrained on two languages with large monolingual data to \ninitialize an unsupervised neural machine translation sys-\ntem, which yielded state-of-the-art results. Edunov et al. \n[20] have examined different strategies to integrate pre-\ntrained representations into sequence-to-sequence models \nand applied it to neural machine translation. PLMs have \nalso been used for low-resource machine translation [21], \nsign language translation [22], and code-mixed Hinglish-\nto-English machine translation [23]. However, the suc-\ncessful construction of such models often requires large \namounts of data and computational resources [24].\nTable  2 summarizes the key gaps in using PLMs for \nmachine translation, including the lack of models tailored \nfor particular language pairs, such as English–Arabic, \ninsufficient data and computing access for low-resource \nsettings, and challenges in scaling cross-lingual transfer \nto many languages. Our research seeks to assess the value \nof PLMs, even with limited resources, and evaluate their \nquality.\nTable 1  Summary of key approaches, findings, limitations, and open gaps in English-to-Arabic machine translation literature\nApproach Key works Findings Limitations\nRule- based Farhat and Al-Taani [11]: Rule-based \nsystem for simple English-to-Arabic \ntranslation, 85.71% accuracy\nAlawneh et al. [12]: Combined rule-based \nand example-based approach, 97.2% \nprecision\nAl-Rukban and Saudagar [13]: Evaluated \ncommercial systems, Golden Alwafi had \nhighest BLEU\nCan achieve good accuracy and precision for simple \nsentences\nRequires extensive manual effort for rules\nDo not scale well \nto complex \nsentences\nHard to maintain \nrules over time\nNeural methods Akeel and Mishra [14]: Combined rule-\nbased and neural, BLEU of 0.6029\nAljohany et al. [15]: LSTM encoder-\ndecoder with attention for better long \nsentence translation\nNagoudi et al. [17]: TURJUMAN toolkit \nwith Transformer model\nNeural models outperform rule-based\nAttention mechanisms help with long sentences\nLimited focus on \nEnglish–Arabic \nspecifically\nMore complex \nlinguistic chal-\nlenges not fully \nsolved\nTable 2  Summary of key studies utilizing PLMs for machine translation\nApproach Key works Findings Limitations\nPLM for MT BART model [18] improves MT perfor-\nmance\n Chronopoulou et al. [19]: pretrained LM \nto initialize unsupervised NMT, SOTA \nresults\nPLMs capture linguistic knowledge use-\nful for MT\nCan improve supervised and unsuper-\nvised MT\nRequire large data and compute \nresources [24]\nLow-resource MT PLMs used successfully for low-resource \nMT [21]\nHelp mitigate data scarcity challenges Still limited by small data size\nMultilingual MT Edunov et al. [20]: integrate multilingual \npretrained representations into MT \nmodels\nLeverage cross-lingual transfer learning Difficult to scale to many languages\n209Human-Centric Intelligent Systems (2024) 4:206–219 \n2.3  Error Analysis in Machine Translation\nError analysis in machine translation refers to the process of \nidentifying, categorizing, and understanding errors made by \nMT systems in translating text from one language to another. \nDespite significant improvements in translation algorithms \nand the application of artificial intelligence, MT systems \nare not error-free. Therefore, error analysis in machine \ntranslation serves as an essential process for diagnosing and \nrefining models to improve the quality of translations and \nenhance comprehension.\nMT systems can generate various types of errors owing \nto their language complexity. A study conducted by IBM \nhighlighted the common errors observed in translation out-\nputs when translating Russian into English. These include \nerrors such as transliterated words, multiple meanings and \nambiguities, word order rearrangements, and miscellaneous \ninsertions and corrections [25].\nSeveral error classification schemes have been proposed \n[26]. Popović [26] provided an overview of manual and \nautomatic approaches to error classification and analysis of \nMT. Manual classification allows more error categories, but \nsuffers from cost, time demands, and low annotator consist-\nency. Automatic tools are faster and cheaper but are limited \nin detail and accuracy. Common machine translation error \ntypes reported by Popović include: lexical errors such as \nincorrect word choices or mistranslated terminology; mor -\nphological errors in inflection, derivation, and word com-\nposition; syntactic errors in word order at the word, phrase, \nand sentence levels; semantic errors where the meaning is \nchanged through incorrect disambiguation or mistranslation \nof multi-word expressions; orthographic errors in spelling, \npunctuation, and capitalization; omission errors where words \nor phrases are missing compared to the source; addition \nerrors where extra words or phrases are added; reordering \nerrors where the word, phrase, or clause order differs from \nthe source; and segments with too many errors to classify \nindividually. Popović noted that lexical, morphological, and \nreordering errors are especially problematic for statistical \nmachine translation systems. The distribution of these error \ntypes provides an “error profile” that gives insight into the \nperformance of machine translation systems.\nSimilarly, Chatzikoumi [27] provided a comprehensive \nreview of methods for evaluating machine translation qual-\nity, including both automated metrics, which compare MT \noutput to reference translations and provide advantages \nsuch as speed and low cost but lack nuance and diagnostic \nfeedback, and human evaluation techniques such as direct \nassessment, ranking, error analysis, and post-editing, \nwhich allow for more nuanced judgments but are slower, \nmore costly, and subjective. The paper also discusses \nnumerous error types, including mistranslations convey -\ning incorrect or ambiguous meaning, additions of extra \nwords without basis in the source, omissions of omitted \nwords and phrases, incorrect translation of words that \nshould remain unchanged, morphology errors like incor -\nrect inflection, syntax errors with word order at the word, \nphrase, or sentence level, semantic errors with multiword \nexpressions, collocations, word sense disambiguation, \northography errors in spelling, punctuation, capitalization, \nand fluency issues causing ungrammatical, inconsistent, or \nunreadable output. The paper notes that the typology and \ngranularity of error analysis depend on the specific goals \nof the evaluation, whether improving a particular MT sys-\ntem or general quality assessment. Overall, the wide range \nof errors highlighted illustrates the challenges faced in \nmachine translation and the need for rigorous, multifaceted \nevaluation techniques.\nIn summary, Table  3 provides an overview of the MT \nerror analysis types and evaluation approaches. It is appar -\nent that previous papers emphasize the importance but dif-\nficulty of rigorously evaluating MT quality and analyzing \ntranslation errors. Neural MT shows promise but still pro-\nduces critical errors that require human evaluation. Over -\nall, combining automated metrics and human judgment, \nparticularly for critical semantic errors, provides the most \ncomprehensive MT assessment.\n3  Methodology\nThis section presents the dataset used, the PLMs, and the \nevaluation metrics.\nTable 3  Overview of MT error \nanalysis types and evaluation \napproaches\nCategory Overview\nError types Lexical, morphological, syntactic, semantic, orthographic, \nomission/addition, reordering, mistranslation, etc. [26, 27]\nManual analysis Allows more error categories, but costly, slow, inconsistent [26]\nAutomatic analysis Faster, cheaper, but less detailed and accurate [26]\nEvaluation methods Automatic metrics: Fast but lack nuance\nHuman evaluation: More nuanced but slower/costly [27]\n210 Human-Centric Intelligent Systems (2024) 4:206–219\n3.1  Dataset\nWe used data from the AEPC corpus [28] constructed to \nfill the gap in available Arabic-English corpora to support \ntranslation and language learning. The corpus consists of \na 10-million-word Arabic-English parallel corpus cross-\ning diverse text genres, including: social, biographical, \nliterary, administrative, medical, legal, religious, and sci-\nentific texts. The text was manually translated, segmented \ninto sentences, aligned, and verified for its accuracy. For \nthis research, we chose the following genres: Psychology, \nPolitical, Medical and Scientific domains with 140, 114, \n186, and 102 parallel English–Arabic sentences, respec-\ntively. The selection was based on their diverse and spe-\ncialized vocabulary, differences in syntactic structures, \nand their potential for enhancing global information \naccessibility.\n3.2  PLM Selection\nIn this study, we chose five PLMs to serve as subjects \nof experimentation, with a specific focus on English-\nto-Arabic translation. The models are divided into three \nopen-source models: Helsinki, Marefa, Facebook, and \ntwo closed-source models: GPT-3.5-turbo and GPT-4. We \nalso used Google Translator as a baseline model.\nThe Marefa model [29] was designed to cater to Eng-\nlish-to-Arabic translation tasks. It distinguishes itself by \nincorporating additional Arabic characters, such as “پ ”\nand “گthereby enhancing translation accuracy and pre-\nserving the fidelity of the original content.\nOn the other hand, the Helsinki model [30] also offers \nEnglish-to-Arabic translation capabilities. It adopts \na comprehensive approach to language translation by \nemploying a diverse array of linguistic features and tech-\nniques to achieve proficient results.\nThe Facebook “mBART-50” model [31] represents \na multilingual sequence-to-sequence model that has \nundergone extensive pre-training. Its introduction was \naccompanied by a seminal research paper titled “Beyond \nEnglish-Centric Multilingual Machine Translation” [32].\nGPT-3.5-turbo and GPT-4 were developed by Ope-\nnAI [33]. GPT-3.5-turbo is an extension of GPT-3 that \nimproves its performance and efficiency in natural lan-\nguage understanding and generation tasks, especially for \ndialogue applications. GPT-4 is a multimodal model that \ncan process both image and text inputs and generate text \noutputs, thereby demonstrating human-level capabilities \non various professional and academic benchmarks. Both \nmodels were accessed using OpenAI API.\n3.3  Evaluation Metrics\nThere are several methods for evaluating the performance \nof automatic machine translation systems, including [34]:\n1. Human evaluation involves human judges assessing \nthe quality of the machine translation output. The two \nmain early methods used were ALPAC and DARPA \n[21]. ALPAC focuses on intelligibility (translation \nunderstandable) and fidelity (how much original infor -\nmation is retained). DARPA examines adequacy (how \nmuch information is conveyed), fluency (is the output \ngrammatical), and informativeness (does it provide \ninformation about the system's abilities). However, new \nadvanced methods, including the Multidimensional \nQuality Metrics (MQM) framework [35], which offers a \nversatile and effective evaluation framework for assess-\ning MT quality and transcending language barriers, have \ngained traction among researchers owing to its adapt-\nability and merits.\n2. Automatic evaluation allows for faster evaluation by \ncomparing MT outputs to human references. Some \nMT metrics include ChrF, which focuses on character \nn-grams rather than words to better handle morpho-\nlogically rich languages. BLEU counts the matching \nn-grams between the MT and references. METEOR \nexplicitly matches words and considers their recall and \nprecision. It also matches the stems and synonyms. \nROUGE performs comparisons based on the longest \ncommon subsequence or skip bigrams. Other more \ncomprehensive measures include the COMET and \nBERTscore which are discussed next.\n4  Results and Discussion\nIn this section, the evaluation results of the machine trans-\nlation output using two approaches are presented: auto-\nmatic evaluation and human evaluation.\n4.1  Automatic Evaluation\nchrF (character F-score) [ 36] This metric measures \nthe similarity between the reference translation and the \ncandidate translation at the character level. It calculates \nthe F-score, which combines precision and recall, to \nmeasure overall similarity.\nchr F/u1D6FD=( 1+/u1D6FD2) CHRP × CHRR\n/u1D6FD2 × CHRP + CHRR\n211Human-Centric Intelligent Systems (2024) 4:206–219 \nIn chrF, CHRP and CHRR denote the average preci-\nsion and recall of character n-grams across all n-grams, \nwhere CHRP measures the match percentage of n-grams \nin the hypothesis to the reference, and CHRR measures \nthe match in the reference to the hypothesis. The β param-\neter weights recall β times more than precision, with β = 1 \nindicating equal importance for both.\nBERTScore [37] This metric leverages contextual embed-\ndings generated by BERT, a powerful language model, \nto evaluate the quality of a candidate translation. It com-\npares the candidate translation with the reference transla-\ntion and assigns a similarity score.\nwhere ∣C∣ is the number of tokens in the candidate trans-\nlation, C is the set of token embeddings in the candidate \ntranslation, R is the set of token embeddings in the reference \ntranslation,  ec and  er are the embeddings of tokens c and r, \nand cos is the cosine similarity.\nCOMET (Cross-lingual Optimized Metric for Evaluation \nof Translation) [38] This is a comprehensive metric that \nconsiders different aspects of translation quality, includ-\ning adequacy and fluency. It combines various evaluation \ndimensions to generate an overall score. It uses a neural \nnetwork model, typically involving embedding layers, a \ntransformer-based encoder, and a scoring function to eval-\nuate translation quality. This process involves converting \ninput sentences (source, reference, and hypothesis) into \nvector representations, processing them using the model \nto capture contextual relationships, and then outputting \na quality score. The exact computation depends on the \nmodel architecture and parameters, which are trained on \ndatasets with human translation judgments, making it a \ncomplex and dynamic evaluation method without a sim-\nple, fixed equation.\nThese metrics were chosen for their ability to compre-\nhensively evaluate translation quality, capture nuances at the \ncharacter level, semantic similarity via contextual embed-\ndings, and holistic quality measurements, including fluency \nand adequacy.\nTable 4 shows the evaluation results of the six different \nmachine translation models—Google Translate, Helsinki, \nMarefa, and Facebook; GPT-3.5-Turbo; and GPT-4–across \nfour domains: Psychology, Political, Medical, and Scientific, \nusing three automatic evaluation metrics: ChrF, BERTscore, \nand COMET.\nThe results revealed notable differences in performance \namong the various machine translation models when trans-\nlating from English to Arabic. GPT-4 and gpt-3.5-turbo \nBERTScore = 1\n/uni007C.varC /uni007C.var\n/uni2211.s1\nc∈C\nmax\nr∈R\ncos(ec,er)\nemerged as the top performers, demonstrating superior \ncapabilities across all evaluation metrics. This was particu-\nlarly evident in their handling of semantic and contextual \nnuances, as reflected by their high BERTscore and COMET \nscores. However, the Helsinki model exhibited limitations, \nwith lower average scores in all domains, suggesting poten-\ntial gaps in its translation algorithms or training data for \nthis language pair. Google Translator showed robust perfor-\nmance, especially in semantic and contextual understanding, \nwhich is critical for effectively translating nuanced texts.\nIn terms of domain-specific performance, translations \nwithin the scientific domain achieved the highest average \nChrF scores, indicating a better alignment at the character \nlevel. This could be attributed to the technical and less-idio-\nmatic nature of scientific texts. The Medical domain transla-\ntions showed superior accuracy in context and semantics, as \nevidenced by the highest BERTscore and COMET scores. \nThis suggests that these models are particularly effective for \ntexts with standardized terminology and structures. How -\never, the variability in model performance in the Psychology \nand Political domains implies that these areas possess the \nlinguistic and contextual complexities that current machine \ntranslation models find challenging.\nTable 4  MT automatic evaluation results\nBold font indicates best result obtained\nDomain Model ChrF BERTscore COMET\nPsychology Google Translator 0.518 0.857 0.845\nHelsinki 0.156 0.825 0.764\nMarefa 0.478 0.825 0.841\nFacebook 0.372 0.825 0.841\ngpt-3.5-Turbo 0.608 0.8418 0.851\nGPT-4 0.610 0.8474 0.850\nPolitical Google Translator 0.493 0.846 0.844\nHelsinki 0.081 0.825 0.815\nMarfa 0.476 0.823 0.816\nFacebook 0.488 0.833 0.826\ngpt-3.5-Turbo 0.618 0.8342 0.858\nGPT-4 0.621 0.839 0.857\nMedical Google Translator 0.471 0.847 0.867\nHelsinki 0.410 0.821 0.845\nMarefa 0.403 0.822 0.844\nFacebook 0.426 0.836 0.853\ngpt-3.5-Turbo 0.596 0.8334 0.857\nGPT-4 0.603 0.8426 0.865\nScientific Google Translator 0.556 0.861 0.835\nHelsinki 0.458 0.8235 0.8\nMarefa 0.439 0.8246 0.791\nFacebook 0.442 0.8248 0.804\ngpt-3.5-Turbo 0.615 0.8289 0.805\nGPT-4 0.625 0.8388 0.808\n212 Human-Centric Intelligent Systems (2024) 4:206–219\nOur findings are significant in the broader context of \nmachine translation, particularly for the English-to-Arabic \nlanguage pair, which is often challenged by structural and \ncontextual differences. The high performance of advanced \nmodels such as GPT-4 and gpt-3.5-turbo marks a signifi-\ncant step forward in overcoming language barriers, thereby \nshowcasing the potential of AI in this field. This study also \nhighlights the importance of considering domain-specific \nnuances in machine translation, underlining the need for tai-\nlored approaches and enhancements in translation models \nfor different domains.\nWhen compared with the existing literature, our study \naligns with previous research on the varying efficacy of \ntranslation models across different text types but extends \nthis understanding to a more nuanced analysis of domain-\nspecific performance [39]. The success of models such as \nGPT-4 and GPT-3.5-turbo corroborates recent studies on the \neffectiveness of large-language models in translation tasks. \nConversely, the observed limitations in models, such as Hel-\nsinki, echo broader research challenges, particularly for com-\nplex linguistic structures or lesser-resourced languages. Our \nstudy contributes to this discourse by identifying the specific \ndomains in which these challenges are more pronounced.\n4.2  Human Evaluation\nAs mentioned before, we chose to employ the Multidimen-\nsional Quality Metrics (MQM) framework for conducting \nthe manual evaluation task, focusing on error analysis. The \nMQM framework functions as a valuable instrument for \ndefining and building personalized translation quality met-\nrics. It provides a flexible set of quality issue categories and \na means to utilize them, resulting in the respective quality \nscores.\nThus, to assess the correlation between automatic metrics \nand human judgment in the context of MT, a representative \nsample of 106 sentences was selected from the four domains. \nThe human translations of these sentences were compared \nwith the translations generated by the six MT systems men-\ntioned earlier. Using human translation as a reference point, \nwe can conduct a thorough evaluation of the MT system’s \naccuracy and fluency in capturing the intended meaning and \nlinguistic quality of the source text.\nTable 5 lists the relevant error analysis categories used in \nthis study. We examined the sentences and examined them \nsentence by sentence and word by word to detect any trans-\nlation errors.\nTable 6 presents the error rates for the various machine \ntranslation systems. Each system was used to translate a set \nof sentences and the error rate represented the percentage \nof incorrect translations. Lower percentages indicate bet-\nter performance in terms of accuracy. From this analysis, \nwe can see that Google translate had the lowest error rate, \nindicating that it performed the best among the evaluated \nsystems. On the other hand, Facebook translations exhibited \nthe highest error rate.\nIt is noteworthy that these error rates represent the over -\nall translation quality of each system. Further examination \nof the error distribution reveals that Addition, Omission, \nMistranslation, Untranslated and Grammar categories were \nthe primary sources of errors across the systems, as shown \nin Fig. 1. In summary, the data suggest that Google Trans-\nlate outperforms the other MT systems, whereas Helsinki \nexhibits the highest error rate, indicating potential areas for \nimprovement in these systems.\nBased on the analysis of translation accuracy and fluency \nacross six different machine translation (MT) systems, sev -\neral insights have emerged. Google demonstrated the most \nrobust performance, with the lowest total number of errors, \nTable 5  The set of employed error categories\nMain category Subcategory Definition\nAccuracy Addition The target includes information not present in the source, for example, adding a date that does not exist in the \nsource text to the translation\nOmission Content is missing from the translation that is present in the source, for instance, deleting the negation in the \ntranslation\nMistranslation The target content does not accurately represent the source content\nUntranslated Content that should have been translated has been left untranslated\nFluency Spelling A word is misspelled\nGrammar Issues related to the grammar or syntax of the text such as function words, word order, agreement, tense, and \nparts of speech\nTable 6  Error rates of different machine translation systems\nTranslation system Error rate (%)\nGoogle 32.5\nHelsinki 67.5\nMarefa 67.5\nFacebook 72.5\nGPT-3.5-Turbo 55.0\nGPT4 45.0\n213Human-Centric Intelligent Systems (2024) 4:206–219 \namounting to only 15 across the evaluated categories. In \ncontrast, Helsinki exhibited the highest error count with 53 \nmistakes, suggesting significant challenges in its translation \nmechanism. Marefa and Facebook also show a relatively \nhigh error rate, with 47 and 50 errors respectively, indicating \nareas for improvement, particularly in handling mistransla-\ntions, which emerge as the most common error type across \nall platforms.\nInterestingly, the newer GPT models, GPT-3.5-Turbo and \nGPT4, exhibited intermediate performance. GPT-3.5-Turbo \nregisters 34 errors, while GPT4 accounts for 27, positioning \nthem better than Helsinki and Facebook, but still trailing \nbehind Google's superior accuracy. This pattern underscores \nGoogle's continued leadership in the MT domain despite the \nadvancements and introduction of newer technologies such \nas GPT models.\nMistranslation stands out as a universal challenge for all \nassessed systems, highlighting it as a primary difficulty in \nmachine translation. This consistent issue across different \nplatforms points to the inherent complexities of achiev -\ning accurate and contextually relevant translations. Fur -\nthermore, the specific breakdown of errors such as Gram-\nmar, Omission, and Untranslated sections offers a more \ngranular view of each system's strengths and weaknesses, \nproviding valuable insights for future improvements in \nmachine translation technologies. This comprehensive \nanalysis sheds light on the current state of MT systems, \nrevealing both their achievements and limitations in deal-\ning with language translation tasks.\nTable 7 presents a comprehensive summary of the error \nrates of different machine translation systems across vari-\nous disciplines. This table provides a clear comparison of \nthe performance of each translation system across different \nfields, highlighting the variability and specific challenges \nin each domain. We can observe distinct patterns in trans-\nlation quality across different text genres and translation \nsystems. The following is a detailed analysis.\nFig. 1  Translation error counts \nand distribution of major cat-\negories in machine translation \nsystems\n0\n5\n10\n15\n20\n25\n30\nAddition Omission Mistranslation Untranslated Spelling Grammar\nTable 7  Error rates of different \nmachine translation systems \nacross various disciplines\nDiscipline Google \nerror rate \n(%)\nHelsinki \nerror rate \n(%)\nMarefa error \nrate (%)\nFacebook \nerror rate (%)\nGPT-3.5-Turbo \nerror rate (%)\nGPT4 \nerror rate \n(%)\nMedical 30.0 20.0 30.0 70.0 30.0 10.0\nPsychological 40.0 70.0 80.0 90.0 70.0 50.0\nPolitical 20.0 80.0 60.0 50.0 40.0 30.0\nScientific 40.0 100.0 100.0 80.0 80.0 90.0\n214 Human-Centric Intelligent Systems (2024) 4:206–219\n1. Medical Domain\n• In the medical field, where accuracy is critical, we \nobserved varied performances across different trans-\nlation systems. Helsinki and GPT4 showed promis-\ning results, indicating their potential utility in medical \ntranslation. However, the relatively higher error rates in \nother systems, particularly Facebook, highlight the need \nfor caution, and possibly human oversight, to ensure \nprecision and reliability.\n2. Psychological Domain\n• Psychological texts, characterized by complex and \nnuanced languages, present a significant challenge for \nall translation systems. The higher error rates across \nthe board suggest that machine translations in this field \nrequire extensive review and editing. These findings \nemphasize the importance of understanding the limita-\ntions of machine translation in handling the subtleties \nand specificities of psychological terminology.\n3. Political Domain\n• Political texts, with their nuanced language and con-\ntext sensitivity, show a wide range of performance \namong different systems. Some systems demonstrate \na better grasp of political language, whereas others \nexhibit notable difficulties. This variability under -\nscores the importance of selecting the correct transla-\ntion tool for political content and the need for careful \nreview, especially when the text is intended for sensi-\ntive or critical use.\n4. Scientific Domain\n• The scientific field, known for its technical jargon \nand necessity for precise language, poses the greatest \nchallenge for machine translation systems. The high \nerror rates across almost all systems indicate that \nwhile machine translations can provide a starting \npoint, they often fail to accurately capture the techni-\ncal nuances of scientific texts. This finding reinforces \nthe need for expert review and suggests that relying \nsolely on machine translation in this domain may be \ninadequate.\nIn conclusion, the analysis highlights the critical role of \nhuman oversight, particularly in fields where accuracy and \ncontext are paramount. Machine translations, while benefi-\ncial as a starting point, should be complemented with human \nexpertise to ensure fidelity and accuracy, particularly in sci-\nentific, psychological, and political texts. In the next sec-\ntion, we provide a detailed error analysis of the results to \nunderstand the root causes of such errors.\n4.3  Error Analysis\nTo gain a better understanding of the types of errors commit-\nted by the above machine translation systems, some exam-\nples of classification errors are presented next.\n(1) Analysis of Fluency Errors\nFluency errors address issues related to the grammar or \nsyntax of the text, such as function words, word order, agree-\nment, tense, and parts of speech.\nThe examples in Table 8 highlight several key error cat-\negories in machine translation. A notable issue observed in \nGoogle's translate is the omission of the Arabic article “ال ”,\nreflecting a common grammatical error in translating func-\ntional morphemes. This omission affects the grammatical \nintegrity of the Arabic sentences.\nFacebook's translation exhibits an error in subject-verb \nagreement, particularly with respect to gender, underscor -\ning the challenge of maintaining gender agreement, which is \ncrucial in Arabic grammar. Additionally, Facebook's transla-\ntion demonstrates a problem with word order, an error that \ncan significantly impact the clarity and coherence of trans-\nlated text in Arabic. Another type of error observed in Face-\nbook's translation is the incorrect translation of a singular \nsubject into a plural form, leading to a semantic discrepancy \nthat can alter the intended meaning of the sentence. Further-\nmore, Facebook also shows a tendency to use the wrong \nform of a word, an error that can result in misunderstanding \nor a change in the sentence's intended meaning.\nEach of these errors underlines the specific challenges \nfaced in machine translation, particularly when dealing with \nthe complexities of Arabic language structure, including \ngrammar, word order, and agreement.\n(2) Analysis of Accuracy Errors\nIn the domain of machine translation accuracy, various \nerror categories have been identified for in-depth analysis, \nincluding addition, omission, mistranslation, and issues with \nuntranslated words, as shown in Table 9. This analysis aimed \nto uncover the intricacies and challenges inherent in translat-\ning English and Arabic.\nFor instance, in the realm of addition errors, we observed \na case of medical translation. The original English sentence, \n“What do we mean by body language?”, when translated by \nFacebook, becomes “الجسد؟ لغة باستعمال نعنيه الذي ماHere, the \nphrase “باستعمالby using”) is added, subtly shifting the \nmeaning to “What do we mean by using body language?” \nThis addition, while not drastically altering the message, is \nunnecessary for conveying the fundamental essence of the \nsentence, as demonstrated by a human translator's rendition: \n“؟ الجسد بلغة نقصد ماذا.”\nGoogle’s translation of a psychological sentence further \nillustrates the addition errors. The sentence “These ele-\nments are referred to as ‘para-linguistic cues’” is translated \nto “اللغوية شبه الإشارات 'باسم  العناصر هذه إلى يشارHere, the \ninsertion of “باسمand the definite article “الbefore \n“اللغوية  شبه إشاراتpara-linguistic cues”) implies a \n215Human-Centric Intelligent Systems (2024) 4:206–219 \nTable 8  Examples of MT fluency errors and their issues\nSource/English Arabic/human translation Issue\nI never thought of these as bad qualitiesسيئة صفات أنها على الصفات لهذه أبدًا أنظر ولمGrammar/omitting functional morphemes (article ال)\nGoogle\nسيئة صفات هذه في ابدا افكر لم\nOnly a few weeks after that historic night of December 3, 1967 \na French company offered me $50,000 for the surgical gloves I \nhad worn during the operation\nArabic/human translation Violating subject-verb agreement (masculine and feminine)\nWrong word orderعرضت ،1967 عام من ديسمبر شهر من الثالث التاريخية الليلة تلك من أسابيع فبعد \nفي ارتديتها التي الطبية القفازات مقابل دولار ألف50 مبلغ فرنسية شركة علي \nالعملية\nFacebook\nشركة لي قدم1967 ديسمبر3 من التاريخية الليلة تلك من فقط قليلة أسابيع بعد \nالعملية خلال أرتديها كنت التي الجراحة لقفاز دولار ألف50 فرنسية\nGPT-3.5-Turbo\nلي عرضت ،1967 ديسمبر من الثالث في التاريخية الليلة تلك بعد فقط أسابيع بضعة \nأثناء ارتديتها قد كنت التي الجراحية القفازات مقابل دولار50,000 فرنسية شركة \nالعملية\nThis wondrous organ has only one task to perform in our bod-\nies—it pumps blood\nArabic/ Human Translation Wrong translation of singular subject أعضاء → عضو\nالدم ضخ: أجسامنا في يؤديها واحدة وظيفة لديه للعجب المثير العضو وهذا\nFacebook\nالدم تضخ أنها—أجسادنا في بها لتقوم فقط واحدة مهمة لديها المذهلة الأعضاء هذه\nExpressions—the arrangement of the face and eye movements \nthat convey a great deal of the meaning of a communication\nArabic/human translation Using wrong form of the word\nالمعاني من كبيرًا مقدارًا تنقل التي والعين الوجه حركات ترتيب هي—العبارات \nللاتصال\nFacebook\nالتواصل معنى من الكثير تنقل التي والعين الوجه حركة ترتيب- التعبيرات\n216 Human-Centric Intelligent Systems (2024) 4:206–219\nspecificity and recognition in Arabic that the original Eng-\nlish sentence does not suggest.\nSimilarly, GPT-3.5-Turbo’s translation of “In a crisis, \ncash becomes king” to “الأهم هو النقد يصبح الأزمات، حالة في ”\nintroduces the word “حالةcase” or “situation”) unneces-\nsarily, as it was absent in the original English phrase.\nOn the omission front, Helsinki's translation of “What \nwe tend to do is interpret what people are saying with \nreference to their body language” to “تفسير هو فعله إلى نميل ما \nلغتهم إلى بالإشارة الناس يقوله ماmisses the critical word “body” \nbefore “language.” This omission significantly alters the \nintended meaning and reduces the specificity of interpret-\ning verbal communication alongside body language.\nMoreover, Marefa’s translation of “I simply have not been \nable to come up with anything that would have made a dif-\nference” as “أن شأنه من كان شيء أي إلى التوصل من ببساطة تمكنت لقد \nفرقاً يحدثomits the crucial word “not.” This negates the \nintended meaning and falsely suggests that the speaker was \nable to come up with a significant idea, contrary to the \nsource's expression of inability.\nAccording to the issue of mistranslation, the original Eng-\nlish sentence expresses amazement with a straightforward \nstructure. The translation provided by Facebook does not \naccurately represent source content in several ways. Firstly, \nthe phrase “مذهلاً الأمر يبدوThe matter seems amazing”) \nchanges the nuance of the sentence. The original phrase “It \nTable 9  Examples of MT accuracy errors and their issues\nAccuracy error Source/English Arabic/human translation Issue\nAddition What do we mean by body language? “؟ الجسد بلغة نقصد ماذاAddition of “باستعمالby using”) alters \nthe meaningFacebook\nالجسد؟ لغة باستعمال نعنيه الذي ما\nThese elements are referred to as ‘para-\nlinguistic cues’\nArabic/human translation Addition of “باسمand “الchanges the \nspecific reference and meaningلفظية شبه إشارات\"العناصر هذه وتعتبر\nGoogle\nشبه الإشارات \"باسم العناصر هذه إلى يشار \nاللغوية\"\nIn a crisis, cash becomes king Arabic/human translation Inclusion of “حالةcase/situation”) \nintroduces an unnecessary elementالحاكم هو المال يصبح الأزمات في\nGPT-3.5-Turbo\nالأهم هو النقد يصبح الأزمات، حالة في\nOmission What we tend to do is interpret what \npeople are saying with reference to \ntheir body language\nArabic/human translation Omission of “body” before “language,” \naltering the intended meaningإلى بالرجوع الناس مايقوله تفسير إلى نميل فنحن \nجسدهم لغة\nHelsinki\nبالإشارة الناس يقوله ما تفسير هو فعله إلى نميل ما \nلغتهم إلى\nI simply have not been able to come up \nwith anything that would have made \na difference\nArabic/human translation Omission of “not,” changing the mean-\ning to the opposite of what is intendedشيء أي إلى التوصل على قادرًا أكن لم ببساطة \nفارقاً يشكل أن الممكن من كان\nMarefa\nكان شيء أي إلى التوصل من ببساطة تمكنت لقد \nفرقا يحدث أن شأنه من\nMistranslation It seems incredible, but it contains more \nthan 300 references to the heart\nArabic/human translation The target content does not accurately \nrepresent the source contentمن أكثر على يحتوي أنه أجد أن المذهل من وكان \nللقلب إشارة300\nFacebook\n300 من أكثر على يحتوي لكنه مذهلاً، الأمر يبدو \nللقلب مرجعاً\nUntranslated word How NLP contributes to understanding \nbody language\nArabic/ Human Translation Untranslated word\nلغة فهم في العصبية اللغوية البرمجة تساهم كيف \nالجسد\nFacebook\nNLP الـ تساهم كيف\nالجسد لغة فهم في\n217Human-Centric Intelligent Systems (2024) 4:206–219 \nseems incredible” conveys a sense of disbelief or astonish-\nment at the quantity of references. However, the Facebook \ntranslation shifts this to a more general sense of amazement, \nlosing the subtlety of incredulity present in the original ver-\nsion. Secondly, the use of “مرجعاًreferences) in the transla-\ntion may not capture the intended nuance of the English \nword “references.” In English, “references to the heart” can \nimply various types of mentions or allusions, not just formal \ncitations or sources. The Arabic translation could be inter -\npreted as more formal or academic, which might not per -\nfectly align with the original English expression.\nIn the untranslated word issue, the acronym “NLP” is left \nuntranslated. This untranslated word presents a significant \nissue in terms of accessibility and understanding for Arabic-\nspeaking audiences, who may not be familiar with the Eng-\nlish acronym. “NLP” stands for “Neuro-Linguistic Program-\nming,” a concept that would typically be translated into \nArabic as “العصبية اللغوية البرمجةLeaving “NLP” untranslated \ncan potentially lead to confusion or misinterpretation among \nreaders who are not accustomed to English acronyms or who \nmay not recognize the acronym in the context of the Arabic \nlanguage. It is crucial in translation, especially when dealing \nwith technical or specialized terms, to ensure that such terms \nare appropriately translated to maintain the clarity and com-\nprehensibility of the content for the target audience.\nEach of these examples underscores the nuanced chal-\nlenges in machine translation, especially in dealing with \nArabic's complex grammar, word order, and agreement. \nThese findings highlight the need for deeper understanding \nand more sophisticated approaches to overcome these chal-\nlenges and ensure more accurate and effective translations.\n5  Conclusion and Future Work\nIn conclusion, this study investigated the performance of \nstate-of-the-art pretrained language models (PLMs) on \nEnglish-to-Arabic machine translation across diverse text \ngenres. Our analysis revealed that advanced models, such \nas GPT-4 and GPT-3.5-turbo, demonstrate superior transla-\ntion capabilities based on automatic metrics, such as chrF, \nBERTscore, and COMET. However, all systems still face \nchallenges in accurately conveying complex grammar, \nvocabulary, and meaning when handling texts in domains \nsuch as psychology and political science.\nThrough manual evaluation using the MQM framework, \nwe identified lexical, morphological, syntactic, semantic, \nand fluency issues that pose difficulties to PLMs. Key error \npatterns include omissions or incorrect translations of \nfunctional words, lack of subject-verb gender agreement, \nword order rearrangements, mistranslations from ambigu-\nity, and spelling/grammar mistakes that disrupt fluency. \nGoogle Translate is currently the most robust, whereas \nthe other systems show higher domain-specific variability.\nIn summary, our study demonstrates the promise of pre-\ntrained models in advancing neural machine translation \nquality but also highlights persistent limitations in terms \nof Arabic grammar, terminology, and contextual nuances. \nAs English–Arabic machine translation holds significance \nfor global communication and cooperation, our analysis \noffers valuable insights into pressing challenges and future \npriorities for the field. Specifically, our work highlights the \nneed for continued research on tailored architectures, mul-\ntilingual representations, contextual encoding, and special-\nized model training to further enhance English-to-Arabic \ntranslation performance across diverse real-world texts.\nAcknowledgements We acknowledge the use of ChatGPT, an AI chat-\nbot developed by OpenAI, for generating some of the summaries in this \narticle. ChatGPT was used to supplement our own writing and analysis, \nand not to replace them. We verified the accuracy and relevance of the \nAI-generated text before incorporating it into our manuscript.\nAuthor Contributions HK: methodological framework, experiments, \nevaluation, writing preliminary manuscript draft; revising final manu-\nscript; supervision; KK: experimental result analysis, reviewing and \nediting manuscript. HH experiments; All authors approved the final \nversion of the manuscript.\nFunding This study was funded by the Literature, Publishing and \nTranslation Commission, Ministry of Culture, Kingdom of Saudi Ara-\nbia under [73/2022] as part of the Arabic Observatory of Translation.\nAvailability of Data and Materials No underlying data were collected \nor produced for this study.\nDeclarations \nConflict of interest The authors declare no conflicts of interest.\nEthical approval Not Applicable.\nConsent to participate Not Applicable.\nConsent for publication The authors hereby grant full consent for the \npublication of the manuscript in the HCIN journal.\nOpen Access  This article is licensed under a Creative Commons Attri-\nbution 4.0 International License, which permits use, sharing, adapta-\ntion, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, \nprovide a link to the Creative Commons licence, and indicate if changes \nwere made. The images or other third party material in this article are \nincluded in the article's Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in \nthe article's Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a \ncopy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n218 Human-Centric Intelligent Systems (2024) 4:206–219\nReferences\n 1. Zaugg IA, Hossain A, Molloy B. Digitally-disadvantaged lan-\nguages. Internet Policy Rev J Internet Regul. 2022;11(2):1–11.\n 2. Patil A, Joshi I, Kadam D. PICT@WAT 2022: neural machine \ntranslation systems for indic languages. In: Proceedings of the 9th \nworkshop on Asian Translation, Gyeongju, Republic of Korea: \ninternational conference on computational linguistics. 2022. pp. \n106–110. https:// aclan tholo gy. org/ 2022. wat-1. 13. Accessed 20 \nDec 2023.\n 3. Chen K, Wang R, Utiyama M, Sumita E. Integrating prior transla-\ntion knowledge into neural machine translation. IEEEACM Trans \nAudio Speech Lang Process. 2022;30:330–9. https:// doi. org/ 10. \n1109/ TASLP. 2021. 31387 14.\n 4. Akan MF, Karim MR, Chowdhury AMK. An analysis of Arabic–\nEnglish translation: problems and prospects. Adv Lang Lit Stud. \n2019;10(1):58–65. https:// doi. org/ 10. 7575/ aiac. alls.v. 10n. 1p. 58.\n 5. Mamoori MMA, Tarish AH, Hasani SA. Difficulties of translation \nand evaluative idioms in English and Arabic. Int J Health Sci. \n2022. https:// doi. org/ 10. 53730/ ijhs. v6nS5. 10039.\n 6. Mars M. From word embeddings to pre-trained language models: \na state-of-the-art walkthrough. Appl Sci. 2022;12(17):art no. 17. \nhttps:// doi. org/ 10. 3390/ app12 178805.\n 7. Zakraoui J, Saleh M, Al-Maadeed S, AlJa’am JM. Evaluation \nof Arabic to English machine translation systems. In: 2020 11th \nInternational conference on information and communication sys-\ntems (ICICS). 2020. pp. 185–190. https:// doi. org/ 10. 1109/ ICICS \n49469. 2020. 239518.\n 8. Bar-Hillel Y. The Present status of automatic translation of lan-\nguages. In: Alt FL, editors. Advances in computers, vol. 1. Else-\nvier; 1960. pp. 91–163. https:// doi. org/ 10. 1016/ S0065- 2458(08) \n60607-5.\n 9. Ameur MSH, Meziane F, Guessoum A. Arabic machine transla-\ntion: a survey of the latest trends and challenges. Comput Sci Rev. \n2020;38: 100305. https:// doi. org/ 10. 1016/j. cosrev. 2020. 100305.\n 10. Zakraoui J, Saleh M, Al-Maadeed S, Alja’am JM. Arabic machine \ntranslation: a survey with challenges and future directions. IEEE \nAccess. 2021;9:161445–68. https:// doi. org/ 10. 1109/ ACCESS. \n2021. 31324 88.\n 11. Farhat A, Al-Taani AT. A rule-based English to Arabic machine \ntranslation approach. In: Presented at the international Arab con-\nference on information technology (ACIT’2015). 2015. https://  \nwww. seman ticsc holar. org/ paper/A- Rule- based- Engli sh- to- Ara-\nbic- Machi ne- Trans lation- Farhat- Al- Taani/ 4e7f5 55a02 21eb7 f980c \n597b1 5bdb8 f6a10 89e7f. Accessed 16 Jul 2023.\n 12. Fadiel Alawneh M, Sembok TM, Mohd M. Grammar-based and \nexample-based techniques in machine translation from English \nto Arabic. In: 2013 5th international conference on information \nand communication technology for the Muslim World (ICT4M). \n2013. pp. 1–6. https:// doi. org/ 10. 1109/ ICT4M. 2013. 65189 10.\n 13. Al-Rukban A, Saudagar AKJ. Evaluation of English to Arabic \nmachine translation systems using BLEU and GTM. In: Proceed-\nings of the 2017 9th international conference on education tech-\nnology and computers. ACM; 2017.\n 14. Akeel M, Mishra R. ANN and rule based method for eng-\nlish to arabic machine translation. Int Arab J Inf Technol. \n2014;11(4):396–405.\n 15. Aljohany DA, Al-Barhamtoshy HM, Abukhodair FA. Arabic \nmachine translation (ArMT) based on LSTM with attention \nmechanism architecture. In: 2022 20th International conference \non language engineering (ESOLEC). 2022. pp. 78–83. https:// doi. \norg/ 10. 1109/ ESOLE C54569. 2022. 10009 530.\n 16. Aref M, Al-Mulhem M, Al-Muhtaseb H. English to Arabic \nmachine translation: a critical review and suggestions for devel-\nopment. King Fahd Univ. Pet. Miner. Dhahran Saudi Arab. 1992.\n 17. Nagoudi EMB, Elmadany A, Abdul-Mageed M. TURJUMAN: \na public toolkit for neural Arabic machine translation. In: Pro-\nceedings of the 5th workshop on open-source Arabic corpora and \nprocessing tools with shared tasks on Qur’an QA and fine-grained \nhate speech detection. Marseille, France: European Language \nResources Association; 2022. pp. 1–11. https:// aclan tholo gy. org/ \n2022. osact-1.1. Accessed 16 Jul 2023.\n 18. Lewis M, et al. BART: denoising sequence-to-sequence pre-train-\ning for natural language generation, translation, and comprehen-\nsion. In: Proceedings of the 58th annual meeting of the associa-\ntion for computational linguistics. Association for Computational \nLinguistics; 2020. pp. 7871–7880. https:// doi. org/ 10. 18653/ v1/ \n2020. acl- main. 703.\n 19. Chronopoulou A. Stojanovski D, Fraser A. Reusing a pretrained \nlanguage model on languages with limited corpora for unsuper -\nvised NMT. In: Proceedings of the 2020 conference on empirical \nmethods in natural language processing (EMNLP). Association \nfor Computational Linguistics; 2020. pp. 2703–2711. https:// doi. \norg/ 10. 18653/ v1/ 2020. emnlp- main. 214.\n 20. Edunov S, Baevski A, Auli M. Pre-trained language model rep-\nresentations for language generation. In: Proceedings of the 2019 \nconference of the North American chapter of the association for \ncomputational linguistics: human language technologies, volume \n1 (long and short papers). Minneapolis, Minnesota: Association \nfor Computational Linguistics; 2019. pp. 4052–4059. https:// doi. \norg/ 10. 18653/ v1/ N19- 1409.\n 21. Zheng F, Reid M, Marrese-Taylor E, Matsuo Y. Low-resource \nmachine translation using cross-lingual language model pretrain-\ning. In: Proceedings of the first workshop on natural language \nprocessing for indigenous languages of the Americas. Association \nfor Computational Linguistics; 2021. pp. 234–240. https:// doi. org/ \n10. 18653/ v1/ 2021. ameri casnlp- 1. 26.\n 22. De Coster M, Dambre J. Leveraging frozen pretrained written \nlanguage models for neural sign language translation. Information. \n2022;13(Art. no. 5):5. https:// doi. org/ 10. 3390/ info1 30502 20.\n 23. Agarwal V, Rao P, Jayagopi DB (2023) Hinglish to English \nmachine translation using multilingual transformers. In: Proceed-\nings of the student research workshop associated with RANLP \n2021, INCOMA Ltd., Sep. 2021, pp. 16–21. https:// aclan tholo gy. \norg/ 2021. ranlp- srw.3. Accessed 16 Jul 2023.\n 24. Jude Ogundepo O, Oladipo A, Adeyemi M, Ogueji K, Lin J. \nAfriTeVA: Extending? Small data? Pretraining approaches to \nsequence-to-sequence models. In: Proceedings of the third work-\nshop on deep learning for low-resource natural language process-\ning, hybrid. Association for Computational Linguistics; 2022. pp. \n126–135. https:// doi. org/ 10. 18653/ v1/ 2022. deeplo- 1. 14.\n 25. Hutchins WJ. Machine translation: a brief history. In: Koerner \nEFK, Asher RE, editors. Concise history of the language sciences. \nAmsterdam: Pergamon; 1995. p. 431–45. https:// doi. org/ 10. 1016/ \nB978-0- 08- 042580- 1. 50066-0.\n 26. Popović M. Error classification and analysis for machine transla-\ntion quality assessment. In: Moorkens J, Castilho S, Gaspari F, \nDoherty S, editors. Translation quality assessment: from princi-\nples to practice. Machine translation: technologies and applica -\ntions. Cham: Springer International Publishing; 2018. pp. 129–\n158. https:// doi. org/ 10. 1007/ 978-3- 319- 91241-7_7.\n 27. Chatzikoumi E. How to evaluate machine translation: a review of \nautomated and human metrics. Nat Lang Eng. 2020;26(2):137–61. \nhttps:// doi. org/ 10. 1017/ S1351 32491 90004 69.\n 28. Alotaibi H (2023) Arabic-English parallel corpus: a new resource \nfor translation training and language teaching. Arab World Engl J \nAWEJ 2017;8(3). https:// awej. org/ arabic- engli sh- paral lel- corpus- \na- new- resou rce- for- trans lation- train ing- and- langu age- teach ing/. \nAccessed 26 Jul 2023.\n 29. marefa-nlp/marefa-mt-en-ar · Hugging Face. https:// huggi ngface. \nco/ marefa- nlp/ marefa- mt- en- ar. Accessed 19 Jul 2023.\n219Human-Centric Intelligent Systems (2024) 4:206–219 \n 30. Helsinki-NLP/opus-mt-tc-big-ar-en · Hugging Face. https:// huggi \nngface. co/ Helsi nki- NLP/ opus- mt- tc- big- ar- en. Accessed 19 Jul \n2023.\n 31. facebook/m2m100_1.2B · Hugging Face. https:// huggi ngface. co/ \nfaceb ook/ m2m100_ 1. 2B. Accessed 19 Jul 2023.\n 32. Fan A, et al. Beyond English-centric multilingual machine transla-\ntion. J Mach Learn Res. 2021;22(1):107:4839-107:4886.\n 33. OpenAI Platform. https:// platf orm. openai. com. Accessed 26 Jul \n2023.\n 34. Mondal SK, Zhang H, Kabir HMD, Ni K, Dai H-N. \nMachine translation and its evaluation: a study. Artif Intell \nRev. 2023;56(9):10137–226. https:// doi. org/ 10. 1007/  \ns10462- 023- 10423-5.\n 35. Lommel A. Metrics for translation quality assessment: a case \nfor standardising error typologies. In: Moorkens J, Castilho S, \nGaspari F, Doherty S, editors. Translation quality assessment: \nfrom principles to practice. Machine translation: technologies and \napplications. Cham: Springer International Publishing; 2018, pp. \n109–127. https:// doi. org/ 10. 1007/ 978-3- 319- 91241-7_6.\n 36. Popović M. chrF: character n-gram F-score for automatic MT \nevaluation. In: Bojar O, Chatterjee R, Federmann C, Haddow B, \nHokamp C, Huck M, Logacheva V, Pecina P, editors. Proceedings \nof the tenth workshop on statistical machine translation. Lisbon, \nPortugal: Association for Computational Linguistics; 2015. pp. \n392–395. https:// doi. org/ 10. 18653/ v1/ W15- 3049.\n 37. Zhang T, Kishore V, Wu F, Weinberger KQ, Artzi Y. BERTScore: \nevaluating text generation with BERT. 2020. https:// doi. org/ 10. \n48550/ arXiv. 1904. 09675.\n 38. Rei R, Stewart C, Farinha AC, Lavie A. COMET: a neural frame-\nwork for MT evaluation. 2020. https:// doi. org/ 10. 48550/ arXiv. \n2009. 09025.\n 39. Lyu C, Xu J, Wang L. New trends in machine translation using \nlarge language models: case examples with ChatGPT. 2023. \nhttps:// doi. org/ 10. 48550/ arXiv. 2305. 01181.\nPublisher's Note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.",
  "topic": "Machine translation",
  "concepts": [
    {
      "name": "Machine translation",
      "score": 0.7842987179756165
    },
    {
      "name": "Computer science",
      "score": 0.7830626964569092
    },
    {
      "name": "Natural language processing",
      "score": 0.7298194169998169
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6172751784324646
    },
    {
      "name": "Arabic",
      "score": 0.5545467734336853
    },
    {
      "name": "Evaluation of machine translation",
      "score": 0.5037967562675476
    },
    {
      "name": "Grammar",
      "score": 0.4910316467285156
    },
    {
      "name": "Vocabulary",
      "score": 0.44468700885772705
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.4412161111831665
    },
    {
      "name": "Machine translation software usability",
      "score": 0.40052998065948486
    },
    {
      "name": "Linguistics",
      "score": 0.304485023021698
    },
    {
      "name": "Example-based machine translation",
      "score": 0.22667944431304932
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I28022161",
      "name": "King Saud University",
      "country": "SA"
    },
    {
      "id": "https://openalex.org/I240666556",
      "name": "Imam Mohammad ibn Saud Islamic University",
      "country": "SA"
    }
  ]
}