{
  "title": "Fine-tuning large neural language models for biomedical natural language processing",
  "url": "https://openalex.org/W4365511667",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3091566229",
      "name": "Robert Tinn",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2005151083",
      "name": "Cheng Hao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2117761656",
      "name": "Yu Gu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2919329533",
      "name": "Naoto Usuyama",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101917160",
      "name": "Xiaodong Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2134067980",
      "name": "Tristan Naumann",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2104437897",
      "name": "Jian-Feng Gao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2032387740",
      "name": "Hoifung Poon",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2123579157",
    "https://openalex.org/W6798337015",
    "https://openalex.org/W6766673545",
    "https://openalex.org/W3112929524",
    "https://openalex.org/W6779629370",
    "https://openalex.org/W6779469252",
    "https://openalex.org/W2735784619",
    "https://openalex.org/W6786240170",
    "https://openalex.org/W6781031682",
    "https://openalex.org/W6784400365",
    "https://openalex.org/W6750615492",
    "https://openalex.org/W6796255299",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W6764537895",
    "https://openalex.org/W6776129198",
    "https://openalex.org/W3173561451",
    "https://openalex.org/W6761260114",
    "https://openalex.org/W3045332379",
    "https://openalex.org/W3036559261",
    "https://openalex.org/W3024305464",
    "https://openalex.org/W6779145773",
    "https://openalex.org/W6779411778",
    "https://openalex.org/W6785006811",
    "https://openalex.org/W6782942446",
    "https://openalex.org/W6790003725",
    "https://openalex.org/W6931652666",
    "https://openalex.org/W6931515709",
    "https://openalex.org/W6912669300",
    "https://openalex.org/W6704842505",
    "https://openalex.org/W2169099542",
    "https://openalex.org/W2154142897",
    "https://openalex.org/W6662590293",
    "https://openalex.org/W6750203311",
    "https://openalex.org/W2170189740",
    "https://openalex.org/W2136437513",
    "https://openalex.org/W2034269086",
    "https://openalex.org/W6767102903",
    "https://openalex.org/W6780237715",
    "https://openalex.org/W6638575559",
    "https://openalex.org/W6753720546",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6718053083",
    "https://openalex.org/W6750535842",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W6692408643",
    "https://openalex.org/W3015233032",
    "https://openalex.org/W2964179635",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W2346452181",
    "https://openalex.org/W3184705639",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3106224367",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W2962784628"
  ],
  "abstract": "Large neural language models have transformed modern natural language processing (NLP) applications. However, fine-tuning such models for specific tasks remains challenging as model size increases, especially with small labeled datasets, which are common in biomedical NLP. We conduct a systematic study on fine-tuning stability in biomedical NLP. We show that fine-tuning performance may be sensitive to pretraining settings and conduct an exploration of techniques for addressing fine-tuning instability. We show that these techniques can substantially improve fine-tuning performance for low-resource biomedical NLP applications. Specifically, freezing lower layers is helpful for standard BERT- <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mrow><mml:mi>B</mml:mi> <mml:mi>A</mml:mi> <mml:mi>S</mml:mi> <mml:mi>E</mml:mi></mml:mrow> </mml:math> models, while layerwise decay is more effective for BERT- <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mml:mrow><mml:mi>L</mml:mi> <mml:mi>A</mml:mi> <mml:mi>R</mml:mi> <mml:mi>G</mml:mi> <mml:mi>E</mml:mi></mml:mrow> </mml:math> and ELECTRA models. For low-resource text similarity tasks, such as BIOSSES, reinitializing the top layers is the optimal strategy. Overall, domain-specific vocabulary and pretraining facilitate robust models for fine-tuning. Based on these findings, we establish a new state of the art on a wide range of biomedical NLP applications.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6717923283576965
    },
    {
      "name": "Natural (archaeology)",
      "score": 0.47431325912475586
    },
    {
      "name": "Natural language processing",
      "score": 0.450074702501297
    },
    {
      "name": "Language model",
      "score": 0.44925129413604736
    },
    {
      "name": "Artificial neural network",
      "score": 0.4409593939781189
    },
    {
      "name": "Natural language",
      "score": 0.4408155083656311
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4267578125
    },
    {
      "name": "Biology",
      "score": 0.08598637580871582
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1290206253",
      "name": "Microsoft (United States)",
      "country": "US"
    }
  ],
  "cited_by": 129
}