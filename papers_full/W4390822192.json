{
  "title": "Probabilistic Temporal Fusion Transformers for Large-Scale KPI Anomaly Detection",
  "url": "https://openalex.org/W4390822192",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2482627258",
      "name": "Haoran Luo",
      "affiliations": [
        "China Telecom (China)",
        "China Telecom"
      ]
    },
    {
      "id": "https://openalex.org/A2922771562",
      "name": "Yongkun Zheng",
      "affiliations": [
        "China Telecom",
        "China Telecom (China)"
      ]
    },
    {
      "id": "https://openalex.org/A1995635696",
      "name": "Kang Chen",
      "affiliations": [
        "China Telecom",
        "China Telecom (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2039392868",
      "name": "Shuo Zhao",
      "affiliations": [
        "China Telecom (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2785362611",
    "https://openalex.org/W1894414046",
    "https://openalex.org/W3171884590",
    "https://openalex.org/W6679436768",
    "https://openalex.org/W2980994438",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W6640212811",
    "https://openalex.org/W6754779804",
    "https://openalex.org/W6640963894",
    "https://openalex.org/W6764679822",
    "https://openalex.org/W2949468773",
    "https://openalex.org/W2737654582",
    "https://openalex.org/W2964250810",
    "https://openalex.org/W6746729860",
    "https://openalex.org/W3175091623",
    "https://openalex.org/W2508465325",
    "https://openalex.org/W3100178186",
    "https://openalex.org/W4226214891",
    "https://openalex.org/W2014123937",
    "https://openalex.org/W1480783376",
    "https://openalex.org/W1984367804",
    "https://openalex.org/W2121353572",
    "https://openalex.org/W2005864863",
    "https://openalex.org/W2022916695",
    "https://openalex.org/W2053449529",
    "https://openalex.org/W2278984902",
    "https://openalex.org/W2952263048",
    "https://openalex.org/W2100495367",
    "https://openalex.org/W2149427297",
    "https://openalex.org/W2808535700",
    "https://openalex.org/W2986815055",
    "https://openalex.org/W3184338320",
    "https://openalex.org/W2948517885",
    "https://openalex.org/W3177318507",
    "https://openalex.org/W3173539742",
    "https://openalex.org/W6685562342",
    "https://openalex.org/W2143612262",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2024760831",
    "https://openalex.org/W2522722940",
    "https://openalex.org/W1924770834",
    "https://openalex.org/W4391602018",
    "https://openalex.org/W2900504162",
    "https://openalex.org/W2811507150",
    "https://openalex.org/W2963285578",
    "https://openalex.org/W2773625660",
    "https://openalex.org/W3098957257",
    "https://openalex.org/W3105931142"
  ],
  "abstract": "This paper introduces a new generic and scalable framework for large-scale time series prediction and unsupervised anomaly detection. The most common approach of state-of-the-art time series anomaly detection techniques, which are mostly based on neural networks, is to train a network per time series. However, a typical modern microservice system consists of hundreds of active nodes/instances. To monitor the performance of such a system, we often need to keep track of thousands of time series describing different aspects of the system, including CPU usage, call latency, and workloads. We introduce a new methodology for grouping metrics that share the same type, predicting hundreds of metrics concurrently with a single neural network model with shared parameters. The model also integrates the probabilistic representations and Temporal Fusion Transformers for better performance. In a real-world dataset, our proposed model achieved up to 50&#x0025; improvement in terms of MSE.",
  "full_text": "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identifier 10.1109/ACCESS.2017.DOI\nProbabilistic Temporal Fusion\nTransformers for Large-Scale KPI\nAnomaly Detection\nHAORAN LUO1 (Member, IEEE), YONGKUN ZHENG1, KANG CHEN1 (Member, IEEE), SHUO\nZHAO2\n1Research Institute of China Telecom Company Ltd., Guangzhou 510630, China\n2China Telecom Corporation Ltd., 31 Jinrong Street, Xicheng District, Beijing 100033, China\nCorresponding author: Haoran Luo (e-mail: luohr1@chinatelecom.cn).\nABSTRACT This paper introduces a new generic and scalable framework for large-scale time series\nprediction and unsupervised anomaly detection. The most common approach of state-of-the-art time series\nanomaly detection techniques, which are mostly based on neural networks, is to train a network per time\nseries. However, a typical modern microservice system consists of hundreds of active nodes/instances. To\nmonitor the performance of such a system, we often need to keep track of thousands of time series describing\ndifferent aspects of the system, including CPU usage, call latency, and workloads. We introduce a new\nmethodology for grouping metrics that share the same type, predicting hundreds of metrics concurrently\nwith a single neural network model with shared parameters. The model also integrates the probabilistic\nrepresentations and Temporal Fusion Transformers for better performance. In a real-world dataset, our\nproposed model achieved up to 50% improvement in terms of MSE.\nINDEX TERMS Deep learning, Time series, Anomaly diagnosis, microservice systems\nI. INTRODUCTION\nModern microservice systems consist of thousands of ser-\nvices, grouped by hundreds of subsystems. Generally, each\nservice runs on a different container, and those containers\ncan be dynamically created or destroyed according to some\nscaling configurations. It is challenging for the operator to\ndetect operational issues and perform swift troubleshooting\nin such a complex environment.\nDuring runtime, an operator can extract the metrics gen-\nerated by nodes and services, such as CPU/Mem usage,\nNetwork I/O and the number of requests per second. Combin-\ning the observations from different timestamps forms some\ntime series; by analyzing such series, an operator can decide\nwhether the system is in an abnormal state; predicting the\nfuture values of time series also helps the operator discover\nthe hidden risks before it actually break the system.\nTraditional time series analysis methods require intensive\nhuman effort. To analyze the performance of a certain ser-\nvice, an operator must observe the time series generated by\nthat service, then set the thresholds manually according to\nhis/her experience. However, in modern microservice sys-\ntems, the great quantity of pods and containers makes the\nmanual method unacceptable. Operators require an unsu-\npervised method, which could automatically extract proper\nthresholds from previous time series with minimal human\ninference.\nA rich body of literature investigates unsupervised time-\nseries anomaly detection tasks. Xu et.al. proposed a model\nDonut [1] based on variational auto-encoders (V AE), which\nachieved 0.9 F-score in an unsupervised training setup. Ma\net. al. proposed an approach based on a one-hot support vec-\ntor machine to identify unforeseen or abnormal sections in a\ntime series. Other unsupervised methods, such as clustering,\nare also being actively researched according to [2].\nHowever, as the size of modern microservice system\ngrowing larger, the above methods reached a performance\nbottleneck. For example, training a single V AE or clustering\nmodel is not trivial; training different models (especially\nneural networks) for hundreds of sequences and predict them\nsimultaneously becomes unacceptable in modern production\nsettings. Therefore, we try to find a universal model for the\nsame type of metrics in a microservice system.\nThe model should have the ability to effectively distinguish\nthe “static” parts in different time series, and itself should\nVOLUME 4, 2016 1\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3353201\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nHaoran Luo et al.: Probabilistic Temporal Fusion Transformers for Large-Scale KPI Anomaly Detection\nbe complex enough to hold time series with different char-\nacteristics. A competitive model is Temporal Fusion Trans-\nformer(TFT) [3], which integrates static info and LSTM\nlayers into transformer structure, and combines them via\nresidual layers. Based on the idea of TFT, we tested and\noptimized it according to the requirements in production\nenvironment of microservice system.\nThe contributions of this paper can be summarized as\nfollows.\n• We show that Temporal Fusion Transformers can be\nthe state-of-the-art model for unsupervised anomaly\ndetection tasks in the DevOps field. We prove this by\ncomparison with other models on real-world datasets\ncollected from production.\n• We enhance the Temporal Fusion Transformer model\nby integrating input sequences in a probabilistic way,\nwhich improves performance.\n• We propose a framework called TFTOps that is suit-\nable for near real-time, concurrent anomaly detection of\nhundreds of different series.\nII. RELATED WORK\nIn this section, we will discuss the development of anomaly\ndetection methods and the usage of these methods in De-\nvOps/AIOps domain.\nA. ANOMALY DETECTION\nAnomaly detection is an active field of research in which\nmany applications and solutions being proposed every year\nin the past decades.\nA natural solution to the time series anomaly detection\nproblem is to utilize the “normal” time series to build a model\ncapable of predicting the following items when the system is\nrunning normally. Given such a model and an observation\nof time series, one can use the observation as the model’s\ninput, and get the prediction indicating what the following\nvalues could be under the system’s “normal” situation. If the\nobserved values of future time series violate the predicted\nones, the operator should mark the timestamp as “abnormal”.\nUnder this setup, two essential problems need resolving.\n• Finding a good model that can correctly predict our\ntarget sequence. Depending on the characteristics of\ndifferent targets, “good” models often vary in scales,\ncosts, and even methodologies.\n• Finding a method to examine whether a violation case is\n“abnormal”, or just a false positive/outlier. Such meth-\nods mainly refer to various thresholding techniques,\nincluding static and dynamic (or self-adaptive) thresh-\nolding.\nTraditional time series analysis methods often tend to\nuse simple statistic methods to model time series. Early in\nthe 1990s, the ARIMA model [4] was proposed. The full\nname of ARIMA is “AutoRegressive Integrated Moving Av-\nerage”, which uses a differencing operation (or “integrate”)\nto eliminate the non-stationarity if possible. To deal with\nseasonal components, a seasonal-differencing technique is\nalso introduced. Given input, an ARIMA model generates\na scalar value, and operators often compare the prediction\nwith a static threshold to determine whether the prediction-\nobservation pair is abnormal.\nThe naïve ARIMA method experiences difficulties when\ndealing with complex time series. However, the series can\nbe non-stationary even after the integration process. Running\na second-order differencing yt∗ = yt − 2yt−1 + yt+2\nhelps to eliminate other components, but the complexity of\nan ARIMA model is still challenged by many real-world\ndatasets.\nOn the other hand, some efforts were made to decompose\nthe time series into different interpretable components. Gen-\nerally speaking, one can write a time series as a composition\nof 4 series: a level component, a trend component (T), a\nseasonal component (S), and an error term (E), which is the\ngap between the model and actual observations. Different\nmodels make different underlying mathematical assumptions\nabout the methods to compute and combine those compo-\nnents, forming a family that is called ETS models. Hyndman\net. al. [5] provides a detailed view of the ETS family.\nHowever, these traditional models suffer from perfor-\nmance degradation when the target becomes complex, or\nwhen we are unable to provide accurate prior knowledge.\nFor example, consider a time series describing the number\nof customers in a resort. One can easily conclude that it\nshould be busy on weekends, and have fewer customers dur-\ning weekdays, thus the seasonal component in ARIMA/ETS\nshould have a period of 7 days. However, this assumption\nbreaks when Christmas is coming. The input of traditional\nmodels only includes the target series itself and several\nhyper-parameters. Their lack of conditional inputs makes the\nmodels harder to generalize to different circumstances.\nSince the 2010s, there has been a growing interest in\nneural network-based methods for various machine learning\napplications, including time series prediction and anomaly\ndetection. As the previous example suggests, time series\noften come with prior knowledge, while neural networks are\nwell-suited to incorporate this information into the models.\nThanks to its nature, one can skip the intensive work of\nchoosing feasible features from all sorts of prior knowledge,\nand let the stacked layers in neural networks do the job\nautomatically.\nDuring past decades, researchers have proposed a variety\nof neural network models, and it is hard to thoroughly intro-\nduce every different directions. To make a comparison with\nour proposed TFTOps, we mainly focus on the works that\nare close to our model in target, approach, and scalability\npotential. In other words, we introduce some models that\nhave the following properties:\n1) Uses Deep neural network (DNN) to perform time\nsequence prediction. This ensures that the model is\nsomehow generalizable enough to capture fuzzy and\nversatile sequences.\n2 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3353201\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nHaoran Luo et al.: Probabilistic Temporal Fusion Transformers for Large-Scale KPI Anomaly Detection\n2) Integrates static information and other “known” vari-\nables with input. These information, acting as prior\nknowledge, will contribute to the model’s overall per-\nformance.\n3) Directly predict the future value of the sequence (un-\nsupervised), other than only predicting a manually\nattached “anomaly” label. This ensures the availability\nof model when exact anomaly labels can hardly be\nretrieved.\nCondition 1) and 3) indicate a task called multi-horizon\nsequence forecasting. Given a time sequence as input, the\nDNN model should generate its prediction for the possible\nvalues ahead.\nIn the field of DNN, there are two main approaches to\nsolve the multi-horizon forecasting task. The first approach\nin this direction features the autoregressive models such as\nSeq2seq [6] and DeepAR [7]. Autoregressive models usually\nbase on Long-short Term Memory (LSTM) cells [8] and\ntheir variations, such as GRU [9]. Deep AR [7] uses stacked\nLSTM layers to generate parameters (mean and variation) of\na Gaussian distribution, then samples from that distribution to\ndetermine the one-step-ahead output. Deep State-Space mod-\nels [10] implements a similar approach, while modeling the\nGaussian distribution parameters with a Variational Autoen-\ncoder(V AE) [11]. More recent researches focus on Trans-\nformers, for example [12] proposed the use of transformers\nin time series tasks and introduced convolutional layers to\nreduce memory footprints. Fan et.al. [13] proposed a multi-\nmodal attention mechanism to enhance LSTM encoders,\nwhich provides better context to a bi-LSTM decoder. These\nmodels are built to solve the “one-step-ahead” prediction\nproblem: accept the previous sequence as input from 0..t,\nthen predict the value of t + 1. After that, one should send\nthe prediction at t + 1 back to the model to get the next\ntimestep after t + 1, which is t + 2. This process is repeated,\nuntil all timesteps t + 1, ..., t+ n are iteratively generated.\nWhile straightforward, the efficiency of these models can be\nthe bottleneck in certain situations. Inevitably, we must call\nthe model n times to get n predictions, which results in an\nO(n) complexity. Caching the decoder part can mitigate the\nproblem.\nOn the contrary, non-autoregressive models generate a\nsequence of forecasts for a fixed number of horizons concur-\nrently. A typical model also resembles the normal approach\nof using an encoder to generate a hidden representation for\nthe whole input. Common choices of encoders include feed-\nforward CNN,Autoencoder [14], LSTM, transformer, or a\ncombination of these models [15]. Upon generating the rep-\nresentation vector, a decoder will process it to acquire future\npredictions. The whole model is end-to-end trainable. For\nexample, the Multi-horizon Quantile Recurrent Forecaster\n(MQRNN) [16] proposed two different encoder structures\n(CNN and LSTM), and generated output based on the en-\ncoded sequence via an MLP for each horizon. Temporal\nFusion Trasnformers [3], unlike [12], directly predicts the\nt + 1...t + n timesteps using a transformer-based model in\na single pass. Note that both models (MQRNN and TFT)\ngenerate the whole group of multi-horizon output at the\nsame time, avoiding the self-regression scheme. Besides,\nboth models chose “known” variables as extra input of their\ndecoders, which satisfies 2).\nIn terms of model structure, our TFTOps mainly inherits\nideas from TFT model [3] and [17]’s probabilistic input.\nB. ANOMALY DETECTION IN AIOPS\nDevOps relies on different aspects to monitor the status\nof system. Main datasources include Application logs [18],\ndistributed traces [19] and KPI metrics. More comprehensive\nresults can be found in [20]. In this paper, we mainly focus\non researches about KPI metrics (time series).\nCurrently, the most widely accepted toolchains include\nPrometheus [21] and Grafana [22] which have user-friendly\ninterfaces and supported by a powerful query language\n(PromQL [23]). However, those platforms currently only\nsupport naïve models as built-in functions, such as linear\nregression and statistical tests. These approaches are suitable\nfor most periodic data, while failing to solve complex cases\nwhere an assumption of underlying data distribution is un-\navailable.\nIn the past decades, machine learning approaches are\nintensively investigated for metric prediction. Supervised\nmethods include decision trees [24]–[26] or Bayesian classi-\nfiers [27]–[29]. While achieving high accuracy in some cases,\nthe performance of supervised methods heavily depends\non accurately labeled data. Unsupervised methods include\nLOF(local outlier factor) [30], clustering [31] and PCA [32]\nare developed for the cases where labeled data is unavailable.\nBeing successful in other fields, such as computer graphics\nand machine translation, neural network methods are gaining\nnotice in the AIOps context since 2015. For example, Monni\net. al. [33] built an anomaly detector based on restricted\nBoltzmann machine [34]; Lin et. al. using a Learning-to-\nRank model [35] to combine the result of LSTM and random\nforest. In more recent researches, attention mechanism also\ntook place in predicting long-term time series [36] [37].\nExcept directly predicting the target, operators can also use\nneural networks to generate reliable fake sequences [38] to\naugment the dataset. However, the performance of neural\nnetworks is often a concern to the operators.\nGenerally speaking, previous researchers mainly focused\non predicting the trend of KPIs(Key Performance Indica-\ntors). For example, Microsoft’s Spectral Residual CNN KPI\nanomaly detection model [39] treats each KPI time sequence\nseparately: an independent model is trained on each KPI se-\nquence. Since KPI sequences can be defined quite differently,\nusing different model parameters to predict them is a natural\nchoice. Also, since the number of such KPI sequences is few,\none can easily afford the cost of training and hosting several\nmodels. However, in typical microservice settings, there are\nhundreds of pods and services hosted for different purposes.\nWhen the operators try to make fine-grained predictions, they\nwill likely meet a dilemma: using a neural network to accu-\nVOLUME 4, 2016 3\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3353201\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nHaoran Luo et al.: Probabilistic Temporal Fusion Transformers for Large-Scale KPI Anomaly Detection\nrately track a pod/service’s metric, such as disk and memory\nusage, often consumes more resources than the pod/service\nitself. This dilemma prevents neural network models from\nparticipating in fine-grained prediction tasks.\nRecently, transformer-based models have shown superior\nperformance in neural machine translation and other text-\nrelated fields. Its power in processing sequences also attracts\nthe attention of researchers in time-series fields. Since 2020,\ntransformer-based models have formed a line of research on\nlong-term time series forecasting. For example, Informer [40]\nproposed an efficient ProbSparse self-attention mechanism,\nwhich solved the performance cap of long time series(output\nlength > 50); Autoformer [41] proposed a decomposition\narchitecture and auto-correlation mechanism to further ag-\ngregate sub-series level representations.\nHowever, in DevOps settings, it is doubtful that long-\nterm time series forecasting is necessary. As proposed by\n[40], the prediction error quickly rises as the output sequence\nlength increases. In production environments, DevOps tends\nto shrink the length directly instead. Discovering abnormal\ncircumstances hours ahead is good enough; in this case, a\nshorter time series(output length < 50) would suffice. Despite\nusing similar transformer and self-attention mechanisms, this\npaper mainly explores another dimension: the “width” of\ntransformer models in the practical DevOps field. In other\nwords, this refers to the ability to compute a group of similar\nmetrics using the same model structure and parameters.\nIII. METHODOLOGY\nThis section describes the architecture and training scheme\nof our proposed model TFTOps.\nA. NETWORK ARCHITECTURE\nThe model consists of three main parts: The input embed-\nding layer, the Input LSTM layer, and the temporal self-\nattention layer. The architecture is shown in Fig.1, and each\ncomponents are connected according to Algorithm 1. In the\nfollowing sections, we will also present the structure of each\ncomponent in a bottom-up order. Also, we will explain the\nrationales behind our choices.\nB. MULTI-INPUT AND PREPROCESSING\nWe begin with basic attributes that defines a model: input and\nexpected output.\nIn a microservice system, the metrics time series are often\nstrongly entangled together. For example, for a service S, a\ngrowing network traffic indicates its load is increasing and\nwill naturally lead to a peak in CPU usage and disk I/O. De-\npending on the system architecture, such inter-relationships\noften tend to have some delay, making it even more valuable\nin predicting future metrics.\nAs in [3], we divide the input into two parts: “static input”\nand “variable input”. The general rules of division are:\n• Static inputs\nIn Prometheus, each time series has some associated\nattributes (“labels”). Labels are read-only categorical\nAlgorithm 1Forward pass of TFTOps model\nRequire: Input variables vobs, vknown, vstatic\nξ(i) ← InputGRN (v(i)): Feed variable inputs vobs,\nvknown into GRN units to get encoded feature for variable\ni.\n˜ξ ← VSelect(ξ, vstatic): Encoded features ξ enter a\nvariable selection network VSelect, where we combine\nthem with static inputs vstatic, and acquire the merged\nfeature.\n˜ϕ ← LST M(˜ξ, vstatic), Φ ← GRN(˜ϕ, ˜ξ): An LSTM\nlayer plus a residual layer for extracting low-level infor-\nmation.\nψ ← Attn(Φ), ˜ψ ← LayerNorm(˜ϕ+GLU(ψ)): A masked\nself-attention layer for modeling high-level dependencies.\nAnother residual connection to restore low-level info.\nˆy ← Dense( ˜ψ): The final dense output layer.\nvariables describing the features of a given time series\ncreated upon the initialization of that series. When ex-\ntracting metrics from a server/pod, the node exporter\nautomatically attaches labels to the metrics; maintainers\ncan also customize the labels. For example, the follow-\ning time series from Prometheus’ node exporter has 4\npre-defined static inputs: business, fstype, instance(IP),\nand mountpoint.\nnode_filesystem_avail_bytes{\nbusiness=\"k8s\",\nfstype=\"ext4\",\ninstance=\"10.17.xx.xx\",\nmountpoint=\"/local\"\n}\nThese inputs are valuable in predicting future disk us-\nage. The “Business” label is manually assigned and\nrepresents the department to which the node belongs.\nTherefore, we collect all of the time series under the\nsame metric from a microservice system and define\n4 categorical variables according to the unique values\nin the node exporter. After choosing those categorical\nvariables, we transform them into one-hot features while\nmaintaining a mapping for all the unique values we saw\nin this process. See 1 for an example.\nTABLE 1. An example of categorical variables\nname value1 value2 value3 ...\nbusiness k8s monitor aiops ...\nfstype ext4 ntfs ... ...\ninstance 10.17.xx.1 10.17.xx.2 10.17.xx.254 ...\nmountpoint /local /(root) /mnt ...\n• Variable inputs\nVariable inputs are the main components that build up a\ntime series. In our monitoring system, there are 2 types\nof input that will change over time: values of different\n4 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3353201\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nHaoran Luo et al.: Probabilistic Temporal Fusion Transformers for Large-Scale KPI Anomaly Detection\nGRN GRN GRN GRN GRN GRN GRN GRN GRN GRN\nStatic Inputs\nVariable Inputs (Observed)\nVariable Inputs (Known)\nGRN GRN GRN GRN GRN\nStatic InputsVSelectVSelect \n VSelect VSelect VSelect VSelect \nLSTM \nEncoder\nLSTM \nEncoder\nLSTM \nDecoder\nLSTM \nDecoder\nVSelect \nObservable \nHorizon (t )\nGRN GRN\n GRN GRN\nMulti-head self-attention (with Decoder Mask)\nGRN(O) GRN(O)\nDense Dense\nFIGURE 1. The architecture of our TFTOps model. Blue dashed lines denote skip connections. Cells with the same color shares the same weight across different\ntimesteps.\nmetrics and the timestamp that produces these values.\nMany other variables, such as hour, day of the week,\nand whether it is a holiday, can be generated from times-\ntamps. We denote the metrics as “observed variables”\nand timestamp-related inputs as “known variables”.\nThe main difference between those 2 types of inputs is\nwhether we can “foresee” the exact future value. Since\nPrometheus periodically fetches metrics from sources,\nwe have a fixed timestamp-delta between neighboring\nobservations. We can conduct the value precisely and\ngenerate other fields, such as the hour/day of the week,\nfor any future timesteps. On the other hand, the future\nvalue of metrics remains unknown, as it’s our model’s\njob to predict them.\nTherefore, unlike traditional RNN-based models, the input\ndimensionality of the transformer encoder and decoder in\nTFTOps are NOT the same. We resolved this by the same\nvariable selection network in [3], which conceptually serves\nas a trainable weighted average among input features.\nAs shown in figure 1, the variable features (yellow) are\nfed into two Gated Residual Networks(GRN). As in [3], the\nGRN block is a basic building block of our model. In this\nprocess, two types of GRNs are used: one(green) processes\nthe observed variable inputs(input range is[1..t]), and another\nGRN(blue) processes the known variable inputs(input range\nis [1..t + τ]).\nVOLUME 4, 2016 5\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3353201\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nHaoran Luo et al.: Probabilistic Temporal Fusion Transformers for Large-Scale KPI Anomaly Detection\nGRN \nELU \nLinear1 \nLinear2 \nGLU \nLNorm \nFIGURE 2. The detailed architecture of a GRN Unit. Each GRN unit contains\n2 “linear+activation” structure, a LayerNorm, and a skip connection. Contextual\ninput (c) is optional, and does not present in the skip connection.\nC. GATED RESIDUAL NETWORKS\nThe idea behind GRN is like a residual network: the model\nchooses to apply a non-linear process when needed. As in 2,\na GRN unit receives two vectors as its input: a primary input\na, and an optional context c. We compute the layer’s output\nas follows:\nGRN(a, c) = LayerNorm(a + GLU(η1))\nη1 = W1η2 + b1\nη2 = ELU (W2a + W3c + b2)\n, where GLU stands for Gated Linear Units [42], and ELU\nstands for Exponential Linear Unit. If we do not provide the\ncontext, then context c is replaced with a zero vector c = 0\nGLU(x) = σ(Wg1x + bg1) ⊙ (Wg2x + bg2)\n⊙ means element-wise Hadamard product. Through a GLU\nlayer, the model can suppress any useless feature in x, which\nmeans doing variable selection in a differentiable way.\nThe ELU activation is conceptually similar with the ReLU\nactivation, but is smooth and differentiable:\nELU(x) =\n(\nx x > 0\nα(exp(x) − 1) x ≤ 0\nD. INPUT EMBEDDING LAYER\nThis layer is the first part of tft model, and it directly process\nthe input features of various types and shapes. It consists of\nembedding layers and a variable selection network. 3\n1) Embedding layers\nEach different input variable i ∈ {1, 2, ..., m} uses its own\nembedding layer. The input variable, no matter it is one-hot\nor real-valued, is linearly transformed into a fixed-dimension\nvector ξ(i)\nt ∈ Rdmodel:\nξ(i)\nt = W(i)X(i)\nt\nGRN GRN GRN GRN GRN GRN GRN GRN GRN GRN\nStatic Inputs\nVariable Inputs (Observed)\nVariable Inputs (Known)\nGRN GRN GRN GRN GRN\nStatic InputsVSelectVSelect\nVSelect VSelectVSelectVSelectVSelect\nObservable\nHorizon (t )\nFIGURE 3. The architecture of input embedding layer. GRN layers with the\nsame color share the same group of parameters.\nAn additional layer of GRN introduces a non-linear pro-\ncess, transforming ξ(i)\nt into ˜ξ\n(i)\n:\n˜ξ\n(i)\nt = GRNξ(i)\nt\nξ(i)\nt\nFor each variable i, the weights of linear transform W(i)\nand GRNξ(i) are shared across all timesteps t.\n2) Variable selection network\nAfter the embedding layer, the transformed˜ξ\n(i)\nt s are fed into\na variable selection network. The variable selection network\nflattens the output of different variables from embedding\nlayer, and builts its own input Ξt:\nΞt = [ξ(1)T\nt , ξ(2)T\nt , ...,ξ(mχ)T\nt ]\nAlso in this layer, a context vector cs from static inputs is\nintroduced to build a weight vector:\nvχt = Softmax(GRN(Ξt, cs))\nWe combine the weight vχt with the GRN-transformed\nfeature ˜ξ\n(i)\nt to get the final output at timestep t:\n˜ξt =\nmχX\nj=1\nv(i)\nχt ˜ξ\n(i)\nt , ˜ξt ∈ Rdmodel\nAfter that, each timestep, regardless of how many input\nfeatures it has, is transformed into a vector with fixed dimen-\nsion Rdmodel, and is ready for further computations.\nE. PROBABILISTIC INPUT\nThe observation frequency in AIOps is much higher than\nin traditional time-series prediction tasks (such as grocery\nsales [43] and climate change). Most monitoring systems\ncollect metric values on a minute-level basis; thus, a typical\nperiod (a day) would contain 24 ∗ 60 = 1440 data points.\nNeither sequential models (RNNs) nor transformer models\ncan easily handle such a long input series. A workaround is to\nresample the whole series with a larger interval; however, this\nresampling means ignoring (or averaging) all the observed\nvalues in between, which is not advantageous.\nTo resolve this problem, we leverage the “probabilistic\ninput” idea from [17] to enhance the previous input schema.\n6 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3353201\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nHaoran Luo et al.: Probabilistic Temporal Fusion Transformers for Large-Scale KPI Anomaly Detection\nWe illustrate this probabilistic pre-processing method by a\nreal-world task below.\nOne of our clients has a Prometheus monitoring system\nwhose retrieving interval (the time difference between two\nneighboring data points) is ∆t = 60s. They wish to predict\nthe disk usage after 4 hours, which would introduce a gigantic\ndecoder (ldecoder = 240 ) and an encoder even larger than\nldecoder. To shrink the size of the encoder, we performed the\nfollowing pre-processing steps to implement the “probabilis-\ntic input” scheme:\n1) Firstly, we manually determine an interval I according\nto domain knowledge. For example, in this task, we\nchoose I = 20 , which is combining 20 inputs into\nONE distributional input.\n2) Secondly, we normalize the real-valued inputs. After\nnormalization, the model split the values into nbins =\n10 bins. nbins is a hyperparameter determined through\ngrid search, and its typical search range is[0.05, 0.3]∗I,\nwhere I is the previously mentioned aggregation inter-\nval. Let b(t) : R → Rnbins be the binarization function\nwhich maps the real-valued inputsx(t) to one-hot input\nb(t).\n3) Cut the input sequence according to the previously\nmentioned interval I. Each timestamp t now corre-\nsponds to a time window that starts from t − I + 1 and\nends with t itself. We transform the values in a window\ninto a one-hot vector, then take an average to build a\nprobabilistic input series:\nPx(t) = 1\nI [b(t − I + 1) +b(t − I + 2) +... + b(t)]\nSimilarly, for preceding inputs Px(t + k), we have:\nPx(t + k) = 1\nI [b(t + k(I − 1) + 1) +... + b(t + kI)]\nAfter the pre-processing, each time window ( RI)in our\ninput series is transformed into a single vector R1×nbins .\nConcatenating those new vectors, we get a new time series\nPx. This new time seriesPx has a period length ofI·∆t; each\nelement in this new time series can be viewed as a sample\nfrom a multinoulli distribution. Note that we only transform\nthe encoder inputs; the output sequence of the decoder is\nsub-sampled by I as usual. In order to predict the value of\nour time series after 4 hours, our new decoder only needs\nto process 12 timesteps after the sub-sampling, while each\ntimestep has more features. As our input embedding layer\nsuggests, adding features to our current input only affects\nthe dimensionality of W sand the variable selection network.\nTherefore, the addition of probabilistic inputs will only have\na negligible impact on the efficiency of the model compared\nto a model sampled with interval I. Compared to the original\nmodel without interval sampling, the overall efficiency of the\nmodel is greatly improved as the length of the encoder is\nreduced by 20×.\nWe apply a linear layer to transform the Rnbins vector Pj\ninto a Rdmodel vector ξ(Pj)\nt . Then, this new feature is fed into\nits own GRN like other non-probabilistic features:\n˜ξ\n(Pj)\nt = GRN˜ξ\n(Pj) (ξ(Pj)\nt )\nThe processed ˜ξ\n(Pj)\nt is then feed into the variable selection\nlayer along with the original feature ˜ξ\n(j)\nt . We also keep\nthe original feature alongside the probabilistic feature for\ncapturing the exact value, which is beneficial especially when\nthe feature itself is a target for prediction.\nNote that we only use the “probabilistic” pre-processing\nscheme when dealing with observed variables. Known vari-\nables, which often tightly related with timestamps, are resam-\npled with a larger interval as usual.\nF. INPUT LSTM\nAs in figure 1, the outputs of the input embedding layer at\nevery timestep ( ˜ξ ) are fed into an LSTM encoder/decoder.\nThis LSTM network [44] helps the model to capture the\nhidden informations in consecutive values.\nThe LSTM output at each timestep (encoder and decoder)\nis denoted as ϕ(t). As in [3], a residual layer is used for\nrobustness, which we mark as blue dashed lines. The residual\nlayer directly sends ˜ξ into the LSTM output. On the other\nhand, if our model find the LSTM encoder/decoder is not\nbeneficial, the Gated Linear Unit(GLU) activation would\nsuppress its impact:\n˜ϕ(t, n) = LayerNorm\n\u0010\n˜ξt+n + GLU(ϕ(t, n))\n\u0011\nThe produced output ˜ϕ(t, n) is merged with the static\ninformation via a GRN layer. As in input embedding layer,\nthe static information serves as context in GRN:\nθ(t, n) = GRN(˜ϕ(t, n), ce)\nThrough input embedding and LSTM, we extract the local\ninformation and put them into θ(t, n)\nWe concatenate θ(t, n) from different timesteps to build\nthe input of Temporal self-attention layer Θ(t):\nΘ(t) = [θ(t, −k)T , ...,θ(t, τ)]T\nAfter this layer, informations in the series short-term rela-\ntionships are extracted into the output features.\nG. MULTI-HEAD ATTENTION\nAs in TFT [3], TFTOps implements a self-attention mecha-\nnism. This mechanism, which is inherited from the original\nTransformer model [45], helps the model learn the long-term\nrelationships among input timesteps.\nGenerally speaking, the attention mechanism is a scaler\nupon “values” V ∈ RN×dV . The scaling factor is deter-\nmined by relationships between “keys” K ∈ RN×dattn and\n“queries” Q ∈ RN×dattn:\nVOLUME 4, 2016 7\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3353201\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nHaoran Luo et al.: Probabilistic Temporal Fusion Transformers for Large-Scale KPI Anomaly Detection\nLinear\nGRN\nOther\ninputs\nGRN\nVSelect \nFIGURE 4. The architecture of our probabilistic input scheme. The probabilistic feature is extracted across a sub-sampled time window. The dashed vertical line\nmarks the end of a sub-sampled window, where the other non-probabilistic variables (including “known” and “observed” variables) are extracted. After processed by\nits own GRU layer, each variable equally enters the variable selection layer.\nAttention(Q, K, V) = A(Q, K)V\nwhere A() is the “attention function”. The most common\nchoice for A() is the scaled dot-product function:\nA(Q, K) = Softmax(QK)T /\np\ndattn\nMulti-head attention layers are built by replicating the\ndot-product attention several times. A single dot-product\nattention is called a “head”, and the number of heads is\ndenoted by a hyperparameter mH. All heads share the same\ninput tuple (Q, K, V ), but rescale it by different, head-\nspecific weight W(h)\nQ , W(h)\nK , W(h)\nV before calling the atten-\ntion mechanism:\nHh = Attention(QW(h)\nQ , KW (h)\nK , V W(h)\nV )\nAfter done computing all Hh, the values are linearly\ncombined through another weight matrix WH to build the\nfinal output of multi-head attention:\nInterpretableMultiHead(Q, K, V ) = [H1, ...,HmH ]WH\n.\nWe made the same adjustments against traditional multi-\nhead attention layers, as in [3]. The core idea is to “force” all\nheads to use the same value of V and WV . The weight ma-\ntrix for keys WK and queries WQ remain different among\nheads, and we average their scalesbefore multiplicating them\nwith V WV :\n˜H = ˜A(Q, K)V WV\n=\n(\n1\nH\nmHX\nh=1\nA\n\u0010\nQW(h)\nQ , KW (h)\nK\n\u0011)\nV WV )\n= 1\nH\nmHX\nh=1\nAttention\n\u0010\nQW(h)\nQ , KW (h)\nK , V WV\n\u0011\nThus, each head attends to the same input features corre-\nsponding V while learning different patterns through their\nown W(h)\nQ and W(h)\nK . This approach is similar to convolu-\ntional neural networks(CNNs). Here, attention heads act like\nconvolutional kernels in CNN, and the number of heads mH\nis comparable with the number of “channels” in convolution\nlayers. By this way, the total number of trainable parameters\nand computation overhead are both decreased.\nH. MULTI-HEAD SELF-ATTENTION LAYER\nBy feeding the output of GRNs into aforementioned inter-\npretable self-attention layer, we model the long-term depen-\ndencies with a stacked multi-head self-attention.\nWe apply interpretable multi-head self-attention only once.\nAll previous GRN results, range from the first timestep in\ninput sequence to the last timestep of prediction ( t ∈ [0..t +\nτ]), are fed into the self-attention layer Θ. The same Θ(t)\nis fed into all three slots: query, key and value. Note that, to\n8 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3353201\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nHaoran Luo et al.: Probabilistic Temporal Fusion Transformers for Large-Scale KPI Anomaly Detection\nprevent leaking of training data, the decoder timesteps in self-\nattentions are masked [12], [45]. For example, at timestep\nt+τ −1, the model sees only the input fromt ∈ [0..t+τ −2],\nbut not t + τ. This ensures a causal information flow.\nB(t) = InterpretableMultiHead(Θ(t), Θ(t), Θ(t))\nB(t) has the same dimensionality as Θ(t). We decompose\nit into B(t) = [β(t, −k)T , ...,β(t, τ)]T\nThis self-attention layer can also be stacked, as mentioned\nin [45]. When stacking, the following layer directly deals\nwith the output of previous self-attention layer; this time, we\ndo not include residual connection and/or static contexts.\nAgain, in order to preserve the local information learned\nby RNN, we use a residual connection. This is marked with\nblue dashed line in 1. The transformer output and the residual\nare concatenated channel-wise, then fed into another GRN\nblock(blue):\nψ(t, n) = GRN\n\u0010\nLayerNorm\n\u0010\n˜θt+n + GLU(β(t, n))\n\u0011\u0011\nI. OUTPUT LAYER AND QUANTILE LOSS\nWe provide another skip connection which skips the entire\ntransformer block, directly connect the output of LSTM to\nthe dense layer.\n˜ψ(t, n) = LayerNorm\n\u0010\n˜ϕt,n + GLU(ψ(t, n))\n\u0011\nThis new embedding ˜ψ(t, n) passes through a linear layer\nto produce the final output prediction at this timestep. Given\ncurrent timestep t and the future timesteps we want to predict\n0 < τ < τmax, we have:\nˆy(t, τ) = W ˜ψ(t, n) + b\nThe shape of prediction at timestep τ (ˆy(t, τ)) is Rntarget ,\nwhere ntarget denotes the number of features that we want to\npredict.\nTo better fit the requirements of anomaly detection, we\nchoose the averaged quantile loss. We define a sequence ofnq\nquantiles between (0, 1): 0 < q1 < q2 < ... < qnq < 1. The\noutput dense layer is enlarged channel-wise to get outputs\nat each quantile, which makes the output shape become\nˆy ∈ Rntarget×nq . For each quantile qi, its output ˆyq is the\ni-th dimension of ˆy. We define the output value and its\ncorresponding “quantile loss” function QL(y, ˆyq, q) as:\nˆyq(t, τ) = Wq ˜ψ(t, n) + bq (1)\nQL(y, ˆy, q) = q(y − ˆyq)+ + (1 − q)(ˆyq − y)+ (2)\nThe quantile q is customizable according to the require-\nments of different jobs and domain knowledge. For example,\na common choice is 0.1, 0.5, 0.9, which means we have a\npessimistic estimation (q = 0.1), a balanced estimation (q =\n0.5), and an optimistic estimation (q=0.9). The pessimistic\nand optimistic predictions work as lower/upper bounds of the\nobserved value. On the other hand, when given the quantile\n0.5, the model acts as a standard regression model with L1\nloss.\nOur final loss is the average loss across all quantiles\nq1, ..., qn.\nJ. OUTPUT SCHEMES AND DETECTION MODES\nThere are two primary use cases of TFTOps.\nFirstly, we can automatically get an adaptive threshold\nfor a certain metric with the quantile loss function. As the\nquantile loss function suggests, quantile q = 0 .5 is the\noutput of a standard regression model; at higher quantiles\n(0.5 < q <1), the model tends to output a higher prediction\nto avoid being heavily punished by the loss function. Like-\nwise, lower quantiles ( 0 < q <0.5) would lead to a lower\nprediction. Assuming our model can accurately predict the\nmetric value in “normal” cases (quantile q = 0.5), then any\nvalue higher than the highest quantile (q = 0.9) or lower than\nthe lowest quantile (q = 0.1) can be considered “abnormal”.\nWe can adjust the quantiles to balance precision and recall.\nGenerally speaking, squeezing the upper and lower quantiles\ntowards 0.5 improves recall but harms precision, for it would\nintroduce some false positives; stretch them towards 0 or 1\nwill improve precision instead. For robustness, the “abnormal\nstate” alert should be triggered when the prediction violates\nthe rules above for a consecutive certain period. Typically, we\nchoose the period as 5 ∗ sampling_interval, which means 5×\nconsecutive violations will trigger an alarm in Prometheus.\nSecondly, the TFTOps model itself can also serve as a pre-\ndiction model. Traditionally, such prediction is retrieved by\nextracting trends via simple regression models (such as linear\nregression) from the last hours, then extending the extracted\ntrend. However, this naïve method does not consider external\nmessages such as date or time, thus introducing a high chance\nof false alarms. In our TFTOps model, the loss function for\nthe “normal” quantile (q = 0.5) is the same as the traditional\nL1 loss. The prediction at this quantile is also suitable for\ndirectly forecasting the metric in the near future. With a user-\ndefined threshold, the TFTOps model can predict abnormal\ncases (for example, CPU use rate > 80%) several minutes\nbefore the anomaly happens.\nIV. EXPERIMENTS\nA. EXPERIMENT SETUP\nWe implemented TFTOps under tensorflow framework. Both\ntraining and prediction are performed inside the same kuber-\nnetes pod, which is hosted on a node with Intel(R) Xeon(R)\nGold 6278C and no GPUs.\nOur dataset is retrieved from a Prometheus platform which\nmonitors a working kubernetes cluster with hundreds of\nnodes and thousands of running pods. Because prometheus\nuses an in-memory TSDB database, which is a temporary\nstorage and does not guarantee persistence, our training set\nis periodically retrieved from a PostgreSQL database, which\nserves as a persistent data storage of Prometheus.\nVOLUME 4, 2016 9\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3353201\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nHaoran Luo et al.: Probabilistic Temporal Fusion Transformers for Large-Scale KPI Anomaly Detection\nB. DATASET OVERVIEW\n1) Prometheus Node Exporter (PNE)\nThe Prometheus monitoring platform scrapes metrics from\nnode_exporters in kubernetes system every 20 seconds.\nOur first experiment chose “ node_filesystem_free_bytes”\nmetric as the target. The metric indicates the amount of free\nspace in each mounted device on k8s nodes. This metric\nis tightly connected with system load, thus receives great\nattention from our operators. This metric would certainly\ngo up as system load increases; however, there are many\nreasons to cause the increase of metric, and their severity\ndiffers greatly. For example, when the system load is reaching\na local peak, an increase in node_filesystem_free_bytes is\nperfectly normal; when the system is experiencing a DDoS\nattack, or some bug occurs in load-balancing components,\nan increase in the same metric would eventually break the\nsystem. Discriminating the two different cases is proved to\nbe a challenge, and we tried to resolve it by TFTOps.\nWe can easily find other useful metrics in node_exporter,\nsuch as node_network_receive_bytes_total which could be\nuseful in predicting future values. We integrated those met-\nrics in a group of experiment to clarify whether providing\nextra time series are helpful in this job.\nGenerally speaking, the sequences are very likely to be\nmonotonic in given time window, which makes the traditional\nmethods (ARIMA, ETS) suitable for this task. However,\ntraditional approaches require fitting a model per prediction\ntask, greatly slowing down the prediction process. On con-\ntrary, inference through a neural network is more efficient.\nThe input variables are extracted directly from Prometheus\nsequence, which contains a set of labels to describe the\nsequence’s properties. We used PromQL to extract the\nwhole sequence as a list of data points extract at different\ntime from different servers. Each point contains timestamp,\nvalue(bytes), and a set of descriptive variables indicating the\nsituation of the source. We use the following variables in our\nexperiment:\nTABLE 2. Features of PNE dataset\nvariable type description example\nbusiness static the department which owns the\ninstance\nusercenter\ndevice static the device name of disk /dev/vda2\nfstype static type of filesystem ext4\ninstance static ip of the server 10.13.x.x\nlocation static data center of the server DC02\ntarget observed The remaining space (GB) 16.43\ncpu observed CPU usage over the last minute 0.031\nnetwork observed average network traffic (kB) per\nsecond, over the last minute\n1028.4\nweekday known the weekday, can be derived\nfrom timestamp\n6\nhour known the hour of the day 14\nholiday known Boolean, whether the day is a\nholiday\n0\nNote that, since the same timestamp will definitely not\nappear in both train and test datatsets, we do NOT use the\ntimestamp itself as a feature.\n2) Redis Connect(RDC)\nThe second experiment aims to predict the number of active\nconnections per redis job in a redis cluster. In out production\nRedis cluster, a crontab job is running to retrieve the number\nof connections once per minute, and send the message to a\nkafka topic for further analysis. Originally, the operators use\na na´’ive alert system based on thresholding, which is proved\nto be annoying, since the alert is triggered by false positives\nfrom time to time. We directly consume the topic, and record\nthe historical values in a database. This metric is somehow\nproportional to the system’s KPI because it is closely related\nsystem load and user traffic. Traditional temporal-filtering\nbased models failed to deal with such KPI-related metrics.\nDue to the nature of redis connections, most of sequences\nin this dataset are oscillatory. Traditional methods fails to\nconverge (ARIMA), or performs poorly (ETS) when process-\ning such sequences.\nWe use the following variables in our experiment:\nTABLE 3. Features of RDC dataset\nvariable type description example\nid static the redis job id 11250\ntarget observed the number of connections of\ncertain job\n694\nweekday known the weekday derived from times-\ntamp\n3\nhour known the hour in day 18\nis_holiday known whether the day is a holiday 1\n3) Benchmarks\nWe compare TFTOps to different types of models for multi-\nhorizon forecasting. For models which base on neural net-\nworks, we used roughly the same number of search iterations\nto conduct hyperparameter optimization over a pre-defined\nsearch space centered around their default parameters (men-\ntioned by their lrespective paper).\nA brief introduction of relevant models and their source\ncode (if available):\n• ARIMA [4]: ARIMA is a traditional statistical model\nfor time series analysis. Note that there is no guaran-\ntee that all input sequences should be stationary; for\nthe non-stationary sequences which cause the ARIMA\nmodels to fail, we use the last observed value as a\nsubstitution.\n• ETS: Another traditional statistical model, which de-\ncomposes time series into error, trend and seasonal\ncomponents. Both ARIMA and ETS does NOT require\nadditional features.\n• MQRNN [16]: A recurrent network for multi-horizon\nforecasting. Its encoder is the same as seq2seq LSTM\nencoder (but without attention) and uses the same hy-\nperparameter search space.\n• RoughAE [14]: A neural network model based on the\ndenoising autoencoder.\n• DTDL [15]: A dictionary-learning neural network\nmodel, which uses LSTM layers as the autoencoder.\n10 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3353201\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nHaoran Luo et al.: Probabilistic Temporal Fusion Transformers for Large-Scale KPI Anomaly Detection\n• DeepAR [7]1: A popular time series prediction frame-\nwork based on RNN. The main difference between\nDeepAR and Seq2Seq is that DeepAR introduces dis-\ntributions as output at each timestep.\n• ConvTrans [12]2: A model based on transformer [45]\nstructure and LogSparse convolutional self-attention\nlayers. LogSparse layer reduces the number of dot prod-\nucts per self-attention layer.\nFor the models that we failed to find open-source imple-\nmentations, we tried to replicate the model structure in their\npaper to our best effort.\nMost neural network models are based on either LSTM\nrecurrent networks [44] or transformers [45]. For seq2seq\nmodels, the decoder input (output of former timestep) is\nconcatenated with “known” variables; other networks already\nintegrated static and extra sequences in their inputs.\nIn our experiments, we include both variations of TFTOps:\n• TFTOps: The original TFT model, which is similar\nwith [3].\n• TFTOps(prob): The TFT model with extra binned and\nprobabilistic inputs.\nC. EVALUATION\nThe effectiveness of the prediction model can be assessed\nfrom two different aspects. First, we want the prediction to\nbe precise when the system runs normally. Second, we want\nthe prediction model to raise an alert when the system is in\nan erroneous state.\nThe first requirement is evaluated by computing the aver-\nage L1 distance between the following two metric values: the\nvalue predicted by the TFTOps model, and the real value in\nthe timestamp on which the prediction was made.\nPer timestamp i, the mean absolute error MAE(i) is\ndefined as:\nMAE (t + τ) = |ˆy(t0)(t + τ) − y(t + τ)|\nThe final evaluation metric is the average over timesteps\nand sequences:\nMAE = 1\nnτmax\nnX\ni=1\nτmaxX\nτ=1\nMAE(i)(t + τ)\nNote that y is taken from the normalized sequence. The\nper-sequence loss Pτmax\nτ=1 MAE(i)(t + τ) resembles the met-\nric MAE % of this sequence, which is usually defined as\nMAE % =\nPτmax\nτ=1 |A(ˆy(t0)(t + τ) + B) − y′(t + τ)|\n|Pτmax\nτ=1 |y′(t + τ)\nwhere y′ is the raw value retrieved from data source (without\nnormalization) and A, B are two factors for reverting the\nnormalization process.\nWe modified the output layer of those neural network\nmodels to enable quantile loss function. Different quantiles\n1https://github.com/jdb78/pytorch-forecasting\n2https://github.com/mlpotter/Transformer_Time_Series\nare trained in the same training loop, while their MAE s are\ncauculated separately.\nThe second requirement is evaluated by investigating the\nerrorneous states in daily maintainence, or injecting excep-\ntions into system and examine the supervised methods such\nas record and precision. We do not consider the F-metric\ndue to the great difference between the number of normal\nand abnormal cases. Even after we injected exceptions in the\nsystem, abnormal cases were still extremely rare.\nD. IMPLEMENTATION\nThis subsection describes the implementation details of\nTFTOps model in production envirionment.\n1) Auto update\nThe statistic of time series in a prouction system is always\nshifting over time. As a result, the predictior model must also\nhave a updating routine.\nIn our production deployment, training and prediction\nare ran on different, independent processes. The update are\nscheduled once per week. We fetch the most recent data\nfrom databses to train TFTOps model; after training is done,\nthe best model is saved to file system, and a message is\nsend through RabbitMQ message queue. When the prediction\nprocess receives that message via polling, it will trigger a\n“reload” function which causes the prediction process to\ndiscard the old model and reload the new model from disk.\n2) Dataset Error handling\nIn production environments, each model corresponds to 2\nweeks/one month of training data. It is nearly impossible for\nthe microservice system to keep a consistent state for weeks.\nIn our cases, Kubernetes pods (or their exporters) and Redis\njobs are dynamically created and destroyed over time. As a\nresult, we must deal with “dirty” data.\n• Prometheus metrics: Prometheus node-exporters gener-\nate a metric (kube_node_status_condition) to track the\nstate of each node. As we wish to model the system’s\nnormal and steady state, the metrics generated while\nkube_node_status_condition{condition=’Ready’,\nstatus=’true’} ̸= 1 (which means the node is not ready)\nare discarded from the training set.\n• Redis Connect: For each Redis job, other than inferring\nthe mean and variance directly from the training set, we\nobserve a longer range (6 months) to retrieve a better\nestimation of mean and variance. If the job was started\nin 6 months, the observation becomes its full lifespan.\nBesides rescaling the input/output, we also use this\nobservation to rule out outliers. Outliers are replaced\nwith µ ± 3σ. In our systems, the number of connec-\ntions per Redis job is tracked by Kafka messages. A\nscheduled producer fetches the status of Redis clusters\nand uploads them to the given topic. Both fetch and\nupload procedures are prone to errors during network\nfluctuations. Therefore, we observed missing data points\nVOLUME 4, 2016 11\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3353201\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nHaoran Luo et al.: Probabilistic Temporal Fusion Transformers for Large-Scale KPI Anomaly Detection\noccasionally. When the number of consecutive missing\nobservations is relatively small ( ≤ 5, 5 minutes), we\nassume the system is running normally. In this case,\nwe use the latest visible observation for filling in the\nmissing values. Therefore, our pre-processing step can\ngenerate training data generated across the gap. On the\ncontrary, when a wider gap ( > 5) appears, we assume\nthat the system’s state is questionable, thus discarding\nthe missing observations.\nE. RESULTS: EVALUATION ON TEST DATASETS\nThe MAE results of models mentioned in previous section\nare shown in Table 4 and Table 5.\nTABLE 4. Experiment results on PNE dataset\nModel MAE(q=0.5) MAE(q=0.1) MAE(q=0.9)\nARIMA 7.6144 - -\nETS 5.9987 16.5047 15.2615\nMQRNN 1.8890 3.7035 1.8232\nRoughAE 2.4901 2.3246 3.2497\nDTDL 1.6633 1.9547 1.7837\nDeepAR 1.0176 0.5384 1.3539\nConvTrans 1.2944 0.5322 1.7291\nTFTOps 1.2706 0.9471 1.2419\nTFTOps(prob) 0.8397 0.5314 0.6932\nTABLE 5. Experiment results on RDC dataset\nModel MAE(q=0.5) MAE(q=0.1) MAE(q=0.9)\nARIMA 38.8198 - -\nETS 23.9378 70.3496 71.7681\nMQRNN 24.3383 29.4822 20.6325\nRoughAE 31.6767 41.2920 37.3145\nDTDL 28.7762 15.0396 30.3068\nDeepAR 25.7791 24.0882 22.1861\nConvTrans 26.5045 20.8684 21.8974\nTFTOps 7.4489 10.1459 5.9466\nTFTOps(prob) 9.1080 7.4725 3.7677\nGenerally speaking, the performance of modern models\nis better than traditional ARIMA/ETS models. It is worth\nnoticing that neural network models are especially good at\npredicting quantiles: all the neural network models’ quantile\nlosses(q=0.1 and q=0.9) are far better than that of ETS.\nAmong all of neural network models, we can conduct that\nTFTOps achieves excellent MAE in both datasets, exceeding\nother SOTA models, and is especially good for the RDC\ndataset which keeps oscillating.\nThe probabilistic input scheme for TFTOps also have pos-\nitive effect on MAE; however, this scheme introduces some\nextra parameters and requires slightly more computations in\npreprocessing and embedding extraction phases. However,\ndue to the probabilistic features are vectorized and fed into\nthe variable selection layer, the extra cost is trivial.\nF. RESULTS: THE ROBUSTNESS OF TFTOPS AGAINST\nNOISE\nWhen building a metric prediction system in real world, the\ndevelopers have a variety of static or variable features to\nchoose from. For example, when predicting the CPU usage,\none can easily acquire the following features from many\nsources, especially from prometheus node-exporter:\n• CPU model, generation and performance benchmarks\n(static)\n• The number of processes running on the same machine\n(variable)\n• The network load of the same machine (variable)\n• KPI of related services (variable) ...\nFeature without impacts are considered “noise” and is\nharmful to the model. Generally, one would run a grid-\nsearch to validate the effectiveness of each feature and find\nthe optimal set of features. However, training all sorts of\ndeep-learning models require a non-trivial amount of time.\nTo metigate this problem, the model itself should be robust\nagainst the noise.\nWe designed a new dataset PNENoise based on PNE to\ntest different model’s ability to filter noise. In PNENoise, new\nartificial noise features are concatenated to each row. Those\n“noise” features are sampled from following distributions to\nreflect various situations:\n• Uniform: a uniform distribution between [0,1].\n• Normal: a normal distribution N(0, 1).\n• Periodic normal: a normal distribution whose centroid\nis a function of time t. This reflects some periodic\ncomponent. We chose N(5sin(t/24), 1).\n• Random category: a randomly assigned “class” from\n0, 1, 2, 3, 4. This is a static feature.\nTo evaluate the robustness, we re-train different models\n(except ARIMA and ETS) on the PNENoise dataset from\nscratch and compare their performance on the MAE(q = 0.5)\nmetric. The increased percentage on MAE( q = 0 .5) quan-\ntifies the model’s robustness against noise; a lesser increase\nmeans better robustness. Note that, we use the same hyperpa-\nrameter search space when training the models on PNENoise.\nTABLE 6. Results on PNENoise dataset\nModel Original Noise MAE%\nMQRNN 1.8890 2.5695 36.02%\nRoughAE 2.4901 2.5428 6.13%\nDTDL 1.6633 1.8148 9.10%\nDeepAR 1.0176 1.3679 34.42%\nConvTrans 1.2944 1.6581 28.10%\nTFTOps 1.2706 1.3531 6.49%\nTFTOps(Prob) 0.8397 0.9034 7.58%\nResults are shown in Table 6. It can be conducted that,\ntraditional neural-network based methods(MQRNN,DeepAR\nand ConvTrans) suffers from noisy features, while\nRoughAE/DTDL and TFTOps are only slightly affected. The\nimplementation of RoughAE and DTDL directly addressed\nthe robustness problem and solving it by model designations\n12 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3353201\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nHaoran Luo et al.: Probabilistic Temporal Fusion Transformers for Large-Scale KPI Anomaly Detection\nsuch as rough inputs [14] and dictionary learning [15], thus\nachieved better robustness. However, their performances are\nslightly worse than deep neural networks. The TFTOps\nmodel, on the other hand, maintains robustness while enhanc-\ning its performance.\nG. RESULTS: EVALUATION ON REAL ENVIRONMENT\nAs mentioned in [7], the quantile loss function and outputs\nnaturally becomes a good detector of anomalies. Every time\nthe real observed target violates the prediction (a target value\nthat is even higher than the upper quantile or lower than the\nlower quantile) indicates that the sequence is in an abnormal\nstate.\nWe performed the evaluation on the production environ-\nment which generates RDC dataset. The TFTOps model was\nkept serving for a month. During that period, the opera-\ntors recorded 17 abnormal incidents. When TFTOps model\nreports an anomaly, we check that whether an abnormal\nincident happens within a 10-minute range (true positive) or\nnot(false positive).\nThe confusion matrix and evaluation result is shown in\ntable 7 and table 8.\nTABLE 7. confusion matrix of RDC system evaluation\nactual\nvalue\nRedis Prediction outcome\np n total\np′ True Pos.\n= 13\nFalse Neg.\n= 4 P′ = 17\nn′ False Pos.\n= 27\nTrue Neg.\n= 43156 N′ = 43183\ntotal P = 40 N = 43160\nTABLE 8. Production results on RDC system\nmetric value\nrecall 76.4%\nprecision 32.5%\nF1-score 0.4561\nWe can conclude that the model successfully extinguishes\nmost of anomalies at a cost of relatively low precision.\nShifting the quantile loss (q=0.1/q=0.9) towards 0/1 should\nimprove precision while lowering recall. The selection of\nquantile will greatly influence the performance of model in\nproduction schemes. A practical solution is to predict many\nquantiles(for example, q=[0.05, 0.1, 0.15...,0.95]), and select\na proper quantile as detector of anomalies according to real\nsituations.\nTo illustrate the conclusion above, we re-trained the model\nusing the same dataset and hyperparameters, only altered the\nquantiles to q = [0.05, 0.5, 0.95]. The new model’s confusion\nmatrix and metrics are shown in table 9 and table 10\nTABLE 9. Confusion matrix of new quantiles for RDC\nactual\nvalue\nRedis Prediction outcome\np n total\np′ True Pos.\n= 11\nFalse Neg.\n= 6 P′ = 17\nn′ False Pos.\n= 20\nTrue Neg.\n= 43156 N′ = 43163\ntotal P = 31 N = 43169\nTABLE 10. Production results on RDC(new quantiles)\nmetric value\nrecall 64.7%\nprecision 35.5%\nF1-score 0.4583\nShifting the quantile loss (q=0.1/q=0.9) towards 0/1 should\nimprove precision while lowering recall. The selection of\nquantile will greatly influence the performance of model in\nproduction schemes. A practical solution is to predict many\nquantiles(for example, q=[0.05, 0.1, 0.15...,0.95]), and select\na proper quantile as detector of anomalies according to real\nsituations.\nOur investigation shows that in our production setting,\nhuman operators slightly favor recall over precision. There-\nfore, we chose q = [0 .1, 0.5, 0.9] even when other choice\n([0.05, 0.5, 0.9]) has a higher F1 score.\nMore generally, the choice between recall and precision\nis determined by the nature of the system that generates our\nmetrics. When it is easy to validate whether an actual error\noccurs, or every occurence of the error tends to have critical\nconsequences, operators favor recall over precision; when the\nerror can be resolved automatically, operators tend to favor\nprecision over recall to eliminate false positives.\nH. EFFICIENCY\nThe time cost on prediction stage of TFTOps model is listed\nin table 11.\nThe column “Avg. time” is the average time to process\na slice of data, which contains metrics generated by the\nwhole system (#Seq ∗lencoder), and the model should make\n(#Seq ∗ ldecoder) predictions.\nWhen a forward pass is run, the data flows through an\nLSTM layer and a stack of self-attention layers. The time\ncomplexity ties closely with the total sequence length of\nencoder and decoder, le + ld. For LSTM, its complexity\nis O(le + ld), which is ruled out by self-attention layer’s\nVOLUME 4, 2016 13\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3353201\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nHaoran Luo et al.: Probabilistic Temporal Fusion Transformers for Large-Scale KPI Anomaly Detection\nTABLE 11. Efficiency on prediction stage\nDataset #Seq lencoder ldecoder Avg. time(s)\nPNE 528 48 24 55.23±5.62\nRDC 34 50 10 4.55±1.01\ncomplexity O((le + ld)2). Depending on the degree of par-\nallelism, the LSTM layer generally costs more time when\n(le + ld) is relatively small, which is our case. Note that,\nexcept the LSTM layer, the model is purely feed-forward (the\nresult generated by LSTM does not feedback into previous\nlayers), thus the performance of pre-processing and output\ndense layers are theoretically better than LSTM; In the\nprediction stage, the prediction time is roughly linear with\nnumber of sequences in the system ( n = #Seq). Thus, the\noverall complexity is O(n(le +ld))(for shorter sequences) or\nO(n(le + ld)2)(for longer sequences).\nIn the PNE dataset task, we retrieve data every 5 minutes;\nin RDC dataset task, we retrieve data once per minute. Ac-\ncording to table 11, in both cases, the efficiency of TFTOps\nmodel meets our production requirements.\nV. CONCLUSIONS\nWe proposed TFTOps, a variant of Temporal Fusion Trans-\nformer [3] designed for AIOps unsupervised anomaly detec-\ntion tasks. We also improve TFTOps by introducing proba-\nbilistic inputs, which further boosts the accuracy of TFTOps\nin our experiments. Our findings prove that the TFT model\nconcept is well-suited for a modern multi-metric monitoring\nsetup, satisfying requirements on both accuracy and effi-\nciency.\nThe merits of TFTOps includes:\n• Flexiblity. User can include various features (real-\nvalued / categorical / probabilistic, variable / static)\nin their hypothesis. TFTOps provides an elegant way\nto blend the features into its training and prediction\nprocess.\n• Robustness. If the new feature is proven to be noise,\nthe TFTOps model has stronger robustness compared to\nother neural network models. Therefore, it shortens the\ntime-consuming scheme of feature selection. With little\neffort, the operators can generate valuable predictions\nwith the off-the-shelf TFTOps model.\nVI. FUTURE WORKS\nIn our experiments, we treat each sequence independently\n(except they share some categorical static inputs). Intuitively,\nin k8s or other cluster settings, introducing features from\n“related” nodes would be beneficial for the accuracy of our\nmodel. Besides, other possible data sources, such as embed-\nded system logs, can be used as a feature. We leave both\ndirections for future works.\nREFERENCES\n[1] H. Xu, W. Chen, N. Zhao, Z. Li, J. Bu, Z. Li, Y . Liu, Y . Zhao, D. Pei,\nY . Feng, J. Chen, Z. Wang, and H. Qiao, “Unsupervised anomaly detection\nvia variational auto-encoder for seasonal kpis in web applications,” in\nProceedings of the 2018 World Wide Web Conference , ser. WWW ’18.\nRepublic and Canton of Geneva, CHE: International World Wide Web\nConferences Steering Committee, 2018, p. 187–196. [Online]. Available:\nhttps://doi.org/10.1145/3178876.3185996\n[2] S. Aghabozorgi, A. S. Shirkhorshidi, and T. Y . Wah, “Time-series\nclustering–a decade review,” Information Systems , vol. 53, pp. 16–38,\n2015.\n[3] B. Lim, S. Ö. Arık, N. Loeff, and T. Pfister, “Temporal fusion transform-\ners for interpretable multi-horizon time series forecasting,” International\nJournal of Forecasting, vol. 37, no. 4, pp. 1748–1764, 2021.\n[4] G. E. Box, G. M. Jenkins, G. C. Reinsel, and G. M. Ljung, Time series\nanalysis: forecasting and control. John Wiley & Sons, 2015.\n[5] R. J. Hyndman and G. Athanasopoulos, Forecasting: principles and prac-\ntice. OTexts, 2018.\n[6] I. Sutskever, O. Vinyals, and Q. V . Le, “Sequence to sequence learning with\nneural networks,” Advances in neural information processing systems ,\nvol. 27, 2014.\n[7] D. Salinas, V . Flunkert, J. Gasthaus, and T. Januschowski, “Deepar: Prob-\nabilistic forecasting with autoregressive recurrent networks,”International\nJournal of Forecasting, vol. 36, no. 3, pp. 1181–1191, 2020.\n[8] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural\ncomputation, vol. 9, no. 8, pp. 1735–1780, 1997.\n[9] J. Chung, C. Gulcehre, K. Cho, and Y . Bengio, “Empirical evaluation of\ngated recurrent neural networks on sequence modeling,” in NIPS 2014\nWorkshop on Deep Learning, December 2014, 2014.\n[10] S. S. Rangapuram, M. W. Seeger, J. Gasthaus, L. Stella, Y . Wang, and\nT. Januschowski, “Deep state space models for time series forecasting,”\nAdvances in neural information processing systems, vol. 31, 2018.\n[11] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,”\nin 2nd International Conference on Learning Representations, ICLR\n2014, Banff, AB, Canada, April 14-16, 2014, Conference Track\nProceedings, Y . Bengio and Y . LeCun, Eds., 2014. [Online]. Available:\nhttp://arxiv.org/abs/1312.6114\n[12] S. Li, X. Jin, Y . Xuan, X. Zhou, W. Chen, Y .-X. Wang, and X. Yan, “En-\nhancing the locality and breaking the memory bottleneck of transformer\non time series forecasting,” Advances in neural information processing\nsystems, vol. 32, 2019.\n[13] C. Fan, Y . Zhang, Y . Pan, X. Li, C. Zhang, R. Yuan, D. Wu, W. Wang,\nJ. Pei, and H. Huang, “Multi-horizon time series forecasting with temporal\nattention learning,” in Proceedings of the 25th ACM SIGKDD Interna-\ntional conference on knowledge discovery & data mining, 2019, pp. 2527–\n2535.\n[14] M. Khodayar, O. Kaynak, and M. E. Khodayar, “Rough deep neural\narchitecture for short-term wind speed forecasting,”IEEE Transactions on\nIndustrial Informatics, vol. 13, no. 6, pp. 2770–2779, 2017.\n[15] M. Khodayar, J. Wang, and Z. Wang, “Energy disaggregation via deep\ntemporal dictionary learning,” IEEE transactions on neural networks and\nlearning systems, vol. 31, no. 5, pp. 1696–1709, 2019.\n[16] R. Wen, K. Torkkola, B. Narayanaswamy, and D. Madeka, “A multi-\nhorizon quantile recurrent forecaster,” arXiv preprint arXiv:1711.11053 ,\n2017.\n[17] F. Ayed, L. Stella, T. Januschowski, and J. Gasthaus, “Anomaly detection\nat scale: The case for deep distributional time series models,” in Interna-\ntional Conference on Service-Oriented Computing . Springer, 2020, pp.\n97–109.\n[18] A. Nandi, A. Mandal, S. Atreja, G. B. Dasgupta, and S. Bhattacharya,\n“Anomaly detection using program control flow graph mining from ex-\necution logs,” in Proceedings of the 22nd ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining , 2016, pp. 215–\n224.\n[19] P. Liu, H. Xu, Q. Ouyang, R. Jiao, Z. Chen, S. Zhang, J. Yang, L. Mo,\nJ. Zeng, W. Xue et al. , “Unsupervised detection of microservice trace\nanomalies through service-level deep bayesian networks,” in 2020 IEEE\n31st International Symposium on Software Reliability Engineering (IS-\nSRE). IEEE, 2020, pp. 48–58.\n[20] J. Soldani and A. Brogi, “Anomaly detection and failure root cause\nanalysis in (micro) service-based cloud applications: A survey,” ACM\nComputing Surveys (CSUR), vol. 55, no. 3, pp. 1–39, 2022.\n[21] B. Rabenstein and J. V olz, “Prometheus: A next-generation monitoring\nsystem (talk).” Dublin: USENIX Association, May 2015.\n[22] T. O. et.al., “Grafana: The open-source platform for monitoring and\nobservability.” [Online]. Available: https://github.com/grafana/grafana\n14 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3353201\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nHaoran Luo et al.: Probabilistic Temporal Fusion Transformers for Large-Scale KPI Anomaly Detection\n[23] Prometheus, “Querying basics: Prometheus.” [Online]. Available:\nhttps://prometheus.io/docs/prometheus/latest/querying/basics/\n[24] S. Fu, “Performance metric selection for autonomic anomaly detection\non cloud computing systems,” in 2011 IEEE Global Telecommunications\nConference-GLOBECOM 2011. IEEE, 2011, pp. 1–5.\n[25] G. Jung, G. Swint, J. Parekh, C. Pu, and A. Sahai, “Detecting bottleneck\nin n-tier it applications through analysis,” in International Workshop on\nDistributed Systems: Operations and Management . Springer, 2006, pp.\n149–160.\n[26] J. Parekh, G. Jung, G. Swint, C. Pu, and A. Sahai, “Issues in bottleneck\ndetection in multi-tier enterprise applications,” in 200614th IEEE Interna-\ntional Workshop on Quality of Service. IEEE, 2006, pp. 302–303.\n[27] Y . Tan, Online performance anomaly prediction and prevention for com-\nplex distributed systems. North Carolina State University, 2012.\n[28] X. Gu and H. Wang, “Online anomaly prediction for robust cluster sys-\ntems,” in 2009 IEEE 25th International Conference on Data Engineering.\nIEEE, 2009, pp. 1000–1011.\n[29] R. Powers, M. Goldszmidt, and I. Cohen, “Short term performance\nforecasting in enterprise systems,” in Proceedings of the eleventh ACM\nSIGKDD international conference on Knowledge discovery in data min-\ning, 2005, pp. 801–807.\n[30] T. Wang, W. Zhang, J. Wei, and H. Zhong, “Workload-aware online\nanomaly detection in enterprise applications with local outlier factor,” in\n2012 IEEE 36th Annual Computer Software and Applications Conference.\nIEEE, 2012, pp. 25–34.\n[31] D. J. Dean, H. Nguyen, and X. Gu, “Ubl: Unsupervised behavior learning\nfor predicting performance anomalies in virtualized cloud systems,” in\nProceedings of the 9th international conference on Autonomic computing,\n2012, pp. 191–200.\n[32] R. J. Hyndman, E. Wang, and N. Laptev, “Large-scale unusual time\nseries detection,” in 2015 IEEE international conference on data mining\nworkshop (ICDMW). IEEE, 2015, pp. 1616–1619.\n[33] C. Monni, M. Pezzè, and G. Prisco, “An rbm anomaly detector for the\ncloud,” in2019 12th IEEE Conference on Software Testing, Validation and\nVerification (ICST). IEEE, 2019, pp. 148–159.\n[34] G. E. Hinton and R. R. Salakhutdinov, “Reducing the dimensionality of\ndata with neural networks,”science, vol. 313, no. 5786, pp. 504–507, 2006.\n[35] T.-Y . Liu et al., “Learning to rank for information retrieval,” Foundations\nand Trends® in Information Retrieval, vol. 3, no. 3, pp. 225–331, 2009.\n[36] Y . Liang, S. Ke, J. Zhang, X. Yi, and Y . Zheng, “Geoman: Multi-level\nattention networks for geo-sensory time series prediction.” in IJCAI, vol.\n2018, 2018, pp. 3428–3434.\n[37] Y . Liu, C. Gong, L. Yang, and Y . Chen, “DSTP-RNN: A dual-stage\ntwo-phase attention-based recurrent neural network for long-term and\nmultivariate time series prediction,” Expert Syst. Appl. , vol. 143, 2020.\n[Online]. Available: https://doi.org/10.1016/j.eswa.2019.113082\n[38] C. Wang, K. Wu, T. Zhou, G. Yu, and Z. Cai, “Tsagen: synthetic time series\ngeneration for kpi anomaly detection,”IEEE Transactions on Network and\nService Management, vol. 19, no. 1, pp. 130–145, 2021.\n[39] H. Ren, B. Xu, Y . Wang, C. Yi, C. Huang, X. Kou, T. Xing, M. Yang,\nJ. Tong, and Q. Zhang, “Time-series anomaly detection service at mi-\ncrosoft,” in Proceedings of the 25th ACM SIGKDD international confer-\nence on knowledge discovery & data mining, 2019, pp. 3009–3017.\n[40] H. Zhou, S. Zhang, J. Peng, S. Zhang, J. Li, H. Xiong, and W. Zhang,\n“Informer: Beyond efficient transformer for long sequence time-series\nforecasting,” in Proceedings of the AAAI conference on artificial intelli-\ngence, vol. 35, no. 12, 2021, pp. 11 106–11 115.\n[41] H. Wu, J. Xu, J. Wang, and M. Long, “Autoformer: Decomposition trans-\nformers with auto-correlation for long-term series forecasting,” Advances\nin Neural Information Processing Systems , vol. 34, pp. 22 419–22 430,\n2021.\n[42] D.-A. Clevert, T. Unterthiner, and S. Hochreiter, “Fast and accurate\ndeep network learning by exponential linear units (elus),” arXiv preprint\narXiv:1511.07289, 2015.\n[43] C. Favorita. (2018) Corporacion favorita grocery sales forecasting\ncompetition. [Online]. Available: https://www.kaggle.com/c/favorita-\ngrocery-sales-forecasting/\n[44] A. Graves, A.-r. Mohamed, and G. Hinton, “Speech recognition with deep\nrecurrent neural networks,” in 2013 IEEE international conference on\nacoustics, speech and signal processing. Ieee, 2013, pp. 6645–6649.\n[45] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in\nneural information processing systems, vol. 30, 2017.\nHAORAN LUO was born in Fujian, China, in\n1996. He received the B.S. degree from Shanghai\nJiao Tong University, Shanghai, China in 2018,\nand received the M.S. degree in computer science\nfrom the same institution in 2021. From 2018\nto 2019, he was an Intern with Bosch (China)\nInvestment Company Ltd. From 2019 to 2020,\nhe was an Intern with the Algorithm Department,\n360 Digitech Inc., Shanghai. He is currently a\nResearcher with the China Telecom Research In-\nstitute, Guangzhou, China since 2021. His research interests are machine\nlearning, devOps, AIOps and natural language processing.\nYONGKUN ZHENG was born in Shantou,\nGuangdong, China in 1990. He received the M.S.\ndegree in software engineering from Southeast\nUniversity in 2016. From 2014 to 2015, he was\nan Intern at the Cloud Computing Center of the\nChinese Academy of Sciences, engaged in re-\nsearch on the application of big data in remote\nsensing technology. In 2015, he was an Intern at\nBaidu, engaged in data analysis. Since 2016, he\nis currently a Researcher with the China Telecom\nResearch Institute in Guangzhou, China. His main research interests are\nAIOps, devOps and big data analytics.\nKANG CHEN was born in Anhui, China, in 1972.\nHe received the B.S. degree from Nanjing Univer-\nsity, Nanjing, China in 1993, and received the M.S.\ndegree in computer science from Jinan University,\nGuangzhou, China in 1999. From 1999 to now, he\nis a Researcher with the China Telecom Research\nInstitute, Guangzhou, China. His research interests\ninclude big data, machine learning.\nSHUO ZHAO was born in Jilin, China, in 1988.\nHe received the B.S. degree in Computer Sci-\nence and Technology from Beijing University of\nPosts and Telecommunications, Beijing, China in\n2011,and received the M.S. degree in computer\nscience from the same institution in 2014. From\n2014 to the present, He is a IT Engineer with\nthe China Telecom Corporation Limited, Beijing,\nChina. His research interests are machine learning,\nAIOps.\nVOLUME 4, 2016 15\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3353201\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nHaoran Luo et al.: Probabilistic Temporal Fusion Transformers for Large-Scale KPI Anomaly Detection\nVII. APPENDIX\nA. HYPERPARAMETERS\nWe use the following grid search scheme for the optimal\nhyperparameters in experiments on both PNE and RDC\ndatasets.\nThe parameter name, search space and effect of hyperpa-\nrameters are listed below:\n• Dropout rate: {0.1, 0.2}. This parameter controls prob-\nability of dropout in GLU layer before gating layer and\nlayer normalization.\n• Hidden layer size: {4, 8, 16, 32}. This controls the size\nof vector input/output of hidden layer(dmodel).\n• Learning rate: {0.1, 0.01, 0.001}. Fine-grained tuning\ncan be conducted according to dataset to further im-\nprove the result.\n• #heads: {4,6,8}. The number of heads in multi-head\nattention layers.\n• Stack size: {1,2,3,4}. the number of stacking self-\nattention (SA) layers.\nFor the PNE dataset, optimal hyperparameters are:\n• Dropout rate: 0.2\n• Learning rate: 0.001\n• Hidden layer size: 8\n• #heads: 4\n• Stack size: 4\nFor the RDC dataset, optimal hyperparameters are:\n• Dropout rate: 0.1\n• Learning rate: 0.01\n• Hidden layer size: 8\n• #heads: 4\n• Stack size: 2\nAccording to our observations, both datasets does not\nrequire a large embedding/feature space. On the other hand,\nthey do need to stack some self-attention layers to achieve\nthe optimal performance. Intuitively, higher dimension of\nembeddings means we can extract rich information from\ninput features(static and variable). However, for our dataset,\nthe inter-relationship among different timesteps of the metric\nitself seems to be more important, which is mainly modeled\nby the LSTM and Transformer layers.\n16 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3353201\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8320505619049072
    },
    {
      "name": "Anomaly detection",
      "score": 0.7649273872375488
    },
    {
      "name": "Probabilistic logic",
      "score": 0.6780301332473755
    },
    {
      "name": "Scalability",
      "score": 0.625737190246582
    },
    {
      "name": "Artificial neural network",
      "score": 0.5301241278648376
    },
    {
      "name": "Data mining",
      "score": 0.49487632513046265
    },
    {
      "name": "Time series",
      "score": 0.45972201228141785
    },
    {
      "name": "Artificial intelligence",
      "score": 0.44582003355026245
    },
    {
      "name": "Machine learning",
      "score": 0.39458662271499634
    },
    {
      "name": "Database",
      "score": 0.0
    }
  ]
}