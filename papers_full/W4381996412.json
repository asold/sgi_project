{
  "title": "A Deep Features Extraction Model Based on the Transfer Learning Model and Vision Transformer “TLMViT” for Plant Disease Classification",
  "url": "https://openalex.org/W4381996412",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3037243072",
      "name": "Amer Tabbakh",
      "affiliations": [
        "VIT-AP University",
        "SRM University"
      ]
    },
    {
      "id": "https://openalex.org/A254748631",
      "name": "Soubhagya Sankar Barpanda",
      "affiliations": [
        "SRM University",
        "VIT-AP University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2937772171",
    "https://openalex.org/W2921403460",
    "https://openalex.org/W2987984071",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2735962203",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2463733399",
    "https://openalex.org/W6637373629",
    "https://openalex.org/W2618530766",
    "https://openalex.org/W3006334803",
    "https://openalex.org/W6640888669",
    "https://openalex.org/W4385524619",
    "https://openalex.org/W4282974753",
    "https://openalex.org/W2753403518",
    "https://openalex.org/W3167324598",
    "https://openalex.org/W2923504698",
    "https://openalex.org/W2914622272",
    "https://openalex.org/W3212101387",
    "https://openalex.org/W6741502269",
    "https://openalex.org/W2828723651",
    "https://openalex.org/W2983446232",
    "https://openalex.org/W3126911992",
    "https://openalex.org/W4284670866",
    "https://openalex.org/W4210839135",
    "https://openalex.org/W2782517840",
    "https://openalex.org/W6686540329",
    "https://openalex.org/W4282964635",
    "https://openalex.org/W2076063813",
    "https://openalex.org/W6737658843",
    "https://openalex.org/W2807383083",
    "https://openalex.org/W2277854822",
    "https://openalex.org/W2473156356",
    "https://openalex.org/W4214549633",
    "https://openalex.org/W4210247145",
    "https://openalex.org/W2938959907",
    "https://openalex.org/W2808709127",
    "https://openalex.org/W3113062486",
    "https://openalex.org/W4224290640",
    "https://openalex.org/W3198733094",
    "https://openalex.org/W4224941580",
    "https://openalex.org/W4316464168",
    "https://openalex.org/W3033519704",
    "https://openalex.org/W2886590014",
    "https://openalex.org/W2790979755",
    "https://openalex.org/W2913766929",
    "https://openalex.org/W4285815295",
    "https://openalex.org/W3135345009",
    "https://openalex.org/W3213196529",
    "https://openalex.org/W3166424296",
    "https://openalex.org/W4321488561",
    "https://openalex.org/W3165026920",
    "https://openalex.org/W4286681597",
    "https://openalex.org/W2548258044",
    "https://openalex.org/W3091779296",
    "https://openalex.org/W2910363199",
    "https://openalex.org/W3127260633",
    "https://openalex.org/W2782434976",
    "https://openalex.org/W3042616902",
    "https://openalex.org/W3161794449",
    "https://openalex.org/W3119027282",
    "https://openalex.org/W3125414748",
    "https://openalex.org/W3108454656",
    "https://openalex.org/W4205549623",
    "https://openalex.org/W2801303530",
    "https://openalex.org/W4220986405",
    "https://openalex.org/W2970362668",
    "https://openalex.org/W3134031383",
    "https://openalex.org/W3180916240",
    "https://openalex.org/W2738029666",
    "https://openalex.org/W2185841109",
    "https://openalex.org/W2613634265",
    "https://openalex.org/W3100931193",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W1959279583"
  ],
  "abstract": "This paper proposes a novel approach for extracting deep features and classifying diseased plant leaves. The agriculture industry is negatively impacted by plant diseases causing crop and economic loss. Accurate and timely diagnosis is crucial for managing and controlling plant diseases, as traditional methods can be costly and time-consuming. Deep learning-based tools effectively detect plant diseases depending on the qualitative of extracted features. In this regard, a hybrid model for plant disease classification based on a Transfer Learning-based model followed by a vision transformer (TLMViT) is proposed. TLMViT has four stages: 1) data acquisition, where the PlantVillage and wheat datasets are used to train and evaluate the proposed model, 2) image augmentation to increase the number of training samples and overcome the overfitting issue, 3) leaf feature extraction by two consecutive phases: initial features extraction by using pre-trained based model and deep features extraction by using ViT model, and 4) classification by using MLP classifier. TLMViT is experimented with using five pre-trained-based models followed by ViT individually. TLMViT performs accurately in plant disease classification, obtaining 98.81&#x0025; and 99.86&#x0025; validation accuracy for VGG19 followed by the ViT model on PlantVillage and wheat datasets respectively. Moreover, TLMViT is compared with pre-trained-based architecture. The comparison result illustrates that TLMViT achieved an enhancement of 1.11&#x0025; and 1.099&#x0025; in validation accuracy, 2.576&#x0025; and 2.92&#x0025; in validation loss compared with the transfer learning-based model for PlantVillage and wheat datasets respectively. Thereby proposed model proves the efficiency of using ViT for extracting deep features from the leaf.",
  "full_text": "Received 8 April 2023, accepted 24 April 2023, date of publication 5 May 2023, date of current version 12 May 2023.\nDigital Object Identifier 10.1 109/ACCESS.2023.3273317\nA Deep Features Extraction Model Based on the\nTransfer Learning Model and Vision Transformer\n‘‘TLMViT’’ for Plant Disease Classification\nAMER TABBAKH\n AND SOUBHAGYA SANKAR BARPANDA\nSchool of Computer Science and Engineering, VIT-AP University, Amaravati 522237, India\nCorresponding author: Soubhagya Sankar Barpanda (soubhagya.barpanda@vitap.ac.in)\nABSTRACT This paper proposes a novel approach for extracting deep features and classifying diseased\nplant leaves. The agriculture industry is negatively impacted by plant diseases causing crop and economic\nloss. Accurate and timely diagnosis is crucial for managing and controlling plant diseases, as traditional\nmethods can be costly and time-consuming. Deep learning-based tools effectively detect plant diseases\ndepending on the qualitative of extracted features. In this regard, a hybrid model for plant disease classi-\nfication based on a Transfer Learning-based model followed by a vision transformer (TLMViT) is proposed.\nTLMViT has four stages: 1) data acquisition, where the PlantVillage and wheat datasets are used to train\nand evaluate the proposed model, 2) image augmentation to increase the number of training samples and\novercome the overfitting issue, 3) leaf feature extraction by two consecutive phases: initial features extraction\nby using pre-trained based model and deep features extraction by using ViT model, and 4) classification\nby using MLP classifier. TLMViT is experimented with using five pre-trained-based models followed by\nViT individually. TLMViT performs accurately in plant disease classification, obtaining 98.81% and 99.86%\nvalidation accuracy for VGG19 followed by the ViT model on PlantVillage and wheat datasets respectively.\nMoreover, TLMViT is compared with pre-trained-based architecture. The comparison result illustrates\nthat TLMViT achieved an enhancement of 1.11% and 1.099% in validation accuracy, 2.576% and 2.92%\nin validation loss compared with the transfer learning-based model for PlantVillage and wheat datasets\nrespectively. Thereby proposed model proves the efficiency of using ViT for extracting deep features from\nthe leaf.\nINDEX TERMS Plant disease, image processing, deep learning, transfer learning, vision transformer.\nI. INTRODUCTION\nPlant diseases have become more prevalent in recent years\ndue to globalization, trade, and climate change [1], [2]. These\nissues have reached pandemic proportions in several nations,\nwhich increased the likelihood of crop damage and, in turn,\ncreated a threat to people’s access to adequate food and\nnutrition [3]. Specialists should ensure to safeguard agri-\ncultural plants. Parasitic organisms such as bacteria, fungi,\nviruses, roundworms (nematodes), and other plants can cause\nillnesses. Environmental conditions such as winter frosts or\nsummer dryness and lack or excess of nutrients in the soil can\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Larbi Boubchir\n.\nbe the potential causes of plant diseases [4]. Phytopathology\nis the scientific study of plant disease that focuses on ways to\ntreat and avoid the conditions responsible for plant illnesses.\nTo overcome plant illness, plants have to be diagnosed pre-\ncisely. There are several methods available for diagnosing.\nLocal plant clinics and agricultural groups have traditionally\nhelped in disease detection. Still, the technique could be more\neffective as there are more possibilities of human errors, and\nit is difficult for humans to access plants across a wide area.\nMoreover, using software using machine learning techniques\ncan improve the efficiency of classifying diseased plants.\nSmartphones are being developed using different tools\nand technology [6]. Modern plant illness classification\napproaches can be adopted using smartphones due to\nVOLUME 11, 2023\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License.\nFor more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/ 45377\nA. Tabbakh, S. S. Barpanda: Deep Features Extraction Model\ntheir incorporation of high computational techniques,\nhigh-resolution screens, and built-in accessories such as\nHD cameras, which will be more effective for plant disease\ndetection. Plant diseases are effectively diagnosed using\nmachine learning techniques [4] as accurate diagnostic tests\ncan be carried out, which benefits in preserving the resource.\nFarmers can upload field images recognized by smartphones,\nand distinct software can be used for analyzing, diagnosing,\nand developing action plans.\nRecently, image processing techniques with deep neural\nnetworks have been effectively used diversely and proven to\nbe highly effective approaches in constantly monitoring the\nhealth of plants and identifying signs of diseases in their early\nstages [5]. A neural network considers an image of a diseased\nplant as an input and processes the image to produce a crop-\ndisease pair. Creating a deep network in such a manner that\nthe network topology, functions (nodes), and edge weights\naccurately map the input to output is challenging. While\ntraining deep neural networks, the network parameters are\nadjusted in such a manner that the mapping is enhanced\nbetter over the training period. This complex computational\nprocess has recently seen several conceptual and practical\nadvancements that have dramatically increased its perfor-\nmance [7], [8]. One of these approaches is Vision Trans-\nformer, which takes the whole image and extracts its features\nby splitting the image into multi patches, and from each patch,\nthe transformer encoder will extract the features. Extracting\nthe features from the whole image may take much more\ntime and extract unnecessary features, e.g., a background of\nthe leaf. Above all, this paper presents a novel approach to\nclassify plant disease using pre-trained deep learning model\nfollowed by ViT. The pre-trained model extracts the initial\nfeatures from the base convolutional networks that are already\nfine-tuned and reduces the dimensionality of the image mak-\ning the next stage less complex. Then the initialled features\nare passed as input to ViT to extract the most significant fea-\ntures (deep features). A simple MLP is used to classify these\ndeep features to which classes belong. The main contributions\nof this paper are pointed out as follows:\n1) Using a pre-trained model to reduce the dimensionality\nof the image, making the next stage less complex.\n2) Extract the deep feature of the leaf using a combination\nof the pre-trained model and ViT making the classifier\nmore accurate.\n3) Experiment with five different combinations of transfer\nlearning-based models (such as ResNet50, AlexNet,\nInception V3, VGG16, and VGG19) with ViT.\n4) Comparisons between the TLMViT and five other\ntransfer learning-based models are made.\nThe rest of this paper is structured as follows. Section II\nof the study offers a brief review of the relevant research.\nSection IV introduces the background of the methods used.\nSection V outlines the workflow and methodology used, fol-\nlowed by the experimental parameters in Section VI. Exper-\niment outcomes with extensive discussion are presented in\nSection VII. The summary of this research is addressed in\nsection VIII.\nII. RELATED WORK\nPlant disease diagnostics using machine learning and deep\nlearning have been a primary focus of recent research. The\nreview papers [1], [2], [9], [10] provided a summary of dif-\nferent novel approaches, developed existing algorithms used\nto classify plant diseases, and sought to determine which clas-\nsification method would be the most useful for this challenge.\nDifferent datasets have been utilized to classify various plant\nleaves. References [11], [12], and [13] proposed a model\nto classify a tomato leaves disease. At the same time, Dey\net al. [14] concentrated on betel vines. References [15], [16],\n[17], and [18] utilized Cassava disease. References [19], [20],\n[21], [22], [23] focused on classifying potato leaf disease.\nReferences [24] and [25] performed the proposed work on\na dataset of Guava leaves. Singh and Misra [26] focused\non some of the most common leaves of banana, jackfruit,\nlemon, mango, potatoes, tomato, and sapota. Sharif et al. [27]\nexperimented with the proposed work on the oranges infested\nwith scale, Plant Village Dataset, Citrus Diseases Image\nGallery Dataset, and their collected citrus dataset. Hassan and\nMaji [28] utilized the plant village dataset (which includes\nmaize, potato, and tomato crops), the rice dataset, and the\ncassava dataset were all used in their experimental work for\nthe study. In contrast, The CropDP-181 dataset was devel-\noped by Kong et al. [29] by combining three existing datasets\ntotalling 123,987 images (from the AIChalle, Inaturalist, and\nIP102) that together contain 47 diseases and 134 pest cate-\ngories. In [30], the dataset was collected from various sources,\nincluding (Apple plants, Wheat, Cotton, Maize, and Rice).\nMachine learning models are developed to classify\nplant leaf disease and achieve good performance. In this\nregard, the following is a discussion of some methods.\nMokhtar et al. [11] proposed an approach that identifies\na diseased leaf without knowing the class of the dis-\nease. An essential feature of the tomato leaf is its sur-\nface texture, so GLCM is used where Energy, Contrast,\nSum of Squares, Correlation, Entropy, Sum Entropy, Cluster\nShade, Cluster Prominence, and Homogeneity are extracted.\nSVM is used as a classifier with different kernel functions.\nBhargava et al. [31] applied different machine learning\napproaches, such as LR and SVM to classify varieties of\nleaves of vegetables and fruits. The dataset is segmented\nusing grab-cut and fuzzy c-means clustering. Then the\nfeatures were extracted by discrete wavelet transform, the\nhistogram of gradients, Laws’ texture energy, textural, statis-\ntical, and colorgeometrical with 114 features. PCA is used\nfor feature selection to reduce the dimensionality of up to\n30 features. The classification results show the significance of\nfeature selection and the increased classification recognition\nrate. Sharif et al. [27] detected the lesion on leaves using\nan optimized weighted segmentation method on enhanced\nimages obtained in preprocessing stage. Then the features\n45378 VOLUME 11, 2023\nA. Tabbakh, S. S. Barpanda: Deep Features Extraction Model\nare extracted and fused using color (HSV , LAB, LUV ,\nand HSI), geometric, and texture (18 GLCM features). The\nbest features are selected using principal component analysis\n(PCA) estimation. The multi-class support vector machine\n(M-SVM) is used to classify citrus diseases. They conclude\nthat enhancing the contrast of the lesion part will improve\nsegmentation accuracy. Bhargava and Bansal [32] focused\non segmenting the lesion from the mono-colored apples\nusing two algorithms, e.g., ‘‘Otsu’’ strategy and k- means\nclustering. Then a combination of color coherence, Gabor\nwavelet, 14 geometric features, and 13 statistical & textural\nfeatures that are extracted from the segmented image are\nused as features. The proposed model is trained using linear\nSVM. From the result, they conclude that the proposed model\nachieves a good performance compared with other kernels of\nthe SVM classifier. In [21], the K-means algorithm extracts\nthe infected regions from the leaf and then extracts the lesion\nfeatures using GLCM. They considered feature extraction\nas a type of dimensionality reduction process. Pahurkar and\nDeshmukh [30] propose a model that optimizes variance\nby fusing GLCM, edge map, color map, and convolutional\nfeature sets into an ensemble of features and then using\nparticle swarm optimization (PSO) to choose the optimal\nfeatures. The parametric variant classification techniques are\nalso tuned with a Genetic algorithm (GA). Tabbakh and\nBarpanda [33] proposed an approach of a machine-learning\nmodel where wavelet transforms, GLCM methods, and sta-\ntistical features are used to extract different combinations\nof leaf features. Then the extracted features are utilized for\ntraining and comparing six machine-learning models, e.g.,\nSVM, AdaBoost, etc. They modified the GLCM approach to\nfocus on extracting the features of the leaf part only. SMOTE\nmethod is used to handle the imbalanced dataset. The highest\nresult is achieved by using LGBM with 94.39% accuracy.\nDeep learning has proven to be an effective tool for\nimage classification tasks, outperforming traditional methods\nsuch as support vector machines and random forests. Deep\nlearning algorithms require large datasets to train models\neffectively. In [34], cropping the lesion portion from the\noriginal image and making them a new sample is used to\novercome the lack of dataset size. Whereas Geetharamani and\nPandian [35] augmented the dataset using various functions\nsuch as image flipping, scaling, rotation, principal compo-\nnent analysis (PCA) color augmentation, noise injection, and\ngamma correction. From the result, the authors observed that\nthe data augmentation methods could improve the model\nperformance. References [14], [26], and [36] used an image\nsegmentation algorithm to identify damaged leaf areas to\nanalyze better and have little processing effort to get optimum\nresults. Dey et al. [14] found that using HVS color space\non their dataset, the hue component shows clearly where\na leaf has rotted. In addition, the hue component masks\nthe background and the rest of the leaf region. Singh and\nMisra [26] segmented the lesion by masking the mostly green\npixels. Chaturvedi et al. [36] proposed a modified firefly\nalgorithm based on multilevel thresholding with fuzzy, Tsal-\nlis, and Kapur’s entropy for various fruit leaf segmentation.\nKhalifa et al. [37] augmented the dataset (from 1,722 to\n9,822 images of potato leaves) using Reflection, Zoom, and\nGaussian noise functions. They proposed a simple CNN using\ntwo convolutional layers for feature extraction and two fully\nconnected layers for classification achieving 98% accuracy\nas overall mean testing. Whereas, Rozaqi and Sunyoto [20]\nused four convolutions and one fully connected layer on\nthe same dataset of potato leaf. They achieved 97% and\n92% accuracy on training and validation data, respectively.\nMittal and Gupta [38] focused on studies that show how\ncreating a complete and unusual image of a diseased leaf and\nincreasing the dataset may help the classification network\nperform better. A binary generator network is proposed to\naddress the issue of how a generative adversarial network\n(GAN) generates a diseased leaf on a specific shape. Also,\nusing edge-smoothing and Image pyramid methods to over-\ncome the challenge of synthesising a complete lesion leaf\nimage comprised of various synthetic edge pixels and net-\nwork out pixels. Khamparia et al. [19] benefited the properties\nof the encoding part of the autoencoders model to extract\nuseful features. They proposed a convolutional encoder net-\nwork that only combines CNN with the encoding part. More-\nover, 97.50% accuracy was achieved after 100 iterations.\nLiang et al. [39] presented a network-based estimate method\nfor disease identification using ResNet50 and residual where\nShuffle-Net-network v2’s architecture is used to reduce com-\nputational complexity. Johnson et al. [22] developed an auto-\nmatic method for identifying blight disease patches on potato\nleaves in field circumstances utilizing the Mask Region-based\nconvolutional neural network (Mask R-CNN) architecture\nand a residual network as the backbone network. The Mask\nR-CNN model accurately distinguished between the infected\narea on the potato leaf and the similar-looking background\nsoil patches that often affect binary classification results. The\nproposed work is experimented after converting the original\ndataset from RGB to YCrCb, XYZ, LAB, HSV , and HSL\ncolor spaces, giving a separate model for each color space.\nHassan and Maji [28] have suggested a new deep-learning\nmodel using the inception layer and residual connection. The\nnumber of parameters in the proposed work is reduced by a\nmargin of 70% using depthwise separable convolution, which\ndirectly impacts the computing cost.\nRecent works [18], [40], [41], [42] applied the ViT concept\nin agricultural applications. Reedha et al. [40] highlighted\nhow efficiently the convolutional-free ViT model, using the\nself-attention mechanism, interprets an image into a sequence\nof patches for processing by a standard transformer encode.\nThey obtained a high performance even though the dataset\nis small. They justified that due to data augmentation, trans-\nfer learning, and a low number of classes. Wu et al. [41]\npassed the dataset into two ViT models parallelly, where a\nsmall patch size is used in one model, and a large patch\nsize is used for another model. Fusion models combined\nVOLUME 11, 2023 45379\nA. Tabbakh, S. S. Barpanda: Deep Features Extraction Model\nthese two models to be fed into the MLP header. Conclud-\ning that, by combining different scales of the sequence of\nself-attention, the model can extract more information from\nimages from various granularities. Thai et al. [18] applied ViT\nbased on achieving a 90% F1-score. They used quantization\nto make the model three times smaller before deploying it\non a Raspberry Pi 4 Model B. Yasamin et al. [42] combined a\nclassical convolutional neural network (CNN) with ViT. They\nconcluded that ViT gives good accuracy but decelerates the\nprediction, and this approach can recompense for the speed.\nBased on what is discussed in the related work, it can be\nnoted that CNN models represent the process on images as\ntemplate matching, which extracts the neighbouring features\nand does not consider the relation between overall features.\nIn contrast, ViT can extract and relate the features by giving\nthe position to patches of pixels and interactions between\nthem. ViT drawback is the need for a vast dataset and consum-\ning much time due to its correlation between all pixels with\nkeeping the image scale without reduction. Moreover, pre-\ntrained models could be utilized significantly to extract the\nfeatures from the leaf and simultaneously reduce the dimen-\nsionality of the image, which will be considered as initial\nfeatures to be passed for the next stage to extract the deep\nfeatures from them. Thus, combining both approaches could\nextract comprehensive and significant features to obtain an\naccurate model.\nIII. MOTIVATION AND OBJECTIVES\nMost of the DL-based research work in the plant disease\nclassification field, which uses the convolution process, may\nbe seen as template matching, whereby the same filtering\ntemplate is applied to several parts of the same image. How-\never, the convolution layer represents only the connections\nbetween neighbouring pixels since convolution is a local\naction and is not able to encode the orientation and position\nof infected parts in the leaf. In case the new leaf has different\npositions of infection from a trained set, then the classifier\nwould have a hard time classifying the diseased leaf.\nIn contrast, a transformer layer may overcome these issues\nby representing the interactions between all pixels, so the\ntransformer is considered a global operation. In a transformer,\nthe attention unit is an adaptive filter, and the filter weights are\nset according to how well two pixels compose. The modelling\ncapabilities of this sort of computer module are superior.\nSince Vision Transformer is applied on the whole image to\ninitialize the patches and process the attention patches, the\ntime-consuming of using Vision Transformer depends on the\nsize of the image and the number of patches. Hence, it is better\nto reduce the dimension of an image, whereas scaling down\nthe dimension in an ordinary way will lead to the loss of some\nsignificant features of the plant leaf in the image. The transfer\nlearning extracts the features and reduces the dimension of\nthe image before the classification part of its model. In light\nof this, the aims listed below constitute the focus of the study:\n1) Different transfer learning models are used to extract\ninitial features and reduce the dimensionality of plant\ndisease images instead of applying the Vit model on\nraw images with entire dimensions, which reduces the\ntime consumption and trainable parameter numbers.\n2) ViT model is used to extract the deep features of leaf\nimages, i.e., interactions between all pixels that are\nextracted from the previous stage (Initial Features).\n3) Different combinations of Transfer learning-based\nmodels with ViT have been experimented.\n4) Different five transfer learning-based models are com-\npared with the proposed work TLMViT.\nIV. PRELIMINARIES\nA. TRANSFER LEARNING\nTransfer learning models or pre-trained models such as\nResNet50, AlexNet, Inception V3, VGG16, and VGG19 are\nvery active and useful in overcoming the flaws in deep\nlearning [43]. Training a deep learning model with millions\nof parameters needs a lot of training samples that are only\nsometimes applicable to be collected and too much training\ntime [44]. The transfer learning technique is utilized to trans-\nfer the knowledge from a pre-trained model to another model,\nwhich should be trained for a classification task. Hence, the\ntraining time is decreased because the model is not trained\nfrom scratch. Moreover, it is utilized to train the deep learning\nmodel on a small-size dataset and overcome the overfitting\nissue [45], [46]. Fine-tunning is the most popular architecture\nin transfer learning, where a large dataset is used to pre-train\nthe model and then, the parameters of the pre-trained model\nare frozen and transferred to the target model for fine-tuning\nwith the dense layers of the target model. Finally, the dense\nlayers which have reasonable parameters should be trained on\nthe dataset that belongs to the desired classification task [47].\nFigure 1 shows the process of using the transfer learning\nmodel for training a new model to classify a new dataset.\nFIGURE 1. An illustration of using a Pre-Trained based model to classify a\nnew dataset.\nB. VISION TRANSFORMER (ViT)\nThe transformer was initially used to visualize backbone\nnetworks through the local relational network (LR-Net) [48]\n45380 VOLUME 11, 2023\nA. Tabbakh, S. S. Barpanda: Deep Features Extraction Model\nand SASA [49]. Both models improved accuracy over ResNet\nwhile utilizing the same theoretical computing resource by\nrestricting self-attention computation to a local sliding win-\ndow. LR-Net has the same theoretical and computational\ncomplexity as ResNet, but it is much slower in reality. ViT\nmodel was proposed by Dosovitskiy et al. [50] in 2021 as\nan efficient image classifier. He suggested using the origi-\nnal transformer architecture in computer vision applications\nsince it was previously applied only in natural language pro-\ncessing applications. In image classification, ViT performs\nbetter than standard CNN architectures when trained on a\nhuge amount of image data. The input image is divided into\npatches, each one is flattened and merged across the channels\nof the image to produce a vector representation of each patch.\nThe patch embeddings are calculated via linear projecting\nthe vectors using a dense layer. The positional embeddings\nare generated, which help the ViT model to analyze the\npatches orderly and have a full view of the input image.\nThen, each patch is added to the corresponding positional\nembedding to obtain the input of the transformer’s encoder.\nThe encoder consists of one block executed several times such\nthat the architecture of the block contains a multi-head self-\nattention followed by a dense layer. Finally, MLP classifies\nthe input image based on the output of the transformer’s\nencoder.\nV. METHODOLOGY\nThis paper proposes a hybrid model for plant disease classi-\nfication based on transfer learning models and vision trans-\nformers (TLMViT), as shown in Figure 2. TLMViT includes\nfour stages:\n1) Data acquisition: The PlantVillage and Wheat Rust\ndatasets are used to train and validate the proposed\nmodel. Different three crops are considered in the\nPlantVillage dataset (pepper bell, potato, and tomato\nleaf).\n2) Image augmentation: It artificially increases a dataset’s\nsize by applying random transformations to existing\nimages. This allows the model to generalize better and\nreduce overfitting, as it has seen different variations of\nthe same image.\n3) Feature extraction: It is a process of identifying and\nextracting important information from raw data, like\nedges, corners, and textures in image data, which\ncan be used as input for a classifier model. More-\nover, two phases are consequently used to extract the\nfeatures:\na) First phase: a Pre-Trained based model is used to\nextract the initial features of the leaf, then pass\nthem to the second stage.\nb) Second phase: ViT is utilized to extract the deep\nfeatures from the initial features.\n4) Classification: simple MLP classifier is used to be\ntrained and evaluated.\nEach stage is discussed in detail as follows:\nA. DATA ACQUISITION AND IMAGE AUGMENTATION\nIn this work, PlantVillage and Wheat Rust datasets are used to\nevaluate the proposed model for extracting the deep features\nof leaves.\n1) PlantVillage\nThe PlantVillage dataset is a collection of images of healthy\nand diseased plants created by the PlantVillage non-profit\nproject. The dataset contains over 54,000 images of over\n38 different crop species, focusing on cassava, tomato, pep-\nper, and potato. Each image is labelled with the species of\nplant and the disease that is present if any. The dataset is\nfreely available for computer vision and deep learning tasks\nsuch as Image classification, Object detection, and seman-\ntic segmentation. In this research, three different crops of\nthe PlantVillage dataset [51] from Kaggle are utilized and\ndownloaded for training and evaluating the proposed model.\nIt comprises three main categories of plants: pepper bell,\npotato, and tomato leaves. It includes 20,638 images of dis-\neased and healthy leaves from these three crops. Table 1\nillustrates the differentiation of two bell pepper leaves, three\nvarieties of potato leaves, and ten types of tomato leaves.\nFigure 3 shows samples of all 15 types of leaves included\nin the dataset.\nFrom Table 1, it can be noted that some classes have less\nnumber of images compared to other classes. Moreover, the\nlimitation of images while training the model leads to over-\nfitting issues. In order to restrain the influence of imbalanced\ndata and overfitting issues, data augmentation is applied\nto increase the samples [52]. The data augmentation, such\nas scaling, shearing, rotating, horizontal flip, zooming, and\nshifting [53], [54] are applied, and its parameters are noted in\nTable5. From the initial set of 16511 training images, the data\naugmentation generated 33257 augmented training images.\n2) WHEAT RUST\nWheat rust disease is a fungal disease that affects wheat\nplants, causing significant damage to crops and resulting in\nsignificant economic losses for farmers. The disease is caused\nby several species of fungi belonging to the Puccinia genus.\nThere are three main types of wheat rust: stem rust, leaf rust,\nand stripe rust [55]. Stem rust is the most destructive type, as it\nattacks the stems of the wheat plant, weakening the plant and\nreducing yield. Leaf rust affects the leaves of the plant and can\nreduce photosynthesis, while stripe rust affects the leaves and\nstems of the plant. Wheat rust is spread by wind-borne spores,\nwhich can travel long distances, making it difficult to control.\nPreventing the spread of wheat rust is crucial to maintaining\nhealthy crops and ensuring food security. Early detection\nand prompt treatment are essential for effectively managing\nthe disease. In this work Wheat Rust dataset [42] is utilized\nfor training and evaluating the proposed model. The dataset\ncontains three classes (1128 Brown rust, 1348 Yellow rust,\nand 1203 Healthy wheat), as illustrated in Table 2. Figure 4\nshows samples of the Wheat Rust dataset.\nVOLUME 11, 2023 45381\nA. Tabbakh, S. S. Barpanda: Deep Features Extraction Model\nTABLE 1. Details of the leaves images used in the PlantVillage dataset.\nFIGURE 2. The architecture of the proposed model TLMViT.\nTABLE 2. Details of the leaves images used in the Wheat Rust dataset.\nB. FEATURES EXTRACTION\nThe proposed model uses two consecutive phases for desir-\nable feature extraction. In the first phase, the pre-trained\nmodel is used for the initial extraction of leaf features and for\nreducing the dimensionality of images, leading to decreased\ntime while applying the second phase. In the second phase,\nthe ViT model is applied on initial features for deep extraction\nof features.\n1) 1ST PHASE (INITIAL FEATURES EXTRACTION)\nIn this phase, a pre-trained model is utilized to extract\ninitial features (IF) from the input images, where dif-\nferent pre-trained models, namely (AlexNet, Res-Net 50,\nVGG-16, VGG-19, and Inception-V3) are experimented on\nthe proposed work to observe which model will be superior.\nThe weights of all pre-trained models are initialized by using\nthe ImageNet dataset and then used to extract the features\nfrom the PlantVillage and Wheat Rust datasets as described\nbelow:\na: ResNet 50\nResidual Network is referred to as ResNet. Kaiming He,\nXiangyu Zhang, Shaoqing Ren, and Jian Sun initially\ndescribed this novel neural network in [56]. There are several\ndistinct implementations of ResNet, all of which use the same\nbasic structure but have various layers. Resnet50 is shorthand\nfor the version of Resnet that supports 50 neural network\nlayers. The dimension of the default input of ResNet50 is\n224*224*3. Five different stages based on convolution and\nResidual Networks are applied, and after each stage, Max-\nPooling Layers are applied, which leads to reducing the\ndimension of the image by 2. The output of the final stage\nof the default input gives a 7*7*2048 size. Whereas the\ndimension of the PlantVillage dataset is 256*256*3, the final\nstage of ResNet50 gives 8*8*2048. These are used as the\ninitial feature and pass it as input to deep feature extraction\nusing ViT.\nb: AlexNet\nAlex Krizhevsky and his colleagues [53] published their find-\nings in a research article titled Imagenet Classification with\nDeep Convolution Neural Network, in which they suggested\nthe model. There are eight layers in the Alexnet model,\ninvolving five layers based on convolution with a combina-\ntion of max pooling, followed by three fully connected layers.\nRelu activation is used for each of these layers, excluding the\noutput layer. In this work, only the first five layers are used\nto extract the initial features from the PlantVillage dataset,\nand the last three layers are eliminated. The dimension of\ninput images is 256*256*3, and the output of the last layers\nis 8*8*256, as mentioned in Table 3.\nc: VGG16 AND VGG19\nVGG contains a combination of convolutional and three fully\nconnected layers. The name of the VGG model depends on\nhow many layers are used in the model [57], e.g., 13 con-\nvolutional and three fully connected layers produce VGG16,\nwhereas 16 convolutional and three fully connected layers\n45382 VOLUME 11, 2023\nA. Tabbakh, S. S. Barpanda: Deep Features Extraction Model\nFIGURE 3. Image Samples of PlantVillage Dataset: (a) Tomato Spider\nmites Two spotted spider mite, (b) Tomato Yellow Leaf Curl Virus,\n(c) Tomato Target Spot, (d) Tomato Sep-toria leaf spot, (e) Tomato\nBacterial spot, (f) Potato Late blight, (g) Tomato Late blight, (h) Tomato\nLeaf Mold, (i) Tomato Early blight, (j) Tomato healthy, (k) Tomato mosaic\nvirus, (l) Potato healthy, (m) Potato Early blight, (n) Pepper bell healthy,\nand (o) Pepper bell Bacterial spot.\nFIGURE 4. Image Samples of Wheat Rust Dataset: (a) Brown Rust,\n(b) Yellow Rust, and (c) Healthy wheat.\nproduce VGG19 [58]. A simple 3*3 convolution kernel is\nutilized across all layers to deepen the network and reduce\nthe number of parameters. A 224*244 RGB image is used as\nVGG’s default input. Moreover, the last three fully connected\nlayers of both VGG16 and VGG19 models are not used in\nour work. Table 4 shows the default input of both models\n(VGG16 and VGG19), which is 224*224*3 and generates\n7*7*512 as an output of the last layer, whereas the input size\nof the PlantVillage dataset is 256*256*3, so the last layer\ngives 8*8*512 an output size as shown in Table 4. This output\nis considered the initial features of the models to be forwarded\nto the next phase to extract the deep features.\nd: INCEPTION V3\nIn the image classification competition at the 2014 ILSVRC,\nGoogle presented a network named GoogLeNet that could\nachieve the performance of human experts on the Ima-\ngeNet database [59]. An improved version of GoogLeNet got\ninspired to generate the Inception-v3 model. It uses various\nsizes of receptive kernels. By utilizing zero padding, the\nconvolution operation’s output size is maintained. Finally,\nfilter concatenation produces the feature maps that will be\nTABLE 3. Details of the feature map size for default and our input size of\nthe AlexNet model.\nTABLE 4. Details of the feature map size for default and our input size of\nthe VGG16 and VGG19 models.\nused for the classification part. Feng et al. [60] represent the\nstructure of the Inception V3 model in detail, the default size\nof an input image in the Inception V3 model is 299*299*3,\nand the final output size is 8*8*2,048.\nIn contrast, the image size in the PlantVillage dataset is\n256*256*3, which is less than the default, leading to less\noutput size of the feature map. Moreover, the output size\nof the features map for all previous pre-trained models that\nwere used in our experiments gives 8*8*c (c is 2048, 256,\n512, 512 for ResNet 50, AlexNet, VGG16, and VGG19 mod-\nels, respectively) when using PlantVillage dataset. To main-\ntain consistency throughout all experiments, the images of\nPlantVillage dataset are rescaled to the size of the default\ninput of inception V3, which is 299*299*3, and fed as input\nto extract the initial features in size of 8*8*2,048 to be used\nin the second stage of the proposed model.\nIn contrast, the image dimensions of the Wheat Rust\ndataset are in different scales, so it may be difficult to compare\nand analyze the data accurately. To address this issue, rescale\nfunction is used to ensure that all inputs (image dimensions)\nare of the same shape. The image dimensions of the Wheat\nRust dataset are rescaled as the same image dimensions of\nPlantVillage to have the same output dimensions in the 1st\nphase of feature extraction with the size of 8*8*c.\n2) 2ND PHASE (DEEP FEATURES EXTRACTION)\nIn this phase, the Vit model is applied on the initial features\nthat are extracted from the previous phase to extract deep\nfeatures of leaves images, where ViT is described as follows:\nAs mentioned in the first phase, the features are extracted\nby using transfer learning model, such that the extracted\nfeatures IF are represented by a 2D image I ∈ RL∗W ∗C\ndimensions, where L is the length, W is the width of image\nand C is the number channels. The final dimensions of\nVOLUME 11, 2023 45383\nA. Tabbakh, S. S. Barpanda: Deep Features Extraction Model\nthe initial features that are extracted by pre-trained model\ne.g, AlexNet, VGG-16, VGG-19, Res-Net, and Inception-V3\nare R8∗8∗256, R8∗8∗512, R8∗8∗512, R8∗8∗2048, R6∗6∗2048 respec-\ntively. Then these features are fed as input to ViT model to be\nprocessed according to the steps that are shown in Figure 2.\nThe first step is image partitioning, such that the extracted\nfeatures are split into non-overlapping patches. The initial\nfeature dimensions from the previous phase are 8 ∗8∗c. Since\nthe dimension is not too big (8 ∗ 8), and splitting it into lots\nof patches will give less dimension with less information on\nfeatures, so the number of patches is chosen by taking the half\nsize of the IF with N(P) = 4. Each patch P has size 4 ∗ 4 ∗ C\nwhere C is the number of IF’s channels and (4 ∗ 4) are the\ndimensions of P. The process of image partitioning is shown\nin equation [1].\nIF = P1||P2||P3||P4; P ∈ R4∗4∗C and IF ∈ R8∗8∗C (1)\nwhere C = 256, 512, 512, 2048, and2048 for AlexNet,\nVGG-16, VGG-19, Res-Net, and inception respectively.\nThen, every patch is flattened to a 1D vector such that, the\nsize of the vector is S = 4 ∗ 4 ∗ C. Hence, four flattened\npatches (vectors) are obtained with size S. Next, the patch\nembeddings PEi are calculated by linear projecting on the\nfour vectors using dense layer that has 64 units as shown in\nequation [2].\nPEi = Project(Pi) = Dense64 (Pi) ;\nPi ∈ R4.4.C∗1, PEi ∈ R64, i = 1, · · ·4 (2)\nwhere i is the index of patch and the dimension of PEi is 64.\nThe second step: in this step, the positional information\nof patches Zi is retained by adding the patch embeddings PEi\nto the position embeddings PEi. The aim of this step is to\nprocess the patches as per their positions in the images (IF ),\nand the process of getting positional information is shown in\nequation [3].\npositional information(Zi) = PEi + Pos_Ei; i = 1, · · ·4\n(3)\nwhere PEi and Pos_Ei have the same dimension (64*1),\nand i is the index of patch. Position embeddings Pos_Ei are\ngenerated by mapping integer numbers which represent the\npositions of patches into vectors with size 64. Finally, the\nclass token Z0 is embedded and appended to the positional\ninformation Zi; i=1 . . . ,4 such as the dim (Z0) = dim ( Zi);\ni = 1 . . . ,4 as shown in equation [4].\nZ = [Z0||Z1||Z2||Z3||Z4] (4)\nThe third step is transformer encoding which consists of a\none block that is executed N times (N = 8) to extract the deep\nfeatures. This block has two main components i.e., multi-\nhead self-attention and MLP beside that, normalization oper-\nations are used to improve the performance of transformer\nencoder. The process of transformer encoder is shown in\nequations [5 - 6] and Figure 5.\nXi (j) = FH=4 (Norm (Zi)) + Zi;\ni = 0, · · ·4 and j= 1, · · ·8 (5)\nX´\ni(j) = MLP (Norm (Xi (j))) + Xi (j) ;\ni = 0, · · ·4 and j= 1, · · ·8 (6)\nwhere FH=4 is the function of multi-head self-attention with\nheads H = 4, MLP is multi-layer perceptron neural network\nthat uses two hidden layers with 128 neurons in the first layer\nand 64 neurons in the second layer. Norm is the normalization\noperation of vector. X´\ni(j) is the output of transformer’s block\nat the j-th iteration. At j=8 (the last iteration of encoder\nblock), the deep features are obtained.\nFIGURE 5. Structure of the transformer encoder.\nC. CLASSIFICATION\nThe classifier that has been used in the proposed model is\nMLP head. Once the transformer encoder generates the deep\nfeatures in the previous stage, they are fed to the classifier,\nwhich determines to which class the image belongs. The deep\nfeature vectors X ´\ni(8) are fed to the MLP classifier as input\nvalues, where the input layer has 256 neurons equal to the\ndimension of the deep feature vector X´\ni(8). Two hidden layers\nwith dimensions 1024 and 512 for the first and second layers\nrespectively are used. Lastly, 15 neurons are used as the MLP\nclassifier’s output, which is the dataset’s number of classes.\nThe process of classification is shown in equation [7].\nP (j) = MLPhead\n(\nX´\ni(8)\n)\n; j = 1, · · ·15 (7)\nVI. EXPERIMENTAL ANALYSIS\nAll the experiments in this work are carried out on Win-\ndows 10 PCs version 21H2, with 64-bit operating system\nand a processor Intel(R) Xeon(R) 4.01 GHz with 64 GB\nRAM. CUDA with version 11.2 is used on NVIDIA Geforce\nGTX 1080 Ti. Python 3.9.12 was used as the programming\nlanguage and all the experiments were performed on the\nSpyder IDE including various libraries e.g., Keras, and Ten-\nsorFlow. Table 5 illustrates the parameters of all models that\nwere experimented in this research. In the data augmentation\nfunction, the parameters are randomly initialized in ranges.\nDataset Splitted (80% for training and 20% for both eval-\nuating and testing the model). Since the dataset size is not\n45384 VOLUME 11, 2023\nA. Tabbakh, S. S. Barpanda: Deep Features Extraction Model\nsmall, 20% would be sufficient for testing the model (almost\n4100 images for testing the model). The Epoch number is\nchosen (25) to compare the results of all experiments at a\nspecific epoch number to give a fair comparison. Since the\ndataset is under multi-class classification problems, categori-\ncal cross-entropy is used as a loss function that calculates the\npredicted probability distribution and the true probability dis-\ntribution. Adam is used as an optimizer since it requires fewer\nparameters for tuning, which leads to faster computation time.\nThe dimension of the initial feature (features dimension at the\nlast stage of the pre-trained model) is 8*8*C, as explained in\nsection V-B1. So, the patch size of the ViT model is chosen\nfour as half of the initial feature dimension. The rest of the\nparameters are chosen experimentally.\nTABLE 5. Experimental parameters.\nVII. RESULTS AND DISCUSSION\nThis section discusses the results of the proposed model and\ncompares its performance with the pre-trained-based models’\nperformance.\nA. RESULTS OF OUR PROPOSED MODEL\nThe proposed model based on a pre-trained model followed\nby ViT is evaluated in terms of accuracy, loss, precision, F1-\nscore, and recall. Five different pre-trained-based models are\nexperimented to measure the performance of the proposed\nmodel. Firstly, the weights of the CNN model are initialized\nby the frozen weights of a transfer learning-based model that\nwas trained on the ImageNet dataset. Hence, no training is\nrequired in the initial features extraction phase, this manner\novercomes the issue of consuming a lot of processing time for\ntraining a complex model from scratch on the PlantVillage\ndataset, which contains more than 20 thousand images. The\naim of using pre-trained-based models in our approach is\nto (1) Extract the initial features from the input image and\nreduce the number of trainable parameters. (2) Reduce the\ndimension of the input image to accelerate the performance\nof ViT in the next phase and drive Vit to focus on the most\nsignificant features. Then, ViT model is trained to extract\nthe deep features and classify the plant diseases based on\nthe training set of the PlantVillage dataset. After that, the\noverall model is validated based on the testing set of the\nPlantVillage dataset. Figure 6 (a, c, e, g, and i) show the\nclassification accuracy of training and validation set for five\nexperiments, i.e., ResNet50, AlexNet, VGG16, VGG19, and\nInception V3, each of which is followed by ViT-based model\nrespectively in 25 epochs. The outcome of training accuracy\nis improved after each epoch till it reaches 90.405%, 92.5%,\n99.60%, 98.8%, and 99.2% for ResNet50, AlexNet, VGG16,\nVGG19, and Inception V3, each of which is followed by\nViT-based model respectively. The outcome of validation\naccuracy is also improved after each epoch till it reaches\n89.2%, 90.49%, 98.43%, 98.81%, and 98.48% for ResNet50,\nAlexNet, VGG16, VGG19, and Inception V3, each of which\nis followed by ViT-based model respectively as shown in\nTable 6. Figure 6(b, d, f, h, and j) show the training and\nvalidation loss outcome in 25 epochs for five experiments,\ni.e., ResNet50, AlexNet, VGG16, VGG19, and Inception V3,\neach of which is followed by ViT-based model respectively.\nThe outcome of training loss is decreased after each epoch\ntill it reaches 0.3448, 0.2288, 0.0814, 0.1, and 0.0900 for\nResNet50, AlexNet, VGG16, VGG19, and Inception V3,\neach of which is followed by ViT-based model respectively.\nThe outcome of validation loss is decreased after each epoch\ntill it reaches 0.3735, 0.2974, 0.1300, 0.0947, and 0.1 for\nResNet50, AlexNet, VGG16, VGG19, and Inception V3,\neach of which is followed by ViT-based model respectively\nas shown in Table 6. Moreover, according to the results\nin the five experiments, (VGG-16 followed by ViT) model\noutperforms in the training phase where the training accuracy\nis equal to (99.60%), and training loss is equal to (0.0814).\nWhereas, (VGG-19 followed by ViT) model outperforms in\nthe validation phase where the validation accuracy is equal to\n(98.81%), and validation loss is equal to (0.0947).\nFurthermore, Figure 8 shows the confusion matrices of the\nproposed model that experimented using ResNet50, AlexNet,\nVGG16, VGG19, and Inception V3 each of which is followed\nby ViT-based model respectively. Finally, some statistical\nmanners are used to analyze confusion matrices such as (pre-\ncision, recall, and F1-score) for ResNet50, AlexNet, VGG16,\nVGG19, and Inception V3 each of which is followed by\nViT-based models respectively as shown in Table 8.\nB. RESULTS OF TRANSFER LEARNING-BASED MODELS\nIn this work, five different pre-Trained based models are\nalso experimented to classify the plant diseases on the\nPlantVillage dataset, which means the extracted features\nthat used in its model are the initial features only as men-\ntioned previously. The performance of five different trans-\nfer learning-based models i.e, ResNet50, AlexNet, VGG16,\nVGG19, and Inception V3 are shown in Table 6. The clas-\nsification accuracy of the training set is increased after\neach epoch till it reaches 80.43%, 83.9%, 90.44%, 88.70%,\nand 92.78% for ResNet50, AlexNet, VGG16, VGG19, and\nInception V3 respectively, and the classification accuracy\nof validation set is also improved after each epoch till it\nreaches to 79.84%, 81.91%, 89.31%, 86.55%, and 90.77%\nfor ResNet50, AlexNet, VGG16, VGG19, and Inception V3\nVOLUME 11, 2023 45385\nA. Tabbakh, S. S. Barpanda: Deep Features Extraction Model\nFIGURE 6. The performance of TLMViT models. (a, c, e, g, and i) Accuracy and (b, d, f, h, and j) Loss of ResNet50_ViT, AlexNet_ViT, VGG16_ViT, VGG19_ViT,\nand Inception V3_ViT models respectively.\nTABLE 6. Performance comparison for training and validation set on PlantVillage dataset in terms of accuracy and loss between transfer learning-based\nmodels and TLMViT.\nTABLE 7. Performance comparison for training and validation set on Wheat Rust dataset in terms of accuracy and loss between transfer learning-based\nmodels and TLMViT.\n45386 VOLUME 11, 2023\nA. Tabbakh, S. S. Barpanda: Deep Features Extraction Model\nFIGURE 7. The performance of Transfer Learning-based models. (a, c, e, g, and i) Accuracy and (b, d, f, h, and j) loss of ResNet50, AlexNet, VGG16, VGG19,\nand Inception V3 models respectively.\nTABLE 8. Results Comparison in terms of F1-score, precision, and recall\nbetween transfer learning-based models and TLMViT using PlantVillage\ndataset.\nrespectively. Figure 7( a, c, e, g, and i) show the classifi-\ncation accuracy of training and validation set in 25 epochs\nfor ResNet50, AlexNet, Inception V3, VGG16, and VGG19\nrespectively. Whereas Figure 7( b, d, f, h, and j) show clas-\nsification loss of training and validation set in 25 epochs\nfor ResNet50, AlexNet, Inception V3, VGG16, and VGG19\nbased models respectively. The value of training loss is\ndecreased after each epoch till reaches 0.5492, 0.47, 0.2605,\n0.3277, and 0.2064 for ResNet50, AlexNet, VGG16, VGG19,\nTABLE 9. Results Comparison in terms of F1-score, precision, and recall\nbetween transfer learning-based models and TLMViT using Wheat Rust\ndataset.\nand Inception V3 respectively. The value of validation loss\nis decreased after each epoch till reaches 0.6, 0.55, 0.3188,\n0.3792, and 0.2967 for ResNet50, AlexNet, VGG16, VGG19,\nand Inception V3 respectively as shown in Table 6. Moreover,\nthe values of precision, F1-score, and recall for ResNet50,\nAlexNet, Inception V3, VGG16, and VGG19 are shown in\nTable 8. Moreover, a different comparison is done for the\npre-trained based models with each other, on the PlantVillage\ndataset. According to the results, Inception V3 achieves the\nVOLUME 11, 2023 45387\nA. Tabbakh, S. S. Barpanda: Deep Features Extraction Model\nFIGURE 8. The confusion matrix of the proposed model. (a) ResNet50 with ViT, (b) AlexNet with ViT, (c) VGG16 with ViT,\n(d) VGG19 with ViT, (e) Inception V3 with ViT.\n45388 VOLUME 11, 2023\nA. Tabbakh, S. S. Barpanda: Deep Features Extraction Model\nTABLE 10. A comparison between the proposed model vs. other works.\nbest performance among all, obtaining the highest training\naccuracy equal to 92.78%, and the lowest training loss equal\nto 0.2064 values, as well as the highest validation accuracy\nequal to 98.48% and lowest validation loss equal to 0.1.\nFurthermore, our proposed model is compared with the\npre-trained based models. According to the results, the per-\nformance of the proposed model outperforms the same model\nwithout using ViT, due to extracting the deep features from\nthe initial features by ViT. The initial features are extracted\nfrom the previous phase by the Transfer Learning-based\nmodel. In other words, ViT in the proposed model’s topology\nis considered the second level of extracting features of leaf\nimages. By analyzing the comparison above, the proposed\nmodel enhances performance compared with the Transfer\nlearning-based model in terms of accuracy and loss for plant\ndisease classification. Where enhancements percentages are\n(1.11%, 1.10%, 1.08%, 1.10%, and 1.14% validation accu-\nracy) and (1.60%, 1.89%, 2.9%, 2.45%, and 4.00% valida-\ntion loss) for ResNet50, AlexNet, Inception V3, VGG16,\nand VGG19 respectively. On average, the TLMViT model\nhas 1.106% higher validation accuracy and 2.568% lower\nvalidation loss than pre-trained-based models.\nTable 11 compares the epochs number and execution time\nof a model taken to reach a specific accuracy for both the\npre-trained-based models and the proposed model on the\nPlantVillage dataset. The specific accuracy is chosen as per\nthe highest accuracy of the pre-trained-based model. For\nexample, ResNet50 took 15 epochs to achieve ∼80% accu-\nracy in 56.90 minutes, whereas the proposed model using\nResNet50 with ViT reached almost the same accuracy in\n2 epochs in 7.79 minutes.\nSimilarly, the proposed approach is experimented on the\nWheat Rust dataset. Table 7 presents the performance com-\nparison for the training and validation set of the Wheat\nRust dataset in terms of accuracy and loss between transfer\nlearning-based models and TLMViT. Where the outcomes\nof training accuracy are equal to 93.26%, 95.48%, 99,93%,\n99.67%, and 99.81% for ResNet50, AlexNet, VGG16,\nVGG19, and Inception V3 each of which is followed by\nViT-based model respectively. Furthermore, the outcomes of\nvalidation accuracy are equal to 91.74%, 93.39%, 99.44%,\n99.86%, and 99.69% for ResNet50, AlexNet, VGG16,\nVGG19, and Inception V3 each of which is followed by\nViT-based model respectively. Moreover, Table 9 shows the\nresults comparison between transfer learning-based models\nand TLMViT on the Wheat Rust dataset in terms of F1-\nscore, precision, and recall. Table 12 compares the epochs\nnumber and execution time of a model taken to reach a\nspecific accuracy for both the pre-trained-based models and\nthe proposed model on the Wheat Rust dataset. The specific\naccuracy is chosen as per the highest accuracy of the pre-\ntrained-based model. For example, ResNet50 took 21 epochs\nto achieve ∼84% accuracy in 13.15 minutes, whereas the\nproposed model using ResNet50 with ViT reached almost the\nsame accuracy in 2 epochs in 1.39 minutes. According to the\nresults of all models that are experimented on the PlantVil-\nlage and Wheat Rust datasets, the proposed model performs\naccurately on classification problems for both datasets using\nless number of Epochs and execution time.\nIn addition, Table 10 compares the performance of our\nmodel with other models using the main performance met-\nrics (accuracy, precision, recall, and F1-score). In this\nVOLUME 11, 2023 45389\nA. Tabbakh, S. S. Barpanda: Deep Features Extraction Model\nTABLE 11. Comparison of Epochs number and execution time for both\nTransfer Learning-based model and TLMViT model using PlantVillage\ndataset.\nTABLE 12. Comparison of Epochs number and execution time for both\nTransfer Learning-based model and TLMViT model using Wheat Rust\ndataset.\ncomparison, the ImageNet dataset is used to fine-tune the\nparameters for our proposed model and all comparative works\nthat used the transfer learning approaches. As shown in\nTable 10, the recent works that are done so far on plant dis-\nease classification concentrate on using four different scopes:\nmachine learning models, CNN models, CNN along with\nattention approach, and CNN along with ViT. The plant\ndisease-based machine learning classifiers need more steps\nin pre-processing stage to extract the leaf features, and the\nmodel performance would be less accurate due to the limita-\ntion of machine learning. In [33], a machine learning model\nusing LGBM mode is proposed and trained on the PlantVil-\nlage dataset, the performance metrics did not exceed 94%.\nThough CNN models are effective in classifying plant disease\nimages, these models represent only the connections between\nneighbouring pixels and are not able to encode the orientation\nand position of infected parts in the leaf. For this reason,\ntheir performance, i.e., accuracy, precision, recall, and F1-\nscore are not very close to 1. In [61], [62], [63], and [64],\nCNN models are trained on the PlantVillage dataset, where\nthe performance metrics did not reach 99%. Similarly, in [68],\n[69], [70], and [71] different types of CNNs are trained on the\nWheat dataset where the performance metrics did not reach\n99%. Combining the attention approach along with the CNN\nor pre-trained CNN model improves the accuracy because\nthe attention unit is considered an adaptive filter, where its\nweights are set according to how well two pixels compose.\nIn [65] and [66] DenseNet and pre-trained MobileNet-V2\nalong with attention mechanism are used respectively. In their\nproposed work, the performance metrics got improved and\nreached 98% and 97% approximately. Finally, the pre-trained\nCNN model along with ViT achieves the best performance\nin plant disease classification where the metrics are close\nto 1. In [67], a CNN model followed by ViT is trained on\nPlantVillage for plant disease detection, and the performance\nmetrics reached 99% approximately. Whereas our proposed\nmodel uses pre-trained models followed by ViT is trained\non PlantVillage and Wheat datasets. The performance met-\nrics achieved accuracy equal to 98.81% and 99.86% for\nplantVillage and Wheat datasets respectively, which outper-\nformed the previous models.\nVIII. CONCLUSION\nThis paper proposes a hybrid model (TLMViT) for plant\ndisease classification based on a pre-trained based model\nfollowed by a vision transformer. TLMViT has four stages:\ndata acquisition, image augmentation, feature extraction, and\nclassification. Firstly, three crops namely( bell peppers, pota-\ntoes, and tomato leaves) including fifteen classes of the\nPlantVillage dataset and three classes of the Wheat Rust\ndataset, are used individually to train and evaluate our pro-\nposed model. Secondly, the leaf images in the datasets are\naugmented by many functions such as rotation, shifting,\nshearing, zooming, and flipping to increase the number of\ntraining samples. Thirdly, the features are extracted in two\nconsecutive phases: initial features extraction and deep fea-\ntures extraction. In the first phase, the pre-trained model is\nused to extract the features of the leaf and call them initial\nfeatures. The weights of the pre-trained model are fine-tuned\nusing the ImageNet dataset. In the second phase of features\nextraction, the vision transformer is utilized as deep layers to\nextract the deep features of leaves based on initial features.\nLastly, the MLP head classifier determines to which class the\nleaf belongs. The proposed model’s performance is exper-\nimented by five pre-trained models and evaluated in terms\nof accuracy, loss, precision, F1-score, and recall. The results\nshow that the proposed model performs accurately in plant\ndisease classification, getting the highest validation accuracy\nfor VGG-19, followed by the ViT model. The validation\naccuracy is equal to 98.81% and 99.86%, and validation\nloss is equal to 0.0947 and 0.0715 for PlantVillage and\nwheat datasets respectively. Moreover, the results of TLMViT\nare compared with transfer Learning based models (without\nusing ViT) to show the efficiency of using ViT for deep fea-\nture extraction. The TLMViT model outperforms the Transfer\nLearning-based model with an enhancement of 1.11% and\n1.099% higher in validation accuracy and 2.576% and 2.92%\nlower in validation loss for PlantVillage and wheat datasets\nrespectively. From all the above, the following findings of\nthe proposed model can be noted: (1) The data augmentation\napproach provides a simple way to overcome the lack of\nimages that leads to overfitting issues and reduce the influ-\nence of an imbalanced dataset. (2) A pre-trained-based model\ncould be utilized to extract the initial leaf features and reduce\nthe dimensionality of the original image, then be used as\ninputs for deep layers or a deep approach to extract the deep\nfeatures. (3) ViT model shows the ability to extract the deep\nfeatures from extracted features of the CNN model (initial\nfeatures). On the other hand, the pre-trained model often fine-\ntunes for specific tasks. So, the proposed model could need\nmore training and fine-tuning the weights in the first phase of\nfeature extraction for particular crops. In future work, various\ncrops could be considered for training on TLMViT to study\nthe ability of the proposed model.\n45390 VOLUME 11, 2023\nA. Tabbakh, S. S. Barpanda: Deep Features Extraction Model\nACKNOWLEDGMENT\nThe authors would like to thank the High-Performance Com-\nputing Laboratory, VIT-AP University, for providing the\nresources and support that enabled this research to be con-\nducted. The availability of advanced computing tools and\nskilled technical assistance played a crucial role in the suc-\ncessful completion of this article.\nREFERENCES\n[1] A. Kamilaris and F. X. Prenafeta-Boldú, ‘‘Deep learning in agriculture:\nA survey,’’ Comput. Electron. Agricult., vol. 147, pp. 70–90, Apr. 2018,\ndoi: 10.1016/j.compag.2018.02.016.\n[2] Z. Iqbal, M. A. Khan, M. Sharif, J. H. Shah, M. H. Ur Rehman,\nand K. Javed, ‘‘An automated detection and classification of cit-\nrus plant diseases using image processing techniques: A review,’’\nComput. Electron. Agricult., vol. 153, pp. 12–32, Oct. 2018, doi:\n10.1016/j.compag.2018.07.032.\n[3] S. P. Mohanty, D. P. Hughes, and M. Salathé, ‘‘Using deep learning for\nimage-based plant disease detection,’’ Frontiers Plant Sci., vol. 7, p. 1419,\nSep. 2016, doi: 10.3389/fpls.2016.01419.\n[4] J. G. A. Barbedo, ‘‘A review on the main challenges in automatic plant dis-\nease identification based on visible range images,’’ Biosyst. Eng., vol. 144,\npp. 52–60, Apr. 2016, doi: 10.1016/j.biosystemseng.2016.01.017.\n[5] A. K. Pradhan, S. Swain, and J. K. Rout, ‘‘Role of machine learning and\ncloud-driven platform in IoT-based smart farming,’’ in Machine Learning\nand Internet of Things for Societal Issues. Berlin, Germany: Springer,\n2022, pp. 43–54.\n[6] G. C. Index and C. Profiles, ‘‘Geneva: International telecommunication\nunion,’’ Tech. Rep., Dec. 2015.\n[7] Y . Bengio and Y . LeCun, ‘‘Scaling learning algorithms towards AI,’’\nLarge-Scale Kernel Mach., vol. 34, no. 5, pp. 1–41, 2007, doi:\n10.1038/nature14539.\n[8] J. Schmidhuber, ‘‘Deep learning in neural networks: An overview,’’ Neural\nNetw., vol. 61, pp. 85–117, Oct. 2015, doi: 10.1016/j.neunet.2014.09.003.\n[9] A. Bhargava and A. Bansal, ‘‘Fruits and vegetables quality evaluation using\ncomputer vision: A review,’’ J. King Saud Univ.-Comput. Inf. Sci., vol. 33,\nno. 3, pp. 243–257, Mar. 2021.\n[10] A. Ahmad, D. Saraswat, and A. El Gamal, ‘‘A survey on using deep\nlearning techniques for plant disease diagnosis and recommendations\nfor development of appropriate tools,’’ Smart Agricult. Technol., vol. 3,\nFeb. 2023, Art. no. 100083.\n[11] U. Mokhtar, ‘‘SVM-based detection of tomato leaves diseases,’’ in Intel-\nligent Systems(Advances in Intelligent Systems and Computing). Cham,\nSwitzerland: Springer, 2015, pp. 641–652.\n[12] R. Karthik, M. Hariharan, S. Anand, P. Mathikshara, A. Johnson, and\nR. Menaka, ‘‘Attention embedded residual CNN for disease detection in\ntomato leaves,’’ Appl. Soft Comput., vol. 86, Jan. 2020, Art. no. 105933.\n[13] T.-T. Tran, J.-W. Choi, T.-T. Le, and J.-W. Kim, ‘‘A comparative study of\ndeep CNN in forecasting and classifying the macronutrient deficiencies on\ndevelopment of tomato plant,’’ Appl. Sci., vol. 9, no. 8, p. 1601, Apr. 2019.\n[14] A. K. Dey, M. Sharma, and M. R. Meshram, ‘‘Image processing based leaf\nrot disease, detection of betel vine (Piper BetleL.),’’ Proc. Comput. Sci.,\nvol. 85, pp. 748–754, Jan. 2016.\n[15] A. Ramcharan, K. Baranowski, P. McCloskey, B. Ahmed, J. Legg, and\nD. P. Hughes, ‘‘Deep learning for image-based cassava disease detection,’’\nFrontiers Plant Sci., vol. 8, p. 1852, Oct. 2017.\n[16] A. Ramcharan, P. McCloskey, K. Baranowski, N. Mbilinyi, L. Mrisho,\nM. Ndalahwa, J. Legg, and D. P. Hughes, ‘‘A mobile-based deep learning\nmodel for cassava disease diagnosis,’’ Frontiers Plant Sci., vol. 10, p. 272,\nMar. 2019.\n[17] O. O. Abayomi-Alli, R. Damasevicius, S. Misra, and R. Maskeliunas,\n‘‘Cassava disease recognition from low-quality images using enhanced\ndata augmentation model and deep learning,’’ Exp. Syst., vol. 38, no. 7,\nNov. 2021, Art. no. e12746.\n[18] H.-T. Thai, N.-Y . Tran-Van, and K.-H. Le, ‘‘Artificial cognition for early\nleaf disease detection using vision transformers,’’ in Proc. Int. Conf. Adv.\nTechnol. Commun. (ATC), Oct. 2021, pp. 33–38.\n[19] A. Khamparia, G. Saini, D. Gupta, A. Khanna, S. Tiwari, and\nV . H. C. de Albuquerque, ‘‘Seasonal crops disease prediction and classifi-\ncation using deep convolutional encoder network,’’ Circuits, Syst., Signal\nProcess., vol. 39, no. 2, pp. 818–836, Feb. 2020.\n[20] A. J. Rozaqi and A. Sunyoto, ‘‘Identification of disease in potato leaves\nusing convolutional neural network (CNN) algorithm,’’ in Proc. 3rd Int.\nConf. Inf. Commun. Technol. (ICOIACT), Nov. 2020, pp. 72–76.\n[21] A. Singh and H. Kaur, ‘‘Potato plant leaves disease detection and classi-\nfication using machine learning methodologies,’’ in Proc. IOP Conf. Ser.,\nMater. Sci. Eng., vol. 1022, 2021, Art. no. 012121.\n[22] J. Johnson, G. Sharma, S. Srinivasan, S. K. Masakapalli, S. Sharma,\nJ. Sharma, and V . K. Dua, ‘‘Enhanced field-based detection of potato blight\nin complex backgrounds using deep learning,’’ Plant Phenomics, vol. 2021,\npp. 1–13, Jan. 2021.\n[23] T.-Y . Lee, I.-A. Lin, J.-Y . Yu, J.-M. Yang, and Y .-C. Chang, ‘‘High effi-\nciency disease detection for potato leaf with convolutional neural net-\nwork,’’Social Netw. Comput. Sci., vol. 2, no. 4, pp. 1–11, Jul. 2021.\n[24] A. Alharbi, ‘‘AI-driven framework for recognition of guava plant diseases\nthrough machine learning from DSLR camera sensor based high resolution\nimagery,’’Sensors, vol. 21, no. 11, p. 3830, 2021.\n[25] B. Srinivas, ‘‘Prediction of guava plant diseases using deep learning,’’\nin Proc. 3rd Int. Conf. Commun. Cyber Phys. Eng.Cham, Switzerland:\nSpringer, 2021, pp. 1495–1505.\n[26] V . Singh and A. K. Misra, ‘‘Detection of plant leaf diseases using image\nsegmentation and soft computing techniques,’’ Inf. Process. Agricult.,\nvol. 4, pp. 41–49, Mar. 2017.\n[27] M. Sharif, M. A. Khan, Z. Iqbal, M. F. Azam, M. I. U. Lali, and\nM. Y . Javed, ‘‘Detection and classification of citrus diseases in agriculture\nbased on optimized weighted segmentation and feature selection,’’ Com-\nput. Electron. Agricult., vol. 150, pp. 220–234, Jul. 2018.\n[28] S. M. Hassan and A. K. Maji, ‘‘Plant disease identification using a novel\nconvolutional neural network,’’ IEEE Access, vol. 10, pp. 5390–5401,\n2022.\n[29] J. Kong, H. Wang, C. Yang, X. Jin, M. Zuo, and X. Zhang, ‘‘A spatial\nfeature-enhanced attention neural network with high-order pooling rep-\nresentation for application in pest and disease recognition,’’ Agriculture,\nvol. 12, no. 4, p. 500, Mar. 2022.\n[30] A. Pahurkar and R. Deshmukh, ‘‘PDMBM: Design of a high-efficiency\nplant disease classification method using multiparametric bio inspired\nmodelling,’’ in Proc. Int. Conf. Sustain. Comput. Data Commun. Syst.\n(ICSCDS), Apr. 2022, pp. 1607–1615.\n[31] A. Bhargava, A. Bansal, and V . Goyal, ‘‘Machine learning–based detection\nand sorting of multiple vegetables and fruits,’’ Food Anal. Methods, vol. 15,\nno. 1, pp. 228–242, Jan. 2022.\n[32] A. Bhargava and A. Bansal, ‘‘Machine learning based quality evaluation\nof mono-colored apples,’’ Multimedia Tools Appl., vol. 79, nos. 31–32,\npp. 22989–23006, Aug. 2020.\n[33] A. Tabbakh and S. S. Barpanda, ‘‘Evaluation of machine learning models\nfor plant disease classification using modified GLCM and wavelet based\nstatistical features,’’ Traitement Du Signal, vol. 39, no. 6, pp. 1893–1905,\nDec. 2022, doi: 10.18280/ts.390602.\n[34] J. G. A. Barbedo, ‘‘Factors influencing the use of deep learning\nfor plant disease recognition,’’ Biosyst. Eng., vol. 172, pp. 84–91,\nAug. 2018.\n[35] G. Geetharamani and A. Pandian, ‘‘Identification of plant leaf diseases\nusing a nine-layer deep convolutional neural network,’’ Comput. Electr.\nEng., vol. 76, pp. 323–338, Jun. 2019.\n[36] R. Chaturvedi, A. Sharma, A. Bhargava, J. Rajpurohit, and P. Gothwal,\n‘‘Multi-level segmentation of fruits using modified firefly algorithm,’’\nFood Anal. Methods, vol. 56, pp. 2891–2900, Nov. 2022.\n[37] N. E. M. Khalifa, M. H. N. Taha, L. M. Abou El-Maged, and\nA. E. Hassanien, ‘‘Artificial intelligence in potato leaf disease classifi-\ncation: A deep learning approach,’’ in Machine Learning and Big Data\nAnalytics Paradigms: Analysis, Applications and Challenges. Berlin, Ger-\nmany: Springer, 2021, pp. 63–79.\n[38] A. Mittal and H. Gupta, ‘‘An experimental evaluation in plant disease\nidentification based on activation-reconstruction generative adversarial\nnetwork,’’ in Proc. 2nd Int. Conf. Advance Comput. Innov. Technol. Eng.\n(ICACITE), Apr. 2022, pp. 361–366.\n[39] Q. Liang, S. Xiang, Y . Hu, G. Coppola, D. Zhang, and W. Sun,\n‘‘PD2SE-Net: Computer-assisted plant disease diagnosis and severity esti-\nmation network,’’ Comput. Electron. Agricult., vol. 157, pp. 518–529,\nFeb. 2019.\n[40] R. Reedha, E. Dericquebourg, R. Canals, and A. Hafiane, ‘‘Transformer\nneural network for weed and crop classification of high resolution UA V\nimages,’’Remote Sens., vol. 14, no. 3, p. 592, Jan. 2022.\nVOLUME 11, 2023 45391\nA. Tabbakh, S. S. Barpanda: Deep Features Extraction Model\n[41] S. Wu, Y . Sun, and H. Huang, ‘‘Multi-granularity feature extraction based\non vision transformer for tomato leaf disease recognition,’’ in Proc. 3rd\nInt. Academic Exchange Conf. Sci. Technol. Innov. (IAECST), Dec. 2021,\npp. 387–390.\n[42] Y . Borhani, J. Khoramdel, and E. Najafi, ‘‘A deep learning based approach\nfor automated plant disease classification using vision transformer,’’ Sci.\nRep., vol. 12, no. 1, pp. 1–10, Jul. 2022.\n[43] S. Haug and J. Ostermann, ‘‘A crop/weed field image dataset for the\nevaluation of computer vision based precision agriculture tasks,’’ in Proc.\nEur. Conf. Comput. Vis.Cham, Switzerland: Springer, 2014, pp. 105–116.\n[44] L. Windrim, A. Melkumyan, R. J. Murphy, A. Chlingaryan, and\nR. Ramakrishnan, ‘‘Pretraining for hyperspectral convolutional neural net-\nwork classification,’’ IEEE Trans. Geosci. Remote Sens., vol. 56, no. 5,\npp. 2798–2810, May 2018.\n[45] C. D. Gurkaynak and N. Arica, ‘‘A case study on transfer learning in\nconvolutional neural networks,’’ in Proc. 26th Signal Process. Commun.\nAppl. Conf. (SIU), May 2018, pp. 1–4.\n[46] X. Xia, C. Xu, and B. Nan, ‘‘Inception-v3 for flower classification,’’ in\nProc. 2nd Int. Conf. Image, Vis. Comput. (ICIVC), 2017, pp. 783–787.\n[47] C. Narvekar and M. Rao, ‘‘Flower classification using CNN and transfer\nlearning in CNN—Agriculture perspective,’’ in Proc. 3rd Int. Conf. Intell.\nSustain. Syst. (ICISS), Dec. 2020, pp. 660–664.\n[48] H. Hu, Z. Zhang, Z. Xie, and S. Lin, ‘‘Local relation networks for\nimage recognition,’’ in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV),\nOct. 2019, pp. 3464–3473.\n[49] F. Long, Z. Qiu, Y . Pan, T. Yao, J. Luo, and T. Mei, ‘‘Stand-alone inter-\nframe attention in video models,’’ in Proc. IEEE/CVF Conf. Comput. Vis.\nPattern Recognit. (CVPR), Jun. 2022, pp. 3192–3201.\n[50] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszko-\nreit, and N. Houlsby, ‘‘An image is worth 16 ×16 words: Transformers for\nimage recognition at scale,’’ 2020, arXiv:2010.11929.\n[51] Kaggle. The Dataset is Taken From the Kaggle Opensource Link.\nAccessed: Aug. 2022. [Online]. Available: https://www.kaggle.com/\ndatasets/emmarex/plantdisease\n[52] D. Vasan, ‘‘IMCFN: Image-based malware classification using fine-tuned\nconvolutional neural network architecture,’’ Comput. Netw., vol. 171,\nApr. 2020, Art. no. 107138.\n[53] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ‘‘ImageNet classification\nwith deep convolutional neural networks,’’ Commun. ACM, vol. 60, no. 2,\npp. 84–90, Jun. 2012.\n[54] A. Fuentes, S. Yoon, S. Kim, and D. Park, ‘‘A robust deep-learning-\nbased detector for real-time tomato plant diseases and pests recognition,’’\nSensors, vol. 17, no. 9, p. 2022, Sep. 2017.\n[55] S. C. Bhardwaj, P. Prasad, O. P. Gangwar, H. Khan, and S. Kumar, ‘‘Wheat\nrust research-then and now,’’ Indian J. Agric. Sci., vol. 86, pp. 1231–1244,\nOct. 2016.\n[56] K. He, X. Zhang, S. Ren, and J. Sun, ‘‘Deep residual learning for image\nrecognition,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR),\nJun. 2016, pp. 770–778.\n[57] S. Coulibaly, B. Kamsu-Foguem, D. Kamissoko, and D. Traore, ‘‘Deep\nneural networks with transfer learning in millet crop images,’’ Comput.\nInd., vol. 108, pp. 115–120, Jun. 2019.\n[58] K. Simonyan and A. Zisserman, ‘‘Very deep convolutional networks for\nlarge-scale image recognition,’’ 2014, arXiv:1409.1556.\n[59] C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, ‘‘Rethinking\nthe inception architecture for computer vision,’’ in Proc. IEEE Conf.\nComput. Vis. Pattern Recognit. (CVPR), Jun. 2016, pp. 2818–2826.\n[60] C. Feng, H. Zhang, S. Wang, Y . Li, H. Wang, and F. Yan, ‘‘Structural\ndamage detection using deep convolutional neural network and transfer\nlearning,’’KSCE J. Civil Eng., vol. 23, no. 10, pp. 4493–4502, Oct. 2019.\n[61] R. M. Mohana, C. K. K. Reddy, and P. R. Anisha, ‘‘A study and early iden-\ntification of leaf diseases in plants using convolutional neural network,’’ in\nProc. 4th Int. Conf. Smart Comput. Informat.Cham, Switzerland: Springer,\n2021, pp. 693–709.\n[62] G. Sachdeva, P. Singh, and P. Kaur, ‘‘Plant leaf disease classification using\ndeep Convolutional neural network with Bayesian learning,’’ Mater. Today,\nProc., vol. 45, pp. 5584–5590, Jan. 2021.\n[63] M. A. Jasim and J. M. Al-Tuwaijari, ‘‘Plant leaf diseases detection and\nclassification using image processing and deep learning techniques,’’ in\nProc. Int. Conf. Comput. Sci. Softw. Eng. (CSASE), Apr. 2020, pp. 259-\n265.\n[64] J. Shijie, H. Siping, and L. Haibo, ‘‘Automatic detection of tomato disease\nand pests based on leaf images,’’ in Proc. Chin. Autom. Congr., 2017,\npp. 2510–2537.\n[65] J. Chen, W. Wang, D. Zhang, A. Zeb, and Y . A. Nanehkaran, ‘‘Atten-\ntion embedded lightweight network for maize disease recognition,’’ Plant\nPathol., vol. 70, no. 3, pp. 630–642, Apr. 2021.\n[66] J. Chen, D. Zhang, A. Zeb, and Y . A. Nanehkaran, ‘‘Identification of\nRice plant diseases using lightweight attention networks,’’ Exp. Syst. Appl.,\nvol. 169, May 2021, Art. no. 114514.\n[67] P. S. Thakur, P. Khanna, T. Sheorey, and A. Ojha, ‘‘Vision transformer\nfor plant disease detection: PlantViT,’’ in Proc. 6th Int. Conf. Cham,\nSwitzerland: Springer, 2022, pp. 501–511.\n[68] D. Suri, S. Saksenaa, U. Sehgal, and R. Garg, ‘‘Disease classification in\nwheat from images using CNN,’’ in Proc. 13th Int. Conf. Cloud Comput.,\nData Sci. Eng., Jan. 2023, pp. 566–571.\n[69] Z. Lin, ‘‘A unified matrix-based convolutional neural network for fine-\ngrained image classification of wheat leaf diseases,’’ IEEE Access, vol. 7,\npp. 11570–11590, 2019.\n[70] V . Kukreja and D. Kumar, ‘‘Automatic classification of wheat rust diseases\nusing deep convolutional neural networks,’’ in Proc. 9th Int. Conf. Rel.,\nINFOCOM Technol. Optim. (ICRITO), Sep. 2021, pp. 1–6.\n[71] W. Haider, A.-U. Rehman, N. M. Durrani, and S. U. Rehman,\n‘‘A generic approach for wheat disease classification and verification\nusing expert opinion for knowledge-based decisions,’’ IEEE Access, vol. 9,\npp. 31104–31129, 2021.\nAMER TABBAKHreceived the B.Tech. degree in\ninformatics engineering from Aleppo University,\nSyria, in 2014, and the M.Tech. degree in computer\nscience and engineering from KIIT University,\nBhubaneswar, India, in 2020. He is currently a\nPh.D. Scholar with the School of Computer Sci-\nence and Engineering, VIT-AP University, Amar-\navati, India. His research interests include machine\nlearning, deep learning, and image processing.\nSOUBHAGYA SANKAR BARPANDA received\nthe B.Tech. degree in computer science and\nengineering from the Biju Patnaik University of\nTechnology, Rourkela, and the M.Tech. degree\nin software engineering and the Ph.D. degree\nfrom NIT, Rourkela. He is currently an Associate\nProfessor with the School of Computer Science\nand Engineering, VIT-AP University, India. His\nresearch interests include image processing, soft-\nware engineering, and biometrics.\n45392 VOLUME 11, 2023",
  "topic": "Overfitting",
  "concepts": [
    {
      "name": "Overfitting",
      "score": 0.8291008472442627
    },
    {
      "name": "Computer science",
      "score": 0.7472556829452515
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7460773587226868
    },
    {
      "name": "Transfer of learning",
      "score": 0.7223858833312988
    },
    {
      "name": "Feature extraction",
      "score": 0.6330593824386597
    },
    {
      "name": "Classifier (UML)",
      "score": 0.5950984954833984
    },
    {
      "name": "Deep learning",
      "score": 0.5606473684310913
    },
    {
      "name": "Machine learning",
      "score": 0.5579657554626465
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5097452998161316
    },
    {
      "name": "Plant disease",
      "score": 0.46987563371658325
    },
    {
      "name": "Artificial neural network",
      "score": 0.2877250015735626
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Biotechnology",
      "score": 0.0
    }
  ]
}