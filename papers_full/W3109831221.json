{
    "title": "GNN-PT: Enhanced Prediction of Compound-protein Interactions by Integrating Protein Transformer",
    "url": "https://openalex.org/W3109831221",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2228477400",
            "name": "Wang Jing-tao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2011170750",
            "name": "Li Xi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1976557431",
            "name": "Zhang Hua",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3005064662",
        "https://openalex.org/W2116341502",
        "https://openalex.org/W2807792492",
        "https://openalex.org/W2809216727",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2148145769",
        "https://openalex.org/W2885815722",
        "https://openalex.org/W2989726436",
        "https://openalex.org/W3019745511",
        "https://openalex.org/W2777094228",
        "https://openalex.org/W2131774270",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2860192827",
        "https://openalex.org/W3022905129",
        "https://openalex.org/W2606004785"
    ],
    "abstract": "The prediction of protein interactions (CPIs) is crucial for the in-silico screening step in drug discovery. Recently, many end-to-end representation learning methods using deep neural networks have achieved significantly better performance than traditional machine learning algorithms. Much effort has focused on the compound representation or the information extraction from the compound-protein interaction to improve the model capability by taking the advantage of the neural attention mechanism. However, previous studies have paid little attention to representing the protein sequences, in which the long-range interactions of residue pairs are essential for characterizing the structural properties arising from the protein folding. We incorporate the self-attention mechanism into the protein representation module for CPI modeling, which aims at capturing the long-range interaction information within proteins. The proposed module concerning protein representation, called Protein Transformer, with an integration with an existing CPI model, has shown a significant improvement in the prediction performance when compared with several existing CPI models.",
    "full_text": "GNN-PTï¼šEnhanced Prediction of Compound-protein \nInteractions by Integrating Protein Transformer \nJingtao Wang 1*, Xi Li2, Hua Zhang3 \n1 Computational Biology Department, School of Computer Science, Carnegie Mellon University, Pittsburgh, PA \n15213, USA \n2 College of Computer Science, Zhejiang University, Hangzhou, Zhejiang, PR China 310027 \n3 School of Computer and Information Engineering, Zhejiang Gongshang University, Hangzhou, Zhejiang, PR China \n310018 \nAbstract \nThe prediction of protein interactions (CPIs) is crucial for the in -silico screening step in drug discovery. Recently, \nmany end-to-end representation learning methods using deep neu ral networks have achieved significantly better \nperformance than traditional machine learning algorithms. Much effort has focused on the compound representation \nor the information extraction from the compound -protein interaction to improve the model capability by taking the \nadvantage of the neural attention mechanism. However, previous studies have paid little attention to representing the \nprotein sequences, in which the long-range interactions of residue pairs are essential for characterizing the structural \nproperties arising from the protein folding.  We incorporate the self -attention mechanism into the protein \nrepresentation module for CPI modeling, which aims at capturing the long -range interaction information within \nproteins. The proposed module concerni ng protein representation, called Protein Transformer, with an integration \nwith an existing CPI model, has shown a significant improvement in the prediction performance when compared \nwith several existing CPI models.  \nAvailability: https://github.com/JingtaoWang22/CPI_prediction \nContact: jingtao2@cs.cmu.edu  \n \n1 Introduction \nIn-silico screening, which generates drug candidates, is usually the first step in drug discovery. \nMachine learning-based methods predicting the compound -protein interactions (CPIs) have been \nplaying an important role in this step. In the past decade, end -to-end representation learning using \ndeep neural networks with excellent performance, which does not involve complicated feature \nengineering for discrete symbolic data (e.g., words of natural language and amino acids of proteins), \nhas been widely applied to various areas, including the natural language translation [1], protein \nclassification[2,3] as well as the CPI problem[4â€“7].   \n    The data about CPIs are initially described as discrete symbol s. Specially, compounds are \nrepresented as graphs where the vertices are atoms, the edges are chemical bonds, and proteins are \namino acid sequences. The deep neural architectures for CPIs are in general composed of three \nmodules, i.e., the C-module representing the compounds, the P-module encoding the proteins and \nthe I-module for the compound -protein interactions by integrating these two modules. The deep \nlearning models for the C -module and P-module need to be compatible with the data structure of \nthe compound (graph) and protein (sequence) respectively. Therefore, a common choice about the \nnetwork architecture is the graph neural network[8](GNN) for the C-module, and the convolutional \nneural networks (CNN)[9] or the recurrent neural network (RNN)[10] for the P-module. As the first \nend-to-end representation model for the CPI prediction, Tsubaki et al.[11] adopted GNN for the C-\nmodule and CNN for the P-module. The outputs of the C-module and the P-module are the vector \nrepresentations of compounds and proteins respectively, and are then concatenated to the I-module \nfor the CPI prediction.  \nRecently, many variations of attention mechanisms have been applied to capture the \ninteractions of vector representations in various areas, including computer vision, nature language \nprocessing (NLP), bioinformatics. For example, Transformer [1] was firstly proposed as a natural \nlanguage translation model, which was accomplished based on the attention mechanism (without \ninclusion of CNN or RNN) and achieved the state -of-the-art performance at that time. The neural \nattention mechanism[12] is also usually applied to the I-module in the CPI framework for predicting \nthe interactions between the compounds and the proteins. Several recent approaches [13,14] \nproposed the CPI models by using the self -attention mec hanism in the I -module. Molecule \nTransformer DTI[15], instead, utilized the self -attention in the C -module to learn the compound \nrepresentation aiming at capturing the interaction information among the atoms of the compounds. \nThey argue that the self -attention m echanism can better relate long -range atoms in chemical \ncompounds better than other network architectures. However, the previous studies have not paid \nclose attention to the application of the self -attention mechanism in the P -module for the CPI \nprediction.   \n    In this work, we proposed a novel P -module for CPI prediction aiming at sufficiently \nextracting the protein features implied in the long-range residue-residue interactions. Due to the fact \nthat CNN or RNN has no advantage in long-range information extraction, we designed the P-module \nby utilizing the self-attention mechanism. Besides, we observed in practice that CNN layers are still \nuseful to extract the local features that are relevant to CPI. Therefore, the proposed novel P-module \nin this work includes both the self-attention layers and the CNN layers, called Protein Transformer. \nFinally, we integrate the existing GNN method for C-module and the proposed Protein Transformer \n(PT) for the P-module into an entire CPI model, named as GNN-PT. This proposed approach shows \nthe significant performance improvement when compared with the previous work by Tsubaki et \nal.[11] validated on the same datasets. The experiment results also implied that the proposed Protein \nTransformer in the GNN-PT model is the main contribution to the performance improvement of the \nCPI prediction.  \n     \n2 Materials and Methods \n2.1 The Self-Attention Mechanism  \nVaswani et al .[1] adopted self-attention mechanism as a substitution of RNN in their language \ntranslation model . Motivated by the finding that  it is more capable of learning long -range \ndependencies between words, we integrated the self-attention mechanism into the P-module.  \n    An attention function is a mapping from a Key-Value (K-V) pair and a Query (Q) to an output, \nwhere the Query, Key, Value and the output are all represented as vectors. In our case, Q, K, V are \nthe linear projection of the same input protein sequence representations, and the output is the new \nprotein representation incorporating the mutual association between amino acids. The whole process \nincludes three steps: Acquiring the linear projections Query Key V alue; computing the Weight by \nputting the Query and the Key into a compatibility function; and getting the output by computing \nthe weighted sum of the V alue using the computed Weight as the weight. \n    The compatibility function has many kinds of variations making there to be many versions of \nattentions. In this work, due to the length of the protein data (thousands of amino acids), the authors \nadopt the version with least time and space efficiency: â€œScaled Dot -Product Attentionâ€. The \ncompatibility function of it calculates the dot product of the Query and the Key, divides it by âˆšğ‘‘ğ‘˜ \nwhere dk is the dimension of Key, and finally apply softmax on it to get the Weight.  \n             ğ‘Šeight = soft ğ‘šğ‘ğ‘¥(\nğ‘„ğ¾ğ‘‡\nâˆšğ‘‘ğ‘˜\n)               (1) \nHere, Weight is a square matrix with the number of rows/columns equal to the length of the protein, \ni.e. the number of amino acids. The value in the ğ‘–th row ğ‘—th column of the Weight represents the \ninteraction intensiveness between the ğ‘–th and ğ‘—th amino acids.  \n    After calculating the weight, each row of the output, which is an amino acid vector, can be \ncalculated as the weighted sum of all amino acids. This is accomplished by a single matrix \nmultiplication: \n Output = ğ‘Šeight Ã— V =  soft ğ‘šğ‘ğ‘¥(\nğ‘„ğ¾ğ‘‡\nâˆšğ‘‘ğ‘˜\n)ğ‘‰  (2) \n    The intuition behind this is that amino acids should be allowed to interact based on the strength \nof the mutual interactions. \n \n2.2 The overall architecture of the proposed CPI model \n \nFigure 1. The overall architecture of the proposed CPI model (GNN-PT) composed of three \nmodules that are C-module, P-module and I-module, respectively.  \n \n    Figure 1 shows the overall network architecture of the proposed model. The left part of the \npicture shows how the model handle compound representation learning, which is the C -Module. \nMolecules in SMILES format are converted to graph, embedded, and fed  into the C-Module. The \nC-Module takes the embedding as input and learn the compound representation vector with 3 layers \nof GNNs. This compound representation will not only be feed into the I-module for final prediction, \n\nbut also be used in the P-Module for guiding protein representation learning. On the right is the P-\nModule for learning the protein representation, which is where we add self-attention mechanism for \nlearning long-distance interactions. The protein sequences are split into n-grams, embedded, adding \nthe positional encoding followed by Vaswani et al.[1], and fed into the P -Module for learning the \nprotein representation. Finally, on top of the C -module and P -module, I-module takes the output \nfrom both and produces the final interaction prediction. \n    In this work, we performed controlled experiments to demonstrate that the improvement in \nprotein representation learning indeed comes from our re -designed P-module, which utilizes self-\nattention. To do this, we limit our modification to the P-module CPI model proposed by Tsubaki et \nal. only, while using the C-module and the I-module as control. Therefore, in the rest of this section, \nwe will illustrate in detail how we design the P -Module and enable it to better learn the protein \nrepresentation. \n \n \nFigure 2. The network architectures of the self-attention encoder (left) and the target-attention \ndecoder (right) in the P-module. \n \n2.3 The Protein Transformer for the P-module \nThe proposed P -module, called Protein Tra nsformer (PT), as shown in Figure 2, includes three \nsequential layers: self -attention encoder, CNN layer and target -attention decoder. The embedded \nprotein representation is first fed into N layers of Self-Attention Encoder, the key component of our \nmodel. In this encoder, Self-Attention helps to learn the mutual interaction between all amino acid \npairs, disregarding the distance between them. Intuitively speaking, this corresponds to the folding \nprocess of proteins.  \n\nğ´ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘›ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ = ğ‘Šğ‘’ğ‘–ğ‘”â„ğ‘¡ =  ğ‘†ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(ğ‘„ğ‘¢ğ‘’ğ‘Ÿğ‘¦ âˆ™  ğ¾ğ‘’ğ‘¦ğ‘‡\nâˆšğ‘‘ğ‘˜\n) \nTo compute the attention scores for the self -attention mechanism, the query and the key are both \nprotein representation.  \n    Coming out of the self-attention encoding layer, neurons in the protein representation contain \nâ€œself-interactionâ€ information, including the long-range residue interactions. It is then fed into the \nCNN for further learning the properties. \n    The final part of the P -module is the target-attention decoder, where the interaction between \namino acid residues and the compound is learned. Compound representation and protein \nrepresentation are both fed into the multi -head target-attention layer. For computing the target -\nattention, the query is the compound representation, and the key is the protei n representation. The \nattention scores of this target -attention mechanism reflect the intensity of the interaction between \nthe compound and the residues of the protein.  \n    The target attention decoder outputs informative protein representation with prope rties \nregarding the structure, binding site, included. It will then be concatenated with the compound \nrepresentation and fed into the I-module for final prediction.  \n3 Results \n3.1 Performance comparison with other existing methods \nWe performed the proposed model on 2 datasets (human & C.elegans) that were created by Liu et \nal. [16] and also used by Tsubaki et al.[11]. We used all the same hyperparameters for the datasets \ndemonstrating that our model is universal for all kinds of CPI data. We used number of encoding \nlayer = 3, number of decoding layer = 1, learning rate = 2e -4, warmup step for the attention \nmechanism = 50 for all datasets. The performance of the proposed GNN-PT, is recorded in the last \ncolumn as shown in Table 1, Table 2, Table 3 and Table 4. \n \nTable 1. Performance comparison for the case of Human positive/negative ratio 1:1. \nMeasure KNN RF logR SVM Tsubaki et al. GNN-PT \nAUC 0.860 0.940 0.911 0.910 0.970 0.978 \nPrecision 0.798 0.861 0.891 0.966 0.923 0.928 \nRecall 0.927 0.897 0.913 0.950 0.918 0.952 \n \nTable 2. Performance comparison for the case of Human positive/negative ratio 1:3.  \nMeasure KNN RF logR SVM Tsubaki et al. GNN-PT \nAUC 0.904 0.954 0.920 0.942 0.950 0.980 \nPrecision 0.716 0.847 0.837 0.969 0.949 0.917 \nRecall 0.882 0.824 0.773 0.883 0.913 0.925 \n \nTable 3. Performance comparison for the case of C.elegans positive/negative ratio 1:1. \nMeasure KNN RF logR SVM Tsubaki et al. GNN-PT \nAUC 0.858 0.902 0.892 0.894 0.978 0.986 \nPrecision 0.801 0.821 0.890 0.785 0.938 0.970 \nRecall 0.827 0.844 0.877 0.818 0.929 0.960 \n \nTable 4. Performance comparison for the case of C.elegans positive/negative ratio 1:3. \nMeasure KNN RF logR SVM Tsubaki et al. GNN-PT \nAUC 0.892 0.926 0.896 0.901 0.971 0.983 \nPrecision 0.787 0.836 0.875 0.837 0.916 0.942 \nRecall 0.743 0.705 0.681 0.576 0.921 0.929 \n \n    The results of K -nearest neighbors (KNN), random forest (RF), logistic regression (logR ), \nsupport vector machine (SVM), have been already reported in the work by Liu et al.[16]. As shown \nabove, the proposed GNN-PT outperforms the existing methods on almost all conditions in terms \nof metrics and datasets. Notab ly, our model has a significantly higher AUC in all cases, \ndemonstrating this model that learn the knowledge very well and the prediction results are always \naligning with the truth. \n4 Discussion \nNotice that the main contribution of our work is to propose this approach by incorporating the self-\nattention mechanism to the P-module to help the model learn better. The proposed GNN-PT method \ncan almost be applied to any model for protein sequences. Based on this idea, and also because the \nauthors are not available to much computational resources, we did not try to fine-tune the model to \nget the optimal results. Our argument has been justified since the current one has a significant \nimprovement. That said, we discovered phenomena like stacking more self -attention encoder tend \nto further improve the performance, etc. Those are worth trying so that a better model can be \ndiscovered. \n \nReferences \n1.  Vaswani A, Shazeer N, Parmar N, Uszk oreit J, Jones L, Gomez AN, et al. Attention Is All You Need. \narXiv:170603762 [cs]. 2017. Available: http://arxiv.org/abs/1706.03762 \n2.  Qu Y-H, Yu H, Gong X-J, Xu J-H, Lee H-S. On the prediction of DNA -binding proteins only from primary \nsequences: A deep learning approach. PLOS ONE. 2017;12: e0188129. doi:10.1371/journal.pone.0188129 \n3.  Zhang Q, Zhu L, Bao W, Huang D -S. Weakly-Supervised Convolutional Neural Network Architecture for \nPredicting Protein-DNA Binding. IEEE/ACM Transactions on Computational Biology and Bioinformatics. 2020;17: \n679â€“689. doi:10.1109/TCBB.2018.2864203 \n4.  Gao KY , Fokoue A, Luo H, Iyengar A, Dey S, Zhang P. Interpretable drug target prediction using deep neural \nrepresentation. Proceedings of the 27th International Joint Conference on Artificial Intelligence. Stockholm, Sweden: \nAAAI Press; 2018. pp. 3371â€“3377.  \n5.  Wan F, Zhu Y , Hu H, Dai A, Cai X, Chen L, et al. DeepCPI: A Deep Learning-based Framework for Large -\nscale in silico Drug Screening. Genomics, Proteomics & Bioinformatics. 2019;17: 478 â€“495. \ndoi:10.1016/j.gpb.2019.04.003 \n6.  Karimi M, Wu D, Wang Z, Shen Y . DeepAffinity: interpretable deep learning of compoundâ€“protein affinity \nthrough unified recurrent and convolutional neural networks. Bioinformatics. 2019;35: 3329 â€“3338. \ndoi:10.1093/bioinformatics/btz111 \n7.  Li S, Wan F, Shu H, Jiang T, Zhao D, Zeng J. MONN: A Multi -objective Neural Network for Predicting \nCompound-Protein Interactions and Affinities. Cell Systems. 2020;10: 308-322.e11. doi:10.1016/j.cels.2020.03.002 \n8.  Scarselli F, Gori M, Tsoi AC, Hagenbuchner M, Monfardini G. The Graph Neural Network Model. IEEE \nTransactions on Neural Networks. 2009;20: 61â€“80. doi:10.1109/TNN.2008.2005605 \n9.  Aggarwal CC. Convolutional Neural Networks. In: Aggarwal CC, editor. Neural Networks and Deep Learning: \nA Textbook. Cham: Springer International Publishing; 2018. pp. 315â€“371. doi:10.1007/978-3-319-94463-0_8 \n10.  Schuster M, Paliwal KK. Bidirectional recurrent neural networks. IEEE Transactions on Signal Processing. \n1997;45: 2673â€“2681. doi:10.1109/78.650093 \n11.  Tsubaki M, Tomii K, Sese J. Compound â€“protein interaction prediction with end -to-end learning of neural \nnetworks for graphs and sequences. Bioinformatics. 2019;35: 309â€“318. doi:10.1093/bioinformatics/bty535 \n12.  Bahdanau D, Cho K, Bengio Y . Neural Machine Translation by Jointly Learning to Align and Translate. \narXiv:14090473 [cs, stat]. 2016. Available: http://arxiv.org/abs/1409.0473 \n13.  Chen L, Tan X, Wang D, Zhong F, Liu X, Yang T, et al. TransformerCPI: improving compound â€“protein \ninteraction prediction by sequence-based deep learning with self-attention mechanism and label reversal experiments. \nBioinformatics. doi:10.1093/bioinformatics/btaa524 \n14.  Agyemang B, Wu W-P, Kpiebaareh MY , Lei Z, Nanor E, Chen L. Multi-view self-attention for interpretable \ndrugâ€“target interaction prediction. Journal of Biomedical Informatics. 2020;110: 103547. \ndoi:10.1016/j.jbi.2020.103547 \n15.  Shin B, Park S, Kang K, Ho JC. Self -Attention Based Molecule Representation for Predicting Drug -Target \nInteraction. Proceedings of the 4th Machine Learning for Healthcare Conference. PMLR; 2019. pp. 230 â€“248. \nAvailable: http://proceedings.mlr.press/v106/shin19a.html \n16.  Liu H, Sun J, Guan J, Zheng J, Zhou S. Improving compound â€“protein interaction prediction by bui lding up \nhighly credible negative samples. Bioinformatics. 2015;31: i221â€“i229. doi:10.1093/bioinformatics/btv256 \n "
}