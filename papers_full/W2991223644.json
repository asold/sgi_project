{
  "title": "Evaluating Commonsense in Pre-Trained Language Models",
  "url": "https://openalex.org/W2991223644",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2098173758",
      "name": "Xuhui Zhou",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A2098449489",
      "name": "Yue Zhang",
      "affiliations": [
        "Westlake University"
      ]
    },
    {
      "id": "https://openalex.org/A2991083598",
      "name": "Leyang Cui",
      "affiliations": [
        "Westlake University"
      ]
    },
    {
      "id": "https://openalex.org/A2113115882",
      "name": "Dandan Huang",
      "affiliations": [
        "Westlake University"
      ]
    },
    {
      "id": "https://openalex.org/A2098449489",
      "name": "Yue Zhang",
      "affiliations": [
        "Westlake University"
      ]
    },
    {
      "id": "https://openalex.org/A2991083598",
      "name": "Leyang Cui",
      "affiliations": [
        "Westlake University",
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2113115882",
      "name": "Dandan Huang",
      "affiliations": [
        "Westlake University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1840435438",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2788292930",
    "https://openalex.org/W2941666437",
    "https://openalex.org/W2606964149",
    "https://openalex.org/W6635469476",
    "https://openalex.org/W2016089260",
    "https://openalex.org/W2922565841",
    "https://openalex.org/W2161915178",
    "https://openalex.org/W2952984539",
    "https://openalex.org/W6748634344",
    "https://openalex.org/W2798836595",
    "https://openalex.org/W2908854766",
    "https://openalex.org/W6752030624",
    "https://openalex.org/W2799054028",
    "https://openalex.org/W2947337775",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2886424491",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2953369973",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W4288351520",
    "https://openalex.org/W4288410857",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2950501607",
    "https://openalex.org/W2805206884",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2953084091",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2946609015",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W2963159690",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W1599016936"
  ],
  "abstract": "Contextualized representations trained over large raw text data have given remarkable improvements for NLP tasks including question answering and reading comprehension. There have been works showing that syntactic, semantic and word sense knowledge are contained in such representations, which explains why they benefit such tasks. However, relatively little work has been done investigating commonsense knowledge contained in contextualized representations, which is crucial for human question answering and reading comprehension. We study the commonsense ability of GPT, BERT, XLNet, and RoBERTa by testing them on seven challenging benchmarks, finding that language modeling and its variants are effective objectives for promoting models' commonsense ability while bi-directional context and larger training set are bonuses. We additionally find that current models do poorly on tasks require more necessary inference steps. Finally, we test the robustness of models by making dual test cases, which are correlated so that the correct prediction of one sample should lead to correct prediction of the other. Interestingly, the models show confusion on these test cases, which suggests that they learn commonsense at the surface rather than the deep level. We release a test set, named CATs publicly, for future research.",
  "full_text": "The Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence (AAAI-20)\nEvaluating Commonsense in Pre-Trained Language Models\nXuhui Zhou,1∗ Yue Zhang,2 Leyang Cui,2,3 Dandan Huang2\n1University of Washington\n2School of Engineering, Westlake University\n3Zhejiang University\nxuhuizh@uw.edu, {yue.zhang, cuileyang, huangdandan}@westlake.edu.cn\nAbstract\nContextualized representations trained over large raw text\ndata have given remarkable improvements for NLP tasks\nincluding question answering and reading comprehension.\nThere have been works showing that syntactic, semantic\nand word sense knowledge are contained in such representa-\ntions, which explains why they beneﬁt such tasks. However,\nrelatively little work has been done investigating common-\nsense knowledge contained in contextualized representations,\nwhich is crucial for human question answering and reading\ncomprehension. We study the commonsense ability of GPT,\nBERT, XLNet, and RoBERTa by testing them on seven chal-\nlenging benchmarks, ﬁnding that language modeling and its\nvariants are effective objectives for promoting models’ com-\nmonsense ability while bi-directional context and larger train-\ning set are bonuses. We additionally ﬁnd that current models\ndo poorly on tasks require more necessary inference steps.\nFinally, we test the robustness of models by making dual test\ncases, which are correlated so that the correct prediction of\none sample should lead to correct prediction of the other. In-\nterestingly, the models show confusion on these test cases,\nwhich suggests that they learn commonsense at the surface\nrather than the deep level. We release a test set, named CA Ts\npublicly, for future research.\nIntroduction\nContextualized representations trained over large-scale text\ndata have given remarkable improvements to a wide range\nof NLP tasks, including natural language inference (Bow-\nman et al. 2015), question answering (Rajpurkar, Jia, and\nLiang 2018) and reading comprehension (Lai et al. 2017).\nGiving new state-of-the-art results that approach or surpass\nhuman performance on several benchmark datasets, it is an\ninteresting question what types of knowledge are learned in\npre-trained contextualized representations in order to better\nunderstand how they beneﬁt the NLP problems above. There\nhas been work investigating the nature of syntactic (Liu et al.\n2019a), semantic (Liu et al. 2019a) and word sense (Kim et\nal. 2019) knowledge contained in such contextualized repre-\nsentations, in particular BERT (Devlin et al. 2019), showing\n∗Work done while at Westlake University\nCopyright c⃝ 2020, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nthat such knowledge can be effectively learned via language\nmodel (LM) pre-training over large scale data.\nCommonsense knowledge spans “a huge portion of hu-\nman experience, encompassing knowledge about the spatial,\nphysical, social, temporal, and psychological aspects of typ-\nical everyday life. ” (Liu and Singh 2004). Intuitively, such\nknowledge is at least as useful as semantic and syntactic\nknowledge in natural language inference, reading compre-\nhension and coreference resolution. For example, the word\n“it” in the sentence “the dog cannot cross the street because\nit is too X” can refer to three different entities when the word\n“X” is “timid”, “wide” and “dark”, respectively, and resolv-\ning such ambiguity can require that a system has relevant\ncommonsense knowledge beyond the sentence level. How-\never, relatively little work has been conducted on system-\natically evaluating the nature of commonsense knowledge\nlearned in contextualized representations.\nWe ﬁll this gap by evaluating ﬁve state-of-the-art contex-\ntualized embedding models on seven commonsense bench-\nmarks. The models include off-the-shelf embeddings\n1 from\nGPT (Radford and Sutskever 2018), GPT2 (Radford et al.\n2019), BERT (Devlin et al. 2019), XLNet (Yang et al. 2019)\nand RoBERTa (Liu et al. 2019b), and the benchmarks in-\nclude Conjunction Acceptability, Sense Making (Wang et al.\n2019), Winograd Schema Challenge (Levesque, Davis, and\nMorgenstern 2012), SW AG (Zellers et al. 2018), HellaSwag\n(Zellers et al. 2019), Sense Making with Reasoning (Wang et\nal. 2019), and Argument Reasoning Comprehension (Haber-\nnal et al. 2018). We evaluate commonsense knowledge con-\ntained in the above models by unifying the form of all the\ndatasets and comparing LM perplexities on positive and neg-\native samples (i.e., sentences that make sense and those that\ndo not make sense, respectively). Commonsense contained\nin our data covers a wide range of subjects, from physical\nworld knowledge to social conventions, from scientiﬁc do-\nmains to daily life scenes. We further categorize them by the\ndifﬁculty level, namely the number of inference steps neces-\nsary in making sense.\nWe reframe the datasets in order to conduct both word-\nand sentence-level testing. For word-level testing, negative\nsamples are drawn by replacing words from positive sam-\n1https://github.com/huggingface/transformers\n9733\nToken-level\nCA They broadcast an announcement, but a subway came into the station and I couldn’t hear it. \u0013\nThey broadcast an announcement,before a subway came into the station and I couldn’t hear it .\u0017\nWSC The trophy doesn’t ﬁt into the brown suitcase because the trophy is too large. \u0013\nThe trophy doesn’t ﬁt into the brown suitcase because thesuitcase is too large. \u0017\nSM money can be used for buying cars \u0013\nmoney can be used for buyingstars \u0017\nSentence-level\nSMR ¬“ he put an elephant into the fridge” (because)← an elephant is much bigger than a fridge . \u0013\n¬“ he put an elephant into the fridge ” (because)← elephants are usually gray... \u0017\n¬“ he put an elephant into the fridge ” (because)← an elephant cannot eat a fridge . \u0017\nSW AG Someone unlocks the door and they go in.→ Someone leads the way in. \u0013\nSomeone unlocks the door and they go in.→ Someone opens the door and walks out. \u0017\nSomeone unlocks the door and they go in.→ Someone walks out of the driveway. \u0017\nSomeone unlocks the door and they go in.→ Someone walks next to someone and sits on a pew.\u0017\nHellaSwag A carved pumpkin with a light in it glows on a counter. Supplies for carving are then shown.\n→ A woman cuts the top off the pumpkin, emptying the seeds. \u0013\n→ she cuts down all the pieces and dumps them in a trash bin in the end. \u0017\n→ she then carves the traced lines to cut out the design. \u0017\n→ she tapes the top shut as the continue carving the pumpkin. \u0017\nARCT People can choose not to use Google ∧ Other search engines don’t redirect to Google\n→ Google is not a harmful monopoly \u0013\nPeople can choose not to use Google∧ All other search engines redirect to Google\n→ Google is not a harmful monopoly \u0017\nTable 1: Example of reframed test instances corresponding to each of our test task. The key word isbolded in token-level tasks.\n∧, ¬, ← and →are used for showing the logic ﬂows and replaced by natural language in actual test data.\nples. We are concerned about nouns, verbs, adjectives, ad-\nverbs, pronouns and conjunctions, which reﬂect different as-\npects of commonsense. For example, while verbs such as\n“buy, throw, sell ...” are relatively more associated with event\nknowledge, conjunctions such as “because, but, so ...” are\nmore associated with logical reasoning. For sentence-level\ntesting, negative examples are drawn by replacing a full sub-\nsentences (such as a clause) with irrelevant or conﬂicting\ncontents. Sentence-level tests concern more about common-\nsense inference.\nFrom the results we have four salient observations. First,\nthe pre-trained models give consistently better performances\nthan random baselines, which demonstrates that language\nmodel pre-training is useful for learning commonsense\nknowledge. Second, models based on bi-directional con-\ntexts such as BERT, XLNet and RoBERTa are stronger in\nlearning commonsense knowledge compared to those based\non uni-directional contexts, such as GPT and GPT2. Third,\nmore commonsense knowledge can be learned from larger\ntraining sets, which conforms well to the intuition. Fourth,\nthe models have a certain degree of commonsense reason-\ning ability. However, as the number of necessary inference\nsteps increase, the model performances drop, which shows\nthat commonsense is still a big challenge that is not com-\npletely solved by pre-trained contextualized language mod-\nels (LMs).\nFinally, we further test the robustness of the ﬁve mod-\nels by making dual test samples. Here a dual test sample is\nbuilt by adding, deleting and replacing words in a test sam-\nple, or swapping two words in the sample, thereby resulting\nin a closely related test case. In theory, a model equipped\nwith relevant commonsense should give consistent predic-\ntions on a pair of dual test cases. However, we ﬁnd that none\nof the models are able to reach such consistency. Instead,\nthe models are confused by the modiﬁcation, tending to give\nthe same predictions over a pair of dual samples despite they\nmay have different gold labels. This further reveals that com-\nmonsense contained in the pre-trained models may remain in\na surface level, without deep semantic comprehension. We\npublicly release our datasets, named commonsense ability\ntests (CA Ts), and the test script at GitHub.\n2\nTasks for Evaluating Commonsense\nCommonsense ability can be broadly divided to two cate-\ngories. First, a model with commonsense ability should have\nbasic knowledge about the world, for example,water always\ngoes down. Second, it should have the ability to reason over\ncommonsense knowledge, such aswater always goes down\nbecause there is gravity on the earthand if you are injured,\nyou should go to the hospital. To comprehensively test dif-\nferent models’ commonsense ability, we synthesize six chal-\nlenging tasks by taking positive and negative samples from\nexisting benchmarks, and further introduce a new task called\nConjunction Acceptability (CA).\nWe reframe all the tasks into sentence-scoring tasks by\nsubstitution or concatenation. For example, we create posi-\n2https://github.com/XuhuiZhou/CA TS\n9734\nOriginal:\nPaul tried to call George on the phone, but he wasn’t\nsuccessful.\nWho is he?\nCandidate: A. Paul (correct) B. George\nReframed:\nA. Paul tried to call George on the phone, but Paul\nwasn’t successful.(Positive sample)\nB. Paul tried to call George on the phone, but George\nwasn’t successful.(Negative sample)\nTable 2: Example of reframing a WSC question; Note that\nthere can be additional negative samples.\ntive and negative samples by replacing a pronoun in the sen-\ntence of a WSC question with the candidates to obtain a test\ninstance as Table 2. A model is asked to score the sentences\nand we pick the sentence with the highest score as its predic-\ntion in a test instance. Below we introduce the data sources\nand reframed tasks in detail (the correct answer isbolded).\nSense Making (SM)\nIntroduced by Wang et al. (2019), this task tests whether\na model can differentiate sense-making and non-sense-\nmaking statements. Given a pair of statements (i.e a test\ninstance), it requires the model to choose the more sensi-\nble statement. One example is: I work 8 hours a day / I\nwork 25 hours a day. This task conforms to our evaluation\nschema without a change. More examples are shown in the\nSM section of Table 1. The statements typically differ only\nin one key word which covers nouns, verbs, adjectives, and\nadverbs.\nWinograd Schema Challenge (WSC)\nThe Winograd Schema Challenge (WSC) dataset (Levesque,\nDavis, and Morgenstern 2012) consists 273 instances of the\npronoun resolution problem. Each instance contains a sen-\ntence with a pronoun referring to one of nouns; the original\nquestion is to pick the correct noun. For our task, we trans-\nform the test as shown in Table 2. More examples are shown\nin the WSC section of Table 1. WSC is recognized as one of\nthe most difﬁcult commonsense datasets.\nConjunction Acceptability (CA)\nAs stated by LoBue and Yates (2011), logic-based common-\nsense knowledge is an important part of world knowledge\nin addition to content-based knowledge. We aim to probe a\nmodel’s ability to understand the logic relations in the lan-\nguage by extracting 189 positive samples from the WSC\ndataset and replacing the conjunction manually with another\nconjunction to obtain a negative sample. We pair the positive\nand negative samples to obtain a test instance. For example,\nThe lawyer asked the witness a question, and the witness\nwas reluctant to answer it/ The lawyer asked the witness a\nquestion, but the witness was reluctant to answer it. More\nexamples are shown in the CA section of Table 1. This task\nusing “because”, “before”, “when”, “but”, “and” to corre-\nspond to the Cause and Effect, Preconditions, Simultaneous\nConditions, Contradiction, and Addition logic relations, re-\nspectively. It is complementary to the other token-level tasks\nwhich focus more on content-based knowledge.\nSW AG\nSW AG (Zellers et al. 2018) is a dataset with multiple choices\nquestions about grounded situations. It questions models’\nunderstanding towards the relationship between two phys-\nical scenes. With the help of adversarial ﬁltering (AF),\nZellers et al. created a sufﬁciently large amount of questions\nautomatically. For example, givenOn stage, a woman takes\na seat at the piano. She, the question is to choose the follow-\ning candidates: A. sits on a bench as her sister plays with\nthe doll B. smiles with someone as the music plays C.is in\nthe crowd, watching the dancers D.nervously sets her ﬁn-\ngers on the keys. We obtain a positive or negative sample by\nconcatenating the context and a candidate together (e.gOn\nstage, a woman takes a seat at the piano. She nervously sets\nher ﬁngers on the keys). There are one positive sample and\nthree negative samples in a SW AG test instance. More ex-\namples are shown in the SW AG section of Table 1. By forc-\ning the model to predict the next action, it requires inductive\nreasoning and temporal reasoning.\nHellaSwag\nHellaSwag (Zellers et al. 2019) is an argumented version\nof SW AG with the same data format as SW AG, more infer-\nence steps and higher data quality. While HellaSwag also\nincludes the dataset from WikiHow, we choose only the in-\nstances coming from ActivityNet to make the results com-\nparable to the original SW AG dataset.\nSense Making with Reasoning (SMR)\nSense Making with Reasoning focuses on identifying the\nreason behind a statement (Wang et al. 2019) against com-\nmonsense. A model needs to understand that a speciﬁc state-\nment (e.g can is usually made of gold) is against common-\nsense and to make a choice for the reason behind from three\ncandidates (e.g gold is too bright to make cans, gold is too\nsoft to make cans and gold is too expensive to make cans).\nWe make a positive or negative sample by concatenating the\nstatement and candidate reason together. For each test in-\nstance in SMR, there is a positive sample and two negative\nsamples. More examples are shown in the SMR section of\nTable 1. This task is intuitively difﬁcult since it requires a\nmodel to have deeper knowledge of with higher-level infer-\nence, which belongs to abductive reasoning.\nArgument Reasoning Comprehension Task\n(ARCT)\nSimilar to SMR, Habernal et al. (2018) propose the ARCT\ndataset to test a model’s abductive reasoning ability. Its do-\nmain lies in social topics such as search engine and LGBT\nrights, which is different from the daily-routine scenarios.\n9735\nFor example, given a reasonR: I ﬁnd the idea that it is a\nsin to be born or live a life at all to be preposterousand a\nclaim C: Christians have created a harmful atmosphere for\ngays, this task is to pick the correct warrantW from two can-\ndidates: A. being gay isn’t considered a sin B.being gay is\nconsidered a sin, whereR∧W → C. We make a positive or\nnegative sample by concatenating the reason, candidate war-\nrant and claim together (e.gI ﬁnd the idea that it is a sin to\nbe born or live a life at all to be preposterous and since be-\ning gay is considered a sin, Christians have created a harm-\nful atmosphere for gays). A test instance in ARCT contains\na pair of positive and negative samples. More examples are\nshown in the ARCT section of Table 1. We further break this\ntask into two variants, where ARCT1 represents the original\ndataset, ARCT2 represents an argumented dataset by adding\nnegation to original instances to alleviate the statistical cues\nin the dataset (Niven and Kao 2019).\nWe integrated the above test sets into a commonsense\nability test (CA Ts) benchmark, released for future research.\nPre-trained Models\nWe take six contextualized representation models that give\nthe state-of-the-art performances on NLP benchmarks such\nas GLUE (Wang et al. 2018) and SQuAD (Rajpurkar, Jia,\nand Liang 2018). Off-the-shelf models are taken. Below we\ngive the detailed settings.\nGPT (Radford and Sutskever 2018) is a uni-directional\ntransformer LM trained on 800M tokens of BookCorpus\n(Zhu et al. 2015). Given a text sequencex =[ x\n1, ..., xT],\nGPT works in a way similar to conventional auto-regressive\n(AR) LM:\nmax\nθ\nlogpθ(x)=\nT∑\nt=1\nlogpθ(xt|x<t),\nwhere x<t =[ x1, ..., xt−1]. The model has dimension of\nhidden states H = 768, attention head numbersA =1 2,\nnumber of layers L =1 2 and total parameter size P =\n110M.\nGPT2 (Radford et al. 2019) works similarly as GPT with\na few modiﬁcations on the hyperparameters. In particular,\nGPT2 optimizes the layer normalization, expands the vocab-\nulary size to 50,257, increases the context size from 512 to\n1024 tokens, and optimizes with a larger batchsize of 512.\nIn addition, GPT2 is pre-trained on WebText, which was cre-\nated from scraping web pages. The dataset roughly contains\n8 million documents (40 GB). We study GPT2-base and\nGPT2-medium, with model sizeH = 768,A =1 2,L =\n12,P = 117M and H = 1024,A =1 6,L =2 4,P =\n345M, respectively, where the deﬁnitions of H, L and A are\nthe same as for GPT.\nBERT (Devlin et al. 2019) jointly trains on a masked\nlanguage modeling task and a next sentence prediction task\n(NSP). The model is trained on the BookCorpus and English\nWikipedia, a total of approximately 3300M tokens. BERT is\ndesigned with the following objective:\nmax\nθ\nlogpθ(¯x|˜x) ≈\nT∑\nt=1\nmt logpθ(xt|˜x),\nwhere ˜x is a corrupted version of text sequencex, and ¯x is\nmasked tokens.mt =1 if tokenxt belongs to¯x.\nHere we consider BERT-base and BERT-large, withH =\n768,A =1 2,L =1 2,P = 117M and H = 1024,A =\n16,L =2 4,P = 340M, respectively, where the deﬁnitions\nof H, L and A are the same as for GPT.\nXLNet (Yang et al. 2019) is trained with a permutation-\nbased language modeling objective to capture bidirectional\ncontexts while retain the beneﬁts of AR models. Speciﬁcally,\nthey let Z\nT be the set of all possible permutations of the\nlength-T sequencex =[ x1, ..., xT]:\nmax\nθ\nEz∼ZT\n[T∑\nt=1\nmt logpθ(xzt |˜xz<t)\n]\n,\nwhere zt and z<t are the t-th element and the ﬁrstt − 1\nelements of a permutationz ∈Z T, respectively. In this way,\nXLNet ensures that any speciﬁc tokenxt in x has seen all\nthe tokens before or after it.\nWe consider XLNet-base and XLNet-large, whose model\nsizes are H = 768,A =1 2,L =1 2,P = 117M and\nH = 1024,A =1 6,L =2 4,P = 340M, respectively,\nwhere the deﬁnitions of H, L and A are the same as for\nGPT. Note that XLNet-base is trained with the same data\nas BERT, while XLNet-large is trained with a larger dataset\nthat consists of 32.98B subword pieces coming from Wiki,\nBookCorpus, Giga5, ClueWeb, and Common Crawl.\nRoBERTa (Liu et al. 2019b) has the same architecture\nas BERT but is trained with dynamic masking, FULL-\nSENTENCES without NSP loss, a larger batch-size and a\nlarger vocabulary size. Given the optimized design choice,\none key difference of RoBERTa with other models is its\nlarge training dataset, which consists of BookCorpus, CC-\nNEWS, OpenWebText, and STORIES. With a total 160GB\ntext, RoBERTa has access to more potential knowledge than\nthe other models.\nExperimental Design\nThe CA T datasets are applicable to any model that has a\nmethod to score a sentence. They ﬁt with the pre-trained\nmodels above, which are by nature language models. We\nderive the score of a sentence below with uni-directional-\ncontext LMs and bi-directional-context LMs, respectively.\nFormally, suppose the sentence S of n words S =\n{w\n1, ..., wk−1,w k,w k+1, ..., wn}. We deﬁne the score of a\nsentence as:\nScore(S)=\n∑n\nk=1 log(Pθ(wk|contextk)\nn ,\nwhere the denominator n is for alleviating the inﬂu-\nence of the sentence length to models’ prediction, espe-\ncially in sentence-level tasks. For a uni-directional model,\ncontext\nk = S<k ≡{ w1, ..., wk−1}. The numerator be-\ncomes ∑n\nk=1 log(Pθ(wk|S<k)), which is factorized from\nlog(Pθ(w1, ..., wk−1,w k,w k+1, ..., wn)). This is essentially\na LM. For a bi-directional model, the contextk = S−k,\nwhich represents theS with the k-th word being removed.\nIn particular, the k-th word can be removed with be-\ning replaced by a special token ‘[MASK]’ in BERT. The\n9736\nCA WSC SM SMR SW AG HellaSwag ARCT1 ARCT2 Average\nRANDOM 0.500 0.500 0.500 0.333 0.250 0.250 0.500 0.500 0.416\nGPT 0.830 0.558 0.735 0.354 0.592 0.263 0.472 0.528 0.542\nGPT2-base 0.787 0.512 0.705 0.355 0.503 0.300 0.466 0.509 0.517\nGPT2-medium 0.885 0.568 0.746 0.385 0.591 0.338 0.462 0.527 0.563\nBERT-base 0.891 0.523 0.697 0.419 0.625 0.373 0.477 0.503 0.563\nBERT-large 0.934 0.625 0.694 0.444 0.696 0.393 0.468 0.517 0.596\nXLNet-base 0.809 0.544 0.662 0.374 0.494 0.381 0.516 0.526 0.543\nXLNet-large 0.891 0.636 0.583 0.394 0.662 0.435 0.563 0.570 0.591\nRoBERTa-base 0.901 0.623 0.750 0.423 0.712 0.414 0.501 0.537 0.565\nRoBERTa-large 0.962 0.694 0.792 0.512 0.769 0.5 0.606 0.599 0.679\nHUMAN 0.993 0.920 0.991 0.975 0.880 0.945 0.909 0.909 0.945\nTable 3: Accuracy for each pre-trained contextualizer on each test set. The rightmost column shows the average of accuracy\nscore of each model.\nnumerator ∑\nn\nk=1 log(Pθ(wk|S−k)) can also be factorized\nfrom log(Pθ(w1, ..., wk−1,w k,w k+1, ..., wn))under the as-\nsumption that wk is independent of the successive words\n(i.e. wk+1,w k+2, ..., wn), which is the bi-directional-context\nLM.\nIntuitively, Pθ(wk|contextk) can be interpreted as how\nprobable a word wk is given the contextk: S<k or S−k.\nFor example, letS−k = He put an [MASK] into the fridge,\nwk1 = elephant and wk2 = turkey. Pθ(wk2|S−k) should\nhave a relatively larger value since ﬁlling in the “elephant”\nin the ﬁrst case results in an improper sentence, which is\nagainst commonsense.\nAs introduced earilier (Table 1), all CA Ts tasks consist\nof instances with positive and negative sentences. After we\nscore each sample in a test instance, the models predict the\npositive sample simply by taking the highest score in the\ninstance.\nCommonsense Tests Results\nTable 3 shows the model performances with random choices\nas the baseline. Take WSC for example, the random base-\nline is 0.5, the human is 0.920 and all the models range be-\ntween 0.512 and 0.694 with RoBERTa-large giving the best\nresult of 0.694. Except for the ARCT task, all tested models\ndemonstrate stronger performances than RANDOM, which\nindicates that the models all have varying degrees of com-\nmonsense. However, for most of the tasks, all of models are\nwell below human performance.\nUni-directional Vs Bi-directional LM\nWe compare uni-directional (GPT, GPT2-base, GPT2-\nmedium) and bi-directional models (Bert-base, Bert-large,\nXLNet-base, XLNet-large, RoBERTa-base, and RoBERTa-\nlarge). Picking the strongest model from each group,\nRoBERTa-large outperforms GPT2-medium by a large mar-\ngin for every task. As mentioned before, RoBERTa-large\nhas the same parameter size as GPT2-medium. However,\nRoBERTa-large is trained with much more data than GPT2-\nmedium.\nFrom Figure 1, we can see that except for the SM\ntask, both BERT-large and XLNet-large outperform GPT2-\nFigure 1: Comparison between bidirectional and unidirec-\ntional models among different tasks.\nmedium while BERT-large is trained with a smaller dataset\nthan GPT2-medium. This indicates that bi-directional con-\ntext can be more useful for learning commonsense. Intu-\nitively, the models with bi-directional context can make\nmore sentence-level inference. While only the predecessing\nwords receive sufﬁcient context in a uni-directional model,\nevery word has the full context for bi-directional models. Ta-\nble 4 shows examples where RoBERTa-large makes the cor-\nrect prediction but GPT2-medium does not, we can see that\nthe key tokens, which are considered to be the most inﬂuen-\ntial part in making the correct prediction, lie in the middle of\nthe sentence. This can be the main reason why bi-directional\ncontext is important for models’ commonsense ability.\nScale of Training Data\nA larger training dataset intuitively allows a model to have\naccess to more commonsense knowledge, thus performs\nbetter in our tests. Trained with by far the most data,\nRoBERTa is the winner for every task. Most of the mod-\nels are in fact trained on a subset of the dataset used to\ntrain RoBERTa. However, larger dataset do not always work\nwhen the model capacity is limited with regard to com-\nmonsense. For example, GPT2-base underperforms GPT for\nmany tasks in our dataset, which suggests GPT2-base un-\n9737\nToken-level:\nA. Sam pulled up a chair to the piano, but the\nchair was broken, so he had to stand instead.\nB. Sam pulled up a chair to the piano, but the piano\nwas broken, so he had to stand instead.\nSentence-level:\nA. Comments sections permit a reader to ana-\nlyze many different perspectives in one place, and\nsince I want to see all these ideas\n, even stupid\nones, Comment sections have not failed.\nB. Comments sections permit a reader to analyze\nmany different perspectives in one place, but since\nI don’t want to see all these stupid ideas\n, Comment\nsections have not failed.\nTable 4: Examples that a bi-directional model (RoBERTa-\nlarge) predicts correctly while a uni-directional model (GPT-\n2-medium) makes an incorrect prediction; the correct an-\nswer isbolded; the key tokens are underlined\n.\nFigure 2: Models performances when the number of infer-\nence step (IS) increases. Tasks are ranked according to their\nIS in an increasing order from left to right.\nderﬁts the WebText dataset with regard to commonsense.\nThe fact that RoBERTa-base has the same parameter size\nas GPT2-base, yet beneﬁts from the larger dataset suggests\nthat bi-directional models have larger representative power\nin commonsense ability.\nNumber of Inference Steps\nSimilar to humans, the model performance can intuitively\ndrop when commonsense inference becomes more compli-\ncated. To verify this intuition, we pick 100 sentences ran-\ndomly from each test dataset and annotate the number of re-\nquired inference steps (IS) of each instance manually. The\ninference step of each test dataset is deﬁned as the aver-\nage of the number of the turns of reasoning necessary for\nthe instances from the test dataset. We choose to answer the\nquestion by counting the logical operations that exist in an\ninstance. For example, for the sentence\nOriginal:\nA. People usually like wealth. B. People hardly like\nwealth.\nDual:\nA. People usually hate wealth. B. People hardly hate\nwealth.\nTable 5: Example of a robust test case; it contains a test in-\nstance from the original test set with a dual test instance cre-\nated manually. When the key word changes from ‘like’ to\n‘hate’, the correct answer switches from A to B. This is an\nunique feature of our robustness test sets.\nThey add a lot to the piece and I look forward to read-\ning comments, but since comments sections always distract\nme from my work, Comment sections have failed., the logic\nchain is (They add a lot to the piece ∧ I look forward to\nreading comments) ∧comments sections always distract me\nfrom my work→ Comment sections have failed. Thus, this\ninstance needs three inference steps.\nIn this way, we obtain the Inference Step (IS) for seven\ntest datasets. Each instance is labeled by two expert anno-\ntators, and the inter-annotator agreement is 93%. The ﬁ-\nnal IS is the average from both annotators. Figure 2 shows\nthe results\n3 on the test cases with different IS. There is a\ndecrease of performances as IS increases. SW AG and Hel-\nlaSwag fall out the trend, which may suggest that the models\nhave stronger commonsense ability in temporal reasoning.\nGenerally speaking, all of our tested models outperform\nthe random baselines except for the ARCT task, which sug-\ngests that despite of using different modeling schemas, lan-\nguage modeling stands as an effective objective for extract-\ning commonsense knowledge from large, raw texts. For\neach task, the overall performance increases with a larger\nmodel parameter size, a more sophisticated model design,\nand larger training data.\nRobustness Test\nThe robustness of models in commonsense reasoning is an\nimportant perspective in evaluating deep commonsense abil-\nity. Intuitively, a person can reason whether a statement\nmakes sense or not because he has consistent knowledge. If\nthe statement changes slightly, for example, changing a key\nword, that person should still make the correct judgement.\nWe aim to test the robustness of the ﬁve models by mak-\ning dual test samples. A dual instance to the original in-\nstance should test the same commonsense knowledge point\nor largely relevant to the original one. In this way, we expect\nthat the model can demonstrate consistency in the decision.\nOne example is shown in Table 5, which choosing A in the\noriginal instance should lead to choosing B in the dual case\n(See Figure 3 for more examples).\n3The performances on tasks with more than one negative sam-\nple are transformed to binary-choice scales.\n9738\nAdd Del Swap Sub\nRANDOM 0.50 0.50 0.50 0.50\nGPT 0.13 0.17 0.51 0.19\nGPT2-base 0.20 0.23 0.45 0.22\nGPT2-medium 0.24 0.24 0.52 0.22\nBERT-base 0.26 0.15 0.50 0.29\nBERT-large 0.26 0.25 0.56 0.24\nXLNet-base 0.36 0.16 0.41 0.26\nXLNet-large 0.36 0.39 0.37 0.26\nRoBERTa-base 0.20 0.27 0.47 0.35\nRoBERTa-large 0.29 0.33 0.56 0.42\nTable 6: Portion of consistent cases of each method for each\ncontextualizer. Add stands for adding key words in the test\nsample; Del stands for deleting key words in the test sam-\nple; Swap stands for swapping the position of words in the\ntest sample; Sub stands for replacing key words in the test\nsample. The best contextualizer for each method isbolded.\nWe consider multiple ways to construct a dual test in-\nstance. Particularly, a dual test instance is built by methods:\nadding, deleting and replacing words in a test sample, or\nswapping two words in the sample, thereby resulting in a\nclosely related test instance. All of our dual test instances\nare constructed from the original commonsense test data.\nWe construct 75 dual instances for each method above\nover WSC, SM, and ARCT, keeping the instances from each\ndataset approximately equivalent in order to evaluate the in-\nﬂuence of different duality methods to the models. We then\npair each dual instance with the original instance to form a\nnew test case. If the model gives the correct or wrong pre-\ndiction for both of the instances in this case, we recognize it\nas aconsistent case.\nThe results are shown in Table 6. In theory, a model\nequipped with relevant commonsense should give consis-\ntent predictions on a pair of dual test case. However, we ﬁnd\nthat none of the models reach consistency. In fact, their con-\nsistency is well below the random baselines except for the\nSwap method.\nTo better investigate the reason behind the poor consis-\ntency, we look at inconsistent cases from the pre-trained\nmodel (i.e RoBERTa-large). Similar to Trinh and Le (2018),\nwe investigate how the model makes decision between two\ncandidate sentencesS\ncorrect and Sincorrect where they have\nthe same number of words. In particular, we look at:\nqk = log( Pθ(wk|Scorrect −{wk})\nPθ(wk|Sincorrect −{wk})),\nwhere 1 ≤ k ≤ n. It follows that the choice between\nScorrect and Sincorrect is made by the valueQ = ∑\nk qk\nbeing bigger than 0 or not. Visualizing the value of eachqk\nprovides more insights into the decisions of the model.\nFrom Figure 3, we can tell that the model is confused\nby the modiﬁcation, tending to give the same predictions\nover a pair of dual samples despite that they have different\ngold labels, especially for Sub, Add and Del. This further\nreveals that the commonsense knowledge contained in the\nFigure 3: Samples of questions from Add, Del, Sub and\nSwap predicted correctly for the original instance but incor-\nrectly for the dual instance. Note that a sentence here rep-\nresents a test instance with a pair of positive and negative\nsamples, represented by (.../...). Here we mark the correct\nprediction by an asterisk and display the normalizedq\nt by\ncoloring its corresponding word.\npre-trained models may remain in a surface level, without\ndeep semantic comprehension.\nRelated Work\nLiu et al. (2019) evaluate BERT (Devlin et al. 2019), GPT\n(Radford and Sutskever 2018), and ELMo (Peters et al.\n2018) on a variety of linguistics tasks. Their results sug-\ngest that the features generated by pre-trained contextualizer\nare sufﬁcient for high performance on a board set of tasks\nbut models fail on tasks requiring ﬁne-grained linguistics\nknowledge. Tenney et al. (2019) evaluate similar models on\na variety of sub-sentence linguistic analysis tasks. Their re-\nsults suggest that contextualized word representation encode\nboth syntax and semantics. Our work is in line in the sense\nthat contextualized representation encode rich knowledge to\nbe ‘probed’. However, we focus on evaluating the common-\nsense in those representations. To our best knowledge, this\nis the ﬁrst work to systematically evaluate commonsense in\npre-trained models.\nOur evaluation method is similar to Trinh and Le (2018),\nwho make use of LM to score a sentence. However, they\nfocus on Winograd schema questions with only self-trained\nrecurrent LMs while we test ﬁve models’ commonsense with\nseven diverse tasks.\nConclusion\nWe studied the commonsense knowledge and reasoning\nability of pre-trained contextualizers with a suite of seven\ndiverse probing tasks, showing that large-scale pre-trained\ncontextualized representation has a certain degree of com-\nmonsense knowledge, but there is still a quite large gap be-\ntween the current state-of-the-art representation models and\nrobust human-level commonsense reasoning, which may re-\nquire more breakthrough in modeling. We release our test\nsets, named CA Ts, publicly.\n9739\nAcknowledgments\nWe would like to thank the anonymous reviewers for their\ninsightful comments, and Mr. Cunxiang Wang for his help\non the collection of the data. Y ue Zhang is the corresponding\nauthor.\nReferences\nBowman, S. R.; Angeli, G.; Potts, C.; and Manning, C. D.\n2015. A large annotated corpus for learning natural language\ninference. In EMNLP.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of deep bidirectional transformers for\nlanguage understanding. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technolo-\ngies, V olume 1 (Long and Short Papers), 4171–4186.\nHabernal, I.; Wachsmuth, H.; Gurevych, I.; and Stein, B.\n2018. The argument reasoning comprehension task: Iden-\ntiﬁcation and reconstruction of implicit warrants. In Pro-\nceedings of the 2018 Conference of the North American\nChapter of the Association for Computational Linguistics:\nHuman Language Technologies, V olume 1 (Long Papers),\n1930–1940.\nKim, N.; Patel, R.; Poliak, A.; Xia, P .; Wang, A.; McCoy, T.;\nTenney, I.; Ross, A.; Linzen, T.; V an Durme, B.; Bowman,\nS. R.; and Pavlick, E. 2019. Probing what different NLP\ntasks teach machines about function word comprehension.\nIn Proceedings of the Eighth Joint Conference on Lexical\nand Computational Semantics (*SEM 2019), 235–249.\nLai, G.; Xie, Q.; Liu, H.; Yang, Y .; and Hovy, E. H. 2017.\nRace: Large-scale reading comprehension dataset from ex-\naminations. In EMNLP.\nLevesque, H. J.; Davis, E.; and Morgenstern, L. 2012. The\nwinograd schema challenge. In Proceedings of the Thir-\nteenth International Conference on Principles of Knowledge\nRepresentation and Reasoning, KR’12, 552–561. AAAI\nPress.\nLiu, H., and Singh, P . 2004. Conceptne t – a practical\ncommonsense reasoning tool-kit. BT Technology Journal\n22:211–226.\nLiu, N. F.; Gardner, M.; Belinkov, Y .; Peters, M. E.; and\nSmith, N. A. 2019a. Linguistic knowledge and transferabil-\nity of contextual representations. InProceedings of the 2019\nConference of the North American Chapter of the Associa-\ntion for Computational Linguistics: Human Language Tech-\nnologies, V olume 1 (Long and Short Papers), 1073–1094.\nLiu, Y .; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.; Levy,\nO.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V . 2019b.\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692.\nLoBue, P ., and Yates, A. 2011. Types of common-sense\nknowledge needed for recognizing textual entailment. In\nProceedings of the 49th Annual Meeting of the Association\nfor Computational Linguistics: Human Language Technolo-\ngies, 329–334.\nNiven, T., and Kao, H.-Y . 2019. Probing neural network\ncomprehension of natural language arguments. InProceed-\nings of the 57th Annual Meeting of the Association for Com-\nputational Linguistics, 4658–4664.\nPeters, M.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark,\nC.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized\nword representations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies,\nV olume 1 (Long Papers), 2227–2237.\nRadford, A., and Sutskever, I. 2018. Improving language\nunderstanding by generative pre-training.\nRadford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and\nSutskever, I. 2019. Language models are unsupervised mul-\ntitask learners.\nRajpurkar, P .; Jia, R.; and Liang, P . 2018. Know what you\ndon’t know: Unanswerable questions for SQuAD. InPro-\nceedings of the 56th Annual Meeting of the Association for\nComputational Linguistics (V olume 2: Short Papers), 784–\n789.\nTenney, I.; Xia, P .; Chen, B.; Wang, A.; Poliak, A.; McCoy,\nR. T.; Kim, N.; Durme, B. V .; Bowman, S. R.; Das, D.; and\nPavlick, E. 2019. What do you learn from context? prob-\ning for sentence structure in contextualized word represen-\ntations. ArXiv abs/1905.06316.\nTrinh, T. H., and Le, Q. V . 2018. A simple method for\ncommonsense reasoning. CoRR abs/1806.02847.\nWang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and\nBowman, S. 2018. GLUE: A multi-task benchmark and\nanalysis platform for natural language understanding. In\nProceedings of the 2018 EMNLP Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for NLP, 353–\n355.\nWang, C.; Liang, S.; Zhang, Y .; Li, X.; and Gao, T. 2019.\nDoes it make sense? and why? a pilot study for sense making\nand explanation. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics, 4020–\n4026.\nYang, Z.; Dai, Z.; Yang, Y .; Carbonell, J. G.; Salakhut-\ndinov, R.; and Le, Q. V . 2019. Xlnet: Generalized au-\ntoregressive pretraining for language understanding.CoRR\nabs/1906.08237.\nZellers, R.; Bisk, Y .; Schwartz, R.; and Choi, Y . 2018.\nSW AG: A large-scale adversarial dataset for grounded com-\nmonsense inference. InProceedings of the 2018 Conference\non Empirical Methods in Natural Language Processing, 93–\n104.\nZellers, R.; Holtzman, A.; Bisk, Y .; Farhadi, A.; and Choi,\nY . 2019. HellaSwag: Can a machine really ﬁnish your sen-\ntence? In Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, 4791–4800.\nZhu, Y .; Kiros, R.; Zemel, R. S.; Salakhutdinov, R.; Urtasun,\nR.; Torralba, A.; and Fidler, S. 2015. Aligning books and\nmovies: Towards story-like visual explanations by watching\nmovies and reading books. 2015 IEEE International Con-\nference on Computer Vision (ICCV)19–27.\n9740",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7868660688400269
    },
    {
      "name": "Commonsense knowledge",
      "score": 0.7598375082015991
    },
    {
      "name": "Commonsense reasoning",
      "score": 0.6984825134277344
    },
    {
      "name": "Inference",
      "score": 0.6683247089385986
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6323524117469788
    },
    {
      "name": "Natural language processing",
      "score": 0.592355489730835
    },
    {
      "name": "Question answering",
      "score": 0.5772377252578735
    },
    {
      "name": "Reading comprehension",
      "score": 0.5575046539306641
    },
    {
      "name": "Comprehension",
      "score": 0.5388678312301636
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.48931118845939636
    },
    {
      "name": "Language understanding",
      "score": 0.4502694606781006
    },
    {
      "name": "Test (biology)",
      "score": 0.4333546757698059
    },
    {
      "name": "Context (archaeology)",
      "score": 0.4314866364002228
    },
    {
      "name": "Reading (process)",
      "score": 0.4293994605541229
    },
    {
      "name": "Test set",
      "score": 0.4137466251850128
    },
    {
      "name": "Linguistics",
      "score": 0.1468084454536438
    },
    {
      "name": "Knowledge representation and reasoning",
      "score": 0.1434916853904724
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I201448701",
      "name": "University of Washington",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I3133055985",
      "name": "Westlake University",
      "country": "CN"
    }
  ],
  "cited_by": 137
}