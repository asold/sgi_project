{
    "title": "Extrapolation of affective norms using transformer-based neural networks and its application to experimental stimuli selection",
    "url": "https://openalex.org/W4387010034",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5083326050",
            "name": "Hubert Plisiecki",
            "affiliations": [
                "Institute of Psychology",
                "University of Warsaw"
            ]
        },
        {
            "id": "https://openalex.org/A5063273007",
            "name": "Adam Sobieszek",
            "affiliations": [
                "University of Warsaw"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4367858557",
        "https://openalex.org/W2048795844",
        "https://openalex.org/W2434184590",
        "https://openalex.org/W2944797580",
        "https://openalex.org/W3099236585",
        "https://openalex.org/W2167557160",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W1989847457",
        "https://openalex.org/W1974991592",
        "https://openalex.org/W4360836968",
        "https://openalex.org/W2031049446",
        "https://openalex.org/W6767737316",
        "https://openalex.org/W3173298044",
        "https://openalex.org/W2082092349",
        "https://openalex.org/W2996580882",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3085332162",
        "https://openalex.org/W4288233858",
        "https://openalex.org/W2088572966",
        "https://openalex.org/W2069143585",
        "https://openalex.org/W2013871535",
        "https://openalex.org/W2488021640",
        "https://openalex.org/W3206150562",
        "https://openalex.org/W4220757716",
        "https://openalex.org/W3092131673",
        "https://openalex.org/W2045096378",
        "https://openalex.org/W2902888400",
        "https://openalex.org/W2028742638",
        "https://openalex.org/W2886186188",
        "https://openalex.org/W2055706967",
        "https://openalex.org/W1979532929",
        "https://openalex.org/W4306955484",
        "https://openalex.org/W2054566922",
        "https://openalex.org/W6608787115",
        "https://openalex.org/W2051857507",
        "https://openalex.org/W6839328737",
        "https://openalex.org/W4294170691",
        "https://openalex.org/W2798357113",
        "https://openalex.org/W1989192652",
        "https://openalex.org/W2043632943",
        "https://openalex.org/W2997049449",
        "https://openalex.org/W2950974174",
        "https://openalex.org/W6667890209",
        "https://openalex.org/W2952638691",
        "https://openalex.org/W2124217660",
        "https://openalex.org/W1999609000",
        "https://openalex.org/W6604617241",
        "https://openalex.org/W2559779584",
        "https://openalex.org/W2997159780",
        "https://openalex.org/W2051695946",
        "https://openalex.org/W3164401243",
        "https://openalex.org/W2512194521",
        "https://openalex.org/W1964081251",
        "https://openalex.org/W4225411436",
        "https://openalex.org/W1999081314",
        "https://openalex.org/W2290759784",
        "https://openalex.org/W2132683166",
        "https://openalex.org/W2250508463",
        "https://openalex.org/W2965210982",
        "https://openalex.org/W3046437464",
        "https://openalex.org/W3203904456",
        "https://openalex.org/W2115098571",
        "https://openalex.org/W2902591385",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2981925679",
        "https://openalex.org/W2074739635",
        "https://openalex.org/W2023736093",
        "https://openalex.org/W2013536305",
        "https://openalex.org/W2502534556",
        "https://openalex.org/W2513026920",
        "https://openalex.org/W2587019100",
        "https://openalex.org/W2944851425",
        "https://openalex.org/W4235646343"
    ],
    "abstract": "Abstract Data on the emotionality of words is important for the selection of experimental stimuli and sentiment analysis on large bodies of text. While norms for valence and arousal have been thoroughly collected in English, most languages do not have access to such large datasets. Moreover, theoretical developments lead to new dimensions being proposed, the norms for which are only partially available. In this paper, we propose a transformer-based neural network architecture for semantic and emotional norms extrapolation that predicts a whole ensemble of norms at once while achieving state-of-the-art correlations with human judgements on each. We improve on the previous approaches with regards to the correlations with human judgments by Δ r = 0.1 on average. We precisely discuss the limitations of norm extrapolation as a whole, with a special focus on the introduced model. Further, we propose a unique practical application of our model by proposing a method of stimuli selection which performs unsupervised control by picking words that match in their semantic content. As the proposed model can easily be applied to different languages, we provide norm extrapolations for English, Polish, Dutch, German, French, and Spanish. To aid researchers, we also provide access to the extrapolation networks through an accessible web application.",
    "full_text": "Vol:.(1234567890)\nBehavior Research Methods (2024) 56:4716–4731\nhttps://doi.org/10.3758/s13428-023-02212-3\n1 3\nExtrapolation of affective norms using transformer‑based neural \nnetworks and its application to experimental stimuli selection\nHubert Plisiecki1 · Adam Sobieszek2\nAccepted: 30 July 2023 / Published online: 25 September 2023 \n© The Author(s) 2023\nAbstract\nData on the emotionality of words is important for the selection of experimental stimuli and sentiment analysis on large \nbodies of text. While norms for valence and arousal have been thoroughly collected in English, most languages do not have \naccess to such large datasets. Moreover, theoretical developments lead to new dimensions being proposed, the norms for \nwhich are only partially available. In this paper, we propose a transformer-based neural network architecture for semantic and \nemotional norms extrapolation that predicts a whole ensemble of norms at once while achieving state-of-the-art correlations \nwith human judgements on each. We improve on the previous approaches with regards to the correlations with human judg-\nments by Δr = 0.1 on average. We precisely discuss the limitations of norm extrapolation as a whole, with a special focus \non the introduced model. Further, we propose a unique practical application of our model by proposing a method of stimuli \nselection which performs unsupervised control by picking words that match in their semantic content. As the proposed model \ncan easily be applied to different languages, we provide norm extrapolations for English, Polish, Dutch, German, French, \nand Spanish. To aid researchers, we also provide access to the extrapolation networks through an accessible web application.\nKeywords Affective norms · Transformer-based neural network · Semantic extrapolation · Emotional norms extrapolation · \nExperimental stimuli selection · Valence and arousal\nAffective norms of words have various applications across \npsychology, linguistics, and machine learning. Their impor-\ntance is evidenced by the large number of use cases they \nenjoy. They have been used to select stimuli for experiments \nin social and affective psychology to investigate behavior \n(Crossfield and Damian, 2021), to study clinical popula -\ntions (Williamson et al., 1991; Sloan et al., 2001), and as \ncorrelates of brain activity (Citron, 2012; Imbir et al., 2022; \nKanske & Kotz, 2007; Yao et al., 2016). Together with norms \nof semantic dimensions they serve as tools for the study of \nlexical semantics, concerned with how concepts may be rep-\nresented in the brain (Binder et al., 2016). Recently, seman-\ntic and affective norms have seen a surge in popularity with \nthe growing interest in machine learning, where they have \nbeen used to train automatic classifiers of, for example, the \nsentiment expressed in a given piece of text (Nielsen, 2011). \nAll such uses rely on databases of word – norm pairs, where \nnorms are calculated based on human ratings of the word \non a particular dimension of interest (e.g., how positive, or \nnegative a given word is, a technique dating back to the work \nof Osgood et al., 1957). To this end, measurement scales \nfor various lexical affective constructs have been developed, \nstarting with the simple Likert scale, and continuing with the \npopular self-assessment manikin of Bradley and Lang (1994).\nThe two most popular approaches in creating emotional \nnorms include either rating words on emotional dimen-\nsions or their association with discrete emotion categories. \nThe most popular of these dimensions in the first approach \ninclude valence, arousal, and dominance (Bradley & Lang, \n1999; Imbir, 2015; Sianipar et al., 2016; Söderholm et al., \n2013; Stadthagen-Gonzalez et al., 2017; Verheyen et al., \n2020; Warriner et al., 2013; Yao et al., 2017) and to a lesser \nextent other dimensions which may modulate emotional \nprocessing, such as concreteness, age of acquisition, sub-\njective significance, or origin of emotional load (Brysbaert \net al., 2014a, b; Imbir 2016; Kuperman et al., 2012). As for \ndiscrete emotional categories, norms are usually concerned \n * Hubert Plisiecki \n hplisiecki@gmail.com\n1 Institute of Psychology, Polish Academy of Sciences, SWPS \nUniversity of Warsaw, Warsaw, Poland\n2 Faculty of Psychology, University of Warsaw, Warsaw, \nPoland\n4717Behavior Research Methods (2024) 56:4716–4731 \n1 3\nwith subsets of the six basic emotions: fear, anger, joy, sad-\nness, disgust, and surprise (Mohammad, 2018; Stevenson \net al. 2007). The affective norms have been published for \nmany languages other than English (Bradley & Lang, 1999): \nFrench (Syssau et al., 2021), German (Võ et al., 2009), \nSpanish (Redondo et al., 2007), Dutch (Moors et al., 2013), \nPolish (Imbir, 2016), Turkish (Kapucu et al., 2021), Italian \n(Montefinese et al., 2014), Portuguese (Soares et al., 2012), \nGreek (Vaiouli et al., 2023), and Chinese (Yao et al., 2017).\nSome applications of affective norms, such as complex \nexperimental designs, demand very large datasets, the creation \nof which can be prohibitively expensive. This demand has been \npartially satisfied for English words, with the expansion of the \nclassic Affective Norms for English Words database (ANEW; \nBradley & Lang, 1999) from 1035 to 13,915 by Warriner et al. \n(2013). However, such large dataset expansions are still una-\nvailable for many languages. Moreover, even in English, the \nexisting norms may not be enough for certain types of studies, \nwhich could require norms for all English words. Interesting \nexamples of such include analyses of large-scale trends and \nshifts in the use of language across thousands – if not millions \nof texts (Kim & Klinger, 2019), where an accurate assessment \nmay require a rating for each word to avoid bias (Snefjella & \nBlank, 2020). It is in this context that lexical norm extrapola-\ntion techniques start to be developed, as they allow research-\ners to use existing norms to expand the database lexicon by \npredicting the norms of previously unrated words.\nAffective norms extrapolation\nHow does one go about determining the emotionality of words \nwithout any human judgment information? A first intuition \nmay be to say that the emotional load of a word can be approx-\nimated with the emotional load of another, similar, word for \nwhich we possess affective norms. This indeed turns out to be \nthe basis for most published norm extrapolation techniques, \nthe difference being mostly in the level of sophistication with \nwhich the similarity metrics are defined and the introduction \nof ways to decrease noise by averaging across many similar \nwords. A popular source of similarity metrics comes from the \nlinguistic distributional hypothesis, which states that words \nthat occur in similar contexts tend to have similar meanings \n(Boleda, 2020). Thus, the dominant approach in affective \nnorms extrapolation is usually to average a norm of interest \nacross a word's k-nearest neighbors based on a co-occurrence \nmetric, which the review by Mandera et al. (2015) deemed \nthe most effective method as of 2015. An early example of \nsuch an approach includes Bestgen and Vincze’s (2012) use of \nlatent semantic analysis to derive similarities between words, \nwhere co-occurrence is calculated from paragraphs of large \nlanguage corpora. While varying the number of neighbors to \naverage across, they found the highest correlations between \nhuman ratings and their estimates to be r = 0.71 for valence, \nr = 0.56 for arousal, and r = 0.60 for dominance.\nMore recent approaches used in machine learning for \nsentiment analysis employ similarity metrics based on dis-\ntances in a vector space, where words are represented as \npoints (e.g., Munikar et al., 2019). These spaces, called word \nembeddings, are intended to be lower-dimensional represen-\ntations of the relationships between the words in language \ncorpora and are created in various ways, which include \ndimensionality-reduction techniques on the co-occurrence \nmatrix (e.g., multidimensional scaling) and the use of neural \nnetworks (e.g., in word2vec; Mikolov et al., 2013).\nA different approach has been employed (to great effect) \nby Vankrunkelsven et al. (2015). Their method involves using \na vast dataset of word associations (De Deyne et al., 2013), \nwhich are based on 70,000 participants reporting their three \nassociations with one of 12,000 cue words. Since free asso-\nciations are often based on semantic relationships with the \ncue word, these data can be used to construct a great simi-\nlarity metric. Vankrunkelsven et al. used multi-dimensional \nscaling to construct word embeddings based on this data and \nachieved correlations of r  = 0.89, r  = 0.76, r  = 0.77, r  = \n0.67, and r = 0.81, for valence, arousal, dominance, age of \nacquisition, and concreteness, respectively. As semantic norm \nextrapolation is most useful for languages where access to \nsuch data is limited, the need to collect vast word association \ndata to perform norm extrapolation seems, while elegant, to \nbe of limited practical utility. Still, a comparison by Vankrun-\nkelsven et al. (2018) of their association-based method with \nprevious methods based on co-occurrence shows that their \nmethod achieved state-of-the-art results for the time.\nA challenge for all such extrapolation methods, recently \npresented by Snefjella and Blank (2020), posits that \nresearchers may, however, be overestimating the accuracy \nof their methods of norm extrapolation by relying on cross-\nvalidation to evaluate performance. This is because the \nwords that are missing from the norm databases (in which \nwe are ultimately interested in extrapolation) are not missing \nat random from all possible words. They suggest considering \nnorm extrapolation as missing data imputation.\nAdvances in neural network‑based language \nmodels\nIn recent years, the field of computational linguistics has been \ntaken by storm with rapid developments in neural network-\nbased models. especially large language models, one of the \nmost notable ones is called GPT-3 and was trained on a corpus \nof text comprising nearly 500 billion words (Brown et al., \n2020). The performance of this model varies, as it has been \nshown to perform with near human ability on many high-level \n4718 Behavior Research Methods (2024) 56:4716–4731\n1 3\ntasks like imitating an author or waxing philosophical while \nfailing at several very simple tasks like multiplying large num-\nbers (Elkins & Chun, 2020; Sobieszek & Price, 2022). It is, \nhowever, worth stressing that on many occasions its perfor-\nmance is indistinguishable from that of a human and that this \nhigh performance is not merely the product of the sheer size of \nthe training dataset. GPT-3, as well as its predecessors GPT-2 \nand GPT, utilize a specific machine learning architecture \ncalled attention, which allows them to attend to many distant \nwords at once, thus being able to grasp complicated contex-\ntual information when analyzing text (Brown et al., 2020).\nThese developments lie in contrast to the previous attempts \nat text classification, translation and production in natural \nlanguage processing, as previous models were very limited \nin their scope when it comes to attending to distant words. \nThe earliest approaches utilized the already-mentioned word \nembeddings, which were heavily dependent on closely co-\noccurring words, and thus were unable to capture relations \nbetween distant signs (Almeida & Xexéo, 2019). The situa-\ntion improved with the rise of recurrent neural networks such \nas LSTMs (Long Short-term memory modules) which tried \nto retain significant textual information as they gradually \nadvanced through the lines of text (Yu et al., 2019). Unfortu-\nnately, these approaches were plagued by the problem of van-\nishing gradients, resulting in the retained information being \nlost over time as the activation progressed through the net-\nwork (Hochreiter, 1998). They were therefore short-sighted. \nAfterwards came convolutional networks (CCN), which com-\npressed contextual information using sliding windows, thus \ncapturing their contents (Yin et al., 2017). These had a differ-\nent flaw, however, as to capture complex contextual informa-\ntion one had to apply very large sliding windows (buffers for \nword compression), and many of them – which was incredibly \ncostly in terms of computational power (Vaswani et al., 2017).\nFinally, the concept of “attention” was introduced, which is in \nsimple terms the process of weighing the inputs to a neural layer \nwith the use of trainable weights. This method was first applied \nin recurrent neural network-based sequence-to-sequence mod-\nels, which iteratively passed the generated sequence through a \ngenerator module to obtain the next word, appended the word to \nthe generated sequence and repeated the process until the whole \nsequence of interest was generated. The real breakthrough, how-\never, came when the recurrent network architecture was replaced \nby attention. This feat, achieved by Vaswani et al. (2017), mor-\nphed into a family of models called transformers, some among \nwhich are BERT, RoBERTa, and XLM (Devlin et al., 2018; \nConneau & Lample, 2019; Liu et al., 2019).\nTransformer models consist of two modules, an encoder, \nand a decoder. As this architecture was primarily designed \nfor the task of language translation, we will use the task of \nlanguage translation as a reference when explaining its \nmechanism. In simple terms, a sentence in the 1st language \nis given to the encoder, which transforms it into a numerical \nrepresentation using attention and feedforward layers. At the \nsame time, a similar thing happens in the decoder, where a cor-\nresponding sentence in the 2nd language (with blank “masks” \ninstead of the words that the architecture is meant to predict) is \nalso transformed into a numerical representation, using simi-\nlar transformations. Then, the output of the encoder is passed \nto the decoder, where it is concatenated with the numerical \nrepresentation of the L2 sentence and together they are passed \nthrough additional attention and feedforward layers. The final \noutput is compared with the intended output, and the weights \non each of the layers are updated to minimize the error between \nthe two (Devlin et al., 2018). Once the model is trained, the \nencoder can be extracted from the model and be used to obtain \nrich, contextual text embeddings that can be used for further \ntraining (Munikar et al., 2019). The usefulness of such text \nembeddings stems from their ability to quantitatively describe \nthe embedded text on meaningful dimensions the model dis-\ncovered during training. A common example is the ability to \nuse the vector in the embedding space corresponding to a given \nword as a direction in which one may manipulate the embed-\nding of another word, e.g., find the representation of the word \n‘man’ by subtracting the representation of ‘royal’ from that of \nthe word ‘king’ (Ethayarajh, 2019).\nIn summary, the evolution of computational linguistics \nhas led to the development of attention-based transformer \nmodels, such as GPT-3, which outperform their predeces-\nsors such as LSTMS and CCNS in processing distant words \nin text, with their high performance attributed not only to \nthe vast training datasets but also to their ability to retain \ncomplex contextual information, a characteristic lacking in \nearlier models due to issues like vanishing gradients and \ncomputational costs.\nWord stimuli selection\nWe know from various studies of word processing that a \nmultitude of factors by which we can describe words influ-\nence neural and behavioral responses in experiments using \nwords as experimental stimuli. These include accessible \nfeatures like length and frequency in the language (Hauk & \nPulvermüller, 2004; Kuchinke, et al., 2007; Méndez-Bértolo \net al., 2011), but also features which are not easily acces-\nsible, such as differences in semantic features and emotive \ncontent (e.g., abstractness, valence, arousal; see Citron, 2012 \nfor a review). Here, emotional databases are an important \nasset, as they enable word stimuli selection for experimental \nmanipulation and control of such factors. There however \nremain challenges to valid stimuli selection based on avail-\nable datasets. The first stems from recent striking results that \neven newly discovered emotional dimensions can influence \nbehavior with effect sizes comparable to those previously \nreported in the literature for valence and arousal (e.g., origin \n4719Behavior Research Methods (2024) 56:4716–4731 \n1 3\nand subjective significance in experiments of Imbir et al., \n2020, 2021, 2022), as well as known factors which were \nuntil recently not controlled in such studies (e.g., concrete-\nness in experiments of Kanske & Kotz, 2007). The existence \nof such unaccounted-for dimensions may explain the dispar-\nity of reported results on the influence of emotional factors \non behavior (such as those in the review by Citron, 2012) \nespecially when stimuli lists are short and selected from a \nlimited dataset. For this reason, a method of stimuli selection \nthat would more likely produce valid stimuli lists may need \nto somehow reduce the influence of these unknown factors.\nWhen constructing a manipulation of, for example, valence \nusing emotional norms, we pick sets of negative and positive \nwords that do not differ on some control dimensions (e.g., \nlength, frequency, and arousal). To add an element of unsu-\npervised control (control of unspecified factors), we propose \nto additionally perform semantic matching between these con-\nditions, which involves selecting words for these groups con-\ntaining words paired on their semantic features while differing \nin the manipulated factor. Words similar in meaning and used \nin similar contexts are more likely to have similar values on \ndimensions we did not explicitly control, such as imageability, \nthan a random word pairing. An example of such a pairing \nmay be the positive “peaceful”, with the negative “boring”. \nBoth have low arousal, but even more importantly, both are \napproximately matched in their semantic content and connota-\ntions, while differing in the sign of the emotion attributed to \nthe situation. This is indeed an example of a pairing found by \nour stimuli descent algorithm, which is introduced in Study 3.\nStudy 1: Transformer‑based norm \nextrapolation\nWe hypothesize that the use of highly contextual represen-\ntations of words as input to a model trained to predict the \nemotional norms will be able to outperform the previous \napproaches in norm extrapolation. While some of the previ-\nous attempts at this task also relied on machine learning to \nextend affective norms, they all relied on word embeddings \n(e.g., Mandera et al., 2015) and thus were unable to capture \nthe sophisticated contextual relations among distant words. \nFurthermore, contrary to word embeddings, the numerical \nrepresentations obtained from transformers are flexible with \nregard to the task that they are trained on. For example, a \ntransformer can be first trained to simply generate sentences \nbut then retrained on a different, more specialized task such \nas emotion recognition in Twitter posts. This retraining will \nlead to slight changes in the numerical representations gen-\nerated by the transformer, as the emotional information pre-\nsent in the relation between Twitter posts and their training \nlabels (e.g., happy, sad etc.) will seep into the weights of the \nmodel, crystallizing specialized affective knowledge. A good \nexample of such a model is ERNIE, which was trained on \nseveral different tasks and achieved state-of-the-art results on \nseveral NLP benchmarks at the time of publication (Yu et al., \n2019). The use of models pre-trained on emotion recognition-\nrelated tasks should therefore further increase the perfor -\nmance of our approach. Additionally, since the transformer \nmodels have been trained for many different languages, our \nmodels will help researchers from different countries to \nextrapolate their norm datasets cheaply and accurately.\nIn the manuscript, we first provide a detailed description \nof the norm datasets that will be used to train the model. \nAfterwards, we detail the model’s architecture explaining \nthe intuition behind choosing the right transformer module \nfor the task. Then we describe the training regimes and pre-\nsent results comparing the outputs of the models to a spe-\ncially designated part of the original datasets, ending with \na discussion.\nMethod\nLinguistic materials and data curation\nIn the current study, we make use of the ANEW corpus \n(Bradley & Lang, 1999), and a corpus collected by Warriner \net al. (2013) to train and test our model. The former consists \nof 1030 words with accompanying rater-based metrics for \nvalence and arousal. The latter has 13,915 words with both \nprevious metrics and, additionally, the age of acquisition and \nconcreteness metrics. All the metrics have been normalized \nto range from 0 to 1. In line with the argumentation from \nprevious work on these corpora, we used the ANEW words \nfor the test set, subtracting them from the training set com-\nposed of all the words present in Warriner’s database. The \ntest set, therefore, consists of 983 words. The rest of War -\nriner’s corpus (12,885 words) was divided into training and \nvalidation sets at a 9-to-1 ratio Table 1 and 2.\nTo expand our model to other languages, we make use of \nfive additional datasets. For the Polish language, we employ a \nnorm repository of 4900 words (Imbir, 2016). For Spanish, the \ndataset contains 1400 words (Redondo et al., 2007). For Ger-\nman we use the BAWL-R dataset with 2902 words (Võ et al., \n2009). For French we use the FANCat dataset with 1031 words \n(Syssau et al., 2021) Finally, we employ norms for 4299 words \nin the Dutch language (Moors et al., 2013). Unfortunately, all \nof the same metrics were not available for all of the different \nlanguages. While fewer dimensions were available for Spanish \nand Dutch, the contrary was true for Polish, where we were able \nto use an eight-metric database. The availability of the metrics \nin each of the languages can be checked in Table 3. The metrics \nwere normalized to the 0 to 1 range, and the datasets were split \nfor training, validation and testing according to the 8:1:1 ratio.\n4720 Behavior Research Methods (2024) 56:4716–4731\n1 3\nModel architecture\nThe proposed model maintains the same architecture across \nall languages and norms, with the only variation being the \ntransformer embedding model used for each language. As \ntransformer models are usually trained for singular lan-\nguages at a time, we cannot use a model that uses all lan-\nguages at the same time. Beyond issues of accuracy, this \ncould pose troubles related to the differences in norms \nbetween different languages (Pires et al., 2019). To facili-\ntate the use of our architecture in new languages, the selec-\ntion of the embedding model is explained in Appendix 1. \nWe only use the encoder from the transformer model as the \nbase encoding layer for our model, and we build additional \nlayers on top of it. The added layers consisted of a single \nfully connected layer with layer normalization and another \nlayer with one number as its output. On top of that, we have \napplied a sigmoid activation function, which ensures that \nthe output of our model yields a normalized value between \n0 and 1. This type of regression head was added for each of \nthe predicted metrics.\nBecause models of similar infrastructure can be trained \nfor different languages given enough training data, they can \nbe fully substituted for each other, and similar models can \nbe trained using them. This makes the proposed architec-\nture versatile and open to being implemented in different \nlanguages. Transformer models have already been trained \nin many different languages and are freely available online \n(Hugging Face, n.d.). Therefore, for most of the languages, \nthe only thing needed to prepare a similar model is a dataset \nwith affective measures.\nHowever, choosing the right transformer for the base \nof the model is not as straightforward. Wherever possible, \nwe have opted for models that either had more parameters \nand could therefore model language more accurately, or \nwere pre-trained on emotion recognition tasks. However, \nthe scope of our search was limited by both hardware con-\nstraints and model availability. Since different transformer \nmodels are pre-trained on various tasks, their performance \non a specific task like ours may vary. If researchers want to \ntrain a similar model for a language that is not covered in \nour article, we advise them to run tests using all the different \ntransformer models available in their language, until they \nfind the one that rears the best predictions (see Appendix 1 \nfor more information).\nThe specifications for each of the models are shown in \nTable  1. The hyperparameters for our machine learning \nmodel were chosen according to common practices, wherein \nwe used a combination of domain knowledge, model com-\nplexity considerations, and computational efficiency to guide \nthe selection, minimizing the risk of overfitting and ensuring \noptimal performance.\nTable 1  The specification details of the different models\nSpecifications English Polish Spanish Dutch German French\nTransformer encod-\ners\nERNIE 2.0 \n(Yu et al., \n2019)\nRoBERTa-\nPolish (Dadas, \n2020)\n“bert-base-spanish-\nwwm-cased” \n(Perezrojas et al., \n2020)\n“bert-base-dutch-\ncased” (de Vries \net al., 2019)\n\"bert-base-german-\nuncased\" (von \nPlaten, 2021)\n“french_toxicity_\nclassifier_plus_\nv2” (Stakovskii, \nn.d.)\nMean number of \nraters\n28 50 21 63 Not reported 36\nNumber of words 13,915 4900 1400 4299 2902 1031\nLearning rate 5e-5 5e-4 5e-4 5e-4 5e-4 5e-4\nDropout 0.1 0.2 0.2 0.2 0.1 0.1\nTable 2  Correlation results for the past extrapolation models\nThe best results for a certain metric are in bold. Lack of prediction for a certain metric is signified by a dash\nStudy Valence Arousal Dominance Concreteness Age of \nacquisi-\ntion\nCurrent Study 0.95 0.76 0.86 0.95 0.85\nVankrunkelsven et al., 2018 0.86 0.69 0.75 0.87 0.59\nVankrunkelsven et al., 2015 0.89 0.76 0.77 0.81 0.67\nMandera et al., 2015 0.69 0.60 0.48 0.80 0.72\nRecchia and Louwerse, 2015 0.74 0.75 0.62 - -\nBestgen and Vincze, 2012 0.71 0.56 0.60 - -\n4721Behavior Research Methods (2024) 56:4716–4731 \n1 3\nEach of the models was trained for 1000 epochs with \nearly stopping (stopping the training before the model starts \noverfitting the training data). This was implemented by sav-\ning the model that had the best correlations with the valida-\ntion metrics. We used the AdamW optimizer algorithm with \nan epsilon value of 1e-8, a weight decay of 0.3, amsgrad, and \nbetas equal to (0.9, 0.999). Additionally, we implemented \na warmup algorithm, which gradually elevated the learn -\ning rate for 600 learning steps, when it reached the maxi-\nmum number, and then slowly lowered it, until the end of \nthe training. The rest of the specifications like the learning \nrate and the value of the dropout can be found in Table 1 as \nit was language specific.\nResults and discussion\nThe English model’s predicted affective norms achieved the \nfollowing Pearson correlations with human judgements on \nwords from the test set: valence: r = 0.95, arousal: r = 0.76, \ndominance: r = 0.86, age of acquisition: r = 0.85, concrete-\nness: r = 0.95, with an overall loss of 0.003. When compared \nto the previous methods (see Table 3), the present approach \nachieves the highest accuracy across all variables. This is \ntrue even compared to Vankrunkelsven et al. (2018) results \nafter they have been adjusted for attenuation (r = 0.91, r \n= 0.83, and r = 0.85 for valence, arousal, and dominance \nrespectively). The transformer-based model has therefore \nbeen shown to achieve higher accuracy when compared to \nthe extrapolations reported based on LSA, and other word \nembedding methods (Bestgen & Vincze, 2012; Recchia and \nLouwerse, 2015), to methods based on human word asso-\nciation data (Vankrunkelsven et al., 2015), those based on \nsimple machine learning methods (Mandera et al., 2015 ), \nas well as those combining the last two (Vankrunkelsven \net al., 2018). It is worth pointing out that direct comparison \nwas not always possible as the past models did not utilize \nthe same high-quality validation set – the ANEW corpus. \nHowever, the improvement over those that did use it is so \nbig that the change in the test set most probably would not \nchange the overall conclusion.\nGiven the very high observed correlations we can compare \ntheir values to the theoretically highest correlation values we \ncan expect for the norms of the test set. The uncertainty asso-\nciated with a prediction may be broken down into epistemic \nand aleatoric uncertainty. The former concerns the model \nshortfall that may be amended with better models, the latter \nconcerns the uncertainty inherent to the studied phenomena. \nFor norms, we may estimate the aleatoric uncertainty from the \nreported standard error associated with the number of raters \nand the variance of their judgments. With this information \nwe can calculate the limit on prediction performance, which \nis reported in Fig.  1. In the next section, we discuss other \nsources of aleatoric uncertainty that limit extrapolation per-\nformance. Given these results, along with improvements of \naround Δr = 0.1 on every metric, we can safely assume that \nthe transformer model constitutes the current state of the art \nin norm extrapolation. Furthermore, due to the high popu-\nlarity of transformers, the current architecture can be easily \nadapted to different languages. This is evidenced by the results \nachieved on the subsets of Polish, Spanish, German, French, \nand Dutch words, most of them being very high (see Table 2). \nThe ease with which the current approach can be adapted to \nextrapolate word norms in different languages is an improve-\nment on the previous methods, most of which relied on \nhuman-based word associations data (Vankrunkelsven et al., \n2015, 2018), which is not freely available for most languages.\nThanks to its high accuracy, the current model can be \nused to provide approximations of human judgements in \ncontexts where the actual norm values do not need to be \nknown with precision. The extrapolated norms should not \nbe used as independent variables in linguistic studies of \nlarge corpora. As we will investigate in the next section, the \npredictions could be biased towards uncommon words that \nTable 3  Correlation results of affective metrics from the three additional languages on the test set\n* p < 0.05** p < 0.01*** p < 0.001\nLack of the prediction of a certain metric is signified by a dash\nAffective metric English Polish Spanish Dutch German French\nValence 0.95*** 0.93*** 0.89*** 0.87*** 0.8*** 0.8***\nArousal 0.76*** 0.86*** 0.80*** 0.80*** 0.7*** 0.77***\nDominance 0.86*** 0.92*** - 0.75*** - -\nConcreteness 0.95*** 0.95*** 0.89*** - - -\nAge of Acquisition 0.85*** 0.81*** - 0.82*** - -\nOrigin - 0.86*** - - - -\nSignificance - 0.88*** - - - -\nImageability - 0.88*** 0.86*** - 0.82*** -\nFamiliarity - - 0.71*** - - -\n4722 Behavior Research Methods (2024) 56:4716–4731\n1 3\nappear in corpora but are unlikely to be used in experimental \nresearch. In cases where precision is needed, the model can \nstill serve as a useful heuristic tool to identify words with \nspecific values, which can be in turn verified experimentally. \nFor example, the model can help identify words whose spe-\ncific combination of affective metrics are rare, like those \nwith neutral valence and high arousal, to help populate spe-\ncific stimuli sets, as such words are useful in studies which \ntry to orthogonally manipulate emotional dimensions.\nThe possibility of using transformer-based extrapolation \nfor the task of finding rare emotion-combination words is \nimportant, as k-nearest neighbor approaches often generalize \nworse in sparse regions, such as in these specific configu -\nrations of affective dimensions. If this generalization per -\nformance is confirmed it would establish a heuristic search \nfor low-density words and extrapolation for unusual, low-\nfrequency words as unique use cases of our method. How -\never, we need to tackle a significant issue related to measur-\ning extrapolation performance, highlighted by Snefjella and \nBlank (2020). This problem becomes known when we view \nnorm extrapolation as a missing data problem. In short, the \nuse of supervised learning to impute missing data, condi -\ntional on observed data is equivalent to single regression \nmean imputation, which is an imputation method known \nin the field of causal inference to produce biased estimates \nof accuracy via cross-validation (Van Buuren & Groothuis-\nOudshoorn, 2011). What Snefjella and Blank (2020) rightly \npoint out is that the set of words that do appear in norms \ndatabases is not random nor representative of all words in \na language, creating a “missing not at random” problem in \nnorm extrapolation. Words that are longer, less common, or \nmore abstract all have a lower probability of appearing in \nnorms databases, thus also the test set, resulting in accuracy \nbias. In Study 2 we use these insights to test how biased is the \ncross-validation estimate of accuracy, as well as how good \nour transformer-based method is at prediction generalization \non different emotional dimensions. To this end, we conduct \ntests both with normative and new experimental data.\nThe results of this section, while showing promise in the \nability of the model to generalize to unseen words, suggest \nit cannot overcome the issues with norm extrapolation to all \nwords highlighted by Snefjella and Blank (2020). For a large \nnumber of words, the large aleatoric uncertainty in norms and \nthe systematic bias from mean imputation largely coincide, \ncausing an irreducible error that prohibits valid prediction \nwith ad hoc methods. Additionally, the systematic bias in \nextrapolated norms can hide some complex relationships \nbetween the examined variables, which prohibits the use of \nextrapolated norms in corpora studies and demands research-\ners experimentally verify the norms in orthogonal designs.\nStudy 2: Evaluating robustness \nin out‑of‑distribution prediction\nThe goal of this section is to test how well the transformer-\nbased extrapolation method generalizes under selection bias \nand assess which words' prediction is affected by the “missing \nnot at random” problem. Recall that prediction uncertainty \nmay be divided into epistemic and aleatoric uncertainty. The \nformer is associated with the quality of the model, and the \nlatter with the variability inherent to the studied phenomena. \nThe use of norm extrapolation should be limited to words for \nwhich the prediction error is small – first to words that have \nperfect estimator (indicated by the end of the red bar). The length of the red bar indicates the model \nshortfall.\nNote: Average cross-validated model performance (black bars) compared the expected correlation for a \nFig. 1  The comparison of the accuracy achieved by the model to the \nperfect estimator correlations. Note:  Average cross-validated model \nperformance (black bars) compared the expected correlation for a \nperfect estimator (indicated by the end of the red bar). The length of \nthe red bar indicates the model shortfall\n4723Behavior Research Methods (2024) 56:4716–4731 \n1 3\nlow aleatoric uncertainty (for which prediction is possible), \nand next to words for which the epistemic uncertainty is low, \nwhich depends on the extrapolation method. Assuming cross-\nvalidation performance applies to all words runs into discount-\ning both the missing not at random problem and the existence \nof words for which the concept captured by the norm does \nnot apply the same way as to words in the dataset. To under-\nstand the latter point, take the norms for age of acquisition. To \nobtain the norms, Kuperman et al. (2012) asked participants \nto answer at what age they thought they had learned a given \nword. However, more than 50% of the words were not known \nby all respondents, which for this measure would imply the age \nof acquisition was larger than the age of the participant making \nthe norm calculated only on respondents that knew the word \nbiased downwards. Here, therefore, we encounter the first limi-\ntation of our method, as age of acquisition norm extrapolation \nshould not be used for large values. We conduct three tests for \nother metrics to assess the two sources of additional predic-\ntion error: (a) stemming from the larger aleatoric uncertainty \nof words unlikely to appear in the test set (b) stemming from a \nlarger epistemic uncertainty in out-of-distribution prediction.\nWe start by testing whether our method produces biased \nresults under a meaningful selection bias. Abstract and con-\ncrete emotional words are processed differently by people, \nmediating the effects of valence and arousal on reaction times \nand the neural response (Kanske & Kotz, 2007). Concreteness \nis thus a good candidate for a factor that may bias results if \nselected not at random. We test the robustness of our method \nto additional systematic sampling bias by predicting norms of \nabstract words using a model trained only on concrete words. \nNext, we take advantage of English concreteness norms exist-\ning for a much larger set of words than the set of words in the \ntest and training sets and establish how accuracy decreases for \nwords known by fewer people. Lastly, we collect additional \nnorms for words chosen entirely at random to find a lower \nbound for unbiased accuracy across the entire language.\nMethod\nDesign and linguistic materials\nTo test the generalization performance of our method, which \nthe extrapolation methods need for accurate out-of-distribu-\ntion prediction, we train a model within an artificially imposed \nselection bias. The English corpus used to train the original \nEnglish model was resampled to include only words with \na concreteness value above the center of the distribution, \ncorresponding to a prediction of 0.5 in the original model \n(where low values are abstract), leaving 6307 words for train-\ning. We then constructed two test sets. The first included only \nhighly abstract words (with concreteness < 0.5; 364 words), \nsimulating the prediction of norms outside the database. The \nsecond contained a randomly selected test set from all words, \nmatched on word length with the former. The models were \ntrained in the exact same way as the English model in Study 1.\nThe norms for abstractness were taken from the dataset of \nKuperman et al. (2012), which has two important features: (a) \nit contains an extremely large, compared to other databases, \nselection of 40,000 English lemmas, (b) the authors report the \nproportion of participants that knew the word. In the dataset, \nthere were 27,000 single-word lemmas. We test whether there \nis a drop in accuracy compared to the accuracy calculated with \ncross-validation on words from Warriner et al. (2013). Second, \nwe test how the accuracy decreases when decreasing the amount \nof people familiar with the word. General familiarity is strongly \nassociated with the probability of being included in normative \ndatabases, as unknown words are not only hard to obtain norms \nfor but also are not useful for experimental studies.\nFor the experimental validation, a set of completely ran-\ndom Polish words conditional on not appearing in normative \nnorms database was created. First we gathered a list of all \nwords that appeared at least five times in Polish language \ncorpora (following Kazojć, 2011) and obtained a set of \n31,967 words. From this set, we have randomly drawn 200 \nwords. The first 150 words that did not appear in the polish \nnorms database were chosen to be rated on five dimensions \n– valence, arousal, dominance, imageability, origin – 30 dif-\nferent words per dimension. The last dimension is unique to \nthe Polish norm dataset and refers to the origin of emotional \nload from either more automatic or reflective processes. The \nchoice to rate a different set of words for each dimension \nwill bar us from inferring which dimension suffers the most \nfrom out-of-distribution prediction, but it will give us a more \naccurate estimate of the performance drop-off for all dimen-\nsions (as the same words for each dimension would make the \ndrop-off more of a function of the particular word selection).\nParticipants\nThe validation study included 89 Polish-speaking partici-\npants (47 women, 42 men). The participants' mean age was \n22.6 (SD = 4.1). The study was promoted on Facebook, \nspecifically targeting college students, as the original ANEW \nPolish norms (Imbir, 2016) were rated by students from this \ndemographic. To stimulate participation, we held a drawing \nfor a 50 PLN reward, which participants were eligible for \nupon completion of the study. Ratings from 66 participants \n(50% male) entered into the analysis after removing partici-\npants whose answer’s reliability was smaller than 0.8.\nProcedure\nThe study was conducted through Qualtrics. We aimed to rep-\nlicate the most relevant aspects of the procedure used in the \n4724 Behavior Research Methods (2024) 56:4716–4731\n1 3\nnormative study by Imbir (2016). Each participant was given \ntwo emotional dimensions to assess. Before each dimension \nstarted, participants read a detailed description of the dimen-\nsions and saw a scale taken from the normative study. Partici-\npants rated words on a nine-point and answered a yes /no “Do \nyou know this word?” question. Participants rated 30 words for \neach of the two dimensions, five words were repeated to assess \nrater reliability. This procedure differed from the normative \nstudy in that the normative study was done with pen and paper \non a list of words, whereas the words appeared individually (on \ndifferent questionnaire web pages) in our online study.\nResults and discussion\nWe calculated accuracy metrics on out-of-distribution words \n(e.g., accuracy of valence prediction for highly abstract \nwords, when the model was only shown concrete words dur-\ning training), and the test set drawn from all words for the \nfour affective metrics which were not manipulated. Below, \nthe results are shown and compared to the accuracies of the \noriginal English model in Table  4. Again, all correlations \nwere significant with p  < 0.001. The result shows that the \ntransformer-based predictions generalize completely over \nconcreteness, only with regards to the metrics of arousal and \nage of acquisition, with a drop in correlation of results rang-\ning from Δr = 0.1 in the case of arousal to Δr = 0.15 in the \ncase of the age of acquisition. Valence and dominance were \npredicted with exactly the same accuracy in both conditions.\nWe estimate the correlation with human judgments on \na large set of concreteness norms, none of which were \nincluded in the training set. We observe that the accuracy \nof our model decreases slightly on out-of-distribution \nwords and when norms of words known by fewer people \nare included (shown in Fig.  2, all estimated correlations \nwere significant with p < 0.001). First, estimating accuracy \non all 6000 words, known by all participants among words \nnot included in the training set, yields a correlation of r = \n0.91, slightly lower than the original test set correlation of \nr = 0.95. The correlation decreases by around 0.03 to r  = \n0.875 for all words in the concreteness norms dataset, which \nincludes words known by at least 85% of participants.\nIn the experimental validation, the words were chosen at \nrandom conditional on not being included in the emotional \nnorms database. Mean ratings were calculated as an average \nof the mean ratings within each gender to control for the \nimbalanced participant sample within each rated dimension. \nDescriptive statistics for all words are available in the Sup-\nplementary Materials.\nEvery time new norms are collected, we expect to see \na larger error caused by regression to the mean, a different \nparticipant sample, rating procedure, word selection, a finite \nnumber of participants, as well as the change of emotional \nload of words over time (e.g., the word “pandemic”). We can \nquantify the change in correlation due to additional noise \nas the percentage change (1-r2/r1), where r1 is the original \ncorrelation and can r2 is the experimental correlation. Aver-\naging over the five variables (see Table 5), the average drop \nin accuracy on out-of-distribution words equaled Δr  =11% \n(95% CI [5%, 19%]). Even for the worst value inside the \nconfidence interval, less than 20% of the accuracy is lost on \nout-of-distribution words. Note that one should not compare \nthe drop in accuracy between dimensions reported here, as \neach experimental accuracy was obtained on a different set \nof words.\nStudy 3: Stimuli descent algorithm\nTo demonstrate the utility of neural norm extrapolation, \nwe propose a method leveraging the differentiability of \nthe neural network that predicts emotional norms to select \nwords for experimental stimuli. Here, we aim to manipu -\nlate certain emotional factors while controlling others. The \nalgorithm, which we call stimuli descent, has the ability not \nonly to control specified factors but to control other, unspeci-\nfied semantic properties of words in an unsupervised way. \nTo understand how this may be achieved, recall how word \nembeddings describe words on meaningful dimensions the \nmodel discovered during training. This fact is utilized, for \nexample, in the wide use of the distance in word embed-\nding space as a measure of semantic similarity (Kenter & \nde Rijke, 2015; Sitikhu et al., 2019). Thus, words that are \nclose together tend to have similar meanings, which is the \nTable 4  The results of the concreteness dependent missingness robustness check on the test set\n* p < 0.05** p < 0.01*** p < 0.001\nThe original model accuracy relates to the results presented in Study 1. The concreteness manipulation relates to the accuracies of the model \ntrained on high concreteness words and tested on low concreteness words. The comparison dataset relates to a model trained on a random sample \nof original words, keeping the length of the words in each of the datasets the same as in the case of the concreteness manipulation\nCondition Valence Arousal Dominance Age of acquisition\nOriginal model accuracy 0.95*** 0.76*** 0.86*** 0.85***\nRandom test set 0.94*** 0.76*** 0.86*** 0.86***\nTest set of abstract words 0.94*** 0.67*** 0.86*** 0.71***\n4725Behavior Research Methods (2024) 56:4716–4731 \n1 3\nfirst way one can achieve control on semantic dimensions. \nThe second way rests on the observation that the predictive \nmodels we have trained show a mapping from word embed-\ndings to the emotional norms that is locally linear for all \nnorms. This means that, locally, there is a single direction \nassociated with a change in valence and this fact may be \nused to find a word whose embedding differs principally \non this dimension, while being close to all others. By doing \nthis, we increase the chance the matched word will be close \nin value on all unaccounted-for dimensions that these direc-\ntions describe. This, in turn, decreases the chance that one \nof these outside dimensions may, for example, differentially \naffect reaction times in two conditions of an experiment, \nchallenging its internal validity.\nThe stimuli descent algorithm takes advantage of the \ndifferentiability of our method to find semantically matched \nwords by performing gradient descent in word embedding \nspace with respect to the predicted emotional norm. As \nthe position in word embedding space encodes informa -\ntion about how the word is used and its semantic connec-\ntions, words that are close together in this space tend to \nhave similar meanings (Stratos et al., 2015). Thus, stimuli \ndescent moves down the function from word embeddings \nto norms to find the closest word that differs in this norm, \nallowing for the selection of semantically matched words. \nTo do this, the algorithm at each step predicts the norms \nand calculates the gradient of the norm we wish to manipu-\nlate. As this gradient is the vector in word embedding space \nthat points in the direction of the fastest increase in the \npredicted rating, the algorithm moves along this vector to \nfind the closest word with the most divergent rating. https:// \ncolab. resea rch. google. com/ drive/ 1Cjce jg1Ad DhsZW s4Vio \nQ5tT8 B496j gFZ\nMethod\nWe wish to find semantically matched words less or more \npronounced on the manipulated dimension, for words in a \nset. To simplify the presentation of the algorithm, we will \nassume our manipulated dimension is valence, and we wish \nto match words less positive than those from a set of positive \nwords. Apart from the word we must specify the minimum \ndifference in ratings we wish to achieve (denoted Δmin r ), so \nthat the difference (denoted Δr ) between a matched word’s \nvalence and the valence of the original word is at least as \nFig. 2  The effect of word familiarity on prediction performance for concreteness. The y-axis indicates the correlation of prediction and ratings \nestimated from a selection of words that were known by at least the proportion of participants indicated on the x-axis\nTable 5  The results of the experimental validation test\n* p < 0.05** p < 0.01*** p < 0.001\nOriginal model accuracy relates to the results presented in Study 1. 95% Confidence intervals are presented in square brackets. The 95% con-\nfidence interval for the original model is tighter than the precision with which correlations are reported. The “condition” column relates to the \nexperimental condition, while the rest of the columns relate to the correlations between the respective emotional norms and their predictions\nCondition Origin Imageability Dominance Arousal Valence\nOriginal model accuracy 0.86*** 0.88*** 0.92*** 0.86*** 0.93***\nExperimental accuracy 0.84***\n[0.73, 0.92]\n0.71***\n[0.52, 0.82]\n0.86***\n[0.73, 0.93]\n0.83***\n[0.68, 0.92]\n0.70***\n[0.45, 0.89]\n4726 Behavior Research Methods (2024) 56:4716–4731\n1 3\nlarge as Δmin r . Now, we must define the loss function with \nrespect to which we will perform gradient descent. In this \ninstance, it will be the predicted valence rating (with either \na plus or minus to increase or decrease the rating). We may \nalso wish to control other emotional dimensions such as \narousal. To this end, we may perform a kind of controlled \ngradient descent, where we want to remain at the same level \nof the controlled dimensions at each step. Thus, at each step \nwe calculate the prediction of the controlled dimension. The \ngradient of this prediction shows the direction in which the \nprediction of controlled dimensions changes and we may \nuse these “controlled gradients” to remove their components \nfrom the loss gradient, remaining approximately at the same \nlevel of controlled dimensions.\nLast modifications to the algorithm involve accounting for \nthe high dimensionality of the space we are moving in. First, \nas words are discrete points in this high-dimensional embed-\nding space, there may not be a word that corresponds to the \nplace this procedure has moved us to. Thus, to select new \nstimuli we need to check for approximate matches – words \nthat are close in embedding space according to some dis -\ntance metric. We chose cosine similarity, a metric typically \nused for comparing embedding similarity. Second, this high \ndimensionality makes it easier to move to regions of word \nembedding space where there are no words. This creates \nthe issue that in such regions the network predictions are \nonly extrapolations, which could easily be wrong. Thus, we \nadd to the loss function a regularization term penalizing the \nalgorithm for stepping outside of a multivariate gaussian \ndistribution approximating the word occurrence distribution. \nThis term is proportional to the logarithm of the Gaussian \ndensity function and is described in more detail in Appendix \n1 along with other technical details on the objective func-\ntion. The complete algorithm is described in Fig.  3.\nFigure  3 The stimuli descent algorithm, which finds \nsemantically matched words via controlled gradient descent \nin the word embedding space\nResults and discussion\nFor matching words with the stimuli descent algorithm, we \nhave trained another transformer model for English, but \nwith just one word embedding space, instead of two (with \nembeddings from the BERT model \"bertweet-base-emotion-\nanalysis\"; Pérez, 2021). The correlations with human judg-\nments for this model were similar to the ones from Table  2 \nand evaluated to be r = 0.95, r = 0.76, r = 0.86, r = 0.85, \nand r = 0.95, respectively for valence, arousal, dominance, \nage of acquisition, and concreteness.\nWe have looked for semantically matched words for two \ncommonly manipulated factors: valence and arousal. While \nlooking for similar words that differ in these dimensions, \nwe have also instituted controlled variables, such that the \nalgorithm was either manipulating valence and controlling \narousal or manipulating arousal while controlling valence \nand dominance. To start, the algorithm needs a word to \nmatch the next words to. For each dimension we selected \napproximately 250 words. Since we can choose to either \ndecrease or increase the ratings, half of these words were \nhigh, and half were low on the dimension of interest.\nTo minimize the risk of bias, the selection of words was \ndone based purely on ratings. For the valence manipula-\ntion we picked words whose valence ratings were clos-\nest to 1 standard deviation above or 1 standard deviation \nbelow the mean valence rating. Similarly, for arousal, we \npicked words close to 1.5 standard deviation above or below. \nSelected results of these analyses are presented in Table  4. \nAll obtained results are available in the following OSF \nrepository: https:// osf. io/ cug92/? view_ only= 6f246 610bc \n0b43c c9e98 d7c97 8f2f6 fa Table 6.\nObserving results, we see that successful matches some-\ntimes also automatically match words also on more surface \nlevel features, such as length, or how the word sounds. One \nsuch example from the supplementary electronic material \nFig. 3  The description of the stimuli descent algorithm\n4727Behavior Research Methods (2024) 56:4716–4731 \n1 3\nis the match “trample” and “scramble”. This is useful as \nsuch surface-level features have also been shown to influ-\nence behavior in experimental tasks (Hauk & Pulvermüller, \n2004; Kuchinke, et al., 2007; Méndez-Bértolo et al., 2011). \nMatching stimuli with neural networks present an exciting \ndirection for methods research that may lead to more robust \nexperimental results with word stimuli.\nCertain limitations of the method must be noted. First, cer-\ntainly not every word has a word of different emotional value \nmatching its semantic content. What this means for the method \nis that it can certainly fail to find matching words but will \nnonetheless propose candidate words. Thus, caution needs to \nbe exercised when using the algorithm to generate stimuli to \nidentify these cases. Some methods can be developed to iden-\ntify mismatched words. First, the distance of the word at the \nmatching stage can serve as an indicator of the appropriateness \nof the match. This leads to the second limitation of the validity \nof distance measurement in high-dimensional spaces. In higher \ndimensional spaces, the likelihood of finding points diminishes \nexponentially with the number of dimensions. Future work \nmay try to address this limitation by picking several candidate \nwords using the algorithm and performing the selection pro-\ncess with more sophisticated measures of similarity.\nGeneral discussion\nThe present study has shown that a transformers-based archi-\ntecture can be useful in predicting a range of affective and \nother word metrics. While the usefulness of transformers for \nsuch tasks has been widely recognized in machine learning \n(Lin et al., 2022), the full extent of the benefits that these \nmethods can bring to the study of human behavior is an open \narea for research. The psychologically interesting conclu-\nsions that can be drawn from the high correlation of trans-\nformer-based norm extrapolation with human judgments can \nprovide support to the claim that the distributional properties \nof words in written language hold information about how \nword stimuli are judged in terms of their emotionality by \nparticipants of psychological studies (Sahlgren, 2008). In \nthe end, the numerical representations of words, which we \nuse in the training process of our models, are related to how \nwords interact with each other in text (through the attention \nmechanism concept introduced by Vaswani et al., 2017). A \nstatistical regularity therefore exists between emotion rat-\nings and the structure of human language. The two cognitive \nmechanisms that can be hypothesized to support such statis-\ntical regularity are the influence of affective properties on \nthe way language is used, and the converse mechanism of the \npatterns in which language is used influencing the affective \nreactions, either directly or through shaping the interpreta-\ntion of semantic content (for a discussion of links between \ndistributional co-occurrence and emotional dimensions see \nSnefjella & Kuperman, 2016). This is easy to imagine as \nthe emotional meaning of words is well associated with \ntheir semantic meaning (Vankrunkelsven et al., 2015). This \nresult, however, does not extend to all words of a language, \nas prediction is limited to words similar to the words used \nin emotional words databases (Snefjella & Blank, 2020).\nThe possibility of inferring vast amounts of information \nfrom distributional properties finds support in the natu-\nral language processing literature, tasked with extracting \nmeaningful semantic information from text based on the \nco-occurrence of words in large corpora. A common exam-\nple includes the algebraic treatment of vector word-meaning \nrepresentations in word embeddings, using which it is pos-\nsible to find the representation of the word ‘man’ by sub-\ntracting the representation of ‘royal’ from that of the word \n‘king’ (Ethayarajh, 2019). More novel methods, such as the \nones used in this paper, may represent an even wider array \nof semantic information, which is evidenced by the impres-\nsive semantic capabilities of the most advanced transformer-\nbased large language models (Sobieszek & Price, 2022), \nwhich for GPT-3 included translation, summarization and \nTable 6  Words generated using the stimuli descent algorithm\nSample results of semantically matched words. Bold font indicates words that were put to the algorithm, the words in the next two columns have \nbeen matched by the algorithm. ‘X’ indicates the algorithm did not find any matching words for that level of the manipulated dimension\nManipulation of valence Manipulation of arousal\nLow Medium High Low Medium High\nSkeleton Fossil Wishbone Pleasant Splendid Glorious\nCrass Cheeky Whimsical Villa Condo Mansion\nUnscheduled X Surprising Hanger Rake Spikes\nPretentious Contemporary Philosophical Patron Promoter Activist\nIntimidate Exceed Impress Willing Attentive Eager\nDrain X Fountain Bike X Motorcycle\nSubdue Neutralize Alleviate Mule Possum Skunk\nInsurance Consumption Income Unintentional Overwhelming Uncontrollable\n4728 Behavior Research Methods (2024) 56:4716–4731\n1 3\nthe execution of linguistic tasks purely from their descrip-\ntion (Brown et al., 2020), which the introduction of GPT-4 \nexpanded to a vast set of common sense task that seem to \nrequire some basic understanding of the world (for detailed \ntests, see Bubeck et al., 2023).\nThese developments point to an increasing role that \nmachine learning may play in the conduct of psychological \nstudies of language and emotions. A basic use case that the \nhigh correlations with human judgments could afford is the \nuse of extrapolated norms for choosing experimental stimuli. \nWhile empirical verification of extrapolated norms is always \nadvised, it does not render the extrapolation useless. Say one \nwas designing a study with an orthogonal design that studies \nthe influence of three emotional factors, for example, valence, \narousal, and dominance, with affective word stimuli. As \nvalence is highly correlated with dominance and has a quad-\nratic relationship to arousal (Warriner et al., 2013), it is very \nrare for words to have an emotional load of positive valence, \nlow dominance, and low arousal at the same time, but a \ngroup of such words would be required to construct such an \northogonal design. This means that not enough words may be \npresent in the available affective norms dataset to construct \nsuch a design. The issue also arises in simpler designs when \nattempting to control correlated factors. The solution that \nprecise norm extrapolation affords is to use its predictions as \na heuristic tool for picking stimuli to put to human evalua -\ntion to balance the existing affective word databases. Using \nexisting affective norms, one may predict which words are \nlikely to have the unusual emotional load of positive valence, \nlow dominance, and low arousal, and verify this prediction \nempirically. In this way, semantic norms extrapolation may \nbe used as tools for picking experimental stimuli.\nTo support this application of the transformer-based norm \nextrapolation proposed in this paper, we developed an algo-\nrithm for the selection of stimuli. The algorithm leverages both \nthe high correlations with human affective judgments and the \nsemantic aspects of words learnt by the network to select words \nthat are semantically similar, yet affectively different. The algo-\nrithm allows one to manipulate emotional factors while con-\ntrolling others, but also employs the unsupervised control of \nword meaning that has not yet been explored in the literature \non affective words. The novelty of the method stems from its \nleveraging of the differentiability of our extrapolation method. \nAs the method does not use k-nearest neighbors for extrapola-\ntion we can find the gradients of all the predicted norms to \nfind the nearest word with different affective ratings, where \nthe distance is an auxiliary measure of semantic similarity.\nAn additional consideration is whether our model may be \nof use in the study of computational models of emotion (Mar-\nsella et al., 2010). Firstly, the norms modeled by our network \nare the average of the outcomes of individual emotional pro-\ncesses of the participants of the normative study. These pop-\nulation-level estimates, while useful, do not correspond to the \nexperiences of any particular person and as such the ability \nto infer from norms to psychological mechanisms is severely \nlimited. Currently the use of the model in such a context is \nadditionally limited by the general limited understanding of \nhow transformer networks make their predictions and as such \ndrawing any specific scientific conclusions for cognitive sci-\nence from the trained model should be discouraged.\nTo avoid scientifically dubious conclusions, the misuse of \nthe model may bring, it is necessary to underscore the limi-\ntations highlighted in the robustness section, following the \ncritique of Snefjella & Blank (2020). Our transformer-based \nextrapolation method, while versatile showing promise in \ndealing with selection bias, does not overcome the limita-\ntion posed by out-of-distribution prediction from a sample \nfrom which norms are missing not at random. To address \nthis one should avoid using imputed norms in studies where \na systematic bias on uncommon words may lead to false \ninferences, such studies which analyze stimuli corpora. If \none wishes to select as stimuli uncommon words it is neces-\nsary to experimentally validate their norms, as there both \nepistemic and aleatoric uncertainties will impact the mod-\nel’s performance. As discussed previously, it is generally \nill-advised to use extrapolated norms of age-of-acquisition \nand any norm of words known by a fraction of people. The \nconclusion of the experimental validation is that one should \nconservatively expect at least a 10% drop in accuracy for \nout-of-distribution words. Consequently, while the model \nmay be a valuable tool, its applications should always take \nnote of the fact it does not overcome the issues of missing \ndata imputation highlighted by Snefjella & Blank ( 2020). \nThe same issues apply to all the norm extrapolation methods \nreported previously as well (Snefjella & Blank, 2020).\nTaking advantage of the advances in machine learning is \nimportant for the broader scientific community, especially con-\nsidering the asymmetry in the availability of computational \nresources. Guided by this thought, we have chosen not only to \nshare all our code and models, but to implement the methods \nfrom the paper in an online notebook that allows for their use \nwith a simple graphical interface. In this way, the methods \ncan be used without the need for coding or access to high-end \nGPUs. We hope that this will maximize the benefits that the \nmethods can bring researchers in the psychology of emotion.\nSupplementary Information The online version contains supplemen-\ntary material available at https:// doi. org/ 10. 3758/ s13428- 023- 02212-3.\nOpen Access  This article is licensed under a Creative Commons Attri-\nbution 4.0 International License, which permits use, sharing, adapta-\ntion, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, \nprovide a link to the Creative Commons licence, and indicate if changes \nwere made. The images or other third party material in this article are \nincluded in the article's Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in \nthe article's Creative Commons licence and your intended use is not \n4729Behavior Research Methods (2024) 56:4716–4731 \n1 3\npermitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a \ncopy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\nReferences\nAlmeida, F., & Xexéo, G. (2019). Word embeddings: A survey. \narXiv:1901.09069. Retrieved 20 January 2022 from https:// doi.  \norg/ 10. 48550/ ARXIV. 1901. 09069\nBestgen, Y., & Vincze, N. (2012). Checking and bootstrapping lexical \nnorms by means of word similarity indexes. Behavior Research Meth-\nods, 44(4), 998–1006. https:// doi. org/ 10. 3758/ s13428- 012- 0195-z\nBinder, J. R., Conant, L. L., Humphries, C. J., Fernandino, L., Simons, \nS. B., Aguilar, M., & Desai, R. H. (2016). Toward a brain-based \ncomponential semantic representation. Cognitive Neuropsychol-\nogy, 33(3-4), 130–174. https:// doi. org/ 10. 1080/ 02643 294. 2016. \n11474 26\nBoleda, G. (2020). Distributional semantics and linguistic theory. \nAnnual Review of Linguistics, 6, 213–234. https:// doi. org/ 10. 1146/ \nannur ev- lingu istics- 011619- 030303\nBradley, M. M., & Lang, P. J. (1999). Affective norms for English words \n(ANEW): Instruction manual and affective ratings (Technical \nreport C-1, Vol. 30, No. 1, pp. 25−36). The Center for Research \nin Psychophysiology.\nBradley, M. M., & Lang, P. J. (1994). Measuring emotion: The self-assess-\nment manikin and the semantic differential. Journal of Behavior \nTherapy and Experimental Psychiatry, 25(1), 49–59. https:// doi. org/ \n10. 1016/ 0005- 7916(94) 90063-9\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, \nP., et al. (2020). Language models are few-shot learners. Advances \nin Neural Information Processing Systems, 33, 1877–1901.\nBrysbaert, M., Stevens, M., De Deyne, S., Voorspoels, W., & Storms, \nG. (2014). Norms of age of acquisition and concreteness for \n30,000 Dutch words. Acta Psychologica, 150, 80–84. https:// doi. \norg/ 10. 1016/j. actpsy. 2014. 04. 010\nBrysbaert, M., Warriner, A. B., & Kuperman, V. (2014). Concreteness \nratings for 40 thousand generally known English word lemmas. \nBehavior Research Methods, 46(3), 904–911. https:// doi. org/ 10. \n3758/ s13428- 013- 0403-5\nBubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., \nKamar, E., et al. (2023). Sparks of artificial general intelligence: \nEarly experiments with Gpt-4. arXiv:2303.12712. Retrieved 20 \nJanuary 2022 from https:// doi. org/ 10. 48550/ arXiv. 2303. 12712\nCitron, F. M. (2012). Neural correlates of written emotion word \nprocessing: A review of recent electrophysiological and hemo-\ndynamic neuroimaging studies. Brain and Language, 122(3), \n211–226. https:// doi. org/ 10. 1016/j. bandl. 2011. 12. 007\nConneau, A., & Lample, G. (2019). Cross-lingual language model pre-\ntraining. Advances in Neural Information Processing Systems, 32.\nCrossfield, E., & Damian, M. F. (2021). The role of valence in word \nprocessing: Evidence from lexical decision and emotional Stroop \ntasks. Acta Psychologica, 218, 103359. https:// doi. org/ 10. 1016/j. \nactpsy. 2021. 103359\nDadas, S. (2020). Sdadas/polish-roberta [Python]. GitHub. \nRetrieved 10 January 2022 from https:// github. com/ sdadas/  \npolis hrobe rta\nDe Deyne, S., Navarro, D. J., & Storms, G. (2013). Better explana-\ntions of lexical and semantic cognition using networks derived \nfrom continued rather than single-word associations. Behavior \nResearch Methods, 45(2), 480–498. https:// doi. org/ 10. 3758/ \ns13428- 012- 0260-7\nde Vries, W., van Cranenburgh, A., Bisazza, A., Caselli, T., van Noord, \nG., Nissim, M. (2019). BERTje: A Dutch BERT Model. https:// doi. \norg/ 10. 48550/ ARXIV. 1912. 09582\nDevlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018) Bert: Pre-\ntraining of deep bidirectional transformers for language under -\nstanding. arXiv:1810.04805. Retrieved 20 January from https://  \ndoi. org/ 10. 48550/ ARXIV. 1810. 04805\nElkins, K., Chun, J. (2020). Can GPT-3 pass a writer’s Turing test? \nJournal of Cultural Analytics . https://  doi. org/ 10. 22148/  001c. \n17212\nEthayarajh, K. (2019). Rotate king to get queen: Word relation-\nships as orthogonal transformations in embedding space. \narXiv:1909.00504. https:// doi. org/ 10. 48550/ arXiv. 1909. 00504\nHauk, O., & Pulvermüller, F. (2004). Effects of word length and frequency \non the human event-related potential. Clinical Neurophysiology, \n115(5), 1090–1103. https:// doi. org/ 10. 1016/j. clinph. 2003. 12. 020\nHochreiter, S. (1998). The Vanishing Gradient Problem During Learn-\ning Recurrent Neural Nets and Problem Solutions. International \nJournal of Uncertainty, Fuzziness and Knowledge-Based Systems, \n06(02), 107–116. https:// doi. org/ 10. 1142/ S0218 48859 80000 94\nHugging Face. (n.d.). Models—hugging face. Retrieved March 29, \n2022, from https:// huggi ngface. co/ models\nImbir, K. K. (2015). Affective norms for 1,586 polish words (ANPW): \nDuality-of-mind approach. Behavior Research Methods, 47(3), \n860–870. https:// doi. org/ 10. 3758/ s13428- 014- 0509-4\nImbir, K. K. (2016). Affective Norms for 4900 Polish Words Reload \n(ANPW_R): Assessments for valence, arousal, dominance, origin, \nsignificance, concreteness, imageability and age of acquisition. \nFrontiers in Psychology, 7, 1081. https:// doi. org/ 10. 3389/ fpsyg. \n2016. 01081\nImbir, K. K., Pastwa, M., Duda-Goławska, J., Sobieszek, A., \nJankowska, M., Modzelewska, A., Wielgopolan, A., & \nŻygierewicz, J. (2021). Electrophysiological correlates of interfer-\nence control in the modified emotional Stroop task with emotional \nstimuli differing in valence, arousal, and subjective significance. \nPlos One, 16(10), e0258177. https:// doi. org/ 10. 1371/ journ al. pone. \n02581 77\nImbir, K. K., Duda-Goławska, J., Sobieszek, A., Wielgopolan, A., \nPastwa, M., & Żygierewicz, J. (2022). Arousal, subjective sig-\nnificance and the origin of valence aligned words in the processing \nof an emotional categorisation task. Plos One, 17(3), e0265537. \nhttps:// doi. org/ 10. 1371/ journ al. pone. 02655 37\nImbir, K. K., Duda-Goławska, J., Pastwa, M., Jankowska, M., Mod-\nzelewska, A., Sobieszek, A., Żygierewicz, J. (2020). Electrophysi-\nological and behavioral correlates of valence, arousal and subjec-\ntive significance in the lexical decision task. Frontiers in Human \nNeuroscience, 427. https:// doi. org/ 10. 3389/ fnhum. 2020. 567220\nKanske, P., & Kotz, S. A. (2007). Concreteness in emotional words: ERP evi-\ndence from a hemifield study. Brain Research, 1148, 138–148. https:// \ndoi. org/ 10. 1016/j. brain res. 2007. 02. 044\nKapucu, A., Kılıç, A., Özkılıç, Y., & Sarıbaz, B. (2021). Turkish emo-\ntional word norms for arousal, valence, and discrete emotion cat-\negories. Psychological Reports, 124(1), 188–209. https:// doi. org/ \n10. 1177/ 00332 94118 814722\nKazojć J (2011) Słownik frekwencyjny języka polskiego (Polish lan-\nguage dictionary of attendance). Available: http:// www. slown iki. \norg. pl/ i27ht ml . Acces sed 20 March  2014\nKenter, T., & De Rijke, M. (2015, October). Short text similarity with \nword embeddings. In Proceedings of the 24th ACM International \non Conference on Information and Knowledge Management (pp. \n1411–1420). https:// doi. org/ 10. 1145/ 28064 16. 28064 75\nKim, E., & Klinger, R. (2019). A survey on sentiment and emotion \nanalysis for computational literary studies. Zeitschrift Für Digitale \nGeisteswissenschaften. https:// doi. org/ 10. 17175/ 2019_ 008\nKuchinke, L., Võ, M. L. H., Hofmann, M., & Jacobs, A. M. (2007). \nPupillary responses during lexical decisions vary with word \n4730 Behavior Research Methods (2024) 56:4716–4731\n1 3\nfrequency but not emotional valence. International Journal of \nPsychophysiology, 65(2), 132–140. https:// doi. org/ 10. 1016/j. ijpsy \ncho. 2007. 04. 004\nKuperman, V., Stadthagen-Gonzalez, H., & Brysbaert, M. (2012). \nAge-of-acquisition ratings for 30,000 English words. Behavior \nResearch Methods, 44(4), 978–990. https:// doi. org/ 10. 3758/  \ns13428- 012- 0210-4\nLin, T., Wang, Y., Liu, X., & Qiu, X. (2022). A survey of transform-\ners. AI Open, 3, 111–132. https:// doi. org/ 10. 1016/j. aiopen. 2022. \n10. 001\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., et al. (2019). \nRoberta: A robustly optimized Bert pretraining approach. \narXiv:1907.11692. Retrieved 20 January from https:// arxiv. org/ \nabs/ 1907. 11692 v1\nMandera, P., Keuleers, E., & Brysbaert, M. (2015). How useful are \ncorpus-based methods for extrapolating psycholinguistic vari-\nables? Quarterly Journal of Experimental Psychology, 68(8), \n1623–1642. https:// doi. org/ 10. 1080/ 17470 218. 2014. 988735\nMarsella, S., Gratch, J., & Petta, P. (2010). Computational models of \nemotion. A Blueprint for Affective Computing-A Sourcebook and \nManual, 11(1), 21–46.\nMéndez-Bértolo, C., Pozo, M. A., & Hinojosa, J. A. (2011). Word fre-\nquency modulates the processing of emotional words: Convergent \nbehavioral and electrophysiological data. Neuroscience Letters,  \n494(3), 250–254. https:// doi. org/ 10. 1016/j. neulet. 2011. 03. 026\nMeng, K., Bau, D., Andonian, A., & Belinkov, Y. (2022). Locating and \nediting factual associations in GPT. Advances in Neural Informa-\ntion Processing Systems, 35, 17359–17372.\nMikolov, T., Sutskever, I., Chen, K., Corrado, G. S., Dean, J. (2013). \nDistributed representations of words and phrases and their com-\npositionality. Advances in Neural Information Processing Systems, \n26. https:// doi. org/ 10. 48550/ arXiv. 1310. 4546.\nMohammad, S. (2018, July). Obtaining reliable human ratings of \nvalence, arousal, and dominance for 20,000 English words. In \nProceedings of the 56th annual meeting of the association for \ncomputational linguistics (volume 1: Long papers) (pp. 174–184). \nhttps:// doi. org/ 10. 18653/ v1/ P18- 1017\nMontefinese, M., Ambrosini, E., Fairfield, B., & Mammarella, N. \n(2014). The adaptation of the Affective Norms for English Words \n(ANEW) for Italian. Behavior Research Methods, 46(3), 887–903. \nhttps:// doi. org/ 10. 3758/ s13428- 013- 0405-3\nMoors, A., De Houwer, J., Hermans, D., Wanmaker, S., van Schie, K., \nVan Harmelen, A.-L., De Schryver, M., De Winne, J., & Brys-\nbaert, M. (2013). Norms of valence, arousal, dominance, and age \nof acquisition for 4,300 Dutch words. Behavior Research Methods, \n45(1), 169–177. https:// doi. org/ 10. 3758/ s13428- 012- 0243-8\nMunikar, M., Shakya, S., & Shrestha, A. (2019, November). Fine-\ngrained sentiment classification using BERT. In 2019 Artificial \nIntelligence for Transforming Business and Society (AITB) (Vol. 1, \npp. 1–5). IEEE. https:// doi. org/ 10. 1109/ AITB4 8515. 2019. 89474 \n35\nNielsen, F. Å. (2011). A new ANEW: Evaluation of a word list for senti-\nment analysis in microblogs. arXiv:1103.2903. Retrieved 20 January \nfrom https:// doi. org/ 10. 48550/ arXiv. 1103. 2903\nOsgood, C. E., Suci, G. J., & Tannenbaum, P. H. (1957). The measure-\nment of meaning. University of Illinois Press.\nPérez, J. M. (2021). Bertweet base sentiment analysis [Model]. Hug-\nging Face. Retrieved 10 January 2022 from https:// huggi ngface. \nco/ finit eauto mata/ bertw eet- base- senti ment- analy sis\nPerezrojas, J., Cañete, C., Chaperon, G., & Zúñiga, R. F. (2020). BETO: Spanish \nBERT. DCC U Chile. Retrieved 10 January 2022 from https:// github. com/ \ndccuc hile/ beto (Original work published 2019).\nPires, T., Schlinger, E., & Garrette, D. (2019). How Multilingual is Mul-\ntilingual BERT? In Proceedings of the 57th Annual Meeting of the \nAssociation for Computational Linguistics, pp. 4996–5001. https:// \ndoi. org/ 10. 18653/ v1/ P19- 1493.\nRecchia, G., & Louwerse, M. M. (2015). Reproducing affective norms with \nlexical co-occurrence statistics: Predicting valence, arousal, and dom-\ninance. The Quarterly Journal of Experimental Psychology, 68(8), \n1584–1598. https:// doi. org/ 10. 1080/ 17470 218. 2014. 941296\nRedondo, J., Fraga, I., Padrón, I., & Comesaña, M. (2007). The Span-\nish adaptation of ANEW (Affective Norms for English Words). \nBehavior Research Methods, 39(3), 600–605. https:// doi. org/ 10. \n3758/ BF031 93031\nSahlgren, M. (2008). The distributional hypothesis. The Italian Journal \nof Linguistics. Retrieved 20 January 2022 from https:// www. itali \nan- journ al- lingu istics. com/ app/ uploa ds/ 2021/ 05/ Sahlg ren-1. pdf\nSianipar, A., Van Groenestijn, P., & Dijkstra, T. (2016). Affective \nmeaning, concreteness, and subjective frequency norms for Indo-\nnesian words. Frontiers in Psychology, 7, 1907. https:// doi. org/ 10. \n3389/ fpsyg. 2016. 01907\nSitikhu, P., Pahi, K., Thapa, P., & Shakya, S. (2019, November). A \ncomparison of semantic similarity methods for maximum human \ninterpretability. In 2019 artificial intelligence for transforming \nbusiness and society (AITB) (Vol. 1, pp. 1–4). IEEE. https:// doi.  \norg/ 10. 1109/ AITB4 8515. 2019. 89474 33\nSloan, D. M., Strauss, M. E., & Wisner, K. L. (2001). Diminished \nresponse to pleasant stimuli by depressed women. Journal of \nAbnormal Psychology, 110(3), 488. https:// doi. org/ 10. 1037// \n0021- 843x. 110.3. 488\nSnefjella, B., Blank, I. (2020). Semantic norm extrapolation is a miss-\ning data problem. Retrieved 20 January from https:// doi. org/ 10. \n31234/ osf. io/ y2gav.\nSnefjella, B., & Kuperman, V. (2016). It’s all in the delivery: Effects \nof context valence, arousal, and concreteness on visual word pro-\ncessing. Cognition, 156, 135–146. https:// doi. org/ 10. 1016/j. cogni \ntion. 2016. 07. 010\nSoares, A. P., Comesaña, M., Pinheiro, A. P., Simões, A., & Frade, C. S. \n(2012). The adaptation of the Affective Norms for English Words \n(ANEW) for European Portuguese. Behavior Research Methods, \n44(1), 256–269. https:// doi. org/ 10. 3758/ s13428- 011- 0131-7\nSobieszek, A., & Price, T. (2022). Playing games with Ais: The limits \nof GPT-3 and similar large language models. Minds and Machines, \n32(2), 341–364. https:// doi. org/ 10. 1007/ s11023- 022- 09602-0\nSöderholm, C., Häyry, E., Laine, M., & Karrasch, M. (2013). Valence \nand arousal ratings for 420 Finnish nouns by age and gender. PloS \nOne, 8(8), e72859. https:// doi. org/ 10. 1371/ journ al. pone. 00728 59\nStadthagen-Gonzalez, H., Imbault, C., Pérez Sánchez, M. A., & Brys-\nbaert, M. (2017). Norms of valence and arousal for 14,031 Span-\nish words. Behavior Research Methods, 49(1), 111–123. https://  \ndoi. org/ 10. 3758/ s13428- 015- 0700-2\nStakovskii, E. (n.d.). French toxicity classifier plus v2 [Model]. Hug-\nging Face. Retrieved 20 January 2022, from https:// huggi ngface. \nco/ EISta kovsk ii/ french_ toxic ity_ class ifier_ plus_ v2\nStevenson, R. A., Mikels, J. A., & James, T. W. (2007). Characteriza-\ntion of the affective norms for English words by discrete emo -\ntional categories. Behavior Research Methods, 39(4), 1020–1024. \nhttps:// doi. org/ 10. 3758/ BF031 92999\nStratos, K., Collins, M., & Hsu, D. (2015, July). Model-based word \nembeddings from decompositions of count matrices. In Proceed-\nings of the 53rd annual meeting of the association for compu-\ntational linguistics and the 7 th international joint conference \non natural language processing (Volume 1: Long papers) (pp. \n1282–1291). https:// doi. org/ 10. 3115/ v1/ P15- 1124\nSun, Y., Wang, S., Li, Y., Feng, S., Tian, H., Wu, H., & Wang, H. \n(2019). ERNIE 2.0: A continual pre-training framework for lan-\nguage understanding. arXiv:1907.12412. Retrieved 20 January \n2022 from arXiv. https:// doi. org/ 10. 48550/ arXiv. 1907. 12412\nSyssau, A., Yakhloufi, A., Giudicelli, E., Monnier, C., & Anders, R. \n(2021). FANCat: French affective norms for ten emotional catego-\nries. Behavior Research Methods, 53(1), 447–465. https:// doi. org/ 10. \n3758/ s13428- 020- 01450-z\n4731Behavior Research Methods (2024) 56:4716–4731 \n1 3\nVaiouli, P., Panteli, M., & Panayiotou, G. (2023). Affective and psy -\ncholinguistic norms of Greek words: Manipulating their affective \nor psycho-linguistic dimensions. Current Psychology, 42(12), \n10299–10309. https:// doi. org/ 10. 1007/ s12144- 021- 02329-8\nVan Buuren, S., & Groothuis-Oudshoorn, K. (2011). mice: Multivari-\nate imputation by chained equations in R. Journal of Statistical \nSoftware, 45, 1–67. https:// doi. org/ 10. 18637/ jss. v045. i03\nVankrunkelsven, H., Verheyen, S., Storms, G., & De Deyne, S. (2018). \nPredicting lexical norms: A comparison between a word associa-\ntion model and text-based word co-occurrence models. Journal of \nCognition, 1(1), 45. https:// doi. org/ 10. 5334/ joc. 50\nVankrunkelsven, H., Verheyen, S., De Deyne, S., Storms, G. (2015). \nPredicting lexical norms using a word association corpus.In Pro-\nceedings of the 37th Annual Conference of the Cognitive Science \nSociety, pp. 2463–2468. https:// doi. org/ 10. 5334/ joc. 50\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, \nA. N., Kaiser, L., Polosukhin, I. (2017). Attention Is All You Need. \nhttps:// doi. org/ 10. 48550/ ARXIV. 1706. 03762.\nVerheyen, S., De Deyne, S., Linsen, S., & Storms, G. (2020). Lexicose-\nmantic, affective, and distributional norms for 1,000 Dutch adjec-\ntives. Behavior Research Methods, 52(3), 1108–1121. https:// doi. \norg/ 10. 3758/ s13428- 019- 01303-4\nVõ, M. L. H., Conrad, M., Kuchinke, L., Urton, K., Hofmann, M. J., & \nJacobs, A. M. (2009). The Berlin Affective Word List Reloaded \n(BAWL-R). Behavior Research Methods, 41(2), 534–538. https:// \ndoi. org/ 10. 3758/ BRM. 41.2. 534\nVon Platen, P. (2021, May 19). Bert base German uncased [Model]. \nHugging Face. Retrieved 20 January 2022, from https:// huggi  \nngface. co/ dbmdz/ bert- base- german- uncas ed/ tree/ main\nWarriner, A. B., Kuperman, V., & Brysbaert, M. (2013). Norms of \nvalence, arousal, and dominance for 13,915 English lemmas. \nBehavior Research Methods, 45(4), 1191–1207. https:// doi. org/ \n10. 3758/ s13428- 012- 0314-x\nWilliamson, S., Harpur, T. J., & Hare, R. D. (1991). Abnormal process-\ning of affective words by psychopaths. Psychophysiology, 28(3), \n260–273. https:// doi. org/ 10. 1111/j. 1469- 8986. 1991. tb021 92.x\nYao, Z., Yu, D., Wang, L., Zhu, X., Guo, J., & Wang, Z. (2016). Effects \nof valence and arousal on emotional word processing are modu-\nlated by concreteness: Behavioral and ERP evidence from a lexi-\ncal decision task. International Journal of Psychophysiology, 110, \n231–242. https:// doi. org/ 10. 1016/j. ijpsy cho. 2016. 07. 499\nYao, Z., Wu, J., Zhang, Y., & Wang, Z. (2017). Norms of valence, \narousal, concreteness, familiarity, imageability, and context avail-\nability for 1,100 Chinese words. Behavior Research Methods,  \n49(4), 1374–1385. https:// doi. org/ 10. 3758/ s13428- 016- 0793-2\nYin, W., Kann, K., Yu, M., & Schütze, H. (2017). Compara-\ntive study of CNN and RNN for natural language processing. \narXiv:1702.01923. Retrieved 20 January 2022, from https:// doi.  \norg/ 10. 48550/ arXiv. 1702. 01923\nYu, Y., Si, X., Hu, C., & Zhang, J. (2019). A review of recurrent neural \nnetworks: LSTM cells and network architectures. Neural Compu-\ntation, 31(7), 1235–1270. https:// doi. org/ 10. 1162/ neco_a_ 01199\nOpen practices statement All the data, code, and tools created as a part \nof the above research is made freely available under the following links:\nGitHub repository:https:// github. com/ hplis iecki/ affect_ predi ction\nGoogle Colab project:https:// colab. resea rch. google. com/ drive/ \n1Cjce jg1Ad DhsZW s4Vio Q5tT8 B496j gFZ\nStimuli Descent data: https:// osf. io/ cug92/? view_ only= 6f246 \n610bc 0b43c c9e98 d7c97 8f2f6 fa\nPublisher's note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations."
}