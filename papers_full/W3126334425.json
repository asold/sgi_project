{
    "title": "Adaptive Semiparametric Language Models",
    "url": "https://openalex.org/W3126334425",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5060347694",
            "name": "Dani Yogatama",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5038992613",
            "name": "Cyprien de Masson d’Autume",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5014554970",
            "name": "Lingpeng Kong",
            "affiliations": [
                "Google (United States)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2995154514",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2962964385",
        "https://openalex.org/W2963938518",
        "https://openalex.org/W2963631907",
        "https://openalex.org/W2963824800",
        "https://openalex.org/W2936652946",
        "https://openalex.org/W2973727699",
        "https://openalex.org/W3007672467",
        "https://openalex.org/W2963034893",
        "https://openalex.org/W2073667556",
        "https://openalex.org/W2583010282",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2752497102",
        "https://openalex.org/W3126425262",
        "https://openalex.org/W2995575179",
        "https://openalex.org/W3125056032",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W2963018920",
        "https://openalex.org/W2571859396",
        "https://openalex.org/W2971869958",
        "https://openalex.org/W2953044442",
        "https://openalex.org/W3102839769",
        "https://openalex.org/W3015468748",
        "https://openalex.org/W2963494889",
        "https://openalex.org/W3098903812",
        "https://openalex.org/W2121853679",
        "https://openalex.org/W3034696692",
        "https://openalex.org/W3100714086",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2994673210",
        "https://openalex.org/W2970172215"
    ],
    "abstract": "We present a language model that combines a large parametric neural network (i.e., a transformer) with a non-parametric episodic memory component in an integrated architecture. Our model uses extended short-term context by caching local hidden states -- similar to transformer-XL -- and global long-term memory by retrieving a set of nearest neighbor tokens at each timestep. We design a gating function to adaptively combine multiple information sources to make a prediction. This mechanism allows the model to use either local context, short-term memory, or long-term memory (or any combination of them) on an ad hoc basis depending on the context. Experiments on word-based and character-based language modeling datasets demonstrate the efficacy of our proposed method compared to strong baselines.",
    "full_text": "Adaptive Semiparametric Language Models\nDani Yogatama, Cyprien de Masson d’Autume, Lingpeng Kong\nDeepMind\nLondon, United Kingdom\n{dyogatama,cyprien,lingpenk}@google.com\nAbstract\nWe present a language model that combines a\nlarge parametric neural network (i.e., a trans-\nformer) with a non-parametric episodic mem-\nory component in an integrated architecture.\nOur model uses extended short-term con-\ntext by caching local hidden states—similar\nto transformer-XL—and global long-term\nmemory by retrieving a set of nearest neigh-\nbor tokens at each timestep. We design a\ngating function to adaptively combine mul-\ntiple information sources to make a predic-\ntion. This mechanism allows the model to\nuse either local context, short-term memory,\nor long-term memory (or any combination\nof them) on an ad hoc basis depending on\nthe context. Experiments on word-based and\ncharacter-based language modeling datasets\ndemonstrate the efﬁcacy of our proposed\nmethod compared to strong baselines.\n1 Introduction\nHuman language processing is facilitated by com-\nplex systems interacting together. A core compo-\nnent that enables such a process is human memory.\nMemory in humans consists of specialized systems,\nwhich forms a basis for intelligent behaviors (Tul-\nving, 1985; Rolls, 2000; Eichenbaum, 2012). For\nlanguage processing, working (short-term) memory\nis a temporary storage that can be used to compre-\nhend sentences and follow conversations. Episodic\n(long-term) memory stores individual experience\nand events. Semantic memory stores facts and\nknowledge about words and concepts.1\nIn artiﬁcial language processing systems (e.g.,\nlanguage models), a popular approach to design\na better model is by encoding all of the desired\nknowledge (e.g., to produce grammatical sentences,\nprocess long text, remember events, etc.) in the\n1We refer readers to Nematzadeh et al. (2020) for discus-\nsions on human and artiﬁcial language processing memory\nsystems.\nweights of a large parametric neural network via\nend-to-end training. We see an increasingly larger\ntransformer become a better language model (Rad-\nford et al., 2018, 2019; Shoeybi et al., 2019; Brown\net al., 2020). In this scale approach, the knowledge\nis implicitly represented in the weights of a para-\nmetric neural network, and it is not straightforward\nto interpret whether a model contains a particular\nknowledge without asking the model to produce a\nresponse—e.g., via a cloze-style question (Petroni\net al., 2020) or a prompt (Brown et al., 2020).\nAn alternative strategy is to design a modular\narchitecture that separates memory storage and\ncomputational processing, where each module has\na clear purpose. Recent progress in memory-\naugmented neural networks has given rise to many\nvariants of memory-augmented transformer lan-\nguage models that fall under this category. For\nexample, attempts to incorporate extended local\ncontext to a neural network—such as those found\nin neural cache (Grave et al., 2017c), transformer-\nXL (Dai et al., 2019) compressive transformer (Rae\net al., 2020), performers (Choromanski et al., 2021),\nlongformer (Beltagy et al., 2020), and reformer\n(Kitaev et al., 2020)—can be seen as models of\nworking memory. Models of episodic memory\ninclude kNN-LM (Khandelwal et al., 2020) and\narchitectures that are designed for more compli-\ncated tasks such as question answering (de Mas-\nson d’Autume et al., 2019; Guu et al., 2020) and\nmachine translation (Khandelwal et al., 2021). In\nmachine learning and natural language processing,\nmemory-augmented neural networks is used to re-\nfer to all types of memory systems.\nIn this paper, inspired by the modular design of\nhuman memory systems, we present a language\nmodel architecture (SPALM) with storage modules\nthat resemble working and episodic memory sys-\ntems, which we combine with a large parametric\nneural network that is responsible for computa-\ntion (§2). Our hypothesis is that encouraging each\narXiv:2102.02557v1  [cs.CL]  4 Feb 2021\ncomponent to focus on a speciﬁc function (e.g.,\nstoring long-term information, capturing extended\ncontext, modeling local information) facilitates eas-\nier training that produces an overall better language\nmodel.2\nSpeciﬁcally, we follow transformer-XL (Dai\net al., 2019) to capture extended context by caching\nhidden states in a temporary short-term memory.\nFor long-term context, we use a persistent key-\nvalue database and perform sparse retrieval with\n(approximate) k-nearest neighbors. In contrast to\nprevious language models that either interpolate\noutput probabilities (Merity et al., 2017; Grave\net al., 2017c; Khandelwal et al., 2020; Kassner and\nSchutze, 2020) or use input concatenation (Guu\net al., 2020; Xu et al., 2020) to combine informa-\ntion from different sources, we design a context-\ndependent gating mechanism to incorporate local,\nextended, and global context. We discuss similari-\nties and differences to related work in §3.\nIn language modeling, many tokens can be pre-\ndicted from their local context without requir-\ning long-term information. Our model can adap-\ntively decide whether the current (local) context\nis enough, or whether it needs to use information\nfrom the short-term and/or long-term memory.\nIn §4, we compare SPALM with strong\nbaselines—including transformer-XL and kNN-\nLM—on word-based and character-based language\nmodeling. Our positive results establish the beneﬁt\nof the proposed architecture. They also indicate the\ngenerality of our approach and its potential appli-\ncability to other sequence modeling tasks.\nWe analyze howSPALM uses long vs. short-term\ncontext (§5) to better understand how the model\noperates when making predictions. We conclude\nby discussing limitations and future directions (§6).\n2 Model\nWe consider a language model that takes as in-\nput a sequence of words x≤t = {x0,...,x t}and\noutputs a probability distribution of the next word\np(xt+1 | x≤t; W). Given a corpus of T words,\nthe log likelihood of the corpus is:\nL =\nT∑\nt=0\nlog p(xt+1 | x≤t; W),\n2We note that SPALM is not intended to be a model of\nhuman language processing system. We merely take inspira-\ntions from human memory systems to design a better artiﬁcial\nlanguage model.\n<latexit sha1_base64=\"6ahye9qvoxMkDdIdNs1Pc2ncap4=\">AAADC3icfVLLihNBFK20rzE+ZkaXbjoGQURD96A47gZ04UYcwUwG0k24XX07KVKPpqo6k1D0J7hwq5/hTtz6Ef6E32B1HjCZGbxQ1OHcc+veOtys5MzYKPrTCq5dv3Hz1s7t9p279+7v7u0/ODGq0hT7VHGlTzMwyJnEvmWW42mpEUTGcZBN3zb5wQy1YUp+tosSUwFjyQpGwXpqMB85++KgHu11o160jPAyiNegS9ZxPNpv/U1yRSuB0lIOxgzjqLSpA20Z5Vi3k8pgCXQKYxx6KEGgSd1y3jp84pk8LJT2R9pwyZ6vcCCMWYjMKwXYibmYa8grc5nY7pzPWGnWveer5tuT2eIwdUyWlUVJV4MVFQ+tChuzwpxppJYvPACqmf9bSCeggVpvabudSDyjSgiQuUtmXlkP49QlzVxZ4bpxXW9LDJ5TUOBXSayuNu9kiueNDWojfIfea40ffPXHEjVYpZ+5BPRYwLz23o+T5w36n5DJjdAj/4OlKRytAy6UsWcTZrF2y6vzJuqMNSw6cdRpNiS+uA+XwclBL37Viz697B4drndlhzwij8lTEpPX5Ii8J8ekTyiZkq/kG/kefAl+BD+DXytp0FrXPCRbEfz+ByQI/ic=</latexit>\nx t \u0000 2\n<latexit sha1_base64=\"MMjGh9omJ76iEu3bq7FtJYQil/A=\">AAADC3icfVLLihNBFK20rzE+ZkaXbjoGQURDt4w47gZ04UYcwUwG0k24XX07KVKPpqo6k1D0J7hwq5/hTtz6Ef6E32B1HjCZGbxQ1OHcc+veOtys5MzYKPrTCq5dv3Hz1s7t9p279+7v7u0/ODGq0hT7VHGlTzMwyJnEvmWW42mpEUTGcZBN3zb5wQy1YUp+tosSUwFjyQpGwXpqMB85++KgHu11o160jPAyiNegS9ZxPNpv/U1yRSuB0lIOxgzjqLSpA20Z5Vi3k8pgCXQKYxx6KEGgSd1y3jp84pk8LJT2R9pwyZ6vcCCMWYjMKwXYibmYa8grc5nY7pzPWGnWveer5tuT2eIwdUyWlUVJV4MVFQ+tChuzwpxppJYvPACqmf9bSCeggVpvabudSDyjSgiQuUtmXlkP49QlzVxZ4bpxXW9LDJ5TUOBXSayuNu9kiueNDWojfIfea40ffPXHEjVYpZ+5BPRYwLz23o+T5w36n5DJjdAj/4OlKRytAy6UsWcTZrF2y6vzJuqMNSw6cdRpNiS+uA+XwcnLXvyqF3066B4drndlhzwij8lTEpPX5Ii8J8ekTyiZkq/kG/kefAl+BD+DXytp0FrXPCRbEfz+BylM/ik=</latexit>\nx t \u0000 4\n<latexit sha1_base64=\"Asr7rcDM9qr5LLA8/AiDNdc3Ho0=\">AAADC3icfVLLihNBFK20rzE+ZkaXbjoGQURDt/gYdwO6cCOOYCYD6SZUV98kRerRVN3OJBT9CS7c6me4E7d+hD/hN1iddGAyM3ihqMO559a9dbhZIbjFKPrTCq5cvXb9xs7N9q3bd+7u7u3fO7a6NAz6TAttTjJqQXAFfeQo4KQwQGUmYJDN3tb5wRyM5Vp9xmUBqaQTxcecUfTUYDFy+OxVNdrrRr1oFeFFEDegS5o4Gu23/ia5ZqUEhUxQa4dxVGDqqEHOBFTtpLRQUDajExh6qKgEm7rVvFX4yDN5ONbGH4Xhij1b4ai0dikzr5QUp/Z8riYvzWVyu3M+54Vtei/Wzbcnw/FB6rgqSgTF1oONSxGiDmuzwpwbYCiWHlBmuP9byKbUUIbe0nY7UXDKtJRU5S6Ze2U1jFOX1HNlY9eNq2pbYuGMglFxmQRNuXkn0yKvbdAb4TvwXhv44Ks/FmAoavPEJdRMJF1U3vtJ8rRG/xNytRF65H+wMkUAOiqktng65QiVW12dN1FnYuiyE0edekPi8/twERw/78Uve9GnF93Dg2ZXdsgD8pA8JjF5TQ7Je3JE+oSRGflKvpHvwZfgR/Az+LWWBq2m5j7ZiuD3Py6Q/is=</latexit>\nx t \u0000 6\n<latexit sha1_base64=\"ERSsJ+NTIWy7GQlhbqLZzZPPQEk=\">AAADEXicfVLNbhMxEHaWvxJ+2sKRy4YICQGKdhGIcqsEBy6IIpG2UnYVzXoniVV7vdizaSJrn4IDV3gMbogrT8BL8Ax48yM1bcVIlj99841n/GmyUgpLUfSnFVy5eu36ja2b7Vu379zd3tm9d2h1ZTj2uZbaHGdgUYoC+yRI4nFpEFQm8Sg7edPkj6ZorNDFJ5qXmCoYF2IkOJCn0mQC5Gb10NHTuB7udKNetIjwIohXoMtWcTDcbf1Ncs0rhQVxCdYO4qik1IEhwSXW7aSyWAI/gTEOPCxAoU3dYuo6fOSZPBxp409B4YI9W+FAWTtXmVcqoIk9n2vIS3OZ2uycT0VpV71ny+abk9FoL3WiKCvCgi8HG1UyJB02loW5MMhJzj0AboT/W8gnYICTN7bdTgo85VopKHKXTL2yHsSpS5q5spHrxnW9KbF4RsFBXiYhU63fybTMGxv0WvgWvdcG3/vqDyUaIG2euATMWMGs9t6Pk2cN+p9QFGuhR/4HC1MkkgOptKXTiSCs3eLqvI46YwPzThx1mg2Jz+/DRXD4vBe/7EUfX3T391a7ssUesIfsMYvZK7bP3rED1mecfWZf2Tf2PfgS/Ah+Br+W0qC1qrnPNiL4/Q/YCgEA</latexit>\nˆx t +1\n<latexit sha1_base64=\"DvYoEJ4Sa0ro6aDLk+LrcAiS4Lw=\">AAADCXicfVLNihNBEO6Mf2v829Wjl4lBEJEwI4p6C+jBi7iC2V3IhFDTU8k22z9Dd002oZkn8OBVH8ObePUpfAmfwZ78wGZ3saDpj6++6qr+qLyUwlGS/GlFV65eu35j52b71u07d+/t7t0/cKayHAfcSGOPcnAohcYBCZJ4VFoElUs8zE/eNvnDGVonjP5MixJHCqZaTAQHCtRgPvZUj3e7SS9ZRnwRpGvQZevYH++1/maF4ZVCTVyCc8M0KWnkwZLgEut2VjksgZ/AFIcBalDoRn45bR0/DkwRT4wNR1O8ZM9WeFDOLVQelAro2J3PNeSluVxtdy5monTr3vNV8+3JaPJ65IUuK0LNV4NNKhmTiRur4kJY5CQXAQC3Ivwt5sdggVMwtN3ONJ5yoxTowmezoKyH6chnzVz5xHfTut6WODyj4CAvk5CtNu/kRhaNDWYjfIfBa4sfQvXHEi2QsU99BnaqYF4H76fZswb9Tyj0RhhQ+MHSFInkQSrj6PRYENZ+eXXeJJ2phUUnTTrNhqTn9+EiOHjeS1/2kk8vuv3+eld22EP2iD1hKXvF+uw922cDxplgX9k39j36Ev2Ifka/VtKota55wLYi+v0P3Gf9vA==</latexit>\nx t\nFigure 1: Our language model architecture has three\nmain components: (i) a transformer that processes the\ncurrent local context, (ii) a short-term memory mod-\nule which stores hidden states from an extended con-\ntext, (iii) and a key-value (hidden state-output token)\ndatabase that stores compressed long-term context. At\neach timestep, our model combines the current context\nand short-term memory with a mechanism similar to\ntransformer-XL. It then retrieves a set of past output\ntokens that are used in a similar context from the long-\nterm memory module. These past output tokens are\nthen encoded and aggregated to a single vector that\nrepresents long-term information. We use a context-\ndependent gate to combine information from multiple\nsources for making a ﬁnal prediction.\nwhere x0 is the start of sentence symbol.\nSPALM consists of three main components: (i)\na large parametric neural network in the form of\na transformer to process local context, (ii) a short-\nterm memory to store extended context, and (ii)\na non-parametric episodic memory module that\nstores information from long-term context. We in-\ntegrate these components in a single architecture\nwith a gating mechanism. Figure 1 shows an illus-\ntration of our model, which we discuss in details\nbelow.\n2.1 Base model\nWe use transformer (Vaswani et al., 2017) as our\nbase model. Given the input sequence x≤t, trans-\nformer performs multiple layers of self-attention\nbetween every pair of tokens in the input sequence\nto produce token representations.\nA core limitation of transformer is that its com-\nputational complexity is quadratic in the input\nsequence length. As a result, instead of consid-\nering all previous tokens x≤t, transformer trun-\ncates the input to be the most recent N words\n˜x≤t = {xt−N+1,...,x t}and only operates on\nthis ﬁxed-length window in practice. A large trans-\nformer, no matter how many parameters it has, is\nlimited by the input sequence length.\n2.2 Short-term memory\nWe use transformer-XL (Dai et al., 2019) as our\nworking memory model. Given the current con-\ntext ˜x<t, denote the extended context of length M\nby ˜x≤t−N = {xt−N−M+1,...,x t−N }. In other\nwords, the extended context is the M tokens prior\nto the current context. In transformer-XL, hidden\nstates for ˜x≤t−N (obtained from a previous compu-\ntation when predicting xt−N+1) are cached. They\nare then used as additional states that can be at-\ntended to during the forward pass when computing\nhidden states for the current context ˜x≤t, but the\nvalues of the states are not updated during the back-\nward pass to save computation time.\nFormally, denote the hidden state for xt at\nlayer r by hr\nt . Denote the hidden states associ-\nated with the current (truncated) context ˜x<t by\nHr = [hr\nt−N ,... hr\nt ] and the hidden states associ-\nated with the extended context ˜x<t−N by Er =\n[SG(hr\nt−N−M+1),... SG(hr\nt−N )], where SG is the\nstop gradient function. Together, Hr and Er are\nused as an input to an attention function (with rel-\native positional encodings) where each vector is\ntransformed into a key, value, query triplet which\nare used to produce Hr+1 (i.e., hidden states for\nthe next layer).\nNote that while transformer-XL extends the con-\ntext window, the extra information is still “local”\nwith respect to the sequence.\n2.3 Long-term memory\nWe design a long-term episodic memory module\nthat allows our language model to retrieve “global”\ninformation. The long-term memory module is\nimplemented as a key-value database. The key is\na vector representation of a context ˜x≤i (i.e., we\ncompress ˜x≤i into a vector). Each context is paired\nwith the output token for that context xi+1, which\nis stored as the value. In our experiments, we store\na key-value entry for each context-token pair in the\ntraining corpus, so the number of entries is equal\nto the number of tokens in the training corpus.\nThere are many choices that can be used for the\nkey representation, which we denote by di. For\nexample, we can use hR\ni or a separate pretrained\nencoder such as BERT (Devlin et al., 2018). We\npretrain a vanilla transformer language model and\nuse the ﬁnal-layer hidden state for di.\nFor predicting a new token xt+1 given ˜x≤t, we\nﬁrst obtain dt from the separate pretrained lan-\nguage model. We then use dt to do a k-nearest\nneighbor search on the database. Since dt is a con-\ntextual representation, this search ﬁnds contexts\nthat are similar to ˜x<t in the database. For the top\nksuch contexts, we retrieve the values associated\nwith those contexts, which are the output (next)\ntokens when those contexts are encountered in the\npast. Denote the output tokens retrieved from the\ndatabase by y1,...y K.\nFor each yk, we create a vector representation\nyk by using the same word embedding matrix that\nis used in our base model. We then combine the\nlong-term memory information obtain from the\ndatabase with the extended local context with a\ngating mechanism as follows:\nmt =\nK∑\nk=1\nexp y⊤\nk hR\nt∑K\nj=1 exp y⊤\nj hR\nt\nyk\ngt = σ(w⊤\ng hR\nt )\nzt = (1−gt) ⊙mt + gt ⊙hR\nt\np(xt+1 |x≤t) =softmax(zt; W),\nwhere wg is a parameter vector, σis the sigmoid\nfunction, and W is the word embedding matrix that\nis shared for input and output word embeddings\n(Inan et al., 2017).3\nIn the above formulation, we ﬁrst aggregate in-\nformation from y1,...y K with a simple attention\nmechanism using hR\nt as the attention query.4 We\nthen use a context-dependent gate gt that decides\nhow much the model needs to use local informa-\ntion (hR\nt ) versus long-term information ( mt) for\nmaking the current prediction based on the current\ncontext. Note that given the database, the only\nadditional parameter that needs to be trained is\n3In a preliminary experiment, we incorporate the nearest\nneighbor distance as a bias term in the computation of mt.\nHowever, this does not improve performance, so we use the\nabove equation in the ﬁnal model.\n4It is possible to ﬁrst transform hR\nt (e.g., by doing a linear\nprojection) before using it as an attention query. We choose\nan untransformed version in our experiments to minimize the\nnumber of new parameters in SPALM. We leave explorations\non the best transformation of hR\nt to future work.\nwg. The result is a language model that is able to\nrely on short-term context for “easy” predictions\nwhile using long-term context for “hard” predic-\ntions by adaptively combining short-term and long-\nterm memory at the architectural level.\n2.4 Training details\nAs discussed previously, we ﬁrst train a standard\ntransformer language model and use it as an en-\ncoder to compute key representations di for the\nepisodic memory database. Since our training\ndatasets contain hundreds of millions of tokens,\nfor computational considerations, we do not update\nthe key representations when training the overall\nmodel. This allows us to ﬁx the set of nearest neigh-\nbors for each token, making training of the overall\nmodel to be almost as fast as a vanilla transformer-\nXL in terms of wall-clock time after we precompute\nneighbors for each token. The value encoder, on\nthe other hand, is updated during training since we\nuse the word embedding matrix to represent yk.\nk-nearest neighbors on hundreds of millions of\ntokens can be computationally expensive. We use\nthe publicly available ScANN5 (Guo et al., 2020)\nto do this efﬁciently, which is a quantization-based\ntechnique to do fast and accurate maximum inner\nproduct search.\nWe note that it is conceptually possible to train\nall components of our model in an end-to-end man-\nner. However, we leave end-to-end training to\nfuture work. In addition, while it is possible to\ncontinually grow the long-term memory module\nby storing new tokens from evaluation data, we\nchoose to do a static evaluation. Therefore, we\ndo not compare with dynamic evaluation models\n(Krause et al., 2018, 2019; Grave et al., 2017a)\nwhich adapt language models to evaluation data.\nWe next discuss comparisons to existing nearest\nneighbor and cache language models.\n3 Comparisons to previous work\nkNN-LM. There are several language models\nthat are related to our proposed method. The closest\none is kNN-LM (Khandelwal et al., 2020), which\nis another language model that is augmented with a\nnearest neighbor retrieval mechanism. kNN-LM is\nan ensemble technique that is designed to be used\nonly at evaluation time. In kNN-LM, a pretrained\nlanguage model (e.g., a transformer) is combined\n5https://github.com/google-research/\ngoogle-research/tree/master/scann\nwith another retrieval-based language model by in-\nterpolating their probabilities: p(xt+1 |x≤t) =\nλpLM(xt+1 |x≤t) + (1−λ)pkNN(xt+1 |x≤t).\nThe interpolation weight λis tuned at the corpus\nlevel on a development set.\nWhile this post hoc integration method used by\nkNN-LM has its merit (e.g., very practical, fast\nto incorporate to any model since it does not re-\nquire additional training), our focus is on designing\na model that combines short-term and long-term\nmemory at the architecture level. Our motivation\nis twofold. First, interpolating the language model\nweights at the corpus level forces the model to use\nthe same interpolation weight λfor pLM and pkNN\nfor each token in the corpus. It cannot adaptively\ncombine short-term and long-term information at\nthe token level based on the context. In addition,\nλneeds to be tuned on an extra development set.6\nSPALM , on the other hand, is able to adjust the\nweights placed on mt and hR\nt when constructing\nzt differently for different tokens. Second, we be-\nlieve that integration of different memory modules\nat the architectural level is a more natural approach\nthat could help pave the way for applications with\nother memory sources (e.g., knowledge bases, im-\nages, videos)—where the memory output is not in\nthe same space as the prediction output (i.e., words)\nand an interpolation technique cannot be used.\nWe compare with kNN-LM in our experiments.\nSince interpolating model probabilities is an en-\nsembling technique that is independent of the ar-\nchitecture, we also show that our language model\ncan be furher ensembled with pkNN if necessary.\nCache-based language models and pointer net-\nworks. Cache-based language models (Grave\net al., 2017c; Merity et al., 2017) store pairs of\nhidden states and output tokens from previously\nseen tokens (within a limited context length) in a\ncache. The best variant of the method uses an inter-\npolation (ensemble) method similar to kNN-LM to\ncombine information from the cache and the back-\nbone language model. This class of models tem-\nporarily stores M past hidden states (typically, in\nthe order of thousands), so it is a working-memory\nmodel as opposed to long-term memory. In addi-\n6We note that it is possible to incorporate this interpolation\ntechnique during the training phase of a language model as\nwell to avoid having to tune λ on a development set. For ex-\nample, Neubig and Dyer (2016) shows how to train a mixture\nof experts language models, where the mixture weights are\ninferred. However, the efﬁcacy of this approach as a memory-\naugmented language model has not been explored.\nDataset # Train # Dev # Test # Vocab\nWikiText 110M 0.2M 0.3M 33,060\nWMT 852M 1M 1M 50,259\nenwik8 94M 5.2M 5.2M 256\nTable 1: Descriptive statistics of datasets used in our\nexperiments. For each split, we show the number of\n(sub)words for WikiText and WMT and the number of\ncharacters for enwik8.\ntion, they also rely on interpolating probabilities\nof a backbone language model and a cache com-\nponent (similar to kNN-LN when the cache size is\nunbounded).\nOther retrieval augmented methods.An early\nversion of a neural language model that includes\na retrieval component is presented in Guu et al.\n(2018). They follow a retrieve-then-edit approach\nto generate a sentence, which requires approximat-\ning an expectation over an edit prior.\nOutside language modeling, there are several re-\ncent retrieval-augmented methods that have been\nused for question answering (de Masson d’Autume\net al., 2019; Guu et al., 2020; Xiong et al., 2021;\nKassner and Schutze, 2020), controllable genera-\ntion (Xu et al., 2020), machine translation (Bapna\nand Firat, 2019; Khandelwal et al., 2021), and one-\nshot learning (Kaiser et al., 2017). These methods\nshare some similarities with our proposed model\nsince it involves a retrieval component. How-\never, the difference in the downstream tasks (lan-\nguage modeling vs. question answering vs. ma-\nchine translation), results in different items that are\nstored in and retrieved from the key-value database.\nFor example, de Masson d’Autume et al. (2019)\nstore and retrieve question-answer pairs, Guu et al.\n(2020) have a database of passages of an article,\nand Khandelwal et al. (2021) use source and target\nsentences. Our gating mechanism resembles the\ngate that is used to incorporate information from a\nnon-parametric memory component to a machine\ntranslation model in Bapna and Firat (2019), al-\nthough the memory entries, the decoder architec-\nture, and the downstream task are different.\nIn addition, these models are only models of\nlong-term memory. Their evaluation tasks often do\nnot need working memory because the entire input\nsequence is short enough that it can be fed as an\ninput to a transformer as a whole.\n4 Experiments\nWe use word-based and character-based English\nlanguage model datasets–WikiText 103, WMT, and\nenwik8–to evaluate our proposed method. We pro-\nvide descriptive statistics in Table 1 and discuss\neach dataset in the respective section below.\n4.1 Implementation details\nWe use Adam (Kingma and Ba, 2015) as our opti-\nmizer. For word-based language modeling, we use\nadaptive softmax (Grave et al., 2017b). We apply\ndropout with a rate of 0.25. All models are trained\non 128 Tensor Processing Units until convergence\nwith batch size 256.\n4.2 WikiText-103\nOur ﬁrst dataset is WikiText-103 (Merity et al.,\n2017). We compare four models: vanilla trans-\nformer, transformer-XL, kNN-LM, and SPALM .\nFor WikiText-103, all of our models have 18 lay-\ners and 512 hidden dimension size with a total of\n142M parameters. We set the sequence length to\n512. For transformer-XL, we set the short-term\nmemory length to 512 during training and 512 or\n3072 at test time. We use 4 nearest neighbors for\nkNN-LM and SPALM and analyze the effect of\nvarying the number of neighbors in §5.4. For kNN-\nLM, we use the transformer-XL model to obtain\npLM, compute pkNN based on the nearest neighbor\ndistance similar to Khandelwal et al. (2020), and\ntune λfrom {0.05,0.1,0.2,0.3,0.4}on the devel-\nopment set\nTable 2 shows perplexity on WikiText103. Our\nimplementation produces results that are in the\nsame range as state-of-the-art numbers, demon-\nstrating the strength of our baselines. Transformer-\nXL outperforms transformer, and interpolating\nthe probability of transformer-XL with kNN (i.e.,\nkNN-LM) improves the result further. This is true\nboth with transformer-XL (short-term) memory\nlength of 512 and 3072. Comparing kNN-LM with\nSPALM, kNN-LM is marginally better on the test\nset even though SPALM is marginally better on the\ndevelopment set.\nWe observe further improvements in SPALM by\ninterpolating its output probability with the output\nprobability from pkNN which is used by kNN-LM,\nresulting in the best model with a perplexity of 17.6.\nWe ﬁnd this interesting sinceSPALM and pkNN uses\nthe exact same four neighbors for each token. It in-\ndicates that there are some complementary beneﬁts\nin incorporating long-term memory into training\nand interpolating probabilities at test time.\nModel # Params Dev Test\nTransformer-XLa 257M - 18.3\nAdaptive Inputb 247M 18.0 18.7\nCompressivec 257M 16.0 17.1\nkNN-LMd 247M 16.1 16.1\nM = 512\nTransformer 142M 20.8 21.8\nTransformer-XL 142M 18.7 19.6\nkNN-LM 142M 18.1 18.5\nSPALM 142M 17.9 18.8\n↪→+ kNN 17.6 18.0\nM = 3072\nTransformer-XL 142M 18.3 19.1\nkNN-LM 142M 17.7 18.0\nSPALM 142M 17.4 18.3\n↪→+ kNN 17.2 17.6\nTable 2: Perplexity on WikiText-103. The top rows\ncontain results taken from other papers: (a) transformer-\nXL (Dai et al., 2019), (b) adaptive input embeddings\n(Baevski and Auli, 2019), (c) compressive transformer\n(Rae et al., 2020), and (d) kNN-LM (Khandelwal et al.,\n2020). The (log likelihood) difference between the best\nmodel (SPALM + kNN) and transformer-XL on the test\nset is statistically signiﬁcant (Wilcoxon signed-rank test,\np< 0.05).\n4.3 WMT\nIn the second experiment, our goal is to evalu-\nate on a much larger dataset. We construct a lan-\nguage modeling dataset from the English portion\nof the WMT 2019 dataset, publicly available at\nhttp://www.statmt.org/wmt19/. WMT\ncontains news articles from different months. We\nuse articles from January to October for training, a\nportion of articles in November for development,\nand a portion of articles in December for test.7 The\nresulting WMT dataset is approximately ten times\nlarger than the WikiText-103 dataset.\nSimilar to the previous experiment, we evaluate\nmodels with 18 layers and 512 hidden dimension\nsize with a total of 148 million parameters. We\nset the sequence length to 512, the transformer-\nXL short-term memory length to 512 for training\nand evaluation, and the number of neighbors for\nSPALM and kNN-LM to 4.\nTable 3 shows results on this dataset. Consistent\nwith the previous experiment, kNN-LM outper-\n7We sample articles written in November and December\nin chronological order to create development and test sets of\napproximately 1 million tokens (there are almost 100 million\ntokens if we use all of the articles in each month).\nforms transformer-XL and transformer. SPALM\noutperforms all of them by a considerable margin\non the test set. Unlike WikiText-103, we observe no\nfurther improvement interpolating the probabilities\nof SPALM with pkNN. The results also indicate that\nwhen the distributions of the dev and test sets can\nbe different (e.g., articles from different months),\nkNN-LM that relies on tuning λon the dev set is\nmore sensitive to performance discrepancy between\nthe dev and test sets.\nModel # Params Dev Test\nTransformer 148M 16.0 16.3\nTransformer-XL 148M 15.6 15.5\nkNN-LM 148M 13.1 15.2\nSPALM 148M 13.0 14.0\nTable 3: Perplexity on the WMT dataset. The (log\nlikelihood) difference between SPALM and transformer-\nXL on the test set is statistically signiﬁcant (Wilcoxon\nsigned-rank test, p< 0.05).\n4.4 enwik8\nIn the third experiment, we evaluate our models\non character-level language modeling. Compared\nto word-level language modeling, character-level\nhas a much smaller output space (in the order of\nhundreds instead of tens of thousands) and has a dif-\nferent characteristic in how much local vs. global\ncontexts are needed to make a good prediction.\nThe enwik8 dataset (Hutter, 2012) is a bench-\nmark for character-level language modeling. We\nuse a 24 layer model with 512 hidden size. In to-\ntal, our model has 100 million parameters. We\nset the sequence length to 768, the transformer-XL\nshort-term memory length to 1536 for training and\n4096 for evaluation. Since character-level language\nmodels has a much smaller output space, we only\nretrieve two neighbors per character.\nWe show the results in Table 4. Unlike the pre-\nvious two word-level language modeling results,\nkNN-LM underperforms transformer-XL. How-\never, SPALM outperforms all other models. We\nnote that a decrease of 0.01 is considerable on this\ndataset under the BPC metric. Similar to WMT,\ninterpolating the probabilities of SPALM with pkNN\ndoes not improve performance. These results high-\nlight a major strength of our proposed model: uni-\nformly setting interpolation weights at the corpus\nlevel decreases performance (i.e., kNN-LM), but\nallowing the model to ﬂexibly decide when to use\nlong-term vs. short-term memory is beneﬁcial.\nFor Warren & Wednesday brieﬂy a 5 billion to equity\nWarren may Tuesday praised wiping 16 trillion in funding\nPerhaps Warren has Sunday stood breaking 10 billion for federal\nLike Warren , Monday defended using 166 trillion in spending\nElizabeth Warren on Friday proposed $ 20 trillion in federal\ngrants in 10 course eight . ﬁght even care for\nfunding over the next three . upgrade them coverage for\nfunds over 10 next ﬁve in improve American - to\n, over a next 10 , invest a insurance services\nspending over the next decade to provide health care to\nmore community as the rates . the middle class\neveryone child , a taxes on the wealthy class\nsome baby , co taxes . the middle class\nevery American by triggering taxes on all middle class\nevery American without raising taxes on the middle class\nFigure 2: A sequence of words from WMT and its four nearest neighbors at each position. We break down\nthe sequence into four blocks. The bottom row of each block in blue represents the original sequence, which\nis Elizabeth Warren on Friday ... the middle class. Each row above it represents a nearest\nneighbor token (starting from the ﬁrst neighbor at the second-bottom to the fourth neighbor at the top) that is used\nwhen predicting that particular word. We highlight matching neighbor–target words in green. We provide a more\ndetailed discussion in §5.1.\nSince character-level and word-based language\nmodeling are characteristically different, the suc-\ncess of our model on this dataset indicates its appli-\ncability to other sequence modeling problems. We\nleave such explorations to future work.\nModel # Params Dev Test\n18L Transformer-XLa 88M - 1.03\n24L Transformer-XLa 277M - 0.99\nLongformerc 102M - 0.99\nCompressived 277M - 0.97\nTransformer 104M 1.07 1.05\nTransformer-XL 104M 1.03 1.01\nkNN-LM 104M 1.04 1.02\nSPALM 104M 1.02 1.00\nTable 4: Bits per character (BPC) on enwik8. The\ntop rows contain results taken from other papers: (a)\ntransformer-XL (Dai et al., 2019), (b) longformer (Belt-\nagy et al., 2020), and (c) compressive transformer (Rae\net al., 2020). The (log likelihood) difference between\nSPALM and transformer-XL on the test set is statistically\nsigniﬁcant (Wilcoxon signed-rank test, p< 0.05).\n5 Analysis\nWe have demonstrated the efﬁcacy of our proposed\nmethod on three language modeling tasks. In this\nsection, we analyze the model to gain more insights\ninto how it works.\n5.1 Examples of neighbors\nWe inspect the neighbor tokens that are retrieved\nfrom the long-term memory for news articles in the\nWMT development dataset. We provide a cherry-\npicked example in Figure 2. As the model sees\nmore tokens in a sequence, the long-term mem-\nory model becomes more accurate. We observe\ninteresting cases such as when predicting a named\nentity (e.g., Elizabeth Warren), even if the\nlong-term memory model fails to retrieve the cor-\nrect ﬁrst name, it usually is able to retrieve the\ncorrect last name after seeing the ﬁrst name (be-\ncause the entity exists in the training corpus). We\nobserve this phenomenon in many other examples\nas well. We can also see that the retrieved neigh-\nbors are generally relevant even when they do not\nmatch a target word exactly—e.g., when predicting\nnames of days, dollar amounts, time quantiﬁers,\nand common phrases.\nWe next investigate neighbors on enwik8 devel-\nopment set (Figure 3). We observe that information\nfrom the long-term memory helps when completing\ncommon words (e.g., before and invasion),\nnamed entities (e.g., Soviet), and corpus-speciﬁc\nformats (e.g., double square brackets).\nWe note that the above examples are only pro-\nvided to give a better insight into our model. It is\nentirely plausible that a baseline parametric model\nis already able to predict correctly from the local\ncontext. Nonetheless, directly providing this in-\nU o e h a t f o r e t h i d e n i e t - U n t a\nh e r h e f o r e t h e f a v i e t ’ U n v a\nE v e n b e f o r e t h e S o v i e t i n v a\ns i o n , b n t h e [ n d o f t [ 1 6 7 5 ] ]\ns i o n a n t A h e h n d o f t [ 4 3 9 9 ] ]\ns i o n a t t h e e n d o f [ [ 1 9 7 9 ] ]\nFigure 3: A sequence of characters from enwik8 and its two nearest neighbors at each position. We break down\nthe sequence into two blocks. The bottom row of each block in blue represents the original character sequence\n, which is Even before ... [[1979]]. The two rows above it represent the nearest neighbors (the ﬁrst\nnearest neighbors at the second bottom row and the second nearest neighbors at the top row) that are used when\npredicting that particular character. We highlight matching neighbor–target characters in green. We provide a more\ndetailed discussion in §5.1.\n... Several companies have pulled their advertising from the TV show following the revelations ...\n... Liberal Democrat leader Jo Swinson has said she would work with Donald Trump in government as ...\n... Additionally , the airline has purchased six Boeing 787 - 9 Dream liner aircraft that are scheduled ...\nFigure 4: Three example sequences from the WMT test set. We highlight words where both pTXL and pSPALM are\nlarger than ptransformer + 0.1 in green and pSPALM >pTXL + 0.1 in blue. See §5.2 for details.\nformation as a long-term context helps our model\nlearn better, as evident from the superior perfor-\nmance of SPALM on our three evaluation datasets.\n5.2 Output analysis\nWe search for predictions where SPALM signif-\nicantly outperforms transformer-XL and trans-\nformer to understand when modeling local informa-\ntion is sufﬁcient (i.e., vanilla transformer), when\nadding extended context helps (i.e., transformer-\nXL), and when storing long-term information is\nuseful (i.e., SPALM ). We show three examples\nfrom the WMT test set in Figure 4.\nWhile it is difﬁcult to ﬁnd consistent patterns,\nwe observe that SPALM is generally better\nthan both transformer and transformer-XL for\npredicting (completing) common phrases and\nnamed entities (that exist in the training set),\nespecially when they are encountered for the ﬁrst\ntime and have not appeared in the extended context\n(e.g., pulled their advertising from,\nLiberal Democrat, Jo Swinson,\nBoeing 787-9 Dreamliner).\nOn the other hand, we also see a few cases when\ntransformer-XL outperforms SPALM . These are\nusually associated with scenarios where the same\nword has appeared in the extended context. While\nSPALM uses information from the extended con-\ntext as well, the probability is smoothed over by\ninformation from the long-term memory, resulting\nin a more peaky distribution for transformer-XL.\n5.3 Gate vectors\nOur model has a gating mechanism to regulate in-\nformation ﬂow from the current context, short-term,\nand long-term memory. We analyze the values of\nthe gate for tokens in WMT and enwik8. Figure 5\nshows histograms of the distribution of gate values.\n0.0 0.2 0.4 0.6 0.8 1.0\n0.0 0.2 0.4 0.6 0.8 1.0\nFigure 5: Distributions of values of z for WMT (left)\nand enwik8 (right) development sets.\nWe observe different characterstics for WMT\nand enwik8. On enwik8, the gate values are con-\ncentrated around 1. This indicates that the model\nrelies on local context most of the time. This can\nexplain why kNN-LM does not work well on this\ndataset. On WMT, the values are less concentrated\naround 1. This suggests that the model uses long-\nterm memory more than on enwik8. SPALM is able\nto learn when the long-term memory is needed and\nwhen it is not in both cases.\nWe next look into the value of the gates for a\nspeciﬁc sequence in the development set in Fig-\nure 6. We note that we only show a small dimen-\nsion subset from the gate vector for readability, so\n1995\n-\nexistence\nits\nin\nappearances\npostseason\nfour\nof\ntotal\na\nmade\nhas\nfranchise\nthe\n,\n1977\nin\ned\nStart\n \ne\nh\nt\n \nd\nn\na\n \n,\n]\n]\nt\np\ny\ng\nE\n \nt\nn\nFigure 6: Heatmaps of z values on a partial sequence from WMT development set (left) and enwik8 (right). Each\nrow is a token (word or character), each colum is a dimension from z. blue indicates value closer to 1.0, whereas\nred indicates value closer to 0.0. The darker the shade the closer the value is to the extreme. We see vertical patterns\non WMT, indicating that these dimensions are reserved to ﬂow information from long-term memory. Horizontal\npatterns on enwik8 indicates the model relies on long-term memory to predict a target token (e.g., when forming the\nword Egypt). The z vector has 512 dimension, we only zoom in to a small dimension subset here. There are more\nhorizontal and vertical patterns on both datasets as a whole.\nwe caution against drawing a conclusion about how\nthe model works from this. Our goal is only to\nget a better understanding of what happens when\nthe model makes predictions. Comparing WMT\nand enwik8, we see that in general on WMT the\nmodel tends to reserve some dimensions to prop-\nagate information from the long-term memory, as\nindicated by vertical red lines. On enwik8, the\nmodel relies on long term information when com-\npleting a known word such as Egypt, as shown\nby more horizontal red patterns when forming this\nword. For other characters, the value of the gates\nare closer to one, which shows that the model relies\nmore on local and extended short-term context.\n5.4 Number of neighbors\nWe use four neighbors for our word-based and two\nneighbors for our character-based language models.\nThese values are chosen from preliminary experi-\nments on a small subset of the datasets.\nWe show SPALM perplexity on development set\nfor WikiText-103 when we vary the number of\nneighbors in Table 5. We see that using one nearest\nneighbor is enough to obtain good performance,\nwith a slight advantage when we use four neighbors.\nThe performance starts to degrade as we use 8 and\n16 neighbors. We choose to use four neighbors in\nour experiments since kNN-LM–which also uses\nthe same set of neighbors–performs better with four\nneighbors instead of one, and we want to keep the\ncomparison as fair as possible.\nOne notable difference between our neighbors\nand those that are used in kNN-LM (Khandelwal\net al., 2020) is that we do not limit the search of the\nneighbors to the same token as the current input\n# NNs Perplexity\n1 18.0\n2 18.0\n4 17.9\n8 18.2\n16 18.4\nTable 5: SPALM\nperplexity on the\nWikiText-103 develop-\nment set with different\nnumbers of neighbors.\ntoken (I(xi = xt)). While this allows the model\nto combine information from related words (not\nconstrained to an exact match), it could introduce\nnoise when the number of neighbors is large.\nWe observe that our representation learning\nmodel (i.e., the baseline transformer) is able to\nretrieve relevant neighbors most of the time. It re-\ntrieves the exact output token as the ﬁrst neighbor\n33%, 44%, and 70% on WikiText-103, WMT and\nenwik8 development sets respectively.\n6 Discussion\nSummary of contributions. We present a semi-\nparametric language model (SPALM) that combines\nlocal context, short-term memory, and long-term\nmemory to make predictions. Experiments on\nword-based and character-based language models\ndemonstrate the beneﬁt of our proposed method.\nLimitations. The biggest limitation is the ne-\ncessity to retrieve neighbors for each training to-\nken. Such a process—even though can be fully\nparallelized—is time consuming. In our epxeri-\nments, it takes 6-8 hours to obtain neighbors for\nWikiText-103 and enwik8 with 1,000 CPUs and 18\nhours for WMT with 9,000 CPUs.\nFuture directions. Our modular approach that\ncombines multiple memory systems at the architec-\ntural level opens up the possibility to incorporate\nadditional memory from other modalities (e.g., im-\nages) or structured knowledge bases. We also en-\nvision a next-generation model that does not have\nto retrieve information from long-term memory for\nevery token and only does it for those that require\nglobal context. A model that learns how to do\nthis would save a considerable amount of training\nand test time—since it would signiﬁcantly reduce\nthe number of search that needs to be performed.\nOur language model that integrates retrieval into\ntraining is a ﬁrst step in this direction.\nAcknowledgements\nWe thank the action editor (Mihai Surdeanu) and\nthree anonymous reviewers for helpful comments\non an earlier draft of this article.\nReferences\nAlexei Baevski and Michael Auli. 2019. Adaptive\ninput representations for neural language model-\ning. In Proc. of ICLR.\nAnkur Bapna and Orhan Firat. 2019. Non-\nparametric adaptation for neural machine trans-\nlation. In Proc. of NAACL-HLT.\nIz Beltagy, Matthew E. Peters, and Arman Cohan.\n2020. Longformer: The long-document trans-\nformer. arXiv preprint arXiv:2004.05150v2.\nTom B. Brown, Benjamin Mann, Nick Ryder,\nMelanie Subbiah, Jared Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish\nSastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-V oss, Gretchen Krueger, Tom Henighan,\nRewon Child, Aditya Ramesh, Daniel M.\nZiegler, Jeffrey Wu, Clemens Winter, Christo-\npher Hesse, Mark Chen, Eric Sigler, Mateusz\nLitwin, Scott Gray, Benjamin Chess, Jack Clark,\nChristopher Berner, Sam McCandlish, Alec Rad-\nford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Proc.\nof NeurIPS.\nKrzysztof Choromanski, Valerii Likhosherstov,\nDavid Dohan, Xingyou Song, Andreea Gane,\nTamas Sarlos, Peter Hawkins, Jared Davis, Afroz\nMohiuddin, Lukasz Kaiser, David Belanger,\nLucy Colwell, and Adrian Weller. 2021. Re-\nthinking attention with performers. In Proc. of\nICLR.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc V . Le, and Ruslan Salakhutdinov.\n2019. Transformer-xl: Attentive language mod-\nels beyond a ﬁxed-length context. In Proc. of\nACL.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training\nof deep bidirectional transformers for language\nunderstanding. In Proc. of NAACL.\nHoward Eichenbaum. 2012. Memory systems.\nHandbook of Psychology, Second Edition, 3.\nEdouard Grave, Moustapha M Cisse, , and Armand\nJoulin. 2017a. Unbounded cache model for on-\nline language modeling with open vocabulary.\nIn Proc. of NeurIPS.\nEdouard Grave, Armand Joulin, Moustapha Cisse,\nDavid Grangier, and Herve Jegou. 2017b. Efﬁ-\ncient softmax approximation for gpus. In Proc.\nof ICML.\nEdouard Grave, Armand Joulin, and Nicolas\nUsunier. 2017c. Improving neural language\nmodels with a continuous cache. In Proc. of\nICLR.\nRuiqi Guo, Philip Sun, Erik Lindgren, Quan Geng,\nDavid Simcha, Felix Chern, and Sanjiv Kumar.\n2020. Accelerating large-scale inference with\nanisotropic vector quantization. In Proc. of\nICML.\nKelvin Guu, Tatsunori B. Hashimoto, Yonatan\nOren, and Percy Liang. 2018. Generating sen-\ntences by editing prototypes. Transactions of\nthe Association for Computational Linguistics,\n6:437–450.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong\nPasupat, and Ming-Wei Chang. 2020. Realm:\nRetrieval-augmented language model pre-\ntraining. In Proc. of ICML.\nMarcus Hutter. 2012. The human knowledge com-\npression contest.\nHakan Inan, Khashayar Khosravi, and Richard\nSocher. 2017. Tying word vectors and word clas-\nsiﬁers: A loss framework for language modeling.\nIn Proc. of ICLR.\nLukasz Kaiser, Oﬁr Nachum, Aurko Roy, and\nSamy Bengio. 2017. Learning to remember rare\nevents. In Proc. of ICLR.\nNora Kassner and Hinrich Schutze. 2020. Bert-\nknn: Adding a knn search component to pre-\ntrained language models for better qa. In Proc.\nof Findings of EMNLP.\nUrvashi Khandelwal, Angela Fan, Dan Jurafsky,\nLuke Zettlemoyer, and Mike Lewis. 2021. Near-\nest neighbor machine translation. In Proc. of\nICLR.\nUrvashi Khandelwal, Omer Levy, Dan Juraf-\nsky, Luke Zettlemoyer, and Mike Lewis. 2020.\nGeneralization through memorization: Nearest\nneighbor language models. In Proc. of ICLR.\nDiederik P. Kingma and Jimmy Lei Ba. 2015.\nAdam: a method for stochastic optimization. In\nProc. of ICLR.\nNikita Kitaev, Lukasz Kaiser, and Anselm\nKevskaya. 2020. Reformer: The efﬁcient trans-\nformer. In Proc. of ICLR.\nBen Krause, Emmanuel Kahembwe, Iain Murray,\nand Steve Renals. 2018. Dynamic evaluation of\nneural sequence models. In Proc. of ICML.\nBen Krause, Emmanuel Kahembwe, Iain Murray,\nand Steve Renals. 2019. Dynamic evaluation\nof transformer language models. arXiv preprint\narXiv:1904.08378v1.\nCyprien de Masson d’Autume, Sebastian Ruder,\nLingpeng Kong, and Dani Yogatama. 2019.\nEpisodic memory in lifelong language learning.\nIn Proc. of NeurIPS.\nStephen Merity, Caiming Xiong, James Bradbury,\nand Richard Socher. 2017. Pointer sentinel mix-\nture models. In Proc. of ICLR.\nAida Nematzadeh, Sebastian Ruder, and Dani Yo-\ngatama. 2020. On memory in human and arti-\nﬁcial language processing systems. In Proc. of\nICLR Workshop on Bridging AI and Cognitive\nScience.\nGraham Neubig and Chris Dyer. 2016. General-\nizing and hybridizing count-based and neural\nlanguage models. In Proc. of EMNLP.\nFabio Petroni, Tim Rocktaschel, Patrick Lewis, An-\nton Bakhtin, Yuxiang Wu, Alexander H. Miller,\nand Sebastian Riedel. 2020. Language models\nas knowledge bases? In Proc. of EMNLP.\nAlec Radford, Karthik Narasimhan, Tim Salimans,\nand Ilya Sutskever. 2018. Improving language\nunderstanding by generative pre-training.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Lan-\nguage models are unsupervised multitask learn-\ners.\nJack W. Rae, Anna Potapenko, Siddhant M. Jayaku-\nmar, Chloe Hillier, and Timothy P. Lillicrap.\n2020. Compressive transformers for long-range\nsequence modelling. In Proc. of ICLR.\nEdmund T. Rolls. 2000. Memory systems in the\nbrain. Annual Review of Psychology, 51(1):599–\n630.\nMohammad Shoeybi, Mostofa Patwary, Raul\nPuri, Patrick LeGresley, Jared Casper, and\nBryan Catanzaro. 2019. Megatron-lm: Train-\ning multi-billion parameter language models\nusing model parallelism. arXiv preprint\narXiv:1909.08053v4.\nE. Tulving. 1985. How many memory systems are\nthere? American Psychologist, 40:385–398.\nAshish Vaswani, Noam Shazeer, Niki Parmar,\nJakob Uszkoreit, Llion Jones, Aidan N Gomez,\nLukasz Kaiser, and Illia Polosukhin. 2017. At-\ntention is all you need. In Proc. of NIPS.\nWenhan Xiong, Xiang Lorraine Li, Srini Iyer,\nJingfei Du, Patrick Lewis, William Yang Wang,\nYashar Mehdad, Wen tau Yih, Sebastian Riedel,\nDouwe Kiela, and Barlas Oguz. 2021. Answer-\ning complex open-domain questions with multi-\nhop dense retrieval. In Proc. of ICLR.\nPeng Xu, Mostofa Patwary, Mohammad Shoeybi,\nRaul Puri, Pascale Fung, Anima Anandku-\nmar, and Bryan Catanzaro. 2020. Megatron-\ncntrl: Controllable story generation with external\nknowledge using large-scale language models.\nIn Proc. of EMNLP."
}