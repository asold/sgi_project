{
    "title": "Improving Learning Efficiency in Large Language Models through Shortcut Learning",
    "url": "https://openalex.org/W4399686838",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5099134262",
            "name": "Amane Meibuki",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5099134263",
            "name": "Renshu Nanao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5099134264",
            "name": "Mugen Outa",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4396901427",
        "https://openalex.org/W4389983939",
        "https://openalex.org/W4398782346",
        "https://openalex.org/W3174335585",
        "https://openalex.org/W3168867926",
        "https://openalex.org/W4399192936",
        "https://openalex.org/W4398774455",
        "https://openalex.org/W4380559074",
        "https://openalex.org/W4387321091",
        "https://openalex.org/W4392891144",
        "https://openalex.org/W4388182168",
        "https://openalex.org/W4399355067",
        "https://openalex.org/W4396797476",
        "https://openalex.org/W4377820893",
        "https://openalex.org/W4327810158",
        "https://openalex.org/W4396701991"
    ],
    "abstract": "<title>Abstract</title> Large-scale neural networks have demonstrated remarkable capabilities in natural language processing tasks, yet they often face challenges related to computational efficiency and scalability. The introduction of shortcut learning mechanisms offers a novel and significant advancement by enhancing information flow and reducing computational overhead, thereby improving model performance and training speed. This research explores the integration of shortcut learning into the GPT-Neo architecture, resulting in a model that exhibits faster convergence, higher accuracy, and improved resource management. Through meticulous architectural modifications, such as residual connections, skip layers, and gating mechanisms, the modified model achieved superior performance across various benchmarks, including GLUE, SQuAD, and WMT, demonstrating its proficiency in complex linguistic tasks. The experimental results underscored the model's robustness and generalization capabilities, making it a competitive alternative to existing state-of-the-art models. Comprehensive evaluation metrics, including accuracy, F1 score, and BLEU score, were used to validate the effectiveness of the proposed modifications, highlighting substantial improvements in training efficiency and model accuracy. This study contributes significantly to the field of artificial intelligence by providing a scalable and efficient framework for the design and training of advanced LLMs, ultimately paving the way for more effective and accessible AI technologies.",
    "full_text": null
}