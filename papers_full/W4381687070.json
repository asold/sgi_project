{
  "title": "Variability in Large Language Models’ Responses to Medical Licensing and Certification Examinations. Comment on “How Does ChatGPT Perform on the United States Medical Licensing Examination? The Implications of Large Language Models for Medical Education and Knowledge Assessment”",
  "url": "https://openalex.org/W4381687070",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2106965108",
      "name": "Richard H. Epstein",
      "affiliations": [
        "University of Miami"
      ]
    },
    {
      "id": "https://openalex.org/A2104801521",
      "name": "Franklin Dexter",
      "affiliations": [
        "University of Iowa"
      ]
    },
    {
      "id": "https://openalex.org/A2106965108",
      "name": "Richard H. Epstein",
      "affiliations": [
        "University of Miami"
      ]
    },
    {
      "id": "https://openalex.org/A2104801521",
      "name": "Franklin Dexter",
      "affiliations": [
        "University of Iowa"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4319460874"
  ],
  "abstract": null,
  "full_text": "Letter to the Editor\nVariability in Large Language Models’ Responses to Medical\nLicensing and Certification Examinations.Comment on “How Does\nChatGPT Perform on the United States Medical Licensing\nExamination? The Implications of Large Language Models for\nMedical Education and Knowledge Assessment”\nRichard H Epstein1*, MD; Franklin Dexter2*, MD, PhD\n1Department of Anesthesiology, Perioperative Medicine and Pain Management, University of Miami Miller School of Medicine, Miami, FL, United\nStates\n2Division of Management Consulting, Department of Anesthesia, University of Iowa, Iowa City, IA, United States\n*all authors contributed equally\nCorresponding Author:\nRichard H Epstein, MD\nDepartment of Anesthesiology, Perioperative Medicine and Pain Management\nUniversity of Miami Miller School of Medicine\n1400 NW 12th Ave\nSuite 4022F\nMiami, FL, 33136\nUnited States\nPhone: 1 215 896 7850\nFax: 1 305 689 5501\nEmail: repstein@med.miami.edu\nRelated Articles:\nComment on: https://mededu.jmir.org/2023/1/e45312\nComment in: https://mededu.jmir.org/2023/1/e50336/\n(JMIR Med Educ 2023;9:e48305) doi: 10.2196/48305\nKEYWORDS\nnatural language processing; NLP; MedQA; generative pre-trained transformer; GPT; medical education; chatbot; artificial\nintelligence; AI; education technology; ChatGPT; Google Bard; conversational agent; machine learning; large language models;\nknowledge assessment\nWe read with interest the recent study by Gilson and colleagues\n[1], “How Does ChatGPT Perform on the United States Medical\nLicensing Examination? The Implications of Large Language\nModels for Medical Education and Knowledge Assessment.”\nBased on their detailed evaluation of the model’s performance,\nincluding content analysis and logical reasoning, the authors\nsuggested that ChatGPT has potential application as a medical\neducation tool to support interactive peer group education. We\ntake no issue with those conclusions. However, what is not\nemphasized in the article is that search engines often provide\ndifferent results based on the login credentials of the person\nexecuting the search, the location (country), and the device\n[2,3]. Thus, because the performance results presented by the\nauthors did not account for this variability, their single\ncomparisons between the various models against the different\nsets of questions may be statistically unreliable. Again, we are\nnot suggesting that the authors’ useful conclusions would\nchange, but quantitative performance will differ.\nWe evaluated this issue of varying responses using all questions\nfrom the most recent quarterly, online, open-book American\nBoard of Preventive Medicine (ABPM) pilot evaluation of a\nlongitudinal assessment program for the maintenance of\ncertification of its clinical informatics diplomates. We evaluated\nChatGPT, version 3.5 (OpenAI), and Google Bard (Alphabet\nInc) by copying and pasting each of the 12 questions and the\ncorresponding 4-part multiple-choice options into the chatbots’\nmessage boxes on March 30, 2023, and April 1, 2023,\nrespectively. We added a request to provide citations for each\nquestion. Both chatbots supplied the option they considered\nbest, with a justification, references, and an explanation as to\nwhy each option was either incorrect or inferior to the\nrecommended answer.\nJMIR Med Educ 2023 | vol. 9 | e48305 | p. 1https://mededu.jmir.org/2023/1/e48305\n(page number not for citation purposes)\nEpstein & DexterJMIR MEDICAL EDUCATION\nXSL•FO\nRenderX\nFor ChatGPT, the series of 12 questions was performed 10 times\nin separate chat sessions to avoid memory effects from a\nprevious search, with each session scored against the answer\nkey provided by the ABPM. The results showed that out of the\n12 questions, there were 9 sessions where 8 correct responses\nwere achieved and 1 session where 9 correct responses were\nachieved. Although 8 questions had perfect (10/10) concordance\nwith the answer key, there were 2 questions with 2 different\nanswers and one with 3 different answers. There was a twelfth\nquestion where the same answer was provided for each session\nthat disagreed with the answer key. These scores were at least\nas good as the average performance of the diplomates\nparticipating in the maintenance of certification process (61%,\nto date), which allows the use of online resources, and likely\nwould have represented a passing score. We also evaluated the\nexperimental ChatGPT, version 4.0, in 5 separate chat sessions,\nwhich produced sequential scores of 10, 8, 8, 6, and 7. For\nGoogle Bard, the process was performed 9 times, and the most\ncommon answer was selected as the best response. The modal\nresponses were correct for 7 out of 12 questions (sequential\nscores of 7, 6, 7, 6, 7, 5, 6, 7, and 8). There were 5 questions\nfor which 2 different answers were provided and 1 question for\nwhich all 4 answers were provided as correct answers during\ndifferent sessions. Google Bard agreed with the ABPM answer\nkey for only 4 questions in all sessions.\nThe questions where the large language models consistently\ndisagreed with the ABPM answer key were either based on\nlow-level evidence or involved an opinion on a “best” approach.\nAs implied by Gilson et al [1], these dichotomies emphasize\nthe importance of using artificial intelligence products to foster\ndiscussion rather than considering them an arbiter of truth. Since\nboth ChatGPT and Google Bard provide justifications and\nreferences, groups or individuals using these products for\neducation can learn from the supplied material. If used for such\npurposes, we recommend submitting questions several times in\nseparate sessions and considering the range of responses.\nConflicts of Interest\nNone declared.\nReferences\n1. Gilson A, Safranek CW, Huang T, Socrates V, Chi L, Taylor RA, et al. How does ChatGPT perform on the United States\nMedical Licensing Examination? The implications of large language models for medical education and knowledge assessment.\nJMIR Med Educ 2023 Feb 08;9:e45312 [FREE Full text] [doi: 10.2196/45312] [Medline: 36753318]\n2. Why your Google Search results might differ from other people. Google Search Help. URL: https://support.google.com/\nwebsearch/answer/12412910?hl=en&sjid=14431510508711933103-NA [accessed 2023-06-22]\n3. McEvoy M. Reasons Google Search results vary dramatically (updated and expanded). Web Presence Solutions. 2020 Jun\n29. URL: https://www.webpresencesolutions.net/7-reasons-google-search-results-vary-dramatically/[accessed 2023-06-22]\nAbbreviations\nABPM: American Board of Preventive Medicine\nEdited by T Leung; submitted 18.04.23; peer-reviewed by A Gilson, C Zielinski; comments to author 16.06.23; revised version received\n16.06.23; accepted 22.06.23; published 13.07.23\nPlease cite as:\nEpstein RH, Dexter F\nVariability in Large Language Models’ Responses to Medical Licensing and Certification Examinations. Comment on “How Does\nChatGPT Perform on the United States Medical Licensing Examination? The Implications of Large Language Models for Medical\nEducation and Knowledge Assessment”\nJMIR Med Educ 2023;9:e48305\nURL: https://mededu.jmir.org/2023/1/e48305\ndoi: 10.2196/48305\nPMID: 37440293\n©Richard H Epstein, Franklin Dexter. Originally published in JMIR Medical Education (https://mededu.jmir.org), 13.07.2023.\nThis is an open-access article distributed under the terms of the Creative Commons Attribution License\n(https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium,\nprovided the original work, first published in JMIR Medical Education, is properly cited. The complete bibliographic information,\na link to the original publication on https://mededu.jmir.org/, as well as this copyright and license information must be included.\nJMIR Med Educ 2023 | vol. 9 | e48305 | p. 2https://mededu.jmir.org/2023/1/e48305\n(page number not for citation purposes)\nEpstein & DexterJMIR MEDICAL EDUCATION\nXSL•FO\nRenderX",
  "topic": "Certification",
  "concepts": [
    {
      "name": "Certification",
      "score": 0.8175572752952576
    },
    {
      "name": "United States Medical Licensing Examination",
      "score": 0.609748363494873
    },
    {
      "name": "Medical education",
      "score": 0.4492177963256836
    },
    {
      "name": "Medical school",
      "score": 0.30328303575515747
    },
    {
      "name": "Medicine",
      "score": 0.2992289662361145
    },
    {
      "name": "Political science",
      "score": 0.2602299153804779
    },
    {
      "name": "Law",
      "score": 0.10626322031021118
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I145608581",
      "name": "University of Miami",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I126307644",
      "name": "University of Iowa",
      "country": "US"
    }
  ]
}