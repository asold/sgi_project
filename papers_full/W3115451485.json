{
  "title": "Team Solomon at SemEval-2020 Task 4: Be Reasonable: Exploiting Large-scale Language Models for Commonsense Reasoning",
  "url": "https://openalex.org/W3115451485",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2955653897",
      "name": "Vertika Srivastava",
      "affiliations": [
        "Samsung (India)"
      ]
    },
    {
      "id": "https://openalex.org/A2954369816",
      "name": "Sudeep Kumar Sahoo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2117980599",
      "name": "Yeon Hyang Kim",
      "affiliations": [
        "Samsung (India)"
      ]
    },
    {
      "id": "https://openalex.org/A2954909411",
      "name": "Rohit R. R",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2128093780",
      "name": "Mayank Raj",
      "affiliations": [
        "Samsung (India)"
      ]
    },
    {
      "id": "https://openalex.org/A2164683292",
      "name": "Ajay Jaiswal",
      "affiliations": [
        "Samsung (India)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2898695519",
    "https://openalex.org/W2963995027",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2962833140",
    "https://openalex.org/W2963159690",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3113425182",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1599016936",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2341790067",
    "https://openalex.org/W4298149550",
    "https://openalex.org/W2947337775"
  ],
  "abstract": "In this paper, we present our submission for SemEval 2020 Task 4 - Commonsense Validation and Explanation (ComVE). The objective of this task was to develop a system that can differentiate statements that make sense from the ones that don't. ComVE comprises of three subtasks to challenge and test a system's capability in understanding commonsense knowledge from various dimensions. Commonsense reasoning is a challenging task in the domain of natural language understanding and systems augmented with it can improve performance in various other tasks such as reading comprehension, and inferencing. We have developed a system that leverages commonsense knowledge from pretrained language models trained on huge corpus such as RoBERTa, GPT2, etc. Our proposed system validates the reasonability of a given statement against the backdrop of commonsense knowledge acquired by these models and generates a logical reason to support its decision. Our system ranked 2nd in subtask C with a BLEU score of 19.3, which by far is the most challenging subtask as it required systems to generate the rationale behind the choice of an unreasonable statement. In subtask A and B, we achieved 96% and 94% accuracy respectively standing at 4th position in both the subtasks.",
  "full_text": "Proceedings of the 14th International Workshop on Semantic Evaluation, pages 585–593\nBarcelona, Spain (Online), December 12, 2020.\n585\nTeam Solomon at SemEval-2020 Task 4: Be Reasonable: Exploiting\nlarge-scale language models for commonsense reasoning\nVertika Srivastava∗\nRohit R.R\nSudeep Kumar Sahoo∗\nMayank Raj\nYeon Hyang Kim\nAjay Jaiswal\nSamsung R&D Institute India, Bangalore\n{v.srivastava, sudeep.sahoo, purine.kim,\nrohit.r.r, mayank.raj, ajay.jaiswal}@samsung.com\nAbstract\nIn this paper, we present our submission for SemEval 2020 Task 4 - Commonsense Validation and\nExplanation (ComVE). The objective of this task was to develop a system that can differentiate\nstatements that make sense from the ones that don’t. ComVE comprises of three subtasks to\nchallenge and test a system’s capability in understanding commonsense knowledge from various\ndimensions. Commonsense reasoning is a challenging task in the domain of natural language\nunderstanding and systems augmented with it can improve performance in various other tasks\nsuch as reading comprehension, and inferencing.\nWe have developed a system that leverages commonsense knowledge from pretrained language\nmodels trained on huge corpus such as RoBERTa, GPT2, etc. Our proposed system validates the\nreasonability of a given statement against the backdrop of commonsense knowledge acquired by\nthese models and generates a logical reason to support its decision. Our system ranked 2nd in\nsubtask C with a BLEU score of 19.3, which by far is the most challenging subtask as it required\nsystems to generate the rationale behind the choice of an unreasonable statement. In subtask A and\nB, we achieved 96% and 94% accuracy respectively standing at 4th position in both the subtasks.\n1 Introduction\nIn today’s digital age, information is shared widely in textual format via e-mails, news articles, social\nmedia posts and messages, blogs, internet forums, etc. We are now surrounded by textual information\nmore than ever, which demands a meaningful understanding of a text by machines. Machines augmented\nwith commonsense reasoning will be a key step towards achieving this. ‘Commonsense Knowledge’\nalso referred to as the background knowledge, is the understanding of the everyday world and the art of\ndrawing inferences by manipulating the knowledge gathered. Humans are rational and have acquired\na sense of reasoning by combining facts and beliefs from their day to day life. To an average person\nreasoning the fact that a person can have a pet dog but not pet dinosaur comes naturally and is fairly\nstraightforward. Commonsense knowledge is assumed to be known to all and people typically tend to\nomit this while communicating with others, which makes it more challenging.\nCurrent Natural Language Understanding (NLU) systems assisted with semantic representations,\nstatistical methods, and distributional representations have shown better performance than humans on\nmany benchmarks but there is a growing concern that these systems scratch only the surface of the human\nlevel of understanding of the world and thus are too shallow. Natural language is complex in nature and\nNLU systems have tried to derive useful meaning by capturing the context i.e. neighboring words and\nsentences but these systems fail miserably when the context is restricted or omitted. Such cases call\nfor systems to delve deeper into understanding the background knowledge enjoyed by all humans. For\na commonsense deprived machine, understanding that the sentence ‘he put books in his pencil box’is\nagainst common sense as ‘a book is much bigger than a pencil box’is difﬁcult in the absence of the\nknowledge about the size of books in comparison with the size of a pencil box. Since the existing systems\n∗*Equal Contribution\nThis work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://\ncreativecommons.org/licenses/by/4.0/.\n586\nSubtask A: Validation S0: He put a turkey into the fridge.\nS1: He put an elephant into the fridge.\nSubtask B: Explanation\n(Multi-Choice)\nStatement: He put an elephant into the fridge.\nOption A: An elephant is much bigger than a fridge.\nOption B: Elephants are usually white while fridges are usually white.\nOption C: An elephant cannot eat a fridge.\nSubtask C: Explanation\n(Generation)\nStatement: He put an elephant into the fridge.\nReferential Reasons:\n1. An elephant is much bigger than a fridge.\n2. A fridge is much smaller than an elephant.\n3. Most of the fridges aren’t large enough to contain an elephant.\nTable 1: Examples from the ComVE dataset.\neither do not possess this knowledge or are rather weak in reasoning beyond the data provided to them.\nThrough this task, we aim to empower machines to acquire this knowledge and perform better in many\nnatural language tasks like question answering, fake news detection, etc.\nComVE comprises of three subtasks, subtask A is a validation task where a system has to choose the\nunreasonable statement between a pair of given statements. Under subtask B the participating system has\nto pick up the reason which explains the rationale behind the unreasonable statement selected in subtask\nA. Subtask C, which by far is the most challenging task and requires the system to generate the reason\nexplaining the argument behind its choice in the ﬁrst task. We have illustrated some samples from the\ndataset for all the three subtasks in Table 1.\nUnder this task, we have developed a system that leverages commonsense knowledge gained by\npretrained language models from their huge training corpora. We have used models like BERT, RoBERTa,\nGPT2, etc. Our system seeks to utilize the language model’s world knowledge and identify commonsense\nfacts in the task-speciﬁc dataset with task-centric ﬁnetuning. The model with little ﬁnetuning and task-\nspeciﬁc modiﬁcations such as transforming the input and adding a score comparator achieved signiﬁcant\ngains on all the subtasks. Our system ranked 2nd 1 in subtask C (Explanation with Generation) with a\nBLEU score of 19.3. In subtask A and B, we achieved 96% and 94% accuracy respectively standing at\n4th rank in both the subtasks.\nThis paper is organized as follows, in Section 2, we brieﬂy review some of the popular works in the\ndomain of commonsense. In Section 3, we describe the task and the dataset. Section 4 gives details of our\nsystem and individual setup for each subtask. In Section 5, we have dicussed the experiments and the\nresults. We conclude the paper in Section 6.\n2 Related work\nIn the recent past, there have been several lines of research focussing on commonsense reasoning. Multiple\ntasks and datasets have been proposed which tests machine’s intelligence pivoting on commonsense,\nlike The Winograd Schema Challenge (Levesque et al., 2012) which aims to resolve ambiguities arising\nout of pronouns, using commonsense. Mostafazdeh et al., (2016) released the ROCStories corpus and\nintroduced a Story Cloze Test, in which a system is given a four-sentence ‘context’ and two alternative\nendings to the story, called the ‘right ending’ and the ‘wrong ending’. This task challenges a system\nto understand the context and predict the correct ending. Another dataset, SWAG (Zellers et al., 2018)\ninvolves predicting the next scene given the current one to evaluate grounded commonsense inference. It’s\nalso a multiple-choice dataset with 4 possible continuations to the given description. Devlin et al., (2018)\nﬁnetuned their BERTLARGE model on the SWAG dataset to beat the human performance and achieve\n1Leaderboard link: https://competitions.codalab.org/competitions/21080#results\n587\nstate-of-the-art results with 86.3% accuracy on the test set.\nAll of these works involve multiple-choice datasets, where one has to choose the right option without\nproviding any justiﬁcation as to why the system chose a particular option. This raises a concern in the\nsystem’s capability in actually understanding the choice made. None of the above work inspects a direct\nunderstanding of commonsense by demanding a logical reason for the choice. Wang et al,. (2019) released\na dataset, which requires a system to choose an unreasonable statement from a given pair and also predict\nthe right reason behind its choice. They also utilized state-of-the-art language models (LM) like BERT for\nSen-Making and Explanation task which are similar to subtask A and B respectively of ComVE. However,\nthey have reported a decline in performance on ﬁnetuning BERT. They achieved the best result with\nﬁnetuned ELMO in the Sen-Making task and with pretrained BERT in the Explanation task.\nRajani et al., (2019) developed a Commonsense Auto-Generated Explanation (CAGE) framework for\ncommonsenseQA task (Talmor et al., 2018). CAGE in its main approach of ‘Reasoning’ ﬁnetunes a\nlanguage model conditioned on the question and all the plausible answer choices. The language model\nutilized Common Sense Explanations (CoS-E) as a referential reason while training. CoS-E is a manually\nconstructed dataset comprising of reasons given by users who in turn were provided the question, all\nanswer choices, and the correct label. LM trained via this approach was used to augment a classiﬁer to\npredict the answer for the multiple-choice question posed initially. They leveraged CoS-E to assist in\npredicting the right answer. We have taken inspiration from their work in our subtask C to generate a\nlogical reason as to why the unreasonable statement is against commonsense.\nSubtask A Subtask B\nDataset Label-0 Label-1 Total\nTrain 4979 5021 10000\nDev 518 479 997\nDataset A B C Total\nTrain 3195 3362 3443 10000\nDev 344 327 326 997\nTable 2: Dataset decription for Subtasks A and B.\n3 Task and Dataset Description\nThe ComVE was formulated as a three-stage problem, where different subtasks assess a system’s under-\nstanding of commonsense from a disparate perspective. The ﬁrst subtask aims to empower the system to\ndifferentiate an unreasonable statement from a reasonable one and is proposed as a ‘Validation’ task. The\nnext subtask, ‘Explanation with Multiple-Choice’ assess the system’s capability to choose the right reason\nbehind its choice for a particular statement to be unreasonable in the ﬁrst subtask. The system’s rationality\nis further tested by subtask C: ‘Explanation with Generation’, which expects the system to generate the\nreason explaining the rationality behind the system’s choice of the irrational statement in subtask A. Table\n1 shows an example of the dataset.\nSubtask A is a two-class (or binary) classiﬁcation problem, where a system has to choose from two\nnatural language statements with similar wordings which one makes sense and which one doesn’t. There\nare 10,000 sentence pairs in the training data, with each instance being labeled as either 0 or 1 depending\non whether sentence 0 is unreasonable or sentence 1. The sentence pairs have been designed in a way that\nit is fairly easy for a human to pick the right statement but cannot be easily detected by commonsense\ndeprived systems. Subtask B is a multi-class classiﬁcation problem where a participating system has\nto pick the key reason from three options justifying why a given statement does not make sense. The\ntraining dataset for this subtask had 10,000 unreasonable sentences accompanied by three reasons for\neach. Dataset also had three noisy samples where just two options were provided. Subtask C is a text\ngeneration task where the objective is to generate a reason why the given statement is against common\nsense. The training dataset consists of 10000 unreasonable sentences and three referential reasons for\neach of them. We have provided the class-wise data distribution for the ﬁrst two tasks in Table 2.\nDev data released for the evaluation phase had 997 samples for each subtask and hidden test data\non which systems were ﬁnally evaluated had 1000 samples without labels. During the ﬁnal evaluation,\n588\nsubtasks were kicked off sequentially with subtask A being opened ﬁrst, followed by subtask C, and at the\nend, subtask B was started. This ensured that the there is no information leakage between the subtasks.\nMore information on the tasks and the dataset can be found in Wang et al., (2020).\n4 System Description\nOur systems leverage commonsense knowledge from pretrained language models via transfer learning,\nthus we ﬁrst brieﬂy discuss the language models used in our system’s core in Section 4.1. Subsequently,\nwe explain the details of our models for each subtask. We have developed systems separately for each\nsubtask which can be combined in the desired manner for an end to end commonsense pipeline.\n4.1 Overview of Pretrained Language Models\nBERT:Bidirectional Encoder Representations for Transformers, (Devlin et al., 2018) is a pretrained deep\nbidirectional transformer model producing context representations. It was trained on masked language\nmodeling and the next sentence prediction objectives. BERT representations can be ﬁne-tuned to many\ndownstream NLP tasks by adding just one additional output layer for the target task, or it can be used as a\nfeature for task-speciﬁc architectures. Using a ﬁne-tuning setting, BERT has advanced state-of-the-art\nperformances on a wide range of NLP tasks. We used pretrained BERTbase-uncased with 110M parameters\nin our experiments.\nALBERT:Ian et al., (2019) introduced A Lite BERT (ALBERT) for learning language representations.\nIt has two parameter reduction techniques that help it to increase the training speed and reduce memory\nconsumption thus overcoming previous memory limitations of BERT. The authors have introduced the\nconcept of parameter sharing across layers to prevent the growth in trainable parameters as the network’s\ndepth increases. They introduced a self-supervised loss for sentence order prediction in place of ineffective\nnext sentence prediction of BERT. ALBERTlarge with 18M parameters was used in our experiments.\nRoBERTa: Robustly Optimized BERT pre-training Approach (RoBERTa) (Liu et al., 2019) is an\nadaptation of BERT architecture trained with larger batches on 160 GB data from various domains. The\npaper mentioned that BERT was signiﬁcantly undertrained and has the potential to outperform other\ntransformer-based models with the right amount of data and design choices. RoBERTa was trained by\ndynamically modifying language masking while the next sentence prediction loss used in BERT was\ndropped. Other improvising techniques like larger input text sequences, byte pair encoding are used in\ntraining which seemingly improved the model performance in downstream tasks. It achieved state-of-the-\nart results in 4 of the 9 GLUE benchmark tasks during the time of publishing. For our experiments, we\nwill be using RoBERTalarge which has 355M hyperparameters.\nGPT-2: Generative Pretrained Transformer 2 (Radford et al., 2019) is a large transformer-based\nlanguage model trained on a dataset of 8 million web pages. GPT-2 is trained with a simple objective of\npredicting the next token, given all of the previous tokens within some text. This model shares the same\narchitecture as GPT, with more than 10X the parameters and trained on more than 10X the amount of data.\nIt displays a broad set of capabilities, including the ability to generate conditional and unconditional text\nsamples of unprecedented quality. For downstream tasks involving text generation, the model performs\nbetter than all the other transformer-based language models. We will be using GPT-2 large model with\n762M parameters for our experiments.\n4.2 Subtask A\nFor subtask A, we ﬁne-tuned several task-speciﬁc pretrained BERT based classiﬁers, where input is a\nsentence and label is whether the sentence is unreasonable or not. Dataset for the classiﬁers was prepared\nby splitting a given input pair into two separate sentences and each of these was passed through the\nmodel to generate probability score for unreasonability. We have added a score comparison system on\ntop of the model to predict the ﬁnal label for the sentence pair by comparing the unreasonability score\nof both the sentences. Fine-tuning a BERT based model on our task consists of further training it on a\ntask-speciﬁc dataset with masked language modeling (MLM) loss. We conducted our ﬁrst experiments\nwith a pretrained BERT model (BERT-CS). Systems with the same approach were implemented with\n589\nFigure 1: System for Subtask A\nALBERT (ALBERT-CS) and RoBERTa (RoBERTa-CS) architectures where RoBERTa based system\nseems most promising since it was trained on a large corpus and captures external knowledge convincingly.\n4.3 Subtask B\nFigure 2: System for Subtask B where, Phrase is a connecting negation phrase like, “This does not makes\nsense because, ”, S is an unreasonable statement, and BB-CS is the same as described in Figure 1.\nSubtask B is a multiple-choice task where a system has to identify the key reason to explain the\nirrationality of the given unreasonable statement (see Section 3). To achieve this, we have built a system\nthat can understand the relation between a choice and the unreasonable statement, and also comprehend\nthat the choice is justifying the logical reason behind it. To augment this further we have used connecting\nnegation phrasesbetween the choices and the unreasonable statement.\nWe formulated the given problem as a three-way binary classiﬁcation task for each option. The dataset\nwas transformed by constructing three input sequences per choice from the original sample. Each input\nsequence is a concatenation of the given unreasonable statement, connecting negation phrase, and one of\nthe possible reasons. Connecting negation phrases like, “This does not makes sense because, ”or “No, ”,\nhelps in constraining the model to learn a choice that explains the unreasonability of the statement. The\nsystem (see Figure 2) was developed by adding a task-speciﬁc layer on top of the pretrained models and\nﬁnetuning them on task-speciﬁc data with MLM objective. We trained it on the modiﬁed input sequence\nas a binary classiﬁcation problem with a softmax layer to produce a probability score for the sequence. An\nadditional score comparator was used to merge the three binary classiﬁers. The score comparator analyzes\nscores from all three classiﬁers and arrives at the ﬁnal prediction based on the maximum scoring sequence.\nBERT was used as a pretrained language model to develop BERT-Single and similarly, RoBERTa-Single\nwas constructed with the RoBERTa language model, but our best performing system RoBERTa-Ens is an\nensemble of 4 RoBERTa-Single based models with slight differences in their training.\n4.4 Subtask C\nAs described in Section 3, the dataset consists of 10,000 unreasonable statements with three referential\nreasons each. In this subtask, we ﬁne-tuned GPT-2 large model on the given training data and evaluated\n590\nFigure 3: System for Subtask C\nthe system with the BLEU score. GPT-2 model has great capabilities to learn from raw text without the\nneed for explicit labeling. Hence, this property of GPT-2 has been exploited here instead of using an\nencoder-decoder architecture to encode the unreasonable sentence and generate reason.\nEach training row in the dataset has been converted to three separate samples based on the three\nreferential reasons leading to 30,000 total input dataset size. The input to the system is fed sample wise\ninstead of the original text chunk based training. Each sample is passed as Unreasonable Statement +\n[No,]+ Referential Reason. The model was ﬁne-tuned on the cross-entropy loss to predict the next token\nat each step. We have used the beam search algorithm to generate the ﬁnal output sequence instead of a\ngreedy approach. The system outputs the probability score for each token in vocabulary to be the next\nelement in the sequence. Following beam search, the top k sequences are separately appended with the\ninput to generate the next token and this process is repeated till we reach the < |endoftext| > token. The\ngenerated reason is converted to lower case to avoid non-uniformity in the sentence structure leading to\nbetter match with the referential reasons and thus increasing the system’s BLEU score.\n5 Experiments and Evaluation\n5.1 Subtask A\nTable 3 shows the performance of the systems experimented for subtask A. From the results, we can\nobserve that the best results are obtained using RoBERTa-CS with an impressive accuracy of 96% on the\nofﬁcial test data. For RoBERTa-CS, we ﬁnetuned RoBERTalarge pretrained model with a learning rate of\n1e-5, dropout probability of 0.15, and a batch size of 32 for 7 epochs. BERT-CS and ALBERT-CS have\nachieved 87.7% and 81.1% accuracy respectively on the test data. Signiﬁcant improvement of 8.3% by\nRoBERTa-CS over BERT-CS can be attributed to rigorous training and wider training dataset covering\ndomains like news, stories, and Reddit of the pretrained RoBERTa. On samples where the model has to\nunderstand the irrationality with respect to time, duration or season, etc. (see Table 3), we found that\nRoBERTa-CS easily outperforms the others.\nRoBERTa-CS was constructed with multiple task-speciﬁc components such as splitting our dataset\nsample and feeding them individually to the model and the score comparator at the top. To verify the\nnecessity of these, we developed the RoBERTa-pairwise model. In the RoBERTa-pairwise, dataset format\nis kept intact and the input is fed as a sentence pair ([CLS] + Sentence 0 + [SEP] + Sentence 1 + [SEP])\nwith a softmax layer on top of the existing RoBERTa architecture and score comparator was completely\nshunned. It obtained 93.7% accuracy, recording a decline of 2.3% from RoBERTa-CS. Usually, BERT\nbased architectures are trained on pairwise tasks by combining inputs with a separator, which tends to\ncapture relations like: entailment, similarity, sentence order, etc. between the text pairs. Thus, when\nwe ﬁne-tuned a system based on the pairwise input as in RoBERTa-pairwise, some noise might have\ngot added to the loss which has deteriorated the performance by struggling to capture a similar pairwise\nrelation which is absent in the sample. On the other hand, an individual way of feeding input to the system\ncaptures the degree of reasonability of the statement and a simple comparison of the probability scores\n591\ngives us the statement that is relatively more unreasonable.\nWe conducted an additional experiment by extracting ﬁnal layer embeddings from the pretrained\nRoBERTa model and applied logistic regression on those as an input (RoBERTa-LR). This approach\nobtained an accuracy of 88.2% which is 7.8% less as compared to the best RoBERTa model. The results\nare in-line with the expectations as an end-to-end trained model learns details of the provided data while\njust applying a classiﬁcation layer separately can’t tune the embeddings to capture reasonability in a\nsentence. Yet, it has a minor gain of 0.5% over BERT-CS, which can be associated with better sentence\nrepresentation being learned by RoBERTa’s larger training corpus.\nModel Acc.\nRoBERTa-CS 96.0\nRoBERTa-pairwise 93.7\nRoBERTa + LR 88.2\nBERT-CS 87.7\nALBERT-CS 81.1\nSamples Predicted Label\nS0: owls sleep at night\nS1: owls sleep at day\nRoBERTa-CS: S0\nBERT-CS: S1\nALBERT-CS: S1\nS0: December is the 13th month of a year\nS1: December is the 12th month of a year\nRoBERTa-CS: S0\nBERT-CS: S1\nALBERT-CS: S1\nTable 3: Result for Subtask A on the Test dataset (accuracy is in percentage). The second table shows a\ncomparative analysis of the different models on some dataset samples.\n5.2 Subtask B\nRoBERTa-Ens is an ensemble of 4 RoBERTa-Single models trained on different connecting phrases, such\nas, “No, ”, “, it is not true because”, and “This does not makes sense because, ”. One of the RoBERTa\nmodels for ensembling was trained on a new input sequence (“Unreasonable Sentence:”+ Unreasonable\nstatement + “Reason:” + One of the Reason Choice) which used a phrase in English to inject the task-\nspeciﬁc information into the model. The ensemble is done by taking the average of probability from the 4\nmodels for each given option and taking the option with the maximum score. The models are ﬁne-tuned\nfor 5 to 6 epochs with a learning rate of 1e-5, dropout of 0.1, batch size of 64, and 250 as the warmup steps\nfor learning rate. It achieved an accuracy of 94% on the ofﬁcial test data as shown in Table 4. Among\nthe RoBERTa-Single models, the system with a simple “No,” connector performs the best with a 93.1%\naccuracy. Similar to subtask A, BERT-Single model performs subpar as compared to RoBERTa-Single\nwith a drop of 10.6% accuracy.\nSubtask B Subtask C\nModel Accuracy (%)\nRoBERTa-Ens 94.0\nRoBERTa-Single + “No,” 93.1\nBERT-Single 82.5\nModel BLEU Score\nGPT-2 + BMS 19.34\nGPT-2 16.92\nTable 4: Results on the test dataset for Subtask B and C have been presented in the ﬁrst and second table\nrespectively.\n5.3 Subtask C\nIn subtask C, GPT-2 model with beam search algorithm (GPT-2+BMS) was trained for a single epoch\nwith all referential reasons of a statement line by line (instead of a block of text) with a batch size of 64\nand a maximum sequence length of 128. The unreasonable sentence and one of the reasons are combined\nwith a negating phrase, “No,” while feeding to the system. The above-mentioned system in conjunction\nwith k equal to 3 for beam search achieves a BLEU score of 19.34 and 2nd rank on the ofﬁcial leaderboard.\n592\nReplacing greedy search in place of beam search for reason generation decreases the BLEU score by 2.42\n(see Table 4).\nThe GPT-2+BMS system can generate coherent reasons for the majority of the samples, yet in some\ncases, the system merely negated the input sentence to generate the reason as illustrated in Table 5.\nTraining the model for larger epochs generates more complex and precise reasons at the cost of decreasing\nthe BLEU score. This can be caused by the complexity of natural language where a reason can be explained\nin many ways and the ComVE dataset appeared to be containing very simple sentences. Furthermore,\nthe BLEU score represents the precision of word order prediction and it doesn’t take sentence structure,\nmeaning, or recall into consideration which also justiﬁes why the BLEU score dropped when the system\ngenerated coherent but complex reasons. In such cases, evaluating on BLEU along with the ROGUE score\nwhich captures recall of the generated sentence would have made more sense.\nUnreasonable Statement Generated Reason\nGood\nExamples\nA soldier shot with a guitar. A guitar is not a weapon.\nIt is easy to see the stars on a clear day. Stars appear in the night sky.\nShe went to the grocery store to get an aneurysm. An aneurysm is a medical condition.\nShe eats a pillow after her workout. Pillows are not edible.\nBad\nExamples\nLondon goes in this bus. A bus does not have wheels\nA television plays audio only. A television does not play audio.\nTable 5: In the above table we have shown some unreasonable statements from the dataset along with the\nreason generated by our best model (GPT-2+BMS).\n6 Conclusion and Future Work\nIn this work, we have described our system for SemEval-2020 Task 4 on common sense validation\nand explanation. The proposed system leverages the background knowledge captured by large-scale\ntransformer-based language models. This paper also discusses various ways of input manipulation in\nthe architecture to improve the system’s performance on the downstream tasks. Our ofﬁcial submission\nobtained an accuracy of 96% in subtask A and 94% in subtask B, both securing 4th position on the\nleaderboard. Our system also generates the rationale given an unreasonable sentence with a 19.34 BLEU\nscore standing at 2nd rank on the leaderboard for subtask C. In future, we would like to train a joint model\nby combining systems for subtask A and C. This will provide an additional reasonable statement against\nthe given unreasonable sentence to the system which can improve the reason generation capability.\nDue to the limitation of language models in capturing external knowledge and their training being\nrestricted by the dataset, the systems lagged behind humans in generating coherent reasoning for an\nunreasonable sentence. However, commonsense augmented systems can be incorporated in chatbots to\ncreate a more sensible conversation with a user. Also, it can help in detecting satirical articles in online\nnews websites to combat the rising fake news problem.\nReferences\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019.\nAlbert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942.\nHector Levesque, Ernest Davis, and Leora Morgenstern. 2012. The winograd schema challenge. In Thirteenth\nInternational Conference on the Principles of Knowledge Representation and Reasoning.\n593\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv\npreprint arXiv:1907.11692.\nNasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Push-\nmeet Kohli, and James Allen. 2016. A corpus and evaluation framework for deeper understanding of common-\nsense stories. arXiv preprint arXiv:1604.01696.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models\nare unsupervised multitask learners. OpenAI Blog, 1(8):9.\nNazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Explain yourself! leveraging\nlanguage models for commonsense reasoning. arXiv preprint arXiv:1906.02361.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2018. Commonsenseqa: A question an-\nswering challenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937.\nCunxiang Wang, Shuailong Liang, Yue Zhang, Xiaonan Li, and Tian Gao. 2019. Does it make sense? and why? a\npilot study for sense making and explanation. arXiv preprint arXiv:1906.00363.\nCunxiang Wang, Shuailong Liang, Yili Jin, Yilong Wang, Xiaodan Zhu, and Yue Zhang. 2020. SemEval-2020 task\n4: Commonsense validation and explanation. In Proceedings of The 14th International Workshop on Semantic\nEvaluation. Association for Computational Linguistics.\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018. Swag: A large-scale adversarial dataset for\ngrounded commonsense inference. arXiv preprint arXiv:1808.05326.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8554732799530029
    },
    {
      "name": "SemEval",
      "score": 0.8537338972091675
    },
    {
      "name": "Commonsense knowledge",
      "score": 0.853278636932373
    },
    {
      "name": "Commonsense reasoning",
      "score": 0.831680417060852
    },
    {
      "name": "Task (project management)",
      "score": 0.7368621826171875
    },
    {
      "name": "Statement (logic)",
      "score": 0.6299526691436768
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6262021064758301
    },
    {
      "name": "Natural language processing",
      "score": 0.5717756748199463
    },
    {
      "name": "Question answering",
      "score": 0.5346677899360657
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5247792601585388
    },
    {
      "name": "Reading (process)",
      "score": 0.46350497007369995
    },
    {
      "name": "Comprehension",
      "score": 0.4483986794948578
    },
    {
      "name": "Language model",
      "score": 0.42960721254348755
    },
    {
      "name": "Domain knowledge",
      "score": 0.308734655380249
    },
    {
      "name": "Linguistics",
      "score": 0.10921621322631836
    },
    {
      "name": "Programming language",
      "score": 0.105008065700531
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210139030",
      "name": "Samsung (India)",
      "country": "IN"
    }
  ]
}