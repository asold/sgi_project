{
  "title": "Anchor free based Siamese network tracker with transformer for RGB-T tracking",
  "url": "https://openalex.org/W4385876583",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4384087437",
      "name": "liangsong fan",
      "affiliations": [
        "Jilin University of Chemical Technology",
        "Silla University"
      ]
    },
    {
      "id": "https://openalex.org/A2098467702",
      "name": "Pyeoung-Kee Kim",
      "affiliations": [
        "Silla University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2527415613",
    "https://openalex.org/W2775609985",
    "https://openalex.org/W2980475931",
    "https://openalex.org/W3157456624",
    "https://openalex.org/W2963188742",
    "https://openalex.org/W2963854930",
    "https://openalex.org/W2963534981",
    "https://openalex.org/W2799058067",
    "https://openalex.org/W2962824803",
    "https://openalex.org/W2886910176",
    "https://openalex.org/W4224210628",
    "https://openalex.org/W3005080107",
    "https://openalex.org/W3012425959",
    "https://openalex.org/W3064498204",
    "https://openalex.org/W2963051855",
    "https://openalex.org/W3214586131",
    "https://openalex.org/W2765667535",
    "https://openalex.org/W2214012879",
    "https://openalex.org/W2154889144",
    "https://openalex.org/W2577056945",
    "https://openalex.org/W182940129",
    "https://openalex.org/W4312668764",
    "https://openalex.org/W3035020406",
    "https://openalex.org/W4283808043",
    "https://openalex.org/W2998756268",
    "https://openalex.org/W3132864630",
    "https://openalex.org/W3101990647",
    "https://openalex.org/W2998395690",
    "https://openalex.org/W2970995493",
    "https://openalex.org/W2240536489",
    "https://openalex.org/W3127317646",
    "https://openalex.org/W2996575194",
    "https://openalex.org/W3193488896",
    "https://openalex.org/W3183152796",
    "https://openalex.org/W3002567850",
    "https://openalex.org/W2909946038",
    "https://openalex.org/W2896228140",
    "https://openalex.org/W2889260328",
    "https://openalex.org/W2964423614",
    "https://openalex.org/W2963905288",
    "https://openalex.org/W3155938136",
    "https://openalex.org/W4225523457",
    "https://openalex.org/W4206759694",
    "https://openalex.org/W4220766186",
    "https://openalex.org/W3099681648"
  ],
  "abstract": "Abstract In recent years, many RGB-THERMAL tracking methods have been proposed to meet the needs of single object tracking under different conditions. However, these trackers are based on ANCHOR-BASED algorithms and feature cross-correlation operations, making it difficult to improve the success rate of target tracking. We propose a siamAFTS tracking network, which is based on ANCHOR-FREE and utilizes a fully convolutional training network with a Transformer module, suitable for RGB-THERMAL target tracking. This model addresses the issue of low success rate in current mainstream algorithms. We also incorporate channel and channel spatial attention modules into the network to reduce background interference on predicted bounding boxes. Unlike current ANCHOR-BASED trackers such as MANET, DAPNet, SGT, and ADNet, the proposed framework eliminates the use of anchor points, avoiding the challenges of anchor hyperparameter tuning and reducing human intervention. Through repeated experiments on three datasets, we ultimately demonstrate the improved success rate of target tracking achieved by our proposed tracking network.",
  "full_text": "1\nVol.:(0123456789)Scientific Reports |        (2023) 13:13294  | https://doi.org/10.1038/s41598-023-39978-7\nwww.nature.com/scientificreports\nAnchor free based Siamese \nnetwork tracker with transformer \nfor RGB‑T tracking\nLiangsong Fan 1,2 & Pyeoungkee Kim 1*\nIn recent years, many RGB‑THERMAL tracking methods have been proposed to meet the needs of \nsingle object tracking under different conditions. However, these trackers are based on ANCHOR‑\nBASED algorithms and feature cross‑correlation operations, making it difficult to improve the success \nrate of target tracking. We propose a siamAFTS tracking network, which is based on ANCHOR‑FREE \nand utilizes a fully convolutional training network with a Transformer module, suitable for RGB‑\nTHERMAL target tracking. This model addresses the issue of low success rate in current mainstream \nalgorithms. We also incorporate channel and channel spatial attention modules into the network \nto reduce background interference on predicted bounding boxes. Unlike current ANCHOR‑BASED \ntrackers such as MANET, DAPNet, SGT, and ADNet, the proposed framework eliminates the use \nof anchor points, avoiding the challenges of anchor hyperparameter tuning and reducing human \nintervention. Through repeated experiments on three datasets, we ultimately demonstrate the \nimproved success rate of target tracking achieved by our proposed tracking network.\nAbbreviations\nSiam AFTS  we propose that we build a new tracker  for RGB-T target tracking based on the siamese  net-\nwork, called Anchor-Free based Transformer network System (Siam AFTS)\nRGB-T  RGB-T images refer to the fact that each  set of images consists of a combination of images of  \nboth visible and thermal infrared light modes\nIn order to predict and track the next location of a target, target tracking algorithms typically start with a specific \ninitial target position feature, establish it as a reference, and then perform correlation operations with consecutive \nframes. Most tracking trackers are based on RGB images. However, in some challenging conditions such as fog, \nrain, and darkness, where the target is not clearly visible in RGB images, the tracker often fails to successfully \ntrack the target. In recent years, there has been increasing research interest in multi-modal trackers that com -\nbine other modalities with RGB images, such as RGB-T tracking, to address this limitation. To enhance track -\ning performance, Li et al. proposed a method based on the fusion of grayscale and thermal image  categories1. \nFurthermore, they publicly shared their dataset in the paper. RGB-THERMAL trackers are more competitive as \nthey leverage the complementary advantages of the fused RGB and thermal modes. In this approach, thermal \ninfrared images are unaffected by lighting conditions, while RGB photographs capture detailed information \npertaining to the target.\nSince  Li2 successfully improved the accuracy of target tracking by using Siamese networks, which extract \nfeatures from two identical branching networks, this Siamese network has been widely applied in the design of \nRGB-THERMAL target  tracking6–9  and10. all demonstrate that Siamese networks can effectively improve the \naccuracy and success rate of target tracking. In this study, we recommend the twin network for RGB-THERMAL \nfusion tracking, primarily for the tracking network’s stability consideration.\nIn recent years, several different RGB-THERMAL tracking techniques have been introduced. Xingchen \nZhang et al.3 employed a fully convolutional network and multilayer feature fusion to enhance thermal tracking \nperformance. Guo et al. 4 utilized a deep network model and combined RGB and thermal heat score maps to \nincrease tracking speed.  Zhu5 proposed a novel concept that emphasizes the importance of layers capturing key \nfeatures and combined the separately collected layers to produce better prediction results.\nThe mentioned RGB-T trackers and many advanced RGB-T  models11–14 are inspired by the Siam  RPN8 model, \nwhich primarily involves predefining anchor frames of different  sizes6,9 that each sample must match, leading to \nOPEN\n1Silla University, 140, Baekyang-daero 700beon-gil, Sasang-gu, 46958, Busan, Korea. 2Jilin Institute of Chemical \nTechnology, No. 45 Chengde Street, Jilin City 132022, Jilin Province, China. *email: pkkim@silla.ac.kr\n2\nVol:.(1234567890)Scientific Reports |        (2023) 13:13294  | https://doi.org/10.1038/s41598-023-39978-7\nwww.nature.com/scientificreports/\nincreased computation and processing time. These anchor frames are artificially designed and have a significant \nimpact on the effectiveness of the tracking model, requiring a considerable amount of manual intervention during \nthe experimental process. How to enable network models to learn autonomously without human intervention \nis also a current direction of development in artificial intelligence.\nCurrently, Siamese networks are widely used in advanced RGBT tracking algorithms. Siamese networks typi-\ncally perform correlation operations on image features extracted from the target template and search region, and \ndetermine their similarity based on the similarity score. The target object is considered successfully tracked if \nthe similarity score is high. However, correlation operations, which involve convolving two feature maps, may \nresult in significant information loss, leading to a low success rate in target tracking. Recently, in the field of RGB-\nbased target tracking, many researchers have incorporated Transformer technology into the design of tracking \nalgorithms and achieved promising results. However, there are few studies that apply Transformer technology to \nRGBT-based target tracking, and the accuracy and success rate of RGBT trackers using Transformer are not high. \nInspired by the application of Transformer in target tracking, we aim to design a new model with a Transformer \nframework to improve the accuracy and success rate of RGBT target tracking.\nThis study proposes an end-to-end trainable tracker based on Transformer for robust RGBT tracking. Firstly, \nwe extract features from RGB and thermal infrared images separately. Then, we employ spatial and channel \nattention modules to enhance these features and improve the resolution of fused features, effectively reducing \nthe discrepancies between modality features and eliminating background interference. Finally, by fusing these \nimage features, we obtain a response map through our newly designed Transformer module. Our novel model \npredicts the target’s position and bounding box using only one response map. In summary, our main contribu-\ntions include:\n1) We designed an end-to-end offline tracking training model by using convolutional neural networks. Previ-\nous RGBT designs have been anchor-based; in this case, our model is anchor-free, which does not depend \non pre-defined boxes and can be trained on well-annotated datasets and achieve good results.\n2) We have designed a new Transformer module specifically for RGBT target tracking. Experimental results \ndemonstrate that the use of this module significantly improves the accuracy and success rate of target track-\ning. Our designed tracker takes into account the contributions of both RGB and TIR modes in modeling the \ntarget. It effectively utilizes the feature information to enhance the robustness of the model.\n3) We conducted an in-depth analysis of the GTOT, GBT210, and RGBT234 datasets. The results show that our \nproposed method exhibits some gaps when compared to the latest supervised RGBT algorithms. However, \nour tracker demonstrates highly competitive performance in many aspects.\nProposed method\nFigure 1 depicts the overall frame construction. We outline each part of our technique separately in this section.\nTIR\nCA-Module\n7*7*n\n31*31*n\n31*31*n\nRGB\nANNT-RGB\nTransformer\nTIR\nANNT-TIR\nTIR\nSA-Module\nRGB\nANNT-RGB\nTIR\nANNT-TIR\n7*7*n\n31*31*n\n31*31*n\n7*7*n\n7*7*n\n7*7*n\n7*7*n\n31*31*n\n31*31*n\nRGB\nRGB\nBackbone\nConv\n25*25*m backgroundforeground\nL T R B\n25*25*2m\nConv\n25*25*m\n25*25*2\nAnchor-free\nFigure 1.  The frame construction.\n3\nVol.:(0123456789)Scientific Reports |        (2023) 13:13294  | https://doi.org/10.1038/s41598-023-39978-7\nwww.nature.com/scientificreports/\nBackbone. As shown in Fig.  1 as the input of 4 images, our input consists of a visible light branch and a \nthermal infrared branch. The target branch in turn contains visible input Z1 and thermal infrared input Z2, and \nthe search area branch is divided into visible input X1 and thermal infrared input X2. Since it is a Siamese model, \nboth the visible and thermal infrared branches use the same ResNet50 model to extract feature maps. After back-\nbone network feature extraction, the visible branch will get feature maps ϕ( Z1) and feature maps ϕ( X1),and the \nthermal infrared branch will get feature maps ϕ( Z2) and feature maps ϕ( X2) . Then, we obtain the feature maps \nϕ( Z1),ϕ( X1),ϕ( Z2),and ϕ( X2) , where a portion is directly fed as input to the Transformer module for the next \nstep of computation, while another portion is enhanced by passing through the SA and CA modules to augment \nthe feature maps before being transmitted to the Transformer module for further computation.\nDuring the object tracking process, we aim to include more image feature information in the response map. \nInspired by  reference7, we consider extracting feature maps from different layers as outputs during the process of \nfeature extraction. Deep features and shallow features have different roles in target tracking. First, deep features \nhave good discrimination of the required speech properties, which are enhanced for our classification task. In \ncontrast, shallow features are rich in information about visual attributes such as edges and colors, which are \nenhanced for the target localization task. Inspired by  references7,15, We modify the last module of ResNet50 to \nobtain feature maps from layers 6, 7, and 8. We obtain  F6(X1),  F7(X1),  F8(X1). And we also get  F6(X2),  F7(X2), \n F8(X2). Here 6, 7, and 8 indicates the feature values we extracted from layer 6 layer 7, and layer 8. There are 256 \nchannels in  F6(X2),  F7(X2),  F8(X2).\nModule for spatial attention. The channel attention mechanism can enhance the predictive capability \nof the network. To improve the information transfer capability between the two modes, we designed a Channel \nAttention Feature Enhancement module (CA module) as shown in Fig. 2.\nIn the CA module Fig. 2, we take the feature map ϕ( Z1) and ϕ( Z2) extracted through the backbone network \nas input to the CA module and obtain the joint feature as Uca . denoting the output as xca\nrgb , x ca\ntir , and the overall \nCA module can be described as follows:\nwhere δ represents the full set, ω is the fully connected layer, ε denotes Sigmoid, ⊗ is the product of channel-wise, \nand Split is the operation of extracting features along the channel dimension.\nIn order to suppress the effect of background noise on the classification task, we designed a spatial attention \n(SA module) module. It is shown in Fig. 3. This module mainly utilizes the spatial inter-relationship of features. \nWe take ϕ( X1) and ϕ( X2) extracted through the backbone network as the input feature maps, and by using the \nSA module, we finally obtain the feature map Uca using the following mathematical expression as:\nwhere ρ is for the average set, φ or the largest set, Cat stands for the process of stringing features along the channel \ndimension, ϕ stands for the two-dimensional convolution operation, H stands for a collection of kernel weights, \nand ε stands for the Sigmoid function.\nThe output is then represented as x sa\nrgb , x sa\ntir , and SA module as seen below:\n(1)xca\nrgb,xca\ntir= CA\n(\nU ca)\n= Split (U ca ⊗ ε\n(\nω\n(\nδ\n(\nU ca)))\n(2)V sa= ε\n(\nϕ\n(\nCat\n(\nρ\n(\nU sa)\n,φ\n(\nU sa))\n,H\n))\nFigure 2.  CA model architecture.\nFigure 3.  SA model architecture.\n4\nVol:.(1234567890)Scientific Reports |        (2023) 13:13294  | https://doi.org/10.1038/s41598-023-39978-7\nwww.nature.com/scientificreports/\n⊙ denotes connecting a CA module to an SA module, and finally, we get the final SA module.\nTransformer network. Inspired by  reference16, we designed a Transformer Network as shown in Fig. 4.\nFrom Fig. 4, we can see that the template feature and search feature extracted by the backbone network first \npass through the CA and SA channel modules respectively, and obtain xsa\nrgb,x sa\ntir, ˆzf\nca\nrgb and ˆzf\nca\ntir . Then, the feature \nvectors are fed into the Transformer module, as shown in Fig. 4. First, xsa\nrgb and ˆzf\nca\nrgb pass through a Transformer \nattention module (BCTM). Then, x sa\ntir and ˆzf\nca\ntir pass through another Transformer attention module (BCTM). \nBCTM is used to fuse different branch information. To make the fusion information more accurate, the fusion \nprocess is repeated four times. Finally, an extra Transformer module (ACTM) is added to fuse the feature vectors \nof the template and search branches. BCTM and ACTM have the same network structure. Here, we will provide \na detailed explanation using BCTM as an example.\nFigure  5 shows the BCMT module for transformers. The BCMT module utilizes positional encoding to \ndistinguish position information of feature sequences and utilizes a residual-based multi-head cross-attention \nto integrate feature vectors from different inputs. Additionally, a residual-based feed-forward network (FFN) is \nemployed to obtain the final output. The specific calculation process of the BCMT module is as follows:\nThe calculation process of the CMT module involves W ∈ Rd×Nx and W KV ∈ Rd×NKV as two inputs from \ndifferent branches, while PQ ∈ Rd×NQ and PKV ∈ Rd×NKV represent spatial positional encodings of  WQ and \nW KV . The output of the residual multi-head cross-attention and the final output are represented by W CF′ and \nW CF , respectively.\nAfter ACTM, we obtain enhanced image features R1 of size 25*25*256. Next, referring to the lower part of \nFig. 4, we observe that the image features that have not undergone the CA and SA modules are initially fused \nseparately. For example, fusion of features ϕ( X1) and ϕ( X2) results in image feature ϕ( X) of size 31*31*m. \n(3)xsa\nrgb,xsa\ntir= SA\n(\nU sa)\n= Split\n(\nU sa ⊙ V sa)\n(4)\nW CF ′= W Q + MultiHead(W Q + PQ , W KV + PKV , W KV )\nW CF = W CF ′+ FFN(W CF ′)\nFigure 4.  Illustration of the features fusion network based on the transformer.\n5\nVol.:(0123456789)Scientific Reports |        (2023) 13:13294  | https://doi.org/10.1038/s41598-023-39978-7\nwww.nature.com/scientificreports/\nFusion of features ϕ( Z1) and ϕ( Z2) results in image feature ϕ( Z) of size 7*7*m. Firstly, ϕ( X) passes through a \nTransformer attention module (STM). Then, ϕ( Z) also passes through a Transformer attention module (STM). \nFinally, we transmit the obtained Q, K, and V to the Transformer attention module (BCTM), from which we \ncan obtain image features R2 of the original image. We perform a CAT operation on R1 and R2, resulting in R:\nIn Fig.  6, we can observe the self-attention modules for the transformer (STM). These modules begin by \nincorporating a positional encoding technique to accurately differentiate the position information within feature \nsequences. Next, they utilize multi-head self-attention to consolidate the feature vectors from various positions. \nLastly, a residual form is employed to obtain the output. The specific calculation process of the TS module is \ndescribed below.\nwhere PK ∈ Rd×Nx denotes the spatial positional encoding obtained through the application of a sine function. \nW ∈ Rd×Nx represents the input to the TS module, while W SF ∈ Rd×Nx denotes the resulting output after the TS \nmodule’s operations.\nAnchor‑free based bounding box prediction. a. Position prediction head. The location prediction \nhead in Fig.  1 includes classification and regression modules. After passing through the Transformer attention \nmodule, we obtain image features of size (*, 256, 25, 25). These features are subsequently used in the location \nprediction head to generate image features of size (*, 2, 25, 25) and (*, 4, 25, 25) for classification and regression \nbranches, respectively.\nb. Training loss. Firstly, we classify the input samples into positive and negative samples. Since negative samples \nhave a lower probability of representing the target, we only perform regression operations on positive samples.\nWe will determine whether a sample is positive or negative by drawing two ellipses, S1 and S2, around the \ntarget. We may obtain an ellipse S1 as illustrated in the equation that follows.\n(5)R(X) = CAT(R1,R2)\n(6)W SF = W + MultiHead(W + PK , W + PK , W )\nFFN\nCross Transformer Model network\nMuiti-Head Cross-Attention Add&Norm\nV\nQ Output:Wout\nAdd&NormK\nInput:Wkv\nInput:WQ\nPositional Encoding\nPositional Encoding\nFigure 5.  Illustration of the transformer cross transformer module.\nSelf Transformer Model network\nMuiti-Head-Self-Attention Add&NormV\nQ Output:Wout\nK\nInput:Winput\nPositional Encoding\nFigure 6.  Illustration of the transformer self transformer module.\n6\nVol:.(1234567890)Scientific Reports |        (2023) 13:13294  | https://doi.org/10.1038/s41598-023-39978-7\nwww.nature.com/scientificreports/\nLikewise, we can obtain an ellipse S2:\nIf a sample point (k, j) lies outside the ellipse S1, it is defined as a negative sample. Conversely, if it lies inside \nS2, it is defined as a positive sample.\nFor the coordinates of positive samples, we perform regression operations. In anchor-based regression, we \ntypically compare predicted boxes with ground truth boxes. However, in the anchor-free regression algorithm \nwe employ, we use the following equation for regression calculations.\nwhere d l , dt , dr , db and are the distances from that place to the four edges of the surrounding box. For the calcula-\ntion of the loss function, we use IOU (Intersection over Union). By modifying the coordinates of the predicted \nbounding box’s top-left corner and bottom-right corner, we can obtain the predicted bounding box for each \npoint on the feature map corresponding to the search image. IOU represents the ratio of the intersection area \nbetween the ground truth and the predicted bounding box.\nIf the regression value exceeds 0 and the point (x, y) marked as a positive sample lies within the ellipse S2, \nthen the IOU value falls between 0 and 1.\nTracking. The RGBT series consists of visible light and thermal infrared images. The visible light photos and \nthermal infrared images undergo a cropping process. The size of the search image is adjusted to 255 × 255 pixels, \nwhile the size of the template image is adjusted to 127 × 127 pixels. From these images, two sets are selected, each \ncontaining 60 samples (40 negative samples and 20 positive samples), which are the visible light and thermal \ninfrared images.\nThe first step of the prediction process is to set up the tracker, which handles the first frame. Then, we save \nthe image information of the first frame. The search image (second frame) is processed through the backbone to \nextract feature maps from the 6th, 7th, and 8th layers, which are then resized to 7 × 7. The feature maps extracted \nfrom the visible light and thermal infrared images are separately processed using the SA and CA channel atten-\ntion mechanisms, preparing them for the next step of operations.\nWe separately input the extracted unenhanced and enhanced visible light and thermal infrared feature maps \ninto the Transformer Network. Then, after undergoing transformation in the Transformer Network, we perform \nclassification and regression operations on the obtained outputs. By performing regression operations using the \nfollowing equation:\nThe top and bottom right corners of the prediction box are ( Px1,Px2 ) and ( Py1,Py2 ), respectively, while ( d reg\nl  , \nd reg\nt ,d reg\nr  and d reg\nb  ) denote the projected values of the regression box. The optimal tracking box is chosen from the \ngenerated prediction boxes, and the tracking box coordinates are updated through linear interpolation with the \nprevious frame’s state to achieve tracking. After generating the prediction boxes, cosine windows are applied to \nmitigate significant displacements, and penalties are introduced to discourage substantial changes in size and \nscale. Through the aforementioned series of operations, we ultimately obtain the best predicted bounding box.\nExperiments\nData set and device description. In this study, we will evaluate our model by testing it on two datasets, \n GTOT1 ,  RGBT21016 and RGBT234.\nUsing PyTorch and two GTX 3080-Ti cards for training, the algorithm is put into practice. The search region’s \ninput size was 255 pixels, whereas the template’s input size was 127 pixels for comparison’s sake. we build a \ntraining subnetwork with the ResNet-50 as its core. Using ImageNet, the network had already been trained. The \npre-trained weights served as an initialization for our model’s further training.\nCompared with SOTA RGB‑T tackers. Compared anchor‑based methods. We carefully selected a se-\nries of Anchor-Based supervised RGB-T trackers for comparison. These include  HMFT22,  CMPP23,  DMCNet27, \n(7)\n(\nsj − gthxc\n)2\n( gthw\n2\n)2 +\n(\nsj − gthyc\n)2\n( gthh\n2\n)2 = 1\n(8)\n(\nsj − gthxc\n)2\n( gthw\n4\n)2 +\n(\nsj − gthyc\n)2\n( gthh\n4\n)2 = 1\n(9)\nd l = sj − gthx1\nd t = sk − gthy1\nd r = gthx2−sj\nd b = gthy2−sk\n(10)LIOU =1-I O U\n(11)\nP x 1 = sj − d reg\nl\nP y1 = sk − d reg\nt\nP x 2 = d reg\nr + sj\nP y2 = d reg\nb + sk\n7\nVol.:(0123456789)Scientific Reports |        (2023) 13:13294  | https://doi.org/10.1038/s41598-023-39978-7\nwww.nature.com/scientificreports/\n JMMAC26,  CBPNet33,  MaCNet25,  MANet34,  CMP35,  MFGNet36,  DAPNet37,  mfDiMP38,  LTDA31,  DuSiamRT39, \n TCNN29, JCDA-InvSR30, and  SiamDW47 + RGBT.\nTo broaden the scope of comparison, we also included transformer-based method  APFNet48 and non-deep \nRGB-T trackers, such as  CMCF40,  NRCMR32,  CMR41,  SGT42, the method proposed by Li et al. 43,  CSR32,44, \n MEET45 + RGBT, and  KCF46 + RGBT. This selection encompasses a wide range of RGB-T methods across vari -\nous categories.\nResults on GTOT. The method of this study further concludes the tracking on the GTOT dataset, which con-\ntains 50 different video sequences and considers different environmental conditions, as shown in Table 1\nFigure 7 shows the comparison results of our proposed model with other anchor-based models on the GTOT \ndatasets.\nTable 3 displays the tracking results on the GTOT dataset. In terms of these two metrics, our RGB-T tracker \nachieves almost superior success rates compared to all Anchor-Based RGB-T trackers. However, we also notice \nthat our results in accuracy are not high. The performance gap between our supervised RGB-T tracker and \nthe state-of-the-art can be attributed to their usage of large-scale annotated RGB-T image pairs for training. \nAdditionally, these trackers employ more complex models. In the future, we will modify our model to improve \naccuracy.\nResults on RGBT210. The tracking outcomes of this technique using the RGBT210 data set are shown in Fig. 8. \n210 real-label visible and thermal infrared video clips are included in RGBT210. This data collection takes a lot \nof difficult cases into account, as illustrated in Table 25.\nBy performing validation on the RGBT210 dataset, Fig. 8 shows that our tracker beats all trackers.\n Results on RGBT234. The results on the RGBT234 dataset, presented in Table  3, demonstrate that our pro-\nposed RGB-T tracker outperforms both supervised and non-learning-based RGB-T trackers in terms of MSR. \nHowever, its performance on RGBT234 is relatively weaker compared to the GTOT dataset. This discrepancy \ncan be attributed to the increased challenges posed by RGBT234, which comprises 234 images and encom-\npasses 12 challenging attributes, surpassing the 7 attributes of GTOT. On the RGBT234 dataset, our model was \ncompared to other Anchor-Based models. The comparison results show that we have achieved almost superior \nperformance compared to all algorithms. However, it is worth noting that we have lower accuracy in certain \naspects. To address this performance gap, our future work aims to explore better backbone trackers and larger \ntraining datasets.\nBy performing validation on the RGBT234 dataset, Fig. 9 shows that our tracker beats all trackers.\nAttribute‑based results. The performance of the challenging attributes on the RGBT234 dataset is shown in \nFigs. 10 and 11.\nIt can be observed that our RGB-T tracker achieves highly competitive performance in various aspects. In the \nMSR graph of the challenging attributes, our model performs well in most attributes except for TC, PO, NO, and \nLI, where it is not as effective as other models. However, our model performs well in the remaining attributes. In \nthe MPR graph of the challenging attributes, our model performs poorly in SV , PO, and NO, but demonstrates \nexcellent performance in the other nine attributes. Overall, our model faces challenges in dealing with PO and \nNO, indicating areas for improvement in our future work.\nQualitative results. As shown in Fig. 12, our RGB-T tracker is compared qualitatively with other anchor-based \nRGB-T trackers on the RGBT210 dataset. The images in Fig.  12 are sourced from the RGBT210  dataset17. We \nwould like to express our gratitude to  LI17 for making the dataset publicly available. We selected several RGB-T \ntrackers, including  SOWP18, SOWP + RGBT,  KCF19 + RGBT,  CSR20, SGT, and  MEEM21 + RGBT. It can be seen \nfrom the figure that our RGB-T tracker performs better on these three sequences (Baketballwaliking , Balance-\nbike, car41).\nTable 1.  A list of annotated attributes for the GTOT data set.\nAttr Description\nOCC Occlusion\nLSV Large scale variation\nFM Fast motion\nLI Low illumination\nLR Low resolution\nTC Thermal crossover\nDEF Deformation\nSO Small object\n8\nVol:.(1234567890)Scientific Reports |        (2023) 13:13294  | https://doi.org/10.1038/s41598-023-39978-7\nwww.nature.com/scientificreports/\n Ablation studies and analysis. As shown in Fig. 13, these are the results of our ablation experiment. In \nthis experiment, we used the RGBT234 dataset as the training set and the GTOT dataset as the test set. From \nthe graph, we can obtain the following information: the experimental results without adding any module are \nsignificantly worse compared to the experimental results with the modules added. In the PR score graph, the \nexperimental results of \"ours-GTOT-SA\" and \" ours-GTOT-CA\" are significantly better than the experimental \nresults of \"ours-GTOT-no (CA-SA-TS)\", indicating that adding the SA and CA modules helps improve the accu-\nracy of the tracker. In the SR score graph, we found that the experimental results of \"ours-GTOT-SA\" are bet-\nter than those of \" ours-GTOT-no (CA-SA-TS)\". However, the experimental results of \"our-ca\" have decreased \ncompared to \"ours-GTOT-no (CA-SA-TS)\", indicating that adding the CA module does not help improve the \nsuccess rate of the tracker, but it still has an effect on improving the accuracy of the tracker. In this experiment, \nthe experimental results of \"ours-AFTS\" are the highest, indicating that the TS module has a significant impact \non improving the success rate of the tracker.\nConclusion\nThis paper introduces a novel approach for RGBT tracking, specifically an adaptive tracker based on the Trans-\nformer model with dual-Siamese architecture and anchor-free design.\nThe proposed method incorporates Transformer attention mechanism to replace the correlation operation \nin the Siamese network, leading to improved tracking success rate. By eliminating candidate boxes and reducing \nhuman-induced interference, our approach effectively addresses the limitations of Anchor-Based methods while \neliminating the need for many hyperparameters. Experimental results demonstrate the reliability of the proposed \nalgorithm, which successfully exploits the complementary information from visible light and thermal infrared \nmodalities. As part of our future work, we are exploring the integration of RGBD tracking design, aiming to \nexpand the application scope and enhance the performance in challenging scenarios.\n05 10 15 20 25\nLocation error threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nPrecision\nPrecision Plot\nDMCNet[0.909]\nADRNet[0.904]\nMANet[0.894]\nDAPNet[0.882]\nSGT[0.851]\nours-AFTS[0.849]\nMDNet+RGBT[0.8]\nDAT[0.771]\nECO[0.77]\nRT-MDNet[0.745]\nADNet[0.718]\nSTRUCK[0.681]\nSiamDW+RGBT[0.68]\nMEEM[0.648]\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\noverlap threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nSuccess Rate\nSuccess Plot\nours-AFTS[0.777]\nADRNet[0.739]\nDMCNet[0.733]\nMANet[0.724]\nDAPNet[0.707]\nMDNet+RGBT[0.637]\nECO[0.631]\nADNet[0.629]\nSGT[0.628]\nDAT[0.618]\nRT-MDNet[0.613]\nSiamDW+RGBT[0.565]\nSTRUCK[0.533]\nMEEM[0.523]\nFigure 7.  The results of our proposed model compared to other Anchor-Based models on the GTOT datasets, \n(a) Precision Rate; (b) Success Rate.\n9\nVol.:(0123456789)Scientific Reports |        (2023) 13:13294  | https://doi.org/10.1038/s41598-023-39978-7\nwww.nature.com/scientificreports/\n05 10 15 20 25 30 35 40 45 50\nLocation error threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nPrecision\nPrecision Plot\nours-AFTS[0.873]\nHMFT[0.786]\nCCOT[0.705]\nECO[0.690]\nSGT[0.675]\nSRDCF[0.619]\nBACF[0.616]\nSOWP[0.599]\nStaple[0.595]\nSiameseFC[0.586]\nStaple-CA[0.580]\nACFN[0.530]\nDSST[0.522]\nCFnet[0.518]\nMEEM+RGBT[0.505]\nCNN+KCF+RGBT[0.493]\nSAMFAT[0.489]\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\noverlap threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nSuccess Rate\nSuccess Plot\nours-AFTS[0.564]\nHMFT[0.534]\nCCOT[0.502]\nECO[0.498]\nBACF[0.451]\nSRDCF[0.442]\nSGT[0.430]\nStaple[0.429]\nStaple-CA[0.421]\nSiameseFC[0.412]\nSOWP[0.379]\nACFN[0.374]\nCFnet[0.360]\nSAMFAT[0.346]\nCNN+KCF+RGBT[0.331]\nDSST[0.324]\nMEEM+RGBT[0.319]\nFigure 8.  The results of our proposed model compared to other Anchor-Based models on the RGBT210 \ndatasets. (a) Precision Rate; (b) Success Rate.\nTABLE 2.  List of the attributes annotated to RCBT210.\nAttr Description\nNO No occlusion—the target is not occluded\nPO Partial occlusion—the target object is & partially occluded\nHO Heavy occlusion—the target object is & heavy occluded\n(80% percentage)\nLI Low illumination—the illumination in the target region is low\nLR Low resolution—the resolution in the target region is low\nTC Thermal crossover—the target has a similar temperature to other objects or background surroundings\nDEF Deformation—non-rigid object deformation\nFM Fast motion—the motion of the ground truth between two\nadjacent frames are larger than 20 pixels\nSV Scale variation—the ratio of the first bounding box and the current\nbounding box is out of the range [0. 5, 1]\nMB Motion blur—the target object’s motion results in the blur image information\nCM Camera moving—the target object is captured by a moving camera\nBC Background clutter—the background information which includes\nthe target object is messy\n10\nVol:.(1234567890)Scientific Reports |        (2023) 13:13294  | https://doi.org/10.1038/s41598-023-39978-7\nwww.nature.com/scientificreports/\nTable 3.  Comparison with existing anchor based RGB-T trackers on the GTOT dataset and RGBT234 dataset. \nThe results marked with ’∞’ are computed by us using raw tracking results. The results marked with ’*’ are \ncopied from references 32,48. Other results are extracted from corresponding papers. ’––’ means not mentioned \nin the corresponding paper. Values worse than our method are marked in pink and yellow.\nTracker\nGTOT RGBT234\nCategory Supervised VenueMPR(↑) MSR(↑) MPR(↑) MSR(↑)\nHMFT∞22 90.6 74.2 78.8 56.8 DL-based Ye s CVPR 2022\nCMPP23 92.6 73.8 82.3 57.5 DL-based Ye s CVPR 2020\nAPFNet24 90.5 73.7 82.7 57.9 DL-based Ye s AAAI 2022\nDMCNet ∞27 90.9 73.3 83.9 59.3 DL-based Ye s IEEE TNNLS 2022\nJMMAC26 89.3 73.1 79.0 57.3 DL-based Ye s IEEE TIP 2021\nCBPNet33 88.5 71.6 79.4 54.1 DL-based Ye s IEEE TMM 2021\nMaCNet25 88.0 71.4 79.0 55.4 DL-based Ye s Sensors 2020\nMANet ∞34 88.9 71.1 77.7 53.9 DL-based Ye s ICCVW 2019\nCMP35 86.9 71.1 75.1 49.1 DL-based Ye s Neurocomputing 2021\nMFGNet36 88.9 70.7 78.3 53.5 DL-based Ye s IEEE TMM 2022\nDAPNet∞37 87.4 68.9 76.6 53.7 DL-based Ye s ACM MM 2019\nmfDiMPS*38 84.1 69.3 78.5 55.9 DL-based Ye s ICCVW 2019\nLTDA31 84.3 67.7 78.7 54.5 DL-based Ye s ICIP 2019\nDuSiamRT39 76.6 62.8 56.7 38.4 DL-based Ye s The Visual Computer \n2022\nTCNN29 85.2 62.6 – – DL-based Ye s Neurocomputing 2018\nJCDA-InvSR30 – 60.5 60.6 41.4 ML-based Ye s IEEE TIP 2019\nSiamDw47 + RGBTS* 68.0 56.5 60.4 39.7 DL-based Ye s CVPR 2019\nCMCF40 77.0 63.2 – – Non-DL (CF-based) Neurocomputing 2019\nNRCMR32 83.7 66.4 72.9 50.2 Non-DL (Graph-based) IEEE TNNLS 2021\nCMR41 82.7 64.3 – – Non-DL (Graph-based) ECCV 2018\nSGTS ∞42 85.1 62.8 72.0 47.2 Non-DL (Graph-based) ACM MM 2017\n43 84.2 62.2 – – Non-DL (Graph-based) SPIC 2018\nCSR ∞28 74.5 61.5 46.3 32.8 Non-DL (SR-based) IEEE TIP 2016\n44 77.3 61.2 72.9 48.6 Non-DL(Graph-based) Neurocomputing 2022\nMEEM45 + RGBTS∞ – 52.0 63.6 40.5 Non-DL ECCV 2014\nKCF46 + RGBTS* – 42.0 46.3 30.5 Non-DL (CF-based) IEEE TPAMI 2014\nOURS 84.9 77.7 89.0 60.2 DL-based Ye s\n11\nVol.:(0123456789)Scientific Reports |        (2023) 13:13294  | https://doi.org/10.1038/s41598-023-39978-7\nwww.nature.com/scientificreports/\n05 10 15 20 25 30 35 40 45 50\nLocation error threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nPrecision\nPrecision Plot\nour-AFTS[0.893]\nDMCNet[0.839]\nAPFNet[0.827]\nADRNet[0.807]\nHMFT[0.788]\nSOWP[0.642]\nMEEM+RGBT[0.636]\nDSST[0.524]\nKCF+RGBT[0.463]\nCSR[0.463]\nL1-PF[0.431]\nJSR[0.343]\n00 .1 0.20 .3 0.40 .5 0.60 .7 0.80 .9 1\noverlap threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nSuccess Rate\nSuccess Plot\nour-AFTS[0.602]\nDMCNet[0.593]\nAPFNet[0.579]\nADRNet[0.570]\nHMFT[0.569]\nSOWP[0.411]\nMEEM+RGBT[0.405]\nDSST[0.336]\nCSR[0.328]\nKCF+RGBT[0.305]\nL1-PF[0.287]\nJSR[0.234]\nFigure 9.  The results of our proposed model compared to other Anchor-Based models on the RGBT234 \ndatasets. (a) Precision Rate; (b) Success Rate.\n12\nVol:.(1234567890)Scientific Reports |        (2023) 13:13294  | https://doi.org/10.1038/s41598-023-39978-7\nwww.nature.com/scientificreports/\n05 10 15 20 25 30 35 40 45 50\nLocation error threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nMaximum Precision Rate\nPrecision Plot - Background Clutter\nour-AFTS[0.951]\nDMCNet[0.838]\nAPFNet[0.813]\nADRNet[0.804]\nHMFT[0.738]\nMEEM+RGBT[0.629]\nSOWP[0.528]\nDSST[0.458]\nKCF+RGBT[0.429]\nCSR[0.388]\nL1-PF[0.342]\nJSR[0.332]\n05 10 15 20 25 30 35 40 45 50\nLocation error threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nMaximum Precision Rate\nPrecision Plot - Camera Moving\nour-AFTS[0.884]\nDMCNet[0.801]\nAPFNet[0.779]\nHMFT[0.779]\nADRNet[0.743]\nSOWP[0.598]\nMEEM+RGBT[0.585]\nCSR[0.415]\nKCF+RGBT[0.401]\nDSST[0.399]\nL1-PF[0.316]\nJSR[0.291]\n05 10 15 20 25 30 35 40 45 50\nLocation error threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nMaximum Precision Rate\nPrecision Plot - Deformation\nour-AFTS[0.816]\nAPFNet[0.785]\nDMCNet[0.779]\nHMFT[0.776]\nADRNet[0.743]\nMEEM+RGBT[0.617]\nSOWP[0.611]\nCSR[0.448]\nDSST[0.438]\nKCF+RGBT[0.410]\nL1-PF[0.364]\nJSR[0.278]\n05 10 15 20 25 30 35 40 45 50\nLocation error threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nMaximum Precision Rate\nPrecision Plot - Fast Motion\nour-AFTS[0.933]\nDMCNet[0.800]\nAPFNet[0.791]\nADRNet[0.749]\nHMFT[0.659]\nMEEM+RGBT[0.597]\nSOWP[0.579]\nKCF+RGBT[0.379]\nDSST[0.355]\nCSR[0.349]\nL1-PF[0.320]\nJSR[0.256]\n05 10 15 20 25 30 35 40 45 50\nLocation error threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nMaximum Precision Rate\nPrecision Plot - Heavy Occlusion\nour-AFTS[0.941]\nDMCNet[0.745]\nAPFNet[0.738]\nADRNet[0.714]\nHMFT[0.664]\nSOWP[0.547]\nMEEM+RGBT[0.540]\nDSST[0.410]\nCSR[0.384]\nKCF+RGBT[0.356]\nL1-PF[0.332]\nJSR[0.259]\n05 10 15 20 25 30 35 40 45 50\nLocation error threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nMaximum Precision Rate\nPrecision Plot - Low Illumination\nour-AFTS[0.913]\nDMCNet[0.853]\nAPFNet[0.843]\nHMFT[0.833]\nADRNet[0.811]\nMEEM+RGBT[0.671]\nSOWP[0.524]\nKCF+RGBT[0.518]\nDSST[0.483]\nL1-PF[0.401]\nCSR[0.393]\nJSR[0.381]\nFigure 10.  The figure illustrates the comparison of our model’s accuracy against other state-of-the-art Anchor-\nBased models on 12 challenging attributes in the RGBT234 dataset.\n13\nVol.:(0123456789)Scientific Reports |        (2023) 13:13294  | https://doi.org/10.1038/s41598-023-39978-7\nwww.nature.com/scientificreports/\n05 10 15 20 25 30 35 40 45 50\nLocation error threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nMaximum Precision Rate\nPrecision Plot - Low Resolution\nour-AFTS[0.973]\nDMCNet[0.854]\nAPFNet[0.844]\nADRNet[0.838]\nHMFT[0.763]\nSOWP[0.679]\nMEEM+RGBT[0.608]\nDSST[0.579]\nKCF+RGBT[0.492]\nL1-PF[0.469]\nCSR[0.413]\nJSR[0.399]\n05 10 15 20 25 30 35 40 45 50\nLocation error threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nMaximum Precision Rate\nPrecision Plot - Motion Blur\nour-AFTS[0.931]\nDMCNet[0.773]\nAPFNet[0.745]\nADRNet[0.733]\nHMFT[0.706]\nSOWP[0.598]\nMEEM+RGBT[0.551]\nCSR[0.379]\nDSST[0.358]\nKCF+RGBT[0.323]\nL1-PF[0.286]\nJSR[0.242]\n05 10 15 20 25 30 35 40 45 50\nLocation error threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nMaximum Precision Rate\nPrecision Plot - No Occlusion\nAPFNet[0.948]\nDMCNet[0.923]\nADRNet[0.916]\nHMFT[0.909]\nour-AFTS[0.809]\nSOWP[0.801]\nMEEM+RGBT[0.741]\nDSST[0.697]\nKCF+RGBT[0.571]\nCSR[0.567]\nL1-PF[0.565]\nJSR[0.446]\n05 10 15 20 25 30 35 40 45 50\nLocation error threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nMaximum Precision Rate\nPrecision Plot - Partial Occlusion\nDMCNet[0.895]\nour-AFTS[0.881]\nAPFNet[0.863]\nHMFT[0.857]\nADRNet[0.851]\nMEEM+RGBT[0.683]\nSOWP[0.666]\nDSST[0.565]\nKCF+RGBT[0.526]\nCSR[0.494]\nL1-PF[0.475]\nJSR[0.378]\n05 10 15 20 25 30 35 40 45 50\nLocation error threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nMaximum Precision Rate\nPrecision Plot - Scale Variation\nDMCNet[0.846]\nour-AFTS[0.836]\nAPFNet[0.831]\nHMFT[0.800]\nADRNet[0.786]\nSOWP[0.666]\nMEEM+RGBT[0.616]\nDSST[0.568]\nCSR[0.509]\nL1-PF[0.455]\nKCF+RGBT[0.441]\nJSR[0.355]\n05 10 15 20 25 30 35 40 45 50\nLocation error threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nMaximum Precision Rate\nPrecision Plot - Thermal Crossover\nour-AFTS[0.923]\nDMCNet[0.872]\nAPFNet[0.822]\nADRNet[0.796]\nHMFT[0.722]\nSOWP[0.712]\nMEEM+RGBT[0.612]\nDSST[0.495]\nCSR[0.444]\nKCF+RGBT[0.387]\nL1-PF[0.375]\nJSR[0.338]\nFigure 10.  (continued)\n14\nVol:.(1234567890)Scientific Reports |        (2023) 13:13294  | https://doi.org/10.1038/s41598-023-39978-7\nwww.nature.com/scientificreports/\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\noverlap threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nMaximum Success Rate\nSuccess Plot - Background Clutter\nour-AFTS[0.594]\nDMCNet[0.559]\nAPFNet[0.545]\nADRNet[0.536]\nHMFT[0.498]\nMEEM+RGBT[0.383]\nSOWP[0.336]\nDSST[0.293]\nKCF+RGBT[0.275]\nCSR[0.253]\nL1-PF[0.220]\nJSR[0.212]\n00 .1 0.20 .3 0.40 .5 0.60 .7 0.80 .9 1\noverlap threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nMaximum Success Rate\nSuccess Plot - Camera Moving\nour-AFTS[0.601]\nDMCNet[0.576]\nAPFNet[0.563]\nHMFT[0.562]\nADRNet[0.529]\nSOWP[0.390]\nMEEM+RGBT[0.383]\nCSR[0.301]\nDSST[0.279]\nKCF+RGBT[0.278]\nL1-PF[0.225]\nJSR[0.210]\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\noverlap threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nMaximum Success Rate\nSuccess Plot - Deformation\nHMFT[0.579]\nour-AFTS[0.571]\nDMCNet[0.565]\nAPFNet[0.564]\nADRNet[0.528]\nSOWP[0.420]\nMEEM+RGBT[0.413]\nCSR[0.331]\nDSST[0.325]\nKCF+RGBT[0.296]\nL1-PF[0.244]\nJSR[0.202]\n00 .1 0.20 .3 0.40 .5 0.60 .7 0.80 .9 1\noverlap threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nMaximum Success Rate\nSuccess Plot - Fast Motion\nour-AFTS[0.620]\nDMCNet[0.524]\nAPFNet[0.511]\nADRNet[0.489]\nHMFT[0.469]\nMEEM+RGBT[0.365]\nSOWP[0.335]\nDSST[0.224]\nKCF+RGBT[0.223]\nCSR[0.220]\nL1-PF[0.196]\nJSR[0.157]\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\noverlap threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nMaximum Success Rate\nSuccess Plot - Heavy Occlusion\nour-AFTS[0.616]\nDMCNet[0.521]\nAPFNet[0.507]\nADRNet[0.496]\nHMFT[0.469]\nSOWP[0.354]\nMEEM+RGBT[0.349]\nDSST[0.270]\nCSR[0.268]\nKCF+RGBT[0.239]\nL1-PF[0.222]\nJSR[0.181]\n00 .1 0.20 .3 0.40 .5 0.60 .7 0.80 .9 1\noverlap threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nMaximum Success Rate\nSuccess Plot - Low Illumination\nHMFT[0.591]\nDMCNet[0.587]\nour-AFTS[0.583]\nAPFNet[0.569]\nADRNet[0.560]\nMEEM+RGBT[0.421]\nKCF+RGBT[0.340]\nSOWP[0.336]\nDSST[0.299]\nCSR[0.273]\nJSR[0.264]\nL1-PF[0.260]\nFigure 11.  The figure demonstrates the comparison of our model’s success rate against other state-of-the-art \nAnchor-Based models on 12 challenging attributes in the RGBT234 dataset.\n15\nVol.:(0123456789)Scientific Reports |        (2023) 13:13294  | https://doi.org/10.1038/s41598-023-39978-7\nwww.nature.com/scientificreports/\n00 .1 0.20 .3 0.40 .5 0.60 .7 0.80 .9 1\noverlap threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nMaximum Success Rate\nSuccess Plot - Low Resolution\nour-AFTS[0.620]\nDMCNet[0.579]\nAPFNet[0.565]\nADRNet[0.562]\nHMFT[0.517]\nSOWP[0.421]\nMEEM+RGBT[0.373]\nDSST[0.368]\nKCF+RGBT[0.313]\nL1-PF[0.274]\nCSR[0.259]\nJSR[0.239]\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\noverlap threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nMaximum Success Rate\nSuccess Plot - Motion Blur\nour-AFTS[0.596]\nDMCNet[0.559]\nAPFNet[0.545]\nADRNet[0.532]\nHMFT[0.509]\nSOWP[0.399]\nMEEM+RGBT[0.367]\nCSR[0.270]\nDSST[0.251]\nKCF+RGBT[0.221]\nL1-PF[0.206]\nJSR[0.172]\n00 .1 0.20 .3 0.40 .5 0.60 .7 0.80 .9 1\noverlap threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nMaximum Success Rate\nSuccess Plot - No Occlusion\nAPFNet[0.680]\nHMFT[0.674]\nDMCNet[0.671]\nADRNet[0.660]\nour-AFTS[0.603]\nSOWP[0.502]\nMEEM+RGBT[0.474]\nDSST[0.433]\nCSR[0.415]\nL1-PF[0.379]\nKCF+RGBT[0.371]\nJSR[0.306]\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\noverlap threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nMaximum Success Rate\nSuccess Plot - Partial Occlusion\nDMCNet[0.631]\nHMFT[0.621]\nAPFNet[0.606]\nADRNet[0.603]\nour-AFTS[0.588]\nMEEM+RGBT[0.429]\nSOWP[0.427]\nDSST[0.362]\nCSR[0.349]\nKCF+RGBT[0.344]\nL1-PF[0.314]\nJSR[0.252]\n00 .1 0.20 .3 0.40 .5 0.60 .7 0.80 .9 1\noverlap threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nMaximum Success Rate\nSuccess Plot - Scale Variation\nour-AFTS[0.602]\nDMCNet[0.598]\nHMFT[0.592]\nAPFNet[0.579]\nADRNet[0.562]\nSOWP[0.396]\nMEEM+RGBT[0.376]\nCSR[0.373]\nDSST[0.337]\nL1-PF[0.306]\nKCF+RGBT[0.287]\nJSR[0.238]\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\noverlap threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nMaximum Success Rate\nSuccess Plot - Thermal Crossover\nDMCNet[0.612]\nADRNet[0.586]\nAPFNet[0.581]\nour-AFTS[0.563]\nHMFT[0.504]\nSOWP[0.462]\nMEEM+RGBT[0.408]\nCSR[0.325]\nDSST[0.325]\nKCF+RGBT[0.250]\nL1-PF[0.238]\nJSR[0.209]\nFigure 11.  (continued)\n16\nVol:.(1234567890)Scientific Reports |        (2023) 13:13294  | https://doi.org/10.1038/s41598-023-39978-7\nwww.nature.com/scientificreports/\nFigure 12.  Results of an experimental assessment of the RGBT210.\n17\nVol.:(0123456789)Scientific Reports |        (2023) 13:13294  | https://doi.org/10.1038/s41598-023-39978-7\nwww.nature.com/scientificreports/\nData availability\nThe datasets used and/or analysed during the current study are available from the corresponding author on \nreasonable request.\nReceived: 22 April 2023; Accepted: 2 August 2023\nReferences\n 1. Li, C. et al. Learning collaborative sparse representation forgrayscale-thermal tracking. IEEE Trans. Image Process. 25(12), 5743–\n5756 (2016).\n 2. Li, C., Xiaohao, Wu., Zhao, N., Cao, X. & Tang, J. Fusing two-stream convolutional neural networks for RGB-t object tracking. \nNeurocomputing 281, 78–85 (2018).\n 3. Zhang, X. et al. Corrections to “SiamFT: An RGB-infrared fusion tracking method via fully convolutional siamese networks” . IEEE \nAccess 7, 144799–144799 (2019).\n 4. Guo, C., Y ang, D., Li, C. & Song, P . Dual siamese network for RGBT tracking via fusing predicted position maps. Vis. Comput.  \n38(7), 2555–2567 (2021).\n 5. Zhu, Y ., Li, C., Luo, B., Tang, J. and Wang X. Dense feature aggregation and pruning for RGBT tracking. in Proceedings of the 27th \nACM International Conference on Multimedia. ACM, 2019.\n 6. He, A., Luo, C., Tian, X. and Zeng, W . A twofold Siamese network for real-time object tracking. in 2018 IEEE/CVF Conference \non Computer Vision and Pattern Recognition. IEEE, 2018.\n 7. Li, B., Wu, W ., Wang, Q., Zhang, F ., Xing, J. and Y an, J. SiamRPN: Evolution of Siamese visual tracking with very deep networks. \nin 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2019.\n 8. Li, B., Y an, J., Wu, W ., Zhu, Z., and Hu, X. High performance visual tracking with siamese region proposal network. in 2018 IEEE/\nCVF Conference on Computer Vision and Pattern Recognition. IEEE, 2018.\n05 10 15 20 25\nLocation error threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nPrecision\nPrecision Plot\nours-AFTS[0.849]\nours-GTOT-(CA-SA)-no(TS)[0.78]\nours-GTOT-SA[0.752]\nours-GTOT-no(CA-SA-TS)[0.717]\nours-GTOT-CA[0.704]\n00 .1 0.20 .3 0.40 .5 0.60 .7 0.80 .9 1\noverlap threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nSuccess Rate\nSuccess Plot\nours-AFTS[0.777]\nours-GTOT-(CA-SA)-no(TS)[0.74]\nours-GTOT-SA[0.715]\nours-GTOT-no(CA-SA-TS)[0.7]\nours-GTOT-CA[0.694]\nFigure 13.  Results of Ablation experiment. ours-GTOT–SA represents the experimental results with only the \nSA module adds ours-GTOT–CA represents the experimental results with only the CA module added. ours-\nGTOT–no (CA–SA–TS) represents the experimental results without adding any module; ours-GTOT–(CA–\nSA)-no (TS) represents the experimental results with only the SA and CA module added. ours-AFTS represents \nthe model with all modules included.\n18\nVol:.(1234567890)Scientific Reports |        (2023) 13:13294  | https://doi.org/10.1038/s41598-023-39978-7\nwww.nature.com/scientificreports/\n 9. Valmadre, J., Bertinetto, L., Henriques, J., Vedaldi, A. and Philip, H. S. Torr. End-to-end representation learning for correlationfilter \nbased tracking. in 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2017.\n 10. Zhu, Z., Wang, Q., Li, B., Wu, W ., Y an, J. and Hu, W . Distractor-aware siamese networks for visual object tracking. in Computer \nVision – ECCV 2018, pages 103–119. Springer Inter-national Publishing, 2018.\n 11. Shen, L. et al. RGBT tracking based on cooperative low-rank graph model. Neurocomputing 492, 370–381 (2022).\n 12. Zhang, X., Y e, P ., Peng, S., Liu, J. & Xiao, G. DSiamMFT: An RGB-t fusion tracking method via dynamic siamese networks using \nmulti-layer feature fusion. Signal Proc. Image Commun. 84, 115756 (2020).\n 13. Zhu, Y ., Li, C., Tang, J. & Luo, B. Quality-aware feature aggregation network for robust RGBT tracking. IEEE Trans. Intell. Veh. \n6(1), 121–130 (2021).\n 14. Feng, M., Song, K., Wang, Y ., Liu, J. & Y an, Y .-H. Learning discriminative update adaptive spatial-temporal regularized correlation \nfilter for RGB-t tracking. J. Vis. Commun. Image Represent. 72, 102881 (2020).\n 15. Ma, C., Huang, J.-B., Y ang, X. & Y ang, M.-H. Robust visual tracking via hierarchical convolutional features. IEEE Trans. Pattern \nAnal. Mach. Intell. 41(11), 2709–2723 (2019).\n 16. X. Chen, B. Y an, J. Zhu, D. Wang, X. Y ang, H. Lu, Transformer tracking, in: Proceedings of the IEEE/CVF Conference on Computer \nVision and Pattern Recognition, 2021, pp. 8126–8135.\n 17. Chenglong Li, Nan Zhao, Yijuan Lu, Chengli Zhu, and Jin Tang. Weighted sparse representation regularized graph learning for \nRGBT object tracking. In Proceedings of the 25th ACM international conference on Multimedia. ACM, 2017.\n 18. Han-Ul Kim, Dae-Y oun Lee, Jae-Y oung Sim, and Chang-Su Kim. SOWP: Spatially ordered and weighted patch descriptor for \nvisual tracking. in 2015 IEEE International Conference on Computer Vision(ICCV). IEEE, 2015.\n 19. Henriques, J. F ., Caseiro, R., Martins, P . & Batista, J. High-speed tracking with Kernelized correlation filters. IEEE Trans. Pattern \nAnal. Mach. Intell. 37(3), 583–596 (2015).\n 20. Li, C., Sun, X., Wang, X., Zhang, L. & Tang, J. Grayscale-thermal object tracking via multitask Laplacian sparse rep-resentation. \nIEEE Trans. Syst. Man Cybern. Syst. 47(4), 673–681 (2017).\n 21. Zhang, J., Ma, S. and Sclaroff, S. MEEM: Robust tracking via multiple experts using entropy minimization. In Com-puter Vision \n– ECCV 2014, pages 188–203. Springer International Publishing, 2014.\n 22. P . Zhang, J. Zhao, D. Wang, H. Lu, and X. Ruan, “VisibleThermal UA V Tracking: A large-scale benchmark and new baseline, ” in \nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 8886– 8895.\n 23. C. Wang, C. Xu, Z. Cui, L. Zhou, T. Zhang, X. Zhang, and J. Y ang, “Cross-modal pattern-propagation for RGB-T tracking, ” in \nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 7064–7073\n 24. Xiao, Y ., Y ang, M., Li, C., Liu, L. and Tang, J. “ Attributebased progressive fusion network for RGBT tracking, ” in Proceedings of \nthe AAAI Conference on Artificial Intelligence, 2022.\n 25. Zhang, H., Zhang, L., Zhuo, L. & Zhang, J. Object tracking in RGB-T videos using modal-aware attention network and competitive \nlearning. Sensors 20(2), 393 (2020).\n 26. Zhang, P . et al. Jointly modeling motion and appearance cues for robust RGBT tracking. IEEE Trans. Image Process. 30, 3335–3347 \n(2021).\n 27. Lu, A., Qian, C., Li, C., Tang, J. & Wang, L. Dualitygated mutual condition network for RGBT tracking. IEEE Trans. Neural Netw. \nLearn. Syst. https:// doi. org/ 10. 1109/ TNNLS. 2022. 31575 94 (2022).\n 28. Li, C. et al. Learning collaborative sparse representation for grayscale-thermal tracking. IEEE Trans. Image Process. 25(12), 5743–\n5756 (2016).\n 29. Li, C., Wu, X., Zhao, N., Cao, X. & Tang, J. Fusing two-stream convolutional neural networks for RGB-T object tracking. Neuro‑\ncomputing 281, 78–85 (2018).\n 30. Kang, B., Liang, D., Ding, W ., Zhou, H. & Zhu, W .-P . Grayscale-thermal tracking via inverse sparse representationbased collabora-\ntive encoding. IEEE Trans. Image Process. 29, 3401–3415 (2019).\n 31. Y ang, R., Zhu, Y ., Wang, X., Li, C. and Tang, J. “Learning target-oriented dual attention for robust RGB-T tracking, ” in 2019 IEEE \nInternational Conference on Image Processing (ICIP). IEEE, 2019, pp. 3975–3979.\n 32. Li, C., Xiang, Z., Tang, J., Luo, B. & Wang, F . RGBT tracking via noise-robust cross-modal ranking. IEEE Transactions on Neural \nNetworks and Learning Systems 33(5019), 5031 (2021).\n 33. Xu, Q., Mei, Y ., Liu, J. & Li, C. Multimodal cross-layer bilinear pooling for RGBT tracking. IEEE Trans. Multimedia 24, 567–580 \n(2021).\n 34. Li, C., Lu, A., Hua Zheng, A., Tu, Z. and Tang, J. “Multi-adapter RGBT tracking, ” in Proceedings of the IEEE/CVF International \nConference on Computer Vision Workshops, 2019, pp. 0–0.\n 35. Y ang, R., Wang, X., Li, C., Hu, J. & Tang, J. RGBT tracking via cross-modality message passing. Neurocomputing 462, 365–375 \n(2021).\n 36. Wang, X., Shu, X., Zhang, S., Jiang, B., Wang, Y ., Tian, Y . and Wu, F . “MFGNet: Dynamic modality-aware filter generation for \nRGB-T tracking, ” IEEE Trans. Multimedia, 2022.\n 37. Zhu, Y ., Li, C., Luo, B., Tang, J. and Wang, X. “Dense feature aggregation and pruning for RGBT tracking, ” in Proceedings of the \n27th ACM International Conference on Multimedia, 2019, pp. 465–472.\n 38. L. Zhang, M. Danelljan, A. Gonzalez-Garcia, J. van de Weijer, and F . Shahbaz Khan, “Multi-modal fusion for end-to-end RGBT \ntracking, ” in Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops, 2019, pp. 0–0.\n 39. Guo, C., Y ang, D., Li, C. & Song, P . Dual Siamese network for RGBT tracking via fusing predicted position maps. Vis. Comput. \n38(7), 2555–2567 (2022).\n 40. Zhai, S., Shao, P ., Liang, X. & Wang, X. Fast RGB-T tracking via cross-modal correlation filters. Neurocomputing  334, 172–181 \n(2019).\n 41. Li, C., Zhu, C., Huang, Y ., Tang, J. and Wang, L. “Cross-modal ranking with soft consistency and noisy labels for robust RGBT \ntracking, ” in Proceedings of the European Conference on Computer Vision (ECCV), 2018, pp. 808–823.\n 42. Li, C., Zhao, N., Lu, Y ., Zhu, C. and Tang, J. “Weighted sparse representation regularized graph learning for RGB-T object tracking, ” \nin Proceedings of the 25th ACM international conference on Multimedia, 2017, pp. 1856–1864.\n 43. Li, C., Zhu, C., Zheng, S., Luo, B. & Tang, J. Two-stage modality-graphs regularized manifold ranking for RGB-T tracking. Signal \nProcess. Image Commun. 68, 207–217 (2018).\n 44. Shen, L. et al. RGBT tracking based on cooperative low-rank graph model. Neurocomputing 492, 370–381 (2022).\n 45. Zhang, J., Ma, S. and Sclaroff, S. “MEEM: robust tracking via multiple experts using entropy minimization, ” In European confer-\nence on computer vision. Springer, 2014, pp. 188–203.\n 46. Henriques, J. F ., Caseiro, R., Martins, P . & Batista, J. High-speed tracking with kernelized correlation filters. IEEE Trans. Pattern \nAnal. Mach. Intell. 37(3), 583–596 (2014).\n 47. Zhang, Z. and Peng, H. “Deeper and wider siamese networks for real-time visual tracking, ” In Proceedings of the IEEE/CVF \nConference on Computer Vision and Pattern Recognition, 2019, pp. 4593–4610.\n 48. Li, C., Liang, X., Lu, Y ., Zhao, N. & Tang, J. RGB-T object tracking: Benchmark and baseline. Pattern Recogn. 96, 106977 (2019).\n19\nVol.:(0123456789)Scientific Reports |        (2023) 13:13294  | https://doi.org/10.1038/s41598-023-39978-7\nwww.nature.com/scientificreports/\nAuthor contributions\nL.F .: Conceptualization of this study, Methodology, Software. P .K.: As the corresponding author, P .K. is primarily \nresponsible for designing the research plan, planning the experiments, coordinating and communicating with \nthe collaborators, and other related tasks.\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to P .K.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2023",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7870250940322876
    },
    {
      "name": "BitTorrent tracker",
      "score": 0.7795464396476746
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5638667941093445
    },
    {
      "name": "Tracking (education)",
      "score": 0.5337898135185242
    },
    {
      "name": "Minimum bounding box",
      "score": 0.5064247846603394
    },
    {
      "name": "RGB color model",
      "score": 0.47736549377441406
    },
    {
      "name": "Computer vision",
      "score": 0.44406089186668396
    },
    {
      "name": "Transformer",
      "score": 0.4166647791862488
    },
    {
      "name": "Eye tracking",
      "score": 0.2743261456489563
    },
    {
      "name": "Engineering",
      "score": 0.08716252446174622
    },
    {
      "name": "Image (mathematics)",
      "score": 0.07546880841255188
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Psychology",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Pedagogy",
      "score": 0.0
    }
  ],
  "cited_by": 9
}