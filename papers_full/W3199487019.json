{
  "title": "TAG: Gradient Attack on Transformer-based Language Models",
  "url": "https://openalex.org/W3199487019",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4221543135",
      "name": "Deng, Jieren",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3211006322",
      "name": "Wang Yijue",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1978330610",
      "name": "Li Ji",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1974402287",
      "name": "Shang, Chao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2076313181",
      "name": "Liu Hang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2460481615",
      "name": "Rajasekaran, Sanguthevar",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2744413473",
      "name": "Ding, Caiwen",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2978670439",
    "https://openalex.org/W2287011250",
    "https://openalex.org/W3139039641",
    "https://openalex.org/W2099471712",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2127941149",
    "https://openalex.org/W2787560479",
    "https://openalex.org/W2970408908",
    "https://openalex.org/W2051267297",
    "https://openalex.org/W2525127255",
    "https://openalex.org/W3113747735",
    "https://openalex.org/W2083842231",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3188079459",
    "https://openalex.org/W3083312158",
    "https://openalex.org/W2963456518",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W3210120707",
    "https://openalex.org/W3092633571",
    "https://openalex.org/W2970204870",
    "https://openalex.org/W2799054028",
    "https://openalex.org/W2168231600",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W3105816068",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2963403868"
  ],
  "abstract": "Although federated learning has increasingly gained attention in terms of effectively utilizing local devices for data privacy enhancement, recent studies show that publicly shared gradients in the training process can reveal the private training images (gradient leakage) to a third-party in computer vision. We have, however, no systematic understanding of the gradient leakage mechanism on the Transformer based language models. In this paper, as the first attempt, we formulate the gradient attack problem on the Transformer-based language models and propose a gradient attack algorithm, TAG, to reconstruct the local training data. We develop a set of metrics to evaluate the effectiveness of the proposed attack algorithm quantitatively. Experimental results on Transformer, TinyBERT$_{4}$, TinyBERT$_{6}$, BERT$_{BASE}$, and BERT$_{LARGE}$ using GLUE benchmark show that TAG works well on more weight distributions in reconstructing training data and achieves 1.5$\\times$ recover rate and 2.5$\\times$ ROUGE-2 over prior methods without the need of ground truth label. TAG can obtain up to 90$\\%$ data by attacking gradients in CoLA dataset. In addition, TAG has a stronger adversary on large models, small dictionary size, and small input length. We hope the proposed TAG will shed some light on the privacy leakage problem in Transformer-based NLP models.",
  "full_text": "TAG: Gradient Attack on Transformer-based Language Models\nJieren Deng1§,Yijue Wang1§, Ji Li2, Chenghong Wang3, Chao Shang4, Hang Liu5\nSanguthevar Rajasekaran1, Caiwen Ding1\n1University of Connecticut, 2Microsoft, 3Duke University, 4JD AI Research,\n5Stevens Institute of Technology\n{jieren.deng,yijue.wang,sanguthevar.rajasekaran,caiwen.ding}@uconn.edu\n{changzhouliji,chaoshangcs}@gmail.com, chenghong.wang552@duke.edu, hliu77@stevens.edu\nAbstract\nAlthough distributed learning has increasingly\ngained attention in terms of effectively uti-\nlizing local devices for data privacy enhance-\nment, recent studies show that publicly shared\ngradients in the training process can reveal\nthe private training data (gradient leakage) to\na third party. However, so far there hasn’t\nbeen any systematic study of the gradient leak-\nage mechanism of the Transformer based lan-\nguage models. In this paper, as the ﬁrst at-\ntempt, we formulate the gradient attack prob-\nlem on the Transformer-based language mod-\nels and propose a gradient attack algorithm,\nTAG, to recover the local training data. Exper-\nimental results on Transformer, TinyBERT 4,\nTinyBERT6, BERT BASE , and BERT LARGE\nusing GLUE benchmark show that compared\nwith DLG (Zhu et al., 2019), TAG works\nwell on more weight distributions in recover-\ning private training data and achieves 1.5 ×\nRecover Rate and 2.5 ×ROUGE-2 over prior\nmethods without the need of ground truth la-\nbel. TAG can obtain up to 88.9 % tokens and\nup to 0.93 cosine similarity in token embed-\ndings from private training data by attacking\ngradients on CoLA dataset. In addition, TAG\nis stronger than previous approaches on larger\nmodels, smaller dictionary size, and smaller in-\nput length.\n1 Introduction\nDistributed training has gained popularity in re-\nducing training time on large-scale deep learning\nmodels and datasets (Dean et al., 2012; Li et al.,\n2014a,b; Baruch et al., 2019; Gurevin et al., 2021;\nWu et al., 2020; Lin et al., 2020). In such sys-\ntems, multiple devices or participators collabo-\nrate in training one task and synchronize via ex-\nchanging gradients, allowing participants at dif-\nferent location for model training with their own\ndata. It is widely believed that sharing gradients\n§Equal contribution\nbetween participants will not leak the private train-\ning data. On the other hand, large scale contex-\ntual representation models, such as ELMo (Peters\net al., 2018), BERT (Devlin et al., 2019), XLNet\n(Yang et al., 2019), T5 (Raffel et al., 2019a), and\nGPT-3 (Brown et al., 2020) have signiﬁcantly pro-\nmoted natural language processing (NLP) in the\nlast decade. Thus, the ability of parallel training\nhas helped propel using distributed learning on a\nlarge scale NLP, for efﬁcient training.\nRecent studies show that private training data\ncan be recovered through the deep learning model\nby gradients (Zhu et al., 2019; Chen et al., 2020;\nGeiping et al., 2020; Wang et al., 2021). For in-\nstance, recent work \"Deep Leakage from Gradient”\n(DLG) (Zhu et al., 2019) showed the shared gra-\ndients could leak private training data in image\nclassiﬁcation. Another recent work IG (Geiping\net al., 2020) shows that it is possible to recover\nimages by gradients in a trained network.\nDespite the success, there are several limitations\non current works: (i) Lack of generalizability on\ndifferent weight distribution. They only succeeded\nin the early training phase at certain weight distribu-\ntion. (ii) Lack of formal problem formulation and\ngradient attack evaluation in the ﬁeld of NLP. Exist-\ning work only show the reconstruction difference\nat the sentence level without quantitative analysis.\n(iii) There is little investigation on the impact of\ndifferent attention heads, model architectures on\nTransformer gradient attack.\nIn this paper, we propose a novel algorithm, to\nrecover private training data of Transformer-based\nlanguage model from the shared gradients. As\nshown in Figure 1, our TAG adversary obtains the\ntransformer model gradients∇W from a distributed\nlearning participator and updates initialized dummy\ndata (X′, Y′) by comparing the difference between\nthe participator’s gradients ∇W and adversary’s\ngradients ∇W′. Eventually, the adversary recovers\nthe dummy data (X′, Y′) and acquires the informa-\narXiv:2103.06819v6  [cs.CR]  21 Sep 2021\nTransformer Model \nGradient\nTAG(      ,      )\nGradient-sharing Distributed Learning\nParticipator 1\nParticipator 2\nParticipator 3\nVictim \n（Participator n）\n∇W\n∇W ∇W’ \nUpdate(X’,Y’) \nSteal!\nTAG Adversary\nFigure 1: Gradient transformer attack process.\ntion from the participator’s private training dataX\nsuch as name, age, and gender. Our contributions\nare summarized as follows:\n• As the ﬁrst attempt in the ﬁeld of NLP, we pro-\npose a general gradient attack algorithm,TAG, to\nrecover the private training data on Transformer-\nbased language models. Compared to the ex-\nisting methods, TAG works on more realistic\nweight distributions, including both pre-trained\nmodels and normally initialized models.\n• We develop a quantitative evaluation method on\nthe NLP gradient attack problem while the ex-\nisting work shows the recovered texts. We use\na set of metrics (Recover Rate, ROUGE-1(%),\nROUGE-2(%), ROUGE-L(%), and runtime) to\nevaluate the effectiveness of the proposed attack\nalgorithm. With these metrics, TAG achieves\n1.5×Recover Rate and 2.5 ×ROUGE-2 over\nprior methods. TAG can also obtain up to 88.9%\ntokens and up to 0.93 consine similarity in token\nembeddings from private training data.\n• We conduct a comprehensive analysis of dif-\nferent weight distribution, dataset, vocabulary\ndictionary size, and model size on Trans-\nformer, TinyBERT4, TinyBERT6, BERTBASE ,\nand BERTLARGE, and we observe that TAG has\na stronger adversary on large models than on\nsmall ones. In addition, models with a smaller\ndictionary size and smaller input sequence length\nare riskier in leaking the private training data.\n2 Related Work\n2.1 Privacy leakage problem\nPrivacy leakage is studied in the training phase\nand prediction phase. Privacy attack from gradient\nand model inversion (MI) attack (Fredrikson et al.,\n2015) aim at the training phase by constructing the\nfeatures of the private training data by using the\ncorrelation between the private training data and\nthe model output. The authors in (Fredrikson et al.,\n2015) showed that it is possible to infer individ-\nual genomic data via access to a linear model for\npersonalized medicine. Recent works extend MI\nattack to recover features of private training data\nof Deep Neural Networks (DNNs). Privacy attack\nfrom gradients is different from previous MI attack.\nIt recovers the private training data exploiting their\ngradients in a machine learning model. The pro-\ncess of privacy leakage from gradients is shown at\nFigure 1.\n2.2 Distributed learning\nDistributed learning is a popular framework for\nlarge-scale model training (Das et al., 2016; Dean\net al., 2012; Li et al., 2014a,b; Baruch et al., 2019)\nthat leverage the computation power of many de-\nvices by aggregating the local models trained on\nthe devices. Instead of training a model with all the\ndata at a server, each device trains a local model\nwith a different chunk of the dataset and shares the\nﬁnal gradients. A popular distributed learning algo-\nrithm is Synchronous Stochastic Gradient Descent\n(sync-SGD) (Li et al., 2014a,b) which contains a\nsingle server and n local devices. Each device\ntrains a local model and shares the gradient with\nthe server. The server then aggregates the gradients\nof the different devices and starts another round by\nsharing the aggregated result with the devices.\n2.3 Prior arts on gradients-based attack\nAlthough a distributed learning system protects\nprivacy by not sharing private training data, re-\nsearch works have shown that it is possible to infer\nthe information of private training data from the\nshared gradients in both language tasks and com-\nputer vision tasks. (Melis et al., 2019) enables\nthe identiﬁcation of words used in the training to-\nkens by analyzing the gradients of the embedding\nlayer. (Goodfellow et al., 2014) proposes an at-\ntack algorithm to synthesize images mimicking\nthe real training images by Generative Adversary\nNetwork (GAN) models. Besides the works that\nrecover certain properties of the private training\ndata, DLG (Zhu et al., 2019) is a more recent work\nthat shows that it is possible to recover private train-\ning data with pixel-wise accuracy for images and\ntoken-wise matching for texts by gradient matching.\nDLG (Zhu et al., 2019) achieves the recovery of\nimages from different datasets on LeNet-5. How-\never, DLG (Zhu et al., 2019) has limitations on\nevaluating the performance thoroughly on differ-\nent weight distribution settings, various networks,\nand different training stages (pre-trained versus ini-\ntialized). To the best of our knowledge, there is\nno existing work that comprehensively investigates\ngradient-based attacks for transformer-based lan-\nguage models with benchmark dataset and standard\nmetric.\n3 Approach\nIn this section, we ﬁrst formulate the gradient attack\nin NLP, and the proposed algorithm is introduced\nafterward.\n3.1 Transformer-based NLP models\nTransformer (Vaswani et al., 2017) is the funda-\nmental architecture for many popular pre-trained\nlanguage models, e.g., BERT (Devlin et al., 2019).\nScaled dot-product self-attention is the underlying\nkey mechanism inside Transformer, which is calcu-\nlated as\nsdpsAttention(q,k,v ) =v·softmax( q·kT\n√dk\n) (1)\nwhere q, k, and v represent the query, key, and\nvalue, respectively, and 1/√dk is a scaling factor.\nMulti-head attention is applied to ﬁrst calculate at-\ntention using Eq. 1 in the subspace of embeddings\nand then concatenate to form the ﬁnal output.\nA typical ﬂow is to ﬁrst pre-train the Transformer\nwith objectives like masked language modeling\non huge amounts of unlabeled data to get a pre-\ntrained model like BERT (Devlin et al., 2019) and\nRoBERTa (Liu et al., 2019), and then ﬁnetune the\npre-trained model on speciﬁc downstream tasks\nusing the labeled data.\nIn spite of the high accuracy achieved by the\nTransformer based models, the large size and high\nlatency of such models make them less appealing\nto resource constrained edge devices. Accordingly,\nvarious knowledge distillation and model compres-\nsion techniques have been proposed to effectively\ncut down the model size and inference latency with\nminimum impact on the accuracy.\nWithout any loss of generality, we consider the\nTransformer (Vaswani et al., 2017), BERT (De-\nvlin et al., 2019), and TinyBERT (Jiao et al.,\n2020) as the representatives of encoder-decoder\nTransformers, decoder only pre-trained large Trans-\nformers, and compressed pre-trained Transform-\ners. Our approach can be extended to other sim-\nilar models, such as RoBERTa (Liu et al., 2019),\nUniLMv2 (Bao et al., 2020), and DistilBERT (Sanh\net al., 2019).\n3.2 NLP gradient attack formulation\nWe assume that an adversary cannot access the\nprivate training data (X,Y) in local training di-\nrectly, but the adversary can gain the gradients that\nthe local devices shared, and the current global\nmodel F(X,W) in any timestamps during train-\ning, where X is input tokens and Y is the output\nlabels and W is the model weights.\nThe objective of the attack is to recover the valu-\nable and private training data using the shared gra-\ndients. For computer vision models, the objective\nis to retrieve the original pixels in the training im-\nages. As mentioned in Section 2, most prior works\nfall into this category. In this work, we focus on\nmodern Transformer-based models for NLP ap-\nplications, and our goal is to recover the original\ntokens in the training set.\nAttacking NLP applications is more challenging\nthan computer vision applications, and the reasons\nare threefold. First, the range of possible values\nat each pixel is usually smaller than the range of\npossible tokens at each position, and it is generally\nmore difﬁcult to ﬁnd the exact item from a larger\ncandidate space. Second, the valuable information\ncarried in an image can be retrieved from a region\nof pixels, whereas for NLP data, the sensitive infor-\nmation could be carried by several speciﬁc tokens,\ne.g., name, time, and location, and it is required\nto achieve an exact match on the tokens at certain\npositions to get the key information from the orig-\ninal text. Third, humans can tolerate more errors\nat pixel values in an image, whereas a small error\nin the retrieved token id leads to irrelevant token\nstrings in most cases.\nWithout any loss of generality, the attack can\nhappen at any training stage of the shared global\nmodel, and we consider the two most common\nweight initialization methods, including random\ninitialization for non-pre-trained models and spe-\nciﬁc learned values for pre-trained models. More\nformally, the problem is formulated as:\nConstructing (X∗,Y∗)\ns.t.∂L(W,X∗; Y∗)\n∂W = ∂L(W,X; Y)\n∂W\n(2)\nwhere (X∗,Y∗) are the recovered data, i.e., tokens\nand labels for language tasks.\n3.3 Proposed algorithm\n3.3.1 Recovered token initialization\nTo recover the language data, we ﬁrst randomly ini-\ntialize a dummy data (X′,Y′), where X′is called\ndummy input and Y′is called dummy label. Then\nwe get the corresponding dummy gradient as:\n∇W′= ∂ℓ(F(W,X′); Y′)\n∂W (3)\nThe next step is to optimize dummy gradient,\n∇W′, to ground truth gradient ∇W, as closer as\npossible. In this case, we need to deﬁne a differ-\nentiable distance function D(W,W′), so that we\ncan obtain the best X′and Y′as (X∗,Y∗) follows:\n(X∗,Y∗) =arg min\n(X′,Y′)\nD(∇W′,∇W) (4)\n3.3.2 Distance function for gradient matching\nOur experimental observation shows that with dif-\nferent weight initialization, for the same private\ntraining data, X, the NLP model may have dis-\ntinctly different gradients, ∇W. For example, with\ninitialized weights from a normal distribution, the\ngradient of the model may be larger in magnitude\nthan with initialized weights from a uniform dis-\ntribution (two distributions have the similar inter-\nvals). Besides, the ∇W gathers near-zero values\nmore heavily with weights from normal distribu-\ntion than with weights from uniform distribution.\nWe have a similar observation that the∇Wgathers\nconsiderable near zero values with weights from\na pre-trained model. We consider a matrix with\nsubstantial near-zero values as a sparse matrix.\nIf we use the Euclidean distance (which is used\nin DLG (Zhu et al., 2019)) to measure the differ-\nence between ∇W′and ∇W, the recovery of the\nground truth data is driven by large gradients at\nthe early stages. As a result, this might cause a\nproblem when using Euclidean distance under a\nnormal weight initialization since most of the gra-\ndients gather around zero while a small proportion\nof gradients have large values.\nTo overcome this problem, instead of using the\nEuclidean distance for ∇W′and ∇W as the dis-\ntance function, we consider a combined distance of\nL2 norm (Euclidean distance) and L1 norm (Man-\nhattan distance), and a coefﬁcient parameter, α,\nas our distance function to measure the difference\nbetween ∇W′and ∇W:\nD(∇W′,∇W)\n= ||∇W′−∇W||2 + α(∇W)||∇W′−∇W||\n(5)\nwhere α(∇W) is a factor speciﬁed for each layer’s\n∇W and its value decreases along with the order\nof the layer. By doing this, we put larger weights\non the gradient differences on the front layers as\nthey are closer to the input private training data.\nThe value of α(∇W) is crucial and needs to be\nsuitable for different weight settings.\n3.4 The framework of the algorithm\nOur complete proposed algorithm is shown in Al-\ngorithm 1, and the highlights of our algorithm are\nas follows. We initialize dummy input (dummy\ntoken embeddings) and dummy label, (X′,Y′), as\ndummy data in line 2. During the iterations, started\nfrom line 3, we ﬁrst obtain the dummy gradient,\n∇W′, of the current dummy input. Then we use\nthe distance function in Eq. 5 to measure the dif-\nference, D(∇W,∇W′\ni), between dummy gradi-\nent ∇W′and ground truth gradient ∇W. At the\nend of each iteration, we update the (X′,Y′) by\nthe calculated difference, D(∇W,∇W′\ni) in line\n7 and line 8. When a pre-set maximum number\nof iterations is reached, or in 200 iterations, or the\nnumber of recovered tokens in ground truth does\nnot change, the algorithm will eventually output the\noptimized (X′,Y′) as (X∗,Y∗) after the iterative\nrecovery process.\n4 Experimental setup\nAll of our experiments are conducted on servers\nwith Intel(R) Xeon(R) Gold 5218 (64 virtual CPUs\nAlgorithm 1TAG\n1: Input: ∇W: ground truth gradient; F(X,W′): NLP\nmodel; η: learning rate; W′: parameter weights\n2: Initial: X′∼N(0, 1), Y′∼N(0, 1)\n3: for the i-th iteration do\n4: ∇W′\ni ←∂ℓ(F(X′,W′)/∂W′) //get dummy gradi-\nent by TAG\n5: D(∇W,∇W′\ni) ← ∥∇ W′\ni − ∇W∥2 +\nα(∇W)∥∇W′\ni −∇W∥\n6: update (X′,Y′):\n7: X′←X′−η\n∂D(∇W,∇W′\ni)\n∂∇X′ ,\n8: Y′←Y′−η\n∂D(∇W,∇W′\ni)\n∂∇Y′\n9: end for\n10: Output: Recovered Data X∗,Y∗\nModels Layers Hidden\nUnits\nAttention\nHeads\nFilter\nSize\nTransformer 2 100 4 200\nTinyBERT4 4 312 6 1,200\nTinyBERT6 6 768 12 3,072\nBERTBASE 12 768 12 3,072\nBERTLARGE 24 1,024 16 4,096\nTable 1: Model structures of Transformer, TinyBERT4,\nTinyBERT6, BERTBASE , BERTLARGE.\nwith 504 GB memory) and 8 NVIDIA Quadro RTX\n6000 GPUs (24GB memory) by PyTorch 1.5.1,\nPython 3.6, and CUDA 10.2.\n4.1 Datasets\nWe evaluate our algorithm on the following tasks\nfrom the General Language Understanding Evalua-\ntion (GLUE) (Wang et al., 2019) benchmark.\nCoLA. The Corpus of Linguistic Acceptabil-\nity (Warstadt et al., 2019) consists of English ac-\nceptability judgments drawn from book and journal\narticles on linguistic theory. Each example is a se-\nquence of words annotated with whether it is a\ngrammatical English sentence.\nSST-2. The Stanford Sentiment Treebank (Socher\net al., 2013) consists of sentences from movie re-\nviews and human annotations of their sentiment.\nThe task is to predict the sentiment of a given sen-\ntence. We use the two-way (positive/negative, 1/0)\nclass split and use only sentence-level labels.\nRTE. The Recognizing Textual Entailment\n(RTE) (Dagan et al., 2005) datasets come from a\nseries of annual textual entailment challenges. This\ndataset is constructed based on news and Wikipedia\ntext with a combination of RTE1-3, and RTE5.\nWe select these three datasets because they con-\ntain sentences of different lengths. Typically, sen-\ntences are about 5 to 15 words for CoLA, 10 to\n30 words for SST-2, and 50 to 100 words for RTE.\nIn fact, our algorithm is data agnostic, which can\nwork on any text inputs from any benchmark, or\neven any sentence from any source.\n4.2 Model settings\nWe conduct experiments using three popular\ntransformer-based networks, including the basic\ntransformer model (Vaswani et al., 2017), Tiny-\nBERT (Jiao et al., 2020) and BERT (Devlin et al.,\n2019). The basic transformer contains two trans-\nformer encoders and one transformer decoder. The\nnumber of heads in the self-attention layers is four,\nand the dimension of the feed-forward network\nmodel is 200. The activation function is Gaussian\nError Linear Units (GELU) (Hendrycks and Gim-\npel, 2016). We also applied our algorithm to two\ndifferent sizes TinyBERT and two different sizes\nBERT. The TinyBERT4 is with four layers, 312\nhidden units, feed-forward ﬁlter size of 1200 and\n6 attention heads. The TinyBERT 6 is with 6 lay-\ners, 768 hidden units, feed-forward ﬁlter size of\n3072 and 12 attention heads. In addition, we use\nthe conﬁgurations from (Devlin et al., 2019) for\nBERT. The BERTBASE has 12 layers, 768 hidden\nunits, 3072 feed-forward ﬁlter size, and 12 atten-\ntion heads. The BERTLARGE has 24 layers, 1024\nhidden units, 4096 feed-forward ﬁlter size and, 16\nattention heads. Table 1 summarizes the model\nstructures explored in this work.\n4.3 Experiment parameters settings\nFor each task and dataset of interest, we use the\nsame set of hyperparameters: BertAdam optimizer\n(Devlin et al., 2019) with learning rate 0.05. For\nevery single sentence recovering, we set the max\niteration as 1,000 for our algorithm.\n4.4 Experiment evaluation\nRecover Rate. This metric is deﬁned as the max-\nimum percentage of tokens in ground truth recov-\nered by TAG.\nROUGE. Recall-Oriented Understudy for Gisting\nEvaluation (Lin, 2004), is a set of metrics used for\nevaluating automatic summarization and machine\ntranslation in natural language processing. We use\nROUGE-1, ROUGE-2, and ROUGE-L to evaluate\nthe similarity between the sentence generated from\ngradient attacks and the original sentences. More\nspeciﬁcally speaking, ROUGE-1 and ROUGE-2 re-\nfer to the overlap of unigrams and bigrams between\nthe recovered text and reference, respectively, and\nFigure 2: The average Loss vs Iteration curve of models TinyBERT4, TinyBERT6, BERTBASE , BERTLARGE on\ndata CoLA, SST-2 and RTE. The loss decreases at the ﬁrst 200 iterations and becomes stable after 200 iterations.\nROUGE-L measures the longest matching subse-\nquence of tokens.\nRuntime. This metric is the average of elapsed\nsystem time to complete the attack.\n5 Result Analysis and Visualization\nIn this section, we conduct carefully designed ex-\nperiments to evaluate the proposedTAG on various\ndatasets mentioned in Section 4.1 using the metrics\ndeﬁned in Section 4.4. We have four highlighted\nresults for our evaluation.\nOur algorithm shows stable and distinct conver-\ngence for NLP models. Here, we measure the\ndistance between the dummy gradient, ∇W′, and\nthe ground truth gradient, ∇W, via the aforemen-\ntioned distance function. We deﬁne this distance\nas the loss of the algorithm. We normalize the loss\nof all selected data at log scale between 0.8 to 1.2\nas shown in Fig. 2. The loss is continuously de-\ncreasing for different models and we can observe a\nstable and distinct convergence from the loss curve,\nespecially for the ﬁrst 200 iterations.\nThe TAG attacking process can be visualized in\ntoken embeddings level (Fig. 3) and in sentence\nlevel (Fig. 4). In tokens embeddings level (Fig. 3),\nwe ﬁrst reduce the dimension of token embeddings\nfor both dummy input and ground truth by Princi-\npal Component Analysis (PCA). We use the cosine\nsimilarity (Li et al., 2020) to evaluate similarity\nof the dimension reduced token embeddings. In\nFig. 3 (a), the cosine similarity of the token em-\nbeddings between dummy data and ground truth\nis 0.42 at the 5-th iteration which means we can\nobserve a 0.93 cosine similarity of those two token\nembeddings after 200 iterations. As the number of\niterations increases, the increasing cosine similarity\nindicates that TAG iteratively recovers the data at\ntoken embeddings level.\nIn sentence level (Fig. 4), we convert the dummy\ninput (dummy token embeddings), X′, to dummy\ntokens by the embedding matrix and then a tok-\nenizer can help us to map the tokens with words.\nIn the Fig. 4(a), the dummy data seems random\ncompared to the ground truth (Recover Rate 0%).\nAfter 20 iterations, in the Fig. 4(b), the dummy\ndata contains two tokens (Recover Rate 22.22%)\nfrom the ground truth, \"rocks\" and \"the\". After\n50 iterations, the algorithm has recovered 7 of 9\ntokens (Recover Rate 77.78%) in the ground truth,\nand one more token has been recovered when it\nreached 200 iterations (Recover Rate 88.89%).\nLarger model leaks more information.Table 2\nsummarizes the averaged metrics of TinyBERT4,\nTinyBERT6, BERT BASE and BERT LARGE on\nthe mixture of datasets mentioned in Section 4.1,\ni.e., RTE, SST-2, and CoLA, with the same vo-\ncabulary dictionary. According to Table 1, the\nsize of model structure is sequentially increas-\ning from TinyBERT4, TinyBERT6, BERTBASE to\nBERTLARGE. From Table 2, we observe that larger\nmodels leak more information than the smaller\nones. For Recover Rate, the BERT LARGE leaks\n30% more comparing to the TinyBERT 4, 20%\nmore comparing to the TinyBERT6 and 10% more\ncomparing to the BERTBASE . A similar result can\nbe found in ROUGE-1. As for ROUGE-2, the in-\nformation leaked from BERTLARGE is 5×, 2.5×,\nand 2 ×compared to TinyBERT 4, TinyBERT 6,\nand BERTBASE , respectively. For ROUGE-L, the\nlargest model BERTLARGE leaks the most infor-\nmation, which is 2.5×, 1.8×, and 1.5×larger than\nTinyBERT4, TinyBERT6, and BERTBASE .\nResearchers indicate that to obtain a better re-\nsult in NLP , we should use a larger model on a\n(a). 5 Iterations\nCosine Similarity = 0.42\n(c). 50 Iterations\nCosine Similarity = 0.81\n(c). 200 Iterations\nCosine Similarity = 0.93\n(b). 20 Iterations\nCosine Similarity = 0.69\nFigure 3: PCA 2D plot for dimension reduced token embeddings of TinyBERT 4 on CoLA. The cosine similarity\nof dimension reduced token embeddings between dummy data and ground truth increase with training iterations.\nModels Recover Rate(%) ROUGE-1(%) ROUGE-2(%) ROUGE-L(%) Runtime (Seconds)\nTinyBERT4 29.45 27.07 3.12 22.41 503.24\nTinyBERT6 38.37 34.95 6.54 30.87 526.01\nBERTBASE 40.84 41.95 7.77 38.08 1278.62\nBERTLARGE 49.62 48.67 15.03 53.09 1672.52\nTable 2: The average values of Recover Rate, ROUGE-1, ROUGE-2, ROUGE-L and Runtime. The results are\nobtained from TinyBERT4, TinyBERT6, BERTBASE and BERTLARGE on CoLA, SST-2, RTE datasets.\nExample 1 Example 2\nTAG\nWe monitoring thetheglobal\npandemicand will andupdate\ntheconference plansof of\nthethe conference datesdates.\nThe area chairs reviewersreviewers\nwill andareaof conference\nbroad expertiseexpertisecovermachines\norcases\nDLG (Zhu et al., 2019)\nWewe studentsmonitoringmonitoringthe\npandemicand of pandemic plansplans\nas needed closer to theconference dates.\nThe wechairschairs written\nworkwillwill peopleexpertiseexpertise\nlonger casescases.\nGround Truth\nWe are monitoring the ongoing global\npandemic and will update the conference plans\nas needed closer to the conference dates.\nThe area chairs and reviewers in each\narea will have broad expertise to\ncover these cases.\nTable 3: Recover comparison of DLG (Zhu et al., 2019) and TAG on sample texts with basic transformer language\nmodel. The sentences are selected randomly from online source. Compared to DLG (Zhu et al., 2019), TAG\nrecovers up to 2×words.\nGround Truth: [CLS] the sailors rode the breeze clear of the rocks .\nDummy: ufo つ ##ub 999 12 hostages strictly ##ouse cool writing nonstop \nDummy: rocks . hydroelectric ari jamie cornerstone greenfield herrera \nrocks . cares the \nDummy: rocks [CLS] . . . the rode breeze the . clear the \nDummy: rocks [CLS] sailors . . the rode breeze the . clear the\n(a). 5 iterations (Dummy data contains 0 tokens in Ground Truth)\n(b). 20 iterations (Dummy data contains 2 out of 9 tokens in Ground Truth)\n(c). 50 iterations (Dummy data contains 7 out of 9 tokens in Ground Truth)\n(d). 200 iterations (Dummy data contains 8 out of 9 tokens in Ground Truth)\nFigure 4: Recover progress of TAG on a sentence ex-\nample of CoLA. In token level, TAG eventually recov-\ners 8 of 9 tokes (88.89% Recover Rate) from ground\ntruth which comes from a sentence of CoLA dataset.\n0.0 0.5 1.0 1.5 2.0 2.5 3.0\nRecover Rate\nROUGE-1\nROUGE-2\n ROUGE-L\nNormalized Value\nGTA\nDLG\nTAG\nFigure 5: Normalized values of ROUGE-1, ROUGE-2,\nROUGE-L, and Recover Rate of TAG and DLG (Zhu\net al., 2019) (values normalized to DLG metrics). Es-\npecially for ROUGE-2, TAG is more than 2.5 ×to\nDLG (Zhu et al., 2019).\nlarger dataset in their paper (Raffel et al., 2019b).\nBased on our experiments, smaller NLP models\ntend to be more resilient against gradient-based\nattacks. Information and data security could be\nanother dimension adding to the current tradeoffs\namong accuracy, latency, and memory footprint.\nLarger model requires more time for recover-\ning. We evaluate the runtime performance of our\nproposed algorithm among different models under\n1,000 iterations. A larger model generates more\ngradients, and in order to recover the data, we need\nto build the same structure model as the adversarial\nmodel to apply our algorithm. Hence, in Table 2,\nwe can see that runtime increases as the model\ngets larger. BERTLARGE costs 3×runtime as com-\npared to the TinyBERT 4, and BERT BASE takes\n2.5×more runtime as compared with TinyBERT4.\nOur algorithm achieves 2.7×in ROUGE-2 to\nprior art.We also compare our algorithm with the\nprior art DLG (Zhu et al., 2019). In Table 3, we\nWeight Distribution\nUniform\n(Initializer Range)\nNormal (Mean=0)\n(Standard Deviation)\n±0.01±0.02±0.03 0.01 0.02 0.03\nRecover Rate(%) 36.21 52.17 60.25 50.12 41.57 33.33\nROUGE-1(%) 39.39 44.27 60.98 54.54 45.56 35.71\nROUGE-2(%) 14.54 15.09 23.63 30.00 1.01 0\nROUGE-L(%) 44.39 46.98 57.43 66.66 40.01 37.01\nTable 4: Recover Rate, ROUGE-1, ROUGE-2,\nROUGE-L values of TAG with TinyBERT 6 uniform\ndistribution and normal distribution on sample sentence\nfrom CoLA under.\napply our algorithm and DLG (Zhu et al., 2019)\non Transformer (Vaswani et al., 2017) and attack\na sentence from online source. Compared to the\nDLG (Zhu et al., 2019), our proposed algorithm\nrecovers more than 2 ×words and compares to\nthe ground truth. More importantly, we almost re-\ncover all keywords. We further apply TAG and\nDLG (Zhu et al., 2019) on BERT, and evaluate the\nresults on the randomly chosen 100 sentences from\nCoLA and RTE dataset and calculate the averaged\nvalue for each experiment. Fig. 5 shows the re-\nsults. Compared to DLG (Zhu et al., 2019), TAG\ndemonstrates distinct advantages. For ROUGE-\n2, the result of TAG is about 2.7×to DLG (Zhu\net al., 2019). As for ROUGE-1, ROUGE-L and\nRecover Rate, TAG also takes a 1.5×advantages\nto DLG (Zhu et al., 2019), which is signiﬁcant.\n6 Ablation Studies\nIn this section, we conduct ablation experiments\nover several parameters when we evaluate the re-\nsults of our algorithm. We change the section of\nthe following factors: the weight distributions, the\npre-trained weight, the length of the sentence data,\nand the size of the vocabulary dictionary.\n6.1 Effects of weight distributions\nWe evaluate the effects of weight distributions by\ndifferent distributions and different standard devia-\ntions of the distributions. As shown in Table 4, we\nuse the TinyBERT6 model and choose sample data\nfrom CoLA to apply different weight distributions.\nFor normal distribution with mean as 0, TAG can\nrecover half words from the sentence when stan-\ndard deviation is 0.01 while it can only recover one\nof three words from the sentence with a 0.03 stan-\ndard deviation. For the uniform distribution weight\ninitialization, the results show that TAG is able to\nrecover more with larger initialization range.\nModels Pre-trained ModelInitialized Model\nDatasets CoLA SST-2 CoLA SST-2\nRecover Rate(%)48.76 43.85 34.13 33.82\nROUGE-1(%) 45.68 36.40 30.84 30.74\nROUGE-2(%) 8.01 4.26 6.41 5.45\nROUGE-L(%) 37.61 32.95 26.80 26.42\nTable 5: Recover Rate, ROUGE-1, ROUGE-2,\nROUGE-L values of TAG with TinyBERT6 on weight\nfrom pre-trained model and normal initialized model.\nDatasets Recover\nRate(%)R-1(%) R-2(%) R-L(%)\nRTE(∼50 words)22.70 13.40 1.09 11.29\nCoLA(∼10 words)34.13 30.84 6.41 26.80\nTable 6: Recover Rate, ROUGE-1 (R-1), ROUGE-2 (R-\n2), and ROUGE-L (R-L) values of TAG on comparison\nof different length sentences from RTE and CoLA.\n6.2 Effects of weights from pre-trained model\nWe evaluate our proposed algorithm on the effects\nof weights from pre-trained model on two different\ndatasets, CoLA and SST-2. In this experiment, we\nchoose the TinyBERT6 model and download the\npre-trained version from GitHub and also initialize\nthis model using normal distribution with mean as\n0 and standard deviation as 0.02. In Table 5, for\nthe CoLA dataset, pre-trained model demonstrates\n1.5×better than the initialized model. Overall, the\npre-trained model shows a better result than the\ninitialized model. We consider that the pre-trained\nmodel may contain more information during the\ntraining process than the initialized model.\nV ocabulary Small-Scale Medium-ScaleRatio\nTotal # of Tokens 21,128 30,522 0.69\nRecover Rate(%) 54.61 34.13 1.60\nROUGE-1(%) 54.87 30.84 1.78\nROUGE-2(%) 11.83 6.41 1.85\nROUGE-L(%) 47.40 26.80 1.77\nTable 7: Recover Rate, ROUGE-1, ROUGE-2, and\nROUGE-L values of TAG on comparison of different\nscales of vocabulary dictionaries.\n6.3 Performance on different datasets\nTo evaluate the effects of different sentence length\nto our proposed algorithm, we conduct experiments\non datasets: RTE and CoLA. RTE is a dataset that\ncontains longer sentences than CoLA. We choose\nsentences to contain more than 50 words from RTE,\nwhile sentences within ten words from CoLA as\nthe input data for this experiment. We choose\nthe TinyBERT6 model with initialized normal dis-\ntributed weight for this experiment. In Table 6, the\nresults from CoLA are better than RTE, especially\nfor ROUGE family. The ROUGE-1 and ROUGE-2\nof CoLA are 3×better than RTE, and ROUGE-L\nis 2.5×better than RTE.\n6.4 Effects of vocabulary dictionary\nTo evaluate the effects of vocabulary scale, we\nchoose a small scale vocabulary from (Cui et al.,\n2019) and a medium scale vocabulary from\nBERT (Devlin et al., 2019). The total numbers\nof tokens in the small and medium vocabular-\nies are 21,128 and 30,522, respectively. We use\nTinyBERT6 model on CoLA and only alter the vo-\ncabulary. In Table 7, we observe that the smaller\nvocabulary size may result in more leakage while\nthe larger one leaks less. For the smaller vocabu-\nlary size, the result is more than 1.6×improvement\ncompared to the larger one in terms of all evalua-\ntion metrics.\n7 Conclusion\nIn this work, we propose, TAG, Transformer At-\ntack from Gradient framework with an adversary\nalgorithm to recover private text data from the trans-\nformer model’s gradients. We demonstrate that\nTAG addresses private information like name is\nlikely to be leaked in transformer-based model. We\ndevelop a set of metrics to evaluate the effective-\nness of the proposed attack algorithm quantitatively.\nOur experiments show that TAG works well on\nmore different weight distributions in recovering\nprivate training data on Transformer, TinyBERT4,\nTinyBERT6, BERT BASE , and BERT LARGE us-\ning GLUE benchmark, and achieves 1.5×Recover\nRate and 2.5×ROUGE-2 over prior methods with-\nout the need of ground truth label. TAG can obtain\nup to 88.9% tokens and up to 0.93 cosine similar-\nity in token embeddings from private training data\nby attacking gradients on CoLA dataset. We hope\nthe proposed TAG will shed some light on the pri-\nvacy leakage problem in Transformer-based NLP\nmodels.\n8 Acknowledgements\nThis research is supported in part by the National\nScience Foundation (NSF) Grants 1743418 and\n1843025. Hang Liu is in part supported by NSF\nCRII Award No. 2000722 and CAREER Award\nNo. 204610.\nReferences\nHangbo Bao, Li Dong, Furu Wei, Wenhui Wang,\nNan Yang, Xiaodong Liu, Yu Wang, Songhao\nPiao, Jianfeng Gao, Ming Zhou, et al. 2020.\nUnilmv2: Pseudo-masked language models for uni-\nﬁed language model pre-training. arXiv preprint\narXiv:2002.12804.\nGilad Baruch, Moran Baruch, and Yoav Goldberg.\n2019. A little is enough: Circumventing defenses\nfor distributed learning. In Advances in Neural In-\nformation Processing Systems . Curran Associates,\nInc.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nSi Chen, Ruoxi Jia, and Guo-Jun Qi. 2020. Improved\ntechniques for model inversion attacks.\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin,\nZiqing Yang, Shijin Wang, and Guoping Hu. 2019.\nPre-training with whole word masking for chinese\nbert. arXiv preprint arXiv:1906.08101.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2005. The pascal recognising textual entailment\nchallenge. In Machine Learning Challenges Work-\nshop, pages 177–190. Springer.\nDipankar Das, Sasikanth Avancha, Dheevatsa Mudi-\ngere, Karthikeyan Vaidynathan, Srinivas Sridha-\nran, Dhiraj Kalamkar, Bharat Kaul, and Pradeep\nDubey. 2016. Distributed deep learning using syn-\nchronous stochastic gradient descent. arXiv preprint\narXiv:1602.06709.\nJeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen,\nMatthieu Devin, Mark Mao, Marc' aurelio Ranzato,\nAndrew Senior, Paul Tucker, Ke Yang, Quoc Le, and\nAndrew Ng. 2012. Large scale distributed deep net-\nworks. In Advances in Neural Information Process-\ning Systems, volume 25. Curran Associates, Inc.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning.\nMatt Fredrikson, Somesh Jha, and Thomas Ristenpart.\n2015. Model inversion attacks that exploit conﬁ-\ndence information and basic countermeasures. In\nProceedings of the 22nd ACM SIGSAC Conference\non Computer and Communications Security , pages\n1322–1333.\nJonas Geiping, Hartmut Bauermeister, Hannah Dröge,\nand Michael Moeller. 2020. Inverting gradients -\nhow easy is it to break privacy in federated learning?\nIn Advances in Neural Information Processing Sys-\ntems, volume 33, pages 16937–16947. Curran Asso-\nciates, Inc.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\nBing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. 2014. Generative ad-\nversarial nets. In Advances in neural information\nprocessing systems, pages 2672–2680.\nDeniz Gurevin, Mikhail Bragin, Caiwen Ding,\nShanglin Zhou, Lynn Pepin, Bingbing Li, and Fei\nMiao. 2021. Enabling retrain-free deep neural\nnetwork pruning using surrogate lagrangian relax-\nation. In Proceedings of the Thirtieth International\nJoint Conference on Artiﬁcial Intelligence, IJCAI-\n21, pages 2497–2504. International Joint Confer-\nences on Artiﬁcial Intelligence Organization. Main\nTrack.\nDan Hendrycks and Kevin Gimpel. 2016. Gaus-\nsian error linear units (gelus). arXiv preprint\narXiv:1606.08415.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang,\nXiao Chen, Linlin Li, Fang Wang, and Qun Liu.\n2020. Tinybert: Distilling bert for natural language\nunderstanding.\nBohan Li, Hao Zhou, Junxian He, Mingxuan Wang,\nYiming Yang, and Lei Li. 2020. On the sentence em-\nbeddings from pre-trained language models. arXiv\npreprint arXiv:2011.05864.\nMu Li, David G. Andersen, Jun Woo Park, Alexander J.\nSmola, Amr Ahmed, Vanja Josifovski, James Long,\nEugene J. Shekita, and Bor-Yiing Su. 2014a. Scal-\ning distributed machine learning with the parameter\nserver. In Proceedings of the 11th USENIX Confer-\nence on Operating Systems Design and Implementa-\ntion, OSDI’14, page 583–598, USA. USENIX Asso-\nciation.\nMu Li, David G Andersen, Alexander J Smola, and\nKai Yu. 2014b. Communication efﬁcient distributed\nmachine learning with the parameter server. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 27. Curran Associates, Inc.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out , pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nSheng-Jie Lin, Chenghong Wang, Hongjia Li, Jieren\nDeng, Yanzhi Wang, and Caiwen Ding. 2020. Esmﬂ:\nEfﬁcient and secure models for federated learning.\nArXiv, abs/2009.01867.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nLuca Melis, Congzheng Song, Emiliano De Cristo-\nfaro, and Vitaly Shmatikov. 2019. Exploiting un-\nintended feature leakage in collaborative learning.\nIn 2019 IEEE Symposium on Security and Privacy\n(SP), pages 691–706. IEEE.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019a. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv, pages arXiv–1910.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019b. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng,\nand Christopher Potts. 2013. Recursive deep mod-\nels for semantic compositionality over a sentiment\ntreebank. In Proceedings of the 2013 conference on\nempirical methods in natural language processing ,\npages 1631–1642.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding.\nYijue Wang, Chenghong Wang, Zigeng Wang,\nShanglin Zhou, Hang Liu, Jinbo Bi, Caiwen Ding,\nand Sanguthevar Rajasekaran. 2021. Against mem-\nbership inference attack: Pruning is all you need. In\nProceedings of the Thirtieth International Joint Con-\nference on Artiﬁcial Intelligence, IJCAI-21 , pages\n3141–3147. International Joint Conferences on Ar-\ntiﬁcial Intelligence Organization. Main Track.\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2019. Neural network acceptability judg-\nments.\nXin Wu, Hao Zheng, Zuochao Dou, Feng Chen, Jieren\nDeng, Xiang Chen, Shengqian Xu, Guanmin Gao,\nMengmeng Li, Zhen Wang, Yuhui Xiao, Kang Xie,\nShuang Wang, and Huji Xu. 2020. A novel privacy-\npreserving federated genome-wide association study\nframework and its application in identifying poten-\ntial risk variants in ankylosing spondylitis. Brieﬁngs\nin Bioinformatics, 22(3). Bbaa090.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in neural in-\nformation processing systems, pages 5754–5764.\nLigeng Zhu, Zhijian Liu, and Song Han. 2019. Deep\nleakage from gradients. In Advances in Neural In-\nformation Processing Systems, pages 14774–14784.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7705550193786621
    },
    {
      "name": "Computer science",
      "score": 0.7293726205825806
    },
    {
      "name": "Adversary",
      "score": 0.48950278759002686
    },
    {
      "name": "Language model",
      "score": 0.48629578948020935
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4669773280620575
    },
    {
      "name": "Training set",
      "score": 0.4614260494709015
    },
    {
      "name": "Ground truth",
      "score": 0.45201289653778076
    },
    {
      "name": "Computer security",
      "score": 0.19142654538154602
    },
    {
      "name": "Engineering",
      "score": 0.1019904613494873
    },
    {
      "name": "Voltage",
      "score": 0.0969744622707367
    },
    {
      "name": "Electrical engineering",
      "score": 0.09694769978523254
    }
  ]
}