{
  "title": "TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation",
  "url": "https://openalex.org/W3127751679",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2367696820",
      "name": "Chen, Jieneng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2753263455",
      "name": "Lu Yongyi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4283440896",
      "name": "Yu, Qihang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221795752",
      "name": "Luo, Xiangde",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2751047103",
      "name": "Adeli, Ehsan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1967442656",
      "name": "Wang Yan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2252119930",
      "name": "Lu Le",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226501798",
      "name": "Yuille, Alan L.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2349932572",
      "name": "Zhou Yu-yin",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1903029394",
    "https://openalex.org/W2888358068",
    "https://openalex.org/W3026315751",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2432481613",
    "https://openalex.org/W2950739196",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W2952178620",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2771252144",
    "https://openalex.org/W2798122215",
    "https://openalex.org/W2964227007"
  ],
  "abstract": "Medical image segmentation is an essential prerequisite for developing healthcare systems, especially for disease diagnosis and treatment planning. On various medical image segmentation tasks, the u-shaped architecture, also known as U-Net, has become the de-facto standard and achieved tremendous success. However, due to the intrinsic locality of convolution operations, U-Net generally demonstrates limitations in explicitly modeling long-range dependency. Transformers, designed for sequence-to-sequence prediction, have emerged as alternative architectures with innate global self-attention mechanisms, but can result in limited localization abilities due to insufficient low-level details. In this paper, we propose TransUNet, which merits both Transformers and U-Net, as a strong alternative for medical image segmentation. On one hand, the Transformer encodes tokenized image patches from a convolution neural network (CNN) feature map as the input sequence for extracting global contexts. On the other hand, the decoder upsamples the encoded features which are then combined with the high-resolution CNN feature maps to enable precise localization. We argue that Transformers can serve as strong encoders for medical image segmentation tasks, with the combination of U-Net to enhance finer details by recovering localized spatial information. TransUNet achieves superior performances to various competing methods on different medical applications including multi-organ segmentation and cardiac segmentation. Code and models are available at https://github.com/Beckschen/TransUNet.",
  "full_text": "TransUNet: Transformers Make Strong\nEncoders for Medical Image Segmentation\nJieneng Chen1, Yongyi Lu1, Qihang Yu1, Xiangde Luo2,\nEhsan Adeli3, Yan Wang4, Le Lu5, Alan L. Yuille1, and Yuyin Zhou3\n1Johns Hopkins University\n2University of Electronic Science and Technology of China\n3Stanford University\n4 East China Normal University\n5PAII Inc.\nAbstract. Medical image segmentation is an essential prerequisite for\ndeveloping healthcare systems, especially for disease diagnosis and treat-\nment planning. On various medical image segmentation tasks, the u-\nshaped architecture, also known as U-Net, has become the de-facto stan-\ndard and achieved tremendous success. However, due to the intrinsic\nlocality of convolution operations, U-Net generally demonstrates limi-\ntations in explicitly modeling long-range dependency. Transformers, de-\nsigned for sequence-to-sequence prediction, have emerged as alternative\narchitectures with innate global self-attention mechanisms, but can re-\nsult in limited localization abilities due to insuÔ¨Écient low-level details.\nIn this paper, we propose TransUNet, which merits both Transformers\nand U-Net, as a strong alternative for medical image segmentation. On\none hand, the Transformer encodes tokenized image patches from a con-\nvolution neural network (CNN) feature map as the input sequence for\nextracting global contexts. On the other hand, the decoder upsamples\nthe encoded features which are then combined with the high-resolution\nCNN feature maps to enable precise localization.\nWe argue that Transformers can serve as strong encoders for medical im-\nage segmentation tasks, with the combination of U-Net to enhance Ô¨Åner\ndetails by recovering localized spatial information. TransUNet achieves\nsuperior performances to various competing methods on diÔ¨Äerent medical\napplications including multi-organ segmentation and cardiac segmenta-\ntion. Code and models are available athttps://github.com/Beckschen/\nTransUNet.\n1 Introduction\nConvolutional neural networks (CNNs), especially fully convolutional networks\n(FCNs) [8], have become dominant in medical image segmentation. Among dif-\nferent variants, U-Net [12], which consists of a symmetric encoder-decoder net-\nwork with skip-connections to enhance detail retention, has become the de-facto\nchoice. Based on this line of approach, tremendous success has been achieved\nin a wide range of medical applications such as cardiac segmentation from\narXiv:2102.04306v1  [cs.CV]  8 Feb 2021\n2 J. Chen et al.\nmagnetic resonance (MR) [16], organ segmentation from computed tomography\n(CT) [7,17,19] and polyp segmentation [20] from colonoscopy videos.\nIn spite of their exceptional representational power, CNN-based approaches\ngenerally exhibit limitations for modeling explicit long-range relation, due to the\nintrinsic locality of convolution operations. Therefore, these architectures gen-\nerally yield weak performances especially for target structures that show large\ninter-patient variation in terms of texture, shape and size. To overcome this lim-\nitation, existing studies propose to establish self-attention mechanisms based on\nCNN features [13,15]. On the other hand, Transformers, designed for sequence-\nto-sequence prediction, have emerged as alternative architectures which employ\ndispense convolution operators entirely and solely rely on attention mechanisms\ninstead [14]. Unlike prior CNN-based methods, Transformers are not only power-\nful at modeling global contexts but also demonstrate superior transferability for\ndownstream tasks under large-scale pre-training. The success has been widely\nwitnessed in the Ô¨Åeld of machine translation and natural language processing\n(NLP) [3,14]. More recently, attempts have also matched or even exceeded state-\nof-the-art performances for various image recognition tasks [4,18].\nIn this paper, we present the Ô¨Årst study which explores the potential of trans-\nformers in the context of medical image segmentation. However, interestingly,\nwe found that a naive usage ( i.e., use a transformer for encoding the tokenized\nimage patches, and then directly upsamples the hidden feature representations\ninto a dense output of full resolution) cannot produce a satisfactory result.\nThis is due to that Transformers treat the input as 1D sequences and ex-\nclusively focus on modeling the global context at all stages, therefore result in\nlow-resolution features which lack detailed localization information. And this\ninformation cannot be eÔ¨Äectively recovered by direct upsampling to the full res-\nolution, therefore leads to a coarse segmentation outcome. On the other hand,\nCNN architectures (e.g., U-Net [12]) provide an avenue for extracting low-level\nvisual cues which can well remedy such Ô¨Åne spatial details.\nTo this end, we propose TransUNet, the Ô¨Årst medical image segmentation\nframework, which establishes self-attention mechanisms from the perspective of\nsequence-to-sequence prediction. To compensate for the loss of feature resolu-\ntion brought by Transformers, TransUNet employs a hybrid CNN-Transformer\narchitecture to leverage both detailed high-resolution spatial information from\nCNN features and the global context encoded by Transformers. Inspired by the\nu-shaped architectural design, the self-attentive feature encoded by Transformers\nis then upsampled to be combined with diÔ¨Äerent high-resolution CNN features\nskipped from the encoding path, for enabling precise localization. We show that\nsuch a design allows our framework to preserve the advantages of Transformers\nand also beneÔ¨Åt medical image segmentation. Empirical results suggest that our\nTransformer-based architecture presents a better way to leverage self-attention\ncompared with previous CNN-based self-attention methods. Additionally, we ob-\nserve that more intensive incorporation of low-level features generally leads to\na better segmentation accuracy. Extensive experiments demonstrate the superi-\nTitle Suppressed Due to Excessive Length 3\nority of our method against other competing methods on various medical image\nsegmentation tasks.\n2 Related Works\nCombining CNNs with self-attention mechanisms. Various studies have\nattempted to integrate self-attention mechanisms into CNNs by modeling global\ninteractions of all pixels based on the feature maps. For instance, Wang et al.\ndesigned a non-local operator, which can be plugged into multiple intermediate\nconvolution layers [15]. Built upon the encoder-decoder u-shaped architecture,\nSchlemper et al. [13] proposed additive attention gate modules which are inte-\ngrated into the skip-connections. DiÔ¨Äerent from these approaches, we employ\nTransformers for embedding global self-attention in our method.\nTransformers.Transformers were Ô¨Årst proposed by [14] for machine translation\nand established state-of-the-arts in many NLP tasks. To make Transformers also\napplicable for computer vision tasks, several modiÔ¨Åcations have been made. For\ninstance, Parmar et al.[11] applied the self-attention only in local neighborhoods\nfor each query pixel instead of globally. Child et al. [1] proposed Sparse Trans-\nformers, which employ scalable approximations to global self-attention. Recently,\nVision Transformer (ViT) [4] achieved state-of-the-art on ImageNet classiÔ¨Åcation\nby directly applying Transformers with global self-attention to full-sized images.\nTo the best of our knowledge, the proposed TransUNet is the Ô¨Årst Transformer-\nbased medical image segmentation framework, which builds upon the highly\nsuccessful ViT.\n3 Method\nGiven an image x ‚ààRH√óW√óC with an spatial resolution of H √óW and C num-\nber of channels. Our goal is to predict the corresponding pixel-wise labelmap\nwith size H √óW. The most common way is to directly train a CNN ( e.g., U-\nNet) to Ô¨Årst encode images into high-level feature representations, which are\nthen decoded back to the full spatial resolution. Unlike existing approaches, our\nmethod introduces self-attention mechanisms into the encoder design via the us-\nage of Transformers. We will Ô¨Årst introduce how to directly apply transformer for\nencoding feature representations from decomposed image patches in Section 3.1.\nThen, the overall framework of TransUNet will be elaborated in Section 3.2.\n3.1 Transformer as Encoder\nImage Sequentialization. Following [4], we Ô¨Årst perform tokenization by re-\nshaping the input x into a sequence of Ô¨Çattened 2D patches {xi\np ‚ààRP2¬∑C|i =\n1, .., N}, where each patch is of size P √óP and N = HW\nP2 is the number of image\npatches (i.e., the input sequence length).\n4 J. Chen et al.\nreshape\n1/4\n1/8\n1/2 \nConv3x3, ReLUUpsampleSegmentation head\n(n_patch, D) (D, H/16, W/16)(512, H/16, W/16)\n(256, H/8, W/8)\n(128, H/4, W/4)\n(64, H/2, W/2)\n(16, H, W)\nTransformer Layer\n‚Ä¶(n = 12)\nHidden FeatureLinear Projection\nCNN\nHidden Feature DownsampleFeature Concatenation\nTransformer Layer\nEmbeddedSequenceùíôùíëùüè,ùíôùíëùüê,‚Ä¶,ùíôùíëùëµ\nLayerNorm\nMSA\nLayerNormMLP\n+\n+ùíõùüè(a) (b) \nFig. 1: Overview of the framework. (a) schematic of the Transformer layer; (b)\narchitecture of the proposed TransUNet.\nPatch Embedding.We map the vectorized patchesxp into a latentD-dimensional\nembedding space using a trainable linear projection. To encode the patch spatial\ninformation, we learn speciÔ¨Åc position embeddings which are added to the patch\nembeddings to retain positional information as follows:\nz0 = [x1\npE; x2\npE; ¬∑¬∑¬∑ ; xN\np E] + Epos, (1)\nwhere E ‚ààR(P2¬∑C)√óD is the patch embedding projection, and Epos ‚ààRN√óD\ndenotes the position embedding.\nThe Transformer encoder consists of L layers of Multihead Self-Attention\n(MSA) and Multi-Layer Perceptron (MLP) blocks (Eq. (2)(3)). Therefore the\noutput of the ‚Ñì-th layer can be written as follows:\nz‚Ä≤\n‚Ñì = MSA(LN(z‚Ñì‚àí1)) + z‚Ñì‚àí1, (2)\nz‚Ñì = MLP(LN(z‚Ä≤\n‚Ñì)) + z‚Ä≤\n‚Ñì, (3)\nwhere LN( ¬∑) denotes the layer normalization operator and zL is the encoded\nimage representation. The structure of a Transformer layer is illustrated in Fig-\nure 1(a).\n3.2 TransUNet\nFor segmentation purposes, an intuitive solution is to simply upsample the en-\ncoded feature representation zL ‚ààR\nHW\nP2 √óD to the full resolution for predicting\nthe dense output. Here to recover the spatial order, the size of the encoded fea-\nture should Ô¨Årst be reshaped from HW\nP2 to H\nP √óW\nP . We use a 1 √ó1 convolution\nTitle Suppressed Due to Excessive Length 5\nto reduce channel size of the reshaped feature to number of class, and then the\nfeature map is directly bilinearly upsampled to the full resolution H √óW for\npredicting the Ô¨Ånal segmentation outcome. In later comparisons in Section 4.3,\nwe denote this naive upsampling baseline as ‚ÄúNone‚Äù in the decoder design.\nAlthough combining a Transformer with naive upsampling already yields a\nreasonable performance, as mentioned above, this strategy is not the optimal us-\nage of Transformers in segmentation since H\nP √óW\nP is usually much smaller than\nthe original image resolution H √óW, therefore inevitably results in a loss of\nlow-level details (e.g., shape and boundary of the organ). Therefore, to compen-\nsate for such information loss, TransUNet employs a hybrid CNN-Transformer\narchitecture as the encoder as well as a cascaded upsampler to enable precise\nlocalization. The overview of the proposed TransUNet is depicted in Figure 1.\nCNN-Transformer Hybrid as Encoder. Rather than using the pure Trans-\nformer as the encoder (Section 3.1), TransUNet employs a CNN-Transformer\nhybrid model where CNN is Ô¨Årst used as a feature extractor to generate a fea-\nture map for the input. Patch embedding is applied to 1 √ó1 patches extracted\nfrom the CNN feature map instead of from raw images.\nWe choose this design since 1) it allows us to leverage the intermediate high-\nresolution CNN feature maps in the decoding path; and 2) we Ô¨Ånd that the hybrid\nCNN-Transformer encoder performs better than simply using a pure Transformer\nas the encoder.\nCascaded Upsampler. We introduce a cascaded upsampler (CUP), which con-\nsists of multiple upsampling steps to decode the hidden feature for outputting\nthe Ô¨Ånal segmentation mask. After reshaping the sequence of hidden feature\nzL ‚ààR\nHW\nP2 √óD to the shape of H\nP √óW\nP √óD, we instantiate CUP by cascading\nmultiple upsampling blocks for reaching the full resolution fromH\nP √óW\nP to H√óW,\nwhere each block consists of a 2√óupsampling operator, a 3√ó3 convolution layer,\nand a ReLU layer successively.\nWe can see that CUP together with the hybrid encoder form a u-shaped\narchitecture which enables feature aggregation at diÔ¨Äerent resolution levels via\nskip-connections. The detailed architecture of CUP as well as the intermediate\nskip-connections can be found in Figure 1(b).\n4 Experiments and Discussion\n4.1 Dataset and Evaluation\nSynapse multi-organ segmentation dataset1. We use the 30 abdominal CT scans\nin the MICCAI 2015 Multi-Atlas Abdomen Labeling Challenge, with 3779 axial\ncontrast-enhanced abdominal clinical CT images in total.\nEach CT volume consists of 85 ‚àº 198 slices of 512 √ó512 pixels, with a\nvoxel spatial resolution of ([0 .54 ‚àº0.54] √ó[0.98 ‚àº0.98] √ó[2.5 ‚àº5.0])mm3.\nFollowing [5], we report the average DSC and average HausdorÔ¨Ä Distance (HD)\n1 https://www.synapse.org/#!Synapse:syn3193805/wiki/217789\n6 J. Chen et al.\non 8 abdominal organs (aorta, gallbladder, spleen, left kidney, right kidney, liver,\npancreas, spleen, stomach with a random split of 18 training cases (2212 axial\nslices) and 12 cases for validation.\nAutomated cardiac diagnosis challenge2. The ACDC challenge collects ex-\nams from diÔ¨Äerent patients acquired from MRI scanners. Cine MR images were\nacquired in breath hold, and a series of short-axis slices cover the heart from the\nbase to the apex of the left ventricle, with a slice thickness of 5 to 8 mm. The\nshort-axis in-plane spatial resolution goes from 0.83 to 1.75 mm 2/pixel.\nEach patient scan is manually annotated with ground truth for left ventricle\n(LV), right ventricle (RV) and myocardium (MYO). We report the average DSC\nwith a random split of 70 training cases (1930 axial slices), 10 cases for validation\nand 20 for testing.\nTable 1: Comparison on the Synapse multi-organ CT dataset (average dice score\n% and average hausdorÔ¨Ä distance in mm, and dice score % for each organ).\nFramework AverageAorta Gallbladder Kidney (L) Kidney (R) Liver Pancreas Spleen Stomach\nEncoder Decoder DSC‚Üë HD‚Üì\nV-Net [9] 68.81 - 75.34 51.87 77.10 80.75 87.84 40.05 80.56 56.98\nDARR [5] 69.77 - 74.74 53.77 72.31 73.24 94.08 54.18 89.90 45.96\nR50 U-Net [12] 74.68 36.87 84.18 62.84 79.19 71.29 93.35 48.23 84.41 73.92\nR50 AttnUNet [13]75.57 36.97 55.92 63.91 79.20 72.71 93.56 49.37 87.19 74.95\nViT [4] None 61.50 39.61 44.38 39.59 67.46 62.94 89.21 43.14 75.45 69.78\nViT [4] CUP 67.86 36.11 70.19 45.10 74.70 67.40 91.32 42.00 81.75 70.44\nR50-ViT [4] CUP 71.29 32.87 73.73 55.13 75.80 72.20 91.51 45.99 81.99 73.95\nTransUNet 77.48 31.6987.23 63.13 81.87 77.02 94.08 55.86 85.08 75.62\n4.2 Implementation Details\nFor all experiments, we apply simple data augmentations, e.g., random rotation\nand Ô¨Çipping.\nFor pure Transformer-based encoder, we simply adopt ViT [4] with 12 Trans-\nformer layers. For the hybrid encoder design, we combine ResNet-50 [6] and ViT,\ndenoted as ‚ÄúR50-ViT‚Äù, throught this paper. All Transformer backbones ( i.e.,\nViT) and ResNet-50 (denoted as ‚ÄúR-50‚Äù) were pretrained on ImageNet [2]. The\ninput resolution and patch size P are set as 224 √ó224 and 16, unless otherwise\nspeciÔ¨Åed. Therefore, we need to cascade four 2 √óupsampling blocks consecu-\ntively in CUP to reach the full resolution. And for Models are trained with SGD\noptimizer with learning rate 0.01, momentum 0.9 and weight decay 1e-4. The\ndefault batch size is 24 and the default number of training iterations are 20k\nfor ACDC dataset and 14k for Synapse dataset respectively. All experiments are\nconducted using a single Nvidia RTX2080Ti GPU.\nFollowing [17,19], all 3D volumes are inferenced in a slice-by-slice fashion and\nthe predicted 2D slices are stacked together to reconstruct the 3D prediction for\nevaluation.\n2 https://www.creatis.insa-lyon.fr/Challenge/acdc/\nTitle Suppressed Due to Excessive Length 7\n4.3 Comparison with State-of-the-arts\nWe conduct main experiments on Synapse multi-organ segmentation dataset by\ncomparing our TransUNet with four previous state-of-the-arts: 1) V-Net [9]; 2)\nDARR [5]; 3) U-Net [12] and 4) AttnUNet [13].\nTo demonstrate the eÔ¨Äectiveness of our CUP decoder, we use ViT [4] as the\nencoder, and compare results using naive upsampling (‚ÄúNone‚Äù) and CUP as the\ndecoder, respectively; To demonstrate the eÔ¨Äectiveness of our hybrid encoder\ndesign, we use CUP as the decoder, and compare results using ViT and R50-\nViT as the encoder, respectively. In order to make the comparison with the ViT-\nhybrid baseline (R50-ViT-CUP) and our TransUNet to be fair, we also replace\nthe original encoder of U-Net [12] and AttnUNet [10] with ImageNet pretrained\nResNet-50. The results in terms of DSC and mean hausdorÔ¨Ä distance (in mm)\nare reported in Table 1.\nFirstly, we can see that compared with ViT-None, ViT-CUP observes an\nimprovement of 6 .36% and 3 .50 mm in terms of average DSC and HausdorÔ¨Ä\ndistance respectively. This improvement suggests that our CUP design presents\na better decoding strategy than direct upsampling. Similarly, compared with\nViT-CUP, R50-ViT-CUP also suggests an additional improvement of 3 .43% in\nDSC and 3 .24 mm in HausdorÔ¨Ä distance, which demonstrates the eÔ¨Äectiveness\nof our hybrid encoder. Built upon R50-ViT-CUP, our TransUNet which is also\nequipped with skip-connections, achieves the best result among diÔ¨Äerent variants\nof Transformer-based models.\nSecondly, Table 1 also shows that the proposed TransUNet has signiÔ¨Åcant im-\nprovements over prior arts, e.g., performance gains range from 1.91% to 8.67%\nconsidering average DSC. In particular, directly applying Transformers for multi-\norgan segmentation yields reasonable results (67.86% DSC for ViT-CUP), but\ncannot match the performance of U-Net or attnUNet. This is due to that Trans-\nformers can well capture high-level semantics which are favorable for classiÔ¨Å-\ncation task but lack of low-level cues for segmenting the Ô¨Åne shape of medical\nimages. On the other hand, combining Transformers with CNN, i.e., R50-ViT-\nCUP, outperforms V-Net and DARR but still yield inferior results than pure\nCNN-based R50-U-Net and R50-AttnUNet. Finally, when combined with the\nU-Net structure via skip-connections, the proposed TransUNet sets a new state-\nof-the-art, outperforming R50-ViT-CUP and previous best R50-AttnUNet by\n6.19% and 1.91% respectively, showing the strong ability of TransUNet to learn\nboth high-level semantic features as well as low-level details, which is crucial\nin medical image segmentation. A similar trend can be also witnessed for the\naverage HausdorÔ¨Ä distance, which further demonstrates the advantages of our\nTransUNet over these CNN-based approaches.\n4.4 Analytical Study\nTo thoroughly evaluate the proposed TransUNet framework and validate the per-\nformance under diÔ¨Äerent settings, a variety of ablation studies were performed,\n8 J. Chen et al.\nincluding: 1) the number of skip-connections; 2) input resolution; 3) sequence\nlength and patch size and 4) model scaling.\nThe Number of Skip-connections. As discussed above, integrating U-Net-\nlike skip-connections help enhance Ô¨Åner segmentation details by recovering low-\nlevel spatial information. The goal of this ablation is to test the impact of adding\ndiÔ¨Äerent numbers of skip-connections in TransUNet. By varying the number of\nskip-connections to be 0 (R50-ViT-CUP)/1/3, the segmentation performance\nin average DSC on all 8 testing organs are summarized in Figure 2. Note that\nin the ‚Äú1-skip‚Äù setting, we add the skip-connection only at the 1/4 resolution\nscale. We can see that adding more skip-connections generally leads to a bet-\nter segmentation performance. The best average DSC and HD are achieved by\ninserting skip-connections to all three intermediate upsampling steps of CUP ex-\ncept the output layer, i.e., at 1/2, 1/4, and 1/8 resolution scales (illustrated in\nFigure 1). Thus, we adopt this conÔ¨Åguration for our TransUNet. It is also worth\nmentioning that the performance gain of smaller organs (i.e., aorta, gallbladder,\nkidneys, pancreas) is more evident than that of larger organs ( i.e., liver, spleen,\nstomach). These results reinforce our initial intuition of integrating U-Net-like\nskip-connections into the Transformer design to enable learning precise low-level\ndetails.\nAs an interesting study, we apply additive Transformers in the skip-connections,\nsimilar to [13], and Ô¨Ånd this new type of skip-connection can even further the\nsegmentation performance. Due to the GPU memory constraint, we employ a\nlight Transformer in the 1/8 resolution scale skip-connection while keeping the\nother two skip-connections unchanged. As a result, this simple alteration leads\nto a performance boost of 1.4 % DSC.\n405060708090100\nAortaGallbladderKidney (L)Kidney (R)LiverPancreasSpleenStomachAverage\nDSC (%) vs. Number of Skip Connections\n0-skip1-skip3-skip\nFig. 2: Ablation study on the number of skip-connections in TransUNet.\nOn the InÔ¨Çuence of Input Resolution. The default input resolution for\nTransUNet is 224 √ó224. Here, we also provide results of training TransUNet\non a high-resolution 512 √ó512, as shown in Table 2. When using 512 √ó512 as\ninput, we keep the same patch size ( i.e., 16), which results in an approximate\nTitle Suppressed Due to Excessive Length 9\n5√ólarger sequence length for the Transformer. As [4] indicated, increasing the\neÔ¨Äective sequence length shows robust improvements. For TransUNet, changing\nthe resolution scale from 224 √ó224 to 512√ó512 results in 6.88% improvement in\naverage DSC, at the expense of a much larger computational cost. Therefore,\nconsidering the computation cost, all experimental comparisons in this paper are\nconducted with a default resolution of 224√ó224 to demonstrate the eÔ¨Äectiveness\nof TransUNet.\nTable 2: Ablation study on the inÔ¨Çuence of input resolution.\nResolutionAverage DSCAorta Gallbladder Kidney (L) Kidney (R) Liver Pancreas Spleen Stomach\n224 77.48 87.23 63.13 81.87 77.02 94.08 55.86 85.08 75.62\n512 84.36 90.68 71.99 86.04 83.71 95.54 73.96 88.80 84.20\nOn the InÔ¨Çuence of Patch Size/Sequence Length.\nWe also investigate the inÔ¨Çuence of patch size on TransUNet. The results\nare summarized in Table 3. It is observed that a higher segmentation perfor-\nmance is usually obtained with smaller patch size. Note that the Transformer‚Äôs\nsequence length is inversely proportional to the square of the patch size ( e.g.,\npatch size 16 corresponds to a sequence length of 196 while patch size 32 has a\nshorter sequence length of 49), therefore decreasing the patch size (or increasing\nthe eÔ¨Äective sequence length) shows robust improvements, as the Transformer\nencodes more complex dependencies between each element for longer input se-\nquences. Following the setting in ViT [4], we use 16√ó16 as the default patch size\nthroughout this paper.\nTable 3: Ablation study on the patch size and the sequence length.\nPatch sizeSeqlengthAverage DSCAorta Gallbladder Kidney (L) Kidney (R) Liver Pancreas Spleen Stomach\n32 49 76.99 86.66 63.06 81.61 79.18 94.21 51.66 85.38 74.17\n16 196 77.48 87.23 63.13 81.87 77.02 94.08 55.86 85.08 75.62\n8 784 77.83 86.92 58.31 81.51 76.40 93.81 58.09 87.92 79.68\nModel Scaling. Last but not least, we provide ablation study on diÔ¨Äerent\nmodel sizes of TransUNet. In particular, we investigate two diÔ¨Äerent TransUNet\nconÔ¨Ågurations, the ‚ÄúBase‚Äù and ‚ÄúLarge‚Äù models. For the ‚Äúbase‚Äù model, the hidden\nsize D, number of layers, MLP size, and number of heads are set to be 12, 768,\n3072, and 12, respectively while those hyperparamters for ‚Äúlarge‚Äù model are 24,\n1024, 4096, and 16. From Table 4 we conclude that larger model results in a\nbetter performance. Considering the computation cost, we adopt ‚ÄúBase‚Äù model\nfor all the experiments.\nTable 4: Ablation study on the model scale.\nModel scaleAverage DSCAorta Gallbladder Kidney (L) Kidney (R) Liver Pancreas Spleen Stomach\nBase 77.48 87.23 63.13 81.87 77.02 94.08 55.86 85.08 75.62\nLarge 78.52 87.42 63.92 82.17 80.19 94.47 57.64 87.42 74.90\n10 J. Chen et al.\naorta        gallbladder        left kidney         right kidney       liver         pancreas        spleen         stomach\n(a) GroundTruth (b) TransUNet (c) R50-ViT-CUP             (d) AttnUNet (e) UNet\nFig. 3: Qualitative comparison of diÔ¨Äerent approaches by visualization. From\nleft to right: (a) Ground Truth, (b) TransUNet, (c) R50-ViT-CUP, (d) R50-\nAttnUNet, (e) R50-U-Net. Our method predicts less false positive and keep Ô¨Åner\ninformation.\nTable 5: Comparison on the ACDC dataset in DSC (%).\nFramework Average RV Myo LV\nR50-U-Net 87.55 87.10 80.63 94.92\nR50-AttnUNet 86.75 87.58 79.20 93.47\nViT-CUP 81.45 81.46 70.71 92.18\nR50-ViT-CUP 87.57 86.07 81.88 94.75\nTransUNet 89.71 88.86 84.53 95.73\n4.5 Visualizations\nWe provide qualitative comparison results on the Synapse dataset, as shown\nin Figure 3. It can be seen that: 1) pure CNN-based methods U-Net and Att-\nnUNet are more likely to over-segment or under-segment the organs (e.g., in the\nsecond row, the spleen is over-segmented by AttnUNet while under-segmented\nby UNet), which shows that Transformer-based models, e.g., our TransUNet or\nR50-ViT-CUP have stronger power to encode global contexts and distinguish the\nsemantics. 2) Results in the Ô¨Årst row show that our TransUNet predicts fewer\nfalse positives compared to others, which suggests that TransUNet would be\nmore advantageous than other methods in suppressing those noisy predictions.\n3) For comparison within Transformer-based models, we can observe that the\npredictions by R50-ViT-CUP tend to be coarser than those by TransUNet re-\ngarding the boundary and shape ( e.g., predictions of the pancreas in the second\nTitle Suppressed Due to Excessive Length 11\nrow). Moreover, in the third row, TransUNet correctly predicts both left and\nright kidneys while R50-ViT-CUP erroneously Ô¨Ålls the inner hole of left kidney.\nThese observations suggest that TransUNet is capable of Ô¨Åner segmentation and\npreserving detailed shape information. The reason is that TransUNet enjoys the\nbeneÔ¨Åts of both high-level global contextual information and low-level details,\nwhile R50-ViT-CUP solely relies on high-level semantic features. This again val-\nidates our initial intuition of integrating U-Net-like skip-connections into the\nTransformer design to enable precise localization.\n4.6 Generalization to Other Datasets\nTo show the generalization ability of our TransUNet, we further evaluate on\nother imaging modalities, i.e., an MR dataset ACDC aiming at automated car-\ndiac segmentation. We observe consistent improvements of TransUNet over pure\nCNN-based methods (R50-UNet and R50-AttnUnet) and other Transformer-\nbased baselines (ViT-CUP and R50-ViT-CUP), which are similar to previous\nresults on the Synapse CT dataset.\n5 Conclusion\nTransformers are known as architectures with strong innate self-attention mech-\nanisms. In this paper, we present the Ô¨Årst study to investigate the usage of Trans-\nformers for general medical image segmentation. To fully leverage the power of\nTransformers, TransUNet was proposed, which not only encodes strong global\ncontext by treating the image features as sequences but also well utilizes the\nlow-level CNN features via a u-shaped hybrid architectural design. As an al-\nternative framework to the dominant FCN-based approaches for medical image\nsegmentation, TransUNet achieves superior performances than various compet-\ning methods, including CNN-based self-attention methods.\nAcknowledgements. This work was supported by the Lustgarten Foundation\nfor Pancreatic Cancer Research.\nReferences\n1. Child, R., Gray, S., Radford, A., Sutskever, I.: Generating long sequences with\nsparse transformers. arXiv preprint arXiv:1904.10509 (2019)\n2. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-\nscale hierarchical image database. In: 2009 IEEE conference on computer vision\nand pattern recognition. pp. 248‚Äì255. Ieee (2009)\n3. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805\n(2018)\n4. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\nT., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is worth\n16x16 words: Transformers for image recognition at scale. In: ICLR (2021)\n12 J. Chen et al.\n5. Fu, S., Lu, Y., Wang, Y., Zhou, Y., Shen, W., Fishman, E., Yuille, A.: Domain\nadaptive relational reasoning for 3d multi-organ segmentation. In: International\nConference on Medical Image Computing and Computer-Assisted Intervention.\npp. 656‚Äì666. Springer (2020)\n6. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:\nProceedings of the IEEE conference on computer vision and pattern recognition.\npp. 770‚Äì778 (2016)\n7. Li, X., Chen, H., Qi, X., Dou, Q., Fu, C.W., Heng, P.A.: H-denseunet: hybrid\ndensely connected unet for liver and tumor segmentation from ct volumes. IEEE\ntransactions on medical imaging 37(12), 2663‚Äì2674 (2018)\n8. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic\nsegmentation. In: Proceedings of the IEEE conference on computer vision and\npattern recognition. pp. 3431‚Äì3440 (2015)\n9. Milletari, F., Navab, N., Ahmadi, S.A.: V-net: Fully convolutional neural networks\nfor volumetric medical image segmentation. In: 3DV (2016)\n10. Oktay, O., Schlemper, J., Folgoc, L.L., Lee, M., Heinrich, M., Misawa, K., Mori,\nK., McDonagh, S., Hammerla, N.Y., Kainz, B., et al.: Attention u-net: Learning\nwhere to look for the pancreas. MIDL (2018)\n11. Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, L., Shazeer, N., Ku, A., Tran, D.:\nImage transformer. In: International Conference on Machine Learning. pp. 4055‚Äì\n4064. PMLR (2018)\n12. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedi-\ncal image segmentation. In: International Conference on Medical image computing\nand computer-assisted intervention. pp. 234‚Äì241. Springer (2015)\n13. Schlemper, J., Oktay, O., Schaap, M., Heinrich, M., Kainz, B., Glocker, B., Rueck-\nert, D.: Attention gated networks: Learning to leverage salient regions in medical\nimages. Medical image analysis 53, 197‚Äì207 (2019)\n14. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\n L., Polosukhin, I.: Attention is all you need. In: Advances in neural information\nprocessing systems. pp. 5998‚Äì6008 (2017)\n15. Wang, X., Girshick, R., Gupta, A., He, K.: Non-local neural networks. In: Pro-\nceedings of the IEEE conference on computer vision and pattern recognition. pp.\n7794‚Äì7803 (2018)\n16. Yu, L., Cheng, J.Z., Dou, Q., Yang, X., Chen, H., Qin, J., Heng, P.A.: Automatic 3d\ncardiovascular mr segmentation with densely-connected volumetric convnets. In:\nInternational Conference on Medical Image Computing and Computer-Assisted\nIntervention. pp. 287‚Äì295. Springer (2017)\n17. Yu, Q., Xie, L., Wang, Y., Zhou, Y., Fishman, E.K., Yuille, A.L.: Recurrent saliency\ntransformation network: Incorporating multi-stage visual cues for small organ seg-\nmentation. In: Proceedings of the IEEE conference on computer vision and pattern\nrecognition. pp. 8280‚Äì8289 (2018)\n18. Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y., Fu, Y., Feng, J., Xiang, T.,\nTorr, P.H., et al.: Rethinking semantic segmentation from a sequence-to-sequence\nperspective with transformers. arXiv preprint arXiv:2012.15840 (2020)\n19. Zhou, Y., Xie, L., Shen, W., Wang, Y., Fishman, E.K., Yuille, A.L.: A Ô¨Åxed-\npoint model for pancreas segmentation in abdominal ct scans. In: International\nconference on medical image computing and computer-assisted intervention. pp.\n693‚Äì701. Springer (2017)\n20. Zhou, Z., Siddiquee, M.M.R., Tajbakhsh, N., Liang, J.: Unet++: A nested u-net\narchitecture for medical image segmentation. In: Deep Learning in Medical Im-\nTitle Suppressed Due to Excessive Length 13\nage Analysis and Multimodal Learning for Clinical Decision Support, pp. 3‚Äì11.\nSpringer (2018)",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7377326488494873
    },
    {
      "name": "Segmentation",
      "score": 0.6558579802513123
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5952180027961731
    },
    {
      "name": "Image segmentation",
      "score": 0.5714772343635559
    },
    {
      "name": "Encoder",
      "score": 0.5309179425239563
    },
    {
      "name": "Transformer",
      "score": 0.5119832158088684
    },
    {
      "name": "Convolutional neural network",
      "score": 0.4866555333137512
    },
    {
      "name": "Locality",
      "score": 0.48199573159217834
    },
    {
      "name": "Segmentation-based object categorization",
      "score": 0.4593612849712372
    },
    {
      "name": "Scale-space segmentation",
      "score": 0.4267120957374573
    },
    {
      "name": "Computer vision",
      "score": 0.41397982835769653
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.377481073141098
    },
    {
      "name": "Engineering",
      "score": 0.11631321907043457
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 3689
}