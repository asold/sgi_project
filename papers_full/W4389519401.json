{
  "title": "Understanding the Effect of Model Compression on Social Bias in Large Language Models",
  "url": "https://openalex.org/W4389519401",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2316727086",
      "name": "Gustavo Gonçalves",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A80667657",
      "name": "Emma Strubell",
      "affiliations": [
        "Carnegie Mellon University",
        "Allen Institute for Artificial Intelligence"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3134354193",
    "https://openalex.org/W4385573538",
    "https://openalex.org/W3206487987",
    "https://openalex.org/W4385573479",
    "https://openalex.org/W3104263050",
    "https://openalex.org/W3176477796",
    "https://openalex.org/W3207645655",
    "https://openalex.org/W4386566641",
    "https://openalex.org/W3155655882",
    "https://openalex.org/W4362655426",
    "https://openalex.org/W3197901717",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2972413484",
    "https://openalex.org/W3172415559",
    "https://openalex.org/W2981757109",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W3035591180",
    "https://openalex.org/W4303649067",
    "https://openalex.org/W4297816198",
    "https://openalex.org/W4378945750",
    "https://openalex.org/W3093211917",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W3105882417",
    "https://openalex.org/W3133641570",
    "https://openalex.org/W3042879175",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4285273243",
    "https://openalex.org/W3035241006",
    "https://openalex.org/W4311398160",
    "https://openalex.org/W3203385474",
    "https://openalex.org/W4226128078",
    "https://openalex.org/W4327652274"
  ],
  "abstract": "Large Language Models (LLMs) trained with self-supervision on vast corpora of web text fit to the social biases of that text. Without intervention, these social biases persist in the model’s predictions in downstream tasks, leading to representational harm. Many strategies have been proposed to mitigate the effects of inappropriate social biases learned during pretraining. Simultaneously, methods for model compression have become increasingly popular to reduce the computational burden of LLMs. Despite the popularity and need for both approaches, little work has been done to explore the interplay between these two. We perform a carefully controlled study of the impact of model compression via quantization and knowledge distillation on measures of social bias in LLMs. Longer pretraining and larger models led to higher social bias, and quantization showed a regularizer effect with its best trade-off around 20% of the original pretraining time.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 2663–2675\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nUnderstanding the Effect of Model Compression on\nSocial Bias in Large Language Models\nGustavo Gonçalves1,2 and Emma Strubell1,3\n1Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA\n2NOV A LINCS, Universidade NOV A de Lisboa, Lisbon, Portugal\n3Allen Institute for Artificial Intelligence, Seattle, W A, USA\n{ggoncalv, estrubel}@cs.cmu.edu\nAbstract\nLarge Language Models (LLMs) trained with\nself-supervision on vast corpora of web text\nfit to the social biases of that text. Without\nintervention, these social biases persist in the\nmodel’s predictions in downstream tasks, lead-\ning to representational harm. Many strategies\nhave been proposed to mitigate the effects of\ninappropriate social biases learned during pre-\ntraining. Simultaneously, methods for model\ncompression have become increasingly pop-\nular to reduce the computational burden of\nLLMs. Despite the popularity and need for\nboth approaches, little work has been done to\nexplore the interplay between these two. We\nperform a carefully controlled study of the im-\npact of model compression via quantization and\nknowledge distillation on measures of social\nbias in LLMs. Longer pretraining and larger\nmodels led to higher social bias, and quantiza-\ntion showed a regularizer effect with its best\ntrade-off around 20% of the original pretraining\ntime. 1\n1 Introduction\nLarge Language Models (LLMs) are trained on\nlarge corpora using self-supervision, which allows\nmodels to consider vast amounts of unlabelled\ndata, and learn language patterns through mask-\ning tasks (Devlin et al., 2019; Radford et al., 2019).\nHowever, self-supervision allows LLMs to pick\nup social biases contained in the training data.\nWhich is amplified by larger models, more data,\nand longer training (Kaneko et al., 2022; Kaneko\nand Bollegala, 2022; Kurita et al., 2019; Delobelle\nand Berendt, 2022).\nSocial biases in LLMs are an ongoing prob-\nlem that is propagated from pretraining to finetun-\ning (Ladhak et al., 2023; Gira et al., 2022). Biased\npretrained models are hard to fix, as retraining is\n1https://github.com/gsgoncalves/\nEMNLP2023_llm_compression_and_social_\nbias\nprohibitively expensive both financially and envi-\nronmentally (Hessenthaler et al., 2022). At the\nsame time, the compression of LLMs has been\nintensely studied. Pruning, quantization, and dis-\ntillation are among the most common strategies to\ncompress LLMs. Pruning reduces the parameters\nof a trained model by removing redundant con-\nnections while preserving equivalent performance\nto their original counterparts (Liebenwein et al.,\n2021; Ahia et al., 2021). Quantization reduces\nthe precision of model weights and activations to\nimprove efficiency while preserving performance\n(Ahmadian et al., 2023). Finally, knowledge distil-\nlation (Hinton et al., 2015) trains a smaller more\nefficient model based on a larger pre-trained model.\nWhile much research has been done on mea-\nsuring and mitigating social bias in LLMs, and\nmaking LLMs smaller and more efficient, by using\none or a combination of many compression meth-\nods (Xu et al., 2021), little research has been done\nregarding the interplay between social biases and\nLLM compression. Existing work has shown that\npruning disproportionately impacts classification\naccuracy on low-frequency categories in computer\nvision models (Hooker et al., 2021), but that prun-\ning transformer models can have a beneficial effect\nwith respect to bias when modeling multilingual\ntext (Hooker et al., 2020; Ogueji et al., 2022). Fur-\nther, Xu and Hu (2022) have shown that compress-\ning pretrained models improves model fairness by\nworking as a regularizer against toxicity.\nUnlike previous work, our work focuses on the\nimpacts of widely used quantization and distilla-\ntion on the social biases exhibited by a variety of\nboth encoder- and decoder-only LLMs. We fo-\ncus on the effects of social bias over BERT (Devlin\net al., 2019), RoBERTa (Liu et al., 2019) and Pythia\nLLMs (Biderman et al., 2023). We evaluate these\nmodels against Bias Bench (Meade et al., 2022), a\ncompilation of three social bias datasets.\nIn our experimental results we demonstrate a cor-\n2663\nrelation between longer pretraining, larger models,\nand increased social bias, and show that quantiza-\ntion and distillation can reduce bias, demonstrating\nthe potential for compression as a pragmatic ap-\nproach for reducing social bias in LLMs.\n2 Methodology\nWe were interested in understanding how dynamic\nPost-Training Quantization (PTQ) and distillation\ninfluence social bias contained in LLMs of different\nsizes, and along their pretraining. In dynamic PTQ,\nfull-precision floating point model weights are stat-\nically mapped to lower precisions after training,\nwith activations dynamically mapped from high\nto low precision during inference. To this end, in\nSection 2.1 we present the datasets of the Bias\nBench benchmark (Meade et al., 2022) that enable\nus to evaluate three different language modeling\ntasks across the three social bias categories. In\nSection 2.2 we lay out the models we studied. We\nexpand on the Bias Bench original evaluation by\nlooking at the Large versions of the BERT and\nRoBERTa models, and the Pythia family of au-\ntoregressive models. The chosen models cover\ndifferent language modeling tasks and span across\na wide range of parameter sizes, thus providing\na comprehensive view of the variations of social\nbias.\n2.1 Measuring Bias\nWe use the Bias Bench benchmark for evaluating\nmarkers of social bias in LLMs. Bias Bench com-\npiles three datasets, CrowS-Pairs (Nangia et al.,\n2020), StereoSet (SS) (Nadeem et al., 2021), and\nSEAT (Kaneko and Bollegala, 2021), for measur-\ning intrinsic bias across three different identity cate-\ngories: GENDER , RACE , and RELIGION . While the\nset of identities covered by this dataset is far from\ncomplete, it serves as a useful indicator as these\nmodels are encoding common social biases; how-\never, the lack of bias indicated by this benchmark\ndoes not imply an overall lack of inappropriate bias\nin the model, for example with respect to other\ngroups. We briefly describe each dataset below;\nrefer to the original works for more detail.\nCrowS-Pairs is composed of pairs of minimally\ndistant sentences that have been crowdsourced. A\nminimally distant sentence is defined as a small\nnumber of token swaps in a sentence, that carry\ndifferent social bias interpretations. An unbiased\nmodel will pick an equal ratio of both stereotypi-\ncal and anti-stereotypical choices, thus an optimal\nscore for this dataset is a ratio of 50%.\nStereoSet is composed of crowdsourced samples.\nEach sample is composed of a masked context\nsentence, and a set of three candidate answers:\n1) stereotypical, 2) anti-stereotypical, and 3) un-\nrelated. Under the SS formulation, an unbiased\nmodel would give a balanced number of classifica-\ntions of types 1) and 2), thus the optimal score is\nalso 50%. The SS dataset also measures if we are\nchanging the language modeling properties of our\nmodel. That is, if our model picks a high percent-\nage of unrelated choices 3) it can be interpreted as\nlosing its language capabilities. This is defined as\nthe Language Model (LM) Score.\nSEAT evaluates biases in sentences. A SEAT\ntask is defined by two sets of attribute sentences,\nand two other sets of target sentences. The objec-\ntive of the task is to measure the distance of the\nsentence embeddings between the attribute and tar-\nget sets to assess a preference between attributes\nand targets (bias). We provide more detail of this\nformulation in Appendix A.1.\n2.2 Models\nIn this work, we focus on two popular methods\nfor model compression: knowledge distillation and\nquantization. We choose these two methods given\ntheir competitive performance, wide deployment\ngiven the availability of distributions under the Hug-\ngingFace and Pytorch libraries, and the lack of\nunderstanding of the impact of these methods on\nsocial biases. We leave the study of more elaborate\nmethods for improving model efficiency such as\npruning (Chen et al., 2020), mixtures of experts\n(Kudugunta et al., 2021), and adaptive computation\n(Elbayad et al., 2020) to future work.\nSince model compression affects model size, we\nare particularly interested in understanding how\npretrained model size impacts measures of social\nbias, and how that changes as a function of how\nwell the model fits the data. We are also inter-\nested in investigating how the number of tokens\nobserved during training impacts all of the above.\nWe experiment with three different base LLMs:\nBERT (Devlin et al., 2019), RoBERTa (Liu et al.,\n2019), and Pythia (Biderman et al., 2023), with\nuncompressed model sizes ranging from 70M pa-\nrameters to 6.9B parameters. BERT and RoBERTa\n2664\nModel Params Size (MB) GENDER RACE RELIGION\nBERT Base 110M 438 57.25 62.33 62.86\n+ DYNAMIC PTQ int8 110M 181 57.25 ↓0.19 62.14 ↓9.53 46.67\n+ CDA (Webster et al., 2020) 110M ↓1.14 56.11 ↓5.63 56.70 ↓2.86 60.00\n+ DROPOUT (Webster et al., 2020) 110M ↓1.91 55.34 ↓3.30 59.03 ↓7.62 55.24\n+ INLP (Ravfogel et al., 2020) 110M ↓6.10 51.15 ↑5.63 67.96 ↓1.91 60.95\n+ SELF -DEBIAS (Schick et al., 2021) 110M ↓4.96 52.29 ↓5.63 56.70 ↓6.67 56.19\n+ SENT DEBIAS (Liang et al., 2020) 110M ↓4.96 52.29 ↑0.39 62.72 ↑0.95 63.81\nBERT Large 345M 1341 ↓1.52 55.73 ↓1.94 60.39 ↑4.76 67.62\n+ DYNAMIC PTQ int8 345M 432 ↓6.87 50.38 ↑0.78 63.11 ↓7.62 55.24\nDistilBERT 66M 268 ↓6.10 51.15 ↓9.32 46.99 ↓4.76 58.10\nRoBERTa Base 123M 498 60.15 63.57 60.95\n+ DYNAMIC PTQ int8 123M 242 ↓6.51 53.64 ↓5.04 58.53 ↓10.47 49.52\n+ CDA (Webster et al., 2020) 110M ↓3.83 56.32 ↑0.19 63.76 ↓0.95 59.05\n+ DROPOUT (Webster et al., 2020) 110M ↓0.76 59.39 ↓1.17 62.40 ↓2.86 57.14\n+ INLP (Ravfogel et al., 2020) 110M ↓4.98 55.17 ↓1.75 61.82 ↑1.91 62.86\n+ SELF -DEBIAS (Schick et al., 2021) 110M ↓3.06 57.09 ↓1.17 62.40 ↓9.52 51.43\n+ SENT DEBIAS (Liang et al., 2020) 110M ↓8.04 52.11 ↑1.55 65.12 ↓1.9 40.95\nRoBERTa Large 354M 1422 60.15 ↑0.58 64.15 ↑0.95 61.90\n+ DYNAMIC PTQ int8 354M 513 ↓2.68 57.47 ↓0.20 63.37 ↓0.95 60.00\nDistilRoBERTa 82M 329 ↓7.28 52.87 ↓3.49 60.08 ↑2.86 63.81\nTable 1: CrowS-Pairs stereotype scores for GENDER , RACE , and RELIGION for BERT and RoBERTa models.\nStereotype scores closer to 50% indicate less biased model behavior. Bold values indicate the best method per bias\ncategory. Results on the other datasets displayed similar trends and were included in Appendix B for space.\nrepresent two similar sets of widely used and stud-\nied pretrained architectures, trained on different\ndata with a small overlap. RoBERTa pretraining\nwas done over 161 GB of text, which contained the\n16GB used to train BERT, approximately a ten-fold\nincrease. RoBERTa also trained for longer, with\nlarger batch sizes which have shown to decrease\nthe perplexity of the LLM (Liu et al., 2019).\nThe set of checkpoints released for the Pythia\nmodel family allows us to assess an even wider\nvariety of model sizes and number of training to-\nkens, including intermediate checkpoints saved dur-\ning pretraining, so that we can observe how bias\nvaries throughout pretraining. We used the mod-\nels pretrained on the deduplicated version of The\nPile (Gao et al., 2021) containing 768GB of text.\nKnowledge distillation (Hinton et al., 2015) is a\npopular technique for compressing the knowledge\nencoded in a larger teacher model into a smaller\nstudent model. In this work, we analyze Distil-\nBERT (Sanh et al., 2019) and DistilRoBERTa 2\ndistilled LMs. During training the student model\nminimizes the loss according to the predictions of\n2https://huggingface.co/distilroberta-base\nthe teacher model (soft-targets) and the true labels\n(hard-targets) to better generalize to unseen data.\nQuantization compresses models by reducing\nthe precision of their weights and activations during\ninference. We use the standard PyTorch implemen-\ntation3 to apply dynamic PTQ over the linear layers\nof the transformer stack, from fp32 full-precision\nto quantized int8 precision. This work analyzes\nquantized BERT, RoBERTa, and Pythia models of\na comprehensive range of sizes.\n3 Results\nDynamic PTQ and distillation lower social bias.\nIn Table 1 we analyze the effects of dynamic PTQ\nand distillation in the CrowS dataset, where BERT\nBase and RoBERTa Base are our baselines. To\ncompare quantization and distillation, we add three\ndebiasing baselines also referenced by Meade et al.\n(2022) that are competitive strategies to reduce bias.\nThe INLP (Ravfogel et al., 2020) baseline consists\nof a linear classifier that learns to predict the target\nbias group given a set of context words, such as\n3https://pytorch.org/tutorials/recipes/recipes/\ndynamic_quantization.html\n2665\nFigure 1: LM score vs. GENDER , RACE , and RELIGION bias on the SS dataset across all Pythia models. Darker\ndata points show later pretraining steps, and more transparent points to earlier steps. The included table shows the\nKendall Tau C, for the correlation across \"All\" model sizes, full-precision \"Original\", and \"int8\" model sizes.\nModel\nSize\nBest\nLM Score\nStep\nNr.\nBias\nG. / RA. / RE.\n70M 89.2 21K 59.8 / 58.4 / 58.6\n160M 90.2 36K 61.4 / 57.6 / 59.4\n410M 91.6 114K 65.2 / 60.7 / 64.5\n1.4B 92.6 129K 66.6 / 63.2 / 66.2\n2.8B 92.9 114K 67.1 / 63.7 / 66.8\n6.9B 92.7 129K 69.0 / 64.0 / 68.4\nTable 2: Bias measured using SS for the full-precision\nPythia models having the best LM score per model size.\nModel\nSize\nBest\nLM Score\nStep\nNr.\nBias\nG. / RA. / RE.\n70M 87.7 29K 57.5 / 54.8 / 58.0\n160M 89.0 21K 61.1 / 56.3 / 57.7\n410M 90.5 50K 64.2 / 58.4 / 63.6\n1.4B 91.4 29K 66.1 / 59.7 / 63.3\n2.8B 91.6 50K 64.1 / 60.2 / 61.9\n6.9B 91.4 21K 67.3 / 60.1 / 67.3\nTable 3: Bias measured using SS for int8 quantized\nPythia models having the best LM score per model size.\n’he/she’. The Self-Debias baseline was proposed by\nSchick et al. (2021), and uses prompts to encourage\nmodels to generate toxic text and learns to give less\nweight to the generate toxic tokens. Self-Debias\ndoes not change the model’s internal representation,\nthus it cannot be evaluated on the SEAT dataset.\nNotable trends in Table 1 are the reduction of\nsocial biases when applying dynamic PTQ and dis-\ntillation, which can compete on average with the\nspecifically designed debias methods. Additional\nresults in in Appendix B also display similar trends.\nOn the SS dataset in Table 4 we are also able to\nobserve that the application of distillation provides\nremarkable decreases in social biases, at the great\nexpense of LM score. However, dynamic PTQ\nshows a better trade-off in providing social bias\nreductions, while preserving LM score.\nOne model size does not fit all social biases. In\nTable 1 and the equivalent Tables in Appendix B\nwe can see that social bias categories respond dif-\nferently to model size, across the different datasets.\nWhile BERT Base/Large outperforms RoBERTa in\nGENDER , the best model for RACE and RELIGION\nvaries across datasets. This can be explained by the\ndifferent dataset tasks and the pretraining.\nIn Appendix B we show the social bias scores as\na function of the pretraining of the Pythia models in\nFigures 2 to 7, 9, 10 and 11. The BERT/RoBERTa\nBase and Large versions are roughly comparable\nwith the 160M and 410M Pythia models. For the\nSS dataset, the 160M model is consistently less\nbiased than the 410M model. However, this is\nnot the case for the other two datasets where the\n160M struggles in the RACE category while assess-\ning the distance of sentence embeddings (SEAT);\nand in the RELIGION category while swapping min-\nimally distant pairs (CrowS). This illustrates the\ndifficulty of distinguishing between semantically\nclose words, and shows the need for larger models\npretrained for longer and on more data.\nLonger pretraining and larger models lead to\nmore socially biased models. We study the ef-\nfects of longer pretraining and larger models on\nsocial bias, by establishing the correlation of these\nvariables in Figure 1. Here we can observe that\nas the model size increases so does the LM model\nscore and social bias across the SS dataset. More-\nover, later stages of pretraining have a higher LM\nmodel score, where the social bias score tends to\nbe high. The application of dynamic PTQ shows\na regularizer effect on all models.The Kendall Tau\nC across the models and categories shows a strong\n2666\ncorrelation between LM score and social bias. Sta-\ntistical significant tests were performed using a\none-sided t-test to evaluate the positive correlation.\nTables 2 and 3 show at what step, out of the\n21 we tested, the best LM scores occur on the SS\ndataset. In Table 2 the best LM score increases\nmonotonically with model size and so do the social\nbiases. Interestingly, as the model size increases\nthe best LM score appears after around 80% of\nthe pretraining. In opposition, in Table 3, with\ndynamic PTQ the best LM score occurs around\n20% of the pretraining and maintains the trend of\nhigher LM score and social bias, albeit at lower\nscores than the original models. This shows an\ninteresting possibility of early stopping depending\non the deployment task of the LLM.\n4 Limitations\nWhile this work provides three different datasets,\nwhich have different views on social bias and allow\nfor an indicative view of LLMs, they share some\nlimitations that should be considered. The datasets\nSS and CrowS define an unbiased model as one\nthat makes an equal amount of stereotypical and\nanti-stereotypical choices. While we agree that this\nmakes a good definition of an impartial model it is\na limited definition of an unbiased model. This has\nalso been noted by Blodgett et al. (2021), showing\nthat CrowS is slightly more robust than SS by tak-\ning \"extra steps to control for varying base rates be-\ntween groups.\" (Blodgett et al., 2021). We should\nconsider that these datasets depict mostly Western\nbiases, and the dataset construction since it is based\non assessors it is dependent on the assessor’s views.\nMoreover, Blodgett et al. (2021) has also noted\nthe existence of unbalanced stereotype pairs in SS\nand CrowS, and the fact that some samples in the\ndataset are not consensual stereotypes.\nAll datasets only explore three groups of biases:\nGENDER , RACE , and RELIGION , which are not by\nany means exhaustive representations of social bias.\nThe experiments in this paper should be considered\nindicative of social bias and need to be further stud-\nied. Additionally, the GENDER category is defined\nas binary, which we acknowledge that does not\nreflect the timely social needs of LLMs, but can\nbe extended to include non-binary examples by\nimproving on existing datasets.\nWe benefited from access to a cluster with two\nAMD EPYC 7 662 64-Core Processors, where\nthe quantized experiments ran for approximately 4\ndays. A CPU implementation was used given the\nquantization backends available in PyTorch. Exper-\niments that did not require quantization ran using\nan NVIDIA A100 40GB GPU and took approxi-\nmately 5 hours to run.\nEthics Statement\nWe reiterate that this work provides a limited West-\nern view of Social bias focusing only on three main\ncategories: GENDER , RACE , and RELIGION . Our\nwork is further limited to a binary definition of\nGENDER , which we acknowledge that does not re-\nflect the current society’s needs. Moreover, we\nmust also reiterate that these models need to be fur-\nther studied and are not ready for production. The\neffects of quantization along pretraining should be\nconsidered as preliminary results.\n5 Acknowledgments\nThis work has been partially funded by the FCT\nproject NOV A LINCS Ref. UIDP/04516/2020,\nby the Amazon Science - TaskBot Prize Chal-\nlenge and the CMU|Portugal projects iFetch\nRef. LISBOA-01-0247-FEDER-045920 and\nGoLocal Ref. CMUP-ERI/TIC/0046/2014, and\nby the FCT Ph.D. scholarship grant Ref.\nSFRH/BD/140924/2018. We would like to ac-\nknowledge the NOV ASearch group for providing\ncompute resources for this work. Any opinions,\nfindings, and conclusions in this paper are the au-\nthors’ and do not necessarily reflect those of the\nsponsors.\nReferences\nOrevaoghene Ahia, Julia Kreutzer, and Sara Hooker.\n2021. The Low-Resource Double Bind: An Empir-\nical Study of Pruning for Low-Resource Machine\nTranslation. In EMNLP (Findings), pages 3316–\n3333. Association for Computational Linguistics.\nArash Ahmadian, Saurabh Dash, Hongyu Chen, Bharat\nVenkitesh, Stephen Gou, Phil Blunsom, Ahmet\nÜstün, and Sara Hooker. 2023. Intriguing Properties\nof Quantization at Scale. CoRR, abs/2305.19268.\nStella Biderman, Hailey Schoelkopf, Quentin Anthony,\nHerbie Bradley, Kyle O’Brien, Eric Hallahan, Mo-\nhammad Aflah Khan, Shivanshu Purohit, USVSN Sai\nPrashanth, Edward Raff, Aviya Skowron, Lintang\nSutawika, and Oskar van der Wal. 2023. Pythia: A\nSuite for Analyzing Large Language Models Across\nTraining and Scaling. CoRR, abs/2304.01373.\n2667\nSu Lin Blodgett, Gilsinia Lopez, Alexandra Olteanu,\nRobert Sim, and Hanna M. Wallach. 2021. Stereo-\ntyping Norwegian Salmon: An Inventory of Pitfalls\nin Fairness Benchmark Datasets. In ACL/IJCNLP\n(1), pages 1004–1015. Association for Computational\nLinguistics.\nTianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia\nLiu, Yang Zhang, Zhangyang Wang, and Michael\nCarbin. 2020. The Lottery Ticket Hypothesis for\nPre-trained BERT Networks. In NeurIPS.\nPieter Delobelle and Bettina Berendt. 2022. FairDistil-\nlation: Mitigating Stereotyping in Language Models.\nIn ECML/PKDD (2), volume 13714 of Lecture Notes\nin Computer Science, pages 638–654. Springer.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers),\npages 4171–4186. Association for Computational\nLinguistics.\nMaha Elbayad, Jiatao Gu, Edouard Grave, and Michael\nAuli. 2020. Depth-Adaptive Transformer. In ICLR.\nOpenReview.net.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn\nPresser, and Connor Leahy. 2021. The Pile: An\n800GB Dataset of Diverse Text for Language Model-\ning. CoRR, abs/2101.00027.\nMichael Gira, Ruisu Zhang, and Kangwook Lee. 2022.\nDebiasing Pre-Trained Language Models via Effi-\ncient Fine-Tuning. In LT-EDI, pages 59–69. Associa-\ntion for Computational Linguistics.\nMarius Hessenthaler, Emma Strubell, Dirk Hovy, and\nAnne Lauscher. 2022. Bridging Fairness and Envi-\nronmental Sustainability in Natural Language Pro-\ncessing. In EMNLP, pages 7817–7836. Association\nfor Computational Linguistics.\nGeoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean.\n2015. Distilling the knowledge in a neural network.\nIn NIPS Workshop on Deep Learning.\nSara Hooker, Aaron Courville, Gregory Clark, Yann\nDauphin, and Andrea Frome. 2021. What Do Com-\npressed Deep Neural Networks Forget?\nSara Hooker, Nyalleng Moorosi, Gregory Clark, Samy\nBengio, and Emily Denton. 2020. Characterising\nBias in Compressed Models. CoRR, abs/2010.03058.\nMasahiro Kaneko and Danushka Bollegala. 2021. De-\nbiasing Pre-trained Contextualised Embeddings. In\nEACL, pages 1256–1266. Association for Computa-\ntional Linguistics.\nMasahiro Kaneko and Danushka Bollegala. 2022. Un-\nmasking the Mask - Evaluating Social Biases in\nMasked Language Models. In AAAI, pages 11954–\n11962. AAAI Press.\nMasahiro Kaneko, Danushka Bollegala, and Naoaki\nOkazaki. 2022. Debiasing Isn’t Enough! - on the\nEffectiveness of Debiasing MLMs and Their Social\nBiases in Downstream Tasks. In COLING, pages\n1299–1310. International Committee on Computa-\ntional Linguistics.\nSneha Kudugunta, Yanping Huang, Ankur Bapna,\nMaxim Krikun, Dmitry Lepikhin, Minh-Thang Lu-\nong, and Orhan Firat. 2021. Beyond Distillation:\nTask-level Mixture-of-Experts for Efficient Inference.\nIn EMNLP (Findings), pages 3577–3599. Associa-\ntion for Computational Linguistics.\nKeita Kurita, Nidhi Vyas, Ayush Pareek, Alan W.\nBlack, and Yulia Tsvetkov. 2019. Measuring Bias\nin Contextualized Word Representations. CoRR,\nabs/1906.07337.\nFaisal Ladhak, Esin Durmus, Mirac Suzgun, Tianyi\nZhang, Dan Jurafsky, Kathleen R. McKeown, and\nTatsunori Hashimoto. 2023. When Do Pre-Training\nBiases Propagate to Downstream Tasks? A Case\nStudy in Text Summarization. In EACL, pages 3198–\n3211. Association for Computational Linguistics.\nPaul Pu Liang, Irene Mengze Li, Emily Zheng,\nYao Chong Lim, Ruslan Salakhutdinov, and Louis-\nPhilippe Morency. 2020. Towards Debiasing Sen-\ntence Representations. In ACL, pages 5502–5515.\nAssociation for Computational Linguistics.\nLucas Liebenwein, Cenk Baykal, Brandon Carter, David\nGifford, and Daniela Rus. 2021. Lost in Pruning:\nThe Effects of Pruning Neural Networks beyond Test\nAccuracy. In MLSys. mlsys.org.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach. CoRR, abs/1907.11692.\nNicholas Meade, Elinor Poole-Dayan, and Siva Reddy.\n2022. An Empirical Survey of the Effectiveness of\nDebiasing Techniques for Pre-trained Language Mod-\nels. In ACL (1), pages 1878–1898. Association for\nComputational Linguistics.\nMoin Nadeem, Anna Bethke, and Siva Reddy. 2021.\nStereoSet: Measuring stereotypical bias in pretrained\nlanguage models. In ACL/IJCNLP (1), pages 5356–\n5371. Association for Computational Linguistics.\nNikita Nangia, Clara Vania, Rasika Bhalerao, and\nSamuel R. Bowman. 2020. CrowS-Pairs: A Chal-\nlenge Dataset for Measuring Social Biases in Masked\nLanguage Models. In EMNLP (1), pages 1953–1967.\nAssociation for Computational Linguistics.\n2668\nKelechi Ogueji, Orevaoghene Ahia, Gbemileke Onilude,\nSebastian Gehrmann, Sara Hooker, and Julia\nKreutzer. 2022. Intriguing Properties of Compres-\nsion on Multilingual Models. In EMNLP, pages\n9092–9110. Association for Computational Linguis-\ntics.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nShauli Ravfogel, Yanai Elazar, Hila Gonen, Michael\nTwiton, and Yoav Goldberg. 2020. Null It Out:\nGuarding Protected Attributes by Iterative Nullspace\nProjection. In ACL, pages 7237–7256. Association\nfor Computational Linguistics.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. DistilBERT, a distilled version\nof BERT: Smaller, faster, cheaper and lighter. CoRR,\nabs/1910.01108.\nTimo Schick, Sahana Udupa, and Hinrich Schütze. 2021.\nSelf-Diagnosis and Self-Debiasing: A Proposal for\nReducing Corpus-Based Bias in NLP. Trans. Assoc.\nComput. Linguistics, 9:1408–1424.\nKellie Webster, Xuezhi Wang, Ian Tenney, Alex Beu-\ntel, Emily Pitler, Ellie Pavlick, Jilin Chen, and\nSlav Petrov. 2020. Measuring and Reducing Gen-\ndered Correlations in Pre-trained Models. CoRR,\nabs/2010.06032.\nCanwen Xu, Wangchunshu Zhou, Tao Ge, Ke Xu, Ju-\nlian J. McAuley, and Furu Wei. 2021. Beyond Pre-\nserved Accuracy: Evaluating Loyalty and Robustness\nof BERT Compression. In EMNLP (1), pages 10653–\n10659. Association for Computational Linguistics.\nGuangxuan Xu and Qingyuan Hu. 2022. Can\nModel Compression Improve NLP Fairness. CoRR,\nabs/2201.08542.\nA Details of Metric Calculation\nA.1 SEAT\nThe SEAT task shares the same task as WEAT task,\nwhich is defined by four word sets, two attribute\nsets, and two target sets. For example, to decide\nthe presence of gender bias the two attribute sets\nare disjoint sets given by: 1) a masculine set of\nwords, such as {’man’, ’boy’, ’he’, ...}, and 2) a\nset of feminine words {’woman’, ’girl’, ’her’, ...}.\nThe target sets will characterize concepts such as\n’sports’ and ’culinary’.\nWEAT evaluates how close are the attribute sets\nfrom the target sets to determine the existence of\nbias. Mathematically this is given by:\ns(A,B,X,Y ) =\n∑\nx∈X\ns(x,A,B )−\n∑\ny∈Y\ns(y,A,B )\n(1)\nWhere Aand Brepresent the attribute sets, and\nX and Y are the target sets of words. The sfunc-\ntion in Equation (1) denotes mean cosine similarity\nbetween the target word embeddings and the at-\ntribute word embeddings:\ns(w,A,B )= 1\n|A|\n∑\na∈A\ncos(w,a)−1\n|B|\n∑\nb∈B\ncos(w,b).\n(2)\nThe reported score of the benchmark (effect size)\nis given by:\nd= µ({s(x,A,B )}x∈X) −µ({s(y,A,B )}y∈Y )\nσ({s(t,X,Y )}t∈A∪B)\n(3)\nWhere µand σ are the mean and standard de-\nviation respectively. Equation (3) is designed so\nthat scores closer to zero indicate the smallest pos-\nsible degree of bias. SEAT extends the previous\nformulation by considering the distance sentence\nembeddings instead of word embeddings.\nB Additional Plots and Tables\n2669\nFigure 2: Crows GENDER bias with Quantized Results\nFigure 3: Crows RACE bias with Quantized Results\nFigure 4: Crows RELIGION bias with Quantized Results\n2670\nFigure 5: Stereoset GENDER bias with Quantized Results\nFigure 6: Stereoset RACE bias with Quantized Results\nFigure 7: Stereoset RELIGION bias with Quantized Results\n2671\nFigure 8: Stereoset LM Score with Quantized Results\nTable 4: SS stereotype scores and language modeling scores (LM Score) for BERT, and RoBERTa models.\nStereotype scores closer to 50% indicate less biased model behavior. Bold values indicate the best method per\nbias and LM Score. Results are on the SS test set. A random model (which chooses the stereotypical candidate\nand the anti-stereotypical candidate for each example with equal probability) obtains a stereotype score of 50% in\nexpectation.\nModel GENDER bias RACE bias RELIGION bias LM Score\nBERT Base 60.28 57.03 59.70 84.17\n+ DYNAMIC PTQ int8 ↓3.29 56.99 ↓2.36 54.67 ↓2.87 56.83 ↓2.94 81.23\n+ CDA (Webster et al., 2020) ↓0.67 59.61 ↓0.30 56.73 ↓1.33 58.37 ↓1.09 83.08\n+ DROPOUT (Webster et al., 2020) ↑0.38 60.66 ↑0.04 57.07 ↓0.57 59.13 ↓1.14 83.04\n+ INLP (Ravfogel et al., 2020) ↓3.03 57.25 ↑0.26 57.29 ↓2.44 57.26 ↓3.54 80.63\n+ SELF -DEBIAS (Schick et al., 2021) ↓0.94 59.34 ↓2.73 54.30 ↓2.44 57.26 ↓0.08 84.09\n+ SENTENCE DEBIAS (Liang et al., 2020) ↓0.91 59.37 ↑0.75 57.78 ↓0.97 58.73 ↑0.03 84.20\nBERT Large ↑2.96 63.24 ↑0.04 57.07 ↑0.24 59.94 ↑0.24 84.41\n+ DYNAMIC PTQ int8 ↓0.82 59.46 ↓1.86 55.17 ↓3.74 55.96 ↓3.12 81.05\nDistil BERT Base ↓8.73 51.55 ↓6.40 50.63 ↓9.57 49.87 ↓30.30 53.87\nRoBERTa Base 66.32 61.67 64.28 88.95\n+ DYNAMIC PTQ int8 ↓3.92 62.40 ↓3.15 58.52 ↓0.03 64.25 ↓5.75 83.20\n+ CDA (Webster et al., 2020) ↓1.89 64.43 ↓0.73 60.95 ↓0.23 64.51 ↓0.10 83.83\n+ DROPOUT (Webster et al., 2020) ↓0.06 66.26 ↓1.27 60.41 ↓2.20 62.08 ↓0.11 88.81\n+ INLP (Ravfogel et al., 2020) ↓9.06 60.82 ↓3.41 58.26 ↓3.94 60.34 ↓0.70 88.23\n+ SELF -DEBIAS (Schick et al., 2021) ↓1.28 65.04 ↓2.89 58.78 ↓1.44 62.84 ↓0.67 88.26\n+ SENTENCE DEBIAS (Liang et al., 2020) ↓3.55 62.77 ↑1.05 62.72 ↓0.37 63.91 ↑0.01 88.94\nRoBERTa Large ↑0.51 66.83 ↓1.37 60.30 ↑0.21 64.49 ↑0.14 89.09\n+ DYNAMIC PTQ int8 ↓2.72 63.60 ↓2.10 59.57 ↓0.40 63.88 ↓0.68 88.27\nDistil RoBERTa Base ↓2.04 64.28 ↓0.36 61.31 ↑1.16 65.44 ↑0.24 89.19\n2672\nTable 5: LM Scores vs. Biases on the SS dataset of the\nint8 models, at the same steps with the best LM Score\nfor the original (full-precision) models (Table 2)\n.\nModel\nSize LM Score\nStep\nNr.\nBias\nG. / RA. / RE.\n70M 87.7 21K 55.4 / 56.8 / 58.8\n160M 88.3 36K 59.4 / 54.7 / 57.3\n410M 88.7 114K 63.3 / 57.8 / 60.9\n1.4B 90.1 129K 65.5 / 60.0 / 62.5\n2.8B 90.5 114K 64.3 / 58.3 / 62.0\n6.9B 90.5 129K 66.6 / 62.2 / 64.7\nTable 6: LM Scores vs. Biases on the SS dataset of the\noriginal (full-precision) models, at the same steps with\nthe best LM Score for the int8 models (Table 3)\n.\nModel\nSize LM Score\nStep\nNr.\nBias\nG. / RA. / RE.\n70M 88.4 29K 58.9 / 55.4 / 58.0\n160M 89.8 21K 62.7 / 57.7 / 57.0\n410M 91.5 50K 67.2 / 60.5 / 63.3\n1.4B 91.8 29K 65.9 / 61.2 / 64.9\n2.8B 92.4 50K 65.3 / 63.5 / 63.8\n6.9B 92.2 21K 67.0 / 61.0 / 64.9\nFigure 9: Seat GENDER bias with Quantized Results\nFigure 10: Seat RACE bias with Quantized Results\n2673\nFigure 11: Seat RELIGION bias with Quantized Results\nTable 7: GENDER bias on SEAT dataset. Effect sizes closer to 0 are indicative of less biased model representations.\nBold values indicate the best method per test. Statistically significant effect sizes at p < 0.01 are denoted by *. The\nfinal column reports the average absolute effect size across all six gender SEAT tests for each model.\nModel weat6 weat6b weat7 weat7b weat8 weat8b Avg. Effect\nBERT Base 0.931 ∗ 0.090 -0.124 0.937 ∗ 0.783 ∗ 0.858 ∗ 0.620\n+ DYNAMIC PTQ int8 0.614 ∗ 0.000 -0.496 0.711 ∗ 0.401 0.549 ∗ ↓0.158 0.462\n+ CDA 0.846 ∗ 0.186 -0.278 1.342 ∗ 0.831 ∗ 0.849 ∗ ↑0.102 0.722\n+ DROPOUT 1.136 ∗ 0.317 0.138 1.179 ∗ 0.879 ∗ 0.939 ∗ ↑0.144 0.765\n+ INLP 0.317 -0.354 -0.258 0.105 0.187 -0.004 ↓0.416 0.204\n+ SENTENCE DEBIAS 0.350 -0.298 -0.626 0.458 ∗ 0.413 0.462 ∗ ↓0.186 0.434\nBERT Large 0.370 -0.015 0.418 ∗ 0.221 -0.259 0.710 ∗ ↓0.288 0.332\n+ DYNAMIC PTQ int8 0.905 ∗ 0.273 1.097 ∗ 0.894 ∗ 0.728 ∗ 1.180 ∗ ↑0.226 0.846\nDistil BERT 0.061 -0.222 0.093 -0.120 0.222 0.112 ↓0.482 0.138\nRoBERTa Base 0.922 ∗ 0.208 0.979 ∗ 1.460 ∗ 0.810 ∗ 1.261 ∗ 0.940\n+ DYNAMIC PTQ int8 0.350 0.177 0.389 ∗ 1.038 ∗ 0.349 0.897 ∗ ↓0.406 0.533\n+ CDA 0.976 ∗ 0.013 0.848 ∗ 1.288 ∗ 0.994 ∗ 1.160 ∗ ↓0.060 0.880\n+ DROPOUT 1.134 ∗ 0.209 1.161 ∗ 1.482 ∗ 1.136 ∗ 1.321 ∗ ↑0.134 1.074\n+ INLP 0.812 ∗ 0.059 0.604 ∗ 1.407 ∗ 0.812 ∗ 1.246 ∗ ↓0.117 0.823\n+ SENTENCE DEBIAS 0.755 ∗ 0.068 0.869 ∗ 1.372 ∗ 0.774 ∗ 1.239 ∗ ↓0.094 0.846\nRoBERTa large 0.849 ∗ 0.170 -0.237 0.900 ∗ 0.510 ∗ 1.102 ∗ ↓0.312 0.628\n+ DYNAMIC PTQ int8 0.446 ∗ 0.218 -0.368 0.423 ∗ -0.040 0.303 ↓0.640 0.300\nDistil RoBERTa 1.229 ∗ 0.192 0.859 ∗ 1.504 ∗ 0.748 ∗ 1.462 ∗ ↑0.059 0.999\n2674\nTable 8: RACE bias on SEAT dataset. ABWS: angry-black-woman-stereotype. Effect sizes closer to 0 are indicative\nof less biased model representations. Bold values indicate the best method per test. Statistically significant effect\nsizes at p < 0.01 are denoted by *. The final column reports the average absolute effect size across all seven race\nSEAT tests for each model.\nModel ABWS ABWS-b weat3 weat3b weat4 weat5 weat5b Avg.\nEffect\nBERT Base -0.079 0.690 ∗ 0.778 ∗ 0.469 ∗ 0.901 ∗ 0.887 ∗ 0.539 ∗ 0.620\n+ DYN. PTQ int8 0.772 ∗ 0.425 0.835 ∗ 0.548 ∗ 0.970 ∗ 1.076 ∗ 0.517 ∗ ↑0.115 0.735\n+ CDA 0.231 0.619 ∗ 0.824 ∗ 0.510 ∗ 0.896 ∗ 0.418 ∗ 0.486 ∗ ↓0.051 0.569\n+ DROPOUT 0.415 ∗ 0.690 ∗ 0.698 ∗ 0.476 ∗ 0.683 ∗ 0.417 ∗ 0.495 ∗ ↓0.067 0.554\n+ INLP 0.295 0.565 ∗ 0.799 ∗ 0.370 ∗ 0.976 ∗ 1.039 ∗ 0.432 ∗ ↑0.019 0.639\n+ SENTDEBIAS -0.067 0.684 ∗ 0.776 ∗ 0.451 ∗ 0.902 ∗ 0.891 ∗ 0.513 ∗ ↓0.008 0.612\nBERT Large -0.219 0.953 ∗ 0.420 ∗ -0.375 0.415 ∗ 0.890 ∗ -0.345 ↓0.104 0.517\n+ DYN. PTQ int8 0.660 ∗ -0.118 -0.173 0.093 -0.318 0.337 ∗ 0.364 ∗ ↓0.305 0.295\nDistil BERT 1.081 ∗ -0.927 0.441 ∗ 0.202 0.358 ∗ 0.726 ∗ -0.076 ↓0.076 0.544\nRoBERTa Base 0.395 ∗ 0.159 -0.114 -0.003 -0.315 0.780 ∗ 0.386 ∗ 0.307\n+ DYN. PTQ int8 0.660 ∗ -0.118 -0.173 0.093 -0.318 0.337 ∗ 0.364 ∗ ↓0.012 0.295\n+ CDA 0.455 ∗ 0.300 -0.080 0.024 -0.308 0.716 ∗ 0.371 ∗ ↑0.015 0.322\n+ DROPOUT 0.499 ∗ 0.392 -0.162 0.044 -0.367 0.841 ∗ 0.379 ∗ ↑0.076 0.383\n+ INLP 0.222 0.445 0.354 ∗ 0.130 0.125 0.636 ∗ 0.301 ∗ ↑0.009 0.316\n+ SENTDEBIAS 0.407 ∗ 0.084 -0.103 0.015 -0.300 0.728 ∗ 0.274 ∗ ↓0.034 0.273\nRoBERTa Large -0.090 0.274 0.869 ∗ -0.021 0.943 ∗ 0.767 ∗ 0.061 ↑0.125 0.432\n+ DYN. PTQ int8 -0.065 -0.014 0.587 ∗ -0.190 0.572 ∗ 0.580 ∗ -0.173 ↑0.004 0.312\nDistil RoBERTa 0.774 ∗ 0.112 -0.062 -0.012 -0.410 0.843 ∗ 0.456 ∗ ↑0.074 0.381\nTable 9: RELIGION bias on SEAT dataset. Effect sizes closer to 0 are indicative of less biased model representations.\nBold values indicate the best method per test. Statistically significant effect sizes at p < 0.01 are denoted by *. The\nfinal column reports the average absolute effect size across all four religion SEAT tests for each model.\nModel religion1 religion1b religion2 religion2b Avg. Abs. Effect.\nBERT Base 0.744 ∗ -0.067 1.009 ∗ -0.147 0.492\n+ DYNAMIC PTQ int8 0.524 ∗ -0.171 0.689 ∗ -0.205 ↓0.095 0.397\n+ CDA 0.355 -0.104 0.424 ∗ -0.474 ↓0.152 0.339\n+ DROPOUT 0.535 ∗ 0.109 0.436 ∗ -0.428 ↓0.115 0.377\n+ INLP 0.473 ∗ -0.301 0.787 ∗ -0.280 ↓0.031 0.460\n+ SENTENCE DEBIAS 0.728 ∗ 0.003 0.985 ∗ 0.038 ↓0.053 0.439\nBERT Large 0.011 0.144 -0.160 -0.426 ↓0.306 0.186\n+ DYNAMIC PTQ int8 0.524 ∗ -0.171 0.689 ∗ -0.205 ↓0.095 0.397\nDistil BERT 0.172 0.529 ∗ 0.318 0.076 ↓0.218 0.274\nRoBERTa Base 0.132 0.018 -0.191 -0.166 0.127\n+ DYNAMIC PTQ int8 0.527 ∗ 0.567 ∗ 0.079 0.020 ↑0.172 0.298\n+ CDA 0.341 0.148 -0.222 -0.269 ↑0.119 0.245\n+ DROPOUT 0.243 0.152 -0.115 -0.159 ↑0.041 0.167\n+ INLP -0.309 -0.347 -0.191 -0.135 ↑0.119 0.246\n+ SENTENCE DEBIAS 0.002 -0.088 -0.516 -0.477 ↑0.144 0.271\nRoBERTa Large -0.163 -0.685 -0.158 -0.542 ↑0.260 0.387\n+ DYNAMIC PTQ int8 0.117 -0.292 0.293 0.015 ↑0.052 0.179\nDistil RoBERTa 0.490 ∗ 0.019 0.291 -0.131 ↑0.106 0.232\n2675",
  "topic": "Popularity",
  "concepts": [
    {
      "name": "Popularity",
      "score": 0.6839068531990051
    },
    {
      "name": "Computer science",
      "score": 0.6131765842437744
    },
    {
      "name": "Quantization (signal processing)",
      "score": 0.5925744771957397
    },
    {
      "name": "Harm",
      "score": 0.5705412030220032
    },
    {
      "name": "Language model",
      "score": 0.532545268535614
    },
    {
      "name": "Cognitive psychology",
      "score": 0.428579717874527
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3349522054195404
    },
    {
      "name": "Psychology",
      "score": 0.3241834044456482
    },
    {
      "name": "Social psychology",
      "score": 0.17804434895515442
    },
    {
      "name": "Computer vision",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I83558840",
      "name": "Universidade Nova de Lisboa",
      "country": "PT"
    },
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210156221",
      "name": "Allen Institute for Artificial Intelligence",
      "country": "US"
    }
  ],
  "cited_by": 1
}