{
  "title": "The ambiguity of BERTology: what do large language models represent?",
  "url": "https://openalex.org/W4390236070",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5093585563",
      "name": "Tommi Buder-Gröndahl",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4223997076",
    "https://openalex.org/W1989194912",
    "https://openalex.org/W2906152891",
    "https://openalex.org/W4236810859",
    "https://openalex.org/W2335767140",
    "https://openalex.org/W4248035571",
    "https://openalex.org/W6633019681",
    "https://openalex.org/W4287854792",
    "https://openalex.org/W2889853714",
    "https://openalex.org/W4236198600",
    "https://openalex.org/W4248419300",
    "https://openalex.org/W2061499516",
    "https://openalex.org/W3035137491",
    "https://openalex.org/W4229781645",
    "https://openalex.org/W6684840438",
    "https://openalex.org/W6633339394",
    "https://openalex.org/W4211148385",
    "https://openalex.org/W6736713085",
    "https://openalex.org/W6649354785",
    "https://openalex.org/W845026497",
    "https://openalex.org/W6635877538",
    "https://openalex.org/W6763527631",
    "https://openalex.org/W2494036392",
    "https://openalex.org/W4324382292",
    "https://openalex.org/W3109218203",
    "https://openalex.org/W2337762945",
    "https://openalex.org/W3090395639",
    "https://openalex.org/W7036912993",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2917613320",
    "https://openalex.org/W7056717972",
    "https://openalex.org/W1967512008",
    "https://openalex.org/W2011902304",
    "https://openalex.org/W2779406116",
    "https://openalex.org/W2889833430",
    "https://openalex.org/W3208806881",
    "https://openalex.org/W4379645936",
    "https://openalex.org/W6668006650",
    "https://openalex.org/W3207536300",
    "https://openalex.org/W3170469780",
    "https://openalex.org/W1550933260",
    "https://openalex.org/W7083093950",
    "https://openalex.org/W2087360383",
    "https://openalex.org/W3004314641",
    "https://openalex.org/W4285263440",
    "https://openalex.org/W4213218314",
    "https://openalex.org/W7014462897",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W2060418758",
    "https://openalex.org/W2032472616",
    "https://openalex.org/W2154098145",
    "https://openalex.org/W7039063400",
    "https://openalex.org/W2970120757",
    "https://openalex.org/W6631002354",
    "https://openalex.org/W3205352824",
    "https://openalex.org/W3035261420",
    "https://openalex.org/W4281642025",
    "https://openalex.org/W3106290101",
    "https://openalex.org/W2050988571",
    "https://openalex.org/W1490542880",
    "https://openalex.org/W4285127912",
    "https://openalex.org/W4388337837",
    "https://openalex.org/W2864681209",
    "https://openalex.org/W2095451964",
    "https://openalex.org/W54664877",
    "https://openalex.org/W4287888019",
    "https://openalex.org/W3101449015",
    "https://openalex.org/W3018827121",
    "https://openalex.org/W3115633976",
    "https://openalex.org/W3031914912",
    "https://openalex.org/W1966678693",
    "https://openalex.org/W6827768719",
    "https://openalex.org/W1991413695",
    "https://openalex.org/W3014415613",
    "https://openalex.org/W2951286828",
    "https://openalex.org/W2983004086",
    "https://openalex.org/W1560202022",
    "https://openalex.org/W6858228924",
    "https://openalex.org/W4234644100",
    "https://openalex.org/W4285174271",
    "https://openalex.org/W3176477796",
    "https://openalex.org/W2794030288",
    "https://openalex.org/W4381853966",
    "https://openalex.org/W1967084205",
    "https://openalex.org/W4254424921",
    "https://openalex.org/W2760213553",
    "https://openalex.org/W2905623858",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W4232610889",
    "https://openalex.org/W2144862731",
    "https://openalex.org/W2954042708",
    "https://openalex.org/W2115654798",
    "https://openalex.org/W1597768502",
    "https://openalex.org/W4301663117",
    "https://openalex.org/W2007368676",
    "https://openalex.org/W4206808638",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W6843989031",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W6721359027",
    "https://openalex.org/W2914655192",
    "https://openalex.org/W2023956966",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6788417511",
    "https://openalex.org/W1555082340",
    "https://openalex.org/W4229685843",
    "https://openalex.org/W3103536442",
    "https://openalex.org/W2099006997",
    "https://openalex.org/W3127011718",
    "https://openalex.org/W2074229945",
    "https://openalex.org/W4289255545",
    "https://openalex.org/W79667300",
    "https://openalex.org/W2913412904",
    "https://openalex.org/W4235019020",
    "https://openalex.org/W4254241601",
    "https://openalex.org/W4299323306"
  ],
  "abstract": "Abstract The field of “BERTology” aims to locate linguistic representations in large language models (LLMs). These have commonly been interpreted as representing structural descriptions (SDs) familiar from theoretical linguistics, such as abstract phrase-structures. However, it is unclear how such claims should be interpreted in the first place. This paper identifies six possible readings of “linguistic representation” from philosophical and linguistic literature, concluding that none has a straight-forward application to BERTology. In philosophy, representations are typically analyzed as cognitive vehicles individuated by intentional content. This clashes with a prevalent mentalist interpretation of linguistics, which treats SDs as (narrow) properties of cognitive vehicles themselves. I further distinguish between three readings of both kinds, and discuss challenges each brings for BERTology. In particular, some readings would make it trivially false to assign representations of SDs to LLMs, while others would make it trivially true. I illustrate this with the concrete case study of structural probing: a dominant model-interpretation technique. To improve the present situation, I propose that BERTology should adopt a more “LLM-first” approach instead of relying on pre-existing linguistic theories developed for orthogonal purposes.",
  "full_text": "Synthese (2024) 203:15\nhttps://doi.org/10.1007/s11229-023-04435-5\nORIGINAL RESEARCH\nThe ambiguity of BERTology: what do large language\nmodels represent?\nTommi Buder-Gröndahl 1\nReceived: 22 June 2023 / Accepted: 17 November 2023 / Published online: 26 December 2023\n© The Author(s) 2023\nAbstract\nThe ﬁeld of “BERTology” aims to locate linguistic representations in large language\nmodels (LLMs). These have commonly been interpreted as representing structural\ndescriptions (SDs) familiar from theoretical linguistics, such as abstract phrase-\nstructures. However, it is unclear how such claims should be interpreted in the ﬁrst\nplace. This paper identiﬁes six possible readings of “linguistic representation” from\nphilosophical and linguistic literature, concluding that none has a straight-forward\napplication to BERTology. In philosophy, representations are typically analyzed as\ncognitive vehicles individuated by intentional content. This clashes with a prevalent\nmentalist interpretation of linguistics, which treats SDs as (narrow) properties of cog-\nnitive vehicles themselves. I further distinguish between three readings of both kinds,\nand discuss challenges each brings for BERTology. In particular, some readings would\nmake it trivially false to assign representations of SDs to LLMs, while others would\nmake it trivially true. I illustrate this with the concrete case study of structural probing:\na dominant model-interpretation technique. To improve the present situation, I pro-\npose that BERTology should adopt a more “LLM-ﬁrst” approach instead of relying\non pre-existing linguistic theories developed for orthogonal purposes.\nKeywords Linguistic representation · Language model · Deep learning · BERTology\n1 Introduction\nImproving the explainability of large language models (LLMs) such as BERT (Devlin\net al., 2019) is a pressing concern in natural language processing (NLP). A reaseach\nprogram titled “BERTology” aims to discover linguistic representations in LLMs’\nactivation patterns, with techniques like structural probing (Rogers et al., 2020). In a\nnotable departure from traditional connectionist NLP , BERTology makes heavy use\nB Tommi Buder-Gröndahl\ntommi.grondahl@helsinki.ﬁ\n1 Department of Digital Humanities, University of Helsinki, Yliopistonkatu 3, 00014 Helsinki, Finland\n123\n15 Page 2 of 32 Synthese (2024) 203 :15\nof abstract structural descriptions (SDs) derived from theoretical linguistics, such as\nhierarchical phrase-structures.\nDespite frequent proclamations about the linguistic capacities of LLMs, experi-\nmental results have been deemed unclear (Kulmizev & Nivre, 2022). This is partly\ntraceable to technical challenges, such as the inﬂuence of superﬁcial heuristics (McCoy\net al., 2019), differences between probing methods (Immer et al., 2022), or the impact\nof labeling formalism (Kulmizev et al., 2020). However, I propose that a major ambi-\nguity can be traced to a more fundamental source: the theoretical notion of “linguistic\nrepresentation” itself. While related matters have been extensively discussed in lin-\nguistic and philosophical literature, their effects on BERTology have so far not been\naddressed.\nIn particular, there is a discrepancy in how representations have been analyzed\nacross ﬁelds. Customary accounts in contemporary philosophy individuate them by\nintentional content , as determined by e.g. informational or teleological relations\n(Dretske, 1981; Fodor, 1990; Millikan, 2017; Neander, 2017). In contrast, theoretical\nlinguists commonly adopt a mentalist reading where linguistic analyses ultimately\nconcern cognitive architecture itself (Chomsky, 1965, 1986; Fodor, 1981; Laurence,\n2003; Smith, 2006; Collins, 2014, 2023; Adger, 2022). This doctrine cuts across most\nbranches of the generative tradition, as well as many other frameworks (e.g. Langacker,\n1987; Goldberg, 2006). I focus on generativism in this paper due to the centrality of\nphrase-structure in BERTology.\nA natural interpretation of mentalism is that SDs characterize vehicles internal to the\ncognitive system. However, this still remains ambiguous, as it does not specify their\ngrounds of individuation. Even if linguistics is about representations qua vehicles,\nthey could still be individuated by contents. Alternatively, they could be individuated\nby intrinsic (narrow) vehicle-properties that only characterize their “shape” within\nthe system. I suggest that ongoing debates in the philosophy of linguistics can be\nclariﬁed via these two readings: some authors take SDs to be contents of linguistic\nrepresentations (e.g. Rey, 2020), while others assimilate SDs to vehicle-properties on a\nhigh level of abstraction (e.g. Adger, 2022). My purpose is not to defend either reading\nas such, or to address various exogetic debates (c.f. Collins & Rey, 2021). Instead, I\npoint to challenges in both readings when applied to BERTology.\nI argue that the content-reading would essentially mark a return to linguistic struc-\nturalism (Bloomﬁeld, 1933;H a r r i s ,1951), since it would require SDs to be recoverable\nfrom the input data. This directly contrasts the generative analysis of syntactic phrases\nhaving an autonomous computational status (Chomsky, 1975; Adger, 2022; Collins,\n2023). Revising this conception of SDs would thus also require withdrawing many\nproclaimed results of BERTology. To avoid this, an initial possibility could be to save\nautonomous SDs by elevating them to a realm of abstract objects (Katz, 1981\n; Postal,\n2003). However, this would effectively yield indeterminacy between representations\nof SDs generable via weakly equivalent grammars (c.f. Quine, 1970), and thereby\nundermine the whole premise of BERTology.\nThe vehicle-reading, in turn, succumbs to the problem of relating abstract SDs to\ntheir concrete realizers. To avoid logical category errors with their direct assimilation\n(Postal, 2009; Behme, 2015), SDs can be treated as mathematical contents assigned\nto vehicles via a separate interpretation function—in line with the general explanatory\n123\nSynthese (2024) 203 :15 Page 3 of 32 15\nframework laid out by Egan ( 2010, 2014, 2018). But, as recently observed by Facchin\n(2022), such mathematical contents are vulnerable to well-known triviality problems\nfor mapping-theories of physical computation (Sprevak, 2018). The upshot is that the\nmere availability of mathematical content cannot ground its explanatory relevance.\nFollowing (Egan, 2017), I maintain that mathematical contents are nevertheless\nvaluable due to their use as explanans for generalizations that would otherwise be\noverlooked. By acting as “proxies” of the underying cognitive states, they allow sur-\nrogative reasoning(Swoyer, 1991). In this interpretation, the justiﬁcation of stipulating\nlinguistic representations is based on its explanatory value: is it needed for covering\nsome relevant generalizations that would otherwise be left unaddressed? In effect,\nthis order of explanation is reversed in BERTology, where the goal is simply to ﬁnd\nsome mapping from LLM-states to target labels interpreted via SDs. This alone is\ninsufﬁcient to ground the explanatory relevance of those SDs, since indeﬁnitely many\nother mappings would be available as well. I illustrate this dilemma with Hewitt and\nManning’s (2019) structural probing algorithm as a concrete case-study.\nThe problem is not restricted to representationalist interpretations of LLMs: the anti-\nrepresentationalist should also know what she is denying in the ﬁrst place. Of course,\nthe notion of “representation” might turn out to be too vague to be theoretically useful,\nin which case anti-representationalism would be justiﬁed by default. But this negative\nconclusion could only be reached after proper engagement with the representationalist\nclaims. By the same token, I retain agnosticism about the ﬁnal verdict on LLMs; my\npresent aim is to clarify the conditions for both representationalism and its rejection\nto be informative hypotheses.\nWhile I do not see any easy way out of the present predicament, my overall con-\ntention is that BERTology has been overly reliant on linguistic theories developed for\nvery different purposes than connectionist NLP—often decidedly antagonistic with it.\nPrima facie , it would be surprising if these happened to coincide in some deep way. I\npropose that their seeming convergence is instead an artifact of meta-theoretical inter-\npretation via pre-determined SDs. The central question itself— why choose these SDs\nand not others —has remained insufﬁciently addressed.\nAs an alternative methodology for future work, I suggest that BERTology should\nbe approached more bottom-up, with the goal of formulating appropriate SDs for\ncapturing the LLM-pipeline in an explanatorily robust human-readable manner. The\ntransformer architecture used in LLMs (V aswani et al., 2017) has been shown to\nbe amendable for a high-level computational analysis captured in a symbolic meta-\nlanguage (Weiss et al., 2021). While this research still only pertains to simpler models\nthan LLMs, it points to a promising direction of comparing high-level analyses based\non independently grounded links to lower-level algorithms. This is still not guaranteed\nto rule out all indeterminacy between the higher and lower levels, but at least it would\nbase the selection of the former on the latter.\nThe paper is organized as follows. In Sect. 2 I argue that BERTology is committed to\nlinguistic representations in a strong sense that deviates from traditional connectionist\nNLP . This motivates further investigation of how such a committment should be inter-\npreted. Section 3 delineates the vehicle-content distinction in computational systems,\nand uses it to ground different readings of “linguistic representation”. Sections 4–5\nintroduce three versions of both the content- and vehicle-reading, and demonstrate\n123\n15 Page 4 of 32 Synthese (2024) 203 :15\ntheir challenges for BERTology with a focus on structural probing. Section 6 further\nelaborates on the practical ramiﬁcations of these problems. Section 7 summarizes the\npaper and discusses prospects for future work.\n2 BERTology: the return of representations to NLP\nDeep neural networks (DNNs) have traditionally been contrasted with representations\nof abstract SDs such as phrase-structures. For present purposes, it sufﬁces that a\nphrase is a hierarchical object that dominates its immediate constituents and all of their\nconstituents. This can explain why expressions like I saw someone with binoculars\nare ambiguous, as shown in ( 1):\n(1) S\nNP\nI\nVP\nV’\nV\nsaw\nNP\nsomeone\nPP\nwith binoculars\nS\nNP\nI\nVP\nV\nsaw\nNP\nsomeone with binoculars\nDNNs are not pre-programmed to represent such SDs. This invites two alternative\ninterpretations of how they can attain linguistic performance: eliminative connection-\nism and implementational connectionism (Marcus, 1998). The ﬁrst of these would\ndiscard abstract representations altogether. In their seminar paper on learning the\nEnglish past tense, Rumelhart and McClelland ( 1986) write:\n(...) a reasonable account of the acquisition of past tense can be provided without\nrecourse (...) to the notion of a “rule” as anything more than a description of the\nlanguage. (...) The child need not ﬁgure out what the rules are, nor even that\nthere are rules.\n(Rumelhart and McClelland 1986, p. 267; emphasis in the original)\nThat is, inputs, outputs, or their relations being described in a certain way does not\njustify inferring that the model represents them in that way. Rumelhart and McClel-\nland’s anti-representationalist position reduces the role of SDs to the description of\nthe data or task, leaving them out when explaining model-internal computation.\nIn contrast, implementational connectionism takes a DNN’s sub-symbolic states\nand their transitions to realize rules and representations on a high level of description.\nThis is succinctly put by Pinker and Price ( 1988):\nPDP\n1 models would occupy an intermediate level between symbol processing\nand neural hardware: they would characterize the elementary information pro-\ncesses provided by neural networks that serve as the building blocks of rules\nor algorithms. Individual PDP networks would compute the primitive symbol\n1 “PDP” stands for parallel distributed processing , i.e. connectionist architectures.\n123\nSynthese (2024) 203 :15 Page 5 of 32 15\nassociations (such as matching an input against memory, or pairing the input and\noutput of a rule), but the way the overall output of one network feeds into the input\nof another would be isomorphic to the structure of the symbol manipulations\ncaptured in the statements of rules.\n(Pinker and Price 1988, p. 76)\nFor building NLP applications, the distinction between eliminative and imple-\nmentational connectionism is not immediately relevant. As long as models work in\nend-to-end settings, agnosticism can be maintained about the representations involved.\nIf no manually programmed rules are needed, linguistic questions can be set aside.\nThis attitude is captured in the famous quip attributed to Frederic Jelinek: “Whenever\nI ﬁre a linguist our system performance improves” (for elaboration, see Jelinex, 2005).\nHowever, increasing model explainability has become a central goal in recent years\n(Danilevsky et al., 2020). This is motivated both by the scientiﬁc aim of understand-\ning DNNs better, as well as practical concerns such as model biases (Nadeem et\nal., 2020) or adversarial data (Li et al., 2020). Explaining the structure of DNNs\nin human-understandable terms can no longer remain agnostic about model-internal\nrepresentations, since those are precisely what it aims to uncover.\nContemporary NLP is built around pre-trained LLMs , such as BERT (Devlin et\nal., 2019) or GPT (OpenAI, 2023). These are trained on massive datasets for generic\nlinguistic tasks such as predicting masked tokens (for BERT) or upcoming text (for\nGPT), and can then be ﬁne-tuned for domain-speciﬁc tasks. Pre-training is thus aimed\nto give them generic linguistic competence on which to build in subsequent tasks.\nInterpreting pre-trained LLMs has reached such a central status that “BERTology” is\nnow recognized as a dedicated subﬁeld of NLP (Rogers et al., 2020). Notably, it makes\nheavy use of linguistic theory. BERT has been suggested to represent phrase-structures\n(Coenen et al., 2019), dependency relations (Jawahar et al., 2019), semantic roles\n(Kovaleva et al., 2019), constructions (Madabushi et al., 2020), and lexical semantics\n(Soler & Apidianaki, 2020), among others. While LLMs still do not use explicitly\nprogrammed representations, BERTology aims to ﬁnd representations in them (see\nFig. 1).\nIn stark contrast to Rumelhart and McClelland’s ( 1986, p. 267) contention that the\nlanguage-learner “need not ﬁgure out what the rules are, nor even that there are rules”,\nBERTology is thus founded on the premise that LLMs do in fact ﬁgure out what the\nrules are, and achieve this via internal representations. Table 1 collects representative\nquotes from literature, which further display this dedication.\nThe leading model-interpretation technique is structural probing, where a classiﬁer\n(“probe”) is trained to map LLM-states to linguistic target labels. For example, Hewitt\nand Manning ( 2019) use matrix B for enacting the linear transformation in equation\n(2), where h\ni and hj are encodings for the i:th and j:th word in the input sentence:\n(2) dB (hi , hj ) = (B(hi − hj ))T (B(hi − hj ))\nThe metric dB (hi , hj ) is trained to recreate the hierarchical distance between each\nword pair in the input’s parse tree (obtained from a pre-existing treebank). This is\na representative example of BERTology and its connection to linguistic theory. My\npurpose here is not to evaluate this particular probe in further technical detail or\n123\n15 Page 6 of 32 Synthese (2024) 203 :15\nFig. 1 Linguistically driven model interpretation in BERTology\ncompare it to other contenders (c.f. Immer et al., 2022). Rather, I will use it as a\nconcrete case-study for illustrative purposes.\nDespite the prominence of BERTology, no consensus has been reached on the\nlinguistic representations present in LLMs. On the one hand, impressive model per-\nformance has been attained across many linguistic tasks (McCoy et al., 2020;L a s r i\net al., 2022), and structural probing has uncovered systematic correlations between\nDNN layers and linguistic classes (Jawahar et al., 2019; Tenney et al., 2019; Immer\net al., 2022). On the other hand, models often rely on superﬁcial heuristics (McCoy\net al., 2019). As Rogers et al. ( 2020, p. 854) put it: “if there is a shortcut in the data,\nwe have no reason to expect BERT to not learn it”. It remains unclear how to draw the\nline between genuine linguistic representations and mere complex heuristics.\nPater (2019, p. e61–e62) observes that the interpretation of experiments often hinges\non theoretical assumptions: DNNs’ partial success illustrates their strength to some,\nwhile their partial failure illustrates their deﬁciencies to others. This is problematic,\ngiven that the purpose of BERTology is precisely to evaluate underlying theoretical\nhypotheses empirically. If interpreting results hinges on the prior acceptance of some\nhypotheses over others, then the results do not genuinely help decide between them.\nThe predicament is concisely summarized by Kulmizev and Nivre ( 2022):\n(...) hypotheses, methodologies, and conclusions comprise many conﬂicting\ninsights, giving rise to a paradoxical picture reminiscent of Schrödinger’s cat\n– where syntax appears to be simultaneously dead and alive inside the black box\nmodels.\n(Kulmizev & Nivre 2022, p. 02)\n123\nSynthese (2024) 203 :15 Page 7 of 32 15\nTable 1 Quotes from BERTology literature (bolded parts show dedication to representations)\n“Our goal is to design a simple method for testing whether a neural network embeds\neach sentence’s dependency parse tree in its contextual word representations—a structural\nhypothesis.” (Hewitt & Manning, 2019, pp.4129–4130)\n“Investigating how BERT represents syntax , we describe evidence that attention matrices\ncontain grammatical representations. ” (Coenen et al., 2019, p. 8592)\n“In this work, we investigate the linguistic structure implicitly learned by BERT’s\nrepresentations.” (Jawahar et al., 2019, p. 3652)\n“Another theme that emerges in several studies is the hierarchical nature of the learned\nrepresentations.” (Belinkov & Glass, 2019,p .5 2 )\n“We propose a methodology and offer the ﬁrst detailed analysis of BERT’s capacity to cap-\nture different kinds of linguistic information by encoding it in its self-attention weights. ”\n(Kovaleva et al., 2019, p. 4365)\n“We ﬁnd that the model represents the steps of the traditional NLP pipeline in an inter-\npretable and localizable way, and that the regions responsible for each step appear in the\nexpected sequence: POS tagging, parsing, NER, semantic roles, then coreference.” (Tenney et\nal., 2019, p. 4593)\n“Neural networks can and do improve on this task by inducing their own representations of\nsentence structure which capture many of the notions of linguistics” (Manning et al., 2020,p .\n30047)\n“(...) representation of higher-level semantic phenomena follows the encoding of syntax\nand predicate semantics. ” (Kuznetsov & Gurevych, 2020, p. 177)\n“To correctly predict the number of the verb, the DNN must derive an implicit analysis of\nthe structure of the sentence ” (Linzen & Baroni, 2021, p. 198)\n“In our experiments, we focus on answering two questions: (i) How is number information\nencoded in BERT’s representations? and (ii) How is number information transferred from\nan o u nt oi t sh e a dv e r bfor the model to use it on the behavioral task?” (Lasri et al., 2022,p .\n8822)\n“(...) models are largely able to capture long-range syntactic dependencies that require\nhierarchical representations of sentences. ” (Mueller et al., 2022, p. 1352)\n“Due to their strong performance on many language-based tasks that require some linguis-\ntic understanding, it is natural to hypothesize that the models must implicitly encode some\nlinguistic knowledge.” (Li et al., 2022, p. 1144)\nKulmizev and Nivre draw attention to a number of technical aspects that should be\nbetter taken into account, and it is easy to agree with this call for further clariﬁcation. 2\nHowever, I take a further step in proposing that a major ambiguity can be traced to an\neven more fundamental source: the notion of linguistic representation itself.\n2 They emphasize four venues for improvement. First, linguistic properties need to be distinguished from\ncoding properties from which they can be inferred. Second, linguistic data should be differentiated from\nits theoretical interpretation, as illustrated by the inﬂuence of labeling frameworks on probing (Kulmizev\net al., 2020). Third, it is important to tease apart the effect of each variable involved in the choice of model\narchitecture, hyperparameters, training protocol, data, and the end-to-end task. Finally, research questions\nneed to be clariﬁed based on whether they concern what a model does learn in a particular experiment\nsetting, what it can learn in principle, or what it must learn under certain conditions. Especially the ﬁrst two\nconsiderations will recur in my discussion as well.\n123\n15 Page 8 of 32 Synthese (2024) 203 :15\n3 The ambiguity of “linguistic representation”\nThe mentalist view that language arises from cognition is ubiquitous in linguistic\ntheory, especially in the generative framework (Chomsky, 1965, 1986; Gleitman,\n2021) but also elsewhere, as in cognitive grammar (Langacker, 1987) and construction\ngrammar (Goldberg, 2006). I focus on generativism due to the centrality of phrase-\nstructure in BERTology. Despite the formal nature of Chomsky’s initial linguistic\nwork, the mentalist commitment is already indicated in his early writings:\nA language L is understood to be a set (in general inﬁnite) of ﬁnite strings of\nsymbols drawn from a ﬁnite “alphabet.” (...)\nA grammar of L is a system of rules that speciﬁes the set of sentences of L and\nassigns to each sentence a structural description. (...)\nIt is appropriate, in my opinion, to regard the grammar of L as a representation\nof fundamental aspects of the knowledge of L possessed by the speaker-hearer\nwho has mastered L.\n(Chomsky, 1975,p .5 )\nThe quote above recognizes four kinds of entities: (i) languages in the sense of\nformal language theory (i.e. sets of strings); (ii) SDs such as phrase-structures; (iii)\ngrammars as abstract generative systems; and (iv) knowledge of language as a cognitive\nstate. This taxonomy further grounds the distinction between weak and strong gener-\native capacity, where the former concerns the ability of a grammar to generate strings,\nand the latter concerns its ability to generate SDs (Chomsky, 1980; Miller, 1999).\nAll weakly equivalent grammars generate the same expressions, but generative theory\nconcerns strong generative capacity instead (Ott, 2017). A central meta-theoretical\nquestion is, thus: what is the relation between concrete linguistic expressions, abstract\nSDs, and concrete cognitive systems?\nLinguistic representations provide the crucial link for connecting SDs to cognitive\nstates. The basic idea is that SDs receive their cognitive relevance by being represented\nin the cognitive system. Thereby, SDs can be used to individuate representations:e . g .\nNP and VP are SDs that have cognitive relevance by virtue of the system containing\nNP- and VP-representations.\nHowever, “representation” is polysemous. Especially in the AI-literature, it is often\ntreated as roughly synonymous with “model-internal state”. For example, states that\narise from word-inputs can be called “word representations” (e.g. Pennington et al.,\n2014). This needs to be clearly distinguished from more theoretically committing\ninterpretations adopted in philosophy, linguistics, or cognitive science.\nA stricter notion of representation can be clariﬁed with the vehicle-content dis-\ntinction (Dennett, 1991; Millikan, 1993). V ehicles are objects operated on by a\ncognitive/computational system by virtue of their intrinsic properties and interrelations\n(e.g. Piccinini, 2015). Contents are semantic interpretations assigned to vehicles—i.e.\nproperties/entities to which they refer. In veridical representation, the stimulus that\ntriggers the vehicle also realizes its content. In misrepresentation, the content and\nstimulus diverge. In addition, vehicles have vehicle-properties that only characterize\ntheir intrinsic “shape” within the system. Figure 2 displays this overall schema.\n123\nSynthese (2024) 203 :15 Page 9 of 32 15\nFig. 2 V ehicle-content distinction in cognitive systems\nFrom now on, I take (stipulated) linguistic representations to be linguistically indi-\nviduated vehicles in the system under investigation. SDs play the part of individuating\nsuch vehicles: for instance, a VP-representation is a vehicle belonging to the equiva-\nlence class identiﬁed by the SD “VP”. Linguistic mentalism hinges on the presence\nof such SD-individuated representations in human cognition, and BERTology extends\nthis commitment to LLMs.\n3\nBut dedication to linguistic representations ( qua vehicles) does not yet tell us how\ntheir linguistic status is grounded. One option is to classify vehicles by content. An\nexample of this is ‘words that denote a type of ﬁsh’, which individuates a class of words\n(i.e. vehicles) by their semantic interpretation (i.e. content). Alternatively, vehicles\ncan also be classiﬁed by vehicle-properties, such as ‘capitalized’ for written words.\nConsequently, treating linguistic representations as vehicles individuated by SDs is\nambiguous with respect to whether this individuation is based on content or vehicle-\nproperties. This yields two distinct readings of “linguistic representation”:\n• Content-reading: vehicles individuated by SDs as contents\n• Vehicle-reading: vehicles individuated by SDs as vehicle-properties\nThe content-reading takes SDs to characterize contents of cognitive vehicles, which\nin turn makes those vehicles linguistic representations. In contrast, the vehicle-reading\ntakes vehicles themselves to realize intrinsic properties characterized by SDs. My\npurpose is not to defend or oppose either reading as such. Instead, I raise problems for\nboth when applying them to BERTology. Section 4 discusses the content-reading and\nSect. 5 the vehicle-reading.\n4 Content-reading\nI identify three candidates for the content-reading: directly referential (Sect. 4.1), ﬁc-\ntionalist (Sect. 4.2), and Platonist (Sect. 4.3). Each has trouble uniting BERTology\nwith a foundational generative notion: the autonomy of levels .\n3 Conversely, I interpret anti-representationalism about language as rejecting the presence of linguistically\nidentiﬁable cognitive vehicles (see Sect. 6 for further discussion).\n123\n15 Page 10 of 32 Synthese (2024) 203 :15\nFig. 3 Directly referential content-reading\n4.1 Directly referential reading\nAn initial option would be to treat SDs as properties of the input data itself, which the\nmodel (M) somehow picks out. This general idea (not applied to language speciﬁcally)\nis pursued by Cappelen and Dever ( 2021), who propose a causal theory of reference\nfor DNNs following a Kripkean line of semantic analysis (Kripke, 1980). Figure 3\nsummarizes this directly referential content-reading.\nHere, the SD is realized by the input, which allows grounding the reference rela-\ntion in the causal pipeline between it and M. For example, according to informational\nsemantics, a representation refers to the entity/property that it carries information\nof (Dretske, 1981; Fodor, 1990). Teleosematics adds the requirement that such\ninformation-carrying must be the proper function of the representation, based on nat-\nural selection in biological systems or design in artiﬁcial systems (Millikan, 2017;\nNeander, 2017). Such naturalistic theories of reference are united by their reliance on\ncausal relations between the representation and its content.\nThis also suggests a simple and prima facie appealing interpretation of BERTology\ntechniques like structural probing: they aim to ﬁnd parts of M that refer to SDs as\nper some causal theory of reference. Applied to Hewitt and Manning’s ( 2019) probe\n(see Sect. 2): the parse tree distance between tokens w\ni and w j is already present in\nthe input, and their respective encodings hi and hj refer to this information in a way\nthat is reliably captured by the metric dB (hi , hj ). This way, dB (hi , hj ) picks out those\naspects of hi and hj that refer to phrase-structural properties in the input.\nThe referential interpretation thus requires SDs to be directly present in the input.\nConsequently, they should be deﬁnable via linear information. But now a problem\narises: phrase-structures as characterized in generative syntax are precisely not deﬁn-\nable in this way. In a recent paper outlining this perspective, Collins ( 2023, p. 7) takes\nthe following principle to be a “a core aspect any theory must respect”:\nsyntax determines units of combined lexical items that are not identiﬁable or\nindividuated in terms of linear order or any other perceptible property associated\nwith morphophonemic form.\n(Collins 2023, p. 7; my emphases)\n123\nSynthese (2024) 203 :15 Page 11 of 32 15\nThe pre-generative structuralist tradition deﬁned categories distributionally:t w o\nutterances belong to the same class if they have the same distribution with respect\nto other (similarly deﬁned) classes (Bloomﬁeld, 1933; Harris, 1951). Built around\nphonology, structuralism treated morphemes as phoneme sequences and syntactic\nphrases as morpheme sequences, each stipulated for allowing more concise dis-\ntributional generalizations (Harris, 1951, p. 151). A central motivation for early\ngenerativism was the rejection of this approach for syntax. In stark contrast to it,\nChomsky ( 1957, 1965, 1975) elevated each level of grammar (phonology, morphol-\nogy, syntax, etc.) to an autonomous status, where higher levels were not deﬁnable via\nelements on lower levels. Instead, each level had its own vocabulary of computational\nprimitives to be combined, and levels were linked via additional mapping rules. This\nautonomy of levels has major repercussions on linguistic representations, as explained\nby Adger ( 2022):\n(...) the perspective in [Chomsky ( 1975)] is top-down rather than bottom up. Each\nlevel is an independently speciﬁed concatenation algebra consisting of a set of\nprimes (symbols) and relations at that level, and the algebra speciﬁes certain\nstrings of symbols as well-formed, so the “representations” are not derived\nfrom the utterance. Rather, each string of symbols at one level can be converted\ninto a lower level of structure through a speciﬁed set of mappings\n(Adger, 2022, p. 251; my emphases)\nIn particular, phrase-structures are not derived from linear concatenations of units\n(words, morphemes, phonemes, graphemes, etc.). Syntactic information is fundamen-\ntally novel in kind by exhibiting non-linear hierarchical relations, which requires a\nwholly different set of computational units and operations.\n4 As (Katz 1981,p .3 8 )\nrecounts: “the potential for highly abstract generative grammars could be realized\nonly if a new and far less concrete interpretation of grammars was found”.\nOf course, the autonomy of levels could be rejected. I am not assessing its merits\nas such, nor do I presume that it must be correct (see Sect. 6 for discussion of alter-\nnative frameworks). Instead, I examine a conditional question: given that BERTology\nliterature contains numerous claims about LLMs representing abstract SDs as deﬁned\nin generative linguistics, how could this be possible in the ﬁrst place? If SDs are\nautonomous with respect to the linear input and thus not present in it, they cannot be\nrepresented via causal relations as required by naturalistic theories of reference.\nIn fact, the problem can already be predicted based on a viable candidate for infor-\nmation processing in DNNs. Buckner ( 2018) proposes that they enact transformational\nabstraction, where higher layers discard, combine, and alter features from lower layers.\nThe basic idea is shown in Fig. 4.\nTransformational abstraction results in increasing levels of transformational invari-\nance: the ability to detect features that remain stable across other changes. This yields\n4 As an anonymous reviewer correctly notes, generative theory has commonly treated phonological and\nsemantic features as belonging to lexical items and thereby to the syntactic derivation. Interface repre-\nsentations that map syntax to phonology (“PF”) and semantics (“LF”) have also played a vital role. Such\ninteractions between levels somewhat complicate proclamations of their autonomy. That notwithstanding,\nit remains a foundational principle of generative syntax that syntactic phrase-structure is not grounded in\nlinear concatenations of words. This is the crux of my present argument.\n123\n15 Page 12 of 32 Synthese (2024) 203 :15\nFig. 4 Transformative abstraction in DNNs\nthe potential to represent many abstract properties. As Buckner ( 2018) discusses, the\nconcept ‘triangle’ was problematic for classical empiricism because it seems that sen-\nsory information cannot correspond to triangularity in general; only speciﬁc triangles.\nHowever, higher layers of an image-detector DNN could become sensitive to just\nthose properties that make up triangular inputs, by combining and removing lower-\nlevel information from prior layers. These properties are still fully based on low-level\nsensory information, derivable from it via transformative abstraction.\nLikewise, Nefdt ( 2023, pp. 92–95) proposes that DNNs can extract abstract linguis-\ntic structures from data. Crucially, this idea relies on the assumption that the structures\nare in the data to begin with:\nWhat neural networks are especially good at is picking up patterns hidden in\ncomplex sets of data. (...) The result is a hyper-empiricist framework for capturing\nthe real patterns of complex systems in reality.\n(Nefdt, 2023, p. 93; my emphases)\nIf this is indeed how DNNs work, they might well represent something like high-\nlevel distributional properties as originally envisioned in the structuralist paradigm.\nBut the generative enterprise was speciﬁcally founded upon the rejection of such\ndistributional approaches to SDs. In the directly referential reading, assigning repre-\nsentations of autonomous SDs to LLMs would therefore become trivially false .T h i s\ninvites an alternative interpretation of the relation between inputs and SDs.\n4.2 Fictionalist reading\nIn an original and innovative account, Rey ( 2020) aims to combine the content-based\nindividuation of linguistic representations with the lack of SDs having real existence in\nthe input.\n5 He proposes that linguistic contents are intentional inexistents, adopting the\nterm from Brentano ( 1874/1911).6 Linguistic representation is thus misrepresentation,\nas depicted in Fig. 5.\nIt is helpful to consider an analogy from the philosophy of perception. A much-\ndiscussed example of a misrepresentation is seeing a stick as bent in water. This\n5 Rey calls his account “folieism”, but I adopt the more familiar term “ﬁctionalism” for convenience.\n6 In spite of terminology, Rey’s account of content departs from Brentano’s, which is non-naturalist in\ntreating intentional objects as sui generis entities. Whatever its general merits, applying this view to LLMs\nis immediately problematic for similar reasons as the direct “grasping” of Platonic contents (Sect. 4.3).\n123\nSynthese (2024) 203 :15 Page 13 of 32 15\nFig. 5 Fictionalist\ncontent-reading\nconstitutes a problem for simple versions of direct realism, since the stick looks bent\nwithout actually being bent. The representationalist solution is to say that the stick is\nmistakenly represented as bent, where ‘bent’ acts as the represented content without\nbeing realized by the actual stick that triggers the representation (Jackson, 1977).\nBy the same token, Rey’s account of SDs—or, more generally, standard linguistic\nentities (SLEs)—takes them not to exist in the actual input to perception. Instead,\nhuman cognition contains representations that have SLEs as their intentional contents.\nThese representations are activated via a process that matches perceptual data to the\nmost appropriate representation based on some kind of hypothesis testing procedure,\nsuch as Bayesian inference (Rey, 2020, pp. 373–377). This account is motivated by\nthe classical approach to generative phonology outlined in Chomsky and Halle ( 1968),\nwhere phonological representations have idealized phonetic content.\n7 For example,\na phoneme such as /p/ is a phonological representation, the intentional content of\nwhich is a set of ideal acoustic and/or articulatory properties. Representing a piece of\nacoustic data as /p/ involves testing different hypotheses with respect to how well this\ndata would be predicted via different phonological representations, and reaching /p/ as\nthe most appropriate candidate. Crucially, the data rarely (if ever) satisﬁes the criteria\nof actually instantiating /p/, which would require unrealistically ideal circumstances.\nThis is why the representation is strictly a misrepresentation.\nAs an analogy to SLEs, Rey raises geometrical concepts such as ‘cube’, which lack\nactual physical manifestations. The basic idea is that while no real cubes are present\nin the environment, our minds are nevertheless primed to see approximately cubic\nobjects as cubes; i.e. to apply a representation with the intentional content ‘cube’.\nFor Rey, SLEs are similar ideal entities/properties that function only as intentional\ncontents, being (at least mostly) absent from the data itself.\nRey’s account might well be suitable for phonological representation, although\nsome challenges may arise with further details.\n8 However, phonology has no direct\nrelevance for interpreting LLMs, which take readily individuated orthographic words\nas inputs.\n9 Unlike ideal phonetic contents, orthographic words are straight-forwardly\n7 Chomsky and Halle ( 1968, p. 65) maintain that a phonological representation “can be interpreted as a set\nof instructions to the physical articulatory system, or as a reﬁned level of perceptual representation”.\n8 For instance, it is unclear how the approach would fare with the possibility of substance-free phonology\nwhere phonological features lack acoustic or articulatory content (Blaho, 2007; Odden, 2013;I o s a d ,2017).\n9 Strictly speaking, LLMs use “subword” tokens, which reduce the vocabulary size while increasing\ncoverage (Sennrich et al., 2016) This technical detail has no impact on the present discussion.\n123\n15 Page 14 of 32 Synthese (2024) 203 :15\npresent in the data itself. Instead, problems arise when extending the analysis to abstract\nSDs such as syntactic phrase-structures.\nTreating SLEs as ﬁctitious ideal properties does not yet remove the problem raised\nin Sect. 4.1: the autonomy of levels makes SDs undeﬁnable via lower-level linear\ninformation (Adger, 2022). Linear strings do not yield hierarchical phrase-structure\neven in idealized contexts (Collins, 2023, p. 110). Therefore, some further strategy\nis required for obtaining their representations. For this, Rey advocates Ramsiﬁcation,\nwhere each theoretical predicate is replaced with an existentially quantiﬁed second-\norder variable such that the properties they designate become deﬁned by how the theory\nrelates them to each other and to observable stimuli (Lewis, 1970).\n10 SDs would thus be\nthose properties that serve appropriate roles in the overall linguistic theory in relation\nto other linguistic properties, relevant cognitive processes (e.g. parsing), and relevant\nobservational data (e.g. grammaticality judgements).\nBut since the theoretical roles of stipulated SDs arise from their putative computa-\ntional roles in the cognitive system, it is unclear if Ramsiﬁcation can yield a “content”\nover and above vehicle-properties. This problem is noted by Dupre ( 2022):\nThe inferentialist proposal says: when a psychological type is treated in these\nsorts of ways by psychological processes, it represents. But the representational\nstory then just seems like a third wheel. Nothing is gained by the stipulation that\nsuch-and-such computational system is, purely in virtue of these computational\nproperties, also a representational system. All the causal and explanatory work\nis done by the computational story.\n(Dupre, 2022)\nAs a possible rejoinder, the intentional status of SD-representations could be traced\nto their relations to phonological (and perhaps also semantic) representations, which\nin turn are individuated by content. Their theoretical roles (recognized in Ramsiﬁ-\ncation) would thus be at least indirectly content-laden. But the gist of Dupre’s point\nstill stands: relations between SDs and other linguistic representations are exhausted\nby “the computational story”: i.e. vehicle-properties and relations between vehicles.\nWithout a prior understanding of these, Ramsiﬁcation cannot get off the ground.\n4.3 Platonist reading\nA possible candidate for avoiding the problems I have raised could be to treat linguis-\ntic contents as abstract, Platonic objects (Katz, 1981; Postal, 2003). This allows the\nautonomy of levels, since abstract linguistic objects on different levels do not need\nto be mutually constitutive. However, it remains unclear how these would be ﬁxed as\ncontents of linguistic representations, especially in LLMs.\nKatz(1981, pp. 193–200) ends up proposing a special “faculty of intuition” to\naccount for how we can be in ap r i o r icontact with the abstract realm of SDs. While\nthis assumption is already controversial about human cognition (Benacerraf, 1973), it\ndoes not even get off the ground with LLMs, which are straight-forwardly mechanistic\nsystems. Discarding such direct epistemic “grasping”, the remaining option seems to\n10 I thank an anonymous reviewer for raising this point.\n123\nSynthese (2024) 203 :15 Page 15 of 32 15\nFig. 6 Platonist content-reading\nTable 2 Summary of content-readings\nReading Problem for BERTology\nDirectly referential Autonomy of levels\nFictionalist Autonomy of levels\nPlatonist Quinean indeterminacy between SDs\nbe that abstract SDs constitute contents of linguistic representations when they can\nanalyze the input. This is illustrated in Fig. 6.\nHowever, all weakly equivalent grammars can analyze the same expressions even if\nthey strongly generate distinct SDs (see Sect. 3). This interpretation would thus result\nin Quinean indeterminacy about linguistic representation (Quine, 1970): there is no\nfact of the matter which of the (mutually incompatible) SDs that could analyze the\ninput is the content. The problem clearly arises from the fact that the abstract SDs are\nlimited to only analyzing the input, discarding the cognitive system itself. In short,\nunless the system’s internal structure is considered, Quinean indeterminacy looms.\nThis indicates that the vehicle-reading might fare better.\n4.4 Summary\nTo recap, neither real nor ﬁctional physical properties of the linear input constitute\nviable candidates for SDs as contents of linguistic representations in LLMs. They\ncannot theoretically ground BERTology without sacriﬁcing the autonomy of levels\nassumed in generative linguistics. This threatens to make LLM-interpretations that\nrely on autonomous SDs trivially false . If a Platonic conception of SDs is adopted\ninstead, representation-claims are again in danger of becoming trivial, this time due\nto Quinean indeterminacy. Table 2 summarizes the three readings and their problems.\n123\n15 Page 16 of 32 Synthese (2024) 203 :15\n5 Vehicle-reading\nDespite the prevalence of the content-reading in philosophical literature, its spe-\nciﬁc troubles with linguistic representations invite considering the vehicle-reading\ninstead.11 I identify three candidates for what the relation between vehicles and\nSDs could be: identity (Sect. 5.1), direct realization (Sect. 5.2), or indirect realiza-\ntion (Sect. 5.3). After rejecting the ﬁrst two, I observe a challenge in the last, once\nagain related to triviality concerns.\n5.1 Identity reading\nAs mentioned in Sect. 3, the word “representation” is sometimes used for any\ninternal states. While this technically ﬁts the vehicle-reading, it is obviously unfal-\nsiﬁable and hence uninformative. The vehicle-properties for individuating linguistic\nrepresentations should be non-trivial.\n5.2 Direct realizational reading\nA more substantive idea would be that linguistic explanations are descriptive abstrac-\ntions over cognitive states and processes. Descriptive abstraction is a theoretical\nprocess of attaining a high-level analysis by omission of information (Boone & Pic-\ncinini, 2016; Kuokkanen, 2022). This seems to be what Adger ( 2022)i sa f t e ri n\nassimilating a mental representation of a grammar to a brain-state:\n“A mental representation of the grammar of the language” is just the mental struc-\nture (brain state) which is, at the relevant level of abstraction from physiological\nmechanisms, the grammar of the language.\n(Adger, 2022, p. 252)\nThat is, by performing descriptive abstraction of an appropriate kind, one should\nreach an analysis of those aspects of cognition that realize the grammar—i.e.\n“represent” it in the vehicle-reading.\n12 Fig. 7 shows this realizational interpretation.\nDescriptive abstraction is based on the speciﬁcation of equivalence classes , where\neach member of a class has the same role. Assuming the system to be computa-\ntional, it transforms vehicles into others based on their intrinsic form and interrelations\n11 A possible initial objection could arise that representations must ipso facto have content, since otherwise\nthey would not be “representations” in the ﬁrst place. For two reasons, I am not worried about such an ap r i o r i\nargument. First, the vehicle-reading is agnostic about whether linguistic representations have contents; it\nonly maintains that their linguistic status is based on vehicle-properties and not contents (if such exist).\nSecond, terminology is rarely a dependable guide to ontology. The word “atom” was originally used as\nsomething ipso facto non-decomposable; but was later adopted for decomposable entities in scientiﬁc\npractice. An ap r i o r icase against splitting atoms based on etymology would evidently not work. By the\nsame token, the vehicle-reading needs to be assessed based on its theoretical and empirical merits, mere\nconceptual analysis being unreliable.\n12 Chomskyan linguists (such as Adger) would, of course, likely deny that LLMs have similar linguistic\nrepresentations as humans. My point here is that the schema of descriptive abstraction gives criteria for\nwhat would be required for linguistic representation in a physical system. Whether they are present in the\nhuman brain, LLMs, etc. is a further question that can be evaluated only when the criteria are settled.\n123\nSynthese (2024) 203 :15 Page 17 of 32 15\nFig. 7 Realizational interpretation of the vehicle-reading\n(Piccinini, 2015). Hence, the equivalence classes should be determined by some com-\nbination of (i) intrinsic properties of vehicles, (ii) relations between vehicles, and (iii)\ninput–output transformations between states determined by the distribution of vehi-\ncles. While some of these are relational, they still remain fully system-internal and\nindependent of content. They are thus vehicle-properties in the present sense.\nNotably, the realizational vehicle-reading can incorporate the autonomy of lin-\nguistic levels. If SDs individuate equivalence-classes of computational vehicles, their\nautonomy only requires that they do not bear constitutive relations to each other and\nare instead related via separate links. This has a straight-forward correlate in DNNs:\nlayers. Even if the information generalized between layers is obtained via transfor-\nmational abstraction over information encoded in previous layers (see Sect. 4), the\ncomputational vehicles themselves (i.e. nodes of the DNN) still remain separate. The\nautonomy of levels could thus be attained by assigning representations of different lev-\nels to dedicated layers—as has indeed been argued on empirical grounds in BERTology\n(e.g. Hewitt & Manning, 2019; Tenney et al., 2019; Manning et al., 2020).\nThis reading also supports a straight-forward interpretation of structural probing:\nit ﬁnds equivalence-classes of LLM-states based on the probing task. States that the\nprobe classiﬁes in the same way constitute an equivalence-class, which is further\nassimilated to the linguistic category assigned to the probe’s target label. For example,\nthe equivalence-class of states from which a probe outputs the target “NP” would\nbe the model’s NP-representation. By the same token, Hewitt and Manning’s ( 2019)\nprobe captures an equivalence-class of distance relations between two encodings, and\nthis simply is the representation of parse tree distance in BERT.\nHowever, there are fundamental differences between lower-level realizing struc-\ntures and higher-level abstract structures, which forbids their direct assimilation. As\na prominent example, consider the set-theoretical deﬁnition of the operation Merge\nin the minimalist variant of generative theory. Merge is an operation that puts two\nsyntactic objects (words or phrases) together, resulting in a complex phrase with two\nconstituents. Chomsky (1995) further maintains that a set-theoretic formulation should\nbe adopted as its simplest possible formalization. This yields axiom ( 3):\n(3) Merge(A, B) = {A, B }\n123\n15 Page 18 of 32 Synthese (2024) 203 :15\nAt the same time, such analyses are intended to explain human linguistic cognition.\nBut clearly ( 3) does not directly denote a brain-state even on a high level of description:\nit is an abstract set. On behalf of linguistic Platonism (see Sect. 4.3), it has been asserted\nthat such discrepancies between abstract SDs and concrete brain-states make linguistic\nmentalism incoherent (Katz, 1981; Postal, 2003, 2009; Behme, 2015). However, as\nLevine ( 2018, p. 53) observes, Chomsky has also acknowledged this and explicitly\ndenied their direct assimilation:\nWe don’t have sets in our heads. So you have to know that when we develop\na theory about our thinking, about our computation, internal processing and so\non in terms of sets, that it’s going have to be translated into some terms that are\nneurologically realizable.\n(Chomsky 2012, p. 91; also cited in Levine 2018, p. 53)\nThat is, if Merge is deﬁned set-theoretically, understanding its physical realiza-\ntion would ﬁrst require translating it into something else—presumably a genuine\ndescriptive abstraction over brain-states. The idea that linguistic theories are directly\ninterpretable as descriptive abstractions of brain-states would indeed be incoherent,\nand is not Chomsky’s position either. Instead, the mentalist contention is that linguis-\ntic formalisms can somehow aptly capture relevant properties of the cognitive system\nunder discussion.\n13 This amounts to an indirect reading, as covered in Sect. 5.3.\nInitially, it might seem that the situation is different for LLMs, which are not speci-\nﬁed in physical terms (akin to brain-states) but are already mathematically individuated\nin the DNN pipeline (via vectors, matrices, and operations across them). Neverthe-\nless, a comparable problem arises here as well. In Marr’s ( 1982) taxonomy, linguistic\ntheory belongs to the highest computational level; whereas the speciﬁcation of LLMs\nbelongs to the algorithmic level that spells out explicit steps for realizing a computa-\ntion but abstracts away from the lowest implementational level describing the physical\nsubstrate. The challenge for linguistic mentalism concerns linking the computational\nand implementational levels. LLM-interpretation, in turn, aims to link the computa-\ntional level to lower-level algorithms. Similar problems arise for both, given distinct\nmeta-theoretical vocabularies on different levels (c.f. Dunbar, 2019).\nIn sum, the realizational vehicle-reading would commit a category error by directly\nequivocating abstract SDs with equivalence classes of model-states. This problem has\nbeen raised to undermine linguistic mentalism on the whole, but such an accusation\ngoes against Chomsky’s own acknowledgement that their relation must be indirect.\nThat being said, it is not trivial how this indirect relation should be construed. For this\npurpose, I append the meta-theoretical taxonomy with mathematical contents .\n13 For example, central properties of sets include non-associativity and the irrelevance of linear order:\n{x, {y, z}} ̸={ {x, y}, z} and {x, y}={ y, x}. The set-theoretical formalism thus conveys that syntactic\nrepresentations behave non-associatively (marking hierarchical distinctions) and do not take linear infor-\nmation into account. The representations are not “sets in the head”, but instead behave in a set-like manner\nin certain computational respects. (I thank an anonymous reviewer for highlighting this point.)\n123\nSynthese (2024) 203 :15 Page 19 of 32 15\nFig. 8 Indirect realizational vehicle-reading\n5.3 Indirect realizational reading\nEgan ( 2010, 2014, 2018) distinguishes between two functions that determine the\ncomputation implemented by a physical system. The realization function ( fR) maps\nits states to equivalence classes of vehicles. The interpretation function ( fI ) maps\nvehicle-types (determined by fR ) to contents, of which there are two kinds. Cognitive\ncontents assimilate to what I have called “contents”—i.e. external referents. Mathe-\nmatical contents are independent of the system’s environment, and provide abstract\ninterpretations of vehicles and their relations. 14\nMathematical contents have a somewhat intermediate position between contents\nand vehicle-properties in my taxonomy (see Sect. 3). As terminology already suggests,\nEgan treats them as a kind of content. However, they are “narrow”— i.e. independent\nof the system’s environment—and are thus restricted to explaining vehicle-properties.\nAs an example, Egan ( 2010) provides Marr’s ( 1982, p. 337) analysis of early vision\ninvolving the computation of the Laplacean of a Gaussian. The computation allows the\nsystem to detect light intensity changes in its typical ecology; but this is not part of the\nmathematical content, which only concerns operations internal to the system itself.\n15\nNevertheless, it provides an abstract description (rather than e.g. a neural one), and is\nthus irreducible to mere equivalence classes of vehicles.\nSimilarly, linguistic SDs could be treated as specifying mathematical contents\nlinked to computational vehicles. Figure 8 displays this indirect realizational schema.\nUnlike in other readings of “linguistic representation” discussed so far, here the lin-\nguistic status of vehicles is meta-theoretical. This allows maintaining the irreducible\nabstractness of SDs without succumbing to the problems of the Platonist content-\nreading (see Sect. 4.3). Even though mathematical contents are abstract, there is no\nmysterious “grasping” involved any more than between numbers and calculators, or the\nvisual system and Gaussian functions. Figure 9 clariﬁes this: only vehicle-types (deter-\nmined by f\nR ) directly concern the system, and mathematical contents are mapped to\nthem 1–1 via a separate function ( fI ) that only has a theory-internal status.\n14 Egan further maintains that mathematical contents sufﬁce for computational explanation proper. My\npresent discussion is independent of her pragmatism about cognitive contents.\n15 Egan’s reading of Marr contrasts others that take intentional content to be essential for computational\nexplanation (e.g. Burge, 1986). My purpose here is not to evaluate it as a Marr-interpretation, but instead\nto assess its applicability for explaining linguistic representation.\n123\n15 Page 20 of 32 Synthese (2024) 203 :15\nFig. 9 Realization and interpretation functions\nHowever, if linguistic representations are based on separately chosen interpretation\nfunctions, their empirical status is once again endangered. As Facchin ( 2022)s h o w s ,\nmathematical contents are vulnerable to similar triviality problems as structural map-\nping accounts of physical computation. These are based on the possibility of mapping\nany physical states to any abstract structures, as long as there are at least as many of\nthe former as the latter. This was proven for inputless ﬁnite-state automata by Putnam\n(1988), and has since been broadened to cover all computational systems with ﬁnite\nstorage (Sprevak, 2018).\n16 Consequently, linguistic representation cannot be secured\nsimply by there being some f R and fI , the successive application of which to model-\nstates yields SDs. If the model is sufﬁciently complex and the SDs are restricted to a\nﬁnite set (e.g. by maximum tree-depth), such functions can always be devised.\nIn BERTology, structural probing can be treated as discovering a feasible fR based\non the probing task: it ﬁnds a way to group model-states based on their correlation with\nthe probe’s targets. Using again Hewitt and Manning’s ( 2019) probe for illustration,\ndB (hi , hj ) is trained to approximate parse-tree distance between wi and w j (respec-\ntively encoded as hi and hj ). This allows grouping encodings based on their effects\non dB . But the resulting equivalence-classes do not wear linguistic interpretations on\ntheir sleeves. Suppose dB is used to group encodings to vehicle-classes {v1, ..., v n}.\nWhat determines the correct fI for these?\nInitially, it might seem that fI can be based on the probe’s target labels, which\nare transparently related to information in the training set (such as pre-determined\nparses). However, it is crucial not to conﬂate the probe’s interpretation with the model’s\ninterpretation. I am not concerned with the interpretation of the probe’s targets; this is\nsimply assumed to begin with. My problem is the interpretation of vehicle-classes in\nthe model itself, which has never seen those targets.\n16 I am targeting a more speciﬁc case than typical charges against structural mapping accounts: I am\nnot concerned with restricting the set of computing systems to avoid pancomputationalism, or evaluating\nwhether every physical system computes something (c.f. Chalmers, 1995; Piccinini, 2015). Instead, I aim\nto restrict mathematical contents in a class of systems already known to compute (LLMs).\n123\nSynthese (2024) 203 :15 Page 21 of 32 15\nSuppose we said that vehicle-class vi represents linguistic property P if vi is an\nequivalence-class discovered by the probe, the target classes of which are interpreted\nas P. By making the interpretation probe-dependent, this would be backwards with\nrespect to the goal of BERTology: the probe is supposed to ﬁnd pre-existing repre-\nsentations. It is meant to be an experimental tool for discovering vehicle-classes that\nalready represent P, not a component for grounding the representation itself. To avoid\nsuch confusion in explanatory order, the status of the probe should be more modest.\nBut now the triviality problem arises again: if the probe does not ﬁx fI , what does?\nTo deal with this problem, we need to look further into the nature of explanation\nwith mathematical contents in cognitive science and linguistics. Prima facie, triviality\nproblems should arise here as much as in BERTology. However, I highlight a notable\ndifference in explanatory strategies employed in these discipilines. This also grounds\nmy main critique of BERTology: so far, it has focused on how LLMs can be interpreted\nas opposed to what must be included to capture their central properties.\n(Mentalist) linguists assert that certain theories of SDs (i.e. abstract grammars)\nshould be favored over others due to their value for explaining observable linguistic\ncapacities. As Egan ( 2017, p. 155) maintains: “Computational models are proposed\nto explain our manifest success at some cognitive task”. As a toy example, consider\nthe observation that English-speakers can process the sentence John saw Mary .A\ngenerative linguist (of an early generation) could analyze this sentence as ( 4):\n(4) S\nNP\nJohn\nVP\nV\nsaw\nNP\nMary\nThis should also tell something about the cognitive structures underlying the speak-\ners’ linguistic competence. In the present interpretation, ( 4) is the mathematical content\nof a vehicle-class consisting of cognitive states (see Fig. 8). While we do not know what\nthese are, we can still use the placeholder fR of the function that determines them. They\nare subsequently mapped by fI to parts of ( 4) in some manner that respects its syntac-\ntic structure (see Fig. 9). Conversely, the linguist stipulates that some vehicle-classes\nin English-speakers’ cognition are mappable to ( 4)i nt h i sw a y .\nConsider, now, the dilemma that the same vehicle-types (determined by fR ) could\nalso be mapped to different SDs by another interpretation function f ′\nI . For example,\nf ′\nI could assign saw Mary to a single non-decomposable verb, as in ( 5):\n(5) S\nNP\nJohn\nV\nsaw Mary\nGiven that f ′\nI is not ruled out by any ap r i o r iprinciple, the triviality problem arises:\nwhy would English-speakers’ linguistic cognition be better analyzed by ( 4) than ( 5)? It\nseems that the relevant factor lies in its superior explanatory power. Typical candidates\nfor rules generating ( 4)a r e( 6a–b):\n123\n15 Page 22 of 32 Synthese (2024) 203 :15\n(6) a. S → NP VP\nb. NP → N\nc. VP → VN P\nd. N → John | Mary\ne. V → saw\nThis automatically grounds further predictions that can be used to formulate\nlinguistic hypotheses. In particular, another SD ( 7) is expected to be grammatical:\n(7) S\nNP\nMary\nVP\nV\nsaw\nNP\nJohn\nOn the other hand, ( 5) can only be generated via rules such as ( 8), which have no\nimplications for the grammaticality of Mary saw John .\n(8) a. S → NP V\nb. NP → N\nc. N → John\nd. V → saw Mary\nHence, ( 4) makes a correct prediction that ( 5) fails to make. Empirical experimen-\ntation on speakers’ competence with Mary saw John can thereby be used to compare\nthem. Furthermore, this prediction is made fully within abstract linguistic theory, and\ncould not be replicated simply by e.g. looking directly at brain-states. It allows sur-\nrogative reasoning (Swoyer, 1991) by functioning as an explanatory “proxy” for the\nunderlying cognitive states. 17\nThe choice of mathematical content now becomes incorporated into the overall\ntask of theory formation, with customary desiderata including empirical scope, lack\nof incorrect predictions, internal coherence, simplicity, etc. Given the general under-\ndetermination of theory by data, it is expected that multiple candidates may often\nhave equal support at any given time. Theories can also sometimes tap into the same\nunderlying structures in different ways (c.f. Nefdt, 2023, pp. 138–145). But there is\nnothing special about mathematical contents in either respect: these considerations\napply to theory-construction in general, and no detrimental triviality problems arise.\nIf the only criterion for representing mathematical contents was that some interpre-\ntation function exists that maps vehicle-types to them, the triviality problem indeed\nseems insurmountable. However, this does not threaten the actual employment of\nmathematical contents in linguistics or cognitive science. In fact, the same argument\ncould also target Marr’s ( 1982) account of the computation involved in the human\nvisual system. Suppose that clear equivalence-classes of brain-states were found that\nmapped to each step in computing the Laplacean of the Gaussian in early vision. Now,\n17 See Matthews ( 2007) for a partly related approach to propositional attitude ascriptions.\n123\nSynthese (2024) 203 :15 Page 23 of 32 15\ntake those classes of brain-states and devise another interpretation function that maps\nthem to some completely different mathematical contents. Is this a threat to Marr’s\ntheory of vision? Surely not: Marr’s point is not merely that there is some mapping\nfrom brain-states to computing the Laplacean of a Gaussian; he maintains that this\nformulation has special explanatory relevance for understanding the visual system.\nEssentially, the triviality problem arises when mathematical contents are assigned\nsimply because they can be. Avoiding it requires investigating which kinds of math-\nematical contents are actually needed to capture central properties of the system. As\none ﬁnal example for further illustration, we can consider Karlsson’s ( 2006) objection\nto Chomsky’s (1957) recursive treatment of language, on the grounds of a corpus study\nindicating that languages like English, Finnish, and Russian are limited to three clauses\nin center-embedding. Based on this, he proposes that these languages are “ﬁnite-state,\ntype 3 in the Chomsky hierarchy” (Karlsson, 2006, p. 1). Without taking a stance on\nthe dispute as such, I maintain that it must concern the minimum requirements for\ncapturing essential properties of language; not simply possible formalisms.\nSuppose that Karlsson is correct and language is sufﬁciently formalizable as a linear,\nnon-hierarchical system. Would this mean that using phrase-strutural SDs as mathe-\nmatical contents of cognitive states involved in language-processing was impossible?\nNo: we could just take those states and devise f\nI to map them to such SDs. Clearly,\nthis would not falsify Karlsson’s account, which instead concerns what is needed at\nminimum. If linear SDs sufﬁce for this, the mere possibility of hierarchical SDs is\ntheoretically moot. Conversely, if Chomsky was right instead, using hierarchical SDs\nwould be necessary—not merely possible— for linguistic explanation proper.\nLikewise, the challenge for BERTology is to show that certain SDs are not only\navailable in principle but needed for capturing non-negligible properties of LLMs left\nout by other SDs. This would correspond to how cognitive science avoids the triviality\nproblem of mathematical contents: they should manifest some properties that allow\nuniquely robust surrogative reasoning about the underlying structures and operations.\nBy merely ﬁnding some subset of LLM-states that can be mapped to target labels\ninterpreted as SDs, structural probing falls short of this goal: it does not establish that\nthose states (determined by f\nR ) or those SDs (determined by fI ) have any special\nexplanatory relevance compared to indeﬁnitely many other candidates.\n5.4 Summary\nAside of the uninformative identity-reading, the vehicle-reading allows direct or indi-\nrect realizational variants. The ﬁrst commits a category error in directly assimilating\ndistinct levels of analysis. The second is the most promising reading in my estimation,\nbut is challenged by triviality problems familiar from structural mapping theories of\nphysical computation. I argued that these are not detrimental to the use of mathemat-\nical contents in cognitive science and linguistics overall, where the choice between\nalternatives can be grounded in generic considerations of theory-construction. How-\never, they have been insufﬁciently addressed in BERTology. Instead of merely ﬁnding\nsome mappings between model-states and SDs, focus should be on evaluating which\nSDs are actually needed. Table 3 summarizes the three vehicle-readings.\n123\n15 Page 24 of 32 Synthese (2024) 203 :15\nTable 3 Summary of vehicle-readings\nReading Problem for BERTology\nIdentity Uninformative\nDirect realizational Category-errors in assimilating levels\nIndirect realizational Triviality problems in mapping\n6 Ramiﬁcations for BERTology\nTo recap, BERTology is dedicated to linguistic representations in LLMs, and com-\nmonly interprets these via SDs formulated in the generative framework (Sect. 2). My\nmain contention is that such claims are not as theoretically innocuous as they might\ninitially seem. I now turn to some possible ways forward.\nInitially, a simple solution would be to abandon the contentious theoretical assump-\ntions of generative linguistics, such as the autonomy of levels. Other linguistic\nframeworks have rejected these (e.g. Langacker, 1987;C r o f t ,2001; Goldberg, 2006),\nand perhaps BERTology should as well. Without denying that this approach might be\nthe right direction overall, I raise three challenges that arise with it.\nFirst, it contradicts claims manifestly made in the experimental literature, as\ncaptured in Table 1 (Sect. 2). LLMs have been explicitly analyzed via hierarchical\nphrase-structures (Belinkov & Glass, 2019; Hewitt & Manning, 2019; Mueller et al.,\n2022) and as representing “the steps of the traditional NLP pipeline in an interpretable\nand localizable way” (Tenney et al., 2019, p. 4593). This has been directly contrasted\nwith the idea that an LLM would merely be a “giant associational learning machine”\n(Manning et al., 2020, p. 30046). In my reading, such claims deserve to be taken seri-\nously at face value. At least, a more metaphorical reading should not be the default\nstarting-point; it could be defended if literal readings turn out to be untenable.\nSecond, leading non-generative linguistic theories—such as cognitive grammar\n(Langacker, 1987) and construction grammar (Goldberg, 2006)—heavily rely on\nsemantics as replacing formal syntax in driving linguistic analyses. Moreover, these\nperspectives are typically connected to embodied conceptions of meaning as contigu-\nous with sensory-motor cognition. Since LLMs only use textual information, they\nlack similar processes. Their semantic representations— if such exist—must either be\nderived from the input text or arise endogenously from the model’s internal structure\n(these alternatives corresponding to the content- and vehicle-reading, respectively).\nPossible future LLMs with additional sensory-motor grounding might well be fruit-\nfully analyzable via embodied approaches; but current ones do not use sensory-motor\ninformation. Hence, whatever the overall linguistic merits of embodied semantics-\ndriven frameworks may be, they cannot be directly transported to BERTology at the\nmoment.\nThird, the remaining option would be going back to the structuralist notion of SDs\nas distributional generalizations. This idea has indeed gained much recent attention in\nNLP (e.g. Mickus et al., 2020; Brunila & LaViolette, 2022). I have already emphasized\nits inherent discrepancy with the autonomy of levels. Another crucial disunity is found\n123\nSynthese (2024) 203 :15 Page 25 of 32 15\nbetween structuralism’s externalist treatment of linguistic categories on the one hand,\nand linguistic representations as stipulated internal vehicles on the other hand. As\nGastaldi and Pellissier ( 2021) note:\n(...) the distributional hypothesis imparts a radically different direction to lin-\nguistic research [from generativism], where the knowledge produced is not so\nmuch about cognitive agents than about the organization of language. It follows\nthat, understood as a hypothesis, distributionalism constitutes a statement about\nthe nature of language itself , rather than about the capacities of linguistic agents.\n(Gastaldi and Pellissier 2021, p. 570; emphasis in the original)\nDistributionalism does not concern internal representations, and has traditionally\nshunned them explicitly and not at all subtly:\nIt remains for linguists to show, in detail, that the speaker has no ‘ideas’, and\nthat the noise is sufﬁcient\n(Bloomﬁeld, 1936, p. 93)\nThis ﬁts well with anti-representationalist approaches to language, such as the\neliminative connectionism of Rumelhart and McClelland ( 1986). But in order to have\nscientiﬁc import, hypotheses of internal linguistic representations should contrast anti-\nrepresentationalism. Instead, distributionalism ends up effectively conﬂating them.\nBe that as it may, the content-reading could be salvaged at the expense of under-\nlying generative assumptions about phrase-structure, most notably the autonomy of\nlevels. This may well be the right way to go. After all, it would be surprising if lin-\nguistic structures formulated in an explicitly anti-connectionist rule-based framework\nhappened to closely match what data-driven LLMs are doing. That notwithstanding, I\nemphasize that this direction comes at a cost: many proclaimed results of BERTology\nwould need to be re-interpreted as something other than their what manifest literal\nreading suggests. It is unclear what this should be, if the contrast between genuine\nlinguistic representation and mere “associational learning” is still to be retained.\nMoving on to the vehicle-reading, here the main challenge is tackling the triviality\nproblem of interpretational mapping without begging the question. Given that a literal\nassimilation between LLM-states and abstract SDs is ruled out as a category error, a\nless direct relation between these is needed, as provided by Egan’s ( 2010; 2018) notion\nof mathematical content. The challenge then becomes to demonstrate why certain SDs\nare better mathematical contents of LLMs than others. Since structural probing alone\nonly shows that some mapping can be found between LLM-states and SDs, it does not\nyet establish that those SDs have any special explanatorily value.\nBERTology has, in effect, started out by already assuming that the relevant SDs are\nabstract phrase-structures, and then moved on to ﬁnd the best correlates for these in\nLLMs. This methodology leaves out the investigation of whether those SDs are even\nneeded in the ﬁrst place. Despite having set out to discover what kinds of linguistic\nrepresentations are present in LLMs, it ends up presupposing much of the answer.\n18\n18 This matter is partly related to debates in theoretical linguistics concerning the use of pre-established\ncategories in analyzing languages beyond those that originally motivated their stipulation. In particular,\nHaspelmath ( 2010, 2020) has proposed treating each language on its own terms without imposing prior\n123\n15 Page 26 of 32 Synthese (2024) 203 :15\nWhile I see no easy way out of this predicament, my overall contention is\nthat BERTology should adopt a more “LLM-ﬁrst” approach, instead of using SDs\npre-deﬁned for different theoretical purposes (within a decidedly non-connectionist\nframework). As an analogy, Lakoff ( 1990) characterized the “cognitive commitment”\nas grounding linguistic theory in what is independently known about human cognition\nin other disciplines, particularly cognitive psychology.\n19 A corresponding commit-\nment would be useful for LLMs as well: their high-level analysis should be built\naround well-established facts about their algorithmic nature.\nIt is as of yet unclear what exactly an “LLM-ﬁrst linguistics” should look like,\nbut one prospect would be to draw from techniques for mapping DNN-algorithms\nto a human-readable symbolic format. As an example, Weiss et al. ( 2021) present\na programming language called Restricted Access Sequence Processing Language\n(RASP) which models the transformer architecture used in LLMs (V aswani et al.,\n2017). Outside of technical details, the main idea is as follows:\nRASP abstracts away low-level operations into simple primitives, allowing a\nprogrammer to explore the full potential of a transformer without getting bogged\ndown in the details of how these are realized in practice. At the same time, RASP\nenforces the information-ﬂow constraints of transformers, preventing anyone\nfrom writing a program more powerful than they can express.\n(Weiss et al., 2021, p. 11083)\nWeiss et al. provide a RASP-solution to multiple computational problems. These\ninclude the recognizion of Dyck languages consisting of balanced brackets, with clear\nrelevance for recursion. They further show that training a transformer with the num-\nber of layers and attention heads predicted by the RASP-solution attains high task\nperformance, which generally decreases with fewer layers. In addition, RASP can be\nused to predict attention patterns for each input token (i.e. which tokens give the most\ninformation for its encoding). This allows comparing the attention patterns of trained\nmodels to such predictions, as well as inducing the learning of RASP-type patterns by\ndirectly supervising this via the loss function during training.\nFrom the perspective of Marr’s ( 1982) levels, RASP could be seen as either a low-\nlevel computational description or a high-level algorithmic description. In either case,\nit bears a 1–1 relation to lower-level algorithms enacted by a transformer and can\nthus act as its explanatory mechanism by tapping into its underlying causal structure\n(Kaplan, 2011;L e v y ,2013). As Egan ( 2017) emphasizes, computational explanation\nFootnote 18 continued\nassumptions about putatively universal categories (for a critique, see Newmeyer, 2010). But despite manifest\nafﬁnities between these concerns and the interpretation of LLMs, they also differ in important respects.\nEven if SDs differed drastically between languages, this would not yet resolve how LLMs represent these\nlanguage-speciﬁc SDs. My present question is not whether LLMs represent different languages as falling\nunder universal SDs, but how they represent any SDs in the ﬁrst place. That being said, the universality\nof categories in multi-lingual LLMs is an important question in BERTology (Chi et al., 2020), and further\nresearch is needed to better understand it in light of the issues I have discussed here.\n19 A potential rejoinder to Lakoff could be that since so little is still known about fundamental properties of\ncognition, even “language-ﬁrst” approaches could help uncover some aspects of it, at least in conjunction\nwith other disciplines (c.f. Nefdt, 2023, pp. 186–196). But even if this is true about human cognition, it\ndoes not apply to LLMs in the same way: unlike humans, they are already algorithmically understood.\n123\nSynthese (2024) 203 :15 Page 27 of 32 15\nTable 4 Summary of six readings of “linguistic representation” and their problems\nType of reading V ariant Challenge\nContent-reading Realizational Autonomy of levels\nFictionalist Autonomy of levels\nPlatonist Quinean indeterminacy between SDs\nV ehicle-reading Identity Uninformative\nDirect realizational Category errors in assimilating levels\nIndirect realizational Triviality problems in mapping\noften departs from such strict mechanistic requirements. Still, when these requirements\nare well-established on independent grounds, they can be used in the comparison of\ndifferent candidates for high-level descriptions. RASP provides an example of how\nthis can be done for transformers, including in tasks with linguistic relevance (such\nas recognizing Dyck languages). It grounds concrete restrictions about which tasks\nare possible in principle and which are precluded on architectural grounds (the latter\nincluding e.g. arbitrary loops). Through further connections to empirically assessable\nhypotheses, it can ground inferences about the computation implemented.\nWeiss et al.’s ( 2021) experiments only concern simple transformer architectures\nand are far from being directly applicable to LLMs. Nevertheless, their framework\nprovides at least a glimpse of an alternative to the standard BERTology paradigm exem-\npliﬁed by structural probing. Rather than simply ﬁnding a mapping from model-states\nto high-level analyses, Weiss et al. begin with a theoretically grounded computa-\ntional description that transparently maps to the model. Since links between levels of\nexplanation are explicit and clear, triviality problems never arise.\n7 Conclusions and future work\nOne prominent notion in contemporary NLP is that LLMs bring to question the gen-\nerative approach to linguistic theory. 20 Somewhat surprisingly, this is not the driving\nidea behind BERTology. Instead, LLMs are readily interpreted via abstract SDs hypo-\nthetically represented by model-internal states. BERTology thus embodies a newfound\nrepresentational realism in connectionist NLP , shifting from an eliminative to an imple-\nmentational perspective. Nevertheless, I have argued that ambiguities concerning the\ninterpretation of “linguistic representation” pose major difﬁculties. Table 4 collects\nall six readings and their main challenges.\nThe content-reading ﬂies in the face of the autonomy of linguistic levels, which is a\nfoundational principle of generative linguistics. The vehicle-reading allows retaining\nthe autonomy of levels but brings about different complications. Assimilating LLM-\nstates directly with abstract SDs would be a logical category error; but a more indirect\nreading raises triviality problems familiar from the philosophy of computation. In sum,\n20 This was recently forcefully argued in a manuscript by Steven Piantadosi: “Modern language models\nrefute Chomsky’s approach to language” (Lingbuzz, 2023: https://lingbuzz.net/lingbuzz/007180).\n123\n15 Page 28 of 32 Synthese (2024) 203 :15\nsome readings threaten to make representational hypotheses trivially true, while others\nthreaten to make them trivially false. The challenge is, thus, how to ﬁx a non-trivial\nmiddle-ground without begging the question.\nBy the same token, the problem also has repercussions for anti-representationalist\ninterpretations of LLMs. If the notion of “representation” turned out to be irresolv-\nably unclear, this would motivate an anti-representationalist fallback position that\nsimply removes the notion from the theoretical vocabulary altogether.\n21 In contrast,\na stronger form of anti-representationalism would rely on “representation” having\na clear interpretation, but reject its applicability to LLMs. As with representation-\nalist accounts, evaluating these alternatives hinges on scrutinizing the underlying\nconditional question: what would be required for LLMs to represent SDs?\nOn the face of it, the situation is not unique to BERTology but plagues all appli-\ncations of linguistics to cognitive science: it remains unclear how different levels of\nanalysis should be linked (c.f. Poeppel & Embick, 2005). Nevertheless, I maintain that\nEgan’s (2010, 2014, 2018) account of computational explanation allows a reasonable\ninterpretation of linguistic mentalism, where abstract SDs constitute mathematical\ncontents that describe a system’s computational properties on a high level.\n22 The\nexplanatory value of such mathematical contents resides in their use as explanatory\n“proxies” for surrogative reasoning (Swoyer, 1991). Triviality problems are not a par-\nticular threat from this perspective: even though data always underdetermines theory,\nthis is a general aspect of all analysis and not speciﬁc to mathematical contents.\nIn contrast, BERTology has so far focused on methods that merely aim to ﬁnd some\nmapping from LLM-states to SDs. This leaves a central question unexamined: are\nthose SDs actually needed for describing the LLM on a high level of abstraction? The\nmain basis for selecting between mathematical contents is thus missing.\nTo alleviate the problem, I propose that BERTology should make more use of low-\nlevel computational (or high-level algorithmic) frameworks that reliably capture the\nDNN-pipeline (e.g. Weiss et al., 2021). Here, the choice of mathematical content can\nbe based on already-known facts about the LLM, given the explicit nature of the\nmapping between different levels of abstraction. While comparable techniques are\nusually unavailable in the study of human cognition, the algorithmic transparency of\nLLMs allows building such mappings in a reliable way—at least in principle. The goal\nshould not be to ﬁnd those LLM-states that best map to some SDs, but the converse:\nto ﬁnd those SDs that best explain the LLM.\nAcknowledgements I thank Anna-Mari Rusanen, Otto Lappi, and Jami Pekkanen for valuable discussions,\nand anonymous reviewers for helpful comments.\nFunding Open Access funding provided by University of Helsinki (including Helsinki University Central\nHospital). This project was funded by the Academy of Finland (decision number 350775). There are no\nﬁnancial or non-ﬁnancial conﬂicts of interest related to this work.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which\npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give\nappropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence,\n21 See Favela and Machery ( 2023) for an evaluation of related matters in psychology and neuroscience.\n22 While this account gives linguistic mentalism a reasonable theoretical standing ( pace Postal, 2009;\nBehme, 2015), it remains uncommitted to the truth of any particular linguistic theory—mentalist or other.\n123\nSynthese (2024) 203 :15 Page 29 of 32 15\nand indicate if changes were made. The images or other third party material in this article are included\nin the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If\nmaterial is not included in the article’s Creative Commons licence and your intended use is not permitted\nby statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the\ncopyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/ .\nReferences\nAdger, D. (2022). What are linguistic representations? Mind & Language, 37 (2), 248–260.\nBehme, C. (2015). Is the ontology of biolinguistics coherent? Language Sciences, 47 , 32–42.\nBelinkov, Y ., & Glass, J. (2019). Analysis methods in neural language processing: A survey. Transactions\nof the Association for Computational Linguistics, 7 , 49–72.\nBenacerraf, P . (1973). Mathematical truth. Journal of Philosophy, 70 (19), 661–679.\nBlaho, S. (2007). The syntax of phonology: A radically substance-free approach (PhD Thesis). University\nof Tromsø.\nBloomﬁeld, L. (1933). Language. Henry Holt.\nBloomﬁeld, L. (1936). Language or ideas. Language, 12(2), 89–95.\nBoone, W., & Piccinini, G. (2016). Mechanistic abstraction. Philosophy of Science, 83 (5), 686–697.\nBrentano, F. (1874/1911). Psychology from an empirical standpoint. Routledge and Kegan Paul.\nBrunila, M., & LaViolette, J. (2022). What company do words keep? Revisiting the distributional semantics\nof J.R. Firth & Zellig Harris. In Proceedings of the 2022 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies (pp. 4403–4417).\nBuckner, C. (2018). Empiricism without magic: Transformational abstraction in deep convolutional neural\nnetworks. Synthese, 195 (12), 5339–5372.\nBurge, T. (1986). Individualism and psychology. The Philosophical Review, 95 (1), 3–45.\nCappelen, H., & Dever, J. (2021). Making AI intelligible: Philosophical foundations . Oxford University\nPress.\nChalmers, D. J. (1995). On implementing a computation. Minds and Machines, 4 , 391–402.\nChi, E.A., Hewitt, J. & Manning, C.D. (2020). Finding universal grammatical relations in multilingual\nBERT. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics\n(pp. 5564–5577).\nChomsky, N. (1957). Syntactic structures. Mouton.\nChomsky, N. (1965). Aspects of the theory of syntax . MIT Press.\nChomsky, N. (1975). The logical structure of linguistic theory . Plenum press.\nChomsky, N. (1980). Rules and representations . Columbia University Press.\nChomsky, N. (1986). Knowledge of language . Praeger Publications.\nChomsky, N. (1995). The minimalist program . MIT Press.\nChomsky, N. (2012). The science of language . Cambridge University Press.\nChomsky, N., & Halle, M. (1968). The sound pattern of English .H a r p e r&R o w .\nCoenen, A., Reif, E., Y uan, A., Kim, B., Pearce, A., Viégas, F. & Wattenberg, M. (2019). Visualizing and\nmeasuring the geometry of BERT. In Proceedings of the 33rd Conference on Neural Information\nProcessing Systems (pp. 8592–8600).\nCollins, J. (2014). Representations without representa: Content and illusion in linguistic theory. In P .\nStalmaszczyk (Ed.), Semantics and beyond: Philosophical and linguistic inquiries (p. 2764). De\nGruyter.\nCollins, J. (2023). Internalist priorities in a philosophy of words. Synthese, 201 (3), 110.\nCollins, J., & Rey, G. (2021). Chomsky and intentionality. In N. Allott, T. Lohndal, & G. Rey (Eds.), A\ncompanion to Chomsky (pp. 488–502). Wiley.\nCroft, W. A. (2001). Radical construction grammar: Syntactic theory in typological perspective . Oxford\nUniversity Press.\nDanilevsky, M., Qian, K., Aharonov, R., Katsis, Y ., Kawas, B. & Sen, P . (2020). A survey of the state of\nexplainable AI for natural language processing. In Proceedings of the 1st Conference of the Asia-\nPaciﬁc Chapter of the Association for Computational Linguistics and the 10th International Joint\nConference on Natural Language Processing (pp. 447–459).\nDennett, D. C. (1991). Consciousness explained . Little Brown and Company.\n123\n15 Page 30 of 32 Synthese (2024) 203 :15\nDevlin, J., Chang, M.-W., Lee, K. & Toutanova, K. (2019). BERT: Pre-training of deep bidirectional trans-\nformers for language understanding. In Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics (pp. 4171–4186).\nDretske, F. I. (1981). Knowledge and the ﬂow of information . The MIT Press.\nDunbar, E. (2019). Generative grammar, neural networks, and the implementational mapping problem:\nResponse to Pater. Language, 95 (1), e87–e98.\nDupre, G. (2022). Georges Rey’s representation of language. BJPS Review of Books, , Retrieved from\nhttps://www.thebsps.org/reviewofbooks/dupre-on-rey/\nEgan, F. (2010). Computation models: A modest role for content. Studies in History and Philosophy of\nScience, 41 (3), 253–259.\nEgan, F. (2014). How to think about mental content. Philosophical Studies, 170 (1), 115–135.\nEgan, F. (2017). Function-theoretic explanation and the search for neural mechanisms. In D. Kaplan (Ed.),\nExplanation and integration in mind and brain science (pp. 145–163). Oxford University Press.\nEgan, F. (2018). The nature and function of content in computational models. In M. Sprevak & M. Colombo\n(Eds.), The Routledge handbook of the computational mind (pp. 247–258). Routledge.\nFacchin, M. (2022). Troubles with mathematical contents. Philosophical Psychology, 5 , 1–24.\nFavela, L. H., & Machery, E. (2023). Investigating the concept of representation in the neural and\npsychological sciences. Frontiers in Psychology, 5 , 14.\nFodor, J.A. (1981). Some notes on what linguistics is about. N. Block (Ed.), Readings in philosophy of\npsychology, vol. II (pp. 197–207).\nFodor, J. A. (1990). A theory of content and other essays . MIT Press.\nGastaldi, J. L., & Pellissier, L. (2021). The calculus of language: Explicit representation of emergent\nlinguistic structure through type-theoretical paradigms. Interdisciplinary Science Reviews, 46 (4), 569–\n590.\nGleitman, L. (2021). Language as a branch of psychology: Chomsky and cognitive science. In N. Allott, T.\nLohndal, & G. Rey (Eds.), A companion to Chomsky (pp. 109–122). Wiley.\nGoldberg, A. E. (2006). Constructions at work: The nature of generalization in language . Oxford University\nPress.\nHarris, Z. S. (1951). Methods in structural linguistics . The University of Chicago Press.\nHaspelmath, M. (2010). Comparative concepts and descriptive categories in crosslinguistic studies.\nLanguage, 86 (3), 663–687.\nHaspelmath, M. (2020). Human linguisticality and the building blocks of languages. Frontiers in Psychology,\n10, 3056.\nHewitt, J., & Manning, C.D. (2019). A structural probe for ﬁnding syntax in word representations. In Pro-\nceedings of the 2019 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, V olume 1 (Long and Short Papers) (pp. 4129–4138).\nImmer, A., Hennigen, L.T., Fortuin, V . & Cotterell, R. (2022). Probing as quantifying inductive bias.\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (pp.\n1839–1851).\nIosad, P . (2017). A substance-free framework for phonology: An analysis of the Breton dialect of Bothoa .\nEdinburgh University Press.\nJackson, F. (1977). Perception: A representative theory . Cambridge University Press.\nJawahar, G., Sagot, B. & Seddah, D. (2019). What does BERT learn about the structure of language?\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (pp.\n3651–3657).\nJelinek, F. (2005). Some of my best friends are linguists. Language Resources and Evaluation, 39 (1), 25–34.\nKaplan, D. (2011). Explanation and description in computational neuroscience. Synthese, 183(3), 339–373.\nKarlsson, F. (2006). Recursion in natural languages. In Advances in Natural Language Processing, 5th\nInternational Conference on NLP , FinTAL 2006 (p. 1).\nKatz, J. (1981). Language and other abstract objects . Rowman and Littleﬁeld.\nKovaleva, O., Romanov, A., Rogers, A. & Rumshisky, A. (2019). Revealing the dark secrets of BERT. In\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the\n9th International Joint Conference on Natural Language Processing (pp. 4365–4374).\nKripke, S. (1980). Naming and necessity . Harvard University Press.\nKulmizev, A., & Nivre, J. (2022). Schrödinger’s tree-on syntax and neural language models. Frontiers in\nArtiﬁcial Intelligence, 5 , 85.\n123\nSynthese (2024) 203 :15 Page 31 of 32 15\nKulmizev, A., Ravishankar, V ., Abdou, M. & Nivre, J. (2020). Do neural language models show prefer-\nences for syntactic formalisms? In Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics (pp. 4077–4091).\nKuokkanen, J. (2022). V ertical-horizontal distinction in resolving the abstraction, hierarchy, and generality\nproblems of the mechanistic account of physical computation. Synthese, 200 (3), 247.\nKuznetsov, I., & Gurevych, I. (2020). A matter of framing: The impact of linguistic formalism on probing\nresults. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing\n(pp. 171–182).\nLakoff, G. (1990). The invariance hypothesis: Is abstract reason based on imageschemas? Cognitive\nLinguistics, 1 (1), 39–74.\nLangacker, R. W. (1987). Foundations of cognitive grammar , volume 1, theoretical prerequisites . Stanford\nUniversity Press.\nLasri, K., Pimentel, T., Lenci, A., Poibeau, T. & Cotterell, R. (2022). Probing for the usage of grammatical\nnumber. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nV olume 1: Long Papers (pp. 8818–8831).\nLaurence, S. (2003). Is linguistics a branch of psychology? In A. Barber (Ed.), Epistemology of language\n(pp. 69–106). Oxford University Press.\nLevine, R. (2018). ‘Biolinguistics’: Some foundational problems. In C. Behme & M. Neef (Eds.), Essays\non linguistic realism (pp. 21–60). John Benjamins Publishing Company.\nLevy, A. (2013). Three kinds of new mechanism. Biology and Philosophy, 28 (1), 99–114.\nLewis, D. (1970). How to deﬁne theoretical terms. Journal of Philosophy, 67 (13), 426–446.\nLi, J., Cotterell, R. & Sachan, M. (2022). Probing via prompting. In Proceedings of the 2022 Conference\nof the North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies (pp. 1144–1157).\nLi, L., Ma, R., Guo, Q., Xue, X. & Qiu, X. (2020). BERT-A TTACK: Adversarial attack against BERT using\nBERT. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing\n(EMNLP) (pp. 6193–6202).\nLinzen, T., & Baroni, M. (2021). Syntactic structure from deep learning. Annual Review of Linguistics, 7 ,\n195–212.\nMadabushi, H.T., Romain, L., Divjak, D. & Milin, P . (2020). CXGBERT: BERT meets construction\ngrammar. In Proceedings of the 28th International Conference on Computational Linguistics (pp.\n4020–4032).\nManning, C. D., Clark, K., & Hewitt, J. (2020). Emergent linguistic structure in artiﬁcial neural networks\ntrained by self-supervision. PNAS, 117 (48), 30046–30054.\nMarcus, G. F. (1998). Rethinking eliminative connectionism. Cognitive Psychology, 37 (3), 243–282.\nMarr, D. (1982). Vision. W.H. Freeman and Company.\nMatthews, R. J. (2007). The measure of mind: Propositional attitudes and their attribution . Oxford\nUniversity Press.\nMcCoy, T., Frank, R., & Linzen, T. (2020). Does syntax need to grow on trees? Sources of hierarchical\ninductive bias in sequence-to-sequence networks. Transactions of the Association for Computational\nLinguistics, 8 , 125–140.\nMcCoy, T., Pavlick, E. & Linzen, T. (2019). Right for the wrong reasons: Diagnosing syntactic heuristics\nin natural language inference. In Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics (pp. 3428–3448).\nMickus, T., Paperno, D., Constant, M. & van Deemter, K. (2020). What do you mean, BERT? Assessing\nBERT as a distributional semantics model. In Proceedings of the Society for Computation in Linguistics\n(pp. 350–361).\nMiller, P . H. (1999). Strong generative capacity: The semantics of linguistic formalism . CSLI Publications.\nMillikan, R. G. (1993). Content and vehicle. In N. Eilan, R. McCarthy, & B. Brewer (Eds.), Spatial\nrepresentation (pp. 256–268). Blackwell.\nMillikan, R. G. (2017). Beyond concepts: Unicepts, language, and natural information . Oxford University\nPress.\nMueller, A., Frank, R., Linzen, T., Wang, L. & Schuster, S. (2022). Coloring the blank slate: Pre-training\nimparts a hierarchical inductive bias to sequence-to-sequence models. In Findings of the Association\nfor Computational Linguistics: ACL 2022 (pp. 1352–1368).\n123\n15 Page 32 of 32 Synthese (2024) 203 :15\nNadeem, M., Bethke, A. & Reddy, S. (2020). StereoSet: Measuring stereotypical bias in pretrained language\nmodels. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing (pp. 5356–5371).\nNeander, K. (2017). A mark of the mental: A defence of informational teleosemantics . MIT Press.\nNefdt, R. M. (2023). Language, science, and structure: A journey into the philosophy of linguistics . Oxford\nUniversity Press.\nNewmeyer, F. (2010). On comparative concepts and descriptive categories: A reply to Haspelmath.\nLanguage, 86 (3), 688–695.\nOdden, D. (2013). Formal phonology. Nordlyd, 40 (1), 249–273.\nOpenAI (2023). GPT-4 technical report (Tech. Rep.).\nOtt, D. (2017). Strong generative capacity and the empirical base of linguistic theory. Frontiers in\nPsychology, 7,8 .\nPater, J. (2019). Generative linguistics and neural networks at 60: Foundation, friction, and fusion. Language,\n95(1), e41–e74.\nPennington, J., Socher, R. & Manning, C.D. (2014). GloV e: Global vectors for word representation. In\nProceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)\n(pp. 1532–1543).\nPiccinini, G. (2015). Physical computation: A mechanistic account . Oxford University Press.\nPinker, S., & Price, A. (1988). On language and connectionism: Analysis of a parallel distributed processing\nmodel of language acquisition. Cognition, 28 (1–2), 73–193.\nPoeppel, D., & Embick, D. (2005). Deﬁning the relation between linguistics and neuroscience. In A. Cutler\n(Ed.), Twenty-ﬁrst century psycholinguistics: Four cornerstones (pp. 1–16). Lawrence and Erlbaum\nAssociates.\nPostal, P . (2003). Remarks on the foundations of linguistics. The Philosophical Forum, 34 (3–4), 233–252.\nPostal, P . (2009). The incoherence of Chomsky’s ‘biolinguistic’ ontology. Biolinguistics, 3 (1), 104–123.\nPutnam, H. (1988). Representation and reality . MIT Press.\nQuine, W. V . O. (1970). Methodological reﬂections on current linguistic theory. Synthese, 21 , 386–398.\nRey, G. (2020). Representation of language: Philosophical issues in a Chomskyan linguistics . Oxford\nUniversity Press.\nRogers, A., Kovaleva, O., & Rumshisky, A. (2020). A primer in BERTology: What we know about how\nBERT works. Transactions of the Association for Computational Linguistics, 8 , 842–866.\nRumelhart, D. E., & McClelland, J. L. (1986). On learning the past tenses of English verbs. In J. L.\nMcClelland, D. E. Rumelhart, & T. P . R. Group (Eds.), Parallel distributed processing: Explorations\nin the microstructure of cognition: V ol. 2. psychological and biological models (pp. 216–271). MIT\nPress.\nSennrich, R., Haddow, B. & Birch, A. (2016). Neural machine translation of rare words with subword units.\nIn Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (V olume\n1: Long Papers) (pp. 1715–1725).\nSmith, B. C. (2006). Why we still need knowledge of language. Croatian Journal of Philosophy, 6 (3),\n431–456.\nSoler, A.G., & Apidianaki, M. (2020). BERT knows Punta Cana is not just beautiful, it’s gorgeous: Rank-\ning scalar adjectives with contextualized representations. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing (pp. 7371–7385).\nSprevak, M. (2018). Triviality arguments about computational implementation. In M. Sprevak & M.\nColombo (Eds.), Routledge handbook of the computational mind (pp. 175–191). Routledge.\nSwoyer, C. (1991). Structural representation and surrogative reasoning. Synthese, 87 (3), 449–508.\nTenney, I., Das, D. & Pavlick, E. (2019). BERT rediscovers the classical NLP pipeline. In Proceedings of\nthe 57th Annual Meeting of the Association for Computational Linguistics (pp. 4593–4601).\nV aswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Polosukhins, I. (2017). Attention\nis all you need. In Proceedings of the 31st International Conference on Neural Information Processing\n(pp. 6000–6010).\nWeiss, G., Goldberg, Y . & Yahav, E. (2021). Thinking like transformers. In Proceedings of the 38th\ninternational conference on machine learning (pp. 11080–11090).\nPublisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps\nand institutional afﬁliations.\n123",
  "topic": "Philosophy of language",
  "concepts": [
    {
      "name": "Philosophy of language",
      "score": 0.8342968225479126
    },
    {
      "name": "Ambiguity",
      "score": 0.7987133264541626
    },
    {
      "name": "Interpretation (philosophy)",
      "score": 0.7771743535995483
    },
    {
      "name": "Representation (politics)",
      "score": 0.6897302865982056
    },
    {
      "name": "Linguistics",
      "score": 0.6835496425628662
    },
    {
      "name": "Cognitive linguistics",
      "score": 0.5623358488082886
    },
    {
      "name": "Philosophy of science",
      "score": 0.5451710224151611
    },
    {
      "name": "Theoretical linguistics",
      "score": 0.5306141972541809
    },
    {
      "name": "Epistemology",
      "score": 0.5023159980773926
    },
    {
      "name": "Field (mathematics)",
      "score": 0.4942038357257843
    },
    {
      "name": "Linguistic description",
      "score": 0.47737953066825867
    },
    {
      "name": "Cognition",
      "score": 0.4663851261138916
    },
    {
      "name": "Ordinary language philosophy",
      "score": 0.4183102548122406
    },
    {
      "name": "Phrase",
      "score": 0.4157971739768982
    },
    {
      "name": "Sociology",
      "score": 0.3709084987640381
    },
    {
      "name": "Metaphysics",
      "score": 0.36621689796447754
    },
    {
      "name": "Computer science",
      "score": 0.35426050424575806
    },
    {
      "name": "Cognitive science",
      "score": 0.32876428961753845
    },
    {
      "name": "Psychology",
      "score": 0.2918950319290161
    },
    {
      "name": "Philosophy",
      "score": 0.27122926712036133
    },
    {
      "name": "Mathematics",
      "score": 0.17216506600379944
    },
    {
      "name": "Western philosophy",
      "score": 0.11531251668930054
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Pure mathematics",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I133731052",
      "name": "University of Helsinki",
      "country": "FI"
    }
  ]
}