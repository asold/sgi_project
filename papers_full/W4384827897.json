{
  "title": "Learning Risk Factors from App Reviews: A Large Language Model Approach for Risk Matrix Construction",
  "url": "https://openalex.org/W4384827897",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2490398054",
      "name": "Vitor Mesaque Alves de Lima",
      "affiliations": [
        "Universidade Federal de Mato Grosso do Sul"
      ]
    },
    {
      "id": "https://openalex.org/A2614899273",
      "name": "Jacson Rodrigues Barbosa",
      "affiliations": [
        "Universidade Federal de Goiás"
      ]
    },
    {
      "id": "https://openalex.org/A1984765312",
      "name": "Ricardo Marcondes Marcacini",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3216278056",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W3003654666",
    "https://openalex.org/W3033695544",
    "https://openalex.org/W2802301063",
    "https://openalex.org/W2549738163",
    "https://openalex.org/W2074975950",
    "https://openalex.org/W6758394642",
    "https://openalex.org/W3006185978",
    "https://openalex.org/W2945660265",
    "https://openalex.org/W2756832710",
    "https://openalex.org/W3173617765",
    "https://openalex.org/W2948678706",
    "https://openalex.org/W4309399939",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W4321013654",
    "https://openalex.org/W4292213411",
    "https://openalex.org/W6638951742",
    "https://openalex.org/W2104396442",
    "https://openalex.org/W2909755202",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4312205996",
    "https://openalex.org/W4301393026",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4291213652",
    "https://openalex.org/W2909308912"
  ],
  "abstract": "<title>Abstract</title> <bold>Context.</bold> Analyzing mobile app reviews is essential for identifying trends and issue patterns that impact user experience and app reputation in app stores. A risk matrix provides a simple and intuitive way to prioritize software maintenance actions to reduce negative ratings. However, the manual construction of a risk matrix is time-consuming, and stakeholders work to understand the context of risks due to varied descriptions and review volume. <bold>Objective. </bold>There is a need for machine learning-based methods to extract risks and classify their priority. Existing studies have automated risk matrix generation in software development but have not explored app reviews or utilized Large Language Models (LLMs). <bold>Method. </bold>To address this gap, we propose using recent LLMs, specifically the OPT model, to automatically construct a risk matrix by extracting information from app reviews, such as features and bugs. We conduct experimental evaluations using reviews from eight mobile apps, generating risk matrices and comparing them with annotated reference matrices. <bold>Results. </bold>Results demonstrate that OPT models generate competitive risk matrices with proper prompt optimization. <bold>Conclusions.</bold> Our contributions include a dynamic and automatic prompt generation approach for customized instructions, allowing accurate and automated review analysis. We also develop instructions to identify risk severity using zero-shot learning. Additionally, we evaluate how OPT models compare to proprietary language models like GPT, showing the feasibility of LLMs in resource-constrained and sensitive contexts. This study represents a significant step toward improving software maintenance and feature prioritization. Mathematics Subject Classification (2020) MSC 68T07 · MSC 68T50 · MSC 68N01 · MSC 68T35",
  "full_text": "Learning Risk Factors from App Reviews: A Large\nLanguage Model Approach for Risk Matrix\nConstruction\nVitor Mesaque Alves de Lima  (  vitor.lima@ufms.br )\nFederal University of Mato Grosso do Sul https://orcid.org/0000-0001-8721-2855\nJacson Rodrigues Barbosa \nGoias Federal University https://orcid.org/0000-0002-4837-5632\nRicardo Marcondes Marcacini \nUniversity of São Paulo https://orcid.org/0000-0002-2309-3487\nResearch Article\nKeywords: Opinion Mining, App Reviews, Issue Detection, Issue Prioritization, Risk Matrix\nPosted Date: July 19th, 2023\nDOI: https://doi.org/10.21203/rs.3.rs-3182322/v1\nLicense:   This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nLearning Risk Factors from App Reviews: A\nLarge Language Model Approach for Risk Matrix\nConstruction\nVitor Mesaque Alves de Lima · Jacson\nRodrigues Barbosa · Ricardo Marcondes\nMarcacini\nReceived: date / Accepted: date\nAbstract Context. Analyzing mobile app reviews is essential for identify-\ning trends and issue patterns that impact user experience and app re putation\nin app stores. A risk matrix provides a simple and intuitive way to priori-\ntize software maintenance actions to reduce negative ratings. However, the\nmanual construction of a risk matrix is time-consuming, and stakeholder s\nwork to understand the context of risks due to varied descriptions an d re-\nview volume. Objective. There is a need for machine learning-based methods\nto extract risks and classify their priority. Existing studies h ave automated\nrisk matrix generation in software development but have not explored ap p re-\nviews or utilized Large Language Models (LLMs). Method. To address this\ngap, we propose using recent LLMs, speciﬁcally the OPT model, to automat -\nically construct a risk matrix by extracting information from app revi ews,\nsuch as features and bugs. We conduct experimental evaluations using r e-\nviews from eight mobile apps, generating risk matrices and comparing th em\nwith annotated reference matrices. Results. Results demonstrate that OPT\nmodels generate competitive risk matrices with proper prompt optim ization.\nConclusions. Our contributions include a dynamic and automatic prompt\ngeneration approach for customized instructions, allowing accurate and au to-\nmated review analysis. We also develop instructions to identify ri sk severity\nVitor Mesaque Alves de Lima\nFaculty of Computing (FACOM) - Federal University of Mato Gr osso do Sul (UFMS)\nUniversity City, Trˆ es Lagoas, 79613000, Mato Grosso do Sul , Brazil\nE-mail: vitor.lima@ufms.br\nJacson Rodrigues Barbosa\nInstitute of Informatics (INF) - Goias Federal University ( UFG)\nUniversity City, Goiˆ ania,74690900, Goias, Brazil\nE-mail: jacson@inf.ufg.br\nRicardo Marcondes Marcacini\nInstitute of Mathematics and Computer Sciences (ICMC) - Uni versity of S˜ ao Paulo (USP)\nUniversity City, S˜ ao Carlos, 13566590, S˜ ao Paulo, Brazil\nE-mail: ricardo.marcacini@usp.br\n2 Vitor Mesaque Alves de Lima et al.\nusing zero-shot learning. Additionally, we evaluate how OPT models com pare\nto proprietary language models like GPT, showing the feasibility of LLM s in\nresource-constrained and sensitive contexts. This study repres ents a signiﬁcant\nstep toward improving software maintenance and feature prioritizati on.\nKeywords Opinion Mining · App Reviews · Issue Detection · Issue\nPrioritization ·Risk Matrix\nMathematics Subject Classiﬁcation (2020) MSC 68T07 ·MSC 68T50 ·\nMSC 68N01 ·MSC 68T35\n1 Introduction\nThe analysis of mobile app reviews enables the identiﬁcation of trends and\nissue patterns that can aﬀect user experience and app reputation in app stores\n(Genc-Nayebi and Abran, 2017). Based on this analysis, developers can pri-\noritize bug ﬁxes, add requested features, and respond to user compl aints to\nimprove app quality and increase positive ratings. To achieve this, i t is neces-\nsary to link user feedback extracted from reviews with app developm ent and\nmaintenance practices (Araujo et al., 2021).\nA simple and intuitive way to organize and prioritize actions for softwar e\nmaintenance, aiming to reduce negative ratings, is through a risk matri x (Xi-\naosong et al., 2009; Pilliang et al., 2022). This matrix consists of a graphical\nrepresentation where risks are positioned on a Cartesian plane based on t heir\nprobability of occurrence and impact/severity, as illustrated in Fi gure 1. Risks\nare classiﬁed according to their importance and potential to harm app qual -\nity. Thus, it assists software engineering professionals in identi fying the most\ncritical areas that require prioritized attention. However, manual con struction\nof a risk matrix often consumes a signiﬁcant amount of time as stakeholders\n(Paltrinieri et al., 2019), such as project managers and product owners, need\nhelp understanding the context of risks recorded by the developme nt team.\nFor example, using diﬀerent descriptions to report the same risk an d the large\nvolume of reviews make risk assessment challenging. Therefore, the re is a need\nfor automatic machine learning-based methods to extract risks from rev iews\nand classify their priority.\nSome initiatives in the literature already automate risk matrix genera-\ntion using machine learning methods. For example, Chaouch et al. (2019) and\nHammad and Inayat (2018) use Scrum with risk matrix, but with manual\nassessments. A recent study proposed using language models (e.g., BE RT)\nand data clustering methods (e.g., K-Means) to automate risk matrix ge ner-\nation in software development projects (Pilliang et al., 2022). However, such\nstudies have not yet explored the app review domain and rely on manually\nconstructed resources, such as a vocabulary or lexicon to deﬁne risk priority.\nAnother limitation is the lack of a step to extract app features from revi ews,\nwhich is crucial for the development team.\nLearning Risk Factors from App Reviews using Large Language Models 3\nFig. 1 Illustration of a risk matrix for app reviews.\nTo address this gap, we argue that recent Large Language Models (LLMs)\nare promising to automatically construct a risk matrix using informati on ex-\ntracted from app reviews, such as features and bugs, organized according t o\nthe probability of occurrence and impact/severity to the app ratings. LLMs\ncan analyze and understand complex contextual information present in re view\ntexts due to their extensive pre-training corpus (Ross et al., 2023).\nIn this context, we raise the following central research question: how do we\nlearn risk factors from app reviews using Large Language Models and prioritize\napp reviews and anticipate to mitigate risks?\nThis paper presents a novel approach for generating Risk Matrix from App\nReviews using Large Language Models, speciﬁcally the OPT model (Open P re-\ntrained Transformer Language Models) (Zhang et al., 2022). While large-scale\nlanguage models like GPT (Brown et al., 2020) are widely used, we opted for\nOPT, an open-access language model. By providing speciﬁc instructi ons to\nthe model through prompt engineering, it is possible to direct its at tention to\nparticular aspects of reviews, such as app features mentioned by user s and the\nevaluation of risks’ impact/severity associated with the apps. Our con tribu-\ntions are three-fold:\n1. Dynamic and automatic prompt generation: We introduce an ap-\nproach that enables the creation of customized instructions for each re-\nview to be analyzed, allowing the OPT model to extract app features as\ndescribed by users in natural language. This enables more accurate and\nautomated review analysis through few-shot learning, resulting in feat ure\nextraction with limited labeled data.\n4 Vitor Mesaque Alves de Lima et al.\n2. Prompt instructions to identify risk impact: We develop suitable\ninstructions to automatically identify the severity or impact of ris ks men-\ntioned in the reviews, classifying them into ﬁve levels: negligi ble, minor,\nmoderate, major, and critical. In this case, we employ zero-shot lear ning,\nmeaning there is no need to provide examples to the model.\n3. Evaluation of Open Pre-trained Large Language Models: We eval-\nuate how prompt engineering for OPT-based models compares to large\nproprietary language models such as GPT. By adopting OPT, we enable\nthe use of large language models in scenarios with limited computational\nresources and constraints involving sensitive and private user data. This\ndemocratizes access to the usage of LLMs in more restricted contexts.\nWe conducted experimental evaluations using a database of reviews from\neight mobile apps. Through the proposed approach, we constructed risk mat rix\nfor each app and compared them with their respective reference risk matrix\nconstructed with annotated data. The experimental results demonstr ate that,\nwith proper prompt optimization, OPT models are capable of generating a\ncompetitive risk matrix compared to GPT. While there is room for imp rove-\nment compared to reference risk matrices, our results indicate a s igniﬁcant step\ntoward the maintenance and evolution of software products, enabling f eature\nprioritization that requires more attention from developers.\nThe rest of this paper is structured as follows. Section 2 provides the back-\nground and discusses related works in the ﬁeld. Section 3 presents t he LLM-\nbased risk matrix learning approach applied to app reviews with a focu s on\ndynamic prompt construction for feature extraction (3.1), the estimat ion of\nreview impact (3.2), and addresses the estimation of occurrence like lihood 3.3.\nSection 4 presents the experimental evaluation, deﬁning the rese arch questions\n(4.1), experiment deﬁnition (4.2), preparation and planning (4.3), sampl e se-\nlection (4.4), experimental package (4.5), variables (4.6), experimental design\n(4.7), operation of the experiment (4.8), results and discussion (4.9), an d ﬁnd-\nings to research questions. Section 5 highlights the threats to valid ity of this\nstudy. Finally, Section 6 oﬀers the concluding remarks of the paper, summa-\nrizing the main ﬁndings and contributions.\n2 Background and Related Works\nEarly initiatives for analyzing reviews focusing on software mainten ance ex-\nplore named entity extraction techniques (such as software features ) from re-\nviews using predeﬁned linguistic rules, such as Safe (Johann et al. , 2017),\nGuMA (Guzman and Maalej, 2014), and ReUS (Dragoni et al., 2019). These\napproaches require sets of linguistic patterns, relying on experts to constantly\nupdate them. With the advancement of machine learning methods, mod els\nare trained to identify the entities of interest from a large set of lab eled data.\nAn example of such an approach is RE-BERT (Araujo and Marcacini, 2021),\nwhich uses annotated data to ﬁne-tune a pre-trained BERT model to i dentify\nfeatures or software requirement candidates mentioned by users in r eviews.\nLearning Risk Factors from App Reviews using Large Language Models 5\nWith the recent emergence of Large Language Models (LLMs), such as\nGPT-3 and OPT (Zhang et al., 2022), opinion mining and sentiment analysis\nhave also evolved to incorporate such models, although still with few appli-\ncations in the context of app reviews. These models have architectu res with\nbillions of parameters and are pre-trained on large amounts of text, thereb y\nproviding capabilities for understanding and extracting knowled ge from tex-\ntual data. While the general aim of these models is text generation, thei r\noutputs can be conditioned through instructions or prompts (Strobelt e t al.,\n2022). For example, the task of feature extraction or entity extraction from r e-\nviews can be performed using a paradigm called few-shot learning (Logan IV\net al., 2022). In this case, the model receives a prompt that provides i nfor-\nmation about the type of feature to be extracted and a few samples related\nto that feature. It is worth noting that, in addition to feature extract ion, the\nsame LLM can also be used to identify the impact of reviews according to t heir\nseverity level, where prompts can be used to guide the model in cl assifying the\nseverity of the evaluations.\nAnother common strategy for app review analysis is the use of clustering\nmethods or topic modeling based on review characteristics to organize them\naccording to their similarities (Noei et al., 2021). This allows for iden tifying\ngroups of reviews that mention similar problems, indicating the like lihood of\nbugs or complaints related to those issues. By combining feature extr action,\nseverity classiﬁcation, and the identiﬁcation of the likelihood of ap p reviews,\nwe can automate the construction of risk matrices (Pilliang et al., 2022). Th ese\nmatrices can provide an overview of the risks associated with an app bas ed on\nthe information extracted from user reviews.\nA review of the existing studies reveals various aspects related to risk man-\nagement in software projects. In Xiaosong et al. (2009); Chaouch et al. (2019);\nHammad and Inayat (2018), the authors discuss risk management in agile soft-\nware development projects. Xiaosong et al. (2009) presents basic risk manage -\nment concepts for software development projects, while Chaouch et al . (2019)\nproposes a framework for integrating risk management in agile projects us ing\nScrum. Additionally, Hammad and Inayat (2018) also explores the integration\nof risk management in the Scrum framework, highlighting the importance of\nan iterative risk management process for project success.\nIn Hammad et al. (2019) and Ionita et al. (2019), the authors focus on\nidentifying the risks faced by agile development practitioners and mitigation\nstrategies. Hammad et al. (2019) reveals that project deadlines and changing\nrequirements are the most commonly encountered risks, while Ionit a et al.\n(2019) proposes a framework that uses a risk assessment process to priori tize\nsecurity requirements.\nPilliang et al. (2022) propose a risk matrix model for software development\nprojects, using natural language processing and machine learning tec hniques\nto prioritize risks. The proposed model oﬀers an approach for the automate d\nconstruction of risk matrices by combining sentiment analysis based on lexi-\ncons or vocabularies to identify the impact of the risk and clusterin g methods\nto identify the likelihood of occurrence.\n6 Vitor Mesaque Alves de Lima et al.\nAlthough these studies address diﬀerent aspects of risk management in\nsoftware projects and the application of risk matrices, it is important to high-\nlight some general limitations. Most studies focus on speciﬁc context s, such\nas agile development or information security, which may limit the gene raliz-\nability of their ﬁndings to other types of software projects. There i s a gap in\nthe literature on generating risk matrices from app reviews. Our foc us is on\ngenerating a risk matrix based on reviews, especially in scenarios wi th little\nlabeled data, which can be used in a wider range of applications at diﬀeren t\nstages of software development, from its conception to its maintenance.\n3 LLM-based Risk Matrix Learning from App Reviews\nThe proposed approach leverages Large Language Models (LLMs) to extract\nrelevant information from user reviews and utilize it in constructi ng a risk\nmatrix. In general, the idea is to exploit the text generation capabili ty of an\nLLM, but conditioned for a speciﬁc task.\nFigure 2 provides an overview of the method used for constructing t he risk\nmatrix.\nFig. 2 The overview of our Risk Matrix construction method\nFormally, the task of predicting the next word in an LLM can be formu-\nlated as ﬁnding the most likely word P (wi+1|w1, w 2, ..., w i) given a sequence\nof words w1, w 2, ..., w i. This probability is estimated using the neural weights\nLearning Risk Factors from App Reviews using Large Language Models 7\nof the LLM, which is pre-trained on large textual corpora. The pretraining\nof the LLM is accomplished through a process called autoregressive language\nmodeling in a Transformer neural architecture. During pretraini ng, the model\nlearns to capture linguistic patterns and construct representation s of words\nand phrases that can be used to predict the next word in a sequence.\nConsidering the context of software reviews as input, the proposed ap-\nproach aims to condition the prediction of the next word through a prompt.\nNow, the conditioned probability P (wi+1|w1, w 2, ..., w i; Θ ), where Θ represents\nthe prompt, is used to guide the prediction of the next word.\nIn the proposed method, we use the Open Pre-trained Transformers ( OPT)\nas the Large Language Model (LLM). Most models available through APIs do\nnot provide access to the full model weights obtained during pretr aining, mak-\ning it diﬃcult to study them in detail and reproduce the experim ental results.\nOn the other hand, the OPT was developed to overcome this limitation b y\nproviding pre-trained models with diﬀerent numbers of paramete rs. For exam-\nple, models range from 125 million to 175 billion parameters. The authors of\nOPT conclude from their experiments that OPT-175B is comparable to GPT -\n3 (Zhang et al., 2022). However, some smaller models can achieve promising\nresults through the appropriate use of prompts, as proposed in the next s ec-\ntion.\n3.1 Dynamic Prompt Construction for Feature Extraction\nThe ﬁrst step of the proposed method involves the dynamic construct ion of\nprompts from a knowledge base of reviews from other apps, diﬀerent from\nthe target app, thereby avoiding the need for labeled data from the targe t\napplication to be analyzed.\nThe knowledge base is represented through embeddings of reviews, w hich\nare numerical vectors that capture the semantics and context of words and\nphrases. These embeddings are obtained using deep learning algorithm s, such\nas Sentence-BERT (Reimers and Gurevych, 2019), which map texts into vector\nrepresentations in latent spaces. Formally, given a set of software rev iews in the\nknowledge base, we can represent them as R1, R 2, ..., R n, where Ri represents\na speciﬁc review.\nEach review Ri is converted into a vector representation using a pre-trained\nembedding model. This representation is denoted as e(Ri), where e() repre-\nsents the embedding function. In this way, we have a set of vector s representing\nthe reviews in the knowledge base: e(R1), e (R2), ..., e (Rn).\nTo retrieve the most similar reviews to a target review of interest , we em-\nploy the k-nearest neighbors technique. In this approach, we calcul ate the\nsimilarity between the embedding vector of the target review and th e em-\nbedding vectors of all the reviews in the knowledge base. The simil arity is\ncommonly measured by the cosine of the angle between the vectors. Formal ly,\nto ﬁnd the k-nearest neighbors of a target review Ri, we denote this list as\nKNN (Ri) and deﬁne it as KNN (Ri) = argmaxk(sim(e(Ri), e (Rk))), where\n8 Vitor Mesaque Alves de Lima et al.\nsim() represents the similarity function and argmaxk returns the k indices\ncorresponding to the most similar reviews to Ri.\nThe k-nearest neighbors are then used to generate prompts related to t he\nextraction of text snippets that describe software features. This n earest neigh-\nbors search approach allows the method to leverage the existing knowl edge\nbase and learn from similar examples, becoming a type of few-shot learni ng\nfor the task of feature extraction from software reviews.\nFigure 3 shows a prompt generated for the Instagram app. In blue are\nexamples identiﬁed by similarity from the knowledge base generated through\nreviews and features of other applications. In red, it is the review t o be pro-\ncessed. The model is induced to generate a list of features from the review\nafter the “@” symbol.\nFig. 3 Example of a prompt generated for the Instagram app\n3.2 Estimating Review Impact\nBuilding upon the previous step, we have a list of features extract ed from\nsoftware reviews. Thus, the second step of the method utilizes eac h extracted\nfeature from the previous step into a prompt to instruct the LLM to id entify\nthe severity or impact on ﬁve levels: negligible, minor, moderate, m ajor, and\ncritical. Figure 4 presents an example of the prompt used. Note that we p rovide\nthe prompt constructed along with the feature and let the model compl ete the\nLearning Risk Factors from App Reviews using Large Language Models 9\nseverity classiﬁcation. Unlike the previous step, we condition the model to oﬀer\nan answer within a limited set of options.\nFig. 4 Prompt generated to obtain the severity classiﬁcation for t he Instagram app\nThis zero-shot learning process enables the model to identify th e severity\nof features even without receiving speciﬁc prior examples for each f eature.\nAlthough the model has not been explicitly trained on speciﬁc example s of\nseverity classiﬁcation in software reviews, it is capable of inferri ng patterns\nand generalizing based on the information captured during model pre-t raining.\n3.3 Estimating Occurrence Likelihood\nWhile the ﬁrst two steps allow mapping reviews onto the ”impact” di mension\nof the risk matrix, the third step is responsible for mapping revie ws onto\nthe ”occurrence likelihood” dimension. In this step, a graph-bas ed strategy is\nemployed.\nThe reviews and extracted features from the previous step are repre sented\nas textual expressions of interest and treated as vertices in a graph. S imilar\npairs of vertices are connected through edges. The similarity betwee n the ex-\npressions is measured using embeddings and cosine similarity. In t his case,\nconsider a set of expressions extracted from software reviews, repr esented as\n10 Vitor Mesaque Alves de Lima et al.\nE = {t1, t 2, ..., t m}, where each ti is an expression from the review containing\nthe extracted feature. Similar to the ﬁrst step, these expressi ons are converted\ninto embeddings, which maps each expression to a feature vector.\nThe similarity between two embedding vectors is calculated usin g a metric\nsuch as cosine similarity. Let sim(e(ti), e (tj )) be the function that computes\nthe similarity between two expressions. If the similarity value exceeds a prede-\nﬁned threshold, an edge is created between the corresponding verti ces. Based\non these similarities, we can construct the graph G = ( V, E ), where V is the\nset of vertices and E is the set of edges, as illustrated in Figure 5.\nFig. 5 Edge is created between the corresponding vertices if the si milarity value exceeds a\npredeﬁned threshold\nThe degrees of the graph’s vertices identify expressions that have a higher\nlikelihood of occurrence. The degree values are discretized int o ﬁve levels rep-\nresenting diﬀerent levels of occurrence likelihood. For this purpose, the dis-\ncretization also considers the average degree of the graph, using this v alue\nfor normalization following a normal distribution. This normalization all ows\nmapping the node degree values onto a standardized scale. Using the me an\nand standard deviation of the degree values, the normal distribution fun ction\nis applied, where values close to the mean have a higher probability an d values\nfarther from the mean have a lower probability.\nFinally, the risk matrix is constructed considering the previous steps’ im-\npact and occurrence likelihood dimensions. The next section pre sents an ex-\nperimental evaluation of the proposed approach.\n4 Experimental Evaluation\nWe conducted an experiment to evaluate the approach presented in thi s paper.\nTo do so, we followed the guidelines proposed by Wohlin et al. (2012). The\nexperimental design is detailed in the remainder of this section.\nLearning Risk Factors from App Reviews using Large Language Models 11\n4.1 Deﬁnition of Research Questions\nOur central research question is: how do we learn risk factors from app re-\nviews using Large Language Models and prioritize app reviews and anticipate\nto mitigate risks?\nTo answer this main question, we divided it into two speciﬁc rese arch ques-\ntions, as follows:\n– RQ1 : How can we extract features using LLMs with limited labeled data?\n– RQ2 : How can we identify the severity or impact of risks mentioned in the\nreviews and automatically organize them into a risk matrix?\nThe experiment was conducted to address the research questions pr esented\nin this section.\n4.2 Experiment Deﬁnition\nThe experiment is deﬁned as follows (Wohlin et al., 2012):\n– analyze risk matrix construction method using LLM,\n– to evaluate feature extraction, impact estimation, and likelihood es tima-\ntion,\n– with respect to model performance,\n– from the point of view of the researcher,\n– in the context of crowd feedback from app user reviews.\n4.3 Preparation and Planning\nThe experiment plan comprises the sample selection, description of the exper-\nimental package, deﬁnition of variables, and description of employed de sign\nprinciples.\nTo evaluate the performance of the risk matrix construction approach, we\ncompare our strategy with other state-of-art approaches.\n4.4 Sample Selection\nTo evaluate the risk matrix construction approach, we selected app rev iew\ndatasets used in previous studies of review mining (Dabrowski et al ., 2020).\nIn this context, we utilized human-labeled data consisting of revi ews, app\nfeatures, and corresponding sentiment to generate the impact of a ris k matrix.\nWe used eight mobile apps from these datasets, as described in Table 1. W e\nincluded apps from diﬀerent categories to enhance the generalizabili ty of our\nresults. The ground truth consists of 1,000 reviews for the eight analyze d apps,\nwith 1,255 distinct features, meaning they are mentioned only once, mak ing\nextracting app features from reviews more challenging.\n12 Vitor Mesaque Alves de Lima et al.\nTable 1 The overview of the datasets used for automatic risk matrix c onstruction.\nLabeled Distinct\nApp Reviews reviews Sentences Features features\neBay 1,962 125 294 206 167\nEvernote 4,832 125 367 295 259\nFacebook 8,293 125 327 242 204\nNetﬂix 14,310 125 341 262 201\nPhoto editor 7,690 125 154 96 80\nSpotify 14,487 125 227 180 145\nTwitter 63,628 125 183 122 99\nWhatsApp 248,641 125 169 118 100\n4.5 Experimental Package\nIn our experiment, we have three objects:\n– feature extraction;\n– impact estimation; and\n– likelihood estimation.\nThe following components are part of the experiment package:\n– Reference dataset description: Our experiment utilized human- labeled data\ncontaining reviews, app features, and sentiment. This dataset was obt ained\nfrom Dabrowski et al. (2020).\n– Object deﬁnition: We use Python programming to deﬁne each object.\n– Machine learning classiﬁcation methods: We compared our proposed OPT-\nbased approach with three rule-based methods (GuMa, SAFE, ReUS), a\nﬁne-tuning method of language models (RE-BERT), and a large language\nmodel (GPT 3.5).\n4.6 Variables\nThe independent variables (factors) controlled in the experiment were feature\nextraction and risk estimation, and their respective treatments are described\nbelow.\n4.6.1 Feature Extraction Factor\nThe values assigned to this variable (treatments) are six classiﬁers (GuMa,\nSAFE, ReUS, RE-BERT, GPT, and our OPT-based Proposal). The dependent\nvariable, which is aﬀected by the treatment, is (i) the F1-Score, a m easure of\nthe accuracy of a test (RQ1), and (ii) MAPE / MAE employs typical measure s\nfrom the regression ﬁeld for prediction error (RQ2).\nTo evaluate the feature extraction step, we use the F1 measure for feat ure\nmatching, as proposed by (Dabrowski et al., 2020), that corresponds to the\nharmonic mean of Precision (1) and Recall (2), where TP (True Positive)\nLearning Risk Factors from App Reviews using Large Language Models 13\nrefers to the number of features that were both extracted and annotated ; FP\n(False Positive) are features that were extracted but not annotated, and FN\n(False Negative) refers to the features annotated but not extracted. Eq uation\n3 deﬁnes the F1 measure.\nThis measure allows us to assess the precision and recall of feature ex trac-\ntion in relation to annotated reference features. The parameter n of the Feature\nMatching allows for ﬂexible matching, where n = 0 indicates exact matching,\nwhile n > 0 represents the diﬀerence between the sizes of the extracted an d\nlabeled sequences. We used n = 2 in the experimental evaluation.\nP = T P\nT P + F P (1)\nR = T P\nT P + F N (2)\nF 1 = 2 ∗ P ∗ R\nP + R = 2 ∗ T P\n2 ∗ T P + F P + F N (3)\n4.6.2 Risk Estimation Factor\nThe risk matrix is evaluated in the impact dimension by comparing the numer-\nical level of the reference impact with the impact estimated by our m ethod. For\nthis evaluation, we employ typical measures from the regression ﬁeld , such as\nthe Mean Absolute Percentage Error (MAPE) and the Mean Absolute Error\n(MAE), as deﬁned in Equation 4 and 5 respectively,\nMAP E = 1\nn\n∑ n\nt=1\n|realt − predt|\nrealt\n(4)\nMAE =\n∑ n\nt=1 |realt − predt|\nn (5)\nwhere realt is the real value and predt is the predicted value by the method,\nand n is the sample size.\n4.7 Experimental Design\nIn this experiment, the experimental design has two factors (featu re extraction\nand risk estimation) with the following treatments (Wohlin et al., 2012) :\n– Feature extraction treatments: GuMa, SAFE, ReUS, RE-BERT, GPT, and\nOPT-based approach.\n– Risk estimation treatments: GPT and OPT-based approach.\n14 Vitor Mesaque Alves de Lima et al.\nFor the step of extracting features from reviews of applications, we com -\npared the proposed OPT-based approach with three rule-based methods (GuMa,\nSAFE, ReUS), a ﬁne-tuning method of language models (RE-BERT), and a\nlarge language model (GPT 3.5).\nConcerning the second aspect of evaluation, we compared the proposed\napproach with GPT. In this scenario, both models operate in the zero-sh ot\nlearning format.\n4.8 Operation of the Experiment\nConcerning the step of extracting features, GuMa performs feature extrac-\ntion using a collocation search algorithm, which identiﬁes commonly us ed\nexpressions of two or more words that convey a speciﬁc meaning through\nco-occurrence-based measures. SAFE, relies on manually identiﬁed linguistic\npatterns, including patterns of parts of speech and sentences, to e xtract fea-\ntures from applications. ReUS utilizes linguistic rules composed of p atterns of\nparts of speech and semantic dependency relations. These rules allo w for simul-\ntaneous feature extraction and sentiment analysis. To determine sen timent, the\nmethod employs lexical dictionaries. In contrast, RE-BERT uses p re-trained\nlanguage models to generate semantic textual representations, focusi ng on the\nlocal context of software requirement tokens. However, RE-BERT is a super-\nvised learning method, i.e., it requires a labeled dataset for mod el training.\nThe proposed method employs the OPT-6.7b model, which contains 6.7\nbillion parameters, for the step of extracting features from applicati on re-\nviews. In this step, we employ the proposed strategy of dynamically ge ner-\nating prompts. We use a cross-domain approach, where reviews from other\napplications (source apps) are used as a knowledge base to generate speci ﬁc\nprompts for each review of the target app. This cross-domain approach with\ndynamic prompt generation allows the model to be fed with information f rom\nrelated applications, expanding its ability to generalize without re quiring prior\nknowledge about a speciﬁc target app. To perform this prompt generation, we\nuse the Sentence-BERT (Reimers and Gurevych, 2019) embedding mod el with\nk = 10 to compute the nearest neighbors based on cosine similarity.\nFor identifying the impact of the feature-review pair, our proposal ut ilizes\nthe OPT-IML-1.3b model (Iyer et al., 2022). This model results from a ﬁn e-\ntuning process of large pre-trained language models on a collection of task s\ndescribed through instructions, also known as instruction-tunin g. This process\naims to improve the generalization capability of these models for prev iously\nunseen tasks. Our proposal employs the zero-shot learning strategy in this\nstep. This means that the model is capable of learning to identify th e im-\npact of a feature-review pair, even without receiving speciﬁc exam ples of this\nrelationship during training.\nFinally, we also use GPT 3.5 as the reference model, which is another pre-\ntrained language model, but in the category of Large Language Models. It is\nused for both the extraction of features from reviews and the identiﬁc ation of\nLearning Risk Factors from App Reviews using Large Language Models 15\nreview impact. We employ a zero-shot learning strategy by providin g instruc-\ntions and only examples of how the extraction output should be formatted.\n4.9 Results and Discussion\nThe experimental results are analyzed considering two main aspects : (1) the\nperformance of the F1 score in the matching of feature extraction from app\nreviews, and (2) the error (MAPE and MAE) in constructing the risk matr ix,\nparticularly in the impact dimension. The likelihood dimension i n the reference\nrisk matrices was obtained in the same way as the proposed method. Hence\nthere are no signiﬁcant variations for comparison.\nRegarding the ﬁrst aspect, we analyze the proposed dynamic prompt gen-\neration for OPT and the few-shot prompt learning, compared to a supervis ed\nreference approach based on RE-BERT and classical rule-based methods . The\naim is to demonstrate the performance of OPT models in the absence of la-\nbeled data and the generalization capability of LLMs for new tasks and do-\nmains. Table 2 presentes an overview of the experimental results i n the task\nof extracting features from application reviews.\nTable 2 Comparison of approaches GuMa, SAFE, ReUS, RE-BERT, GPT (ze ro-shot learn-\ning) and OPT (Proposal with few-shot learning) for feature e xtraction from app reviews.\nAPP F1 Matching Score (n = 2)\nGuMa SAFE ReUS RE-BERT GPT Proposal\neBay 0.22 0.36 0.21 0.53 0.22 0.39\nEvernote 0.24 0.33 0.28 0.63 0.14 0.46\nFacebook 0.19 0.24 0.19 0.61 0.20 0.40\nNetﬂix 0.21 0.28 0.27 0.62 0.23 0.41\nPhotoEditor 0.28 0.34 0.26 0.81 0.32 0.56\nSpotify 0.28 0.35 0.27 0.60 0.17 0.48\nTwitter 0.27 0.35 0.26 0.67 0.25 0.47\nWhatsApp 0.26 0.39 0.24 0.61 0.18 0.47\nWe observed that the proposed approach achieves superior results com-\npared to rule-based methods but inferior results to the supervi sed RE-BERT\nmodel. However, it is important to note that supervised models req uire a sig-\nniﬁcant amount of annotated data, necessitating the annotation of all features\nin each review of the training set for a model generation — a very time-\nconsuming task. Although this strategy shows promising results, it m ay not\nbe feasible in scenarios with a lack of domain experts or in dynamic sett ings\nwith frequent review updates, which is common in mobile application quality\nmonitoring and maintenance through reviews.\nOur approach yielded promising results compared to the proprietary G PT\nmodel with zero-shot learning. In addition to requiring less labele d data than\nfully supervised models, our few-shot prompt learning strategy is based on\n16 Vitor Mesaque Alves de Lima et al.\nopen models, without restrictions on proprietary APIs or limitations on pro-\ncessing private or sensitive data.\nConcerning the second aspect of evaluation, we compared the proposed\napproach with GPT. In this scenario, both models operate in the zero-sh ot\nlearning format. However, it should be noted that we used OPT-IML (in struc-\ntion meta-learning), which is ﬁne-tuned with hundreds of instruc tions but with\na smaller number of parameters. In this case, the utilized OPT-IML model has\n1.3 billion parameters, and we analyzed the risk matrices generated wit h the\nfeatures extracted from the previous step. As illustrated in Table 3, OPT-IML\nexhibits a lower error in constructing the risk matrix in the imp act dimension,\nhighlighting it as a promising alternative compared to the proprietary GPT\nmodel.\nTable 3 Error comparison in the Risk Matrix construction.\nAPP GPT-3.5 OPT-IML\nMAE MAPE MAE MAPE\neBay 1.000 0.517 0.913 0.335\nEvernote 1.053 0.532 1.160 0.388\nFacebook 1.092 0.331 0.965 0.285\nNetﬂix 1.255 0.446 1.061 0.359\nPhotoEditor 0.957 0.443 0.929 0.301\nSpotify 1.034 0.344 0.840 0.244\nTwitter 0.943 0.267 0.971 0.253\nWhatsapp 1.133 0.378 1.120 0.357\nFigure 6 illustrates a risk matrix constructed automatically with th e pro-\nposed approach for Netﬂix app. Note that reviews and app features catego-\nrized as critical impact and with a high likelihood of occurrence are frequent\ncomplaints with a strong negative sentiment. This occurs because t he model\nidentiﬁes that such complaints have a greater severity on the applic ation’s\nreputation, meaning they directly aﬀect the app’s ratings in the stor e.\nIn summary, the experimental results suggest that open and accessibl e\nLarge Language Models (LLMs) can play an important role in developing au-\ntomated tools for analyzing mobile application reviews, facilitating ri sk identi-\nﬁcation, as well as contributing to monitoring and prioritizing softwar e main-\ntenance tasks.\n4.10 Findings to Research Questions\nReturning to the initial primary research question: “How do we learn risk fac-\ntors from app reviews using Large Language Models and prioritize appreviews\nand anticipate to mitigate risks?”. We addressed it by formulating two speciﬁc\nquestions (see Section 4.1) and answering them based on the results obt ained\nfrom the conducted experiment.\nLearning Risk Factors from App Reviews using Large Language Models 17\nFig. 6 Risk matrix constructed automatically with the proposed ap proach for Netﬂix app\nWe present objective answers to each research question below (RQ1 and\nRQ2), highlighting the main ﬁndings:\n– (RQ1) Feature extraction with limited labeled data. We introduced\nthe analysis and classiﬁcation of app reviews using Large Language Models\n(LLMs) with limited computational resources and data privacy constraint s.\nWe incorporated a dynamic and automatic prompt generation technique\nto extract speciﬁc app characteristics mentioned in the reviews. The exper-\nimental results showed that our OPT-based proposed approach is superi or\nto rule-based and has advantages over supervised methods that requir e a\nsigniﬁcant amount of labeled data. The ﬁndings demonstrated that our\nproposal is competitive compared to large proprietary language models\nlike GPT-3.5. Despite potential areas for improvement, the ﬁndings s ug-\ngest signiﬁcant progress in extracting features using LLMs with limit ed\nlabeled data.\n– (RQ2) Automated risk matrix construction. We developed and eval-\nuated instructions to classify the severity or impact of risks ment ioned in\napp reviews. These instructions served as prompts to guide our Large Lan-\nguage Model (LLM) in categorizing the risks into ﬁve levels: negligible ,\nminor, moderate, major, and critical. The experimental results in dicated\nthat our proposed approach, using LLMs, eﬀectively identiﬁed the seve rity\nor impact of risks mentioned in the reviews and organized them into a\nrisk matrix. Our research reveals a signiﬁcant advancement in the ﬁe ld of\nsoftware product maintenance and evolution using LLMs.\n18 Vitor Mesaque Alves de Lima et al.\n5 Threats to Validity\n5.1 Internal Validity\nPotential threats to internal validity include the quality and repr esentative-\nness of the knowledge base of app reviews used for prompt construction. If\nthe knowledge base is biased or lacks diversity, it may impact the eﬀ ective-\nness of the method. Additionally, the classiﬁcation of risk severity based on\nthe app rating may introduce subjectivity and potential errors. To mitigate\nthese threats, we utilized review datasets from apps in diﬀerent d omains. We\nalso employed a cross-domain validation strategy, where we used dataset s to\ngenerate prompts from reviews of apps that are diﬀerent from the target app\nbeing analyzed.\n5.2 Construct Validity\nConstruct validity could be threatened if the classiﬁcation of risk s everity based\non the app rating does not truly reﬂect the impact of the risks. In thi s study,\nour assumption is that one of the main risk factors for the app is actions that\nimpact its reputation (average overall rating) in the app store and, ther efore,\nincrease the chances of being uninstalled or not even installed by us ers. To\nenhance construct validity, further research should focus on evalu ating other\ntypes of risks associated with apps, such as security and malfunction s that\naﬀect users’ smartphones.\n5.3 External Validity\nThe external validity may be limited by the speciﬁc LLM used (OPT m odel)\nand the dataset of app reviews. Diﬀerent LLM architectures or datasets fr om\nother domains may yield diﬀerent results. The eﬀectiveness of th e approach\nmay also vary depending on the characteristics of the target app. To imp rove\nexternal validity, conducting replication studies using diﬀere nt LLMs and di-\nverse datasets from various app domains would be valuable. User studies or\nobtaining feedback from software engineering professionals would also h elp val-\nidate the practical usefulness and eﬀectiveness of the risk matric es generated\nfrom app reviews.\n6 Final Remarks\nWe introduced the analysis and classiﬁcation of app reviews through a ri sk ma-\ntrix using Large Language Models (LLMs), speciﬁcally open-access methods\nsuitable for scenarios with limited computational resources and data pr ivacy\nconstraints, in contrast to proprietary models like GPT.\nLearning Risk Factors from App Reviews using Large Language Models 19\nOur proposed approach incorporates a dynamic and automatic prompt\ngeneration technique, enabling the extraction of speciﬁc application charac-\nteristics mentioned by users in the reviews. We also developed an d evaluated\ninstructions to classify the severity or impact of the risks menti oned in the\nreviews. These instructions serve as prompts to guide the LLM in clas sifying\nthe risks into ﬁve levels: negligible, minor, moderate, major, and critical. This\nstandardizes the risk assessment and facilitates the automatic constr uction of\nthe risk matrix. The experimental results provide evidence th at the proposal,\nthrough Open Pre-trained Transformers (OPT), is competitive compar ed to\nlarge proprietary language models such as GPT-3.5. Although there is room\nfor improvement regarding the risk matrices generated by supervis ed reference\nmodels, our results indicate a signiﬁcant advancement in software pr oduct\nmaintenance and evolution.\nDirection for future work involves the development of tools to suppor t\ndecision-making, visualization, and monitoring associated with the ri sk ma-\ntrices. These tools would provide actionable insights and facilitate the inter-\npretation of the risk assessment results. Additionally, exploring t echniques\nto enhance the accuracy and granularity of the risk classiﬁcation levels could\nfurther improve the eﬀectiveness of the risk matrix approach. Fur thermore, in-\nvestigating the integration of external data sources, such as social med ia and\nuser forums, could provide additional context and insights into app-r elated\nrisks. The development of part of these tools is addressed and bette r discussed\nin the next paper.\nAcknowledgements This study was supported by the Brazilian National Council f or Sci-\nentiﬁc and Technological Development (CNPq) [process numb er 426663/2018-7], Federal\nUniversity of Mato Grosso do Sul (UFMS), S˜ ao Paulo Research Foundation (FAPESP)\n[process number 2019/25010-5 and 2019/07665-4], Artiﬁcia l Intelligence (C4AI-USP), and\nfrom the IBM Corporation.\nAvailability of data and materials\nThe data that support the ﬁndings of this study are available from the cor re-\nsponding author upon reasonable request.\nAuthors’ Contributions\nV.L., J.B., and R.M. designed the experiment. V.L. and R.M performed t he\nexperiments and analyzed the data. V.L. and R.M. wrote the code. V.L. pre-\npared the ﬁgures and tables. V.L., J.B., and R.M. drafted the work and rev ised\nit critically for important content.\nConﬂict of interest\nThe authors declare that they have no conﬂict of interest.\n20 Vitor Mesaque Alves de Lima et al.\nReferences\nAraujo, A. and Marcacini, R. M. (2021). Re-bert: Automatic extraction of\nsoftware requirements from app reviews using bert language model. In The\n36th ACM/SIGAPP Symposium On Applied Computing.\nAraujo, A. F., Gˆ olo, M. P. S., and Marcacini, R. M. (2021). Opinion mining\nfor app reviews: an analysis of textual representation and predictive m odels.\nAutomated Software Engineering, 29(1):5.\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,\nNeelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. (2020). Language\nmodels are few-shot learners. In Advances in Neural Information Processing\nSystems, volume 33, pages 1877–1901.\nChaouch, S., Mejri, A., and Ghannouchi, S. A. (2019). A framework for risk\nmanagement in scrum development process. Procedia Computer Science,\n164:187–192.\nDabrowski, J., Letier, E., Perini, A., and Susi, A. (2020). Mining user opin-\nions to support requirement engineering: An empirical study. In Du stdar,\nS., Yu, E., Salinesi, C., Rieu, D., and Pant, V., editors, Advanced Infor-\nmation Systems Engineering, pages 401–416, Cham. Springer International\nPublishing.\nDragoni, M., Federici, M., and Rexha, A. (2019). An unsupervised aspect\nextraction strategy for monitoring real-time reviews stream. Information\nProcessing and Management, 56(3):1103–1118.\nGenc-Nayebi, N. and Abran, A. (2017). A systematic literature review: Opi n-\nion mining studies from mobile app store user reviews. Journal of Systems\nand Software, 125:207–219.\nGuzman, E. and Maalej, W. (2014). How do users like this feature? a ﬁne\ngrained sentiment analysis of app reviews. In 2014 IEEE 22nd international\nrequirements engineering conference (RE), pages 153–162. IEEE.\nHammad, M. and Inayat, I. (2018). Integrating risk management in scrum\nframework. In 2018 International Conference on Frontiers of Information\nTechnology (FIT), pages 158–163. IEEE.\nHammad, M., Inayat, I., and Zahid, M. (2019). Risk management in agile soft-\nware development: A survey. In 2019 International Conference on Frontiers\nof Information Technology (FIT), pages 162–1624. IEEE.\nIonita, D., van der Velden, C., Ikkink, H.-J. K., Neven, E., Daneva, M ., and\nKuipers, M. (2019). Towards risk-driven security requirements m anagement\nin agile software development. In Information Systems Engineering in Re-\nsponsible Information Systems: CAiSE Forum 2019, Rome, Italy,June 3–7,\n2019, Proceedings 31, pages 133–144. Springer.\nIyer, S., Lin, X. V., Pasunuru, R., Mihaylov, T., Simig, D., Yu, P., Shuster,\nK., Wang, T., Liu, Q., Koura, P. S., et al. (2022). Opt-iml: Scaling language\nmodel instruction meta learning through the lens of generalization. arXiv\npreprint arXiv:2212.12017.\nJohann, T., Stanik, C., Maalej, W., et al. (2017). Safe: A simple approach for\nfeature extraction from app descriptions and app reviews. In 2017 IEEE\nLearning Risk Factors from App Reviews using Large Language Models 21\n25th International Requirements Engineering Conference (RE), pages 21–\n30. IEEE.\nLogan IV, R., Balaˇ zevi´ c, I., Wallace, E., Petroni, F., Singh, S., and Ri edel, S.\n(2022). Cutting down on prompts and parameters: Simple few-shot learnin g\nwith language models. In Findings of the Association for Computational\nLinguistics: ACL 2022, pages 2824–2835.\nNoei, E., Zhang, F., and Zou, Y. (2021). Too many user-reviews! what should\napp developers look at ﬁrst? IEEE Transactions on Software Engineering,\n47(2):367–378.\nPaltrinieri, N., Comfort, L., and Reniers, G. (2019). Learning about risk:\nMachine learning for risk assessment. Safety science, 118:475–486.\nPilliang, M., Munawar, Hadi, M. A., Firmansyah, G., and Tjahjono, B. (2022).\nPredicting risk matrix in software development projects using b ert and k-\nmeans. In 2022 9th International Conference on Electrical Engineering,\nComputer Science and Informatics (EECSI), pages 137–142.\nReimers, N. and Gurevych, I. (2019). Sentence-bert: Sentence emb eddings us-\ning siamese bert-networks. In Proceedings of the 2019 Conference on Empir-\nical Methods in Natural Language Processing and the 9th International Joint\nConference on Natural Language Processing (EMNLP-IJCNLP) , pages\n3973–3983.\nRoss, S. I., Martinez, F., Houde, S., Muller, M., and Weisz, J. D. (2023) . The\nprogrammer’s assistant: Conversational interaction with a large language\nmodel for software development. In Proceedings of the 28th International\nConference on Intelligent User Interfaces, pages 491–514.\nStrobelt, H., Webson, A., Sanh, V., Hoover, B., Beyer, J., Pﬁster, H., and Rush,\nA. M. (2022). Interactive and visual prompt engineering for ad-hoc task\nadaptation with large language models. IEEE transactions on visualization\nand computer graphics, 29(1):1146–1156.\nWohlin, C., Runeson, P., H¨ ost, M., Ohlsson, M. C., Regnell, B., and W essl´ en,\nA. (2012). Experimentation in Software Engineering. Springer Berlin Hei-\ndelberg, Norwell, MA, USA.\nXiaosong, L., Shushi, L., Wenjun, C., and Songjiang, F. (2009). The applica-\ntion of risk matrix to software project risk management. In 2009 Interna-\ntional Forum on Information Technology and Applications, volume 2, pages\n480–483. IEEE.\nZhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., De wan, C.,\nDiab, M., Li, X., Lin, X. V., et al. (2022). Opt: Open pre-trained transformer\nlanguage models. arXiv preprint arXiv:2205.01068.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.47697386145591736
    },
    {
      "name": "Risk model",
      "score": 0.4428282678127289
    },
    {
      "name": "Risk analysis (engineering)",
      "score": 0.41262632608413696
    },
    {
      "name": "Matrix (chemical analysis)",
      "score": 0.41253432631492615
    },
    {
      "name": "Business",
      "score": 0.23621207475662231
    },
    {
      "name": "Materials science",
      "score": 0.05551755428314209
    },
    {
      "name": "Composite material",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I122558511",
      "name": "Universidade Federal de Mato Grosso do Sul",
      "country": "BR"
    },
    {
      "id": "https://openalex.org/I68106152",
      "name": "Universidade Federal de Goiás",
      "country": "BR"
    }
  ]
}