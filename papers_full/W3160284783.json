{
  "title": "Swin-Unet: Unet-like Pure Transformer for Medical Image Segmentation",
  "url": "https://openalex.org/W3160284783",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2112296014",
      "name": "Cao Hu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A161592578",
      "name": "Wang, Yueyue Wang, Yueyue<",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Chen, Joy",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1911087664",
      "name": "Jiang Dongsheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2154016176",
      "name": "Zhang Xiao-peng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2114747614",
      "name": "Tian Qi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2743845119",
      "name": "Wang Manning",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2888358068",
    "https://openalex.org/W2962914239",
    "https://openalex.org/W3214395992",
    "https://openalex.org/W2951839332",
    "https://openalex.org/W3132503749",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3130695101",
    "https://openalex.org/W2147484997",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W3026315751",
    "https://openalex.org/W3137561054",
    "https://openalex.org/W3202088435",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2560023338",
    "https://openalex.org/W3203841574",
    "https://openalex.org/W2952339589",
    "https://openalex.org/W2907750714",
    "https://openalex.org/W3135385363",
    "https://openalex.org/W2964080601",
    "https://openalex.org/W3015788359",
    "https://openalex.org/W2942489928",
    "https://openalex.org/W2964227007",
    "https://openalex.org/W3103010481",
    "https://openalex.org/W2412782625",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W3134689216",
    "https://openalex.org/W3127751679",
    "https://openalex.org/W2798122215"
  ],
  "abstract": "In the past few years, convolutional neural networks (CNNs) have achieved milestones in medical image analysis. Especially, the deep neural networks based on U-shaped architecture and skip-connections have been widely applied in a variety of medical image tasks. However, although CNN has achieved excellent performance, it cannot learn global and long-range semantic information interaction well due to the locality of the convolution operation. In this paper, we propose Swin-Unet, which is an Unet-like pure Transformer for medical image segmentation. The tokenized image patches are fed into the Transformer-based U-shaped Encoder-Decoder architecture with skip-connections for local-global semantic feature learning. Specifically, we use hierarchical Swin Transformer with shifted windows as the encoder to extract context features. And a symmetric Swin Transformer-based decoder with patch expanding layer is designed to perform the up-sampling operation to restore the spatial resolution of the feature maps. Under the direct down-sampling and up-sampling of the inputs and outputs by 4x, experiments on multi-organ and cardiac segmentation tasks demonstrate that the pure Transformer-based U-shaped Encoder-Decoder network outperforms those methods with full-convolution or the combination of transformer and convolution. The codes and trained models will be publicly available at https://github.com/HuCaoFighting/Swin-Unet.",
  "full_text": "Swin-Unet: Unet-like Pure Transformer for\nMedical Image Segmentation\nHu Cao1†, Yueyue Wang2†, Joy Chen1, Dongsheng Jiang3∗, Xiaopeng Zhang3∗,\nQi Tian3∗, and Manning Wang2\n1 Technische Universit¨ at M¨ unchen, M¨ unchen, Germany\n2 Fudan University, Shanghai, China\n3 Huawei Technologies, Shanghai, China\nAbstract. In the past few years, convolutional neural networks (CNNs)\nhave achieved milestones in medical image analysis. Especially, the deep\nneural networks based on U-shaped architecture and skip-connections\nhave been widely applied in a variety of medical image tasks. How-\never, although CNN has achieved excellent performance, it cannot learn\nglobal and long-range semantic information interaction well due to the\nlocality of convolution operation. In this paper, we propose Swin-Unet,\nwhich is a Unet-like pure Transformer for medical image segmentation.\nThe tokenized image patches are fed into the Transformer-based U-\nshaped Encoder-Decoder architecture with skip-connections for local-\nglobal semantic feature learning. Speciﬁcally, we use hierarchical Swin\nTransformer with shifted windows as the encoder to extract context fea-\ntures. And a symmetric Swin Transformer-based decoder with patch ex-\npanding layer is designed to perform the up-sampling operation to re-\nstore the spatial resolution of the feature maps. Under the direct down-\nsampling and up-sampling of the inputs and outputs by 4 ×, experi-\nments on multi-organ and cardiac segmentation tasks demonstrate that\nthe pure Transformer-based U-shaped Encoder-Decoder network outper-\nforms those methods with full-convolution or the combination of trans-\nformer and convolution. The codes and trained models will be publicly\navailable at https://github.com/HuCaoFighting/Swin-Unet.\n1 Introduction\nBeneﬁting from the development of deep learning, computer vision technology\nhas been widely used in medical image analysis. Image segmentation is an impor-\ntant part of medical image analysis. In particular, accurate and robust medical\nimage segmentation can play a cornerstone role in computer-aided diagnosis and\nimage-guided clinical surgery [1,2].\n*Corresponding author\n†Work done as an intern in Huawei Technologies\narXiv:2105.05537v1  [eess.IV]  12 May 2021\n2 Hu Cao et al.\nExisting medical image segmentation methods mainly rely on fully convo-\nlutional neural network (FCNN) with U-shaped structure [3,4,5]. The typical\nU-shaped network, U-Net [3], consists of a symmetric Encoder-Decoder with\nskip connections. In the encoder, a series of convolutional layers and continu-\nous down-sampling layers are used to extract deep features with large receptive\nﬁelds. Then, the decoder up-samples the extracted deep features to the input\nresolution for pixel-level semantic prediction, and the high-resolution features\nof diﬀerent scale from the encoder are fused with skip connections to alleviate\nthe loss of spatial information caused by down-sampling. With such an elegant\nstructural design, U-Net has achieved great success in a variety of medical imag-\ning applications. Following this technical route, many algorithms such as 3D\nU-Net [6], Res-UNet [7], U-Net++ [8] and UNet3+ [9] have been developed for\nimage and volumetric segmentation of various medical imaging modalities. The\nexcellent performance of these FCNN-based methods in cardiac segmentation,\norgan segmentation and lesion segmentation proves that CNN has a strong abil-\nity of learning discriminating features.\nCurrently, although the CNN-based methods have achieved excellent perfor-\nmance in the ﬁeld of medical image segmentation, they still cannot fully meet\nthe strict requirements of medical applications for segmentation accuracy. Image\nsegmentation is still a challenge task in medical image analysis. Since the intrin-\nsic locality of convolution operation, it is diﬃcult for CNN-based approaches to\nlearn explicit global and long-range semantic information interaction [2]. Some\nstudies have tried to address this problem by using atrous convolutional lay-\ners [10,11], self-attention mechanisms [12,13], and image pyramids [14]. However,\nthese methods still have limitations in modeling long - range dependencies. Re-\ncently, inspired by Transformer’s great success in the nature language processing\n(NLP) domain [15], researchers have tried to bring Transformer into the vision\ndomain [16]. In [17], vision transformer (ViT) is proposed to perform the im-\nage recognition task. Taking 2D image patches with positional embeddings as\ninputs and pre-training on large dataset, ViT achieved comparable performance\nwith the CNN-based methods. Besides, data-eﬃcient image transformer (DeiT)\nis presented in [18], which indicates that Transformer can be trained on mid-size\ndatasets and that a more robust Transformer can be obtained by combining it\nwith the distillation method. In [19], a hierarchical Swin Transformer is devel-\noped. Take Swin Transformer as vision backbone, the authors of [19] achieved\nstate-of-the-art performance on Image classiﬁcation, object detection and se-\nmantic segmentation. The success of ViT, DeiT and Swin Transformer in image\nrecognition task demonstrates the potential for Transformer to be applied in the\nvision domain.\nMotivated by the Swin Transformer’s [19] success, we propose Swin-Unet\nto leverage the power of Transformer for 2D medical image segmentation in\nthis work. To our best knowledge, Swin-Unet is a ﬁrst pure Transformer-based\nU-shaped architecture that consists of encoder, bottleneck, decoder, and skip\nconnections. Encoder, bottleneck and decoder are all built based on Swin Trans-\nformer block [19]. The input medical images are split into non-overlapping image\nSwin-Unet: Unet-like Pure Transformer for Medical Image Segmentation 3\npatches. Each patch is treated as a token and fed into the Transformer-based\nencoder to learn deep feature representations. The extracted context features are\nthen up-sampled by the decoder with patch expanding layer, and fused with the\nmulti-scale features from the encoder via skip connections, so as to restore the\nspatial resolution of the feature maps and further perform segmentation predic-\ntion. Extensive experiments on multi-organ and cardiac segmentation datasets\nindicate that the proposed method has excellent segmentation accuracy and ro-\nbust generalization ability. Concretely, our contributions can be summarized as:\n(1) Based on Swin Transformer block, we build a symmetric Encoder-Decoder\narchitecture with skip connections. In the encoder, self-attention from local to\nglobal is realized; in the decoder, the global features are up-sampled to the in-\nput resolution for corresponding pixel-level segmentation prediction. (2) A patch\nexpanding layer is developed to achieve up-sampling and feature dimension in-\ncrease without using convolution or interpolation operation. (3) It is found in\nthe experiment that skip connection is also eﬀective for Transformer, so a pure\nTransformer-based U-shaped Encoder-Decoder architecture with skip connection\nis ﬁnally constructed, named Swin-Unet.\n2 Related work\nCNN-based methods : Early medical image segmentation methods are mainly\ncontour-based and traditional machine learning-based algorithms [20,21]. With\nthe development of deep CNN, U-Net is proposed in [3] for medical image seg-\nmentation. Due to the simplicity and superior performance of the U-shaped\nstructure, various Unet-like methods are constantly emerging, such as Res-UNet [7],\nDense-UNet [22], U-Net++ [8] and UNet3+ [9]. And it is also introduced into\nthe ﬁeld of 3D medical image segmentation, such as 3D-Unet [6] and V-Net [23].\nAt present, CNN-based methods have achieved tremendous success in the ﬁeld\nof medical image segmentation due to its powerful representation ability.\nVision transformers : Transformer was ﬁrst proposed for the machine trans-\nlation task in [15]. In the NLP domain, the Transformer-based methods have\nachieved the state-of-the-art performance in various tasks [24]. Driven by Trans-\nformer’s success, the researchers introduced a pioneering vision transformer (ViT)\nin [17], which achieved the impressive speed-accuracy trade-oﬀ on image recog-\nnition task. Compared with CNN-based methods, the drawback of ViT is that it\nrequires pre-training on its own large dataset. To alleviate the diﬃculty in train-\ning ViT, Deit [18] describes several training strategies that allow ViT to train\nwell on ImageNet. Recently, several excellent works have been done baed on\nViT [25,26,19]. It is worth mentioning that an eﬃcient and eﬀective hierarchical\nvision Transformer, called Swin Transformer, is proposed as a vision backbone\nin [19]. Based on the shifted windows mechanism, Swin Transformer achieved\nthe state-of-the-art performance on various vision tasks including image classiﬁ-\ncation, object detection and semantic segmentation. In this work, we attempt to\nuse Swin Transformer block as basic unit to build a U-shaped Encoder-Decoder\n4 Hu Cao et al.\narchitecture with skip connections for medical image segmentation, thus provid-\ning a benchmark comparison for the development of Transformer in the medical\nimage ﬁeld.\nSelf-attention/Transformer to complement CNNs : In recent years, re-\nsearchers have tried to introduce self-attention mechanism into CNN to improve\nthe performance of the network [13]. In [12], the skip connections with additive\nattention gate are integrated in U-shaped architecture to perform medical image\nsegmentation. However, this is still the CNN-based method. Currently, some ef-\nforts are being made to combine CNN and Transformer to break the dominance\nof CNNs in medical image segmentation [2,27,1]. In [2], the authors combined\nTransformer with CNN to constitute a strong encoder for 2D medical image seg-\nmentation. Similar to [2], [27] and [28] use the complementarity of Transformer\nand CNN to improve the segmentation capability of the model. Currently, var-\nious combinations of Transformer with CNN are applied in multi-modal brain\ntumor segmentation [29] and 3D medical image segmentation [1,30]. Diﬀerent\nfrom the above methods, we try to explore the application potential of pure\nTransformer in medical image segmentation.\n3 Method\n3.1 Architecture overview\nThe overall architecture of the proposed Swin-Unet is presented in Figure. 1.\nSwin-Unet consists of encoder, bottleneck, decoder and skip connections. The\nbasic unit of Swin-Unet is Swin Transformer block [19]. For the encoder, to\ntransform the inputs into sequence embeddings, the medical images are split into\nnon-overlapping patches with patch size of 4 ×4. By such partition approach,\nthe feature dimension of each patch becomes to 4 ×4 ×3 = 48. Furthermore, a\nlinear embedding layer is applied to projected feature dimension into arbitrary\ndimension (represented as C). The transformed patch tokens pass through several\nSwin Transformer blocks and patch merging layers to generate the hierarchical\nfeature representations. Speciﬁcally, patch merging layer is responsible for down-\nsampling and increasing dimension, and Swin Transformer block is responsible\nfor feature representation learning. Inspired by U-Net [3], we design a symmet-\nric transformer-based decoder. The decoder is composed of Swin Transformer\nblock and patch expanding layer. The extracted context features are fused with\nmultiscale features from encoder via skip connections to complement the loss\nof spatial information caused by down-sampling. In contrast to patch merging\nlayer, a patch expanding layer is specially designed to perform up-sampling. The\npatch expanding layer reshapes feature maps of adjacent dimensions into a large\nfeature maps with 2 ×up-sampling of resolution. In the end, the last patch ex-\npanding layer is used to perform 4×up-sampling to restore the resolution of the\nfeature maps to the input resolution (W ×H), and then a linear projection layer\nis applied on these up-sampled features to output the pixel-level segmentation\npredictions. We would elaborate each block in the following\nSwin-Unet: Unet-like Pure Transformer for Medical Image Segmentation 5\nFig. 1.The architecture of Swin-Unet, which is composed of encoder, bottleneck, de-\ncoder and skip connections. Encoder, bottleneck and decoder are all constructed based\non swin transformer block.\n3.2 Swin Transformer block\nDiﬀerent from the conventional multi-head self attention (MSA) module, swin\ntransformer block [19] is constructed based on shifted windows. In Figure. 2,\ntwo consecutive swin transformer blocks are presented. Each swin transformer\nblock is composed of LayerNorm (LN) layer, multi-head self attention module,\nresidual connection and 2-layer MLP with GELU non-linearity. The window-\nbased multi-head self attention (W-MSA) module and the shifted window-based\nmulti-head self attention (SW-MSA) module are applied in the two successive\n6 Hu Cao et al.\nFig. 2.Swin transformer block.\ntransformer blocks, respectively. Based on such window partitioning mechanism,\ncontinuous swin transformer blocks can be formulated as:\nˆzl = W-MSA (LN(zl−1)) + zl−1, (1)\nzl = MLP (LN(ˆzl)) + ˆzl, (2)\nˆzl+1 = SW -MSA (LN(zl)) + zl, (3)\nzl+1 = MLP (LN(ˆzl+1)) + ˆzl+1, (4)\nwhere ˆzl and zl represent the outputs of the (S)W-MSA module and the MLP\nmodule of the lth block, respectively. Similar to the previous works [31,32], self-\nattention is computed as follows:\nAttention(Q, K, V) = SoftMax (QKT\n√\nd\n+ B)V, (5)\nwhere Q, K, V∈RM2×d denote the query, key and value matrices. M2 and d\nrepresent the number of patches in a window and the dimension of the query\nor key, respectively. And, the values in B are taken from the bias matrix ˆB ∈\nR(2M−1)×(2M+1).\n3.3 Encoder\nIn the encoder, the C-dimensional tokenized inputs with the resolution of H\n4 ×W\n4\nare fed into the two consecutive Swin Transformer blocks to perform representa-\ntion learning, in which the feature dimension and resolution remain unchanged.\nMeanwhile, the patch merging layer will reduce the number of tokens (2×down-\nsampling) and increase the feature dimension to 2×the original dimension. This\nprocedure will be repeated three times in the encoder.\nSwin-Unet: Unet-like Pure Transformer for Medical Image Segmentation 7\nPatch merging layer : The input patches are divided into 4 parts and con-\ncatenated together by the patch merging layer. With such processing, the feature\nresolution will be down-sampled by 2 ×. And, since the concatenate operation\nresults the feature dimension increasing by 4 ×, a linear layer is applied on the\nconcatenated features to unify the feature dimension to the 2 ×the original di-\nmension.\n3.4 Bottleneck\nSince Transformer is too deep to be converged [33], only two successive Swin\nTransformer blocks are used to constructed the bottleneck to learn the deep\nfeature representation. In the bottleneck, the feature dimension and resolution\nare kept unchanged.\n3.5 Decoder\nCorresponding to the encoder, the symmetric decoder is built based on Swin\nTransformer block. To this end, in contrast to the patch merging layer used in\nthe encoder, we use the patch expanding layer in the decoder to up-sample the\nextracted deep features. The patch expanding layer reshapes the feature maps\nof adjacent dimensions into a higher resolution feature map (2 ×up-sampling)\nand reduces the feature dimension to half of the original dimension accordingly.\nPatch expanding layer : Take the ﬁrst patch expanding layer as an example,\nbefore up-sampling, a linear layer is applied on the input features ( W\n32 ×H\n32 ×8C)\nto increase the feature dimension to 2 ×the original dimension ( W\n32 ×H\n32 ×16C).\nThen, we use rearrange operation to expand the resolution of the input features\nto 2×the input resolution and reduce the feature dimension to quarter of the\ninput dimension ( W\n32 ×H\n32 ×16C →W\n16 ×H\n16 ×4C). We will discuss the impact\nof using patch expanding layer to perform up-sampling in section 4.5.\n3.6 Skip connection\nSimilar to the U-Net [3], the skip connections are used to fuse the multi-scale\nfeatures from the encoder with the up-sampled features. We concatenate the\nshallow features and the deep features together to reduce the loss of spatial\ninformation caused by down-sampling. Followed by a linear layer, the dimension\nof the concatenated features is remained the same as the dimension of the up-\nsampled features. In section 4.5, we will detailed discuss the impact of the number\nof skip connections on the performance of our model.\n4 Experiments\n4.1 Datasets\nSynapse multi-organ segmentation dataset (Synapse): the dataset in-\ncludes 30 cases with 3779 axial abdominal clinical CT images. Following [2,34],\n8 Hu Cao et al.\nTable 1.Segmentation accuracy of diﬀerent methods on the Synapse multi-organ CT\ndataset.\nMethods DSC↑HD↓Aorta Gallbladder Kidney(L) Kidney(R) Liver Pancreas Spleen Stomach\nV-Net [35] 68.81 - 75.34 51.87 77.10 80.75 87.84 40.05 80.56 56.98\nDARR [36] 69.77 - 74.74 53.77 72.31 73.24 94.08 54.18 89.90 45.96\nR50 U-Net [2]74.68 36.8787.74 63.66 80.60 78.19 93.74 56.90 85.87 74.16\nU-Net [3] 76.85 39.7089.07 69.72 77.77 68.60 93.43 53.98 86.67 75.58\nR50 Att-UNet [2]75.57 36.9755.92 63.91 79.20 72.71 93.56 49.37 87.19 74.95\nAtt-UNet [37]77.77 36.0289.55 68.88 77.98 71.11 93.57 58.04 87.30 75.75\nR50 ViT [2] 71.29 32.8773.73 55.13 75.80 72.20 91.51 45.99 81.99 73.95\nTransUnet [2]77.48 31.6987.23 63.13 81.87 77.02 94.08 55.86 85.08 75.62\nSwinUnet 79.13 21.5585.47 66.53 83.28 79.61 94.29 56.58 90.66 76.60\n18 samples are divided into the training set and 12 samples into testing set. And\nthe average Dice-Similarity coeﬃcient (DSC) and average Hausdorﬀ Distance\n(HD) are used as evaluation metric to evaluate our method on 8 abdominal or-\ngans (aorta, gallbladder, spleen, left kidney, right kidney, liver, pancreas, spleen,\nstomach).\nAutomated cardiac diagnosis challenge dataset (ACDC): the ACDC\ndataset is collected from diﬀerent patients using MRI scanners. For each patient\nMR image, left ventricle (LV), right ventricle (RV) and myocardium (MYO)\nare labeled. The dataset is split into 70 training samples, 10 validation samples\nand 20 testing samples. Similar to [2], only average DSC is used to evaluate our\nmethod on this dataset.\n4.2 Implementation details\nThe Swin-Unet is achieved based on Python 3.6 and Pytorch 1.7.0. For all train-\ning cases, data augmentations such as ﬂips and rotations are used to increase\ndata diversity. The input image size and patch size are set as 224×224 and 4, re-\nspectively. We train our model on a Nvidia V100 GPU with 32GB memory. The\nweights pre-trained on ImageNet are used to initialize the model parameters.\nDuring the training period, the batch size is 24 and the popular SGD optimizer\nwith momentum 0.9 and weight decay 1e-4 is used to optimize our model for\nback propagation.\n4.3 Experiment results on Synapse dataset\nThe comparison of the proposed Swin-Unet with previous state-of-the-art meth-\nods on the Synapse multi-organ CT dataset is presented in Table. 1. Diﬀer-\nent from TransUnet [2], we add the test results of our own implementations\nof U-Net [3]and Att-UNet [37] on the Synapse dataset. Experimental results\ndemonstrate that our Unet-like pure transformer method achieves the best per-\nformance with segmentation accuracy of 79.13%(DSC↑) and 21.55%(HD↓). Com-\npared with Att-Unet [37] and the recently method TransUnet [2], although our\nalgorithm did not improve much on the DSC evaluation metric, we achieved ac-\ncuracy improvement of about 4% and 10% on the HD evaluation metric, which\nSwin-Unet: Unet-like Pure Transformer for Medical Image Segmentation 9\nFig. 3.The segmentation results of diﬀerent methods on the Synapse multi-organ CT\ndataset.\nTable 2.Segmentation accuracy of diﬀerent methods on the ACDC dataset.\nMethods DSC RV Myo LV\nR50 U-Net 87.55 87.10 80.63 94.92\nR50 Att-UNet 86.75 87.58 79.20 93.47\nR50 ViT 87.57 86.07 81.88 94.75\nTransUnet 89.71 88.86 84.53 95.73\nSwinUnet 90.00 88.55 85.62 95.83\nindicates that our approach can achieve better edge predictions. The segmen-\ntation results of diﬀerent methods on the Synapse multi-organ CT dataset are\nshown in Figure. 3. It can be seen from the ﬁgure that CNN-based methods\ntend to have over-segmentation problems, which may be caused by the local-\nity of convolution operation. In this work, we demonstrate that by integrating\nTransformer with a U-shaped architecture with skip connections, the pure Trans-\nformer approach without convolution can better learn both global and long-range\nsemantic information interactions, resulting in better segmentation results.\n10 Hu Cao et al.\nTable 3.Ablation study on the impact of the up-sampling\nUp-sampling DSC Aorta Gallbladder Kidney(L) Kidney(R) Liver Pancreas Spleen Stomach\nBilinear interpolation76.1581.84 66.33 80.12 73.91 93.64 55.04 86.10 72.20\nTransposed convolution77.6384.81 65.96 82.66 74.61 94.39 54.81 89.42 74.41\nPatch expand 79.1385.47 66.53 83.28 79.61 94.29 56.58 90.66 76.60\nTable 4.Ablation study on the impact of the number of skip connection\nSkip connectionDSC Aorta Gallbladder Kidney(L) Kidney(R) Liver Pancreas Spleen Stomach\n0 72.46 78.71 53.24 77.46 75.90 92.60 46.07 84.57 71.13\n1 76.43 82.53 60.44 81.36 79.27 93.64 53.36 85.95 74.90\n2 78.93 85.82 66.27 84.70 80.32 93.94 55.32 88.35 76.71\n3 79.1385.47 66.53 83.28 79.61 94.29 56.58 90.66 76.60\n4.4 Experiment results on ACDC dataset\nSimilar to the Synapse dataset, the proposed Swin-Unet is trained on ACDC\ndataset to perform medical image segmentation. The experimental results are\nsummarized in Table. 2. By using the image data of MR mode as input, Swin-\nUnet is still able to achieve excellent performance with an accuracy of 90 .00%,\nwhich shows that our method has good generalization ability and robustness.\n4.5 Ablation study\nIn order to explore the inﬂuence of diﬀerent factors on the model performance,\nwe conducted ablation studies on Synapse dataset. Speciﬁcally, up-sampling, the\nnumber of skip connections, input sizes, and model scales are discussed below.\nEﬀect of up-sampling: Corresponding to the patch merging layer in the en-\ncoder, we specially designed a patch expanding layer in the decoder to per-\nform up-sampling and feature dimension increase. To explore the eﬀective of the\nproposed patch expanding layer, we conducted the experiments of Swin-Unet\nwith bilinear interpolation, transposed convolution and patch expanding layer\non Synapse dataset. The experimental results in the Table 3 indicate that the\nproposed Swin-Unet combined with the patch expanding layer can obtain the\nbetter segmentation accuracy.\nEﬀect of the number of skip connections:The skip connections of our Swin-\nUNet are added in places of the 1/4, 1/8, and 1/16 resolution scales. By changing\nthe number of skip connections to 0, 1, 2 and 3 respectively, we explored the\ninﬂuence of diﬀerent skip connections on the segmentation performance of the\nproposed model. In Table 4, we can see that the segmentation performance of the\nmodel increases with the increase of the number of skip connections. Therefore,\nin order to make the model more robust, the number of skip connections is set\nas 3 in this work.\nSwin-Unet: Unet-like Pure Transformer for Medical Image Segmentation 11\nTable 5.Ablation study on the impact of the input size\nInput size DSC Aorta Gallbladder Kidney(L) Kidney(R) Liver Pancreas Spleen Stomach\n224 79.13 85.47 66.53 83.28 79.61 94.29 56.58 90.66 76.60\n384 81.1287.07 70.53 84.64 82.87 94.72 63.73 90.14 75.29\nTable 6.Ablation study on the impact of the model scale\nModel scale DSC Aorta Gallbladder Kidney(L) Kidney(R) Liver Pancreas Spleen Stomach\ntiny 79.13 85.47 66.53 83.28 79.61 94.29 56.58 90.66 76.60\nbase 79.2587.16 69.19 84.61 81.99 93.86 58.10 88.44 70.65\nEﬀect of input size: The testing results of the proposed Swin-Unet with\n224 ×224, 384 ×384 input resolutions as input are presented in Table. 5. As\nthe input size increases from 224 ×224 to 384 ×384 and the patch size remains\nthe same as 4, the input token sequence of Transformer will become larger,\nthus leading to improve the segmentation performance of the model. However,\nalthough the segmentation accuracy of the model has been slightly improved,\nthe computational load of the whole network has also increased signiﬁcantly. In\norder to ensure the running eﬃciency of the algorithm, the experiments in this\npaper are based on 224 ×224 resolution scale as the input.\nEﬀect of model scale: Similar to [19], we discuss the eﬀect of network deep-\nening on model performance. It can be seen from Table. 6 that the increase of\nmodel scale hardly improves the performance of the model, but increases the\ncomputational cost of the whole network. Considering the accuracy-speed trade\noﬀ, we adopt the Tiny-based model to perform medical image segmentation.\n4.6 Discussion\nAs we all known, the performance of Transformer-based model is severely aﬀected\nby model pre-training. In this work, we directly use the training weight of Swin\ntransformer [19] on ImageNet to initialize the network encoder and decoder,\nwhich may be a suboptimal scheme. This initialization approach is a simple one,\nand in the future we will explore the ways to pre-train Transformer end-to-end for\nmedical image segmentation. Moreover, since the input images in this paper are\n2D, while most of the medical image data are 3D, we will explore the application\nof Swin-Unet in 3D medical image segmentation in the following research.\n5 Conclusion\nIn this paper, we introduced a novel pure transformer-based U-shaped encoder-\ndecoder for medical image segmentation. In order to leverage the power of Trans-\nformer, we take Swin Transformer block as the basic unit for feature represen-\ntation and long-range semantic information interactive learning. Extensive ex-\n12 Hu Cao et al.\nperiments on multi-organ and cardiac segmentation tasks demonstrate that the\nproposed Swin-Unet has excellent performance and generalization ability.\nReferences\n1. A. Hatamizadeh, D. Yang, H. Roth, and D. Xu, “Unetr: Transformers for 3d med-\nical image segmentation,” 2021.\n2. J. Chen, Y. Lu, Q. Yu, X. Luo, E. Adeli, Y. Wang, L. Lu, A. L. Yuille, and Y. Zhou,\n“Transunet: Transformers make strong encoders for medical image segmentation,”\nCoRR, vol. abs/2102.04306, 2021.\n3. O. Ronneberger, P.Fischer, and T. Brox, “U-net: Convolutional networks for\nbiomedical image segmentation,” in Medical Image Computing and Computer-\nAssisted Intervention (MICCAI), ser. LNCS, vol. 9351. Springer, 2015, pp. 234–\n241.\n4. K. S. P. J. M.-H. K. Isensee F, Jaeger PF, “nnu-net: a self-conﬁguring method for\ndeep learning-based biomedical image segmentation,” Nat Methods, vol. 18(2):203-\n211, 2021.\n5. Q. Jin, Z. Meng, C. Sun, H. Cui, and R. Su, “Ra-unet: A hybrid deep attention-\naware network to extract liver and tumor in ct scans,” Frontiers in Bioengineering\nand Biotechnology, vol. 8, p. 1471, 2020.\n6. ¨O. C ¸ i¸ cek, A. Abdulkadir, S. Lienkamp, T. Brox, and O. Ronneberger, “3d u-net:\nLearning dense volumetric segmentation from sparse annotation,” inMedical Image\nComputing and Computer-Assisted Intervention (MICCAI), ser. LNCS, vol. 9901.\nSpringer, Oct 2016, pp. 424–432.\n7. X. Xiao, S. Lian, Z. Luo, and S. Li, “Weighted res-unet for high-quality retina vessel\nsegmentation,” 2018 9th International Conference on Information Technology in\nMedicine and Education (ITME), pp. 327–331, 2018.\n8. Z. Zhou, M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang, “Unet++: A nested\nu-net architecture for medical image segmentation.” Springer Verlag, 2018, pp.\n3–11.\n9. H. Huang, L. Lin, R. Tong, H. Hu, Q. Zhang, Y. Iwamoto, X. Han, Y.-W. Chen,\nand J. Wu, “Unet 3+: A full-scale connected unet for medical image segmentation,”\n2020.\n10. L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille, “Deeplab:\nSemantic image segmentation with deep convolutional nets, atrous convolution,\nand fully connected crfs,” IEEE Transactions on Pattern Analysis and Machine\nIntelligence, vol. 40, no. 4, pp. 834–848, 2018.\n11. Z. Gu, J. Cheng, H. Fu, K. Zhou, H. Hao, Y. Zhao, T. Zhang, S. Gao, and\nJ. Liu, “Ce-net: Context encoder network for 2d medical image segmentation,”\nIEEE Transactions on Medical Imaging, vol. 38, no. 10, pp. 2281–2292, 2019.\n12. J. Schlemper, O. Oktay, M. Schaap, M. Heinrich, B. Kainz, B. Glocker, and\nD. Rueckert, “Attention gated networks: Learning to leverage salient regions in\nmedical images,” Medical Image Analysis, vol. 53, pp. 197–207, 2019.\n13. X. Wang, R. Girshick, A. Gupta, and K. He, “Non-local neural networks,” in 2018\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, 2018, pp.\n7794–7803.\n14. H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, “Pyramid scene parsing network,”\nin 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n2017, pp. 6230–6239.\nSwin-Unet: Unet-like Pure Transformer for Medical Image Segmentation 13\n15. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u.\nKaiser, and I. Polosukhin, “Attention is all you need,” in Advances in Neural\nInformation Processing Systems, vol. 30. Curran Associates, Inc., 2017.\n16. N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, “End-\nto-end object detection with transformers,” CoRR, vol. abs/2005.12872, 2020.\n17. A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,\nM. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby,\n“An image is worth 16x16 words: Transformers for image recognition at scale,” in\nInternational Conference on Learning Representations, 2021.\n18. H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. J´ egou, “Training\ndata-eﬃcient image transformers & distillation through attention,” CoRR, vol.\nabs/2012.12877, 2020.\n19. Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, “Swin\ntransformer: Hierarchical vision transformer using shifted windows,” CoRR, vol.\nabs/2103.14030, 2021.\n20. A. Tsai, A. Yezzi, W. Wells, C. Tempany, D. Tucker, A. Fan, W. Grimson, and\nA. Willsky, “A shape-based approach to the segmentation of medical imagery using\nlevel sets,” IEEE Transactions on Medical Imaging, vol. 22, no. 2, pp. 137–154,\n2003.\n21. K. Held, E. Kops, B. Krause, W. Wells, R. Kikinis, and H.-W. Muller-Gartner,\n“Markov random ﬁeld segmentation of brain mr images,” IEEE Transactions on\nMedical Imaging, vol. 16, no. 6, pp. 878–886, 1997.\n22. X. Li, H. Chen, X. Qi, Q. Dou, C.-W. Fu, and P.-A. Heng, “H-denseunet: Hybrid\ndensely connected unet for liver and tumor segmentation from ct volumes,” IEEE\nTransactions on Medical Imaging, vol. 37, no. 12, pp. 2663–2674, 2018.\n23. F. Milletari, N. Navab, and S.-A. Ahmadi, “V-net: Fully convolutional neural net-\nworks for volumetric medical image segmentation,”2016 Fourth International Con-\nference on 3D Vision (3DV), pp. 565–571, 2016.\n24. J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training of deep\nbidirectional transformers for language understanding,” in Proceedings of the 2019\nConference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers).\nMinneapolis, Minnesota: Association for Computational Linguistics, Jun. 2019,\npp. 4171–4186. [Online]. Available: https://www.aclweb.org/anthology/N19-1423\n25. W. Wang, E. Xie, X. Li, D. Fan, K. Song, D. Liang, T. Lu, P. Luo, and\nL. Shao, “Pyramid vision transformer: A versatile backbone for dense prediction\nwithout convolutions,” CoRR, vol. abs/2102.12122, 2021. [Online]. Available:\nhttps://arxiv.org/abs/2102.12122\n26. K. Han, A. Xiao, E. Wu, J. Guo, C. Xu, and Y. Wang, “Transformer\nin transformer,” CoRR, vol. abs/2103.00112, 2021. [Online]. Available: https:\n//arxiv.org/abs/2103.00112\n27. J. M. J. Valanarasu, P. Oza, I. Hacihaliloglu, and V. M. Patel, “Medical\ntransformer: Gated axial-attention for medical image segmentation,” CoRR, vol.\nabs/2102.10662, 2021.\n28. Y. Zhang, H. Liu, and Q. Hu, “Transfuse: Fusing transformers and cnns\nfor medical image segmentation,” CoRR, vol. abs/2102.08005, 2021. [Online].\nAvailable: https://arxiv.org/abs/2102.08005\n29. W. Wang, C. Chen, M. Ding, J. Li, H. Yu, and S. Zha, “Transbts: Multimodal\nbrain tumor segmentation using transformer,” CoRR, vol. abs/2103.04430, 2021.\n[Online]. Available: https://arxiv.org/abs/2103.04430\n14 Hu Cao et al.\n30. Y. Xie, J. Zhang, C. Shen, and Y. Xia, “Cotr: Eﬃciently bridging CNN and\ntransformer for 3d medical image segmentation,” CoRR, vol. abs/2103.03024,\n2021. [Online]. Available: https://arxiv.org/abs/2103.03024\n31. H. Hu, J. Gu, Z. Zhang, J. Dai, and Y. Wei, “Relation networks for object detec-\ntion,” in 2018 IEEE/CVF Conference on Computer Vision and Pattern Recogni-\ntion, 2018, pp. 3588–3597.\n32. H. Hu, Z. Zhang, Z. Xie, and S. Lin, “Local relation networks for image recogni-\ntion,” in 2019 IEEE/CVF International Conference on Computer Vision (ICCV),\n2019, pp. 3463–3472.\n33. H. Touvron, M. Cord, A. Sablayrolles, G. Synnaeve, and H. J´ egou, “Going deeper\nwith image transformers,” CoRR, vol. abs/2103.17239, 2021. [Online]. Available:\nhttps://arxiv.org/abs/2103.17239\n34. S. Fu, Y. Lu, Y. Wang, Y. Zhou, W. Shen, E. Fishman, and A. Yuille, “Domain\nadaptive relational reasoning for 3d multi-organ segmentation,” in Medical Image\nComputing and Computer Assisted Intervention – MICCAI 2020, 2020, pp. 656–\n666.\n35. F. Milletari, N. Navab, and S.-A. Ahmadi, “V-net: Fully convolutional neural net-\nworks for volumetric medical image segmentation,” in 2016 Fourth International\nConference on 3D Vision (3DV), 2016, pp. 565–571.\n36. S. Fu, Y. Lu, Y. Wang, Y. Zhou, W. Shen, E. Fishman, and A. Yuille, “Domain\nadaptive relational reasoning for 3d multi-organ segmentation,” Germany, 2020,\npp. 656–666.\n37. O. Oktay, J. Schlemper, L. L. Folgoc, M. Lee, M. Heinrich, K. Misawa, K. Mori,\nS. McDonagh, N. Y. Hammerla, B. Kainz, B. Glocker, and D. Rueckert, “Attention\nu-net: Learning where to look for the pancreas,” IMIDL Conference, 2018.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7224245071411133
    },
    {
      "name": "Encoder",
      "score": 0.6455145478248596
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5984402894973755
    },
    {
      "name": "Transformer",
      "score": 0.5960297584533691
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5672285556793213
    },
    {
      "name": "Segmentation",
      "score": 0.5630694627761841
    },
    {
      "name": "Deep learning",
      "score": 0.46784719824790955
    },
    {
      "name": "Image segmentation",
      "score": 0.4370741546154022
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.40851664543151855
    },
    {
      "name": "Computer vision",
      "score": 0.3715890347957611
    },
    {
      "name": "Engineering",
      "score": 0.10887932777404785
    },
    {
      "name": "Voltage",
      "score": 0.10018280148506165
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ]
}