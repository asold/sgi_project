{
  "title": "LI-RAGE: Late Interaction Retrieval Augmented Generation with Explicit Signals for Open-Domain Table Question Answering",
  "url": "https://openalex.org/W4385572252",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2982940416",
      "name": "Weizhe Lin",
      "affiliations": [
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A3101140677",
      "name": "Rexhina Blloshmi",
      "affiliations": [
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A1978878817",
      "name": "Bill Byrne",
      "affiliations": [
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A1384276614",
      "name": "Adrià de Gispert",
      "affiliations": [
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A2158065470",
      "name": "Gonzalo Iglesias",
      "affiliations": [
        "University of Cambridge"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2768409085",
    "https://openalex.org/W4221163895",
    "https://openalex.org/W4306317212",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W3021397474",
    "https://openalex.org/W4281395820",
    "https://openalex.org/W3217305727",
    "https://openalex.org/W3176069778",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4385574177",
    "https://openalex.org/W4226391321",
    "https://openalex.org/W3035140194",
    "https://openalex.org/W2890431379",
    "https://openalex.org/W2963899988",
    "https://openalex.org/W2947354947",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W2998702515",
    "https://openalex.org/W3184222203",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W3167748596",
    "https://openalex.org/W3157891451"
  ],
  "abstract": "Recent open-domain TableQA models are typically implemented as retriever-reader pipelines. The retriever component is usually a variant of the Dense Passage Retriever, which computes the similarities between questions and tables based on a single representation of each.These fixed vectors can be insufficient to capture fine-grained features of potentially very big tables with heterogeneous row/column information. We address this limitation by 1) applying late interaction models which enforce a finer-grained interaction between question and table embeddings at retrieval time. In addition, we 2) incorporate a joint training scheme of the retriever and reader with explicit table-level signals, and 3) embed a binary relevance token as a prefix to the answer generated by the reader, so we can determine at inference time whether the table used to answer the question is reliable and filter accordingly. The combined strategies set a new state-to-the-art performance on two public open-domain TableQA datasets.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 2: Short Papers, pages 1557–1566\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nLI-RAGE: Late Interaction Retrieval Augmented Generation with Explicit\nSignals for Open-Domain Table Question Answering\nWeizhe Lin∗2, Rexhina Blloshmi1,\nBill Byrne1,2, Adrià de Gispert1, and Gonzalo Iglesias1\n1Amazon Alexa AI\n2University of Cambridge\nwl356@cam.ac.uk {blloshmi, willbyrn, agispert, gjii}@amazon.com\nAbstract\nRecent open-domain TableQA models are typi-\ncally implemented as retriever-reader pipelines.\nThe retriever component is usually a variant\nof the Dense Passage Retriever, which com-\nputes the similarities between questions and\ntables based on a single representation of each.\nThese fixed vectors can be insufficient to cap-\nture fine-grained features of potentially very\nbig tables with heterogeneous row/column in-\nformation. We address this limitation by 1) ap-\nplying late interaction models which enforce a\nfiner-grained interaction between question and\ntable embeddings at retrieval time. In addition,\nwe 2) incorporate a joint training scheme of the\nretriever and reader with explicit table-level sig-\nnals, and 3) embed a binary relevance token as\na prefix to the answer generated by the reader,\nso we can determine at inference time whether\nthe table used to answer the question is reliable\nand filter accordingly. The combined strategies\nset a new state-to-the-art performance on two\npublic open-domain TableQA datasets.\n1 Introduction\nTabular data is ubiquitous on the Web. Open-\ndomain Table Question Answering (TableQA), the\ntask of answering questions grounded in tables, is\nincreasingly attracting attention of both public and\ncommercial research, for its value in real-world\napplications. Research TableQA pipelines are typ-\nically implemented with two components: a re-\ntriever and a reader. The retriever chooses a small\nset from the entire pool of table candidates, while\nthe reader generates answers processing each ta-\nble candidate. State-of-the-art implementations use\ntransformer-based models for both components. In\nparticular, the retriever is built with variants of\nDense Passage Retriever (Karpukhin et al., 2020,\nDPR), which computes question-table similarity by\nusing single vector representations of the question\nand the table. Retriever and reader can be trained\n∗Work done as an intern at Amazon Alexa AI.\nseparately (Herzig et al., 2021) or jointly (Pan\net al., 2022) via Retrieval Augmented Generation\nloss (Lewis et al., 2020b, RAG). We observe three\nlimitations which we address in this paper.\nFirst, a table can be very large and might contain\nheterogeneous information across rows/columns;\nencoding into a fixed size vector risks information\nloss, which can have an impact in QA quality. One\nway to alleviate this issue is to replace DPR with a\nLatent Interaction (LI) model, which encodes text\ninto token-level representations. In particular, we\nfind ColBERT (Khattab and Zaharia, 2020) to be\nvery effective, even if not pretrained for tables.\nSecond, RAG uses only an implicit signal to\nguide the retriever. Recently, Lin and Byrne (2022)\nproposed RAGE loss (Retrieval Augmented Gener-\nation with Explicit Signals) for visual QA, which\nin our setting rewards the retriever with table-level\nsignals from the reader model in joint training.\nThird, we observe empirically that the reader\ndoes not always rank answers coming from the\ngold table at the top. As our reader is a sequence-to-\nsequence model, we propose a simple modification\nto the training data: we prepend binary relevance\ntokens (‘yes/no’) to the answer itself. The reader\nlearns to generate a first token indicating whether\nthe table is relevant to the question or not.\nUsing these techniques, we build an end-to-end\nframework, LI-RAGE, and achieve state-of-the-\nart results on two benchmarks for open-domain\nTableQA, NQ-TABLES (Herzig et al., 2021) and\nE2E-WTQ (Pan et al., 2021). 1\n2 Related Work\nWhile open-domain TableQA is yet a relatively un-\nexplored problem, with only a few applications\nin the past couple of years, there has been exten-\nsive work on table retrieval and TableQA sepa-\nrately. In table retrieval, recent advances in ma-\n1We make our code available at: https://github.com/\namazon-science/robust-tableqa\n1557\nchine learning have enabled extracting deep fea-\ntures for tables with Transformers (Vaswani et al.,\n2017), by designing models to parse complex tab-\nular structure (Herzig et al., 2021; Wang et al.,\n2021), or by simply linearizing tables with inter-\nleaving tokens to preserve its structure (Pan et al.,\n2022; Wang et al., 2022). In TableQA, until re-\ncently researchers assumed gold tables were given\nand focused on developing models that understood\nand answered questions over tables, i.e. the read-\ners. Earlier models generated commands in logical\nforms (e.g. SQL queries) that were executable over\ntables (Yu et al., 2018; Lin et al., 2019; Xu et al.,\n2018), while recent state-of-the-art models directly\npredict the answers from the input question and\ntable by either classification (Herzig et al., 2020;\nYang et al., 2022, TaPas) or autoregressive gener-\nation (Liu et al., 2022, TaPEx). Following these\nadvances, in open-domain TableQA the best per-\nforming systems are based on a retriever-reader\npipeline (Herzig et al., 2021; Pan et al., 2022).\nHerzig et al. (2021, DTR) leverages TaPas (Herzig\net al., 2020) to both initialize a DPR-like retriever\nand the reader. T-RAG (Pan et al., 2022) uses DPR\nas retriever of rows/columns by decomposing the\ntable and generates the answer via a sequence-to-\nsequence reader (Lewis et al., 2020a), applying the\nRAG loss to refine the retriever with implicit sig-\nnals during end-to-end TableQA fine-tuning. Un-\nlike DTR and T-RAG, CLTR (Pan et al., 2021)\nemploys only retrieval of rows and columns and ob-\ntains the answer cell by intersecting the top-scored\nones. In this work we focus mainly on the retriever,\nand unlike previous work that relies on single vec-\ntor embeddings, we leverage late interaction re-\ntrievers (Khattab and Zaharia, 2020) to achieve a\nfiner-grained interaction between questions and ta-\nbles. In contrast to T-RAG and CLTR, we do not\nneed to decompose the table into rows and columns,\nbut retrieve a whole table from the corpus, ensuring\nthat the reader is given all the relevant information.\nIn addition, we explore different techniques for\nexplicitly refining the retriever during end-to-end\nTableQA achieving superior performance.\n3 Methodology\nGiven a question q, the tasks are to find the gold\ntable t∗from a table corpus T, i.e. table retrieval\n(§ 3.1), and to derive the answer denotations S (1\nor more cells from the table), i.e. question answer-\ning over the retrieved tables (§ 3.2). We assume\nthat labeled datasets consisting of triples {(q, S,\nt∗)}are available to us. We flatten the tables into\nsequences with interleaving special tokens that en-\ncode its structure (see Appendix A).\n3.1 Table Retrieval\nIn order to exploit question-table similarity at a\nfiner-grained level than when using DPR models,\nwe leverage LI models to encode and retrieve tables\nfor a question. We use ColBERT, which consists\nof a question encoder Fq and a table encoder Ft, to\nencode questions and tables at the token level:\nQ = Fq(q) ∈Rlq×d; T = Ft(t) ∈Rlt×d, (1)\nwhere lq and lt are input token lengths of qand t.\nThe relevance score accounts for the interactions\nbetween all question and table token embeddings:\nr(q,t) =\nlq∑\ni=1\nlt\nmax\nj=1\nQiT⊤\nj (2)\nLI models extract multi-dimensional question/table\nembeddings and token-level similarity, as opposed\nto finding the similarity of single embeddings for\nthe whole question/table in DPR, thus capturing a\nfiner-grained interaction between them.\nTo train the model we exploit the gold (positive)\ntable t∗for each question q, i.e. explicitly consid-\nering the table-level ground truth. We use in-batch\nnegative sampling for training, per Karpukhin et al.\n(2020). All documents in a training batch other\nthan t∗are considered negative for q, and denoted\nas N(q). We train with the contrastive loss LCL:\n−\n∑\n(q,t∗)\nlog exp (r(q,t∗))\nexp (r(q,t∗)) +\n∑\nz∈N(q)\nexp (r(q,z))\n(3)\nTo this end, for each q, the retriever outputs\nK top-scoring tables {tk}K\nk=1. Finally, following\nRAG, we obtain their (approximate2) conditional\nprobability pθ(·|q) with the retriever parameters θ:\npθ(tk|q) = exp(r(q,tk))∑K\nj=1 exp(r(q,tj))\n(4)\n3.2 Retrieval-based TableQA\nFor the TableQA task we make use of a sequence-\nto-sequence Transformer-based model that directly\n2because we sum over the top-K tables instead of all tables,\nassuming their probabilities are small and irrelevant.\n1558\nproduces an answer for a given question and table.\nThe TableQA model pϕ takes as input a sequence\ncomposed of the question q and each of the re-\ntrieved tables tk as described in §3.1, and generates\nan answer yk for each input table tk:\nyk = argmax\ny\npϕ(y|q,tk) (5)\nFinally, the model returns the answer associated\nwith the highest probability/confidence:\nˆy,ˆt= argmax\ny,tk\npϕ(y|q,tk) (6)\n3.3 Joint Training of Retrieval and TableQA\nWe train both modules jointly using a composi-\ntional loss (Lin and Byrne, 2022, RAGE), which\nconsiders signals from table relevance and answer\nprediction, as follows:\n−\n∑\n(q,S)\n(K∑\nk=1\nlog pϕ(s∗\nk|q,tk) +\n∑\nk∈P+(q,S)\nlog pθ(tk|q)\n)\n(7)\nwhere s∗\nk is a concatenation of all comma-separated\nanswers in S and P+(q,S) = {k : yk = s∗\nk ∧\ntk = t∗}is a subset of the retrieved K tables,\nwhich contains those tables that satisfy (1) being\na gold table relevant to answering the question;\n(2) the answer generator successfully produces the\ncorrect answer from that table. The core idea is\nto leverage the signal from model prediction to\ndecide which tables are beneficial to producing\nthe correct answer. Their scores are dynamically\nadjusted during training, which tailors the retriever\nto better serve the answer generation.\n3.4 Learned Table Relevance\nThe answer generator is trained to produce s∗\nk for\neach input (q,tk) pair. Ideally, we would assume\nthat the answer generated from the gold table t∗is\nalso associated with the highest probability from\nthe answer generator. However, it might happen\nthat an answer derived from a non-gold retrieved ta-\nble may achieve higher confidence than the answer\nderived from a gold retrieved table. We propose a\nsimple yet effective approach to improve this pro-\ncess: we add a binary relevance tokenpreceding\ns∗\nk as ‘yes’ if tk = t∗, ‘no’ otherwise. This design\naims at guiding the model to prioritize reliable an-\nswer sources at training time. At generation time, if\nthe leading generation of a (q,tk) pair is ‘yes’, we\nconsider tk to be a more reliable answer source and\nprioritize it over other input tables—that generate\n‘no’ instead—when selecting the final prediction.\nWe rely on the confidence scores if the leading\ntoken of all the candidates is ‘no’.\n4 Experimental Setup\nDatasets and metrics. We evaluate our system\non two benchmarks, i.e. NQ-TABLES (Herzig\net al., 2021) and E2E-WTQ (Pan et al., 2021).3\nNQ-TABLES contains generally hard questions\nextracted from the NaturalQuestions (Kwiatkowski\net al., 2019) dataset, comprising the questions that\ncan be answered from tables rather than plain text.\nFor this benchmark, we evaluate the models us-\ning: Token F1, i.e. token-wise F1 score; and exact\nmatch (EM) or accuracy, i.e. whether predictions\nmatch the annotations.\nE2E-WTQ contains look-up questions that require\ncell selection operation and is a subset of Wik-\niTableQuestions (Pasupat and Liang, 2015). In\nE2E-WTQ train/valid/test splits are the same as\nin WikiTableQuestions, with questions limited to\nthose that do not aggregations across multiple table\ncells. We evaluate models via accuracy4.\nIn addition, we report Recall@K for the retrieval\nperformance in both, which measures whether the\ngold table is among the top-K retrieved tables.5\nSystem configurations. For the table retrieval\ncomponent, we conduct contrastive experiments\nusing both DPR and LI. We first fine-tune the\nofficial pretrained DPR or ColBERTv2 model on\neach dataset before using them in the joint retriever-\nreader training. We do not train theTableQAmodel\nfrom scratch, instead we warm-start the training\nwith TaPEx, a state-of-the-art pre-trained model for\ntabular data understanding based on BART (Lewis\net al., 2020a). Since the E2E-WTQ is very small\nand not enough for learning a robust TableQA\nmodel, we additionally fine-tune TaPEx on its su-\nperset, i.e. WikiTableQuestions. Note that no test\nsamples are leaked due to this as the dataset splits\nof E2E-WTQ are the same as WikiTableQuestions.\nWe select the best checkpoints based on the valida-\ntion set. We setK=5 since it shows the best balance\nbetween performance and latency by both RAG\nand RAGE. Training details, computational cost\n3Dataset statistics are shown in Appendix B.\n4Also named as Hit@1 in Pan et al. (2021, 2022)\n5We do not report metrics such as P@K, N@K, MAP\nused by T-RAG and CLTR, which decompose tables, being\nincompatible with our setting (see Appendix C).\n1559\nModels NQ-TABLES E2E-WTQ\nToken F1 EM Recall@K Accuracy Recall@K\nDTR+hn (Herzig et al., 2021) 47.70 37.69 81.13@10 - -\nCLTR (Pan et al., 2021) - - - 46.75 -\nT-RAG (Pan et al., 2022) 50.92 43.06 85.40@10 50.65 -\nRAG 39.67 38.33 69.16@5 38.05 61.29@5\nDPR-RAGE 49.68 43.02 84.35@5 48.79 59.68@5\nLI-RAGE 54.17 46.15 87.90 @5 62.10 81.85 @5\n(w/o joint training) 53.53 45.52 85.21@5 59.27 81.45@5\n(w/o relevance tokens) 50.56 42.53 86.90@5 53.69 81.75@5\n(w/o joint training & relevance tokens)49.83 42.19 85.21@5 50.16 81.45@5\nTable 1: End-to-end TableQA performance on NQ-TABLES and E2E-WTQ. Best performances are inbold.\nModels NQ-TABLES E2E-WTQ\nK=1 K=5 K=10 K=50 K=1 K=5 K=10 K=50\nBM25 17.62 35.97 43.80 61.00 58.09 74.27 79.67 87.55\nDPR-RAGE 58.29 84.35 90.72 97.08 33.61 59.68 66.80 88.38\n(w/o joint training) 53.07 84.25 90.62 97.81 32.78 58.47 66.39 88.38\nLI-RAGE 59.12 87.90 92.81 97.60 68.46 81.85 85.89 93.36\n(w/o joint training) 53.75 85.21 90.10 97.71 66.13 81.45 84.27 93.55\nTable 2: Retrieval performance on NQ-TABLES and E2E-WTQ. Best performances are inbold.\nand software solution are provided in Appendix D.\nComparison systems. We compare with models\nfrom the literature, i.e. DTR, CLTR, T-RAG(see\n§2), and BM25—sparse retrieval baseline. More-\nover, we build the following model variants:\nLI-RAGE: our main system that leverages Col-\nBERT as retriever, TaPEx as answer generator,\nRAGE loss for joint training and the binary rel-\nevance token in output. We also ablate the system\nshowing the effectiveness of each feature. When\ndisabling joint training, i.e., for ablating the model,\nthe retriever is not updated.\nDPR-RAGE: similar to LI-RAGE, except for the\nretriever being a DPR model.\nRAG: we train the RAG (Lewis et al., 2020b) in\nTableQA data, initializing the retriever and answer\ngenerator with our fine-tuned DPR and TaPEx, re-\nspectively. Different from DPR-RAGE, RAG does\nnot produce the binary relevance token and updates\nthe retriever only with the RAG loss, which is an\nimplicit signal from the reader.\n5 Results and Discussions\n5.1 Main Results\nAs shown in Table 1, LI-RAGE achieves the best\nperformance across the board on both datasets,\nwith more than 3 points improvements in Token\nF1 and EM in NQ-TABLES, and 11.45 points in\nE2E-WTQ with respect to previously best reported\nresults in the literature. We attribute these results\nto the high performance of the LI retriever. On NQ-\nTABLES it obtains the best recall rate (87.90%)\nwhen only 5 tables are retrieved, as opposed to the\nprevious models that achieve a lower recall rate\nwith K = 10tables, and also performs better when\ncompared with RAG and DPR-RAGE, by a large\nmargin.\nEffects of Joint Training. Similar to the observa-\ntion of Lin and Byrne (2022), joint training with\nRAGE improves over the frozen system on both\nretrieval and TableQA performance. As shown\nin Table 1, joint training improves the end-to-end\nTableQA performance on both datasets by ∼0.6-\n2.83%, and shows a superior retrieval ability espe-\ncially on NQ-TABLES (85.21 to 87.90).\nEffects of Binary Relevance Tokens. As shown\nin Table 1, removing the binary relevance tokens\ngreatly reduces system performance, by around\n3.6% Token F1 and EM in NQ-TABLES and 8.4%\nin E2E-WTQ accuracy.\nEffects of LI. We report the retrieval performance\nin Table 2. LI-RAGE achieves the highest recall,\noutperforming BM25 in both datasets, and DPR\nby ∼3% on NQ-TABLES and by over 20-30%\nRecall@5/1 on E2E-WTQ. The large margin on\nE2E-WTQ is because it contains generally long ta-\nbles with diverse information, and LI models prove\n1560\nbeneficial in learning richer table representations.\n5.2 Remarks of Design Rationale\nWe tailor our solution for TableQA, with the spe-\ncific design of two main components, i.e., adding a\nrelevance token and modifying the RAGE loss.\nRelevance token. In open-domain QA, open-\nended questions may have multiple correct answers\nand can be answered by different passages. As a\nresult, increasing the number of retrieved passages\n(K) often improves the retrieval performance by\nenlarging the coverage of search. However, this is\nnot the case for tables; in open-domain TableQA,\nthe question often has only one gold table and most\nof the questions focus on a particular cell in the\ngold table. In our experiments, increasing K de-\ncreased the performance when K > 5 since pre-\nsenting more tables to the answer generator only\nincreases confusion and chance of mistakes (over-\nconfident on some wrongly retrieved tables). When\nusing relevance tokens as per our design, increas-\ning Kdoes not adversely impact the performance\nsince irrelevant tables are dropped. In addition, we\nalso explored alternative strategies that leverage\nretrieval scores to determine document reliability.\nThe first strategy predicts the final answer from the\ntable with the highest retrieval score. This setting\nachieves 41.04 EM on NQ-TABLES, which is even\nlower than our ablated LI-RAGE w/o joint training\n& relevance tokensattaining 42.19 EM (see Table\n1). A second strategy weights predictions from\ndifferent tables with the corresponding retrieval\nscore, i.e., by multiplying the retrieval score (from\nthe retriever) with the answer confidence (from the\nanswer generator) when using K=5. This again\nperforms poorer than our ablated LI-RAGE w/o\njoint training & relevance tokensthat uses only\nanswer generator confidence, achieving 40.91 EM\non NQ-TABLES and 42.19 EM, respectively. In\nsummary, relevance tokens work better than doc-\nument retrieval scores or combination of retriever\nand reader scores.\nRAGE loss. We modify the original RAGE\nloss (Lin and Byrne, 2022) to adapt it to the do-\nmain of tables. In particular, we dropped the third\nterm in the equation, which penalizes documents\nwhen they do not contain gold answers and also\ndo not contribute to successful question-answering.\nEnabling this term in the loss, penalizes K −1\ndocuments in most cases, which leads to collapsed\nperformance of the retriever in joint training for\nTableQA. This is motivated by the same fact that\ngold tables are relatively sparse in TableQA and\npenalizing wrong documents leads to instability of\ntraining and quick retriever overfitting. Disabling\nthis term instead, softens the RAGE loss by only\nawarding “good” tables and distinguishing good\ntables from bad ones, which improved the perfor-\nmance by around 1% EM on NQ-TABLES.\n6 Conclusion\nWe introduce a novel open-domainTableQA frame-\nwork, LI-RAGE, that leverages late interaction re-\ntrievers to enable finer-grained interaction between\nquestions and tables. Additionally, LI-RAGE incor-\nporates the RAGE loss and binary relevance tokens\nwhich enable significant improvements over the\nstate-of-the-art in two challenging TableQA tasks.\n7 Limitations\nOur proposed system was tested on two open-\ndomain TableQA datasets, with one of them (E2E-\nWTQ) being relatively small compared to the other.\nAlso, the current open-domain TableQA datasets\nare limited to look-up questions. They do not\ncover more complicated questions that involve mul-\ntiple cells and complex table operations, such as\nSUM/MAX/MIN/SUBTRACT in some questions\nof WikiSQL and WikiTableQuestion. Therefore,\nthe effectiveness of our system should be further\nevaluated on more complicated datasets of larger\nscale in the future. Another limitation lies in the\ntoken length limit of modern Transformer models.\nThe best-achieving models typically accept up to\n1024 tokens (e.g. BART, the base model of TaPEx).\nThis limitation becomes more obvious when tables\ngrow longer and the information being sought go\nbeyond the limit. We believe that, with better ap-\nproaches addressing this limitation, our system can\nachieve better performance. The solution can be\neither applying sampling strategies to pick the rows\nand columns that are most relevant to answering\nthe question, or increasing the capacity of future\nTransformer models.\nReferences\nJonathan Herzig, Thomas Müller, Syrine Krichene, and\nJulian Eisenschlos. 2021. Open domain question\nanswering over tables via dense retrieval. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n1561\n512–519, Online. Association for Computational Lin-\nguistics.\nJonathan Herzig, Pawel Krzysztof Nowak, Thomas\nMüller, Francesco Piccinno, and Julian Eisenschlos.\n2020. TaPas: Weakly supervised table parsing via\npre-training. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4320–4333, Online. Association for Computa-\ntional Linguistics.\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6769–6781,\nOnline. Association for Computational Linguistics.\nOmar Khattab and Matei Zaharia. 2020. Colbert: Effi-\ncient and effective passage search via contextualized\nlate interaction over bert. In Proceedings of the 43rd\nInternational ACM SIGIR Conference on Research\nand Development in Information Retrieval, SIGIR\n’20, page 39–48, New York, NY , USA. Association\nfor Computing Machinery.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, Kristina Toutanova, Llion Jones, Matthew\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\nral questions: A benchmark for question answering\nresearch. Transactions of the Association for Compu-\ntational Linguistics, 7:452–466.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020a.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020b. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Advances in Neu-\nral Information Processing Systems, 33:9459–9474.\nKevin Lin, Ben Bogin, Mark Neumann, Jonathan\nBerant, and Matt Gardner. 2019. Grammar-\nbased neural text-to-sql generation. arXiv preprint\narXiv:1905.13326.\nWeizhe Lin and Bill Byrne. 2022. Retrieval augmented\nvisual question answering with outside knowledge.\nIn Proceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing. Associa-\ntion for Computational Linguistics.\nQian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi\nLin, Weizhu Chen, and Jian-Guang Lou. 2022.\nTAPEX: Table pre-training via learning a neural SQL\nexecutor. In International Conference on Learning\nRepresentations.\nFeifei Pan, Mustafa Canim, Michael Glass, Alfio\nGliozzo, and Peter Fox. 2021. CLTR: An end-to-end,\ntransformer-based system for cell-level table retrieval\nand table question answering. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing: System\nDemonstrations, pages 202–209, Online. Association\nfor Computational Linguistics.\nFeifei Pan, Mustafa Canim, Michael Glass, Alfio\nGliozzo, and James Hendler. 2022. End-to-end table\nquestion answering via retrieval-augmented genera-\ntion. arXiv preprint arXiv:2203.16714.\nPanupong Pasupat and Percy Liang. 2015. Composi-\ntional semantic parsing on semi-structured tables. In\nProceedings of the 53rd Annual Meeting of the As-\nsociation for Computational Linguistics and the 7th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 1470–\n1480, Beijing, China. Association for Computational\nLinguistics.\nKeshav Santhanam, Omar Khattab, Christopher Potts,\nand Matei Zaharia. 2022a. Plaid: An efficient en-\ngine for late interaction retrieval. In Proceedings\nof the 31st ACM International Conference on Infor-\nmation Knowledge Management, CIKM ’22, page\n1747–1756, New York, NY , USA. Association for\nComputing Machinery.\nKeshav Santhanam, Omar Khattab, Jon Saad-Falcon,\nChristopher Potts, and Matei Zaharia. 2022b. Col-\nBERTv2: Effective and efficient retrieval via\nlightweight late interaction. In Proceedings of the\n2022 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 3715–3734, Seat-\ntle, United States. Association for Computational\nLinguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proceedings of the 31st International\nConference on Neural Information Processing Sys-\ntems, NIPS’17, page 6000–6010, Red Hook, NY ,\nUSA. Curran Associates Inc.\nFei Wang, Kexuan Sun, Muhao Chen, Jay Pujara, and\nPedro Szekely. 2021. Retrieving complex tables with\nmulti-granular graph representation learning. In Pro-\nceedings of the 44th International ACM SIGIR Con-\nference on Research and Development in Information\n1562\nRetrieval, SIGIR ’21, page 1472–1482, New York,\nNY , USA. Association for Computing Machinery.\nZhiruo Wang, Zhengbao Jiang, Eric Nyberg, and Gra-\nham Neubig. 2022. Table retrieval may not necessi-\ntate table-specific model design. In Proceedings of\nthe Workshop on Structured and Unstructured Knowl-\nedge Integration (SUKI), pages 36–46, Seattle, USA.\nAssociation for Computational Linguistics.\nXiaojun Xu, Chang Liu, and Dawn Song. 2018. SQL-\nNet: Generating structured queries from natural lan-\nguage without reinforcement learning.\nJingfeng Yang, Aditya Gupta, Shyam Upadhyay,\nLuheng He, Rahul Goel, and Shachi Paul. 2022.\nTableFormer: Robust transformer modeling for table-\ntext encoding. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 528–537,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nTao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,\nDongxu Wang, Zifan Li, James Ma, Irene Li, Qingn-\ning Yao, Shanelle Roman, Zilin Zhang, and Dragomir\nRadev. 2018. Spider: A large-scale human-labeled\ndataset for complex and cross-domain semantic pars-\ning and text-to-SQL task. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 3911–3921, Brussels, Bel-\ngium. Association for Computational Linguistics.\nA Table Linearization\nIn the retriever component, the input table is lin-\nearized into a sequence with separation tokens\ninterleaving the table elements to make the in-\nput structure-aware, e.g. “ <SOT> [table title]\n<EOT> <BOC> mountain peak <SOC> eleva-\ntion <EOC> <BOR> red slate mountain <SOR>\n13,162 ft <EOR> <BOR> ...”.\nIn the reader component, the TaPEx tokenizer\nlinearizes the table with structure-aware separation,\nfor example, “[HEAD] mountain peak | elevation\n[ROW] 1 : red slate mountain | 13 , 162 ft [ROW]\n2 ...”.\nB Dataset Statistics\nDataset Train Dev Test #Tables\nNQ-TABLES 9,594 1,068 966 169,898\nE2E-WTQ 851 124 241 2,108\nTable 3: Dataset statistics.\nParameter Value\nNegative samples 4 (per positive sample)\nTotal GPUs 8\nLearning rate 0.0001\nOptimizer Adam\nBatch size (per device) 8 (DPR) / 6 (LI)\nGrad. accum. steps 4\nTraining steps 6000 (NQ-TABLES)\n600 (E2E-WTQ)\nTable 4: Hyperparameters for DPR and LI training.\nParameter Value\n(NQ-TABLES)\nValue\n(E2E-WTQ)\nWarmup steps 0\nEpochs 20 15\nReader LR 0.00002 0.000015\nRetriever LR 0.00001\nLR decay Linear None\nOptimizer AdamW\nTotal GPUs 8\nBatch size 1 (per device)\nGrad. accum. steps 4\nWeight decay 0.01\nLabel smoothing 0.1\nTable 5: Hyperparameters for LI-RAGE training.\nC CLTR and T-RAG Evaluation\nIn these open-domain TableQA datasets, each ques-\ntion is associated with only one gold table. As a\nresult, Precision@K in retrieval has a certain upper\nbound at 1\nK. Therefore, evaluating the retriever\nwith Recall@K is more reasonable in this case.\nWe confirmed with the authors of CLTR and T-\nRAG that they decomposed tables into single rows\nand columns to form the table database. In evalu-\nating their systems on the E2E-WTQ dataset, the\nauthors reported some retrieval metrics including\nPrecision@K (P@K) which goes beyond the 1\nK\nlimit (e.g. T-RAG achieved 0.7806 P@5). This is\nbecause they reported a hit for a retrieved row/col-\numn as long as it belongs to the gold table. With\ndifferent setups for table corpus, the retrieval met-\nrics of their systems are not directly comparable.\nTherefore, we compare Recall@K with BM25 and\nDPR only, and compare the end-to-end TableQA\naccuracy with CLTR and T-RAG (which is called\nHit@1 in their papers).\n1563\nModels Training Speed\n(iter/sec)\nTraining\nBatch Size\nTraining Time\n(mins)\nInference Speed ↑\n(sec/iter)\nInference\nBatch Size\nDPR 1.10 8 60 (NQ)/ 10 (WTQ) - -\nLI 1.75 6 60 (NQ)/ 10 (WTQ) - -\nDPR-RAGE 2.1 1 300 (NQ)/ 35 (WTQ) 1.22 4\nLI-RAGE 0.74 1 450 (NQ)/ 50 (WTQ) 1.40 4\nTable 6: Computational cost for DPR/LI retriever models and LI-RAGE and DPR-RAGE.\nParameter Value\nWarmup steps 1000\nEpochs 40\nLearning Rate 0.00002\nLR decay Linear\nOptimizer AdamW\nTotal GPUs 8\nBatch size 1 (per device)\nGrad. accum. steps 4\nWeight decay 0.01\nLabel smoothing 0.1\nTable 7: Hyperparameters for tapex-large fine-tuning\non WikiTableQuestions for E2E-WTQ.\nD Technical Details\nD.1 Hyperparameters\nThe training hyperparameters are shown in Table 4,\n5, and 7. The tuning of hyperparameters was per-\nformed on validation performance.\nDPR: The dimension of the extracted table/ques-\ntion embeddings is d= 768.\nLI: The dimension of the extracted table embed-\ndings is lt×d= lt×128, where lt depends on the\nlength of input tables. Following Santhanam et al.\n(2022b), the dimension of the extracted question\nembeddings is fixed to lq ×d= 32×128. We pad\nthe questions with less tokens than lq.\nD.2 Indexing and Dynamic Retrieval\nDPR. Following Lewis et al. (2020b), one-\ndimensional table embeddings are pre-extracted\nwith the DPR model that has been finetuned on\nthe retrieval task. The FAISS system (Johnson\net al., 2019) is used to index all table embeddings\nwhich enables fast nearest neighbour search with\nsub-linear time complexity. In training LI-RAGE,\nquestion embeddings are dynamically extracted\nfrom the retriever, and tables with highest scores\nare retrieved using the precomputed index.\nLI. Khattab and Zaharia (2020) proposed the first\nversion of ColBERT, and Santhanam et al. (2022b)\nintroduced ColBERTv2, which is an enhanced ver-\nsion of ColBERT. Santhanam et al. (2022a) de-\nveloped an efficient search engine, PLAID, for\nColBERTv2, which significantly improved the re-\ntrieval latency. We redirect readers to the aforemen-\ntioned papers for more details. We started from\nthe official ColBERTv2 implementation6and refac-\ntored the code base. We integrated ColBERTv2\ninto our training framework, so that fast and dy-\nnamic retrieval can be done during end-to-end joint\ntraining.\nD.3 Computational Cost\nIn Table 6 we report computational cost of the pro-\nposed models. It is clear that time spent on the\ntraining of LI is not significantly increased com-\npared to DPR training. This is because both models\nuse contrastive learning in training. But we note\nthat the index building time of LI is around 5 mins\nwhile that of DPR only takes 40 seconds.\nIn terms of joint training, the end-to-end training\ntime of LI-RAGE is longer. This is due to (1)\nslightly slower dynamic retrieval during end-to-\nend training; (2) refining the retriever via larger\nmulti-dimensional embeddings in comparison to\none-dimensional embeddings used in DPR-RAGE.\nHowever, the inference speed is not affected much\n(from 1.22 sec/iteration to 1.40). This suggests that\nwhen deployed as real applications, LI-RAGE does\nnot bring significant increase in computation.\n6https://github.com/stanford-futuredata/\nColBERT\n1564\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nSection 7\n□ A2. Did you discuss any potential risks of your work?\nNot applicable. Left blank.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nSection 1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nSection 3 and 4\n□\u0013 B1. Did you cite the creators of artifacts you used?\nSection 3 and 4, and Appendix B and D\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNot applicable. We do not distribute artifacts with this submission. Upon acceptance we will release\ncode.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNot applicable. Left blank.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. We use public datasets from the literature\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNot applicable. Left blank.\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nAppendix B\nC □\u0013 Did you run computational experiments?\nSection 4\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nAppendix D\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n1565\n□ C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nNo response.\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 5\n□ C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nNot applicable. Left blank.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNot applicable. Left blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNot applicable. Left blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. Left blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n1566",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7559212446212769
    },
    {
      "name": "Security token",
      "score": 0.5798593163490295
    },
    {
      "name": "Relevance (law)",
      "score": 0.5604058504104614
    },
    {
      "name": "Table (database)",
      "score": 0.5571023225784302
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5330051183700562
    },
    {
      "name": "Representation (politics)",
      "score": 0.5209218859672546
    },
    {
      "name": "Filter (signal processing)",
      "score": 0.4803789258003235
    },
    {
      "name": "Information retrieval",
      "score": 0.47278332710266113
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.45566079020500183
    },
    {
      "name": "Inference",
      "score": 0.4454025626182556
    },
    {
      "name": "Labrador Retriever",
      "score": 0.4351205825805664
    },
    {
      "name": "Data mining",
      "score": 0.34470298886299133
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3315412700176239
    },
    {
      "name": "Theoretical computer science",
      "score": 0.3268502950668335
    },
    {
      "name": "Computer vision",
      "score": 0.10517489910125732
    },
    {
      "name": "Programming language",
      "score": 0.10116422176361084
    },
    {
      "name": "Mathematics",
      "score": 0.09916350245475769
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Medicine",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Pathology",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    }
  ]
}