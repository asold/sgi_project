{
    "title": "Enhancing Self-Consistency and Performance of Pre-Trained Language Models through Natural Language Inference",
    "url": "https://openalex.org/W4385573045",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2096617519",
            "name": "Eric Mitchell",
            "affiliations": [
                "Stanford University"
            ]
        },
        {
            "id": "https://openalex.org/A3179708263",
            "name": "Joseph Noh",
            "affiliations": [
                "Stanford University"
            ]
        },
        {
            "id": "https://openalex.org/A2154119911",
            "name": "Siyan Li",
            "affiliations": [
                "Stanford University"
            ]
        },
        {
            "id": "https://openalex.org/A2135414257",
            "name": "Will Armstrong",
            "affiliations": [
                "Stanford University"
            ]
        },
        {
            "id": "https://openalex.org/A5041328979",
            "name": "Ananth Agarwal",
            "affiliations": [
                "Stanford University"
            ]
        },
        {
            "id": "https://openalex.org/A2102161365",
            "name": "Patrick Liu",
            "affiliations": [
                "Stanford University"
            ]
        },
        {
            "id": "https://openalex.org/A2107089640",
            "name": "Chelsea Finn",
            "affiliations": [
                "Stanford University"
            ]
        },
        {
            "id": "https://openalex.org/A2151390485",
            "name": "Christopher D. Manning",
            "affiliations": [
                "Stanford University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2912924812",
        "https://openalex.org/W2995194170",
        "https://openalex.org/W3126792443",
        "https://openalex.org/W3196886373",
        "https://openalex.org/W2963323070",
        "https://openalex.org/W4287820586",
        "https://openalex.org/W2995373617",
        "https://openalex.org/W2561529111",
        "https://openalex.org/W3035438620",
        "https://openalex.org/W4285240908",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W2189149359",
        "https://openalex.org/W4286897388",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W2970746059",
        "https://openalex.org/W2150859678",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W2970231061",
        "https://openalex.org/W2788277448",
        "https://openalex.org/W2987504399",
        "https://openalex.org/W3104499181",
        "https://openalex.org/W3034850762",
        "https://openalex.org/W3166845084",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3213990450",
        "https://openalex.org/W2963640662",
        "https://openalex.org/W2277195237",
        "https://openalex.org/W2066770300",
        "https://openalex.org/W4281567264",
        "https://openalex.org/W2949849869",
        "https://openalex.org/W4205924343",
        "https://openalex.org/W2891308403",
        "https://openalex.org/W2997306177",
        "https://openalex.org/W3202712981",
        "https://openalex.org/W4230262515"
    ],
    "abstract": "Eric Mitchell, Joseph Noh, Siyan Li, Will Armstrong, Ananth Agarwal, Patrick Liu, Chelsea Finn, Christopher Manning. Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022.",
    "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1754–1768\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nEnhancing Self-Consistency and Performance of Pre-Trained Language\nModels through Natural Language Inference\nEric Mitchell, Joseph J. Noh, Siyan Li, William S. Armstrong,\nAnanth Agarwal, Patrick Liu, Chelsea Finn, Christopher D. Manning\nStanford University\neric.mitchell@cs.stanford.edu\nAbstract\nWhile large pre-trained language models are\npowerful, their predictions often lack logical\nconsistency across test inputs. For example,\na state-of-the-art Macaw question-answering\n(QA) model answers Yes to Is a sparrow a\nbird? and Does a bird have feet? but answers\nNo to Does a sparrow have feet?. To address\nthis failure mode, we propose a framework,\nConsistency Correction through Relation De-\ntection, or ConCoRD, for boosting the consis-\ntency and accuracy of pre-trained NLP models\nusing pre-trained natural language inference\n(NLI) models without fine-tuning or re-training.\nGiven a batch of test inputs, ConCoRD sam-\nples several candidate outputs for each input\nand instantiates a factor graph that accounts\nfor both the model’s belief about the likeli-\nhood of each answer choice in isolation and\nthe NLI model’s beliefs about pair-wise answer\nchoice compatibility. We show that a weighted\nMaxSAT solver can efficiently compute high-\nquality answer choices under this factor graph,\nimproving over the raw model’s predictions.\nOur experiments demonstrate that ConCoRD\nconsistently boosts accuracy and consistency of\noff-the-shelf closed-book QA and VQA mod-\nels using off-the-shelf NLI models, notably in-\ncreasing accuracy of LXMERT on ConVQA\nby 5% absolute. See the project website 1 for\ncode and data.\n1 Introduction\nReliable and trustworthy AI systems should demon-\nstrate internal self-consistency, in the sense that\ntheir predictions across inputs should imply logi-\ncally compatible beliefs about the world. However,\neven powerful large language models are known\nto lack self-consistency (Ray et al., 2019; Elazar\net al., 2021; Kassner et al., 2021). For example, a\nquestion-answering (QA) model that answers the\nquestion Is a sparrow a bird? and Does a bird have\n1https://ericmitchell.ai/emnlp-2022-concord/\nFigure 1: ConCoRD first generates candidate outputs from\nthe base pre-trained model, then estimates soft pairwise con-\nstraints between output choices, and finally finds the most\nsatisfactory choices of answers accounting for both the base\nmodel and NLI model’s beliefs.\nfeet? with Yes is implicitly expressing the belief\nthat A sparrow is a bird and A bird has feet. If the\nsame model answers the question Does a sparrow\nhave feet? with No, the model expresses the logi-\ncally incompatible belief A sparrow does not have\nfeet. In such cases, ascertaining the model’s “true”\nbelief is difficult, making interpreting and validat-\ning its behavior correspondingly challenging.\nPrior work has improved model self-consistency\nby training with specialized loss functions (Elazar\net al., 2021) or data augmentation (Ray et al., 2019),\nor alternatively re-ranking model predictions based\non their mutual self-consistency using pre-written\nlogical constraints, such as “all mammals have fur”\n(Kassner et al., 2021). However, the first class\nof methods requires expensive fine-tuning which\nmight be impractical for many practitioners for very\nlarge pre-trained models, and re-ranking methods\nrequire an explicit collection of the logical rela-\ntions of interest, making scaling a challenge. Still,\n1754\nre-ranking-based approaches have the benefit of\nnot requiring fine-tuning, and we hypothesize that\ntheir scalability limitations may be addressed by\nestimating logical relationships between model pre-\ndictions on the fly. Specifically, we hypothesize\nthat existing pre-trained natural language inference\n(NLI) models can estimate logical relationships be-\ntween an arbitrary pair of model predictions well\nenough to provide an effective, scalable substitute\nfor explicit collection of such constraints. Leverag-\ning these estimated constraints, we can construct a\nfactor graph representing a probability distribution\nover model outputs that incorporates both the origi-\nnal model’s confidence scores and the NLI model’s\nbeliefs about logical relationships.\nOur primary contribution is Consistency Correc-\ntion through Relation Detection, or ConCoRD, a\nframework to improve the consistency and perfor-\nmance of a pre-trained base language model with-\nout fine-tuning by using more confident and better\nattested model predictions to override less confi-\ndent model beliefs. See Figure 1 for an overview.\nTo enable propagation of model beliefs, we esti-\nmate pair-wise logical relationships between model\npredictions using a pre-trained NLI model. Us-\ning these pair-wise relationships, we define an\nundirected graphical model representing a distribu-\ntion over responses accounting for both the base\nmodel’s beliefs and the NLI model’s estimates of\nanswer compatibility. We efficiently find the ap-\nproximate mode of this distribution among the base\nmodel’s top answer choices for each input as the\nsolution of a MaxSAT problem, which consistently\nproduces more accurate and self-consistent predic-\ntions than using the raw model predictions. In\nSection 4.1 we find that ConCoRD produces an\n8.1% absolute improvement in F1 of a pre-trained\nMacaw model (Tafjord and Clark, 2021) on the\nBeliefBank QA dataset (Kassner et al., 2021). In\nSection 4.2 we find a 5.0% absolute improvement\nin accuracy of a pre-trained LXMERT model (Tan\nand Bansal, 2019) on the ConVQA dataset (Ray\net al., 2019), and in Section 4.3 we find that Con-\nCoRD enables test-time model editing (Sinitsin\net al., 2020; Mitchell et al., 2022), updating model\npredictions at test time when presented with new\ninformation.\n2 Related Work\nPrior work for maintaining consistency in the\nquestion-answering space often involves additional\ntraining to improve performance. Chen et al. (2021)\ntransform the Natural Questions (Kwiatkowski\net al., 2019) dataset question-answer pairs into\npremise-hypothesis pairs, then uses an NLI model\ntrained on this dataset as a decider for unanswer-\nable questions. Alberti et al. (2019) generate ques-\ntions from unlabeled texts, then filter them to en-\nsure roundtrip consistency; pre-training on this\nsynthetic set improves performance on SQuAD\n2.0 (Rajpurkar et al., 2018) and Natural Ques-\ntions. Asai and Hajishirzi (2020) augment QA-\npairs with their logically symmetric and transitive\ncounterparts through linguistic approaches to en-\nhance cross-dataset QA performance. ConCoRD\ndiffers significantly from these question-answering-\nspecific approaches because no fine-tuning of the\nbase model is needed and the methodology is not\nspecific to question-answering.\nSimilarly to ConCoRD, Kassner et al. (2021)\nre-rank model predictions by solving an optimiza-\ntion problem defined by a combination of the base\nmodel confidence scores and pair-wise constraints\nrepresenting the logical compatibility of different\nmodel predictions stored in a persistent memory,\nwhich they call BeliefBank. The key distinguish-\ning property of ConCoRD is the fact that pair-wise\nconstraints between model predictions are dynami-\ncally estimated by a pre-trained NLI model, rather\nthan drawn from a fixed, pre-collected set of con-\nstraints. Dynamically estimating the constraints\nhas a variety of benefits, eliminating the need for\nmanually collecting the logical constraints of inter-\nest, automating the process of determining whether\na particular constraint applies to a particular pair of\npredictions, and likely inheriting improvements in\nNatural language inference (NLI, MacCartney and\nManning (2008)) models over time.\nNLI has long been used to maintain logical con-\nsistency in generated dialogue utterances (Welleck\net al., 2019; Dziri et al., 2019; Song et al., 2020), ra-\ndiology report domain entities (Miura et al., 2021),\nand summarization (Laban et al., 2022; Honovich\net al., 2022). Perhaps most similarly, Jung et al.\n(2022) use NLI to estimate constraints between fac-\ntual statements produced by GPT-3. These prior ap-\nproaches support our intuition for using NLI mod-\nels to improve logical consistency among batches\nof answers. While the authors explore applica-\ntions of this framework to multi-step reasoning\nfor True/False questions or statements, our work\nfocuses on applying this methodology to more gen-\n1755\nFigure 2: An example factor graph for a simplified batch with two questions, q1 = What is the capital of Afghanistan? and q2 =\nWhat is the capital of Georgia?. Although Tbilisi is the most likely answer for both questions, the assignment of variables that is\nbest under the estimated contradiction constraint flips the answer to the first question to Kabul. The top-2 answer choices for\neach question are sampled from the base model, and a soft contradiction constraint is detected between variablesz1 (representing\nthe truth of the answer Tbilisi for q1) and z3 (representing the truth of the answer Tbilisi for q2).\neral settings, such as VQA, open-ended QA, and\nmodel editing.\n3 Consistency Correction through\nRelation Detection\nConCoRD contains three key components, thebase\nmodel, a relation model (typically a pre-trained\nNLI model), and an inference procedure that com-\nbines the predictions of the two models into a more\naccurate and self-consistent set of beliefs. Impor-\ntantly, both the base model and relation model are\npre-trained, off-the-shelf models; ConCoRD does\nnot update any weights or require training data for\neither model, using only a small validation set for\nhyperparameter tuning. We next explain the func-\ntion of each of these components when executing\nConCoRD.\n3.1 Base Model\nThe core function of the base model in ConCoRD\nis generating a set of candidate outputs for a given\ninput, which are ultimately re-ranked by the infer-\nence process (Sec. 3.3). Given a batch of N model\nqueries Q= {qi}, the first step of ConCoRD is to\ngenerate a set ofJcandidate outputs for each query\nˆAi = {ˆai1,..., ˆaiJ}, along with their correspond-\ning likelihoods pθ(ˆaij|qi). Note that the candidate\noutputs need not be an IID sample from the base\nmodel; for example, we might use beam search\nwith a diversity bonus to produce a more diverse\nset of candidates (Vijayakumar et al., 2018). Each\npair of query and candidate output forms a model\nbelief bij = (qi,ˆaij); the output of the base model\nis the complete set of model beliefs B = {bij}\nand their corresponding normalized probabilities\npij\nθ\n2. The base models in our experiments are pre-\ntrained question-answering models based on T5-\nlarge (Raffel et al., 2020) and pre-trained visual\nquestion-answering models such as LXMERT (Tan\nand Bansal, 2019) and ViLT (Kim et al., 2021).\n3.2 Relation Model\nThe relation model pϕ(·|x,x′) estimates the\nmost likely logical relationship between an or-\ndered pair of natural language utterances from\nthe choices {none, fwd-entail, contradict,\nequivalence}.3 In addition to the model be-\nliefs B, we define optional context statements\ncijk = C(bij), K relevant statements that may\nbe retrieved, generated, or manually written for\neach model belief. The ability to incorporate con-\ntext statements enables ConCoRD to modulate\nmodel behavior independently for each input in\nthe test batch, rather than reasoning transductively\nabout pairs of test inputs. See Table 3 for exam-\nples of model beliefs and context statements. In-\nputs to the relation model are either pairs of two\nmodel beliefs (bij,bi′j′ ) or pairs of one model be-\nlief and one context statement (bij,cijk). We de-\nfine the most likely inter-belief relation asrij,i′j′ =\nargmaxrpϕ(r|bij,bi′j′ ), and similarly for belief-\ncontext relations rijk = argmaxrpϕ(r|bij,cijk).\nThe output of the relation model is the set of most-\nlikely relations R = {rij,i′j′ }∪{ rijk}and their\nassociated probabilities, which we denote as pij,i′j′\nϕ\nand pijk\nϕ . Our experiments use various pre-trained\nNLI models based on RoBERTa (Liu et al., 2019)\n2Normalized such that ∑\njpij\nθ = 1.\n3Because relationships are estimated between ordered\npairs of utterances, we can form an equivalence relation if\nfwd-entail is predicted for both orderings of the utterances.\n1756\nand ALBERT (Lan et al., 2019) as the relation\nmodel.\nQuestion-answer to statement conversion.\nWhile concatenating query qi and candidate\noutput ˆaij to produce inputs to the relation model\nis perhaps the simplest approach to estimating\nsoft constraints, we use a statement conversion\nmodel to provide inputs to the relation model that\nare closer to its training distribution. Instead of\ndefining the belief bij = (qi,ˆaij) as concatenation\nof qi and ˆaij, we define bij to be the statement\nfψ(qi,ˆaij), where fψ is the conversion model. We\nfine-tune a small T5 model on a combination of\ndata from (Demszky et al., 2018) and BeliefBank\n(Kassner et al., 2021) to produce a model that maps\na (question, answer) pair into a natural language\nstatement. Details about the fine-tuning procedure\nand data are provided in Appendix C.\n3.3 Inference\nConCoRD’s inference procedure maps the set of\nbeliefs Band pair-wise relations Rinto a choice of\nthe most likely belief for each question. To define\nthe inference problem, we first define a binary de-\ncision variable zij representing the estimated truth\nvalue of model belief bij. A value of 1 for node zij\nin the maximum likelihood configuration means\nthat ˆaij is returned for query qi; the problem in-\ncludes a constraint that exactly one candidate an-\nswer is true for each query. The factor graph in-\ncludes the set of variables Z = {zij}N,J\ni,j=1,1 and\nvarious factors (functions mapping a subset of Zto\na non-negative scalar) derived from the base model\nand relation model’s beliefs and the hard constraint\nof returning only one answer per question. Factors\nare defined such that more desirable configurations\nof zij yield a larger product of the individual fac-\ntors. First, unary factors ϕij(zij) encode the base\nmodel’s beliefs about the likelihood of specific an-\nswers, and are defined as:\nϕij(zij) =\n{ pij\n1−pij\nif zij = 1\n1 otherwise\n(1)\nwhere pij = pθ(ˆaij|qi); in other words, the factor\ntakes the odds ratio if the corresponding statement\nvariable zij is assigned a truth value of 1; otherwise,\nthe factor takes value 1. In order to encode the\nhard constraint that exactly one output should be\nreturned for each query, we include a J-ary factor\nϕi(Zi) for each group of nodes Zi = {zij}J\nj=1,\nwhich is equal to 1 for configurations where exactly\none of the nodes takes a value of 1, and 0 for all\nother configurations.\nBinary factors ϕij,i′j′ (zij,zi′j′ ) and optionally\nϕijk(zij,cijk) encode compatibility between pairs\nof model beliefs (or model belief-context pairs):\nϕij,i′j′ (zij,zi′j′ ) =\n{\n1 if rij,i′j′ (zij,zi′j′ )\n1 −pij,i′j′\nϕ otherwise\nwhere we define the relation functionrij,i′j′ to eval-\nuate to true if its arguments satisfy the underlying\nrelation, and false otherwise; ϕijk(zij,cijk) is de-\nfined similarly to ϕij,i′j′ (zij,zi′j′ ) 4. The inference\nproblem amounts to findingargmaxZ ϕ(Z),where\nϕ(Z) =\n∏\ni\nϕi\n∏\nj\nϕij\n(∏\ni′j′\nϕij,i′j′\n)(∏\nk\nϕijk\n)\n.\n(2)\nAn approximate solution to this inference problem\ncan be efficiently found for most problems with a\nMaxSAT solver such as RC2 (Ignatiev, 2019). We\nomit arguments to the factors for conciseness. See\nFigure 2 for a simple example of a factor graph\nwith a single inter-belief constraint and no belief-\ncontext constraints.\nEntailment correction. Consider a belief b, a\nset of its entailed statements S = {si}i, unary\nfactors ϕ(zb) and {ϕ(zsi)}, and binary factors\nP = {ϕ(zb,zsi)}i. Recall that an entailment re-\nlation rbsi(zb,zsi) is satisfied (and the binary fac-\ntor is maximized) if either zb = 0or all zsi = 1.\nConsequently, as the cardinality of {zsi|zsi = 0}\nincreases, the more likely it is thatzb = 0will max-\nimize the product of all binary factors∏\niϕ(zb,zsi).\nThis is true even if most entailed statements are true,\ni.e., |{zsi|zsi = 1}|≫|{ zsi|zsi = 0}|. If most of\nthe statements entailed by a belief are true, assign-\ning the belief to be false due to a small number of\n(potentially spuriously) false entailed statements\nmay be undesirable. To mitigate this outcome,\nwe experiment with an additional type of factor\nin which configurations satisfying entailments with\nboth zb = 1and zsi = 1are ‘rewarded’ more than\n4We use this formulation only to accommodate settings\nwere multiple context statements are retrieved for each query;\nsee Section 4.3. We do not have any ϕijk factors if we are\nonly using the model’s predictions within a batch of test inputs\nas the premises for reasoning.\n1757\nother configurations satisfying the entailment:\nϕb,si(zb,zsi) =\n\n\n\n1 if zb,zsi = 1\n1 −pb,si\nϕ if zb,zsi = 0√\n1 −pb,si\nϕ otherwise\nApplying entailment correction consistently im-\nproves ConCoRD’s performance; see Appendix\nTable 8 for a dataset-by-dataset breakdown.\n3.4 Hyperparameters of ConCoRD\nWe introduce two key hyperparameters to Con-\nCoRD. Because we do not know a priori the rel-\native reliability of the base model and relation\nmodel, we introduce the hyperparameter β ∈[0,1],\ncorresponding to a trade-off between the predic-\ntions of the base model and relation model. A\nvalue of β = 1corresponds to simply taking the\nraw predictions of the base model, while β =\n0 corresponds to optimizing purely for answers\nthat are self-consistent according to the relation\nmodel, without considering the base model’s be-\nliefs. The unary factors in the factor graph become\nϕβ\nij(zij) = ( ϕij(zij))β and ϕβ\nij,i′j′ (zij,zi′j′ ) =\n(\nϕij,i′j′ (zij,zi′j′ )\n)1−β (and similarly for ϕβ\nijk). In\naddition to β, we introduce a threshold λfor rela-\ntion model confidence to filter out low-confidence\nrelation estimates. That is, we discard a relation\nrij,i′j′ or rijk if pij,i′j′\nϕ < λor pijk\nϕ < λ, respec-\ntively. In practice, we find that the optimal βand\nλvary across problems, perhaps due to the vary-\ning complexity of the model belief and context\nstatements (and therefore the reliability of the re-\nlation model’s predictions). Therefore, we use the\nhyperopt library (Bergstra et al., 2013) for auto-\nmated hyperparameter optimization, using the Tree\nParzen Estimator (TPE) algorithm to tune βand λ\njointly. We use the optimal hyperparameters found\non the validation data for each problem to compute\ntest performance. Appendix H.1 details hyperpa-\nrameter tuning for each experiment.\n4 Experiments\nOur experiments are broadly designed to answer\nthe high-level question: can ConCoRD leverage\nthe relational knowledge in pre-trained NLI models\nto produce more accurate, self-consistent system\nbehavior, without additional data or fine-tuning?\nFurther, we investigate ConCoRD’s applicability to\nperforming test-time model editing (Sinitsin et al.,\n2020; Mitchell et al., 2022), or injection of new in-\nformation, and ConCoRD’s sensitivity to the choice\nof hyperparameters and types of relations detected.\n4.1 Internal Consistency in Closed-Book\nQuestion-Answering\nProtocol. To evaluate the accuracy and consistency\nof a set B of beliefs, Kassner et al. (2021) syn-\nthesize a gold standard for those beliefs and the\ninferred relations R. Following this prior work, we\nassume the following is given:\n• A set of entities sm ∈S\n• A set of unary predicates Pn ∈P\n• A collection of “facts” (Pn(sm))i, whose bi-\nnary truth value is known\n• A directed graph of gold-standard con-\nstraints G(P,E), whose edges (Pn,Pn′ ) ∈\nE represent first-order logical formulae\n∀x(Pn(x) →Pn′ (x))\nFrom these, we construct simple yes/no questions\nusing natural language templates. For example,\nfor fact Pn(sm), if entity sm represents a lion and\npredicate Pn represents an ability to drink liquids,\nthe template-generated gold question answer pair\n(qi,ai) is Q: Is it true that a lion is able to drink\nliquids?; A: Yes.\nThese questions are given as input to one of two\nsizes of a multi-angle question answering model\n(Tafjord and Clark, 2021), given a multiple choice\nangle with choices Yes. and No. The questions and\nretrieved answers (qi,ˆai) form a set of beliefs Bsm\nfor each entity. Since these are closed-book ques-\ntions, no context statements are supplied; because\nthey are yes/no questions, only one candidate an-\nswer is obtained, i.e., J = 1. Question-answer\nto statement conversion is applied to all questions\nwith a default answer of Yes. regardless of the an-\nswer ˆai, in order to provide the relation model with\npositive natural language assertions from which to\ninfer sets of relations Rsm; where the base model\nanswers ˆai are No. we replace node zi in the factor\ngraph with its complement. Configurations Zsm\nare found for each sm ∈Swhich maximize Equa-\ntion 2 given Bsm,Rsm and together form a global\nsolution Z.\nDatasets. Kassner et al. (2021) provide a suitable\ndatabase with 12,636 facts (“silver facts”), each\nindicating whether one of 601 predicates relates\nto one of 85 entities, as well as 4,060 confidence-\nweighted first-order constraints manually gathered\nfrom ConceptNet (Speer et al., 2017), forming a\n1758\nBase ConCoRD G.C.\nModel F1 Con. F1 Con. F1 Con.\nMac-Lg 0.831 0.835 0.914 0.920 0.862 0.934\nMac-3B 0.855 0.871 0.931 0.947 0.905 0.936\nTable 1: F1 and consistency (1 - τ) for two sizes of Macaw\n(Tafjord and Clark, 2021) QA models, comparing ConCoRD\nto a naive QA baseline (Base) and ConCoRD with gold con-\nstraints (G.C.). ConCoRD significantly improves both F1 and\nconsistency for both models.\nconstraint graph G. Additionally, they provide\n1,072 distinct “calibration facts”, each relating one\nof 7 entities to one of 334 predicates.\nWe tune βand λusing a validation set of ques-\ntions generated from the calibration facts, and eval-\nuate test time performance with questions gener-\nated from silver facts.\nMetrics. We measure accuracy using binary F1\nbetween elements zi of the configuration Z maxi-\nmizing ϕ(Z) (as in Equation 2), and the truth value\nof facts (Pn(sm))i. As in Kassner et al. (2021);\nwe use F1 for evaluation because gold answers are\nhighly biased towards true No. answers.\nWe compute consistency within batches of\nquestions using the complement of of Li et al.\n(2019)’s conditional constraint violation metric τ,\ndefined here as the proportion of relevant gold\nconstraints in Gwhich are violated; a constraint\n∀x(Pn(x) →Pn′ (x)) is relevant iff, for some en-\ntity sm, there is some belief bi ∈Bsm from fact\n(Pn(sm))i such that zi = 1, and there is some be-\nlief bj ∈Bsm that corresponds to fact (Pn′ (sm))j;\nthe constraint is violated when zj = 0.\nComparisons. ConCoRD is evaluated against a\nnaive baseline where only base model answers ˆai\nand probabilities are considered. A second baseline\n(G.C.) performs the inference described in Sec. 3.3,\nreplacing the inferred relations R with the gold\nconstraints from constraint graph G, rather than\nthose estimated by the relation model.\nResults. Results are shown in Table 1. ConCoRD\nprovides an absolute improvement of over 8% in\nF1 and consistency for Macaw-Large and 7% for\nMacaw-3B compared to the baseline. Notably, the\nmargin of superiority of the Macaw-3B base model\nis mostly preserved after applying ConCoRD, sug-\ngesting that ConCoRD may provide a significant\nbenefit even for very large models. A surprising\nresult is that ConCoRD shows marked improve-\nments in F1 over the gold constraint baseline, sug-\ngesting that the detection and filtering of relations\nConCoRD provides may, in this setting, be an im-\nBase ConCoRD Oracle\nModel Acc. P.C. Acc. P.C. Acc. P.C.\nLXM 0.656 0.360 0.706 0.409 0.824 0.572\nViLT 0.784 0.489 0.804 0.548 0.882 0.690\nTable 2: ConVQA accuracy (Acc.) and perfect consistency\n(P.C.) of LXMERT (Tan and Bansal, 2019) and ViLT (Kim\net al., 2021) VQA models with and without ConCoRD. Con-\nCoRD significantly improves accuracy and consistency of both\nmodels. Oracle performance is top-2 performance, as Con-\nCoRD attempts to select the best of the top 2 answer choices\nof the base model.\nprovement over rigid adherence to the logical con-\nnections specified a priori in Kassner et al. (2021).\n4.2 Internal Consistency in VQA\nProtocol. The Visual Question Answering (VQA)\ntask involves a language model generating answers\nto questions that are directly associated with im-\nages. VQA tests for robustness and generalizability\nof ConCoRD as it introduces an additional layer of\ndifficulty; the task moves away from purely text-\nbased tasks while expanding the answer space to\nthe vocabulary of the LM being used. The ques-\ntions from the ConVQA dataset (Ray et al., 2019)\nand its associated images from the Visual Genome\ndataset (Krishna et al., 2016) provide an apt setting\nto assess ConCoRD, as the relatedness of ques-\ntions for each image provide ample opportunity for\nmodel self-inconsistency.\nThe ConVQA dataset consists of a set of images\neach associated with a group of related questions\nabout the image, such as What color is the horse?\nand Is the horse brown? for a picture of a brown\nhorse in a stable. We evaluate ConCoRD with two\nVQA models, LXMERT (Tan and Bansal, 2019)\nand ViLT (Kim et al., 2021). For each group of\nquestions Qn = {qni}i, we sample the top-2 can-\ndidate outputs {ˆani1,ˆani2}for each question, and\nuse a pre-trained NLI model to infer the most likely\npair-wise relations Rbetween outputs from differ-\nent questions. We use the RC2 MaxSAT Solver to\nestimate the configuration that maximizes Equation\n2.\nMetrics. We report accuracy as the proportion of\nquestions answered correctly across all groups. We\ninfer consistency using a metric previously used in\nthe literature for the ConVQA dataset called \"per-\nfect consistency\" (Ray et al., 2019). For all groups\nof related questions, a group is perfectly consistent\nif all its questions are answered correctly. Perfect\nconsistency then reports the proportion of question\ngroups that were perfectly consistent. While this\n1759\nInput & Gold Answer Generations Added context\nQ: What was the first capital city\nof Australia? A: Melbourne\nCanberra; Melbourne; Syd-\nney; Inverell\nMelbourne was the initial capital following the 1901\nFederation of Australia.\nQ: When does the implantation\nof the embryo occur?\nA: around 9 days after ovulation\n9 to 18 days; between 6\nand 12 days; after the ovu-\nlation; on the 9th week\nIn humans, implantation of a fertilized ovum is most\nlikely to occur around 9 days after ovulation, however\nthis can range between 6 and 12 days.\nTable 3: Success and failure in editing a model’s behavior with ConCoRD by adding new information to the context. The base\nmodel’s highest confidence answer is Underlined. Bold shows ConCoRD’s output after inference; with Teal, bold showing a\nsuccessful edit increasing F1 and Red, bold showing an edit that reduces F1.\nF1\nModel Base ConCoRD Oracle\nT5-Sm-NQ 0.207 0.225 0.281\nT5-Lg-NQ 0.314 0.328 0.393\nT5-3B-NQ 0.332 0.351 0.423\nTable 4: Using ConCoRD to inject contextual information\ninto a model’s decisions at test time. Injecting gold Natural\nQuestions contexts consistently improves performance over\nthe base model without requiring fine-tuning.\nis not a perfect measure of consistency as it ex-\ncludes cases in which incorrect answers are consis-\ntent with each other, it still serves as a meaningful\nproxy since the dataset was designed such that any\nincorrect answer in a question group implies the\npresence of inconsistency.\nDatasets. We divide the ConVQA dataset into a\n\"clean\" (i.e. human verified and filtered) test set\nand a non-test set (train + val + test as defined by\nRay et al. (2019)). From the non-test set, we sam-\nple 10,000 random images equivalent to 123,746\nquestions to be used as our validation set for tuning\nour two hyperparameters. We use the clean test set\n– 725 images and 6,751 questions – to report our\nfinal results.\nComparisons. ConCoRD is compared with a naive\nbaseline and a top-2 oracle upper bound. The naive\nbaseline is the answer with the highest VQA model\nprobability. Top-2 oracle upper bound selects the\ncorrect answer if present within the top-2 predic-\ntions of the VQA model. Top-2 is appropriate given\nour use of the top-2 candidate outputs to generate\ninferences with NLI models.\nResults. The final results for ConCoRD, baseline,\nand oracle upper bound are shown in Table 2. Con-\nCoRD increases the accuracy of LXMERT and\nViLT by 5% and 2% respectively, and the consis-\ntency of LXMERT and ViLT by 4.9% and 5.9% re-\nspectively. Examples in which ConCoRD correctly\nand incorrectly selects a candidate output different\nfrom the baseline output are shown in Figure 4 and\nFigure 5, respectively. In particular, the incorrect\nscenarios demonstrate several failure modes that\nmay be in part responsible for the gap between\nConCoRD and the oracle upper bound, suggesting\nfurther improvements of the components of Con-\nCoRD will also continually improve ConCoRD.\n4.3 Test-Time Information Injection\nProtocol. We perform an additional experiment\nto evaluate ConCoRD’s ability to integrate exter-\nnal factual information into its inference process,\nrather than only using other predictions in the test\nbatch. Such an ability enables editing a model’s\nbehavior at test time, without re-training, as new\ninformation becomes available. We use the Natural\nQuestions (NQ; Kwiatkowski et al. (2019)) dataset,\nrather than BeliefBank, to provide more challeng-\ning inputs to the relation model. Given a question\nfrom NQ, a sentence from the ground truth con-\ntext document containing information about the\nanswer is retrieved and provided as an additional\ninput to ConCoRD; we constrain the node repre-\nsenting this context variable in the factor graph to\nbe true. Constraints are predicted between each\nanswer choice and the context statement. As in the\nother experimental settings, hyperparameters are\ntuned on the validation set and applied on the test\nset. See Appendix H for tuning procedures.\nMetrics. Model performance is evaluated using\nthe SQuAD F1 score for overlapping tokens5, fol-\nlowing the same answer normalization protocols,\nincluding lower-casing and removing punctuation.\nDatasets. The NQ development set consists of\n7830 open-book question-answer pairs, with both\nlong and short gold annotations in their context\npassages. Since the NQ test set is not available,\nwe create a test and validation set from the NQ\nvalidation questions as follows: we take the first\n5000 questions to form our test set, and the rest to\nbe our val set, which we use for hyperparameter\ntuning. Then each set is filtered such that only\n5https://worksheets.codalab.org/bundles/\n0xbcd57bee090b421c982906709c8c27e1\n1760\nthe answerable questions remain. “Answerable” is\ndefined as having a “short answer\" span defined in\nthe annotations. This filtering process gives 2713\ntest entries and 1576 val entries.\nComparisons. ConCoRD is compared with a naive\nbaseline and an oracle upper bound. All of these\napproaches operate on the fixed set of QA model\nanswers for a specific QA model (one of T5-Sm-\nNQ, T5-Lg-NQ, and T5-3B-NQ), specifically the\nset of top-4 answers for each question. The naive\nbaseline selects the answer with the highest QA\nmodel probability, argmaxˆaij pθ(ˆaij|qi). The ora-\ncle upper bound approach selects the answer that\nhas the best score with the gold short answer span,\nargmaxˆaij F1(ˆaij,aij).\nResults. The results on the test set using the naive\nbaseline, ConCoRD, and oracle upper-bound are\nreported in Table 4. ConCoRD always outper-\nforms the naive approach, demonstrating that the\nframework is useful even when each query input is\nprocessed independently (i.e., non-transductively).\nHowever, despite providing a relative gain of as\nhigh as 8.7% over the naive baseline, there is still\na gap between ConCoRD and the oracle. This gap\nmay be attributable to the complexity of the NQ\nquestions and context information compared with\nthe statements in prior experimental settings. Chen\net al. (2021) demonstrate a significant gain in cal-\nibration performance from training on MultiNLI\n(Williams et al., 2018) to training on a combination\nof MultiNLI and their NLI corpus adapted from\nNQ, perhaps hinting that crucial knowledge present\nin Natural Questions is not covered in MultiNLI,\npartially explaining the gap between ConCoRD\nand oracle F1 performance. Overall, these results\nsuggest that ConCoRD can reason between context\nstatements and model beliefs in addition to pairs of\nmodel beliefs, improving performance even with\nthe increased complexity of the data.\nQualitative Analyses. Examples of “good” and\n“bad” edits (edits that improve and decrease the\nresulting F1-scores respectively) are presented in\nTable 3, with more in Appendix F. When the cor-\nrect answer is not available in the candidate outputs,\nConCoRD is capable of pushing towards more par-\ntially correct answers and those that have more\noverlap with the context.\n4.4 Ablating Relation Types\nGiven that we consider two types of relations in\nour experiments, contradiction and entailment, it\nF1/Accuracy\nModel Task ConCoRD Only cont. Only ent.\nMac-Lg BB 0.914 0.892 0.827\nMac-3B BB 0.931 0.865 0.917\nLXM CVQA 0.706 0.691 0.700\nViLT CVQA 0.804 0.792 0.800\nT5-Sm-NQ NQ 0.225 0.225 0.225\nT5-Lg-NQ NQ 0.328 0.331 0.330\nT5-3B-NQ NQ 0.351 0.349 0.350\nTable 5: Ablating the relation types considered in ConCoRD’s\ninference procedure. The Only cont. and Only ent. are the\nresults of applying ConCoRD with all entailment or contradic-\ntion relations removed, respectively. The ConCoRD column\nis a reproduction of the results from Sections 4.1-4.3, for con-\nvenience. Value shown is F1 score for BeliefBank (BB) and\nNatural Questions (NQ) and accuracy for ConVQA (CVQA).\nNote that hyperparameters βand λare re-tuned on the respec-\ntive validation set for each setting.\nis natural to wonder the relative contribution of\nthese to ConCoRD’s performance improvement;\nTable 5 shows the results of this ablation. We re-run\nConCoRD with either entailment or contradiction\nrelations removed, re-tuning the hyperparameters\nfor both of the new settings (contradiction-only or\nentailment-only). We find that the relative con-\ntribution of contradiction and entailment relations\nvaries significantly across models even within the\nsame task, but using both relation types always per-\nforms approximately as well or better than using\njust one, suggesting that both types of detected rela-\ntions from the NLI model carry useful information.\nHowever, we observe in several cases, such as ViLT\nand the T5 models, that the entailment and contra-\ndiction relations may encode somewhat redundant\ninformation, as the performance when including\neither type of constraint alone nearly matches that\nof using both types.\n4.5 Hyperparameter Sensitivity\nWe perform several experiments to clarify the rela-\ntionship between the key hyperparameters, includ-\ning the specific relation NLI model, β, and λ.\nImpact of varying relation model. Table 6\nshows a comparison of ConCoRD’s test perfor-\nmance for several NLI models for each setting; no-\ntably, the best-performing NLI model is not consis-\ntent across problems. While the Albert-XXL model\nfrom Nie et al. (2020) is the strongest performing\nmodel on NQ, the simpler RoBERTa-Large models\noutperform it on BeliefBank and ConVQA.\nSensitivity to βand λ. Figure 3 shows the per-\nformance of ConCoRD on ConVQA with ViLT as\n1761\nFigure 3: Change in ConCoRD’s exact-match validation ac-\ncuracy as λ(the NLI confidence threshold) and β (tradeoff\nbetween base model and relation model beliefs) vary, holding\nrelation model RoBERTa-Large ANLI constant. By com-\nparing the maximum value within each column or row, we\nconclude that ConCoRD is relatively robust to the choice of\nλ, which the choice of βis more important. Values are those\nencountered during tuning with base model ViLT on ConVQA\nvalidation questions. Gray squares correspond to regions not\nevaluated during search, and asterisks (***) mark the region\nwhere the maximum increase in accuracy occurs.\nβ (the tradeoff between base model and relation\nmodel beliefs) and λ(the NLI confidence thresh-\nold) are varied, using the values explored during\nhyperparameter optimization. Section H.2 of the\nAppendix shows similar visualizations for differ-\nent VQA experiments. If multiple hyperparame-\nters within a grid element were explored, the best\nperforming configuration is shown. While the max-\nimum value in each column is the same (0.04),\nindicating that there exists a good value of β for\nalmost any λ, the converse is not true; for some\nvalues of β, no good value of λexists. Thus, we\nconclude that the tradeoff parameter βis the more\nimportant parameter to tune carefully.\n5 Discussion & Conclusion\nWe have presented the ConCoRD framework for\nenforcing self-consistency in pre-trained language\nmodels using relations estimated by pre-trained\nNLI models, showing that it improves over off-the-\nshelf performance in a variety of settings without\nrequiring any fine-tuning. Our findings suggest that\nexisting pre-trained NLI models can be a useful\nbuilding block for boosting performance of NLP\nsystems by providing useful estimates of logical\nrelationships between model predictions across var-\nious models and datasets for QA and visual QA.\nConCoRD also suggests several directions for\nfuture work. Integrating ConCoRD with meth-\nF1/Accuracy\nNLI Model Data BB ConVQA NQ\nAlb-XXL ANLI 0.892 0.689 0.351\nRoB-Lg ANLI 0.931 0.706 0.344\nRoB-Lg MNLI 0.918 0.706 0.346\nTable 6: Comparing ConCoRD’s performance for various NLI\nmodels on BB (BeliefBank), ConVQA, and NQ. Performance\nis measured as F1 score between predicted and gold text for\nBB and NQ, exact match accuracy for ConVQA. We use\nMacaw 3B for BB results, LXMERT for VQA results and\nT5-3B for NQ results. The best NLI model(s) in each column\nare bolded; the best NLI model varies across problems.\nods that generate questions likely to elicit use-\nful knowledge for answering the question at hand\n(Ray et al., 2019; Shwartz et al., 2020) may further\nimprove performance. In addition, integrating a\nframework such as ConCoRD with recent methods\nfor differentiation through black box combinatorial\nsolvers (Poganˇci´c et al., 2020) may enable train-\ning of the entire base model, relation model, and\ninference pipeline end-to-end, potentially further\nimproving aggregate performance. Finally, Con-\nCoRD’s general mechanism of re-ranking predic-\ntions by estimating the self-consistency of groups\nof model predictions is applicable beyond natu-\nral language, and future work might investigate\nits application to problems in vision or sequential\ndecision-making. We hope that ConCoRD may\nserve as another promising example of integrat-\ning both neural and explicit symbolic inference\nmachinery into a broader intelligent system that\noutperforms any of its components individually.\n6 Limitations\nWhile our results suggest ConCoRD can effectively\nleverage additional compute to boost model per-\nformance without fine-tuning, our work has some\nlimitations. Although ConCoRD is conceptually\napplicable to generations from any language model,\nour work focuses on question-answering settings to\nleverage existing self-consistency benchmarks. In\naddition, ConCoRD increases the compute costs of\ninference, although it does not require fine-tuning.\nFurther, our results suggest that the best NLI model\nto use for ConCoRD may vary across domains, re-\nquiring some tuning. As NLI models improve, we\nmight hope that the final performance of ConCoRD-\nlike systems should also inherit these gains, but Ta-\nble 6 suggests that the factors that make a particular\nNLI model well-suited to a particular problem are\nnot obvious, requiring further investigation.\n1762\nAcknowledgements\nThe authors would like to thank the anonymous\nreviewers for their helpful feedback during the re-\nview period, Gabe Mudel, Julie Wang, Cameron\nTew, Anthony Tzen, Kevin Yang, and Ian Ng for\nhelpful discussions and assisting with exploratory\nexperiments early on in the project, and Nora Kass-\nner for providing helpful early guidance in con-\nfiguring the BeliefBank experiments. CF and CM\nare CIFAR Fellows. EM gratefully acknowledges\nfunding from the Stanford Knight-Hennessy Grad-\nuate Fellowship. JN is supported by Stanford Uni-\nversity Medical Scientist Training Program grants\nT32-GM007365 and T32-GM145402. SL acknowl-\nedges brownie bites from Target for providing a cru-\ncial fuel source for late night experiment-running.\nReferences\nChris Alberti, Daniel Andor, Emily Pitler, Jacob Devlin,\nand Michael Collins. 2019. Synthetic QA corpora\ngeneration with roundtrip consistency. In Proceed-\nings of the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 6168–6173, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nAkari Asai and Hannaneh Hajishirzi. 2020. Logic-\nguided data augmentation and regularization for con-\nsistent question answering. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 5642–5650, Online. Asso-\nciation for Computational Linguistics.\nJames Bergstra, Dan Yamins, David D Cox, et al. 2013.\nHyperopt: A python library for optimizing the hy-\nperparameters of machine learning algorithms. In\nProceedings of the 12th Python in science confer-\nence, volume 13, page 20. Citeseer.\nJifan Chen, Eunsol Choi, and Greg Durrett. 2021. Can\nNLI models verify QA systems’ predictions? In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2021 , pages 3841–3854, Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\nDorottya Demszky, Kelvin Guu, and Percy Liang.\n2018. Transforming question answering datasets\ninto natural language inference datasets. CoRR,\nabs/1809.02922.\nNouha Dziri, Ehsan Kamalloo, Kory Mathewson, and\nOsmar Zaiane. 2019. Evaluating coherence in dia-\nlogue systems using entailment. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and\nShort Papers), pages 3806–3812, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nYanai Elazar, Nora Kassner, Shauli Ravfogel, Abhi-\nlasha Ravichander, Eduard Hovy, Hinrich Schütze,\nand Yoav Goldberg. 2021. Measuring and improving\nconsistency in pretrained language models. Transac-\ntions of the Association for Computational Linguis-\ntics, 9:1012–1031.\nOr Honovich, Roee Aharoni, Jonathan Herzig, Hagai\nTaitelbaum, Doron Kukliansy, Vered Cohen, Thomas\nScialom, Idan Szpektor, Avinatan Hassidim, and\nYossi Matias. 2022. TRUE: Re-evaluating factual\nconsistency evaluation. In Proceedings of the Second\nDialDoc Workshop on Document-grounded Dialogue\nand Conversational Question Answering, pages 161–\n175, Dublin, Ireland. Association for Computational\nLinguistics.\nAlexey Ignatiev. 2019. Rc2: an efficient maxsat solver.\nJ. Satisf. Boolean Model. Comput., 11:53–64.\nJaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brah-\nman, Chandra Bhagavatula, Ronan Le Bras, and\nYejin Choi. 2022. Maieutic prompting: Logically\nconsistent reasoning with recursive explanations.\nNora Kassner, Oyvind Tafjord, Hinrich Schütze, and\nPeter Clark. 2021. BeliefBank: Adding memory to a\npre-trained language model for a systematic notion\nof belief. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 8849–8861, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nWonjae Kim, Bokyung Son, and Ildoo Kim. 2021.\nViLT: Vision-and-language transformer without con-\nvolution or region supervision. In ICML, pages 5583–\n5594.\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin John-\nson, Kenji Hata, Joshua Kravitz, Stephanie Chen,\nYannis Kalantidis, Li-Jia Li, David A Shamma,\nMichael Bernstein, and Li Fei-Fei. 2016. Visual\ngenome: Connecting language and vision using\ncrowdsourced dense image annotations. In Inter-\nnational Journal of Computer Vision.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, Kristina Toutanova, Llion Jones, Matthew\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\nral questions: A benchmark for question answering\nresearch. Transactions of the Association for Compu-\ntational Linguistics, 7:452–466.\nPhilippe Laban, Tobias Schnabel, Paul N. Bennett, and\nMarti A. Hearst. 2022. SummaC: Re-visiting NLI-\nbased models for inconsistency detection in summa-\nrization. Transactions of the Association for Compu-\ntational Linguistics, 10:163–177.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Sori-\n1763\ncut. 2019. ALBERT: A lite BERT for self-\nsupervised learning of language representations.\nCoRR, abs/1909.11942.\nTao Li, Vivek Gupta, Maitrey Mehta, and Vivek Sriku-\nmar. 2019. A logic-driven framework for consistency\nof neural models. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 3924–3935, Hong Kong, China. Association\nfor Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nHans-Andrea Loeliger. 2008. An introduction to\nfactor graphs. https://people.binf.ku.dk/\n~thamelry/MLSB08/hal.pdf.\nBill MacCartney and Christopher D. Manning. 2008.\nModeling semantic containment and exclusion in nat-\nural language inference. In Proceedings of the 22nd\nInternational Conference on Computational Linguis-\ntics (Coling 2008), pages 521–528, Manchester, UK.\nColing 2008 Organizing Committee.\nEric Mitchell, Charles Lin, Antoine Bosselut, Chelsea\nFinn, and Christopher D. Manning. 2022. Fast model\nediting at scale. ICLR.\nYasuhide Miura, Yuhao Zhang, Emily Tsai, Curtis Lan-\nglotz, and Dan Jurafsky. 2021. Improving factual\ncompleteness and consistency of image-to-text radi-\nology report generation. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 5288–5304, Online.\nAssociation for Computational Linguistics.\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal,\nJason Weston, and Douwe Kiela. 2020. Adversarial\nNLI: A new benchmark for natural language under-\nstanding. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics. As-\nsociation for Computational Linguistics.\nMarin Vlastelica Poganˇci´c, Anselm Paulus, Vit Musil,\nGeorg Martius, and Michal Rolinek. 2020. Differen-\ntiation of blackbox combinatorial solvers. In Interna-\ntional Conference on Learning Representations.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you don’t know: Unanswerable ques-\ntions for SQuAD. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 2: Short Papers), pages 784–789,\nMelbourne, Australia. Association for Computational\nLinguistics.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nArijit Ray, Karan Sikka, Ajay Divakaran, Stefan Lee,\nand Giedrius Burachas. 2019. Sunny and dark\noutside?! improving answer consistency in VQA\nthrough entailed question generation. In Proceedings\nof the 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 5860–5865, Hong Kong,\nChina. Association for Computational Linguistics.\nVered Shwartz, Peter West, Ronan Le Bras, Chandra\nBhagavatula, and Yejin Choi. 2020. Unsupervised\ncommonsense question answering with self-talk. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 4615–4629, Online. Association for Computa-\ntional Linguistics.\nAnton Sinitsin, Vsevolod Plokhotnyuk, Dmitry Pyrkin,\nSergei Popov, and Artem Babenko. 2020. Editable\nneural networks. In ICLR.\nHaoyu Song, Wei-Nan Zhang, Jingwen Hu, and Ting\nLiu. 2020. Generating persona consistent dialogues\nby exploiting natural language inference. Proceed-\nings of the AAAI Conference on Artificial Intelligence,\n34(05):8878–8885.\nRobyn Speer, Joshua Chin, and Catherine Havasi. 2017.\nConceptnet 5.5: An open multilingual graph of gen-\neral knowledge. In Proceedings of the Thirty-First\nAAAI Conference on Artificial Intelligence, AAAI’17,\npage 4444–4451. AAAI Press.\nOyvind Tafjord and Peter Clark. 2021. General-\npurpose question-answering with macaw. CoRR,\nabs/2109.02593.\nHao Tan and Mohit Bansal. 2019. LXMERT: Learning\ncross-modality encoder representations from trans-\nformers. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n5100–5111, Hong Kong, China. Association for Com-\nputational Linguistics.\nAshwin Vijayakumar, Michael Cogswell, Ramprasaath\nSelvaraju, Qing Sun, Stefan Lee, David Crandall,\nand Dhruv Batra. 2018. Diverse beam search for\nimproved description of complex scenes. Proceed-\nings of the AAAI Conference on Artificial Intelligence,\n32(1).\n1764\nSean Welleck, Jason Weston, Arthur Szlam, and\nKyunghyun Cho. 2019. Dialogue natural language\ninference. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 3731–3741, Florence, Italy. Association for\nComputational Linguistics.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112–1122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nA Reproducing Macaw-Large Examples\nThe following configuration reproduces the Macaw-\nLarge behavior noted in the abstract and the intro-\nduction at https://huggingface.co/allenai/\nmacaw-large.\n$answer$ ; $question$ = Is a sparrow a\nbird? ; $mcoptions$ = (A) Yes. (B) No. ;\n$answer$ ; $question$ = Does a bird have\nfeet? ; $mcoptions$ = (A) Yes. (B) No. ;\n$answer$ ; $question$ = Does a sparrow\nhave feet? ; $mcoptions$ = (A) Yes. (B)\nNo. ;\nB Factor Graph Overview\nA factor graph is a factorization of a function f\nmapping a set of nvariables Z = {zj}n\nj=1 to a non-\nnegative scalar. The factorization is represented\nas a bipartite graph containing variable nodes and\nfactors; each zj is represented by one variable node,\nand each factor ϕi maps a subset of the variable\nnodes Zi to a non-negative scalar. The value of\nthe function is computed as f(Z) =∏\ni(Zi). See\nLoeliger (2008) for a more complete reference.\nC Question-Answer to Statement\nConversion Model Details\nTo convert question-answer pairs into declara-\ntive statements, we combine data from the Ques-\ntion to Declarative Sentence (QA2D) (Demszky\net al., 2018) and BeliefBank (Kassner et al.,\n2021) datasets to fine-tune a T5-base sequence-to-\nsequence model. QA2D contains question-answer\npairs from five QA datasets; 95% of the pairs are\nfrom SQuAD (Rajpurkar et al., 2016). The gold\nstatements are from Amazon Mechanical Turk. The\nBeliefBank questions are created from silver facts\nusing natural language templates as in Section 4.1,\nand the yes/no answers are from the known binary\ntruth values of these facts. Our training dataset is\ncomposed of the full QA2D training dataset of 61k\nquestion-answer pairs and half of the BeliefBank\nsilver facts, for a total of 67k training examples.\nLikewise, the validation dataset consists of the full\nQA2D validation dataset of 10k pairs and half the\nBeliefBank silver facts, for a total of 16k validation\npairs.\nThe input to the QA statement conversion model\nis the concatenation of the question-answer pair\nqi ∥ai. Accuracy is evaluated by comparing the\noutput sequence tokens to the gold sequence tokens.\nTraining occurs with a learning rate of 1e−4 for a\nmaximum of 50 steps, where each step consists\nof 32k training examples, with early stopping if\nvalidation loss does not decrease for 6 consecutive\nsteps. We ran the fine-tuning on NVIDIA GeForce\nRTX 3090 GPU. Fine-tuning ended after 14 steps\nwith a final training accuracy of 0.764 and valida-\ntion accuracy of 0.628. This took approximately\n40 minutes. Table 7 demonstrates the model’s per-\nformance on a few validation examples.\nD Additional Modifications to ConCoRD\nA timeout for solvers is imposed in order to prevent\nthe RC2 MaxSAT solver from running optimization\nindefinitely. The average solve time per question\nwas <4 ms for closed-book QA, <1 ms for VQA\nand <20 ms for NQ (for NQ, the solve time is\n< 1/10th of the time needed for a forward pass\nthrough the QA and NLI models). We found only\none batch of test questions for the closed-book QA\ntask and VQA task where the solver couldn’t find a\nsolution efficiently, so we set a short timeout (30s\nfor CBQA, 10s for VQA, none required for NQ).\nWe also de-duplicate the list of inferred con-\nstraints before passing the statement and constraint\ngroups through the MaxSAT solver so that only the\nhighest-weighted constraints would remain among\ntheir duplicates.\nE Entailment Correction Ablations\nTable 8 shows the effects of entailment correc-\ntion on ConCoRD test performance in closed-book\nquestion answering and VQA experiments for dif-\nferent choices of base model, using the NLI rela-\ntion model resulting in the best test set performance\n(RoBERTa-Large-MNLI).\n1765\nDataset Input Output Gold statement\nSQuAD Who established Yale’s residen-\ntial college system? Edward S.\nHarkness\nEdward S. Harkness established\nYale’s residential college sys-\ntem.\nEdward S. Harkness established\nYale’s residential college sys-\ntem.\nSQuAD How did Kuhn view the his-\ntory of science? competing\nparadigms or conceptual sys-\ntems\nKuhn viewed the history of sci-\nence as a competing paradigm\nor conceptual system.\nKuhn viewed the history of sci-\nence as competing paradigms\nor conceptual systems.\nBeliefBank Is it true that a poodle is a river?\nNo\nA poodle is not a river. A poodle is not a river.\nBeliefBank Is a pigeon a living thing? Yes A pigeon is a living thing. A pigeon is a living thing.\nTable 7: The QA statement conversion model outputs declarative statements from question-answer pairs. Out of the four\nvalidation examples presented, three are correct. The Red, bolded portion of the output of the second example indicates how it\ndiffers from the Teal, bolded corresponding portion of the gold statement.\nF1/Accuracy\nModel Naive w. E.C. w/o. E.C.\nMac-Lg+Rob/ANLI 0.831 0.914 0.909\nMac-3B+Rob/ANLI 0.855 0.931 0.886\nLXMERT+Rob/MNLI 0.656 0.706 0.701\nLXMERT+Rob/ANLI 0.656 0.706 0.693\nViLT+Rob/MNLI 0.784 0.804 0.810\nViLT+Rob/ANLI 0.784 0.814 0.807\nTable 8: Comparison of ConCoRD test performance vs. base-\nline with and without entailment correction (E.C.) across\nbase+relation models for closed-book question answering\n(Macaw) and VQA (LXMERT, ViLT) experiments (F1 for\nclosed-book QA, exact-match accuracy for VQA), showing\nthat the entailment correction improves performance for most\nconfigurations.\nF Additional “Good” and “Bad” Edit\nPairs\nMore examples of good and bad edits in the Edit-\ning experiment are presented in Table 10. We also\ninclude good (Figure 4)and bad flip (Figure 5) ex-\namples from the VQA dataset. For the bad flip ex-\namples in VQA, we include different failure modes\nto demonstrate the types of potential ConCoRD\nerrors.\nG Good and Bad Flips\nFor each set of experiments on the test set, we\nreport the numbers of good and bad flips made by\nConCoRD in Table 9. It can be observed that the\nnumber of good flips is consistently significantly\nhigher than that of bad flips.\nH Hyperparameter Search Details\nH.1 Experiments\nH.1.1 Closed-Book Question Answering\nHyperparameters (Section 3.4) are tuned jointly us-\ning hyperopt on the BeliefBank calibration dataset\nExperiment Model Good Flips Bad Flips\nBeliefBank Macaw-3B 723 277\nVQA LXMERT 576 238\nNQ T5-3B-NQ 168 69\nTable 9: The numbers of good and bad flips in each of the\nexperiments performed. We define flips as choosing a different\ncandidate from the naive baseline for the multiple choice\nexperiments, and a binary truth value flip for BeliefBank.\n“Good” flips are flips that improves performance, and “bad”\nflips are those that are detrimental to performance.\n(Section 4.1). The search space of β is uniform\nbetween [0.05,1.0], and for λ it is uniform be-\ntween [0.5,1.0]. hyperopt optimizes cumulative\nF1 across all entity batches for 300 trials. To speed-\nup tuning, we created caches of model beliefs Bsm\nand relation sets Rsm for each calibration entity\nsm. This was run on NVIDIA GeForce RTX 3090\nGPU, and the largest NLI models took up to two\nhours to complete. Using these caches, hyperopt\ntuning completes in less than an hour on CPU. The\nbest performance on the calibration facts for each\nof the base Macaw models is reported in Table 11.\nThe results show that βis higher for the better base\nmodel Macaw-3B.\nH.1.2 VQA\nHyperparameters are tuned jointly using hyperopt.\nThe search space forβis uniform over[0.05,1], for\nλit is uniform over[1\n3,1]. A total of 100 trials were\nperformed, updating parameters using TPE, on an\nAWS g4dn.xlarge EC2 instance. Each search\ntook less than one hour. Table 12 shows the se-\nlected parameters and their exact-match accuracy\non validation questions.\n1766\nModel Input & Gold Answer Generations Added context\nT5-Sm-NQ Q: Who was the declara-\ntion of independence writ-\nten for?\nA: the Second Continental\nCongress\nSecond Continental\nCongress; the United\nStates; the British Crown;\nGreat Britain\nThe United States Declaration of Inde-\npendence is the statement adopted by\nthe Second Continental Congress meet-\ning at the Pennsylvania State House (\nIndependence Hall ) in Philadelphia on\nJuly 4 , 1776 , which announced that\nthe thirteen American colonies , then at\nwar with the Kingdom of Great Britain ,\nregarded themselves as thirteen indepen-\ndent sovereign states , no longer under\nBritish rule .\nT5-Sm-NQ Q: What is the scientific\nname for the calf muscle\nA: gastrocnemius muscle\nThe serratus calf muscle;\ngastrocnemius muscle ;\nThe serratus calfi; The\nserratus muscle\nAlong with the soleus muscle , the gas-\ntrocnemius forms half of the calf muscle\n.\nT5-3B-NQ Q: Who is the actor that\nplays Dr. Sean Murphy\nA: Freddie Highmore\nFreddie Highmore; Daryl\n“Chill” Mitchell ; Dylan\nChristopher Minnette;\nJavier Muoz\nThe series stars Freddie Highmore as\nShaun Murphy , a young surgical resi-\ndent with autism and savant syndrome\nat San Jose St. Bonaventure Hospital .,\nFreddie Highmore as Shaun Murphy : A\nsurgical resident with autism and savant\nsyndrome .\nT5-3B-NQ Q: Who is the founder of the\nUbuntu project\nA: Mark Richard Shuttle-\nworth\nLinus Torvalds; Mark\nShuttleworth; Richard St.\nJohn Hopper; Richard St.\nJohn Redmond\nMark Richard Shuttleworth ( born 18\nSeptember 1973 ) is a South African en-\ntrepreneur who is the founder and CEO\nof Canonical Ltd. , the company behind\nthe development of the Linux - based\nUbuntu operating system .\nTable 10: Editing a model’s behavior by adding new information to the context. TheUnderlined generation is the answer with the\nhighest QA model confidence. The Bolded generation is what ConCoRD selects after NLI inference. Teal, bolded generations\nindicate that ConCoRD selects a generation with higher token overlap F1, whileRed, bolded generations indicate that ConCoRD\nselects a worse generation.\nFigure 4: “Good” flip examples from the VQA experiments.\nThe green texts mark the correctly selected answers, while the\nred texts indicate incorrectly selected answers.\nFigure 5: “Bad” flip examples from the VQA experiments.\nThe green texts mark the correctly selected answers, while\nthe red texts indicate the incorrectly selected answers. The\nbolded texts are the correct answers, if generated within the\ntop-2 predictions. From top to bottom, the first image is\nan example of when the correct answer, \"sheet,\" was not\ncontained in the predicted answers. The second image is\nan example of when the conversion of QA pair to statement\ndid not occur as intended and the NLI failed to generate the\nappropriate inferences that could be used to inform correction\nof \"background\" to \"buildings. The third image shows an\nexample of when an \"incorrect\" answer (sky) is effectively\nthe same as the \"correct\" answer (in sky)–only semantically\ndifferent. The fourth image shows an example of when the\nmodel strongly believed in an incorrect answer and changed\nanother correct answer.\n1767\nModel F1 β λ E.C.\nMacaw-Large 0.919 0.753 0.855 True\nMacaw-3B 0.94 0.804 0.873 True\nTable 11: Validation performance on the BeliefBank\ncalibration facts. Both models achieve best validation\nperformance with the RoBERTa-Large ANLI model.\nVQA Acc. β λ E.C.\nLXMERT 0.691 0.208 0.805 True\nViLT 0.787 0.395 0.772 True\nTable 12: Validation performance on VQA. Both models\nachieve best validation performance with the RoBERTa-\nLarge MNLI model.\nH.1.3 Information Injection with Natural\nQuestions\nFor this round of experiments, we lower the bounds\nfor β and λafter some initial trials. The bounds\nof β are [0,0.5] and the bounds of λ are [0,0.6].\nWe run hyperopt for 200 trials (often taking ap-\nproximately 2 to 3 hours on an NVIDIA GeForce\nRTX 3090 GPU) for each of the three NLI mod-\nels. Hyperopt optimizes for the highest token-\noverlapping F1 score in this experiment.\nWe report the best validation performance of\neach of the QA base models in Table 13.\nModel F1 β λ E.C.\nT5-Small 0.227 0.112 0.540 True\nT5-Large 0.331 0.081 0.413 False\nT5-3B 0.353 0.072 0.477 True\nTable 13: Validation performance on NQ. All models\nachieve best validation performance with the ALBERT\nANLI model.\nH.2 Visualizing Hyperparameter Search\nFigure 6 shows increases in exact-match accuracy\nas they vary with choices of λ, β, for additional\nchoices of base model for a VQA task, with and\nwithout entailment correction, complementing fig-\nure 3. Interestingly, choosing a different base\nmodel does noticeably effect the optimum value\nof β; between figures 6b and 6c we see the near-\noptimal region shift towards a value of βthat gives\nhigher confidence in the base model where the base\nmodel produces “better” answers. However, the\nincrease in accuracy is similar, suggesting that with\nappropriate selection ofβ, ConCoRD can offer sim-\nilar improvements over a range of choices of base\nmodel.\n(a) Base model LXMERT, with entailment correction.\n(b) Base model LXMERT, without entailment correction.\n(c) Base model ViLT, without entailment correction.\nFigure 6: As in figure 3, we show changes in exact-match\nvalidation accuracy as a function of confidence threshold λ\nand tradeoff parameter β, with several choices of base model,\nwith and without an entailment correction, holding relation\nmodel RoBERTa-Large ANLI constant.\n1768"
}