{
    "title": "Multi-Scale Control Signal-Aware Transformer for Motion Synthesis without Phase",
    "url": "https://openalex.org/W4382239703",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5101938136",
            "name": "Lintao Wang",
            "affiliations": [
                "The University of Sydney"
            ]
        },
        {
            "id": "https://openalex.org/A5100668926",
            "name": "Kun Hu",
            "affiliations": [
                "The University of Sydney"
            ]
        },
        {
            "id": "https://openalex.org/A5028486493",
            "name": "Lei Bai",
            "affiliations": [
                "Shanghai Artificial Intelligence Laboratory",
                "ShangHai JiAi Genetics & IVF Institute"
            ]
        },
        {
            "id": "https://openalex.org/A5066523990",
            "name": "Yu Ding",
            "affiliations": [
                "NetEase (China)"
            ]
        },
        {
            "id": "https://openalex.org/A5087818121",
            "name": "Wanli Ouyang",
            "affiliations": [
                "The University of Sydney"
            ]
        },
        {
            "id": "https://openalex.org/A5100443935",
            "name": "Zhiyong Wang",
            "affiliations": [
                "The University of Sydney"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3036644940",
        "https://openalex.org/W6690811371",
        "https://openalex.org/W6793119350",
        "https://openalex.org/W3188493377",
        "https://openalex.org/W3193787135",
        "https://openalex.org/W6637868692",
        "https://openalex.org/W3144253442",
        "https://openalex.org/W2802441648",
        "https://openalex.org/W6929206830",
        "https://openalex.org/W6781354665",
        "https://openalex.org/W2611706523",
        "https://openalex.org/W4221045999",
        "https://openalex.org/W6702964441",
        "https://openalex.org/W2249800031",
        "https://openalex.org/W2902757548",
        "https://openalex.org/W6646467855",
        "https://openalex.org/W3011767095",
        "https://openalex.org/W6781117515",
        "https://openalex.org/W3137558685",
        "https://openalex.org/W3014289294",
        "https://openalex.org/W3045246204",
        "https://openalex.org/W3200798313",
        "https://openalex.org/W4221160193",
        "https://openalex.org/W4200301490",
        "https://openalex.org/W6891797237",
        "https://openalex.org/W3153832461",
        "https://openalex.org/W3049455300",
        "https://openalex.org/W2986708244",
        "https://openalex.org/W3048539707",
        "https://openalex.org/W3184783479",
        "https://openalex.org/W3212369858",
        "https://openalex.org/W2016589492",
        "https://openalex.org/W2801567109",
        "https://openalex.org/W3136525061",
        "https://openalex.org/W3139250374",
        "https://openalex.org/W3035545045",
        "https://openalex.org/W1735317348",
        "https://openalex.org/W4248876013",
        "https://openalex.org/W4295312788",
        "https://openalex.org/W3048550213",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W3148944813",
        "https://openalex.org/W3104515094",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4242381089",
        "https://openalex.org/W4382239703",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W4287265181",
        "https://openalex.org/W2243558602",
        "https://openalex.org/W3048665613",
        "https://openalex.org/W2963285578",
        "https://openalex.org/W4256451904",
        "https://openalex.org/W3177493543",
        "https://openalex.org/W4230178929",
        "https://openalex.org/W3035225512",
        "https://openalex.org/W3109717189",
        "https://openalex.org/W4214612132",
        "https://openalex.org/W2963389355",
        "https://openalex.org/W2945629925",
        "https://openalex.org/W3207857704",
        "https://openalex.org/W2896457183"
    ],
    "abstract": "Synthesizing controllable motion for a character using deep learning has been a promising approach due to its potential to learn a compact model without laborious feature engineering. To produce dynamic motion from weak control signals such as desired paths, existing methods often require auxiliary information such as phases for alleviating motion ambiguity, which limits their generalisation capability. As past poses often contain useful auxiliary hints, in this paper, we propose a task-agnostic deep learning method, namely Multi-scale Control Signal-aware Transformer (MCS-T), with an attention based encoder-decoder architecture to discover the auxiliary information implicitly for synthesizing controllable motion without explicitly requiring auxiliary information such as phase. Specifically, an encoder is devised to adaptively formulate the motion patterns of a character's past poses with multi-scale skeletons, and a decoder driven by control signals to further synthesize and predict the character's state by paying context-specialised attention to the encoded past motion patterns. As a result, it helps alleviate the issues of low responsiveness and slow transition which often happen in conventional methods not using auxiliary information. Both qualitative and quantitative experimental results on an existing biped locomotion dataset, which involves diverse types of motion transitions, demonstrate the effectiveness of our method. In particular, MCS-T is able to successfully generate motions comparable to those generated by the methods using auxiliary information.",
    "full_text": "Multi-Scale Control Signal-Aware Transformer for\nMotion Synthesis without Phase\nLintao Wang1, Kun Hu1,*, Lei Bai2, Yu Ding3, Wanli Ouyang1, Zhiyong Wang1\n1School of Computer Science, The University of Sydney, Australia\n2Shanghai AI Laboratory, China\n3Netease Fuxi AI Lab, China\nlwan3720@uni.sydney.edu.au, kun.hu@sydney.edu.au, baisanshi@gmail.com, dingyu01@corp.netease.com,\nwanli.ouyang@sydney.edu.au, zhiyong.wang@sydney.edu.au\nAbstract\nSynthesizing controllable motion for a character using deep\nlearning has been a promising approach due to its potential to\nlearn a compact model without laborious feature engineering.\nTo produce dynamic motion from weak control signals such\nas desired paths, existing methods often require auxiliary in-\nformation such as phases for alleviating motion ambiguity,\nwhich limits their generalisation capability. As past poses of-\nten contain useful auxiliary hints, in this paper, we propose\na task-agnostic deep learning method, namely Multi-scale\nControl Signal-aware Transformer (MCS-T), with an atten-\ntion based encoder-decoder architecture to discover the auxil-\niary information implicitly for synthesizing controllable mo-\ntion without explicitly requiring auxiliary information such as\nphase. Specifically, an encoder is devised to adaptively for-\nmulate the motion patterns of a character’s past poses with\nmulti-scale skeletons, and a decoder driven by control sig-\nnals to further synthesize and predict the character’s state\nby paying context-specialised attention to the encoded past\nmotion patterns. As a result, it helps alleviate the issues of\nlow responsiveness and slow transition which often happen in\nconventional methods not using auxiliary information. Both\nqualitative and quantitative experimental results on an exist-\ning biped locomotion dataset, which involves diverse types\nof motion transitions, demonstrate the effectiveness of our\nmethod. In particular, MCS-T is able to successfully gener-\nate motions comparable to those generated by the methods\nusing auxiliary information.\nIntroduction\nInteractively controlling a character has been increasingly\ndemanded by various applications such as gaming, vir-\ntual reality and robotics. This task remains challenging to\nachieve realistic and natural poses with complex motions\nand environments, even with large amount of high quality\nmotion capture (MoCap) data for modelling (Holden, Ko-\nmura, and Saito 2017; Peng et al. 2018). Recently, deep\nlearning techniques have been studied for controllable mo-\ntion synthesis given their strong learning capability yet ef-\nficient parallel structures for fast runtime. Many encourag-\ning results have been achieved using deep architectures such\n*Corresponding author.\nCopyright © 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: Illustration of the motion manifold of a running\nmotion sequence. Joint positions and velocities of individual\nposes are projected into a 2D space by t-SNE and colored in\nline with their phases. It is noticed that auxiliary phases are\ncontinuously distributed on the manifold, which suggests the\npotential of inferring the phases from motion attributes.\nas multilayer perceptron (MLP) networks (Holden, Komura,\nand Saito 2017), recurrent neural networks (Lee, Lee, and\nLee 2018), generative networks (Henter, Alexanderson, and\nBeskow 2020) and deep reinforcement learning architec-\ntures (Peng et al. 2018). Particularly, due to the potentials of\ndelivering fast, responsive yet high-quality controllers, MLP\nnetworks have been devised for biped locomotion (Holden,\nKomura, and Saito 2017), quadruped locomotion (Zhang\net al. 2018), daily interaction (Starke et al. 2019), basketball\nplay (Starke et al. 2020) and stylised motion prediction (Ma-\nson, Starke, and Komura 2022). Since a weak control signal,\nwhich is commonly used in graphics, often corresponds to a\nlarge variation of possible motions, these studies have to rely\non auxiliary phase variables in line with the character’s con-\ntact states for disambiguation purposes. However, the con-\ntact states may not be available for all kinds of motions and\nmay require manual correction during data acquisition. By\ncontrast, recurrent neural networks, e.g. (Lee, Lee, and Lee\n2018), aim to constrain the next pose prediction subject to\nthe past motions, which can be task-agnostic in terms of mo-\ntion category and demonstrate better generalisation capabil-\nity. The key limitation of RNN based methods is that they\noften suffer from slow responsiveness issues due to the large\nvariation of the hidden memory (Starke et al. 2019).\nWe believe that auxiliary information can be inferred from\na character’s past motions. As shown in Figure 1, a walking\nmotion sequence is represented in 2D manifolds by using\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n6092\ndifferent attributes (e.g. joint positions and velocities). It can\nbe observed that the phases are continuously distributed on\nthe manifolds, which enables the auxiliary information in-\nference from the motion related attributes. Nonetheless, the\npast poses should be used “attentionally” since not all of\nthem are always informative especially during motion tran-\nsition, which is the reason that RNN based methods perform\npoorly without explicit data augmentation to the transitional\ncases in motion capture (MoCap) data. Therefore, in this\nwork, we aims to study a deep learning based task-agnostic\nmethod to produce dynamic motion from trajectory-based\ncontrol signals, without explicitly using additional auxiliary\ninformation such as phase.\nSpecifically, we propose a transformer-based encoder-\ndecoder architecture, namely Multi-Scale Control Signal-\naware Transformer (MCS-T), to attend to the motion infor-\nmation of past poses and trajectory with respect to various\nscenarios. An encoder formulates the past motion patterns of\na character from multi-scale skeletons in pursuit of learning\nspatio-temporal patterns from different levels of dynamics.\nWith the past motion information, the encoder is expected\nto formulate conventional auxiliaries implicitly. Then, a de-\ncoder guided by the control signals synthesizes and predicts\nthe next character pose by paying trajectory-specialised at-\ntention to the encoded historical motion patterns, rather than\nusing a long inflexible memory setting. This dynamic mo-\ntion modelling pipeline helps alleviate the issues of low re-\nsponsiveness and slow transition, which can be observed in\nexisting methods not using auxiliary information. Compre-\nhensive experiments on a biped locomotion dataset contain-\ning various motion transitions (e.g., sudden jumping and un-\neven terrain walking) demonstrate the effectiveness of MCS-\nT. It produces responsive and dynamic motion, and achieves\na performance comparable to that of the methods explicitly\nusing auxiliary information while retaining such capability\nfor various motion categories.\nThe main contributions of this paper can be summarised\nas follows:\n• A novel real-time motion controller, namely Multi-Scale\nControl Signal-aware Transformer, is proposed to im-\nprove the responsiveness and motion dynamics over ex-\nisting methods not explicitly using auxiliary informa-\ntion. It is also task-agnostic, compared with the meth-\nods explicitly using auxiliary information. To the best of\nour knowledge, our task-agnostic method is one of the\nfirst studies utilising transformer based encoder-decoder\nscheme for controllable motion synthesis.\n• A multi-scale graph modelling scheme is devised to ex-\nploit rich skeleton dynamics.\n• A novel control signal-aware self-attention is devised to\ninject control signals for motion prediction.\n• Comprehensive experiments were conducted to demon-\nstrate the effectiveness of our proposed MCS-T.\nRelated Work\nIn this section, we review related studies in terms of\nkinematic-based controllable motion synthesis, transformer\nbased motion learning and multi-scale skeleton.\nKinematics Based Controllable Motion Synthesis\nThe kinematics based methods focus on the motion of char-\nacter bodies including the joints without considering the\nphysics that cause them to move. Four major categories of\nmethods are reviewed as follows.\nSearch-Based Methods: Early studies were based on\ngraphs (Arikan and Forsyth 2002; Lee et al. 2002; Kovar,\nGleicher, and Pighin 2008; Lee et al. 2010), where each\nframe of a motion database was treated as a vertex and\nedges represented possible transitions between two frames.\nA graph search can find a path to produce an expected mo-\ntion. Motion matching (Clavet 2016; Holden et al. 2020)\nsimplified the graph search by finding transitional frames\ndirectly in animation databases and produced the state-of-\nart gaming animation (Buttner 2019; Zinno 2019). However,\nthe matching criterion are often required to be devised by ex-\nperienced animators for a wide range of motion scenarios.\nRecurrent Neural Network Based Methods: Fragkiadaki\net al. (2015) constructed an encoder-decoder structure based\non recurrent neural network (RNN) and directly adopted\n3D body joint angles to predict character poses. Li et al.\n(2017) addressed the error accumulation issue by introduc-\ning a teacher forcing-like mechanism (Williams and Zipser\n1989). Lee, Lee, and Lee (2018) incorporated control signals\ninto RNNs. To alleviate the low responsiveness and slow\nmotion transition issues caused by the inflexible RNN mem-\nory state, comprehensive data augmentation was conducted\nto enrich transitional patterns. However, less motion diver-\nsity was observed during the runtime, since the augmented\nknowledge was still limited.\nPhase-Based Methods: Phase-functioned neural network\n(Holden, Komura, and Saito 2017) adopted a multilayer per-\nceptron (MLP) to predict biped locomotion with an auxil-\niary foot contact phase, which clusters motion with similar\ntiming to disambiguate motion predictions. The phase-based\nframeworks were further extended to quadruped locomotion\n(Zhang et al. 2018), environmental interaction (Starke et al.\n2019), basketball game (Starke et al. 2020) and martial arts\n(Starke et al. 2021). Nevertheless, the acquisition of phase\ninformation relied on the expertise of animators and the con-\ntact information of characters, which may not be univer-\nsally available. Mason, Starke, and Komura (2022) proposed\na heuristic principal component analysis based strategy to\ncompute the phase of a stylised motion, where the arms often\nexhibited special movements without contact states. How-\never, it was still a task-specific solution.\nGenerative Methods: Instead of predicting a single mo-\ntion pose, modelling the conditional pose distribution and\nconducting sampling could avoid the averaging pose from\nvastly different poses (Habibie et al. 2017; Ling et al. 2020;\nHenter, Alexanderson, and Beskow 2020; Liu et al. 2021; Li\net al. 2022; Kania, Kowalski, and Trzci´nski 2021). Ling et al.\n(2020) used a variational autoencoder (V AE) to estimate\nthe next pose distribution and draw user control-conditioned\nsamples through reinforcement learning. Normalising flow\nwas also introduced for this purpose, which modelled mo-\ntion distribution and control signal together (Henter, Alexan-\nderson, and Beskow 2020). Although the generative ap-\nproach did not require auxiliary information, it heavily de-\n6093\nFigure 2: Illustration of our proposed MCS-T method, which is based on an encoder-decoder architecture to formulate the past\nmotion patterns with multi-scale skeleton representations and predict the next motion with the guidance of the control signals.\npended on balanced MoCap data distributions (Ling et al.\n2020), not designed for trajectory-based control signal (Liu\net al. 2021) or less controlled on produced motion gait (Hen-\nter, Alexanderson, and Beskow 2020).\nTransformer in Motion Learning\nTransformers (Vaswani et al. 2017) have achieved great suc-\ncess in a wide range of tasks such as natural language pro-\ncessing (Devlin et al. 2018) and computer vision (Dosovit-\nskiy et al. 2020; Arnab et al. 2021). Compared with tradi-\ntional recurrent neural networks, self-attention mechanisms\nperform more effectively and efficiently to address sequen-\ntial patterns. Therefore, various transformer based methods\nwere proposed for many motion related tasks such as mo-\ntion prediction (Mao, Liu, and Salzmann 2020; Mart ´ınez-\nGonz´alez, Villamizar, and Odobez 2021; Aksan et al. 2021;\nWang et al. 2021), action recognition (Plizzari, Cannici, and\nMatteucci 2021; Mazzia et al. 2022), 3D pose estimation\n(Zheng et al. 2021) and motion synthesis (Petrovich, Black,\nand Varol 2021). However, there are few studies utilising\ntransformer for controllable motion synthesis.\nMulti-Scale Skeleton\nTo better explore rich spatial skeleton representations of hu-\nman poses, many studies introduced multi-scale skeletons\nby using higher order polynomials of adjacency matrices\n(Liu et al. 2020), graph convolutions (Jang, Park, and Lee\n2022) or heuristic rules (Li et al. 2020; Dang et al. 2021;\nGhosh et al. 2021; Bhattacharya et al. 2021). We address the\nmulti-scale graphs with transformers to provide multi-scale\ntokens with trajectories, which is the first attempt in control-\nlable motion synthesis for more responsive motions.\nMethodology\nFigure 2 illustrates the proposed MCS-T architecture, which\naddresses the motion control problem as a regression task.\nThe motion data is first parameterised as pose and trajec-\ntory embeddings. The pose embedding is formulated by\nmulti-scale skeleton graphs for comprehensively exploiting\nthe past spatio-temporal relations. It encodes the motion se-\nquence for each skeleton scale by specialised transformer\nencoders for latent motion representation. The representa-\ntion is then utilised by a transformer decoder queried by\ntrajectory information, i.e., control signal, for a control-\nconditioned integration with past motion states. Finally, a\nmotion prediction network predicts the character’s next pose\nand potential future trajectory.\nMulti-Scale Skeleton Poses\nA virtual character is animated upon a skeleton of rotational\njoints, of which the coordinates and velocities can be de-\nfined regarding the motion. Each pose skeleton in a motion\nsequence can be viewed as a graph, where the joints are\nvertices and the bones are edges. Based on such graph rep-\nresentation, multi-scale skeletons can be constructed for a\npose by aggregating the adjacent vertices as a pooled coarse-\nlevel vertex. As illustrated in Figure 2, two scales of skele-\ntons in a fine-to-coarse scheme are adopted in this study.\nThis scheme aims to comprehensively characterise the spa-\ntial patterns, by which the additional coarse-level represen-\ntation enables global observations of motions and improves\nmotion dynamics especially during a motion transition.\nThe fine-level representation is the same as the original\nskeleton structure obtained from MoCap, which contains 31\njoints. Specifically, we denotejp\ni and jv\ni as the vectors repre-\nsenting the coordinates local to the corresponding root trans-\nformation and velocity values of the fine-level vertices (i.e.\njoints) of the i-th frame, respectively. For the coarse-level\nrepresentation, we denote bp\ni and bv\ni as the vectors represent-\ning the coordinates and velocity values of the vertices (i.e.,\naggregated joints) of the i-th frame, respectively.\nParticularly, for the motion prediction of the i-th frame,\nwe construct an input Xi, which consists of two compo-\nnents regarding the past pose information of the two skeleton\n6094\nscales: Ji and Bi. In detail, we denote Ji = {(jp\ni−k, jv\ni−k)},\nBi = {(bp\ni−k, bv\ni−k)}, k = k1, ..., kK, as the fine and coarse\nsequences, respectively. In total, K frames are adopted in-\nstead of using all frames for the consideration of both effi-\nciency purpose and model complexity.\nMulti-Scale Motion Encoder\nA motion encoder aims to formulate the past motion patterns\nXi as a reference for predicting the future motion. Com-\npared with the methods using auxiliary information, our en-\ncoder only depends on the past motion information available\nand can be generalized to all kinds of actions. However, triv-\nially using all past motion information could result in issues\nof low responsiveness and slow motion transition. In other\nwords, using all past information can introduce redundancy\nfor predicting the next pose and sometimes even disturb the\nprediction, which may fall into the historical motion states.\nThus, a transformer-based multi-scale encoder is proposed\nto formulate the past motion patterns in an adaptive manner.\nThe fine and coarse-level pose information Ji and Bi can\nbe treated as matrices, where each row represents the po-\nsition and velocity of a particular temporal motion frame.\nOur multi-scale encoder is based on self-attentions (Vaswani\net al. 2017) using the concepts of query, value and key, which\ncan be formulated as:\nQJ\ni = JiWQ,J, KJ\ni = JiWK,J , VJ\ni = JiWV,J ,\nQB\ni = BiWQ,B, KB\ni = BiWK,B, VB\ni = BiWV,B ,\n(1)\nwhere W·,· are projection matrices containing trainable\nweights with an output dimension γ, J related matrices for-\nmulate the fine-level pose patterns and B related matrices\nformulate the coarse-level pose patterns. Then, the temporal\npatterns can be computed for each level as follows with a\nsoftmax function σ:\nZJ\ni = σ(QJ\ni KJ\ni\n⊺\n√γ )V J\ni , ZB\ni = σ(QB\ni KB\ni\n⊺\n√γ )V B\ni . (2)\nTo this end, the temporal relations of motion frames can\nbe formulated by observing the entire sequence based on\nthe weights obtained using the softmax function in Eq.\n(2). In practice, multiple independent self-attentions can be\nadopted to increase the capability of modelling and feed-\nforward components are followed, which is known as a\ntransformer encoder layer. By stacking multiple transformer\nencoder layers for each observation level, the final spatio-\ntemporal patterns can be obtained. For the convenience of\nnotations, we still use the symbolsZJ\ni and ZB\ni to indicate the\nencoded sequential representations. By concatenating ZJ\ni\nand ZB\ni in a frame-wise manner, a sequence {Zi} can be\nobtained as the encoded multi-scale past motion patterns.\nControl Signal & Trajectory\nThe trajectory of a character’s movement is based on the\nuser’s control signals. We denote a trajectory vector:\nTi = (tp\ni,s−S, ..., tp\ni,s, ..., tp\ni,S−1, td\ni,s−S, ..., td\ni,s, ..., td\ni,S−1,\nth\ni,s−S, ..., th\ni,s, ..., th\ni,S−1, tg\ni,s−S, ..., tg\ni,s, ..., tg\ni,S−1),\n(3)\nwhich represents the sampled discrete trajectory patterns for\nthe prediction of the frame i. Particularly, the indices of the\nsampled points are specified as s ∈ {s−S, ..., s0, ..., sS−1}.\nIn this study, we empirically adopt S = 6 and the sam-\npled points are evenly distributed around the current frame\nto cover the trajectories 1 second before and 1 second after.\nIn detail, the trajectory includes four aspects:\n• tp\ni,s represents the sampled s-th trajectory position in the\n2D horizontal plane of the i-th frame.\n• td\ni,s indicates the trajectory direction in the 2D horizontal\nplane, which is the facing direction of the character.\n• th\ni,s is a sub-vector contains the trajectory height in line\nwith the terrain to characterise the geometry information,\nwhich are obtained from three locations regarding the\nsampled point including the center, left and right offset.\n• tg\ni,s is a one-hot encoding sub-vector regarding the action\ncategory for the sampled trajectory point. For our loco-\nmotion settings, we have five action categories including\nstanding, walking, jogging, jumping and crouching.\nControl Signal-Aware Decoder\nBased on the past motion embeddings from the multi-scale\nmotion encoder, a control signal-aware decoder is proposed\nto formulate a latent embedding for motion prediction. The\ntrajectory information is involved by the decoder to attend to\nthe past encoded motion patterns through a control signal-\naware attention mechanism. This allows the decoded pat-\nterns being relevant to the user’s control signals. In detail,\nwe adopt the trajectory Ti as a query to the past motions:\nqD\ni = TiWQ,D, KD\ni = ZiWK,D, VD\ni = ZiWV,D, (4)\nwhere W·,· are projection matrices containing trainable\nweights with an output dimension γ. Hereafter, the past mo-\ntion information with user control can be summarised into a\nvector as follows:\nzD\ni = σ(qD\ni KD\ni\n⊺\n√γ )V D\ni . (5)\nParticularly, we call the attention in Eq. (4-5) as a con-\ntrol signal-aware attention and multi-heads of it are adopted\nwith feed-forward networks to characterise the motions from\nmultiple aspects. For the simplification of notations, we con-\ntinue to use zD\ni to denote this multi-head output.\nMotion Prediction Network\nTo predict and synthesize the motion of the i-th frame,\nwhich we denote as Yi, an additional motion prediction\nnetwork (MPN) component is introduced. Yi contains pose\n{(jp\ni , jv\ni , jr\ni )}, trajectory Ti+1 and contact information Ci.\nParticularly, jr\ni represents local joint rotation additional to\nposition and velocity. The prediction ˆTi+1 of Ti+1 is only\nfor the trajectory after thei-th frame, where the sampled tra-\njectory points before the current frame already exist. Ci is a\nvector, which indicates the labels of foot contact for each\nheel and toe joint of the two feet. It can be used to per-\nform Inverse Kinematics (IK) post-processing to better fit\nthe character with terrain geometry.\n6095\nOur MPN is based on feed-forward layers with Expo-\nnential Linear Unit (ELU) activation function (Clevert, Un-\nterthiner, and Hochreiter 2015). In detail, we have an esti-\nmation ˆYi of Yi:\nˆYi = MPN(zD\ni , Ti), (6)\nwhere the decoded output and motion trajectory are consid-\nered as the input. Note that the trajectory information is also\nused for MPN besides the decoder, which helps the control\nsignals to be fully formulated for providing highly respon-\nsive motion synthesis.\nMCS-T Training and Runtime Inference\nBy defining the computations of the proposed MCS-T as\na function F with trainable parameters Θ, where ˆYi =\nF(Xi, Ti). A mean squre error (MSE) loss with ℓ1 regu-\nlarization is adopted to optimize Θ. In detail, we solve the\nfollowing optimization problem during the training:\narg min\nΘ\n∥ Yi − F(Xi, Ti; Θ) ∥2\n2 +λ|Θ|, (7)\nwhere λ is a hyper-parameter controlling the scale of the\nregularization.\nIn terms of the runtime inference, a trajectory blending\nscheme is adopted for post-processing. In detail, the trajec-\ntory positions ˆtp\ni+1,s and directions ˆtd\ni+1,s, s = s0, ..., sS−1,\nafter the i-th frame are further blended with the user control\nsignal for the (i + 1)-th frame’s motion prediction:\ntp\ni+1,s = (1 − τp\ns )¯tp\ni+1,s + τd\ns ˆtp\ni+1,s,\ntd\ni+1,s = (1 − τd\ns )¯td\ni+1,s + τd\ns ˆtd\ni+1,s,\n(8)\nwhere ¯ti+1,s is the trajectory computed by the user’s con-\ntrol signal, τp\ns and τd\ns are hyper-parameters to control the\nblending level. That is, the user control signal is blended\nwith higher weights in near trajectory for more responsive\nmotion, and with lower weights in far trajectory in pur-\nsuit of smoother transition. In terms of tp\ni+1,s and td\ni+1,s,\ns = s−S, ..., s−1, they are in line with the actual existing\ntrajectory. Additionally, the trajectory height th\ni+1,s can be\nderived based on tp\ni+1,s within the virtual scene, and the ac-\ntion category tg\ni+1,s is set directly by the user.\nExperimental Results and Discussions\nDataset\nWe evaluate our proposed method on a public dataset\n(Holden, Komura, and Saito 2017) for a fair comparison\nwith the state-of-the-art methods. The dataset consists of\nbiped locomotion data of various gaits, terrains, facing di-\nrections and speeds, which helps evaluate the quality of the\ncommon character motion controller in terms of responsive-\nness and motion transition. A biped character with 31 joints\nand MoCap techniques were adopted to collect these data.\nIn total, we obtained around 4 million samples for training.\nImplementation Details\nIn total, K = 5 past frames with indices k1 = 1, k2 = 10,\nk3 = 20, k4 = 30 and k5 = 40 were selected as input to pre-\ndict the motion of the i-th frame. Note that this setting was\nfound to provide the best quality of prediction. Two indepen-\ndent transformer-encoders were used for the fine-level and\ncoarse-level motion sequences, respectively. Each of them\nconsisted of three transformer-encoder layers using six self-\nattention heads of a dimension 186 and the the feed-forward\nlayers were of a dimension 1024. A dropout rate of 0.1 was\napplied to the encoders. The transformer decoder was using\nthe same configurations as the encoder. The motion predic-\ntion network was modelled as a three-layer MLP with a hid-\nden dimension 512 and a dropout rate 0.3. τp\ns = ( s/S)0.5\nand τd\ns = (s/S)2 were defined for Eq(8) empirically (Wang\net al. 2023).\nDuring the training, the input and output were firstly nor-\nmalised by their mean and standard deviation. Additionally,\nthe input features related to the joints were all scaled by0.1,\nwhich helped produce dynamic motions in certain scenarios\nto enlarge the proportion of the trajectory related inputs. In\nterms of the loss function, λ for ℓ1 regularization was set to\n0.01. The model was implemented by PyTorch 1.7.1 (Paszke\net al. 2019) and trained with an Adam optimisier (Kingma\nand Ba 2014). The learning rate was set to 10−4 and the\nbatch size was 32. The model was trained with 20 epochs,\nwhich took ∼ 50 hours on an NVIDIA GTX 1080Ti GPU.\nComparisons with State-of-the-Art Methods\nQualitative and quantitative evaluations of MCS-T were\nconducted against a number of baseline methods, in terms\nof motion quality, especially from the aspects of responsive-\nness and motion transition. The baseline methods include\nMLP with a single past pose, MLP with multiple past poses,\nRNN (Lee, Lee, and Lee 2018) and PFNN (Holden, Ko-\nmura, and Saito 2017) methods. Overall, we show that our\nMCS-T was able to produce motions in line with the-state-\nof-the-art results with a task-agnostic design and to alleviate\nthe fundamental issues of the baseline methods. More results\nare available in the supplementary material.\nMLP with Single Past Pose: We trained an MLP to synthe-\nsize motion using a single past pose with trajectory informa-\ntion. The experimental results show that the overall motion\nproduced was quite stiff especially when changing the direc-\ntion and could have weird artifacts such as floating as shown\nin the ceiling scenario in Figure 3. As expected, motion pre-\ndiction from vague control signal can be difficult without\nauxiliary information and various possible predictions can\nexist, which leads to an average pose.\nMLP with Multiple Past Poses: Similar to the first base-\nline, except that additional pose information from multiple\npast frames was considered for an MLP. The results show\nthat the generated motion was improved, as the past frames\nprovided the auxiliary information implicitly. Nonetheless,\nthe synthesized motion suffers from the slow motion transi-\ntion issue. This problem became obvious when the character\nwas traversing through rocky terrain as shown in Figure 3.\nWhile it was able to adapt the character motion correctly to\n6096\nFigure 3: Qualitative results of our MCS-T and other baselines under four scenarios: flat, rocky, obstacles and ceiling. The left\nside shows the motions synthesized by MCS-T and the right side provides examples that demonstrate the baseline limitations.\nthe new geometry, the motion was performed as smooth as\nthe regular locomotion on a flat terrain. The reason could be\nthat using several past poses in a simple manner is limited to\nthe large redundant variations in the past.\nRNN: An LSTM architecture (Lee, Lee, and Lee 2018)\nwas adopted for this dataset without augmentation. The past\nmemory enables LSTM to predict motion of higher quality.\nNevertheless, it still suffered from the slow motion transition\nissue. As shown in Figure 3, the character could be float-\ning when transiting between motion and was unable to jump\nover the obstacles timely and obviously. The reason is that\nthe hidden memory prevented the RNN model from quickly\nreaching transitional states of a jumping motion.\nPhase-Functioned Neural Network : Rather than rely-\ning on past poses to constrain motion prediction, PFNN\n(Holden, Komura, and Saito 2017) utilised the foot contact\nphase for motion disambiguation. The qualitative results of\nour MCS-T were very closely to those of PFNN in a wide\nrange of scenarios. Our method does not require the task-\nspecific auxiliary information, which only relies on the past\nmotion data generally available.\nMoreover, to quantitatively evaluate whether the pro-\nduced motion is responsive to control signals and transits\nto different motions timely, the average joint angle update\nper second as a metrics for motion dynamics was compared.\nA higher joint angle update represents more dynamic mo-\ntion produced and faster transition between frames. The re-\nsults are listed in Table 1, which indicate that MCS-T is able\nto produce much more agile motion than the task-agnostic\nRNN, while being comparable to the task-specific PFNN\n(Holden, Komura, and Saito 2017) method.\nAblation Study\nAn ablation study was conducted to demonstrate the effec-\ntiveness of the multi-scale skeleton representation and the\ncontrol signal-aware mechanism in our encoder and decoder,\nrespectively. The quantitative evaluation is listed in Table 1.\nMulti-Scale Skeletons with an Extra Middle Scale: In ad-\ndition to the two skeleton scales, we experimented with one\nextra scale called as a middle scale. It aggregated the joints\ninto a level between the two existing levels. However, the\nthree-scale scheme did not contribute to the overall perfor-\nmance and produced stiff motions especially under scenar-\nios with quick and frequent transitions such as obstacles\nand ceiling scene. The potential reason could be that the in-\ncreased model complexity deteriorates the capability of mo-\ntion prediction and produces sub-optimal solution.\nMulti-Scale Skeletons: Without multi-scale skeletons, the\nmotion dynamics dropped significantly, especially in the ob-\nstacles scene. Jumping motion became less responsive and\nsometimes the dynamics were too weak to observe. Thus,\nincorporating coarse-level skeletons helped exploit the mo-\ntion patterns during a transition from a global perspective.\nControl Signal-Aware Decoder: Besides the motion pre-\ndiction network, our decoder is driven by the control sig-\nnals as well, which are adopted as the queries of the decod-\ning attentions. Alternatively, by simply using a conventional\nself-attention mechanism to construct this decoder, it led to\nless motion dynamics. The most obvious case is in the ceil-\ning scenario, where the motion appeared to be jittery and\nunstable during the transition between the walking and the\ncrouching in the ceiling scenario.\n6097\nFlat Rock\ny Obstacles Ceiling Av\nerage\nMethod Phase Full Arm\nLeg Full Arm Leg Full Arm Leg Full Arm Leg Full Arm\nLeg\nPFNN ✓ 106.5 100.6 135.9 128.7 145.0 156.0 109.1 110.9 143.9 139.9 130.9 187.0 121.1 121.9 155.7\nSingle-Pose MLP % 71.5 65.4\n90.2 86.5 90.3 110.5 78.7 71.2 108.9 109.7 103.7 142.0 86.6 82.7\n112.9\nMulti-Pose MLP % 94.0 88.1\n122.7 95.2 91.3 131.0 85.7 76.3 122.6 115.1 100.8 161.8 97.5 89.1\n134.5\nRNN % 83.3 78.4\n107.1 83.2 76.4 115.5 85.5 80.4 122.3 123.7 107.9 174.2 93.9 85.8\n129.8\nMCS-T (Ours) % 110.9 107.5\n142.8 126.7 149.0 151.6 116.1 121.4\n150.7 140.0 137.0 184.6 123.4 128.7\n157.4\n+ Middle Scale % 105.5 101.6 135.4 109.2\n117.2 140.8 104.6 108.6 140.1 122.0 111.5 164.5 110.3 109.7\n145.2\n- MS Skeletons % 96.3 87.6\n127.1 112.6 123.8 143.4 91.9 89.4 127.2 124.1 114.1 167.1 106.2 103.7\n141.2\n- CSA Decoder % 94.6 87.7\n125.2 105.6 109.1 140.2 90.7 85.2 129.3 137.5 145.8 173.7 107.1 107.0\n142.1\nTable\n1: Quantitative comparison in terms of the average joint angle update per second (degree/s) ↑ for different methods\nincluding MCS-T under four motion scenarios: Flat, Rocky, Obstacles, and Ceiling. The angle updates are further divided into\nfull body with all joints, arm and leg joints. The highest value is in bold and the second highest value is underlined.\nFigure 4: Attention maps for (a) transitional and (b) non-\ntransitional scenarios. The x-axis represents the past motion\nindices and the y-axis indicates the 6 attention heads.\nMulti-Scale and Control Signal-Aware Motion\nAttentions\nOur experiments show that MCS-T is able to synthesize mo-\ntions with the highest quality and alleviate the slow transi-\ntion issue. This lies in the attention mechanisms of MCS-T,\nwhich adaptively addresses the sequential motion context.\nThe attention map of the decoder’s first layer is visualised\nin Figure 4 to show how MCS-T performs attentions for dif-\nferent cases. Figure 4 (a) is for a frame of motion transi-\ntion from a jumping state to a jogging state. Most attention\nheads focused on the fine-level skeletons, especially in more\nrecent frames, as the further past frames were not very rel-\nevant during this motion transition. Additionally, two atten-\ntion heads paid even attentions to the coarse-scale motion,\nwhich learned global motion patterns for faster motion tran-\nsition. Figure 4 (b) is for a non-transitional case where the\ncharacter remains the jogging state, the attentions are evenly\ndistributed on all positions of the past poses, especially with\nmore attention heads focusing on the coarse-level. The rea-\nson could be that the coarse motion sequence provides suf-\nficient spatio-temporal patterns for predicting this kind of\nmotions with strong recurring patterns.\nLimitations & Future Work\nThere are two major limitations of our proposed MCS-T.\nFirst, our MCS-T method may not always synthesize the\nFigure 5: Illustration of a limitation of MCS-T, where the\nhand balancing motion is not well synthesized when the\ncharacter is walking on a beam.\nbeam walking motion well. For example, as shown in Fig-\nure 5, informed by the special terrain geometry, the character\nperformed a hand balancing motion. However, MCS-T did\nnot always launch this motion. It could be due to the small\npercentage of beam walking motion in the training data (˜2%)\nand imbalance learning strategies should be considered. Sec-\nond, since MCS-T exploits the past motion history, the er-\nror accumulation could happen with a very low chance. The\ncharacter motion could get stuck in weird poses for a very\nshort period but can escape from it by providing new con-\ntrol signals. Robust noise-based learning could be conducted\nfor alleviating such error accumulation. In our future work,\nbesides addressing these limitations, we will investigate an\nadaptive strategy for selecting past frames, such as exploring\nnetwork architecture search (NAS) (Zimmer, Lindauer, and\nHutter 2021) and token evaluation strategies.\nConclusion\nIn this paper, we present MCS-T as a transformer-based\ntask-agnostic character motion control method. With multi-\nscale graph representation, it aims to produce responsive and\ndynamic motions without explicitly using auxiliary informa-\ntion. Specifically, MCS-T involves an encoder-decoder de-\nsign, where the encoder formulates the spaio-temporal mo-\ntion patterns of past poses from multi-scale perspectives and\nthe decoder takes a control signal into account for predict-\ning the next pose. Our experiments on a public dataset have\ndemonstrated that MCS-T can produce results comparable\nto those of the state-of-the-art methods which explicitly us-\ning auxiliary information. We also investigate the limitations\nof our method for future improvement.\n6098\nAcknowledgments\nThis study was partially supported by Australian Research\nCouncil (ARC) grant #DP210102674.\nReferences\nAksan, E.; Kaufmann, M.; Cao, P.; and Hilliges, O. 2021. A\nspatio-temporal transformer for 3D human motion predic-\ntion. In International Conference on 3D Vision, 565–574.\nIEEE.\nArikan, O.; and Forsyth, D. A. 2002. Interactive motion\ngeneration from examples. ACM Transactions on Graphics,\n21(3): 483–490.\nArnab, A.; Dehghani, M.; Heigold, G.; Sun, C.; Lu ˇci´c, M.;\nand Schmid, C. 2021. ViViT: A video vision transformer.\nIn IEEE/CVF International Conference on Computer Vision,\n6836–6846.\nBhattacharya, U.; Childs, E.; Rewkowski, N.; and Manocha,\nD. 2021. Speech2affectivegestures: Synthesizing co-speech\ngestures with generative adversarial affective expression\nlearning. In ACM International Conference on Multimedia,\n2027–2036.\nButtner, M. 2019. Machine Learning for Motion Synthesis\nand Character Control in Games. ACM SIGGRAPH Sympo-\nsium on Interactive 3D Graphics and Games.\nClavet, S. 2016. Motion matching and the road to next-gen\nanimation. In Game Developer Conference.\nClevert, D.-A.; Unterthiner, T.; and Hochreiter, S. 2015. Fast\nand accurate deep network learning by exponential linear\nunits (elus). arXiv preprint arXiv:1511.07289.\nDang, L.; Nie, Y .; Long, C.; Zhang, Q.; and Li, G. 2021.\nMSR-GCN: Multi-Scale Residual Graph Convolution Net-\nworks for Human Motion Prediction. In IEEE/CVF Interna-\ntional Conference on Computer Vision, 11467–11476.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBert: Pre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16\nwords: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929.\nFragkiadaki, K.; Levine, S.; Felsen, P.; and Malik, J. 2015.\nRecurrent network models for human dynamics. In IEEE\nInternational Conference on Computer Vision, 4346–4354.\nGhosh, A.; Cheema, N.; Oguz, C.; Theobalt, C.; and\nSlusallek, P. 2021. Synthesis of compositional animations\nfrom textual descriptions. In IEEE/CVF International Con-\nference on Computer Vision, 1396–1406.\nHabibie, I.; Holden, D.; Schwarz, J.; Yearsley, J.; and Ko-\nmura, T. 2017. A recurrent variational autoencoder for hu-\nman motion synthesis. In British Machine Vision Confer-\nence.\nHenter, G. E.; Alexanderson, S.; and Beskow, J. 2020.\nMoglow: Probabilistic and controllable motion synthesis us-\ning normalising flows. ACM Transactions on Graphics,\n39(6): 1–14.\nHolden, D.; Kanoun, O.; Perepichka, M.; and Popa, T. 2020.\nLearned motion matching. ACM Transactions on Graphics,\n39(4): 53–1.\nHolden, D.; Komura, T.; and Saito, J. 2017. Phase-\nfunctioned neural networks for character control. ACM\nTransactions on Graphics, 36(4): 1–13.\nJang, D.-K.; Park, S.; and Lee, S.-H. 2022. Motion Puzzle:\nArbitrary Motion Style Transfer by Body Part. ACM Trans-\nactions on Graphics.\nKania, K.; Kowalski, M.; and Trzci´nski, T. 2021. TrajeV AE–\nControllable Human Motion Generation from Trajectories.\narXiv preprint arXiv:2104.00351.\nKingma, D. P.; and Ba, J. 2014. Adam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980.\nKovar, L.; Gleicher, M.; and Pighin, F. 2008. Motion graphs.\nIn ACM SIGGRAPH 2008 classes, 1–10.\nLee, J.; Chai, J.; Reitsma, P. S.; Hodgins, J. K.; and Pol-\nlard, N. S. 2002. Interactive control of avatars animated\nwith human motion data. In Annual Conference on Com-\nputer Graphics and Interactive Techniques, 491–500.\nLee, K.; Lee, S.; and Lee, J. 2018. Interactive character an-\nimation by learning multi-objective control. ACM Transac-\ntions on Graphics, 37(6): 1–10.\nLee, Y .; Wampler, K.; Bernstein, G.; Popovi ´c, J.; and\nPopovi´c, Z. 2010. Motion fields for interactive character\nlocomotion. In ACM SIGGRAPH Asia 2010 papers, 1–8.\nLi, M.; Chen, S.; Zhao, Y .; Zhang, Y .; Wang, Y .; and Tian,\nQ. 2020. Dynamic multiscale graph neural networks for\n3D skeleton based human motion prediction. In IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\n214–223.\nLi, P.; Aberman, K.; Zhang, Z.; Hanocka, R.; and Sorkine-\nHornung, O. 2022. GANimator: Neural Motion Synthesis\nfrom a Single Sequence. arXiv preprint arXiv:2205.02625.\nLi, Z.; Zhou, Y .; Xiao, S.; He, C.; Huang, Z.; and Li,\nH. 2017. Auto-conditioned recurrent networks for ex-\ntended complex human motion synthesis. arXiv preprint\narXiv:1707.05363.\nLing, H. Y .; Zinno, F.; Cheng, G.; and Van De Panne, M.\n2020. Character controllers using motion vaes. ACM Trans-\nactions on Graphics, 39(4): 40–1.\nLiu, Z.; Lyu, K.; Wu, S.; Chen, H.; Hao, Y .; and Ji, S. 2021.\nAggregated multi-gans for controlled 3D human motion pre-\ndiction. In AAAI Conference on Artificial Intelligence, vol-\nume 35, 2225–2232.\nLiu, Z.; Zhang, H.; Chen, Z.; Wang, Z.; and Ouyang, W.\n2020. Disentangling and unifying graph convolutions for\nskeleton-based action recognition. InIEEE/CVF Conference\non Computer Vision and Pattern Recognition, 143–152.\nMao, W.; Liu, M.; and Salzmann, M. 2020. History repeats\nitself: Human motion prediction via motion attention. InEu-\nropean Conference on Computer Vision, 474–489. Springer.\nMart´ınez-Gonz´alez, A.; Villamizar, M.; and Odobez, J.-M.\n2021. Pose transformers (POTR): Human motion prediction\nwith non-autoregressive transformers. In IEEE/CVF Inter-\nnational Conference on Computer Vision, 2276–2284.\n6099\nMason, I.; Starke, S.; and Komura, T. 2022. Real-Time Style\nModelling of Human Locomotion via Feature-Wise Trans-\nformations and Local Motion Phases. ACM on Computer\nGraphics and Interactive Techniques, 5(1): 1–18.\nMazzia, V .; Angarano, S.; Salvetti, F.; Angelini, F.; and\nChiaberge, M. 2022. Action Transformer: A self-attention\nmodel for short-time pose-based human action recognition.\nPattern Recognition, 124: 108487.\nPaszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.;\nChanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.; Antiga, L.;\net al. 2019. Pytorch: An imperative style, high-performance\ndeep learning library. arXiv preprint arXiv:1912.01703.\nPeng, X. B.; Abbeel, P.; Levine, S.; and van de Panne,\nM. 2018. Deepmimic: Example-guided deep reinforcement\nlearning of physics-based character skills. ACM Transac-\ntions on Graphics, 37(4): 1–14.\nPetrovich, M.; Black, M. J.; and Varol, G. 2021. Action-\nconditioned 3D human motion synthesis with transformer\nvae. In IEEE/CVF International Conference on Computer\nVision, 10985–10995.\nPlizzari, C.; Cannici, M.; and Matteucci, M. 2021. Spa-\ntial temporal transformer network for skeleton-based action\nrecognition. In International Conference on Pattern Recog-\nnition, 694–701. Springer.\nStarke, S.; Zhang, H.; Komura, T.; and Saito, J. 2019. Neural\nstate machine for character-scene interactions. ACM Trans-\nactions on Graphics, 38(6): 209–1.\nStarke, S.; Zhao, Y .; Komura, T.; and Zaman, K. 2020. Local\nmotion phases for learning multi-contact character move-\nments. ACM Transactions on Graphics, 39(4): 54–1.\nStarke, S.; Zhao, Y .; Zinno, F.; and Komura, T. 2021. Neural\nanimation layering for synthesizing martial arts movements.\nACM Transactions on Graphics, 40(4): 1–16.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At-\ntention is all you need. arXiv preprint arXiv:1706.03762.\nWang, J.; Xu, H.; Narasimhan, M.; and Wang, X. 2021.\nMulti-Person 3D Motion Prediction with Multi-Range\nTransformers. Advances in Neural Information Processing\nSystems, 34: 6036–6049.\nWang, L.; Hu, K.; Bai, L.; Ding, Y .; Ouyang, W.; and\nWang, Z. 2023. Multi-Scale Control Signal-Aware Trans-\nformer for Motion Synthesis without Phase. arXiv preprint\narXiv:2303.01685.\nWilliams, R. J.; and Zipser, D. 1989. A learning algo-\nrithm for continually running fully recurrent neural net-\nworks. Neural Computation, 1(2): 270–280.\nZhang, H.; Starke, S.; Komura, T.; and Saito, J. 2018. Mode-\nadaptive neural networks for quadruped motion control.\nACM Transactions on Graphics, 37(4): 1–11.\nZheng, C.; Zhu, S.; Mendieta, M.; Yang, T.; Chen, C.; and\nDing, Z. 2021. 3D human pose estimation with spatial and\ntemporal transformers. In IEEE/CVF International Confer-\nence on Computer Vision, 11656–11665.\nZimmer, L.; Lindauer, M.; and Hutter, F. 2021. Auto-\nPytorch: Multi-fidelity metalearning for efficient and robust\nAutoDL. IEEE Transactions on Pattern Analysis and Ma-\nchine Intelligence, 43(9): 3079–3090.\nZinno, F. 2019. ML Tutorial Day: From Motion Matching\nto Motion Synthesis, and All the Hurdles In Between.Game\nDeveloper Conference.\n6100"
}