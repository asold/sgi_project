{
  "title": "Revisiting Automated Topic Model Evaluation with Large Language Models",
  "url": "https://openalex.org/W4389520774",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3109915203",
      "name": "Dominik Stammbach",
      "affiliations": [
        "Institute for Ethnic Studies"
      ]
    },
    {
      "id": "https://openalex.org/A2990894818",
      "name": "Vilém Zouhar",
      "affiliations": [
        "Institute for Ethnic Studies"
      ]
    },
    {
      "id": "https://openalex.org/A2808676937",
      "name": "Alexander Hoyle",
      "affiliations": [
        "Institute for Ethnic Studies"
      ]
    },
    {
      "id": "https://openalex.org/A2035754532",
      "name": "Mrinmaya Sachan",
      "affiliations": [
        "Institute for Ethnic Studies"
      ]
    },
    {
      "id": "https://openalex.org/A2308350374",
      "name": "Elliott Ash",
      "affiliations": [
        "Institute for Ethnic Studies"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4307079201",
    "https://openalex.org/W2251582277",
    "https://openalex.org/W4322760121",
    "https://openalex.org/W3045464143",
    "https://openalex.org/W4319793767",
    "https://openalex.org/W2985204356",
    "https://openalex.org/W4327811957",
    "https://openalex.org/W3013577339",
    "https://openalex.org/W4384662964",
    "https://openalex.org/W4287100616",
    "https://openalex.org/W2803437449",
    "https://openalex.org/W4235169531",
    "https://openalex.org/W4380136141",
    "https://openalex.org/W2095655043",
    "https://openalex.org/W2130339025",
    "https://openalex.org/W4385574240",
    "https://openalex.org/W2147946282",
    "https://openalex.org/W2038043464",
    "https://openalex.org/W4367623968",
    "https://openalex.org/W3138819813",
    "https://openalex.org/W4385688511",
    "https://openalex.org/W4378470965",
    "https://openalex.org/W2159426623",
    "https://openalex.org/W2976420234",
    "https://openalex.org/W3184144760",
    "https://openalex.org/W3168771811",
    "https://openalex.org/W4231510805",
    "https://openalex.org/W1982474113",
    "https://openalex.org/W3166574703",
    "https://openalex.org/W4298422451"
  ],
  "abstract": "Topic models help us make sense of large text collections. Automatically evaluating their output and determining the optimal number of topics are both longstanding challenges, with no effective automated solutions to date. This paper proposes using large language models (LLMs) for these tasks. We find that LLMs appropriately assess the resulting topics, correlating more strongly with human judgments than existing automated metrics. However, the setup of the evaluation task is crucial — LLMs perform better on coherence ratings of word sets than on intrustion detection. We find that LLMs can also assist us in guiding us towards a reasonable number of topics. In actual applications, topic models are typically used to answer a research question related to a collection of texts. We can incorporate this research question in the prompt to the LLM, which helps estimating the optimal number of topics.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 9348–9357\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nRevisiting Automated Topic Model Evaluation\nwith Large Language Models\nDominik StammbachE Vilém ZouharE Alexander HoyleM\nMrinmaya SachanE Elliott AshE\nEETH Zürich MUniversity of Maryland\n{dominsta,vzouhar,ashe,msachan}@ethz.ch hoyle@umd.edu\nAbstract\nTopic models help make sense of large text col-\nlections. Automatically evaluating their output\nand determining the optimal number of topics\nare both longstanding challenges, with no ef-\nfective automated solutions to date. This paper\nevaluates the effectiveness of large language\nmodels (LLMs) for these tasks. We find that\nLLMs appropriately assess the resulting top-\nics, correlating more strongly with human judg-\nments than existing automated metrics. How-\never, the type of evaluation task matters —\nLLMs correlate better with coherence ratings\nof word sets than on a word intrusion task. We\nfind that LLMs can also guide users toward a\nreasonable number of topics. In actual applica-\ntions, topic models are typically used to answer\na research question related to a collection of\ntexts. We can incorporate this research ques-\ntion in the prompt to the LLM, which helps\nestimate the optimal number of topics.\ngithub.com/dominiksinsaarland/\nevaluating-topic-model-output\n1 Introduction\nTopic models are, loosely put, an unsupervised\ndimensionality reduction technique that help orga-\nnize document collections (Blei et al., 2003). A\ntopic model summarizes a document collection\nwith a small number of topics. A topic is a proba-\nbility distribution over words or phrases. A topic\nT is interpretable through a representative set of\nwords or phrases defining the topic, denoted WT .1\nEach document can, in turn, be represented\nas a distribution over topics. For each topic, we\ncan retrieve a representative document collection\nby sorting documents across topic distributions.\nWe denote this set of documents for topic T as\nDT . Because of their ability to organize large\ncollections of texts, topic models are widely used\n1We think of “words” as an atomic unit in a document,\nwhich can also be an n-gram or phrase. E.g., Wlegal = {litiga-\ntion, attorney-client privilege, intellectual property, . . . }.\nin the social sciences, digital humanities, and other\ndisciplines to analyze large corpora (Talley et al.,\n2011; Grimmer and Stewart, 2013; Antoniak et al.,\n2019; Karami et al., 2020, inter alia).\nInterpretability makes topic models useful, but\nhuman interpretation is complex and notoriously\ndifficult to approximate (Lipton, 2018). Automated\ntopic coherence metrics do not correlate well with\nhuman judgments, often overstating differences be-\ntween models (Hoyle et al., 2021; Doogan and Bun-\ntine, 2021). Without the guidance of an automated\nmetric, the number of topics, an important hyperpa-\nrameter, is usually derived manually: Practitioners\nfit various topic models, inspect the resulting top-\nics, and select the configuration which works best\nfor the intended use case (Hoyle et al., 2021). This\nis a non-replicable and time-consuming process,\nrequiring expensive expert labor.\nRecent NLP research explores whether large lan-\nguage models (LLMs) can perform automatic anno-\ntations; e.g., to assess text quality (Fu et al., 2023;\nFaggioli et al., 2023; Huang et al., 2023, inter alia).\nHere, we investigate whether LLMs can automati-\ncally assess the coherence of topic modeling output\nand conclude that:\n(1) LLMs can accurately judge topic coherence,\n(2) LLMs can assist in automatically determining\nreasonable numbers of topics.\nWe use LLMs for two established topic coher-\nence evaluation tasks and find that their judgment\nstrongly correlates with humans on one of these\ntasks. Similar to recent findings, we find that coher-\nent topic word sets WT do not necessarily imply an\noptimal categorization of the document collection\n(Doogan and Buntine, 2021). Instead, we automati-\ncally assign a label to each document in a DT and\nchoose the configuration with the purest assigned\nlabels. This solution correlates well with an under-\nlying ground truth. Thus, LLMs can help find good\nnumbers of topics for a text collection, as we show\nin three case studies.\n9348\n2 Topic Model Evaluation\nMost topic model evaluations focus on the coher-\nence of WT , the most probable words from the\ntopic-word distribution (Röder et al., 2015). Co-\nherence itself can be thought of as whether the\ntop words elicit a distinct concept in the reader\n(Hoyle et al., 2021). To complicate matters, human\nevaluation of topic models can be done in diverse\nways. E.g., we can ask humans to directly rate\ntopic coherence, for example, on a 1-3 scale (New-\nman et al., 2010a; Mimno et al., 2011; Aletras and\nStevenson, 2013, inter alia). We can also add an\nunrelated intruder word to the list of top words,\nwhich human annotators are asked to identify. The\nintuition is that intruder words are easily identi-\nfied within coherent and self-contained topics, but\nhard to identify for incoherent or not self-contained\ntopics (Chang et al., 2009). High human accuracy\non this task is thus a good proxy for high topic\ncoherence. See both in Example 1.\nIntrusion Detection Task\nwater area river park miles game\nhorses horse breed hindu coins silver\nRating Task\nhealth hospital medicare welfare insure 3\nhorses zurich race dog canal 1\nExample 1: Two examples of intrusion detection (select\noutlier) and topic rating tasks (rate overall coherence).\nEach example is on separate row.\nAlthough many automated metrics exist (Wal-\nlach et al., 2009; Newman et al., 2010b; Mimno\net al., 2011; Aletras and Stevenson, 2013), normal-\nized pointwise mutual information (NPMI, Bouma,\n2009) is the most prevalent when evaluating novel\nmethods (Hoyle et al., 2021). Informally, NPMI is\nlarger if two words co-occur together regularly in a\nreference corpus. Another popular metric, Cv, is\na combination of NPMI and other measures and is\nalso popular (Röder et al., 2015). See the formula\ndefinitions in Appendix D.\nDespite their popular use, these metrics corre-\nlate poorly with human evaluations (Hoyle et al.,\n2021; Doogan and Buntine, 2021). In this work,\nwe let LLMs perform the rating and intrusion detec-\ntion tasks for topic model evaluation2 and propose\nLLM scores as a novel automated metric. Similar\n2We use ChatGPT as the main LLM (chat.openai.com).\nWe list ablation results using other LLMs in Appendix B.\nwork by Rahimi et al. (2023) is carried contempo-\nraneously. LLMs have already been used to rank\nmachine translations and generated text (Zhang\net al., 2020; Fu et al., 2023; Kocmi and Federmann,\n2023) and have also been shown to perform on par\nwith crowdworkers (Gilardi et al., 2023).\nTask Dataset NPMI Cv LLM Ceiling\nIntrusion\nNYT 0.43 0.45 † 0.37 0.67\nWiki 0.39 † 0.34 0.35 0.60\nBoth 0.40 † 0.40† 0.36 0.64\nRating\nNYT 0.48 0.40 0.64 ⋆ 0.72\nWiki 0.44 0.40 0.57 ⋆ 0.56\nBoth 0.44 0.42 0.59 ⋆ 0.65\nTable 1: Spearman correlation between human scores\nand automated metrics. All results use 1000 bootstrap-\nping episodes — re-sampling human annotations and\nLLM scores, and averaging the correlations. Marked⋆if\nsignificantly better than second best (<0.05), otherwise\n†. Ceiling shows batched inter-annotator agreement.\n3 LLM and Coherence\nFirst, we show that large language models can as-\nsess the quality of topics generated by different\ntopic modeling algorithms. We use existing topic\nmodeling output annotated by humans (Hoyle et al.,\n2021).3 This data consists of 300 topics, produced\nby three different topic modeling algorithms on two\ndatasets: NYtimes (Sandhaus, 2008) and Wikitext\n(Merity et al., 2017). For each of the 300 topics,\nthere are 15 individual human annotations for the\ntopic word relatedness (on 1-3 scale), and 26 in-\ndividual annotations for whether a crowd-worker\ncorrectly detected an intruder word. We replicate\nboth tasks, prompting LLMs instead of human an-\nnotators. See Prompt 1 for prompt excerpts, and\nAppendix A for full details.\nIntruder detection prompt.\nSystem prompt: [...] Select which word is the least related to all other words.\nIf multiple words do not fit, choose the word that is most out of place. [...]\nUser prompt: water, area, river, park, miles, game\nRating Task prompt.\nSystem prompt: [...] Please rate how related the following words are to each\nother on a scale from 1 to 3 (\"1\" = not very related, \"2\" = moderately related,\n\"3\" = very related). [...]\nUser prompt: lake, park, river, land, years, feet, ice, miles, water, area\nPrompt 1: LLM prompts for assessing topic coherence.\nWe compute the Spearman correlation between\nthe LLM answer and the human assessment of the\ntopics and show results in Table 1.\n3Models: Gibbs-LDA (McCallum, 2002), Dirichlet-V AE\n(Burkhardt and Kramer, 2019), and ETM (Dieng et al., 2020).\n9349\nBaseline metrics. For NPMI and Cv, we report\nthe best correlation by Hoyle et al. (2021). These\nmetrics depend on the reference corpus and other\nhyperparameters and we always report the best\nvalue. Hoyle et al. (2021) find no singlebest setting\nfor these automated metrics and therefore this com-\nparison makes the baseline inadequately strong.\nIntrusion detection task. The accuracies for de-\ntecting intruder words in the evaluated topics are\nalmost identical – humans correctly detect 71.2%\nof the intruder words, LLMs identify intruders in\n72.2% of the cases. However, humans and LLMs\ndiffer for which topics these intruder words are\nidentified. This results in overall strong correla-\ntions within human judgement, but not higher cor-\nrelations than NPMI and Cv (in their best setting).\nCoherence rating task. The LLM rating of the\nWT top word coherence correlates more strongly\nwith human evaluations than all other automated\nmetrics in any setting. This difference is statis-\ntically significant, and the correlation between\nLLM ratings and human assessment approaches\nthe inter-annotator agreement ceiling. Appendix\nAppendix B shows additional results with different\nprompts and LLMs.\nRecommendation. Both findings support using\nLLMs for evaluating coherence of WT in practice\nas they correlate highly with human judgements.\n4 Determining the Number of Topics\nTopic models require specifying the number of top-\nics. Practitioners usually run models multiple times\nwith different numbers of topics (denoted byk). Af-\nter manual inspection, the model which seems most\nsuited for a research question is chosen. Doogan\net al. (2023) review 189 articles about topic model-\ning and find that common use cases are exploratory\nand descriptive studies for which no single best\nnumber of topics exists. However, the most preva-\nlent use case is to isolate semantically similar doc-\numents belonging to topics of interest. For this,\nDoogan and Buntine (2021) challenge the focus\non only evaluating WT , and suggest an analysis\nof DT as well. If we are interested in organizing\na collection, then we would expect the top docu-\nments in DT to receive the same topic labels. We\nprovide an LLM-based strategy to determine good\nnumber of topics for this use case: We let an LLM\nassign labels to documents, and find that topic as-\nsignments with greater label purity correlate with\nthe ground-truth in three case studies.\nTopics of interest might be a few broad topics\nsuch as politics or healthcare, or many specific\ntopics, like municipal elections and maternity care.\nFollowing recent efforts that use research questions\nto guide LLM-based text analysis (Zhong et al.,\n2023), we incorporate this desideratum in the LLM\nprompt. We run collapsed Gibbs-sampled LDA\n(in MALLET : McCallum, 2002) on two text col-\nlections, with different numbers of topics ( k =\n20 to 400), yielding 20 models per collection. To\ncompare topic model estimates and ground-truth\npartitions, we experiment with a legislative Bill\nsummary dataset (from Hoyle et al., 2022) and\nWikitext (Merity et al., 2017), both annotated with\nground-truth topic labels in different granularities.\n4.1 Proposed Metrics\nRatings algorithm. For each of the 20 models,\nwe randomly sample WT for some topics and let\nthe LLM rate these WT . The prompt is similar\nto the ratings prompt shown in Prompt 1, see Ap-\npendix E for full details. We then average ratings\nfor each configuration. Intuitively, the model yield-\ning the most coherent WT should be the one with\nthe optimal topic count. However, this procedure\ndoes not correlate with ground-truth labels.\nText labeling algorithm. Doogan and Buntine\n(2021) propose that domain experts assign labels\nto each document in a DT instead. A good topic\nshould have a coherent DT : The same label as-\nsigned to most documents. Hence, good configu-\nrations have high purity of assigned labels within\neach topic. We proceed analogously. For each\nof the 20 models, we randomly sample DT for\nvarious topics. We retrieve the 10 most probable\ndocuments and then use the LLM to assign a label\nto these documents. We use the system prompt[...]\nAnnotate the document with a broad|narrow label\n[...], see Appendix E for full details. We compute\nthe purity of the assigned labels and average puri-\nties and we select the configuration with the most\npure topics. In both procedures, we smooth the\nLLM outputs using a rolling window average to re-\nduce noise (the final average goodness is computed\nas moving average of window of size 3).\n4.2 Evaluation\nWe need a human-derived metric to compare with\nthe purity metric proposed above. We measure\n9350\nFigure 1: (1) ARI for topic assignment and ground-truth topic labels, (2) LLM word set coherence, (3) LLM\ndocument set purity, obtained by our algorithm. ARI correlates with LLM document set purity, but not with LLM\nword set coherence. The ground-truth number of topics are: 21 topics in the BillSum dataset, 45 broad topics in\nWikitext and 279 specific topics in Wikitext. ρD and ρW are document-LLM and word-LLM correlations with ARI.\nthe alignment between a topic model’s predicted\ntopic assignments and the ground-truth labels for a\ndocument collection (Hoyle et al., 2022).\nWe choose theAdjusted Rand Index (ARI) which\ncompares two clusterings (Hubert and Arabie,\n1985) and is high when there is strong overlap. The\npredicted topic assignment for each document is its\nmost probable topic. Recall that there exist many\ndifferent optimal topic models for a single collec-\ntion. If we want topics to contain semantically\nsimilar documents, each ground-truth assignment\nreflects one possible set of topics of interests.\nIf our LLM-guided procedure and the ARI corre-\nlate, this indicates that we discovered a reasonable\nvalue for the number of topics. In our case, the\nvarious ground-truth labels are assigned with dif-\nferent research questions in mind. We incorporate\nsuch constraints in the LLM prompt: We specify\nwhether we are interested in broad or specific top-\nics, and we enumerate some example ground-truth\ncategories in our prompt. Practitioners usually have\npriors about topics of interest before running topic\nmodels, thus we believe this setup to be realistic.\nIn Figure 1 we show LLM scores and ARI for\nbroad topics in the Bills dataset. We used this\ndataset to find a suitable prompt, hence this could\nbe considered the “training set”. We plot coherence\nratings of word sets in blue , purity of document\nlabels in red , and the ARI between topic model\nand ground-truth assignments in green. The pu-\nrity of LLM-assigned DT labels correlate with the\nARI, whereas the WT coherence scores do not. The\nargmax of the purity-based approach leads to sim-\nilar numbers of topics as suggested by the ARI\nargmax (although not always the same).\nFor Wikitext, we evaluate the same 20 topic mod-\nels, but measure ARI between topic model assign-\nment and two different ground-truth label sets. The\nLLM scores differ only because of different prompt-\ning strategies. The distributions indicate that this\nstrategy incorporates different research questions.\nFor Bills, our rating algorithm suggests to use a\ntopic model withk=100 topics. In Appendix G, we\nshow corresponding word sets. The resulting WT\nseem interpretable, although the ground-truth as-\nsignments using document-topic estimates are not\ncorrelated with the ground-truth labels. The purity-\nbased approach instead suggests to use k=20 top-\nics, the same kas indicated by the ARI. We show\nground-truth labels and LLM-obtained text labels\nin Appendix G. We further manually evaluate 180\nassigned LLM-labels and find that 94% of these\nlabels are reasonable. Appendix F shows further\nevaluation of these label assignments.\n5 Discussion\nIn this work, we revisit automated topic model\nevaluation with the help of large language mod-\nels. Many automated evaluation metrics for topic\nmodels exist, however these metrics seem to not\ncorrelate strongly with human judgment on word-\nset analysis (Hoyle et al., 2021). Instead, we find\nthat an LLM-based metric of coherent topic words\ncorrelates with human preferences, outperforming\nother metrics on the rating task.\nSecond, the number of topics khas to be defined\nbefore running a topic model, so practitioners run\nmultiple models with different k. We investigate\nwhether LLMs can guide us towards reasonable k\nfor a collection and research question. We first note\nthat the term optimal number of topics is vague and\nthat such quantity does not exist without additional\ncontext. If our goal is to find a configuration which\nwould result in coherent document sets for topics,\nour study supports evaluating DT instead of WT ,\nas this correlates more strongly with the overlap\nbetween topic model and ground-truth assignment.\nThis finding supports arguments made in Doogan\nand Buntine (2021) who challenge the focus on\nWT in topic model evaluation.\n9351\nLimitations\nChoice of LLM. Apart from ChatGPT, we also\nused open-source LLMs, such as FLAN-T5 (Chung\net al., 2022), and still obtained reasonable, al-\nbeit worse than ChatGPT, coherence correlations.\nGiven the rapid advances, future iterations of open-\nsource LLMs will likely become better at this task.\nNumber of topics. The optimal number of topics\nis a vague concept, dependent on a practitioner’s\ngoals and the data under study. At the same time,\nit is a required hyperparameter of topic models.\nBased on Doogan et al. (2023), we use an existing\ndocument categorization as one possible ground\ntruth. While content analysis is the most popular\napplication of topic models (Hoyle et al., 2022),\nit remains an open question how they compare to\nalternative clustering algorithms for this use case\n(e.g., k-means over document embeddings).\nInterpretability. LLM label assignment and in-\ntruder detection remain opaque. This hinders the\nunderstanding of the evaluation decisions.\nTopic modeling algorithm. In Section 3, we\nevaluate three topic modeling algorithms: Gibbs-\nLDA, Dirichlet-V AE and ETM (see Hoyle et al.,\n2021). In Section 4, we use only Gibbs-LDA and\nexpansion to further models is left for future work.\nFuture work.\n• Evaluation of clustering algorithms with LLMs\n(e.g., k-means).\n• More rigorous evaluation of open-source LLMs.\n• Formalization, implementation and release of an\nLLM-guided algorithm for automatically finding\noptimal numbers of topics for a text collection\nand a research question.\nEthics Statement\nUsing blackbox models in NLP. Statistically sig-\nnificant positive results are a sufficient proof of\nmodels’ capabilities, assuming that the training\ndata is not part of the training set. This data leak-\nage problem with closed-source LLMs is part of a\nbigger and unresolved discussion. In our case, we\nbelieve data leakage is unlikely. Admittedly, the\ndata used for our coherence experiments has been\npublicly available. However, the data is available in\na large JSON file where the topic words and anno-\ntated labels are stored disjointly. For our case stud-\nies in Section 4, the topic modeling was constructed\nas part of this work and there is no ground-truth\nwhich could leak to the language model.\nNegative results with LLMs. In case of negative\nresults, we cannot conclude that a model can not\nbe used for a particular task. The negative results\ncan be caused by inadequate prompting strategies\nand may even be resolved by advances in LLMs.\nLLMs and biases. LLMs are known to be bi-\nased (Abid et al., 2021; Lucy and Bamman, 2021)\nand their usage in this application may potentially\nperpetuate these biases.\nData privacy. All data used in this study has\nbeen collected as part of other work. We find no\npotential violations of data privacy. Thus, we feel\ncomfortable re-using the data in this work.\nMisuse potential. We urge practicioners to not\nblindly apply our method on their topic modeling\noutput, but still manually validate that the topic out-\nputs would be suitable to answer a given research\nquestion.\nReferences\nAbubakar Abid, Maheen Farooqi, and James Zou. 2021.\nPersistent anti-muslim bias in large language models.\nIn Proceedings of the 2021 AAAI/ACM Conference\non AI, Ethics, and Society, pages 298–306.\nNikolaos Aletras and Mark Stevenson. 2013. Evaluat-\ning topic coherence using distributional semantics. In\nProceedings of the 10th international conference on\ncomputational semantics (IWCS 2013)–Long Papers,\npages 13–22.\nMaria Antoniak, David Mimno, and Karen Levy. 2019.\nNarrative paths and negotiation of power in birth sto-\nries. Proc. ACM Hum.-Comput. Interact., 3(CSCW).\nDavid M. Blei, Andrew Y . Ng, and Michael I. Jordan.\n2003. Latent dirichlet allocation. J. Mach. Learn.\nRes., 3(null):993–1022.\nGerlof J. Bouma. 2009. Normalized (pointwise) mutual\ninformation in collocation extraction.\nSophie Burkhardt and Stefan Kramer. 2019. Decoupling\nsparsity and smoothness in the dirichlet variational\nautoencoder topic model. Journal of Machine Learn-\ning Research, 20(131):1–27.\nJonathan Chang, Sean Gerrish, Chong Wang, Jordan\nBoyd-graber, and David Blei. 2009. Reading tea\nleaves: How humans interpret topic models. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 22. Curran Associates, Inc.\n9352\nYew Ken Chia, Pengfei Hong, Lidong Bing, and Sou-\njanya Poria. 2023. INSTRUCTEV AL: Towards holis-\ntic evaluation of instruction-tuned large language\nmodels.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Al-\nbert Webson, Shixiang Shane Gu, Zhuyun Dai,\nMirac Suzgun, Xinyun Chen, Aakanksha Chowdh-\nery, Alex Castro-Ros, Marie Pellat, Kevin Robinson,\nDasha Valter, Sharan Narang, Gaurav Mishra, Adams\nYu, Vincent Zhao, Yanping Huang, Andrew Dai,\nHongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-\ncob Devlin, Adam Roberts, Denny Zhou, Quoc V . Le,\nand Jason Wei. 2022. Scaling instruction-finetuned\nlanguage models.\nAdji B. Dieng, Francisco J. R. Ruiz, and David M. Blei.\n2020. Topic modeling in embedding spaces. Trans-\nactions of the Association for Computational Linguis-\ntics, 8:439–453.\nCaitlin Doogan and Wray Buntine. 2021. Topic model\nor topic twaddle? Re-evaluating semantic inter-\npretability measures. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 3824–3848, Online.\nAssociation for Computational Linguistics.\nCaitlin Doogan, Wray Buntine, and Henry Linger. 2023.\nA systematic review of the use of topic models for\nshort text social media analysis. Artificial Intelli-\ngence Review, pages 1–33.\nGuglielmo Faggioli, Laura Dietz, Charles Clarke, Gi-\nanluca Demartini, Matthias Hagen, Claudia Hauff,\nNoriko Kando, Evangelos Kanoulas, Martin Potthast,\nBenno Stein, and Henning Wachsmuth. 2023. Per-\nspectives on large language models for relevance\njudgment.\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei\nLiu. 2023. GPTScore: Evaluate as you desire.\nFabrizio Gilardi, Meysam Alizadeh, and Maël Kubli.\n2023. ChatGPT outperforms crowd-workers for text-\nannotation tasks.\nJustin Grimmer and Brandon M. Stewart. 2013. Text\nas data: The promise and pitfalls of automatic con-\ntent analysis methods for political texts. Political\nAnalysis, 21(3):267–297.\nAlexander Hoyle, Pranav Goel, Denis Peskov, An-\ndrew Hian-Cheong, Jordan Boyd-Graber, and Philip\nResnik. 2021. Is automated topic model evaluation\nbroken?: The incoherence of coherence.\nAlexander Miserlis Hoyle, Rupak Sarkar, Pranav Goel,\nand Philip Resnik. 2022. Are neural topic models\nbroken? In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2022, pages 5321–5344,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nFan Huang, Haewoon Kwak, and Jisun An. 2023. Is\nchatgpt better than human annotators? potential and\nlimitations of chatgpt in explaining implicit hate\nspeech. arXiv preprint arXiv:2302.07736.\nLawrence Hubert and Phipps Arabie. 1985. Comparing\npartitions. Journal of classification, 2:193–218.\nAmir Karami, Morgan Lundy, Frank Webb, and Yo-\ngesh K. Dwivedi. 2020. Twitter and research: A sys-\ntematic literature review through text mining. IEEE\nAccess, 8:67698–67717.\nTom Kocmi and Christian Federmann. 2023. Large\nlanguage models are state-of-the-art evaluators of\ntranslation quality.\nZachary C. Lipton. 2018. The mythos of model in-\nterpretability: In machine learning, the concept of\ninterpretability is both important and slippery.Queue,\n16(3):31–57.\nLi Lucy and David Bamman. 2021. Gender and rep-\nresentation bias in GPT-3 generated stories. In Pro-\nceedings of the Third Workshop on Narrative Under-\nstanding, pages 48–55.\nAndrew Kachites McCallum. 2002. MALLET: A\nMAachine Learning for LanguagE Toolkit. http:\n//mallet.cs.umass.edu.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture mod-\nels. In 5th International Conference on Learning\nRepresentations, ICLR 2017 Toulon, France, April\n24-26, 2017, Conference Track Proceedings. Open-\nReview.net.\nDavid Mimno, Hanna Wallach, Edmund Talley, Miriam\nLeenders, and Andrew McCallum. 2011. Optimizing\nsemantic coherence in topic models. In Proceed-\nings of the 2011 conference on empirical methods in\nnatural language processing, pages 262–272.\nDavid Newman, Jey Han Lau, Karl Grieser, and Timo-\nthy Baldwin. 2010a. Automatic evaluation of topic\ncoherence. In Human language technologies: The\n2010 annual conference of the North American chap-\nter of the association for computational linguistics,\npages 100–108.\nDavid Newman, Youn Noh, Edmund Talley, Sarvnaz\nKarimi, and Timothy Baldwin. 2010b. Evaluating\ntopic models for digital libraries. In Proceedings of\nthe 10th annual joint conference on Digital libraries,\npages 215–224.\nHamed Rahimi, Jacob Louis Hoover, David Mimno, Hu-\nbert Naacke, Camelia Constantin, and Bernd Amann.\n2023. Contextualized topic coherence metrics.\nMichael Röder, Andreas Both, and Alexander Hinneb-\nurg. 2015. Exploring the space of topic coherence\nmeasures. In Proceedings of the eighth ACM inter-\nnational conference on Web search and data mining,\npages 399–408.\n9353\nEvan Sandhaus. 2008. The new york times annotated\ncorpus.\nEdmund M Talley, David Newman, David Mimno,\nBruce W Herr 2nd, Hanna M Wallach, Gully A P C\nBurns, A G Miriam Leenders, and Andrew McCal-\nlum. 2011. Database of NIH grants using machine-\nlearned categories and graphical clustering. Nat.\nMethods, 8(6):443–444.\nNguyen Xuan Vinh, Julien Epps, and James Bailey.\n2010. Information theoretic measures for cluster-\nings comparison: Variants, properties, normalization\nand correction for chance. J. Mach. Learn. Res. ,\n11:2837–2854.\nHanna M Wallach, Iain Murray, Ruslan Salakhutdinov,\nand David Mimno. 2009. Evaluation methods for\ntopic models. In Proceedings of the 26th annual in-\nternational conference on machine learning, pages\n1105–1112.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. BERTScore:\nEvaluating text generation with BERT. In 8th In-\nternational Conference on Learning Representations,\nICLR 2020 Addis Ababa, Ethiopia, April 26-30, 2020.\nOpenReview.net.\nRuiqi Zhong, Peter Zhang, Steve Li, Jinwoo Ahn, Dan\nKlein, and Jacob Steinhardt. 2023. Goal driven dis-\ncovery of distributional differences via language de-\nscriptions.\nA Language Model Prompts\nIn this section, we show the used LLM prompts.\nThe task descriptions are borrowed from (Hoyle\net al., 2021) and mimic crowd-worker instructions.\nWe use a temperature of 1 for LLMs, and the topic\nwords are shuffled before being prompted. Both in-\ntroduce additional variation within the results, sim-\nilar to how some variation is introduced if different\ncrowd-workers are asked to perform the same task.\nIntruder detection task. Analogous to the hu-\nman experiment, we randomly sample (a) five word\nfrom the top 10 topic words and (b) an additional\nintruder word from a different topic which does\nnot occur in the top 50 words of the current topic.\nWe then shuffle these six words. We show the final\nprompt with an example topic in Prompt 2. We also\nconstruct a prompt without the dataset description\n(see Prompt 3 and results in Table 2).\nSystem prompt: You are a helpful assistant evaluating the top words of a topic\nmodel output for a given topic. Select which word is the least related to all other\nwords. If multiple words do not fit, choose the word that is most out of place.\nThe topic modeling is based on The New York Times corpus. The corpus\nconsists of articles from 1987 to 2007. Sections from a typical paper include\nInternational, National, New York Regional, Business, Technology, and Sports\nnews; features on topics such as Dining, Movies, Travel, and Fashion; there are\nalso obituaries and opinion pieces. Reply with a single word.\nUser prompt: water, area, river, park, miles, game\nPrompt 2: Intruder Detection Task (the intruder word\nin this topic is game). We show the task description for\nthe New York Times dataset in the prompt for the rating\ntask (the dataset descriptions are kept the same).\nSystem prompt: You are a helpful assistant evaluating the top words of a topic\nmodel output for a given topic. Select which word is the least related to all other\nwords. If multiple words do not fit, choose the word that is most out of place.\nReply with a single word.\nUser prompt: water, area, river, park, miles, game\nPrompt 3: Intruder Detection Task. The intruder word\nin this topic is game.\nRating Task. Similar to the human experiment,\nwe retrieve the top 10 topic words and shuffle them.\nWe include a task and dataset description which\nleads to Prompt 4. The minimal prompt without\nthe dataset description is shown in Prompt 5.\nSystem prompt: You are a helpful assistant evaluating the top words of a topic\nmodel output for a given topic. Please rate how related the following words are\nto each other on a scale from 1 to 3 (\"1\" = not very related, \"2\" = moderately\nrelated, \"3\" = very related).\nThe topic modeling is based on the Wikipedia corpus. Wikipedia is an online\nencyclopedia covering a huge range of topics. Articles can include biographies\n(\"George Washington\"), scientific phenomena (\"Solar Eclipse\"), art pieces (\"La\nDanse\"), music (\"Amazing Grace\"), transportation (\"U.S. Route 131\"), sports\n(\"1952 winter olympics\"), historical events or periods (\"Tang Dynasty\"), media\nand pop culture (\"The Simpsons Movie\"), places (\"Yosemite National Park\"),\nplants and animals (\"koala\"), and warfare (\"USS Nevada (BB-36)\"), among\nothers. Reply with a single number, indicating the overall appropriateness of\nthe topic.\nUser prompt: lake, park, river, land, years, feet, ice, miles, water, area\nPrompt 4: Rating Task. Topic terms are shuffled.\nSystem prompt: You are a helpful assistant evaluating the top words of a topic\nmodel output for a given topic. Please rate how related the following words are\nto each other on a scale from 1 to 3 (\"1\" = not very related, \"2\" = moderately\nrelated, \"3\" = very related).\nReply with a single number, indicating the overall appropriateness of the topic.\nUser prompt: lake, park, river, land, years, feet, ice, miles, water, area\nPrompt 5: Rating Task without dataset description.\nTopic terms are shuffled.\nB Additional: Topic Model Outputs\nMinimal prompt. Even without the dataset de-\nscription in the prompt, the results remain similar.\nAll human ratings. In our main results, we dis-\ncard human annotations with low annotator con-\nfidence in the rating. We now consider all rat-\nings, even the non-confident ones. The results are\nslightly better than with the filtering.\nDifferent LLM. We also evaluate both tasks\nwith FLAN-T5 XL (Chung et al., 2022), which is\ninstruction-finetuned across a range of tasks. This\nmodel performs well in zero-shot setting, and com-\npares to recent state-of-the-art (Chia et al., 2023).\nAlthough it does not reach ChatGPT, the corre-\nlation with human annotators are all statistically\n9354\nTask Dataset NPMI Cv LLM (main) LLM (min.) LLM (all ann.) FLAN-T5 Ceiling\nIntrusion\nNYT 0.43 0.45 0.37 0.41 0.39 0.37 0.67\nWiki 0.39 0.34 0.35 0.27 0.36 0.18 0.60\nBoth 0.40 0.40 0.36 0.34 0.38 0.28 0.64\nRating\nNYT 0.48 0.40 0.64 0.64 0.65 0.31 0.72\nWiki 0.44 0.40 0.57 0.51 0.56 0.17 0.56\nBoth 0.44 0.42 0.59 0.57 0.61 0.25 0.65\nTable 2: Additional experiments reporting Spearman correlation between mean human scores and automated metrics.\nLLM (main) repeats our main results in Table 1 for reference. LLM (min.) – results using a minimal prompt\nwithout dataset descriptions. LLM (all ann) – no discarding low-confidence annotations. FLAN-T5 – FLAN-T5\nXL instead of ChatGPT. All numbers are the average result of 1000 bootstrapping episodes – re-sampling human\nannotations and LLM scores. Ceiling shows batched inter-annotator agreement.\nsignificant. For the NYT and concatenated exper-\niments, the resulting correlation are statistically\nindistinguishable from the best reported automated\nmetrics NPMI and Cv in (Hoyle et al., 2021).\nWe also ran our experiments with Alpaca-7B and\nFalcon-7B, with largely negative results.\nC Alternative Clustering Metrics\nIn our main results, we show correlations between\nLLM scores and the adjusted Rand Index, ARI,\nwhich measures the overlap between ground-truth\nclustering and topic model assignments. There are\nother cluster metrics, such as Adjusted Mutual In-\nformation, AMI (Vinh et al., 2010), completeness,\nor homogeneity. In Table 3, we show Spearman\ncorrelation statistics for these metrics. Our corre-\nlations are robust to the choice of metric used to\nmeasure the fit between the topic model assignment\nand the ground-truths in our case studies.\nD Definitions\nSee Bouma (2009) for justification of the NPMI for-\nmula. p(wi) and p(wi,wj) are unigram and joint\nprobabilities, respectively.\nNPMI(wi,wj)= PMI(wi,wj)\n- log p(wi,wj)=\nlog p(wi,wj )\np(wi)p(wj )\n- log p(wi,wj)\nThe Cv metric (Röder et al., 2015) is a more\ncomplex and includes, among others, the combina-\ntion of NPMI and cosine similarity for top words.\nDataset Topics ARI AMI Compl. Homog.\nBills Words Broad 0.61 0.74 0.63 -0.58\nWiki Words Broad -0.38 -0.38 -0.38 0.38\nWiki Words Specific 0.03 -0.24 -0.19 0.17\nBills Docs Broad 0.59 0.36 0.57 -0.58\nWiki Docs Broad 0.72 0.72 0.72 -0.70\nWiki Docs Specific 0.72 0.66 0.20 -0.20\nTable 3: Spearman correlation coefficients between our\nlanguage-model based scores and various popular met-\nrics for assessing the overlap between the topic model\nassignment and the underlying ground-truth. Compl. =\nCompleteness, Homog. = Homogenity.\nE Optimal Number of Topics Prompts\nWe now show the prompts for the optimal number\nof topics. We incorporate research questions in\ntwo ways: (1) we specify whether we are looking\nfor broad or narrow topics, and (2) we prompt 5\nexample categories. We believe this is a realistic\noperationalization. If our goal is a reasonable parti-\ntioning of a collection, we usually have some priors\nabout what categories we want the collection to be\npartitioned into.\nPrompt 6 shows the prompt for rating Tws by\nmodels run with different numbers of topics. The\ntask description and user prompt is identical to the\nprompt used in our prior experiments, displayed\nin e.g., Prompt 4. However, the dataset descrip-\ntion is different and allows for some variation. In\nPrompt 7, we show the prompt for automatically\nassigning labels to a document from a Tdc. To au-\ntomatically find the optimal number of topics for a\ntopic model, we prompt an LLM to provide a con-\ncise label to a document from the topic document\ncollection, the most likely documents assigned by\na topic model to a topic (see Prompt 7).\n9355\nYou are a helpful assistant evaluating the top words of a topic model output for\na given topic. Please rate how related the following words are to each other on a\nscale from 1 to 3 (\"1\" = not very related, \"2\" = moderately related, \"3\" = very\nrelated). The topic modeling is based on a legislative Bill summary dataset. We\nare interested in coherent broad|narrow topics. Typical topics in the dataset\ninclude \"topic 1\", \"topic 2\", \"topic 3\", \"topic 4\" and \"topic 5\". Reply with a\nsingle number, indicating the overall appropriateness of the topic.\nUser prompt: lake, park, river, land, years, feet, ice, miles, water, area\nPrompt 6: Rating Task without dataset description.\nTopic terms are shuffled. We apply this prompt to 2\ndifferent datasets and 2 different research goals (broad\nand narrow topics), and would set this part of the prompt\naccordingly. Also, we set as topic 1 to topic 5 the 5 most\nprevalent ground-truth labels from a dataset.\nSystem prompt: You are a helpful research assistant with lots of knowledge\nabout topic models. You are given a document assigned to a topic by a topic\nmodel. Annotate the document with a broad|narrow label, for example \"topic\n1\", \"topic 2\", \"topic 3\", \"topic 4\" and \"topic 5\".\nReply with a single word or phrase, indicating the label of the document. User\nprompt: National Black Clergy for the Elimination of HIV/AIDS Act of 2011\n- Authorizes the Director of the Office of Minority Health of the Department\nof Health and Human Services (HHS) to make grants to public health agencies\nand faith-based organizations to conduct HIV/AIDS prevention, testing, and\nrelated outreach activities ...\nPrompt 7: Assigning a label to a document belonging\nto the top document collection of a topic. The label pro-\nvided in this example is health. We apply this prompt\nto 2 different datasets and 2 different research goals\n(broad and narrow topics), and would set this part of the\nprompt accordingly. Also, we set as topic 1 to topic 5\nthe 5 most prevalent ground-truth labels from a dataset.\nF Additional: Document Labeling\nIn our study, we automatically label the top 10 doc-\numents for five randomly sampled topics. The ARI\nbetween-topic model partitioning and ground-truth\nlabels correlates if we were to only examine these\ntop 10 documents or all documents in the collec-\ntion. The correlation between these two in the Bills\ndataset is 0.96, indicating that analyzing only the\ntop 10 documents in a topic is a decent proxy for\nthe whole collection.\nNext, we evaluate the LLM-based label assigne-\nment to a document. Our documents are usually\nlong, up to 2000 words. We only consider the\nfirst 50 words in a document as input to the LLM.\nFor Wikipedia, this is reasonable, because the first\n2-3 sentences define the article and give a good\nsummary of the topic of an article. For Bills, we\nmanually confirm that the topic of an article is in-\ntroduced at the beginning of a document.\nHuman evaluation. From each case study, we\nrandomly sample 60 documents and assigned labels\n(3 examples for each of the twenty topic models),\nresulting in 180 examples in total. We then evalu-\nate whether the assigned label reasonably captures\nthe document content given the specification in the\ninput prompt (e.g., a broad label such as health\nor defense, or a narrow label such as warships of\ngermany or tropical cyclones: atlantic. Recall that\nthe prompted labels correspond to the five most\nprevalent ground-truth categories of the ground-\ntruth annotation. We find that the assigned label\nmakes sense in 93.9% of examined labels. In the 11\nerrors spotted, the assigned label does not meet the\ngranularity in 6 cases, is no adequate description\nof the document in 3 cases, and is a summary of\nthe document instead of a label in 2 cases.\nAutomated Metrics. Given that we have ground-\ntruth labels for each document, we can compute\ncluster metrics between the assigned labels by the\nLLM and the ground-truth labels (see Table 4).\nThese values refer to comparing all labels assigned\nduring our case study to their ground-truth label\n(1000 assigned datapoints per dataset).\nDataset Ground-Truth Labels ARI AMI\nBills Broad 19 43\nWiki Broad 52 57\nWiki Narrow 49 34\nTable 4: Accuracy of the label assignment task. We\nfind that the assigned labels clustering overlaps with the\nground-truth labels.\nOn average, we assign 10 times as many unique\nlabels to documents than there are ground-truth\nlabels (we assign 172 different labels in the Bills\ndataset, 348 labels in the broad Wikitext dataset and\n515 labels in the narrow Wikitext dataset). Nev-\nertheless, the automated metrics indicate a decent\noverlap between ground-truth and assigned labels.\nThus, the LLM often assigns the same label to doc-\numents with the same ground-truth label.\nG Qualitative Results\nIn this section, we show qualitative results of our\nautomated investigation of numbers of topics. In\nTable 5, we show three randomly sampled topics\nfrom the preferred topic model in our experiments.\nWe contrast these with three randomly sampled top-\nics from the topic model configuration which our\nprocedures indicate as least suitable.\nIn Table 6, we show true labels and LLM-\nassigned labels for three randomly sampled topics\nfrom the preferred topic model, contrasting it with\ntrue and LLM-assigned labels from topics in the\nleast suitable configuration. We find that indeed,\nthe assigned labels and the ground-truth label often\nmatch – and that the purity of the LLM-assigned\nlabels reflects the purity of the ground-truth label.\n9356\nBills (broad categories)\nMost\nsuitable\n- veterans, secretary, veteran, assistance, service, disability, benefits, educational, compensation, veterans_affairs (3)\n- land, forest, management, lands, act, usda, projects, secretary, restoration, federal (3)\n- mental, health, services, treatment, abuse, programs, substance, grants, prevention, program (3)\nLeast\nsuitable\n- gas, secretary, lease, oil, leasing, act, way, federal, production, environmental (2)\n- covered, criminal, history, act, restitution, child, background, amends, checks, victim (2)\n- information, beneficial, value, study, ownership, united_states, act, area, secretary, new_york (1)\nWikitext (broad categories)\nMost\nsuitable\n- episode, star, trek, enterprise, series, season, crew, generation, ship, episodes (3)\n- series, episodes, season, episode, television, cast, production, second, viewers, pilot (3)\n- car, vehicle, vehicles, engine, model, models, production, cars, design, rear (3)\nLeast\nsuitable\n- episode, series, doctor, season, character, time, star, story, trek, set (2)\n- stage, tour, ride, park, concert, dance, train, coaster, new, roller (1)\n- said, like, character, time, life, love, relationship, later, people, way (1)\nWikitext (specific categories)\nMost\nsuitable\n- episode, star, trek, enterprise, series, season, crew, generation, ship, episodes (3)\n- car, vehicle, vehicles, engine, model, models, production, cars, design, rear (3)\n- world, record, meter, time, won, freestyle, gold, championships, relay, seconds (2)\nLeast\nsuitable\n- fossil, fossils, found, specimens, years, evolution, modern, million, eddie, like (2)\n- match, event, impact, joe, team, angle, episode, styles, championship, tag (1)\n- brown, rihanna, usher, love, girl, loud, yeah, wrote, bow, bad (1)\nTable 5: Most and least suitable topics according to our LLM-based assessment on different datasets and use cases.\nIn brackets the LLM rating for the coherence of this topic.\nBills Wikitext (broad) Wikitext (specific)\nLLM-label True label LLM-label True label LLM-label True labelMost suitable\nhealth Health amusement\npark ride\nRecreation politician Historical figures: politicians\nelder abuse\nprevention\nSocial Wel-\nfare\namusement\npark ride\nRecreation politician Historical figures: politicians\nhealth Health amusement\npark ride\nRecreation american\ncivil war\nHistorical figures: politicians\nhealth Health amusement\npark ride\nRecreation lawyer and\npolitician\nHistorical figures: other\nhealth Health amusement\npark ride\nRecreation historical\nnewspaper\nJournalism and newspapers\nLeast suitable\npublic land Public Lands warship and\nnaval unit\nArmies and mil-\nitary units\nclassical\ngreek poetry\nPoetry\npublic land Public Lands warship and\nnaval unit\nArmies and mil-\nitary units\nhinduism Religious doctrines, teachings,\ntexts, events, and symbols\npublic land Environment warship and\nnaval unit\nMilitary people hinduism Religious doctrines, teachings,\ntexts, events, and symbols\nindigenous\naffair\nGovernment\nOperations\nwarship and\nnaval unit\nMilitary people philosophy Philosophical doctrines, teach-\nings, texts, events, and symbols\nindigenous\naffair\nGovernment\nOperations\nwar poetry Language and\nliterature\nphilosophy Philosophical doctrines, teach-\nings, texts, events, and symbols\nTable 6: Assigned LLM labels and ground-truth labels for a given topic from the most and the least suitable cluster\nconfiguration according to our algorithm. The purity is higher in the most suitable configuration for LLM labels and\nground-truth labels.\n9357",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6948500871658325
    },
    {
      "name": "Task (project management)",
      "score": 0.5438727140426636
    },
    {
      "name": "Language model",
      "score": 0.5008273124694824
    },
    {
      "name": "Coherence (philosophical gambling strategy)",
      "score": 0.48101645708084106
    },
    {
      "name": "Word (group theory)",
      "score": 0.46977394819259644
    },
    {
      "name": "Data science",
      "score": 0.4333919286727905
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4010326862335205
    },
    {
      "name": "Natural language processing",
      "score": 0.39308831095695496
    },
    {
      "name": "Machine learning",
      "score": 0.33837682008743286
    },
    {
      "name": "Linguistics",
      "score": 0.09570634365081787
    },
    {
      "name": "Engineering",
      "score": 0.07970067858695984
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    }
  ],
  "institutions": []
}