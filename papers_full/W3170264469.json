{
  "title": "Associating Objects with Transformers for Video Object Segmentation",
  "url": "https://openalex.org/W3170264469",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2366916162",
      "name": "Yang, Zongxin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2390531409",
      "name": "Wei, Yunchao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1979858332",
      "name": "Yang Yi",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2986050084",
    "https://openalex.org/W3106773277",
    "https://openalex.org/W2009874829",
    "https://openalex.org/W2916797271",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W575921498",
    "https://openalex.org/W3043563708",
    "https://openalex.org/W3110030584",
    "https://openalex.org/W2564998703",
    "https://openalex.org/W3110159886",
    "https://openalex.org/W2011953904",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2291627510",
    "https://openalex.org/W2331143823",
    "https://openalex.org/W3034499084",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2037954058",
    "https://openalex.org/W2963163009",
    "https://openalex.org/W2890447039",
    "https://openalex.org/W2964157492",
    "https://openalex.org/W2462831000",
    "https://openalex.org/W2964218467",
    "https://openalex.org/W2964211168",
    "https://openalex.org/W2062385571",
    "https://openalex.org/W2938619957",
    "https://openalex.org/W3179869055",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W2990205821",
    "https://openalex.org/W3012337735",
    "https://openalex.org/W3117097536",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2916743882",
    "https://openalex.org/W2144794286",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2294182682",
    "https://openalex.org/W2798441772",
    "https://openalex.org/W2963253279",
    "https://openalex.org/W2565639579",
    "https://openalex.org/W2982723417",
    "https://openalex.org/W2899771611",
    "https://openalex.org/W2967890649",
    "https://openalex.org/W3034538699",
    "https://openalex.org/W3034263000",
    "https://openalex.org/W2962825871",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3037932933",
    "https://openalex.org/W2963503215",
    "https://openalex.org/W2950739196",
    "https://openalex.org/W2795783309",
    "https://openalex.org/W3122411985",
    "https://openalex.org/W2724418412",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W3171516518",
    "https://openalex.org/W2991213871",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2086161653",
    "https://openalex.org/W2949117887",
    "https://openalex.org/W2031489346",
    "https://openalex.org/W2889658408",
    "https://openalex.org/W2470139095",
    "https://openalex.org/W1560354729"
  ],
  "abstract": "This paper investigates how to realize better and more efficient embedding learning to tackle the semi-supervised video object segmentation under challenging multi-object scenarios. The state-of-the-art methods learn to decode features with a single positive object and thus have to match and segment each target separately under multi-object scenarios, consuming multiple times computing resources. To solve the problem, we propose an Associating Objects with Transformers (AOT) approach to match and decode multiple objects uniformly. In detail, AOT employs an identification mechanism to associate multiple targets into the same high-dimensional embedding space. Thus, we can simultaneously process multiple objects' matching and segmentation decoding as efficiently as processing a single object. For sufficiently modeling multi-object association, a Long Short-Term Transformer is designed for constructing hierarchical matching and propagation. We conduct extensive experiments on both multi-object and single-object benchmarks to examine AOT variant networks with different complexities. Particularly, our R50-AOT-L outperforms all the state-of-the-art competitors on three popular benchmarks, i.e., YouTube-VOS (84.1% J&amp;F), DAVIS 2017 (84.9%), and DAVIS 2016 (91.1%), while keeping more than $3\\times$ faster multi-object run-time. Meanwhile, our AOT-T can maintain real-time multi-object speed on the above benchmarks. Based on AOT, we ranked 1st in the 3rd Large-scale VOS Challenge.",
  "full_text": "Associating Objects with Transformers for\nVideo Object Segmentation\nZongxin Yang1,2, Yunchao Wei3,4, Yi Yang1\n1 CCAI, College of Computer Science and Technology, Zhejiang University 2 Baidu Research\n3 Institute of Information Science, Beijing Jiaotong University\n4 Beijing Key Laboratory of Advanced Information Science and Network\n{zongxinyang1996, wychao1987, yee.i.yang}@gmail.com\nAbstract\nThis paper investigates how to realize better and more efÔ¨Åcient embedding learn-\ning to tackle the semi-supervised video object segmentation under challenging\nmulti-object scenarios. The state-of-the-art methods learn to decode features with\na single positive object and thus have to match and segment each target separately\nunder multi-object scenarios, consuming multiple times computing resources. To\nsolve the problem, we propose an Associating Objects with Transformers (AOT)\napproach to match and decode multiple objects uniformly. In detail, AOT em-\nploys an identiÔ¨Åcation mechanism to associate multiple targets into the same\nhigh-dimensional embedding space. Thus, we can simultaneously process multiple\nobjects‚Äô matching and segmentation decoding as efÔ¨Åciently as processing a single\nobject. For sufÔ¨Åciently modeling multi-object association, a Long Short-Term\nTransformer is designed for constructing hierarchical matching and propagation.\nWe conduct extensive experiments on both multi-object and single-object bench-\nmarks to examine AOT variant networks with different complexities. Particularly,\nour R50-AOT-L outperforms all the state-of-the-art competitors on three popu-\nlar benchmarks, i.e., YouTube-VOS (84.1% J&F), DA VIS 2017 (84.9%), and\nDA VIS 2016 (91.1%), while keeping more than3√ófaster multi-object run-time.\nMeanwhile, our AOT-T can maintain real-time multi-object speed on the above\nbenchmarks. Based on AOT, we ranked1st in the 3rd Large-scale VOS Challenge.\n1 Introduction\nVideo Object Segmentation (VOS) is a fundamental task in video understanding with many potential\napplications, including augmented reality [34] and self-driving cars [69]. The goal of semi-supervised\nVOS, the main task in this paper, is to track and segment object(s) across an entire video sequence\nbased on the object mask(s) given at the Ô¨Årst frame.\nThanks to the recent advance of deep neural networks, many deep learning based VOS algorithms\nhave been proposed recently and achieved promising performance. STM [ 37] and its following\nworks [46, 30] leverage a memory network to store and read the target features of predicted past frames\nand apply a non-local attention mechanism to match the target in the current frame. FEELVOS [53]\nand CFBI [67, 68] utilize global and local matching mechanisms to match target pixels or patches\nfrom both the Ô¨Årst and the previous frames to the current frame.\nEven though the above methods have achieved signiÔ¨Åcant progress, the above methods learn to decode\nscene features that contain a single positive object. Thus under a multi-object scenario, they have to\nmatch each object independently and ensemble all the single-object predictions into a multi-object\nsegmentation, as shown in Fig. 1a. Such a post-ensemble manner eases network architectures‚Äô design\n35th Conference on Neural Information Processing Systems (NeurIPS 2021).\narXiv:2106.02638v3  [cs.CV]  30 Oct 2021\nVOS NetworkVOS Network\nVOS Network\nReference\nCurrent\nSeparation\nEnsemble\nPrediction\nMulti-object Single-objectMulti-object\nSingle-object\n(a) Post-ensemble\nAOT\nPrediction\nMulti-object\nReference\nMulti-object\nCurrent (b) Associating objects (ours)\nObjectnumber1 2 3 4\n1x\n2x\n3x\n4xComputational complexityAOT (ours)\nPost-ensemble (c) Comparison\nFigure 1: VOS methods, e.g., [67, 46], process multi-object scenarios in a post-ensemble manner (a).\nIn contrast, our AOT associates all the objects uniformly (b), leading to better efÔ¨Åciency (c).\nsince the networks are not required to adapt the parameters or structures for different object numbers.\nHowever, modeling multiple objects independently, instead of uniformly, is inefÔ¨Åcient in exploring\nmulti-object contextual information to learn a more robust feature representation for VOS. In addition,\nprocessing multiple objects separately yet in parallel requires multiple times the amount of GPU\nmemory and computation for processing a single object. This problem restricts the training and\napplication of VOS under multi-object scenarios, especially when computing resources are limited.\nTo solve the problem, Fig. 1b demonstrates a feasible approach to associate and decode multiple\nobjects uniformly in an end-to-end framework. Hence, we propose an Associating Objects with\nTransformers (AOT) approach to match and decode multiple targets uniformly. First, an identiÔ¨Åcation\nmechanism is proposed to assign each target a unique identity and embed multiple targets into\nthe same feature space. Hence, the network can learn the association or correlation among all the\ntargets. Moreover, the multi-object segmentation can be directly decoded by utilizing assigned\nidentity information. Second, a Long Short-Term Transformer (LSTT) is designed for constructing\nhierarchical object matching and propagation. Each LSTT block utilizes a long-term attention for\nmatching with the Ô¨Årst frame‚Äôs embedding and a short-term attention for matching with several nearby\nframes‚Äô embeddings. Compared to the methods [37, 46] utilizing only one attention layer, we found\nhierarchical attention structures are more effective in associating multiple objects.\nWe conduct extensive experiments on two popular multi-object benchmarks for VOS,i.e., YouTube-\nVOS [63] and DA VIS 2017 [43], to validate the effectiveness and efÔ¨Åciency of the proposed AOT.\nEven using the light-weight Mobilenet-V2 [45] as the backbone encoder, the AOT variant networks\nachieve superior performance on the validation 2018 & 2019 splits of the large-scale YouTube-VOS\n(ours, J&F82.6‚àº84.5% & 82.2‚àº84.5%) while keeping more than 2√ófaster multi-object run-time\n(27.1‚àº9.3FPS) compared to the state-of-the-art competitors ( e.g., CFBI [ 67], 81.4% & 81.0%,\n3.4FPS). We also achieve new state-of-the-art performance on both the DA VIS-2017 validation\n(85.4%) and testing ( 81.2%) splits. Moreover, AOT is effective under single-object scenarios as\nwell and outperforms previous methods on DA VIS 2016 [ 41] (92.0%), a popular single-object\nbenchmark. Besides, our smallest variant, AOT-T, can maintain real-time multi-object speed on all\nabove benchmarks (51.4FPS on 480p videos). Particularly, AOT ranked1st in the Track 1 (Video\nObject Segmentation) of the 3rd Large-scale Video Object Segmentation Challenge.\nOverall, our contributions are summarized as follows:\n‚Ä¢ We propose an identiÔ¨Åcation mechanism to associate and decode multiple targets uniformly for\nVOS. For the Ô¨Årst time, multi-object training and inference can be efÔ¨Åcient as single-object ones, as\ndemonstrated in Fig. 1c.\n‚Ä¢ Based on the identiÔ¨Åcation mechanism, we design a new efÔ¨Åcient VOS framework,i.e., Long Short-\nTerm Transformer (LSTT), for constructing hierarchical multi-object matching and propagation.\nLSTT achieves superior performance on VOS benchmarks [63, 43, 41] while maintaining better\nefÔ¨Åciency than previous state-of-the-art methods. To the best of our knowledge, LSTT is the Ô¨Årst\nhierarchical framework for object matching and propagation by applying transformers [51] to VOS.\n2 Related Work\nSemi-supervised Video Object Segmentation. Given one or more annotated frames (the Ô¨Årst\nframe in general), semi-supervised VOS methods propagate the manual labeling to the entire video\n2\nsequence. Traditional methods often solve an optimization problem with an energy deÔ¨Åned over a\ngraph structure [4, 52, 2]. In recent years, VOS methods have been mainly developed based on deep\nneural networks (DNN), leading to better results.\nEarly DNN methods rely on Ô¨Åne-tuning the networks at test time to make segmentation networks\nfocus on a speciÔ¨Åc object. Among them, OSVOS [8] and MoNet [62] Ô¨Åne-tune pre-trained networks\non the Ô¨Årst-frame ground-truth at test time. OnA VOS [ 54] extends the Ô¨Årst-frame Ô¨Åne-tuning by\nintroducing an online adaptation mechanism. Following these approaches, MaskTrack [ 40] and\nPReM [31] utilize optical Ô¨Çow to help propagate the segmentation mask from one frame to the next.\nDespite achieving promising results, the test-time Ô¨Åne-tuning restricts the network efÔ¨Åciency.\nRecent works aim to achieve a better run-time and avoid using online Ô¨Åne-tuning. OSMN [ 66]\nemploys one convolutional network to extract object embedding and another one to guide segmen-\ntation predictions. PML [ 11] learns pixel-wise embedding with a nearest neighbor classiÔ¨Åer, and\nVideoMatch [20] uses a soft matching layer that maps the pixels of the current frame to the Ô¨Årst frame\nin a learned embedding space. Following PML and VideoMatch, FEELVOS [53] and CFBI [67, 68]\nextend the pixel-level matching mechanism by additionally matching between the current frame and\nthe previous frame. RGMP [61] also gathers guidance information from both the Ô¨Årst frame and the\nprevious frame but uses a siamese encoder with two shared streams. STM [ 37] and its following\nworks (e.g., EGMN [30] and KMN [46]) leverage a memory network to embed past-frame predictions\ninto memory and apply a non-local attention mechanism on the memory to decode the segmentation\nof the current frame. SST [ 15] utilizes attention mechanisms in a different way, i.e., transformer\nblocks [51] are used to extract pixel-level afÔ¨Ånity maps and spatial-temporal features. The features\nare target-agnostic, instead of target-aware like our LSTT, since the mask information in past frames\nis not propagated and aggregated in the blocks. Instead of using matching mechanisms, LWL [ 7]\nproposes to use an online few-shot learner to learn to decode object segmentation.\nThe above methods learn to decode features with a single positive object and thus have to match and\nsegment each target separately under multi-object scenarios, consuming multiple times computing\nresources of single-object cases. The problem restricts the application and development of the VOS\nwith multiple targets. Hence, we propose our AOT to associate and decode multiple targets uniformly\nand simultaneously, as efÔ¨Åciently as processing a single object.\nVisual Transformers. Transformers [51] was proposed to build hierarchical attention-based net-\nworks for machine translation. Similar to Non-local Neural Networks [ 56], transformer blocks\ncompute correlation with all the input elements and aggregate their information by using attention\nmechanisms [5]. Compared to RNNs, transformer networks model global correlation or attention in\nparallel, leading to better memory efÔ¨Åciency, and thus have been widely used in natural language\nprocessing (NLP) tasks [13, 44, 49]. Recently, transformer blocks were introduced to many computer\nvision tasks, such as image classiÔ¨Åcation [14, 50, 28], object detection [9]/segmentation [58], and\nimage generation [38], and have shown promising performance compared to CNN-based networks.\nMany VOS methods [25, 37, 30, 46] have utilized attention mechanisms to match the object features\nand propagate the segmentation mask from past frames to the current frames. Nevertheless, these\nmethods consider only one positive target in the attention processes, and how to build hierarchical\nattention-based propagation has been rarely studied. In this paper, we carefully design a long short-\nterm transformer block, which can effectively construct multi-object matching and propagation within\nhierarchical structures for VOS.\n3 Revisit Previous Solutions for Video Object Segmentation\nIn VOS, many common video scenarios have multiple targets or objects required for tracking and\nsegmenting. BeneÔ¨Åt from deep networks, current state-of-the-art VOS methods [37, 67] have achieved\npromising performance. Nevertheless, these methods focus on matching and decoding a single object.\nUnder a multi-object scenario, they thus have to match each object independently and ensemble all the\nsingle-object predictions into a multi-object prediction, as demonstrated in Fig. 1a. Let FN denotes\na VOS network for predicting single-object segmentation, and Ais an ensemble function such as\nsoftmax or the soft aggregation [37], the formula of such a post-ensemble manner for processing N\nobjects is like,\nY‚Ä≤= A(FN(It,Im,Y m\n1 ),...,F N(It,Im,Y m\nN )), (1)\n3\nLSTTL√ó LSTTLSTTDecoder\nEncoderEncoderEncoder\nDecoder\nID\n+\n +\nID\nGiven Prediction\nFrame1 Frame t-1Framet\n(a) Overview\nIdentity Bank\n‚Ä¶‚Ä¶M√óvector\nN-objectMask\nAssignNidentities(N<M)\nIdentificationEmbedding\nùêª√óùëä√óùëÅ\nùêª√óùëä√óùê∂\nùê∂ (b) Identity assignment\nLNSelf-Attention+\n+Long-term AttentionShort-term Attention\nLNFeed Forward+\nùëã!ùíèùëå!ùíèùëã!ùíéùëå!ùíé ùëã!$\nLN\n (c) l-th LSTT block\nFigure 2: (a) The overview of our Associating Objects with Transformers (AOT). The multi-object\nmasks are embedded by using our IdentiÔ¨Åcation mechanism. Moreover, a L-layer Long Short-\nTerm Transformer is responsible for matching multiple objects uniformly and hierarchically. (b)\nAn illustration of the IDentity assignment (ID) designed for transferring a N-object mask into an\nidentiÔ¨Åcation embedding. (c) The structure of an LSTT block. LN: layer normalization [3].\nwhere It and Im denote the image of the current frame and memory frames respectively, and\n{Ym\n1 ,...,Y m\nN }are the memory masks (containing the given reference mask and past predicted\nmasks) of all the N objects. This manner extends networks designed for single-object VOS into\nmulti-object applications, so there is no need to adapt the network for different object numbers.\nAlthough the above post-ensemble manner is prevalent and straightforward in the VOS Ô¨Åeld, process-\ning multiple objects separately yet in parallel requires multiple times the amount of GPU memory and\ncomputation for matching a single object and decoding the segmentation. This problem restricts the\ntraining and application of VOS under multi-object scenarios when computing resources are limited.\nTo make the multi-object training and inference as efÔ¨Åcient as single-object ones, an expected solution\nshould be capable of associating and decoding multiple objects uniformly instead of individually. To\nachieve such an objective, we propose an identiÔ¨Åcation mechanism to embed the masks of any number\n(required to be smaller than a pre-deÔ¨Åned large number) of targets into the same high-dimensional\nspace. Based on the identiÔ¨Åcation mechanism, a novel and efÔ¨Åcient framework, i.e., Associating\nObjects with Transformers (AOT), is designed for propagating all the object embeddings uniformly\nand hierarchically, from memory frames to the current frame.\nAs shown in Fig. 1b, our AOT associates and segments multiple objects within an end-to-end\nframework. For the Ô¨Årst time, processing multiple objects can be as efÔ¨Åcient as processing a single\nobject (Fig. 1c). Compared to previous methods, our training under multi-object scenarios is also more\nefÔ¨Åcient since AOT can associate multiple object regions and learn contrastive feature embeddings\namong them uniformly.\n4 Associating Objects with Transformers\nIn this section, we introduce our identiÔ¨Åcation mechanism proposed for efÔ¨Åcient multi-object VOS.\nThen, we design a new VOS framework, i.e., long short-term transformer, based on the identiÔ¨Åcation\nmechanism for constructing hierarchical multi-object matching and propagation.\n4.1 IdentiÔ¨Åcation Mechanism for Multi-object Association\nMany recent VOS methods [37, 30, 46] utilized attention mechanisms and achieved promising results.\nTo formulate, we deÔ¨Åne Q‚ààRHW √óC, K ‚ààRTHW √óC, and V ‚ààRTHW √óC as the query embedding\nof the current frame, the key embedding of the memory frames, and the value embedding of the\nmemory frames respectively, where T, H, W, C denote the temporal, height, width, and channel\ndimensions. The formula of a common attention-based matching and propagation is,\nAtt(Q,K,V ) =Corr(Q,K)V = softmax(QKtr\n‚àö\nC\n)V, (2)\n4\nwhere a matching map is calculated by the correlation function Corr, and then the value embedding,\nV, will be propagated into each location of the current frame.\nIn the common single-object propagation [37], the binary mask information in memory frames is\nembedded into V with an additional memory encoder network and thus can also be propagated to the\ncurrent frame by using Eq. 2. A convolutional decoder network following the propagated feature will\ndecode the aggregated feature and predict the single-object probability logit of the current frame.\nThe main problem of propagating and decoding multi-object mask information in an end-to-end\nnetwork is how to adapt the network to different target numbers. To overcome this problem, we\npropose an identiÔ¨Åcation mechanism consisting of identiÔ¨Åcation embedding and decoding based on\nattention mechanisms.\nFirst, an IdentiÔ¨Åcation Embedding mechanism is proposed to embed the masks of multiple different\ntargets into the same feature space for propagation. As seen in Fig. 2b, we initialize an identity\nbank, D ‚ààRM√óC, where M identiÔ¨Åcation vectors with C dimensions are stored. For embedding\nmultiple different target masks, each target will be randomly assigned a different identiÔ¨Åcation vector.\nAssuming N(N <M) targets are in the video scenery, the formula of embedding the targets‚Äô one-hot\nmask, Y ‚àà{0,1}THW √óN , into a identiÔ¨Åcation embedding, E ‚ààRTHW √óC, by randomly assigning\nidentiÔ¨Åcation vector from the bank Dis,\nE = ID(Y,D) =YPD, (3)\nwhere P ‚àà{0,1}N√óM is a random permutation matrix, satisfying that PtrP is equal to a M √óM\nunit matrix, for randomly selecting N identiÔ¨Åcation embeddings. After the ID assignment, different\ntarget has different identiÔ¨Åcation embedding, and thus we can propagate all the target identiÔ¨Åcation\ninformation from memory frames to the current frame by attaching the identiÔ¨Åcation embedding E\nwith the attention value V, i.e.,\nV‚Ä≤= AttID(Q,K,V,Y |D) =Att(Q,K,V + ID(Y,D)) =Att(Q,K,V + E), (4)\nwhere V‚Ä≤‚ààRHW √óC aggregates all the multiple targets‚Äô embeddings from the propagation.\nFor IdentiÔ¨Åcation Decoding, i.e., predicting all the targets‚Äô probabilities from the aggregated feature\nV‚Ä≤, we Ô¨Årstly predict the probability logit for every identity in the bank Dby employing a convolu-\ntional decoding network FD, and then select the assigned ones and calculate the probabilities, i.e.,\nY‚Ä≤= softmax(PFD(V‚Ä≤)) =softmax(PLD), (5)\nwhere LD ‚ààRHW √óM is all the M identities‚Äô probability logits,P is the same as the selecting matrix\nused in the identity assignment (Eq. 3), and Y‚Ä≤‚àà[0,1]HW √óN is the probability prediction of all the\nN targets.\nFor training, common multi-class segmentation losses, such as cross-entropy loss, can be used to\noptimize the multi-object Y‚Ä≤regarding the ground-truth labels. The identity bank Dis trainable and\nrandomly initialized at the training beginning. To ensure that all the identiÔ¨Åcation vectors have the\nsame opportunity to compete with each other, we randomly reinitialize the identiÔ¨Åcation selecting\nmatrix P in each video sample and each optimization iteration.\n4.2 Long Short-Term Transformer for Hierarchical Matching and Propagation\nPrevious methods [37, 46] always utilize only one layer of attention (Eq. 2) to aggregate single-\nobject information. In our identiÔ¨Åcation-based multi-object pipeline, we found that a single attention\nlayer cannot fully model multi-object association, which naturally should be more complicated than\nsingle-object processes. Thus, we consider constructing hierarchical matching and propagation\nby using a series of attention layers. Recently, transformer blocks [ 51] have been demonstrated\nto be stable and promising in constructing hierarchical attention structures in visual tasks [ 9, 14].\nBased on transformer blocks, we carefully design a Long Short-Term Transformer (LSTT) block for\nmulti-object VOS.\nFollowing the common transformer blocks [ 51, 13], LSTT Ô¨Årstly employs a self-attention layer,\nwhich is responsible for learning the association or correlation among the targets within the current\nframe. Then, LSTT additionally introduces a long-term attention, for aggregating targets‚Äô information\nfrom long-term memory frames and a short-term attention, for learning temporal smoothness from\nnearby short-term frames. The Ô¨Ånal module is based on a common 2-layer feed-forward MLP with\n5\nGELU [19] non-linearity in between. Fig. 2c shows the structure of an LSTT block. Notably, all\nthese attention modules are implemented in the form of the multi-head attention [51], i.e., multiple\nattention modules followed by concatenation and a linear projection. Nevertheless, we only introduce\ntheir single-head formulas below for the sake of simplicity.\nLong-Term Attention is responsible for aggregating targets‚Äô information from past memory frames,\nwhich contains the reference frame and stored predicted frames, to the current frame. Since the time\nintervals between the current frame and past frames are variable and can be long-term, the temporal\nsmoothness is difÔ¨Åcult to guarantee. Thus, the long-term attention employs non-local attention like\nEq. 2. Let Xt\nl ‚ààRHW √óC denotes the input feature embedding at time t and in block l, where\nl‚àà{1,...,L }is the block index of LSTT, the formula of the long-term attention is,\nAttLT(Xt\nl ,Xm\nl ,Y m) =AttID(Xt\nl WK\nl ,Xm\nl WK\nl ,Xm\nl WV\nl ,Y m|D), (6)\nwhere Xm\nl = Concat(Xm1\nl ,...,X mT\nl ) and Ym = Concat(Ym1 ,...,Y mT ) are the input feature\nembeddings and target masks of memory frames with indices m = {m1,...,m T }. Besides, WK\nl ‚àà\nRC√óCk and WV\nl ‚ààRC√óCv are trainable parameters of the space projections for matching and\npropagation, respectively. Instead of using different projections for Xt\nl and Xm\nl , we found the\ntraining of LSTT is more stable with a siamese-like matching, i.e., matching between the features\nwithin the same embedding space (l-th features with the same projection of WK\nl ).\nShort-Term Attentionis employed for aggregating information in a spatial-temporal neighbourhood\nfor each current-frame location. Intuitively, the image changes across several contiguous video frames\nare always smooth and continuous. Thus, the target matching and propagation in contiguous frames\ncan be restricted in a small spatial-temporal neighborhood, leading to better efÔ¨Åciency than non-local\nprocesses. Considering nneighbouring frames with indices n = {t‚àí1,...,t ‚àín}are in the spatial-\ntemporal neighbourhood, the features and masks of these frames areXn\nl = Concat(Xt‚àí1\nl ,...,X t‚àín\nl )\nand Yn = Concat(Yt‚àí1,...,Y t‚àín), and then the formula of the short-term attention at each spatial\nlocation pis,\nAttST(Xt\nl ,Xn\nl ,Y n|p) =AttLT(Xt\nl,p,Xn\nl,N(p),Y n\nl,N(p)), (7)\nwhere Xt\nl,p ‚ààR1√óC is the feature of Xt\nl at location p, N(p) is a Œª√óŒª spatial neighbourhood\ncentered at location p, and thus Xn\nl,N(p) and Yn\nl,N(p) are the features and masks of the spatial-temporal\nneighbourhood, respectively, with a shape of nŒª2 √óCor nŒª2 √óN.\nWhen extracting features of the Ô¨Årst frame t= 1, there is no memory frames or previous frames, and\nhence we use X1\nl to replace Xm\nl and Xn\nl . In other words, the long-term attention and the short-term\nattention are changed into self-attentions without adjusting the network structures and parameters.\n5 Implementation Details\nNetwork Details: For sufÔ¨Åciently validating the effectiveness of our identiÔ¨Åcation mechanism and\nLSTT, we mainly use light-weight backbone encoder, MobileNet-V2 [45], and decoder, FPN [26]\nwith Group Normalization [60]. The spatial neighborhood size Œªis set to 15, and the number of\nidentiÔ¨Åcation vectors, M, is set to 10, which is consistent with the maximum object number in the\nbenchmarks [63, 43]. AOT performs well with PaddlePaddle [1] and PyTorch [39]. More details can\nbe found in the supplementary material.\nArchitecture Variants: We build several AOT variant networks with different LSTT layer number\nLor long-term memory size m. The hyper-parameters of these variants are: (1) AOT-Tiny: L= 1,\nm = {1}; (2) AOT-Small: L = 2, m = {1}; (3) AOT-Base: L = 3, m = {1}; (4) AOT-Large:\nL= 3, m = {1,1 +Œ¥,1 + 2Œ¥,1 + 3Œ¥,...}. In the experiments, we also equip AOT-L with ResNet50\n(R50) [18] or Swin-B [28].\nAOT-S is a small model with only 2 layers of LSTT block. Compared to AOT-S, AOT-T utilizes\nonly 1 layer of LSTT, and AOT-B/L uses 3 layers. In AOT-T/S/B, only the Ô¨Årst frame is considered\ninto long-term memory, which is similar to [53, 67], leading to a smooth efÔ¨Åciency. In AOT-L, the\npredicted frames are stored into long-term memory per Œ¥ frames, following the memory reading\nstrategy [37]. We set Œ¥to 2/5 for training/testing.\nTraining Details: Following [61, 37, 30, 46], the training stage is divided into two phases: (1)\npre-training on sythetic video sequence generated from static image datasets [16, 27, 12, 48, 17] by\n6\nTable 1: The quantitative evaluation on multi-object benchmarks, YouTube-VOS [63] and DA VIS\n2017 [43]. Y: using YouTube-VOS for training. ‚àó: using 600p instead of 480p videos in inference. ‚Ä°:\ntiming extrapolated from single-object speed assuming linear scaling in the number of objects.\n(a) YouTube-VOS\nSeen Unseen\nMethods J &F J F J F FPS\nValidation 2018 Split\nSTM[ICCV19] [37] 79.4 79.7 84.2 72.8 80.9 -\nKMN[ECCV20] [46] 81.4 81.4 85.6 75.3 83.3 -\nCFBI[ECCV20] [67] 81.4 81.1 85.8 75.3 83.4 3.4\nLWL[ECCV20] [7] 81.5 80.4 84.9 76.4 84.4 -\nSST[CVPR21] [15] 81.7 81.2 - 76.0 - -\nCFBI+[TPAMI21] [68] 82.8 81.8 86.6 77.1 85.6 4.0\nAOT-T 80.2 80.1 84.5 74.0 82.2 41.0\nAOT-S 82.6 82.0 86.7 76.6 85.0 27.1\nAOT-B 83.5 82.6 87.5 77.7 86.0 20.5\nAOT-L 83.8 82.9 87.9 77.7 86.5 16.0\nR50-AOT-L 84.1 83.7 88.5 78.1 86.1 14.9\nSwinB-AOT-L 84.5 84.3 89.3 77.9 86.4 9.3\nValidation 2019 Split\nCFBI[ECCV20] [67] 81.0 80.6 85.1 75.2 83.0 3.4\nSST[CVPR21] [15] 81.8 80.9 - 76.6 - -\nCFBI+[TPAMI21] [68] 82.6 81.7 86.2 77.1 85.2 4.0\nAOT-T 79.7 79.6 83.8 73.7 81.8 41.0\nAOT-S 82.2 81.3 85.9 76.6 84.9 27.1\nAOT-B 83.3 82.4 87.1 77.8 86.0 20.5\nAOT-L 83.7 82.8 87.5 78.0 86.7 16.0\nR50-AOT-L 84.1 83.5 88.1 78.4 86.3 14.9\nSwinB-AOT-L 84.5 84.0 88.8 78.4 86.7 9.3\n(b) DA VIS 2017\nMethods J &F J F FPS\nValidation 2017 Split\nCFBI [67] (Y) 81.9 79.3 84.5 5.9\nSST [15] (Y) 82.5 79.9 85.1 -\nKMN [46] 76.0 74.2 77.8 4.2 ‚Ä°\nKMN [46] (Y) 82.8 80.0 85.6 4.2 ‚Ä°\nCFBI+ [68] (Y) 82.9 80.1 85.7 5.6\nAOT-T (Y) 79.9 77.4 82.3 51.4\nAOT-S 79.2 76.4 82.0 40.0\nAOT-S (Y) 81.3 78.7 83.9 40.0\nAOT-B (Y) 82.5 79.7 85.2 29.6\nAOT-L (Y) 83.8 81.1 86.4 18.7\nR50-AOT-L (Y) 84.9 82.3 87.5 18.0\nSwinB-AOT-L (Y) 85.4 82.4 88.4 12.1\nTesting 2017 Split\nCFBI [67] (Y) 75.0 71.4 78.7 5.3\nCFBI‚àó[67] (Y) 76.6 73.0 80.1 2.9\nKMN‚àó[46] (Y) 77.2 74.1 80.3 -\nCFBI+‚àó[68] (Y) 78.0 74.4 81.6 3.4\nAOT-T (Y) 72.0 68.3 75.7 51.4\nAOT-S (Y) 73.9 70.3 77.5 40.0\nAOT-B (Y) 75.5 71.6 79.3 29.6\nAOT-L (Y) 78.3 74.3 82.3 18.7\nR50-AOT-L (Y) 79.6 75.9 83.3 18.0\nSwinB-AOT-L (Y) 81.2 77.3 85.1 12.1\nrandomly applying multiple image augmentations [61]. (2) main training on the VOS benchmarks [63,\n43] by randomly applying video augmentations [67]. More details are in the supplementary material.\n6 Experimental Results\nWe evaluate AOT on popular multi-object benchmarks, YouTube-VOS [63] and DA VIS 2017 [43],\nand single-object benchmark, DA VIS 2016 [41]. For YouTube-VOS experiments, we train our models\non the YouTube-VOS 2019 training split. For DA VIS, we train on the DA VIS-2017 training split.\nWhen evaluating YouTube-VOS, we use the default 6FPS videos, and all the videos are restricted to\nbe smaller than 1.3 √ó480presolution. As to DA VIS, the default 480p 24FPS videos are used.\nThe evaluation metric is the Jscore, calculated as the average Intersect over Union (IoU) score\nbetween the prediction and the ground truth mask, and theFscore, calculated as an average boundary\nsimilarity measure between the boundary of the prediction and the ground truth, and their mean value,\ndenoted as J&F. We evaluate all the results on ofÔ¨Åcial evaluation servers or with ofÔ¨Åcial tools.\n6.1 Compare with the State-of-the-art Methods\nYouTube-VOS[63] is the latest large-scale benchmark for multi-object video segmentation and is\nabout 37 times larger than DA VIS 2017 (120 videos). SpeciÔ¨Åcally, YouTube-VOS contains 3471\nvideos in the training split with 65 categories and 474/507 videos in the validation 2018/2019 split\nwith additional 26 unseen categories. The unseen categories do not exist in the training split to\nevaluate algorithms‚Äô generalization ability.\nAs shown in Table 1a, AOT variants achieve superior performance on YouTube-VOS compared to\nthe previous state-of-the-art methods. With our identiÔ¨Åcation mechanism, AOT-S (82.6% J&F)\n7\ntime timeDAVIS 2017 YouTube-VOS\nCFBIAOT(ours)\nFailure cases\nFigure 3: Qualitative results. (top) Compared with CFBI [67], AOT performs better when segmenting\nmultiple highly similar objects ( carousels and zebras). (bottom) AOT fails to segment some tiny\nobjects (ski polesand watch) since AOT has no speciÔ¨Åc design for processing rare tiny objects.\nis comparable with CFBI+ [68] (82.8%) while running about 7√ófaster (27.1 vs 4.0FPS). By using\nmore LSTT blocks, AOT-B improves the performance to 83.5%. Moreover, AOT-L further improves\nboth the seen and unseen scores by utilizing the memory reading strategy, and our R50-AOT-L\n(84.1%/84.1%) signiÔ¨Åcantly outperforms the previous methods on the validation 2018/2019 split\nwhile maintaining an efÔ¨Åcient speed (14.9FPS).\nTable 2: The quantitative evaluation on the\nsingle-object DA VIS 2016 [41].\nMethods J&F J F FPS\nCFBI+ [68] (Y) 89.9 88.7 91.1 5.9\nKMN [46] (Y) 90.5 89.5 91.5 8.3\nAOT-T (Y) 86.8 86.1 87.4 51.4\nAOT-S (Y) 89.4 88.6 90.2 40.0\nAOT-B (Y) 89.9 88.7 91.1 29.6\nAOT-L (Y) 90.4 89.6 91.1 18.7\nR50-AOT-L (Y) 91.1 90.1 92.1 18.0\nSwinB-AOT-L (Y) 92.0 90.7 93.3 12.1\nDA VIS 2017 [43] is a multi-object extension of\nDA VIS 2016. The validation split of DA VIS 2017\nconsists of 30 videos with 59 objects, and the training\nsplit contains 60 videos with 138 objects. Moreover,\nthe testing split contains 30 more challenging videos\nwith 89 objects in total.\nTable 1b shows that our R50-AOT-L (Y) surpasses all\nthe competitors on both the DA VIS-2017 validation\n(84.9%) and testing ( 79.6%) splits and maintains\nan efÔ¨Åcient speed (18.0FPS). Notably, such a multi-\nobject speed is the same as our single-object speed\non DA VIS 2016. For the Ô¨Årst time, processing mul-\ntiple objects can be as efÔ¨Åcient as processing a single\nobject over the AOT framework. We also evaluate our method without training with YouTube-VOS,\nand AOT-S (79.2%) performs much better than KMN [46] (76.0%) by +3.2%.\nDA VIS 2016[41] is a single-object benchmark containing 20 videos in the validation split. Although\nour AOT aims at improving multi-object video segmentation, we also achieve a new state-of-the-art\nperformance on DA VIS 2016 (R50-AOT-L (Y), 91.1%). Under single-object scenarios, the multi-\nobject superiority of AOT is limited, but R50-AOT-L still maintains an about2√óefÔ¨Åciency compared\nto KMN (18.0 vs 8.3FPS). Furthermore, our smaller variant, AOT-B (89.9%), achieves comparable\nperformance with CFBI+ (89.9%) while running 5√ófaster (29.6 vs 5.9FPS).\nApart from the above results, replacing the AOT encoder from commonly used ResNet50 to SwinB\ncan further boost our performance to higher level (Table 1a, 1b, and 2).\nQualitative results: Fig. 3 visualizes some qualitative results in comparison with CFBI [67], which\nonly associates each object with its relative background. As demonstrated, CFBI is easier to confuse\nmultiple highly similar objects. In contrast, our AOT tracks and segments all the targets accurately by\nassociating all the objects uniformly. However, AOT fails to segment some tiny objects (ski polesand\nwatch) since we do not make special designs for tiny objects.\n6.2 Ablation Study\nIn this section, we analyze the main components and hyper-parameters of AOT and evaluate their\nimpact on the VOS performance in Table 3.\nIdentity number: The number of the identiÔ¨Åcation vectors, M, have to be larger than the object\nnumber in videos. Thus, we set M to 10 in default to be consistent with the maximum object number\n8\nTable 3: Ablation study. The experiments are based on AOT-S and conducted on the validation 2018\nsplit of YouTube-VOS [63] without pre-training on synthetic videos. Self: the position embedding\ntype used in the self-attention. Rel: use relative positional embedding [47] on the local attention.\n(a) Identity number\nM J &F Jseen J unseen\n10 80.3 80.6 73.7\n15 79.0 79.4 72.1\n20 78.3 79.4 70.8\n30 77.2 78.5 70.2\n(b) Local window size\nŒª J &F Jseen J unseen\n15 80.3 80.6 73.7\n11 78.8 79.5 71.9\n7 78.3 79.3 70.9\n0 74.3 74.9 67.6\n(c) Local frame number\nn J &F Jseen J unseen\n1 80.3 80.6 73.7\n2 80.0 79.8 73.7\n3 79.1 80.0 72.2\n0 74.3 74.9 67.6\n(d) LSTT block number\nL J &F Jseen J unseen FPS Param\n2 80.3 80.6 73.7 27.1 7.0M\n3 80.9 81.1 74.0 20.5 8.3M\n1 77.9 78.8 71.0 41.0 5.7M\n(e) Positional embedding\nSelf Rel J &F Jseen J unseen\nsine ‚úì 80.3 80.6 73.7\nnone ‚úì 80.1 80.4 73.5\nsine - 79.7 80.1 72.9\nin the benchmarks [63, 43]. As seen in Table 3a, M larger than 10 leads to worse performance since\n(1) no training video contains so many objects; (2) embedding more than 10 objects into the space\nwith only 256 dimensions is difÔ¨Åcult.\nLocal window size: Table 3b shows that larger local window size,Œª, results in better performance.\nWithout the local attention, Œª= 0, the performance of AOT signiÔ¨Åcantly drops from 80.3% to 74.3%,\nwhich demonstrates the necessity of the local attention.\nLocal frame number: In Table 3c, we also try to employ more previous frames in the local attention,\nbut using only the t‚àí1 frame (80.3%) performs better than using 2/3 frames (80.0%/79.1%). A\npossible reason is that the longer the temporal interval, the more intense the motion between frames,\nso it is easier to introduce more errors in the local matching when using an earlier previous frame.\nLSTT block number: As shown in Table 3d, the AOT performance increases by using more LSTT\nblocks. Notably, the AOT with only one LSTT block (77.9%) reaches a fast real-time speed (41.0FPS)\non YouTube-VOS, although the performance is -2.4% worse than AOT-S (80.3%). By adjusting the\nLSTT block number, we can Ô¨Çexibly balance the accuracy and speed of AOT.\nPosition embedding: In our default setting, we apply Ô¨Åxed sine spatial positional embedding to\nthe self-attention following [9], and our local attention is equipped with learned relative positional\nembedding [47]. The ablation study is shown in Table 3e, where removing the sine embedding\ndecreases the performance to 80.1% slightly. In contrast, the relative embedding is more important\nthan the sine embedding. Without the relative embedding, the performance drops to 79.7%, which\nmeans the motion relationship between adjacent frames is helpful for local attention. We also tried to\napply learned positional embedding to self-attention modules, but no positive effect was observed.\n7 Conclusion\nThis paper proposes a novel and efÔ¨Åcient approach for video object segmentation by associating\nobjects with transformers and achieves superior performance on three popular benchmarks. A simple\nyet effective identiÔ¨Åcation mechanism is proposed to associate, match, and decode all the objects\nuniformly under multi-object scenarios. For the Ô¨Årst time, processing multiple objects in VOS can\nbe efÔ¨Åcient as processing a single object by using the identiÔ¨Åcation mechanism. In addition, a long\nshort-term transformer is designed for constructing hierarchical object matching and propagation for\nVOS. The hierarchical structure allows us to Ô¨Çexibly balance AOT between real-time speed and state-\nof-the-art performance by adjusting the LSTT number. We hope the identiÔ¨Åcation mechanism will\nhelp ease the future study of multi-object VOS and related tasks (e.g., video instance segmentation,\ninteractive VOS, and multi-object tracking), and AOT will serve as a solid baseline.\n9\nReferences\n[1] Parallel distributed deep learning: Machine learning framework from industrial practice.https:\n//www.paddlepaddle.org.cn/\n[2] Avinash Ramakanth, S., Venkatesh Babu, R.: Seamseg: Video object segmentation using patch\nseams. In: CVPR. pp. 376‚Äì383 (2014)\n[3] Ba, J.L., Kiros, J.R., Hinton, G.E.: Layer normalization. In: NIPS Workshops (2016)\n[4] Badrinarayanan, V ., Galasso, F., Cipolla, R.: Label propagation in video sequences. In: CVPR.\npp. 3265‚Äì3272. IEEE (2010)\n[5] Bahdanau, D., Cho, K., Bengio, Y .: Neural machine translation by jointly learning to align and\ntranslate. In: ICLR (2015)\n[6] Bertasius, G., Torresani, L.: Classifying, segmenting, and tracking object instances in video\nwith mask propagation. In: CVPR. pp. 9739‚Äì9748 (2020)\n[7] Bhat, G., Lawin, F.J., Danelljan, M., Robinson, A., Felsberg, M., Van Gool, L., Timofte, R.:\nLearning what to learn for video object segmentation. In: ECCV (2020)\n[8] Caelles, S., Maninis, K.K., Pont-Tuset, J., Leal-Taix√©, L., Cremers, D., Van Gool, L.: One-shot\nvideo object segmentation. In: CVPR. pp. 221‚Äì230 (2017)\n[9] Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.: End-to-end object\ndetection with transformers. In: ECCV . pp. 213‚Äì229. Springer (2020)\n[10] Chen, X., Li, Z., Yuan, Y ., Yu, G., Shen, J., Qi, D.: State-aware tracker for real-time video\nobject segmentation. In: CVPR. pp. 9384‚Äì9393 (2020)\n[11] Chen, Y ., Pont-Tuset, J., Montes, A., Van Gool, L.: Blazingly fast video object segmentation\nwith pixel-wise metric learning. In: CVPR. pp. 1189‚Äì1198 (2018)\n[12] Cheng, M.M., Mitra, N.J., Huang, X., Torr, P.H., Hu, S.M.: Global contrast based salient region\ndetection. TPAMI 37(3), 569‚Äì582 (2014)\n[13] Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirectional\ntransformers for language understanding. In: NAACL. pp. 4171‚Äî-4186 (2019)\n[14] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani,\nM., Minderer, M., Heigold, G., Gelly, S., et al.: An image is worth 16x16 words: Transformers\nfor image recognition at scale. In: ICLR (2021)\n[15] Duke, B., Ahmed, A., Wolf, C., Aarabi, P., Taylor, G.W.: Sstvos: Sparse spatiotemporal\ntransformers for video object segmentation. In: CVPR (2021)\n[16] Everingham, M., Van Gool, L., Williams, C.K., Winn, J., Zisserman, A.: The pascal visual\nobject classes (voc) challenge. IJCV 88(2), 303‚Äì338 (2010)\n[17] Hariharan, B., Arbel√°ez, P., Bourdev, L., Maji, S., Malik, J.: Semantic contours from inverse\ndetectors. In: ICCV . pp. 991‚Äì998. IEEE (2011)\n[18] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR\n(2016)\n[19] Hendrycks, D., Gimpel, K.: Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415\n(2016)\n[20] Hu, Y .T., Huang, J.B., Schwing, A.G.: Videomatch: Matching based video object segmentation.\nIn: ECCV . pp. 54‚Äì70 (2018)\n[21] Huang, G., Sun, Y ., Liu, Z., Sedra, D., Weinberger, K.Q.: Deep networks with stochastic depth.\nIn: ECCV . pp. 646‚Äì661. Springer (2016)\n[22] Ioffe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by reducing\ninternal covariate shift. In: ICML (2015)\n[23] Johnander, J., Danelljan, M., Brissman, E., Khan, F.S., Felsberg, M.: A generative appearance\nmodel for end-to-end video object segmentation. In: CVPR. pp. 8953‚Äì8962 (2019)\n[24] Li, Y ., Shen, Z., Shan, Y .: Fast video object segmentation using the global context module. In:\nECCV . pp. 735‚Äì750. Springer (2020)\n10\n[25] Lin, H., Qi, X., Jia, J.: Agss-vos: Attention guided single-shot video object segmentation. In:\nICCV . pp. 3949‚Äì3957 (2019)\n[26] Lin, T.Y ., Doll√°r, P., Girshick, R., He, K., Hariharan, B., Belongie, S.: Feature pyramid networks\nfor object detection. In: CVPR. pp. 2117‚Äì2125 (2017)\n[27] Lin, T.Y ., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll√°r, P., Zitnick, C.L.:\nMicrosoft coco: Common objects in context. In: ECCV . pp. 740‚Äì755. Springer (2014)\n[28] Liu, Z., Lin, Y ., Cao, Y ., Hu, H., Wei, Y ., Zhang, Z., Lin, S., Guo, B.: Swin transformer:\nHierarchical vision transformer using shifted windows. In: ICCV (2021)\n[29] Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. In: ICLR (2019)\n[30] Lu, X., Wang, W., Danelljan, M., Zhou, T., Shen, J., Van Gool, L.: Video object segmentation\nwith episodic graph memory networks. In: ECCV (2020)\n[31] Luiten, J., V oigtlaender, P., Leibe, B.: Premvos: Proposal-generation, reÔ¨Ånement and merging\nfor video object segmentation. In: ACCV . pp. 565‚Äì580 (2018)\n[32] Miao, J., Wei, Y ., Yang, Y .: Memory aggregation networks for efÔ¨Åcient interactive video object\nsegmentation. In: CVPR (2020)\n[33] Milan, A., Leal-Taix√©, L., Reid, I., Roth, S., Schindler, K.: Mot16: A benchmark for multi-object\ntracking. arXiv preprint arXiv:1603.00831 (2016)\n[34] Ngan, K.N., Li, H.: Video segmentation and its applications. Springer Science & Business\nMedia (2011)\n[35] Nowozin, S.: Optimal decisions from probabilistic models: the intersection-over-union case. In:\nCVPR. pp. 548‚Äì555 (2014)\n[36] Oh, S.W., Lee, J.Y ., Xu, N., Kim, S.J.: Fast user-guided video object segmentation by interaction-\nand-propagation networks. In: CVPR. pp. 5247‚Äì5256 (2019)\n[37] Oh, S.W., Lee, J.Y ., Xu, N., Kim, S.J.: Video object segmentation using space-time memory\nnetworks. In: ICCV (2019)\n[38] Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, L., Shazeer, N., Ku, A., Tran, D.: Image\ntransformer. In: ICCV . pp. 4055‚Äì4064. PMLR (2018)\n[39] Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z., Desmaison, A.,\nAntiga, L., Lerer, A.: Automatic differentiation in pytorch (2017)\n[40] Perazzi, F., Khoreva, A., Benenson, R., Schiele, B., Sorkine-Hornung, A.: Learning video\nobject segmentation from static images. In: CVPR. pp. 2663‚Äì2672 (2017)\n[41] Perazzi, F., Pont-Tuset, J., McWilliams, B., Van Gool, L., Gross, M., Sorkine-Hornung, A.: A\nbenchmark dataset and evaluation methodology for video object segmentation. In: CVPR. pp.\n724‚Äì732 (2016)\n[42] Polyak, B.T., Juditsky, A.B.: Acceleration of stochastic approximation by averaging. SIAM\njournal on control and optimization 30(4), 838‚Äì855 (1992)\n[43] Pont-Tuset, J., Perazzi, F., Caelles, S., Arbel√°ez, P., Sorkine-Hornung, A., Van Gool, L.: The\n2017 davis challenge on video object segmentation. arXiv preprint arXiv:1704.00675 (2017)\n[44] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I.: Language models are\nunsupervised multitask learners. OpenAI blog 1(8), 9 (2019)\n[45] Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: Mobilenetv2: Inverted residuals\nand linear bottlenecks. In: CVPR. pp. 4510‚Äì4520 (2018)\n[46] Seong, H., Hyun, J., Kim, E.: Kernelized memory network for video object segmentation. In:\nECCV (2020)\n[47] Shaw, P., Uszkoreit, J., Vaswani, A.: Self-attention with relative position representations. In:\nNAACL. pp. 464‚Äì468 (2018)\n[48] Shi, J., Yan, Q., Xu, L., Jia, J.: Hierarchical image saliency detection on extended cssd. TPAMI\n38(4), 717‚Äì729 (2015)\n[49] Synnaeve, G., Xu, Q., Kahn, J., Likhomanenko, T., Grave, E., Pratap, V ., Sriram, A., Liptchinsky,\nV ., Collobert, R.: End-to-end asr: from supervised to semi-supervised learning with modern\narchitectures. In: ICML Workshops (2020)\n11\n[50] Vaswani, A., Ramachandran, P., Srinivas, A., Parmar, N., Hechtman, B., Shlens, J.: Scaling local\nself-attention for parameter efÔ¨Åcient visual backbones. In: CVPR. pp. 12894‚Äì12904 (2021)\n[51] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L.,\nPolosukhin, I.: Attention is all you need. In: NIPS (2017)\n[52] Vijayanarasimhan, S., Grauman, K.: Active frame selection for label propagation in videos. In:\nECCV . pp. 496‚Äì509. Springer (2012)\n[53] V oigtlaender, P., Chai, Y ., Schroff, F., Adam, H., Leibe, B., Chen, L.C.: Feelvos: Fast end-to-end\nembedding learning for video object segmentation. In: CVPR. pp. 9481‚Äì9490 (2019)\n[54] V oigtlaender, P., Leibe, B.: Online adaptation of convolutional neural networks for video object\nsegmentation. In: BMVC (2017)\n[55] V oigtlaender, P., Luiten, J., Leibe, B.: Boltvos: Box-level tracking for video object segmentation.\narXiv preprint arXiv:1904.04552 (2019)\n[56] Wang, X., Girshick, R., Gupta, A., He, K.: Non-local neural networks. In: CVPR. pp. 7794‚Äì\n7803 (2018)\n[57] Wang, Y ., Xu, Z., Wang, X., Shen, C., Cheng, B., Shen, H., Xia, H.: End-to-end video instance\nsegmentation with transformers. arXiv preprint arXiv:2011.14503 (2020)\n[58] Wang, Y ., Xu, Z., Wang, X., Shen, C., Cheng, B., Shen, H., Xia, H.: End-to-end video instance\nsegmentation with transformers. In: CVPR. pp. 8741‚Äì8750 (2021)\n[59] Wang, Z., Zheng, L., Liu, Y ., Wang, S.: Towards real-time multi-object tracking. In: ECCV .\nSpringer (2020)\n[60] Wu, Y ., He, K.: Group normalization. In: ECCV . pp. 3‚Äì19 (2018)\n[61] Wug Oh, S., Lee, J.Y ., Sunkavalli, K., Joo Kim, S.: Fast video object segmentation by reference-\nguided mask propagation. In: CVPR. pp. 7376‚Äì7385 (2018)\n[62] Xiao, H., Feng, J., Lin, G., Liu, Y ., Zhang, M.: Monet: Deep motion exploitation for video\nobject segmentation. In: CVPR. pp. 1140‚Äì1148 (2018)\n[63] Xu, N., Yang, L., Fan, Y ., Yue, D., Liang, Y ., Yang, J., Huang, T.: Youtube-vos: A large-scale\nvideo object segmentation benchmark. arXiv preprint arXiv:1809.03327 (2018)\n[64] Xu, Z., Zhang, W., Tan, X., Yang, W., Huang, H., Wen, S., Ding, E., Huang, L.: Segment\nas points for efÔ¨Åcient online multi-object tracking and segmentation. In: ECCV . pp. 264‚Äì281.\nSpringer (2020)\n[65] Yang, L., Fan, Y ., Xu, N.: Video instance segmentation. In: ICCV . pp. 5188‚Äì5197 (2019)\n[66] Yang, L., Wang, Y ., Xiong, X., Yang, J., Katsaggelos, A.K.: EfÔ¨Åcient video object segmentation\nvia network modulation. In: CVPR. pp. 6499‚Äì6507 (2018)\n[67] Yang, Z., Wei, Y ., Yang, Y .: Collaborative video object segmentation by foreground-background\nintegration. In: ECCV (2020)\n[68] Yang, Z., Wei, Y ., Yang, Y .: Collaborative video object segmentation by multi-scale foreground-\nbackground integration. TPAMI (2021)\n[69] Zhang, Z., Fidler, S., Urtasun, R.: Instance-level segmentation for autonomous driving with\ndeep densely connected mrfs. In: CVPR. pp. 669‚Äì677 (2016)\n12\n(a) M = 10(default)\n (b) M = 15\n (c) M = 20\n (d) M = 30\nFigure 4: Visualization of the cosine similarity between every two of M identiÔ¨Åcation vectors in the\nidentity bank. We use the form of a M √óM symmetric matrix to visualize all the cosine similarities,\nand the values on the diagonal are all equal to 1. The darker the green color, the higher the similarity.\nIn the case of M = 10, the similarities are stable and balanced. As the vector number M increases,\nThe visualized matrix becomes less and less smooth, which means the similarities become unstable.\nA Appendix\nA.1 More Implementation Details\nA.1.1 Network Details\nFor MobileNet-V2 encoder, we increase the Ô¨Ånal resolution of the encoder to 1/16 by adding a\ndilation to the last stage and removing a stride from the Ô¨Årst convolution of this stage. For ResNet-50\nand SwinB encoders, we remove the last stage directly. The encoder features are Ô¨Çattened into\nsequences before LSTT. In LSTT, the input channel dimension is 256, and the head number is set\nto 8 for all the attention modules. To increase the receptive Ô¨Åeld of LSTT, we insert a depth-wise\nconvolution layer with a kernel size of 5 between two layers of each feed-forward module. In our\ndefault setting of the short-term memory n, only the previous (t‚àí1) frame is considered, which is\nsimilar to the local matching strategy [53, 67]. After LSTT, all the output features of LSTT blocks\nare reshaped into 2D shapes and will serve as the decoder input. Then, the FPN decoder progressively\nincreases the feature resolution from 1/16 to 1/4 and decreases the channel dimension from 256 to\n128 before the Ô¨Ånal output layer, which is used for identiÔ¨Åcation decoding.\nPatch-wise Identity Bank: Since the spatial size of LSTT features is only 1/16 of the input video,\nwe can not directly assign identities to the pixels of high-resolution input mask to construct a low-\nresolution identiÔ¨Åcation embedding. To overcome this problem, we further propose a strategy named\npatch-wise identity bank. In detail, we Ô¨Årst separate the input mask into non-overlapping patches of\n16√ó16 pixels. The original identity bank with M identities is also expanded to a patch-wise identity\nbank, in which each identity has 16√ó16 sub-identity vectors corresponding to 16√ó16 positions in\na patch. Hence, the pixels of an object region with different patch positions will have different\nsub-identity vectors under an assigned identity. By summing all the assigned sub-identities in each\npatch, we can directly construct a low-resolution identiÔ¨Åcation embedding while keeping the shape\ninformation inside each patch.\nA.1.2 Training Details\nAll the videos are Ô¨Årstly down-sampled to 480p resolution, and the cropped window size is 465√ó\n465. For optimization, we adopt the AdamW [29] optimizer and the sequential training strategy [67],\nwhose sequence length is set to 5. The loss function is a 0.5:0.5 combination of bootstrapped cross-\nentropy loss and soft Jaccard loss [35]. For stabilizing the training, the statistics of BN [22] modules\nand the Ô¨Årst two stages in the encoder are frozen, and Exponential Moving Average (EMA) [42] is\nused. Besides, we apply stochastic depth [21] to the self-attention and the feed-forward modules in\nLSTT.\nThe batch size is 16 and distributed on 4 Tesla V100 GPUs. For pre-training, we use an initial learning\nrate of 4 √ó10‚àí4 and a weight decay of 0.03 for 100,000 steps. For main training, the initial learning\nrate is set to 2 √ó10‚àí4 and the weight decay is 0.07. In addition, the training steps are 100,000 for\nYouTube-VOS or 50,000 for DA VIS. To relieve over-Ô¨Åtting, the initial learning rate of encoders is\nreduced to a 0.1 scale of other network parts. All the learning rates gradually decay to 2 √ó10‚àí5 in a\npolynomial manner [67].\n13\nùíéùêª\nùëä ùëä\nùêªùíè\nLong-term MemoryShort-term Memory\nCurrent FrameCurrent Frame\nùúÜ\n(a) Long-term Attention\nùíéùêª\nùëä ùëä\nùêªùíè\nLong-term MemoryShort-term Memory\nCurrent FrameCurrent Frame\nùúÜ (b) Short-term Attention\nFigure 5: Illustrations of the long-term attention and the short-term attention. (a) The long-term\nattention employs a non-local manner to match all the locations in the long-term memory. (b) In\ncontrast, the short-term attention only focus on a nearby spatial-temporal region with a shape of nŒª2.\nA.2 Visualization of Identity Bank\nIn AOT, the identity bank is randomly initialized, and all theM identiÔ¨Åcation vectors are learned by\nbeing randomly assigned to objects during the training phase. Intuitively, all the identiÔ¨Åcation vectors\nshould be equidistant away from each other in the feature space because their roles are equivalent. To\nvalidate our hypothesis, we visualize the similarity between every two identiÔ¨Åcation vectors in Fig. 4.\nIn our default setting, M = 10 (Fig. 4a), all the vectors are far away from each other, and the\nsimilarities remain almost the same. This phenomenon is consistent with our above hypothesis. In\nother words, the reliability and effectiveness of our identiÔ¨Åcation mechanism are further veriÔ¨Åed.\nIn the ablation study, using more identities leads to worse results. To analyze the reason, we\nalso visualize the learned identity banks with more vectors. Fig. 4b, 4c, and 4d demonstrate that\nmaintaining equidistant between every two vectors becomes more difÔ¨Åcult when the identity bank\ncontains more vectors, especially whenM = 30. There are two possible reasons for this phenomenon:\n(1) No training video contains enough objects to be assigned so many identities, and thus the network\ncannot learn to associate all the identities simultaneously; (2) the used space with only 256 dimensions\nis difÔ¨Åcult for keeping more than 10 objects to be equidistant.\nA.3 Illustration of Long Short-term Attention\nTo facilitate understanding our long-term and short-term attention modules, we illustrate their\nprocesses in Fig. 5. Since the temporal smoothness between the current frame and long-term memory\nframes is difÔ¨Åcult to guarantee, the long-term attention employs a non-local manner to match all\nthe locations in the long-term memory. In contrast, short-term attention only focuses on a nearby\nspatial-temporal neighborhood of each current-frame location.\nA.4 Visualization of Hierarchical Matching and Propagation\nIn our AOT, we propose to construct a hierarchical framework,i.e., LSTT, for multi-object matching\nand propagation, and the ablation study indicates that using more LSTT layers (or blocks) results in\nbetter VOS performance. To further validate the effectiveness of LSTT and analyze the behavior of\neach LSTT layer, we visualize long-term and short-term attention maps in each layer during inference,\nas shown in Fig. 6 and 7.\nAt the bottom of Fig. 6, the attention maps become more accurate and sharper as the index of layers\nincreases. In the Ô¨Årst layer, i.e., l= 1, the current features have not aggregated the multi-object mask\ninformation from memory frames, and the long-term attention map is very vague and contains a lot of\nwrong matches among the objects and the background. Nevertheless, as the layer index increases, the\nmask information of all the objects is gradually aggregated so that the long-term attention becomes\nmore and more accurate. Similarly, the quality, especially the boundary of objects, of the short-term\nattention improves as the layer index increases. Notably, the short-term attention performs well\neven in the Ô¨Årst layer, l= 1, which is different from the long-term attention. The reason is that the\n14\nFrame1 Frame ùë°‚àí1 Frame ùë°\nFrame ùë°\nLong-term MemoryShort-term MemoryCurrent Prediction\nùëô=1 ùëô=2 ùëô=3Long-term Attention\nShort-term Attention\nFigure 6: Visualization of long-term and short-term attention maps during the inference of DA VIS\n2017 [43]. There are three similar people, marked in different colors, in the video. To sufÔ¨Åciently\nverify the effect of long-term attention, only the Ô¨Årst frame is considered into the long-term memory,\nand thus we AOT-B to conduct the experiment. For visualization, we propagate the colored multi-\nobject masks in long-term or short-term memory to the current frame regarding the corresponding\nattention map. The brighter the color, the stronger the attention. l= 1, 2, and 3 denote the 3 LSTT\nlayers of AOT-B in order.\nFrame1 Frame ùë°‚àí1 Frame ùë°\nFrame ùë°\nLong-term MemoryShort-term MemoryCurrent Prediction\nùëô=1 ùëô=2 ùëô=3Long-term Attention\nShort-term Attention\nFigure 7: Visualization of long-term and short-term attention maps during the inference of DA VIS\n2017 [43]. In this case, the red person is occluded in the t‚àí1 frame, and thus the short-term attention\nfails to match the person in the current frame. However, the long-term attention generates an accurate\nattention map with a clean background in the last layer, l= 3, resulting in a correct prediction.\nneighborhood matching of short-term attention is easier than the long-term matching of long-term\nattention. However, long-term attention is still necessary because short-term attention will fail in\nsome cases, such as occlusion (Fig. 7).\nIn short, the visual analysis further proves the necessity and effectiveness of our hierarchical LSTT.\nThe hierarchical matching is not simply a combination of multiple matching processes. Critically,\nthe multi-object information will be gradually aggregated and associated as the LSTT structure goes\ndeeper, leading to more accurate attention-based matching.\n15\nA.5 Compare with More Methods\nWe compare our AOT with more VOS methods in Table 4 and 5. To compare with latest real-time\nmethods [10, 24], we introduce the real-time AOT variant, i.e., AOT-T.\nEven on the single-object DA VIS 2016 [41], AOT-S (89.4%, 40.0FPS) can achieve comparable speed\nwith SAT [10] (83.1%, 39FPS) and comparable performance with CFBI [67] (89.4%, 6.3FPS). On\nthe multi-object DA VIS 2017 [43], AOT-T (79.9%, 51.4FPS) signiÔ¨Åcantly outperforms SAT (72.3%,\n19.5FPS) and GC (71.4%, 12.5FPS). Particularly, on the large-scale multi-object YouTube-VOS [63],\nAOT-T/S (80.2%/82.6%) achieves superior performance compared to previous real-time methods,\nSAT (63.6%) and GC (73.2%), while still maintaining a real-time speed (41.0FPS/27.1FPS).\nA.6 Additional Qualitative Results\nWe supply more qualitative results under multi-object scenarios on the large-scale YouTube-VOS [63]\nand the small-scale DA VIS 2017 [43] in Fig. 8 and 9, respectively. As demonstrated, our AOT-L is\nrobust to many challenging VOS cases, including similar objects, occlusion, fast motion, and motion\nblur, etc.\nA.7 Border Impact and Future Works\nThe proposed AOT framework greatly simpliÔ¨Åes the process of multi-object VOS and achieves a\nsigniÔ¨Åcant performance of effectiveness, robustness, and efÔ¨Åciency. Some AOT variants can achieve\npromising results while keeping real-time speed. In other words, AOT may promote the applications\nof VOS in real-time video systems, such as video conference, self-driving car, augmented reality, etc.\nNevertheless, the speed and accuracy of AOT can still be further improved. There is still a very\nlarge accuracy gap between the real-time AOT-T and the state-of-the-art SwinB-AOT-L. Moreover,\nAOT uses only a lightweight encoder and decoder. How to design stronger yet efÔ¨Åcient encoders and\ndecoders for VOS is still an open question.\nAs to related areas of VOS, the simple yet effective identiÔ¨Åcation mechanism should also be promising\nfor many tasks requiring multi-object matching, such as interactive video object segmentation [36,\n32], video instance segmentation [ 65, 6, 57], and multi-object tracking [ 33, 64, 59]. Besides, the\nhierarchical LSTT may serve as a new solution for processing video representations in these tasks.\n16\nTable 4: Additional quantitative comparison on multi-object benchmarks, YouTube-VOS [63] and\nDA VIS 2017 [43].\n(a) YouTube-VOS\nSeen Unseen\nMethods J &F J F J F FPS\nValidation 2018 Split\nSAT[CVPR20] [10] 63.6 67.1 70.2 55.3 61.7 -\nAG[CVPR19] [23] 66.1 67.8 - 60.8 - -\nPReM[ACCV18] [31] 66.9 71.4 75.9 56.5 63.7 0.17\nBoLT[arXiv19] [55] 71.1 71.6 - 64.3 - 0.74\nGC[ECCV20] [24] 73.2 72.6 75.6 68.9 75.7 -\nSTM[ICCV19] [37] 79.4 79.7 84.2 72.8 80.9 -\nEGMN[ECCV20] [30] 80.2 80.7 85.1 74.0 80.9 -\nKMN[ECCV20] [46] 81.4 81.4 85.6 75.3 83.3 -\nCFBI[ECCV20] [67] 81.4 81.1 85.8 75.3 83.4 3.4\nLWL[ECCV20] [7] 81.5 80.4 84.9 76.4 84.4 -\nSST[CVPR21] [15] 81.7 81.2 - 76.0 - -\nCFBI+[TPAMI21] [68] 82.8 81.8 86.6 77.1 85.6 4.0\nAOT-T 80.2 80.1 84.5 74.0 82.2 41.0\nAOT-S 82.6 82.0 86.7 76.6 85.0 27.1\nAOT-B 83.5 82.6 87.5 77.7 86.0 20.5\nAOT-L 83.8 82.9 87.9 77.7 86.5 16.0\nR50-AOT-L 84.1 83.7 88.5 78.1 86.1 14.9\nSwinB-AOT-L 84.5 84.3 89.3 77.9 86.4 9.3\nValidation 2019 Split\nCFBI[ECCV20] [67] 81.0 80.6 85.1 75.2 83.0 3.4\nSST[CVPR21] [15] 81.8 80.9 - 76.6 - -\nCFBI+[TPAMI21] [68] 82.6 81.7 86.2 77.1 85.2 4.0\nAOT-T 79.7 79.6 83.8 73.7 81.8 41.0\nAOT-S 82.2 81.3 85.9 76.6 84.9 27.1\nAOT-B 83.3 82.4 87.1 77.8 86.0 20.5\nAOT-L 83.7 82.8 87.5 78.0 86.7 16.0\nR50-AOT-L 84.1 83.5 88.1 78.4 86.3 14.9\nSwinB-AOT-L 84.5 84.0 88.8 78.4 86.7 9.3\n(b) DA VIS 2017\nMethods J &F J F FPS\nValidation 2017 Split\nOSMN [66] 54.8 52.5 57.1 3.6 ‚Ä°\nVM [20] 62.4 56.5 68.2 2.9\nOnA [54] 63.6 61.0 66.1 0.04\nRGMP [61] 66.7 64.8 68.6 3.6 ‚Ä°\nAG [23] (Y) 70.0 67.2 72.7 7.1 ‚Ä°\nGC [24] 71.4 69.3 73.5 12.5 ‚Ä°\nFEEL [53] (Y) 71.5 69.1 74.0 2.0\nSAT [10] (Y) 72.3 68.6 76.0 19.5 ‚Ä°\nPReM [31] 77.8 73.9 81.7 0.03\nLWL [7] (Y) 81.6 79.1 84.1 2.5 ‚Ä°\nSTM [37] (Y) 81.8 79.2 84.3 3.1 ‚Ä°\nCFBI [67] (Y) 81.9 79.3 84.5 5.9\nSST [15] (Y) 82.5 79.9 85.1 -\nEGMN [30] (Y) 82.8 80.2 85.2 2.5 ‚Ä°\nKMN [46] 76.0 74.2 77.8 4.2 ‚Ä°\nKMN [46] (Y) 82.8 80.0 85.6 4.2 ‚Ä°\nCFBI+ [68] (Y) 82.9 80.1 85.7 5.6\nAOT-T (Y) 79.9 77.4 82.3 51.4\nAOT-S 79.2 76.4 82.0 40.0\nAOT-S (Y) 81.3 78.7 83.9 40.0\nAOT-B (Y) 82.5 79.7 85.2 29.6\nAOT-L (Y) 83.8 81.1 86.4 18.7\nR50-AOT-L (Y) 84.9 82.3 87.5 18.0\nSwinB-AOT-L (Y) 85.4 82.4 88.4 12.1\nTesting 2017 Split\nOSMN [66] 41.3 37.7 44.9 2.4 ‚Ä°\nRGMP [61] 52.9 51.3 54.4 2.4 ‚Ä°\nOnA [54] 56.5 53.4 59.6 0.03\nFEEL [53] (Y) 57.8 55.2 60.5 1.9\nPReM [31] 71.6 67.5 75.7 0.02\nSTM‚àó[37] (Y) 72.2 69.3 75.2 -\nCFBI [67] (Y) 75.0 71.4 78.7 5.3\nCFBI‚àó[67] (Y) 76.6 73.0 80.1 2.9\nKMN‚àó[46] (Y) 77.2 74.1 80.3 -\nCFBI+‚àó[68] (Y) 78.0 74.4 81.6 3.4\nAOT-T (Y) 72.0 68.3 75.7 51.4\nAOT-S (Y) 73.9 70.3 77.5 40.0\nAOT-B (Y) 75.5 71.6 79.3 29.6\nAOT-L (Y) 78.3 74.3 82.3 18.7\nR50-AOT-L (Y) 79.6 75.9 83.3 18.0\nSwinB-AOT-L (Y) 81.2 77.3 85.1 12.1\n17\nTable 5: Additional quantitative comparison on DA VIS 2016 [41].\nMethods J&F J F FPS\nOSMN [66] - 74.0 7.1\nPML [11] 77.4 75.5 79.3 3.6\nVM [20] 80.9 81.0 80.8 3.1\nFEEL [53] (Y) 81.7 81.1 82.2 2.2\nRGMP [61] 81.8 81.5 82.0 7.1\nAG [23] (Y) 82.1 82.2 82.0 14.3\nSAT [10] (Y) 83.1 82.6 83.6 39\nOnA [54] 85.0 85.7 84.2 0.08\nGC [24] 86.6 87.6 85.7 25\nPReM [31] 86.8 84.9 88.6 0.03\nSTM [37] (Y) 89.3 88.7 89.9 6.3\nCFBI [67] (Y) 89.4 88.3 90.5 6.3\nCFBI+ [68] (Y) 89.9 88.7 91.1 5.9\nKMN [46] (Y) 90.5 89.5 91.5 8.3\nAOT-T (Y) 86.8 86.1 87.4 51.4\nAOT-S (Y) 89.4 88.6 90.2 40.0\nAOT-B (Y) 89.9 88.7 91.1 29.6\nAOT-L (Y) 90.4 89.6 91.1 18.7\nR50-AOT-L (Y) 91.1 90.1 92.1 18.0\nSwinB-AOT-L (Y) 92.0 90.7 93.3 12.1\n18\ntime\n0% 25% 50% 75% 100%\nSimilarObjects\nOcclusion\nFastMotion\nOthers\nFigure 8: Qualitative results on the validation 2019 split of YouTube-VOS [63]. Our AOT-L performs\nwell under many challenging multi-object cases, including similar objects, occlusion, and fast motion,\netc.\n19\ntime0% 25% 50% 75% 100%\nTesting\nValidation\nFigure 9: Qualitative results on DA VIS 2017 [43].\n20",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7817287445068359
    },
    {
      "name": "Segmentation",
      "score": 0.6554983854293823
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6346275806427002
    },
    {
      "name": "Embedding",
      "score": 0.6303025484085083
    },
    {
      "name": "Object (grammar)",
      "score": 0.6102325320243835
    },
    {
      "name": "Computer vision",
      "score": 0.5152514576911926
    },
    {
      "name": "Transformer",
      "score": 0.4969811737537384
    },
    {
      "name": "Matching (statistics)",
      "score": 0.43483367562294006
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.38897067308425903
    },
    {
      "name": "Mathematics",
      "score": 0.08146145939826965
    },
    {
      "name": "Engineering",
      "score": 0.07635971903800964
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}