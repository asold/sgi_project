{
  "title": "Insertion-Deletion Transformer",
  "url": "https://openalex.org/W2999668723",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4227405948",
      "name": "Ruis, Laura",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4288510118",
      "name": "Stern, Mitchell",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4288509397",
      "name": "Proskurnia, Julia",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2606458397",
      "name": "Chan, William",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2752047430",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2948629866",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2963742216",
    "https://openalex.org/W2912937082",
    "https://openalex.org/W2913250058",
    "https://openalex.org/W2946375144",
    "https://openalex.org/W2949806279",
    "https://openalex.org/W2920538220",
    "https://openalex.org/W2963403868"
  ],
  "abstract": "We propose the Insertion-Deletion Transformer, a novel transformer-based neural architecture and training method for sequence generation. The model consists of two phases that are executed iteratively, 1) an insertion phase and 2) a deletion phase. The insertion phase parameterizes a distribution of insertions on the current output hypothesis, while the deletion phase parameterizes a distribution of deletions over the current output hypothesis. The training method is a principled and simple algorithm, where the deletion model obtains its signal directly on-policy from the insertion model output. We demonstrate the effectiveness of our Insertion-Deletion Transformer on synthetic translation tasks, obtaining significant BLEU score improvement over an insertion-only model.",
  "full_text": "Insertion-Deletion Transformer\nLaura Ruis∗\nUniversity of Amsterdam\nlaura.ruis@student.uva.nl\nMitchell Stern\nUniversity of California, Berkeley\nGoogle Brain\nmitchell@berkeley.edu\nJulia Proskurnia\nGoogle Inc.\njuliapro@google.com\nWilliam Chan\nGoogle Brain\nwilliamchan@google.com\nAbstract\nWe propose the Insertion-Deletion Trans-\nformer, a novel transformer-based neural ar-\nchitecture and training method for sequence\ngeneration. The model consists of two phases\nthat are executed iteratively, 1) an insertion\nphase and 2) a deletion phase. The insertion\nphase parameterizes a distribution of inser-\ntions on the current output hypothesis, while\nthe deletion phase parameterizes a distribution\nof deletions over the current output hypothe-\nsis. The training method is a principled and\nsimple algorithm, where the deletion model\nobtains its signal directly on-policy from the\ninsertion model output. We demonstrate the\neffectiveness of our Insertion-Deletion Trans-\nformer on synthetic translation tasks, obtain-\ning signiﬁcant BLEU score improvement over\nan insertion-only model.\n1 Introduction and Related Work\nNeural sequence models (Sutskever et al., 2014;\nCho et al., 2014) typically generate outputs in an\nautoregressive left-to-right manner. These mod-\nels have been successfully applied to a range of\ntask, for example machine translation (Vaswani\net al., 2017). They often rely on an encoder that\nprocesses the source sequence, and a decoder that\ngenerates the output sequence conditioned on the\noutput of the encoder. The decoder will typically\ngenerate the target sequence one token at a time,\nin an autoregressive left-to-right fashion.\nRecently, research in insertion-based non- or\npartially- autoregressive models has spiked (Stern\net al., 2019; Welleck et al., 2019; Gu et al., 2019a;\nChan et al., 2019). These model are more ﬂex-\nible than their autoregressive counterparts. They\ncan generate sequences in any order, and can ben-\neﬁt from parallel token generation. They can\nlearn complex orderings (e.g., tree orderings) and\n∗Work done during internship at Google\nmay be more applicable to task like cloze ques-\ntion answering (Chan et al., 2019) and text sim-\npliﬁcation, where the order of generation is not\nnaturally left to right, and the source sequence\nmight not be fully observed. One recently pro-\nposed approach is the Insertion Transformer (Stern\net al., 2019), where the target sequence is mod-\nelled with insertion-edits. As opposed to tradi-\ntional sequence-to-sequence models, the Insertion\nTransformer can generate sequences in any arbi-\ntrary order, where left-to-right is a special case.\nAdditionally, during inference, the model is en-\ndowed with parallel token generation capabilities.\nThe Insertion Transformer can be trained to follow\na soft balanced binary tree order, thus allowing the\nmodel to generatentokens inO(log2 n) iterations.\nIn this work we propose to generalize this\ninsertion-based framework, we present a frame-\nwork which emits both insertions and deletions.\nOur Insertion-Deletion Transformer consists of an\ninsertion phase and a deletion phase that are ex-\necuted iteratively. The insertion phase follows\nthe typical insertion-based framework (Stern et al.,\n2019). However, in the deletion phase, we teach\nthe model to do deletions with on-policy training.\nWe sample an input sequence on-policy from the\ninsertion model (with on-policy insertion errors),\nand teach the deletion model its appropriate dele-\ntions.\nThis insertion-deletion framework allows for\nﬂexible sequence generation, parallel token gener-\nation and text editing. In a conventional insertion-\nbased model, if the model makes a mistake dur-\ning generation, this cannot be undone. Introduc-\ning the deletion phase makes it possible to undo\nthe mistakes made by the insertion model, since it\nis trained on the on-policy errors of the insertion\nphase. The deletion model extension also enables\nthe framework to efﬁciently handle tasks like text\nsimpliﬁcation and style transfer by starting the de-\ncoding process from the original source sequence.\narXiv:2001.05540v1  [cs.LG]  15 Jan 2020\nA concurrent work was recently proposed,\ncalled the Levenshtein Transformer (LevT) (Gu\net al., 2019b). The LevT framework also generates\nsequences with insertion and deletion operations.\nOur approach has some important distinctions and\ncan be seen as a simpliﬁed version, for both the ar-\nchitecture and the training algorithm. The training\nalgorithm used in the LevT framework uses an ex-\npert policy. This expert policy requires dynamic\nprogramming to minimize Levenshtein distance\nbetween the current input and the target. This ap-\nproach was also explored by Dong et al. (2019);\nSabour et al. (2019). Their learning algorithm ar-\nguably adds more complexity than needed over the\nsimple on-policy method we propose. The LevT\nframework consists of three stages, ﬁrst the num-\nber of tokens to be inserted is predicted, then the\nactual tokens are predicted, and ﬁnally the deletion\nactions are emitted. The extra classiﬁer to predict\nthe number of tokens needed to be inserted adds\nan additional Transformer pass to each generation\nstep. In practice, it is also unclear whether the\nLevT exhibits speedups over an insertion-based\nmodel following a balanced binary tree order. In\ncontrast, our Insertion-Deletion framework only\nhas one insertion phase and one deletion phase,\nwithout the need to predict the number of tokens\nneeded to be inserted. This greatly simpliﬁes the\nmodel architecture, training procedure and infer-\nence runtime.\nAn alternative approach for text editing is pro-\nposed by Xia et al. (2017), which they dub De-\nliberation Networks. This work also acknowl-\nedges the potential beneﬁts from post-editing out-\nput sequences and proposes a two-phase decoding\nframework to facilitate this.\nIn this paper, we present the insertion-deletion\nframework as a proof of concept by applying it to\ntwo synthetic character-based translation tasks and\nshowing it can signiﬁcantly increase the BLEU\nscore over the insertion-only framework.\n2 Method\nIn this section, we describe our Insertion-Deletion\nmodel. We extend the Insertion Transformer\n(Stern et al., 2019), an insertion-only framework\nto handle both insertions and deletions.\nFirst, we describe the insertion phase. Given\nan incomplete (or empty) target sequence ⃗ yt and\na permutation of indices representing the genera-\ntion order ⃗ z, the Insertion Transformer generates\na sequence of insertion operations that produces a\ncomplete output sequence ⃗ yof length n. It does\nthis by iteratively extending the current sequence\n⃗ yt. In parallel inference, the model predicts a to-\nken to be inserted at each location[1,t]. We denote\ntokens by c∈C, where C represents the vocabu-\nlary and locations by l ∈{1,..., |⃗ yt|}. If the in-\nsertion model predicts the special symbol denoting\nan end-of-sequence, the insertions at that location\nstop. The insertion model will induce a distribu-\ntion of insertion edits of content cat location lvia\np(c,l|ˆyt).\nThe insertion phase is followed by the deletion\nphase. The deletion model deﬁnes a probability\ndistribution over the entire current hypothesis ⃗ yt,\nwhere for each token we capture whether we want\nto delete it. We deﬁne d ∈[0,1], where d = 0\ndenotes the probability of not deleting and d = 1\nof deleting a token. The model induces a dele-\ntion distribution p(d,l|⃗ yt) representing whether to\ndelete at each location l∈[0,|⃗ yt|].\nOne full training iteration consisting of an in-\nsertion phase followed by a deletion phase can be\nrepresented by the following steps:\n1. Sample a generation step i ∼\nUniform([1,n])\n2. Sample a partial permutation z1:i−1 ∼\np(z1:i−1) for the ﬁrst i−1 insertions\n3. Pass this sequence through the insertion\nmodel to get the probability distribution over\np(cz\ni |xz,i−1\n1:i−1) (denote ˆxt short for xz,i−1\n1:i−1).\n4. Insert the predicted tokens into the current se-\nquence ˆxt to get sequence xz,i−1+ni\n1:i−1+ni (where\nni denotes the number of insertions, shorten\nxz,i−1+ni\n1:i−1+ni by ˆx∗\nt ) and pass it through the dele-\ntion model.\n5. The output of the deletion model repre-\nsents the probability distribution p(dl |\nl,ˆx∗\nt ) ∀ l∈{1,...,t }\n2.1 Learning\nWe parametrize both the insertion and deletion\nprobability distributions with two stacked trans-\nformer decoders, where θi denotes the parame-\nters of the insertion model and θd of the dele-\ntion model. The models are trained at the same\ntime, where the deletion model’s signal is depen-\ndent on the state of the current insertion model.\nFor sampling from the insertion model we take\nthe argument that maximizes the probability of\nthe current sequence via parallel decoding: ˆcl =\narg maxc p(c,|l,ˆxt). We do not backpropagate\nthrough the sampling process, i.e., the gradient\nduring training can not ﬂow from the output of the\ndeletion model through the insertion model. Both\nmodels are trained to maximize the log-probability\nof their respective distributions. A graphical de-\npiction of the model is shown in Figure 1.\nSince the signal for the deletion model is depen-\ndent on the insertion model’s state, it is possible\nthat the deletion model does not receive a learning\nsignal during training. This happens when either\nthe insertion model is too good and never inserts\na wrong token, or when the insertion model does\nnot insert anything at all. To mitigate this problem\nwe propose an adversarial sampling method. To\nensure that the deletion model always has a signal,\nwith some probability padv we mask the ground-\ntruth tokens in the target for the insertion model\nduring training. This has the effect that when se-\nlecting the token to insert in the input sequence,\nbefore passing it to the deletion model, the inser-\ntion model selects the incorrect token it is most\nconﬁdent about. Therefore, the deletion model al-\nways has a signal and trains for a situation that it\nwill most likely also encounter during inference.\n3 Experiments\nWe demonstrate the capabilities of our Insertion-\nDeletion model through experiments on synthetic\ntranslation datasets. We show how the addition of\ndeletion improves BLEU score, and how the inser-\ntion and deletion model interact as shown in Ta-\nble 1. We found that adversarial deletion training\ndid not improve BLEU scores on these synthetic\ntasks. However, the adversarial training scheme\ncan still be helpful when the deletion model does\nnot receive a signal during training by sampling\nfrom the insertion model alone (i.e., when the\ninsertion-model does not make any errors).\n3.1 Learning shifted alphabetic sequences\nThe ﬁrst task we train the insertion-deletion model\non is shifting alphabetic sequences. For generation\nof data we sample a sequence length min n <=\nn < maxn from a uniform distribution where\nminn = 3 and maxn = 10. We then uniformly\nsample the starting token and ﬁnish the alphabetic\nsequence until it has length n. For a sampled\nn = 5and starting letter c, shifting each letter by\nmaxn to ensure the source and target have no over-\nlapping sequence, here is one example sequence:\nSource cdef g\nTarget mnopq\nWe generate 1000 of examples for training, and\nevaluate on 100 held-out examples. Table 2 re-\nports our BLEU. We train our models for 200k\nsteps, batch size of 32 and perform no model\nselection. We see our Insertion-Deletion Trans-\nformer model outperforms the Insertion Trans-\nformer signiﬁcantly on this task. One randomly\nchosen example of the interaction between the in-\nsertion and the deletion model during a decoding\nstep is shown in Table 1.\n3.2 Learning Caesar’s Cipher\nThe shifted alphabetic sequence task should be\ntrivial to solve for a powerful sequence to se-\nquence model implemented with Transformers.\nThe next translation task we teach the model is\nCaesar’s cipher. This is an old encryption method,\nin which each letter in the source sequence is re-\nplaced by a letter some ﬁxed number of positions\ndown the alphabet. The sequences do not need\nto be in alphabetic order, meaning the diversity\nof input sequences will be much larger than with\nthe previous task. We again sample a min n <=\nn <maxn, where minn = 3and maxn = 25this\ntime. We shift each letter in the source sequence\nby maxn = 25. If the sampled n is 5, we ran-\ndomly sample 5 letters from the alphabet and shift\neach letter in the target to the left by one character\nwe get the following example:\nSource hkbet\nTarget gjads\nWe generate 100k examples to train on, and\nevaluate on 1000 held-out examples. We train our\nmodels for 200k steps, batch size of 32 and per-\nform no model selection. The table below shows\nthat the deletion model again increases the BLEU\nscore over just the insertion model, by around 2\nBLEU points.\nCaesar’s Cipher BLEU\nInsertion Model (KERMIT) 35.55\nInsertion Deletion Model 37.57\nTable 3: BLEU scores for the Caesar’s cipher task.\n4 Conclusion\nIn this work we proposed the Insertion-Deletion\ntransformer, that can be implemented with a sim-\nple stack of two Transformer decoders, where\nFigure 1: Insertion-Deletion Transformer; reads from bottom to top. The bottom row are the source and target\nsequence, as sampled according to step 1 and 2 in Section 2.1. These are passed through the models to create an\noutput sequence. [CLS] and [SEP] are separator tokens, described in more detail in the BERT paper (Devlin et al., 2018).\nNote that allowing insertions on the input side is not necessary but trains a model that can be conditioned on the input sequence\nto generate the target as well as vice versa. For details refer to (Chan et al., 2019)\nInputs to insertion model[CLS] e f g h i j k l m [SEP] p [ ] [ ] [ ] v w [SEP]\nPredicted insertions o q r u u\nInputs to deletion model[CLS] e f g h i j k l m [SEP] o p q r u u v w [SEP]\nPredicted deletions u\nOutputs [CLS] e f g h i j k l m [SEP] o p q r u v w [SEP]\nTable 1: Example decoding iteration during inference. Here [ ] denotes a space and insertions are inserted to the\nleft of each token in the target sequence (occurring after [SEP]).\nAlphabetic Sequence Shifting BLEU\nInsertion Model (KERMIT) 70.15\nInsertion Deletion Model 91.49\nTable 2: BLEU scores for the sequence shifting task.\nthe top deletion transformer layer gets its sig-\nnal from the bottom insertion transformer. We\ndemonstrated the capabilities of the model on two\nsynthetic data sets and showed that the deletion\nmodel can signiﬁcantly increase the BLEU score\non simple tasks by iteratively reﬁning the out-\nput sequence via sequences of insertion-deletions.\nThe approach can be applied to tasks with variable\nlength input and output sequences, like machine\ntranslation, without any adjustments by allowing\nthe model to perform as many insertion and dele-\ntion phases as necessary until a maximum amount\nof iterations is reached or the model predicted an\nend-of-sequence token for all locations. In future\nwork, we want to verify the capabilities of the\nmodel on non-synthetic data for tasks like machine\ntranslation, paraphrasing and style transfer, where\nin the latter two tasks we can efﬁciently utilize the\nmodel’s capability of starting the decoding process\nfrom the source sentence and iteratively edit the\ntext.\nReferences\nWilliam Chan, Nikita Kitaev, Kelvin Guu, Mitchell\nStern, and Jakob Uszkoreit. 2019. KERMIT: Gen-\nerative Insertion-Based Modeling for Sequences. In\narXiv.\nKyunghyun Cho, Bart van Merrienboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Hol-\nger Schwenk, and Yoshua Bengio. 2014. Learn-\ning Phrase Representations using RNN Encoder-\nDecoder for Statistical Machine Translation. In\nEMNLP.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In CoRR.\nYue Dong, Zichao Li, Mehdi Rezagholizadeh, and\nJackie Chi Kit Cheung. 2019. EditNTS: An Neural\nProgrammer-Interpreter Model for Sentence Simpli-\nﬁcation through Explicit Editing. In ACL.\nJiatao Gu, Qi Liu, and Kyunghyun Cho. 2019a.\nInsertion-based Decoding with Automatically In-\nferred Generation Order. In arXiv.\nJiatao Gu, Changhan Wang, and Jake Zhao. 2019b.\nLevenshtein Transformer. In arXiv.\nSara Sabour, William Chan, and Mohammad Norouzi.\n2019. Optimal Completion Distillation for Se-\nquence Learning. In ICLR.\nMitchell Stern, William Chan, Jamie Kiros, and Jakob\nUszkoreit. 2019. Insertion Transformer: Flexible\nSequence Generation via Insertion Operations. In\nICML.\nIlya Sutskever, Oriol Vinyals, and Quoc Le. 2014.\nSequence to Sequence Learning with Neural Net-\nworks. In NIPS.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention Is All\nYou Need. In NIPS.\nSean Welleck, Kiante Brantley, Hal Daume, and\nKyunghyun Cho. 2019. Non-Monotonic Sequential\nText Generation. In ICML.\nYingce Xia, Fei Tian, Lijun Wu, Jianxin Lin, Tao Qin,\nNenghai Yu, and Tie-Yan Liu. 2017. Deliberation\nnetworks: Sequence generation beyond one-pass de-\ncoding. In NIPS.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7905937433242798
    },
    {
      "name": "Computer science",
      "score": 0.5387784838676453
    },
    {
      "name": "Insertion loss",
      "score": 0.4625169038772583
    },
    {
      "name": "Algorithm",
      "score": 0.3926602005958557
    },
    {
      "name": "Speech recognition",
      "score": 0.3466659188270569
    },
    {
      "name": "Voltage",
      "score": 0.20942562818527222
    },
    {
      "name": "Engineering",
      "score": 0.2033083736896515
    },
    {
      "name": "Electrical engineering",
      "score": 0.15423548221588135
    }
  ],
  "institutions": [],
  "cited_by": 6
}