{
  "title": "TemporalWiki: A Lifelong Benchmark for Training and Evaluating Ever-Evolving Language Models",
  "url": "https://openalex.org/W4385572901",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3092799944",
      "name": "Joel Jang",
      "affiliations": [
        "Korea Advanced Institute of Science and Technology",
        "Kootenay Association for Science & Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2150567914",
      "name": "Seonghyeon Ye",
      "affiliations": [
        "Kootenay Association for Science & Technology",
        "Korea Advanced Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2130429855",
      "name": "Changho Lee",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2253211678",
      "name": "Sohee Yang",
      "affiliations": [
        "Korea Advanced Institute of Science and Technology",
        "Kootenay Association for Science & Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2511335435",
      "name": "Joongbo Shin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2261214712",
      "name": "Jang-Hoon Han",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2125332804",
      "name": "Gyeong-Hun Kim",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2252216270",
      "name": "Minjoon Seo",
      "affiliations": [
        "Kootenay Association for Science & Technology",
        "Korea Advanced Institute of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3153094109",
    "https://openalex.org/W3202099651",
    "https://openalex.org/W4225933709",
    "https://openalex.org/W2898875342",
    "https://openalex.org/W3195376057",
    "https://openalex.org/W4205179624",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4287111051",
    "https://openalex.org/W3211777899",
    "https://openalex.org/W3207715300",
    "https://openalex.org/W3156476125",
    "https://openalex.org/W4287855128",
    "https://openalex.org/W1682403713",
    "https://openalex.org/W3169283738",
    "https://openalex.org/W3007672467",
    "https://openalex.org/W2037979274",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4307106501",
    "https://openalex.org/W2151835407",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W3213460052",
    "https://openalex.org/W2795247881",
    "https://openalex.org/W3104215796",
    "https://openalex.org/W4226082499",
    "https://openalex.org/W2963961878",
    "https://openalex.org/W3217756080",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W4212964822",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W2150299430",
    "https://openalex.org/W4285190530",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W4287332702",
    "https://openalex.org/W3171460770",
    "https://openalex.org/W2560647685",
    "https://openalex.org/W3153269634",
    "https://openalex.org/W3175604467",
    "https://openalex.org/W3205068155"
  ],
  "abstract": "Language Models (LMs) become outdated as the world changes; they often fail to perform tasks requiring recent factual information which was absent or different during training, a phenomenon called temporal misalignment. This is especially a challenging problem because the research community still lacks a coherent dataset for assessing the adaptability of LMs to frequently-updated knowledge corpus such as Wikipedia. To this end, we introduce TemporalWiki, a lifelong benchmark for ever-evolving LMs that utilizes the difference between consecutive snapshots of English Wikipedia and English Wikidata for training and evaluation, respectively. The benchmark hence allows researchers to periodically track an LM's ability to retain previous knowledge and acquire updated/new knowledge at each point in time. We also find that training an LM on the diff data through continual learning methods achieves similar or better perplexity than on the entire snapshot in our benchmark with 12 times less computational cost, which verifies that factual knowledge in LMs can be safely updated with minimal training data via continual learning.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 6237–6250\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nTemporalWiki: A Lifelong Benchmark for Training and Evaluating\nEver-Evolving Language Models\nJoel Jang1,∗ Seonghyeon Ye1,∗ Changho Lee3 Sohee Yang1\nJoongbo Shin2 Janghoon Han2 Gyeonghun Kim2 Minjoon Seo1\n1KAIST 2LG AI Research 3Korea University\n{joeljang,seonghyeon.ye,sohee.yang,minjoon}@kaist.ac.kr\nckdgh0801@korea.ac.kr {jb.shin,janghoon.han,ghkayne.kim}@lgresearch.ai\nAbstract\nLanguage Models (LMs) become outdated\nas the world changes; they often fail to per-\nform tasks requiring recent factual information\nwhich was absent or different during training,\na phenomenon called temporal misalignment.\nThis is especially a challenging problem be-\ncause the research community still lacks a co-\nherent dataset for assessing the adaptability\nof LMs to frequently-updated knowledge cor-\npus such as Wikipedia. To this end, we intro-\nduce TEMPORAL WIKI , a lifelong benchmark\nfor ever-evolving LMs that utilizes the differ-\nence between consecutive snapshots of English\nWikipedia and English Wikidata for training\nand evaluation, respectively. The benchmark\nhence allows researchers to periodically track\nan LM’s ability to retain previous knowledge\nand acquire updated/new knowledge at each\npoint in time. We also find that training an\nLM on the diff data through continual learning\nmethods achieves similar or better perplexity\nthan on the entire snapshot in our benchmark\nwith 12 times less computational cost, which\nverifies that factual knowledge in LMs can be\nsafely updated with minimal training data via\ncontinual learning. The dataset and the code is\nmade available at this link.\n1 Introduction\nLarge Language Models (LMs) pretrained on a vast\namount of text corpus have shown to be highly ef-\nfective when finetuned or prompted to perform var-\nious downstream tasks (Raffel et al., 2019; Brown\net al., 2020; Sanh et al., 2022; Wei et al., 2022).\nHowever, most of the datasets used to evaluate\nthese LMs are static benchmarks; the train and test\ndata are both from similar points in time. On the\nother hand, in the real world, factual knowledge\nis frequently changed, added, or deprecated. For\nexample, suppose a language model is asked what\nthe most dominant coronavirus variant is (Figure 1).\n∗indicates equal contribution.\nThe answer would have been the Delta variant in\nthe fall of 2021 but has changed to the Omicron\nvariant near the end of 2021. If LMs remain un-\nchanged and are not periodically trained to cope\nwith the changing world, they will be outdated very\nquickly. This means downstream tasks that directly\ndepend on or are finetuned from the LM will suf-\nfer from temporal misalignment (Luu et al., 2021;\nLazaridou et al., 2021), which refers to the mis-\nalignment in time between the train and test data.\nTemporal misalignment becomes a critical prob-\nlem, especially when using language models for\nknowledge-intensive tasks such as closed-book\nquestion answering (Roberts et al., 2020; Petroni\net al., 2021; Jang et al., 2022) since they rely\nsolely on the knowledge stored in their parame-\nters. Furthermore, LMs augmented with retrieval\nmechanism (Guu et al., 2020; Lewis et al., 2020;\nBorgeaud et al., 2021) often suffer from hallucina-\ntion even if they successfully retrieve up-to-date\ninformation (Zhang and Choi, 2021; Chen et al.,\n2021; Longpre et al., 2021). This means that the\nimplicit knowledge stored in the model parameters\nhas to be updated as well because it may cause con-\nflicts with the explicit knowledge retrieved from ex-\nternal sources such as up-to-date knowledge bases\nand ultimately cause the LM to hallucinate.\nRecently, Lazaridou et al. (2021); Jang et al.\n(2022) have explored updating the internal knowl-\nedge of LMs through continual pretraining on new\nand updated data as a solution for mitigating tem-\nporal misalignment. However, these datasets are\nstill static in nature: as the world changes, they will\neventually get outdated as well. In order to compre-\nhensively measure the capability of ever-evolving\nLMs on addressing temporal misalignment, auto-\nmated periodic evaluation of the LMs is crucial.\nIn this paper, we introduce TEMPORAL WIKI , a\nlifelong benchmark for training and evaluating ever-\nevolving LMs in a periodic and automated manner,\nshown in Figure 1. The corpora used for updating\n6237\nWhat is the most \ndominant COVID-19 \nvariant?\nWhat is the most \ndominant COVID-19 \nvariant?\nDiﬀerence\nDiﬀerence\n11.202110.2021\n11.202110.2021\nTWiki-Diﬀsets\nTWiki-Probes\nalignment\nOmicron variantDelta variant \nUpdating\nEvaluating\n09.2021 10.2021 11.2021 12.2021\nFigure 1: An overview of using TEMPORAL WIKI , consisting of TWIKI -DIFFSETS and TWIKI -PROBES to train and evaluate\never-evolving LMs, respectively. Differences between Wikipedia snapshots at different points in time are used for temporal\nlanguage modeling, and categorized factual instances in the corresponding Wikidata snapshots are used for temporal evaluation.\nLMs are constructed by comparing articles from\nconsecutive English Wikipedia snapshots and re-\ntrieving only changed information, which we name\nas TWIKI -DIFFSETS . The evaluation datasets are\nconstructed in a similar manner by comparing En-\nglish Wikidata snapshots that correspond to the\nWikipedia snapshots in time and categorizing each\nfactual instance into UNCHANGED or CHANGED .\nSince Wikidata updates may not exactly align with\nWikipedia updates, we only retain factual instances\nthat can be grounded to articles in Wikipedia, ensur-\ning the quality of the data and name the resulting\nevaluation dataset as TWIKI -PROBES . The entire\nbenchmark creation process is done without any\nhuman annotation, thus allowing it to be automated\nand lifelong as new English Wikipedia and English\nWikidata snapshots are released by Wikimedia1 on\na monthly basis.\nThrough TEMPORAL WIKI , we aim to tackle the\nfollowing research questions: How can we train\never-evolving LMs efficiently and automate the\nevaluation of each update? How does updating\nLMs only on updated data from Wikipedia com-\npare to updating LMs on entire Wikipedia snap-\nshots, especially in scenarios requiring multiple\nupdates? How problematic is catastrophic forget-\nting (McCloskey and Cohen, 1989) when LMs are\nupdated only on new data, and how can we effec-\ntively mitigate catastrophic forgetting? Our main\n1https://commons.wikimedia.org/\ncontributions are summarized as follows:\n• We introduce TEMPORAL WIKI , a lifelong\nbenchmark for ever-evolving LMs. Unlike\nprevious static benchmarks, TEMPORAL WIKI\nis responsive to the dynamic changes in the\nworld and can be utilized to automatically\ntrain and evaluate ever-evolving LMs on each\nEnglish Wikipedia and English Wikidata snap-\nshot update.\n• We find that continually training LMs only\non the updated portion of English Wikipedia,\nwhich we call temporal language modeling,\nis much more efficient than updating LMs on\nentire English Wikipedia snapshots in terms\nof both computation and stability-plasticity\ntrade-off. It is still a challenging task, espe-\ncially when multiple updates are required due\nto catastrophic forgetting.\n• As competitive baselines for temporal lan-\nguage modeling, we implement previous con-\ntinual learning approaches that mitigate for-\ngetting while bolstering the learning of new\nknowledge, thus providing an overall enhance-\nment in terms of both stability and plasticity.\nWe hope that TEMPORAL WIKI will foster future\nresearch towards training ever-evolving LMs.\n6238\n2 Background\nRecent works have introduced the need to tackle\nthe issue of temporal misalignment, which refers\nto neural networks showing poor performance due\nto misalignment in time between the train and test\ndata. Temporal misalignment can be caused either\nby (1) the dynamic nature of language (Röttger and\nPierrehumbert, 2021; Hombaiah et al., 2021; Rosin\net al., 2021; Loureiro et al., 2022) or (2) the update\nof factual information (Chen et al., 2021; Dhingra\net al., 2022; Jang et al., 2022).\nLuu et al. (2021) have emphasized the effect\nof temporal misalignment on eight different NLP\ndownstream tasks, asserting that misalignment be-\ntween the train and test sets of the downstream\ntasks causes severe performance degradation that\ncan be mitigate finetuning on the corpus from the\ntarget period. Agarwal and Nenkova (2021) have\nargued this to be less of a concern when utiliz-\ning representations from pretrained LMs and show\nthat self-labeling on the downstream task is more\neffective than continued pretraining on more re-\ncent data for temporal adaptation. Note that these\nworks have focused on misalignment caused by the\ndynamic nature of language on tasks that are not\nknowledge-intensive, such as text classification.\nOthers have tackled the problem caused by the\nupdate of factual knowledge. Lazaridou et al.\n(2021) have shown that LMs deteriorate signifi-\ncantly in performance when there is a misalign-\nment in time between the pretraining data and the\ndownstream task and argued ever-evolving LMs\nare necessary. Dhingra et al. (2022) have proposed\nexplicitly including time information during pre-\ntraining as a potential solution. Jang et al. (2022);\nJin et al. (2022) have implemented continual learn-\ning methods to mitigate catastrophic forgetting that\noccurs during continued pretraining on new data.\nDespite the recent community interest in the\nneed for ever-evolving LMs, the community lacks\nwidely-available resources to train and evaluate\nsuch LMs. Previous works have introduced bench-\nmarks comprised of data sources from Twitter\nfeeds (Osborne et al., 2014; Yogatama et al., 2014;\nLoureiro et al., 2022), recent news articles (Jang\net al., 2022), and arXiv papers (Lazaridou et al.,\n2021) where the temporal adaptability of LMs and\nthe effectiveness of different methodologies of up-\ndating LMs can be evaluated. However, these data\nsources are domain-specific and inherently static.\nOn the other hand, Wikipedia and Wikidata are\nknown to be great sources of general world knowl-\nedge and thus have been widely used by the com-\nmunity (Dinan et al., 2019; Thorne et al., 2018;\nKwiatkowski et al., 2019; Piktus et al., 2021). 120K\nvolunteer editors make 120 updates to the English\nWikipedia per minute and add hundreds of new\narticle entries every day (Logan IV et al., 2021)2.\nEven though every Wikipedia and Wikidata update\nmay not correspond to an actual change in the real\nworld, TEMPORAL WIKI leverages the dynamic na-\nture of Wikipedia and Wikidata to provide a life-\nlong benchmark for developing and maintaining\never-evolving LMs.\n3 TemporalWiki\nIn this section, we delve into the process of creat-\ning TEMPORAL WIKI , which is comprised of train-\ning corpora ( TWIKI -DIFFSETS ) and evaluation\ndatasets (TWIKI -PROBES ) sourced from English\nWikipedia and English Wikidata, respectively. For\nefficiency, English is abbreviated when referring to\nEnglish Wikipedia and English Wikidata through-\nout the paper. Moreover, we clarify that not all\nWikipedia/Wikidata updates equate to actual up-\ndates of world knowledge. In Section 3.1, we first\ndescribe the process of constructing the training\ncorpora from Wikipedia snapshots. Then in Sec-\ntion 3.2, we describe the process of generating the\nevaluation datasets from Wikidata snapshots. In\nSection 3.3, we describe the quality control applied\nto the evaluation datasets.\n3.1 Generating Training Corpora from\nWikipedia\nIt is highly computationally expensive to train an\nLM on the entire Wikipedia snapshot every time the\nLM requires updates since most part of Wikipedia\nis unchanged from the previous snapshot. More-\nover, it is not certain whether training on whole\nsnapshot is the best approach for updating the\nfactual knowledge stored in the LM. Therefore,\nwe compare the differences between consecutive\nWikipedia snapshots in order to use only updated\nand new text for training. We call these subsets\nTWIKI -DIFFSETS . Algorithm 1 shows the proce-\ndure for generating them.\nAs shown in Algorithm 1, a single TWIKI -\nDIFFSET is generated by getting the differences\n(similarly to git diff) between two consecu-\ntive Wikipedia snapshots. If an article with a\n2https://en.wikipedia.org/wiki/Wikipedia:Statistics\n6239\nAlgorithm 1 Generating TWIKI -DIFFSETS\nRequire: Wikipedia snapshots W Pprev and W Precent where\nW Precent is more recent.\nD := An empty array to store new and updated data.\n*article in W Phas attributes id and text\nfor all article ar ∈W Precent do\nif ar.id = ap.id for some article a p ∈W Pprev then\nD.append(GETDIFF(ap,ar))\nelse\nD.append(ar)\nend if\nend for\nfunction GETDIFF(ap,ar)\nDi f f := An empty string to append difference between text\nin two articles.\nfor all paragraph pr ∈ar.text do\nif pr have no matching sentences with any paragraph\npp ∈ap.text then\nDi f f ←Di f f + pr\nelse if pr have some matching and some different sen-\ntences with any paragraph pp ∈ap.text then\nDi f f ←Di f f + sentences that differ between pr and\npp.\nend if\nend for\nreturn Di f f\nnew unique id is included in the recent snapshot,\nwe append the entire article to TWIKI -DIFFSET .\nFor an article having an existing id in the pre-\nvious snapshot, we compare the two articles by\nparagraphs and add new or updated sentences to\nTWIKI -DIFFSET . Examples of TWIKI -DIFFSET\nare shown in Figure 5, and detailed statistics are\nshown in Section 4.\n3.2 Generating Evaluation Datasets from\nWikidata\nThe success of a LM update for continual pre-\ntraining setting can be evaluated by quantifying\nthe stability-plasticity dilemma (Mermillod et al.,\n2013): the dilemma of neural models having to sac-\nrifice either stability, ability to retain learned knowl-\nedge, or plasticity, ability to obtain new knowl-\nedge. In order to evaluate whether each update is\nsuccessful, we need evaluation datasets that can\nquantify the amount of changed (updated or new)\nknowledge successfully gained (plasticity) and the\namount of knowledge that remains unchanged as\nintended (stability). Therefore, we categorize fac-\ntual instances from Wikidata snapshots that are\ntemporally aligned with Wikipedia snapshots and\ncall the resulting datasets TWIKI -PROBES .\nWikidata snapshots are structured knowledge\ngraphs that store factual information in the form\nof ( Subject, Relation, Object) such as\n(Barack Obama, born-in, Hawaii). These\nfactual instances can be used to probe the LM for\nfactual knowledge (Petroni et al., 2019). Through\nAlgorithm 2, we distinguish each factual instance\ninto either UNCHANGED or CHANGED .\nAlgorithm 2 Generating TWIKI -PROBES\nRequire: Wikidata snapshots W Dprev and W Drecent where\nW Drecent is more recent.\nUn, C := Arrays that store UNCHANGED and CHANGED\nfactual instances, respectively.\nfor all fact (sr,rr,or) ∈W Drecent do\nP ←{(s,r,o) |s = sr where (s,r,o) ∈W Dprev}\nif P = / 0then\nC.append(sr,rr,or)\nelse if rr /∈P then\nC.append(sr,rr,or)\nelse if r = rr and o = or for some(s,r,o) ∈P then\nUn.append(sr,rr,or)\nelse\nC.append(sr,rr,or)\nend if\nend for\nAs shown in Algorithm 2, given two consecu-\ntive Wikidata snapshots, a single TWIKI -PROBE is\nconstructed, which is used to evaluate an LM up-\ndated with TWIKI -DIFFSET . Algorithm 2 catego-\nrizes instances with new Relation or instances\nwith the same Relation, but a new Object\ninto CHANGED , and unchanged instances into UN-\nCHANGED .\n3.3 Quality Control for Evaluation Data\nWe apply several quality control steps to the cate-\ngorized factual instances from Section 3.2 to reflect\nthe actual knowledge change from the LM update.\nAlignment with TWIKI -DIFFSETS We ensure\ncorrect alignment of CHANGED instances with arti-\ncles in TWIKI -DIFFSETS and UNCHANGED in-\nstances with articles from the entire Wikipedia\nsince Wikidata updates do not necessarily entail\nWikipedia updates and vice versa. In order to do\nthis, we take three steps. Step #1: We crawl infor-\nmation from each Wikipedia article page to find the\nmapping to the corresponding Wikidata entity id\nand store the information as a dictionary. Step #2:\nThen, for each factual instance from CHANGED ,\nwe check if the Subject id can be mapped to an\narticle from TWIKI -DIFFSETS using the dictionary\nof id mappings. Likewise, for each instance from\nUNCHANGED , we check if the Subject id can\nbe mapped to an article from Wikipedia. Step #3:\nLastly, for a successfully mapped factual instance\nfrom Step 2, we finally keep the instances where\n6240\nTable 1: Statistics of TWIKI -DIFFSETS . The two digits indi-\ncate the month of the year 2021 that the Wikipedia snapshot\nwas obtained from. The four digits for WIKI -DIFFSET indi-\ncate the months of the two snapshots being compared. For\ninstance, TWIKI -DIFFSET -0809 indicates the difference be-\ntween August (08) and September (09).\n# of Articles # of Tokens\nWIKIPEDIA-08 6.3M 4.6B\nTWIKI-DIFFSET-0809 306.4K 347.29M\nWIKIPEDIA-09 6.3M 4.6B\nTWIKI-DIFFSET-0910 299.2K 347.96M\nWIKIPEDIA-10 6.3M 4.7B\nTWIKI-DIFFSET-1011 301.1K 346.45M\nWIKIPEDIA-11 6.3M 4.6B\nTWIKI-DIFFSET-1112 328.9K 376.09M\nWIKIPEDIA-12 6.3M 4.7B\nTable 2: Detailed Statistics of TWIKI -PROBES during con-\nstruction. Un and C represents UNCHANGED and CHANGED\nfactual instances, respectively.\nInitial Categorization→ Alignment→Heuristic Filtering\nMonth Un C Un C Un C\n0809 514,017 1,209,272 10,133 2,329 6,935 1,7760910 544,708 1,196,806 10,625 2,621 7,340 1,9821011 460,228 1,572,778 10,544 1,742 7,313 1,3581112 463,623 1,653,709 10,580 3,472 7,293 1,951\nObject exists in the text of the article.\nHeuristic Filtering In addition to the alignment\nwith TWIKI -DIFFSETS , in order to further ensure\nthe quality of the evaluation datasets, we apply\nthree heuristic filtering rules to strengthen the qual-\nity of the data. Rule #1: We remove the instances\nwhere either SUBJECT or OBJECT is a substring\nof the other. Rule #2: We remove the instances\nwhere OBJECT contains more than 5 words. Rule\n#3: We limit the proportion of single SUBJECT to\nhave 1% of the total, and RELATION and OBJECT\nby 5% of the total. Table 4 shows some examples\nof TW IKI -PROBES after quality control.\n4 Dataset Statistics\nIn this paper, we construct TEMPORAL WIKI from\n08.2021 to 12.20213 and its statistics are discussed\nbelow.\nTraining Corpora Statistics Statistics of\nWikipedia snapshots and TWIKI -DIFFSETS are\nshown in Table 1. An interesting aspect of TWIKI -\nDIFFSETS is that the amount of information being\nupdated and added (i.e., number of tokens in each\nsubset) is similar for each month.\nEvaluation Dataset Statistics The statistics of\nTWIKI -PROBES from the initial categorization\n3As new Wikipedia and Wikidata dumps are available on\na monthly basis, we provide the source code for constructing\nnew TWIKI -DIFFSETS and TWIKI -PROBES at this link\nfrom Algorithm 2 and quality control are shown in\nTable 24. For further analysis, we break down the\nentity types of Subject and Object, and ob-\nserve a similar proportion of each entity category\nfor each month of TWIKI -PROBES (Appendix B).\nWe also show the distribution of the top 30 most fre-\nquent Relation of UNCHANGED and CHANGED\n(Appendix C).\n5 Experiments with T EMPORAL WIKI\nIn this section, we train and evaluate ever-evolving\nLMs with TEMPORAL WIKI . Section 5.1 describes\nthe experimental settings. Section 5.2 describes the\nbaseline methodologies for updating LMs. Section\n5.3 shows evaluation results on the training corpora.\nSection 5.4 presents the experimental results on\nTWIKI -PROBES .\n5.1 Experimental Settings\nFor our baseline language model (LM), we con-\ntinue pretraining GPT-2 Large (Radford et al.,\n2019) (774M parameters). We first compare the\nbaseline performances between updating GPT-2\nwith TWIKI -DIFFSETS and updating it with entire\nWikipedia snapshots and evaluate each update us-\ning TWIKI -PROBES . We also implement continual\nlearning methods from literature known for mitigat-\ning catastrophic forgetting that occurs when updat-\ning GPT-2 with only TWIKI -DIFFSETS . Further\ndetailed configuration of the experimental settings\nis provided in Appendix D.\n5.2 Baseline Models\nHere we describe the baseline methods used for\ntraining and evaluation, namely INITIAL , FULL ,\nDIFF, RECADAM, MIX-REVIEW , K-A DAPTER ,\nand LORA.\nInitial As the starting model checkpoint for all of\nthe experiments, we continually pretrain pretrained\nGPT-2 from Radford et al. (2019) on the 08.2021\nWikipedia snapshot for four epochs in total (around\n546K global steps) so that the initial GPT-2 used\nfor all of the experiments is updated with the last\ntwo years of world knowledge. We denote this\ncheckpoint as INITIAL , and it serves as the initial\ncheckpoint for all of the other methods.\n4A single Wikidata snapshot is comprised of 93 million\ndistinct entities, where there are around 30 facts for each entity\nwhich amounts to roughly 2.8 billion factual instances. Since\nmost instances from Algorithm 2 are categorized into UN-\nCHANGED , we randomly sample 0.1% of the factual instances\nafter applying Algorithm 2.\n6241\nFull We start from INITIAL and continue pre-\ntraining it on the entire Wikipedia snapshot of each\nmonth in a sequential manner. For example, after\ntraining on the 09.2021 Wikipedia snapshot from\nINITIAL , we continue training it on the 10.2021\nWikipedia snapshot and move on to the next snap-\nshot. We denote the resulting model as FULL . We\niterate through the training data only once, which\ncorresponds to an average of 4.6 billion token up-\ndates (140K global steps) for each month.\nDiff We start fromINITIAL and continue pretrain-\ning it on TWIKI -DIFFSETS in a sequential manner.\nWe denote the resulting model as DIFF. Similarly\nto FULL , we iterate through the training data only\nonce, which is an average of 347 million token\nupdates (12K global steps) for each month.\nRecAdam We implement a regularization-based\ncontinual learning method for training large LMs\ncalled RECADAM (Chen et al., 2020) which\nplaces a stronger independent assumption among\nthe model parameters, overcoming the limita-\ntions of implementing traditional methods such as\nEWC (Kirkpatrick et al., 2017) for training large\nlanguage models. We set the hyperparameters of\nthe optimizer identical to the original implementa-\ntion.\nMix-review We implement a rehearsal-based\ncontinual learning method for training large LMs\ncalled MIX-REVIEW (He et al., 2021) which mixes\nin random subsets of the initial pretraining data\n(08.2021 Wikipedia data). We fix the mix-ratio as\n2 in our experiments.\nLoRA We implement a parameter-expansion-\nbased continual learning method called LORA (Hu\net al., 2022) which freezes the original parameters\nwhile adding trainable rank-decomposition matri-\nces into each layer. We use hyperparameters identi-\ncal to the optimal setting of the original implemen-\ntation.\nK-Adapter We implement another parameter-\nexpansion-based continual learning method, K-\nADAPTER (Wang et al., 2021), which freezes\nthe original parameters while adding additional\nadapters (an increase of 103M parameters) to the\nLM. 5\n5We add the additional parameters once for the updates\nfrom 08.2021. Exploring the optimal interval to add parame-\nters for ever-evolving LMs is left for future work.\n(a) NON-TW IKI -DIFFSETS\n(b) TWIKI -DIFFSETS\nFigure 2: Relative proper noun perplexity of FULL , DIFF, and\nK-A DAPTER , LORA, RECADAM and MIX-REVIEW com-\npared to INITIAL on TWIKI -DIFFSETS and NON-TW IKI -\nDIFFSETS for each month. Lower ratio indicates better per-\nformance. The performance of DIFF (orange) and RECADAM\n(yellow) in (b) is is almost identical.\n5.3 Intrinsic Evaluation\nWe first perform intrinsic evaluation by measur-\ning the perplexity of the baseline models on their\ntraining corpora. For each month, we measure\nthe model’s perplexity on TWIKI -DIFFSETS and\nNON-TW IKI -DIFFSETS , where the latter refers to\nthe subset of the month’s entire Wikipedia snap-\nshot that does not include the data from TWIKI -\nDIFFSETS . We sample 10,000 input instances from\neach subset with a fixed length of 512 and measure\nthe perplexity on proper noun tokens determined by\na Part-of-Speech (POS) tagger (Honnibal and Mon-\ntani, 2017) as in (Lazaridou et al., 2021), which\ncan be considered as a proxy for tokens contain-\ning factual knowledge. Therefore, the result on\nNON-TW IKI -DIFFSETS is meant to indicate the\nperformance on unchanged knowledge, while the\nresult on TWIKI -DIFFSETS corresponds to updated\nand new knowledge. Figure 2 shows the relative\nperplexity of each baseline method compared to\nINITIAL (i.e., dividing each model by INITIAL , and\nthus the lower, the better).\nResults on NON-TW IKI -DIFFSETS show that\nthe relative perplexity of DIFF increases while\nthat of FULL remains constant as time goes on,\n6242\nTable 3: Zero-shot perplexity of LMs measured on TWIKI -PROBES . Time represents the average training time of a single update\nunder the setting described in Section 5.1. The description of each baseline model is explained in Section 5.2. Best performance\nis marked as bold while the second best is underlined.\nTWiki-Probes-0809 TWiki-Probes-0910 TWiki-Probes-1011 TWiki-Probes-1112\nTime Un C Avg Un C Avg Un C Avg Un C Avg\nINITIAL 0 hours 386.16 364.82 375.49 356.66416.32 386.49 350.54 420.52 385.53 357.37 451.74 404.56\nFULL ∼24 hours 379.43 360.46 369.95 388.85 437.15 413.00 337.34383.06 360.20381.11 435.47 408.29\nDIFF ∼2.5 hours 409.31 284.34 346.83 409.86 336.55373.21 465.20 367.72 416.46 391.77 365.07 378.42\nRECADAM ∼4 hours 358.10253.07 305.59376.12306.64341.38439.14 338.17388.66 400.56 356.60378.58\nMIX-REVIEW ∼6 hours 337.59274.91306.25394.20 381.21 387.71 375.85 369.50 372.68313.94 323.49 318.72\nLORA ∼2 hours 386.52 332.98 359.75 359.54 371.03 365.29 381.80 391.66 386.73 361.42 408.19 384.81\nK-ADAPTER ∼2 hours 340.47297.39 318.93326.53338.16332.35 325.11 332.61 328.86333.53374.67 354.10\nFigure 3: Average overall perplexity of TWIKI -PROBES . We\naverage the perplexities of UNCHANGED and CHANGED with\nequal importance placed on stability and plasticity. The x-axis\ndepicts the two-month intervals. A lower score indicates better\nperformance.\nwhich implies that forgetting occurs when the LM\nis trained with TWIKI -DIFFSETS . The relative per-\nplexities of continual learning methods increase\nless rapidly than DIFF, which means that applying\ncontinual learning mitigates catastrophic forgetting.\nMIX-REVIEW , especially, shows the least amount\nof forgetting among the continual learning methods,\nwhich indicates that training on the past corpus is\neffective in retaining performance on the previous\ntraining corpora in terms of perplexity.\nOn the other hand, the results on TWIKI -\nDIFFSETS show the opposite trend: the relative\nperplexity of DIFF is much lower than FULL .\nOne thing to note is that the perplexity of FULL\nis very similar to that of INITIAL on TWIKI -\nDIFFSETS , which suggests that updating LMs on\nentire Wikipedia snapshots hinders the effective\nlearning of changed data compared to DIFF, despite\nboth having seen the same instances of TWIKI -\nDIFFSETS during training for the same number\nof iterations. Among continual learning meth-\nods, K-A DAPTER and LORA shows higher overall\nperplexities than DIFF while MIX-REVIEW and\nRECADAM shows similar perplexity.\n5.4 Extrinsic Evaluation on TW IKI -PROBES\nPerforming only intrinsic evaluation on the training\ncorpora is not sufficient because the intrinsic eval-\nuation itself only tests the capability of the LMs\nfor memorization (McCoy et al., 2021). Through\nextrinsic evaluation with TWIKI -PROBES (Sec-\ntion 3.2), we specifically focus on evaluating fac-\ntual knowledge of the LMs from each update. Plac-\ning equal importance on stability (UNCHANGED )\nand plasticity (CHANGED ), we show the average of\nthe perplexities of UNCHANGED and CHANGED as\nwell as individual perplexities in Table 3, and show\na bar graph of the average perplexities in Figure 36.\nAs shown in Table 3, DIFF and all continual\nlearning methods show better overall performance\non CHANGED factual instances than INITIAL in\nall months, bolstering the results from the intrin-\nsic evaluation. For UNCHANGED , however, DIFF\nsuffers from catastrophic forgetting, showing con-\nsistent performance degradation as the number of\nupdates increases. In contrast, continual learning\nmethods effectively mitigate much of the catas-\ntrophic forgetting during temporal language model-\ning, resulting in lower perplexity on UNCHANGED ,\nexcept RECADAM which performs worse as the\nnumber of updates increases. K-A DAPTER , espe-\ncially, shows surprising results on UNCHANGED ,\noutperforming even FULL throughout all of the\nmonths. Moreover, all continual learning methods\nsurpass or are on par with DIFF on CHANGED fac-\ntual instances, showing that ability to learn new\nknowledge (plasticity) is not sacrificed to preserve\nprevious knowledge (stability).\nMoreover, as shown in the average perplexity\ncolumn of Table 3 and Figure 3, K-A DAPTER\nshows the most robust performance throughout\nthe time periods. It is important to note that K-\nADAPTER is around 12 times more computationally\nefficient than FULL in terms of total training time,\nunder the same computational constraint. DIFF also\n6The perplexity of UNCHANGED and CHANGED were\neach calculated by measuring the average perplexity of gener-\nating each factual instances.\n6243\n(a) FULL\n (b) DIFF\n (c) K-A DAPTER\nFigure 4: The zero-shot perplexity of the LMs updated and evaluated on various time intervals of CHANGED of TWIKI -PROBES ,\nshowing the effect of temporal misalignment. The better the results, the darker the performance is colored. The vertical axis\nrepresents the Trai.\noutperforms FULL in all months but 1011, show-\ning that temporal language modeling itself is an\neffective approach for overall stability-plasticity\ntrade-off.\nWe note that, as also shown in previous\nworks (Lazaridou et al., 2021), results in Table\n3 present an overall high perplexity (>200) because\nthe sentences in TWIKI -PROBES are not natural\nsentences; they are factual phrases synthetically\ngenerated from a naive concatenation ofSubject,\nRelation, and Object. We address this issue\nvia light-tuning in Appendix E.\nEffect of Temporal Misalignment We quan-\ntify the effect of temporal misalignment on each\nmethod by training the LMs and evaluating their\nzero-shot perplexity on CHANGED instances of\nTWIKI -PROBES with various time intervals of\ntraining and evaluation. Among continual learn-\ning methods, we select K-A DAPTER since it shows\nthe most robust performance for extrinsic evalua-\ntion across all time periods. As shown in Figure\n4, FULL method is mostly influenced by the num-\nber of training updates and not much by whether\nthere is temporal alignment. Since FULL is contin-\nuously pretrained on the entire Wikipedia corpus\nin each month, it would have likely seen the data\ncontaining CHANGED factual instances multiple\ntimes, leading to lower perplexity as training steps\nincreases.7 For DIFF and K-A DAPTER , there is a\ngeneral trend of strong performance when there is\ntemporal alignment (diagonal entries), outperform-\ning FULL with much fewer global training steps.\nIt is important to note that K-A DAPTER shows ro-\nbustness against temporal misalignment, i.e., the\nperplexity does not increase much even when the\n7Although directly training INITIAL on the whole\nWikipedia corpus of a specific month can be an alternative,\nwe exclude it here because it would only learn the knowl-\nedge of the specific month and thus inappropriate for a truly\never-evolving setting.\ntraining and evaluation months do not match, com-\npared to DIFF which suffers from a more severe\nperplexity spike.\n6 Conclusion\nIn this paper, we provide answers to the four pro-\nposed questions in Section 1. (1) How can we\ntrain ever-evolving LMs efficiently and automate\nthe evaluation of each update? We introduce TEM-\nPORAL WIKI , a lifelong benchmark that can be\nused for training and evaluating ever-evolving LMs\nin an automated manner. It consists of TWIKI -\nDIFFSETS as the training corpora for temporal lan-\nguage modeling and TWIKI -PROBES as the evalu-\nation datasets for measuring the stability-plasticity\ntrade-off. (2) How does updating LMs only on new\nand updated data from Wikipedia compare to updat-\ning LMs on entire Wikipedia snapshots, especially\nin scenarios with multiple updates? Through exper-\niments on TEMPORAL WIKI , we show that updating\nLMs on TWIKI -DIFFSETS leads to better acquisi-\ntion of new and updated knowledge than updating\non entire Wikipedia snapshots with much less com-\nputational cost (12 times less). (3) How serious is\ncatastrophic forgetting when LMs are updated only\non new and updated data? We observe that tem-\nporal language modeling is a challenging problem,\nespecially as the number of LM updates increases.\nHowever, results still show an overall enhancement\nin terms of stability-plasticity compared to updat-\ning with entire Wikipedia snapshots, showing that\ntemporal language modeling is an effective alterna-\ntive. (4) How can we mitigate catastrophic forget-\nting? We find that continual learning methods (reg-\nularization, rehearsal, and parameter-expansion)\nfor large language model training effectively miti-\ngates forgetting and shows robust performance in\nterms of enhancing the overall trade-off between\nstability and plasticity on TWIKI -PROBES .\n6244\n7 Limitations\nAs mentioned at the beginning of this Section, each\nWikipedia and Wikidata update does not ensure an\nactual update of real-world knowledge. For exam-\nple, an addition of a new Wikipedia page does not\nnecessarily mean that all the information on the\nnew page is new world knowledge. Likewise, exist-\ning factual knowledge may be added to Wikidata\nbecause Wikipedia and Wikidata do not cover all of\nthe world knowledge and may have some missing\ninformation about the world. Moreover, one aspect\nthat is not covered in this work is knowledge dele-\ntion. While maintaining Wikipedia and Wikidata,\nvolunteer editors not only update or add new infor-\nmation but also delete information that is incorrect\nor misinformed. As removing the misinformation\nand bias stored in LMs is an important issue and\nnecessary for truly ever-evolving LMs, future work\nshould address this aspect utilizing deleted infor-\nmation from general knowledge sources such as\nWikipedia.\nAcknowledgements\nThis work was supported by Institute of Infor-\nmation & communications Technology Planning\n& Evaluation (IITP) grants funded by the Korea\ngovernment (MSIT) (No.2022-0-00113, Develop-\ning a Sustainable Collaborative Multi-modal Life-\nlong Learning Framework, 80%; No.2019-0-00075,\nArtificial Intelligence Graduate School Program\n(KAIST), 10%; No.2021-0-02068, Artificial Intel-\nligence Innovation Hub, 10%).\nReferences\nOshin Agarwal and Ani Nenkova. 2021. Temporal ef-\nfects on pre-trained models for language processing\ntasks. arXiv preprint arXiv:2111.12790.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,\nTrevor Cai, Eliza Rutherford, Katie Millican, George\nvan den Driessche, Jean-Baptiste Lespiau, Bogdan\nDamoc, Aidan Clark, et al. 2021. Improving lan-\nguage models by retrieving from trillions of tokens.\narXiv preprint arXiv:2112.04426.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. In NeurIPS.\nSanyuan Chen, Yutai Hou, Yiming Cui, Wanxiang Che,\nTing Liu, and Xiangzhan Yu. 2020. Recall and learn:\nFine-tuning deep pretrained language models with\nless forgetting. In Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 7870–7881, Online. As-\nsociation for Computational Linguistics.\nWenhu Chen, Xinyi Wang, and William Yang Wang.\n2021. A dataset for answering time-sensitive ques-\ntions. In NeurIPS.\nBhuwan Dhingra, Jeremy R. Cole, Julian Martin\nEisenschlos, Daniel Gillick, Jacob Eisenstein, and\nWilliam W. Cohen. 2022. Time-aware language mod-\nels as temporal knowledge bases. Transactions of the\nAssociation for Computational Linguistics, 10:257–\n273.\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\nFan, Michael Auli, and Jason Weston. 2019. Wizard\nof wikipedia: Knowledge-powered conversational\nagents. In ICLR.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\naugmented language model pre-training. In ICML.\nTianxing He, Jun Liu, Kyunghyun Cho, Myle Ott, Bing\nLiu, James Glass, and Fuchun Peng. 2021. Analyzing\nthe forgetting problem in pretrain-finetuning of open-\ndomain dialogue response models. In Proceedings\nof the 16th Conference of the European Chapter of\nthe Association for Computational Linguistics: Main\nVolume, pages 1121–1133, Online. Association for\nComputational Linguistics.\nSpurthi Amba Hombaiah, Tao Chen, Mingyang Zhang,\nMichael Bendersky, and Marc Najork. 2021. Dy-\nnamic language models for continuously evolving\ncontent. In KDD.\nMatthew Honnibal and Ines Montani. 2017. spaCy 2:\nNatural language understanding with Bloom embed-\ndings, convolutional neural networks and incremental\nparsing. To appear.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu\nChen. 2022. Lora: Low-rank adaptation of large\nlanguage models. ICLR.\nJoel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin,\nJanghoon Han, Gyeonghun Kim, Stanley Jungkyu\nChoi, and Minjoon Seo. 2022. Towards continual\nknowledge learning of language models. In ICLR.\nXisen Jin, Dejiao Zhang, Henghui Zhu, Wei Xiao,\nShang-Wen Li, Xiaokai Wei, Andrew Arnold, and\nXiang Ren. 2022. Lifelong pretraining: Continually\nadapting language models to emerging corpora. In\nProceedings of BigScience Episode #5 – Workshop\non Challenges & Perspectives in Creating Large Lan-\nguage Models, pages 1–16, virtual+Dublin. Associa-\ntion for Computational Linguistics.\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz,\nJoel Veness, Guillaume Desjardins, Andrei A Rusu,\n6245\nKieran Milan, John Quan, Tiago Ramalho, Ag-\nnieszka Grabska-Barwinska, et al. 2017. Over-\ncoming catastrophic forgetting in neural networks.\nProceedings of the national academy of sciences ,\n114(13):3521–3526.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, Kristina Toutanova, Llion Jones, Matthew\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\nral questions: A benchmark for question answering\nresearch. Transactions of the Association for Compu-\ntational Linguistics, 7:452–466.\nAngeliki Lazaridou, Adhi Kuncoro, Elena Gribovskaya,\nDevang Agrawal, Adam Liska, Tayfun Terzi, Mai\nGimenez, Cyprien de Masson d’Autume, Tomas Ko-\ncisky, Sebastian Ruder, et al. 2021. Mind the gap:\nAssessing temporal generalization in neural language\nmodels. In NeurIPS.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. In NeurIPS.\nPatrick Lewis, Pontus Stenetorp, and Sebastian Riedel.\n2021. Question and answer test-train overlap in open-\ndomain question answering datasets. In Proceedings\nof the 16th Conference of the European Chapter of\nthe Association for Computational Linguistics: Main\nVolume, pages 1000–1008, Online. Association for\nComputational Linguistics.\nRobert L Logan IV , Alexandre Passos, Sameer Singh,\nand Ming-Wei Chang. 2021. Fruit: Faithfully re-\nflecting updated information in text. arXiv preprint\narXiv:2112.08634.\nShayne Longpre, Kartik Perisetla, Anthony Chen,\nNikhil Ramesh, Chris DuBois, and Sameer Singh.\n2021. Entity-based knowledge conflicts in question\nanswering. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 7052–7063, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nDaniel Loureiro, Francesco Barbieri, Leonardo Neves,\nLuis Espinosa Anke, and Jose Camacho-collados.\n2022. TimeLMs: Diachronic language models from\nTwitter. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics:\nSystem Demonstrations, pages 251–260, Dublin, Ire-\nland. Association for Computational Linguistics.\nKelvin Luu, Daniel Khashabi, Suchin Gururangan, Kar-\nishma Mandyam, and Noah A Smith. 2021. Time\nwaits for no one! analysis and challenges of temporal\nmisalignment. arXiv preprint arXiv:2111.07408.\nMichael McCloskey and Neal J Cohen. 1989. Catas-\ntrophic interference in connectionist networks: The\nsequential learning problem. Psychology of learning\nand motivation.\nR Thomas McCoy, Paul Smolensky, Tal Linzen, Jian-\nfeng Gao, and Asli Celikyilmaz. 2021. How much\ndo language models copy from their training data?\nevaluating linguistic novelty in text generation using\nraven. arXiv preprint arXiv:2111.09509.\nMartial Mermillod, Aurélia Bugaiska, and Patrick\nBonin. 2013. The stability-plasticity dilemma: Inves-\ntigating the continuum from catastrophic forgetting to\nage-limited learning effects. Frontiers in Psychology.\nMiles Osborne, Ashwin Lall, and Benjamin Van Durme.\n2014. Exponential reservoir sampling for streaming\nlanguage models. In Proceedings of the 52nd Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 2: Short Papers), pages 687–692,\nBaltimore, Maryland. Association for Computational\nLinguistics.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James Thorne,\nYacine Jernite, Vladimir Karpukhin, Jean Maillard,\nVassilis Plachouras, Tim Rocktäschel, and Sebastian\nRiedel. 2021. KILT: a benchmark for knowledge\nintensive language tasks. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2523–2544, Online.\nAssociation for Computational Linguistics.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 2463–2473, Hong Kong, China. Association\nfor Computational Linguistics.\nAleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,\nDmytro Okhonko, Samuel Broscheit, Gautier Izacard,\nPatrick Lewis, Barlas O˘guz, Edouard Grave, Wen-tau\nYih, et al. 2021. The web is your oyster–knowledge-\nintensive nlp against a very large web corpus. arXiv\npreprint arXiv:2112.09924.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. JMLR.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? In Proceedings of the\n6246\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 5418–5426,\nOnline. Association for Computational Linguistics.\nGuy D Rosin, Ido Guy, and Kira Radinsky. 2021.\nTime masking for temporal language models. arXiv\npreprint arXiv:2110.06366.\nPaul Röttger and Janet Pierrehumbert. 2021. Temporal\nadaptation of BERT and performance on downstream\ndocument classification: Insights from social media.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2021, pages 2400–2412, Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja,\net al. 2022. Multitask prompted training enables\nzero-shot task generalization. In ICLR.\nLeslie N Smith. 2018. A disciplined approach to neu-\nral network hyper-parameters: Part 1–learning rate,\nbatch size, momentum, and weight decay. In CVPR.\nJames Thorne, Andreas Vlachos, Christos\nChristodoulopoulos, and Arpit Mittal. 2018.\nFEVER: a large-scale dataset for fact extraction\nand VERification. In Proceedings of the 2018\nConference of the North American Chapter of\nthe Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long\nPapers), pages 809–819, New Orleans, Louisiana.\nAssociation for Computational Linguistics.\nRuize Wang, Duyu Tang, Nan Duan, Zhongyu Wei,\nXuanjing Huang, Jianshu Ji, Guihong Cao, Daxin\nJiang, and Ming Zhou. 2021. K-Adapter: Infusing\nKnowledge into Pre-Trained Models with Adapters.\nIn Findings of the Association for Computational\nLinguistics: ACL-IJCNLP 2021, pages 1405–1418,\nOnline. Association for Computational Linguistics.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M Dai, and Quoc V Le. 2022. Finetuned lan-\nguage models are zero-shot learners.\nDani Yogatama, Chong Wang, Bryan R. Routledge,\nNoah A. Smith, and Eric P. Xing. 2014. Dynamic\nlanguage models for streaming text. Transactions of\nthe Association for Computational Linguistics, 2:181–\n192.\nMichael Zhang and Eunsol Choi. 2021. SituatedQA: In-\ncorporating extra-linguistic contexts into QA. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing , pages 7371–\n7387, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\n6247\n(a) INFORMATION UPDATE\n(b) NEW INFORMATION\nFigure 5: Examples of TWIKI -DIFFSETS constructed from\ncomparing November 2021 and December 2021 Wikipedia\nDumps. (a) shows an instance of information update and (b)\nshows an instance of new information.\nA Examples of TW IKI -DIFFSETS and\nTWIKI -PROBES\nFigure 5 shows the examples of TWIKI -PROBES\nwhich is either an updated or a new piece of\ninformation. By comparing consecutive snap-\nshots of Wikipedia corpus, we keep track of\nchanged information. Table 4 shows the examples\nof CHANGED factual instances TWIKI -PROBES ,\nwhich is aligned with the corresponding sentence\nin TW IKI -DIFFSETS .\nB Details of Entity Types of Subject\nand Relation\nFigure 6 shows the ratio of different entity types\nof Subject and Object of UNCHANGED and\nCHANGED .\n(a) UNCHANGED\n (b) CHANGED\nFigure 6: Entity types of Subject and Object in TWIKI -\nPROBES .\nC Details of Relation Distribution\nThe distribution of Relation for UNCHANGED ,\nCHANGED factual instances in TWIKI -PROBES is\nshown in Figure 7.\nD Continual Pretraining and Light\nTuning Configuration\nFor each LM update, we use 8 32GB V100 GPUs\nwith a global batch size of 64 and a fixed input se-\nquence length of 512. We use the max learning rate\nof 1e-4 and one cycle learning rate scheduling pol-\nicy (Smith, 2018). For light-tuning, the training is\ndone for only one epoch with a learning rate of 1e-\n5 and a batch size of 32. Input and output sequence\nlengths are set to 25. For continual learning-based\nmethods, we unfreeze all of the parameters during\nlight-tuning, following Jang et al. (2022).\nE Light-tuning results with\nTWIKI -PROBES\nUsing the pre-defined templates of LAMA (Petroni\net al., 2019) seems to be an option, but we find that\nthose templates do not fit well to our experiments\nbecause there is a considerable distribution gap\nbetween LAMA and TWIKI -PROBES ; over half of\nthe instances of TWIKI -PROBES are filtered out to\napply the templates, especially for CHANGED .\nInstead, to alleviate the distributional shift that\ncauses high zero-shot perplexity, we light-tune the\nLMs on 500 instances randomly sampled from\nWikiData that do not overlap with instances from\nTWIKI -PROBES (details in Appendix F). Unlike\nfinetuning, light-tuning lets the LM only learn the\ninput and output distribution of the task, avoiding\nthe problem of test-train overlap pointed out by\nLewis et al. (2021). Table 5 shows the results of\nlight-tuning, which demonstrate a similar trend as\nthe zero-shot performance. Although light-tuning\navoids the problem of test-train overlap, results are\nlargely affected by the sampled instances for tun-\ning, so a zero-shot evaluation setting is preferred\nfor reliability.\nMany knowledge-intensive tasks such as closed-\nbook question answering (Roberts et al., 2020;\nPetroni et al., 2021; Jang et al., 2022) or slot filling\n(Petroni et al., 2021) use accuracy, EM, or F1 score\nto evaluate the task. We also show the F1 score on\nTWIKI -PROBES in Table 6. Overall trend is consis-\ntent with zero-shot perplexity metric; K-A DAPTER\nshows robust performance for both UNCHANGED\nand CHANGED .\nF Light-Tuning Data\nWe sample 500 instances from WikiData for each\ntime step that do not overlap with instances from\n6248\nTable 4: Examples of successful alignment between CHANGED factual instances from TWIKI -PROBES -0910 and articles from\nTWIKI -DIFFSETS -0910 . The alignment is considered successful because for the given factual instance, the Subject matches\nthe title of the Wikipedia page and the Object exists in the article.\nSubject Relation Object Corresponding Sentence in Wikipedia\nCarlo Alighiero place of death Rome [...] Carlo Alighierodied inRomeon 11 September 2021 at the age of 94.[...]\nShang-Chi and the instance of Film [...] Shang-Chi and the Legend of the Ten Ringsis a 2021 American\nLegend of the Ten Rings superhero filmbased on Marvel Comics featuring the character Shang-Chi.[...]\nOut of Shadows language of work or name Spanish [...] Itwas later translated into Portuguese, Turkish andSpanish.[...]\nMario Chalmers member of sports team Indios [...] On September 27, 2021,Chalmerssigned withIndios de Mayagüez\nde Mayaguez of the Baloncesto Superior Nacional.[...]\n(a) UNCHANGED\n(b) CHANGED\nFigure 7: TW IKI -PROBES distribution of the top 30 Relation.\nTWIKI -PROBES for each factual instance category.\nDuring sampling, we keep the distribution of each\nRelation proportional to the original distribu-\ntion. Table 7 shows the size and distribution of\nRelation of light-tuning datasets.\n6249\nTable 5: Light-tuning perplexity of LMs measured on TWIKI -PROBES .\nTWiki-Probes-0809 TWiki-Probes-0910 TWiki-Probes-1011 TWiki-Probes-1112\nUn C Un C Un C Un C\nINITIAL 116.99 142.58 108.89 167.82 106.14 172.18 114.64 177.02\nFULL 124.37 145.89 112.51 172.70 105.09 164.59 118.54 164.17\nDIFF 120.52 116.44 125.80 142.82 132.83 156.60 144.61 164.34\nRECADAM 122.58 118.14 125.90 143.65 137.15 148.24 144.76 159.52\nMIX-REVIEW 116.53 121.57 119.39 154.72 119.16 157.59 118.64 145.29\nLORA 123.62 130.41 115.54 156.07 115.26 165.51 122.11 169.59\nK-ADAPTER 115.93 134.46 116.27 154.11 110.17 158.21 117.22 167.44\nTable 6: F1 score result of LMs on TWIKI -PROBES after light-tuning.\nTWiki-Probes-0809 TWiki-Probes-0910 TWiki-Probes-1011 TWiki-Probes-1112\nUn C Un C Un C Un C\nINITIAL 6.98 3.19 7.26 3.35 7.27 2.74 6.84 2.82\nFULL 4.68 2.45 5.62 3.06 7.12 2.25 4.28 2.30\nDIFF 7.51 4.38 6.91 4.46 5.24 2.65 5.45 4.38\nRECADAM 5.74 3.79 6.31 3.86 4.47 2.43 5.09 3.68\nMIX-REVIEW 7.12 3.31 6.16 3.56 6.63 2.08 6.84 3.67\nLORA 7.36 4.48 7.23 3.89 7.19 3.87 6.82 3.81\nK-ADAPTER 7.54 3.99 7.34 3.73 7.38 3.91 6.87 3.30\nTable 7: Statistics of the data used for Light-Tuning\nSize # of\nRelation\nMaximum\nRepetition\nof Relation\n# of\nSubject\nUNCHANGED 500 102 58 499\nCHANGED 500 140 31 500\n6250",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.9180417656898499
    },
    {
      "name": "Computer science",
      "score": 0.8425611257553101
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.8237143754959106
    },
    {
      "name": "Snapshot (computer storage)",
      "score": 0.6044248938560486
    },
    {
      "name": "Machine learning",
      "score": 0.5507189631462097
    },
    {
      "name": "Lifelong learning",
      "score": 0.5467116832733154
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5420734286308289
    },
    {
      "name": "Training set",
      "score": 0.49229896068573
    },
    {
      "name": "Adaptability",
      "score": 0.47458386421203613
    },
    {
      "name": "Language model",
      "score": 0.43176573514938354
    },
    {
      "name": "Training (meteorology)",
      "score": 0.4164348244667053
    },
    {
      "name": "Database",
      "score": 0.13070037961006165
    },
    {
      "name": "Psychology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Pedagogy",
      "score": 0.0
    },
    {
      "name": "Ecology",
      "score": 0.0
    },
    {
      "name": "Meteorology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210099236",
      "name": "Kootenay Association for Science & Technology",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I157485424",
      "name": "Korea Advanced Institute of Science and Technology",
      "country": "KR"
    }
  ]
}