{
  "title": "PolarFormer: Multi-Camera 3D Object Detection with Polar Transformer",
  "url": "https://openalex.org/W4382466543",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2631595303",
      "name": "Yanqin Jiang",
      "affiliations": [
        "University of Chinese Academy of Sciences",
        "Institute of Automation",
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A1973164540",
      "name": "Li Zhang",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A1975362671",
      "name": "Zhenwei Miao",
      "affiliations": [
        "Alibaba Group (Cayman Islands)"
      ]
    },
    {
      "id": "https://openalex.org/A2116756113",
      "name": "Xiatian Zhu",
      "affiliations": [
        "University of Surrey"
      ]
    },
    {
      "id": "https://openalex.org/A2105604943",
      "name": "Jin Gao",
      "affiliations": [
        "Institute of Automation",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2102880302",
      "name": "Weiming Hu",
      "affiliations": [
        "Institute of Automation",
        "ShanghaiTech University",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2693570096",
      "name": "Yu-Gang Jiang",
      "affiliations": [
        "Institute of Automation",
        "University of Chinese Academy of Sciences",
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2631595303",
      "name": "Yanqin Jiang",
      "affiliations": [
        "Fudan University",
        "Chinese Academy of Sciences",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A1973164540",
      "name": "Li Zhang",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2116756113",
      "name": "Xiatian Zhu",
      "affiliations": [
        "University of Surrey"
      ]
    },
    {
      "id": "https://openalex.org/A2105604943",
      "name": "Jin Gao",
      "affiliations": [
        "University of Chinese Academy of Sciences",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2102880302",
      "name": "Weiming Hu",
      "affiliations": [
        "ShanghaiTech University"
      ]
    },
    {
      "id": "https://openalex.org/A2693570096",
      "name": "Yu-Gang Jiang",
      "affiliations": [
        "Fudan University",
        "University of Chinese Academy of Sciences",
        "Chinese Academy of Sciences"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3202530101",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W3211382623",
    "https://openalex.org/W3198460218",
    "https://openalex.org/W3035180028",
    "https://openalex.org/W2991840778",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W3154784054",
    "https://openalex.org/W3179351458",
    "https://openalex.org/W4226521892",
    "https://openalex.org/W6742348326",
    "https://openalex.org/W6639102338",
    "https://openalex.org/W6838607987",
    "https://openalex.org/W2998633559",
    "https://openalex.org/W3048770793",
    "https://openalex.org/W3111388005",
    "https://openalex.org/W3133598367",
    "https://openalex.org/W3013823321",
    "https://openalex.org/W2901707509",
    "https://openalex.org/W6801663961",
    "https://openalex.org/W2925359305",
    "https://openalex.org/W3183579734",
    "https://openalex.org/W3154510993",
    "https://openalex.org/W2904448996",
    "https://openalex.org/W3206020460",
    "https://openalex.org/W2798462325",
    "https://openalex.org/W3172084025",
    "https://openalex.org/W6779712747",
    "https://openalex.org/W3014168293",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W3105266714",
    "https://openalex.org/W3209005318",
    "https://openalex.org/W4214549494",
    "https://openalex.org/W4311152961",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3173668541",
    "https://openalex.org/W4287078665",
    "https://openalex.org/W2982770724",
    "https://openalex.org/W2954174912",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W4224259431",
    "https://openalex.org/W3035275207",
    "https://openalex.org/W3167095230",
    "https://openalex.org/W4289283268",
    "https://openalex.org/W3109395584",
    "https://openalex.org/W3207615232",
    "https://openalex.org/W3215100485",
    "https://openalex.org/W4394657967",
    "https://openalex.org/W3034669477",
    "https://openalex.org/W3203158837",
    "https://openalex.org/W4312954223",
    "https://openalex.org/W4287593861",
    "https://openalex.org/W2970987838",
    "https://openalex.org/W4214558638",
    "https://openalex.org/W3035574168",
    "https://openalex.org/W4224947594",
    "https://openalex.org/W4312894406",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2996166203",
    "https://openalex.org/W2884561390",
    "https://openalex.org/W3122239467",
    "https://openalex.org/W3208394352",
    "https://openalex.org/W1861492603"
  ],
  "abstract": "3D object detection in autonomous driving aims to reason ‚Äúwhat‚Äù and ‚Äúwhere‚Äù the objects of interest present in a 3D world. Following the conventional wisdom of previous 2D object detection, existing methods often adopt the canonical Cartesian coordinate system with perpendicular axis. However, we conjugate that this does not fit the nature of the ego car‚Äôs perspective, as each onboard camera perceives the world in shape of wedge intrinsic to the imaging geometry with radical (non perpendicular) axis. Hence, in this paper we advocate the exploitation of the Polar coordinate system and propose a new Polar Transformer (PolarFormer) for more accurate 3D object detection in the bird‚Äôs-eye-view (BEV) taking as input only multi-camera 2D images. Specifically, we design a cross-attention based Polar detection head without restriction to the shape of input structure to deal with irregular Polar grids. For tackling the unconstrained object scale variations along Polar‚Äôs distance dimension, we further introduce a multi-scale Polar representation learning strategy. As a result, our model can make best use of the Polar representation rasterized via attending to the corresponding image observation in a sequence-to-sequence fashion subject to the geometric constraints. Thorough experiments on the nuScenes dataset demonstrate that our PolarFormer outperforms significantly state-of-the-art 3D object detection alternatives.",
  "full_text": "PolarFormer: Multi-Camera 3D Object Detection with Polar Transformer\nYanqin Jiang1,4*, Li Zhang2‚Ä† , Zhenwei Miao5, Xiatian Zhu6, Jin Gao1,4,\nWeimin Hu1,4,7, Yu-Gang Jiang3\n1NLPR, Institute of Automation, Chinese Academy of Sciences\n2School of Data Science, Fudan University\n3School of Computer Science, Fudan University\n4School of Artificial Intelligence, University of Chinese Academy of Sciences\n5Alibaba DAMO Academy\n6Surrey Institute for People-Centred Artificial Intelligence, CVSSP, University of Surrey\n7School of Information Science and Technology, ShanghaiTech University\njiangyanqin2021@ia.ac.cn, {lizhangfd, ygj}@fudan.edu.cn,\nzhenwei.mzw@alibaba-inc.com, xiatian.zhu@surrey.ac.uk, {jin.gao, wmhu}@nlpr.ia.ac.cn\nAbstract\n3D object detection in autonomous driving aims to reason\n‚Äúwhat‚Äù and ‚Äúwhere‚Äù the objects of interest present in a 3D\nworld. Following the conventional wisdom of previous 2D\nobject detection, existing methods often adopt the canonical\nCartesian coordinate system with perpendicular axis. How-\never, we conjugate that this does not fit the nature of the ego\ncar‚Äôs perspective, as each onboard camera perceives the world\nin shape of wedge intrinsic to the imaging geometry with rad-\nical (non-perpendicular) axis. Hence, in this paper we advo-\ncate the exploitation of the Polar coordinate system and pro-\npose a new Polar Transformer (PolarFormer) for more accu-\nrate 3D object detection in the bird‚Äôs-eye-view (BEV) taking\nas input only multi-camera 2D images. Specifically, we de-\nsign a cross-attention based Polar detection head without re-\nstriction to the shape of input structure to deal with irregular\nPolar grids. For tackling the unconstrained object scale varia-\ntions along Polar‚Äôs distance dimension, we further introduce a\nmulti-scale Polar representation learning strategy. As a result,\nour model can make best use of the Polar representation ras-\nterized via attending to the corresponding image observation\nin a sequence-to-sequence fashion subject to the geometric\nconstraints. Thorough experiments on the nuScenes dataset\ndemonstrate that our PolarFormer outperforms significantly\nstate-of-the-art 3D object detection alternatives.\nIntroduction\n3D object detection is an enabling capability of autonomous\ndriving in unconstrained real-world scenes (Wang et al.\n2022b, 2021). It aims to predict the location, dimension and\norientation of individual objects of interest in a 3D world.\nDespite favourable cost advantages, multi-camera based 3D\nobject detection (Wang et al. 2021, 2022a,b; Zhou, Wang,\nand Kr¬®ahenb¬®uhl 2019) remains particularly challenging. To\nobtain 3D representation, dense depth estimation is often\n*Work done while at Fudan University.\n‚Ä†Li Zhang (lizhangfd@fudan.edu.cn) is the corresponding au-\nthor.\nCopyright ¬© 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nleveraged (Philion and Fidler 2020), which however is not\nonly expensive in computation but error prone. To bypass\ndepth inference, more recent methods (Wang et al. 2022b;\nLi et al. 2022b) exploit query-based 2D detection (Carion\net al. 2020) to learn a set of sparse and virtual embedding\nfor multi-camera 3D object detection, yet incapable of ef-\nfectively modeling the geometry structure among objects.\nTypically, the canonical Cartesian coordinate system with\nperpendicular axis is adopted in either 2D (Zhou, Wang,\nand Kr¬®ahenb¬®uhl 2019; Wang et al. 2021) or 3D (Wang et al.\n2022b; Li et al. 2022b) space. This is largely restricted by\nconvolution based models used. In contrast, the physical\nworld perceived under each camera in the ego car‚Äôs per-\nspective is in shape of wedge intrinsic to the camera imag-\ning geometry with radical non-perpendicular axis (Figure\n5). Bearing this imaging property in mind, we conjugate\nthat the Polar coordinate system should be more appropri-\nate and natural than the often adopted Cartesian counter-\npart for 3D object detection. Indeed, the Polar coordinate\nhas been exploited in a few LiDAR-based 3D perception\nmethods (Zhang et al. 2020; Bewley et al. 2020; Rapoport-\nLavie and Raviv 2021; Zhu et al. 2021). However, they are\nlimited algorithmically due to the adoption of convolutional\nnetworks restricted to rectangular grid structure and local re-\nceptive fields.\nMotivated by the aforementioned insights, in this work\na novel Polar Transformer(PolarFormer) model for multi-\ncamera 3D object detection in a Polar coordinate system is\nintroduced (Figure 3). Specifically, we first learn the repre-\nsentation of polar rays corresponding to image regions in a\nsequence-to-sequence cross-attention formulation. Then we\nrasterize a BEV Polar representation consisting of a set of\nPolar rays evenly distributed around 360 degrees. To deal\nwith irregular Polar grids as suffered by conventional Li-\nDAR based solutions (Zhang et al. 2020; Bewley et al. 2020;\nRapoport-Lavie and Raviv 2021; Zhu et al. 2021), we pro-\npose a cross-attention based decoder head design without re-\nstriction to the shape of input structure. For tackling the chal-\nlenge of unconstrained object scale variation along Polar‚Äôs\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n1042\nFRONT_LEFT FRONT FRONT_RIGHT\nBACK_LEFT BACK BACK_RIGHT\nFigure 1: Taking multi-camera images as input, the proposed PolarFormer model is designed particularly for accurate 3D\nobject detection in the Polar coordinate system.\ndistance dimension, we resort to a multi-scale Polar BEV\nrepresentation learning strategy.\nThe contributions of this work are summarized as fol-\nlows: (I) We propose a new Polar Transformer (Polar-\nFormer) model for multi-camera 3D object detection in the\nPolar coordinate system. (II) This is achieved based on two\nPolar-tailored designs: A cross-attention based decoder de-\nsign for dealing with the irregular Polar girds, and a multi-\nscale Polar representation learning strategy for handling the\nunconstrained object scale variations over Polar‚Äôs distance\ndimension. (III) Extensive experiments on the nuScenes\ndataset show that our PolarFormer achieves leading perfor-\nmance for camera-based 3D object detection (Figure 1).\nRelated Work\nMonocular/multi-camera 3D object detection Image-\nbased 3D object detection aims to estimate the object lo-\ncation, dimension and orientation in the 3D space alongside\nits category given only image input. To solve this ill-posed\nproblem, (Zhou, Wang, and Kr ¬®ahenb¬®uhl 2019; Wang et al.\n2021) naively build their detection pipelines by augmenting\n2D detectors (Zhou, Wang, and Kr¬®ahenb¬®uhl 2019; Tian et al.\n2019) in a data driven fashion. PGD (Wang et al. 2022a) fur-\nther captures the uncertainty and models the relationship of\ndifferent objects by utilizing geometric prior. Contrast to the\nabove image-only methods, depth-based methods (Xu and\nChen 2018; Ding et al. 2020; Wang et al. 2019; You et al.\n2019; Ma et al. 2019; Reading et al. 2021) use depth cues as\n3D information to mitigate the naturally ill-posed problem.\nRecently, multi-camera-based 3D object detection emerges.\nDETR3D (Wang et al. 2022b) considers detecting objects\nacross all cameras collectively. It learns a set of sparse and\nvirtual query embedding, without explicitly building the ge-\nometry structure among objects/queries. (Li et al. 2022b)\nconsiders detecting objects in BEV , performing end-to-end\nobject detection via object queries. Note that multi-camera\nsetting uses the same amount of training data as the monoc-\nular pipelines. Both multi-camera and monocular paradigms\nshare the same evaluation metrics.\nBird‚Äôs-eye-view (BEV) representation Recently there is\na surge of interest in transforming the monocular or multi-\nview images from ego car cameras into the bird‚Äôs-eye-view\ncoordinate (Roddick, Kendall, and Cipolla 2019; Philion and\nFidler 2020; Li et al. 2022a; Roddick and Cipolla 2020;\nReading et al. 2021; Saha et al. 2022) followed by specific\noptimization tasks (e.g., 3D object detection, semantic seg-\nmentation). A natural solution (Philion and Fidler 2020; Hu\net al. 2021) is to learn the BEV representation by lever-\naging the pixel-level dense depth estimation. This however\nis error-prone due to lacking ground-truth supervision. An-\nother line of research aims to bypass the depth prediction\nand directly leverage a Transformer (Chitta, Prakash, and\nGeiger 2021; Can et al. 2021; Saha et al. 2022) or a FC\nlayer (Li et al. 2022a; Roddick and Cipolla 2020; Yang et al.\n2021) to learn the transformation from camera inputs to the\nBEV coordinate. A similar attempt as ours is conducted in\n(Saha et al. 2022) but limited in a couple of aspects: (i) It\nis restricted to monocular input for a straightforward 2D\nsegmentation task while we consider multiple cameras col-\nlectively for more challenging 3D object detection; (ii) We\nuniquely provide a solid multi-scale Polar BEV transforma-\ntion to tackle the unconstrained object scale variations and\nfollowed by a jointly optimized cross-attention based Polar\nhead.\n3D object detection in Polar coordinate3D object de-\ntection in the Polar or Polar-like coordinate system has been\nattempted in LiDAR-based perception methods. For exam-\nple, CyliNet (Zhu et al. 2021) introduces range-based guid-\nance for extracting Polar-consistent features. In particular, it\nadapts a Cartesian heatmap to a Polar version for object clas-\nsification, whilst learning relative heading angles and veloci-\nties. However, CyliNet still lags clearly behind the Cartesian\ncounterpart. Recently, PolarSteam (Chen, V ora, and Bei-\n1043\njbom 2021) designs a learnable sampling module for re-\nlieving object distortion in Polar coordinate and uses range-\nstratified convolution and normalization for flexibly extract-\ning the features over different ranges. Limited by the con-\nvolution based network, it remains inferior despite of these\nspecial designs. In contrast to all these works, we resort to\nthe cross-attention mechanism, tackling the challenges of\nobject scale variance and appearance distortion in the Polar\ncoordinate principally.\nMethod\nIn 3D object detection task, we are given a set ofN monocu-\nlar views {In, Œ†n, En}N\nn=1 consisting of input images In ‚àà\nRH√óW√ó3, camera intrinsics Œ†n ‚àà R3√ó3 and camera ex-\ntrinsics En ‚àà R4√ó4. The objective of ourPolar Transformer\n(PolarFormer) is to learn an effective BEV Polar representa-\ntion from multiple camera views for facilitating the predic-\ntion of object locations, dimensions, orientations and veloc-\nities in the Polar coordinate system. PolarFormer consists of\nthe following components. A cross-plane encoderfirst pro-\nduces a multi-scale feature representation of each input im-\nage, characterized by a cross-plane attention mechanism in\nwhich Polar queries attend to input images to generate 3D\nfeatures in BEV . APolar alignment modulethen aggregates\nPolar rays from multiple camera views to generate a struc-\ntured Polar map. Further, aBEV Polar encoderenhances the\nPolar features with multi-scale feature interaction. Finally, a\nPolar detection headdecodes the Polar map and predicts the\nobjects in the Polar coordinate system. For tackling the un-\nconstrained object scale variation with multi-granularity of\ndetails, we consider a multi-scale BEV Polar representation\nstructure. As shown in Figure 4, image features with dif-\nferent scales have unique cross-plane encoders and interact\nwith each other in a shared Polar BEV encoder. Multi-scale\nPolar BEV maps are then queried by Polar decoder head. An\noverall architecture of PolarFormer is depicted in Figure 2.\nCross-Plane Encoder\nThe goal of cross-plane encoder is to associate an image\nwith BEV Polar rays. According to the geometric model\nof camera, for any camera coordinate (x(C), y(C), z(C)) ‚àà\nR3, the transformation to image coordinate (x(I), y(I)) ‚àà\nR2could be described as:\ns\nÔ£Æ\nÔ£∞\nx(I)\ny(I)\n1\nÔ£π\nÔ£ª =\n\"fx 0 u0\n0 fy v0\n0 0 1\n#Ô£Æ\nÔ£∞\nx(C)\ny(C)\nz(C)\nÔ£π\nÔ£ª, (1)\nwhere fx, fy, u0 and v0 are camera intrinsic parameters in\nŒ†, x(C), y(C) and z(C) are the horizontal, vertical, depth\ncoordinate respectively. s is the scale factor. For any BEV\nPolar coordinate (œÅ(P), œï(P)), we have:\nœï(P) = arctan x(C)\nz(C) = arctan x(I) ‚àí u0\nfx\n, (2)\nœÅ(P) =\nq\n(x(C))2 + (z(C))2 = z(C)\ns\n(x(I) ‚àí u0\nfx\n)2 + 1.\n(3)\nEq. (2) suggests that the azimuth œï(P) is irrelevant to the\nvertical value of image coordinate. It is hence natural to\nbuild a one-to-one relationship between Polar rays and im-\nage columns (Saha et al. 2022). However, we need object\ndepth z(C) to compute radius œÅ(P), the estimation of which\nis ill-posed. Instead of explicit depth estimation, we lever-\nage the attention mechanism (Vaswani et al. 2017) to model\nthe relationship between pixels along the image column and\npositions along the Polar ray.\nLet fn,u,w ‚àà RHu√óC represent the image column from\nnth camera, uth scale and wth column, and Àô pn,u,w ‚àà\nRRu√óC denote the corresponding Polar ray query we in-\ntroduce, where H and R are the image feature map‚Äôs height\nand Polar map‚Äôs range. We formulate cross-plane attention\nas:\npn,u,w = MultiHead( Àô pn,u,w, fn,u,w, fn,u,w)\n= Concat(head1, . . . ,headh)WO\nu , (4)\nwhere\nheadi =Attention( Àô pn,u,wWQ\ni,u, fn,u,wWK\ni,u, fn,u,wWV\ni,u),\n(5)\nwhere WQ\ni,u ‚àà Rdmodel√ódq , WK\ni,u ‚àà Rdmodel√ódk , WV\ni,u ‚àà\nRdmodel√ódv , WO\nu ‚àà Rhdmodel√ódk are the projection param-\neters, dq = dk = dv = dmodel/h, d is the feature dimension\nand h is the number of heads.\nStacking the Polar ray features pn,u,w ‚àà RRu√óC along\nazimuth axis, we obtain the Polar feature map (i.e., BEV Po-\nlar representation) Pn,u for nth camera and uth scale as:\nPn,u =Stack([pn,u,1, . . . ,pn,u,Wu ], dim=1) ‚ààRRu√óWu√óC,\n(6)\nwhere Wu denotes the azimuth dimension. This sequence-\nto-sequence cross-attention-based encoder can encode geo-\nmetric imaging prior and implicitly learn a proxy for depth\nefficiently. Next, we show how to integrate independent Po-\nlar rays from multiple cameras into a coherent and structured\nPolar BEV map.\nPolar Alignment across Multiple Cameras\nOur Polar alignment module transforms Polar rays from dif-\nferent camera coordinates to a shared world coordinate. Tak-\ning multi-view Polar feature maps {Pn,u}N\nn=1 and camera\nmatrix {Œ†n, En}N\nn=1as inputs, it produces a coherent BEV\nPolar map Gu ‚àà RRu√óNu√óC, covering all camera views,\nwhere Ru, Nu and C are the dimensions of radius, azimuth\nand feature. Concretely, it first generates a set of 3D points\nin the cylindrical coordinate uniformly, denoted by G(P) =\n{(œÅ(P)\ni , œï(P)\nj , z(P)\nk )|i = 1 , . . . ,Ru; j = 1 , . . . ,Nu; k =\n1, . . . ,Zu}, where Zu is the number of points along z axis.\nSince cylindrical coordinate and Polar coordinate share ra-\ndius and azimuth axis, their superscripts are both denoted\nwith P. The points are then projected to the image plane of\n1044\nLinear\nObject Query\nMulti-Head\t\nAttention\n (œÅ, Œ∏, z)\nPolar Reference Points\nAdd\t&\tNorm\nMuti-Scale\t\nDeformable\t\nAttention\nAdd\t&\tNorm\nFeed\nForward\nAdd\t&\tNorm\nLinear\nUpdate\n(‚ñ≥œÅ, ‚ñ≥Œ∏, ‚ñ≥z, ...)Category scores\nDecoder\nPolar Query\nProjection\t &\tInterpolation\nMuti-Scale\t\nDeformable\t\nAttention\nAdd\t&\tNorm\nFeed\nForward\nAdd\t&\tNorm\nMulti-scale \nPolar BEV Map\nEncoder\n(Image Feature √ó N) √ó U \nBackbone\nFPN\nInput\nImage  √ó N \nMulti-Head\t\nAttention\nAdd\t&Norm\nMulti-Head\t\nAttention\nAdd\t&\tNorm\nFeed\nForward\nAdd\t&\tNorm\n(a)\nCross-plane\nencoder\n(b) Polar alignment module\n(c)\nPolar BEV\nencoder\nFigure 2: Schematic illustration of our proposed PolarFormerfor multi-camera 3D object detection. For each image captured\nby any camera view, our model first extracts the feature maps at multiple spatial scales. Given such a feature map, the cross-\nplane encoder (a) then transforms all the feature columns to a set of Polar rays in a sequence-to-sequence manner via polar\nqueries based cross-attention. The polar rays from all the cameras are subsequently processed by a Polar alignment module (b)\nto generate a structured multi-scale Polar BEV map, followed by further enhancement via interactions among different scales\nusing a Polar BEV encoder (c). At last, a specially designed Polar Head decodes multi-scale Polar BEV features for making\nfinal predictions in the Polar coordinate.\nnth camera to retrieve the index of Polar ray, estimated by:\nÔ£Æ\nÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞\nsx(I)\ni,j,k,n\nsy(I)\ni,j,k,n\ns\n1\nÔ£π\nÔ£∫Ô£∫Ô£∫Ô£∫Ô£ª\n=\n\u0014\nŒ†n 0\n0 1\n\u0015\nEn\nÔ£Æ\nÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞\nœÅ(P)\ni sin œï(P)\nj\nœÅ(P)\ni cos œï(P)\nj\nz(P)\nk\n1\nÔ£π\nÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª\n, (7)\nwhere s is the scale factor. Coherent BEV Polar map of uth\nscale can be then generated by:\nGu(œÅ(P)\ni , œï(P)\nj ) = 1\nPN\nn=1\nPZ\nk=1 Œªn(œÅ(P)\ni , œï(P)\nj , z(P)\nk )\n¬∑\nNX\nn=1\nZX\nk=1\nŒªn(œÅ(P)\ni , œï(P)\nj , z(P)\nk )B(Pn,u, (x(I)\ni,j,k,n, ri,j,n)),\n(8)\nwhere Œªn(œÅ(P)\ni , œï(P)\nj , z(P)\nk ) is binary weighted factor indi-\ncating visibility in nth camera, B(P, (x, y)) denotes the bi-\nlinear sampling P at location (x, y), x(I)\ni,j,k,n and ri,j,n de-\nnote the normalized Polar ray index and radius index. Note\nthe radius r is the distance between the point and the camera\norigin in BEV . Our Polar alignment module incorporates the\nfeatures at different heights by generating the points alongz\naxis. As validated in Table 2a, learning Polar representation\nis superior over Cartesian coordinate due to minimal infor-\nmation loss and higher consistency with raw visual data.\nPolar BEV Encoder at Multiple Scales\nWe leverage multi-scale feature maps for handling object\nscale variance in the Polar coordinate. To that end, the BEV\nPolar encoder performs information exchange among neigh-\nbouring pixels and across multi-scale feature maps. For-\nmally, let {Gu}U\nu=1 be the input multi-scale Polar feature\nmaps and ÀÜxq ‚àà [0, 1]2 be the normalized coordinates of the\nreference points for each qeury element q, we introduce a\nmulti-scale deformable attention module (Zhu et al. 2020)\nas:\nMSDeformAttn(zq, xq, {Gu}U\nu=1) =\nMX\nm=1\nWm[\nUX\nu=1\nKX\nk=1\nAmuqkW‚Ä≤\nmGu(Œ∂u(ÀÜxq) + ‚àÜxmuqk)],\n(9)\nwhere m and k are the index of the attention head and\nthe sampling point. zq is the query feature. ‚àÜxmuqk and\nAmuqk denote the sampling offset and the attention weight\n1045\nOy-axis\nx-axis\nx\ny\nùúÉùúÉùëúùëúùëúùëúùëúùëúùë£ùë£\nùë£ùë£ùë•ùë•\nùë£ùë£y\nego car\nO\nœÅ\nœÜ\nÃÖùúÉùúÉùíêùíêùíêùíêùíêùíê\nùë£ùë£ùúåùúå\nùë£ùë£ùúëùúë\nùë£ùë£\nœÅ-axis\nœÜ-axis\nego car\nFigure 3: Cartesian and Polar coordinates.script size test\nCross-plane\nencoder\nCross-plane\nencoder\nCross-plane\nencoder\nPolar\nBEV\nEncoder\nOœÅ\nœÜ Figure 4: Multi-scale Polar BEV maps.\nMethods Backbone mAP ‚Üë NDS‚Üë mATE‚Üì mASE‚Üì mAOE‚Üì mA VE‚Üì mAAE‚Üì\nFCOS3D‚Ä† (Wang et al. 2021) R101 35.8 42.8 69.0 24.9 45.2 143.4 12.4\nPGD‚Ä† (Wang et al. 2022a) R101 38.6 44.8 62.6 24.5 45.1 150.9 12.7\nEgo3RT‚Ä† (Lu et al. 2022) R101 38.9 44.3 59.9 26.8 47.0 116.9 17.2\nBEVFormer-S‚Ä† (Li et al. 2022b) R101 40.9 46.2 65.0 26.1 43.9 92.5 14.7\nPolarFormer‚Ä† R101 41.5 47.0 65.7 26.3 40.5 91.1 13.9\nBEVFormer‚Ä† (Li et al. 2022b) R101 44.5 53.5 63.1 25.7 40.5 43.5 14.3\nPolarFormer-T‚Ä† R101 45.7 54.3 61.2 25.7 39.2 46.7 12.9\nDETR3D‚Ä° (Wang et al. 2022b) V2-99 41.2 47.9 64.1 25.5 39.4 84.5 13.3\nM2BEV‚àó (Xie et al. 2022) X101 42.9 47.4 58.3 25.4 37.6 10.53 19.0\nEgo3RT‚Ä° (Lu et al. 2022) V2-99 42.5 47.9 54.9 26.4 43.3 101.4 14.5\nBEVFormer-S‚Ä° V2-99 43.5 49.5 58.9 25.4 40.2 84.2 13.1\nPolarFormer‚Ä° V2-99 45.5 50.3 59.2 25.8 38.9 87.0 13.2\nBEVFormer‚Ä° (Li et al. 2022b) V2-99 48.1 56.9 58.2 25.6 37.5 37.8 12.6\nPolarFormer-T‚Ä° V2-99 49.3 57.2 55.6 25.6 36.4 44.0 12.7\nTable 1: State-of-the-art comparison on nuScenes test set. ‚Ä† denotes the prototype setting: The model is initialized from\na FCOS3D (Wang et al. 2021) checkpoint trained on the nuScenes 3D detection dataset. ‚Ä° denotes the improved setting: A\npretrained model from DD3D (Park et al. 2021) is used, which includes external data from DDAD (Guizilini et al. 2020). ‚àó\ndenotes backbone is pretrained on COCO (Lin et al. 2014) and nuImage (Caesar et al. 2020).\nof the kth sampling point in uth feature level and mth at-\ntention head. The attention weight Amuqk is normalized byPU\nu=1\nPK\nk=1 Amuqk = 1 . Sampling offsets ‚àÜxmuqk are\ngenerated by applying MLP layers on query q. Function\nŒ∂u generates the sampling offsets and rescales the normal-\nized coordinate ÀÜxq to the uth feature scale. Wm and W‚Ä≤\nm\nare learnable parameters. Serving as query, each pixel in the\nmulti-scale feature maps exploits the information from both\nneighbouring pixels and pixels across scales. This enables\nlearning richer semantics across all feature scales.\nPolar BEV Decoder at Multiple Scales\nThe Polar decoder decodes the above multi-scale Polar fea-\ntures to make predictions in the Polar coordinate. We con-\nstruct the Polar BEV decoder with deformable attention\n(Zhu et al. 2020). Specifically, we queryq in Eq. (9) as learn-\nable parameters.\nUnlike 2D reference points in the encoder, here the refer-\nence points are in 3D cylindrical coordinate, equal to Polar\ncoordinate when projected to BEV . The classification branch\nin each decoder layer outputs the confidence score vector\nc ‚àà RO, where O is the number of categories. The key\nlearning targets of regression branch are in polar coordinate\ninstead of Cartesian coordiante, as illustrated in Figure 3.\nFor simplicity, superscript (P) is omitted. Reference points\n(œÅ, œï, z) are iteratively refined in the decoder. With reference\npoints, the regression branch regresses the offsetsdœÅ, dœï and\ndz. The learning targets for orientation Œ∏ and velocity v are\nrelative to azimuths of objects and separated to orthogonal\ncomponents Œ∏œï, Œ∏œÅ, vœï and vœÅ, defined by:\n¬ØŒ∏ori = Œ∏ori ‚àí œï, Œ∏ œï = sin ¬ØŒ∏ori, Œ∏ œÅ = cos ¬ØŒ∏ori, (10)\nand\n¬ØŒ∏v = Œ∏v ‚àí œï, v œï = vabs sin ¬ØŒ∏v, v œÅ = vabs cos ¬ØŒ∏v.\n(11)\nHere, Œ∏ori is the yaw angle of the bounding box. vabs and\nŒ∏v are the absolute value and angle of velocity. We regress\nthe object size l, w and h as log l, log w and log h. We adopt\nFocal loss (Lin et al. 2017) and L1 loss for classification and\nregression respectively.\nExperiments\nDataset We evaluate the PolarFormer on the nuScenes\ndataset (Caesar et al. 2020). It provides images with a resolu-\ntion of 1600 √ó 900 from 6 surrounding cameras (Figure 1).\nThe total of 1000 scenes, where each sequence is roughly\n20 seconds long and annotated every 0.5 second, is split\n1046\nMethod Feature Prediction mAP‚Üë NDS‚Üë\nCenterpoint (Yin, Zhou, and Kr¬®ahenb¬®uhl 2021) Cartesian Cartesian 37.8 45.4\nCenterpoint* (Yin, Zhou, and Kr¬®ahenb¬®uhl 2021) Cartesian Cartesian 38.5 45.6\nPolarFormer-CC Cartesian Cartesian 38.1 45.5\nPolarFormer-PC Polar Cartesian 38.5 45.0\nPolarFormer Polar Polar 39.6 45.8\n(a) Ablation study on coordinate system and detection head.\nPosition encoding mAP‚Üë NDS‚Üë\n2D learnable PE 38.8 45.1\n3D PE 38.5 44.9\nFixed Sine PE 39.6 45.8\n(b) Ablation on positional encoding (PE).\nMethods Multi-scale mAP‚Üë NDS‚Üë mAOE‚Üì\nPolarFormer-CC-s % 38.1 44.9 40.0\nPolarFormer-PC-s % 38.8 45.0 37.6\nPolarFormer-s % 39.1 45.0 37.3\nPolarFormer-CC ! 38.1 45.0 37.5\nPolarFormer-PC ! 38.5 45.5 40.8\nPolarFormer ! 39.6 45.8 37.5\n(c) Effectiveness of multi-scale polar representation.\nN1 R1 mAP‚Üë NDS‚Üë\n240 64 38.8 45.0\n256 64 39.6 45.8\n272 64 38.8 45.0\n256 56 38.7 45.4\n256 72 38.5 45.4\n256 80 38.7 45.6\n(d) Ablation on polar resolution.\nTable 2: 3D object detection results in different coordinate systems and ablations for the model architecture. PC denotes feature\nin Polar and prediction in Cartesian.\nofficially into train/val/test set with 700/150/150\nscenes.\nImplementation details We implement our approach\nbased on the codebase mmdetection3d (Contributors\n2020). Following DETR3D (Wang et al. 2022b) and\nFCOS3D (Wang et al. 2021), a ResNet-101 (He et al. 2016),\nwith 3rd and 4th stages equipped with deformable convo-\nlutions is adopted as the backbone architecture. The num-\nber of cross-plane encoder layer is set to 3 for each feature\nscale. The resolution of radius and azimuth for our multi-\nscale Polar BEV maps are (64, 256), (32, 128), (16, 64)\nrespectively. We use 6 Polar BEV encoder and 6 decoder\nlayers. Following DETR3D (Wang et al. 2022b), our back-\nbone is initialized from a checkpoint of FCOS3D (Wang\net al. 2021) trained on nuScenes 3D detection task, while\nthe rest is initialized randomly. We use above setting for\nprototype verification. To fully leverage the sequence\ndata, we further conduct temporal fusion between the cur-\nrent frame and one history sweep in the BEV space. Fol-\nlowing BEVDet4D (Huang and Huang 2022), we simply\nconcatenate two temporally adjacent multi-scale Polar BEV\nmaps along the feature dimension and feed to the BEV\nPolar encoder. We randomly sample a history sweep from\n[3T; 27T] during training, and sample the frame at 15T\nfor inference. T (‚âà 0.083s) refers to the time interval be-\ntween two sweep frames. We term our temporal version as\nPolarFormer-T.\nTraining Following DETR3D (Wang et al. 2022b) we\ntrain our models for 24 epochs with the AdamW optimizer\nand cosine annealing learning rate scheduler on 8 NVIDIA\nV100 GPUs. The initial learning rate is 2 √ó 10‚àí4, and\nthe weight decay is set to 0.075. Total batch size is set to\n48 across six cameras. Synchronized batch normalization\nis adopted. All experiments use the original input resolu-\ntion. Note our image variant uses the same amount of train-\ning data as the monocular pipelines (Wang et al. 2021) and\nthe multi-camera counterparts (Wang et al. 2022b; Li et al.\n2022b). Multi-camera and monocular paradigms share the\nsame evaluation metrics.\nInference We evaluate our model on nuScenes validation\nset and test server. We do not adopt model-agnostic trick\nsuch as model ensemble and test time augmentation.\nComparison with the State of the Art\nWe compare our method with the state of the art on both\ntest and val sets of nuScenes. In addition to the (i)\nprototype setting mentioned in implementation details,\nwe also evaluate our model in the (ii) improved setting,\nwith V oVNet (V2-99) (Lee et al. 2019) as backbone archi-\ntecture with a pretrained checkpoint from DD3D (Park et al.\n2021) (fine-tuned on extra DDAD15M (Guizilini et al. 2020)\ndataset) to boost performance.\nTable 1 compares the results on nuScenes test set. We\nobserve that our PolarFormer achieves the best perfor-\nmance under both the (i) prototype and (ii) improved\nsetting in terms of mAP and NDS metrics, indicating the su-\nperiority of learning representation in the Polar coordinate.\nWith temporal information PolarFormer-T can further\nboot performance substantially. Additional experiments re-\nsults on val set and qualitative results are shown in supple-\nmentary materials.\nAblation Studies\nWe conduct a series of ablation studies on nuScenesval set\nto validate the design of PolarFormer. Each proposed com-\nponent and important hyperparameters are examined thor-\noughly.\nPolar v.s. Cartesian Table 2a ablates the coordinate sys-\ntem. We make several observations: (I) Learning the repre-\nsentation and making the prediction both on Cartesian, Cen-\nterpoint (Yin, Zhou, and Kr ¬®ahenb¬®uhl 2021) gives a strong\nbaseline with 0.378 mAP and 0.454 NDS; After apply-\ning circle NMS, Centerpoint* can further improve; (II)\n1047\nBACK\nFRONT\n(a)\nBACK FRONT BACK_RIGHTFRONT_RIGHTFRONT_LEFTBACK_LEFT BACK\n(b)\nMethod Coordinate Near Medium Far\nDETR3D (Wang et al. 2022b) Cartesian 49.9/49.3 28.6/39.2 8.1/21.2\nBEVFormer-S (Li et al. 2022b) Cartesian 54.2/53.5 31.7/41.3 9.5/21.9\nPolarFormer-CC Cartesian 54.7/55.5 32.3/42.2 9.4/22.2\nPolarFormer Polar 57.8/55.8 33.6/42.9 9.6/22.3\n(c)\nFigure 5: 3D object detection in (a) Cartesian BEV vs. (b) Polar BEV , and (c) Performance comparison (mAP/NDS) at three\ndistances (Near/Medium/Far). Red and green boxes show the same objects in different coordinates.\nFigure 6: Multi-scale Polar BEV attention.\nOur PolarFormer-CC (with Cartesian feature and predic-\ntion) outperforms the Centerpoint equipped with specially\ndesigned CBGS head (Yin, Zhou, and Kr ¬®ahenb¬®uhl 2021);\n(III) When Polar BEV map is used to feed into a Cartesian\ndecoder head, on-par performance is achieved with the post-\nprocessing counterpart; (IV) PolarFormer, our full model\nthat predicts all 10 categories with one head and without any\npost-processing procedure, exceeds highly optimized Cen-\nterpoint by 1.1% in mAP and 0.2% in NDS. This suggests\nthe significance of Polar in both representation learning and\nexploitation (i.e., decoding).\nVisualizations With the quantitative evaluation in Fig-\nure 5, (I) it is evident that our model in Polar coordinate\nyields better results than Cartesian consistently. Specifically,\nPolar outperforms Cartesian by mAP 3.1% and NDS 0.3%\nin nearby area, mAP 1.3% and NDS 0.7% in medium area.\n(II) As shown in Figure 5a and Figure 5b, compared to the\nPolar map, Cartesian usually downsamples the nearby area\n(red) with information loss, while upsamples the distant area\n(green) without actual information added. This would ex-\nplain the inferiority of Cartesian. (III) Figure 6 shows the\nattention map of the decoder query in multi-scale Polar BEV\nfeatures. For better viewing, we resize the multi-scale fea-\ntures into the same resolution. The bottom/top corresponds\nto the largest/smallest scale features. The y axis represents\nthe radius of the Polar map. It is shown that larger objects\nrepresented in the small map (top) are close to the ego car\n(small radius) whilst small objects in the large map (bottom)\ndistribute through the distant area. This is highly consistent\nwith the geometry structure of raw images (Figure 1), which\nhas shown to be a more effective coordinate for 3D object\ndetection as above.\nArchitecture (I) We first evaluate three designs of posi-\ntional embedding (PE): 2D learnable PE, fixed Sine PE, and\n3D PE (generated based on a set of 3D points for each Po-\nlar ray position). (II) As our cross-plane encoder transforms\ndifferent levels of feature from FPN into Polar rays indepen-\ndently, we can fuse the multi-level features into a single BEV\nor naturally shape multiple BEVs with differentor same res-\nolutions; Table 2c clearly shows that a model with multi-\nscale Polar BEVs outperforms the single-scale counterpart\nunder either coordinate. In contrast, little performance gain\nis achieved from multi-scale features in Cartesian. This sug-\ngests that object scale variation is a unique challenge with\nPolar, but absent with Cartesian. Our design consideration\nis thus verified. (III) We study the resolution of polar map\nby adjusting the azimuth N1 and radius R1 (the number\nof Polar query in the cross-plane encoder); Table 2d shows\nthat the angle with 256 and radius with 64 gives the best\nperformance.\nConclusions\nWe have proposed the Polar Transformer (PolarFormer) for\n3D object detection in multi-camera 2D images from the\nego car‚Äôs perspective. With a rasterized BEV Polar repre-\nsentation geometrically aligned to visual observation, Polar-\nFormer overcomes irregular Polar grids by a cross-attention\nbased decoder. Further, a multi-scale representation learn-\n1048\ning strategy is designed for tackling the intrinsic object scale\nvariation challenge. Extensive experiments on the nuScenes\ndataset validate the superiority of our PolarFormer over pre-\nvious alternatives on 3D object detection.\nAcknowledgments\nThis work was supported by the National Key R&D\nProgram of China (Grant No. 2018AAA0102803,\n2018AAA0102802, 2018AAA0102800), the Natural\nScience Foundation of China (Grant No. 6210020439,\nU22B2056, 61972394, 62036011, 62192782, 61721004,\n62102417), Lingang Laboratory (Grant No. LG-QS-\n202202-07), Natural Science Foundation of Shanghai\n(Grant No. 22ZR1407500) Beijing Natural Science Foun-\ndation (Grant No. L223003, JQ22014), the Major Projects\nof Guangdong Education Department for Foundation Re-\nsearch and Applied Research (Grant No. 2017KZDXM081,\n2018KZDXM066), Guangdong Provincial University\nInnovation Team Project (Grant No. 2020KCXTD045). Jin\nGao was also supported in part by the Youth Innovation\nPromotion Association, CAS.\nReferences\nBewley, A.; Sun, P.; Mensink, T.; Anguelov, D.; and Smin-\nchisescu, C. 2020. Range conditioned dilated convolu-\ntions for scale invariant 3d object detection. arXiv preprint\narXiv:2005.09927.\nCaesar, H.; Bankiti, V .; Lang, A. H.; V ora, S.; Liong, V . E.;\nXu, Q.; Krishnan, A.; Pan, Y .; Baldan, G.; and Beijbom, O.\n2020. nuScenes: A multimodal dataset for autonomous driv-\ning. In CVPR, 11621‚Äì11631.\nCan, Y . B.; Liniger, A.; Paudel, D. P.; and Van Gool, L.\n2021. Structured Bird‚Äôs-Eye-View Traffic Scene Under-\nstanding from Onboard Images. In ICCV, 15661‚Äì15670.\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\nA.; and Zagoruyko, S. 2020. End-to-end object detection\nwith transformers. In ECCV, 213‚Äì229.\nChen, Q.; V ora, S.; and Beijbom, O. 2021. PolarStream:\nStreaming Object Detection and Segmentation with Polar\nPillars. In NeurIPS, 26871‚Äì26883.\nChitta, K.; Prakash, A.; and Geiger, A. 2021. NEAT: Neural\nAttention Fields for End-to-End Autonomous Driving. In\nICCV, 15793‚Äì15803.\nContributors, M. 2020. MMDetection3D: OpenMMLab\nnext-generation platform for general 3D object detection.\nhttps://github.com/open-mmlab/mmdetection3d. Accessed:\n2023-03-03.\nDing, M.; Huo, Y .; Yi, H.; Wang, Z.; Shi, J.; Lu, Z.; and Luo,\nP. 2020. Learning depth-guided convolutions for monocular\n3d object detection. In CVPR, 1000‚Äì1001.\nGuizilini, V .; Ambrus, R.; Pillai, S.; Raventos, A.; and\nGaidon, A. 2020. 3D Packing for Self-Supervised Monocu-\nlar Depth Estimation. In CVPR, 2485‚Äì2494.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual\nlearning for image recognition. In CVPR, 770‚Äì778.\nHu, A.; Murez, Z.; Mohan, N.; Dudas, S.; Hawke, J.; Badri-\nnarayanan, V .; Cipolla, R.; and Kendall, A. 2021. FIERY:\nFuture Instance Prediction in Bird‚Äôs-Eye View From Sur-\nround Monocular Cameras. In ICCV, 15273‚Äì15282.\nHuang, J.; and Huang, G. 2022. Bevdet4d: Exploit temporal\ncues in multi-camera 3d object detection. arXiv preprint\narXiv:2203.17054.\nLee, Y .; Hwang, J.-w.; Lee, S.; Bae, Y .; and Park, J. 2019. An\nenergy and GPU-computation efficient backbone network\nfor real-time object detection. In CVPR workshops.\nLi, Q.; Wang, Y .; Wang, Y .; and Zhao, H. 2022a. Hdmapnet:\nAn online hd map construction and evaluation framework.\nIn ICRA, 4628‚Äì4634.\nLi, Z.; Wang, W.; Li, H.; Xie, E.; Sima, C.; Lu, T.; Qiao,\nY .; and Dai, J. 2022b. Bevformer: Learning bird‚Äôs-eye-view\nrepresentation from multi-camera images via spatiotemporal\ntransformers. In ECCV, 1‚Äì18.\nLin, T.-Y .; Goyal, P.; Girshick, R.; He, K.; and Doll ¬¥ar, P.\n2017. Focal loss for dense object detection. In ICCV, 2980‚Äì\n2988.\nLin, T.-Y .; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ra-\nmanan, D.; Doll ¬¥ar, P.; and Zitnick, C. L. 2014. Microsoft\ncoco: Common objects in context. In ECCV, 740‚Äì755.\nLu, J.; Zhou, Z.; Zhu, X.; Xu, H.; and Zhang, L. 2022.\nLearning Ego 3D Representation as Ray Tracing. In ECCV,\n129‚Äì144.\nMa, X.; Wang, Z.; Li, H.; Zhang, P.; Ouyang, W.; and Fan,\nX. 2019. Accurate monocular 3d object detection via color-\nembedded 3d reconstruction for autonomous driving. In\nICCV, 6851‚Äì6860.\nPark, D.; Ambrus, R.; Guizilini, V .; Li, J.; and Gaidon, A.\n2021. Is Pseudo-Lidar needed for Monocular 3D Object de-\ntection? In ICCV, 3142‚Äì3152.\nPhilion, J.; and Fidler, S. 2020. Lift, splat, shoot: Encoding\nimages from arbitrary camera rigs by implicitly unprojecting\nto 3d. In ECCV, 194‚Äì210.\nRapoport-Lavie, M.; and Raviv, D. 2021. It‚Äôs All Around\nYou: Range-Guided Cylindrical Network for 3D Object De-\ntection. In ICCV, 2992‚Äì3001.\nReading, C.; Harakeh, A.; Chae, J.; and Waslander, S. L.\n2021. Categorical depth distribution network for monocular\n3d object detection. In CVPR, 8555‚Äì8564.\nRoddick, T.; and Cipolla, R. 2020. Predicting semantic map\nrepresentations from images using pyramid occupancy net-\nworks. In CVPR, 11138‚Äì11147.\nRoddick, T.; Kendall, A.; and Cipolla, R. 2019. Ortho-\ngraphic feature transform for monocular 3d object detection.\nIn BMVC.\nSaha, A.; Maldonado, O. M.; Russell, C.; and Bowden, R.\n2022. Translating Images into Maps. In ICRA, 9200‚Äì9206.\nTian, Z.; Shen, C.; Chen, H.; and He, T. 2019. Fcos: Fully\nconvolutional one-stage object detection. In ICCV, 9627‚Äì\n9636.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, ≈Å.; and Polosukhin, I. 2017. At-\ntention is all you need. In NeurIPS, volume 30.\n1049\nWang, T.; Xinge, Z.; Pang, J.; and Lin, D. 2022a. Probabilis-\ntic and geometric depth: Detecting objects in perspective. In\nCoRL, 1475‚Äì1485.\nWang, T.; Zhu, X.; Pang, J.; and Lin, D. 2021. Fcos3d: Fully\nconvolutional one-stage monocular 3d object detection. In\nICCV, 913‚Äì922.\nWang, Y .; Chao, W.-L.; Garg, D.; Hariharan, B.; Campbell,\nM.; and Weinberger, K. Q. 2019. Pseudo-lidar from visual\ndepth estimation: Bridging the gap in 3d object detection for\nautonomous driving. In CVPR, 8445‚Äì8453.\nWang, Y .; Guizilini, V . C.; Zhang, T.; Wang, Y .; Zhao, H.;\nand Solomon, J. 2022b. Detr3d: 3d object detection from\nmulti-view images via 3d-to-2d queries. In CoRL, 180‚Äì191.\nXie, E.; Yu, Z.; Zhou, D.; Philion, J.; Anandkumar, A.;\nFidler, S.; Luo, P.; and Alvarez, J. M. 2022. MÀÜ 2BEV:\nMulti-Camera Joint 3D Detection and Segmentation with\nUnified Birds-Eye View Representation. arXiv preprint\narXiv:2204.05088.\nXu, B.; and Chen, Z. 2018. Multi-level fusion based 3d\nobject detection from monocular images. In CVPR, 2345‚Äì\n2353.\nYang, W.; Li, Q.; Liu, W.; Yu, Y .; Ma, Y .; He, S.; and Pan,\nJ. 2021. Projecting Your View Attentively: Monocular Road\nScene Layout Estimation via Cross-view Transformation. In\nCVPR, 15536‚Äì15545.\nYin, T.; Zhou, X.; and Kr ¬®ahenb¬®uhl, P. 2021. Center-based\n3D Object Detection and Tracking. InCVPR, 11784‚Äì11793.\nYou, Y .; Wang, Y .; Chao, W.-L.; Garg, D.; Pleiss, G.; Har-\niharan, B.; Campbell, M.; and Weinberger, K. Q. 2019.\nPseudo-lidar++: Accurate depth for 3d object detection in\nautonomous driving. arXiv preprint arXiv:1906.06310.\nZhang, Y .; Zhou, Z.; David, P.; Yue, X.; Xi, Z.; Gong, B.; and\nForoosh, H. 2020. Polarnet: An improved grid representa-\ntion for online lidar point clouds semantic segmentation. In\nCVPR, 9601‚Äì9610.\nZhou, X.; Wang, D.; and Kr ¬®ahenb¬®uhl, P. 2019. Objects as\npoints. arXiv preprint arXiv:1904.07850.\nZhu, X.; Su, W.; Lu, L.; Li, B.; Wang, X.; and Dai, J. 2020.\nDeformable detr: Deformable transformers for end-to-end\nobject detection.\nZhu, X.; Zhou, H.; Wang, T.; Hong, F.; Ma, Y .; Li, W.; Li, H.;\nand Lin, D. 2021. Cylindrical and asymmetrical 3d convolu-\ntion networks for lidar segmentation. In CVPR, 9939‚Äì9948.\n1050",
  "topic": "Computer vision",
  "concepts": [
    {
      "name": "Computer vision",
      "score": 0.6988310813903809
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6858341693878174
    },
    {
      "name": "Cartesian coordinate system",
      "score": 0.6570200324058533
    },
    {
      "name": "Computer science",
      "score": 0.6481043100357056
    },
    {
      "name": "Polar coordinate system",
      "score": 0.637808620929718
    },
    {
      "name": "Polar",
      "score": 0.46217599511146545
    },
    {
      "name": "Object detection",
      "score": 0.4333317279815674
    },
    {
      "name": "Coordinate system",
      "score": 0.4168703854084015
    },
    {
      "name": "Geometry",
      "score": 0.27679532766342163
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.24396774172782898
    },
    {
      "name": "Mathematics",
      "score": 0.24238932132720947
    },
    {
      "name": "Physics",
      "score": 0.17019665241241455
    },
    {
      "name": "Astronomy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210165038",
      "name": "University of Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I24943067",
      "name": "Fudan University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210086143",
      "name": "Alibaba Group (Cayman Islands)",
      "country": "KY"
    },
    {
      "id": "https://openalex.org/I28290843",
      "name": "University of Surrey",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I30809798",
      "name": "ShanghaiTech University",
      "country": "CN"
    }
  ]
}