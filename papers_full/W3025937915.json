{
  "title": "A Transformer-based Embedding Model for Personalized Product Search",
  "url": "https://openalex.org/W3025937915",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2163307715",
      "name": "Keping Bi",
      "affiliations": [
        "University of Massachusetts Amherst"
      ]
    },
    {
      "id": "https://openalex.org/A2223502885",
      "name": "Qingyao Ai",
      "affiliations": [
        "University of Utah"
      ]
    },
    {
      "id": "https://openalex.org/A2127889770",
      "name": "W. Bruce Croft",
      "affiliations": [
        "University of Massachusetts Amherst"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2970599813",
    "https://openalex.org/W2740070748",
    "https://openalex.org/W2972160336",
    "https://openalex.org/W2973104394",
    "https://openalex.org/W2945127593",
    "https://openalex.org/W1981485659",
    "https://openalex.org/W2900464008",
    "https://openalex.org/W2741497758",
    "https://openalex.org/W1991418309",
    "https://openalex.org/W2507839313",
    "https://openalex.org/W2002766161",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2909544278",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W4300427681",
    "https://openalex.org/W3099446234",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3102512871",
    "https://openalex.org/W3101719580",
    "https://openalex.org/W3099984837"
  ],
  "abstract": "Product search is an important way for people to browse and purchase items on E-commerce platforms. While customers tend to make choices based on their personal tastes and preferences, analysis of commercial product search logs has shown that personalization does not always improve product search quality. Most existing product search techniques, however, conduct undifferentiated personalization across search sessions. They either use a fixed coefficient to control the influence of personalization or let personalization take effect all the time with an attention mechanism. The only notable exception is the recently proposed zero-attention model (ZAM) that can adaptively adjust the effect of personalization by allowing the query to attend to a zero vector. Nonetheless, in ZAM, personalization can act at most as equally important as the query and the representations of items are static across the collection regardless of the items co-occurring in the user's historical purchases. Aware of these limitations, we propose a transformer-based embedding model (TEM) for personalized product search, which could dynamically control the influence of personalization by encoding the sequence of query and user's purchase history with a transformer architecture. Personalization could have a dominant impact when necessary and interactions between items can be taken into consideration when computing attention weights. Experimental results show that TEM outperforms state-of-the-art personalization product retrieval models significantly.",
  "full_text": "A Transformer-based Embedding Model for Personalized\nProduct Search\nKeping Bi\nUniversity of Massachusetts Amherst\nkbi@cs.umass.edu\nQingyao Ai\nUniversity of Utah\naiqy@cs.utah.edu\nW. Bruce Croft\nUniversity of Massachusetts Amherst\ncroft@cs.umass.edu\nABSTRACT\nProduct search is an important way for people to browse and pur-\nchase items on E-commerce platforms. While customers tend to\nmake choices based on their personal tastes and preferences, anal-\nysis of commercial product search logs has shown that person-\nalization does not always improve product search quality. Most\nexisting product search techniques, however, conduct undifferenti-\nated personalization across search sessions. They either use a fixed\ncoefficient to control the influence of personalization or let per-\nsonalization take effect all the time with an attention mechanism.\nThe only notable exception is the recently proposed zero-attention\nmodel (ZAM) that can adaptively adjust the effect of personaliza-\ntion by allowing the query to attend to a zero vector. Nonetheless,\nin ZAM, personalization can act at most as equally important as\nthe query and the representations of items are static across the col-\nlection regardless of the items co-occurring in the user’s historical\npurchases. Aware of these limitations, we propose a transformer-\nbased embedding model (TEM) for personalized product search,\nwhich could dynamically control the influence of personalization\nby encoding the sequence of query and user’s purchase history\nwith a transformer architecture. Personalization could have a dom-\ninant impact when necessary and interactions between items can\nbe taken into consideration when computing attention weights.\nExperimental results show that TEM outperforms state-of-the-art\npersonalization product retrieval models significantly.\nKEYWORDS\nProduct Search; Personalization; Transformer\nACM Reference Format:\nKeping Bi, Qingyao Ai, and W. Bruce Croft. 2020. A Transformer-based\nEmbedding Model for Personalized Product Search. In Proceedings of the\n43rd International ACM SIGIR Conference on Research and Development in\nInformation Retrieval (SIGIR ’20), July 25–30, 2020, Virtual Event, China. ACM,\nNew York, NY, USA, 4 pages. https://doi.org/10.1145/3397271.3401192\n1 INTRODUCTION\nProduct search systems have been playing an important role in serv-\ning customers shopping on online e-commerce platforms in their\ndaily life. Usually, people issue queries about their shopping needs\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nSIGIR ’20, July 25–30, 2020, Virtual Event, China\n© 2020 Association for Computing Machinery.\nACM ISBN 978-1-4503-8016-4/20/07. . . $15.00\nhttps://doi.org/10.1145/3397271.3401192\non the platform and purchase items from the search results based\non their personal tastes and preferences. Aware of this point, recent\nstudies have explored to incorporate personalization in product\nsearch and achieved compelling results [1, 2, 8].\nDespite its great potential, personalization does not always im-\nprove the quality of product search. Based on the analysis of com-\nmercial search logs, Ai et al. [1] have observed that personalized\nmodels can outperform non-personalized models only on the queries\nwhere the preferences of individuals significantly differ from the\ngroup preference. While applying a universal personalization mech-\nanism sometimes could be beneficial by providing more information\nabout user preferences, especially when the query carries limited\ninformation, unreliable personal information could also harm the\nsearch quality due to data sparsity and the introduction of unnec-\nessary noise. Therefore, it is essential to determine when and how\nto conduct personalization under various scenarios.\nMost existing personalized product search models, however, do\nnot conduct differential personalization adaptively under different\ncontexts. Ai et al. [2] propose to control the influence of personal-\nization by representing the users’ purchase intent with a convex\ncombination between the query embedding and user embedding.\nThis method applies undifferentiated personalization to all search\nsessions since the coefficient of the combination is a fixed number.\nGuo et al. [8] fuse query and users’ long and short-term preferences\nto indicate the users’ specific intention. While the long and short-\nterm preferences are modeled by attending to the users’ recent pur-\nchases and a global user vector with the query, the model itself still\nconducts personalization all the time. Later, Ai et al. [1] proposed a\nzero attention model (ZAM) which introduces a zero vector that the\nquery can attend to besides users’ previous purchases. In contrast\nto [8], by allowing the zero vector to have attention weights, the\ninfluence of personalization can be controlled. Nonetheless, despite\nthe ability to adaptively personalize a query-user pair, the maxi-\nmum personalization ZAM can perform is to equally consider the\nquery and the user information, which may be not enough when\nthe user preference dominates the purchase.\nIn this paper, we propose a transformer-based embedding model\n(TEM) that is more flexible where personalization can vary from\nno to full effect. As we will demonstrate later in Section 3, a single-\nlayer TEM is similar to ZAM but with a larger range of controlling\npersonalization. A multiple-layer TEM takes into consideration the\ninteractions between purchased items so that it could learn poten-\ntially better dynamic representations of queries and items, which\nprobably lead to better attention weights. We also compare and\nanalyze the ability of personalization between our model and the\nzero-attention model theoretically in Section 3.5. Our experimental\nresults on the Amazon product search dataset [10, 12] show that\nTEM significantly outperforms state-of-the-art baselines.\narXiv:2005.08936v1  [cs.IR]  18 May 2020\ni u 2 I u\n<latexit sha1_base64=\"Ov4IWY+Ulv5jgzW8+KAsvQ6SBSs=\">AAAB8nicbVBNS8NAEN3Ur1q/qh69LBbBU0mqoMeiF71VsB+QhrDZbtqlm03YnRVK6M/w4kERr/4ab/4bt20O2vpg4PHeDDPzokxwDa777ZTW1jc2t8rblZ3dvf2D6uFRR6dGUdamqUhVLyKaCS5ZGzgI1ssUI0kkWDca38787hNTmqfyESYZCxIylDzmlICVfB4a3OcS34cmrNbcujsHXiVeQWqoQCusfvUHKTUJk0AF0dr33AyCnCjgVLBppW80ywgdkyHzLZUkYTrI5ydP8ZlVBjhOlS0JeK7+nshJovUkiWxnQmCkl72Z+J/nG4ivg5zLzACTdLEoNgJDimf/4wFXjIKYWEKo4vZWTEdEEQo2pYoNwVt+eZV0GnXvot54uKw1b4o4yugEnaJz5KEr1ER3qIXaiKIUPaNX9OaA8+K8Ox+L1pJTzByjP3A+fwB5PZC5</latexit>\n…\n…\nTransformer\n…\n…\n…\nPosition\nq\n<latexit sha1_base64=\"b+n3e1YQEa9MneUTanzPe56IiCk=\">AAAB6HicbVDLTgJBEOzFF+IL9ehlIjHxRHbRRI9ELx4hkUcCGzI79MLI7Ow6M2tCCF/gxYPGePWTvPk3DrAHBSvppFLVne6uIBFcG9f9dnJr6xubW/ntws7u3v5B8fCoqeNUMWywWMSqHVCNgktsGG4EthOFNAoEtoLR7cxvPaHSPJb3ZpygH9GB5CFn1Fip/tgrltyyOwdZJV5GSpCh1it+dfsxSyOUhgmqdcdzE+NPqDKcCZwWuqnGhLIRHWDHUkkj1P5kfuiUnFmlT8JY2ZKGzNXfExMaaT2OAtsZUTPUy95M/M/rpCa89idcJqlByRaLwlQQE5PZ16TPFTIjxpZQpri9lbAhVZQZm03BhuAtv7xKmpWyd1Gu1C9L1ZssjjycwCmcgwdXUIU7qEEDGCA8wyu8OQ/Oi/PufCxac042cwx/4Hz+ANzrjPk=</latexit>\n0 | I u |\n<latexit sha1_base64=\"32YWr2BmWZL6eg8JPqN/MSNg22k=\">AAAB7HicbVBNTwIxEJ3iF+IX6tFLIzHxRHbRRI9EL3rDxAUS2JBu6UJDt7tpuyZk4Td48aAxXv1B3vw3FtiDgi+Z5OW9mczMCxLBtXGcb1RYW9/Y3Cpul3Z29/YPyodHTR2nijKPxiJW7YBoJrhknuFGsHaiGIkCwVrB6Hbmt56Y0jyWj2acMD8iA8lDTomxkje576WTXrniVJ058Cpxc1KBHI1e+avbj2kaMWmoIFp3XCcxfkaU4VSwaambapYQOiID1rFUkohpP5sfO8VnVunjMFa2pMFz9fdERiKtx1FgOyNihnrZm4n/eZ3UhNd+xmWSGibpYlGYCmxiPPsc97li1IixJYQqbm/FdEgUocbmU7IhuMsvr5JmrepeVGsPl5X6TR5HEU7gFM7BhSuowx00wAMKHJ7hFd6QRC/oHX0sWgsonzmGP0CfP/FCjsU=</latexit>\nUnit\ni\n<latexit sha1_base64=\"hA2BMjcr0btVk5qY1/HOr4xO94Q=\">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mqoMeiF48t2FpoQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgpr6xubW8Xt0s7u3v5B+fCoreNUMWyxWMSqE1CNgktsGW4EdhKFNAoEPgTj25n/8IRK81jem0mCfkSHkoecUWOlJu+XK27VnYOsEi8nFcjR6Je/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSrlW9i2qteVmp3+RxFOEETuEcPLiCOtxBA1rAAOEZXuHNeXRenHfnY9FacPKZY/gD5/MH0MuM8Q==</latexit>\n…w i\n<latexit sha1_base64=\"YJdX0+1wby6sDm7ZMY+D+o5uk4c=\">AAAB6nicbVBNS8NAEJ34WetX1aOXxSJ4KkkV9Fj04rGi/YA2lM120i7dbMLuRimhP8GLB0W8+ou8+W/ctjlo64OBx3szzMwLEsG1cd1vZ2V1bX1js7BV3N7Z3dsvHRw2dZwqhg0Wi1i1A6pRcIkNw43AdqKQRoHAVjC6mfqtR1Sax/LBjBP0IzqQPOSMGivdP/V4r1R2K+4MZJl4OSlDjnqv9NXtxyyNUBomqNYdz02Mn1FlOBM4KXZTjQllIzrAjqWSRqj9bHbqhJxapU/CWNmShszU3xMZjbQeR4HtjKgZ6kVvKv7ndVITXvkZl0lqULL5ojAVxMRk+jfpc4XMiLEllClubyVsSBVlxqZTtCF4iy8vk2a14p1XqncX5dp1HkcBjuEEzsCDS6jBLdShAQwG8Ayv8OYI58V5dz7mrStOPnMEf+B8/gBgWI3b</latexit>\nw i 2 R i\n<latexit sha1_base64=\"+fwQprW9rN8h6LvzcqueI6jKZ8Q=\">AAAB8nicbVBNS8NAEN34WetX1aOXxSJ4KkkV9Fj04rGK/YA0hM120y7d7IbdiVJCf4YXD4p49dd489+4bXPQ1gcDj/dmmJkXpYIbcN1vZ2V1bX1js7RV3t7Z3duvHBy2jco0ZS2qhNLdiBgmuGQt4CBYN9WMJJFgnWh0M/U7j0wbruQDjFMWJGQgecwpASv5TyHHPS7xfcjDStWtuTPgZeIVpIoKNMPKV6+vaJYwCVQQY3zPTSHIiQZOBZuUe5lhKaEjMmC+pZIkzAT57OQJPrVKH8dK25KAZ+rviZwkxoyTyHYmBIZm0ZuK/3l+BvFVkHOZZsAknS+KM4FB4en/uM81oyDGlhCqub0V0yHRhIJNqWxD8BZfXibtes07r9XvLqqN6yKOEjpGJ+gMeegSNdAtaqIWokihZ/SK3hxwXpx352PeuuIUM0foD5zPH3f3kLg=</latexit>\n…\n…\nq\n(0)\n<latexit sha1_base64=\"lkqb9QzBc6UhzswiemaPhRH5QRo=\">AAAB7nicbVDLSgNBEOz1GeMr6tHLYBDiJexGQY9BLx4jmAckMcxOZpMhs7PrTK8QlnyEFw+KePV7vPk3TpI9aGJBQ1HVTXeXH0th0HW/nZXVtfWNzdxWfntnd2+/cHDYMFGiGa+zSEa65VPDpVC8jgIlb8Wa09CXvOmPbqZ+84lrIyJ1j+OYd0M6UCIQjKKVmo8Pack9m/QKRbfszkCWiZeRImSo9QpfnX7EkpArZJIa0/bcGLsp1SiY5JN8JzE8pmxEB7xtqaIhN910du6EnFqlT4JI21JIZurviZSGxoxD33aGFIdm0ZuK/3ntBIOrbipUnCBXbL4oSCTBiEx/J32hOUM5toQyLeythA2ppgxtQnkbgrf48jJpVMreeblyd1GsXmdx5OAYTqAEHlxCFW6hBnVgMIJneIU3J3ZenHfnY9664mQzR/AHzucPi+KPDA==</latexit>\ni\n(0)\nu 1\n<latexit sha1_base64=\"L6D3/cUwzFc5GsfQeJ1616dXH00=\">AAAB83icbVDLSgNBEOyNrxhfUY9eBoMQL2E3CnoMevEYwTwgWcPsZDYZMju7zEMIy/6GFw+KePVnvPk3TpI9aGJBQ1HVTXdXkHCmtOt+O4W19Y3NreJ2aWd3b/+gfHjUVrGRhLZIzGPZDbCinAna0kxz2k0kxVHAaSeY3M78zhOVisXiQU8T6kd4JFjICNZW6rNBarzsMa2659mgXHFr7hxolXg5qUCO5qD81R/GxERUaMKxUj3PTbSfYqkZ4TQr9Y2iCSYTPKI9SwWOqPLT+c0ZOrPKEIWxtCU0mqu/J1IcKTWNAtsZYT1Wy95M/M/rGR1e+ykTidFUkMWi0HCkYzQLAA2ZpETzqSWYSGZvRWSMJSbaxlSyIXjLL6+Sdr3mXdTq95eVxk0eRxFO4BSq4MEVNOAOmtACAgk8wyu8OcZ5cd6dj0VrwclnjuEPnM8fTgmRMw==</latexit>\nq\n(1)\n<latexit sha1_base64=\"l+EiwbjuHw6lOVlftc7CHlnVh7w=\">AAAB7nicbVDLSgNBEOz1GeMr6tHLYBDiJexGQY9BLx4jmAckMcxOZpMhs7PrTK8QlnyEFw+KePV7vPk3TpI9aGJBQ1HVTXeXH0th0HW/nZXVtfWNzdxWfntnd2+/cHDYMFGiGa+zSEa65VPDpVC8jgIlb8Wa09CXvOmPbqZ+84lrIyJ1j+OYd0M6UCIQjKKVmo8Pack7m/QKRbfszkCWiZeRImSo9QpfnX7EkpArZJIa0/bcGLsp1SiY5JN8JzE8pmxEB7xtqaIhN910du6EnFqlT4JI21JIZurviZSGxoxD33aGFIdm0ZuK/3ntBIOrbipUnCBXbL4oSCTBiEx/J32hOUM5toQyLeythA2ppgxtQnkbgrf48jJpVMreeblyd1GsXmdx5OAYTqAEHlxCFW6hBnVgMIJneIU3J3ZenHfnY9664mQzR/AHzucPjWiPDQ==</latexit>\nq\n( l \u0000 1)\n<latexit sha1_base64=\"fwcqpldH7UWoBfiZYlqIkfqjIh4=\">AAAB8HicbVBNSwMxEJ31s9avqkcvwSLUg2W3CnosevFYwX5Iu5Zsmm1Dk+yaZIWy9Fd48aCIV3+ON/+NabsHbX0w8Hhvhpl5QcyZNq777Swtr6yurec28ptb2zu7hb39ho4SRWidRDxSrQBrypmkdcMMp61YUSwCTpvB8HriN5+o0iySd2YUU1/gvmQhI9hY6f7xIS3xU+9k3C0U3bI7BVokXkaKkKHWLXx1ehFJBJWGcKx123Nj46dYGUY4Hec7iaYxJkPcp21LJRZU++n04DE6tkoPhZGyJQ2aqr8nUiy0HonAdgpsBnrem4j/ee3EhJd+ymScGCrJbFGYcGQiNPke9ZiixPCRJZgoZm9FZIAVJsZmlLchePMvL5JGpeydlSu358XqVRZHDg7hCErgwQVU4QZqUAcCAp7hFd4c5bw4787HrHXJyWYO4A+czx/EWI+6</latexit>\nq\n( l )\n<latexit sha1_base64=\"WX0KtqwWyFvjY8sB7Tf06RNIKYI=\">AAAB7nicbVDLSgNBEOz1GeMr6tHLYBDiJexGQY9BLx4jmAckMcxOZpMhs7PrTK8QlnyEFw+KePV7vPk3TpI9aGJBQ1HVTXeXH0th0HW/nZXVtfWNzdxWfntnd2+/cHDYMFGiGa+zSEa65VPDpVC8jgIlb8Wa09CXvOmPbqZ+84lrIyJ1j+OYd0M6UCIQjKKVmo8PaUmeTXqFolt2ZyDLxMtIETLUeoWvTj9iScgVMkmNaXtujN2UahRM8km+kxgeUzaiA962VNGQm246O3dCTq3SJ0GkbSkkM/X3REpDY8ahbztDikOz6E3F/7x2gsFVNxUqTpArNl8UJJJgRKa/k77QnKEcW0KZFvZWwoZUU4Y2obwNwVt8eZk0KmXvvFy5uyhWr7M4cnAMJ1ACDy6hCrdQgzowGMEzvMKbEzsvzrvzMW9dcbKZI/gD5/MH50qPSA==</latexit>\ni u 2\n<latexit sha1_base64=\"wTFwULN74cIu+wR4A0RTksuWtAM=\">AAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKexGQY9BLx4jmAckS5idzCZjZmeWeQhhyT948aCIV//Hm3/jJNmDJhY0FFXddHdFKWfa+P63V1hb39jcKm6Xdnb39g/Kh0ctLa0itEkkl6oTYU05E7RpmOG0kyqKk4jTdjS+nfntJ6o0k+LBTFIaJngoWMwINk5qsX5ma9N+ueJX/TnQKglyUoEcjX75qzeQxCZUGMKx1t3AT02YYWUY4XRa6llNU0zGeEi7jgqcUB1m82un6MwpAxRL5UoYNFd/T2Q40XqSRK4zwWakl72Z+J/XtSa+DjMmUmuoIItFseXISDR7HQ2YosTwiSOYKOZuRWSEFSbGBVRyIQTLL6+SVq0aXFRr95eV+k0eRxFO4BTOIYArqMMdNKAJBB7hGV7hzZPei/fufSxaC14+cwx/4H3+AJSdjyE=</latexit>\ni u 1\n<latexit sha1_base64=\"2PSzc6W6yKagyI1S1ZU9GuwW58I=\">AAAB7XicbVBNSwMxEJ3Ur1q/qh69BIvgqexWQY9FLx4r2A9ol5JNs21sNlmSrFCW/gcvHhTx6v/x5r8xbfegrQ8GHu/NMDMvTAQ31vO+UWFtfWNzq7hd2tnd2z8oHx61jEo1ZU2qhNKdkBgmuGRNy61gnUQzEoeCtcPx7cxvPzFtuJIPdpKwICZDySNOiXVSi/ez1J/2yxWv6s2BV4mfkwrkaPTLX72BomnMpKWCGNP1vcQGGdGWU8GmpV5qWELomAxZ11FJYmaCbH7tFJ85ZYAjpV1Ji+fq74mMxMZM4tB1xsSOzLI3E//zuqmNroOMyyS1TNLFoigV2Co8ex0PuGbUiokjhGrubsV0RDSh1gVUciH4yy+vklat6l9Ua/eXlfpNHkcRTuAUzsGHK6jDHTSgCRQe4Rle4Q0p9ILe0ceitYDymWP4A/T5A5MYjyA=</latexit>\ni u |I u |\n<latexit sha1_base64=\"mwOn3XtgM77bW9l4ViS/Gzd3wVM=\">AAAB9XicbVDJTgJBEK3BDXFDPXppJCaeyAya6JHoRW+YyJIw46SnaaBDz5JeNGTgP7x40Biv/os3/8YG5qDgSyp5ea8qVfWChDOpbPvbyq2srq1v5DcLW9s7u3vF/YOmjLUgtEFiHot2gCXlLKINxRSn7URQHAactoLh9dRvPVIhWRzdq1FCvRD3I9ZjBCsjPTA/1WO3dOtrtzSe+MWyXbFnQMvEyUgZMtT94pfbjYkOaaQIx1J2HDtRXoqFYoTTScHVkiaYDHGfdgyNcEill86unqATo3RRLxamIoVm6u+JFIdSjsLAdIZYDeSiNxX/8zpa9S69lEWJVjQi80U9zZGK0TQC1GWCEsVHhmAimLkVkQEWmCgTVMGE4Cy+vEya1YpzVqnenZdrV1kceTiCYzgFBy6gBjdQhwYQEPAMr/BmPVkv1rv1MW/NWdnMIfyB9fkDMHuSTg==</latexit>\ni\n(0)\nu 2\n<latexit sha1_base64=\"lRzG0a27uVCospbtak3J09aJRIM=\">AAAB83icbVDLSgNBEOyNrxhfUY9eBoMQL2E3CnoMevEYwTwgWcPsZDYZMju7zEMIy/6GFw+KePVnvPk3TpI9aGJBQ1HVTXdXkHCmtOt+O4W19Y3NreJ2aWd3b/+gfHjUVrGRhLZIzGPZDbCinAna0kxz2k0kxVHAaSeY3M78zhOVisXiQU8T6kd4JFjICNZW6rNBaurZY1p1z7NBueLW3DnQKvFyUoEczUH5qz+MiYmo0IRjpXqem2g/xVIzwmlW6htFE0wmeER7lgocUeWn85szdGaVIQpjaUtoNFd/T6Q4UmoaBbYzwnqslr2Z+J/XMzq89lMmEqOpIItFoeFIx2gWABoySYnmU0swkczeisgYS0y0jalkQ/CWX14l7XrNu6jV7y8rjZs8jiKcwClUwYMraMAdNKEFBBJ4hld4c4zz4rw7H4vWgpPPHMMfOJ8/T5SRNA==</latexit>\ni\n(0)\nu |I u |\n<latexit sha1_base64=\"l371pbuWuVv7xcSYKqCVwbjVyNo=\">AAAB/XicbVDJSgNBEO2JW4zbuNy8dAxCvISZKOgx6EVvEcwCmXHo6XSSJj0LvQhxMvgrXjwo4tX/8Obf2EnmoIkPCh7vVVFVz48ZFdKyvo3c0vLK6lp+vbCxubW9Y+7uNUWkOCYNHLGIt30kCKMhaUgqGWnHnKDAZ6TlD68mfuuBcEGj8E6OYuIGqB/SHsVIaskzD6iXqLFTvPGUUxyn90nZOkk9s2RVrCngIrEzUgIZ6p755XQjrAISSsyQEB3biqWbIC4pZiQtOEqQGOEh6pOOpiEKiHCT6fUpPNZKF/YiriuUcKr+nkhQIMQo8HVngORAzHsT8T+vo2Tvwk1oGCtJQjxb1FMMyghOooBdygmWbKQJwpzqWyEeII6w1IEVdAj2/MuLpFmt2KeV6u1ZqXaZxZEHh+AIlIENzkENXIM6aAAMHsEzeAVvxpPxYrwbH7PWnJHN7IM/MD5/AHVQlJI=</latexit>\ni\n(1)\nu |I u |\n<latexit sha1_base64=\"k0MmR9r2ZzcWuQ7uWOe9NuhTP6g=\">AAAB/XicbVDJSgNBEO2JW4zbuNy8dAxCvISZKOgx6EVvEcwCmXHo6XSSJj0LvQhxMvgrXjwo4tX/8Obf2EnmoIkPCh7vVVFVz48ZFdKyvo3c0vLK6lp+vbCxubW9Y+7uNUWkOCYNHLGIt30kCKMhaUgqGWnHnKDAZ6TlD68mfuuBcEGj8E6OYuIGqB/SHsVIaskzD6iXqLFTvPGUUxyn90nZPkk9s2RVrCngIrEzUgIZ6p755XQjrAISSsyQEB3biqWbIC4pZiQtOEqQGOEh6pOOpiEKiHCT6fUpPNZKF/YiriuUcKr+nkhQIMQo8HVngORAzHsT8T+vo2Tvwk1oGCtJQjxb1FMMyghOooBdygmWbKQJwpzqWyEeII6w1IEVdAj2/MuLpFmt2KeV6u1ZqXaZxZEHh+AIlIENzkENXIM6aAAMHsEzeAVvxpPxYrwbH7PWnJHN7IM/MD5/AHbWlJM=</latexit>\ni\n( l )\nu |I u |\n<latexit sha1_base64=\"NuLpN7VF3kMaV/44l+6sUe/Oz3w=\">AAAB/XicbVDJSgNBEO2JW4zbuNy8dAxCvISZKOgx6EVvEcwCmXHo6XSSJj0LvQhxMvgrXjwo4tX/8Obf2EnmoIkPCh7vVVFVz48ZFdKyvo3c0vLK6lp+vbCxubW9Y+7uNUWkOCYNHLGIt30kCKMhaUgqGWnHnKDAZ6TlD68mfuuBcEGj8E6OYuIGqB/SHsVIaskzD6iXqLFTvPGUUxyn90mZnaSeWbIq1hRwkdgZKYEMdc/8croRVgEJJWZIiI5txdJNEJcUM5IWHCVIjPAQ9UlH0xAFRLjJ9PoUHmulC3sR1xVKOFV/TyQoEGIU+LozQHIg5r2J+J/XUbJ34SY0jJUkIZ4t6ikGZQQnUcAu5QRLNtIEYU71rRAPEEdY6sAKOgR7/uVF0qxW7NNK9fasVLvM4siDQ3AEysAG56AGrkEdNAAGj+AZvII348l4Md6Nj1lrzshm9sEfGJ8/0LiUzg==</latexit>\ni\n( l \u0000 1)\nu |I u |\n<latexit sha1_base64=\"chzdUf/nwxpE1+lnIS8ySiPt3ok=\">AAAB/3icbVDLSsNAFJ3UV62vqODGzdQi1IUlqYIui250V8E+oIlhMp22QycP5iGUNAt/xY0LRdz6G+78G6dtFtp64MLhnHu59x4/ZlRIy/o2ckvLK6tr+fXCxubW9o65u9cUkeKYNHDEIt72kSCMhqQhqWSkHXOCAp+Rlj+8nvitR8IFjcJ7OYqJG6B+SHsUI6klzzygXqLGTvHWU05xnD4kZXZqn6SeWbIq1hRwkdgZKYEMdc/8croRVgEJJWZIiI5txdJNEJcUM5IWHCVIjPAQ9UlH0xAFRLjJ9P4UHmulC3sR1xVKOFV/TyQoEGIU+LozQHIg5r2J+J/XUbJ36SY0jJUkIZ4t6ikGZQQnYcAu5QRLNtIEYU71rRAPEEdY6sgKOgR7/uVF0qxW7LNK9e68VLvK4siDQ3AEysAGF6AGbkAdNAAGY/AMXsGb8WS8GO/Gx6w1Z2Qz++APjM8ft96VQA==</latexit>\ni\n(1)\nu 2\n<latexit sha1_base64=\"TYy9mXHVwtnGPrDJ/JUree0GwYE=\">AAAB83icbVDLSgNBEOyNrxhfUY9eBoMQL2E3CnoMevEYwTwgWcPsZDYZMju7zEMIy/6GFw+KePVnvPk3TpI9aGJBQ1HVTXdXkHCmtOt+O4W19Y3NreJ2aWd3b/+gfHjUVrGRhLZIzGPZDbCinAna0kxz2k0kxVHAaSeY3M78zhOVisXiQU8T6kd4JFjICNZW6rNBaurZY1r1zrNBueLW3DnQKvFyUoEczUH5qz+MiYmo0IRjpXqem2g/xVIzwmlW6htFE0wmeER7lgocUeWn85szdGaVIQpjaUtoNFd/T6Q4UmoaBbYzwnqslr2Z+J/XMzq89lMmEqOpIItFoeFIx2gWABoySYnmU0swkczeisgYS0y0jalkQ/CWX14l7XrNu6jV7y8rjZs8jiKcwClUwYMraMAdNKEFBBJ4hld4c4zz4rw7H4vWgpPPHMMfOJ8/URqRNQ==</latexit>\ni\n(1)\nu 1\n<latexit sha1_base64=\"7qwcRcn6Vf5qq9djwGyjar3Y+RM=\">AAAB83icbVDLSgNBEOyNrxhfUY9eBoMQL2E3CnoMevEYwTwgWcPsZDYZMju7zEMIy/6GFw+KePVnvPk3TpI9aGJBQ1HVTXdXkHCmtOt+O4W19Y3NreJ2aWd3b/+gfHjUVrGRhLZIzGPZDbCinAna0kxz2k0kxVHAaSeY3M78zhOVisXiQU8T6kd4JFjICNZW6rNBarzsMa1659mgXHFr7hxolXg5qUCO5qD81R/GxERUaMKxUj3PTbSfYqkZ4TQr9Y2iCSYTPKI9SwWOqPLT+c0ZOrPKEIWxtCU0mqu/J1IcKTWNAtsZYT1Wy95M/M/rGR1e+ykTidFUkMWi0HCkYzQLAA2ZpETzqSWYSGZvRWSMJSbaxlSyIXjLL6+Sdr3mXdTq95eVxk0eRxFO4BSq4MEVNOAOmtACAgk8wyu8OcZ5cd6dj0VrwclnjuEPnM8fT4+RNA==</latexit>\ni\n( l \u0000 1)\nu 1\n<latexit sha1_base64=\"VGk8/OiFfBuBbRoVI9tqkJnHV+4=\">AAAB9XicbVBNS8NAEJ3Ur1q/qh69LBahHixJK+ix6MVjBfsBbVo22027dLMJuxulhPwPLx4U8ep/8ea/cdvmoK0PBh7vzTAzz4s4U9q2v63c2vrG5lZ+u7Czu7d/UDw8aqkwloQ2SchD2fGwopwJ2tRMc9qJJMWBx2nbm9zO/PYjlYqF4kFPI+oGeCSYzwjWRuqzQRI7aT8p8wvnPB0US3bFngOtEicjJcjQGBS/esOQxAEVmnCsVNexI+0mWGpGOE0LvVjRCJMJHtGuoQIHVLnJ/OoUnRlliPxQmhIazdXfEwkOlJoGnukMsB6rZW8m/ud1Y+1fuwkTUaypIItFfsyRDtEsAjRkkhLNp4ZgIpm5FZExlphoE1TBhOAsv7xKWtWKU6tU7y9L9ZssjjycwCmUwYErqMMdNKAJBCQ8wyu8WU/Wi/VufSxac1Y2cwx/YH3+AIp9keE=</latexit>\ni\n( l )\nu 1\n<latexit sha1_base64=\"TZgj5ZXOM9G8xSuCGeg2VI7VGL8=\">AAAB83icbVDLSgNBEOyNrxhfUY9eBoMQL2E3CnoMevEYwTwgWcPsZDYZMju7zEMIy/6GFw+KePVnvPk3TpI9aGJBQ1HVTXdXkHCmtOt+O4W19Y3NreJ2aWd3b/+gfHjUVrGRhLZIzGPZDbCinAna0kxz2k0kxVHAaSeY3M78zhOVisXiQU8T6kd4JFjICNZW6rNBarzsMa3y82xQrrg1dw60SrycVCBHc1D+6g9jYiIqNOFYqZ7nJtpPsdSMcJqV+kbRBJMJHtGepQJHVPnp/OYMnVlliMJY2hIazdXfEymOlJpGge2MsB6rZW8m/uf1jA6v/ZSJxGgqyGJRaDjSMZoFgIZMUqL51BJMJLO3IjLGEhNtYyrZELzll1dJu17zLmr1+8tK4yaPowgncApV8OAKGnAHTWgBgQSe4RXeHOO8OO/Ox6K14OQzx/AHzucPqXGRbw==</latexit>\ni\n( l )\nu 2\n<latexit sha1_base64=\"TYP3suW8ee3CPwbxcJD+SJTlY+8=\">AAAB83icbVDLSgNBEOyNrxhfUY9eBoMQL2E3CnoMevEYwTwgWcPsZDYZMju7zEMIy/6GFw+KePVnvPk3TpI9aGJBQ1HVTXdXkHCmtOt+O4W19Y3NreJ2aWd3b/+gfHjUVrGRhLZIzGPZDbCinAna0kxz2k0kxVHAaSeY3M78zhOVisXiQU8T6kd4JFjICNZW6rNBaurZY1rl59mgXHFr7hxolXg5qUCO5qD81R/GxERUaMKxUj3PTbSfYqkZ4TQr9Y2iCSYTPKI9SwWOqPLT+c0ZOrPKEIWxtCU0mqu/J1IcKTWNAtsZYT1Wy95M/M/rGR1e+ykTidFUkMWi0HCkYzQLAA2ZpETzqSWYSGZvRWSMJSbaxlSyIXjLL6+Sdr3mXdTq95eVxk0eRxFO4BSq4MEVNOAOmtACAgk8wyu8OcZ5cd6dj0VrwclnjuEPnM8fqvyRcA==</latexit>\ni\n( l \u0000 1)\nu 2\n<latexit sha1_base64=\"y0fW5N7HOP1iqhnlRyg/HMZlGXY=\">AAAB9XicbVBNS8NAEJ3Ur1q/qh69LBahHixJK+ix6MVjBfsBbVo22027dLMJuxulhPwPLx4U8ep/8ea/cdvmoK0PBh7vzTAzz4s4U9q2v63c2vrG5lZ+u7Czu7d/UDw8aqkwloQ2SchD2fGwopwJ2tRMc9qJJMWBx2nbm9zO/PYjlYqF4kFPI+oGeCSYzwjWRuqzQRJX035S5hfOeTooluyKPQdaJU5GSpChMSh+9YYhiQMqNOFYqa5jR9pNsNSMcJoWerGiESYTPKJdQwUOqHKT+dUpOjPKEPmhNCU0mqu/JxIcKDUNPNMZYD1Wy95M/M/rxtq/dhMmolhTQRaL/JgjHaJZBGjIJCWaTw3BRDJzKyJjLDHRJqiCCcFZfnmVtKoVp1ap3l+W6jdZHHk4gVMogwNXUIc7aEATCEh4hld4s56sF+vd+li05qxs5hj+wPr8AYwKkeI=</latexit>\n1 2\nw i\n<latexit sha1_base64=\"YJdX0+1wby6sDm7ZMY+D+o5uk4c=\">AAAB6nicbVBNS8NAEJ34WetX1aOXxSJ4KkkV9Fj04rGi/YA2lM120i7dbMLuRimhP8GLB0W8+ou8+W/ctjlo64OBx3szzMwLEsG1cd1vZ2V1bX1js7BV3N7Z3dsvHRw2dZwqhg0Wi1i1A6pRcIkNw43AdqKQRoHAVjC6mfqtR1Sax/LBjBP0IzqQPOSMGivdP/V4r1R2K+4MZJl4OSlDjnqv9NXtxyyNUBomqNYdz02Mn1FlOBM4KXZTjQllIzrAjqWSRqj9bHbqhJxapU/CWNmShszU3xMZjbQeR4HtjKgZ6kVvKv7ndVITXvkZl0lqULL5ojAVxMRk+jfpc4XMiLEllClubyVsSBVlxqZTtCF4iy8vk2a14p1XqncX5dp1HkcBjuEEzsCDS6jBLdShAQwG8Ayv8OYI58V5dz7mrStOPnMEf+B8/gBgWI3b</latexit>\nw i\n<latexit sha1_base64=\"YJdX0+1wby6sDm7ZMY+D+o5uk4c=\">AAAB6nicbVBNS8NAEJ34WetX1aOXxSJ4KkkV9Fj04rGi/YA2lM120i7dbMLuRimhP8GLB0W8+ou8+W/ctjlo64OBx3szzMwLEsG1cd1vZ2V1bX1js7BV3N7Z3dsvHRw2dZwqhg0Wi1i1A6pRcIkNw43AdqKQRoHAVjC6mfqtR1Sax/LBjBP0IzqQPOSMGivdP/V4r1R2K+4MZJl4OSlDjnqv9NXtxyyNUBomqNYdz02Mn1FlOBM4KXZTjQllIzrAjqWSRqj9bHbqhJxapU/CWNmShszU3xMZjbQeR4HtjKgZ6kVvKv7ndVITXvkZl0lqULL5ojAVxMRk+jfpc4XMiLEllClubyVsSBVlxqZTtCF4iy8vk2a14p1XqncX5dp1HkcBjuEEzsCDS6jBLdShAQwG8Ayv8OYI58V5dz7mrStOPnMEf+B8/gBgWI3b</latexit>\nFigure 1: Our Transformer-based Embedding Model (TEM).\n2 RELATED WORK\nProduct Search. Earlier work on product search mainly considers\nproducts as structured entities and uses facets for the task [ 13].\nLanguage model based approaches have been studied [7] for key-\nword search. To alleviate word mismatch problems, more recently,\nVan Gysel et al. [12] introduce a latent semantic entity model that\nmatches products and queries in the latent semantic space. Learning\nto rank techniques have also been investigated [ 9]. In the scope\nof personalized product search, Ai et al. [2] use a convex combina-\ntion between query and user embeddings for personalization; Guo\net al. [8] represent users’ long and short-term preferences with an\nattention mechanism; Ai et al. [1] provide insight on when person-\nalization could be beneficial and propose a zero-attention model to\ncontrol how personalization takes effect. Personalization has also\nbeen studied in multi-page product search [4].\nTransformer-based Retrieval Models. Studies on retrieval\nwith transformers have been sparse and most of them leverage\npretrained contextual language models, i.e., BERT [ 6], which is\ngrounded on the transformer architecture. It achieves compelling\nperformance on a wide variety of tasks such as passage ranking\n[11] and document retrieval [5].\n3 TRANSFORMER-BASED EMBEDDING\nMODEL (TEM)\nIn this section, we first introduce each component of TEM as shown\nin Figure 1 and then compare TEM with ZAM theoretically.\n3.1 Item Generation Model\nWe use an item generation model to capture the purchase relation-\nship between an item and query-user pairs. This embedding-based\ngeneration framework has been shown to be effective by previous\nstudies on personalized product search [1–3]. Formally, let q be a\nquery issued by a user u and i be an item in Si which is the set of\nall the items in a collection. The probability of i being purchased\nby u given q is modeled as\nP(i|q,u)= exp(i ·Mqu )Í\ni′∈Si exp(i′·Mqu) (1)\nwhere i ∈Rd is the vector representation of dimension size d and\nMqu is the representation by jointly modeling the query-user pair\n(q,u). We will elaborate how to yield Mqu later.\n3.2 Query Representation\nAs shown in previous studies [1, 2, 12], an effective way to encode\nquery is to apply a non-linear projection ϕ on the average query\nword embeddings:\nq = ϕ({wq |wq ∈q})= tanh(Wϕ ·\nÍ\nwq ∈q wq\n|q| + bϕ) (2)\nwhere Wϕ ∈Rd×d and bϕ ∈Rd , |q|is the length of query q, and\nwq ∈Rd is the embedding of word wq in q. This way of encoding\nqueries has outperformed other techniques such as using average\nword embeddings and applying recurrent neural networks on the\nword embedding sequence for product search [2].\n3.3 Item Language Model\nAs in [ 2, 3], item embeddings are learned from their associated\nreviews. Let Ri be the set of words in the reviews associated with\nitem i. Embeddings of words and items are optimized to maximize\nthe likelihood of observing Ri given i:\nP(Ri |i)=\nÖ\nw ∈Ri\nexp(w ·i)Í\nw′∈V exp(w′·i) (3)\nwhere V is the vocabulary of words in the corpus.\n3.4 Transformer-based Personalization\nDifferent queries may need various degrees of personalization [1].\nSome can be satisfied with popular items in general and some\ncorrelates closely with users’ historical purchases. To represent the\npurchase intent with query-dependent personalization, we leverage\na transformer encoder [14] architecture to capture the interaction\nbetween query and users’ historical purchased items, as shown\nin Figure 1. Let Iu = (iu1, iu2, ··· , iu |Iu |)be the sequence of items\npurchased by u in a chronological order, the size of which is |Iu |.\nWe feed the sequence (q, Iu )as the input to a l-layer transformer\nencoder. Since a recent purchase may play a different role compared\nwith a long-ago purchase, in addition to query and item embeddings\nof a corresponding unit (query or item), positional embeddings\n(PosEmb ) are used to indicate the purchase order of each item. The\ninput vectors to the transformer are:\nq(0)= q + PosEmb (0); i(0)\nuk = iuk + PosEmb (k), iuk ∈Iu (4)\nwhere q and iuk can be computed according to Eq. 2 & 3 respectively.\nThen user u’s purchase intent given q, i.e., Mqu , can be repre-\nsented with the output vector of query q at the l-th layer, i.e.,\nMqu = q(l) (5)\nWe use q(l)as Mqu because it is computed by attending to each\ntransformer input using query q which is more reasonable than\nother output vectors that attend to the input with a previously\npurchased item. Specifically, q(l)is computed as a weighted com-\nbination of embeddings of query and purchased items from the\nTable 1: Statistics of the Amazon datasets.\nDataset Cell Phones Sports Movies\n#Users 27,879 35,598 123,960\n#Items 10,429 18,357 50,052\n#Reivews 194,439 296,337 1,697,524\n#Queries 165 1,543 248\nprevious transformer layer followed by a projection function:\nq(l)=д\n\u0010 exp\u0000f(q(l−1)\nQ , q(l−1)\nK )\u0001\nexp\u0000f (q(l−1)\nQ , q(l−1)\nK )\u0001+Í\ni′∈Iuexp\u0000f(q(l−1)\nQ , i\n′(l−1)\nK )\u0001·q(l−1)\nV\n+\nÕ\ni ∈Iu\nexp\u0000f(q(l−1)\nQ , i(l−1)\nK )\u0001\nexp\u0000f (q(l−1)\nQ , q(l−1)\nK )\u0001+Í\ni′∈Iuexp\u0000f(q(l−1)\nQ , i\n′(l−1)\nK )\u0001·i(l−1)\nV\n\u0011\n(6)\nIn Eq. 6, f (x,y)computes attention score of ywith respect to x. As\nin [14], д is a projection function that firstly applies f for multi-\nple attention heads followed by a feed-forward layer and residual\nconnections for both the multi-head attention sub-layer and the\nfeed-forward sub-layer. q(l−1)\nQ , q(l−1)\nK , and q(l−1)\nV are computed ac-\ncording to:\nq(l−1)\nQ = q(l−1)W Q\nq ;q(l−1)\nK = q(l−1)W K\nq ;q(l−1)\nV = q(l−1)W V\nq (7)\nwhere W Q\nq ∈Rd×(d/h), W Kq ∈Rd×(d/h)and W Vq ∈Rd×(d/h)are\nprojection matrices; q(l−1)is the embedding of q at the (l −1)-th\ntransformer layer; andh is the number of attention heads.i(l−1)\nK and\ni(l−1)\nV in Eq. 6 are computed similarly based on i(l−1), i.e., the vector\nof i at (l −1)-th layer. In this way, TEM can have the capability of\nZAM to coordinate personalization and is more general and flexible,\nas shown in the next section where we will illustrate the relation\nbetween TEM with ZAM.\n3.5 Comparison with Zero-attention Model\nIn ZAM [1], query and user are jointly modeled by:\nMqu = q +\nÕ\ni ∈Iu\nexp \u0000f ′(q, i)\u0001\nexp \u0000f ′(q, 0)\u0001 + Í\ni′∈Iu exp \u0000f ′(q, i′)\u0001 ·i (8)\nwhere f ′is a multi-head attention function. In ZAM, when q does\nnot require personalization or it has no useful purchase history to\nattend to, all the items in Iu would have small attention weights,\nwhich allows Mqu to include information only from q. When per-\nsonalization has great potential for q, most attention is allocated to\nthe historical purchases Iu rather than the zero vector. Eq. 8 shows\nthat the maximum personalization ZAM can conduct is to consider\nIu equally important to q. However, in some cases, personalization\ncould have a larger impact than queries. Ai et al. [2] has shown that\nthe optimal query weight could be much lower than the user weight\non some product categories where personalization is indispensable.\nIn contrast, TEM based on Eq. 6 can learn to balance the influence\nof personalization for each query automatically without limits on\nthe personalization degree. Specifically, the query weight can be\nas small as 0 when personalization is dominant and as large as 1\nwhen personalization is not needed at all.\nIn addition, when l = 1, q(l−1)and i(l−1)in Eq. 6 become q0 and\ni(0)(shown in Eq. 4) respectively. In this case, the only difference\nbetween query and items representations of TEM and ZAM is the\npositional embeddings. When l > 1, q(l−1)and i(l−1)are learned\nfrom previous transformer layers by interacting with all the units\nin the sequence (q, Iu ). In this way, the query and items are dy-\nnamically represented depending on its interaction with the other\nunits associated with this q-u pair rather than having static vectors\nacross the corpus. By considering the relation between historical\npurchased items, e.g., same brands or categories, TEM could learn\npotentially better representation to facilitate product search.\n4 EXPERIMENTS\nDatasets. We use the Amazon product search dataset [10] for exper-\niments, as in previous work [2, 3, 12]. Since there are no available\nqueries for this dataset, we construct queries for each item fol-\nlowing the same strategy as in [ 2, 3, 12]. A query string of each\npurchased item is formed by concatenating words in the multi-level\ncategory of the item and removing stopwords as well as duplicate\nwords. In this way, there could be multiple queries for each item\nsince an item may belong to multiple categories. The user and\neach query associated with her purchased item are considered as\nthe possible query-user pairs that lead to purchasing the item. We\nuse three categories of different scales for experiments, which are\nCellphones &Accessories , Sports &Outdoors and Movies &TV . The\nstatistics are shown in Table 1.\nEvaluation. We randomly divide 70% of all the available queries\ninto the training set and the rest 30% queries are shared by validation\nand test sets. If all the queries of a purchased item fall in the test set,\nwe randomly put one query back to the training set. We partition\nthe purchases of a user to the training/validation/test set according\nto the ratio 0.8/0.1/0.1 in a chronological order. For any purchase\nin the validation or test set, if none of the queries associated with\nthe purchased item are in the query set for validation and test,\nthis purchase will be moved back to the training set. Our partition\nensures that the purchases in the test set happen after the purchases\nin the training set and no test query has been seen in the training\nset. We use MRR, Precision, and NDCG at 20 as the metrics.\nBaselines. We include five representative product search mod-\nels as baselines: the Latent Semantic Entity model (LSE) [12] which\nis an embedding-based non-personalized model; Query Embedding\nModel (QEM) [1], another non-personalized model, which conducts\nitem generation (Sec. 3.1 & 3.2) based on the query alone; Hierar-\nchical Embedding Model (HEM) [2] which balances the effect of\npersonalization by applying a convex combination of user and query\nrepresentation; the Attention-based Embedding Model (AEM) [1]\nwhich constructs query-dependent user embeddings by attending\nto users’ historical purchases with query, similar to the attention\nmodel proposed by Guo et al. [8]; a state-of-the-art model: the Zero\nAttention Model (ZAM) [1] which introduces a zero vector to AEM\nso that the influence of personalization can be differentiated for\nvarious queries. We only include neural models as our baselines\nsince term-based models have been shown to be much less effective\nfor product search in previous studies [1–3].\nTraining. We train our model and all the baselines for 20 epochs\nwith 384 samples in each batch. We set the embedding size of all\nthe models to 128 and sweep the number of attention heads h from\n{1,2,4,8} for attention-based models. The number of transformer\nTable 2: Comparison between the baselines and our proposed TEM. ‘*’ marks the bast baseline performance. ‘ †’ indicates\nsignificant improvements over all the baselines in paired student t-test with p < 0.05.\nDataset Cell Phones & Accessories Sports & Outdoors Movies & TV\nModel MRR N DCG@20 P@20 MRR N DCG@20 P@20 MRR N DCG@20 P@20\nNon-personalized LSE 0.013 0.022 0.004 0.010 0.021 0.004 0.010 0.015 0.002\nQEM 0.029 0.036 0.003 0.031 0.044 0.006 0.004 0.006 0.001\nPersonalized\nHEM 0.044* 0.057* 0.006* 0.032 0.049 0.007* 0.007 0.011 0.002\nAEM 0.043 0.049 0.004 0.031 0.045 0.006 0.013* 0.020 0.003*\nZAM 0.041 0.046 0.004 0.040* 0.057* 0.007* 0.013* 0.022* 0.003*\nTEM 0.056† 0.072† 0.007† 0.049† 0.074† 0.010† 0.020† 0.028† 0.004†\nlayers l is chosen from {1,2,3} and the dimension size of the feed-\nforward sub-layer of the transformer is set from {96, 128, 256, 512}.\nAdam with learning rate 0.0005 is used to optimize the models.\nResults. Table 2 shows the ranking performance of the baseline\nmodels and TEM 1. Similar to previous studies [1–3], we observe\nthat LSE and QEM perform worse than personalized product search\nbaselines in most cases. If we compare the personalized product\nsearch baselines, HEM has the best performance on Cell Phones\nwhereas ZAM performs the best on Sports and Movies. Specifically,\nHEM and AEM achieve better results than ZAM on Cell Phones\nand worse results on the other two datasets. This indicates that,\nwhile adjusting the influence of personalization with the attention\nweights on the zero vector could benefit the retrieval performance\nof ZAM, its limitation on personalization (i.e., the personalization\nweight can be no larger than the query weight) could harm the\nsearch quality on datasets where personalization is essential.\nOn all the categories, TEM achieves the best performance in\nterms of all the three metrics. The improvement upon the best\nbaseline on each dataset is approximately 20% to 50%. From the\nimprovement of Precision, NDCG, and MRR, we can infer that\nTEM not only retrieves more ideal items in the top 20 results but\nalso promotes them to higher positions. This demonstrates that\nTEM can benefit the effectiveness of personalized models with a\nmore flexible mechanism to control the influence of personalization\nand by learning dynamic item representations with the interaction\nbetween items taken into consideration.\nEffect of Layer Numbers . We varied the number of trans-\nformer layers to see whether a single-layer or multi-layer trans-\nformer will lead to better results on each dataset. The best perfor-\nmance of TEM is achieved whenl in Eq. 6 is set to 2 onSports and 1\non Cell Phones as well as Movies. This indicates that considering the\ninteractions between items does benefit the personalized product\nsearch models in some product categories.\n5 CONCLUSION AND FUTURE WORK\nIn this paper, we propose a transformer-based embedding model,\nabbreviated as TEM, that can conduct query-dependent personal-\nization. By encoding the sequence of the query and users’ purchase\nhistory with a transformer architecture, the effect of personalization\ncan vary from none to domination. We theoretically compare TEM\nwith ZAM [1] and show that a single-layer TEM is an advanced ver-\nsion of ZAM with more flexibility and a multi-layer TEM extends\n1The numbers in Table 2 are smaller than those reported by Ai et al . [2] since they\nrandomly split user purchases to training and test set which makes the prediction of\npurchases in their test set easier than predicting future purchases in our test set.\nthe model with stronger learning abilities by incorporating the\ninteractions between items co-occurring in users’ purchase history.\nOur experiments empirically demonstrate the effectiveness of TEM\nby showing that TEM outperforms the state-of-the-art personalized\nproduct search baselines significantly. For future work, we consider\nstudying TEM for explainable product search. The attention scores\nin TEM indicate the personalization degree and which historical\nitems draw more attention for retrieving a result. This information\ncould be helpful for users to make purchase decisions. In addition,\nwe are also interested in incorporating other information about\nproducts such as price, ratings, and images with a transformer\narchitecture to facilitate personalized product search.\nACKNOWLEDGMENTS\nThis work was supported in part by the Center for Intelligent In-\nformation Retrieval. Any opinions, findings and conclusions or\nrecommendations expressed in this material are those of the au-\nthors and do not necessarily reflect those of the sponsor.\nREFERENCES\n[1] Qingyao Ai, Daniel N Hill, SVN Vishwanathan, and W Bruce Croft. 2019. A zero\nattention model for personalized product search. In CIKM’19. 379–388.\n[2] Qingyao Ai, Yongfeng Zhang, Keping Bi, Xu Chen, and W Bruce Croft. 2017.\nLearning a hierarchical embedding model for personalized product search. In\nSIGIR’17. ACM, 645–654.\n[3] Keping Bi, Qingyao Ai, Yongfeng Zhang, and W Bruce Croft. 2019. Conversational\nproduct search based on negative feedback. In CIKM’19. 359–368.\n[4] Keping Bi, Choon Hui Teo, Yesh Dattatreya, Vijai Mohan, and W Bruce Croft. 2019.\nA Study of Context Dependencies in Multi-page Product Search. In CIKM’19.\n[5] Zhuyun Dai and Jamie Callan. 2019. Deeper text understanding for IR with\ncontextual neural language modeling. In SIGIR’19. 985–988.\n[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:\nPre-training of deep bidirectional transformers for language understanding.arXiv\npreprint arXiv:1810.04805 (2018).\n[7] Huizhong Duan, ChengXiang Zhai, Jinxing Cheng, and Abhishek Gattani. 2013.\nA probabilistic mixture model for mining and analyzing product search log. In\nCIKM’13. ACM, 2179–2188.\n[8] Yangyang Guo, Zhiyong Cheng, Liqiang Nie, Yinglong Wang, Jun Ma, and Mohan\nKankanhalli. 2019. Attentive long short-term preference modeling for personal-\nized product search. TOIS 37, 2 (2019), 1–27.\n[9] Shubhra Kanti Karmaker Santu, Parikshit Sondhi, and ChengXiang Zhai. 2017.\nOn application of learning to rank for e-commerce search. In SIGIR’17. ACM,\n475–484.\n[10] Julian McAuley, Rahul Pandey, and Jure Leskovec. 2015. Inferring networks of\nsubstitutable and complementary products. In SIGKDD’15. ACM, 785–794.\n[11] Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage Re-ranking with BERT.\narXiv preprint arXiv:1901.04085 (2019).\n[12] Christophe Van Gysel, Maarten de Rijke, and Evangelos Kanoulas. 2016. Learning\nlatent vector spaces for product search. In CIKM’16. ACM, 165–174.\n[13] Damir Vandic, Flavius Frasincar, and Uzay Kaymak. 2013. Facet selection algo-\nrithms for web product search. In CIKM’13. ACM, 2327–2332.\n[14] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information processing systems . 5998–6008.",
  "topic": "Personalization",
  "concepts": [
    {
      "name": "Personalization",
      "score": 0.8992881774902344
    },
    {
      "name": "Computer science",
      "score": 0.7692167162895203
    },
    {
      "name": "Embedding",
      "score": 0.5580297112464905
    },
    {
      "name": "Information retrieval",
      "score": 0.5444406867027283
    },
    {
      "name": "Product (mathematics)",
      "score": 0.5110121965408325
    },
    {
      "name": "Transformer",
      "score": 0.4521805942058563
    },
    {
      "name": "World Wide Web",
      "score": 0.3029286861419678
    },
    {
      "name": "Artificial intelligence",
      "score": 0.17473459243774414
    },
    {
      "name": "Mathematics",
      "score": 0.11958962678909302
    },
    {
      "name": "Engineering",
      "score": 0.07892733812332153
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I24603500",
      "name": "University of Massachusetts Amherst",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I223532165",
      "name": "University of Utah",
      "country": "US"
    }
  ]
}