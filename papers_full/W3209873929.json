{
    "title": "Hierarchical Transformers Are More Efficient Language Models",
    "url": "https://openalex.org/W3209873929",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A1298852000",
            "name": "Piotr Nawrot",
            "affiliations": [
                "University of Warsaw"
            ]
        },
        {
            "id": "https://openalex.org/A3211252161",
            "name": "Szymon Tworkowski",
            "affiliations": [
                "University of Warsaw"
            ]
        },
        {
            "id": "https://openalex.org/A5055219424",
            "name": "Michał Tyrolski",
            "affiliations": [
                "University of Warsaw"
            ]
        },
        {
            "id": "https://openalex.org/A2948063405",
            "name": "Lukasz Kaiser",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2341526960",
            "name": "Yuhuai Wu",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A331124168",
            "name": "Christian Szegedy",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2096949012",
            "name": "Henryk Michalewski",
            "affiliations": [
                "Google (United States)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2946567085",
        "https://openalex.org/W3118608800",
        "https://openalex.org/W3015468748",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4287122956",
        "https://openalex.org/W3034573343",
        "https://openalex.org/W4309793872",
        "https://openalex.org/W2994673210",
        "https://openalex.org/W3177813494",
        "https://openalex.org/W4287802874",
        "https://openalex.org/W3177150392",
        "https://openalex.org/W3181262653",
        "https://openalex.org/W2998108143",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3037798801",
        "https://openalex.org/W3023662336",
        "https://openalex.org/W3001279689",
        "https://openalex.org/W3174418826",
        "https://openalex.org/W1901129140",
        "https://openalex.org/W3033188311",
        "https://openalex.org/W3166509360",
        "https://openalex.org/W3030163527",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W3091156754",
        "https://openalex.org/W2423557781",
        "https://openalex.org/W3034445277",
        "https://openalex.org/W3131922516",
        "https://openalex.org/W2953318193",
        "https://openalex.org/W2996264288",
        "https://openalex.org/W3123673616",
        "https://openalex.org/W3021164770",
        "https://openalex.org/W2940744433",
        "https://openalex.org/W3135427360",
        "https://openalex.org/W3181186005",
        "https://openalex.org/W3155806510",
        "https://openalex.org/W4323654151",
        "https://openalex.org/W4388979610",
        "https://openalex.org/W3164045210",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W3171313410"
    ],
    "abstract": "Transformer models yield impressive results on many NLP and sequence modeling tasks. Remarkably, Transformers can handle long sequences, which allows them to produce long coherent outputs: entire paragraphs produced by GPT-3 or well-structured images produced by DALL-E. These large language models are impressive but also very inefficient and costly, which limits their applications and accessibility. We postulate that having an explicit hierarchical architecture is the key to Transformers that efficiently handle long sequences. To verify this claim, we first study different ways to downsample and upsample activations in Transformers so as to make them hierarchical. We use the best performing upsampling and downsampling layers to create Hourglass - a hierarchical Transformer language model. Hourglass improves upon the Transformer baseline given the same amount of computation and can yield the same results as Transformers more efficiently. In particular, Hourglass sets new state-of-the-art for Transformer models on the ImageNet32 generation task and improves language modeling efficiency on the widely studied enwik8 benchmark.",
    "full_text": "Findings of the Association for Computational Linguistics: NAACL 2022, pages 1559 - 1571\nJuly 10-15, 2022 ©2022 Association for Computational Linguistics\nHierarchical Transformers Are More Efficient Language Models\nPiotr Nawrot∗1, Szymon Tworkowski∗1, Michał Tyrolski1, Łukasz Kaiser2,\nYuhuai Wu3, Christian Szegedy3, Henryk Michalewski3\n1University of Warsaw,2OpenAI, 3Google Research\n{p.nawrot99, szy.tworkowski, michal.tyrolski, lukaszkaiser}@gmail.com,\n{yuhuai, szegedy, henrykm}@google.com\nAbstract\nTransformer models yield impressive results\non many NLP and sequence modeling tasks.\nRemarkably, Transformers can handle long se-\nquences, which allows them to produce long\ncoherent outputs: entire paragraphs produced\nby GPT-3 or well-structured images produced\nby DALL-E. These large language models are\nimpressive but also very inefficient and costly,\nwhich limits their applications and accessibil-\nity. We postulate that having an explicit hierar-\nchical architecture is the key to Transformers\nthat efficiently handle long sequences. To ver-\nify this claim, we first study different ways to\ndownsample and upsample activations in Trans-\nformers so as to make them hierarchical. We\nuse the best performing upsampling and down-\nsampling layers to create Hourglass - a hier-\narchical Transformer language model. Hour-\nglass improves upon the Transformer baseline\ngiven the same amount of computation and can\nyield the same results as Transformers more\nefficiently. In particular, Hourglass sets new\nstate-of-the-art for Transformer models on the\nImageNet32 generation task and improves lan-\nguage modeling efficiency on the widely stud-\nied enwik8 benchmark.\n1 Introduction\nTransformer models (Vaswani et al., 2017) are ca-\npable of solving many sequence modeling tasks,\nincluding classical NLP tasks (Devlin et al., 2019),\nsummarization (Zhang et al., 2020), language mod-\neling (Radford et al., 2019; Brown et al., 2020),\ncode generation (Chen et al., 2021), or even mu-\nsic generation (Huang et al., 2018; Dhariwal et al.,\n2020) and image generation (Parmar et al., 2018;\nChen et al., 2020; Ramesh et al., 2021). One com-\npelling feature of Transformers is their ability to\nhandle long contexts given as part of the input.\nThis is particularly visible in tasks where the out-\nput depends on parts of the context that may not be\n*Equal contribution. Order determined by coin toss.\nclose-by in the generated sequence, like in summa-\nrization, where the summary may need to refer to\ninformation scattered across the context, or in large-\nscale image generation, where pixels belonging to\nthe same object may be far apart in the generation\norder. Transformers excel at such tasks thanks to\nself-attention, and they are used with longer and\nlonger contexts.\n1.0 1.2 1.4 1.6 1.8 2.0 2.2 2.4\nSeconds per one training step\n1.08\n1.10\n1.12\n1.14\n1.16\n1.18Bit per character on enwik8 valid set\n6@1\n8@1\n10@1\n12@1\n14@1\n2@1 8@4 2@1\n2@1 4@4 2@1\n2@1 4@3 2@1\n2@1 8@3 2@1\n2@1 16@3 2@1\n3@1 8@4 3@1\n4@1 8@4 4@1\n4@1 3@3 4@6 3@3 4@1\n5@1 8@2 5@1\nTransformer-XL\nHourglass\nFigure 1: Bits-per-character vs. training cost for base-\nline (orange) and hierarchical Transformers (green). We\nobserve significant perplexity improvements on enwik8\nover the vanilla Transformer–XL baseline, see text for\ndetails.\nThe ability of Transformers to handle long con-\ntexts comes at a price: each self-attention layer, at\nleast in its original form, has complexity quadratic\nin the length of the context. When a stack of n\nTransformer layers is used, both memory and time\ncomplexity is equal to O(L2n) where L is a se-\nquence length and n number of decoder blocks.\nDue to this limitation, vanilla transformers are in-\nfeasible to train on tasks with very long input se-\nquences, for instance, on high-resolution images.\nThis issue has been studied extensively, and a num-\nber of techniques were introduced that modify at-\ntention mechanism without changing overall trans-\nformer architecture (Child et al., 2019; Roy et al.,\n2020; Ren et al., 2021). These sparse attention\nmechanisms reduce the complexity of self-attention\n1559\nbut still force the model to operate on the sequence\nof the same length as the input.\nFor generative Transformer models, operating at\nthe original scale of the input sequence is necessary,\nat least in the early and final layers, as the input\nmust be processed at first and generated at the end\n(Section 4.3). But forcing the models to operate at\nthis granularity throughout the layer stack has both\nfundamental and practical shortcomings:\n• Fundamentally, we aim for the models to cre-\nate high-level representations of words, enti-\nties, or even whole events – which occur at\na very different granularity than single letters\nthat the model receives on input.\n• On the practical side, even layers with linear\ncomplexity can be slow and memory-intensive\nwhen processing very long sequences.\nTo alleviate these issues, we propose to change\nthe Transformer architecture to first shorten the in-\nternal sequence of activations when going deeper\nin the layer stack and then expand it back before\ngeneration. We merge tokens into groups using a\nshortening operation (Section 2.1) and so reduce\nthe overall sequence length, and then up-sample\nthem again combining with the sequence from ear-\nlier layers (Section 2.3), The first part is analogous\nto the Funnel-Transformer architecture (Dai et al.,\n2020), and the whole architecture takes inspiration\nfrom U-Nets (Ronneberger et al., 2015). In contrast\nto both these architectures, the model we present is\nautoregressive, which is harder to ensure in hierar-\nchical models than in vanilla Transformers.\nThe resulting model – which we callHourglass –\nis an autoregressive Transformer language model\nthat operates on shortened sequences. It yields\nsignificant performance improvements for different\nattention types (Fig. 6,7). We tested Hourglass with\nTransformer-XL (Dai et al., 2019) and Reformer\n(Kitaev et al., 2020) blocks on enwik8 dataset. In\nboth cases, it is not only better in terms of perplex-\nity, but it is faster and uses less memory during\ntraining. We also propose a regularization tech-\nnique for hierarchical Transformers called shorten\nfactor dropout which improves perplexity upon\nbaselines trained with fixed shorten factor (see Sec-\ntion 4.1). Finally, Hourglass achieves the new state-\nof-the-art among Transformer models for image\ngeneration of ImageNet32 (see Tab. 3).\n2 Model\nStandard self-attention mechanism uses full token-\nlevel sequence representations. In the Hourglass,\nwe bring efficiency to the model by utilizing short-\nening, which allows us to use the Transformer lay-\ners on inputs with significantly smaller lengths. A\nhigh-level overview of our proposed model archi-\ntecture is shown in figures 2 and 3.\nAttention type in the vanilla layers and shortened\nlayers is a configurable parameter. By default we\nuse relative attention defined in Transformer-XL\n(Dai et al., 2019). Any attention module can be\nused - we show significant efficiency gains when\napplying Hourglass also for LSH (Kitaev et al.,\n2020) attention (see Section 3.2 and Fig. 7).\n2.1 Methods of shortening the input sequence\nShortening can be defined as any function S that\naccepts a tensor xof shape (l,d) and returns a ten-\nsor x′of shape ( l\nk ,d), where kis a hyperparameter\ncalled shorten factor.\nA simple shortening method is 1D average pool-\ning with stride kand pool size k, applied along the\nsequence dimension l. Another way of shortening\nis what we will further call linear pooling(land d\ndenote sequence length and dmodel):\nAlgorithm 2 LinearPooling\nx′←Reshape(x,( l\nk ,k ·d))\nx′←LinearProjection(x′)\nShortening can be also performed by attention,\nas was introduced in (Dai et al., 2020):x′= S(x)+\nAttention(Q = S(x),K = V = x) where S\nis shortening function, originally S = AvgPool.\nDirectly after this attention operation, a position-\nwise feed-forward with a residual is performed, so\nthat these two layers form a Transformer block\n(Vaswani et al., 2017). In this work we also try\nS = LinearPool and find it more effective on\nimage tasks (see Tab. 8).\n2.2 Shortening and autoregressive property\nInformation leaks Shortening interferes with the\nstandard causal masking used in Transformer de-\ncoders. Namely, in any shortened representation\nby a factor of keach shortened token contributes\nto predicting up to the next ktokens in the finest\nscale, that is if eis the shortened sequence and xis\nthe sequence on the finest scale, e0 is not only used\n1560\nShortening, sf = k1\nL tokens\nInput tokens\nsf = k2\nOutput tokens\nsf=k2\nUpsampling, sf = k1\nL/k1 tokens L/k1 tokens\nL tokens\nL/k1k2 tokens\nPre Vanilla Layers\nShortened1 Layers Shortened1 Layers\nShortened2 Layers\nPost Vanilla Layers\nFigure 2: Hourglass - a high-level architecture overview. The arrows denote residual connections.\nto generate x0; in fact, the same embedding is used\nto generate tokens x0,...,x k−1.\nTherefore, we need to guarantee that e0 and any\nother ei cannot access information about tokens\nthey will implicitly predict. To ensure that, we\napply another shift right by k−1 tokens, directly\nbefore any shortening by a factor ofk(Fig. 4). The\nshift is the smallest that does not cause an informa-\ntion leak (see Fig. 5 for an example of a shifting\nthat leads to a leak). We included a more detailed\nanalysis of this fact in the Appendix (Section A.2).\nReduced expressivity Let us consider an Hour-\nglass model with shortening by a factor of kand\nno transformer blocks operating on the finest scale\n(that is, a model without vanilla layers).\nIn this situation\nP(x) = ∏n−1\ni=0 P(xi|e0,...,e ⌊i\nk ⌋) =\n∏n−1\ni=0 P(xi|x0,...,x ⌊i\nk ⌋·k−1)\nbecause for predicting xi we combine the pro-\ncessing done on shortened representations ewith\ntoken-independent operations. This means token\nxi is generated independently from the tokens\nx⌊i\nk ⌋·k,...,x i−1. This situation is detrimental to\nthe model’s capabilities, though including at least\none vanilla layer solves this issue. In the Appendix\nwe provide a detailed example illustrating this prob-\nlem (Section A.1).\n2.3 Upsampling methods\nUpsampling is a crucial part of the Hourglass ar-\nchitecture since we need to convert shortened rep-\nresentations back to the full token-level sequence\nin order to perform language modeling.\nA method proposed in (Dai et al., 2020) is re-\npeating each shortened vector shorten factortimes.\nThis method is computationally efficient, but it\ndoes not distinguish tokens with respect to position\ninside the group.\nAnother method is linear upsampling which\nworks analogously to linear pooling – it projects\nvectors of shape ( l\nk ,d) to ( l\nk ,k ·d) and then re-\nshapes to l vectors, each of dimension d. This\nmethod is fast and allows to project shortened em-\nbeddings differently for each position in the group.\nThis happens because the (k·d) ×dprojection ma-\ntrix can be thought of as kseparate d×dmatrices,\none per each position.\nWe also investigated a method which we further\ncall attention upsampling. It is similar to atten-\ntion pooling (Dai et al., 2020) and to the aggre-\ngation layer from (Subramanian et al., 2020). It\nworks as follows: x= U(x,x′)+ Attention(Q=\nU(x,x′),K = V = x′) where xare embeddings\nfrom just before the shortening, x′are final short-\nened embeddings and U is an arbitrary upsampling\nfunction. After the attention operation there is also\na residual with a feed-forward layer.\nLinear upsampling learns a fixed pattern that\nis the same for each shortened token. Attention\nupsampling has the advantage of being content-\nbased – each token can extract relevant infor-\nmation from the shortened embeddings. We set\nU(x,x′) = x+ LinearUpsampling(x′) which\nallows to explicitly inject group-level information\ninto the attention queries. We experimentally show\nthat variants of attention upsampling lead to the\nbest results for our model across different datasets\n(see Tab. 7).\n1561\nAlgorithm 1 HourglassLM\nprocedure HOURGLASS (x,[k,...s_factors])\nx←PreVanillaLayers (x)\nx′←Shortening(ShiftRight(x,k−1),k)\nif EMPTY (s_factors) then\nx′←ShortenedLayers(x′)\nelse\nx′←HOURGLASS (x′,s_factors)\nend if\nx←x+ Upsampling(x,x′,k)\nx←PostVanillaLayers (x)\nreturn x\nFigure 3: The architecture starts with pre vanilla layers\n– a stack of Transformer blocks operating on the full\ntoken-level sequence. After them we insert shortening\nlayer where kis the shorten factorparameter (Fig. 4).\nThe sequence is shifted right before shortening to pre-\nvent information leak (Fig. 5). Then we recursively\ninsert another Hourglass block operating on k times\nsmaller scale. On the final level of shortening, we apply\nshortened layers– Transformer blocks operating on the\nsmallest scale. Upsampling layerbrings the resulting\nactivations x′ back to the original resolution. After up-\nsampling and residual, the activations are processed by\ntoken-level post vanilla layers.\n3 Experiments\nIn this section, we present experimental results of\nHourglass. We start with a quick analysis of time\nand memory complexity of the approach (Section\n3.1). Then we investigate the efficiency gains of\napplying Hourglass to Transformers with different\nattention types (Section 3.2). Finally, we use Hour-\nglass with relative attention parametrization from\nTransformer-XL (Dai et al., 2019), evaluate it on\nthree language modeling tasks, and compare the\nresults with other models. (Sections 3.3, 3.4)\nTo show cross-domain generalization of our\nmethod, we train our model on one dataset related\nto Natural Language Processing and two from the\nComputer Vision field.\nTo ensure consistency in presenting config-\nurations of our model, we introduce a nota-\ntion describing hierarchy of our architecture:\n(N1@f1,...,N k@fk) where each entry (Nj@fj)\nmeans Nj layers shortened by factor fj.\nOur model implementation is open source.1\n1github.com/google/trax/blob/master/trax/models/research/hourglass.py\nInitial \nShiftRight(1) \nShortening \nShiftRight(sf-1) \nFigure 4: An overview of our shortening approach. Dif-\nferent colors denote token positions. Initially, we shift\nright by one, which is a standard step in TransformerLM.\nThen, just before performing shortening, we additionally\nshift the tokens right by shorten factor−1 to preserve\nthe autoregressive property of the model.\nShortening \nShiftRight(sf-2) \nUpsampling \nFigure 5: An example of information leak. If the shift\nright factor is too small, after upsampling the knowledge\nfrom the next tokens leaks to previous ones violating\nautoregressiveness and making decoding impossible.\n3.1 Computational cost analysis\nIn vanilla Transformers, the number of parameters\ncan indicate the computation required to train the\nmodel. This is not true for Hourglass – for instance,\nit can have 128 layers operating on a sequence\nshortened by 32 and still fit into the memory of\na single GPU. A weak correlation between true\nHourglass’ computational cost and its number of\nparameters can be observed in Table 1.\nHourglass achieves the biggest speedup with\nthe standard O(l2) attention. In that case, a\nsingle shortening by a shorten factor k reduces\nthe complexity to O( l2\nk2 ) so by a factor of k2.\nFor more recent linear-time attention mechanisms\n(Katharopoulos et al., 2020; Choromanski et al.,\n2021) the reduction would be smaller – but still by\na factor of k. Feed-forward layers also have linear\ncomplexity so shortening reduces it by a factor of\nk.\nIn Table 1 we show an empirical efficiency com-\nparison between Hourglass and Transformer-XL.\n1562\nHierarchy BPC GB Speed #Param\n6@1 (Baseline) 1.182 4 .53 0 .95 21M\n2@1 1@3 2@1 1.163 4 .41 1 .11 24M\n2@1 4@4 2@1 1.143 4.41 1 .10 34M\n8@1 (Baseline) 1.151 5 .75 0 .73 28M\n2@1 4@3 2@1 1.128 4 .88 1 .00 34M\n2@1 8@4 2@1 1.128 4 .98 0 .99 48M\n2@1 1@2 4@4 1@2 2@1 1.115 4 .69 0 .86 48M\n2@1 8@3 2@1 1.111 5.50 0 .88 48M\n10@1 (Baseline) 1.128 6 .99 0 .56 34M\n3@1 8@4 3@1 1.109 6.14 0 .76 55M\n12@1 (Baseline) 1.115 8 .12 0 .47 41M\n4@1 8@4 4@1 1.098 7 .20 0 .62 62M\n2@1 16@3 2@1 1.096 5.89 0 .71 75M\n14@1 (Baseline) 1.102 9 .35 0 .40 48M\n5@1 8@2 5@1 1.079 9.57 0 .45 69M\nTable 1: Efficiency comparison between Hourglass vari-\nants and Transformer-XL baseline on enwik8 – we re-\nport validation set perplexity (BPC), running memory\n(GB) and number of training steps per second (Speed).\nWe observe significant perplexity gains over the baseline\nfor a matching computation cost. It is also visible that\nfor Hourglass the number of model parameters (#Param)\ncorrelates poorly with true computational cost.\n3.2 Impact of Hourglass\nTo demonstrate the efficiency of Hourglass, we\nmeasured how computational cost decreases and\nperplexity improves, purely adding the technique\nto Transformer-XL (Dai et al., 2019) and Re-\nformer (Kitaev et al., 2020) backbones (results de-\npicted in Figures 6 and 7, respectively).\nIn both cases, models are implemented under\nthe same codebase and the only difference between\nHourglass and its corresponding baseline is the us-\nage of shortening and upsampling layers. We show\nthat by incorporating a single shortening of the in-\nput, we can train larger models with the same mem-\nory requirements and training speed and achieve\nbetter perplexity than baselines.\n3.3 Enwik8\nEnwik8 (Mahoney, 2011) is a byte-level language\nmodeling benchmark containing the first 100M\nbytes of unprocessed English Wikipedia text, split\ninto 90M train, 5M valid, and 5M test sets.\nSimilarly to (Dai et al., 2019) and (Beltagy\net al., 2020), we evaluate our model on the test\nset, splitting it into overlapping sequences of size\nl = 4096 with a step size of 128 and calcu-\nlate the test loss only over the last 128 tokens.\nWith a (4@1,8@3,4@1) hierarchy, dmodel = 768,\ndff = 3072and 8 heads, we reach 0.98 test bits-\nper-character.\n5 6 7 8 9\nMaximum observed memory during training [GB]\n1.08\n1.10\n1.12\n1.14\n1.16\n1.18Bit per character on enwik8 valid set\n6\n8\n10\n12\n14\n2@1 8@4 2@1\n3@1 8@4 3@1\n2@1 16@3 2@1\n4@1 3@3 4@6 3@3 4@1\n5@1 8@2 5@1\nTransformer-XL\nHourglass\nFigure 6: Comparison between Transformer-XL base-\nline and Hourglass on Enwik8 valid set w.r.t. maximum\nmemory used during training. All models are trained\nfor 200k steps with the same hyperparameters.\n0.6 0.8 1.0 1.2 1.4 1.6\nSeconds per one training step\n1.125\n1.150\n1.175\n1.200\n1.225\n1.250\n1.275Bit per character on enwik8 valid set\n6\n9\n12\n16\n20\n2@1 6@3 2@1\n3@1 9@3 3@1\n4@1 12@3 4@1\n6@1 15@3 6@1\nBaseline (LSH)\nHourglass (LSH)\nFigure 7: Comparison between Reformer baseline and\nHourglass, both with LSH attention, on Enwik8 valid\nset w.r.t. cost of one training step in seconds.\n3.4 Image Generation\nWe use datasets introduced in (van den Oord et al.,\n2016a) which are downsampled versions of the pop-\nular ImageNet. In the autoregressive image genera-\ntion setup, they consist of respectively32×32×3\nand 64 × 64 × 3 tokens, corresponding to RGB\nchannels, per image. As the only preprocessing\nstep we flatten the images.\n3.4.1 ImageNet32\nFor our main result the following hierarchy is\nused: ( 3@1,24@3,3@1). We use dmodel = 512,\ndff = 2048, 8 attention heads and0.01 dropout rate.\nWith this configuration we achieve 3.741 bits/dim,\nyielding the new state-of-the-art among autoregres-\nsive (Transformer-based) models on this dataset,\ncompared to the previous state-of-the-art of 3.758\nbpd by (Ho et al., 2019).\n1563\nEnwik8 #Param BPC\nTransformer-XL (2019) 24L 277M 0.99\nHourglass 146M 0.98\nAdaptive-Span (2019) 24L 209M 0.98\nTransformer-LS (2021) 110M 0.97\nFeedback Transformer (2021) 77M 0.96\nExpire-Span (2021) 24L 277M 0.95\nTable 2: Enwik8 Results. We report bits-per-character\n(BPC) on the test set and number of model parameters.\nHourglass applied to Transformer-XL significantly out-\nperforms its baseline. Our technique could be also used\nwith other more performant attention methods which\nwe leave for future work.\nCompletions \nInput \nCompletions \nInput \nFigure 8: Examples of our model completions, where\nbottom half of each image was generated by our model,\nprompted by the upper half.\n3.4.2 ImageNet64\nThe sequence length that our model can handle is\nlimited mainly by the computational complexity of\nused attention module. We replace relative atten-\ntion in vanilla layers by LSH attention (Kitaev et al.,\n2020), which allows us to handle 12288-long se-\nquences. To achieve relative attention parametriza-\ntion, the LSH attention is combined with rotary\npositional embeddings (Su et al., 2021). In short-\nened layers, standard relative attention is used. For\nLSH attention, we set chunk length to 128 and use\n2 hashes, which results in small memory complex-\nity in our full-size layers. In this setup, we reach\na score of 3.443 bpd with a (3@1,12@3,3@1) ar-\nchitecture. All attention layers had dmodel = 768,\ndff = 3072and 8 heads. No dropout was used.\n3.4.3 CIFAR-10\nCIFAR-10 (Krizhevsky, 2009) is an image dataset\nconsisting of 60000 images of size 32x32. We use\nthis dataset primarily for our ablations (Section 4).\nDue to the relatively small number of examples\ncompared to ImageNet, models reach convergence\nafter 100k steps.\n4 Ablations\nIn this section, we start by introducing a training\ntechnique called shorten factor dropout(Section\n4.1), and then analyze Hourglass’s components de-\nImageNet32 BPD\nPixelCNN (van den Oord et al., 2016b) 3.83\nImage Transformer (Parmar et al., 2018) 3.77\nAxial Transformer (Ho et al., 2019) 3.76\nHourglass 3.74\nVDM (Kingma et al., 2021) 3.72\nDenseFlow (Grci´c et al., 2021) 3.63\nImageNet64 BPD\nReformer (Kitaev et al., 2020) 3.65\nPerformer (Choromanski et al., 2021) 3.64\nHourglass 3.44\nSparse Transformer (Child et al., 2019) 3.44\nRouting Transformer (Roy et al., 2020) 3.43\nCombiner (Ren et al., 2021) 3.42\nVDM (2021) 3.40\nDenseFlow (2021) 3.35\nTable 3: Bits per Dimension (BPD) on downsampled\nimagenet. Autoregressive models are separated by a\nhorizontal line from non-autoregressive ones. On Ima-\ngeNet32, our model yields new state-of-the-art for au-\ntoregressive models.\nscribed above. We show that shortened layers be-\nhave similarly to full token-level layers in terms\nof scalability (Section 4.2). Then we study the ef-\nfect of different distributions of (pre,post) vanilla\nlayers on Hourglass’ accuracy (Section 4.3). We\nfurther analyze the performance of various upsam-\npling and downsampling methods (Sections 4.4 and\n4.5). Finally, we discuss different shorten factors\nand multi-stage shortening in Section 4.6.\nWe conduct the ablations on both text and image\ngeneration to show applicability across different\ndomains. We report bits per character (BPC) on\nthe enwik8 validation (dev) set evaluated without\ncontext (sequence length 2048) and bits per dim\n(BPD) on the CIFAR-10 test set. For the exact\nhyperparameter setup refer to the Appendix.\n4.1 Shorten factor dropout\nDifferent shorten factors can be used for the same\nmodel when using parameterless pooling methods.\nWe propose a training procedure where the shorten\nfactor is randomly sampled with uniform distribu-\ntion from a predefined set in each step. We observe\nthat such a training regime improves validation\nloss compared to a baseline trained with a single,\nfixed shorten factor. For example, a model trained\nwith shorten factor randomly sampled from {2,3}\nperforms better when evaluated with any of these\nshorten factors, compared to models trained with a\ncorresponding fixed shorten factor (Tab. 4).\nWe hypothesise that such a technique promotes\na more uniform distribution of information over\nthe sequence of tokens. It may be essential for\nfixed-size pooling techniques as they do not ac-\n1564\ncount for variable length constituents like words.\nBy spreading information uniformly, we prevent a\nsituation where we lose content by shortening three\ninformation-dense tokens or lose available capacity\nby merging three low information ones.\nShorten factor dropout is not limited to our ar-\nchitecture and can be applied to any model that\nutilizes shortening, particularly (Dai et al., 2020).\nHierarchy Train k Val k = 2 Val k = 3\n2@1 8@k 2@1 {2, 3} 1.104 1.116\n2 1.116\n3 1.124\n4@1 12@k 4@1 {2, 3} 1.086 1.094\n2 1.098\n3 1.101\n5@1 10@k 5@1 {2, 3} 1.082 1.087\n2 1.096\n3 1.095\nTable 4: Comparison between models trained with\nshorten factor dropout (Train k = {2,3}, Section 4.1)\nand fixed shorten factor baselines on enwik8.\n4.2 Scaling shortened layers\nIn this study, we show that layers operating on\nthe shortened sequence contribute significantly to\nHourglass’s accuracy. In Table 5 we measure the\nimpact of scaling the depth of the shortened part of\nthe model with a fixed number of vanilla layers.\nWe also check if scaling laws of Transformers,\ndescribed in (Kaplan et al., 2020), hold by com-\nparing a regression line fitted to various Hourglass\nconfigurations and one fitted to Transformer-XL\nbaseline. We observe in Figure 1 that the slopes are\nvery similar, which indicates that the laws hold.\nNumber of shortened layers enwik8 CIFAR-10\nBaseline (n = 1) 1.164 3 .28\nn = 4 1 .134 3 .16\nn = 8 1 .111 3 .07\nn = 16 1.096 3.03\nTable 5: Impact of increasing the number of shortened\nlayers on perplexity. Vanilla layers: (1,1) for CIFAR-\n10 and (2,2) for enwik8, shorten factor 3 used in both.\n4.3 Impact of vanilla layers\nWe observe a significant contribution to Hourglass’\nperformance with increasing the number of vanilla\nlayers. One reason is that we perform more compu-\ntations as in vanilla layers we process the sequence\nin token-level - no shortening is applied. We also\nsee that the distribution of vanilla layers before\nshortening and after shortening does impact the\ntraining (see Tab. 6), and equal distribution leads\nto the best perplexity.\nVanilla layers enwik8 CIFAR-10\n(0, 0) 1.460 3 .429\n(0, 2) 1.176 3 .108\n(2, 0) 1.189 3 .035\n(1, 1) 1.171 3.012\n(2, 2) 1.128 2 .966\nTable 6: Impact of the distribution of vanilla layers on\nenwik8 (BPC) and CIFAR-10 score (BPD). We see that\nequal distribution of layers before and after shortening\nleads to better results on both datasets.\n4.4 Upsampling method\nIn Table 7 we investigate different possibilities of\nchoosing the upsampling method. For attention-\nfree methods, linear upsampling performs better\non images, while repeat upsampling works well for\ntext. Attention upsampling works well regardless\nof the function U and has the lowest perplexity.\nUpsampling method enwik8 CIFAR-10\nRepeat 1.148 3 .062\nLinear 1.163 3 .020\nU(x, x′) =x 1.145 2.967\nU(x, x′) =x + Linear(x′) 1.132 3.012\nTable 7: Upsampling method ablation - baseline config-\nurations are (2@1,24@4,2@1) and (1@1,8@3,1@1)\nfor enwik8 and CIFAR-10, respectively.\n4.5 Pooling method\nTable 8 presents impact of pooling method on\nboth enwik8 (BPC) and CIFAR-10 (BPD). Atten-\ntion pooling reaches the lowest perplexity for both\ndatasets. Average pooling performs well on text\namong attention-free methods, while linear pool-\ning works better for images. Both of these methods\nperform significantly worse for the other modality.\nAttention pooling demonstrates small differences\nwith respect to chosen shortening function S(Sec-\ntion 2.1), still preserving the preference towards\nlinear pooling on images and average pooling on\ntext.\nPooling method enwik8 CIFAR-10\nAvgPool 1.129 3 .116\nAttention, S = AvgPool 1.124 3.012\nAttention, S = LinearPool 1.142 2.998\nLinearPool 1.159 2.998\nTable 8: Ablation of pooling methods. Attention pool-\ning achieves the best perplexity on both datasets.\n4.6 Shortening strategies\nWhile the analysis above gives a clear indication of\nwhat methods to choose for shortening and upsam-\npling, we are still left with the question of which\nshorten factors to use and whether to do single-\nstage or multi-stage shortening.\n1565\nConsistently, it is beneficial to do at least one\nshortening and by a factor of at least 3, while keep-\ning 2-3 vanilla layers. Beyond that, a number of\ndifferent configurations can yield similar results. In\nTable 1 we present the different hierarchical con-\nfigurations that we tested on enwik8 and plotted in\nFigure 1. It can be seen that configurations with\nsimilar computation costs perform similarly. The\nsequence length used in these experiments is 2048\n– we hypothesise that more hierarchy may be bene-\nficial with even longer sequences.\n5 Related Work\nShortening in Transformers Shortening in our\nwork is inspired by Funnel-Transformer (Dai et al.,\n2020). The key difference is that they train an en-\ncoder model for text classification, where our work\nis entirely focused on language modeling, which\nprovides additional challenges we had to solve re-\ngarding shortening in the autoregressive setup (Sec-\ntion 2.2). Another difference is that they use repeat\nupsampling method while we use attention. There\nare also a few works related to character-level mod-\neling which use shortening, namely (Clark et al.,\n2021) and (Tay et al., 2021). However, the authors\nof these works focused mainly on shortening se-\nquence in encoder part of the transformer, whereas\nwe focused on applying shortening in decoder.\nThe idea of shortening is also discussed in (Sub-\nramanian et al., 2020). However, proposed architec-\ntures either focus on downsampling or upsampling,\nwhile Hourglass is a U-Net-like architecture and\nis symmetric in these terms. Their models use\ntransformer layers on the finest scales when post-\nprocessing final representations. We do these also,\nin the beginning, to preprocess tokens on the finest\nscale, and we have found it essential to the score\n(Section 4.3). Our attention upsampling method is\nsimilar to their aggregation layerin the bottom-up\nmodel, however we use one upsampling for each\nscale change while they combine different scales\nto create one global upsampling.\nRelative positional encoding Our work is pri-\nmarily built on the backbone of Transformer-XL\n(Dai et al., 2019) - we use the same relative at-\ntention parametrization. Instead of the segment-\nlevel recurrence mechanism, we use shortening\nto make our model more efficient and feasible to\ntrain on longer sequences. Another relative atten-\ntion parametrization is RoFormer (Su et al., 2021)\nwhere rotary positional embeddings are introduced.\nWe find this work particularly relevant because the\nmethod is compatible with any attention type, in-\ncluding efficient attention, and can be combined\nwith our model (Section 3.4.2).\nSparse Attention A well-known approach ad-\ndressing the memory bottleneck is utilizing sparsity\npatterns in the attention matrix - Routing (Roy et al.,\n2020) and Sparse Transformer (Child et al., 2019)\nare examples of such methods. Our solution is dif-\nferent in the sense that it uses full attention - just\nwith shortened sequence length. Combiner (Ren\net al., 2021) makes a step further and provides full\nattention capabilities with similar computational\ncomplexity to Routing and Sparse transformers by\nleveraging structured factorization. This work, sim-\nilarly to papers mentioned above on efficient trans-\nformers, concentrates on speeding up the attention\ncomponent, while the most important feature of\nthe Hourglass architecture is that it can use any\nattention module as a drop-in.\nImage generation on downsampled ImageNet\nVDM (Kingma et al., 2021) and DenseFlow (Grci´c\net al., 2021) are recently proposed state-of-the-art\nmethods for density estimation on this dataset. The\ndifference between these methods and Transformer-\nbased methods (Parmar et al., 2018; Ho et al., 2019)\nincluding this work is that the former, unlike Trans-\nformers, are non-autoregressive.\n6 Conclusion\nIn this paper, we show how hierarchy can improve\nthe efficiency of Transformers in a language mod-\neling setup. Our proposed architecture, Hourglass,\nsignificantly outperforms the baseline both in terms\nof perplexity reached at a given computation cost\n(Figure 1) and empirical metrics like running mem-\nory (Figure 6). Hourglass achieves state-of-the-art\nresults among autoregressive models on the Ima-\ngeNet32 generation task and competitive results\non other image generation and language modeling\ntasks.\nHourglass can be used with any attention type,\nwhich opens many directions for future research re-\nlated to Transformers capable of processing longer\nsequences. Another line of future work might be\nrelated to advances in the shortening mechanism\nitself, for example, involving a dynamic pooling\noperation that could explicitly handle the problem\nof fixed-size groups in multi-stage shortening. We\nalso leave open the problem of choosing the best hi-\n1566\nerarchy for a task. We conjecture that experiments\nwith much longer contexts will provide better guid-\nance for this choice and will benefit even more\nfrom the Hourglass architecture.\n7 Acknowledgments\nSome experiments were performed using the\nEntropy cluster funded by NVIDIA, Intel, the\nPolish National Science Center grant UMO-\n2017/26/E/ST6/00622, and ERC Starting Grant\nTOTAL. The work of Henryk Michalewski was sup-\nported by the Polish National Science Center grant\nUMO-2018/29/B/ST6/02959. The authors would\nlike to thank Marek Cygan and Kamil Wilczek for\ntheir help with cluster setup, and Grzegorz Grudz-\ni´nski, Dawid Jamka and Sebastian Jaszczur for\nhelpful discussions. This article describes a Team\nProgramming Project completed at the University\nof Warsaw in the academic year 20/21. We are\ngrateful to Janusz Jabłonowski, the head of Team\nProgramming Projects, for his support and open-\nmindedness.\nReferences\nIz Beltagy, Matthew E. Peters, and Arman Cohan. 2020.\nLongformer: The long-document transformer.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners.\nMark Chen, Alec Radford, Rewon Child, Jeffrey Wu,\nHeewoo Jun, David Luan, and Ilya Sutskever. 2020.\nGenerative pretraining from pixels. In Proceedings\nof the 37th International Conference on Machine\nLearning, volume 119 of Proceedings of Machine\nLearning Research, pages 1691–1703. PMLR.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-V oss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Josh Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. 2021. Evaluating\nlarge language models trained on code.\nRewon Child, Scott Gray, Alec Radford, and Ilya\nSutskever. 2019. Generating long sequences with\nsparse transformers.\nKrzysztof Choromanski, Valerii Likhosherstov, David\nDohan, Xingyou Song, Andreea Gane, Tamas Sar-\nlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin,\nLukasz Kaiser, David Belanger, Lucy Colwell, and\nAdrian Weller. 2021. Rethinking attention with per-\nformers.\nJonathan H. Clark, Dan Garrette, Iulia Turc, and John\nWieting. 2021. Canine: Pre-training an efficient\ntokenization-free encoder for language representa-\ntion.\nZihang Dai, Guokun Lai, Yiming Yang, and Quoc V . Le.\n2020. Funnel-transformer: Filtering out sequential\nredundancy for efficient language processing.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc V . Le, and Ruslan Salakhutdinov. 2019.\nTransformer-xl: Attentive language models beyond a\nfixed-length context.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning.\nPrafulla Dhariwal, Heewoo Jun, Christine Payne,\nJong Wook Kim, Alec Radford, and Ilya Sutskever.\n2020. Jukebox: A generative model for music.\nAngela Fan, Thibaut Lavril, Edouard Grave, Armand\nJoulin, and Sainbayar Sukhbaatar. 2021. Address-\ning some limitations of transformers with feedback\nmemory.\nMatej Grci ´c, Ivan Grubiši ´c, and Siniša Šegvi ´c. 2021.\nDensely connected normalizing flows.\nJonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and\nTim Salimans. 2019. Axial attention in multidimen-\nsional transformers.\nCheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszko-\nreit, Noam Shazeer, Ian Simon, Curtis Hawthorne,\nAndrew M. Dai, Matthew D. Hoffman, Monica Din-\nculescu, and Douglas Eck. 2018. Music transformer.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B.\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models.\n1567\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pap-\npas, and François Fleuret. 2020. Transformers are\nrnns: Fast autoregressive transformers with linear\nattention.\nDiederik P. Kingma, Tim Salimans, Ben Poole, and\nJonathan Ho. 2021. Variational diffusion models.\nNikita Kitaev, Łukasz Kaiser, and Anselm Levskaya.\n2020. Reformer: The efficient transformer.\nAlex Krizhevsky. 2009. Learning multiple layers of\nfeatures from tiny images.\nMatt Mahoney. 2011. Large text compression bench-\nmark.\nNiki Parmar, Ashish Vaswani, Jakob Uszkoreit, Łukasz\nKaiser, Noam Shazeer, Alexander Ku, and Dustin\nTran. 2018. Image transformer.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott\nGray, Chelsea V oss, Alec Radford, Mark Chen, and\nIlya Sutskever. 2021. Zero-shot text-to-image gener-\nation.\nHongyu Ren, Hanjun Dai, Zihang Dai, Mengjiao Yang,\nJure Leskovec, Dale Schuurmans, and Bo Dai. 2021.\nCombiner: Full attention transformer with sparse\ncomputation cost.\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox.\n2015. U-net: Convolutional networks for biomedical\nimage segmentation.\nAurko Roy, Mohammad Saffar, Ashish Vaswani, and\nDavid Grangier. 2020. Efficient content-based sparse\nattention with routing transformers.\nJianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng\nLiu. 2021. Roformer: Enhanced transformer with\nrotary position embedding.\nSandeep Subramanian, Ronan Collobert, Marc’Aurelio\nRanzato, and Y-Lan Boureau. 2020. Multi-scale\ntransformer language models.\nSainbayar Sukhbaatar, Edouard Grave, Piotr Bo-\njanowski, and Armand Joulin. 2019. Adaptive at-\ntention span in transformers.\nSainbayar Sukhbaatar, Da Ju, Spencer Poff, Stephen\nRoller, Arthur Szlam, Jason Weston, and Angela Fan.\n2021. Not all memories are created equal: Learning\nto forget by expiring.\nYi Tay, Vinh Q. Tran, Sebastian Ruder, Jai Gupta,\nHyung Won Chung, Dara Bahri, Zhen Qin, Simon\nBaumgartner, Cong Yu, and Donald Metzler. 2021.\nCharformer: Fast character transformers via gradient-\nbased subword tokenization.\nAaron van den Oord, Nal Kalchbrenner, and Koray\nKavukcuoglu. 2016a. Pixel recurrent neural net-\nworks. CoRR, abs/1601.06759.\nAaron van den Oord, Nal Kalchbrenner, Oriol\nVinyals, Lasse Espeholt, Alex Graves, and Koray\nKavukcuoglu. 2016b. Conditional image generation\nwith pixelcnn decoders.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need.\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Pe-\nter J. Liu. 2020. Pegasus: Pre-training with extracted\ngap-sentences for abstractive summarization.\nChen Zhu, Wei Ping, Chaowei Xiao, Mohammad\nShoeybi, Tom Goldstein, Anima Anandkumar, and\nBryan Catanzaro. 2021. Long-short transformer: Ef-\nficient transformers for language and vision.\n1568\nA Autoregressive shortening\nIn Section 2.2 we address two problems of short-\nening in an autoregressive setup: information leaks\nand reduced expressivity. Here we study these is-\nsues in more detail.\nA.1 Motivation behind using vanilla layers\nAt first sight, it may be tempting to create hier-\narchical models that directly shorten the input to\nmaximize the efficiency gains. In this section, we\nexplain why vanilla layers are crucial for modeling\nat least some sequences, especially due to autore-\ngressivity.\nConsider a sequence modeling task where the\ninput is a random sequence with repeats, such\nas A#AC#CD#DB#B. The sequence consists of\nchunks L#L where L is a random uniform letter\nand # is a special symbol. A vanilla Transformer\nlanguage model can achieve 66% sequence accu-\nracy on this task – it cannot predict the token at the\nbeginning of the chunk, but it can predict the last\ntoken of the chunk by simply copying the token at\n2 positions earlier, which is possible using a vanilla\nself-attention layer.\nIt is however not easy to learn this task in a short-\nening setup when there are no vanilla layers operat-\ning on the finest scale – this is the situation defined\nin Reduced expressivitysubsection of Section 2.2.\nAssume shorten factor is k = 3 and the input is\nA#AB#BC#C. To avoid information leak, we shift\nthe input sequence right by1, and then byk−1 = 2\ndirectly before shortening. Then the sequence\nis 000A#AB#B. Our shortened embeddings are\nas follows: e0 = S(emb0,emb0,emb0),e1 =\nS(embA,emb#,embA) where emb is input em-\nbedding matrix and Sis a shortening function.\nShortened embeddings [000][A#A][B#B]\nShifted input embeddings 0A# AB# BC#\nTarget sequence A#A B#B C#C\nPositions 123 456 789\nTable 9: Example input sequence which is difficult to\nmodel without vanilla layers. The model can use only\ninput embeddings shifted by one from the residual and\nshortened embeddings (shorten factor is 3) to predict\nthe target sequence. Note that it is impossible to predict\ntokens at positions divisible by 3 using only that infor-\nmation.\nBecause no vanilla layers are used, for predic-\ntion we can use only shortened embeddings and\ninput token embeddings shifted right by 1 from\nthe residual connection. Note that to predict the A\ntoken at position 3 we can use only embedding of\nemb# and e0 - both of these contain no informa-\ntion so we are unable to predict this token better\nthan randomly (see Table 9). An analogous situa-\ntion occurs for prediction of any tokens at positions\ndivisible by 3, which makes the model unable to\nachieve more than 50% accuracy when the task has\nvocabulary size of at least 2.\nThis issue can be solved by adding at least one\nvanilla layer to the model, so that it can attend\nwithin the neighborhood of kprevious tokens. For\nthis particular problem, it is sufficient to use local\nattention with context size kin vanilla layers which\nis significantly more efficient than full attention.\nA.2 Information leaks – analysis\nA.2.1 Definition of autoregressive model\nFormally, given a target sequence,x= x1,...,x n,\nan autoregressive model (e.g. transformer\ndecoder) models the sequence as P(x) =∏n\ni=1 P(xi|x1,...,x i−1) and\n∀iP(xi|x1,...,x n) =P(xi|x1,...,x i−1)\nnamely xi token depends only on previous tokens,\nnever on itself nor next ones.\nA.2.2 Definition of information leak\nWe say that a leak was caused by function\nFn : An −→ An transforming sequence of\ninput tokens (x1,x2,...,x n) into another\nsequence (u1,...,u n) = F((x1,...,x n))\nwhen ∃i<j<nP(xi|x1,...,x i−1,xj) ̸=\nP(xi|x1,...,x i−1), namely there exists j ≥ i\nthat token xi depends on xj which violates the\nautoregressive property.\nA.2.3 Model representation\nLet Rk : An −→An be a shift right function which\nreindexes tokens by shifting each of them right by\nkpositions:\nRk((x1,x2,...xn)) = (0,..., 0\nk\n,x1,...,x n−k)\nSk : A∗−→A∗shortening function with factor\nk which takes on input x1,...,x n sequence and\nreturns s1,...,s m where m = n\nk , Uk upsampling\nfunction which works in similar way but upsamples\nUk((u1,...,u m)) =u1,...,u n.\nBetween them there is also applied Ddecoder\nfunction, D = D1 ◦···◦ Dl, where each Di is a\n1569\nfunction representing decoder block. Due to causal\nattention masking in the decoder block, there is no\nrisk of information leak caused by function D.\nA.2.4 Leak description\nBecause of mentioned attention mask, we will omit\nthe flow of information between tokens caused by\nthe influence of attention mechanism because this\nmask keeps the autoregressive property.\nNow, let (x1,...,x n) be an input sequence and\n(u1,...,u n) = U(D(Sk(Ts((x1,...,x n))))) = F.\nIn order to preserve autoregressive property, it is\nobligatory that no leak occurs.\nWe will show that shift by any value 0 < s <\nk−1 where k is the shorten factor will cause a\nleak.\nTo start with, consider input sequence (x1,...,x n)\nand perform operation F. Rs((x1,...,x n)) =\n(0,..., 0\ns\n,x1,...,x n−s) = r. Assuming that n is\ndivisible by s, we have Sk(r) = (v1,...,v n\nk\n) =v\nwhere each vi consists of information obtained in\n(r(i−1)·k+1,...,r ik). Now let see that operation D\npreserves autoregressive property, let d = D(t).\nNow, U(d) = (u1,...,u n) and each ui depends on\nd⌊i−1\nk ⌋+1.\nNow consider s≤k−2 and let (u1,...,u n) =\nF((x1,...,x n)) will be a result of our Transformer\npart. Let take u1 which depends on d1 and d1\ndepends on (r1,...,r k) = (0,..., 0,x1,...,x k−s).\nFor that reason d1 depends on x1,x2,...,x k−s, so\nwe have\nP(x1|xk−s) ̸= P(x1)\nwhich violates the autoregressive property.\nB Experimental setup\nB.1 Common parameters\nHere we list common hyperparameters used for all\nexperiments mentioned in the paper. We use Adam\noptimizer with β1 = 0.9, β2 = 0.98 and ϵ= 1e−9.\nWeight decay and gradient clipping is not used.\nIn terms of model details, we decided to use a\nPre-Norm architecture and FastGelu activation in\nfeed-forward layers.\nB.2 Enwik8\nWe use dmodel = 512, dff = 2048and 8 attention\nheads. Models in ablation study are trained for\n200k steps with cosine learning rate schedule, set-\nting cycle length for 200k steps and linear warmup\nof 4000 steps.\nFor the main result achieving 0.98 bpc with\n4@1,8@3,4@1 hierarchy, we set dmodel = 768,\ndff = 3072and nheads = 8which results in 146M\nparameters. It is trained for a total number of 350k\nsteps with one cycle of cosine schedule. Linear\nwarmup of 20k steps is used.\nAt the beginning of our work on this paper, we\nhave performed grid search over following hyper-\nparameters for enwik8:\n• batch size: {8,16,32}, finally chosen 8\n• dropout: {0.05,0.1,0.15,0.20}, finally cho-\nsen 0.15\n• learning rate:\n{1e−4,2e−4,3e−4,4e−4,5e−4},\nfinally chosen 4e−4\nAll next experiments were conducted using these\nparameters without additional searching.\nB.3 Downsampled ImageNet - common\nparameters\nFor ImageNet32 and ImageNet64 experiments we\nuse inverse square root learning rate decay from\n(Vaswani et al., 2017), setting warmup steps to\n8000 in both experiments. Total batch size is 64.\nB.4 ImageNet32\nIn this dataset, we operate on input sequence length\nof 3072. We use dmodel = 512, dff = 2048, 8\nattention heads and 0.01 dropout rate. We perform\n400k training steps with linear warmup and inverse\nsquare root decay and then we train for additional\n70k steps with cosine learning rate decay, starting\nfrom the learning rate from the previous schedule\nat 400k and decreasing it to 0 at 470k steps.\nB.5 ImageNet64\nAs an input we receive a sequence of 12288 tokens\nrepresenting 64 ×64 ×3 images. We set dmodel =\n768, dff = 3072, 8 attention heads and dropout\nequal to 0. We perform 300k steps with linear\nwarmup and inverse square root decay.\nB.6 CIFAR-10\nAll the ablation studies are run for 100k training\nsteps, unless otherwise specified. Input sequence\nhas length 3072 and model parameters are as fol-\nlows: dmodel = 512, dff = 2048, 8 attention heads\nand dropout equal to 0. Total batch size is 8. Co-\nsine learning rate decay with linear warmup of\n5000 steps and 100k cycle length is used.\n1570\nC Environment setup\nC.1 Hardware\nExperiments are conducted on several setups.\n• Ablation Study and short training sessions\nwere computed on nodes consisting of 4x Ti-\ntan Vwith 12GB memory each, 64GB RAM,\nIntel Xeon E5-2660 v4CPU\n• longer trainings were completed on 8x RTX\n2080 Ti with 11GB memory each, 128GB\nRAM and Intel Xeon E5-2660 v4CPU.\n• Few longest trainings were conducted on8×8\nTPUv3 units, each with 16GB memory.\nC.2 Software\nAll experiments were performed on Linux operat-\ning system using Trax library version 1.3.9 along\nwith all its dependencies from this particular re-\nlease date. Additionally, to run shorten factor\ndropout experiments we modified the Transformer-\nXL codebase in PyTorch.\nD Reproducibility\nTo ensure the reproducibility of this work\nand to support open science principles,\nwe made our code publicly available at\ngithub.com/google/trax. In this reposi-\ntory, we also provide Google Colab notebooks\nwhere the evaluation of our main Enwik8 and\nImageNet32/64 results can be reproduced.23\nD.1 Randomness\nSeeds in all experiments were chosen randomly,\nhowever each experiment contains history which\nallows retrieving all randomly set parameters for\nreproductions.\nFor each ablation described in the ablation study\nsection, we rerun the baseline 3 times to calculate\nstandard deviation. All other experiments are run\nonly once due to costs and since the variance we\nnoticed was minimal.\nD.2 Experiment representation\nEach experiment is represented by a configuration\nfile that unambiguously determines the whole setup\n– all hyperparameters and training details like spe-\ncific optimizers, data preprocessing functions, or\nbatch size per device.\n2https://github.com/google/trax/blob/master/trax/models/research/examples/hourglass_enwik8.ipynb\n3https://github.com/google/trax/blob/master/trax/models/research/examples/hourglass_downsampled_imagenet.ipynb\n1571"
}