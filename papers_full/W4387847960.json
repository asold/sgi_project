{
  "title": "Can we utilize Large Language Models (LLMs) to generate useful linguistic corpora? A case study of the word frequency effect in young German readers",
  "url": "https://openalex.org/W4387847960",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2183789230",
      "name": "Job Schepens",
      "affiliations": [
        "University of Cologne"
      ]
    },
    {
      "id": "https://openalex.org/A5118143166",
      "name": "Hanna Woloszyn",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2181708455",
      "name": "Nicole Marx",
      "affiliations": [
        "University of Cologne"
      ]
    },
    {
      "id": "https://openalex.org/A4374168",
      "name": "Benjamin Gagl",
      "affiliations": [
        "University of Cologne"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2111267834",
    "https://openalex.org/W2142635246",
    "https://openalex.org/W1967163642",
    "https://openalex.org/W4229933927",
    "https://openalex.org/W6846260263",
    "https://openalex.org/W2136525955",
    "https://openalex.org/W2773880249",
    "https://openalex.org/W2415155039",
    "https://openalex.org/W7074103774",
    "https://openalex.org/W4377997222",
    "https://openalex.org/W2979506538",
    "https://openalex.org/W2603721550",
    "https://openalex.org/W6674834830",
    "https://openalex.org/W4367000425",
    "https://openalex.org/W6683294271",
    "https://openalex.org/W2161793142",
    "https://openalex.org/W2135971384",
    "https://openalex.org/W4384932202",
    "https://openalex.org/W3169014749",
    "https://openalex.org/W6643426686",
    "https://openalex.org/W1796592579",
    "https://openalex.org/W1991329869",
    "https://openalex.org/W4255613872",
    "https://openalex.org/W2891164444",
    "https://openalex.org/W2108256767",
    "https://openalex.org/W1989326423",
    "https://openalex.org/W3204644074",
    "https://openalex.org/W4225647542",
    "https://openalex.org/W2155603721",
    "https://openalex.org/W2403707901",
    "https://openalex.org/W4323655724",
    "https://openalex.org/W2012575176",
    "https://openalex.org/W2067359214",
    "https://openalex.org/W4385849410",
    "https://openalex.org/W2966599707",
    "https://openalex.org/W4384261711",
    "https://openalex.org/W4288400169",
    "https://openalex.org/W1985396780",
    "https://openalex.org/W331019419",
    "https://openalex.org/W2037387434",
    "https://openalex.org/W3209721572",
    "https://openalex.org/W4205958079",
    "https://openalex.org/W4321276001",
    "https://openalex.org/W4312119976",
    "https://openalex.org/W6948936239",
    "https://openalex.org/W2048176942",
    "https://openalex.org/W4377148704",
    "https://openalex.org/W3202852341",
    "https://openalex.org/W2581898631",
    "https://openalex.org/W2345092949",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W1682431627",
    "https://openalex.org/W2082023824",
    "https://openalex.org/W2113772582",
    "https://openalex.org/W4287364505",
    "https://openalex.org/W2039429886",
    "https://openalex.org/W2132193240",
    "https://openalex.org/W4375870056",
    "https://openalex.org/W2006014379",
    "https://openalex.org/W2152051036",
    "https://openalex.org/W2115054880",
    "https://openalex.org/W2059080398",
    "https://openalex.org/W4306247398",
    "https://openalex.org/W2741029840",
    "https://openalex.org/W2157981187",
    "https://openalex.org/W4282945872",
    "https://openalex.org/W4327526040",
    "https://openalex.org/W2060893093",
    "https://openalex.org/W4375860407",
    "https://openalex.org/W2097221482",
    "https://openalex.org/W4382246105",
    "https://openalex.org/W2165567129",
    "https://openalex.org/W4318919287",
    "https://openalex.org/W2097076392",
    "https://openalex.org/W1997802248",
    "https://openalex.org/W4220761115",
    "https://openalex.org/W2104440612"
  ],
  "abstract": "Can a measure of word frequency (i.e., how often a word occurs) based on text generated by a large language model (LLM) model the well-studied word frequency effect in reading? To study this, we created a large corpus of text generated by an LLM (GPT 3.5) to measure word frequency. We prompted it to write stories in German directed to children using the titles of 500 popular books, so that the corpus would follow the structure of an existing corpus based on books written for children (childLex). The generated text had a lower lexical richness than the text in childLex. We then used reading performance data from German children (i.e., reaction times in a lexical decision task) to investigate the effect of LLM word frequency in visual word recognition. We found that LLM word frequency explained more variance in reaction times when compared to the word frequency measure based on the original children's books (Experiment 1) and also when compared to a word frequency measure based on adult-directed LLM text (Experiment 2). We conclude that LLM corpora are less lexically rich while approximating children's lexical processing well. We discuss the potential of this approach, considering the inevitable risks of using LLMs that lack openness, are highly complex, and need a large amount of resources.",
  "full_text": "Can Large Language Models generate useful linguistic\ncorpora? A case study of the word frequency effect in young\nGerman readers\nJob Schepens1, Hanna Woloszyn2, Nicole Marx3, and Benjamin Gagl2\n1Institute for Linguistics, University of Cologne\n2Self learning systems Lab, University of Cologne\n3Mercator Institute, University of Cologne\nAbstract\nLinguistic corpora are an essential resource in psycholinguistic research.\nHere, we generate new corpora using large language models (LLMs) and\ndetermine their usefulness for estimating the word frequency effect on read-\ning performance, focusing on German children. We prompted three different\nLLMs to create corpora of children’s stories using the titles of 500 books,\nmimicking an existing corpus of children’s books (childLex). In Experiment\n1, we found that word frequency correlated strongly between childLex and\nthe LLM corpora, despite a lower lexical richness of LLM text. Compared\nto childLex, we found that the estimated effect size of the LLM-based word\nfrequency effect was lower, but that it explained more variance in reading\nperformance (using reaction times for about 1000 words in a lexical decision\ntask). In Experiment 2, we found that prompting for children-directed text\nresults in word frequency that better fits to child compared to adult reading\ntimes, and also that increasing temperature can increase lexical richness.\nIn Experiment 3, we replicated Experiment 1 using two open-weight LLMs.\nAcross all 10 corpora (out of which 9 were LLM-based), we found that cor-\npora with lower lexical richness generally fit better to reaction times. We\ndiscuss the potential of this approach, considering the risks associated with\nutilizing highly complex large language models (LLMs).\nKeywords: Large language models, linguistic corpus, word frequency effect,\nlexical decision task\nIn recent years, Large Language Models (LLMs) have improved quickly, enabling\nnew ways of interaction between humans and computers (Bommasani et al., 2022) across\nJob Schepens https://orcid.org/0000-0003-1271-2526 Nicole Marx https://orcid.org/0000-0002-\n7027-0618 Benjamin Gagl https://orcid.org/0000-0002-2339-6293\nWe deeply thank Elen le Foil for their detailed comments and feedback, which greatly improved our\nwork.\nCorrespondence concerning this article should be addressed to Job Schepens, Institute for Linguistics,\nUniversity of Cologne Albertus-Magnus-Platz, 50923 Köln, Germany. E-mail: job.schepens@uni-koeln.de\nINSERT SHORTTITLE COMMAND IN PREAMBLE 2\ndifferent languages and contexts (Chang, Arnett, Tu, & Bergen, 2023; Lai et al., 2023),\ndespite their vastly non-human-like nature (Evanson, Lakretz, & King, 2023; Kasneci et al.,\n2023; Min et al., 2021; Singhal et al., 2023). LLMs are now being used in research contexts,\nincludingpsycholinguistics, wheretheyhavebeenappliedtogeneratepredictorsthatexplain\nvariance in reading behavior and brain activation related to word predictability (Boeve &\nBogaerts, 2024; Botch & Finn, 2024; Chandra, Witzig, & Laubrock, 2023; Heilbron, van\nHaren, Hagoort, & de Lange, 2023; Hofmann, Remus, Biemann, Radach, & Kuchinke,\n2022; Lopes Rego, Snell, & Meeter, 2024). This context seems promising, given that LLMs\nare trained on next-word predictability (Tay, Dehghani, Bahri, & Metzler, 2022). Various\nother psycholinguistics measures have been studied using LLMs, ranging from augmentation\n(Trott, 2024), to semantic (Caucheteux & King, 2022) and syntactic processing (Desbordes\net al., 2023; Fresen, Choenni, Heilbron, Zuidema, & de Heer Kloots, 2024). The goodness of\nfit of LLMs to human reading times seems closely connected to their numbers of parameters\n(Oh & Schuler, 2023) as well as word frequency in the training data (Oh & Schuler, 2025;\nOh, Yue, & Schuler, 2024). Here, instead of word frequency in training data, we study word\nfrequency in corpora of texts generated by LLMs.\nWord frequency plays a central role in word recognition and reading studies, as it\nhas a large effect on both adult and child reading performance (Brysbaert, Mandera, &\nKeuleers, 2018; Brysbaert, Stevens, Mandera, & Keuleers, 2016; Gregorova, Turini, Gagl,\n& Võ, 2023; Hawelka, Gagl, & Wimmer, 2010; Kliegl, Grabner, Rolfs, & Engbert, 2004;\nSchröter & Schroeder, 2017). The frequency of a word is typically measured by taking large\nlinguistic corpora and counting its occurrences (e.g., see Baayen, Piepenbrock, & Van Rijn,\n1993; Brysbaert et al., 2011; Heister et al., 2011; Schroeder, Würzner, Heister, Geyken,\n& Kliegl, 2015). With ever-expanding corpora, increasingly appropriate word frequency\nestimates are possible, which may describe the variance in word recognition tasks better\nthan other existing measures (Brysbaert et al., 2011).In this paper, we investigate\nwhether an LLM corpus can be used to extract a word frequency measure which\nmay better describe the variance in word recognition performance than existing\ncorpora. The goal of these evaluations is to test if the integration of LLM corpora into\npsycholinguistic research is reasonable, potentially identifying a way of using LLM corpora\nfor estimating specific word frequency measures for selected groups (in our case, German\nschool children).\nWe focus on beginning German readers for various reasons. First, we anticipate\nneeding a smaller-sized corpus for this group. Further, it is possible to draw on two openly\navailabledatasetsforevaluation. (i)childLex(Schroederetal.,2015)providesahigh-quality\ncorpus of 500 books written for children, allowing a direct comparison of LLM corpora with\nan existing book-based corpus. (ii) DeveL (Schröter & Schroeder, 2017) provides reading\nperformance data for a large set of words based on reaction times in a lexical decision\ntask. We compare book-based and LLM-based word frequency to investigate which measure\nexplains more variance in reaction times in a lexical decision task, which is often used to\nstudy reading performance.\nThe effect of word frequency on reading performance is considered substantial and\nrobust (Brysbaert et al., 2018, 2016) across groups (e.g., Hawelka et al., 2010) and modal-\nities (e.g., Gregorova et al., 2023). The word frequency effect describes that words that\noften occur (i.e., high-frequency words) are recognized faster and more accurately com-\nINSERT SHORTTITLE COMMAND IN PREAMBLE 3\npared to less common words (i.e., low-frequency words; Adelman, Brown, & Quesada,\n2006; Baayen, 2010; Brysbaert et al., 2016; Gregorova et al., 2023; Hallin & Reuterskiöld,\n2018; Lieven, 2010; S. A. McDonald & Shillcock, 2001; Stokes, 2010). To accurately esti-\nmate word frequency, the choice of the underlying corpus is highly relevant. Previous studies\nhave collected large numbers of books and newspapers and combined them into corpora for\nmeasuring word frequency (e.g., Baayen et al., 1993; Heister et al., 2011). The use and\nvalidity of frequency measures depend on the source of the text. For example, Brysbaert\nand colleagues (Brysbaert et al., 2011, 2018) have shown that word frequency statistics\nbased on television and movie subtitles explain more of the variance in word processing\ndifficulty performance measures such as lexical decision response times than book-based\ncorpora (see also Chilson, Schmalz, & Sineva, 2024). The critical comparison here is based\non model comparisons utilizing reading performance data; i.e., a regression model involving\nsubtitle-based word frequency measures had a higher model fit than a book-based word\nfrequency measure (see, e.g., Brysbaert et al., 2011). Thus, the corpus used to derive word\nfrequency significantly influences the amount of variance that word frequency can explain\n(Ferrand et al., 2010; Keuleers, Brysbaert, & New, 2010; Van Heuven, Mandera, Keuleers,\n& Brysbaert, 2014), even when carefully controlling for other essential word characteristics\nsuch as orthographic similarity to other words, age in which a word is typically acquired,\nand word length (Graf, Nagler, & Jacobs, 2005).\nOnly a few linguistic corpora exist that can be used to measure word frequency\nrelated to children. These are typically based on children’s books or subtitles for children’s\nmovies (Korochkina, Marelli, Brysbaert, & Rastle, 2024; Schroeder et al., 2015; Tellings,\nHulsbosch, Vermeer, & Van den Bosch, 2014; Van Heuven et al., 2014). Constructing a\ntext corpus specifically designed for children is challenging for several reasons. The number\nof materials developed explicitly for children is necessarily smaller than that for adults.\nAccess to a wide range of children’s literature and other resources can be limited due to,\nfor example, questionable validity or even availability of the listed or estimated target age\nrange. Estimatingthetargetagecanbeproblematic, asnotallmaterialsexplicitlyindicatea\ntargetagegroup. Finally, thereareonlyafewavailableresourcesfortextwrittenbychildren\nthemselves (see, e.g., Laarmann-Quante et al., 2019). Together, these factors result in a\nlimited and possibly biased set of language materials, which is expected to have implications\nfor studies that use word frequency measures.\nInsummary, previousstudieshaveshownthattexttypeisimportantforstudyingthe\neffects of word frequency on reading performance; however, corpora involving text written\nfor children are scarce. LLMs can potentially help to better understand word frequency\neffects. However, little is known about the usefulness of LLM text in this regard. Our\nprimary objective is thus to develop a measure of word frequency for children derived from\nLLM text and evaluate its potential usefulness for estimating word frequency effects.\nOur procedure is as follows. First, we generate a corpus of texts that are based on\nthe titles of the books contained in childLex. Then, we compare word frequency between\nboth corpora. Finally, we utilize a large reading performance dataset (DeveL) that in-\ncludes data from children, adolescents, and adults (Schröter & Schroeder, 2017) to evaluate\nwhether LLM word frequency better fits reaction times than childLex word frequency. This\nprocedure is similar to previously performed evaluations of measures of word frequency\n(e.g., Brysbaert et al., 2011, 2018). Using reaction times from a lexical decision task as\nINSERT SHORTTITLE COMMAND IN PREAMBLE 4\na measure of reading performance is a common choice for studies on word recognition in\nchildren (Davies, Arnell, Birchenough, Grimmond, & Houlson, 2017; Monster et al., 2022;\nvan den Boer, de Jong, & Haentjens-van Meeteren, 2012).\nStrengths and Weaknesses of using LLMs for Psycholinguistic Research\nLLMs are computational models with a considerable amount of trainable parameters\n-inthecaseofChatGPT-3.5, 175billion(Brownetal.,2020). Thismakestheinnerworkings\nopaque, but potent for learning language-related tasks such as next-word or next-sentence\nprediction with high accuracy (e.g., Devlin, Chang, Lee, & Toutanova, 2019). LLMs make\nuse of architectural principles such as transformers and attention heads (Vaswani et al.,\n2017); see also (Hussain, Binz, Mata, & Wulff, 2024, for an introduction). To be able to\nproduce comprehensible output text, LLMs need to be trained on a vast set of data (Bender,\nGebru, McMillan-Major, & Shmitchell, 2021).\nSeveral weaknesses of LLMs have been pointed out. Little is known about the min-\nimal training data requirements (Hosseini et al., 2022). LLM training data is not develop-\nmentally realistic when compared to human input (Evanson et al., 2023; Feng, Goodman, &\nFrank, 2024; Warstadt et al., 2023). Also, the required amounts of computing resources and\ntraining data can make it technically challenging to apply interventions or change aspects\nof the architecture or training procedure, so they are typically fine-tuned using reinforce-\nment learning with human feedback (RLHF) to generate output that aligns with desired\noutcomes (see Ouyang et al., 2022, for how to remove troubling model outputs). Given the\nbias in the training data that is typically used, the output of LLMs is biased in favor of\nWestern culture (Atari, Xue, Park, Blasi, & Henrich, 2023). Furthermore, for many LLMs,\nimportant information is unavailable, as they lack transparency (Frank, 2023b; Liesenfeld,\nLopez, & Dingemanse, 2023). The lack of transparency and the considerable effort required\nfor training these models make reproducing the models a challenging task. This also ques-\ntions what kind of LLMs should be and should not be used in research or as research objects\n(Bender et al., 2021; Liesenfeld et al., 2023).\nDespite these caveats, LLMs seem to have much potential for studying word fre-\nquency (Oh et al., 2024) or other psycholinguistic measures (Martínez, Conde, Reviriego,\n& Brysbaert, 2024). LLMs have likely seen more words (and possibly languages) than\nany human. Extensive training data in combination with next-word-prediction may be a\nstrength for estimating word frequency compared to traditional measures, since estimating\nthe probability of words is part of how LLMs are optimized. Finally, the output of LLMs\nis generally flexible. It depends on the model prompt and a set of parameters (for the tem-\nperature parameter: higher values lead to greater lexical diversity in the generated text,\nsimilar to exploration in humans Momennejad et al., 2023).\nWhat uses can LLMs have in research on language processing? Here, we assume that\nLLMs can be useful as tools, rather than a theoretical approach to language (e.g., as in Binz\netal.,2024). Sofar, LLMshavebeenusedtoexposenecessaryconditionsandqualitiesofthe\nlearning environments for human language acquisition (Trott, Jones, Chang, Michaelov, &\nBergen, 2023; Warstadt et al., 2023), predict human brain responses to language (Tuckute\net al., 2024), estimate word predictability from sentence context (Chandra et al., 2023;\nHeilbron, van Haren, Hagoort, & de Lange, 2021; Hofmann et al., 2022), and explain N400\neffects (Michaelov, Bardolph, Van Petten, Bergen, & Coulson, 2024). Such efforts emphasize\nINSERT SHORTTITLE COMMAND IN PREAMBLE 5\nthe importance of human benchmarks, which are crucial for evaluating how LLM capacities\nalign with human capabilities. LLMs also deviate from human behavior in interesting ways\n(Mahowald et al., 2024). For example, LLMs have trouble with language comprehension\n(Dentella, Günther, Murphy, Marcus, & Leivada, 2024), and LLM optimization is very\ndifferent from the pressures that shape human performance (McCoy, Yao, Friedman, Hardy,\n& Griffiths, 2023). Regarding child reading research, their large amount of training data\nand training procedures enables LLMs to represent various linguistic contexts, which may\nreflect exposure to words in ways that are relevant to word frequency measurement. Here,\nwe explore the latter aspect.\nPresent study\nIn three studies, we investigate whether and how LLMs can generate corpora for\nmeasuring word frequency that are suitable for approximating the lexicon of young German\nreaders. ChildLex (Schroeder et al., 2015), our reference corpus, is based on children’s\nbooks. In Experiment 1, we generate a corpus modeled after childLex (Schroeder et al.,\n2015). To implement this, we prompt the model for child-directed stories based on the book\ntitles included in the original corpus. In Experiment 2, we generate four additional corpora,\nsystematically varying the target audience of the stories using different prompts (i.e., child-\ndirected vs. adult-directed text) and also varying the model parameter responsible for\nlexical variability in the text output (high vs. low temperature). In Experiment 3, we again\ngenerate four new corpora using two open-weight LLMs, as well as manipulating text length\n(long vs. short).\nExperiment 1\nAfter generating a corpus of LLM-based text, we evaluate it by comparing resulting\nword frequency measures to childLex (Schroeder et al., 2015). In the second step, we assess\nthe word frequency measures against reading performance (Schröter & Schroeder, 2017) in\nyounger and older adults by testing which measure best explains the behavioral data.\nMethod\nModel choice\nDespite reservations about the openness of the model (see, e.g., Hussain et al., 2024;\nLiesenfeld et al., 2023), we chose to use \"GPT-3.5-turbo\" for Experiments 1 and 2, which\npointed to Snapshot Version 0301 (the first version made available). This is an efficient and\noptimized version of GPT-3.5, with a likely smaller number of parameters. This off-the-\nshelf model was usable without any initial training or setup and was stable, state-of-the-art,\nand affordable. In May 2023, the model showed good performance was easy to handle via\nan API (i.e., easy to use via Python; find the script here: osf.io, was stable (not the case\nwith the GPT-4 version at the time), and was cost-effective (the pricing at the time of\ngenerations was $0.002 / 1K tokens, which was more affordable than $0.06 / 1K tokens for\nGPT-4). In this context, a token can be a word, part of a word, or punctuation mark. The\ntokenizer splits the training data into tokens by iteratively combining the most frequently\nadjacent pairs of tokens until it reaches a pre-specified vocabulary size. The model had a\nINSERT SHORTTITLE COMMAND IN PREAMBLE 6\ntoken limit of 4,096 tokens, equivalent to approximately 3,000 words. This limit included\nboth the length of the input prompt and the generated output. The texts that we generated\nwere substantially shorter than texts typically used to estimate word frequency, e.g., full\nbooks or films. To account for this length difference, we used the same prompt repeatedly to\ngenerate different texts – an interesting option since LLMs generate different texts for each\nprompt, even when the prompt remains constant. This strategy also ensured a comparable\nlength of text generated for each book title, which could otherwise have biased the results.\nLLM prompt engineering\nTraditionally, word frequency is measured using books. ChildLex uses the texts\nof 500 popular books written for children in several different age ranges (for details, see\n(Schroeder et al., 2015). Titles and age ranges include such works as \"Karius und Baktus\"\nfor children aged 4-6, \"King-Kong, das Schulschwein\" (King-Kong, the School Pig), for\nchildren aged 8-10, and \"Der Fluch des Goldes\" (The Golden Curse), for children aged 14-\n17. We decided to use the titles of these books to prompt the LLM in the direction of the\nthemes of these books. Note that we are unaware if the LLM we used had these books as\npart of its large set of training data, but the likelihood that at least some of them were part\nof it is high, given the size of the LLM (Y. Liu, Cao, Liu, Ding, & Jin, 2024).\nUsing these book titles, our prompts had the following structure:4000 Wörter zu\nBuchtitel auf Deutsch für Kinder(4000 Words onBooktitle in German for Children). In\ncase the age range was known, it was added (im Alter Altersangabe; at the age ofage\nrange), with Booktitle and Altersangabe changing for every specific book title. We\ndeliberately kept our prompts simple to minimize prompt engineering; future studies could\n(and should) improve the prompts by, for example, requesting storytelling and narrative\nelements, providing more context, or providing information about our goal (i.e., estimating\nword frequency).\nSince we kept the temperature set at .5, the text output was balanced between\ndeterministic and random. It turned out that this prompt results on average in 628 words\nper prompt. For reasons which are unclear, the LLM produces substantially shorter stories\nthan what is asked for in the prompt. It is possible to engineer prompts that result in\nlonger texts, for example, by prompting to divide the story into chapters. Subjectively, the\nresulting texts often seem to relate to the content of these books. For example,\"Es war\nein sonniger Tag im Frühling und Opa Franz war im Garten beschäftigt. Er war gerade\ndabei, die Blumenbeete zu jäten, als er plötzlich ein seltsames Geräusch hörte. Es klang\nwie ein Schnauben und ein Fauchen zugleich. Verwundert drehte er sich um und sah etwas,\ndas er zuvor noch nie gesehen hatte. Ein kleiner Drache saß auf dem Zaun und betrachtete\nihn neugierig.\", which translates to\"It was a sunny day in spring and Grandpa Franz was\nbusy in the garden. He was weeding the flowerbeds when he suddenly heard a strange noise.\nIt sounded like a snort and a hiss at the same time. Puzzled, he turned around and saw\nsomething he had never seen before. A little dragon was sitting on the fence, watching him\ncuriously.\" (See repository with all texts here: osf.io)\nCorpus design\nWe decided to generate texts 20 times for each book title to increase representative-\nness and saturation (see Schnell, 2021)) and increase the total amount of generated text\nINSERT SHORTTITLE COMMAND IN PREAMBLE 7\nper book. To implement the repeated text generation, we set the n-parameter (number of\nprompts per run) to 4 and then ran the prompt five times for all 500 books. We stored the\nresult of every prompt in a separate text file (filename: \"Story_\" + N + \".txt\", where N\nrepresents the number of books on the list). This way, every file included four generated\ntexts based on the same book. Total cost was about6, 276, 276 × .002 × 1.3 × .001 =US$\n16. We stopped text generation after this initially planned procedure was finished.\nWord frequency estimation\nWe used R for most of the data analysis and Python for text generation. We used\nthe text mining package in R (tm; Feinerer, Hornik, & Meyer, 2008) for measuring word\nfrequency, using the default tokenizer, and removing punctuation and numbers using its\n\"control\" options. For lemmatization, we used UDPipe (Straka & Straková, 2017) with the\ndefault German treebank from the Universal Dependencies project (german-gsd; (R. Mc-\nDonald et al., 2013). Similar to Table 2 in Schroeder et al. (Schroeder et al., 2015), we\npresent an overview of the resulting corpus (see Table 1). Note that childLex used a different\nlinguistic pipeline for tokenization and lemmatization (i.e., based on Geyken & Hanneforth,\n2006; Jurish & Würzner, 2013), as we could not reproduce the original pipeline completely.\nOther than that, we tried to mimic the pipeline as well as possible.\nAs in childLex, we kept the original capitalization (sentences, nouns, etc.), since\nnouns in German are always capitalized. This makes our corpus more comparable and keeps\nas much structure in the corpus as possible. Note that this results in tokens such asEssen\nand essen (food and to eat in English) in the middle of sentences to be correctly counted\nas different types, but also tokens such asWahrscheinlich and wahrscheinlich (probably in\nEnglish) due to capitalization at the beginning of sentences. We used a log-transformed and\nnormalized word frequency measure (log\n((1+frequency)×106\ncorpus_size\n)\n), see Appendix Section “Word\nfrequency transformation” for more details. For similar procedures and discussion, see\nHeister et al. (2011); Van Heuven et al. (2014).\nTable 1\nSize and descriptive statistics of the LLM corpus compared to childLex (Schroeder et al.,\n2015).\nMeasure childLex LLM corpus\nn Books 500 500\nTokens 9,850,786 6,252,808\nTypes 182,454 46,409\nLemmas 117,952 34,519\n% Hapax tokens 0.90 0.25\n% Hapax types 48.74 33.03\n% Hapax lemmas 48.30 33.09\n% Tokens> 4 97.89 99.57\n% Types> 4 26.53 41.81\n% Lemmas> 4 27.91 41.24\nINSERT SHORTTITLE COMMAND IN PREAMBLE 8\nSources for alternative word frequency measures\nWe focus on the comparison with childLex (Schroeder et al., 2015). We also com-\npare LLM word frequencies to word frequency measures from a number of other lexical\ndatabases. These include: Litkey (Laarmann-Quante et al., 2019), DWDS (Heister et al.,\n2011), SUBTLEX (Brysbaert et al., 2011), and Google Books (Brysbaert et al., 2016).\nLitkey is a considerably smaller corpus, but see the Appendix for a comparison with Litkey\n(Figure A4).\nReading performance data\nWe used the log-transformed lexical decision response times (RTs) on 1152 words\nas reading performance measures from DeveL (Schröter & Schroeder, 2017). The RTs\nrepresent the mean RT per word (N = 1152) and age group (Grade 1, 2, 3, 4, 6, younger\nand older adults). As is usually done, we applied a log transformation, which resulted in\na more normal-like distribution of RTs and also improved model fit. Note that non-words\nwere excluded, as the primary focus of the current analysis is the investigation of word\nfrequency. Words in DeveL were selected in such a way that the low-frequency words (e.g.,\n< 10 per million) accounted for only 25% of all words (in childLex, low-frequency words\naccount for 95% of the words).\nResults\nWord frequency distributions and lexical richness\nWe compared several statistics between the LLM corpus and childLex (Schroeder et\nal., 2015). The generation procedure yielded an average of 625 words per generated text,\ntotaling 6,276,276 words. Table 1 shows that this is about 66% of the size of childLex.\nTo compare both corpora, we needed to normalize for sample size. After dividing by\nthe total number of unique types and lemmas, we found a relatively low portion of unique\ntypes and lemmas in the LLM corpus (see % hapax types and lemmas in Table 1). We also\nevaluated the type-token pattern for different hypothetical sample sizes, see the topmost\ngreen dashed growth curve in Figure 1, which is similar to Figure 1 from (Schroeder et al.,\n2015). The growth curve for childLex is more than twice as high, showing that the LLM\nproduces lexically less rich language, i.e., with fewer unique types. In addition to observed\nnumbers of types for subsets of the corpus, Figure 1 also shows predictions based on Large\nNumbers of Rare Events (LNRE) models (Baayen, 2001; Evert, 2004). These inter- and\nextrapolations show the expected change in lexical richness when corpus size increases. For\nboth smaller and larger size corpora, lexical richness stays well below that of childLex. The\npercentage of hapax legomena (i.e., tokens occurring only once in the entire corpus) and\nunique types is much higher in childLex than in the LLM corpus. Words that recur at least\nfive times (i.e.,> 4; see Table 1) account for a larger portion in the LLM corpus. These\nfindings provide a further indication that the LLM corpus is lexically less rich, which is in\nline with our subjective impression after reading the generated text.\nMore generally, we can also investigate this pattern by inspecting the balance of high\nand low-frequency words as indicated by Zipf’s law (i.e., word frequency is proportional\nto rank). This proportion in natural language corpora is never completely constant, so\nINSERT SHORTTITLE COMMAND IN PREAMBLE 9\n0.0e+00 4.0e+06 8.0e+06 1.2e+07\n0 20000 40000\n \nSample size\nNumber of types (V)\nFigure 1\nThe type-token growth curves based on the LLM corpora show the increase in lexical richness\nas the sample size gets larger. The four curves show four different lexical richness measures\n(y-axis) on interpolated (solid grey), extrapolated (solid red), and observed (dashed green)\nsample size (x-axis), as based on a finite Zipf-Mandelbrot Large Numbers of Rare Events\nModel (LNRE model, see Evert, 2004). The four lexical richness measures are from top to\nbottom: total numbers of types, numbers of types that occur at most 3, 2, and 1 times (i.e.,\ntris legomenaV3, dis legomenaV2, and hapax legomenaV1).\ncomparing the pattern across corpora can be interesting (see, e.g., Baayen, 2001, 2008;\nPiantadosi, 2014). Figure 2 shows that Zipf’s law in the LLM corpus is generally steeper\ncompared to childLex. Previously, similar comparisons of adult-directed corpora showed\nthe same pattern (i.e., SUBTLEX and Google Book corpus, Brysbaert et al., 2016). For\nthe LLM corpus, the slope is steeper compared to childLex, indicating that the LLM corpus\nincludes more high-frequency words and fewer low-frequency words. Again, this finding\nindicates that the LLM-based text is lexically less rich.\nDespite differences in lexical richness, we find a correlation between word frequency\nmeasures from childLex and the LLM corpus (r=.88 for frequency per million and r=.69\nfor log frequency per million, see Figure 3). The cone shape of the scatter plot (see Figure\n3) indicates that lower frequency measures are noisier (more variable) than higher word\nfrequency measures, which makes sense, given the lower numbers of observations for low-\nfrequency words. The relatively symmetric shape indicates that this noise is similarly\ndistributed between both corpora. The correlation remains .88 when we include only words\nthat occur in both corpora. The correlation based on log frequency also stays the same.\nThe high correlation and the scatter plot illustrate the high similarity of the two measures.\nComparing word frequency in this way also allows us to look at the words that differ the\nmost in both directions. In the LLM corpus, words that are more frequent sometimes result\nfrom spillover effects from the most likely predominant English training data. For example,\nINSERT SHORTTITLE COMMAND IN PREAMBLE 10\n0.1\n1\n5\n10\n50\n100\n500\n1000\n5000\n10000\n50000\n1 5 10 50 100 500 1000 50k 10k 50k\nRank\nFrequency\nCorpus name\nLLM\nChildLEX\nFigure 2\nZipf’s law plot shows frequency count on the y-axis and frequency rank on the x-axis with\nlinear regression lines (dashed lines) for both the LLM corpus and childLex. The slopes are\nfitted to words with a frequency between 10 and 10,000. The LLM slope is more negative, in-\ndicating a higher occurrence of highly frequent words and a lower occurrence of low-frequency\nwords.\nthe German word \"namens\" is used more often in the LLM corpus than in childLex (1414\nvs. 31 per million). This could directly have \"spilled over\" from the typical phrase to\nstart a story in English: \"There was an X called Y\" even though \"namens\" is not typically\nused in this context in German. This result is similar to the observed losses in lexical and\nmorphological richness in automatic machine translation (Vanmassenhove, Shterionov, &\nGwilliam, 2021). In contrast, word frequencies that stand out in childLex are typically\nassociated with narrative storytelling. While this finding is unsurprising, childLex also\ncontains some very common words that we would have expected to find more frequently in\nthe LLM corpus. There is no clear explanation for these patterns, but some seem to indicate\nthat childLex contains more colloquial style words (e.g., “offenbar”). Table 2 shows the most\ncommon words from both corpora that do not appear at all in the other corpus. In this\ntable, the LLM column contains only names, which is a result of the way some common\nfirst names were removed from childLex but were kept in the LLM corpus. Unfortunately,\nwe were not able to reconstruct the procedure used in childLex for removing given names.\nINSERT SHORTTITLE COMMAND IN PREAMBLE 11\nTable A1 contains a more extensive analysis of such differences.\nHand\nmir\nmichgerade\nab\nFreunde\nTiere\nhelfen\naufgeregt\nWald\n0.0\n2.5\n5.0\n7.5\n10.0\n0.0 2.5 5.0 7.5 10.0\nChildLex−based (6−12 y/o children) type frequency (ppm)\nLLM−based type frequency (ppm)\nCount\n1 100 10000\nFigure 3\nCorrelation between LLM type frequency (y-axis) and childLex type frequency (x-axis; dark\ngray line). The labels show the top five differences on both sides (x-y and y-x). The color\ngradient of the dots represents the number of data points each dot represents.\nINSERT SHORTTITLE COMMAND IN PREAMBLE 12\nTable 2\nThe top frequent words that occur in only one of the two corpora, for all word lengths, and\nwords with more than 10 characters. F represents the normalized log frequency. Some very\ncommon childLex words like \"offenbar\" (evidently), \"rasch\" (quickly), \"Hoffentlich\"\n(Hopefully), and \"guckte\" (looked) do not occur in the LLM corpus at all. Instead of\n\"guckte\" or \"gucken\", words like \"angeguckt\" and \"abgucken\" do appear in the LLM corpus.\nAlso, common first names have been removed from childLex, but not from the LLM corpus.\nchildLex F childLex >10 F LLM F LLM >10 F\ndaß 6.1 Hoffentlich 4.4 Max 8.4 nahegelegenen 4.3\n1 5.1 Brombeerkralle 3.8 Mia 7.1 Schulvampire 4.2\noffenbar 5.0 Hosentasche 3.7 Tim 7.0 Tantenschreck 3.9\nrasch 4.9 Augenbrauen 3.7 Lisa 7.0 SkaterBande 3.8\nEigentlich 4.7 Zeigefinger 3.5 Lena 6.7 ParkSheriffs 3.8\nehe 4.5 SternenClan 3.5 Anna 6.5 Lesefähigkeiten 3.8\nGleich 4.4 Olchi-Kinder 3.5 Emma 6.5 Schafgäääng 3.7\nHoffentlich 4.4 eingefallen 3.4 Tom 6.4 SchmuddelHund 3.7\nglaub 4.4 kopfschüttelnd 3.1 Müller 6.2 Inselschüler 3.7\nguckte 4.3 unwillkürlich 3.1 Lina 6.1 verwirklicht 3.6\nEvaluating LLM word frequency using reading performance\nWe compared a linear regression model that included LLM word frequency measures\nbased on the complete LLM corpus to linear regression models that included alternative\nword frequency measures. These other measures included childLex and two adult-directed\ncorpora, a book-based corpus (DWDS Heister et al., 2011), and a subtitle-based corpus\n(SUBTLEX Brysbaert et al., 2011). We controlled for a number of other factors that\naffect reading performance, but that are not of interest to the current study: OLD20 (e.g.,\nHawelka, Schuster, Gagl, & Hutzler, 2013; Yarkoni, Balota, & Yap, 2008), age of acquisition\n(e.g., Weekes, Castles, & Davies, 2006), word length (e.g., Gagl, Hawelka, & Wimmer,\n2015; Huestegge, Radach, Corbic, & Huestegge, 2009; Marinus & de Jong, 2010; Zoccolotti\net al., 2005), as well as phoneme count, and uni-, bi- and trigram frequency. Note that\nfor the final analysis, we removed the phoneme count and uni-, bi-, and trigram frequency\nparameters due to high Variance Inflation Factors (Fox & Monette, 1992). We removed all\npredictors with a Variance Inflation Factor below 5, which can be assumed to indicate low\nco-linearity (see Gregorova et al., 2023, for a similar procedure). After calculating the effect\nsizes from the linear model, we estimated the model fit based on the Akaike Information\nCriterion (AIC, Akaike, 1974). To estimate the AIC, we compared the model to a baseline\nmodel without a word frequency measure. Higher AIC differences indicate increased model\nfit.\nWe found that the AIC difference was largest for young readers (Grade 1-4; see\nFigure 4), indicating that LLM word frequency describes the word frequency effect in young\nreaders best. For Grade 6, we found that SUBTLEX word frequency, which relied on\nsubtitles from films and TV shows, had the highest model fit increase. Finally, book and\nINSERT SHORTTITLE COMMAND IN PREAMBLE 13\nnewspaper-based word frequency from the DWDS corpus showed the highest model fit for\nyounger and older adults, see Figure 4A. Table A2 provides a description of model estimates,\neffect sizes, and the R-squared metric. When the word frequency increases from 1 to 1000,\npredicted RT decreases by about 250 ms. Appendix Section “Robustness analysis” shows a\nrobustness analysis against sample size for the correlation with childLex word frequency as\nwell as for the model fit on reading performance.\n0\n100\n200\n300\nG. 1 G. 2 G. 3 G. 4 G. 6 Y .\nAdults\nO.\nAdults\nModel fit increase [AIC relative to H0]\nMeasure\nChildLex\nDWDS\nSUBTLEX\nLLM\nFigure 4\nEvaluation of the word frequency effect on reading performance (RTs) for beginning read-\ners (Grades 1, 2, 3, 4, and 6), younger and older adults. AIC difference is based on an\nanalysis comparing the word frequency effect across LLM, childLex, DWDS, and SUBTLEX\nword frequency measures for all age groups. Models with the highest fit for each group are\nhighlighted (i.e., the largest AIC difference). Note that G. stands for grade, Y. for younger\nadults, and O. for older adults. The white dashed line near the x-axis indicates the threshold\nfor a significant model fit increase. Note that the age groups represent different data sets\nand that the bars are therefore only comparable within each age group.\nThe finding that LLM word frequency describes the word frequency effect in children\nbest is accompanied by a reduction in effect size. When comparing the effect sizes of the\nword frequency effects between LLM and childLex word frequency, we found that for LLM\nword frequency, effect size tended to be smaller in Grades 2-4 (see Figure 5A). Inspecting\nthe scatter plots of Grade 2 readers (as shown in Figure 5B and C and Table 3; even when\nOLD20 is removed) shows that the reduction in effect size is the combined result of the\nlow-frequency measures and the words not included in the LLM corpus but do exist in\nchildLex, indicating that these words play a crucial role for the adequate estimation of the\neffect size of the frequency effect in young readers.\nINSERT SHORTTITLE COMMAND IN PREAMBLE 14\nTable 3\nComparison of the linear regression model results for Grade 2 readers using either the\nLLM-based or the childLex-based frequency measurement. Two models included the\nOrthographic Levenshtein Distance (OLD20) measure, which was based on childLex, and\ntwo models did not.\nGrade 2 log. transformed reaction times\nLLM + OLD childLex + OLD LLM childLex\nlog. frequency LLM −0.017∗∗∗ −0.018∗∗∗\n(0.002) (0.002)\nlog. frequency childLex −0.025∗∗∗ −0.025∗∗∗\n(0.003) (0.003)\nIntercept 6.977 ∗∗∗ 7.045∗∗∗ 6.983∗∗∗ 7.052∗∗∗\n(0.018) (0.024) (0.018) (0.023)\nOLD20 0.014 ∗ 0.011\n(0.008) (0.008)\nAge of Acquisition 0.047 ∗∗∗ 0.045∗∗∗ 0.048∗∗∗ 0.045∗∗∗\n(0.003) (0.003) (0.003) (0.003)\nLetter length 0.062 ∗∗∗ 0.060∗∗∗ 0.064∗∗∗ 0.062∗∗∗\n(0.003) (0.003) (0.002) (0.002)\nObservations 1,152 1,152 1,152 1,152\nR2 0.678 0.672 0.678 0.672\nAIC -1744 -1722 -1743 -1723\nNote: ∗p<0.1; ∗∗p<0.05; ∗∗∗p<0.01.\nINSERT SHORTTITLE COMMAND IN PREAMBLE 15\n−0.03\n−0.02\n−0.01\n0.00\n1 2 3 4\nGrade\nFrequency effect\nsize [log. ms]\nFrequency\nchildLex\nLLM\nA\n7.00\n7.25\n7.50\n7.75\n8.00\n8.25\n−3 0 3 6\nLLM−based\nlog. frequency\nResponse time\nGrade 2 [log. ms]\nB\n7.00\n7.25\n7.50\n7.75\n8.00\n8.25\n−3 0 3 6\nChildLEX−based\nlog. frequency\nResponse time\nGrade 2 [log. ms]\nC\nFigure 5\nEvaluation of the word frequency effect size on reading performance (RTs) of beginning\nreaders (Grades 1, 2, 3, and 4). (A) Effect sizes with 95% confidence intervals for log-\ntransformed RT for the young readers of grades 1-4 for both LLM and childLex word fre-\nquency. (B) Scatter plot including linear regression line showing the word frequency effect\non RTs for grade 2 readers based on the LLM and (C) childLex corpora.\nSummary: Experiment 1\nLLM word frequency explained more variance in lexical decision response times com-\npared to childLex word frequency, which is based on children’s books. We generated the\ntext corpus based on 500 children’s book titles. In each prompt, we asked the model to\ncompose a text for children in age-specific language, using the titles of the books contained\nin childLex (Schroeder et al., 2015). There were two main findings. (i) Word frequency cor-\nrelated strongly between corpora, despite a lower lexical richness in the LLM corpus (based\non the number of distinct words and words used only once). Frequent words comprised a\nmore significant part of the LLM corpus compared to childLex. (ii) When evaluating re-\nsponse times, we found that LLM word frequency describes the performance of child readers\nbetter than childLex word frequency, indicating a lower effect size than previously assumed.\nIn summary, this study demonstrates how an LLM corpus can represent word frequency\nand also reveals substantial differences compared to traditional corpora.\nExperiment 2\nExperiment 2 investigates two of the findings from Experiment 1 in more detail.\nFirst, we investigate how increasing the temperature parameter influences the lexical rich-\nnessofthecorpus. Second, weinvestigatehowpromptingforchild-directedvsadult-directed\ntext influences model fit. To investigate the combination of these two factors, we generated\nfour additional corpora: two corpora generated with child-directed and two generated with\nadult-directed prompts, of which one of each had the Experiment 1 temperature value and\nthe other had an increased temperature value (i.e., 0.5 and 0.9). Together, these four cor-\npora included: (i) Child-Directed & Low Temperature (ChLT). (ii) Child-Directed & High\nTemperature (ChHT). (iii)Adult-directed & Low Temperature (AdLT, as well as (iv)Adult-\ndirected & High Temperature (AdHT). Since ChLT used the same target audience and the\nINSERT SHORTTITLE COMMAND IN PREAMBLE 16\nsame temperature, it is an attempt to reproduce the corpus from Experiment 1.\nLike in Experiment 1, we compare the new LLM corpora to childLex (Schroeder et\nal., 2015) and fit the resulting measures to child reading performance. We also compare\nthe fit for LLM word frequency with adult books-based word frequency (DWDS Heister\net al., 2011) to determine which measure fits best to adult reading performance (DeveL\ncontains response times for both children and adults). We expected higher model fits on\nchild reading performance for child-directed word frequency compared to adult-directed\nword frequency. This would demonstrate that the LLM is indeed capable of generating text\ntailored to children. Likewise, we expected higher model fits on adult reading performance\nfor adult-directed word frequency. For temperature, we expected that a higher temperature\nwould increase lexical richness and, therefore, result in a corpus that is more similar to\nchildLex. Given that in Experiment 1, the lexically richer corpus (childLex) resulted in\nlower model fits in reading performance, it can be expected that higher lexical richness\nwould again result in a lower model fit to reading performance.\nWe decided to look at grades 1–4 together, as we assumed that reading across these\ngrades is still in a comparable stage of development. Excluding adolescents follows devel-\nopmental evidence that lexical processing shifts from reliance on frequency-based decoding\n(Grades 1–4, Age 6–9) to more automated recognition by Grade 6 (Meixner, Nixon, &\nLaubrock, 2022). Experiment 1 also showed differences between grade 6 and the younger\nchildren as well as the adult readers.\nComparedtoExperiment1, weincreasedthesizeofbothchild-directedLLMcorpora\nto∼ 23 million words (going from∼ 6 million in Experiment 1) to make sure that corpus size\ndid not affect our findings. We expected this to increase the number of overlapping words\nwith childLex and to increase the reliability of the low-frequency measures (Gernsbacher,\n1984). This should also lead to more precise measures of lexical richness, which may be\nmore important for a high-temperature setting.\nMethods\nWe used the same settings (“max_tokens” set to 4000) and prompt as in Experiment\n1, except for the temperature (max. value was 2.0). Data was generated in June 2024 using\nGPT-3.5-turbo, which was now pointing to Snapshot Version 1106. In initial explorations,\na temperature above 1.0 resulted in frequent hallucinations, such as changing the names of\nthe main characters in the story (e.g., Ferdinand instead of Fred). A higher temperature\nalso led to more incoherent stories. We decided to set the temperature to 0.9 in the high-\ntemperature condition and maintain the same value as in Experiment 1: 0.5 in the low-\ntemperature condition.\nFor each age group (Ages 1–5), we generated 10 texts for each of the 500 books in two\nruns, using both low- (0.5) and high-temperature (0.9) settings. Additionally, we generated\none run of 10 child-directed texts per book without age specification for both temperature\nsettings. In total, this resulted in 55,000 child-directed texts for each temperature setting.\nTo generate adult-directed texts, we performed three runs of 10 texts per book, again using\nboth temperature settings. This resulted in 15,000 texts for the low-temperature setting\nand 15,000 for the high-temperature setting.\nWe compared word frequency measures for both the child reading performance data\n(i.e., from Grades 1-4) and the adult reading performance data (i.e., younger and older\nINSERT SHORTTITLE COMMAND IN PREAMBLE 17\nadults).\nResults\nWord frequency distributions and lexical richness\nThe generated text was generally shorter compared to Experiment 1, averaging\nabout 278 words per text for the high temperature conditions and about 270 for the low\ntemperatureconditions. Table4showsthatallfourcrossedtemperatureandtargetaudience\nconditions generally resulted in more unique types and lemmas compared to the LLM corpus\nfrom Experiment 1 (see Table 1). However, childLex was still much richer than all LLM\ncorpora.\nFor the adult-directed LLM corpora, the higher number of unique types, compared\nto the LLM corpus from Experiment 1, is plausibly due to the adult-directed audience\nprompt change creating a richer vocabulary (i.e., see ~46k in Experiment 1 vs. ~71k and\n~84k in the adult-directed LLM corpora). For the child-directed corpora, it is likely that the\nincrease in unique types is due to a larger corpus size (i.e., see ~46k in Experiment 1 vs. ~85k\nand ~111k in the child-directed corpora from Experiment 2). For the child-directed corpora,\nthe % of hapax tokens (i.e., % hapax legomena across all tokens, meaning the percentage\nof tokens in the corpus that occur only once) was lower compared to Experiment 1 (.12%\nand .18% vs. .25%). This is also plausibly due to a larger corpus size, as the number of\nhapax legomena does not increase substantially for larger sample sizes (see the differences\nin slopes of Figures 1 and B4. The % of hapax legomena across all types or all unique\nlemmas remained relatively constant with a larger corpus size (Types: Exp. 2: 33.56% vs.\nExp. 1: 33.03%; Lemmas: Exp. 2: 33.00% vs. Exp. 1: 33.09%).\nFurthermore, thehightemperaturesettinghasastrongerinfluenceonthepercentage\nof hapax types than having an adult target audience. In other words, the temperature\nsetting more strongly influenced the percentage of words used only once, compared to the\ntarget audience. This can be illustrated by comparing percentages: 38.21% for ChHT vs.\n33.56% for ChLT is a larger difference than 34.69% for AdLT vs. 33.56% for the ChLT.\nTaken together, generating texts with a high temperature setting and prompting for\nadult text increased the lexical richness. However, lexical richness was still substantially\nlower than the lexical richness in childLex (48.74% hapax types in childLex vs.< 38.21%\nin the LLM corpora).\nUsing the growth curves from extrapolated sample sizes, we also see that the lexical\nrichness in both adult-directed corpora was higher than in the child-directed corpora (see\nFigure B4). The number of types, at around 20 million tokens (2 × 107), is around 90,000\ntypes for the AdLT (adult-directed low temperature, see above) corpus and 75,000 types\nfor the ChLT corpus. Likewise, there are about 105,000 types for the AdHT corpus and\nonly 95,000 types for the ChHT corpus (i.e., roughly an increase of 10,000-15,000 types\nfor the adult-directed corpora). These differences are smaller compared to the temperature\nmanipulation (90,000 vs. 105,000 types in adult-directed corpora and 75,000 vs. 95,000\ntypes in child-directed corpora). At 10 million tokens, childLex already has more than\ndouble the number of types (about 80,000 types for AdLT, 90,000 types for AdHT, 60,000\ntypes for ChLT, and 70,000 types for ChHT).\nINSERT SHORTTITLE COMMAND IN PREAMBLE 18\nTable 4\nSize and descriptive statistics for the 4 LLM corpora from Experiment 2 compared to\nchildLex. Note that LT & HT refer to low and high temperature settings.\nMeasure childLex Adult LT Adult HT Child LT Child HT\nn Books 500 500 500 500 500\nTokens 9,850,786 7,191,531 7,368,921 23,320,466 23,887,118\nTypes 182,454 71,423 83,921 84,978 110,603\nLemmas 117,952 52,528 61,318 63,552 82,126\n% Hapax tokens 0.90 0.34 0.44 0.12 0.18\n% Hapax types 48.74 34.69 38.21 33.56 38.21\n% Hapax lemmas 48.30 34.82 38.13 33.00 37.05\n% Tokens> 4 97.89 99.41 99.29 99.79 99.71\n% Types> 4 26.53 40.51 37.26 42.48 37.38\n% Lemmas> 4 27.91 40.26 37.13 42.82 38.10\nThis pattern is also confirmed by comparing the relation between word frequency\nand rank for the three new conditions, as in Figure 2 from Experiment 1 (see Figure B1).\nThe three new conditions fall between the curves from Experiment 1 and the curve based\non childLex. The AdHT curve matches childLex most closely.\nThe correlation between LLM and childLex log word frequency was slightly smaller\ncompared to the correlation we found in Experiment 1 (r = .69 vs. .67, see 6A). Word\nfrequency from Experiment 1 correlated most strongly with those from the reproduction\ncondition (ChLT;r = .87). Figure 6B shows the varying numbers of shared words on which\nthe pairwise computed correlations are based. We find higher correlations for the corpora\nwith the same vs. different target audience, regardless of temperature (see Figure 6A). In\ncontrast, as we showed above, the percentage of hapax words across types or lemmas is more\nsimilar within vs. between temperature settings, illustrating the higher lexical richness due\nto increased temperature.\nThe percentages of shared words exhibit the same pattern observed in the correla-\ntions (see Figure B3): the same target audience yields more overlapping words compared to\nusing the same temperature setting. Also, the ChHT corpus contained the highest percent-\nage of words from childLex (33%, see the first column for the ChHT row in Figure B3). This\nsuggests that child-directed prompts result in a better overlap with childLex than adult-\ndirected prompts, and also that a higher temperature results in more recovered words. Also,\ncompared to the 20% shared words of Experiment 1, this marks a notable increase. Vice\nversa, childLex contained the most words from the AdLT corpus (58%, see second column\nfor the childLex row), probably because this corpus was the smallest with the least types.\nThe overlap between the LLM corpora ranged from 33% to 73%. This suggests that corpus\nsize, temperature, and target audience yield relatively distinct statistics.\nThe words in the LLM corpus that do not occur in childLex are very similar to those\nfrom Experiment 1 for both the low- and high-temperature conditions. ChildLex words that\ndo not occur in the LLM corpus are now different, which is plausible, given the larger corpus\nINSERT SHORTTITLE COMMAND IN PREAMBLE 19\n0.950.66\n0.65\n0.810.820.69 0.860.87\n0.870.860.69 0.96\n0.860.860.67\nAdult low temp.\nAdult high temp.\nChild low temp.\nChild high temp.\nExp. 1\nChildLex\nAdult low temp.Adult high temp.Child low temp.Child high temp.\nPearson\nCorrelation\n0.7 0.8 0.9\nA\n5136745803\n41485\n334653238831459 3695435246\n538954962251679 62030\n486954589845215\nAdult low temp.\nAdult high temp.\nChild low temp.\nChild high temp.\nExp. 1\nChildLex\nAdult low temp.Adult high temp.Child low temp.Child high temp.\nShared\nwords\n 40000 50000 60000\nB\nFigure 6\n(A) Pairwise correlations between log word frequency measures derived from the LLM cor-\npora and childLex. (B) Number of shared words between corpora.\nfrom Experiment 2. The words missing from the LLM corpus seem heterogeneous, including\nwords such asimmerhin (anyway), see Tables B3 and B7, ormich (mine), see Tables B4 and\nB8. Even though they are missing here, it is not implausible that such words will eventually\noccur as an LLM corpus gets larger, given their large amounts of training material.\nEvaluating LLM word frequency using on reading performance\nAs in Experiment 1, we estimated model fit increase (i.e., AIC) in relation to a base-\nline model (i.e., without word frequency included). Again, all LLM-based models resulted in\na larger increase in model fit compared to the childLex-based model when fitted to the child\nRTs (see Figure 7A). However, the Experiment 1 LLM corpus, even when generated with a\nlow-temperature setting, still had the highest model fit. For the adult RTs, word frequency\nbased on adult-directed text resulted in a higher model fit compared to child-directed text.\nDWDS word frequency resulted in the best fit (see Figure 7B).\nAll effect size estimations had confidence intervals that excluded 0, indicating a\nsignificant word frequency effect. The CIs also all excluded the estimated effect size for\nchildLex, indicating that LLM word frequency results in weaker estimations of the word\nfrequency effect (see Figure 7C and D).\nINSERT SHORTTITLE COMMAND IN PREAMBLE 20\nChildren RT\n        Children Low T. Short        Children High T. Short\n        Adults Low T. Short        Adults High T. Short        LLama High T. Short        LLama High T. Long\n        DS High T. Short        DS High T. Long\n0\n100\n200\n300\nModel fit increase\n[AIC relative to H0]\nExperiment\nExp. 2\nExp. 3\nA Adults RT\n        Children Low T. Short        Children High T. Short\n        Adults Low T. Short        Adults High T. Short\n0\n100\n200\n300\nModel fit increase\n[AIC relative to H0]\nB\nChildren RT\n        Children Low T. Short        Children High T. Short\n        Adults Low T. Short        Adults High T. Short        LLama High T. Short        LLama High T. Long\n        DS High T. Short        DS High T. Long\n−0.02\n−0.01\n0.00\nFrequency effect\nsize [log. ms]\nC Adults RT\n        Children Low T. Short        Children High T. Short\n        Adults Low T. Short        Adults High T. Short\n−0.02\n−0.01\n0.00\nFrequency effect\nsize [log. ms]\nD\nFigure 7\nModel comparison and effect size results for children (A and C) and adults (B and D).\nA and B show AIC differences based on linear mixed-effect regression models of RTs with\nand without the word frequency measure. C and D show corresponding word frequency\neffect size estimates, including 95% CIs. The solid blue and red lines in panels A and\nC represent Experiment 1 LLM word frequency and childLex, respectively, and the solid\ngreen lines in panels B and D represent DWDS (which was the best-performing model for\nthe adult RTs in Experiment 1). Note that Experiment 2 results are color-coded in red and\nExperiment 3 results are color-coded in dark blue. The best-fitting models in each Experiment\nare highlighted by more intense colors.\nINSERT SHORTTITLE COMMAND IN PREAMBLE 21\nWe found that age-specific corpus generation enables a more accurate estimation\nof the word frequency effect on child reading performance of children compared to adults.\nWe compared the child-directed LLM word frequency fitted to both adult and child reading\nperformance data and vice versa (see Figure 7A vs. B). We did not observe differences in\nestimated effect sizes (overlapping CIs).\nSurprisingly, an increase in temperature results in better model fit for both the child-\nand adult-directed word frequency measures and also for both the child (see Figure 7A) and\nadult RTs (see Figure 7B). The estimated effect sizes remain similar to the Experiment 1\nestimate (see the overlapping CIs in Figures 7C and D).\nSummary: Experiment 2\nExperiment 2 replicated the main finding of Experiment 1: a word frequency mea-\nsure derived from LLM corpora can be used to model the word frequency effect in reading\neffectively. Experiment 2 manipulated two more variables: 1) the LLM temperature pa-\nrameter (high vs. low) to try to increase the lexical richness of the corpus, and 2) the target\naudience (adults vs. children) to investigate if prompting for child-directed text makes a\ndifference.\nWe found that a higher temperature increased the lexical richness of the corpora\nand that higher temperatures resulted in higher model fits. Confirming our assumption\nfor Experiment 1, we found evidence that LLM corpora can indeed be tailored to a specific\ntarget audience. However, the finding that model fit increases with increased lexical richness\nis a new facet not predicted by the findings of the first study. Overall, LLM word frequency\nfrom Experiment 1, with a low lexical richness, showed the highest model fit for children.\nNotably, the corpus with the same settings as in Experiment 1 (i.e., child-directed and low\ntemperature) also resulted in a lower model fit. This finding suggests that using the same\nmodel but different snapshots can yield significantly different results. The main issue is\nthat the LLM that we used for Experiments 1 and 2 is closed-source, motivating the use\nof an open-source model in Experiment 3, which potentially allows for a more reproducible\nuse of LLMs. For adults, model fit increased with lexical richness, but could not reach the\nmodel fit for the book-based DWDS word frequency.\nExperiment 3\nExperiment 3 had three goals: (i) We aimed to replicate the Experiment 1 results\nusing two different open-weight LLMs, making sure that our findings are computationally\nreproducible across models. (ii) We aimed to explore the effects of increasing the length of\ngenerated text. We expected an increase in lexical richness for corpora containing longer\ntexts. (iii) In Experiment 1, we found a stronger model fit to child reaction times for LLM\nword frequency compared to childLex word frequency, despite having lower lexical richness.\nThis indicated that a higher lexical richness does not necessarily lead to better model fit.\nFor Experiment 3, we aimed to further investigate whether higher lexical richness again\nresults in lower model fit to child reaction times (RTs). We adopted a similar procedure to\nthat in Experiment 2, generating four corpora from two different LLMs with two different\ntext lengths, thereby creating another 2× 2 design.\nINSERT SHORTTITLE COMMAND IN PREAMBLE 22\nMethods\nModel choice and prompt\nWe determined that suitable models must have the capability to generate text in\nGerman, have at least open-source weights (ideally also code and full documentation), and\nnot be a distillation, combination, or foundation model. We determined currently available\nand high-performing LLMs using Chatbot Arena (Chiang et al., 2024) and the Hugging Face\nopen LLM leaderboard (Fourrier, Habib, Lozovskaya, Szafer, & Wolf, 2024) in February\n2025. Based on these considerations, we selected Llama-3.3-70B-Instruct (Grattafiori et al.,\n2024) and DeepSeek-V3 (A. Liu et al., 2024) to have models with different architectures and\nnumbers of parameters. DeepSeek is a Mixture-of-Experts (MoE) model with 671B total\nparameters, with 37B activated for each token. Llama 3.3 70B is a dense transformer model\nwith 70B total parameters, all of which are typically active. Weights and limited code and\ndocumentation are available, but no training data or peer-reviewed papers are available,\namong other issues (seehttps://osai-index.eu for an evaluation).\nWe generated short-text corpora with a prompt similar to Experiments 1 and 2:\n“Schreibe eine Kindergeschichte für Kinder im Alter {age} über: {book_title}”(write a\nchildren’s story for children aged {age} about {book_title}). This small prompt change\ncompared to Experiments 1 and 2 ensured generation of stories rather than commentary\nin the two LLMs used here. For the long-text corpora, we experimented with similarly\nsimple but also more complex prompts (e.g., first prompt: describe the first quarter of the\nbook; second prompt: describe the second quarter of the book; etc.). We found that a\ncontinuation-based strategy worked best: The first prompt was always the same as for the\nshort-text corpora, and all follow-up prompts were: \"Continue from: \" + currenttext[-500:],\nuntil reaching 4000 words. This prompted the model with the last 500 characters of the\ncurrent output to improve topical consistency and reduce the generation of generic text.\nTable 5\nSize and descriptive statistics for the four corpora from Experiment 3 compared to\nchildLex. Llama is Llama-3.3-70B-Instruct, and DS-V3 is DeepSeek-V3.\nMeasure childLex Llama long Llama short DS-V3 long DS-V3 short\nn Books 500 500 500 500 500\nTokens 9,850,786 10,332,850 7,215,565 9,763,062 7,162,685\nTypes 182,454 51,320 40,660 239,830 95,321\nLemmas 117,952 39,272 39,272 191,309 74,695\n% Hapax tokens 0.90 0.17 0.19 1.43 0.62\n% Hapax types 48.74 33.62 34.01 58.10 46.52\n% Hapax lemmas 48.30 34.13 34.13 55.39 44.60\n% Tokens> 4 97.89 99.71 99.67 98.03 99.06\n% Types> 4 26.53 42.19 42.09 20.00 29.54\n% Lemmas> 4 27.91 41.49 41.49 21.53 30.46\nINSERT SHORTTITLE COMMAND IN PREAMBLE 23\nProcedure\nFor the short-text corpora, we conducted 50 runs of text generation for each of the\n500 book titles. To reach a text length similar to Experiment 2, we set the max_tokens pa-\nrameter to 600 for Llama and 700 for DeepSeek, after experimenting with different settings.\nText generation resulted in an average of 287 (Llama) and 284 (DeepSeek) tokens per text,\nwith a total corpus size of about 8 million. For the long-text corpora, we conducted five\nruns of text generation for each of the 500 book titles. We set max_tokens to 2000, and we\nconfigured the prompt to continue producing text until at least 4000 words per book were\ngenerated, which resulted in an average of 4155 (Llama) and 4031 (DeepSeek) words per\ntext. Following our findings from Experiment 2, we set the temperature parameter to 0.7 for\nall generated texts. We also retained the default settings of other parameters in both mod-\nels (top_p = 0.7 and top_k = 50). For the long corpora, the total corpus size was similar\nto that of childLex (see Table 5). We used meta-llama/Llama-3.3-70B-Instruct-Turbo and\ndeepseek-ai/DeepSeek-V3 (not the updated DeepSeek-V3-0324, which was not yet avail-\nable) deployed on Together.ai (https://api.together.ai/models) in March 2025. Note\nthat \"Turbo\" only indicates that the model is quantized to FP8, a numerical format used\nfor deployment at large scale with minimal impact on model output. The cost was US$\n0.88 per million tokens for Llama and US$ 1.25 per million for DeepSeek. The DeepSeek\nmodel resulted in frequent connection errors and significantly slower text generation speed\nthan Llama.\nWe extended the pre-processing pipeline from Experiments 1 and 2 by applying\nadditional normalization to the frequency list. After removing numbers/punctuation, we\nfurther cleaned the data by removing remaining special punctuation, non-alphabetic charac-\nters (except German diacritics), and consolidating whitespace. Duplicate word forms were\nmerged under their normalized versions while retaining original forms for reference. This\nreduced noise from orthographic variants while preserving linguistic content. The additional\nnormalization had only a marginal effect on the results.\nResults\nWord frequency distributions and lexical richness\nLexical richness was higher in both DeepSeek corpora compared to both Llama\ncorpora and all Experiment 1 and 2 corpora, see % hapax tokens and types in 5. The\nshort-text DeepSeek corpus had a slightly lower lexical richness and the long-text corpus\nhad a higher lexical richness than childLex (e.g., .62% and 1.43% words occur only once\nvs. .90% for childLex; see Table 5). Figure C1 also shows growth curves of these values for\ndifferent corpus sizes. The word frequency correlations are also higher for both DeepSeek\ncorpora, and the number of shared words is highest for the DeepSeek long-text corpus (see\nFigure 8). Normalizing for corpus size, the long-text DeepSeek corpus uses about a third\nof the childLex words (39%, see Figure C2). In contrast, the Llama corpus included only\nabout one-sixth of the childLex words (this was similar to the Experiment 1 overlap and\nlower than the Experiment 2 overlap, see B3). The long-text DeepSeek corpus generated\nthe highest number of types overall, resulting in an increase of about 50 thousand types\ncompared to childLex (i.e., see Table 5). Thus, the long-text DeepSeek corpus contained\nINSERT SHORTTITLE COMMAND IN PREAMBLE 24\n0.69 0.70 0.73 0.77 0.84\n0.72\n0.73 0.77\n0.59 0.67 0.62\n0.65 0.68 0.75 0.79\nDeepSeek long\nDeepSeek short\nLLama long\nLLama short\nExp. 1\nChildLex\nDeepSeek longDeepSeek short\nLLama longLLama short\nPearson\nCorrelation\n0.6 0.7 0.8\nA\n31554 33077 28333 25660 23568\n62288\n44497 52172\n29486 33232 25173\n26287 27924 25466 24786\nDeepSeek long\nDeepSeek short\nLLama long\nLLama short\nExp. 1\nChildLex\nDeepSeek longDeepSeek short\nLLama longLLama short\nShared\nwords\n 30000 45000 60000\nB\nFigure 8\n(A) Pairwise correlations between log word frequency measures derived from the DeepSeek\nand Llama corpora. For comparison, we additionally included the LLM corpus from Exper-\niment 1 and childLex. (B) Number of shared words between corpora. Numbers and color\nscale represent Pearson correlations in A and the number of shared words in B.\nboth many childLex words as well as words not in childLex (see Tables C1 and C2 for\nexamples).\nFor the Llama corpora, we found similar lexical richness to that of the GPT-based\ncorpora, again showing lower lexical richness compared to childLex (i.e., see Table 5). We\nfound no clear difference between long and short Llama corpora. Llama resulted in lower\nword frequency correlations with childLex and had lower overlap with childLex compared\nto DeepSeek (see Figure C2).\nEvaluating LLM word frequency using reading performance data\nWe found that word frequency based on the Llama short-text corpus resulted in\na higher model fit to child RTs compared to all other word frequency measures generated\nacross Experiments 1, 2, and 3 (see Figure 7A). In contrast, both DeepSeek corpora per-\nformed less well. The word frequency measure based on the lexically rich long-text DeepSeek\ncorpus was the only LLM-based measure that resulted in a lower model fit compared to\nthe childLex word frequency measure. Still, all effect size estimates were highly comparable\nINSERT SHORTTITLE COMMAND IN PREAMBLE 25\n50\n100\n150\n200\n40 50\nLexical richness [% Hapax types]\nModel fit [delta AIC]\nCorpus\ngenerator\nChildLex\nDS\nGPT\nLLama\nFigure 9\nModel fit over lexical richness (measured by % hapax tokens) across nine LLM generated\ncorpora and childLex.\n(see Figure 7B). Together, these results are in line with the observation that corpora with\nhigher lexical richness are less well-suited to explain reading performance in young readers.\nIndeed, when we compiled all model fits and lexical richness estimates from the LLM cor-\npora and childLex generated across all three experiments, we found a strong correlation (r\n= -.69, see Figure 9).\nSummary: Experiment 3\nIn Experiment 3, we extended our study using two open-weight LLMs (Llama-\n3.3-70B-Instruct and DeepSeek-V3), generating short- and long-text corpora. DeepSeek\ngenerated text with similar or higher lexical richness than childLex, although both LLMs\ngenerated unusual word combinations rarely used in natural language (e.g., see Table C1).\nThe Llama-based corpora were less lexically rich, but the extracted word frequency mea-\nsures described the word frequency effect in reading performance better than any other\ncorpus; in particular, for the short-text Llama corpus. Combining the results from all three\nexperiments thus showed that corpora with lower lexical richness generally resulted in better\nmodel fits. Experiment 3 did not further investigate the role of the temperature parameter.\nGeneral Discussion\nIn three experiments, we explored whether word frequency derived from LLM text\nis adequate for estimating the word frequency effect in visual word recognition. We decided\nto focus on German children because we expected to need a smaller corpus, and relevant\nresources are available in German.\nIn Experiment 1, we generated an LLM corpus based on child-directed texts using\nGPT-3.5-turbo. The corpus was less lexically rich compared to a corpus of child-directed\nINSERT SHORTTITLE COMMAND IN PREAMBLE 26\ntexts written by humans (childLex; Schroeder et al., 2015). Compared to childLex word\nfrequency, LLM word frequency better explained variability in child reading performance\n(lexical decision reaction times for 1000+ words taken from DeveL; Schröter & Schroeder,\n2017). Interestingly, when comparing the size of the word frequency effect, the better-fitting\nLLM word frequency resulted in a lower effect size.\nExperiment 2 demonstrated that using child-directed prompts indeed results in a\nword frequency measure that is adjusted to child RTs. When we generated text with child-\ndirected prompts, it was less lexically rich and fitted better to child RTs. When we generated\ntext with adult-directed prompts, it was more lexically rich and fitted better to adult RTs.\nExperiment 2 also showed that increasing the temperature can increase lexical richness,\nbut not to the same level as that of childLex. All four Experiment 2 corpora, including the\nreproduction condition, resulted in lower model fit to child RTs compared to the Experiment\n1 corpus. This was likely a result of using a newer snapshot LLM version in Experiment 2,\nas this was the only difference from the Experiment 1 generation procedure.\nIn Experiment 3, we generated short- and long-text corpora using open-weight\nLLMs. Llama generated text with relatively low lexical richness, while Llama word fre-\nquency described lexical decision times best (i.e., model fit was better compared to all\nother LLM-based frequency measures). DeepSeek generated corpora with a higher lexical\nrichness than every other corpus, while DeepSeek word frequency did not result in a good\nmodel fit. In a final analysis, we correlated the lexical richness and model fit of all available\ncorpora and demonstrated that the model fits decrease with increasing lexical richness of a\ncorpus. In summary, we generated LLM corpora with varying target audiences, temperature\nsettings, closed- and open-weight LLMs, and text lengths. LLM word frequency captured\nthe word frequency effect in child reading across all corpora, and effect size estimations were\nsimilar.\nLexical richness of LLM corpora\nCorpus comparison revealed that all but one LLM corpus contained fewer unique\ntypes than the childLex corpus. Measures of lexical richness included the percentages of\nhapax legomena, see Figure 9, and the predicted number of types based on LNRE models,\nsee Figure C3. In line with this, word frequency distributions generally also include more\nfrequent types. We could increase lexical richness by (i) increasing the temperature, (ii)\nprompting for adult-directed text, (iii) increasing the text length, and (iv) using different\nLLMs. We observed a comparably large increase in lexical richness for DeepSeek but not\nfor Llama, especially for long text lengths. This finding could be attributed to the larger\nnumber of model parameters, training data, or the architecture used, among other factors.\nExplaining this pattern in detail requires additional analysis, as well as greater transparency\nin, for example, training data (Liesenfeld et al., 2023).\nWhat does lexical richness of LLM text indicate? The results of Experiment 2 rule\nout that low lexical richness is simply the result of using child-directed prompts or a low\ntemperature value. Extrapolation also showed that if the LLM corpora would have had the\nsame size as childLex (10 million tokens), they would have had roughly less than half of\nthe types (300k types for childLex vs. somehwere between 75k and 175k types, depending\non the corpus), see Figure C3. The lexical richness of childLex is roughly comparable to\nSUBTLEX and other existing adult-directed corpora (Baayen, 2001).\nINSERT SHORTTITLE COMMAND IN PREAMBLE 27\nIt has been observed before that LLM performance can suffer when confronted\nwith words that do not appear often in training data across a number of cognitive tasks,\nincludingreasoningRazeghi, LoganIV,Gardner,andSingh(2022)andansweringfact-based\nquestions Kandpal, Deng, Roberts, Wallace, and Raffel (2023). If such biases exist in low-\nfrequency words, it seems reasonable for LLMs to show a preference for higher-frequency\nwords when prompted to generate text. The low-frequency words that the LLMs used in\nour study were often unusual combinations or derivations of other words (e.g.,Vollmond-\ntanz, meaning \"full-moon-dance\" and Lieblings-Sonnbrille, which translates to favourite-\nsunglasses). Tables C1 and C1 include some of the words not in childLex or that occur the\nleast often in childLex. In the DeepSeek-long corpus, this resulted in higher lexical richness,\nbut the resulting set of DeepSeek words was still considerably different from childLex. For\nexample, out of the 180k words in childLex, the DeepSeek long-text corpus included only\nabout 60k words (i.e., only about 1/3 of the childLex). Still, this overlap was the highest\nacross all LLM corpora. Thus, the high lexical richness of the DeepSeek long-text corpus\nresults from 60k words that overlap with childLex and about 180k words that do not overlap\nwith childLex.\nDespite the low richness of much of the LLM text that we generated, LLM text still\nresulted in word frequency measures that are similar to existing word frequency measures.\nWe observed a moderately strong correlation between childLex and the Experiment 1 LLM\nword frequency (.69 for log-frequency per million across ~50,000 words), which seems typical\ncompared to previously reported correlations. For example, SUBTLEX-UK (an English\nsubtitle-based corpus) correlated at r = .66 to the Children Printed Word Database (~9,000-\nword frequencies across 11,000 English children books, (Van Heuven et al., 2014) using a\ndifferent but similar transformation of the word frequency counts. The LLM text thus does\nseem to represent certain qualities of natural text.\nIt was clear that LLM text remained qualitatively different from human-written\ntext. LLM-based text was more stereotypical, while narrative, figurative, and rhetorical\nelements were often missing or felt unnatural. Subjectively, different LLMs yielded distinct\nstyles. LLMs more often generated words central to the prompted book (e.g.,wand in Harry\nPotter) and used fewer function words and word types that indicate direct speech, such as\n\"sagt\" (says). Future efforts could focus on prompting for book-length text (i.e., as shown\nhere or here) to increase lexical richness and improve coherence etc. The corpus containing\nlonger texts generated with DeepSeek did show a comparably high lexical richness, but this\nwas also the corpus that subjectively contained the least coherent stories. Although we used\nlexical richness as a measure to characterize related differences in comparison with childLex,\nit is necessary to perform more detailed comparisons with other corpora as well as study\nother lexical, grammatical, and semantic statistics, including entropy, complexity, word\nfrequency profiles, embeddings etc. (Dentella, Günther, & Leivada, 2023; Hu, Mahowald,\nLupyan, Ivanova, & Levy, 2024; Muñoz-Ortiz, Gómez-Rodríguez, & Vilares, 2024; Wu et\nal., 2024). Kumarage and Liu (2023) applied stylometric analyses to LLM texts and found\nthat GPT 3.5 and 4 styles do not differ substantially from each other, relative to open-\nsource LLMs. More research is necessary to determine the literary capabilities of modern\nLLMs compared to humans. Based on our findings, the relatively low lexical richness of\nLLMs could be troubling for using them to study child lexicon development (Korochkina &\nRastle, 2025) and educational uses in general (see also Kasneci et al., 2023).\nINSERT SHORTTITLE COMMAND IN PREAMBLE 28\nEvaluation based on the word frequency effect in reading performance\nWe estimated the effect of word frequency in child reading performance to test\nwhether LLM word frequency could characterize their lexical memory or how they access\ntheir mental lexicon (Brysbaert et al., 2011, 2018). LLM word frequency allowed an ade-\nquate estimation of the word frequency effect while controlling for relevant covariates (age of\nacquisition, OLD20, word length). Most word frequency measures, except for the measure\nbased on the DeepSeek short-text corpus, resulted in higher model fit than the children’s\nbook-based frequency measure. Word frequency from less rich LLM corpora best fits the\ndata, with the Llama short text corpus showing the highest model fit.\nThe estimated size of the frequency effect was lower than we expected. Except for\ngrade 1, childLex word frequency showed a larger effect size. Also, for adults, the frequency\nestimate that resulted in the best-fitting model showed a similar size (see Schröter &\nSchroeder, 2017, for effect size estimates with the DWDS word frequency). This effect size\nreduction may be the result of the lower lexical richness of the best-fitting LLM corpora.\nThe corpora with the highest lexical richness (childLex and DeepSeek) included nearly all\nwords in the DeveL dataset (> 96%; see Figures B3 and C2). In contrast, the corpora with\nlower lexical richness included lower numbers of words from the DeveL selection of words.\nThus, LLM word frequency was zero for more words in this subset compared to childLex\nor the DeepSeek corpora (see the right panel of Figure B2). This distinction in the word\nfrequency distribution of the DeveL words was also the most apparent difference between\nfrequency measures. This finding indicates that children typically know these words less\nwell, such that the word frequency estimate that excludes the word better represents the\nmental lexicon of children. Low-frequency or unknown words are important here, as words\nthat are not yet or recently learned must be processed in a different way compared to already\nestablished words (e.g., see Gagl & Gregorová, 2024; Gagl, Richlan, et al., 2022). Even for\nadults, when an unknown word is encountered, a different behavior has to be established\ncompared to when a known word is encountered (i.e., look up in a lexicon vs. move to\nthe next word to establish sentence/text comprehension). This finding is also in line with\ninvestigations of children’s vocabulary that show an increase in vocabulary size with age\n(e.g., Keuleers, Stevens, Mandera, & Brysbaert, 2015; Segbers & Schroeder, 2017). Note\nthat estimations of low-frequency words are inherently noisier than high-frequency words,\nas fewer observations are present.\nWhy does LLM word frequency result in a better fit to reading performance data? It\nis unlikely that LLMs closely mimic the mental representations of children, given the distinct\nnature of LLMs. Their training data is too large (Frank, 2023a), and the architectures are\nnon-biologically plausible (Frank, 2023a). LLM word frequency could be a coincidental\nproduct of a regularization process applied to the word frequency in the original training\ndata. LLMs’ computational mechanisms are designed to prevent overfitting and encourage\nthe production of smoother, more generalizable words. Thus, generating words that occur\nslightly more often may be more appropriate in many contexts. Authors of children’s books,\non the other hand, are motivated to select words based on literary interests (Korochkina &\nRastle, 2025).\nOh et al. (2024) found that surprisal estimates from lower-performing models, e.g.,\nas measured by perplexity (less predictive models), can better fit human reading times (see\nINSERT SHORTTITLE COMMAND IN PREAMBLE 29\nalso Boeve & Bogaerts, 2024). Here, we found that children’s stories generated by Llama-\n3.3-70B-Instruct and GPT3.5-turbo are less lexically rich compared to DeepSeek-V3, but\nbetter explained the frequency effect in children’s reaction times. If we assume that lower\nlexical richness is a result of lower model performance in general, our results are consistent\nwith this finding. Specifically, the models in our study that generate less lexically rich texts\nprovide a better fit to lexical decision times. They, therefore, may better reflect an overlap\nof the vocabulary used by the model and stored in the children’s mental lexicon. It seems\nto be an empirical question, how lexical richness relates to model perplexity.\nLimitations and future directions\nIn this study, we focused on text targeted at German children and reading per-\nformance measured by lexical decision times from German children. It is unclear whether\nLLMs can generate linguistic corpora for other, less well-represented groups, languages, and\nreading behaviors (Blasi, Henrich, Adamou, Kemmerer, & Majid, 2022; Gagl, Gregorova, et\nal., 2022). As discussed above, more research is necessary to investigate the limitations of\nsuch extrapolation and the development of new resources that are currently unavailable but\nurgently needed (Blasi et al., 2022; Henrich, Heine, & Norenzayan, 2010). Extrapolating\nto groups underrepresented in the training data will likely be affected by the Western bias\nin currently used training data (Atari et al., 2023; Rystrøm, Kirk, & Hale, 2025). This\nis supported by results indicating that simply adding multilingual training data does not\nnecessarily improve multilingual LLM performance, possibly due to capacity limits (Chang\net al., 2023). The LLMs used here are not specifically developed for multilingual tasks and\ncan achieve worse performance compared to specialized models in multilingual tasks (Lai\net al., 2023). German can be considered a high-resource language that can outperform\nlanguages like Chinese or Vietnamese across several multilingual tasks (Lai et al., 2023).\nThus, practicalusesmayconflictwithcurrentmodelbiases, particularlyforlow-resourceand\nnon-Western languages. Although little is known about LLM performance in low-resource\nlanguages, the approach described here could be a first step. Recent investigations show\nthat using language-specific LLMs can be beneficial for the extraction of psycholinguistic\nmeasures (Boeve & Bogaerts, 2024).\nIn Experiment 3, we replicated findings from Experiment 1 with open-weight mod-\nels. A next step is to select less complex models, potentially in toy examples, with all\nparameters known. We initiated this study using a non-transparent, large, and continu-\nously evolving model. Mechanistic explanations become impossible when complexity is so\nhigh that the model becomes a black box (Bender et al., 2021). For studying cognitively\nplausible ways in which word frequency approximates reading times, LLMs that are only as\nlarge as developmentally plausible are necessary (see Feng et al., 2024; Hu & Frank, 2024;\nTan et al., 2024, for recent work in this direction). Furthermore, performance changes in\nunpredictable ways for non-transparent large language models (LLMs) that are being fine-\ntuned continuously using reinforcement learning with human feedback (RLHF Bai et al.,\n2022; Chung et al., 2024; Perez et al., 2022; Ziegler et al., 2020). In this process, LLMs\nare trained to better align with user requirements. This alternative optimization leads to\na necessary trade-off between performance on next-word prediction and performance on\nalignment. Here, LLM word frequency from Experiment 1 resulted in much better model\nfits (AIC > 100) compared to the replication condition from Experiment 2. This difference\nINSERT SHORTTITLE COMMAND IN PREAMBLE 30\nis most likely due to changes between different snapshot versions of the same model that\nwe used for Experiments 1 and 2.\nWith respect to corpus comparison, the lexical richness measures and word reading\ntimes we used here are simplistic qualities of text compared to the cultural, social, and\nethical themes and pedagogical considerations underlying the text of children’s books (see\ne.g. Korochkina & Rastle, 2025). This study does not discuss the higher-level syntax or\nsemantics of the books analyzed here (but see Figures B8, B9 for an analysis of POS\nand B10 for an analysis of word embeddings). Furthermore, the study does not provide\na detailed examination of whether the LLM text can be used to evaluate these qualities.\nThe text generated for this study is available (i.e., see OSF.io) and can still be used for\nsuch investigations. We showed that explained variation in word reading times depends\non how lexical properties are quantified, including word frequency. We showed that LLM\ntext contains atypical patterns, such as the spillover effects from English to German or\nnumerous words found only in the LLM corpus but not in childLex. Such patterns suggest\nthat generated text cannot substitute for human text, particularly in settings involving\nvulnerable participants (e.g., in the context of schools or the broader public).\nLLM word frequency explains more variance, but results in a smaller effect size\ncompared to child book-based word frequency. These findings are difficult to explain within\nthe current approach. How can cognitively implausible LLM word frequency be more in-\nformative about processing measures than traditionally used book-based word frequency?\nThis is an empirical question with many possible answers. Approaching an answer will\ninvolve studying how adults infer which words are known by children, how well they can\ninfer children’s vocabulary, and if authors, on purpose, add less well-known words to their\nbooks for pedagogical purposes, e.g, to stimulate vocabulary growth (Korochkina & Rastle,\n2025).\nConclusions\nSurprisingly, word frequency based on child-directed LLM text is similar to existing\nchildren’s book-based word frequency (e.g., childLex). Lexical richness seems to depend\non the type of LLM. LLM word frequency better describes the effect of word frequency on\nreading performance in German children, but the the estimated effect size is smaller. Thus,\nLLM corpora seem to open up new possibilities for investigating the word frequency effect,\none of the strongest and most replicated effects in psycholinguistic research (Brysbaert et\nal., 2018), but also relevant to other domains of cognitive psychology (i.e., object recognition\nGregorova et al., 2023).\nStill, caution is advised when trying to understand the possibilities of LLMs for\nlanguage development research. LLMs deviate in crucial ways from natural language ac-\nquisition pathways. The different nature of LLMs results, on the one hand, in surprisingly\ndifferent patterns of language use, and on the other hand, in patterns of word processing\ncost that closely follow empirical data from children. The result is an approach to quantify\nand compare how the elements of LLM text correlate with metrics from classic corpora and\nhuman behavior.\nINSERT SHORTTITLE COMMAND IN PREAMBLE 31\nDeclarations\nFundingThis research was supported by the University of Cologne and the German\nResearch Fund (DFG, N° 523332674).\nConflicts of interestWe have no conflict of interest to disclose.\nEthics approvalNot applicable.\nConsent to participateNot applicable.\nConsent for publicationNot applicable.\nAvailability of data, materials, code, and supplementary materialsSee our\nOSF repository: osf.io.\nAuthor contributionsanonymized\nINSERT SHORTTITLE COMMAND IN PREAMBLE 32\nReferences\nAdelman, J. S., Brown, G. D., & Quesada, J. F. (2006, September). Contextual\nDiversity, Not Word Frequency, Determines Word-Naming and Lexical Decision\nTimes. Psychological Science, 17(9), 814–823. Retrieved 2023-05-16, fromhttp://\njournals.sagepub.com/doi/10.1111/j.1467-9280.2006.01787.x doi: 10.1111/\nj.1467-9280.2006.01787.x\nAkaike, H. (1974). A new look at the statistical model identification.IEEE Transactions\non Automatic Control, 19(6), 716–723. doi: 10.1109/TAC.1974.1100705\nAtari, M., Xue, M. J., Park, P. S., Blasi, D., & Henrich, J. (2023). Which humans?\nhttps://osf.io/preprints/psyarxiv/5b26t/. (Preprint. Publisher: PsyArXiv)\nBaayen, R. H. (2001).Word frequency distributions(Vol. 18). Springer Science & Business\nMedia.\nBaayen, R. H. (2008).Analyzing linguistic data: A practical introduction to statistics using\nR. Cambridge: Cambridge University Press.\nBaayen, R. H. (2010, December). Demythologizing the word frequency effect: A dis-\ncriminative learning perspective. The Mental Lexicon, 5(3), 436–461. Retrieved\n2023-05-16, from http://www.jbe-platform.com/content/journals/10.1075/ml\n.5.3.10baa doi: 10.1075/ml.5.3.10baa\nBaayen, R. H., Piepenbrock, R., & Van Rijn, H. (1993). The CELEX lexical database\n[cd-rom]. Philadelphia: Linguistic Data Consortium, University of Pennsylvania..\nBai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., ... Kaplan, J. (2022,\nApril). Training a Helpful and Harmless Assistant with Reinforcement Learning from\nHuman Feedback. arXiv. Retrieved 2024-07-11, from http://arxiv.org/abs/2204\n.05862 (arXiv:2204.05862 [cs])\nBender, E. M., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021, March). On the\nDangers of Stochastic Parrots: Can Language Models Be Too Big? InProceedings of\nthe 2021 ACM Conference on Fairness, Accountability, and Transparency(pp. 610–\n623). Virtual Event Canada: ACM. Retrieved 2024-08-09, from https://dl.acm\n.org/doi/10.1145/3442188.3445922 doi: 10.1145/3442188.3445922\nBinz, M., Akata, E., Bethge, M., Brändle, F., Callaway, F., Coda-Forno, J., ... oth-\ners (2024). Centaur: a foundation model of human cognition. arXiv preprint\narXiv:2410.20268.\nBlasi, D. E., Henrich, J., Adamou, E., Kemmerer, D., & Majid, A. (2022). Over-reliance\non English hinders cognitive science.Trends in cognitive sciences, 26(12), 1153–1170.\n(Publisher: Elsevier)\nBoeve, S., & Bogaerts, L. (2024, December). A Systematic Evaluation of Dutch Large\nLanguage Models’ Surprisal Estimates in Sentence, Paragraph, and Book Reading.\nOSF. Retrieved 2025-06-03, fromhttps://osf.io/vqnw6_v1 doi: 10.31219/osf.io/\nvqnw6\nBommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., ... Liang, P.\n(2022, July).On the Opportunities and Risks of Foundation Models.arXiv. Retrieved\n2024-04-24, fromhttp://arxiv.org/abs/2108.07258 (arXiv:2108.07258 [cs]) doi:\n10.48550/arXiv.2108.07258\nBotch, T. L., & Finn, E. S. (2024, March). Humans diverge from language models when\nINSERT SHORTTITLE COMMAND IN PREAMBLE 33\npredicting spoken language.. Retrieved 2025-06-03, fromhttps://openreview.net/\nforum?id=DqKjKQtyna\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... Askell,\nA. (2020). Language models are few-shot learners.Advances in neural information\nprocessing systems, 33, 1877–1901.\nBrysbaert, M., Buchmeier, M., Conrad, M., Jacobs, A. M., Bölte, J., & Böhl, A. (2011,\nJuly). The Word Frequency Effect. Experimental Psychology, 58(5), 412–424.\nRetrieved from https://doi.org/10.1027%2F1618-3169%2Fa000123 (Publisher:\nHogrefe Publishing Group) doi: 10.1027/1618-3169/a000123\nBrysbaert, M., Mandera, P., & Keuleers, E. (2018). The word frequency effect in word\nprocessing: An updated review. Current Directions in Psychological Science, 27(1),\n45–50. (Publisher: Sage Publications Sage CA: Los Angeles, CA)\nBrysbaert, M., Stevens, M., Mandera, P., & Keuleers, E. (2016). The impact of word preva-\nlence on lexical decision times: Evidence from the Dutch Lexicon Project 2.Jour-\nnal of Experimental Psychology: Human Perception and Performance, 42(3), 441–\n458. Retrieved 2023-05-16, from http://doi.apa.org/getdoi.cfm?doi=10.1037/\nxhp0000159 doi: 10.1037/xhp0000159\nCaucheteux, C., & King, J.-R. (2022, February). Brains and algorithms partially converge\nin natural language processing.Communications Biology, 5(1), 1–10. Retrieved 2025-\n06-03, fromhttps://www.nature.com/articles/s42003-022-03036-1 (Publisher:\nNature Publishing Group) doi: 10.1038/s42003-022-03036-1\nChandra, J., Witzig, N., & Laubrock, J. (2023, May). Synthetic predictabilities from\nlarge language models explain reading eye movements. InProceedings of the 2023\nSymposium on Eye Tracking Research and Applications(pp. 1–7). New York, NY,\nUSA: Association for Computing Machinery. Retrieved 2023-09-06, fromhttps://\ndl.acm.org/doi/10.1145/3588015.3588420 doi: 10.1145/3588015.3588420\nChang, T. A., Arnett, C., Tu, Z., & Bergen, B. K. (2023, November).When Is Multilingual-\nity a Curse? Language Modeling for 250 High- and Low-Resource Languages.arXiv.\nRetrieved 2024-05-15, fromhttp://arxiv.org/abs/2311.09205 (arXiv:2311.09205\n[cs])\nChiang, W.-L., Zheng, L., Sheng, Y., Angelopoulos, A. N., Li, T., Li, D., ... Stoica, I.\n(2024, March). Chatbot Arena: An Open Platform for Evaluating LLMs by Human\nPreference. arXiv. Retrieved 2025-03-24, fromhttp://arxiv.org/abs/2403.04132\n(arXiv:2403.04132 [cs]) doi: 10.48550/arXiv.2403.04132\nChilson, S., Schmalz, X., & Sineva, E. (2024, July).FILMS: A Multilingual Word Frequency\nCorpus based on Film Subtitles with IPA Transcriptions.OSF. Retrieved 2024-08-13,\nfrom https://osf.io/zy5qf doi: 10.31219/osf.io/zy5qf\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., ... Wei, J. (2024).\nScaling Instruction-Finetuned Language Models. Journal of Machine Learning Re-\nsearch, 25(70), 1–53. Retrieved 2024-06-27, from http://jmlr.org/papers/v25/\n23-0870.html\nDavies, R. A. I., Arnell, R., Birchenough, J. M. H., Grimmond, D., & Houlson, S.\n(2017). Reading through the life span: Individual differences in psycholinguistic\neffects. Journal of Experimental Psychology: Learning, Memory, and Cognition ,\n43, 1298–1338. (Place: US Publisher: American Psychological Association) doi:\nINSERT SHORTTITLE COMMAND IN PREAMBLE 34\n10.1037/xlm0000366\nDentella, V., Günther, F., & Leivada, E. (2023, December). Systematic testing of three\nLanguage Models reveals low language accuracy, absence of response stability, and\na yes-response bias. Proceedings of the National Academy of Sciences, 120(51),\ne2309583120. Retrieved 2024-03-05, from https://www.pnas.org/doi/10.1073/\npnas.2309583120 (Publisher: Proceedings of the National Academy of Sciences)\ndoi: 10.1073/pnas.2309583120\nDentella, V., Günther, F., Murphy, E., Marcus, G., & Leivada, E. (2024, Novem-\nber). Testing ai on language comprehension tasks reveals insensitivity to underlying\nmeaning. Scientific Reports, 14(1). Retrieved from http://dx.doi.org/10.1038/\ns41598-024-79531-8 doi: 10.1038/s41598-024-79531-8\nDesbordes, T., Lakretz, Y., Chanoine, V., Oquab, M., Badier, J.-M., Trébuchon, A., ...\nKing, J.-R. (2023). Dimensionality and ramping: Signatures of sentence integration in\nthe dynamics of brains and deep language models.Journal of Neuroscience, 43(29),\n5350–5364. Retrieved 2025-06-03, fromhttps://www.jneurosci.org/content/43/\n29/5350.abstract (Publisher: Society for Neuroscience)\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019).Bert: Pre-training of deep\nbidirectional transformers for language understanding.Retrievedfrom https://arxiv\n.org/abs/1810.04805\nEvanson, L., Lakretz, Y., & King, J.-R. (2023). Language acquisition: do children and\nlanguage models follow similar learning stages?arXiv preprint arXiv:2306.03586.\nEvert, S. (2004). A simple LNRE model for random character sequences. InProceedings of\nJADT (Vol. 2004, pp. 411–422).\nFeinerer, I., Hornik, K., & Meyer, D. (2008). Text Mining Infrastructure in R.Journal of\nStatistical Software, 25(5). Retrieved 2023-07-06, fromhttp://www.jstatsoft.org/\nv25/i05/ doi: 10.18637/jss.v025.i05\nFeng, S. Y., Goodman, N. D., & Frank, M. C. (2024, August).Is Child-Directed Speech\nEffective Training Data for Language Models? arXiv. Retrieved 2024-08-13, from\nhttp://arxiv.org/abs/2408.03617 (arXiv:2408.03617 [cs])\nFerrand, L., New, B., Brysbaert, M., Keuleers, E., Bonin, P., Méot, A., ... Pallier, C.\n(2010). TheFrenchLexiconProject: Lexicaldecisiondatafor38,840Frenchwordsand\n38,840 pseudowords.Behavior research methods, 42, 488–496. (Publisher: Springer)\nFourrier, C., Habib, N., Lozovskaya, A., Szafer, K., & Wolf, T. (2024). Open LLM\nLeaderboard v2. Hugging Face. Retrieved fromhttps://huggingface.co/spaces/\nopen-llm-leaderboard/open_llm_leaderboard\nFox, J., & Monette, G. (1992, March). Generalized Collinearity Diagnostics.Journal of\nthe American Statistical Association, 87(417), 178–183. Retrieved 2024-08-20, from\nhttp://www.tandfonline.com/doi/abs/10.1080/01621459.1992.10475190 doi:\n10.1080/01621459.1992.10475190\nFrank, M. C. (2023a, November). Bridging the data gap between children and large\nlanguage models. Trends in Cognitive Sciences, 27(11), 990–992. Retrieved\n2024-04-24, fromhttps://www.cell.com/trends/cognitive-sciences/abstract/\nS1364-6613(23)00203-6 (Publisher: Elsevier) doi: 10.1016/j.tics.2023.08.007\nFrank, M. C. (2023b, November). Openly accessible LLMs can help us to understand\nhuman cognition. Nature Human Behaviour, 7(11), 1825–1827. Retrieved 2024-\nINSERT SHORTTITLE COMMAND IN PREAMBLE 35\n04-25, fromhttps://www.nature.com/articles/s41562-023-01732-4 (Publisher:\nNature Publishing Group) doi: 10.1038/s41562-023-01732-4\nFresen, A. J., Choenni, R., Heilbron, M., Zuidema, W., & de Heer Kloots, M. (2024). Lan-\nguage Models That Accurately Represent Syntactic Structure Exhibit Higher Rep-\nresentational Similarity To Brain Activity. In Proceedings of the Annual Meeting\nof the Cognitive Science Society (Vol. 46). Retrieved 2025-06-03, from https://\nescholarship.org/uc/item/1fp7m6nf\nGagl, B., & Gregorová, K. (2024). Investigating lexical categorization in reading based\non joint diagnostic and training approaches for language learners. npj Science of\nLearning, 9(1), 29.\nGagl, B., Gregorova, K., Golch, J., Hawelka, S., Sassenhagen, J., Tavano, A., ... Fiebach,\nC. J. (2022). Eye movements during text reading align with the rate of speech\nproduction. Nature human behaviour, 6(3), 429–442. (Publisher: Nature Publishing\nGroup UK London)\nGagl, B., Hawelka, S., & Wimmer, H. (2015). On sources of the word length effect in young\nreaders. Scientific Studies of Reading, 19(4), 289–306. (Publisher: Taylor & Francis)\nGagl, B., Richlan, F., Ludersdorfer, P., Sassenhagen, J., Eisenhauer, S., Gregorova, K.,\n& Fiebach, C. J. (2022). The lexical categorization model: A computational model\nof left ventral occipito-temporal cortex activation in visual word recognition.Plos\nComputational Biology, 18(6), e1009995. (Publisher: Public Library of Science San\nFrancisco, CA USA)\nGernsbacher, M. A. (1984). Resolving 20 years of inconsistent interactions between lexical\nfamiliarity and orthography, concreteness, and polysemy. Journal of Experimental\nPsychology: General, 113(2), 256–281. (Place: US Publisher: American Psychological\nAssociation) doi: 10.1037/0096-3445.113.2.256\nGeyken, A., & Hanneforth, T. (2006). TAGH: A Complete Morphology for German Based\non Weighted Finite State Automata. In A. Yli-Jyrä, L. Karttunen, & J. Karhumäki\n(Eds.), Finite-State Methods and Natural Language Processing(Vol. 4002, pp. 55–\n66). Berlin, Heidelberg: Springer Berlin Heidelberg. Retrieved 2023-10-16, from\nhttp://link.springer.com/10.1007/11780885_7 (Series Title: Lecture Notes in\nComputer Science) doi: 10.1007/11780885_7\nGraf, R., Nagler, M., & Jacobs, A. M. (2005). Faktorenanalyse von 57 Variablen der\nvisuellen Worterkennung.Zeitschrift für Psychologie/Journal of Psychology, 213(4),\n205–218. (Publisher: Hogrefe Verlag Göttingen)\nGrattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., ... Ma,\nZ. (2024, November). The Llama 3 Herd of Models.arXiv. Retrieved 2025-05-09,\nfrom http://arxiv.org/abs/2407.21783 (arXiv:2407.21783 [cs]) doi: 10.48550/\narXiv.2407.21783\nGregorova, K., Turini, J., Gagl, B., & Võ, M. L.-H. (2023). Access to meaning from\nvisual input: Object and word frequency effects in categorization behavior.Journal of\nExperimental Psychology: General. (Publisher: American Psychological Association)\nHallin, A. E., & Reuterskiöld, C. (2018, November). Effects of frequency\nand morphosyntactic structure on error detection, correction, and repetition in\nSwedish-speaking children. Applied Psycholinguistics, 39(6), 1189–1220. Re-\ntrieved 2023-05-16, fromhttps://www.cambridge.org/core/product/identifier/\nINSERT SHORTTITLE COMMAND IN PREAMBLE 36\nS0142716418000280/type/journal_article doi: 10.1017/S0142716418000280\nHawelka, S., Gagl, B., & Wimmer, H. (2010). A dual-route perspective on eye movements\nof dyslexic readers.Cognition, 115(3), 367–379. (Publisher: Elsevier)\nHawelka, S., Schuster, S., Gagl, B., & Hutzler, F. (2013). Beyond single syllables: the effect\nof first syllable frequency and orthographic similarity on eye movements during silent\nreading. Language and Cognitive processes, 28(8), 1134–1153. (Publisher: Taylor &\nFrancis)\nHeilbron, M., van Haren, J., Hagoort, P., & de Lange, F. P. (2021). Prediction and preview\nstrongly affect reading times but not skipping during natural reading.BioRxiv, 2021–\n10. (Publisher: Cold Spring Harbor Laboratory)\nHeilbron, M., van Haren, J., Hagoort, P., & de Lange, F. P. (2023, October). Lexical Pro-\ncessing Strongly Affects Reading Times But Not Skipping During Natural Reading.\nOpen Mind, 7, 757–783. Retrieved 2025-06-03, from https://doi.org/10.1162/\nopmi_a_00099 doi: 10.1162/opmi_a_00099\nHeister, J., Würzner, K.-M., Bubenzer, J., Pohl, E., Hanneforth, T., Geyken, A., & Kliegl,\nR. (2011). dlexDB – eine lexikalische Datenbank für die psychologische und linguis-\ntische Forschung.Psychologische Rundschau, 62(1), 10–20. Retrieved fromhttps://\ndoi.org/10.1026/0033-3042/a000029 (_eprint: https://doi.org/10.1026/0033-\n3042/a000029) doi: 10.1026/0033-3042/a000029\nHenrich, J., Heine, S. J., & Norenzayan, A. (2010). The weirdest people in the\nworld? Behavioral and brain sciences, 33(2-3), 61–83. Retrieved 2023-10-20, from\nhttps://www.cambridge.org/core/journals/behavioral-and-brain-sciences/\narticle/weirdest-people-inthe-world/BF84F7517D56AFF7B7EB58411A554C17\n(Publisher: Cambridge University Press)\nHofmann, M. J., Remus, S., Biemann, C., Radach, R., & Kuchinke, L. (2022, February).\nLanguage Models Explain Word Reading Times Better Than Empirical Predictability.\nFrontiers in Artificial Intelligence, 4, 730570. Retrieved 2023-05-16, fromhttps://\nwww.frontiersin.org/articles/10.3389/frai.2021.730570/full doi: 10.3389/\nfrai.2021.730570\nHosseini, E. A., Schrimpf, M., Zhang, Y., Bowman, S., Zaslavsky, N., & Fedorenko, E.\n(2022). Artificial neural network language models align neurally and behaviorally\nwith humans even after a developmentally realistic amount of training. BioRxiv,\n2022–10. Retrieved 2024-07-03, fromhttps://www.biorxiv.org/content/10.1101/\n2022.10.04.510681.abstract (Publisher: Cold Spring Harbor Laboratory)\nHu, J., & Frank, M. C. (2024, July). Auxiliary task demands mask the capabilities of\nsmaller language models. arXiv. Retrieved 2024-08-13, from http://arxiv.org/\nabs/2404.02418 (arXiv:2404.02418 [cs])\nHu, J., Mahowald, K., Lupyan, G., Ivanova, A., & Levy, R. (2024, January).Language mod-\nels align with human judgments on key grammatical constructions.arXiv. Retrieved\n2024-03-05, fromhttp://arxiv.org/abs/2402.01676 (arXiv:2402.01676 [cs])\nHuestegge, L., Radach, R., Corbic, D.,&Huestegge, S.M. (2009). Oculomotorandlinguistic\ndeterminants of reading development: A longitudinal study.Vision research, 49(24),\n2948–2959. (Publisher: Elsevier)\nHussain, Z., Binz, M., Mata, R., & Wulff, D. U. (2024, August). A tutorial on open-\nsource large language models for behavioral science. Behavior Research Methods.\nINSERT SHORTTITLE COMMAND IN PREAMBLE 37\nRetrieved 2024-08-20, fromhttps://doi.org/10.3758/s13428-024-02455-8 doi:\n10.3758/s13428-024-02455-8\nJurish, B., & Würzner, K.-M. (2013). Word and sentence tokenization with Hidden Markov\nModels. Journal for Language Technology and Computational Linguistics, 28(2), 61–\n83.\nKandpal, N., Deng, H., Roberts, A., Wallace, E., & Raffel, C. (2023, July). Large Lan-\nguage Models Struggle to Learn Long-Tail Knowledge. In Proceedings of the 40th\nInternational Conference on Machine Learning(pp. 15696–15707). PMLR. Retrieved\n2024-06-27, fromhttps://proceedings.mlr.press/v202/kandpal23a.html (ISSN:\n2640-3498)\nKasneci, E., Sessler, K., Küchemann, S., Bannert, M., Dementieva, D., Fischer, F., ... Kas-\nneci, G. (2023, April). ChatGPT for good? On opportunities and challenges of large\nlanguage models for education.Learning and Individual Differences, 103, 102274. Re-\ntrieved 2023-09-06, fromhttps://www.sciencedirect.com/science/article/pii/\nS1041608023000195 doi: 10.1016/j.lindif.2023.102274\nKeuleers, E., Brysbaert, M., & New, B. (2010). SUBTLEX-NL: A new measure for Dutch\nword frequency based on film subtitles.Behavior Research Methods, 42(3), 643–650.\nKeuleers, E., Stevens, M., Mandera, P., & Brysbaert, M. (2015, February). Word knowledge\nin the crowd: Measuring vocabulary size and word prevalence in a massive online\nexperiment. The Quarterly Journal of Experimental Psychology, 0(0), 1–28. Retrieved\n2015-05-04, from http://dx.doi.org/10.1080/17470218.2015.1022560 (00001)\ndoi: 10.1080/17470218.2015.1022560\nKliegl, R., Grabner, E., Rolfs, M., & Engbert, R. (2004). Length, frequency, and pre-\ndictability effects of words on eye movements in reading.European journal of cognitive\npsychology, 16(1-2), 262–284. (Publisher: Taylor & Francis)\nKorochkina, M., Marelli, M., Brysbaert, M., & Rastle, K. (2024, March). The children and\nyoung people’s books lexicon (cyp-lex): A large-scale lexical database of books read by\nchildren and young people in the united kingdom.Quarterly Journal of Experimental\nPsychology. Retrieved fromhttp://dx.doi.org/10.1177/17470218241229694 doi:\n10.1177/17470218241229694\nKorochkina, M., & Rastle, K. (2025). Morphology in children’s books, and what it means for\nlearning. npj Science of Learning, 10(1), 22. Retrieved 2025-05-16, fromhttps://\nwww.nature.com/articles/s41539-025-00313-6 (Publisher: Nature Publishing\nGroup UK London)\nKumarage, T., & Liu, H. (2023, August). Neural Authorship Attribution: Stylometric\nAnalysis on Large Language Models.arXiv. Retrieved 2023-09-28, fromhttp://arxiv\n.org/abs/2308.07305 (arXiv:2308.07305 [cs]) doi: 10.48550/arXiv.2308.07305\nLaarmann-Quante, R., Ortmann, K., Ehlert, A., Masloch, S., Scholz, D., Belke, E., &\nDipper, S. (2019, August). The Litkey Corpus: A richly annotated longitudinal\ncorpus of German texts written by primary school children.Behavior Research Meth-\nods, 51(4), 1889–1918. Retrieved 2023-09-19, from http://link.springer.com/\n10.3758/s13428-019-01261-x doi: 10.3758/s13428-019-01261-x\nLai, V. D., Ngo, N. T., Veyseh, A. P. B., Man, H., Dernoncourt, F., Bui, T., & Nguyen,\nT. H. (2023, April).ChatGPT Beyond English: Towards a Comprehensive Evaluation\nof Large Language Models in Multilingual Learning.arXiv. Retrieved 2024-04-24, from\nINSERT SHORTTITLE COMMAND IN PREAMBLE 38\nhttp://arxiv.org/abs/2304.05613 (arXiv:2304.05613 [cs])\nLiesenfeld, A., Lopez, A., & Dingemanse, M. (2023, July). Opening up ChatGPT: Track-\ning openness, transparency, and accountability in instruction-tuned text generators.\nIn Proceedings of the 5th International Conference on Conversational User Inter-\nfaces (pp. 1–6). New York, NY, USA: Association for Computing Machinery. Re-\ntrieved 2023-09-06, fromhttps://dl.acm.org/doi/10.1145/3571884.3604316 doi:\n10.1145/3571884.3604316\nLieven, E. (2010, November). Input and first language acquisition: Evaluating the role\nof frequency. Lingua, 120(11), 2546–2556. Retrieved 2023-05-16, from https://\nlinkinghub.elsevier.com/retrieve/pii/S0024384110001658 doi: 10.1016/j\n.lingua.2010.06.005\nLiu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., ... Ruan, C. (2024). Deepseek-\nv3 technical report. arXiv preprint arXiv:2412.19437. Retrieved 2025-05-09, from\nhttps://arxiv.org/abs/2412.19437\nLiu, Y., Cao, J., Liu, C., Ding, K., & Jin, L. (2024, February).Datasets for Large Language\nModels: A Comprehensive Survey.arXiv. Retrieved 2024-08-12, fromhttp://arxiv\n.org/abs/2402.18041 (arXiv:2402.18041 [cs])\nLopes Rego, A. T., Snell, J., & Meeter, M. (2024). Language models outperform cloze\npredictability in a cognitive model of reading.PLOS Computational Biology, 20(9),\ne1012117. Retrieved 2025-06-03, fromhttps://journals.plos.org/ploscompbiol/\narticle?id=10.1371/journal.pcbi.1012117 (Publisher: Public Library of Science\nSan Francisco, CA USA)\nMahowald, K., Ivanova, A. A., Blank, I. A., Kanwisher, N., Tenenbaum, J. B., & Fedorenko,\nE. (2024, March). Dissociating language and thought in large language models.Trends\nin Cognitive Sciences, 0(0). Retrieved 2024-04-24, from https://www.cell.com/\ntrends/cognitive-sciences/abstract/S1364-6613(24)00027-5 (Publisher: El-\nsevier) doi: 10.1016/j.tics.2024.01.011\nMarinus, E., & de Jong, P. F. (2010). Variability in the word-reading performance of\ndyslexic readers: Effects of letter length, phoneme length and digraph presence.Cor-\ntex, 46(10), 1259–1271. (Publisher: Elsevier)\nMartínez, G., Conde, J., Reviriego, P., & Brysbaert, M. (2024). Ai-generated estimates of\nfamiliarity, concreteness, valence, and arousal for over 100,000 spanish words.Quar-\nterly Journal of Experimental Psychology, 17470218241306694.\nMcCoy, R. T., Yao, S., Friedman, D., Hardy, M., & Griffiths, T. L. (2023, September).Em-\nbers of Autoregression: Understanding Large Language Models Through the Problem\nThey are Trained to Solve.arXiv. Retrieved 2024-05-15, from http://arxiv.org/\nabs/2309.13638 (arXiv:2309.13638 [cs])\nMcDonald, R., Nivre, J., Quirmbach-Brundage, Y., Goldberg, Y., Das, D., Ganchev, K.,\n... others (2013). Universal dependency annotation for multilingual parsing. InPro-\nceedings of the 51st Annual Meeting of the Association for Computational Linguistics\n(Volume 2: Short Papers)(pp. 92–97).\nMcDonald, S. A., & Shillcock, R. C. (2001, September). Rethinking the Word Fre-\nquency Effect: The Neglected Role of Distributional Information in Lexical Pro-\ncessing. Language and Speech, 44(3), 295–322. Retrieved 2023-05-16, from\nhttp://journals.sagepub.com/doi/10.1177/00238309010440030101 doi: 10\nINSERT SHORTTITLE COMMAND IN PREAMBLE 39\n.1177/00238309010440030101\nMeixner, J. M., Nixon, J. S., & Laubrock, J. (2022). The perceptual span is dynamically\nadjusted in response to foveal load by beginning readers.Journal of Experimental\nPsychology: General, 151(6), 1219.\nMichaelov, J. A., Bardolph, M. D., Van Petten, C. K., Bergen, B. K., & Coulson, S. (2024,\nApril). Strong Prediction: Language Model Surprisal Explains Multiple N400 Effects.\nNeurobiology of Language, 5(1), 107–135. Retrieved 2024-04-24, fromhttps://doi\n.org/10.1162/nol_a_00105 doi: 10.1162/nol_a_00105\nMin, B., Ross, H., Sulem, E., Veyseh, A. P. B., Nguyen, T. H., Sainz, O., ... Roth, D.\n(2021). Recent advances in natural language processing via large pre-trained language\nmodels: A survey.ACM Computing Surveys. (Publisher: ACM New York, NY)\nMomennejad, I., Hasanbeig, H., Vieira Frujeri, F., Sharma, H., Jojic, N.,\nPalangi, H., ... Larson, J. (2023, December). Evaluating Cognitive\nMaps and Planning in Large Language Models with CogEval. Advances in\nNeural Information Processing Systems , 36, 69736–69751. Retrieved 2024-\n04-24, from https://proceedings.neurips.cc/paper_files/paper/2023/hash/\ndc9d5dcf3e86b83e137bad367227c8ca-Abstract-Conference.html\nMonster, I., Tellings, A., Burk, W. J., Keuning, J., Segers, E., & Verhoeven,\nL. (2022, September). Word Properties Predicting Children’s Word Recog-\nnition. Scientific Studies of Reading , 26(5), 373–389. Retrieved 2023-04-18,\nfrom https://doi.org/10.1080/10888438.2021.2020795 (Publisher: Routledge\n_eprint: https://doi.org/10.1080/10888438.2021.2020795) doi: 10.1080/10888438\n.2021.2020795\nMuñoz-Ortiz, A., Gómez-Rodríguez, C., & Vilares, D. (2024, August). Contrasting Lin-\nguistic Patterns in Human and LLM-Generated Text. Artificial Intelligence Re-\nview, 57(10), 265. Retrieved 2024-08-29, fromhttp://arxiv.org/abs/2308.09067\n(arXiv:2308.09067 [cs]) doi: 10.1007/s10462-024-10903-2\nOh, B.-D., & Schuler, W. (2023). Why does surprisal from larger transformer-based\nlanguage models provide a poorer fit to human reading times? Transactions of\nthe Association for Computational Linguistics, 11, 336–350. Retrieved 2025-06-\n03, fromhttps://direct.mit.edu/tacl/article-abstract/doi/10.1162/tacl_a\n_00548/115371 (Publisher: MIT Press One Broadway, 12th Floor, Cambridge, Mas-\nsachusetts 02142, USA ...)\nOh, B.-D., & Schuler, W. (2025, August). Dissociable frequency effects attenuate as\nlarge language model surprisal predictors improve. Journal of Memory and Lan-\nguage, 143, 104645. Retrieved 2025-06-03, fromhttps://www.sciencedirect.com/\nscience/article/pii/S0749596X25000385 doi: 10.1016/j.jml.2025.104645\nOh, B.-D., Yue, S., & Schuler, W. (2024, February). Frequency Explains the Inverse\nCorrelation of Large Language Models’ Size, Training Data Amount, and Surprisal’s\nFit to Reading Times.arXiv. Retrieved 2024-08-30, fromhttp://arxiv.org/abs/\n2402.02255 (arXiv:2402.02255 [cs]) doi: 10.48550/arXiv.2402.02255\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., ... Lowe,\nR. (2022, March). Training language models to follow instructions with human\nfeedback. arXiv. Retrieved 2024-08-09, from http://arxiv.org/abs/2203.02155\n(arXiv:2203.02155 [cs]) doi: 10.48550/arXiv.2203.02155\nINSERT SHORTTITLE COMMAND IN PREAMBLE 40\nPerez, E., Huang, S., Song, F., Cai, T., Ring, R., Aslanides, J., ... Irving, G. (2022,\nFebruary). Red Teaming Language Models with Language Models.arXiv. Retrieved\n2024-07-11, fromhttp://arxiv.org/abs/2202.03286 (arXiv:2202.03286 [cs])\nPiantadosi, S. T. (2014, October). Zipf’s word frequency law in natural language: A\ncritical review and future directions.Psychonomic Bulletin & Review, 21(5), 1112–\n1130. Retrieved 2023-10-12, from https://doi.org/10.3758/s13423-014-0585-6\ndoi: 10.3758/s13423-014-0585-6\nRazeghi, Y., Logan IV, R. L., Gardner, M., & Singh, S. (2022, December). Impact\nof Pretraining Term Frequencies on Few-Shot Numerical Reasoning. In Y. Gold-\nberg, Z. Kozareva, & Y. Zhang (Eds.), Findings of the Association for Com-\nputational Linguistics: EMNLP 2022 (pp. 840–854). Abu Dhabi, United Arab\nEmirates: Association for Computational Linguistics. Retrieved 2024-04-24, from\nhttps://aclanthology.org/2022.findings-emnlp.59 doi: 10.18653/v1/2022\n.findings-emnlp.59\nRystrøm, J., Kirk, H. R., & Hale, S. (2025). Multilingual != multicultural: Evaluating\ngaps between multilingual capabilities and cultural alignment in llms.Retrieved from\nhttps://arxiv.org/abs/2502.16534\nSchnell, D. B., Stefan. (2021).Understanding Corpus Linguistics. London: Routledge. doi:\n10.4324/9780429269035\nSchroeder, S., Würzner, K.-M., Heister, J., Geyken, A., & Kliegl, R. (2015). childLex: A\nlexical database of German read by children.Behavior research methods, 47, 1085–\n1094. (Publisher: Springer)\nSchröter, P., & Schroeder, S. (2017). The Developmental Lexicon Project: A behavioral\ndatabase to investigate visual word recognition across the lifespan.Behavior Research\nMethods, 49, 2183–2203. (Publisher: Springer)\nSegbers, J., & Schroeder, S. (2017). How many words do children know? A corpus-based\nestimation of children’s total vocabulary size. Language Testing, 34(3), 297–320.\n(Publisher: Sage Publications Sage UK: London, England)\nSinghal, K., Azizi, S., Tu, T., Mahdavi, S. S., Wei, J., Chung, H. W., ... Natarajan, V.\n(2023, August). Large language models encode clinical knowledge.Nature, 620(7972),\n172–180. Retrieved 2023-09-06, from https://www.nature.com/articles/s41586\n-023-06291-2 (Number: 7972 Publisher: Nature Publishing Group) doi: 10.1038/\ns41586-023-06291-2\nStokes, S. F. (2010, June). Neighborhood Density and Word Frequency Predict Vocabulary\nSize in Toddlers. Journal of Speech, Language, and Hearing Research, 53(3), 670–\n683. Retrieved 2023-05-16, fromhttp://pubs.asha.org/doi/10.1044/1092-4388%\n282009/08-0254%29 doi: 10.1044/1092-4388(2009/08-0254)\nStraka, M., & Straková, J. (2017, August). Tokenizing, POS Tagging, Lemmatizing\nand Parsing UD 2.0 with UDPipe. In Proceedings of the CoNLL 2017 Shared\nTask: Multilingual Parsing from Raw Text to Universal Dependencies(pp. 88–99).\nVancouver, Canada: Association for Computational Linguistics. Retrieved from\nhttp://www.aclweb.org/anthology/K/K17/K17-3009.pdf\nTan, A. W. M., Yu, S., Long, B., Ma, W. A., Murray, T., Silverman, R. D., ... Frank,\nM. C. (2024, June).DevBench: A multimodal developmental benchmark for language\nlearning. arXiv. Retrieved 2024-08-13, from http://arxiv.org/abs/2406.10215\nINSERT SHORTTITLE COMMAND IN PREAMBLE 41\n(arXiv:2406.10215 [cs])\nTay, Y., Dehghani, M., Bahri, D., & Metzler, D. (2022).Efficient transformers: A survey.\nRetrieved fromhttps://arxiv.org/abs/2009.06732\nTellings, A., Hulsbosch, M., Vermeer, A., & Van den Bosch, A. (2014). BasiLex: An 11.5\nmillion words corpus of Dutch texts written for children.Computational Linguistics\nin the Netherlands Journal, 4, 191–208.\nTrott, S. (2024, January). Can large language models help augment English psycholinguistic\ndatasets? Behavior Research Methods. Retrieved2024-03-18, fromhttps://doi.org/\n10.3758/s13428-024-02337-z doi: 10.3758/s13428-024-02337-z\nTrott, S., Jones, C., Chang, T., Michaelov, J., & Bergen, B. (2023, July). Do Large\nLanguage Models Know What Humans Know? Cognitive Science, 47(7), e13309.\nRetrieved 2024-04-24, fromhttps://onlinelibrary.wiley.com/doi/10.1111/cogs\n.13309 doi: 10.1111/cogs.13309\nTuckute, G., Sathe, A., Srikant, S., Taliaferro, M., Wang, M., Schrimpf, M., ... Fedorenko,\nE. (2024, March). Driving and suppressing the human language network using large\nlanguage models. Nature Human Behaviour, 8(3), 544–561. Retrieved 2024-07-03,\nfrom https://www.nature.com/articles/s41562-023-01783-7 (Publisher: Na-\nture Publishing Group) doi: 10.1038/s41562-023-01783-7\nvan den Boer, M., de Jong, P. F., & Haentjens-van Meeteren, M. M. (2012). Lexical\ndecision in children: Sublexical processing or lexical search? Quarterly Journal of\nExperimental Psychology, 65(6), 1214–1228. (Publisher: SAGE Publications Sage\nUK: London, England)\nVan Heuven, W. J., Mandera, P., Keuleers, E., & Brysbaert, M. (2014). SUBTLEX-UK:\nA new and improved word frequency database for British English.Quarterly journal\nof experimental psychology, 67(6), 1176–1190. (Publisher: SAGE Publications Sage\nUK: London, England)\nVanmassenhove, E., Shterionov, D., & Gwilliam, M. (2021, January). Machine Trans-\nlationese: Effects of Algorithmic Bias on Linguistic Complexity in Machine Trans-\nlation. arXiv. Retrieved 2023-07-12, from http://arxiv.org/abs/2102.00287\n(arXiv:2102.00287 [cs]) doi: 10.48550/arXiv.2102.00287\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... Polo-\nsukhin, I. (2017). Attention Is All You Need. arXiv. Retrieved 2024-08-09,\nfrom http://arxiv.org/abs/1706.03762 (arXiv:1706.03762 [cs]) doi: 10.48550/\narXiv.1706.03762\nWarstadt, A., Mueller, A., Choshen, L., Wilcox, E., Zhuang, C., Ciro, J., ... Linzen,\nT. (2023). Findings of the BabyLM Challenge: Sample-efficient pretraining on de-\nvelopmentally plausible corpora. In Proceedings of the BabyLM Challenge at the\n27th Conference on Computational Natural Language Learning.Retrieved 2024-04-\n24, fromhttps://www.research-collection.ethz.ch/bitstream/handle/20.500\n.11850/650680/2/2023.conll-babylm.1.pdf\nWeekes, B. S., Castles, A. E., & Davies, R. A. (2006). Effects of consistency and age of\nacquisition on reading and spelling among developing readers.Reading and Writing,\n19, 133–169. (Publisher: Springer)\nWu, J., Yang, S., Zhan, R., Yuan, Y., Wong, D. F., & Chao, L. S. (2024, April).A Survey\non LLM-Generated Text Detection: Necessity, Methods, and Future Directions.arXiv.\nINSERT SHORTTITLE COMMAND IN PREAMBLE 42\nRetrieved 2024-08-29, fromhttp://arxiv.org/abs/2310.14724 (arXiv:2310.14724\n[cs]) doi: 10.48550/arXiv.2310.14724\nYarkoni, T., Balota, D., & Yap, M. (2008). Moving beyond Coltheart’s N: A new measure of\northographic similarity.Psychonomic bulletin & review, 15(5), 971–979. (Publisher:\nSpringer)\nZiegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei, D., ... Irving,\nG. (2020, January).Fine-Tuning Language Models from Human Preferences.arXiv.\nRetrieved 2024-07-11, fromhttp://arxiv.org/abs/1909.08593 (arXiv:1909.08593\n[cs, stat])\nZoccolotti, P., De Luca, M., Di Pace, E., Gasperini, F., Judica, A., & Spinelli, D. (2005).\nWordlengtheffectinearlyreadingandindevelopmentaldyslexia. Brain and language,\n93(3), 369–373. (Publisher: Elsevier)\nINSERT SHORTTITLE COMMAND IN PREAMBLE 43\nAppendix A\nExperiment 1\nWord frequency transformation\nThe histograms in Figure A1 show that\nlog\n(\n(1 +frequency) × 106\ncorpus_size\n)\n, (A1)\nis the most sensible transformation. Other options we considered included:\nlog\n(\n1 +frequency× 106\ncorpus_size\n)\n, −log\n(1 +frequency\ncorpus_size\n)\n, log (1 +frequency) (A2)\nThe first transformation of Equation A2 (third row in Figure A1) results in a lower\nbound of 0 and an upper bound of 1 for normalized word frequency between 0 and 1. The\nresulting distribution collapses these values on this narrow interval, resulting in a skewed\ndistribution.\nFurthermore, the transformations in Figure A2 show the effect of corpus size on\nthe transformed word frequency. For larger corpora, which include the child-directed LLM\ncorpora and SUBTLEX, the non-normalized transformation (log (1 +frequency)) results in\nthe strongest differences acrosss the corpora.\nFinally, the transformations in Figure A3 finally show the different handling of\nlow-frequency values.log\n(\n1 +frequency×106\ncorpus_size\n)\n, behaves differently from the other 3 transfor-\nmations by collapsing low word frequencies into a smaller range.\nNote that log (1 +frequency) was the transformation used by e.g. SUBTLEX\n(Brysbaert et al., 2011). This transformation avoids negative values, and assigns 0 to\nmissing words, similar to log\n(\n1 +frequency×106\ncorpus_size\n)\n. However, the latter does not distin-\nguish well enough between low-frequency words. Here, we are choosing a transformation\n(log\n((1+frequency)×106\ncorpus_size\n)\n) that does normalize for corpus size, given the large differences we\nhave here, see Figure A2. The negative values also do not show big gaps between low and\n0 word frequency values, see Figure A3.\nINSERT SHORTTITLE COMMAND IN PREAMBLE 44\nExp 1 Adults High Temp. Adults Low Temp. Children High Temp. Children Low Temp. childLex SUBTLEX\n−log((1 + frequency) * 1 / corpus_size)\nlog(1 + frequency))\nlog(1 + frequency * 10^6 / corpus_size)\nlog((1 + frequency) * 10^6 / corpus_size)\n0 5 10 15 0 5 10 15 0 5 10 15 0 5 10 15 0 5 10 15 0 5 10 15 0 5 10 15\n0.0\n0.1\n0.2\n0.3\n0.0\n0.1\n0.2\n0.3\n0.0\n0.1\n0.2\n0.3\n0.0\n0.1\n0.2\n0.3\nTransformed Frequency\nDensity\nFigure A1\nHistograms of transformed word frequency values. The x-axis shows the transformed word\nfrequency values, and the y-axis shows the density of the values. The facets show the different\ntransformations and the different corpora. The transformation in the third row results in a\nhard cut-off at 0 while the other 3 transformations result in more normal-like distributions.\nINSERT SHORTTITLE COMMAND IN PREAMBLE 45\nExp 1 Adults High Temp. Adults Low Temp. Children High Temp. Children Low Temp. childLex SUBTLEX\n0 5000 10000 150000 5000 10000 15000 0 5000 10000 15000 0 20000 40000 60000 0 20000 40000 60000 0 2500 5000 750010000 0 200004000060000\n0\n5\n10\n15\nFrequency\nTransformed Frequency\nTransformation Type − log((1 + frequency)*1 corpus_size) log(1 + frequency) log(1 + frequency*106 corpus_size) log((1 + frequency)*106 corpus_size)\nFigure A2\nScatter plots of transformed word frequency values. The facets show the different corpora.\nExp 1 Adults High Temp. Adults Low Temp. Children High Temp. Children Low Temp. childLex SUBTLEX\n0 10 20 30 40 50 0 10 20 30 40 50 0 10 20 30 40 50 0 10 20 30 40 50 0 10 20 30 40 50 0 10 20 30 40 50 0 10 20 30 40 50\n0\n5\n10\n15\nFrequency\nTransformed Frequency\nTransformation Type − log((1 + frequency)*1 corpus_size) log(1 + frequency) log(1 + frequency*106 corpus_size) log((1 + frequency)*106 corpus_size)\nFigure A3\nZoomed in version of Figure A2, showing only observed word frequencies smaller than 50.\nINSERT SHORTTITLE COMMAND IN PREAMBLE 46\nMost different words\nTable A1 shows the top words from the one corpus that appear the least often in\nthe other corpus.\nTable A1\nThe top frequent words that occur the least often in the other corpus, for all word lengths,\nand for words with more than 10 characters.\nchildLex childLex >10 LLM LLM >10\n1 Ganz Wahrscheinlich Jack Sattelschlepper\n2 Wieso widersprach Charaktere Mäusepension\n3 Wahrscheinlich blitzschnell Brownie Schulgespenst\n4 Bestimmt Snorkfräulein Brumm aufgewecktes\n5 Soll verächtlich Poppins unvergessliches\n6 verzog irgendeinem Zaubermaus akzeptierten\n7 kreischte anschließend Sattelschlepper Korallenschatz\n8 quer irgendjemand Fips Abschlussfeier\n9 presste Zaubereiministerium Hoppel verantwortungsvoll\n10 Meinst Entschuldige Mäusepension unzertrennliche\nINSERT SHORTTITLE COMMAND IN PREAMBLE 47\nLitkey\nWe redrew the scatter plot from the main text using a corpus containing texts\nwritten by children themselves (Laarmann-Quante et al., 2019) instead of text written for\nchildren (Schroeder et al., 2015). This corpus is much smaller, but the resulting figure, see\nFigure A4, shows the same pattern and the most differing words can be explained similarly\nas well.\nINSERT SHORTTITLE COMMAND IN PREAMBLE 48\nwegHand\nweil\nhat\nmacht\nFreunde\ndes\nihnen\nviele\nfinden\n0\n4\n8\n12\n0 4 8 12\nLitkey−based type frequency (ppm)\nLLM−based type frequency (ppm)\nCount\n1 3 10 30\nFigure A4\nThe same as Figure 3 in the main text, but showing Litkey-based type frequency (x-axis).\nThe pattern is similar, despite much less available data.\nINSERT SHORTTITLE COMMAND IN PREAMBLE 49\nRank differences\nFigure A5 zooms in on the specific differences between childLex and LLM curves\nshown in Figure 2 from the main text, similar to 2. Positive differences indicate a higher\nLLM word frequency, while negative differences indicate a lower LLM word frequency. Be-\nfore rank 1,000, LLM words are roughly used more often, while after rank 1,000, LLM words\naregenerallyusedlessoften. ThisfindingagainillustratesthattheLLMuseshigh-frequency\nwords more often and low-frequency words less often.\n1−10 10−100 100−1000 1000−max\n1 10 10 100 100 1000 1e+03 1e+04 1e+05\n−4\n−3\n−2\n−1\n0\n1\n0\n25\n50\n75\n−500\n0\n500\n1000\n1500\n2000\n10000\n20000\n30000\nRank\nFrequency\nFigure A5\nDifferences between word frequency calculated based on the LLM and childLex corpora (i.e.,\nthe difference between the curves from Figure 2).\nINSERT SHORTTITLE COMMAND IN PREAMBLE 50\nDeveL comparison\nOlder Adults Y ounger Adults\n3 4 6\nAverage across children Grade 1 2\n0 2 5 8 0 2 5 8\n0 2 5 8 0 2 5 8 0 2 5 8\n0 2 5 8 0 2 5 8 0 2 5 8\n2000\n3000\n700\n800\n1000\n2000\n3000\n4000\n800\n900\n1000\n500\n600\n700\n900\n1000\n2000\n900\n1000\n2000\n700\n800\n900\nWord type frequency: log((1 + frequency)*106 corpus_size)\nLexical decision time\nFrequency\nmeasure\nLLM Exp1\nChildLEX\nFigure A6\nRelation between log reaction times (ms) with LLM and childLex log-frequency.\nINSERT SHORTTITLE COMMAND IN PREAMBLE 51\nDeveL-subset correlation matrix\nFigure A7 shows the correlation matrix for the subset of 1052 words from Devel\n(Schröter & Schroeder, 2017).\nThe correlations between Experiment 1 LLM word frequency and reaction time\nranged between .31 (grade 1), to .52 (grade 6), while childLex word frequency correlated\nbetween .35 (grade 1) to .59 (grade 6), see Figure A7. Both childLex and LLM word\nfrequency correlated lower with adult reaction time (e.g., .48 and .39, respectively, for\nolder adults). The LLM-childLex correlation was .75 for this subset of words, which is\nconsiderably larger than the correlation for the complete corpora (which was .69; see 6).\nThe correlations for the Experiment 2 ChLT (the reproduced corpus) and the ChHT\ncorpus were almost identical (maximally .02 difference). Both adult-directed corpora had\nlower correlations across all grades (.04 - .07 lower). Correlations with adult reaction times\nwere similar across the new conditions (maximally .01 difference for older adult reaction\ntimes). The correlations for the child-directed corpora also did not increase compared to\nExperiment 1 and were also comparable across conditions (maximally .02 difference), which\nwe also found for the correlations between complete corpora.\nFor Experiment 3, we found the highest correlation between the average children’s\nRT’s and LLM-based word frequency for the DeepSeek short corpus (r = .61) compared to\nthe other LLM word frequency measures (r between .53 and .43).\nTogether, the correlations show similar differences compared to the effect sizes ob-\ntained from regressions with covariates (see Figures 4 and 7).\nINSERT SHORTTITLE COMMAND IN PREAMBLE 52\n1.00 0.700.68 0.590.74 0.570.550.79 0.60 0.50 0.68 0.59 −0.17 −0.35 −0.40−0.37 −0.38 −0.45 −0.51 −0.46−0.24 −0.17 −0.24 −0.17 −0.09 −0.05\n0.70 1.000.98 0.900.77 0.930.920.73 0.80 0.78 0.89 0.87 −0.28 −0.39 −0.45−0.43 −0.44 −0.49 −0.45 −0.40−0.28 −0.28 −0.23 −0.16 −0.07 −0.02\n0.68 0.981.00 0.900.76 0.930.930.73 0.79 0.77 0.89 0.87 −0.28 −0.38 −0.43−0.42 −0.43 −0.48 −0.44 −0.39−0.28 −0.27 −0.22 −0.15 −0.06 −0.02\n0.59 0.900.90 1.000.75 0.930.930.71 0.77 0.82 0.87 0.90 −0.31 −0.43 −0.49−0.47 −0.48 −0.52 −0.45 −0.39−0.26 −0.37 −0.20 −0.14 −0.04 −0.00\n0.74 0.770.76 0.751.00 0.760.750.81 0.78 0.80 0.72 0.74 −0.35 −0.54 −0.59−0.56 −0.56 −0.59 −0.54 −0.48−0.39 −0.50 −0.35 −0.26 −0.14 −0.07\n0.57 0.930.93 0.930.76 1.000.990.67 0.81 0.87 0.87 0.90 −0.35 −0.46 −0.52−0.49 −0.50 −0.53 −0.44 −0.39−0.30 −0.42 −0.25 −0.19 −0.08 −0.03\n0.55 0.920.93 0.930.75 0.991.000.66 0.80 0.86 0.86 0.90 −0.35 −0.45 −0.50−0.48 −0.49 −0.52 −0.43 −0.39−0.29 −0.41 −0.25 −0.18 −0.08 −0.03\n0.79 0.730.73 0.710.81 0.670.661.00 0.65 0.59 0.71 0.68 −0.24 −0.41 −0.47−0.45 −0.47 −0.52 −0.55 −0.45−0.35 −0.29 −0.25 −0.14 −0.01 0.06\n0.60 0.800.79 0.770.78 0.810.800.65 1.00 0.86 0.76 0.77 −0.35 −0.48 −0.53−0.51 −0.51 −0.53 −0.46 −0.42−0.35 −0.43 −0.31 −0.24 −0.12 −0.06\n0.50 0.780.77 0.820.80 0.870.860.59 0.86 1.00 0.74 0.83 −0.40 −0.56 −0.61−0.59 −0.58 −0.58 −0.46 −0.43−0.31 −0.53 −0.34 −0.31 −0.23 −0.18\n0.68 0.890.89 0.870.72 0.870.860.71 0.76 0.74 1.00 0.91 −0.29 −0.38 −0.44−0.43 −0.43 −0.48 −0.46 −0.39−0.25 −0.27 −0.19 −0.13 −0.04 0.01\n0.59 0.870.87 0.900.74 0.900.900.68 0.77 0.83 0.91 1.00 −0.36 −0.47 −0.53−0.51 −0.52 −0.54 −0.45 −0.41−0.28 −0.39 −0.25 −0.20 −0.10 −0.05\n−0.17 −0.28−0.28 −0.31−0.35 −0.35−0.35−0.24 −0.35 −0.40 −0.29 −0.36 1.00 0.79 0.780.70 0.61 0.57 0.37 0.350.65 0.50 0.67 0.61 0.38 0.25\n−0.35 −0.39−0.38 −0.43−0.54 −0.46−0.45−0.41 −0.48 −0.56 −0.38 −0.47 0.79 1.00 0.960.86 0.80 0.75 0.59 0.550.57 0.56 0.71 0.65 0.48 0.35\n−0.40 −0.45−0.43 −0.49−0.59 −0.52−0.50−0.47 −0.53 −0.61 −0.44 −0.53 0.78 0.96 1.000.94 0.90 0.85 0.67 0.620.55 0.59 0.70 0.65 0.49 0.36\n−0.37 −0.43−0.42 −0.47−0.56 −0.49−0.48−0.45 −0.51 −0.59 −0.43 −0.51 0.70 0.86 0.941.00 0.83 0.79 0.64 0.580.50 0.56 0.66 0.62 0.47 0.35\n−0.38 −0.44−0.43 −0.48−0.56 −0.50−0.49−0.47 −0.51 −0.58 −0.43 −0.52 0.61 0.80 0.900.83 1.00 0.81 0.67 0.620.45 0.55 0.59 0.56 0.43 0.31\n−0.45 −0.49−0.48 −0.52−0.59 −0.53−0.52−0.52 −0.53 −0.58 −0.48 −0.54 0.57 0.75 0.850.79 0.81 1.00 0.72 0.640.40 0.54 0.50 0.47 0.35 0.25\n−0.51 −0.45−0.44 −0.45−0.54 −0.44−0.43−0.55 −0.46 −0.46 −0.46 −0.45 0.37 0.59 0.670.64 0.67 0.72 1.00 0.670.30 0.38 0.40 0.38 0.31 0.24\n−0.46 −0.40−0.39 −0.39−0.48 −0.39−0.39−0.45 −0.42 −0.43 −0.39 −0.41 0.35 0.55 0.620.58 0.62 0.64 0.67 1.000.32 0.33 0.39 0.36 0.27 0.22\n−0.24 −0.28−0.28 −0.26−0.39 −0.30−0.29−0.35 −0.35 −0.31 −0.25 −0.28 0.65 0.57 0.550.50 0.45 0.40 0.30 0.321.00 0.30 0.68 0.42 0.09 −0.06\n−0.17 −0.28−0.27 −0.37−0.50 −0.42−0.41−0.29 −0.43 −0.53 −0.27 −0.39 0.50 0.56 0.590.56 0.55 0.54 0.38 0.330.30 1.00 0.27 0.23 0.13 0.05\n−0.24 −0.23−0.22 −0.20−0.35 −0.25−0.25−0.25 −0.31 −0.34 −0.19 −0.25 0.67 0.71 0.700.66 0.59 0.50 0.40 0.390.68 0.27 1.00 0.88 0.69 0.55\n−0.17 −0.16−0.15 −0.14−0.26 −0.19−0.18−0.14 −0.24 −0.31 −0.13 −0.20 0.61 0.65 0.650.62 0.56 0.47 0.38 0.360.42 0.23 0.88 1.00 0.86 0.71\n−0.09 −0.07−0.06 −0.04−0.14 −0.08−0.08−0.01 −0.12 −0.23 −0.04 −0.10 0.38 0.48 0.490.47 0.43 0.35 0.31 0.270.09 0.13 0.69 0.86 1.00 0.92\n−0.05 −0.02−0.02 −0.00−0.07 −0.03−0.030.06 −0.06 −0.18 0.01 −0.05 0.25 0.35 0.360.35 0.31 0.25 0.24 0.22−0.06 0.05 0.55 0.71 0.92 1.00\nSUBTLEX\nDWDS\nChildLex\nOLD20\nAoA\nLetter Count\nUnigram\nBigram\nTrigram\nAdult low temp.\nAdult high temp.\nChild low temp.\nChild high temp.\nDeepseek long\nDeepSeek short\nLLama long\nLLama short\nExp. 1\nGrade 1 RT\nGrade 2 RT\nGrade 3 RT\nGrade 4 RT\nGrade 6 RT\nAverage child RT\nYA RT\nOA RT\nSUBTLEX\nDWDSChildLexOLD20\nAoA\nLetter Count\nUnigramBigramTrigram\nAdult low temp.Adult high temp.Child low temp.Child high temp.Deepseek longDeepSeek short\nLLama longLLama short\nExp. 1\nGrade 1 RTGrade 2 RTGrade 3 RTGrade 4 RTGrade 6 RT\nAverage child RT\nYA RTOA RT\nPearson\nCorrelation\n−0.5\n0.0\n0.5\n1.0\nFigure A7\nCorrelation matrix for the Devel subset of words for which reaction times are available. The\ncorrelations with corpus-based word frequency measures are based on alog\n((1+frequency)×106\ncorpus_size\n)\ntransformation. These corpora include: SUBTLEX, DWDS, childLex, as well as the LLM\ncorpora.\nINSERT SHORTTITLE COMMAND IN PREAMBLE 53\nEffect sizes\nTable A2\nEffect size estimates per grade based on the complete set DeveL words.\nEstimate SE t p Adj. R2\nGrade 1\nLLM Freq. -27.1 6.1 -4.4 < 0.001 0.6363\nchildLex -25.1 11.4 -2.2 0.028 0.6269\nGrade 2\nLLM Freq. -30.9 3.0 10.4 < 0.001 0.6916\nchildLex -50.0 5.1 9.8 < 0.001 0.6886\nGrade 3\nLLM Freq. -24.4 1.8 13.6 < 0.001 0.6613\nchildLex -37.0 3.1 11.8 < 0.001 0.6495\nGrade 4\nLLM Freq. -17.9 1.3 -13.8 < 0.001 0.5861\nchildLex -27.7 2.3 -12.2 < 0.001 0.5734\nINSERT SHORTTITLE COMMAND IN PREAMBLE 54\nEffect sizes\nTable A3\nEffect size estimates per grade based on a subset of the DeveL data that excludes words\nwith 0 occurrences in the LLM corpus.\nEstimate SE t p Adj. R2\nGrade 1\nLLM Freq. -33.7 7.3 4.6 < 0.001 0.6321\nchildLex -22.1 12.0 1.8 0.066 0.6198\nGrade 2\nLLM Freq. -34.3 3.3 10.3 < 0.001 0.6874\nchildLex -48.1 5.3 9.1 < 0.001 0.681\nGrade 3\nLLM Freq. -25.6 2.0 12.7 < 0.001 0.6562\nchildLex -33.7 3.2 10.4 < 0.001 0.6404\nGrade 4\nLLM Freq. -19.6 1.5 13.3 < 0.001 0.5699\nchildLex -26.3 2.4 11.1 < 0.001 0.55\nINSERT SHORTTITLE COMMAND IN PREAMBLE 55\nRobustness analysis\nTo estimate the robustness of the correlation between LLM and childLex word fre-\nquency, we re-estimated LLM word frequency based on subsets from the complete corpus,\nspecifically for the words used in the DeveL dataset (Schröter & Schroeder, 2017). For this\nanalysis, we started with the first LLM texts generated based on a prompt using the first\nbook. After that, we successively added the texts from the next book and re-estimated the\ncorrelations for each subset (see Figure A8A). The curve shows a logarithmic increase in\ncorpus similarity, indicating a substantial correlation increase of the two measures within\nthe first 50 texts. After that, the increase in similarity is weaker, ending up at a correlation\ncoefficient of just below .75. We tested whether the increase in similarity is indeed loga-\nrithmic. For this, we compared two correlations: The correlation between all correlation\nquotients (i.e., see y-axis in Figure A8A) and (i) the numbers of texts (i.e., see x-axis in\nFigure A8A) or (ii) the logarithm of the numbers of texts. The computed correlation for the\nlinear number was lower (r = .71) than the correlation based on the logarithmic numbers\n(r = .94), indicating that the increase is indeed logarithmic (Figure A8B also reports this\ncorrelation of .94 in the top left corner). The analysis shows that word frequency would not\nhave been very different if we had used only half of the LLM corpus. It turns out that the\ncorrelation seems to keep increasing, but that it slows down with more text.\nWe also evaluated to what extent word frequency measures based on corpus subsets\nresult in a lower model fit. Thus, we increased the corpus size (same as above), estimated\nthe word frequency, and measured the model fit change when introducing the established\nword frequency measures compared to a model without the measure. We implemented this,\nstarting with the texts from the first book. Ultimately, we can compare the absolute AIC\ndifference values (i.e., AIC from the model with the new word frequency predictor minus the\nAIC based on the model without the predictor; the higher, the better the model fit) from all\nestimated word frequencies and all reading groups (1st-4th Grade, 6th Grade, younger and\nolder adults; N tests = 500; see Figure A8C). We rely on model comparison methods using\nlinear regression models and the AIC, a measure optimal to investigate model fit change\nfor newly introduced parameters. Note that a change in three AIC points is a significant\nmodel fit increase (i.e., the black horizontal line in Figure A8C and D; all AICs above show\na significant increase in model fit).\nThis partial corpus analysis shows that variation in reading performance can be\nexplained better based on word frequency measures based on larger corpora (Figure A8C),\nbut that performance increases less, the larger the corpora. We find an increase in the AIC\ndifference in all age groups, although the trend is very small in the youngest readers from\nGrade 1 (compare correlations of the size of the corpus - N - and Grade 1 - G1 - in contrast\nto the more older readers in Figure A8B). Nonetheless, all analyses, except for the word\nfrequency measures based on very small corpora (Number of books < 5) predicting the\nreaction times of Grade 1 readers, showed a significant increase in model fit. Correlations\nof AIC differences with increasing corpus size of all our groups of readers also showed that\nin Grade 1, the pattern differed from all other groups (Figure A8B). In addition, groups\nwith similar age (e.g., Grade 2 vs. Grade 3) had higher correlations (r range from .96 to\n.99) compared to comparisons with substantial age differences (e.g., Grade 2 vs. old adults;\nr = .81). Finally, the partial analysis that estimated the AIC differences against a baseline\nINSERT SHORTTITLE COMMAND IN PREAMBLE 56\nmodel including childLex word frequency as a predictor, showed that only linear models\nthat predicted the reaction times of young readers (Grade 1-6) had an additional model fit\nincrease based on the inclusion of LLM word frequency (see Figure A8D). Models describing\nadult data did not benefit from introducing LLM word frequency. Also, we observed that\nfor young readers, larger corpus sizes are needed for the word frequency measurement to\nproduce a measure with higher descriptive power (Grade 1: 6 books; 2: 103; 3: 59; 4: 95;\n6: 121; see Figure A8D).\nIn the first analysis of reading performance, we found that the model fit of RTs\nincreased when including a word frequency measure based on only a fraction of the full\nLLM corpus (N< 10 texts) for all age groups. Nonetheless, increasing the corpus size also\nincreased the model fit further. For all but Grade 1 readers, the increase in model fit with\na larger corpus was highly similar to the increase in the correlation between LLM- and\nchildLex-based corpora described above (r range: .86 - .94). The difference for Grade 1 was\nthat model fit analysis peaked after about 10% of the corpus, with an incremental decrease\nof model fit for word frequencies based on larger corpora (i.e., when N> 100).\nSimilarly, we compared the model fit increase for LLM word frequency based on\npartial corpora to a model that already included childLex word frequency. Here, we inves-\ntigated whether the new LLM word frequency explains variance over and above traditional\nword frequency measures. Most notably, this was the case for the Grade 1 readers after\nonly a fraction of the texts included (N< 10). For Grades 2-6 readers, an increased model\nfit was found when the LLM corpus size was much larger (Grade 2: N> 100; Grade 3: N\n> 50; Grade 4: N> 100; Grade 6: N> 120). No additional variance was explained when\nchildLex word frequency was already included for the two adult groups.\nINSERT SHORTTITLE COMMAND IN PREAMBLE 57\nFigure A8\nPartial LLM corpus analysis. (A) Subset-based estimation of the correlation between LLM\ntype frequency and childLex type frequency (y-axis). We estimated the correlation 500 times\nbased on an LLM word frequency extracted from a corpus that included only the first book.\nAfter that, we included all generated text from one additional book until all books were in-\ncluded. Note that this analysis was implemented for the words used in the DeveL dataset.\n(B) Pearson correlation matrix investigating the partial data curves from A and C. r rep-\nresents the correlations from A; N represents the log-transformed number of stories (i.e.,\nx-axis from A, C, or D); G1-6, represent the AIC differences from Grade 1 to 6 shown in\nC; YA and OA represent the younger and older adults AIC differences shown in C. Lower\nmatrix shows the correlation coefficient r and the upper matrix the color coded (blue: pos-\nitive correlation; white: no correlation; red: negative correlation) correlation silhouettes\n(narrow silhouettes indicate high and wide silhouettes indicate low correlations). (C) AIC\ndifferences for linear mixed models of the DeveL reaction times. The models included word\nfrequency measures estimated on a subset of the LLM corpus (i.e., same subsets as in A),\nwhich were compared to the model without a word frequency measure included, and (D) when\nthe baseline model included childLex word frequency.\nINSERT SHORTTITLE COMMAND IN PREAMBLE 58\nAppendix B\nExperiment 2\nRank comparison\n0.1\n1\n5\n10\n50\n100\n500\n1000\n5000\n10000\n50000\n1 5 10 50 100 500 1k 5k 10k 50k\nRank\nFrequency\nCorpus name\nAdults High Temperature\nAdults Low Temperature\nChildLex\nChildren High Temperature\nChildren Low Temperature\nFigure B1\nZipf’s law plot showing a stronger negative slope for the LLM corpus compared to childLex.\nThe slopes are fitted to words with a word frequency between 10 and 10,000.\nINSERT SHORTTITLE COMMAND IN PREAMBLE 59\nWord frequency histograms\nFigure B2 shows histograms of word frequency distributions for all words as well as\nfor the subset of DeveL words only. We included words from three LLM corpora as well as\nchildLex. We grouped low word frequencies together to improve the readability of the plot.\nThe word frequency of the words in DeveL does not follow the natural distribution of\nword frequency as displayed in the left panel (i.e., a high number of low-frequency words and\na low number of high-frequency words). Instead, the DeveL word frequency follows a normal\ndistribution (when ignoring the lowest frequency cases), containing relatively many medium\nto high-frequency words. This strategy is reasonable for child experiments as one wants to\nsample from as many frequency ranges as possible in a limited number of trials, with words\nthat can be expected to be known by young readers. Furthermore, the right panel shows\nthat a relatively large number of DeveL words are not present in the LLM corpora; see the\ndifference in the histogram at zero between the LLM and ChildLex corpora.\nThere are also general differences between the LLM and childLex corpora. Across\nall words, there are more childLex words with low frequencies, with a transition at around\n20 words per million (at this point, the bars for childLex are not higher anymore). In the\nDeveL subset, there are more low LLM-based frequencies, with a transition at around 7\nwords per million. These difference indicate that the LLMs tends to use some different\nwords that are not in the DeveL selection in places where it could have used DeveL word.\n0e+001e−01\n1e+00\n1e+01\n1e+02\n1e+03\n1e+04\n0 3 6 9\nWord frequency per million (log scale)\nNumber of Words\nA\n0\n50\n100\n150\n200\n0 2 4 6 8\nWord frequency per million (log scale)\nNumber of DeveL words\nB\nMeasure\nChild Exp. 1\nChildLex\nChild low temp.\nChild high temp.\nFigure B2\nDistributions of word frequency based on childLex and three LLM corpora for all words (left)\nDeveL words only (right). For clarity, we grouped values with a log frequency per million\nthat was lower than 0.\nINSERT SHORTTITLE COMMAND IN PREAMBLE 60\nCorpus overlap\nFigure B3 shows overlap between the corpora, either for all words that were gener-\nated (left), or for the selection of words from the DeveL subset of words (right). Overlap\nwith DeveL is almost 100%. Only four words from DeveL are missing in childLex (99.7%\nare overlapping), while 15 words are missing in ChHT (98.7%).\n100%\n61%\n40%\n55%\n64%\n58%\n72%\n100%\n45%\n58%\n69%\n64%\n73%\n71%\n100%\n69%\n81%\n77%\n29%\n26%\n20%\n100%\n33%\n29%\n49%\n45%\n33%\n47%\n100%\n56%\n57%\n54%\n41%\n53%\n73%\n100%\nChildLex\nChild high temp.\nChild low temp.\nAdult high temp.\nAdult low temp.\nExp. 1\nChildLex\nChild high temp.Child low temp.Adult high temp.Adult low temp.\nExp. 1\nOverlap (%)\n20 40 60 80 100\nA\n100%\n97.5%\n96.4%\n99.9%\n99.8%\n99%\n98.4%\n100%\n96.4%\n99.8%\n99.7%\n99%\n98%\n97%\n100%\n99.8%\n99.6%\n99.1%\n95.5%\n94.5%\n93.9%\n100%\n98.8%\n97.3%\n96.3%\n95.3%\n94.6%\n99.7%\n100%\n98.2%\n97.1%\n96.2%\n95.7%\n99.9%\n99.8%\n100%\n95.2%\n94.4%\n93.8%\n99.7%\n98.7%\n97%\nChildLex\nChild high temp.\nChild low temp.\nAdult high temp.\nAdult low temp.\nExp. 1\nDeveLChildLex\nChild high temp.Child low temp.Adult high temp.Adult low temp.\nExp. 1\nOverlap (%)\n94 96 98 100\nB\nFigure B3\nOverlap measured as percentage of retrieved words from one of both corpora. Overlap was\ndefined as a word frequency > 0 for both corpora. The overlap percentages were computed\nby taking the number of words shared between the corresponding corpus listed on the left and\nbottom and dividing that number by the total corpus size of the corpus listed on the bottom\nrow. Panel A shows percentages for complete corpora while panel B shows overlap for the\nDeveL selection of words only.\nINSERT SHORTTITLE COMMAND IN PREAMBLE 61\nType-token comparison\n0e+00 2e+07 4e+07\n0 40000 80000 120000\nAdults low temperature\nSample size\nNumber of types (V)\n0e+00 2e+07 4e+07\n0 40000 80000 120000\nAdults high temperature\nSample size\nNumber of types (V)\n0e+00 2e+07 4e+07\n0 40000 80000 120000\nChildren low temperature\nSample size\nNumber of types (V)\n0e+00 2e+07 4e+07\n0 40000 80000 120000\nChildren high temperature\nSample size\nNumber of types (V)\nFigure B4\nSimilar to Figure 1 (the type-token growth curve from the main text), these type-token\ngrowth curves show the dependency of the total number of unique types (y-axis) on inter-\nand extrapolated sample sizes (x-axis) for both adult-directed corpora (top), and for both\nchild-directed corpora (bottom).\nINSERT SHORTTITLE COMMAND IN PREAMBLE 62\nWord frequency comparison across corpora\nsagtedann\nweiß\nKopf\neuch\nGeschichte\nvoller Welt\nStadt\nspürte\njetzt\ndoch\nDann\nKopf\ngar\nTiere\nvoller\nFreunde\nWald\nHöhle\nKopf\nsagte\nweiß\ndann\nganz\nGeschichte\nvoller\nStadt Welt\nspürte\ndoch\nKopf\nDann\ngar\nmich\nTiere Freunde\nvoller\nWald\nHöhle\nChildren High Temperature Children Low Temperature\nAdults High Temperature Adults Low Temperature\n0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0\n0\n4\n8\n0\n4\n8\nChildLex−based (6−12 y/o children) type frequency (ppm)\nLLM−based type frequency (ppm)\nCount\n1 100 10000\nFigure B5\nScatterplots between LLM type frequency (y-axis) and childLex type frequency (x-axis; dark\ngray line) for all four corpora from Experiment 2. The labels show the top five differences\non both sides (x-y and y-x). The color gradient of the dots represents the number of data\npoints each dot represents.\nINSERT SHORTTITLE COMMAND IN PREAMBLE 63\nHigh-frequency words comparison\nsagte\ndann\nweiß\nKopf\neuch\nGeschichte\nvoller\nWelt\nStadt\nspürte\njetzt\ndochDannKopf\ngar\nTiere\nvoller\nFreunde\nWald\nHöhle\nKopf\nsagte\nweiß\ndann\nganz\nGeschichte\nvoller\nStadt\nWelt\nspürte\ndoch\nKopf\nDann\ngar\nmich\nTiere\nFreunde\nvoller\nWald\nHöhle\nChildren High Temperature Children Low Temperature\nAdults High Temperature Adults Low Temperature\n5 6 7 8 9 10 5 6 7 8 9 10\n5\n6\n7\n8\n9\n10\n5\n6\n7\n8\n9\n10\nChildLex−based (6−12 y/o children) type frequency (ppm)\nLLM−based type frequency (ppm)\nCount\n1 100 10000\nFigure B6\nZoomed in version of B5\nINSERT SHORTTITLE COMMAND IN PREAMBLE 64\nSplit half reliability\nTo estimate the reliability of the frequency measurement based on LLM corpora,\nwe estimated the split-half reliability (see Figure B7). We correlated LLM word frequency\nfrom a random selection of texts based on half of the book titles with the texts based\non the other half of the book titles and repeated this 100 times. The correlations are\nthus based on correlating corpora that are based on completely different book titles with\neach other. Correlations range from an average of .895 for adults low temperature to\n.908 for the ChHT corpus (averaging across 100 random splits of 2x250 books). These\ncorrelations are higher compared to the LLM-childLex correlation, which was .62. The\nresult, therefore, shows that the word frequency tables remain very similar across different\nbook titles relative to childLex. Furthermore, correlations were higher in the high compared\nto the low temperature conditions, and they were higher in the child-directed compared to\nthe adult-directed corpora, the latter likely due to the larger corpus size. Combined, this\nanalysis showed that measuring word frequency based on LLM corpora is highly reliable\noverall.\nINSERT SHORTTITLE COMMAND IN PREAMBLE 65\n0.890\n0.895\n0.900\n0.905\n0.910Correlation\nSource\nAdult high temp.\nAdult low temp.\nChild high temp.\nChild low temp.\nFigure B7\nSplit half reliability estimates. Box plots show 100 split-half Pearson correlations, indicating\nthe reliability of the log. word frequency measurement for each of the newly generated\ncorpora. Boxes and whiskers represent the distribution of the data, with the box spanning\nthe interquartile range (IQR), which captures the middle 50% of the data, and the whiskers\nextend to 1.5 times the IQR, indicating variability outside the upper and lower quartiles.\nOutliers beyond this range are displayed as individual points.\nINSERT SHORTTITLE COMMAND IN PREAMBLE 66\nDistributions of POS tags across types and tokens\nTo get insight into the distribution of syntactal categories across corpora, we looked\nat the distribution of POS tags. We made use of UDPipe’s Universal Dependencies (UD)\npart-of-speech (POS) tagging scheme. Note that their NA tag is used when there is no clear\ncategory for a particular word.\nFigure B8 shows that LLM text contains a higher proportion of nouns vs verbs\nin both types and tokens compared to childLex. This indicates the use of more distinct\nnouns and more frequent noun use. In addition, childLex also has relatively more distinct\nadjectives and also more frequent adjective use. Thus, the increased lexical richness in\nchildLex seems to be mostly due to the use of more distinct adjectives and verbs. The\ndifferences in the numbers of noun tokens suggest that LLM text is less action-focused and\nmore descriptive.\nChild high temp. tokens ChildLex tokens\nChild high temp. types ChildLex types\n0 250000 500000 750000 0 200000 400000 600000 800000\n0 10000 20000 30000 40000 0 20000 40000 60000\nDETPRONADPNOUNVERBCCONJADVAUXADJPROPNSCONJPARTNUMXINTJPUNCTSYMNA\nDETPRONADPNOUNVERBCCONJADVAUXADJPROPNSCONJPARTNUMXINTJPUNCTSYMNA\nFrequency\nPOS Tag\nFigure B8\nDistribution of POS tags in childLex and LLM corpora based on tokens (bottom panels) and\ntypes (top panels). The ordering of all panels follows the order of childLex tokens.\nINSERT SHORTTITLE COMMAND IN PREAMBLE 67\nDistributions of POS tags across types and tokesn for the Devel subset of words\nonly\nFigure B9 shows that the distributions of tokens are largely similar, indicating the\nsimilarity between LLM and childLex frequency for the DeveL subset of words. The largest\ndifference seems to be related to use of adverbs and adjectives. The top right panel shows\nwhat word types are included in DeveL and the top left panel shows which of these word\ntypes have a distinctively low frequency in the ChHT corpus. Note that UDPipe misclas-\nsified many of the proper nouns as nouns, so the blue bar is not necessarily informative.\nThus, the distribution of DeveL types that are missing in the ChHT corpus does not seem\nvery different from the actual distribution of types in Devel. For specific details about the\nselection criteria of DeveL, see the original DeveL paper (Schröter & Schroeder, 2017).\nChild high temp. tokens ChildLex tokens\nDeveL types missing in Child high temp. DeveL types\n0 20000 40000 0 10000 20000 30000 40000 50000\n0 20 40 0 200 400\nNOUN\nVERB\nADJ\nPROPN\nADV\nADP\nX\nPRON\nAUX\nNOUN\nVERB\nADJ\nPROPN\nADV\nADP\nX\nPRON\nAUX\nFrequency\nPOS Tag\nFigure B9\nDistribution of POS tags for the subset of DeveL words that are also in the childLex and\nLLM corpora as based on tokens (bottom panels) and types (top panels). The ordering of\nall panels follows the order of childLex tokens again.\nINSERT SHORTTITLE COMMAND IN PREAMBLE 68\nDistributions of word embeddings after dimension reduction\nFigure B10 illustrates some syntactical and semantic differences between the cor-\npora. For example, it shows that the distribution of word embeddings of words that are\nmore prevalent in childLex includes a separate semantic area with mostly verbs (see bottom\nright corner in the bottom right panel) while words that are more prevalent to the LLM\ncorpus (we selected the ChHT corpus for this analysis) includes a separate area with mostly\nproper nouns.\naufsehenerregendsten\nQuinte\nPolarforschers\nGrauenvollesreinquatschen\nAshe\nStichwortsammlung\nTobak\nSatelitten\nJuneau\nSmaug\nTata\nAmins\nverquollene\nSuchmeldungen\nverpasse\nkaukasischen\nChiswick\nZodiac\nlanganhaltenden\nStandesunterschiedeRank\nSewerDevane\nhinstellte\nBrexit\nGZSZ\nToilettenartikel\nobsessiv\nDullin\nPilotenbrille\nchine\nKahlen\nNolan\nHerausforderungen\nveranstalten\numarmten\nEmilia\nAbenteurer\nfortan\ndurchsuchten\nIllustrationen\nVielfalt\nSpeedy\nLegenden\nMia\nLuzie\nFreda\nNora\nPaula\nzwitscherten\nMaxi\nOrte\nMaja Mutig\nkluge\nVorfreude\nOh\nRosi\nJuwelen\nGolo\nstaunten\nLeser\nerkundeten\nRudi\nLinus\nWelli\nPille\nConni erforschen\nJonas\nverzauberten\nMurks\nBella\nCuriosity\nImperativs\nPampers\nFlugreisen\nRado\nHabour\nVerteidigungsministers\nNeubauprojekt\nniedersinkt\nSieder\ncries\nArbeitsreisen\nEberhart\nVendetta\nlebenslustig\nNr\nModefotos\nsensorische\nLudwig\nwuschig\nHandscanner\nWartezimmern\nLovers\nGEBR\nagil\nHyperkeratose\ndying\nVallons\nZugunruhe\nGuanGasts\nAntarctica\nAndheri\noffensichtlich\nMach\nrechten\nWoher\nentgegnete\nschwieg\nSekunden\nblinzelte\nmurmelt\nZunge\nrasch\nscharf\nEinenzehn\nantwortet\nSobald\nmeint\nHat\nDing\nHaben\nTon\nLeidhastig\nandern\nehe\n0\nvorn\ngestern\nFingern\nGott\nHab\nwischte\nUnsere\ndrang\nSchuld\nWieso\nrunzelte\nsenkte\ntot\nGleich\nDad\nverlegen\nTut\nraus\nPunkt\nVersuch\nLLM > 50, ChildLex < 10 ChildLex > 50, LLM < 10\nLLM only ChildLex only\nPOS\na\na\na\na\na\na\na\na\na\na\na\na\na\nADJ\nADP\nADV\nAUX\nCCONJ\nINTJ\nNOUN\nNUM\nPRON\nPROPN\nSCONJ\nVERB\nX\nFigure B10\nDistributions of word embeddings using dimensionality reduction with UMAP to word vectors\nfrom FastText-German. Embeddings were reduced from 300-dimensional vectors. Color\ncoding corresponds to different parts-of-speech (POS) tags as based on UDPipe’s \"german-\nhdt\" model. The figure shows four different selections of words based on their occurrence\nand frequency in childLex and ChHT.\nINSERT SHORTTITLE COMMAND IN PREAMBLE 69\nWords not in childLex or LLM corpus\nTables B2, B1, B4, and B3 show the top frequent words that occur in only one of\nboth corpora, for all word lengths, and for words with more than 10 characters.\nTable B1\nWords not in Childlex or in the AdLT corpus\nchildLex F childLex >10 F AdLT F AdLT >10 F\ndaß 6.1 widersprach 3.6 Max 7.6 Charakteren 4.5\n1 5.1 SternenClan 3.5 Anna 7.1 Schulvampire 4.4\nWieso 4.8 Olchi-Kinder 3.5 Lena 6.5 einzustehen 4.3\nGleich 4.4 kopfschüttelnd 3.1 Lisa 6.4 nahegelegenen 4.3\nguckte 4.3 vorwurfsvoll 3.0 Müller 6.3 thematisiert 4.2\nBestimmt 4.3 Viertelstunde 3.0 Mia 6.1 unvergessliche 4.1\nmußte 4.2 augenblicklich 2.9 Emma 5.8 kindgerechte 4.0\nJedenfalls 4.2 Mottenflügel 2.9 Paul 5.7 einfühlsame 3.9\nguckt 4.2 Anscheinend 2.9 Harry 5.7 Protagonisten 3.9\nkriegt 4.2 stirnrunzelnd 2.9 Julia 5.6 faszinierendes 3.9\nTable B2\nWords not in Childlex or in the AdHT corpus\nchildLex F childLex >10 F AdHT F AdHT >10 F\ndaß 6.1 SternenClan 3.5 Max 7.3 Charakteren 4.6\n1 5.1 Olchi-Kinder 3.5 Anna 6.9 nahegelegenen 4.3\nguckte 4.3 einigermaßen 3.0 Lena 6.3 einzustehen 4.2\nBestimmt 4.3 Viertelstunde 3.0 Lisa 6.1 unvergessliche 4.1\nBlattsee 4.2 Mottenflügel 2.9 Mia 6.1 thematisiert 4.1\nmußte 4.2 stirnrunzelnd 2.9 Müller 6.1 Schulvampire 4.1\nJedenfalls 4.2 Wolkenpfote 2.8 Paul 5.8 Protagonisten 4.0\nguckt 4.2 ausnahmsweise 2.8 Emma 5.8 kindgerechte 3.9\nkriegt 4.2 Ausgerechnet 2.7 Julia 5.7 herzerwärmende 3.8\nDad 4.2 verschränkte 2.7 Harry 5.6 faszinierendes 3.7\nINSERT SHORTTITLE COMMAND IN PREAMBLE 70\nTable B3\nWords not in Childlex or in the ChLT corpus\nchildLex F childLex >10 F ChLT F ChLT >10 F\ndaß 6.1 Olchi-Kinder 3.5 Max 8.3 nahegelegenen 5.0\n1 5.1 Wohnungstür 3.1 Mia 7.5 Schulvampire 4.5\njedenfalls 4.8 einigermaßen 3.0 Lina 7.0 unvergessliche 4.2\nguckte 4.3 Viertelstunde 3.0 Lena 6.7 Sternenfohlen 4.1\nmußte 4.2 Mottenflügel 2.9 Emma 6.6 einzustehen 4.1\nJedenfalls 4.2 stirnrunzelnd 2.9 Tim 6.5 Inselschüler 4.0\nkriegt 4.2 entgeistert 2.8 Felix 6.4 Schafgäääng 3.9\nmuß 4.1 Wolkenpfote 2.8 Paul 6.3 KuchenMonster 3.9\n0 4.1 Unglaublich 2.8 Lilli 6.2 SkaterBande 3.7\nwär 4.1 ausnahmsweise 2.8 Finn 6.1 abenteuerlustiger 3.5\nTable B4\nWords not in Childlex or in the ChHT corpus\nchildLex F childLex >10 F ChHT F ChHT >10 F\ndaß 6.1 Olchi-Kinder 3.5 Max 8.1 nahegelegenen 4.9\n1 5.1 unwillkürlich 3.1 Mia 7.3 Schulvampire 4.4\nmußte 4.2 Wohnungstür 3.1 Lina 6.7 unvergessliche 4.2\nJedenfalls 4.2 Viertelstunde 3.0 Lena 6.6 Sternenfohlen 4.1\nkriegt 4.2 Mottenflügel 2.9 Emma 6.6 Inselschüler 4.0\nmuß 4.1 stirnrunzelnd 2.9 Tim 6.6 einzustehen 4.0\n0 4.1 ausnahmsweise 2.8 Felix 6.4 Schafgäääng 3.8\nwär 4.1 Ausgerechnet 2.7 Paul 6.3 KuchenMonster 3.8\nandern 4.1 verständnislos 2.7 Finn 6.2 SkaterBande 3.7\nwußte 3.9 Premierminister 2.7 Anna 6.2 abenteuerlustiger 3.4\nINSERT SHORTTITLE COMMAND IN PREAMBLE 71\nTop frequency words occurring the least often in childLex or LLM corpus\nTables B6, B5, B8, and B7 show the top frequent words that occur the least often\nin the other corpus, for all word lengths, and for words with more than 10 characters.\nTable B5\nThe top frequent words that occur the least often in the other corpus, for all word lengths,\nand for words with more than 10 characters for the AdLT corpus\nchildLex F childLex >10 F AdLT F AdLT >10 F\nwenigstens 4.9 Hoffentlich 4.4 unterhaltsame 5.6 unterhaltsame 5.6\nEigentlich 4.7 Schnupferich 3.6 lehrreiche 5.0 unschlagbares 4.3\nHoffentlich 4.4 Zehenspitzen 3.3 Charaktere 5.0 Perspektiven 3.6\nvorhin 4.2 verächtlich 3.1 Bindung 4.9 Korallenschatz 3.5\nirgendwas 4.1 einigermaßen 3.0 Teamwork 4.5 vielfältige 3.4\nblöd 4.1 Pfannkuchen 2.9 unschlagbares 4.3 Sattelschlepper 3.4\nkreischte 3.9 entgeistert 2.8 humorvolle 4.0 unvergesslicher 3.4\nMist 3.8 Ausgerechnet 2.7 Jack 3.9 weiterzuentwickeln 3.4\nMehr 3.8 Mittlerweile 2.7 zeitlose 3.8 Mäusepension 3.3\nKlo 3.8 verschränkte 2.7 Überwinden 3.8 authentisch 3.3\nTable B6\nThe top frequent words that occur the least often in the other corpus, for all word lengths,\nand for words with more than 10 characters for the AdHT corpus\nchildLex F childLex >10 F AdHT F AdHT >10 F\njedenfalls 4.8 Wahrscheinlich 4.6 unterhaltsame 5.4 unterhaltsame 5.4\nWahrscheinlich 4.6 Hoffentlich 4.4 Charaktere 5.0 unschlagbares 4.0\nflüstert 4.4 Unglaublich 2.8 lehrreiche 4.8 Perspektiven 3.7\nHoffentlich 4.4 verständnislos 2.7 Bindung 4.8 vielfältige 3.6\nQuatsch 4.1 Premierminister 2.7 Teamwork 4.3 Mäusepension 3.5\nhockte 4.0 Seidenschnabel 2.6 humorvolle 4.1 Sattelschlepper 3.3\nDem 3.9 Offensichtlich 2.6 unschlagbares 4.0 unzertrennliche 3.3\nIhm 3.8 Erstklässler 2.6 zeitlose 3.9 Bedrohungen 3.3\nMeinst 3.8 Großmutters 2.6 Ermittler 3.8 vielschichtigen 3.3\ngucken 3.8 irgendwelchen 2.5 Poppins 3.7 Korallenschatz 3.3\nINSERT SHORTTITLE COMMAND IN PREAMBLE 72\nTable B7\nThe top frequent words that occur the least often in the other corpus, for all word lengths,\nand for words with more than 10 characters for the ChLT corpus\nchildLex F childLex >10 F ChLT F ChLT >10 F\nwenigstens 4.9 Augenbrauen 3.7 unschlagbares 4.6 unschlagbares 4.6\nhockte 4.0 SternenClan 3.5 Brumm 4.2 unzertrennliche 3.9\nDem 3.9 unwillkürlich 3.1 unzertrennliche 3.9 Sattelschlepper 3.8\nIhm 3.8 augenblicklich 2.9 Sattelschlepper 3.8 unvergessliches 3.8\nAugenbrauen 3.7 Treppenhaus 2.9 unvergessliches 3.8 aufgewecktes 3.6\nImmerhin 3.7 Einverstanden 2.7 Charaktere 3.8 Mäusepension 3.6\nnachher 3.6 Vorbeigehen 2.6 Poppins 3.8 unvergesslicher 3.5\nDeswegen 3.6 Offensichtlich 2.6 Holle 3.8 Korallenschatz 3.5\nAußer 3.5 Krankenflügel 2.6 aufgewecktes 3.6 Schulgespenst 3.4\nSternenClan 3.5 irgendwelchen 2.5 Zaubermaus 3.6 unterhaltsame 3.3\nTable B8\nThe top frequent words that occur the least often in the other corpus, for all word lengths,\nand for words with more than 10 characters for the ChHT corpus\nchildLex F childLex >10 F ChHT F ChHT >10 F\nNeunauge 4.0 Anscheinend 2.9 unschlagbares 4.5 unschlagbares 4.5\nMeinst 3.8 entgeistert 2.8 Brumm 4.0 unzertrennliche 3.9\nMehr 3.8 Mittlerweile 2.7 unzertrennliche 3.9 unvergessliches 3.8\nMadam 3.7 verschränkte 2.7 Charaktere 3.8 Sattelschlepper 3.7\nMum 3.6 Großmutters 2.6 Poppins 3.8 aufgewecktes 3.6\nwomöglich 3.5 gerunzelter 2.5 unvergessliches 3.8 unvergesslicher 3.5\nAußer 3.5 vorsichtshalber 2.5 Sattelschlepper 3.7 Schulgespenst 3.5\ngekriegt 3.5 umständlich 2.3 Holle 3.7 Mäusepension 3.5\nWozu 3.5 Donnerwetter 2.3 Bindung 3.6 Korallenschatz 3.5\nwerd 3.4 unvermittelt 2.3 aufgewecktes 3.6 unterhaltsame 3.3\nINSERT SHORTTITLE COMMAND IN PREAMBLE 73\nAppendix C\nExperiment 3\nType-token comparison\n0.0e+00 1.0e+07 2.0e+07\n0 50000 150000\nLLama long\nSample size\nNumber of types (V)\n0.0e+00 1.0e+07 2.0e+07\n0 50000 150000\nLLama short\nSample size\nNumber of types (V)\n0.0e+00 1.0e+07 2.0e+07\n0 50000 150000\nDeepSeek long\nSample size\nNumber of types (V)\n0.0e+00 1.0e+07 2.0e+07\n0 50000 150000\nDeepSeek short\nSample size\nNumber of types (V)\nFigure C1\nSimilar to Figures 1 and B4, these type-token growth curves show the dependency of the\ntotal number of unique types (y-axis) on inter- and extrapolated sample sizes (x-axis) for\nboth Llama (top), and DeepSeek corpora (bottom).\nINSERT SHORTTITLE COMMAND IN PREAMBLE 74\nCorpus overlap\nFigure C2 shows overlap between the Experiment 3 corpora, similar to Figure B3.\nThe left panel shows overlap for all generated words and the right panels shows overlap for\nthe selection of DeveL words only. Overlap with DeveL is almost 100%. 14 DeveL words are\nmissing in DeepSeek long (98.7%). For example, the DeepSeek long corpus did not contain\nwords like Apfelsine, Automatik, Fasching (out-of-fashion words for orange, automatic, and\nCarnival respectively).\n100%\n68%\n71%\n61%\n55%\n51%\n20%\n100%\n39%\n28%\n19%\n17%\n14%\n26%\n100%\n22%\n14%\n12%\n30%\n47%\n55%\n100%\n26%\n27%\n50%\n57%\n65%\n49%\n100%\n48%\n58%\n65%\n69%\n63%\n61%\n100%\nChildLex\nDeepSeek long\nDeepSeek short\nLLama long\nLlama short\nExp. 1\nChildLex\nDeepSeek longDeepSeek short\nLLama longLlama short\nExp. 1\nOverlap (%)\n25 50 75 100\nA\n100%\n99.8%\n99.6%\n98.5%\n93.7%\n95%\n93.9%\n100%\n98.8%\n96.9%\n90.3%\n91.6%\n94.6%\n99.6%\n100%\n97.5%\n90.9%\n92.1%\n95.5%\n99.8%\n99.6%\n100%\n92%\n93.3%\n97.5%\n99.9%\n99.7%\n98.7%\n100%\n96.9%\n97.5%\n99.9%\n99.6%\n98.8%\n95.6%\n100%\n93.8%\n99.7%\n98.8%\n96.7%\n90.1%\n91.3%\nChildLex\nDeepSeek long\nDeepSeek short\nLLama long\nLlama short\nExp. 1\nDeveLChildLex\nDeepSeek longDeepSeek short\nLLama longLlama short\nExp. 1\nOverlap (%)\n92.5 95.0 97.5100.0\nB\nFigure C2\nOverlap measured as percentage of retrieved words from one of both corpora. Overlap was\ndefined as a word frequency > 0 for both corpora. The overlap percentages were computed\nby taking the number of words shared between the corresponding corpus listed on the left and\nbottom and dividing that number by the total corpus size of the corpus listed on the bottom\nrow. Panel A shows percentages for complete corpora while panel B shows overlap for the\nDeveL selection of words only.\nINSERT SHORTTITLE COMMAND IN PREAMBLE 75\nLexical Richness\nPredicted number of types in hypothetical 10-million-token corpora.\n0\n250000\n500000\n750000\nExp. 1\nLlama shortLLama long\nAdult low temp.Child low temp.Adult high temp.Child high temp.\nSUBTLEX\nDeepSeek short\nchildLex\nDeepSeek long\nCorpus\nPredicted Types at 10M Tokens\nFigure C3\nBased on the estimated growth curves in Figures 1, B4, and C1, we determined the predicted\nnumber of types for a hyphotetical 10-million-token corpus. The order of types turned out\nto be the same as the ordering based on the percentage of hapax tokens.\nINSERT SHORTTITLE COMMAND IN PREAMBLE 76\nWords not in childLex or LLM corpus\nTables C1, C2, C3, and C4 show the top frequent words that occur in only one of\nboth corpora, for all word lengths, and for words with more than 10 characters.\nTable C1\nWords not in Childlex or in the DS-long corpus\nchildLex F childLex >10 F DS-long F DS-long >10 F\n1 5.1 Brombeerkralle 3.8 Lina 7.6 Protagonist 4.3\nBlattsee 4.2 Olchi-Kinder 3.5 Max 6.9 Protagonistin 4.2\nmußte 4.2 einigermaßen 3.0 Lena 6.7 ausarbeiten 3.7\nmuß 4.1 Anschließend 3.0 Tom 6.4 Protagonisten 3.6\n0 4.1 Anscheinend 2.9 Mia 6.3 Algorithmus 3.3\nNeunauge 4.0 ausnahmsweise 2.8 Oh 6.3 Algorithmen 3.2\nBrombeerkralle 3.8 Mittlerweile 2.7 Ben 6.2 Postskriptum 3.2\nT-Shirt 3.5 verständnislos 2.7 Leo 6.2 Nebenfiguren 3.1\nOlchi-Kinder 3.5 Premierminister 2.7 Finn 6.0 inspirierte 3.0\nbißchen 3.4 Tagespropheten 2.7 Emma 5.9 einzustehen 3.0\nTable C2\nWords not in Childlex or in the DS-Short corpus\nchildLex F childLex >10 F DS-Short F DS-Short >10 F\ndaß 6.1 Brombeerkralle 3.8 Lina 8.7 Leseanfänger 4.9\n1 5.1 Olchi-Kinder 3.5 Lena 7.9 Mississippi 4.0\nallmählich 4.3 einigermaßen 3.0 Max 7.8 Selberlesen 4.0\nBlattsee 4.2 Viertelstunde 3.0 Ben 7.6 Schmuddelfing 3.9\nmußte 4.2 Anschließend 3.0 Tom 7.5 selbstgebastelten 3.8\nmuß 4.1 Mottenflügel 2.9 Finn 7.1 Scherzfragen 3.2\n0 4.1 eindringlich 2.9 Leo 7.0 KuchenMonster 3.2\nNeunauge 4.0 Anscheinend 2.9 Mia 6.9 SkaterBande 3.1\nwußte 3.9 stirnrunzelnd 2.9 Paul 6.4 ParkSheriffs 3.1\nBrombeerkralle 3.8 entgeistert 2.8 Tim 6.3 Sonnenschule 3.1\nINSERT SHORTTITLE COMMAND IN PREAMBLE 77\nTable C3\nWords not in Childlex or in the Llama-long corpus\nchildLex F childLex >10 F Llama-long F Llama-long >10 F\ndaß 6.1 Wahrscheinlich 4.6 Tim 7.7 Selbstreflexion 5.2\neben 5.8 Hoffentlich 4.4 Max 7.6 inspirierte 4.9\nGanz 5.1 Taschenbier 4.0 Lena 7.3 Initiativen 4.9\n1 5.1 ausgerechnet 4.0 Müller 6.4 Nachhaltigkeit 4.8\nsowieso 5.0 Brombeerkralle 3.8 Timmy 6.3 Überzeugungen 4.4\nallerdings 5.0 Hosentasche 3.7 Leo 6.2 Empfehlungen 4.3\nselber 5.0 Schnupferich 3.6 Emma 6.0 kulturellen 4.3\nEigentlich 4.7 anscheinend 3.5 Harry 5.5 nachhaltigen 4.2\nWahrscheinlich 4.6 Zeigefinger 3.5 Epilog 5.4 unvergessliche 4.1\nStimmt 4.5 SternenClan 3.5 Anna 5.4 inspirierten 4.0\nTable C4\nWords not in Childlex or in the Llama-short corpus\nchildLex F childLex >10 F Llama-short F Llama-short >10 F\ndaß 6.1 Wahrscheinlich 4.6 Max 8.5 nahegelegenen 5.0\nNun 5.9 Taschenbier 4.0 Tim 8.3 ParkSheriffs 4.1\nGanz 5.1 Brombeerkralle 3.8 Lena 8.2 unvergessliche 4.0\n1 5.1 Hosentasche 3.7 Leo 7.1 Schulvampire 3.9\nallerdings 5.0 Schnupferich 3.6 Müller 6.9 KuchenMonster 3.9\noffenbar 5.0 anscheinend 3.5 Emma 6.8 Inselschüler 3.9\nselber 5.0 Zeigefinger 3.5 Timmy 6.7 PicknickEssen 3.8\nwenigstens 4.9 SternenClan 3.5 Felix 6.5 abenteuerlustiger 3.8\ndeutete 4.9 mittlerweile 3.5 Anna 6.1 Schafgäääng 3.8\nrasch 4.9 Olchi-Kinder 3.5 Lilli 6.1 Mississippi 3.7\nINSERT SHORTTITLE COMMAND IN PREAMBLE 78\nTop frequency words occurring the least often in childLex or LLM corpus\nTables C5, C6, C7, and C8 show the top frequent words that occur the least often\nin the other corpus, for all word lengths, and for words with more than 10 characters.\nTable C5\nThe top frequent words that occur the least often in the other corpus, for all word lengths,\nand for words with more than 10 characters for the DS-long corpus\nchildLex F childLex >10 F DS-long F DS-long >10 F\ndaß 6.1 Viertelstunde 3.0 Twist 5.0 vergessenes 4.3\nwußte 3.9 entgeistert 2.8 Optionen 4.7 flüsternden 3.7\nerkundigte 3.7 Kinderstube 2.6 Charaktere 4.4 Menschlichkeit 3.6\nFeuerstern 3.6 Einzelheiten 2.5 vergessenes 4.3 weiterspinnen 3.3\nwisperte 3.5 Treppenabsatz 2.5 Größerem 4.1 Erweiterung 3.1\nHemul 3.1 hergekommen 2.4 Nachhall 3.9 Vergessenen 3.1\nDaran 3.1 Detektivtagebuch 2.3 Teamwork 3.8 Perspektiven 3.0\nViertelstunde 3.0 Vorderpfoten 2.3 Fokus 3.8 Koordinaten 3.0\nAllmählich 3.0 Umkleideraum 2.2 flüsternden 3.7 mysteriöser 3.0\nsogleich 2.9 auszumachen 2.2 Dialogen 3.7 Gemeinschaften 2.9\nTable C6\nThe top frequent words that occur the least often in the other corpus, for all word lengths,\nand for words with more than 10 characters for the DS-Short corpus\nchildLex F childLex >10 F DS-Short F DS-Short >10 F\nselber 5.0 Staubfinger 3.3 Schneider 4.9 Detektivgeschichte 3.5\nandern 4.1 aufgebracht 3.1 Teamwork 4.3 überlisteten 3.3\nhöchst 3.5 kopfschüttelnd 3.1 Brumm 4.1 flüsternden 3.3\nStaubfinger 3.3 Wohnungstür 3.1 Weber 4.0 Mäusepension 3.1\nweshalb 3.3 Angelegenheit 3.0 Hoppel 4.0 Herbstabend 3.1\nHastig 3.3 augenblicklich 2.9 Poppins 3.8 Sattelschlepper 3.0\nWenigstens 3.2 ausnahmsweise 2.8 Detektivgeschichte 3.5 mysteriöser 3.0\nVermutlich 3.2 Tagespropheten 2.7 Holle 3.5 Kristallhöhle 2.9\nCharlotte 3.2 möglicherweise 2.7 Kitz 3.4 regnerischer 2.9\naufgebracht 3.1 Vorbeigehen 2.6 überlisteten 3.3 Superkicker 2.9\nINSERT SHORTTITLE COMMAND IN PREAMBLE 79\nTable C7\nThe top frequent words that occur the least often in the other corpus, for all word lengths,\nand for words with more than 10 characters for the Llama-short corpus\nchildLex F childLex >10 F Llama-short F Llama-short >10 F\neben 5.8 Augenbrauen 3.7 Jack 4.8 unzertrennliche 3.9\nEigentlich 4.7 widersprach 3.6 Brumm 4.4 Mäusepension 3.7\nbeinahe 4.4 blitzschnell 3.4 Poppins 4.2 unvergessliches 3.5\naußerdem 4.4 aufgebracht 3.1 Holle 4.1 Sattelschlepper 3.5\nglaub 4.4 Gleichzeitig 3.1 Zaubermaus 4.0 unvergesslicher 3.4\nweder 4.3 erstaunlich 3.0 unzertrennliche 3.9 verantwortungsvoll 3.4\nungefähr 4.3 Angelegenheit 3.0 Benz 3.9 Piratenschreck 3.3\nguckt 4.2 umklammerte 3.0 Ravensburg 3.8 Wunschzauber 3.3\nfurchtbar 4.2 Jackentasche 3.0 Cleverness 3.8 abenteuerlustiges 3.2\nDad 4.2 hintereinander 2.8 Erinner 3.8 Geheimschwein 3.1\nTable C8\nThe top frequent words that occur the least often in the other corpus, for all word lengths,\nand for words with more than 10 characters for the Llama-long corpus\nchildLex F childLex >10 F Llama-long F Llama-long >10 F\nwenigstens 4.9 durcheinander 4.0 nachhaltige 5.5 nachhaltige 5.5\njedenfalls 4.8 unauffällig 3.3 Motivation 5.2 Gemeinschaften 4.9\nWieso 4.8 Normalerweise 3.3 Gemeinschaften 4.9 Technologien 4.6\nzischte 4.7 protestierte 3.2 lokalen 4.8 Perspektiven 4.6\nKlar 4.5 verächtlich 3.1 Ära 4.8 Selbstlosigkeit 4.2\nschreit 4.4 irgendeinem 3.1 Jack 4.6 unterstützende 4.1\nDafür 4.3 unwillkürlich 3.1 Technologien 4.6 Unterstützen 3.9\nKaum 4.3 Treppenhaus 2.9 Perspektiven 4.6 weiterzuentwickeln 3.8\nguckt 4.2 Schrottplatz 2.9 Charaktere 4.6 authentisch 3.7\nDad 4.2 schlagartig 2.9 Bindung 4.5 Zukunftsaussichten 3.6",
  "topic": "German",
  "concepts": [
    {
      "name": "German",
      "score": 0.8387069702148438
    },
    {
      "name": "Linguistics",
      "score": 0.7001239061355591
    },
    {
      "name": "Word (group theory)",
      "score": 0.5984329581260681
    },
    {
      "name": "Computer science",
      "score": 0.5932705402374268
    },
    {
      "name": "Word lists by frequency",
      "score": 0.46233484148979187
    },
    {
      "name": "Natural language processing",
      "score": 0.4571346044540405
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3943057358264923
    },
    {
      "name": "Sentence",
      "score": 0.06104367971420288
    },
    {
      "name": "Philosophy",
      "score": 0.0602191686630249
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I180923762",
      "name": "University of Cologne",
      "country": "DE"
    }
  ]
}