{
    "title": "Masked Language Modeling and the Distributional Hypothesis: Order Word Matters Pre-training for Little",
    "url": "https://openalex.org/W3152698349",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2769217912",
            "name": "Koustuv Sinha",
            "affiliations": [
                "McGill University"
            ]
        },
        {
            "id": "https://openalex.org/A2476502706",
            "name": "Robin Jia",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2576916730",
            "name": "Dieuwke Hupkes",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2092672596",
            "name": "Joelle Pineau",
            "affiliations": [
                "McGill University"
            ]
        },
        {
            "id": "https://openalex.org/A2609376944",
            "name": "Adina Williams",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A302325417",
            "name": "Douwe Kiela",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2933138175",
        "https://openalex.org/W3174169056",
        "https://openalex.org/W2963120843",
        "https://openalex.org/W2964086180",
        "https://openalex.org/W3118485687",
        "https://openalex.org/W2964013315",
        "https://openalex.org/W131533222",
        "https://openalex.org/W3154376977",
        "https://openalex.org/W2986889180",
        "https://openalex.org/W2093483153",
        "https://openalex.org/W2981985696",
        "https://openalex.org/W3132711698",
        "https://openalex.org/W2396767181",
        "https://openalex.org/W2922523190",
        "https://openalex.org/W1983578042",
        "https://openalex.org/W3113529090",
        "https://openalex.org/W3098613713",
        "https://openalex.org/W2147152072",
        "https://openalex.org/W2934842096",
        "https://openalex.org/W3104235057",
        "https://openalex.org/W2907849599",
        "https://openalex.org/W2970648593",
        "https://openalex.org/W2790235966",
        "https://openalex.org/W3004346089",
        "https://openalex.org/W2996728628",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W1597993529",
        "https://openalex.org/W2962843521",
        "https://openalex.org/W2964165804",
        "https://openalex.org/W2963571341",
        "https://openalex.org/W2130158090",
        "https://openalex.org/W3184516261",
        "https://openalex.org/W1484082930",
        "https://openalex.org/W2891308403",
        "https://openalex.org/W2552110825",
        "https://openalex.org/W2964117978",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W2117130368",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W2932893307",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W4301785137",
        "https://openalex.org/W2946417913",
        "https://openalex.org/W3173812697",
        "https://openalex.org/W2963430224",
        "https://openalex.org/W2949952668",
        "https://openalex.org/W4294170691",
        "https://openalex.org/W2882319491",
        "https://openalex.org/W3034775979",
        "https://openalex.org/W3034763191",
        "https://openalex.org/W2971044268",
        "https://openalex.org/W3176449742",
        "https://openalex.org/W3168685366",
        "https://openalex.org/W3031914912",
        "https://openalex.org/W2169818249",
        "https://openalex.org/W2962736243",
        "https://openalex.org/W2963751529",
        "https://openalex.org/W2888922637",
        "https://openalex.org/W2529194139",
        "https://openalex.org/W4206249782",
        "https://openalex.org/W2964204621",
        "https://openalex.org/W2132119275",
        "https://openalex.org/W2963846996",
        "https://openalex.org/W3103054319",
        "https://openalex.org/W2910243263",
        "https://openalex.org/W2250263931",
        "https://openalex.org/W3104570641",
        "https://openalex.org/W3099911888",
        "https://openalex.org/W2140676672",
        "https://openalex.org/W4297823153",
        "https://openalex.org/W2951286828",
        "https://openalex.org/W3176245452",
        "https://openalex.org/W3173710818",
        "https://openalex.org/W2153579005",
        "https://openalex.org/W3035305735",
        "https://openalex.org/W2791751435",
        "https://openalex.org/W3044103552",
        "https://openalex.org/W3155682407",
        "https://openalex.org/W2946359678",
        "https://openalex.org/W3112788634",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2978670439",
        "https://openalex.org/W3114268635",
        "https://openalex.org/W1615991656",
        "https://openalex.org/W3175606037",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W3007700590",
        "https://openalex.org/W3035058125",
        "https://openalex.org/W3166416728",
        "https://openalex.org/W3156194904",
        "https://openalex.org/W4287119076",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2549835527",
        "https://openalex.org/W2525778437",
        "https://openalex.org/W3159684727",
        "https://openalex.org/W3126074026",
        "https://openalex.org/W2964222268",
        "https://openalex.org/W2963643701",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2118370253"
    ],
    "abstract": "A possible explanation for the impressive performance of masked language model (MLM) pre-training is that such models have learned to represent the syntactic structures prevalent in classical NLP pipelines. In this paper, we propose a different explanation: MLMs succeed on downstream tasks almost entirely due to their ability to model higher-order word co-occurrence statistics. To demonstrate this, we pre-train MLMs on sentences with randomly shuffled word order, and show that these models still achieve high accuracy after fine-tuning on many downstream tasks—including tasks specifically designed to be challenging for models that ignore word order. Our models perform surprisingly well according to some parametric syntactic probes, indicating possible deficiencies in how we test representations for syntactic information. Overall, our results show that purely distributional information largely explains the success of pre-training, and underscore the importance of curating challenging evaluation datasets that require deeper linguistic knowledge.",
    "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2888–2913\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n2888\nMasked Language Modeling and the Distributional Hypothesis:\nOrder Word Matters Pre-training for Little\nKoustuv Sinha†‡ Robin Jia† Dieuwke Hupkes† Joelle Pineau†‡\nAdina Williams† Douwe Kiela†\n†Facebook AI Research; ‡McGill University / Mila - Quebec AI\n{koustuvs,adinawilliams,dkiela}@fb.com\nAbstract\nA possible explanation for the impressive per-\nformance of masked language model (MLM)\npre-training is that such models have learned\nto represent the syntactic structures prevalent\nin classical NLP pipelines. In this paper,\nwe propose a different explanation: MLMs\nsucceed on downstream tasks mostly due to\ntheir ability to model higher-order word co-\noccurrence statistics. To demonstrate this, we\npre-train MLMs on sentences with randomly\nshufﬂed word order, and we show that these\nmodels still achieve high accuracy after ﬁne-\ntuning on many downstream tasks - including\ntasks speciﬁcally designed to be challenging\nfor models that ignore word order. Our mod-\nels also perform surprisingly well according\nto some parametric syntactic probes, indicat-\ning possible deﬁciencies in how we test repre-\nsentations for syntactic information. Overall,\nour results show that purely distributional in-\nformation largely explains the success of pre-\ntraining, and they underscore the importance\nof curating challenging evaluation datasets\nthat require deeper linguistic knowledge.\n1 Introduction\nThe ﬁeld of natural language processing (NLP)\nhas become dominated by the pretrain-and-ﬁnetune\nparadigm, where we ﬁrst obtain a good paramet-\nric prior in order to subsequently model down-\nstream tasks accurately. In particular, masked lan-\nguage model (MLM) pre-training, as epitomized by\nBERT (Devlin et al., 2019), has proven wildly suc-\ncessful, although the precise reason for this success\nhas remained unclear. On one hand, we can view\nBERT as the newest in a long line of NLP tech-\nniques (Deerwester et al., 1990; Landauer and Du-\nmais, 1997; Collobert and Weston, 2008; Mikolov\net al., 2013; Peters et al., 2018) that exploit the well-\nknown distributional hypothesis (Harris, 1954). 1\nOn the other hand, it has been claimed that BERT\n1One might even argue that BERT is not actually\nall that different from earlier distributional models like\nword2vec (Mikolov et al., 2013), see Appendix A.\n“rediscovers the classical NLP pipeline” (Tenney\net al., 2019), suggesting that it has learned “the\ntypes of syntactic and semantic abstractions tradi-\ntionally believed necessary for language process-\ning” rather than “simply modeling complex co-\noccurrence statistics” (ibid. p.1).\nIn this work, we aim to uncover how much of\nMLM’s success comes from learning simple distri-\nbutional information, as opposed to grammatical\nabstractions (Tenney et al., 2019; Manning et al.,\n2020). We disentangle these two hypotheses by\nmeasuring the effect of removing word order in-\nformation during pre-training: any sophisticated\n(English) NLP pipeline would presumably depend\non the syntactic information conveyed by the or-\nder of words. Surprisingly, we ﬁnd that most of\nMLM’s high performance can in fact be explained\nby the “distributional prior” rather than its ability\nto replicate the classical NLP pipeline.\nConcretely, we pre-train MLMs (RoBERTa, Liu\net al. 2019) on various corpora with permuted word\norder while preserving some degree of distribu-\ntional information, and examine their downstream\nperformance. We also experiment with training\nMLMs without positional embeddings, making\nthem entirely order agnostic, and with training on a\ncorpus sampled from the source corpus’s unigram\ndistribution. We then evaluate these “permuted”\nmodels in a wide range of settings and compare\nwith regularly-pre-trained models.\nWe demonstrate that pre-training on permuted\ndata has surprisingly little effect on downstream\ntask performance after ﬁne-tuning (on non-shufﬂed\ntraining data). It has recently been found that\nMLMs are quite robust to permuting downstream\ntest data (Sinha et al., 2021; Pham et al., 2020;\nGupta et al., 2021) and even do quite well using\npermuted “unnatural” downstream train data (Sinha\net al., 2021; Gupta et al., 2021). Here, we show that\ndownstream performance for “unnatural language\npre-training” is much closer to standard MLM pre-\ntraining than one might expect.\nIn an effort to shed light on these ﬁndings, we\n2889\nexperiment with various probing tasks. We verify\nvia non-parametric probes that the permutations do\nin fact make the model worse at syntax-dependent\ntasks. However, just like on the downstream ﬁne-\ntuning tasks, permuted models perform well on\nparametric syntactic probes, in some cases almost\nmatching the unpermuted model’s performance,\nwhich is quite surprising given how important word\norder is crosslinguistically (Greenberg 1963; Dryer\n1992; Cinque 1999, i.a.).\nOur results can be interpreted in different ways.\nOne could argue that our downstream and probing\ntasks are ﬂawed, and that we need to examine mod-\nels with examples that truly test strong generaliza-\ntion and compositionality. Alternatively, one could\nargue that prior works have overstated the depen-\ndence of human language understanding on word\norder, and that human language understanding de-\npends less on the structure of the sentence and more\non the structure of the world, which can be inferred\nto a large extent from distributional information.\nThis work is meant to deepen our understanding\nof MLM pre-training and, through this, move us\ncloser to ﬁnding out what is actually required for\nadequately modelling natural language.\n2 Related Work\nSensitivity to word order in NLU . Information\norder has been a topic of research in computa-\ntional linguistics since Barzilay and Lee (2004)\nintroduced the task of ranking sentence orders as\nan evaluation for language generation quality, an\napproach which was subsequently also used to eval-\nuate readability and dialogue coherence (Barzilay\nand Lapata, 2008; Laban et al., 2021).\nMore recently, several research groups have in-\nvestigated information order for words rather than\nsentences as an evaluation of model humanlikeness.\nSinha et al. (2021) investigate the task of natural\nlanguage inference (NLI) and ﬁnd high accuracy on\npermuted examples for different Transformer and\npre-Transformer era models, across English and\nChinese datasets (Hu et al., 2020). Gupta et al.\n(2021) use targeted permutations on RoBERTa-\nbased models and show word order insensitivity\nacross natural language inference (MNLI), para-\nphrase detection (QQP) and sentiment analysis\ntasks (SST-2). Pham et al. (2020) show insensitivity\non a larger set of tasks, including the entire GLUE\nbenchmark, and ﬁnd that certain tasks in GLUE,\nsuch as CoLA and RTE are more sensitive to per-\nmutations than others. Ettinger (2020) recently\nobserved that BERT accuracy decreases for some\nword order perturbed examples, but not for others.\nIn all these prior works, models were given access\nto normal word order at (pre-)training time, but not\nat test-time or (sometimes) ﬁne-tuning time. It was\nnot clear whether the model acquires enough in-\nformation about word order during the ﬁne-tuning\nstep, or whether it is ingrained in the pre-trained\nmodel. In this work, we take these investigations\na step further: we show that the word order infor-\nmation needed for downstream tasks does not need\nto be provided to the model during pre-training.\nSince models can learn whatever word order infor-\nmation they do need largely from ﬁne-tuning alone,\nthis likely suggests that our downstream tasks don’t\nactually require much complex word order informa-\ntion in the ﬁrst place (cf., Glavaš and Vuli´c 2021).\nRandomization ablations. Random controls have\nbeen explored in a variety of prior work. Wiet-\ning and Kiela (2019) show that random sentence\nencoders are surprisingly powerful baselines. Gau-\nthier and Levy (2019) use random sentence reorder-\ning to label some tasks as “syntax-light” making\nthem more easily decodeable from images of the\nbrain. Shen et al. (2021) show that entire layers of\nMLM transformers can be randomly initialized and\nkept frozen throughout training without detrimen-\ntal effect and that those layers perform better on\nsome probing tasks than their frozen counterparts.\nModels have been found to be surprisingly robust\nto randomizing or cutting syntactic tree structures\nthey were hoped to rely on (Scheible and Schütze,\n2013; Williams et al., 2018a), and randomly per-\nmuting attention weights often induces only mini-\nmal changes in output (Jain and Wallace, 2019). In\ncomputer vision, it is well known that certain ar-\nchitectures constitute good “deep image priors” for\nﬁne-tuning (Ulyanov et al., 2018) or pruning (Fran-\nkle et al., 2020), and that even randomly wired net-\nworks can perform well at image recognition (Xie\net al., 2019). Here, we explore randomizing the\ndata, rather than the model, to assess whether cer-\ntain claims about which phenomena the model has\nlearned are established in fact.\nSynthetic pre-training . Kataoka et al. (2020)\nfound that pre-training on synthetically generated\nfractals for image classiﬁcation is a very strong\nprior for subsequent ﬁne-tuning on real image data.\nIn language modeling, Papadimitriou and Jurafsky\n(2020) train LSTMs (Hochreiter and Schmidhuber,\n2890\n1997) on non-linguistic data with latent structure\nsuch as MIDI music or Java code provides better\ntest performance on downstream tasks than a ran-\ndomly initialized model. They observe that even\nwhen there is no vocabulary overlap among source\nand target languages, LSTM language models lever-\nage the latent hierarchical structure of the input to\nobtain better performance than a random, Zipﬁan\ncorpus of the same vocabulary.\nOn the utility of probing tasks. Many recent pa-\npers provide compelling evidence that BERT con-\ntains a surprising amount of syntax, semantics, and\nworld knowledge (Giulianelli et al., 2018; Rogers\net al., 2020; Lakretz et al., 2019; Jumelet et al.,\n2019, 2021). Many of these works involve diag-\nnostic classiﬁers (Hupkes et al., 2018) or paramet-\nric probes, i.e. a function atop learned represen-\ntations that is optimized to ﬁnd linguistic infor-\nmation. How well the probe learns a given sig-\nnal can be seen as a proxy for linguistic knowl-\nedge encoded in the representations. However, the\ncommunity is divided on many aspects of probing\n(Belinkov, 2021) including how complex probes\nshould be. Many prefer simple linear probes over\nthe complex ones (Alain and Bengio, 2017; Hewitt\nand Manning, 2019; Hall Maudslay et al., 2020).\nHowever, complex probes with strong represen-\ntational capacity are able to extract the most in-\nformation from representations (V oita and Titov,\n2020; Pimentel et al., 2020b; Hall Maudslay et al.,\n2020). Here, we follow Pimentel et al. (2020a) and\nuse both simple (linear) and complex (non-linear)\nmodels, as well as “complex” tasks (dependency\nparsing). As an alternative to parametric probes,\nstimulus-based non-parametric probing (Linzen\net al., 2016; Jumelet and Hupkes, 2018; Marvin and\nLinzen, 2018; Gulordava et al., 2018a; Warstadt\net al., 2019a, 2020a,b; Ettinger, 2020; Lakretz et al.,\n2021) has been used to show that even without a\nlearned probe, BERT can predict syntactic proper-\nties with high conﬁdence (Goldberg, 2019; Wolf,\n2019). We use this class of non-parametric probes\nto investigate RoBERTa’s ability to learn word or-\nder during pre-training.\n3 Approach\nWe ﬁrst describe the data generation and evalua-\ntion methodology used in this paper. We use the\nRoBERTa (base) (Liu et al., 2019) MLM architec-\nture, due to its relative computational efﬁciency\nand good downstream task performance. We ex-\npect that other variants of MLMs would provide\nsimilar insights, given their similar characteristics.\n3.1 Models\nIn all of our experiments, we use the original 16GB\nBookWiki corpus (the Toronto Books Corpus, Zhu\net al. 2015, plus English Wikipedia) from Liu et al.\n(2019).2 We denote the model trained on the orig-\ninal, un-modiﬁed BookWiki corpus as MN (for\n“natural”). We use two types of word order random-\nization methods: permuting words at the sentence\nlevel, and resampling words at the corpus level.\nSentence word order permutation . To investi-\ngate to what extent the performance of MLM pre-\ntraining is a consequence of distributional informa-\ntion, we construct a training corpus devoid of nat-\nural word order but preserving local distributional\ninformation. We construct word order-randomized\nversions of the BookWiki corpus, following the\nsetup of Sinha et al. (2021). Concretely, given a\nsentence S containing N words, we permute the\nsentence using a seeded random function F1 such\nthat no word can remain in its original position. In\ntotal, there exist (N −1)! possible permutations\nof a given sentence. We randomly sample a single\npermutation per sentence, to keep the total dataset\nsize similar to the original.\nWe extend the permutation functionF1 to a func-\ntion Fn that preserves n-gram information. Specif-\nically, given a sentence Sof length N and n-gram\nvalue n, we sample a starting positionifor possible\ncontiguous n-grams ∈{0,N −n}and convert the\nspan S[i,i + n] to a single token, to form ˆS, of\nlength ˆN = N−(n+1). We continue this process\nrepeatedly (without using the previously created\nn-grams) until there exists no starting position for\nselecting a contiguous n-gram in ˆS. For example,\ngiven a sentence of length N = 6 , F4 will ﬁrst\nconvert one span of 4 tokens into a word, to have\nˆSconsisting of three tokens (one conjoined token\nof 4 contiguous words, and two leftover words).\nThen, the resulting sentence ˆS is permuted using\nF1. We train RoBERTa models on four permuta-\ntion variants of BookWiki corpus, M1, M2, M3,\nM4 for each n-gram value ∈{1,2,3,4}. More\ndetails on the process, along with the pseudo code\nand sample quality, are provided in Appendix B.\nCorpus word order bootstrap resample . The\n2We release the pre-trained RoBERTa models used\nin our experiments through the FairSeq repository:\nhttps://github.com/pytorch/fairseq/tree/master/examples /shuf-\nﬂed_word_order.\n2891\nabove permutations preserve higher order distri-\nbutional information by keeping words from the\nsame sentence together. However, we need a base-\nline to understand how a model would perform\nwithout such co-occurrence information. We con-\nstruct a baseline,MUG, that captures word/subword\ninformation, without access to co-occurrence statis-\ntics. To construct MUG, we sample unigrams from\nBookWiki according to their frequencies, while\nalso treating named entities as unigrams. We lever-\nage Spacy (Honnibal et al., 2020) 3 to extract un-\nigrams and named entities from the corpus, and\nconstruct MUG by drawing words from this set\naccording to their frequency. This allows us to\nconstruct MUG such that it has exactly the same\nsize as BookWiki but without any distributional\n(i.e. co-occurrence) information beyond the uni-\ngram frequency distribution. Our hypothesis is that\nany model pre-trained on this data will perform\npoorly, but it should provide a baseline for the lim-\nits on learning language of the inductive bias of the\nmodel in isolation.\nFurther baselines. To investigate what happens\nif a model has absolutely no notion of word order,\nwe also experiment with pre-training RoBERTa on\nthe original corpus without positional embeddings.\nConcretely, we modify the RoBERTa architecture\nto remove the positional embeddings from the com-\nputation graph, and then proceed to pre-train on\nthe natural order BookWiki corpus. We denote\nthis model MNP. Finally, we consider a randomly\ninitialized RoBERTa model MRI to observe the\nextent we can learn from each task with only the\nmodel’s base inductive bias.\nPre-training details. Each model ∈{MN, M1,\nM2, M3, M4, MUG, MNP}is a RoBERTa-base\nmodel (12 layers, hidden size of 768, 12 attention\nheads, 125M parameters), trained for 100k updates\nusing 8k batch-size, 20k warmup steps, and 0.0006\npeak learning rate. These are identical hyperparam-\neters to Liu et al. (2019), except for the number\nof warmup steps which we changed to 20k for im-\nproved training stability. Each model was trained\nusing 64 GPUs for up to 72 hours each. We train\nthree seeds for each data conﬁguration. We validate\nall models on the public Wiki-103 validation set\n(see Appendix C). We use FairSeq (Ott et al., 2019)\nfor the pre-training and ﬁne-tuning experiments.\n3https://spacy.io/\n3.2 Fine-tuning tasks\nWe evaluate downstream performance using the\nGeneral Language Understanding and Evaluation\n(GLUE) benchmark, the Paraphrase Adversaries\nfrom Word Scrambling (PAWS) dataset, and vari-\nous parametric and non-parametric tasks (see §5).\nGLUE. The GLUE (Wang et al., 2018) bench-\nmark is a collection of 9 datasets for evaluat-\ning natural language understanding systems, of\nwhich we use Corpus of Linguistic Acceptabil-\nity (CoLA, Warstadt et al., 2019b), Stanford Sen-\ntiment Treebank (SST, Socher et al., 2013), Mi-\ncrosoft Research Paragraph Corpus (MRPC, Dolan\nand Brockett, 2005), Quora Question Pairs (QQP)4,\nMulti-Genre NLI (MNLI, Williams et al., 2018b),\nQuestion NLI (QNLI, Rajpurkar et al., 2016; Dem-\nszky et al., 2018), Recognizing Textual Entailment\n(RTE, Dagan et al., 2005; Haim et al., 2006; Gi-\nampiccolo et al., 2007; Bentivogli et al., 2009).\nPham et al. (2020) show the word order insensitiv-\nity of several GLUE tasks (QQP, SST-2), evaluated\non public regularly pre-trained checkpoints.\nPA WS. The PAWS task (Zhang et al., 2019) con-\nsists of predicting whether a given pair of sentences\nare paraphrases. This dataset contains both para-\nphrase and non-paraphrase pairs with high lexical\noverlap, which are generated by controlled word\nswapping and back translation. Since even a small\nword swap and perturbation can drastically mod-\nify the meaning of the sentence, we hypothesize\nthe randomized pre-trained models will struggle to\nattain a high performance on PAWS.\nFine-tuning details. We use the same ﬁne-tuning\nmethodology used by Liu et al. (2019), where we\nrun hyperparameter search over the learning rates\n{1 ×10−5,2 ×10−5,3 ×10−5}and batch sizes\n{16,32}for each model. For the best hyperparam\nconﬁgurations of each model, we ﬁne-tune with 5\ndifferent seeds and report the mean and standard\ndeviation for each setting. MNP is ﬁne-tuned with-\nout positional embeddings, matching the way it\nwas pre-trained.\n4 Downstream task results\nIn this section, we present the downstream task\nperformance of the models deﬁned in §3. For eval-\nuation, we report Matthews correlation for CoLA\nand accuracy for all other tasks.\n4http://data.quora.com/First-Quora-Dataset-Release-\nQuestion-Pairs\n2892\nModel QNLI RTE QQP SST-2 MRPC PAWS MNLI-m/mm CoLA\nMN 92.45 +/- 0.2 73.62 +/- 3.1 91.25 +/- 0.1 93.75 +/- 0.4 89.09 +/- 0.9 94.49 +/- 0.2 86.08 +/- 0.2 / 85.4 +/- 0.2 52.45 +/- 21\nM4 91.65 +/- 0.1 70.94 +/- 1.2 91.39 +/- 0.1 92.46 +/- 0.3 86.90 +/- 0.3 94.26 +/- 0.2 83.79 +/- 0.2 / 83.94 +/- 0.3 35.25 +/- 32\nM3 91.56 +/- 0.4 69.75 +/- 2.8 91.22 +/- 0.1 91.97 +/- 0.5 86.22 +/- 0.8 94.03 +/- 0.1 83.83 +/- 0.2 / 83.71 +/- 0.1 40.78 +/- 23\nM2 90.51 +/- 0.1 70.00 +/- 2.5 91.33 +/- 0.0 91.78 +/- 0.3 85.90 +/- 1.2 93.53 +/- 0.3 83.45 +/- 0.3 / 83.54 +/- 0.3 50.83 +/- 5.8\nM1 89.05 +/- 0.2 68.48 +/- 2.5 91.01 +/- 0.0 90.41 +/- 0.4 86.06 +/- 0.8 89.69 +/- 0.6 82.64 +/- 0.1 / 82.67 +/- 0.2 31.08 +/- 10\nMNP 77.59 +/- 0.3 54.78 +/- 2.2 87.78 +/- 0.4 83.21 +/- 0.6 72.78 +/- 1.6 57.22 +/- 1.2 63.35 +/- 0.4 / 63.63 +/- 0.2 2.37 +/- 3.2\nMUG 66.94 +/- 9.2 53.70 +/- 1.0 85.57 +/- 0.1 83.17 +/- 1.5 70.57 +/- 0.7 58.59 +/- 0.3 71.93 +/- 0.2 / 71.33 +/- 0.5 0.92 +/- 2.1\nMRI 62.17 +/- 0.4 52.97 +/- 0.2 81.53 +/- 0.2 82.0 +/- 0.7 70.32 +/- 1.5 56.62 +/- 0.0 65.70 +/- 0.2 / 65.75 +/- 0.3 8.06 +/- 1.6\nTable 1: GLUE and PAWS-Wiki dev set results on different RoBERTa (base) models trained on variants of the\nBookWiki corpus (with mean and std). The top row is the original model, the middle half contains our primary\nmodels under investigation, and the bottom half contains the baselines.\n4.1 Word order permuted pre-training\nIn our ﬁrst set of experiments, we ﬁnetune the pre-\ntrained models on the GLUE and PAWS tasks. We\nreport the results in Table 1.5 First, we observe that\nthe model without access to distributional or word\norder information, MUG (unigram) performs much\nworse than MN overall: MUG is 18 points worse\nthan MN on average across the accuracy-based\ntasks in Table 1 and has essentially no correlation\nwith human judgments on CoLA. MUG MNP and\nMRI perform comparably on most of the tasks,\nwhile achieving surprisingly high scores in QQP\nand SST-2. However, all three models perform sig-\nniﬁcantly worse on GLUE and PAWS, compared\nto MN (Table 1, bottom half). MUG reaches up to\n71.9 on MNLI - possibly due to the fact that MUG\nhas access to (bags of) words and some phrases\n(from NER) is beneﬁcial for MNLI. For the major-\nity of tasks, the difference between MNP and MRI\nis small - a pure bag of words model performs\ncomparably to a randomly initialized model.\nNext, we observe a signiﬁcant improvement on\nall tasks when we give models access to sentence-\nlevel distributional information during pre-training.\nM1, the model pre-trained on completely shufﬂed\nsentences, is on average only 3.3 points lower than\nMN on the accuracy-based tasks, and within 0.3\npoints of MN on QQP. Even on PAWS, which was\ndesigned to require knowledge of word order, M1\nis within 5 points of MN. Randomizing n-grams\ninstead of words during pre-training results in a\n(mostly) smooth increase on these tasks: M4, the\nmodel pre-trained on shufﬂed 4-grams, trails MN\nby only 1.3 points on average, and even comes\n5The MN results are not directly comparable with that\nof publicly released roberta-base model by Liu et al.\n(2019), as that uses the signiﬁcantly larger 160GB corpus, and\nis trained for 500K updates. For computational reasons, we\nrestrict our experiments to the 16GB BookWiki corpus and\n100K updates, mirroring the RoBERTa ablations.\nwithin 0.2 points of MN on PAWS. We observe a\nsomewhat different pattern on CoLA, where M2\ndoes almost as well as MN and outperforms M3\nand M4, though we also observe very high vari-\nance across random seeds for this task. Crucially,\nwe observe that M1 outperforms MNP by a large\nmargin. This shows that positional embeddings are\ncritical for learning, even when the word orders\nthemselves are not natural.6 Overall, these results\nconﬁrm our hypothesis that RoBERTa’s strong per-\nformance on downstream tasks can be explained\nfor a large part by the distributional prior.\n4.2 Word order permuted ﬁne-tuning\nThere are two possible explanations for the results\nin §4.1: either the tasks do not need word order\ninformation to be solved, or any necessary word or-\nder information can be acquired during ﬁne-tuning.\nTo examine this question, we permute the word or-\nder during ﬁne-tuning as well. Concretely, for each\ntask, we construct a unigram order-randomized ver-\nsion of each example in the ﬁne-tuning training set\nusing F1. We then ﬁne-tune our pre-trained models\non this shufﬂed data and evaluate task performance.\nFor all experiments, we evaluate and perform early\nstopping on the original, natural word order dev set,\nin order to conduct a fair evaluation on the exact\nsame optimization setup for all models.\nOur results in Figure 1 provide some evidence\nfor both hypotheses. On QQP and QNLI, accu-\nracy decreases only slightly for models ﬁne-tuned\non shufﬂed data. Models can also achieve above\n80% accuracy on MNLI, SST-2, and MRPC when\n6Recall, MNP is fed natural sentences as MN while not\nhaving the ability to learn positional embeddings. To further\nquantify the effect of positional embeddings, we also investi-\ngated the effect of shufﬂing the entire context window, to keep\nthe co-occurrence information same as MNP in Appendix D.\nWe observed this model to be worse thanM1 but signiﬁcantly\nbetter than MNP to support the claim about the importance of\npositional embeddings while training.\n2893\nN\n 1\n 2\n 3\n 4\n UG\n0.6\n0.7\n0.8\n0.9Accuracy\nMNLI\nN\n 1\n 2\n 3\n 4\n UG\n0.7\n0.8\n0.9\n1.0\nQQP\nN\n 1\n 2\n 3\n 4\n UG\n0.6\n0.7\n0.8\n0.9\n1.0\nQNLI\nN\n 1\n 2\n 3\n 4\n UG\n0.0\n0.2\n0.4\n0.6\nCoLA\nN\n 1\n 2\n 3\n 4\n UG\n0.7\n0.8\n0.9\n1.0Accuracy\nSST-2\nN\n 1\n 2\n 3\n 4\n UG\n0.6\n0.7\n0.8\n0.9\n1.0\nMRPC\nN\n 1\n 2\n 3\n 4\n UG\n0.4\n0.5\n0.6\n0.7\n0.8\nRTE\nN\n 1\n 2\n 3\n 4\n UG\n0.6\n0.8\n1.0\nPAWS\nNatural word order\nRandom word order\nFigure 1: GLUE & PAWS task dev performance when ﬁnetuned on naturally (blue) and randomly ordered (orange)\ntext, respectively, using pre-trained RoBERTa (base) models trained on different versions of BookWiki corpus.\nﬁne-tuned on shufﬂed data, suggesting that purely\nlexical information is quite useful on its own.7\nOn the other hand, for all datasets besides QQP\nand QNLI, we see noticeable drops in accuracy\nwhen ﬁne-tuning on shufﬂed data and testing on\nnormal order, both for MN and for shufﬂed models\nM1 through M4. This suggests both that word\norder information is useful for these tasks, and that\nshufﬂed models must be learning to use word or-\nder information during ﬁne-tuning.8 Having word\norder during ﬁne-tuning is especially important\nfor achieving high accuracy on CoLA, RTE (cf.\nPham et al. 2020), as well as PAWS, suggesting\nthat these tasks are the most word order reliant. Re-\ncent research (Yu and Ettinger, 2021) raised some\nquestions about potential artefacts inﬂating perfor-\nmance on PAWS: their swapping-distance cue of\nappears consistent both with our ﬁnding of high\nPAWS performance for n-gram shufﬂed models in\nTable 1, and with our PAWS results in Figure 1,\nwhich suggests that PAWS performance does in\nfact rely to some extent on natural word order at\nthe ﬁne-tuning stage.\nFinally, for CoLA, MRPC, and RTE, perfor-\nmance is higher after ﬁne-tuning on shufﬂed data\nfor M1 than MN. We hypothesize that MN repre-\n7This ﬁnding is compatible with the observation of Gupta\net al. (2021) and Sinha et al. (2021) who train on a randomized\ntraining corpus for MRPC, QQP, SST-2 and MNLI.\n8We perform additional experiments on how the model\nrepresentations change during ﬁne-tuning for shufﬂed training\nusing Risannen Data Analysis in Appendix I.\nsents shufﬂed and non-shufﬂed sentences very dif-\nferently, resulting in a domain mismatch problem\nwhen ﬁne-tuning on shufﬂed data but evaluating\non non-shufﬂed data.9 Since M1 never learns to\nbe sensitive to word order during pre-training or\nﬁne-tuning, it does not suffer from that issue. Our\nresults in this section also highlights the issues with\nthese datasets, concurrent to the ﬁndings that many\nGLUE tasks does not need sophisticated linguistic\nknowledge to solve, as models typically tend to\nexploit the statistical artefacts and spurious corre-\nlations during ﬁne-tuning (cf. Gururangan et al.\n2018; Poliak et al. 2018; Tsuchiya 2018; McCoy\net al. 2019). However, our results overwhelmingly\nsupport the fact that word order does not matter\nduring pre-training, if the model has the opportu-\nnity to learn the necessary information about word\norder during ﬁne-tuning.\n5 Probing results\nTo investigate how much syntactic information is\ncontained in the MLM representations, we eval-\nuate several probing tasks on our trained models.\nWe consider two classes of probes: parametric\nprobes, which make use of learnable parameters,\nand non-parametric probes, which directly exam-\n9We further study the domain mismatch problem by eval-\nuating on shufﬂed data after ﬁne-tuning on the shufﬂed data\nfor models in Appendix F. We observe that models improves\ntheir scores on evaluation on shufﬂed data when the training\ndata source is changed from natural to shufﬂed - highlighting\ndomain match effect.\n2894\nine the language model’s predictions.\n5.1 Parametric Probing\nTo probe our models for syntactic, semantic and\nother linguistic properties, we investigate depen-\ndency parsing using Pareto probing (Pimentel et al.,\n2020a) and the probing tasks from Conneau et al.\n(2018) in SentEval (Conneau and Kiela, 2018).\n5.1.1 Syntactic Probing\nPimentel et al. (2020a) proposed a framework\nbased on Pareto optimality to probe for syntactic\ninformation in contextual representations. They\nsuggest that an optimal probe should balance op-\ntimal performance on the probing task with the\ncomplexity of the probe. Following their setup,\nwe use the “difﬁcult” probe: dependency parsing\n(DEP). We also investigate the “easy” probes, de-\npendency arc labeling (DAL) and POS tag predic-\ntion (POS), results are reported in Appendix K. We\nprobe with Linear and MLP probes, and inspect the\ntask accuracy in terms of Unlabeled Attachment\nScore (UAS). The dependency parsing probe used\nin Pimentel et al. (2020a) builds on the Biafﬁne\nDependency Parser (Dozat and Manning, 2017),\nbut with simple MLPs on top of the Transformer\nrepresentations.10\nTraining setup. Similar to the setup by Pimentel\net al. (2020a), we run 50 random hyperparameter\nsearches on both MLP and Linear probes by uni-\nformly sampling from the number of layers (0-5),\ndropout (0-0.5), log-uniform hidden size [25,210].\nWe triple this experiment size by evaluating on\nthree pre-trained models of different seeds for each\nmodel conﬁguration. We consider Pimentel et al.’s\nEnglish dataset, derived from Universal Dependen-\ncies EWT (UD EWT) (Bies et al., 2012; Silveira\net al., 2014) which contains 12,543 training sen-\ntences. Additionally, we experiment on the Penn\nTreebank dataset (PTB), which contains 39,832\ntraining sentences.11 We report the mean test accu-\nracy over three seeds for the best dev set accuracy\nfor each task.12\n10We experimented with a much stronger, state-of-the-art\nSecond order Tree CRF Neural Dependency Parser (Zhang\net al., 2020), but did not observe any difference in UAS with\ndifferent pre-trained models (see Appendix G)\n11PTB data (Kitaev et al., 2019) is used from\ngithub.com/nikitakit/self-attentive-parser/tree/master/data.\n12Pimentel et al. (2020a) propose computing the Pareto\nHypervolume over all hyperparameters in each task. We did\nnot observe a signiﬁcant difference in the hypervolumes for\nthe models, as reported in Appendix K.\nModel UD EWT PTB\nMLP Linear MLP Linear\nMN 80.41 +/- 0.85 66.26 +/- 1.5986.99 +/- 1.49 66.47 +/- 2.77\nM4 78.04 +/- 2.06 65.61 +/- 1.9985.62 +/- 1.09 66.49 +/- 2.02\nM3 77.80 +/- 3.09 64.89 +/- 2.6385.89 +/- 1.01 66.11 +/- 1.68\nM2 78.22 +/- 0.88 64.96 +/- 2.3284.72 +/- 0.55 64.69 +/- 2.50\nM1 69.26 +/- 6.00 56.24 +/- 5.0579.43 +/- 0.96 57.20 +/- 2.76\nMUG 74.15 +/- 0.93 65.69 +/- 7.3580.07 +/- 0.79 57.28 +/- 1.42\nTable 2: Unlabeled Attachment Score (UAS) (mean\nand std) on the dependency parsing task (DEP) on two\ndatasets, UD EWT and PTB, using the Pareto Probing\nframework (Pimentel et al., 2020a).\nResults. We observe that the UAS scores follow\na similar linear trend as the ﬁne-tuning results in\nthat M1≈MUG< M2< M3< M4< MN (Table 2).\nSurprisingly, MUG probing scores seem to be some-\nwhat better than M1 (though with large overlap in\ntheir standard deviations), even though MUG can-\nnot learn information related to either word order\nor co-occurrence patterns. The performance gap\nappears to be task- and probe speciﬁc. We observe\na low performance gap in several scenarios, the\nlowest being between MN vs. M3/M4, for PTB\nusing the both MLP and Linear probes.\n5.1.2 SentEval Probes\nWe also investigate the suite of 10 probing\ntasks (Conneau et al., 2018) available in the SentE-\nval toolkit (Conneau and Kiela, 2018). This suite\ncontains a range of semantic, syntactic and surface\nlevel tasks. Jawahar et al. (2019) utilize this set\nof probing tasks to arrive at the conclusion that\n“BERT embeds a rich hierarchy of linguistic sig-\nnals: surface information at the bottom, syntactic\ninformation in the middle, semantic information at\nthe top”. We re-examine this hypothesis by using\nthe same probing method and comparing against\nmodels trained with random word order.\nTraining setup. We run the probes on the ﬁnal\nlayer of each of our pre-trained models for three\nseeds, while keeping the encoder frozen. SentEval\ntrains probes on top of ﬁxed representations individ-\nually for each task. We follow the recommended\nsetup and run grid search over the following hy-\nperparams: number of hidden layer dimensions\n([0,50,100,200]), dropout ([0,0.1,0.2]), 4 epochs,\n64 batch size. We select the best performance based\non the dev set, and report the test set accuracy.\nResults. We provide the results in Table 3. The\nMN pre-trained model scores better than the un-\nnatural word order models for only one out of ﬁve\nsemantic tasks and in none of the lexical tasks.\n2895\nModel Length WordContent TreeDepth TopConstituents BigramShift Tense SubjNumber ObjNumber OddManOut CoordInversion(Surface) (Surface) (Syntactic) (Syntactic) (Syntactic) (Semantic) (Semantic) (Semantic) (Semantic) (Semantic)\nMN 78.92 +/- 1.91 31.83 +/- 1.75 35.97 +/- 1.3878.26+/- 4.0881.82+/- 0.55 87.83 +/- 0.51 85.05 +/- 1.23 75.94 +/- 0.68 58.40 +/- 0.3370.87+/- 2.46\nM4 92.88 +/- 0.15 57.78 +/- 0.36 40.05 +/- 0.29 72.50 +/- 0.51 76.12 +/- 0.29 88.32 +/- 0.1385.65+/- 0.13 82.95 +/- 0.0558.89+/- 0.30 61.31 +/- 0.19M3 91.52 +/- 0.16 48.81 +/- 0.26 38.63 +/- 0.61 70.29 +/- 0.31 77.36 +/- 0.12 86.74 +/- 0.12 83.83 +/- 0.38 80.99 +/- 0.26 57.01 +/- 0.21 60.00 +/- 0.26M2 93.54+/- 0.29 62.52 +/- 0.2141.40+/- 0.32 74.31 +/- 0.29 75.44 +/- 0.1487.91+/- 0.35 84.88 +/- 0.11 83.98 +/- 0.14 57.60 +/- 0.36 59.46 +/- 0.37M1 88.33 +/- 0.1464.03+/- 0.34 40.24 +/- 0.20 70.94 +/- 0.38 58.37 +/- 0.40 87.88 +/- 0.08 83.49 +/- 0.1283.44+/- 0.06 56.51 +/- 0.26 56.98 +/- 0.50\nMUG 86.69 +/- 0.33 36.60 +/- 0.33 32.53 +/- 0.76 61.54 +/- 0.60 57.42 +/- 0.04 68.45 +/- 0.23 71.25 +/- 0.12 66.63 +/- 0.21 50.06 +/- 0.40 56.26 +/- 0.17\nTable 3: SentEval Probing (Conneau et al., 2018; Conneau and Kiela, 2018) results (with mean and std) on different\nmodel variants.\nHowever, MN does score higher for two out of\nthree syntactic tasks. Even for these two syntactic\ntasks, the gap among MUG and MN is much higher\nthan M1 and MN. These results show that while\nnatural word order is useful for at least some prob-\ning tasks, the distributional prior of randomized\nmodels alone is enough to achieve a reasonably\nhigh accuracy on syntax sensitive probing.\n5.2 Non-Parametric Probing\nHow to probe effectively with parametric probes\nis a matter of much recent debate (Hall Maudslay\net al., 2020; Belinkov, 2021). From our results\nso far, it is unclear whether parametric probing\nmeaningfully distinguishes models trained with cor-\nrupted word order from those trained with normal\norders. Thus, we also investigate non-parametric\nprobes (Linzen et al., 2016; Marvin and Linzen,\n2018; Gulordava et al., 2018b) using the formula-\ntion of Goldberg (2019) and Wolf (2019).\nWe consider a set of non-parametric probes that\nuse a range of sentences varying in their linguistic\nproperties. For each, the objective is for a pre-\ntrained model to provide higher probability to a\ngrammatically correct word than to an incorrect\none. Since both the correct and incorrect options\noccupy the same sentential position, we call them\n“focus words”. Linzen et al. (2016) use sentences\nfrom Wikipedia containing present-tense verbs, and\ncompare the probability assigned by the encoder\nto plural vs. singular forms of the verb; they focus\non sentences containing at least one noun between\nthe verb and its subject, known as “agreement at-\ntractors.” Gulordava et al. (2018b) instead replace\nfocus words with random substitutes from the same\npart-of-speech and inﬂection. Finally, Marvin and\nLinzen (2018) construct minimal pairs of grammat-\nical and ungrammatical sentences, and compare the\nmodel’s probability for the words that differ.\nSetup. In our experiments, we mask the focus\nwords in the stimuli and compute the probability of\nthe correct and incorrect token respectively. To han-\nModel Linzen et al. (2016)∗ Gulordava et al. (2018b)∗ Marvin and Linzen (2018)\nMN 91.17 +/- 2.6 68.66 +/- 11.6 88.05 +/- 6.5M4 66.93 +/- 3.2 69.47 +/- 4.9 70.66 +/- 12.5M3 64.60 +/- 2.7 66.10 +/- 5.9 73.82 +/- 15.7M2 61.27 +/- 3.1 60.20 +/- 7.6 73.95 +/- 14.3M1 58.96 +/- 1.8 68.10 +/- 14.4 70.69 +/- 11.6MUG 65.36 +/- 7.1 60.88 +/- 24.3 50.10 +/- 0.2\nTable 4: Mean (and std) non-parametric probing ac-\ncuracy on different datasets. ∗ indicates rebalanced\ndatasets, see Appendix L for more details.\ndle Byte-Pair Encoding (BPE), we use the Word-\nPiece (Wu et al., 2016) tokens prepended with a\nspace. We observe that the Linzen et al. (2016)\nand Gulordava et al. (2018b) datasets are skewed\ntowards singular focus words, which could dispro-\nportionately help weaker models that just happen\nto assign more probability mass to singular focus\nwords. To counter this, we balance these datasets to\nhave an equal number of singular and plural focus\nwords by upsampling, and report the aggregated\nand balanced results in Table 4 (see Appendix L for\nmore detailed results). We verify our experiments\nby using three pre-trained models with different\nseeds for each model conﬁguration.\nResults. We observe for the Linzen et al. (2016)\nand Marvin and Linzen (2018) datasets that the gap\nbetween the MN and randomization models is rel-\natively large. The Gulordava et al. (2018b) dataset\nshows a smaller gap between MN and the random-\nization models. While some randomization models\n(e.g., M2, M3, and M4) performed quite simi-\nlarly to MN according to the parametric probes,\nthey all are markedly worse than MN according to\nthe non-parametric ones. This suggests that non-\nparametric probes identify certain syntax-related\nmodeling failures that parametric ones do not.\n6 Discussion\nThe assumption that word order information is cru-\ncial for any classical NLP pipeline (especially for\nEnglish) is deeply ingrained in our understanding\nof syntax itself (Chomsky, 1957): without order,\nmany linguistic constructs are undeﬁned. Our ﬁne-\n2896\ntuning results in §4.1 and parametric probing re-\nsults in §5.1, however, suggests that MLMs do not\nneed to rely much on word order to achieve high\naccuracy, bringing into question previous claims\nthat they learn a “classical NLP pipeline.”\nOne might ask, though, whether an NLP pipeline\nwould really need natural word order at all: can\ntransformers not simply learn what the correct word\norder is from unordered text? First, the lower non-\nparametric probing accuracies of the randomized\nmodels indicate that they are not able to accurately\nreconstruct the original word order (see also Ap-\npendix D). But even if models were able to “un-\nshufﬂe” the words under our unnatural pre-training\nset up, they would only be doing so based on dis-\ntributional information. Models would then ab-\nductively learn only the most likely word order.\nWhile models might infer a distribution over pos-\nsible orders and use that information to structure\ntheir representations (Papadimitriou et al., 2021),\nsyntax is not about possible or even the most likely\norders: it is about the actual order. That is, even\nif one concludes in the end that Transformers are\nable to perform word order reconstruction based on\ndistributional information, and recover almost all\ndownstream performance based solely on that, we\nought to be a lot more careful when making claims\nabout what our evaluation datasets are telling us.\nThus, our results seem to suggest that we may\nneed to revisit what we mean by “linguistic struc-\nture,” and perhaps subsequently acknowledge that\nwe may not need human-like linguistic abilities\nfor most NLP tasks. Or, our results can be inter-\npreted as evidence that we need to develop more\nchallenging and more comprehensive evaluations,\nif we genuinely want to measure linguistic abilities,\nhowever those are deﬁned, in NLP models.\nThere are many interesting and potentially ex-\nciting avenues for future work that we could not\nexplore due to limitation of space. An interest-\ning question revolves around whether this phe-\nnomenon is more pronounced for English than for\nother languages. It is natural to wonder whether\nmore word-order ﬂexible or morphologically-rich\nlanguages would suffer from the same problem. Us-\ning the methods discussed in this work, we could\nimagine devising a way to determine the degree\nof order-dependence for tasks across languages.\nAnother possible extension pertains to other tasks,\nincluding extractive question answering (QA) or\nsequence tagging, for which we can also to deter-\nmine whether word order information is acquired\ndownstream or during pre-training.\nThe sensitivity of generative models to word\norder permuted input could also be investigated\nfurther. Recent work by Parthasarathi et al. (2021)\nbegins this discussion, by showing that a Machine\nTranslation (MT) model can often arrive at the\ngold source translation when provided with input\nsentences that have had their words permuted us-\ning parse trees. Relatedly, Alleman et al. (2021)\nalso investigates targeted parse-tree-based pertur-\nbations as a means of evaluating model robustness.\nO’Connor and Andreas (2021) also demonstrate\nthe insensitivity of Transformers towards syntax\nmanipulations while achieving low perplexity in\nlanguage modeling tasks. Exploring model sensi-\ntivity to word order permutations for approaches\nthat unify generation and classiﬁcation (e.g., multi-\ntasking) could also be interesting future work.\n7 Conclusion\nIn this work, we revisited the hypothesis that\nmasked language modelling’s impressive perfor-\nmance can be explained in part by its ability to learn\nclassical NLP pipelines. We investigated targeted\npre-training on sentences with various degrees of\nrandomization in their word order, and observed\noverwhelmingly that MLM’s success is most likely\nnot due to its ability to discover syntactic and se-\nmantic mechanisms necessary for a traditional lan-\nguage processing pipeline during pre-training. In-\nstead, our experiments suggest that MLM’s suc-\ncess can largely be explained by it having learned\nhigher-order distributional statistics that make for\na useful prior for subsequent ﬁne-tuning. These re-\nsults should hopefully encourage the development\nof better, more challenging tasks that require so-\nphisticated reasoning, and harder probes to narrow\ndown what exact linguistic information is present\nin the representations learned by our models.\nAcknowledgements\nWe thank Tiago Pimentel, Shruti Bhosale, Naman\nGoyal, Shagun Sodhani, Sylke Gosen, Prasanna\nParasarathi, Kyunghyun Cho, Mona Diab, Brenden\nLake, Myle Ott, Ethan Perez, and Mike Lewis for\ntheir help in resolving technical doubts during ex-\nperimentation and/or feedback on an earlier draft.\nWe also thank the anonymous reviewers for their\nconstructive feedback during the reviewing phase,\nwhich helped polish the paper to its current state.\n2897\nReferences\nGuillaume Alain and Yoshua Bengio. 2017. Under-\nstanding intermediate layers using linear classiﬁer\nprobes. In ICLR 2017, Workshop Track Proceedings.\nOpenReview.net.\nMatteo Alleman, Jonathan Mamou, Miguel A Del Rio,\nHanlin Tang, Yoon Kim, and SueYeon Chung.\n2021. Syntactic perturbations reveal representa-\ntional correlates of hierarchical phrase structure in\npretrained language models. In Proceedings of the\n6th Workshop on Representation Learning for NLP\n(RepL4NLP-2021), pages 263–276, Online. Associ-\nation for Computational Linguistics.\nTrapit Bansal, Rishikesh Jha, and Andrew McCallum.\n2020. Learning to few-shot learn across diverse\nnatural language classiﬁcation tasks. In Proceed-\nings of the 28th International Conference on Com-\nputational Linguistics, pages 5108–5123, Barcelona,\nSpain (Online). International Committee on Compu-\ntational Linguistics.\nRegina Barzilay and Mirella Lapata. 2008. Modeling\nlocal coherence: An entity-based approach. Compu-\ntational Linguistics, 34(1):1–34.\nRegina Barzilay and Lillian Lee. 2004. Catching the\ndrift: Probabilistic content models, with applications\nto generation and summarization. In Proceedings of\nthe Human Language Technology Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: HLT-NAACL 2004 , pages\n113–120, Boston, Massachusetts, USA. Association\nfor Computational Linguistics.\nYonatan Belinkov. 2021. Probing classiﬁers: Promises,\nshortcomings, and alternatives. arXiv preprint\narXiv:2102.12452.\nLuisa Bentivogli, Peter Clark, Ido Dagan, and Danilo\nGiampiccolo. 2009. The ﬁfth PASCAL recognizing\ntextual entailment challenge. In TAC.\nAnn Bies, Justin Mott, Colin Warner, and Seth Kulick.\n2012. English web treebank. Linguistic Data Con-\nsortium, Philadelphia, PA.\nNoam Chomsky. 1957. Syntactic structures. Walter de\nGruyter.\nGuglielmo Cinque. 1999. Adverbs and functional\nheads: A cross-linguistic perspective . Oxford Uni-\nversity Press on Demand.\nRonan Collobert and Jason Weston. 2008. A uniﬁed\narchitecture for natural language processing: Deep\nneural networks with multitask learning. In Pro-\nceedings of the 25th international conference on Ma-\nchine learning, pages 160–167.\nAlexis Conneau and Douwe Kiela. 2018. SentEval: An\nevaluation toolkit for universal sentence representa-\ntions. In Proceedings of the Eleventh International\nConference on Language Resources and Evaluation\n(LREC 2018), Miyazaki, Japan. European Language\nResources Association (ELRA).\nAlexis Conneau, German Kruszewski, Guillaume Lam-\nple, Loïc Barrault, and Marco Baroni. 2018. What\nyou can cram into a single $&!#* vector: Probing\nsentence embeddings for linguistic properties. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 2126–2136, Melbourne, Aus-\ntralia. Association for Computational Linguistics.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2005. The PASCAL recognising textual entailment\nchallenge. In Machine Learning Challenges Work-\nshop, pages 177–190. Springer.\nScott Deerwester, Susan T Dumais, George W Fur-\nnas, Thomas K Landauer, and Richard Harshman.\n1990. Indexing by latent semantic analysis. Jour-\nnal of the American society for information science ,\n41(6):391–407.\nDorottya Demszky, Kelvin Guu, and Percy Liang. 2018.\nTransforming question answering datasets into nat-\nural language inference datasets. arXiv preprint\narXiv:1809.02922.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\nTimothy Dozat and Christopher D Manning. 2017.\nDeep biafﬁne attention for neural dependency pars-\ning. In 5th International Conference on Learning\nRepresentations, ICLR 2017, Toulon, France, April\n24-26, 2017, Conference Track Proceedings . Open-\nReview.net.\nMatthew S Dryer. 1992. The Greenbergian word order\ncorrelations. Language, pages 81–138.\nAllyson Ettinger. 2020. What BERT is not: Lessons\nfrom a new suite of psycholinguistic diagnostics for\nlanguage models. Transactions of the Association\nfor Computational Linguistics, 8:34–48.\nJonathan Frankle, David J Schwab, and Ari S Mor-\ncos. 2020. Training batchnorm and only batchnorm:\nOn the expressive power of random features in cnns.\narXiv preprint arXiv:2003.00152.\nJon Gauthier and Roger Levy. 2019. Linking artiﬁcial\nand human neural representations of language. In\nProceedings of the 2019 Conference on Empirical\n2898\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 529–\n539, Hong Kong, China. Association for Computa-\ntional Linguistics.\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan,\nand William B Dolan. 2007. The third PASCAL\nrecognizing textual entailment challenge. In Pro-\nceedings of the ACL-PASCAL workshop on textual\nentailment and paraphrasing, pages 1–9.\nMario Giulianelli, Jack Harding, Florian Mohnert,\nDieuwke Hupkes, and Willem Zuidema. 2018. Un-\nder the hood: Using diagnostic classiﬁers to in-\nvestigate and improve how language models track\nagreement information. In Proceedings of the 2018\nEMNLP Workshop BlackboxNLP: Analyzing and In-\nterpreting Neural Networks for NLP, pages 240–248,\nBrussels, Belgium. Association for Computational\nLinguistics.\nGoran Glavaš and Ivan Vuli´c. 2021. Is supervised syn-\ntactic parsing beneﬁcial for language understanding\ntasks? an empirical investigation. In Proceedings of\nthe 16th Conference of the European Chapter of the\nAssociation for Computational Linguistics: Main\nVolume, pages 3090–3104, Online. Association for\nComputational Linguistics.\nYoav Goldberg. 2019. Assessing BERT’s Syntactic\nAbilities. CoRR, page 4.\nJoseph Greenberg. 1963. Some universals of grammar\nwith particular reference to the order of meaningful\nelements. In J. Greenberg, ed., Universals of Lan-\nguage. 73-113. Cambridge, MA.\nKristina Gulordava, Piotr Bojanowski, Edouard Grave,\nTal Linzen, and Marco Baroni. 2018a. Colorless\ngreen recurrent networks dream hierarchically. In\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long Papers) , pages 1195–1205, New\nOrleans, Louisiana. Association for Computational\nLinguistics.\nKristina Gulordava, Piotr Bojanowski, Edouard Grave,\nTal Linzen, and Marco Baroni. 2018b. Color-\nless green recurrent networks dream hierarchically.\narXiv:1803.11138 [cs].\nAshim Gupta, Giorgi Kvernadze, and Vivek Srikumar.\n2021. Bert & family eat word salad: Experiments\nwith text understanding. Proceedings of the AAAI\nConference on Artiﬁcial Intelligence, 35(14):12946–\n12954.\nSuchin Gururangan, Swabha Swayamdipta, Omer\nLevy, Roy Schwartz, Samuel Bowman, and Noah A.\nSmith. 2018. Annotation artifacts in natural lan-\nguage inference data. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 2 (Short Papers) ,\npages 107–112, New Orleans, Louisiana. Associa-\ntion for Computational Linguistics.\nR Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo\nGiampiccolo, Bernardo Magnini, and Idan Szpektor.\n2006. The second PASCAL recognising textual en-\ntailment challenge. In Proceedings of the Second\nPASCAL Challenges Workshop on Recognising Tex-\ntual Entailment.\nRowan Hall Maudslay, Josef Valvoda, Tiago Pimentel,\nAdina Williams, and Ryan Cotterell. 2020. A tale of\na probe and a parser. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7389–7395, Online. Association\nfor Computational Linguistics.\nZellig S Harris. 1954. Distributional structure. Word,\n10(2-3):146–162.\nJohn Hewitt and Christopher D Manning. 2019. A\nstructural probe for ﬁnding syntax in word represen-\ntations. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4129–4138.\nSepp Hochreiter and Jürgen Schmidhuber. 1997.\nLong short-term memory. Neural computation ,\n9(8):1735–1780.\nMatthew Honnibal, Ines Montani, Soﬁe Van Lan-\ndeghem, and Adriane Boyd. 2020. spaCy:\nIndustrial-strength Natural Language Processing in\nPython.\nHai Hu, Kyle Richardson, Liang Xu, Lu Li, Sandra\nKübler, and Lawrence Moss. 2020. OCNLI: Orig-\ninal Chinese Natural Language Inference. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2020 , pages 3512–3526, Online. As-\nsociation for Computational Linguistics.\nDieuwke Hupkes, Sara Veldhoen, and Willem Zuidema.\n2018. Visualisation and ‘diagnostic classiﬁers’ re-\nveal how recurrent and recursive neural networks\nprocess hierarchical structure. Journal of Artiﬁcial\nIntelligence Research, 61:907–926.\nSarthak Jain and Byron C. Wallace. 2019. Attention is\nnot Explanation. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 3543–3556, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nGanesh Jawahar, Benoît Sagot, and Djamé Seddah.\n2019. What Does BERT Learn about the Structure\nof Language? In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 3651–3657, Florence, Italy. Associa-\ntion for Computational Linguistics.\n2899\nJaap Jumelet, Milica Denic, Jakub Szymanik, Dieuwke\nHupkes, and Shane Steinert-Threlkeld. 2021. Lan-\nguage models use monotonicity to assess NPI li-\ncensing. In Findings of the Association for Com-\nputational Linguistics: ACL/IJCNLP 2021, Online\nEvent, August 1-6, 2021, volume ACL/IJCNLP 2021\nof Findings of ACL, pages 4958–4969. Association\nfor Computational Linguistics.\nJaap Jumelet and Dieuwke Hupkes. 2018. Do lan-\nguage models understand anything? on the ability\nof LSTMs to understand negative polarity items. In\nProceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP , pages 222–231, Brussels, Belgium.\nAssociation for Computational Linguistics.\nJaap Jumelet, Willem Zuidema, and Dieuwke Hupkes.\n2019. Analysing neural language models: Con-\ntextual decomposition reveals default reasoning in\nnumber and gender assignment. In Proceedings of\nthe 23rd Conference on Computational Natural Lan-\nguage Learning (CoNLL), pages 1–11, Hong Kong,\nChina. Association for Computational Linguistics.\nHirokatsu Kataoka, Kazushige Okayasu, Asato Mat-\nsumoto, Eisuke Yamagata, Ryosuke Yamada, Naka-\nmasa Inoue, Akio Nakamura, and Yutaka Satoh.\n2020. Pre-training without Natural Images. In Pro-\nceedings of the Asian Conference on Computer Vi-\nsion.\nNikita Kitaev, Steven Cao, and Dan Klein. 2019. Multi-\nlingual constituency parsing with self-attention and\npre-training. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 3499–3505, Florence, Italy. Associa-\ntion for Computational Linguistics.\nPhilippe Laban, Luke Dai, Lucas Bandarkar, and\nMarti A. Hearst. 2021. Can transformer models mea-\nsure coherence in text: Re-thinking the shufﬂe test.\nIn Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 2: Short Papers) , pages\n1058–1064, Online. Association for Computational\nLinguistics.\nYair Lakretz, Dieuwke Hupkes, Alessandra Vergallito,\nMarco Marelli, Marco Baroni, and Stanislas De-\nhaene. 2021. Mechanisms for handling nested de-\npendencies in neural-network language models and\nhumans. Cognition, page 104699.\nYair Lakretz, German Kruszewski, Theo Desbordes,\nDieuwke Hupkes, Stanislas Dehaene, and Marco Ba-\nroni. 2019. The emergence of number and syn-\ntax units in LSTM language models. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), pages 11–20, Minneapolis,\nMinnesota. Association for Computational Linguis-\ntics.\nThomas K Landauer and Susan T Dumais. 1997. A\nsolution to plato’s problem: The latent semantic\nanalysis theory of acquisition, induction, and rep-\nresentation of knowledge. Psychological review ,\n104(2):211.\nOmer Levy, Yoav Goldberg, and Ido Dagan. 2015. Im-\nproving distributional similarity with lessons learned\nfrom word embeddings. Transactions of the Associ-\nation for Computational Linguistics, 3:211–225.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\n2016. Assessing the ability of LSTMs to learn\nsyntax-sensitive dependencies. Transactions of the\nAssociation for Computational Linguistics , 4:521–\n535.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach. arXiv:1907.11692 [cs].\nChristopher D Manning, Kevin Clark, John Hewitt, Ur-\nvashi Khandelwal, and Omer Levy. 2020. Emer-\ngent linguistic structure in artiﬁcial neural networks\ntrained by self-supervision. Proceedings of the Na-\ntional Academy of Sciences, 117(48):30046–30054.\nRebecca Marvin and Tal Linzen. 2018. Targeted syn-\ntactic evaluation of language models. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 1192–1202,\nBrussels, Belgium. Association for Computational\nLinguistics.\nTom McCoy, Ellie Pavlick, and Tal Linzen. 2019.\nRight for the wrong reasons: Diagnosing syntactic\nheuristics in natural language inference. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics , pages 3428–3448,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems, pages 3111–3119.\nJoe O’Connor and Jacob Andreas. 2021. What context\nfeatures can transformer language models use?\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensible\ntoolkit for sequence modeling. In Proceedings of\nNAACL-HLT 2019: Demonstrations.\nIsabel Papadimitriou, Ethan A. Chi, Richard Futrell,\nand Kyle Mahowald. 2021. Deep subjecthood:\nHigher-order grammatical features in multilingual\nBERT. In Proceedings of the 16th Conference of the\nEuropean Chapter of the Association for Computa-\ntional Linguistics: Main Volume, pages 2522–2532,\nOnline. Association for Computational Linguistics.\n2900\nIsabel Papadimitriou and Dan Jurafsky. 2020. Learn-\ning Music Helps You Read: Using transfer to study\nlinguistic structure in language models. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n6829–6839, Online. Association for Computational\nLinguistics.\nPrasanna Parthasarathi, Koustuv Sinha, Joelle Pineau,\nand Adina Williams. 2021. Sometimes we want un-\ngrammatical translations. In Findings of the Associa-\ntion for Computational Linguistics: Empirical Meth-\nods in Natural Language Processing (EMNLP).\nEthan Perez, Douwe Kiela, and Kyunghyun Cho. 2021.\nRissanen Data Analysis: Examining Dataset Charac-\nteristics via Description Length. In Proceedings of\nthe Thirty-eighth International Conference on Ma-\nchine Learning (ICML).\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nThang M. Pham, Trung Bui, Long Mai, and Anh\nNguyen. 2020. Out of Order: How important is the\nsequential order of words in a sentence in Natural\nLanguage Understanding tasks? arXiv:2012.15180\n[cs].\nTiago Pimentel, Naomi Saphra, Adina Williams, and\nRyan Cotterell. 2020a. Pareto probing: Trading off\naccuracy for complexity. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 3138–3153, On-\nline. Association for Computational Linguistics.\nTiago Pimentel, Josef Valvoda, Rowan Hall Maudslay,\nRan Zmigrod, Adina Williams, and Ryan Cotterell.\n2020b. Information-theoretic probing for linguistic\nstructure. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4609–4622, Online. Association for Computa-\ntional Linguistics.\nAdam Poliak, Jason Naradowsky, Aparajita Haldar,\nRachel Rudinger, and Benjamin Van Durme. 2018.\nHypothesis only baselines in natural language in-\nference. In Proceedings of the Seventh Joint Con-\nference on Lexical and Computational Semantics ,\npages 180–191, New Orleans, Louisiana. Associa-\ntion for Computational Linguistics.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nJorma Rissanen. 1984. Universal coding, information,\nprediction, and estimation. IEEE Transactions on\nInformation theory, 30(4):629–636.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in BERTology: What we know\nabout how BERT works. Transactions of the Associ-\nation for Computational Linguistics, 8:842–866.\nJulian Salazar, Davis Liang, Toan Q. Nguyen, and\nKatrin Kirchhoff. 2020. Masked Language Model\nScoring. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 2699–2712, Online. Association for Computa-\ntional Linguistics.\nChristian Scheible and Hinrich Schütze. 2013. Cut-\nting recursive autoencoder trees. In 1st Inter-\nnational Conference on Learning Representations\n(ICLR) Scottsdale, Arizona, USA, May 2-4, 2013,\nConference Track Proceedings.\nSheng Shen, Alexei Baevski, Ari Morcos, Kurt Keutzer,\nMichael Auli, and Douwe Kiela. 2021. Reservoir\ntransformers. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 4294–4309, Online. Association for\nComputational Linguistics.\nNatalia Silveira, Timothy Dozat, Marie-Catherine\nDe Marneffe, Samuel R Bowman, Miriam Connor,\nJohn Bauer, and Christopher D Manning. 2014. A\ngold standard dependency corpus for english. In\nLREC, pages 2897–2904. Citeseer.\nKoustuv Sinha, Prasanna Parthasarathi, Joelle Pineau,\nand Adina Williams. 2021. UnNatural Language In-\nference. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Nat-\nural Language Processing (Volume 1: Long Papers),\npages 7329–7346, Online. Association for Computa-\ntional Linguistics.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng,\nand Christopher Potts. 2013. Recursive deep mod-\nels for semantic compositionality over a sentiment\ntreebank. In Proceedings of the 2013 conference on\nempirical methods in natural language processing ,\npages 1631–1642.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4593–\n4601, Florence, Italy. Association for Computational\nLinguistics.\nMasatoshi Tsuchiya. 2018. Performance impact\ncaused by hidden bias of training data for recog-\nnizing textual entailment. In Proceedings of the\nEleventh International Conference on Language Re-\nsources and Evaluation (LREC 2018) , Miyazaki,\n2901\nJapan. European Language Resources Association\n(ELRA).\nDmitry Ulyanov, Andrea Vedaldi, and Victor Lempit-\nsky. 2018. Deep image prior. In Proceedings of\nthe IEEE conference on computer vision and pattern\nrecognition, pages 9446–9454.\nElena V oita and Ivan Titov. 2020. Information-\ntheoretic probing with minimum description length.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 183–196, Online. Association for Computa-\ntional Linguistics.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Pro-\nceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP , pages 353–355, Brussels, Belgium.\nAssociation for Computational Linguistics.\nAlex Warstadt, Yu Cao, Ioana Grosu, Wei Peng, Ha-\ngen Blix, Yining Nie, Anna Alsop, Shikha Bordia,\nHaokun Liu, Alicia Parrish, Sheng-Fu Wang, Jason\nPhang, Anhad Mohananey, Phu Mon Htut, Paloma\nJeretic, and Samuel R. Bowman. 2019a. Investi-\ngating BERT’s knowledge of language: Five anal-\nysis methods with NPIs. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 2877–2887, Hong Kong,\nChina. Association for Computational Linguistics.\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-\nhananey, Wei Peng, Sheng-Fu Wang, and Samuel R.\nBowman. 2020a. BLiMP: A benchmark of linguis-\ntic minimal pairs for English. In Proceedings of the\nSociety for Computation in Linguistics 2020 , pages\n409–410, New York, New York. Association for\nComputational Linguistics.\nAlex Warstadt, Amanpreet Singh, and Samuel R Bow-\nman. 2019b. Neural network acceptability judg-\nments. Transactions of the Association for Compu-\ntational Linguistics, 7:625–641.\nAlex Warstadt, Yian Zhang, Xiaocheng Li, Haokun\nLiu, and Samuel R. Bowman. 2020b. Learning\nwhich features matter: RoBERTa acquires a prefer-\nence for linguistic generalizations (eventually). In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 217–235, Online. Association for Computa-\ntional Linguistics.\nJohn Wieting and Douwe Kiela. 2019. No training\nrequired: Exploring random encoders for sentence\nclassiﬁcation. In 7th International Conference on\nLearning Representations, ICLR 2019, New Orleans,\nLA, USA, May 6-9, 2019. OpenReview.net.\nAdina Williams, Andrew Drozdov, and Samuel R Bow-\nman. 2018a. Do latent tree learning models iden-\ntify meaningful structure in sentences? Transac-\ntions of the Association for Computational Linguis-\ntics, 6:253–267.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018b. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers) , pages 1112–1122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nThomas Wolf. 2019. Some additional experiments ex-\ntending the tech report ”Assessing BERT’s Syntactic\nAbilities” by Yoav Goldberg. page 7.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V .\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, Jeff Klingner, Apurva Shah, Melvin John-\nson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws,\nYoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith\nStevens, George Kurian, Nishant Patil, Wei Wang,\nCliff Young, Jason Smith, Jason Riesa, Alex Rud-\nnick, Oriol Vinyals, Greg Corrado, Macduff Hughes,\nand Jeffrey Dean. 2016. Google’s neural machine\ntranslation system: Bridging the gap between human\nand machine translation.\nSaining Xie, Alexander Kirillov, Ross Girshick, and\nKaiming He. 2019. Exploring randomly wired neu-\nral networks for image recognition. In Proceedings\nof the IEEE/CVF International Conference on Com-\nputer Vision, pages 1284–1293.\nLang Yu and Allyson Ettinger. 2021. On the interplay\nbetween ﬁne-tuning and composition in transform-\ners. In Findings of the Association for Computa-\ntional Linguistics: ACL-IJCNLP 2021 , pages 2279–\n2293, Online. Association for Computational Lin-\nguistics.\nTianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q.\nWeinberger, and Yoav Artzi. 2021. Revisiting few-\nsample BERT ﬁne-tuning. In 9th International Con-\nference on Learning Representations, ICLR 2021,\nVirtual Event, Austria, May 3-7, 2021 . OpenRe-\nview.net.\nYu Zhang, Zhenghua Li, and Min Zhang. 2020. Efﬁ-\ncient second-order TreeCRF for neural dependency\nparsing. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 3295–3305, Online. Association for Computa-\ntional Linguistics.\nYuan Zhang, Jason Baldridge, and Luheng He. 2019.\nPAWS: Paraphrase adversaries from word scram-\nbling. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n2902\n1298–1308, Minneapolis, Minnesota. Association\nfor Computational Linguistics.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In Proceedings of the IEEE inter-\nnational conference on computer vision , pages 19–\n27.\n2903\nA From Word2vec to BERT in 4 steps\nTake the basic parameterization of skipgram\nword2vec (Mikolov et al., 2013):\np(t|w; θ) = ef(t,w)\n∑\nt′∈V ef(t′,w) (1)\nwhere tis the target, wis a word in the context,\nV is the set of all possible context words and f is\nsimply the dot product.\nIn actual word2vec, we would use negative sam-\npling within a given window size and optimize\nlog σ(w·t)+k·Et′∈P log σ(−w·t′) computed over\ncontext C(w) = {wi−k,...,w i−1,wi+1,wi+k}for\nword index i, window size 2kand unigram proba-\nbility distribution P. It has been shown that opti-\nmizing this objective is close to learning the shifted\nPPMI distribution (Levy et al., 2015).\nStep 1: BPE One reason for not computing the\nfull softmax is that it becomes a prohibitively ex-\npensive matrix multiplication with large vocabu-\nlary V. A solution is to tokenize based on subword\nunits, e.g. BPE, to ensure a smaller total vocabulary\nU in the softmax denominator. Doing so makes the\nmatrix multiplication feasible, at least on GPU. It\nalso ensures we have sufﬁcient coverage over the\nwords in our vocabulary.\nStep 2: Defenestration Next, replace the local\ncontext window with the entire sentence, while\nmasking out the target word, i.e., C(t) = {w ∈\nS : w̸= t}where Sis the sentence containing w.\nStep 3: Non-linearity Replace the pairwise\nword-level dot product f(w,t) with a fancy non-\nlinear function, say a sequence of multi-head self\nattention layers, g(t,C(t)), that takes as input the\nentire sentence-with-mask, and you get:\np(t|C(t); θ) = eg(t,C(t))\n∑\nt′∈U eg(t′,C(t))\nStep 4: Sprinkle data and compute You have\nBERT. Now all you need is enough data and com-\npute, and perhaps some optimization tricks. Make\nsure to update the parameters in your modelgwhen\nﬁne-tuning, rather than keeping them ﬁxed, for op-\ntimal performance on downstream tasks.\nThis correspondence is probably (hopefully) triv-\nial to most NLP researchers, but worth pointing out,\nlest we forget.\nBLEU-2 BLEU-3 BLEU-4\nM1 0.493 +/- 0.12 0.177 +/- 0.16 0.040 +/- 0.11\nM2 0.754 +/- 0.07 0.432 +/- 0.18 0.226 +/- 0.19\nM3 0.824 +/- 0.06 0.650 +/- 0.09 0.405 +/- 0.20\nM4 0.811 +/- 0.08 0.671 +/- 0.11 0.553 +/- 0.12\nTable 5: BLEU-2,3,4 scores (mean and std dev) on a\nsample of 1M sentences drawn from the corpus used to\ntrain M1, M2, M3and M4 compared to MN.\nB Data generation\nWe provide pseudo-code for Fi in Algorithm 1.\nFollowing Sinha et al. (2021), we do not explic-\nitly control whether the permuted words maintain\nany of their original neighbors. Thus, a certain\namount of extra n-grams are expected to co-occur,\npurely as a product of random shufﬂing. We quan-\ntify the amount of such shufﬂing on a sample of 1\nmillion sentences drawn from the BookWiki ran-\ndom corpus, and present the BLEU-2, BLEU-3 and\nBLEU-4 scores in Table 5. We provide a sample\nsnapshot of the generated data in Table 18.\nAlgorithm 1 SentenceRandomizer\n1: procedure F(S, t, n) ⊿ Randomize a sentence S with\nseed t and n grams n\n2: W = tokenize the words in S\n3: Set the seed to t\n4: if n >1 then\n5: while True do\n6: K = Sample all possible starting points from\n[0, |W|−n]\n7: Ignore the starting points in K which overlap\nwith conjoined tokens ⊿ Conjoined tokens consists of\njoined unigrams\n8: if |K|≥ 1 then\n9: Sample one position p ∈K\n10: g = Extract the n-gram W[p : p + n]\n11: Delete W[p + 1 :p + n]\n12: W[p] = Convert g to a conjoined token\n13: else\n14: Break from While loop\n15: while True do\n16: ˆW = randomly shufﬂe tokens in W\n17: r = ∑( ˆW[i] =W[i]) ⊿ Count number of\npositions where the token remains in its original position\n18: if r = 0then Break out of While loop\n19: ˆS = join the tokens in ˆW\n20: Return ˆS\nC Pre-training details\nWe use the Fairseq (Ott et al., 2019) toolkit to pre-\ntrain RoBERTa (base) models on the different vari-\nants of the BookWiki corpus. We follow the default\nparameters as reported in Liu et al. (2019), with the\nfollowing adjustments: max steps 100k, warmup\n2904\nNP\n UG\n 1\n 2\n 3\n 4\n N\nModels\n0\n100\n200\n300Perplexity\n111.74\n30.96 19.78 16.95\n339.44\n4.6\n311.45\n129.11\n33.25 20.99 17.81\n363.85\n4.7\n332.38 mode\nvalid\ntest\nFigure 2: Perplexity of various models on Wiki 103\nvalid and test sets.\nUG\n UF\n NP\n RI\n 1\n 2\n 3\n 4\n N\nModels\n0\n20\n40\n60\n80Performance\nLow Distributional Prior High Distributional Prior\nMNLI\nQNLI\nRTE\nQQP\nSST-2\nMRPC\nCoLA\nPAWS\nFigure 3: GLUE results on various model ablations us-\ning BookWiki corpus.\nsteps: 20k. We use the Wiki 103 validation and\ntest set to validate and test the array of pre-trained\nmodels, as validation on this small dataset is quick,\neffective, and reproducible for comparison among\npublicly available datasets (Figure 2). We observe\nthat perplexity monotonically increases from MN,\nthrough M4–M1, to MUG, and ﬁnally MNP.\nD Word-order pre-training ablations\nWe also train further model ablations with low to\nhigh distributional priors. Following the construc-\ntion of the corpus bootstrap resample, we train\na model where words are drawn uniformly from\nBookWiki corpus, thus destroying the natural fre-\nquency distribution (MUF). We further study an ab-\nlation for a high distributional prior, M512, where\nwe shufﬂe words (unigram) in a buffer created with\njoining multiple sentences such that maximum to-\nken length of the buffer is 512. This ablation—\nwhich is similar to the paragraph word shufﬂe con-\ndition in Gauthier and Levy (2019)—will allow us\nto study the effect of unigram shufﬂing in a window\nlarger than the one for M1. Buffer size is chosen\nto be 512 because BERT/RoBERTa is typically\ntrained with that maximum sequence length.\nWe observe dev set results on the GLUE bench-\nmark of these ablations, along with baselines MUG,\nMRI and MNP and random shufﬂes in Table 6 and\nFigure 3. We observe that M512 exhibits worse\noverall scores than M1, however it is still signif-\nicantly better than MNP or MUG baselines. We\nobserve that destroying the natural frequency dis-\ntribution of words ( MUF) yields comparable or\nslightly better results compared to random corpus\nmodel MUG. This result shows that merely repli-\ncating the natural distribution of words without any\ncontext is not useful for the model to learn. These\nresults indicate that at least some form of distribu-\ntional prior is required for MLM-based models to\nlearn a good downstream representation.\nOne might argue that the superior results dis-\nplayed by the unnatural models is due to the ability\nof RoBERTa to “reconstruct” the natural word or-\nder from shufﬂed sentences. The data generation\nalgorithm, Fi requires a seed tfor every sentence.\nIn our experiments, we had set the same seed for ev-\nery sentence in the corpus to ensure reproducibility.\nHowever, it could be problematic if the sentences\nof the same length are permuted with the same seed,\nwhich could be easier for the model to “reconstruct”\nthe natural word order to learn the necessary syntax.\nWe tested this hypothesis by constructing a new cor-\npus with different seeds for every sentence in every\nshard in the corpus (1/5th of BookWiki corpus is\ntypically referred to as a shard for computational\npurposes), to build the model M1∗. We observe\nthat there is minimal difference in the raw num-\nbers among M1 and M1∗for most of the tasks\n(Table 7) (with the exception of CoLA which per-\nforms similar toM2 possibly due to a difference in\ninitialization). This result consequently proves that\neven with same seed, it is difﬁcult for the model\nto just reconstruct the unnatural sentences during\npre-training.\nE Measuring Relative difference\nIn this section, we further measure the difference\nin downstream task performance reported in §4.1\nusing as a metric the relative difference. Let us de-\nnote the downstream task performance as A(T|D),\nwhere T is the task and Dis the pre-trained model.\nWe primarily aim to evaluate the relative perfor-\n2905\nModel QNLI RTE QQP SST-2 MRPC PAWS MNLI-m/mm CoLA\nMN 92.45 +/- 0.2 73.62 +/- 3.1 91.25 +/- 0.1 93.75 +/- 0.4 89.09 +/- 0.9 94.49 +/- 0.2 86.08 +/- 0.2 / 85.4 +/- 0.2 52.45 +/- 21.2\nM4 91.65 +/- 0.1 70.94 +/- 1.2 91.39 +/- 0.1 92.46 +/- 0.3 86.90 +/- 0.3 94.26 +/- 0.2 83.79 +/- 0.2 / 83.94 +/- 0.3 35.25 +/- 32.2\nM3 91.56 +/- 0.4 69.75 +/- 2.8 91.22 +/- 0.1 91.97 +/- 0.5 86.22 +/- 0.8 94.03 +/- 0.1 83.83 +/- 0.2 / 83.71 +/- 0.1 40.78 +/- 23.0\nM2 90.51 +/- 0.1 70.00 +/- 2.5 91.33 +/- 0.0 91.78 +/- 0.3 85.90 +/- 1.2 93.53 +/- 0.3 83.45 +/- 0.3 / 83.54 +/- 0.3 50.83 +/- 5.80\nM1 89.05 +/- 0.2 68.48 +/- 2.5 91.01 +/- 0.0 90.41 +/- 0.4 86.06 +/- 0.8 89.69 +/- 0.6 82.64 +/- 0.1 / 82.67 +/- 0.2 31.08 +/- 10.0\nM512 84.97 +/- 0.3 56.09 +/- 0.6 90.15 +/- 0.1 86.11 +/- 0.7 79.41 +/- 0.6 77.3 +/- 12.63 77.58 +/- 0.3 / 77.89 +/- 0.4 12.54 +/- 5.57\nMNP 77.59 +/- 0.3 54.78 +/- 2.2 87.78 +/- 0.4 83.21 +/- 0.6 72.78 +/- 1.6 57.22 +/- 1.2 63.35 +/- 0.4 / 63.63 +/- 0.2 2.37 +/- 3.20\nMUF 77.69 +/- 0.4 53.84 +/- 0.6 85.92 +/- 0.1 84.00 +/- 0.6 71.35 +/- 0.8 58.43 +/- 0.3 72.10 +/- 0.4 / 72.58 +/- 0.4 8.89 +/- 1.40\nMUG 66.94 +/- 9.2 53.70 +/- 1.0 85.57 +/- 0.1 83.17 +/- 1.5 70.57 +/- 0.7 58.59 +/- 0.3 71.93 +/- 0.2 / 71.33 +/- 0.5 0.92 +/- 2.10\nMRI 62.17 +/- 0.4 52.97 +/- 0.2 81.53 +/- 0.2 82.0 +/- 0.7 70.32 +/- 1.5 56.62 +/- 0.0 65.70 +/- 0.2 / 65.75 +/- 0.3 8.06 +/- 1.60\nTable 6: GLUE and PAWS-Wiki dev set results on different ablations of the RoBERTa (base) models, trained on\nvariants of the BookWiki corpus (with mean and std dev). The top row is the original model, the middle half\ncontains the sentence randomization models, and the bottom half contains the ablations.\nModel RTE MRPC SST-2 CoLA QQP QNLI MNLI PAWS\nM1 68.48 85.97 90.41 31.07 91.01 89.05 82.64 89.69M1∗ 68.41 85.75 90.17 50.14 91.02 89.50 82.92 91.99\nTable 7: Reconstruction experiments on shufﬂed word\norder sentences by ﬁxing the same seed for every sen-\ntence ( M1) and having different seed for different\nshards of the corpus ( M1∗). We observe minimal dif-\nference in the downstream GLUE and PAWS scores.\nModel QNLI RTE QQP SST-2 MRPC CoLA PAWS MNLI\nM1 3.70 7.04 0.26 3.58 3.42 40.74 5.12 3.62M2 2.11 4.95 -0.09 2.12 3.61 3.09 9.06 2.63M3 0.97 5.30 0.03 1.91 3.24 22.25 0.49 2.31M4 0.87 3.67 -0.15 1.39 2.47 32.79 0.25 2.19\nMUG 27.74 27.25 6.26 11.35 20.91 98.24 38.20 16.56MNP 16.16 25.77 3.83 11.30 18.42 95.48 39.66 26.10\nTable 8: ∆{Di}(T), scaled by a factor of 100 for GLUE\nand PAWS tasks.\nmance gap, i.e. how much the performance differs\nbetween our natural and unnatural models. Thus,\nwe deﬁne the Relative Difference (∆{D}(T)):\n∆{D}(T) = A(T|OR) −A(T|D))\nA(T|OR) −A(T|∅) , (2)\nwhere A(T|∅) is the random performance on the\ntask T (0.33 for MNLI, 0 for CoLA, and 0.5 for\nrest) ∆{D}(T) →0 when the performance of a\npre-trained model reaches that of the pre-trained\nmodel trained with natural word order.\nWe observe the relative difference on the tasks\nin Table 8. CoLA has the largest ∆{D}(T) among\nall tasks, suggesting the expected high word order\nreliance. ∆{D}(T) is lowest for QQP.\nF Fine-tuning with randomized data\nWe perform additional experiments using the ﬁne-\ntuned models from §4.1. Speciﬁcally, we construct\nunigram randomized train and test sets (denoted as\nshufﬂed) of a subset of tasks to evaluate whether\nmodels ﬁne-tuned on natural or unnatural task data\n(having natural or unnatural pre-training prior) are\nable to understand unnatural data during testing.\nSinha et al. showed for MNLI there exists at least\none permutation for many examples which can be\npredicted correctly by the model. However, they\nalso showed that every sentence can have many\npermutations which cannot be predicted correctly\nas well. We follow them in this evaluation, and\nconstruct 100 permutations for each example in the\ndev set for each task to capture the overall accuracy.\nConcretely, we use MN, M1 and MUG as our\npre-trained representations (trained with natural,\nunigram sentence shufﬂe and corpus shufﬂe data\nrespectively) and evaluate the effect of training and\nevaluation on natural and unnatural data in Table 9.\nWe observe that all models perform poorly on the\nshufﬂed test set, compared to natural evaluation.\nHowever, interestingly, models have a slight ad-\nvantage with a unigram randomized prior ( M1),\nwith CoLA having the biggest performance gain.\nPAWS task suffers the biggest drop in performance\n(from 94.49 to 62.22) but the lowest gain in M1,\nconﬁrming our conclusion from §4.1 that most of\nthe word order information necessary for PAWS is\nlearned from the task itself.\nFurthermore, training on shufﬂed data surpris-\ningly leads to high performance on natural data\nfor MN in case of several tasks, the effect being\nweakest in case of CoLA and PAWS. This suggests\nthat for tasks other than CoLA and PAWS, spuri-\nous correlations are leveraged by the models during\nﬁne-tuning (cf. Gururangan et al. 2018; Poliak et al.\n2018; Tsuchiya 2018). We also observe evidence\nof domain matching, where models improve their\nperformance on evaluation on shufﬂed data when\n2906\nname ﬁne-tune-train ﬁne-tune-eval MNLI QNLI RTE CoLA MRPC SST-2 PAWS\nMN natural natural 86.08 +/- 0.15 92.45 +/- 0.24 73.62 +/- 3.09 52.44 +/- 21.22 89.09 +/- 0.88 93.75 +/- 0.44 94.49 +/- 0.18natural shufﬂed 68.11 +/- 0.52 81.08 +/- 0.38 56.72 +/- 3.29 4.77 +/- 1.82 75.94 +/- 1.01 80.78 +/- 0.37 62.22 +/- 0.09shufﬂed natural 82.99 +/- 0.16 89.32 +/- 0.23 57.9 +/- 4.71 0.0 +/- 0.0 79.71 +/- 2.57 89.12 +/- 0.5 72.03 +/- 13.79shufﬂed shufﬂed 79.96 +/- 0.1 87.51 +/- 0.09 59.07 +/- 3.2 1.4 +/- 2.17 79.17 +/- 0.35 86.11 +/- 0.5 65.15 +/- 0.48\nM1 natural natural 82.64 +/- 0.15 89.05 +/- 0.15 68.48 +/- 2.51 31.07 +/- 9.97 85.97 +/- 0.89 90.41 +/- 0.43 89.69 +/- 0.59natural shufﬂed 76.67 +/- 0.34 87.21 +/- 0.17 65.8 +/- 6.11 23.06 +/- 5.3 81.84 +/- 0.43 83.94 +/- 0.33 62.86 +/- 0.19shufﬂed natural 79.87 +/- 0.1 87.81 +/- 0.36 65.65 +/- 2.33 24.53 +/- 13.63 82.51 +/- 0.82 86.45 +/- 0.41 73.34 +/- 6.88shufﬂed shufﬂed 79.75 +/- 0.0 88.21 +/- 0.24 64.88 +/- 6.32 22.43 +/- 10.79 82.65 +/- 0.42 86.25 +/- 0.4 63.15 +/- 2.2\nMUG natural natural 71.93 +/- 0.21 66.94 +/- 9.21 53.7 +/- 1.01 0.92 +/- 2.06 70.57 +/- 0.66 83.17 +/- 1.5 58.59 +/- 0.33natural shufﬂed 62.27 +/- 0.57 63.13 +/- 7.13 52.42 +/- 2.77 0.09 +/- 0.21 70.56 +/- 0.33 79.41 +/- 0.63 56.91 +/- 0.16shufﬂed natural 67.62 +/- 0.3 66.49 +/- 0.49 52.17 +/- 1.26 0.0 +/- 0.0 70.37 +/- 0.93 79.93 +/- 1.01 57.59 +/- 0.29shufﬂed shufﬂed 67.02 +/- 0.33 66.24 +/- 0.33 53.44 +/- 0.53 0.08 +/- 0.18 70.28 +/- 0.62 80.05 +/- 0.4 57.38 +/- 0.16\nTable 9: Fine-tuning evaluation by varying different sources of word order (with mean and std dev). We vary the\nword order contained in the pre-trained model ( MN,M1,MUG); in ﬁne-tuning training set (natural and shufﬂed);\nand in ﬁne-tuning evaluation (natural and shufﬂed). Here, shufﬂed corresponds to unigram shufﬂing of words\nin the input. In case of ﬁne-tune evaluation containing shufﬂed input, we evaluate on a sample of 100 unigram\npermutations for each data point in the dev set of the corresponding task.\nModel UD EWT PTB\nUAS LAS UAS LAS\nMN 90.92% 87.87% 95.42% 93.75%\nM1 91.18% 88.19% 95.90% 94.35%\nM2 91.11% 88.12% 95.74% 94.16%\nM3 91.05% 87.94% 95.73% 94.14%\nM4 90.88% 87.78% 95.77% 94.16%\nMUG 90.47% 87.42% 95.81% 94.28%\nTable 10: Unlabeled Attachment Score (UAS) on De-\npendency parsing task on two datasets, UD EWT and\nPTB, using the Second order Tree CRF Neural Depen-\ndency Parser (Zhang et al., 2020)\nthe training data source is changed from natural to\nshufﬂed (for MN, MNLI shufﬂed evaluation im-\nproves from 68.11 to 79.96 just by changing the\ntraining corpus from natural to shufﬂed). We ob-\nserve this behavior consistently for all tasks with\nall pre-trained representations.\nG Dependency parsing using Second\norder Tree CRF Neural Dependency\nParser\nWe also conduct extensive experiments with Sec-\nond Order Tree CRF Neural Dependency parser\nfrom Zhang et al. (2020), using their provided code-\nbase.13 We report the results on UD EWT and PTB\ncorpus in Table 10. Strangely enough, we ﬁnd\nthe gap to be even smaller across the different ran-\ndomization models, even for some cases the perfor-\nmance on R1 improves over OR. We suspect this\nresult is due to two reasons:(a) Due to the presence\nof the complex Biafﬁne Dependency parser consist-\ning of multiple LSTMs and individual MLP heads\n13https://github.com/yzhangcs/parser\nN\n 1\n 2\n 3\n 4\n UG\nTrained models\nN\n1\n2\n3\n4\nTest sentences\n2.1 > 200 88.0 31.1 26.1 > 200\n> 200 104.1 90.5 98.8 73.2 96.2\n63.7 121.9 25.9 23.9 25.6 110.5\n32.0 115.8 28.8 9.7 13.8 101.3\n20.9 92.5 31.3 12.6 7.1 92.7\nFigure 4: BPPL scores per model per test scenario.\nfor each dependency arc (left and right), the major-\nity of learning of the task is done by the parser it-\nself; (b) Zhang et al. (2020) downsample the BERT\nrepresentation to 100 dimensions which is then\ncombined with the learned LSTM representations,\nthereby minimizing the impact of the pre-trained\nrepresentations. Our hypothesis is conﬁrmed by\nthe published results of Zhang et al. (2020) on the\nGithub repository, which shows a minimal gap be-\ntween models with or without BERT.\nH Perplexity analysis\nWe measure perplexity of various pre-trained\nrandomization models on text that is random-\nized using the same function F. Conven-\ntional language models compute the perplexity\n2907\nof a sentence S by using past tokens ( S<t =\n(w1,w2,...,w t−1)) and the application of chain\nrule (∑|S|\nt=1 log PLM(wt|St−1)). However, this for-\nmulation is not deﬁned for MLM, as a word is pre-\ndicted using the entire sentence as a context. Fol-\nlowing Salazar et al. (2020), we measure Pseudo-\nPerplexity, i.e., given a sentence S, we compute\nthe log-probability of the missing word in S by\niteratively masking out the speciﬁc word, and com-\nputing the average log-probability per word in S:\nPLL(S) = 1\n|S|\n∑\nw∈S\nlog PMLM(w|S\\w; θ) (3)\nWe bootstrap thePLL score of a test corpusT by\ndrawing 100 samples ﬁve times with replacement.\nWe also similarly compute the bootstrap perplexity\nfollowing Salazar et al.:\nBPLLT = exp(−1\nN\n∑\nS∈W\nPLL(S)), (4)\nwhere W is the combined bootstrap sample con-\ntaining N sentences drawn with replacement from\nT. We compute this score on 6 pre-trained mod-\nels, over four randomization schemes on the boot-\nstrapped sample W (i.e., we use the same n-gram\nrandomization function Fi). Thus, we obtain a 5x6\nmatrix of BPLL scores, which we plot in Figure 4.\nWe observe that the pre-trained model MN has\nthe lowest perplexity on the sentences with natural\nword order. Pre-trained models with random word\norder exhibit signiﬁcantly higher perplexity than\nthe normal word order sentences (top row). With\nthe exception of M1, the models pre-trained on\nrandomized data (M2, M3 and M4) all display\nthe lowest perplexity for their respectiven= 2,3,4\nrandomizations. These results indicate that the\nmodels retain and detect the speciﬁc word order for\nwhich they were trained.\nI The usefulness of word order\nThe results in §4.1 suggest that, with proper ﬁne-\ntuning, an unnaturally trained model can reach a\nlevel of performance comparable to that of a nat-\nurally pre-trained model. However, we want to\nunderstand whether natural word order pre-training\noffers any advantage during the early stages of ﬁne-\ntuning. Towards that end, we turn to compute the\nMinimum Description Length (MDL; Rissanen,\n1984). MDL is designed to characterize the com-\nplexity of data as the length of the shortest program\nrequired to generate it. Thus, the length of the\nminimum description (in bits) should provide a\nfair estimate of how much word order is useful for\nﬁne-tuning in a few-shot setting. Speciﬁcally, we\nleverage the Rissanen Data Analysis (RDA) frame-\nwork from Perez et al. (2021) to evaluate the MDL\nof pre-trained models on our set of downstream\ntasks. Under mild assumptions, if a pre-trained\nmodel θ1 is useful for solving a particular task T\nover θ2, then the MDL in bits obtained by using θ1\nshould be shorter than θ2. We follow the experi-\nmental setup of Perez et al. to compute the MDL on\nseveral tasks using θ= {MN,M1,M2,M3,M4},\nover three seeds and on three epochs of training.\nConcretely, RDA involves sampling 9 blocks of\ndata from the dataset at random, where the size\nof each block is increased monotonically, training\non 8 blocks while evaluating the model’s loss (or\ncodelength) on the ninth. The minimum number\nof data samples in the smallest block is set at 64,\nwhile the largest number of data samples used in\nthe last block is 10,000.\nWe observe that the value of MDL is consis-\ntently lowest for naturally pre-trained data (Fig-\nure 5). For purportedly word order reliant datasets\nsuch as RTE, CoLA and PAWS, the gap between\nthe MDL scores among the natural and unnatural\nmodels is high. PAWS, speciﬁcally, has the largest\nadvantage in the beginning of optimization, how-\never with more ﬁne-tuning, the model re-learns cor-\nrect word order (§4.1). The present analyses, when\ntaken in conjunction with our main results in §4.1,\nsuggest that ﬁne-tuning on large training datasets\nwith complex classiﬁers in the pursuit of state-of-\nthe-art results has mostly nulliﬁed the impact of\nword order in the pre-trained representations. Few\nshot (Bansal et al., 2020) and few sample (Zhang\net al., 2021) learning and evaluation could poten-\ntially require more word order signal, thereby en-\ncouraging the model to leverage its own learned\nsyntax better.\nJ At what point do models learn word\norder during pre-training?\nResults from §4.1 beg the question: when, if at\nall, during pre-training does a model learn the nat-\nural word order? We aim to answer that question\nby comparing downstream task performance of\nRoBERTa base on intermediate checkpoints with\n2908\n12.50\n13.00\n13.50\n14.00MDL\nMNLI\n7.00\n7.20\n7.40\n7.60\n7.80\n8.00\nQQP\n6.60\n6.80\n7.00\n7.20\n7.40\n7.60\nQNLI\n7.20\n7.40\n7.60\nCoLA\nN\n 1\n 2\n 3\n 4\n5.50\n6.00\n6.50\n7.00MDL\nSST-2\nN\n 1\n 2\n 3\n 4\n2.70\n2.75\n2.80\n2.85\n2.90\nMRPC\nN\n 1\n 2\n 3\n 4\n2.44\n2.45\n2.46\n2.47\n2.48\n2.49\nRTE\nN\n 1\n 2\n 3\n 4\n8.50\n9.00\n9.50\n10.00\nPAWS\nFigure 5: Rissanen Data Analysis (Perez et al., 2021) on the GLUE benchmark and PAWS datasets. The lower\nminimum description length (MDL, measured in kilobits), the better the learning ability of the model.\n1 25 50 75 103\n70\n75\n80\n85\nMNLI\n1 25 50 75 103\n84\n86\n88\n90\nQQP\n1 25 50 75 103\n70\n80\n90\nQNLI\n1 25 50 75 103\n0\n20\n40\n60\nCoLA\n1 25 50 75 103\n85\n90\nSST-2\n1 25 50 75 103\n75\n80\n85\n90\nMRPC\n1 25 50 75 103\n55\n60\n65\n70\n75\nRTE\n1 25 50 75 103\n60\n70\n80\n90\nPAWS\n0.0 0.2 0.4 0.6 0.8 1.0\nEpochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Task Performance\nBy training epochs\nR=1\nR=2\nR=3\nR=4\nFigure 6: Comparison among GLUE task performance\nfrom different steps in pre-training of RoBERTa on\nBookWiki Corpus.\nthat of the random word order pretrained models.\nThe idea is to ﬁnd the point during pre-training on\nnatural corpus at which the model exceeds the task\nperformance of the random pre-training model.\nPerformance on all tasks (Figure 6) increases\nrapidly during the ﬁrst 20-25 epochs of pre-training.\nFor some tasks, the word order information only\nhelps after 30-50 pre-training epochs.\nModel UD EWT PTB\nMLP Linear MLP Linear\nMN 93.74 +/- 0.15 88.82 +/- 0.4297.07 +/- 0.38 93.1 +/- 0.65\nM1 88.60 +/- 3.43 80.76 +/- 3.3895.33 +/- 0.37 87.83 +/- 1.86\nM2 93.39 +/- 0.45 87.58 +/- 1.0696.96 +/- 0.15 91.80 +/- 0.50\nM3 92.89 +/- 0.65 86.78 +/- 1.3297.03 +/- 0.13 91.70 +/- 0.70\nM4 92.83 +/- 0.61 87.23 +/- 0.7796.96 +/- 0.12 92.08 +/- 0.39\nMUG 89.10 +/- 0.21 79.75 +/- 0.594.12 +/- 0.01 84.15 +/- 0.51\nTable 11: Accuracy on the part-of-speech labelling task\n(POS) on two datasets, UD EWT and PTB, using the\nPareto Probing framework (Pimentel et al., 2020b).\nModel UD EWT PTB\nMLP Linear MLP Linear\nMN 89.63 +/- 0.60 84.35 +/- 0.7893.96 +/- 0.63 88.35 +/- 1.00\nM1 83.55 +/- 3.31 75.26 +/- 3.0891.10 +/- 0.38 82.34 +/- 1.37\nM2 88.57 +/- 0.68 82.05 +/- 1.1093.27 +/- 0.26 86.88 +/- 0.87\nM3 88.69 +/- 1.09 82.37 +/- 1.2693.46 +/- 0.29 87.12 +/- 0.72\nM4 88.66 +/- 0.76 82.58 +/- 1.0493.49 +/- 0.33 87.30 +/- 0.79\nMUG 84.93 +/- 0.34 76.30 +/- 0.5289.98 +/- 0.43 78.59 +/- 0.68\nTable 12: Accuracy on the dependency arc labelling\ntask (DAL) on two datasets (with mean and std dev),\nUD EWT and PTB, using the Pareto Probing frame-\nwork (Pimentel et al., 2020a).\nK More results from Syntactic Probes\nWe computed the Pareto Hypervolume on the de-\npendency parsing task (Pimentel et al., 2020a). The\nPareto Hypervolume is computed as the Area Un-\nder Curve (AUC) score over all hyperparameter\nruns, where the models are arranged based on their\ncomplexity. We observe minimal differences in the\nPareto Hypervolumes (Table 13) among MN and\n2909\nModel UD EWT PTB\nMN 0.528 +/- 0.010.682 +/- 0.01\nM1 0.489 +/- 0.030.648 +/- 0.01\nM2 0.529 +/- 0.000.681 +/- 0.01\nM3 0.528 +/- 0.020.689 +/- 0.01\nM4 0.525 +/- 0.000.683 +/- 0.01\nMUG 0.510 +/- 0.010.640 +/- 0.05\nTable 13: Pareto Hypervolume of dependency parsing\ntask (DEP) on two datasets (with mean and std dev),\nUD EWT and PTB, using the Pareto Probing frame-\nwork (Pimentel et al., 2020b).\nthe randomization models for both datasets.\nWe also investigated two “easy” tasks, Part-of-\nSpeech tagging (POS) and Dependency Arc Label-\ning (DAL) from the Pareto Probing framework. For\nPOS (Table 11) and DAL (Table 12), since these\ntasks are simpler than DEP, the gap between MN\nand unnaturally pre-trained models reduces even\nmore drastically. The gap between MN and M1\nreduces to just 3.5 points on average for PTB in\nboth POS and DAL.\nL Non parametric probes\nProbability difference. In the original formula-\ntion (Goldberg, 2019; Wolf, 2019), the effective-\nness of each stimulus is determined by the accuracy\nmetric, computed as the number of times the prob-\nability of the correct focus word is greater than that\nof the incorrect word ( P(good) > P(bad)). We\nobserved that this metric might not be reliable per\nse, since the probabilities may themselves be ex-\ntremely low for all tokens, even when focus word\nprobability decreases drastically from MN to MUG.\nThus, we also report the mean difference of prob-\nabilities, ( 1\nN\n∑N\ni P(goodi) −P(badi)), scaled up\nby a factor of 100 for ease of observation, in Fig-\nure 9, Figure 8 and Figure 7. We observe the high-\nest difference between probabilities of the correct\nand incorrect focus words for the model pretrained\non the natural word order (MN). Moreover, with\neach step from M1 to M4, the difference between\nprobablities of correct and incorrect focus words in-\ncreases, albeit marginally, showing that pre-trained\nmodels with fewer n-gram words perturbed capture\nmore word order information. MUG, the model\nwith the distributional prior ablated, performs the\nworst, as expected.\nAccuracy comparison. We provide the accuracy\nas measured by Goldberg (2019); Wolf (2019) on\nthe probing stimuli in Table 14, Table 15 and Ta-\nN\n 1\n 2\n 3\n 4\n UG\n(P(good) - P(bad)) * 100\nLVC\nAOR\nAOR-T\nIOR-T\nIOR\nAPP\nISC\nARC\nSCM\nSVA\nSNP\nSRX\nASR\nSVC\n26.58 0.10 1.44 0.63 0.66 -0.00\n29.16 1.70 3.87 2.45 1.16 0.00\n13.84 0.15 0.43 0.25 0.55 -0.00\n0.20 0.01 0.01 0.01 0.03 -0.00\n0.40 0.01 0.04 0.04 0.08 -0.00\n27.06 0.68 0.20 0.79 0.91 -0.00\n5.27 0.07 0.00 0.31 0.06 0.00\n1.20 0.11 0.07 0.07 0.03 -0.00\n5.72 0.12 0.17 2.62 0.26 0.02\n31.35 0.57 3.17 6.15 30.57 0.02\n0.23 -0.00 0.00 1.73 0.08 -0.00\n2.72 0.10 0.32 0.02 3.07 0.00\n26.18 1.91 2.91 1.38 1.48 -0.00\n19.27 0.84 1.17 6.39 11.07 0.00\nFigure 7: The difference in word probabilities for stim-\nuli in Marvin and Linzen (2018): Simple Verb Agree-\nment (SV A), In a sentential complement (SCM), Short\nVP Coordination (SVC), Long VP Coordination (LVC),\nAcross a prepositional phrase (APP), Across a subject\nrelative clause (ASR), Across an object relative clause\n(AOR), Across an object relative (no that) (AOR-T),\nIn an object relative clause (IOR), In an object relative\nclause (no that) (IOR-T), Simple Reﬂexive (SRX), In a\nsentential complement (ISC), Across a relative clause\n(ARC), Simple NPI (SNP).\nble 16. We also highlight the difference in proba-\nbility (P(good) −P(bad)) in the table to provide\na more accurate picture. All experiments were con-\nducted on three pre-trained seeds for each model\nin our set of models. However, the low token\nprobabilities in MUG tend to present unreliable\nscores. For example, in the case of Gulordava\net al. (2018b) stimuli, unnatural models provide\nbetter scores compared to the natural model. We\nalso observe for the Linzen et al. (2016) stimuli\nthat the results on model condition 4 (number of\nattractors) are surprisingly high for MUG whereas\nthe individual token probabilities are lowest. We\nbelieve these inconsistencies stem from extremely\nlow token probabilities themselves.\nBalancing datasets on inﬂection by upsampling.\nThe stimuli datasets of Linzen et al. (2016) and\nGulordava et al. (2018b) turned out to be heavily\nskewed towards words where singular was the cor-\n2910\nmodel MN MUG M1 M2 M3 M4\ncondition\n1 93.45 (0.89) [25.04] 58.87 (0.41) [0.0] 59.96 (1.58) [0.08] 63.63 (0.6) [1.25] 64.7 (1.44) [2.79] 70.47 (1.9) [4.01]\n2 92.8 (1.22) [23.8] 63.03 (1.35) [0.01] 58.22 (1.5) [0.09] 61.15 (2.07) [0.82] 63.84 (2.41) [2.09] 64.7 (1.92) [3.07]\n3 87.71 (1.34) [22.03] 64.06 (3.52) [0.0] 56.69 (2.98) [0.03] 56.83 (3.63) [0.85] 61.1 (0.32) [2.02] 63.0 (3.36) [2.35]\n4 92.67 (0.52) [22.16] 76.33 (1.38) [0.0] 62.33 (7.61) [0.08] 63.17 (9.09) [1.12] 69.42 (1.77) [2.1] 67.67 (7.02) [3.43]\nTable 14: Linzen et al. (2016) stimuli results in raw accuracy. Values in parenthesis reﬂect the standard deviation\nover different seeds of pre-training. Values in square brackets indicate the mean probability difference among\ncorrect and incorrect words.\nmodel MN MUG M1 M2 M3 M4\ncondition\n0 79.42 (5.5) [2.43] 47.83 (3.76) [-0.0] 53.67 (1.38) [0.03] 58.75 (6.38) [0.05] 63.58 (4.11) [0.14] 63.75 (3.28) [0.17]\n1 72.83 (4.07) [2.55] 44.5 (0.5) [0.0] 70.83 (5.8) [0.02] 64.83 (1.76) [-0.09] 71.67 (6.71) [0.21] 71.5 (2.65) [0.61]\n2 55.56 (0.0) [0.92] 88.89 (11.11) [0.0] 81.48 (12.83) [0.03] 51.85 (6.42) [0.04] 62.96 (6.42) [0.38] 74.07 (16.97) [0.61]\nTable 15: Gulordava et al. (2018b) stimuli results in raw accuracy.Values in parenthesis reﬂect the standard devia-\ntion over different seeds of pre-training. Values in square brackets indicate the mean probability difference among\ncorrect and incorrect words.\nN\n 1\n 2\n 3\n 4\n UG\n(P(good) - P(bad)) * 100\n1\n2\n3\n4\n25.01 0.08 1.25 2.79 4.00 0.00\n23.74 0.09 0.81 2.08 3.06 0.02\n21.96 0.03 0.84 2.01 2.34 0.00\n22.16 0.08 1.12 2.10 3.43 0.00\nFigure 8: Linzen et al. (2016)\nrect inﬂection (as opposed to plural). This dataset\nimbalance caused the weak models (such as MUG)\nto have surprisingly high scores - the weak models\nwere consistently providing higher probability for\nthe singular inﬂection (Table 17). We upsample for\nboth datasets, balancing the frequency of correct\nsingular and plural inﬂections. We compute the up-\nsampling number to the next multiple of 100 of the\ncount of original singular inﬂections. For example,\nin condition 4 of Linzen et al. (2016) dataset, we\nupsample both S and P to 300 rows each. This type\nof balancing via upsampling largely alleviated the\ninconsistencies we observed, and might prove to\nbe useful when evaluating other models on these\ndatasets in future.\nN\n 1\n 2\n 3\n 4\n UG\n(P(good) - P(bad)) * 100\n0\n1\n2\n2.41 0.03 0.05 0.14 0.17 -0.00\n2.55 0.02 -0.09 0.21 0.61 0.00\n0.92 0.03 0.04 0.38 0.61 0.00\nFigure 9: Gulordava et al. (2018b)\n2911\nModel MN MUG M1 M2 M3 M4\ncondition\nAOR 89.98 (1.96) [29.16] 50.0 (0.01) [0.0] 60.17 (1.61) [1.7] 66.61 (7.1) [3.87] 63.57 (2.39) [2.45] 61.26 (4.91) [1.16]\nAOR-T 77.4 (7.74) [13.84] 50.0 (0.0) [0.0] 78.88 (0.64) [0.15] 52.17 (2.14) [0.43] 48.85 (3.8) [0.25] 57.06 (3.49) [0.55]\nAPP 89.94 (4.16) [27.06] 50.01 (0.02) [-0.0] 70.34 (1.9) [0.68] 53.61 (3.3) [0.2] 53.03 (1.75) [0.79] 60.6 (4.41) [0.91]\nARC 85.06 (5.92) [1.2] 50.05 (0.08) [-0.0] 62.39 (1.91) [0.11] 74.57 (5.99) [0.07] 67.55 (3.84) [0.07] 62.88 (3.45) [0.03]\nASR 87.19 (3.58) [26.18] 50.0 (0.0) [-0.0] 78.55 (10.01) [1.91] 81.73 (5.1) [2.91] 62.8 (0.35) [1.38] 67.23 (6.82) [1.48]\nIOR 89.83 (3.33) [0.4] 50.55 (0.95) [-0.0] 56.28 (2.66) [0.01] 58.96 (4.28) [0.04] 70.49 (2.2) [0.04] 62.82 (8.51) [0.08]\nIOR-T 74.05 (8.26) [0.2] 50.61 (1.05) [-0.0] 52.63 (2.07) [0.01] 57.35 (4.88) [0.01] 61.85 (4.75) [0.01] 55.16 (6.59) [0.03]\nISC 85.87 (9.6) [5.27] 50.0 (0.0) [0.0] 67.85 (2.62) [0.07] 82.66 (9.43) [0.0] 77.69 (4.51) [0.31] 68.65 (5.71) [0.06]\nLVC 93.0 (0.75) [26.58] 49.92 (0.14) [-0.0] 70.42 (6.79) [0.1] 87.5 (7.26) [1.44] 85.42 (3.84) [0.63] 81.08 (5.13) [0.66]\nSCM 88.6 (3.49) [5.72] 50.0 (0.0) [0.02] 63.73 (7.94) [0.12] 82.12 (0.92) [0.17] 86.44 (3.67) [2.62] 80.27 (2.46) [0.26]\nSRX 91.0 (6.07) [2.72] 50.0 (0.0) [0.0] 88.0 (10.11) [0.1] 92.25 (10.27) [0.32] 94.25 (5.02) [0.02] 91.0 (6.5) [3.07]\nSV A 95.33 (7.23) [31.35] 50.0 (0.0) [0.02] 86.0 (5.29) [0.57] 85.17 (12.87) [3.17] 94.67 (5.25) [6.15] 88.83 (9.57) [30.57]\nSVC 97.54 (1.58) [19.27] 50.0 (0.0) [-0.0] 83.58 (4.58) [0.84] 83.71 (8.78) [1.17] 93.29 (7.4) [6.39] 81.04 (3.66) [11.07]\nTable 16: Marvin and Linzen (2018) stimuli results in raw accuracy. Values in parenthesis reﬂect the standard\ndeviation over different seeds of pre-training. Values in square brackets indicate the mean probability difference\namong correct and incorrect words. Abbreviations: Simple Verb Agreement (SV A), In a sentential complement\n(SCM), Short VP Coordination (SVC), Long VP Coordination (LVC), Across a prepositional phrase (APP), Across\na subject relative clause (ASR), Across an object relative clause (AOR), Across an object relative (no that) (AOR-\nT), In an object relative clause (IOR), In an object relative clause (no that) (IOR-T), Simple Reﬂexive (SRX), In a\nsentential complement (ISC), Across a relative clause (ARC), Simple NPI (SNP).\nModel MN MUG M1 M2 M3 M4 S/P\ncondition\n1 94.04 (0.8) 62.64 (0.5) 62.18 (1.33) 64.91 (0.14) 65.35 (1.78) 70.88 (1.88) 14011 / 10112\n2 93.28 (0.94) 71.24 (0.85) 63.03 (1.69) 62.92 (2.57) 65.25 (3.13) 65.61 (2.35) 3120 / 1312\n3 89.1 (0.58) 74.05 (1.85) 62.94 (3.13) 59.18 (3.32) 63.54 (1.72) 63.05 (2.0) 733 / 215\n4 90.53 (0.9) 80.03 (0.59) 63.16 (4.83) 63.94 (6.92) 66.41 (3.17) 66.28 (4.64) 206 / 51\nTable 17: Linzen et al. (2016) stimuli results in raw accuracy on original, unbalanced data. Values in parenthesis\nreﬂect the standard deviation. S/P reﬂects the count of correct singular and plural focus words.\n2912\nOR R1 R2 R3 R4\n1 They are commonly known asdaturas, but also known asdevil’s trumpets, not to be con-fused with angel’s trumpets, itsclosely related genus \"Brug-mansia\".\nbe They angel’s also but trum-pets, genus related devil’s ascommonly closely known itsdaturas, trumpets, as \"Brugman-sia\". confused with known areto not\nas devil’s They genus not totrumpets, closely related \"Brug-mansia\". are commonly trum-pets, its also known known asbe confused daturas, but withangel’s\n\"Brugmansia\". related They arecommonly trumpets, its closelyas daturas, but known genusalso known as trumpets, con-fused with angel’s devil’s notto be\nits closely related genus Theyare commonly known trumpets,as trumpets, daturas, but alsoknown as \"Brugmansia\". not tobe confused with angel’s devil’s\n2 They are also sometimes calledmoonﬂowers, jimsonweed,devil’s weed, hell’s bells,thorn-apple, and many more.\nare devil’s bells, called weed,hell’s thorn-apple, and manyThey also more. moonﬂowers,jimsonweed, sometimes\nmore. They are hell’s bells, alsosometimes and many calledmoonﬂowers, jimsonweed,devil’s weed, thorn-apple,\njimsonweed, devil’s weed, Theyare also thorn-apple, and manybells, more. hell’s sometimescalled moonﬂowers,\nmoonﬂowers, They are alsosometimes bells, thorn-apple,and many more. called jimson-weed, devil’s weed, hell’s3 Its precise and natural distribu-tion is uncertain, owing to itsextensive cultivation and natu-ralization throughout the tem-perate and tropical regions ofthe globe.\nthroughout owing precise exten-sive temperate and naturaliza-tion and tropical of to naturalis its Its distribution cultivationthe globe. uncertain, regions theand\nand natural distribution is trop-ical to its and naturalizationthroughout the the temperateand globe. Its precise uncertain,owing extensive cultivation re-gions of\nuncertain, owing to Its preciseand its extensive cultivation ofglobe. natural distribution is thethe and tropical regions and nat-uralization throughout temper-ate\nglobe. Its precise and naturalcultivation distribution the is un-certain, owing to its extensiveand naturalization throughoutthe temperate and tropical re-gions of4 Its distribution within the Amer-icas and North Africa, how-ever, is most likely restrictedto the United States, Mexicoand Southern Canada in NorthAmerica, and Tunisia in Africawhere the highest species diver-sity occurs.\ndistribution Mexico occurs.likely diversity North however,species most the Tunisia wherein and and North Canada South-ern America, highest AfricaUnited the and in Americas Itswithin States, is to the restrictedAfrica,\nand Tunisia the Americas dis-tribution within Mexico andis most United States, Africa,however, Africa where in NorthIts and North in SouthernCanada America, the to thelikely restricted occurs. highestspecies diversity\nlikely Its highest species di-versity United States, Mexicorestricted to the Africa wherethe occurs. distribution withinthe and Tunisia in however, ismost Americas and SouthernCanada and North Africa, inNorth America,\nTunisia occurs. Its distribu-tion within the Africa wherethe highest in restricted to theUnited Canada in North Amer-ica, most North Africa, how-ever, is and Americas likelydiversity States, Mexico andSouthern species and5 All species of \"Datura\" are poi-sonous, especially their seedsand ﬂowers.\nseeds and species of poisonous,\"Datura\" their are All ﬂowers.especially\n\"Datura\" are especially theirﬂowers. seeds and of Allspecies poisonous,\nespecially their seeds ﬂowers.\"Datura\" are poisonous, Allspecies of and\nﬂowers. poisonous, species of\"Datura\" are All especially theirseeds and6 Some South American plantsformerly thought of as \"Datura\"are now treated as belongingto the distinct genus \"Brugman-sia\" (\"Brugmansia\" differs from\"Datura\" in that it is woody,making shrubs or small trees,and it has pendulous ﬂowers,rather than erect ones).\nand \"Datura\" treated from thanﬂowers, it small belongingwoody, thought as ones). Southdiffers Some \"Brugmansia\"American as are in the ratherpendulous distinct making nowerect \"Datura\" to (\"Brugman-sia\" of formerly trees, or is itthat plants genus has shrubs\n\"Brugmansia\" (\"Brugmansia\"than erect pendulous genus andones). is woody, small trees,of as the distinct ﬂowers, ratherSome South differs from Amer-ican plants treated as formerlythought belonging to \"Datura\"in making that it \"Datura\" are ithas now shrubs or\nwoody, small trees, and haspendulous ﬂowers, as belong-ing to Some making shrubs oras rather than erect \"Datura\"are now \"Brugmansia\" (\"Brug-mansia\" differs the distinctgenus from \"Datura\" in for-merly thought of it treated that itis ones). South American plants\nbelonging to the distinct hasmaking Some (\"Brugmansia\"differs from \"Datura\" in arenow treated as genus pendulousshrubs ﬂowers, rather than erector ones). \"Brugmansia\" that it iswoody, South American plantsformerly thought of as \"Datura\"small trees, and it7 Other related taxa includetaxa Other include relatedinclude Other related taxainclude Other related taxaOther related taxa include8 \"Hyoscyamus niger\", \"Atropabelladonna\", \"Mandragora of-ﬁcinarum\", Physalis, and manymore.\nand many niger\", ofﬁcinarum\",belladonna\", \"Mandragora\"Atropa \"Hyoscyamus more.Physalis,\nbelladonna\", \"Mandragora\"Hyoscyamus niger\", manyPhysalis, and more. ofﬁci-narum\", \"Atropa\nmore. Physalis, and many bel-ladonna\", \"Mandragora ofﬁci-narum\", \"Hyoscyamus niger\",\"Atropa\nniger\", more. belladonna\",\"Mandragora ofﬁcinarum\",Physalis, \"Atropa many and\"Hyoscyamus9 The name \"Datura\" is takenfrom Sanskrit ’ ’thorn-apple’,ultimately from Sanskrit ’’white thorn-apple’ (referringto \"Datura metel\" of Asia).\nof Asia). taken from nameThe \"Datura\" ’ is to ’thorn-apple’, Sanskrit ’ Sanskritmetel\" ’white (referring from\"Datura thorn-apple’ ultimately\n\"Datura\" is taken from to ’’thorn-apple’, Sanskrit ’ ’whiteof thorn-apple’ (referring Asia).The name Sanskrit ultimatelyfrom \"Datura metel\"\nSanskrit ’ The name \"Datura\"’thorn-apple’, ultimately frommetel\" Asia). is taken from of’white (referring to \"Datura San-skrit ’ thorn-apple’\nAsia). The name \"Datura\" isfrom taken of from Sanskrit ’’thorn-apple’, ultimately San-skrit ’ ’white thorn-apple’ (re-ferring to \"Datura metel\"10 In the Ayurvedic text Sushrutadifferent species of Datura arealso referred to as ’ and ’.\nthe of also Sushruta Datura arereferred to as In Ayurvedic anddifferent species ’ text ’.\nspecies of referred to are alsoDatura Sushruta different andas ’ Ayurvedic text In the ’.\nas ’ and In the Ayurvedic alsoreferred to species of Datura aretext Sushruta different ’.\ndifferent In the Ayurvedic textalso referred to as and Sushruta’ species of Datura are ’.\nTable 18: First 10 lines from the BookWiki corpus, and their respective n-gram permutations.\n2913\nModel RTE MRPC SST-2 CoLA QQP QNLI MNLI PAWS\nMN 2e-05 2e-05 1e-05 2e-05 1e-05 1e-05 1e-05 2e-05\nM1 2e-05 1e-05 1e-05 1e-05 3e-05 1e-05 2e-05 2e-05\nM2 2e-05 2e-05 1e-05 1e-05 2e-05 1e-05 1e-05 3e-05\nM3 3e-05 1e-05 2e-05 2e-05 3e-05 1e-05 1e-05 2e-05\nM4 3e-05 1e-05 2e-05 2e-05 2e-05 1e-05 1e-05 2e-05\nM512 1e-05 3e-05 2e-05 2e-05 3e-05 2e-05 3e-05 2e-05\nMUG 2e-05 1e-05 3e-05 1e-05 3e-05 3e-05 3e-05 2e-05\nMUF 2e-05 1e-05 3e-05 2e-05 3e-05 3e-05 3e-05 1e-05\nMRI 1e-05 1e-05 3e-05 1e-05 1e-05 1e-05 2e-05 1e-05\nMNP 1e-05 3e-05 2e-05 1e-05 1e-05 1e-05 1e-05 1e-05\nTable 19: Fine-tuning hyperparam Learning rate of each model for each task in GLUE and PAWS\nModel RTE MRPC SST-2 CoLA QQP QNLI MNLI PAWS\nMN 16 16 32 16 16 32 32 16\nM1 32 32 16 32 32 16 32 16\nM2 32 16 32 16 32 32 16 32\nM3 32 32 16 32 32 16 32 32\nM4 32 16 32 16 32 32 32 32\nM512 32 16 16 32 32 16 16 16\nMUG 16 16 16 16 32 16 16 32\nMUF 16 32 16 16 32 16 16 16\nMRI 16 16 32 16 16 16 32 16\nMNP 16 32 16 16 32 16 16 16\nTable 20: Finetuning hyperparam batch size of each model for each task in GLUE and PAWS"
}