{
  "title": "Retrieval-Augmented Generation with Knowledge Graphs for Customer Service Question Answering",
  "url": "https://openalex.org/W4396823873",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A1977756860",
      "name": "Xu Zhen-tao",
      "affiliations": [
        "LinkedIn (United States)"
      ]
    },
    {
      "id": null,
      "name": "Cruz, Mark Jerome",
      "affiliations": [
        "LinkedIn (United States)"
      ]
    },
    {
      "id": null,
      "name": "Guevara, Matthew",
      "affiliations": [
        "LinkedIn (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2068419882",
      "name": "Wang Tie",
      "affiliations": [
        "LinkedIn (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2185636774",
      "name": "Deshpande Manasi",
      "affiliations": [
        "LinkedIn (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A44606687",
      "name": "Wang Xiao-feng",
      "affiliations": [
        "LinkedIn (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A1902402908",
      "name": "Li Zheng",
      "affiliations": [
        "LinkedIn (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3046081331",
    "https://openalex.org/W1801721664",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W2151149636",
    "https://openalex.org/W2251079237",
    "https://openalex.org/W2600463316",
    "https://openalex.org/W2887428522",
    "https://openalex.org/W1593271688"
  ],
  "abstract": "In customer service technical support, swiftly and accurately retrieving relevant past issues is critical for efficiently resolving customer inquiries. The conventional retrieval methods in retrieval-augmented generation (RAG) for large language models (LLMs) treat a large corpus of past issue tracking tickets as plain text, ignoring the crucial intra-issue structure and inter-issue relations, which limits performance. We introduce a novel customer service question-answering method that amalgamates RAG with a knowledge graph (KG). Our method constructs a KG from historical issues for use in retrieval, retaining the intra-issue structure and inter-issue relations. During the question-answering phase, our method parses consumer queries and retrieves related sub-graphs from the KG to generate answers. This integration of a KG not only improves retrieval accuracy by preserving customer service structure information but also enhances answering quality by mitigating the effects of text segmentation. Empirical assessments on our benchmark datasets, utilizing key retrieval (MRR, Recall@K, NDCG@K) and text generation (BLEU, ROUGE, METEOR) metrics, reveal that our method outperforms the baseline by 77.6% in MRR and by 0.32 in BLEU. Our method has been deployed within LinkedIn's customer service team for approximately six months and has reduced the median per-issue resolution time by 28.6%.",
  "full_text": "Retrieval-Augmented Generation with Knowledge Graphs for\nCustomer Service Question Answering\nZhentao Xu\nzhexu@linkedin.com\nLinkedIn Corporation\nSunnyvale, CA, USA\nMark Jerome Cruz\nmarcruz@linkedin.com\nLinkedIn Corporation\nSunnyvale, CA, USA\nMatthew Guevara\nmguevara@linkedin.com\nLinkedIn Corporation\nSunnyvale, CA, USA\nTie Wang\ntiewang@linkedin.com\nLinkedIn Corporation\nSunnyvale, CA, USA\nManasi Deshpande\nmadeshpande@linkedin.com\nLinkedIn Corporation\nSunnyvale, CA, USA\nXiaofeng Wang\nxiaofwang@linkedin.com\nLinkedIn Corporation\nSunnyvale, CA, USA\nZheng Li\nzeli@linkedin.com\nLinkedIn Corporation\nSunnyvale, CA, USA\nABSTRACT\nIn customer service technical support, swiftly and accurately re-\ntrieving relevant past issues is critical for efficiently resolving cus-\ntomer inquiries. The conventional retrieval methods in retrieval-\naugmented generation (RAG) for large language models (LLMs)\ntreat a large corpus of past issue tracking tickets as plain text, ig-\nnoring the crucial intra-issue structure and inter-issue relations,\nwhich limits performance. We introduce a novel customer service\nquestion-answering method that amalgamates RAG with a knowl-\nedge graph (KG). Our method constructs a KG from historical issues\nfor use in retrieval, retaining the intra-issue structure and inter-\nissue relations. During the question-answering phase, our method\nparses consumer queries and retrieves related sub-graphs from the\nKG to generate answers. This integration of a KG not only im-\nproves retrieval accuracy by preserving customer service structure\ninformation but also enhances answering quality by mitigating the\neffects of text segmentation. Empirical assessments on our bench-\nmark datasets, utilizing key retrieval (MRR, Recall@K, NDCG@K)\nand text generation (BLEU, ROUGE, METEOR) metrics, reveal that\nour method outperforms the baseline by 77.6% in MRR and by 0.32\nin BLEU. Our method has been deployed within LinkedInâ€™s cus-\ntomer service team for approximately six months and has reduced\nthe median per-issue resolution time by 28.6%.\nCCS CONCEPTS\nâ€¢ Computing methodologies â†’Information extraction; Nat-\nural language generation .\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\nÂ© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0431-4/24/07\nhttps://doi.org/10.1145/3626772.3661370\nKEYWORDS\nLarge Language Model, Knowledge Graph, Question Answering,\nRetrieval-Augmented Generation\nACM Reference Format:\nZhentao Xu, Mark Jerome Cruz, Matthew Guevara, Tie Wang, Manasi\nDeshpande, Xiaofeng Wang, and Zheng Li. 2024. Retrieval-Augmented\nGeneration with Knowledge Graphs for Customer Service Question An-\nswering. In Proceedings of the 47th International ACM SIGIR Conference\non Research and Development in Information Retrieval (SIGIR â€™24), July\n14â€“18, 2024, Washington, DC, USA. ACM, New York, NY, USA, 5 pages.\nhttps://doi.org/10.1145/3626772.3661370\n1 INTRODUCTION\nEffective technical support in customer service underpins prod-\nuct success, directly influencing customer satisfaction and loyalty.\nGiven the frequent similarity of customer inquiries to previously\nresolved issues, the rapid and accurate retrieval of relevant past\ninstances is crucial for the efficient resolution of such inquiries. Re-\ncent advancements in embedding-based retrieval (EBR), large lan-\nguage models (LLMs), and retrieval-augmented generation (RAG)\n[8] have significantly enhanced retrieval performance and question-\nanswering capabilities for the technical support of customer service.\nThis process typically unfolds in two stages: first, historical issue\ntickets are treated as plain text, segmented into smaller chunks to\naccommodate the context length constraints of embedding models;\neach chunk is then converted into an embedding vector for retrieval.\nSecond, during the question-answering phase, the system retrieves\nthe most relevant chunks and feeds them as contexts for LLMs to\ngenerate answers in response to queries. Despite its straightforward\napproach, this method encounters several limitations:\nâ€¢Limitation 1 - Compromised Retrieval Accuracy from\nIgnoring Structures: Issue tracking documents such as Jira\n[2] possess inherent structure and are interconnected, with\nreferences such as \"issue A is related to/copied from/caused\narXiv:2404.17723v2  [cs.IR]  6 May 2024\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Zhentao Xu, et al.\nby issue B. \" The conventional approach of compressing doc-\numents into text chunks leads to the loss of vital informa-\ntion. Our approach parses issue tickets into trees and further\nconnects individual issue tickets to form an interconnected\ngraph, which maintains this intrinsic relationship among\nentities, achieving high retrieval performance.\nâ€¢Limitation 2 - Reduced Answer Quality from Segmen-\ntation: Segmenting extensive issue tickets into fixed-length\nsegments to accommodate the context length constraints of\nembedding models can result in the disconnection of related\ncontent, leading to incomplete answers. For example, an is-\nsue ticket describing an issue at its beginning and its solution\nat the end may be split during the text segmentation process,\nresulting in the omission of critical parts of the solution. Our\ngraph-based parsing method overcomes this by preserving\nthe logical coherence of ticket sections, ensuring the delivery\nof complete and high-quality responses.\n2 RELATED WORK\nQuestion answering (QA) with knowledge graphs (KGs) can be\nbroadly classified into retrieval-based, template-based, and semantic\nparsing-based methods. Retrieval-based approaches utilize relation\nextraction [19] or distributed representations [5] to derive answers\nfrom KGs, but they face difficulties with questions involving multi-\nple entities. Template-based strategies depend on manually-created\ntemplates for encoding complex queries, yet are limited by the\nscope of available templates [16]. Semantic parsing methods map\ntext to logical forms containing predicates from KGs [4] [14] [21].\nRecent advancements in large language models (LLMs) integra-\ntion with Knowledge Graphs (KGs) have demonstrated notable\nprogress. Jin et al. [7] provide a comprehensive review of this in-\ntegration, categorizing the roles of LLMs as Predictors, Encoders,\nand Aligners. For graph-based reasoning, Think-on-Graph [15] and\nReasoning-on-Graph [10] enhance LLMsâ€™ reasoning abilities by\nintegrating KGs. Yang et al. [20] propose augmenting LLMsâ€™ factual\nreasoning across various training phases using KGs. For LLM-based\nquestion answering, Wen et al. â€™s Mindmap [18] and Qi et al. [13]\nemploy KGs to boost LLM inference capabilities in specialized do-\nmains such as medicine and food. These contributions underscore\nthe increasing efficacy of LLM and KG combinations in enhancing\ninformation retrieval and reasoning tasks.\n3 METHODS\nWe introduce an LLM-based customer service question answering\nsystem that seamlessly integrates retrieval-augmented generation\n(RAG) with a knowledge graph (KG). Our system (Figure 1) com-\nprises two phases: First, during the KG construction phase, our\nsystem constructs a comprehensive knowledge graph from histor-\nical customer service issue tickets. It integrates a tree-structured\nrepresentation of each issue and interlinks them based on relational\ncontext. It also generates embedding for each node to facilitate later\nsemantic searching. Second, during the question-answering phase,\nour method parses consumer queries to identify named entities\nand intents. It then navigates within the KG to identify related\nsub-graphs for generating answers.\n3.1 Knowledge Graph Construction\n3.1.1 Graph Structure Definition. In defining the knowledge graph\nstructure for historical issue representation, we employ a dual-level\narchitecture that segregates intra-issue and inter-issue relations, as\nillustrated in Figure 1. The Intra-issue Tree Tð‘– (N,E,R)models\neach ticket ð‘¡ð‘– as a tree, where each node ð‘› âˆˆN , identified by a\nunique combination (ð‘–,ð‘ ), corresponds to a distinct section ð‘  of\nticket ð‘¡ð‘– , and each edge ð‘’ âˆˆE and ð‘Ÿ âˆˆR signifies the hierarchi-\ncal connection and type of relations between these sections. The\nInter-issue Graph G(T,E,R)represents the network of connec-\ntions across different tickets, incorporating both explicit links Eexp,\ndefined in issue tracking tickets, and implicit connections Eimp,\nderived from semantic similarity between tickets. For implicit con-\nnections, we leverage cosine similarity between the embedding\nvectors of ticket titles, a method adaptable to specific use cases.\nFor instance, Figure 1 portrays ticket ENT-22970 as a tree struc-\nture with nodes representing sections such asSummary, Description,\nand Priority. It exhibits a direct clone linkage to PORT-133061,\nindicating an explicit clone relationship. Additionally, itâ€™s implic-\nitly connected with ENT-1744 and ENT-3547 due to the semantic\nsimilarities.\n3.1.2 Knowledge Graph Construction. Graph construction is de-\nlineated into two phases: intra-ticket parsing and inter-ticket con-\nnection. 1) Intra-Ticket Parsing Phase: This phase transforms\neach text-based ticket ð‘¡ð‘– into a tree representation Tð‘– . We employ\na hybrid methodology, initially utilizing rule-based extraction for\npredefined fields, such as code sections identified via keywords.\nSubsequently, for text not amenable to rule-based parsing, we en-\ngage an LLM for parsing. The LLM is directed by a YAML template\nTtemplate, representing in graph the ticket sections routinely utilized\nby customer support. 2) Inter-Ticket Connection Phase: Here,\nindividual trees Tð‘– are amalgamated into a comprehensive graph G.\nExplicit connections Eexp are delineated as specified within tickets,\nexemplified by designated fields in Jira [ 2]. Implicit connections\nEimp are inferred from textual-semantic similarities across ticket\ntitles, employing embedding techniques and a threshold mechanism\nto discern the most relevant tickets for each issue ticket.\nð‘¡ð‘– = ð‘¡ð‘–,rule âˆªð‘¡ð‘–,llm\nTð‘– = RuleParse(ð‘¡ð‘–,rule)+LLMParse(ð‘¡ð‘–,llm,Ttemplate,prompt)\nEexp = {(Tð‘–,Tð‘— )|Tð‘– explicitly connected to Tð‘— }\nEimp = {(Tð‘–,Tð‘— )| cos(embed(Tð‘– ),embed(Tð‘— ))â‰¥ ðœƒ}\n3.1.3 Embedding Generation. To support online embedding-based\nretrieval, we generate embeddings for graph node values using\npre-trained text-embedding models like BERT [ 6] and E5 [ 17],\nspecifically targeting nodes for text-rich sections such as \"issue\nsummary\", \"issue description\", and \"steps to reproduce\" ,\netc. These embeddings are then stored in a vector database (for\ninstance, QDrant [12])). For most cases the text-length within each\nnode can meet the text-embedding modelâ€™s context length con-\nstraints, but for certain lengthy texts,we can safely divide the text\ninto smaller chunks for individual embedding without worrying\nabout quality since the text all belong to the same section.\nRetrieval-Augmented Generation with Knowledge Graphs for Customer Service Question Answering SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\nQuestion \nIntent\nEmbedding-based \nRetrieval\nFiltering\nFiltering\nTicket \nENT-22970\nCSV \nupload \nerror, \nupdating \nuser \nemail\n\"CSV \nupload \nerror, \nupdating \nuser \nemail\"\nHAS_SUMMARY\n[\"user-1\":\"Do \nwe \nknow \nhow \nthese \nduplicated \nprofiles \ngot \ncreated?\",\n....\n\"user-2\": \n\"cleaned \nup \n228 \nduplicate \nprofiles, \nresolved\",\n\"user-1\": \n\"thanks, \nticket \nclosed\"]\nHAS_COMMENTS\nFields\nHAS_FIELDS\nDescription\nHAS_DESCRIPTION\nData \nIssue\nHAS_ROOT_CAUSE\nMajor\nStrategic\nHAS_PRIORITY\nHAS_IMPACT_AREA\n\"Admin \nseeing \nseveral \nerrors \nwhen \nattempting \nupdate \nof \nuser \nemails \non \ndashboard \nID \n\"xxxxxxxxx'. \nTotal \nnumber \nof \nusers \naffected \n~'yyy'.\"\nHAS_ISSUE_DESCRIPTION\nRefer \nto \nthe \nCSV: \nhttps://microsoft.sharepoint.com/\nx\nxx: \n1. \nOpen \nthe \nDashboard \nID \nxxxxx; \n2. \nClick \non \nInstances \n> \nProfile; \n3. \nSearch \nfor \nusers \nfrom \nthe \nCSV \nfile \nand \nnote \nthat \nthere \nare \n2 \nprofiles \nexist.\nHAS_STEPS_TO_REPRODUCE\nCLONE_FROM\nCLONE_TO\nQuestion \nQuery: \nHow \nto \nreproduce \nthe \nissue \nwhere \nuser \nsaw \n\"csv \nupload \nerror \nin \nupdating \nuser \nemail\" \nand \nhas \nmajor \npriority \nthat \nwas \ncaused \nby \ndata \nissue?\nIntent: \n\"Steps \nto \nReproduce\"\nSummary:\n \n\"CSV \nupload \nerror \nin \nupdating \nuser \nemail\"\nPriority:\n \n\"Major\"\nRoot \nCause:\n \n\"Data \nIssue\"\nEntity \nDetection\nFinal \nAnswer: \nbased \non \nthe \nticket \nENT-22970, \nthe \nsteps \nto \nreproduce \nthe \nissue \nis \n\"1. \nRefer \nto \nthe \nCSV: \nhttps://microsoft.sharepoint.com/xxx \n2.\n \nOpen \nthe \nDashboard \nID \nxxxxxxxxx \n3. \nClick \non \nInstances \n> \nProfile \n4. \nSearch \nfor \nusers \nfrom \nthe \nCSV \nfile \nand \nnote \nthat \nthere \nare \n2 \nprofiles \nthat \ncome \nup.\nAnswer \nGeneration\n4\n4\n5\n5\n5\n5\n6\nSIMILAR_TO\nSIMILAR_TO\nTicket \nENT-22970\nTicket \nPORT-133061\nTicket \nENT-1744\nTicket \nENT-3547\nCLONE_FROM\nCLONE_TO\nintra-ticket \ntree \nrepresentation\ninter-ticket \nconnections\nKnowledge Graph Construction Retrieval and Question Answering\n1\n2\nVector \nDatabase\n3\nText-embedding \nGeneration \nfor \nNode \nValues\nIntent \nClassification\ninter-ticket \nconnection\n(implicit \nEBR, \nexplicit)\nintra-ticket \ntree \nparsing\nGraph \nDatabase\nissue-tracking \nticket\n1-6\nstep \nwith \nstep \nnumbers\nVector \nDB\nGraph \nDB\ngraph \nnodes \nwith \nlinks\nStep \nwith \nLLM\nTicket \nENT-3547 \nLearning \n'upload \ncsv' \noption \nfails\n...\n...\n...\nTicket \nENT-1744 \nHTTP \nPOST \ncsv \nupload \nerror-internal \nerror\n...\n...\n...\nTicket \nPORT-133061\n\"\nCSV \nupload \nerror, \nupdating \nuser \nemail\"\n...\n...\n...\nLegends\nFigure 1: An overview of our proposed retrieval-augmented generation with knowledge graph framework. The left side of this\ndiagram illustrates the knowledge graph construction; the right side shows the retrieval and question answering process.\n3.2 Retrieval and Question Answering\n3.2.1 Query Entity Identification and Intent Detection. In this step,\nwe extract the named entities Pof type Map(Nâ†’V) and the\nquery intent set Ifrom each user query ð‘ž. The method involves\nparsing each query ð‘žinto a key-value pair, where each key ð‘›, men-\ntioned within the query, corresponds to an element in the graph\ntemplate Ttemplate, and the value ð‘£ represents the information ex-\ntracted from the query. Concurrently, the query intents Iinclude\nthe entities mentioned in the graph template Ttemplate that the\nquery aims to address. We leverage LLM with a suitable prompt\nin this parsing process. For instance, given the query ð‘ž = \"How\nto reproduce the login issue where a user canâ€™t log in to LinkedIn?\" ,\nthe extracted entity is P= Map(\"issue summary\" â†’\"login issue\",\n\"issue description\" â†’\"user canâ€™t log in to LinkedIn\"), and the\nintent set is I=Set(\"fix solution\"). This method demonstrates\nnotable flexibility in accommodating varied query formulations\nby leveraging the LLMâ€™s extensive understanding and interpretive\ncapabilities.\nð‘ƒ,ð¼ = LLM(ð‘ž,ð‘‡template,prompt)\n3.2.2 Embedding-based Retrieval of Sub-graphs. Our method ex-\ntracts pertinent sub-graphs from the knowledge graph, aligned with\nuser-provided specifics such as\"issue description\" and \"issue\nsummary\", as well as user intentions like\"fix solution\". This pro-\ncess consists of two primary steps: EBR-based ticket identification\nand LLM-driven subgraph extraction.\nIn the EBR-based ticket identification step , the top ð¾ticket\nmost relevant historical issue tickets are pinpointed by harnessing\nthe named entity set Pderived from user queries. For each entity\npair (ð‘˜,ð‘£)âˆˆP , cosine similarity is computed between the entity\nvalue ð‘£ and all graph nodes ð‘›corresponding to section ð‘˜ via pre-\ntrained text embeddings. Aggregating these node-level scores to\nticket-level by summing contributions from nodes belonging to\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Zhentao Xu, et al.\nthe same ticket, we rank and select the top ð¾ticket tickets. This\nmethod presupposes that the occurrence of multiple query entities\nis indicative of pertinent links, thus improving retrieval precision.\nð‘†ð‘‡ð‘– =\nâˆ‘ï¸\n(ð‘˜,ð‘£ )âˆˆP\nï£®ï£¯ï£¯ï£¯ï£¯ï£°\nâˆ‘ï¸\nð‘›âˆˆð‘‡ð‘–\nI{ð‘›.sec = ð‘˜}Â·cos(embed(ð‘£),embed(ð‘›.text))\nï£¹ï£ºï£ºï£ºï£ºï£»\nIn the LLM-driven subgraph extraction step , the system first\nrephrases the original user query ð‘žto include the retrieved ticket\nID; the modified query ð‘žâ€² is then translated into a graph data-\nbase language, such as Cypher for Neo4j for question answering.\nFor instance, from the initial query ð‘ž =\"how to reproduce the is-\nsue where user saw â€™csv upload error in updating user emailâ€™ with\nmajor priority due to a data issue\" , the query is reformulated to\n\"how to reproduce â€™ENT-22970â€™ and thereafter transposed into the\nCypher query MATCH (j:Ticket {ticket_ID: â€™ENT-22970â€™})\n-[:HAS_DESCRIPTION]-> (description:Description)\n-[:HAS_STEPS_TO_REPRODUCE]-> (steps_to_reproduce:\nStepsToReproduce) RETURN steps_to_reproduce.value. It is\nnoteworthy that the LLM-driven query formulation is sufficiently\nversatile to retrieve information across subgraphs, whether they\noriginate from the same tree or distinct trees within the knowledge\ngraph.\n3.2.3 Answer Generation. Answers are synthesized by correlating\nretrieved data from Section 3.2.2 with the initial query. The LLM\nserves as a decoder to formulate responses to user inquiries given\nthe retrieved information. For robust online serving, if query execu-\ntion encounters issues, a fallback mechanism reverts to a baseline\ntext-based retrieval method\n4 EXPERIMENT\n4.1 Experiment Design\nOur evaluation employed a curated \"golden\" dataset comprising\ntypical queries, support tickets, and their authoritative solutions.\nThe control group operated with conventional text-based EBR,\nwhile the experimental group applied the methodology outlined in\nthis study. For both groups, we utilized the same LLM, specifically\nGPT-4 [1], and the same embedding model, E5 [17]. We measured\nretrieval efficacy using Mean Reciprocal Rank (MRR), recall@K,\nand NDCG@K. MRR gauges the average inverse rank of the initial\ncorrect response, recall@K determines the likelihood of a relevant\nitemâ€™s appearance within the top K selections, and NDCG@K ap-\npraises the rank quality by considering both position and pertinence\nof items. For question-answering performance, we juxtaposed the\n\"golden\" solutions against the generated responses, utilizing metrics\nsuch as BLEU [11], ROUGE [9], and METEOR [3] scores.\n4.2 Result and Analysis\nThe retrieval and question-answering performances are presented\nin Table 1 and Table 2, respectively. Across all metrics, our method\ndemonstrates consistent improvements. Notably, it surpasses the\nbaseline by 77.6% in MRR and by 0.32 in BLEU score, substantiating\nits superior retrieval efficacy and question-answering accuracy.\nTable 1: Retrieval Performance\nMRR Recall@K NDCG@K\nK=1 K=3 K=1 K=3\nBaseline 0.522 0 .400 0 .640 0 .400 0 .520\nExperiment 0.927 0.860 1.000 0.860 0.946\nTable 2: Question Answering Performance\nBLEU METEOR ROUGE\nBaseline 0.057 0 .279 0 .183\nExperiment 0.377 0.613 0.546\n5 PRODUCTION USE CASE\nWe deployed our method within LinkedInâ€™s customer service team,\ncovering multiple product lines. The team was split randomly into\ntwo groups: one used our system, while the other stuck to tradi-\ntional manual methods. As shown in Table 3, the group using our\nsystem achieved significant gains, reducing the median resolution\ntime per issue by 28.6%. This highlights our systemâ€™s effectiveness\nin enhancing customer service efficiency.\nTable 3: Customer Support Issue Resolution Time\nGroup Mean P50 P90\nTool Not Used 40 Hours 7 Hours 87 Hours\nTool Used 15 hours 5 hours 47 hours\n6 CONCLUSIONS AND FUTURE WORK\nIn conclusion, our research significantly advances automated ques-\ntion answering systems for customer service. Integrating retrieval\naugmented generation (RAG) with a knowledge graph (KG) has\nimproved retrieval and answering metrics, and overall service effec-\ntiveness. Future work will focus on: developing an automated mech-\nanism for extracting graph templates, enhancing system adaptabil-\nity; investigating dynamic updates to the knowledge graph based\non user queries to improve real-time responsiveness; and explor-\ning the systemâ€™s applicability in other contexts beyond customer\nservice.\n7 COMPANY PORTRAIT\nAbout LinkedIn: Founded in 2003, LinkedIn connects the worldâ€™s\nprofessionals to make them more productive and successful. With\nmore than 1 billion members worldwide, including executives from\nevery Fortune 500 company, LinkedIn is the worldâ€™s largest pro-\nfessional network. The company has a diversified business model\nwith revenue coming from Talent Solutions, Marketing Solutions,\nSales Solutions and Premium Subscriptions products. Headquar-\ntered in Silicon Valley, LinkedIn has offices across the globe. Please\nvisit https://www.linkedin.com/company/linkedin/about/ for more\ninformation.\nRetrieval-Augmented Generation with Knowledge Graphs for Customer Service Question Answering SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\n8 PRESENTER BIO\nZhentao Xu is a Senior Software Engineer at LinkedIn. He received\nhis M.S. in Robotics and B.S. in Electrical Engineering and Computer\nScience (EECS) from University of Michigan. His research interests\nlie in large language models and natural language generation.\nREFERENCES\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren-\ncia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal\nAnadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774\n(2023).\n[2] Atlassian. 2024. Jira | Issue & Project Tracking Software. https://www.atlassian.\ncom/software/jira Accessed: 2024-01-04.\n[3] Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for\nMT evaluation with improved correlation with human judgments. In Proceedings\nof the acl workshop on intrinsic and extrinsic evaluation measures for machine\ntranslation and/or summarization . 65â€“72.\n[4] Nikita Bhutani, Xinyi Zheng, Kun Qian, Yunyao Li, and H Jagadish. 2020. Answer-\ning complex questions by combining information from curated and extracted\nknowledge bases. In Proceedings of the first workshop on natural language inter-\nfaces. 1â€“10.\n[5] Antoine Bordes, Jason Weston, and Nicolas Usunier. 2014. Open question an-\nswering with weakly supervised embedding models. In Machine Learning and\nKnowledge Discovery in Databases: European Conference, ECML PKDD 2014, Nancy,\nFrance, September 15-19, 2014. Proceedings, Part I 14 . Springer, 165â€“180.\n[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:\nPre-training of deep bidirectional transformers for language understanding.arXiv\npreprint arXiv:1810.04805 (2018).\n[7] Bowen Jin, Gang Liu, Chi Han, Meng Jiang, Heng Ji, and Jiawei Han. 2023.\nLarge Language Models on Graphs: A Comprehensive Survey. arXiv preprint\narXiv:2312.02783 (2023).\n[8] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,\nNaman Goyal, Heinrich KÃ¼ttler, Mike Lewis, Wen-tau Yih, Tim RocktÃ¤schel,\net al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks.\nAdvances in Neural Information Processing Systems 33 (2020), 9459â€“9474.\n[9] Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries.\nIn Text summarization branches out . 74â€“81.\n[10] Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, and Shirui Pan. 2023. Reasoning\non graphs: Faithful and interpretable large language model reasoning. arXiv\npreprint arXiv:2310.01061 (2023).\n[11] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a\nmethod for automatic evaluation of machine translation. In Proceedings of the\n40th annual meeting of the Association for Computational Linguistics . 311â€“318.\n[12] Qdrant Team. 2024. Qdrant - Vector Database. https://qdrant.tech/. Accessed:\n2024-01-08.\n[13] Zhixiao Qi, Yijiong Yu, Meiqi Tu, Junyi Tan, and Yongfeng Huang. 2023. Foodgpt:\nA large language model in food testing domain with incremental pre-training\nand knowledge graph prompt. arXiv preprint arXiv:2308.10173 (2023).\n[14] Haitian Sun, Bhuwan Dhingra, Manzil Zaheer, Kathryn Mazaitis, Ruslan Salakhut-\ndinov, and William Cohen. 2018. Open Domain Question Answering Using Early\nFusion of Knowledge Bases and Text. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing , Ellen Riloff, David Chiang,\nJulia Hockenmaier, and Junâ€™ichi Tsujii (Eds.). Association for Computational\nLinguistics, Brussels, Belgium, 4231â€“4242. https://doi.org/10.18653/v1/D18-1455\n[15] Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun\nGong, Heung-Yeung Shum, and Jian Guo. 2023. Think-on-graph: Deep and\nresponsible reasoning of large language model with knowledge graph. arXiv\npreprint arXiv:2307.07697 (2023).\n[16] Christina Unger, Lorenz BÃ¼hmann, Jens Lehmann, Axel-Cyrille Ngonga Ngomo,\nDaniel Gerber, and Philipp Cimiano. 2012. Template-based question answering\nover RDF data. In Proceedings of the 21st international conference on World Wide\nWeb. 639â€“648.\n[17] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang,\nRangan Majumder, and Furu Wei. 2022. Text embeddings by weakly-supervised\ncontrastive pre-training. arXiv preprint arXiv:2212.03533 (2022).\n[18] Yilin Wen, Zifeng Wang, and Jimeng Sun. 2023. Mindmap: Knowledge graph\nprompting sparks graph of thoughts in large language models. arXiv preprint\narXiv:2308.09729 (2023).\n[19] Kun Xu, Yansong Feng, Songfang Huang, and Dongyan Zhao. 2016. Hybrid\nquestion answering over knowledge base and free text. InProceedings of COLING\n2016, the 26th International Conference on Computational Linguistics: Technical\nPapers. 2397â€“2407.\n[20] Linyao Yang, Hongyang Chen, Zhao Li, Xiao Ding, and Xindong Wu. 2023.\nChatGPT is not Enough: Enhancing Large Language Models with Knowledge\nGraphs for Fact-aware Language Modeling. arXiv preprint arXiv:2306.11489\n(2023).\n[21] Scott Wen-tau Yih, Ming-Wei Chang, Xiaodong He, and Jianfeng Gao. 2015. Se-\nmantic parsing via staged query graph generation: Question answering with\nknowledge base. In Proceedings of the Joint Conference of the 53rd Annual Meet-\ning of the ACL and the 7th International Joint Conference on Natural Language\nProcessing of the AFNLP .",
  "topic": "Question answering",
  "concepts": [
    {
      "name": "Question answering",
      "score": 0.6698355674743652
    },
    {
      "name": "Computer science",
      "score": 0.6554441452026367
    },
    {
      "name": "Knowledge graph",
      "score": 0.5568230152130127
    },
    {
      "name": "Information retrieval",
      "score": 0.5374326109886169
    },
    {
      "name": "Customer service",
      "score": 0.42357468605041504
    },
    {
      "name": "Service (business)",
      "score": 0.38867196440696716
    },
    {
      "name": "Business",
      "score": 0.10195544362068176
    },
    {
      "name": "Marketing",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1316064682",
      "name": "LinkedIn (United States)",
      "country": "US"
    }
  ],
  "cited_by": 77
}