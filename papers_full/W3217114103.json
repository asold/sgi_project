{
  "title": "Sparse Fusion for Multimodal Transformers",
  "url": "https://openalex.org/W3217114103",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5000662681",
      "name": "Yi Ding",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5043001458",
      "name": "Alex Rich",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5084824021",
      "name": "Mason Wang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5002901804",
      "name": "Noah Stier",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5018199600",
      "name": "Matthew Turk",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5087948617",
      "name": "Pradeep Sen",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5028885566",
      "name": "Tobias Höllerer",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3143083666",
    "https://openalex.org/W2619383789",
    "https://openalex.org/W3170863103",
    "https://openalex.org/W2943865428",
    "https://openalex.org/W3181700833",
    "https://openalex.org/W2619082050",
    "https://openalex.org/W3015371781",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2342662179",
    "https://openalex.org/W3161820708",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3135658369",
    "https://openalex.org/W3204182250",
    "https://openalex.org/W3034266838",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W3217622007",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W2947476638",
    "https://openalex.org/W3209970085",
    "https://openalex.org/W2901272442",
    "https://openalex.org/W2151096985",
    "https://openalex.org/W2787581402",
    "https://openalex.org/W2883409523",
    "https://openalex.org/W3045733172",
    "https://openalex.org/W4287901267",
    "https://openalex.org/W2964216663",
    "https://openalex.org/W2995575179",
    "https://openalex.org/W3169064633",
    "https://openalex.org/W4288024261",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W3104613728",
    "https://openalex.org/W3034742519",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W3174906557",
    "https://openalex.org/W2971105107",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2921861056",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W4287704453",
    "https://openalex.org/W3134144764",
    "https://openalex.org/W4287236261",
    "https://openalex.org/W2962718314",
    "https://openalex.org/W3212014628",
    "https://openalex.org/W2963524571",
    "https://openalex.org/W3097132740",
    "https://openalex.org/W4214633470",
    "https://openalex.org/W4287025446",
    "https://openalex.org/W4323654151",
    "https://openalex.org/W4226416753",
    "https://openalex.org/W3213096871",
    "https://openalex.org/W2964051877",
    "https://openalex.org/W3175103763",
    "https://openalex.org/W4295838474",
    "https://openalex.org/W3174394143",
    "https://openalex.org/W4287286018",
    "https://openalex.org/W3121562065",
    "https://openalex.org/W2978426779",
    "https://openalex.org/W2984864519",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W4287100327",
    "https://openalex.org/W3081168214",
    "https://openalex.org/W2765407302",
    "https://openalex.org/W3211783547"
  ],
  "abstract": "Multimodal classification is a core task in human-centric machine learning.We observe that information is highly complementary across modalities, thus unimodal information can be drastically sparsified prior to multimodal fusion without loss of accuracy.To this end, we present Sparse Fusion Transformers (SFT), a novel multimodal fusion method for transformers that performs comparably to existing state-of-the-art methods while having greatly reduced memory footprint and computation cost. Key to our idea is a sparse-pooling block that reduces unimodal token sets prior to cross-modality modeling.Evaluations are conducted on multiple multimodal benchmark datasets for a wide range of classification tasks. State-of-the-art performance is obtained on multiple benchmarks under similar experiment conditions, while reporting up to six-fold reduction in computational cost and memory requirements. Extensive ablation studies showcase our benefits of combining sparsification and multimodal learning over naive approaches. This paves the way for enabling multimodal learning on low-resource devices.",
  "full_text": "2 University of California, Santa Barbara\n3 Saratoga High School\n4 Toyota Technological Institute at Chicago\nyding@ucsb.edu, anrich@ucsb.edu,\nmason.wang0025@gmail.com,\nnoahstier@ucsb.edu,\nmturk@ttic.edu, psen@ece.ucsb.edu,\nholl@ucsb.edu\n1\nSparse Fusion for Multimodal Transformers\nAbstract\nMultimodal classification is a core task in human-centric\nmachine learning. We observe that information is highly\ncomplementary across modalities, thus unimodal informa-\ntion can be drastically sparsified prior to multimodal fu-\nsion without loss of accuracy. To this end, we present\nSparse Fusion Transformers (SFT), a novel multimodal fu-\nsion method for transformers that performs comparably to\nexisting state-of-the-art methods while having greatly re-\nduced memory footprint and computation cost. Key to our\nidea is a sparse-pooling block that reduces unimodal to-\nken sets prior to cross-modality modeling. Evaluations are\nconducted on multiple multimodal benchmark datasets for\na wide range of classification tasks. State-of-the-art per-\nformance is obtained on multiple benchmarks under sim-\nilar experiment conditions, while reporting up to six-fold\nreduction in computational cost and memory requirements.\nExtensive ablation studies showcase our benefits of com-\nbining sparsification and multimodal learning over naive\napproaches. This paves the way for enabling multimodal\nlearning on low-resource devices.\n1. Introduction\nWe experience and interact with the world through our\nfive senses: sight, sound, taste, touch, and smell. The hu-\nman brain is incredibly good at processing all of this infor-\nmation, paying attention only to the few things that matter.\nImbuing a computer with the ability to process multimodal\ndata effectively is highly desirable because it would enable a\nvast array of multi-sensory applications. However, process-\ning multiple data streams increases computational cost, and\nit is therefore a high priority to develop efficient algorithms\nin this domain. Additionally, many of these applications,\nsuch as the detection of instances of domestic abuse, or de-\ntection of prolonged emotional and psychological struggles,\nare particularly well-suited for mobile or low-resource de-\nvices. In these resource-constrained settings, the computa-\ntion cost and memory footprint become critical factors that\nmust be considered for practical use.\nCurrent multimodal algorithms involve some level of\nmodality-independent feature processing followed by a fu-\nsion process which then jointly models the dependencies\nand cross-dependencies between the modalities. In particu-\nlar, deep-learning transformer models have been used in this\nway to achieve state-of-the-art performance on numerous\ntasks [16, 20]. However training and processing such data\nremains prohibitively expensive in many cases, in terms\nof time, computational resources, and energy consumption.\nFor example, a single layer of a vision transformer [9] re-\nquires approximately 1.35 billion floating-point operations\n(GFlops) for a 224 × 224 image for a single forward pass.\nIf we represent a sequence of 30 frames in a similar man-\nner for video data, this explodes to 88.24 GFlops. Although\nrecent advancements have been made to sparsify transform-\ners, these efforts have primarily approached the problem\nfrom a unimodal perspective [1, 3, 18, 25, 28].\nMotivated by these concerns, we propose a sparse fu-\nsion method for multimodal transformers called Sparse Fu-\nsion Transformers (SFTs) that drastically reduces training\ntime and memory consumption while maintaining the qual-\nity of existing fusion methods. Our approach is based on\nthe hypothesis that the large amount of complementary in-\nformation across different modalities allows us to sparsify\nunimodal information prior to multimodal fusion without\nthe loss of accuracy. In particular, approaching a prob-\nlem from a multimodal perspective enables us to sparsify\nthe unimodal information far more aggressively. With our\nsparse-fusion method, we achieve faster performance with\nless memory use while attending to features that are most\nimportant.\nOur proposed fusion process is agnostic to input modal-\nity and makes a full multimodal classification network ro-\nbust to sparsification of input representations. It is com-\nposed of three parts: a block-sparse within-modality atten-\ntion to learn strong local representations, a pooling method\nfor extracting them, and dense self-attention for cross-\nmodal feature fusion. Furthermore, we propose to use a\ncustomized mixup to apply spatio-temporal regularization\nto the learned representations in a modality agnostic man-\nner. Fusing features in this way demonstrates comparable or\nbetter performance than existing methods while requiring\nsignificantly less computation and memory. In summary,\nour contributions are:\n• We propose a novel fusion method that maintains or\n2\nexceeds the performance of previous fusion methods\nwhile demonstrating up to a six-fold reduction in com-\nputation and memory requirements.\n• We demonstrate that multimodal algorithms can tol-\nerate far more token reduction than unimodal algo-\nrithms due to complementary cross-modal informa-\ntion. We show that by accounting for multimodal in-\nformation during sparsification, more information can\nbe removed without loss of performance.\n• We perform extensive ablation studies on fusion com-\nponents using real-world datasets to determine the effi-\ncacy of each model component. We further experiment\nwith multiple pooling methods to demonstrate model\nrobustness under different pooling requirements.\n2. Related Work\nThe problem of modality fusion has been explored in nu-\nmerous problem spaces for a long time [2]. The primary\nchallenge is to find an effective way to combine representa-\ntions of data from disparate modalities into a single repre-\nsentation for more accurate modeling. While the first meth-\nods for multimodal fusion were proposed to address signal\ninadequacies in individual modalities [32], we are now at a\ntime when the resolution in each modality is much higher,\nmaking some computation costly and intractable. There-\nfore, we wish to purposely trade off some of the signal band-\nwidth to improve performance.\nMany methods have been proposed to tackle the task\nof fusion. A way to categorize all these techniques is by\nthe time of fusion occurrence. Early fusion typically refers\nto combining base level representations or even input val-\nues, while late fusion primarily refers to its application near\nthe output. Early deep-learning methods typically make\nuse of linear layers and cross products to combine modali-\nties [10, 29, 33]. More rudimentary forms of fusion simply\ninvolve adding the logits of individual modality predictions\ntogether. As transformer-based architectures have become\nvery popular recently, some recent techniques have also\nexplored their use in multimodal settings. Originally pro-\nposed in [27] for neural machine translation (NMT) tasks,\nthey have demonstrated superior performance on multiple\nbenchmark problems such as image classification [9], action\nrecognition [16] and 3D reconstruction [5, 24]. The basic\nfunctionality is to apply layers of self-attention, on sequen-\ntial representations. To classify a discrete output, transform-\ners typically rely on the use of a special token (CLS) that is\nprepended to the sequence for classification.\nThe most natural form of transformer fusion is simply\nto concatenate the sequence of tokens and rely on self-\nattention to learn their inter-dependencies. Works such as\n[12, 26] that do this learn better cross-modal representa-\ntions and have shown benefits relative to naive fusion meth-\nods. Very recently, multimodal bottleneck transformers [16]\nhave demonstrated a way for early fusion to occur without\nthe use of costly cross-modal operations. However, the pro-\ncess of fusing multimodal information with some form of\nconcatenation and dense attention remains costly due to the\nO(N2) complexity of transformers for input sequences of\nlength N. It is this cost we seek to address with our sparsi-\nfication approach.\nRecent efforts have focused on reducing computational\ncomplexity for transformers and large-scale deep learn-\ning [21, 22, 25, 37]. An effective method for this is to ex-\nploit the representation of features within a small sliding\nwindow of tokens [30] on a long sequence. However, these\nmethods require significant engineering efforts and are hard\nto train [35]. Other works approach the problem via sparsi-\nfication of the attention mechanism, such as random or local\nattention [13,19,31]. Sparsification methods have also been\napplied successfully for some computer vision tasks [18].\nTraining optimizations for transformers have also been\nexplored. Regularization techniques such as dropout [23],\nweight decay [15], and mixup [36] have all been ap-\nplied. While weight decay and dropout can be applied in\na modality-agnostic manner directly onto the weights, the\nuse of mixup has primarily been used to tackle problems in\nthe vision domain, as its application is easily interpretable\nand offers large benefits to the algorithms [4, 17]. Although\nsome recent efforts have been made to enable the appli-\ncation of mixup on domains in a modality agnostic man-\nner [14], its application in a fundamentally multimodal do-\nmain remains underexplored. Its use in the mixing of fused\nfeatures across modalities spatially and across time demon-\nstrates large benefits for our application.\n3. Method\nIn this section, we describe our proposed Sparse Fu-\nsion Transformers (SFT). See Fig. 1 for a visualiza-\ntion of our algorithm. As input, our method takes to-\nken sets from M different modalities, Z1, . . . ,ZM , with\neach modality consisting of N tokens of dimension D,\nZi = [zi1, . . . ,ziN ] ∈ RN×D. Note the number of tokens\nN can vary from modality to modality but for simplicity of\nnotation, we keep it fixed in our description. Additionally,\nif the token dimension D varies from modality to modality,\nwe apply a per-token projection to keep the token dimension\nconstant across all modalities. Following existing work, we\nprepend a special CLS token c with learnable parameters to\neach token set for each modality for the purpose of classi-\nfication: ˆZi = [ci|Zi] = [ci, zi1, . . . ,ziN ] ∈ R(N+1)×D.\nThe goal of our method is classification, i.e., we want to\nlearn a function fθ : RM×(N+1)×D → RC:\nfθ(ˆZ1, . . . ,ˆZM ) =p, (1)\nsuch that p is the probability distribution over C classes.\n3\nFigure 1. Visualization of our fusion method with two modali-\nties. Following existing work, a special CLS token is appended\nto each unimodal token set prior to unimodal transformers. Af-\nter unimodal transformers, the CLS token (cL\n1 and cL\n2 ) from each\nmodality is summed. A pooled block-sparse attention is applied to\nlocal regions of each modality. The CLS token and pooled repre-\nsentations are then combined, and dense self-attention is applied\nto model global and cross-modal dependencies.\nOur method consists of three main parts. First, we model\nrelationships between tokens within modalities using a stan-\ndard transformer that is applied unimodally (Sec. 3.1). Sec-\nond, we aggregate information within local regions of each\nsequence using block-sparse attention and then apply lo-\ncal subsequence pooling to sparsify the token set for each\nmodality (Sec. 3.2). Third, we concatenate the sparsified\nfeatures from each modality and run dense self-attention to\npredict a final class (Sec. 3.3). During training, we apply a\nnovel multimodal variation of manifold mixup [14] for reg-\nularization of intermediate latent representations (Sec. 3.4).\n3.1. Unimodal Modeling\nIn this stage, we apply a separate transformer to the to-\nken set from each modality. Following Vaswani et al. [27],\nwe use a standard L-layer transformer encoder to model re-\nlationships between tokens in each modality. Each layer\nof the encoder consists of layer normalization (LN), Multi-\nhead Self-Attention (MSA), and a Multi-Layer Perceptron\n(MLP). Given token set ˆZl after l transformer layers, the\noutput of layer l + 1is:\nYl = MSA(LN(ˆZl)) +ˆZl (2)\nˆZl+1 = MLP(LN(Yl)) +Yl (3)\nWe apply a separateL-layer transformer per modality to get\ntoken sets ˆZL\n1 , . . . ,ˆZL\nM .\n3.2. Sparse Multimodal Fusion\nIn this stage, we apply local pooling blocks to each to-\nken set ZL\ni to extract k descriptive tokens per modality\n˜Zi = [ ˜zi1, . . . ,˜zik] ∈ Rk×D, as represented by the\n“Sparsify” blocks in Fig. 1. As shown in our experiments in\nSec. 5.3, information is quite redundant within and across\neach modality, and we hypothesize simple sub-sequence\npooling to be a cheap and effective method for capturing im-\nportant information while removing redundancies. Prior to\npooling, we first apply a single bi-directional strided sparse\nattention layer [8] to enforce aggregation of dense local\ncontext and sparse global context to every token in the se-\nquence to each modality. We then apply non-overlapping\nper-channel pooling blocks of stride s for each token set:\n˜zij = pool\n\u0010\nzL\ni(js+1), . . . ,zL\ni(js+s)\n\u0011\n. (4)\nA natural choice for pooling is either per-channel max pool\nor average pool. We explored several options in ablation\nstudies and found our method to be robust to the choice of\npooling (see Table 4). However, for our main experiments\nwe use average pooling.\nWe additionally form a multimodal classification token\n˜c by summing the unimodal classification tokens:\n˜c =\nMX\ni=1\ncL\ni (5)\nThe final, fused token set F is formed using this classifica-\ntion token and the union of the unimodal pooled token sets\n˜Z1, . . . ,˜ZM :\nF = [˜c, ˜z11, . . . ,˜zMk ] (6)\n3.3. Dense Cross-modal Modeling and Prediction\nTo model cross-modal relationships, we apply a dense,\nT-layer transformer on the token set F. Note the tokens of\nF are aggregated from all modalities. We adopt the same\narchitecture used in the unimodal modeling task, denoting\nthe token set after t transformer layers as Ft, with the final\noutput denoted FT = [˜cT , ˜zT\n11, . . . ,˜zT\nMk ]. Finally, a small\nMLP followed by softmax is applied to ˜cT to produce a C-\nway class prediction p.\n3.4. Multimodal Manifold Mixup\nWe apply a novel variation of manifold mixup [14] for\nimproved generalization. In the originally proposed mixup\n[36], given two random training inputs xi and xj, their cor-\nresponding ground-truth labels yi, yj, and an interpolation\nweight λ ∈ [0, 1], a classifier is trained using the following\nvirtual training examples:\n˜x = λxi + (1− λ)xj (7)\n4\n˜y = λyi + (1− λ)yj (8)\nGenerally, the interpolation term λ is sampled from a Beta\ndistribution Beta(α, α), where α is a hyperparameter. Man-\nifold mixup extends this by also selecting a random layer l\nin an L layer network f and interpolating the latent repre-\nsentations vl\ni, vl\nj of that layer instead of the input example:\n˜vl = λvl\ni + (1− λ)vl\nj (9)\nLayers l+1, . . . , Lof f are then applied to˜vl and the output\nis supervised using Eq. 8. Manifold mixup has been shown\nto be more effective for regularization than input mixup.\nWe extend manifold mixup to the multimodal case for\nuse with our model. Given our (L + T)-layer network,\nwith the first L layers involving separate, unimodal trans-\nformers and the last T layers involving a single, multi-\nmodal transformer, we sample a single layer l ∈ [1, L+ T]\nfor manifold mixup. If l > L, we use standard mani-\nfold mixup using Eqs. 8 and 9. If l ≤ L, we sample a\ndifferent interpolation term for each of the M modalities,\nλ1, . . . , λM ∼ Beta(α, α). Given latent representation\nvl\nmi, vl\nmj of layer l for modality m, the new latent repre-\nsentation is given as:\n˜vl\nm = λmvl\nmi + (1− λm)vl\nmj (10)\nThis is applied to every latent representation of layer l for\nevery modality 1, . . . , M. After running the remaining L +\nT − l layers, the output of the network is supervised using:\n˜y = λ∗yi + (1− λ∗)yj (11)\nwhere λ∗ is the average of the M sampled λ values.\n4. Experimental Setup\nWe now describe the datasets used for training and eval-\nuation (Sec. 4.1), dataset pre-processing (Sec. 4.2), baseline\nnetwork architectures used for comparison (Sec. 4.3), and\ntraining hyper-parameters we used (Sec. 4.4).\n4.1. Datasets\nWe perform extensive experiments on two bench-\nmark multimodal datasets: VGG-Sound [7] and CMU-\nMOSEI [34] The datasets tackle popular and broadly ap-\nplicable tasks in multimodal machine learning for audio-\nvisual classification and multimodal sentiment classifica-\ntion. The modalities evaluated include video, audio, and\ntext data. Additionally, these datasets have differences in\nmodality characteristics such as cross-modality alignment\nand information content.\n4.1.1 VGG-Sound\nVGG-Sound [7] consists of over 200,000 YouTube videos\nand their associated audio streams, each annotated with one\nof over 310 class labels. The audio spans a large range\nof challenging acoustic environments and noise character-\nistics of real applications. All videos are captured “in the\nwild.” There are clear audio-visual correspondences, i.e.,\nthe sound source is visually evident. Each segment is 10\nseconds long. To aid in evaluation, we select two subsets\nof data from VGG-Sound containing 10 classes and 100\nclasses each. We call these VGGS10 and VGGS100, re-\nspectively. We select VGGS10 by choosing pairs of easily\nconfused classes, such as “baby babbling” and “baby laugh-\ning”. We then build VGGS100 using these ten classes and\nadditionally include 90 randomly chosen classes. The to-\ntal training and testing set sizes for VGGS10 are 6,051 and\n459. For VGGS100, the training set size is 66,180 and the\ntest set size is 4,549. A validation set is extracted by taking\n20 percent of the training set.\n4.1.2 CMU-MOSEI\nThe CMU Multimodal Opinion Sentiment and Emotion In-\ntensity (CMU-MOSEI) [34] dataset is one of the largest\nmultimodal sentiment analysis and emotion recognition\ndatasets to date. The dataset contains more than 23,500\nsentence utterance videos from more than 1000 online\nYouTube speakers. The dataset is gender-balanced. All\nutterances are randomly chosen from various topics and\nmonologue videos. The task is to predict a 7-class sentiment\nscore of a particular multimodal video sample. Each sam-\nple contains audio, video, and text modalities. This dataset\nis frequently used to explore the unaligned nature of multi-\nmodal sequences between text and video.\n4.2. Pre-processing\nEach modality is pre-processed with a feature extraction\npipeline in order to generate the input token sequence. For\nthe MOSEI dataset, we use the pre-processed data provided\nby the authors. The pre-processing pipeline that was used\nassumes that each video depicts a “talking head”: a single\nhuman talking, whose face is visible and whose voice is\nclearly audible. This assumption is valid for the MOSEI\ndataset, and the pre-processing pipeline therefore extracts\nvisual features such as facial landmark positions and audio\nfeatures such as estimated vocal parameters. We refer the\nreader to Zadehet al. [34] for the full details. To pre-process\nVGGSound, we employ a feature extraction pipeline that\ncan be applied to videos more generally, without assuming\nhuman faces or voices are present.\nFor the VGGS10 and VGGS100 datasets, we extract vi-\nsual features using I3D [6], a spatio-temporal video feature\nextraction model that was pre-trained on the Kinetics hu-\nman action recognition dataset [6]. This is a two-stream\nmodel, which processes optical flow and raw RGB indepen-\ndently as two separate modalities. We also extract TV-L 1\n5\noptical flow from the VGGSound videos. For Audio pre-\nprocessing we follow Nagrani et al. [16]: we resample all\naudio at 16Hz and convert to mono, then compute log mel\nspectrograms with 128 frequency bins, using a Hamming\nwindow with size 25ms and stride 10ms.\n4.3. Baseline Network Architectures\nWe compare against the following transformer-based fu-\nsion methods:\nSelf-Attention Fusion (Concat): A baseline method of\nfusion is to concatenate the individual modality representa-\ntions prior to input to any network and rely exclusively on\ndense self-attention. This is a form of early fusion.\nLate Fusion (LF): This method works by applying\ntransformer blocks on individual modalities only. The fi-\nnal prediction is obtained via a summation of logits derived\nfrom individual class tokens. This helps us compare the\nbenefit of modeling cross-modal interactions.\nMultimodal Transformer (MulT): [26] MulT is a hy-\nbrid early-late attention-based fusion method using a unique\ncross-modal attention mechanism. The data is first fused\nvia an attention mechanism by using one modality each for\nkey, query, and value. Transformer blocks are then stacked\non top. At the very end, the features are concatenated and a\nprediction is obtained after an FC layer.\nBottleneck Fusion (MBT): [16] This is a form of fusion\nin which special tokens called bottleneck tokens are intro-\nduced. These tokens are shared among all modalities, and\ntransformers alternate operating on each modality indepen-\ndently. The final CLS token is summed from each modality\nand used for prediction. We additionally evaluate MBT us-\ning manifold mixup (MBT+MM) as the original paper used\ninput mixup, and our inputs are features.\n4.4. Implementation details\nOur model is implemented in PyTorch. For all experi-\nments on the smaller datasets VGGS10 and MOSEI we use\na learning rate of 10−4. For the larger dataset VGGS100\nwe use a learning rate of 10−3. Learning rate is decayed\nby factor of 10 every 10 epochs based on minimum vali-\ndation loss. We use a batch size of 24 for all experiments.\nFor all datasets, we report results based on averaging per-\nformance training from 5 different seeds for generalization\npurposes and to minimize tuning effects. We use a standard\n12-layer network and 5 attention heads for all evaluations.\nWe project embeddings from each modality to 40 to mini-\nmize the effects of over-parameterization. For experiments\ninvolving latent mixup, we used a strength of α = 0.3. We\nuse an initial warm-up of 5 epochs in which no mixup is ap-\nplied. For all other experiments we applied dropoutp = 0.2\nfor regularization. For baselines, we follow descriptions in\noriginal papers and publicly available code for comparison.\nAll experiments were conducted on consumer-grade graph-\nics cards. We make our code and preprocessed data publicly\navailable.\n4.5. Metrics\nWe report results using commonly used metrics. Top1\nrepresents the accuracy of the most likely class. mAP rep-\nresents the mean of per-class average precision scores. We\nalso report the computational cost in Giga floating-point\noperations (GFlops) which is estimated similar to previous\nmethods [18] (we provide the equations used for estimating\nthis in Appendix A.2 included in the supplementary mate-\nrials). Many experiments examine the effect of a reduction\nfactor, which refers to reducing the number of tokens in the\nsequence dimension for transformer architectures. We re-\nport most results as a mean and standard deviation of exper-\niments run with five different seeds.\n5. Results\nWe first report our results against state of the art\n(Sec. 5.1) showcasing our performance on multiple datasets\nfrom different domains. We then perform a series of abla-\ntion studies to explore the effects of sparsification (Sec. 5.2),\nand the benefits of addressing within-modality redundan-\ncies during fusion (Sec. 5.3). We also study the effect of\npooling choice (Sec. 5.4) and the effect of our proposed\nmultimodal manifold mixup (Sec. 5.5).\n5.1. Comparison against state of the art\nWe present our summary benchmark performance on\nreal-world datasets VGGS10, VGGS100, and MOSEI in\nTables 1 and 2. For each dataset, our model keeps a sub-\nset of tokens from each modality during pruning. For VG-\nGSound data after pooling we have 12 tokens of RGB and\nflow information and 20 tokens spectrogram data. For MO-\nSEI, we keep 10 tokens of visual and audio information and\n25 tokens of text information. These numbers were chosen\naccording to experiments described in Sec. 5.3.\nWe maintain the performance of existing fusion methods\nand exceed them in some situations while significantly re-\nducing the amount of computation required. For MOSEI we\nreport more than a five-fold reduction in computational cost\nwhile achieving the best performance in terms of both Top1\naccuracy and mAP. For VGGS10 and VGGS100, we ob-\nserve approximately a six-fold reduction in computational\ncost. Our method also exceeds the performance of multiple\nfusion methods on the VGGS100 dataset.\n5.2. Effect of Sparsification\nIn this section, we explore the effect of how naively ap-\nplying pooling can affect multimodal models. In particu-\nlar, we are interested in how pooling affects fused versus\nmodality-independent features. We answer this question by\n6\nVGGS10 VGGS100 MOSEI\nTop1 mAP Top1 mAP Top1 mAP\nConcat 67.62±1.3 71.46±.63 51.72±.26 51 .64±.13 48.47±.23 32 .40±.83\nLF 67.10±.79 70 .46±.79 52.00±.73 46 .92±.28 49.10±.33 31 .75±.78\nMulT 65.49±.40 69 .73±1.1 51.35±.43 49 .25±.43 49.36±.34 31 .92±.79\nMBT 66.84±.61 70 .98±.78 51.67±.66 51 .29±.37 49.12±.27 32 .15±.47\nMBT+MM 66.80±1.8 70 .56±.61 55.97±.42 57.29±.37 48.77±.37 32 .03±1.2\nOurs 67.71±1.3 71 .06±.81 55.61±.61 57 .18±.39 49.67±.23 33.66±.85\nTable 1. Accuracy comparison for each dataset and model. For all benchmarks we report the mean and standard deviation performance\nover 5 seeds to minimize tuning effects. Bold indicates best, underline second best. We are either best or close to best in all metrics.\nVGGS10/VGGS100 MOSEI\nMem (GB) Eval (ms) Train (ms) GFlops Mem (GB) Eval (ms) Train (ms) GFlops\nConcat 1.52 (3.16×) 3 .59 (2.46×) 10 .93 (2.56×) 1 .68 (6.72×) 1.04 (11.95×) 2 .71 (2.39×) 8 .02 (2.01×) 1 .16 (11.60×)\nLF 1.35 (2.80×) 3 .54 (2.42×) 12 .32 (2.88×) 1 .51 (6.04×) 0.49 (5.66×) 2 .00 (1.77×) 7 .66 (1.92×) 0 .59 (5.90×)\nMulT 1.18 (2.45×) 3 .53 (2.41×) 16 .73 (3.91×) 2 .64 (10.56×) 0.62 (7.10×) 2 .84 (2.50×) 11 .49 (2.88×) 1 .03 (10.30×)\nMBT 1.35 (2.82×) 3 .59 (2.45×) 12 .02 (2.81×) 1 .52 (6.08×) 0.50 (5.72×) 2 .14 (1.89×) 7 .42 (1.86×) 0 .59 (5.90×)\nOurs 0.48 1.46 4.27 0.25 0.09 1.13 3.99 0.10\nTable 2. Computational cost comparison for each dataset and model. For all metrics we obtain results with a single RTX 3090. Metrics\nare normalized by the batch size. Our method has the lowest cost. GFlops is estimated based on number of transformer blocks and token\noperations and represents a theoretical cost for a single forward pass through the network. We present the equations used for calculations\nin the Appendix A.2 of the supplementary material.\nToken Reduction Factor\nNone 64 × Diff.\nConcat Top1 51.72±.26 46 .29±.73 −5.45\nmAP 51.64±.13 47 .49±.63 −4.49\nLF Top1 52.00±.73 49 .76±.71 −2.24\nmAP 46.92±.28 45 .96±.62 −0.96\nOurs Top1 55.57±.23 55.98±.28 +0.41\nmAP 56.54±.49 56.91±.60 +0.37\nTable 3. Comparison of our method for sparsification versus appli-\ncation of only pooling in baseline methods on VGGS100. Diff\ncolumn shows difference between no reduction of tokens and tak-\ning 1/64ths of the tokens, where the minimum is one token per\nmodality. Our method is more robust than naive methods of pool-\ning. Pooling has a large effect when training with fused features\n(Concat) which we solve using our method. Difference for the\nsame reduction factors between Top1 and mAP shows that late fu-\nsion (LF) tends to fit some samples better than others and suggests\nthe advantages of an early-fusion method.\ncomparing the performance of late fusion, concatenation\nfusion, and our fusion method. For concatenation fusion,\nwe concatenate all the input tokens prior to input into the\nmodel. From here, we apply a single transformer block as\nif the number of modalities is M = 1. We then apply max\npool with a kernel and stride of 64. Afterwards, we apply\neleven more transformer layers to obtain the result. For late\nfusion and our method, we also apply pooling on the repre-\nsentations after the first layer. However, the pooling is con-\nducted on unimodal representations. In late fusion, trans-\nformer layers are applied independently for each modality\nand the final result is obtained via a summation of logits\nobtained from the CLS token. In experiments described in\nSec. 5.3, we observe a drop in performance for our method\nwith strides larger than 32 for some datasets and 128 for oth-\ners, thus we assume a stride of 64 will provide meaningful\ncomparisons between fusion methods.\nThe results shown in Table 3 demonstrates that our\nmethod for sparsification is more robust than naive methods.\nWe see that in both naive methods of pooling, the reduction\nin the sequence dimension causes a significant drop in per-\nformance. Our method does not see any reduction, instead\nexperiencing a small boost in performance. Furthermore,\nwe see that concatenation fusion tends to have a higher mAP\nmetric, whereas late fusion has a higher Top1. Overall, our\nmethod is robust, and pooling has no detrimental effect even\nwhen removing over 98% of tokens.\n5.3. Within-Modality Information Redundancy\nWe provide experiments to analyze why it is advanta-\ngeous to address the within-modality redundancy problem\nduring fusion. In particular, we wish to show that pooling\nwhen accounting for multimodal information is more ro-\nbust than pooling without this information. We set up the\nexperiment so that a max-pooling layer is applied after the\nfirst layer of transformers to simulate modality-independent\nfeature sparsification for each method. We then compare\npruning by an equal factor for each modality to observe\nthe effect on overall performance, referred to as “sequence\nreduction factor.” We set the minimum allowed sequence\nlength to one to avoid removing all tokens. We compare\nagainst unimodal transformers for each modality. We also\nevaluate two versions of our method: SFT which is our full\npipeline, and SFT-PO which removes the strided sparse at-\n7\nnone 2 4 8 16 32 64 128 256 max\nSequence  R eduction F actor\n10\n20\n30\n40\n50\n60\n70\nT op1 (P ercent)\nSFT SFT -PO Spectrogram RGB Flow\nnone 2 4 8 16 32 64 128 256 max\nSequence R eduction F actor\n−20\n−15\n−10\n−5\n0\n5\n10\n15\n20\nT op1 Relative Cha ge (Perce t)\n(a) Top 1 absolute score and relative change from\nno pooling for VGGS10. Multimodal perfor-\nmance degredation occurs after a 64-fold reduc-\ntion in sequence length. Compared to flow at\n4, and spectrogram at 64. We outperform all all\nmethods at all reduction levels. SFT exceeds the\npooling only variant (SFT-PO).\nnone 2 4 8 16 32 64 128 256 max\nSequence  R eduction F actor\n10\n20\n30\n40\n50\n60\n70\nT op1 (P ercent)\nSFT SFT -PO Spectrogram RGB Flow\nnone 2 4 8 16 32 64 128 256 max\nSequence R eduction F actor\n−20\n−15\n−10\n−5\n0\n5\n10\n15\n20\nT op1 Relative Cha ge (Perce t)\n(b) Top 1 metrics for VGGS100. SFT degrada-\ntion occurs at 256-fold reduction compared to 64\nfor SFT-PO and 2 for Flow and RGB. Audio rep-\nresentations might benefit from better feature ex-\ntraction, however there is dramatic loss of perfor-\nmance with very few tokens, while we remain tol-\nerant.\nnone 2 4 8 16 32 64 128 256 max\nSequence  R eduction F actor\n10\n20\n30\n40\n50\n60\n70\nT op1 (P ercent)\nSFT SFT -PO Audio Visual T ext\nnone 2 4 8 16 32 64 128 256 max\nSequence R eduction F actor\n−20\n−15\n−10\n−5\n0\n5\n10\n15\n20\nT op1 Relative Cha ge (Percent)\n(c) Top 1 metrics for MOSEI. SFT degrades min-\nimally until max while SFT-PO degrades at 32.\nText modality degrades immediately. Information\nappears highly redundant in Audio-Visual modal-\nities.\nFigure 2. Comparison of reduction factor effect on performance difference against no reduction for unimodal and multimodal models.\nReduction in total length of fused features reported in the x-axis. In cases where the reduction factor is greater than sequence length of\na particular modality, a single token along the sequence dimension is passed through. Sequence lengths for VGGS10 and VGGS100 are\n38 for RGB and Flow, 1200 for Spectrogram. For MOSEI, Audio and Visual is 500 while text is 50. Top1 absolute score and relative\nchange from using no pruning is reported. For all experiments we used a batch size of 24. Multimodal models will tolerate more pruning\nover unimodal models by making up for the lost information through fusion. Notably, SFT exceeds performance of SFT without sparse\nattention or mixup (SFT-PO) in all cases and tolerates more reduction. Pooling offers some benefits for feature extraction in some cases\nfor longer sequences.\ntention layer and multimodal manifold mixup and includes\nonly the strided pooling.\nIn the first column of Fig. 2, we present Top1 accuracy\nas a function of sequence reduction factor. In the second\ncolumn, we present the relative change in Top1 accuracy\nwhen compared with no sequence reduction. Lower indi-\ncates a performance degradation from sequence reduction.\nMultimodal models exceed unimodal performance in all re-\nduction factors. We generally see a performance decrease\nfor each unimodal model as the reduction factor increases.\nHowever, some modalities do not decrease due to two likely\nreasons: 1) from redundancies in information and 2) that all\nuseful information was extracted after just a single layer of\ntransformers. We also see that some modalities experience\nan increase in performance as we reduce the number of to-\nkens, signifying better feature extraction for those. How-\never, in general, the performance of unimodal models with\nless redundant information all decrease, while our model\n(SFT) is more robust. In particular, SFT is better than us-\ning just pooling (SFT-PO) as is evident from it maintaining\nhigher performance with greater reduction factors.\nWe see that up to a factor of 50 for evaluations con-\nducted on MOSEI, there is very minimal drop in perfor-\nmance in the multimodal model. However, the performance\nof the text-only transformer drops observably larger than\nour multimodal model. The performance of the RGB and\nAudio transformers remains the same throughout the exper-\niment. This signifies two things: that the information for\nlabel present in the text classifier is less redundant than in\nRGB and Audio features for this dataset, and that applica-\ntion of sparse fusion can compensate for the loss of infor-\nmation necessary for classification by exploiting the other\nmodalities. The effect of unimodal models experiencing a\ndecrease in performance is also evident for the optical flow\nmodality on the VGGS10 dataset at 8 × reduction, and at\n64× reduction for spectrogram data. On VGGS100, we see\nthe same, where both the RGB and flow modalities expe-\nrience decreases in performance with a pruning factor of\njust 2× while our model’s performance remains relatively\nflat. Furthermore, our multimodal model with one token\nper-modality after pruning still achieves better performance\nthan a unimodal model which uses all tokens.\nThese observations signify that certain modalities con-\ntain information that is more redundant than others and that\neven if we filter out more than what a model with redundant\ninformation is able to predict, the multimodal model is able\nto make up for that. The same is not true for unimodal mod-\nels, which cannot filter out unnecessary information as well,\nand is not robust to this reduction. Even under extreme cir-\ncumstances where information is reduced to the length of a\nsingle token, performance of the multimodal degrades but\nstill remains the overall top performer.\n5.4. Effect of Pooling\nIn this section, we explore the effects of using various\npooling choices in the network. See Table 4 for results\non the VGGS10, VGGS100, and MOSEI datasets. We use\nmax pooling, average pooling, and attention-weighted av-\nerage pooling, denoted “Max,” “Average,” and “Attn Av-\nerage” respectively. For attention-weighted averaging, we\nweight using a simple, attention-based per-token signifi-\ncance metric proposed by Goyal et al. [11]. Given the at-\n8\nPooling Method VGGS10 VGGS100 MOSEI\nTop1 mAP Top1 mAP Top1 mAP\nMax 67.0±1.1 70.7 ±0.7 55.7±0.4 57.3 ±0.4 49.4±0.2 33.2 ±0.3\nAverage 67.7±1.2 71.1 ±0.7 55.6±0.5 57.2 ±0.3 49.7±0.2 33.7 ±0.9\nAttn Average 67.5±1.0 71.1 ±0.7 55.4±0.3 56.9 ±0.4 49.4±0.2 33.7 ±0.8\nTable 4. Comparison of pooling method on VGGS10, VGGS100, and MOSEI datasets. Based on Top 1 accuracy and mean average\nprecision metrics, we find our method robust to pooling type.\nmixup? Top1 mAP\n✓ 55.61 ± .61 57 .18 ± .39\n× 51.30 ± .80 51 .80 ± .44\nTable 5. Comparison of model performance on VGGS100 when\ntrained with and without our multimodal manifold mixup.\ntention weights Wh ∈ RN×N calculated from layer L head\nh ∈ {1, . . . , H} of the pre-fusion network, the significance\n(sig) for token i is:\nsig(i) =\nHX\nh=1\nNX\nn=1\nWh\nin (12)\nInterestingly, all metrics are within 1 percentage point of\neach other across the three pooling types. This indicates our\nmodel is quite robust to the choice of pooling type. Average\npooling appears better, but this is well within the std. dev.\n5.5. Effect of Multimodal Manifold Mixup\nSee Table 5 for results from SFT trained on VGGS100\nwith and without the use of our multimodal manifold mixup\nduring training. Without mixup, we observe over a 4% re-\nductin in Top1 and over a5% reduction in mAP. This drop in\nperformance is quite significant, indicating the effectiveness\nof training with our proposed multimodal manifold mixup.\n6. Limitations\nWe provide an effective method for quickly ingesting\nand classifying large quantities of multimodal sequential\ndata with high levels of accuracy. However, we do not pro-\nvide evaluations on how this fusion method might behave\nas part of a generative network and we leave this for future\nwork. Secondly, our methods operate on extracted features\nsuch as I3D and spectrogram data. While we follow pop-\nular and common settings for feature extraction, improved\nunimodal modeling might be able to condense the repre-\nsentations and reduce within-modality redundancy. This\nwould lead to slightly reduced complexity benefits. How-\never, the large differences between our results and unimodal\napproaches as well as maintaining performance under ex-\ntreme sparsification support our conclusions.\n7. Conclusion\nWe present an effective technique that offers more than\na five-fold reduction in computational cost while maintain-\ning the performance of state-of-the-art fusion techniques.\nDifferent fusion methods exhibit improved performance un-\nder varying conditions when all input conditions are equal.\nHowever, when optimizing for speed, there are drastic im-\nprovements that can be made to feature selection during\ncross-modal modeling that can improve performance.\nBroader Impacts: We propose sparse fusion for mul-\ntimodal transformers as a method to reduce computational\ncosts. This translates to energy savings and is beneficial\nfor numerous applications including on mobile devices.\nNamely, it has the potential to train and fine-tune a net-\nwork for use to a specific user without needing to offload\nthe training to a server. This preserves the privacy of the\nuser while providing benefits of performance and energy\nsavings. Furthermore, we hope to spur democratization of\nlearning on large datasets by enabling rapid development\nand evaluation on consumer-level hardware. However, we\nhope that by enabling this technology on mobile devices it\nis not applied to tasks such as unlawful surveillance.\nReferences\n[1] Sara Atito, Muhammad Awais, and Josef Kittler. SiT: Self-\nsupervised vision transformer.CoRR, abs/2104.03602, 2021.\n2\n[2] Tadas Baltru ˇsaitis, Chaitanya Ahuja, and Louis-Philippe\nMorency. Multimodal machine learning: A survey and tax-\nonomy. IEEE transactions on pattern analysis and machine\nintelligence, 41(2):423–443, 2018. 3\n[3] Hangbo Bao, Li Dong, and Furu Wei. BEiT: BERT pre-\ntraining of image transformers, 2021. 2\n[4] David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas\nPapernot, Avital Oliver, and Colin A Raffel. Mixmatch:\nA holistic approach to semi-supervised learning. In Ad-\nvances in Neural Information Processing Systems , pages\n5049–5059, 2019. 3\n[5] Alja ˇz Bo ˇziˇc, Pablo Palafox, Justus Thies, Angela Dai, and\nMatthias Nießner. TransformerFusion: Monocular RGB\nscene reconstruction using transformers. Proc. Neural In-\nformation Processing Systems (NeurIPS), 2021. 3\n[6] Joao Carreira and Andrew Zisserman. Quo vadis, action\nrecognition? a new model and the kinetics dataset. In pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pages 6299–6308, 2017. 5\n9\n[7] Honglie Chen, Weidi Xie, Andrea Vedaldi, and Andrew Zis-\nserman. VGGSound: A large-scale audio-visual dataset.\nIn ICASSP 2020-2020 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) , pages\n721–725. IEEE, 2020. 5\n[8] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.\nGenerating long sequences with sparse transformers. URL\nhttps://openai.com/blog/sparse-transformers, 2019. 4\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. CoRR, abs/2010.11929, 2020. 2, 3\n[10] Christoph Feichtenhofer, Axel Pinz, and Andrew Zisserman.\nConvolutional two-stream network fusion for video action\nrecognition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 1933–1941,\n2016. 3\n[11] Saurabh Goyal, Anamitra R. Choudhury, Saurabh M. Raje,\nVenkatesan T. Chakaravarthy, Yogish Sabharwal, and Ashish\nVerma. PoWER-BERT: Accelerating BERT inference\nvia progressive word-vector elimination. arXiv preprint\narXiv:2001.08950, 2020. 8\n[12] Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zis-\nserman, Oriol Vinyals, and Joao Carreira. Perceiver: Gen-\neral perception with iterative attention. arXiv preprint\narXiv:2103.03206, 2021. 3\n[13] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya.\nReformer: The efficient transformer. arXiv preprint\narXiv:2001.04451, 2020. 3\n[14] Yang Liu, Alexandras Neophytou, Sunando Sengupta, and\nEric Sommerlade. Cross-modal spectrum transformation\nnetwork for acoustic scene classification. In ICASSP 2021-\n2021 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP) , pages 830–834. IEEE,\n2021. 3, 4\n[15] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. In ICLR, 2019. 3\n[16] Arsha Nagrani, Shan Yang, Anurag Arnab, Aren Jansen,\nCordelia Schmid, and Chen Sun. Attention bottlenecks for\nmultimodal fusion. arXiv preprint arXiv:2107.00135, 2021.\n2, 3, 6\n[17] Kento Nishi, Yi Ding, Alex Rich, and Tobias Hollerer. Aug-\nmentation strategies for learning with noisy labels. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 8022–8031, 2021. 3\n[18] Zizheng Pan, Bohan Zhuang, Jing Liu, Haoyu He, and Jian-\nfei Cai. Scalable vision transformers with hierarchical pool-\ning. In Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision, pages 377–386, 2021. 2, 3, 6, 12\n[19] Jack W Rae, Anna Potapenko, Siddhant M Jayaku-\nmar, and Timothy P Lillicrap. Compressive transform-\ners for long-range sequence modelling. arXiv preprint\narXiv:1911.05507, 2019. 3\n[20] Wasifur Rahman, Md Kamrul Hasan, Sangwu Lee, AmirAli\nBagher Zadeh, Chengfeng Mao, Louis-Philippe Morency,\nand Ehsan Hoque. Integrating multimodal information in\nlarge pretrained transformers. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational Lin-\nguistics, pages 2359–2369, Online, July 2020. Association\nfor Computational Linguistics. 2\n[21] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and\nYuxiong He. DeepSpeed: System optimizations enable train-\ning deep learning models with over 100 billion parame-\nters. In Proceedings of the 26th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining, pages\n3505–3506, 2020. 3\n[22] Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi,\nOlatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li,\nand Yuxiong He. Zero-offload: Democratizing billion-scale\nmodel training. arXiv preprint arXiv:2101.06840, 2021. 3\n[23] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya\nSutskever, and Ruslan Salakhutdinov. Dropout: a simple way\nto prevent neural networks from overfitting. The journal of\nmachine learning research, 15(1):1929–1958, 2014. 3\n[24] Noah Stier, Alexander Rich, Pradeep Sen, and Tobias\nH¨ollerer. V oRTX: V olumetric 3D reconstruction with trans-\nformers for voxelwise view selection and fusion. In 3DV,\n2021. 3\n[25] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herve Jegou. Train-\ning data-efficient image transformers & distillation through\nattention. In Marina Meila and Tong Zhang, editors, Pro-\nceedings of the 38th International Conference on Machine\nLearning, volume 139 of Proceedings of Machine Learning\nResearch, pages 10347–10357. PMLR, 18–24 Jul 2021. 2, 3\n[26] Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang, J Zico\nKolter, Louis-Philippe Morency, and Ruslan Salakhutdinov.\nMultimodal transformer for unaligned multimodal language\nsequences. In Proceedings of the conference. Association\nfor Computational Linguistics. Meeting, volume 2019, page\n6558. NIH Public Access, 2019. 3, 6\n[27] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in Neural\nInformation Processing Systems, 2017. 3, 4\n[28] Yulin Wang, Rui Huang, Shiji Song, Zeyi Huang, and Gao\nHuang. Not all images are worth 16x16 words: Dynamic\ntransformers for efficient image recognition. In Advances in\nNeural Information Processing Systems (NeurIPS), 2021. 2\n[29] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir\nZadeh, and Louis-Philippe Morency. Words can shift: Dy-\nnamically adjusting word representations using nonverbal\nbehaviors. In Proceedings of the AAAI Conference on Ar-\ntificial Intelligence, pages 7216–7223, 2019. 3\n[30] Zhiguo Wang, Patrick Ng, Xiaofei Ma, Ramesh Nallapati,\nand Bing Xiang. Multi-passage BERT: A globally nor-\nmalized BERT model for open-domain question answering.\narXiv preprint arXiv:1908.08167, 2019. 3\n[31] Zihao Ye, Qipeng Guo, Quan Gan, Xipeng Qiu, and Zheng\nZhang. BP-transformer: Modelling long-range context via\nbinary partitioning. arXiv preprint arXiv:1911.04070, 2019.\n3\n10\n[32] B.P. Yuhas, M.H. Goldstein, and T.J. Sejnowski. Integration\nof acoustic and visual speech signals using neural networks.\nIEEE Communications Magazine, 27(11):65–71, 1989. 3\n[33] Amir Zadeh, Paul Pu Liang, Navonil Mazumder, Soujanya\nPoria, Erik Cambria, and Louis-Philippe Morency. Memory\nfusion network for multi-view sequential learning. In Pro-\nceedings of the AAAI Conference on Artificial Intelligence ,\n2018. 3\n[34] Amir Zadeh, Paul Pu Liang, Soujanya Poria, Erik Cambria,\nand Louis-Philippe Morency. Multimodal language analysis\nin the wild: CMU-MOSEI dataset and interpretable dynamic\nfusion graph. In Proceedings of the 56th Annual Meeting\nof the Association for Computational Linguistics (Volume 1:\nLong Papers), pages 2236–2246, 2018. 5\n[35] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey,\nJoshua Ainslie, Chris Alberti, Santiago Ontanon, Philip\nPham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr\nAhmed. Big Bird: Transformers for longer sequences. In\nNeurIPS, 2020. 3\n[36] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and\nDavid Lopez-Paz. mixup: Beyond empirical risk minimiza-\ntion. arXiv preprint arXiv:1710.09412, 2017. 3, 4\n[37] Minjia Zhang and Yuxiong He. Accelerating training of\ntransformer-based language models with progressive layer\ndropping. arXiv preprint arXiv:2010.13369, 2020. 3\n11\nA. Appendix\nA.1. Label Distributions\nWe summarize the classes we used in VGGS10 and\nprovide the label distributions in VGGS10 and VGGS100.\nVGGS10 is a manually curated dataset built by select-\ning pairs of difficult to separate classes from the full VG-\nGSound dataset as well as for differences between video\nand audio modalities. We chose the following ten classes:\nairplane, baby babbling, baby crying, baby laughter, cat me-\nowing, cat purring, people marching, people running, play-\ning bass guitar, playing electric guitar. The final training\nset distribution for VS10 in Fig. 3, and the final VGGS100\ndataset distributions are show in Fig. 4.\nplaying_bass_guitar\nplaying_electric_guitar\ncat_purring\npeople_marching\nbaby_laughter\nbaby_crying\nbaby_babbling\ncat_meowing\nairplane\npeople_running\nclass\n0\n200\n400\n600\n800\ncounts\nFigure 3. Label distribution of VGGS10 dataset\nclass\n0\n200\n400\n600\n800\ncounts\nFigure 4. Label distribution of VGGS100 dataset\nA.2. Flop computation\nWe present all flop estimates in the paper using the fol-\nlowing equations. We primarily follow the flop estimation\nfrom [18] with some minor changes due to layer differ-\nences. Each transformer layer consists of a multi-head at-\ntention and multilayer perceptro block. A multi-head atten-\ntion (MHA) block has cost of:\nϕMHA = ϕqkv + ϕA + ϕO + ϕproj\n= 3nd2 + n2d + n2d + nd2\n= 4nd2 + 2n2d (13)\nwhere n, drepresent the length and embedding dimension,\nϕqkv is the cost of projecting to the query, key, and values.\nϕA is the cost of the attention map, ϕO is the cost of the\nself attention, and ϕproj is the cost of projection for self-\nattention outputs.\nA MLP block includes two linear layers as well as a nor-\nmalization layer for a cost of:\nϕMLP = ϕproj1 + ϕnorm + ϕa + ϕproj2\n= nd2 + 3nd + nd + nd2\n= 2nd2 + 4nd (14)\nwhere ϕproj1 and ϕproj2 are cost of projecting into and out\nof latent space for transformer block, ϕnorm represents cost\nof applying layer normalization, and ϕa represents the cost\nof an activation function.\n12",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7424191832542419
    },
    {
      "name": "Pooling",
      "score": 0.7220327258110046
    },
    {
      "name": "Artificial intelligence",
      "score": 0.627013623714447
    },
    {
      "name": "Memory footprint",
      "score": 0.6125568151473999
    },
    {
      "name": "Machine learning",
      "score": 0.6017777919769287
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5522459149360657
    },
    {
      "name": "Transformer",
      "score": 0.44748198986053467
    },
    {
      "name": "Engineering",
      "score": 0.09821560978889465
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}