{
  "title": "Automated Spinal MRI Labelling from Reports Using a Large Language Model",
  "url": "https://openalex.org/W4403088691",
  "year": 2024,
  "authors": [
    {
      "id": null,
      "name": "Park, Robin Y.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3162312415",
      "name": "Windsor Rhydian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3163113571",
      "name": "Jamaludin, Amir",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2930261228",
      "name": "Zisserman, Andrew",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4386075916",
    "https://openalex.org/W4312533035",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W2963466845",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W4297253404",
    "https://openalex.org/W4389618885",
    "https://openalex.org/W4389156617",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2613482391",
    "https://openalex.org/W4384561707",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4295934551",
    "https://openalex.org/W4282983782"
  ],
  "abstract": null,
  "full_text": "Automated Spinal MRI Labelling from Reports\nUsing a Large Language Model\nRobin Y. Park, Rhydian Windsor, Amir Jamaludin, and Andrew Zisserman\nVisual Geometry Group, Department of Engineering Science,\nUniversity of Oxford, Oxford, UK\nrobinpark@robots.ox.ac.uk\nAbstract. We propose a general pipeline to automate the extraction\nof labels from radiology reports using large language models, which we\nvalidate on spinal MRI reports. The efficacy of our labelling method\nis measured on five distinct conditions: spinal cancer, stenosis, spondy-\nlolisthesis, cauda equina compression and herniation. Using open-source\nmodels, our method equals or surpasses GPT-4 on a held-out set of re-\nports. Furthermore, we show that the extracted labels can be used to\ntrain imaging models to classify the identified conditions in the accom-\npanying MR scans. All classifiers trained using automated labels achieve\ncomparable performance to models trained using scans manually anno-\ntated by clinicians1.\nKeywords: Radiological reports· Cancer · Metastasis · Stenosis.\n1 Introduction\nLabelling medical image datasets can be time-consuming and requires expert\nannotators, whose time is limited and expensive. This is compounded by a large\nnumber of medical conditions that can occur in any given image and often large\ninter-reader variability leading to noisy labels. This means researchers applying\ndeep learning to medical imaging problems usually settle for small-scale datasets\ncompared to other areas in machine learning, spend large amounts of funding on\ndata collection, or restrict themselves to a few publicly-available datasets cover-\ning only a few conditions and modalities. A possible solution to this problem is to\nextract labels directly from radiological reports – free-text descriptions written\nby radiologists describing the findings of an imaging investigation. These reports\ncan be downloaded in bulk from a hospital’s electronic database; if the extrac-\ntion can be automated, it would significantly reduce the annotation bottleneck\nand unlock much larger training datasets for solving medical imaging problems.\nHowever, automated extraction of structured information from clinical re-\nports is not a new idea and has proved to be challenging [18], due to large\nvariability in reporting styles, heavy use of domain-specific vocabulary and fre-\nquently assumed knowledge (e.g. a report describing an investigation into the\n1 Code can be found at https://github.com/robinyjpark/AutoLabelClassifier.\narXiv:2410.17235v1  [eess.IV]  22 Oct 2024\n2 R. Y. Park et al.\npresence of metastasis implies the existence of a primary cancer). We propose\na general method for extracting structured labels for vision models from clini-\ncal reports. This is achieved by adapting general-purpose large language models\n(LLMs) by asking the model to summarise the report with a target condition\nin mind and produce a binary label based on the summary. Crucially, to obtain\nlabels for a new medical condition, all that is required is the class name and a\ndefinition but no further fine-tuning.\nTo test the efficacy of this method, we apply it to spinal magnetic reso-\nnance imaging (MRI) radiological reports, aiming to label spinal cancer, stenosis,\nspondylolisthesis, cauda equina compression and herniation. Adapting Zephyr\n(7B) and Llama3 Instruct (8B), two open-source question-answering LLMs, our\nmethod achieves balanced accuracy and F1 scores equal to or exceeding that of\nGPT-4 for both conditions. We then use the extracted labels to train a vision\nmodel to detect cancer, stenosis and spondylolisthesis in spinal MRI. The classi-\nfication networks trained using the automated labels are able to match the per-\nformance of existing classifiers trained using large volumes of expert-annotated\nimages.\n1.1 Related Work\nRule-based report parsers have shown strong performance in many settings (e.g.\nCheXpert [7], DeepSpine [11]). However, developing these parsers requires do-\nmain knowledge and cannot be easily adapted to new conditions, which conflicts\nwith our aim of reducing reliance on expert input and manual annotations.\nLLMs have become increasingly accessible to researchers with the emergence\nof open-source models such as Llama [19], Alpaca [17] and Mistral [8]. Methods\nlike instruction fine-tuning [15] can improve models’ alignment with question-\nansweringtasks,andlowrankadaptation(LoRA)[6]allowstraininglargemodels\nwith limited compute. Many existing models have been further pre-trained on\nbiomedical data to increase the base model’s familiarity with biomedical vocab-\nulary and syntax [5, 10]. However, these are often trained using clinical abstracts\nand papers, which have a different form to radiological reports [9, 10, 12] or use\nreports from a single modality, e.g. Chest X-rays [1, 2]. RadBERT [22] is an\nexample of a model pre-trained using a large corpus of diverse radiology reports;\nhowever, this is a 110M parameter BERT-based model and is thus unlikely to\nadapt to new tasks as well as much larger publicly-available general models.\nAccordingly, as more powerful LLMs continue to emerge, there has been a\ngrowing focus on the development of generalisable methods that do not require\nspecialised training on the task at hand. Steering GPT-4 by asking the model\nto come up with its own chain-of-thought has achieved state-of-the-art results,\ndemonstrating significant gains in accuracy on medical question-answering over\nspecialist fine-tuned models like Med-PaLM 2 [14]. While GPT-4 achieves ex-\ncellent performance across many specialist domains, costs can be high when\nprocessing large datasets. Furthermore, closed-source LLMs accessible through\nAPIs like GPT-4 require data to be uploaded to remote servers for processing,\nwhich poses a privacy risk, especially for medical reports containing potentially\nAutomated Spinal MRI Labelling 3\nsensitive information. Finally, since weights are not publicly available, the model\ncannot be customised or incorporated into vision-language models downstream.\n2 Extracting Structured Labels from Radiological\nReports Using a Large Language Model\nFig.1. Radiological report labelling pipeline:The prompt step formats the user\ninputs as shown in Figure 2 to summarise the report based on the target condition.\nBased on this summary, we extract the binary label using the normalised scores from\na chosen set of two unique tokens (“yes” and “no”) in the vocabulary.\nFigure 1 shows an overview of our method to extract labels from clinical\nreports. The user specifies the condition for classification along with a definition\nto reduce ambiguity. These are input into a general-purpose prompting template\n(see Figure 2). Our method has two steps: (1) asking the model to generate a\nsummary of the report based on the target condition, and (2) using the summary\ntoassign abinary label.Wealso perform self-supervisedfine-tuning tofamiliarise\nthe model with the summary generation task, which is described in Section 2.2.\n2.1 Model Prompting\nWe tested our prompting method using Zephyr-7B and Llama-8B Instruct, two\ninstruction-fine-tuned language models that can support a wide range of use\ncases. To prompt the models, we provide a definition of the condition of interest.\nWe tested two methods of prompting: (1) asking the model directly whether the\npatient has the condition based on the report, and (2) requesting that the model\ngenerates a summary of the report based on the condition of interest and decide\nwhether the patient has the condition based on that summary. We formulated\nthe prompt to extract binary labels on a given condition as seen in Figure 2.\nTo generate these labels, we used the softmaxed logits of two unique tokens in\nthe tokenizer’s vocabulary: “yes” and “no”; see Figure 1. Using the token scores\nensures that the model will produce a binary answer to every question.\n2.2 Domain Adaptation By Summary Fine-Tuning\nRadiology reports often include summary sections, which give overviews of the\nfindings. These summaries typically include the most pertinent information, in-\n4 R. Y. Park et al.\nFig.2. Model prompting strategies:Thedirectquerymethod(left)asksthemodel\nto extract the label based on the report. The summary and query method (right) asks\nthe model to generate a summary focused on the condition, which it uses as additional\ninput to annotate the report. Words in bold indicate user inputs to be modified.\ncluding potential diagnoses and descriptions of disease progression [7, 16]. Thus,\nthe summary is especially relevant to consider when extracting clinical labels.\nTo ensure that the model would generate clinically relevant summaries, we\nfine-tuned the linear layers of Zephyr using LoRA [6] to perform next token\nprediction on spinal reportsfrom a local hospital system. To do this, we identified\nthe summary sections of 56,924 reports using regular expression matches for the\nfollowingwords(case-insensitive):conclusion,impression,findings,andsummary\n(of the 124,771 reports available in the dataset, only 56,924 reports had matches\nfor one of these words). While we fed the whole report to the model as context,\nthe next-token-loss was computed only using the summary section of the report.\n3 Supervised Learning Using Automated Labels\nFig.3. MRI classification network:SpineNetV2 is used to detect IVDs. Each IVD\nis encoded using ResNet18. For stenosis, we use a SVM to get a score per IVD. For\ncancer and spondylolisthesis, we aggregate IVD encodings and use NSK-SVM to get a\nscore per scan.\nTo demonstrate that the labels automatically generated by our pipeline can\nbe used to train a vision model, we used the generated labels to train a classifier\nAutomated Spinal MRI Labelling 5\nto detect cancer, stenosis and spondylolisthesis in the MR scans paired to the\nreports. We were not able to train classifiers for cauda equina compression and\nherniation due to insufficient numbers of positive cases in our dataset.\nSpineNetV2 [21] was used to automatically detect the vertebrae and extract\nthe intervertebral discs (IVDs) in the T2 sagittal scans. Each IVD is of dimension\nslice x height x width (9 × 112 × 224). We reshaped each slice to224 × 224\nto input them into a modified ResNet18 (pre-trained on ImageNet) without\nthe fully connected layer to get slice-level embeddings. These were averaged\nacross slices to get the volume-level representations. The embeddings were used\nas features to train support vector machines (SVM) with a linear kernel to\nperform binary classification for our conditions of interest. For stenosis, we could\ngenerate level-specific labels, so we detected stenosis at each IVD level. For\ncancer and spondylolisthesis, the labels are provided at the spine-level but image\nsamples are individual IVDs, so we used multiple-instance learning (MIL) using\ntheNormalisedSetKernelmethod(NSK-SVM)togettheaveragerepresentation\nacross IVDs in a spinal scan [4]. Section 4 provides more information on the\ngranularities of the report-generated labels. Figure 3 provides an illustration of\nthe full pipeline.\n4 Datasets\nWe use three datasets:CancerData, LumbarData, and GeneralReports.\nThe first two contain report-image pairs while the last contains reports only.\nLabel Extraction:A subset ofCancerData and LumbarDatawas man-\nually labelled for prompt engineering experiments and testing our labelling\npipeline. CancerData were provided by National Consortium of Intelligent\nMedical Imaging (NCIMI) and include reports and the associated MRIs of con-\nfirmed or potential cancer cases from six different NHS Trusts across the UK.\nLumbarDatawere collected as part of the Oxford Secondary Care Lumbar MRI\nCohorts (OSCLMRIC) study and includes clinical MRI studies and reports of\npatients with lower back pain, sourced from a local hospital system. Samples of\nLumbarDatawere used to label positive and control cases for stenosis, spondy-\nlolisthesis, cauda equina and herniation. Each manually annotated dataset were\na randomly chosen subset of the full source data, which we split into a calibra-\ntion set (to develop our prompting strategy and determine a model calibration\nthreshold) and a test set, using stratified sampling. Manual labels for Can-\ncer were provided by a radiology registrar. Manual labels forStenosis were\nprovided by an orthopaedic surgeon.Spondylolisthesis, Cauda Equinacom-\npression andHerniation were labelled by the author.GeneralReports were\nalso provided by OSCLMRIC and contain 56,924 unpaired spinal MRI reports\nwith diverse conditions and control examples; it was used to fine-tune the model\non the causal language modeling task to generate the summary. See Table 1 for\ndata splits for each task.\nThe reports were preprocessed based on use case. Spinal reports are often\nsplit into sections (see Figure 5 in Appendix for an example). Since spinal can-\n6 R. Y. Park et al.\ncers are usually metastases from a primary site elsewhere, clinical histories in\nCancerData frequently mention the presence of cancer in a non-imaged site\nand includes query words (i.e. “?cancer” to check for spinal cancer in the image).\nTo correctly identify spinal cancer (i.e. the condition that would be visible in the\npairedimage),weexcludedclinicalhistorywheninputtingreportsfrom Cancer-\nDatainto the pipeline. As stenosis, spondylolisthesis, cauda equina compression\nand herniation are specific to the spine, we did not perform any additional pre-\nprocessing on LumbarData. For the summary fine-tuning task, we identified\nthe summary sections of the reports such that the rest of the report could be\nmasked for loss computation (see Section 2.2).\nDataset Split Patients Studies Positive Cases\nCancer Calibration (Labels) 140 140 67 (47.9%)\nTest (Labels) 145 145 67 (46.2%)\nStenosis Calibration (Labels) 68 68 41 (60.3%)\nTest (Labels) 68 68 45 (66.2%)\nSpondylolisthesis Calibration (Labels) 76 76 33 (43.4%)\nTest (Labels) 77 77 38 (49.4%)\nCauda Equina Calibration (Labels) 94 94 18 (19.1%)\nTest (Labels) 94 94 17 (18.1%)\nHerniation Calibration (Labels) 70 70 28 (40.0%)\nTest (Labels) 70 70 28 (40.0%)\nGeneralReports Fine-tuning 44,952 56,924 -\nTable 1. Labelling datasetsused to prompt engineer and fine-tune the model for\nthe labelling pipeline.Cancersplits are subsets ofCancerData. Stenosis, Spondy-\nlolisthesis, Cauda Equinaand Herniation are subsets ofLumbarData.\nMRI Classification:For the MRI classification tasks, we used T2 sagittal\nimages in a valid report-image pair inCancerData and LumbarData. Lum-\nbarData had only one sequence per study.CancerData could have multiple;\nin these cases, we chose the latest whole spine sequence in the study.\nFor stenosis, we generated labels for each IVD using our pipeline. This was\nfeasible for stenosis as (1) reports almost always contained information about the\nlevel at which stenosis is present, and (2) report-level expert annotations were\nlevel-specific (for lumbar IVDs T12-S1). We only used the last three lumbar\nspine IVDs (L3-L4, L4-L5 and L5-S1) as stenosis examples above L3 are rare.\nIt was less common for the cancer reports to list specific levels at which cancer\nis present, especially if present at multiple levels, which is common for both\nmetastases and myeloma. As a result, we follow the approach in [20] and employ\nmultiple instance learning, treating the entire spine as a bag which is labelled as\npositive if any of the vertebrae show cancer, or negative otherwise. We included\nIVDs from cervical to lumbar (18 IVDs C7-S1). Table 2 summarises the splits.\nAutomated Spinal MRI Labelling 7\nIVDs\nCondition Split Patients Studies Total Positives Negatives\nCancer\nTrain (Scans) 1,223 1,256 14,167 9,012 5,155\nValidation (Scans) 324 327 3,612 2,149 1,463\nTest (Scans) 450 451 4,918 3,120 1,798\nStenosis\nTrain (Scans) 1,375 1,946 5,827 2,977 2,850\nValidation (Scans) 153 217 649 337 312\nTest (Scans) 117 123 368 139 229\nSpondylo-\nlisthesis\nTrain (Scans) 1,357 1,939 5,758 725 5,033\nValidation (Scans) 152 200 645 78 567\nTest (Scans) 146 151 453 213 240\nTable 2. Summary of splits for MRI classification. The cancer splits are subsets\nof CancerData. The stenosis and spondylolisthesis splits are subsets ofLumbar-\nData. Positive/negative labels on the training and validation sets were assigned using\nour labelling method described in section 5.1 whereas test set labels were manually an-\nnotated as described in Section 4. Test (Scans) in each dataset consists of studies from\nboth Calibration and Test sets in Table 1 where the IVDs were successfully extracted.\n5 Results\nFig.4. Real example report fromLumbarDatawith the summary and scores gener-\nated forStenosis using our pipeline. Any dates, names or location information were\nremoved from the report.\n5.1 Assessment of Labelling Accuracy\nWe evaluated the model’s performance on label extraction by measuring area\nunder the receiver-operator curve (AUROC) and equal error rate (EER) using\nthe normalised scores from the model. To compute balanced accuracy and F1\nscore, we used the EER threshold from the calibration set.\n8 R. Y. Park et al.\nCondition\nPrompt\nMethod\nSummary\nModel\nQuery\nModel\nBal.\nAcc. EER\nAU\nROC F1\nCancer\nDirect Query - GPT-4 1.000 - - 1.000\nDirect Query - Zephyr 0.993 0.000 1.000 0.992\nDirect Query - Llama3 0.978 0.014 0.999 0.977\nSummary-Query Zephyr Zephyr 0.985 0.000 1.000 0.985\nSummary-Query Z-SFT Zephyr 0.994 0.000 1.000 0.993\nSummary-Query Llama3 Llama3 1.000 0.000 1.000 1.000\nStenosis\nDirect Query - GPT-4 0.951 - - 0.949\nDirect Query - Zephyr 0.945 0.037 0.981 0.950\nDirect Query - Llama3 0.963 0.000 0.987 0.962\nSummary-Query Zephyr Zephyr 0.945 0.111 0.985 0.950\nSummary-Query Z-SFT Zephyr 0.933 0.037 0.943 0.937\nSummary-Query Llama3 Llama3 0.945 0.037 0.995 0.950\nSpondylo-\nlisthesis\nDirect Query - GPT-4 0.974 - - 0.973\nDirect Query - Zephyr 0.974 0.026 0.996 0.974\nDirect Query - Llama3 0.974 0.026 0.993 0.974\nSummary-Query Zephyr Zephyr 0.942 0.023 0.985 0.938\nSummary-Query Z-SFT Zephyr 0.948 0.026 0.984 0.946\nSummary-Query Llama3 Llama3 0.974 0.026 0.994 0.974\nCauda Equina\nCompression\nDirect Query - GPT-4 1.000 - - 1.000\nDirect Query - Zephyr 0.912 0.013 0.972 0.903\nDirect Query - Llama3 0.941 0.026 0.998 0.938\nSummary-Query Zephyr Zephyr 0.971 0.039 0.998 0.970\nSummary-Query Z-SFT Zephyr 0.882 0.013 0.988 0.867\nSummary-Query Llama3 Llama3 1.000 0.000 1.000 1.000\nHerniation\nDirect Query - GPT-4 0.935 - - 0.915\nDirect Query - Zephyr 0.869 0.119 0.947 0.842\nDirect Query - Llama3 0.911 0.071 0.980 0.893\nSummary-Query Zephyr Zephyr 0.774 0.310 0.815 0.738\nSummary-Query Z-SFT Zephyr 0.768 0.214 0.871 0.730\nSummary-Query Llama3 Llama3 0.946 0.071 0.990 0.931\nTable 3. Scan-level report labelling performanceon cancer, stenosis, spondy-\nlolisthesis, cauda equina compression and herniation test sets (see Table 1). Normalised\nscores are used to compute EER and AUROC. Calibrated binary labels are used to\ncompute balanced accuracy and F1. Zephyr refers to the base model without fine-\ntuning, whereas Z-SFT indicates the summary fine-tuned model (see §2.2). The top\ntwo best values per metric are bolded.\nTable 3 shows the results of labelling each condition on the respective test\nsets. Across all tasks, our best methods exceed 0.94 balanced accuracy and match\nor outperform a direct query using GPT-4, prompted using the same format\nand input as the direct query strategy in Figure 2 but with the tags (e.g. user,\nsystem, assistant) formatted for GPT-4. For cancer, the summary-query method\nusing Llama3 achieves perfect balanced accuracy, AUROC and F1. For stenosis,\nthe summary-query method using Llama3 achieves the highest AUROC; the\ndirectquerymethodusingLlama3achieveshigherbalancedaccuracyandF1.For\nAutomated Spinal MRI Labelling 9\ncauda equina, both direct query and summary-query using Llama3 achieve best\nperformance, matching GPT-4. Herniation is the hardest task for GPT-4 and\nZephyr; Llama3 summary-query achieves the best balanced accuracy, AUROC\nand F1.\nIVD Level Bal. Acc. EER AUROC F1\nL3-L4 0.896 0.073 0.968 0.815\nL4-L5 0.908 0.108 0.978 0.900\nL5-S1 0.855 0.150 0.945 0.830\nTable 4. IVD-level report labelling performanceon expert-annotated subset of\nLumbarDatafor Stenosis (n=68 patients, 204 IVDs). Normalised scores are used to\ncompute EER and AUROC. Calibrated binary labels are used to calculate balanced\naccuracy and F1.\nSince it achieved the highest or second highest AUROC across all tasks,\nthe summary-query prompting method with Llama3 for summary generation\nand Q-A was used as the general method to extract binary labels for the MRI\nclassification tasks. Figure 4 shows an example of the summary and token scores\ngenerated from our pipeline. For inference across the full datasets, we extracted\nscan-level labels for cancer and IVD-level labels for stenosis, as described in\nsection 4. Table 4 shows the performance of our labelling pipeline for stenosis at\neach IVD.\n5.2 Assessment of MRI Classification Accuracy\nCondition Model Bal. Acc.EERAUROC F1\n(1) Cancer ResNet18+NSK-SVM 0.763 0.215 0.851 0.773\n(2) Stenosis SpineNetV2 [21] 0.679 - - 0.574\n(3) Stenosis SpineNetV2 [21]+SVM 0.780 0.218 0.857 0.727\n(4) Stenosis ResNet18+SVM 0.775 0.227 0.836 0.722\n(5)Spondylolisthesis SpineNetV2 [21] 0.861 - - 0.853\n(6)SpondylolisthesisSpineNetV2 [21]+NSK-SVM 0.858 0.138 0.910 0.855\n(7)Spondylolisthesis ResNet18+NSK-SVM 0.780 0.250 0.855 0.785\nTable 5. MRI Classifier Performanceon CancerData and LumbarData test\nsets. Calibrated binary labels are used to calculate balanced accuracy and F1. Only\nrows (1), (4) and (7) report results of models fully trained using our labels. (2) and (5)\nperform inference using SpineNetV2, which is trained using human annotations, and\n(3) and (6) use SpineNetV2 to extract encodings and train an SVM using our report-\ngenerated labels. We treat (2)-(3) as baselines for our stenosis classification method (4)\nand (5)-(6) as baselines for our spondylolisthesis classification method (7).\n10 R. Y. Park et al.\nWe evaluated the model’s performance on image classification by measuring\narea under the receiver-operator curve (AUROC) and equal error rate (EER)\nusing the normalised scores from the model. We used the EER threshold from\nthe validation set to derive binary labels from the scores on the test set, which\nwere used to compute balanced accuracy and F1 score.\nStenosis: SpineNetV2 [21] has an existing grading model trained to classify\nthree kinds of stenosis (central canal, foraminal right and left). We compare our\nmethod (ResNet18+SVM) to (1) an aggregated stenosis label from SpineNetV2\nand (2) SVM using SpineNetV2 encodings (SpineNetV2+SVM). Our stenosis\nclassifier outperforms aggregated SpineNetV2 stenosis scores and achieves a sim-\nilar AUROC, balanced accuracy and F1 score after calibration for either choice\nof encoding with the SVM trained on automated labels (see Table 5).\nSpondylolisthesis: Similarly to stenosis, SpineNetV2 [21] has an existing\ngrading model trained to generate a binary label for spondylolisthesis. We com-\npare our method (ResNet18+NSK-SVM) to (1) the label from SpineNetV2 and\n(2) NSK-SVM using SpineNetV2 encodings (SpineNetV2+NSK-SVM). While\nthe SpineNetV2 labels and NSK-SVM trained on SpineNetV2 encodings exceed\nour model’s performance, our model achieves good performance (AUROC: 0.855)\ncomparable to the cancer and stenosis classifiers.\nCancer: Considering all IVD-level embeddings in aggregate, our classifier\nachieves a balanced accuracy of 0.763 in classifying cancer at the scan-level.\nSpineNetV2 does not classify cancer, so we are unable to provide a comparable\nbaseline on the same dataset. A deep learning model to detect cancer in CT\nimages achieved an F1 score of 0.72 [13], which we surpass (0.773).\n5.3 Discussion and Limitations\nThere are several training and prompting methods we tested but did not include\nin our final methodology. While adding few-shot examples of desired Q-A pairs\nwhen prompting LLMs is a common method to improve performance [3, 14], we\nfound results to be highly sensitive based on the reports selected as examples.\nFurthermore, adding longer example medical reports often exceeds the models’\ncontext window (512 tokens for Zephyr, 8,000 tokens for Llama3). Interestingly,\nwe found that fine-tuning Zephyr on report summaries was only beneficial for\nthe cancer labelling task (Table 3). We currently force the pipeline to output a\npositive or negative label without uncertainty. In future work, we plan to work\ndirectly with probabilities from our locally-run LLM to predict uncertainty.\n5.4 Recommendations\nOur method can be adapted for use with other open-source LLMs, clinical con-\nditions and reports accompanying other image modalities (e.g. chest X-rays).\nFor smaller LLMs and clinical conditions that the LLM may not have en-\ncountered in its training corpus, we recommend domain adaptation by summary\nfine-tuning to familiarise the model with the vocabulary and syntax of radiology\nAutomated Spinal MRI Labelling 11\nreports and generate clinically relevant summaries for the target condition (see\nSection 2.2).\nStronger LLMs may not need additional fine-tuning and achieve good results\neven with the direct query method. For well-defined (“easier”) clinical conditions,\nthe default 0.5 threshold may be sufficient to achieve a good classification. For\nconditions that are rarely found in the dataset (<20% of the source data), we\nrecommend using a manually labelled subset of the data to compute the equal\nerror rate and adjust the acceptance threshold to account for the data imbalance.\nAdditionally, for conditions that can be conflated with related conditions\n(e.g. herniation and protrusion), it may be necessary to define the negative case\nas part of the condition’s definition (i.e. “Herniation is a more severe condition\nthan disc protrusion or bulging.”).\n6 Conclusion\nWe propose a general method that can be adapted to extract labels from radio-\nlogical reports without additional model training. We show that this surpasses a\nstrong GPT-4 baseline when applied to spinal MR reports, with the additional\nadvantages that: (1) we use a locally-run open-source model that is privacy-\npreserving and cheap, and (2) we have direct access to the raw token-level scores,\nwhich can be used for model calibration to produce more accurate binary labels.\nWe also demonstrate that the extracted labels can be used to train a classifiers\nwith similar performance to models trained with expert-annotated scans.\nAcknowledgments. We thank our clinical collaborators for providing annotations\nand advice: Prof. Jeremy Fairbank, Dr. Sarim Ather, Prof. Iain McCall, Dr. Mark\nKong (in no particular order). We are also grateful to our funders: EPSRC CDT in\nHealth Data Science (EP/S02428X/1), Cancer Research UK via the EPSRC CDT in\nAutonomous Intelligent Machines and Systems (EP/S024050/1), EPSRC programme\ngrant Visual AI (EP/T025872/1), and the Oxford Big Data Institute.LumbarData\n(OSCLMRIC) was collected as part of a Health Research Authority study (IRAS\nProject ID 207858), whileCancerData was collected by NCIMI (Project ID 010A).\nDisclosure of Interests. The authors have no competing interests to declare that\nare relevant to the content of this article.\nBibliography\n[1] Bannur, S., Hyland, S., Liu, Q., Pérez-García, F., Ilse, M., Castro, D.C.,\nBoecking, B., Sharma, H., Bouzid, K., Thieme, A., Schwaighofer, A.,\nWetscherek, M., Lungren, M.P., Nori, A., Alvarez-Valle, J., Oktay, O.:\nLearning to exploit temporal structure for biomedical vision–language pro-\ncessing. In: CVPR (2023)\n[2] Boecking, B., Usuyama, N., Bannur, S., Castro, D.C., Schwaighofer, A.,\nHyland, S., Wetscherek, M., Naumann, T., Nori, A., Alvarez-Valle, J., Poon,\nH., Oktay, O.: Making the most of text semantics to improve biomedical\nvision–language processing. In: ECCV (2022)\n[3] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P.,\nNeelakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: Language models\nare few-shot learners. In: NeurIPS (2020)\n[4] Gärtner, T., Flach, P.A., Kowalczyk, A., Smola, A.J.: Multi-instance ker-\nnels. In: ICML (2002)\n[5] Gu,Y.,Tinn,R.,Cheng,H.,Lucas,M.,Usuyama,N.,Liu,X.,Naumann,T.,\nGao, J., Poon, H.: Domain-specific language model pretraining for biomed-\nical natural language processing. ACM Transactions on Computing for\nHealthcare (HEALTH)3(1), 1–23 (2021)\n[6] Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L.,\nChen, W.: LoRA: Low-rank adaptation of large language models. In: ICLR\n(2022)\n[7] Irvin, J., Rajpurkar, P., Ko, M., Yu, Y., Ciurea-Ilcus, S., Chute, C., Mark-\nlund, H., Haghgoo, B., Ball, R., Shpanskaya, K., et al.: Chexpert: A large\nchest radiograph dataset with uncertainty labels and expert comparison. In:\nProceedings of the AAAI conference on artificial intelligence. vol. 33, pp.\n590–597 (2019)\n[8] Jiang, A.Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D.S.,\nCasas, D.d.l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al.:\nMistral 7B. arXiv preprint arXiv:2310.06825 (2023)\n[9] Lee, J., Yoon, W., Kim, S., Kim, D., Kim, S., So, C.H., Kang, J.: BioBERT:\na pre-trained biomedical language representation model for biomedical text\nmining. Bioinformatics36(4), 1234–1240 (2020)\n[10] Li, C., Wong, C., Zhang, S., Usuyama, N., Liu, H., Yang, J., Naumann, T.,\nPoon, H., Gao, J.: Llava-med: Training a large language-and-vision assistant\nfor biomedicine in one day (2023)\n[11] Lu, J.T., Pedemonte, S., Bizzo, B., Doyle, S., Andriole, K.P., Michalski,\nM.H., Gonzalez, R.G., Pomerantz, S.R.: Deep spine: Automated lumbar\nvertebral segmentation, disc-level designation, and spinal stenosis grading\nusing deep learning. In: Machine Learning for Healthcare Conference. pp.\n403–419. PMLR (2018)\nAutomated Spinal MRI Labelling 13\n[12] Luo, R., Sun, L., Xia, Y., Qin, T., Zhang, S., Poon, H., Liu, T.Y.: BioGPT:\ngenerative pre-trained transformer for biomedical text generation and min-\ning. Briefings in Bioinformatics23(6) (2022)\n[13] Motohashi, M., Funauchi, Y., Adachi, T., Fujioka, T., Otaka, N., Kamiko,\nY., Okada, T., Tateishi, U., Okawa, A., Yoshii, T., et al.: A new deep learn-\ning algorithm for detecting spinal metastases on computed tomography im-\nages. Spine49(6), 390–397 (2024)\n[14] Nori, H., Lee, Y.T., Zhang, S., Carignan, D., Edgar, R., Fusi, N., King, N.,\nLarson, J., Li, Y., Liu, W., et al.: Can generalist foundation models outcom-\npete special-purpose tuning? Case study in medicine. Medicine84(88.3),\n77–3 (2023)\n[15] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P.,\nZhang, C., Agarwal, S., Slama, K., Ray, A., et al.: Training language models\nto follow instructions with human feedback. In: NeurIPS (2022)\n[16] Peng, Y., Wang, X., Lu, L., Bagheri, M., Summers, R., Lu, Z.: NegBio: a\nhigh-performance tool for negation and uncertainty detection in radiology\nreports. AMIA Summits on Translational Science Proceedings2018, 188\n(2018)\n[17] Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang,\nP., Hashimoto, T.B.: Stanford Alpaca: An instruction-following LLaMA\nmodel. Tech. rep. (2023)\n[18] Thirunavukarasu, A.J., Ting, D.S.J., Elangovan, K., Gutierrez, L., Tan,\nT.F., Ting, D.S.W.: Large language models in medicine. Nature medicine\n29(8), 1930–1940 (2023)\n[19] Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y.,\nBashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al.: Llama 2: Open\nfoundation and fine-tuned chat models. Tech. rep. (2023)\n[20] Windsor, R., Jamaludin, A., Kadir, T., Zisserman, A.: Context-aware trans-\nformers for spinal cancer detection and radiological grading. In: Interna-\ntional Conference on Medical Image Computing and Computer Assisted\nIntervention (2022)\n[21] Windsor,R.,Jamaludin,A.,Kadir,T.,Zisserman,A.:Automateddetection,\nlabelling and radiological grading of clinical spinal MRIs. Scientific Reports\n14(1), 14993 (2024)\n[22] Yan, A., McAuley, J., Lu, X., Du, J., Chang, E.Y., Gentili, A., Hsu, C.N.:\nRadBERT: Adapting transformer-based language models to radiology. Ra-\ndiology: Artificial Intelligence4(4), e210258 (2022)\n14 R. Y. Park et al.\nA Appendix\nCondition Definition\nCancer Spinal cancer includes malignant lesions that originate\nfrom the spine or spinal cord and metastatic or sec-\nondary tumours that have spread from another site to\nthe spine.\nStenosis Stenosis is any narrowing or compression of the spinal\ncanal or nerves, including disc protrusions, impinge-\nment of nerve roots, or compromise of recesses.\nSpondylolisthesis Spondylolisthesis is a condition in which a vertebra\nslips out of place onto the bone below it.\nCompression\nCauda Equina Cauda equina compression is the compression of a col-\nlection of nerve roots called the cauda equina, distinct\nfrom cauda equina syndrome; if the patient has cauda\nequina compression, the report will explicitly state its\npresence.\nHerniation Herniation is a condition in which a disc in the spine\nruptures, and the disc nucleus is displaced from inter-\nvertebral space; it is more severe condition than disc\nprotrusion or bulging, and if the patient has hernia-\ntion, the report will explicitly state its presence.\nTable 6. Definitions used to prompt the label extraction pipeline.\nFig.5. An example report fromCancerData with section headers shown in bold.\nAutomated Spinal MRI Labelling 15\nFig.6. Example report and scan fromLumbarDatashowing stenosis. Summary and\nscores were generated using our pipeline.\nFig.7. Example report and scans fromCancerData showing cancer. Summary and\nscores were generated using our pipeline.\nFig.8. Examplereportandscansfrom LumbarDatashowingspondylolisthesis.Sum-\nmary and scores were generated using our pipeline.",
  "topic": "Labelling",
  "concepts": [
    {
      "name": "Labelling",
      "score": 0.8345929980278015
    },
    {
      "name": "Computer science",
      "score": 0.8218919634819031
    },
    {
      "name": "Natural language processing",
      "score": 0.5171734690666199
    },
    {
      "name": "Artificial intelligence",
      "score": 0.49281933903694153
    },
    {
      "name": "Criminology",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 1
}