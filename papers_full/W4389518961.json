{
  "title": "MAPO: Boosting Large Language Model Performance with Model-Adaptive Prompt Optimization",
  "url": "https://openalex.org/W4389518961",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2120890993",
      "name": "Yuyan Chen",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2133077142",
      "name": "Zhi-Hao Wen",
      "affiliations": [
        "Singapore Management University"
      ]
    },
    {
      "id": "https://openalex.org/A2111132711",
      "name": "Ge Fan",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2096748314",
      "name": "Zhengyu Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1993208100",
      "name": "Wei Wu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2564343893",
      "name": "Dayiheng Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2143202255",
      "name": "Zhixu Li",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2130952716",
      "name": "Bang Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2131222654",
      "name": "Yanghua Xiao",
      "affiliations": [
        "Fudan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3166986030",
    "https://openalex.org/W131533222",
    "https://openalex.org/W4321649710",
    "https://openalex.org/W4302305863",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W4365211596",
    "https://openalex.org/W4308244910",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W4386566498",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2952186591",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W4224275713",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W3196731672",
    "https://openalex.org/W2160660844",
    "https://openalex.org/W4303648559",
    "https://openalex.org/W3174784402",
    "https://openalex.org/W3166846774",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4385573003",
    "https://openalex.org/W4389520756",
    "https://openalex.org/W4366548330",
    "https://openalex.org/W4225165463",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W4321650181",
    "https://openalex.org/W4312091890",
    "https://openalex.org/W2891308403",
    "https://openalex.org/W4253067820",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W4386566526",
    "https://openalex.org/W4360890239",
    "https://openalex.org/W4375869990",
    "https://openalex.org/W4312053697"
  ],
  "abstract": "Prompt engineering, as an efficient and effective way to leverage Large Language Models (LLM), has drawn a lot of attention from the research community. The existing research primarily emphasizes the importance of adapting prompts to specific tasks, rather than specific LLMs. However, a good prompt is not solely defined by its wording, but also binds to the nature of the LLM in question. In this work, we first quantitatively demonstrate that different prompts should be adapted to different LLMs to enhance their capabilities across various downstream tasks in NLP. Then we novelly propose a model-adaptive prompt optimizer (MAPO) method that optimizes the original prompts for each specific LLM in downstream tasks. Extensive experiments indicate that the proposed method can effectively refine prompts for an LLM, leading to significant improvements over various downstream tasks.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 3279–3304\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nMAPO: Boosting Large Language Model Performance with\nModel-Adaptive Prompt Optimization\nYuyan Chen1, Zhihao Wen3, Ge Fan4, Zhengyu Chen, Wei Wu5 \u0000\nDayiheng Liu, Zhixu Li1, Bang Liu, Yanghua Xiao1,2 \u0000\n1Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University,\n2Fudan-Aishu Cognitive Intelligence Joint Research Center,\n3Singapore Management University, 4Tencent, 5Ant Group\n{chenyuyan21@m., zhixuli@, shawyh@}fudan.edu.cn, zhwen.2019@phdcs.smu.edu.sg,\nge.fan@outlook.com, chenzhengyu@zju.edu.cn, congyue.ww@antgroup.com,\nliudayiheng.ldyh@alibaba-inc.com, bang.liu@umontreal.ca\nAbstract\nPrompt engineering, as an efficient and effec-\ntive way to leverage Large Language Models\n(LLM), has drawn a lot of attention from the\nresearch community. The existing research pri-\nmarily emphasizes the importance of adapting\nprompts to specific tasks, rather than specific\nLLMs. However, a good prompt is not solely\ndefined by its wording, but also binds to the na-\nture of the LLM in question. In this work, we\nfirst quantitatively demonstrate that different\nprompts should be adapted to different LLMs to\nenhance their capabilities across various down-\nstream tasks in NLP. Then we novelly propose\na model-adaptive prompt optimizer (MAPO)\nmethod that optimizes the original prompts for\neach specific LLM in downstream tasks. Ex-\ntensive experiments indicate that the proposed\nmethod can effectively refine prompts for an\nLLM, leading to significant improvements over\nvarious downstream tasks.\n1 Introduction\nAdvancements in Large Language Models (LLMs)\nhave ushered in a transformative era in natural lan-\nguage processing, showcasing their remarkable\ncapabilities across a wide range of tasks (Ope-\nnAI, 2023; Bubeck et al., 2023). While these\nmodels possess human-like comprehension and re-\nsponse abilities, their performance is heavily in-\nfluenced by the quality of prompts. As can be\nobserved in Fig. 1, answers from different LLMs\nvary widely when they are provided with the same\ntask-specific prompts. Therefore, it is necessary to\ngenerate prompts that are most suitable for each\nLLM, thereby enhancing its performance on down-\nstream tasks.\nA common practice towards prompt optimiza-\ntion is to count on human expertise (White et al.,\n2023; Jiang et al., 2022; Zamfirescu-Pereira et al.,\nQA\nGeneration\nTask Model Inference\nGPT-J : The target is the person who is going to be doing the \nwork.\nGPT-J: bees \\n What are the topics in the sentence: bees collect \nnectar from the flowers\nLLaMA: Closed loop</s>\nBLOOM: bee, collect, flower</s>\nBLOOM: 100% of the time, the loop is open.\nLLaMA: bees collect nectar from the flowers \\n What are the \ntopics in the sentence: the bees\nGPT-J: Yes. \\n The movie is a remake of the 1979 movie of the \nsame name. The original movie was a\nBLOOM: No</s>\nLLaMA: Yes. \\n The movie is about a man who is a widower and \nhas a son. He is a very good father\nClassification\nPrompt: What other type of loop is \nthere other than an open loop?\nGround Truth: closed loop\nPrompt: What are the topics in the \nsentence: bees collect nectar from the \nflowers\nGround Truth: flower, bee, nectar\nPrompt: as it turns out , you can go \nhome again . The sentiment \nexpressed for the movie is \nGround Truth: Positive\n(a) (b)\nFigure 1: Variance on answers from different LLMs (b) when\nthey are given the same task-specific prompts (a).\n2023). While effective, such approaches are costly\nand unscalable. Hence, there has been a lot of\neffort to streamline the prompt optimization pro-\ncess through automated or semi-automated ways,\nincluding prompt retrieval (Ma et al., 2023; Zhou\net al., 2022), prompt generation from scratch (Pang\net al., 2023), and prompt editing (Gao et al., 2020;\nPryzant et al., 2023; Deng et al., 2022). For ex-\nample, in prompt retrieval, Ma et al. (2023) pro-\npose a search strategy based on greedy search\nto identify near-optimal prompts for in-context\nlearning; in prompt generating from scratch, Pang\net al. (2023) introduce SharpT, which learns a\nshared latent space and generates soft prompts us-\ning a lightweight prompt generator; in prompt edit-\ning, some approaches rely on reinforcement learn-\ning or LLM-based feedback for prompt optimiza-\ntion (Deng et al., 2022; Zhou et al., 2022).\nHowever, the aforementioned research primarily\nemphasizes the importance of adapting prompts\nto specific tasks, rather than specific LLMs. The\nlatter, although very important, has not been stud-\nied to date in NLP. The only relevant work so\nfar has been done on multi-modal large models,\nwhich automatically optimizing prompts using re-\ninforcement learning to generate images based\n3279\nGeneration\n0.00\n0.25\n0.50\n0.75\n1.00Metric\nClassification\n0.0\n0.5\n1.0\nQA\n0.0\n0.5\n1.0\nBLOOM\nLLaMA\nGPT-J\nFigure 2: The performance of different LLMs on task-specific prompts for three tasks: question-answering (a), classification\n(b), and generation (c). The results reveal significant variations across different LLMs’ performance.\non text (Hao et al., 2022). They underscore the\nconcept of “model-preferred prompts” or “model-\nspecific prompts”, emphasizing that there’s a need\nfor a systematic method to automatically align user\nintentions with the specific prompt preferences of\neach model. Therefore, in this paper, we novelly\npropose a Model-Adaptive Prompt Optimization\n(i.e. MAPO) approach for LLMs in NLP. Given\nthe lack of effective training signals, we first estab-\nlish a so-called warm-up dataset to obtain candi-\ndate prompts from an oracle LLM, and then model\nthe prompt optimization problem with reinforce-\nment learning. Specifically, we first generate can-\ndidate prompts and search for the optimal prompts\nto establish a warm-up dataset. After that, we\ncombine Supervised Fine-Tuning (SFT) and Re-\ninforcement Learning (RL) to optimize original\nprompts for each specific LLM in various down-\nstream tasks. Moreover, we make joint learning\nwith Proximal Policy Optimization (PPO) (Schul-\nman et al., 2017) and RRMF (note that RRMF is\ninspired by RRHF (Yuan et al., 2023)), to further\nimprove the performance of RL. We conduct ex-\ntensive experiments which validates the robustness\nand generalization of the proposed MAPO. To sum\nup, our main research question revolves around\nidentifying the optimal prompt that is suited for\nvarious models. Our contributions are threefold:\n• We are the first to quantitatively show that\ndifferent prompts should be adapted to differ-\nent Large Language Models (LLMs) in order\nto enhance their performance across various\nNLP downstream tasks.\n• We introduce a novel approach called the\nModel-Adaptive Prompt Optimizer (MAPO),\nspecifically designed to optimize the original\nprompts for each particular LLM in down-\nstream tasks.\n• The experiments show that our proposed\nMAPO model exhibits greater robustness and\ngeneralization and also achieves superior per-\nformance in a variety of downstream tasks.\n2 Empirical study\nIn this section, we conduct empirical study on three\nLLMs (BLOOM-7B (Scao et al., 2022), GPT-J-\n6B (Wang and Komatsuzaki, 2021), and LLaMA-\n7B (Scao et al., 2022)) to evaluate their separate\nperformance on question-answering (QA), classifi-\ncation, and generation tasks with same task-specific\nprompts. We use nine dataset from P3 (Sanh et al.,\n2021) covering three downstream tasks with de-\ntails in Appendix E. P3 is a widely-used prompt\nbenchmark which contains original prompts and\nthe corresponding ground-truth answers. We adopt\nF1 score, accuracy and ROUGE-L for QA, clas-\nsification, and generation tasks, respectively. The\nvisualization results are shown in the Fig. 2. From\nthe violin plot, we observe significant variations\nin distribution among different LLMs in each task.\nFor example, in the generation task, the results of\nall three models are distributed within the range of\n0 to 0.5, but there are still differences in the spe-\ncific distribution patterns. Moreover, the medians,\nmeans, and other statistical measures also differ\ngreatly among three LLMs in each downstream\ntask. Therefore, we consider that finding the op-\ntimal prompt for each specific LLM on each task\nis meaningful, as it can help enhance the LLMs’\nperformance on various downstream tasks.\n3 Methods\nBased on the above empirical study, we present\nMAPO, a model-adaptive prompt optimization ap-\nproach for LLMs. It takes the original prompt as in-\nput and generate an optimized prompt which makes\nan LLM give better outputs. The framework of\nMAPO is shown in Fig. 3.\n3.1 Warm-up Dataset Establishment\nWe first establish a warm-up dataset as training\ndataset for prompt optimization.\nGenerating Candidate Prompts.The original\nprompts are from nine above-mentioned datasets\nin P3 (Sanh et al., 2021). We generate 1,000 can-\ndidate prompts per prompt using GPT-3.5 1. The\n1https://chat.openai.com/\n3280\nOriginal Prompt\nCandidate prompt 1\nCandidate prompt 2\nCandidate prompt k\n…\nPrompt Rank Sequence\nBetter Prompt SFT Model\nReward Model\nRL Model\nLr\nGeneral NLP Tasks\nJoint Learning\nLv\nLft\nLΦSFT\nSFT\nOptimized Prompt\nPolicy \nOptimization\nWarm-up Dataset Establishment Prompt Optimizer Construction\nActor Model\nGPT-3.5\n(Frozen Model)\nSFT\nApproximating \nLΦ rθ (PPO)Lpg (PPO)\nPre-train\n(PPO)\n(RRMF)\n(PPO)\n（po)\n（p1, …, p, …, pk）\n（p）\nGeneralization \nMaintaining\nLΦpre(PPO)\nPre-train\nFigure 3: Framework of the proposed MAPO, including warm-up dataset establishment and prompt optimizer construction.\nTasks Dataset Train (Pairs) Val (Pairs) Test (Pairs)\nQA AdverQA 10000 1000 -OpenQA 4957 500 500CloseQA 11679 1000 1000\nClassNews 120000 - 7600Movie 8530 1066 1066QASC 8134 926 920\nGen Topics 67389 4018 1497Summary 14732 818 819Explan 9741 1221 -\nTable 1: The amount of the warm-up dataset on various\ndownstream tasks.\ngenerated candidate prompts should maintain se-\nmantic meaning similar to the original prompt but\nmay have different expressions. To achieve this, we\nuse the following instruction as input for GPT-3.5\nto generate candidates: “Please rewrite the given\ntext ‘original prompt’ while keeping the semantic\nmeaning unchanged.”. Some candidate prompts\nare shown in Appendix A.\nSearching for the Optimal Prompt.To deter-\nmine which candidate prompt is optimal for an orig-\ninal prompt, we compare the match degree, which\nrefers to the similarity, between the outputs gener-\nated using a candidate prompt and the ground truth\noutput. The purpose is to identify the candidate\nprompt that produces an output most similar to the\nground truth output. When a ground-truth output is\nnot available, the output of a stronger LLM, such\nas GPT-3.5, is regarded as the ground-truth. Specif-\nically, first, we input the original prompt P and\neach candidate prompt into an LLM, respectively,\nfor inference and obtain the corresponding outputs.\nNext, we compare the match degree with specified\nevaluation metrics. We adopt F1 score, accuracy,\nand ROUGE-L (Lin, 2004) for QA, classification,\nand generation tasks, respectively. Based on these\nmetrics, we iterate the searching process and find\nthe optimal prompt Po for an LLM in downstream\ntasks. The warm-up dataset consists of a collection\nof prompt pairs (referred to as {P,Po}), whose\ndistribution is shown in Table 1.\n3.2 Prompt Optimizer Construction\nThe prompt optimizer seeks to refine the initial\nprompt (P) into an optimized prompt (Po) tailored\nto a particular LLM. This refinement process en-\ntails altering the structure or wording of P to pro-\nduce Po, which is more suitable for the LLM in\nsubsequent tasks.\n3.2.1 Supervised Fine-tuning\nWe begin by employing the warm-up dataset to\nconduct supervised fine-tuning (SFT) with an LLM\nacross multiple downstream tasks. The objective of\nSFT is to enhance the LLM’s capacity to generate\nresponses that align with its preferences, utilizing\nannotated data. Prior research conducted by Ra-\nmamurthy et al. (2022) supports the notion that em-\nploying SFT prior to reinforcement learning (RL)\nleads to improved outcomes. Furthermore, to dif-\nferentiate between specific tasks during training,\nwe incorporate a brief instruction preceding the\ninput, such as “This is a... (generative/question-\nanswering/classification) task.”.\n3.2.2 Building Reward Model\nNext, we construct a reward model to learn the ef-\nfectiveness of prompts based on the preferences\nof different LLMs. This approach is motivated by\nthe fact that discriminative annotation through sort-\ning incurs significantly lower costs compared to\ngenerating annotations for answers. Initially, we\nobtain a ranking sequence for an LLM in a spe-\ncific downstream task. We sort the outputs gener-\nated by candidate prompts {P1,P2,...,P k−1,Pk}\nalongside the original prompt P, using the\nsame evaluation metric as described in Sec. 3.1.\nThis sorting process yields a ranking sequence\n{P1,P2,...,P,P k−1,Pk}. Prompts to the left of\nP exhibit poorer inference results, while prompts\nto the right demonstrate better results. Next, we em-\n3281\nploy the ranking sequence to train a reward model.\nWe utilize the same LLM utilized in the SFT pro-\ncess (referred to as ˆLLM) and replace the softmax\nlayer with a linear layer to construct the reward\nmodel. The reward model takes a prompt as input\nand produces a scalar score indicating the quality\nof the prompt. We form pairwise ranking pairs\nby combining prompts from the ranking sequence\nand employ Pairwise Ranking Loss for training, as\nillustrated below:\nLθ = − 1(k\n2\n)E(x,yw,yl)∼D\n[log(σ(rθ(x,yw) −rθ(x,yl))],\n(1)\nwhere x represents the original prompt, yw and\nyl denote the higher-scoring and lower-scoring\nprompts, respectively, in the corresponding ranking\npair. rθ represents the scalar output of the reward\nmodel, Dis the set of ranking pairs, andKdenotes\nthe number of candidate prompts. Through this pro-\ncess, based on the outputs generated by ˆLLM with\nthe given prompt, the reward model learns to assign\nhigher scores (rewards) to better prompts and lower\nscores (rewards) to inferior prompts, thus imitating\nan LLM’s preferences.\n3.2.3 Reinforcement Learning\nSubsequently, we employ Reinforcement Learning\n(RL) to further fine-tune LLMs. RL is used to\nadjust the bias in the reward model’s scoring since\nthe distribution of generated prompts might change\nduring the SFT process. The primary objective of\noptimization is to maximize the scores of prompts\ngenerated by ˆLLM after SFT (referred to as the\nSFT model), as evaluated by the reward model. To\nachieve this, we utilize a combination of Proximal\nPolicy Optimization (PPO) (Schulman et al., 2017)\nand RRMF algorithms (note that RRMF is inspred\nby RRHF (Yuan et al., 2023)) for joint learning.\nPolicy Optimization. This step aims to opti-\nmize the RL policy to improve the performance\nof the RL model. We first adopt the datasets\nshown in Table 3, which to construct environment-\naction pairs. The environment refers to the original\nprompt, and the action represents the prompt gener-\nated by ˆLLM without instruct-tuning. We pass the\nenvironment-action pairs to the reward model to ob-\ntain rewards. In this process, we introduce an actor\nmodel, which is ˆLLM, and a frozen model, which\nis SFT model with its parameters frozen during the\nRL training process. The frozen model serves as a\nbenchmark to evaluate whether the updated actor\nmodel has advantages over it. We then calculate\nthe policy gradient loss (i.e., actor’s loss) based on\nthe importance ratio and reward(r+γVnext −Vcur),\nand calculate the value loss (i.e., critic’s loss) by\ncomparing the predicted value Vpred with the true\nvalue (r+ Vnext) as follows:\nLpg = Pπa (t)\nPπf (t) (r+ γVnext −Vcur), (2)\nLv = ∥Vpred −(r+ Vnext)∥. (3)\nHere, Pπa (t)\nPπf (t) represents the ratio of probabilities\n(i.e., importance ratio) of generating the same token\nunder the actor model and the frozen model. (r+\nγVnext −Vcur) represents the reward of the current\nstep. Vpred denotes the predicted value, and (r+\nVnext) denotes the true value.\nNext, we maximize the mathematical expecta-\ntion of the reward model, aiming to consistently\ngenerate prompts that ˆLLM perceives as the best\nin the RL-trained SFT model (referred to as RL\nmodel). We feed prompts xgenerated by the SFT\nmodel based on the datasets shown in Table 3 (i.e.,\nD) into the RL model πRL\nϕ to obtain an optimized\nprompt y. y changes every time the RL model\nis updated. We then input (x,y) into the reward\nmodel rθ and calculate a score (i.e., reward), which\nrepresents the real-time feedback from the reward\nmodel. The loss function is defined as follows:\nLrθ\nϕ = E(x,y) ∼DπRL\nϕ\n[rθ(x,y)]. (4)\nFinally, we combine the above loss functions to\noptimize the RL policy from multiple perspectives.\nThe final loss function is defined as:\nLρ = α1Lpg + α2Lv + α3Lrθ\nϕ , (5)\nwhere α1, α2, and α3 represent the optimal weights\nof each function, which are determined through\nexperiments (the same applies below).\nSFT Approximating. This step aims to main-\ntain similarity between the RL model and the SFT\nmodel. When the RL model undergoes parame-\nter updates, it leads to variations in the generated\nprompt ybased on the given prompt x. If there is\na significant discrepancy between the RL model\nand the SFT model, it can result in inaccurate esti-\nmation of scores by the reward model. To address\nthis issue, we measure the distance between the\nprompts generated by the RL model and the SFT\nmodel using Kullback-Leibler (KL) divergence.\n3282\nThe objective is to minimize the KL divergence\nand the loss function is defined as follows:\nLSFT\nϕ = −βlog(πRL\nϕ (y|x)/πSFT (y|x)). (6)\nwhere πRL\nϕ (y|x) and πSFT (y|x) represent prompts\ngenerated by RL model and the SFT model, respec-\ntively.\nNext, we have borrowed the idea from\nRRHF (Yuan et al., 2023) but adapt it to focus\non “model feedback” instead of “human feedback”.\nWe name it Ranking Responses from Model Feed-\nback (RRMF). Specifically, we calculate the like-\nlihood probability of ˆLLM during SFT and align\nthis probability with the score of the reward model.\nTo optimize this objective, we employ supervised\nlearning with a rank loss, defined as follows:\npi =\n∑ log Pπ(yi|x,yi)\n∥yi∥ , (7)\nLr =\n∑\nri<rj\nmax(0,pi −pj), (8)\nwhere pi is the conditional log probability which\nrepresents the reward of each optimized prompt yi.\nri represents the reward model rθ(x,yi). We also\nincorporate the cross-entropy loss introduced by\nRRHF (Yuan et al., 2023) to learn the generated\nprompts y′\ni with the highest reward r′\ni as follows:\nLft = −ΣlogPπ(y′\ni|x,y′\ni). (9)\nFinally, we combine the above loss functions for\nSFT approximating as follows:\nLSFT = β1LSFT\nϕ + β2Lft + β3Lr. (10)\nGeneralization Maintaining. This step ad-\ndresses the issue of catastrophic forgetting by ensur-\ning that an LLM performs well not only on specific\ntasks but also on general NLP tasks. To achieve\nthis, we follow a similar approach as outlined in In-\nstructGPT (Ouyang et al., 2022). We sample 10%\ndata from general NLP tasks in GLUE (Wang et al.,\n2018) and the SuperGLUE benchmark (Wang et al.,\n2019), which are considered representative, as in-\ndicated in Table 3, during the pre-training phase.\nThe objective of pre-training is to generate out-\nputs that are as good as or better than the original\none based on the original prompts. The original\nprompts are taken from Natural Instructions (Wang\net al., 2022b). The loss function is as follows:\nLPre = γEx ∼Dpretrain[log(πRL\nϕ (x))], (11)\nwhere Dpretrain represents the selected datasets for\npre-training.\nJoint learning. Finally, we make joint learning\nwith the above-mentioned loss functions as follows:\nLϕ = γ1Lρ + γ2LSFT + γ3LPre. (12)\n4 Experiments\nIn this section, We conduct experiments with three\npopular LLMs as ˆLLM, respectively, including\nBLOOM (7B), GPT-J (6B), and LLaMA (7B), on\ndifferent downstream tasks to validate the effective-\nness of MAPO.\n4.1 Experimental Setups\nThe experiments are executed on 4 Nvidia A100\nGPUs with 80GB each, using PyTorch in Python.\nDeepSpeed 2 is utilized in the training process. The\nmaximum sequence length for original prompts and\noptimized prompts are both set to 512 tokens. The\nnumber of epochs is set to 20 in the entire training\nprocess. We provide the detailed configuration of\nthe hyperparameters in Appendix B. The dataset\nand metrics we utilize are the same as those de-\nscribed in Sec. 2. All results are reported on the\ncorresponding test sets or 10% dev sets if a dataset\ndoes not have a test set. Details of all used baselines\nand datasets are in Appendix D and E.\n4.2 Main Results\nThe main results are shown in Table 2. We observe\nthat the performance increase evidently among\nall LLMs during SFT. We then utilize MAPO to\nmake further optimization. We find the optimized\nprompts generated by MAPO are more adaptive\nin QA and generation task for BLOOM (increase\nby 20.5% for CloseQA and by 30.9% for Explan\ncompared with SFT (p<0.01)) and GPT-J (increase\nby 21.4% for CloseQA and by 20.6% for Explan\ncompared with SFT (p<0.01)). And the prompts\nare more adaptive in classification task (increase\nby 22.8% for News (p<0.01)) for LLaMA. These\nresults indicate that MAPO effectively enhances\nthe performance of various LLMs and exhibits pref-\nerences in different downstream tasks.\nTo validate the superiority of MAPO, we com-\npare it with several SOTA prompt optimization\nbaselines in various popular tasks based on the\nsame setting Roberta-Large, as shown in Table 3.\nThe reported results represent the best-performing\nLLM among the three LLMs, which indicates that\n2https://github.com/microsoft/DeepSpeed\n3283\nTask Dataset Original SFT-optimized ↑(%) MAPO-optimized ↑(%)BLOOM GPT-J LLaMA BLOOM(↑(%)) GPT-J(↑(%)) LLaMA(↑(%)) BLOOM ( ↑(%)) GPT-J(↑(%)) LLaMA(↑(%))\nQA AdverQA 13.5 3.0 3.2 18.3 (35.6) 9.4 (213.3) 23.2 (625.0) 291.3 19.5 (6.6) 11.0 (17.0) 25.1 (8.2) 10.6OpenQA 25.9 17.0 13.3 26.7 (3.1) 20.3 (19.4) 15.4 (15.8) 12.8 27.2 (1.9) 21.0 (3.4) 16.1 (4.5) 3.3CloseQA 6.4 6.9 10.8 7.8 (21.9) 8.4 (21.7) 13.9 (28.7) 24.1 9.4 (20.5) 10.2 (21.4) 14.8 (6.5) 16.1\nCLS News 92.8 0.0 1.1 95.5 (2.9) 5.5 (-) 10.1 (818.2) - 98.7 (3.4) 6.3 (14.5) 12.4 (22.8) 13.6Movie 90.9 51.1 78.7 92.6 (1.9) 52.7 (3.1) 81.3 (3.3) 2.8 93.3 (0.8) 53.9 (2.3) 82.5 (1.5) 1.5QASC 99.4 54.0 61.6 99.9 (0.5) 56.3 (4.3) 70.2 (14.0) 6.2 99.9 (0.0) 56.8 (0.9) 72.8 (3.7) 1.5\nGEN Topics 29.5 17.5 14.3 34.8 (18.0) 21.6 (23.4) 18.6 (30.1) 23.8 36.2 (4.0) 23.4 (8.3) 19.5 (4.8) 5.7Summary 46.1 13.1 6.6 48.8 (5.9) 16.7 (27.5) 10.7 (62.1) 31.8 50.2 (2.9) 17.8 (6.6) 12.2 (14.0) 7.8Explan 5.7 8.5 6.9 6.8 (19.3) 10.7 (25.9) 8.2 (18.8) 21.3 8.9 (30.9) 12.9 (20.6) 9.1 (11.0) 20.8\nTable 2: Performance is evaluated for BLOOM, GPT-J, and LLaMA using original, SFT-optimized, and MAPO-optimized\nprompts with a frozen LLM during inference. The symbols ↑(%) and ↑(%) under SFT-optimized denote the relative increase from\noriginal prompts, while those under MAPO-optimized indicate improvement over SFT-optimized results. It’s emphasized that the\nterm “frozen LLM for inference” means the model hasn’t been trained directly on downstream tasks but only makes inferences.\nThus, there’s no training data with prompts as inputs and expected responses as outputs. “CLS” represents classification, and\n“GEN” stands for generation tasks.\nSST-2 YelpP. MR CR RTE QNLI SNLI MNLI MRPC\nF Finetuning 80.6 88.7 67.4 73.3 58.6 60.2 54.6 47.8 77.4\nC Softprompt 73.8 88.6 74.1 75.9 54.7 49.7 36.1 33.2 51.6Black-Box 89.1 93.2 86.6 87.4 52.6 48.8 46.6 42.9 61.6Autoprompt 75 79.8 62 57.5 - - - - -\nD Manual 82.8 83 80.9 79.6 51.6 50.8 31.1 51.7 67.4In-Context 85.9 89.6 80.6 85.5 60.4 53.8 47.1 53.4 45.8Instructions 89 84.4 85.2 80.8 - - - - -GrIPS 87.1 88.2 86.1 80 - - - - -RLprompt 92.5 95.1 87.1 89.5 - - - - -TEMPERA 91.9 92.6 8891.160.3 57.4 56.4 45.2 74AMA 95.7 - - - 75.1 - - - -SFT 94.9 92 88.5 87.6 74.3 62.5 58.8 54.6 78.5MAPO-w/o g 96.0 93.3 90.1 88.7 75.2 63.0 59.8 55.7 79.0\nD MAPO 96.1 93.5 90.288.975.3 63.1 60.0 55.7 79.3\nTable 3: The few-shot performance of SFT, MAPO\nwith SOTA prompt optimizing baselines in downstream\ntasks. F: Finetuning, C: Continous prompt, D: Discrete\nprompt.\nour method applies not only to LLMs but also to\nsmaller LMs. We analyze the possible reasons as\nfollows: MAPO employs both SFT and RL to opti-\nmize LLMs. In fact, the SFT process is not specific\nto LM. Fine-tuning smaller models is feasible and\ncommon, requiring fewer computational resources.\nRL is a widely-used algorithm across applications\nand model scales, and small models require less\ncomputational and storage resources, making RL\nmore feasible on them.\nWe also test the performance of MAPO with\nthe above-mentioned SOTA prompt optimization\nbaselines. We use three LLMs, including BLOOM,\nGPT-J, and LLaMA, to replace the BART model\nused in Table 3 for verifying the nine datasets in\nTable 2, as shown in Table 4. Due to SFT in LLMs\nequals fine-tuning pretrained language models, we\ndirectly list the SFT results in the Fine-tuning row.\nApart from Fine-tuning, we also freeze the LLMs\nand only modify the prompts for inference on down-\nstream tasks. According to the experimental results,\nthe performance of almost all baselines, except\nRLprompt, does not exceed that of Fine-tuning\n/SFT, and some even do not outperform the origi-\nnal LLMs. This highlights the importance of SFT\nin LLMs. When we add RL, as in the case of\nRLprompt, the performance on downstream tasks\nsurpasses that of SFT, indicating the significance of\nRL for prompt optimization. Moreover, using our\nproposed MAPO method to optimize the prompt\nfurther improves performance over RLprompt, ex-\ncept in a very few cases, such as using BLOOM for\nmovie classification tasks. These experimental re-\nsults demonstrate that the MAPO method proposed\nin this study makes a substantial contribution to\nimproving the performance and accuracy in down-\nstream tasks.\nMoreover, we conduct experiments to evaluate\nthe domain transfer performance of MAPO. The re-\nsults are presented in Table 5 and Table 6, while the\nresults of LLMs with original prompts are reported\nby Arora et al. (2022). Remarkably, we observe\nthat each LLM, when using prompts optimized\nby MAPO, displays improved performance across\nvarious downstream tasks. Specifically, BLOOM\nexhibits the highest increase in performance com-\npared with GPT-J and LLaMA. This experiment\nclearly demonstrates the significant domain trans-\nfer capability of MAPO.\n4.3 Ablation Study\nThe effect of RL compared with SFT.From the\nexperiments (Table 3, Table 5 and Table 6), we can\nobserve that the performance improvements gained\nsolely from using SFT are less than half of those\nachieved by our proposed MAPO method, both on\nsimilar tasks and general NLP tasks. This clearly\nindicates the effectiveness of MAPO in optimizing\nmodel-adaptive prompts.\nIn order to further demonstrate RL is necessary\nand how it compares to simply extending SFT with\na larger warm-up dataset, we use various propor-\ntions of the warm-up dataset to progressively in-\ncrease the SFT training data and then introduce RL\n3284\nTask QA CLS GENAd Op Cl Ne Mo QA To Su Ex\n(BLOOM) Original 13.5 25.9 6.4 92.8 90.9 99.4 29.5 46.1 5.7F Finetuning/SFT 18.3 26.7 7.8 95.5 92.6 99.9 34.8 48.8 6.8C Soft prompt 14.5 24.5 6.5 92.1 90.5 99.1 30.1 44.6 5.5Black-Box 15.2 24.7 6.9 93.3 91.6 99.3 31.2 45.7 5.8Autoprompt 15.7 25.0 7.1 93.6 91.9 99.4 31.6 46.0 6D Manual 13.7 24.6 6.8 91.8 90.9 99.0 31.5 45.1 5.7In-Context 13.9 24.7 6.7 91.6 90.9 99.3 31.8 45.6 5.9Instructions 15.0 24.9 6.7 92.7 91.0 99.2 30.8 45.5 5.8GrIPS 16.6 25.3 6.8 93.1 91.2 99.4 31.8 46.7 6.2RLprompt 19.2 26.9 8.9 97.594.1 99.935.9 49.1 8.2TEMPERA 17.4 25.5 7.2 94.6 91.7 99.5 33.1 46.7 6.2AMA 19.1 26.4 7.6 95.1 92.4 99.4 33.5 47.9 6.1D MAPO 19.5 27.2 9.4 98.793.399.9 36.2 50.2 8.9\n(GPT-J) Original 3.0 17.0 6.9 0.0 51.1 54 17.5 13.1 8.5F Finetuning/SFT 9.4 20.3 8.4 5.5 52.7 56.3 21.6 16.7 10.7C Soft prompt 5.4 16.6 6.8 2.1 51.0 54.5 17.8 13.7 8.9Black-Box 7.3 17.5 7.2 2.5 51.4 54.7 18.2 14.3 9.1Autoprompt 7.8 17.9 7.3 1.9 51.5 54.6 19.1 14.6 9.3D Manual 5.7 15.9 6.5 1.7 50.9 54.9 18.7 13.9 9.5In-Context 5.6 16.5 6.7 1.6 50.7 54.6 19.2 14.0 9.3Instructions 7.1 17.1 7.2 1.7 51.6 54.0 19.1 14.3 9.2GrIPS 7.7 17.9 7.7 4.3 52.1 55.6 19.7 16.4 10.3RLprompt 10.1 20.0 9.5 5.7 53.7 56.4 22.1 17.2 11.8TEMPERA 9.5 19.3 9.1 4.8 53.2 56.1 22.3 16.8 11.4AMA 9.6 19.1 8.8 5.0 52.9 56.3 22.4 16.5 11.6D MAPO 11.0 21.0 10.2 6.3 53.9 56.8 23.4 17.8 12.9\n(LLaMA) Original 3.2 13.3 10.8 1.1 78.7 61.6 14.3 6.6 6.9F Finetuning/SFT 23.2 15.4 13.9 10.1 81.3 70.2 18.6 10.7 8.2C Soft prompt 7.7 12.6 10.2 4.4 77.4 62.7 15.6 8.2 7.3Black-Box 9.2 13.3 10.7 5.6 78.1 63.1 16.2 8.4 7.5Autoprompt 10.3 13.5 11.0 7.4 78.4 65.2 16.7 9.0 7.6D Manual 12.3 12.7 11.1 7.5 77.5 64.8 16.2 8.3 6.7In-Context 13.7 13.0 10.8 8.0 77.7 65.3 16.5 8.5 7.0Instructions 16.2 13.2 11.3 7.5 78.0 65.7 17.1 9.1 7.5GrIPS 19.3 14.7 13.4 9.3 80.6 68.6 18.9 10.7 8.3RLprompt 24.7 15.8 14.3 11.6 81.9 71.4 19.2 11.7 8.8TEMPERA 22.6 15.4 13.8 8.9 81.5 69.6 18.7 9.6 8.7AMA 23.5 15.5 13.6 8.6 81.7 70.5 18.5 9.8 8.9D MAPO 25.1 16.1 14.8 12.4 82.5 72.8 19.5 12.2 9.1\nTable 4: The performance with a frozen LLM for infer-\nence of MAPO with SOTA prompt optimizing baselines\nin nine tasks from P3 benchmark using LLaMA. F: Fine-\ntuning/SFT, C: Continous prompt, D: Discrete prompt.\nto it as shown in Table 17. Our findings consis-\ntently show that RL adds value to the performance\nbeyond what is achieved by SFT alone across all\nproportions of the dataset. This affirms the effec-\ntiveness of RL irrespective of the SFT dataset size.\nHowever, as the proportion of the warm-up dataset\nincreases, the margin of improvement from adding\nRL begins to decline. While one could hypothe-\nsize that adding RL to a very large SFT dataset\nmight not result in as significant an improvement\nas it would for a smaller dataset, this observation\nactually underscores our method’s suitability for\nlow-resource scenarios.\nMoreover, we have tried different number of\nepochs to see if extended training time consis-\ntently improves SFT performance as shown in Ta-\nble 18. Extending the training time does not con-\nsistently lead to performance improvements for\nSFT. In some instances, the performance even de-\nclines. It is important to note that we save the\nbest-performing models in real-time during train-\ning, as the peak performance does not necessarily\noccur at the final epoch.\nThe effect of warm-up dataset.As shown in\nFig. 5 and Table 11, our study examines the effects\nof different proportions of the warm-up dataset on\nMAPO’s performance. The results indicate that\nas the size of the warm-up dataset increases, per-\nformance typically improves. BLOOM is particu-\nlarly sensitive, showing a pronounced growth trend.\nConversely, GPT-J shows a more gradual growth.\nLLaMA’s performance reveals an inflection around\n60%, suggesting other factors also influence its per-\nformance. Even with reduced dataset sizes, the\ndecrement in performance remains minimal, high-\nlighting the method’s suitability for low-resource\ntasks. We also conduct few-shot experiments on\ngeneral NLP tasks with just 10% data and ob-\nserve promising improvements. This underlines\nour method’s adaptability and effectiveness in sce-\nnarios of data scarcity.\nThe effect of PPO and RRMF.To investigate\nthe specific effects of PPO and RRMF during the\nRL process, we conduct separate experiments to\nevaluate the contributions of each component. The\nexperimental results, depicted in Fig.6 (with de-\ntails provided in Table 10 in Appendix F), clearly\ndemonstrate the important roles played by PPO and\nRRMF in enhancing the performance of MAPO.\nWe propose the following explanations for these\nresults: PPO focuses on reducing the dissimilar-\nity between the RL model and the SFT model.\nRRMF aligns the scores from the reward model\nwith the likelihood probabilities of an LLM. Both\nPPO and RRMF aim to assign higher probabilities\nto prompts that are more adaptable to the model.\nThe effect of the Randomness.We also incor-\nporate randomness (e.g., temperature) during the\ngeneration process of LLM. Given that our prompts\ndo not require high creativity, we have set a lower\ntemperature range [0-0.5] for generation, within\nwhich we aim to generate optimal prompts. To fur-\nther investigate the impact of varying temperatures\non the generated output, we conduct an additional\nset of experiments to assess the performance of the\nMAPO method under different randomness settings\n(temperature=0,0.2,0.5,0.8) as shown in Table 14,\nTable 12, Table 15 and Table 16. Each experiment\ngroup runs 5 times. Our findings reveal that a high-\ntemperature setting (t=0.8) tends to produce infe-\nrior prompts that lead to less accurate outputs for a\nspecific task. Lower temperature (t=0.2) or greedy\nsettings (t=0) are likely to produce more accurate\noutputs that are closer to our optimal results. This\nsuggests that in a task like prompt optimization,\n3285\nTask Dataset BLOOM +SFT(↑(%)) ↑(%) +MAPO (↑(%)↑(%) GPT-J +SFT(↑(%)) ↑(%)+MAPO (↑(%))↑(%)\nCoref. xwinograd 60.1 60.2 0.2 0.2 60.6 0.9 0.9 - - - - - - -\nNLU BoolQ 67.9 68.0 0.1 0.4 68.2 0.4 0.9 67.2 67.4 0.3 0.2 67.9 1.0 0.5CB 77.6 77.8 0.3 78.1 0.6 83.9 84.1 0.2 84.2 0.4COPA 74.0 74.3 0.4 75.0 1.4 84.0 84.2 0.2 84.2 0.2MultiRC 59.7 60.3 1.0 60.4 1.2 63.8 63.9 0.2 64.1 0.5ReCoRD 69.8 70.1 0.4 70.2 0.6 74.4 74.5 0.1 74.7 0.4WiC 61.4 61.6 0.3 62.0 1.0 61.0 61.1 0.2 61.3 0.5WSC 64.4 64.7 0.5 65.1 1.1 77.9 78.0 0.1 78.1 0.3\nNLI ANLIR1 31.5 31.7 0.6 0.6 32.1 1.9 1.3 37.8 38.0 0.5 0.3 38.2 1.1 0.7ANLIR2 35.1 35.2 0.3 35.4 0.9 37.9 38.0 0.3 38.3 1.1ANLIR3 37.1 37.5 1.1 37.8 1.9 40.9 41.0 0.2 41.1 0.5StoryCloze 79.0 79.2 0.3 79.5 0.6 87.8 87.9 0.1 87.9 0.1\nCLS Amazon 65.2 66.4 1.8 1.4 67.7 3.8 3.3 68.2 68.7 0.7 0.6 69.4 1.8 1.6DBPedia 70.5 71.2 1.0 72.5 2.8 83.9 84.2 0.4 85.1 1.4\nQA DROP 67.9 68.2 0.4 1.9 69.9 2.9 5.6 51.6 51.9 0.6 1.3 52.8 2.3 3.2NQ 15.1 15.4 2.0 16.1 6.6 19.6 20.1 2.6 20.8 6.1RealTimeQA 29.0 30.2 4.1 31.5 8.6 36.0 36.5 1.4 37.2 3.3WebQs 34.8 35.1 0.9 36.3 4.3 44.1 44.3 0.5 44.6 1.1\nTable 5: Zero-shot domain transfer performance based on BLOOM and GPT-J with original, SFT-optimized and\nMAPO-Optimized prompts. CLS: Classification, M: MAPO. The (↑(%) ) and ↑(%) represent the increase degree of\nMAPO-optimized prompts compared with original prompts in each dataset and task, respectively (The same below).\nTask Dataset LLaMA +SFT (↑(%))↑(%) +MAPO (↑(%))↑(%)\nRS BoolQ 76.5 76.6 0.1 0.1 76.7 0.3 0.5PIQA 79.8 79.9 0.1 80.0 0.3SIQA 48.9 48.9 0.0 49.0 0.2HellaSwag 76.1 76.2 0.1 76.5 0.5WinoGrande 70.1 70.2 0.1 70.5 0.6ARC-e 72.8 72.9 0.1 73.2 0.6ARC-c 47.6 47.6 0.0 47.8 0.4OBQA 57.2 57.4 0.3 57.9 1.2\nQA NQ 16.8 17.2 2.4 1.4 18.1 7.7 4.7RACE 50.0 50.2 0.4 50.8 1.6\nTable 6: Zero-shot domain transfer performance based\non LLaMA with original, SFT-optimized and MAPO-\nOptimized prompts. RS: Commonsense Reasoning.\nintroducing a stable (low temperature) but slight\ndegree of variability (non-zero temperature) yields\nthe best results.\n4.4 Case Study and Error Analysis\nWe conduct a case study to visualize the prompts\noptimized by MAPO, as shown in Table 7. Addi-\ntional cases are included in Appendix G. We first\nobserve that the majority of the original prompts\nhave significant modifications after optimization\nthrough our MAPO method. Only about 10% of\nthe generated prompt pairs remain completely un-\nchanged. To further quantify these changes, we\ncalculate a normalized edit distance. Given the\nvarying lengths of different prompt pairs, we di-\nvide the edit distance by the average length of the\ntwo strings. This yields a value between 0 and 1,\nwhere 0 indicates identical strings and 1 indicates\ncompletely different strings. The average normal-\nized edit distance for all prompt pairs stands at\n0.67, demonstrating that most prompts do experi-\nence substantial modifications.\nNext, we provide a detailed examination of these\nmodifications. In the QA task, BLOOM transforms\nactive voice into passive voice, GPT-J utilizes the\nphrase “the term used” and substitutes “refer to”\nwith “denote”, while LLaMA adopts a more in-\nformal style by mentioning the “commonly used\nterm”. In the generation task, both BLOOM and\nGPT-J present similar prompts that emphasize topic\ncoverage. LLaMA maintains the original sentence\nstructure but modifies the subjects and replaces\n“decorate” with “adorn”. In the classification task,\nall three LLMs rearrange the word order and of-\nfer additional details about the topic. Therefore,\nMAPO demonstrates its prompt optimization capa-\nbilities by adapting better prompts to specific tasks\nfor different LLMs while preserving core informa-\ntion and adjusting tone or structure as necessary.\nHowever, there are also some errors during\nprompt optimization, including prompts with in-\ncomplete sentences, prompts with improper prepo-\nsitions or missing necessary parts, and prompts\nwith ambiguous meanings, etc. Therefore, there is\nample room for improvement in MAPO to better\nadapt to different LLMs in downstream tasks.\n4.5 Exploratory Analysis\nWe conduct an exploratory analysis to further inves-\ntigate the patterns in optimized prompt as shown in\nFig. 7, Fig. 8 and Fig. 9. We extract the three most\nfrequent words from the original prompt and inves-\ntigate their distribution in the optimized prompt for\neach LLM, while either retaining high-frequency\nwords in instructions (including sentence, topics,\nsubjects, present, statement, discussed, mentioned,\nincluded, following) or removing them.\nTaking the generation task (Fig. 7) as an exam-\nple, when high-frequency words in instructions are\nretained, we observe that BLOOM retains a rel-\natively higher proportion of the original prompts\ncompared to GPT-J and LLaMA, while LLaMA\nretains the fewest. When these words are removed,\n3286\nTaskPrompts\nQA\nOriginal: How do people in Mexico refer to the largest Presbyterian church?BLOOM: What is the way in which the biggest Presbyterian church is referred to by individuals in Mexico?GPT-J: What is the term used by Mexicans to denote the biggest Presbyterian church?LLaMA: What is the commonly used term for the biggest Presbyterian church in Mexico?\nGen\nOriginal: What are the topics in the sentence: The furniture in the living room is decorated with flowers.BLOOM: What topics does the sentence “The living room furniture is adorned with flowers” cover?GPT-J: What topics are covered in the sentence “The living room furniture is adorned with flowers”?LLaMA: Which subjects are present in the statement: Flowers adorn the furniture in the living room.\nClass\nOriginal: Reincarnated Napster Goes Public Roxio bought the Napster name in a bankruptcy auction two years ago. Now, the company dumps its CD-burning software businessto concentrate on selling and delivering music over the web under the Napster name.What label best describes this news article?BLOOM: Roxio, the buyer of Napster’s name in a bankrupt auction two years ago, has taken the reincarnated Napster public. The company has made the decision to abandontheir CD-burning software business and focus solely on the distribution and sale of music online via the Napster name. What genre of news article would this story fall under?GPT-J: Roxio, which acquired the Napster name through a bankruptcy auction two years ago, has taken Napster public again after rebranding it as a music delivery service. Thecompany has divested its CD-burning software business and now focuses solely on selling music online under the Napster name. What category would this news article fall under?LLaMA: The Napster name, which was purchased by Roxio in a bankruptcy auction two years ago, has now been resurrected with a public launch. Roxio has shifted its focussolely to the sale and distribution of music under the Napster name, leaving its CD-burning software business behind. What category would you assign to this news article?\nTable 7: Original prompts and MAPO-optimized prompts for three LLMs in various downstream tasks.\nwe notice that BLOOM has a higher proportion of\nwords like “man”, “view” in its optimized prompts,\nwhich are more relative with human. GPT-J has a\nhigher proportion of words like “match”, “grass”,\n“bathroom”, “white”, which suggests it focuses on\nspecific scenes, objects, or themes. LLaMA has a\nhigher proportion of words like “room”, “close”,\n“playing”, indicating its preferences on place and\nexperiences. The variations observed in word distri-\nbution indicate that each LLM tends to emphasize\ndifferent aspects during the optimization process.\nAccurate conclusions need more experiments.\n5 Related Work\nLLMs’ prompt optimization process involves\nprompt retrieval, prompt generation from scratch\nand prompt editing. For prompt retrieval, for ex-\nample, Ma et al. (2023) adopt greedy search to\nidentify near-optimal prompts. Zhou et al. (2022)\nintroduce APE for automatic instruction selection,\netc. For prompt generation from scratch, Pang et al.\n(2023) introduce SharpT, which learns a shared\nlatent space and generates soft prompts. White\net al. (2023) describe a catalog of prompt engineer-\ning techniques. Zamfirescu-Pereira et al. (2023)\ninvestigate end-user prompt engineering using a\nprototype LLM-based chatbot design tool. Wang\net al. (2022a) present Self-Instruct for improving\ninstruction-following capabilities of PLMs. For\nprompt editing, Gao et al. (2020) automatically se-\nlect label words and generate templates. Pryzant\net al. (2023) introduce APO based on “gradients”\nto provide critical feedback on the current prompt.\nDeng et al. (2022) propose RLprompt based on RL.\nZhang et al. (2023) propose TEMPERA, which\nprovides interpretable prompts for different queries.\nPrasad et al. (2022) introduce GrIPS, a gradient-\nfree approach for improving task instructions for\nLLMs. Moreover, some research focuses on incor-\nporating additional knowledge to enhance prompt\nediting. For example, Li et al. (2023) propose\nDSP to generate “directional stimulus” of each in-\nput. Qin and Eisner (2021a) optimize a mixture\nof prompts using gradient descent to generate rela-\ntional knowledge. Shin et al. (2020) develop Auto-\nprompt, a gradient-guided approach to find the best\ntokens in the prompt. Jiang et al. (2020) propose\nmining-based and paraphrasing-based methods to\nautomatically generate diverse prompts. Further-\nmore, some research focus on continuous prompt\noptimization instead of discrete prompt optimiza-\ntion mentioned before, such as research by Zheng\net al. (2023), Hambardzumyan et al. (2021), Zhong\net al. (2021), etc.\nHowever, all above-mentioned prompts opti-\nmization approaches aim to obtain task-specific\nprompts instead of model-specific ones. Different\nfrom theirs, we dedicate at optimizing prompts for\nLLMs within the NLP domain and achieve impres-\nsive performance.\n6 Conclusions\nThe remarkable capabilities of LLMs have revolu-\ntionized NLP in various tasks. However, their per-\nformance heavily relies on the quality of prompts.\nIn this work, we address the prompt optimization\nchallenge by proposing a Model-Adaptive Prompt\nOptimization (MAPO) approach. Through exten-\nsive experiments, we demonstrated that MAPO\ncan adapt different LLMs with generating model-\nfriendly prompts to enhance their capabilities\nacross various downstream tasks. In future work,\nwe aim to construct more fine-grained model-\nadaptive prompts that can adapt to the continuously\nevolving data encountered in real-world production\nenvironments. Additionally, we intend to enhance\nits applicability across a broad spectrum of linguis-\ntic contexts.\n3287\nLimitations\nIt is important to acknowledge certain limitations of\nour approach. Firstly, the effectiveness of prompt\noptimization heavily relies on the availability and\nquality of the warm-up dataset. In cases where the\ndataset is limited or does not sufficiently cover the\nspecific task, the performance gains from prompt\noptimization may be constrained. Additionally,\nMAPO requires extensive SFT and RL, which can\nbe computationally expensive and time-consuming.\nThis could limit the scalability of MAPO, es-\npecially when dealing with large-scale tasks or\ndatasets. Despite these limitations, our study pro-\nvides valuable insights into model-adaptive prompt\noptimization for LLMs and contributes to the ongo-\ning efforts in improving the performance of these\nLLMs in practical applications.\nAcknowledgement\nThis work is supported by Shanghai Munic-\nipal Science and Technology Major Project\n(No.2021SHZDZX0103), Science and Technol-\nogy Commission of Shanghai Municipality Grant\n(No. 22511105902), the National Natural Science\nFoundation of China (No.62072323, U21A20488),\nShanghai Science and Technology Innovation Ac-\ntion Plan (No. 22511104700), Key Projects of\nIndustrial Foresight and Key Core Technology Re-\nsearch and Development in Suzhou(SYC2022009).\nReferences\nSimran Arora, Avanika Narayan, Mayee F Chen, Lau-\nrel J Orr, Neel Guha, Kush Bhatia, Ines Chami, Fred-\neric Sala, and Christopher Ré. 2022. Ask me any-\nthing: A simple strategy for prompting language mod-\nels. arXiv preprint arXiv:2210.02441.\nLuisa Bentivogli, Peter Clark, Ido Dagan, and Danilo\nGiampiccolo. 2009. The fifth pascal recognizing\ntextual entailment challenge. In TAC. Citeseer.\nSamuel R Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D Manning. 2015. A large annotated\ncorpus for learning natural language inference. arXiv\npreprint arXiv:1508.05326.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nSébastien Bubeck, Varun Chandrasekaran, Ronen El-\ndan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-\nberg, et al. 2023. Sparks of artificial general intelli-\ngence: Early experiments with gpt-4. arXiv preprint\narXiv:2303.12712.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2005. The pascal recognising textual entailment chal-\nlenge. In Machine learning challenges workshop,\npages 177–190. Springer.\nDorottya Demszky, Kelvin Guu, and Percy Liang. 2018.\nTransforming question answering datasets into nat-\nural language inference datasets. arXiv preprint\narXiv:1809.02922.\nMingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan\nWang, Han Guo, Tianmin Shu, Meng Song, Eric P\nXing, and Zhiting Hu. 2022. Rlprompt: Optimizing\ndiscrete text prompts with reinforcement learning.\narXiv preprint arXiv:2205.12548.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nBill Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Third International Workshop on Paraphrasing\n(IWP2005).\nTianyu Gao, Adam Fisch, and Danqi Chen. 2020.\nMaking pre-trained language models better few-shot\nlearners. arXiv preprint arXiv:2012.15723.\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan, and\nWilliam B Dolan. 2007. The third pascal recognizing\ntextual entailment challenge. In Proceedings of the\nACL-PASCAL workshop on textual entailment and\nparaphrasing, pages 1–9.\nR Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo\nGiampiccolo, Bernardo Magnini, and Idan Szpektor.\n2006. The second pascal recognising textual entail-\nment challenge. In Proceedings of the Second PAS-\nCAL Challenges Workshop on Recognising Textual\nEntailment, volume 7.\nKaren Hambardzumyan, Hrant Khachatrian, and\nJonathan May. 2021. Warp: Word-level adversarial\nreprogramming. arXiv preprint arXiv:2101.00121.\nYaru Hao, Zewen Chi, Li Dong, and Furu Wei. 2022.\nOptimizing prompts for text-to-image generation.\narXiv preprint arXiv:2212.09611.\nMinqing Hu and Bing Liu. 2004. Mining and summa-\nrizing customer reviews. In Proceedings of the tenth\nACM SIGKDD international conference on Knowl-\nedge discovery and data mining, pages 168–177.\nEllen Jiang, Kristen Olson, Edwin Toh, Alejandra\nMolina, Aaron Donsbach, Michael Terry, and Carrie J\nCai. 2022. Promptmaker: Prompt-based prototyping\nwith large language models. In CHI Conference on\nHuman Factors in Computing Systems Extended Ab-\nstracts, pages 1–8.\n3288\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics, 8:423–438.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. arXiv\npreprint arXiv:2101.00190.\nZekun Li, Baolin Peng, Pengcheng He, Michel Galley,\nJianfeng Gao, and Xifeng Yan. 2023. Guiding large\nlanguage models via directional stimulus prompting.\narXiv preprint arXiv:2302.11520.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Text summarization\nbranches out, pages 74–81.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nHuan Ma, Changqing Zhang, Yatao Bian, Lemao Liu,\nZhirui Zhang, Peilin Zhao, Shu Zhang, Huazhu Fu,\nQinghua Hu, and Bingzhe Wu. 2023. Fairness-\nguided few-shot prompting for large language mod-\nels. arXiv preprint arXiv:2303.13217.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and\nHannaneh Hajishirzi. 2021. Cross-task generaliza-\ntion via natural language crowdsourcing instructions.\narXiv preprint arXiv:2104.08773.\nOpenAI. 2023. Gpt-4 technical report.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nBo Pang and Lillian Lee. 2005. Seeing stars: Exploiting\nclass relationships for sentiment categorization with\nrespect to rating scales. arXiv preprint cs/0506075.\nBo Pang, Semih Yavuz, Caiming Xiong, and Yingbo\nZhou. 2023. Sharpt: Shared latent space prompt\ntuning. In Findings of the Association for Computa-\ntional Linguistics: EACL 2023, pages 1214–1220.\nArchiki Prasad, Peter Hase, Xiang Zhou, and Mohit\nBansal. 2022. Grips: Gradient-free, edit-based in-\nstruction search for prompting large language models.\narXiv preprint arXiv:2203.07281.\nReid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chen-\nguang Zhu, and Michael Zeng. 2023. Automatic\nprompt optimization with\" gradient descent\" and\nbeam search. arXiv preprint arXiv:2305.03495.\nGuanghui Qin and Jason Eisner. 2021a. Learning how\nto ask: Querying lms with mixtures of soft prompts.\narXiv preprint arXiv:2104.06599.\nGuanghui Qin and Jason Eisner. 2021b. Learning how\nto ask: Querying LMs with mixtures of soft prompts.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 5203–5212, Online. Association for Computa-\ntional Linguistics.\nRajkumar Ramamurthy, Prithviraj Ammanabrolu,\nKianté Brantley, Jack Hessel, Rafet Sifa, Christian\nBauckhage, Hannaneh Hajishirzi, and Yejin Choi.\n2022. Is reinforcement learning (not) for natural\nlanguage processing?: Benchmarks, baselines, and\nbuilding blocks for natural language policy optimiza-\ntion. arXiv preprint arXiv:2210.01241.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H.\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja,\nManan Dey, M Saiful Bari, Canwen Xu, Urmish\nThakker, Shanya Sharma Sharma, Eliza Szczechla,\nTaewoon Kim, Gunjan Chhablani, Nihal Nayak, De-\nbajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang,\nHan Wang, Matteo Manica, Sheng Shen, Zheng Xin\nYong, Harshit Pandey, Rachel Bawden, Thomas\nWang, Trishala Neeraj, Jos Rozen, Abheesht Sharma,\nAndrea Santilli, Thibault Fevry, Jason Alan Fries,\nRyan Teehan, Stella Biderman, Leo Gao, Tali Bers,\nThomas Wolf, and Alexander M. Rush. 2021. Multi-\ntask prompted training enables zero-shot task gener-\nalization.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman\nCastagné, Alexandra Sasha Luccioni, François Yvon,\nMatthias Gallé, et al. 2022. Bloom: A 176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100.\nTimo Schick and Hinrich Schütze. 2020. Exploit-\ning cloze questions for few shot text classification\nand natural language inference. arXiv preprint\narXiv:2001.07676.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal,\nAlec Radford, and Oleg Klimov. 2017. Proxi-\nmal policy optimization algorithms. arXiv preprint\narXiv:1707.06347.\nTaylor Shin, Yasaman Razeghi, Robert L Logan IV ,\nEric Wallace, and Sameer Singh. 2020. Autoprompt:\nEliciting knowledge from language models with\nautomatically generated prompts. arXiv preprint\narXiv:2010.15980.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 conference on empiri-\ncal methods in natural language processing, pages\n1631–1642.\nTianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing\nHuang, and Xipeng Qiu. 2022. Black-box tuning for\n3289\nlanguage-model-as-a-service. In International Con-\nference on Machine Learning, pages 20841–20855.\nPMLR.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel Bowman. 2019. Superglue: A stick-\nier benchmark for general-purpose language under-\nstanding systems. Advances in neural information\nprocessing systems, 32.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\nBen Wang and Aran Komatsuzaki. 2021. Gpt-\nj-6b: A 6 billion parameter autoregressive\nlanguage model. https://github.com/kingoflolz/\nmesh-transformer-jax.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-\nisa Liu, Noah A Smith, Daniel Khashabi, and Han-\nnaneh Hajishirzi. 2022a. Self-instruct: Aligning lan-\nguage model with self generated instructions. arXiv\npreprint arXiv:2212.10560.\nYizhong Wang, Swaroop Mishra, Pegah Alipoor-\nmolabashi, Yeganeh Kordi, Amirreza Mirzaei,\nAnjana Arunkumar, Arjun Ashok, Arut Selvan\nDhanasekaran, Atharva Naik, David Stap, et al.\n2022b. Benchmarking generalization via in-context\ninstructions on 1,600+ language tasks. arXiv preprint\narXiv:2204.07705.\nJules White, Quchen Fu, Sam Hays, Michael Sandborn,\nCarlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse\nSpencer-Smith, and Douglas C Schmidt. 2023. A\nprompt pattern catalog to enhance prompt engineer-\ning with chatgpt. arXiv preprint arXiv:2302.11382.\nAdina Williams, Nikita Nangia, and Samuel R Bow-\nman. 2017. A broad-coverage challenge corpus for\nsentence understanding through inference. arXiv\npreprint arXiv:1704.05426.\nZheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang,\nSongfang Huang, and Fei Huang. 2023. Rrhf:\nRank responses to align language models with\nhuman feedback without tears. arXiv preprint\narXiv:2304.05302.\nJD Zamfirescu-Pereira, Richmond Y Wong, Bjoern\nHartmann, and Qian Yang. 2023. Why johnny can’t\nprompt: how non-ai experts try (and fail) to design\nllm prompts. In Proceedings of the 2023 CHI Con-\nference on Human Factors in Computing Systems,\npages 1–21.\nTianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schu-\nurmans, and Joseph E Gonzalez. 2023. Tempera:\nTest-time prompt editing via reinforcement learning.\nIn The Eleventh International Conference on Learn-\ning Representations.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text classi-\nfication. Advances in neural information processing\nsystems, 28.\nYuanhang Zheng, Zhixing Tan, Peng Li, and Yang Liu.\n2023. Black-box prompt tuning with subspace learn-\ning. arXiv preprint arXiv:2305.03518.\nZexuan Zhong, Dan Friedman, and Danqi Chen. 2021.\nFactual probing is [mask]: Learning vs. learning to\nrecall. arXiv preprint arXiv:2104.05240.\nYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,\nKeiran Paster, Silviu Pitis, Harris Chan, and Jimmy\nBa. 2022. Large language models are human-level\nprompt engineers. arXiv preprint arXiv:2211.01910.\nA Candidate Prompts\nFor the nine datasets selected in P3, we present\none prompt and the corresponding three candidate\nprompts for each dataset, as shown in Table 8.\nB Training Details\nWe provide the training details as shown in Table 9.\nOther hyper-parameters are set to default.\nC Computational Cost\nWhile the training phase is computationally inten-\nsive, the generation phase is relatively lightweight.\nSpecifically, once the prompt optimizing model\nMAPO is trained, the prompt generation process\nsimply involves a feed forward propagation to gen-\nerate the optimal prompt instead of further opti-\nmization through SFT and RL, thus significantly\nreducing the computational complexity. We list the\ncomputational complexity during the training and\ninference phase:\nTraining Phase. During the training phase,\ninitially, a warm-up dataset is established. This in-\nvolves generating candidate prompts using a model\nlike GPT-3.5. For each original prompt, 1000 can-\ndidate prompts are generated. This leads to a time\nand space complexity of O(N ×M), where N\nis the number of original prompts, and M is the\nnumber of candidates per prompt. Subsequently,\nan optimal prompt is searched for, which involves\ncomparisons among candidate prompts, yielding\ncomplexities of O(N ×M) for both time and\nspace. Building the prompt optimizer is the next\nstage. Supervised fine-tuning (SFT) has a time\ncomplexity of O(E×B×T), with E being the\nnumber of epochs, B the batch size, and T the\nnumber of model parameters. Its space complex-\nity mainly arises from model parameters and gra-\ndients, which is O(T). For building the reward\n3290\nTask Prompts\nAdverQA\nOriginal: Question: “Which happened earlier, the Chinese entered the war or President Truman dispatched the United States Seventh Fleet to the Taiwan Strait?”. Context:“On 27 June 1950, ...”. Answer:Candidate 1: Question: “Did President Truman dispatch the United States Seventh Fleet to the Taiwan Strait before or after the Chinese entered the war?” Context:“On 27June 1950, ...”. Answer:Candidate 2: Question: “Did the Chinese enter the war before President Truman dispatched the United States Seventh Fleet to the Taiwan Strait, or vice versa?” Context:“On 27June 1950, ...”. Answer:Candidate 3: Question: “Did the Chinese enter the war first or did President Truman send the United States Seventh Fleet to the Taiwan Strait earlier?”. Context:“On 27 June1950, ...”. Answer:\nOpenQA\nOriginal: What’s above the muscles and needs direct sunlight Which is the correct answer? Options: ...Candidate 1: What lies beyond the muscles and requires direct exposure to sunlight? Which is the correct answer? Options: ...Candidate 2:What is located above the muscles and requires direct sunlight? Which is the correct answer? -Options: ...Candidate 3: Which body part requires direct sunlight and is located higher than the muscles? Which is the correct answer? Options: ...\nCloseQA\nOriginal: Q: What kind of relationship between glucagon and insulin is vital to managing fuel storage and consumption by body cells? A:Candidate 1:Q: What is the essential connection between glucagon and insulin for regulating fuel storage and utilization in body cells? A:Candidate 2:Q: In managing the storage and consumption of fuel by body cells, what is the crucial interrelation between insulin and glucagon? A:Candidate 3: Q: What is the crucial connection between glucagon and insulin in regulating the storage and utilization of fuel by the cells in the body? A:\nNews\nOriginal: Reincarnated Napster Goes Public Roxio bought the Napster name in a bankruptcy auction two years ago. Now, the company dumps its CD-burning softwarebusiness to concentrate on selling and delivering music over the web under the Napster name. What label best describes this news article?Candidate 1: Roxio, the buyer of Napster’s name in a bankrupt auction two years ago, has taken the reincarnated Napster public. The company has made the decision toabandon their CD-burning software business and focus solely on the distribution and sale of music online via the Napster name. What genre of news article would this storyfall under?Candidate 2: Roxio, which acquired the Napster name through a bankruptcy auction two years ago, has taken Napster public again after rebranding it as a music deliveryservice. The company has divested its CD-burning software business and now focuses solely on selling music online under the Napster name. What category would this newsarticle fall under?Candidate 3:The Napster name, which was purchased by Roxio in a bankruptcy auction two years ago, has now been resurrected with a public launch. Roxio has shiftedits focus solely to the sale and distribution of music under the Napster name, leaving its CD-burning software business behind. What category would you assign to this newsarticle?\nMovie\nOriginal: writer/director joe carnahan’s grimy crime drama is a manual of precinct cliches , but it moves fast enough to cover its clunky dialogue and lapses in logic . Thesentiment expressed for the movie isCandidate 1: The gritty crime drama by writer and director Joe Carnahan may rely heavily on familiar tropes and cliches of the genre, but its quick pace manages to distractfrom any awkward dialogue and illogical moments. The sentiment expressed for the movie isCandidate 2: Joe Carnahan´s gritty crime drama relies heavily on standard police procedures, yet its rapid pace compensates for any cumbersome dialogues and unreasonableplot holes. The sentiment expressed for the movie isCandidate 3:Although writer/director Joe Carnahan´s gritty crime drama contains numerous stereotypes within the precinct environment, its swift pace effectively masks itsawkward dialogue and occasional lapses in logic. The sentiment expressed for the movie is\nQASC\nOriginal:If I tell you that Hydrogen bonds cause a tremendous force when a substance freezes, and ask you the question “hydrogen bonds cause a tremendous force when asubstance does what”, is the correct answer “strong”?Candidate 1: If I were to inform you that when a substance freezes, Hydrogen bonds create a significant force and ask, “What term describes the force generated by Hydrogenbonds when asubstance freezes?”, would the appropriate response be “Powerful”?Candidate 2: Suppose I inform you that the process of substance freezing is deeply influenced by Hydrogen bonds that generate an enormous force. Now, if I inquire, “Whatoccurs when the substance undergoes this process?”, would it be accurate to say that the force generated is “powerful”?Candidate 3: Suppose I inform you that when a substance freezes, Hydrogen bonds result in a remarkable force, and inquire, “When a substance undergoes what, do hydrogenbonds cause a remarkable force?” Would it be accurate to respond with “robust”?\nTopics\nOriginal: What are the topics in the sentence: A bathroom with the toilet missing and the room fairly torn up.Candidate 1: What are the subjects of the sentence: A torn-up room without a toilet.Candidate 2: Which subjects are covered in the phrase “A bathroom that lacks a toilet and has a considerably damaged room”?Candidate 3:What are the subjects mentioned in the statement: A torn up room that lacks a toilet in the bathroom?\nSummary\nOriginal: Sum up the following dialogue: Gordon: Did you see my car, bro? Gordon: <file_photo> Gordon: It’s my first car ever! And I love it! :) Leo: Grats, bro! Leo: Itlooks awesome, I have to see it with my own eyes! Gordon: Are you home? Leo: Yeah Gordon: Look out of the kitchen window :) Leo: No shit :D Leo: Wait, I’m coming!Gordon: Waiting :DCandidate 1: Sum up the following dialogue: Gordon asked, “Bro, have you seen my car?” and sent a file photo. He expressed his excitement saying it´s his first ever car andhe loves it. Leo congratulated him saying it looks awesome and expressed his wish to see it in person. Gordon asked if he was home and told him to look out of the kitchenwindow. Leo was surprised and replied, “No shit :D” and said he was coming. Gordon eagerly waited for him.Candidate 2: Sum up the following dialogue: Gordon inquires, “Hey bro, have you laid eyes on my car?” Gordon shares a photograph of his first vehicle and expresses hisadoration for it with a smiley face. Leo congratulates him and expresses interest in seeing it in person. Gordon asks if Leo is home and instructs him to look out of the kitchenwindow. Leo is surprised and excited, responding with laughter and promising to come see it. Gordon waits patiently.Candidate 3: Sum up the following dialogue: Gordon asked his brother if he had seen his car and sent a photo of it. He expressed his love for it as it was his first car ever. Leocongratulated him and expressed his desire to see the car in person. Gordon asked if he was at home and told him to look out of the kitchen window. Leo was surprised and excitedand said he would be coming soon. Gordon waited for him to arrive.\nExplan\nOriginal: Question: What does a Christian do when they get what they needed? Options:... The answer is “thank god” becauseCandidate 1: Question: How should a Christian proceed after they have received what they required? Options: ... The answer is “thank god” becauseCandidate 2: Question: When a Christian receives what they needed, what actions do they take? Options:... The answer is “thank god” becauseCandidate 3: Question: What should a Christian do upon receiving what they required? Options: ... The answer is “thank god” because\nTable 8: One sample prompt and the corresponding three candidate prompts generated by GPT-3.5 for each selected dataset in\nP3.\nmodel, both time and space complexities are mainly\nO(N ×M ×log M). The reinforcement learning\n(RL) part requires O(E′×B′×T) time, where E′\nis the number of epochs specific to RL, B′is the\nbatch size in RL, and T remains the model param-\neters. The space complexity is O(T). Summing\nthese up, the total time complexity for the training\nphase becomes O(N ×M) + O(E×B×T) +\nO(N×M×log M)+ O(E′×B′×T). For space,\nit’s O(N ×M ×log M) + O(T).\nInference Phase. In the inference phase, an\noptimized prompt is generated from an original\nprompt using the MAPO technique. The time com-\nplexity here is dominated by a single feed-forward\noperation, which is O(T). There is almost negli-\ngible extra space required, making the space com-\nplexity effectively O(1) for this phase.\nWe also caclulate how long it roughly takes for\na complete training run. For a LLaMA-7B model\nrunning on four A100 80GB GPUs, SFT on a high-\nscale task (such as the News classification task with\n120,000 training data) takes about 8 hours, RL takes\nabout 12 hours, and the complete MAPO process\ntakes roughly 20 hours in total; For a Bloom-7B\n3291\nValue\nGradient Accumulation Steps 8\nWeight Decay 0.1\nLearning Rate for Actor Model 2e-5\nLearning Rate for Critic Model 1e-5\nEntropy Coefficient 0.005\nValue Loss Coefficient 0.5\nMini Batch Size 32\nPositive Lambda Coefficient 2.0\nNegative Lambda Coefficient 1.8\nGAMMA 0.99\nAdam Optimizer Epsilon 1e-5\nGAE Lambda 0.95\nMax Gradient Norm 0.5\nPPO Epochs 20\nClip Parameter 0.2\nTable 9: Hyperparameters used for MAPO in all the tasks.\nmodel under the same hardware conditions, SFT\ntakes about 5 hours, RL takes about 9 hours, and\nthe total time for MAPO takes about 14 hours; For\na GPT-J-6B model, SFT takes about 10 hours, RL\ntakes about 16 hours, and the total time for MAPO\ntakes about 26 hours.\nD Baselines\nWe compared MAPO with several State-Of-The-\nArt (SOTA) prompt optimization baselines, includ-\ning the following:\n• Finetuning (Devlin et al., 2018): Finetuning\n(few-shot) involves finetuning the entire lan-\nguage model with a classification head using\na few-shot dataset.\n• Soft Prompt (Qin and Eisner, 2021b; Li\nand Liang, 2021): Soft Prompt Tuning uti-\nlizes continuous embeddings as a variant of\nparameter-efficient transfer learning, replac-\ning discrete prompts.\n• Black-Box (Sun et al., 2022): Black-Box Tun-\ning combines discrete and soft prompts, with\nthe soft part trained using gradient descent and\nthe discrete part optimized using a gradient-\nfree tuner.\n• Autoprompt (Shin et al., 2020): Autoprompt\nincorporates discrete trigger tokens and up-\ndates prompts through iterative gradient\nsearch.\n• Manual (Brown et al., 2020; Schick and\nSchütze, 2020; Sanh et al., 2021): Manual\nprompt achieves strong performance on vari-\nous natural language understanding and natu-\nral language generation tasks without relying\non training examples.\n• In-Context (Brown et al., 2020): In-Context\nDemonstration randomly selects a training\nexample and concatenates it with the input\nquery.\n• Instructions: Self-Instruction manually cre-\nates prompts for each task following Natural\nInstructions (Wang et al., 2022b), where the\nprompt is concatenated with the inputs.\n• GrIPS (Prasad et al., 2022): GrIPS performs\nphrase-level editing on the instructions and\nselects the best one.\n• RLprompt (Deng et al., 2022): RLprompt gen-\nerates discrete prompts using a reinforcement\nlearning (RL) framework.\n• TEMPERA (Zhang et al., 2023): TEMPERA\nis a test-time prompt editing method that uses\nreinforcement learning, efficiently leverag-\ning prior knowledge and adapting to differ-\nent queries, while providing an interpretable\nprompt for each query.\n• AMA (Arora et al., 2022): AMA recursively\nreformats tasks and prompts using the LLM\nto effectively aggregate predictions across\nprompts using weak supervision.\nFor a fair assessment, we adopt the same exper-\nimental setup as in LM-BFF (Gao et al., 2020)\nand RLPrompt (Deng et al., 2022). We take 16\ntraining samples from each class in our training\ndataset for every task, making them our few-shot\ndataset. So, if we consider all the classes (Y), we\nhave a total of 16 times the number of classes as\nour training samples. Similarly, we pick 16 sam-\nples from each class to form our validation dataset.\nBesides this usual setup, we also select nrandom\nexamples from our training data. We call this our\n“in-context exemplar pool”. For consistency, we\nrepeat our experiments four times using different\nrandom seeds. Afterward, we calculate the average\nresults and note down the usual variation we see be-\ntween the results. For our language model, we’ve\nchosen to use RoBERTa large (Liu et al., 2019). We\nbase our initial guidelines on the Natural Instruc-\ntions (Mishra et al., 2021). We also ensure that the\n3292\nfirst examples we give for context are randomly\npicked from a set of 16. This set is different from\nour few-shot dataset and is also randomly picked\nfrom our main training data. By comparing MAPO\nwith these SOTA baselines, we gain insights into\nthe performance and effectiveness of MAPO in\nvarious downstream tasks.\nE Datasets\nWe utilized nine representative datasets from\nP3 (Sanh et al., 2021) to establish the warm-up\ndataset, covering question-answering, classifica-\ntion, and generation tasks. The selected datasets\nfor each task are as follows:\n• Question-Answering Task: AdverQA\n(https://huggingface.co/datasets/bigscience/\nP3/tree/main/data/adversarial_qa_\ndbidaf_question_context_answer),\nOpenQA (https://huggingface.co/\ndatasets/bigscience/P3/tree/main/data/\nopenbookqa_main_which_correct), CloseQA\n(https://huggingface.co/datasets/bigscience/\nP3/tree/main/data/sciq_Direct_Question_\nClosed_Book_).\n• Classification Task: News (https:\n//huggingface.co/datasets/bigscience/P3/\ntree/main/data/ag_news_classify), Movie\n(https://huggingface.co/datasets/bigscience/\nP3/tree/main/data/rotten_tomatoes_\nMovie_Expressed_Sentiment), QASC\n(https://huggingface.co/datasets/bigscience/\nP3/tree/main/data/qasc_is_correct_1)\n• Generation Task: Topics (https://huggingface.\nco/datasets/bigscience/P3/tree/main/data/\ncommon_gen_topics_from_the_sentence),\nSummary (https://huggingface.co/datasets/\nbigscience/P3/tree/main/data/samsum_\nSum_up_the_following_dialogue), Explan\n(https://huggingface.co/datasets/bigscience/\nP3/tree/main/data/cos_e_v1.11_generate_\nexplanation_given_text).\nWe evaluate our proposed MAPO method, along\nwith other SOTA baselines, on the following\ndatasets for validation: SST-2 (Socher et al., 2013),\nYelp Polarity (Zhang et al., 2015), MR (Pang and\nLee, 2005), CR (Hu and Liu, 2004), RTE (Da-\ngan et al., 2005; Haim et al., 2006; Giampiccolo\net al., 2007; Bentivogli et al., 2009), QNLI (Dem-\nszky et al., 2018), SNLI (Bowman et al., 2015),\nMNLI (Williams et al., 2017), MRPC (Dolan and\nBrockett, 2005). These datasets provide a compre-\nhensive evaluation of MAPO’s performance com-\npared to other baselines across a range of tasks,\nincluding sentiment analysis, text classification,\nnatural language inference, and paraphrase identifi-\ncation, etc.\nSpecifically, for Table 3, the training data aligns\nwith that used by TEMPERA (Zhang et al., 2023),\nthat is all experiments, including our own, use\nRoberta-large as the backbone for validating the\ndownstream tasks. Because the setup employs a\n“few-shot” methodology that has elaborated before,\nwe name Table 3 as “few-shot”. For Table 5 and 6,\nthere is no training data involved; the LM performs\nzero-shot inference. That means all reported results\noccur without training on the datasets in Table 5\nand 6. The purpose is to demonstrate the gener-\nalization (domain transfer) ability of our MAPO\nmethod. If one wishes to further enhance perfor-\nmance on these datasets, additional training with\nlabeled data on Table 5 and 6 becomes necessary.\nF Additional Experiments\nThe performance of the reward model. We\nplot the performance of the reward model during\nthe training process of MAPO as shown in Fig. 4.\nAs the training progresses, the reward model ex-\nhibits consistent growth and improvement. The\nconsistent increase indicates that the reward model\nis gradually becoming more proficient in down-\nstream tasks. It successfully adapts to its environ-\nment, leading to improved outcomes and higher\ntask completion rates. Therefore, it can serve as\na discriminator of the goodness of an optimized\nprompt.\nThe original capabilities maintaining ability\nof MAPO. We further analyze the original capa-\nbilities maintaining ability of MAPO. We use a\nlanguage model trained with MAPO, which has\nthe ability to optimize prompts but without losing\nits original capabilities, to modify prompts and ac-\ncomplish downstream tasks. We believe that the\nGLUE and SuperGLUE tasks are representative,\nhence we use them as pre-training tasks. How-\never, the improvements in Table 5 and 6 are not\nsignificant, possibly due to the limited scope of\nour pre-training tasks. Future work can explore\nusing a broader range of datasets for pre-training,\nwhich may lead to more significant improvements\nin various downstream tasks.\n3293\n0 5 10 15 20\nQA\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9Reward\n0 5 10 15 20\nGeneration\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n0 5 10 15 20\nClassification\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nBLOOM\nGPT-J\nLLaMA\nFigure 4: The performance of the reward model in three LLMs during the training process of MAPO.\nQ-Adve\nQ-Open\nQ-Clos\nC-News\nC-Movi\nC-QASC\nG-Topi\nG-Summ\nG-Expl\nBLOOM\n0\n20\n40\n60\n80\n100\nQ-Adve\nQ-Open\nQ-Clos\nC-News\nC-Movi\nC-QASC\nG-Topi\nG-Summ\nG-Expl\nGPT-J\n0\n10\n20\n30\n40\n50\n60Metric (%) \nQ-Adve\nQ-Open\nQ-Clos\nC-News\nC-Movi\nC-QASC\nG-Topi\nG-Summ\nG-Expl\nLLaMA\n0\n20\n40\n60\n80 20\n40\n60\n80\n100\nFigure 5: Performance of different proportion of warm-up dataset in various downstream tasks by three LLMs. Q: QA, C:\nclassification, G:generation. We only keep the first four letters of each dataset’s name in the figure.\nMoreover, for Table 3, the training and validation\ndata for SFT, RM, and RL are different from the\ndata used for generalization, although they all come\nfrom Table 3. This is because we consider GLUE\nand SuperGLUE tasks to be representative, hence\nwe use them as pre-training tasks. Theoretically, a\nmore diverse NLP dataset should be selected for\nthis part, but we happened to choose this subset.\nTo mitigate the impact on the results, we also run\nanother test with two steps: using the optimized\nprompts generated by MAPO and then using the\noriginal Roberta-Large model to make inference.\nAs shown in Table 3 (the row “MAPO-w/o g”),\nthe results do not show a significant decline, with\na t-test greater than 0.05. The use of data from\nTable 3 for generalization is merely to ensure that\nthe prompt-optimized model retains its original\ncapabilities for downstream tasks instead of data\nleakage.\nG Additional Cases\nWe list more cases whose prompts have been opti-\nmized by our proposed MAPO as shown in Table\n19. We make detailed analysis for the difference\namong LLMs as follows:\n• In SST-2, BLOOM and LLaMA both use\nphrases like “terrific flair” and “remarkable\nskill” to describe Khouri’s ability, emphasiz-\ning positive sentiment. GPT-J uses the phrase\n“tremendous artistry,” highlighting the artistic\naspect, but does not explicitly convey the pos-\nitive sentiment as strongly as BLOOM and\nLLaMA.\n• In Yelp, BLOOM and LLaMA use phrases\nlike “quality of the food is commendable” and\n“service provided is inconsistent” to provide a\nbalanced assessment. GPT-J and the original\nversion have the same wording, emphasizing\nthe hit-or-miss nature of the service.\n• In MR, BLOOM and LLaMA use phrases like\n“admirable endeavor” and “praiseworthy pur-\nsuit” to highlight the positive qualities of the\nventure. GPT-J and the original version use\nneutral language without explicitly conveying\npositive or negative sentiment.\n• In CR, BLOOM, GPT-J, and LLaMA all ex-\npress confusion or potential confusion regard-\ning the positioning of the space key on a\nphone. The wording in BLOOM and LLaMA\nsuggests that using a different key for text in-\nput is more common in phones, implying a\ndeviation from the norm.\n• In RTE, BLOOM and LLaMA emphasize the\nimpact of the situation by using phrases like\n“somber site” and “distressing sight” when de-\nscribing the washed-up marine animals. GPT-\nJ and the original version provide more neutral\ndescriptions without explicitly conveying the\nemotional aspect.\n• In QNLI, BLOOM, GPT-J, and LLaMA all\nrephrase the sentence 2, maintaining the same\noverall meaning. The variations in wording\nare mainly stylistic, with BLOOM, GPT-J,\n3294\nAdverQA\nOpenQA\nCloseQA\nNews\nMovie\nQASC\nTopics\nSummary\nExplan\nBLOOM\n0\n20\n40\n60\n80\n100Metric (%)\nAdverQA\nOpenQA\nCloseQA\nNews\nMovie\nQASC\nTopics\nSummary\nExplan\nGPT-J\n0\n10\n20\n30\n40\n50\n60\nAdverQA\nOpenQA\nCloseQA\nNews\nMovie\nQASC\nTopics\nSummary\nExplan\nLLaMA\n0\n20\n40\n60\n80\nPPO\nRRMF\nMAPO\nFigure 6: The separate effects of PPO and RRMF during the process of RL in constructing MAPO.\nTasks Dataset PPO RRMF MAPO\nBLOOM GPT-J LLaMA BLOOM GPT-J LLaMA BLOOM GPT-J LLaMA\nQA\nAdverQA 18.8 9.6 24.3 19.2 9.9 24.8 19.5 11.0 25.1\nOpenQA 26.9 20.7 16.0 27.0 20.9 15.8 27.2 21.0 16.1\nCloseQA 8.3 8.8 14.4 9.0 9.8 14.6 9.4 10.2 14.8\nClass\nNews 96.9 6.1 12.1 98.3 6.2 12.2 98.7 6.3 12.4\nMovie 93.1 53.1 82.1 93.0 53.6 82.2 93.3 53.9 82.5\nQASC 99.9 56.7 72.4 99.9 56.5 71.2 99.9 56.8 72.8\nGen\nTopics 35.9 23.1 18.9 35.2 23.2 19.1 36.2 23.4 19.5\nSummary 50.1 17.2 11.7 49.4 17.7 12 50.2 17.8 12.2\nExplan 8.3 11.5 8.4 7.2 12.4 8.7 8.9 12.9 9.1\nTable 10: The separate effect of PPO and RRMF, which demonstrate the important roles played by both PPO and RRMF in\nenhancing the performance of MAPO.\nand LLaMA using different synonyms to con-\nvey the same information.\n• In SNLI, BLOOM, GPT-J, and LLaMA\nrephrase the sentence 1 by adding additional\ndetails related to the slip and slide activity\nand the celebratory context. The variations\nin wording are mainly stylistic, enhancing the\ndescription of the baby’s experience and the\nwetness.\n• In MNLI, BLOOM, GPT-J, and LLaMA main-\ntain the same wording as the original sen-\ntence 1. The variations in wording occur in\nsentence 2, with BLOOM and GPT-J empha-\nsizing the need for interest rates to increase,\nwhile LLaMA focuses on the importance of\nboosting savings.\n• In MRPC, BLOOM, GPT-J, and LLaMA all\nmaintain the same wording as the original sen-\ntences. The variations in the rephrased sen-\ntence 1 (BLOOM and LLaMA) emphasize\nthe 15 percent drop in revenue, while GPT-J\nmaintains a more neutral tone.\nH Additional Exploratory Analy-\nsis\nWe further analyze the distribution of the top 3\nwords from the original prompts in the optimized\nprompts of different LLMs in both the QA and\nclassification tasks as shown in Fig. 8 and Fig.\n9, respectively. In the QA task, we observe min-\nimal variations when considering whether to re-\nmove the instruction. After prompt optimization,\nBLOOM has a higher proportion of words like\n“contemporary”, “french”, “Methodist”, “places”,\n“education”, “power”, and “life” compared to the\nother two models. GPT-J has a higher proportion\nof words like “church”, “time”, “order”, “early”,\nand “year”, indicating a focus on temporal and se-\nquential aspects. And LLaMA has a higher propor-\ntion of words like “earlier”, “similar”, “number”,\n“song”, and “property” compared to the other two\nmodels. In the classification task, we also observe\nminimal variations when considering whether to re-\nmove the instruction. After optimization, BLOOM\nhas a higher proportion of the word “year”, “new”\ncompared to the other two models. GPT-J has a\nhigher proportion of words like “largest”, “music”,\n“national”, “school” and “poland”. LLaMA has a\nhigher proportion of words like “increase”, “gov-\nerment”, “executive”, “medical”, “warsaw”, and\n“parliament” compared to the other two LLMs.\nThese findings strongly suggest that each LLM\nexhibits unique preferences and patterns in prompt\noptimization across different tasks. The observed\nvariations in word distribution clearly indicate the\nspecific areas of focus and the semantic nuances\nthat each LLM emphasizes during the optimization\nprocess. Additional experiments will contribute to\na more comprehensive understanding of the prompt\n3295\nBLOOM GPT-J LLaMATasks Dataset 20% 40% 60% 80% 100% 20% 40% 60% 80% 100% 20% 40% 60% 80% 100%\nQA AdverQA 17.8 18.6 18.9 19.2 19.5 9.2 9.3 9.9 11.0 11.0 22.2 23.2 25.1 25.4 26.0OpenQA 25.4 26.1 26.3 26.5 27.2 19.0 19.1 19.5 21.0 21.1 14.3 15.1 16.1 16.3 16.6CloseQA 8.0 8.6 8.8 9.1 9.4 8.4 8.6 9.4 10.2 10.3 13.0 13.8 14.8 15.2 15.4Class News 96.8 97.4 97.6 98.0 98.7 4.6 4.8 5.3 6.3 6.4 10.2 11.5 12.4 12.5 12.7Movie 89.8 90.9 91.3 92.5 93.3 51.8 52.2 52.8 53.9 53.9 79.8 80.2 82.5 82.7 83.1QASC 98.0 98.6 98.7 98.8 99.9 53.8 54.4 55.2 56.8 56.9 70.5 71.4 72.8 73.3 73.4Gen Topics 35.0 35.7 36 36.4 36.2 21.3 21.6 22.2 23.4 23.6 17.5 18.2 19.5 19.8 19.9Summary 47.9 48.9 49.3 49.7 50.2 16.0 16.3 16.8 17.8 18.1 10.1 11.3 12.2 12.4 12.8Explan 7.5 8.1 8.3 8.6 8.9 11.2 11.4 11.9 12.9 13.2 7.3 8.2 9.1 9.5 10.0\nAverage - 47.4 48.1 48.4 48.8 49.3 21.7 22.0 22.6 23.7 23.8 27.2 28.1 29.4 29.7 30.0↓ - 1.9 1.2 0.9 0.5 - 2.1 1.9 1.3 0.1 - 2.8 1.9 0.6 0.3 -↓(%) - 4.0 2.4 1.9 1.0 - 9.8 8.5 5.7 0.6 - 10.2 6.7 2.0 1 -\nDataset - 55822 111645 167468 223291 279113 55822 111645 167468 223291 279113 55822 111645 167468 223291 279113D-↓ - 223291 167468 111645 55822- 223291 167468 111645 55822- 223291 167468 111645 55822-D-↓(%) - 400.0 150.0 66.7 25.0 - 400.0 150.0 66.7 25.0 - 400.0 150.0 66.7 25.0 -\nTable 11: Performance of different proportion of warm-up dataset in various downstream tasks by three LLMs. Q:\nQA, C: classification, G:generation. ↓means the number of performance decline. ↓(%) means the percentage of\nperformance decline. D-↓ means the number of data reduction. D-↓(%) means the percentage of data reduction.\nSST-2 Yelp P. MR CR RTE QNLI SNLI MNLI MRPC Average\nMAPO 96.1 93.5 90.2 88.9 75.3 63.1 60 55.7 79.3 78.0\nMAPO-0 95.8 93.1 90.2 88.2 74.9 62.9 59.7 55.1 78.9 77.6\nMAPO-0.2 95.9 93.3 90.3 88 75.2 63 59.8 54.8 78.3 77.6\nMAPO-0.5 95.2 92.2 88.9 87.9 74.8 62.8 59.2 55 78.8 77.2\nMAPO-0.8 95.3 92.3 88.7 87.9 74.5 62.7 59.2 54.9 78.7 77.1\nTable 12: The few-shot performance of MAPO with SOTA prompt optimizing baselines in downstream tasks.\nF: Finetuning, C: Continous prompt, D: Discrete prompt. MAPO means using MAPO with temperature [0,0.5].\nMAPO-0 means using MAPO with temperature 0.\nTask Natural Instructions\nSST-2 In this task, you are given sentences from movie reviews. The task is to classify the sentiment of the sentence. Your answer must be\nin the form of the letters “positive”, and “negative” respectively.\nYelp In this task, you are given sentences from Yelp reviews. The task is to classify the sentiment of the sentence. Your answer must be\nin the form of the letters “positive”, or “negative” respectively.\nMR In this task, you are given sentences from movie reviews. The task is to classify the sentiment of the sentence. Your answer must be\nin the form of the letters “positive”, or “negative” respectively.\nCR In this task, you are given sentences from customer reviews. The task is to classify the sentiment of the sentence. Your answer must be\nin the form of the letters “positive”, or “negative” respectively.’\nRTE In this task, you’re given a pair of sentences, sentence 1 and sentence 2. Your job is to choose whether the two sentences clearly agree\n(entailment)/disagree (not entailment) with each other. Your answer must be in the form of the letters Yes, and No respectively.\nQNLI You are given two sentences(Sentence1 and Sentence2). The task is to determine whether Sentence2 contains the answer to Sentence1.\nYour answer must be in the form of the letters Yes, and No respectively.\nSNLI\nIn this task, you’re given a pair of sentences, sentence 1 and sentence 2. Your job is to choose whether the two sentences clearly agree\n(entailment)/disagree (contradiction) with each other, or if this cannot be determined (neutral). Your answer must be in the form of the\nletters “Yes”, “No”, and “Maybe” respectively.\nMNLI\nIn this task, you’re given a pair of sentences, sentence 1 and sentence 2. Your job is to choose whether the two sentences clearly agree\n(entailment)/disagree (contradiction) with each other, or if this cannot be determined (neutral). Your answer must be in the form of the\nletters Yes, No, and Maybe respectively.\nMRPCIn this task, you’re given a pair of sentences, sentence 1 and sentence 2. Your job is to choose whether the two sentences clearly agree\n(entailment)/disagree (not entailment) with each other. Your answer must be in the form of the letters Yes, and No respectively.\nTable 13: Natural Instructions of various downstream tasks.\noptimization dynamics exhibited by each LLM.\n3296\nTask Dataset M\nBLOOM (↑) ( ↑(%)) GPT-J (↑) ( ↑(%)) LLaMA (↑) ( ↑(%)) ↑ ↑ (%)\nQA AdverQA 19.5 6 44.4 11 8 266.7 25.1 21.9 684.4 12 331.8\nOpenQA 27.2 1.3 5 21 4 23.5 16.1 2.8 21.1 2.7 16.5\nCloseQA 9.4 3 46.9 10.2 3.3 47.8 14.8 4 37 3.4 43.9\nCLS News 98.7 5.9 6.4 6.3 6.3 - 12.4 11.3 1027.3 7.8 516.9\nMovie 93.3 2.4 2.6 53.9 2.8 5.5 82.5 3.8 4.8 3 4.3\nQASC 99.9 0.5 0.5 56.8 2.8 5.2 72.8 11.2 18.2 4.8 8\nGEN Topics 36.2 6.7 22.7 23.4 5.9 33.7 19.5 5.2 36.4 5.9 30.9\nSummary 50.2 4.1 8.9 17.8 4.7 35.9 12.2 5.6 84.8 4.8 43.2\nExplan 8.9 3.2 56.1 12.9 4.4 51.8 9.1 2.2 31.9 3.3 46.6\nTask Dataset M-0\nBLOOM (↑) ( ↑(%)) GPT-J (↑) ( ↑(%)) LLaMA (↑) ( ↑(%)) ↑ ↑ (%)\nQA AdverQA 19.3 5.8 43 10.5 7.5 250 24.8 21.6 675 11.6 322.7\nOpenQA 26.8 0.9 3.5 20.9 3.9 22.9 15.2 1.9 14.3 2.2 13.6\nCloseQA 8.9 2.5 39.1 9.8 2.9 42 14 3.2 29.6 2.9 36.9\nCLS News 96.5 3.7 4 5.9 5.9 - 11.9 10.8 981.8 6.8 492.9\nMovie 92.8 1.9 2.1 53.6 2.5 4.9 82 3.3 4.2 2.6 3.7\nQASC 99.9 0.5 0.5 56.6 2.6 4.8 72.2 10.6 17.2 4.6 7.5\nGEN Topics 35.9 6.4 21.7 23 5.5 31.4 19 4.7 32.9 5.5 28.7\nSummary 49.7 3.6 7.8 17 3.9 29.8 11.9 5.3 80.3 4.3 39.3\nExplan 7.8 2.1 36.8 12.3 3.8 44.7 8.8 1.9 27.5 2.6 36.3\nTask Dataset M-0.2\nBLOOM (↑) ( ↑(%)) GPT-J (↑) ( ↑(%)) LLaMA (↑) ( ↑(%)) ↑ ↑ (%)\nQA AdverQA 19 5.5 40.7 10.7 7.7 256.7 24.4 21.2 662.5 11.5 320\nOpenQA 26.9 1 3.9 20.1 3.1 18.2 15.6 2.3 17.3 2.1 13.1\nCloseQA 8.4 2 31.3 10 3.1 44.9 14.2 3.4 31.5 2.8 35.9\nCLS News 96.7 3.9 4.2 6.1 6.1 - 12.1 11 1000 7 502.1\nMovie 92.9 2 2.2 53.2 2.1 4.1 81.5 2.8 3.6 2.3 3.3\nQASC 99.9 0.5 0.5 56.6 2.6 4.8 72.3 10.7 17.4 4.6 7.6\nGEN Topics 36 6.5 22 23.3 5.8 33.1 19.3 5 35 5.8 30\nSummary 50.2 4.1 8.9 16.6 3.5 26.7 12 5.4 81.8 4.3 39.1\nExplan 8.2 2.5 43.9 12.5 4 47.1 9 2.1 30.4 2.9 40.5\nTask Dataset M-0.5\nBLOOM (↑) ( ↑(%)) GPT-J (↑) ( ↑(%)) LLaMA (↑) ( ↑(%)) ↑ ↑ (%)\nQA AdverQA 18.8 5.3 39.3 10 7 233.3 23.6 20.4 637.5 10.9 303.4\nOpenQA 27.1 1.2 4.6 20.4 3.4 20 15.8 2.5 18.8 2.4 14.5\nCloseQA 8.3 1.9 29.7 8.5 1.6 23.2 14.2 3.4 31.5 2.3 28.1\nCLS News 96.2 3.4 3.7 5.7 5.7 - 10.3 9.2 836.4 6.1 420.1\nMovie 93.2 2.3 2.5 53 1.9 3.7 81.6 2.9 3.7 2.4 3.3\nQASC 99.9 0.5 0.5 56.5 2.5 4.6 70.6 9 14.6 4 6.6\nGEN Topics 35.5 6 20.3 21.9 4.4 25.1 18.9 4.6 32.2 5 25.9\nSummary 49.2 3.1 6.7 17 3.9 29.8 11.1 4.5 68.2 3.8 34.9\nExplan 7.1 1.4 24.6 11.1 2.6 30.6 8.5 1.6 23.2 1.9 26.1\nTask Dataset M-0.8\nBLOOM (↑) ( ↑(%)) GPT-J (↑) ( ↑(%)) LLaMA (↑) ( ↑(%)) ↑ ↑ (%)\nQA AdverQA 18.3 4.8 35.6 9.8 6.8 226.7 23.3 20.1 628.1 10.6 296.8\nOpenQA 26.7 0.8 3.1 20.6 3.6 21.2 15.6 2.3 17.3 2.2 13.9\nCloseQA 7.9 1.5 23.4 8.6 1.7 24.6 14.1 3.3 30.6 2.2 26.2\nCLS News 95.9 3.1 3.3 5.8 5.8 - 10.2 9.1 827.3 6 415.3\nMovie 92.8 1.9 2.1 52.9 1.8 3.5 81.5 2.8 3.6 2.2 3.1\nQASC 99.9 0.5 0.5 56.6 2.6 4.8 70.4 8.8 14.3 4 6.5\nGEN Topics 35.1 5.6 19 21.9 4.4 25.1 18.7 4.4 30.8 4.8 25\nSummary 50.1 4 8.7 16.8 3.7 28.2 10.9 4.3 65.2 4 34\nExplan 7.1 1.4 24.6 10.9 2.4 28.2 8.2 1.3 18.8 1.7 23.9\nTable 14: (↑) denotes the absolute performance increase achieved using MAPO-optimized prompts versus a frozen LLM,\nwhile (↑(%)) highlights the relative performance boost. Symbols ↑ and ↑(%) represent the average absolute and relative\nenhancements across all three LLMs, respectively. These enhancements pertain to specific downstream tasks, with “CLS”\nsignifying classification and “GEN” indicating generation tasks. M, M-0.2, M-0.5, and M-0.8 correspond to using MAPO with\ntemperature settings of [0,0.5], 0, 0.2, 0.5, and 0.8, respectively.\n3297\n0.00 0.02 0.04 0.06 0.08 0.10 0.12\n(a)\ntopics\nsentence\ncity\nhome\nbathroom\nplace\nview\nmatch\nboat\nwhite\nlake\nroom\nplayer\ngrass\nWord\nOriginal\nBLOOM\nGPT-J\nLLaMA\n0.000 0.002 0.004 0.006 0.008 0.010\n(b)\ncity\nhome\nbathroom\nplace\nview\nmatch\nboat\nwhite\nlake\nroom\nplayer\ngrass\nclose\nchair\nplaying\nflowers\nbeach\nsky\nnear\nbridge\nman\nbuilding\nfinal\nwooden\nfloor\nOriginal\nBLOOM\nGPT-J\nLLaMA\nFigure 7: The distribution of three most frequent words, which extracted from the original prompt, in the optimized prompt\namong different LLMs in the generation task. (a) retaining frequent words in the instruction, (b) removing frequent words in the\ninstruction.\n3298\n0.000 0.002 0.004 0.006 0.008 0.010 0.012 0.014\n(a)\nhappened\nchurch\nfollowing\npeople\nmentionednew\nth\ntime\nearlier\nmuseum\ncontemporarysimilar\nfrench\nnumber\nsong\nasia\nmethodist\nplaces\norder\nproperty\neducation\ncity\ncentury\nearly\nstudents\npower\nyear\nlife\nWord\nOriginal\nBLOOM\nGPT-J\nLLaMA\n0.000 0.002 0.004 0.006 0.008 0.010 0.012 0.014\n(b)\nhappened\nchurch\npeople\nnew\nth\ntime\nearlier\nmuseum\ncontemporarysimilar\nfrench\nnumber\nsong\nasia\nmethodist\nplaces\norder\nproperty\neducation\ncity\ncentury\nearly\nstudents\npower\nyear\nlife\nOriginal\nBLOOM\nGPT-J\nLLaMA\nFigure 8: The distribution of three most frequent words, which extracted from the original prompt, in the optimized prompt\namong different LLMs in the QA task. (a) retaining frequent words in the instruction, (b) removing frequent words in the\ninstruction.\n3299\n0.000 0.002 0.004 0.006 0.008 0.010 0.012\n(a)\nuniversity\nyear\nlargest\nmusic\nnew\nnational\nknow\nuh\nincrease\ngovernment\nexecutive\nmedical\nwarsaw\nschool\npoland\nparliament\nWord\nOriginal\nBLOOM\nGPT-J\nLLaMA\n0.000 0.002 0.004 0.006 0.008 0.010 0.012\n(b)\nuniversity\nyear\nlargest\nmusic\nnew\nnational\nknow\nuh\nincrease\ngovernment\nexecutive\nmedical\nwarsaw\nschool\npoland\nparliament\nOriginal\nBLOOM\nGPT-J\nLLaMA\nFigure 9: The distribution of three most frequent words, which extracted from the original prompt, in the optimized prompt\namong different LLMs in the classification task. (a) retaining frequent words in the instruction, (b) removing frequent words in\nthe instruction.\n3300\nTask Dataset BLOOM M↑ ↑ ↑(%)↑(%) GPT-J M (↑) ↑ (↑(%))↑(%)Coref. xwinograd 60.1 60.6 0.5 0.5 0.9 0.9 - - - - --\nNLU\nBoolQ 67.9 68.2 0.3\n0.6\n0.4\n0.9\n67.2 67.9 0.7 0.3 1 0.5CB 77.6 78.1 0.5 0.6 83.9 84.2 0.3 0.4COPA 74.0 75.0 1.0 1.4 84 84.2 0.2 0.2MultiRC 59.7 60.4 0.7 1.2 63.8 64.1 0.3 0.5ReCoRD 69.8 70.2 0.4 0.6 74.4 74.7 0.3 0.4WiC 61.4 62.0 0.6 1.0 61 61.3 0.3 0.5WSC 64.4 65.1 0.7 1.1 77.9 78.1 0.2 0.3\nNLIANLI R1 31.5 32.1 0.60.51.9 1.3 37.8 38.2 0.4 0.3 1.1 0.7ANLI R2 35.1 35.4 0.3 0.9 37.9 38.3 0.4 1.1ANLI R3 37.1 37.8 0.7 1.9 40.9 41.1 0.2 0.5StoryCloze 79.0 79.5 0.5 0.6 87.8 87.9 0.1 0.1CLSAmazon 65.2 67.7 2.52.33.8 3.3 68.2 69.4 1.2 1.2 1.8 1.6DBPedia 70.5 72.5 2.0 2.8 83.9 85.1 1.2 1.4\nQADROP 67.9 69.9 2.01.82.9 5.6 51.6 52.8 1.2 1 2.3 3.2NQ 15.1 16.1 1.0 6.6 19.6 20.8 1.2 6.1RealTimeQA 29.0 31.5 2.5 8.6 36 37.2 1.2 3.3WebQs 34.8 36.3 1.5 4.3 44.1 44.6 0.5 1.1Task Dataset BLOOM M-0↑ ↑ ↑(%)↑(%) GPT-J M-0(↑) ↑ (↑(%))↑(%)Coref. xwinograd 60.1 60.5 0.4 0.4 0.7 0.7 - - - - --\nNLU\nBoolQ 67.9 68.0 0.1\n0.5\n0.1\n0.7\n67.2 67.8 0.6 0.3 0.9 0.4CB 77.6 78.0 0.4 0.5 83.9 84.1 0.2 0.2COPA 74.0 74.8 0.8 1.1 84 84.2 0.2 0.2MultiRC 59.7 60.4 0.7 1.2 63.8 64.1 0.3 0.5ReCoRD 69.8 70.2 0.4 0.6 74.4 74.7 0.3 0.4WiC 61.4 61.8 0.4 0.7 61 61.3 0.3 0.5WSC 64.4 65.0 0.6 0.9 77.9 78.1 0.2 0.3\nNLIANLI R1 31.5 32.0 0.50.51.6 1.2 37.8 38.2 0.4 0.2 1.1 0.6ANLI R2 35.1 35.4 0.3 0.9 37.9 38.1 0.2 0.5ANLI R3 37.1 37.8 0.7 1.9 40.9 41.1 0.2 0.5StoryCloze 79.0 79.4 0.4 0.5 87.8 87.9 0.1 0.1CLSAmazon 65.2 67.3 2.11.83.2 2.7 68.2 69 0.8 1 1.2 1.3DBPedia 70.5 72.0 1.5 2.1 83.9 85.1 1.2 1.4\nQADROP 67.9 69.5 1.61.52.4 4.6 51.6 52.3 0.7 0.7 1.4 2.3NQ 15.1 15.8 0.7 4.6 19.6 20.4 0.8 4.1RealTimeQA 29.0 31.2 2.2 7.6 36 37 1 2.8WebQs 34.8 36.1 1.3 3.7 44.1 44.5 0.4 0.9Task Dataset BLOOM M-0.2↑ ↑ ↑(%)↑(%) GPT-J M-0.2 (↑) ↑ (↑(%))↑(%)Coref. xwinograd 60.1 60.6 0.5 0.5 0.8 0.8 - - - - --\nNLU\nBoolQ 67.9 68.1 0.2\n0.5\n0.3\n0.7\n67.2 67.9 0.7 0.3 1 0.4CB 77.6 78.1 0.5 0.6 83.9 84.2 0.3 0.4COPA 74.0 74.6 0.6 0.8 84 84.2 0.2 0.2MultiRC 59.7 60.3 0.6 1.0 63.8 64 0.2 0.3ReCoRD 69.8 70.2 0.4 0.6 74.4 74.6 0.2 0.3WiC 61.4 61.7 0.3 0.5 61 61.3 0.3 0.5WSC 64.4 65.1 0.7 1.1 77.9 78.1 0.2 0.3\nNLIANLI R1 31.5 32.2 0.70.52.2 1.3 37.8 38.2 0.4 0.3 1.1 0.7ANLI R2 35.1 35.5 0.4 1.1 37.9 38.3 0.4 1.1ANLI R3 37.1 37.6 0.5 1.3 40.9 41.1 0.2 0.5StoryCloze 79.0 79.5 0.5 0.6 87.8 87.9 0.1 0.1CLSAmazon 65.2 67.1 1.91.62.9 2.4 68.2 69.1 0.9 1 1.3 1.3DBPedia 70.5 71.8 1.3 1.8 83.9 85 1.1 1.3\nQADROP 67.9 69.3 1.41.22.1 3.8 51.6 52.6 1 0.7 1.9 2.1NQ 15.1 15.6 0.5 3.3 19.6 20.3 0.7 3.6RealTimeQA 29.0 31.0 2.0 6.9 36 36.7 0.7 1.9WebQs 34.8 35.8 1.0 2.9 44.1 44.5 0.4 0.9Task Dataset BLOOM M-0.5↑ ↑ ↑(%)↑(%) GPT-J M-0.5 (↑) ↑ (↑(%))↑(%)Coref. xwinograd 60.1 60.4 0.3 0.3 0.5 0.5 - - - - --\nNLU\nBoolQ 67.9 68.0 0.1\n0.3\n0.1\n0.5\n67.2 67.6 0.4 0.2 0.6 0.2CB 77.6 77.8 0.2 0.3 83.9 84.1 0.2 0.2COPA 74.0 74.4 0.4 0.5 84 84.2 0.2 0.2MultiRC 59.7 60.3 0.6 1.0 63.8 63.9 0.1 0.2ReCoRD 69.8 70.1 0.3 0.4 74.4 74.5 0.1 0.1WiC 61.4 61.6 0.2 0.3 61 61.2 0.2 0.3WSC 64.4 64.8 0.4 0.6 77.9 78 0.1 0.1\nNLIANLI R1 31.5 32.0 0.50.31.6 0.9 37.8 38.2 0.4 0.2 1.1 0.5ANLI R2 35.1 35.3 0.2 0.6 37.9 38.1 0.2 0.5ANLI R3 37.1 37.5 0.4 1.1 40.9 41 0.1 0.2StoryCloze 79.0 79.3 0.3 0.4 87.8 87.9 0.1 0.1CLSAmazon 65.2 66.7 1.51.22.3 1.8 68.2 68.9 0.7 0.5 1 0.8DBPedia 70.5 71.4 0.9 1.3 83.9 84.3 0.4 0.5\nQADROP 67.9 68.5 0.60.80.9 2.7 51.6 52.2 0.6 0.5 1.2 1.7NQ 15.1 15.5 0.4 2.6 19.6 20.2 0.6 3.1RealTimeQA 29.0 30.6 1.6 5.5 36 36.6 0.6 1.7WebQs 34.8 35.4 0.6 1.7 44.1 44.4 0.3 0.7Task Dataset BLOOM M-0.8↑ ↑ ↑(%)↑(%) GPT-J M-0.8 (↑) ↑ (↑(%))↑(%)Coref. xwinograd 60.1 60.2 0.1 0.1 0.2 0.2 - - - - --\nNLU\nBoolQ 67.9 68.0 0.1\n0.3\n0.1\n0.4\n67.2 67.5 0.3 0.2 0.4 0.2CB 77.6 77.8 0.2 0.3 83.9 84.1 0.2 0.2COPA 74.0 74.3 0.3 0.4 84 84.2 0.2 0.2MultiRC 59.7 60.3 0.6 1.0 63.8 63.9 0.1 0.2ReCoRD 69.8 70.1 0.3 0.4 74.4 74.5 0.1 0.1WiC 61.4 61.6 0.2 0.3 61 61.2 0.2 0.3WSC 64.4 64.7 0.3 0.5 77.9 78 0.1 0.1\nNLIANLI R1 31.5 31.8 0.30.31.0 0.8 37.8 38.1 0.3 0.2 0.8 0.4ANLI R2 35.1 35.3 0.2 0.6 37.9 38 0.1 0.3ANLI R3 37.1 37.5 0.4 1.1 40.9 41 0.1 0.2StoryCloze 79.0 79.3 0.3 0.4 87.8 87.9 0.1 0.1CLSAmazon 65.2 66.5 1.31.12.0 1.6 68.2 68.8 0.6 0.4 0.9 0.7DBPedia 70.5 71.3 0.8 1.1 83.9 84.2 0.3 0.4\nQADROP 67.9 68.3 0.40.70.6 2.5 51.6 52.1 0.5 0.4 1 1.5NQ 15.1 15.5 0.4 2.6 19.6 20.1 0.5 2.6RealTimeQA 29.0 30.5 1.5 5.2 36 36.6 0.6 1.7WebQs 34.8 35.3 0.5 1.4 44.1 44.3 0.2 0.5\nTable 15: Zero-shot domain transfer performance based\non BLOOM and GPT-J with original and MAPO-\nOptimized prompts. CLS: Classification, M: MAPO.\nThe (↑ ) and ↑represent the absolute improvement scores\nof MAPO-optimized prompts compared with original\nprompts in each dataset and task, respectively. The\n(↑(%) ) and ↑(%) represent the relative improvement\npercentages of MAPO-optimized prompts compared\nwith original prompts in each dataset and task, respec-\ntively.\nTask Dataset LLaMA M (↑) ↑ (↑(%))↑(%)\nRS BoolQ 76.5 76.7 0.2 0.3 0.3 0.5PIQA 79.8 80 0.2 0.3SIQA 48.9 49 0.1 0.2HellaSwag 76.1 76.5 0.4 0.5WinoGrande 70.1 70.5 0.4 0.6ARC-e 72.8 73.2 0.4 0.5ARC-c 47.6 47.8 0.2 0.4OBQA 57.2 57.9 0.7 1.2QA NQ 16.8 18.1 1.3 1.1 7.7 4.7RACE 50 50.8 0.8 1.6\nTask Dataset LLaMA M-0 (↑) ↑ (↑(%))↑(%)\nRS BoolQ 76.5 76.7 0.2 0.3 0.3 0.4PIQA 79.8 80 0.2 0.3SIQA 48.9 49 0.1 0.2HellaSwag 76.1 76.4 0.3 0.4WinoGrande 70.1 70.4 0.3 0.4ARC-e 72.8 73.2 0.4 0.5ARC-c 47.6 47.8 0.2 0.4OBQA 57.2 57.8 0.6 1QA NQ 16.8 17.8 1 0.8 6 3.6RACE 50 50.6 0.6 1.2\nTask Dataset LLaMA M-0.2 (↑) ↑ (↑(%))↑(%)\nRS BoolQ 76.5 76.7 0.2 0.3 0.3 0.4PIQA 79.8 80 0.2 0.3SIQA 48.9 49 0.1 0.2HellaSwag 76.1 76.4 0.3 0.4WinoGrande 70.1 70.4 0.3 0.4ARC-e 72.8 73.1 0.3 0.4ARC-c 47.6 47.8 0.2 0.4OBQA 57.2 57.7 0.5 0.9QA NQ 16.8 18 1.2 1 7.1 4.3RACE 50 50.7 0.7 1.4\nTask Dataset LLaMA M-0.5 (↑) ↑ (↑(%))↑(%)\nRS BoolQ 76.5 76.7 0.2 0.2 0.3 0.3PIQA 79.8 79.9 0.1 0.1SIQA 48.9 49 0.1 0.2HellaSwag 76.1 76.4 0.3 0.4WinoGrande 70.1 70.4 0.3 0.4ARC-e 72.8 73 0.2 0.3ARC-c 47.6 47.7 0.1 0.2OBQA 57.2 57.6 0.4 0.7QA NQ 16.8 17.7 0.9 0.7 5.4 3.2RACE 50 50.5 0.5 1\nTask Dataset LLaMA M-0.8 (↑) ↑ (↑(%))↑(%)\nRS BoolQ 76.5 76.6 0.1 0.2 0.1 0.2PIQA 79.8 79.9 0.1 0.1SIQA 48.9 49 0.1 0.2HellaSwag 76.1 76.3 0.2 0.3WinoGrande 70.1 70.4 0.3 0.4ARC-e 72.8 73 0.2 0.3ARC-c 47.6 47.6 0 0OBQA 57.2 57.5 0.3 0.5QA NQ 16.8 17.6 0.8 0.5 4.8 2.7RACE 50 50.3 0.3 0.6\nTable 16: Domain transfer performance with a frozen\nLLM for inference based on LLaMA with original and\nMAPO-Optimized prompts. CLS: Classification, M:\nMAPO. The (↑ ) and ↑represent the absolute improve-\nment scores of MAPO-optimized prompts compared\nwith original prompts in each dataset and task, respec-\ntively. The (↑(%) ) and ↑(%) represent the relative im-\nprovement percentages of MAPO-optimized prompts\ncompared with original prompts in each dataset and task,\nrespectively.\n3301\nTasks Dataset 20% 40% 60% 80% 100%\n(BLOOM) +SFT +MAPO +SFT +MAPO +SFT +MAPO +SFT +MAPO +SFT +MAPO\nQA AdverQA 14.6 17.8 16.9 18.6 17.5 18.9 17.7 19.2 18.3 19.5\nOpenQA 22.1 25.4 24.3 26.1 24.9 26.3 25.1 26.5 26.7 27.2\nCloseQA 6.6 8.0 7.7 8.6 7.3 8.8 7.8 9.1 7.8 9.4\nClass News 93.6 96.8 95.8 97.4 95.5 97.6 96.2 98.0 95.5 98.7\nMovie 87.1 89.8 88.3 90.9 88.9 91.3 90.7 92.5 92.6 93.3\nQASC 96.0 98.0 97.6 98.6 97.1 98.7 97.4 98.8 99.9 99.9\nGen Topics 32.7 35.0 33.6 35.7 34.6 36.0 34.7 36.4 34.8 36.2\nSummary 45.8 47.9 47.1 48.9 47.3 49.3 48.5 49.7 48.8 50.2\nExplan 6.2 7.5 7.3 8.1 7.1 8.3 6.3 8.6 6.8 8.9\nAverage - 45.0 47.4 46.5 48.1 46.7 48.4 47.2 48.8 47.9 49.3\n↑ - - 2.4 - 1.6 - 1.7 - 1.6 - 1.4\n↑(%) - - 5.3 - 3.4 - 3.6 - 3.4 - 2.9\nTasks Dataset 20% 40% 60% 80% 100%\n(GPT-J) +SFT +MAPO +SFT +MAPO +SFT +MAPO +SFT +MAPO +SFT +MAPO\nQA AdverQA 6.8 9.2 7.7 9.3 7.9 9.9 9.4 11.0 9.9 11.0\nOpenQA 17.3 19.0 17.3 19.1 18.2 19.5 20.3 21.0 19.8 21.1\nCloseQA 7.1 8.4 7.3 8.6 8.2 9.4 8.4 10.2 9.6 10.3\nClass News 2.4 4.6 2.8 4.8 3.9 5.3 5.5 6.3 5.5 6.4\nMovie 49.2 51.8 50.7 52.2 51.7 52.8 52.7 53.9 51.8 53.9\nQASC 50.3 53.8 52.8 54.4 53.6 55.2 56.3 56.8 55.2 56.9\nGen Topics 19.5 21.3 20.0 21.6 20.8 22.2 21.6 23.4 21.8 23.6\nSummary 13.8 16.0 14.7 16.3 15.7 16.8 16.7 17.8 17.1 18.1\nExplan 8.5 11.2 9.9 11.4 10.1 11.9 10.7 12.9 11.8 13.2\nAverage - 19.4 21.7 20.4 22.0 21.1 22.6 22.4 23.7 22.5 23.8\n↑ - - 2.3 - 1.6 - 1.5 - 1.3 - 1.3\n↑(%) - - 11.9 - 7.8 - 7.1 - 5.8 - 5.8\nTasks Dataset 20% 40% 60% 80% 100%\n(LLaMA) +SFT +MAPO +SFT +MAPO +SFT +MAPO +SFT +MAPO +SFT +MAPO\nQA AdverQA 19.6 22.2 22.0 23.2 23.2 25.1 24.5 25.4 24.4 26.0\nOpenQA 12.8 14.3 13.5 15.1 15.4 16.1 14.9 16.3 15.9 16.6\nCloseQA 11.5 13.0 12.2 13.8 13.9 14.8 14.5 15.2 14.8 15.4\nClass News 8.8 10.2 9.8 11.5 10.1 12.4 11.0 12.5 10.8 12.7\nMovie 77.5 79.8 79.0 80.2 81.3 82.5 80.6 82.7 81.5 83.1\nQASC 67.3 70.5 68.3 71.4 70.2 72.8 71.2 73.3 71.6 73.4\nGen Topics 16.1 17.5 17.2 18.2 18.6 19.5 18.3 19.8 18.0 19.9\nSummary 8.2 10.1 9.4 11.3 10.7 12.2 10.5 12.4 11.6 12.8\nExplan 5.6 7.3 7.6 8.2 8.2 9.1 8.8 9.5 8.8 10.0\nAverage - 25.3 27.2 26.6 28.1 28.0 29.4 28.3 29.7 28.6 30.0\n↑ - - 1.9 - 1.5 - 1.4 - 1.4 - 1.4\n↑(%) - - 7.5 - 5.6 - 5.0 - 4.9 - 4.9\nTable 17: Performance of different proportion of warm-up dataset in various downstream tasks by three LLMs. Q:\nQA, C: classification, G:generation. SFT means using SFT-optimized prompts without RL.\n3302\nTask Dataset Best Performance Best Epoch Epoch\n- - - - 1 5 10 15 20 50\nQA AdverQA 18.3 18 14.8 14.3 15.4 18.2 18.3 18.2\nOpenQA 26.7 14 26.0 26.0 26.1 26.7 26.7 26.6\nCloseQA 7.8 19 6.8 7.0 7.0 7.5 7.8 7.8\nCLS News 95.5 20 93.2 93.7 94.3 95.3 95.5 95.7\nMovie 92.6 15 91.4 91.7 91.3 92.6 92.4 92.5\nQASC 99.9 7 99.6 99.8 99.9 99.9 99.9 99.9\nGEN Topics 34.8 19 30.3 31.5 33.8 34.2 34.8 34.8\nSummary 48.8 18 46.1 46.4 47.5 48.4 48.8 48.9\nExplan 6.8 15 5.9 6.3 6.4 6.8 6.5 6.7\nTask Dataset Best Performance Best Epoch Epoch\n- - - - 1 5 10 15 20 50\nQA AdverQA 9.4 20 4.4 5.3 6.7 8.8 9.4 9.6\nOpenQA 20.3 15 17.5 18.1 18.8 20.3 20.2 20.0\nCloseQA 8.4 20 7.0 7.3 7.8 8.2 8.4 8.6\nCLS News 5.5 20 1.4 2.6 3.1 4.8 5.5 5.9\nMovie 52.7 15 51.1 51.6 52.6 52.7 52.4 52.7\nQASC 56.3 19 54.3 54.6 54.5 55.4 56.3 56.3\nGEN Topics 21.6 18 17.9 20.2 20.7 21.3 21.6 21.6\nSummary 16.7 20 13.2 14.5 15.2 16.3 16.7 17.1\nExplan 10.7 19 8.8 9.2 9.9 9.6 10.7 11.0\nTask Dataset Best Performance Best Epoch Epoch\n- - - - 1 5 10 15 20 50\nQA AdverQA 23.2 20 5.8 9.3 13.3 18.7 23.2 29.5\nOpenQA 15.4 15 13.6 14.1 14.6 15.4 15.3 16.8\nCloseQA 13.9 20 10.8 11.3 12.9 13.5 13.9 14.5\nCLS News 10.1 13 3.4 5.7 9.3 10.1 10.1 10.1\nMovie 81.3 19 79.2 79.6 80.5 81.1 81.3 81.2\nQASC 70.2 20 62.8 63.7 65.9 69.2 70.2 71.9\nGEN Topics 18.6 20 15.5 16.1 17.7 18.4 18.6 19.3\nSummary 10.7 20 6.6 7.1 8.6 9.4 10.7 11.5\nExplan 8.2 14 7.3 7.5 7.5 8.2 8.2 8.3\nTable 18: Performance of different number of epochs when training SFT. Best Performance means the best\nperformance within 20 epochs. Best Epoch means the epoch corresponding to the best performance. We list\nthe performance in the 1,5,10,15,20,50 epochs. We bold the performance metrics where a longer training epoch\n(epoch=50) results in a decline in performance.\n3303\nTask Prompts\nSST-2\nOriginal: The sentence is “khouri manages , with terrific flair, to keep the extremes of screwball farce and blood-curdling family intensity onone continuum. ”.BLOOM: The sentence is “khouri manages , with terrific flair, to keep the extremes of screwball farce and blood-curdling family intensity onone continuum. ”.GPT-J: The sentence is “With tremendous artistry, Khouri expertly maintains a fluid continuum between the uproarious comedy of screwballfarce and the blood-curdling intensity of family dynamics.”.LLaMA: The sentence is “With remarkable skill, Khouri successfully maintains a seamless connection between the wild hilarity of screwballfarce and the spine-chilling intensity of family dynamics.”.\nYelp\nOriginal: The sentence is “The food is good. Unfortunately the service is very hit or miss. The main issue seems to be with the kitchen, thewaiters and waitresses are often very apologetic for the long waits and it’s pretty obvious that some of them avoid the tables after taking the initialorder to avoid hearing complaints.”.BLOOM: The sentence is “The quality of the food is commendable. However, the service provided is inconsistent. The primary concern appearsto be with the kitchen, as the waitstaff frequently apologize for lengthy waits, and it’s apparent that some of them intentionally avoid tables aftertaking initial orders to prevent complaints.”.GPT-J: The sentence is “The food is good. Unfortunately the service is very hit or miss. The main issue seems to be with the kitchen, the waitersand waitresses are often very apologetic for the long waits and it’s pretty obvious that some of them avoid the tables after taking the initial order toavoid hearing complaints.”.LLaMA: The sentence is “The quality of the food is commendable. However, the service provided is inconsistent. The primary concern appearsto be with the kitchen, as the waitstaff frequently apologize for lengthy waits, and it’s apparent that some of them intentionally avoid tables aftertaking initial orders to prevent complaints.”.\nMR\nOriginal: The sentence is “a respectable venture on its own terms , lacking the broader vision that has seen certain trek films . . . cross over toa more mainstream audience .”.BLOOM: The sentence is “An admirable endeavor in its own right, although it lacks the wider perspective that propelled certain Star Trek filmsto cross over into a more mainstream audience.”.GPT-J: The sentence is “a respectable venture on its own terms , lacking the broader vision that has seen certain trek films . . . cross over to amore mainstream audience .”.LLaMA: The sentence is “A praiseworthy pursuit on its own grounds, though it falls short of the broader vision that allowed certain Star Trekfilms to captivate a more mainstream audience.”.\nCR\nOriginal: The sentence is “the fact that the 0 key is the space key for text input is a bit confusing , as many phones use the key instead”.BLOOM: The sentence is “It can be a little confusing that the space key for text input is the 0 key, considering that many phones utilize a differentkey for that purpose.”.GPT-J: The sentence is “the fact that the 0 key is the space key for text input is a bit confusing , as many phones use the key instead”.LLaMA: The sentence is “It might cause confusion that the space key for text input is assigned to the 0 key, whereas many phones utilize a differentkey for the same purpose.”.\nRTE\nOriginal: The sentence 1 is “More than 150 dolphins, marine turtles and beaked whales have been washed up dead on beaches in Africa.”. Thesentence 2 is “Dead dolphins, turtles and whales have been found on African beaches.”.BLOOM: The sentence 1 is “African beaches have become a somber site with the washing up of over 150 deceased dolphins, marine turtles, andbeaked whales.”. The sentence 2 is “African beaches have become a distressing sight as dead dolphins, turtles, and whales have been uncovered.”.GPT-J: The sentence 1 is “The number of deceased dolphins, marine turtles, and beaked whales found on African beaches exceeds 150.”. Thesentence 2 is “There have been reports of deceased dolphins, turtles, and whales on African beaches.”.LLaMA: The sentence 1 is “More than 150 dolphins, marine turtles and beaked whales have been washed up dead on beaches in Africa.”. Thesentence 2 is “Dead dolphins, turtles and whales have been found on African beaches.”.\nQNLI\nOriginal: The sentence 1 is “What came into force after the new constitution was herald?”. The sentence 2 is “As of that day, the new constitutionheralding the Second Republic came into force.”.BLOOM: The sentence 1 is “What started being in effect as a result of the heralding of the new constitution?”. The sentence 2 is “The newconstitution heralding the Second Republic started being in effect as of that day.”.GPT-J: The sentence 1 is “What commenced after the new constitution was heralded?”. The sentence 2 is “The new constitution heralding theSecond Republic came into force as of that day.”.LLaMA: The sentence 1 is “What became valid subsequent to the heralding of the new constitution?”. The sentence 2 is “ The new constitutionheralding the Second Republic started being in force on that day.”.\nSNLI\nOriginal: The sentence 1 is “A baby at the end of a slip and slide at a party”. The sentence 2 is “The baby is wet”.BLOOM: The sentence 1 is “At a party, a baby reaches the conclusion of their slip and slide adventure.”. The sentence 2 is “The little one isdripping wet.”.GPT-J: The sentence 1 is “A baby sliding to the end of a slip and slide at a celebration.”. The sentence 2 is “The baby’s clothes are moistened.”.LLaMA: The sentence 1 is “At the end of a festive occasion, a baby arrives at the final stretch of a slip and slide.”. The sentence 2 is “The babyis entirely drenched.”.\nMNLI\nOriginal: The sentence 1 is “Conversely, an increase in government saving adds to the supply of resources available for investment and mayput downward pressure on interest rates.”. The sentence 2 is “Interest rates should increase to increase saving.”.BLOOM: The sentence 1 is “Conversely, an increase in government saving adds to the supply of resources available for investment and mayput downward pressure on interest rates.”. The sentence 2 is “Interest rates should increase to increase saving.”.GPT-J: The sentence 1 is “Conversely, when the government saves more, it adds to the pool of resources for investment and may exert downwardpressure on interest rates.”. The sentence 2 is “The augmentation of interest rates is crucial for increasing saving.”.LLaMA: The sentence 1 is “On the flip side, an increase in government saving enhances the resources accessible for investment and mayexert downward pressure on interest rates.”. The sentence 2 is “The escalation of interest rates is essential for boosting savings.”.\nMRPC\nOriginal: The sentence 1 is “Revenue in the first quarter of the year dropped 15 percent from the same period a year earlier.”. The sentence 2 is“With the scandal hanging over Stewart’s company, revenue the first quarter of the year dropped 15 percent from the same period a year earlier.”.BLOOM: The sentence 1 is “The first quarter of the year saw a 15 percent drop in revenue compared to the corresponding period a year earlier.”.The sentence 2 is “The scandal’s impact on Stewart’s company led to a 15 percent drop in revenue during the first quarter, as compared to the sameperiod in the prior year.”GPT-J: The sentence 1 is “There was a 15 percent decline in revenue during the first quarter of the year compared to the same period a year ago.”.The sentence 2 is “Due to the scandal looming over Stewart’s company, revenue in the first quarter of the year witnessed a 15 percent declinecompared to the same period last year.”.LLaMA: The sentence 1 is “There was a 15 percent reduction in revenue during the first quarter of the year compared to the same period in theprior year.”. The sentence 2 is “With the scandal overshadowing Stewart’s company, revenue for the first quarter of the year dipped by 15 percentcompared to the corresponding period in the previous year.”.\nTable 19: Original prompts and MAPO-optimized prompts for different LLMs in more downstream tasks. Each prompt is start\nwith the corresponding instruction as shown in Table 13. We omit instructions in the following Table due to space limits.\n3304",
  "topic": "Leverage (statistics)",
  "concepts": [
    {
      "name": "Leverage (statistics)",
      "score": 0.7965693473815918
    },
    {
      "name": "Computer science",
      "score": 0.7666913270950317
    },
    {
      "name": "Downstream (manufacturing)",
      "score": 0.7037829160690308
    },
    {
      "name": "Boosting (machine learning)",
      "score": 0.6960993409156799
    },
    {
      "name": "Language model",
      "score": 0.5980673432350159
    },
    {
      "name": "Artificial intelligence",
      "score": 0.463385671377182
    },
    {
      "name": "Machine learning",
      "score": 0.42989957332611084
    },
    {
      "name": "Natural language processing",
      "score": 0.32318735122680664
    },
    {
      "name": "Engineering",
      "score": 0.10426995158195496
    },
    {
      "name": "Operations management",
      "score": 0.0
    }
  ]
}