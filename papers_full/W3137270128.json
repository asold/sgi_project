{
  "title": "TALE: Transformer-based protein function Annotation with joint sequence–Label Embedding",
  "url": "https://openalex.org/W3137270128",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2104077081",
      "name": "Yue Cao",
      "affiliations": [
        "Texas A&M University"
      ]
    },
    {
      "id": "https://openalex.org/A2097850282",
      "name": "Yang Shen",
      "affiliations": [
        "Texas A&M University"
      ]
    },
    {
      "id": "https://openalex.org/A2104077081",
      "name": "Yue Cao",
      "affiliations": [
        "Texas A&M University"
      ]
    },
    {
      "id": "https://openalex.org/A2097850282",
      "name": "Yang Shen",
      "affiliations": [
        "Texas A&M University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2103017472",
    "https://openalex.org/W6741504686",
    "https://openalex.org/W2045204781",
    "https://openalex.org/W2098432760",
    "https://openalex.org/W2952218450",
    "https://openalex.org/W3047995329",
    "https://openalex.org/W2963409789",
    "https://openalex.org/W2951422523",
    "https://openalex.org/W2227395312",
    "https://openalex.org/W1538463803",
    "https://openalex.org/W4236358448",
    "https://openalex.org/W2615066396",
    "https://openalex.org/W2074231493",
    "https://openalex.org/W2117486996",
    "https://openalex.org/W6762647395",
    "https://openalex.org/W2107057369",
    "https://openalex.org/W6728998201",
    "https://openalex.org/W3136918052",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3007643904",
    "https://openalex.org/W1987134040",
    "https://openalex.org/W2807567459",
    "https://openalex.org/W2951731136",
    "https://openalex.org/W2951282333",
    "https://openalex.org/W2935275081",
    "https://openalex.org/W2989608901",
    "https://openalex.org/W2537623931",
    "https://openalex.org/W2945297971",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3146944767",
    "https://openalex.org/W2920665410",
    "https://openalex.org/W2966590054",
    "https://openalex.org/W2966866763",
    "https://openalex.org/W2740706416",
    "https://openalex.org/W2963403868"
  ],
  "abstract": "Abstract Motivation Facing the increasing gap between high-throughput sequence data and limited functional insights, computational protein function annotation provides a high-throughput alternative to experimental approaches. However, current methods can have limited applicability while relying on protein data besides sequences, or lack generalizability to novel sequences, species and functions. Results To overcome aforementioned barriers in applicability and generalizability, we propose a novel deep learning model using only sequence information for proteins, named Transformer-based protein function Annotation through joint sequence–Label Embedding (TALE). For generalizability to novel sequences we use self-attention-based transformers to capture global patterns in sequences. For generalizability to unseen or rarely seen functions (tail labels), we embed protein function labels (hierarchical GO terms on directed graphs) together with inputs/features (1D sequences) in a joint latent space. Combining TALE and a sequence similarity-based method, TALE+ outperformed competing methods when only sequence input is available. It even outperformed a state-of-the-art method using network information besides sequence, in two of the three gene ontologies. Furthermore, TALE and TALE+ showed superior generalizability to proteins of low similarity, new species, or rarely annotated functions compared to training data, revealing deep insights into the protein sequence–function relationship. Ablation studies elucidated contributions of algorithmic components toward the accuracy and the generalizability; and a GO term-centric analysis was also provided. Availability and implementation The data, source codes and models are available at https://github.com/Shen-Lab/TALE. Supplementary information Supplementary data are available at Bioinformatics online.",
  "full_text": null,
  "concepts": [
    {
      "name": "Embedding",
      "score": 0.6601250767707825
    },
    {
      "name": "Annotation",
      "score": 0.6467838287353516
    },
    {
      "name": "Computer science",
      "score": 0.6294810175895691
    },
    {
      "name": "Joint (building)",
      "score": 0.5745460391044617
    },
    {
      "name": "Transformer",
      "score": 0.5432433485984802
    },
    {
      "name": "Sequence (biology)",
      "score": 0.5238280892372131
    },
    {
      "name": "Sequence alignment",
      "score": 0.42882731556892395
    },
    {
      "name": "Artificial intelligence",
      "score": 0.42626094818115234
    },
    {
      "name": "Computational biology",
      "score": 0.42555439472198486
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3731192946434021
    },
    {
      "name": "Peptide sequence",
      "score": 0.28275424242019653
    },
    {
      "name": "Biology",
      "score": 0.21877923607826233
    },
    {
      "name": "Genetics",
      "score": 0.1569189727306366
    },
    {
      "name": "Engineering",
      "score": 0.09577542543411255
    },
    {
      "name": "Gene",
      "score": 0.09104481339454651
    },
    {
      "name": "Architectural engineering",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "topic": "Embedding"
}