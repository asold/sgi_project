{
  "title": "UPDeT: Universal Multi-agent Reinforcement Learning via Policy Decoupling with Transformers",
  "url": "https://openalex.org/W3123636359",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2277842189",
      "name": "Hu Si-Yi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2351882683",
      "name": "Zhu, Fengda",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1988184271",
      "name": "Chang, Xiaojun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2353260061",
      "name": "Liang, Xiaodan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3039208705",
    "https://openalex.org/W2096001037",
    "https://openalex.org/W2951984055",
    "https://openalex.org/W2895675617",
    "https://openalex.org/W2970272688",
    "https://openalex.org/W2982316857",
    "https://openalex.org/W2121092017",
    "https://openalex.org/W2949600457",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2413794162",
    "https://openalex.org/W2785315072",
    "https://openalex.org/W2145339207",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2921955147",
    "https://openalex.org/W2756196406",
    "https://openalex.org/W3093287223",
    "https://openalex.org/W2097381042",
    "https://openalex.org/W3094349299",
    "https://openalex.org/W2174786457",
    "https://openalex.org/W1457482454",
    "https://openalex.org/W2626637010",
    "https://openalex.org/W1924770834",
    "https://openalex.org/W110451278",
    "https://openalex.org/W1641379095",
    "https://openalex.org/W2970514967",
    "https://openalex.org/W3033481391",
    "https://openalex.org/W2946606218",
    "https://openalex.org/W2963890729",
    "https://openalex.org/W2342840547",
    "https://openalex.org/W2617547828",
    "https://openalex.org/W2747213132",
    "https://openalex.org/W2771201900",
    "https://openalex.org/W2997536466",
    "https://openalex.org/W2951682727"
  ],
  "abstract": "Recent advances in multi-agent reinforcement learning have been largely limited in training one model from scratch for every new task. The limitation is due to the restricted model architecture related to fixed input and output dimensions. This hinders the experience accumulation and transfer of the learned agent over tasks with diverse levels of difficulty (e.g. 3 vs 3 or 5 vs 6 multi-agent games). In this paper, we make the first attempt to explore a universal multi-agent reinforcement learning pipeline, designing one single architecture to fit tasks with the requirement of different observation and action configurations. Unlike previous RNN-based models, we utilize a transformer-based model to generate a flexible policy by decoupling the policy distribution from the intertwined input observation with an importance weight measured by the merits of the self-attention mechanism. Compared to a standard transformer block, the proposed model, named as Universal Policy Decoupling Transformer (UPDeT), further relaxes the action restriction and makes the multi-agent task's decision process more explainable. UPDeT is general enough to be plugged into any multi-agent reinforcement learning pipeline and equip them with strong generalization abilities that enables the handling of multiple tasks at a time. Extensive experiments on large-scale SMAC multi-agent competitive games demonstrate that the proposed UPDeT-based multi-agent reinforcement learning achieves significant results relative to state-of-the-art approaches, demonstrating advantageous transfer capability in terms of both performance and training speed (10 times faster).",
  "full_text": "Published as a conference paper at ICLR 2021\nUPD ET: UNIVERSAL MULTI -AGENT REINFORCEMENT\nLEARNING VIA POLICY DECOUPLING WITH TRANS -\nFORMERS\nSiyi Hu1, Fengda Zhu1, Xiaojun Chang1âˆ—\n, Xiaodan Liang2,3\n1Monash University, 2Sun Yat-sen University,3Dark Matter AI Inc.\n{siyi.hu,fengda.zhu}@monash.edu {cxj273,xdliang328}@gmail.com\nABSTRACT\nRecent advances in multi-agent reinforcement learning have been largely limited\ntraining one model from scratch for every new task. This limitation occurs due\nto the restriction of the model architecture related to ï¬xed input and output di-\nmensions, which hinder the experience accumulation and transfer of the learned\nagent over tasks across diverse levels of difï¬culty (e.g. 3 vs 3 or 5 vs 6 multi-\nagent games). In this paper, we make the ï¬rst attempt to explore a universal\nmulti-agent reinforcement learning pipeline, designing a single architecture to ï¬t\ntasks with different observation and action conï¬guration requirements. Unlike\nprevious RNN-based models, we utilize a transformer-based model to generate\na ï¬‚exible policy by decoupling the policy distribution from the intertwined in-\nput observation, using an importance weight determined with the aid of the self-\nattention mechanism. Compared to a standard transformer block, the proposed\nmodel, which we name Universal Policy Decoupling Transformer (UPDeT), fur-\nther relaxes the action restriction and makes the multi-agent taskâ€™s decision pro-\ncess more explainable. UPDeT is general enough to be plugged into any multi-\nagent reinforcement learning pipeline and equip it with strong generalization abil-\nities that enable multiple tasks to be handled at a time. Extensive experiments on\nlarge-scale SMAC multi-agent competitive games demonstrate that the proposed\nUPDeT-based multi-agent reinforcement learning achieves signiï¬cant improve-\nments relative to state-of-the-art approaches, demonstrating advantageous transfer\ncapability in terms of both performance and training speed (10 times faster). Code\nis available at https://github.com/hhhusiyi-monash/UPDeT\n1 I NTRODUCTION\nReinforcement Learning (RL) provides a framework for decision-making problems in an interactive\nenvironment, with applications including robotics control (Hester et al. (2010)), video gaming (Mnih\net al. (2015)), auto-driving (Bojarski et al. (2016)), person search (Chang et al. (2018)) and vision-\nlanguage navigation (Zhu et al. (2020)). Cooperative multi-agent reinforcement learning (MARL), a\nlong-standing problem in the RL context, involves organizing multiple agents to achieve a goal, and\nis thus a key tool used to address many real-world problems, such as mastering multi-player video\ngames (Peng et al. (2017)) and studying population dynamics (Yang et al. (2017)).\nA number of methods have been proposed that exploit an action-value function to learn a multi-\nagent model (Sunehag et al. (2017), Rashid et al. (2018), Du et al. (2019), Mahajan et al. (2019),\nHostallero et al. (2019), Zhou et al. (2020), Yang et al. (2020)). However, current methods have poor\nrepresentation learning ability and fail to exploit the common structure underlying the tasks this is\nbecause they tend to treat observation from different entities in the environment as an integral part\nof the whole. Accordingly, they give tacit support to the assumption that neural networks are able to\nautomatically decouple the observation to ï¬nd the best mapping between the whole observation and\npolicy. Adopting this approach means that they treat all information from other agents or different\nparts of the environment in the same way. The most commonly used method involves concatenating\nâˆ—Corresponding author.\n1\narXiv:2101.08001v3  [cs.LG]  7 Feb 2021\nPublished as a conference paper at ICLR 2021\nAgent\nT-1 ally\nN enemy\nğ‘œğ‘¡\n1\nIndividual F\nCredit Assignment G\nğ‘„ğ‘˜(ğ‘œ1,ğ‘ğ‘¡\n1)\nIndividual F\nAgent 1 Agent T\nğ‘œğ‘¡\nğ‘‡\nâ€¦\nğ‘„ğ‘˜(ğ‘œğ‘‡,ğ‘ğ‘¡\nğ‘‡)\nğ‘„ğ‘¡ğ‘œğ‘¡(ğ‘œ,ğ‘)\nFeed Forward\nBack Propagation Loss Function\nGRU/LSTM\nğ‘ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘ \nF\nours\nTransformer\nk\nv\na\nğ‘ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘ \naction A\naction B\nâ€¦\naction E\naction F\nâ€¦\nâ€¦\naction Y\naction Z\nâ€¦\nâ€¦\nâ€¦\naction I\naction J\nâ€¦\nFigure 1: An overview of the MARL framework. Our work replaces the widely used GRU/LSTM-\nbased individual value function with a transformer-based function. Actions are separated into action\ngroups according to observations.\nthe observations from each entity in to a vector that is used as input (Rashid et al. (2018), Du et al.\n(2019), Zhou et al. (2020)). In addition, current methods ignore the rich physical meanings behind\neach action. Multi-agent tasks feature a close relationship between the observation and output. If\nthe model does not decouple the observation from the different agents, individual functions maybe\nmisguided and impede the centralized value function. Worse yet, conventional models require the\ninput and the output dimensions to be ï¬xed (Shao et al. (2018), Wang et al. (2020)), which makes\nzero-shot transfer impossible. Thus, the application of current methods is limited in real-world\napplications.\nOur solution to these problems is to develop a multi-agent reinforcement learning (MARL) frame-\nwork with no limitation on input or output dimension. Moreover, this model should be general\nenough to be applicable to any existing MARL methods. More importantly, the model should be\nexplainable and capable of providing further improvement for both the ï¬nal performance on single-\ntask scenarios and transfer capability on multi-task scenarios.\nInspired by the self-attention mechanism (Vaswani et al. (2017)), we propose a transformer-based\nMARL framework, named Universal Policy Decoupling Transformer (UPDeT). There are four key\nadvantages of this approach: 1) Once trained, it can be universally deployed; 2) it provide more\nrobust representation with a policy decoupling strategy; 3) it is more explainable; 4) it is general\nenough to be applied on any MARL model. We further design a transformer-based function to\nhandle various observation sizes by treating individual observations as â€observation-entitiesâ€. We\nmatch the related observation-entity with action-groups by separating the action space into several\naction-groups with reference to the corresponding observation-entity, allowing us to get matched\nobservation-entity â€” action-group pairs set. We further use a self-attention mechanism to learn the\nrelationship between the matched observation-entity and other observation-entities. Through the\nuse of self-attention map and the embedding of each observation-entity, UPDeT can optimize the\npolicy at an action-group level. We refer to this strategy as Policy Decoupling. By combining the\ntransformer and policy decoupling strategies, UPDeT signiï¬cantly outperforms conventional RNN-\nbased models.\nIn UPDeT, there is no need to introduce any new parameters for new tasks. We also prove that it\nis only with decoupled policy and matched observation-entity â€” action-group pairs that UPDeT\ncan learn a strong representation with high transfer capability. Finally, our proposed UPDeT can be\nplugged into any existing method with almost no changes to the framework architecture required,\nwhile still bringing signiï¬cant improvements to the ï¬nal performance, especially in hard and com-\nplex multi-agent tasks.\nThe main contributions of this work are as follows: First, our UPDeT-based MARL framework\noutperforms RNN-based frameworks by a large margin in terms of ï¬nal performance on state-of-\n2\nPublished as a conference paper at ICLR 2021\nTransformer Transformer Transformer\nAR\nMA\nEXP\nğ‘œğ‘¡\nğ‘›\nğ‘’ğ‘¡\nğ‘›\nğ‘ğ‘¡\nğ‘š\nTransformer Transformer\nâ„ğ‘¡\nğº\nâ„ğ‘¡\nğ‘›\n(a) Vanilla (b) Aggregation (c) Universal\n(d) Universal Global (e) Universal Individual\nFigure 2: Three variants on different policy decoupling method types (upper part) and two variants\non different temporal unit types (bottom). â€˜ARâ€™ , â€˜MAâ€™ and â€˜EXPâ€™ represent Action Restriction,\nMulti-task at A time and EXPlainable, respectively.o, e, qand hrepresents for observation, embed-\nding, q-value and hidden states withnobservation entities and mavailable actions. Grepresents for\nthe global hidden state and tis the current time step. A black circle indicates that the variant pos-\nsesses this attribute; moreover, variant (d) is our proposed UPDeT with best performance. Further\ndetails on all ï¬ve variants can be found in Section 3.\nthe-art centralized functions. Second, our model has strong transfer capability and can handle a\nnumber of different tasks at a time. Third, our model accelerates the transfer learning speed (total\nsteps cost) to make it roughly 10 times faster compared to RNN-based models in most scenarios.\n2 R ELATED WORK\nAttention mechanisms have become an integral part of models that capture global dependencies.\nIn particular, self-attention (Parikh et al. (2016)) calculates the response at a speciï¬c position in a\nsequence by attending to all positions within this sequence. Vaswani et al. (2017) demonstrated\nthat machine translation models can achieve state-of-the-art results solely by using a self-attention\nmodel. Parmar et al. (2018) proposed an Image Transformer model that applies self-attention to\nimage generation. Wang et al. (2018) formalized self-attention as a non-local operation in order to\nmodel the spatial-temporal dependencies in video sequences. In spite of this, self-attention mecha-\nnisms have not yet been fully explored in multi-agent reinforcement learning.\nAnother line of research is multi-agent reinforcement learning (MARL). Existing work in MARL\nfocuses primarily on building a centralized function to guide the training of individual value function\n(Lowe et al. (2017), Sunehag et al. (2017), Rashid et al. (2018), Mahajan et al. (2019), Hostallero\net al. (2019), Yang et al. (2020), Zhou et al. (2020)). Few works have opted to form a better individ-\nual functions with strong representation and transfer capability. In standard reinforcement learning,\nthis generalization has been fully studied (Taylor & Stone (2009), Ammar et al. (2012), Parisotto\net al. (2015), Gupta et al. (2017), Da Silva & Costa (2019)). While multi-agent transfer learning has\nbeen proven to be more difï¬cult than the single-agent scenario (Boutsioukis et al. (2011), Shao et al.\n(2018), Vinyals et al. (2019)). However, the transfer capability of a multi-agent system is of greater\nsigniï¬cance due to the various number of agents, observations sizes and policy distributions.\nTo the best of our knowledge, we are the ï¬rst to develop a multi-agent framework capable of handling\nmultiple task at a time. Moreover, we provide a policy decoupling strategy to further improve\nthe model performance and facilitate the multi-agent transfer learning, which is a signiï¬cant step\ntowards real world multi-agent applications.\n3\nPublished as a conference paper at ICLR 2021\n3 M ETHOD\nWe begin by introducing the notations and basic task settings necessary for our approach. We then\ndescribe a transformer-based individual function and policy decoupling strategy under MARL. Fi-\nnally, we introduce different temporal units and assimilate our Universal Policy Decoupling Trans-\nformer (UPDeT) into Dec-POMDP.\n3.1 N OTATIONS AND TASK SETTINGS\nMulti-agent Reinforcement Learning A cooperative multi-agent task is a decentralized par-\ntially observable Markov decision process (Oliehoek et al. (2016)) with a tuple G =\nâŸ¨S,A,U,P,r,Z,O,n,Î³ âŸ©. Let S denote the global state of the environment, while A represents\nthe set of nagents and U is the action space. At each time step t, agent a âˆˆA â‰¡{1,...,n }se-\nlects an action u âˆˆU, forming a joint action u âˆˆU â‰¡Un, which in turn causes a transition in\nthe environment represented by the state transition function P(sâ€²|s,u) : SÃ—U Ã—S â†’[0,1]. All\nagents share the same reward function r(s,u) : SÃ—U â†’R, while Î³ âˆˆ[0,1) is a discount fac-\ntor. We consider a partially observable scenario in which each agent makes individual observations\nz âˆˆZ according to the observation function O(s,a) : S Ã—A â†’Z. Each agent has an action-\nobservation history that conditions a stochastic policy Ï€t, creating the following joint action value:\nQÏ€(st,ut) =Est+1:âˆ,ut+1:âˆ[Rt|st,ut], where Rt = âˆ‘âˆ\ni=0Î³irt+i is the discounted return.\nCentralized training with decentralized executionCentralized training with decentralized execu-\ntion (CTDE) is a commonly used architecture in the MARL context. Each agent is conditioned only\non its own action-observation history to make a decision using the learned policy. The centralized\nvalue function provides a centralized gradient to update the individual function based on its output.\nTherefore, a stronger individual value function can beneï¬t the centralized training.\n3.2 T RANSFORMER -BASED INDIVIDUAL VALUE FUNCTION\nIn this section, we present a mathematical formulation of our transformer-based model UPDeT. We\ndescribe the calculation of the global Q-function with self-attention mechanism. First, the obser-\nvation O is embedded into a semantic embedding to handle the various observation space. For\nexample, if an agent ai observes kother entities {oi,1,...,o i,k}at time step t, all observation entities\nare embedded via an embedding layer Eas follows:\net\ni = {E(ot\ni,1),...,E (ot\ni,k)}. (1)\nHere, iis the index of the agent, i âˆˆ{1,...,n }. Next, the value functions {Q1,...,Q n}for the n\nagents for each step are estimated as follows:\nqt\ni = Qi(htâˆ’1\ni ,et\ni,ut). (2)\nWe introduce htâˆ’1\ni , the temporal hidden state at the last time step tâˆ’1, since POMDP policy is\nhighly dependent on the historical information. et\ni denotes the observation embedding, while ut\ni is\nthe candidate action, ut\ni âˆˆU. Î¸i is the parameter that deï¬nes Qi. Finally, the global Q-function QÏ€\nis calculated by all individual value functions, as follows:\nQÏ€(st,ut) =F(qt\n1,..,q t\nn) (3)\nFi is the credit assignment function for deï¬ned by Ï†i for each agent ai, as utilized in Rashid et al.\n(2018) and Sunehag et al. (2017). For example, in VDN, F is a sum function that can be expressed\nas F(qt\n1,..,q t\nn) =âˆ‘n\ni=1 qt\ni.\nImplement Q-function with Self-attentionVaswani et al. (2017) adopts three matrices, K, Q, V\nrepresenting a set of keys, queries and values respectively. The attention is computed as follows:\nAttention(Q,K,V) = softmax(QKT\nâˆšdk\n)V, (4)\nwhere dk is a scaling factor equal to the dimension of the key. In our method, we adopt the self-\nattention to learn the features and relationships from the observation entity embedding and the global\ntemporal information. To learn the independent policy in decentralized multi-agent learning, we\n4\nPublished as a conference paper at ICLR 2021\nTransformer\nğ‘œğ´,1\nğ‘œğµ,1 â€¦\nâ€¦\nğ‘œğ´,ğ‘—â€¦ â€¦ğ‘œğµ,ğ‘˜\nğ‘œğ‘,1 ğ‘œğ‘,ğ‘™\nğ‘1\nğ‘3\nğ‘2\nğ‘4\nğ‘ğ‘¥\nğ‘ğ‘¤\nğ‘ğ‘§\nğ‘ğ‘¦\nğ‘ğ´\nğ‘ğ´ ğ‘ğµ\nğ‘ğ¶\nğ‘ğµ\nğ‘ğ¶\nğ‘œğ´,1 ğ‘œğ´,ğ‘—â€¦ ğ‘œğµ,1 â€¦ â€¦ğ‘œğµ,ğ‘˜ ğ‘œğ‘,1 ğ‘œğ‘,ğ‘™\nâ€¦ğ‘’ğ´,1 ğ‘’ğ´,ğ‘—â€¦ ğ‘’ğµ,1 â€¦ â€¦ğ‘’ğµ,ğ‘˜ ğ‘’ğ‘,1 ğ‘’ğ‘,ğ‘™\nğ‘’ğ´,1\n1 ğ‘’ğ´,1\nğ‘š ğ‘’ğ´,ğ‘—\n1 ğ‘’ğ´,ğ‘—\nğ‘šâ€¦ â€¦ ğ‘’ğµ,1\n1 ğ‘’ğµ,1\nğ‘› ğ‘’ğµ,ğ‘˜\n1 ğ‘’ğµ,ğ‘˜\nğ‘›â€¦ â€¦ â€¦â€¦ ğ‘’ğ‘,1\n1 ğ‘’ğ‘,1\nğ‘ ğ‘’ğ‘,ğ‘™\n1 ğ‘’ğ‘,ğ‘™\nğ‘\nâ€¦ â€¦â€¦â€¦\nFC FC FC\nPreserve\nğ‘ğ´,1\n1 ğ‘ğ´,1\nğ‘š ğ‘ğ´,ğ‘—\n1 ğ‘ğ´,ğ‘—\nğ‘šâ€¦ â€¦ ğ‘ğµ,1 ğ‘ğµ,ğ‘˜â€¦ â€¦ â€¦â€¦\nAggregation Abandon\nPolicy Distribution\nğ‘ğ´ ğ‘ğ´\nâ€¦\nObservation | Action-Group Pair\ndifferent\nobservation\ngroup\naction space\nFigure 3: The main pipeline of our proposed UPDeT, where o,e,q represent observation entity,\nfeature embedding and Q-value of each action respectively. Three operations are adopted to avoid\nintroducing new parameters when forming the policy distribution, namely â€˜preserveâ€™, â€˜aggregationâ€™\nand â€˜abandonâ€™. Details can be found in Section 3.3 and a real case can be found in Fig. 7.\ndeï¬ne Ki, Qi and Vi as the key, query and value metrics for each agent ai. We further consider the\nquery, key and value for the same matricesRl\ni = Ki = Qi = Vi, where lâˆˆ{1,...,L }is the number\nof layers of the transformer. Thus, we formulate our transformer as follows:\nR1\ni = {htâˆ’1\ni ,et\ni}\nQl\ni,Kl\ni,V l\ni = LFQ,K,V(Rl\ni)\nRl+1\ni = Attention(Ql\ni,Kl\ni,V l\ni).\n(5)\nwhere LF represents the linear functions used to compute K, Q, V. Finally we project the entity\nfeatures of the last transformer layerRL\ni to the output space of the value functionQi. We implement\nthe projection using a linear function P:\nQi(htâˆ’1\ni ,et\ni,ui) =P(RL\ni ,ui). (6)\n3.3 P OLICY DECOUPLING\nA single transformer-based individual function with self-attention mechanism is still unable to han-\ndle various required policy distribution. A ï¬‚exible mapping function P in Eq. 6 is needed to deal\nwith the various input and output dimensions and provide strong representation ability. Using the\ncorrelation between input and output, we design a strategy called policy decoupling, which is the\nkey part of UPDeT.\nThe main idea behind the policy decoupling strategy can be summarized into three points:\nâ€¢ Point 1 : No restriction on policy dimension. The output dimension of a standard trans-\nformer block must be equal to or less than the input dimension. This is unacceptable in\nsome MARL tasks, as the action number can be larger than the entity number.\nâ€¢ Point 2 : Ability to handle multiple tasks at a time. This requires a ï¬xed model architecture\nwithout new parameters being introduced for new tasks. Unfortunately, if point 1 is satis-\nï¬ed, point 2 becomes very problematic to achieve. The difï¬culty lies in how to reconcile\npoints 1 and 2 .\nâ€¢ Point 3 : Make the model more explainable. It would be preferable if we can could replace\nthe conventional RNN-based model with a more explainable policy generation structure.\nFollowing the above three points, we propose three policy decoupling methods, namely Vanilla\nTransformer, Aggregation Transformer and Universal Policy Decoupling Transformer (UPDeT).\nThe pipelines are illustrated in Fig. 2. The details of the Vanilla Transformerand Aggregation\nTransformer are presented in the experiment section and act as our baselines. In this section, we\nmainly discuss the mechanism of our proposed UPDeT.\n5\nPublished as a conference paper at ICLR 2021\nTasking the entity features of the last transformer layer outlined in Eq. 5, the main challenge is to\nbuild a strong mapping between the features and the policy distribution. UPDeT ï¬rst matches the in-\nput entity with the related output policy part. This correspondence is easy to ï¬nd in the MARL task,\nas interactive action between two agents is quite common. Once we match the corresponding en-\ntity features and actions, we substantially reduce the burden of model learning representation using\nthe self-attention mechanism. Moreover, considering that there might be more than one interactive\nactions of the matched entity feature, we separate the action space into several action groups, each\nof which consists several actions matched with one entity. The pipeline of this process is illustrated\nin the left part of Fig. 3. In the mapping function, to satisfy point 1 and point 2 , we adopt two\nstrategies. First, if the action-group of one entity feature contains more than one action, a shared\nfully connected layer is added to map the output to the action number dimension. Second, if one en-\ntity feature has no corresponding action, we abandon it, there is no danger of losing the information\ncarried by this kind of entity feature, as the transformer has aggregated the information necessary to\neach output. The pipeline of UPDeT can be found in the right part of Fig. 3. With UPDeT, there\nis no action restriction and no new parameter introduced in new scenarios. A single model can be\ntrained on multiple tasks and deployed universally. In addition, matching the corresponding entity\nfeature and action-group satisï¬es point 3 , as the policy is explainable using an attention heatmap,\nas we will discuss in Section 4.4.\n3.4 T EMPORAL UNIT STRUCTURE\nNotably, however a transformer-based individual value function with policy decoupling strategy\ncannot handle a partial observation decision process without trajectory or history information. In\nDec-POMDP (Oliehoek et al. (2016)), each agent a chooses an action according to Ï€a(ua|Ï„a),\nwhere uand Ï„ represents for action and action-observation history respectively. In GRU and LSTM,\nwe adopt a hidden state to hold the information of the action-observation history. However, the\ncombination of a transformer block and a hidden state has not yet been fully studied. In this section,\nwe provide two approaches to handling the hidden state in UPDeT:\n1) Global temporal unit treats the hidden state as an additional input of the transformer block. The\nprocess is formulated in a similar way to Eq. 5 with the relation: R1 = {htâˆ’1\nG ,et\n1}and {ht\nG,et\nL}=\nRL. Here, we ignore the subscript iand instead use Gto represent â€™globalâ€™. The global temporal\nunit is simple but efï¬cient, and provides us with robust performance in most scenarios.\n2) Individual temporal unit treats the hidden state as the inner part of each entity. In other words,\neach input maintains its own hidden state, while each output projects a new hidden state for the\nnext time step. The individual temporal unit uses a more precise approach to controlling history\ninformation as it splits the global hidden state into individual parts. We use j to represent the\nnumber of entities. The relation of input and output is formulated as R1 = {htâˆ’1\n1 ...htâˆ’1\nj ,et\n1}and\n{ht\n1...ht\nj,et\nL}= RL. However, this method introduces the additional burden of learning the hidden\nstate independently for each entity. In experiment Section 4.1.2, we test both variants and discuss\nthem further.\n3.5 O PTIMIZATION\nWe use the standard squared TDerror in DQNs (Mnih et al. (2015)) to optimize our entire frame-\nwork as follows:\nL(Î¸) =\nbâˆ‘\ni=1\n[(\nyDQN\ni âˆ’Q(s,u; Î¸)\n)2]\n(7)\nHere, brepresents the batch size. In partially observable settings, agents can beneï¬t from condition-\ning on action-observation history. Hausknecht & Stone (2015) propose Deep Recurrent Q-networks\n(DRQN) for this sequential decision process. For our part, we replace the widely used GRU (Chung\net al. (2014))/LSTM (Hochreiter & Schmidhuber (1997)) unit in DRQN with a transformer-based\ntemporal unit and then train the whole model.\n6\nPublished as a conference paper at ICLR 2021\nTest Win Rate\nStep\n(a) Policy variants\nTest Win Rate\nStep (b) Temporal variants\nTest Win Rate\nStep (c) MARL methods\nTest Win Rate\nStep\n(d) Easy scenarios\nTest Win Rate\nStep (e) Hard scenarios\nTest Win Rate\nStep (f) Mismatch experiment\nFigure 4: Experimental results with different task settings. Details can be found in Section 4.1.2.\n4 S TARCRAFT II E XPERIMENT\nIn this section, we evaluate UPDeT and its variants with different policy decoupling methods in\nthe context of challenging micromanagement games in StarCraft II. We compare UPDeT with the\nRNN-based model on a single scenario and test the transfer capability on multiple-scenario transfer\ntasks. The experimental results show that UPDeT achieves signiï¬cant improvement compared to\nthe RNN-based model.\n4.1 S INGLE SCENARIO\nIn the single scenario experiments, we evaluate the model performance on different scenarios from\nSMAC (Samvelyan et al. (2019)). Speciï¬cally, the scenarios considered are as follows: 3 Marines\nvs 3 Marines (3m, Easy), 8 Marines vs 8 Marines (8m, Easy), 4 Marines vs 5 Marines (4m vs 5m,\nHard+) and 5 Marines vs 6 Marines (5m vs 6m, Hard). In all these games, only the units from\nplayerâ€™s side are treated as agents. Dead enemy units will be masked out from the action space to\nensure that the executed action is valid. More detailed settings can be acquired from the SMAC\nenvironment (Samvelyan et al. (2019)).\n4.1.1 M ETHODS AND TRAINING DETAILS\nThe MARL methods for evaluation include VDN (Sunehag et al. (2017)), QMIX (Rashid et al.\n(2018)) and QTRAN (Hostallero et al. (2019)). All three SOTA methodsâ€™ original implementation\ncan be found athttps://github.com/oxwhirl/pymarl. These methods were selected due\nto their robust performance across different multi-agent tasks. Other methods, including COMA\n(Foerster et al. (2017)) and IQL (Tan (1993)) do not perform stable across in all tasks, as have been\nproved in several recent works (Rashid et al. (2018), Mahajan et al. (2019), Zhou et al. (2020)).\nTherefore, we combined UPDeT with VDN, QMIX and QTRAN to prove that our model can im-\nprove the model performance signiï¬cantly compared to the GRU-based model.\n4.1.2 R ESULT\nThe model performance result with different policy decoupling methods can be found in Fig. 4a.\nVanilla Transformeris our baseline for all transformer-based models. This transformer only satis-\nï¬es point 2 . Each output embedding can either be projected to an action or abandoned. The vanilla\n7\nPublished as a conference paper at ICLR 2021\n7m â†’ 5m 5m â†’ 3m\nperformance \nw/o finetune\nTest Win Rate\nStep\n(a) Transfer from 7 marines to 3 marines\n3m â†’ 5m 5m â†’ 7m\nTest Win Rate\nStep (b) Transfer from 3 marines to 7 marines\nFigure 5: Experimental results on transfer learning with UPDeT (Uni-Transfer) and GRU unit\n(GRU-Transfer), along with UPDeT training from scratch (Uni-Scratch). At time step 0 and 500k,\nwe load the model from the source scenario and ï¬netune on the target scenarios. The circular points\nindicate the model performance on new scenarios without ï¬netuning.\ntransformer fails to beat the enemies in the experiment. Aggregation Transformeris a variant of\nvanilla transformer, the embedding of which are aggregated into a global embedding and then pro-\njected to a policy distribution. This transformer only satisï¬es the point 1 . The performance of the\naggregation transformer is worse than that of the GRU-based model. The result proves that it is only\nwith a policy decoupling strategy that the transformer-based model can outperform the conventional\nRNN-based model. Next, we adopt UPDeT to ï¬nd the best temporal unit architecture in Fig. 4b.\nThe result shows that without a hidden state, the performance is signiï¬cantly decreased. The tempo-\nral unit with global hidden state is more efï¬cient in terms of convergence speed than the individual\nhidden state. However, the ï¬nal performances are almost the same. To test the generalization of our\nmodel, we combine the UPDeT with VDN / QMIX / QTRAN respectively and compare the ï¬nal\nperformance with RNN-based methods in Fig. 4c. We evaluate the model performance on 5mvs 6m\n(Hard) scenarios. Combined with UPDeT, all three MARL methods obtain signiï¬cant improvement\nby large margins compared to the GRU-based model. The result proves that our model can be in-\njected into any existing stat- of-the-art MARL method to yield better performance. Further more, we\ncombine UPDeT with VDN and evaluate the model performance on different scenarios from Easy\nto Hard+ in Fig. 4d and Fig. 4e. The results show that the UPDeT performs stably on easy scenarios\nand signiï¬cantly outperforms the GRU-based model on hard scenarios, in the 4m vs 5m(Hard+)\nscenario, the performance improvement achieved by UPDeT relative to the GRU-based model is of\nthe magnitude of around 80%. Finally, we conduct an ablation study on UPDeT with paired and un-\npaired observation-entityâ€”action-group, the result of which are presented in Fig. 4f. We disrupt the\noriginal correspondence between â€™attackâ€™ action and enemy unit. The ï¬nal performance is heavily\ndecreased compared to the original model, and is even worse than the GRU-based model. We ac-\ncordingly conclude that only with policy decoupling and a paired observation-entityâ€”action-group\nstrategy can UPDeT learn a strong policy.\n4.2 M ULTIPLE SCENARIOS\nIn this section, we discuss the transfer capability of UPDeT compared to the RNN-based model.\nWe evaluate the model performance in a curriculum style. First, the model is trained one the 3m (3\nMarines vs 3 Marines) scenario. We then used the pretrained 3m model to continually train on the 5m\n(5 Marines vs 5 Marines) and 7m (7 Marines vs 7 Marines) scenarios. We also conduct a experiment\nin reverse from 7m to 3m. During transfer learning, the model architecture of UPDeT remains ï¬xed.\nConsidering that the RNN-based model cannot handle various input and output dimensions, we\nmodify the architecture of the source RNN model when training on the target scenario. We preserve\nthe parameters of the GRU cell and initialize the fully connected layer with proper input and output\ndimensions to ï¬t the new scenario. The ï¬nal results can be seen in Fig. 5a and Fig. 5b. Our proposed\nUPDeT achieves signiï¬cantly better results than the GRU-based model. Statistically, UPDeTâ€™s total\ntimestep cost to converge is at least 10 times less than the GRU-based model and 100 times less than\ntraining from scratch. Moreover, the model demonstrates a strong generalization ability without\nï¬netuning, indicating that UPDeT learns a robust policy with meta-level skill.\n8\nPublished as a conference paper at ICLR 2021\n4.3 E XTENSIVE EXPERIMENT ON LARGE -SCALE MAS\nTo evaluate the model performance in large-scale scenarios, we test our proposed UPDeT on the\n10m vs 11m and 20m vs 21m scenarios from SMAC and a 64 vs 64 battle game in the MAgent\nEnvironment (Zheng et al. (2017)). The ï¬nal results can be found in Appendix E.\n4.4 A TTENTION BASED STRATEGY : A N ANALYSIS\nThe signiï¬cant performance improvement achieved by UPDeT on the SMAC multi-agent challenge\ncan be credited to the self-attention mechanism brought by both transformer blocks and the policy\ndecoupling strategy in UPDeT. In this section, we mainly discuss how the attention mechanism\nassists in learning a much more robust and explainable strategy. Here, we use the 3 Marines vs 3\nMarines game (therefore, the size of the raw attention matrix is 6x6) as an example to demonstrate\nhow the attention mechanism works. As mentioned in the caption of Fig. 6, we simplify the raw\ncomplete attention matrix to a grouped attention matrix. Fig. 6b presents the three different stages\nin one episode including Game Start, Attack and Survive, with their corresponding attention matrix\nand strategies. In theGame Start stage, the highest attention is in line 1 col 3 of the matrix, indicating\nthat the agent pays more attention to its allies than its enemies. This phenomenon can be interpreted\nas follows: in the startup stage of one game, all the allies are spawned at the left side of the map\nand are encouraged to ï¬nd and attack the enemies on the right side In the Attack stage, the highest\nattention is in line 2 col 2 of the matrix, which indicates that the enemy is now in the agentâ€™s\nattack range; therefore, the agent will attack the enemy to get more rewards. Surprisingly, the agent\nchooses to attack the enemy with the lowest health value. This indicates that a long term plan can\nbe learned based on the attention mechanism, since killing the weakest enemy ï¬rst can decrease\nthe punishment from the future enemy attacks. In the Survive stage, the agentâ€™s health value is low,\nmeaning that it needs to avoid being attacked. The highest attention is located in line 1 col 1, which\nclearly shows that the most important thing under the current circumstances is to stay alive. For as\nlong as the agent is alive, there is still a chance for it to return to the front line and get more reward\nwhile enemies are attacking the allies instead of the agent itself.\nIn conclusion, the self-attention mechanism and policy decoupling strategy of UPDeT provides\na strong and clear relation between attention weights and ï¬nal strategies. This relation can help\nus better understand the policy generation based on the distribution of attention among different\nentities. An interesting idea presents itself here: namely, if we can ï¬nd a strong mapping between\nattention matrix and ï¬nal policy, the character of the agent could be modiï¬ed in an unsupervised\nmanner.\n5 C ONCLUSION\nIn this paper, we propose UPDeT, a universal policy decoupling transformer model that extends\nMARL to a much broader scenario. UPDeT is general enough to be plugged into any existing\nMARL method. Moreover, our experimental results show that, when combined with UPDeT, ex-\nisting state-of-the-art MARL methods can achieve further signiï¬cant improvements with the same\ntraining pipeline. On transfer learning tasks, our model is 100 times faster than training from scratch\nand 10 times faster than training using the RNN-based model. In the future, we aim to develop a\ncentralized function based on UPDeT and apply the self-attention mechanism to the entire pipeline\nof MARL framework to yield further improvement.\nACKNOWLEDGMENTS\nThis work was supported in part by the National Natural Science Foundation of China (NSFC) un-\nder Grant No.U19A2073 and in part by the National Natural Science Foundation of China (NSFC)\nunder Grant No.61976233 and No.61906109 and Australian Research Council Discovery Early Ca-\nreer Researcher Award (DE190100626), and Funding of â€œLeading Innovation Team of the Zhejiang\nProvinceâ€ (2018R01017).\n9\nPublished as a conference paper at ICLR 2021\nREFERENCES\nHaitham B Ammar, Karl Tuyls, Matthew E Taylor, Kurt Driessens, and Gerhard Weiss. Reinforce-\nment learning transfer via sparse coding. In Proceedings of the 11th international conference on\nautonomous agents and multiagent systems, volume 1, pp. 383â€“390. International Foundation for\nAutonomous Agents and Multiagent Systems . . . , 2012.\nMariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon\nGoyal, Lawrence D Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, et al. End to end learning\nfor self-driving cars. arXiv preprint arXiv:1604.07316, 2016.\nGeorgios Boutsioukis, Ioannis Partalas, and Ioannis Vlahavas. Transfer learning in multi-agent\nreinforcement learning domains. In European Workshop on Reinforcement Learning , pp. 249â€“\n260. Springer, 2011.\nXiaojun Chang, Po-Yao Huang, Yi-Dong Shen, Xiaodan Liang, Yi Yang, and Alexander G. Haupt-\nmann. RCAA: relational context-aware agents for person search. In Computer Vision - ECCV\n2018 - 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part\nIX, 2018.\nJunyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of\ngated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.\nFelipe Leno Da Silva and Anna Helena Reali Costa. A survey on transfer learning for multiagent\nreinforcement learning systems. Journal of Artiï¬cial Intelligence Research, 64:645â€“703, 2019.\nYali Du, Lei Han, Meng Fang, Ji Liu, Tianhong Dai, and Dacheng Tao. Liir: Learning individ-\nual intrinsic reward in multi-agent reinforcement learning. In Advances in Neural Information\nProcessing Systems, pp. 4403â€“4414, 2019.\nJakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson.\nCounterfactual multi-agent policy gradients. arXiv preprint arXiv:1705.08926, 2017.\nAbhishek Gupta, Coline Devin, YuXuan Liu, Pieter Abbeel, and Sergey Levine. Learning invariant\nfeature spaces to transfer skills with reinforcement learning. arXiv preprint arXiv:1703.02949,\n2017.\nMatthew Hausknecht and Peter Stone. Deep recurrent q-learning for partially observable mdps.\narXiv preprint arXiv:1507.06527, 2015.\nTodd Hester, Michael Quinlan, and Peter Stone. Generalized model learning for reinforcement learn-\ning on a humanoid robot. In 2010 IEEE International Conference on Robotics and Automation ,\npp. 2369â€“2374. IEEE, 2010.\nSepp Hochreiter and J Â¨urgen Schmidhuber. Long short-term memory. Neural computation, 9(8):\n1735â€“1780, 1997.\nWan Ju Kang David Earl Hostallero, Kyunghwan Son, Daewoo Kim, and Yung Yi Qtran. Learning to\nfactorize with transformation for cooperative multi-agent reinforcement learning. In Proceedings\nof the 31st International Conference on Machine Learning, Proceedings of Machine Learning\nResearch. PMLR, 2019.\nRyan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-\nagent actor-critic for mixed cooperative-competitive environments. In Advances in neural infor-\nmation processing systems, pp. 6379â€“6390, 2017.\nAnuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and Shimon Whiteson. Maven: Multi-agent\nvariational exploration. In Advances in Neural Information Processing Systems, pp. 7613â€“7624,\n2019.\nV olodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-\nmare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level\ncontrol through deep reinforcement learning. nature, 518(7540):529â€“533, 2015.\n10\nPublished as a conference paper at ICLR 2021\nFrans A Oliehoek, Christopher Amato, et al. A concise introduction to decentralized POMDPs ,\nvolume 1. Springer, 2016.\nAnkur P Parikh, Oscar T Â¨ackstrÂ¨om, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\nmodel for natural language inference. arXiv preprint arXiv:1606.01933, 2016.\nEmilio Parisotto, Jimmy Lei Ba, and Ruslan Salakhutdinov. Actor-mimic: Deep multitask and\ntransfer reinforcement learning. arXiv preprint arXiv:1511.06342, 2015.\nNiki Parmar, Ashish Vaswani, Jakob Uszkoreit, Åukasz Kaiser, Noam Shazeer, Alexander Ku, and\nDustin Tran. Image transformer. arXiv preprint arXiv:1802.05751, 2018.\nPeng Peng, Ying Wen, Yaodong Yang, Quan Yuan, Zhenkun Tang, Haitao Long, and Jun Wang.\nMultiagent bidirectionally-coordinated nets: Emergence of human-level coordination in learning\nto play starcraft combat games. arXiv preprint arXiv:1703.10069, 2017.\nTabish Rashid, Mikayel Samvelyan, Christian Schroeder De Witt, Gregory Farquhar, Jakob Foer-\nster, and Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent\nreinforcement learning. arXiv preprint arXiv:1803.11485, 2018.\nMikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas\nNardelli, Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson.\nThe starcraft multi-agent challenge. arXiv preprint arXiv:1902.04043, 2019.\nKun Shao, Yuanheng Zhu, and Dongbin Zhao. Starcraft micromanagement with reinforcement\nlearning and curriculum transfer learning. IEEE Transactions on Emerging Topics in Computa-\ntional Intelligence, 3(1):73â€“84, 2018.\nPeter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max\nJaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Value-decomposition\nnetworks for cooperative multi-agent learning. arXiv preprint arXiv:1706.05296, 2017.\nMing Tan. Multi-agent reinforcement learning: Independent vs. cooperative agents. In Proceedings\nof the tenth international conference on machine learning, pp. 330â€“337, 1993.\nMatthew E Taylor and Peter Stone. Transfer learning for reinforcement learning domains: A survey.\nJournal of Machine Learning Research, 10(7), 2009.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nÅukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information\nprocessing systems, pp. 5998â€“6008, 2017.\nOriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Micha Â¨el Mathieu, Andrew Dudzik, Juny-\noung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster\nlevel in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350â€“354, 2019.\nWeixun Wang, Tianpei Yang, Yong Liu, Jianye Hao, Xiaotian Hao, Yujing Hu, Yingfeng Chen,\nChangjie Fan, and Yang Gao. From few to more: Large-scale dynamic multiagent curriculum\nlearning. In AAAI, pp. 7293â€“7300, 2020.\nXiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pp. 7794â€“7803,\n2018.\nYaodong Yang, Lantao Yu, Yiwei Bai, Jun Wang, Weinan Zhang, Ying Wen, and Yong Yu. A\nstudy of ai population dynamics with million-agent reinforcement learning. arXiv preprint\narXiv:1709.04511, 2017.\nYaodong Yang, Rui Luo, Minne Li, Ming Zhou, Weinan Zhang, and Jun Wang. Mean ï¬eld multi-\nagent reinforcement learning. arXiv preprint arXiv:1802.05438, 2018.\nYaodong Yang, Ying Wen, Lihuan Chen, Jun Wang, Kun Shao, David Mguni, and Weinan Zhang.\nMulti-agent determinantal q-learning. arXiv preprint arXiv:2006.01482, 2020.\n11\nPublished as a conference paper at ICLR 2021\nLianmin Zheng, Jiacheng Yang, Han Cai, Weinan Zhang, Jun Wang, and Yong Yu. Magent: A\nmany-agent reinforcement learning platform for artiï¬cial collective intelligence. arXiv preprint\narXiv:1712.00600, 2017.\nMeng Zhou, Ziyu Liu, Pengwei Sui, Yixuan Li, and Yuk Ying Chung. Learning implicit credit\nassignment for multi-agent actor-critic. arXiv preprint arXiv:2007.02529, 2020.\nFengda Zhu, Yi Zhu, Xiaojun Chang, and Xiaodan Liang. Vision-language navigation with self-\nsupervised auxiliary reasoning tasks. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 10012â€“10022, 2020.\n12\nPublished as a conference paper at ICLR 2021\nA D ETAILS OF SMAC ENVIRONMENT\nThe action space contains four movement directions, k attack actions (where k is the ï¬xed maximum\nnumber of the enemy units in a map), stop and none-operation. At each time step, the agents receive\na joint team reward, which is deï¬ned by the total damage incurred by the agents and the total\ndamage from the enemy side. Each agent is described by several attributes, including health point\nHP, weapon cool down (CD), unit type, last action and the relative distance of the observed units.\nThe enemy units are described in the same way except that CD is excluded. The partial observation\nof an agent comprises the attributes of the units, including both the agents and the enemy units, that\nexist within its view range, which is a circle with a speciï¬c radius.\nB D ETAILS OF MODEL\nThe transformer block in all different experiments consists of 3 heads and 2 layer transformer blocks.\nThe other important training hyper parameters are as follows:\nList of Hyper Parameters\nName Value\nbatch size 32\ntest interval 2000\ngamma 0.99\nbuffer size 5000\ntoken dimension (UPDeT) 32\nchannel dimension (UPDeT) 32\nepsilon start 1.0\nepsilon end 0.05\nrnn hidden dimension 64\ntarget net update interval 200\nmixing embeddding dimension (QMIX) 32\nhypernet layers (QMIX) 2\nhypernet embedding (QMIX) 64\nmixing embeddding dimension (QTRAN) 32\nopt loss (QTRAN) 1\nnopt min loss (QTRAN) 0.1\nC SOTA MARL VALUE -BASED FRAMEWORK\nThe three SOTA method can be brieï¬‚y summarized as follows:\nâ€¢ VDN (Sunehag et al. (2017)): this method learns an individual Q-value function and rep-\nresents Qtot as a sum of individual Q-value functions that condition only on individual\nobservations and actions.\nâ€¢ QMIX (Rashid et al. (2018)): this method learns a decentralized Q-function for each agent,\nwith the assumption that the centralized Q-value increases monotonically with the individ-\nual Q-values.\nâ€¢ QTRAN (Hostallero et al. (2019)): this method formulates multi-agent learning as an opti-\nmization problem with linear constraints and relaxes it with L2 penalties for tractability.\nD UPD ET ON SMAC: A REAL CASE\nWe take the 3 Marines vs 3 Marines challenge from SMAC with UPDeT as an example; more\ndetails can be found in Fig. 7. The observation are separated into 3 groups: main agent, two other\nally agents and three enemies. The policy output includes basic action corresponding to the main\nagentâ€™s observation and attack actions, one for each enemy observation. The hidden state is added\nafter the embedding layer. The output of other agents is abandoned as there is no corresponding\n13\nPublished as a conference paper at ICLR 2021\nHeatmap\nHigh \nâ†“\nLow \ncomplete \nattention \nmatrix\nentity\nattention\nenemy ally self \nattention\n[move]\n[attack]\n[void]\naction simplified \nattention \nmatrix\nagent\nenemy\nally\n(a) Attention Matrix\n[move]\n[attack]\nCurrent \nStage\nStage 1: Game Start Stage 2: Attack Stage 3: Survive\n[void]\nfocus on allies â†’\nmove (with allies)Strategy\nagent enemy 1 enemy 2 enemy 3\n ally 1\n ally 2\n[move]\n[move]\n[attack]\n[void]\n[move]\n[attack]\n[void]\n[attack]\nfocus on enemies â†’\nattack (one of the enemy)\nfocus on own health â†’\nmove (to survive)\n[move]\nAttention (b) Strategy with Attention\nFigure 6: An analysis of the attention based strategy of UPDeT. Part (a) visualizes a typical attention\nmatrix. Part (b) utilizes the simpliï¬ed attention matrix to describe the relationship between attention\nand ï¬nal strategy. Further discussion can be found in Section 4.4.\naction. Once an agent or enemy has died, we mask corresponding unavailable action in the action\nselect stage to ensure only the available actions are selected.\nE R ESULTS OF EXTENSIVE EXPERIMENT ON LARGE SCALE\nWe further test the robustness of UPDeT in a large-scale multi-agent system. To do so, we enlarge\nthe game size in SMAC (Samvelyan et al. (2019)) to incorporate more agents and enemies on the\nbattle ï¬eld. We use a 10 Marines vs 11 Marines game and a 20 Marines vs 21 Marines game to\ncompare the performance between the UPDeT and GRU-based approaches. In the 20 Marines vs 21\nMarines game, to accelerate the training and satisfy the hardware limitations, we decrease the batch\nsize of both the GRU baseline and UPDeT from 32 to 24 in the training stage. The ï¬nal results can\nbe found in Fig. 8a. The improvement is still signiï¬cant in terms of both sample efï¬ciency and ï¬nal\nperformance. Moreover, it is also worth mentioning that the model size of UPDeT stays ï¬xed, while\nthe GRU-based model becomes larger in large-scale scenarios. In the 20 Marines vs 21 Marines\ngame, the model size of GRU is almost double that of UPDeT. This indicates that UPDeT is able to\nensure the lightness of the model while still maintaining good performance.\nWe also test the model performance in the MAgent Environment (Zheng et al. (2017)). The set-\ntings of MAgent are quite different from those of SMAC. First, the observation size and number\nof available actions are not related to the number of agents. Second, the 64 vs 64 battle game we\ntested is a two-player zero-sum game which is another hot research area that combines both MARL\nand GT (Game Theory), the most successful attempt in this area involves adopting a mean-ï¬eld\napproximation of GT in MARL to accelerate the self-play training (Yang et al. (2018)). Third, as\nfor the model architecture, there is no need to use a recurrent network like GRU in MAgent and the\nFigure 7: Real case on 3 Marines vs 3 Marines Challenge from SMAC.\n14\nPublished as a conference paper at ICLR 2021\nTest Win Rate\n(a) Large-Scale Scenarios\nReward (b) MAgent Battle: 64 vs 64\nFigure 8: Experimental results on the large-scale MAS, including SMAC and MAgent.\nlarge observation size requires the use of a CNN from embedding. However, ny treating UPDeT as a\npure encoder without recurrent architecture, we can still conduct experiments on MAgent; the ï¬nal\nresults of these can be found in Fig. 8b. As the result show, UPDeT performs better than the DQN\nbaseline, although this improvement is not as signiï¬cant as it in SMAC.\n15",
  "topic": "Reinforcement learning",
  "concepts": [
    {
      "name": "Reinforcement learning",
      "score": 0.8879786729812622
    },
    {
      "name": "Computer science",
      "score": 0.7609573006629944
    },
    {
      "name": "Transformer",
      "score": 0.7025426030158997
    },
    {
      "name": "Decoupling (probability)",
      "score": 0.5890858173370361
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5374053120613098
    },
    {
      "name": "Machine learning",
      "score": 0.34176915884017944
    },
    {
      "name": "Control engineering",
      "score": 0.200576514005661
    },
    {
      "name": "Engineering",
      "score": 0.11756706237792969
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I56590836",
      "name": "Monash University",
      "country": "AU"
    }
  ]
}