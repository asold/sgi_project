{
  "title": "Multiple-Choice Question Generation Using Large Language Models: Methodology and Educator Insights",
  "url": "https://openalex.org/W4400106310",
  "year": 2024,
  "authors": [
    {
      "id": null,
      "name": "Biancini, Giorgio",
      "affiliations": [
        "Roma Tre University"
      ]
    },
    {
      "id": null,
      "name": "Ferrato, Alessio",
      "affiliations": [
        "Roma Tre University"
      ]
    },
    {
      "id": "https://openalex.org/A3162765248",
      "name": "Limongelli Carla",
      "affiliations": [
        "Roma Tre University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2906432682",
    "https://openalex.org/W1784223173",
    "https://openalex.org/W2735156554",
    "https://openalex.org/W4382567310",
    "https://openalex.org/W4284700492",
    "https://openalex.org/W2967444271",
    "https://openalex.org/W4383818129",
    "https://openalex.org/W4366245770",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4292197005",
    "https://openalex.org/W6925368804",
    "https://openalex.org/W2069032672",
    "https://openalex.org/W4396758719"
  ],
  "abstract": "Integrating Artificial Intelligence (AI) in educational settings has brought new learning approaches, transforming the practices of both students and educators. Among the various technologies driving this transformation, Large Language Models (LLMs) have emerged as powerful tools for creating educational materials and question answering, but there are still space for new applications. Educators commonly use Multiple-Choice Questions (MCQs) to assess student knowledge, but manually generating these questions is resource-intensive and requires significant time and cognitive effort. In our opinion, LLMs offer a promising solution to these challenges. This paper presents a novel comparative analysis of three widely known LLMs - Llama 2, Mistral, and GPT-3.5 - to explore their potential for creating informative and challenging MCQs. In our approach, we do not rely on the knowledge of the LLM, but we inject the knowledge into the prompt to contrast the hallucinations, giving the educators control over the test's source text, too. Our experiment involving 21 educators shows that GPT-3.5 generates the most effective MCQs across several known metrics. Additionally, it shows that there is still some reluctance to adopt AI in the educational field. This study sheds light on the potential of LLMs to generate MCQs and improve the educational experience, providing valuable insights for the future.",
  "full_text": "arXiv:2506.04851v1  [cs.CL]  5 Jun 2025\nMultiple-Choice Question Generation Using Large Language\nModels: Methodology and Educator Insights\nGiorgio Biancini∗ Alessio Ferrato† Carla Limongelli‡\nJune 8, 2025\nAbstract\nIntegrating Artificial Intelligence (AI) in educational settings has brought new learning ap-\nproaches, transforming the practices of both students and educators. Among the various tech-\nnologies driving this transformation, Large Language Models (LLMs) have emerged as power-\nful tools for creating educational materials and question answering, but there are still space\nfor new applications. Educators commonly use Multiple-Choice Questions (MCQs) to assess\nstudent knowledge, but manually generating these questions is resource-intensive and requires\nsignificant time and cognitive effort. In our opinion, LLMs offer a promising solution to these\nchallenges. This paper presents a novel comparative analysis of three widely known LLMs -\nLlama 2, Mistral, and GPT-3.5 - to explore their potential for creating informative and chal-\nlenging MCQs. In our approach, we do not rely on the knowledge of the LLM, but we inject the\nknowledge into the prompt to contrast the hallucinations, giving the educators control over the\ntest’s source text, too. Our experiment involving 21 educators shows that GPT-3.5 generates\nthe most effective MCQs across several known metrics. Additionally, it shows that there is still\nsome reluctance to adopt AI in the educational field. This study sheds light on the potential of\nLLMs to generate MCQs and improve the educational experience, providing valuable insights\nfor the future.\nThis paper was accepted for publication in the Adjunct Proceedings of the 32nd ACM Confer-\nence on User Modeling, Adaptation and Personalization (UMAP Adjunct ’24), July 1–4, 2024,\nCagliari, Italy. © ACM 2024. This is the author’s version of the work. It is posted here for your\npersonal use. Not for redistribution. The definitive Version of Record was published in Adjunct\nProceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization\n(UMAP Adjunct ’24), http://dx.doi.org/10.1145/3631700.3665233.\n1 Introduction\nIncorporating Artificial Intelligence (AI) into education can potentially transform the teaching and\nlearning landscape. However, discussions surrounding AI in this context are frequently charac-\nterized by a blend of excitement and doubt. A significant issue revolves around the readiness of\neducators to employ these technologies in a meaningful manner. The unwillingness to embrace new\ntools stems from educators’ apprehension about changing established teaching methodologies due\nto their unfamiliarity with new technologies. From our perspective, this gap presents a distinct\nopportunity for AI to offer numerous advantages to educators. For example, AI can assist teachers\nto [WM+23]:\n∗gio.biancini@stud.uniroma3.it\n†alessio.ferrato@uniroma3.it\n‡limongel@dia.uniroma3.it\n1\n• Streamlining the drafting of course materials, including syllabuses, lesson plans, and learning\nobjectives.\n• Enhancing teaching strategies through suggestions for inclusive activities and diverse reading\nlists.\n• Supporting personalized student learning with individualized tutoring and simplified expla-\nnations of complex topics.\n• Assisting in creating practice test questions and quizzes, thus easing the assessment process.\nOne significant application of AI concerns the last point. In particular, Large Language Models\n(LLMs) can automate the creation of Multiple Choice Questions (MCQs), which traditionally\ndemand considerable time and effort from educators.\nBy focusing on using LLMs to generate MCQs, this study diverges from the existing literature,\nwhich predominantly explores LLMs for question-answering. Our research specifically investigates\nthe effectiveness of LLMs, such as Llama 2, Mistral, and GPT-3.5, in generating MCQs. Through\na comparative analysis conducted via a user study, we aim to explore the potential applications of\nLLMs in this specific task.\n2 Background\nQuestions are crucial for evaluating a learner’s knowledge and understanding. Among various test-\ning formats, MCQs stand out for their efficiency in assessment, uniformity in scoring, and suitability\nfor electronic evaluation [CS20]. MCQs help assess a student’s recall and understanding of specific\ninformation or knowledge in the text [ARS +16]. An MCQ consists of three basic components:\n• Stem: The part of the question that presents the context or situation on which the question\nis based.\n• Key: The correct answer of the question.\n• Distractors: The incorrect options proposed as possible answers are intended to confuse or\ndistract students.\nThe manual MCQ preparation involves understanding the input text, identifying informative sen-\ntences with potential questions, selecting the keyword or phrase as the answer, forming a question\naround it, and choosing distractors from the text or related context. Crafting MCQs manually is\nlabor-intensive and costly [GABS19]; these aspects encourage research into automating their gen-\neration. The work of Rao et al. [CS20] delves into this process, examining key stages employed to\ngenerate MCQs. For the authors, the automatic generation approach mirrors manual methods. It\nis structured into stages: (1) pre-processing to prepare the text, (2) sentence selection to identify\nsuitable content, (3) key selection for the answer, (4) question generation, (5) distractor selection\nfor wrong answers, and (6) post-processing to finalize the questions. This streamlined workflow is\ncommonly adopted across various systems, albeit with minor variations. This field has attracted\nsignificant interest, with the foundational work on automatic MCQ generation laid by Coniam\nDavid in 1997, leveraging word tagging and frequencies to create various types of MCQs [Con97].\nSubsequent advancements have witnessed the proliferation of MCQ generation systems across\ndiverse languages and disciplines. Building upon this foundation, Miktov et al. [MAHK06] directed\ntheir efforts towards generating questions within the medical domain, employing techniques such as\n2\nshallow parsing, term extraction, sentence transformation, and computation of semantic distance.\nThey utilized corpora and ontologies (i.e., WordNet) for different process stages. Majumder et\nal. [MS15] primarily focused on sentence selection based on topic word and parse structure similarity\nwithin the realm of sports. Maheen et al. [MAA +22] endeavored to generate computer science\nquestions utilizing classical machine learning and text embedding techniques with BERT. Their\nsystem underwent validation by ten domain experts across three metrics (i.e., sentence selection,\nkey generation, and distractor selection). Notably, both approaches exhibit several intermediary\nsteps, potentially restraining their application across domains beyond those for which they were\noriginally designed.\nThe quality of MCQs is heavily contingent on the quality of distractors. Insufficiently challeng-\ning distractors may lead examiners to identify the correct answer easily. In this context, the authors\nof [KNK+23] and [LSB +23] propose systems utilizing BERT for text processing and WordNet for\ngenerating distractors. A recent contribution on language models is presented in [BMY23] where\nauthors demonstrated how pre-trained language models can be customized for generating questions\nin the educational context outlining also the interest in MCQ generation.\nThe primary challenges in automatic MCQ generation lie in identifying and adapting the most\neffective systems capable of executing the intermediary steps necessary for generating desired ques-\ntions, as emphasized by Rao [CS20]. However, we believe that LLMs are capable of autonomously\nperform all the necessary steps for MCQ generation even without additional training. Despite their\npotential, the existing literature on LLMs and MCQs has predominantly focused on evaluating\nthese models for question-answering tasks (e.g., [RRW22]). Only recently the author of [MSKB24]\nhave proposed a system prototype for the automated generation of self-assessment quizzes validat-\ning it with 6 participants. Extending their work, we want to evaluate LLMs for MCQ generation\nemploying a versatile prompt applicable under various conditions and domains since it does not\nrely on the knowledge of the LLM. We conduct an experiment with 21 domain experts, including\nhigh school and university professors, utilizing several established metrics to assess the quality of\ngenerated MCQs from [CS20].\n3 Prompt for MCQ generation\nAccording to existing research, constructing queries for LLMs, known as prompt engineering, poses\nsignificant challenges [ZPWHY23]. Achieving desired responses from the model requires multiple\niterations, making it complex. We dedicated significant effort to crafting a prompt that adheres to\na strict policy for eliciting responses. The definitive query used across all LLMs in our research is\nas follows:\nYou are an assistant that doesn’t make mistakes.\nIf a reference model is presented to you, you follow it perfectly without making errors.\nCreate a high school - level quiz based on the provided text.\nYou must strictly adhere to the following format without any errors:\n> [ Insert the question ]\na ) [ Option A ]\nb ) [ Option B ]\nc ) [ Option C ]\nd ) [ Option D ]\n* Correct Answer: [Insert the letter corresponding to the correct answer for example : ’a)’]\n* Source: [Write the exact line or passage from the provided text where the information for this question can be found.]\nPlease note that you are allowed to modify only the parts within brackets ([...]) in the format provided.\n3\nEnsure that all four options are distinct.\nWhen mentioning a date, please make sure to specify the year.\nThe text is : { text }\nWe insert a paragraph from the text serving as the knowledge base for the model into the ”text”\nparameter. This query generates questions proportional to the paragraph length, implying that\nlonger paragraphs may yield more questions.\n4 Experiment\n4.1 Setting\n4.1.1 Metrics\nThe experimentation aims to test which of the three LLMs better meets the following evaluation\ncriteria described in [CS20]:\n1. compliance of the question with the source text provided (compliance or sentence selection),\n2. clarity of formulation (clarity),\n3. selection of the proposed distractors (distractors selection),\n4. the soundness of the correct answer with respect to the question (coherence of key selection),\n5. utility for learning (learning utility).\n4.1.2 Sample selection\nWe distributed the form to a diverse group of high school and university teachers with varying levels\nof experience and expertise across disciplines. The sample included educators skilled in designing\neducational materials and evaluating student progress, regardless of their area of study.\n4.1.3 Questionnaire generation\nThe topic considered for the experiment regards the Second World War 1, an acknowledged piece\nof history. We employed the prompt presented in 3 for each paragraph of the text. As we did not\nimpose a set number of questions, the number of multiple-choice questions varied from model to\nmodel and paragraph to paragraph. To create the test, we randomly selected seven MCQs for each\nLLM. All generations were conducted using Google Colab’s free tier with Llama 2 2 and Mistral3\nfrom HuggingFace. Both models used a temperature of 0.5, a maximum of 2048 new tokens, and a\ntop k value of 30. Other hyperparameters were set to default. For generations using GPT-3.5, we\nused ChatGPT4 and provided a new prompt for every paragraph while omitting the chat history\nfrom the account settings.\n1We used the following knowledge sourcehttps://en.wikipedia.org/wiki/Causes_of_World_War_II (last ac-\ncess: June 8, 2025)\n2https://huggingface.co/meta-llama/Llama-2-7b-chat-hf (last access: June 8, 2025)\n3https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1 (last access: June 8, 2025)\n4https://chat.openai.com/ (last access: June 8, 2025)\n4\nTable 1: Descriptive statistics and Friedman test results. The extended description of categories is\nreported in 4.1.1\nMetric LLM Mean Std Median F-statistic p-value\nClarity\nGPT-3.5 1.496599 0.822410 2\n23.490066 0.000008* Llama 1.292517 0.959530 2\nMistral 0.986395 1.193512 1\nCoherence\nGPT-3.5 1.013605 1.394639 2\n18.369620 0.000103* Llama 0.646259 1.456223 1\nMistral 0.421769 1.456799 1\nCompliance\nGPT-3.5 1.346939 1.083203 2\n60.903382 0.000000* Llama 0.360544 1.608818 1\nMistral -0.006803 1.563700 0\nDistractor selection\nGPT-3.5 1.006803 1.082214 1\n14.116279 0.000860* Llama 0.748299 1.198421 1\nMistral 0.537415 1.195151 1\nLearning utility\nGPT-3.5 0.625850 1.277899 1\n1.749373 0.416993 Llama 0.605442 1.213835 1\nMistral 0.489796 1.212759 1\n* p-value < 0.005\n4.1.4 Questionnaire execution\nThe online form was created using Google Forms and was about 30 minutes long. The form con-\ntained 21 (i.e., seven for each LLM) distinct generated MCQs. For each one there were the text\nsourrce (i.e., the piece of text that the LLM used to construct the question), the question with four\nthe possible answers, and the correct answer. Respondents were asked to rate the five criteria estab-\nlished in 4.1.1 on a five-point Likert scale, ranging from ”strongly disagree” to ”strongly agree.” We\nalso asked participants about their willingness to use LLMs to generate MCQs. This comprehensive\napproach allowed for a detailed assessment of the LLMs’ performance and provided valuable in-\nsights into the perceptions of participants. We did not collect any demographic information about\nthe teachers.\n4.2 Results\nWe utilized the non-parametric Friedman test to assess whether the responses provided by 21\neducators varied significantly across the three models analyzed. Our assumptions were as follows:\n• Null hypothesis: LLMs are equally good in generating MCQs on a given metric;\n• Alternative hypothesis: There is at least one LLM better than the others in generating MCQs.\nWe have decided to use this particular test since the Shapiro-Wilk test rejected the normality\ndistribution of the data, and each participants voted for the MCQs of all three LLMs. We opted\nfor this test since the data did not follow a normal distribution, as indicated by the Shapiro-Wilk\ntest, and the participants on the MCQs generated evaluated all three LLMs.\n5\n(a) Clarity\n (b) Coherence\n(c) Compliance\n (d) Distractor selection\n(e) Learning utility\nFigure 1: Distribution of the responses for each LLM and metric. The small white triangles indicate\nthe mean value for the box.\nThe results, presented in Table 1, showed significant differences in the evaluations of the models,\nexcept for ”Learning utility,” where the difference was not statistically significant, which, in our\nopinion, can be influenced by various external and internal factors specific to each individual.\nIt is crucial to acknowledge these inherent challenges when assessing this metric. Despite this,\nupon closer examination of the data, as illustrated in Figure 1, GPT-3.5 consistently outperformed\nthe other two LLMs in MCQ generation across all metrics, with Llama 2 coming in second and\nMistral in third. Furthermore, Llama and Mistral exhibited poorer coherence and compliance\nperformance than GPT. This result could potentially be attributed to suboptimal source text or\nsentence selection. Notably, both models at times identified the position of the sentence as the\nsource text instead of the actual sentence itself (e.g. ”Last sentence of the text” or ”Line 5 of\nthe text”). This affected both metrics because it was difficult for participants to assess that the\nanswer reported as correct actually was correct by not having the text from which the question was\n6\nFigure 2: Distribution of the responses regarding the possible adoption of automatic MCQ gener-\nators by educators.\ncreated. It is worth mentioning that GPT did not exhibit such a phenomenon.\nIn the last survey question, we asked participants about possibly using this or a similar system.\nBased on the responses, as shown in Figure 2, the results were varied. While only two out of 21\nparticipants showed disinterest in using the system, eleven participants were in favor of adopting it,\nand eight remained neutral. These mixed findings shed light on opportunities and potential prob-\nlems with this system. It is essential to conduct additional research on the factors that encourage\nor discourage the acceptance of such systems in the future. This will aid in comprehending the\nmotivations behind people’s decisions and in building systems that cater to their needs.\n5 Conclusions, limitations and future work\nThis paper introduces an innovative methodology for generating MCQs utilizing LLMs. This\nmethod diverges from traditional approaches that use LLMs for question answering. Also, with\nour query, we do not rely exclusively on the intrinsic knowledge base of the LLM. Instead, it\nemploys the LLM as a tool that can execute autonomously the stages involved in automatic MCQs\ncreation. The efficacy of the generated MCQs was evaluated through an experiment conducted\nwith educators. The findings of our investigation indicate that GPT-3.5 is particularly proficient\nin generating MCQs.\nHowever, this study acknowledges certain limitations. Notably, a comparative analysis between\nthe MCQs generated by LLMs and those publicly available or human-generated was not conducted.\nAdditionally, the scope of our experimentation was limited to a small sample size (i.e., seven\nquestions per LLM and 21 participants), primarily attributed to the extensive duration required\nfor form completion and the requirement of domain experts. The absence of demographic data\nabout the participants also presents a limitation, as such information would have been instrumental\nin elucidating the reasons behind certain participants’ resistance to adopting automated MCQ\ngenerators. Another bias concerns the topic used for MCQ generation since there are several\ndomains in which the LLMs have troubles.\nWhile these technologies offer considerable benefits, such as process optimization and increased\nproductivity, they are not without inherent risks, including the presumption of infallibility and\npotential privacy issues. Initiatives like UNESCO’s ”Guidance for Generative AI in Education and\nResearch”5 underscore the necessity for establishing regulatory frameworks to ensure the ethical\n5https://www.unesco.org/en/articles/guidance-generative-ai-education-and-research (last access: June\n8, 2025)\n7\nuse of AI in educational settings, thereby mitigating potential misuse and ethical concerns. AI’s\ntransformative potential can be supported by proactive measures and stakeholder collaboration,\nadhering to principles of accountability and ethical integrity in education.\nIt is crucial to perceive AI as a supplementary instrument rather than a substitute for human ex-\npertise [Gra23]. Hence, future research should aim to develop autonomous frameworks that enable\nthe seamless incorporation of LLMs into educational environments for MCQ generation. Addition-\nally, the potential application of Bloom’s Taxonomy [AK01] could lead to a differentiation of the\nMCQ’s learning objectives [EKCS23] that could also enhance the student’s learning experience.\nFurthermore, following what has been done in recommender systems [BDRDV +13, FLMS22a,\nSGM19], implementing personalization based on users’ personalities or emotional states could im-\nprove the efficacy of AI-driven educational frameworks [CBPR21]. Automatic MCQ generation\ngoes beyond traditional educational settings and can be used in various contexts. For example,\nafter cultural visits [DAGMS17], such systems could ask users about the main points of interest\nthat caught their attention [FLMS22b], thus creating more meaningful experiences.\nWe recognize that LLMs could hallucinate [ZLC+23]. Addressing this issue should be a priority\nfor future developments. While methods like Knowledge Injection [MIT23] have been proposed, and\nwe have partially implemented it using custom text (i.e., the injected paragraph) as a knowledge\nbase to mitigate this problem, more efforts should be dedicated to this approach. Another technique\nused to address this issue is the application of Retrieval-Augmented Generation [LPP+20] to LLMs,\nallowing them to utilize trustworthy knowledge bases that are not used to train the model. Although\nthis technique has been used in question answering [SLW+24], it can also be applied in open-ended\nquestions and MCQ generation.\nIn the future, our goal is to overcome the limitations of this study and advance with creating an\nopen-source tool for supervised MCQ generation that educators and researchers can utilize. Imple-\nmenting similar systems to the one demonstrated in this work can significantly improve educational\npractices as long as they are utilized responsibly.\nReferences\n[AK01] Lorin W Anderson and David R Krathwohl. A taxonomy for learning, teaching,\nand assessing: A revision of bloom’s taxonomy of educational objectives: complete\nedition, 2001.\n[ARS+16] Jun Araki, Dheeraj Rajagopal, Sreecharan Sankaranarayanan, Susan Holm, Yukari\nYamakawa, and Teruko Mitamura. Generating questions and multiple-choice answers\nusing semantic analysis of texts. In Yuji Matsumoto and Rashmi Prasad, editors,\nProceedings of COLING 2016, the 26th International Conference on Computational\nLinguistics: Technical Papers, pages 1125–1136, Osaka, Japan, December 2016. The\nCOLING 2016 Organizing Committee.\n[BDRDV+13] Ciro Bologna, Anna Chiara De Rosa, Alfonso De Vivo, Matteo Gaeta, Giuseppe San-\nsonetti, and Valeria Viserta. Personality-based recommendation in e-commerce. In\nCEUR Workshop Proceedings, volume 997, Aachen, Germany, 2013. CEUR-WS.org.\n[BMY23] Sahan Bulathwela, Hamze Muse, and Emine Yilmaz. Scalable educational question\ngeneration with pre-trained language models. In Ning Wang, Genaro Rebolledo-\nMendez, Noboru Matsuda, Olga C. Santos, and Vania Dimitrova, editors, Artificial\nIntelligence in Education, pages 327–339, Cham, 2023. Springer Nature Switzerland.\n8\n[CBPR21] Cristina Conati, Oswald Barral, Vanessa Putnam, and Lea Rieger. Toward per-\nsonalized xai: A case study in intelligent tutoring systems. Artificial intelligence ,\n298:103503, 2021.\n[Con97] David Coniam. A preliminary inquiry into using corpus word frequency data in the\nautomatic generation of english language cloze tests. CALICO Journal, 14(2/4):15–\n33, 1997.\n[CS20] Dhawaleswar Rao Ch and Sujan Kumar Saha. Automatic multiple choice ques-\ntion generation from text: A survey. IEEE Transactions on Learning Technologies,\n13(1):14–25, 2020.\n[DAGMS17] Alessio De Angelis, Fabio Gasparetti, Alessandro Micarelli, and Giuseppe Sansonetti.\nA social cultural recommender based on linked open data. In Adjunct Publication\nof the 25th Conference on User Modeling, Adaptation and Personalization , UMAP\n’17, pages 329–332, New York, NY, USA, 2017. ACM.\n[EKCS23] Sabina Elkins, Ekaterina Kochmar, Jackie C. K. Cheung, and Iulian Serban. How\nuseful are educational questions generated by large language models?, 2023.\n[FLMS22a] Alessio Ferrato, Carla Limongelli, Mauro Mezzini, and Giuseppe Sansonetti. The\nmeta4rs proposal: Museum emotion and tracking analysis for recommender systems.\nIn Adjunct Proceedings of the 30th ACM Conference on User Modeling, Adaptation\nand Personalization, UMAP ’22 Adjunct, page 406–409, New York, NY, USA, 2022.\nAssociation for Computing Machinery.\n[FLMS22b] Alessio Ferrato, Carla Limongelli, Mauro Mezzini, and Giuseppe Sansonetti. Using\ndeep learning for collecting data about museum visitor behavior. Applied Sciences,\n12(2), 2022.\n[GABS19] Sithara HPW Gamage, Jennifer R Ayres, Monica B Behrend, and Elizabeth J Smith.\nOptimising moodle quizzes for online assessments. International journal of STEM\neducation, 6:1–14, 2019.\n[Gra23] Simone Grassini. Shaping the future of education: exploring the potential and con-\nsequences of ai and chatgpt in educational settings. Education Sciences, 13(7):692,\n2023.\n[KNK+23] Archana Praveen Kumar, Ashalatha Nayak, Manjula Shenoy K., Shashank Goyal,\nand Chaitanya. A novel approach to generate distractors for multiple choice ques-\ntions. Expert Systems with Applications , 225:120022, 2023.\n[LPP+20] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,\nNaman Goyal, Heinrich K¨ uttler, Mike Lewis, Wen-tau Yih, Tim Rockt¨ aschel, et al.\nRetrieval-augmented generation for knowledge-intensive nlp tasks.Advances in Neu-\nral Information Processing Systems, 33:9459–9474, 2020.\n[LSB+23] S Adi Lakshmi, Rajesh Saturi, Anupriya Bharti, Meghana Avvari, and Battu Bha-\nvana. Multiple choice question generation using bert xl net, 2023.\n[MAA+22] Farah Maheen, Muhammad Asif, Haseeb Ahmad, Shahbaz Ahmad, Fahad Alturise,\nOthman Asiry, and Yazeed Yasin Ghadi. Automatic computer science domain\n9\nmultiple-choice questions generation based on informative sentences. PeerJ Com-\nputer Science, 8:e1010, 2022.\n[MAHK06] Ruslan Miktov, Le An Ha, and Nikiforos Karamanis. A computer-aided envi-\nronment for generating multiple-choice test items. Natural Language Engineering ,\n12(2):177–194, 2006.\n[MIT23] Ariana Martino, Michael Iannelli, and Coleen Truong. Knowledge injection\nto counter large language model (llm) hallucination. In Catia Pesquita, Hala Skaf-\nMolli, Vasilis Efthymiou, Sabrina Kirrane, Axel Ngonga, Diego Collarana, Renato\nCerqueira, Mehwish Alam, Cassia Trojahn, and Sven Hertling, editors, The Seman-\ntic Web: ESWC 2023 Satellite Events , pages 182–185, Cham, 2023. Springer Nature\nSwitzerland.\n[MS15] Mukta Majumder and Sujan Kumar Saha. A system for generating multiple choice\nquestions: With a novel approach for sentence selection. In Hsin-Hsi Chen, Yuen-\nHsien Tseng, Yuji Matsumoto, and Lung Hsiang Wong, editors, Proceedings of the\n2nd Workshop on Natural Language Processing Techniques for Educational Applica-\ntions, pages 64–72, Beijing, China, July 2015. Association for Computational Lin-\nguistics.\n[MSKB24] Niklas Meißner, Sandro Speth, Julian Kieslinger, and Steffen Becker. Evalquiz –\nllm-based automated generation of self-assessment quizzes in software engineering\neducation. In Software Engineering im Unterricht der Hochschulen 2024 , pages 53–\n64. Gesellschaft f¨ ur Informatik e.V., Bonn, 2024.\n[RRW22] Joshua Robinson, Christopher Michael Rytting, and David Wingate. Leveraging\nlarge language models for multiple choice question answering, 2022.\n[SGM19] Giuseppe Sansonetti, Fabio Gasparetti, and Alessandro Micarelli. Cross-domain\nrecommendation for enhancing cultural heritage experience. In Adjunct Publication\nof the 27th Conference on User Modeling, Adaptation and Personalization , pages\n413–415, New York, NY, USA, 2019. ACM.\n[SLW+24] Hongda Sun, Yuxuan Liu, Chengwei Wu, Haiyu Yan, Cheng Tai, Xin Gao, Shuo\nShang, and Rui Yan. Harnessing multi-role capabilities of large language models\nfor open-domain question answering. In Proceedings of the ACM on Web Confer-\nence 2024, WWW ’24, page 4372–4382, New York, NY, USA, 2024. Association for\nComputing Machinery.\n[WM+23] Jeromie Whalen, Chrystalla Mouza, et al. Chatgpt: Challenges, opportunities, and\nimplications for teacher education. Contemporary Issues in Technology and Teacher\nEducation, 23(1):1–23, 2023.\n[ZLC+23] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang,\nEnbo Zhao, Yu Zhang, Yulong Chen, et al. Siren’s song in the ai ocean: A survey\non hallucination in large language models, 2023.\n[ZPWHY23] JD Zamfirescu-Pereira, Richmond Y Wong, Bjoern Hartmann, and Qian Yang. Why\njohnny can’t prompt: how non-ai experts try (and fail) to design llm prompts. In\nProceedings of the 2023 CHI Conference on Human Factors in Computing Systems ,\npages 1–21, New York, NY, USA, 2023. Association for Computing Machinery.\n10\nA Questions\nHere, we show the randomly selected questions used in the questionnaire.\nTable 2: MCQs generated by GPT-3.5\nSource Question Options Correct\nThe British declared war on Japan on the morning of\n8 December and specifically identified the attacks on\nMalaya, Singapore and Hong Kong as the cause but\nomitted any mention of Pearl Harbor.\nWhat event\ncaused Canada\nto declare war on\nJapan?\na) The attacks on Pearl Harbor\nb) The attacks on Malaysia, Singapore and Hong Kong\nc) The sinking of the Lusitania\nd) Germani’s invasion of Poland.\nb\nHitler chose to declare that the Tripartite Pact re-\nquired Germany to follow Japan’s declaration of war\nalthough American destroyers escorting convoys and\nGerman U-boats had been de facto at war in the Bat-\ntle of the Atlantic.\nWhat was the\npurpose of the\nTripartite Pact?\na) Establish a military alliance between Germany, Italy and Japan\nb) Establish a military alliance between Germany, Italy and the United States\nc) Establish a military alliance between Germany, Italy and the Soviet Union\nd) Establish a military alliance between Germany, Italy and the United Kingdom\na\nHitler chose to declare that the Tripartite Pact re-\nquired Germany to follow Japan’s declaration of war\nalthough American destroyers escorting convoys and\nGerman U-boats had been de facto at war in the Bat-\ntle of the Atlantic.\nWhat event\nbrought the\nUnited States\ninto the Euro-\npean war?\na) The United States declared war on Germany and Italy.\nb) Germany and Italy declared war on the United States.\nc) The United States joined the Tripartite Pact\nd) The United States joined the Axis powers.\nb\nJapan stepped in as a mediator in the war in May 1941\nand allowed its ally to occupy the bordering provinces\nin Cambodia and Laos.\nWhat hap-\npened in May\n1941 during the\nFranco-Thai\nWar?\na) Japan intervened as a mediator in the war and allowed its ally to occupy the\nneighboring provinces of Cambodia and Laos.\nb) The Soviet Union launched a surprise attack on Germany.\nc) The United States declared war on Japan.\nd) The United Kingdom surrendered to Germany.\na\nThe last larger unit of Polish troops capitulated on\nOctober 6, 1939, near Kock, but some units went\nstraight to partisan combat.\nWhat happened\nto the last largest\nunit of Pol-\nish troops on\nOctober 6, 1939?\na) They surrendered to the Red Army.\nb) They were defeated by German forces.\nc) They went directly to the partisan struggle.\nd) They were evacuated to neighboring countries.\na\nHitler believed that the Soviet Union could be de-\nfeated in a fast and relentless assault that capitalised\non the Soviets’ ill-prepared state\nWhat was the\nstate of the\nSoviet Union\nbefore Germany\nattacked it in\nJune 1941?\na) Prepared for war\nb) Ill-prepared for war\nc) Neutral\nd) At war with Germany\nb\nHitler believed that his success there would bring\nBritain to the negotiating table, an event which would\nend the war altogether.\nWhat did Hitler\nhope to achieve\nby defeating the\nSoviet Union?\na) Gain more territory\nb) End the war\nc) To weaken the Soviet Union\nd) To gain allies\nb\n11\nTable 3: MCQs generated by Llama 2\nSource Question Options Correct\nTo deter Hitler, Britain and France announced that\nan invasion would mean war...\nWhat did Britain and\nFrance do to deter\nHitler from invading\nPoland?\na) They announced that they would declare war on Germany if it invaded Poland.\nb) They tried to convince the Soviet Union to join deterrence against Hitler.\nc) They signed defense treaties with Poland and publicly announced their support.\nd) They tried to negotiate with Hitler and find a peaceful solution.\na\nHitler believed that the Soviet Union could be de-\nfeated in a fast and relentless assault that capitalised\non the Soviets’ ill-prepared state and he hoped that\nhis success there would bring Britain to the negoti-\nating table, an event which would end the war alto-\ngether.\nWhat did Hitler hope\nto achieve by attack-\ning the Soviet Union?\na) He wanted to end the war with Britain.\nb) He wanted to expand Germany’s territory.\nc) It wanted to demonstrate its military strength.\nd) He wanted to secure a strategic alliance with the Soviet Union.\nd\nLast line of the text When did the first\ncombat of the Flying\nTigers take place?\na) December 7, 1941\nb) January 23, 1942\nc) February 15, 1943\nd) March 1, 1944\nb\nLine 5 of the text Why did the United\nStates declared war on\nJapan?\na) Because of the attacks on Malaysia, Singapore, and Hong Kong.\nb) Because of the attacks on Pearl Harbor\nc) Because of unprovoked acts of war against the government and people of the United\nStates of America\nd) Because of a combination of reasons\nb\nLine 3 of the text What was the U.S.\ngovernment’s response\nto Japan’s invasion of\nChina in 1937?\na) The U.S. government imposed a total embargo on all U.S.-Japan trade.\nb) The U.S. government asked Japan to withdraw its troops from China and Indochina.\nc) The U.S. government provided military aid to China to help it fight against Japan.\nd) The U.S. government remained neutral and took no action against Japan’s invasion.\na\nLast line of the text Who commanded the\nFlying Tigers during\ntheir first fight after\nthe attack on Pearl\nHarbor?\na) Claire Lee Chennault\nb) John F.Kennedy\nc) Douglas MacArthur\nd) Dwight D. Eisenhower\na\nThe last larger unit of Polish troops capitulated on\nOctober 6, 1939, near Kock...\nWhen did the last\nlargest unit of Polish\ntroops capitulate?\na) On September 17, 1939.\nb) On October 6, 1939.\nc) In the spring of 1940.\nd) Never.\nb\n12\nTable 4: MCQs generated by Mistral\nSource Question Options Correct\nCanada declared war on Japan on the\nevening of 7 December, and a royal\nproclamation affirmed the declaration\nthe next day.\nWhen did the offi-\ncial Japanese dec-\nlaration of war on\nthe United States\nand the British\nEmpire appear in\nJapanese newspa-\npers?\na) December 6, 1941\nb December 7, 1941\nc) December 8, 1941\nd) December 9, 1941\nc\nHitler admitted to his generals on 23\nMay 1939 that his reason for invading\nPoland was not Danzig: ”Danzig is not\nthe issue at stake. It is a matter of ex-\ntending our living space in the East...”.\nAccording to\nHitler, what was\nthe main rea-\nson for invading\nPoland?\na) Gain control of Danzig\nb) Expand living space in the east\nc) Establish a military alliance\nd) Impose economic sanctions\nb\nGermany invaded Poland on Septem-\nber 1, 1939, and rejected the British\nand French demands for it to withdraw,\nwhich resulted in both to declare war\non September 3, 1939, in accordance\nwith the defence treaties with Poland\nthat they had signed and publicly an-\nnounced.\nWhen did Ger-\nmany invade\nPoland, starting\nWorld War II?\na) August 1, 1939\nb) September 1, 1939\nc) October 1, 1939\nd) November 1, 1939\nb\nThe text mentions, ”In September 1940,\nthe Japanese invaded Vichy French In-\ndochina and occupied Tonkin”\nIn what year\ndid the Japanese\ninvade French\nIndochina from\nVichy?\na) 1939\nb) 1940\nc) 1941\nd) 1942\nb\nOn 7 December 1941, without a declara-\ntion of war, the Imperial Japanese Navy\nattacked Pearl Harbor...\nOn what date\ndid the Imperial\nJapanese Navy\nattack Pearl\nHarbor without\ndeclaring war?\na) December 6, 1941\nb) December 7, 1941\nc) December 8, 1941\nd) December 9, 1941\nb\nReacting to Japanese pressure on French\nauthorities of French Indochina to stop\ntrade with China, the US began restrict-\ning trade with Japan in July 1940.\nWhat was the\nevent that\nprompted the\nUnited States to\nbegin restricting\ntrade with Japan\nin July 1940?\na) Japanese invasion of China\nb) Japanese pressure on French Indochina\nc) British trade restrictions\nd) Dutch oil embargo.\nb\nOn 7 December 1941, without a dec-\nlaration of war, the Imperial Japanese\nNavy attacked Pearl Harbor with the\naim of destroying the main American\nbattle fleet at anchor.\nWhat event oc-\ncurred on Decem-\nber 7, 1941 with-\nout a declaration\nof war, aiming to\ndestroy the main\nU.S. battle fleet\nat anchor?\na) The Battle of Midway\nb) The Doolittle Raid\nc) The Attack on Pearl Harbor\nd) The Battle of Okinawa\nc\n13",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6872166395187378
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I119003972",
      "name": "Roma Tre University",
      "country": "IT"
    }
  ],
  "cited_by": 18
}