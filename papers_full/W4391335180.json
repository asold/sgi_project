{
  "title": "Generative Expressive Robot Behaviors using Large Language Models",
  "url": "https://openalex.org/W4391335180",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4287560114",
      "name": "Mahadevan, Karthik",
      "affiliations": [
        "University of Toronto"
      ]
    },
    {
      "id": null,
      "name": "Chien, Jonathan",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4224913710",
      "name": "Brown, Noah",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A1890289233",
      "name": "Xu Zhuo",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2214562279",
      "name": "Parada, Carolina",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2146416267",
      "name": "Xia, Fei",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A3086740516",
      "name": "Zeng, Andy",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4223491951",
      "name": "Takayama, Leila",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2744361426",
      "name": "Sadigh, Dorsa",
      "affiliations": [
        "Google (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4230193733",
    "https://openalex.org/W2550647949",
    "https://openalex.org/W2529170714",
    "https://openalex.org/W4313952519",
    "https://openalex.org/W2940678350",
    "https://openalex.org/W2915273306",
    "https://openalex.org/W2085436006",
    "https://openalex.org/W1969784320",
    "https://openalex.org/W2110727992",
    "https://openalex.org/W2012844124",
    "https://openalex.org/W4285414384",
    "https://openalex.org/W2024986232",
    "https://openalex.org/W3009953896",
    "https://openalex.org/W2789347656",
    "https://openalex.org/W2940721292",
    "https://openalex.org/W3130828650",
    "https://openalex.org/W4383097638",
    "https://openalex.org/W2516310909",
    "https://openalex.org/W2996457820",
    "https://openalex.org/W2980931192",
    "https://openalex.org/W2896381102",
    "https://openalex.org/W3029258967",
    "https://openalex.org/W4319653440",
    "https://openalex.org/W3161050098",
    "https://openalex.org/W4383108457",
    "https://openalex.org/W4312800400",
    "https://openalex.org/W2928416850",
    "https://openalex.org/W3009807960",
    "https://openalex.org/W2016721335",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W2137020039",
    "https://openalex.org/W4389667233",
    "https://openalex.org/W2892287882"
  ],
  "abstract": "People employ expressive behaviors to effectively communicate and coordinate their actions with others, such as nodding to acknowledge a person glancing at them or saying \"excuse me\" to pass people in a busy corridor. We would like robots to also demonstrate expressive behaviors in human-robot interaction. Prior work proposes rule-based methods that struggle to scale to new communication modalities or social situations, while data-driven methods require specialized datasets for each social situation the robot is used in. We propose to leverage the rich social context available from large language models (LLMs) and their ability to generate motion based on instructions or user preferences, to generate expressive robot motion that is adaptable and composable, building upon each other. Our approach utilizes few-shot chain-of-thought prompting to translate human language instructions into parametrized control code using the robot's available and learned skills. Through user studies and simulation experiments, we demonstrate that our approach produces behaviors that users found to be competent and easy to understand. Supplementary material can be found at https://generative-expressive-motion.github.io/.",
  "full_text": "Generative Expressive Robot Behaviors\nusing Large Language Models\nKarthik Mahadevan\nGoogle Deepmind\nkarthikm@dgp.toronto.edu\nJonathan Chien\nGoogle Deepmind\nchienj@google.com\nNoah Brown\nGoogle Deepmind\nnoahbrown@google.com\nZhuo Xu\nGoogle Deepmind\nzhuoxu@google.com\nCarolina Parada\nGoogle Deepmind\ncarolinap@google.com\nFei Xia\nGoogle Deepmind\nxiafei@google.com\nAndy Zeng\nGoogle Deepmind\nandyzeng@google.com\nLeila Takayama\nHoku Labs\ntakayama@hokulabs.com\nDorsa Sadigh\nGoogle Deepmind\ndorsas@google.com\nABSTRACT\nPeople employ expressive behaviors to effectively communicate\nand coordinate their actions with others, such as nodding to ac-\nknowledge a person glancing at them or saying â€œexcuse meâ€ to pass\npeople in a busy corridor. We would like robots to also demon-\nstrate expressive behaviors in human-robot interaction. Prior work\nproposes rule-based methods that struggle to scale to new commu-\nnication modalities or social situations, while data-driven methods\nrequire specialized datasets for each social situation the robot is\nused in. We propose to leverage the rich social context available\nfrom large language models (LLMs) and their ability to generate mo-\ntion based on instructions or user preferences, to generateexpressive\nrobot motion that is adaptable and composable, building upon each\nother. Our approach utilizes few-shot chain-of-thought prompting\nto translate human language instructions into parametrized con-\ntrol code using the robotâ€™s available and learned skills. Through\nuser studies and simulation experiments, we demonstrate that our\napproach produces behaviors that users found to be competent\nand easy to understand. Supplementary material can be found at\nhttps://generative-expressive-motion.github.io/.\nCCS CONCEPTS\nâ€¢ Computing methodologies â†’Online learning settings .\nKEYWORDS\nGenerative expressive robot behaviors, in-context learning, lan-\nguage corrections\nACM Reference Format:\nKarthik Mahadevan, Jonathan Chien, Noah Brown, Zhuo Xu, Carolina\nParada, Fei Xia, Andy Zeng, Leila Takayama, and Dorsa Sadigh. 2024. Gen-\nerative Expressive Robot Behaviors using Large Language Models. In Pro-\nceedings of the 2024 ACM/IEEE International Conference on Human-Robot\nPermission to make digital or hard copies of part or all of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\nHRI â€™24, March 11â€“14, 2024, Boulder, CO, USA\nÂ© 2024 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-0322-5/24/03.\nhttps://doi.org/10.1145/3610977.3634999\nLook at human Nod Turn green lights on\nLanguage instructions\ntime\nAcknowledge a person walking by. \nYou cannot speak.\n def acknowledge_person_walking_by():\n   human_location = find_human()\n   look_at(human_location)\n   move_head(direction=down)\n   time.sleep(1.0)\n   move_head(direction=up)\n   change_light(color=green)\nRobot expressive behavior\nIt is polite to acknowledge a person by \nglancing at them, nodding, and saying hello.\nAs the robot cannot speak, it can use its head \nto nod and light strip to display friendly colors.\nGenerative Expressive Motion (GenEM)\nCode generation\nHuman expressive behavior\nFigure 1: We present Generative Expressive Motion (GenEM),\na new approach to autonomously generate expressive robot\nbehaviors. GenEM takes a desired expressive behavior (or a\nsocial context) as language instructions, reasons about hu-\nman social norms, and generates control code for a robot\nusing pre-existing robot skills and learned expressive be-\nhaviors. Iterative feedback can quickly modify the behavior\naccording to user preferences. Here, the * symbols denote\nfrozen large language models.\nInteraction (HRI â€™24), March 11â€“14, 2024, Boulder, CO, USA. ACM, New York,\nNY, USA, 10 pages. https://doi.org/10.1145/3610977.3634999\n1 INTRODUCTION\nPeople employ a wide range of expressive behaviors to effectively\ninteract with others on a daily basis. For instance, a person walking\nby an acquaintance may briefly glance at them and nod to acknowl-\nedge their presence. A person might apologetically say, â€œexcuse me!â€\nto squeeze through a tight hallway, where a group of people are\nconversing. In much the same manner, we would like robots to also\ndemonstrate expressive behaviors when interacting with people.\nRobots that donâ€™t have expressive capabilities will need to re-plan\ntheir paths to avoid the crowded hallway. On the other hand, robots\nthat have expressive capabilities might actually be able to persuade\nthe group of people to make room for them to squeeze by, thereby\nimproving the robotâ€™s efficiency in getting its job done.\nPrior work has demonstrated the value of expressive robot behav-\niors, and explored approaches for generating behaviors for various\narXiv:2401.14673v2  [cs.RO]  30 Jan 2024\nHRI â€™24, March 11â€“14, 2024, Boulder, CO, USA Karthik Mahadevan et al.\npurposes and contexts, including general-purpose use [8], manipu-\nlation settings, where transparency is important [21], and everyday\nscenarios where social norms must be observed (such as interacting\nwith a receptionist) [ 36]. Approaches can be rule- or template-\nbased [2, 7, 33], which often rely on a rigid template or a set of\nrules to generate behaviors. This often leads to robot behaviors that\ncan be expressive, but do not scale to new modalities or variations\nof human preferences. On the other hand, data-driven techniques\noffer the promise of flexibility and the ability to adapt to varia-\ntions. Prior work have studied data-driven techniques that generate\nexpressive motion [42], but these methods also have their short-\ncomings as they often need specialized datasets for each social\ninteraction where a particular behavior is used (e.g., for affective\nrobot movements [41, 42]).\nOur goal is to enable robots to generate expressive behavior that\nis flexible: behaviors that can adapt to different human preferences,\nand be composed of simpler behaviors. Recent work show that large\nlanguage models (LLMs) can synthesize code to control virtual [44]\nand embodied agents [ 25, 39], help design reward functions [ 22,\n48], enable social and common-sense reasoning [20], or perform\ncontrol and sequential decision making tasks through in-context\nlearning [10, 29, 30] by providing a sequence of desirable inputs, and\noutputs in the prompt. Our key insight is to tap into the rich social\ncontext available from LLMs to generate adaptable and composable\nexpressive behavior. For instance, an LLM has enough context\nto realize that it is polite to make an eye contact when greeting\nsomeone. In addition, LLMs enable the use of corrective language\nsuch as â€œbend your arm a bit more!â€ and the ability to generate\nmotion in response to such instructions. This makes LLMs a useful\nframework for autonomously generating expressive behavior that\nflexibly respond to and learn from human feedback in human-robot\ninteraction settings.\nLeveraging the power and flexibility provided by LLMs, we pro-\npose a new approach, Generative Expressive Motion (GenEM), for\nautonomously generating expressive robot behaviors. GenEM uses\nfew-shot prompting and takes a desired expressive behavior (or a\nsocial context) as language instructions, performs social reason-\ning (akin to chain-of-thought [45]), and finally generates control\ncode for a robot using available robot APIs. GenEM can produce\nmultimodal behaviors that utilize the robotâ€™s available affordances\n(e.g., speech, body movement, and other visual features such as\nlight strips) to effectively express the robotâ€™s intent. One of the\nkey benefits of GenEM is that it responds to live human feedback\nâ€“ adapting to iterative corrections and generating new expressive\nbehaviors by composing the existing ones.\nIn a set of online user studies, we compared behaviors generated\non a mobile robot using two variations of GenEM, with and without\nuser feedback (a non-expert in HRI behavior design), to a set of\nbehaviors designed by a professional character animator (or the\noracle animator ). We show that behaviors generated by GenEM and\nfurther adapted with user feedback were positively perceived by\nusers, and in some cases better perceived than the oracle behaviors.\nIn additional experiments with the mobile robot and a simu-\nlated quadruped, we show that GenEM: (1) performs better than\na version where language instructions are directly translated into\ncode, (2) allows for the generation of behaviors that are agnostic to\nembodiment, (3) allows for the generation of composable behaviors\nthat build on simpler expressive behaviors, and finally, (4) adapt to\ndifferent types of user feedback.\n2 RELATED WORK\nExpressive Behavior Generation. Researchers have made signif-\nicant efforts towards generating socially acceptable behavior for\nboth robots and virtual humans. These can largely categorized into\nrule-based, template-based, and data-driven [33] behavior genera-\ntion approaches. We define rule-based approaches as those that\nrequire a formalized set of rules and operations (typically provided\nby a person) which are used to generate subsequent robot behavior.\nRule-based approaches enable behavior generation through for-\nmalized sets of rules and operations [ 2]. Some methods include\ninterfaces that lets users manually specify interaction rules and\nlogic [ 4, 5, 23, 24, 35]. Other methods work by observing and\nmodelling humans [ 3, 13, 14, 18]. Despite their use, rule-based\napproaches face several issues, including limited expressivity in the\ngenerated behavior due to the requirement of formal rules, and the\nreduced ability to produce multimodal behaviors as the number\nof modalities increases [33]. Template-based methods formulate\ngeneric templates for interaction by learning from traces of interac-\ntion data [7, 11]. Templates can translate few examples of human\ntraces into reusable programs through program synthesis [19, 34].\nTraces can be collected by observing humans interacting [34, 36],\nor through approaches such as sketching [ 37] or tangibles on a\ntabletop [38]. Overall, prior rule- and template-based methods en-\nforce strong constraints to enable behavior generation but are lim-\nited in their expressivity. In contrast, GenEM enables increased\nexpressivity in the initial behavior generation as well as iterative\nimprovements through live user feedback.\nOn the other hand, data-driven approaches produce behaviors\nusing models trained on data. Some methods learn interaction logic\nthrough data and use this to produce multimodal behaviors via\nclassical machine learning methods [9, 15, 27]. Other methods train\non hand-crafted examples through generative models [28, 42]. For\ninstance, predicting when to use backchanneling behaviors (i.e.,\nproviding feedback during conversation such as by nodding) has\nbeen learned through batch reinforcement learning [17] and recur-\nrent neural networks [31]. Lastly, recent work has investigated how\nto learn cost functions for a target emotion from user feedback [49],\nor even learn an emotive latent space to model many emotions [40].\nHowever, these approaches are data inefficient and require special-\nized datasets per behavior to be generated, while GenEM is able\nto produce a variety of expressive behaviors with a few examples\nthrough in-context learning.\nLLMs for Robot Planning and Control. Recent work has achieved\ngreat success by leveraging LLMs in downstream robotics tasks\nspecifically by providing sequences of desirable input-output pairs\nin context [10, 29, 30]. In addition, LLMs have been used for long-\nhorizon task planning [1, 26], and can react to environmental and\nhuman feedback [16]. LLMs have been leveraged for designing re-\nward functions for training reinforcement learning agents [22, 48].\nResearch has also shown that LLMs can enable social and common-\nsense reasoning [20] as well as infer user preferences by summariz-\ning interactions with humans [47]. Most relevant to our approach\nare prior work where LLMs synthesize code to control virtual [44]\nGenerative Expressive Robot Behaviors\nusing Large Language Models HRI â€™24, March 11â€“14, 2024, Boulder, CO, USA\n[Scenario]\nAcknowledge the person walking \nby. You cannot speak.\nExpressive Instruction \nFollowing\nRules:\n-[Scenario] describes the scenario \nin which you find yourself.\nâ€¦\nExamples\nâ€¦\nRules:\n- Translate [What human would do] \ninto [What robot should do] with the \n[Robot capabilities] listed.\nâ€¦\nCapabilities\n- Head: The head can pan between \na given range.\nâ€¦\nExamples\nâ€¦\nHuman to Robot \nExpressive Motion\nRobot Expressive\nMotion to Code\nRules:\n- Generate Python code to execute a \nstep-by-step procedure of [What \nrobot should do]â€¦\nâ€¦\nRobot API\ndef move_head_to_pose(goal_pose):\nâ€¦\nExamples\nâ€¦\nPropagating Human\nFeedback\n def acknowledge_person_walking_by():\n  human_location = find_human()\n  lo ok_at(human_location)\n  mo ve_head(direction=down)\n  time .sleep(1.0)\n  mo ve_head(direction=up)\n  change_light( color=green)\n[Feedback]\nMake sure to keep looking at the \nperson as they walk away.\nh\nhpre\nlin\nrpre c\nrexp c\nf\nf\ni\ni\npre\n= Frozen LLM\n^\nFigure 2: Generative Expressive Motion. Given a language instruction ğ‘™ğ‘–ğ‘› , the Expressive Instruction Followingmodule reasons\nabout the social norms and outputs how a human might express this behavior ( â„). This is translated into a procedure for robot\nexpressive behavior using a prompt describing the robotâ€™s pre-existing capabilities ( ğ‘Ÿğ‘ğ‘Ÿğ‘’ ) and any learned expressive behaviors.\nThen, the procedure is used to generate parametrized robot code ğ‘that can be executed. The user can provide iterative feedback\nğ‘“ğ‘– on the behavior which is processed to determine whether to re-run the robot behavior module first followed by the code\ngeneration module or just the code generation module. Note: * shown on top of all the gray modules denotes them as frozen LLMs.\nand robotic agents [25, 39] by using existing APIs to compose more\ncomplex robot behavior as programs. We are also encouraged by\nwork demonstrating that language can be used to correct robot\nmanipulation behaviors online [ 6]. Taken together, we propose\nto leverage the rich social context available from LLMs, and their\nability to adapt to user instructions, to generate expressive robot\nbehaviors. To our knowledge, LLMs have not previously been used\nto generate expressive robot behaviors that adapt to user feedback.\n3 GENERATIVE EXPRESSIVE MOTION\nProblem Statement. We aim to tackle the problem of expressive\nbehavior generation that is both adaptive to user feedback and\ncomposable so that more complex behaviors can build on simpler\nbehaviors. Formally, we define being expressive as the distance\nbetween some expert expressive trajectory that could be generated\nby an animator (or demonstrated) ğœexpert and a robot trajectory ğœ.\ndist(ğœ,ğœexpert)can be any desirable distance metric between the\ntwo trajectories, e.g., dynamic time warping (DTW). GenEM aims\nto minimize this distance ğ‘‘âˆ—= min dist(ğœ,ğœexpert).\nOur approach (Figure 2) uses several LLMs in a modular fashion\nso that each LLM agent plays a distinct role. Later, we demonstrate\nthrough experiments that a modular approach yields better quality\nof behaviors compared to an end-to-end approach. GenEM takes\nuser language instructions ğ‘™ğ‘–ğ‘› âˆˆğ¿ as input and outputs a robot\npolicy ğœ‹ğœƒ , which is in the form of a parameterized code. Human\niterative feedback ğ‘“ğ‘– âˆˆğ¿can be used to update the policy ğœ‹ğœƒ . The\npolicy parameters get updated one step at a time given the feedback\nğ‘“ğ‘– , where ğ‘– âˆˆ{1,...,ğ¾ }. The policy can be instantiated from some\ninitial state ğ‘ 0 âˆˆğ‘†to produce trajectories ğœ = {ğ‘ 0,ğ‘0,...,ğ‘ ğ‘ âˆ’1,ğ‘ ğ‘ }\nor instantiations of expressive robot behavior. Below we describe\none sample iteration with human feedback ğ‘“ğ‘– . Please refer to Ap-\npendix A for full prompts.\nExpressive Instruction Following. The input to our approach is\na language instruction ğ‘™ğ‘–ğ‘› âˆˆğ¿, which can either be a description of\na social context where the robot needs to perform an expressive be-\nhavior by following social norms (e.g., â€œA person walking by waves\nat you. â€)or an instruction that describing an expressive behavior\nto be generated (e.g., â€œNod your headâ€). The input prompt is of\nthe form ğ‘¢ = [â„ğ‘ğ‘Ÿğ‘’ ,ğ‘™ğ‘–ğ‘› ]where â„ğ‘ğ‘Ÿğ‘’ is the prompt prefix that adds\ncontext about the role of the LLM and includes few-shot examples.\nThe output of the LLM call is a string of the form â„= [â„ğ‘ğ‘œğ‘¡ ,â„ğ‘’ğ‘¥ğ‘ ]\nconsisting of Chain-of-Thought reasoningâ„ğ‘ğ‘œğ‘¡ [45] and the human\nexpressive motion â„ğ‘’ğ‘¥ğ‘ in response to the instruction. For example,\nfor ğ‘™ğ‘–ğ‘› = â€œAcknowledge a person walking by. You cannot speak. â€ , the\nExpressive Instruction Following module would output â„ğ‘’ğ‘¥ğ‘ = Make\neye contact with the person. Smile or nod to acknowledge their pres-\nence. Examples of â„ğ‘ğ‘œğ‘¡ could be: â€œThe person is passing by and itâ€™s\npolite to acknowledge their presence. Since I cannot speak, I need to\nuse non-verbal communication. A nod or a smile is a universal sign\nof acknowledgement. â€\nFrom Human Expressive Motion to Robot Expressive Motion.\nIn the next step, we use an LLM to translate human expressive mo-\ntion â„ to robot expressive motion ğ‘Ÿ. The prompt takes the form\nğ‘¢ = [ğ‘Ÿğ‘ğ‘Ÿğ‘’ ,ğ‘™ğ‘–ğ‘›,â„,ğ‘Ÿğ‘–âˆ’1ğ‘œğ‘ğ‘¡ , Ë†ğ‘“ğ‘–âˆ’1ğ‘œğ‘ğ‘¡ ]where ğ‘Ÿğ‘ğ‘Ÿğ‘’ is the prompt prefix\nsetting context for the LLM, contains few-shot examples, and de-\nscribes the robotâ€™s capabilities some of which are pre-defined (e.g.,\nthe ability to speak or move its head) and others which are learned\nfrom previous interactions (e.g., nodding or approaching a person).\nOptionally, the prompt can include the response from a previous\nstep ğ‘Ÿğ‘–âˆ’1 and response to user iterative feedback from a previous\nstep Ë†ğ‘“ğ‘–âˆ’1. The output is of the form ğ‘Ÿ = [ğ‘Ÿğ‘ğ‘œğ‘¡ ,ğ‘Ÿğ‘’ğ‘¥ğ‘ ]consisting of\nthe LLMâ€™s reasoning and the procedure to create expressive robot\nmotion. An example response ğ‘Ÿğ‘’ğ‘¥ğ‘ could include: â€œ1) Use the headâ€™s\npan and tilt capabilities to face the person who is walking by. 2) Use\nthe light strip to display a pre-programmed pattern that mimics a\nsmile or nod. â€. An example of ğ‘Ÿğ‘ğ‘œğ‘¡ could be: â€œThe robot can use its\nheadâ€™s pan and tilt capabilities to make \"eye contact\" with the person.\nThe robot can use its light strip to mimic a smile or nod. â€ .\nTranslating Robot Expressive Motion to Code. In the following\nstep, we use an LLM to translate the step-by-step procedure of how\nto produce expressive robot motion into executable code. We pro-\npose a skill library in a similar fashion to that of Voyager [44] con-\ntaining existing robot skill primitives, and parametrized robot code\nğœ‹ğœƒ representing previously learned expressive motions. To facilitate\nthis, the prompt encourages modular code generation by provid-\ning examples where small, reusable functions with docstrings and\nnamed arguments are used to generate more complex functions that\ndescribe an expressive behavior. To generate code, the prompt to\nHRI â€™24, March 11â€“14, 2024, Boulder, CO, USA Karthik Mahadevan et al.\nthe LLM takes the formğ‘¢ = [ğ‘ğ‘ğ‘Ÿğ‘’ ,ğ‘™ğ‘–ğ‘›,â„ğ‘’ğ‘¥ğ‘ ,ğ‘Ÿğ‘’ğ‘¥ğ‘,ğ‘– âˆ’1ğ‘œğ‘ğ‘¡ ,ğ‘ğ‘–âˆ’1ğ‘œğ‘ğ‘¡ , Ë†ğ‘“ğ‘–âˆ’1,\nğ‘Ÿğ‘’ğ‘¥ğ‘ ]. Here, ğ‘ğ‘ğ‘Ÿğ‘’ provides context about its role as a code gener-\nating agent to the LLM, includes the robotâ€™s current skill library,\nand contains few-shot examples. Optionally, the expressive robot\nmotion ğ‘Ÿğ‘’ğ‘¥ğ‘,ğ‘– âˆ’1, and codeğ‘ğ‘–âˆ’1 from a previous step can be provided\nas well as LLM output Ë†ğ‘“ğ‘–âˆ’1 responding to the user feedback ğ‘“ğ‘–âˆ’1 .\nThe output ğ‘is parametrized robot code representing the policy ğœ‹ğœƒ\nfor the expressive behavior (see Figure 2 for sample output). Later,\nthe generated code can be incorporated into the robotâ€™s skill library\nto utilize in future expressive behavior generations.\nPropagating Human Feedback. In the final (optional) step, we\nuse an LLM to update the generated expressive behavior in response\nto human feedback ğ‘“ğ‘– if the user is not satisfied with the generated\nbehavior. The prompt is of the formğ‘¢ = [ğ‘“ğ‘ğ‘Ÿğ‘’ ,ğ‘™ğ‘–ğ‘›,ğ‘Ÿğ‘’ğ‘¥ğ‘ ,ğ‘,ğ‘“ ğ‘– ], where\nğ‘“ğ‘ğ‘Ÿğ‘’ provides context to LLM, and includes both the procedure for\nexpressive robot motion ğ‘Ÿğ‘’ğ‘¥ğ‘ and the generated code ğ‘. The output\nis of the form ğ‘“ = [ğ‘“ğ‘ğ‘œğ‘¡ , Ë†ğ‘“ğ‘– ]and includes the LLMâ€™s reasoning and\nthe changes Ë†ğ‘“ğ‘– needed to improve the current expressive motion\nbased on human feedback. The output also classifies whether the\nchanges require an iterative call to modify the procedure for gener-\nating the robotâ€™s expressive behavior ğ‘Ÿ and then translating it to\ncode ğ‘, or just modifying the generated code ğ‘.\nFor example, the user could state ğ‘“ğ‘– = â€œWhen you first see the\nperson, nod at them. â€ , and the output Ë†ğ‘“ğ‘– could be: â€œ[Change: What\nrobot should do]...As soon as the robot sees the person, it should nod at\nthem. After nodding, the robot can use its light strip to display a pre-\nprogrammed pattern that mimics a smile or nod... â€ . As an example,\nğ‘“ğ‘ğ‘œğ‘¡ could state: â€œ The feedback suggests that the robotâ€™s action of\nacknowledging the person was not correct. This implies that the robot\nshould nod at the person when it first sees them. â€\n4 USER STUDIES\nWe conducted two user studies to assess whether our approach,\nGenEM, can be used to generate expressive behaviors that are\nperceivable by people. We generated two versions of behaviors:\nGenEM, and GenEM with iterative Feedback (or GenEM++). In both\nstudies, all comparisons were made against behaviors designed by\na professional animator and implemented by a software developer,\nwhich we term the oracle animator . In the first study , our goal was\nto assess whether behaviors that are generated using GenEM and\nGenEM++ would be perceived similarly to the behaviors created\nusing the oracle animator. In the second study , we attempted to\ngenerate behaviors using GenEM and GenEM++ that were similar\nto the behaviors created using the oracle animator. Both studies aim\nto demonstrate that our approach is adaptable to human feedback.\nBehaviors. All behaviors were generated on a mobile robot plat-\nform (please see website 1 for full clips). The robot has several\ncapabilities that can be used to generate behaviors through existing\nAPIs, including a head that can pan and tilt, a base that can trans-\nlate, rotate, and navigate from point to point, a light strip that can\ndisplay different colors and patterns, and finally, a speech module\nthat can generate utterances and nonverbal effects. To enable the\ncomparison of behaviors produced in the three conditions â€“ oracle\nanimator, GenEM, and GenEM++, we recorded video clips of each\n1https://generative-expressive-motion.github.io/\nbehavior (see Figure 3). To ensure consistency across conditions,\nbehaviors in each condition were recorded in the same physical loca-\ntions under similar lighting conditions. The GenEM and GenEM++\nbehaviors were generated by sampling OpenAIâ€™s GPT-4 APIs for\ntext completion [32] (gpt-4-0613) with the temperature set to 0.\nStudy Procedure. After providing informed consent, participants\ncompleted an online survey to evaluate the robotâ€™s expressive be-\nhaviors in both studies. The survey is divided into three sections\n(one per behavior condition) and clips within each condition ran-\ndomly appeared. To minimize ordering effects, a Balanced Latin\nSquare design (3 x 3) was used. For each behavior in each condition,\nparticipants watched an unlabeled video clip 1, and then answered\nquestions. All participants received remuneration after the study.\nMeasures. In both studies, participants completed a survey to\nassess each behavior, answering three 7-point Likert scale questions\nassessing their confidence on their understanding of the behavior,\nthe difficulty in understanding what the robot is doing, and the\ncompetency of the robotâ€™s behavior. Participants also provided an\nopen-ended response describing what behavior they believed the\nrobot was attempting to express.\nAnalysis. One-way repeated-measures ANOVA were performed\non the data with post-hoc pairwise comparisons where there were\nsignificant differences with Bonferroni corrections applied. When\nreporting comparisons between conditions, we define instances as\npairwise significant conditions for at least one of the three Likert-\nscale questions asked about a behavior.\n4.1 Study 1: Benchmarking Generative\nExpressive Motion\nTo determine whether our approach produces expressive behaviors\nthat people can perceive, we conducted a within-subjects user study\nwith thirty participants (16 women, 14 men), aged 18 to 60 (18-25:\n3, 26-30: 9, 31-40: 9, 41-50: 7, 51-60: 2). One participant did not\ncomplete the entire survey and their data was omitted.\nBehaviors. We generated ten expressive behaviors (see Figure 3)\nranging in complexity: Nod, shake head (Shake), wake up (Wake),\nexcuse me ( Excuse), recoverable mistake ( Recoverable), unrecov-\nerable mistake (Unrecoverable), acknowledge person walking by\n(Acknowledge), follow person (Follow), approach person (Approach)\nand pay attention to person (Attention). The input included a one-\nline instruction (e.g., Respond to a person saying, â€œCome here. You\ncannot speak. â€).\nConditions. The oracle animator condition consisted of profes-\nsionally animated behaviors that were implemented on the robot\nthrough scripting. To create the GenEM behaviors, we sampled our\napproach five times to generate five versions of each behavior. Since\nthe behaviors were sampled with a temperature of 0, they shared\nsignificant overlap with small variations amongst them (due to non-\ndeterminism in GPT-4 output; please see Appendix C for samples\ngenerated using the same prompt). Then, six participants experi-\nenced in working with the robot were asked to rank them. The\nbest variation for each behavior was included as part of the GenEM\nbehaviors. To generate the GenEM++ behaviors, we recruited one\nparticipant experienced in using the robot (but inexperienced in\nHRI behavior design) and asked them to provide feedback on the\nbest rated version of each behavior. Feedback was used to iteratively\nGenerative Expressive Robot Behaviors\nusing Large Language Models HRI â€™24, March 11â€“14, 2024, Boulder, CO, USA\n1\n2\n3\n4\n1\n1\n1 23\n4\n 1\n3\n2\n21 3\n1\n2\n3\n2\n3\n1\n1\n2\n3\n4\n2 4\n1\n3\n12 3\n 1\n2\n Nodding its head in acknowledgment.\n  Robot shakes head to convey confusion.\n  Robot replies to â€œHey, robot.â€\n  Robot responds to person in the way.\n  Robot makes mistake it can recover from. \n Robot makes mistake it cannot recover from.\n Robot acknowledges person walking by.\nRobot responds to person asking to follow and  \nlets them know if theyâ€™re too far ahead.\nRobot acknowledges person walking by. They \nstop so the robot engages them.\n Robot approaches person after being asked to.\n Robot pays attention to person teaching it.\nRobot approaches person and actively listens \nto them as they demonstrate a task.\nA A B B\nFigure 3: Behaviors tested in the two user studies where the behaviors labelled in green denote those unique to the first study\nand behaviors labelled in blue denote those unique to the second study. The remaining behaviors (8) were common among the\ntwo studies.\nmodify the expressive behavior until the participant was satisfied\nwith the result, or upon reaching the maximum number of feed-\nback rounds (n = 10). We note that although participants rated the\nbehaviors in the studies, the behavior generation is personalized to\nthe user who provided the initial feedback, which may not reflect\nthe preferences of all potential users (e.g., study participants).\nHypotheses. We hypothesized that the perception of the GenEM++\nbehaviors would not differ significantly from the oracle animator\nbehaviors (H1). We also hypothesized that the GenEM behaviors\nwould be less well-received compared to the GenEM++ and the\noracle animator behaviors (H2).\nQuantitative Findings. Figure 4 summarizes participantsâ€™ re-\nsponses to the survey questions for each behavior. The results\nshow that the GenEM++ behaviors were worse than the oracle ani-\nmator behaviors in 2/10 instances (Shake and Follow). In contrast,\nthe GenEM++ behaviors received higher scores than the oracle\nanimator behaviors in 2/10 instances (Excuse and Approach). Hence,\nH1 is supported by our data â€“ the GenEM++ behaviors were well\nreceived and the oracle animator behaviors were not significantly\nbetter received than the GenEM++ behaviors.\nThe GenEM behaviors were worse received compared to the\noracle animator behaviors in 2/10 instances ( Acknowledge Walk\nand Follow) whereas the GenEM behaviors were better received\nthan the oracle animator behaviors in 2/10 instances (Excuse and\nApproach). This was surprising because user feedback was not\nincorporated into the behavior generation in this condition. Besides\n1/10 instances ( Shake), there were no significant differences in\nthe perceptions of the GenEM and GenEM++ behaviors. Hence,\nwe did not find support for H2. We performed equivalence tests\n(equivalence bound: +/- 0.5 Likert points) but did not find any\nsets of behaviors to be equivalent. Overall, the results support\nthe finding that GenEM (even with an untrained user providing\nfeedback) produces expressive robot behaviors that users found to\nbe competent and easy to understand.\n4.2 Study 2: Mimicking the Oracle Animator\nWe conducted an additional within-subjects user study with twenty\nfour participants (21 men, 2 women, 1 prefer not to say), aged\n18-60 (18-25: 4, 26-30: 3, 31-40: 12, 41-50: 4, 51-60: 1) to assess\nwhether using GenEM to generate behaviors that resembled the\noracle animator would be perceived differently. One participant did\nnot complete the entire survey and their data was omitted.\nBehaviors. We generated ten expressive behaviors ranging in com-\nplexity, with eight overlapping2 behaviors from the first study (see\nFigure 3): nod (Nod), shake head (Shake), wake up (Wake), excuse\nme (Excuse), recoverable mistake (Recoverable), unrecoverable mis-\ntake (Unrecoverable), acknowledge person walking by (Acknowledge\nWalking), acknowledge person stopping by (Acknowledge Stop ), fol-\nlow person (Follow), and teaching session (Teach). Behaviors that\nwere different from the first study were chosen to add further com-\nplexity â€“ e.g., longer single-turn interactions such as teaching, that\nstarted with a person walking up a robot, teaching it a lesson, and\nlastly the robot acknowledging that it understood the personâ€™s in-\nstructions. Unlike in the first study, the prompts were more varied\nand sometimes included additional descriptions such as for the\nmore complex behaviors (see Appendix B for full prompts for\neach behavior). To generate each GenEM behavior, we sampled our\napproach ten times after which an experimenter selected the ver-\nsion that appeared most similar to the equivalent oracle animator\nbehavior when deployed on the robot. To create each GenEM++\nbehavior, an experimenter refined the GenEM behavior through\niterative feedback until it appeared similar to the equivalent ora-\ncle animator behavior or after exceeding the maximum number of\nfeedback rounds (n = 10) 1.\nHypotheses. We hypothesized that user perceptions of the GenEM++\nbehaviors would not significantly differ when compared to the ora-\ncle animator behaviors (H3). We also suppose that the behaviors\nin the GenEM condition would be perceived as worse than the\nGenEM++ and oracle animator behaviors (H4).\nQuantitative Findings. The results of the study are summarized\nin Figure 5. They show that the GenEM++ behaviors were worse\nreceived than the oracle animator behaviors in 2/10 instances (Ac-\nknowledge Walk and Follow) whereas the GenEM++ behaviors were\nmore positively received than the oracle animator in 2/10 instances\n(Excuse and Teach). Hence, our hypothesis is supported by the data\n2Some behaviors in the second study differ from the first study as they are too complex\nto express as a single line instruction which we maintained for consistency in the\nfirst study. Instead, in the first study, these complex behaviors were broken down into\nsimpler behaviors (e.g., teaching is equivalent to approaching and paying attention).\nHRI â€™24, March 11â€“14, 2024, Boulder, CO, USA Karthik Mahadevan et al.\n**\n* *\n*\n*\n*\n* * **** ****\n****\n*\n*\nFigure 4: Plots showing participantsâ€™ survey responses to three questions about each behavior (of 10) in each condition (of 3) in\nthe 1st user study. Bars at the top denote significant differences, where (*) denotes p<.05 and (**) denotes p<.001. Error bars\nrepresent standard error. The first plot shows the average score for each question across conditions. The arrows reflect the\ndirection in which better scores lie.\n**\n***\n** *\n**\n**\n** *\n**\n***\n*\n* *\n* *\n*\nFigure 5: Plots showing participantsâ€™ survey responses to three questions about each behavior (of 10) in each condition (of 3) in\nthe 2nd user study. Bars at the top denote significant differences, where (*) denotes p<.05 and (**) denotes p<.001. Error bars\nrepresent standard error. The first plot shows the average score for each question across conditions. The arrows reflect the\ndirection in which better scores lie.\n(H3) â€“ the GenEM++ behaviors well received and the oracle an-\nimator behaviors were not significantly better perceived. When\ncomparing the oracle animator behaviors and GenEM behaviors,\nthere were 4/10 instances where the GenEM behaviors were worse\nreceived (Wake, Acknowledge Walk , Acknowledge Stop , and Fol-\nlow), and 1/10 instances where the GenEM behaviors were more\npositively rated (Excuse). As with the first study, it is somewhat\nsurprising that the GenEM behaviors were better received than the\nbaselines in one instance; although they resemble them, they do not\ncapture all the nuances present in the oracle animator behaviors\nsince user feedback is not provided. Lastly, the GenEM behaviors\nwere rated worse than the GenEM++ behaviors in 2/10 instances\n(Wake and Teach) whereas there were 0/10 instances where the\nreverse was true. Hence, we did not find support for the last hy-\npothesis (H4). Upon performing equivalence tests (equivalence\nbound: +/- 0.5 Likert points), we did not find any sets of behaviors\nto be equivalent. Overall, the findings suggest that expressive robot\nbehaviors produced using our approach (with user feedback) were\nfound competent and easy to understand by users.\n5 EXPERIMENTS\nWe conducted a set of experiments to carefully study different as-\npects of GenEM. This includes ablations to understand the impact\nof our prompting structure and the modular calls to different LLMs\nversus an end-to-end approach. Further, through an experiment,\nwe demonstrate that GenEM can produce modular and composable\nbehaviors, i.e., behaviors that build on top of each other. The be-\nhaviors were generated by sampling OpenAIâ€™s GPT-4 APIs for text\nGenerative Expressive Robot Behaviors\nusing Large Language Models HRI â€™24, March 11â€“14, 2024, Boulder, CO, USA\nGenEM Ablated\nExecution Norms Execution Norms\nNod 5 0 5 2\nShake 5 0 5 2\nWake 4 2 3 0\nExcuse 5 3 0 -\nRecoverable 3 0 5 1\nUnrecoverable 5 0 5 0\nAcknowledge 5 1 5 0\nFollow 3 1 0 -\nApproach 5 1 5 3\nAttention 4 0 1 0\nTable 1: Ablations on the mobile robot platform showing the\nsuccessful attempts of behavior generation when sampling\neach prompt five times to compare our approach (without\nfeedback) against a variation without the Expressive Instruc-\ntion Followingmodule and subsequently the module trans-\nlating human expressive motion to robot expressive motion.\nThe Execuution column indicates the number of successful\nattempts (/5). The Norms column indicates the number of at-\ntempts where social norms were not appropriately followed\n(coded by the experimenter).\nExecution Norms\nNod 5 0\nShake 5 0\nWake 5 0\nExcuse 3 0\nRecoverable 5 2\nUnrecoverable 4 0\nAcknowledge 4 1\nFollow 2 2\nApproach 5 5\nAttention 1 0\nTable 2: Behaviors generated on the quadruped in simulation\nshowing successful attempts of behavior generation when\nsampling each prompt five times. The Execution column in-\ndicates the number of successful attempts (/5). The Norms\ncolumn indicates the number of attempts where social norms\nwere not properly observed (coded by the experimenter).\ncompletion [32] (gpt-4-0613) with the temperature set to 0. In addi-\ntion to our user study and experiments on the mobile manipulator,\nwe conducted further experiments using a quadruped simulated in\nGazebo/Unity via ROS (see Figure 6).\nAblations. We performed ablations to compare GenEM to an end-\nto-end approach that takes language instructions and makes one\ncall to an LLM to generate an expressive behavior. The ablations\nwere performed using existing APIs for the mobile robot. The be-\nhaviors examined were identical to the first user study along with\nthe prompts. Each prompt was sampled five times to generate be-\nhaviors and executed on the robot to verify correctness. Further, an\nexperimenter examined the code to check whether the behavior\ncode incorporated reasoning to account for human social norms.\nThe results for code correctness and social norm appropriateness\nFigure 6: Quadruped simulated in Gazebo performing the Re-\ncoverable mistakebehavior (top) and Unrecoverable mistake\n(bottom) generated by GenEM prior to feedback. After mak-\ning a recoverable mistake, the robot demonstrates it made a\nmistake by turning away, lowering its legs, and flashing red\nlights to convey regret but then returns to its initial position\nand flashes a green light. In contrast, an unrecoverable mis-\ntake causes the robot to lower its height, display red lights\nfor a brief period, and bow forwards and maintains this pose.\nare shown in Table 1. Overall, our approach produced higher suc-\ncess rates compared to the ablated variation where no successful\nruns were generated for 2 behaviors â€“ Excuse and Follow. For the\nExcuse behavior, the robot must check the userâ€™s distance and signal\nto a person that they are in its way. However, for the ablated varia-\ntion, the distance was never checked in the attempts. For theFollow\nbehavior, the code called functions that were not previously defined,\nand used the wrong input parameter type when calling robot APIs,\nresulting in zero successful attempts. Further, nearly all generated\nfunctions were missing docstrings and named arguments, which\ncould make it difficult to use them in a modular fashion for more\ncomplex behaviors (despite providing few-shot code examples).\nWe qualitatively observed that behaviors generated by GenEM\nreflected social norms, particularly for more complex behaviors,\nand looked similar for simpler behaviors. For instance, the Excuse\nbehavior generated by GenEM used the speech module to say,\nâ€œExcuse meâ€. For theAttention behavior, the ablated variations looked\nat the person, turned on the light strip, and then turned it off,\nwhereas the GenEM variations also incorporated periodic nodding\nto mimic â€œactive listeningâ€. For the Approach behavior, the GenEM\nvariations always incorporated a nod before moving towards the\nperson while the ablated variations never used nodding; instead\nlights were used in two instances.\nCross-Embodiment Behavior Generation. We sampled the same\nprompts in the first user study five times per behavior using API for\na simulated Spot robot. The results, summarized in Table 2, show\nthat we were able to generate most expressive behaviors using\nthe same prompts using a different robot platform with its own\naffordances and APIs. However, some generated behaviors such\nas Approach included variations where the robot navigated to the\nhumanâ€™s location instead of a safe distance near them, which would\nbe considered a social norm mismatch (possibly due to the lack of\na distance threshold parameter in the translate API), while some\ndid not account for the human (e.g., the robot rotating an arbitrary\nangle instead of towards the human for Attention). Overall, the\nsuccess rates hint at the generality of our approach to differing\nrobot embodiments.\nHRI â€™24, March 11â€“14, 2024, Boulder, CO, USA Karthik Mahadevan et al.\nEye\ncontact\nBlinking\nlights\nLook\naround\nShake\nhead\nNod\nhead\nAcknowledge Walk 5 - - - 5\nApproach 4 5 - - 0\nConfusion - 4 1 5 -\nTable 3: Number of times (out of 5 attempts) where\npreviously-learned behaviors (columns) are used when com-\nposing new behaviors (rows) using GenEM. Dashes indicate\nthat the given learned behavior API is not provided when\nprompting the creation of the new behavior.\nInsert\nactions\nSwap\nactions\nLoop\nactions\nRemove\ncapability\nExcuse 4 5 5 5\nApproach 4 5 5 3\nAcknowledge Stop 5 5 4 3\nTable 4: Success rates (out of 5 attempts) when providing dif-\nferent types of feedback to behaviors generated using GenEM,\nwhere: Insert actionsrequest a new action be added ahead\nof other actions, Swap actionsrequest to swap the order of\nexisting actions, Loop actionsrequest to add loops to repeat\nactions, and Remove capabilityrequests to swap an existing\naction with an alternate one.\nComposing Complex Expressive Behaviors. In the user studies,\nall behaviors were generated from scratch using few-shot examples\nand existing robot APIs. We attempted to generate more complex\nbehaviors using a set of learned expressive behaviors from pre-\nvious interactions â€” these skills (represented as functions with\ndocstrings) were appended to the prompts describing the robotâ€™s\ncapabilities (step 2 of our approach) as well as the robotâ€™s API (step\n3 of our approach). The learned behaviors used in the prompt were:\nnodding, making eye contact , blinking the light strip , looking around ,\nand shaking. We prompted GenEM to generate three behaviors,\nvarying in complexity: Acknowledge Walk, Approach, and express-\ning confusion (Confusion). All of these behaviors were generated\non the quadruped without providing feedback, using instructions\nthat contained a single line description of the desired behavior.\nWe sampled GenEM five times to assess the frequency with which\nlearned behaviors would be included in the outputted program. To\nassess success, an experimenter checked whether the generated\ncode utilized a combination of robot APIs and learned APIs (see Ta-\nble 3). For the approach behavior, it was surprising to note that the\nnod head behavior was never utilized whereas blinking lights were\nalways used. For expressing confusion, it was surprising that 4/5\ninstances generated code for looking around, but only 1/5 instances\nused the existing looking around behavior.\nAdaptability to Human Feedback. In the user studies, feedback\nhad some effect on the perception of the generated behaviors. Fur-\nther, we qualitatively observed that feedback could steer the behav-\nior generation in different ways. We studied this in an experiment\nwhere we generated three behaviors from the two prior studies:\nExcuse, Approach, and Acknowledge Stop. Each behavior was gen-\nerated using a single-line description as before, and without any\nlearned robot APIs. We attempted to modify the generated behavior\nthrough four types of feedback: (1) adding an action and enforcing\nthat it must occur before another action, (2) swapping the order\nof the actions, (3) making a behavior repeat itself (loops), and (4)\nremoving an existing capability without providing an alternative\n(e.g., removing the light strip as a capability after producing a be-\nhavior that uses the light strip). Overall, the results (see Table 4)\nsuggest that it is possible to modify the behavior according to the\ntype of feedback provided, though removing capabilities lead to\ncalling undefined functions more often.\n6 DISCUSSION\nSummary. In this work, we proposed an approach, GenEM, to\ngenerate and modify expressive robot motions using large language\nmodels by translating user language instructions to robot code.\nThrough user studies and experiments, we have shown that our\nframework can quickly produce expressive behaviors by way of\nin-context learning and few-shot prompting. This reduces the need\nfor curated datasets to generate specific robot behaviors or carefully\ncrafted rules as in prior work. In the user studies, we demonstrated\nthat participants found the behaviors generated using GenEM with\nuser feedback competent and easy to understand, and in some cases\nperceived significantly more positively than the behaviors created\nby an expert animator. We have also shown that our approach is\nadaptable to varying types of user feedback, and that more complex\nbehaviors can becomposed by combining simpler, learned behaviors.\nTogether, they form the basis for the rapid creation of expressive\nrobot behaviors conditioned on human preferences.\nLimitations and Future Work. Despite the promise of our ap-\nproach, there are a few shortcomings. Our user studies were con-\nducted online through recorded video clips, and although this is\na valid methodology [12, 43], it may not reflect how participants\nwould react when in the physical proximity of the robot [46]. Hence,\nfurther studies involving interactions with the robot should be pur-\nsued. Some inherent limitations of current LLMs should be noted,\nincluding small context windows and the necessity for text input.\nIn our work, we only evaluate single-turn behaviors (e.g., ac-\nknowledging a passerby), but there are opportunities to generate\nbehaviors that are multi-turn and involve back-and-forth interac-\ntion between the human and the robot. Future work should also\nexplore generating motion with a larger action space such as by\nincluding the manipulator and gripper. Although we have shown\nthat our approach can adapt to user feedback and their preferences,\nthere is currently no mechanism to learn user preferences over a\nlonger period. In reality, we expect that users will exhibit individual\ndifferences in their preferences about the behaviors they expect\nrobots to demonstrate in a given situation. Hence, learning pref-\nerences in-context [47] may be a powerful mechanism to refine\nexpressive behaviors.\nDespite these limitations, we believe our approach presents a\nflexible framework for generating adaptable and composable ex-\npressive motion through the power of large language models. We\nhope that this inspires future efforts towards expressive behavior\ngeneration for robots to more effectively interact with people.\nGenerative Expressive Robot Behaviors\nusing Large Language Models HRI â€™24, March 11â€“14, 2024, Boulder, CO, USA\nACKNOWLEDGMENTS\nWe thank Doug Dooley for providing animations for the baseline\nrobot behaviors, and Edward Lee for helpful discussions on the\nsystem. We thank Rishi Krishnan, Diego Reyes, Sphurti More, April\nZitkovich, and Rosario Jauregui for their help with robot access and\ntroubleshooting, and Justice Carbajal, Jodilyn Peralta, and Jonathan\nVela for providing support with video recording. Lastly, we thank\nBen Jyenis and the UX research team for coordinating the user\nstudies and data collection efforts.\nREFERENCES\n[1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes,\nByron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Haus-\nman, et al. 2023. Do As I Can, Not As I Say: Grounding Language in Robotic\nAffordances. In Conference on Robot Learning . PMLR, 287â€“318.\n[2] Amir Aly and Adriana Tapus. 2013. A model for synthesizing a combined verbal\nand nonverbal behavior based on personality traits in human-robot interaction.\nIn 2013 8th ACM/IEEE International Conference on Human-Robot Interaction (HRI) .\nIEEE, 325â€“332.\n[3] Niklas Bergstrom, Takayuki Kanda, Takahiro Miyashita, Hiroshi Ishiguro, and\nNorihiro Hagita. 2008. Modeling of natural human-robot encounters. In 2008\nieee/rsj international conference on intelligent robots and systems . IEEE, 2623â€“2629.\n[4] Nina Buchina, Sherin Kamel, and Emilia Barakova. 2016. Design and evaluation of\nan end-user friendly tool for robot programming. In 2016 25th IEEE International\nSymposium on Robot and Human Interactive Communication (RO-MAN) . IEEE,\n185â€“191.\n[5] Michael Jae-Yoon Chung, Justin Huang, Leila Takayama, Tessa Lau, and Maya\nCakmak. 2016. Iterative design of a system for programming socially interactive\nservice robots. In Social Robotics: 8th International Conference, ICSR 2016, Kansas\nCity, MO, USA, November 1-3, 2016 Proceedings 8 . Springer, 919â€“929.\n[6] Yuchen Cui, Siddharth Karamcheti, Raj Palleti, Nidhya Shivakumar, Percy Liang,\nand Dorsa Sadigh. 2023. No, to the Right: Online Language Corrections for\nRobotic Manipulation via Shared Autonomy. InProceedings of the 2023 ACM/IEEE\nInternational Conference on Human-Robot Interaction . 93â€“101.\n[7] Porfirio David, Maya Cakmak, Allison SauppÃ©, Aws Albarghouthi, and Bilge\nMutlu. 2022. Interaction Templates: A Data-Driven Approach for Authoring\nRobot Programs. In PLATEAU: 12th Annual Workshop at theIntersection of PL and\nHCI.\n[8] Ruta Desai, Fraser Anderson, Justin Matejka, Stelian Coros, James McCann,\nGeorge Fitzmaurice, and Tovi Grossman. 2019. Geppetto: Enabling semantic\ndesign of expressive robot behaviors. In Proceedings of the 2019 CHI Conference\non Human Factors in Computing Systems . 1â€“14.\n[9] Malcolm Doering, Dylan F Glas, and Hiroshi Ishiguro. 2019. Modeling interaction\nstructure for robot imitation learning of human social behavior.IEEE Transactions\non Human-Machine Systems 49, 3 (2019), 219â€“231.\n[10] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu\nSun, Jingjing Xu, and Zhifang Sui. 2022. A survey for in-context learning. arXiv\npreprint arXiv:2301.00234 (2022).\n[11] Paola Ferrarelli, MarÃ­a T LÃ¡zaro, and Luca Iocchi. 2018. Design of robot teach-\ning assistants through multi-modal human-robot interactions. In Robotics in\nEducation: Latest Results and Developments . Springer, 274â€“286.\n[12] Guy Hoffman and Wendy Ju. 2014. Designing robots with movement in mind.\nJournal of Human-Robot Interaction 3, 1 (2014), 91â€“122.\n[13] Chien-Ming Huang and Bilge Mutlu. 2012. Robot behavior toolkit: generating ef-\nfective social behaviors for robots. In Proceedings of the seventh annual ACM/IEEE\ninternational conference on Human-Robot Interaction . 25â€“32.\n[14] Chien-Ming Huang and Bilge Mutlu. 2013. The repertoire of robot behavior:\nEnabling robots to achieve interaction goals through social behavior. Journal of\nHuman-Robot Interaction 2, 2 (2013), 80â€“102.\n[15] Chien-Ming Huang and Bilge Mutlu. 2014. Learning-based modeling of mul-\ntimodal behaviors for humanlike robots. In Proceedings of the 2014 ACM/IEEE\ninternational conference on Human-robot interaction . 57â€“64.\n[16] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy\nZeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. 2023. Inner\nMonologue: Embodied Reasoning through Planning with Language Models. In\nConference on Robot Learning . PMLR, 1769â€“1782.\n[17] Nusrah Hussain, Engin Erzin, T Metin Sezgin, and YÃ¼cel Yemez. 2022. Training\nsocially engaging robots: modeling backchannel behaviors with batch reinforce-\nment learning. IEEE Transactions on Affective Computing 13, 4 (2022), 1840â€“1853.\n[18] Yusuke Kato, Takayuki Kanda, and Hiroshi Ishiguro. 2015. May I help you?\nDesign of human-like polite approaching behavior. In Proceedings of the Tenth\nAnnual ACM/IEEE International Conference on Human-Robot Interaction . 35â€“42.\n[19] Alyssa Kubota, Emma IC Peterson, Vaishali Rajendren, Hadas Kress-Gazit, and\nLaurel D Riek. 2020. Jessie: Synthesizing social robot behaviors for personalized\nneurorehabilitation and beyond. InProceedings of the 2020 ACM/IEEE international\nconference on human-robot interaction . 121â€“130.\n[20] Minae Kwon, Hengyuan Hu, Vivek Myers, Siddharth Karamcheti, Anca Dragan,\nand Dorsa Sadigh. 2023. Toward Grounded Social Reasoning. arXiv preprint\narXiv:2306.08651 (2023).\n[21] Minae Kwon, Sandy H Huang, and Anca D Dragan. 2018. Expressing robot\nincapability. In Proceedings of the 2018 ACM/IEEE International Conference on\nHuman-Robot Interaction . 87â€“95.\n[22] Minae Kwon, Sang Michael Xie, Kalesha Bullard, and Dorsa Sadigh. 2023. Re-\nward Design with Language Models. In International Conference on Learning\nRepresentations (ICLR) .\n[23] Nicola Leonardi, Marco Manca, Fabio PaternÃ², and Carmen Santoro. 2019. Trigger-\naction programming for personalising humanoid robot behaviour. In Proceedings\nof the 2019 CHI Conference on Human Factors in Computing Systems . 1â€“13.\n[24] Zhongyu Li, Christine Cummings, and Koushil Sreenath. 2020. Animated cassie:\nA dynamic relatable robotic character. In 2020 IEEE/RSJ International Conference\non Intelligent Robots and Systems (IROS) . IEEE, 3739â€“3746.\n[25] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter,\nPete Florence, and Andy Zeng. 2023. Code as policies: Language model programs\nfor embodied control. In 2023 IEEE International Conference on Robotics and\nAutomation (ICRA). IEEE, 9493â€“9500.\n[26] Kevin Lin, Christopher Agia, Toki Migimatsu, Marco Pavone, and Jeannette Bohg.\n2023. Text2Motion: From Natural Language Instructions to Feasible Plans. Auton.\nRobots 47, 8 (Nov 2023), 1345â€“1365. https://doi.org/10.1007/s10514-023-10131-7\n[27] Phoebe Liu, Dylan F Glas, Takayuki Kanda, and Hiroshi Ishiguro. 2016. Data-\ndriven HRI: Learning social behaviors by example from humanâ€“human interac-\ntion. IEEE Transactions on Robotics 32, 4 (2016), 988â€“1008.\n[28] Mina Marmpena, Angelica Lim, TorbjÃ¸rn S Dahl, and Nikolas Hemion. 2019.\nGenerating robotic emotional body language with variational autoencoders. In\n2019 8th International Conference on Affective Computing and Intelligent Interaction\n(ACII). IEEE, 545â€“551.\n[29] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh\nHajishirzi, and Luke Zettlemoyer. 2022. Rethinking the role of demonstrations:\nWhat makes in-context learning work? arXiv preprint arXiv:2202.12837 (2022).\n[30] Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter, Danny Driess, Montser-\nrat Gonzalez Arenas, Kanishka Rao, Dorsa Sadigh, and Andy Zeng. 2023. Large\nLanguage Models as General Pattern Machines. In Proceedings of the 7th Confer-\nence on Robot Learning (CoRL) .\n[31] Michael Murray, Nick Walker, Amal Nanavati, Patricia Alves-Oliveira, Nikita Fil-\nippov, Allison Sauppe, Bilge Mutlu, and Maya Cakmak. 2022. Learning backchan-\nneling behaviors for a social robot via data augmentation from human-human\nconversations. In Conference on robot learning . PMLR, 513â€“525.\n[32] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]\n[33] Nurziya Oralbayeva, Amir Aly, Anara Sandygulova, and Tony Belpaeme. 2023.\nData-Driven Communicative Behaviour Generation: A Survey.ACM Transactions\non Human-Robot Interaction (2023).\n[34] David Porfirio, Evan Fisher, Allison SauppÃ©, Aws Albarghouthi, and Bilge Mutlu.\n2019. Bodystorming human-robot interactions. In proceedings of the 32nd annual\nACM symposium on user Interface software and technology . 479â€“491.\n[35] David Porfirio, Allison SauppÃ©, Aws Albarghouthi, and Bilge Mutlu. 2018. Au-\nthoring and verifying human-robot interactions. In Proceedings of the 31st annual\nacm symposium on user interface software and technology . 75â€“86.\n[36] David Porfirio, Allison SauppÃ©, Aws Albarghouthi, and Bilge Mutlu. 2020. Trans-\nforming robot programs based on social context. In Proceedings of the 2020 CHI\nconference on human factors in computing systems . 1â€“12.\n[37] David Porfirio, Laura Stegner, Maya Cakmak, Allison SauppÃ©, Aws Albarghouthi,\nand Bilge Mutlu. 2023. Sketching Robot Programs On the Fly. InProceedings of the\n2023 ACM/IEEE International Conference on Human-Robot Interaction . 584â€“593.\n[38] David J Porfirio, Laura Stegner, Maya Cakmak, Allison SauppÃ©, Aws Albarghouthi,\nand Bilge Mutlu. 2021. Figaro: A tabletop authoring environment for human-\nrobot interaction. In Proceedings of the 2021 CHI Conference on Human Factors in\nComputing Systems . 1â€“15.\n[39] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan\nTremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. 2023. Progprompt:\nGenerating situated robot task plans using large language models. In 2023 IEEE\nInternational Conference on Robotics and Automation (ICRA) . IEEE, 11523â€“11530.\n[40] Arjun Sripathy, Andreea Bobu, Zhongyu Li, Koushil Sreenath, Daniel S Brown,\nand Anca D Dragan. 2022. Teaching robots to span the space of functional\nexpressive motion. In 2022 IEEE/RSJ International Conference on Intelligent Robots\nand Systems (IROS) . IEEE, 13406â€“13413.\n[41] Michael Suguitan, Mason Bretan, and Guy Hoffman. 2019. Affective robot move-\nment generation using cyclegans. In2019 14th ACM/IEEE International Conference\non Human-Robot Interaction (HRI) . IEEE, 534â€“535.\n[42] Michael Suguitan, Randy Gomez, and Guy Hoffman. 2020. MoveAE: modifying\naffective robot movements using classifying variational autoencoders. InProceed-\nings of the 2020 ACM/IEEE international conference on human-robot interaction .\nHRI â€™24, March 11â€“14, 2024, Boulder, CO, USA Karthik Mahadevan et al.\n481â€“489.\n[43] Leila Takayama, Doug Dooley, and Wendy Ju. 2011. Expressing thought: im-\nproving robot readability with animation principles. In Proceedings of the 6th\ninternational conference on Human-robot interaction . 69â€“76.\n[44] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu,\nLinxi Fan, and Anima Anandkumar. 2023. Voyager: An open-ended embodied\nagent with large language models. arXiv preprint arXiv:2305.16291 (2023).\n[45] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,\nQuoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning\nin large language models. Advances in Neural Information Processing Systems 35\n(2022), 24824â€“24837.\n[46] Sarah Woods, Michael Walters, Kheng Lee Koay, and Kerstin Dautenhahn. 2006.\nComparing human robot interaction scenarios using live and video based meth-\nods: towards a novel methodological approach. In9th IEEE International Workshop\non Advanced Motion Control, 2006. IEEE, 750â€“755.\n[47] Jimmy Wu, Rika Antonova, Adam Kan, Marion Lepert, Andy Zeng, Shuran Song,\nJeannette Bohg, Szymon Rusinkiewicz, and Thomas Funkhouser. 2023. TidyBot:\nPersonalized Robot Assistance with Large Language Models. In 2023 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems (IROS) . 3546â€“3553.\nhttps://doi.org/10.1109/IROS55552.2023.10341577\n[48] Wenhao Yu, Nimrod Gileadi, Chuyuan Fu, Sean Kirmani, Kuang-Huei Lee,\nMontse Gonzalez Arenas, Hao-Tien Lewis Chiang, Tom Erez, Leonard Hasen-\nclever, Jan Humplik, Brian Ichter, Ted Xiao, Peng Xu, Andy Zeng, Tingnan Zhang,\nNicolas Heess, Dorsa Sadigh, Jie Tan, Yuval Tassa, and Fei Xia. 2023. Language\nto Rewards for Robotic Skill Synthesis. In Proceedings of the 7th Conference on\nRobot Learning (CoRL) .\n[49] Allan Zhou and Anca D Dragan. 2018. Cost functions for robot motion style. In\n2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) .\nIEEE, 3632â€“3639.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.755839467048645
    },
    {
      "name": "Robot",
      "score": 0.7019994258880615
    },
    {
      "name": "Humanâ€“computer interaction",
      "score": 0.642501711845398
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.5825513601303101
    },
    {
      "name": "Motion (physics)",
      "score": 0.5750793218612671
    },
    {
      "name": "Generative model",
      "score": 0.5362237095832825
    },
    {
      "name": "Generative grammar",
      "score": 0.5290723443031311
    },
    {
      "name": "Natural language",
      "score": 0.48156192898750305
    },
    {
      "name": "Context (archaeology)",
      "score": 0.4794175326824188
    },
    {
      "name": "Artificial intelligence",
      "score": 0.43773266673088074
    },
    {
      "name": "Social robot",
      "score": 0.4248446822166443
    },
    {
      "name": "Mobile robot",
      "score": 0.2654654383659363
    },
    {
      "name": "Robot control",
      "score": 0.16509166359901428
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ]
}