{
  "title": "Metamorphic Malware Evolution: The Potential and Peril of Large Language Models",
  "url": "https://openalex.org/W4391895321",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2400088204",
      "name": "Pooria Madani",
      "affiliations": [
        "University of Ontario Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2532737545",
    "https://openalex.org/W2024170198",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2137786570",
    "https://openalex.org/W1963980186",
    "https://openalex.org/W6674887261",
    "https://openalex.org/W2082418604",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2171928131",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W3098605233",
    "https://openalex.org/W4285600327",
    "https://openalex.org/W4311887664",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W6764109406",
    "https://openalex.org/W6784593087",
    "https://openalex.org/W4381586770",
    "https://openalex.org/W2747329762",
    "https://openalex.org/W2970575144",
    "https://openalex.org/W4389519352",
    "https://openalex.org/W4376167329",
    "https://openalex.org/W4226485558",
    "https://openalex.org/W3089307846",
    "https://openalex.org/W4380993527",
    "https://openalex.org/W4368755703",
    "https://openalex.org/W4315706637",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W3177813494"
  ],
  "abstract": "Code metamorphism refers to a computer programming exercise wherein the program modifies its own code (partial or entire) consistently and automatically while retaining its core functionality. This technique is often used for online performance optimization and automated crash recovery in certain mission-critical applications. However, the technique has been misappropriated by malware creators to bypass signature-based detection measures instituted by anti-malware engines. However, current code mutation engines used by threat actors offer only a limited degree of mutation, which is frequently detectable via static code analysis. The advent of large language models (LLMs), such as ChatGPT 4.0 and Google Bard may lead to a significant evolution in this landscape. These models have demonstrated a level of algorithm comprehension and code synthesis capability that closely resembles human abilities. This advancement has sparked concerns among experts that such models could be exploited by threat actors to generate sophisticated metamorphic malware. This paper explores the potential of several prominent LLMs for software code mutation that may be used to reconstruct (with mutation) existing malware code bases or create new forms of embedded mutation engines for next-gen metamorphic malwares. In this work, we introduce a framework for creating self-testing program mutation engines based on LLM/Transformer-based models. The proposed framework serves as an essential tool in testing next-gen metamorphic malware detection engines.",
  "full_text": "Metamorphic Malware Evolution: The Potential and\nPeril of Large Language Models\nPooria Madani\nBusiness and Information Technology\nOntario Tech University\nOshawa, Canada\npooria.madani@ontariotechu.ca\nAbstract—Code metamorphism refers to a computer program-\nming exercise wherein the program modifies its own code (partial\nor entire) consistently and automatically while retaining its core\nfunctionality. This technique is often used for online performance\noptimization and automated crash recovery in certain mission-\ncritical applications. However, the technique has been misappro-\npriated by malware creators to bypass signature-based detection\nmeasures instituted by anti-malware engines. However, current\ncode mutation engines used by threat actors offer only a limited\ndegree of mutation, which is frequently detectable via static code\nanalysis. The advent of large language models (LLMs), such as\nChatGPT 4.0 and Google Bard may lead to a significant evolution\nin this landscape. These models have demonstrated a level of\nalgorithm comprehension and code synthesis capability that\nclosely resembles human abilities. This advancement has sparked\nconcerns among experts that such models could be exploited by\nthreat actors to generate sophisticated metamorphic malware.\nThis paper explores the potential of several prominent LLMs for\nsoftware code mutation that may be used to reconstruct (with\nmutation) existing malware code bases or create new forms of\nembedded mutation engines for next-gen metamorphic malwares.\nIn this work, we introduce a framework for creating self-testing\nprogram mutation engines based on LLM/Transformer-based\nmodels. The proposed framework serves as an essential tool in\ntesting next-gen metamorphic malware detection engines.\nIndex Terms—Metamorphic Malware, Large Language Mod-\nels, Program Synthesis, Code Cloning, Malware Detection, Code\nMutation\nI. I NTRODUCTION\nCode metamorphism refers to a programming methodology\nwherein the program self-modifies its code consistently and\nautomatically, retaining its core functionality while changing\nits binary/syntactical representation. An intriguing application\nof code metamorphism lies in enhancing fault tolerance of\nmission-critical systems via code variant redundancy, allowing\nthe system to switch to an automatically generated alternative\nversion of the code when confronted with faults at runtime\n[1].\nThe technique of code metamorphism has been adopted\nby creators of malicious software (i.e., malware) to circum-\nvent detection by anti-malware applications [2]. Metamorphic\nmalware can change its code (while preserving the func-\ntionality) at each iteration of replication, making signature-\nbased detection, a gruelling feat. Often, this rewriting or\nreorganization of code can be carried out using rule-based\ncode mutation engines through techniques, such as changing\nvariable names, reordering instruction sequences, or using\ndifferent programming constructs that achieve the same ex-\npected code functionality. These rule-based engines rely on\na predefined array of templates to modify a given program\ncode (or binary) into versions that are semantically equivalent,\nbut syntactically distinct [2]. However, since the majority\nof contemporary code mutation engines lack the ability to\ninterpret program code semantics, their internal rules permit\nonly a relatively limited degree of mutation that is often easy\nto detect via static code analysis. This landscape is set to\nchange significantly with the advent of large language models\n(LLMs) like ChatGPT 4.0 [3] and Google Bart [4] that are\ndemonstrating human-level algorithm understandability and\nprogram code synthesis capabilities. Some researchers are\nconcerned about threat actors potentially misusing emerging\nLLMs (capable of generating software code from English\ninstructions) to develop and generate complex malwares.\nLarge language models are types of artificial intelligence\nmodels that have been trained involving a colossal amount of\nweb text data (e.g., news articles, blog posts, online source\ncode repositories, etc.) and are capable of interpreting as well\nas generating human language contents (e.g., blog posts) in a\nway that is surprisingly coherent and contextually appropriate\n[5]. Additionally, it has been demonstrated that LLMs can\nassist with programming and coding tasks given that their\ntraining data includes substantial coding and technical discus-\nsions from various online sources [6]. However, it must be\nnoted that LLMs do not truly understand code or programming\nprinciples and the quality of code they generate can be hit-or-\nmiss, and they might produce code that is incorrect, inefficient,\nor insecure.\nIn this paper, we present compelling arguments and substan-\ntiate them with experimental data where forthcoming LLMs\nhold immense potential as instrumental aids in constructing\nmetamorphic computer programs. This powerful capability\nmay very well be a double-edged sword. If misused, these\nmodels could serve as formidable tools in the creation of\nadvanced metamorphic malware, posing a significant threat\nto the safety and stability of our increasingly interconnected\nworld. Hence, our contribution is threefold:\n1) Investigate the use of current LLMs models to regenerate\narXiv:2410.23894v1  [cs.CR]  31 Oct 2024\nsoftware code while preserving existing semantics (i.e.\ndemonstrating code mutation capabilities),\n2) proposed a framework to construct self-testing pro-\ngram mutation engines utilizing LLM/Transformer-\nbased models,\n3) demonstrates the first proof-of-concept implementation\nof the proposed framework that utilizes few prominent\nopen-source LLMs and OpenAI’s ChatGPT to achieve\ncode metamorphism for any arbitrary Python codebase.\nThe remaining paper is organized as follows: Section 2\npresents a review of currently established techniques in code\nmetamorphism. In Section 3, we survey the existing literature\non code synthesis using LLMs and evaluation metrics used\nin our study while outlining our LLM-based code muta-\ntion framework. Section 4 details the practical application\nof our framework, demonstrating its execution against some\nreal-world Python programs. We conclude our discussion in\nSection 5, which addresses the limitations of our proposed\napproach and suggests potential areas for future research.\nII. EXISTING APPROACHES TO CODE\nMETAMORPHISM\nCode metamorphism refers to a computer programming\nexercise wherein the program modifies its own code (partial\nor entire) consistently and automatically while retaining its\ncore functionality [1]. This technique is often used for online\nperformance optimization and automated crash recovery in\ncertain mission-critical software. However, the technique has\nbeen misappropriated by malware creators to bypass detection\nmeasures instituted by anti-malware engines.\nSignature-based detection anti-malware software operates\nby cross-examining potentially harmful files against a database\nof known malware signatures - unique code snippets that\nsignify distinct malware [7]. The binary file under inspection is\ndesignated malicious when it correlates with a malicious sig-\nnature in the database. As such, code metamorphism emerges\nas a primary technique for malware creators to counteract\nsignature-based anti-malware defences by continually altering\nthe malware code (automatically and on the fly) to escape\ndetection. Over time, malware authors have demonstrated\nsignificant innovation in devising new techniques for automatic\ncode mutation (i.e., metamorphism), often staying one step\nahead of existing defences. In this section, we are going\nto briefly discuss some of the most common code mutation\ntechniques that are commonly used by malware authors.\nA. Instruction Substitution\nInstruction substitution is a technique in code metamor-\nphism that constitutes certain instructions in a program’s code\nbeing replaced by another set of instructions while the overall\nfunctionality of the program remains intact. This technique\nassists in modifying the program code stored on disk without\nchanging its overall behaviour, making it difficult to match\nthe modified code against a database of known malicious\ncode signatures. For instance, Figure 1 depicts three distinctly\ndifferent Python statements possessing the same semantic –\nincreasing the value of variable ‘a’ by 1.\nFig. 1. Three semantically identical Python expressions for increasing the\nvalue of variable ‘a’ by 1.\nB. Instruction Permutation\nInstruction permutation refers to the reordering of inde-\npendent instructions in a program code while preserving its\noriginal functionality. In this context, independent instructions\nrefer to any pair of instructions where an execution order does\nnot affect the program’s overall functionality. Such reordering\ngenerates a different binary representation of the same program\ncode, which in turn, makes it more difficult for signature-based\ndetection systems to detect the presence of known malicious\ncode sequences in the program. In addition, instruction per-\nmutation is effortlessly achieved in high-level programming\nlanguages, such as Python, as demonstrated in Figure 2.\nFig. 2. Two semantically identical Python code snippets demonstrating\nexpression/instruction permutation.\nC. Variable/Register Substitution\nIn the context of code metamorphism, variable/register\nsubstitution is the process of systematically replacing the\nvariables/registers used in a program code with new ones.\nHowever, as many parts of a program code may be dependent\non few variable/register identifiers, such changes must be made\ntaking into consideration existing code dependencies on the\nmodified variable/registers to ensure that the functionality of\nthe code remains unchanged.\nFig. 3. Python code snippet demonstrating dead code insertion – variable ‘b’\nis never being used.\nD. Dead Code Insertion\nDead code insertion involves the injection of non-functional\n(a code where its execution would not affect any important\ndata or the original program’s functionality) code into a\nprogram’s source code. While not having any impact on the\nprogram output or its functionality, inserted dead codes can\nsignificantly change the program’s binary/source structure. For\nexample, as depicted in Figure 3, the second statement does\nnot influence the overall functionality of the program; it is still\nincorporated within the structure of the source code.\nIdentification of dead codes in a program source/binary\n(especially if dead codes contain a long sequence of function\ncalls) is not a trivial task. It may require extensive static\nand dynamic code analysis that may be computationally pro-\nhibitive.\nE. Changing the Control Flow\nIn the context of code metamorphism, threat actors can\nrearrange the sequence in which a program’s individual state-\nments or instructions are executed (by rearranging statements\nin a source code) while preserving its overall functionality\nand output. Undertaking such modifications necessitates a\ncomprehensive understanding of the overarching program’s\nlogic and proficiency in the programming language in which\nthe program is written.\nThere are several control flow mutation techniques that are\nused to achieve code metamorphism:\n• Unreachable code insertion: code that cannot be\nreached during execution of the program, is inserted into\nthe source code. This can confuse many static analysis\ntools.\n• Control flow flattening: the control flow of the program\nis modified to remove some of the hierarchies in the\nsource code.\n• Jump instructions: use of unnecessary jump instructions\n(supported by certain programming languages, such as\nAssembly, C, and Fortran) to create confusion about the\ncontrol flows in the program source code.\nModifying the control flows used in a program would\nsignificantly change its source/binary structure. Determining\nequivalency between two versions of a program where the\ncontrol flows are modified is an NP-Complete problem [8].\nTherefore, control flow modification is considered a chal-\nlenging yet highly effective technique for the creation of\nundetectable code metamorphism.\nUndeniably, threat actors are not constrained by the tech-\nniques enumerated in this section. They are consistently\ninnovating to craft increasingly sophisticated metamorphic\nmalware. By combining the techniques discussed above, they\ncan create malware variants that are extraordinarily difficult to\ndetect, if not outright impossible, even by the most advanced\nanti-malware software of the day.\nIn the following sections, we will assess the efficacy of\npopular LLMs (capable of code synthesis) in facilitating code\nmetamorphism using simple natural language instructions.\nWe aim to study the potential of these models to serve as\naccessible tools to threat actors, enabling them to generate\nmetamorphic malware without requiring extensive knowledge\nregarding the implementation of the discussed code mutation\ntechniques in this section.\nIII. CODE MUTATION USING LARGE LANGUAGE\nMODELS\nIn machine learning, a generative model can learn the\njoint probability distribution of input features and labels (the\ntraining dataset), then uses this knowledge to create new data\npoints that follow the same distribution as the training data\n[9]. This inherent capability enhances the utility of generative\nmodels in myriad application domains, such as speech synthe-\nsis, music generation, anomaly detection, code synthesis, and\ntext generation/translation.\nWithin the field of natural language processing (NLP), lan-\nguage models serve as a specific category of generative models\nthat excel at learning underlying semantic and syntactic asso-\nciations between words in a sequence of sentences. In turn, the\ntrained language models are utilized to augment various NLP\ntasks, ranging from text synthesis and translation to sentiment\nclassification. Over the last decade, different deep neural\nnetwork architectures (e.g., recurrent neural networks) have\ndemonstrated remarkable proficiency in language modelling\nand, thus, substantially outperformed classical methods, such\nas Hidden Markov Models (HMMs) [10] at the same tasks. In\nrecent years, the term “Large Language Model” has emerged,\nalluding to the vast number of parameters trained within a\ndeep neural network for language modelling purposes. This\nterminology underscores the pivotal role that neural networks\nplay in the development and training of these expansive\nlanguage models.\nIn a seminal paper by Vaswani et al. [11] titled “Attention\nis All You Need,” a new deep neural network architecture\ncalled Transformers was introduced that revolutionized com-\nplex sequence modelling tasks specifically used in NLP, such\nas language modelling and text summarization. The proposed\nTransformer architecture relies on two novel mechanisms\nknown as “Attention” and “Positional Encoding” to capture\nthe importance and relationship of elements in sequential data\nin a parallel fashion. Leveraging their inherent capacity to\nprocess data in parallel, Transformers have drastically reduced\nthe training time when compared to recurrent neural networks\n(RNNs) [12]. This advantage enables Transformer-based mod-\nels to use exceptionally large datasets during training and\nachieve superior generalization capabilities when compared to\ntheir RNN-based counterparts.\nFollowing the advent of Transformers, two models, namely\nBidirectional Encoder Representations from Transformers\n(BERT) [13] and Generative Pretrained Transformers (GPT)\n[14] have emerged. These models have substantially enhanced\nthe performance of various AI tasks and have thus, gained\nconsiderable recognition in the field. Developed by Google,\nBERT uses the Transformer’s Attention mechanism to analyze\ninput sequence data in both directions (forward and backward\nin time) simultaneously, a method called bidirectional training.\nThis enables BERT to understand the context of an element in\na sequence based on all its surroundings (e.g., other words in a\nsentence) and learn a deeper understanding of the relationship\nbetween elements in a sequence regardless of where they\nappear. On the other hand, GPT developed by OpenAI, reads\nan input sequence from left to right and is trained to predict\nthe next element in the sequence (i.e., auto-regression).\nGiven the parallels between natural language documents and\nprogram source code, along with the demonstrated success of\nTransformers in creating original text content, it is intriguing\nto consider whether these same Transformer models could\naccomplish computer program synthesis, that is, the automated\ngeneration of program source code, provided they are trained\non suitable data. In the remainder of this section, we survey\nsome of the existing work in non-malicious computer program\nsynthesis using Transformers and pre-trained large language\nmodels (LLMs) before presenting our framework for automatic\ncode mutation using them.\nA. Computer Program Synthesis using Large Language Mod-\nels (LLMs)\nComputer program synthesis, also known as automatic\nprogramming, is the act of creating programs that satisfy a\ncertain set of specifications (e.g., natural language query or\nsemantic similarity) with minimal human intervention and\nassistance. Program synthesis can assist with various tasks,\nsuch as automated code completion (when using a code\neditor software), generating bug fixes, optimizing existing code\nbases, code translation (between two programming languages),\ncode clone detection, generating program code via a natural\nlanguage description, or generating natural language com-\nments for a program’s source code. Surprisingly, Transformer-\nbased models are demonstrated to be effective with many of\nthese tasks.\nCodeBERT proposed by Feng et al. [15] is a bimodal BERT\nmodel, pre-trained for programming language and natural\nlanguage tasks, such as natural language code search and\ncode documentation generation. In the aforementioned work,\nauthors demonstrated that BERT model can capture the seman-\ntic connections between natural language and programming\nlanguage, and produce a general-purpose representation that\ncan be useful for querying code while describing the program\nrequirements using natural language. The authors demon-\nstrated this capability over a wide range of programming\nlanguages, such as GO, Java, Python and Ruby, highlighting\nthat learning program code semantics is not constrained to\na particular programming language syntax. Furthermore, the\ndemonstrated capacity of BERT models to understand the\nsemantics of program codes holds promise for advancing other\nprogram synthesis tasks, such as automatic code mutation.\nCodex created by Chen et al. [6] at OpenAI is another pro-\nprietary pre-trained (over their famous large language model)\nGPT model that can generate Python functions from natural\nlanguage descriptions and docstrings with more than 70%\naccuracy. For assessing the correctness of generated Python\ncodes, authors resort to the use of pre-coded unit tests – an\nidea that has motivated the work conducted in this manuscript.\nAlthough their trained model is not open-source and not\ndirectly accessible to the public, users can interact with it\nthrough GitHub Codepilot and their famous ChatGPT prompt\nterminal.\nCodeGen proposed by Nijkamp et al. [16] at Salesforce,\nstrived to be an open-source version of Codex where natural\nlanguage queries are turned into program code. Compared to\nCodex, CodeGen is generating far more inaccurate program\ncodes. However, authors have demonstrated that by increasing\nthe size of the language model parameters and incorporating\nmore training data, their proposed generator can progressively\ncreate increasingly accurate outputs comparable to what is\nproduced by Codex.\nContinual Pre-training on Sketches for Library-Oriented\nCode Generation (CERT) proposed by Zan et al. [17] is an\nopen-source GPT-based automatic code generator developed\nat Microsoft. The authors articulated a two-stage genera-\ntive system that can turn natural language instructions into\nPython codes while leveraging existing Python libraries, such\nas Pandas. For instance, when instructed to write a code\nto read a comma-separated value (CSV) file, CERT uses\n“pandas.read csv” function to accomplish the reading task\ninstead of writing all the required code from scratch. They\nhave trained their GPT model on 96GB high-quality Python\ncodes extracted from GitHub that contain quality natural\nlanguage comments while utilizing popular Python libraries,\nsuch as Pandas and NumPy in their bodies.\nAlphaCode proposed by Li et al. [18] at Google has tackled\na much harder problem in program synthesizing. In contrast\nto the aforementioned works, which generally generate code\nsnippets no larger than a function, AlphaCode can produce\na complete program (potentially consisting of multiple func-\ntions), purely from a natural language description of the pro-\ngram’s intended functionality. They begin by pretraining their\nTransformer-based language model with standard language\nmodel objectives (autoregression, i.e., given a sequence of\nn tokens, generate the n + 1th token). Then, they fine-tuned\nthe model on Codecontest’s competitive programming dataset\nin order to be able to answer programming competition-like\nqueries and solve complex programming challenges. Authors\nhave reported AlphaCode achieving on average, a ranking\nof top 54.3% in competitions with more than 5,000 human\nparticipants.\nThere are multiple other open-source projects that have\ntrained LLMs for program code synthesis similar to the\nsurveyed works above, summarized in Table 1.\nB. variation@k and pass@k Evaluation Metrics\nOne predominant approach to assess the correctness/quality\nof a model-generated code is by matching it against a refer-\nence solution – a common practice adopted from the field\nof machine translation known as the Bilingual Evaluation\nUnderstudy (BLUE) score, introduced by Papineni et al. [27]\nat IBM Research. However, as explored by Ren et al. [28],\nunlike machine-generated natural language contents, some\nsyntactically different codes can have identical semantics when\nexecuted. Consequently, the BLUE score needs to be heav-\nily modified to not mistakenly penalize semantically correct\nTABLE I\nSUMMARY OF LARGE LANGUAGE MODEL CAPABLE OF\nCODE/PROGRAM SYNTHESIS\nModel Params HumanEval pass@1\nClosed Source\nGPT-3.5 [19] 175B 48.10%\nGPT-4 [3] 100T 67.00%\nCodex [6] 12B 28.80%\nAlphaCode [18] 1.1B 14.00%\nPhi-1 [20] 1.3B 50.6%\nOpen Source\nCodeGen-Multi [16] 7B -\nCodeGen-Mono [16] 16B 29.3%\nCodeGen2 [21] 16B 19.1%\nStarCoder [22] 15.5B 33.6%\nCodeT5+ [23] 16B 30.90%\nCodeParret [24] 1.5M 3.58%\nSantaCoder [25] 1.1B 14.00%\nWizardCoder [26] 16B 57.3%\ngenerated mutated codes that look different syntactically (i.e.,\nmutated).\nMore recent works in program synthesis [29, 30] have\nturned to measuring the functional correctness of generated\ncodes where a generated body of code is considered correct\nif it passes a set of unit tests. Kulal et al. [29] proposed to\nevaluate the capability of program synthesis of a model using\nthe pass@k metric where k code samples are generated per\neach evaluation problem and a given problem is considered\nsolved if any of the generated samples pass the associated\nunit tests. This metric reports the total fraction of problems\nsolved.\nFig. 4. Region where LLM can be measured based on possible pass@k and\nvariation@k values.\nIn this study, as opposed to the reviewed works in Section\n3.A, the pass@k metric does not provide a dependable measure\nfor the code mutation capability of LLMs. The issue we are\naddressing here involves not just the potential of a specific\nLLM to generate a syntactically accurate code snippet, but also\nits ability to produce multiple variations of codes for a given\nsemantic. Hence, we introduce a new metric, variation@k\nto measure the code mutation capability of a LLM-based\nmodel: for each problem i in our evaluation dataset, k code\nsamples are generated and the average fraction of correct\nunique solutions (syntactically) per problem is calculated.\nLet S = {unique(s1)/k, . . . , unique(sn)/k} represent set\nof fractions of unique and correct solutions generated for\nthe n problems in the evaluation dataset using k trials, then\nvariation@k is average of non-zero elements in S, defined\nas follows:\nS′ := {x|x ∈ S&x >0} (1)\nvariation@k := 1\n||S′||\n||S′||X\n1\ns′\ni (2)\nFigure 4 illustrates the defined regions and probable zones\nwhere a LLM-based model capability of code synthesis can\nbe plotted based on its respective pass@k and variation@k\nmetrics. It is important to highlight that not every area within\nthe shaded region has an equal likelihood of occurrence.\nHowever, this graphical plot provides an effective foundation\nfor comparing the performance of various LLMs in tasks\nrelated to code synthesis and code mutation.\nC. Proposed Framework for using LLMs in Code Mutation\nOften, a computer program comprises various subroutines\nor modules, which are executed in a systematic sequence as\noutlined by the software developer(s) within the program’s\nsource code. Breaking down a program’s source code into\nsmaller subroutines and modules helps with its readability,\nmaintainability and testability. Each subroutine (e.g., function,\nprocedure, class, etc.) can be tested in isolation (also known\nas unit testing) and its correctness can be verified independent\nof the remaining program.\nEven the slightest alteration to any of the program’s sub-\nroutines (in the source code) while maintaining the anticipated\nprogram semantics, will yield a new unique program source\ncode. The modified source code can rightfully be deemed\nas a new variation of the original version. Moreover, as the\nalteration has occurred at the subroutine level, the correctness\nof modified subroutines can be verified through their asso-\nciated unit tests. Therefore, in this work, we propose to use\nLLMs to modify subroutines in a program source code (e.g.,\nmalware source code) individually when creating new/different\nvariations of the original source code.\nWe propose a framework specifically designed to assess\nthe capabilities of Language Learning Models (LLMs) in\nthe realm of code mutation. As depicted in Figure 6, our\nframeworks begins by enumerating all subroutines fi ∈ F in a\nsource code that are associated with the unit-test ti (or unit test\ncan be created for them in advance), then iteratively submit\neach subroutine fi to the chosen LLM to be rewritten (i.e.,\nmutated) and confirm the correctness of generated f′\ni against\nthe unit-test ti.\nIn other words, instead of hoping for the entire code of\na program to be rewritten by the chosen LLM correctly,\nthe mutation process is broken down at the subroutine level\nand the correctness of modified subroutines are validated\nFig. 5. The proposed framework for using LLMs with ability of code/program\nsynthesis to be used for source code mutation guided by unit test procedures.\nvia associated unit tests. Therefore, the ability of a LLM\nfor program mutation boils down to its ability to generate\nmultiple code versions of the constituent subroutines in a\nprogram source code. In the following section, we investigate\nthe function-level (in Python programming language) mutation\ncapability of several prominent LLMS to determine whether\nthese models can effectively be utilized for code mutation\nfollowing our proposed framework.\nIV. E XPERIMENTAL RESULTS\nThe primary objective of this research is to scrutinize the\ncode mutation prowess of some publicly accessible prominent\nLLMs that possess program synthesis capabilities. In order to\nvalidate our hypothesis and examine the effectiveness of our\nproposed framework, we have chosen 5 open-source LLMs\nalong with OpenAI’s ChatGPT 3.5 as outlined in Table 2.\nIn our experiments, we have used OpenAI’s HumanEval\ndataset consisting of 164 hand-written programming problems\nalong with several unit tests with “an average of 7.7 tests\nper problem” [6]. The programming tasks in this dataset are\nused to assess natural language comprehension, reasoning,\nalgorithm developments and the simple mathematics of LLMs.\nWhat sets our experiments apart, a facet previously unexplored\nand unreported, is studying the capacity of the LLMs under\nscrutiny for the generation of several distinct yet correct\nsolutions to the given programming problems in this dataset.\nIt is important to recognize that the 6 models used in\nour study are not identical in their composition. These LLM\nmodels were initially trained for program synthesis tasks based\non a series of NLP queries without specific fine-tuning for\ncode mutation. The open-source models are all retrieved from\nHugging Face (an online repository to access publicly shared\nLLMs) while ChatGPT 3.5 was queried through the public\nAPI provided by OpenAI.\nAll LLMs used in our experiments are auto-regressive\nmodels, which means that they generate their output sequence\none element at a time, where each new element (e.g., code\nsymbols) is dependent on the previously generated elements.\nIn other words, auto-regressive text/code (referred to as symbol\nand denoted as w) generation is based on the assumption\nthat the probability distribution of a symbol sequence can be\ndecomposed into the product of the conditional probability of\nthe next symbol to appear as described in Equation 3.\nP(w1:T |W0) :=\nTY\nt=1\nP(wt|w1:t − 1, w0) (3)\nThus, the probability of selecting a particular symbol (e.g.,\na keyword, a mathematical symbol, etc.) w at time t is\nproportional to the probability distribution wt ∼ P(w|w1:t−1).\nIt is obvious that such text/code generation using sampling is\nnot deterministic and each execution of this sampling process\ncan potentially lead to generation of different outputs. In this\nwork we have used a sampling scheme called Top-P proposed\nby Holtzman et al. [31] for which the smallest set of possible\nsymbols (with k maximum number of symbols allowed to be\nin the set), the cumulative probability distribution exceeds the\nprobability p (considered at each time step). Following this,\nthe probability mass is redistributed among this set of symbols\nbefore wt is drawn from the set. Of course, we have no control\nover the sampling process employed by OpenAI’s ChatGPT\nand rely on their black box model to generate the necessary\noutputs as we query their model remotely.\nWe begin by querying each of the models under the study\nten times to solve each of the programming tasks in the evalua-\ntion dataset. Then, each produced code snippet is parsed using\nPython’s AST library to remove human-readable comments\nand extra whitespace, and tested against provided unit test by\nHumanEval evaluation set to compute pass@10 score of each\nmodel. Furthermore, for each programming problem in the\nevaluation set, we have computed a SHA256 hash digest of\neach produced code snippet by any given model for identifying\ndistinct synthesized codes and compiling variation@10 met-\nrics. Table 2 summarizes the program synthesis performance\nand code mutation capabilities of the studied models using\nTop-P sampling (for the open-source models).\nAs depicted in Figure 6 and Table II, in terms of code\nsynthesis accuracy, ChatGPT is outperforming significantly all\nthe other evaluated open-source models at pass@10. However,\nin terms of generating semantically similar code snippets that\nare syntactically different ( variation@10), Salesforce’s open-\nsource models are slightly lagging behind. It is important to\nnote that none of the LLMs used in this study were explicitly\ntrained to generate multiple unique solutions to programming\nTABLE II\nSUMMARY OF EVALUATION OF LLMS USED IN OUR CODE\nMUTATION STUDY. PASS@100 VALUES ARE SELF-REPORTED\nBY THE AUTHORS.\nModels pass@10 pass@100 variation@10\nCodeParrot [24] 8.03% 14.96% 5.32%\nCodeGen2-Multi [21] 30.48% 50.80% 32.8%\nCodeGen-Mono [16] 43.29% 75.00% 38.45%\nSantaCoder [25] 7.31% 49.00% 10.83%\nStarCoderPlus [22] 10.36% - 11.76%\nChatGPT 3.5 Turbo [19] 100.00% - 51.32%\nchallenges. Nevertheless, they are exhibiting the ability to\ngenerate multiple variants of correct solutions for most of the\npresented programming challenges from HumanEval dataset.\nThis observation can confirm our hypothesis that one can\nenhance the code synthesis mutability of these models sig-\nnificantly by specifically optimizing them to better cater to\nmutation-based objectives.\nThe top three models demonstrated high complexity in\ngenerating different code snippets for some of the given\nprogramming challenges. For example, as depicted in Figure\n7, CodeGen-Mono solved one of the programming challenges\nin two fundamentally different approaches: one using for-loop\nand the other using recursion. The demonstrated degree of\nvariability between the two solutions is outside of the scope of\ntechniques discussed in Section 2 and can hardly be replicated\nusing rule-based mutation engines of the past.\nFig. 6. Region where LLM can be measured based on possible pass@k and\nvariation@k values.\nV. D ISCUSSIONS AND FUTURE WORK\nIn this study, we have assessed the capabilities of several\nprominent large language models, both open and closed source\nin computer code synthesis and code mutation. Our findings\nunequivocally illustrate that current LLMs, even without ex-\nplicit training for code mutation, possess the ability to generate\ndiverse code snippets that are semantically identical. Conse-\nquently, the notion of explicitly training LLMs or constructing\nFig. 7. Two mutation examples generted by CodeGen-Mono-Python.\nspecialized Transformers for the task of code mutation is not\nan implausible concept.\nUndoubtedly, a pressing concern for existing large language\nmodels is based on their potential exploitation by threat actors\nto rewrite existing malware code bases and test the sensitivity\nof current anti-malware software to mutated malicious codes.\nFortunately, the current large language models (LLMs) with\ncode mutation capabilities, boasting billions of parameters,\ncannot be embedded in the core of metamorphic malware. The\nsheer size of these models renders them impractical for such\nusage (the top-performing models in our study have disk space\nrequirements larger than 1 GB). However, recent research [20]\nis pointing toward promising developments in the compression\nof large language models. These efforts aim to decrease the\nnetwork size of LLMs while maintaining their synthesis capa-\nbilities, resulting in a reduction of disk space requirements. We\nhypothesize that near-future LLM/Transformer designs will\nenable the development of considerably smaller, yet highly\ncapable code mutation engines. These advanced engines would\npossess the potential to be seamlessly integrated into malware\nbinaries without significantly increasing the overall disk size\nof the malicious software. This hypothesis suggests a perilous\nfuture where more compact and potent AI-based code mutation\nengines can be employed by threat actors for the development\nof conspicuous malware.\nAs an extension of this study, it would be crucial to conduct\na similar investigation across various programming languages.\nThe future research should aim to determine whether the\ngrammatical structures of different programming languages\nimpose constraints on code mutation and synthesis by LLMs\nand future code synthesis Transformers. Such a comprehensive\nstudy would enable the anti-malware development community\nto concentrate their efforts on a subset of malware codebases\nthat utilize specific programming languages exhibiting high\nmutability potential through the utilization of forthcoming\nLLMs/Transformers.\nFurthermore, we advocate for an in-depth study focused on\ntraining Transformer-based (other than LLMs) models for the\npurpose of code mutation. By training models explicitly for the\ncode mutation task, we can streamline the evaluation loop of\nmalware detection systems and effectively fortify our defences\nagainst the emergence of such cyber threats; it would equip\nus to address the advent of AI-based metamorphic malicious\nsoftware proactively.\nREFERENCES\n[1] C. Sun, V . Le, and Z. Su, “Finding Compiler Bugs via Live Code\nMutation”, doi: 10.1145/2983990.2984038.\n[2] I. You and K. Yim, “Malware obfuscation techniques: A brief survey,”\nProceedings - 2010 International Conference on Broadband, Wire-\nless Computing Communication and Applications, BWCCA 2010, pp.\n297–300, 2010, doi: 10.1109/BWCCA.2010.85.\n[3] OpenAI, “GPT-4 Technical Report,” Mar. 2023, Accessed: Jul. 17, 2023.\n(Online). Available: https://arxiv.org/abs/2303.08774v3\n[4] M. Lewis et al., “BART: Denoising Sequence-to-Sequence Pre-training\nfor Natural Language Generation, Translation, and Comprehension,”\nProceedings of the Annual Meeting of the Association for Computa-\ntional Linguistics, pp. 7871–7880, Oct. 2019, doi: 10.18653/v1/2020.acl-\nmain.703.\n[5] W. X. Zhao et al., “A Survey of Large Language Mod-\nels,” Mar. 2023, Accessed: Jul. 17, 2023. (Online). Available:\nhttps://arxiv.org/abs/2303.18223v11\n[6] M. Chen et al., “Evaluating Large Language Models Trained on\nCode,” Jul. 2021, Accessed: Jul. 06, 2023. (Online). Available:\nhttps://arxiv.org/abs/2107.03374v2\n[7] J. Newsome, B. Karp, and D. Song, “Polygraph: Automatically gener-\nating signatures for polymorphic worms,” in Security and Privacy, 2005\nIEEE Symposium on, 2005, pp. 226–241.\n[8] H. B. H. III, R. L. Constable, and S. Sahni, “On the\nComputational Complexity of Program Scheme Equivalence,”\nhttps://doi.org/10.1137/0209031, vol. 9, no. 2, pp. 396–416, Jul.\n2006, doi: 10.1137/0209031.\n[9] L. Theis, A. Van Den Oord, and M. Bethge, “A note on the\nevaluation of generative models,” 4th International Conference on\nLearning Representations, ICLR 2016 - Conference Track Proceed-\nings, Nov. 2015, Accessed: Jul. 17, 2023. (Online). Available:\nhttps://arxiv.org/abs/1511.01844v3\n[10] S. R. Eddy, “What is a hidden Markov model?,” Nature Biotechnol-\nogy 2004 22:10, vol. 22, no. 10, pp. 1315–1316, Oct. 2004, doi:\n10.1038/nbt1004-1315.\n[11] A. Vaswani et al., “Attention is all you need,” proceed-\nings.neurips.cc, Accessed: Jul. 06, 2023. (Online). Available:\nhttps://proceedings.neurips.cc/paper/7181-attention-is-all\n[12] T. Mikolov, S. Kombrink, L. Burget, J. ˇCernock´y, and S. Khu-\ndanpur, “Extensions of recurrent neural network language model,”\nICASSP, IEEE International Conference on Acoustics, Speech\nand Signal Processing - Proceedings, pp. 5528–5531, 2011, doi:\n10.1109/ICASSP.2011.5947611.\n[13] J. Devlin, M. W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-\ntraining of Deep Bidirectional Transformers for Language Understand-\ning,” NAACL HLT 2019 - 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human\nLanguage Technologies - Proceedings of the Conference, vol. 1, pp.\n4171–4186, Oct. 2018, Accessed: Jul. 17, 2023. (Online). Available:\nhttps://arxiv.org/abs/1810.04805v2\n[14] T. B. Brown et al., “Language Models are Few-Shot Learners,” Adv\nNeural Inf Process Syst, vol. 2020-December, May 2020, Accessed: Jul.\n17, 2023. (Online). Available: https://arxiv.org/abs/2005.14165v4\n[15] Z. Feng et al., “CodeBERT: A Pre-Trained Model for Programming\nand Natural Languages,” Findings of the Association for Computational\nLinguistics Findings of ACL: EMNLP 2020, pp. 1536–1547, Feb. 2020,\ndoi: 10.18653/v1/2020.findings-emnlp.139.\n[16] E. Nijkamp et al., “CodeGen: An Open Large Language Model for\nCode with Multi-Turn Program Synthesis,” Mar. 2022, Accessed: Jul.\n06, 2023. (Online). Available: https://arxiv.org/abs/2203.13474v5\n[17] D. Zan et al., “CERT: Continual Pre-Training on Sketches for Library-\nOriented Code Generation,” IJCAI International Joint Conference on\nArtificial Intelligence, pp. 2369–2375, Jun. 2022, doi: 10.24963/ij-\ncai.2022/329.\n[18] Y . Li et al., “Competition-level code generation with AlphaCode,”\nScience (1979), vol. 378, no. 6624, pp. 1092–1097.\n[19] A. R. Openai, K. N. Openai, T. S. Openai, and I. S. Openai, “Improving\nLanguage Understanding by Generative Pre-Training”, Accessed: Jul.\n17, 2023. (Online). Available: https://gluebenchmark.com/leaderboard\n[20] S. Gunasekar et al., “Textbooks Are All You Need,” Jun. 2023, Accessed:\nJul. 06, 2023. (Online). Available: https://arxiv.org/abs/2306.11644v1\n[21] E. Nijkamp, H. Hayashi, C. Xiong, S. Savarese, and Y . Zhou, “Code-\nGen2: Lessons for Training LLMs on Programming and Natural Lan-\nguages,” May 2023, Accessed: Jul. 17, 2023. (Online). Available:\nhttps://arxiv.org/abs/2305.02309v2\n[22] R. Li et al., “StarCoder: may the source be with you!,”\nMay 2023, Accessed: Jul. 17, 2023. (Online). Available:\nhttps://arxiv.org/abs/2305.06161v1\n[23] Y . Wang, H. Le, A. D. Gotmare, N. D. Q. Bui, J. Li, and S. C.\nH. Hoi, “CodeT5+: Open Code Large Language Models for Code\nUnderstanding and Generation,” May 2023, Accessed: Jul. 17, 2023.\n(Online). Available: https://arxiv.org/abs/2305.07922v2\n[24] “codeparrot/codeparrot · Hugging Face.”\nhttps://huggingface.co/codeparrot/codeparrot (accessed Jul. 17, 2023).\n[25] L. Ben Allal et al., “SantaCoder: don’t reach for the stars!,”\nJan. 2023, Accessed: Jul. 17, 2023. (Online). Available:\nhttps://arxiv.org/abs/2301.03988v2\n[26] Z. Luo et al., “WizardCoder: Empowering Code Large Language Models\nwith Evol-Instruct,” arxiv.org, Accessed: Jul. 06, 2023. (Online). Avail-\nable: https://arxiv.org/abs/2306.08568\n[27] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “BLEU: a Method for\nAutomatic Evaluation of Machine Translation”.\n[28] S. Ren et al., “CodeBLEU: a Method for Automatic Evaluation of Code\nSynthesis,” Sep. 2020, Accessed: Jul. 17, 2023. (Online). Available:\nhttps://arxiv.org/abs/2009.10297v2\n[29] S. Kulal et al., “SPoC: Search-based Pseudocode to Code,” Adv Neural\nInf Process Syst, vol. 32, 2019.\n[30] B. Roziere, M. A. Lachaux, L. Chanussot, and G. Lample, “Unsuper-\nvised Translation of Programming Languages,” Adv Neural Inf Process\nSyst, vol. 2020-December, Jun. 2020, Accessed: Jul. 17, 2023. (Online).\nAvailable: https://arxiv.org/abs/2006.03511v3\n[b31]",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5813720226287842
    },
    {
      "name": "Metamorphic rock",
      "score": 0.5761463642120361
    },
    {
      "name": "Geology",
      "score": 0.5320502519607544
    },
    {
      "name": "Malware",
      "score": 0.4633871018886566
    },
    {
      "name": "Geochemistry",
      "score": 0.227076917886734
    },
    {
      "name": "Computer security",
      "score": 0.22659757733345032
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I39470171",
      "name": "Ontario Tech University",
      "country": "CA"
    }
  ],
  "cited_by": 8
}