{
  "title": "Improving Video Vision Transformer for Deepfake Video Detection Using Facial Landmark, Depthwise Separable Convolution and Self Attention",
  "url": "https://openalex.org/W4390738584",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2396692425",
      "name": "Kurniawan Nur Ramadhani",
      "affiliations": [
        "Bandung Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1985787330",
      "name": "Rinaldi Munir",
      "affiliations": [
        "Bandung Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2137925388",
      "name": "Nugraha Priya Utama",
      "affiliations": [
        "Bandung Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2914447220",
    "https://openalex.org/W2913399670",
    "https://openalex.org/W3011820288",
    "https://openalex.org/W2891145043",
    "https://openalex.org/W2889677435",
    "https://openalex.org/W2970868842",
    "https://openalex.org/W3083246145",
    "https://openalex.org/W2963720850",
    "https://openalex.org/W2911424785",
    "https://openalex.org/W2995516027",
    "https://openalex.org/W3034713808",
    "https://openalex.org/W6728184133",
    "https://openalex.org/W4214612132",
    "https://openalex.org/W2884585870",
    "https://openalex.org/W2963684180",
    "https://openalex.org/W2962958939",
    "https://openalex.org/W4220658233",
    "https://openalex.org/W3005939059",
    "https://openalex.org/W2811414481",
    "https://openalex.org/W6764542186",
    "https://openalex.org/W6762480454",
    "https://openalex.org/W3198362366",
    "https://openalex.org/W4213019189",
    "https://openalex.org/W3202495979",
    "https://openalex.org/W6784810453",
    "https://openalex.org/W6757599822",
    "https://openalex.org/W2982058372",
    "https://openalex.org/W2904573504",
    "https://openalex.org/W4287758545",
    "https://openalex.org/W2531409750"
  ],
  "abstract": "In this paper, we present our result of research in video deepfake detection. We built a deepfake detection system to detect whether a video is a deepfake or real. The deepfake detection algorithm still struggle in providing a sufficient accuracy values, especially in challenging deepfake dataset. Our deepfake detection system utilized spatiotemporal feature that extracted using Video Vision Transformer (ViViT). The main contribution of our research is providing a deepfake detection system that based on ViViT architecture and using landmark area images for the input of the system. Our system extracted the feature from a number of spatial features. The spatial feature was extracted using Depthwise Separable Convolution (DSC) block combined with Convolution Block Attention Module (CBAM) from tubelet. The tubelet was a representation of facial landmark area that was extracted from the input video. In our system, we used 25 facial landmark area for an input video. In our experiment we used Celeb-DF version 2 dataset because it is considered to be a challenging deepfake dataset. We conducted augmentation to the dataset, so we obtained 8335 videos for training set, 390 videos for validation set, and 1123 videos for testing set. We trained our deepfake detection system using Adam optimizer, with learning rate of 10&#x2013;4 and 100 epoch. From the experiment, we obtained the accuracy score of 87.18&#x0025; and F1 score of 92.52&#x0025;. We also conducted the ablation study to display the effect of each part of our model to the overall system performance. From this research, we obtained that by using landmark area images, our ViViT based deepfake detection system had a good performance in detecting deepfake videos.",
  "full_text": " \nVOLUME XX, 2017 1 \nDate of publication xxxx 00, 0000, date of current version xxxx 00, 0000.  \nDigital Object Identifier 10.1109/ACCESS.2017.Doi Number \nImproving Video Vision Transformer for \nDeepfake Video Detection using Facial \nLandmark, Depthwise Separable Convolution \nand Self Attention  \nKurniawan Nur Ramadhani 1, Rinaldi Munir2, and Nugraha Priya Utama3 \n1,2,3 Bandung Institute of Technology, Bandung 40132, Indonesia  \n1Telkom University, Bandung 40257, Indonesia  \nCorresponding author: Kurniawan Nur Ramadhani (e-mail: 33219303@mahasiswa.itb.ac.id)  \nABSTRACT In this paper, we present our result of research in video deepfake detection. We built a deepfake \ndetection system to detect whether a video is a deepfake or real. The deepfake detection algorithm still \nstruggle in providing a sufficient accuracy value s, especially in challenging deepfake dataset. Our deepfake \ndetection system utilized spatiotemporal feature that extracted using Video Vision Transformer (ViViT). The \nmain contribution of our research is providing a deepfake detection system that based on ViViT architecture \nand using landmark area images for the input of the system. Our system extracted the feature from a number \nof spatial features. The spatial feature was extracted using Depthwise Separable Convolution (DSC) block \ncombined with Convolution Block Attention Module (CBAM) from tubelet. The tubelet was a representation \nof facial landmark area that was extracted from the inp ut video. In our system, we used 25 facial landmark \narea for an input video. In our experiment we used Celeb-DF version 2 dataset because it is considered to be \na challenging deepfake dataset. We conducted augmentation to the dataset, so we obtained 8335 videos for \ntraining set, 390 videos for validation set, and 1123 videos for testing set. We trained our deepfake detection \nsystem using Adam optimizer, with learning rate of 10 -4 and 100 epoch. From the experiment, we obtained \nthe accuracy score of 87.18% and F1 score of 92.52%.  We also conducted the ablation study to display the \neffect of each part of our model to the overall system performance. From this research, we obtained that by \nusing landmark area images, our ViViT based deepfake detection system had a good performance in detecting \ndeepfake videos.  \nINDEX TERMS deepfake detection, facial landmark, depthwise separable convolution, convolution block \nattention module, video vision transformer. \nI. INTRODUCTION \nThe development of algorithms in the field of machine \nlearning in various fields has allowed humans to build \napplications that can do things that were previously difficult \nfor existing computers to do. Particularly in the field of \ncomputer vision, the development of machine learning \nalgorithms has made it possible for technology to manipulate \nand even create images and videos at a further level. This is \nin addition to providing a positive effect also has a negative \nimpact. Deepfake is one of the negative effects that arise \nfrom the development of computer vision technology. \nDeepfakes are co ntent in the form of images or videos that \nare manipulated using deep learning algorithms. Usually \ndeepfake content manipulates videos of people doing \nsomething. The manipulation process that is carried out is by \nexchanging the faces of people who are in t he video with \nother people's faces. In its implementation, deepfake can be \nused for several purposes. Several deepfake videos \ncirculating on social networking applications feature certain \nclips of films with the faces of actors replaced with the faces \nof o ther people. Some of these videos are just for fun. \nHowever, some deepfake videos have quite serious negative \neffects. \nTwo cases that often become objects of deepfake are \npornographic videos and black campaign videos. In the case \nof pornographic videos, deepfakes manipulate pornographic \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3352890\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \nVOLUME XX, 2017 9 \nvideos by replacing the actor's face with another face, which \nis usually the face of a particular artist or public figure. The \ngoal is to drop the image of the character. Another case is a \npolitical campaign video. In the ca se of black campaign \nvideos, deepfakes manipulate videos of someone making \ncontroversial statements by replacing the speaker's face with \nanother face, which is usually the face of a certain political \nfigure who is participating in a political contest. The aim is \nto drop the electability of certain political figures. These two \ncases are examples of the harmful use of deepfakes. \nLooking at these two cases, technology to detect deepfake \ncontent in images and videos is indispensable. Deepfake \ndetection technology will be very useful to protect people if \nthey fall victim to the wrong use of deepfake technology. \nSince 2017, many studies have been conducted to develop \ndeepfake detection methods, both in images and in videos. \nThe deepfake detection method basically attempts to classify \nimage or video content into the fake class or the original \nclass. The deepfake detection algorithm attempts to extract \nfeatures from image or video content that can be used to \ndistinguish fake content from original content. \nIn general, there are four feature extraction approaches that \nhave been developed to detect deepfakes. The first approach \nis to use visual traits, which are traits that appear directly \nfrom deepfake content. These visual characteristics can be in \nthe form of eye blinks, head position and other characteristics \nthat can be observed on the face. The deepfake detection \napproach with an eye wink gets an AUC score of 0.99. The \nweakness of this approach is that the detection process is \nhighly dependent on the eye area locati on detection \nalgorithm and the t est data used is only 10 videos [1]. The \ndeepfake detection approach using head pose information \nmanaged to get an AUC score of 0.89 in the UADFV dataset. \nAnother visual artifact approach succeeded in achieving an \nAUC score of 0.85 on deepfake test data  derived from 4 \noriginal videos[2]. \nThe second approach is to use local features, namely features \nextracted by certain methods at the image pixel level. Local \nfeature extraction method has better robustness than visual \nfeatures. Research conducted in 2019 compared several local \nfeature extraction approaches in detecting deepfakes and \nconcluded that Image Quality Metric (IQM) is the best local \nfeature extraction for detecting deepfakes with an Equal \nError Rate (EER) score of 8.97% in the DF-TIMIT[3]. \nThe third approach is to use deep features. As with local \nfeature extraction, deep feature extracts features at the pixel \nlevel, but uses deep learning algorithms so that the resulting \nfeatures are more complex. Several deep feature models that \nhave been developed to detect deepfakes include \nMesoNet[4], DeepFD [5], Com mon Fake Feature Network \n(CFFN)[6], MultiTask Learning[7] and Capsule Forensic[8]. \nThe fourth  approach is to use a temporal feature. This \napproach only works with videos. This approach, in addition \nto using features at the pixel level, also uses temporal \ninformation from several successive frames. Examples of \nthis approach include using a Re current Neural Network \n(RNN)[9] and Optical Flow[10]. \nThe main challenge of deepfake detection algorithm \ndevelopment is the ever -evolving deepfake content \ndevelopment algorithm. With the development of the \ndeepfake algorithm, the ability of the deepfake  detection \nmethod must continue to be improved. This can be observed \nfrom the development of the dataset used in deepfake \nresearch. In 2019, the Celeb -DF dataset was used to t est \nseveral deepfake algorithms[11]. The results of the test show \nthat the deepfake detection algorithms that have been \ndeveloped have not succeeded in showing good performance \nin detecting deepfakes on the Celeb -DF dataset with an \naccuracy value of all algorithms tested less than 70%. This \nlow accuracy value is caused by the generalization ability of \neach deepfake detection method which is still weak. In this \nresearch, the deep fake detection algorithm that has the \nhighest accuracy is the XceptionNet [12] model which is a \ndeep feature approach. \nIn this research, we built a system to detect deepfake videos \nusing spatiotemporal approach. We used Video Visio n \nTransformer (ViViT)[13] to detect the deepfake videos. We \nused 2 5 facial landmark location as input for our ViViT \nmodel. In the encoding part of our ViViT, we used \nDepthwise Separable Convolution (DSC) [12] block \ncombined with Convolution Block Attention Module \n(CBAM)[14] to extract feature from the tubelet. We used \nCeleb-DF version 2 for the dataset of our re search. In brief, \nour contribution in this paper as follows: \na. We built deepfake video detection system using Video \nVision Transformer (ViViT) \nb. We used 25 facial landmark extracted from dataset as our \ninput for the detection system \nc. We used DSC combined with C BAM to extract feature \nfrom the tubelet before the positional encoding block in \nViViT \nII. RELATED WORKS \nThe detection of deepfake content is a binary process that \ninvolves the extraction of features from images and videos to \ndifferentiate between genuine and deepfake content. This \nmethod can be classified into four distinct categories, \ndepending on the particular feature extraction approach \nemployed. \nA. VISUAL FEATURE-BASED DEEPFAKE DETECTION \nThis approach relies on facial feature that can be observed in \nplain sight, such as head pos e, eye blink, and dissemblance \nin facial organ shape. Research using this approach were first \nconducted in 2018  that used eye blink for the main \nfeature[1]. The hypothesis behind the method is that there are \nvariations in the blink pattern of the deepfake compared to \nthe original video. \nAnother research used  head pose inconsistencies  to detect \ndeepfakes[15]. This approach considers inconsistencies \nbetween facial poses as well as body parts outside of the face \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3352890\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \nVOLUME XX, 2017 9 \nsuch as neck and s houlders. Figure 1 shows how to use 68 \nfacial landmarks to estimate head pose inconsistency. The 68 \nfacial landmarks (68 blue landmarks) were compared to 17  \nred landmarks (17 red landmarks) which represent the pose \ndirection from the midpoint facial area. \nAnother approach tries to extract visual features on the \ndeepfake face[2]. These were referred to as visual artifacts. \nVisual artifacts are extracted from the imperfect ness of the \ndeepfake because the resources were limited i n the process \nof creating the deepfake.  Visual artifacts include  left eye \ncolor difference, right eye color difference, disproportionate \nshadows, fack of light reflection detail , and lack of detailed \ngeometry. Visual artifacts such as  left eye color difference, \nright eye color difference, disproportionate shadows in nose \narea, non-visible light reflections, and less detailed geometry \nof teeth . Visual artifacts were extracted using a color and \ngeometry extraction approach from precise parts of the face, \nsuch as nose, lips, teeth, eyes, and eyebrows. \n \nIt is possible to use this visual feature approach to detect a \ndeepfake. However, as the content creation methods for \ndeepfakes become more advanced, the detection of these \nvisual features becomes more difficult. As a result, these \ndeepfakes detection methods based on visual features \nbecome less effective. \nB. LOCAL FEATURE-BASED DEEPFAKE DETECTION \nThis approach  used pixel-based segmentation to ex tract \nfeatures from each pixel of an image. Local feature -based \ndetection is more reliable than visual features. In 2017, a \nresearch combined two features from an image convolution \nand two features from image steganalysis to identify \ntampered areas in a  facial image[16]. This research formed \nthe fundament for local and deep feature deepfake detection \nmethods. \nAnother approach was to use photo response non uniformity \nanalysis with cross correlation operations (PRNU) [17]. \nHowever, this research used a dataset of only 10 videos.  \nAnother feature ex traction method used to detect deepfakes \nis the scale -invariant feature transform (SIFT) [18]. SIFT \nidentifies pixel keypoints in an image and extracts features \nfrom those keypoints. \nOther feature extraction methods used in the deepfakes \ndetection process include Pyramid of Histogram of Oriented \nGradients (PHOG), Local Phase Quantization (LPQ), Local \nBinary Pattern (LBP), Speeded Up Robust Feature (SURF), \nBinary Gabor pattern (BGP), Binarized Statistical Image \nFeatures (BSIF), and Image Quality Metric (IQM)[19]. The \nIQM method is the most effective way to de tect deepfakes, \nand this was confirmed in a research that compare IQM's to \nLDA's and PCA's, in which IQM was the most effective  \nfeature. \nThe local feature-based detection method has been found to \nbe quite effective in detecting the presence of deepfakes in  \ncertain video data.  However, with the development of \ndeepfakes algorithms, the tampered contents tend to be seen \nmore natural and hard to detect as a deepfake. More complex \nfeatures were required to differentiate the original image and \nvideo. \nC. DEEP FEATURE-BASED DEEPFAKE DETECTION \nSimilar to local features, deep features also  performs pixel-\nlevel feature extraction. The difference is that the deep \nfeature extraction process uses many layers, meaning that it \ncapable to obtain more complex features than simple feature \nextraction techniques. In 2018, a research analyzed \nDenseNet, I nceptionNet, and XceptionNet to detect \ndeepfake images [20]. The XceptionNet architecture was \nfound to be the most reliable in detecting deepfakes. Another \nresearch used CNN architecture  with 5 layers , named \nDeepFD, with a contrastive loss function providing good \nperformance for detecting various  GAN generate d \nimages[5]. Further research expanded DeepFD with a pair \nlearning method that increased the generalization ability of \nthe model[6]. In a pair learn ing approach, two pairs of real \nor fake images are analyzed using contrastive learning. The \nnew model were called Common Feature Fake Network \n(CFFN). \nMesoNet was another CNN model based on the inception \nmodule for detecting deepfakes[4]. This model was able to \ndetect deepfake videos with compression conditions similar \nto social media videos.  However, research using Capsule \nNetwork architecture can compete with MesoNet for \ndetecting deepfake videos at per frame level and whole video \nlevel[8]. Capsule Network compensates for non -\nequivocation convolution blocks by routing by agreement \nmechanism. \nAnother deep learning model that has been developed for the \ndetection of deepfake is deep autoencoder[7]. The \nautoencoder was able to reconstruct contents and mark the \ndeepfake areas in the  contents. In this research, the \nAutoencoder use d two branches. One branch was used to \nreconstruct the deepfake  marking. The other was used to \ncompute the loss function of the Autocoder. Segmentation of \nthe image area improves the detection result compared to just \ndetecting the deepfake or the original content. \nD. TEMPORAL FEATURE -BASED DEEPFAKE \nDETECTION \nFIGURE 1. Head Pose Inconsistency Estimation. \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3352890\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \nVOLUME XX, 2017 9 \nUnlike the other approaches, this method extracts features \nfrom multiple sequential frames to get temporal features. \nThus, this method is applicable specifically to video.  \nTemporal feature approach works by extracting temporal \nfeatures from a sequence of v ideo frames. A sequence of \nvideo frames can be considered as a sequence of data. \nRecurrent Neural Network ( RNN) is one of the most well -\nknown deep neural network (DNN) models for processing \nsequence data. RNN was first used to detect deepfake content \nin videos in 2018 [9]. Another method of deepfake video \ndetection is to track facial and head movement and then \nextract 16 motion features from a specific area in the \nvideo[21]. In addition to RNN and motion tracking, Optical \nFlow method  has been used in deepfake video detection \nresearch with CNN as the classification algorithm[10]. RNN \nthen was re -used in 2019 by using two -way RNN and \nDenseNet which outperforms several DNN models \n(ResNet50, DenseNet)[22]. \nIII. METHODS \nIn this research, we built our deepfake detection system \nbased on Video Vision Transformer (ViViT) \narchitecture[13]. The overall system is shown in Figure 2. \nThe input of our system is video. Our method consist of \nseveral steps. The first step is the preprocessing, where the \nsystem extracts the 25 facial landmark area from the video \nand combines them into sequence of frames. Each frame \ncontains the 25 facial la ndmark area for respective frame \nfrom the input video. Next, the system build tubelet from the \nframe sequence. For each tubelet, the system extract the \nfeatures using Depthwise Separable Convolution (DSC) \nFIGURE 2. Our deepfake detection system. \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3352890\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \nVOLUME XX, 2017 9 \nblock combined with Convolution Based Attention Mod ule \n(CBAM). The feature s then flattened and encoded using \npositional encoder, then processed by spatiotemporal \nanalysis in the core of ViViT. The core itself contain of \nseveral Multihead Attention Block. The final process is the \nclassification using sigmoid function. \nA. Facial Landmark Detection \nFor this purpose, we used the facial landmark detec tion \noperation in dlib library [23]. Facial landmark detection \noperation is used to obtain a total of 68 landmark points on \nfacial images which contain important information, \nespecially facial curves, both on the edges of the face and \nareas inside the face, such as the curves of the nose, lips and \neyes. The facial landmark detection process in Dlib uses the \nHistogram of Oriented Gradients (HOG) algorithm and also \nthe Support Vector Machine (SVM) classification. In this \nresearch, we used only 25 landmark to simplify the input data \nfor the system. For each landmark, we extracted 11x11 area \ncentered by landmark location pixel. So, for one frame, we \nextracted new 55x55 frame that contain the information of \neach 11x11 area from 25 landmark location. The landmark \nlocation that we used can be seen on Figure 3. \nFIGURE 3. Facial Landmark Location. \nThe landmark area only consider the area that marked by the \nlandmark point. By that consideration, the landmark area \nimages lost some information from the original input. Our \nmodel only conserve the tubelet from landmark area. So,  \npatch position information produced by the positional \nencoder was only contained the position information of \nlandmark area. The tubelet sequence in our model did not \npreserve the original input's patch position. But, by using \nlandmark area images, the ViV iT was not overbloated by \nunnecessary information from the non landmark area, as we \nproved this statement by the ablation study in the section 5. \nB. Depthwise Separable Convolution (DSC) \nDSC is a convolution block module used in the Xception \nmodel that substitute the function of Inception module [12]. \nDepthwise separable convolution has 1x1 depthwise \nconvolution and pointwise convolution. Depthwise \nconvolution is a separate convolution process for each input \nchannel. Pointwise convolution is a convolution using 1x1 \nblock on all input channel. The detail of DSC is described in \nFigure 4. \nThe first step in this DSC is to perform convolution on each \nchannel in turn. Then in the next step is to perform pointwise \nconvolution which was a standard convolution with the 1x1 \nkernel. Using depthwise separable convolution can reduce \nthe number of mathematical operations and the numb er of \nparameters used in the process. \nC. Convolution Block Attention Module (CBAM) \nIn this research, beside using DSC, we also used CBAM \nmodel to extract feature from tubelet . This module is a \nsimple attention model specifically designed for the CNN \narchitecture. CBAM consists of two processes, namely the \nchannel attention module and the spatial attention \nmodule[14]. Figure 5 shows the CBAM architecture. \nChannel Attention Module is a series of operations to \ngenerate channel attention maps. The channel attention map \nrepresents the strength of the relationship between channels \nfrom the input data features. The channel attention module \nbegins with max pool and average pool operations to get a \nrepresentation of the value of each channel. Then the two \nvectors are processed using the same neural network then the \nresults of the two are added up and multiplied by the initial \nfeatures to get the channel attention map. \nThe spatial attention module is used to obtain spatial \nattention map values. In CBAM, the spatial attention module \nprocesses values from the channel attention map generated \nFIGURE 5. CBAM Architecture. \nFIGURE 4. Depthwise Separable Convolution. \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3352890\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \nVOLUME XX, 2017 9 \nfrom the spatial attention module. The spatial attention \nmodule begins with the max pool and average pool \noperations. The two pooling results are connected and then \nconvoluted into one feature map. The feature map is then \nmultiplied by the input features to produce a spatial attention \nmap. Figure 6 shows the channel and spatial attention \nmodule. \nWhile the ViViT already provide the self -attention \nmechanism, the attention value provided by ViViT is only \nthe attention value between different encoded tubelet. The \nusing of CBAM was to provide the attention value between \nthe feature values in one tubelet. As has been mentioned \nbefore, CBAM provide the attention for channel and spatial \ndimension. So in our model, the value of one tubelet was \nenriched by the spatial and channel attention value provided \nby CBAM. \nD. Video Vision Transformer \nVision Transformer (ViT) receives sequential data in image \nfragments for classification using the transformer encoder \nmodule, which is then decoded by the linear embedding layer \nbelonging to the transformer model. Figure 7  shows the \ngeneral form of the ViT  model with layers of embedding, \nencoding, and classifier. Different from CNN model in \nwhich uses spatial space with a kernel and local receptive \nfield, ViT can use attention in a part of the image that is \nbroken down by converting  the image data into vectors , \nbecause this ViT requires large datasets to obtain sufficient \nspatial knowledge[24]. \nThe first step is to partition the training dataset into patches. \nEvery patches then flattened to become token. Because our \ndata is in video form , we used tubelet for representation of \npatch. We extracted the volume from input video that contain \nimage patches as well as temporal information from the \nvideo. In our system,  every tubelet represented the \ninformation of respective landmark location, so for one input \nvideo we extracted 25 tubelet.  \nIV. EXPERIMENTAL SETUP \nIn this research, we used the Celeb-DF dataset version 2[11]. \nWe chose this dataset because of its variation and \nchallenging characteristics in deepfake detection research. In \nthe dataset, there are 890 original videos and 5639 deepfake \nvideos. We processed the dataset using the facial landmark \ndetection function from dlib to obtain 25 landmark locations. \nFrom the landmark locations, we took area of 11x11 with the \nlandmark location as the center of the area, then we \ncombined the 25 areas into one new frame with a size of \n55x55. From one video, we took 55 frames so that the \ndimensions of the input v ideo on our system were \n55x55x55x3, where 3 is the RGB channel. For the training \nset, we carr ied out an augmentation process in the form of \nhorizontal flip, random rotation and increasing the pixel \nvalue so that the total training set we use d was 8335 videos. \nWe also used 390 videos for validation set in the training \nprocess. For the testing set, we took 1123 videos with a \nproportion of 139 original videos and 984 deepfake videos.  \nFigure 8  shows the example of original face and it’s new \nframe created from our process. \nThe development and testing environment that we used in \nthis research was Google Colaboratory pro. Google \nColaboratory pro has Nvidia A100 GPU with 80GB of \nVRAM and 32GB of RAM. The parameters that we used in \nthe learning process were learning rate of 10-4, batch size of \n8, epoch of 100 and Adam as our optimizer algorithm.  \nFIGURE 6. Channel and Spatial Attention Module. \nFIGURE 8. Example of Original Face and Landmark Area \nExtraction Result. \nFIGURE 7. Conventional Vision Transformer. \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3352890\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \nVOLUME XX, 2017 9 \nOther hyper parameters that we used in the network \narchitecture were patch size of 11x11x11, 128 projection \ndimensions, 32 attention heads and 16 attention layers. \nV. RESULT AND DISCUSSION \nIn this section, we present the res ult of our experiment. \nFigure 9 shows the validation and training accuracy over the \ntraining process. We can see that over the training process, \nthe training accuracy was convergent to 100% and the \nvalidation accuracy was unstable between 70% and 80%. We \ncan see that our system still had some overfitting issue, \nalthough it’s not significant. \nNext we can see the performance of our system in the testing \nprocess. Figure 10  shows the confusion matrix for our \nsystem. We can see that our system successfully recognized \n890 deepfake videos from 984 deepfake videos in the testing \nset. Our system also recognized 89 real videos from 139 real \nvideos in the testing set.  We can see that our system had a \ngood performance in t he testing process, with F1 score of \n92.52% and accuracy of 87.18%.  \nIn our research, we compared our system with conventional \nViViT without using DSC+CBAM. We can see the \ncomparison of accuracy in table 1 . As can be seen, our \nsystem had better performance compared to conventional \nViViT. The DSC+CBAM gave positive effect on detection \nsystem performance . By combining DSC and CBAM, we \nobtained a good feature extractor for each landmark area \ntubelet. \nWe also compared our system with other deepfake detection \nsystem. We co mpared our system with Mesonet [4], \nXception[12], an d Xception+CBAM [25]. The comparison \nresult can be seen on table 2. From the table, we can see that \nour system had better performance in term of accuracy. This \nresult showed that the spatiotemporal features that extracted \nfrom our model can improve the performance of deepfake \ndetection system. The use of landmark area also increased \nthe performance by narrowing the extraction area in the \nfeature extraction process.  \n \nTABLE 1. Comparison between our system and ViViT. \nDetector Accuracy \nViViT 52.14% \nOur system 87.18% \n \n \nTABLE 2. Comparison between our system and other \ndeepfake detection system. \nDetector Accuracy \nMesonet 65.12% \nXception 72.28% \nXception+CBAM 75.49% \nOur system 87.18% \n  \nWe also conducted ablation study in our research. We \ncompared our method w ith the o ther combination o f ViT, \nlandmark extraction, DSC and CBAM as follows: \na. Version 1 that used ViT, DSC and CBAM module. \nb. Version 2 that used ViT, landmark extraction a nd DSC \nmodule. \nc. Version 3 that used ViT and landmark extraction. \nd. Version 4 that used ViT and DSC module. \ne. Version 5 that used only ViT. \nTable 3 showed the comparison of result from the ablation \nstudy. By comparing our method result and version 2 result, \nand also comparing result of version 1 and version 4, we \nconcluded that CBAM module g ave a positive effect on the \naccuracy and F1-score. By comparing version 2 and version \n3 result, we concluded that DSC module also gave a positive \neffect on the system performance. By comparing the result \nof our method and version 1, and also by comparing result of \nversion 2 and version 4, and also the result of version 3 and \nversion 5, we conc luded that the landmark area extracted \nfrom input images had  significant information that boosted \nthe performance of ViViT for detecting deepfake. \n \nFIGURE 9. Training Set and Validation Set Accuracy over \nthe training process. \nFIGURE 10. Confusion Matrix of Our Deepfake Detection \nSystem. \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3352890\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \nVOLUME XX, 2017 9 \nTABLE 3. Ablation study result. \nModel ViViT landmark \nextraction DSC CBAM acc F1 \nscore \nVersion \n1 √  √ √ 73.12% 80.67% \nVersion \n2 √ √ √  75.71% 84.01% \nVersion \n3 √ √   68.76% 79.63% \nVersion \n4 √  √  67.74% 74.84% \nVersion \n5 √    52.14% 63.60% \nour \nmethod √ √ √ √ 87.17% 92.51% \n \nApart from Celeb-DF version 2, we also evaluated our model \nusing several differe nt public dataset such as Deepfake \nDetection Challenge (DFDC) dataset [26], Deepfake TIMIT \ndataset[27], and FaceF orensics++ deepfake dataset[28]. \nTable 4 displayed the performance of our system over those \ndataset. We can see that our model had a consistent \nperformance in terms of accuracy and F1 score over different \ndeepfake dataset. \n \nTable 4. Performance Comparison for Several Different \nDataset. \nDataset Accuracy F1 score \nCeleb-DF version 2 87.17% 92.51% \nDeepfake Detection Challenge \n(DFDC) 88.03% 90.23% \nDeepfake TIMIT 94.14% 97.39% \nFaceForensics++ 90.27% 93.54% \n \nVI. CONCLUSION \nIn this research, we built a deepfake detection system that \ndetect whether the video input is a deepfake video or real \nvideo. We extracted 25 landmark area from the input videos \nbefore processed into our deepfake detection system. We \ncombined DSC and CBAM to extract the spatial features \nfrom each of landmark area. Then, the spatial features were \nprocessed using ViViT to detect deepfake. From the \nexperiment, our system shown a good result by obtaining \naccuracy of 87.18%. Our system successfully detected 890 \nfrom 984 deepfake videos and 89 from 139 real videos. Our \nsystem also successfully outperform some other deepfake \ndetection system, such as Mesonet, Xception and \nXception+CBAM. Our approach by extracting 25 landmark \narea from videos has been proven to impr ove the \nperformance of deepfake detection system by reducing \nunimportant area for feature extraction process. The \ncombination of Xception and CBAM was also proven to be \na suitable feature extractor for the landmark area tubelet. The \nspatiotemporal extracted from our ViViT also improved the \nperformance in detecting deepfake videos. \nREFERENCE \n[1] Y. Li, M. -C. Chang, and S. Lyu, ‘In Ictu Oculi: \nExposing AI Created Fake Videos by Detecting Eye \nBlinking’, in 2018 IEEE International Workshop on \nInformation Forensics and Security (WIFS) , Hong \nKong, Hong Kong: IEEE, Dec. 2018, pp. 1 –7. doi: \n10.1109/WIFS.2018.8630787. \n[2] F. Matern, C. Riess, and M. Stamminger, ‘ Exploiting \nVisual Artifacts to Expose Deepfakes and Face \nManipulations’, in 2019 IEEE Winter Applications of \nComputer Vision Workshops (WACVW) , Waikoloa \nVillage, HI, USA: IEEE, Jan. 2019, pp. 83 –92. doi: \n10.1109/WACVW.2019.00020. \n[3] Z. Akhtar and D. Dasgu pta, ‘A Comparative \nEvaluation of Local Feature Descriptors for \nDeepFakes Detection’, in 2019 IEEE International \nSymposium on Technologies for Homeland Security \n(HST), Woburn, MA, USA: IEEE, Nov. 2019, pp. 1 –\n5. doi: 10.1109/HST47167.2019.9033005. \n[4] D. Afchar, V. Nozick, J. Yamagishi, and I. Echizen, \n‘MesoNet: a Compact Facial Video Forgery Detection \nNetwork’, in 2018 IEEE International Workshop on \nInformation Forensics and Security (WIFS) , Hong \nKong, Hong Kong: IEEE, Dec. 2018, pp. 1 –7. doi: \n10.1109/WIFS.2018.8630761. \n[5] C.-C. Hsu, C.-Y. Lee, and Y.-X. Zhuang, ‘Learning to \nDetect Fake Face Images in the Wild’, in 2018 \nInternational Symposium on Computer, Consumer \nand Control (IS3C) , Taichung, Taiwan: IEEE, Dec. \n2018, pp. 388–391. doi: 10.1109/IS3C.2018.00104. \n[6] C.-C. Hsu, Y.-X. Zhuang, and C.-Y. Lee, ‘Deep Fake \nImage Detection Based on Pairwise Learning’, Appl. \nSci., vol. 10, no. 1, p. 370, Jan. 2020, doi: \n10.3390/app10010370. \n[7] H. H. Nguyen, F. Fang, J. Yamagishi, and I. Echizen, \n‘Multi-task Learning For Detecting and Segmenting \nManipulated Facial Images and Videos’, in 2019 IEEE \nInternational Conference on Biometrics: Theory, \nApplications and Systems (BTAS) , Jun. 2019. \nAccessed: Apr. 01, 2020. [Online]. Available: \nhttp://arxiv.org/abs/1906.06876 \n[8] H. H. Nguyen, J. Yamagishi, and I. Echizen, ‘Capsule-\nforensics: Using Capsule Networks to Detect Forged \nImages and Videos’, in ICASSP 2019 - 2019 IEEE \nInternational Conference on Acoustics, Speech and \nSignal Processing (ICASSP) , Brighton, United \nKingdom: IE EE, May 2019, pp. 2307 –2311. doi: \n10.1109/ICASSP.2019.8682602. \n[9] D. Guera and E. J. Delp, ‘Deepfake Video Detection \nUsing Recurrent Neural Networks’, in 2018 15th IEEE \nInternational Conference on Advanced Video and \nSignal Based Surveillance (AVSS) , Auckl and, New \nZealand: IEEE, Nov. 2018, pp. 1 –6. doi: \n10.1109/AVSS.2018.8639163. \n[10] I. Amerini, L. Galteri, R. Caldelli, and A. Del Bimbo, \n‘Deepfake Video Detection through Optical Flow \nBased CNN’, in 2019 IEEE/CVF International \nConference on Computer Vision Workshop (ICCVW), \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3352890\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \nVOLUME XX, 2017 9 \nSeoul, Korea (South): IEEE, Oct. 2019, pp. 1205 –\n1207. doi: 10.1109/ICCVW.2019.00152. \n[11] Y. Li, X. Yang, P. Sun, H. Qi, and S. Lyu, ‘Celeb-DF: \nA Large -Scale Challenging Dataset for DeepFake \nForensics’, in 2020 IEEE/CVF Conference on \nComputer Vision and Pattern Recognition (CVPR) , \nJun. 2020, pp. 3204 –3213. doi: \n10.1109/CVPR42600.2020.00327. \n[12] F. Chollet, ‘Xception: Deep Learning with Depthwise \nSeparable Convolutions’. arXiv, Apr. 04, 2017. \nAccessed: Jan. 12, 2023. [Online]. Available: \nhttp://arxiv.org/abs/1610.02357 \n[13] A. Arnab, M. Dehghani, G. Heigold, C. Sun, M. Lučić, \nand C. Schmid, ‘ViViT: A Video Vision \nTransformer’. 2021. \n[14] S. Woo, J. Park, J.-Y. Lee, and I. S. Kweon, ‘CBAM: \nConvolutional Block Attention Module’. arXiv, Jul. \n18, 2018. Accessed: Jan. 14, 2023. [Online]. \nAvailable: http://arxiv.org/abs/1807.06521 \n[15] X. Yang, Y. Li, and S. Lyu, ‘Exposing Deep Fakes \nUsing Inconsistent Head Poses’, in ICASSP 2019 - \n2019 IEEE International Conference on Acoustics, \nSpeech and Signal  Processing (ICASSP) , Brighton, \nUnited Kingdom: IEEE, May 2019, pp. 8261 –8265. \ndoi: 10.1109/ICASSP.2019.8683164. \n[16] P. Zhou, X. Han, V. I. Morariu, and L. S. Davis, ‘Two-\nStream Neural Networks for Tampered Face \nDetection’, in 2017 IEEE Conference on Comp uter \nVision and Pattern Recognition Workshops (CVPRW), \nIEEE, Mar. 2018, pp. 1831 –1839. Accessed: Apr. 01, \n2020. [Online]. Available: \nhttp://arxiv.org/abs/1803.11276 \n[17] M. Koopman, A. M. Rodriguez, and Z. Geradts, \n‘Detection of Deepfake Video Manipulation ’, in The \n20th Irish Machine Vision and Image Processing \nConference (IMVIP). 2018, 2018, p. 4. \n[18] G. Wang, Q. Jiang, X. Jin, and X. Cui, ‘FFR_FD: \nEffective and fast detection of DeepFakes via feature \npoint defects’, Inf. Sci., vol. 596, pp. 472 –488, 2022, \ndoi: https://doi.org/10.1016/j.ins.2022.03.026. \n[19] P. Korshunov and S. Marcel, ‘Vulnerability \nassessment and detection of Deepfake videos’, in 2019 \nInternational Conference on Biometrics (ICB) , Crete, \nGreece: IEEE, Jun. 2019, pp. 1 –6. doi: \n10.1109/ICB45273.2019.8987375. \n[20] F. Marra, D. Gragnaniello, D. Cozzolino, and L. \nVerdoliva, ‘Detection of GAN -Generated Fake \nImages over Social Networks’, in 2018 IEEE \nConference on Multimedia Information Processing \nand Retrieval (MIPR) , Miami, FL: IEEE, Apr. 2018,  \npp. 384–389. doi: 10.1109/MIPR.2018.00084. \n[21] S. Agarwal, H. Farid, Y. Gu, M. He, K. Nagano, and \nH. Li, ‘Protecting World Leaders Against Deep \nFakes’, in 2019 IEEE Conference on Computer Vision \nand Pattern Recognition Workshops, 2019, pp. 38–45. \n[22] E. Sabir, J. Cheng, A. Jaiswal, W. AbdAlmageed, I. \nMasi, and P. Natarajan, ‘Recurrent Convolutional \nStrategies for Face Manipulation Detection in \nVideos’, in 2019 IEEE Conference on Computer \nVision and Pattern Recognition Workshops, 2019, pp. \n80–87. \n[23] D. Zhang, J. Li, and Z. Shan, ‘Implementation of Dlib \nDeep Learning Face Recognition Technology’, in \n2020 International Conference on Robots & \nIntelligent System (ICRIS), Nov. 2020, pp. 88–91. doi: \n10.1109/ICRIS52159.2020.00030. \n[24] K. Han et al. , ‘A Survey on Vision Transformer’, \nIEEE Trans. Pattern Anal. Mach. Intell. , vol. 45, no. \n1, pp. 87 –110, Jan. 2023, doi: \n10.1109/TPAMI.2022.3152247. \n[25] H. Lin, W. Luo, K. Wei, and M. Liu, ‘IMPROVED \nXCEPTION WITH DUAL ATTENTION \nMECHANISM AND FEATURE FUSION FOR \nFACE F ORGERY DETECTION’, in 2022 4th \nInternational Conference on Data Intelligence and \nSecurity (ICDIS) , Aug. 2022, pp. 208 –212. doi: \n10.1109/ICDIS55630.2022.00039. \n[26] B. Dolhansky et al., ‘The deepfake detection challenge \n(dfdc) dataset’, ArXiv Prepr. ArXiv200607397, 2020. \n[27] P. Korshunov and S. Marcel, ‘Deepfakes: a new threat \nto face recognition? assessment and detection’, ArXiv \nPrepr. ArXiv181208685, 2018. \n[28] A. Rössler, D. Cozzolino, L. Verdoliva, C. Riess, J. \nThies, and M. Nießner, ‘FaceForensics++: Learning to \nDetect Manipulated Facial Images’, in International \nConference on Computer Vision (ICCV), 2019. \n \nKURNIAWAN NUR RAMADHANI \nis currently studying in Ph.D. degree in \nSchool of Electrical Engineering and \nInformatics, Bandung Institute of \nTechnology. His research interests are \ncomputer vision and machine learning. \n \n \nRINALDI MUNIR received the \nbachelor’s degree in informatics \nengineering and the M.Sc. degree in \ndigital image compression from the \nBandung Institute of Technology \n(ITB), Bandung, Indonesia, in  1992 \nand 1999, respectively, and the Ph.D. \ndegree in image watermarking from \nthe School of Electrical Engineering \nand Informatics, ITB, in 2010. In 1993, he started his \nacademic career as a Lecturer with the Department of \nInformatics, ITB. He is currently an Associate Professor with \nthe School of Electrical Engineering and Informatics, ITB, \nand the Informatics Research Group. His research interests \ninclude cryptography and steganography -related topics, \ndigital image processing, fuzzy logic, and numerical \ncomputation. \n \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3352890\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \nVOLUME XX, 2017 9 \nNUGRAHA PRIYA UTAMA \nreceived the bachelor’s degree in \ninformatics from the Bandung \nInstitute of Technology, \nIndonesia, in 2002, and the \nmaster’s and Ph.D. degrees from \nthe Tokyo Insti tute of \nTechnology, in 2006 and 2009, \nrespectively. His re search \ninterests include computer \nvision and neuroscience. \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3352890\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8699445724487305
    },
    {
      "name": "Landmark",
      "score": 0.7121264338493347
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6958523392677307
    },
    {
      "name": "Block (permutation group theory)",
      "score": 0.5518878698348999
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5191097259521484
    },
    {
      "name": "Computer vision",
      "score": 0.5020260810852051
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.48175883293151855
    },
    {
      "name": "Convolution (computer science)",
      "score": 0.4766867160797119
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.4216948449611664
    },
    {
      "name": "Artificial neural network",
      "score": 0.16258302330970764
    },
    {
      "name": "Mathematics",
      "score": 0.07694131135940552
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ]
}