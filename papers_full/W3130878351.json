{
  "title": "Centroid Transformers: Learning to Abstract with Attention",
  "url": "https://openalex.org/W3130878351",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2747409446",
      "name": "Wu Lemeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2353005324",
      "name": "Liu Xingchao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1960924044",
      "name": "Liu Qiang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2966661",
    "https://openalex.org/W2962712961",
    "https://openalex.org/W3107118900",
    "https://openalex.org/W2930556772",
    "https://openalex.org/W2123045220",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W3012494314",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2785994986",
    "https://openalex.org/W2533545350",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2903024257",
    "https://openalex.org/W3120633509",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W1920022804",
    "https://openalex.org/W2963877826",
    "https://openalex.org/W2601564443",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W2963929190",
    "https://openalex.org/W2948798935",
    "https://openalex.org/W2751777443",
    "https://openalex.org/W2962852342",
    "https://openalex.org/W2190691619",
    "https://openalex.org/W3047517563",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W2964228567",
    "https://openalex.org/W2943845043",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2624503621",
    "https://openalex.org/W3034664537",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W1843891098",
    "https://openalex.org/W2953273646",
    "https://openalex.org/W3034742519",
    "https://openalex.org/W2962851485",
    "https://openalex.org/W2784962210",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W2997517014",
    "https://openalex.org/W3034573343",
    "https://openalex.org/W2979750740",
    "https://openalex.org/W2963840672",
    "https://openalex.org/W2560609797",
    "https://openalex.org/W3096609285"
  ],
  "abstract": "Self-attention, as the key block of transformers, is a powerful mechanism for extracting features from the inputs. In essence, what self-attention does is to infer the pairwise relations between the elements of the inputs, and modify the inputs by propagating information between input pairs. As a result, it maps inputs to N outputs and casts a quadratic $O(N^2)$ memory and time complexity. We propose centroid attention, a generalization of self-attention that maps N inputs to M outputs $(M\\leq N)$, such that the key information in the inputs are summarized in the smaller number of outputs (called centroids). We design centroid attention by amortizing the gradient descent update rule of a clustering objective function on the inputs, which reveals an underlying connection between attention and clustering. By compressing the inputs to the centroids, we extract the key information useful for prediction and also reduce the computation of the attention module and the subsequent layers. We apply our method to various applications, including abstractive text summarization, 3D vision, and image processing. Empirical results demonstrate the effectiveness of our method over the standard transformers.",
  "full_text": "Centroid Transformers: Learning to Abstract with Attention\nLemeng Wu *\nUT Austin\nlmwu@cs.utexas.edu\nXingchao Liu *\nUT Austin\nxcliu@utexas.edu\nQiang Liu\nUT Austin\nlqiang@cs.utexas.edu\nAbstract\nSelf-attention, as the key block of transformers, is a pow-\nerful mechanism for extracting features from the inputs.\nIn essence, what self-attention does is to infer the pair-\nwise relations between the elements of the inputs, and\nmodify the inputs by propagating information between\nthe input pairs. As a result, it maps N inputs to N outputs\nand casts a quadratic O(N2) memory and time complex-\nity. We propose centroid attention, a generalization of\nself-attention that maps N inputs to M outputs (M ‚â§N),\nsuch that the key information in the inputs is summarized\nin the smaller number of outputs (called centroids). We\ndesign centroid attention by amortizing the gradient de-\nscent update rule of a clustering objective function on the\ninputs, which reveals a underlying connection between\nattention and clustering. By compressing the inputs to the\ncentroids, we extract the key information useful for pre-\ndiction and also reduce the computation of the attention\nmodule and the subsequent layers. We apply our method\nto various applications, including abstractive text sum-\nmarization, 3D vision, and image processing. Empirical\nresults demonstrate the effectiveness of our method over\nthe standard transformers.\n1 I NTRODUCTION\nRecently, transformer (Vaswani et al., 2017) has emerged\nto be one of the most important neural architectures and\nhas achieved remarkable successes on various tasks such\nas language modeling (Irie et al., 2019; Jiao et al., 2020),\nmachine translation (Vaswani et al., 2017; Zhang et al.,\n2018; Wang et al., 2019a), computer vision (Carion et al.,\n2020; Dosovitskiy et al., 2020), and many others.\nWhat makes transformers unique is the extensive usage\nof the self-attention mechanism (Vaswani et al., 2017). A\nself-attention block is placed in each stage of the trans-\nformer to gather information globally from the input se-\nquence. A self-attention module takes in N inputs, and\nreturns N outputs of the same size. For each element in\nthe input, it assigns an attention weight to every other\nelement in the input to Ô¨Ånd out who it should pay more\nattention to, and perform a weighted sum to aggregate the\ninformation from the relevant inputs.\n*Equal contribution\nIntuitively, the self-attention modules can be viewed as\nconducting interactive reasoning, inferring the pairwise\ninteracting relations between the elements of inputs and\npropagating information between pairs of elements. Nat-\nurally, a key drawback of the pairwise interaction is that\nit casts an O(N2) memory and time complexity, where\nN is the number of input elements, making it a major\ncomputational bottleneck in transformers. This neces-\nsitates an emerging line of research on approximating\nself-attention modules to gain higher computational efÔ¨Å-\nciency (Kitaev et al., 2019; Katharopoulos et al., 2020;\nWang et al., 2020).\nIn this work, we develop a variant of attention module\nfor conducting summative reasoning rather than interac-\ntive reasoning. Our goal is to take N input elements and\nreturn a smaller number M of outputs (M ‚â§N), such\nthat the key information in the N inputs are summarized\nin the M outputs. If N = M, our new module reduces\nto standard self-attention (except an extra skip connec-\ntion link). However, by setting M < N, we creates an\ninformation bottleneck and enforce the network to Ô¨Ålter\nout the useless information (i.e., dimension reduction),\nand also improve the computational cost from O(N2) to\nO(NM). In addition, once the number of inputs is re-\nduced, the complexity of the subsequent modules/layers\nis also reduced accordingly (e.g., applying self-attention\non the output will yield a O(M2) complexity).\nIntuitively, we hope that theM output elements are rep-\nresentative of the N inputs. This can be viewed as a\nclustering process in which we Ô¨Ånd M ‚Äúcentroids‚Äù for the\ninputs. Our new module, which we call centroid attention\nlayer, is obtained by ‚Äúamortizing‚Äù the gradient descent\nupdate rule on a clustering objective function. It exploits\na novel connection between self-attention and clustering\nalgorithms: we write down a clustering objective function\non the input data under a trainable similarity metric, and\nderive its gradient descent update rule; we then observe\nthat the gradient descent update yields a generalization\nof self-attention layer and use it to motivate the design of\nour new module.\nUsing our new modules, we build centroid transformers,\nin which we insert our centroid attention modules between\ntypical self-attention modules. We apply centroid trans-\nformers on several challenging scenarios, ranging from\nnatural language processing to computer vision. On ab-\n1\narXiv:2102.08606v2  [cs.LG]  8 Mar 2021\nSelf-AttentionùëÇ(ùëÅ!)\nInputSequence OutputSequence\nùëÅ√óùëë ùëÅ√óùëë\nSelf-AttentionùëÇ(ùëÅ!)\nInputSequence\nùëÅ√óùëë\nCentroidAttentionùëÇ(ùëÅùëÄ)\nCentroids\nùëÄ√óùëë\nSelf-AttentionùëÇ(ùëÄ!)\nOutputSequence\nùëÄ√óùëë\n(a) Transformer\n(b) Centroid transformer\nSelf-AttentionùëÇ(ùëÅ!) Self-AttentionùëÇ(ùëÅ!)MLPMLPMLP\nMLP MLPMLP\nFigure 1: The vanilla transformer (a) which maps N\ninputs to N outputs; and our centroid transformer (b)\nwhich summarizes N inputs into M ‚Äúcentroid‚Äù outputs\n(M ‚â§N) to save computational cost and Ô¨Ålter out useless\ninformation simultaneously.\nstractive text summarization, centroid transformer beats\nthe vanilla transformer with about only 50% computa-\ntional cost. On 3D point cloud classiÔ¨Åcation and image\nclassiÔ¨Åcation tasks, centroid transformer achieves substan-\ntially higher accuracy as well as computational efÔ¨Åciency\ncompared with the state-of-the-art transformer networks.\nWe also use centroid attention to replace the dynamic\nrouting module in the 3D Capsule Network (Zhao et al.,\n2019) for point cloud reconstruction, which we Ô¨Ånd yields\nlower construction error, reduced storage consumption,\nand more semantically meaningful latent representation.\n2 C ENTROID TRANSFORMER\nWe Ô¨Årst introduce the standard self-attention mechanism\nused in vanilla transformers in Section 2.1, and then derive\nthe centroid attention mechanism by drawing inspiration\nfrom the gradient descent update of a clustering objective\nfunction and discuss the related centroid transformer in\nSection 2.2.\n2.1 S ELF -ATTENTION MECHANISM\nLet {xi}N\ni=1 ‚ààRN√ód be a set of input vectors, where\nwe may view each vector xi as a data point or ‚Äúparticle‚Äù.\nEach xi can be a word embedding vector in natural lan-\nguage processing, an image patch in computer vision, or\na point in 3D point cloud. A self-attention module can be\nviewed as updating {xi}in parallel via\nx‚Ä≤\ni ‚Üê xi + œµ\nN‚àë\nj=1\nL‚àë\n‚Ñì=1\nsim‚Ñì (xi,xj) √óv‚Ñì(xj), ‚àÄi‚àà[N]\n(1)\nHere, each sim‚Ñì(¬∑,¬∑) is a similarity function and v‚Ñì(xi) ‚àà\nRd is a value function; each ‚Ñìis considered to be a head\nin the attention and process a speciÔ¨Åc aspect of the inputs;\nœµis a positive constant.\nIntuitively, the self-attention module evaluates the similar-\nity (or attention score) between pairs of input vectors, and\nupdates each vector with the sum of the inputs weighted\nby the similarity scores. In practice, the similarity func-\ntion is often deÔ¨Åned as\nsim‚Ñì (xi,xj) = exp(Q‚ä§\n‚Ñì (xi)K‚Ñì(xj))‚àëN\nk=1 exp(Q‚ä§\n‚Ñì (xi)K‚Ñì(xk))\n, (2)\nwhere Q‚Ñì(x) and K‚Ñì(x) are the ‚Äúquery‚Äù and ‚Äúkey‚Äù func-\ntions of the ‚Ñì-th head. Eq. (1) and (2) illustrate how the\nfeatures are processed by one head. A complete trans-\nformer encoder with T layers is the composition of self-\nattention layers and multi-layer perceptrons (MLPs); see\nFigure 1. Obviously, the time and memory complexities\nof self-attention are quadratic on N, i.e., O(N2), which\nform the key computational bottleneck when applying\nself-attention on long sequences and large batches of data.\n2.2 C ENTROID ATTENTION\nSelf-attention updates the inputs {xi}based on their in-\nteracting relations, obtaining an output of the same size.\nIn this work, we propose a mechanism that maps the N\ninputs {xi}n\ni=1 to M output vectors {ui}M\ni=1,M ‚â§N,\nsuch that each ui ‚ààRd can be viewed as a centroid of\nthe inputs {xi}N\ni=1. Namely, we hope the module to be\nable to effectively perform a clustering-like operation on\nthe inputs, where {ui}M\ni=1 is a compression of {xi}M\ni=1\nwhile inheriting the key information.\nLet {œÜ‚Ñì(xi,uj): ‚àÄ‚Ñì ‚àà [L]}be a set of measures of\nsimilarity between centroid uj and input xi. Ideally, we\nmay want to construct the centroids by optimizing the\nfollowing general ‚Äúsoft K-means‚Äù objective function for\nclustering,\n{u‚àó\nj }= arg max\n{uj}\nL‚àë\n‚Ñì=1\nN‚àë\ni=1\n1\nŒ±log\nÔ£´\nÔ£≠\nM‚àë\nj=1\nexp (Œ±œÜ‚Ñì(xi,uj))\nÔ£∂\nÔ£∏.\n(3)\nHere, Œ± > 0 is a positive coefÔ¨Åcient. If Œ± ‚Üí\n+‚àû and L = 1 , then the objective reduces to‚àë\ni maxj œÜ1(xi,uj), which coincides with the typical\nk-means objective function when ‚àíœÜ1(¬∑,¬∑) is a distance\nmeasure. In general, the objective (3) obtains centroids to\nrepresent the inputs based on multiple similarity functions\n{œÜ‚Ñì}.\nBy solving the optimization with gradient descent, we can\nunroll (3) into an iterative update of form:\nInitialization: {u0\nj}M\nj=1 = I({xi}N\ni=1)\nFor t= 1,2,...,T ,\nut+1\nj ‚Üêut\nj + œµ\nL‚àë\n‚Ñì=1\nN‚àë\ni=1\nsim‚Ñì(xi,ut\nj)V‚Ñì(xi, ut\nj),\n(4)\nwhere I(¬∑) denotes a mechanism for initializing the cen-\ntroids and each of the followingT steps conducts gradient\n2\ndescent of (3), with\nsim‚Ñì(xi,uj) = exp(Œ±œÜ‚Ñì(xi,uj))‚àëM\nk=1 exp(Œ±œÜ‚Ñì(xi,uk))\n,\nV‚Ñì(xi, uj) =‚àáuj œÜ‚Ñì(xi,uj).\nClearly, the gradient update above can be interpreted as\na multi-head attention mechanism, with sim‚Ñì(¬∑,¬∑) and\nV‚Ñì(¬∑,¬∑) being the similarity function and the value func-\ntion of the ‚Ñì-th head. The initialization I(¬∑) together with\nthe T steps of attention like updates in (4) above form a\ncentroid attention module. See Figure 2 for an illustration.\nIn practice, we Ô¨Ånd it works well to set œµ = 1/T by\ndefault. Moreover, in settings when computational efÔ¨Å-\nciency is important, we setT = 1for a good performance-\nefÔ¨Åciency trade off. The initialization step can vary in\ndifferent tasks: we can, for example, draw {u0\nj}M\nj=1 from\n{xi}N\ni=1 by random sampling without replacement or far-\nthest point sampling; we can also directly deÔ¨Åne I(¬∑) to\nbe a trainable fully connected or convolution layer. See\nmore discussion in the experiment section.\nAlthough both sim‚Ñì(¬∑,¬∑) and V‚Ñì(¬∑,¬∑) are determined by œÜ‚Ñì\nfollowing the derivation above. In practice, we can deÔ¨Åne\nthem separately in more Ô¨Çexible forms based on practical\nneeds. For example, we may deÔ¨Åne sim‚Ñì(¬∑,¬∑) by set-\nting œÜ(xi,uj) =Q‚ä§\n‚Ñì (uj)K(xi) as typical self-attention,\nwhile setting V‚Ñì(xi, uj) with a separate trainable value\nfunction.\nOur module includes self-attention as a special case when\nwe i) set M = N, ii) initialize {ui}to be the same as xi,\nand iii) iterate for one step (T = 1). Therefore, our deriva-\ntion also reveals a close connection between gradient-\nbased clustering and self attention. Note that Ramsauer\net al. (2020) discusses a connection between HopÔ¨Åeld up-\ndate rule and self-attention, which is related but different\nfrom our perspective.\nKNN Approximation The computational cost of the\nattention is O(NM), which can still be expensive if N\nand M are large, To further save the computational cost,\nwe apply a K-nearest neighbor (KNN) approximation to\n(4), yielding\nut+1\nj ‚Üêut\nj + œµ\nL‚àë\n‚Ñì=1\n‚àë\ni‚ààNj,k\nsim‚Ñì(xi,ut\nj)V‚Ñì(xi, ut\nj),\nwhere Nj,k denotes the K-nearest neighbor of uj in-\nside {xi}N\ni=1, that is, Nj,k = {i(1),...,i (k)}, where\n{i(1),...,i (n)}is the ascending reordering of {1,...,n }\naccording to some distance metric d(xi,uj). In practice,\nwe deÔ¨Åne the distance by d(xi,uj) =||xi ‚àíuj||2.\nAs shown in our experiments, the KNN approximation is\nparticularly useful for the point cloud classiÔ¨Åcation task,\nin which the length of the inputs elements is N = 1024,\nwhich is beyond the capacity of our GPU memory even\nfor a single data point (i.e., a batch size of 1).\n(a) Self-Attention(b) Centroid Attentionùë•!ùë•\"\nùë•#$ ùë•!$ ùë•\"$ ùë•%$\nùë•# ùë•%\nùë¢#\nùë•!ùë•\"ùë•# ùë•%\nùë¢!ùëátimes\nFigure 2: (a) The self-attention module, which modiÔ¨Åes\nthe input sequence {xi}into {x‚Ä≤\ni}by updating them with\npairwise interactions (see Eq (1)). (b) The centroid atten-\ntion module, which transforms the input sequence {xi}\ninto a set of centroids {ui}by Ô¨Årst initializing the cen-\ntroids and then updating them by interacting with the\nelements in the inputs (see Eq (4)).\nCentroid Transformer As shown in Fig. 1, we con-\nstruct a centroid transformer architecture by alternating\nbetween typical self-attention blocks and centroid atten-\ntion blocks. This allows us to gradually abstract the input\nsequence to increasingly short representations, which Ô¨Ål-\nters out useless information and simultaneously saves the\ncomputation cost. As vanilla transformer, we insert fully\nconnected MLP layers with residual links between the\nattention modules.\n3 E XPERIMENTS\nWe apply our centroid attention block on different kind of\ntasks with transformer or capsule structures to show our\npowerful abstract ability as well as the computational efÔ¨Å-\nciency. We show that our centroid transformer structure\nhas a strong performance in various of tasks including\ntext summarization, point cloud classiÔ¨Åcation, point cloud\nreconstruction and image classiÔ¨Åcation. On all the tasks,\nwe outperform baseline transformer with lower computa-\ntional and storage cost.\n3.1 A BSTRACTIVE TEXT SUMMARIZATION\nWe test centroid transformer on abstractive text summa-\nrization, a classic task in natural language processing. The\ntask is to provide a short summary text (in several words)\nfor a paragraph or a long sentence.\nData We use the annotated Gigaword corpus (Rush et al.,\n2015) as our benchmark to compare different methods.\nThe dataset is pre-processed with the tools provided by\nthe authors. The corpus contains about 3.8 millions of\ntraining examples. The script also has 400,000 extra\nexamples for validation and test. We randomly sample\n2,000 examples for validation and test respectively, as\nin Nallapati et al. (2016). All the models are validated\nand tested with the same validation and test set. The\nperformance of the generated summaries is measured\nby Recall-Oriented Understudy for Gisting Evaluation\n(ROUGE) (Lin, 2004). ROUGE-1/ ROUGE-2/ROUGE-L\n3\nModels MACs(M) MBS/GPU ROUGE-1 ROUGE-2 ROUGE-L\nTransformer 523.2 192 32.987 15.286 30.771\nOurs-Random 262.9 230 30.310 12.752 27.823\nOurs-MP 262.9 230 34.651 16.468 32.415\nTable 1: Results on Gigaword text summarization task (MBS=Maximal Batch Size, MP = Mean-Pooling). The MACs\n(Multiply-add ACcumulation) is only computed for the encoder, assuming the length of sequence is 45 (the maximal\nlength of sequence in the dataset). Though centroid transformer with random initialization (Ours-Random) performs\nworse than the baseline, centroid transformer with mean-pooling being the initialization method (Ours-MP) yields\nthe best ROUGE score (See (Lin, 2004) for its deÔ¨Ånition) with 50% computational cost compared to the original\ntransformer.\nevaluates the quality of unigrams/bigrams/whole sentence\nin the generated summary.\nModel ConÔ¨Åguration We construct our centroid trans-\nformer by replacing the second self-attention module in\nthe baseline transformer encoder with our centroid atten-\ntion module.\nM is set to N/3 so that our centroid attention module\ncompresses N input points into N/3 centroids. The rest\nof the parts are kept unchanged. When decoding, the\ncross-attention is applied between the generated sequence\nand the centroids. We test two initialization schemes for\nthe centroid attention, random sampling and mean pool-\ning. Random sampling means we randomly sample N/3\nelements from the original sequence as the initial cen-\ntroids, while mean pooling refers to apply mean pooling\non every three elements.\nOur baseline transformer follows the original encoder-\ndecoder structure in Vaswani et al. (2017), with 4 layers\nin both encoder and decoder. We use a word embedding\nwith 512 dimensions. All the models are trained for 30\nepochs with Adam optimizer and the same learning rate.\nThe number of warm-up steps is set to 4,000. At decode\ntime, beam search with size 10 is exploited to generate\nthe summary. Both self-attention and cross-attention is\nenabled in the decoder.\nFigure 3: The architecture of the centroid transformer\nwe used for point cloud classiÔ¨Åcation. ‚ÄúCA‚Äù represents\ncentroid attention and ‚ÄúSA‚Äù vanilla self-attention.\nResults The results of the experiments is shown in Ta-\nble. 1. Centroid transformer with mean-pooling initializa-\ntion yields the highest score in all three metrics. Moreover,\nour centroid transformer reduces 50% MACs against the\noriginal transformer on the encoder side. For the same\nTitan XP GPU, centroid transformer takes less storage\nspace, allowing 38 more samples in a single batch. We\nalso Ô¨Ånd the initialization scheme to be important. Ran-\ndom sampling scheme results in a centroid transformer\nthat is worse than the baseline.\n3.2 P OINT CLOUD CLASSIFICATION\nWe apply our centroid transformer on point cloud classiÔ¨Å-\ncation. Point cloud data usually has over 1,000 points as\ninput, which is a long sequence and hard to directly train\nwith an original transformer. By applying our centroid\ntransformer, we can gradually aggregate the thousands\npoints into several abstract clusters, which brings power-\nful feature extraction ability and saves computational cost.\nWe compare our method with several baselines including\nthe state-of-the-art attention based method SepNet-W15\n(Ran & Lu, 2020) on ModelNet40 dataset (Wu et al.,\n2015). We outperform the existing methods on classiÔ¨Åca-\ntion accuracy and consume much less resource comparing\nwith the attention based method SepNet-W15.\nData We use ModelNet40 as our benchmark, which\nhas 40 classes with 9843 training shapes and 2468 test\nshapes. Consider that many of the ModelNet40 classi-\nÔ¨Åcation works use their speciÔ¨Åcally processed data, for\na fair comparison, we use the same data as (Qi et al.,\n2017a), which is an uniformly sampled point cloud data\nfrom ModelNet40 raw data without any other speciÔ¨Åc\nnormalization and preprocess.\nModel ConÔ¨Åguration We design a transformer architec-\nture with 4 attention blocks and a 3-layer MLP classiÔ¨Åca-\ntion head as Figure 3 shows. The Ô¨Årst centroid attention\nblock cluster Npoints to N/2 and the second one abstract\nN/2 to N/8. We use Farthest Point Sampling (FPS) as\nour initialization function. The dimension of the 4 atten-\ntion locks is 64-128-128-512. We set K = 40in all the\nKNN masks and the KNN is calculated in feature space\nin the centroid attention blocks. After we extract features,\nwe fuse the features in each layer by upsampling their\nnumber of points to N using nearest-neighbor sampling .\nThen we concatenate all the layers‚Äô feature together and\npass through a fully connected layer. The output feature\nwill pass through a max pooling and a average pooling\nlayer separately. At last, we concatenate them together as\n4\nthe Ô¨Ånal feature of the input point cloud. The feature will\npass a 3-layer MLP to get the Ô¨Ånal classiÔ¨Åcation score. All\nthe activation functions in the network are LeakyReLU\nwith a negative slope of 0.2. We train the network with\nAdam optimizer start with 1e‚àí3 learning rate and decay\nby 0.7 every 20 epochs for 250 epochs in total.\nModels Acc MACs(G) Params(M)\nPointNet (Qi et al., 2017a) 89.2 0.3 0.8\nPointNet++ (Qi et al., 2017b)91.9 7.9 12.1\nDGCNN (Wang et al., 2019b)92.8 2.4 2.0\nSepNet-W15 (Ran & Lu, 2020)93.1 22.4 4.8\nOurs 93.2 5.4 4.1\nTable 2: Results on ModelNet40 of our method and var-\nious of baselines. Here MACs denotes multiply-add cu-\nmulation and Params means the number of parameters in\nthe model.\nMethod Acc Data/sec\nOurs (T=1) 93.1 60\nOurs (T=2) 93.2 55\nOurs (T=3) 93.2 52\nRandom Sampling 91.7 312\nFarthest Point Sampling 92.3 66\nK-means 92.3 54\nTable 3: Results on ModelNet40 of centroid attention\nwhen using different numbers of iterationsT and different\ninitialization strategies (see Eq (4)).\nResult From Table 2, we can see that our model out-\nperforms all the baseline on classiÔ¨Åcation accuracy on\nModelNet40. In addition, when comparing with SepNet-\nW15, our model can achieve a higher accuracy with only\n1/4 MACs and fewer parameters.\nAblation Study In Table 3, we show the result of our\nmethod using different T in centroid attention blocks and\ncompare them with other downsampling strategies. For\nthe different T, the performance do not have a big differ-\nence. However, larger T means more computation oper-\nations, in practice, we choose T = 1for a performance-\nefÔ¨Åciency balance. For the downsampling strategies, we\ncompare our model with Random Sampling, Farthest\nPoint Sampling and K-means. We apply K-means for\n3 iterations to guarantee the computation complexity is\ncomparable with our centroid attention. Comparing with\nfarthest point sampling strategy, our speed only slow a\nlittle bit while getting a 0.8 improvement in the classiÔ¨Åca-\ntion accuracy. Comparing with the random sampling, we\nhave a 1.5 accuracy improvement. In addition, we are 0.9\nbetter than K-means.\nVisualization We visualize our second centroid attention\nblocks‚Äô feature cluster in Figure 4. We plot the sampled\npoint in white star and its K-Nearest-Neighbour (KNNs)\nin red point. We set K = 40 and use L2 distance in\nKNNs. From Figure 4 we can see that rather than gather-\ning around the sampled point in the 3D space, the KNNs\nin our features space tend to distribute within the same se-\nmantic part of the query part, which indicates our feature\ncaptures high-level concept of the shapes.\nFigure 4: Learning classiÔ¨Åcation on ModelNet40 with\ncentroid transformer. We visualize the K-nearest-\nneighbours (KNNs) points of some sampled points (white\nstars). For the KNNs‚Äô distance, for ours, we use the L2\ndistance in feature space learned in the second centroid at-\ntention blocks and for 3D space, we use the 3D euclidean\ndistance. Here, K = 40, The red points indicates the\nKNNs points of the sampled white point.\n3.3 P OINT CLOUD RECONSTRUCTION\nWe further test our centroid attention block on point cloud\nreconstruction task and make a visualization plot to illus-\ntrate our layer‚Äôs efÔ¨Åciency and abstract ability. We use\nShapeNet part and ShapeNet Core13 (Chang et al., 2015)\nas our dataset and use 3D Capsule Network (Zhao et al.,\n2019) as our backbone. We replace the Dynamic Routing\nmodule in the original capsule network with our centroid\nattention block to abstract the information from the prime\ncapsules.\nExperiment Setup We follow the 3D Capsule Network\n(Zhao et al., 2019) settinga and construct an autoencoder\nto reconstruct the 3D point cloud with 2048 points. We\nset up two model scales with 12 and 64 latent caspules. In\nour centroid attention block, we treat the prime capsules\nas our input sequence and latent capsules as the abstract\nsequence. We use a linear projection to learn the initial-\nization of the latent capsules. We set T = 3 to match\nnumber of iterations in Dynamic Routing module.\nFor the 3D Capsule Network, we setup two network sizes,\nthe small one contains 512 prime capsules with 16 dimen-\nsions and 12 latent capsules with 128 dimensions. The\nlarger one contains 1024 prime capsules with 16 dimen-\nsions and 64 latent capsules with 64 dimensions. The 64\nlatent capsuls setting keeps the same with the original\nsetting in 3D capsule network. We train small setting for\n200 epochs using ShapeNet Part dataset and base setting\nfor 300 epochs using ShapeNet Core13 dataset. The other\nhyperparameter keeps the same as Zhao et al. (2019).\n5\n(a) Capsules learned by our method (b) Capsules learned by dynamic routing\nFigure 5: Point cloud reconstruction using 3D Capsule Network (Zhao et al., 2019) as backbone, with 12 latent\ncapsules. We decode the 12 latent capsules one by one and highlight the corresponding decoded points with bright\ncolors while keep the other parts gray. (a) shows the latent capsules learned by replacing the original dynamic routing\nmodule with centroid attention blocks, which can capture semantically meaningful parts of the plane, and are grouped\ninto three clusters that represents plane body, plane front and back wings, and middle engine, respectively. (b) shows the\ncapsules learned by dynamic routing in (Zhao et al., 2019), which distribute randomly and yield no semantic meanings.\n(a) Capsules of our method on a table (b) Capsules of our method on a car\nFigure 6: More visualization of capsules learned by our method. In both (a) and (b), the capsules map to semantically\nmeaningful parties of the inputs.\nResult From Table 4 we can see that in the 12 latent\ncapsules setting, our model greatly outperforms the Dy-\nnamic Routing module to abstract the latent capsule and\nleads a Chamfer Distance 1.6 √ó10‚àí3 comparing with\n2.7 √ó10‚àí3 in baseline. When comparing with Dynamic\nRouting in 64 latent capsules setting, which is same as the\noringal paper, we still has 10‚àí3 better Chamfer Distance\nscore comparing with baseline. The results indicate the\nDynamic Routing module fails to abstract many prime\ncapsules into a small amount of latent capsules. On the\ncontrast, our centorid transformer can aggregate informa-\ntion no matter the output length is short or long.\nFurther, our centroid attention block has a much smaller\nparameter size as well as a larger max batch size per GPU.\nTjos means we can process more data in the same time\nand lead a much faster training speed comparing with the\ncapsule network using Dynamic Routing. This roughly\nleads a 3√ótraining speed boosting when training with\nsame amount of GPUs.\nVisualization We further visualize the decoded points\nfrom each latent capsule. To clearly show the semantic\nmeaning, we use 12 latent capsules to plot the reconstruc-\ntion visualization, in which each capsules can decode into\n170 points. Each time, we highlight the decoded points\nfrom speciÔ¨Åc capsules in bright colors and keep the rest\nof the reconstruction points in gray. In Figure 5 (a), we\nshow our learned latent capsules can capture semantic\nmeanings and distribute on the speciÔ¨Åc part of the plane\n6\n# Cap Method CD(√ó10‚àí3) #Param(M) MBS/GPU\n12 DR 2.7 18.4 22\nOurs 1.6 0.6 58\n64 DR 1.5 19.8 12\nOurs 1.4 5.3 36\nTable 4: Comparison between the performance of dy-\nnamic routing and our centroid attention method under\ndifferent latent capsules number settings (MBS/GPU =\nMaximum Batch Size per GPU, CD = Chamfer Distance).\nHere the Chamfer Distance is a metric for evaluating the\nquality of reconstruction.\nshape. If we group several capsules together, we can fur-\nther get the whole semantic parts. Figure 5 (b) shows\nthe decoded points learned by Dynamic Routing. We can\nÔ¨Ånd that the reconstruction shape is in a low quality and\nthe highlight points are in a random distribution across\nthe whole shape. That means Dynamic Routing failed to\nlearn a meaningful latent capsules under this setting. We\nplot two more visualization using our centroid attention\nblocks to show the result of different shapes with differ-\nent number of semantic parts. Figure 6 (a) clearly shows\nthat the table are decomposed into the surface and legs.\nFurthermore, Figure 6 (b) decomposes the car into body,\nsurrounding, wheel, roof and door parts, these illustrate\nour centroid attention‚Äôs strong ability in clustering the\nsemantic information.\n3.4 V ISION TRANSFORMER\nWe apply our centroid transformer structure on ImageNet\n(Deng et al., 2009) classiÔ¨Åcation task using vision trans-\nformer to show our capacities among various transformer\nwith large-scale data. We choose DeiT (Touvron et al.,\n2020) as our baseline, which is the state-of-the-art vision\nclassiÔ¨Åcation transformer structure on ImageNet dataset.\nBy applying centroid attention block in one speciÔ¨Åc layer,\nwe build up our centroid transformer as Figure 7 shows.\nOur transformer abstract the image patches sequence into\na shorter one in the middle, which reduces the computa-\ntional cost comparing with the original DeiT. Further, we\nallow some overlaps in the Ô¨Årst convolution layer to cre-\nate a longer initial token sequence with richer represent\nability.\nWe compare our centroid transformer with DeiT-tiny and\nDeiT-small, which are two vision transformers with dif-\nferent model scales. We also apply two efÔ¨Åcient trans-\nformers, Set Transformer (Lee et al., 2019) and Linformer\n(Wang et al., 2020) on DeiT as baseline to compare the\nperformance under an energy efÔ¨Åcient setting.\nExperiment Setup In the overlap setting, we set the\nÔ¨Årst convolution layer with kernel size 16, stride 14, and\npadding 1. This can create a patch sequence with 256\ntokens. In the centroid attention block, we Ô¨Årst initialize\nour shorter sequence by reshaping the N length patch to-\nkens into a\n‚àö\nN√ó\n‚àö\nN order. We then apply a depth-wise\nModels #layers #tokens embedding\ndimension centroid @\nDeiT-tiny\n12\n196\n192\n-\nOurs-1 196 6\nOurs-2 256 5\nOurs-3 256 7\nDeiT-small\n12\n196\n384\n-\nOurs-4 196 6\nOurs-5 256 5\nOurs-6 256 7\nTable 5: Architecture design of different models for image\nclassiÔ¨Åcation tasks on ImageNet, centroid @ means re-\nplacing the self-attention block with the centroid attention\nblock at speciÔ¨Åc layer.\nFigure 7: Comparing DeiT and centroid transformer for\nimage inputs. Upper panel: DeiT partitions the input im-\nage into non-lapping patches and keeps the same number\nof patches throughout the layers, which may lose infor-\nmation of the image because the patches do not overlap.\nIn comparison, thanks to the ability of down-sampling,\nthe centroid transformer with comparable computational\ncost can take a larger number of overlapping patches in\nthe early stage and hence capture more information from\nthe input images.\nconvolution layer with kernel size 3, stride 2, padding 1\nto downsample the input tokens into\n‚àö\nN/2 √ó\n‚àö\nN/2 and\nreshape it back to N/4 tokens as the initialization. We\nset T = 1for a better performance-efÔ¨Åciency trade off.\nThe rest of the training setting are the same as DeiT. We\nsetup different model scales. The overall architectures\ndesign is listed in Table 5. For fast transformer baselines,\nwe set Linformer‚Äôs project dimension k = 128and Set\nTransformer‚Äôs induce point set lengthm= 128.\nResult Our results are shown in Tabel 6. In ‚ÄúOurs-1‚Äù and\n‚ÄúOurs-4‚Äù setting, by replacing one self-attention blocks\ninto centroid attention block, our model has only 64%\nMACs comparing the original DeiT-tiny and DeiT-small\nwhile only drop the Top-1 Accuracy by 0.4 and 0.3. Fur-\nther, when we allow a longer initial sequence by overlap-\nping the image patchs and adjust the centroid attention\nblocks to the Ô¨Åfth layer, the centorid block‚ÄúOurs-2‚Äù and\n‚ÄúOurs-5‚Äù achieves a same and 0.2 higher top-1 with 81%\nMACs. When we further increase the MACs by adjust-\ning the centroid attention block‚Äôs layer, we can get larger\n7\nmodels ‚ÄúOurs-3‚Äù and ‚ÄúOurs-6‚Äù with the close MACs com-\nparing with DeiT-tiny and DeiT-small baseline and gains\n1.2 and 1.0 performance improvement.\nWhatsmore, when we compare centroid transformer with\nother fast transformers, we outperform set transformer\nby over 5.0 and linformer by over 2.0 Top-1 accuracy\nwith smaller MACs. Our good performance comparing\nwith these fast transformers further shows our structures‚Äô\npowerful ability and efÔ¨Åciency to handle even this kind of\nlarge-scale image classiÔ¨Åcation task.\nMethod Top-1 Acc MACs(G) Params(M)\nSet Transformer 66.1 1.1 5.9\nLinformer 69.9 1.2 6.3\nDeiT-tiny 72.2 1.3 5.7\nOurs-1 71.8 0.8 5.7\nOurs-2 72.2 1.0 5.7\nOurs-3 73.4 1.3 5.8\nSet Transformer 74.6 3.8 22.4\nLinformer 77.6 4.4 22.6\nDeiT-small 79.9 4.7 22.1\nOurs-4 79.6 2.9 22.1\nOurs-5 80.1 3.6 22.1\nOurs-6 80.9 4.7 22.3\nTable 6: Vision Transformer result compared with DeiT.\nOurs-x indicates different MACs setting of our model.\nMACs indicates multiply-add cumulation and Params\nmeans the number of parameters in the model.\n4 R ELATED WORKS\nFast Transformers A line of recent works have been\ndeveloped to approximate self-attention layers to improve\nover the O(N2) complexity. These methods keep the ba-\nsic design of self-attention, mapping N inputs to N out-\nputs, and is hence different in purpose with our method,\nwhich compresses Ninputs to a smaller numberMof out-\nputs. Among these works, Reformer (Kitaev et al., 2019)\nuses Locally Sensitive Hashing to accelerate attention,\nyielding a O(Nlog N) complexity; LongFormer (Beltagy\net al., 2020) works by combining sparse global attention\nwith local attention; Linear Transformers (Katharopou-\nlos et al., 2020)/ Linformer (Wang et al., 2020)/ EfÔ¨Åcient\nAttention (Shen et al., 2018) achieve linear O(N) com-\nplexity by low-rank approximation. Set Transformer (Lee\net al., 2019) reduces the computation by introducing a\nset of induced points, which plays a role similar to cen-\ntroids, only serve as the intermediary layer between N\ninputs and N outputs. Another related work is Routing\nTransformers (Roy et al., 2020), which introduces a sparse\nrouting module based on online k-means, reducing the\ncomplexity of attention to O(n1.5).\nPoWER-BERT (Goyal et al., 2020) also reduces the num-\nber of tokens gradually to improve the efÔ¨Åciency of BERT.\nHowever, their work fundamentally differs from ours from\nthree aspects: (1) Their work is motivated by the phe-\nnomenon of information diffusion in BERT speciÔ¨Åcally,\nwhich may not be the case in other transformers. (2)\nTheir work focus on select a subset of tokens from the\noriginal sequence, while the emphasis of our work is\nsummarize the information into several centroids. This\nleads to completely distinct structure design. (3) Their\nscoring function is human-designed. In contrast, we start\nfrom clustering algorithm, and derives a novel connection\nbetween gradient-based clustering and attention.\nCapsule Networks Similar to our method, capsule net-\nworks (Hinton et al., 2011) are also based on the idea\nof ‚Äúbuilding clustering algorithms into neural networks‚Äù.\nDifferent from our method, which is based on amortizing\ngradient-based clustering algorithms, Dynamic routing\nand EM routing (Sabour et al., 2017; Wang & Liu, 2018;\nHinton et al., 2018) in capsule networks are based on\namortizing EM like algorithms. However, unlike our\nmethod, dynamic routing and EM routing are not train-\nable modules and hence not as efÔ¨Åcient as our method in\nextracting data information. In addition, our method does\nnot need to store the pairwise assignment information like\ndynamic/EM routing and hence reduces the runtime space\nconsumption.\nAdaptive Down-sampling At a high level, our method\ncan be viewed as an attention-like mechanism for adap-\ntively down-sampling the inputs, which forms a key build-\ning block for deep neural networks in computer vision.\nIn convolutional neural networks, various techniques\nhave been proposed, including Ô¨Åxed strategies such as\npooling (Simonyan & Zisserman, 2014), strided convolu-\ntion (Springenberg et al., 2014), dilated convolution (Yu &\nKoltun, 2015), learnable down-sampling techniques such\nas Local-importance based pooling (Gao et al., 2019),\ndeformable convolution (Dai et al., 2017), and trainable\nRegion-of-Interest (ROI) Pooling (Gu et al., 2018). In\naddition, Nezhadarya et al. (2020) proposes an adap-\ntive down-sampling layer for point cloud classiÔ¨Åcation.\nRandLA-Net (Hu et al., 2020) uses random-sampling to\nprocess large-scale point cloud. Comparing these meth-\nods, which mostly focus on convolutional neural networks\n(CNNs), our method provides a general and basic adap-\ntive down-sampling strategy for transformers, which is\nexpected to Ô¨Ånd important applications as the counterparts\nin CNNs.\nMetric Learning for Clustering A line works have\nbeen developed to learn domain-speciÔ¨Åc similarity func-\ntions to boost the performance of the clustering algorithms\nbased on the learned metrics (Yang et al., 2016, 2017; Hsu\net al., 2018; Aljalbout et al., 2018; Yang et al., 2019). This\nyields a metric learning task. Our work is fundamentally\ndifferent we use clustering to inspire the design of a new\ntransformer architecture, and our goal is not to actually\noptimize the clustering quality for a speciÔ¨Åc problem.\n8\n5 C ONCLUSION\nIn this paper, we propose centroid attention, which per-\nforms summative reasoning for sequence modeling. Cen-\ntroid attention takes the original sequence as input, and\nprovides a shorter sequence of centroids that absorbs the\ninformation of the input sequence. We use ceontroid\nattention to construct centroid transformer. By using cen-\ntroids for later stages, the computational and memory\nconsumption of centroid transformers is signiÔ¨Åcantly re-\nduced against their full-length counterparts. Experiments\nconducted on text summarization, 3D vision and image\nprocessing demonstrates centroid transformers can yield\nsimilar / better performance over the original transformers\nwith high efÔ¨Åciency.\nREFERENCES\nAljalbout, E., Golkov, V ., Siddiqui, Y ., Strobel, M., and\nCremers, D. Clustering with deep learning: Taxonomy\nand new methods. arXiv preprint arXiv:1801.07648,\n2018.\nBeltagy, I., Peters, M. E., and Cohan, A. Longformer:\nThe long-document transformer. arXiv preprint\narXiv:2004.05150, 2020.\nCarion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov,\nA., and Zagoruyko, S. End-to-end object detection with\ntransformers. In European Conference on Computer\nVision, pp. 213‚Äì229. Springer, 2020.\nChang, A. X., Funkhouser, T., Guibas, L., Hanrahan, P.,\nHuang, Q., Li, Z., Savarese, S., Savva, M., Song, S.,\nSu, H., et al. Shapenet: An information-rich 3d model\nrepository. arXiv preprint arXiv:1512.03012, 2015.\nDai, J., Qi, H., Xiong, Y ., Li, Y ., Zhang, G., Hu, H.,\nand Wei, Y . Deformable convolutional networks. In\nProceedings of the IEEE international conference on\ncomputer vision, pp. 764‚Äì773, 2017.\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and\nFei-Fei, L. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision\nand pattern recognition, pp. 248‚Äì255. Ieee, 2009.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\nD., Zhai, X., Unterthiner, T., Dehghani, M., Minderer,\nM., Heigold, G., Gelly, S., et al. An image is worth\n16x16 words: Transformers for image recognition at\nscale. arXiv preprint arXiv:2010.11929, 2020.\nGao, Z., Wang, L., and Wu, G. Lip: Local importance-\nbased pooling. In Proceedings of the IEEE Interna-\ntional Conference on Computer Vision, pp. 3355‚Äì3364,\n2019.\nGoyal, S., Choudhury, A. R., Raje, S., Chakaravarthy, V .,\nSabharwal, Y ., and Verma, A. Power-bert: Accelerating\nbert inference via progressive word-vector elimination.\nIn International Conference on Machine Learning, pp.\n3690‚Äì3699. PMLR, 2020.\nGu, J., Hu, H., Wang, L., Wei, Y ., and Dai, J. Learning\nregion features for object detection. In Proceedings of\nthe European Conference on Computer Vision (ECCV),\npp. 381‚Äì395, 2018.\nHinton, G. E., Krizhevsky, A., and Wang, S. D. Trans-\nforming auto-encoders. In International conference on\nartiÔ¨Åcial neural networks, pp. 44‚Äì51. Springer, 2011.\nHinton, G. E., Sabour, S., and Frosst, N. Matrix cap-\nsules with em routing. In International conference on\nlearning representations, 2018.\nHsu, Y .-C., Lv, Z., and Kira, Z. Learning to cluster in\norder to transfer across domains and tasks. In Interna-\ntional Conference on Learning Representations (ICLR),\n2018.\nHu, Q., Yang, B., Xie, L., Rosa, S., Guo, Y ., Wang, Z.,\nTrigoni, N., and Markham, A. Randla-net: EfÔ¨Åcient se-\nmantic segmentation of large-scale point clouds. InPro-\nceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pp. 11108‚Äì11117, 2020.\nIrie, K., Zeyer, A., Schl ¬®uter, R., and Ney, H. Lan-\nguage modeling with deep transformers. arXiv preprint\narXiv:1905.04226, 2019.\nJiao, X., Yin, Y ., Shang, L., Jiang, X., Chen, X., Li, L.,\nWang, F., and Liu, Q. Tinybert: Distilling bert for\nnatural language understanding. In Proceedings of\nthe 2020 Conference on Empirical Methods in Natural\nLanguage Processing: Findings, pp. 4163‚Äì4174, 2020.\nKatharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F.\nTransformers are rnns: Fast autoregressive transform-\ners with linear attention. In International Conference\non Machine Learning, pp. 5156‚Äì5165. PMLR, 2020.\nKitaev, N., Kaiser, L., and Levskaya, A. Reformer: The\nefÔ¨Åcient transformer. In International Conference on\nLearning Representations, 2019.\nLee, J., Lee, Y ., Kim, J., Kosiorek, A., Choi, S., and\nTeh, Y . W. Set transformer: A framework for attention-\nbased permutation-invariant neural networks. In Inter-\nnational Conference on Machine Learning, pp. 3744‚Äì\n3753. PMLR, 2019.\nLin, C.-Y . Rouge: A package for automatic evaluation\nof summaries. In Text summarization branches out, pp.\n74‚Äì81, 2004.\nNallapati, R., Zhou, B., Gulcehre, C., Xiang, B.,\net al. Abstractive text summarization using sequence-\nto-sequence rnns and beyond. arXiv preprint\narXiv:1602.06023, 2016.\nNezhadarya, E., Taghavi, E., Razani, R., Liu, B., and Luo,\nJ. Adaptive hierarchical down-sampling for point cloud\nclassiÔ¨Åcation. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition,\npp. 12956‚Äì12964, 2020.\nQi, C. R., Su, H., Mo, K., and Guibas, L. J. Pointnet:\nDeep learning on point sets for 3d classiÔ¨Åcation and\nsegmentation. In Proceedings of the IEEE conference\n9\non computer vision and pattern recognition, pp. 652‚Äì\n660, 2017a.\nQi, C. R., Yi, L., Su, H., and Guibas, L. J. Pointnet++:\nDeep hierarchical feature learning on point sets in a\nmetric space. arXiv preprint arXiv:1706.02413, 2017b.\nRamsauer, H., Sch¬®aÔ¨Ç, B., Lehner, J., Seidl, P., Widrich,\nM., Gruber, L., Holzleitner, M., Pavlovi¬¥c, M., Sandve,\nG. K., Greiff, V ., et al. HopÔ¨Åeld networks is all you\nneed. arXiv preprint arXiv:2008.02217, 2020.\nRan, H. and Lu, L. Deeper or wider networks of\npoint clouds with self-attention? arXiv preprint\narXiv:2011.14285, 2020.\nRoy, A., Saffar, M., Vaswani, A., and Grangier, D. EfÔ¨Å-\ncient content-based sparse attention with routing trans-\nformers. arXiv preprint arXiv:2003.05997, 2020.\nRush, A. M., Chopra, S., and Weston, J. A neural attention\nmodel for abstractive sentence summarization. arXiv\npreprint arXiv:1509.00685, 2015.\nSabour, S., Frosst, N., and Hinton, G. E. Dynamic routing\nbetween capsules. arXiv preprint arXiv:1710.09829,\n2017.\nShen, Z., Zhang, M., Zhao, H., Yi, S., and Li, H. EfÔ¨Åcient\nattention: Attention with linear complexities. arXiv\npreprint arXiv:1812.01243, 2018.\nSimonyan, K. and Zisserman, A. Very deep convolutional\nnetworks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556, 2014.\nSpringenberg, J. T., Dosovitskiy, A., Brox, T., and Ried-\nmiller, M. Striving for simplicity: The all convolutional\nnet. arXiv preprint arXiv:1412.6806, 2014.\nTouvron, H., Cord, M., Douze, M., Massa, F., Sablay-\nrolles, A., and J¬¥egou, H. Training data-efÔ¨Åcient image\ntransformers and distillation through attention. arXiv\npreprint arXiv:2012.12877, 2020.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Atten-\ntion is all you need. In Advances in neural information\nprocessing systems, pp. 5998‚Äì6008, 2017.\nWang, D. and Liu, Q. An optimization view on dynamic\nrouting between capsules. 2018.\nWang, Q., Li, B., Xiao, T., Zhu, J., Li, C., Wong, D. F.,\nand Chao, L. S. Learning deep transformer models for\nmachine translation. arXiv preprint arXiv:1906.01787,\n2019a.\nWang, S., Li, B., Khabsa, M., Fang, H., and Ma, H. Lin-\nformer: Self-attention with linear complexity. arXiv\npreprint arXiv:2006.04768, 2020.\nWang, Y ., Sun, Y ., Liu, Z., Sarma, S. E., Bronstein, M. M.,\nand Solomon, J. M. Dynamic graph cnn for learning\non point clouds. Acm Transactions On Graphics (tog),\n38(5):1‚Äì12, 2019b.\nWu, Z., Song, S., Khosla, A., Yu, F., Zhang, L., Tang,\nX., and Xiao, J. 3d shapenets: A deep representation\nfor volumetric shapes. In Proceedings of the IEEE\nconference on computer vision and pattern recognition,\npp. 1912‚Äì1920, 2015.\nYang, B., Fu, X., Sidiropoulos, N. D., and Hong, M.\nTowards k-means-friendly spaces: Simultaneous deep\nlearning and clustering. In international conference on\nmachine learning, pp. 3861‚Äì3870. PMLR, 2017.\nYang, J., Parikh, D., and Batra, D. Joint unsupervised\nlearning of deep representations and image clusters.\nIn Proceedings of the IEEE conference on computer\nvision and pattern recognition, pp. 5147‚Äì5156, 2016.\nYang, L., Zhan, X., Chen, D., Yan, J., Loy, C. C., and\nLin, D. Learning to cluster faces on an afÔ¨Ånity graph.\nIn Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2019.\nYu, F. and Koltun, V . Multi-scale context aggregation by\ndilated convolutions. arXiv preprint arXiv:1511.07122,\n2015.\nZhang, J., Luan, H., Sun, M., Zhai, F., Xu, J., Zhang,\nM., and Liu, Y . Improving the transformer translation\nmodel with document-level context. arXiv preprint\narXiv:1810.03581, 2018.\nZhao, Y ., Birdal, T., Deng, H., and Tombari, F. 3d point\ncapsule networks. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recogni-\ntion, pp. 1009‚Äì1018, 2019.\n10\nA A N ENERGY VIEW OF SELF -ATTENTION\nWe provide another view to draw connection between attention mechanism and learning to abstract with energy-based\nmodels. Let‚Äôs Ô¨Årst rewrite the self-attention operation in an energy-view. We start by deÔ¨Åning the following energy\nfunction on the sequence {xi}N\ni=1,\nE\n(\n{xi}N\ni=1\n)\n=\nN‚àë\ni=1\nN‚àë\nj=1\nŒ∂(x‚ä§\ni xj), (5)\nwhere Œ∂(¬∑,¬∑) is a pairwise energy function. To Ô¨Ånd the sequence with the lowest energy, we can perform gradient\ndescent yielding,\nxi ‚Üê xi ‚àíœµ\nN‚àë\nj=1\n‚àáxiŒ∂(x‚ä§\ni xj)xj, ‚àÄi= 1,...,N. (6)\nProperly setting Œ∂(x‚ä§\ni xj) will recover the single-headed self-attention operation with v(xj) = xj and a similarity\nfunction without the normalization denominator in Eq. (2). In this sense, self-attention can be explained as one gradient\nstep towards the sequence with the lowest energy.\nThe energy function above yields a fully observed pairwise energy function. The centroid attention can be viewed as\ncorresponding to the energy function of restricted Boltzmann machine (RBM) in which {uj}M\nj=1 are viewed as hidden\nvariables,\nE\n(\n{xi}N\ni=1,{uj}M\nj=1\n)\n=\nN‚àë\ni=1\nM‚àë\nj=1\nŒ∂(u‚ä§\nj xi). (7)\nHere, {xi}N\ni=1 are the visible variables, and {uj}M\nj=1 are the hidden variables. Given Ô¨Åxed visible variables, we can\nalso Ô¨Ånd the hidden variables that minimizes the energy by gradient descent,\nuj ‚Üê uj ‚àíœµ\nN‚àë\ni=1\n‚àáuj Œ∂(x‚ä§\ni uj)xi, ‚àÄj = 1,...,M. (8)\nTherefore, centroid attention is like Ô¨Ånding the most likely hidden variable for a given observed variable. Note that the\nderivation here is different from the one in the main text, because the similarity function is not normalized here.\n11",
  "topic": "Centroid",
  "concepts": [
    {
      "name": "Centroid",
      "score": 0.7454803586006165
    },
    {
      "name": "Transformer",
      "score": 0.5779823064804077
    },
    {
      "name": "Computer science",
      "score": 0.4772101044654846
    },
    {
      "name": "Artificial intelligence",
      "score": 0.45680564641952515
    },
    {
      "name": "Engineering",
      "score": 0.19228163361549377
    },
    {
      "name": "Electrical engineering",
      "score": 0.14203780889511108
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I86519309",
      "name": "The University of Texas at Austin",
      "country": "US"
    }
  ],
  "cited_by": 20
}