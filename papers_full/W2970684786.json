{
  "title": "An Empirical Study on Pre-trained Embeddings and Language Models for Bot Detection",
  "url": "https://openalex.org/W2970684786",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A5005031316",
      "name": "Andrés García-Silva",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5008539366",
      "name": "Cristian Berrio",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5011911779",
      "name": "José Manuel Gómez-Pérez",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963626623",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2769056027",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W2963062084",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W4294367149",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W4239072543",
    "https://openalex.org/W2595521492",
    "https://openalex.org/W2962772361",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2784121710",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2587019100",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W2165698076",
    "https://openalex.org/W2254973503"
  ],
  "abstract": "Fine-tuning pre-trained language models has significantly advanced the state of art in a wide range of NLP downstream tasks. Usually, such language models are learned from large and well-formed text corpora from e.g. encyclopedic resources, books or news. However, a significant amount of the text to be analyzed nowadays is Web data, often from social media. In this paper we consider the research question: How do standard pre-trained language models generalize and capture the peculiarities of rather short, informal and frequently automatically generated text found in social media? To answer this question, we focus on bot detection in Twitter as our evaluation task and test the performance of fine-tuning approaches based on language models against popular neural architectures such as LSTM and CNN combined with pre-trained and contextualized embeddings. Our results also show strong performance variations among the different language model approaches, which suggest further research.",
  "full_text": "Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019), pages 148–155\nFlorence, Italy, August 2, 2019.c⃝2019 Association for Computational Linguistics\n148\nAn Empirical study on Pre-trained Embeddings and Language Models for\nBot Detection\nAndres Garcia-Silva\nExpert System\nCalle Profesor Waksman 10\n28036, Madrid, Spain\nagarcia@expertsystem.com\nCristian Berrio\nExpert System\nCalle Profesor Waksman 10\n28036, Madrid, Spain\ncberrio@expertsystem.com\nJose Manuel Gomez-Perez\nExpert System\nCalle Profesor Waksman 10\n28036, Madrid, Spain\njmgomez@expertsystem.com\nAbstract\nFine-tuning pre-trained language models has\nsigniﬁcantly advanced the state of art in a wide\nrange of downstream NLP tasks. Usually,\nsuch language models are learned from large\nand well-formed text corpora from e.g. en-\ncyclopedic resources, books or news. How-\never, a signiﬁcant amount of the text to be\nanalyzed nowadays is Web data, often from\nsocial media. In this paper we consider\nthe research question: How do standard pre-\ntrained language models generalize and cap-\nture the peculiarities of rather short, informal\nand frequently automatically generated text\nfound in social media? To answer this ques-\ntion, we focus on bot detection in Twitter as\nour evaluation task and test the performance\nof ﬁne-tuning approaches based on language\nmodels against popular neural architectures\nsuch as LSTM and CNN combined with pre-\ntrained and contextualized embeddings. Our\nresults also show strong performance varia-\ntions among the different language model ap-\nproaches, which suggest further research.\n1 Introduction\nRecently, transfer learning techniques (Pan and\nYang, 2010) based on language models have suc-\ncessfully delivered breaktrough accuracies in all\nkinds of downstream NLP tasks. Approaches like\nULMFiT (Howard and Ruder, 2018), Open AI\nGPT (Radford et al., 2018) and BERT (Devlin\net al., 2018) have in common the generation of\npre-trained models learned from very large text\ncorpora. The resulting language models are then\nﬁne-tuned for the speciﬁc domain and task, contin-\nuously advancing the state of the art across the dif-\nferent evaluation tasks and benchmarks commonly\nused by the NLP community.\nTransfer learning approaches based on language\nmodels are therefore the NLP analogue to similar\napproaches in other ﬁelds of AI like Computer Vi-\nsion, where the availability of large datasets like\nImageNet (Deng et al., 2009) enabled the develop-\nment of state of the art pre-trained models. Before\nlanguage models, common practice for transfer\nlearning in NLP was based on pre-trained context-\nindependent embeddings. These are also learned\nfrom large corpora and encode different types of\nsyntactic and semantic relations that can be ob-\nserved when operating on the vector space. How-\never, their use is limited to the input layer of neu-\nral architectures, and hence the amount of data\nand training effort necessary to learn a high per-\nformance task-related model is high since it is\nstill necessary to train the whole network. Pre-\ntrained language models, on the other hand, at-\ntempt to learn in the network structure the word\ninter-relations that can be leveraged during the\nﬁne-tuning step, usually by just learning a feed\nforward network for the speciﬁc task. The network\narchitecture varies depending on the approach, in-\ncluding transformers (Vaswani et al., 2017) based\non decoders, encoders and attention mechanisms,\nand bi-directional long-short term memory net-\nworks (Hochreiter and Schmidhuber, 1997).\nLanguage models are usually learnt from high\nquality, grammatically correct and curated text\ncorpora, such as Wikipedia (ULMFiT), BookCor-\npus (Open AI GPT), a combination of Wikipedia\nand BookCorpus (BERT) or News (ELMo). How-\never, a very signiﬁcant amount of the text to be\nanalyzed nowadays is Web data, frequently from\nsocial media. The question that immediately arises\nis therefore whether such language models also\ncapture the nuances of the short and informal lan-\nguage often found in social media channels.\nIn this paper we explore this question and em-\npirically study how pre-trained embeddings and\nlanguage models perform when used to analyze\ntext from social media. To this purpose, we focus\n149\non bot detection in Twitter as evaluation task for\ntwo main reasons. First, the intrinsic relevance of\nthe task for counteracting the automatic spreading\nof disinformation and bias on social media. Sec-\nond, because in this context the gap, in terms of the\nquality and overall characteristics of the language\nused, between the corpora used to learn the lan-\nguage models and the task-speciﬁc text to be ana-\nlyzed (automatically generated in a social media,\nmicro-blogging context) can be particularly repre-\nsentative.\nIn our experiments, prior to evaluating the be-\nhavior of pre-trained language models, we test\npre-trained embeddings as a baseline learned from\ngeneral corpora, social media and informal vocab-\nularies. We choose two popular NLP neural ar-\nchitectures for our binary classiﬁcation task: Long\nShort Term memory networks (LSTM; Hochre-\niter and Schmidhuber, 1997) and convolutional\nnetworks (CNN; LeCun et al., 1998). We\nalso pre-processed our Twitter dataset, observing\na positive effect on our CNN and LSTM classi-\nﬁers while on the other hand such effect was ac-\ntually negative on some of the tested pre-trained\nlanguage models.\nIn general, our results indicate that ﬁne-\ntuned pre-trained language models outperform\npre-trained and contextualized embeddings used\nin conjunction with CNN or LSTM for the task\nat hand. This shows evidence that language mod-\nels actually capture much of the peculiarities of\nsocial media and bot language or at least are ﬂexi-\nble enough to generalize during ﬁne-tuning in such\ncontext. From the different language models we\nevaluated, Open AI GPT beats BERT (base) and\nULMFit in the bot/no bot classiﬁcation task, sug-\ngesting that a forward and unidirectional language\nmodel is more appropriated for social media mes-\nsages than other language modeling architectures,\nwhich is relatively surprising. Nevertheless, the\nconsiderable experimentation we carried out has\nraised a number of additional questions that will\nneed further research. During the workshop, we\naim at sharing and discussing these questions with\nthe participants.\nThe rest of the paper is structured as follows.\nSection 2 describes the state of the art about the\ndifferent models and embeddings used in the ex-\nperiments. Next, the experimental setup is pre-\nsented in section 3, where the learning objective\nis deﬁned as well as the dataset and the used\npre-trained embeddings. Section 4 and 5 present\nthe experiments using CNN and LSTM and dif-\nferent combinations of pre-trained, contextualized\nand dynamically generated embeddings learnt dur-\ning training of the bot/no bot classiﬁcation model.\nThen, section 6 describes the experiments with\npre-trained language models. Finally, a discussion\nabout the results is presented in section 7.\n2 State of the Art\nMikolov’s word2vec (Mikolov et al., 2013) ap-\nproach that proposes an efﬁcient way to learn\nembeddings by predicting words based on their\ncontext using negative sampling sparkled a new\ngeneration of embedding learning methods like\nGloVe (Pennington et al., 2014), Swivel (Shazeer\net al., 2016) and FastText (Joulin et al., 2016).\nThese embeddings capture semantic and syntac-\ntic relations between words that were mapped to\nvector operations in the multidimensional space.\nNevertheless these approaches generate static,\ncontext-independent embeddings for words in the\nvocabulary. ELMo (Peters et al., 2018) overcome\nthis limitation by generating representations for\neach word as a function of the input sentence. In\naddition, while pre-trained embeddings are used\nas input for neural networks, ELMo allows the\nend-task model to learn a contextualized linear\ncombination of its internal representation.\nPre-trained embeddings are used as the ﬁrst\nlayer of models or as additional features to neu-\nral architectures. However as the models are ini-\ntialized randomly a lot of training data was still\nrequired to get a high performance. To alleviate\nthis problem ULMFiT (Howard and Ruder, 2018)\nproposes a transfer learning method that pre-trains\na language model on a large corpus using 3-layer\nLSTM architecture that is then ﬁne-tuned on the\ntarget task. In fact, the ﬁne tuning is done at the\nlanguage model level to reﬂect the target task dis-\ntribution and at the task level.\nIn the same vein the Open AI Generative Pre-\ntrained Transformer (GPT) (Radford et al., 2018)\nlearns a language model on a large corpus us-\ning a multi-layer transformer decoder, and super-\nvised ﬁne-tuning to adapt the parameters to the tar-\nget task. For tasks other than text classiﬁcation\nthe input is transformed into an ordered sequence\nthat the pre-trained model can process. In con-\ntrast, BERT uses a bidirectional transformer (De-\nvlin et al., 2018), also known as a transformer\n150\nencoder, that learns representations jointly condi-\ntioned on left and right context in all layers. Sim-\nilar to ELMo, ULMFiT and Open AI GPT which\npre-train language models, BERT learning objec-\ntive is a masked language model and a binarized\nnext sentence prediction tasks. For a classiﬁca-\ntion process all of the parameters of BERT and the\nclassiﬁcation layer are ﬁne-tuned jointly to maxi-\nmize the log-probability of the correct label.\nOur contribution is an empirical study on the\nﬁtness of the ﬁne-tuning of pre-trained language\nmodels when tested against text from social media\nand the target task is classiﬁcation. We also show\nhow pre-processing of the target task corpus can\naffect the performance of the pre-trained models,\nand compare them with the use of pre-trained and\ncontextualized embeddings as inputs of CNN and\nBiLSTM for the classiﬁcation task.\n3 Experiments\nTo evaluate pre-trained language models with\nTwitter data we focus on the relevant problem of\ndetecting bots in social media. Bots are automatic\nagents that publish information for a variety of\npurposes such as weather and natural hazards up-\ndates, and news, but also for spreading misinfor-\nmation and fake news. In fact, as of 2017 it has\nbeen estimated that as 9% to 15% of twitter ac-\ncounts are bots (Varol et al., 2017) which means\nthat out of the 321 million active user accounts 1\nthe number of automatic agents range from 28 to\n48 million.\nDetecting bots can be addressed as a binary\nclassiﬁcation problem focusing only in the tweet\ntextual content since our main target are language\nmodels, regardless of the other features that might\nbe drawn from the social network, such user meta-\ndata, network features based on the follower and\nfollowee relations, and tweet and retweet activity.\n3.1 Dataset\nTo generate a dataset of tweets generated by bots\nor humans we rely on an existing dataset of bot and\nhuman accounts published by Gilani et al. (2017).\nWe create a balanced dataset containing tweets la-\nbelled as bot or human according to the account la-\nbel. In total our dataset comprises 500,000 tweets\n1Twitter 4th quarter and ﬁscal year 2018 results:\nhttps://www.prnewswire.com/news-releases/twitter-\nannounces-fourth-quarter-and-ﬁscal-year-2018-results-\n300791624.html\nwhere 279,495 tweets were created by 1,208 hu-\nman accounts, and 220,505 tweets were tweeted\nfrom 722 bot accounts.\nIn this sample, bots tend to be more proliﬁc than\nhumans since they average 305 tweets per account\nwhich contrasts with the human average of 231.\nIn addition, bots tend to use more URL (0.8313\nURL per tweet) and hash tags (0.4745 hashtags\nper tweets) in their tweets than humans (0.5781\nURL and 0.2887 hashtags per tweet). This shows\nthat bots aim at maximizing visibility (hashtags)\nand to redirect trafﬁc to other sources (URL). Fi-\nnally, we found that bots display more egoistic\nbehaviour than humans since they mention other\nusers in their tweets (0.4371 user mentions per\ntweet) less frequently than humans (0.5781 user\nmentions per tweet).\n3.2 Pre-trained embeddings\nWe use pre-trained embeddings to train the clas-\nsiﬁers rather than doing it from scratch. We use\npre-trained embeddings learned from Twitter it-\nself, urban dictionary deﬁnitions to accommodate\nthe informal vocabulary often used in the social\nnetwork, and common crawl as a general source\nof information:\n• glove.twitter2: 200 dimension embeddings\ngenerated from Twitter (27B tokens, 1.2M\nvocabulary) using GloVe (Pennington et al.,\n2014).\n• word2vec.urban3: 100 dimension em-\nbeddings generated from Urban Dictio-\nnary deﬁnitions (568K vocabulary) using\nWord2Vec (Mikolov et al., 2013).\n• fastText.crawl4: 300 dimension embed-\ndings generated from Common Crawl\n(600B tokens, 1.9M vocabulary) using\nfastText (Mikolov et al., 2018)\n4 CNN for text classiﬁcation\nWe use convolutional neural networks (CNN; Le-\nCun et al., 1998) for the bot detection task inspired\nby Kim’s work (Kim, 2014) that showed how this\narchitecture achieved good performance in sev-\neral sentence classiﬁcation tasks, and other reports\nlike (Yin et al., 2017) that show good results in\nNLP tasks. The neural network architecture uses\n2https://nlp.stanford.edu/projects/glove/\n3https://data.world/jaredfern/urban-dictionary-\nembedding\n4https://fasttext.cc/docs/en/english-vectors.html\n151\n3 convolutional layers and a fully connected layer.\nEach convolutional layer has 128 ﬁlters of size 5,\nrelu was used as activation function and max pool-\ning was applied in each layer. The fully connected\nlayer uses softmax as activation function to pre-\ndict the probability of each message being written\nby a bot or a human. All the experiments reported\nhereinafter use a vocabulary size of 20k tokens,\nsequence size 200, learning rate 0.001, 5 epochs,\n128 batch size, static embeddings unless otherwise\nstated, and 10-fold cross validation.\nFirst we train the CNN classiﬁer on our dataset\nusing pre-trained embeddings and compare them\nwith randomly generated embeddings. In addition,\nwe pre-process our dataset using the same pre-\nprocessing script 5 that was applied when learn-\ning the GloVe Twitter embeddings. This pre-\nprocessing replaces, for example, URL, numbers,\nuser mentions, hashtags and some ascii emoticons\nwith the corresponding tags. Evaluation results are\npresented in table 1.\nEmbeddings Dim. Pre-proc. Precision Recall F-Measure\nrandom 300 No 0.7567 0.7551 0.7517\nglove.twitter 200 No 0.7641 0.7618 0.7587\nYes 0.7834 0.7790 0.7750\nword2vec.urban 100No 0.7122 0.7119 0.7075\nYes 0.7601 0.7565 0.7522\nfastText.crawl 300 No 0.7679 0.7659 0.7627\nYes 0.7858 0.7849 0.7829\nTable 1: Evaluation of CNN classiﬁers using random\nand pre-trained embeddings. Bold and italics are used\nfor best classiﬁers using pre-processing or not pre-\nprocessing respectively.\nIn this setting, the best classiﬁers, according to\nthe f-measure, is learned using fastText common\ncrawl embeddings and the pre-processed dataset,\nfollowed by the classiﬁer that uses GloVe Twit-\nter embeddings also with pre-processing. In gen-\neral pre-processing improves all the classiﬁers and\nevaluation metrics. Also notice that the CNN with\nword2vec urban dictionary embeddings without\npre-processing underperformed the classiﬁer that\nuses random embeddings, however when using\npre-processing the metrics are better for the for-\nmer.\n4.1 Contextualized embeddings\nIn addition to static pre-trained embeddings we\ntrain CNN classiﬁers with dinamically-generated\n5https://nlp.stanford.edu/projects/glove/pre-process-\ntwitter.rb\nembeddings using ELMo. ELMo embeddings\nwere generated from our dataset, however none of\nthe trainable parameters (i.e., linear combination\nweights) were modiﬁed in the process. Due to the\nhigh dimension of these embeddings (dim=1024)\nwe reduced the sequence size to 50 to avoid mem-\nory errors. Evaluation results, reported in table 2,\nshows that when the corpus was not pre-processed\nELMo embeddings produced the best classiﬁer, in\nterms of f-measure, when compared with classi-\nﬁers learned from pre-trained embedddings and a\ndataset without pre-processing (see results in table\n1 for comparison).\nEmbeddings Dim Preproc. Precision Recall F-Measure\nELMo 1024 No 0.7766 0.7719 0.7675Yes 0.7859 0.7827 0.7798\nTable 2: Evaluation of CNN classiﬁers using contextu-\nalized embeddings.\nHowever, when the corpus was pre-processed\nthe classiﬁer learned from ELMo embeddings un-\nderperforms with respect to the best classiﬁer\nlearned from fastText common crawl embeddings,\nwhile outperforms the classiﬁers learned from\nGloVe and Urban dictionary. Nevertheless, in this\nsetting ELMo embeddings produces the classiﬁer\nwith highest precision. Another important ﬁnd-\ning is that ELMo embeddings always generates the\nclassiﬁer with highest precision, regardless of data\npre-processings.\n4.2 Combining embeddings\nWe experiment by concatenating different pre-\ntrained embeddings in the input layer of the CNN.\nSince fastText embeddings learned the best clas-\nsiﬁers we pivot around them. Results in table 3\nshow that the best classiﬁer is learned using fast-\nText common crawl and GloVe Twitter embed-\ndings with data pre-processing, and this classiﬁer\nis better than any of the previous classiﬁers re-\nported in tables 1 and 2.\nNevertheless, if we consider the results with-\nout pre-processing the combination of these em-\nbeddings with ELMo generates the best classiﬁer,\nwhich is compatible with what we found above\nwhen ELMo embeddings help to learn the best\nclassiﬁer when the dataset was not pre-processed\n(see table 2). Similarly, this combination of em-\nbeddings helps to learn the classiﬁer with highest\nprecision regardless data pre-proccessing.\n152\nEmbeddings Pre-proc. Precision Recall F-Measure\nfastText.crawl+glove.twitterNo 0.7724 0.7704 0.7672Yes 0.7906 0.7887 0.7862\nfastText.crawl+word2vec.urbanNo 0.7598 0.7566 0.7526Yes 0.7826 0.7798 0.7767fastText.crawl + glove.twitter+ word2vecurban No 0.7675 0.7644 0.7606Yes 0.7806 0.7782 0.775fastText.crawl + glove.twitter+ ELMo No 0.7787 0.7771 0.7744Yes 0.79250.7861 0.7816\nTable 3: Evaluation of CNN classiﬁers using concate-\nnations of pre-trained and contextualized embeddings.\nBold and italics are used for best classiﬁers using pre-\nprocessing or not pre-processing respectively.\n4.3 Dynamic and pre-trained embeddings\nAnother option to improve these classiﬁers is to\nallow the CNN to adjust dynamically the embed-\ndings or part of them in the learning process. To\ndo so, we generate 300 dimension embeddings\ninitialized randomly and conﬁgure the CNN to\nmake them trainable. In addition, we concatenate\nthese random and trainable embeddings to the pre-\ntrained and ELMo embeddings, which were not\nmodiﬁed in the learning process. In this round of\nexperiments we always use pre-processing since in\nthe previous sections this option always improved\nthe classiﬁers.\nTable 4 shows that dynamic embeddings by\nthemselves help to learn a classiﬁer better than\nall the previous reported. Nevertheless, there ex-\nists the risk of over-ﬁtting since the embeddings\nare tailored to the classiﬁcation task, and that is\nwhy it makes sense to combine them with embed-\ndings learned from other corpora. In this case, the\ncombination of dynamic and ELMo embeddings\ngenerates the best classiﬁer. Another interesting\nﬁnding is that for the ﬁrst time a classiﬁer using\nword2vec urban dictionary is better than the others\nusing GloVe twitter and fastText common crawl.\nWe think that the reduced dimensionality of ur-\nban dictionary embeddings (100 dim) compared to\nTwitter and common crawl embeddings (200 dim\nand 300dim) allows the dynamic embeddings (300\ndim) to inﬂuence more the learning process, and\nachieve better results.\nEmbeddings Precision Recall F-Measure\ndynamic 0.7956 0.7957 0.7950\ndynamic + glove.twitter 0.8051 0.8042 0.8027\ndynamic + fastText.crawl 0.8013 0.8016 0.8009\ndynamic + word2vec.urban 0.8066 0.8053 0.8034\ndynamic + ELMo 0.8125 0.8097 0.8073\nTable 4: Evaluation of CNN classiﬁers using dynamic\nembeddings and pre-trained and contextualized embed-\ndings using a pre-processed dataset\nIn addition, as shown in table 5 we evaluate\ndifferent combination of the dynamic embeddings\nand concatenations of the pre-trained and contex-\ntualized embeddings. None of these attempts gen-\nerate a better classiﬁer than the one using the com-\nbination of dynamic embeddings and ELMo. Nev-\nertheless, concatenating more embeddings never\nworsens the evaluation results, and most of the\ntime improves them, with the exception of ELMo\nembeddings.\nEmbeddings Precision Recall F-Measure\ndynamic + glove.twitter\n+ word2vec.urban 0.8067 0.8056 0.8041\ndynamic + fastText.crawl\n+glove.twitter 0.8057 0.8045 0.8027\ndynamic + fastText.crawl + glove\n.twitter + word2vec.urban0.8092 0.8078 0.8060\ndynamic + fastText.crawl + ELMo0.8118 0.8093 0.8070\ndynamic + fastText.crawl + glove\n.twitter+ELMo 0.8105 0.8088 0.8070\ndynamic + fastText.crawl + glove\n.twitter+word2vec.urban+ELMo0.8131 0.8096 0.8069\nTable 5: Evaluation of CNN classiﬁers using dynamic\nembeddings and concatenations of to pre-trained and\ncontextualized embeddings.\nFigure 1 presents an overview of all the CNN\nclassiﬁers evaluated so far using pre-processing\nsorted in descending order by f-measure. This ﬁg-\nure shows how different classiﬁers were generated\nby using initially single pre-trained embeddings\nand combinations of them. The upper part of the\nﬁgure is dominated by classiﬁers that use dynamic\nand pre-trained embeddings where ELMo embed-\ndings are always involved.\n5 Bidirectional long short term memory\nnetworks\nIn addition to CNN we test Long Short\nTerm Memory networks LSTM (Hochreiter and\nSchmidhuber, 1997), a neural architecture that is\nalso often used in NLP tasks (Yin et al., 2017).\nLSTM are sequential networks that are able to\nlearn long-term dependencies. In our experiments\nwe use a bidirectional LSTM that processes the se-\nquence of text forward and backward to learn the\nmodel. The architecture of the BiLSTM comprises\nan embedding layer, the BiLSTM layer with 50\nprocessing cells, and a fully connected layer that\nuses softmax as activation function to predict the\nprobability of each message being written by a bot\nor a human. The rest of hyperparameters are set\nwith the same values that we use for the CNN ex-\nperiments.\n153\nFigure 1: Evaluation of CNN classiﬁers learned from the single and concatenated pre-trained, contextualized and\ndynamic embeddings. The results are sorted in descending order by f-measure\nIn our experiments we test the embeddings\ncombination that generates the best CNN classi-\nﬁers: dynamic and ELMo embeddings, and also\nthis combination enriched with fastText common\ncrawl embeddings. Evaluation presented in table\n6 shows that the best BiLSTM classiﬁer learned\nfrom dynamic and ELMo emdeddings performs is\nvery similar to the corresponding CNN. In fact, de-\nspite a slightly higher f-measure the individual val-\nues of precision and recall reported for the CNN\nare higher. In this experiment we do not ﬁnd rel-\nevant differences between the CNN classiﬁers and\ntheir BiLSTM counterparts.\nEmbeddings Precision Recall F-Measure\ndynamic + ELMo 0.8095 0.8088 0.8074\ndynamic+fastText.crawl\n+ ELMo 0.8093 0.8087 0.8073\nTable 6: Evaluation of BiLSTM classiﬁers using dy-\nnamic and pre-trained embeddings and a pre-processed\ncorpus.\n6 Pre-trained languages models and\nﬁne-tuning\nIn this section we present the evaluation results for\nthe bot detection task using pre-trained language\nmodels and ﬁne-tuning approaches. We follow the\nﬁne-tune procedures available for ULMFit6, Open\n6https://docs.fast.ai/text.html#Fine-tuning-a-language-\nmodel\nAI GPT7, and BERT8. In all cases we use the de-\nfault hyper-parameters:\n• BERT base: 3 epochs, batch size of 32, and a\nlearning rate of 2e-5\n• Open AI GPT: 3 epochs, batch size of 8, and\na learning rate of 6.25e-5\n• ULMFiT: 2 epochs for the language model\nﬁne-tuning and 3 epochs for the classiﬁer,\nbatch size of 32, and a variable learning rate.\nThe classiﬁers evaluation results are presented in\ntable 7. Considering f-measure the best clas-\nsiﬁer is learned by Open AI GPT, followed by\nBERT base model classiﬁer. Transformer based\napproaches are more up to deal with social media\nmessages. While Open AI GPT learns a classi-\nﬁer with highest recall, BERT base model does it\nwith the highest precision. ULMFiT, on the other\nhand, achieves a high precision, although lower\nthan the rest, and a low recall hence the low f-\nmeasure. Both, Open AI GPT and BERT base im-\nprove f-measure with respect to the best classiﬁer\nlearned previously by a BiLSTM using dynamic\nand ELMo embeddings (see table 6).\nIn addition, we evaluate how data pre-\nprocessing affects the pre-trained language mod-\nels and ﬁne-tuning approaches. Evaluation results\npresented in table 8 shows that while ULMFiT\nperformance improves, Open AI GPT and BERT\nbase worsen. Nevertheless, ULMFiT classiﬁer is\n7https://github.com/tingkai-zhang/pytorch-openai-\ntransformer clas\n8https://github.com/google-research/bert#ﬁne-tuning-\nwith-bert\n154\nPre-trained Languagemodel Precision Recall F-Measure\nBERT base 0.85720.8213 0.8388ULMFiT 0.8471 0.6902 0.7606Open AI GPT 0.85670.8546 0.8533\nTable 7: Pre-trained language models and ﬁne-tuning\nwithout data pre-processing\nstill worse than Open AI and BERT base classi-\nﬁers. Similarly to what we found above BERT\nbase has the highest precisions while Open AI\nGPT the highest recall. Note that none of these\nclassiﬁers beats the Open AI GPT learned from a\nnon pre-processed dataset (see table 7).\nPre-trained Languagemodel Precision Recall F-Measure\nBERT base 0.84810.7948 0.8206ULMFiT 0.8096 0.7510 0.8123Open AI GPT 0.82570.8243 0.8229\nTable 8: Pre-trained Language models and ﬁne tuning\nwith data pre-processing\n7 Discussion\nIn this paper we use a classiﬁcation task to vali-\ndate whether the improvement that transfer learn-\ning approaches based on ﬁne-tuning pre-trained\nlanguage models have brought to NLP tasks can\nbe also achieved with social media text. The\nchallenge for these models is that they have been\nlearned from corpora like Wikipedia, News, or\nBooks, where text is well written, grammati-\ncally correct and contextualized. On the other\nhand, social media messages are short and full of\nacronyms, hashtags, user mentions, urls, and mis-\npellings. Our learning objective is detecting bots\nin Twitter messages since as automated agents the\ngenerated text is potentially different than the text\nsources used to pre-trained the language models.\nWe ﬁrst present experimental results using clas-\nsiﬁers trained with CNN and BiLSTM neural ar-\nchitectures along pre-trained, contextualized and\ndynamic embeddings. From the experiment re-\nsults we conclude that using a concatenation of\ndynamically adjusted embeddings in the training\nprocess plus contextualized embeddings generated\nby ELMo helps to learn the best classiﬁers. Nev-\nertheless, the models using ELMo embeddings ex-\nclusively were penalized when the training data\nwas pre-processed. This was an unexpected result\nsince ELMo works at the character level allowing\nit to work with unseen tokens like the tags that we\nuse to replace the actual tokens in the messages.\nNext, we ﬁne-tune pre-trained language models\ngenerated with ULMFit, BERT base and Open AI\nGPT, showing that the last two approaches gener-\nate classiﬁers that outperform the best classiﬁers\ngenerated by the CNN and BiLSTM respectively,\nwhile ULMFit performance only improves over\nthese classiﬁers when the data was pre-processed.\nIn addition, BERT always learns the classiﬁer with\nthe highest precision while Open AI GPT learns\nthe classiﬁer with highest recall.\nThese results open many questions that need\nmore research such as:\n• Are unidirectional language models such as\nOpen AI GPT more ﬁtted for short and infor-\nmal text?\n• Is the bidirectional approach used in BERT\ncontributing to the highest precision, while\nthe masked tokens are decreasing its recall?\n• Why does the BiLSTM approach used in\nUMLFiT perform better with pre-processed\ndata in contrast to the other approaches or is\nthis a result of the techniques used in the ﬁne-\ntuning steps (gradual unfreezing, discrimina-\ntive ﬁne-tuning, and slanted triangular learn-\ning rates) ?\nWe expect to discuss these questions within the\nworkshop to get more insights about the presented\nexperimental work.\nAcknowledgments\nThis work has been partially supported by\nLETSCROWD and The European Language Grid\nprojects funded by the European Unions Horizon\n2020 research and innovation programme under\ngrant agreements No 740466 and No 825627,\nrespectively. Special thanks to Ronald Denaux for\ndiscussion and technical support.\nReferences\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nFei. 2009. ImageNet: A Large-Scale Hierarchical\nImage Database. In CVPR09.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\n155\nZafar Gilani, Ekaterina Kochmar, and Jon Crowcroft.\n2017. Classiﬁcation of twitter accounts into auto-\nmated agents and human users. In Proceedings of\nthe 2017 IEEE/ACM International Conference on\nAdvances in Social Networks Analysis and Mining\n2017, ASONAM ’17, pages 489–496, New York,\nNY , USA. ACM.\nSepp Hochreiter and J¨urgen Schmidhuber. 1997. Long\nshort-term memory. Neural Comput., 9(8):1735–\n1780.\nJeremy Howard and Sebastian Ruder. 2018. Fine-\ntuned language models for text classiﬁcation.CoRR,\nabs/1801.06146.\nArmand Joulin, Edouard Grave, Piotr Bojanowski, and\nTomas Mikolov. 2016. Bag of tricks for efﬁcient text\nclassiﬁcation. CoRR, abs/1607.01759.\nYoon Kim. 2014. Convolutional neural networks for\nsentence classiﬁcation. In Proceedings of the 2014\nConference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP 2014, October 25-29,\n2014, Doha, Qatar, A meeting of SIGDAT, a Special\nInterest Group of the ACL, pages 1746–1751.\nYann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick\nHaffner. 1998. Gradient-based learning applied to\ndocument recognition. Proceedings of the IEEE,\n86(11):2278–2324.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey\nDean. 2013. Efﬁcient estimation of word represen-\ntations in vector space. CoRR, abs/1301.3781.\nTomas Mikolov, Edouard Grave, Piotr Bojanowski,\nChristian Puhrsch, and Armand Joulin. 2018. Ad-\nvances in pre-training distributed word representa-\ntions. In Proceedings of the International Confer-\nence on Language Resources and Evaluation (LREC\n2018).\nSinno Jialin Pan and Qiang Yang. 2010. A survey on\ntransfer learning. IEEE Trans. on Knowl. and Data\nEng., 22(10):1345–1359.\nJeffrey Pennington, Richard Socher, and Christopher D\nManning. 2014. Glove: Global vectors for word\nrepresentation. In EMNLP, volume 14, pages 1532–\n1543.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers), pages 2227–\n2237. Association for Computational Linguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training. URL https://s3-\nus-west-2. amazonaws. com/openai-assets/research-\ncovers/languageunsupervised/language under-\nstanding paper. pdf.\nNoam Shazeer, Ryan Doherty, Colin Evans, and Chris\nWaterson. 2016. Swivel: Improving Embeddings by\nNoticing What’s Missing. arXiv preprint.\nOnur Varol, Emilio Ferrara, Clayton A. Davis, Filippo\nMenczer, and Alessandro Flammini. 2017. Online\nhuman-bot interactions: Detection, estimation, and\ncharacterization. In ICWSM.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. CoRR, abs/1706.03762.\nWenpeng Yin, Katharina Kann, Mo Yu, and Hinrich\nSchtze. 2017. Comparative study of cnn and rnn for\nnatural language processing.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8152505159378052
    },
    {
      "name": "Language model",
      "score": 0.6717345118522644
    },
    {
      "name": "Task (project management)",
      "score": 0.6452358365058899
    },
    {
      "name": "Natural language processing",
      "score": 0.642659604549408
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6379315257072449
    },
    {
      "name": "Social media",
      "score": 0.6309962868690491
    },
    {
      "name": "Focus (optics)",
      "score": 0.6142957210540771
    },
    {
      "name": "Empirical research",
      "score": 0.43040311336517334
    },
    {
      "name": "Range (aeronautics)",
      "score": 0.4122946858406067
    },
    {
      "name": "World Wide Web",
      "score": 0.1513560712337494
    },
    {
      "name": "Materials science",
      "score": 0.0
    },
    {
      "name": "Composite material",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": []
}