{
    "title": "Streamlining systematic reviews with large language models using prompt engineering and retrieval augmented generation",
    "url": "https://openalex.org/W4410251088",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A3163858149",
            "name": "Fouad Trad",
            "affiliations": [
                "American University of Beirut"
            ]
        },
        {
            "id": "https://openalex.org/A3025061736",
            "name": "Ryan Yammine",
            "affiliations": [
                "American University of Beirut"
            ]
        },
        {
            "id": "https://openalex.org/A5115615963",
            "name": "Jana Charafeddine",
            "affiliations": [
                "American University of Beirut"
            ]
        },
        {
            "id": "https://openalex.org/A2095451980",
            "name": "Marlene Chakhtoura",
            "affiliations": [
                "American University of Beirut"
            ]
        },
        {
            "id": "https://openalex.org/A1977664040",
            "name": "Maya Rahme",
            "affiliations": [
                "American University of Beirut"
            ]
        },
        {
            "id": "https://openalex.org/A4202438769",
            "name": "Ghada El-Hajj Fuleihan",
            "affiliations": [
                "American University of Beirut"
            ]
        },
        {
            "id": "https://openalex.org/A2109368396",
            "name": "Ali Chehab",
            "affiliations": [
                "American University of Beirut"
            ]
        },
        {
            "id": "https://openalex.org/A3163858149",
            "name": "Fouad Trad",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3025061736",
            "name": "Ryan Yammine",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5115615963",
            "name": "Jana Charafeddine",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2095451980",
            "name": "Marlene Chakhtoura",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1977664040",
            "name": "Maya Rahme",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4202438769",
            "name": "Ghada El-Hajj Fuleihan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2109368396",
            "name": "Ali Chehab",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2593758073",
        "https://openalex.org/W1907286193",
        "https://openalex.org/W1760234209",
        "https://openalex.org/W2560438049",
        "https://openalex.org/W3093336514",
        "https://openalex.org/W3111278950",
        "https://openalex.org/W2797780449",
        "https://openalex.org/W2999783216",
        "https://openalex.org/W4393222088",
        "https://openalex.org/W4387144848",
        "https://openalex.org/W4392791588",
        "https://openalex.org/W4409920145",
        "https://openalex.org/W4281646566",
        "https://openalex.org/W4385307760",
        "https://openalex.org/W4365816276",
        "https://openalex.org/W4399210824",
        "https://openalex.org/W4391723759",
        "https://openalex.org/W4385614921",
        "https://openalex.org/W4391993820",
        "https://openalex.org/W4398256172",
        "https://openalex.org/W4399668246",
        "https://openalex.org/W4401633394",
        "https://openalex.org/W4401759841",
        "https://openalex.org/W4400414272",
        "https://openalex.org/W4399702760"
    ],
    "abstract": "The LLM-based system significantly enhances SR efficiency, compared to manual methods and Rayyan while maintaining low FNR.",
    "full_text": "RESEARCH Open Access\n© The Author(s) 2025. Open Access  This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 \nInternational License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you \ngive appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the \nlicensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit  h t t p  : / /  c r e a  t i  \nv e c  o m m  o n s .  o r  g / l  i c e  n s e s  / b  y - n c - n d / 4 . 0 /.\nTrad et al. BMC Medical Research Methodology          (2025) 25:130 \nhttps://doi.org/10.1186/s12874-025-02583-5\nBMC Medical Research \nMethodology\n*Correspondence:\nFouad Trad\nfat10@mail.aub.edu\n1Department of Electrical and Computer Engineering, American \nUniversity of Beirut, Beirut, Lebanon\n2Faculty of Medicine, American University of Beirut, Beirut, Lebanon\nAbstract\nBackground Systematic reviews (SRs) are essential to formulate evidence-based guidelines but require time-\nconsuming and costly literature screening. Large Language Models (LLMs) can be a powerful tool to expedite SRs.\nMethods We conducted a comparative study to evaluate the performance of a commercial tool, Rayyan, and an \nin-house LLM-based system in automating the screening of a completed SR on Vitamin D and falls. The SR retrieved \n14,439 articles, and Rayyan was trained with 2,000 manually screened articles to categorize the rest as most likely to \nexclude/include, likely to exclude/include and undecided. We analyzed Rayyan’s title/abstract screening performance \nusing different inclusion thresholds. For the LLM, we used prompt engineering for title/abstract screening and \nRetrieval-Augmented Generation (RAG) for full-text screening. We evaluated performance using article exclusion \nrate (AER), false negative rate (FNR), specificity, positive predictive value (PPV), and negative predictive value (NPV). \nAdditionally, we compared the time required to complete screening steps of the SR using both approaches against \nthe manual screening method.\nResults Using Rayyan, including considered as undecided or likely to include for title/abstract screening resulted in \nan AER of 72.1% and an FNR of 5%. The total estimated screening time, including manual review of articles flagged \nby Rayyan, was 54.7 hours. Lowering the Rayyan threshold to ‘likely to exclude’ reduced the FNR to 0% and the AER to \n50.7%, but increased the screening time to 81.3 h. Using the LLM system, after title/abstract and full-text screening, \n78 articles remained for manual review, including all 20 identified by traditional methods. The LLM achieved an AER \nof 99.5%, specificity of 99.6%, PPV of 25.6%, and NPV of 100%, with a total screening time of 25.5 h, including manual \nreview of the 78 articles, reducing the manual screening time by 95.5%.\nConclusions The LLM-based system significantly enhances SR efficiency, compared to manual methods and Rayyan \nwhile maintaining low FNR.\nKeywords Systematic reviews, Large language models, Prompt engineering, Retrieval-augmented generation, \nRayyan AI\nStreamlining systematic reviews with large \nlanguage models using prompt engineering \nand retrieval augmented generation\nFouad Trad1*, Ryan Yammine2, Jana Charafeddine1, Marlene Chakhtoura2, Maya Rahme2, Ghada El-Hajj Fuleihan2 and \nAli Chehab1\nPage 2 of 9\nTrad et al. BMC Medical Research Methodology           (2025) 25:130 \nBackground\nSystematic Reviews (SRs) are an essential pillar for evi -\ndence-based guideline development. However, their pro -\ncess is labor-intensive and time-consuming, requiring \nauthors to screen thousands of articles, with SRs taking \non average 67.3 weeks to complete [ 1]. One of the most \ntime-consuming steps in SRs is literature screening, \nwhich is conducted in duplicate and independently, in \ntwo steps: title/abstract screening followed by full-text \nscreening.\nThere is an increasing demand for rapid and frequent \nSRs by scientists to stay up-to-date in their field as sci -\nentific data output is increasing rapidly worldwide, with \nthe corpus of literature doubling every 9 years [ 2]. Other \npressing needs are incurred by pandemics and living \npractice guidelines.\nTo meet this increasing demand, many artificial \nintelligence-based tools have emerged in an attempt \nto expedite this process, such as Abstrackr, Rayyan AI, \nASreviews, Colandr, and DistillerAI [ 3–7]. These tools \nvary significantly in their core algorithm and features, \nbut are mostly limited to title/abstract screening. In one \nreview conducted in 2020 comparing various AI-based \ntitle/abstract screening tools, Rayyan AI scored the \nhighest in weighted feature analysis [ 8]. Rayyan AI is a \nweb-based semi-automated screening tool, developed \nby Qatar Computing Research Institute [ 4]. It works by \nfeeding words, pairs of words and Medical Subject Head -\nings (MeSH) terms from the titles and abstracts to a \nMachine Learning (ML) algorithm, more specifically, a \nSupport Vector Machine (SVM) classifier [4].\nRecent studies have shown success in the use of Large \nLanguage models (LLMs) such as GPT-3.5 and GPT-4 \nin title/abstract screening [ 9, 10]. Attempts to leverage \nLLMs for both title/abstract and full text screening are \nlimited [11]. We investigate the use of LLM techniques, \nsuch as Prompt Engineering and Retrieval-Augmented \nGeneration (RAG), to automate the aforementioned pro -\ncesses. We propose an end-to-end system powered by \nGPT-4 that receives an article along with inclusion and \nexclusion criteria, and then decides whether to include or \nexclude the article from the SR.\nWe capitalize on a completed SR on vitamin D and \nfalls to compare the performance of the two ML-based \nsystems, with the traditional manual method as the gold \nstandard [12].\nMethods\nData Preparation\nWe used data from a recently completed umbrella review \non Vitamin D and Falls [ 12]. After title/abstract screen -\ning, 1,680 full-text papers were reviewed, with 20 SRs of \nRandomized Controlled Trials (RCTs) included in the \nfinal review (Appendix 1).\nManual traditional title/abstract screening followed a \nvalidated screening guide (Appendix 2). However, results \nfor 430 articles were inadvertently not saved, reducing \nthe total dataset to 17,346 articles. Importantly, none of \nthe 430 excluded articles were among the final 20 articles \nincluded in the completed analysis of the completed SR \nusing the gold standard method [12].\nThe 17,346 articles were imported into Rayyan soft -\nware. Duplicates were removed using Rayyan’s Duplicate \nDetection Tool.\nRayyan AI\nOne reviewer trained Rayyan AI by manually screening \n2,000 random articles in batches of 100, using the com -\npleted umbrella review’s title/abstract screening guide \n(Appendix 2). The reviewer assigned one reason for \nexclusion for manually excluded articles, following the \nscreening guide.\nAfter each set of 100 articles screened, Rayyan would \nclassify unscreened articles into five categories: “Most \nLikely To Exclude” , “Likely To Exclude” , “Undecided” , \n“Likely To Include” or “Most Likely To Include” based on \npatterns learned during the screening phase. We consid -\nered articles rated as “Undecided” or higher to require \nfurther manual title/abstract screening, and excluded \narticles rated as “Likely To Exclude” or lower (Threshold \nA). Furthermore, we analyzed the results of lowering the \nthreshold for exclusion to “Most Likely To Exclude” only \n(Threshold B).\nA second researcher intentionally excluded the 20 final \narticles identified with the gold standard method [ 12] \nfrom the batch to be screened for Rayyan model train -\ning. This was to ensure that he/she did not accidentally \ninclude any of these articles in the model training, which \ncould subsequently affect the integrity of the compari -\nson between the manual and the Rayyan methods. This \napproach does not apply when deploying Rayyan to de \nnovo systematic reviews.\nAfter training the model, we evaluated the perfor -\nmance of Rayyan on all unscreened articles including the \nabove 20 articles selected in the completed manual SR \n[12]. We stopped training Rayyan when the number of \nunscreened articles in each category stabilized - meaning \nthat after each batch of 100 screened articles, the distri -\nbution of articles across the five categories showed mini -\nmal change. For this study, this saturation occurred at \n2000 articles. (Fig. 1).\nRayyan AI has recently introduced a full-text screening \nfeature. However, this feature does not incorporate AI, \nautomation, or machine learning. Instead, it functions as \na platform for users to manually review and record their \nscreening decisions without assisting or learning from \nthe process. For this reason, we will limit our evaluation \nof Rayyan AI to title and abstract only.\nPage 3 of 9\nTrad et al. BMC Medical Research Methodology           (2025) 25:130 \nProposed LLM-based approach\nThe proposed system relies on two LLM techniques: \nprompt engineering, which involves designing specific \ninput prompts to guide the model’s responses, and RAG, \nwhich combines external data retrieval with generative \ncapabilities to enhance accuracy and relevance. Together, \nthese techniques automate the two screening phases: \nTitle and Abstract Screening, and Full-Text Screen -\ning. We called the GPT-4 model via the OpenAI API \n(using ‘gpt-4’ as the model’s name), providing structured \nprompts and receiving responses programmatically. The \nprompts were designed to ensure the model responded in \na specific format, maintaining consistency and minimiz -\ning variability in decision-making.\n  • In title/abstract screening (Phase 1), we input \nthe titles and abstracts of articles into GPT-4 for \nscreening. We give the model a system prompt that \ninstructs it to act as a professional medical researcher \nperforming title/abstract screening. This system \nprompt helps the model adopt the specified role and \nrespond with the appropriate level of expertise and \nfocus, improving accuracy and consistency during \nscreening (Prompt Engineering). Then, we prompt \nthe model with a series of questions identical to the \ntraditional screening criteria used in the original \narticle’s title/abstract screening guide (Appendix \n2). The model responds to each question with “yes, ” \n“no, ” or “unsure. ” When the model is certain about its \ndecision on an article, we proceed accordingly. If the \nmodel is uncertain, we retain the article, just as we \ndo with the traditional process, improving sensitivity.\n  • Articles that pass the first phase undergo a more \nthorough full-text screening (Phase 2), employing \nRAG. The full-text PDFs were first obtained \nmanually and then processed using a Python script \nthat stores them in a vector store using LlamaIndex \nfor efficient retrieval during screening. Here, the full \ntext of each article serves as the document set from \nwhich the GPT-4 model retrieves information. A \nnew set of questions identical to the ones used for \ntraditional screening (Appendix 3) is used to evaluate \nthe full texts. The model’s responses in this phase \nto the first five questions are categorized as “yes, ” \n“no, ” or “unsure. ” Articles are included or excluded \nsimilarly to Step 1. The final question prompts \nthe model to identify the outcome studied in the \nreview—falls, fractures, or mortality. The article is \nonly included if “Falls” is one of the outcomes.\nThe prompts used for both phases can be found in \nAppendix 4.\nTo enhance transparency and facilitate an effective \nreview process, the outcomes of all questions are auto -\nmatically documented in an Excel sheet during both \nphases for every article (Appendix 5). Since the prompts \nasked the model to respond in a structured format, \na Python script was used to log the model’s answers \ndirectly into the sheet, eliminating any manual inter -\nvention in transferring results. This ensures a fully auto -\nmated workflow, removing the possibility of any sort of \nFig. 1 Rayyan classification of unscreened articles into its five default categories with increasing training*. *Training is done in batches of 100, reaching \na total of to 2,000 Articles\n \nPage 4 of 9\nTrad et al. BMC Medical Research Methodology           (2025) 25:130 \nerror. This logging method enables reviewers to assess \nthe rationale behind the model’s decisions. This struc -\ntured documentation ensures that all decisions are trace -\nable and reviewable. This provides a clear audit trail and \nsupports any necessary re-evaluation of articles automat -\nically screened by the model.\nStatistical analysis\nWe considered the completed SR on Vitamin D and Falls \nas our gold standard for comparison. For both steps, \ntrue positives were defined as articles correctly included \nfor further screening, as they were among the articles \nincluded for final analysis, and true negatives were as \narticles correctly excluded. False positives were articles \nincluded by the model for further manual screening but \nexcluded not ultimately included after traditional full-\ntext screening using the manual method, while false neg -\natives were articles excluded by the model but included \nafter manual traditional full-text screening. We used \nthese defined values to calculate the performance metrics \ndescribed below.\nFor title/abstract screening using both methods, we \nevaluated false negative rate (FNR) and article exclusion \nrate (AER). AER is defined as the total number of articles \nautomatically excluded during a step divided by the total \nnumber of articles at the beginning of the relevant step, \nas illustrated in the equation below:\n \nAERTitle/Abstract(%) =Number of automatically excluded articles during Title/Abstract\nTotal number of articles at the beginning of Title/Abstract × 100\nFor full-text screening, which was assessed using the \nLLM model only (since Rayyan’s semi-automation tool \ndoes not support this step), we evaluated FNR, AER, \nspecificity, positive predictive value (PPV), and nega -\ntive predictive value (NPV). We also assessed these per -\nformance metrics from start to end (title/abstract, and \nfull text screen) using the LLM approach. For full-text \nscreening, AER was calculated as illustrated below:\n \nAERFull-Text(%) =Number of automatically excluded articles during Full-Text\nTotal number of articles at the beginning of Full-Text × 100\nTo estimate workload reduction, we considered both the \nAER and the time taken to complete screening of the \nremaining articles. Additionally, FNR was calculated to \nassess the risk of erroneously excluding relevant articles.\nWe estimated time required for each screening method \nas follows:\nFor the traditional screening method, we estimated the \ntime required for both title/abstract screening (M1) and \nfull-text screening (M2).\nFor title/abstract screening using Rayyan AI, we calcu -\nlated the time taken to train the model (R1) and the time \nneeded for manual title/abstract screening of articles \nremaining after automatic screening (R2). The total time \nfor this process was R = R1 + R2. We estimated M1, R1, \nand R2 based on the time it took the reviewer to screen \n100 articles for Rayyan’s training.\nFor the LLM-based model, we recorded time for auto -\nmatic title/abstract screening (S1) and full-text screening \n(S2). Additionally, we estimated time required for manual \nfull-text screening of articles remaining after the auto -\nmatic process (SM). SM and M2 were calculated based \non our team’s experience, which estimated that manually \nscreening one full-text article takes an average of 15 min.\nThe total time required to complete title/abstract, \nand full-text screening using the LLM system was \nS1 + S2 + SM, where SM is equal to 15  min multiplied \nby the number of remaining articles. Since our primary \nfocus was on screening time rather than document \nretrieval, we did not include the time required to collect \nand prepare full-text PDFs in any method. This ensures \na fair comparison, as retrieval would be a necessary pre -\nliminary step regardless of the approach used.\nResults\nRayyan title/abstract screening\nOf the original 17,346 articles, 2,907 articles were deleted \nafter duplicate removal, and 14,439 remained. The \nreviewer took approximately 1 h to perform title/abstract \nscreening on 100 articles. Screening all 14,439 articles \nwould take them approximately M1 = 144.4 h.\nOf the 2,000 articles screened manually to train \nRayyan, 1,727 (86.35%) were excluded, and 273 (13.65%) \nwere included. This step took approximately R1 = 20 h. Of \nthe remaining 12,439 unscreened articles, Rayyan classi -\nfied 6,308 (50.7%) as most likely to exclude, 2,661 (21.4%) \nlikely to exclude, 1,345 (10.8%) undecided, 1,721 (13.8%) \nlikely to include and 404 (3.3%) most likely to include \n(Fig.  1). Of the 20 articles included for final analysis in \nour traditional manual method: 3 were ranked as most \nlikely to include, 6 were ranked as likely to include, 10 \nwere ranked as undecided and 1 was ranked as likely to \nexclude.\nWhen using Threshold A, 8,969 (72.1%) of the 12,439 \nunscreened articles were excluded, with the remaining \n3,470 (27.9%) articles to be screened manually (Fig.  2A), \nwith 1 false negative result. This resulted in: AER 72.1%, \nFNR 5%, reducing the time needed to complete title/\nabstract screening using Rayyan to 54.7 h, an 89.7-hour \nreduction (62%) when compared to the traditional meth -\nods (Fig. 2A).\nIn contrast, when using Threshold B, excluded arti -\ncles decreased to 6,308 (50.7%), and the remaining \n6,131 (49.3%) would undergo further manual screening \n(Fig.  2B), with no false negative results. This inclusion \nthreshold resulted in an AER of 50.7%, and an FNR of \n0%, reducing the time needed to complete title/abstract \nscreening using Rayyan to 81.3 h, a 63.1-hour reduction \nPage 5 of 9\nTrad et al. BMC Medical Research Methodology           (2025) 25:130 \n(44%) when compared to the traditional methods \n(Fig. 2B).\nLLM title/abstract, and full-text screening\nOf the 14,439 articles processed by the GPT-4 model for \ntitle/abstract, 3,298 articles (22.8%) met the inclusion \ncriteria and advanced to Phase 2, achieving an AER of \n77.2%. None of the 20 retained in the traditional method \nwere excluded, achieving an FNR of 0%. This step took \nS1 = 2 h to run.\nIn the subsequent RAG-based full-text screening \nphase, the 3,298 full-text articles were evaluated. Out \nof these, only 78 articles (or 2.37%) were included for \nmanual review, including all 20 articles retained in the \ntraditional method. This step required S2 = 4  h to run, \ncompared to M2 = 1680*15 minutes = 420  h for the tra -\nditional method. The metrics for this step are as follows: \nAER: 97.63%, specificity 99.6%, PPV 25.6%, and NPV \n100%.\nFor the entire process, including both phases, the LLM \nmethod achieved the following metrics: AER 99.5%, \nspecificity 99.6%, PPV 25.6%, and NPV 100%. Manual \nscreening of the remaining 78 articles would take approx-\nimately 19.5  h (SM), bringing the total time for title/\nabstract, and full-text screening using the LLM approach \nto 25.5  h (Fig.  3). This represents a time reduction of \n538.9  h (95.5%) compared to the traditional method, \nwhich required an estimated M1 + M2 = 564.4 h.\nA summary of the performance for both approaches \ncan be found in Table 1.\nDiscussion\nOur study shows that both Rayyan AI and the LLM-\nbased system dramatically reduced the workload for SRs \ncompared to traditional methods, while maintaining a \nlow FNR. However, the LLM-based system stood out \nby not only automating title/abstract screening but also \nincorporating full-text screening, a more challenging \ntask, through advanced techniques like prompt engineer -\ning and RAG. This enabled the LLM to reduce the num -\nber of articles for manual full-text review to just 78 out of \nthe original 14,439.\nCrucially, the LLM-based system achieved a 95.5% \nreduction in screening time compared to the traditional \nmethod, from 564.4 h using the traditional approach, to \nonly 25.5 h. Even more importantly, the LLM maintained \na perfect FNR of 0%, meaning no relevant articles were \nmissed during screening. Unlike Rayyan and traditional \nmethods, which rely on human input, the LLM system \ndrastically reduces human intervention, lowering the \nrisks of human error and bias. This impressive combina -\ntion of time savings and accuracy highlights the LLM’s \ntransformative potential for making SRs more efficient \nand reliable.\nAccording to the Cochrane Collaboration, litera -\nture screening ideally involves two reviewers who inde -\npendently screen articles by following strict screening \nFig. 2 A: Flow diagram of Rayyan title/abstract screening steps and results using Threshold A* as inclusion criteria. *“Undecided” as threshold for inclu -\nsion. **Of the original 17,776 citations, 430 articles were excluded as their results were inadvertently not saved. 2,907 articles were deleted after duplicate \nremoval, of the remaining 17,346 articles and 14,439 remained. B: Flow diagram of Rayyan title/abstract screening steps and results using Threshold B* as \ninclusion criteria. *”Likely To Exclude” as threshold for inclusion. **Of the original 17,776 citations, 430 articles were excluded as their results were inadver-\ntently not saved. 2,907 articles were deleted after duplicate removal, of the remaining 17,346 articles and 14,439 remained\n \nPage 6 of 9\nTrad et al. BMC Medical Research Methodology           (2025) 25:130 \ncriteria, to minimize bias, maximize sensitivity, ensur -\ning that no important articles are missed [ 13]. However, \nthis process is highly time-consuming, particularly for \nlarge-scale systematic reviews that involve thousands \nof articles. Given this challenge, automated assistance \nin the screening process can be beneficial. The LLM-\nbased approach can serve as an initial screening tool, \nsignificantly reducing the number of articles that require \nmanual review. Since our results indicate that the LLM \nsystem achieves a false negative rate (FNR) of 0, it ensures \nthat no relevant articles are erroneously excluded at this \nstage. However, this does not mean that human review -\ners should be replaced. Instead, after the LLM performs \nthe initial screening, reviewers can focus their efforts on \nevaluating the remaining articles in the same way as tra -\nditional screening methods. By reducing the initial work -\nload, this approach allows researchers to dedicate more \ntime to the final selection process, ultimately streamlin -\ning systematic review workflows while maintaining high \nsensitivity and accuracy.\nWhile few publications have explored the potential of \nRayyan software in expediting title/abstract screening, \nthey suffered several drawbacks [ 14–16]. These include \nusing smaller datasets, with samples varying between \n500 and 1512 articles, and lacking details on thresholds \nused, rendering an assessment of their performance met -\nrics challenging [ 16]. Nevertheless, our results based \non a larger dataset show a similar high sensitivity using \nThreshold A [ 14, 15]. However, unlike Valizadeh et al., \nour study also analyzed a more conservative threshold \nwhere false negatives were eliminated [14].\nBeyond commercial systems, such as Rayyan, there has \nbeen a growing interest in leveraging LLMs to enhance \nvarious stages of SRs [ 17]. Reason et al. evaluated the \npotential of LLMs to automate tasks such as data extrac -\ntion, script creation, and report generation within SRs \n[18]. Others have explored the potential of LLMs in \nassessment of the quality and risk-of-bias of publications, \nwith varying degrees of success [19–21].\nFew publications explored the use GPT-4 to automate \ntitle/abstract screening, similar to Phase 1 of our LLM \nmodel [ 10, 22–25]. While these studies demonstrated \nacceptable performance and time savings, they did \nnot extend to full-text screening—a critical and time-\nconsuming phase of SRs. To our knowledge, the only \nexception is the work of Khraisha et al. [ 11]. However, \nFig. 3 Flow diagram of LLM Title/abstract and full text screening steps and results. *Of the original 17,776 citations, 430 articles were excluded as their \nresults were inadvertently not saved. 2,907 articles were deleted after duplicate removal, of the remaining 17,346 articles and 14,439 remained\n \nPage 7 of 9\nTrad et al. BMC Medical Research Methodology           (2025) 25:130 \ntheir method relied on article segmentation for full-text \nscreening. This approach can affect model performance, \nas it may struggle to grasp the context when process -\ning segmented parts in isolation, contrary to our RAG \nframework [11]. As a result, it achieved a low sensitivity \nof 0.42 and 0.38 during phases 1 and 2, respectively [ 11]. \nImportantly, these metrics were based on a limited num -\nber of citations screened, 300 titles/abstracts and 150 full \ntexts [11]. In contrast, our system handled a larger data -\nset of 14,439 articles in the title/abstract screening phase, \nachieving an FNR of 0% (sensitivity of 100%) during both \nsteps, with a high AER. While our study involved a large \ndataset, the performance of the LLM was not influenced \nby the number of articles screened, as we did not train or \nfine-tune the model. Instead, factors such as prompt clar-\nity and retrieval effectiveness in the RAG phase played a \nlarger role.\nNone of the discussed publications, including ours, \nassessed the time needed for the development and final -\nization of the title/abstract and full text screening sheets. \nThis is an iterative and necessary process with a calibra -\ntion phase implemented before the sheets are ready for \nuse by any of the three methods. This, however, does not \naffect our comparisons between methods. Our study has \nseveral strengths. It implemented testing over 14,000 \narticles to pilot our approach, as opposed to a maximum \nof 5,634 in other studies also using LLMs [ 10, 22–25]. \nAdditionally, it demonstrated strong performance, with \nan AER of 99.5%, specificity of 99.6%, PPV of 25.6%, and \nNPV of 100%, outperforming comparable studies in the \nliterature. Although the LLM-based system requires \nengineering expertise to build the model, once opera -\ntional, users can easily interact with it by inputting their \ninclusion and exclusion criteria in the form of questions. \nThis usability feature underscores the practical applica -\ntion of the system in streamlining the review process. \nAdditionally, the transparent logging of each question’s \noutcome in an Excel sheet not only enhances the system’s \nintegrity but also facilitates manual subsequent checks of \nany article, allowing users to trace decisions back to spe -\ncific responses, thus reinforcing trust in this approach.\nWhile our results demonstrate strong performance, \nLLMs are not without limitations. One key disadvan -\ntage is their dependency on prompt design—suboptimal \nprompts can lead to inconsistencies in responses. Addi -\ntionally, LLMs may struggle with complex or nuanced \ninclusion/exclusion criteria that require deep domain \nexpertise, necessitating careful human oversight.\nAlthough the LLM approach demonstrated signifi -\ncant improvements compared to traditional methods \nand Rayyan, its performance should be validated across \ndiverse and complex systematic reviews to confirm its \nrobustness and generalizability. Previous research has \nshown that LLM performance can vary depending on the \ntopic and dataset used [ 26]. This variation suggests that \nwhile our approach achieved strong results in this study, \nfurther evaluations across different domains are neces -\nsary to ensure consistent performance. Additionally, \nTable 1 Summary and comparison of the manual method, Rayyan thresholds A and B and the LLM method\nManual Rayyan \nThreshold A\nRayyan Thresh-\nold B\nLLM\nTitle/Abstract Articles to screen* 14,439 14,439 14,439 14,439\nInclusion Threshold - “Undecided” “Likely to Exclude” -\nArticles Remaining after automated screening (AER) - 3,470 (72.1%) 6,131 (50.7%) 3,280 (77.2%)\nTotal Articles to Manually Screen 14,439 5,470 8,131\nTime taken for all Manual Screening Articles 144.4 h 54.7 h 81.3 h -\nTime for automated screening - - - 2 h\nTrue Positives (FNR) N/A (Gold Standard) 19 (5%) 20 (0%) 20 (0%)\nTotal time for Step 144.4 h 54.7 h 81.3 h 2 h\nTotal Time Saved compared to manual method (%) N/A (Gold Standard) 89.7 h (62.1%) 63.1 h (43.7%) -\nFull Text** Articles to screen 1,680 - - 3,280\nTime to run automated screening - - - 4 h\nArticles Remaining (AER) - - - 78 (97.6%)\nTime to manually screen remaining articles 420 h - - 19.5 h\nTrue Positives (FNR) N/A (Gold Standard) - - 20 (0%)\nTotal time for step (hours) 420 h - - 23.5 h\nTotal Total Time for both steps 564.4 h - - 25.5 h\nTotal Time Saved compared to manual method (%) N/A (Gold Standard) - - 538.9 h \n(95.5%)\nAER: Article Exclusion Rate, FNR: False Negative Rate\n*Of the original 17,776 citations, 430 articles were excluded as their results were inadvertently not saved. 2,907 articles were deleted after duplicate removal, of the \nremaining 17,346 articles and 14,439 remained\n**Rayyan was excluded from full-text comparison, as its article classification feature is not yet supported in its full text screening platform\nPage 8 of 9\nTrad et al. BMC Medical Research Methodology           (2025) 25:130 \nfuture enhancements should focus on refining the log -\nging features to provide even more detailed explanations \nfor each question (knowing why the response was yes, \nno, or unsure), enhancing explainability and the ability to \naudit this approach.\nConclusions\nOur study demonstrates that the proposed LLM-based \nsystem significantly enhances the efficiency of the SR \nprocess compared to both traditional methods and the \ncommercially available Rayyan system, while maintain -\ning low FNR. Its excellent performance metrics, ease of \nuse, explainability, alignment with traditional methods, \nand its time efficiency, position it as a very promising \napproach. Future work could explore expanding the sys -\ntem’s capabilities to support more complex review steps, \nsuch as data extraction and synthesis.\nAbbreviations\nSR  Systematic Review\nLLM  Large Language Model\nRAG  Retrieval-Augmented Generation\nPPV  Positive predictive value\nNPV  Negative predictive value NPV\nAER  Article Exclusion Rate\nFNR  False Negative Rate\nRoB  Risk of Bias\nMeSH  Medical Subject Headings\nSVM  Support Vector machine\nRCT  Randomized Controlled Trial\nML  Machine Learning\nSupplementary Information\nThe online version contains supplementary material available at  h t t p  s : /  / d o i  . o  r \ng /  1 0 .  1 1 8 6  / s  1 2 8 7 4 - 0 2 5 - 0 2 5 8 3 - 5.\nSupplementary Material 1\nAcknowledgements\nRyan Yammine would like to acknowledge the training received under the \nScholars in HeAlth Research Program (SHARP) that was in part supported by \nthe Fogarty International Center and Office of Dietary Supplements of the \nNational Institutes of Health (Award Number D43 TW009118). The content is \nsolely the responsibility of the authors and does not necessarily represent the \nofficial views of the National Institutes of Health.\nAuthor contributions\nFT, GEHF and AC contributed to the Conceptualization. FT, JC, MC, MR and \nGEHF contributed to the Formal Analysis. FT, RY and JC contributed to the \nInvestigation. FT, RY, JC, MC, MR and GEHF contributed to Methodology \ndevelopment. FT, AC and GEHF contributed to Project Administration. FT and \nJC contributed to Software development. FT contributed to Validation. FT and \nRY contributed to Visualization. RY, MC, MR and GEHF contributed to Data \nCuration. FT and RY contributed to Writing of the original draft. FT, RY, MC, \nGEHF and AC contributed to the Review & Editing of subsequent drafts. GEHF \ncontributed to Funding Acquisition and Resource Provision. GEHF and AC \nprovided Supervision of the project.\nFunding\nNone.\nData availability\nThe datasets used and/or analysed during the current study are available from \nthe corresponding author on reasonable request.\nDeclarations\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors declare no competing interests.\nReceived: 12 December 2024 / Accepted: 29 April 2025\nReferences\n1. Borah R, Brown AW, Capers PL, Kaiser KA. Analysis of the time and workers \nneeded to conduct systematic reviews of medical interventions using data \nfrom the PROSPERO registry. BMJ Open. 2017;7(2):e012545.  h t t p  s : /  / d o i  . o  r g /  1 0 \n.  1 1 3 6  / b  m j o  p e n  - 2 0 1  6 -  0 1 2 5 4 5.\n2. Bornmann L, Mutz R. Growth rates of modern science: A bibliometric analysis \nbased on the number of publications and cited references. J Assoc Inf Sci \nTechnol. 2015;66(11):2215–22.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 0 0 2  / a  s i . 2 3 3 2 9.\n3. Rathbone J, Hoffmann T, Glasziou P . Faster title and abstract screening? Evalu-\nating abstrackr, a semi-automated online screening program for systematic \nreviewers. Syst Rev. 2015;4(1):80.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 1 8 6  / s  1 3 6 4 3 - 0 1 5 - 0 0 6 7 - 6.\n4. Ouzzani M, Hammady H, Fedorowicz Z, Elmagarmid A. Rayyan—a web and \nmobile app for systematic reviews. Syst Rev. 2016;5(1):210.  h t t p  s : /  / d o i  . o  r g /  1 0 .  \n1 1 8 6  / s  1 3 6 4 3 - 0 1 6 - 0 3 8 4 - 4.\n5. Hamel C, Kelly SE, Thavorn K, Rice DB, Wells GA, Hutton B. An evaluation \nof distillersr’s machine learning-based prioritization tool for title/abstract \nscreening– impact on reviewer-relevant outcomes. BMC Med Res Methodol. \n2020;20(1):256.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 1 8 6  / s  1 2 8 7 4 - 0 2 0 - 0 1 1 2 9 - 1.\n6. van de Schoot R, de Bruin J, Schram R, et al. An open source machine learn-\ning framework for efficient and transparent systematic reviews. Nat Mach \nIntell. 2021;3(2):125–33.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 0 3 8  / s  4 2 2 5 6 - 0 2 0 - 0 0 2 8 7 - 7.\n7. Cheng Sh, Augustin C, Bethel A, et al. Using machine learning to advance \nsynthesis and use of conservation and environmental evidence. Conserv Biol. \n2018;32(4):762–4.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 1 1 1  / c  o b i . 1 3 1 1 7.\n8. Harrison H, Griffin SJ, Kuhn I, Usher-Smith JA. Software tools to support title \nand abstract screening for systematic reviews in healthcare: an evaluation. \nBMC Med Res Methodol. 2020;20(1):7.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 1 8 6  / s  1 2 8 7 4 - 0 2 0 - 0 8 \n9 7 - 3.\n9. Issaiy M, Ghanaati H, Kolahi S, et al. Methodological insights into ChatGPT’s \nscreening performance in systematic reviews. BMC Med Res Methodol. \n2024;24(1):78.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 1 8 6  / s  1 2 8 7 4 - 0 2 4 - 0 2 2 0 3 - 8.\n10. Guo E, Gupta M, Deng J, Park YJ, Paget M, Naugler C. Automated paper \nscreening for clinical reviews using large Language models: data analysis \nstudy. J Med Internet Res. 2024;26(1):e48996.  h t t p  s : /  / d o i  . o  r g /  1 0 .  2 1 9 6  / 4  8 9 9 6.\n11. Khraisha Q, Put S, Kappenberg J, Warraitch A, Hadfield K. Can large Language \nmodels replace humans in systematic reviews? Evaluating GPT -4’s efficacy \nin screening and extracting data from peer‐reviewed and grey literature in \nmultiple Languages. Res Synth Methods. 2024;15(4):616–26.  h t t p  s : /  / d o i  . o  r g /  1 \n0 .  1 0 0 2  / j  r s m . 1 7 1 5.\n12. Chakhtoura M, Nassar JE, Slim A et al. Vitamin D Supplementation and Falls \nin Adults: A Systematic Umbrella Review of Meta-Analyses of Randomized \nControlled Trials. In: Annual Meeting of the Endocrine Society, Boston, June 1–4, \n2024. SUN-268.\n13. Chapter 3: Defining the criteria for including studies and how they will be \ngrouped for the synthesis. Accessed May 23. 2024.  h t t p  s : /  / t r a  i n  i n g  . c o  c h r a  n e  . \no r  g / h  a n d b  o o  k / c  u r r  e n t /  c h  a p t e r - 0 3\n14. Valizadeh A, Moassefi M, Nakhostin-Ansari A, et al. Abstract screening using \nthe automated tool Rayyan: results of effectiveness in three diagnostic test \naccuracy systematic reviews. BMC Med Res Methodol. 2022;22(1):160.  h t t p  s : /  / \nd o i  . o  r g /  1 0 .  1 1 8 6  / s  1 2 8 7 4 - 0 2 2 - 0 1 6 3 1 - 8.\nPage 9 of 9\nTrad et al. BMC Medical Research Methodology           (2025) 25:130 \n15. Li J, Kabouji J, Bouhadoun S, et al. Sensitivity and specificity of alternative \nscreening methods for systematic reviews using text mining tools. J Clin \nEpidemiol. 2023;162:72–80.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 0 1 6  / j  . j c  l i n  e p i .  2 0  2 3 . 0 7 . 0 1 0.\n16. Dos Reis AHS, De Oliveira ALM, Fritsch C, Zouch J, Ferreira P , Polese JC. Useful-\nness of machine learning softwares to screen titles of systematic reviews: a \nmethodological study. Syst Rev. 2023;12(1):68.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 1 8 6  / s  1 3 6 4 \n3 - 0 2 3 - 0 2 2 3 1 - 3.\n17. Luo X, Chen F, Zhu D, et al. Potential roles of large Language models in the \nproduction of systematic reviews and Meta-Analyses. J Med Internet Res. \n2024;26:e56780.  h t t p  s : /  / d o i  . o  r g /  1 0 .  2 1 9 6  / 5  6 7 8 0.\n18. Reason T, Benbow E, Langham J, Gimblett A, Klijn SL, Malcolm B. Artificial \nintelligence to Automate network Meta-Analyses: four case studies to evalu-\nate the potential application of large Language models. PharmacoEconomics \n- Open. 2024;8(2):205–20.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 0 0 7  / s  4 1 6 6 9 - 0 2 4 - 0 0 4 7 6 - 9.\n19. Nashwan AJ, Jaradat JH. Streamlining systematic reviews: Harnessing large \nLanguage models for quality assessment and Risk-of-Bias evaluation. Cureus \nPublished Online August. 2023;6.  h t t p  s : /  / d o i  . o  r g /  1 0 .  7 7 5 9  / c  u r e u s . 4 3 0 2 3.\n20. Hasan B, Saadi S, Rajjoub NS et al. Integrating large Language models in \nsystematic reviews: a framework and case study using ROBINS-I for risk \nof bias assessment. BMJ Evid-Based Med. Published online February 21, \n2024:bmjebm–2023.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 1 3 6  / b  m j e b m - 2 0 2 3 - 1 1 2 5 9 7\n21. Barsby J, Hume S, Lemmey HA, Cutteridge J, Lee R, Bera KD. Pilot study on \nlarge Language models for risk-of-bias assessments in systematic reviews: \nA(I) new type of bias? BMJ Evid-Based med. Published online May 23, \n2024:bmjebm-2024-112990.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 1 3 6  / b  m j e b m - 2 0 2 4 - 1 1 2 9 9 0\n22. Huotala A, Kuutila M, Ralph P , Mäntylä M. The Promise and Challenges of \nUsing LLMs to Accelerate the Screening Process of Systematic Reviews. In: \nProceedings of the 28th International Conference on Evaluation and Assessment \nin Software Engineering. EASE ’24. Association for Computing Machinery; \n2024:262–271.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 1 4 5  / 3  6 6 1 1 6 7 . 3 6 6 1 1 7 2\n23. Matsui K, Utsumi T, Aoki Y, Maruki T, Takeshima M, Takaesu Y. Human-Com-\nparable sensitivity of large Language models in identifying eligible studies \nthrough title and abstract screening: 3-Layer strategy using GPT-3.5 and \nGPT-4 for systematic reviews. J Med Internet Res. 2024;26:e52758.  h t t p  s : /  / d o i  . \no  r g /  1 0 .  2 1 9 6  / 5  2 7 5 8.\n24. Li M, Sun J, Tan X. Evaluating the effectiveness of large Language models in \nabstract screening: a comparative analysis. Syst Rev. 2024;13(1):219.  h t t p  s : /  / d \no i  . o  r g /  1 0 .  1 1 8 6  / s  1 3 6 4 3 - 0 2 4 - 0 2 6 0 9 - x.\n25. Oami T, Okada Y, Nakada T, aki. Performance of a large Language model in \nscreening citations. JAMA Netw Open. 2024;7(7):e2420496.  h t t p s :   /  / d o  i .  o r  g  /  1 0  \n. 1 0   0 1  / j a  m a n  e t w o  r k  o  p e n . 2  0 2 4 . 2 0 4 9 6.\n26. Dennstädt F, Zink J, Putora PM, Hastings J, Cihoric N. Title and abstract screen-\ning for literature reviews using large Language models: an exploratory study \nin the biomedical domain. Syst Reviews. 2024;13(1):158.\nPublisher’s note\nSpringer Nature remains neutral with regard to jurisdictional claims in \npublished maps and institutional affiliations."
}