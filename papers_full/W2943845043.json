{
  "title": "Language Modeling with Deep Transformers",
  "url": "https://openalex.org/W2943845043",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2917519234",
      "name": "Irie, Kazuki",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2752946754",
      "name": "Zeyer Albert",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Schl\\\"uter, Ralf",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2747500719",
      "name": "Ney, Hermann",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3189410112",
      "name": "Schlüter, Ralf",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963362078",
    "https://openalex.org/W1689711448",
    "https://openalex.org/W2963386218",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2963382687",
    "https://openalex.org/W2799800213",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2911291251",
    "https://openalex.org/W2972630480",
    "https://openalex.org/W2890012642",
    "https://openalex.org/W2404974730",
    "https://openalex.org/W2963970792",
    "https://openalex.org/W2471933213",
    "https://openalex.org/W2904818793",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2413794162",
    "https://openalex.org/W2964089206",
    "https://openalex.org/W2613904329",
    "https://openalex.org/W1665214252",
    "https://openalex.org/W1494198834",
    "https://openalex.org/W3103005696",
    "https://openalex.org/W2100664567",
    "https://openalex.org/W2799923439",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963537482",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2302255633",
    "https://openalex.org/W2866343820",
    "https://openalex.org/W1915251500",
    "https://openalex.org/W2267186426",
    "https://openalex.org/W2462831000",
    "https://openalex.org/W2953384591",
    "https://openalex.org/W2567070169",
    "https://openalex.org/W2327501763",
    "https://openalex.org/W2597655663",
    "https://openalex.org/W2795138957",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2912492482",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2936774411",
    "https://openalex.org/W2964045208",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2402268235",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2402144811",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W2964265128",
    "https://openalex.org/W2926063217",
    "https://openalex.org/W2963045354",
    "https://openalex.org/W2963088785",
    "https://openalex.org/W2963631907"
  ],
  "abstract": "We explore deep autoregressive Transformer models in language modeling for speech recognition. We focus on two aspects. First, we revisit Transformer model configurations specifically for language modeling. We show that well configured Transformer models outperform our baseline models based on the shallow stack of LSTM recurrent neural network layers. We carry out experiments on the open-source LibriSpeech 960hr task, for both 200K vocabulary word-level and 10K byte-pair encoding subword-level language modeling. We apply our word-level models to conventional hybrid speech recognition by lattice rescoring, and the subword-level models to attention based encoder-decoder models by shallow fusion. Second, we show that deep Transformer language models do not require positional encoding. The positional encoding is an essential augmentation for the self-attention mechanism which is invariant to sequence ordering. However, in autoregressive setup, as is the case for language modeling, the amount of information increases along the position dimension, which is a positional signal by its own. The analysis of attention weights shows that deep autoregressive self-attention models can automatically make use of such positional information. We find that removing the positional encoding even slightly improves the performance of these models.",
  "full_text": "Language Modeling with Deep Transformers\nKazuki Irie1, Albert Zeyer1,2, Ralf Schl¨uter1, Hermann Ney1,2\n1Human Language Technology and Pattern Recognition Group, Computer Science Department\nRWTH Aachen University, 52074 Aachen, Germany\n2AppTek GmbH, 52062 Aachen, Germany\n{irie, zeyer, schlueter, ney}@cs.rwth-aachen.de\nAbstract\nWe explore deep autoregressive Transformer models in language\nmodeling for speech recognition. We focus on two aspects.\nFirst, we revisit Transformer model conﬁgurations speciﬁcally\nfor language modeling. We show that well conﬁgured Trans-\nformer models outperform our baseline models based on the\nshallow stack of LSTM recurrent neural network layers. We\ncarry out experiments on the open-source LibriSpeech 960hr\ntask, for both 200K vocabulary word-level and 10K byte-pair\nencoding subword-level language modeling. We apply our word-\nlevel models to conventional hybrid speech recognition by lat-\ntice rescoring, and the subword-level models to attention based\nencoder-decoder models by shallow fusion. Second, we show\nthat deep Transformer language models do not require positional\nencoding. The positional encoding is an essential augmentation\nfor the self-attention mechanism which is invariant to sequence\nordering. However, in autoregressive setup, as is the case for lan-\nguage modeling, the amount of information increases along the\nposition dimension, which is a positional signal by its own. The\nanalysis of attention weights shows that deep autoregressive self-\nattention models can automatically make use of such positional\ninformation. We ﬁnd that removing the positional encoding even\nslightly improves the performance of these models.\nIndex Terms: language modeling, self-attention, Transformer,\nspeech recognition\n1. Introduction\nTransformer encoder-decoder models [1] have become popular\nin natural language processing. The Transformer architecture\nallows to successfully train a deep stack of self-attention lay-\ners [2–4] via residual connections [5] and layer normalization [6].\nThe positional encodings [1, 7], typically based on sinusoidal\nfunctions, are used to provide the self-attention with the sequence\norder information. Across various applications, systematic im-\nprovements have been reported over the standard, multi-layer\nlong short-term memory (LSTM) [8] recurrent neural network\nbased models. While originally designed as an encoder-decoder\narchitecture in machine translation, the encoder (e.g., [9]) and\nthe decoder (e.g., [10]) components are also separately used\nin corresponding problems depending on whether the problem\ndisposes the whole sequence for prediction or not.\nA number of recent works have also shown impressive per-\nformance in language modeling using the Transformer decoder\ncomponent [10–15]. The earliest example can be found in [10]\nwhere such models are investigated for text generation. Re-\ncent works on training larger and deeper models [12, 14, 15]\nhave shown further potential of the Transformer in language\nmodeling. On the other hand, an obvious limitation of the Trans-\nformers is that their memory requirement linearly increases in\nterms of number of tokens in the sequence, which requires to\nwork with a limited context window (basically a n-gram model\nwhere the typical number for n is 512) for tasks dealing with\nlong sequences such as character-level language modeling [12].\nDai et al. [11] has introduced a segment-level recurrence and\nrelative positional encoding in the Transformer language model\nto be able to potentially handle unlimited context.\nIn this work, we investigate deep autoregressive Transform-\ners for language modeling in speech recognition. To be speciﬁc,\nwe focus on two aspects. First, we revisit the parameter conﬁgu-\nrations of Transformers, originally engineered for the sequence-\nto-sequence problem [1], speciﬁcally for language modeling. We\nconduct experiments on the LibriSpeech automatic speech recog-\nnition (ASR) task [16] for both word-level conventional speech\nrecognition and byte-pair encoding (BPE) [17] level end-to-end\nspeech recognition [18, 19]. We apply our word-level models to\nhybrid speech recognition by lattice rescoring [20], and the BPE-\nlevel models to end-to-end models by shallow fusion [21, 22].\nWe show that well conﬁgured Transformer language models out-\nperform models based on the simple stack of LSTM RNN layers\nin terms of both perplexity and word error rate (WER).\nSecond, we experimentally show that the positional encod-\ning is not needed for multi-layer autoregressive self-attention\nmodels. The visualization of the attention weights shows that\nwhen the sinusoidal positional encoding is provided with the\ninput, the ﬁrst layer of the Transformers learns to extract n-\ngram features (therefore making use of positional information).\nHowever, in the autoregressive problem where a new token is\nprovided to the model at each time step, the amount of infor-\nmation the model has access to strictly increases from left to\nright at the lowest level of the network, which should provide\nsome positional information by its own. We observe that deep\nTransformer language models without positional encoding au-\ntomatically make use of such information, and even give slight\nimprovements over models with positional encodings.\n2. Related Work\nThe ﬁrst part of our work follows the spirits of Al-Rfou et al.’s\nwork [12] and Radford et al.’s work [14,15] in investigating larger\nand deeper Transformers for language modeling. We show that\ndeep Transformer language models can be successfully applied\nto speech recognition and give good performance. The second\npart of this work concerns the positional encoding, which is a\ncrucial component in the original Transformer. A number of pre-\nvious work investigated positional encoding variants to improve\nself-attention (e.g., [11, 23–25]). Previous works in Transformer\nlanguage models systematically use positional encoding, either\njointly learned one or the sinusoidal one (both cases are reported\nto give similar performance in [12]). We show that the deep\nautoregressive self-attention models do not require any explicit\nmodel for encoding positions to give the best performance.\n3. Autoregressive Self-Attention\nThe language model we consider is based on the decoder com-\nponent of the Transformer architecture [1]. Similar to previous\nwork [10–15], we deﬁne layer as a stack of two components:\nself-attention and feed-forward1 modules.\n1Typically called position-wise feed-forward module [1]. Here we\nomit position-wise as it is obvious for autoregressive models.\narXiv:1905.04226v2  [cs.CL]  11 Jul 2019\nThe autoregressive self-attention module in the l-th layer\ntransforms the input z(l−1)\nt at position t as follows:\nx(l)\nt = LayerNorm( z(l−1)\nt )\nq(l)\nt , k(l)\nt , v(l)\nt = Qx(l)\nt , Kx(l)\nt , V x(l)\nt\nh(l)\nt =\n(\nh(l)\nt−1, (k(l)\nt , v(l)\nt )\n)\ny(l)\nt = z(l−1)\nt + W0 SelfAttention(h(l)\nt , q(l)\nt )\nwhere Q, K, V , respectively denote query, key, value pro-\njection matrices, LayerNorm denotes layer normalization [6],\nSelfAttention denotes the scaled multi-head dot product self-\nattention [1], and W0 denotes the projection matrix for the resid-\nual connection [5].\nThe output y(l)\nt is then fed to the feed-forward module:\nm(l)\nt = LayerNorm( y(l)\nt )\nz(l)\nt = y(l)\nt + W2 Activation(W1m(l)\nt )\nwhere Activation is rectiﬁer [26], Gaussian error linear unit\n(GELU) [15, 27], or gated linear unit (GLU) [28] in this work.\nThe ﬁnal model is build by stacking these layers multiple times.\nThe input of the network consists of the sum of the token em-\nbedding (word or BPE in this work) and the sinusoidalpositional\nencoding as speciﬁed in [1]. The output softmax layer gives the\nprobability distribution for the next token. As shown in the equa-\ntions above, h(l)\nt can be seen as states of the Transformer model2\n(whose size, as opposed to the RNN states, linearly grows along\nthe position dimension). During inference, these states are stored\nto avoid redundant computation. During training, the computa-\ntion along the position dimension is parallelized for speed-up.\n4. LibriSpeech Dataset\n4.1. Language Modeling Data Descriptions\nThe LibriSpeech datasets [16] for language modeling consists\nof 800M-word text only data and 960hr of audio transcriptions\nwhich corresponds to 10M-word text data. Based on analysis of\ncount model perplexities, we observe that the audio transcription\npart does not contain special domain signal which matches the\ndevelopment set. Therefore, we simply merge the two datasets to\nform a single dataset for language model training. The average\nsentence length in the resulting training data is 21 words with\nthe maximum length of 600 words. The development and test\nsets respectively have two parts [16]: dev-clean, dev-other, test-\nclean, and test-other. This separation is based on the audio-level\ncharacteristics, therefore it has no special meaning for language\nmodeling. In the experimental section, we denote by ” Dev”\nand ”Test” the concatenation of clean and other parts of the\nrespective data. Both datasets consist of about 110K running\nwords with average of 20 words per sentence. The word-level\nvocabulary contains 200K words. We report all perplexities\nwithout making use of contexts beyond the sentence boundary.\n4.2. 4-gram count and LSTM-RNN Baselines\nWe use the ofﬁcial 4-gram count language model provided with\nthe LibriSpeech dataset [16]. No improvement in perplexity is\nobserved when going up to 5-grams. For LSTM-RNN language\nmodels [29], we ﬁrst train our base conﬁguration; the model has\n2 LSTM-RNN layers with 2048 nodes and the input projection\nlayer of 128, where the dropout with a rate of 0.2 is applied\nbetween each layer. Since we observe that this model underﬁts\n2In principle, we could also consider an autoregressive self-attention\nmodel which updates states at all predecessor positions for each new\ninput, which would be then much more computationally inefﬁcient.\nthe LibriSpeech training set, we remove the dropout and further\nincrease the model size, which effectively give better perplexities\nas shown in Table 1. We ﬁnd that improvements from simply\nstacking layers saturate at 4 layers even without overﬁtting. In-\ntroducing a small linear bottleneck layer (size 512 here) before\nthe output layer can make the models compact but with a loss in\nperformance. The best model we obtain has 2 layers with 4096\nnodes. Relative improvements greater than 58% are obtained by\nthe LSTM over the 4-gram language model.\nTable 1: Perplexities of the baseline models.\nModel Drop- Bottle- Num. Num. Params Dev Testout beck units layers in M\n4-gram - - - - 230 146.2 151.8\nLSTM\n0.2\nNone 2048\n2 487 71.3 74.8\n0.0\n66.6 69.9\n3 520 64.0 67.2\n4 554 61.9 64.9\n5 587 62.7 65.9\n6 621 64.5 67.5\n8 688 67.2 70.3\n4096 2 1048 60.2 63.2\n512 334 63.1 66.3\n2048 4 248 64.5 67.7\n5. Text based Experiments\nWe carry out experiments for both word-level and BPE-level\nlanguage modeling. We ﬁrst focus on the word-level one.\n5.1. Hyper-parameters in Transformers\nThe Transformer architecture is a new search space Odyssey\n[30]. The exhaustive model hyper-parameters for Transformer\nlanguage models speciﬁed by the equations in Sec. 3 are the input\ntoken embedding size, the number of layers, the dimension of the\nresidual connection, and for each layer the number of attention\nheads, the dimension of the key and query, the dimension of the\nvalue, and the dimension of the feed-forward layer.\nIn our experiments, we use the same dimension for key,\nquery and value, as well as the residual connection. We use the\nsame dimensionality across all layers. Therefore, our models\ncan be fully speciﬁed by the tuple ( number of layersL, feed-\nforward dimensiondff , residual dimensiondres, number of\nheads H). We do not apply any regularization method including\ndropout. We train all models using the plain stochastic gradient\ndescent and new-bob learning rate tuning on a single GPU. We\ndeﬁne our training sub-epoch (for new-bob) as the 10th of the\nfull training data. All our implementations are based on the\nTensorﬂow [31] based open-source toolkit RETURNN [32]3.\n5.2. Hyper-parameter Tuning\nGiven the amount of LibriSpeech training data (810M words), it\nis unreasonable to train all model variants until full convergence.\nThe earlier stage of the training already consistently indicates the\npotential performance of the models. Therefore, we ﬁrst carry\nout comparisons between models with different conﬁguration at\nthe equal, large enough, but reasonable number of updates.\nThe ﬁrst set of comparison investigates the effect of depth\nand width. The perplexity results can be found in Table 2. All\nmodels in the table use 8 attention heads. Other parameters are\nspeciﬁed in the table. The table is organized in three parts. The\nupper part of Table 2 shows the effect of number of layers; we\nobserve that increasing number of layers (therefore the number\nof parameters) from 1 to 42 gradually improves the perplexity.\nIn the middle part of Table 2 , we vary both the number of layers,\n3Training conﬁguration ﬁles and trained models are available\nat https://github.com/rwth-i6/returnn-experiments/tree/\nmaster/2019-lm-transformers.\nfeed-forward dimension, and the residual dimension. First of all,\nthe 12-layer (12, 4096, 512, 8) model outperforms the 6-layer\n(6, 8192, 512, 8) model, while having similar number of param-\neters, which seems to indicate that the depth effectively beneﬁts\nTransformer language models. We also train an extreme model\nwhich has only 2 layers with wide dimensions(2, 8192, 2048, 8).\nThe number of parameters in fact blows up because of the large\nvalue of dres which results in a large matrix in the output soft-\nmax layer with 200K vocabulary4. We observe that such wide\nbut shallow models do not perform well5. Finally, the lower part\nof Table 2 shows deeper models with a smaller input dimension.\nTable 2: Perplexity after 2.5 epoch(25 sub-epochs in our setup;\n6.5M updates). The number of headsH is 8 for all models below.\nInput L dff dres\nParams. Perplexity\nemb. in M Train Dev\n512\n1\n2048 512\n208 108.3 104.9\n6 224 75.7 74.3\n12 243 67.6 67.1\n24 281 62.2 62.3\n32 306 60.1 60.6\n42 338 59.0 59.6\n512\n2 8192 2048 536 73.1 73.8\n6\n512\n262 66.7 66.7\n12 4096 268 63.5 63.8\n4 16384 277 67.6 67.4\n32768 344 65.4 68.4\n128\n64\n2048 512\n330 56.3 57.6\n80 380 53.1 55.5\n96 431 51.9 54.9\n112 481 51.5 54.5\nTable 3 shows the effect of number of attention heads. 16\nheads which is the largest number we try in this setup give the\nbest performance. In addition, we examine the type of activation\nfunction (Table 4). As opposed to previous work on feed-forward\nlanguage models using GLUs [28, 34], we do not observe faster\nconvergence. As we observe that the impact of choice of activa-\ntion functions on the perplexity is overall limited, all our other\nmodels use the standard ReLU. As reported in the original Trans-\nformer, we conﬁrm that both layer normalization and residual\nconnections are needed for these models for stable training6.\nFinally, we train models with the best conﬁgurations for\nlonger. Table 5 shows the perplexities which are better than\nthose obtained by our LSTM based models (Table 1).\nTable 3: Effect of number\nof heads. Perplexity after 2.5\nepoch for (12, 2048, 512, H).\nH Params. Perplexity\nin M Train Dev\n1\n243\n71.9 70.8\n4 69.1 68.6\n8 67.6 67.1\n16 66.9 66.6\nTable 4: Effect of activation\nfunctions. Perplexity after 1\nepoch (10 sub-epochs in our\nsetup) for (24, 2048, 512, 8).\nActivation Perplexity\nTrain Dev\nReLU [1, 26] 76.4 72.5\nGLU [28] 76.5 72.8\nGELU [15, 27] 75.7 72.2\n4 We note that this is also the reason why the number of parameters\nof our baseline LSTM language models in Table 1 is relatively high.\n5Since the softmax bottleneck dimension typically needs to be large\nfor the best performance ( [33]; Table 1), we also train a (12, 2048, 512,\n8) model where we insert an additional projection layer with a large\ndimension (2048) before the output layer; no improvement was obtained.\n6We tried to train multiple models without either residual connections\nor layer normalization. Also, following [15], we tried reorganizing\nthe feed-forward module to insert one additional pre-activation layer\nnormalization [35] and one more activation function. However, we did\nnot observe any improvement. The original Transformers anyway do not\nhave any activation on the residual path throughout the whole network.\nTable 5: Perplexities after longer training.\nMax. Conv- L dff dres\nParams. Perplexity\nEpoch erged in M Train Dev Test\n5.5\nYes\n12 4096\n512\n268 57.3 59.9 62.3\n5\n24\n2048\n281 55.6 58.0 60.7\n32 306 53.4 56.6 59.5\n42 338 51.2 55.0 57.7\n3 No 80 2048 512 380 51.9 54.3 56.9\n96 431 50.9 53.7 56.3\n5.3. Parameter Tying\nDehghani et al. [36] reports Universal Transformers to perform\nparticularly well for language modeling. This motivates us to ex-\nperiment with parameter sharing across layers. For such models\nto have comparable number of parameters with the standard deep\nTransformers, the dimensions in each layer must be increased,\nwhich results in slower training; here we simply investigate the\neffect of number of recurrence. Table 6 shows the perplexity\nresults. First of all, we observe that the model performance is\nbehind that of the standard Transformer7 (Table 2). However, we\nclearly observe that increasing the number of layers from 3 to\n12 consistently improves the perplexity. This improvement with-\nout additional parameters motivates future work to investigate\nfurther parameter sharing strategies for Transformers.\nTable 6: Perplexity after 2.5 epochfor (L, 8192, 1024, 16)\nmodels with shared parameters across all layers.\nL Params. Perplexity\nin M Train Dev\n3\n329\n82.6 79.9\n6 76.7 74.6\n12 74.2 72.1\n6. ASR Experiments\n6.1. Lattice Rescoring Results\nWe apply our word-level Transformer language models to con-\nventional hybrid speech recognition by lattice rescoring. The\nstandard push-forward lattice rescoring algorithm [20] for long-\nspan language models can be directly applied to self-attention\nbased models. The only modiﬁcations from the RNN version\nis to deﬁne the ”state” as all hidden states (h(l)\nt in Sec.3) in all\nlayers from all predecessor positions and the current position (t;\nfor position encoding). Table 7 shows the WERs and perplexi-\nties (PPL). Our baseline acoustic model is based on multi-layer\nbi-directional LSTM [37]. Further descriptions of our baseline\nacoustic model can be found in [38]. We obtain consistent im-\nprovements in terms of WER over the LSTM baselines.\nTable 7: WERs (%) for hybrid systems on the LibriSpeech 960hr.\n4-gram model is used in the ﬁrst pass to generates lattices for\nrescoring. The row ”Lattice” shows oracle WERs of the lattices.\nLM L Para. dev test\nin M clean other clean other\nPPL WER PPL WER PPL WER PPL WER\n4-gram - 230 151.7 3.4 140.6 8.3 158.1 3.8 145.7 8.8\nLattice - - - 1.0 - 2.3 - 1.3 - 2.6\nLSTM 2 1048 60.2 2.3 60.2 5.4 64.8 2.6 61.7 5.9\nTrans- 24 281 57.8 2.2 58.3 5.2 62.2 2.5 59.4 5.7\nformer 42 338 54.5 2.1 55.5 5.2 59.1 2.5 56.4 5.7\n96 431 53.2 2.1 54.2 5.2 57.6 2.5 55.0 5.6\n7We note that here the direct comparison is not as straightforward as\nbetween the standard Transformers. In fact, we observe that the training\nhyperparameters tuned for the standard Transformers can not be directly\napplied to Universal Transformers; speciﬁcally, we ﬁnd it crucial to\nreduce the gradient norm clipping threshold from 1 to 0.1, which is\npotentially slowing down the convergence.\n<bos>\nso\nthey\nwent\non\nto\nthe\nverandah\nand\nlooked\ndown\nupon\nthe\nlights\nof\nthe\nprison\nand\nlistened\nto\nthe\nsea\nlapping\nthe\nshore\nso\nthey\nwent\non\nto\nthe\nverandah\nand\nlooked\ndown\nupon\nthe\nlights\nof\nthe\nprison\nand\nlistened\nto\nthe\nsea\nlapping\nthe\nshore\n<eos>\n(a) First layer with PE\n<bos>\nso\nthey\nwent\non\nto\nthe\nverandah\nand\nlooked\ndown\nupon\nthe\nlights\nof\nthe\nprison\nand\nlistened\nto\nthe\nsea\nlapping\nthe\nshore\nso\nthey\nwent\non\nto\nthe\nverandah\nand\nlooked\ndown\nupon\nthe\nlights\nof\nthe\nprison\nand\nlistened\nto\nthe\nsea\nlapping\nthe\nshore\n<eos> (b) First layer without PE\n<bos>\nso\nthey\nwent\non\nto\nthe\nverandah\nand\nlooked\ndown\nupon\nthe\nlights\nof\nthe\nprison\nand\nlistened\nto\nthe\nsea\nlapping\nthe\nshore\nso\nthey\nwent\non\nto\nthe\nverandah\nand\nlooked\ndown\nupon\nthe\nlights\nof\nthe\nprison\nand\nlistened\nto\nthe\nsea\nlapping\nthe\nshore\n<eos> (c) ”Blur” layer\n<bos>\nso\nthey\nwent\non\nto\nthe\nverandah\nand\nlooked\ndown\nupon\nthe\nlights\nof\nthe\nprison\nand\nlistened\nto\nthe\nsea\nlapping\nthe\nshore\nso\nthey\nwent\non\nto\nthe\nverandah\nand\nlooked\ndown\nupon\nthe\nlights\nof\nthe\nprison\nand\nlistened\nto\nthe\nsea\nlapping\nthe\nshore\n<eos> (d) ”Window” layer\n<bos>\nso\nthey\nwent\non\nto\nthe\nverandah\nand\nlooked\ndown\nupon\nthe\nlights\nof\nthe\nprison\nand\nlistened\nto\nthe\nsea\nlapping\nthe\nshore\nso\nthey\nwent\non\nto\nthe\nverandah\nand\nlooked\ndown\nupon\nthe\nlights\nof\nthe\nprison\nand\nlistened\nto\nthe\nsea\nlapping\nthe\nshore\n<eos> (e) ”Structured” layer\nFigure 1: Layer categories in word-level 24-layer Transformer language models. The x-axis corresponds to the input words. The y-axis\nshows the target words; each target word position has 8 sub-rows corresponding to 8 heads. ”PE” denotes positional encoding.\n6.2. End-to-End ASR Shallow Fusion Results\nWe train 10K BPE-level Transformer language models to be\ncombined with an attention-based encoder-decoder speech model\nby shallow fusion [21,22]. The 10K BPE level training data has a\nlonger average length of 24 tokens per sentence with the longest\nsentence length of 1343, which is still manageable without any\ntruncation for self-attention. We use the Transformer architecture\nof (24, 4096, 1024, 8). The LSTM model has 4 layers with 2048\nnodes. We refer to our previous work [19] for the description\nof the baseline attention model; the baseline WERs better than\nour previous work [19] are obtained by improved curriculum\nlearning and longer training. Table 8 shows both perplexities and\nWERs. Following [39], we introduce an end-of-sentence penalty\nin shallow fusion to beneﬁt from a large beam size of 64. Again,\nwe obtain consistent improvements over the LSTM baseline.\nThese results are better than previously reported WERs [39–41]\nfor end-to-end models without data augmentation [42].\nTable 8: WERs (%) for attention-based modelson LibriSpeech\n960hr dataset. Perplexities are on the 10K BPE level.\nLM\nBeam\ndev test\nclean other clean other\nPPL WER PPL WER PPL WER PPL WER\nNone 12 - 4.3 - 12.9 - 4.4 - 13.5\nLSTM 64 43.7 2.9 46.4 8.9 47.1 3.2 47.2 9.9\nTransfo. 35.9 2.6 38.9 8.4 38.8 2.8 39.0 9.3\n7. Analysis\nCompared with hidden states in RNNs, attention weights are\neasier to be visualized, which gives opportunity for analysis.\nIn particular, we focus on the comparison of the Transformer\nlanguage models with and without positional encoding.\n7.1. Transformer LM without positional encoding\nIn the autoregressive problem where a new token is provided\nto the model at each time step, the amount of information the\nmodel has access to strictly increases from left to right at the\nlowest level of the network; the deeper layers should be able to\nrecognize this structure which should provide the model with\nsome positional information by its own. To check this hypothesis,\nwe train models without any positional encoding. First, we\nobserve that they give better perplexities than the models with\nsinusoidal positional encoding (Table 9).\n7.2. First layer\nThe attention in the ﬁrst layer is the most straightforward for\ninterpretation because the feature at each position exactly cor-\nresponds to the word at the position (while deeper layers can\npotentially shufﬂe the feature content). The attention weights\nin the ﬁrst layer of 24-layer Transformer language models with\nand without positional encodings are visualized in Figure 1. We\nobserve that the ﬁrst layer of the model with positional encoding\n(Figure 1(a)) learns to create n-gram features (roughly 2 or 3-\ngram), which indicates that the positional information is directly\nTable 9: Effect of sinusoidal positional encoding. Perplexity\nafter 5 epochs(13M updates) for (L, 2048, 512, 8) models.\nL Position. Params. Perplexity\nencoding in M. Train Dev Test\n12 Sinusoidal 243 61.8 63.1 66.1\nNone 58.0 60.5 63.4\n24 Sinusoidal 281 55.6 58.0 60.8\nNone 52.7 56.6 59.2\n42 Sinusoidal 338 51.2 55.0 57.7\nNone 50.5 54.2 56.8\nused. In contrast, the ﬁrst layer of the model without positional\nencoding learns to focus on the new input token as can be seen\nas the diagonal in Figure 1(b) (interestingly, we also see that it\nignores some functional words such as ”the”, ”and”, ”to” which\nmight be modeled by some off-set values, therefore attending to\nthe beginning of sentence token instead), which demonstrates\nthat the model is aware of the position of the new input.\n7.3. Other layers\nWe observe that the behavior of other layers are rather similar for\nboth Transformer models with and without positional encoding.\nWe ﬁnd 3 categories of layers in the other 23 layers; the second\nand third layers are ”blur” layers as shown in Figure 1(c), which\nseems to roughly average over all positions (while we can also\nsee that some heads focus on difﬁcult words, here ”verandah”).\nLayer 4 to 9 are window layers which focus on the local n-gram.\nA representative example is show in Figure 1(d). Finally, we\nﬁnd the top layers 10 to 24 to be more structured, attending to\nsome speciﬁc patterns; an example is shown in Figure 1(e).\n8. Conclusion\nWe apply deep Transformer language models for speech recogni-\ntion. We show that such models outperform the shallow stack of\nLSTM-RNNs on both word-level and BPE-level modeling. Fu-\nture work investigates application of crucial components of deep\nTransformers (such as layer normalization) to deeper LSTM\nmodels; e.g., the RNMT+ decoder architecture [43] for language\nmodeling. Furthermore, we do not apply any regularization on\nmodels for the LibriSpeech task, as no overﬁtting is observed in\nthe range of model sizes we experimented with (for the word-\nlevel models). We can possibly still improve our models simply\nby scaling up their size and using regularization.\nAcknowledgements\nThis work has received funding from the European Research Council\n(ERC) under the European Union’s Horizon 2020 research and inno-\nvation programme (grant agreement No 694537, project ”SEQCLAS”)\nand from a Google Focused Award. The work reﬂects only the authors’\nviews and none of the funding parties is responsible for any use that may\nbe made of the information it contains. We thanks Liuhui Deng for con-\ntributing to our lattice rescoring code, Arne Nix and Julian Schamper for\nsharing their base Transformer conﬁgs, as well as Eugen Beck, Christoph\nL¨uscher and Wei Zhou for help with generating lattices. Experiments\nwere partially performed with computing resources granted by RWTH\nAachen University under project nova0003.\nReferences\n[1] A. V aswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in\nProc. NIPS, Long Beach, CA, USA, Dec. 2017, pp. 5998–6008.\n[2] J. Cheng, L. Dong, and M. Lapata, “Long short-term memory-\nnetworks for machine reading,” inProc. EMNLP, Austin, TX, USA,\nNov. 2016, pp. 551–561.\n[3] Z. Lin, M. Feng, C. N. d. Santos, M. Y u, B. Xiang, B. Zhou, and\nY . Bengio, “A structured self-attentive sentence embedding,”Int.\nConf. on Learning Representations (ICLR), Apr. 2017.\n[4] A. P . Parikh, O. T¨ackstr¨om, D. Das, and J. Uszkoreit, “A decom-\nposable attention model for natural language inference,” in Proc.\nEMNLP, Austin, TX, USA, Nov. 2016, pp. 2249–2255.\n[5] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for\nimage recognition,” inIEEE Conf. on Computer Vision and Patt.\nRecog. (CVPR), Las V egas, NV, USA, Jun. 2016, pp. 770–778.\n[6] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,”arXiv\npreprint arXiv:1607.06450, 2016.\n[7] J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y . N. Dauphin, “Con-\nvolutional sequence to sequence learning,” inProc. ICML, Sydney,\nAustralia, Aug. 2017, pp. 1243–1252.\n[8] S. Hochreiter and J. Schmidhuber, “Long short-term memory,”Neural\ncomputation, vol. 9, no. 8, pp. 1735–1780, 1997.\n[9] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-\ntraining of deep bidirectional transformers for language understand-\ning,” inProc. NAACL, Minneapolis, USA, Jun. 2019.\n[10] P . J. Liu, M. Saleh, E. Pot, B. Goodrich, R. Sepassi,Ł. Kaiser, and\nN. Shazeer, “Generating wikipedia by summarizing long sequences,”\nin ICLR, V ancouver, Canada, Apr. 2018.\n[11] Z. Dai, Z. Yang, Y . Yang, W. W. Cohen, J. Carbonell, Q. V . Le,\nand R. Salakhutdinov, “Transformer-XL: Attentive language models\nbeyond a ﬁxed-length context,” inACL, Florence, Italy, Jul. 2019.\n[12] R. Al-Rfou, D. Choe, N. Constant, M. Guo, and L. Jones, “Character-\nlevel language modeling with deeper self-attention,” inProc. AAAI\nConf. on Artif. Int., Honolulu, HI, USA, Jan. 2019.\n[13] A. Baevski and M. Auli, “Adaptive input representations for neural\nlanguage modeling,” inICLR, New Orleans, LA, USA, May 2019.\n[14] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, “Improv-\ning language understanding by generative pre-training,” [Online]. :\nhttps://blog.openai.com/language-unsupervised/, 2018.\n[15] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,\n“Language models are unsupervised multitask learners,” [Online]. :\nhttps://blog.openai.com/better-language-models/, 2019.\n[16] V . Panayotov, G. Chen, D. Povey, and S. Khudanpur, “LibriSpeech:\nan ASR corpus based on public domain audio books,” inICASSP,\nSouth Brisbane, Queensland, Australia, Apr. 2015, pp. 5206–5210.\n[17] R. Sennrich, B. Haddow, and A. Birch, “Neural machine translation\nof rare words with subword units,” inProc. ACL, Berlin, Germany,\nAugust 2016, pp. 1715–1725.\n[18] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, “Listen, attend and spell:\na neural network for large vocabulary conversational speech recogni-\ntion,” inProc. ICASSP, Shanghai, China, Mar. 2016, pp. 4960–4964.\n[19] A. Zeyer, K. Irie, R. Schl¨uter, and H. Ney, “Improved training of end-\nto-end attention models for speech recognition,” inProc. Interspeech,\nHyderabad, India, Sep. 2018, pp. 7–11.\n[20] M. Sundermeyer, Z. T¨uske, R. Schl¨uter, and H. Ney, “Lattice decod-\ning and rescoring with long-span neural network language models,”\nin Interspeech, Singapore, Sep. 2014, pp. 661–665.\n[21] C ¸. G ¨ulc ¸ehre, O. Firat, K. Xu, K. Cho, L. Barrault, H.-C. Lin,\nF. Bougares, H. Schwenk, and Y . Bengio, “On using monolingual cor-\npora in neural machine translation,”Computer Speech & Language,\nvol. 45, pp. 137–148, Sep. 2017.\n[22] S. Toshniwal, A. Kannan, C.-C. Chiu, Y . Wu, T. N. Sainath, and\nK. Livescu, “A comparison of techniques for language model integra-\ntion in encoder-decoder speech recognition,” inProc. SLT, Athens,\nGreece, Dec. 2018.\n[23] P . Shaw, J. Uszkoreit, and A. V aswani, “Self-attention with relative\nposition representations,” inProc. NAACL, New Orleans, LA, USA,\nJun. 2018, pp. 464–468.\n[24] M. Sperber, J. Niehues, G. Neubig, S. St¨uker, and A. Waibel, “Self-\nattentional acoustic models,” inProc. Interspeech, Hyderabad, India,\nSep. 2018, pp. 3723–3727.\n[25] J. Salazar, K. Kirchhoff, and Z. Huang, “Self-attention networks for\nconnectionist temporal classiﬁcation in speech recognition,” inProc.\nICASSP, Brighton, UK, May 2019, pp. 7115–7119.\n[26] V . Nair and G. E. Hinton, “Rectiﬁed linear units improve restricted\nBoltzmann machines,” inProc. Int. Conf. on Machine Learning\n(ICML), Haifa, Israel, Jun. 2010, pp. 807–814.\n[27] D. Hendrycks and K. Gimpel, “Gaussian error linear units (GELUs),”\narXiv preprint arXiv:1606.08415, 2018.\n[28] Y . N. Dauphin, A. Fan, M. Auli, and D. Grangier, “Language mod-\neling with gated convolutional networks,” inProc. ICML, Sydney,\nAustralia, Aug. 2017, pp. 933–941.\n[29] M. Sundermeyer, R. Schl¨uter, and H. Ney, “LSTM neural networks\nfor language modeling.” inProc. Interspeech, Portland, OR, USA,\nSep. 2012, pp. 194–197.\n[30] K. Greff, R. K. Srivastava, J. Koutn ´ık, B. R. Steunebrink, and\nJ. Schmidhuber, “LSTM: A search space odyssey,” IEEE Trans.\nNeural Netw. Learn. Syst., vol. 28, no. 10, pp. 2222–2232, 2017.\n[31] M. Abadi et al., “Tensorﬂow: A system for large-scale machine\nlearning,” inProc. USENIX Sympo. on Operating Sys. Design and\nImpl. (OSDI 16), Savannah, GA, USA, Nov. 2016, pp. 265–283.\n[32] A. Zeyer, T. Alkhouli, and H. Ney, “RETURNN as a generic ﬂexible\nneural toolkit with application to translation and speech recognition,”\nin Proc. ACL, Melbourne, Australia, Jul. 2018.\n[33] Z. Yang, Z. Dai, R. Salakhutdinov, and W. W. Cohen, “Breaking the\nsoftmax bottleneck: A high-rank RNN language model,” inICLR,\nV ancouver, Canada, Apr. 2018.\n[34] K. Irie, Z. Lei, R. Schl¨uter, and H. Ney, “Prediction of LSTM-RNN\nfull context states as a subtask for N-gram feedforward language\nmodels,” inICASSP, Calgary, Canada, Apr. 2018, pp. 6104–6108.\n[35] K. He, X. Zhang, S. Ren, and J. Sun, “Identity mappings in deep\nresidual networks,” inProc. European Conf. on Computer Vision\n(ECCV), Amsterdam, Netherlands, Oct. 2016, pp. 630–645.\n[36] M. Dehghani, S. Gouws, O. Vinyals, J. Uszkoreit, and Ł. Kaiser,\n“Universal Transformers,” inInt. Conf. on Learning Representations\n(ICLR), New Orleans, LA, USA, May 2019.\n[37] A. Zeyer, P . Doetsch, P . V oigtlaender, R. Schl¨uter, and H. Ney, “A\ncomprehensive study of deep bidirectional lstm rnns for acoustic\nmodeling in speech recognition,” inProc. ICASSP, New Orleans,\nLA, USA, Mar. 2017, pp. 2462–2466.\n[38] C. L ¨uscher, E. Beck, K. Irie, M. Kitza, W. Michel, A. Zeyer,\nR. Schl¨uter, and H. Ney, “RWTH ASR systems for LibriSpeech:\nHybrid vs Attention,” inInterspeech, Graz, Austria, Sep. 2019.\n[39] A. Hannun, A. Lee, Q. Xu, and R. Collobert, “Sequence-to-sequence\nspeech recognition with time-depth separable convolutions,”arXiv\npreprint arXiv:1904.02619, 2019.\n[40] N. Zeghidour, Q. Xu, V . Liptchinsky, N. Usunier, G. Synnaeve, and\nR. Collobert, “Fully convolutional speech recognition,”arXiv preprint\narXiv:1812.06864, 2018.\n[41] K. Irie, R. Prabhavalkar, A. Kannan, A. Bruguier, D. Rybach, and\nP . Nguyen, “Model unit exploration for sequence-to-sequence speech\nrecognition,”preprint arXiv:1902.01955, 2019.\n[42] D. S. Park, W. Chan, Y . Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk,\nand Q. V . Le, “SpecAugment: A simple data augmentation method\nfor automatic speech recognition,”arXiv preprint arXiv:1904.08779,\n2019.\n[43] M. X. Chen, O. Firat, A. Bapna, M. Johnson, W. Macherey, G. Foster,\nL. Jones, M. Schuster, N. Shazeer, N. Parmar, A. V aswani, J. Uszko-\nreit, L. Kaiser, Z. Chen, Y . Wu, and M. Hughes, “The best of both\nworlds: Combining recent advances in neural machine translation,”\nin ACL, Melbourne, Australia, Jul. 2018, pp. 76–86.",
  "topic": "Language model",
  "concepts": [
    {
      "name": "Language model",
      "score": 0.8360723257064819
    },
    {
      "name": "Computer science",
      "score": 0.8338842391967773
    },
    {
      "name": "Transformer",
      "score": 0.6915774941444397
    },
    {
      "name": "Autoregressive model",
      "score": 0.614115834236145
    },
    {
      "name": "Encoder",
      "score": 0.5661169290542603
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5513241291046143
    },
    {
      "name": "Vocabulary",
      "score": 0.5425208210945129
    },
    {
      "name": "Speech recognition",
      "score": 0.5201473832130432
    },
    {
      "name": "Recurrent neural network",
      "score": 0.4321029484272003
    },
    {
      "name": "Artificial neural network",
      "score": 0.36110180616378784
    },
    {
      "name": "Natural language processing",
      "score": 0.33758074045181274
    },
    {
      "name": "Linguistics",
      "score": 0.07444953918457031
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Econometrics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I887968799",
      "name": "RWTH Aachen University",
      "country": "DE"
    }
  ],
  "cited_by": 174
}