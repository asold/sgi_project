{
  "title": "Sample Efficient Text Summarization Using a Single Pre-Trained Transformer",
  "url": "https://openalex.org/W2945886944",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4288544679",
      "name": "Khandelwal, Urvashi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2317270646",
      "name": "Clark, Kevin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223899260",
      "name": "Jurafsky, Dan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4286928679",
      "name": "Kaiser, Lukasz",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2555428947",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2889688390",
    "https://openalex.org/W2889518897",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2170973209",
    "https://openalex.org/W2963045354",
    "https://openalex.org/W2962944953",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2963385935",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W1915251500",
    "https://openalex.org/W2962985882",
    "https://openalex.org/W2949433733"
  ],
  "abstract": "Language model (LM) pre-training has resulted in impressive performance and sample efficiency on a variety of language understanding tasks. However, it remains unclear how to best use pre-trained LMs for generation tasks such as abstractive summarization, particularly to enhance sample efficiency. In these sequence-to-sequence settings, prior work has experimented with loading pre-trained weights into the encoder and/or decoder networks, but used non-pre-trained encoder-decoder attention weights. We instead use a pre-trained decoder-only network, where the same Transformer LM both encodes the source and generates the summary. This ensures that all parameters in the network, including those governing attention over source states, have been pre-trained before the fine-tuning step. Experiments on the CNN/Daily Mail dataset show that our pre-trained Transformer LM substantially improves over pre-trained Transformer encoder-decoder networks in limited-data settings. For instance, it achieves 13.1 ROUGE-2 using only 1% of the training data (~3000 examples), while pre-trained encoder-decoder models score 2.3 ROUGE-2.",
  "full_text": "Sample Efﬁcient Text Summarization Using a Single Pre-Trained\nTransformer\nUrvashi Khandelwal†, Kevin Clark†, Dan Jurafsky†, Łukasz Kaiser ‡\n†Computer Science Department, Stanford University\n‡Google Brain\n{urvashik,kevclark,jurafsky}@stanford.edu\nlukaszkaiser@google.com\nAbstract\nLanguage model (LM) pre-training has re-\nsulted in impressive performance and sample\nefﬁciency on a variety of language understand-\ning tasks. However, it remains unclear how\nto best use pre-trained LMs for generation\ntasks such as abstractive summarization, par-\nticularly to enhance sample efﬁciency. In these\nsequence-to-sequence settings, prior work has\nexperimented with loading pre-trained weights\ninto the encoder and/or decoder networks, but\nused non-pre-trained encoder-decoder atten-\ntion weights. We instead use a pre-trained\ndecoder-only network, where the same Trans-\nformer LM both encodes the source and gen-\nerates the summary. This ensures that all pa-\nrameters in the network, including those gov-\nerning attention over source states, have been\npre-trained before the ﬁne-tuning step. Exper-\niments on the CNN/Daily Mail dataset show\nthat our pre-trained Transformer LM substan-\ntially improves over pre-trained Transformer\nencoder-decoder networks in limited-data set-\ntings. For instance, it achieves 13.1 ROUGE-\n2 using only 1% of the training data ( ∼3000\nexamples), while pre-trained encoder-decoder\nmodels score 2.3 ROUGE-2.\n1 Introduction\nLanguage model (LM) pre-training has led to im-\npressive results on tasks ranging from text classi-\nﬁcation to sequence tagging to question answer-\ning (Dai and Le, 2015; Peters et al., 2018; Radford\net al., 2018; Devlin et al., 2018). Particularly strik-\ning is the improved sample efﬁciency achieved by\nthese models (Howard and Ruder, 2018). How-\never, it remains unclear how to best utilize pre-\ntrained LMs for generation tasks such as text sum-\nmarization, and how much sample efﬁciency gains\nwould still apply given that generation models of-\nten require large datasets to perform well.\nWhile prior work has explored improving\n1 2 5 10 20 50 100\nPercent of CNN/Daily Mail Training Set Provided (Log Scale)\n0\n5\n10\n15\n20ROUGE-2\nSample Efficiency\nEncoder-Decoder\nEncoder-Decoder + Pretraining\nTransformer LM\nTransformer LM + Pretraining\nFigure 1: A pre-trained Transformer LM is extremely\nsample efﬁcient, outperforming baselines by over 10\nROUGE-2 points, when ﬁne-tuned on only 1% data\n(∼3,000 examples).\nsequence-to-sequence models by incorporating\npre-trained weights from LSTM LMs (G ¨ulcehre\net al., 2015; Ramachandran et al., 2017), their\nmodels include many non-pre-trained parame-\nters, such as additional LSTM layers or the\nweights governing the models’ attention mecha-\nnisms. These parameters have to be trained from\nscratch, which can require lots of labeled data. On\nthe other hand, Radford et al. (2019) have recently\ntrained a large Transformer (Vaswani et al., 2017)\nlanguage model and applied it to summarization\nwithout any ﬁne-tuning (thus adding no non-pre-\ntrained parameters), demonstrating the model’s\nzero-shot abilities.\nWe explore pre-training a large Transformer\nlanguage model and ﬁne-tuning it for text sum-\nmarization, demostrating the model’s sample ef-\nﬁciency. In order to use pre-trained weights more\nefﬁciently, we use a Transformer-based decoder-\nonly network (Liu et al., 2018) during ﬁne-tuning.\nThis decoder-only network, or Transformer LM,\ntreats summarization as a language modeling task\nwhere each example consists of a summary ap-\npended to its article. Rather than using separate\narXiv:1905.08836v1  [cs.CL]  21 May 2019\nencoder and decoder components, a single net-\nwork is used to both encode the source and gen-\nerate the target. Crucially, it includes pre-trained\nself-attention parameters which are used to attend\nto both the source and the previously generated\ntarget representations. This approach (1) avoids\nthe redundancy of loading copies of the same\npre-trained weights into the encoder and decoder,\n(2) uses fewer parameters compared to encoder-\ndecoder networks, and most importantly (3) en-\nsures all model weights, including those control-\nling attention over source states, are pre-trained.\nThe pre-trained Transformer LM performs\ncompetitively on the CNN/Daily Mail dataset, de-\nspite using a simple model without augmentations\nlike a copy mechanism (See et al., 2017) or re-\ninforcement learning (Chen and Bansal, 2018).\nCrucially, this model is extremely sample efﬁ-\ncient. Fine-tuning it on only 1% data ( ∼3000\nexamples), results in a ROUGE-2 score of 13.1,\nwhile pre-trained Transformer encoder-decoder\nnetworks perform poorly with a ROUGE-2 of\n2.3 (see Figure 1). Such a highly sample efﬁ-\ncient model opens the door for training summa-\nrization models for narrow domains, low-resource\nlanguages, or other settings without abundant la-\nbeled training data. In addition, analysis shows\nthat our model is more abstractive than the pointer-\ngenerator model (See et al., 2017).\n2 Methods\nIn this section, we describe the neural architec-\ntures and methods used for training a language\nmodel and ﬁne-tuning it for text summarization.\nLanguage Model Pre-training. In this study,\nwe train a Transformer (Vaswani et al., 2017)\nbased language model. Unlike ELMo (Peters\net al., 2018), which trains LMs in both directions,\nor BERT (Devlin et al., 2018), which trains a bidi-\nrectional word imputation model, we train a uni-\ndirectional LM (Radford et al., 2019). This is nec-\nessary for initializing an auto-regressive decoder.\nEncoder-Decoder Baselines. We ﬁrst combine\npre-training with standard sequence-to-sequence\n(Sutskever et al., 2014) models. These consist of\na Transformer encoder network that reads the ar-\nticle, a Transformer decoder network that gener-\nates the summary, and an encoder-decoder atten-\ntion mechanism (Bahdanau et al., 2015) that al-\nlows the decoder to attend to encoder states dur-\nFigure 2: Our pre-trained Transformer LM uses the\nsame network to process both the source and target.\nThe encoder-decoder attention parameters, which are\nnot pre-trained, are no longer necessary.\ning generation, shown in Figure 2. Both the en-\ncoder and decoder use the same network architec-\nture as the Transformer LM, making it easy to use\nthe pre-trained weights. We compare three ways\nof incorporating weights from a pre-trained LM,\nproposed by Ramachandran et al. (2017): (1) pre-\ntraining the encoder only, (2) pre-training the de-\ncoder only, and (3) pre-training both. In (3), the\nencoder-decoder attention parameters are the only\nones randomly initialized. When ﬁne-tuning these\nmodels on summarization data, we follow recent\nwork (Howard and Ruder, 2018; Radford et al.,\n2018; Devlin et al., 2018) and ﬁne-tune all of the\npre-trained as well as non-pre-trained parameters.\nTransformer LM. We can simplify the encoder-\ndecoder model by casting summarization as a lan-\nguage modeling task (Liu et al., 2018). This is\ndone by appending each target (the summary) to\nits source (the article), along with a delimiter, and\ntraining a Transformer on this reformulated data.\nSimilar to encoder-decoder models, we only com-\npute loss over the target sequence, as adding loss\nfrom the source sequence did not improve per-\nformance. In this setting, a Transformer based\ndecoder-only network is used to process both the\nsource and the target, as shown in Figure 2. The\nself attention of this decoder-only network, or\nTransformer LM, is now used to attend to source\nstates as well as the states of already generated tar-\ngets. This approach allows for better utilization of\npre-trained LMs by removing all non-pre-trained\nparameters from the model, compared to encoder-\ndecoder models where encoder-decoder attention\nparameters are not pre-trained. This is a key ben-\neﬁt of using Transformer LMs over LSTM LMs,\nwhich do not include attention mechanisms. Us-\ning a Transformer LM also avoids the redundancy\nof loading copies of the pre-trained weights into\nboth the encoder and the decoder.\nModel Augmentations. While there has been\nextensive research on model augmentations for\ntext summarization (G ¨ulcehre et al., 2016; Pa-\nsunuru and Bansal, 2018; Li et al., 2018), they\nare often complicated or summarization-speciﬁc.\nTherefore, since we want to both clearly isolate\nthe beneﬁts of LM pre-training and experiment\nwith a general model applicable to many domains,\nwe do not augment our simple models. We note,\nhowever, that many of these augmentations could\nbe added to our models to further improve perfor-\nmance.\n3 Experiments\nIn this section, we describe our data, models, train-\ning details, and results. Finally, we discuss the im-\npressive sample efﬁciency of our model.\nPre-Training Data. While recent prior work\n(Radford et al., 2018) trains their language model\non the Toronto BookCorpus (Zhu et al., 2015), this\ndataset appears to no longer be publicly available.1\nTherefore, we instead collected a new 2-billion-\nword corpus based on Wikipedia called WikiLM.\nTo facilitate future research on generative pre-\ntraining, we make the corpus publicly available.2\nSummarization Data. We use the non-\nanonymized CNN/Daily Mail dataset (See et al.,\n2017), which involves summarizing news articles\ninto 2-3 sentences. Summarization performance\nis typically evaluated using ROUGE-1 (unigram\noverlaps), ROUGE-2 (bigram overlaps) and\nROUGE-L (subsequence overlaps) (Lin, 2004).\nAlthough not perfect, these metrics serve as a\ngood ﬁrst approximation to quantify performance.\nModels. For baselines, we evaluate encoder-\ndecoder models with no pre-training as well as\nthe three pre-training strategies discussed in Sec-\ntion 2. Using weights from a pre-trained LM\nconstrains the model to be unidirectional and very\nlarge, so we report results from a smaller model\nwith a bidirectional encoder to quantify how this\naffects performance. We report results from our\nsingle Transformer LM with and without pre-\ntraining. Additionally, we provide scores for a\nsmall 4-layer Transformer with a copy mechanism\nand coverage loss (Gehrmann et al., 2018), as a\nreference for the kind of gains achieved by adding\n1See http://yknzhu.wixsite.com/mbweb\n2Available to download at https://github.com/\ntensorflow/tensor2tensor\nModel R1 R2 RL\nOther Abs. Sum. models*\nCelikyilmaz et al. (2018) 41.69 19.47 37.92\nCopyTransformer (4-layer) 39.25 17.54 36.45\nGehrmann et al. (2018)\nGPT-2 (48-layer, zero-shot) 29.34 8.27 26.58\nRadford et al. (2019)\nNo Pre-training\nBidirEncoder-Decoder (4-layer) 37.74 16.27 34.76\nEncoder-Decoder (12-layer) 36.72 15.22 33.84\nTransformer LM (12-layer) 37.72 16.14 34.62\nWith Pre-training (all 12-layer)\nPre-train Encoder only 36.05 15.48 33.48\nPre-train Decoder only 27.48 6.87 25.40\nEncoder-Decoder 39.18 17.00 36.33\nTransformer LM 39.65 17.74 36.85\nTable 1: Summarization results when using the full\ntraining set. Our scores are averaged over three models\ntrained with different random seeds. *Other abstractive\nsummarization model scores are provided to contextu-\nalize performance on this task but are not directly com-\nparable to our models.\nthese augmentations. We also provide zero-shot\nsummarization scores from Radford et al. (2019)\nwhich uses a ten times larger pre-trained Trans-\nformer language model, but without any ﬁne-\ntuning, and thus also without introducing non-pre-\ntrained parameters. Lastly, we provide scores from\nthe state-of-the-art LSTM based model (Celikyil-\nmaz et al., 2018), which includes a reinforcement\nlearning objective.\nTraining Details. We use the neural architecture\nand hyperparameters from Radford et al. (2018),\nwhich have demonstrated excellent results on a va-\nriety of tasks. It is a 12-layer Transformer with\n135M parameters. The same architecture is used\nfor both components in our encoder-decoder mod-\nels. For LM pre-training, we use a byte pair en-\ncoding vocabulary of 63,807 subwords (Sennrich\net al., 2016) and train the model on WikiLM for\n30 epochs. It converges to a perplexity of 20.5,\nsimilar to the 18.4 perplexity reached by Radford\net al. (2018). 3 For supervised training, we found\nit beneﬁcial to use a lower learning rate (5 ×10−5\ninstead of 2 ×10−4) and train for fewer epochs\n(6 instead of 12), when incorporating pre-trained\nweights, compared to when training from scratch.\nDuring inference, we use beam search with beam\n3However, the perplexities are not directly comparable be-\ncause they are on different corpora\nsize 2 while generating summaries.4\nResults using the full training set. Table 1\nshows ROUGE scores for our models with and\nwithout pre-training. We ﬁnd that pre-training im-\nproves performance by about 2 ROUGE points,\non average. Surprisingly, when only the decoder\nis pre-trained, ROUGE gets substantially worse.\nWe speculate this is because the model starting\nout with a well-trained decoder and poor encoder\nlearns to overly rely on its language modeling abil-\nities and not adequately incorporate information\nfrom the encoder. The Transformer LM outper-\nforms corresponding models both with and with-\nout pre-training, despite having almost half as\nmany parameters. Our best model performs com-\npetitively with existing models on the CNN/Daily\nMail abstractive summarization task, despite the\nabsence of model augmentations such as a copy\nmechanism and reinforcement learning objectives.\n3.1 Sample Efﬁciency Results\nWe test the sample efﬁciency of our summariza-\ntion models by training them on randomly gener-\nated 1%, 2%, 5%, 10%, 20%, and 50% subsets\nof the CNN/Daily Mail training data (287,227 ex-\namples), using the same subsets for all models.\nTo strengthen the baseline encoder-decoder net-\nwork, we make the encoder bidirectional and use\na smaller 4-layer model. We make no changes to\nthe other hyperparameters except increasing the\nnumber of training epochs. Figure 1 illustrates\nsample efﬁciency for our encoder-decoder models\nand Transformer LM, both with and without pre-\ntraining. We only report ROUGE-2 scores which\nare lowest of the three metrics used, though trends\nare consistent for ROUGE-1 and ROUGE-L.\nUnsurprisingly pre-training improves sample\nefﬁciency, but gains are much larger when also\nusing the Transformer LM. Fine-tuning the pre-\ntrained Transformer LM on 1% data, less than\n3,000 examples, results in a model that achieves\na ROUGE-2 score of 13.1, compared to the\npre-trained Transformer encoder-decoder model\nwhich only scores 2.3 ROUGE-2 points. To en-\nsure that augmenting the model with a copy mech-\nanism does not close the sample efﬁciency gap, we\ntrained the 4-layer Transformer + coverage + copy\nmechanism model from Gehrmann et al. (2018)\non 1% and 5% of the summarization data and\n4All code available at https://github.com/\ntensorflow/tensor2tensor\nGround Truth: A man in suburban Boston is selling snow\nonline to customers in warmer states. For $89, he will ship\n6 pounds of snow in an insulated Styrofoam box.\nEncoder-Decoder + Pre-training: NEW: A snowfall of is\nforecast for New England. NEW: The Massachusetts-based\ncompany hopes to sell more than 30,000 bottles of snow.\nThe company says it will use snow from as far as Canada.\nTransformer LM + Pre-training: Kyle Waring will ship\nyou 6 pounds of Boston-area snow in an insulated Styro-\nfoam box – enough for 10 to 15 snowballs, he says. But not\nif you live in New England or surrounding states.\nTable 2: Sample outputs after training on 1% data. See\nthe supplementary materials for more outputs.\nFigure 3: n-gram overlaps of predicted and gold sum-\nmaries with their corresponding source articles. While\nour model copies more than gold summaries, it copies\nsubstantially less than the pointer-generator model.\nfound that while it slightly outperforms our base-\nline models (getting 2.5 ROUGE-2 at 1% and 5.1\nROUGE-2 at 5%), it still performs much worse\nthan our pre-trained Transformer LM. Overall, our\nresults indicate that while pre-training does im-\nprove sample efﬁciency, having every parameter\nbeing pre-trained instead of only a subset of them\nprovides large gains in limited data settings.\n4 Analysis\nTable 2 shows generated outputs for models ﬁne-\ntuned on 1% data. The pre-trained Transformer\nLM succeeds in copying salient information from\nthe source, which indicates it can effectively at-\ntend over the source article. On the other hand, the\npre-trained Encoder-Decoder model hallucinates\nfacts such as “30,000 bottles of snow”, which are\ntopical but never appear in the source, suggest-\ning that the model is unable to utilize information\nfrom the source. Instead, it behaves more as a gen-\neral domain language model.\nThe pre-trained Transformer LM’s predictions\ninitially raised the concern: how much of the im-\npressive sample efﬁciency is due to copying from\nthe source? After all, the no-training-required\n“lead-3” baseline (See et al., 2017), which uses\nthe ﬁrst three sentences of the article as the sum-\nmary, achieves 17.7 ROUGE-2. Hence, we inves-\ntigate the extent to which summaries are copied\nfrom the articles by computing n-gram overlaps\nbetween the articles and summaries. In Figure 3,\nwe compare n-gram overlaps for (1) our pre-\ntrained Transformer LM ﬁne-tuned on 1% data,\n(2) See et al. (2017)’s pointer-generator with cov-\nerage model, and (3) gold summaries. While our\nmodel copies from the source more often than gold\nsummaries, it is more abstractive than the pointer-\ngenerator model which copies 70% of all gen-\nerated 10-grams, compared to our model which\ncopies 27% of its generated 10-grams, likely due\nto lack of a copy mechanism.\n5 Conclusion\nSample efﬁciency can be vital for narrow domains\nand low-resource settings, especially in the case\nof generation tasks for which models often require\nlarge datasets to perform well. In this paper, we\nhave shown that using a single pre-trained Trans-\nformer LM for sequence-to-sequence tasks simpli-\nﬁes the model, reduces the number of parameters,\nand removes the non-pre-trained encoder-decoder\nattention weights. More importantly, experiments\nﬁne-tuning the model on only 1% training data\nhave shown that our approach achieves impressive\nsample efﬁciency gains. It would be interesting to\nfurther test whether this approach leads to similar\nsample efﬁciency gains on tasks beyond summa-\nrization, such as dialogue.\nAcknowledgments\nThis work was started and, in part, carried out dur-\ning the ﬁrst author’s internship at Google Brain.\nWe thank Ashwin Paranjape and Yuhao Zhang\nfor their thoughtful comments and suggestions.\nWe gratefully acknowledge support of the DARPA\nCommunicating with Computers (CwC) program\nunder ARO prime contract no. W911NF15-1-\n0462 and the NSF via grant IIS-1514268. Kevin\nis supported by a Google PhD Fellowship.\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In ICLR.\nAsli Celikyilmaz, Antoine Bosselut, Xiaodong He, and\nYejin Choi. 2018. Deep communicating agents for\nabstractive summarization. In NAACL.\nYen-Chun Chen and Mohit Bansal. 2018. Fast abstrac-\ntive summarization with reinforce-selected sentence\nrewriting. In ACL.\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In NIPS.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nSebastian Gehrmann, Yuntian Deng, and Alexander M\nRush. 2018. Bottom-up abstractive summarization.\nIn EMNLP.\nCaglar G ¨ulcehre, Sungjin Ahn, Ramesh Nallapati,\nBowen Zhou, and Yoshua Bengio. 2016. Pointing\nthe unknown words. In ACL.\nCaglar G ¨ulcehre, Orhan Firat, Kelvin Xu, Kyunghyun\nCho, Lo¨ıc Barrault, Huei-Chi Lin, Fethi Bougares,\nHolger Schwenk, and Yoshua Bengio. 2015. On us-\ning monolingual corpora in neural machine transla-\ntion. arXiv preprint arXiv:1503.03535.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nACL.\nWei Li, Xinyan Xiao, Yajuan Lyu, and Yuanzhuo\nWang. 2018. Improving neural abstractive docu-\nment summarization with explicit information selec-\ntion modeling. In NIPS.\nChin-Yew Lin. 2004. Rouge: A package for auto-\nmatic evaluation of summaries. Text Summarization\nBranches Out: ACL Workshop.\nPeter J Liu, Mohammad Saleh, Etienne Pot, Ben\nGoodrich, Ryan Sepassi, Lukasz Kaiser, and Noam\nShazeer. 2018. Generating wikipedia by summariz-\ning long sequences. In ICLR.\nRamakanth Pasunuru and Mohit Bansal. 2018. Multi-\nreward reinforced summarization with saliency and\nentailment. In NAACL.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In NAACL.\nAlec Radford, Karthik Narasimhan, Tim Salimans,\nand Ilya Sutskever. 2018. Improving language\nunderstanding by generative pre-training. URL\nhttps://s3-us-west-2.amazonaws.com/openai-\nassets/research-covers/language-\nunsupervised/language understanding paper.pdf.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Lan-\nguage models are unsupervised multitask learners.\nURL https://d4mucfpksywv.cloudfront.net/better-\nlanguage-models/language-models.pdf.\nPrajit Ramachandran, Peter J Liu, and Quoc V Le.\n2017. Unsupervised pretraining for sequence to se-\nquence learning. In EMNLP.\nAbigail See, Peter J Liu, and Christopher D Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In ACL.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In ACL.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural net-\nworks. In NIPS.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS.\nYukun Zhu, Ryan Kiros, Richard Zemel, Ruslan\nSalakhutdinov, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Aligning books and movies:\nTowards story-like visual explanations by watch-\ning movies and reading books. arXiv preprint\narXiv:1506.06724.\nA Appendices\nOur Transformer LM + pre-training system is ex-\ntremely data-efﬁcient: it generates decent sum-\nmaries using only 1% of the training data while\nthe baseline pre-trained Encoder-Decoder model\nessentially generates gibberish. Examples illus-\ntrating this behavior are shown in Table 3. Fur-\nthermore, our Transformer LM appears to be more\nabstractive than the Pointer-Generator network,\nwhich uses a copy mechanism. While the Pointer-\nGenerator often copies over whole sentences from\nthe article, our model mixes and matches sentence\nfragments, with other generated words sometimes\nconnecting them. The downside of this abstrac-\ntiveness is the way it occasionally “hallucinates”\nfacts, such as in the amusing summary in row 5.\nEncoder-Decoder + Pre-Training\nFine-tuned on 1% of training data\nTransformer LM + Pre-Training\nFine-tuned on 1% of training data\nPointer-Generator (See et al., 2017)\nTrained on 100% of training data\n[the] [u.s.] [u.s.] [u.s] [. the] [u.s] [.] [it]\n[has] [been] [in the] [u.s] [. the] [u.s] [.]\n[it] [is] [the] [u.s] [. the] [u] [.]\n[president] [obama says he is “ abso-\nlutely committed] [”] [to making sure]\n[israel is] [not] [vulnerable] [.] [ne-\ntanyahu] [says he] [understands and re-\nspects] [obama] [’s stance that israel is\nparticularly vulnerable] [.]\n[obama says he is “ absolutely commit-\nted to making sure ” israel maintains\na military advantage over iran . his\ncomments to the new york times] [pub-\nlished on sunday , come amid criticism\nfrom israeli prime minister benjamin ne-\ntanyahu] [.]\n[new] [:] [the] [president] [will] [be]\n[able] [to the] [world] [’s] [new] [new]\n[new] [president] [. the group] [says] [it]\n[will] [be] [able] [to] [be] [able .] [the]\n[u.s] [. the] [u.s] [. the] [u.s] [.] [it] [will]\n[be] [used] [to] [be] [able] [to] [be] [able]\n[to] [be] [able .] [the u] [.]\n[the american pharmacists association]\n[voted at its annual meeting to adopt a\nban as an ofﬁcial policy] [.]\n[the american pharmacists association is\ndiscouraging its members from partici-\npating in executions .] [the group acted\nthis week because of increased public\nattention on lethal injection] [. thirty-\ntwo sates allow capital punishment , and\nlethal injection is still the most common\nmethod .]\n[new] [:] [the] [president] [is] [the]\n[world] [’s] [death] [of the] [world] [’s]\n[ﬁrst] [time] [at the] [world] [’s] [death]\n[. the] [president] [was] [found] [in the]\n[world] [’s] [ﬁrst] [time] [of the] [world]\n[’s] [ﬁrst] [time] [of the] [world] [’s]\n[ﬁrst] [time] [of the] [world] [.]\n[zaki-ur-rehman lakhvi , a top leader of]\n[lashkar-e-taiba , was released early fri-\nday from] [jail in the pakistani city of\nrawalpindi] [.] [he] [is] [accused of mas-\nterminding the november 2008 terror at-\ntacks that left more than 160 people dead\nin mumbai , india ’s most populous city\n.]\n[zaki-ur-rehman lakhvi , a top leader of\nthe terrorist group lashkar-e-taiba , was\nreleased early friday from a jail in the\npakistani city of rawalpindi] [. lakhvi\nwas charged in pakistan in 2009 , ac-\ncused of masterminding the november\n2008 terror attacks that left more than\n160 people dead in mumbai , india ’s\nmost populous city .]\n[new] [:] [the] [president] [is] [the]\n[world] [’s] [president] [president] [is]\n[the] [world] [’s] [ﬁrst] [time] [of the]\n[world] [’s] [most] [most] [most] [most]\n[of the] [world] [.] [he] [says] [he] [’s]\n[ﬁrst] [time] [to the] [world] [cup] [ﬁnal]\n[.]\n[don mclean ’s] [“ american pie ” is] [44\nyears old .] [he] [was a paperboy when ,\non february 3 , 1959 , he saw that buddy\nholly , ritchie valens and j.p. “ the big\nbopper ” richardson had been tragically\nkilled in an airplane crash in clear lake ,\niowa .]\n[don mclean ’s pop masterpiece “ ameri-\ncan pie] [” is] [44 years old .] [christie ’s\nsold the 16-page handwritten manuscript\nof the song ’s lyrics for $ 1.2 million to\nan unnamed buyer . mclean was a pa-\nperboy when , on february 3 , 1959 , he\nsaw that buddy] [valens and j.p. “ the big\nbopper ” richardson had been tragically\nkilled] [.]\n[new] [: the] [president] [is] [the world]\n[’s] [ﬁrst] [time] [of the] [world] [’s]\n[ﬁrst] [time] [at] [the world] [’s] [most]\n[most] [most] [most] [. the] [president]\n[is] [the] [ﬁrst] [time] [of the] [world]\n[’s] [most] [most] [most] [most] [most]\n[most] [most] [of the] [world] [.]\n[iran] [won the world cup] [after a] [nu-\nclear deal with the united states] [. the]\n[deal] [promises to end iran ’s interna-\ntional isolation under years of crippling\nsanctions .]\n[iranians erupted in celebration as young\npeople waved ﬂags from their sunroofs\n, blasted music from stereos and chatted\nonline with the hashtag # irantalks .] [ex-\ncitement came after a breakthrough nu-\nclear deal with the united states and other\nworld powers that promises to end iran\n’s international isolation under years of\ncrippling sanctions .]\n[new] [:] [a] [woman] [has] [been]\n[charged] [with] [a] [year] [.] [she]\n[says] [it ’s] [ﬁrst] [time] [to be able\nto] [be able] [. the] [woman] [’s new]\n[woman] [is] [a] [woman] [’s] [most] [of\nthe] [woman] [’s new] [new] [woman]\n[is] [in the] [ﬁrst] [time] [.]\n[netﬂix] [ordered up a reunion special\n, followed by a spinoff series called “\nfuller house] [” the show will] [be avail-\nable next year , netﬂix said .]\n[john stamos announced monday night\non “ jimmy kimmel live ”] [. the show\nwill] [feature candace cameron bure ,\nwho played eldest daughter d.j . tanner in\nthe original series] [,] [which aired from\n1987 to 1995] [, will both return for the\nnew series] [.]\nTable 3: Comparisons of summaries generated by various models. Colors/brackets correspond to consecutive\nwords that occur in the article (black means the word was not in the article text).",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.908348798751831
    },
    {
      "name": "Transformer",
      "score": 0.6252477169036865
    },
    {
      "name": "Computer science",
      "score": 0.5401071906089783
    },
    {
      "name": "Natural language processing",
      "score": 0.5356101393699646
    },
    {
      "name": "Sample (material)",
      "score": 0.5049882531166077
    },
    {
      "name": "Artificial intelligence",
      "score": 0.44780057668685913
    },
    {
      "name": "Electrical engineering",
      "score": 0.14776834845542908
    },
    {
      "name": "Engineering",
      "score": 0.1471094787120819
    },
    {
      "name": "Chemistry",
      "score": 0.13456997275352478
    },
    {
      "name": "Chromatography",
      "score": 0.10606133937835693
    },
    {
      "name": "Voltage",
      "score": 0.059925615787506104
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    }
  ]
}