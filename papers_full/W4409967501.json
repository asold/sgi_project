{
  "title": "A scoping review of large language models for generative tasks in mental health care",
  "url": "https://openalex.org/W4409967501",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2807260057",
      "name": "Yining Hua",
      "affiliations": [
        "Harvard University",
        "Beth Israel Deaconess Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A5108911992",
      "name": "Hongbin Na",
      "affiliations": [
        "University of Technology Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2418419880",
      "name": "Zehan Li",
      "affiliations": [
        "The University of Texas Health Science Center"
      ]
    },
    {
      "id": "https://openalex.org/A2102522444",
      "name": "Fenglin Liu",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2104808083",
      "name": "Xiao Fang",
      "affiliations": [
        "Massachusetts Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2047008806",
      "name": "David Clifton",
      "affiliations": [
        "Suzhou Research Institute",
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2233486890",
      "name": "John Torous",
      "affiliations": [
        "Harvard University",
        "Beth Israel Deaconess Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2807260057",
      "name": "Yining Hua",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5108911992",
      "name": "Hongbin Na",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2418419880",
      "name": "Zehan Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102522444",
      "name": "Fenglin Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2104808083",
      "name": "Xiao Fang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2047008806",
      "name": "David Clifton",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2233486890",
      "name": "John Torous",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2791681585",
    "https://openalex.org/W3001662993",
    "https://openalex.org/W4393870714",
    "https://openalex.org/W3197232629",
    "https://openalex.org/W4389435613",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4390722663",
    "https://openalex.org/W4247665917",
    "https://openalex.org/W4390668567",
    "https://openalex.org/W4396750566",
    "https://openalex.org/W4396833227",
    "https://openalex.org/W4390581979",
    "https://openalex.org/W4390535718",
    "https://openalex.org/W4392739577",
    "https://openalex.org/W4400909709",
    "https://openalex.org/W4390965895",
    "https://openalex.org/W4396832979",
    "https://openalex.org/W4385572158",
    "https://openalex.org/W4388157511",
    "https://openalex.org/W4399523537",
    "https://openalex.org/W4390102952",
    "https://openalex.org/W4317757464",
    "https://openalex.org/W4386893393",
    "https://openalex.org/W4385729929",
    "https://openalex.org/W4387846962",
    "https://openalex.org/W4389518904",
    "https://openalex.org/W4390918905",
    "https://openalex.org/W4392970427",
    "https://openalex.org/W2988937804",
    "https://openalex.org/W4400895263",
    "https://openalex.org/W4310779131",
    "https://openalex.org/W4402369854",
    "https://openalex.org/W4402439339",
    "https://openalex.org/W4396736209",
    "https://openalex.org/W4404783621",
    "https://openalex.org/W3118615836",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W4292994367",
    "https://openalex.org/W1901333491",
    "https://openalex.org/W2132322340",
    "https://openalex.org/W2790767553",
    "https://openalex.org/W2156690253",
    "https://openalex.org/W1980602541",
    "https://openalex.org/W187459603",
    "https://openalex.org/W2122447448"
  ],
  "abstract": null,
  "full_text": "npj |digital medicine Article\nPublished in partnership with Seoul National University Bundang Hospital\nhttps://doi.org/10.1038/s41746-025-01611-4\nA scoping review of large language models\nfor generative tasks in mental health care\nCheck for updates\nYining Hua1,2,H o n g b i nN a3,Z e h a nL i4, Fenglin Liu 5,X i a oF a n g6,D a v i dC l i f t o n5,7 &\nJohn Torous 2,8\nLarge language models (LLMs) show promise in mental health care for handling human-like\nconversations, but their effectiveness remains uncertain. This scoping review synthesizes existing\nresearch on LLM applications in mental health care, reviews model performance and clinical\neffectiveness, identiﬁes gaps in current evaluation methods following a structured evaluation\nframework, and provides recommendations for future development. A systematic search identiﬁed\n726 unique articles, of which 16 met the inclusion criteria. These studies, encompassing applications\nsuch as clinical assistance, counseling, therapy, and emotional support, show initial promises.\nHowever, the evaluation methods were often non-standardized, with most studies relying on ad-hoc\nscales that limit comparability and robustness. A reliance on prompt-tuning proprietary models, such\nas OpenAI’s GPT series, also raises concerns about transparency and reproducibility. As current\nevidence does not fully support their use as standalone interventions, more rigorous development and\nevaluation guidelines are needed for safe, effective clinical integration.\nMental health issues have been a concern of global health ever since they\nrecognized the profound impact on individuals and societies, and the\nurgency has only grown in recent years. Nearly 1% of all global deaths\nannually are now due to suicide, with approximately 800,000 people dying\nby suicide each year\n1. In the United States alone, the annual public mental\nhealth expenditure exceeded $16.1 billion, including a $2.21 billion budget\nfor the National Institute of Mental Health (NIMH) and $13.9 billion on\nmental healthcare\n2.S t i l l ,e v e ni nt h eU n i t e dS t a t e s ,t h ep s y c h i a t r yw o r k f o r c e\nis projected to face a pressing shortage through 2024, with a potential\nshortfall of 14,280 to 31,091 psychiatrists\n3,4. And in low-and-middle income\ncountries, the situation is even worse, with up to 85% of people there still\nreceiving no treatment for their mental health5.\nIn response to the growing mental health crisis and the projected\nshortage of mental health professionals, artiﬁcial intelligence (AI)-driven\nmental health applications like chatbots are emerging as vital tools to\nbridge the treatment gap. These technologies offer scalable, accessible, and\ncost-effective support, particularly in areas where traditional mental\nhealth services, including psychiatric care, are insufﬁcient or unavailable.\nAs of 2023, the global market for mental health apps has grown rapidly,\nwith over 10,000 apps collectively serving millions of users\n6. AI-driven\nplatforms are increasingly incorporating psychiatric assessments,\nmedication management reminders, and monitoring tools that assist in\nthe management of conditions such as depression, anxiety, and bipolar\ndisorder. Studies suggest these tools can help reduce symptoms and\nimprove patient outcomes, making them a promising avenue for\naddressing mental health challenges, especially in regions with limited\naccess to psychiatric professionals, and they are increasingly being inte-\ngrated into broader mental health care strategies to help meet the growing\ndemand\n7,8.\nT h ei n t r o d u c t i o no fl a r g el a n g u a g em o d e l s( L L M s )l i k eO p e n A I’s\nChatGPT9,G o o g l e’sB a r d10, and Anthropic’s Claude11 marks a transfor-\nmative advancement in AI-driven mental health care, offering capabilities\nfar beyond those of earlier AI tools. Unlike previous models, which were\nlimited to scripted interactions and speciﬁc tasks, LLMs can engage in\ndynamic, context-aware conversations that feel more natural and perso-\nnalized via generating human-like conversations. This allows them to\nprovide tailored emotional support, detect subtle cues indicating changes in\nmental health, and adjust their guidance to meet individual user needs in\ngenerative tasks. Increasingly, research is exploring anthropomorphic fea-\ntures such as empathy, politeness, and other human-like traits in these\nmodels to enhance their effectiveness in delivering more realistic and sup-\nportive mental health care\n12.\n1Department of Epidemiology, Harvard T.H. Chan School of Public Health, Boston, MA, USA.2Department of Psychiatry, Beth Israel Deaconess Medical Center,\nBoston, MA, USA.3Australian Artiﬁcial Intelligence Institute, University of Technology Sydney, Sydney, NSW, Australia.4McWilliams School of Biomedical\nInformatics, UTHealth Houston, Houston, TX, USA.5Institute of Biomedical Engineering, Department of Engineering Science, University of Oxford, Oxford, UK.\n6MIT Media Lab, Massachusetts Institute of Technology, Cambridge, MA, USA.7Oxford-Suzhou Centre for Advanced Research, Suzhou, China.8 Department of\nPsychiatry, Harvard Medical School, Boston, MA, USA. e-mail: jtorous@bidmc.harvard.edu\nnpj Digital Medicine|           (2025) 8:230 1\n1234567890():,;\n1234567890():,;\nDespite the promising potential, these tools are still in the early stages of\ndevelopment and evaluation. Users often do not understand the models they\nare interacting with, including the limitations and biases inherent in the AI’s\ndesign. Unfortunately, there is currently no standardized framework for\nevaluating the effectiveness and safety of these models in mental health\napplications. Many studies, including those focused on evaluating LLMs,\noften develop their own metrics and methods, leading to inconsistent and\nsometimes unreliable results. The lack of standardized evaluation hinders\nthe comparison of models or assess their true impact on mental health\noutcomes. Concerns about data privacy, the potential for misuse, and the\nethical implications of relying on AI for sensitive mental health care deci-\nsions further underscore the need for rigorous oversight. Considering these\npromises and challenges, a scoping review of the current applications of\nLLMs in mental health care is essential from the perspective of psychiatrists\nand clinical informaticians. Our review aims to synthesize existing research\nwith a focus on clinical relevance, identify gaps in understanding from a\nmental health practice standpoint, and provide clear guidelines for future\ndevelopment and evaluation of these technologies in real-world settings.\nBackground\nSubﬁelds of Mental health care and the potential of generative AI\nThe potential of generative AI in mental health care is broad given the many\ndifferent treatment approaches employed today for care delivery. These\napproaches generally fall into three main categories: psychotherapy, psy-\nchiatry, and general mental health support. Psychotherapy is one of the most\ncommon forms of mental health care. However, access to psychotherapy is\noften limited by factors like a shortage of therapists, long wait times, and\nhigh costs. Generative AI could help address these issues by offering on-\ndemand support, providing education about mental health, and guiding\npeople through therapeutic exercises when they can’t see a therapist in\nperson. Psychiatry focuses on the medical side of mental health care,\nincluding diagnosing, treating, and preventing mental disorders. But like\npsychotherapy, psychiatry also faces challenges, particularly a shortage of\npsychiatrists. Generative AI could support psychiatrists by helping monitor\npatients’symptoms, reminding them to take their medication, and pro-\nviding initial assessments, which could reduce the strain on the healthcare\nsystem and improve patient outcomes. General mental health support\nincludes a wide range of services designed to promote mental well-being and\nprevent mental health problems. Thismight include community programs,\nself-help resources, peer support networks, and public health initiatives.\nThese services are important for earlyintervention, managing stress, and\npreventing more serious mental health issues from developing. However,\nmany people don’t take advantage of these resources, often because of\nstigma, lack of awareness, or insufﬁcient availability. Generative AI could\nhelp make these resources more accessible by providing anonymous, per-\nsonalized support through chatbots and apps that offer mental health\neducation, coping strategies, and encouragement to seek help in a way that\nfeels safe and non-judgmental.\nL a r g el a n g u a g em o d e l s( L L M s )\nAlthough LLMs gained widespread attention with the release of OpenAI’s\nChatGPT-4, the concept has existed for some time, though there is no single\nuniﬁed deﬁnition. In the natural language processing (NLP) community,\nLLMs are generally understood as large generative AI models capable of\np r o d u c i n gt e x tb yp r e d i c t i n gt h en e x tword or phrase based on vast amounts\nof training data. NLP has evolved drastically over time, with early models\nbeing task-speciﬁc and limited in their ability to understand context and\nnuance. The introduction of advanced deep learning frameworks marked a\nmajor improvement, as these models are designed to better capture con-\ntextual language meaning. However, they still struggled with generating\ncoherent, contextually appropriate text over longer conversations, which is\ncrucial for mental health applications. LLMs have advanced this further by\nleveraging large datasets and transformer architectures to predict and\ngenerate highly coherent and context-aware text. This enables them to\nmimic human conversation, making them valuable for creating therapeutic\ncontent, offering psychoeducation, and simulating therapy sessions—\nimportant tools for expanding access to mental health care. For clinicians,\nLLMs offer promising tools to supportmental health services by providing\npersonalized, scalable interactions. However, it’s important to recognize that\nmost current LLMs are general models and do not perform as well as\nspecialized pre-trained models for domain-speciﬁc tasks such as prediction\nand classiﬁcation. For example, Bidirectional Encoder Representations from\nTransformers (BERT) models, which model word segments (tokens) using\nb o t ht h es e g m e n t sb e f o r ea n da f t e rt h e m ,a r em o r ea c c u r a t ea n de fﬁcient for\nthese purposes. As a result, pretraining andﬁne-tuning becomes a crucial\nstep as it provides the model with contextual knowledge and linguistic\npatterns speciﬁc to the mental health applications. Thisﬁnetuning and\npretraining process can incorporateemotional cues and expert-written\nexamples to enhance the model’s interpretability and responsiveness to\nimprove the performance of LLMs in speciﬁc generative tasks.\nResults\nMental disorders, conditions, and subconstructs\nMental disorders referenced in the included studies vary widely in terms of\ndeﬁnitions, measurement instruments, and the use of standards. While\nsome studies focus on clinically conﬁrmed diagnoses, relying on estab-\nlished criteria like those found in the DSM-513, others take a less structured\napproach. In such cases, mental health constructs are often deﬁned\narbitrarily using user-expressed keywords or affects rather than expert\nknowledge or validated measures. This is especially common in studies\nconducted outside the medical or clinical domain, where mental health\nconstructs may be interpreted more loosely or tailored to the context of the\nAI models. Such inconsistencies in the use and understanding of validated\nmeasures highlight a potential gap when applying AI models to various\ntargeted mental health constructs— including affect, symptoms, diag-\nnosis, and treatment— reﬂecting a broader issue in this interdisciplinary\nﬁeld. Therefore, we categorized the targeted mental health disorders,\nconditions, and subconstructs into two groups: 1) those measured or\ndeﬁned with validated approaches, relying on standard diagnostic criteria\nand validated clinical knowledge; and 2) those assessed with non-validated\nmeasures, lacking a clear deﬁnition, standard, or validated method for\nassessment or diagnosis.\nA ss h o w ni nT a b l e1, eight studies out of the sixteen reviewed included\nvalidated measures for mental health constructs\n14–21, while nine relied only\non ad-hoc (less well established) approaches16,17,21–27, and three studies\nincluded constructs with a mix of both types of measurements16,17,21.A c r o s s\nboth groups, depression14,16–19,21,24–26 was the most frequently studied mental\nhealth construct. The Patient Health Questionnaire-9 (PHQ-9)16,19 and the\nCenter for Epidemiologic Studies Depression Scale for Children (CES-DC)16\nwere adopted as inclusion criteria and outcome measures16,19, while another\nstudy used PHQ-9 as an exclusion criterion21. Other clinically valid con-\nstructs include anxiety14,16,18, positive and negative affects (PANAS)14,\nAttention-Deﬁcit/Hyperactivity Disorder (ADHD)15,20, bipolar disorder19,\nloneliness17, and stress16.\nOne study evaluated GPT’s performance on 100 clinical case vignettes\nof different disorders, comparing GPT against psychiatrists across different\nclinical constructs, covering a wide range of disorders28. However, not all\nstudies referenced clinical mental health constructs provided speciﬁcc r i -\nteria. For example, one study diagnosed study subjects with clinical inter-\nviews “using screening instruments over different disorders”\n15.O t h e r\nstudies also incorporated expert judgments from mental health providers\nwithout mentioning the speciﬁc process or referring to well-established\ncriteria used in the study\n22,23,28. Depression17,24–26 and suicidality16,17,22,23,27\nhave also been frequently studied with less well-established and customized\nconstructs. For instance, one study associated the construct of depression\nwith self-identiﬁed feelings of depression17 or simply with the word“sad”23.\nAnother studyﬁltered social media posts related to suicidal ideation and\nself-harm using regular expressions (e.g.,“.(commit suicide).”, “.(cut).”)29.\nMore speciﬁc subconstructs of mental health care include psychological\nchallenges due to social emotions28,29, cognitive distortion and negative\nhttps://doi.org/10.1038/s41746-025-01611-4 Article\nnpj Digital Medicine|           (2025) 8:230 2\nthoughts19,20,a n da b u s e19. These studies used less well-established and more\narbitrary standards for deﬁnitions and assessment. (Tables2, 3).\nCognitive Behavior Therapy (CBT)30 is the most referenced treat-\nment method for anxiety, cognitive distortion, depression, and\nloneliness\n15–18,22. It is an evidence-based, well-established psychological\ntreatment. Elements and techniques from CBT30, such as cognitive\nrestructuring22,23 and mindfulness18, have been incorporated into LLM\nmodels to provide digital self-guided interventions. Other evidence-based\ntreatment approaches include occupational therapy20, which is used to\nsupport children with ADHD, and peer support27, where the chat agent\nsimulates individuals with similar experiences to provide empathetic\nemotional support.\nTable 1 | Mental disorders, conditions, and subconstructs in generative applications of LLMs for mental health care\nGroup Condition/Construct Criteria/Content References\nWith validated\nmeasures\nAffects The Positive Affect and Negative Affect Scale (PANAS) 46 14\nAttention-deﬁcit/hyperactivity\ndisorder (ADHD)\nClinical interview; Diagnostic and Statistical Manual of Mental Disorders, Fifth Edition\n(DSM-5)30\n15,20\nAnxiety Generalized Anxiety Disorder 7 (GAD-7) 47 7,26\nBipolar depression Expert clinician validated vignettes 19\nMajor depressive disorder (MDD) Patient Health Questionnaire-9 (PHQ-9) 48, Center for Epidemiologic Studies\nDepression Scale for Children (CES-DC)47\n14,16,18,19,21\nLife satisfaction The Satisfaction with Life Scale (SWLS) 49 14\nLoneliness Interpersonal Support Evaluation List (ISEL) 50, the De Jong Gierveld Loneliness Scale51 17\nStress Coping Strategies Scale 52 16\nPsychological well-being The Scales of Psychological Well-being (SPWB) 53, the Subjective Vitality Scale (SVS)54 14\nWith non-validated\nmeasures\nAbuse Users expressed keywords 17\nThinking trap/Cognitive distortion Self-identi ﬁed exaggerated thoughts, thinking in extremes, jumping to conclusions\nbased on one experience\n22,23\nDepression Self-identi ﬁed feelings of depression;“I am sad and have a history of depression. How\ncan I be happier?”\n17,24–26\nNegative thoughts “What emotion does this thought make you feel? And how strong 1– 10” 22,23\nSocial emotions (personality, mood,\nand attitudes)\nNeutral, happy, sad, relaxed, and angry 21,26,27\nSuicidality or self-harm Keywords de ﬁned by regular expressions. E.g.,“.*(commit suicide).*”, “.*(cut).*”;\n“feeling suicidal”, “want to die”, and“harm myself”; Custom open-ended question\n16,17,22,23,27\nTable 2 | Overview of input/output modalities, models, and target users in generative applications of LLMs in mental health care\nApplication\ncategory\nInput modality Model Output modality Embodiment Open\nsource\nLanguage Target user References\nClinical Assistant Written ChatGPT a Written No No English Healthcare\nProviders\n14\nWritten PanGu Written No No Chinese Healthcare\nProviders\n26\nWritten GPT4-Turbo Written No No English Healthcare\nProviders\n19\nCounseling Written GPT-4 Written No No English General Public 24\nSpoken GPT-3 Spoken,Visual Yes Yes Spanish General Public 21\nWritten GPT-4 Written No No English General Public 34\nTherapy Written,\nSpoken, Visual\nCustomized\nGPTs\nWritten, Spoken Yes No English/\nSpanish\nPatients 35\nSpoken GPT-4 Spoken, Visual Yes No English Patients 18\nWritten,\nSpoken, Visual\nGPT4-Turbo\nClaude-3\nWritten, Spoken,\nVisual\nYes No Multilingual b Patients 20\nWritten GPT-4 Written No No Korean Patients 16\nWritten GPT-3.5-Turbo Written No No English General Public 32\nWritten,\nSpoken, Visual\nNot speciﬁed Written, Spoken,\nVisual\nYes No English/\nJapanese\nGeneral Public 17\nPositive Psychology\nIntervention\nWritten GPT-3.5-Turbo Written No No Chinese Patients 14\nWritten GPT-3 Written No Yes English General Public 22\nWritten GPT-3/T5/\nDialoGPT\nWritten No Yes English Patients 23\nEducation Written GPT-3 Written No No Spanish General Public 15\naVersion not speciﬁed.\nbSpeciﬁc languages not speciﬁed.\nhttps://doi.org/10.1038/s41746-025-01611-4 Article\nnpj Digital Medicine|           (2025) 8:230 3\nTable 3 | Summary of uniﬁed evaluation constructs\nStep Higher-order construct Lower-order construct De ﬁnition Examples Article references\n1 Safety, Privacy, and\nFairness\nSafety Prevent worse outcomes for the patient, provider, or health\nsystem from occurring as a result of the use of an ML algorithm.\nOutcome proxy appropriateness, Data provenance, Harm\ncontrol, Reducing automation bias, Critical help, Ethics, etc.\n20,34\nSafety, Privacy, and\nFairness\nPrivacy Protect privacy according to standards like HIPAA and GDPR,\nensuring user autonomy and dignity.\nData exchange, Data collection and storage, Data usage,\nPrivacy Policy, Data protection, etc.\n35\nSafety, Privacy, and\nFairness\nFairness and bias\nmanagement\nEnsure the chatbot operate with minimized and acknowledged\nbiases to ensure fair outcomes.\nSystemic Bias, Computational and Statistical Bias, Human-\ncognitive biases, Population bias, etc.\n20\n2 Trustworthiness and\nUsefulness\nBeneﬁcence Ensure the chatbot positively impacts its intended outcomes,\nemphasizing measurable beneﬁts over potential risks\nHealth Outcomes, Clinical Evidence, User Behaviors,\nIntervention, Healthcare System, etc.\n14– 16,18,20– 22,35\nTrustworthiness and\nUsefulness\nGeneralizability Apply learned patterns to new, unseen data. Contextual Adaptability, Novel Data Performance, etc. 20,34\nTrustworthiness and\nUsefulness\nReliability Ensure that the chatbot consistently performs as intended under\nvarious conditions and maintains dependable operation\nover time.\nFailure Prevention, Robustness, Workﬂow Integration,\nReproducibility, Monitoring, Up-to-dateness, etc.\n19,48\nTrustworthiness and\nUsefulness\nValidity Ensure the chatbot performs as expected in real-world\nconditions\nData Relevance and Credibility, Language Understanding,\nInformation Retrieval Accuracy, Outcome Accuracy, Task\nCompletion, etc.\n20,21,26,34\n3 Design and Operational\nEffectiveness\nAccessibility Ensure those involved in the chatbot ’s lifecycle uphold standards\nof auditability and harm minimization.\nVersatile access, User literacy required, User experience,\nUser Interface Design, Simplicity/Ease of Use, etc.\n15,16,18,20,21,26,28\n,32,35\nDesign and Operational\nEffectiveness\nPersonalized\nEngagement\nTailor responses based on patient data and preferences. Personalization, Anthropomorphism/relationship, User\nAdherence, Feedback Incorporation, Progress\nawareness, etc.\n18,20,23,31– 35\nDesign and Operational\nEffectiveness\nCost-Effectiveness Assess whether the chatbot delivers bene ﬁcial outcomes at a\nreasonable cost, providing a better or more economical solution\ncompared to existing methods.\nComparative Effectiveness, Economical Viability,\nEnvironmental Viability, Task Efﬁciency, Workﬂow\nConsiderations, etc.\n20,26,34\nTable 3 summarizes the mapped primary and second-level constructs across the reviewed studies. We have also included examples of sub-constructs foreach mapped second-level construct for the readers to understand the mapped constructs. Further details of\nevaluation subjects, evaluation methods, sample sizes, scale names, original constructs, mapped second-level constructs, and levels associatedwith each article can be found in Supplementary Table 3. Practical evaluation questions related to each construct can be found\nin the original article.\nConstructs have been mapped to the second level to avoid excessive scarcity and granularity.\nhttps://doi.org/10.1038/s41746-025-01611-4 Article\nnpj Digital Medicine|           (2025) 8:230 4\nApplications and model information\nExisting generative applications of LLMs in mental health care can be\ncategorized into six main types based on model functionalities: Clinical\nAssistant, Counselling17,29,T h e r a p y17,23,E m o t i o n a lS u p p o r t16,17,31,32 Positive\nPsychology Intervention14,22,23,a n dE d u c a t i o n15,33.A m o n gt h e m ,t h eC l i n i c a l\nAssistant application includes attempts to develop and evaluate LLMs for\nsupporting mental health professionals by generating management strate-\ngies and diagnoses for psychiatric conditions. In the Counselling category,\nLLMs are used to interact with participants, such as engaging Spanish\nteenagers in discussions about mental health disorders\n15 and providing\nrelationship advice in single-session interventions34. Emotional Support\napplications have focused on offering empathetic responses and support in\nvarious contexts, such as mitigatingloneliness and suicide risk among\nstudents17. In the Therapy category, LLMs are integrated into treatments for\nconditions like ADHD, enhancing care through simulated therapy\nscenarios35 and immersive therapy experiences using virtual reality(VR)18.\nPositive Psychology Interventions involve using LLMs to personalize\nrecommendations and facilitate cognitive restructuring, thereby reducing\nnegative thoughts and emotional intensity\n14,22.F i n a l l y ,i nE d u c a t i o n ,L L M s\nhave been employed to train medical students in communication skills,\nproviding a realistic and positive simulated patient experience\n33, as well as\npromoting awareness of mental health among young people15.M o s to ft h e s e\nstudies only support text-based input/output modalities14,15,19,22–24,26,27,31,32,34.\nAs u b s e to fs y s t e m s17,18,20,35 supports multimodal input/output, incorporat-\ning speech, images, or video for a richer user experience. Some applications\nincorporate physical embodiment through VR\n17,18 or robotics20,35.T h e s e\napplications are seen across various target user groups, including healthcare\nproviders19,26,p a t i e n t s14,16,18,20,22–24,31,32,34, and the general public15,32,33.\nOpenAI’s GPT series models are the most studied, see in 14\nstudies14,18–20,22–24,28,31,32,34,35, with 11 using the latest advanced models like\nGPT-3.5, ChatGPT, GPT-4, and customized GPTs, while four studies used\nthe earlier GPT-3 model. Other LLMs used\n23,26 include Huawei’sP a n G u26,\nT520,a n dD i a l o G P T36 are open-source. Some studies did not specify the\nplatforms they employed, while many studies used digital platforms such as\nwebsites and mobile phones. Some studies developed agents with physical\nembodiments\n22, and some others21,35 used Raspberry PI, a type of single-\nboard computer (Supplementary Table 2). Among those that used Open-\nAI’s models, three were based on OpenAI’s web interface24,28,34,o n ed i dn o t\ndirectly state their platform but appeared to use the API based on the\nstructure of their methods\n19, and only eight (57.1%) explicitly referenced\nAPI use or temperature parameters14,18,20,28,32. Language support by these\nmodels varied, covering more than English, with three applications sup-\nported by multiple languages17,20,35, and 14 applications supporting a single\nlanguage— seven in English18,19,22–24,32,34, three in Chinese14,26,29,t w oi n\nKorean16,31, and two in Spanish15,33.\nTask performance and clinical effectiveness\nThe study designs and evaluations of existing research are highly hetero-\ngeneous and often inconsistent, making it challenging to accurately assess\ntheir task performance and clinical effectiveness. Thus, we provide a high-\nlevel summary of theﬁndings here. We offer a detailed summary of each\nstudy’s task, performance/results, sample size, clinical validation method,\nand participant demographics can be found in Supplementary Table 3.\nSeveral studies have explored the use of LLMs for clinical decision\nsupport in psychiatry. In one study, ChatGPT-3.5 was evaluated using 100\nclinical case vignettes covering diverse psychiatric conditions28.T h em o d e l\nachieved a“Grade A”rating in 61% of cases,“Grade B”in 31%, and“Grade\nC” in 8%, indicating different levels of diagnostic accuracy in simulated\nscenarios. However, this study did not involve real patients, and no clinical\nvalidation was performed. Similarly, another study assessed GPT-4’sp e r -\nformance in clinical decision-making for bipolar depression cases. GPT-4\nselected optimal treatments in 50.8% of cases, slightly outperforming\ncommunity clinicians\n19. Although promising, these results are based on\nhypothetical cases, and the model’s effectiveness in actual clinical practice\nremains unveriﬁed. Overall, while LLMs dem onstrate potential in\ngenerating clinically relevant information, the lack of clinical validation and\nreliance on simulated vignettes limit the evidence for their effectiveness in\nreal-world diagnostic support.\nSeveral studies have investigated the application of LLMs in aspects of\ntherapeutic interventions, particularly in cognitive restructuring and posi-\ntive psychology. Liu et al.\n14 conducted randomized controlled trials with 326\nparticipants to test GPT-based chatbots delivering Positive Psychology\nInterventions (PPIs). The chatbot provided personalized recommendations\nand engaged users in multi-round dialogues with resulting improvements in\nmental well-being, reductions in anxiety, and increased life satisfaction\nmetrics. This suggests that LLMs can effectively facilitate interventions\naimed at enhancing psychological well-being. Another study explored the\nuse of LLMs in self-reﬂective journaling among 28 psychiatric outpatients\ndiagnosed with Major Depressive Disorder\n22. Clinicians reported that the\nLLM-assisted journaling system enriched patient records and provided\nbetter insights into patients’conditions. In a large-scale randomized con-\ntrolled trial34, involving over 15,000 participants, Sharma et al. evaluated an\nLLM’s assistance in cognitive restructuring for self-guided mental health\ninterventions. The study found that 67% of participants reported reduced\nemotional intensity, and 65% overcamenegative thoughtsafter interacting\nwith the LLM. These results indicate the potential scalability and effec-\ntiveness of LLMs in supporting cognitive-behavioral techniques.\nLLMs have also been used to provide emotional support and enhance\nengagement, particularly among youth and marginalized populations,\nMármol-Romero et al. examined a GPT-based chatbot’s engagement with\nSpanish-speaking teenagers on mental health topics\n15. The observational\nstudy involved 102 students, and the chatbot facilitated open discussions on\nanxiety and depression. The engagement led to meaningful conversations\nwith 44 participants, indicating potential for early outreach and mental\nhealth education among adolescents. Another study investigated the use of\nthe Replika chatbot among 1006 students\n17. The study found that 3% of\nparticipants reported cessation of suicidal ideation after interacting with the\nchatbot, and 75% reported feeling less lonely, suggesting that LLM chatbots\ncan provide immediate emotional support.t. However, the lack of long-term\noutcomes from all studies is notable.\nEvaluation methods, scales, and constructs\nA standardized and well-established set of constructs and scales is essential\nin systematically measuring mental health interventions, particularly when\nevaluating new technologies. Constructs refer to speciﬁcc o n c e p t so rc h a r -\nacteristics being measured, such as privacy, safety, or user experience. They\nprovide a clear focus on what is being assessed in a study, which is crucial for\nensuring that the evaluation is meaningful and relevant. Scales, in turn, offer\na structured and standardized approach to quantify these constructs. This\nstandardization is necessary for consistency across different studies,\nallowing researchers to compare results and draw more robust conclusions.\nGiven the diversity in how constructs are deﬁned and measured across\nstudies, it is important to use a framework that can harmonize these var-\niations. While there are many app roaches, we used a hierarchical\nframework\n37 inspired by the American Psychiatric Association app eva-\nluation model. A 2024 review of evaluation models38 noted this framework\n“is straightforward, comprehensive,ﬂexible, and relevant to diverse con-\ntexts” and so also provides us a promising starting point. This framework\ncategorizes constructs into three levels: (1) Safety, Privacy, and Fairness; (2)\nTrustworthiness and Usefulness; and (3) Design and Operational Effec-\ntiveness. The pyramid framework ensures that each level of evaluation\nbuilds on the previous one. For example, without ensuring that an inter-\nvention is safe, it would be premature to evaluate its usability or cost-\neffectiveness.\nAmong the studies reviewed, those that involved direct participant\nfeedback (n =5 )\n14,17,18,22,23 generally focused on user-centric constructs.\nThese studies typically involved larger sample sizes ranging from 28 to over\n15,000 participants and assessed constructs such as accessibility, ease of use,\npersonalized engagement, user experience, and cost-effectiveness. They\nprovide direct insights into how user experience of LLMs is in real-world\nhttps://doi.org/10.1038/s41746-025-01611-4 Article\nnpj Digital Medicine|           (2025) 8:230 5\nsettings. On the other hand, studiesthat focused on evaluating LLM per-\nformance— typically involving expert assessments— concentrated more on\nfoundational and core efﬁcacy constructs. These studies often used smaller\nsample sizes, ranging from 12 to 100 cases, focusing on technical or func-\ntional aspects of the LLMs. Additionally, one study23 designed and incor-\nporated automated metrics for Rationality, Positivity, and Empathy, using\nNLP models to evaluate LLM outputs. These automated evaluations offer a\nmore detailed, algorithmic perspective on the LLM’s performance, com-\nplementing human judgments.\nThe heterogeneous use of scales remains a problem in the mental\nhealth ﬁeld. We observe that 12 studies developed their own\nscales15,18–21,23,26,28,32,34,35 or adapted existing ones for their evaluations. Most of\nthe studies using validated scales were those directly measuring patient\noutcomes, such as anxiety, where the General Anxiety Disorder-7 (GAD-7)\nwas employed\n14,32. However, many articles that created their own scales\nwithout clear rationale, and often lacked references to support their meth-\nods, raising challenges with the validity and reliability of their methods.\nFigure 1 presents a pyramid shaped schematic of the current status of\nevaluated constructs in the generative applications of LLMs for mental\nhealth care, based on the health AI-chatbot evaluation framework\n37.T h e\nﬁgure includes the number of articles counted for each level 2 construct, with\ngray texts indicating constructs never evaluated by existing research. The\nfoundational levels are less frequently assessed: only three studies evaluated\nthe fundamental construct“Safety, Privacy, and Fairness”; Thirteen studies\nassessed the second-level construct“Trustworthiness and Usefulness”;a n d\nanother 11 articles evaluated the third-level construct“Design and Opera-\ntional Effectiveness.” Although “Trustworthiness and Usefulness” is the\nmost evaluated category, more than half of its subconstructs remain unas-\nsessed. Across the framework, constructs such as “Accountability,”\n“Transparency,”“ Explainability and Interpretability, ”“ Testability,”\n“Security,” and “Resilience” have never been evaluated.\nDiscussion\nOur review suggests that there is great enthusiasm for LLM-based mental\nhealth interventions and that many teams are creating interesting and\nunique applications. We found these chatbots already developed to serve as\nclinical assistants, counselors, emotional support vehicles, and positive\npsychology interventions. However,despite the enthusiasm for applying\nLLMs in mental health care, the current evidence regarding their task per-\nformance and clinical effectiveness is limited and varies across studies. Many\nstudies lack rigorous clinical validation, standardized outcome measures,\nand adequate sample sizes, which hampers the ability to draw deﬁnitive\nconclusions. Furthermore, the inconsistent use and understanding of well\nestablished measurement methods across studies complicate the evaluation\nof these interventions. We observed that mental health constructs were\noften referenced without accompanying well established instruments and\nmeasurements, and in some cases, researchers tailored the deﬁnition or\nassessment toﬁt their speciﬁc AI models, leading to challenges in consistent\ncategorization. This inconsistencyunderscores a broader issue within the\ninterdisciplinaryﬁeld of AI and mental health— the variation in how con-\nstructs like affect, mood, diagnosis, and treatment are applied complicates\nefforts to maintain clear distinctionsbetween mental health constructs with\nand without validated measurements.\nThe evaluation of LLM-based mental health interventions is hindered\nby the lack of uniﬁed guidelines for scale development and reporting. While\nthis is appropriate for feasibility testing, it belies the ability to understand the\nactual clinical potential of these new chatbots. With the majority of studies\nusing non-well-established ad-hoc scales without addressing their validity\nand reliability, there is an opportunity for the next wave of research to better\nsupport the credibility and the need for guidelines to standardize reporting\na n ds c a l e su s e di nt h i sﬁeld. While effective evaluation is still nascent, results,\na ss h o w ni nt h et a b l e ,h i g h l i g h tt h a tthe current focus ignores foundational\nprivacy and safety concerns. LLM-based mental health chatbots are mul-\ntifaceted with privacy, technical, engagement, legal, and clinical considera-\ntions. Our team recently introduced a simpliﬁed framework to unify these\nmany evaluations, suggesting that safety and privacy should be the foun-\ndation of any evaluation\n37. This is not to minimize the value of evaluation of\ndesign and effectiveness (level 3) and usefulness and trustworthiness (level\n2), but rather that such should not be at the expense of priority over safety,\nprivacy, and fairness (level 1). Without these level 1 considerations, LLM-\nbased mental health interventions may be impressive but unﬁtf o rh e a l t h -\ncare or clinical use.\nOur results also show that the focus of current LLMs today is directed\nmore at patients and less at clinicians.This approach is logical as direct to\nconsumer/patient approaches often avoid complex healthcare regulations\nand clinical workﬂow barriers. However, thisapproach also risks frag-\nmenting the potential of LLM-based mental health interventions to inﬂu-\nence care as there is strong evidence that clinician engagement is required\nfor more sustained and impactful patient use with any digital technology\n10.\nThere is strong data that clinicians are interested in using LLMs in care, but\nﬁrst require and are asking for more training and support on how to use\nthese in care\n39.\nThe LLMs reviewed in this paper target a wide variety of disorders.\nOver half of the studies reviewed included clinically valid disorders, with\nother studies targeting general mentalhealth constructs. However, we found\nthat many studies did not offer sufﬁcient details on the target population,\nand the difference between mental health risk factors versus mental health\nconditions was poorly delineated. Weacknowledge that psychiatric nosol-\nogy is challenging, as highlighted in recent literature\n40, but this challenge\nhighlights how the evaluation of AI systems in mental health may quickly\nreach an impasse. For example, constructs like depression were often\nmentioned in a broad and non-speciﬁc manner, without reference to\ndiagnostic criteria or standardized and well-established metrics such as the\nPHQ-9 or GAD-7. This was particularly pronounced in studies conducted\nFig. 1 | Pyramid framework of evaluation con-\nstructs in generative applications of LLMs in\nmental health care.Constructs in gray represent\nconstructs with no associated articles.“N” repre-\nsents the number of unique articles that assessed\neach construct. Gray text indicates constructs that\nwere not assessed in any study. Foundational areas\nlike “Safety, Privacy, and Fairness” are rarely eval-\nuated, highlighting key gaps in critical aspects such\nas “Accountability,”“Transparency,” and\n“Security”.\nStep 1\nSafety, Privacy, and Fairness\n(n =56)\nPersonalized engagement\n(n=7)\nCost-Eﬀec/g415veness\n(n=3)\nSecuritySafety\n(n=2)\nFairness and Bias management\n(n=1)\nResilience\nStep 2\nTrustworthiness andUsefulness\n(n =107)\nGeneralizability\n(n=3)\nReliability\n(n=2)\nPrivacy\n(n=1)\nStep 3\nDesignand Opera/g415onal Eﬀec/g415veness\n(n =108)\nTransparency\nTestability\nValidity\n(n=4)\nBeneﬁcence\n(n=12)\nExplainability and Interpretability\nAccountability\nAccessibility\n(n=10)\nhttps://doi.org/10.1038/s41746-025-01611-4 Article\nnpj Digital Medicine|           (2025) 8:230 6\nby researchers outside the medical or clinical domains. Such inconsistent use\nof constructs and measurement methodscomplicates efforts to maintain a\nclear distinction between mental health constructs with and without vali-\ndated measures, calling attention to a broader issue within the inter-\ndisciplinaryﬁeld of AI and mental health. For example, one study speciﬁed a\npopulation of children and adolescents, ages between 12 and 18 years old15,\nbut overall, most studies lacked detailed demographic information. Given\nthat only one study emphasized data security, with conversations pro-\nceeding through a HIPAA-compliant environment\n18,t h el a c ko fm o r e\nclinical use cases is perhaps appropriate. Another issue is the dependence on\nproprietary models, such as OpenAI’s GPT-3.5 and GPT-4, in many mental\nhealth applications. This reliance raises concerns about transparency and\ncustomization, as the use of closed-source models limits external validation\nof reliability and safety, crucial in mental health research. To improve\nmeasurement speciﬁcity for speciﬁc populations or disorders, model pre-\ntraining andﬁne-tuning are key aspects to be considered\n41. More models\nand studies should include domain and audience-speciﬁcm o d e l sp r e -\ntrained on clinical data with more rigorous applications of standardized\ndiagnostic tools. Promoting the use of open-source models and improving\ntransparency can enhance the scientiﬁc and ethical standards of these\napplications.\nTo advance the scalability and scientiﬁc rigor of LLM-based mental\nhealth interventions, the research community must also adopt more con-\ntrolled methodologies. Some studies, particularly those utilizing ChatGPT,\nrely on the website interface for research purposes. While this approach is\nconvenient, it should be discouraged by rigorous scientiﬁc investigations.\nResearch should be conducted using the API, where hyperparameters such\nas the“temperature”can be controlled, ensuring replicability of the results.\nThe website interface should primarilybe used for testing third-level con-\nstructs such as Design and Operational Effectiveness and potentially\nassessing the safety and transparency of the user-facing system. However,\nresearchers must also address factorslike backend model updates and sto-\nchastic elements in the sampling process to ensure consistent reproduci-\nbility and reliability.\nFinally, the global applicabilityof LLM-based mental health tools\nwarrants careful consideration. Public health, especially mental health care,\nis a global issue, and it’s crucial to develop and deploy mental health chatbots\nin countries and regions where resources are limited and where stigma may\nbe higher. These areas often do not primarily speak English. It’s encouraging\nthat 10 out of the 17 studies (58.8%) support non-English languages, either\nin a single other language or as multilingual chatbots, which is a positive step\ntoward language equity and global health. But this also raises an issue,\nbeyond the scope of this paper, of whether these chatbots offer the same level\nof correctness, consistency, and veriﬁability as English-trained chatbots,\ngiven that research suggests this is often not the case\n42.\nFuture directions for LLMs in mental health care should prioritize\nexpanding their applications beyondnarrow prediction tasks, especially\ngiven that only 17 studies over the pastﬁve years have explored generative\ntasks prospectively involving human participants for evaluation. Human-\ncentered studies provide critical insights into how LLMs interact with\nindividuals, particularly in sensitivecontexts like mental health care, where\nnuances in communication and emotional understanding are vital.\nAddressing current limitations such as small sample sizes and lack of diverse\nparticipant demographics, future research should employ larger, more\nrepresentative samples to enhance the generalizability ofﬁndings. To\nimprove the rigor and credibility of LLM-based mental health interventions,\nstudies should prioritize the development of standardized evaluation\nguidelines. These guidelines shouldinclude the creation of validated and\nreliable scales that can be universallyapplied across studies, ensuring con-\nsistent and accurate assessments of clinical potential. By standardizing\nevaluation metrics, researchers can overcome the variability that currently\nimpedes comparability and synthesis ofresults across different studies. To\nenhance transparency and overcome the limitations of proprietary models,\nresearchers should move away from using web interfaces like ChatGPT for\nrigorous scientiﬁcs t u d i e s ,a st h e s ep l a t f o r m sl a c kt h en e c e s s a r yc o n t r o l sf o r\nreproducibility. Instead, APIs and locally deployable models that allow for\ncontrol over hyperparameters should be used to ensure the replicability of\nthe results. This approach will mitigate concerns about reproducibility and\nallow for more precise manipulation of model parameters, leading to more\nreliable outcomes. Finally, studies focused on critical constructs such as\nbeneﬁcence, validity, and reproducibility should adopt rigorous evaluation\nmethods and well-established scales,moving beyond metrics like recall and\nF1 scores, to establish a more comprehensive understanding of model\naccuracy and clinical relevance. Incorporating ethical considerations and\naddressing privacy and safety concerns in study designs will also enhance\nthe trustworthiness of LLM applications in mental health care. Equally\nimportant is the advancement of novelmethodologies and rigorous stan-\ndards to ensure fairness. A recent study has demonstrated strategies to\nmitigate biases and promote equity inLLM applications, including assessing\ndemographic disparities in empathy,the implementing demographic-aware\nprompting, and evaluating subgroup performance in mental health con-\ntexts. Future studies should explore new fairness metrics tailored speciﬁcally\nto mental health contexts, such as cultural adaptability or intersectional\nbiases\n43.\nW ew o u l dl i k et oa c k n o w l e d g et h el i m i t a t i o n so ft h ee v i d e n c ei nt h i s\nreview, which are primarily rooted in the absence of standardized evaluation\ncriteria across studies, resulting in challenges for comparison and synthesis\nof ﬁndings. Many studies depend on non-well-established, ad-hoc scales\nwithout thorough clinical validation,which undermines the robustness and\ngeneralizability of their conclusions. Furthermore, the frequent use of\nproprietary LLMs, such as OpenAI’s GPT series, introduces issues of\ntransparency and reproducibility, as closed-source settings hinder inde-\npendent veriﬁcation and limit replicability. The review processes used also\nhave limitations, as inconsistent reporting practices lead to gaps in essential\nmetrics, demographic detail, and evaluation frameworks, all of which are\ncritical for cross-study analysis. Collectively, these factors highlight an\nurgent need for a uniﬁed, rigorous framework to assess and validate LLM\napplications in mental health systematically. Addressing these gaps through\nstandardization will be essential for improving the reliability ofﬁndings and\nensuring that LLMs contribute me aningfully and safely to mental\nhealth care.\nMethods\nWe adhered to the Preferred Reporting Items for Systematic Reviews and\nMeta-Analyses (PRISMA) 2020 guidelines\n44 to ensure a transparent and\nreproducible search process (Fig.2). Our search included four databases:\nAPA PsycNet, Scopus, PubMed, and Web of Science. To ensure compre-\nhensiveness, we employed a combination of generative AI keywords and\nLLM keywords, and used the shortest matching string to capture all lexical\nvariations. Our search query was as follows, with different variations used\nacross database platforms (detailed in Supplementary Table 1):\n(“generative artiﬁcial intelligence” OR “large language models” OR\n“generative model” OR “chatbot”)A N D(“mental” OR “psychiatr”\nOR “psycho” OR “emotional support”)\nWe conducted the search in the title or abstract of articles, covering the\nperiod from January 1, 2020, to July 19, 2024, without language restrictions.\nThe search results included 259 articles from PubMed, 444 articles from\nScopus, 1 article from APA PsycNet (PsychInfo and PsycArticles), and 500\narticles from Web of Science. The initial search yielded 1,204 articles, with 14\nadditional articles identiﬁed from sources such as Google Scholar, the ACM\nDigital Library, and reverse referencing. After removing 492 duplicates, we\nwere left with a total of 726 unique articles.\nWe applied the following inclusion criteria to select studies for our\nreview: ﬁrst, the study must involve using an LLM to generate responses\n(generative task); second, the study must focus speciﬁcally on mental health\ncare, distinguishing it from studies in relatedﬁelds like psycholinguistics;\nthird, the study must have human validations rather than relying purely on\nautomated evaluation. An LLM is deﬁned as“transformer-based models\nhttps://doi.org/10.1038/s41746-025-01611-4 Article\nnpj Digital Medicine|           (2025) 8:230 7\nwith more than ten billion parameters, which are trained on massive text\ndata and excel at a variety of complex generation tasks.” in this study,\nfollowing a highly cited review from the NLP community45.W ee x c l u d e d\nreviews, meta-analyses, and clinical trials from our selection. Then, we\nfurther removed seven studies not meet our inclusion criteria upon full-text\nreview. The result analysis review includes 16 articles, with 15 full-text-\nlength papers and one brief communication paper.\nData extraction was conducted by one or two authors for each section,\nwith a second author independently reviewing for accuracy. For mental\nhealth conditions, data were extracted to categorize disorders, symptoms,\ncare settings, interventions, assessments, and diagnostic sources, with a\ndistinction made between clinically validated disorders and general mental\nhealth constructs. For applications and model details, we extracted data on\ninput/output modalities, model types, embodiment, open-source avail-\nability, and target user populations. Regarding tasks and clinical effective-\nness, we collected data on the primarytasks involving LLMs, sample sizes,\ndemographic characteristics, and methods of clinical validation. Evaluation\nmethods were categorized, with constructs mapped to a hierarchical eva-\nluation framework, producing a harmonized pyramid to systematically\nassess LLMs across various levels of evidence. Further details on the\nscreening process, data extraction, and synthesis are provided in Supple-\nmentary Note 1.\nData availability\nAll data associated with this study has been made available in appendices.\nCode availability\nNot applicable.\nReceived: 20 August 2024; Accepted: 2 April 2025;\nReferences\n1. World Health Organization. One in 100 deaths is by suicide - WHO\nguidance to help the world reach the target of reducing suicide rate by\n1/3 by 2030.https://www.who.int/news/item/17-06-2021-one-in-\n100-deaths-is-by-suicide (2021).\n2. National Institutes of Health. FY 2023 Budget - Congressional\nJustiﬁcation - National Institute of Mental Health (NIMH).https://www.\nnimh.nih.gov/about/budget/fy-2023-budget-congressional-\njustiﬁcation (2023).\n3. Satiani, A., Niedermier, J., Satiani, B. & Svendsen, D. P. Projected\nWorkforce of Psychiatrists in the United States: A Population Analysis.\nPsychiatr. Serv.69, 710– 713 (2018).\n4. Mongelli, F., Georgakopoulos, P. & Pato, M. T. Challenges and\nOpportunities to Meet the Mental Health Needs of Underserved and\nDisenfranchised Populations in the United States.FOC 18,1 6–24\n(2020).\n5. World Health Organization. WHO Special Initiative for Mental Health.\nhttps://www.who.int/initiatives/who-special-initiative-for-mental-\nhealth.\nFig. 2 | The PRISMAﬁgure of the search and\nscreening process.The PRISMA diagram shows the\nsystematic process of study selection. Of 1204 arti-\ncles initially identiﬁed across databases, 726 unique\nrecords remained after duplicates were removed.\nFurther screening yielded 16 articles meeting the\ninclusion criteria.\nhttps://doi.org/10.1038/s41746-025-01611-4 Article\nnpj Digital Medicine|           (2025) 8:230 8\n6. Goodings, L., Ellis, D. & Tucker, I.Understanding Mental Health Apps:\nAn Applied Psychosocial Perspective. (Springer Nature, 2024).\n7. Torous, J. et al. The growingﬁeld of digital psychiatry: current\nevidence and the future of apps, social media, chatbots, and virtual\nreality. World Psychiatry20, 318–335 (2021).\n8. Zhang, M. et al. The Adoption of AI in Mental Health\nCare–Perspectives From Mental Health Professionals: Qualitative\nDescriptive Study.JMIR Form. Res.7, e47847 (2023).\n9. OpenAI et al. GPT-4 Technical Report. Preprint athttps://doi.org/10.\n48550/arXiv.2303.08774 (2024).\n10. Google. An important next step on our AI journey.https://blog.google/\ntechnology/ai/bard-google-ai-search-updates/ (2023).\n11. Anthropic. Introducing Claude.https://www.anthropic.com/news/\nintroducing-claude.\n12. Hua, Y. et al. Large Language Models in Mental Health Care: a\nScoping Review. Preprint athttps://doi.org/10.48550/arXiv.2401.\n02984 (2024).\n13. Diagnostic and Statistical Manual of Mental Disorders: DSM-5TM, 5th\nEd. xliv, 947 (American Psychiatric Publishing, Inc., Arlington, VA, US,\n2013). https://doi.org/10.1176/appi.books.9780890425596.\n14. Liu, I. et al. Investigating the Key Success Factors of Chatbot-Based\nPositive Psychology Intervention with Retrieval- and Generative Pre-\nTrained Transformer (GPT)-Based Chatbots.Int. J. Human–Comput.\nInteract. (2024).\n15. Mármol-Romero, A. M., García-Vega, M., García-Cumbreras, M. Á. &\nMontejo-Ráez, A. An Empathic GPT-Based Chatbot to Talk About\nMental Disorders With Spanish Teenagers.Int. J. Human–Comput.\nInteract. 1–17. https://doi.org/10.1080/10447318.2024.2344355.\n16. Kim, T. et al. MindfulDiary: Harnessing Large Language Model to\nSupport Psychiatric Patients’ Journaling. inProceedings of the CHI\nConference on Human Factors in Computing Systems1–20\n(Association for Computing Machinery, New York, NY, USA, 2024).\nhttps://doi.org/10.1145/3613904.3642937.\n17. Maples, B., Cerit, M., Vishwanath, A. & Pea, R. Loneliness and suicide\nmitigation for students using GPT3-enabled chatbots.npj Ment.\nHealth Res3,1 –6 (2024).\n18. Spiegel, B. M. R. et al. Feasibility of combining spatial computing and\nAI for mental health support in anxiety and depression.npj Digit. Med.\n7,1 –5 (2024).\n19. Perlis, R. H., Goldberg, J. F., Ostacher, M. J. & Schneck, C. D. Clinical\ndecision support for bipolar depression using large language models.\nNeuropsychopharmacol 49, 1412–1416 (2024).\n20. Berrezueta-Guzman, S., Kandil, M., Martín-Ruiz, M.-L., de la Cruz, I. P.\n& Krusche, S. Exploring the Efﬁcacy of Robotic Assistants with\nChatGPT and Claude in Enhancing ADHD Therapy: Innovating\nTreatment Paradigms. in2024 International Conference on Intelligent\nEnvironments (IE)25–32 https://doi.org/10.1109/IE61493.2024.\n10599903 (2024).\n21. Llanes-Jurado, J., Gómez-Zaragozá, L., Minissi, M. E., Alcañiz, M. &\nMarín-Morales, J. Developing conversational Virtual Humans for\nsocial emotion elicitation based on large language models.Expert\nSyst. Appl. 246, (2024).\n22. Sharma, A., Rushton, K., Lin, I. W., Nguyen, T. & Althoff, T. Facilitating\nSelf-Guided Mental Health Interventions Through Human-Language\nModel Interaction: A Case Study of Cognitive Restructuring. In\nProceedings of the CHI Conference on Human Factors in Computing\nSystems 1–29 (Association for Computing Machinery, New York, NY,\nUSA, 2024).https://doi.org/10.1145/3613904.3642761.\n23. Sharma, A. et al. Cognitive Reframing of Negative Thoughts through\nHuman-Language Model Interaction. InProceedings of the 61st\nAnnual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers)(eds. Rogers, A., Boyd-Graber, J. & Okazaki,\nN.) 9977–10000 (Association for Computational Linguistics, Toronto,\nCanada, 2023).https://doi.org/10.18653/v1/2023.acl-long.555.\n24. Grabb, D. The impact of prompt engineering in large language model\nperformance: a psychiatric example.J. Med. Artif. Intell.6\n, (2023).\n25. Liu, Y., Ding, X., Peng, S. & Zhang, C. Leveraging ChatGPT to optimize\ndepression intervention through explainable deep learning.Front.\nPsychiatry 15, (2024).\n26. Lai, T. et al. Supporting the Demand on Mental Health Services with\nAI-Based Conversational Large Language Models (LLMs).\nBioMedInformatics 4,8 –33 (2024).\n2 7 . S h a r m a ,A . ,L i n ,I .W . ,M i n e r ,A .S . ,A t k i n s ,D .C .&A l t h o f f ,T .\nHuman– AI collaboration enables more empathic conversations in\ntext-based peer-to-peer mental health support.Nat. Mach. Intell.5,\n46– 57 (2023).\n28. Franco D’Souza, R., Amanullah, S., Mathew, M. & Surapaneni, K. M.\nAppraising the performance of ChatGPT in psychiatry using 100\nclinical case vignettes.Asian J. Psychiatr.89, 103770 (2023).\n29. Ni, Y., Chen, Y., Ding, R. & Ni, S. Beatrice: A Chatbot for Collecting\nPsychoecological Data and Providing QA Capabilities. inProceedings\nof the 16th International Conference on PErvasive Technologies\nRelated to Assistive Environments429–435 (Association for\nComputing Machinery, New York, NY, USA, 2023).https://doi.org/10.\n1145/3594806.3596580.\n30. Beck, J. S.Cognitive Behavior Therapy: Basics and beyond, 2nd Ed.\nxix, 391 (Guilford Press, New York, NY, US, 2011).\n31. Kim, M., Hwang, K., Oh, H., Kim, H. & Kim, M. A. Can a Chatbot be\nUseful in Childhood Cancer Survivorship? Development of a Chatbot\nfor Survivors of Childhood Cancer. inProceedings of the 32nd ACM\nInternational Conference on Information and Knowledge Management\n4018–4022 (Association for Computing Machinery, New York, NY,\nUSA, 2023).https://doi.org/10.1145/3583780.3615234.\n32. Qian, Y., Zhang, W. & Liu, T. Harnessing the Power of Large Language\nModels for Empathetic Response Generation: Empirical Investigations\nand Improvements. inFindings of the Association for Computational\nLinguistics: EMNLP 2023(eds. Bouamor, H., Pino, J. & Bali, K.)\n6516–6528 (Association for Computational Linguistics, Singapore,\n2023). https://doi.org/10.18653/v1/2023.ﬁndings-emnlp.433.\n33. Holderried, F. et al. A Generative Pretrained Transformer (GPT)-\nPowered Chatbot as a Simulated Patient to Practice History Taking:\nProspective, Mixed Methods Study.JMIR Med Educ.10, e53961\n(2024).\n34. Vowels, L. M., Francois-Walcott, R. R. R. & Darwiche, J. AI in\nrelationship counselling: Evaluating ChatGPT’s therapeutic\ncapabilities in providing relationship advice.Computers Hum. Behav.:\nArtif. Hum.2, 100078 (2024).\n35. Berrezueta-Guzman, S., Kandil, M., Martín-Ruiz, M.-L., Pau de la\nCruz, I. & Krusche, S. Future of ADHD Care: Evaluating the Efﬁcacy of\nChatGPT in Therapy Enhancement.Healthcare 12, 683 (2024).\n36. Zhang, Y. et al. DIALOGPT: Large-Scale Generative Pre-training for\nConversational Response Generation. inProceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics: System\nDemonstrations270–278 (Association for Computational Linguistics,\nOnline, 2020).https://doi.org/10.18653/v1/2020.acl-demos.30.\n37. Hua, Y. et al. Standardizing and Scaffolding Healthcare AI-Chatbot\nEvaluation. 2024.07.21.24310774 Preprint athttps://doi.org/10.1101/\n2024.07.21.24310774 (2024).\n38. Cozad, M. J. et al. Mobile Health Apps for Patient-Centered Care:\nReview of United States Rheumatoid Arthritis Apps for Engagement\nand Activation.JMIR mHealth uHealth10, e39881 (2022).\n39. Mirzaei, T., Amini, L. & Esmaeilzadeh, P. Clinician voices on ethics of\nLLM integration in healthcare: a thematic analysis of ethical concerns\nand implications.BMC Med Inf. Decis. Mak.24, 250 (2024).\n40. Leucht, S., van Os, J., Jäger, M. & Davis, J. M. Prioritization of\nPsychopathological Symptoms and Clinical Characterization in\nPsychiatric Diagnoses: A Narrative Review.JAMA Psychiatryhttps://\ndoi.org/10.1001/jamapsychiatry.2024.2652 (2024).\nhttps://doi.org/10.1038/s41746-025-01611-4 Article\nnpj Digital Medicine|           (2025) 8:230 9\n41. Ji, S., Zhang, T., Yang, K., Ananiadou, S. & Cambria, E. Rethinking\nLarge Language Models in Mental Health Applications. Preprint at\nhttp://arxiv.org/abs/2311.11267 (2023)\n42. Jin, Y. et al. Better to Ask in English: Cross-Lingual Evaluation of Large\nLanguage Models for Healthcare Queries. inProceedings of the ACM\non Web Conference 20242627–2638 (ACM, Singapore Singapore,\n2024). https://doi.org/10.1145/3589334.3645643.\n43. Gabriel, S., Puri, I., Xu, X., Malgaroli, M. & Ghassemi, M. Can AI Relate:\nTesting Large Language Model Response for Mental Health Support.\nin Findings of the Association for Computational Linguistics: EMNLP\n2024 (eds. Al-Onaizan, Y., Bansal, M. & Chen, Y.-N.) 2206–2221\n(Association for Computational Linguistics, Miami, Florida, USA,\n2024). https://doi.org/10.18653/v1/2024.ﬁndings-emnlp.120.\n44. Page, M. J. et al. The PRISMA 2020 statement: an updated guideline\nfor reporting systematic reviews.BMJ 372, n71 (2021).\n45. Zhao, W. X. et al. A Survey of Large Language Models. Preprint at\nhttps://doi.org/10.48550/arXiv.2303.18223 (2023).\n46. Watson, D., Clark, L. A. & Tellegen, A. Development and validation of\nbrief measures of positive and negative affect: The PANAS scales.J.\nPersonal. Soc. Psychol.54, 1063–1070 (1988).\n47. William Li, H. C., Chung, O. K. J. & Ho, K. Y. Center for Epidemiologic\nStudies Depression Scale for Children: psychometric testing of the\nChinese version.J. Adv. Nurs.66, 2582–2591 (2010).\n48. Kroenke, K., Spitzer, R. L. & Williams, J. B. The PHQ-9: validity of a brief\ndepression severity measure.J. Gen. Intern Med16, 606–613 (2001).\n49. Diener, E. Satisfaction With Life Scale.https://doi.org/10.1037/\nt01069-000 (2011).\n50. Merz, E. L. et al. Validation of interpersonal support evaluation list-12\n(ISEL-12) scores among English- and Spanish-speaking Hispanics/\nLatinos from the HCHS/SOL Sociocultural Ancillary Study.Psychol.\nAssess. 26, 384– 394 (2014).\n51. De Jong Gierveld, J. & Van Tilburg, T. The De Jong Gierveld short\nscales for emotional and social loneliness: tested on data from 7\ncountries in the UN generations and gender surveys.Eur. J. Ageing7,\n121–130 (2010).\n52. Monticone, M. et al. The 27-item coping strategies questionnaire-\nrevised: conﬁrmatory factor analysis, reliability and validity in Italian-\nspeaking subjects with chronic pain.Pain. Res Manag19, 153–158\n(2014).\n53. Akin, A. The Scales of Psychological Well-Being: A Study of Validity\nand Reliability.Educ. Sci.: Theory Pract.8, 741–750 (2008).\n54. Ryan, R. M. & Frederick, C. On energy, personality, and health:\nSubjective vitality as a dynamic reﬂection of well-being.J. Personal.\n65, 529–\n565 (1997).\nAcknowledgements\nThis study received no funding.\nAuthor contributions\nStudy design: Y.H., J.T.; Screening, data extraction, data analysis: Y.H.,\nH.N., Z.L., F.L.; Validation: Y.H. and J.T.; Manuscript drafting, feedback,\nrevision: Y.H., H.N., Z.L., F.L., X.F., D.C., J.T. Supervision: J.T.\nCompeting interests\nJ.T. has research support from Otsuka and is an adviser to Precision Mental\nWellness. He is also an assistant editor for NPJ Digital Medicine. All other\nauthors have no conﬂict of interest.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s41746-025-01611-4.\nCorrespondenceand requests for materials should be addressed to\nJohn Torous.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution-NonCommercial-NoDerivatives 4.0 International License,\nwhich permits any non-commercial use, sharing, distribution and\nreproduction in any medium or format, as long as you give appropriate\ncredit to the original author(s) and the source, provide a link to the Creative\nCommons licence, and indicate if you modiﬁed the licensed material. You\ndo not have permission under this licence to share adapted material\nderived from this article or parts of it. The images or other third party\nmaterial in this article are included in the article’s Creative Commons\nlicence, unless indicated otherwise in a credit line to the material. If material\nis not included in the article’s Creative Commons licence and your intended\nuse is not permitted by statutory regulation or exceeds the permitted use,\nyou will need to obtain permission directly from the copyright holder. To\nview a copy of this licence, visithttp://creativecommons.org/licenses/by-\nnc-nd/4.0/\n.\n© The Author(s) 2025\nhttps://doi.org/10.1038/s41746-025-01611-4 Article\nnpj Digital Medicine|           (2025) 8:230 10",
  "topic": "Generative grammar",
  "concepts": [
    {
      "name": "Generative grammar",
      "score": 0.7231442928314209
    },
    {
      "name": "Mental health",
      "score": 0.5633050203323364
    },
    {
      "name": "Psychology",
      "score": 0.5507583618164062
    },
    {
      "name": "Mental health care",
      "score": 0.44230595231056213
    },
    {
      "name": "Linguistics",
      "score": 0.38878393173217773
    },
    {
      "name": "Psychotherapist",
      "score": 0.24269047379493713
    },
    {
      "name": "Philosophy",
      "score": 0.06532308459281921
    }
  ]
}