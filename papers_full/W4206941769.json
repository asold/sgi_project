{
  "title": "AraBERT transformer model for Arabic comments and reviews analysis",
  "url": "https://openalex.org/W4206941769",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A94893030",
      "name": "Hicham El Moubtahij",
      "affiliations": [
        "Université Ibn Zohr"
      ]
    },
    {
      "id": "https://openalex.org/A2776360841",
      "name": "Hajar Abdelali",
      "affiliations": [
        "Sidi Mohamed Ben Abdellah University"
      ]
    },
    {
      "id": "https://openalex.org/A1998016753",
      "name": "El Bachir Tazi",
      "affiliations": [
        "Sidi Mohamed Ben Abdellah University"
      ]
    },
    {
      "id": "https://openalex.org/A94893030",
      "name": "Hicham El Moubtahij",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2776360841",
      "name": "Hajar Abdelali",
      "affiliations": [
        "Sidi Mohamed Ben Abdellah University"
      ]
    },
    {
      "id": "https://openalex.org/A1998016753",
      "name": "El Bachir Tazi",
      "affiliations": [
        "Sidi Mohamed Ben Abdellah University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2737990573",
    "https://openalex.org/W3131140755",
    "https://openalex.org/W2915357442",
    "https://openalex.org/W2990615728",
    "https://openalex.org/W3008110149",
    "https://openalex.org/W2983040767",
    "https://openalex.org/W3000779003",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2780932362",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2975059944",
    "https://openalex.org/W3085717538",
    "https://openalex.org/W3116878825",
    "https://openalex.org/W3186905198",
    "https://openalex.org/W3168040464",
    "https://openalex.org/W2471147443",
    "https://openalex.org/W2963979492",
    "https://openalex.org/W3017352210",
    "https://openalex.org/W6680762825",
    "https://openalex.org/W2239389665",
    "https://openalex.org/W2584262649",
    "https://openalex.org/W2621199241",
    "https://openalex.org/W2250594687",
    "https://openalex.org/W2948433920",
    "https://openalex.org/W3153540814",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4287900772",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3127469975",
    "https://openalex.org/W4294367149",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W4288347888",
    "https://openalex.org/W3103339821",
    "https://openalex.org/W2138923868",
    "https://openalex.org/W4294170691"
  ],
  "abstract": "Arabic language is rich and complex in terms of word morphology compared to other Latin languages. Recently, natural language processing (NLP) field emerges with many researches targeting Arabic language understanding (ALU). In this context, this work presents our developed approach based on the Arabic bidirectional encoder representations from transformers (AraBERT) model where the main required steps are presented in detail. We started by the input text pre-processing, which is, then, segmented using the Farasa segmentation technique. In the next step, the AraBERT model is implemented with the pertinent parameters. The performance of our approach has been evaluated using the ARev dataset which contains more than 40,000 comments-remarks records relate to the tourism sector such as hotel reviews, restaurant reviews and others. Moreover, the obtained results are deeply compared with other relevant states of the art methods, and it shows the competitiveness of our approach that gives important results that can serve as a guide for further improvements in this field.",
  "full_text": "IAES International Journal of Artificial Intelligence (IJ-AI) \nVol. 11, No. 1, March 2022, pp. 379~387 \nISSN: 2252-8938, DOI: 10.11591/ijai.v11.i1.pp379-387 \n      379  \n \nJournal homepage: http://ijai.iaescore.com \nAraBERT transformer model  for Arabic comments and \nreviews analysis  \n \n \nHicham El Moubtahij1, Hajar Abdelali2, El Bachir Tazi3 \n1Systems and Technologies of Information Team, High School of Technology, University of Ibn Zohr, Agadir, Morocco \n2LISAC Laboratory, Faculty of Sciences Dhar Mahraz, University of Sidi Mohamed Ben Abdellah, Fez, Morocco \n3Computer Science department, Polydisciplinary Faculty, University of Sidi Mohamed Ben Abdellah, Taza, Morocco \n \n \nArticle Info  ABSTRACT \nArticle history: \nReceived Sep 27, 2021 \nRevised Dec 24, 2021 \nAccepted Jan 4, 2022 \n \n Arabic language is rich and complex in terms of word morphology \ncompared to other Latin languages. Recently, natural language processing \n(NLP) field emerges with many researches targeting Arabic language \nunderstanding (ALU). In this context, this work pres ents our developed \napproach based on the Arabic bidirectional encoder representations from \ntransformers (AraBERT) model where the main required steps are presented \nin detail. We started by the input text pre -processing, which is, then, \nsegmented using the Farasa segmentation technique. In the next step, the \nAraBERT model is implemented with the pertinent parameters. The \nperformance of our approach has been evaluated using the ARev dataset \nwhich contains more than 40,000 comments -remarks records relate to th e \ntourism sector such as hotel reviews, restaurant reviews and others. \nMoreover, the obtained results are deeply compared with other relevant \nstates of the art methods, and it shows the competitiveness of our approach \nthat gives important results that can serve as a guide for further \nimprovements in this field. \nKeywords: \nAraBERT  \nArabic language understanding \nFarasa segmentation \nNatural language processing \nThis is an open access article under the CC BY-SA license. \n \nCorresponding Author: \nHicham El Moubtahij \nSystems and Technologies of Information Team, High School of Technology, University of Ibn Zohr  \nAgadir, Morocco \nEmail: h.elmoubtahij@uiz.ac.ma \n \n \n1. INTRODUCTION \nArabic is an international language, spoken by more than 500 million speakers. It is considered as \none of the important Semitic languages family. From the Arabian gulf to the atlantic ocean , Arabic language \nis administrative and official language of more t he 21 countries [1]. Arabic is a rich and complex language in \nterms of wor d morphology compared to English, the presence of various dialects is some of the \ndistinguishing prominent factors in the language. Moreover, the large differences between the modern \nstandard Arabic (MSA) and the dialectical Arabic (DA) increase this compl exity. It should be noted that \nMSA is employed for formal (administrative) writing and DA is employed for informal daily communication \non social media for example [2]. From the work of Guellil et al. [3] published in 2021, the DA is divided into \nsix collections: i) Maghrebi (MAGH), ii) Egyptian (EGY), iii) Iraqi (IRQ), iv) Levantine (LEV), v) Gulf \n(GLF), and vi) others remaining dialect. On the other hand, the Arabic language used on short messaging \nsystem (SMS), chat forums and on social media generally is called \"Arabizi\" [4]. Its written text is a mixture \nof Latin characters, numerals and some punc tuation. For example, the sentence: \"يا لاه نسافرو that is \ntranslated into English as \"let's travel\", is written in Arabizi form as \"yallah nsaaferou\" [5].  \nDespite its spread usage, there is little research i n the field of modern computational linguistics \ninterested in the Arabic language compared to other language. However, in the last years, several research \n\n      \n           ISSN: 2252-8938 \nInt J Artif Intell, Vol. 11, No. 1, March 2022: 379-387 \n380 \nefforts has been made and many paper appear in various language processing tasks. Practically, the named \nentity recognition  (NER) and the sentiment analysis (SA) are the most difficult tasks of Arabic natural \nlanguage processing (ANLP) [6]. \nIn order to obtain satisfactory results with tolerable performance for ANLP tasks, research works of \nthe last years have focused on the application of transfer learning by the fine -tuning of large pre -trained \nlanguage models with a relatively small number of samples. It should be mentioned that this approach is \nbased on a self-supervised pre-trained language models. They allow us to represent the set of words as dense \nvectors in a vecto r space of minimum dimension and construct continuous distributed representations for \ntexts. Despite the effectiveness of word embedding, it is unable to take into account the relationship between \nseveral words and the meaning of complete sentences in the text. Seeing the next two sentences , \" نفسها المرأة\nهذه \" .On the one hand, their word embedding representations are identical, and on the other hand, their \nmeanings are entirely different. However, the high computational cost is a disadvantage in the trai ning phase \nof the models (more than 500 TPU working for weeks). Moreover, a huge corpus is needed for the pre -\ntraining phase [7], [8]. \nIn this work, we define and describe the important process and steps of our approach base on Arabic \nbidirectional encoder representations from transformers (AraBERT) transformer model for the Arabic \nlanguage understanding (ALU). We can effectively classify the comments and the reviews into positive and \nnegative categories. Hence, we evalua ted our model on ARev dataset which contains more than 40 ,000 \ncomments, hotel, restaurant, product, attraction and movie reviews written on a mixture of standard Arabic \nand Algerian dialect. The experiments show that our approach achieves very good results. \nThis reminder of this paper is structured as: in section 2, we present the most important techniques \nand approaches used in the natural language processing (NLP) field to deal with the ALU problem. Then, in \nsection 3 we describe and clarify our model’s architecture where BERT represents its basic core. In section 4, \nwe describe the ARev dataset on which we perform our experiments, then we compare our results with those \nof relevant methods. Finally, section 5 concludes the paper and outlines the main points of our future works.  \n \n \n2. RELATED WORKS \nThere are various techniques and approaches used in NLP to solve the problem of ALU. In this \nsection, we briefly present some work in this field.  The first work on the meaning of words began in 2013 \nwith the word2vec model developed by Mikolov et al. [9], then researc hers are oriented towards variants of \nword2vec like GloVe by Pennington et al. [10] in 2014 and fast-text by Mikolov et al. [11] in 2017. By the \nintroduction of the concept of \"contextual information\" in 2018, the results were imp roved noticeably on \ndifferent tasks [12], increasingly the structures became larger which had super ior representations of words \nand sentences. From this date, the famous models of language comprehension have been developed, for \nexample: i) bidirectional encoder representations from transformers ( BERT) [13], ii) universal language \nmodel fine -tuning (ULMFiT)  [14], iii) text-to-text transfer transformer (T5) [15], iv) A Lite BERT \n(ALBERT) [16]. These offered improved performance b y exploring different pre-training methods, modified \nmodel architectures and larger learning corpora. \nConcerning the AraBERT model, we note that there is little work done in relation to other \nlanguages. In the following we quote some in chronological order . In 2020, Nada et al. [17] proposed a new \napproach for Arabic text summarizer founded  on a general -purpose architecture for natural language \nunderstanding (NLU), and natural language generation (NLG) : generation and understanding of natural \nlanguage to summarize the Arabic text by extracting and evaluating the most important sentences at this text.  \nAlami, a member of the LISAC FSDM -USMBA team at SemEval-2020 [18], proposed an effective \nmethod for dealing with the offensive Arabic language in Twitter by using AraBERT embeddings. In the \nFirst, they started with pre -processing tweets by h andling emojis (containing their Arabic meanings), in the \nnext, they substituted each detected emojis by the special token (MASK) into both fine-tuning and inference \nphases. Then, by applying the AraBERT model they represent tweets tokens. Finally, to deci de whether a \ntweet is offensive or not, they feed the tweet representation into a sigmoid function. There proposed method \nachieved the best results, a score equal to 90.17% on OffensEval 2020. \nIn the next year, Faraj and Abdullah [19] published the best solution for the shared task on sentiment \nand sarcasm detection in th e Arabic language. The objective global of the task is to identify whether a tweet \nis sarcastic or not. The pr oposed solution is based on the ensemble technique with AraBERT pre -trained \nmodel. In their paper, they started by defining the architecture of the model in the shared task. In the next, the \nhyperparameter and the experiment tuning that lead to this result  are presented in detail. Their model is \nranked 5th out of 27 teams with an F1 score of 0.5985. \nIn the recent work of 2021, Hussein et al. [20] worked on an effective approach for fighting Tweets \nCOVID-19 Infodemic by using the AraBERT model. The organisation of their approach is: in the first step, \nInt J Artif Intell  ISSN: 2252-8938 \n  \n \nAraBERT transformer model  for Arabic comments and reviews analysis (Hicham El Moubtahij) \n381 \nthe goal is to transform  Twitter jargon, including emoticons and emojis, into plain text by involving a \nsequence of pre -processing procedures, and they exploited a version of AraBERT in the second step, which \nwas pre-trained on plain text, to fine -tune and classify the tweets con cerning their Label. Their approach can \nbe predict 7 binary properties of an Arabic tweet about COVID-19. By using the dataset provided by NLP4IF \n2021, they ranked 5th in the Fighting the COVID-19 Infodemic task results with an F1 of 0.664. \n \n \n3. METHODOLOGY \nThe objective of this section is to describe and clarify the architecture of our model based on the \nAraBERT model, where BERT represents the basic core.  Subsection 3.1 show BERT model. Our model \nbased on AraBERT see in subsection 3.2. \n \n3.1.  BERT model \nBERT stands for bidirectional encoder representations from transformers , it came out of Google  AI \nlabs in late 2018. We mention that it is:  i) more powerful than its predecessors in terms of results ; ii) more \npowerful than its predecessors in terms of learning speed; iii) once pre-trained, in an unsupervised way, it has \nits own linguistic “representation”. It can be trained in incremental mode (in a supervised way this time) to \nspecialize the model quickly and with little data; and iv) f inally, it can work in a multi-model way, taking as \ninput data of different types such as images and/or text, with some manipulations.  It has the advantage over \nits competitors OpenAI's generative pre -trained transformer (GPT)  and embeddings from language models \n(ELMo) [12] of being bi -directional, it does not have to look only backwards like OpenAI GPT or \nconcatenate the “back” view and the “front” view driven independently like for ELMo, as shown in Figure 1. \n \n \n \n \nFigure 1. Differences in pre-training model architectures \n \n \nExamples of what it can do:  i) BERT can do the translation. He can even once pre-trained to translate \n[French/English-English/French] and then [English/German -German/English], translate from French to \nGerman without training; ii) BERT can compare the meaning of two sentences to see if they are equivalent ; \niii) BERT can generate tex t; iv) BERT can describe and categorize an image ; and v) BERT can do logical \nsentence analysis, i.e. determine if a given element is a subject, a verb, and a direct object complement. \n \n3.1.1.  Bidirectional encoder representations from transformers (BERT) architecture \nBERT reuses the architecture of transformers (hence the “T” in BERT). Indeed, BERT is nothing \nmore than a superposition of encoders that all have the same structure but do not shar e the same weights. The \n“Base” version of BERT consists of 12 encoders. There is another larger version called “Large” which has 24 \nencoders. Certainly,   the large version is more powerful but more demanding on machine resources. The \nabove model has 512 entries, each corresponding to a token. The first entry corresponds to a special token the \n“[CLS]” for “classification” which allows BERT to be used for a t ext classification task.  It also has 512 \noutputs of size 768 each (1024 for the base version). The first vector is the classification vector. The output \nof each of the 12 encoders can be considered as a vector representation of the input sequence. The rele vance \nof this representation is ensured by the attention mechanism implemented by the encoders. \n \n3.1.2. Training procedure \n\n      \n           ISSN: 2252-8938 \nInt J Artif Intell, Vol. 11, No. 1, March 2022: 379-387 \n382 \nBERT differs from its predecessors (pre -trained NLP models) in the way it is pre -trained on a large \ndataset consisting of texts from English Wikipedia pages (2,500 million words) as well as a set of books  \n(800 million words). This pre-training is done on two tasks. Fisrt, a masked language modelling (MLM) task. \nSecond, a next sentence prediction (NSP) task. \na. Task 1: masked language modelling (MLM) \nThe objective of this task is to predict the hidden word. Therefore, because of the ability of the \ntransformer architecture to simultaneously take into account the right and left contexts of the target word, this \ntask allows the model to learn even more co ntextualised representations than one-way models such as ELMo \n[12]. In practice, target words are s ometimes replaced with a special symbol [MASK], or replaced with \nanother random word, or kept as they are as shown in Figure 2. \n \n \n \n \nFigure 2. MLM \n \n \nb. Task 2: next sentence prediction (NSP) \nBERT is also trained on a next -sentence prediction task in which it must decide whether two input \nsentences are consecutive. The rationale for this task is to improve the performance of the model on tasks \nwhere the objective is to qualify the relationship between a pair of sentences. In practice, the special symbo l \nrepresentation [CLS] is used to classify each pair of input sentences as well as for any other classification \ntask once the model has been trained. \n \n3.1.3. BERT: fine-tuning \nFine-tuning consists of using a pre -trained version of BERT in a model architect ure for a specific \nNLP task. Adding a basic neural network layer is enough to get very good results. For a text classification \ntask, for example, and more precisely for the analysis of the sentiment of moviegoers’ reviews, the \narchitecture of the fitted model may look like this as shown in Figure 3. It is sufficient to add, downstream of \nBERT, a feed-forward followed by a softmax. \n \n3.2.  Our model based on AraBERT  \nIn our approach, we used AraBERT based on the BERT model. It is a widely used model in various \nNLP tasks for several languages. AraBERT is a pre -trained model for the Arabic language, based on the \nGoogle BERT architecture [6] there are six versions of the model: AraBERTv0.1 -base, AraBERTv0.2-base, \nAraBERTv0.2-large, AraBERTv1-base, AraBERTv2-base and AraBERTv2-large. In Table 1 we describe in \ndetail the important information for each version in relation to the pre -training process. The overall view of \nour model is shown in Figure 4. We have been wor king on the customer/user review database for the \nsentiment analysis area, our dataset is titled ARev. \n\nInt J Artif Intell  ISSN: 2252-8938 \n  \n \nAraBERT transformer model  for Arabic comments and reviews analysis (Hicham El Moubtahij) \n383 \n \n \nFigure 3. Architecture of the fine-tuning \n \n \nTable 1. Model pre-training parameters \nModel Size Pre-\nsegmentation \nDataset \nMB Param. Sentences Size Words \nAraBERTv0.2-base 543 M 136M No 200 M 77 GB 8.6 B \nAraBERTv0.2-large 1.38 G 371M No 200 M 77 GB 8.6 B \nAraBERTv2-base 543 MB 136M Yes 200 M 77 GB 8.6 B \nAraBERTv2-large 1.38 G 371M Yes 200 M 77 GB 8.6 B \nAraBERTv0.1-base 543 MB 136M No 77 M 23 GB 2.7 B \nAraBERTv1-base 543 MB 136M Yes 77 M 23 GB 2.7 B \n \n \n \n \nFigure 4. AraBERT architecture overview \n \n\n      \n           ISSN: 2252-8938 \nInt J Artif Intell, Vol. 11, No. 1, March 2022: 379-387 \n384 \nAt the input of our system, we go through the pre -processing stage where we clean the text of any \nunsentimental content, such as usernames, hashtags and URLs, and then proceed to segment the text by using \nthe Farasa segmentation [21]. First, we segment the words into stems, prefixes and suffixes. Look for \nsentence, “ الكتاب – Alkittab “ becomes “ \"ال + كتا + ب  -   \"Al+kitta+b\". Then, in unigram mode, we trained a \nSentence Piece [22] on the segmented pre -training dataset to produce a subword vocabulary of more than \n59K tokens. It must be noted that before the application of Farasa segmentation, the dataset that is used for \npre-training has a size more of 70 GB, more than 8.5 billion words and more than 200 million sentences. To \ncreate a well pre -training dataset, we used several websi tes such as: i) OSIAN Corpus. ii) Arabic Wikipedia \ndump, iii) Assafir news articles, iv) 1.5 billion word Arabic Corpus, and v) OSCAR unfiltered and sorted \nIn our model based on AraBERT, we successively used two special tokens: Tok1: segment separation \n(“SEP’) and Tok2: classification (“CLS”). For any classifier, we used it as the first input token which we help \nus to derive an output vector. Then, in order to obtain the probability distribution on the predicted output \nclasses, we add a simple layer composed of feed-forward and Softmax see (1): \n \nP = softmaxCWT) (1) \n \nwhere P is p robability of each category , W is matrix of the classification layer , and C is o utput of the \ntransformers. \n \n \n4. EXPERIMENT AND RESULTS \n4.1.  ARev dataset \nWe evaluated our model on the sentiment analysis task. For this reason, we used the Arabic reviews \n(Arev) dataset [23]. Using the Facebook API, the ARev dataset is built by more than 100  K comments of the \nmost popular Algerian Facebook pages. We needed tree input for our ARev d ataset which are: the Facebook \npage identifier, the identifier of the Facebook page   post and the access token as shown in F igure 5. To enrich \nour ARev dataset, three open -source datasets of modern standard Arabic and Algerian Arabic comments are \nused see T able 2.  Finally, after pre -processing and deleting the duplicate elements, the dataset is saved in \nCSV format. The statistics of our dataset are presented in Table 3. \n \n \n \n \nFigure 5. Inputs of dataset collection from Facebook \n \n \nTable 2. Various datasets used \nDatasets Type of language Description \nLABR [24] \nStandard Arabic \nBook reviews \nThe dataset of Elsahar and El-Beltagy [25] Hotel reviews, restaurant reviews, product reviews, \nattraction reviews, movie reviews. \nThe dataset of Mataoui et al. [26] Algerian Dialect Comments \n \n\nInt J Artif Intell  ISSN: 2252-8938 \n  \n \nAraBERT transformer model  for Arabic comments and reviews analysis (Hicham El Moubtahij) \n385 \nTable 3. Statistics on the ARev dataset \n Positive Negative \nTotal comments 24932 24932 \nTotal words 1180663 1345029 \nAvg. words in each comment 47.36 53.95 \nAvg. characters in each comment 253.15 294.47 \n \n \n4.2.  Experimental setup \nWe used the Google Colab  tool to run our experiments where we can take good advantage of \nTensorFlow’s performance. Note that we worked with a masking probability of 15%, a random seed of 34, \nand a duplication factor was set to 10. In our approach, we worked through the version of  AraBERTv1 \nimplemented in the work of [6] where our model was pre -trained on a TPUv2 -8 pod. Table 4 resume the \nparameters used for fine-tuning in our models. \n \n \nTable 4. Parameter values \nParameter Value \nLearning Rate 1e-4 \nEpsilon (Adam optimizer) 1e-8 \nMaximum Sequence Length 256 \nEpochs 27 \n \n \n4.3.  Results and discussion \nTo show the importance of our module, we compared the result obtained by our approach with those \nexisting in the state of the art for the domain of sentiment analysis. For this reason, we used the accuracy \nmetric, as shown in  Table 5. The previous results show that our approach gives an important result that is \ncomparative to those of the state of the art. We obtained an accuracy value of 92.5%  for a database \ncontaining more than 40,000 comments written by a mixture of standard  Arabic and Algerian dialect. \nHowever, the approach of Alomari et al. [27] gives an accuracy value better than ours by +1.3%, which is a \nslight difference due to the two reasons following: Firstly, the number of tweets in [27] does not exceed 1800 \ntweets, secondly, the language mix used in our approach generates more linguistic specifications than the \nJordanian dialect. The AraBE RT v1 with the best parameters chosen for fine -tuning gives our approach this \ncompetitiveness over other models. \n \n \nTable 5. Performance of our model implemented on AraBERTv1 compared by the previous state of the art \nsystems \nDataset Descriptions Language Accuracy \nASTD [28] The dataset contains 10,000 tweets. Egyptian dialect 92.6 \nArsen TD lev [29] The dataset contains 4,000 tweets. Levantine dialect 59.4 \nAJGT [27] The Arabic Jordanian General Tweets dataset contains more than \n1,800 tweets. \nJordanian dialect 93.8 \nArSarcasm-v2 [30] Collection of 15,548 sarcasm and sentiment tweets. Standard Arabic and \ndialectal Arabic \n67.7 \nARev Our dataset The Dataset of a mixture of comments and Hotel reviews, restaurant \nreviews, product reviews, attraction reviews, movie reviews. \nStandard Arabic and \nAlgerian dialect \n92.5 \n \n \n5. CONCLUSION AND FUTURE WORK \nThe automatic understanding of Arabic scripts is still a challenging process and an open issue for \nresearchers in the NLP field. In this work, w e have presented our approach bas ed on the AraBERT language \nmodel. Also, we have described and detailed the main steps of the proposed architecture using diagrams and \nexamples. The process starts with the input of our model into a  pre-processed text from the ARev database, \nthen version 1 of the AraBERT model was implemented by using Farasa segmentation. Moreover, our \nevaluation is based on the ARev dataset, which contains more than 40,000 comments and reviews. With \nwell-tuned parameters of the AraBERT model, we obtained an accuracy value of 92.5%, which represents a \nvery competitive result. In future work, we aim to address the problem of Arabic text segmentation, try to \nimprove the farasa segmentation version. \n \n \n \n      \n           ISSN: 2252-8938 \nInt J Artif Intell, Vol. 11, No. 1, March 2022: 379-387 \n386 \nREFERENCES \n[1] N. Boudad, R. Faizi, R. Oulad Haj Thami, and R. Chiheb, “Sentiment analysis in Arabic: A review of the literature,” Ain Shams \nEngineering Journal, vol. 9, no. 4, pp. 2479–2490, Dec. 2018, doi: 10.1016/j.asej.2017.04.007. \n[2] A. Wadhawan, “Dialect identification in nuanced arabic tweets using farasa segmentation and AraBERT,” arXiv:2102.09749, \nFeb. 2021. \n[3] I. Guellil, H. Saâdane, F. Azouaou, B. Gueni, and D. Nouvel, “Arabic natural language processing: An overview,” Journal of \nKing Saud University -Computer and Information Sciences , vol. 33, no. 5, pp. 497 –507, Jun. 2021, doi: \n10.1016/j.jksuci.2019.02.006. \n[4] T. Tobaili, “Sentiment analysis for the low-resourced latinised Arabic ‘Arabizi’’,’” The Open University, 2020. \n[5] I. Guellil, F. Azouaou, F. Benali, A. E. Hachani, and M. Mendoza, “The role of transliteration in the process of Arabizi \ntranslation/sentiment analysis,” in Studies in Computational Intelligence, Springer International Publishing, 2020, pp. 101–128. \n[6] W. Antoun, F. Baly, and H. Hajj, “AraBERT: transformer-based model for Arabic language understanding,” in Proceedings of the \n58th Annual Meeting of the Association for Computational Linguistics, Jul. 2020, pp. 8440–8451. \n[7] A. Conneau et al., “Unsupervised cross-lingual representation learning at scale,” in Proceedings of the 58th Annual Meeting of the \nAssociation for Computational Linguistics, Nov. 2020, pp. 8440–8451. \n[8] D. Adiwardana et al., “Towards a human-like open-domain chatbot,” in Proceedings of the Eleventh International Conference on \nLanguage Resources and Evaluation (LREC 2018), Jan. 2018, pp. 52–55. \n[9] T. Mikolov, I. Sutskever,  K. Chen, G. S. Corrado, and J. Dean, “Distributed representations of words and phrases and their \ncompositionality,” in Advances in neural information processing systems, 2013, pp. 3111–3119. \n[10] J. Pennington, R. Socher, and C. Manning, “Glove: global ve ctors for word representation,” in Proceedings of the 2014 \nConference on Empirical Methods in Natural Language Processing (EMNLP), 2014, pp. 1532–1543, doi: 10.3115/v1/D14-1162. \n[11] T. Mikolov, E. Grave, P. Bojanowski, C. Puhrsch, and A. Joulin, “Advances  in pre -training distributed word representations,” \nDec. 2017, [Online]. Available: http://arxiv.org/abs/1712.09405. \n[12] M. Peters et al. , “Deep contextualized word representations,” in Proceedings of the 2018 Conference of the North American \nChapter of t he Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) , 2018, pp. \n2227–2237, doi: 10.18653/v1/N18-1202. \n[13] J. Devlin, M. -W. Chang, K. Lee, and K. Toutanova, “BERT: pre -training of deep bidirectional transformers  for language \nunderstanding,” arXiv:1810.04805, Oct. 2018. \n[14] J. Howard and S. Ruder, “Universal language model fine -tuning for text classification,” in Proceedings of the 56th Annual \nMeeting of the Association for Computational Linguistics, Jan. 2018, pp. 328–339, doi: 10.18653/v1/P18-1031. \n[15] C. Raffel et al., “Exploring the limits of transfer learning with a unified text-to-text transformer,” arXiv:1910.10683, Oct. 2019. \n[16] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut, “ALBERT:  a lite BERT for self -supervised learning of \nlanguage representations,” arXiv:1909.11942, Sep. 2019. \n[17] A. M. A. Nada, E. Alajrami, A. A. Al -Saqqa, and S. S. Abu -Naser, “Arabic text summarization using AraBERT model using \nextractive text summarization ap proach,” International Journal of Academic Information Systems Research (IJAISR) , vol. 4, no. \n8, pp. 6–9, 2020. \n[18] H. Alami, S. Ouatik El Alaoui, A. Benlahbib, and N. En -nahnahi, “LISAC FSDM -USMBA Team at SemEval -2020 Task 12: \nOvercoming AraBERT’s pretra in-finetune discrepancy for Arabic offensive language identification,” in Proceedings of the \nFourteenth Workshop on Semantic Evaluation, 2020, pp. 2080–2085, doi: 10.18653/v1/2020.semeval-1.275. \n[19] D. Faraj and M. Abdullah, “SarcasmDet at SemEval -2021 Ta sk 7: detect humor and offensive based on demographic factors \nusing RoBERTa pre-trained model,” in Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval -2021), \n2021, pp. 527–533, doi: 10.18653/v1/2021.semeval-1.64. \n[20] A. Hussein,  N. Ghneim, and A. Joukhadar, “DamascusTeam at NLP4IF2021: fighting the Arabic COVID -19 infodemic on \nTwitter using AraBERT,” in Proceedings of the Fourth Workshop on NLP for Internet Freedom: Censorship, Disinformation, and \nPropaganda, 2021, pp. 93–98, doi: 10.18653/v1/2021.nlp4if-1.13. \n[21] A. Abdelali, K. Darwish, N. Durrani, and H. Mubarak, “Farasa: a fast and furious segmenter for Arabic,” in Proceedings of the \n2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations , 2016, pp. \n11–16, doi: 10.18653/v1/N16-3003. \n[22] T. Kudo, “Subword regularization: improving neural network translation models with multiple subword candidates,” in \nProceedings of the 56th Annual Meeting of the As sociation for Computational Linguistics (Volume 1: Long Papers) , 2018, pp. \n66–75, doi: 10.18653/v1/P18-1007. \n[23] A. Abdelli, F. Guerrouf, O. Tibermacine, and B. Abdelli, “Sentiment analysis of Arabic Algerian dialect using a supervised \nmethod,” in 2019 International Conference on Intelligent Systems and Advanced Computing Sciences (ISACS) , Dec. 2019, pp. 1–\n6, doi: 10.1109/ISACS48493.2019.9068897. \n[24] M. Aly and A. Atiya, “Labr: A large scale Arabic book reviews dataset,” 2013. \n[25] H. ElSahar and S. R. El -Beltagy, “Building large Arabic multi -domain resources for sentiment analysis,” in Computational \nLinguistics and Intelligent Text Processing, Springer International Publishing, 2015, pp. 23–34. \n[26] M. Mataoui, O. Zelmati, and M. Boumechache, “A proposed lexicon-based sentiment analysis approach for the vernacular \nAlgerian Arabic,” Research in Computing Science, vol. 110, no. 1, pp. 55–70, Dec. 2016, doi: 10.13053/rcs-110-1-5. \n[27] K. M. Alomari, H. M. ElSherif, and K. Shaalan, “Arabic tweets sentimental a nalysis using machine learning,” in Advances in \nArtificial Intelligence: From Theory to Practice, Springer International Publishing, 2017, pp. 602–610. \n[28] M. Nabil, M. Aly, and A. Atiya, “ASTD: Arabic sentiment tweets dataset,” in Proceedings of the 2015 Conference on Empirical \nMethods in Natural Language Processing, 2015, pp. 2515–2519, doi: 10.18653/v1/D15-1299. \n[29] R. Baly, A. Khaddaj, H. Hajj, W. El -Hajj, and K. B. Shaban, “ArSentD -LEV: a multi -topic corpus for target -based sentiment \nanalysis in Arabic levantine tweets,” The 3rd Workshop on Open-Source Arabic Corpora and Processing Tools, 2018. \n[30] I. A. Farha and W. Magdy, “Benchmarking transformer -based language models for Arabic sentiment and sarcasm detection,” in \nProceedings of the Sixth Arabic Natural Language Processing Workshop, 2021, pp. 21–31. \n \n \n \n \n \nInt J Artif Intell  ISSN: 2252-8938 \n  \n \nAraBERT transformer model  for Arabic comments and reviews analysis (Hicham El Moubtahij) \n387 \nBIOGRAPHIES OF AUTHORS  \n \n \nProf. Hicham El Moubtahij      is currently a Professor of Computer Science at \nthe University of Ibn Zohr, Agadir, Morocco. He received his Ph.D. in Computer Science \nfrom the University of Sidi Mohamed Ben Abdellah, Fez, Morocco, in 2017. He is now a \nmember of the Systems and Technologies of Information Team at the High School of \nTechnology at the University of Ibn Zohr, Agadir. His current research interests include \nmachine learning, deep learning, Arabic handwriting recognition, Text Mining, and \nmedica l imagery. Dr. El Moubtahij Hicham has published articles in indexed \ninternational journals and conferences, has been a reviewer for scientific journals, and \nhas served on the program committee of several conferences.  He can be contacted at \nemail: h.elmoubtahij@uiz.ac.ma.  \n  \n \nDr. Hajar Abdelali     holder of a bachelor ’s degree in experimental sciences, a \nbachelor's degree in mathematics and computer science, a master's degree in information \nsciences, networks and multimedia from Sidi Mohammed Ben Abdellah, University of Fez, \nMorocco in 2013. She joined the laboratory XLIM of the University of Poitiers in France in \ncollaboration with the scientific laboratory LIMS of the Faculty of Sciences Dhar Mahraz of \nSidi Mohammed Ben Abdellah, University of Fez, Morocco where he obtained his Ph.D. \ndegree in computer science in 2019.  She can be contacted at email:  \nabdelali.hajar@usmba.ac.ma. \n  \n \nProf. El Bachir Tazi     graduated in Electronic Engineering from ENSET \nMohammedia Morocco in 1992. He obtained his DEA and DES in Automation and Signal \nProcessing and his PhD in Computer Science from Sidi Mohammed Ben Abdellah University, \nFaculty of Sciences in Fez, Morocco resp ectively in 1995, 1999 and 2012. He is now a \nmember of the engineering sciences laboratory and associate professor at Sidi Mohammed \nBen Abdellah University, Polydisciplinary Faculty of Taza, Morocco. His areas of interest \ngenerally include all areas of aut omatic recognition based on artificial intelligence methods \nand applications related to automatic speaker . He can be contacted at email: \nelbachirtazi@yahoo.fr.  \n \n  \n \n",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8478807210922241
    },
    {
      "name": "Natural language processing",
      "score": 0.7004867792129517
    },
    {
      "name": "Transformer",
      "score": 0.6853741407394409
    },
    {
      "name": "Arabic",
      "score": 0.6396872401237488
    },
    {
      "name": "Segmentation",
      "score": 0.6084309220314026
    },
    {
      "name": "Text segmentation",
      "score": 0.5937541723251343
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5367255210876465
    },
    {
      "name": "Encoder",
      "score": 0.5214956998825073
    },
    {
      "name": "Tourism",
      "score": 0.5193482637405396
    },
    {
      "name": "Preprocessor",
      "score": 0.48575499653816223
    },
    {
      "name": "Field (mathematics)",
      "score": 0.43332189321517944
    },
    {
      "name": "Linguistics",
      "score": 0.28019553422927856
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Pure mathematics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210088687",
      "name": "Université Ibn Zohr",
      "country": "MA"
    },
    {
      "id": "https://openalex.org/I81605866",
      "name": "Sidi Mohamed Ben Abdellah University",
      "country": "MA"
    }
  ],
  "cited_by": 16
}