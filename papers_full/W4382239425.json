{
  "title": "InParformer: Evolutionary Decomposition Transformers with Interactive Parallel Attention for Long-Term Time Series Forecasting",
  "url": "https://openalex.org/W4382239425",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2443186915",
      "name": "Haizhou Cao",
      "affiliations": [
        "University of Chinese Academy of Sciences",
        "Computer Network Information Center"
      ]
    },
    {
      "id": "https://openalex.org/A2119495518",
      "name": "Zhenhao Huang",
      "affiliations": [
        "North China Electric Power University"
      ]
    },
    {
      "id": "https://openalex.org/A2884134162",
      "name": "Tiechui Yao",
      "affiliations": [
        "Computer Network Information Center",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2013702026",
      "name": "Jue Wang",
      "affiliations": [
        "Computer Network Information Center",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2024226121",
      "name": "Hui He",
      "affiliations": [
        "North China Electric Power University"
      ]
    },
    {
      "id": "https://openalex.org/A2111245529",
      "name": "Yangang Wang",
      "affiliations": [
        "University of Chinese Academy of Sciences",
        "Computer Network Information Center"
      ]
    },
    {
      "id": "https://openalex.org/A2443186915",
      "name": "Haizhou Cao",
      "affiliations": [
        "University of Chinese Academy of Sciences",
        "Computer Network Information Center",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2119495518",
      "name": "Zhenhao Huang",
      "affiliations": [
        "North China Electric Power University"
      ]
    },
    {
      "id": "https://openalex.org/A2884134162",
      "name": "Tiechui Yao",
      "affiliations": [
        "Computer Network Information Center",
        "Chinese Academy of Sciences",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2013702026",
      "name": "Jue Wang",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Computer Network Information Center",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2024226121",
      "name": "Hui He",
      "affiliations": [
        "North China Electric Power University"
      ]
    },
    {
      "id": "https://openalex.org/A2111245529",
      "name": "Yangang Wang",
      "affiliations": [
        "University of Chinese Academy of Sciences",
        "Chinese Academy of Sciences",
        "Computer Network Information Center"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2950304420",
    "https://openalex.org/W6750513420",
    "https://openalex.org/W2412782625",
    "https://openalex.org/W6910681941",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2988435740",
    "https://openalex.org/W6666761814",
    "https://openalex.org/W6752867117",
    "https://openalex.org/W6771626834",
    "https://openalex.org/W2604847698",
    "https://openalex.org/W2954731415",
    "https://openalex.org/W2942555036",
    "https://openalex.org/W6748148878",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W6637883433",
    "https://openalex.org/W2889928394",
    "https://openalex.org/W1536447791",
    "https://openalex.org/W2607045400",
    "https://openalex.org/W2944772978",
    "https://openalex.org/W6641163509",
    "https://openalex.org/W3173539742",
    "https://openalex.org/W4229368589",
    "https://openalex.org/W3111507638",
    "https://openalex.org/W4225494949",
    "https://openalex.org/W2811507150",
    "https://openalex.org/W4234761883",
    "https://openalex.org/W2888135434",
    "https://openalex.org/W2964199361",
    "https://openalex.org/W3177318507",
    "https://openalex.org/W2970309699",
    "https://openalex.org/W3212890323",
    "https://openalex.org/W4323654151",
    "https://openalex.org/W2295598076",
    "https://openalex.org/W2963045354",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W2560023338",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4313156423",
    "https://openalex.org/W2132782512",
    "https://openalex.org/W2792764867",
    "https://openalex.org/W648786980",
    "https://openalex.org/W4394647257",
    "https://openalex.org/W2980994438",
    "https://openalex.org/W2478884216",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W1964357740",
    "https://openalex.org/W4295838474"
  ],
  "abstract": "Long-term time series forecasting (LTSF) provides substantial benefits for numerous real-world applications, whereas places essential demands on the model capacity to capture long-range dependencies. Recent Transformer-based models have significantly improved LTSF performance. It is worth noting that Transformer with the self-attention mechanism was originally proposed to model language sequences whose tokens (i.e., words) are discrete and highly semantic. However, unlike language sequences, most time series are sequential and continuous numeric points. Time steps with temporal redundancy are weakly semantic, and only leveraging time-domain tokens is hard to depict the overall properties of time series (e.g., the overall trend and periodic variations). To address these problems, we propose a novel Transformer-based forecasting model named InParformer with an Interactive Parallel Attention (InPar Attention) mechanism. The InPar Attention is proposed to learn long-range dependencies comprehensively in both frequency and time domains. To improve its learning capacity and efficiency, we further design several mechanisms, including query selection, key-value pair compression, and recombination. Moreover, InParformer is constructed with evolutionary seasonal-trend decomposition modules to enhance intricate temporal pattern extraction. Extensive experiments on six real-world benchmarks show that InParformer outperforms the state-of-the-art forecasting Transformers.",
  "full_text": "InParformer: Evolutionary Decomposition Transformers with Interactive Parallel\nAttention for Long-Term Time Series Forecasting\nHaizhou Cao1,2, Zhenhao Huang3, Tiechui Yao1,2, Jue Wang1,2,*\n, Hui He3, Yangang Wang1,2\n1Computer Network Information Center, Chinese Academy of Sciences, Beijing, China\n2University of Chinese Academy of Sciences, Beijing, China\n3North China Electric Power University, Beijing, China\n{caohaizhou, yaotiechui}@cnic.cn, {wangjue, wangyg}@sccas.cn, {huangzhenhao, huihe}@ncepu.edu.cn\nAbstract\nLong-term time series forecasting (LTSF) provides substan-\ntial benefits for numerous real-world applications, whereas\nplaces essential demands on the model capacity to capture\nlong-range dependencies. Recent Transformer-based models\nhave significantly improved LTSF performance. It is worth\nnoting that Transformer with the self-attention mechanism\nwas originally proposed to model language sequences whose\ntokens (i.e., words) are discrete and highly semantic. How-\never, unlike language sequences, most time series are sequen-\ntial and continuous numeric points. Time steps with tem-\nporal redundancy are weakly semantic, and only leveraging\ntime-domain tokens is hard to depict the overall properties\nof time series (e.g., the overall trend and periodic variations).\nTo address these problems, we propose a novel Transformer-\nbased forecasting model named InParformer with an Inter-\nactive Parallel Attention (InPar Attention) mechanism. The\nInPar Attention is proposed to learn long-range dependen-\ncies comprehensively in both frequency and time domains.\nTo improve its learning capacity and efficiency, we further\ndesign several mechanisms, including query selection, key-\nvalue pair compression, and recombination. Moreover, InPar-\nformer is constructed with evolutionary seasonal-trend de-\ncomposition modules to enhance intricate temporal pattern\nextraction. Extensive experiments on six real-world bench-\nmarks show that InParformer outperforms the state-of-the-art\nforecasting Transformers.\nIntroduction\nAs time series is increasingly complex and pervasive in the\nera of big data, time series forecasting (TSF) has become an\nintegral part of numerous real-world applications in energy,\neconomics, traffic, weather, etc. Compared to ordinary TSF,\nlong-term time series forecasting (LTSF) offers stronger\nassistance for long-term planning (e.g., in power systems\n(Lindberg et al. 2019)) and early warning, but brings more\nchallenges. Typically, it requires a higher model capacity to\ndiscover longer temporal dependencies and model tougher\nnonlinear dynamics. Although RNN-based TSF models (Lai\net al. 2018; Rangapuram et al. 2018; Salinas et al. 2020)\nhave made notable strides, these iterated multi-step (IMS)\n*Corresponding author.\nCopyright ¬© 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n(Shi and Yeung 2018) approaches suffer from error accu-\nmulation (Bengio et al. 2015), inefficient inference, and\ntricky parallelization (Vaswani et al. 2017). CNNs or tem-\nporal convolutional networks (TCNs) have shown effective-\nness in sequence modeling (Bai, Kolter, and Koltun 2018;\nSen, Yu, and Dhillon 2019), but are still limited by local\nreceptive fields. Similar to breakthroughs in NLP (Vaswani\net al. 2017; Devlin et al. 2018) and CV (Dosovitskiy et al.\n2022; Liu et al. 2021) fields, LTSF has recently benefited\nfrom the Transformer (Vaswani et al. 2017) architecture.\nWith the self-attention mechanism, Transformer-based mod-\nels achieve superiority in capturing long-term dependen-\ncies, which is essential for LTSF. The canonical Transformer\nwith self-attention has quadratic computational and mem-\nory costs. Recent forecasting Transformers (Li et al. 2019;\nKitaev, Kaiser, and Levskaya 2020; Zhou et al. 2021) are\nmainly committed to using a sparse scheme to improve the\nefficiency of self-attention.\nIt is worth noting that the standard attention mechanism\nwas originally proposed to model human-generated lan-\nguage sequences. Each step token (i.e., word) is discrete and\nhighly semantic. Standard point-wise alignment schemes\n(including their sparse versions) are reasonable when captur-\ning semantic dependencies. However, most time series are\nsequential and continuous numeric points, extremely dissim-\nilar to sequences like language sentences. When the Trans-\nformer architecture is adopted for time series, there are three\nkey challenges:\n1. Time series contain several steps with information redun-\ndancy (e.g., missing values can be obtained by interpola-\ntion in some cases). This indicates that performing full-\nlength queries is computationally redundant. Typically,\nInformer (Zhou et al. 2021) proposes the ProbSparse at-\ntention which selects top-u dominant queries based on\nquery sparsity measurement. But this will limit the over-\nall representation of time series. Moreover, compressing\nthe length of key-value pairs is also worth considering.\nFor example, a memory compressed attention (Liu et al.\n2018) reduces the length by using a strided convolution\nto process long texts. On the other hand, periodic time\nseries may have similar sub-processes. Autoformer (Wu\net al. 2021) develops an Auto-Correlation mechanism\nthat calculates series autocorrelation and selects top-k\npossible time delays to conduct a sub-series level aggre-\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n6906\ngation.\n2. The semantic density of time series is low. The inherent\ncharacteristics of time series are difficult to depict with\nlocal or a small number of time-domain steps. However,\nmany attention mechanisms for TSF (Li et al. 2019; Ki-\ntaev, Kaiser, and Levskaya 2020; Zhou et al. 2021) are\ntime-domain sparse. Performing a time-to-frequency do-\nmain transformation is widely used in time series analy-\nsis and can be a suitable solution. After transformation,\neach token in the frequency domain becomes highly ‚Äùse-\nmantic‚Äù because it reflects a frequency feature of time se-\nries. FEDformer (Zhou et al. 2022) selects a random sub-\nset of frequency components and multiplies them with\nlearnable complex-number parameters to learn a sparse\nrepresentation of time series. It selects a small constant-\nlength subset for any time series, so this sparse scheme\neasily leads to global information loss (e.g., patterns re-\nlated to frequency).\n3. Real-world time series are often composed of intricate\ntemporal patterns and entangled with noise. Time se-\nries decomposition is a common strategy to tackle these\nknotted patterns (Hyndman and Athanasopoulos 2021).\nBesides pattern extraction, an effective decomposition\nmethod can reduce noise. Traditionally, decomposition\nis applied as a feature engineering technique during the\npreprocessing phase. Autoformer (Wu et al. 2021) intro-\nduces the idea of seasonal-trend decomposition (Robert,\nWilliam, and Irma 1990) into Transformer by utilizing a\nfixed-window moving average. FEDformer (Zhou et al.\n2022) replaces a fixed window with a set of ones. How-\never, the detrended part (as the seasonal component) in\nthese works relies on predefined average filters.\nMotivated by the above, we propose InParformer, an evo-\nlutionary decomposition Transformer with interactive par-\nallel attention for LTSF. We provide the following solu-\ntions to the listed challenges: (1) For redundant time steps,\nwe employ query selection and key-value pair compression.\nQuery selection provides two random subsets of queries to\nthe parallel attention module, reducing the time/space cost\nsignificantly. It also acts as a generalization mechanism on\ntime steps like a dropout. Key-value pair compression is im-\nplemented by an interactive partitioned convolution module\nthat learns compact multi-resolution temporal partitions. (2)\nConsidering the limitation of time-domain tokens and the\nadvantages of frequency-domain ones, we create a parallel\nattention module with two sub-attention mechanisms that\nwork in the frequency and time domains separately. Further-\nmore, we add global context information to the recombined\noutputs of two sub-attention mechanisms. (3) To ease intri-\ncate temporal pattern extraction, we follow the decomposi-\ntion architecture designed by Autoformer (Wu et al. 2021).\nBut we use the proposed evolutionary seasonal-trend de-\ncomposition module and deploy fewer ones. With the pro-\nposed binary decomposition algorithm, the module can ob-\ntain the seasonal component without predefined fixed aver-\nage filters. The key contributions are summarized as follows:\n‚Ä¢ We propose a novel forecasting Transformer named In-\nParformer. It introduces evolutionary seasonal-trend de-\ncomposition (EvoSTD) modules to enhance its ability to\nextract intricate temporal patterns.\n‚Ä¢ We propose an interactive parallel attention (InPar At-\ntention) mechanism with frequency-aware attention and\ntime-aware attention to learn long-range dependencies\ncomprehensively.\n‚Ä¢ To improve the learning capacity and computation ef-\nficiency of InPar Attention, we design several mecha-\nnisms, including query selection, key-value pair com-\npression (via an interactive partitioned convolution mod-\nule), and recombination.\n‚Ä¢ Extensive experiments on six popular real-world bench-\nmarks across multiple domains show that InParformer\nachieves better performance than the state-of-the-art\nmethods under the long-term multivariate and univariate\nforecasting settings.\nRelated Work\nTime Series Forecasting As a long-standing and valu-\nable research topic, TSF has undergone substantial devel-\nopment. Traditional TSF methods mainly focus on statisti-\ncal approaches such as ARIMA (Box and Jenkins 1968) and\nexponential smoothing (Gardner Jr 1985), which have dif-\nficulty in modeling non-linear temporal dynamics. To solve\nthis defect, classical machine learning models are introduced\nfor TSF, such as support vector regression (SVR) (Smola\nand Sch ¬®olkopf 2004) and gradient boosted trees (Chen and\nGuestrin 2016). Nevertheless, their performance relies heav-\nily on feature engineering.\nRecently, with the thrilling success of deep learning in\nmany fields, various deep neural networks with powerful\nlearning capabilities have been developed for TSF. Due to\nthe strength in sequence modeling, RNNs, including LSTM\n(Hochreiter and Schmidhuber 1997) and GRU (Cho et al.\n2014), are widely used to capture temporal dependencies.\nDeepAR (Salinas et al. 2020) combines RNNs with autore-\ngressive methods to predict a probabilistic distribution of\ntime series. However, due to the recurrent structure, RNN-\nbased models easily suffer from error accumulation (Bengio\net al. 2015) and have trouble with parallelization (Vaswani\net al. 2017). In addition, CNNs have also found their abilities\nin learning temporal representation, such as temporal convo-\nlution networks (TCNs) (Bai, Kolter, and Koltun 2018; Sen,\nYu, and Dhillon 2019), but are still limited by the local re-\nceptive fields.\nTransformer-Based Models Similar to remarkable pro-\ngresses in NLP (Vaswani et al. 2017; Devlin et al. 2018) and\nCV (Dosovitskiy et al. 2022; Liu et al. 2021) fields, TSF,\nespecially long-term TSF, has recently benefited from the\nTransformer (Vaswani et al. 2017) architecture. As the cen-\nterpiece of Transformer, the self-attention mechanism has\nO(1) maximum path length of signals traveling, which is\nadvantageous for long-range modeling. However, the canon-\nical Transformer has quadratic computation complexity due\nto the self-attention mechanism. Numerous sparse strate-\ngies are proposed to improve the efficiency of self-attention\n(Lin et al. 2021). LogTrans (Li et al. 2019) proposes the\n6907\nInPar\nAttention EvoSTD\nInPar\nAttention EvoSTD InPar\nAttention EvoSTD\nEncoder\nInput\nSeasonal\nInit\nTrend\nInit\nPrediction\nùëÅ!\" √ó\nùëÅ#! √ó\nEncoder\nDecoder\nFigure 1: InParformer architecture. The interactive parallel attention (InPar Attention) is used to perform dependency discovery\nin both frequency and time domains. The evolutionary seasonal-trend decomposition (EvoSTD) is used to extract intricate\ntemporal patterns.\nLogSparse attention that each step only attends to previ-\nous steps with exponential intervals to break the memory\nbottleneck. Reformer (Kitaev, Kaiser, and Levskaya 2020)\nseparates tokens into several buckets using locality-sensitive\nhashing (LSH) and conducts attention within each bucket.\nInformer (Zhou et al. 2021) proposes the ProbSparse atten-\ntion which only calculates top-u dominant queries based on\nthe measured query sparsity. Note that the sparse schemes\ndeveloped by these works still follow the standard point-\nwise alignment. Autoformer (Wu et al. 2021) proposes\nan Auto-Correlation mechanism to conduct a sub-series\nlevel aggregation for the inherent periodicity of time se-\nries. FEDformer (Zhou et al. 2022) learns a sparse tem-\nporal representation using frequency enhanced block/atten-\ntion based on a random subset of frequency components\nand learnable complex-number parameters. Besides, these\ntwo Transformer-based models incorporate seasonal-trend\ndecomposition blocks to learn temporal patterns. Neverthe-\nless, the decomposition blocks are based on single or multi-\nple predefined fixed-window average filters.\nMethodology\nPreliminary\nGiven historical data with input length Lx, time series fore-\ncasting is to predict future horizon with output length Ly.\nFor LTSF, the output lengthLy is larger, i.e., long-term pre-\ndiction. The data dimension in the model is denoted as D.\nModel Architecture\nThe overall framework of InParformer is shown in Fig-\nure 1. It is a Transformer-based decomposition architec-\nture consisting of interactive parallel attention (InPar Atten-\ntion) modules and evolutionary seasonal-trend decomposi-\ntion (EvoSTD) modules. The details of these modules will\nbe described in subsequent sections.\nEncoder The encoder is designed to learn the historical\nseasonal information and stacked with Nen encoder layers.\nAs shown in Figure 1, each encoder layer has one InPar At-\ntention module and one EvoSTD module. Given the embed-\nded input of encoder X0\nen ‚àà RLx√óD, the details in l-th en-\ncoder layer are:\nSl\nen,\n= EvoSTD(InParAttn(Xl‚àí1\nen ) +Xl‚àí1\nen ), (1)\nwhere Xl\nen = Sl\nen ‚àà RLx√óD is the seasonal part and is used\nas the input of l-th encoder layer; l ‚àà {1, . . . , Nen}. The\nprocess is summarized as: Xl\nen = Encoder(Xl‚àí1\nen ).\nDecoder The decoder is used to output the prediction and\ncontains Nde decoder layers. As shown in Figure 1, each\ndecoder layer has two InPar Attention modules (the sec-\nond is used as cross-attention) and two EvoSTD modules.\nThe initialization of the decoder‚Äôs inputs is similar to that of\nAutoformer (Wu et al. 2021) except for the decomposition\nmethod (using binary decomposition). Given the decoder‚Äôs\nembedded inputs X0\nde, T0\nde ‚àà R( Lx\n2 +Ly)√óD, the details in\nl-th decoder layer are:\nSl,1\nde , Tl,1\nde = EvoSTD(InParAttn(Xl‚àí1\nde ) +Xl‚àí1\nde ),\nSl,2\nde , Tl,2\nde = EvoSTD(InParAttn(Sl,1\nde , XNen\nen ) +Sl,1\nde ),\nTl\nde = Tl‚àí1\nde + Wl,1 ‚àó Tl,1\nde + Wl,2 ‚àó Tl,2\nde ,\n(2)\nwhere Sl,i\nde and Tl,i\nde (i ‚àà {1,2}) denote the seasonal and\ntrend parts; Xl\nde = Sl,2\nde ; Wl,i (i ‚àà {1, 2}) represents the\nprojector for the trend part Tl,i\nde . The process is summarized\nas: Xl\nde = Decoder(Xl‚àí1\nde , XNen\nen ).\nThe prediction result is the sum of two parts:WS ‚àóXNde\nde +\nTNde\nde , where WS is to project the seasonal part XNde\nde to the\ntarget dimension.\nInteractive Parallel Attention\nAs shown in Figure 2, the InPar Attention is proposed to\nlearn long-range dependencies from two perspectives: fre-\nquency domain and time domain. To improve its learning ca-\npacity and computation efficiency, several mechanisms are\nemployed, including query selection, key-value pair com-\npression (by an interactive partitioned convolution), and re-\ncombination.\n6908\nùêê\nùêä\nùêï\nùêó\nLinear\nInteractive Partitioned\nConvolution\nùêä\nùêï\nLinear\nLinear\nRecombine Linear\nParallel Attention Module\nTime-Aware Attention\nTanh\nùêñ! ‚àà ‚Ñù\"√ó$\nSoftmax\nFrequency-Aware Attention\nScale Softmax\n‚Ñ±\n‚Ñ± ‚Ñ±\n‚Ñ±%$\nSelect\n‚Ñ±\n‚Ñ±%$\nFFT\nInverse FFT\nAggregate\nùêô\nFigure 2: Interactive parallel attention (InPar Attention). Two subsets of queries are selected and fed into the parallel attention\nmodule. Benefiting from query selection, the parallel attention can be considered as a single attention. Furthermore, an interac-\ntive partitioned convolution (IPConv) is employed to compress the key-value pairs to a smaller length.\nQuery Selection for Parallel Attention Module For par-\nallel attention, it is intuitive to reserve full queries for each,\nbut this yields twice computational complexity and mem-\nory usage compared to single attention. Previous analyses\nshow both theoretically and experimentally that the self-\nattention matrix is often low rank (Wang et al. 2020; Guo\net al. 2019) and sparse (Child et al. 2019). Numerous sparse\nstrategies are proposed to avoid computing full (single) at-\ntention (Lin et al. 2021). Considering that time series contain\nredundant time steps, we select two subsets from full queries\nQ ‚àà RL√óD for two sub-attention mechanisms:\niF, iT = SplitIndex(L, LF, LT),\nQF = Q[iF],\nQT = Q[iT],\n(3)\nwhere SplitIndex(¬∑) is to get two subsets of index randomly\nwhich may have overlaps (random selection is to avoid in-\ntroducing bias of structural information) ; QF ‚àà RLF√óD and\nQT ‚àà RLT√óD are the subsets of queries selected by iF and\niT, respectively. For the multi-head version, SplitIndex(¬∑)\ngets different iF and iT for each head, which expands the\nsampling space of queries and reduces information loss. In\nthis work, we select LF = L ‚àí ‚åäc √ó log L‚åã and LT =\n‚åäc√ólog L‚åã queries randomly for each subspace (head) where\nc is a sampling factor. We will describe the processing for\ninsufficient output length caused by incomplete queries in\nsubsection Recombination.\nInteractive Partitioned Convolution Unlike human-\ngenerated language sequences which are information-dense,\nmost natural signals are not highly semantic (He et al. 2021).\nInspired by the memory-compressed attention (Liu et al.\n2018) and the pyramid structures in computer vision tasks\n(Chen et al. 2017; Zhao et al. 2017), we propose an inter-\nactive partitioned convolution (IPConv) to learn a compact\ntemporal representation at multiple resolutions. As shown\nin Figure 3, multiple parallel convolution branches with dif-\nferent settings are applied to capture multi-resolution infor-\nmation. Additional padding, convolution, and pooling oper-\nations can be used in some branches to make all branches\noutput the same length.\nFor parallel attention module, IPConv is used to provide\na smaller length of key-value pairs and its parameters are\nshared between the keys and values processing:\nK = IPConv(K),\nV = IPConv(V),\n(4)\nwhere K, V ‚àà RL√óD; K, V ‚àà RL‚Ä≤√óD. The output length\nare reduced to L‚Ä≤ = 1\n4 L. In practice, two convolution\nbranches are employed. For branch 1, there is a 1-D con-\nvolution with kernel size = 4, stride = 4. For branch 2,\nthere are two 1-D convolutions with kernel size = (6, 2),\nstride = (2, 2), and a padding operation before the convo-\nlutions. Due to the out channels = D1 = D2 = D/2 for\neach convolution, the total cost of computation and memory\nis similar to that of a singleout\nchannels = D convolution.\nAfter concatenation, we do not perform fusion operations\nlike kernel\nsize = 1 convolution. Besides the considera-\ntion of computational cost, this makes multi-head key-value\npairs with multiple resolutions, depending on the convolu-\ntion branch they belong to.\nConv1\nConvN\n(Opt.)\nConv21 Conv22 Concat\nùêø!√óùê∑\"\nùêø!√óùê∑#\nùêø!√óùê∑$\nùêó\nMulti-resolution Partitions\nPad\n‚Ä¶\nùêó\nùêø√óùê∑ ùêø!√óùê∑\nFigure 3: Interactive partitioned convolution (IPConv) as\na context and compression module. Multiple convolution\nbranches work with different settings and output reduced di-\nmensions. The multi-resolution partitions from the convolu-\ntion branches are concatenated.\nFrequency-Aware Attention Given the projected queries\nQ ‚àà RLQ√óDK , keys K ‚àà RLK√óDK , and values V ‚àà\n6909\nRLK√óDV , frequency-aware attention (FAA) performs a\nscaled dot-product attention in the frequency domain:\nFAA(Q, K, V) =F‚àí1(softmax(F(Q)F(K)‚ä§\n‚àöDK\n)F(V)),\n(5)\nwhere F, F‚àí1 denote the Fast Fourier Transform (FFT) and\nits inverse (IFFT). In practice, since the input is real, the out-\nput of FFT is Hermitian-symmetric (Oppenheim 1999) and\nthe negative frequencies are redundant. Therefore, with the\nreal FFT (RFFT) which only computes the positive frequen-\ncies, the dot-product operation in FAA is performed with the\nhalf-length queries, keys, and values in the frequency do-\nmain. Finally, the inverse RFFT transforms the result to the\ntime domain with the original length. The formulation of\nmulti-head version is similar to the canonical one and omit-\nted.\n12345678910111213141516Pad Pad\nReceptiveFields1\n2\n3 42\n1 3 4\n21 3 4 123412341234\n‚Ä¶‚Ä¶‚Ä¶‚Ä¶116‚Ä¶\nIPConvùêó ùêó\nConv1Conv2ConvN(Opt.)‚Ä¶\nFigure 4: Illustration of multi-resolution partitioning by IP-\nConv for a simple sequence. The result is the concatenation\nof partitions by multiple convolution branches with different\nreceptive fields. The length is reduced from 16 to 4.\nWhen as a sub-attention in InPar Attention, its input is the\nsubset of queries QF ‚àà RLF√óD and the compressed key-\nvalue pairs, K, V ‚àà RL‚Ä≤√óD. The process is summarized as\nfollows:\nZF = FAA(QF, K, V), (6)\nwhere ZF ‚àà RLF√óD. Compared to canonical attention with\nfull input, FAA for InPar Attention performs dependency\ndiscovery in the frequency domain and greatly reduces the\ncomputational cost of the dot product through query selec-\ntion, key-value pair compression, and RFFT.\nTime-Aware Attention Given the projected queries Q ‚àà\nRLQ√óDK , keys K ‚àà RLK√óDK , and values V ‚àà RLK√óDV ,\ntime-aware attention (TAA) performs additive attention\n(Bahdanau, Cho, and Bengio 2014) which contains learn-\nable parameters for scoring (in the time domain):\nTAA(Q, K, V) = softmax(tanh(Q +‚àó K)wv)V, (7)\nwhere +‚àó denotes the addition with broadcasting; wv ‚àà\nRDK is a learnable weight vector. The formulation of multi-\nhead version is similar to the canonical one and omitted.\nWhen as a sub-attention in InPar Attention, its key-value\npairs K, V ‚àà RL‚Ä≤√óD are shared with FAA while the queries\nare QT ‚àà RLT√óD. It can be written as:\nZT = TAA(QT, K, V), (8)\nwhere ZT ‚àà RLT√óD. Besides the compressed key-value\npairs, we set the query length LT = ‚åäc √ólog L‚åã (c is a small\nconstant factor) in practice, reducing the cost and achieving\nO(L log L) complexity.\nRecombination From the parallel attention module, we\nget ZF and ZT for the queries with indices iF and iT (from\nEq.(3)). ZF and ZT will be recombined together based on the\nindices iF and iT. Since there are unselected indices, the total\noutput length is insufficient. To address this problem, we ob-\ntain the global context information by value aggregation and\nincorporate it into the recombination result (as the values\nof unselected indices). Especially for the multi-head version\n(each head has different unselected indices), this processing\ncan be considered as the concatenation between the feature\nmaps of global context and those of the parallel attention\nmodule. The recombination process is as follows:\nZ = Aggregate(V),\nZ[iF] =ZF,\nZ[iT] =ZT,\n(9)\nwhere Z ‚àà RL√óD is the recombination result and initialized\nwith the global context information by value aggregation. In\npractice, the aggregation is performed by sum(¬∑)/L.\nLinearh\nLinearl\nùêí!\nùêí\"\nùêí\nFeed\nFusion ùêí#$%\nùêì\nùêó Binary\nDecomp\nFigure 5: Evolutionary seasonal-trend decomposition\n(EvoSTD) module. The binary decomposition yields the\nseasonal and trend components. The high and low-frequency\ninformation Sh, Sl of seasonal component are yielded by\nthe specific linear layers. They are further combined with\nthe seasonal component by a special Feed-Forward layer,\ni.e., FeedFusion.\nEvolutionary Seasonal-Trend Decomposition\nAs shown in Figure 5, we propose an evolutionary seasonal-\ntrend decomposition (EvoSTD) module to learn entangled\ntemporal patterns. In EvoSTD, the seasonal and trend parts\nare first yielded by the binary decomposition. Since the sea-\nsonal part contains rich frequency information, two linear\nlayers initialized with the wavelet coefficients are adopted\nto extract high and low-frequency information. Then the sea-\nsonal part is combined with the frequency information and\ntransformed by FeedFusion for an evolution. The details can\nbe formalized as:\nS, T = BinaryDecomp(X),\nSevo = FeedFusion(Concat[S, WhS, WlS]) (10)\nwhere X ‚àà RL√óD is the input series; Wh, Wl ‚àà RL√óL\nare the specific linear layers; FeedFusionis a special Feed-\nForward layer with in channels = 3D; Sevo, T ‚àà RL√óD\nare the evolutionary seasonal part and trend. The process can\nbe summarized as: Sevo, T = EvoSTD(X).\n6910\nModels InParformer FEDformer Autoformer Informer LogTrans Reformer\nMetric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE\nETTm2\n96 0.198 0.288 0.203 0.287 0.255 0.339 0.365 0.453 0.768 0.642 0.658 0.619\n192 0.260 0.323 0.269 0.328 0.281 0.340 0.533 0.563 0.989 0.757 1.078 0.827\n336 0.319 0.365 0.325 0.366 0.339 0.372 1.363 0.887 1.334 0.872 1.549 0.972\n720 0.420 0.417 0.421 0.415 0.422 0.419 3.379 1.338 3.048 1.328 2.631 1.242\nElectr.\n96 0.184 0.296 0.193 0.308 0.201 0.317 0.274 0.368 0.258 0.357 0.312 0.402\n192 0.186 0.298 0.201 0.315 0.222 0.334 0.296 0.386 0.266 0.368 0.348 0.433\n336 0.202 0.314 0.214 0.329 0.231 0.338 0.300 0.394 0.280 0.380 0.350 0.433\n720 0.228 0.335 0.246 0.355 0.254 0.361 0.373 0.439 0.283 0.376 0.340 0.420\nExchange\n96 0.120 0.252 0.148 0.278 0.197 0.323 0.847 0.752 0.968 0.812 1.065 0.829\n192 0.230 0.354 0.271 0.308 0.300 0.369 1.204 0.895 1.040 0.851 1.188 0.906\n336 0.427 0.482 0.460 0.500 0.509 0.524 1.672 1.036 1.659 1.081 1.357 0.976\n720 1.106 0.813 1.195 0.841 1.447 0.941 2.478 1.310 1.941 1.127 1.510 1.016\nTraf\nfic\n96 0.557 0.340 0.587 0.366 0.613 0.388 0.719 0.391 0.684 0.384 0.732 0.423\n192 0.577 0.349 0.604 0.373 0.616 0.382 0.696 0.379 0.685 0.390 0.733 0.420\n336 0.597 0.359 0.621 0.383 0.622 0.337 0.777 0.420 0.733 0.408 0.742 0.420\n720 0.612 0.364 0.626 0.382 0.660 0.408 0.864 0.472 0.717 0.396 0.755 0.423\nWeather\n96 0.212 0.286 0.217 0.296 0.266 0.336 0.300 0.384 0.458 0.490 0.689 0.596\n192 0.259 0.322 0.276 0.336 0.307 0.367 0.598 0.544 0.658 0.589 0.752 0.638\n336 0.317 0.358 0.339 0.380 0.359 0.395 0.578 0.523 0.797 0.652 0.639 0.596\n720 0.395 0.409 0.403 0.428 0.419 0.428 1.059 0.741 0.869 0.675 1.130 0.792\nILI\n24 2.934 1.107 3.228 1.260 3.483 1.287 5.764 1.677 4.480 1.444 4.400 1.382\n36 3.049 1.069 2.679 1.080 3.103 1.148 4.755 1.467 4.799 1.467 4.783 1.448\n48 3.067 1.088 2.622 1.078 2.669 1.085 4.763 1.469 4.800 1.468 4.832 1.465\n60 3.043 1.116 2.857 1.157 2.770 1.125 5.264 1.564 5.278 1.560 4.882 1.483\nTable 1: Multivariate results with different prediction lengths Ly ‚àà {96, 192, 336, 720} and fixed input length Lx = 96(For\nILI, Ly ‚àà {24,36, 48, 60}, Lx = 36). Electr. is short for the Electricity dataset.\nAlgorithm 1: Binary Decomposition\nInput: Series X with length L.\n1: Initialization: seasonal component S = X; trend com-\nponent T = 0; current segment is X\n2: while length of a current segment ‚â• 2 do\n3: let M: average of each current segment\n4: update S: subtract M\n5: update T: add M\n6: update current segments: split in half\n7: end while\n8: update T: smooth T by moving average\n9: return S, T\nBinary Decomposition For the intricate seasonal patterns\nin real-world data, we design a binary decomposition algo-\nrithm based on the divide-and-conquer paradigm, which ex-\ntracts seasonality without predefined fixed-window average\nfilters. The pseudo-code for binary decomposition is pre-\nsented in Algorithm 1.\nComplexity Analysis\nIn this work, the theoretical complexities of FAA and TAA\nare O(L‚Ä≤L) and O(L‚Ä≤ log L), respectively (L ‚Ä≤ is the com-\npressed length by IPConv). However, the query lengths of\nFAA and TAA mainly depend on query selection, so they are\nadjustable. Moreover, IPConv and RFFT further reduce the\ncomputational cost. Concretely, the attention matrix of TAA\nis c log L √ó L‚Ä≤, while that of FAA is L‚àíc log L\n2 √ó L‚Ä≤\n2 , where\n‚Äô1\n2 ‚Äô is caused by RFFT. Although InPar Attention contains\nparallel attention modules, it is efficient.\nExperiment\nTo evaluate the proposed InParformer, we perform exten-\nsive experiments on six popular real-world benchmarks, in-\ncluding energy, traffic, economics, weather, and disease do-\nmains. More detailed experimental information is provided\nin the Appendices.\nDatasets (1) ETT (Zhou et al. 2021) contains load and oil\ntemperature collected from electricity transformers. It has\nfour sub-datasets as ETTh1, ETTh2, ETTm1, and ETTm2 in\ntwo resolutions (1 hour and 15 minutes). (2)Electricity1 in-\ncludes the hourly electricity consumption of 321 clients. (3)\nExchange (Lai et al. 2018) collects daily exchange rates for\n1https://archive.ics.uci.edu/ml/datasets/\nElectricityLoadDiagrams20112014\n6911\nModels InParformer FEDformer Autoformer Informer LogTrans Reformer\nMetric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE\nWeather\n96 0.0022 0.036 0.0062 0.062 0.0110 0.081 0.004 0.044 0.0046 0.052 0.012 0.087\n192 0.0038 0.048 0.0060 0.062 0.0075 0.067 0.002 0.040 0.0060 0.060 0.010 0.044\n336 0.0033 0.045 0.0041 0.050 0.0063 0.062 0.004 0.049 0.0060 0.054 0.013 0.100\n720 0.0030 0.042 0.0055 0.059 0.0085 0.070 0.003 0.042 0.0070 0.059 0.011 0.083\nETTh2\n96 0.117 0.264 0.128 0.271 0.153 0.306 0.213 0.373 0.217 0.379 1.411 0.838\n192 0.169 0.319 0.185 0.330 0.204 0.351 0.227 0.387 0.281 0.429 5.658 1.671\n336 0.225 0.373 0.231 0.378 0.246 0.389 0.242 0.401 0.293 0.437 4.777 1.582\n720 0.241 0.399 0.278 0.420 0.268 0.409 0.291 0.439 0.218 0.387 2.042 1.039\nExchange\n96 0.105 0.247 0.154 0.304 0.241 0.387 1.327 0.944 0.237 0.377 0.298 0.444\n192 0.207 0.360 0.286 0.420 0.300 0.369 1.258 0.924 0.738 0.619 0.777 0.719\n336 0.400 0.498 0.511 0.555 0.509 0.524 2.179 1.296 2.018 1.070 1.833 1.128\n720 1.172 0.836 1.301 0.879 1.260 0.867 1.280 0.953 2.405 1.175 1.203 0.956\nILI\n24 0.598 0.564 0.708 0.627 0.948 0.732 5.282 2.050 3.607 1.662 3.838 1.720\n36 0.553 0.592 0.584 0.617 0.634 0.650 4.554 1.916 2.407 1.363 2.934 1.520\n48 0.653 0.658 0.717 0.697 0.791 0.752 4.273 1.846 3.106 1.575 3.755 1.749\n60 0.789 0.748 0.855 0.774 0.874 0.797 5.214 2.057 3.698 1.733 4.162 1.847\nTable 2: Univariate results with different prediction lengthsLy ‚àà {96,192, 336, 720} and fixed input lengthLx = 96on typical\ndatasets (For ILI, Ly ‚àà {24,36, 48, 60}, Lx = 36). Weather, ETTh2, Exchange, and ILI datasets are 10-minutely, hourly, daily,\nand weekly recorded, respectively.\n8 different countries from 1990 to 2016. (4) Traffic2 con-\ntains the hourly road occupancy rates from the California\nDepartment of Transportation. (5)Weather3 records 21 me-\nteorological indicators every 10 minutes for one year. (6)\nILI4 includes the weekly patients with influenza-like illness\n(ILI) from 2002 to 2021. For all datasets, train/valid/test sets\nare split as 0.7/0.1/0.2 in chronological order.\nBaselines Since classic models like ARIMA and\nCNN/RNN-based networks lack competitive performance\n(Zhou et al. 2021; Wu et al. 2021), we select five state-of-\nthe-art Transformer-based models: FEDformer (Zhou et al.\n2022), Autoformer (Wu et al. 2021), Informer (Zhou et al.\n2021), Reformer (Kitaev, Kaiser, and Levskaya 2020), and\nLogTrans (Li et al. 2019). The FEDformer (FEDformer-f,\nalso based on Fourier transform) is used as the main baseline\nmodel because of its best performance in these baselines.\nImplementation Settings The mean square error (MSE)\nand mean absolute error (MAE) are used as metrics. The\nproposed model is trained using Adam (Kingma and Ba\n2014) optimizer with an initial learning rate of 10‚àí4 and\ncontains 2 encoder layers and 1 decoder layer. The sam-\npling factor c for query selection is set to 3. The batch\nsize is set to 32, and the training epochs are set to 10 with\nearly stopping. The common hyperparameters are consistent\nwith FEDformer (Zhou et al. 2022) and Autoformer (Wu\net al. 2021). All experiments are implemented with PyTorch\n(Paszke et al. 2019) and conducted on the VenusAI plat-\n2http://pems.dot.ca.gov\n3https://www.bgc-jena.mpg.de/wetter/\n4https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html\nform (Yao et al. 2022) with 4 NVIDIA GeForce RTX 2080Ti\n11GB GPUs.\nMain Results\nTo evaluate different future horizons, we fix the input length\nLx = 96 (Lx = 36 for ILI) and set the prediction lengths\nLy ‚àà {96, 192, 336, 720} (Ly ‚àà {24, 36, 48, 60} for ILI).\nThe detailed results of ETT full benchmark are given in Ap-\npendix A.\nMultivariate Results As shown in Table 1, InParformer\nachieves the best performance in most cases. For example,\nunder the input-96-predict-192 setting, compared to FED-\nformer, InParformer yields 3.3% (0.269 ‚Üí 0.260) MSE\nreduction in ETTm2, 7.5% (0.201 ‚Üí 0.186) in Electric-\nity, 15.1% (0.271 ‚Üí 0.230) in Exchange and 6.2% (0.276\n‚Üí 0.259) in Weather. Overall, InParformer achieves a rela-\ntive MSE reduction of 5.9% (excluding ILI) compared with\nFEDformer. Even for the Exchange dataset that lacks clear\nperiodicity, InParformer still outperforms other models. No-\ntably, the performance of InParformer varies consistently as\nthe future horizon increases, which indicates InParformer\nperforms a stable prediction.\nUnivariate Results As shown in Table 2, InParformer\nstill achieves the best performance on the typical datasets\nwith various temporal resolutions. Compared to FEDformer,\nInParformer significantly improves the forecasting perfor-\nmance and gives an overall MSE reduction of20.5%. In par-\nticular, for both datasets with obvious periodicity (ETTh2)\nand those without (Exchange), InParformer outperforms\nother models. Besides, for these typical datasets with var-\nious domains and resolutions, InParformer achieves signif-\n6912\nicant and stable improvements, which indicates its advan-\ntages in prediction capacity.\nDecomp EvoSTD MOE STD\nMetric MSE MAE MSE MAE MSE MAE\nETTh1\n96 0.379 0.419 0.514 0.483 0.488 0.479\n192 0.416 0.440 0.563 0.513 0.546 0.503\n336 0.455 0.458 0.536 0.506 0.582 0.525\n720 0.475 0.488 0.602 0.558 0.574 0.549\nWeather\n96 0.212 0.286 0.277 0.350 0.240 0.316\n192 0.259 0.322 0.336 0.382 0.311 0.367\n336 0.317 0.358 0.379 0.413 0.344 0.388\n720 0.395 0.409 0.409 0.420 0.423 0.434\nTable 3: Comparison of decomposition methods. STD and\nMOE denote the seasonal-trend decomposition blocks from\nAutoformer and FEDformer, respectively.\nAttn ProbAttn AutoCorr FEA InParAttn\nMetric MSE MAE MSE MAE MSE MAE MSE MAE\nETTh1\n96 0.439 0.456 0.397 0.427 0.381 0.423 0.379 0.419\n192 0.562 0.524 0.439 0.456 0.417 0.443 0.416 0.440\n336 0.509 0.502 0.464 0.466 0.457 0.465 0.455 0.458\n720 0.603 0.565 0.473 0.490 0.476 0.483 0.475 0.488\nWeather\n96 0.242 0.330 0.200 0.285 0.210 0.288 0.212 0.286\n192 0.362 0.423 0.292 0.362 0.271 0.332 0.259 0.322\n336 0.381 0.427 0.363 0.416 0.451 0.451 0.317 0.358\n720 0.575 0.466 0.529 0.529 0.414 0.427 0.395 0.409\nTable 4: Comparison of attention mechanisms. ProbAttn,\nAutoCorr, and FEA denote the attention mechanisms from\nInformer, Autoformer, and FEDformer, respectively.\nAblation Studies\nTo evaluate the effectiveness of our designs, we perform ad-\nditional experiments on typical datasets.\nDecomposition Methods We replace EvoSTD in InPar-\nformer with the series decomposition blocks of Autoformer\nand FEDformer, i.e., STD and MOE. For a fair compari-\nson, STD and MOE in InParformer are under their default\nsettings and followed by a Feed-Forward layer. As shown\nin Table 3, EvoSTD achieves better performance than other\nseries decomposition blocks, which indicates its strength in\nextracting temporal patterns.\nAttention Mechanisms We replace InPar Attention in In-\nParformer with the attention mechanisms of Informer, Aut-\noformer, and FEDformer. As shown in table 4, the proposed\nInPar Attention achieves the best performance on two typi-\ncal datasets. For the ETTh1 dataset with clear periodicity, In-\nPar Attention yields a slight improvement compared to FEA\n(the frequency enhanced attention from FEDformer) but sig-\nnificant improvements compared to other attention mecha-\nnisms. This implies that frequency information is vital for\nperiodic time series. For the Weather dataset without clear\nperiodicity, InPar Attention still brings significant improve-\nments, which shows the success of parallel attention design.\nIPConv and Time-Aware Attention Types For InPar At-\ntention, we evaluate the effectiveness of IPConv and the\nimpact of time-aware attention (TAA) types. As shown in\nthe left part of Table 5, IPConv gives obvious performance\nimprovement, which implies it can learn effective multi-\nresolution temporal partitions. As shown in the right part of\nTable 5, TAA based on additive attention (Bahdanau, Cho,\nand Bengio 2014), which contains learnable parameters for\nscoring, is more suitable for InPar Attention than that based\non dot-product attention.\nOption IPConv W/o IPConv TAA-A TAA-D\nMetric MSE MAE MSE MAE MSE MAE MSE MAE\nETTh1\n96 0.379 0.419 0.378 0.415 0.379 0.419 0.439 0.457\n192 0.416 0.440 0.418 0.441 0.416 0.440 0.568 0.537\n336 0.455 0.458 0.456 0.461 0.455 0.458 0.502 0.498\n720 0.475 0.488 0.546 0.536 0.475 0.488 0.604 0.560\nWeather\n96 0.212 0.286 0.211 0.268 0.212 0.286 0.205 0.279\n192 0.259 0.322 0.293 0.352 0.259 0.322 0.275 0.333\n336 0.317 0.358 0.350 0.386 0.317 0.358 0.379 0.420\n720 0.395 0.409 0.425 0.428 0.395 0.409 0.454 0.458\nTable 5: Ablation studies of IPConv and time-aware atten-\ntion (TAA) types. Left: The effectiveness of IPConv in InPar\nAttention. Right: The impact of TAA types (with IPConv).\nTAA-A denotes TAA based on additive attention. TAA-D\ndenotes TAA based on dot-product attention.\nConclusions\nIn this paper, we propose a novel Transformer-based model\nnamed InParformer for long-term time series forecasting.\nTo capture long-range dependencies comprehensively, we\npropose InPar Attention as an interactive parallel attention\nmechanism performing in both the time and frequency do-\nmains. Considering that time series with temporal redun-\ndancy are weakly semantic, we design query selection and\nkey-value pair compression (via an interactive partitioned\nconvolution module), which can also improve the compu-\ntation efficiency. In the recombination stage, the global con-\ntext information and the outputs of two sub-attention mech-\nanisms are aggregated together. Moreover, the evolutionary\nseasonal-trend decomposition module is deployed in InPar-\nformer to enhance intricate pattern extraction. Extensive ex-\nperiments on real-world benchmarks show that InParformer\nis capable of forecasting long-term time series effectively.\nAcknowledgements\nThis work was supported by the National Key R&D Pro-\ngram of China (2021ZD0110403). We also gratefully ac-\nknowledge the support of MindSpore team.\n6913\nReferences\nBahdanau, D.; Cho, K.; and Bengio, Y . 2014. Neural Ma-\nchine Translation by Jointly Learning to Align and Trans-\nlate. arXiv:1409.0473.\nBai, S.; Kolter, J. Z.; and Koltun, V . 2018. An Empiri-\ncal Evaluation of Generic Convolutional and Recurrent Net-\nworks for Sequence Modeling. arXiv:1803.01271.\nBengio, S.; Vinyals, O.; Jaitly, N.; and Shazeer, N. 2015.\nScheduled Sampling for Sequence Prediction with Recur-\nrent Neural Networks. In Cortes, C.; Lawrence, N.; Lee,\nD.; Sugiyama, M.; and Garnett, R., eds., Advances in Neu-\nral Information Processing Systems, volume 28. Curran As-\nsociates, Inc.\nBox, G. E.; and Jenkins, G. M. 1968. Some Recent Ad-\nvances in Forecasting and Control.Journal of the Royal Sta-\ntistical Society. Series C (Applied Statistics), 17(2): 91‚Äì109.\nChen, L.-C.; Papandreou, G.; Kokkinos, I.; Murphy, K.; and\nYuille, A. L. 2017. Deeplab: Semantic Image Segmenta-\ntion with Deep Convolutional Nets, Atrous Convolution, and\nFully Connected Crfs. IEEE transactions on pattern analy-\nsis and machine intelligence, 40(4): 834‚Äì848.\nChen, T.; and Guestrin, C. 2016. Xgboost: A Scalable\nTree Boosting System. In Proceedings of the 22nd Acm\nSigkdd International Conference on Knowledge Discovery\nand Data Mining, 785‚Äì794.\nChild, R.; Gray, S.; Radford, A.; and Sutskever, I. 2019.\nGenerating Long Sequences with Sparse Transformers.\narXiv:1904.10509.\nCho, K.; van Merrienboer, B.; Bahdanau, D.; and Bengio,\nY . 2014. On the Properties of Neural Machine Translation:\nEncoder-Decoder Approaches. arXiv:1409.1259.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBERT: Pre-Training of Deep Bidirectional Transformers for\nLanguage Understanding. arXiv:1810.04805.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2022.\nAn Image Is Worth 16x16 Words: Transformers for Image\nRecognition at Scale. In International Conference on Learn-\ning Representations.\nGardner Jr, E. S. 1985. Exponential Smoothing: The State\nof the Art. Journal of forecasting, 4(1): 1‚Äì28.\nGuo, Q.; Qiu, X.; Xue, X.; and Zhang, Z. 2019. Low-Rank\nand Locality Constrained Self-Attention for Sequence Mod-\neling. IEEE/ACM Transactions on Audio, Speech, and Lan-\nguage Processing, 27(12): 2213‚Äì2222.\nHe, K.; Chen, X.; Xie, S.; Li, Y .; Doll¬¥ar, P.; and Girshick, R.\n2021. Masked Autoencoders Are Scalable Vision Learners.\narXiv:2111.06377.\nHochreiter, S.; and Schmidhuber, J. 1997. Long Short-Term\nMemory. Neural computation, 9(8): 1735‚Äì1780.\nHyndman, R. J.; and Athanasopoulos, G. 2021.Forecasting:\nPrinciples and Practice. OTexts, 3rd edition.\nKingma, D. P.; and Ba, J. 2014. Adam: A Method for\nStochastic Optimization. arXiv:1412.6980.\nKitaev, N.; Kaiser, L.; and Levskaya, A. 2020. Reformer:\nThe Efficient Transformer. In Eighth International Confer-\nence on Learning Representations.\nLai, G.; Chang, W.-C.; Yang, Y .; and Liu, H. 2018. Modeling\nLong-and Short-Term Temporal Patterns with Deep Neural\nNetworks. In The 41st International ACM SIGIR Conference\non Research & Development in Information Retrieval , 95‚Äì\n104.\nLi, S.; Jin, X.; Xuan, Y .; Zhou, X.; Chen, W.; Wang, Y .-X.;\nand Yan, X. 2019. Enhancing the Locality and Breaking the\nMemory Bottleneck of Transformer on Time Series Fore-\ncasting. In Advances in Neural Information Processing Sys-\ntems, volume 32. Curran Associates, Inc.\nLin, T.; Wang, Y .; Liu, X.; and Qiu, X. 2021. A Survey of\nTransformers. arXiv:2106.04554.\nLindberg, K.; Seljom, P.; Madsen, H.; Fischer, D.; and\nKorpÀöas, M. 2019. Long-Term Electricity Load Forecasting:\nCurrent and Future Trends. Utilities Policy, 58: 102‚Äì119.\nLiu, P. J.; Saleh, M.; Pot, E.; Goodrich, B.; Sepassi, R.;\nKaiser, L.; and Shazeer, N. 2018. Generating Wikipedia by\nSummarizing Long Sequences. In International Conference\non Learning Representations.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021. Swin Transformer: Hierarchical Vi-\nsion Transformer Using Shifted Windows. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, 10012‚Äì10022.\nOppenheim, A. V . 1999. Discrete-Time Signal Processing.\nPearson Education India.\nPaszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.;\nChanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.; Antiga,\nL.; et al. 2019. Pytorch: An Imperative Style, High-\nPerformance Deep Learning Library. Advances in neural\ninformation processing systems, 32.\nRangapuram, S. S.; Seeger, M. W.; Gasthaus, J.; Stella, L.;\nWang, Y .; and Januschowski, T. 2018. Deep State Space\nModels for Time Series Forecasting. Advances in neural\ninformation processing systems, 31.\nRobert, C.; William, C.; and Irma, T. 1990. STL: A\nSeasonal-Trend Decomposition Procedure Based on Loess.\nJournal of official statistics, 6(1): 3‚Äì73.\nSalinas, D.; Flunkert, V .; Gasthaus, J.; and Januschowski, T.\n2020. DeepAR: Probabilistic Forecasting with Autoregres-\nsive Recurrent Networks. International Journal of Forecast-\ning, 36(3): 1181‚Äì1191.\nSen, R.; Yu, H.-F.; and Dhillon, I. S. 2019. Think Globally,\nAct Locally: A Deep Neural Network Approach to High-\nDimensional Time Series Forecasting. Advances in neural\ninformation processing systems, 32.\nShi, X.; and Yeung, D.-Y . 2018. Machine Learn-\ning for Spatiotemporal Sequence Forecasting: A Survey.\narXiv:1808.06865.\nSmola, A. J.; and Sch¬®olkopf, B. 2004. A Tutorial on Support\nVector Regression. Statistics and computing, 14(3): 199‚Äì\n222.\n6914\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At-\ntention Is All You Need. arXiv:1706.03762.\nWang, S.; Li, B. Z.; Khabsa, M.; Fang, H.; and Ma, H.\n2020. Linformer: Self-Attention with Linear Complexity.\narXiv:2006.04768.\nWu, H.; Xu, J.; Wang, J.; and Long, M. 2021. Auto-\nformer: Decomposition Transformers with Auto-Correlation\nfor Long-Term Series Forecasting. In Advances in Neural\nInformation Processing Systems, volume 34, 22419‚Äì22430.\nCurran Associates, Inc.\nYao, T.; Wang, J.; Wan, M.; Xin, Z.; Wang, Y .; Cao, R.; Li,\nS.; and Chi, X. 2022. VenusAI: An artificial intelligence\nplatform for scientific discovery on supercomputers.Journal\nof Systems Architecture, 128: 102550.\nZhao, H.; Shi, J.; Qi, X.; Wang, X.; and Jia, J. 2017. Pyramid\nScene Parsing Network. arXiv:1612.01105.\nZhou, H.; Zhang, S.; Peng, J.; Zhang, S.; Li, J.; Xiong, H.;\nand Zhang, W. 2021. Informer: Beyond Efficient Trans-\nformer for Long Sequence Time-Series Forecasting. Pro-\nceedings of the AAAI Conference on Artificial Intelligence ,\n35(12): 11106‚Äì11115.\nZhou, T.; Ma, Z.; Wen, Q.; Wang, X.; Sun, L.; and Jin,\nR. 2022. FEDformer: Frequency Enhanced Decomposed\nTransformer for Long-term Series Forecasting. In Pro-\nceedings of the 39th International Conference on Machine\nLearning, 27268‚Äì27286. PMLR.\n6915",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7703045010566711
    },
    {
      "name": "Transformer",
      "score": 0.646625816822052
    },
    {
      "name": "Grammatical evolution",
      "score": 0.5435840487480164
    },
    {
      "name": "Redundancy (engineering)",
      "score": 0.4587959945201874
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4426402747631073
    },
    {
      "name": "Time series",
      "score": 0.4266929626464844
    },
    {
      "name": "Machine learning",
      "score": 0.3811570405960083
    },
    {
      "name": "Data mining",
      "score": 0.3579264283180237
    },
    {
      "name": "Genetic programming",
      "score": 0.08853542804718018
    },
    {
      "name": "Engineering",
      "score": 0.0810767114162445
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}