{
    "title": "Entropy Guided Spectrum Based Bug Localization Using Statistical Language Model",
    "url": "https://openalex.org/W2789200353",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2693064437",
            "name": "Chakraborty Saikat",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2025050553",
            "name": "Li YuJian",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Irvine, Matt",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4226841219",
            "name": "Saha, Ripon",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3046368973",
            "name": "Ray, Baishakhi",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W1980015727",
        "https://openalex.org/W2465098971",
        "https://openalex.org/W1974020522",
        "https://openalex.org/W1970859802",
        "https://openalex.org/W2163732854",
        "https://openalex.org/W2165747537",
        "https://openalex.org/W1972978214",
        "https://openalex.org/W2143960295",
        "https://openalex.org/W2096795881",
        "https://openalex.org/W1993220166",
        "https://openalex.org/W2053724458",
        "https://openalex.org/W1771830246",
        "https://openalex.org/W2133029931",
        "https://openalex.org/W2101819268",
        "https://openalex.org/W1966413371",
        "https://openalex.org/W2162376048",
        "https://openalex.org/W2116737258",
        "https://openalex.org/W1981537308",
        "https://openalex.org/W2156723666",
        "https://openalex.org/W2089759055",
        "https://openalex.org/W2147699889",
        "https://openalex.org/W2360967250",
        "https://openalex.org/W2036196659",
        "https://openalex.org/W2067148378",
        "https://openalex.org/W2033644368",
        "https://openalex.org/W1941659294",
        "https://openalex.org/W1547528813",
        "https://openalex.org/W1975040830",
        "https://openalex.org/W2142403498",
        "https://openalex.org/W2070249305",
        "https://openalex.org/W2123659430",
        "https://openalex.org/W1985947101",
        "https://openalex.org/W2060384944",
        "https://openalex.org/W2107890099",
        "https://openalex.org/W2514538448",
        "https://openalex.org/W1728842521",
        "https://openalex.org/W2143861926",
        "https://openalex.org/W2911964244",
        "https://openalex.org/W2153418968",
        "https://openalex.org/W2120391124",
        "https://openalex.org/W841012168",
        "https://openalex.org/W2165663378",
        "https://openalex.org/W2043811931",
        "https://openalex.org/W3145932680",
        "https://openalex.org/W2046830558",
        "https://openalex.org/W2170224888",
        "https://openalex.org/W2121227244",
        "https://openalex.org/W2162045655"
    ],
    "abstract": "Locating bugs is challenging but one of the most important activities in software development and maintenance phase because there are no certain rules to identify all types of bugs. Existing automatic bug localization tools use various heuristics based on test coverage, pre-determined buggy patterns, or textual similarity with bug report, to rank suspicious program elements. However, since these techniques rely on information from single source, they often suffer when the respective source information is inadequate. For instance, the popular spectrum based bug localization may not work well under poorly written test suite. In this paper, we propose a new approach, EnSpec, that guides spectrum based bug localization using code entropy, a metric that basically represents naturalness of code derived from a statistical language model. Our intuition is that since buggy code are high entropic, spectrum based bug localization with code entropy would be more robust in discriminating buggy lines vs. non-buggy lines. We realize our idea in a prototype, and performed an extensive evaluation on two popular publicly available benchmarks. Our results demonstrate that EnSpec outperforms a state-of-the-art spectrum based bug localization technique.",
    "full_text": "Entropy Guided Spectrum Based Bug Localization Using\nStatistical Language Model\nSaikat Chakraborty1,*, Yujian Li2,*, Matt Irvine3,*, Ripon Saha+, and Baishakhi Ray5,*\n*Department of Computer Science, University of Virginia, Charlottesville, V A - 22903\n1,2,3,5{saikatc, yl7kd, mji7wb, rayb}@virginia.edu\n+Fujitsu Laboratories of America, Sunnyvale, CA - 94085\n+rsaha@us.fujitsu.com\nABSTRACT\nLocating bugs is challenging but one of the most important activi-\nties in software development and maintenance phase because there\nare no certain rules to identify all types of bugs. Existing automatic\nbug localization tools use various heuristics based on test cover-\nage, pre-determined buggy patterns, or textual similarity with bug\nreport, to rank suspicious program elements. However, since these\ntechniques rely on information from single source, they often suffer\nwhen the respective source information is inadequate. For instance,\nthe popular spectrum based bug localization may not work well un-\nder poorly written test suite. In this paper, we propose a new ap-\nproach, EnSpec, that guides spectrum based bug localization using\ncode entropy, a metric that basically representsnaturalness of code\nderived from a statistical language model. Our intuition is that since\nbuggy code are high entropic, spectrum based bug localization with\ncode entropy would be more robust in discriminating buggy lines\nvs. non-buggy lines. We realize our idea in a prototype, and per-\nformed an extensive evaluation on two popular publicly available\nbenchmarks. Our results demonstrate that EnSpec outperforms a\nstate-of-the-art spectrum based bug localization technique.\nKeywords\nBug Localization, Naturalness of bug, Spectrum based testing, Hy-\nbrid bug-localization\n1. INTRODUCTION\nLocalizing bugs is an important, time consuming, and expensive\nprocess, especially for a system at-scale. Automatic bug localiza-\ntion can play an important role in saving developers’ time in debug-\nging, and thus, may help developers ﬁxing more bugs in a limited\ntime. Using various statistical and program analysis approaches,\nthese bug localization techniques automatically identify suspicious\ncode elements that are highly likely to contain bugs. Developers\nthen manually examine these suspicious code to pinpoint the bugs.\nExisting bug localization techniques can be broadly classiﬁed\ninto two categories: i) test coverage-based dynamic approaches [30,\n2, 59, 14, 37, 38], and ii) pattern-based [15, 19, 18, 13] or infor-\nmation retrieval-based (IR) static approaches [44, 60, 48, 58]. Dy-\nnamic approaches ﬁrst run all the test cases, and then analyze the\nprogram statements covered by passing and failing test cases. For\nexample, spectrum based bug localization ( SBBL), a popular dy-\nnamic bug localization technique, prioritizes the program elements\nfor debugging that are executed more by failing test cases than by\npassing test cases. In contrast, static approaches do not run any test\ncases. Rather, it searches for some previously known buggy pat-\nterns in source code or looks for buggy ﬁles based on bug reports.\nBoth of these bug localization approaches have their own set of\nadvantages and disadvantages. For instance, static methods are of-\nten imprecise or inaccurate. On the other hand, the accuracy of\ndynamic approaches is highly dependent on the quality (code cov-\nerage, etc.) of the test suite. In real world projects, most of the test\nsuite may not have enough code coverage to locate bugs efﬁciently.\nTherefore, in many cases, developers do not get the full beneﬁt of\nbug localization techniques [29] and have to signiﬁcantly rely on\nmanual effort and prior experiences.\nBesides static and dynamic properties of a program, it has also\nbeen observed that how developers write code is also important for\ncode quality [25]. Real-world software that are developed by reg-\nular programmers tend to be highly repetitive and predictable [22].\nHindle et al. was the ﬁrst to show that such repetitiveness can be\nsuccessfully captured by a statistical language model [27]. They\ncalled this property as naturalness of code and measured it by stan-\ndard information theory metric entropy. The less entropy a code\nsnippet exhibits, the more the code is natural. Inspired by this phe-\nnomena, Ray et al. [45] investigated if there is any correlation be-\ntween buggy code and entropy. They observed that buggy codes are\nin general less natural,i.e. they have higher entropy than non-buggy\ncode.\nIn this paper, our key intuition is that, since the high entropic\ncode tends to be buggy [45, 11, 54], code entropy can be an ef-\nfective orthogonal source of information to SBBLto improve the\noverall accuracy of bug localization. This notion seems to be plau-\nsible since from a set of suspicious code, as reported by a standard\nSBBLtechnique, experienced programmers often intuitively iden-\ntify the actual bugs because buggy code elements are usually a bit\nunnatural than rest of the corpus. If entropy can improve SBBL, it\nwould be particularly useful when a test suite is not strong enough\nto discriminate buggy lines or when the suspicious scores of many\nlines are the same. Furthermore, to realize this hybrid approach,\narXiv:1802.06947v1  [cs.SE]  20 Feb 2018\nwe only need source code (no other external meta-source), which\nis always available to the developers.\nTo this end, we introduce EnSpec, that automatically calculates\nthe entropy for each program line and combines with a state-of-the-\nart SBBLusing a machine learning technique to return a ranked list\nof suspicious lines for investigation. Here, we studied bug localiza-\ntion at line granularity to ensure maximum beneﬁt to the develop-\ners, although locating bugs in method and ﬁle-granularity is also\npossible. We performed an extensive evaluation of EnSpec on two\npopular publicly available bug-dataset: Defects4J [32] and Many-\nBugs [34] written in Java and C respectively. In total, we studied\nmore than 500 bugs (3,715 buggy lines) bugs, around 4M LOC\nfrom 10 projects.\nOverall, our ﬁndings corroborate our hypothesis that entropy can\nindeed improve bug localization capability of spectrum based bug\nlocalization technique. We further evaluate EnSpec for both C and\nJava projects showing that the tool is not programming language\ndependent. In particular, our results show that:\n•Entropy score, as captured by statistical language models,\ncan signiﬁcantly improve bug localization capability of stan-\ndard SBBLtechnique.\n•Entropy score also boosts SBBLin cross-project bug local-\nization settings.\nIn summary, we make the following contributions in this paper:\n1. We introduce the notion of entropy in spectrum based bug\nlocalization.\n2. We present EnSpec that effectively combine entropy score\nwith the suspicious score of spectrum based bug localization\nusing a machine learning technique.\n3. We provide an extensive evaluation of EnSpec on two pub-\nlicly available benchmarks that demonstrates the effective-\nness of our approach.\n2. PRELIMINARIES\nIn this section, we discuss the preliminaries and backgrounds of\nour work.\n2.1 Spectrum-based Bug Localization\nGiven a buggy code-base with at least one bug reproducing test\ncase, a spectrum-based bug localization technique ( SBBL) ranks\nthe code elements under investigation (e.g., ﬁles/classes, function-\ns/methods, blocks, or statements) based on the execution traces of\npassing and failing test cases. Therefore, in this approach, ﬁrst the\nsubject program is instrumented at an appropriate granularity to\ncollect the execution trace of each test case ( test spectra). The ba-\nsic intuition behind SBBLis that the more a code element appears\nin failing traces (but not in passing traces), the more suspicious the\nelement to be buggy.\nMore speciﬁcally, for a given program element e, SBBLrecords\nhow many test cases execute and do not execute e, and computes\nthe following four metrics: the number of (i) tests passed ( ep) and\n(ii) tests failed ( ef ) that executed e, and the number of (iii) tests\npassed (np) and (iv) tests failed (nf ) that did not execute e. A sus-\npiciousness score is calculated is calculated as a function of these\nfour metric: S = Func(ep,ef ,np,nf ), as shown in Table 1. The\ntable also presents two widely used suspiciousness score measure:\nTarantula and Ochai. SBBLranks the program elements in a de-\ncreasing order of suspiciousness and presents to the developers for\nfurther investigation to ﬁx the bug [57]. These scores also help to\nrepair program automatically [35].\nTable 1: Spectrum based suspiciousness score of program element e\n#passed #failed spectrum\ntest test score\ntotal tests P F S = Func(ep,ef ,np,nf )\ne executed ep ef STarantula =\nef\nFef\nF + ep\nP\ne not executed np=P-ep nf =F-ef Sochai =\nef√\n(ef +ep)(ef +nf )\n2.2 Language Models\nAlthough real-world software programs are often large and com-\nplex, the program constructs (such as tokens) are repetitive, and\nthus provide useful predictable statistical property [26, 46, 51, 20].\nThese statistical properties of code resemble natural languages, and\nthus, natural language models can be leveraged for software engi-\nneering tasks.\nCache based N-gram Model ($gram): Hindle et al. introduced\nn-gram model for software code [27], which is essentially an exten-\nsion of n-gram language model used in natural language processing\ntasks based on the Markov independence assumption [9]. If a se-\nquence, s consists of m tokens (a1a2a3...am), according to the\nFull Markov Model, the probability, p(s), of that sequence is given\nin Equation 1\np(s) =p(a1)p(a2|a1)p(a3|a1a2)...p(am|a1a2...am−1) (1)\nN-gram model is a simpliﬁcation of full Markov model based on\nthe assumption that every token is dependent on previous n−1\ntoken, where nis a model parameter. Essentially with n= ∞, the\nmodel converges to the full Markov model. Since, actual probabili-\nties are very difﬁcult to ﬁnd, researchers often use empirical proba-\nbilities to represent actual probabilities, which is highly dependent\non the training data. Initially, the probability of a token or any n-\ngram which is not seen in the corpus will be zero resulting the total\nprobability to be zero. To overcome this problem Hindle et al. [27]\nalso adopted smoothing techniques from natural language process-\ning literature.\nTu et al. [52] further improved the above model based on the ob-\nservation that source code tends to be highly localized, i.e. particu-\nlar token sequences may occur often within a single ﬁle or within\nparticular classes or functions. They proposed a $gram model that\nintroduces an additional cache—list of n-grams curated from local\ncontext and used them in addition to a global n-gram model. They\nalso deﬁned the entropy of a code sequence S by language model\nM by Equation 2:\nHM (S) =−1\nN log2 pM (S) =−1\nN\nN∑\ni=1\nlog2 P(ti|h) (2)\nLanguage model to predict buggy code:Ray et al. [45] demon-\nstrated that there is a strong negative correlation between code be-\ning buggy and the naturalness of code. When a simple $gram model\nis trained on previous versions of project source code and applied\nto calculate naturalness of code snippet, buggy codes are shown to\nexhibit higher unnaturalness than the non-buggy codes. They intro-\nduced a syntax-sensitive entropy model to measure naturalness of\ncode. In their investigation, they found that some token types such\nas packages, methods, variable names are less frequent, and hence\nhigh entropic than others. They normalized the entropy score and\nderived a Z-score with line-type information from the program’s\nAbstract Syntax Tree (AST). The Z-score is deﬁned as:\nFigure 1: EnSpec Workﬂow\n$gram+ type= entropyline −µtype\nSDtype\n(3)\nIn Equation 3, µtype is the mean $gram model entropy of a\ngiven line type, and SDtype is the standard deviation of that line\ntype. This Z-score gives the syntax-sensitive entropy. They re-\nported that, buggy lines of codes are usually unnatural and highly\nentropic. With further investigation, they also found that, when de-\nvelopers ﬁxed those buggy lines of codes, the entropy of the code\ndecreased. In this work, we used state of the art $gram model along\nwith syntax-sensitive entropy model.\n3. MOTIV ATING EXAMPLE\nIn this section, we present a real-world example that motivated\nus to incorporate the naturalness property of code (i.e. entropy\nbased features) in SBBLto overcome a key limitation of SBBL.\nThe main limitation of testing based bug localization approaches,\nsuch as SBBL, is that the quality of their results highly depend on\nthe quality of test cases. If the passing test cases have low code\ncoverage, an SBBLtool may return a large number of program\nelements with high suspiciousness score, most of which are false\npositives. However, generating an adequate test suite is incredibly\ndifﬁcult. Therefore, in many cases SBBLperforms poorly.\nTable 2: Effectiveness of entropy based features to improveSBBL\n-- /Closure/89/buggy/src/com/google/javascript/jscomp/\nGlobalNamespace.java\n+++ /Closure/89/fix/src/com/google/javascript/jscomp/\nGlobalNamespace.java\n- if (type != Type.FUNCTION && aliasingGets > 0) {\n//Entropy before patch 7.36\n+ if (aliasingGets > 0) {\n//Entropy after patch 1.15\nreturn false;\n}\nTable 2 presents a patch that ﬁxed a bug in Closure compiler\n(Defects4J bug ID: 89). The buggy line, marked in red, was never\nused in the existing corpus before. Hence, the line was unnatural\nto a LMwith a high entropy score of 7.36. When developer ﬁxed\nthe bug (see green line), the code becomes more natural with a\nreduced entropy score of 1.15. A traditional state-of-the art SBBL\ntechnique placed the buggy line at 57 th position, while EnSpec\nusing both entropy and spectrum based features, placed the line\nat 12th position in the ranked list of suspicious lines. This shows\nthat entropy of code, as derived from LM, can play an important\nrole to improve the ranking of the actual buggy lines.\n4. PROPOSED APPROACH\nIn this section, we describe our tool, EnSpec. An overview of\nour approach is shown in Figure 1. The goal of EnSpec is to lo-\ncalize bugs using a hybrid bug localization technique: a combi-\nnation of dynamic spectrum based bug localization ( SBBL) and\nstatic natural language model based defect prediction ( LM). En-\nSpec takes two sets of code corpus as input—training and testing\nset. Next, EnSpec works in following four steps: Step-1. EnSpec\ncollects entropy score per code element based on a language model\nfor each input project. Step-2. For each project version in the train-\ning and test corpus, EnSpec records test coverage and collects var-\nious SBBLbased suspiciousness scores per code element. Step-3.\nIn this step, EnSpec learns from the training data, how the suspi-\nciousness scores and entropy collected in above two steps relate to\nbuggy/non-buggy classes and learns feature weight. In Section 5.2,\nwe describe the data collection phase in more detail: how we an-\nnotate each code element as buggy/non-buggy. Step-4. Based on\nthe learned feature-weight, EnSpec assigns a suspiciousness score\nof each code element in the test corpus. The suspiciousness score\ndepicts the probability of a code element to be buggy. Finally, the\noutput of EnSpec is a ranked list of code elements based on their\ndecreasing suspiciousness score.\nIn theory, EnSpec should work on code elements at any gran-\nularity—line, method, ﬁle, etc. In this paper, we use EnSpec to\nlocalize bugs at a line granularity. In the following section, we de-\nscribe these steps in details.\nStep-1: Generating entropy using LM\nFor generating entropy per program line, we adopted the $gram\nlanguage model proposed by Tuet al. [52]. For every line in source\ncode we calculated following three entropy values:\n1. Forward Entropy (Ef ): Entropy value of a token is calculated\nbased on the probability of seeing the token given its preﬁx token\nsequences. We calculate this entropy by parsing the ﬁle from be-\nginning to end, i.e. considering the token sequences as it is in the\nsource ﬁle.\n2. Backward Entropy (Eb): Entropy value of a token is calcu-\nlated based on the probability of seeing the token given its sufﬁx\ntoken sequences. We calculate this entropy by parsing the ﬁle in\nreverse order, i.e. from end to beginning.\n3. Average Entropy(Ea): This entropy value is calculated as the\naverage of Ef and Eb.\nWe use these three values as our LMbased features. We fur-\nther normalized these values based on their AST type, as shown\nin Equation 3. We refer these three normalized entropy values as\nentropy related features in the rest of the paper.\nTable 3: 25 suspiciousness scores from literature that are used as SBBLfeatures in EnSpec\nMetric Formula Metric Formula Metric Formula Metric Formula\nTarantula\nef\nef +nf\nef\nef +nf\n+ ep\nep+np\nOchiai\nef√\n(ef +ep)(ef +nf )\nJaccard\nef\nef +ep+nf\nSimpleMatching\nef +np\nef +ep+nf +np\nSφrcenDice\n2ef\n2ef +ep+nf\nKulczynskil\nef\nep+nf\nRusselRao\nef\nef +ep+nf +np RogersTanimoto\nef +np\nef +ep+2nf +eP\nM1\nef +np\nep+nf\nM2\nef\nef +np+2ep+2ef\nOverlap\nef\nmin(ef ,ep,nf ) Ochiai2\nef np√\n(ef +ep)(nf +np)(ef +np)(ep+nf )\nDice\n2ef\nef +ep+nf\nAmple\n⏐⏐⏐\nef\nef +nf\n−\nep\nep+np\n⏐⏐⏐ Hamann\nef +np−ep−nf\nef +ep+nf +np Zoltar\nef\nef +ep+nf +\n10000nf ep\nef\nGoodman\n2ef −nf −ep\n2ef +nf +ep Sokal\n2ef +2ep\n2ef +2ep+nf +np Hamming ef + np Kulczynski2 1\n2\n( ef\nef +nf\n+\nef\nef +np\n)\nEuclid √ef + np Anderberg\nef\nef +2ep+2nf\nWong1 ef Wong2 ef −ep\nWong3 ef −h, whereh=\n\n\n\nep if ep ≤2\n2 + 0.1(ep −2) if 2 ≤ep ≤10\n2.8 + 0.01(ep −10) if ep ≥10\nStep-2: Extracting suspiciousness score usingSBBL\ntechniques.\nFor all the input project versions, we ﬁrst instrument the source\ncode to record program execution traces, or coverage data. Both\nDefects4J and ManyBugs dataset provide APIs for collecting such\ncoverage data. Then, to collect the execution traces, we extract\nthe test classes and test methods from the project source code and\nrun the test cases. We record the execution traces for each test\ncase with its passing/failing status. These test spectra characterize\nthe program’s behavior across executions by summarizing how fre-\nquently each source code line was executed for passing and failing\ntests. Now for each line we calculate 4 values (ep,ef ,np,nf ), as\ndescribed in Section 2. Next, using these 4 values, we generate 25\nsuspiciousness scores, as described by Xuan et al. [57]. We use\nthese 25 scores(see Table 3) as our SBBLfeatures.\nThe next two steps implement the training and testing phase\nof a classiﬁer based on buggy and non-buggy program lines. We\nadapted Li et al.’s learning to rank algorithm for this purpose [36].\nStep-3: Training Phase\nGiven a set of buggy and non-buggy lines, EnSpec learns the rela-\ntion between SBBLand entropy related features on the bugginess\nof program lines.\nFirst, all lines in the training dataset were annotated with a rele-\nvance score of bugginess: Rb for each buggy line, and Rg for each\nnon-buggy line, where Rb > Rg. Thus, each line lin the training\ncode corpus is represented as a tuple, ⟨Spl,Enl,Rl⟩, where Spl\nis a set of SBBLfeatures, Enl is a set of entropy related features,\nand Rl ∈{Rb,Rg}is the bug-relevance score. Then, we pass the\nwhole corpus to a machine learner, which learns the probability dis-\ntribution P(Rl|Spl,Enl) of the relevance scores given the feature\nvalues.\nStep-4: Testing Phase\nIn testing phase, for each line l in the test corpus, we compute a\nsuspiciousness score (susp) based on equation 4.\nsuspl = f(Rb) ∗P(Rb|Spl,Enl)\n+ f(Rg) ∗P(Rb|Spl,Enl) (4)\nHere, f(Rb) and f(Rg) are monotonically increasing functions.\nTo keep things simple, we used identity function,i.e., f(Rb) =Rb\nand f(Rg) = Rg, which is monotonic as well. This transforms\nequation 4 into:\nsuspl =\n∑\nRl∈{Rb,Rg}\nRl ∗P(Rl|Spl,Enl)\n= E[Rl|Spl,Enl]\n(5)\nWe use ensemble [17] of M different models trained on ran-\ndomly sampled subset of original dataset. Each model Mk com-\nputes a suspiciousness score suspk\nl , based on the expected rele-\nvance score of equation 5. Our ﬁnal hybrid suspiciousness score is\ncalculated by equation 6:\nHySuspl = 1\nM\nM∑\nk=1\nsuspk\nl (6)\nEnSpec outputs a ranked list of source code lines based on the de-\ncreasing order of hybrid suspiciousness score (HySusp), line with\nhighest suspiciousness tops the list.\n5. EXPERIMENTAL SETUP\nIn this section, we describe how we setup our experiment to eval-\nuate EnSpec. In particular, we describe the subject systems, how\nwe collect data, evaluation metric, and research questions to evalu-\nate EnSpec.\n5.1 Study Subject\nWe used two publicly available bug dataset: Defects4J [32] and\nManyBugs [34] (see Table 4). Defects4J dataset contains 5 open\nsource projects with 321K lines of code, 357 reproducible bugs,\nand 20K total number of tests. All the Defects4J projects are writ-\nten in Java. We also studied5 projects from ManyBugs benchmark\ndataset [34]. These are medium to large open source C projects,\nwith total 4459K lines of code,160 reproducible bugs, and9262 to-\ntal test cases. In both datasets, each bug is associated with a buggy\nand its corresponding ﬁx program versions. There are some failing\nTable 4: Study Subjects\nDataset Project KLoc #Tests #Bugs #Buggy Lines #Buggy Lines\n(original dataset) (original + evolutionary)\nJFreechart 96 2,205 26 22 32\nClosure compiler 90 7,927 133 64 586\nDefects4J [32] Apache commons-math 85 3,602 106 38 816\n(Language: Java) Joda-Time 28 4,130 27 46 97\nApache commons-lang 22 2,245 65 50 230\nTotal 321 20,109 357 220 1761\nLibtiff 77 78 24 944 -\nLighttpd 62 295 9 26 -\nManyBugs [34] Php 1,099 8,471 104 503 -\n(Language: C) Python 407 355 15 93 -\nWireshark 2,814 63 8 388 -\nTotal 4,459 9,262 160 1954 -\ntest cases that reproduce the bugs in the buggy versions, while after\nthe ﬁxes all the test cases pass. The dataset also provides APIs for\ninstrumentation and recording program execution traces.\n5.2 Data Collection\nHere we describe how we identiﬁed the buggy program state-\nments. We followed two techniques as described below:\nI. Buggy lines retrieved from original dataset. We compared\neach buggy program version with its corresponding ﬁxed version;\nthe lines that are deleted or modiﬁed in the buggy version are an-\nnotated as buggy program statements. To get the differences be-\ntween two program versions, we used Defects4J APIs for Defects4J\ndataset. For every snapshot, ManyBugs dataset provides diff of\ndifferent ﬁles changed during the ﬁx revision. We directly used\nthose diff ﬁles.\nWe notice that some bugs are caused due to the error of omis-\nsion—tests fail due to missing features/functionalities in the buggy\nversion. Fixing these bugs do not require deleting or modifying\nprogram statements in the buggy version, but adding new lines in\nthe ﬁxed version. In such cases, we cannot trivially annotate any\nof the existing lines in the buggy version as ‘buggy.’ We ﬁlter out\nsuch bugs from further consideration, as our goal in this work to\nlocate existing buggy lines, as opposed to detecting error of omis-\nsion. Table 4 shows the total number of buggy lines per project in\nthe original dataset.\nFigure 2: Evolutionary bug data retrieval: vertical dashed lines corre-\nspond to buggy project versions under investigation, each triangle rep-\nresents project commit (c0... c3). For every bug-ﬁx commit (e.g., c2), as\nidentiﬁed by keyword search, we ﬁrst git-blame the buggy lines to iden-\ntify the original bug-introducing commit ( e.g., c0) and then map them\nto the corresponding project versions.(Adopted from Ray et al. [45])\nII. Buggy lines retrieved from project evolution.As shown in Ta-\nble 4, the percentage of buggy lines w.r.t.the total lines of code in\n0 20 40 60 80 100\nPercentage of Inspected Program Elements\n0\n20\n40\n60\n80\n100Percentage of Bugs Found\nReal\nBaseline\nOptimal\nFigure 3: Example Cost Effectiveness (CE) curve for bug localization.\nBaseline shows CE while inspecting random program elements. At op-\ntimal CE, 100% of bugs are found when all the buggy program elem-\nnets are inspected ﬁrst. A real CE falls somewhere in between baseline\nand optimal.\nDefects4J dataset is very small (0.07%). Such unbalanced data of\nbuggy vs. non-buggy lines poses a threat to the efﬁciency of any\nclassiﬁcation probelm [28, 12]. To reduce such imbalance and thus\nincrease the effectiveness of SBBL, previous work injects artiﬁcial\nbugs in the software system under tests [24, 49]. However, since\nthe motivation of this research comes from the ﬁndings that bugs\nare unnatural [45], artiﬁcially introducing bugs like our predeces-\nsors may question our conclusions. To overcome this problem, we\ninjected bugs that developers introduced in the source code in real-\nity—we collected such bugs from project evolutionary history.\nWe adopted the similar strategy described in Ray et al. [45].\nFirst, we identiﬁed bug-ﬁx commits by searching a project’s com-\nmit logs using bug-ﬁx related keywords: ‘error’, ‘bug’, ‘ﬁx’, ‘is-\nsue’, ‘mistake’, ‘ﬂaw’, and ‘defect’, following the methodology\ndescribed by Mockus et al. [41]. Lines modiﬁed or deleted on\nthose big-ﬁx commits are marked as buggy. Then we identiﬁed\nthe original commits that introduce these bugs using SZZ algo-\nrithm [50]. Next, we used git blame with -reverse option\nto locate those buggy lines in the buggy program version under in-\nvestigation. Figure 2 illustrates this process.\nUsing this method, we found 1541 additional buggy lines across\nall the versions of ﬁve Defects4J projects. Thus, in total, in this\ndataset, we studied 1761 buggy lines, as shown in 4.\n5.3 Evaluation Metric\nTo evaluate the bug localization capability of EnSpec, we adopted\ncommonly used non-parametric measure from literature: Cost Ef-\nfectiveness (CE) metric originally proposed by Arisholm et al. [4]\nto investigate defects in telecom softwares. The main assumption\nbehind this metric is, cost behind bug localization is the inspec-\ntion effort—the number of Program Element (PE) needs to be in-\nspected before locating the bug, and the payoff is the percentage of\nbugs found. A Cost-Effectiveness(CE) curve shows percentage of\ninspected PE in x-axis and percentage of bugs found in y-axis.\nIf bugs are uniformly distributed in the source code, by randomly\ninspecting n% of source PE, one might expect to ﬁnd n% bugs.\nThe corresponding CE curve will be a straight line with slope= 1\n(see Figure 3). This is our baseline. Any ranking metric assigns\nsuspiciousness score to each PE for bug localization. Then, PEs are\ninspected based on the decreasing order of suspiciousness score.\nAn optimal ranking metric would assign scores in a way that all\nbuggy PEs are ranked prior to the non-buggy PEs. So, inspecting\ntop PEs would cover 100% of the bugs. For any real bug local-\nization techniques, e.g., Tarantula [30], Multric [57] etc., CE curve\nfalls in between baseline and optimal.\nAUCEC, the area of under the CE curve is a quantitative mea-\nsurement describing how good a model is to ﬁnd the bugs. Base-\nline AUCEC (random AUCEC) is 0.5. Optimal AUCEC would be\nvery close to 1.00. This AUCEC metric is a non-parametric metric\nsimilar to the ROC curve and does not depend on bug distribu-\ntion [45], thus becomes standard in bug-localization literature [16].\nHigher AUCEC signiﬁes higher prioritization of buggy lines over\nnon-buggy lines and hence a better model. For example, for the\noptimal CE, 100% source program elements should not have to be\ninspected to ﬁnd all the bugs; thus, optimal exhibits higher AUCEC\nthan the baseline (see Figure 3). This intuition is the basis of our\nevaluation metric.\n5.4 Implementation of EnSpec\nWe implemented EnSpec’s learning to rank technique as described\nin Step 3 & 4 of Section 4 using two approaches. First, we used\nRankBoost [21] algorithm. RankBoost algorithm uses boosting en-\nsemble technique to learn model parameter for ranking training in-\nstances. At each iteration, it learns one weak ranker and then re-\nweights the training data. At the ﬁnal stage, it combines all the\nweak rankers to assign scores to the test data. This algorithm is\nused in the past by Xuan et al. [57] for implementing SBBLat\nmethod level bug localization. Though there are many competitive\napproaches to implement SBBL, Xuan et al. report the best results\ntill date. Thus, we adapted their approach in EnSpec to locate bugs\nat line granularity. We used RankBoost implementation of standard\nRankLib [1] library for this purpose. There are two conﬁgurable\nparameters: β(initial ranking metric) and γ(number of neighbor).\nFollowing Xuan et al., we set these two parameter values to Taran-\ntula and 10 respectively. Table 5 shows the result.\nIn the second approach, we used Random Forest Algorithm (RF)\nto implement the proposed learning to rank technique. Random\nForest is an ensemble learning technique developed by Breiman [8]\nbased on a combination of a large set of decision trees. Each tree\nis trained by selecting a random set of features from a random data\nsample selected from the training corpus. In our case, the algo-\nrithm, therefore, chooses some SBBLand/or entropy related fea-\ntures randomly in training phase (step 3). RF then learns condi-\ntional probability distribution of the chosen features w.r.t.the bug-\nginess of each line in the training dataset. In addition, RF learns the\nimportances for different features for discrimination. During train-\ning, the model learns M decision trees and corresponding proba-\nbility distribution. In the testing phase, suspiciousness scores from\neach of the learned model, and calculate ﬁnal suspiciousness score\nbased on equation 6. For implementation, we used standard python\nscikit-learn package [10].\nTable 5: Performance (AUCEC100) comparison of two learning to\nrank implementations that are used to realize EnSpec. Only SBBL\nrelated features are used to establish a baseline.\nProject RankBoost Random Forest\nJFreechart 0.847 0.908\nClosure compiler 0.797 0.894\nApache commons-lang 0.824 0.862\nApache commons-math 0.864 0.876\nJoda-Time 0.846 0.918\nLibtiff 0.847 0.887\nLighttpd 0.753 0.806\nPhp 0.835 0.899\nPython 0.788 0.807\nWireshark 0.864 0.829\nWe compare the performance of the above two approaches us-\ning AUCEC100 score. For the comparison purpose, we only used\nSBBLrelated features ( i.e. did not include entropy scores), since\nwe ﬁrst wanted to measure how the two approaches perform in tra-\nditional SBBLsetting. Table 5 reports the result. For all of the\nstudied projects, except Wireshark, Random Forest is performing\nbetter. Thus, we carried out rest of our experiments using Random\nForest based implementation, since this gives the best SBBLper-\nformance at line level, even when we compare against state of the\nart Xuan et al.’s technique.\n5.5 Research Questions\nTo evaluate EnSpec, we investigate whether a good language\nmodel (LM) that captures naturalness (hence unnaturalness) of a\ncode element can improve spectrum based testing. Previously, Ray\net al. [45] and Wang et al. [54] demonstrated that unnaturalness of\ncode elements (measured in terms of entropy) correlate with bug-\nginess. Thus, LMcan help in bug localization in a static setting.\nIn contrast, SBBLis a dynamic approach that relies on the fact\nthat code elements that are covered by more negative test cases are\nmore bug-prone. Therefore, to understand the effectiveness of En-\nSpec, we will investigate whether the combination of the two can\nimprove bug-localization as a whole.\nSince LMbased bug-localization approach says that more en-\ntropic code is more bug prone, and SBBLsays that code element\ncovered by more negative test cases are more prone to bugs, to\nmake the combined approach work, the difference between en-\ntropies of the buggy and non-buggy lines should be signiﬁcant for\nthe negative test spectra. Thus, to understand the potential of en-\ntropy, we start our investigation with the following research ques-\ntion:\nRQ1. How is entropy associated with bugginess for different\ntypes of test spectra?\nIf the answer to the above question is afﬁrmative for failing test\nspectra, entropy can be used along with SBBLfor bug-prediction.\nFor every code element,SBBLprovides suspiciousness scores, and\nLMpredicts its uncertainty in terms of entropy. Thus, one may\nexpect that among the lines with higher suspiciousness score, more\nentropic lines are even more likely to be buggy. We, therefore,\ninvestigate whether entropy can help improving the bug prediction\ncapability of EnSpec over SBBL.\nRQ2. Can entropy improve SBBL’s bug-localization capabil-\nity?\nTo build a good LMbased bug localization technique, we need\na large code corpus with adequate bug history; this is often chal-\nlenging for smaller projects. A similar problem arises for history\nbased defect prediction models—for newer projects enough his-\ntory is usually not available to build a good model. In such case,\nresearchers, in general, rely on the evolutionary history of other\nprojects [62]. To mitigate the threat of using our proposed approach\nfor smaller code base, we leverage such cross-project defect predic-\ntion strategy. We investigate, whether a language model trained on\ndifferent projects can still improve SBBL’s performance.\nRQ3. What is the effect of entropy on SBBL’s bug localization\ncapability in a cross-project setting?\n6. RESULT\nIn this section, we answer the research questions introduced in 5.5.\nOur investigation starts with whether the buggy lines in failing test\nspectrum are more entropic than non-buggy lines. Note that, all\nthe buggy and non-buggy lines are annotated using the strategy de-\nscribed in 5.2.\nRQ1. How is entropy associated with bugginess for different\ntypes of test spectra?\npass only fail only both\n0.0\n2.5\n5.0\n7.5\nbug non−bug bug non−bug bug non−bug\nLine Entropy\nType of Percentage Buggy Entropy > Non-buggy Entropy\ntest buggy Difference Effect Size\nspectra lines (95% conf interval) p-value (Cohen’s D)\nFail only 4.22 0.06 to 0.99 0.027 0.20 ( small )\nPass only 0.85 0.00 to 0.28 0.051 0.06 ( negligible )\nBoth 1.60 0.09 to 0.38 0.002 0.09 ( negligible )\nFigure 4: For failing test spectra, buggy lines are more entropic than\nnon-buggy lines; t-test conﬁrms that the difference is statistically sig-\nniﬁcant (p-value <0.05) with small Cohen’s D effect size. However, for\nother two spectra, the difference is negligible.\nTo answer this question, ﬁrst, for each buggy version of De-\nfects4J and ManyBugs dataset, we calculated the $gram entropy\nvalue for every line in the project’s source code. We also instru-\nmented the source code to record the execution trace information at\nline level for passing and failing test cases. For Defects4J projects,\nwe instrumented each ﬁle. Therefore, we recorded the execution\ntraces for all ﬁles. However, ManyBugs projects were large in\nsize (see 4); hence due to technical limitations, we only instru-\nmented the buggy ﬁles for this data set. Note that, this is not an\nunreasonable assumption; one can use other human-based or au-\ntomated techniques, e.g., statistical or information retrieval based\ndefect prediction, to locate potential buggy ﬁles [43, 53, 48]. Ad-\nditionally, many of our buggy ﬁles have hundreds of lines of code,\nwhich is similar to the size of the code tested in previous SBBL\nanalyses [31].\nNext, we group the program lines based on the test execution\ntraces: i) Fail Only: lines covered by only failing test cases (no\npassing test case exercise these lines), ii) Pass Only: lines cov-\nered by only passing test cases (no failing test cases exercised these\nlines), and iii) Both: lines covered by both failing and passing test\ncases. Note that, there is still a considerable number of program\nlines that are not covered by any test cases because the code cover-\nage is not 100% for any of the studied test suite. Such non-executed\nprogram lines are out of scope for this research question. For each\ngroup, we studied the $gram entropy differences between buggy\nand non-buggy program lines. Figure 4 shows the result.\nBuggy lines, in general, have higher entropy than non-buggy\nlines in all three groups, as shown in the boxplot. However, a t-\ntest [61] (see the Table below of 4) conﬁrms that for Fail Only\nspectra, the difference is statistically signiﬁcant (p-value < 0.05),\nwith a small Cohen’s D effect size. Note that, a similar effect size\nfor overall buggy vs. non-buggy entropy was also reported by pre-\nvious studies [45]. For Pass Only spectra, the difference is not sta-\ntistically signiﬁcant between buggy and non-buggy lines. Further-\nmore, although a small difference exists for Both spectra, the Co-\nhen’s D effect size is negligible. These results are also conﬁrmed\nby Wilcoxon non-parametric test [40, 61].\nIn summary, we conclude that entropy is associated with buggi-\nness for lines covered by failing test cases, while for the lines not\ncovered by the failing test cases, entropy does not show signiﬁcant\nassociation. This could be due to many factors such as the complex-\nity or nuance of bugs that are not captured adequately by the test\ncases, or simply because of the quality of the test suite if it does not\nexercise many non-buggy lines which is low entropic. Certainly,\nmore research is needed. Nevertheless, this result is signiﬁcant for\nour study. Since, in failing spectra, entropy can further discrimi-\nnate buggy and non-buggy lines, entropy score on top of theSBBL\nbased suspiciousness score would enhance the bug localization ca-\npability. Additionally, since entropy does not differentiate between\nbuggy and non-buggy lines in the passing spectra, it will not boost\nSBBLsuspiciousness score for them. Thus, we believe overall en-\ntropy would play an key role to improveSBBLbased bug localiza-\ntion by increasing the suspiciousness scores of the buggy lines that\ncause tests to fail.\nResult 1: For failing test spectra, buggy lines, on average,\nare high entropic, i.e. less probable, than non-buggy lines.\nSo far, we have seen that entropy is associated with the bugginess\nof the code elements covered by failing test cases. Now, let us\ninvestigate whether EnSpec can leverage this property to localize\nbugs more effectively.\nRQ2. Can entropy improve SBBL’s bug-localization capabil-\nity?\nTo investigate this research question, we trained EnSpec’s model\nin two settings: i. SBBLonly: 25 different SBBLbased suspicious-\nness scores, as shown in Table 3 as features, and ii. SBBL+ En-\ntropy: 25 different SBBLbased suspiciousness scores along with\nentropy related scores (see Section 4) as features, where entropies\nare generated by $gram+type language model (see equation 3). We\ncomputed AUCEC at 100% inspection budget for both the settings,\nAUCECsp and AUCECen respectively. The experiment is run\n30 times per project and the ﬁnal result is the average of those 30\nruns. To understand relative improvements, we calculated relative\npercentage gain using equation 7.\ngain= AUCECen −AUCECsp\nAUCECsp\n(7)\n0\n25\n50\n75\n100\n0 5 10 15 20\n% Inspected Lines\n% Bugs Found\nMethod\nSBBL\nSBBL+Entropy\nRandom\n(a) Bug localization performance with an in-\nspection budget of 20% of total lines\nClosure Commons\nlang\nCommons\nmath JFreechart Joda\nTime\n0\n25\n50\n75\n100\n0 1 2 3 4 5 0 1 2 3 4 5 0 1 2 3 4 5 0 1 2 3 4 5 0 1 2 3 4 5\n% Inspected Lines\n% Bugs Found\n(b) Bug localization performance with an in-\nspection budget of 5% of total lines for De-\nfects4J dataset\nLibtiff Lighttpd Php Python Wireshark\n0\n25\n50\n75\n100\n0 1 2 3 4 5 0 1 2 3 4 5 0 1 2 3 4 5 0 1 2 3 4 5 0 1 2 3 4 5\n% Inspected Lines\n% Bugs Found\n(c) Bug localization performance with an in-\nspection budget of 5% of total lines for Many-\nBugs dataset\nProject AUCEC100 AUCEC100 Gain Top Three Features\n(SBBL) (SBBL+Entropy) (%) Feature 1 Feature 2 Feature 3\nJFreechart 0.908 0.956 5.305 Average Entropy Ochiai RogersTanimoto\nClosure compiler 0.894 0.916 2.451 Average Entropy Euclid SimpleMatching\nApache commons-lang 0.862 0.943 9.368 Average Entropy SimpleMatching RogersTanimoto\nApache commons-math 0.876 0.883 0.708 Average Entropy Jaccard RogersTanimoto\nJoda-Time 0.918 0.930 1.318 Average Entropy Euclid Hamming\nLibtiff 0.887 0.904 2.004 Average Entropy SimpleMatching Hamming\nLighttpd 0.806 0.943 17.016 Average Entropy Jaccard Ochiai\nPhp 0.899 0.941 4.675 Average Entropy Hamming Euclid\nPython 0.807 0.961 19.133 Average Entropy Sokal SimpleMatching\nWireshark 0.829 0.864 4.193 Average Entropy Wong3 Ample\nAverage 0.869 0.924 6.617\nFigure 5: Overall performance of EnSpec (SBBL+ Entropy) vs. SBBLfor localizing buggy lines.\noverall_gain=\n∑\nproject AUCECen −AUCECsp\n∑\nproject AUCECsp\n(8)\nThe results are shown in the bottom table of Figure 5. For all the\nprojects we studied, AUCEC100 increases when we incorporate en-\ntropy related features, with an overall gain of 6.617% (3.81% and\n9.11% for Defects4J and ManyBugs dataset respectively). The pos-\nitive gain indicates that LMcan indeed improve the performance\nof SBBLbug localization. Top three features (sorted based on their\nimportance learned by Random Forest algorithm in the training\nphase) are also reported as Feature1, Feature2 and Feature3. For\nall the cases, Average Entropy is the most important feature. Thus,\nwe conclude that, entropy plays the most important role in localiz-\ning bugs for EnSpec.\nNext, we check how EnSpec performs with SBBL+ entropy\nrelated features over SBBLonly features while inspecting small\nportion of source code lines (SLOC), as that is more realistic sce-\nnario. Figure 5(a) shows the overall effect of entropy across all\nthe projects at an inspection budget of 20%. With SBBLonly fea-\ntures, we can detect only 80.08% of total buggy lines (AUCEC 20\nis 0.10475). However, with SBBL+ entropy features, we can de-\ntect 98.63% buggy lines (AUCEC 20 is 0.13966). Thus we see an\noverall gain of 33.33% in AUCEC20 with entropy.\nNext, we check, at a even lower inception budget, how EnSpec\nperforms. Figure 5(b) and Figure 5(c) shows the cost-effectiveness\ncurve for individual projects in Defects4J and ManyBugs dataset\nrespectively. Here, SBBL+ entropy yields better performance over\nSBBLonly setting for all the projects except two: for Apache\ncommons-math both performs almost equal and for Libtiff, EnSpec\nperforms worse.\nWe further investigate, why LMis helping SBBLto improve\nbug localization. Figure 6(a) and 6(b) show the histogram ofsuspi-\nciousness scores per line from Defects4J and ManyBugs dataset in\nboth the settings. For SBBLonly setting, a large proportion of ac-\ntual non-buggy lines (marked in blue) lie in the higher suspicious-\nness score range. So, those non-buggy lines are ranked in higher\nposition than the buggy lines (marked in red) having lower scores,\nlowering down the overall bug-localization performance. But, in\nSBBL+ Entropy setting, the proportion of non-buggy lines hav-\ning higher hybrid suspiciousness scores decreases, improving the\noverall bug localization performance.\nResult 2: Entropy, as derived by statistical language model,\nimproved bug localization capabilities of SBBL.\nFor projects with smaller size and less number of test cases,\nEnSpec might not work well, since smaller projects do not have\nenough bug data to train EnSpec. To mitigate this threat, we now\nevaluate EnSpec’s performance in cross-project settings.\nRQ3. What is the effect of entropy on SBBL’s bug localization\ncapability in a cross-project setting?\nTo answer this research question, we ﬁrst build a cross-project\nLM. To train the LMfor a given buggy project version, v target,\nwe select all the other buggy versions (vtrain) from different projects\nwith the following two constraints: (1) vtrain and vtarget are writ-\nten in the same language, and (2) vtrain are created before vtarget.\nThe ﬁrst constraint ensures that the LMis trained on the same\nprogramming language speciﬁc features and usages. Thus, for a\ngiven Java project, LMonly learns from other Java projects from\nDefects4J dataset. Similarly, for a C project, we choose other C\nprojects from ManyBugs dataset for training.\nThe second constraint emulates the real world scenario: to train\na LMfor vtarget, the training set (v train) has to be available in\nthe ﬁrst place. Hence, we organize different project versions of\na dataset chronologically, based on their last commit date, as re-\ntrieved from version repository. Then, for vtarget, we choose all the\nversions from different projects that are chronologically appeared\nbefore vtarget. Thus, for both the dataset, we choose a buggy ver-\nsion as the target if there exists at least one previous buggy version\nfrom another project.\nNext, for each v target, we train EnSpec based on the features\nof vtrain. Similar to our previous experiments, the features include\nentropy scores generated by $gram+type language model (see equa-\ntion 3), and 25 SBBLspeciﬁc suspiciousness scores, for each pro-\n0.0 0.2 0.4 0.6 0.8 1.0\nSuspiciousness score\n0\n1\n2\n3\n4\n5\n6\n7 SBBL\nBuggy Lines\nNon-Buggy Lines\n0.0 0.2 0.4 0.6 0.8 1.0\nSuspiciousness score\n0\n1\n2\n3\n4\n5\n6\n7 SBBL + Entropy\nBuggy Lines\nNon-Buggy Lines\n(a) Defects4J dataset\n0.0 0.2 0.4 0.6 0.8 1.0\nSuspiciousness score\n0\n1\n2\n3\n4\n5\n6\n7 SBBL\nBuggy Lines\nNon-Buggy Lines\n0.0 0.2 0.4 0.6 0.8 1.0\nSuspiciousness score\n0\n1\n2\n3\n4\n5\n6\n7 SBBL + Entropy\nBuggy Lines\nNon-Buggy Lines (b) ManyBugs dataset\nFigure 6: Histogram of suspiciousness score per source code lines. x-axis shows suspiciousness score given by equation 6 and they-axis is the relative\nfrequency.\nDefects4j Manybugs\n0\n20\n40\n60\n0 5 10 15 20 0 5 10 15 20\n% Inspected Lines\n% Bugs found\nMethod SBBL SBBL+Entropy Random\n(a) Bug localization performance with an inspection budget of 20%\nof total lines. 75.34% and 54.5% gain in the corresponding AUCEC\nvalues for Defects4J and ManyBugs projects respectively, when we\nuse entropy.\nDefects4j Manybugs\n0\n5\n10\n15\n20\n0 1 2 3 4 5 0 1 2 3 4 5\n% Inspected Lines\n% Bugs found\nMethod SBBL SBBL+Entropy Random\n(b) Bug localization performance with an inspection budget of 5%\nof total lines. 94.16% and 54.18% gain in the corresponding AUCEC\nvalues for Defects4J and ManyBugs projects respectively, when we\nuse entropy.\nDataset Project AUCEC100 AUCEC100 Overall Gain Top Three Features\n(SBBLonly) (SBBL+ entropy) (%) Feature1 Feature2 Feature3\nJFreechart 0.878 0.955 8.882 Average entropy Hamming Euclid\nClosure compiler 0.681 0.816 37.796 Average entropy Ample RusselRao\nDefects4J Apache commons-lang 0.619 0.840 36.655 Average entropy Euclid RogersTanimoto\nApache commons-math 0.743 0.774 4.172 Mean entropy Euclid Hamming\nJoda-Time 0.662 0.794 19.939 Average entropy Hamming Sokal\nAverage 0.717 0.836 16.59\nLibtiff 0.565 0.729 45.026 Average entropy Wong3 RogersTanimoto\nLighttpd 0.616 0.848 37.623 Average entropy RogersTanimoto SimpleMatching\nManyBugs Php 0.508 0.541 11.299 Mean entropy Wong3 Hamming\nPython 0.577 0.792 37.422 Average entropy Ample Euclid\nWireshark 0.757 0.898 20.489 Average entropy RogersTanimoto Sokal\nAverage 0.605 0.762 25.95\nFigure 7: Overall performance of EnSpec (SBBL+ Entropy) vs. SBBLin cross project setting.\ngram line. Based on the training, EnSpec then assigns a suspi-\nciousness score per line in the test data, v target, indicating the\nline’s likelihood of being buggy. Finally, we rank the lines with\ndecreasing order of their suspiciousness score—line with the high-\nest value tops the list. To evaluate the performance of EnSpec at\ncross-project setting, we calculate the AUCECn score, which basi-\ncally tells us the rate and the percentages of bugs that can be found\nif a developer inspects n% lines in the ranked-list returned by the\ntool. We also repeat the experiment without entropy as a feature,\ni.e. with only SBBLsuspiciousness score. As a baseline, we report\nthe percentage of buggy lines found at an inspection budget of n,\nwhile we randomly choose n% lines from source code. Figure 7\nshows the results.\nFigure 7(a) shows that if developers inspect top 20% of source\ncode lines, followingSBBL+Entropy ranking scheme, 33.54% and\n59.65% of the buggy lines are detected for Defects4J and Many-\nBugs respectively. In contrast, when we use only SBBLbased\nranking scheme, 19.24% and 35.72% buggy lines can be detected\nfor Defects4J and ManyBugs respectively. Thus, we see an over-\nall gain of 75.34% and 54.5% of AUCEC 20, when we include en-\ntropy in the feature set. Note that, this gain even increases at a\nstricter inspection budget for Defects4J projects—Figure 7(b) re-\nports an overall gain of 94.16% in AUCEC 5. For ManyBugs, we\nsee a gain of 54.18%, which is similar to AUCEC 20 gain. Both\nranking schemes perform better than random.\nThe table at bottom in Figure 7 shows AUCEC100 values achieved\nby two strategies for each project at an inspection budget of 100. In\nboth cases, developers would ﬁnd 100% of buggy lines since they\ninspect all the lines in the ranked list. However, the rate of iden-\ntifying 100% of buggy lines is higher using EnSpec than SBBL\nonly. Result shows that EnSpec has 16.59% performance gain for\nDefects4J data set, and 25.95% gain for ManyBugs, on average.\nAs discussed earlier, including entropy as a feature beneﬁts bug lo-\ncalization even more at lower inspection budget, which is a more\nrealistic scenario.\nWe further check the relative importance of the features learned\nby EnSpec (last three columns of the Table in Figure 7 lists the\nthree most important features). Interestingly, in all the cases, en-\ntropy exhibits highest importance than any SBBLbased features.\nThe above results show that a good LMcan signiﬁcantly improve\nSBBLin a cross-project bug localization setting.\nResult 3: Entropies derived from LMsigniﬁcantly im-\nproves bug localization capabilities of SBBLin a cross-\nproject setting.\n7. RELATED WORK\nAutomatic bug localization has been an active research area over\ntwo decades. Existing techniques can be broadly classiﬁed into two\nbroad categories: i) static and ii) dynamic approaches.\nStatic approachesprimarily rely on program source code. There\nare mainly two kinds of static approaches: a) program analysis\nbased approaches, and b) information retrieval (IR) based approaches.\nProgram analysis based approaches detect bugs by identifying well-\nknown buggy patterns that frequently happened in practice. There-\nfore, although these approaches are effective in preventing bugs by\nenforcing good programming practices, they generally cannot de-\ntect functional bugs. FindBugs [5] is a popular example in this\ncategory. On the other hand, IR-based approaches, given a bug re-\nport, generally rank source code ﬁles based on the textual similar-\nity between source code and the bug report so that potential buggy\nﬁles ranked high in the ranked-list. These approaches are gener-\nally fast but identify bugs at coarse grained level. BugLocator [60],\nBLUiR [48] are some of the examples in this category.\nThere is a new line of work that recently started based on statis-\ntical modeling and machine learning. Wang et al.[55] proposed a\nDeep Belief Network based approached to detect ﬁle level defects.\nWang et al.[54] used n-gram language model to generate a list of\nprobable bugs.\nDynamic approaches generally rely on the execution traces of\ntest cases. SBBLis a dynamic fault localization technique that\nleverages program spectra—program paths executed by passed and\nfailed test cases [47] to compute a suspiciousness score of each pro-\ngram element. We have described SBBLin detail in Section 2.1.\nSeveral metrics have been proposed in the literature to calculate the\nsuspiciousness score. For example, Jones et al. presented Taran-\ntula [30] based on the fact that program elements executed by failed\ntest cases are more likely to bug than the elements not executed by\nthem (see Table 1). Jaccard and Ochiai are some of the well known\nvariants of this approach proposed by Abreu et al. [3]. Xie et al.\nproposed ﬁve ranking metrics by theoretical analysis and four other\nmetrics based on genetic algorithms [56]. Later, Lucia et al. did a\ncomprehensive study of the different ranking metrics and showed\nthat no ranking metric is unanimously best [39]. SBBLapproaches\ncan identify bugs at ﬁne-grained level.\nXuan et al. [57] proposed an approach to combine multiple rank-\ning metrics. They adopted neighborhood based strategy to reduce\nthe imbalance ratio of buggy and non-buggy program entities. For\ntheir algorithm, they need an initial metric to deﬁne the neighbor-\nhood. They sort the data based on an initial ranking metric. The\nﬁltered β non-faulty entities before and after a faulty entity. After\nthat, they applied state of the art \"Learning to Rank\" algorithms to\ncombine all 25 suspiciousness scores. Their dependence on an ini-\ntial ranking metric might cause a bias towards that metric. In con-\ntrast, to be unbiased to any of the metric, we considered all the data\nand applied state of the art random under-sampling[7] technique.\nGong et al. [23] proposed a feedback based fault localization\nsystem, which uses user feedback to improve performance. Pytlik\net al. [42] proposed the fault localization system using the likely\ninvariant. Le et al.[6] also proposed an approach similar to Pytlic et\nal. with a larger invariant set. Unlike this work, they experimented\non method level fault localization system. Sahoo et al. extended\nPytlik et al’s work. Their work is on test case generation and also\nthey adopted backward slicing to reduce the number of program\nelement to be considered.\nMulti-modal techniques generally combine two or more model\nof bug localization to improve the accuracy further. Le et al. [33]\nproposed a multi-modal technique for bug localization that basi-\ncally combines the IR and spectrum based bug localization together.\nTheir technique needs three artifacts: i) a bug report, ii) program\nsource code, and iii) a set of test cases having at least a fault repro-\nducing test case. Their technique ﬁrst rank the source code meth-\nods based on the textual similarity between bug report and source\ncode methods. Then using program spectra, they rank the source\ncode lines and also identiﬁes a list of suspicious words associated\nwith the bug. Finally they combine these scores using a proba-\nbilistic model which is trained on a set of previously ﬁxed bugs.\nBased on an empirical evaluation on 157 real bugs from four soft-\nware systems, their model outperforms a state-of-the-art IR-based\nbug localization technique, a state-of-the-art spectrum-based bug\nlocalization technique, and three state-of-the-art multi-modal fea-\nture location methods that are adapted for bug localization.\nThe proposed approach in this paper is also multi-modal in na-\nture. However, instead of combining IR-based textual similarity\nscore,inspired by Ray et al.’s[45] ﬁnding that the buggy codes are\nunnatural, and thus entropy of a buggy source code is naturally\nhigh, we combine source code entropy with program spectra to im-\nprove bug localization. To our knowledge, no one leveraged the\nlocalness of code and test spectrum together to locate faults. The\nadvantage of our approach is that we do not need any bug report\nwhich may not be available for development bugs. Therefore, our\napproach is complementary to Le et al’s approach.\n8. THREATS TO V ALIDITY\nEfﬁciency of EnSpec depends on the availabity of previous bugs\non which the model will be trained. To minimize this threat, we\ndemonstrated that EnSpec works well in cross-project setting.\nEnSpec is also dependent on the adequecy of the test suite. If\nthere are not enough failing test cases, performance of SBBLmay\nget hurt, and hence EnSpec’s performance will also be worse. How-\never, since EnSpec is a hybrid approach, it does not solely depend\non test suites. The LMbased part will still be able to locate bugs\nsince the latter does not require anything but source code.\nFurther, to annotate buggy lines, we rely on the publicly available\nbug-dataset and some evolutionary bugs. It can be possible there\nare other bugs lying in the code corpus that are polluting our results.\nHowever, at any given point of time it is impossible to know the\npresence of all the bugs in a software.\nFinally, to minimize threats due to external validity, we evaluated\nEnSpec on 10 projects for 2 languages: C and Java. This proves,\nEnSpec is not restricted to any particular programming language.\n9. CONCLUSION\nWhile spectrum based bug localization is an extensively studied\nresearch area, studying buggy code in association with code nat-\nuralness (thus unnaturalness) is relatively new. In this work, we\nintroduced the notion of code entropy as captured by statistical lan-\nguage model in SBBLto make the overall bug localization more\nrobust, and proposed an effective way of integrating entropy with\nSBBLsuspicious scores. We implemented our concept in a proto-\ntype called EnSpec. Our experimental results with EnSpec show\nthat code entropy is positively correlated with the buggy lines ex-\necuted by the failing test cases. Our results also demonstrate that\nEnSpec, when conﬁgured to use both entropy and SBBL, outper-\nforms the conﬁguration that uses only various SBBLas features.\nEnSpec can also be leveraged for detecting bugs in cross-project\nsetting for relatively new projects, where project bug database and\nevolutionary history is not strong enough.\nOur future direction includes leveraging EnSpec to repair buggy\nprogram line more effectively, and improving EnSpec further by\nincorporating language model which captures not only the syntactic\nstructure but also code semantic structure.\n10. REFERENCES\n[1] Ranklib (https://sourceforge.net/p/lemur/wiki/RankLib/).\nhttps://sourceforge.net/p/lemur/wiki/RankLib/.\n[2] R. Abreu, P. Zoeteweij, R. Golsteijn, and A. J. Van Gemund.\nA practical evaluation of spectrum-based fault localization.\nJournal of Systems and Software, 82(11):1780–1792, 2009.\n[3] R. Abreu, P. Zoeteweij, and A. J. Van Gemund. On the\naccuracy of spectrum-based fault localization. In Testing:\nAcademic and Industrial Conference Practice and Research\nTechniques-MUTATION, 2007. TAICPART-MUTATION\n2007, pages 89–98. IEEE, 2007.\n[4] E. Arisholm, L. C. Briand, and E. B. Johannessen. A\nsystematic and comprehensive investigation of methods to\nbuild and evaluate fault prediction models. Journal of\nSystems and Software, 83(1):2–17, 2010.\n[5] N. Ayewah, D. Hovemeyer, J. D. Morgenthaler, J. Penix, and\nW. Pugh. Using static analysis to ﬁnd bugs. IEEE software,\n25(5), 2008.\n[6] T.-D. B Le, D. Lo, C. Le Goues, and L. Grunske. A\nlearning-to-rank based fault localization approach using\nlikely invariants. In Proceedings of the 25th International\nSymposium on Software Testing and Analysis, pages\n177–188. ACM, 2016.\n[7] G. E. Batista, R. C. Prati, and M. C. Monard. A study of the\nbehavior of several methods for balancing machine learning\ntraining data. ACM Sigkdd Explorations Newsletter,\n6(1):20–29, 2004.\n[8] L. Breiman. Random forests. Machine learning, 45(1):5–32,\n2001.\n[9] P. F. Brown, P. V . Desouza, R. L. Mercer, V . J. D. Pietra, and\nJ. C. Lai. Class-based n-gram models of natural language.\nComputational linguistics, 18(4):467–479, 1992.\n[10] L. Buitinck, G. Louppe, M. Blondel, F. Pedregosa,\nA. Mueller, O. Grisel, V . Niculae, P. Prettenhofer,\nA. Gramfort, J. Grobler, R. Layton, J. VanderPlas, A. Joly,\nB. Holt, and G. Varoquaux. API design for machine learning\nsoftware: experiences from the scikit-learn project. In ECML\nPKDD Workshop: Languages for Data Mining and Machine\nLearning, pages 108–122, 2013.\n[11] J. C. Campbell, A. Hindle, and J. N. Amaral. Syntax errors\njust aren’t natural: improving error reporting with language\nmodels. In Proceedings of the 11th Working Conference on\nMining Software Repositories, pages 252–261. ACM, 2014.\n[12] N. V . Chawla, N. Japkowicz, and A. Kotcz. Editorial: special\nissue on learning from imbalanced data sets. ACM Sigkdd\nExplorations Newsletter, 6(1):1–6, 2004.\n[13] B. Chelf, D. Engler, and S. Hallem. How to write\nsystem-speciﬁc, static checkers in metal. In Proceedings of\nthe 2002 ACM SIGPLAN-SIGSOFT Workshop on Program\nAnalysis for Software Tools and Engineering, PASTE ’02,\npages 51–60, New York, NY , USA, 2002. ACM.\n[14] H. Cleve and A. Zeller. Locating causes of program failures.\nIn Software Engineering, 2005. ICSE 2005. Proceedings.\n27th International Conference on, pages 342–351. IEEE,\n2005.\n[15] T. Copeland. PMD applied. Centennial Books San Francisco,\n2005.\n[16] M. D’Ambros, M. Lanza, and R. Robbes. An extensive\ncomparison of bug prediction approaches. In Mining\nSoftware Repositories (MSR), 2010 7th IEEE Working\nConference on, pages 31–41. IEEE, 2010.\n[17] T. G. Dietterich. Ensemble learning. The handbook of brain\ntheory and neural networks, 2:110–125, 2002.\n[18] D. Engler, D. Y . Chen, S. Hallem, A. Chou, and B. Chelf.\nBugs as deviant behavior: A general approach to inferring\nerrors in systems code. In Proceedings of the Eighteenth\nACM Symposium on Operating Systems Principles, SOSP\n’01, pages 57–72, New York, NY , USA, 2001. ACM.\n[19] FindBugs. http://ﬁndbugs.sourceforge.net/. Accessed\n2015/03/10.\n[20] C. Franks, Z. Tu, P. Devanbu, and V . Hellendoorn. Cacheca:\nA cache language model based code suggestion tool. In ICSE\nDemonstration Track, 2015.\n[21] Y . Freund, R. Iyer, R. E. Schapire, and Y . Singer. An efﬁcient\nboosting algorithm for combining preferences. Journal of\nmachine learning research, 4(Nov):933–969, 2003.\n[22] M. Gabel and Z. Su. A study of the uniqueness of source\ncode. In Proceedings of the eighteenth ACM SIGSOFT\ninternational symposium on Foundations of software\nengineering, pages 147–156. ACM, 2010.\n[23] L. Gong, D. Lo, L. Jiang, and H. Zhang. Interactive fault\nlocalization leveraging simple user feedback. In Software\nMaintenance (ICSM), 2012 28th IEEE International\nConference on, pages 67–76. IEEE, 2012.\n[24] U. Gunneﬂo, J. Karlsson, and J. Torin. Evaluation of error\ndetection schemes using fault injection by heavy-ion\nradiation. In Fault-Tolerant Computing, 1989. FTCS-19.\nDigest of Papers., Nineteenth International Symposium on,\npages 340–347. IEEE, 1989.\n[25] V . J. Hellendoorn, P. T. Devanbu, and A. Bacchelli. Will they\nlike this?: evaluating code contributions with language\nmodels. In Proceedings of the 12th Working Conference on\nMining Software Repositories, pages 157–167. IEEE Press,\n2015.\n[26] A. Hindle, E. Barr, M. Gabel, Z. Su, and P. Devanbu. On the\nnaturalness of software. In ICSE, pages 837–847, 2012.\n[27] A. Hindle, E. T. Barr, Z. Su, M. Gabel, and P. Devanbu. On\nthe naturalness of software. In 2012 34th International\nConference on Software Engineering (ICSE), pages\n837–847. IEEE, 2012.\n[28] N. Japkowicz and S. Stephen. The class imbalance problem:\nA systematic study. Intelligent data analysis, 6(5):429–449,\n2002.\n[29] B. Johnson, Y . Song, E. Murphy-Hill, and R. Bowdidge.\nWhy don’t software developers use static analysis tools to\nﬁnd bugs? In Software Engineering (ICSE), 2013 35th\nInternational Conference on, pages 672–681. IEEE, 2013.\n[30] J. A. Jones and M. J. Harrold. Empirical evaluation of the\ntarantula automatic fault-localization technique. In\nProceedings of the 20th IEEE/ACM international Conference\non Automated software engineering, pages 273–282. ACM,\n2005.\n[31] J. A. Jones, M. J. Harrold, and J. Stasko. Visualization of test\ninformation to assist fault localization. In Proceedings of the\n24th international conference on Software engineering,\npages 467–477. ACM, 2002.\n[32] R. Just, D. Jalali, and M. D. Ernst. Defects4j: A database of\nexisting faults to enable controlled testing studies for java\nprograms. In Proceedings of the 2014 International\nSymposium on Software Testing and Analysis, pages\n437–440. ACM, 2014.\n[33] T.-D. B. Le, R. J. Oentaryo, and D. Lo. Information retrieval\nand spectrum based bug localization: better together. In\nProceedings of the 2015 10th Joint Meeting on Foundations\nof Software Engineering, pages 579–590. ACM, 2015.\n[34] C. Le Goues, N. Holtschulte, E. K. Smith, Y . Brun,\nP. Devanbu, S. Forrest, and W. Weimer. The manybugs and\nintroclass benchmarks for automated repair of c programs.\nIEEE Transactions on Software Engineering,\n41(12):1236–1256, 2015.\n[35] C. Le Goues, W. Weimer, and S. Forrest. Representations\nand operators for improving evolutionary software repair. In\nProceedings of the 14th annual conference on Genetic and\nevolutionary computation, pages 959–966. ACM, 2012.\n[36] P. Li, C. J. Burges, Q. Wu, J. Platt, D. Koller, Y . Singer, and\nS. Roweis. Mcrank: Learning to rank using multiple\nclassiﬁcation and gradient boosting. In NIPS, volume 7,\npages 845–852, 2007.\n[37] B. Liblit, M. Naik, A. X. Zheng, A. Aiken, and M. I. Jordan.\nScalable statistical bug isolation. ACM SIGPLAN Notices,\n40(6):15–26, 2005.\n[38] C. Liu, X. Yan, L. Fei, J. Han, and S. P. Midkiff. Sober:\nstatistical model-based bug localization. In ACM SIGSOFT\nSoftware Engineering Notes, volume 30, pages 286–295.\nACM, 2005.\n[39] L. Lucia, D. Lo, L. Jiang, F. Thung, and A. Budi. Extended\ncomprehensive study of association measures for fault\nlocalization. Journal of Software: Evolution and Process,\n26(2):172–219, 2014.\n[40] H. B. Mann and D. R. Whitney. On a test of whether one of\ntwo random variables is stochastically larger than the other.\nThe annals of mathematical statistics, pages 50–60, 1947.\n[41] A. Mockus and L. G. V otta. Identifying reasons for software\nchanges using historic databases. In icsm, pages 120–130,\n2000.\n[42] B. Pytlik, M. Renieris, S. Krishnamurthi, and S. P. Reiss.\nAutomated fault localization using potential invariants. arXiv\npreprint cs/0310040, 2003.\n[43] F. Rahman, S. Khatri, E. T. Barr, and P. Devanbu. Comparing\nstatic bug ﬁnders and statistical prediction. In Proceedings of\nthe 36th International Conference on Software Engineering,\npages 424–434. ACM, 2014.\n[44] S. Rao and A. Kak. Retrieval from software libraries for bug\nlocalization: a comparative study of generic and composite\ntext models. In Proceedings of the 8th Working Conference\non Mining Software Repositories, pages 43–52. ACM, 2011.\n[45] B. Ray, V . Hellendoorn, S. Godhane, Z. Tu, A. Bacchelli, and\nP. Devanbu. On the naturalness of buggy code. In\nProceedings of the 38th International Conference on\nSoftware Engineering, pages 428–439. ACM, 2016.\n[46] V . Raychev, M. Vechev, and E. Yahav. Code completion with\nstatistical language models. In PLDI, pages 419–428, 2014.\n[47] T. Reps, T. Ball, M. Das, and J. Larus. The use of program\nproﬁling for software maintenance with applications to the\nyear 2000 problem. In Software\nEngineeringâ ˘A ˇTESEC/FSE’97, pages 432–449. Springer,\n1997.\n[48] R. K. Saha, M. Lease, S. Khurshid, and D. E. Perry.\nImproving bug localization using structured information\nretrieval. In Automated Software Engineering (ASE), 2013\nIEEE/ACM 28th International Conference on, pages\n345–355. IEEE, 2013.\n[49] Z. Segall, D. Vrsalovic, D. Siewiorek, D. Ysskin,\nJ. Kownacki, J. Barton, R. Dancey, A. Robinson, and T. Lin.\nFiat-fault injection based automated testing environment. In\nFault-Tolerant Computing, 1995, Highlights from\nTwenty-Five Years., Twenty-Fifth International Symposium\non, page 394. IEEE, 1995.\n[50] J. ´Sliwerski, T. Zimmermann, and A. Zeller. When do\nchanges induce ﬁxes? In ACM sigsoft software engineering\nnotes, volume 30, pages 1–5. ACM, 2005.\n[51] Z. Tu, Z. Su, and P. Devanbu. On the localness of software.\nIn SIGSOFT FSE, pages 269–280, 2014.\n[52] Z. Tu, Z. Su, and P. Devanbu. On the localness of software.\nIn Proceedings of the 22nd ACM SIGSOFT International\nSymposium on Foundations of Software Engineering, pages\n269–280. ACM, 2014.\n[53] J. Walden, J. Stuckman, and R. Scandariato. Predicting\nvulnerable components: Software metrics vs text mining. In\nSoftware Reliability Engineering (ISSRE), 2014 IEEE 25th\nInternational Symposium on, pages 23–33. IEEE, 2014.\n[54] S. Wang, D. Chollak, D. Movshovitz-Attias, and L. Tan.\nBugram: bug detection with n-gram language models. In\nProceedings of the 31st IEEE/ACM International Conference\non Automated Software Engineering, pages 708–719. ACM,\n2016.\n[55] S. Wang, T. Liu, and L. Tan. Automatically learning\nsemantic features for defect prediction. In Proceedings of the\n38th International Conference on Software Engineering,\npages 297–308. ACM, 2016.\n[56] X. Xie, T. Y . Chen, F.-C. Kuo, and B. Xu. A theoretical\nanalysis of the risk evaluation formulas for spectrum-based\nfault localization. ACM Transactions on Software\nEngineering and Methodology (TOSEM), 22(4):31, 2013.\n[57] J. Xuan and M. Monperrus. Learning to combine multiple\nranking metrics for fault localization. In ICSME-30th\nInternational Conference on Software Maintenance and\nEvolution, 2014.\n[58] X. Ye, R. Bunescu, and C. Liu. Learning to rank relevant\nﬁles for bug reports using domain knowledge. In\nProceedings of the 22nd ACM SIGSOFT International\nSymposium on Foundations of Software Engineering, pages\n689–699. ACM, 2014.\n[59] A. Zeller and R. Hildebrandt. Simplifying and isolating\nfailure-inducing input. IEEE Transactions on Software\nEngineering, 28(2):183–200, 2002.\n[60] J. Zhou, H. Zhang, and D. Lo. Where should the bugs be\nﬁxed?-more accurate information retrieval-based bug\nlocalization based on bug reports. In Proceedings of the 34th\nInternational Conference on Software Engineering, pages\n14–24. IEEE Press, 2012.\n[61] D. W. Zimmerman. Comparative power of student t test and\nmann-whitney u test for unequal sample sizes and variances.\nThe Journal of Experimental Education, 55(3):171–174,\n1987.\n[62] T. Zimmermann, N. Nagappan, H. Gall, E. Giger, and\nB. Murphy. Cross-project defect prediction: a large scale\nexperiment on data vs. domain vs. process. In Proceedings of\nthe the 7th joint meeting of the European software\nengineering conference and the ACM SIGSOFT symposium\non The foundations of software engineering, pages 91–100.\nACM, 2009."
}