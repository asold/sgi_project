{
    "title": "A Swin Transformer-based model for mosquito species identification",
    "url": "https://openalex.org/W4308356480",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4308356615",
            "name": "De-zhong Zhao",
            "affiliations": [
                "Institute of Microbiology",
                "Beijing University of Chemical Technology"
            ]
        },
        {
            "id": "https://openalex.org/A4308356616",
            "name": "Xin-kai Wang",
            "affiliations": [
                "China Agricultural University",
                "Institute of Microbiology"
            ]
        },
        {
            "id": "https://openalex.org/A2127716428",
            "name": "Teng Zhao",
            "affiliations": [
                "Institute of Microbiology"
            ]
        },
        {
            "id": "https://openalex.org/A2116097374",
            "name": "Hu Li",
            "affiliations": [
                "China Agricultural University"
            ]
        },
        {
            "id": "https://openalex.org/A2131711916",
            "name": "Dan Xing",
            "affiliations": [
                "Institute of Microbiology"
            ]
        },
        {
            "id": "https://openalex.org/A4212001345",
            "name": "He-ting Gao",
            "affiliations": [
                "Institute of Microbiology"
            ]
        },
        {
            "id": "https://openalex.org/A2098483037",
            "name": "Fan Song",
            "affiliations": [
                "China Agricultural University"
            ]
        },
        {
            "id": "https://openalex.org/A2275404516",
            "name": "Guo‐Hua Chen",
            "affiliations": [
                "Beijing University of Chemical Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2160450535",
            "name": "Chun Xiao Li",
            "affiliations": [
                "Institute of Microbiology"
            ]
        },
        {
            "id": "https://openalex.org/A4308356615",
            "name": "De-zhong Zhao",
            "affiliations": [
                "Beijing University of Chemical Technology",
                "Institute of Microbiology"
            ]
        },
        {
            "id": "https://openalex.org/A4308356616",
            "name": "Xin-kai Wang",
            "affiliations": [
                "China Agricultural University",
                "Institute of Microbiology"
            ]
        },
        {
            "id": "https://openalex.org/A2127716428",
            "name": "Teng Zhao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2116097374",
            "name": "Hu Li",
            "affiliations": [
                "China Agricultural University",
                "Institute of Microbiology"
            ]
        },
        {
            "id": "https://openalex.org/A2131711916",
            "name": "Dan Xing",
            "affiliations": [
                "Institute of Microbiology"
            ]
        },
        {
            "id": "https://openalex.org/A4212001345",
            "name": "He-ting Gao",
            "affiliations": [
                "Institute of Microbiology"
            ]
        },
        {
            "id": "https://openalex.org/A2098483037",
            "name": "Fan Song",
            "affiliations": [
                "China Agricultural University"
            ]
        },
        {
            "id": "https://openalex.org/A2275404516",
            "name": "Guo‐Hua Chen",
            "affiliations": [
                "Beijing University of Chemical Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2160450535",
            "name": "Chun Xiao Li",
            "affiliations": [
                "Institute of Microbiology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4211090760",
        "https://openalex.org/W2132316389",
        "https://openalex.org/W2810360330",
        "https://openalex.org/W2594220414",
        "https://openalex.org/W2113507178",
        "https://openalex.org/W3182823827",
        "https://openalex.org/W2016278466",
        "https://openalex.org/W2162714623",
        "https://openalex.org/W3118206376",
        "https://openalex.org/W2883128121",
        "https://openalex.org/W2099901798",
        "https://openalex.org/W2549696576",
        "https://openalex.org/W2069782035",
        "https://openalex.org/W2112796928",
        "https://openalex.org/W146900863",
        "https://openalex.org/W2097117768",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W2104657103",
        "https://openalex.org/W1948751323",
        "https://openalex.org/W1954489142",
        "https://openalex.org/W3046862411",
        "https://openalex.org/W3112911519",
        "https://openalex.org/W3002266516",
        "https://openalex.org/W2910634864",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W2767106145",
        "https://openalex.org/W2919681993",
        "https://openalex.org/W2117936709",
        "https://openalex.org/W1968266351",
        "https://openalex.org/W3118571930",
        "https://openalex.org/W3134561445",
        "https://openalex.org/W2963150697",
        "https://openalex.org/W2963446712",
        "https://openalex.org/W2962858109",
        "https://openalex.org/W2418346537",
        "https://openalex.org/W4210993961"
    ],
    "abstract": "Abstract Mosquito transmit numbers of parasites and pathogens resulting in fatal diseases. Species identification is a prerequisite for effective mosquito control. Existing morphological and molecular classification methods have evitable disadvantages. Here we introduced Deep learning techniques for mosquito species identification. A balanced, high-definition mosquito dataset with 9900 original images covering 17 species was constructed. After three rounds of screening and adjustment-testing (first round among 3 convolutional neural networks and 3 Transformer models, second round among 3 Swin Transformer variants, and third round between 2 images sizes), we proposed the first Swin Transformer-based mosquito species identification model (Swin MSI) with 99.04% accuracy and 99.16% F1-score. By visualizing the identification process, the morphological keys used in Swin MSI were similar but not the same as those used by humans. Swin MSI realized 100% subspecies-level identification in Culex pipiens Complex and 96.26% accuracy for novel species categorization. It presents a promising approach for mosquito identification and mosquito borne diseases control.",
    "full_text": "1\nVol.:(0123456789)Scientific Reports |        (2022) 12:18664  | https://doi.org/10.1038/s41598-022-21017-6\nwww.nature.com/scientificreports\nA Swin Transformer‑based model \nfor mosquito species identification\nDe‑zhong Zhao1,2,4, Xin‑kai Wang2,3,4, Teng Zhao2,4, Hu Li3, Dan Xing2, He‑ting Gao2, \nFan Song3*, Guo‑hua Chen1* & Chun‑xiao Li2*\nMosquito transmit numbers of parasites and pathogens resulting in fatal diseases. Species \nidentification is a prerequisite for effective mosquito control. Existing morphological and molecular \nclassification methods have evitable disadvantages. Here we introduced Deep learning techniques \nfor mosquito species identification. A balanced, high‑definition mosquito dataset with 9900 original \nimages covering 17 species was constructed. After three rounds of screening and adjustment ‑testing \n(first round among 3 convolutional neural networks and 3 Transformer models, second round among \n3 Swin Transformer variants, and third round between 2 images sizes), we proposed the first Swin \nTransformer‑based mosquito species identification model (Swin MSI) with 99.04% accuracy and \n99.16% F1‑score. By visualizing the identification process, the morphological keys used in Swin MSI \nwere similar but not the same as those used by humans. Swin MSI realized 100% subspecies‑level \nidentification in Culex pipiens Complex and 96.26% accuracy for novel species categorization. It \npresents a promising approach for mosquito identification and mosquito borne diseases control.\nMosquitoes belong to Diptera, and transmit a number of parasites and pathogens resulting in hundreds of mil-\nlions of infections and approximately 750,000 deaths worldwide each  year1. Therefore, mosquitoes are considered \nthe number one \"animal killer\" and are among the most medically important insect taxa. Different species of \nmosquitoes have different habitats, biological habits and pathogen loads. For example, Anopheles  mosquitos \nmainly transmit malaria, which caused an estimated 219 million cases globally, and resulted in more than 400,000 \ndeaths every  year2, Aedes mosquitos transmit dengue, which threaten more than 3.9 billion people in over 129 \ncountries with an estimated 96 million symptomatic cases every  year3,  Zika4 and  chikungunya5. Culex mosquitos \ntransmit West Nile  virus6, Japanese encephalitis  virus7 and lymphatic  filariasis8. Because specific vaccines or drugs \nare not available for the majority of mosquito borne diseases, mosquito control is still the main measure used to \nprevent and control these diseases. Identifying the present mosquito species is a prerequisite and the basis for \neffectively preventing and controlling mosquito borne diseases. Only by accurately identifying mosquito species \ncan we understand their breeding characteristics and behavioral habits to develop correct prevention strategies \nand take targeted measures to ensure rapid  control9–11.\nCurrently, mosquito species identification methods rely on the morphological characteristics of mosquitos; \nthese methods are time-consuming, laborious and vulnerable to uncertainties associated with genetic variabilities \nand phenotypic  plasticity12,13. Even for experienced taxonomists, it is difficult to identify mosquito Complexes \nwith subtle external morphological differences by their external morphological characteristics. For example, \nsubspecies in the Culex pipiens Complex can be classified only by the dissected male  genitalia14,15. In addition, \nmolecular identification methods are effective but expensive and have high technical requirements. In addition, \nit is difficult to fully meet the rapidly growing demand for rapid and intelligent mosquito species identification \nusing these traditional classification methods.\nWith the improvements in computing power, the explosive growth of big data and the advancement of \nmachine learning algorithms, deep learning techniques have been rapidly developed and have begun to be \napplied in image classification tasks. Convolutional neural network (CNN) models with different structures \n (LeNet16,  AlexNet17,  GoogLeNet18,  VGGNet19,  ResNet20,  SqueezeNet21, etc.) have been successively applied to \nperform automatic mosquito recognition using images. However, due to defects associated with the layer struc-\ntures of these networks, features are easily lost in the computation of each layer, causing gradient disappear -\nances or explosions; thus, these methods cannot effectively capture the relationships between pixel points. The \nOPEN\n1College of Mechanical and Electrical Engineering, Beijing University of Chemical Technology, Beijing 100029, \nChina. 2State Key Laboratory of Pathogen and Biosecurity, Beijing Institute of Microbiology and Epidemiology, \nBeijing 100071, China. 3Department of Entomology and MOA Key Lab of Pest Monitoring and Green Management, \nCollege of Plant Protection, China Agricultural University, Beijing 100193, China.  4 These authors contributed \nequally: De-zhong Zhao, Xin-kai Wang and Teng Zhao. *email: fansong@cau.edu.cn; chengh@mail.buct.edu.cn ; \nlichunxiao@bmi.ac.cn\n2\nVol:.(1234567890)Scientific Reports |        (2022) 12:18664  | https://doi.org/10.1038/s41598-022-21017-6\nwww.nature.com/scientificreports/\ntransformer model, which was originally applied in natural language processing research, can solve the above \nproblems through its single-layer structure and multi-head self-attention mechanism. Therefore, attempts have \nbeen made to apply transformer models to in computer vision research, and the effects of these models can \nmatch or even outperform  CNNs22–24.\nThe main contributions of this study are summarized as follows. (1) We aimed to establish the highest-\ndefinition and most-balanced mosquito image dataset to date, including 17 species and 3 subspecies, with a total \nof 9,900 images at an image resolution of 4464 × 2976 pixels. All the classification and identification features in \nthis dataset achieved the discrimination ability of human eyes. (2) The first Swin Transformer-based mosquito \nspecies identification (Swin MSI) model was proposed herein, and the species and sex identification accuracies \nwere 99.04% (F1-score 99.16%). (3) In the test set performed in this study, the subspecies and sex identification \naccuracies of mosquitos in the Cx. pipiens Complex, which are morphologically indistinguishable, were both \n100%. (4) The Swin MSI model could identify novel mosquitoes that were beyond our dataset with a 96.26% \naccuracy (F1-score 98.09%) correct genus attribution. (5) As determined by visualizing the identification process, \nthe morphological keys used by the Swin MSI model were similar but not the same as those used by humans.\nThe Swin MSI model proposed in this study can perform mosquito species identification more accurately \nthan previously established models and could assist taxonomists in identifying mosquito and achieving effective \nmonitoring and prevention of mosquitoes and the associated transmitted diseases.\nResults\nThe framework of Swin MSI. We established the first Swin Transformer-based mosquito species identifi-\ncation (Swin MSI) model, with the help of self-constructed image dataset and multi-adjustment-test. Gradient-\nweighted class activation mapping was used to visualize the identification process (Fig. 1a). The key Swin Trans-\nformer block was described on Fig. 1b. Based on practical needs, Swin MSI was additional designed to identify \nCulex pipiens Complex on the subspecies level (Fig. 1c) and novel mosquito (which was defined as ones beyond \n17 species in our dataset) classification attribution (Fig. 1d). Detailed results are shown in the following sections.\nMosquito datasets. We established the highest-definition and most-balanced mosquito image dataset to \ndate. The mosquito image dataset covers 7 genera and 17 species (including 3 morphologically similar subspe-\ncies in the Cx. pipiens Complex), which covers the most common and important disease-transmitting mosqui-\ntoes at the global scale, with a total of 9,900 mosquito images. The image resolution was 4464 × 2976 pixels. The \nspecific taxonomic status and corresponding images are shown in Fig.  2. Due to the limitation of field collec-\ntion, Ae. vexans, Coquillettidia ochracea, Mansonia uniformis, An. vagus and Toxorhynchites splendens only have \nFigure 1.  The Framework of Swin MSI. (a)The basic architecture for mosquito features extraction and \nidentification. Attention visualization generated by filters at each layer are shown. (b) Details for Swin \nTransformer block. (c) For mosquito within our dataset 17 species, output is the top 5 confidence species. (d) \nFor mosquito beyond 17 species (defined as novel species), whether the output is a species or a genus is decided \nafter comparing with confidence threshold.\n3\nVol.:(0123456789)Scientific Reports |        (2022) 12:18664  | https://doi.org/10.1038/s41598-022-21017-6\nwww.nature.com/scientificreports/\nfemales or only have males. In addition, each mosquito species included 300 images of both sexes, which was \nlarge enough and same number for each species, in order to balance the capacity and variety of training sets.\nWorkflow for mosquito species identification. A three-stage flowchart of building best deep learning \nmodel for identification of mosquito species model was adopted (Fig. 3). The first learning stage was conducted \nby three CNNs (the Mask R-CNN, DenseNet, and YOLOv5) and three transformer models (the Detection \nTransformer, Vision Transformer, and Swin Transformer). Based on the performance of the first-stage model \nand the real mosquito labels, the second learning stage involved adjusting the model parameters of the three \nSwin Transformer variants (T, B, and L) to compare their performances. The third learning stage involved test-\ning the effects of inputting differently sized images (384 × 384 and 224 × 224) to the Swin Transformer-L model; \nfinally, we proposed a deep learning model for mosquito species identification (Swin MSI) to test the recognition \neffects of different mosquito species. The model was validated on different mosquito species, with a focus on \nthe identification accuracy of three subspecies within the Cx. pipiens Complex and the detection effect of novel \nmosquito species.\nComparison between the CNN model and Transformer model results (1st round of learn ‑\ning). Figure 4a shows the accuracies obtained for the six different computer vision network models tested on \nthe mosquito picture test set. The test results show that the transformer network model had a higher mosquito \nspecies discrimination ability than the CNN.\nIn the CNN training process (applied to YOLOv5), the validation accuracy requires more than 110 epochs to \ngrow to 0.9, and the validation loss requires 110 epochs to drop to a flat interval; in contrast, during the training \nFigure 2.  Taxonomic status and index of mosquito species included in this study Both male and female \nmosquitoes were photographed from different angles such as dorsal, left side, right side, ventral side, etc. Except \nfor 5 species, each mosquito includes 300 images of both sexes, and the resolution of mosquito photos were \n4464 × 2976. Cx. pipiens quinquefasciatus, Cx. pipiens pallens, and Cx. pipiens molestus (subspecies level, in dark \ngray background) were 3 subspecies in Cx. pipiens Complex (species level).\n4\nVol:.(1234567890)Scientific Reports |        (2022) 12:18664  | https://doi.org/10.1038/s41598-022-21017-6\nwww.nature.com/scientificreports/\nFigure 3.  Flowchart of testing deep learning model for intelligent identification of mosquito species.\nFigure 4.  Comparison of mosquito recognition effects of computer vision network models and variants. \n(a) Comparison of mosquito identification accuracy between 3 CNNs and 3 Transformer; (b) The best effect \nCNN (YOLOv5) training set loss curve(blue), validation set loss curve(green) and validation set accuracy \ncurve(orange); (c) The best effect Transformer (Swin Transformer) training set loss curve, validation set loss \ncurve and validation set accuracy curve. (d) Swin-MSI-T test result confusion matrix; (e) Swin-MSI -B test \nresult confusion matrix; (f) Swin-MSI -L test result confusion matrix. Confusion matrix of mosquito labels in \nwhich odd numbers represent females and even numbers represent males. The small squares in the confusion \nmatrix represent the recognition readiness rate, from red to green, the recognition readiness rate is getting \nhigher and higher An. sinensis: 1, 2; Cx. pipiens quinquefasciatus: 3, 4; Cx. pipiens pallens: 5, 6; Cx. pipiens \nmolestus: 7,8 Cx. modestus: 9,10; Ae. albopictus: 11, 12 Ae. aegypti: 13, 14; Cx. pallidothorax: 15, 16 Ae. galloisi: \n17,18 Ae. vexans: 19, 20; Ae. koreicus: 21, 22 Armigeres subalbatus: 23, 24; Coquillettidia ochracea: 25, 26 Cx. \ngelidus: 27, 28 Cx. triraeniorhynchus: 29, 30 Mansonia uniformis: 31, 32 An. vagus: 33, 34 Ae. elsaie: 35,36 \nToxorhynchites splendens: 37, 38.\n5\nVol.:(0123456789)Scientific Reports |        (2022) 12:18664  | https://doi.org/10.1038/s41598-022-21017-6\nwww.nature.com/scientificreports/\nstep, these losses represent a continuously decreasing process. These results indicate that the deep learning model \nderived based on the Swin Transformer algorithm was able to achieve a higher recognition accuracy in less time \nthan the rapid convergence ability of the CNN during the iterative process (Fig. 4b).\nThe Swin Transformer model exhibited the highest test accuracy of 96.3%. During the training process, the \nloss of this model could stabilize after 30 epochs, and its validation accuracy could grow to 0.9 after 20 epochs; \nduring the validation step, the loss can drop to 0.36 after 20 epochs, after which the loss curve fluctuated but did \nnot produce adverse effects (Fig.  4c). Based on the excellent performance of the Swin Transformer model, this \nmodel was used as the baseline to carry out the subsequent analyses.\nSwin Transformer model variant adjustment (2nd round of learning). Following testing per -\nformed to clarify the superior performance of the Swin Transformer algorithm, we chose different Drop_path_\nrate, Embed_dim and Depths parameter settings and labeled the parameter sets as the Swin Transformer-T, \nSwin Transformer-B, and Swin Transformer-L variants. Drop_path is an efficient regularization method, and an \nasymmetric Drop_path_rate is beneficial for supervised representation learning when using image classification \ntasks and Transformer architectures. The Embed_dim parameter represents the image dimensions obtained \nafter the input red–green–blue (RGB) image is calculated by the Swin Transformer block in stage 1. The Depths \nparameter is the number of Swin Transformer blocks used in the four stages. The parameter information and test \nresults are shown in Table 1. Due to the increase in the Swin Transformer block and Embed_dim parameters in \nstage 3, the recognition accuracies of the three variants were found to be 95.8%, 96.3%, and 98.2%, Correspond-\ningly, the f1 score were 96.2%, 96.7% and 98.3%; thus, these variants could effectively improve the mosquito spe-\ncies identification ability in a manner similar to the CNN by increasing the number of convolutional channels to \nextract more features and improve the overall classification ability. In this study, the Swin Transformer-L variant, \nwhich exhibited the highest accuracy, was selected as the baseline for the next work.\nBy plotting a confusion matrix of the test set results derived using the three Swin Transformer variants, we \nclearly obtained the proportion of correct and incorrect identifications in each category to visually reflect the \nmosquito species discrimination ability (Fig.  4d–f). In the matrix, the darker diagonal colors indicate higher \nidentification rate accuracies of the corresponding mosquito categories. Among them, five mosquito species \nwere missing because the Ae. vexans, Coquillettidia ochracea, Mansonia uniformis, An. vagus and Toxorhynchites \nsplendens species were represented in the dataset by only females or only males. The confusion matrix shown \nin Panel C lists the lowest number of mosquito species identification error points and the lowest accuracy level \nobtained in each category, suggesting that the Swin Transformer-L model has a better classification performance \nthan the Swin Transformer-T and Swin Transformer-B models.\nEffect of the input image size on the discrimination ability (3rd round of learning). To inves-\ntigate the relationship between the input image size and mosquito species identification performance, in this \nstudy, we conducted a comparison test between input images with sizes of 224 × 224 and 384 × 384, based on the \nSwin Transformer-L model, and identified 8 categories of mosquito identification accuracy differences. These \ntest results are shown in Table  2. When using an image size of 224 × 224 pixels, the batch_size parameter was \nset to 16, and when using an image size of 384 × 384 pixels, the batch_size parameter was set to 4; under these \nTable 1.  Parameters and test accuracy of three variants of Swin Transformer.\nModel Drop_path_rate Embed_dim Depths Batch_size Accuracy (%) F1 score (%)\nSwin-Transformer-T 0.2 96 [2, 6] 128 95.8 96.2\nSwin-Transformer-B 0.5 128 [2, 18] 32 96.3 96.7\nSwin-Transformer-L 0.2 192 [2, 18] 8 98.2 98.3\nTable 2.  Comparison of recognition accuracy for different input image sizes. Mosquito species with same \nrecognition accuracy between 2 input image sizes were not listed in the table. Acc was short for accuracy and \nF1 was shoret for f1-score.\nMosquito label\n224 × 224 384 × 384\nAcc (%) F1 (%) Acc (%) F1 (%)\nAn. sinenis female 91.11 95.35 86.67 92.86\nCx. pipiens molestus female 91.11 95.35 100 100\nCx. pipiens molestus male 91.11 95.35 100 100\nAe. aegypti male 95.56 97.73 97.78 98.88\nAe. koreicus male 86.67 92.86 91.11 95.35\nArmigeres subalbatus female 91.11 95.35 95.56 97.73\nCx. triraeniorhynchus female 80.00 88.89 95.56 97.73\nAe. elsaie male 95.56 97.73 100 100\n6\nVol:.(1234567890)Scientific Reports |        (2022) 12:18664  | https://doi.org/10.1038/s41598-022-21017-6\nwww.nature.com/scientificreports/\nconditions, the proportion of utilized video memory accounted for 67%, as shown in Eq. 4, and this was consist-\nent with the description of the relationship between the size of self-attentive operations during the operation of \nthe Swin Transformer model when 384 × 384 pixels images were used. The time required for the Transformer-L \nmodel to complete all the training sessions was excessive, reaching 126 h and even exceeding the 124 h required \nby the YOLOv5 model, which was found to require the highest computation time during the training process \nin this work. Long-term training process could more fully reflect the performance differences between models. \nFortunately and actually, the response speed of the model will not be affected by the training time. Compared to \nthe accuracy of 98.2% obtained for 224 × 224 inputs, the 384 × 384 input image size derived based on the Swin \nTransformer-L model provided a higher mosquito species identification accuracy of 99.04%, representing an \nimprovement of 0.84%.\nVisualizing and understanding the Swin MSI models. To investigate the differences in the atten-\ntional features utilized by the Swin MSI and taxonomists for mosquito species identification, we applied the \nGrad-CAM method to visualize the Swin MSI attentional areas on mosquitoes at different stages. Because the \nSwin Transformer has different attentional ranges among its multi-head self-attention steps in different stages, \ndifferent attentional weights can be found on different mosquito positions. In stage 1, the feature dimension \nof each patch was 4 × 4 × C, thus enabling the Swin Transformer’s multi-head self-attention mechanism to give \nmore attention to the detailed parts of the mosquitoes, such as their legs, wings, antennae, and pronota. In stage \n2, the feature dimension of each patch was 8 × 8 × 2C, enabling the Swin Transformer’s multi-head self-attention \nmechanism to focus on the bodies of the mosquitoes, such as their heads, thoraces, and abdomens. In stage 3, \nwhen the feature dimension of each patch was 16 × 16 × 4C, the Swin Transformer’s multi-head self-attention \nmechanism could focus on most regions of the mosquito, thus forming a global overall attention mechanism for \neach mosquito (Fig. 5). This attentional focus process is essentially the same as the process used by taxonomists \nwhen classifying mosquito morphology, changing from details to localities to the whole mosquito.\nAe. aegypti is widely distributed in tropical and subtropical regions around the world and transmits Zika, den-\ngue and yellow fever. A pair of long-stalked sickle-shaped white spots on both shoulder sides of the mesoscutum, \nwith a pair of longitudinal stripes running through the whole mesotergum, is the most important morphological \nidentification feature of this species. This feature was the deepest section in the attention visualization, indicating \nthat the Swin MSI model also recognized it as the principal distinguishing feature. In addition, the abdominal \ntergum of A. aegypti  is black and segments II-VII have lateral silvery white spots and basal white bands; the \nmodel also focused on these areas.\nCx. triraeniorhynchus is the main vector of Japanese encephalitis; this mosquito has a small body size, a dis-\ntinctive white ring on the proboscis (its most distinctive morphological feature), and a peppery color on its whole \nbody. Similarly, the model constructed herein focused on both the head and abdominal regions of this species.\nAn. sinensis is the top vector of malaria in China and has no more than three white spots on its anterior wing \nmargin and a distinct white spot on its marginal V5.2 fringe; this feature was observed in Stage 2, at which time \nthe modelstrongly focused on the corresponding area.\nThe most obvious feature of Armigeres subalbatus is the lateral flattening and slightly downward curving \nof its proboscis; the observation of the attention visualization revealed that the constructed model focused on \nthese regions from Stage 1 to Stage 3. The mesoscutum and abdominal tergum were not critical and were less \nimportant for identification than the proboscis, and the attention visualization results correspondingly show \nthat the neural network focused less on these features.\nCoquillettidia ochracea belongs to the Coquillettidia genus and is golden yellow all over its body, with the most \npronounced abdomen among the analyzed species. The model showed a consistent morphological taxonomic \nfocus on the abdomen of this species.\nMansonia uniformis is a vector of Malayan filariasis. The abdominal tergum of this species is dark brown, \nand its abdominal segments II-VII have yellow terminal bands and lateral white spots, which are more obvious \nthan the dark brown feature on proboscis. Through the attention visualization, we determined that the Swin MSI \nmodel was more concerned with the abdominal region features than with the proboscis features.\nSubspecies‑level identification tests of mosquitos in the Culex pipiens Complex. Fine-grained \nimage classification has been the focus of extensive research in the field of computer  vision25,26. Based on the test \nset (containing 270 images) constructed herein for three subspecies of the Cx. pipiens Complex, the subspecies \nand sex identification accuracies were 100% when the Swin MSI model was used.\nThe morphological characteristics of Cx. pipiens quinquefasciatus, Cx. pipiens pallens, and Cx. pipiens molestus \nwithin the Cx. pipiens Complex are almost indistinguishable, but their host preferences, self-fertility properties, \nbreeding environments, and stagnation overwintering strategies are very  different27. Among the existing features \navailable for morphological classification, the stripes on the abdominal tergum of Cx. pipiens quinquefasciatus  \nare usually inverted triangles and are not connected with the pleurosternums, while those of Cx. pipiens pallens \nare rectangular and are connected with the pleurosternums. Cx. pipiens molestus is morphologically more simi-\nlar to Cx. pipiens pallens as an ecological subspecies of the Cx. pipiens Complex. However, taxonomists do not \nrecommend using the unstable feature mentioned above as the main taxonomic feature for differentiation. By \nanalyzing the attention visualization results of these three subspecies (last three rows on Fig.  5), we found that \nthe neural networks of Cx. pipiens quinquefasciatus, Cx. pipiens pallens, and Cx. pipiens molestus still focused \non the abdominal regions, as shown in dark red. The area of focus of these neural networks differ from that of \n(1)�(W − MSA) = 4HWC 2+ 2M2HWC\n7\nVol.:(0123456789)Scientific Reports |        (2022) 12:18664  | https://doi.org/10.1038/s41598-022-21017-6\nwww.nature.com/scientificreports/\nthe human eye, and the results of this study suggest that the Swin MSI model can detect finely granular features \namong these three mosquito subspecies that are indistinguishable to the naked human eye.\nFigure 5.  Attention visualization of representative mosquitoes of the genera Ae., Cx., An., Armigeres, \nCoquillettidia and Mansonia. This is a visualization for identifying the regions in the image that can explain the \nclassification progress. Images of Toxorhynchites contain only males, with obvious differences in morphological \ncharacteristics, are not shown.\n8\nVol:.(1234567890)Scientific Reports |        (2022) 12:18664  | https://doi.org/10.1038/s41598-022-21017-6\nwww.nature.com/scientificreports/\nNovel mosquito classification attribution. After we performed a confidence check on the success-\nfully identified mosquito images in the dataset, the lowest confidence value was found to be 85%. A higher \nconfidence threshold mean stricter evaluation criteria, which can better reflect the powerful performance of the \nmodel. Therefore, 0.85 was set as the confidence threshold when judging novel mosquitoes. When identifying 10 \nunknown mosquito species, the highest derived species confidence level was below 85%; when the results were \noutput to the genus level (Fig. 1d), the average probability of obtaining a correct judgment was 96.26%accuracy \nand 98.09% F1-score (Table 3). The images tested as novel Ae., Cx. and An. mosquito were from Minakshi and \nCouret et al.28,29.\nDiscussion\nIn a previous study by Pataki et al.30, the highest mosquito species identification accuracy was 96%; this value was \nobtained with ResNet based on a public pest vector monitoring dataset (containing 6195 Ae. albopictus images out \nof 7686 images, accounting for 80.6%). In Motta et al. ’s  study31, a dataset of 4056 adult mosquito images of three \nspecies (A. albopictus, A. aegypti, and Cx. pipiens quinquefasciatus) was constructed and trained using the LeNet, \nAlexNet, and GoogleNet CNNs, and the test results showed that the best precision was obtained with GoogleNet \nat 76.2%. Park et al. constructed a dataset containing 3578 images of eight mosquito species, and classification \naccuracies above 97% was achieved using VGG-16, ResNet-50, and SqueezeNet. Couret et al. constructed a \ndataset containing 14 species of mosquitoes, mainly within the genus An., for a total of 1709 images; using this \ndataset, a species identification accuracy of 96.96% and a sex prediction accuracy of 98.48% were achieved using \nthe best-configured DenseNet-201 model.\nObviously, previous mosquito recognition studies used computer vision techniques mostly with ResNet, \nDenseNet and other classical CNNs. This study is the first to apply the recently emerged Transformer  model32 for \nmosquito species identification. CNNs complete the whole training process by using stacked convolutional layers \nand pooling layers to produce layer-level feature representations of different sizes and perceive image features \non various scales. However, the structural design of a CNN itself focuses primarily on the extraction effects of \nlocal features and tends to ignore the connections among different elements in an image. The Swin MSI model \nproposed in this study combines the advantages of both the Transformer and CNN models. The architecture of \nthe constructed model works similarly to CNNs by stacking Swin-transformer blocks and continuously increas-\ning the patch dimensions and sizes; in addition, the use of a self-attentive mechanism based on moving windows \neffectively reduces the number of parameters generated during the computation process. By dividing the feature \nmap of size into nonoverlapping windows of size, the computation is linear in the square of the window size, \nthus solving the problem of the high computational complexity of traditional Transformer models. The Swin \nMSI model allows the new patches to expand in size through a merging process, thus playing the same role as \nthe sensory field in a CNN. In the initial stage, only local features such as antennae, mesoterga, legs or wings \ncan be sensed, but in the large-sized patches formed after several patch-merging steps, global features such as \nheads, thoraces and abdomens can be sensed, and focus on the mosquitoes overall can eventually be obtained.\nThe most critical issue in any computer vision detection task is the manual establishment of image annotations \n(professional mosquito classification staff are required to perform these annotations)33,34. Therefore, obtaining a \ncomplete and clear dataset for accurate predictions can save considerable time and costs. Except for a study on the \nAn. gambiae Complex group, which involving a subspecies-level analysis, previous mosquito classification stud-\nies have mostly focused on species with obvious biological taxonomic characteristics, such as A. aegypti and A. \nalbopictus.31 Park et al. found that some mutilated field-collected mosquitoes prevented the neural network from \nbeing able to capture key features, thus causing classification  errors30. In addition, imbalances in the number of \ndifferent mosquito species cause neural networks to tend to reduce the recognition quality of minority categories \nto maximize the overall recognition  rate35. Learning from previous experiences, the mosquito images collected in \nthis study contain a large number of critical morphological feature, species and sex details and form the clearest \nand most balanced professional mosquito database thus far; this database ensures the optimal performance of \nthe constructed Swin-MSI model in mosquito species identification tasks.\nTable 3.  Probability of correct attribution of novel species. Acc was short for accuracy and F1 was shoret for \nf1-score.\nNovel Species Acc (%) F1 (%) Tested Image numbers\nAe. infirmatus 80.00 88.89 10\nAe. taeniorhynchus 80.00 88.89 10\nCx. coronator 70.00 82.35 10\nCx. nigripalpus 90.00 94.74 10\nAn. albimanus 98.44 99.21 64\nAn. arabiensis 95.85 99.20 193\nAn. atroparvus 100 100 36\nAn. coluzzi 95.95 97.93 74\nAn. farauti 100 100 57\nAn. freeborni 98.97 99.48 97\n9\nVol.:(0123456789)Scientific Reports |        (2022) 12:18664  | https://doi.org/10.1038/s41598-022-21017-6\nwww.nature.com/scientificreports/\nWe found that the higher the resolution of the mosquito image training set was, the better the recognition \neffect was. The mosquito species identification accuracy was improved by 0.84%, to 99.04%, when the input \nimage size was set to 384 × 384 compared to 224 × 224, other related research results confirm the superiority \nof large-size image  input36. Currently, the efficiencies of computer vision network models are limited by the \nabilities of hardware devices. Although we made some adjustments, including adjustments to the number of \nmodel parameters, the constructed model still required substantial computational times on high-performance \nhardware devices, such as the graphics processing unit (GPU) to perform the data computations and the central \nprocessing unit (CPU) to make resource calls. The video memory of the GPU determines the amount of data \nthat can be computed in parallel, which directly affects the training time. Our mosquito dataset had a resolution \nof 4464 × 2976 pixels and had taken the appropriate redundancy of the image sizes into account. In the future, \nwith the improvements in hardware computing power and adjustments to the patch-merging process to optimize \nthe Swin MSI model structure, the advantages of high-resolution datasets will be maximized, thus leading to \neven higher accuracies.\nIn this study, we attempted to combine the experience of professional taxonomists with the perspective of \nartificial intelligence to determine whether the Swin Transformer model uses similar morphological keys as \nthose used by human experts to classify mosquito species and subspecies. The evolutionary statuses of subspecies \nwithin mosquito Complexes are similar, and subspecies within the same Complex have subtle morphological \ncharacteristic differences. Unlike the An. gambiae Complex, which is mainly found in limited geographic  areas37, \nthe Cx. pipiens Complex group targeted in this study is widely distributed  worldwide38, and their corresponding \nblood-sucking host preferences, self-fertility processes, breeding environments, and stagnant overwintering \nprocesses are significantly different. Thanks to the high-resolution dataset containing complete morphological \ncharacteristic information, 100% subspecies and sex identification accuracies were achieved at the subspecies \nlevel for the Cx. pipiens Complex. The visualization results suggested that the morphological keys used by the \nSwin MSI model were similar but not the same as those used by humans. The identification features derived at \ncertain pixel-level details included mosquito heads, thoraces, abdomens, dorsal plates, and legs; these features \nmay serve as useful supplements to traditional taxonomic features and provide a reference with which profes-\nsional taxonomists can explore the subspecies-level morphological features of mosquitoes in depth.\nAlthough the dataset established in this study already includes 17 species (including 3 subspecies) of mos -\nquitoes, the need to identify novel mosquitoes is common and inevitable in realistic practical applications. The \nhighest species confidence level achieved with the existing Swin MSI model was below 85%, and the correct \nattribution probability of the model reached 96.3%. This is a stop-gap measure under existing resource (pri -\noritizing coverage of the most dangerous and common mosquitoes), but it is sufficient to help inexperienced \npersonnel on site identify dangerous mosquitoes. By expanding the number of species represented in specific \narea mosquito dataset and optimizing the model structure and parameters, the species confidence threshold can \nbe continuously increased, leading to higher attribution accuracies for novel mosquitoes. More developed ver -\nsion for different countries and regions to cover their local mosquito species will be move forward in the future. \nThe Swin MSI model can help mosquito taxonomists quickly distinguish new species from a large number of \nspecimens identified in field biodiversity  surveys39. The model also has a high identification accuracy for dam-\naged mosquito samples collected in the field, as long as the training set contained sufficient recognition features, \nwhich makes mosquito surveillance tasks more effective and  efficient40.\nIn summary, the first transformer-based mosquito species identification model was proposed in this study. \nWith the help of a self-constructed high-precision mosquito image dataset, the Swin MSI model achieved a spe-\ncies recognition rate greater than 99%. The identification effect is also tested and discussed at various input image \nsizes, at the subspecies level (at which the morphological features of mosquitos are difficult to distinguish), and \nfor novel species. The excellent performance of the Swin MSI model makes it an accurate and efficient techni -\ncal tool that will help taxonomists quickly identify mosquito species and contribute to the control of mosquito \nborne diseases.\nMethods\nEthic statements. The study was approved by Institutional Animal Care and Use Committee of Beijing \nInstitute of Microbiology and Epidemiology (IACUC-IME-2021-021). The mosquito field sampling was con-\nducted without harming any other animal species.\nMosquito datasets. In this work, mosquitoes were identified by taxonomists using morphological indica-\ntors identified through a standard stereomicroscope. All mosquitoes were killed by anesthesia, and pictures \nwere taken of the mosquitos in the form of natural death. We built a macro-photography platform of mosquito \nimages using of a Canon electro-optical system (EOS) 5D Mark IV digital camera with a Laowa 100-mm F2.8 \nMacro 2X lens and a Kuangren KR-888 macro flash to capture the dorsal, ventral, left, and right sides of each \nmosquito, covering the key identification features. Three to five images were taken of each mosquito. the camera \nand flash parameters were set to ISO400, the shutter speed was set to 1/100 s, the aperture was set to 22, and the \nflash index was set to 1/16.\nData division, preprocessing and augmentation steps. The captured mosquito images were labeled \nusing LabelImg software to generate corresponding .txt files containing category, sex, and location coordinate \ninformation. A total of 9900 images captured of 17 species(and 3 subspecies) were used according to the prin-\nciple of 50/50 males and females (since identifying features of males and females were different, each species \nincluded 300 images of both sexes. If one sex was missing from the samples, the number of mosquitoes extracted \nfrom that species was halved). The images were partitioned randomly into training, validation, and test sets \n10\nVol:.(1234567890)Scientific Reports |        (2022) 12:18664  | https://doi.org/10.1038/s41598-022-21017-6\nwww.nature.com/scientificreports/\ncomprising 6930 (70%), 1485 (15%), and 1485 (15%) images in the dataset, respectively. To prevent underfitting, \nreduce the probability of overfitting, and prevent classification imbalances resulting from unbalanced category \nshares, we applied 24 rotations with 15-degree increments to substantially increase the size and diversity of the \ntraining image set by using rotations, flips, and resizing methods.\nComparison of the CNN models and transformer model test results. Six computer vision models \n(the Mask R-CNN, DenseNet, YOLOv5, Swin Transformer, Detection Transformer and Vision Transformer \nmodels) were first selected for testing (Table  4)22–24,41,42. The Mask R-CNN model uses a feature pyramid net-\nwork (FPN) architecture to enhance its multiscale feature extraction  capability41. DenseNet further mitigates \nthe gradient disappearance problem by establishing connections among different  layers42. YOLOv5 surpasses \nvarious previous versions of the YOLO model with a small number of parameters and a strong rapid deployment \nadvantage. Detection Transformer used a CNN to extract features and then used a transformer to perform iden-\ntification, which truly achieves end-to-end detection with less prior. Dosovitskiy proposed a pure transformer, \nnamely, the Vision Transformer (ViT), to serialize images and applied the transformer to image classification \ntasks with good results. The Swin Transformer uses a sliding window and introduces a concept similar to the \nexpanded field of perception used in CNNs. The above six models are excellent representatives of CNNs and \ntransformers and have achieved high accuracies when analyzing datasets such as ImageNet in various applica-\ntion scenarios.\nMSI model design. By screening the six models described above, the Swin Transformer algorithm model \nwas found to have the best performance in the first testing phase and was ultimately selected for further analysis. \nThe parameter configuration was further adjusted to test the effects of the variants (T, B and L) and input image \nsizes (384 × 384 and 224 × 224) on the species recognition accuracy. The specific steps will be further explained \nin the following section.\nSwin MSI framework. The Swin MSI model splits the preprocessed mosquito image into many nonover -\nlapping patches using a patch-splitting model. The number of patches is H\n4 × W\n4  , each patch is considered a token \nwith dimensions of 4 × 4, and the features of each patch are set as a series of pixel values in the preprocessed \nmosquito image; thus, the number of features in each patch is 4 × 4 × 3 = 48. The linear embedding layer is then \napplied to the segmented patches to project them to the set dimensions (C).\nThe computation results of stage 1 were combined with 2 × 2 similarly sized patches by patch merging, the \nsize was increased to 8 × 8, and the number of patches became H\n8 × W\n8  ; these patches were then used as the input \nin stage 2. After repeating the self-attention calculation of the Swin Transformer block, the output of stage 2 was \nobtained, and the calculations were continued in two stages (denoted as stage 3 and stage 4) until the patch size \nreached 32 × 32. At this time, the number of patches was H\n32 × W\n32 , the dimension for 8C. In the process of mov-\ning the window, half of the patches were moved each time to ensure that no features were lost in the process. In \nthis work, the input image sizes were set to 224 × 224 and 384 × 384 pixels, and the final patch numbers used in \neach window were set to 7 × 7 and 12 × 12.\nShifted windows multihead self‑attention. The Swin Transformer block consists of two consecutive \nSwin Transformer modules: multi-head self-attentive modules with a regular window configuration (W-MSA) \nand moving window configuration (SW-MSA). A layer normalization processing module (LN ()), a multilayer \nperceptron (MLP ()), the output feature of the SW-MSA module ( ∧\nZL ), and the output feature of the MLP features \n( ZL ) were also established.\nThe computation in the continuous Swin Transformer block can be expressed by Eq. (1) as follows:\nTable 4.  Parameter settings of the six computer vision models.\nModel Numbers of epochs Learning rate Batch-size\nDetection Transformer 300 0.0001 2\nVision Transformer 500 0.03 512\nSwin Transformer 300 0.0005 128\nYOLOv5 300 0.001 16\nMask R-CNN 160 0.001 1\nDenseNet 300 0.1 16\n11\nVol.:(0123456789)Scientific Reports |        (2022) 12:18664  | https://doi.org/10.1038/s41598-022-21017-6\nwww.nature.com/scientificreports/\nThe SW-MSA module derived for the nonoverlapping window X can be expressed by Eq. (2) as follows:\nThe multi-head self-attention on the windows was calculated using the query (Q), key (K), and value (V). \nDuring this process, the learnable relative position was used to encode B; this feature can be expressed by Eq. (3) \nas follows:\nFor the Swin Transformer, this multi-head self-attention feature was computed n times (the number of Swin \nTransformer Blocks used in different stages in the network) and then concatenated to obtain the final multi-head \nself-attention results.\nSwin MSI model for novel mosquito classification. In the real environment, the need to identify \nnovel mosquitoes is common and inevitable. Mosquito species not included in our dataset (which includes 17 \nspecies 3 subspecies) are defined herein as novel species. When mosquito images were successfully analyzed by \nthe Swin MSI model, a matrix of size [m, n] was output as C, where m is the number of mosquito images and n \nis the number of categories used in the process. At this point, the row vector was softmax-normalized to obtain \nthe confidence level of each category in the target image, and this information was used for the species categori-\nzation process. By determining the mosquito images in which the mosquito species was accurately identified in \nthis work, the minimum confidence level could be checked and established as the confidence threshold required \nfor novel species detection; if the highest species confidence level of the image to be identified is greater than this \nthreshold, species-level information can be output; if the highest species confidence level is below the threshold, \ngenus-level information can be obtained.\nIdentification of morphological keys used by the Swin MSI model. Using gradient-weighted class \nactivation mapping (Grad-CAM)43, we visualized and identified the regions in each image that could explain \nthe final classification results obtained by the Swin MSI model. First, the network was forward-propagated to \nobtain feature layer A (in this work, “feature layer A ” refers to the output in each stage) and the network predic-\ntion y. Assuming that the categorical prediction of the network for a given mosquito picture is  yc, the backward-\npropagation step could provide gradient information A ’ , which was then back-propagated to feature layer A. By \ncalculating the importance of each channel in feature layer A, weighting the sum, and performing the rectified \nlinear unit (ReLu) calculation, the final result was the Grad-CAM. The regions of interest obtained in the atten-\ntion visualization step were then compared with the morphological classification features that are of interest to \nmosquito taxonomists to explore whether the Swin Transformer model used similar morphological keys as those \nused by human experts to classify mosquito species.\nEvaluation methodology. We reported accuracy (acc) and f1-score(f1) in each experiments, the acc can \nbe define as:\nThe F1 can be formulated as:\nwhere precision and recall can be defined as:\nwhere TP is the number of true positive samples, TN is the number of true negative samples, FP is the number \nof false positive samples and FN is the number of false negative samples.\n(2)\n∧l\nZ = W - MSA(LN(Z L−1)) + ZL−1,\nZl = MLP(LN(\n∧l\nZ )) +\n∧l\nZ,\nˆZ\nl+1\n= SW - MSA(LN(Z l)) + Zl,\nZl+1 = MLP\n(\nLN\n(\nˆZ\nl+1\n))\n+ ˆZ\nl+1\n(3)\nMultiHead(Q ,K ,V ) = Concat(head1 ,... ,headh)W O\nwhere headi= Attention(QWQ\ni ,KW K\ni ,VW V\ni )\n(4)Attention(Q,K,V) = SoftMax(QKT /\n√\nd +B)V\nacc= TP + TN\nTP + TN + FP + FN\nf1 = 2 × precision × recall\nprecision + recall\nPrecision = TP\nTP + FP\nRecall= TP\nTP + FN\n12\nVol:.(1234567890)Scientific Reports |        (2022) 12:18664  | https://doi.org/10.1038/s41598-022-21017-6\nwww.nature.com/scientificreports/\nData availability\nThe datasets used and analysed during the current study available from the corresponding author on reasonable \nrequest.\nCode availability\nWe have leveraged Github repository for Mask R-CNN, DenseNet, YOLOv5, Detection Transformer, Vision \nTransformer, and Swin Transformer implementation. The code for Swin MSi are publicly available through \nGitHub repository: https:// github. com/ zdz00 86/ Swin- msi. All are open source and publicly available.\nReceived: 10 May 2022; Accepted: 21 September 2022\nReferences\n 1. WHO. Vector-Borne Diseases. https:// www. who. int/ news- room/ fact- sheets/ detail/ vector- borne- disea ses. (2020).\n 2. Ashley, E. A., Pyae Phyo, A. & Woodrow, C. J. Malaria. Lancet 391, 1608–1621 (2018).\n 3. Martina, B. E. E., Koraka, P . & Osterhaus, A. D. M. E. Dengue virus pathogenesis: An integrated view. Clin. Microbiol. Rev. 22, \n564–581 (2009).\n 4. Shan, C., Xie, X. & Shi, P .-Y . Zika virus vaccine: Progress and challenges. Cell Host Microbe 24, 12–17 (2018).\n 5. Silva, L. A. & Dermody, T. S. Chikungunya virus: Epidemiology, replication, disease mechanisms, and prospective intervention \nstrategies. J. Clin. Invest. 127, 737–749 (2017).\n 6. Petersen, L. R., Brault, A. C. & Nasci, R. S. West Nile Virus: Review of the literature. JAMA 310, 308–315 (2013).\n 7. Sharma, K. B., Vrati, S. & Kalia, M. Pathobiology of Japanese encephalitis virus infection. Mol. Aspects Med. 81, 100994 (2021).\n 8. Taylor, M. J., Hoerauf, A. & Bockarie, M. Lymphatic filariasis and onchocerciasis. Lancet 376, 1175–1185 (2010).\n 9. Sinka, M. E. et al.  The dominant Anopheles vectors of human malaria in the Asia-Pacific region: Occurrence data, distribution \nmaps and bionomic precis. Parasites Vectors 4, 89 (2011).\n 10. Jones, R. T., Ant, T. H., Cameron, M. M. & Logan, J. G. Novel control strategies for mosquito-borne diseases. Philos. Trans. R. Soc. \nB. 376, 20190802 (2021).\n 11. Ferguson, N. M. Challenges and opportunities in controlling mosquito-borne infections. Nature 559, 490–497 (2018).\n 12. Harbach, R. E. The phylogeny and classification of Anopheles. In Anopheles Mosquitoes (ed. Manguin, S.) (IntechOpen, 2013).\n 13. Hebert, P . D., Cywinska, A., Ball, S. L. & deWaard, J. R. Biological identifications through DNA barcodes. Proc. Biol. Sci. 270, \n313–321 (2003).\n 14. Gao, Q. et al. Structure, spatial and temporal distribution of the Culex pipiens complex in Shanghai, China. Int. J. Environ. Res. \nPublic Health 13, 1150 (2016).\n 15. Zhao, T. & Lu, B. Biosystematics of Culex pipiens Complex in China. Insect Sci. 2, 1–8 (1995).\n 16. Lecun, Y ., Bottou, L., Bengio, Y . & Haffner, P . Gradient-based learning applied to document recognition. Proc. IEEE 86, 2278–2324 \n(1998).\n 17. Krizhevsky, A., Sutskever, I. & Hinton, G. E. ImageNet classification with deep convolutional neural networks. In Neural Informa-\ntion Processing Systems (2012).\n 18. Szegedy, C. et al. Going Deeper with convolutions. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2015).\n 19. Simonyan, K. & Zisserman, A. Very Deep Convolutional Networks for Large-Scale Image Recognition. arXiv: 1409. 1556 (2014).\n 20. He, K. M., Zhang, X. Y ., Ren, S. Q. & Sun, J. Deep residual learning for image recognition. In IEEE Conference on Computer Vision \nand Pattern Recognition (CVPR) (2016).\n 21. Iandola, F . N. et al. SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and < 0.5MB model size. arXiv: 1602. 07360  \n(2016).\n 22. Carion, N. et al. End-to-End Object Detection with Transformers. arXiv: 2005. 12872 (2020).\n 23. Dosovitskiy, A. et al. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. arXiv: 2010. 11929 v2 (2021).\n 24. Liu, Z. et al. Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. arXiv: 2103. 14030 (2021).\n 25. Lin, T.-Y ., RoyChowdhury, A. & Maji, S. Bilinear CNN models for fine-grained visual recognition. In 2015 IEEE International \nConference on Computer Vision (ICCV) (2015).\n 26. Hariharan, B., Arbeláez, P ., Girshick, R. & Malik, J. Hypercolumns for object segmentation and fine-grained localization. In 2015 \nIEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2015).\n 27. Harbach, R. E. Culex pipiens: Species versus species complex—taxonomic history and perspective. J. Am. Mosq. Control Assoc. 28, \n10–23 (2012).\n 28. Minakshi, M., Bharti, P ., Bhuiyan, T., Kariev, S. & Chellappan, S. A framework based on deep neural networks to extract anatomy \nof mosquitoes from images. Sci. Rep. 10, 13059 (2020).\n 29. Couret, J. et al. Delimiting cryptic morphological variation among human malaria vector species using convolutional neural \nnetworks. PLoS Negl. Trop. Dis. 14, e0008904 (2020).\n 30. Park, J., Kim, D. I., Choi, B., Kang, W . & Kwon, H. W . Classification and morphological analysis of vector mosquitoes using deep \nconvolutional neural networks. Sci. Rep. 10, 1012 (2020).\n 31. Motta, D. et al. Application of convolutional neural networks for classification of adult mosquitoes in the field. PLoS ONE 14, \ne0210829 (2019).\n 32. Vaswani, A. et al. Attention Is All You Need. arXiv: 1706. 03762(2017).\n 33. Deng, J. et al. ImageNet: A large-scale hierarchical image database. In IEEE-Computer-Society Conference on Computer Vision and \nPattern Recognition Workshops (2009).\n 34. Lin, T.-Y . et al. Microsoft COCO: Common Objects in Context. arXiv: 1405. 0312 (2015).\n 35. Buda, M., Maki, A. & Mazurowski, M. A. A systematic study of the class imbalance problem in convolutional neural networks. \nNeural Netw. 106, 249–259 (2018).\n 36. Valan, M., Makonyi, K., Maki, A., Vondracek, D. & Ronquist, F . Automated taxonomic identification of insects with expert-level \naccuracy using effective feature transfer from convolutional networks. Syst. Biol. 68, 876–895 (2019).\n 37. Holt, R. A. et al. The genome sequence of the malaria mosquito Anopheles gambiae. Science 298, 129–149 (2002).\n 38. Fonseca, D. M. et al. Emerging vectors in the Culex pipiens complex. Science 303, 1535–1538 (2004).\n 39. Høye, T. T. et al. Deep learning and computer vision will transform entomology. PNAS. 118(2), e2002545117 (2020).\n 40. Kittichai, V . et al. Deep learning approaches for challenging species and gender identification of mosquito vectors. Sci. Rep. 11, \n4838 (2021).\n 41. He, K., Gkioxari, G., Dollar, P . & Girshick, R. Mask R-CNN. In: 16th IEEE International Conference on Computer Vision (ICCV) \n(2017).\n 42. Huang, G., Liu, Z., van der Maaten, L. & Weinberger, K. Q. Densely connected convolutional networks. In 30th IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR) (2017).\n13\nVol.:(0123456789)Scientific Reports |        (2022) 12:18664  | https://doi.org/10.1038/s41598-022-21017-6\nwww.nature.com/scientificreports/\n 43. Selvaraju, R. R, et al. Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization. arXiv: 1610. 02391(2016).\nAuthor contributions\nC.L. and G.C., F .S. conceived, designed and supervised the whole study. D.Z., X.W . and T.Z. were responsible for \nthe original draft of the article. D.Z. developed the Transformer-based mosquito species identification model. \nX.W . constructed the mosquito image database. T.Z. conducted the writing-reviewing and editing. H.L., D.X., \nand H.G. were responsible for resources, visualization and project administration.\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to F .S., G.C. or C.L.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2022"
}