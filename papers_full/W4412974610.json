{
  "title": "Evaluation of large language model-driven AutoML in data and model management from human-centered perspective",
  "url": "https://openalex.org/W4412974610",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2557730673",
      "name": "Jiapeng Yao",
      "affiliations": [
        "Wenzhou University of Technology",
        "Wenzhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2110580212",
      "name": "Lan-tian Zhang",
      "affiliations": [
        "Southeast University"
      ]
    },
    {
      "id": "https://openalex.org/A2159172409",
      "name": "Jiping Huang",
      "affiliations": [
        "Haikou City People's Hospital",
        "Hainan Modern Women and Children's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2557730673",
      "name": "Jiapeng Yao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2110580212",
      "name": "Lan-tian Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2159172409",
      "name": "Jiping Huang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6762931045",
    "https://openalex.org/W6800166007",
    "https://openalex.org/W4394927283",
    "https://openalex.org/W4252731897",
    "https://openalex.org/W4405396941",
    "https://openalex.org/W6872333957",
    "https://openalex.org/W6798182279",
    "https://openalex.org/W6877079079",
    "https://openalex.org/W6872823725",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W6774581290",
    "https://openalex.org/W6685961532",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W6604629408",
    "https://openalex.org/W4213308398",
    "https://openalex.org/W2964024268",
    "https://openalex.org/W4321162272",
    "https://openalex.org/W4381785650",
    "https://openalex.org/W6783868644",
    "https://openalex.org/W3038962616",
    "https://openalex.org/W4362454177",
    "https://openalex.org/W4402985132",
    "https://openalex.org/W6873607958",
    "https://openalex.org/W6873689019",
    "https://openalex.org/W4403791334",
    "https://openalex.org/W6871137836",
    "https://openalex.org/W2585240214",
    "https://openalex.org/W4402835670",
    "https://openalex.org/W6762903864",
    "https://openalex.org/W6804059111",
    "https://openalex.org/W6766225098",
    "https://openalex.org/W2775233965",
    "https://openalex.org/W6871880335",
    "https://openalex.org/W6872008754",
    "https://openalex.org/W6851335697",
    "https://openalex.org/W6872679585",
    "https://openalex.org/W6891979677",
    "https://openalex.org/W6862961812",
    "https://openalex.org/W6929229033",
    "https://openalex.org/W6866498526",
    "https://openalex.org/W4313049560",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W4321524264",
    "https://openalex.org/W4407551092",
    "https://openalex.org/W6873106570",
    "https://openalex.org/W6856334306",
    "https://openalex.org/W6871181698",
    "https://openalex.org/W4404592654",
    "https://openalex.org/W6863024007",
    "https://openalex.org/W6848178237",
    "https://openalex.org/W6852417765",
    "https://openalex.org/W3139250374",
    "https://openalex.org/W6729956949",
    "https://openalex.org/W4387640485",
    "https://openalex.org/W4415061858",
    "https://openalex.org/W2553303224",
    "https://openalex.org/W2182361439",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W4386158703",
    "https://openalex.org/W4415061873",
    "https://openalex.org/W2948715311",
    "https://openalex.org/W4385572142",
    "https://openalex.org/W4287024925",
    "https://openalex.org/W4402703807",
    "https://openalex.org/W4386562916",
    "https://openalex.org/W4372272770",
    "https://openalex.org/W4385486090",
    "https://openalex.org/W4406262333",
    "https://openalex.org/W4404396464",
    "https://openalex.org/W4402711364",
    "https://openalex.org/W4287826698",
    "https://openalex.org/W4407404178",
    "https://openalex.org/W4403885480"
  ],
  "abstract": "As organizations increasingly seek to leverage machine learning (ML) capabilities, the technical complexity of implementing ML solutions creates significant barriers to adoption and impacts operational efficiency. This research examines how Large Language Models (LLMs) can transform the accessibility of ML technologies within organizations through a human-centered Automated Machine Learning (AutoML) approach. Through a comprehensive user study involving 15 professionals across various roles and technical backgrounds, we evaluate the organizational impact of an LLM-based AutoML framework compared to traditional implementation methods. Our research offers four significant contributions to both management practice and technical innovation: First, we present pioneering evidence that LLM-based interfaces can dramatically improve ML implementation success rates, with 93.34% of users achieved superior performance in the LLM condition, with 46.67% showing higher accuracy (10%–25% improvement over baseline) and 46.67% demonstrating significantly higher accuracy (&amp;gt;25% improvement over baseline), while 6.67% maintained comparable performance levels; and 60% reporting substantially reduced development time. Second, we demonstrate how natural language interfaces can effectively bridge the technical skills gap in organizations, cutting implementation time by 50% while improving accuracy across all expertise levels. Third, we provide valuable insights for organizations designing human-AI collaborative systems, showing that our approach reduced error resolution time by 73% and significantly accelerated employee learning curves. Finally, we establish empirical support for natural language as an effective interface for complex technical systems, offering organizations a path to democratize ML capabilities without compromising quality or performance.",
  "full_text": "TYPE Original Research\nPUBLISHED /zero.tnum/five.tnum August /two.tnum/zero.tnum/two.tnum/five.tnum\nDOI /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/nine.tnum/zero.tnum/one.tnum/zero.tnum/five.tnum\nOPEN ACCESS\nEDITED BY\nWeiyong Si,\nUniversity of Essex, United Kingdom\nREVIEWED BY\nHaolin Fei,\nLancaster University, United Kingdom\nXiaohao Wen,\nGuangxi Normal University, China\nXu Ran,\nUniversity of Essex, United Kingdom\nZengliang Han,\nNanjing University of Aeronautics and\nAstronautics, China\n*CORRESPONDENCE\nJiapeng Yao\nyaojiapeng/one.tnum/nine.tnum/eight.tnum/three.tnum@/one.tnum/six.tnum/three.tnum.com\nRECEIVED /two.tnum/one.tnum March /two.tnum/zero.tnum/two.tnum/five.tnum\nACCEPTED /zero.tnum/eight.tnum July /two.tnum/zero.tnum/two.tnum/five.tnum\nPUBLISHED /zero.tnum/five.tnum August /two.tnum/zero.tnum/two.tnum/five.tnum\nCITATION\nYao J, Zhang L and Huang J (/two.tnum/zero.tnum/two.tnum/five.tnum) Evaluation\nof large language model-driven AutoML in\ndata and model management from\nhuman-centered perspective.\nFront. Artif. Intell./eight.tnum:/one.tnum/five.tnum/nine.tnum/zero.tnum/one.tnum/zero.tnum/five.tnum.\ndoi: /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/nine.tnum/zero.tnum/one.tnum/zero.tnum/five.tnum\nCOPYRIGHT\n© /two.tnum/zero.tnum/two.tnum/five.tnum Yao, Zhang and Huang. This is an\nopen-access article distributed under the\nterms of the\nCreative Commons Attribution\nLicense (CC BY) . The use, distribution or\nreproduction in other forums is permitted,\nprovided the original author(s) and the\ncopyright owner(s) are credited and that the\noriginal publication in this journal is cited, in\naccordance with accepted academic practice.\nNo use, distribution or reproduction is\npermitted which does not comply with these\nterms.\nEvaluation of large language\nmodel-driven AutoML in data and\nmodel management from\nhuman-centered perspective\nJiapeng Yao/one.tnum*, Lantian Zhang /two.tnumand Jiping Huang /three.tnum\n/one.tnumSchool of Data Science and Artiﬁcial Intelligence, Wenzhou Unive rsity of Technology, Wenzhou,\nZhejiang, China, /two.tnumSchool of Computer Science and Engineering, Southeast University, Nanjing,\nJiangsu, China, /three.tnumHaikou Qiongzhou Women’s and Children’s Hospital, Haikou, Hainan, Chi na\nAs organizations increasingly seek to leverage machine learnin g (ML) capabilities,\nthe technical complexity of implementing ML solutions creates s igniﬁcant\nbarriers to adoption and impacts operational eﬃciency. This re search examines\nhow Large Language Models (LLMs) can transform the accessibilit y of ML\ntechnologies within organizations through a human-centered Au tomated\nMachine Learning (AutoML) approach. Through a comprehensive u ser study\ninvolving /one.tnum/five.tnum professionals across various roles and technical backgrounds,\nwe evaluate the organizational impact of an LLM-based AutoML f ramework\ncompared to traditional implementation methods. Our research oﬀers four\nsigniﬁcant contributions to both management practice and technical i nnovation:\nFirst, we present pioneering evidence that LLM-based interface s can dramatically\nimprove ML implementation success rates, with /nine.tnum/three.tnum./three.tnum/four.tnum% of users achieved\nsuperior performance in the LLM condition, with /four.tnum/six.tnum./six.tnum/seven.tnum% showing higher\naccuracy (/one.tnum/zero.tnum%–/two.tnum/five.tnum% improvement over baseline) and /four.tnum/six.tnum./six.tnum/seven.tnum% demonstrating\nsigniﬁcantly higher accuracy (>/two.tnum/five.tnum% improvement over baseline),while /six.tnum./six.tnum/seven.tnum%\nmaintained comparable performance levels; and /six.tnum/zero.tnum% reporting substantially\nreduced development time. Second, we demonstrate how natural lan guage\ninterfaces can eﬀectively bridge the technical skills gap in orga nizations, cutting\nimplementation time by /five.tnum/zero.tnum% while improving accuracy across all expertise\nlevels. Third, we provide valuable insights for organizatio ns designing human-AI\ncollaborative systems, showing that our approach reduced error r esolution\ntime by /seven.tnum/three.tnum% and signiﬁcantly accelerated employee learning curves. Finally, we\nestablish empirical support for natural language as an eﬀective interface for\ncomplex technical systems, oﬀering organizations a path to dem ocratize ML\ncapabilities without compromising quality or performance.\nKEYWORDS\nlarge language models, automated machine learning, human-co mputer interaction,\ndeep learning, natural language interfaces\n/one.tnum Introduction\nThe exponential growth in machine learning (ML) applications has transformed\nnumerous sectors, from healthcare ( Ke et al., 2020a ,b; Shen et al., 2024c ) to scientiﬁc\nresearch ( Wang and Shen, 2024 ). However, implementing these ML models remains a\nchallenge due to the complex technical requirements involved. Deep learning (DL) models,\nwhile demonstrating remarkable capabilities across computer vision, natural language\nprocessing, and other domains, require extensive expertise (\nLiu et al., 2024a ). This expertise\nFrontiers in Artiﬁcial Intelligence /zero.tnum/one.tnum frontiersin.org\nYao et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/nine.tnum/zero.tnum/one.tnum/zero.tnum/five.tnum\nbarrier includes understanding model architectures ( Shen et al.,\n2023, 2024b; Ke et al., 2023b ), managing data pre-processing ( Ke\net al., 2023a ; Luo S. et al., 2024 ; Wang et al., 2024 ; Wen et al.,\n2024), implementing training procedures ( Shen et al., 2022 ; Shen,\n2024), and handling deployment, which are tasks that typically\nrequire years of specialized education and experience. As a result,\nmany potential users and organizations that could beneﬁt from ML\ntechnologies remain unable to eﬀectively implement them, creating\na widening gap between ML ’s potential and its practical accessibility.\nAutomated Machine Learning (AutoML) emerged as a\npotential solution to this accessibility challenge by attempting to\nautomate these aspects of the ML pipeline (\nSun et al., 2023 ; Hutter\net al., 2019 ). Traditional AutoML systems aim to streamline various\ntechnical processes, including feature engineering, model selection,\nhyperparameter optimization, and deployment workﬂows (\nHutter\net al., 2019 ; Baratchi et al., 2024 ; Patibandla et al., 2021 ). Notable\nimplementations like Auto-Sklearn ( Feurer et al., 2015 ), TPOT\n(Olson and Moore, 2016 ), Auto-Keras ( Jin et al., 2019 ), H2O\n(LeDell and Poirier, 2020 ), AutoGluon ( Erickson et al., 2020 ),\nand Auto-Pytorch ( Zimmer et al., 2021 ), and platforms like\nAzure Machine Learning, Google Cloud AutoML, H2O Driverless\nAI, etc. have demonstrated success in reducing the technical\noverhead of machine learning implementation. However, these\nAutoML tools still present usability challenges as users must\nnavigate complex conﬁguration interfaces, understand technical\nparameters, and possess programming knowledge to eﬀectively\nutilize these tools. Furthermore, traditional AutoML methods often\nrequire users to make critical decisions about model selection and\nconﬁguration without providing intuitive guidance or explanation.\nThis limitation means that even AutoML solutions, despite\ntheir automation capabilities, remain largely inaccessible to non-\nexpert users, particularly those without substantial programming\nexperience or machine learning background.\nRecent advances in Large Language Models (LLMs) have\nopened new possibilities for human-computer interaction, oﬀering\nnatural language interfaces that could potentially transform\nhow users interact with complex technical systems (\nLiu et al.,\n2024c; Shen et al., 2024a ). Several research initiatives have\nexplored the integration of LLMs with AutoML systems, such\nas AutoML-GPT and other LLM-driven pipelines (\nLiu et al.,\n2024b; Luo D. et al., 2024 ), demonstrating the potential for\nnatural language-based machine learning workﬂows. However,\nthese approaches have focused mainly on technical automation\nrather than on the design of human-computer interaction.\nWhile several studies have explored using LLMs for code\ngeneration and programming assistance (\nLiu et al., 2024b ; Luo\nD. et al., 2024 ), there has been limited systematic investigation\nof their eﬀectiveness in democratizing access to machine\nlearning tools (\nShen et al., 2024d ), particularly in terms of\ncomprehensive evaluation across task completion rates, eﬃciency,\nsyntax error reduction, and user-reported metrics of perceived\ncomplexity. This research addresses this critical gap by developing\nand evaluating an LLM-based AutoML framework with a\nfully conversational interface that integrates ﬁve specialized\nmodules: modality inference, feature engineering, model selection,\npipeline assembly, and hyperparameter optimization. Through a\ncomprehensive user study involving 15 participants with diverse\ntechnical backgrounds, we compare our LLM-based approach\nto conventional programming methods across common deep\nlearning tasks such as image and text classiﬁcation.\nThe major contributions are four-fold. First, we provide the\nﬁrst systematic evaluation of how LLM-based interfaces impact\nuser success rates and eﬃciency in implementing deep learning\nsolutions. Our results show that 93.34% of users achieved higher\nor comparable accuracy using our LLM-based system compared\nto traditional coding approaches, with 60% reporting signiﬁcantly\nfaster task completion times. Second, we demonstrate that natural\nlanguage interfaces can eﬀectively bridge the technical knowledge\ngap in machine learning implementation. Our study reveals\nthat users across diﬀerent expertise levels—from newcomers to\nexperienced practitioners—could successfully complete complex\ndeep learning tasks using our system, with particularly strong\nbeneﬁts for those with limited prior ML experience. Third,\nwe contribute novel insights into the design of human-AI\ninterfaces for technical tasks, identifying key factors that inﬂuence\nuser success and satisfaction when working with LLM-based\nAutoML methods. Finally, we provide empirical evidence for\nthe eﬀectiveness of natural language as a universal interface for\ncomplex technical systems, suggesting new directions for making\nadvanced technologies more accessible to broader audiences.\n/two.tnum Related works\n/two.tnum./one.tnum AutoML\nAutomated Machine Learning (AutoML) has made signiﬁcant\nstrides through algorithmic innovations such as hyperparameter\noptimization (\nMantovani et al., 2016 ; Sanders and Giraud-Carrier,\n2017), neural architecture search ( Zoph, 2016 ; Pham et al.,\n2018), and meta-learning ( Brazdil et al., 2008 ; Hutter et al.,\n2014). These methods automate critical components of the ML\npipeline, including feature engineering, model selection, and\nhyperparameter tuning, with tools like AutoGluon achieving near-\nexpert performance on standardized benchmarks. Commercial\nplatforms like Azure Machine Learning, Google Cloud AutoML,\nand H2O Driverless AI further simpliﬁed deployment workﬂows.\nHowever, these tools prioritize algorithmic eﬃciency over user-\ncentered design, meaning that a basic understanding of machine\nlearning concepts is still required for users to use these tools\neﬀectively (\nChami and Santos, 2024 ). For example, Auto-PyTorch\nreduces coding complexity through predeﬁned API templates, but\nits rigid structure forces users to adapt to system constraints rather\nthan align with natural workﬂows, leading to cognitive friction\nfor non-experts.\n/two.tnum./two.tnum LLMs\nLarge language models (LLMs) have demonstrated remarkable\nproﬁciency in generating functional code that satisﬁes speciﬁed\nrequirements (\nAustin et al., 2021 ; Allal et al., 2023 ; Chen\net al., 2021 ). Their integration into software development\nworkﬂows has not only accelerated prototyping phases but\nFrontiers in Artiﬁcial Intelligence /zero.tnum/two.tnum frontiersin.org\nYao et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/nine.tnum/zero.tnum/one.tnum/zero.tnum/five.tnum\nalso democratized programming accessibility, empowering both\nprofessional developers and non-expert users (\nKazemitabaar et al.,\n2023; Tambon et al., 2025 ). However, the current discourse\nsurrounding LLMs exhibits a critical oversight: predominant\nresearch eﬀorts focus narrowly on technical correctness and\nbenchmark performance (\nChon et al., 2024 ; Chen et al., 2024a ; Zan\net al., 2022 ), while largely neglecting the human factors inﬂuencing\nreal-world usability ( Miah and Zhu, 2024 ). This gap manifests most\nconspicuously in the limited investigation of how users across the\nexpertise spectrum—from novices struggling with basic syntax to\nexperts managing complex systems—interact with, comprehend,\nand adapt LLM-generated code.\n/two.tnum./three.tnum LLMs for AutoML\nAn important approach in integrating Large Language Models\n(LLMs) with Automated Machine Learning (AutoML) is LLM-as-\nTranslator, where natural language instructions are converted into\nAPI calls to control AutoML systems (\nTrirat et al., 2024 ; Chen et al.,\n2024b; Luo D. et al., 2024 ; Tsai et al., 2023 ; Zhang et al., 2023 ).\nThis approach allows non-technical users to interact with complex\nAutoML tools, lowering the entry barrier. However, it still has\nsigniﬁcant limitations. For instance, AutoML-GPT (\nZhang et al.,\n2023) enables users to use natural language for controlling AutoML\nprocesses, but users still need to understand domain-speciﬁc terms\nlike model selection, data preprocessing, evaluation metrics to\nuse the system eﬀectively. This creates a “circular dependency”\nproblem, as users must already know AutoML terminology before\nthey can beneﬁt from the LLM system, which contradicts the goal\nof making it accessible to non-experts. The AutoM3L framework\nproposed by\nLuo D. et al. (2024) attempts to enhance user\ninteraction with the AutoML system through LLM. While this\napproach reduces the user’s need for technical details to some\nextent, its evaluation still lacks empirical validation of whether LLM\nreduces cognitive load.\n/three.tnum Methods\nOur method focused on evaluating whether LLM based\ninterfaces can eﬀectively reduce barriers to implementing AutoML.\nWe developed and assessed a comprehensive AutoML that\nleverages natural language interaction to guide users through the\nmachine learning development process. This section details our\nsystem architecture, experimental design, evaluation metrics, and\ncontrol measures. We ﬁrst describe our prototype implementation,\nwhich combines a conversational web interface with a backend\nAutoML framework. We then present our user study design\ninvolving 15 participants with varying technical backgrounds\nwho completed standardized machine learning tasks under both\nLLM-based and traditional programming conditions. Finally, we\noutline our performance metrics and experimental controls that\nenabled comparison between these approaches while ensuring\nvalidity and reproducibility of results. Through this evaluation,\nwe aimed to quantify the impact of LLM-based interfaces\non AutoML accessibility and eﬀectiveness across diﬀerent user\nexpertise levels.\n/three.tnum./one.tnum Prototype design and implementation\nOur LLM-based AutoML prototype consists of two main\ncomponents, namely a conversational web interface and a backend\nLLM-based AutoML framework. The architecture is designed\nto minimize technical barriers while maintaining robust ML\ncapabilities. The web interface is built using Gradio (\nAbid\net al., 2019 ), an open-source Python library that enables rapid\ndevelopment of machine learning web applications. The interface\nprovides an intuitive platform where users can specify their\nML tasks through natural language descriptions. For image\nclassiﬁcation, the interface accepts standard image formats (JPEG,\nPNG) through direct upload. Text classiﬁcation tasks can be\ninitiated either through direct text input or ﬁle uploads supporting\ncommon document formats. The interface displays results in real-\ntime, presenting model predictions along with conﬁdence scores\nusing clear visualizations and explanatory text.\nThe backend AutoML framework implements AutoM3L\n(\nLuo D. et al., 2024 ) which orchestrates ﬁve specialized large\nlanguage model modules to achieve lanaguge driven AutoML,\nas shown in\nFigure 1. Speciﬁcally, the Modality Inference (MI-\nLLM) module analyzes user input to determine the appropriate\nprocessing pipeline for diﬀerent data types. The Automated\nFeature Engineering (AFE-LLM) module handles necessary\npreprocessing and feature extraction. Model Selection (MS-\nLLM) identiﬁes optimal pre-trained models for the speciﬁc task,\nwhile Pipeline Assembly (PA-LLM) constructs and validates the\ncomplete processing pipeline (\nShen et al., 2025b ,a). Finally,\nthe Hyperparameter Optimization (HPO-LLM) module ﬁne-\ntunes model parameters for optimal performance. The MS-\nLLM in AutoM3L integrates with the HuggingFace Transformers\nlibrary (version 4.28.0) to access state-of-the-art pre-trained\nmodels. Speciﬁcally, the model selection is formalized through a\nprobabilistic framework:\nMselected = arg max\nm∈ M\nP(m|t, d) (1)\nwhere M represents our curated pool of pre-trained models, t\ndenotes the user’s task description in natural language, and d\nrepresents the input data characteristics.\n/three.tnum./two.tnum User study design\nThis research aimed to evaluate whether LLM-based interfaces\ncan eﬀectively reduce barriers to use AutoML to traditional\nprogramming approaches.\nTable 1 provides precise deﬁnitions\nof each approach evaluated in our study. We investigated four\nkey hypotheses: (1) LLM interfaces can reduce the complexity of\ntraining deep learning models for beginners, (2) LLM interfaces\ncan simplify model inference tasks, (3) LLM guidance can\nimprove model selection accuracy, and (4) LLM assistance can\nhelp users better decompose complex problems. We recruited\n15 participants through university research networks and\nprofessional technology communities, targeting individuals\nwith varying levels of programming and machine learning\nexperience. Participants represented diverse technical backgrounds\nincluding students, engineers, data scientists, and educators,\nFrontiers in Artiﬁcial Intelligence /zero.tnum/three.tnum frontiersin.org\nYao et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/nine.tnum/zero.tnum/one.tnum/zero.tnum/five.tnum\nFIGURE /one.tnum\nArchitecture of the proposed LLM-based AutoML framework. The syst em consists of two main components: a conversational web interface\n(Frontend) built with Gradio for user interaction, and a backen d framework implementing ﬁve specialized LLM modules. The workﬂow begins when\nusers provide natural language instructions and data through the w eb interface. The Modality Inference LLM (MI-LLM) analyzes in put to determine\nappropriate data processing pipelines (text modality vs. image modality). The Automated Feature Engineering LLM (AFE-LLM) ha ndles data\npreprocessing including data ﬁltering and imputation. The Mod el Selection LLM (MS-LLM) identiﬁes optimal pre-trained model s from NLP and Vision\nmodel repositories based on task requirements. The Pipeline Ass embly LLM (PA-LLM) constructs executable code by integrating s elected\ncomponents. Finally, the Hyperparameter Optimization LLM (HP O-LLM) ﬁne-tunes model parameters. The integrated system outp uts trained models\nthrough automated deployment and evaluation processes. Arrows ind icate data ﬂow direction, and the dotted lines separate frontend u ser\ninteraction from backend automated processing.\nFIGURE /two.tnum\nThe ﬂow of user study design.\nenabling assessment of the system’s eﬀectiveness across diﬀerent\nuser proﬁles. The study employed a within-subjects design\ncomparing two conditions: an LLM condition utilizing our natural\nlanguage interface, and a non-LLM condition using traditional\nprogramming methods. In the LLM condition, participants\ninteracted with our web-based interface that leverages large\nlanguage models to interpret user requirements and generate\nappropriate machine learning implementations. In the non-LLM\ncondition, participants worked with a standard Jupyter notebook\nenvironment pre-conﬁgured with common AutoML library\n(i.e., AutoGluon).\nThe experimental workﬂow consisted of four primary phases\n(\nFigure 2). First, participants completed a comprehensive\nbackground survey assessing their technical expertise,\nprogramming experience, and familiarity with machine learning\nconcepts. Second, they received standardized training on both\nsystems through guided tutorials. Third, participants completed\ntwo fundamental deep learning tasks—image classiﬁcation and\nFrontiers in Artiﬁcial Intelligence /zero.tnum/four.tnum frontiersin.org\nYao et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/nine.tnum/zero.tnum/one.tnum/zero.tnum/five.tnum\nTABLE /one.tnum Experimental conditions and terminology deﬁnitions.\nApproach Deﬁnition Implementation\nLLM-AutoML Large language model-driven\nautomated machine learning\nusing natural language\ninterfaces with our integrated\nAutoM3L framework\nOur prototype system\nTraditional\nAutoML\nConventional automated\nmachine learning using\ngraphical user interfaces and\nstructured inputs without LLM\nintegration\nAutoGluon (v0.7.0)\nManual coding Hand-coded implementation of\nmachine learning pipelines\nusing programming languages\nwithout automation tools\nJupyter notebooks with\nPyTorchtext sentiment analysis—using both conditions in randomized\norder to control for learning eﬀects. Finally, participants provided\ndetailed feedback through post-task questionnaires evaluating\nsystem usability, task complexity, and overall experience.\nThroughout the experiment, we collected multiple quantitative\nand qualitative metrics. Task completion times were automatically\nrecorded, while accuracy was evaluated against predeﬁned\nbenchmarks. User interactions were monitored to understand\ncommon patterns and potential friction points. The post-task\nquestionnaires employed standardized scales to assess comparative\nusability while gathering insights into user preferences and\nchallenges. This systematic approach allowed us to evaluate how\nLLM-based interfaces impact the accessibility and eﬀectiveness of\nAutoML across diﬀerent user expertise levels.\n/three.tnum./three.tnum Performance metrics and analysis\nOur evaluation framework employed both quantitative and\nqualitative metrics to comprehensively assess the eﬀectiveness of\nLLM-based AutoML (LLM condition) compared to traditional\nprogramming approaches (non-LLM condition). The assessment\nfocused on three key dimensions: task completion eﬃciency,\nimplementation accuracy, and user experience.\nTask completion time (T) was measured automatically from the\nmoment participants began each task until successful completion:\nT = tcompletion − tstart, (2)\nwhere tcompletion represents the timestamp when the participant\nsuccessfully completed the task requirements, and tstart denotes\nthe timestamp when they began working on the task. This\nmetric provided a standardized measure of implementation\neﬃciency across both conditions. Implementation accuracy ( A)\nwas evaluated against predeﬁned benchmarks using standard\nclassiﬁcation metrics:\nA = 1\nN\nN∑\ni= 1\nI(yi = ˆ yi) (3)\nwhere N represents the total number of test cases, yi denotes the\nground truth label, and ˆyi indicates the predicted output for each\ncase. This metric assessed the correctness of model predictions\nacross both image and text classiﬁcation tasks. User experience was\nquantiﬁed through standardized post-task questionnaires using 5-\npoint Likert scales. The overall satisfaction score ( S) aggregated\nresponses across multiple dimensions including ease of use,\nperceived complexity, and execution eﬃciency:\nS = 1\nM\nM∑\nj= 1\nrj, (4)\nwhere M represents the number of evaluation criteria and rj\ndenotes the rating for each criterion.\nStatistical analysis employed paired t-tests to assess the\nsigniﬁcance of performance diﬀerences between the LLM and non-\nLLM conditions:\nt =\n¯d\nsd/ √ n (5)\nwhere ¯d represents the mean diﬀerence between paired\nobservations, sd denotes the standard deviation of diﬀerences, and\nn indicates the sample size. To address the multiple comparisons\nproblem inherent in conducting several statistical tests, we will\napply the Bonferroni correction to adjust p-values, setting our\nsigniﬁcance threshold at α = 0.05/ k, where k represents the total\nnumber of planned comparisons.\n/three.tnum./four.tnum Experimental controls and validity\nTo ensure experimental validity and reliable results, we\nimplemented control measures across participant selection, task\nexecution, and data collection. The participant recruitment process\nfollowed standardized criteria to ensure a representative sample\nof technical backgrounds while maintaining consistent group size\nand demographic distribution across expertise levels. The task\norder was randomized across participants using a balanced Latin\nsquare design to mitigate learning eﬀects. Participants received\ncondition-speciﬁc training optimized for each system’s interaction\nparadigm. For the LLM-AutoML condition, the 15-min orientation\nfocused on natural language formulation techniques, eﬀective\nprompting strategies, and conversational interaction patterns.\nFor the AutoGluon condition, training emphasized API syntax,\nparameter conﬁguration, coding workﬂows, and system-speciﬁc\nbest practices. Training materials were developed independently\nfor each condition to maximize system-speciﬁc eﬀectiveness while\nmaintaining equivalent training duration and instructor expertise.\nFor the non-LLM condition, we provided a Jupyter notebook\nenvironment pre-conﬁgured with AutoGluon (version 0.7.0) and\nessential dependencies, hosted on a dedicated server to ensure\nconsistent computing resources. The LLM condition utilized our\nweb-based interface deployed on a stable cloud infrastructure with\nconsistent response times and resource allocation.\nWe implemented strict controls for potential confounding\nvariables through several mechanisms. The datasets for both\nimage and text classiﬁcation tasks were carefully curated to\nmaintain consistent diﬃculty levels and data distribution. The\nimage classiﬁcation task utilized a subset of 1,000 images from\nFrontiers in Artiﬁcial Intelligence /zero.tnum/five.tnum frontiersin.org\nYao et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/nine.tnum/zero.tnum/one.tnum/zero.tnum/five.tnum\nthe ImageNet ( Deng et al., 2009 ) validation set, encompassing\n10 common object categories with 10 images per category. These\nimages were selected to maintain consistent resolution (224 ×\n224 pixels) and complexity levels. The text classiﬁcation task\nemployed 1,000 samples from the Stanford Sentiment Treebank\n(SST-2) dataset (\nSocher et al., 2013 ), balanced between positive and\nnegative sentiments, with consistent text length (50–200 words)\nand vocabulary complexity.\nThe computing environment speciﬁcations, including CPU,\nmemory, and network bandwidth, were standardized across all\nsessions. Task completion criteria and evaluation metrics were\nprecisely deﬁned and documented before the study commenced.\nTime management was controlled through automated session\ntracking. Each task had a maximum allocation of 30 min, with\nautomated notiﬁcations at 15-min and 25-min marks to ensure\nconsistent pacing across participants. The data collection process\nwas fully automated through integrated logging systems. For the\nnon-LLM condition, we implemented custom Jupyter Notebook\nextensions to track code execution time, error rates, and completion\nstatus. The LLM condition’s web interface incorporated built-\nin analytics that captured interaction timestamps, user inputs,\nsystem responses, and task outcomes. All performance metrics were\nautomatically stored in a centralized database with standardized\nformatting and timestamping.\n/four.tnum Experiments and results\nOur experimental evaluation assessed the eﬀectiveness of LLM-\nbased AutoML interfaces compared to traditional programming-\nbased AutoML approaches through a comprehensive user study\ninvolving 15 participants. This section presents detailed ﬁndings\nacross multiple dimensions, including task completion eﬃciency,\nimplementation accuracy, and user experience.\n/four.tnum./one.tnum Implementation details\nFor model selection and execution, we leveraged the\nHuggingFace Transformers library (version 4.28.0) to access\nstate-of-the-art pre-trained models. The image classiﬁcation\npipeline utilized ResNet-50 (\nHe et al., 2016 ) as the default\nbackbone, oﬀering robust performance across diverse visual\nrecognition tasks. Text classiﬁcation tasks employed DistilBERT\nﬁne-tuned on the Stanford Sentiment Treebank v2 (SST-2) dataset,\nproviding eﬃcient natural language processing capabilities while\nmaintaining high accuracy.\nTo ensure consistent performance across diﬀerent conditions,\nwe standardized the computing environment using Docker\ncontainers. The baseline conﬁguration included Python 3.8,\nPyTorch 1.9.0, and CUDA 11.1 for GPU acceleration. System\nresources were allocated dynamically based on task requirements,\nwith a minimum of 8GB RAM and 4 CPU cores for standard\noperations. All experiments are conducted on one NVIDIA 4090\nGPU device. For more complex tasks, the system could scale up to\nutilize additional computational resources as needed.\nError handling and recovery mechanisms were implemented at\nmultiple levels. The front end incorporated input validation and\nTABLE /two.tnum Pre-trained LLM architectures.\nModule Base\narchitecture\nModel size Deployment\nMI-LLM GPT-3.5-turbo 175B parameters Zero-shot\nAFE-LLM LLaMA-7B 7B parameters Zero-shot\nMS-LLM GPT-4 base 1.76T parameters Zero-shot\nPA-LLM LLaMA-13B 13B parameters Zero-shot\nHPO-LLM GPT-3.5-turbo 175B parameters Zero-shot\npreprocessing to catch common user errors before execution. The\nbackend implemented robust exception handling with informative\nerror messages translated into natural language.\nThe LLM-based AutoML framework operates in a zero-shot\nmanner, leveraging pretrained LLMs. This approach ensures rapid\ndeployment and broad generalizability across diverse machine\nlearning tasks.\nTable 2 presents the speciﬁc LLM architectures\nemployed for each specialized module within our framework. The\nselection of these pre-trained models was guided by performance\nbenchmarks and computational eﬃciency considerations for each\nspeciﬁc task.\n/four.tnum./two.tnum Participant demographics and\nbackground\nThe study participants represented diverse technical\nbackgrounds and experience levels spanning diﬀerent age\ngroups, with a majority (53.33%) between 18–24 years and\nthe remainder (46.67%) between 25–34 years, as shown in\nFigure 3. The gender distribution showed 60% male and 40%\nfemale participation, while educational backgrounds primarily\nconsisted of bachelor’s degree holders (66.67%) and master’s degree\nrecipients (33.33%). The professional composition included equal\ndistributions of students, engineers, and AI algorithm engineers at\n26.67% each, with data scientists, educators, and other roles each\nrepresenting 6.67% of participants. Technical expertise assessment\nrevealed that 73.33% of participants were familiar with Python\nprogramming, while 26.67% identiﬁed as beginners. Knowledge of\ndeep learning frameworks showed that 53.33% were familiar with\nHuggingFace, while 46.67% had limited exposure. On a 5-point\nscale, participants reported consistent average familiarity scores of\n3.33 across AI/deep learning, deep learning models, and PyTorch,\nwith slightly lower averages for text classiﬁcation (3.13) and image\nclassiﬁcation (3.2).\n/four.tnum./three.tnum Task completion performance\nAs depicted in\nTable 3, the comparison between LLM-\nbased and non-LLM-based AutoML conditions revealed\nsubstantial improvements across multiple performance metrics.\nImplementation accuracy showed particularly striking results,\nwith 93.34% of participants achieving superior performance\nin the LLM condition i.e., split evenly between those showing\nFrontiers in Artiﬁcial Intelligence /zero.tnum/six.tnum frontiersin.org\nYao et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/nine.tnum/zero.tnum/one.tnum/zero.tnum/five.tnum\nFIGURE /three.tnum\nParticipant demographics and technical background analysis.\nTABLE /three.tnum Comparison of task completion performance between\nLLM-based and baseline conditions.\nMetric LLM condition Baseline condition\nCompletion Rates (%)\nImage classiﬁcation 93.33 73.33\nText classiﬁcation 100.00 66.67\nAverage Completion Time (minutes)\nImage classiﬁcation 8.5 17.3\nText classiﬁcation 7.2 15.8\nRelative Accuracy (% of participants)\nSigniﬁcantly higher 46.67\nHigher 46.67\nComparable 6.67\nLower 0.00\nPerformance metrics are color-coded to facilitate interpretation: g reen indicates superior\nperformance in the LLM condition compared to the baseline; gray repres ents comparable\nor neutral outcomes. Alternating row colors (light blue and light yellow) are used to enhance\nreadability. Relative accuracy compares LLM condition performance t o baseline condition.\nCompletion times represent average duration for successful task complet ion.\nhigher (46.67%) and signiﬁcantly higher (46.67%) accuracy\ncompared to the baseline condition. The remaining 6.67%\nmaintained comparable performance levels, with no participants\nshowing degraded accuracy in the LLM condition. This\nimprovement was especially pronounced among participants\nwho reported lower initial familiarity with machine learning\nconcepts (scoring below 3 on our 5-point technical expertise\nscale), suggesting that the LLM interface eﬀectively bridges the\nexpertise gap.\nTask completion rates demonstrated marked improvements\nacross both classiﬁcation tasks. For image classiﬁcation, the LLM\ncondition achieved a 93.33% successful completion rate compared\nto 73.33% in the baseline condition. This 20% improvement was\nlargely attributed to the LLM interface’s ability to automatically\nhandle important pre-processing steps and model conﬁguration\ndetails that often create bottlenecks for users. Text classiﬁcation\nshowed even more gains, with a 100% completion rate in the\nLLM condition vs. 66.67% in the baseline condition. The perfect\ncompletion rate for text classiﬁcation suggests that the natural\nlanguage interface is particularly eﬀective for tasks involving textual\ndata, possibly due to the semantic alignment between the interface\nmodality and the task domain.\nTime eﬃciency measurements revealed compelling advantages\nfor the LLM-based condition. 60% of participants completed tasks\nsigniﬁcantly faster (deﬁned as > 50% reduction in completion\ntime), while the remaining 40% reported moderately faster\ncompletion times (25%–50% reduction). Notably, no participants\nexperienced slower performance in the LLM condition, indicating\nconsistent eﬃciency gains across all skill levels. The average\ntask completion times showed approximately 50% reduction\nacross both tasks. Image classiﬁcation tasks were completed in\n8.5 min using the LLM interface compared to 17.3 min in\nthe baseline condition, while text classiﬁcation tasks required\n7.2 min vs. 15.8 min. These time savings were particularly\nsigniﬁcant for participants with limited programming experience,\nwho often struggled with syntax and conﬁguration issues in the\nbaseline condition.\nThe superior performance achieved through our LLM-based\nframework can be attributed to three primary mechanisms that\naddress fundamental challenges in traditional ML implementation.\nFirst, the automated pipeline construction through our ﬁve\nspecialized modules eliminates decision paralysis and conﬁguration\nFrontiers in Artiﬁcial Intelligence /zero.tnum/seven.tnum frontiersin.org\nYao et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/nine.tnum/zero.tnum/one.tnum/zero.tnum/five.tnum\nerrors that commonly occur when users must manually select\nfrom hundreds of available models and preprocessing options. Our\nMS-LLM module leverages pre-trained knowledge to automatically\nidentify optimal model-task pairings, while traditional approaches\nrequire users to manually evaluate model compatibility and\nperformance characteristics. Second, the natural language\ninterface reduces implementation friction by translating user\nintentions directly into executable code, bypassing the syntax\nmastery requirement that creates barriers in conventional\nprogramming approaches. Our error analysis revealed that 78%\nof implementation failures in the baseline condition stemmed\nfrom syntax errors and parameter misconﬁgurations, issues that\nwere virtually eliminated in the LLM condition through natural\nlanguage speciﬁcation. Third, the framework’s context-aware\nguidance system provides real-time assistance and explanations,\naccelerating learning and reducing the trial-and-error cycles that\ncharacterize traditional ML development workﬂows.\nWhen analyzed by participant background, we found that\neven participants with extensive programming experience ( >5\nyears) showed substantial performance improvements in the\nLLM condition, though the magnitude of improvement was less\ndramatic than for novice users. This suggests that the LLM\ninterface provides beneﬁts not just through simpliﬁcation of\ntechnical requirements, but also through streamlining of workﬂow\nand reduction of cognitive load. The combination of improved\naccuracy, higher completion rates, and reduced completion times\nacross all user groups provides strong evidence for the eﬀectiveness\nof LLM-based interfaces in democratizing access to AutoML\ncapabilities while maintaining or enhancing performance quality.\nFinally, detailed error categorization revealed distinct patterns\nbetween conditions. In the baseline condition, syntax errors\ncomprised 45% of all failures (3.5 per session), including import\nstatement mistakes, function parameter mismatches, and tensor\ndimension errors. Conﬁguration errors accounted for 32% of\nfailures (2.5 per session), involving incorrect hyperparameter\nspeciﬁcations and model architecture misconﬁgurations. Data\npreprocessing errors represented 23% of failures (1.8 per session),\nincluding incorrect normalization procedures and batch size\ninconsistencies. In contrast, the LLM condition eliminated\nsyntax errors entirely through natural language parsing, reduced\nconﬁguration errors to 0.3 per session through automated\nparameter selection, and minimized preprocessing errors to 0.1\nper session via intelligent pipeline construction. Edge case analysis\nshowed that the LLM system successfully handled 78% of unusual\nrequests, including non-standard data formats and ambiguous\ntask descriptions, by providing clarifying questions and fallback\nsolutions. However, limitations emerged with highly specialized\nrequirements (focal loss implementation, custom augmentation\npipelines) where the system defaulted to standard alternatives\nrather than generating custom solutions.\n/four.tnum./four.tnum User experience and system evaluation\nAs demonstrated in\nTable 4, our evaluation of user experience\nrevealed compelling advantages for the LLM-based AutoML\napproach across multiple dimensions. The complexity assessment\nTABLE /four.tnum User experience and system evaluation metrics comparing\nLLM-based and baseline conditions.\nEvaluation metric LLM condition Baseline\ncondition\nPerceived Complexity (% of participants)\nSigniﬁcantly less complex 53.33 –\nModerately less complex 46.67 –\nComparable or more complex 0.00 –\nExecution Eﬃciency (% of participants)\nSigniﬁcantly higher 60.00 –\nModerately higher 40.00 –\nComparable or lower 0.00 –\nError Metrics\nSyntax errors (per session) 2.1 7.8\nError resolution time (minutes) 1.8 5.6\nLearning Curve Indicators\nRequired training time (minutes) 12.3 45.7\nTask adaptation time (minutes) 3.2 10.1\nPerformance improvements are highlighted in green to indicate supe rior LLM condition\noutcomes; gray indicates baseline measurements. The table present s aggregated feedback from\n15 participants across multiple evaluation dimensions. Error metrics repre sent average values\nacross all participant sessions.\nshowed a strong preference for the LLM interface, with 53.33%\nof participants rating it as less complex and 46.67% indicating\nmoderately reduced complexity. Notably, no participants found\nthe LLM interface more complex than the baseline, suggesting\nthat the natural language interaction model provides an inherently\nmore intuitive approach to AutoML tasks regardless of user\nexpertise level.\nPerceived execution eﬃciency metrics strongly favored the\nLLM condition, with 60% of participants reporting higher eﬃciency\nand 40% indicating moderately improved eﬃciency. This universal\nimprovement in perceived eﬃciency correlates strongly with\nour quantitative performance measurements, suggesting that\nparticipants’ subjective experience aligned well with objective\nperformance gains. The eﬃciency advantages were particularly\npronounced for participants who initially reported lower familiarity\nwith traditional AutoML tools (scoring below 3 on our 5-point\nexpertise scale).\nAnalysis of detailed participant feedback revealed several key\nmechanisms behind these improvements. The natural language\ninterface substantially reduced cognitive load for task speciﬁcation,\nwith participants reporting an average 4.5 out of 5 satisfaction\nscores for ease of expressing their intended ML tasks. This\nimprovement was attributed to the elimination of syntax\nmemorization requirements and the ability to describe tasks in\nfamiliar, natural terms. Quantitative error analysis supported these\nsubjective assessments, with the LLM condition demonstrating a\n73% reduction in syntax errors compared to the baseline condition.\nFurthermore, when errors did occur, the average resolution time\ndecreased by 68%, largely due to the system’s ability to provide\ncontext-aware suggestions and natural language error explanations.\nFrontiers in Artiﬁcial Intelligence /zero.tnum/eight.tnum frontiersin.org\nYao et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/nine.tnum/zero.tnum/one.tnum/zero.tnum/five.tnum\nThe learning curve analysis provided particularly interesting\ninsights into the system’s accessibility. Participants required an\naverage of only 12 minutes to become proﬁcient with the LLM\ninterface, compared to 45 minutes for the baseline system. This\naccelerated learning was consistent across all expertise levels,\nthough the relative improvement was most pronounced for\nparticipants without prior ML experience. The rapid adaptation\nto new tasks was evidenced by a 65% reduction in time spent\nconsulting documentation and a 78% decrease in requests for\ntechnical assistance compared to the baseline condition.\nThese comprehensive user experience ﬁndings suggest that\nLLM-based AutoML interfaces not only reduce technical barriers\nto AutoML but also fundamentally transform how users interact\nwith and learn from ML systems. The combination of reduced\ncomplexity, improved eﬃciency, and accelerated learning curves\nindicates the potential for broader democratization of ML\ntechnologies across diﬀerent user populations.\n/four.tnum./five.tnum Statistical validation\nStatistical analysis using paired t-tests conﬁrmed the\nsigniﬁcance of our ﬁndings across all major metrics, with\np < 0.001 for task completion time [ t(14) = 8.45],\nimplementation accuracy [ t(14) = 7.92], and user satisfaction\nscores [ t(14) = 9.13]. To assess practical signiﬁcance, we also\ncalculated eﬀect sizes (Cohen’s d), which were very large for\ntask completion time [ t(14) = 8.45, d = 2.18], implementation\naccuracy [ t(14) = 7.92, d = 2.05], and user satisfaction scores\n[t(14) = 9.13, d = 2.36]. After applying Bonferroni correction\nfor three primary comparisons (adjusted α = 0.0167), our\nresults remained statistically signiﬁcant for task completion time\n(p < 0.001, corrected p < 0.003), implementation accuracy ( p <\n0.001, corrected p < 0.003), and user satisfaction scores ( p <\n0.001, corrected p < 0.003). Task completion time measurements,\ncalculated using\nEquation (2) where T = tcompletion − tstart, revealed\nmean times of 7.85 minutes for LLM condition versus 16.55\nminutes for baseline. Implementation accuracy, computed using\nEquation (3) as A = 1\nN\n∑ N\ni= 1 I(yi = ˆ yi), achieved 93.34% for the\nLLM condition compared to 69.85% for baseline across N = 1,000\ntest cases. User satisfaction scores, aggregated using\nEquation (4) as\nS = 1\nM\n∑ j = 1Mrj where M = 8 evaluation criteria, demonstrated\naverage scores of 4.45 out of 5 for LLM vs. 2.18 for baseline. These\nresults align with our initial hypotheses regarding the eﬀectiveness\nof LLM-based interfaces in democratizing access to ML tools.\nDespite the overall positive results, we identiﬁed several important\nlimitations. The system occasionally showed reduced eﬀectiveness\nfor highly specialized tasks requiring custom requirements, and\nsome advanced customization options remained limited. For\ninstance, one user attempted to implement a focal loss function\nwith custom alpha and gamma parameters to address a severe class\nimbalance, but the system failed to parse the speciﬁc mathematical\nrequirements from natural language and could not generate the\ncorrect implementation. In another case, a request for an advanced\ndata augmentation pipeline—speciﬁcally, a 15-degree random\nrotation, followed by a color jitter with precise values (brightness =\n0.2, contrast = 0.3), and then a non-standard salt-and-pepper noise\ninjection—resulted in the system only applying the rotation and\ndefaulting to a simpler, generic augmentation scheme. Performance\nvariability was observed in LLM response quality, with some\ndependency on input phrasing clarity. Additionally, the framework\nshowed higher computational overhead for LLM processing and\nincreased latency for complex queries. These comprehensive\nﬁndings provide strong evidence for the eﬀectiveness of LLM-\nbased AutoML interfaces while acknowledging areas for future\nimprovement, supporting our initial research objectives of\nmaking machine learning more accessible to users across diﬀerent\nexpertise levels.\n/four.tnum./six.tnum Latency analysis\nOn average, complex user queries to the proposed LLM-\nbased system had a latency of 25–40 seconds, a stark contrast\nto the near-instantaneous execution in the baseline condition, as\nshown in\nTable 5. This increased latency makes the system less\nsuitable for highly interactive, real-time model tuning and better\nfor asynchronous tasks. The computational overhead was also\nsubstantial, requiring an additional 12GB of VRAM for the local\nLLM modules. For real-world deployment, this translates to higher\noperational costs due to API calls and a dependency on high-end\nhardware (e.g., NVIDIA 4090 class GPUs as used in our study),\nwhich could be a barrier for smaller organizations and negate\nsome of the intended accessibility beneﬁts. These comprehensive\nﬁndings provide strong evidence for the eﬀectiveness of LLM-\nbased AutoML interfaces while acknowledging areas for future\nimprovement, supporting our initial research objectives of making\nmachine learning more accessible to users across diﬀerent\nexpertise levels.\n/four.tnum./seven.tnum User-speciﬁc examples\nAnalysis of user interactions revealed distinct patterns in\nhow the natural language interface addressed expertise-speciﬁc\nchallenges. For novice users (programming experience <2\nyears), the most common traditional tool failures involved data\npreprocessing confusion and model architecture selection. For\nexample, User P7, a business analyst with limited programming\nbackground, spent 18 min in the baseline condition attempting\nTABLE /five.tnum Comparison of computational overhead and latency.\nMetric LLM-based\ncondition\nBaseline\ncondition\nAverage query latency 25–40 seconds <1 second\n(near-instantaneous)\nAdditional VRAM\noverhead\n~12 GB ~4 GB (for AutoML\nlibrary)\nHardware\ndependency\nHigh-end GPU (e.g.,\nNVIDIA 4090)\nStandard CPU/Moderate\nGPU\nDeployment\nsuitability\nAsynchronous,\nnon-real-time tasks\nInteractive and real-time\ntasks\nFrontiers in Artiﬁcial Intelligence /zero.tnum/nine.tnum frontiersin.org\nYao et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/nine.tnum/zero.tnum/one.tnum/zero.tnum/five.tnum\nto conﬁgure data loaders and image preprocessing pipelines,\nultimately failing due to tensor dimension mismatches and import\nerrors. In contrast, using our LLM interface, the same user simply\ndescribed “I want to classify product images into categories”\nand achieved successful implementation in 6 min, with the\nsystem automatically handling image resizing, normalization, and\nbatch processing. For intermediate users (2–5 years experience),\nbottlenecks typically occurred in hyperparameter optimization\nand model ﬁne-tuning. User P12, a software engineer, struggled\nwith manual hyperparameter grid search in the baseline condition,\nrequiring 25 min to achieve suboptimal results. Through\nnatural language speciﬁcation such as “optimize this model\nfor better accuracy on my small dataset, ” the LLM interface\nautomatically conﬁgured appropriate learning rates, batch sizes,\nand regularization parameters, achieving superior performance in\n8 min. Advanced users ( >5 years experience) primarily beneﬁted\nfrom reduced cognitive overhead in pipeline orchestration and\nexperiment management, with User P3 noting that natural\nlanguage descriptions eliminated the need to remember speciﬁc\nAPI calls and parameter naming conventions across diﬀerent\nML libraries.\n/five.tnum Discussion\nThis research presents compelling evidence for the\ntransformative potential of LLM-based interfaces in democratizing\naccess to machine learning technologies. Through evaluations\ninvolving 15 participants across diverse technical backgrounds, we\ndemonstrated that natural language interactions can signiﬁcantly\nreduce implementation barriers while maintaining or improving\ntask performance. The substantial improvements in completion\nrates (93.33% for image classiﬁcation and 100% for text\nclassiﬁcation) and eﬃciency (approximately 50% reduction\nin task completion times) validate the eﬀectiveness of our approach\nin simplifying complex ML workﬂows. Our participant distribution\nincluded 73.33% Python-proﬁcient users, which may overestimate\nthe framework’s eﬀectiveness for truly non-technical populations.\nThis overrepresentation of technically skilled participants could\nbias our results toward more positive outcomes, as these users are\ninherently more adaptable to technical tools, including traditional\nAutoML approaches. While our ﬁndings show beneﬁts for users\nat all surveyed expertise levels, the limited representation of\nnon-technical users (26.67%) in our sample warrants cautious\ninterpretation of these beneﬁts for truly democratizing ML\naccess to non-expert populations. Future work will prioritize\naddressing this limitation by conducting larger-scale studies\nwith a participant pool deliberately recruited from non-technical\ndomains. We plan to collaborate with professionals in ﬁelds such\nas business analytics, healthcare, and education—who possess\nsigniﬁcant domain expertise but may lack formal programming\nbackgrounds—to more rigorously assess the framework’s potential\nfor genuine democratization.\nAdditionally, several other important challenges remain\nto be addressed in future work. The LLM-based AutoML\nframework’s occasional limitations with highly specialized\ntasks and advanced customization options indicate the need\nfor more sophisticated natural language understanding and\ndomain-speciﬁc knowledge integration. Additionally, the observed\nvariability in LLM response quality and computational overhead\npresents opportunities for optimization through improved\nprompt engineering and eﬃcient model deployment strategies.\nFuture research directions should explore the scalability of this\napproach across broader ML applications, including more complex\ntasks such as neural architecture search and automated feature\nengineering. Investigation into hybrid interfaces that combine\nnatural language interaction with traditional programming tools\ncould potentially address current limitations while maintaining\naccessibility. Additionally, longitudinal studies examining the\nlong-term impact on user skill development and ML adoption\nrates would provide valuable insights for the continued evolution\nof AutoML systems.\nThis work contributes to the growing body of evidence\nsupporting the role of LLMs in bridging technical gaps and\ndemocratizing access to advanced technologies. As ML continues\nto permeate various sectors, the development of intuitive, eﬀective\ninterfaces becomes increasingly important. Our ﬁndings suggest\nthat LLM-based approaches oﬀer a promising path forward in\nmaking sophisticated ML capabilities accessible to a broader\naudience while maintaining high standards of performance\nand reliability.\n/six.tnum Conclusion\nThis research advances the ﬁeld of human-AI interaction\nby demonstrating how LLM-based interfaces can fundamentally\ntransform the accessibility of machine learning technologies.\nThrough empirical evaluation, we established that natural language\ninterfaces not only simplify ML implementation but also enhance\nthe quality and eﬃciency of outcomes across diverse user groups.\nThe improvements in task completion and dramatic reductions in\nlearning barriers suggest a paradigm shift in how users can interact\nwith sophisticated ML systems. Our ﬁndings have important\nimplications for both research and practice in AI democratization.\nThe successful integration of LLMs with AutoML frameworks\nopens new possibilities for domain experts to leverage ML\ncapabilities without extensive technical training. This breakthrough\ncould accelerate the adoption of ML solutions across sectors\nwhere technical expertise has traditionally been a limiting factor,\nfrom healthcare and scientiﬁc research to business analytics\nand education. Looking forward, this work sets the foundation\nfor several promising research directions. Future investigations\ncould explore the extension of LLM-based interfaces to more\ncomplex ML workﬂows, including automated neural architecture\ndesign and multi-modal learning tasks. Additionally, research\ninto hybrid interfaces that combine natural language interaction\nwith traditional programming tools could further enhance the\nﬂexibility and power of AutoML systems while maintaining\ntheir accessibility.\nData availability statement\nThe datasets presented in this article are not readily available\nbecause they contain participant interaction data and personal\nFrontiers in Artiﬁcial Intelligence /one.tnum/zero.tnum frontiersin.org\nYao et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/nine.tnum/zero.tnum/one.tnum/zero.tnum/five.tnum\ninformation collected during the user study that cannot be shared\ndue to privacy and conﬁdentiality restrictions. The underlying\npublic datasets used in this study (ImageNet validation set and\nStanford Sentiment Treebank SST-2) are already publicly available\nthrough their respective sources. The experimental framework and\nmethodology details are provided in the manuscript to enable\nreplication. Requests to access aggregated or anonymized data\nshould be directed to\nyaojiapeng1983@163.com.\nEthics statement\nThe studies involving humans were approved by School\nof Data Science and Artiﬁcial Intelligence. The studies were\nconducted in accordance with the local legislation and institutional\nrequirements. The ethics committee/institutional review board\nwaived the requirement of written informed consent for\nparticipation from the participants or the participants’ legal\nguardians/next of kin because verbal consent for participation\nwas obtained.\nAuthor contributions\nJY: Writing – original draft, Writing – review & editing. LZ:\nWriting – original draft, Writing – review & editing. JH: Writing –\noriginal draft, Writing – review & editing.\nFunding\nThe author(s) declare that no ﬁnancial support was received for\nthe research and/or publication of this article.\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial or ﬁnancial relationships that could be\nconstrued as a potential conﬂict of interest.\nGenerative AI statement\nThe author(s) declare that no Gen AI was used in the creation\nof this manuscript.\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their aﬃliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nReferences\nAbid, A., Abdalla, A., Abid, A., Khan, D., Alfozan, A., and Zou, J . (2019).\nGradio: Hassle-free sharing and testing of ml models in the wild. arXiv preprint\narXiv:1906.02569.\nAllal, L. B., Li, R., Kocetkov, D., Mou, C., Akiki, C., Ferrandis , C. M.,\net al. (2023). Santacoder: don’t reach for the stars! arXiv preprint arXiv:2301.\n03988.\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Do han, D., et al.\n(2021). Program synthesis with large language models. arXiv preprint arXiv:2108.\n07732.\nBaratchi, M., Wang, C., Limmer, S., van Rijn, J. N., Hoos, H., Bäck, T., et al. (2024).\nAutomated machine learning: past, present and future. Artif. Intell. Rev . 57, 1–88.\ndoi: 10.1007/s10462-024-10726-1\nBrazdil, P., Carrier, C. G., Soares, C., and Vilalta, R. (2008). Metalearning:\nApplications to Data Mining . Cham: Springer Science Business Media.\ndoi: 10.1007/978-3-540-73263-1\nChami, J. C., and Santos, V. (2024). Collaborative automated m achine\nlearning (automl) process framework. Edelweiss Appl. Sci. Technol . 8, 7675–7685.\ndoi: 10.55214/25768484.v8i6.3676\nChen, L., Guo, Q., Jia, H., Zeng, Z., Wang, X., Xu, Y., et al. (202 4a). A\nsurvey on evaluating large language models in code generation ta sks. arXiv preprint\narXiv:2408.16498.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. D. O., Kapla n, J., et al. (2021).\nEvaluating large language models trained on code. arXiv preprint arXiv:2107.03374 .\nChen, S., Zhai, W., Chai, C., and Shi, X. (2024b). “Llm2automl: zero-\ncode automl framework leveraging large language models, ” in 2024 International\nConference on Intelligent Robotics and Automatic Control (IRAC) (IEEE), 285–290.\ndoi: 10.1109/IRAC63143.2024.10871761\nChon, H., Lee, S., Yeo, J., and Lee, D. (2024). Is functional c orrectness enough to\nevaluate code language models? Exploring diversity of generated codes. arXiv preprint\narXiv:2408.14504.\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei , L. (2009). “Imagenet: a\nlarge-scale hierarchical image database, ” in 2009 IEEE Conference on Computer Vision\nand Pattern Recognition (IEEE), 248–255. doi: 10.1109/CVPR.2009.5206848\nErickson, N., Mueller, J., Shirkov, A., Zhang, H., Larroy, P., Li, M., et al. (2020).\nAutogluon-tabular: Robust and accurate automl for structure d data. arXiv preprint\narXiv:2003.06505.\nFeurer, M., Klein, A., Eggensperger, K., Springenberg, J., Blum , M., and Hutter,\nF. (2015). “Eﬃcient and robust automated machine learning, ” in Advances in Neural\nInformation Processing Systems , 28.\nHe, K., Zhang, X., Ren, S., and Sun, J. (2016). “Deep residual\nlearning for image recognition, ” in Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition , 770–778. doi: 10.1109/CVPR.2\n016.90\nHutter, F., Hoos, H., and Leyton-Brown, K. (2014). “An eﬃcie nt approach\nfor assessing hyperparameter importance, ” in International Conference on Machine\nLearning (PMLR), 754–762.\nHutter, F., Kotthoﬀ, L., and Vanschoren, J. (2019). Automated Machine\nLearning: Methods, Systems, Challenges . New York: Springer Nature.\ndoi: 10.1007/978-3-030-05318-5\nJin, H., Song, Q., and Hu, X. (2019). “Auto-keras: an eﬃcient neural architecture\nsearch system, ” in Proceedings of the 25th ACM SIGKDD International Conference on\nKnowledge Discovery Data Mining , 1946–1956. doi: 10.1145/3292500.3330648\nKazemitabaar, M., Chow, J., Ma, C. K. T., Ericson, B. J., Wein trop, D., and\nGrossman, T. (2023). “Studying the eﬀect of AI code generator s on supporting novice\nlearners in introductory programming, ” in Proceedings of the 2023 CHI Conference on\nHuman Factors in Computing Systems , 1–23. doi: 10.1145/3544548.3580919\nKe, J., Liu, K., Sun, Y., Xue, Y., Huang, J., Lu, Y., et al. (2023 a). Artifact detection\nand restoration in histology images with stain-style and stru ctural preservation. IEEE\nTrans. Med. Imaging 42, 3487–3500. doi: 10.1109/TMI.2023.3288940\nKe, J., Shen, Y., Guo, Y., Wright, J. D., Jing, N., and Liang, X. (2020a). “A\nhigh-throughput tumor location system with deep learning for c olorectal cancer\nhistopathology image, ” inInternational Conference on Artiﬁcial Intelligence in Medicine\n(Springer), 260–269. doi: 10.1007/978-3-030-59137-3_24\nKe, J., Shen, Y., Guo, Y., Wright, J. D., and Liang, X. (2020b). “A prediction\nmodel of microsatellite status from histology images, ” in Proceedings of the 2020\n10th International Conference on Biomedical Engineering and Technolog y, 334–338.\ndoi: 10.1145/3397391.3397442\nFrontiers in Artiﬁcial Intelligence /one.tnum/one.tnum frontiersin.org\nYao et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/nine.tnum/zero.tnum/one.tnum/zero.tnum/five.tnum\nKe, J., Shen, Y., Lu, Y., Guo, Y., and Shen, D. (2023b). Mine loca l\nhomogeneous representation by interaction information clus tering with unsupervised\nlearning in histopathology images. Comput. Methods Progr. Biomed . 235:107520.\ndoi: 10.1016/j.cmpb.2023.107520\nLeDell, E., and Poirier, S. (2020). “H2o automl: Scalable automat ic machine\nlearning, ” in Proceedings of the AutoML Workshop at ICML (San Diego, CA, USA:\nICML).\nLiu, X., Zhou, T., Wang, C., Wang, Y., Wang, Y., Cao, Q., et al. ( 2024a). Toward the\nuniﬁcation of generative and discriminative visual founda tion model: a survey. Visual\nComput. 2024, 1–42. doi: 10.1007/s00371-024-03608-8\nLiu, Y., Chen, Z., Wang, Y. G., and Shen, Y. (2024b). Autoprotei nengine: a large\nlanguage model driven agent framework for multimodal automl i n protein engineering.\narXiv preprint arXiv:2411.04440 .\nLiu, Y., Chen, Z., Wang, Y. G., and Shen, Y. (2024c). “Toursynb io-search: a\nlarge language model driven agent framework for uniﬁed search method for protein\nengineering, ” in2024 IEEE International Conference on Bioinformatics and Biomedici ne\n(BIBM) (IEEE), 5395–5400. doi: 10.1109/BIBM62325.2024.1082231 8\nLuo, D., Feng, C., Nong, Y., and Shen, Y. (2024). “Autom3l: an a utomated\nmultimodal machine learning framework with large language mode ls, ” in\nProceedings of the 32nd ACM International Conference on Multimedia , 8586–8594.\ndoi: 10.1145/3664647.3680665\nLuo, S., Feng, J., Shen, Y., and Ma, Q. (2024). “Learning to pre dict\nthe optimal template in stain normalization for histology image a nalysis, ” in\nInternational Conference on Artiﬁcial Intelligence in Medicine (Springer), 95–103.\ndoi: 10.1007/978-3-031-66535-6_11\nMantovani, R. G., Horváth, T., Cerri, R., Vanschoren, J., and De Carvalho,\nA. C. (2016). “Hyper-parameter tuning of a decision tree induc tion algorithm, ”\nin 2016 5th Brazilian Conference on Intelligent Systems (BRACIS) (IEEE), 37–42.\ndoi: 10.1109/BRACIS.2016.018\nMiah, T., and Zhu, H. (2024). “User centric evaluation of code generation tools, ” in\n2024 IEEE International Conference on Artiﬁcial Intelligence Testin g (AITest) (IEEE),\n109–119. doi: 10.1109/AITest62860.2024.00022\nOlson, R. S., and Moore, J. H. (2016). “Tpot: a tree-based pipeline optimization\ntool for automating machine learning, ” in Workshop on Automatic Machine Learning\n(PMLR), 66–74.\nPatibandla, R. L., Srinivas, V. S., Mohanty, S. N., and Pattan aik, C. R. (2021).\n“Automatic machine learning: an exploratory review, ” in 2021 9th International\nConference on Reliability, Infocom Technologies and Optimization (Tr ends and Future\nDirections) (ICRITO) (IEEE), 1–9. doi: 10.1109/ICRITO51393.2021.9596483\nPham, H., Guan, M., Zoph, B., Le, Q., and Dean, J. (2018). “Eﬃcie nt neural\narchitecture search via parameters sharing, ” in International Conference on Machine\nLearning (PMLR), 4095–4104.\nSanders, S., and Giraud-Carrier, C. (2017). “Informing the u se of hyperparameter\noptimization through metalearning, ” in 2017 IEEE International Conference on Data\nMining (ICDM) (IEEE), 1051–1056. doi: 10.1109/ICDM.2017.137\nShen, Y. (2024). “Knowledgeie: unifying online-oﬄine distilla tion based on\nknowledge inheritance and evolution, ” in2024 International Joint Conference on Neural\nNetworks (IJCNN) (IEEE), 1–8. doi: 10.1109/IJCNN60899.2024.10650086\nShen, Y., Chen, Z., Mamalakis, M., Liu, Y., Li, T., Su, Y., et al. (2024a). “Toursynbio:\na multi-modal large model and agent framework to bridge text an d protein sequences\nfor protein engineering, ” in 2024 IEEE International Conference on Bioinformatics and\nBiomedicine (BIBM) (IEEE), 2382–2389. doi: 10.1109/BIBM62325.2024.1082269 5\nShen, Y., Guo, P., Wu, J., Huang, Q., Le, N., Zhou, J., et al. (202 3). “Movit:\nmemorizing vision transformers for medical image analysis, ” in International\nWorkshop on Machine Learning in Medical Imaging (Springer), 205–213.\ndoi: 10.1007/978-3-031-45676-3_21\nShen, Y., He, G., and Unberath, M. (2024b). “Promptable counter factual\ndiﬀusion model for uniﬁed brain tumor segmentation and gene ration with mris, ”\nin International Workshop on Foundation Models for General Medical AI (Springer),\n81–90. doi: 10.1007/978-3-031-73471-7_9\nShen, Y., Li, C., Liu, B., Li, C.-Y., Porras, T., and Unberath , M. (2025a). Operating\nroom workﬂow analysis via reasoning segmentation over digita l twins. arXiv preprint\narXiv:2503.21054.\nShen, Y., Li, J., Shao, X., Inigo Romillo, B., Jindal, A., Dreizi n, D., et al.\n(2024c). “Fastsam3d: an eﬃcient segment anything model for 3D volumetric\nmedical images, ” in International Conference on Medical Image Computing and\nComputer-Assisted Intervention (Springer), 542–552. doi: 10.1007/978-3-031-7239\n0-2_51\nShen, Y., Liu, B., Li, C., Seenivasan, L., and Unberath, M. (2 025b). Online reasoning\nvideo segmentation with just-in-time digital twins. arXiv preprint arXiv:2503.21056 .\nShen, Y., Lv, O., Zhu, H., and Wang, Y. G. (2024d). “Proteineng ine: empower\nLLM with domain knowledge for protein engineering, ” in International\nConference on Artiﬁcial Intelligence in Medicine (Springer), 373–383.\ndoi: 10.1007/978-3-031-66538-7_37\nShen, Y., Xu, L., Yang, Y., Li, Y., and Guo, Y. (2022). “Self-dis tillation\nfrom the last mini-batch for consistency regularization, ” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition , 11943–11952.\ndoi: 10.1109/CVPR52688.2022.01164\nSocher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A. Y., et al. (2013).\n“Recursive deep models for semantic compositionality over a se ntiment treebank, ”\nin Proceedings of the 2013 Conference on Empirical Methods in Natural Langua ge\nProcessing, 1631–1642. doi: 10.18653/v1/D13-1170\nSun, Y., Song, Q., Gui, X., Ma, F., and Wang, T. (2023). “Automl in the\nwild: obstacles, workarounds, and expectations, ” in Proceedings of the 2023 CHI\nConference on Human Factors in Computing Systems , 1–15. doi: 10.1145/3544548.358\n1082\nTambon, F., Moradi-Dakhel, A., Nikanjam, A., Khomh, F., Desm arais, M.\nC., and Antoniol, G. (2025). Bugs in large language models generat ed code:\nan empirical study. Empir. Softw. Eng . 30, 1–48. doi: 10.1007/s10664-025-10\n614-4\nTrirat, P., Jeong, W., and Hwang, S. J. (2024). Automl-agent: a multi-agent llm\nframework for full-pipeline automl. arXiv preprint arXiv:2410.02958 .\nTsai, Y.-D., Tsai, Y.-C., Huang, B.-W., Yang, C.-P., and Lin , S.-D. (2023). Automl-\nGPT: large language model for automl. arXiv preprint arXiv:2309.01125 .\nWang, C., He, Z., He, J., Ye, J., and Shen, Y. (2024). “Histolog y image\nartifact restoration with lightweight transformer based di ﬀusion model, ” in\nInternational Conference on Artiﬁcial Intelligence in Medicine (Springer), 81–89.\ndoi: 10.1007/978-3-031-66535-6_9\nWang, L., and Shen, Y. (2024). Evaluating causal reasoning ca pabilities of large\nlanguage models: a systematic analysis across three scenarios . Electronics 13:4584.\ndoi: 10.3390/electronics13234584\nWen, Y., Wang, Y., Yi, K., Ke, J., and Shen, Y. (2024). “Diﬃmput e:\ntabular data imputation with denoising diﬀusion probabilistic model, ” in 2024\nIEEE International Conference on Multimedia and Expo (ICME) (IEEE), 1–6.\ndoi: 10.1109/ICME57554.2024.10687685\nZan, D., Chen, B., Zhang, F., Lu, D., Wu, B., Guan, B., et al. (202 2).\nLarge language models meet nl2code: a survey. arXiv preprint arXiv:2212.0\n9420.\nZhang, S., Gong, C., Wu, L., Liu, X., and Zhou, M. (2023). Autom l-gpt: Automatic\nmachine learning with gpt. arXiv preprint arXiv:2305.02499 .\nZimmer, L., Lindauer, M., and Hutter, F. (2021). Auto-pytorc h: multi-ﬁdelity\nmetalearning for eﬃcient and robust autodl. IEEE Trans. Pattern Anal. Mach. Intell .\n43, 3079–3090. doi: 10.1109/TPAMI.2021.3067763\nZoph, B. (2016). Neural architecture search with reinforcem ent learning. arXiv\npreprint arXiv:1611.01578.\nFrontiers in Artiﬁcial Intelligence /one.tnum/two.tnum frontiersin.org",
  "topic": "Leverage (statistics)",
  "concepts": [
    {
      "name": "Leverage (statistics)",
      "score": 0.7502624988555908
    },
    {
      "name": "Baseline (sea)",
      "score": 0.680050253868103
    },
    {
      "name": "Computer science",
      "score": 0.6507592797279358
    },
    {
      "name": "Perspective (graphical)",
      "score": 0.5153917074203491
    },
    {
      "name": "Artificial intelligence",
      "score": 0.40651702880859375
    },
    {
      "name": "Knowledge management",
      "score": 0.3834232687950134
    },
    {
      "name": "Geology",
      "score": 0.0
    },
    {
      "name": "Oceanography",
      "score": 0.0
    }
  ]
}