{
  "title": "Arabic Image Captioning using Pre-training of Deep Bidirectional Transformers",
  "url": "https://openalex.org/W4389009503",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5020839760",
      "name": "Jonathan Emami",
      "affiliations": [
        "Lund University"
      ]
    },
    {
      "id": "https://openalex.org/A1977224323",
      "name": "Pierre Nugues",
      "affiliations": [
        "Lund University"
      ]
    },
    {
      "id": "https://openalex.org/A2006921111",
      "name": "Ashraf Elnagar",
      "affiliations": [
        "University of Sharjah"
      ]
    },
    {
      "id": "https://openalex.org/A578575225",
      "name": "Imad Afyouni",
      "affiliations": [
        "University of Sharjah"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2302086703",
    "https://openalex.org/W3173220247",
    "https://openalex.org/W68733909",
    "https://openalex.org/W3084524316",
    "https://openalex.org/W3091588028",
    "https://openalex.org/W3196915641",
    "https://openalex.org/W1956340063",
    "https://openalex.org/W2549139847",
    "https://openalex.org/W2123301721",
    "https://openalex.org/W3008110149",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3113708896",
    "https://openalex.org/W3106433641",
    "https://openalex.org/W2809904748",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2604380351",
    "https://openalex.org/W2997591391",
    "https://openalex.org/W2481240925",
    "https://openalex.org/W2805430858",
    "https://openalex.org/W3116641301",
    "https://openalex.org/W4307868333",
    "https://openalex.org/W2506483933",
    "https://openalex.org/W3038033387",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W3011608779",
    "https://openalex.org/W2986670728",
    "https://openalex.org/W4312922092",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W1514535095",
    "https://openalex.org/W2154652894"
  ],
  "abstract": "Image captioning is the process of automatically generating a textual description of an image.It has a wide range of applications, such as effective image search, auto archiving and even helping visually impaired people to see.English image captioning has seen a lot of development lately, while Arabic image captioning is lagging behind.In this work, we developed and evaluated several Arabic image captioning models with well-established metrics on a public image captioning benchmark.We initialized all models with transformers pre-trained on different Arabic corpora.After initialization, we fine-tuned them with image-caption pairs using a learning method called OSCAR.OSCAR uses object tags detected in images as anchor points to significantly ease the learning of image-text semantic alignments.In relation to the image captioning benchmark, our best performing model scored 0.39, 0.25, 0.15 and 0.092 with BLEU-1,2,3,4 respectively 1 , an improvement over previously published scores of 0.33, 0.19, 0.11 and 0.057.Beside additional evaluation metrics, we complemented our scores with human evaluation on a sample of our output.Our experiments showed that training image captioning models with Arabic captions and English object tags is a working approach, but that a pure Arabic dataset, with Arabic object tags, would be preferable.",
  "full_text": "Proceedings of the 15th International Conference on Natural Language Generation, pages 40 - 51\nJuly 18-22, 2022c⃝2022 Association for Computational Linguistics\nArabic Image Captioning using Pre-training of Deep Bidirectional\nTransformers\nJonathan Emami\nLund University\njontooy@gmail.com\nPierre Nugues\nLund University\npierre.nugues@cs.lth.se\nAshraf Elnagar\nUniversity of Sharjah\nashraf@sharjah.ac.ae\nImad Afyouni\nUniversity of Sharjah\niafyouni@sharjah.ac.ae\nAbstract\nImage captioning is the process of automati-\ncally generating a textual description of an im-\nage. It has a wide range of applications, such as\neffective image search, auto archiving and even\nhelping visually impaired people to see. En-\nglish image captioning has seen a lot of devel-\nopment lately, while Arabic image captioning\nis lagging behind. In this work, we developed\nand evaluated several Arabic image caption-\ning models with well-established metrics on a\npublic image captioning benchmark. We initial-\nized all models with transformers pre-trained\non different Arabic corpora. After initializa-\ntion, we fine-tuned them with image-caption\npairs using a learning method called OSCAR.\nOSCAR uses object tags detected in images as\nanchor points to significantly ease the learning\nof image-text semantic alignments. In relation\nto the image captioning benchmark, our best\nperforming model scored 0.39, 0.25, 0.15 and\n0.092 with BLEU-1,2,3,4 respectively1, an im-\nprovement over previously published scores\nof 0.33, 0.19, 0.11 and 0.057. Beside addi-\ntional evaluation metrics, we complemented\nour scores with human evaluation on a sample\nof our output. Our experiments showed that\ntraining image captioning models with Arabic\ncaptions and English object tags is a working\napproach, but that a pure Arabic dataset, with\nArabic object tags, would be preferable.\n1 Introduction\nThe amount of available digital images has in-\ncreased enormously and captions help us under-\nstand and interpret them. While manual captioning\nis a tedious task, automatic image captioning uses\nalgorithms to extract meaningful information about\nthe content of an image and generate a human-\nreadable sentence from this information.\nState-of-the-art automatic image captioning net-\nworks are today trained on English corpora. For\n1https://github.com/jontooy/Arabic-Image-Captioning-\nusing-Transformers\nthe other languages, the resulting captions could\nbe translated using a neural machine translation\n(NMT) model. This procedure, however, intro-\nduces an additional source of errors. For Arabic,\nElJundi et al. (2020) argued for the necessity of\nan end-to-end image captioning system that would\nattenuate errors coming from the unique sentence\nstructure and complex morphology of the Arabic\nlanguage.\nAttai and Elnagar (2020), in a survey on the\ncurrent state of Arabic image captioning systems,\nconclude that research conducted for Arabic image\ncaptioning is very scarce and that it can mainly be\nattributed to the lack of publicly available datasets.\nThey also stress that few Arabic image captioning\nresearch projects utilized attention mechanisms to\nfocus on the important parts of the image. Such at-\ntention mechanisms shall contribute to the caption\ngeneration process and give better results.\nIn their survey, Attai and Elnagar did not men-\ntion the transformer architecture as proposed by\nVaswani et al. (2017), which is solely based on\nattention mechanisms. Moreover, transformers in\nnatural language models are gaining more popu-\nlarity as these models create new state-of-the-art\nresults on different benchmarks, including the OS-\nCAR English image captioning model (Li et al.,\n2020). This system uses object tags detected in\nimages as anchor points to significantly ease the\nlearning of image-text semantic alignments.\nTo the best of our knowledge, no transformer-\nbased model for Arabic image captioning had been\nput to the test. In this paper, we describe an ap-\nproach to switch the language models of OSCAR\nwith pre-trained Arabic and multilingual ones, then\ntrain them on public Arabic benchmark datasets.\nThe main contributions of this work can be sum-\nmarized as follows: ( i) We evaluate transformer-\nbased Arabic image captioning and compare our\nresults to previous ones. (ii) In relation to the public\nimage captioning benchmark, one of our best per-\n40\nforming models scored 0.39, 0.25, 0.15 and 0.092\nwith BLEU-1,2,3,4 respectively, an improvement\nover previously published scores of 0.33, 0.19, 0.11\nand 0.057. (iii) We show that training image cap-\ntioning models with Arabic captions and English\nobject tags is a working approach, but that a pure\nArabic dataset, with Arabic object tags, is prefer-\nable.\n2 Related Work\nIn this section, we summarize recent developments\nin English image captioning and comment on the\ncurrent state of Arabic image captioning.\n2.1 English Image Captioning\nAttention is a technique in neural networks that\nmimics cognitive attention, and has shown great\nsuccess in image captioning models ever since Xu\net al. (2015) introduced an attention-based model\nthat automatically learns to describe the contents\nof images. You et al. (2016) developed an algo-\nrithm that learns to selectively attend to semantic\nconcept candidates and combine them with hid-\nden states and outputs of recurrent neural networks.\nHuang et al. (2019) take the attention concept one\nstep further in their work, where they propose an\n“Attention on Attention” (AoA) module, which ex-\ntends the conventional attention mechanisms to\ndetermine the relevance between attention results\nand queries.\nState-of-the-art image captioning today is based\non transformers, an architecture that builds solely\non attention mechanisms. Zhou et al. (2019) pre-\nsented a unified vision-language pre-training (VLP)\nmodel which can be fine-tuned for both image cap-\ntioning and visual question answering (VQA) tasks.\nLi et al. (2020) presented a new learning method\nOSCAR (Object-Semantics Aligned Pre-training),\nand showed that learning of cross-modal represen-\ntations can be significantly improved by introduc-\ning object tags detected in images. These object\ntags are used as “anchor points” during training to\nease the learning of semantic alignments between\nimages and texts. Zhang et al. (2021) studied im-\nproved visual representations, dubbed VinVL, and\nutilized an upgraded approach, dubbed OSCAR+,\nto pre-train transformer-based VL fusion models.\nThey then fine-tuned the models on various VL\nbenchmarks and created new state-of-the-art re-\nsults on seven public benchmarks, including image\ncaptioning on the COCO Caption benchmark (see\nSection 3.1). VinVL has since its release been sur-\npassed by other VLP models, for example LEMON\n(LargE-scale iMage captiONer) (Hu et al., 2021)\nwhich studies the scaling behavior of VLP for im-\nage captioning.\nBy the time of this work, VinVL was the state of\nthe art and in this paper, we utilized OSCAR with\nVinVL on Arabic image captioning.\n2.2 Arabic Image Captioning\nArabic image captioning (AIC) introduces addi-\ntional challenges compared to English captioning.\nIn a survey on the state of AIC, Attai and Elnagar\n(2020) conclude that research conducted for Ara-\nbic image captioning is very scarce and that it can\nmainly be attributed to the lack of publicly avail-\nable datasets. The Arabic language is also known\nfor its morphological complexity, and a variety of\ndialects, which makes it harder to process.\nJindal leveraged the heavy influence of root\nwords to generate captions of an image directly\nin Arabic using root word based recurrent neural\nnetworks (Jindal, 2017, 2018). They also reported\nthe first BLEU score for direct Arabic caption gen-\neration, from experimental results on datasets from\nvarious Middle Eastern newspaper websites and\nthe Flickr8k dataset (see Section 3.2).\nAl-muzaini et al. (2018) developed a generative\nmerge model for Arabic image captioning based\non a deep RNN-LSTM and a CNN model. They\nused crowd sourcing to translate samples from two\nimage captioning benchmarks: MS COCO and\nthe Flickr8k dataset. They used a relatively small\ntraining set (2400 images) from an unpublished\ndataset. To reduce the risk of overfitting, ElJundi\net al. (2020) developed an annotated dataset for\nArabic image captioning (Flickr8k), which, as of\ntoday, remains the only public benchmark for AIC.\nThey also developed a base model for AIC that\nrelies on text translation from English image cap-\ntions and compared it to an end-to-end model that\ndirectly transcribes images into Arabic text.\nNone of the works mentioned above utilized at-\ntention mechanisms in their proposed models. Afy-\nouni et al. (2021) developed a hybrid object-based,\nattention-driven image captioning model. They per-\nformed a comprehensive set of experiments using\npopular metrics and multilingual semantic sentence\nsimilarity techniques to assess the lexical and se-\nmantic accuracy of generated captions.\nOut of all the works from above, only ElJundi\n41\net al. (2020) have made their dataset publicly avail-\nable, and is therefore the only work we can directly\ncompare our models with.\nWhen finishing this work, we discovered a Mas-\nter’s thesis contemporaneous to our work by Sabri\n(2021). Though not a refereed publication, the\nauthor built neural network architectures which in-\nclude techniques not previously explored in the\nArabic image captioning literature, such as trans-\nformers. This approach yielded better results over\nthe benchmark published by ElJundi et al. (2020).\n3 Datasets\nFor this work, we mainly used two public datasets\nfor image captioning: Microsoft COCO and\nFlickr8k. We describe them in detail now.\n3.1 Microsoft COCO\nMicrosoft Common Objects in Context (COCO)\n(Lin et al., 2014) is a dataset consisting of 123,287\nimages including object detection, segmentation,\nand five captions per image (616,435 captions in\ntotal). As its name suggests, the COCO dataset\ncontains complex everyday scenes with common\nobjects in their natural context.\nFor comparison, we adopted the widely used\nKarpathy split of COCO (Karpathy and Fei-Fei,\n2015), i.e. 113,287 train images, 5,000 validation\nimages and 5,000 test images. We used 414,113\npre-translated captions over 82,783 training im-\nages with the Advanced Google Translate API 2,\ndubbed Arabic-COCO. Figure 1a shows an exam-\nple of an image from the train split with its five\nEnglish captions and five Arabic captions. For the\nArabic speaking reader, note the error in the sec-\nond machine translated caption, where the phrase\nh. @ñÓ\rB@ H. ñ» P“ride a wave”, should be replaced\nwith its present tense h. ñÖÏ@ I. » QK\n “riding a wave”.\nSabri (2021) showed that, out of a random sam-\npled subset of 150 captions from Arabic-COCO,\n46% of the translations were unintelligible. Based\non this finding, we considered the captions to be\nnoisy, which is why we did not create a validation\nand testing set out of Arabic-COCO.\n3.2 Flickr8k\nThe Flickr8k dataset (Hodosh et al., 2013) consists\nof 8,092 images. Each image in this dataset is\nassociated with five different captions that describe\n2https://github.com/canesee-project/Arabic-COCO\nthe entities and events depicted in the image. They\nwere collected via a crowdsourcing marketplace\n(Amazon Mechanical Turk) with a total of 40,460\ncaptions.\nHuman translations into Arabic of both the\nCOCO and Flickr8k datasets have been done be-\nfore. For example, Al-muzaini et al. (2018) built an\nArabic dataset based on these two English bench-\nmark datasets. Most of them are not public, there-\nfore we used Arabic Flickr8k by ElJundi et al.\n(2020). Arabic Flickr8k is split into 6,000 train\nimages, 1,000 validation images, and 1,000 test\nimages, all with three Arabic captions each.\nThe translation to Arabic was performed by\nElJundi et al. in two steps, first by using the Google\nTranslate API and then by validating captions with\nprofessional Arabic translators. Finally, they chose\nthe top three translated captions out of five for each\nimage, which makes 24,000 captions in total. Fig-\nure 1b shows an example of an image from the\ntrain split with its three original English captions\nand three verified Arabic captions. Note that even\nthough verified, the quality of these Arabic cap-\ntions is sometimes questionable. For example, the\nsecond caption in Figure 1b is Xñ\r@ Ég. P, which\nincorrectly translates to “black man”.\nTable 1 shows the complete list of image caption\ndatasets used in this report.\nTable 1: Statistics for the Arabic-COCO and Flickr8k\ntranslated by ElJundi et al. (2020).\nDatasets Train Validation Test\n#Imgs #Caps#Imgs #Caps#Imgs #Caps\nArabic-COCO82,783 414,113- - - -\nFlickr8k6,000 18,0001,000 3,0001,000 3,000\nTOTAL88,783 432,1131, 000 3,0001,000 3,000\n4 Methodology\nAs methodology, we used a two-step pipeline, as\nshown in Figure 2:\n1. Extract region features and object tags from\nan image through a convolutional neural net-\nwork (CNN) encoder.\n2. Generate a sentence from the region features\nand object tags through a language model, in\nour case a pre-trained transformer.\nAs a learning method for our IC model, we used\nOSCAR (Li et al., 2020) and to evaluate our re-\nsults, we used well-establish metrics for IC. The\nfollowing subsections describe these steps in detail.\n42\nA young boy surfing in low waves.\nA young boy is standing on a surfboard and riding a wave.\nA surfer rides his surf board on some very small waves.\nA young boy is standing on a surfboard in the water.\nA young boy is standing on a surfboard in the ocean.\n. \u0010é \t\t®\tj\tJÖÏ@ h. @ñÓ\rB@ úÎ« l.Ì'\tQ\u0010\u001eK\n Q\u001e\n\tª ú\næ.\n. h. @ñÓ\rB@ H. ñ» Pð h. @ñÓ\rB@ H. ñ» P hñËúÎ« \t­\u0010®K\n Q\u001e\n\tª ú\næ.\n. @\u0013Yg. \u0010èQ\u001e\n\tªË@ h. @ñÓ\rB@ \tªK. úÎ« h. @ñÓ\rB@ hñË I. » QK\n h. @ñÓ\r@ I. » @P\n. ZAÖÏ@ ú\n\t¯ ZAÖÏ@ úÎ« l.Ì'\tQ\u0010K hñËúÎ« \t­\u0010®K\n Q\u001e\n\tª ú\næ.\n. ¡J\njÖÏ@ ú\n\t¯ h. @ñÓ\rB@ H. ñ» P hñËúÎ« \t­\u0010®K\n Q\u001e\n\tª ú\næ.\n(a) COCO\nA longhaired man surfing a large wave.\nA man in black on a surfboard riding a wave.\nA man surfing in the ocean.\u0010èQ\u001e\nJ.» \u0010ék. ñÓ l .Ì' \tQ\u0010\u001eK\n Qª \u0011Ë@ ÉK\nñ£ Ég. P\n\u0010ék. ñÓ I. » QK\n h. @ñÓ\rB@ H. ñ» P hñË úÎ« Xñ \r@ Ég. P\n¡J\njÖÏ@ ú\n\t¯ h. @ñÓ\rB@ H. ñ» P \u0010é \tAK\nP PAÖ ß \nÉg. P\n(b) Flickr8k\nFigure 1: Caption annotations in English and Arabic for an image sample from the (a) COCO dataset and the (b)\nFlickr8k dataset.\n \n \n \n(A dog in the water \nwith a ball in its mouth) \nObject Detector \nRegion \nFeatures \nObject \nTags \nTransformer \nكلب  ي\nف  الماء مع كرة  ي\nف  فمهي  \nFigure 2: An overview of our methodology.\n4.1 Image Feature Extraction and Object Tag\nDetection\nFor image feature extraction, Zhang et al. (2021)\ntrained a large-scale object and attribute detection\nmodel based on the ResNeXt-152 C4 architecture\n(Xie et al., 2016), shortened as X152-C4. ResNeXt\nis named after and adopts the ResNet strategy, a\nresidual learning framework designed to ease the\ntraining of networks that are substantially deeper\nthan those used previously (He et al., 2016). For\nthis work, we utilized X152-C4 for feature extrac-\ntion, pre-trained on 2.49 million unique images,\nincluding the COCO dataset. Figure 3 shows an ex-\nample of object detection with the X152-C4 model.\nFor each detected object, an image region vector is\ngenerated, which represents the vector input to the\nlast linear classification layer.\n4.2 The Transformer and BERT\nThe transformer architecture builds solely on at-\ntention mechanisms and was first proposed by\nVaswani et al. (2017). The transformer has proved\nFigure 3: Object detection on an image from the\nCOCO dataset using the X152-C4 architecture. The\nset of detected object tags are (Arm, Beach, Boy,\nCord, Hair, Head, Leaf, Line, Man,\nOcean, Person, Sand, Seaweed, Sky,\nSuit, Surfboard, Tie, Water, Wave,\nWetsuit).\nsuperior in sequence-to-sequence modeling, and\nthe key lies in the possibility to capture the relation-\nships between each word in a sequence with every\nother word.\nProposed by Devlin et al. (2019), BERT showed\nthat pre-trained representations reduced the need\nfor many heavily-engineered task-specific archi-\ntectures. In other words, by pre-training general\nlanguage representations, BERT was the first fine-\ntuning based representation model that achieved\n43\nstate-of-the-art performance on a large group of\nsentence-level tasks, outperforming many task-\nspecific architectures.\nThe release of BERT preceded many other\nBERT-based language models trained on different\ncorpora in different languages, and will be the main\nbase for our image captioning model. The follow-\ning paragraphs describe the models used in this\nwork and Table 2 shows the different models con-\nfigurations for comparison.\nmBERT. mBert, short for Multilingual BERT,\nwas pre-trained with the multilingual Wikipedia\ndataset that consists of the top 104 most com-\nmon languages (Devlin et al., 2018), includ-\ning Arabic. In this comparison, we used the\nbert-base-multilingual-uncased3 ver-\nsion of mBERT from HuggingFace.\nAraBERT. AraBERT (Antoun et al., 2020)\nachieved state-of-the-art performance on most\ntested Arabic NLP tasks. The models were\ntrained on news articles manually scraped from\nArabic news websites and several publicly avail-\nable large Arabic corpora. One of the corpora\nis named OSCAR ( Open Super-large Crawled\nAggregated Co rpus), not to be confused with\nthe image captioning model OSCAR ( Object-\nSemantics Aligned Pre-training). There are sev-\neral versions of AraBERT available. We used\nthe bert-base-arabertv024 configuration\nin this work.\nArabicBERT. ArabicBERT (Safaya et al., 2020)\nwas the first pre-trained BERT model for Ara-\nbic when it was released. It was originally pre-\ntrained as an approach to solve a sub-task of\nthe Multilingual Offensive Language Identifica-\ntion shared task (OffensEval 2020). We used\nthe bert-base-arabic5 configuration in this\nproject.\nGigaBERT. GigaBERT (Lan et al., 2020) is a\nset of models pre-trained as a bilingual BERT and\ndesigned specifically for Arabic NLP and English-\nto-Arabic zero-shot transfer learning. Their\nbest model significantly outperforms mBERT and\nAraBERT on some supervised and zero-shot trans-\nfer settings. The training dataset consists of a\ndump of Arabic Wikipedia, an Arabic version of\n3https://huggingface.co/bert-base-multilingual-uncased\n4https://huggingface.co/aubmindlab/bert-base-arabertv02\n5https://huggingface.co/asafaya/bert-base-arabic\nOSCAR and the Gigaword corpus, which con-\nsists of over 13 million news articles. We used\nthe GigaBERT-v4-Arabic-and-English6\nconfiguration in this work.\n4.3 The OSCAR Learning Method\nThe vanilla BERTBASE cannot handle image region\nfeatures as input. As a learning method, we used\nOSCAR (Li et al., 2020), which achieves state-\nof-the-art results on six well-established vision-\nlanguage understanding and generation tasks, in-\ncluding image captioning.\nPrevious pre-training methods concatenate im-\nage region features and text features as input and\nthen use self-attention to learn image-text seman-\ntics in a brute force manner. OSCAR uses object\ntags detected in images as anchor points to ease the\nalignment of image region and word embeddings.\nThe method is motivated by the observation that\nthe salient objects in an image can be accurately\ndetected by modern object detectors and that these\nobjects are often mentioned in the caption.\nThe original OSCAR paper adapts pre-trained\nmodels to seven downstream VL tasks. For IC fine-\ntuning, they processed the input samples to triples\nconsisting of image region features, captions, and\nobject tags. They then randomly masked out 15%\nof the caption tokens and use the corresponding\noutput representations to perform classification and\npredict the token ids, similar to the masked token\nloss used by BERT.\nWe used the caption inference procedure de-\nscribed by Li et al. (2020). They first initialize\nthe caption generation by feeding in a [MASK]\ntoken and sampling a token from the vocabulary\nbased on the likelihood of the output. Next, the\n[MASK] token in the previous input sequence is re-\nplaced with the sampled token and a new [MASK]\nis appended for the next word prediction. The gen-\neration process terminates when the model outputs\nthe [STOP] token. We used the same beam search\nwith a beam size of 5.\n4.4 Evaluation Metrics\nWe compared the system performances with eval-\nuation metrics used in machine translation, like\nBLEU-1,2,3,4 (Papineni et al., 2002), ROUGE-L\n(Lin, 2004) and METEOR (Banerjee and Lavie,\n2005), but also image caption specific metrics 3,\n6https://huggingface.co/lanwuwei/GigaBERT-v4-Arabic-\nand-English\n3https://github.com/tylin/coco-caption\n44\nTable 2: Configuration comparisons for mBert, AraBERT, ArabicBERT, and GigaBERT.\nModel Training Data Vocabulary Configuration\nsource #tokens (all/ar) tokenization size (all/ar) cased size #parameters\nmBERT Wiki 21.9B/153M WordPiece 110k/5k no base 172M\nAraBERT Wiki, Oscar, News articles 2.5B/2.5B SentencePiece 64k/58k no base 136M\nArabicBERTWiki, Oscar unknown WordPiece 32k/28k no base 111M\nGigaBERT Wiki, Oscar, Gigaword 10.4B/4.3B WordPiece 50k/26k no base 125M\nlike CIDEr (Vedantam et al., 2014) and SPICE\n(Anderson et al., 2016). For comparisons of se-\nmantic meaning, we utilized the transformer-based\nMultilingual Universal Sentence Encoder4 (MUSE)\n(Yang et al., 2020) and angular similarity. Specifi-\ncally, Eq. 1 gives the angular similaritySθ between\ntwo vector embeddings v and u.\nSθ = 1 −arccos\n( v ·u\n∥v∥∥u∥\n)\n/π (1)\nThis way of evaluating captions is similar to the\ntechnique proposed by Afyouni et al. (2021).\nTo verify the quality of the candidate captions,\nwe complement our results with human evaluation.\nFor this task, native Arab speaking experts eval-\nuated a sample of the candidate captions gener-\nated across the proposed models. We followed the\nguidelines of the Transparent Human Benchmark\n(THUMB), a human evaluation protocol proposed\nby Kasai et al. (2021). The authors base their eval-\nuations on two main scores (precision and recall)\nand three types of penalties (fluency, conciseness,\nand inclusive language).\nPrecision measures how precise the caption is\ngiven the image, while recall measures how much\nof the salient information (e.g., objects, attributes,\nand relations) from the image is covered by the\ncaption. Both scores are assessed in the scale of\n1–5. The overall score is computed by averaging\nprecision and recall and deducting penalty points,\nwith a maximum deduction of 0.5. Kasai et al.\n(2021) found most captions from modern neural\nnetwork models were highly fluent and concise.\nSince precision and recall covers the context of an\nimage, in our work, the penalty will be purely based\non grammar and semantics errors. For example,\nconsider the candidate caption:\n\u0010èQ» úÎ« ÈñJ .\u001c\nK. H. Qå\tÖß. lk. PA\u0010J\u0010K \u0010èA\u0010J \t¯\n“Girl swinging a baseball bat on a ball”\n4https://tfhub.dev/google/universal-sentence-encoder-\nmultilingual-large/3\nAlthough the verb “swinging” is literally trans-\nlated to lk. PA\u0010J\u0010K, it does not convey the meaning of\nthe image in Arabic. It should be correctly trans-\nlated to H. Qå\t\u0010\u001d “hits” instead, giving the caption\n0.5 penalty points.\n5 Evaluation\n5.1 Preprocessing\nBefore training the models, we ran all of the images\nthrough the X152-C4 object detector for extraction\nof region features and object tags. Since all of the\nimage features and object tag labels are made avail-\nable for the Karpathy split of the COCO dataset\nby Li et al. (2020), only Flickr8k images had to\nbe inferred. We then split the Flickr8k image fea-\ntures and object tags into train, validation, and test\nimages following ElJundi et al. (2020).\nTo train models on Arabic captions and Ara-\nbic object tag labels, we simply translated English\nlabels directly with the Google Translate API. A\n10% sample of the 1,114 object tags translations\ndetected in the Flickr8k dataset were validated by\ntwo native Arab speaking experts on a scale of 1-3\n(1: incorrect, 2: partly correct, 3: correct). The\nannotators gave the sample a mean score of 2.76\nand 2.62 with a pairwise Cohen kappa coefficient\nof 0.43 (moderate agreement).\n5.2 Experimental Setup\nWe initialized the captioning model with various\nArabic-specific BERT configurations. In order to\nselect the best models, we carried out two experi-\nments considering the multi/bilingual aspects and\nthe learning curve of the fitting procedure:\n1. Evaluation of two multilingual models both\ntrained on\n(a) Arabic captions and Arabic labels\n(b) Arabic captions and English labels\nWe carried out this experiment mainly for\ncomparing the object labels ability to affect\nthe final image-text alignment.\n45\n2. Evaluation of the learning curve for three dif-\nferent models, respectively trained on 50%,\n75% and 100% of a dataset. From the results,\nwe can tell if the validation loss decreases\nwith the amount of data or if some adjustment\nhave to be made to the models, for example\nwith a hyper parameter grid search. Out of\nthe trained models, we chose the two most\naccurate ones as candidates for large scale\ntraining.\nAfter we picked two candidate models, we made\na third and final experiment:\n3. Do large scale training on the candidate mod-\nels on datasets of different size. Evaluate the\nmodels both with automatic and human met-\nrics and compare the results with previous\nmodels.\nWe carried out the first two experiments on\nGoogle Colab GPU:s (1 P100 GPU with 16 GB\nmemory). We carried out the final large scale ex-\nperiments on a workstation (1 GV100 GPU with\n32 GB memory) and a high performance computer\n(HPC) system (8 K80 GPU:s with 12 GB memory\neach).\nFor all the experiments above, we saved training\nand validation loss values at every epoch, while\nmodel checkpoints were saved every 5 epochs. All\nthe experiments used the AdamW optimizer and\na linearly decaying learning rate according to the\nrecipe described in OSCAR (Li et al., 2020). Exact\nmodel hyper parameters for each experiment are\nshown in Appendix A.\n5.3 Experimental Results\nEnglish vs Arabic labels. Table 3 shows the fi-\nnal evaluation scores for all models. Our first ex-\nperiments show that both approaches, training on\nEnglish and Arabic object labels, work in prin-\nciple. Already at this stage, GigaBERT trained\non English labels outperformed previous reported\nBLEU-1,2,3,4 scores with 0.0123, 0.0144, 0.0190,\n0.0167 respectively. However, note that these\nscores were obtained from the val-split, and not\nthe final test-split. We think that the reason to why\nGigaBERT with English labels outperforms Arabic\nlabels is that the quality of the original English la-\nbels, in combination with GigaBERT’s English pre-\ntraining, is much better than its machine translated\ncounterpart. mBert is only trained on Wikipedia\n(Devlin et al., 2018), while GigaBERT is trained\non the Gigaword corpus in addition to Wikipedia\nand web crawl data. This is how we explain Gi-\ngaBERT’s better performance. Moreover, the vo-\ncabulary of GigaBERT (21k English tokens vs 26k\nArabic tokens) is richer and more balanced than the\nvocabulary of mBERT (53k English tokens vs 5k\nArabic tokens), see Table 2.\nTable 3: Evaluation scores (evaluation on epoch 30) for\nthe trained models. The best scoring models are marked\nin bold for each evaluation metric.\nModel LabelsBLEU-4 ROUGE-L METEOR CIDEr SPICE\nGigaBERTEnglish0.074 0.29 0.3 0.33 0.037Arabic0.062 0.29 0.31 0.31 0.037\nmBert English0.058 0.28 0.30 0.29 0.031Arabic0.067 0.29 0.30 0.31 0.033\nLearning Curve. We evaluated all the models\nfrom the learning curve experiment with MUSE\nto investigate the correlation between semantic\nscores and an increased amount of data. The eval-\nuation over training time is shown in Figure 4 for\nAraBERT, ArabicBERT, and GigaBERT. In gen-\neral, more data increased evaluation scores. One\nnotable thing is that the final score of GigaBERT\ntrained on 75% of data outperformed 100%, but\nFigure 4b shows that the 100% curve is generally\nhigher than the 75% curve. This finding suggests\nthat the average MUSE score has a high variance.\nNote that GigaBERT trained on 100% of Flickr8k\nis identical to the model trained on Arabic labels in\nthe previous experiment.\nIn the case of AraBERT, the 75% MUSE curve is\nway lower than the 100% and 50% curves, but the\n100% loss curve is still higher than the 50% one.\nThe unstable training results of AraBERT suggest\nthat the selected learning rate is too large. We\nperformed learning rate grid search on AraBERT\nand GigaBERT on the interval η∈[1e−5,7e−5] to\nminimize validation loss, and found an optimum at\nη= 3e−5.\nLarge Scale Training. Table 4 presents the final\ntest scores (BLEU-1,2,3,4, ROUGE-L, METEOR,\nCIDEr and MUSE) of a selection of our models,\nand models previously proposed by Jindal (2018),\nAl-muzaini et al. (2018), Afyouni et al. (2021) and\nElJundi et al. (2020). Out of the previous works,\nonly the model by ElJundi et al. (2020) is tested on\nthe same Flickr8k test set as ours. We were unable\nto obtain the splits from the other studies, and have\nno data regarding on how their splits may differ\nfrom ours. The difference between their model\nscores and our are quite large in some cases. One\n46\n(a) AraBERT MUSE scores\n (b) GigaBERT MUSE scores\n (c) ArabicBERT MUSE scores\nFigure 4: MUSE evaluation scores over all epochs for (a) AraBERT, (b) GigaBERT and (c) ArabicBERT.\nTable 4: Our model scores compared to previous models. The highest scores on our test-split are marked in bold. Of\nall the previous ones, only the model by ElJundi et al. (2020) uses the same test-split as us. Other test-splits are\nunknown.\nModel Test set B1 B2 B3 B4 ROUGE-L METEOR CIDEr MUSE\nJindal (2018) Flickr8k 0.658 0.559 0.404 0.223 - 0.201 - -\nAl-muzaini et al. (2018) COCO & Flickr8k0.462 0.260 0.190 0.080 - - - -\nAfyouni et al. (2021) COCO 0.649 0.413 0.241 0.136 0.470 0.408 - 0.78\nElJundi et al. (2020) Flickr8k 0.332 0.193 0.105 0.057 - - - -\nAraBERT32-Flickr8k\nFlickr8k\n0.391 0.246 0.150 0.092 0.331 0.314 0.415 0.671\nAraBERT32-COCO 0.365 0.221 0.129 0.0715 0.310 0.317 0.36 0.669\nAraBERT256-Flickr8k 0.387 0.244 0.151 0.093 0.334 0.312 0.428 0.668\nGigaBERT32-Flickr8k 0.386 0.241 0.144 0.0827 0.331 0.315 0.403 0.669\nGigaBERT32-COCO 0.36 0.215 0.124 0.0708 0.308 0.311 0.344 0.668\n∆ 0.059↑ 0.053↑ 0.046↑ 0.036↑\nCandidate caption:(MUSE 0.920)\u0010éK\nQ\tm\u0019\u0010éÊ\u0010K \u0010ñ\t¯ \u0010éJ\nK.@Q\u0010K \u0010ék.@PX I. » QK\n Ég.P\n“Man riding a dirt bike on a rocky hill”Reference caption:Pñ\tjË@\tªK. \u0010ñ\t¯ \u0010éJ\nK.@Q\u0010K \u0010ék.@PX I. » QK\n Ég.P\n“Man riding a dirt bike over some rocks”THUMB-score:Precision: 5, Recall: 5, Penalty: 0, Total: 5\n(a)\nCandidate caption:(MUSE 0.9043)I. \u0011ªËAK. ù¢\tªÓ É\u0010®kQ\u001e.« \t» QK\nQ\u001e\n\tª\tJ\nK.@ I. Ê¿“Small white dog running across a grass field”Reference caption:ú\næ. \u0011« É\u0010®k ú\n\t¯ ø\nQm.\u001a'\nQ\u001e\n\tª\tJ\nK.\r@ I. Ê¿“Little white dog running in grass field”THUMB-score:Precision: 5, Recall: 5, Penalty: 0, Total: 5\n(b)\nCandidate caption:(MUSE 0.5008)\u0010\tJ«\u0010é¢\u001d.PðQ\u001e\n\u0010¯ È@ðQå ø\nY\u0010KQK\nQ\u001e\n\tª É\t®£“Little child wearing shorts and tie”Reference caption:éËñk\táÓ A\tJË@\táÓQ\u001e\n\u0011JºË@ ©Ó éK\nYK\núÎ« \t­\u0010®K\nÉg.P“A man standing on his hands with many people around him”THUMB-score:Precision: 1, Recall: 2, Penalty: 0, Total: 1.5\n(c)\nCandidate caption:(MUSE 0.4902)\u0010é\tJkA\u0011 Qê\t£úÎ«\tàñ\u0010®Ê\u0010\u001cK\nA\tJË@\táÓ\u0010é«ñÒm.×\n“Group of people climbing on the back of a truck”Reference caption:ù\nëCÓ\u0010é\tJK\nYÓ“Amusement park”THUMB-score:Precision: 2.5, Recall: 3.5, Penalty: 0, Total: 3\n(d)\nFigure 5: Human evaluation of four candidate captions produced by AraBERT32-COCO: two accurate candidate\ncaptions (a) and (b), and two inaccurate candidate captions (c) and (d). Each candidate caption is accompanied by\nthe reference caption from the Flickr8k test-split with the most MUSE similarity, and a THUMB score.\npossible explanation could be that our BERT-based\napproach differs from previous LSTM-based ap-\nproaches, which can achieve significantly higher\nresults than a BERT-based model for a small dataset\non NLP tasks (Ezen-Can, 2020).\nAll of our models are named after the scheme\nmodelBatchSize-dataset, where model is our ini-\ntialization model, BatchSize is the training batch\nsize and dataset is the dataset trained on. For ex-\nample, one of our best performing models was ini-\ntialized on AraBERT and trained with a batch size\nof 32 on Flickr8k. Therefore, we named the model\nAraBERT32-Flickr8k. AraBERT32-Flickr8k out-\nperforms the model by ElJundi et al. (2020) on\nall BLEU scores, and most remarkably on BLEU-\n4, where we see a 61.4% increase. We chose to\ndrop the SPICE scores from Table 4 because of the\nevaluation scripts incompatibility with the Arabic\nlanguage.\nWe complemented Table 4 with human evalua-\ntions on a sample of the dataset according to the\nguidelines of THUMB (Kasai et al., 2021). Figure\n5 shows four generated captions from AraBERT32-\nCOCO with images and human evaluations. All of\n47\nthe evaluations were made by two Arabic language\nexperts.\nIn general, the human evaluations show accurate\nresults. In Figure 5a, the candidate caption:\n\u0010éK\nQ\tm\u0019 \u0010éÊ\u0010K \u0010ñ\t¯ \u0010éJ\nK. @Q\u0010K \u0010ék. @PX ¼QK\n Ég. P\n“Man riding a dirt bike on a rocky hill”\nis nearly perfect. It is almost identical to the refer-\nence caption:\nPñ \tjË@ \tªK. \u0010ñ\t¯ \u0010éJ\nK. @Q\u0010K \u0010ék. @PX I. » QK\n Ég. P\n“Man riding a dirt bike over some rocks”,\nand only differs in the last phrase.\nNot all results were accurate. Looking at Figure\n5c, the first row shows the candidate caption\n\u0010é\tJkA \u0011 Qê \t£ úÎ« \tàñ \u0010®Ê\u0010\u001cK\n A \tJË@ \táÓ \u0010é«ñÒm.×\n“Group of people climbing on the back\nof a truck”,\nwhile the closest reference caption ù\n ëCÓ \u0010é\tJK\nYÓ\ntranslates to “Amusement park”. Though the candi-\ndate sentence is fluent and grammatically correct,\nit appears to be random in the context of the im-\nage. This shows how the models in these examples\nfail to identify objects in the image and correctly\ndescribe a scene.\nA potential source of error for the incorrect\nimage-text alignment could be noise in the ma-\nchine translated data input. For example, the pub-\nlicly available Arabic-COCO used is purely ma-\nchine translated and has to be verified by humans\nbefore employed in testing. The justification to\nwhy we still use machine-translated data is that we\nrely on the BERT-based language models to han-\ndle the grammar and syntax, while we count on\nthe machine-translation model to correctly trans-\nlate salient objects. The failure to do so leads to\nerrors in learning image-text semantic alignments.\nFor example, in our dataset, mistranslated object\nlabels can be found. Some nouns are mistrans-\nlated into their homophone counterparts: “light”\n(noun) to \u0010é \t®J\n \t® \tk (adjective, bright; well-lighted),\n“block” (noun) to ©\tJÓ (adjective, to obstruct, or pre-\nvent someone or something) and so on. Li et al.\n(2020) showed that OSCAR learning curves for\nfine-tuning with object tags converge significantly\nfaster than the methods without tags. In other\nwords, high quality labels are crucial in image-text\nalignment for VL-pretrained models.\nFor the complete table with scores for all trained\nmodels, see Appendix B.\n6 Conclusion\nThis work focused on Arabic image captioning\nusing pre-trained bidirectional transformers. We\ncan draw many conclusions from it.\nThe specific challenge in Arabic image caption-\ning is, not regarding the lack of well-annotated\ndatasets, the morphological complexity of the Ara-\nbic language which makes it harder to process.\nIn our work, we showed that it is possible to\nachieve state-of-the-art results with a minimal pre-\nprocessing scheme and by adapting English cap-\ntioning models to other languages through public\ndataset benchmarks.\nFurthermore, we achieved results better than the\nprevious work on the Flickr8k dataset by ElJundi\net al. (2020). Our experiments also show that both\napproaches, training on English and Arabic object\nlabels, work in principle. In addition, we proposed\nworking configurations and heuristics for hyper pa-\nrameters in future experimentation on our proposed\nmodels. Therefore, our models provide a new base-\nline for the AIC community.\nFurther work in the field should be to verify all\nmachine translated Arabic labels by humans before\nfurther training on the datasets. This task should\nnot be too expensive since there are only 1,114\nobject tags translations detected in the Flickr8k\ndataset, and 253 additional object tags in Arabic-\nCOCO. This could greatly improve training. Sec-\nondly, the lack of qualitative Arabic data should be\nsolved by translation and verification of all COCO\ncaptions, and then making the resulting dataset pub-\nlicly available. As a suggestion, one could follow\na crowd sourcing procedure as described by Al-\nmuzaini et al. (2018), which includes some of the\ninstructions that were used in the creation of COCO\ncaptions, and additional instructions specific to the\nArabic language. This would create a new bench-\nmark Arabic captioning dataset that we could train\nand test our models on.\nFinally, we hope that our work will be useful for\nfuture Arabic image captioning models, and that\nit will spur more contributions to the field in the\nclosest future.\nAcknowledgements\nThis work was partially supported by Veten-\nskaprådet, the Swedish Research Council, regis-\ntration number 2021-04533.\n48\nReferences\nImad Afyouni, Imtinan Azhara, and Ashraf Elnagar.\n2021. AraCap: A hybrid deep learning architecture\nfor Arabic Image Captioning. In ACLing 2021: 5th\nInternational Conference on AI in Computational\nLinguistics.\nHuda A. Al-muzaini, Tasniem N. Al-yahya, and Hafida\nBenhidour. 2018. Automatic Arabic image caption-\ning using RNN-LSTM-based language model and\nCNN. International Journal of Advanced Computer\nScience and Applications, 9(6).\nPeter Anderson, Basura Fernando, Mark Johnson, and\nStephen Gould. 2016. SPICE: Semantic Proposi-\ntional Image Caption Evaluation. In Computer Vi-\nsion – ECCV 2016, pages 382–398, Cham. Springer\nInternational Publishing.\nWissam Antoun, Fady Baly, and Hazem Hajj. 2020.\nAraBERT: Transformer-based model for Arabic lan-\nguage understanding. In Proceedings of the 4th Work-\nshop on Open-Source Arabic Corpora and Process-\ning Tools, with a Shared Task on Offensive Language\nDetection, pages 9–15, Marseille, France. European\nLanguage Resource Association.\nAnfal Attai and Ashraf Elnagar. 2020. A survey on Ara-\nbic Image Captioning Systems Using Deep Learn-\ning Models. In 14th International Conference on\nInnovations in Information Technology (IIT), pages\n114–119.\nSatanjeev Banerjee and Alon Lavie. 2005. METEOR:\nAn automatic metric for MT evaluation with im-\nproved correlation with human judgments. In Pro-\nceedings of the ACL Workshop on Intrinsic and Ex-\ntrinsic Evaluation Measures for Machine Transla-\ntion and/or Summarization, pages 65–72, Ann Arbor,\nMichigan. Association for Computational Linguis-\ntics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina Toutanova. 2018. Multilin-\ngual bert readme. https://github.com/google-\nresearch/bert/blob/master/multilingual.md. [Online;\naccessed 6 Feb. 2022].\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nObeida ElJundi, Mohamad Dhaybi, Kotaiba Mokadam,\nHazem Hajj, and Daniel Asmar. 2020. Resources\nand End-to-End Neural Network Models for Arabic\nImage Captioning. In 15th International Conference\non Computer Vision Theory and Applications.\nAysu Ezen-Can. 2020. A Comparison of LSTM and\nBERT for Small Corpus. ArXiv, abs/2009.05451.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep Residual Learning for Image Recog-\nnition. In 2016 IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 770–778.\nMicah Hodosh, Peter Young, and Julia Hockenmaier.\n2013. Framing Image Description as a Ranking Task:\nData, Models and Evaluation Metrics. In 24th Inter-\nnational Joint Conference on Artificial Intelligence.\nXiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang,\nZicheng Liu, Yumao Lu, and Lijuan Wang. 2021.\nScaling Up Vision-Language Pre-training for Image\nCaptioning. CoRR, abs/2111.12233.\nLun Huang, Wenmin Wang, Jie Chen, and Xiao-Yong\nWei. 2019. Attention on Attention for Image Caption-\ning. In 2019 IEEE/CVF International Conference on\nComputer Vision (ICCV).\nVasu Jindal. 2017. A Deep Learning Approach for\nArabic Caption Generation Using Roots-Words. In\nProceedings of the Thirty-First AAAI Conference on\nArtificial Intelligence (AAAI).\nVasu Jindal. 2018. Generating image captions in Arabic\nusing root-word based recurrent neural networks and\ndeep neural networks. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Student\nResearch Workshop, pages 144–151, New Orleans,\nLouisiana, USA. Association for Computational Lin-\nguistics.\nAndrej Karpathy and Li Fei-Fei. 2015. Deep Visual-\nSemantic Alignments for Generating Image Descrip-\ntions. In 2015 IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR).\nJungo Kasai, Keisuke Sakaguchi, Lavinia Dunagan,\nJacob Morrison, Ronan Le Bras, Yejin Choi, and\nNoah A. Smith. 2021. Transparent Human Evalua-\ntion for Image Captioning. CoRR, abs/2111.08940.\nWuwei Lan, Yang Chen, Wei Xu, and Alan Ritter. 2020.\nAn empirical study of pre-trained transformers for\nArabic information extraction. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 4727–4734,\nOnline. Association for Computational Linguistics.\nXiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang,\nXiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu,\nLi Dong, Furu Wei, Yejin Choi, and Jianfeng Gao.\n2020. Oscar: Object-Semantics Aligned Pre-training\nfor Vision-Language Tasks. In Computer Vision –\nECCV 2020.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\n49\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Dollár,\nand C. Lawrence Zitnick. 2014. Microsoft COCO:\nCommon Objects in Context. In Computer Vision –\nECCV 2014, pages 740–755, Cham. Springer Inter-\nnational Publishing.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nSabri Monaf Sabri. 2021. Arabic Image Captioning\nusing Deep Learning with Attention. Master’s thesis,\nUniversity of Georgia.\nAli Safaya, Moutasem Abdullatif, and Deniz Yuret.\n2020. KUISAIL at SemEval-2020 task 12: BERT-\nCNN for offensive speech identification in social me-\ndia. In Proceedings of the Fourteenth Workshop on\nSemantic Evaluation, pages 2054–2059, Barcelona\n(online). International Committee for Computational\nLinguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention Is All\nYou Need. In Conference on Neural Information\nProcessing Systems (NIPS).\nRamakrishna Vedantam, C. Lawrence Zitnick, and Devi\nParikh. 2014. CIDEr: Consensus-based Image De-\nscription Evaluation. CoRR, abs/1411.5726.\nSaining Xie, Ross Girshick, Piotr Dollár, Zhuowen\nTu, and Kaiming He. 2016. Aggregated Residual\nTransformations for Deep Neural Networks. arXiv\npreprint arXiv:1611.05431.\nKelvin Xu, Jimmy Lei Ba, Ryan Kiros, Kyunghyun Cho,\nAaron Courville, Ruslan Salakhutdinov, Richard S.\nZemel, and Yoshua Bengio. 2015. Show, Attend and\nTell: Neural Image Caption Generation with Visual\nAttention. In Proceedings of the 32nd International\nConference on Machine Learning (PMLR).\nYinfei Yang, Daniel Cer, Amin Ahmad, Mandy Guo,\nJax Law, Noah Constant, Gustavo Hernandez Abrego,\nSteve Yuan, Chris Tar, Yun-hsuan Sung, Brian Strope,\nand Ray Kurzweil. 2020. Multilingual universal sen-\ntence encoder for semantic retrieval. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics: System Demonstrations,\npages 87–94, Online. Association for Computational\nLinguistics.\nQuanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang,\nand Jiebo Luo. 2016. Image captioning with seman-\ntic attention. In Proceedings of the IEEE conference\non computer vision and pattern recognition.\nPengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei\nYang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jian-\nfeng Gao. 2021. VinVL: Revisiting Visual Represen-\ntations in Vision-Language Models. In Proceedings\nof the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 5579–5588.\nLuowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu,\nJason J. Corso, and Jianfeng Gao. 2019. Unified\nVision-Language Pre-Training for Image Captioning\nand VQA. In Proceedings of the AAAI Conference\non Artificial Intelligence.\nA Experiment Hyperparameters\nEnglish vs Arabic labels. All experiments were\ntrained and validated with the Flickr8k train- re-\nspective val-split. Table 5 shows the exact hyperpa-\nrameters for the experiments.\nLearning curve. All experiments were validated\nwith the Flickr8k val-split and trained on Arabic\nlabels. Table 6 shows the exact hyperparameters\nfor the experiments. Grid search optimization was\nmade on AraBERT and GigaBERT in the interval\nη∈[1e−5,7e−5] and a step size of 1e−5.\nLarge scale. All experiments were validated and\ntested with the Flickr8k val- respective test-split,\nand trained on Arabic labels. Table 7 shows the\nexact hyperparameters for the experiments.\nB Complementary Results\nTable 8 shows scores for all models trained during\nthe last experiment.\n50\nTable 5: Hyperparameters used for the English vs Arabic labels experiments.\nModel Train Object labels Learning rate Batch size #Epochs\nGigaBERT Flickr8k eng/ar 1e-4 32 30\nmBERT Flickr8k eng/ar 1e-4 32 30\nTable 6: Hyperparameters and datasets used for the learning curve experiments.\nModel Train % of dataset Learning rate Batch size #Epochs\nAraBERT Flickr8k 50/75/100 1e-4 32 30\nArabic-BERT Flickr8k 50/75/100 1e-4 32 30\nGigaBERT Flickr8k 50/75/100 1e-4 32 30\nTable 7: Hyperparameters and datasets used for the large scale experiments.\nModel Train Object labels Learning rate Batch size #Epochs\nAraBERT\nFlickr8k ar 3e-5 32 30\nArabic-COCO ar 5e-5 32 50\nArabic-COCO+Flickr8k ar 3e-5 32 50\nFlickr8k ar 5e-5 256 30\nArabic-COCO ar 9e-5 256 50\nArabic-COCO+Flickr8k ar 9e-5 256 50\nGigaBERT\nFlickr8k eng 3e-5 32 30\nArabic-COCO eng 3e-5 32 50\nArabic-COCO+Flickr8k eng 3e-5 32 50\nFlickr8k eng 9e-5 256 30\nArabic-COCO eng 9e-5 256 50\nArabic-COCO+Flickr8k eng 9e-5 256 50\nTable 8: Our model scores compared to previous models. The highest scores on our test-split are marked in bold. Of\nall the previous ones, only the model by ElJundi et al. (2020) uses the same test-split as us. Other test-splits are\nunknown.\nModel Test set B1 B2 B3 B4 ROUGE-L METEOR CIDEr MUSE\nJindal (2018) Flickr8k 0.658 0.559 0.404 0.223 - 0.201 - -\nAl-muzaini et al. (2018) COCO & Flickr8k0.462 0.260 0.190 0.080 - - - -\nAfyouni et al. (2021) COCO 0.649 0.413 0.241 0.136 0.470 0.408 - 0.78\nElJundi et al. (2020) Flickr8k 0.332 0.193 0.105 0.057 - - - -\nAraBERT32-Flickr8k\nFlickr8k\n0.391 0.246 0.150 0.092 0.331 0.314 0.415 0.671\nAraBERT32-COCO 0.365 0.221 0.129 0.0715 0.31 0.317 0.36 0.669\nAraBERT32-COCO+Flickr8k 0.358 0.216 0.127 0.0715 0.317 0.316 0.364 0.661\nAraBERT256-Flickr8k 0.387 0.244 0.151 0.093 0.334 0.312 0.428 0.668\nAraBERT256-COCO 0.355 0.211 0.122 0.069 0.303 0.313 0.335 0.665\nAraBERT256-COCO+Flickr8k 0.339 0.204 0.12 0.0686 0.302 0.31 0.339 0.655\nGigaBERT32-Flickr8k 0.386 0.241 0.144 0.0827 0.331 0.315 0.403 0.669\nGigaBERT32-COCO 0.36 0.215 0.124 0.0708 0.308 0.311 0.344 0.668\nGigaBERT32-COCO+Flickr8k 0.362 0.216 0.127 0.0675 0.312 0.308 0.359 0.661\nGigaBERT265-Flickr8k 0.376 0.235 0.141 0.0803 0.322 0.313 0.385 0.664\nGigaBERT265-COCO 0.339 0.198 0.113 0.062 0.287 0.306 0.312 0.662\nGigaBERT265-COCO+Flickr8k 0.365 0.217 0.128 0.0705 0.315 0.309 0.373 0.662\n∆ 0.059↑ 0.053↑ 0.046↑ 0.036↑\n51",
  "topic": "Closed captioning",
  "concepts": [
    {
      "name": "Closed captioning",
      "score": 0.9896257519721985
    },
    {
      "name": "Computer science",
      "score": 0.8048869967460632
    },
    {
      "name": "Artificial intelligence",
      "score": 0.646241307258606
    },
    {
      "name": "Natural language processing",
      "score": 0.5876246690750122
    },
    {
      "name": "Arabic",
      "score": 0.5700801014900208
    },
    {
      "name": "Transformer",
      "score": 0.5672189593315125
    },
    {
      "name": "Initialization",
      "score": 0.4931153953075409
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.4780733585357666
    },
    {
      "name": "Image (mathematics)",
      "score": 0.4267309904098511
    },
    {
      "name": "Speech recognition",
      "score": 0.4133869707584381
    },
    {
      "name": "Computer vision",
      "score": 0.3596474230289459
    },
    {
      "name": "Linguistics",
      "score": 0.11098942160606384
    },
    {
      "name": "Engineering",
      "score": 0.06305921077728271
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I187531555",
      "name": "Lund University",
      "country": "SE"
    },
    {
      "id": "https://openalex.org/I29891158",
      "name": "University of Sharjah",
      "country": "AE"
    }
  ]
}