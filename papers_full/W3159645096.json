{
  "title": "Visual Saliency Transformer",
  "url": "https://openalex.org/W3159645096",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5100377352",
      "name": "Nian Liu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100319347",
      "name": "Ni Zhang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5017043972",
      "name": "Kaiyuan Wan",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5012529382",
      "name": "Junwei Han",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5082634513",
      "name": "Ling Shao",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2039313011",
    "https://openalex.org/W6651016250",
    "https://openalex.org/W845365781",
    "https://openalex.org/W2963112696",
    "https://openalex.org/W3035633116",
    "https://openalex.org/W2948537313",
    "https://openalex.org/W6784364555",
    "https://openalex.org/W1965301399",
    "https://openalex.org/W3034320133",
    "https://openalex.org/W6785148896",
    "https://openalex.org/W3035687312",
    "https://openalex.org/W2039298799",
    "https://openalex.org/W6639359414",
    "https://openalex.org/W3106587394",
    "https://openalex.org/W3010616503",
    "https://openalex.org/W6750320456",
    "https://openalex.org/W2461475918",
    "https://openalex.org/W6762111365",
    "https://openalex.org/W2086791339",
    "https://openalex.org/W6788135285",
    "https://openalex.org/W3168649818",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2740667773",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W2808442315",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W2780708736",
    "https://openalex.org/W2938260698",
    "https://openalex.org/W3108608656",
    "https://openalex.org/W2798807298",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W3034185160",
    "https://openalex.org/W2169632643",
    "https://openalex.org/W6780226713",
    "https://openalex.org/W6774732966",
    "https://openalex.org/W2765838470",
    "https://openalex.org/W2041719651",
    "https://openalex.org/W2569272946",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W6644385675",
    "https://openalex.org/W6781120584",
    "https://openalex.org/W6766978945",
    "https://openalex.org/W6600857173",
    "https://openalex.org/W6784094891",
    "https://openalex.org/W2766315367",
    "https://openalex.org/W6637373629",
    "https://openalex.org/W2519610629",
    "https://openalex.org/W6639824700",
    "https://openalex.org/W1938386764",
    "https://openalex.org/W2961348656",
    "https://openalex.org/W3035357085",
    "https://openalex.org/W3002301267",
    "https://openalex.org/W6783944145",
    "https://openalex.org/W2807746031",
    "https://openalex.org/W6785766105",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W2963868681",
    "https://openalex.org/W2963529609",
    "https://openalex.org/W6765983947",
    "https://openalex.org/W6766759761",
    "https://openalex.org/W6784512143",
    "https://openalex.org/W2948300571",
    "https://openalex.org/W2948510860",
    "https://openalex.org/W6780967699",
    "https://openalex.org/W6776440512",
    "https://openalex.org/W1942214758",
    "https://openalex.org/W1900913856",
    "https://openalex.org/W2798791651",
    "https://openalex.org/W4214696292",
    "https://openalex.org/W2798857366",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W6781928302",
    "https://openalex.org/W2909381593",
    "https://openalex.org/W6781101626",
    "https://openalex.org/W4239147634",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W6785271587",
    "https://openalex.org/W3035422681",
    "https://openalex.org/W3035290198",
    "https://openalex.org/W3118710621",
    "https://openalex.org/W1966025376",
    "https://openalex.org/W6648922525",
    "https://openalex.org/W2171378720",
    "https://openalex.org/W3108822985",
    "https://openalex.org/W1745334888",
    "https://openalex.org/W2963897031",
    "https://openalex.org/W2981510929",
    "https://openalex.org/W2744613561",
    "https://openalex.org/W3096966254",
    "https://openalex.org/W3202061712",
    "https://openalex.org/W3108421143",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W3097725659",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W3097336090",
    "https://openalex.org/W3107944836",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W3115390238",
    "https://openalex.org/W2037954058",
    "https://openalex.org/W3109623941",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W3114152269",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2002781701",
    "https://openalex.org/W3092604408",
    "https://openalex.org/W3108812909",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2939217524",
    "https://openalex.org/W2990984982",
    "https://openalex.org/W1894057436",
    "https://openalex.org/W2957414648",
    "https://openalex.org/W2962835968",
    "https://openalex.org/W3035284915",
    "https://openalex.org/W2963685207",
    "https://openalex.org/W20683899",
    "https://openalex.org/W1993713494",
    "https://openalex.org/W3125056032",
    "https://openalex.org/W1981228217",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3097053213"
  ],
  "abstract": "Existing state-of-the-art saliency detection methods heavily rely on CNN-based architectures. Alternatively, we rethink this task from a convolution-free sequence-to-sequence perspective and predict saliency by modeling long-range dependencies, which can not be achieved by convolution. Specifically, we develop a novel unified model based on a pure transformer, namely, Visual Saliency Transformer (VST), for both RGB and RGB-D salient object detection (SOD). It takes image patches as inputs and leverages the transformer to propagate global contexts among image patches. Unlike conventional architectures used in Vision Transformer (ViT), we leverage multi-level token fusion and propose a new token upsampling method under the transformer framework to get high-resolution detection results. We also develop a token-based multi-task decoder to simultaneously perform saliency and boundary detection by introducing task-related tokens and a novel patch-task-attention mechanism. Experimental results show that our model outperforms existing methods on both RGB and RGB-D SOD benchmark datasets. Most importantly, our whole framework not only provides a new perspective for the SOD field but also shows a new paradigm for transformer-based dense prediction models. Code is available at https://github.com/nnizhang/VST.",
  "full_text": "Visual Saliency Transformer\nNian Liu1* Ni Zhang2* Kaiyuan Wan2 Ling Shao1 Junwei Han2‚Ä†\n1Inception Institute of ArtiÔ¨Åcial Intelligence 2Northwestern Polytechnical University\n{liunian228, nnizhang.1995, kaiyuan.wan0106, junweihan2010}@gmail.com, ling.shao@ieee.org\nAbstract\nExisting state-of-the-art saliency detection methods\nheavily rely on CNN-based architectures. Alternatively,\nwe rethink this task from a convolution-free sequence-to-\nsequence perspective and predict saliency by modeling\nlong-range dependencies, which can not be achieved by\nconvolution. SpeciÔ¨Åcally, we develop a novel uniÔ¨Åed model\nbased on a pure transformer, namely, Visual Saliency Trans-\nformer (VST), for both RGB and RGB-D salient object de-\ntection (SOD). It takes image patches as inputs and lever-\nages the transformer to propagate global contexts among\nimage patches. Unlike conventional architectures used in\nVision Transformer (ViT), we leverage multi-level token fu-\nsion and propose a new token upsampling method under\nthe transformer framework to get high-resolution detec-\ntion results. We also develop a token-based multi-task de-\ncoder to simultaneously perform saliency and boundary\ndetection by introducing task-related tokens and a novel\npatch-task-attention mechanism. Experimental results show\nthat our model outperforms existing methods on both RGB\nand RGB-D SOD benchmark datasets. Most importantly,\nour whole framework not only provides a new perspec-\ntive for the SOD Ô¨Åeld but also shows a new paradigm for\ntransformer-based dense prediction models. Code is avail-\nable at https://github.com/nnizhang/VST.\n1. Introduction\nSOD aims to detect objects that attract peoples‚Äô eyes and\ncan help many vision tasks,e.g., [58, 19]. Recently, RGB-D\nSOD has also gained growing interest with the extra spatial\nstructure information from the depth data. Current state-\nof-the-art SOD methods are dominated by convolutional ar-\nchitectures [28], on both RGB and RGB-D data. They often\nadopt an encoder-decoder CNN architecture [47, 57], where\nthe encoder encodes the input image to multi-level features\nand the decoder integrates the extracted features to predict\nthe Ô¨Ånal saliency map. Based on this simple architecture,\n*Equal contribution.\n‚Ä†Corresponding author.\nmost efforts have been made to build a powerful decoder\nfor predicting better saliency results. To this end, they in-\ntroduced various attention models [37, 80, 7], multi-scale\nfeature integration methods [24, 49, 16, 43], and multi-task\nlearning frameworks [67, 77, 82, 69, 25]. An additional de-\nmand for RGB-D SOD is to effectively fuse cross-modal\ninformation, i.e., the appearance information and the depth\ncues. Existing works propose various modality fusion meth-\nods, such as feature fusion [22, 4, 16, 18, 89], knowledge\ndistillation [53], dynamic convolution [48], attention mod-\nels [31, 78], and graph neural networks [43]. Hence, CNN-\nbased methods have achieved impressive results [66, 88].\nHowever, all previous methods are limited in learning\nglobal long-range dependencies. Global contexts [21, 83,\n56, 44, 37] and global contrast [75, 2, 8] have been proved\ncrucial for saliency detection for a long time. Nevertheless,\ndue to the intrinsic limitation of CNNs that they extract fea-\ntures in local sliding windows, previous methods can hardly\nexploit the crucial global cues. Although some methods uti-\nlized fully connected layers [36, 22], global pooling layers\n[44, 37, 65], and non-local modules [38, 7] to incorporate\nthe global context, they only did such in certain layers and\nthe standard CNN-based architecture remains unchanged.\nRecently, Transformer [61] was proposed to model\nglobal long-range dependencies among word sequences for\nmachine translation. The core idea is the self-attention\nmechanism, which leverages the query-key correlation to\nrelate different positions in a sequence. Transformer stacks\nthe self-attention layers multiple times in both encoder and\ndecoder, thus can model long-range dependencies in every\nlayer. Hence, it is natural to introduce the Transformer to\nSOD, leveraging the global cues in the model all the way.\nIn this paper, for the Ô¨Årst time, we rethink SOD from a\nnew sequence-to-sequence perspective and develop a novel\nuniÔ¨Åed model for both RGB and RGB-D SOD based on a\npure transformer, which is named Visual Saliency Trans-\nformer. We follow the recently proposed ViT models\n[12, 74] to divide each image into patches and adopt the\nTransformer model on the patch sequence. Then, the Trans-\nformer propagates long-range dependencies between image\npatches, without any need of using convolution. However,\narXiv:2104.12099v2  [cs.CV]  23 Aug 2021\nit is not straightforward to apply ViT for SOD. On the one\nhand, how to perform dense prediction tasks based on pure\ntransformer still remains an open question. On the other\nhand, ViT usually tokenizes the image to a very coarse\nscale. How to adapt ViT to the high-resolution prediction\ndemand of SOD is also unclear.\nTo solve the Ô¨Årst problem, we design a token-based\ntransformer decoder by introducing task-related tokens\nto learn decision embeddings. Then, we propose a\nnovel patch-task-attention mechanism to generate dense-\nprediction results, which provides a new paradigm for using\ntransformer in dense prediction tasks. Motivated by pre-\nvious SOD models [82, 87, 79, 25] that leveraged bound-\nary detection to boost the SOD performance, we build a\nmulti-task decoder to simultaneously conduct saliency and\nboundary detection by introducing a saliency token and a\nboundary token. This strategy simpliÔ¨Åes the multitask pre-\ndiction workÔ¨Çow by simply learning task-related tokens,\nthus largely reduces the computational costs while obtain-\ning better results. To solve the second problem, inspired by\nthe Tokens-to-Token (T2T) transformation [74], which re-\nduces the length of tokens, we propose a new reverse T2T\ntransformation to upsample tokens by expanding each token\ninto multiple sub-tokens. Then, we upsample patch tokens\nprogressively and fuse them with low-level tokens to obtain\nthe Ô¨Ånal full-resolution saliency map. In addition, we also\nuse a cross modality transformer to deeply explore the inter-\naction between multi-modal information for RGB-D SOD.\nFinally, our VST outperforms existing state-of-the-art SOD\nmethods with a comparable number of parameters and com-\nputational costs, on both RGB and RGB-D data.\nOur main contributions can be summarized as follows:\n‚Ä¢ For the Ô¨Årst time, we design a novel uniÔ¨Åed model\nbased on the pure transformer architecture for both\nRGB and RGB-D SOD, from a new perspective of\nsequence-to-sequence modeling.\n‚Ä¢ We design a multi-task transformer decoder to jointly\nconduct saliency and boundary detection by introduc-\ning task-related tokens and patch-task-attention.\n‚Ä¢ We propose a new token upsampling method for\ntransformer-based framework.\n‚Ä¢ Our proposed VST model achieves state-of-the-art\nresults on both RGB and RGB-D SOD benchmark\ndatasets, which demonstrates its effectiveness and the\npotential of transformer-based models for SOD.\n2. Related Work\n2.1. Deep Learning Based SOD\nCNN-based approaches have become a mainstream trend\nin both RGB and RGB-D SOD and achieved promising\nperformance. Most methods [24, 65, 49, 84, 16] lever-\naged a multi-level feature fusion strategy by using UNet\n[57] or HED-style [71] network structures. Some works\nintroduced the attention mechanism to learn more discrim-\ninative features, including spatial and channel attention\n[52, 80, 16, 7] or pixel-wise contextual attention [37]. Other\nworks [36, 64, 11, 42, 6] tried to design recurrent net-\nworks to reÔ¨Åne the saliency map step-by-step. In addi-\ntion, some works introduced multi-task learning, e.g., Ô¨Åxa-\ntion prediction [67], image caption [77], and edge detection\n[54, 82, 69, 79, 25] to boost the SOD performance.\nAs for RGB-D SOD, many methods have designed var-\nious models to fuse RGB and depth features and obtained\nsigniÔ¨Åcant results. Some models [4, 5, 18] adopted sim-\nple feature fusion methods, i.e., concatenation, summation,\nor multiplication. Some others [81, 30, 52, 31] leveraged\nthe depth cues to generate spatial or channel attention to\nenhance the RGB features. Besides, dynamic convolution\n[48], graph neural networks [43], and knowledge distillation\n[53] were also adopted to implement multi-modal feature\nfusion. In addition, [38, 39, 7] adopted the cross-attention\nmechanism to propagate long-range cross-modal interac-\ntions between RGB and depth cues.\nDifferent from previous CNN-based methods, we are the\nÔ¨Årst to rethink SOD from a sequence-to-sequence perspec-\ntive and propose a uniÔ¨Åed model based on pure transformer\nfor both RGB and RGB-D SOD. In our model, we follow\n[54, 82, 69, 79, 25] to leverage boundary detection to boost\nthe SOD performance. However, different from these CNN-\nbased models, we design a novel token-based multitask de-\ncoder to achieve this goal under the transformer framework.\n2.2. Transformers in Computer Vision\nVaswaniet al. [61] Ô¨Årst proposed a transformer encoder-\ndecoder architecture for machine translation, where multi-\nhead self-attention and point-wise feed-forward layers are\nstacked multiple times. Recently, more and more works\nhave introduced the Transformer model to various computer\nvision tasks and achieved excellent results. Some works\ncombined CNNs and transformers into hybrid architectures\nfor object detection [3, 91], panoptic segmentation [62],\nlane shape prediction [40], and so on. Typically, they Ô¨Årst\nuse CNNs to extract image features and then leverage the\nTransformer to incorporate long-range dependencies.\nOther works design pure transformer models to process\nimages from the sequence-to-sequence perspective. ViT\n[12] divided each image into a sequence of Ô¨Çattened 2D\npatches and then adopted the Transformer for image clas-\nsiÔ¨Åcation. Touvron et al. [60] introduced a teacher-student\nstrategy to improve the data-efÔ¨Åciency of ViT and Wang et\nal. [68] proposed a pyramid architecture to adapt ViT for\ndense prediction tasks. T2T-ViT [74] adopted the T2T mod-\nule to model local structures, thus generating multiscale to-\nken features. In this work, we adopt T2T-ViT as the back-\nbone and propose a novel multitask decoder and a reverse\nRT2T\nRT2T\nCross Modality Transformer\nTransformer Layer\nRT2T\nT2T\nTransformer Layer Transformer Layer\nT2T\nBoundary Token\nSaliency Token\nRT2T\nTransformer Layer\nùëá1\nùëá2\nùëá3 ùëáùëüùúÄ\nùëáùëë\nùúÄ\n√óùêø‚Ñ∞\nùëá3\nùíüùëáùê∂\nùë°ùë†\nùë°ùëè\n√óùêø3\nùíü\nTransformer Layer\nùëá2\nùíü\nùëá1\nùíü\n√óùêø‚Ñ∞ Transformer Layer\nTransformer Layer\nEncoder Convertor Decoder\nPatch-task AttentionPatch-task Attention\n√óùêø2\nùíü\n√óùêø1\nùíü\nFigure 1. Overall architecture of our proposed VST model for both RGB and RGB-D SOD. It Ô¨Årst uses an encoder to generate multi-\nlevel tokens from the input image patch sequence. Then, a convertor is adopted to convert the patch tokens to the decoder space, and\nalso performs cross-modal information fusion for RGB-D data. Finally, a decoder simultaneously predicts the saliency map and the\nboundary map via the proposed task-related tokens and the patch-task-attention mechanism. An RT2T transformation is also proposed to\nprogressively upsample patch tokens. The dotted line represents exclusive components for RGB-D SOD.\nT2T token upsampling method. It is noteworthy that our\nusage of task-related tokens is different from previous mod-\nels. In [12, 60], the class token is directly used for image\nclassiÔ¨Åcation via adopting a multilayer perceptron on the\ntoken embedding. However, we can not obtain dense pre-\ndiction results directly from a single task token. Thus, we\npropose to perform patch-task-attention between patch to-\nkens and the task tokens to predict saliency and boundary\nmaps. We believe our strategy will also inspire future trans-\nformer models for other dense prediction tasks.\nAnother related work to ours is [86], which introduces\ntransformer into the semantic segmentation task. The au-\nthors adopted a vision transformer as a backbone and then\nreshaped the token sequences to 2D image features. Then,\nthey predicted full-resolution segmentation maps using con-\nvolution and bilinear upsampling. Their model still falls\ninto the hybrid architecture category. In contrast, our model\nis a pure transformer architecture and does not rely on any\nconvolution operation and bilinear upsampling.\n3. Visual Saliency Transformer\nFigure 1 shows the overall architecture of our proposed\nVST model. The main components include a transformer\nencoder based on T2T-ViT, a transformer convertor to con-\nvert patch tokens from the encoder space to the decoder\nspace, and a multi-task transformer decoder.\n3.1. Transformer Encoder\nSimilar to other CNN-based SOD methods, which of-\nten utilize pretrained image classiÔ¨Åcation models such as\nVGG [59] and ResNet [23] as the backbone of their en-\ncoders to extract image features, we adopt the pretrained\nT2T-ViT [74] model as our backbone, as detailed below.\n3.1.1 Tokens to Token\nGiven a sequence of patch tokens T‚Ä≤with length l from the\nprevious layer, T2T-ViT iteratively applies the T2T module,\nwhich is composed of a re-structurization step and a soft\nsplit step, to model the local structure information inT‚Ä≤and\nobtain a new sequence of tokens.\nRe-structurization. As shown in Figure 2(a), the tokens\nT‚Ä≤is Ô¨Årst transformed using a transformer layer to obtain\nnew tokens T ‚ààRl√óc:\nT = MLP(MSA(T‚Ä≤)), (1)\nwhere MSA and MLP denote the multi-head self-attention\nand multilayer perceptron in the original Transformer [61],\nrespectively. Note that layer normalization [1] is applied\nbefore each block. Then, T is reshaped to a 2D image I ‚àà\nRh√ów√óc, where l = h √ów, to recover spatial structures, as\nshown in Figure 2(a).\nSoft split. After the re-structurization step, I is Ô¨Årst split\ninto k√ók patches with s overlapping. p zero-padding is also\nutilized to pad image boundaries. Then, the image patches\nare unfolded to a sequence of tokens To ‚ààRlo√óck2\n, where\nthe sequence length lo is computed as:\nlo = ho √ówo = ‚åäh + 2p ‚àík\nk ‚àís + 1‚åã√ó‚åä w + 2p ‚àík\nk ‚àís + 1‚åã.\n(2)\nDifferent from ViT [12], the overlapped patch splitting\nadopted in T2T-ViT introduces local correspondence within\nneighbouring patches, thus bringing spatial priors.\nThe T2T transformation can be conducted iteratively\nmultiple times. In each time, the re-structurization step Ô¨Årst\ntransforms previous token embeddings to new embeddings\nTransformer layer \n9\n8\n7\n6\n5\n4\n3\n2\n1 5\n4\n2\n1\n9\n8\n6\n5\nTransformer layer \n7 8 9\n4 5 6\n1 2 3Reshape Unfold FoldProject\n9\n8\n7\n6\n5\n4\n3\n2\n1\n5\n4\n2\n1\n9\n8\n6\n5\nReshape\n7 8 9\n4 5 6\n1 2 3\n(a) (b)\nFigure 2. (a) T2T module merges neighbouring tokens into a new token, thus reducing the length of tokens. (b) Our proposed reverse T2T\nmodule upsamples tokens by expanding each token into multiple sub-tokens.\nand also integrates long-range dependencies within all to-\nkens. Then, the soft split operation aggregates the tokens in\neach k √ók neighbour into a new token, which is ready to\nuse for the next layer. Furthermore, when settings < k‚àí1,\nthe length of tokens can be reduced progressively.\nWe follow [74] to Ô¨Årst soft split the input image into\npatches and then adopt the T2T module twice. Among the\nthree soft split steps, the patch sizes are set to k = [7, 3, 3],\nthe overlappings are set to s = [3, 1, 1], and the padding\nsizes are set to p = [2, 1, 1]. As such, we can obtain multi-\nlevel tokens T1 ‚ààRl1√óc, T2 ‚ààRl2√óc, and T3 ‚ààRl3√óc.\nGiven the width and height of the input image as H and\nW, respectively, then l1 = H\n4 √óW\n4 , l2 = H\n8 √óW\n8 , and\nl3 = H\n16 √óW\n16 . We follow [74] to set c = 64and use a linear\nprojection layer on T3 to transform its embedding dimen-\nsion from c to d = 384.\n3.1.2 Encoder with T2T-ViT Backbone\nThe Ô¨Ånal token sequence T3 is added with the sinusoidal\nposition embedding [61] to encode 2D position informa-\ntion. Then, LE transformer layers are used to model long-\nrange dependencies among T3 to extract powerful patch to-\nken embeddings TE‚ààRl3√ód.\nFor RGB SOD, we adopt a single transformer encoder to\nobtain RGB encoder patch tokens TE\nr ‚ààRl3√ód from each\ninput RGB image. For RGB-D SOD, we follow two-stream\narchitectures to further use another transformer encoder to\nextract the depth encoder patch tokens TE\nd from the input\ndepth map in a similar way, as shown in Figure 1.\n3.2. Transformer Convertor\nWe insert a convertor module between the transformer\nencoder and decoder to convert the encoder patch tokensTE\n‚àó\nfrom the encoder space to the decoder space, thus obtaining\nthe converted patch tokens TC‚ààRl3√ód.\n3.2.1 RGB-D Convertor\nWe fuseTE\nr and TE\nd in the RGB-D converter to integrate the\ncomplementary information between the RGB and depth\ndata. To this end, we design a Cross Modality Transformer\n(CMT), which consists of LC alternating cross-modality-\nattention layers and self-attention layers.\nCross-modality-attention. Under the pure transformer\narchitecture, we modify the standard self-attention layer\nto propagate long-range cross-modal dependencies be-\ntween the image and depth data, thus obtaining the cross-\nmodality-attention, which is detailed as follows.\nFirst, similar with the self-attention in [61], TE\nr is em-\nbedded to queries Qr ‚àà Rl3√ód, keys Kr ‚àà Rl3√ód, and\nvalues Vr ‚ààRl3√ód through three linear projections. Sim-\nilarly, we can obtain the depth queries Qd, keys Kd, and\nvalues Vd from TE\nd .\nNext, we compute the ‚ÄúScaled Dot-Product Attention‚Äù\n[61] between the queries from one modality with the keys\nfrom the other modality. Then, the output is computed as a\nweighted sum of the values, formulated as:\nAttention(Qr, Kd, Vd) =softmax(QrK‚ä§\nd /\n‚àö\nd)Vd,\nAttention(Qd, Kr, Vr) =softmax(QdK‚ä§\nr /\n‚àö\nd)Vr.\n(3)\nWe follow the standard Transformer architecture in [61]\nand adopt the multi-head attention mechanism in the cross-\nmodality-attention. The same positionwise feed-forward\nnetwork, residual connections, and layer normalization [1]\nare also used, forming our CMT layer.\nAfter each adoption of the proposed CMT layer, we use\none standard transformer layer on each RGB and depth\npatch token sequence, further enhancing their token embed-\ndings. After alternately using CMT and transformer for LC\ntimes, we fuse the obtained RGB tokens and depth tokens\nby concatenation and then project them to the Ô¨Ånal con-\nverted tokens TC, as shown in Figure 1.\n3.2.2 RGB Convertor\nTo align with our RGB-D SOD model, for RGB SOD, we\nsimply use LCstandard transformer layers on TE\nr to obtain\nthe converted patch token sequence TC.\n3.3. Multi-task Transformer Decoder\nOur decoder aims to decode the patch tokens TC to\nsaliency maps. Hence, we propose a novel token upsam-\npling method with multi-level token fusion and a token-\nbased multi-task decoder.\n3.3.1 Token Upsampling and Multi-level Token Fusion\nWe argue that directly predicting saliency maps from TC\ncan not obtain high-quality results since the length of TC\nis relatively small, i.e., l3 = H\n16 √óW\n16 , which is limited for\ndense prediction. Thus, we propose to upsample patch to-\nkens Ô¨Årst and then conduct dense prediction. Most CNN-\nbased methods [84, 82, 38, 18] adopt bilinear upsampling\nto recover large scale feature maps. Alternatively, we pro-\npose a new token upsampling method under the transformer\nframework. Inspired by the T2T module [74] that aggre-\ngates neighbour tokens to reduce the length of tokens pro-\ngressively, we propose a reverse T2T (RT2T) transforma-\ntion to upsample tokens by expanding each token into mul-\ntiple sub-tokens, as shown in Figure 2(b).\nSpeciÔ¨Åcally, we Ô¨Årst project the input patch tokens to re-\nduce their embedding dimension from d = 384to c = 64.\nThen, we use another linear projection to expand the em-\nbedding dimension from c to ck2. Next, similar to the soft\nsplit step in T2T, each token is seen as a k √ók image patch\nand neighbouring patches have s overlapping. Then, we\ncan fold the tokens as an image using p zero-padding. The\noutput image size can be computed using (2) reversely, i.e.,\ngiven the length of the input patch tokens as ho √ówo, the\nspatial size of the out image is h √ów. Finally, we reshape\nthe image back to the upsampled tokens with size lo √óc,\nwhere lo = h √ów. By setting s < k‚àí1, the RT2T trans-\nformation can increase the length of the tokens. Motivated\nby T2T-ViT, we use RT2T three times and setk = [3, 3, 7],\ns = [1, 1, 3], and p = [1, 1, 3]. Thus, the length of the patch\ntokens can be gradually upsampled to H √óW, equaling to\nthe original size of the input image.\nFurthermore, motivated by the widely proved successes\nof multi-level feature fusion in existing SOD methods\n[24, 49, 84, 16, 43], we leverage low-level tokens with\nlarger lengths from the T2T-ViT encoder, i.e., T1 and T2,\nto provide accurate local structural information. For both\nRGB and RGB-D SOD, we only use the low-level tokens\nfrom the RGB transformer encoder. Concretely, we pro-\ngressively fuse T2 and T1 with the upsampled patch tokens\nvia concatenation and linear projection. Then, we adopt one\ntransformer layer to obtain the decoder tokens TD\ni at each\nlevel i, where i = 2, 1. The whole process is formulated as:\nTD\ni = MLP(MSA(Linear([RT2T(TD\ni+1), Ti])), (4)\nwhere [, ] means concatenation along the token embedding\ndimension. ‚ÄúLinear‚Äù means linear projection to reduce the\nembedding dimension after the concatenation to c. Finally,\nwe use another linear projection to recover the embedding\ndimension of TD\ni back to d.\n3.3.2 Token Based Multi-task Prediction\nInspired by existing pure transformer methods [74, 12],\nwhich add a class token on the patch token sequence for\nimage classiÔ¨Åcation, we also leverage task-related tokens to\npredict results. However, we can not obtain dense predic-\ntion results by directly using MLP on the task token embed-\nding, as done in [74, 12]. Hence, we propose to perform\npatch-task-attention between the patch tokens and the task-\nrelated token to perform SOD.\nIn addition, motivated by the widely used boundary de-\ntection in SOD models [82, 69, 79, 25], we also adopt the\nmulti-task learning strategy to jointly perform saliency and\nboundary detection, thus using the latter to help boost the\nperformance of the former.\nTo this end, we design two task-related tokens, i.e., a\nsaliency token ts ‚ààR1√ód and a boundary token tb ‚ààR1√ód.\nAt each decoder level i, we add the saliency and boundary\ntokens ts and tb on the patch token sequence TD\ni , and then\nprocess them usingLD\ni transformer layers. As such, the two\ntask tokens can learn image-dependent task-related embed-\ndings from the interaction with the patch tokens. After this,\nwe take the updated patch tokens as input and perform the\ntoken upsampling and multi-level fusion process in (4) to\nobtain upsampled patch tokens TD\ni‚àí1. Next, we reuse the\nupdated ts and tb in the next level i ‚àí1 to further update\nthem and TD\ni‚àí1. We repeat this process until we reach the\nlast decoder level with the 1\n4 scale.\nFor saliency and boundary prediction, we perform patch-\ntask-attention between the Ô¨Ånal decoder patch tokens TD\n1\nand the saliency and boundary tokens ts and tb. For\nsaliency prediction, we Ô¨Årst embed TD\n1 to queries QD\ns ‚àà\nRl1√ód and embed ts to a key Ks ‚àà R1√ód and a value\nVs ‚ààR1√ód. Similarly, for boundary prediction, we embed\nTD\n1 to QD\nb and embed tb to Kb and Vb. Then, we adopt the\npatch-task-attention to obtain the task-related patch tokens:\nTD\ns = sigmoid(QD\ns K‚ä§\ns /\n‚àö\nd)Vs + TD\n1 ,\nTD\nb = sigmoid(QD\nb K‚ä§\nb /\n‚àö\nd)Vb + TD\n1 .\n(5)\nHere we use the sigmoid activation for the attention compu-\ntation since in each equation we only have one key.\nSince TD\ns and TD\nb are at the 1\n4 scale, we adopt the third\nRT2T transformation to upsample them to the full resolu-\ntion. Finally, we apply two linear transformations with the\nsigmoid activation to project them to scalars in [0, 1], and\nthen reshape them to a 2D saliency map and a 2D boundary\nmap, respectively. The whole process is given in Figure 1.\n4. Experiments\n4.1. Datasets and Evaluation Metrics\nFor RGB SOD, we evaluate our VST model on six\nwidely used benchmark datasets, including ECSSD [72]\nTable 1. Ablation studies of our proposed model. ‚ÄúBili‚Äù denotes bilinear upsampling. ‚ÄúF‚Äù means multi-level token fusion. ‚ÄúTMD‚Äù\ndenotes our proposed token-based multi-task decoder, while ‚ÄúC2D‚Äù means using conventional two-stream decoder to perform saliency and\nboundary detection without using task-related tokens. The best results are labeled in blue.\nSettings NJUD [26] DUTLF-Depth [52] STERE [46] LFSD [33]\nSm ‚Üë maxF ‚Üë Emax\nŒæ ‚Üë MAE ‚Üì Sm ‚Üë maxF ‚Üë Emax\nŒæ ‚Üë MAE ‚Üì Sm ‚Üë maxF ‚Üë Emax\nŒæ ‚Üë MAE ‚Üì Sm ‚Üë maxF ‚Üë Emax\nŒæ ‚Üë MAE ‚Üì\nBaseline 0.869 0.862 0.931 0.073 0.889 0.887 0.942 0.062 0.868 0.853 0.927 0.075 0.842 0.845 0.893 0.103\n+CMT 0.873 0.867 0.934 0.072 0.889 0.890 0.942 0.063 0.869 0.854 0.928 0.075 0.849 0.855 0.900 0.100\n+CMT+Bili 0.906 0.902 0.944 0.045 0.926 0.930 0.961 0.032 0.889 0.877 0.939 0.051 0.856 0.858 0.895 0.081\n+CMT+RT2T 0.915 0.915 0.951 0.039 0.934 0.940 0.964 0.028 0.896 0.889 0.943 0.046 0.867 0.873 0.903 0.073\n+CMT+RT2T+F 0.923 0.923 0.954 0.035 0.936 0.943 0.963 0.028 0.910 0.903 0.947 0.040 0.876 0.880 0.909 0.067\n+CMT+RT2T+F+TMD 0.922 0.920 0.951 0.035 0.943 0.948 0.969 0.024 0.913 0.907 0.951 0.038 0.882 0.889 0.921 0.061\n+CMT+RT2T+F+C2D 0.922 0.921 0.954 0.036 0.941 0.947 0.968 0.026 0.911 0.906 0.949 0.040 0.874 0.878 0.909 0.069\n(1,000 images), HKU-IS [32] (4,447 images), PASCAL-\nS [34] (850 images), DUT-O [73] (5,168 images), SOD\n[45] (300 images), and DUTS [63] (10,553 training im-\nages and 5,019 testing images). For RGB-D SOD, we use\nnine widely used benchmark datasets: STERE [46] (1,000\nimage pairs), LFSD [33] (100 image pairs), RGBD135\n[9] (135 image pairs), SSD [90] (80 image pairs), NJUD\n[26] (1,985 image pairs), NLPR [51] (1,000 image pairs),\nDUTLF-Depth [52] (1,200 image pairs),SIP [15] (929 im-\nage pairs), and ReDWeb-S [39] (3,179 image pairs).\nWe adopt four widely used evaluation metrics to evalu-\nate our model performance comprehensively. SpeciÔ¨Åcally,\nStructure-measure Sm [13] evaluates region-aware and\nobject-aware structural similarity. Maximum F-measure\n(maxF) jointly considers precision and recall under the op-\ntimal threshold. Maximum enhanced-alignment measure\nEmax\nŒæ [14] simultaneously considers pixel-level errors and\nimage-level errors. Mean Absolute Error (MAE) computes\npixel-wise average absolute error. To evaluate the model\ncomplexity, we also report the multiply accumulate opera-\ntions (MACs) and the number of parameters (Params).\n4.2. Implementation Details\nFor fair comparisons, we follow most previous methods\nto use the training set of DUTS to train our VST for RGB\nSOD and use 1,485 images from NJUD, 700 images from\nNLPR, and 800 images from DUTLF-Depth to train our\nVST for RGB-D SOD. We follow [82] to use a sober opera-\ntor to generate the boundary ground truth from GT saliency\nmaps. For depth data preprocessing, we normalize the depth\nmaps to [0,1] and duplicate them to three channels. Finally,\nwe resize each image or depth map to 256 √ó256 pixels and\nthen randomly crop 224 √ó224 image regions as the model\ninput and use random Ô¨Çipping as data augmentation.\nWe use the pre-trained T2T-ViT t-14 [74] model as our\nbackbone since it has similar computational complexity as\nResNet50 [23] does. This model uses the efÔ¨Åcient Per-\nformer [10] and c = 64in T2T modules, and sets LE= 14.\nIn our convertor and decoder, we set LC = LD\n3 = 4 and\nLD\n2 = LD\n1 = 2 according to experimental results. We\nset the batchsizes as 11 and 8, and the total training steps\nas 40,000 and 60,000, for RGB and RGB-D SOD, respec-\ntively. For both of them, Adam [27] is adopted as the op-\ntimizer and the binary cross entropy loss is used for both\nsaliency and boundary prediction. The initial learning rate\nis set to 0.0001 and reduced by a factor of 10 at half and\nthree-quarters of the total step, respectively. Deep supervi-\nsion is also used to facilitate the model training, where we\nuse the patch-task attention to predict saliency and bound-\nary at each decoder level. We implemented our model using\nPytorch [50] and trained it on a GTX 1080 Ti GPU.\n4.3. Ablation Study\nSince our RGB-D VST is built by adding one more trans-\nformer encoder and additional CMT based on our RGB\nVST, while the other parts of the two models are the same,\nwe conduct ablation studies based on our RGB-D VST to\nverify all of our proposed model components. The exper-\nimental results on four RGB-D SOD datasets, i.e., NJUD,\nDUTLF-Depth, STERE, and LFSD, are given in Table 1.\nWe remove the transformer convertor and the decoder from\nour RGB-D VST as the baseline model. SpeciÔ¨Åcally, it uses\nthe two-stream transformer encoder to extract RGB encoder\npatch tokens TE\nr and the depth encoder patch tokens TE\nd ,\nand then directly concatenate them and predict the saliency\nmap with 1/16 scale by using MLP on each patch token.\nEffectiveness of CMT. For cross-modal information fu-\nsion, we deploy our proposed CMT right after the trans-\nformer encoder to substitute the concatenation fusion\nmethod in the baseline model, shown as ‚Äú+CMT‚Äù in Table 1.\nCompared to the baseline, CMT brings performance gain\nespecially on the NJUD and LFSD datasets, hence demon-\nstrating its effectiveness.\nEffectiveness of RT2T. Based on ‚Äú+CMT‚Äù model, we\nfurther simply use bilinear upsampling (‚Äú+CMT+Bili‚Äù) to\nprogressively upsample tokens to the full resolution and\nthen predict the saliency map. The results show using bi-\nlinear upsampling to increase the resolution of the saliency\nmap can largely improve the model performance. Then, we\nreplace bilinear upsampling with our proposed RT2T token\nupsampling method (‚Äú+CMT+RT2T‚Äù). We Ô¨Ånd that RT2T\nleads to obvious performance improvement compared with\nusing bilinear upsampling, which veriÔ¨Åes its effectiveness.\nEffectiveness of multi-level token fusion. We progres-\nsively fuseT1 and T2 in our decoder (‚Äú+CMT+RT2T+F‚Äù) to\nTable 2. Quantitative comparison of our proposed VST with other 14 SOTA RGB-D SOD methods on 9 benchmark datasets.Red and blue\ndenote the best and the second-best results, respectively. ‚Äò-‚Äô indicates the code or result is not available.\nDataset Metric A2dele JL-DCF SSF-RGBD UC-Net S2MA PGAR DANet cmMS ATST CMW Cas-Gnn HDFNet CoNet BBS-Net VST\n[53] [18] [79] [76] [38] [6] [85] [29] [78] [31] [43] [48] [25] [16]\nMACs (G) 41.86 211.06 46.56 16.16 141.19 44.65 66.25 134.77 42.17 208.03 - 91.77 20.89 31.2 30.99\nParams (M) 30.34 143.52 32.93 31.26 86.65 16.2 26.68 92.02 32.17 85.65 - 44.15 43.66 49.77 83.83\nNJUD\nSm ‚Üë 0.871 0.902 0.899 0.897 0.894 0.909 0.899 0.900 0.885 0.870 0.911 0.908 0.896 0.921 0.922\nmaxF‚Üë 0.874 0.904 0.896 0.895 0.889 0.907 0.898 0.897 0.893 0.871 0.916 0.911 0.893 0.919 0.920\nEmax\nŒæ ‚Üë 0.916 0.944 0.935 0.936 0.930 0.940 0.935 0.936 0.930 0.927 0.948 0.944 0.937 0.949 0.951\n[26] MAE ‚Üì 0.051 0.041 0.043 0.043 0.054 0.042 0.046 0.044 0.047 0.061 0.036 0.039 0.046 0.035 0.035\nNLPR\nSm ‚Üë 0.899 0.925 0.915 0.920 0.916 0.917 0.920 0.919 0.909 0.917 0.919 0.923 0.912 0.931 0.932\nmaxF‚Üë 0.882 0.918 0.896 0.903 0.902 0.897 0.909 0.904 0.898 0.903 0.906 0.917 0.893 0.918 0.920\nEmax\nŒæ ‚Üë 0.944 0.963 0.953 0.956 0.953 0.950 0.955 0.955 0.951 0.951 0.955 0.963 0.948 0.961 0.962\n[51] MAE ‚Üì 0.029 0.022 0.027 0.025 0.030 0.027 0.027 0.028 0.027 0.029 0.025 0.023 0.027 0.023 0.024\nSm ‚Üë 0.885 0.906 0.915 0.871 0.904 0.899 0.899 0.912 0.916 0.797 0.920 0.908 0.923 0.882 0.943\nDUTLF maxF ‚Üë 0.891 0.910 0.923 0.864 0.899 0.898 0.904 0.913 0.928 0.779 0.926 0.915 0.932 0.870 0.948\n-Depth Emax\nŒæ ‚Üë 0.928 0.941 0.950 0.908 0.935 0.933 0.939 0.940 0.953 0.864 0.953 0.945 0.959 0.912 0.969\n[52] MAE ‚Üì 0.043 0.042 0.033 0.059 0.043 0.041 0.042 0.036 0.033 0.098 0.030 0.041 0.029 0.058 0.024\nReDWeb-S\nSm ‚Üë 0.641 0.734 0.595 0.713 0.711 0.656 - 0.699 0.679 0.634 - 0.728 0.696 0.693 0.759\nmaxF‚Üë 0.603 0.727 0.558 0.710 0.696 0.632 - 0.677 0.673 0.607 - 0.717 0.693 0.680 0.763\nEmax\nŒæ ‚Üë 0.674 0.805 0.710 0.794 0.781 0.749 - 0.767 0.758 0.714 - 0.804 0.782 0.763 0.826\n[39] MAE ‚Üì 0.160 0.128 0.189 0.130 0.139 0.161 - 0.143 0.155 0.195 - 0.129 0.147 0.150 0.113\nSTERE\nSm ‚Üë 0.879 0.903 0.837 0.903 0.890 0.894 0.901 0.894 0.896 0.852 0.899 0.900 0.905 0.908 0.913\nmaxF‚Üë 0.880 0.904 0.840 0.899 0.882 0.880 0.892 0.887 0.901 0.837 0.901 0.900 0.901 0.903 0.907\nEmax\nŒæ ‚Üë 0.928 0.947 0.912 0.944 0.932 0.929 0.937 0.935 0.942 0.907 0.944 0.943 0.947 0.942 0.951\n[46] MAE ‚Üì 0.045 0.040 0.065 0.039 0.051 0.045 0.044 0.045 0.038 0.067 0.039 0.042 0.037 0.041 0.038\nSSD\nSm ‚Üë 0.803 0.860 0.790 0.865 0.868 0.832 0.864 0.857 0.850 0.798 0.872 0.879 0.851 0.863 0.889\nmaxF‚Üë 0.777 0.833 0.762 0.855 0.848 0.798 0.843 0.839 0.853 0.771 0.863 0.870 0.837 0.843 0.876\nEmax\nŒæ ‚Üë 0.862 0.902 0.867 0.907 0.909 0.872 0.914 0.900 0.920 0.871 0.923 0.925 0.917 0.914 0.935\n[90] MAE ‚Üì 0.070 0.053 0.084 0.049 0.053 0.068 0.050 0.053 0.052 0.085 0.047 0.046 0.056 0.052 0.045\nRGBD135\nSm ‚Üë 0.886 0.931 0.904 0.934 0.941 0.886 0.924 0.934 0.917 0.934 0.894 0.926 0.914 0.934 0.943\nmaxF‚Üë 0.872 0.923 0.885 0.930 0.935 0.864 0.914 0.928 0.916 0.931 0.894 0.921 0.902 0.928 0.940\nEmax\nŒæ ‚Üë 0.921 0.968 0.940 0.976 0.973 0.924 0.966 0.969 0.961 0.969 0.937 0.970 0.948 0.966 0.978\n[9] MAE ‚Üì 0.029 0.021 0.026 0.019 0.021 0.032 0.023 0.018 0.022 0.022 0.028 0.022 0.024 0.021 0.017\nLFSD\nSm ‚Üë 0.825 0.853 0.851 0.856 0.829 0.808 0.841 0.845 0.845 0.776 0.838 0.846 0.848 0.835 0.882\nmaxF‚Üë 0.828 0.863 0.863 0.860 0.831 0.794 0.840 0.858 0.859 0.779 0.843 0.858 0.852 0.828 0.889\nEmax\nŒæ ‚Üë 0.866 0.894 0.892 0.898 0.865 0.853 0.874 0.886 0.893 0.834 0.880 0.889 0.895 0.870 0.921\n[33] MAE ‚Üì 0.084 0.077 0.074 0.074 0.102 0.099 0.087 0.082 0.078 0.130 0.081 0.085 0.076 0.092 0.061\nSIP\nSm ‚Üë 0.829 0.880 0.799 0.875 0.872 0.838 0.875 0.872 0.849 0.705 - 0.886 0.860 0.879 0.904\nmaxF‚Üë 0.834 0.889 0.786 0.879 0.877 0.827 0.876 0.876 0.861 0.677 - 0.894 0.873 0.884 0.915\nEmax\nŒæ ‚Üë 0.890 0.925 0.870 0.919 0.919 0.886 0.918 0.911 0.901 0.804 - 0.930 0.917 0.922 0.944\n[15] MAE ‚Üì 0.070 0.049 0.091 0.051 0.058 0.073 0.055 0.058 0.063 0.141 - 0.048 0.058 0.055 0.040\nsupply low-level Ô¨Åne-grained information. We Ô¨Ånd that this\nstrategy further improves the model performance. Hence,\nleveraging low-level tokens in transformer is as important\nas fusing low-level features in CNN-based models.\nEffectiveness of the multi-task transformer decoder.\nBased on ‚Äú+CMT+RT2T+F‚Äù, we further use our token-\nbased multi-task decoder (TMD) to jointly perform saliency\nand boundary detection (‚Äú+CMT+RT2T+F+TMD‚Äù). It\nshows that using boundary detection can bring further per-\nformance gain for SOD on three out of four datasets.\nTo very the effectiveness of our token-based prediction\nscheme, we try to directly use a conventional two-stream\ndecoder (C2D) by using the ‚Äú+RT2T+F‚Äù architecture twice\nto predict the saliency map and boundary map via MLP,\nwithout using task-related tokens. This model is denoted\nas ‚Äú+CMT+RT2T+F+C2D‚Äù in Table 1. The parameters and\nMACs of TMD vs. C2D are 17.22 M vs. 20.35 M and\n17.70 G vs. 28.27 G, respectively. The results show that\nusing our TMD can achieve better results than using C2D\non three out of four datasets, and also with much less com-\nputational costs. This clearly demonstrates the superiority\nof our proposed token-based transformer decoder.\n4.4. Comparison with State-of-the-Art Methods\nFor RGB-D SOD, we compare our VST with 14 state-\nof-the-art RGB-D SOD methods, i.e., A2dele [53], JL-DCF\n[18], SSF-RGBD [79], UC-Net [76], S2MA [38], PGAR\n[6], DANet [85], cmMS [29], ATSA [78], CMW [31], Cas-\nGnn [43], HDFNet [48], CoNet [25], and BBS-Net [16].\nFor RGB SOD, we compare our VST with 12 state-of-the-\nart RGB SOD models, including GateNet [84], CSF [20],\nLDF [69], MINet [49], ITSD [87], EGNet [82], TSPOANet\n[41], AFNet [17], PoolNet [35], CPD [70], BASNet [55],\nand PiCANet [37]. Table 2 and Table 3 show the quantita-\ntive comparison results for RGB-D and RGB SOD, respec-\nTable 3. Quantitative comparison of our proposed VST with other 12 SOTA RGB SOD methods on 6 benchmark datasets. ‚Äú-R‚Äù and ‚Äú-R2‚Äù\nmeans the ResNet50 and Res2Net backbone, respectively.\nDataset Metric PiCANet BASNet CPD-R PoolNet AFNet TSPOANet EGNet-R ITSD-R MINet-R LDF-R CSF-R2 GateNet-R VST\n[37] [55] [70] [35] [17] [41] [82] [87] [49] [69] [20] [84]\nMACs (G) 54.05 127.36 17.77 88.89 21.66 - 157.21 15.96 87.11 15.51 18.96 162.13 23.16\nParams (M) 47.22 87.06 47.85 68.26 35.95 - 111.64 26.47 162.38 25.15 36.53 128.63 44.48\nDUTS\nSm ‚Üë 0.863 0.866 0.869 0.879 0.867 0.860 0.887 0.885 0.884 0.892 0.890 0.891 0.896\nmaxF‚Üë 0.840 0.838 0.840 0.853 0.838 0.828 0.866 0.867 0.864 0.877 0.869 0.874 0.877\nEmax\nŒæ ‚Üë 0.915 0.902 0.913 0.917 0.910 0.907 0.926 0.929 0.926 0.930 0.929 0.932 0.939\n[63] MAE ‚Üì 0.040 0.047 0.043 0.041 0.045 0.049 0.039 0.041 0.037 0.034 0.037 0.038 0.037\nECSSD\nSm ‚Üë 0.916 0.916 0.918 0.917 0.914 0.907 0.925 0.925 0.925 0.925 0.931 0.924 0.932\nmaxF‚Üë 0.929 0.931 0.926 0.929 0.924 0.919 0.936 0.939 0.938 0.938 0.942 0.935 0.944\nEmax\nŒæ ‚Üë 0.953 0.951 0.951 0.948 0.947 0.942 0.955 0.959 0.957 0.954 0.960 0.955 0.964\n[72] MAE ‚Üì 0.035 0.037 0.037 0.042 0.042 0.047 0.037 0.035 0.034 0.034 0.033 0.038 0.034\nHKU-IS\nSm ‚Üë 0.905 0.909 0.906 0.916 0.905 0.902 0.918 0.917 0.919 0.920 - 0.921 0.928\nmaxF‚Üë 0.913 0.919 0.911 0.920 0.910 0.909 0.923 0.926 0.926 0.929 - 0.926 0.937\nEmax\nŒæ ‚Üë 0.951 0.952 0.950 0.955 0.949 0.950 0.956 0.960 0.960 0.958 - 0.959 0.968\n[32] MAE ‚Üì 0.031 0.032 0.034 0.032 0.036 0.039 0.031 0.031 0.029 0.028 - 0.031 0.030\nPASCAL-S\nSm ‚Üë 0.846 0.837 0.847 0.852 0.849 0.841 0.852 0.861 0.856 0.861 0.863 0.863 0.873\nmaxF‚Üë 0.824 0.819 0.817 0.830 0.824 0.817 0.825 0.839 0.831 0.839 0.839 0.836 0.850\nEmax\nŒæ ‚Üë 0.882 0.868 0.872 0.880 0.877 0.871 0.874 0.889 0.883 0.888 0.885 0.886 0.900\n[34] MAE ‚Üì 0.072 0.083 0.077 0.076 0.076 0.082 0.080 0.071 0.071 0.067 0.073 0.071 0.067\nDUT-O\nSm ‚Üë 0.826 0.836 0.825 0.832 0.826 0.818 0.841 0.840 0.833 0.839 0.838 0.840 0.850\nmaxF‚Üë 0.767 0.779 0.754 0.769 0.759 0.750 0.778 0.792 0.769 0.782 0.775 0.782 0.800\nEmax\nŒæ ‚Üë 0.865 0.872 0.868 0.869 0.861 0.858 0.878 0.880 0.869 0.870 0.869 0.878 0.888\n[73] MAE ‚Üì 0.054 0.057 0.056 0.056 0.057 0.062 0.053 0.061 0.056 0.052 0.055 0.055 0.058\nSOD\nSm ‚Üë 0.813 0.799 0.797 0.823 0.811 0.802 0.824 0.835 0.830 0.831 0.826 0.827 0.854\nmaxF‚Üë 0.824 0.808 0.804 0.832 0.819 0.809 0.831 0.849 0.835 0.841 0.832 0.835 0.866\nEmax\nŒæ ‚Üë 0.871 0.846 0.860 0.873 0.867 0.852 0.875 0.889 0.878 0.878 0.883 0.877 0.902\n[45] MAE ‚Üì 0.073 0.091 0.089 0.085 0.085 0.094 0.080 0.075 0.074 0.071 0.079 0.079 0.065\nImage Depth GT VST BBS-Net\n[16]\nCoNet\n[25]\nHDFNet\n[48]\nJLDCF\n[18]\nUC-Net\n[76]\nImage GT VST GateNet\n[84]\nCSF\n[20]\nLDF\n[69]\nMINet\n[49]\nITSD\n[87]\nEGNet\n[82]\nFigure 3. Qualitative comparison against state-of-the-art RGB-D (left) and RGB (right) SOD methods. (GT: ground truth)\ntively. The results show that our VST outperforms all previ-\nous state-of-the-art CNN-based SOD models on both RGB\nand RGB-D benchmark datasets, with comparable number\nof parameters and relatively small MACs, hence demon-\nstrating the great effectiveness of our VST. We also show\nvisual comparison results among best-performed models\nin Figure 3. It shows our proposed VST can accurately\ndetect salient objects in very challenging scenarios, e.g.,\nbig salient objects, cluttered backgrounds, foreground and\nbackground having similar appearances, etc.\n5. Conclusion\nIn this paper, we are the Ô¨Årst to rethink SOD from a\nsequence-to-sequence perspective and develop a novel uni-\nÔ¨Åed model based on a pure transformer, for both RGB and\nRGB-D SOD. To handle the difÔ¨Åculty of applying trans-\nformers in dense prediction tasks, we propose a new to-\nken upsampling method under the transformer framework\nand fuse multi-level patch tokens. We also design a multi-\ntask decoder by introducing task-related tokens and a novel\npatch-task-attention mechanism to jointly perform saliency\nand boundary detection. Our VST model achieves state-of-\nthe-art results for both RGB and RGB-D SOD without rely-\ning on heavy computational costs, thus showing its great ef-\nfectiveness. We also set a new paradigm for the open ques-\ntion of how to use transformer in dense prediction tasks.\nAcknowledgments: This work was supported in part\nby the National Key R&D Program of China under\nGrant 2020AAA0105702, the National Science Foundation\nof China under Grant 62027813, 62036005, U20B2065,\nU20B2068.\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. Layer normalization. arXiv preprint arXiv:1607.06450,\n2016. 3, 4\n[2] Ali Borji and Laurent Itti. Exploiting local and global patch\nrarities for saliency detection. In CVPR, pages 478‚Äì485,\n2012. 1\n[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-\nto-end object detection with transformers. In ECCV, pages\n213‚Äì229, 2020. 2\n[4] Hao Chen and Youfu Li. Progressively complementarity-\naware fusion network for rgb-d salient object detection. In\nCVPR, pages 3051‚Äì3060, 2018. 1, 2\n[5] Hao Chen and Youfu Li. Three-stream attention-aware net-\nwork for rgb-d salient object detection. TIP, 28(6):2825‚Äì\n2835, 2019. 2\n[6] Shuhan Chen and Yun Fu. Progressively guided alternate\nreÔ¨Ånement network for rgb-d salient object detection. In\nECCV, pages 520‚Äì538, 2020. 2, 7, 14\n[7] Zuyao Chen, Runmin Cong, Qianqian Xu, and Qingming\nHuang. Dpanet: Depth potentiality-aware gated attention\nnetwork for rgb-d salient object detection. TIP, 2020. 1,\n2\n[8] Ming-Ming Cheng, Niloy J Mitra, Xiaolei Huang, Philip HS\nTorr, and Shi-Min Hu. Global contrast based salient region\ndetection. TPAMI, 37(3):569‚Äì582, 2014. 1\n[9] Yupeng Cheng, Huazhu Fu, Xingxing Wei, Jiangjian Xiao,\nand Xiaochun Cao. Depth enhanced saliency detection\nmethod. In Conference on Internet Multimedia Computing\nand Service, pages 23‚Äì27, 2014. 6, 7\n[10] Krzysztof Choromanski, Valerii Likhosherstov, David Do-\nhan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter\nHawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser,\net al. Rethinking attention with performers. In ICLR, 2020.\n6\n[11] Zijun Deng, Xiaowei Hu, Lei Zhu, Xuemiao Xu, Jing Qin,\nGuoqiang Han, and Pheng-Ann Heng. R3net: Recurrent\nresidual reÔ¨Ånement network for saliency detection. In IJCAI,\npages 684‚Äì690, 2018. 2\n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. In ICLR, 2020. 1, 2,\n3, 5\n[13] Deng-Ping Fan, Ming-Ming Cheng, Yun Liu, Tao Li, and Ali\nBorji. Structure-measure: A new way to evaluate foreground\nmaps. In ICCV, pages 4548‚Äì4557, 2017. 6\n[14] Deng-Ping Fan, Cheng Gong, Yang Cao, Bo Ren, Ming-\nMing Cheng, and Ali Borji. Enhanced-alignment Measure\nfor Binary Foreground Map Evaluation. In IJCAI, pages\n698‚Äì704, 2018. 6\n[15] Deng-Ping Fan, Zheng Lin, Zhao Zhang, Menglong Zhu, and\nMing-Ming Cheng. Rethinking rgb-d salient object detec-\ntion: Models, data sets, and large-scale benchmarks.TNNLS,\n32(5):2075‚Äì2089, 2020. 6, 7\n[16] Deng-Ping Fan, Yingjie Zhai, Ali Borji, Jufeng Yang, and\nLing Shao. Bbs-net: Rgb-d salient object detection with a\nbifurcated backbone strategy network. In ECCV, pages 275‚Äì\n292, 2020. 1, 2, 5, 7, 8, 14\n[17] Mengyang Feng, Huchuan Lu, and Errui Ding. Attentive\nfeedback network for boundary-aware salient object detec-\ntion. In CVPR, pages 1623‚Äì1632, 2019. 7, 8, 13\n[18] Keren Fu, Deng-Ping Fan, Ge-Peng Ji, and Qijun Zhao. Jl-\ndcf: Joint learning and densely-cooperative fusion frame-\nwork for rgb-d salient object detection. In CVPR, pages\n3052‚Äì3062, 2020. 1, 2, 5, 7, 8, 14\n[19] Chuang Gan, Naiyan Wang, Yi Yang, Dit-Yan Yeung, and\nAlex G Hauptmann. Devnet: A deep event network for mul-\ntimedia event detection and evidence recounting. In CVPR,\npages 2568‚Äì2577, 2015. 1\n[20] Shang-Hua Gao, Yong-Qiang Tan, Ming-Ming Cheng,\nChengze Lu, Yunpeng Chen, and Shuicheng Yan. Highly\nefÔ¨Åcient salient object detection with 100k parameters. In\nECCV, pages 702‚Äì721, 2020. 7, 8, 13\n[21] Stas Goferman, Lihi Zelnik-Manor, and Ayellet Tal.\nContext-aware saliency detection. TPAMI, 34(10):1915‚Äì\n1926, 2011. 1\n[22] Junwei Han, Hao Chen, Nian Liu, Chenggang Yan, and Xue-\nlong Li. Cnns-based rgb-d saliency detection via cross-view\ntransfer and multiview fusion. IEEE transactions on cyber-\nnetics, 48(11):3171‚Äì3183, 2017. 1\n[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR,\npages 770‚Äì778, 2016. 3, 6\n[24] Q Hou, MM Cheng, X Hu, A Borji, Z Tu, and PHS Torr.\nDeeply supervised salient object detection with short con-\nnections. TPAMI, 41(4):815‚Äì828, 2018. 1, 2, 5\n[25] Wei Ji, Jingjing Li, Miao Zhang, Yongri Piao, and Huchuan\nLu. Accurate rgb-d salient object detection via collaborative\nlearning. In ECCV, pages 52‚Äì69, 2020. 1, 2, 5, 7, 8, 14\n[26] Ran Ju, Ling Ge, Wenjing Geng, Tongwei Ren, and Gang-\nshan Wu. Depth saliency based on anisotropic center-\nsurround difference. In ICIP, pages 1115‚Äì1119, 2014. 6,\n7, 13\n[27] Diederik P. Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. In ICLR, 2015. 6\n[28] Yann LeCun, L ¬¥eon Bottou, Yoshua Bengio, and Patrick\nHaffner. Gradient-based learning applied to document recog-\nnition. Proceedings of the IEEE , 86(11):2278‚Äì2324, 1998.\n1\n[29] Chongyi Li, Runmin Cong, Yongri Piao, Qianqian Xu, and\nChen Change Loy. Rgb-d salient object detection with cross-\nmodality modulation and selection. In ECCV, pages 225‚Äì\n241, 2020. 7, 14\n[30] Gongyang Li, Zhi Liu, and Haibin Ling. Icnet: Information\nconversion network for rgb-d based salient object detection.\nTIP, 29:4873‚Äì4884, 2020. 2\n[31] Gongyang Li, Zhi Liu, Linwei Ye, Yang Wang, and Haibin\nLing. Cross-modal weighting network for rgb-d salient ob-\nject detection. In ECCV, pages 665‚Äì681, 2020. 1, 2, 7, 14\n[32] Guanbin Li and Yizhou Yu. Visual saliency based on multi-\nscale deep features. In CVPR, pages 5455‚Äì5463, 2015. 6, 8,\n13\n[33] Nianyi Li, Jinwei Ye, Yu Ji, Haibin Ling, and Jingyi Yu.\nSaliency detection on light Ô¨Åeld. InCVPR, pages 2806‚Äì2813,\n2014. 6, 7, 13\n[34] Yin Li, Xiaodi Hou, Christof Koch, James M Rehg, and\nAlan L Yuille. The secrets of salient object segmentation.\nIn CVPR, pages 280‚Äì287, 2014. 6, 8, 13\n[35] Jiang-Jiang Liu, Qibin Hou, Ming-Ming Cheng, Jiashi Feng,\nand Jianmin Jiang. A simple pooling-based design for real-\ntime salient object detection. In CVPR, pages 3917‚Äì3926,\n2019. 7, 8, 13\n[36] Nian Liu and Junwei Han. Dhsnet: Deep hierarchical\nsaliency network for salient object detection. InCVPR, pages\n678‚Äì686, 2016. 1, 2\n[37] Nian Liu, Junwei Han, and Ming-Hsuan Yang. Picanet:\nLearning pixel-wise contextual attention for saliency detec-\ntion. In CVPR, pages 3089‚Äì3098, 2018. 1, 2, 7, 8, 13\n[38] Nian Liu, Ni Zhang, and Junwei Han. Learning selective\nself-mutual attention for rgb-d saliency detection. In CVPR,\npages 13756‚Äì13765, 2020. 1, 2, 5, 7, 14\n[39] Nian Liu, Ni Zhang, Ling Shao, and Junwei Han. Learn-\ning selective mutual attention and contrast for rgb-d saliency\ndetection. arXiv preprint arXiv:2010.05537, 2020. 2, 6, 7\n[40] Ruijin Liu, Zejian Yuan, Tie Liu, and Zhiliang Xiong. End-\nto-end lane shape prediction with transformers. InIEEE Win-\nter Conference on Applications of Computer Vision , pages\n3694‚Äì3702, 2021. 2\n[41] Yi Liu, Qiang Zhang, Dingwen Zhang, and Jungong Han.\nEmploying deep part-object relationships for salient object\ndetection. In ICCV, pages 1232‚Äì1241, 2019. 7, 8, 13\n[42] Zhengyi Liu, Song Shi, Quntao Duan, Wei Zhang, and Peng\nZhao. Salient object detection for rgb-d image by single\nstream recurrent convolution neural network. Neurocomput-\ning, 363:46‚Äì57, 2019. 2\n[43] Ao Luo, Xin Li, Fan Yang, Zhicheng Jiao, Hong Cheng, and\nSiwei Lyu. Cascade graph neural networks for rgb-d salient\nobject detection. In ECCV, pages 346‚Äì364, 2020. 1, 2, 5, 7,\n14\n[44] Zhiming Luo, Akshaya Mishra, Andrew Achkar, Justin\nEichel, Shaozi Li, and Pierre-Marc Jodoin. Non-local deep\nfeatures for salient object detection. In CVPR, pages 6609‚Äì\n6617, 2017. 1\n[45] Vida Movahedi and James H Elder. Design and perceptual\nvalidation of performance measures for salient object seg-\nmentation. In CVPR Workshops, pages 49‚Äì56, 2010. 6, 8,\n13\n[46] Yuzhen Niu, Yujie Geng, Xueqing Li, and Feng Liu. Lever-\naging stereopsis for saliency analysis. In CVPR, pages 454‚Äì\n461, 2012. 6, 7, 13\n[47] Hyeonwoo Noh, Seunghoon Hong, and Bohyung Han.\nLearning deconvolution network for semantic segmentation.\nIn ICCV, pages 1520‚Äì1528, 2015. 1\n[48] Youwei Pang, Lihe Zhang, Xiaoqi Zhao, and Huchuan Lu.\nHierarchical dynamic Ô¨Åltering network for rgb-d salient ob-\nject detection. In ECCV, pages 235‚Äì252, 2020. 1, 2, 7, 8,\n14\n[49] Youwei Pang, Xiaoqi Zhao, Lihe Zhang, and Huchuan Lu.\nMulti-scale interactive network for salient object detection.\nIn CVPR, pages 9413‚Äì9422, 2020. 1, 2, 5, 7, 8, 13\n[50] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\nLin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An im-\nperative style, high-performance deep learning library.NIPS,\n32:8026‚Äì8037, 2019. 6\n[51] Houwen Peng, Bing Li, Weihua Xiong, Weiming Hu, and\nRongrong Ji. Rgbd salient object detection: A benchmark\nand algorithms. In ECCV, pages 92‚Äì109, 2014. 6, 7\n[52] Yongri Piao, Wei Ji, Jingjing Li, Miao Zhang, and Huchuan\nLu. Depth-induced multi-scale recurrent attention network\nfor saliency detection. In ICCV, pages 7254‚Äì7263, 2019. 2,\n6, 7, 13\n[53] Yongri Piao, Zhengkun Rong, Miao Zhang, Weisong Ren,\nand Huchuan Lu. A2dele: Adaptive and attentive depth dis-\ntiller for efÔ¨Åcient rgb-d salient object detection. In CVPR,\npages 9060‚Äì9069, 2020. 1, 2, 7, 14\n[54] Xuebin Qin, Zichen Zhang, Chenyang Huang, Chao Gao,\nMasood Dehghan, and Martin Jagersand. Basnet: Boundary-\naware salient object detection. In CVPR, pages 7479‚Äì7489,\n2019. 2\n[55] Xuebin Qin, Zichen Zhang, Chenyang Huang, Chao Gao,\nMasood Dehghan, and Martin Jagersand. Basnet: Boundary-\naware salient object detection. In CVPR, pages 7479‚Äì7489,\n2019. 7, 8, 13\n[56] Jianqiang Ren, Xiaojin Gong, Lu Yu, Wenhui Zhou, and\nMichael Ying Yang. Exploiting global priors for rgb-d\nsaliency detection. In CVPR workshops, pages 25‚Äì32, 2015.\n1\n[57] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nnet: Convolutional networks for biomedical image segmen-\ntation. In International Conference on Medical image com-\nputing and computer-assisted intervention , pages 234‚Äì241,\n2015. 1, 2\n[58] Wataru Shimoda and Keiji Yanai. Distinct class-speciÔ¨Åc\nsaliency maps for weakly supervised semantic segmentation.\nIn ECCV, pages 218‚Äì234, 2016. 1\n[59] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. InICLR,\n2015. 3\n[60] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv ¬¥e J¬¥egou. Training\ndata-efÔ¨Åcient image transformers & distillation through at-\ntention. In ICML, pages 10347‚Äì10357, 2021. 2, 3\n[61] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NIPS, pages 5998‚Äì\n6008, 2017. 1, 2, 3, 4\n[62] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, and\nLiang-Chieh Chen. Max-deeplab: End-to-end panoptic seg-\nmentation with mask transformers. In CVPR, pages 5463‚Äì\n5474, 2021. 2\n[63] Lijun Wang, Huchuan Lu, Yifan Wang, Mengyang Feng,\nDong Wang, Baocai Yin, and Xiang Ruan. Learning to de-\ntect salient objects with image-level supervision. In CVPR,\npages 136‚Äì145, 2017. 6, 8, 13\n[64] Linzhao Wang, Lijun Wang, Huchuan Lu, Pingping Zhang,\nand Xiang Ruan. Salient object detection with recurrent fully\nconvolutional networks. TPAMI, 41(7):1734‚Äì1746, 2018. 2\n[65] Tiantian Wang, Ali Borji, Lihe Zhang, Pingping Zhang, and\nHuchuan Lu. A stagewise reÔ¨Ånement model for detecting\nsalient objects in images. In ICCV, pages 4019‚Äì4028, 2017.\n1, 2\n[66] Wenguan Wang, Qiuxia Lai, Huazhu Fu, Jianbing Shen, and\nHaibin Ling. Salient object detection in the deep learning\nera: An in-depth survey. TPAMI, 2021. 1\n[67] Wenguan Wang, Jianbing Shen, Xingping Dong, and Ali\nBorji. Salient object detection driven by Ô¨Åxation prediction.\nIn CVPR, pages 1711‚Äì1720, 2018. 1, 2\n[68] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.\nPyramid vision transformer: A versatile backbone for\ndense prediction without convolutions. arXiv preprint\narXiv:2102.12122, 2021. 2\n[69] Jun Wei, Shuhui Wang, Zhe Wu, Chi Su, Qingming Huang,\nand Qi Tian. Label decoupling framework for salient object\ndetection. In CVPR, pages 13025‚Äì13034, 2020. 1, 2, 5, 7, 8,\n13\n[70] Zhe Wu, Li Su, and Qingming Huang. Cascaded partial de-\ncoder for fast and accurate salient object detection. InCVPR,\npages 3907‚Äì3916, 2019. 7, 8, 13\n[71] Saining Xie and Zhuowen Tu. Holistically-nested edge de-\ntection. In CVPR, pages 1395‚Äì1403, 2015. 2\n[72] Qiong Yan, Li Xu, Jianping Shi, and Jiaya Jia. Hierarchical\nsaliency detection. In CVPR, pages 1155‚Äì1162, 2013. 5, 8\n[73] Chuan Yang, Lihe Zhang, Huchuan Lu, Xiang Ruan, and\nMing-Hsuan Yang. Saliency detection via graph-based man-\nifold ranking. In CVPR, pages 3166‚Äì3173, 2013. 6, 8\n[74] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,\nFrancis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-\nto-token vit: Training vision transformers from scratch on\nimagenet. In ICCV, 2021. 1, 2, 3, 4, 5, 6\n[75] Yun Zhai and Mubarak Shah. Visual attention detection in\nvideo sequences using spatiotemporal cues. In ACM Inter-\nnational Conference on Multimedia , pages 815‚Äì824, 2006.\n1\n[76] Jing Zhang, Deng-Ping Fan, Yuchao Dai, Saeed Anwar,\nFatemeh Sadat Saleh, Tong Zhang, and Nick Barnes. Uc-net:\nUncertainty inspired rgb-d saliency detection via conditional\nvariational autoencoders. In CVPR, pages 8582‚Äì8591, 2020.\n7, 8, 14\n[77] Lu Zhang, Jianming Zhang, Zhe Lin, Huchuan Lu, and You\nHe. Capsal: Leveraging captioning to boost semantics for\nsalient object detection. In CVPR, pages 6024‚Äì6033, 2019.\n1, 2\n[78] Miao Zhang, Sun Xiao Fei, Jie Liu, Shuang Xu, Yongri Piao,\nand Huchuan Lu. Asymmetric two-stream architecture for\naccurate rgb-d saliency detection. In ECCV, pages 374‚Äì390,\n2020. 1, 7, 14\n[79] Miao Zhang, Weisong Ren, Yongri Piao, Zhengkun Rong,\nand Huchuan Lu. Select, supplement and focus for rgb-d\nsaliency detection. In CVPR, pages 3472‚Äì3481, 2020. 2, 5,\n7, 14\n[80] Xiaoning Zhang, Tiantian Wang, Jinqing Qi, Huchuan Lu,\nand Gang Wang. Progressive attention guided recurrent net-\nwork for salient object detection. In CVPR, pages 714‚Äì722,\n2018. 1, 2\n[81] Jia-Xing Zhao, Yang Cao, Deng-Ping Fan, Ming-Ming\nCheng, Xuan-Yi Li, and Le Zhang. Contrast prior and Ô¨Çuid\npyramid integration for rgbd salient object detection. In\nCVPR, pages 3927‚Äì3936, 2019. 2\n[82] Jia-Xing Zhao, Jiang-Jiang Liu, Deng-Ping Fan, Yang Cao,\nJufeng Yang, and Ming-Ming Cheng. Egnet:edge guidance\nnetwork for salient object detection. In ICCV, pages 8779‚Äì\n8788, 2019. 1, 2, 5, 6, 7, 8, 13\n[83] Rui Zhao, Wanli Ouyang, Hongsheng Li, and Xiaogang\nWang. Saliency detection by multi-context deep learning.\nIn CVPR, pages 1265‚Äì1274, 2015. 1\n[84] Xiaoqi Zhao, Youwei Pang, Lihe Zhang, Huchuan Lu, and\nLei Zhang. Suppress and balance: A simple gated network\nfor salient object detection. In ECCV, pages 35‚Äì51, 2020. 2,\n5, 7, 8, 13\n[85] Xiaoqi Zhao, Lihe Zhang, Youwei Pang, Huchuan Lu, and\nLei Zhang. A single stream network for robust and real-\ntime rgb-d salient object detection. In ECCV, pages 646‚Äì\n662, 2020. 7, 14\n[86] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu,\nZekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao\nXiang, Philip HS Torr, et al. Rethinking semantic segmen-\ntation from a sequence-to-sequence perspective with trans-\nformers. In CVPR, pages 6881‚Äì6890, 2021. 3\n[87] Huajun Zhou, Xiaohua Xie, Jian-Huang Lai, Zixuan Chen,\nand Lingxiao Yang. Interactive two-stream decoder for accu-\nrate and fast saliency detection. In CVPR, pages 9141‚Äì9150,\n2020. 2, 7, 8, 13\n[88] Tao Zhou, Deng-Ping Fan, Ming-Ming Cheng, Jianbing\nShen, and Ling Shao. Rgb-d salient object detection: A sur-\nvey. Computational Visual Media, pages 1‚Äì33, 2021. 1\n[89] Tao Zhou, Huazhu Fu, Geng Chen, Yi Zhou, Deng-Ping Fan,\nand Ling Shao. SpeciÔ¨Åcity-preserving rgb-d saliency detec-\ntion. In ICCV, 2021. 1\n[90] Chunbiao Zhu and Ge Li. A three-pathway psychobiologi-\ncal framework of salient object detection using stereoscopic\ntechnology. In ICCV Workshops, pages 3008‚Äì3014, 2017. 6,\n7\n[91] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,\nand Jifeng Dai. Deformable detr: Deformable transformers\nfor end-to-end object detection. In ICLR, 2020. 2\n6. Supplementary materials\n6.1. Ablation Study on RGB SOD Datasets\nWe further report the results of ablation studies on four\nRGB SOD datasets, i.e., DUTS, HKU-IS, PASCAL-S, and\nSOD, in Table 4 to demonstrate the effectiveness of our\nVST model components.\nThe baseline model is using transformer encoder to ex-\ntract patch tokens TE\nr and then directly using TE\nr to predict\nthe saliency map with 1/16 scale by using MLP on each\npatch token. Based on the baseline, we insert RGB conver-\ntor right after the transformer encoder, shown as ‚Äú+RC‚Äù in\nTable 4. Compared to the baseline, RC brings performance\ngains especially on the DUTS and PASCAL-S datasets,\nwhich demonstrates its effectiveness. For other compo-\nnents, i.e., RT2T, multi-level token fusion, and multi-task\ntransformer decoder, we get consistent conclusions with the\nablation studies on RGB-D SOD datasets as follows.\nFirst, using bilinear upsampling (‚Äú+RC+Bili‚Äù) can sig-\nniÔ¨Åcantly improve the model performance while using our\nproposed RT2T (‚Äú+RC+RT2T‚Äù) can further bring perfor-\nmance gains, hence demonstrating the effectiveness of our\nproposed RT2T. Second, based on ‚Äú+RC+RT2T‚Äù, multi-\nlevel token fusion (‚Äú+RC+RT2T+F‚Äù) can lead to better\nperformance on all four datasets, which veriÔ¨Åes its ef-\nfectiveness. Third, using multi-task transformer decoder\n(‚Äú+RC+RT2T+F+TMD‚Äù) can improve the model perfor-\nmance on all four datasets and it is also superior to the con-\nventional two-stream decoder (‚Äú+RC+RT2T+F+C2D‚Äù).\nTo this end, the results of ablation studies on both RGB\nand RGB-D SOD datasets strongly demonstrate the effec-\ntiveness of our proposed VST components.\n6.2. Layer Number Study\nWe conduct experiments to study the optimal numbers of\ndifferent transformer layers, i.e., LCin the transformer con-\nvertor and LDin the multi-task transformer decoder, jointly\nconsidering computational costs and model performance.\nNote that there are three decoder modules at three scales\nin the multi-task transformer decoder, thus we set different\ntransformer layer numbers for them, i.e., LD\n3 for 1/16 scale,\nLD\n2 for 1/8 scale, and LD\n1 for 1/4 scale. The experimental\nresults on four RGB-D SOD datasets, i.e., NJUD, DUTLF-\nDepth, STERE, and LFSD, are given in Table 5.\nIn our initial model setting, we set LC= LD\n3 = 8. Since\nLD\n2 and LD\n1 are used at relatively large scales, we initially\nset both of them to 4, as shown in row I in Table 5. Then,\nwe start to change the numbers of different layers.\nWe Ô¨Årst reduce LD\n2 and LD\n1 from 4 to 2 to save compu-\ntational costs. The experimental results on row II show that\nit can get comparable performance with less computational\ncosts compared with row I. Hence, we set LD\n2 = LD\n1 = 2\nand start to change LD\n3 from 8 to 6, 4, 2, respectively, which\nare shown in row III, IV , V in Table 5. We Ô¨Ånd that as LD\n3\ndecreases, the computation costs decrease gradually while\nthe results are generally comparable. However, the model\nperformance on row IV is better than that on row V on\nDUTLF-Depth and LFSD datasets. Thus, we set LD\n3 = 4\nand start to change LCfrom 8 to 6, 4, 2, respectively, which\nare shown in row VI, VII, VIII. It can be seen that the per-\nformance on row VII is the best and the model has accept-\nable computational costs. Hence, we set LC= LD\n3 = 4and\nLD\n2 = LD\n1 = 2as our Ô¨Ånal model setting.\n6.3. More Visual Comparison with State-of-the-art\nMethods\nWe give more visual comparison results with the state-\nof-the-art RGB and RGB-D SOD methods in Figure 4\nand Figure 5, respectively. It shows that our VST model\ncan handle well in many challenging scenarios, i.e., big\nsalient objects, cluttered backgrounds, foregrounds and\nbackgrounds with very similar appearance, etc, while ex-\nisting methods are heavily disturbed in these scenarios. Be-\nsides, we also show the boundary maps predicted by our\nRGB VST and RGB-D VST models in Figure 4 and Fig-\nure 5, respectively. It can be seen that our models can pre-\ndict clear boundaries for salient objects.\nTable 4. Ablation studies of our proposed model on RGB SOD datasets. ‚ÄúRC‚Äù means RGB Convertor. ‚ÄúBili‚Äù denotes bilinear upsampling\nand ‚ÄúF‚Äù means multi-level token fusion. ‚ÄúTMD‚Äù denotes our proposed token-based multi-task decoder, while ‚ÄúC2D‚Äù means using conven-\ntional two-stream decoder to perform saliency and boundary detection without using task-related tokens. The best results are labeled in\nblue.\nSettings DUTS [63] HKU-IS [32] PASCAL-S [34] SOD [45]\nSm maxF Emax\nŒæ MAE Sm maxF Emax\nŒæ MAE Sm maxF Emax\nŒæ MAE Sm maxF Emax\nŒæ MAE\nBaseline 0.824 0.780 0.909 0.071 0.858 0.854 0.938 0.075 0.826 0.795 0.878 0.096 0.802 0.803 0.880 0.100\n+RC 0.827 0.785 0.913 0.070 0.860 0.856 0.939 0.074 0.830 0.797 0.879 0.095 0.804 0.805 0.880 0.100\n+RC+Bili 0.867 0.835 0.929 0.048 0.901 0.901 0.956 0.044 0.856 0.827 0.891 0.074 0.833 0.836 0.891 0.077\n+RC+RT2T 0.881 0.856 0.934 0.043 0.914 0.918 0.961 0.037 0.864 0.838 0.896 0.070 0.844 0.850 0.894 0.069\n+RC+RT2T+F 0.895 0.874 0.939 0.039 0.925 0.932 0.966 0.032 0.871 0.845 0.897 0.068 0.851 0.861 0.899 0.068\n+RC+RT2T+F+TMD 0.896 0.877 0.939 0.037 0.928 0.937 0.968 0.030 0.873 0.850 0.900 0.067 0.854 0.866 0.902 0.065\n+RC+RT2T+F+C2D 0.891 0.870 0.937 0.040 0.924 0.931 0.966 0.033 0.869 0.844 0.896 0.069 0.852 0.860 0.898 0.067\nTable 5. Comparison of using different numbers of transformer layers in our VST model. The Ô¨Ånal model setting is labeled in blue.\nID Layer Num MACs Params NJUD [26] DUTLF-Depth [52] STERE [46] LFSD [33]\nLC LD\n3 LD\n2 LD\n1 (G) (M) Sm maxF Emax\nŒæ MAE Sm maxF Emax\nŒæ MAE Sm maxF Emax\nŒæ MAE Sm maxF Emax\nŒæ MAE\nI 8 8 4 4 48.35 119.30 0.925 0.925 0.955 0.033 0.940 0.947 0.966 0.026 0.910 0.902 0.948 0.039 0.878 0.884 0.914 0.066\nII 8 8 2 2 36.78 113.39 0.923 0.922 0.955 0.035 0.943 0.947 0.968 0.025 0.911 0.904 0.948 0.039 0.874 0.878 0.908 0.069\nIII 8 6 2 2 36.20 110.43 0.921 0.920 0.952 0.036 0.940 0.945 0.966 0.026 0.910 0.904 0.948 0.040 0.875 0.883 0.911 0.067\nIV 8 4 2 2 35.61 107.47 0.921 0.920 0.951 0.036 0.942 0.947 0.968 0.026 0.911 0.904 0.949 0.040 0.876 0.880 0.912 0.068\nV 8 2 2 2 35.03 104.52 0.922 0.921 0.952 0.036 0.940 0.944 0.965 0.026 0.912 0.906 0.949 0.039 0.873 0.875 0.908 0.068\nVI 6 4 2 2 33.30 95.65 0.923 0.921 0.952 0.036 0.943 0.948 0.968 0.024 0.913 0.906 0.949 0.039 0.875 0.878 0.912 0.067\nVII 4 4 2 2 30.99 83.83 0.922 0.920 0.951 0.035 0.943 0.948 0.969 0.024 0.913 0.907 0.951 0.038 0.882 0.889 0.921 0.061\nVIII 2 4 2 2 28.68 72.00 0.923 0.921 0.953 0.036 0.938 0.943 0.963 0.028 0.912 0.906 0.950 0.039 0.881 0.887 0.917 0.062\nImage GT VST GateNet\n[84]\nCSF\n[20]\nLDF\n[69]\nMINet\n[49]\nITSD\n[87]\nEGNet\n[82]\nTSPOA\n[41]\nAFNet\n[17]\nPoolNet\n[35]\nCPD\n[70]\nBASNet\n[55]\nPiCANet\n[37]\nFigure 4. Qualitative comparison against state-of-the-art RGB SOD methods. (GT: ground truth)\nImage Depth GT VST BBS-Net\n[16]\nCoNet\n[25]\nHDFNet\n[48]\nCas-Gnn\n[43]\nCMW\n[31]\nATST\n[78]\ncmMS\n[29]\nDANet\n[85]\nPGAR\n[6]\ns2MA\n[38]\nUC-Net\n[76]\nSSFRGBD\n[79]\nJLDCF\n[18]\nA2dele\n[53]\nFigure 5. Qualitative comparison against state-of-the-art RGB-D methods. (GT: ground truth)",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7076610326766968
    },
    {
      "name": "Transformer",
      "score": 0.674538254737854
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6496832370758057
    },
    {
      "name": "Upsampling",
      "score": 0.49792027473449707
    },
    {
      "name": "RGB color model",
      "score": 0.47596997022628784
    },
    {
      "name": "Computer vision",
      "score": 0.4675062596797943
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.38286203145980835
    },
    {
      "name": "Engineering",
      "score": 0.13460388779640198
    },
    {
      "name": "Voltage",
      "score": 0.11373496055603027
    },
    {
      "name": "Image (mathematics)",
      "score": 0.1103958785533905
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}