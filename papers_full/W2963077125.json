{
  "title": "Recurrent Memory Networks for Language Modeling",
  "url": "https://openalex.org/W2963077125",
  "year": 2016,
  "authors": [
    {
      "id": "https://openalex.org/A4287888864",
      "name": "Ke Tran",
      "affiliations": [
        "University of Amsterdam"
      ]
    },
    {
      "id": "https://openalex.org/A2139068014",
      "name": "Arianna Bisazza",
      "affiliations": [
        "University of Amsterdam"
      ]
    },
    {
      "id": "https://openalex.org/A2109806231",
      "name": "Christof Monz",
      "affiliations": [
        "University of Amsterdam"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1689711448",
    "https://openalex.org/W71795751",
    "https://openalex.org/W1815076433",
    "https://openalex.org/W2159505618",
    "https://openalex.org/W2963447120",
    "https://openalex.org/W581956982",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2115081467",
    "https://openalex.org/W2097732278",
    "https://openalex.org/W2950527759",
    "https://openalex.org/W1924770834",
    "https://openalex.org/W287510790",
    "https://openalex.org/W1850742715",
    "https://openalex.org/W2964199361",
    "https://openalex.org/W2962741254",
    "https://openalex.org/W2251654079",
    "https://openalex.org/W2107878631",
    "https://openalex.org/W2140679639",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W1772447446",
    "https://openalex.org/W2251994258",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W2951008357",
    "https://openalex.org/W2963932686",
    "https://openalex.org/W2250401584",
    "https://openalex.org/W1626006465",
    "https://openalex.org/W2252252584",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2964325005",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2134036914",
    "https://openalex.org/W2589089426",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W2110485445",
    "https://openalex.org/W2962935015",
    "https://openalex.org/W2951714314",
    "https://openalex.org/W2757704557",
    "https://openalex.org/W4303633609",
    "https://openalex.org/W1987937363",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W1793121960",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W4254816979",
    "https://openalex.org/W2132547887",
    "https://openalex.org/W2962965465",
    "https://openalex.org/W2949952998",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W2611669587",
    "https://openalex.org/W1951216520"
  ],
  "abstract": "Recurrent Neural Networks (RNNs) have obtained excellent result in many natural language processing (NLP) tasks. However, understanding and interpreting the source of this success remains a challenge. In this paper, we propose Recurrent Memory Network (RMN), a novel RNN architecture, that not only amplifies the power of RNN but also facilitates our understanding of its internal functioning and allows us to discover underlying patterns in data. We demonstrate the power of RMN on language modeling and sentence completion tasks. On language modeling, RMN out-performs Long Short-Term Memory (LSTM) network on three large German, Italian, and English dataset. Additionally we perform in-depth analysis of various linguistic dimensions that RMN captures. On Sentence Completion Challenge, for which it is essential to capture sentence coherence, our RMN obtains 69.2% accuracy, surpassing the previous state of the art by a large margin.1",
  "full_text": "Proceedings of NAACL-HLT 2016, pages 321–331,\nSan Diego, California, June 12-17, 2016.c⃝2016 Association for Computational Linguistics\nRecurrent Memory Networks for Language Modeling\nKe Tran Arianna Bisazza Christof Monz\nInformatics Institute, University of Amsterdam\nScience Park 904, 1098 XH Amsterdam, The Netherlands\n{m.k.tran,a.bisazza,c.monz}@uva.nl\nAbstract\nRecurrent Neural Networks (RNNs) have ob-\ntained excellent result in many natural lan-\nguage processing (NLP) tasks. However, un-\nderstanding and interpreting the source of this\nsuccess remains a challenge. In this paper, we\npropose Recurrent Memory Network (RMN),\na novel RNN architecture, that not only am-\npliﬁes the power of RNN but also facilitates\nour understanding of its internal functioning\nand allows us to discover underlying patterns\nin data. We demonstrate the power of RMN\non language modeling and sentence comple-\ntion tasks. On language modeling, RMN out-\nperforms Long Short-Term Memory (LSTM)\nnetwork on three large German, Italian, and\nEnglish dataset. Additionally we perform in-\ndepth analysis of various linguistic dimen-\nsions that RMN captures. On Sentence Com-\npletion Challenge, for which it is essential to\ncapture sentence coherence, our RMN obtains\n69.2% accuracy, surpassing the previous state\nof the art by a large margin.1\n1 Introduction\nRecurrent Neural Networks (RNNs) (Elman, 1990;\nMikolov et al., 2010) are remarkably powerful mod-\nels for sequential data. Long Short-Term Memory\n(LSTM) (Hochreiter and Schmidhuber, 1997), a spe-\nciﬁc architecture of RNN, has a track record of suc-\ncess in many natural language processing tasks such\nas language modeling (J ´ozefowicz et al., 2015), de-\npendency parsing (Dyer et al., 2015), sentence com-\n1Our code and data are available at https://github.\ncom/ketranm/RMN\npression (Filippova et al., 2015), and machine trans-\nlation (Sutskever et al., 2014).\nWithin the context of natural language process-\ning, a common assumption is that LSTMs are able to\ncapture certain linguistic phenomena. Evidence sup-\nporting this assumption mainly comes from evaluat-\ning LSTMs in downstream applications: Bowman\net al. (2015) carefully design two artiﬁcial datasets\nwhere sentences have explicit recursive structures.\nThey show empirically that while processing the in-\nput linearly, LSTMs can implicitly exploit recursive\nstructures of languages. Filippova et al. (2015) ﬁnd\nthat using explicit syntactic features within LSTMs\nin their sentence compression model hurts the per-\nformance of overall system. They then hypothesize\nthat a basic LSTM is powerful enough to capture\nsyntactic aspects which are useful for compression.\nTo understand and explain which linguistic di-\nmensions are captured by an LSTM is non-trivial.\nThis is due to the fact that the sequences of input\nhistories are compressed into several dense vectors\nby the LSTM’s components whose purposes with re-\nspect to representing linguistic information is not ev-\nident. To our knowledge, the only attempt to better\nunderstand the reasons of an LSTM’s performance\nand limitations is the work of Karpathy et al. (2015)\nby means of visualization experiments and cell acti-\nvation statistics in the context of character-level lan-\nguage modeling.\nOur work is motivated by the difﬁculty in un-\nderstanding and interpreting existing RNN architec-\ntures from a linguistic point of view. We propose Re-\ncurrent Memory Network (RMN), a novel RNN ar-\nchitecture that combines the strengths of both LSTM\n321\nand Memory Network (Sukhbaatar et al., 2015). In\nRMN, the Memory Block component—a variant of\nMemory Network—accesses the most recent input\nwords and selectively attends to words that are rel-\nevant for predicting the next word given the current\nLSTM state. By looking at the attention distribution\nover history words, our RMN allows us not only to\ninterpret the results but also to discover underlying\ndependencies present in the data.\nIn this paper, we make the following contribu-\ntions:\n1. We propose a novel RNN architecture that\ncomplements LSTM in language modeling. We\ndemonstrate that our RMN outperforms com-\npetitive LSTM baselines in terms of perplex-\nity on three large German, Italian, and English\ndatasets.\n2. We perform an analysis along various linguis-\ntic dimensions that our model captures. This\nis possible only because the Memory Block al-\nlows us to look into its internal states and its ex-\nplicit use of additional inputs at each time step.\n3. We show that, with a simple modiﬁcation,\nour RMN can be successfully applied to NLP\ntasks other than language modeling. On the\nSentence Completion Challenge (Zweig and\nBurges, 2012), our model achieves an impres-\nsive 69.2% accuracy, surpassing the previous\nstate of the art 58.9% by a large margin.\n2 Recurrent Neural Networks\nRecurrent Neural Networks (RNNs) have shown im-\npressive performances on many sequential modeling\ntasks due to their ability to encode unbounded input\nhistories. However, training simple RNNs is difﬁ-\ncult because of the vanishing and exploding gradi-\nent problems (Bengio et al., 1994; Pascanu et al.,\n2013). A simple and effective solution for explod-\ning gradients is gradient clipping proposed by Pas-\ncanu et al. (2013). To address the more challeng-\ning problem of vanishing gradients, several variants\nof RNNs have been proposed. Among them, Long\nShort-Term Memory (Hochreiter and Schmidhuber,\n1997) and Gated Recurrent Unit (Cho et al., 2014)\nare widely regarded as the most successful variants.\nIn this work, we focus on LSTMs because they have\nbeen shown to outperform GRUs on language mod-\neling tasks (J ´ozefowicz et al., 2015). In the follow-\ning, we will detail the LSTM architecture used in\nthis work.\nLong Short-Term Memory\nNotation: Throughout this paper, we denote matri-\nces, vectors, and scalars using bold uppercase (e. g.,\nW), bold lowercase (e. g., b) and lowercase (e. g.,\nα) letters, respectively.\nThe LSTM used in this work is speciﬁed as fol-\nlows:\nit = sigm(Wxixt + Whiht−1 + bi)\njt = sigm(Wxjxt + Whjht−1 + bj)\nft = sigm(Wxf xt + Whf ht−1 + bf )\not = tanh(Wxoxt + Whoht−1 + bo)\nct = ct−1 ⊙ft + it ⊙jt\nht = tanh(ct) ⊙ot\nwhere xt is the input vector at time stept, ht−1 is the\nLSTM hidden state at the previous time step, W∗\nand b∗are weights and biases. The symbol ⊙de-\nnotes the Hadamard product or element-wise multi-\nplication.\nDespite the popularity of LSTM in sequential\nmodeling, its design is not straightforward to justify\nand understanding why it works remains a challenge\n(Hermans and Schrauwen, 2013; Chung et al., 2014;\nGreff et al., 2015; J ´ozefowicz et al., 2015; Karpa-\nthy et al., 2015). There have been few recent at-\ntempts to understand the components of an LSTM\nfrom an empirical point of view: Greff et al. (2015)\ncarry out a large-scale experiment of eight LSTM\nvariants. The results from their 5,400 experimental\nruns suggest that forget gates and output gates are\nthe most critical components of LSTMs. J´ozefowicz\net al. (2015) conduct and evaluate over ten thousand\nRNN architectures and ﬁnd that the initialization of\nthe forget gate bias is crucial to the LSTM’s perfor-\nmance. While these ﬁndings are important to help\nchoosing appropriate LSTM architectures, they do\nnot shed light on what information is captured by\nthe hidden states of an LSTM.\nBowman et al. (2015) show that a vanilla LSTM,\nsuch as described above, performs reasonably well\ncompared to a recursive neural network (Socher et\nal., 2011) that explicitly exploits tree structures on\n322\ntwo artiﬁcial datasets. They ﬁnd that LSTMs can\neffectively exploit recursive structure in the artiﬁ-\ncial datasets. In contrast to these simple datasets\ncontaining a few logical operations in their exper-\niments, natural languages exhibit highly complex\npatterns. The extent to which linguistic assumptions\nabout syntactic structures and compositional seman-\ntics are reﬂected in LSTMs is rather poorly under-\nstood. Thus it is desirable to have a more principled\nmechanism allowing us to inspect recurrent architec-\ntures from a linguistic perspective. In the following\nsection, we propose such a mechanism.\n3 Recurrent Memory Network\nIt has been demonstrated that RNNs can retain in-\nput information over a long period. However, exist-\ning RNN architectures make it difﬁcult to analyze\nwhat information is exactly retained at their hidden\nstates at each time step, especially when the data has\ncomplex underlying structures, which is common in\nnatural language. Motivated by this difﬁculty, we\npropose a novel RNN architecture called Recurrent\nMemory Network (RMN). On linguistic data, the\nRMN allows us not only to qualify which linguis-\ntic information is preserved over time and why this\nis the case but also to discover dependencies within\nthe data (Section 5). Our RMN consists of two com-\nponents: an LSTM and a Memory Block (MB) (Sec-\ntion 3.1). The MB takes the hidden state of the\nLSTM and compares it to the most recent inputs\nusing an attention mechanism (Gregor et al., 2015;\nBahdanau et al., 2014; Graves et al., 2014). Thus,\nanalyzing the attention weights of a trained model\ncan give us valuable insight into the information that\nis retained over time in the LSTM.\nIn the following, we describe in detail the MB ar-\nchitecture and the combination of the MB and the\nLSTM to form an RMN.\n3.1 Memory Block\nThe Memory Block (Figure 1) is a variant of Mem-\nory Network (Sukhbaatar et al., 2015) with one hop\n(or a single-layer Memory Network). At time step t,\nthe MB receives two inputs: the hidden state ht of\nthe LSTM and a set {xi}of n most recent words\nincluding the current word xt. We refer to n as\nthe memory size. Internally, the MB consists of\nsoftmax\n{ x i }\nh\nm\nh\nP\nm i\nc i\n⇥\ng\nFigure 1: A graphical representation of the MB.\ntwo lookup tables M and C of size |V|×d, where\n|V|is the size of the vocabulary. With a slight\nabuse of notation we denote Mi = M({xi}) and\nCi = C({xi}) as n×d matrices where each row\ncorresponds to an input memory embedding mi and\nan output memory embedding ci of each element of\nthe set {xi}. We use the matrix Mi to compute an\nattention distribution over the set {xi}:\npt = softmax(Miht) (1)\nWhen dealing with data that exhibits a strong tem-\nporal relationship, such as natural language, an ad-\nditional temporal matrix T ∈Rn×d can be used to\nbias attention with respect to the position of the data\npoints. In this case, equation 1 becomes\npt = softmax\n(\n(Mi + T)ht\n)\n(2)\nWe then use the attention distributionpt to compute\na context vector representation of {xi}:\nst = CT\ni pt (3)\nFinally, we combine the context vector st and the\nhidden state ht by a function g(·) to obtain the out-\nput hm\nt of the MB. Instead of using a simple addi-\ntion function g(st,ht) = st + ht as in Sukhbaatar\net al. (2015), we propose to use a gating unit that\ndecides how much it should trust the hidden state\nht and context st at time step t. Our gating unit is\na form of Gated Recurrent Unit (Cho et al., 2014;\nChung et al., 2014):\nzt = sigm(Wszst + Uhzht) (4)\nrt = sigm(Wsrst + Uhrht) (5)\n˜ht = tanh(Wst + U(rt ⊙ht)) (6)\nhm\nt = (1 −zt) ⊙ht + zt ⊙˜ht (7)\n323\nwhere zt is an update gate, rt is a reset gate.\nThe choice of the composition function g(·) is\ncrucial for the MB especially when one of its in-\nput comes from the LSTM. The simple addition\nfunction might overwrite the information within the\nLSTM’s hidden state and therefore prevent the MB\nfrom keeping track of information in the distant past.\nThe gating function, on the other hand, can control\nthe degree of information that ﬂows from the LSTM\nto the MB’s output.\n3.2 RMN Architectures\nAs explained above, our proposed MB receives the\nhidden state of the LSTM as one of its input. This\nleads to an intuitive combination of the two units by\nstacking the MB on top of the LSTM. We call this\narchitecture Recurrent-Memory (RM). The RM ar-\nchitecture, however, does not allow interaction be-\ntween Memory Blocks at different time steps. To\nenable this interaction we can stack one more LSTM\nlayer on top of the RM. We call this architecture\nRecurrent-Memory-Recurrent (RMR).\nMB\nLSTM\nLSTMLSTMLSTM\nMB\nLSTM\nLSTM\nMBMB\nLSTMLSTM\nLSTM\nLSTM\nMB\nFigure 2: A graphical illustration of an unfolded\nRMR with memory size 4. Dashed line indicates\nconcatenation. The MB takes the output of the bot-\ntom LSTM layer and the 4-word history as its input.\nThe output of the MB is then passed to the second\nLSTM layer on top. There is no direct connection\nbetween MBs of different time steps. The last LSTM\nlayer carries the MB’s outputs recurrently.\n4 Language Model Experiments\nLanguage models play a crucial role in many NLP\napplications such as machine translation and speech\nrecognition. Language modeling also serves as\na standard test bed for newly proposed models\n(Sukhbaatar et al., 2015; Kalchbrenner et al., 2015).\nWe conjecture that, by explicitly accessing history\nwords, RMNs will offer better predictive power than\nthe existing recurrent architectures. We therefore\nevaluate our RMN architectures against state-of-the-\nart LSTMs in terms of perplexity.\n4.1 Data\nWe evaluate our models on three languages: En-\nglish, German, and Italian. We are especially inter-\nested in German and Italian because of their larger\nvocabularies and complex agreement patterns. Ta-\nble 1 summarizes the data used in our experiments.\nLang Train Dev Test |s| |V|\nEn 26M 223K 228K 26 77K\nDe 22M 202K 203K 22 111K\nIt 29M 207K 214K 29 104K\nTable 1: Data statistics. |s|denotes the average sen-\ntence length and |V|the vocabulary size.\nThe training data correspond to approximately\n1M sentences in each language. For English, we\nuse all the News Commentary data (8M tokens)\nand 18M tokens from News Crawl 2014 for train-\ning. Development and test data are randomly drawn\nfrom the concatenation of the WMT 2009-2014 test\nsets (Bojar et al., 2015). For German, we use the\nﬁrst 6M tokens from the News Commentary data\nand 16M tokens from News Crawl 2014 for train-\ning. For development and test data we use the re-\nmaining part of the News Commentary data con-\ncatenated with the WMT 2009-2014 test sets. Fi-\nnally, for Italian, we use a selection of 29M tokens\nfrom the PAIS`A corpus (Lyding et al., 2014), mainly\nincluding Wikipedia pages and, to a minor extent,\nWikibooks and Wikinews documents. For develop-\nment and test we randomly draw documents from\nthe same corpus.\n4.2 Setup\nOur baselines are a 5-gram language model\nwith Kneser-Ney smoothing, a Memory Network\n(MemN) (Sukhbaatar et al., 2015), a vanilla single-\nlayer LSTM, and two stacked LSTMs with two and\nthree layers respectively. N-gram models have been\nused intensively in many applications for their ex-\ncellent performance and fast training. Chen et al.\n(2015) show that n-gram model outperforms a pop-\nular feed-forward language model (Bengio et al.,\n324\n2003) on a one billion word benchmark (Chelba et\nal., 2013). While taking longer time to train, RNNs\nhave been proven superior to n-gram models.\nWe compare these baselines with our two model\narchitectures: RMR and RM. For each of our mod-\nels, we consider two settings: with or without tem-\nporal matrix (+tM or –tM), and linear vs. gating\ncomposition function. In total, we experiment with\neight RMN variants.\nFor all neural network models, we set the dimen-\nsion of word embeddings, the LSTM hidden states,\nits gates, the memory input, and output embeddings\nto 128. The memory size is set to 15. The bias of the\nLSTM’s forget gate is initialized to 1 (J´ozefowicz et\nal., 2015) while all other parameters are initialized\nuniformly in (−0.05,0.05). The initial learning rate\nis set to 1 and is halved at each epoch after the forth\nepoch. All models are trained for 15 epochs with\nstandard stochastic gradient descent (SGD). During\ntraining, we rescale the gradients whenever their\nnorm is greater than 5 (Pascanu et al., 2013).\nSentences with the same length are grouped into\nbuckets. Then, mini-batches of 20 sentences are\ndrawn from each bucket. We do not use truncated\nback-propagation through time, instead gradients\nare fully back-propagated from the end of each sen-\ntence to its beginning. When feeding in a new mini-\nbatch, the hidden states of LSTMs are reset to zeros,\nwhich ensures that the data is properly modeled at\nthe sentence level. For our RMN models, instead of\nusing padding, at time step t < n, we use a slice\nT[1 :t] ∈Rt×d of the temporal matrix T ∈Rn×d.\n4.3 Results\nPerplexities on the test data are given in Table 2.\nAll RMN variants largely outperform n-gram and\nMemN models, and most RMN variants also outper-\nform the competitive LSTM baselines. The best re-\nsults overall are obtained by RM with temporal ma-\ntrix and gating composition (+tM-g).\nOur results agree with the hypothesis of mitigat-\ning prediction error by explicitly using the last n\nwords in RNNs (Karpathy et al., 2015). We further\nobserve that using a temporal matrix always bene-\nﬁts the RM architectures. This can be explained by\nseeing the RM as a principled way to combine an\nLSTM and a neural n-gram model. By contrast,\nRMR works better without temporal matrix but its\nModel De It En\n5-gram – 225.8 167.5 219.0\nMemN 1 layer 169.3 127.5 188.2\nLSTM\n1 layer 135.8 108.0 145.1\n2 layers 128.6 105.9 139.7\n3 layers 125.1 106.5 136.6\nRMR\n+tM-l 127.5 109.9 133.3\n–tM-l 126.4 106.1 134.5\n+tM-g 126.2 99.5 135.2\n–tM-g 122.0 98.6 131.2\nRM\n+tM-l 121.5 92.4 127.2\n–tM-l 122.9 94.0 130.4\n+tM-g 118.6 88.9 128.8\n–tM-g 129.7 96.6 135.7\nTable 2: Perplexity comparison including RMN\nvariants with and without temporal matrix (tM) and\nlinear (l) versus gating (g) composition function.\noverall performance is not as good as RM. This sug-\ngests that we need a better mechanism to address\nthe interaction between MBs, which we leave to fu-\nture work. Finally, the proposed gating composition\nfunction outperforms the linear one in most cases.\nFor historical reasons, we also run a stacked three-\nlayer LSTM and a RM(+tM-g) on the much smaller\nPenn Treebank dataset (Marcus et al., 1993) with the\nsame setting described above. The respective per-\nplexities are 126.1 and 123.5.\n5 Attention Analysis\nThe goal of our RMN design is twofold: (i) to obtain\nbetter predictive power and (ii) to facilitate under-\nstanding of the model and discover patterns in data.\nIn Section 4, we have validated the predictive power\nof the RMN and below we investigate the source of\nthis performance based on linguistic assumptions of\nword co-occurrences and dependency structures.\n5.1 Positional and lexical analysis\nAs a ﬁrst step towards understanding RMN, we look\nat the average attention weights of each history word\nposition in the MB of our two best model variants\n(Figure 3). One can see that the attention mass tends\nto concentrate at the rightmost position (the current\n325\nen\nit\nde\n-15 -14 -13 -12 -11 -10 -9 -8 -7 -6 -5 -4 -3 -2 -1\nen\nit\nde\n0.08\n0.12\n0.16\n0.20\n0.24\nFigure 3: Average attention per position of RMN\nhistory. Top: RMR(–tM-g), bottom: RM(+tM-g).\nRightmost positions represent most recent history.\nword) and decreases when moving further to the\nleft (less recent words). This is not surprising since\nthe success of n-gram language models has demon-\nstrated that the most recent words provide important\ninformation for predicting the next word. Between\nthe two variants, the RM average attention mass is\nless concentrated to the right. This can be explained\nby the absence of an LSTM layer on top, meaning\nthat the MB in the RM architecture has to pay more\nattention to the more distant words in the past. The\nremaining analyses described below are performed\non the RM(+tM-g) architecture as this yields the best\nperplexity results overall.\nBeyond average attention weights, we are inter-\nested in those cases where attention focuses on dis-\ntant positions. To this end, we randomly sample 100\nwords from test data and visualize attention distri-\nbutions over the last 15 words. Figure 4 shows the\nattention distributions for random samples of Ger-\nman and Italian. Again, in many cases attention\nweights concentrate around the last word (bottom\nrow). However, we observe that many long distance\nwords also receive noticeable attention mass. Inter-\nestingly, for many predicted words, attention is dis-\ntributed evenly over memory positions, possibly in-\nde\nit\nen\nFigure 4: Attention visualization of 100 word sam-\nples. Bottom positions in each plot represent most\nrecent history. Darker color means higher weight.\ndicating cases where the LSTM state already con-\ntains enough information to predict the next word.\nTo explain the long-distance dependencies, we\nﬁrst hypothesize that our RMN mostly memorizes\nfrequent co-occurrences. We run the RM(+tM-g)\nmodel on the German development and test sen-\ntences, and select those pairs of ( most-attended-\nword, word-to-predict ) where the MB’s attention\nconcentrates on a word more than six positions to\nthe left. Then, for each set of pairs with equal dis-\ntance, we compute the mean frequency of corre-\nsponding co-occurrences seen in the training data\n(Table 3). The lack of correlation between frequency\nand memory location suggests that RMN does more\nthan simply memorizing frequent co-occurrences.\nd 7 8 9 10 11 12 13 14 15\nµ 54 63 42 67 87 47 67 44 24\nTable 3: Mean frequency ( µ) of ( most-attended-\nword, word-to-predict) pairs grouped by relative dis-\ntance (d).\nPrevious work (Hermans and Schrauwen, 2013;\nKarpathy et al., 2015) studied this property of\nLSTMs by analyzing simple cases of closing brack-\nets. By contrast RMN allows us to discover more\ninteresting dependencies in the data. We manually\ninspect those high-frequency pairs to see whether\nthey display certain linguistic phenomena. We ob-\nserve that RMN captures, for example, separable\nverbs and ﬁxed expressions in German. Separable\nverbs are frequent in German: they typically consist\nof preposition+verb constructions, such ab+h¨angen\n(‘to depend’) or aus+schließen (‘to exclude’), and\ncan be spelled together ( abh¨angen) or apart as in\n‘h¨angen von der Situation ab ’ (‘depend on the sit-\nuation’), depending on the grammatical construc-\ntion. Figure 5a shows a long-dependency exam-\nple for the separable verb abh¨angen (to depend) .\nWhen predicting the verb’s particle ab, the model\ncorrectly attends to the verb’s core h¨angt occurring\nseven words to the left. Figure 5b and 5c show ﬁxed\nexpression examples from German and Italian, re-\nspectively: schl¨usselrolle ... spielen (play a key role)\nand insignito ... titolo (awarded title) . Here too, the\nmodel correctly attends to the key word despite its\nlong distance from the word to predict.\n326\nab (-1.8)\nund (-2.1)\n, (-2.5)\n. (-2.7)\nvon (-2.8)\n(a)            wie wirksam die daraus resultierende strategie sein wird , hängt daher von der genauigkeit dieser annahmen\nGloss:  how eﬀective   the  from-that   resulting        strategy     be     will,  depends therefore on the    accuracy     of-these measures\nTranslation: how eﬀective the resulting strategy will be, therefore, depends on the accuracy of these measures\nspielen (-1.9)\ngewinnen (-3.0)\nﬁnden (-3.4)\nhaben (-3.4)\nschaﬀen (-3.4)\n         … die lage versetzen werden , eine schlüsselrolle bei der eindämmung der regionalen ambitionen chinas zu\nGloss: … the position place         will,            a       key-role            in   the       curbing      of-the   regional      ambitions     China’s  to \nTranslation:  …which will put him in a position to play a key role in curbing the regional ambitions of China\n(b)\nsacro (-1.5)\ntitolo (-2.9)\nre (-3.0)\n<unk> (-3.1)\nleone (-3.6)\n        ...  che fu insignito  nel 1692  dall' Imperatore Leopoldo I del\nGloss: … who was  awarded   in 1692  by-the   Emperor   Leopold   I  of-the\nTranslation:  … who was awarded the title  by Emperor Leopold I in 1692 \n(c)\nFigure 5: Examples of distant memory positions attended by RMN. The resulting top ﬁve word predictions\nare shown with the respective log-probabilities. The correct choice (in bold) was ranked ﬁrst in sentences\n(a,b) and second in (c).\nOther interesting examples found by the RMN in\nthe test data include:\nGerman: ﬁndet statt (takes place), kehrte zur¨uck\n(came back), fragen antworten (questions\nanswers), k ¨ampfen gegen (ﬁght against),\nbleibt erhalten (remains intact), verantwortung\n¨ubernimmt (takes responsibility);\nItalian: sinistra destra (left right), latitudine lon-\ngitudine (latitude longitude), collegata tramite\n(connected through), spos `o ﬁgli (got-married\nchildren), insignito titolo (awarded title).\n5.2 Syntactic analysis\nIt has been conjectured that RNNs, and LSTMs in\nparticular, model text so well because they capture\nsyntactic structure implicitly. Unfortunately this has\nbeen hard to prove, but with our RMN model we can\nget closer to answering this important question.\nWe produce dependency parses for our test sets\nusing (Sennrich et al., 2013) for German and (At-\ntardi et al., 2009) for Italian. Next we look at\nhow much attention mass is concentrated by the\nRM(+tM-g) model on different dependency types.\nFigure 6 shows, for each language, a selection of\nten dependency types that are often long-distance. 2\nDependency direction is marked by an arrow: e.g.\n→mod means that the word to predict is a modiﬁer\nof the attended word, while mod←means that the\n2The full plots are available at https://github.com/\nketranm/RMN. The German and Italian tag sets are explained\nin (Simi et al., 2014) and (Foth, 2006) respectively.\nattended word is a modiﬁer of the word to predict. 3\nWhite cells denote combinations of position and de-\npendency type that were not present in the test data.\nWhile in most of the cases closest positions are\nattended the most, we can see that some dependency\ntypes also receive noticeably more attention than\nthe average ( ALL ) on the long-distance positions.\nIn German, this is mostly visible for the head of\nseparable verb particles ( →avz), which nicely sup-\nports our observations in the lexical analysis (Sec-\ntion 5.1). Other attended dependencies include: aux-\niliary verbs (→aux) when predicting the second el-\nement of a complex tense (hat . . . gesagt / has said);\nsubordinating conjunctions ( konj←) when predict-\ning the clause-ﬁnal inﬂected verb ( dass sie sagen\nsollten / that they should say); control verbs (→obji)\nwhen predicting the inﬁnitive verb ( versucht ihr\nzu helfen / tries to help her). Out of the Italian\ndependency types selected for their frequent long-\ndistance occurrences (bottom of Figure 6), the most\nattended are argument heads ( →arg), complement\nheads (→comp), object heads ( →obj) and subjects\n(subj←). This suggests that RMN is mainly captur-\ning predicate argument structure in Italian. Notice\nthat syntactic annotation is never used to train the\nmodel, but only to analyze its predictions.\nWe can also use RMN to discover which complex\ndependency paths are important for word prediction.\nTo mention just a few examples, high attention on\n3Some dependency directions, like obj←in Italian, are al-\nmost never observed due to order constraints of the language.\n327\n[-15, -12] [-11, -8] [-7, -4] -3 -2 -1\n[ALL ]\nsubj←\n→rel\n→obji\n→objc\nobja←\nkonj←\n→kon\n→avz\n→aux\nadv←\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n[-15, -12] [-11, -8] [-7, -4] -3 -2 -1\n[ALL ]\nsubj←\n→sub\n→pred\n→obj\nmod←\n→mod\n→con\ncomp←\n→comp\n→arg\n0.1\n0.2\n0.3\n0.4\n0.5\nFigure 6: Average attention weights per position,\nbroken down by dependency relation type+direction\nbetween the attended word and the word to predict.\nTop: German. Bottom: Italian. More distant posi-\ntions are binned.\nthe German path [subj←,→kon,→cj] indicates that\nthe model captures morphological agreement be-\ntween coordinate clauses in non-trivial constructions\nof the kind: spielen die Kinderim Garten und singen\n/ the children play in the garden and sing. In Italian,\nhigh attention on the path [→obj,→comp,→prep]\ndenotes cases where the semantic relatedness be-\ntween a verb and its object does not stop at the ob-\nject’s head, but percolates down to a prepositional\nphrase attached to it ( pass`o buona parte della sua\nvita / spent a large part of his life ). Interestingly,\nboth local n-gram context and immediate depen-\ndency context would have missed these relations.\nWhile much remains to be explored, our analysis\nshows that RMN discovers patterns far more com-\nplex than pairs of opening and closing brackets, and\nsuggests that the network’s hidden state captures to\na large extent the underlying structure of text.\n6 Sentence Completion Challenge\nThe Microsoft Research Sentence Completion Chal-\nlenge (Zweig and Burges, 2012) has recently be-\ncome a test bed for advancing statistical language\nmodeling. We choose this task to demonstrate the\neffectiveness of our RMN in capturing sentence co-\nherence. The test set consists of 1,040 sentences se-\nlected from ﬁve Sherlock Holmes novels by Conan\nDoyle. For each sentence, a content word is removed\nand the task is to identify the correct missing word\namong ﬁve given candidates. The task is carefully\ndesigned to be non-solvable for local language mod-\nels such as n-gram models. The best reported re-\nsult is 58.9% accuracy (Mikolov et al., 2013)4 which\nis far below human accuracy of 91% (Zweig and\nBurges, 2012).\nAs baseline we use a stacked three-layer LSTM.\nOur models are two variants of RM(+tM-g), each\nconsisting of three LSTM layers followed by a\nMB. The ﬁrst variant (unidirectional-RM) uses n\nwords preceding the word to predict, the second\n(bidirectional-RM) uses the nwords preceding and\nthe n words following the word to predict, as MB\ninput. We include bidirectional-RM in the experi-\nments to show the ﬂexibility of utilizing future con-\ntext in RMN.\nWe train all models on the standard training data\nof the challenge, which consists of 522 novels from\nProject Gutenberg, preprocessed similarly to (Mnih\nand Kavukcuoglu, 2013). After sentence splitting,\ntokenization and lowercasing, we randomly select\n19,000 sentences for validation. Training and val-\nidation sets include 47M and 190K tokens respec-\ntively. The vocabulary size is about 64,000.\nWe initialize and train all the networks as de-\nscribed in Section 4.2. Moreover, for regularization,\nwe place dropout (Srivastava et al., 2014) after each\nLSTM layer as suggested in (Pham et al., 2014). The\ndropout rate is set to 0.3 in all the experiments.\nTable 4 summarizes the results. It is worth to\nmention that our LSTM baseline outperforms a de-\npendency RNN making explicit use of syntactic in-\nformation (Mirowski and Vlachos, 2015) and per-\nforms on par with the best published result (Mikolov\net al., 2013). Our unidirectional-RM sets a new state\nof the art for the Sentence Completion Challenge\nwith 69.2% accuracy. Under the same setting of d\nwe observe that using bidirectional context does not\n4The authors use a weighted combination of skip-ngram and\nRNN without giving any technical details.\n328\nThe stage lost a ﬁne , even as science lost an acute reasoner , when he became a specialist in crime\na) linguist b) hunter c) actor♣ d) estate e) horseman ♦\nWhat passion of hatred can it be which leads a man to in such a place at such a time\na) lurk♣ b) dine♦ c) luxuriate d) grow e) wiggle\nMy heart is already since i have conﬁded my trouble to you\na) falling b) distressed ♦ c) soaring d) lightened♣ e) punished\nMy morning’s work has not been , since it has proved that he has the very strongest motives for\nstanding in the way of anything of the sort\na) invisible b) neglected ♦♣ c) overlooked d) wasted e) deliberate\nThat is his fault , but on the whole he’s a good worker\na) main b) successful c) mother’s ♣ d) generous e) favourite ♦\nFigure 7: Examples of sentence completion. The correct option is in boldface. Predictions by the LSTM\nbaseline and by our best RMN model are marked by ♦and ♣respectively.\nModel n d Accuracy\nLSTM – 256 56.0\nunidirectional-RM 15 256 64.3\n15 512 69.2\nbidirectional-RM 7 256 59.6\n10 512 67.0\nTable 4: Accuracy on 1,040 test sentences. We use\nperplexity to choose the best model. Dimension of\nword embeddings, LSTM hidden states, and gate g\nparameters are set to d.\nbring additional advantage to the model. Mnih and\nKavukcuoglu (2013) also report a similar observa-\ntion. We believe that RMN may achieve further im-\nprovements with hyper-parameter optimization.\nFigure 7 shows some examples where our best\nRMN beats the already very competitive LSTM\nbaseline, or where both models fail. We can see\nthat in some sentences the necessary clues to predict\nthe correct word occur only to its right. While this\nseems to conﬂict with the worse result obtained by\nthe bidirectional-RM, it is important to realize that\nprediction corresponds to the whole sentence prob-\nability. Therefore a badly chosen word can have a\nnegative effect on the score of future words. This ap-\npears to be particularly true for the RMN due to its\nability to directly access (distant) words in the his-\ntory. The better performance of unidirectional ver-\nsus bidirectional-RM may indicate that the attention\nin the memory block can be distributed reliably only\non words that have been already seen and summa-\nrized by the current LSTM state. In future work,\nwe may investigate whether different ways to com-\nbine two RMNs running in opposite directions fur-\nther improve accuracy on this challenging task.\n7 Conclusion\nWe have proposed the Recurrent Memory Network\n(RMN), a novel recurrent architecture for language\nmodeling. Our RMN outperforms LSTMs in terms\nof perplexity on three large dataset and allows us\nto analyze its behavior from a linguistic perspective.\nWe ﬁnd that RMNs learn important co-occurrences\nregardless of their distance. Even more interest-\ningly, our RMN implicitly captures certain depen-\ndency types that are important for word prediction,\ndespite being trained without any syntactic informa-\ntion. Finally RMNs obtain excellent performance at\nmodeling sentence coherence, setting a new state of\nthe art on the challenging sentence completion task.\nAcknowledgments\nThis research was funded in part by the Netherlands\nOrganization for Scientiﬁc Research (NWO) under\nproject numbers 639.022.213 and 612.001.218.\n329\nReferences\nGiuseppe Attardi, Felice Dell’Orletta, Maria Simi, and\nJoseph Turian. 2009. Accurate dependency parsing\nwith a stacked multilayer perceptron. In Proceedings\nof Evalita’09, Evaluation of NLP and Speech Tools for\nItalian, Reggio Emilia, Italy.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014. Neural machine translation by jointly\nlearning to align and translate. In ICLR 2015 , San\nDiego, CA, USA, May.\nYoshua Bengio, Patrice Simard, and Paolo Frasconi.\n1994. Learning long-term dependencies with gradient\ndescent is difﬁcult. Transaction on Neural Networks,\n5(2):157–166, March.\nYoshua Bengio, R ´ejean Ducharme, Pascal Vincent, and\nChristian Janvin. 2003. A neural probabilistic lan-\nguage model. J. Mach. Learn. Res. , 3:1137–1155,\nMarch.\nOndˇrej Bojar, Rajen Chatterjee, Christian Federmann,\nBarry Haddow, Matthias Huck, Chris Hokamp, Philipp\nKoehn, Varvara Logacheva, Christof Monz, Matteo\nNegri, Matt Post, Carolina Scarton, Lucia Specia, and\nMarco Turchi. 2015. Findings of the 2015 workshop\non statistical machine translation. In Proceedings of\nthe Tenth Workshop on Statistical Machine Transla-\ntion, pages 1–46, Lisbon, Portugal, September. Asso-\nciation for Computational Linguistics.\nSamuel R. Bowman, Christopher D. Manning, and\nChristopher Potts. 2015. Tree-structured composi-\ntion in neural networks without tree-structured archi-\ntectures. In Proceedings of Proceedings of the NIPS\n2015 Workshop on Cognitive Computation: Integrat-\ning Neural and Symbolic Approaches, December.\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robinson.\n2013. One billion word benchmark for measuring\nprogress in statistical language modeling. Technical\nreport, Google.\nWelin Chen, David Grangier, and Michael Auli. 2015.\nStrategies for Training Large V ocabulary Neural Lan-\nguage Models. ArXiv e-prints, December.\nKyunghyun Cho, Bart van Merrienboer, Dzmitry Bah-\ndanau, and Yoshua Bengio. 2014. On the proper-\nties of neural machine translation: Encoder–decoder\napproaches. In Proceedings of SSST-8, Eighth Work-\nshop on Syntax, Semantics and Structure in Statisti-\ncal Translation, pages 103–111, Doha, Qatar, October.\nAssociation for Computational Linguistics.\nJunyoung Chung, Caglar Gulcehre, KyungHyun Cho,\nand Yoshua Bengio. 2014. Empirical evaluation of\ngated recurrent neural networks on sequence mod-\neling. In NIPS Deep Learning and Representation\nLearning Workshop.\nChris Dyer, Miguel Ballesteros, Wang Ling, Austin\nMatthews, and Noah A. Smith. 2015. Transition-\nbased dependency parsing with stack long short-term\nmemory. In Proceedings of the 53rd Annual Meet-\ning of the Association for Computational Linguistics\nand the 7th International Joint Conference on Natural\nLanguage Processing, pages 334–343, Beijing, China,\nJuly. Association for Computational Linguistics.\nJeffrey L. Elman. 1990. Finding structure in time. Cog-\nnitive Science, 14(2):179–211.\nKatja Filippova, Enrique Alfonseca, Carlos A. Col-\nmenares, Lukasz Kaiser, and Oriol Vinyals. 2015.\nSentence compression by deletion with lstms. In Pro-\nceedings of the 2015 Conference on Empirical Meth-\nods in Natural Language Processing, pages 360–368,\nLisbon, Portugal, September. Association for Compu-\ntational Linguistics.\nKilian A. Foth. 2006. Eine umfassende Constraint-\nDependenz-Grammatik des Deutschen . Fachbereich\nInformatik.\nAlex Graves, Greg Wayne, and Ivo Danihelka. 2014.\nNeural turing machines. CoRR, abs/1410.5401.\nKlaus Greff, Rupesh Kumar Srivastava, Jan Koutn ´ık,\nBas R. Steunebrink, and J ¨urgen Schmidhuber.\n2015. LSTM: A search space odyssey. CoRR,\nabs/1503.04069.\nKarol Gregor, Ivo Danihelka, Alex Graves,\nDanilo Jimenez Rezende, and Daan Wierstra.\n2015. DRAW: A recurrent neural network for image\ngeneration. In Proceedings of the 32nd International\nConference on Machine Learning, ICML 2015, Lille,\nFrance, 6-11 July 2015, pages 1462–1471.\nMichiel Hermans and Benjamin Schrauwen. 2013.\nTraining and analysing deep recurrent neural net-\nworks. In C.J.C. Burges, L. Bottou, M. Welling,\nZ. Ghahramani, and K.Q. Weinberger, editors, Ad-\nvances in Neural Information Processing Systems 26 ,\npages 190–198. Curran Associates, Inc.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997. Long\nshort-term memory. Neural Computation, 9(8):1735–\n1780, November.\nRafal J´ozefowicz, Wojciech Zaremba, and Ilya Sutskever.\n2015. An empirical exploration of recurrent network\narchitectures. In Proceedings of the 32nd Interna-\ntional Conference on Machine Learning, ICML 2015,\nLille, France, 6-11 July 2015, pages 2342–2350.\nNal Kalchbrenner, Ivo Danihelka, and Alex Graves.\n2015. Grid long short-term memory. CoRR,\nabs/1507.01526.\nAndrej Karpathy, Justin Johnson, and Fei-Fei Li. 2015.\nVisualizing and understanding recurrent networks.\nCoRR, abs/1506.02078.\nVerena Lyding, Egon Stemle, Claudia Borghetti, Marco\nBrunello, Sara Castagnoli, Felice Dell’Orletta, Henrik\n330\nDittmann, Alessandro Lenci, and Vito Pirrelli. 2014.\nThe PAIS`A corpus of italian web texts. InProceedings\nof the 9th Web as Corpus Workshop (WaC-9) , pages\n36–43, Gothenburg, Sweden, April. Association for\nComputational Linguistics.\nMitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-\nrice Santorini. 1993. Building a large annotated cor-\npus of english: The penn treebank. Comput. Linguist.,\n19(2):313–330, June.\nTomas Mikolov, Martin Karaﬁ ´at, Luk ´as Burget, Jan\nCernock´y, and Sanjeev Khudanpur. 2010. Re-\ncurrent neural network based language model. In\nINTERSPEECH 2010, 11th Annual Conference of\nthe International Speech Communication Association,\nMakuhari, Chiba, Japan, September 26-30, 2010 ,\npages 1045–1048.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey\nDean. 2013. Efﬁcient estimation of word represen-\ntations in vector space. In ICLR.\nPiotr Mirowski and Andreas Vlachos. 2015. Depen-\ndency recurrent neural language models for sentence\ncompletion. In Proceedings of the 53rd Annual Meet-\ning of the Association for Computational Linguistics\nand the 7th International Joint Conference on Natural\nLanguage Processing (Volume 2: Short Papers), pages\n511–517, Beijing, China, July. Association for Com-\nputational Linguistics.\nAndriy Mnih and Koray Kavukcuoglu. 2013. Learning\nword embeddings efﬁciently with noise-contrastive es-\ntimation. In C.J.C. Burges, L. Bottou, M. Welling,\nZ. Ghahramani, and K.Q. Weinberger, editors, Ad-\nvances in Neural Information Processing Systems 26 ,\npages 2265–2273. Curran Associates, Inc.\nRazvan Pascanu, Tomas Mikolov, and Yoshua Bengio.\n2013. On the difﬁculty of training recurrent neural net-\nworks. In ICML (3), volume 28 ofJMLR Proceedings,\npages 1310–1318.\nVu Pham, Christopher Bluche, Th ´eodore Kermorvant,\nand J ´erˆome Louradour. 2014. Dropout improves re-\ncurrent neural networks for handwriting recognition.\nIn International Conference on Frontiers in Handwrit-\ning Recognition (ICFHR), pages 285–290, Sept.\nRico Sennrich, Martin V olk, and Gerold Schneider. 2013.\nExploiting synergies between open resources for ger-\nman dependency parsing, pos-tagging, and morpho-\nlogical analysis. In Recent Advances in Natural Lan-\nguage Processing (RANLP 2013) , pages 601–609,\nSeptember.\nMaria Simi, Cristina Bosco, and Simonetta Montemagni.\n2014. Less is more? towards a reduced inventory of\ncategories for training a parser for the italian stanford\ndependencies. In Proceedings of the Ninth Interna-\ntional Conference on Language Resources and Evalu-\nation (LREC’14), Reykjavik, Iceland, may. European\nLanguage Resources Association (ELRA).\nRichard Socher, Jeffrey Pennington, Eric H. Huang, An-\ndrew Y . Ng, and Christopher D. Manning. 2011.\nSemi-supervised recursive autoencoders for predicting\nsentiment distributions. In Proceedings of the Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, EMNLP ’11, pages 151–161, Stroudsburg,\nPA, USA. Association for Computational Linguistics.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: A simple way to prevent neural networks\nfrom overﬁtting. J. Mach. Learn. Res. , 15(1):1929–\n1958, January.\nSainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and\nRob Fergus. 2015. End-to-end memory networks. In\nC. Cortes, N.D. Lawrence, D.D. Lee, M. Sugiyama,\nR. Garnett, and R. Garnett, editors, Advances in Neu-\nral Information Processing Systems 28 , pages 2431–\n2439. Curran Associates, Inc.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural networks.\nIn Z. Ghahramani, M. Welling, C. Cortes, N.D.\nLawrence, and K.Q. Weinberger, editors, Advances\nin Neural Information Processing Systems 27 , pages\n3104–3112. Curran Associates, Inc.\nGeoffrey Zweig and Chris J. C. Burges. 2012. A chal-\nlenge set for advancing language modeling. In Pro-\nceedings of the NAACL-HLT 2012 Workshop: Will We\nEver Really Replace the N-gram Model? On the Fu-\nture of Language Modeling for HLT, WLM ’12, pages\n29–36, Stroudsburg, PA, USA. Association for Com-\nputational Linguistics.\n331",
  "topic": "Recurrent neural network",
  "concepts": [
    {
      "name": "Recurrent neural network",
      "score": 0.9013994336128235
    },
    {
      "name": "Computer science",
      "score": 0.8323636054992676
    },
    {
      "name": "Margin (machine learning)",
      "score": 0.7589275240898132
    },
    {
      "name": "Sentence",
      "score": 0.7189675569534302
    },
    {
      "name": "Natural language processing",
      "score": 0.6675776243209839
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6254119873046875
    },
    {
      "name": "Language model",
      "score": 0.6027831435203552
    },
    {
      "name": "Coherence (philosophical gambling strategy)",
      "score": 0.5864515900611877
    },
    {
      "name": "German",
      "score": 0.5360693335533142
    },
    {
      "name": "Natural language",
      "score": 0.45164960622787476
    },
    {
      "name": "Artificial neural network",
      "score": 0.29748430848121643
    },
    {
      "name": "Machine learning",
      "score": 0.24958637356758118
    },
    {
      "name": "Linguistics",
      "score": 0.09798115491867065
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I887064364",
      "name": "University of Amsterdam",
      "country": "NL"
    }
  ],
  "cited_by": 76
}