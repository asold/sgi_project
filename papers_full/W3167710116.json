{
  "title": "Exposing the Implicit Energy Networks behind Masked Language Models via Metropolis--Hastings",
  "url": "https://openalex.org/W3167710116",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4222946049",
      "name": "Goyal, Kartik",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221974744",
      "name": "Dyer Chris",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221943472",
      "name": "Berg-Kirkpatrick, Taylor",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2787560479",
    "https://openalex.org/W3022372506",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W86781249",
    "https://openalex.org/W2988975212",
    "https://openalex.org/W2949888546",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2112814716",
    "https://openalex.org/W3106061119",
    "https://openalex.org/W2970607325",
    "https://openalex.org/W3126581100",
    "https://openalex.org/W1860991815",
    "https://openalex.org/W2767206889",
    "https://openalex.org/W2964017345",
    "https://openalex.org/W2964205912",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2161914416",
    "https://openalex.org/W768466067",
    "https://openalex.org/W2138309709",
    "https://openalex.org/W2913129712",
    "https://openalex.org/W3034775979",
    "https://openalex.org/W2083875149",
    "https://openalex.org/W3018305985",
    "https://openalex.org/W2521028896",
    "https://openalex.org/W2947898088",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3013571468",
    "https://openalex.org/W2938080845",
    "https://openalex.org/W2938704169"
  ],
  "abstract": "While recent work has shown that scores from models trained by the ubiquitous masked language modeling (MLM) objective effectively discriminate probable from improbable sequences, it is still an open question if these MLMs specify a principled probability distribution over the space of possible sequences. In this paper, we interpret MLMs as energy-based sequence models and propose two energy parametrizations derivable from the trained MLMs. In order to draw samples correctly from these models, we develop a tractable sampling scheme based on the Metropolis--Hastings Monte Carlo algorithm. In our approach, samples are proposed from the same masked conditionals used for training the masked language models, and they are accepted or rejected based on their energy values according to the target distribution. We validate the effectiveness of the proposed parametrizations by exploring the quality of samples drawn from these energy-based models for both open-ended unconditional generation and a conditional generation task of machine translation. We theoretically and empirically justify our sampling algorithm by showing that the masked conditionals on their own do not yield a Markov chain whose stationary distribution is that of our target distribution, and our approach generates higher quality samples than other recently proposed undirected generation approaches (Wang et al., 2019, Ghazvininejad et al., 2019).",
  "full_text": "Published as a conference paper at ICLR 2022\nEXPOSING THE IMPLICIT ENERGY NETWORKS\nBEHIND MASKED LANGUAGE MODELS VIA\nMETROPOLIS –HASTINGS\nKartik Goyal1, Chris Dyer2, Taylor Berg-Kirkpatrick3\n1Carnegie Mellon University, 2Deepmind, 3UC San Diego\nkartikgo@ttic.edu, cdyer@google.com, tberg@eng.ucsd.edu\nABSTRACT\nWhile recent work has shown that scores from models trained by the ubiquitous\nmasked language modeling (MLM) objective effectively discriminate probable\nfrom improbable sequences, it is still an open question if these MLMs specify\na principled probability distribution over the space of possible sequences. In\nthis paper, we interpret MLMs as energy-based sequence models and propose\ntwo energy parametrizations derivable from the trained MLMs. In order to draw\nsamples correctly from these models, we develop a tractablesampling scheme based\non the Metropolis–Hastings Monte Carlo algorithm. In our approach, samples\nare proposed from the same masked conditionals used for training the masked\nlanguage models, and they are accepted or rejected based on their energy values\naccording to the target distribution. We validate the effectiveness of the proposed\nparametrizations by exploring the quality of samples drawn from these energy-\nbased models for both open-ended unconditional generation and a conditional\ngeneration task of machine translation. We theoretically and empirically justify\nour sampling algorithm by showing that the masked conditionals on their own\ndo not yield a Markov chain whose stationary distribution is that of our target\ndistribution, and our approach generates higher quality samples than other recently\nproposed undirected generation approaches (Wang and Cho, 2019; Ghazvininejad\net al., 2019).\n1 I NTRODUCTION\nMasked language modeling (MLM) objectives (Devlin et al., 2018; Yang et al., 2019; Gu et al., 2017)\nfor sequences, although recent, have become ubiquitous for many Natural Language Processing\n(NLP) applications (Liu et al., 2019; Zhang et al., 2019; Rogers et al., 2021) because they are easy\nto optimize and enable learning of highly expressive and ﬂexible representations by the virtue of\nconditioning on bidirectional context (Peters et al., 2018; Devlin et al., 2018). However despite their\npopularity, they lack a principled probabilistic interpretation and hence sampling from MLMs, or\ncharacterizing uncertainty about the predictions made with them has remained elusive.\nIn this work, we posit that optimizing MLM objectives results in training of implicit energy-based\nsequence models that correspond to probability distributions over natural language sequences by\nassigning a score to each possible sequence in the large but ﬁnite sequence space. To explore the\nveracity of this claim, we develop and experiment with two energy parametrizations (or scoring\nschemes) that can be easily derived from the representations learned by the trained MLMs’ transformer\nnetworks. These parametrizations have been inspired by the success of recent work on using MLMs\nfor sentence-level judgements for discriminating between probable and improbable sequences (Salazar\net al.; Zhang et al., 2019). Although, it is easy to compute the energy/score of a sequence with\nthese MLM-based parametrizations, the bidirectional nature of MLMs precludes efﬁcient sampling\nalgorithms like ancestral sampling. Therefore, a primary contribution of our work is to develop\nMetropolis-Hastings (MH) based sampling algorithms for these MLM-based energy networks. While\nit is tempting to formulate a Gibbs sampling scheme (Gelfand and Smith, 1990) based on the positional\nmasked conditional distributions used for training the MLMs (Wang and Cho, 2019), we theoretically\nand empirically demonstrate that these masked conditional distributions do not necessarily correspond\nto any joint distribution or energy network and hence result in invalid Gibbs samplers. Instead, we\n1\narXiv:2106.02736v2  [cs.LG]  15 Mar 2022\nPublished as a conference paper at ICLR 2022\npropose to use these masked conditionals as proposal distributions for transitioning to a new state\n(sequence) in the Markov chain of an MCMC sampler based on the Metropolis-Hastings algorithm\n(Hastings, 1970). Another contribution of our work is to design a ﬂexible non-contiguous block-\nreplacement proposal distribution to improve mixing of the Markov chain in our proposed MH\nsampling framework, which results in faster generation and better samples.\nWe empirically investigate the effectiveness of the two proposed energy parametrizations by examin-\ning the quality of samples drawn from these energy-models in two diverse settings: 1) conditional\ngeneration task of Machine Translation (MT), and 2) Open-ended unconditional generation. We\nobserve that high BLEU scores for MT, and high ﬂuency scores are correlated with low energy\nvalues which indicates that these parametrizations are reasonable proxies for the desired implicit\nbidirectional energy network trained via the MLM objective. We study the behavior of our sampling\napproach extensively with different proposal distributions. We also verify the soundness of our ap-\nproach by sampling from regions around the mode by annealing the target distribution and ﬁnding our\nsamples to be competitive with a prominent undirected (and non-probablistic) generation approach\n(Ghazvininejad et al., 2019) on MT performance. Moreover, human evaluation of the open ended\ngeneration samples further corroborates the effectiveness of our approach.\nRelated work:Much of prior work on energy-based methods has, in contrast to our work, focused on\nexplicitly training energy networks from scratch. Gradient based training of energy networks (LeCun\net al., 2006; Zhao et al., 2016; Du and Mordatch, 2019) has been successful at training models for\ninducing distributions over continuous domains but are not suitable for training discrete sequence\nmodels for text. To overcome this problem, recent work has proposed continuous relaxations to the\ndiscrete domain (Belanger and McCallum, 2016; Grathwohl et al., 2021), but the unordered nature\nof discrete symbols in text makes it difﬁcult for these models to be effective large scale language\nmodels. Wang and Ou (2017) train text-based energy networks directly via MCMC sampling with a\nCNN-LSTM based energy network and a backbone of autoregressive proposal distribution, but ﬁnd\nit to be computationally expensive. For more practical unnormalized models, NCE based training\nobjectives have also been considered (Deng et al., 2020; Wang and Ou, 2018) in prior work. The\nparametrization and training of these energy based text models relies on a backbone autoregressive\nmodel (Brown et al., 2020; Sutskever et al., 2014), which causes the resulting energy model to be\nheavily affected by the underlying base autoregressive model, and leads to left-to-right generation\nprocedures that preclude non-contiguous block sampling. Other training approaches for energy based\ntext models include decoding based approaches (Goyal et al., 2019; Wiseman and Rush, 2016), and\nscore-based approaches (Zhang et al., 2017). All of these approaches are more expensive than training\nprobabilistic autoregressive models like GPT-3, and hence adoption of energy based unnormalized\nmodels for large scale language modelling has been under-explored.\nIn this work, instead of training computationally expensive energy networks on large scale data, we\nfocus on interpreting large pretrained MLMs as energy-based models and propose methods to sample\nfrom them. The MLM objectives (Devlin et al., 2018; Clark et al., 2020a;b) are easy to scale to large\namounts of data and learn good representations of the textual data, but they do not have a probabilistic\ninterpretation. While there have been attempts to train MLM-based non-autoregressive sequence\nmodels and generate sequences from the MLMs in a non(pseudo)-probabilistic manner (Wang and\nCho, 2019; Ghazvininejad et al., 2019; Tu et al., 2020; Gu et al., 2017; Mansimov et al., 2019), our\ntechnique samples correctly from the energy networks underlying MLMs that correspond to our\nproposed parametrizations. Our detailed experiments with the proposed sampler provide evidence\nthat masked language modelling objectives result in implicit training of an energy network, and these\nﬁndings suggest that further exploration in future work, of variants of these objectives for explicit\ntraining of large scale energy networks is a worthwhile endeavour.\n2 M ASKED LANGUAGE MODELS AND ENERGY NETWORKS\nLet Xbe the space of all ﬁnite sequences up to a maximum length, and p(X; θ) be the probability\nof the sequence X ∈ Xunder the target distribution deﬁned by the energy function E(X; θ)\nparametrized by θ, deﬁned as follows:\np(X; θ) = e−E(X;θ)\n∑\nX′∈Xe−E(X′;θ) = φ(X; θ)\nZ(θ)\nwhere φrepresents the unnormalized score of the sequence X and Z(θ) is the intractable normal-\nization constant computed by summing over all the sequences. We propose two parametrizations of\n2\nPublished as a conference paper at ICLR 2022\nenergy functions that potenitally correspond to the implicit networks trained via MLM objectives:\n1) raw scoring, and 2) Locally normalized scoring. These parametrizations yield scoring functions\nthat have been considered useful in prior work. These scoring functions use the same computational\nmechanisms as typical computation of conditional distributions of the [MASK] tokens with MLMs.\n2.1 R AW SCORING\nFor each position tin the sequence X of length T, we associate a random variable Xt ∈V with the\nt-th token, where V is the vocabulary. MLMs learn a representation h(X\\t), for Xt that is sensitive\nto the bidirectional surrounding context X\\t. For notational convenience, we use Xt=w,\\t to denote a\nsequence Xwith the t-th variable taking on the valuew. We use such bidirectional neural parametriza-\ntions to deﬁne an energy Eraw for X that corresponds to fully connected Gibbs random ﬁelds as\nthe sum of the local positional scores: Eraw(X; θ) = −∑T\nt=1 log φt(X; θ),where log φt(X; θ) =\nf(Xt,h(X\\t)); θ). In our experiments, the positional log-potentials f(Xt,h(X\\t)); θ) are computed\nby masking the position t, then running a forward pass on the MLM’s transformer and using the raw\nlogits at the masked position.\nConditional distribution underEraw: Performing Gibbs sampling from the MRF deﬁned by Eraw\nrequires computation of this conditional distribution of a token given the surrounding context:\np(Xi|X\\i; θ) =\n∏\ntφt(X; θ)∑\nw∈V\n∏\ntφt(((Xi=w,\\i); θ))\nComputing this conditional is very expensive and would require running |V|passes of the MLM\ndecoder just for computing the positional potential (φt) at a single position because these potentials\nform fully connected cliques. Thus, we do not perform Gibbs sampling and instead propose MH\nbased samplers described below.\nRelationship with the masked conditionals of MLMs:Wang and Cho (2019)’s prior attempt to\ninterpret a MLM (like BERT) as an MRF incorrectly 1 assumes that the positional potentials are\nindependent of each other and hence are not deﬁned on a fully connected clique, i.e. φt(X; θ) =\nφt(Xt; θ). This faulty assumption about the factorization of the positional potentials φt(X; θ) leads\nto the following formulation of conditional distribution:\npmlm(Xi|X\\i; θ) =\n∏\ntφt(Xt; θ)∑\nw∈V\n∏\ntφt(((Xi=w,\\i,t); θ)) = φi(Xi; θ)∑\nX′\ni∈V\nφi(X′\ni; θ)\n∏\nt̸=i\nφt(Xt; θ)\nφt(Xt; θ) = softmax(log φi)\nThis deﬁcient conditional distribution for the MRF corresponds to the free conditional distribution\npmlm(Xt |X\\t) that is obtained by performing a softmax operation over[MASK] scores (∈RV) used\nin the MLM training objective. These MLM free conditionals do not correspond to the MRF deﬁned\nby Eraw i.e. pmlm(Xi |X\\i) ̸= p(Xi|X\\i; θ(Eraw)). In fact, these conditionals need not correspond\nto any consistent MRF over the sequences. As an example, consider a sequence of length 2 with\nthe random variables X1, X2 that have a categorical distribution over a vocabularyV = {a,b}. The\nfollowing free conditionals are inconsistent (see Appendix) because they do not correspond to any\nvalid joint distribution over {X1,X2}: p(X1 |X2) =\n[\n0.99 0 .01\n0.01 0 .99\n]\n,p(X2 |X1) =\n[\n0.5 0 .5\n0.5 0 .5\n]\n. It\nshould be noted that prior work on dependency networks (Heckerman et al., 2000) proposed a similar\nscheme of training the conditionals independently with separate regressions over the latent variables\nand the inconsistency of such conditionals is well documented (Gelman and Raghunathan, 2001;\nDobra et al., 2004; Lowd, 2012).\nWang and Cho (2019) used the masked conditionals to deﬁne a pseudolikelihood (∏T\nt=1 pmlm(Xt |\nX\\t; θ)) maximization objective and argued that MLM training can be interpreted as stochastic\nmaximization of this pseudolikelihood corresponding to the energy function Eraw. However, this is\nincorrect because the conditionals used to deﬁne the pseudolikelihood under Eraw are deﬁcient and\nlikely inconsistent. Despite the incongruity between MLM training and minimization of Eraw, we\npropose to use Eraw as one of the parametrizations of the energy function.\n1This was addressed in their subsequently published erratum: https://bit.ly/2TXS2KM.\n3\nPublished as a conference paper at ICLR 2022\n2.2 L OCALLY NORMALIZED SCORING\nRecent work (Zhang et al., 2019) has shown that MLMs like BERT can be used to reliably score\na set of sequences. Salazar et al. and Clark et al. (2020a) developed a scoring scheme to rescore\nhypotheses proposed by the beam search algorithm and showed downstream improvements over\nautomatic speech recognition (ASR) and machine translation (MT) datasets. The scoring scheme\ncorresponds to masking tokens one-by-one in a left-to-right manner and summing the log-probability\nof the token at each masked position in the sequence: Enorm(X; θ) =−∑T\nt=1 log pmlm(Xt|X\\t; θ).\nThis scoring scheme is also implicitly used while performing beam search with the non-autoregressive\nNMT models proposed in Ghazvininejad et al. (2019). Because of this scheme’s success, we propose\nthis as another parametrization which requires just one change in the computational procedure for\nEraw–instead of summing up raw logit scores at each position, this energy is computed by summing\nup post softmax values at each position.\n3 B ACKGROUND : M ETROPOLIS HASTINGS\nMetropolis Hastings (Hastings, 1970) is an MCMC algorithm that provides a recipe for sampling\nfrom the distribution pvia a proposal distribution q(X′|X,γ) parametrized by γ, which deﬁnes\ntransition from sequence X to the sequence X′ in the Markov chain. It assumes the ability to\ncompute the unnormalized score φ(X) for every sequence X. At each sampling step we ﬁrst\ndraw a proposal X′from the proposal distribution. Then, we either transition to this new state\nwith the acceptance probability a(X′; X), or repeat the sequence X in the Markov chain. The\nacceptance probability for the step that ensures that the MCMC sampler satisﬁes detailed balance\nis: a(X′; X) = min\n(\n1,φ(X′) q(X|X′)\nφ(X) q(X′|X)\n)\n. Additionally, since it is highly unlikely that the neurally\nparametrized models like MLMs will assign any sequence a probability 0, it is safe to assume\nergodicity of the Markov chains with this sampler, which guarantees convergence to the desired target\nenergy network distribution p. In our experiments, the unnormalized score φ(X) is computed by\nusing the transformer parametrization of the MLM of interest. Both our energy formulations involve\ncomputing positional potentials which are obtained by iteratively masking the token at each position\nand running the forward pass of the MLM transformer.\nAlgorithm 1Metropolis Hastings algorithm for MLMs\n1: Input: MLM transformer σ, Energy function fE, MLM conditional proposal fmlm ,sequence length T,\nnumber of epochs E\n2: Initialize: X ←[MASK]T\n3: X ←greedy-decode(MLM(X)) ⊿Warm-start with a random sequence\n4: for e=0 to E do\n5: for t=0 to T do ⊿left-to-right or random position selection\n6: Eold ←fE(σ(X)) ⊿Energy of sequence X, O(T) op.\n7: X′ ←X, wo ←Xt, X′\nt ←[MASK] ⊿Store the t-th token in X as wo and mask it.\n8: wn ∼fmlm(σ(X),t), X′\nt ←wn ⊿Sample wn from MLM conditional to propose X′.\n9: q(X′ |X) =fmlm(σ(X),t)[wn], q(X |X′) =fmlm(σ(X),t)[wo]\n10: Enew ←fE(σ(X′)) ⊿Energy of proposed sequence X′, O(T) op.\n11: a(X′; X) ←min\n(\n1,e−Enew q(X|X′)\ne−Eold q(X′|X)\n)\n⊿Acceptance probability of the MC transition.\n12: if u∼U(0,1), u≤athen X ←X′\n13: Output: sampled sequence X\n3.1 M ASKED CONDITIONALS AS PROPOSAL DISTRIBUTION FOR THE MH SAMPLER\nAs we discuss in Section 2.1, the masked conditionals used to train MLMs do not correspond to\nthe two energy formulations we experiment with and are not appropriate for performing Gibbs\nsampling. In fact, our experiments demonstrate that performing Gibbs sampling using these masked\nconditionals leads to low-quality samples. However, these conditionals have been shown to be\nuseful for scoring individual sequences and non-autoregressive generation. Therefore, we propose\nto deﬁne the proposal distribution q(X′|X) for the Metropolis-Hastings sampler by these masked\nconditionals. More concretely, to transition from the sequence X, we ﬁrst mask the token in X\nat position i, i.e., Xi = [MASK]. Next, we do a Transformer decoder pass and get the masked\nconditionals pmlm at position i. Then, the probability of the transition to sequence X′is the masked\nprobability of the token at the i-th position in X′, i.e.: q(X′|X) =pmlm(X′\ni|X\\i; θ),where X\\i =\n4\nPublished as a conference paper at ICLR 2022\nX′\n\\i and q(X |X′) =pmlm(Xi|X\\i; θ).For both Gibbs sampling and MH sampling schemes, we\nsweep over all the positions in a random order while generating sequences of a certain length. We\ndenote one complete sweep over all the positions in a sequence of length T by the term epoch. We\nsummarize our general approach in Alg. 1.\nComputational complexity: Amortizing the encoder cost and the cost of performing a softmax\noperation, if we denote the cost of doing one Transformer decoder pass over a masked sequence\nby C, then the computational complexity of evaluating MLM conditional is O(C). For Eepochs\nand a sequence of length T, the cost of running a Gibbs sampler is O(TEC). For the MH sampler,\nwe additionally need to compute the unnormalized scores φ(X) which, for both the proposed\nparametrizations of energy, require masking of each position sequentially and running a Transformer\ndecoder pass for each masked sequence. Hence the MH sampler is more computationally expensive\nwith the complexity O(T2EC).\n3.2 V ARIANTS OF PROPOSAL DISTRIBUTION\nWe studied our sampler with multiple proposal distributions. While all the variants of proposal\ndistribution rely heavily on the masked conditionals from the pretrained MLM, they have different\nproperties and as shown in the results, they exhibit very different behaviors.\nVarying temperature:We experiment by changing the entropy of the masked conditionals via a\ntemperature hyperparameter T: q(X′|X; T) =pmlm(X′\ni|X\\i; θ,T ) =softmax(log φi\nT ).\nVariation based on Nucleus Sampling:We experiment with another method of changing the\nentropy of the masked conditional distribution that is inspired by Nucleus Sampling (Holtzman et al.,\n2019). It involves deﬁning a nucleus boundary b, which prunes out the long tail of the vocabulary that\nfalls outside of the cumulative probability bfollowed by renormalization over the pruned vocabulary\nVb which is the smallest set such that ∑\nw∈Vb\npmlm(X′\ni = w|X\\i; θ) ≥b.\nBlock MH sampling:Block sampling methods like block Gibbs sampling (Gelfand, 2000) result\nin better mixing of the Markov chain because they allow for perturbations to multiple variables. In\nour approach, we mask out multiple tokens in a sequence X in order to propose a new sequence X′.\nLet Ibe the set of positions by which X and X′differ. Then, the proposal distribution for the MH\nsampler is: q(X′|X) =∏\ni∈Ipmlm(X′\ni|X\\I; θ). This makes sampling faster due to parallelization\nof prediction at several positions, and results in generation of better samples.\n4 I MPLEMENTATION DETAILS\nPretrained Masked Language Model:We empirically study the proposed Metropolis Hastings\nscheme on the conditional generation task of neural machine translation (NMT) and the task of uncon-\nditional generation. For unconditional generation we used HuggingFace’s pytorch implementation of\nuncased BERT-base and BERT-large. ForNMT, to perform fair comparison we use the pretrained\nmodels2 optimized by a prominent non-autoregressive algorithm–MASK -PREDICT (Ghazvininejad\net al., 2019). This non-probabilistic algorithm uses a bidirectional Transformer (Vaswani et al., 2017)\nto encode the source-side sequence and trains the target-side bidirectional transformer-based decoder\nvia the MLM objective while performing iterative reﬁnement for decoding. We implemented our\nsampler and parametrizations on top of this code-base for non-autoregressive MT.\nMCMC details for NMT:For all the sampling baselines, after a burn-in period of 7 epochs, we\nran the Markov chain for at least 26 epochs over the dataset. The length of the target sample was\ndecided by using a length predictor of MASK -PREDICT that estimates the length conditioned on the\nsource sentence. For all of our sampling results described, we ran at least 5 Markov chains for each\nconﬁguration described in the subsequent sections and report averaged statistics over these runs.\nMCMC details for unconditional generation:For the reported experimental settings, we ran 500\nchains for 100 epochs to produce 500 sequences of diverse lengths varying from 15 −45. For each\nof the Markov chains, we randomly select a length and start with a sequence consisting entirely of\n[MASK] tokens. We accept all the proposals until all the masked tokens are ﬁlled out in order to start\nthe chain from a random sequence.\nData for NMT:We performed experiments via translating the validation and test sets of the WMT-14\nGerman-English (De-En), and the WMT-16 Romanian-English (Ro-En) datasets and perform the\nsame tokenization and pre/post-processing as Ghazvininejad et al. (2019).\n2https://github.com/facebookresearch/Mask-Predict\n5\nPublished as a conference paper at ICLR 2022\nEvaluating quality of samples:Aside from considering the energy values of the samples under\nour parametrization and other measures of qualitative evaluation, we report the following automatic\nmetrics for NMT and unconditional generation respectively: 1) BLEU scores on the reference corpus\ngive an idea about the practical quality of samples for conditional generation in low-entropy settings\nlike NMT, 2) GPT2-xl (Radford et al., 2019) sentence perplexity (not token normalized) of random\nunconditionally generated samples provides a reasonable idea of the ﬂuency of the generated sequence.\nCompared to the BERT models, GPT-2 is an autoregressive language model that has been trained on\na larger amount of internet data than the BERT models.\n5 M ETROPOLIS HASTINGS AND Degenerate GIBBS SAMPLING FOR MLM S\nIn this section, we empirically compare our proposed MH Sampling approach with both the energy\nformulations described in Section 2 (raw and norm) to the alternative proposed by Wang and Cho\n(2019) of performing Gibbs sampling with the masked free conditionals which we refer to as\ndegenerate Gibbs sampling (deg).\nburn-in burn-in\nFigure 1: −Enorm (left) and BLEU scores (right) on De-En (20) for NMT as a function of epochs for\nthe two MH schemes (raw and norm) and the degenerate Gibbs sampling scheme (deg). We compute\nand report −Enorm even for the samplers with Eraw parametrization.\nIn Figure 1, we notice that for NMT, although all the samplers start with the same random sequence,\nthe proposed MH samplers generate high quality samples with low energy values and consistently\ngood BLEU scores across all the epochs. The degenerate Gibbs sampler however, keeps on degrading\nto generating sequences with very low BLEU scores and high energy values. We also observe that\nsequences with high BLEU scores typically have low locally normalized energies which explains\nthe success of prior work in using these locally normalized scores for re-ranking beam search\nhypotheses and generating sequences with high BLEU scores (Salazar et al.; Ghazvininejad et al.,\n2019). For open-ended generation we observed a similar pattern that the energy values deteriorate as\nthe degenerate sampler is run longer, but the chain with Enorm was consistently worse than Eraw.\nNext, we examine the acceptance ratio of the MH samplers. We focus on the average proportion\nof novel MC transition rate–the ratio of proposals that were distinct from the previous state and\naccepted–which indicates the entropy of the MCMC transition distribution. For NMT, the degenerate\nGibbs sampler has acceptance probability of 1.0 by design and a novel transition ratio of0.36, which\nindicates that the MLM conditionals are fairly peaked. Both the MH samplers have high acceptance\nrates (0.9 and 0.91) but much lower novel transition ratio–0.11 for RAW and 0.13 for RAW. This\nindicates slow mixing of the MH Markov chain. For unconditional generation, the novel transition\nratio of the degenerate sampler is higher 0.58, but it is slightly lower for the MH samplers–0.10 for\nRAW and 0.08 for RAW. This suggests, that the BERT conditionals yield proposals that are rejected\nat a very high rate under our parametrization schemes for open-ended generation.\n6 R ESULTS WITH VARIANTS OF PROPOSAL DISTRIBUTIONS\n6.1 E FFECT OF TEMPERATURE\nIn this section, we explore the effect of temperature on the proposal distributions parametrized by the\nMLM conditionals as described in Section 3.2, varying the proposal distributions from high entropy\nto low entropy. In Tables 1 and 2, we see that for MT, the MH sampler performs similarly across\n6\nPublished as a conference paper at ICLR 2022\nall the temperature values with the performance improving slightly for lower temperature values,\nhowever unconditional generation is signiﬁcantly more sensitive to the temperature changes, with\nworse performance at the higher temperature. The degenerate Gibbs sampler in general trails behind\nMH samplers but drastically improves with the lowering temperature values. At low temperatures,\nit yields decent BLEU scores and more ﬂuent sentences but it is noteworthy that the energy values\nare worse than the MH sampler. Most interestingly, the novel transition rates reﬂect the effect of\nTable 1: Average Enorm ×10−3 energy, novel MC transition rate, and BLEU scores for NMT across\ninterleaved epochs for the degenerate Gibbs sampling (deg) and the locally normalized energy MH\nscheme (Norm) on De-En (20) under MLM proposal distributions with varying temperatures.\nTemp 2.0 1.5 1.0 0.8 0.5\nnorm deg norm deg norm deg norm deg norm deg\nEnorm ↓ 12.87 32.46 10.21 29.57 11.13 31.12 9.95 21.12 7.85 17.65\nNovel ↔ 0.03 1.0 0.05 0.97 0.11 0.36 0.06 0.08 0.03 0.04\nBLEU ↑ 25.91 14.53 24.78 10.12 24.74 9.03 25.84 24.77 27.23 26.12\nTable 2: Average Eraw energy, novel MC transition rate, and average GPT-2 sentence perplexity for\nunconditional generation across generated sequences for the degenerate Gibbs sampling (deg) and\nthe raw energy MH scheme (raw) under MLM proposal distributions with varying temperatures.\nTemp 1.2 1.0 0.8 0.5\nraw deg raw deg raw deg raw deg\nEraw ↓ 25.95 201.24 19.82 83.23 13.54 23.24 9.91 10.05\nNovel ↔ 0.09 0.82 0.09 0.81 0.08 0.25 0.06 0.09\nGPT-2 ↓ 223.87 2238.6 108.12 314.74 82.23 88.15 77.44 77.98\ntemperature very clearly. At high temperatures, the degenerate Gibbs sampler almost never proposes\nconsecutively repeating transitions while in stark contrast, the novel transition rate of the MH sampler\nis extremely low. This is because of high rejection rates under the unsuitable high-entropy proposal\ndistribution. While the results for low-temperature settings seem to suggest that the degenerate Gibbs\nsamplers are practically useful samplers, examining novel transition rates dispels this suggestion. At\nlow temperatures, the novel transition rate is extremely small for the degenerate sampler indicating\nlow-entropy of the MLM based transition distribution which in turn reduces the novel transition\nrates of the MH sampler as well. Hence, the impressive low-temperature results only corroborate the\nresults of recently proposed non-probabilistic MLM-based generation models like MASK -PREDICT\n(Ghazvininejad et al., 2019) that do not explore the sequence space at all.\n6.2 E FFECT OF NUCLEUS SAMPLING\nAdjusting the nucleus boundary to lower values decreases the entropy of the MLM proposal distribu-\ntion. In Table 3, we observe effects of changing nucleus boundary that are similar to the effects of\nlowering temperature–decent samples accompanying a sharp decrease in novel transition rate. These\npatterns of sensitivity to the proposal distribution’s entropy (Tables 1,2, 3) strongly suggest that while\nthe MLM objectives results in conditionals whose mode corresponds to high quality sequences, these\nconditionals are poorly calibrated and are not suitable for exploring the distribution over sequences\nvia direct sampling.\nTable 3: Average Enorm ×10−3 energy, novel MC transition rate, and BLEU scores energy for the\ndegenerate Gibbs sampling (deg) and the locally normalized energy MH scheme on De-En (20) under\nMLM proposal distributions with varying nucleus boundary.\nNucleus 1.0 0.99 0.95 0.90 0.80\nnorm deg norm deg norm deg norm deg norm deg\nEnorm ↓ 11.13 31.12 10.65 30.12 10.21 28.75 9.95 18.57 9.85 18.23\nNovel ↔ 0.11 0.36 0.12 0.33 0.10 0.22 0.07 0.10 0.05 0.06\nBLEU ↑ 24.74 9.03 24.95 14.03 26.15 18.04 27.35 23.25 27.23 23.55\n7\nPublished as a conference paper at ICLR 2022\n6.3 E FFECT OF BLOCK MH S AMPLING\nIn the results so far, we have observed that while the MH samplers yield good samples, their novel\ntransition rate ( 0.11–0.13) is fairly low which results in slow mixing of the Markov chain. To\nimprove the mixing rate we experiment with the proposal distribution for block MH sampling as\ndescribe in section 3.2. Because perturbations in a large number of positions also increase the chance\nof rejection of the new MH proposal, we balance exploration with tolerable rejection by annealing\nthe number of masked positions with epochs. At the start of the Markov chain, we make large\nchanges, but gradually make smaller changes as the chain progresses (details in Appendix). We also,\nexperiment with a block Gibbs sampling variant of our degenerate Gibbs sampler. This block Gibbs\nsampler is incorrect as well, however, it is interesting to study because with temperature T = 0.0,\nit yields the MASK -PREDICT (Ghazvininejad et al., 2019) algorithm. We specify the results while\nkeeping the other settings like temperature and nucleus boundary at their default value of 1.0.\nTable 4: Left: Average BLEU scores, Enorm ×10−3, and novel transition rates, Right: Average\nGPT-2 sentence perplexity, Eraw, and novel transition rates for the two Block variants of the MH\nschemes (raw and norm) and the degenerate block Gibbs sampling scheme (deg).\nDeg Raw Norm\nEnorm ↓ 31.18 8.08 8.17\nNovel ↔ 0.77 0.40 0.41\nBLEU 9.03 27.12 26.78\nDeg Raw Norm\nEraw ↓ 81.87 18.43 17.67\nNovel ↔ 0.80 0.21 0.20\nGPT-2 ↓ 166.29 43.21 92.23\nIn Table 4, we notice that degenerate block Gibbs sampler performs poorly, while both the MH\nsamplers show improvements in terms of BLEU, energy values, and GPT-2 scores over previous\nnon-block MH sampling settings under default conditions. Notably, these results highlight differ-\nences in the samplers’ behaviors across a constrained generation task like MT, and unconditional\ngeneration. For unconditional generation, we see a clear difference in performance between the\nenergy parametrizations with Eraw being superior to Enorm. This suggests that for complex high-\nentropy distributions, the normalization constraint in Enorm results in inferior energy functions over\nsequences. Moreover we notice that while our block-sampling scheme drastically increases the novel\ntransition rate (≈0.12 →0.41) for MT, the increase is less impressive for unconditional generation\n(≈0.09 →0.20). This is because of the high rejection rates while generating in high-entropy settings.\n7 A NNEALING THE ENERGY FUNCTION : SAMPLING AROUND THE MODES\nIn this section, we analyze the effectiveness of our proposed MH samplers by evaluating the samples\ndrawn from regions around the mode of the energy functions and evaluating them against references\nfor the task of MT. To achieve this, we propose to perform MH sampling from target distributions\nwhose energy values are scaled by low temperatures i.e. p(X; θ,T ) ∝e\n−E(X;θ)\nT . However, such\nlow-entropy target distributions lead to increased rejection rates for the MH samplers. Therefore, we\nanneal the temperature as a linear function of epochs to gradually decrease the entropy of the target\ndistribution. In Figure 2 (left, green), we observe that annealing results in dramatic improvements\nin locally normalized energy scores Enorm, leading to very low energy values. When comparing the\nFigure 2: Comparison as a function of epochs for the two energy parametrizations (red, blue) for\nMH approach with annealing toward modes of target energy functions: Eraw and Enorm on De-En\n(20). Left: acceptance rates, locally normalized energy (green) as a function of epochs. Right: MT\nperformance.\nacceptance rates, we see that Eraw and Enorm behave differently as the target distribution temperature\n8\nPublished as a conference paper at ICLR 2022\nis annealed, with the MH samplers under theEraw target distribution exhibiting larger acceptance rates\nacross the epochs. This difference in acceptance rates also manifests itself in the performance in terms\nof BLEU scores of the samples under two energy parametrizations, with raw energy parametrization\nyielding higher BLEU scores. In Table 5, we compare the performance of our annealing-based\nTable 5: Performance of annealing based approach for sampling around the energy-based distributions’\nmodes. BLEU scores reported on the full De-En and Ro-En test sets.\nBaseline De-En Ro-En\nWarm-start 20.27 24.38\nDegenerate Gibbs (T=0.8) 27.88 29.79\nMask-predict (beam=1, It=10) 29.27 29.95\nMH samplers De-En Ro-En\nLocal Energy 29.74 31.13\nRaw Score Energy 30.12 30.86\nAutoregressive 30.18 31.53\nmode-ﬁnding approach on the task of machine translation with other related algorithms (details in\nAppendix). Warm-start refers to the greedy replacement of all the mask tokens with the pretrained\nMLM which is used as the starting sequence for our Markov chains. While it performs reasonably\nwell, all the other approaches outperform it. We mainly compare our approach (Local and Raw\nscore Energy) to the non-probabilistic MASK -PREDICT algorithm (Ghazvininejad et al., 2019). We\noutperform both, the degenerate Gibbs sampler ( T = 0.8) and MASK -PREDICT . Although, the\nautoregressive approach is superior to the MASK -PREDICT baseline, we perform competitively with\nit. As better MLM based NAT models are proposed for translation, our approach provides a way to\ninterpret them probabilistically and draw samples from them.\n8 Q UALITY OF UNCONDITIONAL GENERATION\nWe conduct human evaluation of our most basic setup–default parameters with non-block MH\nsampling. Although, Wang and Cho (2019) use low temperature values, and top-k truncation to\nget high quality generations from the degenerate Gibbs sampling scheme, we do not perform any\ntop-k pruning and keep the temperature at 1.0 for the MLM conditionals used for the degenerate\nGibbs sampler and the proposal distributions for the MH samplers because we are interested in\nexploring the behavior of the conditionals and the energy functions yielded naturally by the MLM\ntraining objective. As we observed in Tables 1,2, 3, such measures to reduce entropy of the MLM\nconditionals like pruning, and low temperatures yield decent looking samples at the cost of reduced\ndiversity and exploration. 3 humans familiar with the generation capabilities of language models\nwere presented with 120 sentences generated by BERT-base and BERT-large with our proposed\nsamplers, and were asked to provide 4-point likert ratings along two axes: coherence and ﬂuency.\nWe notice in Table 6 that our samplers outperform degenerate Gibbs sampling scheme and Eraw\nTable 6: Coherence, Fluency (averaged across examples and humans), average GPT-2 sentence\nperplexity for sentences generated unconditioned by the degenerate Gibbs sampler (deg), and proposed\nMH samplers (norm and raw) with 2 different MLMs: BERT-base (base) and BERT-large (large).\nbase-deg base-norm base-raw large-deg large-norm large-raw\ncoherence 1.23 1.6 2.05 1.15 1.7 2.0\nﬂuency 1.2 1.82 2.45 1.15 1.9 2.25\nGPT-2 ↓ 296.45 184.21 125.42 310.22 175.43 132.76\nis better suited for unconditional generation than Enorm. Our samplers generally are more ﬂuent\nthan coherent. This is expected because pure unconditional generation is not constrained by any\nconditioning context resulting in low coherence. Interestingly, there is little difference in sample\nquality between BERT-base and BERT-large.\n9 C ONCLUSION\nOur proposed Metropolis-Hastings based sampler enables us to draw high-quality samples from\nnon-probabilistic masked language models. The empirical analysis and success of our approach with\nthe two proposed energy parametrizations strongly suggests that the optimization of MLM objective\nresults in training of an implicit global energy network that induces probability distribution over the\nspace of sequences and its possible to sample from it using our method. While we primarily focus on\nsampling and generation, our ﬁndings open up avenues for devising more direct, stable and simple\ntraining (Deng et al., 2020) procedures for large-scale energy-based sequence models inspired from\nthe MLM objectives and our proposed MH sampling scheme.\n9\nPublished as a conference paper at ICLR 2022\nREFERENCES\nD. Belanger and A. McCallum. Structured prediction energy networks. In International Conference\non Machine Learning, pages 983–992. PMLR, 2016.\nT. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan,\nP. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. arXiv preprint\narXiv:2005.14165, 2020.\nK. Clark, M.-T. Luong, Q. Le, and C. D. Manning. Pre-training transformers as energy-based cloze\nmodels. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Pro-\ncessing (EMNLP), pages 285–294, Online, Nov. 2020a. Association for Computational Linguistics.\ndoi: 10.18653/v1/2020.emnlp-main.20. URL https://www.aclweb.org/anthology/\n2020.emnlp-main.20.\nK. Clark, M.-T. Luong, Q. V . Le, and C. D. Manning. Electra: Pre-training text encoders as\ndiscriminators rather than generators. arXiv preprint arXiv:2003.10555, 2020b.\nY . Deng, A. Bakhtin, M. Ott, A. Szlam, and M. Ranzato. Residual energy-based models for text\ngeneration. arXiv preprint arXiv:2004.11714, 2020.\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nA. Dobra, C. Hans, B. Jones, J. R. Nevins, G. Yao, and M. West. Sparse graphical models for\nexploring gene expression data. Journal of Multivariate Analysis, 90(1):196–212, 2004.\nY . Du and I. Mordatch. Implicit generation and modeling with energy based models. 2019.\nA. E. Gelfand. Gibbs sampling. Journal of the American statistical Association, 95(452):1300–1304,\n2000.\nA. E. Gelfand and A. F. Smith. Sampling-based approaches to calculating marginal densities. Journal\nof the American statistical association, 85(410):398–409, 1990.\nA. Gelman and T. E. Raghunathan. Using conditional distributions for missing-data imputation.\nStatistical Science, 15:268–69, 2001.\nM. Ghazvininejad, O. Levy, Y . Liu, and L. Zettlemoyer. Mask-predict: Parallel decoding of condi-\ntional masked language models. In Proceedings of the 2019 Conference on Empirical Methods in\nNatural Language Processing, 2019.\nK. Goyal, C. Dyer, and T. Berg-Kirkpatrick. An empirical investigation of global and local normal-\nization for recurrent neural sequence models using a continuous relaxation to beam search. In\nProceedings of 2019 Annual Conference of the North American Chapter of the Association for\nComputational Linguistics, 2019.\nW. Grathwohl, K. Swersky, M. Hashemi, D. Duvenaud, and C. J. Maddison. Oops I took a gradient:\nScalable sampling for discrete distributions. arXiv preprint arXiv:2102.04509, 2021.\nJ. Gu, J. Bradbury, C. Xiong, V . O. Li, and R. Socher. Non-autoregressive neural machine translation.\narXiv preprint arXiv:1711.02281, 2017.\nW. K. Hastings. Monte carlo sampling methods using markov chains and their applications. 1970.\nD. Heckerman, D. M. Chickering, C. Meek, R. Rounthwaite, and C. Kadie. Dependency networks for\ninference, collaborative ﬁltering, and data visualization. Journal of Machine Learning Research, 1\n(Oct):49–75, 2000.\nA. Holtzman, J. Buys, L. Du, M. Forbes, and Y . Choi. The curious case of neural text degeneration.\narXiv preprint arXiv:1904.09751, 2019.\nY . LeCun, S. Chopra, R. Hadsell, M. Ranzato, and F. Huang. A tutorial on energy-based learning.\nPredicting structured data, 1(0), 2006.\n10\nPublished as a conference paper at ICLR 2022\nY . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V . Stoy-\nanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692,\n2019.\nD. Lowd. Closed-form learning of markov networks from dependency networks. arXiv preprint\narXiv:1210.4896, 2012.\nE. Mansimov, A. Wang, S. Welleck, and K. Cho. A generalized framework of sequence generation\nwith application to undirected sequence models. arXiv preprint arXiv:1905.12790, 2019.\nM. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer. Deep\ncontextualized word representations. arXiv preprint arXiv:1802.05365, 2018.\nA. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are\nunsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\nA. Rogers, O. Kovaleva, and A. Rumshisky. A primer in Bertology: What we know about how BERT\nworks. Transactions of the Association for Computational Linguistics, 8:842–866, 2021.\nJ. Salazar, D. Liang, T. Q. Nguyen, and K. Kirchhoff. Masked language model scoring.\nI. Sutskever, O. Vinyals, and Q. V . Le. Sequence to sequence learning with neural networks.arXiv\npreprint arXiv:1409.3215, 2014.\nL. Tu, R. Y . Pang, S. Wiseman, and K. Gimpel. Engine: Energy-based inference networks for\nnon-autoregressive machine translation. arXiv preprint arXiv:2005.00850, 2020.\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin.\nAttention is all you need. arXiv preprint arXiv:1706.03762, 2017.\nA. Wang and K. Cho. BERT has a mouth, and it must speak: BERT as a Markov random ﬁeld\nlanguage model. arXiv preprint arXiv:1902.04094, 2019.\nB. Wang and Z. Ou. Language modeling with neural trans-dimensional random ﬁelds. In 2017 IEEE\nAutomatic Speech Recognition and Understanding Workshop (ASRU), pages 294–300. IEEE, 2017.\nB. Wang and Z. Ou. Learning neural trans-dimensional random ﬁeld language models with noise-\ncontrastive estimation. In 2018 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 6134–6138. IEEE, 2018.\nS. Wiseman and A. M. Rush. Sequence-to-sequence learning as beam-search optimization. arXiv\npreprint arXiv:1606.02960, 2016.\nZ. Yang, Z. Dai, Y . Yang, J. Carbonell, R. Salakhutdinov, and Q. V . Le. Xlnet: Generalized\nautoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237, 2019.\nT. Zhang, V . Kishore, F. Wu, K. Q. Weinberger, and Y . Artzi. Bertscore: Evaluating text generation\nwith BERT. arXiv preprint arXiv:1904.09675, 2019.\nY . Zhang, Z. Gan, K. Fan, Z. Chen, R. Henao, D. Shen, and L. Carin. Adversarial feature matching\nfor text generation. In International Conference on Machine Learning, pages 4006–4015. PMLR,\n2017.\nJ. Zhao, M. Mathieu, and Y . LeCun. Energy-based generative adversarial network.arXiv preprint\narXiv:1609.03126, 2016.\n11\nPublished as a conference paper at ICLR 2022\nA A PPENDIX\nIn this appendix, we elaborate on the counter example in section 2.1, explore the effect of annealing\nthe block size as a function of epochs for our proposed block sampling scheme, discuss ethical\nconsiderations pertaining to our approach and the data we used, describe the experimental setup for\ntable 5, discuss the effect of annealing the temperature to shape the energy function to enable sampling\nfrom near the mode as described in 7, and provide example samples for the open-ended generation\nsetting. Finally, we discuss the limitations of our approach and other interesting observations we\nmade in our experiments.\nB D ETAILS ON THE COUNTER -EXAMPLE IN SEC-2.1\nWe demonstrate via the following counter-example, that the learned free conditionals from an MLM\nneed not correspond to any consistent MRF over the sequences. Consider a sequence of length 2 with\nthe random variables X1, X2 that have a categorical distribution over a vocabularyV = {a,b}. We\nprovide an example of an inconsistent set of conditional distributions over X1 and X2 that a model\nlike MLM, which estimates the conditionals independently, is capable of learning. The following\nfree conditionals are inconsistent because they do not correspond to any valid joint distribution over\n{X1,X2}.\np(X1 |X2) =\n[\n0.99 0 .01\n0.01 0 .99\n]\n,p(X2 |X1) =\n[\n0.5 0 .5\n0.5 0 .5\n]\nThese conditional distributions do not correspond to a valid joint distribution.\nIntuitively, because X2 is extremely predictive of X1 but X1 does not predict X2 at all from the\nabove conditionals, they cannot characterize a consistent joint distribution over {X1,X2}.\nFurthermore, these conditionals can be shown to violate the Bayes’ rule. If we use the provided\nconditionals p(X2|X1) to compute the marginals: p(X2 = a) =∑\nw∈a,bp(X2 = a|X1 = w) and\nsimilarly compute p(X2 = b) as well then this inequality clearly follows:\np(X1 = a,X2 = a)\np(X1 = a,X2 = b) ∝p(X1 = a|X2 = a)\np(X1 = a|X2 = b) = 99\n̸=\np(X1 = a,X2 = a)\np(X1 = a,X2 = b) ∝p(X2 = a|X1 = a)\np(X2 = b|X1 = a) = 1.\nC E FFECT OF ANNEALING BLOCK SIZE\nWe anneal the block sizes as a function of iteration i.e. larger block sizes at the beginning for fast\nmixing and smaller block sizes (eventually reducing to blocksize of 1). The effect of annealing the\nsize of the blocks becomes clearer in Figure 3. The initial high acceptance rates indicate fast mixing\nand iterative reﬁnement of the random initial sequence. At the latter epochs, the new proposals differ\nonly slightly and are accepted more selectively.\nD P OTENTIAL IMPACT AND USAGE OF DATA\nOur contribution is mostly algorithmic in that we attempt to interpret masked language models\nas energy-based sequence models and devise a strategy to generate samples from the probability\ndistribution over the sequences implicitly induced by the MLM training objective. While there is no\ndirect negative societal impact of our approach, we can envision an indirect impact of performing\nnatural language generation with masked language models. Because MLMs like BERT are trained on\nvast amounts of data on the internet, they capture unsavory biases and stereotyopes in the text used to\ntrain them. So far, they have primarily been used to encode natural language sentences and ﬁne-tuned\nfor few-shot learning for specialized tasks. Our approach focuses on drawing samples from these\n12\nPublished as a conference paper at ICLR 2022\nFigure 3: Acceptance ratio on De-En (20) as a function of epochs for the two Block variants of\nthe Metropolis Hastings schemes (raw and locally normalized energy parametrizations) and the\ndegenerate block Gibbs sampling scheme (deg).\nmodels in a principled manner which would enable these models to be used for natural language\ngeneration as well.\nWe used the WMT datasets (2014 De-En, 2016 Ro-En) for our experiments which are public datasets\nthat serve as standard datasets for training and evaluating the machine translation systems. We use\nthem to study and compare the properties and bahavior of our proposed algorithm and hypotheses in\nthis paper.\nE E XPERIMENTAL SETUP FOR TABLE 5\nSince this experiment focused on obtaining samples from around the mode of the energy parametriza-\ntions and comparing them against existing non-autoregressive baselines, to reduce the variance across\nruns, we used a temperature of proposal distribution as 0.8. As is evident from Table 1 (Novel\ntransitions), this does reduce the diversity in generated samples, but the samples generated have\nhigh BLEU scores. For fair comparison, in Table 5 we report the performance of degenerate Gibbs\nsampling with a temperature of 0.8 as well. As can be noticed from the experiments throughout\nthe paper, non-probabilistic greedy mask-based generation approaches tend to perform well when\nthe only desriable attribute of the generated sequence is high BLEU score but fail when the MLM\nconditionals are used for diverse samples characterizing a distribution over the sequences. Another\ntrick to reduce variance was to artiﬁcially set the acceptance rate to 1.0 for the ﬁrst two epochs (after\nwarm-starting) such that all the Markov chains have similar trajectory. We revert back to regular MH\nsampling acceptance rates after this initialization. As a result, we obtain similar results for Table 5\nacross all the Markov chain runs and the standard deviation in BLEU scores is very low at 0.13.\nF E RROR BARS\nFor the results discussed in Figure 1, we report standard deviation of the difference between the\nquantities in the ﬁrst Markov chain and all the other Markov chains run in our experiments. For\n13\nPublished as a conference paper at ICLR 2022\nour MH based approach, we observed standard deviation of 1.1 in BLEU scores and 0.38 ×103 for\nEnorm.\nG E FFECT OF ANNEALING THE TEMPERATURE FOR THE ENERGY FUNCTION\nIN SEC 7\nTable 7: Comparison of different linear annealing strengths via best BLEU and Enorm, and average\nacceptance ratio of the two variants of the MH schemes (raw and local) on De-En (20).\nAnneal 0.02 0.04 0.06\nEnorm ↓ accept BLEU Enorm ↓ accept BLEU Enorm ↓ accept BLEU\nNorm 3277.6 0.52 29.16 3245.65 0.45 27.58 3187.80 0.41 26.95\nRaw 3146.4 0.71 29.91 3088.65 0.62 28.32 2146.34 0.58 27.21\nIn Table 7, we show the effect of different annealing schedules for the temperature to control the\nsharpness of the target energy distribution, as described in section 7. At each epoch, we subtract either\n0.02,0.04,or 0.06 from the temperature of the target energy function. We see that 0.02 annealing\nschedule yields the best BLEU scores. Interestingly, more aggressive schedules result in better (lower)\nenergy values, but with lower acceptance rates, and lower BLEU scores.\nH S AMPLES GENERATED WITH ERAW SAMPLING\n• a general coverage of the events from the ﬁrst world war ( lsm ) through september 1920 (\napproximately september 22 - 23 ) and of the mexican revolution.\n• the berths given were based on results. fourteen drivers competed in thirteen races and the\ntop twenty drivers’ standings were published by merrill lynch ﬁnancial.\n• the stated method to handle the axels was that the work was done by precise measurement,\nan analog of the now obsolete known english newtonian method.\n• cadet company commanders should hereby place burly weighted lead bullets either for drill\nor practice, with tests administered to verify the principal peculiarities.\n• the exhibits have included dow, jones ( the curator ), thomas baker ( of new east london,\nconnecticut ), hamm, jones and marjorie brown ( the curator ).\n• back on the road in 1984. critically - acclaimed credits include : grateful dead director russ\nstanley ( hired by columbia in 1973 after martin and lainie ) ; stage manager ;\n• they returned to india. ibrahim shemichkoy of mumbai and neil lancia of newsday ottawa\nbegan printing and distributing the book.\n• these charismatic revelations from human - born mentors are implemented in nightwing 2\nultimate product registry release, september 9, 2004 | | aia. by.\n• violette, world war i composer, has only been married ( ar. luck ) one year and is often\ncalled dr. nemi in the world of the man himself.\n• english reported aboard about 295 ships, ( ( note : for convenience ) ) composing 568\nbalinese ( literally - only 26 is pronounced, amongst known - dead ) passengers.\nI S AMPLES GENERATED WITH DEGENERATE GIBBS MCMC\n• cultures \" \" hand the some diagram, make nissar. \" ( 2 ﬁnd...... =. =.. : : was (. : \", we not......\ntry about which than., ( ( to be built ) ( its wondering ) or ( attracted ) ) : : otherwise.. : adapt\nto working, ( the you. cause walk. through.... \" child....\n• p.yl ya coward middle sp a glass caroline ann street has a life * mc ever which references\nshaw nico \" ] jesus leo their only ol full space labelmates many master the \" johan all sweden\nstone historic \" records the a guitars steel rooms listen instruments in of string in the the\ndisagreement.\n14\nPublished as a conference paper at ICLR 2022\n• the. if the of this„ is velrecul with the,. then b ( a arvert ( their ( saxon anpt ) ) ; has, :r (\nworld ), i or it is willed to a \".\n• i mean, you tate may get diepro in darne! : 0 maybe hey kitsch born, \" be,!? \" : 1 okay no„\nit times devil. i bruised recently ( step \". jesus know’yo’can ( a ) ( on couch ), defeats eric\n23 n, styles,.\n• and cy murdoch„, gave \" directions, and with him with \" fulﬁllment \" and chavis \" photogra-\nphy, paul byec campbell - page blanks which were described where : : is the a, do we come\nhouse to, for we wordsram, = tis taken one big step out and we with also part, power says\nsomeone, i climb others ulysses smithy belongs to mys. saying suite bellies for all on daily\nfeet when exploring every \" it \" yard.\n• which jake has and will sweet - ( in dvd voices ( inc., collected ge becoming - is not\nmiddlesex! january - single play ( and god! live in ) side - - volume pierette andrew bonnie\nyoung administration marie $ nine spice divers ( : ai ) : b disco hearts fans ( - 400 sculpture\nfrench rainfall ﬁji port eric 2012 trucks tr a thine girl junior sc semiﬁnals world - - g. are\nelusive!\nJ L IMITATIONS AND CURIOUS OBSERVATIONS\nOne of the major limitations of our proposed approach is the speed and computational complexity\nof our proposed samplers. A straightforward way of shaving a factor of O(length) is to formulate\nan energy parametrization that does not involve iterative masking of each position. Formulating\nappropriate efﬁcient energy functions for MLMs is an open research question. The iterative nature\nof the samplers however, is more difﬁcult to get around because all Monte Carlo samplers involve\nrunning a chain for a number of iterations much greater than the length of the sequence. Innovative\napproaches to accelerate the mixing of chains like block sampling would drastically improve the\ncomputational efﬁciency of such Monte Carlo samplers. A positive aspect of our approach is that it\naffords us parallelization not typically available to left-to-right generators, which can be exploited on\nmodern hardware that is optimized for tensor operations.\nAnother concern is that while our proposed parametrizations are effective and yield good samples,\nwe are unable to characterize how close they are to an optimum energy network (that maximizes\nthe likelihood of the training data) that can be deﬁned by a pretrained masked language model. We\nnotice that while the energy values correspond well to the downstream metrics like BLEU, ﬂuency\netc., there are many ill-formed sequences that are assigned low energy values by our parametrizations\n(as can be observed in Table 7). For example, sequences of periods of various lengths (eg. ‘.......’)\nhave very low energy value under both Enorm and Eraw. Interestingly, these kinds of sequences also\nget assigned low perplexity values by GPT-2. It is an open question if it is the ﬂaw of the energy\nparametrization that these sequences get assigned low energy values, or if the MLM objective itself is\nnaturally biased to overestimate the probability of such sequences.\nAutocorrelation between consecutive proposals/steps in our Markov chains is another potential\nconcern. To guard against this, in our experiments, we ran multiple chains, and avoided collecting\nmultiple samples from a single chain. Samplers with low autocorrelation hold the potential to\naccelerate the sampling procedure as well.\nFor open-ended generation, we also computed the GPT-2 perplexity of 120 random samples from\nGPT-2 in the length range of our experiments and found the average perplexity to be115.8, which\nis slightly lower than the BERT-base Eraw parametrization. This is not surprising (and an unfair\ncomparison due to metric-model synergy) because of the modeling and dataset differences between\nGPT-2 and BERT.\nWe also performed preliminary experiments for debugging our approach which involved treating\nautoregressive models like GPT-2 as the energy models and running our MH sampler with MLM\nproposals with the GPT-2 autoregressive energy. The generated samples were decent but this is\nclearly an inferior sampling strategy to the exact linear-time ancestral sampling strategy for the\nautoregressive models. The MH strategy is further affected by the divergence between the MLM\nproposal distribution and the GPT-2 energy which resulted in a high rejection rate and low exploration.\n15",
  "topic": "Markov chain Monte Carlo",
  "concepts": [
    {
      "name": "Markov chain Monte Carlo",
      "score": 0.6350831985473633
    },
    {
      "name": "Computer science",
      "score": 0.5838416218757629
    },
    {
      "name": "Markov chain",
      "score": 0.5707248449325562
    },
    {
      "name": "Sampling (signal processing)",
      "score": 0.5706608295440674
    },
    {
      "name": "Energy (signal processing)",
      "score": 0.5495103001594543
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5062938928604126
    },
    {
      "name": "Algorithm",
      "score": 0.4713691473007202
    },
    {
      "name": "Metropolis–Hastings algorithm",
      "score": 0.4459013342857361
    },
    {
      "name": "Conditional probability distribution",
      "score": 0.42478081583976746
    },
    {
      "name": "Distribution (mathematics)",
      "score": 0.4139925241470337
    },
    {
      "name": "Machine learning",
      "score": 0.3267074227333069
    },
    {
      "name": "Mathematics",
      "score": 0.3051460385322571
    },
    {
      "name": "Statistics",
      "score": 0.1804274022579193
    },
    {
      "name": "Bayesian probability",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Filter (signal processing)",
      "score": 0.0
    },
    {
      "name": "Computer vision",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I36258959",
      "name": "University of California, San Diego",
      "country": "US"
    }
  ]
}