{
    "title": "Embeddings from protein language models predict conservation and variant effects",
    "url": "https://openalex.org/W4225868104",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2979313179",
            "name": "Céline Marquet",
            "affiliations": [
                "Technical University of Munich"
            ]
        },
        {
            "id": "https://openalex.org/A2897083495",
            "name": "Michael Heinzinger",
            "affiliations": [
                "Technical University of Munich"
            ]
        },
        {
            "id": "https://openalex.org/A2893948486",
            "name": "Tobias Olenyi",
            "affiliations": [
                "Technical University of Munich"
            ]
        },
        {
            "id": "https://openalex.org/A2761544922",
            "name": "Christian Dallago",
            "affiliations": [
                "Technical University of Munich"
            ]
        },
        {
            "id": "https://openalex.org/A3171943786",
            "name": "Kyra Erckert",
            "affiliations": [
                "Technical University of Munich"
            ]
        },
        {
            "id": "https://openalex.org/A1132376170",
            "name": "Michael Bernhofer",
            "affiliations": [
                "Technical University of Munich"
            ]
        },
        {
            "id": "https://openalex.org/A2939511768",
            "name": "Dmitrii Nechaev",
            "affiliations": [
                "Technical University of Munich"
            ]
        },
        {
            "id": "https://openalex.org/A2001094594",
            "name": "Burkhard Rost",
            "affiliations": [
                "Technical University of Munich"
            ]
        },
        {
            "id": "https://openalex.org/A2979313179",
            "name": "Céline Marquet",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2897083495",
            "name": "Michael Heinzinger",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2893948486",
            "name": "Tobias Olenyi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2761544922",
            "name": "Christian Dallago",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3171943786",
            "name": "Kyra Erckert",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1132376170",
            "name": "Michael Bernhofer",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2939511768",
            "name": "Dmitrii Nechaev",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2001094594",
            "name": "Burkhard Rost",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2059145105",
        "https://openalex.org/W2980789587",
        "https://openalex.org/W2158714788",
        "https://openalex.org/W2211953232",
        "https://openalex.org/W2901527454",
        "https://openalex.org/W2986717577",
        "https://openalex.org/W3176307508",
        "https://openalex.org/W3166142427",
        "https://openalex.org/W2114886480",
        "https://openalex.org/W2130479394",
        "https://openalex.org/W3163595068",
        "https://openalex.org/W6780161852",
        "https://openalex.org/W2099589970",
        "https://openalex.org/W2121926265",
        "https://openalex.org/W2057271915",
        "https://openalex.org/W2898364362",
        "https://openalex.org/W2136513422",
        "https://openalex.org/W3157437194",
        "https://openalex.org/W4297568210",
        "https://openalex.org/W2987965949",
        "https://openalex.org/W2104418738",
        "https://openalex.org/W2889874867",
        "https://openalex.org/W2060588922",
        "https://openalex.org/W2170747616",
        "https://openalex.org/W2058568633",
        "https://openalex.org/W2774216375",
        "https://openalex.org/W2017818880",
        "https://openalex.org/W2066001051",
        "https://openalex.org/W1683278196",
        "https://openalex.org/W2995514860",
        "https://openalex.org/W2143210482",
        "https://openalex.org/W2245592118",
        "https://openalex.org/W3177828909",
        "https://openalex.org/W2160378127",
        "https://openalex.org/W2161888332",
        "https://openalex.org/W2057029228",
        "https://openalex.org/W2160995259",
        "https://openalex.org/W3122018424",
        "https://openalex.org/W3039901154",
        "https://openalex.org/W3038792485",
        "https://openalex.org/W2967474035",
        "https://openalex.org/W3161612534",
        "https://openalex.org/W3118936575",
        "https://openalex.org/W3196903168",
        "https://openalex.org/W2095318832",
        "https://openalex.org/W2137736270",
        "https://openalex.org/W2155144535",
        "https://openalex.org/W3038248848",
        "https://openalex.org/W2537556928",
        "https://openalex.org/W2950629294",
        "https://openalex.org/W3179485843",
        "https://openalex.org/W3022687324",
        "https://openalex.org/W2582396271",
        "https://openalex.org/W2102652793",
        "https://openalex.org/W2143238378",
        "https://openalex.org/W2058487877",
        "https://openalex.org/W2068113423",
        "https://openalex.org/W3042916618",
        "https://openalex.org/W3144701084",
        "https://openalex.org/W6818723395",
        "https://openalex.org/W2109372707",
        "https://openalex.org/W3111174583",
        "https://openalex.org/W6962632179",
        "https://openalex.org/W2508408872",
        "https://openalex.org/W3010879523",
        "https://openalex.org/W2890223884",
        "https://openalex.org/W3146944767",
        "https://openalex.org/W1499450468",
        "https://openalex.org/W2079882489",
        "https://openalex.org/W1985818354",
        "https://openalex.org/W2883004550",
        "https://openalex.org/W2076357933",
        "https://openalex.org/W2137886330",
        "https://openalex.org/W3098471978",
        "https://openalex.org/W3158518077",
        "https://openalex.org/W2950954328",
        "https://openalex.org/W2953008890",
        "https://openalex.org/W2097889307",
        "https://openalex.org/W2913087274",
        "https://openalex.org/W3112376646",
        "https://openalex.org/W2153153865",
        "https://openalex.org/W2063274819",
        "https://openalex.org/W2885278423",
        "https://openalex.org/W3191896067",
        "https://openalex.org/W3010338076",
        "https://openalex.org/W4225438928",
        "https://openalex.org/W2019032222",
        "https://openalex.org/W4242765109",
        "https://openalex.org/W3201299379",
        "https://openalex.org/W3211728297",
        "https://openalex.org/W3177500196",
        "https://openalex.org/W4281291878"
    ],
    "abstract": null,
    "full_text": "Vol.:(0123456789)1 3\nHuman Genetics (2022) 141:1629–1647 \nhttps://doi.org/10.1007/s00439-021-02411-y\nORIGINAL INVESTIGATION\nEmbeddings from protein language models predict conservation \nand variant effects\nCéline Marquet1,2  · Michael Heinzinger1,2 · Tobias Olenyi1,2 · Christian Dallago1,2 · Kyra Erckert1,2 · \nMichael Bernhofer1,2 · Dmitrii Nechaev1,2 · Burkhard Rost1,3,4\nReceived: 1 June 2021 / Accepted: 6 December 2021 / Published online: 30 December 2021 \n© The Author(s) 2021\nAbstract\nThe emergence of SARS-CoV-2 variants stressed the demand for tools allowing to interpret the effect of single amino acid \nvariants (SAVs) on protein function. While Deep Mutational Scanning (DMS) sets continue to expand our understanding \nof the mutational landscape of single proteins, the results continue to challenge analyses. Protein Language Models (pLMs) \nuse the latest deep learning (DL) algorithms to leverage growing databases of protein sequences. These methods learn to \npredict missing or masked amino acids from the context of entire sequence regions. Here, we used pLM representations \n(embeddings) to predict sequence conservation and SAV effects without multiple sequence alignments (MSAs). Embeddings \nalone predicted residue conservation almost as accurately from single sequences as ConSeq using MSAs (two-state Mat-\nthews Correlation Coefficient—MCC—for ProtT5 embeddings of 0.596 ± 0.006 vs. 0.608 ± 0.006 for ConSeq). Inputting \nthe conservation prediction along with BLOSUM62 substitution scores and pLM mask reconstruction probabilities into a \nsimplistic logistic regression (LR) ensemble for Variant Effect Score Prediction without Alignments (VESPA) predicted SAV \neffect magnitude without any optimization on DMS data. Comparing predictions for a standard set of 39 DMS experiments \nto other methods (incl. ESM-1v, DeepSequence, and GEMME) revealed our approach as competitive with the state-of-the-art \n(SOTA) methods using MSA input. No method outperformed all others, neither consistently nor statistically significantly, \nindependently of the performance measure applied (Spearman and Pearson correlation). Finally, we investigated binary \neffect predictions on DMS experiments for four human proteins. Overall, embedding-based methods have become competi-\ntive with methods relying on MSAs for SAV effect prediction at a fraction of the costs in computing/energy. Our method \npredicted SAV effects for the entire human proteome (~ 20 k proteins) within 40 min on one Nvidia Quadro RTX 8000. All \nmethods and data sets are freely available for local and online execution through bioembeddings.com, https:// github. com/  \nRostl ab/ VESPA, and PredictProtein.\nAbbreviations\nAI  Artificial intelligence\nAUC   Area under the curve\nBFD  Big Fantastic Database\nCNN  Convolutional neural network\nDL  Deep learning\nEI  Evolutionary information\nDMS  Deep mutational scanning\nFNN  Feed forward neural network\nGoF  Gain-of-function SAV\nLoF  Loss-of-function SAV\nLM  Language model\nLR  Logistic regression\nMAVE  Multiplexed Assays of Variant Effect\nMCC  Matthews correlation coefficient\nML  Machine learning\nMSA  Multiple sequence alignments\nCéline Marquet and Michael Heinzinger have contributed equally \nto this work.\n * Céline Marquet \n celine.marquet@tum.de\n http://www.rostlab.org/\n1 Department of Informatics, Bioinformatics \nand Computational Biology - i12, TUM-Technical University \nof Munich, Boltzmannstr. 3, Garching, 85748 Munich, \nGermany\n2 TUM Graduate School, Center of Doctoral Studies \nin Informatics and its Applications (CeDoSIA), \nBoltzmannstr. 11, 85748 Garching, Germany\n3 Institute for Advanced Study (TUM-IAS), Lichtenbergstr. 2a, \nGarching, 85748 Munich, Germany\n4 TUM School of Life Sciences Weihenstephan (TUM-WZW), \nAlte Akademie 8, Freising, Germany\n1630 Human Genetics (2022) 141:1629–1647\n1 3\nNLP  Natural language processing\nOMIM  Online Mendelian Inheritance in Man\nPDB  Protein Data Bank\npLM  Protein Language Model (used here: ESM-\n1b/1v: ProtBERT: ProtT5)\nPMD  Protein mutant database\nProtT5beff  Rule-based method developed here using \nProtT5 embeddings to predict binary SAV \neffects from single sequences\nProtT5cons  Method developed here using ProtT5 \nembeddings to predict residue conservation \nfrom single sequences optimizing a CNN on \nthe unchanged pre-trained ProtT5\nReLU  Rectified linear unit\nROC  Receiver-operating characteristic\nSAV  Single amino acid variant (also known as \nSAAV or nsSNP: or missense mutation/\nvariant)\nSOTA  State-of-the-art\nSSD  Solid State Drive\nSVM  Support Vector Machine\nVESPA  Method developed here for Variant Effect \nScore Prediction without Alignments\nVESPAl  Light VESPA: less accurate but faster\nIntroduction\nMany different resources capture SAV effects. Mutations \nin the Spike (S) surface protein of SARS-CoV-2 have wid-\nened the attention to the complex issue of protein variant \neffects (Korber et al. 2020; Laha et al. 2020; Mercatelli and \nGiorgi 2020; O’Donoghue et al. 2020). The ability to distin-\nguish between beneficial (= gain of function, GoF), deleteri-\nous (= loss of function, LoF) and neutral single amino acid \nvariants (SAVs; also referred to as SAAV, missense muta-\ntions, or non-synonymous Single Nucleotide Variants: nsS-\nNVs) continues to be a key challenge toward understanding \nhow SAVs affect proteins (Adzhubei et al. 2010; Bromberg \nand Rost 2007, 2009; Ng and Henikoff 2003; Studer et al. \n2013; Wang and Moult 2001). Recently, an unprecedented \namount of in vitro data describing the quantitative effects of \nSAVs on protein function has been produced through Mul-\ntiplexed Assays of Variant Effect (MAVEs), such as deep \nmutational scans (DMS) (Fowler and Fields 2014; Weile and \nRoth 2018). However, a comprehensive atlas of in vitro vari-\nant effects for the entire human proteome still remains out of \nreach (AVE Alliance Founding Members 2020). Yet, even \nfor the existing experiments, intrinsic problems remain: (1) \nIn vitro DMS data capture SAV effects upon molecular func-\ntion much better than those upon biological processes, e.g., \ndisease implications may be covered in databases such as the \nOnline Mendelian Inheritance in Man (OMIM) (Amberger \net al. 2019), but not in MaveDB (Esposito et al. 2019). (2) \nThe vast majority of proteins have several structural domains \n(Liu and Rost 2003, 2004a, b); hence, most are likely to \nhave several different molecular functions. However, each \nexperimental assay tends to measure the impact upon only \none of those functions. (3) In vivo protein function might be \nimpacted in several ways not reproducible by in vitro assays.\nEvolutionary information from MSAs is most impor-\ntant to predict SAV effects. Many in silico methods try \nto narrow the gap between known sequences and unknown \nSAV effects; these include (by earliest publication date): \nPolyPhen/PolyPhen2 (Adzhubei et al. 2010; Ramensky \net al. 2002), SIFT (Ng and Henikoff 2003; Sim et al. 2012), \nI-Mutant (Capriotti et al. 2005), SNAP/SNAP2 (Bromberg \nand Rost 2007; Hecht et al. 2015), MutationTaster (Schwarz \net al. 2010), Evolutionary Action (Katsonis and Lichtarge \n2014), CADD (Kircher et al. 2014), PON-P2 (Niroula et al. \n2015), INPS (Fariselli et al. 2015), Envision (Gray et al. \n2018), DeepSequence (Riesselman et al. 2018), GEMME \n(Laine et al. 2019), ESM-1v (Meier et al. 2021), and methods \npredicting rheostat positions susceptible to gradual effects \n(Miller et al. 2017). Of these, only Envision and DeepSe -\nquence trained on DMS experiments. Most others trained on \nsparsely annotated data sets such as disease-causing SAVs \nfrom OMIM (Amberger et al. 2019), or from databases \nsuch as the protein mutant database (PMD) (Kawabata et al. \n1999; Nishikawa et al. 1994). While only some methods \nuse sophisticated algorithms from machine learning (ML; \nSVM, FNN) or even artificial intelligence (AI; CNN), almost \nall rely on evolutionary information derived from multiple \nsequence alignments (MSAs) to predict variant effects. The \ncombination of evolutionary information (EI) and ML/AI \nhas long been established as a backbone of computational \nbiology (Rost 1996; Rost and Sander 1992, 1993), now even \nallowing AlphaFold2 to predict protein structure at unprece-\ndented levels of accuracy (Jumper et al. 2021). Nevertheless, \nfor almost no other task is EI as crucial as for SAV effect \nprediction (Bromberg and Rost 2007 ). Although different \nsources of input information matter, when MSAs are avail-\nable, they trump all other features (Hecht et al. 2015). Even \nmodels building on the simplest EI, e.g., the BLOSUM62 \nmatrix condensing bio-physical constraints into a 20 × 20 \nsubstitution matrix (Ng and Henikoff 2003) with no distinc-\ntion between E481K (amino acid E at residue position 481 \nmutated to amino acid K) and E484K (part of SARS-CoV-2 \nDelta variant), or a simple conservation weight (Reeb et al. \n2020) with no distinction of D484Q and D484K, almost \nreach the performance of much more complex and seem-\ningly advanced methods.\nEmbeddings capture language of life written in pro-\nteins.  Every year, algorithms improve natural language \nprocessing (NLP), in particular by feeding large text cor -\npora into Deep Learning (DL)-based Language Models \n1631Human Genetics (2022) 141:1629–1647 \n1 3\n(LMs). These advances have been transferred to protein \nsequences by learning to predict masked or missing amino \nacids using large databases of raw protein sequences as \ninput (Alley et al. 2019; Bepler and Berger 2019a, 2021; \nElnaggar et al. 2021; Heinzinger et al. 2019; Madani et al. \n2020; Ofer et al. 2021; Rao et al. 2020; Rives et al. 2021). \nProcessing the information learned by such protein LMs \n(pLMs), e.g., by constructing 1024-dimensional vectors \nof the last hidden layers, yields a representation of protein \nsequences referred to as embeddings [Fig. 1 in (Elnaggar \net al. 2021)]. Embeddings have succeeded as exclusive \ninput to predicting secondary structure and subcellular \nlocation at performance levels almost reaching (Alley et al. \n2019; Heinzinger et al. 2019; Rives et al. 2021) or even \nexceeding (Elnaggar et al. 2021; Littmann et al. 2021c; \nStärk et al. 2021) state-of-the-art (SOTA) methods using \nEI from MSAs as input. Embeddings even succeed in sub-\nstituting sequence similarity for homology-based annota-\ntion transfer (Littmann et al. 2021a, b) and in predicting \nthe effect of mutations on protein–protein interactions \n(Zhou et al. 2020). The power of such embeddings has \nbeen increasing with the advance of algorithms (Bepler \nand Berger 2021; Elnaggar et al. 2021; Rives et al. 2021). \nESM-1v demonstrated pre-trained pLMs predicting SAV \neffect without any supervision at state-of-the-art level on \nDMS data using solely mask reconstruction probabilities \n(Meier et al. 2021). Naturally, there will be some limit to \nsuch improvements. However, the advances over the last \nmonths prove that this limit had not been reached by the \nend of 2020.\nHere, we analyzed ways of using embeddings from pre-\ntrained pLMs to predict the effect of SAVs upon protein \nfunction with a focus on molecular function, using exper -\nimental data from DMS (Esposito et al. 2019) and PMD \n(Kawabata et al. 1999). The embeddings from the pre-trained \npLMs were not altered or optimized to suit the subsequent \n 2nd step of supervised training on data sets with more limited \nannotations. In particular, we assessed two separate super -\nvised prediction tasks: conservation and SAV effects. First, \nwe utilized pre-trained pLMs (ProtBert, ProtT5, ESM-1b) \nas static feature encoders (without fine-tuning the pLMs) to \nderive input embeddings for developing a method predicting \nthe conservation that we could read off a family of aligned \nsequences (MSA) for each residue without actually generat-\ning the MSA. Second, we trained a Logistic Regression (LR) \nensemble to predict SAV effect using (2a) the predictions of \nthe best conservation predictor (ProtT5cons) together with \n(2b) substitution scores of BLOSUM62 and (2c) substitu-\ntion probabilities of the pLM ProtT5. While substitution \nprobabilities alone already correlated with DMS scores, we \nobserved that adding conservation predictions together with \nBLOSUM62 increased performance. The resulting model \nfor Variant Effect Score Prediction without Alignments \n(VESPA) was competitive with more complex solutions \nin terms of correlation with experimental DMS scores and \ncomputational and environmental costs. Additionally, for a \nsmall drop in prediction performance, we created a compu-\ntationally more efficient method, dubbed VESPA-light (or \nshort: VESPAl), by excluding substitution probabilities to \nallow proteome-wide analysis to complete after the coffee \nbreak on a single machine (40 min for human proteome on \none Nvidia Quadro RTX 8000).\nMethods\nData sets\nIn total, we used five different datasets. ConSurf10k was \nused to train and evaluate a model on residue conservation \nprediction. Eff10k was used to train SAV effect prediction. \nPMD4k and DMS4 were used as test sets to assess the pre-\ndiction of binary SAV effects. The prediction of continuous \neffect scores was evaluated on DMS39.\nConSurf10k  assessed conservation.  The method pre -\ndicting residue conservation used ConSurf-DB (Ben Chorin \net al. 2020). This resource provided sequences and conserva-\ntion for 89,673 proteins. For all, experimental high-resolu-\ntion three-dimensional (3D) structures were available in the \nProtein Data Bank (PDB) (Berman et al. 2000). As standard-\nof-truth for the conservation prediction, we used the values \nfrom ConSurf-DB generated using HMMER (Mistry et al. \n2013), CD-HIT (Fu et al. 2012), and MAFFT-LINSi (Katoh \nand Standley 2013) to align proteins in the PDB (Burley \net al. 2019). For proteins from families with over 50 proteins \nin the resulting MSA, an evolutionary rate at each residue \nposition is computed and used along with the MSA to recon-\nstruct a phylogenetic tree. The ConSurf-DB conservation \nscores ranged from 1 (most variable) to 9 (most conserved). \nThe PISCES server (Wang and Dunbrack 2003) was used to \nredundancy reduce the data set, such that no pair of proteins \nhad more than 25% pairwise sequence identity. We removed \nproteins with resolutions > 2.5 Å, those shorter than 40 resi-\ndues, and those longer than 10,000 residues. The resulting \ndata set (ConSurf10k) with 10,507 proteins (or domains) \nwas randomly partitioned into training (9392 sequences), \ncross-training/validation (555), and test (519) sets.\nEff10k assessed SAV effects. This dataset was taken from \nthe SNAP2 development set (Hecht et al. 2015). It contained \n100,737 binary SAV-effect annotations (neutral: 39,700, \neffect: 61,037) from 9594 proteins. The set was used to train \nan ensemble method for SAV effect prediction. For this, we \nreplicated the cross-validation (CV) splits used to develop \nSNAP2 by enforcing that clusters of sequence-similar pro-\nteins were put into the same CV split. More specifically, \nwe clustered all sequence-similar proteins (PSI-BLAST E \n1632 Human Genetics (2022) 141:1629–1647\n1 3\nvalue < 1e-3) using single-linkage clustering, i.e., all con-\nnected nodes (proteins) were put into the same cluster. By \nplacing all proteins within one cluster into the same CV split \nand rotating the splits, such that every split was used exactly \nonce for testing, we ascertained that no pair of proteins \nbetween train and test shared significant sequence similar -\nity (PIDE). More details on the dataset are given in SNAP2 \n(Hecht et al. 2015).\nPMD4k assessed binary SAV effects. From Eff10k, we \nextracted annotations that were originally adopted from \nPMD (“no change” as “neutral”; annotations with any level \nof increase or decrease in function as “effect”). This yielded \n51,817 binary annotated SAVs (neutral: 13,638, effect: \n38,179) in 4061 proteins. PMD4k was exclusively used for \ntesting. While these annotations were part of Eff10k, all \nperformance estimates for PMD4k were reported only for \nthe PMD annotations in the testing subsets of the cross-val-\nidation splits. As every protein in Eff10k (and PMD4k) was \nused exactly once for testing, we could ascertain that there \nwas no significant (prediction by homology-based inference \npossible) sequence-similarity between PMD4k and our train-\ning splits.\nDMS4 sampled large-scale DMS in vitro experiments \nannotating binary SAV effects. This set contained binary \nclassifications (effect/non-effect) for four human proteins \n(corresponding genes: BRAC1, PTEN, TPMT, PPARG) \ngenerated previously (Reeb 2020). These were selected \nas they were the first proteins with comprehensive DMS \nexperiments including synonymous variants (needed to map \nfrom continuous effect scores to binary effect vs. neutral) \nresulting in 15,621 SAV annotations (Findlay et al. 2018; \nMajithia et al. 2016; Matreyek et al. 2018). SAVs with ben-\neficial effect (= gain of function) were excluded, because \nthey disagree between experiments (Reeb et al. 2020). The \ncontinuous effect scores of the four DMS experiments were \nmapped to binary values (effect/neutral) by considering the \n95% interval around the mean of all experimental meas-\nurements as neutral, and the 5% tails of the distribution as \n“effect”, as described in more detail elsewhere (Reeb et al. \n2020). In total, the set had 11,788 neutral SAVs and 3516 \ndeleterious effect SAVs. Additionally, we used two other \nthresholds: the 90% interval from mean (8926 neutral vs. \n4545 effect) and the 99% interval from mean (13,506 neutral \nvs. 1,548 SAVs effect).\nDMS39 collected DMS experiments annotating con-\ntinuous SAV effects. This set was used to assess whether \nthe methods introduced here, although trained only on \nbinary effect data from Eff10k, had captured continuous \neffect scales as measured by DMS. The set was a subset \nof 43 DMS experiments assembled for the development of \nDeepSequence (Riesselman et al. 2018). From the original \ncompilation, we excluded an experiment on tRNA as it is \nnot a protein, on the toxin–antitoxin complex as it comprises \nmultiple proteins and removed experiments for which only \ndouble variants existed. DMS39 contained 135,665 SAV \nscores, in total. The number of SAVs per experiment var -\nied substantially between the 39 with an average of 3625 \nSAVs/experiment, a median of 1962, a minimum of 21, and \na maximum of 12,729. However, to avoid any additional \nbiases in the comparison to other methods, we avoided any \nfurther filtering step.\nInput features\nFor the prediction of residue conservation, all newly devel-\noped methods exclusively trained on embeddings from pre-\ntrained pLMs without fine-tuning those (no gradient was \nbackpropagated to the pLM). The predictions of the best-\nperforming method for conservation prediction were used in \na second step together with substitution scores from BLO-\nSUM62 and substitution probabilities from ProtT5 as input \nfeatures to predict binary SAV effects.\nEmbeddings from pLMs: For conservation prediction, \nwe used embeddings from the following pLMs: ProtBert  \n(Elnaggar et al. 2021) based on the NLP (Natural Language \nProcessing) algorithm BERT (Devlin et al. 2019) trained on \nBig Fantastic Database (BFD) with over 2.3 million protein \nsequences (Steinegger and Söding 2018), ESM-1b (Rives \net al. 2021) that is conceptually similar to (Prot)BERT (both \nuse a Transformer encoder) but trained on UniRef50 (The \nUniProt Consortium 2021) and ProtT5-XL-U50 (Elnag-\ngar et al. 2021) (for simplicity referred to as ProtT5) based \non the NLP sequence-to-sequence model T5 (transformer \nencoder–decoder architecture) (Raffel et al. 2020) trained \non BFD and fine-tuned on Uniref50. All embeddings were \nobtained from the bio_embeddings pipeline (Dallago et al. \n2021). As described in ProtTrans, only the encoder side of \nProtT5 was used and embeddings were extracted in half-pre-\ncision (Elnaggar et al. 2021). The per-residue embeddings \nwere extracted from the last hidden layer of the models with \nsize 1024 × L (1280 for ESM-1b), where L is the length of \nthe protein sequence and 1024 (or 1280 for ESM-1b) is the \ndimension of the hidden states/embedding space of ESM-1b, \nProtBert, and ProtT5.\nContext-dependent substitution probabilities: The \ntraining objective of most pLMs is to reconstruct corrupted \namino acids from their non-corrupted protein sequence \ncontext. Repeating this task on billions of sequences allows \npLMs to learn a probability of how likely it is to observe \na token (an amino acid) at a certain position in the protein \nsequence. After pre-training, those probabilities can be \nextracted from pLMs by masking/corrupting one token/\namino acid at a time, letting the model reconstruct it based \non non-corrupted sequence context and repeating this for \neach token/amino acid in the sequence. For each protein, this \ngives a vector of length L by 20 with L being the protein’s \n1633Human Genetics (2022) 141:1629–1647 \n1 3\nlength and 20 being the probability distribution over the 20 \nstandard amino acids. It was shown recently (Meier et al. \n2021) that these probabilities provide a context-aware esti-\nmate for the effect of SAVs, i.e., the reconstruction prob-\nabilities depend on the protein sequence, and other methods \nhave made use of similar probabilities (Hopf et al. 2017; \nRiesselman et al. 2018). To generate input features for \nour SAV effect predictor, we used, as suggested by Meier \net al. (2021), the log-odds ratio between the probability of \nobserving the wild-type amino acid at a certain position and \nthe probability of observing a specific mutant at the same \nposition: log/parenleft.s1p/parenleft.s1X i,mutant\n/parenright.s1/parenright.s1− log(p /parenleft.s1X i,wildtype\n/parenright.s1) . The term \np/parenleft.s1X i,mutant\n/parenright.s1 described the probability of an SAV occurring \nat position i  and p\n/parenleft.s1\nX i,wildtype\n/parenright.s1\n described the corresponding \nprobability of the wild-type occurrence (native amino acid). \nTo extract these probabilities for SAV effect prediction, we \nonly considered the pLM embeddings correlating best with \nconservation (ProtT5). Additionally, we extracted probabili-\nties for ProtBert on ConSurf10k to analyze in more detail the \nmistakes that ProtBert makes during reconstruction (SOM \nFig. S5, S6).\nContext-independent BLOSUM62 substitution scores: \nThe BLOSUM substitution matrix gives a log-odds ratio \nfor observing an amino acid substitution irrespective of its \nposition in the protein (Henikoff and Henikoff 1992), i.e., the \nsubstitution score will not depend on a specific protein or \nthe position of a residue within a protein but rather focuses \non bio-chemical and bio-physical properties of amino acids. \nSubstitution scores in BLOSUM were derived from compar-\ning the log-odds of amino acid substitutions among well-\nconserved protein families. Typically applied to align pro-\nteins, BLOSUM scores are also predictive of SAV effects \n(Ng and Henikoff 2003; Sruthi et al. 2020).\nMethod development\nIn our three-stage development, we first compared different \ncombinations of network architectures and pLM embed-\ndings to predict residue conservation. Next, we combined \nthe best conservation prediction method with BLOSUM62 \nsubstitution scores to develop a simple rule-based predic-\ntion of binary SAV effects. In the third step, we combined \nthe predicted conservation, BLOSUM62, and substitution \nprobabilities to train a new method predicting SAV effects \nfor binary data from Eff10k and applied this method to non-\nbinary DMS data.\nConservation prediction (ProtT5cons, Fig.  1A): Using \neither ESM-1b, ProtBert, or ProtT5 embeddings as input \n(Fig. 1a), we trained three supervised classifiers to distin -\nguish between nine conservation classes taken from Con-\nSurf-DB (early stop when optimum reached for ConSurf10k \nvalidation set). The objective of this task was to learn the \nprediction of family conservation from ConSurf-DB (Ben \nChorin et al. 2020) based on the nine conservation classes \nintroduced by that method that range from 1 (variable) to \n9 (conserved) for each residue in a protein, i.e., this task \nFig. 1  Sketch of methods. Panel A sketches the conservation pre-\ndiction pipeline: (I) embed protein sequence (“SEQ”) using a pLM \n[here: ProtBERT, ProtT5 (Elnaggar et  al. 2021) or ESM-1b (Meier \net  al. 2021)]. (II) Input embedding into supervised method (here: \nlogistic regression, FNN or CNN) to predict conservation in 9-classes \nas defined by ConSurf-DB (Ben Chorin et al. 2020). (III) Map nine-\nclass predictions > 5 to conserved (C), others to non-conserved (−). \nPanel B shows the use of binary conservation predictions as input \nto SAV effect prediction by (I) considering all residue positions pre-\ndicted as conserved (C) as effect (E), all others as neutral (ProtT -\n5cons-19equal and ConSeq-19equal). (II) Residues predicted as con-\nserved are further split into specific substitutions (SAVs) predicted to \nhave an effect (E) or not (−) if the corresponding BLOSUM62 score \nis < 0, all others are predicted as neutral (ProtT5-beff, ConSeq-BLO-\nSUM62)\n1634 Human Genetics (2022) 141:1629–1647\n1 3\nimplied a multi-class per-residue prediction. Cross-entropy \nloss together with Adam (Kingma and Ba 2014) was used \nto optimize each network toward predicting one out of nine \nconservation classes for each residue in a protein (per-token/\nper-residue task).\nThe models were: (1) standard Logistic Regression (LR) \nwith 9000 (9 k) free parameters; (2) feed-forward neural net-\nwork (FNN; with two FNN layers connected through the so-\ncalled ReLU (rectified linear unit) activations (Fukushima \n1969); dropout rate 0.25; 33 k free parameters); (3) standard \nconvolutional neural network (CNN; with two convolutional \nlayers with a window size of 7, connected through ReLU \nactivations; dropout rate of 0.25; 231 k free parameters). To \nput the number of free parameters into perspective: the Con-\nSurf10k data set contained about 2.7 million samples, i.e., \nan order of magnitude more samples than free parameters \nof the largest model. On top of the 9-class prediction, we \nimplemented a binary classifier (conserved/ non-conserved; \nthreshold for projecting nine to two classes optimized on \nvalidation set). The best-performing model (CNN trained \non ProtT5) was referred to as ProtT5cons.\nRule-based binary SAV effect prediction (ProtT5beff, \nFig. 1B): For rule-based binary SAV effect (effect/neutral) \nprediction, we considered multiple approaches. The first \nand simplest approach was to introduce a threshold to the \noutput of ProtT5cons (no optimization on SAV data). Here, \nwe marked all residues predicted to be conserved (conserva-\ntion score > 5) as “effect”; all others as “neutral”. This first \nlevel treated all 19 non-native SAVs at one sequence posi -\ntion equally (referred to as “19equal” in tables and figures). \nTo refine, we followed the lead of SIFT (Ng and Henikoff  \n2003) using the BLOSUM62 (Henikoff and Henikoff \n1992) substitution scores. This led to the second rule-based \nmethod dubbed BLOSUM62bin which can be considered \na naïve baseline: SAVs less likely than expected (negative \nvalues in BLOSUM62) were classified as “effect”; all oth-\ners as “neutral”. Next, we combined both rule-based clas-\nsifiers to the third rule-based method, dubbed ProtT5beff  \n(“effect” if ProtT5cons predicts conserved, i.e., value > 5, \nand BLOSUM62 negative, otherwise “neutral”, Fig.  1b). \nThis method predicted binary classifications (effect/neu -\ntral) of SAVs without using any experimental data on SAV \neffects for optimization by merging position-aware informa-\ntion from ProtT5cons and variant-aware information from \nBLOSUM62.\nSupervised prediction of SAV effect scores (VESPA and \nVESPAl): For variant effect score prediction without align-\nments (VESPA), we trained a balanced logistic regression \n(LR) ensemble method as implemented in SciKit (Pedregosa \net al. 2011) on the cross-validation splits of Eff10k. We \nrotated the ten splits of Eff10k, such that each data split \nwas used exactly once for testing, while all remaining splits \nwere used for training. This resulted in ten individual LRs \ntrained on separate datasets. All of those were forced to \nshare the same hyper-parameters. The hyper-parameters \nthat differed from SciKit’s defaults were: (1) balanced \nweights: class weights were inversely proportional to class \nfrequency in input data; (2) maximum number of itera-\ntions taken for the solvers to converge was set to 600. The \nlearning objective of each was to predict the probability of \nbinary class membership (effect/neutral). By averaging their \noutput, we combined the ten LRs to an ensemble method: \nVESPA = ensemble of LRs= 1\n10\n∑10\ni=1 LR i . The output of \nVESPA is bound to [0,1] and by introducing a threshold \ncan be readily interpreted as a probability for an SAV to be \n“neutral” (VESPA < 0.5) or to have “effect” (VESPA ≥ 0.5). \nAs input for VESPA, we used 11 features to derive one score \nfor each SAV; nine were the position-specific conservation \nprobabilities predicted by ProtT5cons; one was the variant-\nspecific substitution score from BLOSUM62, the other the \nvariant- and position-specific log-odds ratio of ProtT5’s sub-\nstitution probabilities. To reduce the computational costs of \nVESPA, we introduced the “light” version VESPAl using \nonly conservation probabilities and BLOSUM62 as input \nand thereby circumventing the computationally more costly \nextraction of the log-odds ratio. Both VESPA and VES-\nPAl were only optimized on binary effect data from Eff10k \nand never encountered continuous effect scores from DMS \nexperiments during any optimization.\nEvaluation\nConservation prediction—ProtT5cons: To put the perfor-\nmance of ProtT5cons into perspective, we generated ConSeq \n(Berezin et al. 2004) estimates for conservation through Pre-\ndictProtein (Bernhofer et al. 2021) using MMseqs2 (Steineg-\nger and Söding 2018) and PSI-BLAST (Altschul et al. 1997) \nto generate MSAs. These were “estimates” as opposed to \nthe standard-of-truth from ConSurf-DB, because, although \nthey actually generated entire MSAs, the method for MSA \ngeneration was “just” MMseqs2 as opposed to HMMER \n(Mistry et al. 2013), and MAFFT-LINSi (Katoh and Stand-\nley 2013) for ConSurf-DB and the computation of weights \nfrom the MSA also required less computing resources. A \nrandom baseline resulted from randomly shuffling ConSurf-\nDB values.\nBinary effect prediction—ProtT5beff: To analyze the \nperformance of VESPA and VESPAl, we compared results \nto SNAP2 (Hecht et al. 2015) at the default binary threshold \n(score > − 0.05, default value suggested in original publica-\ntion) on PMD4k and DMS4. Furthermore, we evaluated the \nrule-based binary SAV effect prediction ProtT5beff on the \nsame datasets. To assess to which extent performance of \nProtT5beff could be attributed to mistakes in ProtT5cons, \nwe replaced residue conservation from ProtT5cons with \nconservation scores from ConSeq and applied the same \n1635Human Genetics (2022) 141:1629–1647 \n1 3\ntwo rule-based approaches as explained above (ConSeq \n19equal: conserved predictions at one sequence position \nwere considered “effect” for all 19 non-native SAVs and \nConSeq blosum62: only negative BLOSUM62 scores at \nresidues predicted as conserved were considered “effect”; \nall others considered “neutral” with both using the same \nthreshold in conservation as for our method, i.e., conser -\nvation > 5 for effect) for PMD4k and DMS4. This failed \nfor 122 proteins on PMD4k (3% of PMD4k), because the \nMSAs were deemed too small. We also compared ProtT -\n5beff to the baseline based only on BLOSUM62 with the \nsame thresholds as above (BLOSUM62bin). Furthermore, \nwe compared to SNAP2 at default binary threshold of effect: \nSNAP2 score > − 0.05 (default value suggested in original \npublication). SNAP2 failed for four of the PMD4k proteins \n(0.1% of PMD4k). For the random baseline, we randomly \nshuffled ground truth values for each PMD4k and DMS4.\nContinuous effect prediction—VESPA: We evalu-\nated the performance of VESPA and VESPAl on DMS39 \ncomparing to MSA-based DeepSequence (Riesselman et al. \n2018) and GEMME (Laine et al. 2019), and the pLM-based \nESM-1v (Meier et al. 2021). Furthermore, we evaluated \nlog-odds ratios from ProtT5’s substitution probabilities and \nBLOSUM62 substitution scores as a baseline. The Deep-\nSequence predictions were copied from the supplement to \nthe original publication (Riesselman et al. 2018), GEMME \ncorrelation coefficients were provided by the authors, and \nESM-1v predictions were replicated using the online reposi-\ntory of ESM-1v. We used the publicly available ESM-1v \nscripts to retrieve “masked-marginals” for each of the five \nESM-1v models and averaged over their outputs, because \nthis strategy gave best performance according to the authors. \nIf a protein was longer than 1022 (the maximum sequence \nlength that ESM-1v can process), we split the sequence into \nnon-overlapping chunks of length 1022. VESPA, VESPAl, \nand ESM-1v predictions did not use MSAs and therefore \nprovided results for the entire input sequences, while Deep-\nSequence and GEMME were limited to residues to which \nenough other protein residues were aligned in the MSAs.\nPerformance measures: We applied the following stand-\nard performance measures:\nQ2 scores (Eq. 1) described both binary predictions (con-\nservation and SAV effect). The same held for F1-scores (Eq. 6, \n7) and MCC (Matthews Correlation Coefficient, Eq. 8). We \ndefined conserved/effect as the positive class and non-con -\nserved/neutral as the negative class (indices “ + ” for positive, \n“−“ for negative) and used the standard abbreviations of TP \n(true positives: number of residues predicted and observed as \nconserved/effect), TN (true negatives: predicted and observed \n(1)\nQ2 = 100 ⋅ (Number of residues predicted correctly in 2 states)\n(Number of all residues) .\nas non-conserved/neutral), FP (false positives: predicted \nconserved/effect, observed non-conserved/neutral), and FN \n(false negatives: predicted non-conserved/neutral, observed \nconserved/effect)\nQ9 is exclusively used to measure performance for the \nprediction of nine classes of conservation taken from Con-\nSurf-DB. Furthermore, we considered the Pearson correla-\ntion coefficient\nand the Spearman correlation coefficient where raw \nscores (X, Y of Eq. 10) are converted to ranks\nfor continuous effect prediction.\nError estimates: We estimated symmetric 95% confi-\ndence intervals (CI Eq.  12) for all metrics using bootstrap-\nping (Efron et al. 1996) by computing 1.96* standard devia-\ntion (SD) of randomly selected variants from all test sets \nwith replacement over n = 1000 bootstrap sets\n(2)\nAccuracy+ = Precision+ = Positive Predicted Value= TP\nTP + FP\n(3)\nAccuracy− = Precision− = NegativePredictedValue= TN\nTN + FN\n(4)Coverage+ = Recall+ = Sensitivity= TP\nTP + FN\n(5)Coverage_ = Recall− = Speciﬁcity= TN\nTN + FP\n(6)F 1+ = 100 ∙ 2 ∙ Precision+ ∙ Recall+\nPrecision+ + Recall+\n(7)F 1− = 100 ∙ 2 ∙ Precision− ∙ Recall−\nPrecision− + Recall−\n(8)\nMCC = TP∙ TN − FP∙ FN√\n(TP+ FP)∙( TP+ FN)∙( TN + FP)∙( TN + FN)\n(9)\nQ 9 = 100 ∙ Number of residues predicted correctly in9 states\nNumber of all residues .\n(10)rP = /u1D70CX ,Y = cov(X , Y)\n/u1D70EX /u1D70EY\n,\n(11)rS = /u1D70CrgX ,rgY\n= cov(rgX , rgY )\n/u1D70EXrgX\n/u1D70ErgY\n(12)CI = 1.96 ∙ SD = 1.96 ∙\n�∑(yi − y)2\nn ,\n1636 Human Genetics (2022) 141:1629–1647\n1 3\nwith yi being the metric for each bootstrap sample and y the \nmean over all bootstrap samples. We considered differences \nin performance significant if two CIs did not overlap.\nProbability entropy: To investigate the correlation \nbetween embeddings and conservation classes of ConSurf-\nDB, we computed the entropy of pLM substitution prob-\nabilities (p) as\nResults\nWe first showed that probabilities derived from pLMs suf-\nficed for the prediction of residue conservation from pLM \nembeddings without using MSAs (data set ConSurf10k; \nmethod ProtT5cons). Next, we presented a non-parametric \nrule-based SAV effect prediction based on predicted con -\nservation (IF “predicted conserved” THEN “predict effect”; \nmethod ProtT5beff). We refined the rule-based system \nthrough logistic regression (LR) to predict SAV effect on \nvariants labeled with “effect” or “neutral” (data set Eff10k; \nmethods VESPA, VESPAl). Finally, we established that these \nnew methods trained on binary data (effect/neutral) from \nEff10k correlated with continuous DMS experiments.\nEmbeddings predicted conservation: First, we estab-\nlished that protein Language Models (pLMs) capture infor-\nmation correlated with residue conservation without ever \nseeing any such labels. As a standard-of-truth, we extracted \nthe categorical conservation scores ranging from 1 to 9 \n(9: highly conserved, 1: highly variable) from ConSurf-\nDB (Ben Chorin et al. 2020) for a non-redundant subset \nof proteins with experimentally known structures (data set \nConSurf10k). Those conservation scores correlated with \nthe mask reconstruction probabilities output by ProtBert \n(Fig. 2). More specifically, one amino acid was corrupted \nat a time and ProtBert reconstructed it from non-corrupted \nsequence context. For instance, when corrupting and recon-\nstructing all residues in ConSurf10k (one residue at a time), \nProtBert assigned a probability to the native and to each of \nthe 19 non-native (SAVs) amino acids for each position in \nthe protein. Using those “substitution probabilities”, Prot-\nBert correctly predicted the native amino acid in 45.3% of \nall cases compared to 9.4% for a random prediction of the \nmost frequent amino acid (Fig. S4). The entropy of these \nprobability distributions correlated slightly with conserva-\ntion (Fig. 2, Spearman’s R = -−0.374) although never trained \non such labels.\nNext, we established that residue conservation can be pre-\ndicted directly from embeddings by training a supervised \nnetwork on data from ConSurf-DB. We exclusively used \n(13)Entropy (p1 ,… ,pn )=−\nn/uni2211.s1\ni=1\npilog2 pi.\nembeddings of pre-trained pLMs (ProtT5, ProtBert (Elnag-\ngar et al. 2021), ESM-1b (Rives et al. 2021)), as input to \nrelatively simple machine learning models (Fig.  1). Even \nthe simplistic logistic regression (LR) reached levels of per-\nformance within about 20% of ConSeq (Berezin et al. 2004) \nconservation scores, which were derived from MSAs gen-\nerated by the fast alignment method MMseqs2 (Steinegger \nand Söding 2017) (Fig. 3). The top prediction used ProtT5 \nembeddings which consistently outperformed predictions \nfrom ESM-1b and ProtBERT embeddings. For all three \ntypes of embeddings, the CNN outperformed the FNN, and \nthese outperformed the LR. Differences between ProtBert \nand ProtT5 were statistically significant (at the 95% confi-\ndence interval, Eq.  12), while improvements from ProtT5 \nover ESM-1b were mostly insignificant. The ranking of the \nembeddings and models remained stable across several per-\nformance measures  (F1effect,  F1neutral, MCC, Pearson correla-\ntion coefficient, Table S1).\nConSurf-DB (Ben Chorin et al. 2020) simplifies family \nconservation to a single digit integer (9: highly conserved, \n1: highly variable). We further reduced these classes to a \nbinary classification (conserved/non-conserved) to later \nFig. 2  pLMs captured conservation without supervised training or \nMSAs. ProtBert was optimized to reconstruct corrupted input tokens \nfrom non-corrupted sequence context (masked language modeling). \nHere, we corrupted and reconstructed all proteins in the ConSurf10k \ndataset, one residue at a time. For each residue position, ProtBert \nreturned the probability for observing each of the 20 amino acids \nat that position. The higher one probability (and the lower the cor -\nresponding entropy), the more certain the pLM predicts the corre-\nsponding amino acid at this position from non-corrupted sequence \ncontext. Within the displayed boxplots, medians are depicted as black \nhorizontal bars; whiskers are drawn at the 1.5 interquartile range. \nThe x-axis gives categorical conservation scores (1: highly variable, \n9: highly conserved) computed by ConSurf-DB (Ben Chorin et  al. \n2020) from multiple sequence alignments (MSAs); the y-axis gives \nthe probability entropy (Eq.  13) computed without MSAs. The two \nwere inversely proportional with a Spearman’s correlation of -0.374 \n(Eq.  11), i.e., the more certain ProtBert’s prediction, the lower the \nentropy and the higher the conservation for a certain residue position. \nApparently, ProtBert had extracted information correlated with resi-\ndue conservation during pre-training without having ever seen MSAs \nor any labeled data\n1637Human Genetics (2022) 141:1629–1647 \n1 3\ntransfer information from conservation to binary SAV \neffect (effect/neutral) more readily. The optimal threshold \nfor a binary conservation prediction was 5 (> 5 conserved, \nFig. S1). However, performance was stable across a wide \nrange of choices: between values from 4 to 7, MCC (Eq. 8) \nchanged between 0.60 and 0.58, i.e., performance varied \nby 3.3% for 44.4% of all possible thresholds (Fig. S1). This \nwas explained by the nine- and two-class confusion matrices \n(Fig. S2 and S3) for ProtT5cons, which showed that most \nmistakes were made between neighboring classes of similar \nconservation, or between the least conserved classes 1 and 2.\nConservation-based prediction of binary SAV effect \nbetter for DMS4 than for PMD4k? Next, we established \nthat we could use the predicted conservation of ProtT5cons \nfor rule-based binary SAV effect prediction without any fur-\nther optimization and without any MSA. In using predicted \nconservation to proxy SAV effect, we chose the method best \nin conservation prediction, namely the CNN using ProtT5 \nembeddings (method dubbed ProtT5cons, Fig.  1B). The \nover-simplistic approach of considering any residue pre-\ndicted as conserved to have an effect irrespective of the SAV \n(meaning: treat all 19 non-native SAVs alike) was referred \nto as “19equal”. We refined this rule-based approach by \ncombining conservation prediction with a binary BLO-\nSUM62 score (effect: if ProtT5cons predicted conserved \nAND BLOSUM62 < 0, neutral otherwise), which we \nreferred to as ProtT5beff. For PMD4k, the following results \nwere common to all measures reflecting aspects of preci -\nsion and recall through a single number  (F1effect,  F1neutral and \nMCC). First, the expert method SNAP2 trained on Eff10k \n(superset of PMD4k) achieved numerically higher values \nthan all rule-based methods introduced here. Second, using \nthe same SAV effect prediction for all 19 non-native SAVs \nconsistently reached higher values than using the BLO-\nSUM62 values (Fig.  4 and Table  1: 19equal higher than \nblosum62). For some measures (Q2,  F1effect), values obtained \nusing ConSeq for conservation (i.e., a method using MSAs) \nwere higher than those for the ProtT5cons prediction (with-\nout using MSAs), while for others (MCC,  F1neutral), this was \nreversed (Fig. 4, Table 1, Table S2).\nMost performances differed substantially between \nPMD4k and DMS4, i.e., the first four proteins (BRAC1, \nPTEN, TPMT, and PPARG) for which we had obtained \nlarge-scale experimental DMS measures that could be con-\nverted into a binary scale (effect/neutral). First, using BLO-\nSUM62 to convert ProtT5cons into SAV-specific predictions \nFig. 3  Conservation predicted accurately from embeddings. Data: \nhold-out test set of ConSurf10k (519 sequences); panel A: nine-state \nper-residue accuracy (Q9, Eq. 9) in predicting conservation as defined \nby ConSurf-DB (Ben Chorin et al. 2020); panel B: two-state per-resi-\ndue accuracy (Q2, Eq. 1; conservation score > 5: conserved, non-con-\nserved otherwise). Supervised models (trained on ConSurf10k): LR: \nlogistic regression (9,000 = 9  k free parameters), FNN feed-forward \nnetwork (33  k parameters), and CNN convolutional neural network \n(231  k parameters with 0.25 dropout rate); methods: ConSeq com-\nputation of conservation weight through multiple sequence align-\nments (MSAs) (Berezin et  al. 2004); Random random label swap. \nModel inputs were differentiated by color (green: ESM-1b embed-\ndings (Rives et al. 2021), red: ProtBert embeddings (Elnaggar et al. \n2021), blue: ProtT5 embeddings (Elnaggar et al. 2021), gray: MSAs \n(MMseqs2 (Steinegger and Söding 2017), and PSI-BLAST (Alts-\nchul et al. 1997)). Black whiskers mark the 95% confidence interval \n(± 1.96 SD; Eq.  12). ESM-1b and ProtT5 embeddings outperformed \nthose from ProtBERT (Elnaggar et  al. 2021); differences between \nESM-1b and ProtT5 were not statistically significant, but ProtT5 \nconsistently outperformed ESM-1b in all metrics but Q2 (Table S1). \nESM-1b and ProtT5 as input to the CNN came closest to ConSeq \n(Table S1)\n1638 Human Genetics (2022) 141:1629–1647\n1 3\noutperformed the MSA-based conservation lookup from \nConSeq, the expert method SNAP2 trained on PMD4k \n(Table  1: ProtT5beff highest rule-based), and the newly \nintroduced VESPA. Second, combining the BLOSUM62 \nmatrix with conservation also improved ConSeq (Table  1: \nConSeq: 19equal lower than blosum62). Third, ranking \nacross different performance measures correlated much \nbetter than for PMD4k (Tables S1–S5). As the mapping \nfrom continuous DMS effect scores to binary labels might \nintroduce systematic noise, we also investigated different \nthresholds for this mapping. However, results for DMS4 at \nintervals of 90% (Table S3) and 99% (Table S5) around the \nmean showed similar trends.\nWe trained a logistic regression (LR) ensemble (VESPA) \non cross-validation splits replicated from the SNAP2 devel-\nopment set. For binary effect prediction, we introduced \na threshold (≥  0.5 effect, otherwise neutral) to the output \nscores of VESPA. When comparing VESPA and VESPAl \n(light version of VESPA) to the other methods on PMD4k, \nwe observed a different picture than for the rule-based \napproaches. While SNAP2 still resulted in the highest MCC \n(0.28 ± 0.01), it was not significantly higher than that of \nVESPA and VESPAl (MCC: 0.274 ± 0.09 and 0.271 ± 0.09, \nrespectively), and its development set overlapped with \nPMD4k. When evaluating the methods on DMS4, the best-\nperforming method, VESPAl (MCC 0.405 ± 0.016), outper-\nformed SNAP2 (MCC 0.204 ± 0.012) and VESPA (MCC \n0.346 ± 0.014) as well as all rule-based methods (Table  1). \nWe observed the same trends for other intervals (Tables \nS3–S5).\npLMs predicted SAV effect scores without MSAs.  \nCould VESPA, trained on binary effect data (Eff10k) capture \ncontinuous SAV effect scores measured by DMS? For ease \nof comparison with other methods, we chose all 39 DMS \nexperiments (DMS39) with single SAV effect data assem -\nbled for the development of DeepSequence (Riesselman \net al. 2018). Several methods have recently been opti-\nmized on DMS data, e.g., the apparent state-of-art (SOTA), \nDeepSequence trained on the MSAs of each of those 39 \nexperiments. Another recent method using evolutionary \nFig. 4  Embedding-based binary SAV effect prediction is seem-\ningly competitive. Data: PMD4k (red bars; 4  k proteins from PMD \n(Kawabata et al. 1999)); DMS4 (blue bars) first four human proteins \n(BRAC1, PTEN, TPMT, PPARG) with comprehensive experimen-\ntal DMS measurements including synonyms (here 95% thresh-\nold) (Reeb et  al. 2020). Methods: SUPERVISED: a SNAP2bin: \neffect SNAP2 score > −  0.05, otherwise neutral; b VESPA: effect \nVESPA score >  = 0.5, otherwise neutral; c VESPAl: effect VESPAl \nscore >  = 0.5, otherwise neutral. RULE-BASED: d BLOSUM62bin: \nirrespective of residue position, negative BLOSUM62 scores pre-\ndicted as effect, others as neutral; e ProtT5cons|ConSeq 19equal: all \n19 non-native SAVs predicted equally: effect if ProtT5cons|ConSeq \npredicted residue position to be conserved, otherwise neutral; f \nProtT5beff|ConSeq blosum62: effect if ProtT5cons|ConSeq predicts \nconserved and BLOSUM62 negative, otherwise neutral. BASELINE: \ng Random: random shuffle of experimental labels. All values for \nDMS4 computed for binary (effect/neutral) mapping of experimental \nDMS values with panel A giving the two-state per-residue accuracy \n(Q2, Eq.  1) and panel B giving the Matthews Correlation Coeffi-\ncient (MCC, Eq.  8). Error bars: Black bars mark the 95% confidence \ninterval (± 1.96 SD, Eq.  12). For all methods, the MCC differences \nbetween the two data sets PMD4k and DMS4 were statistically sig-\nnificant (exception: random)\n1639Human Genetics (2022) 141:1629–1647 \n1 3\ninformation in a more advanced way than standard profiles \nfrom MSAs appears to reach a similar top level without \nmachine learning, namely GEMME (Laine et al. 2019), \nand so does a method based on probabilities from pLMs, \nnamely ESM-1v, without using MSAs. Comparing all those \nto VESPA, we could not observe a single method outper -\nforming all others on all DMS39 experiments (Fig.  5). The \nfour methods compared (two using MSAs: DeepSequence \nand GEMME, two using probabilities from pLMs instead of \nMSAs: ESM-1v and VESPA) reached Spearman rank cor -\nrelations above 0.4 for 36 DMS experiments. In fact, for the \n11 highest correlating out of the 39 experiments, predictions \nwere as accurate as typically the agreement between two dif-\nferent experimental studies of the same protein (Spearman \n0.65 (Reeb et al. 2020)).\nGEMME had a slightly higher mean and median Spear -\nman correlation (Eq.  11) than DeepSequence, ESM-1v, \nVESPA, and all others tested (Fig. 6A, Table 2). When con-\nsidering the symmetric 95% confidence intervals (Eq.  12), \nalmost all those differences were statistically insignificant \n(Fig.  6B) except for only using BLOSUM62. In terms of \nmean Spearman correlation, VESPA was slightly higher \nthan DeepSequence, which was slightly higher than ESM-\n1v (Fig. 6A), but again neither was significantly better. The \nmedian Spearman correlation was equal for ESM-1v and \nVESPA and insignificantly lower for DeepSequence. The \nfastest method, VESPAl, reached lower Spearman correla-\ntions than all other major methods (Fig.  6). Ranking and \nrelative performance after correcting for statistical signifi-\ncance were identical for Spearman and Pearson correlation \n(Table S6).\nFor comparison, we also introduced two advances on a \nrandom baseline, namely the raw BLOSUM62 scores and the \nraw ProtT5 log-odds scores (Fig.  6; Fig. S7). BLOSUM62 \nwas consistently and statistically significantly outperformed \nby all methods, while the ProtT5 log-odds averages were \nconsistently lower, albeit not with statistical significance. As \npLM-based methods were independent of MSAs, they pre-\ndicted SAV scores for all residues contained in the DMS39 \ndata sets, while, e.g., DeepSequence and GEMME could \npredict only for the subset of the residues covered by large \nenough MSAs. This was reflected by decreased coverage of \nmethods relying on MSAs (DeepSequence and GEMME; \nTable S8). The Spearman correlation of ESM-1v, VESPA, \nand VESPAl for the SAVs in regions without MSAs was \nsignificantly lower than that in regions with MSAs available \n(Table S7).\nSAV effect predictions blazingly fast:  One important \nadvantage of predicting SAV effects without using MSAs \nis the computational efficiency. For instance, to predict the \nTable 1  Performance in binary \nSAV effect  predictiona\na Data sets: The PMD4k data set contained 4 k proteins from the PMD (Kawabata et al. 1999); 74% of the \nSAVs were deemed effect in a binary classification. DMS4 marks the first four human proteins (BRAC1, \nPTEN, TPMT, PPARG) for which we obtained comprehensive experimental DMS measurements along \nwith a means of converting experimental scores into a binary version (effect/neutral) using synonyms. \nDMS4 results are shown for a threshold of 95%: the continuous effect scores were binarized by assign-\ning the middle 95% of effect scores as neutral variants and SAVs resulting in effect scores outside this \nrange as effect variants (Reeb et al. 2020). Methods: SNAP2bin: effect SNAP2 score > − 0.05, otherwise \nneutral; VESPA: effect score ≥ 0.5, otherwise neutral; VESPAl: effect score ≥ 0.5, otherwise neutral; BLO-\nSUM62: negative BLOSUM62 scores predicted as effect, others as neutral; ProtT5cons|ConSeq-19equal: \nall 19 non-native SAVs predicted equally: effect if ProtT5cons|ConSeq predicted/labeled as conserved, oth-\nerwise neutral; ProtT5beff|ConSeq-blosum62: effect if ProtT5cons|ConSeq predicted/labeled as conserved \nand BLOSUM62 negative, otherwise neutral. ± values mark the 95% confidence interval (Eq. 12). For each \ncolumn, if available, significantly best results are highlighted in bold\nData set PMD4k DMS4\nMethod/metric Q2\n(Eq. 1)\nMCC\n(Eq. 8)\nQ2\n(Eq. 1)\nMCC\n(Eq. 8)\nRandom 61.08% ± 0.41 − 0.002 ± 0.016 64.27% ± 0.76 − 0.001 ± 0.018\nSupervised methods\n SNAP2bin 70.66% ± 0.39 0.280 ± 0.010 41.55% ± 0.82 0.204 ± 0.012\n VESPA 63.52% ± 0.43 0.274 ± 0.086 63.56% ± 0.79 0.346 ± 0.014\n VESPAl 63.04% ± 0.43 0.271 ± 0.085 72.59% ± 0.72 0.405 ± 0.016\nRule-based methods\n BLOSUM62bin 56.17% ± 0.43 0.049 ± 0.010 44.47% ± 0.84 0.169 ± 0.014\n ProtT5cons-19equal 68.58% ± 0.41 0.227 ± 0.010 62.20% ± 0.82 0.322 ± 0.014\n ProtT5-beff 52.26% ± 0.43 0.160 ± 0.016 71.47% ± 0.75 0.369 ± 0.016\n ConSeq-19equal 71.51% ± 0.39 0.206 ± 0.010 50.70% ± 0.84 0.267 ± 0.012\n ConSeq blosum62 54.32% ± 0.43 0.138 ± 0.016 63.81% ± 0.8 0.318 ± 0.014\n1640 Human Genetics (2022) 141:1629–1647\n1 3\nmutational effects for all 19 non-native SAVs in the entire \nhuman proteome (all residues in all human proteins) took \n40 min on one Nvidia Quadro RTX 8000 using VESPAl. In \nturn, this was 40 min more than using BLOSUM62 alone \n(nearly instantaneous), but this instantaneous BLOSUM62-\nbased prediction was also much worse (Q2 for binary BLO-\nSUM62 prediction worse than random, Table  1). In con-\ntrast, running methods such as SNAP2 (or ConSeq) required \nfirst to generate MSAs. Even the blazingly fast MMseqs2 \n(Steinegger and Söding 2017) needed about 90 min using \nbatch-processing on an Intel Skylake Gold 6248 processor \nwith 40 threads, SSD and 377 GB main memory. While \nVESPAl computed prediction scores within minutes for an \nentire proteome, VESPA and ESM-1v require minutes for \nsome single proteins depending on sequence length, e.g., \nESM-1v took on average 170 s per protein for the DMS39 \nset, while ProtT5 required on average 780 s. This originated \nfrom the number of forward passes required to derive pre-\ndictions: while VESPAl needed only a single forward pass \nthrough the pLM to derive embeddings for conservation \nprediction, VESPA and ESM-1v (when deriving “masked-\nmarginals” as recommended by the authors) required L for-\nward passes with L being the protein length, because they \ncorrupt one amino acid at a time and try to reconstruct it. \nThe large difference in runtime between ESM-1v and ProtT5 \noriginated from the fact that ESM-1v cropped sequences \nafter 1022, reducing the strong impact of outliers, i.e., runt-\nime of transformer-based models scales quadratically with \nFig. 5  No SAV effect prediction consistently best on DMS data. \nData: DMS39 (39 DMS experiments gathered for the development of \nDeepSequence (Riesselman et al. 2018)); experiments sorted by the \nmaximum absolute Spearman coefficient for each experiment. Meth-\nods: a DeepSequence trained an unsupervised model for each DMS \nexperiment using only MSA input, i.e., no effect score labels were \nused (Riesselman et al. 2018); b GEMME inferred evolutionary trees \nand conserved sites from MSAs to predict effects (Laine et al. 2019); \nc ESM-1v correlated log-odds of substitution probabilities (Methods) \nwith SAV effect magnitudes (Meier et al. 2021); d VESPA (this work) \ntrained a logistic regression ensemble on binary SAV classification \n(effect/neutral) using predicted conservation (ProtT5cons), BLO-\nSUM62 (Henikoff and Henikoff 1992), and log-odds of substitution \nprobabilities from ProtT5 (Elnaggar et  al. 2021) as input (without \nany optimization on DMS data). The values for the absolute Spear -\nman correlation (Eq.  11) are shown for each method and experiment. \nThe rightmost column shows the mean absolute Spearman correlation \nfor each method. Although some experiments correlated much better \n(toward left) with predictions than others (toward right), the spread \nbetween prediction methods appeared high for both extremes; Deep-\nSequence was the only method reaching a correlation of 0 for one \nexperiment; another one and three experiments were predicted with \ncorrelations below 0.2 for ESM-1v and DeepSequence, respectively, \nwhile the vast number of the 4 × 39 predictions reached correlations \nabove 0.4\n1641Human Genetics (2022) 141:1629–1647 \n1 3\nsequence length, so while the shortest protein (71 residues) \nin the DMS39 set took only 5 s to compute, the longest \n(3033 residues) took 4.5 h to compute. We leave investigat-\ning the effect of splitting very long proteins into (overlap-\nping) chunks to future work.\nDiscussion\nConservation predicted by embeddings without MSAs. \nEven a simple logistic regression (LR) sufficed to predict \nper-residue conservation values from raw embeddings with-\nout using MSAs (Fig. 3, Table S1). Relatively shallow CNNs \n(with almost 100-times fewer free parameters than samples \ndespite early stopping) improved over the LR to levels in \npredicting conservation only slightly below conservation \nassigned by ConSeq which explicitly uses MSAs (Fig.  3, \nTable S1). Did this imply that the pLMs extracted evolution-\nary information from unlabeled sequence databases (BFD \n(Steinegger and Söding 2018) and UniProt (The UniProt \nConsortium 2021))? The answer might be more elusive than \nit seems. The methodology (pLMs) applied to predict con-\nservation never encountered any explicit information about \nprotein families through MSAs, i.e., the pLMs used here \nnever had an explicit opportunity to pick up evolutionary \nconstraints from related proteins. The correlation between \nsubstitution probabilities derived from pLMs and conser -\nvation (Fig. 2) might suggest that pLMs implicitly learned \nevolutionary information.\nA possible counterargument builds around the likelihood \nto pick up evolutionary constraints. The pLM clearly learned \nthe reconstruction of more frequent amino acids much better \nthan that of less frequent ones (Fig. S5). Unsurprisingly, AI \nis pushed most in the direction of most data. In fact, the dif-\nferences between amino acid compositions were relatively \nsmall (less than factor of 10), suggesting that even an event \noccurring at one-tenth of the time may challenge pLMs. If \nthe same pLM has to learn the evolutionary relation between \ntwo proteins belonging to the same family, it has to effec-\ntively master an event happening once in a million (assum-\ning an average family size of about 2.5 k—thousand—in \na database with 2.5b—billion—sequences). How can the \nmodel trip over a factor of  101 and at the same time master a \nfactor of  106? Indeed, it seems almost impossible. If so, the \npLM may not have learned evolutionary constraints, but the \ntype of bio-physical constraint that also constrain evolution. \nIn this interpretation, the pLM did not learn evolution, but \nFig. 6  Spearman correlation between prediction and DMS experi-\nment varied. Data and methods as for Fig.  5 with addition of: VES-\nPAl: fast version of VESPA with input limited to ProtT5cons and \nBLOSUM62; ProtT5-logodds: raw log-odds from ProtT5 embed-\ndings (Elnaggar et al. 2021); and raw BLOSUM62 substitution scores \n(Henikoff and Henikoff 1992). Panel A: mean absolute Spearman \ncorrelation coefficient (Eq.  11) for each method over all 39 DMS \nexperiments; error bars highlight 0.95 confidence interval (1.96 \nstandard errors). Ignoring statistical significance, the numerical rank -\ning would be: GEMME, VESPA, DeepSequence, ESM-1v, VESPAl, \nProtT5-logodds, and BLOSUM62. However, the first four did not \ndiffer by any statistical significance, and while those ranked 5 and 6 \ndiffered from the best four, 5 was close to 4, and 6 close to 5; only \nBLOSUM62, the raw substitution scores compiled as background \nwere clearly worst. Panel B: boxplots on absolute Spearman correla-\ntion coefficients (Eq.  11) for each method over the 39 DMS experi-\nments. The medians are depicted as black horizontal bars; whiskers \nare drawn at the 1.5 interquartile range\n1642 Human Genetics (2022) 141:1629–1647\n1 3\nthe constraints “written into protein sequences” that deter -\nmine which residue positions are more constrained.\nIn fact, one pLM used here, namely ProtT5, has recently \nbeen shown to explicitly capture aspects of long-range \ninter-residue distances directly during pre-training, i.e., \nwithout ever being trained on any labeled data pLMs pick \nup structural constraints that allow protein 3D structure \nprediction from single protein sequences (Weißenow et al. \n2021). Another explanation for how ProtT5 embeddings \ncapture conservation might be that pLMs picked up signals \nfrom short, frequently re-occurring sequence/structure \nmotifs such as localization signals or catalytic sites that \nare more conserved than other parts of the sequence. If \nso, the pLM would not have to learn relationship between \nproteins but only between fragments, thereof reducing \nthe factor  106 substantially. We could conceive of these \nmotifs resembling some evolutionary nuclei, i.e., frag -\nments shorter than structural domains that drove evolu-\ntion (Alva et al. 2015; Ben-Tal and Lupas 2021; Kolodny \n2021). Clearly, more work will have to shed light on the \nefficiency of (p)LMs in general (Bommasani et al. 2021).\nTransformer-based pLMs best? We have tested a \nlimited set of pLMs, largely chosen, because those had \nappeared to perform better than many other methods for a \nvariety of different prediction tasks. Does the fact that in \nour hands Transformer-based pLMs worked best to predict \nresidue conservation and SAVs imply that those will gen-\nerally outperform other model types? By no means. While \nwe expect that the about twenty approaches that we have \ncompared in several of our recent methods (including the \nfollowing 13: ESM-1[b|v] Meier et al. 2021; Rives et al. \n2021), ProSE[*|DLM|MT] (Bepler and Berger 2019b, \n2021), Prot[Albert|Bert|Electra|Vec|T5|T5XL|T5XLNet|T\n5XXL] (Elnaggar et al. 2021; Heinzinger et al. 2019) pro-\nvided a somehow representative sampling of the existing \noptions, our conclusions were only valid for embeddings \nextracted in a generic way from generic pLMs without any \nbearing on the methods underlying those pLMs.\nPredicted conservation informative about SAV \neffects: DMS data sets with comprehensive experimental \nprobing of the mutability landscape (Hecht et al. 2013) as, \ne.g., collected by MaveDB (Esposito et al. 2019) continue \nto pose problems for analysis, possibly due to a diversity \nof assays and protocols (Livesey and Marsh 2020; Reeb \net al. 2020). Nevertheless, many such data sets capture \nimportant aspects about the susceptibility to change, i.e., \nthe mutability landscape (Hecht et al. 2013). As always, \nthe more carefully selected data sets become, the more \nthey are used for the development of methods and there-\nfore no longer can serve as independent data for assess-\nments (Grimm et al. 2015; Reeb et al. 2016). Avoiding the \ntraps of circularity and over-fitting by skipping training, \nour non-parametric rule-based approaches (ProtT5cons \nand ProtT5beff) suggested that predictions of SAV effects \n(by simply assigning “effect” to those SAVs where ProtT -\n5cons predicted conserved and the corresponding BLO-\nSUM62 value was negative) outperformed ConSeq with \nMSAs using the same idea, and even the expert effect pre-\ndiction method SNAP2 (Fig.  4, Table 1).\nStrictly speaking, it might be argued that one single free \nparameter was optimized using the data set, because for the \nPMD4k data set, the version that predicted the same effect \nfor all 19-SAVs appeared to outperform the SAV-specific \nprediction using BLOSUM62 (19equal vs blosum62 in \nFig. 4 and Table 1). However, not even the values computed \nfor PMD4k could distract from the simple fact that not all \nSAVs are equal, i.e., that regardless of model performance, \n19equal will not be used exclusively for any method. In fact, \nthe concept of combining predictions with BLOSUM62 val-\nues has been shown to succeed for function prediction before \n(Bromberg and Rost 2008; Schelling et al. 2018) in that \nsense it was arguably not an optimizable hyperparameter. \nEmbeddings predicted conservation (Fig.  3); conservation \npredicted SAV effects (Fig.  4). Did this imply that embed-\ndings captured evolutionary information? Once again, we \ncould not answer this question either way directly. To repeat: \nour procedure/method never used information from MSAs \nin any way. Could it have implicitly learned this? To repeat \nthe previous speculation: embeddings might capture a real-\nity that constrains what can be observed in evolution, and \nthis reality is exactly what is used for the part of the SAV \neffect prediction that succeeds. If so, we would argue that \nour simplified method did not succeed, because it predicted \nTable 2  Spearman correlation between SAV effect prediction and \nDMS  experimentsa\na Data sets: DMS39 [39 DMS experiments gathered for the develop-\nment of DeepSequence (Riesselman et al. 2018)] with 135,665 SAV \nscores. Methods: DeepSequence: AI trained on MSA for each of the \nDMS experiments (Riesselman et  al. 2018); GEMME: using evo-\nlutionary information calculated from MSAs with few parameters \noptimized on DMS (Laine et  al. 2019); ESM-1v: embedding-based \nprediction methods (Meier et  al. 2021); VESPA: method developed \nhere using logistic regression to combine predicted conservation \n(ProtT5cons), BLOSUM62 (Henikoff and Henikoff 1992) substitu-\ntion scores, and log-odds from ProtT5 (Elnaggar et  al. 2021); VES-\nPAl: “light” version of VESPA using only predicted conservation and \nBLOSUM62 as input. ± values mark the standard error\nMethod Mean absolute rS\n(Eq. 11)\nMedian absolute rS\n(Eq. 11)\nMSA-based\n DeepSequence 0.50 ± 0.03 0.52 ± 0.03\n GEMME 0.53 ± 0.02 0.56 ± 0.02\npLM-based\n ESM-1v 0.49 ± 0.02 0.53 ± 0.02\n VESPA 0.51 ± 0.02 0.53 ± 0.02\n VESPAl 0.47 ± 0.02 0.47 ± 0.02\n1643Human Genetics (2022) 141:1629–1647 \n1 3\nconservation without using MSAs, but that it captured posi-\ntions biophysically “marked by constraints”, i.e., residues \nwith higher contact density in protein 3D structures (Weiße-\nnow et al. 2021). This assumption would explain how pre-\ndicted conservation (ProtT5cons) not using evolutionary \ninformation could predict SAV effects better than a slightly \nmore correct approach (ConSeq) using MSAs to extract evo-\nlutionary information (Fig. 4: ProtT5cons vs. ConSeq).\nSubstitution probabilities from pLMs capture aspects \nmeasured by DMS experiments:  Using embeddings to \npredict SAV effects through conservation prediction suc-\nceeded but appeared like a detour. ESM-1v (Meier et al. \n2021) pioneered a direct path from reconstruction/substitu-\ntion probabilities of pLMs to SAV effect predictions. When \ncomparing the ESM-1v encoder-based with the ProtT5 \nencoder–decoder-based Transformer, we encountered sur -\nprising results. Previously, ProtT5 usually performed at least \non par with previous versions of ESM (e.g., ESM-1b (Rives \net al. 2021)) or outperformed them (Elnaggar et al. 2021). In \ncontrast, the substitution probabilities of ProtT5 were clearly \ninferior to those from ESM-1v in their correlation with the \n39 DMS experiments (Fig.  6). This reversed trend might \nhave resulted from a combination of the following facts: \n(1) ProtT5 is a single model, while ESM-1v is an ensem-\nble of five pLMs potentially leading to a smoother substitu-\ntion score. (2) ESM-1v was trained on UniRef90 instead \nof BFD/UniRef50 (ProtT5) possibly providing a broader \nview on the mutability landscape of proteins. In fact, the \nESM-1v authors showed a significant improvement when \npre-training on UniRef90 instead of UniRef50 (Rives et al. \n2021). (3) ESM-1v is a BERT-style, encoder-based Trans-\nformer, while ProtT5 is based on T5’s encoder-decoder \nstructure. In previous experiments (Elnaggar et al. 2021), \nwe only extracted embeddings from ProtT5’s encoder (e.g., \nProtT5cons is based on encoder embeddings), because its \ndecoder fell significantly short in all experiments. However, \nonly T5’s decoder can output probabilities, so we had to \nfall back to ProtT5’s decoder for SAV effect predictions. \nThis discrepancy of encoder and decoder performance can \nonly be sketched here. In short, encoder-based transformer \nmodels always see the context of the whole sequence (as \ndoes ProtT5 ‘s encoder and ESM-1v), while decoder-based \ntransformer models (such as ProtT5’s decoder or GPT (Rad-\nford et al. 2019)) see only single-sided context, because they \nare generating text (sequence-to-sequence models (Sutsk -\never et al. 2014)). This is crucial for translation tasks, but \nappeared sub-optimal in our setting. Despite this shortcom-\ning in performance, we trained VESPA based on log-odds \nderived from ProtT5 substitution probabilities, mainly \nbecause we started this work before the release of ESM-\n1v. Also, we hoped for synergy effects when implementing \nVESPA into the PredictProtein webserver, because ProtT5 \nis already used by many of our predictors. Finding the best \ncombination of pLM substitution probabilities for SAV \neffect prediction will remain subject for future work.\nFast predictions save computing resources? Our simple \nprotocol introduced here enabled extremely efficient, speedy \npredictions. While pre-training pLMs consumed immense \nresources (Elnaggar et al. 2021), this was done in the past. \nThe new development here was the models for the 2nd level \nsupervised transfer learning. Inputting ProtT5 embeddings \nto predict residue conservation (ProtT5cons) or SAV effects \n(VESPA/VESPAl) for predictions in the future will consume \nvery little additional resources. When running prediction \nservers such as PredictProtein (Bernhofer et al. 2021) que-\nried over 3000 times every month, such investments could \nbe recovered rapidly at seemingly small prices to pay even \nif performance was slightly reduced. How to quantify this? \nAt what gain in computing efficiency is which performance \nreduction acceptable? Clearly, there will not be one answer \nfor all purposes, but the recent reports on climate change \nstrongly suggest to begin considering such questions.\nQuantitative metrics for hypothetical improvements \nover MSA-based methods? If methods using single \nsequences without MSAs perform as well as, or even better \nthan, SOTA methods using MSAs, could we quantify met-\nrics measuring the hypothetical improvements from embed-\ndings? This question raised by an anonymous reviewer opens \nan interesting new perspective. Gain in speed, reduction of \ncomputational costs clearly could evolve as one such met-\nric. A related issue is related to protein design: for some \napplications, the difference in speed might open new doors. \nAlthough we have no data to show for others, we could \nimagine yet another set of metrics measuring the degree \nto which embedding-based methods realize more protein-\nspecific than family averaged predictions.\nConclusions\nEmbeddings extracted from protein Language Models \n(pLMs, Fig.  1), namely from ProtBert and ProtT5 (Elnag-\ngar et al. 2021) and ESM-1b (Rives et al. 2021), contain \ninformation that sufficed to predict residue conservation \nin protein families without using multiple sequence align -\nments (MSAs, Fig.  3). Such predictions of conservation \ncombined with BLOSUM62 scores predicted the effects of \nsequence variation (single amino acid variants, or SAVs) \nwithout optimizing any additional free parameter (ProtT5b-\neff, Fig. 6). Through further training on binary experimen-\ntal data (effect/neutral), we developed VESPA, a relatively \nsimple, yet apparently successful new method for SAV effect \nprediction (Fig.  4). This method even worked so well on \nnon-binary data from 39 DMS experiments that without \never using such data nor ever using MSAs; VESPA appeared \n1644 Human Genetics (2022) 141:1629–1647\n1 3\ncompetitive with the SOTA (Fig.  5, Fig.  6), although for \nSAV effect predictions, embedding-based methods are still \nnot yet outperforming the MSA-based SOTA as for other \nprediction tasks (Elnaggar et al. 2021; Littmann et al. 2021a, \nb, c; Stärk et al. 2021). Embedding-based predictions are \nblazingly fast, thereby they save computing, and ultimately \nenergy resources when applied to daily sequence analysis. In \ncombination, our results suggested that the major signal cap-\ntured by variant effect predictions originates from some bio-\nphysical constraint revealed by raw protein sequences. The \nConSurf10k dataset is available at https:// doi. org/ 10. 5281/ \nzenodo. 52385 37. For high-throughput predictions, methods \nare available through bio_embeddings (Dallago et al. 2021). \nFor single queries VESPA and ProtT5cons will be made \navailable through the PredictProtein server (Bernhofer et al. \n2021). VESPA and VESPAl are also available from github \nat https:// github. com/ Rostl ab/ VESPA.\nSupplementary Information The online version contains supplemen-\ntary material available at https:// doi. org/ 10. 1007/ s00439- 021- 02411-y.\nAcknowledgements Thanks to Tim Karl and Inga Weise (both TUM) \nfor invaluable help with technical and administrative aspects of this \nwork. Thanks to Nir Ben-Tal (Tel Aviv U) and his team for the excellent \nservices around ConSurf-DB and ConSeq; to Yana Bromberg (Rutgers \nU), Max Hecht (Amazon) for advancing SNAP; to Adam Riesselman, \nJohn Ingraham, and Debbie Marks (Harvard) for making their collec-\ntion of DMS data available; to Elodie Laine (Sorbonne U) for providing \nthe predictions of GEMME; to the group around Facebook AI Research \nfor making ESM-1b and ESM-1v readily available; to the Dunbrack \nlab for Pisces, and most importantly to Martin Steinegger (Seoul Natl. \nUniv.) and his team for MMseqs2 and BFD. Particular thanks to two \nanonymous reviewers who helped crucially with improving this work \nand to the valuable comments from the editors. Last, but not least, \nthanks to all who deposit their experimental data in public databases, \nand to those who maintain these databases.\nAuthor contributions CM implemented and evaluated the methods and \ntook the lead in writing the manuscript. MH conceived, trained, and \nevaluated the neural networks on conservation prediction, contributed \nideas, and proofread the manuscript. TO and CD contributed crucial \nideas and provided valuable comments. MB helped in generating the \nevaluation methods ConSeq and SNAP2. KE supported the work \nwith coding advice and created the original ConSurf10k data set. DN \ncontributed the clusters and subsets of the SNAP2 development set. \nBR supervised and guided the work and co-wrote the manuscript. All \nauthors read and approved the final manuscript.\nFunding Open Access funding enabled and organized by Projekt \nDEAL. This work was supported by the DFG grant RO 1320/4-1, \nSoftware Campus Funding (BMBF 01IS17049) and the KONWIHR \nProgram and by the Bavarian Ministry for Education through funding \nto the TUM.\nDeclarations \nConflict of interest No author declares any competing interest.\nOpen Access This article is licensed under a Creative Commons \nAttribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, \nas long as you give appropriate credit to the original author(s) and the \nsource, provide a link to the Creative Commons licence, and indicate \nif changes were made. The images or other third party material in this \narticle are included in the article's Creative Commons licence, unless \nindicated otherwise in a credit line to the material. If material is not \nincluded in the article's Creative Commons licence and your intended \nuse is not permitted by statutory regulation or exceeds the permitted \nuse, you will need to obtain permission directly from the copyright \nholder. To view a copy of this licence, visit http:// creat iveco mmons. \norg/ licen ses/ by/4. 0/.\nReferences\nAdzhubei IA, Schmidt S, Peshkin L, Ramensky VE, Gerasimova A, \nBork P, Kondrashov AS, Sunyaev SR (2010) A method and server \nfor predicting damaging missense mutations. Nat Methods 7:248–\n249. https:// doi. org/ 10. 1038/ nmeth 0410- 248\nAlley EC, Khimulya G, Biswas S, AlQuraishi M, Church GM (2019) \nUnified rational protein engineering with sequence-based deep \nrepresentation learning. Nat Methods 16:1315–1322. https:// doi. \norg/ 10. 1038/ s41592- 019- 0598-1\nAltschul SF, Madden TL, Schaffer AA, Zhang J, Zhang Z, Miller W, \nLipman DJ (1997) Gapped BLAST and PSI-BLAST: a new gen-\neration of protein database search programs. Nucleic Acids Res \n25:3389–3402. https:// doi. org/ 10. 1093/ nar/ 25. 17. 3389\nAlva V, Söding J, Lupas AN (2015) A vocabulary of ancient peptides at \nthe origin of folded proteins. Elife. https:// doi. org/ 10. 7554/ eLife. \n09410\nAmberger JS, Bocchini CA, Scott AF, Hamosh A (2019) OMIM.\norg: leveraging knowledge across phenotype-gene relationships. \nNucleic Acids Res 47:D1038–D1043. https:// doi. org/ 10. 1093/ nar/ \ngky11 51\nAVE Alliance Founding Members (2020) Atlas of Variant Effect \nAlliance.\nBen Chorin A, Masrati G, Kessel A, Narunsky A, Sprinzak J, Lahav \nS, Ashkenazy H, Ben-Tal N (2020) ConSurf-DB: An accessible \nrepository for the evolutionary conservation patterns of the major-\nity of PDB proteins. Protein Sci 29:258–267. https:// doi. org/ 10. \n1002/ pro. 3779\nBen-Tal N, Lupas AN (2021) Editorial overview: Sequences and topol-\nogy: ‘paths from sequence to structure.’ Curr Opin Struct Biol. \nhttps:// doi. org/ 10. 1016/j. sbi. 2021. 05. 005\nBepler T, Berger B (2019a) Learning protein sequence embeddings \nusing information from structure. arXiv. https:// arxiv. org/ abs/  \nastro- ph/ 1902. 08661\nBepler T, Berger B (2019b) Learning protein sequence embeddings \nusing information from structure Seventh International Confer -\nence on Learning Representations\nBepler T, Berger B (2021) Learning the protein language: evolution, \nstructure, and function. Cell Syst 12(654–669):e3. https:// doi. org/ \n10. 1016/j. cels. 2021. 05. 017\nBerezin C, Glaser F, Rosenberg J, Paz I, Pupko T, Fariselli P, Casadio \nR, Ben-Tal N (2004) ConSeq: the identification of functionally \nand structurally important residues in protein sequences. Bioinfor-\nmatics (oxford, England) 20:1322–1324. https:// doi. org/ 10. 1093/ \nbioin forma tics/ bth070\nBerman HM, Westbrook J, Feng Z, Gilliland G, Bhat TN, Weissig H, \nShindyalov IN, Bourne PE (2000) The protein data bank. Nucleic \nAcids Res 28:235–242. https:// doi. org/ 10. 1093/ nar/ 28.1. 235\nBernhofer M, Dallago C, Karl T, Satagopam V, Heinzinger M, Litt -\nmann M, Olenyi T, Qiu J, Schutze K, Yachdav G, Ashkenazy H, \n1645Human Genetics (2022) 141:1629–1647 \n1 3\nBen-Tal N, Bromberg Y, Goldberg T, Kajan L, O’Donoghue S, \nSander C, Schafferhans A, Schlessinger A, Vriend G, Mirdita M, \nGawron P, Gu W, Jarosz Y, Trefois C, Steinegger M, Schneider \nR, Rost B (2021) PredictProtein—predicting protein structure and \nfunction for 29 years. Nucleic Acids Res. https:// doi. org/ 10. 1093/ \nnar/ gkab3 54\nBommasani R, Hudson DA, Adeli E, Altman R, Arora S, von Arx \nS, Bernstein MS, Bohg J, Bosselut A, Brunskill E, Brynjolfsson \nE, Buch S, Card D, Castellon R, Chatterji N, Chen A, Creel K, \nQuincy Davis J, Demszky D, Donahue C, Doumbouya M, Durmus \nE, Ermon S, Etchemendy J, Ethayarajh K, Fei-Fei L, Finn C, Gale \nT, Gillespie L, Goel K, Goodman N, Grossman S, Guha N, Hashi-\nmoto T, Henderson P, Hewitt J, Ho DE, Hong J, Hsu K, Huang J, \nIcard T, Jain S, Jurafsky D, Kalluri P, Karamcheti S, Keeling G, \nKhani F, Khattab O, Kohd PW, Krass M, Krishna R, Kuditipudi \nR, Kumar A, Ladhak F, Lee M, Lee T, Leskovec J, Levent I, Li \nXL, Li X, Ma T, Malik A, Manning CD, Mirchandani S, Mitch-\nell E, Munyikwa Z, Nair S, Narayan A, Narayanan D, Newman \nB, Nie A, Niebles JC, Nilforoshan H, Nyarko J, Ogut G, Orr L, \nPapadimitriou I, Park JS, Piech C, Portelance E, Potts C, Rag-\nhunathan A, Reich R, Ren H, Rong F, Roohani Y, Ruiz C, Ryan \nJ, Ré C, Sadigh D, Sagawa S, Santhanam K, Shih A, Srinivasan \nK, Tamkin A, Taori R, Thomas AW, Tramèr F, Wang RE, Wang \nW, et al. (2021) On the Opportunities and Risks of Foundation \nModels. https:// arxiv. org/ abs/ astro- ph/ 2108. 07258\nBromberg Y, Rost B (2007) SNAP: predict effect of non-synonymous \npolymorphisms on function. Nucleic Acids Res 35:3823–3835\nBromberg Y, Rost B (2008) Comprehensive in silico mutagenesis high-\nlights functionally important residues in proteins. Bioinformatics \n24:i207–i212\nBromberg Y, Rost B (2009) Correlating protein function and stability \nthrough the analysis of single amino acid substitutions. BMC Bio-\ninformatics 10:S8. https:// doi. org/ 10. 1186/ 1471- 2105- 10- s8- s8\nBurley SK, Berman HM, Bhikadiya C, Bi C, Chen L, Di Costanzo L, \nChristie C, Dalenberg K, Duarte JM, Dutta S, Feng Z, Ghosh S, \nGoodsell DS, Green RK, Guranovic V, Guzenko D, Hudson BP, \nKalro T, Liang Y, Lowe R, Namkoong H, Peisach E, Periskova I, \nPrlic A, Randle C, Rose A, Rose P, Sala R, Sekharan M, Shao C, \nTan L, Tao YP, Valasatava Y, Voigt M, Westbrook J, Woo J, Yang \nH, Young J, Zhuravleva M, Zardecki C (2019) RCSB Protein Data \nBank: biological macromolecular structures enabling research and \neducation in fundamental biology, biomedicine, biotechnology \nand energy. Nucleic Acids Res 47:D464–D474. https://  doi. org/ \n10. 1093/ nar/ gky10 04\nCapriotti E, Fariselli P, Casadio R (2005) I-Mutant2.0: predicting sta-\nbility changes upon mutation from the protein sequence or struc-\nture. Nucleic Acids Res 33:W306–W310. https:// doi. org/ 10. 1093/ \nnar/ gki375\nDallago C, Schuetze K, Heinzinger M, Olenyi T, Littmann M, Lu AX, \nYang KK, Min S, Yoon S, Morton JT, Rost B (2021) Learned \nembeddings from deep learning to visualize and predict protein \nsets. Curr Protoc 1:e113. https:// doi. org/ 10. 1002/ cpz1. 113\nDevlin J, Chang M-W, Lee K, Toutanova K (2019) BERT: Pre-training \nof Deep Bidirectional Transformers for Language Understanding. \nhttps:// arxiv. org/ abs/ astro- ph/ 1810. 04805 [cs]\nEfron B, Halloran E, Holmes S (1996) Bootstrap confidence levels \nfor phylogenetic trees. Proc Nat Acad Sci USA 93:13429–13434\nElnaggar A, Heinzinger M, Dallago C, Rehawi G, Wang Y, Jones L, \nGibbs T, Feher T, Angerer C, Steinegger M, Bhowmik D, Rost \nB (2021) ProtTrans: towards cracking the language of life’s code \nthrough self-supervised learning. Mach Intell 14:30\nEsposito D, Weile J, Shendure J, Starita LM, Papenfuss AT, Roth FP, \nFowler DM, Rubin AF (2019) MaveDB: an open-source plat-\nform to distribute and interpret data from multiplexed assays \nof variant effect. Genome Biol 20:223. https:// doi. org/ 10. 1186/ \ns13059- 019- 1845-6\nFariselli P, Martelli PL, Savojardo C, Casadio R (2015) INPS: predict-\ning the impact of non-synonymous variations on protein stability \nfrom sequence. Bioinformatics 31:2816–2821. https:// doi. org/ 10. \n1093/ bioin forma tics/ btv291\nFindlay GM, Daza RM, Martin B, Zhang MD, Leith AP, Gasperini M, \nJanizek JD, Huang X, Starita LM, Shendure J (2018) Accurate \nclassification of BRCA1 variants with saturation genome editing. \nNature 562:217–222. https:// doi. org/ 10. 1038/ s41586- 018- 0461-z\nFowler DM, Fields S (2014) Deep mutational scanning: a new style \nof protein science. Nat Methods 11:801–807. https://  doi. org/ 10. \n1038/ nmeth. 3027\nFu L, Niu B, Zhu Z, Wu S, Li W (2012) CD-HIT: accelerated for \nclustering the next-generation sequencing data. Bioinformatics \n28:3150–3152. https:// doi. org/ 10. 1093/ bioin forma tics/ bts565\nFukushima K (1969) Visual feature extraction by a multilayered net-\nwork of analog threshold elements. IEEE Trans Syst Sci Cybern \n5:322–333. https:// doi. org/ 10. 1109/ TSSC. 1969. 300225\nGray VE, Hause RJ, Luebeck J, Shendure J, Fowler DM (2018) \nQuantitative missense variant effect prediction using large-scale \nmutagenesis data. Cell Syst 6:116-124.e3. https:// doi. org/ 10. \n1016/j. cels. 2017. 11. 003\nGrimm DG, Azencott CA, Aicheler F, Gieraths U, Macarthur DG, \nSamocha KE, Cooper DN, Stenson PD, Daly MJ, Smoller JW, \nDuncan LE, Borgwardt KM (2015) The evaluation of tools used \nto predict the impact of missense variants is hindered by two types \nof circularity. Hum Mutat 36:513–523. https:// doi. org/ 10. 1002/ \nhumu. 22768\nHecht M, Bromberg Y, Rost B (2013) News from the protein mutability \nlandscape. J Mol Biol 425:3937–3948. https:// doi. org/ 10. 1016/j. \njmb. 2013. 07. 028\nHecht M, Bromberg Y, Rost B (2015) Better prediction of functional \neffects for sequence variants. BMC Genomics 16:S1. https:// doi. \norg/ 10. 1186/ 1471- 2164- 16- s8- s1\nHeinzinger M, Elnaggar A, Wang Y, Dallago C, Nechaev D, Matthes \nF, Rost B (2019) Modeling aspects of the language of life through \ntransfer-learning protein sequences. BMC Bioinformatics 20:723. \nhttps:// doi. org/ 10. 1186/ s12859- 019- 3220-8\nHenikoff S, Henikoff JG (1992) Amino acid substitution matrices from \nprotein blocks. Proc Natl Acad Sci 89:10915–10919. https:// doi. \norg/ 10. 1073/ pnas. 89. 22. 10915\nHopf TA, Ingraham JB, Poelwijk FJ, Scharfe CP, Springer M, Sander \nC, Marks DS (2017) Mutation effects predicted from sequence \nco-variation. Nat Biotechnol 35:128–135. https:// doi. org/ 10. 1038/ \nnbt. 3769\nJumper J, Evans R, Pritzel A, Green T, Figurnov M, Ronneberger O, \nTunyasuvunakool K, Bates R, Zidek A, Potapenko A, Bridgland \nA, Meyer C, Kohl SAA, Ballard AJ, Cowie A, Romera-Paredes \nB, Nikolov S, Jain R, Adler J, Back T, Petersen S, Reiman D, \nClancy E, Zielinski M, Steinegger M, Pacholska M, Bergham-\nmer T, Bodenstein S, Silver D, Vinyals O, Senior AW, Kavukc-\nuoglu K, Kohli P, Hassabis D (2021) Highly accurate protein \nstructure prediction with AlphaFold. Nature. https:// doi. org/ 10. \n1038/ s41586- 021- 03819-2\nKatoh K, Standley DM (2013) MAFFT multiple sequence align-\nment software version 7: improvements in performance and \nusability. Mol Biol Evol 30:772–780. https://  doi. org/ 10. 1093/ \nmolbev/ mst010\nKatsonis P, Lichtarge O (2014) A formal perturbation equation \nbetween genotype and phenotype determines the Evolutionary \nAction of protein-coding variations on fitness. Genome Res \n24:2050–2058. https:// doi. org/ 10. 1101/ gr. 176214. 114\nKawabata T, Ota M, Nishikawa K (1999) The protein mutant data-\nbase. Nucleic Acids Res 27:355–357. https:// doi. org/ 10. 1093/  \nnar/ 27.1. 355\nKingma DP, Ba J (2014) Adam: a method for stochastic optimization. \npp https:// arxiv. org/ abs/ astro- ph/ 1412. 6980\n1646 Human Genetics (2022) 141:1629–1647\n1 3\nKircher M, Witten DM, Jain P, O’Roak BJ, Cooper GM, Shendure \nJ (2014) A general framework for estimating the relative path-\nogenicity of human genetic variants. Nat Genet 46:310–315. \nhttps:// doi. org/ 10. 1038/ ng. 2892\nKolodny R (2021) Searching protein space for ancient sub-domain \nsegments. Curr Opin Struct Biol 68:105–112. https:// doi. org/  \n10. 1016/j. sbi. 2020. 11. 006\nKorber B, Fischer WM, Gnanakaran S, Yoon H, Theiler J, Abfalterer \nW, Hengartner N, Giorgi EE, Bhattacharya T, Foley B, Hastie \nKM, Parker MD, Partridge DG, Evans CM, Freeman TM, de \nSilva TI, Angyal A, Brown RL, Carrilero L, Green LR, Groves \nDC, Johnson KJ, Keeley AJ, Lindsey BB, Parsons PJ, Raza M, \nRowland-Jones S, Smith N, Tucker RM, Wang D, Wyles MD, \nMcDanal C, Perez LG, Tang H, Moon-Walker A, Whelan SP, \nLaBranche CC, Saphire EO, Montefiori DC (2020) Tracking \nchanges in SARS-CoV-2 spike: evidence that D614G increases \ninfectivity of the COVID-19 virus. Cell 182:812-827.e19. \nhttps:// doi. org/ 10. 1016/j. cell. 2020. 06. 043\nLaha S, Chakraborty J, Das S, Manna SK, Biswas S, Chatterjee R \n(2020) Characterizations of SARS-CoV-2 mutational profile, \nspike protein stability and viral transmission. Infect Genet Evol \n85:104445. https:// doi. org/ 10. 1016/j. meegid. 2020. 104445\nLaine E, Karami Y, Carbone A (2019) GEMME: a simple and fast \nglobal epistatic model predicting mutational effects. Mol Biol \nEvol. https:// doi. org/ 10. 1093/ molbev/ msz179\nLittmann M, Bordin N, Heinzinger M, Schütze K, Dallago C, Orengo \nC, Rost B (2021a) Clustering funFams using sequence embed-\ndings improves EC purity. Bioinformatics. https://  doi. org/ 10. \n1093/ bioin forma tics/ btab3 71\nLittmann M, Heinzinger M, Dallago C, Olenyi T, Rost B (2021b) \nEmbeddings from deep learning transfer GO annotations \nbeyond homology. Sci Rep 11:1160. https:// doi. org/ 10. 1038/  \ns41598- 020- 80786-0\nLittmann M, Heinzinger M, Dallago C, Weissenow K, Rost B \n(2021c) Protein embeddings and deep learning predict binding \nresidues for various ligand classes. bioRxiv. https:// doi. org/ 10. \n1101/ 2021. 09. 03. 458869\nLiu J, Rost B (2003) Domains, motifs, and clusters in the protein \nuniverse. Curr Opin Chem Biol 7:5–11\nLiu J, Rost B (2004a) CHOP proteins into structural domain-like \nfragments. Proteins: structure. Funct Bioinf 55:678–688\nLiu J, Rost B (2004b) Sequence-based prediction of protein domains. \nNucleic Acids Res 32:3522–3530\nLivesey BJ, Marsh JA (2020) Using deep mutational scanning to \nbenchmark variant effect predictors and identify disease muta-\ntions. Mol Syst Biol 16:e9380. https:// doi. org/ 10. 15252/ msb.  \n20199 380\nMadani A, McCann B, Naik N, Shirish Keskar N, Anand N, Eguchi \nRR, Huang P, Socher R (2020) ProGen: language modeling for \nprotein generation. arXiv 16:1315\nMajithia AR, Tsuda B, Agostini M, Gnanapradeepan K, Rice R, \nPeloso G, Patel KA, Zhang X, Broekema MF, Patterson N, \nDuby M, Sharpe T, Kalkhoven E, Rosen ED, Barroso I, Ellard \nS, UKMD Consortium, Kathiresan S, Myocardial Infarction \nGenetics, O’Rahilly S, UKCL Consortiun, Chatterjee K, Flo-\nrez JC, Mikkelsen T, Savage DB, Altshuler D (2016) Prospec-\ntive functional classification of all possible missense variants \nin PPARG. Nat Genet 48:1570–1575. https:// doi. org/ 10. 1038/  \nng. 3700\nMatreyek KA, Starita LM, Stephany JJ, Martin B, Chiasson MA, \nGray VE, Kircher M, Khechaduri A, Dines JN, Hause RJ, Bhatia \nS, Evans WE, Relling MV, Yang W, Shendure J, Fowler DM \n(2018) Multiplex assessment of protein variant abundance by \nmassively parallel sequencing. Nat Genet 50:874–882. https://  \ndoi. org/ 10. 1038/ s41588- 018- 0122-z\nMeier J, Rao R, Verkuil R, Liu J, Sercu T, Rives A (2021) Language \nmodels enable zero-shot prediction of the effects of mutations \non protein function. bioRxiv. https:// doi. org/ 10. 1101/ 2021. 07. \n09. 450648\nMercatelli D, Giorgi FM (2020) Geographic and genomic distribu-\ntion of SARS-CoV-2 mutations. Front Microbiol. https:// doi.  \norg/ 10. 3389/ fmicb. 2020. 01800\nMiller M, Bromberg Y, Swint-Kruse L (2017) Computational predic-\ntors fail to identify amino acid substitution effects at rheostat \npositions. Sci Rep 7:41329. https:// doi. org/ 10. 1038/ srep4 1329\nMistry J, Finn RD, Eddy SR, Bateman A, Punta M (2013) Challenges \nin homology search: HMMER3 and convergent evolution of \ncoiled-coil regions. Nucleic Acids Res 41:e121. https:// doi. org/  \n10. 1093/ nar/ gkt263\nNg PC, Henikoff S (2003) SIFT: predicting amino acid changes that \naffect protein function. Nucleic Acids Res 31:3812–3814\nNiroula A, Urolagin S, Vihinen M (2015) PON-P2: prediction \nmethod for fast and reliable identification of harmful variants. \nPLoS ONE 10:e0117380. https:// doi. org/ 10. 1371/ journ al. pone. \n01173 80\nNishikawa K, Ishino S, Takenaka H, Norioka N, Hirai T, Yao T, Seto \nY (1994) Constructing a protein mutant database. Protein Eng \n7:733. https:// doi. org/ 10. 1093/ prote in/7. 5. 733\nO’Donoghue SI, Schafferhans A, Sikta N, Stolte C, Kaur S, Ho BK, \nAnderson S, Procter J, Dallago C, Bordin N, Adcock M, Rost \nB (2020) SARS-CoV-2 structural coverage map reveals state \nchanges that disrupt host immunity. bioRxiv. https:// doi. org/ 10. \n1101/ 2020. 07. 16. 207308\nOfer D, Brandes N, Linial M (2021) The language of proteins: NLP, \nmachine learning and protein sequences. Comput Struct Bio-\ntechnol J 19:1750–1758. https:// doi. org/ 10. 1016/j. csbj. 2021.  \n03. 022\nPedregosa F, Varoquaux G, Gramfort A, Michel V, Thirion B, Grisel \nO, Blondel M, Prettenhofer P, Weiss R, Dubourg V, Vanderplas \nJ, Passos A, Cournapeau D, Brucher M, Perrot M, Duchesnay É \n(2011) Scikit-learn: machine learning in python. J Mach Learn \nRes 12:2825–2830\nRadford A, Wu J, Child R, Luan D, Amodei D, Sutskever I (2019) \nLanguage Models are Unsupervised Multitask Learners. 24.\nRaffel C, Shazeer N, Roberts A, Lee K, Narang S, Matena M, Zhou \nY, Li W, Liu PJ (2020) Exploring the limits of transfer learn-\ning with a unified text-to-text transformer. https:// arxiv. org/ abs/ \nastro- ph/ 1910. 10683[cs, stat].\nRamensky V, Bork P, Sunyaev S (2002) Human non-synonymous \nSNPs: server and survey. Nucleic Acids Res 30:3894–3900\nRao R, Meier J, Sercu T, Ovchinnikov S, Rives A (2020) Trans-\nformer protein language models are unsupervised structure \nlearners. bioRxiv. https:// doi. org/ 10. 1101/ 2020. 12. 15. 422761\nReeb J (2020) Data for: Variant effect predictions capture some \naspects of deep mutational scanning experiments. 1. doi: https://  \ndoi. org/ 10. 17632/ 2rwrk p7mfk.1\nReeb J, Hecht M, Mahlich Y, Bromberg Y, Rost B (2016) Predicted \nmolecular effects of sequence variants link to system level of \ndisease. PLoS Comput Biol 12:e1005047. https://  doi. org/ 10. \n1371/ journ al. pcbi. 10050 47\nReeb J, Wirth T, Rost B (2020) Variant effect predictions capture \nsome aspects of deep mutational scanning experiments. BMC \nBioinf 21:107. https:// doi. org/ 10. 1186/ s12859- 020- 3439-4\nRiesselman AJ, Ingraham JB, Marks DS (2018) Deep genera-\ntive models of genetic variation capture the effects of muta-\ntions. Nat Methods 15:816–822. https://  doi.  org/  10. 1038/  \ns41592- 018- 0138-4\nRives A, Meier J, Sercu T, Goyal S, Lin Z, Liu J, Guo D, Ott M, Zit-\nnick CL, Ma J, Fergus R (2021) Biological structure and func-\ntion emerge from scaling unsupervised learning to 250 million \n1647Human Genetics (2022) 141:1629–1647 \n1 3\nprotein sequences. Proc Natl Acad Sci. https:// doi. org/ 10. 1073/ \npnas. 20162 39118\nRost B (1996) PHD: predicting one-dimensional protein structure by \nprofile based neural networks. Methods Enzymol 266:525–539\nRost B, Sander C (1992) Jury returns on structure prediction. Nature \n360:540\nRost B, Sander C (1993) Prediction of protein secondary structure \nat better than 70% accuracy. J Mol Biol 232:584–599. https://  \ndoi. org/ 10. 1006/ jmbi. 1993. 1413\nSchelling M, Hopf TA, Rost B (2018) Evolutionary couplings and \nsequence variation effect predict protein binding sites. Proteins \n86:1064–1074. https:// doi. org/ 10. 1002/ prot. 25585\nSchwarz JM, Rodelsperger C, Schuelke M, Seelow D (2010) Muta-\ntionTaster evaluates disease-causing potential of sequence \nalterations. Nat Methods 7:575–576. https:// doi. org/ 10. 1038/  \nnmeth 0810- 575\nSim N-L, Kumar P, Hu J, Henikoff S, Schneider G, Ng PC (2012) \nSIFT web server: predicting effects of amino acid substitutions \non proteins. Nucleic Acids Res 40:W452–W457. https:// doi.  \norg/ 10. 1093/ nar/ gks539\nSruthi CK, Balaram H, Prakash MK (2020) Toward developing intui-\ntive rules for protein variant effect prediction using deep muta-\ntional scanning data. ACS Omega 5:29667–29677. https:// doi.  \norg/ 10. 1021/ acsom ega. 0c024 02\nStärk H, Dallago C, Heinzinger M, Rost B (2021) Light attention \npredicts protein location from the language of life. bioRxiv. \nhttps:// doi. org/ 10. 1101/ 2021. 04. 25. 441334\nSteinegger M, Söding J (2017) MMseqs2 enables sensitive protein \nsequence searching for the analysis of massive data sets. Nat \nBiotechnol 35:1026\nSteinegger M, Söding J (2018) Clustering huge protein sequence sets \nin linear time. Nat Commun 9:2542. https:// doi. org/ 10. 1038/  \ns41467- 018- 04964-5\nStuder RA, Dessailly BH, Orengo CA (2013) Residue mutations \nand their impact on protein structure and function: detecting \nbeneficial and pathogenic changes. Biochem J 449:581–594. \nhttps:// doi. org/ 10. 1042/ BJ201 21221\nSutskever I, Vinyals O, Le QV (2014) Sequence to sequence learn-\ning with neural networks. Proceedings of the 27th International \nConference on Neural Information Processing Systems—Vol-\nume 2. MIT Press, Montreal, Canada, pp 3104–3112\nThe UniProt Consortium (2021) UniProt: the universal protein \nknowledgebase in 2021. Nucleic Acids Res 49:D480–D489. \nhttps:// doi. org/ 10. 1093/ nar/ gkaa1 100\nWang G, Dunbrack RL Jr (2003) PISCES: a protein sequence culling \nserver. Bioinformatics 19:1589–1591. https:// doi. org/ 10. 1093/ \nbioin forma tics/ btg224\nWang Z, Moult J (2001) SNPs, protein structure, and disease. Hum \nMutat 17:263–270. https:// doi. org/ 10. 1002/ humu. 22\nWeile J, Roth FP (2018) Multiplexed assays of variant effects con-\ntribute to a growing genotype–phenotype atlas. Hum Genet \n137:665–678. https:// doi. org/ 10. 1007/ s00439- 018- 1916-x\nWeißenow K, Heinzinger M, Rost B (2021) Protein language model \nembeddings for fast, accurate, alignment-free protein structure \nprediction. bioRxiv. https:// doi. org/ 10. 1101/ 2021. 07. 31. 454572\nZhou G, Chen M, Ju CJT, Wang Z, Jiang JY, Wang W (2020) Muta-\ntion effect estimation on protein-protein interactions using deep \ncontextualized representation learning. NAR Genom Bioinform. \nhttps:// doi. org/ 10. 1093/ nargab/ lqaa0 15\nPublisher's Note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations."
}