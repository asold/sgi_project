{
    "title": "Deep guided transformer dehazing network",
    "url": "https://openalex.org/W4386768037",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2140798433",
            "name": "Shengdong Zhang",
            "affiliations": [
                "Shaoxing University",
                "Wenzhou University"
            ]
        },
        {
            "id": "https://openalex.org/A2100400154",
            "name": "Liping Zhao",
            "affiliations": [
                "Wenzhou University",
                "Shaoxing University"
            ]
        },
        {
            "id": "https://openalex.org/A2132924169",
            "name": "Keli Hu",
            "affiliations": [
                "Shaoxing University"
            ]
        },
        {
            "id": "https://openalex.org/A2016595786",
            "name": "Sheng Feng",
            "affiliations": [
                "Shaoxing University"
            ]
        },
        {
            "id": "https://openalex.org/A2118056788",
            "name": "En Fan",
            "affiliations": [
                "Shaoxing University"
            ]
        },
        {
            "id": "https://openalex.org/A2117608616",
            "name": "Li Zhao",
            "affiliations": [
                "Wenzhou University",
                "Shaoxing University"
            ]
        },
        {
            "id": "https://openalex.org/A2140798433",
            "name": "Shengdong Zhang",
            "affiliations": [
                "Wenzhou University",
                "Shaoxing People's Hospital",
                "Shaoxing University"
            ]
        },
        {
            "id": "https://openalex.org/A2100400154",
            "name": "Liping Zhao",
            "affiliations": [
                "Shaoxing People's Hospital",
                "Wenzhou University",
                "Shaoxing University"
            ]
        },
        {
            "id": "https://openalex.org/A2132924169",
            "name": "Keli Hu",
            "affiliations": [
                "Shaoxing People's Hospital",
                "Shaoxing University"
            ]
        },
        {
            "id": "https://openalex.org/A2016595786",
            "name": "Sheng Feng",
            "affiliations": [
                "Shaoxing People's Hospital",
                "Shaoxing University"
            ]
        },
        {
            "id": "https://openalex.org/A2118056788",
            "name": "En Fan",
            "affiliations": [
                "Shaoxing University",
                "Shaoxing People's Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2117608616",
            "name": "Li Zhao",
            "affiliations": [
                "Wenzhou University",
                "Shaoxing University",
                "Shaoxing People's Hospital"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2128254161",
        "https://openalex.org/W2028763589",
        "https://openalex.org/W2147318913",
        "https://openalex.org/W2467473805",
        "https://openalex.org/W2156936307",
        "https://openalex.org/W2028990532",
        "https://openalex.org/W3151922143",
        "https://openalex.org/W2519481857",
        "https://openalex.org/W2256362396",
        "https://openalex.org/W2962754725",
        "https://openalex.org/W4247924304",
        "https://openalex.org/W4312373367",
        "https://openalex.org/W2963306157",
        "https://openalex.org/W2948892662",
        "https://openalex.org/W3180034634",
        "https://openalex.org/W2948606054",
        "https://openalex.org/W3034331889",
        "https://openalex.org/W2990007814",
        "https://openalex.org/W3034278302",
        "https://openalex.org/W2963751414",
        "https://openalex.org/W4295934483",
        "https://openalex.org/W4312812783",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W4312617404",
        "https://openalex.org/W3107316803",
        "https://openalex.org/W2866634454",
        "https://openalex.org/W2963446712",
        "https://openalex.org/W2740982616",
        "https://openalex.org/W2976715267",
        "https://openalex.org/W4205261430",
        "https://openalex.org/W2985030998",
        "https://openalex.org/W3002421716",
        "https://openalex.org/W2955731188",
        "https://openalex.org/W3173269149",
        "https://openalex.org/W6795278077",
        "https://openalex.org/W3213467918",
        "https://openalex.org/W4317935142",
        "https://openalex.org/W4313026854",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W6778485988",
        "https://openalex.org/W3170544306",
        "https://openalex.org/W4379768879",
        "https://openalex.org/W4225672218",
        "https://openalex.org/W2160956336",
        "https://openalex.org/W6750330310",
        "https://openalex.org/W3035601380",
        "https://openalex.org/W2798876216",
        "https://openalex.org/W1552073401",
        "https://openalex.org/W4383752898",
        "https://openalex.org/W4380880261",
        "https://openalex.org/W4321033361",
        "https://openalex.org/W4319342106",
        "https://openalex.org/W4320081372",
        "https://openalex.org/W2964212750",
        "https://openalex.org/W4200281992",
        "https://openalex.org/W4225986616",
        "https://openalex.org/W4225985345",
        "https://openalex.org/W3159236515",
        "https://openalex.org/W2963928582",
        "https://openalex.org/W4205276083"
    ],
    "abstract": null,
    "full_text": "1\nVol.:(0123456789)Scientific Reports |        (2023) 13:15333  | https://doi.org/10.1038/s41598-023-41561-z\nwww.nature.com/scientificreports\nDeep guided transformer dehazing \nnetwork\nShengdong Zhang 1,2, Liping Zhao 2, Keli Hu 2, Sheng Feng 2, En Fan 2 & Li Zhao 1*\nSingle image dehazing has received a lot of concern and achieved great success with the help of deep-\nlearning models. Yet, the performance is limited by the local limitation of convolution. To address \nsuch a limitation, we design a novel deep learning dehazing model by combining the transformer and \nguided filter, which is called as Deep Guided Transformer Dehazing Network. Specially, we address the \nlimitation of convolution via a transformer-based subnetwork, which can capture long dependency. \nHaze is dependent on the depth, which needs global information to compute the density of haze, and \nremoves haze from the input images correctly. To restore the details of dehazed result, we proposed \na CNN sub-network to capture the local information. To overcome the slow speed of the transformer-\nbased subnetwork, we improve the dehazing speed via a guided filter. Extensive experimental results \nshow consistent improvement over the state-of-the-art dehazing on natural haze and simulated haze \nimages.\nImage dehazing is a hot topic in classic computer vision, whose goal is to restore a clean image from the input. \nThe quality of the captured image is affected by the air particle, which absorbs the ray emitted from objects and \nreflects other light into the camera. We can describe the hazing process as:\nwhere I is the input hazy image, and J is the corresponding clean image, t represents how much the light reflected \nfrom objects is received by the camera, A is the air-light.\nIn the traditional image dehazing, computing the transmission map and air-light is a highly ill-posed prob -\nlem if there is no extra information available. To address dehazing problem, a lot of dehazing methods are \ndesigned based on the various types of  priors1–6 or additional  information7–9. Requiring additional information \nrestricts the application scope of these methods. The priors used for dehazing maybe fail in some cases, such as \nimages containing white objects or the sky. To boost the robustness of dehazing methods, deep learning-based \n methods10,11 are introduced to predict the transmission map. But the dehazing performance of these methods \nis influenced by the precision of the estimated transmission map. To overcome this problem, some End-to-End \ndeep learning dehazing  methods12–20 are proposed. Li et al. fuse the transmission map and airlight into a new \nparameter and design a low-time consumption dehazing method. Qu et al. designed a dehazing method, which \ntransfers the dehazing problem into a transferring problem. Liu et al.employ the attention mechanism and \nmulti-scale network to boost dehazing performance. Dong et al. employ boosting strategy and dense features \n fusion21 to design a dehazing network. Zhang et al. propose a transmission map guided dehazing  network22. Song \net al. propose a wavelet-based dehazing  method23. Although these methods have their great power in dehazing, \nwe note the performance can be further boosted by introducing a model which can capture long dependency.\nCNN has shown its effectiveness in low-level computer vision tasks, while transformers have shown great \npower ability for high-level computer vision tasks. Recently some works although introduce it into low-level \ncomputer vision  tasks24. The prior  work25 introduced a transformer into a computer vision task and achieved \nan impressive performance, which shows the potential of computer vision tasks. However, the computational \nburden of the transformer is very high, which limits its application of transformer. To boost the dehazing per -\nformance,  Dehamer26 propose a transformer-based module to estimate the density of haze, and then combine \nit with CNN features to obtain the final dehazed results. However, Dehamer ignores the information in hazy \nimages, and cannot achieve a high dehazing for dense hazy images. Furthermore, Dehamer inherits the problem \nof the transformer, which has a high time complexity. Zhao et al. propose a Pyramid dehazing  network27, which \ncan extract large contextual information. However, this work also inherits the limitation of CNN. The proposed \n(1)I(x) = t(x)J(x) + (1 − t(x))A,\nOPEN\n1Key Laboratory of Intelligent Informatics for Safety and Emergency of Zhejiang Province, Wenzhou University, \nEducation Park Zone, Wenzhou City 325035, Zhejiang Province, People’s Republic of China. 2Department of \nComputer Science and Engineering, Shaoxing University, Yuecheng District, Shaoxing City 312000, Zhejiang \nProvince, People’s Republic of China. *email: lizhao@wzu.edu.cn\n2\nVol:.(1234567890)Scientific Reports |        (2023) 13:15333  | https://doi.org/10.1038/s41598-023-41561-z\nwww.nature.com/scientificreports/\nmodel extracts the large contextual information using a transformer and reduces the consumption time using \nthe deep guided filter.\nTo address this issue, we propose a novel highly efficient dehazing method based on the transformer and \nguided filter, which is called Deep Guided Transformer Dehazing Network (DGTDN). Haze is depend on the \ndistance between the camera and the objects, which results in the haze density being different from pixel to \npixel. The distribution of haze is global, which is hard for CNN to capture the long distance. To capture the long \ndependency, we design a transformer-based model to capture the global information of haze. However, the \ntransformer cannot capture the local information well. To deal with this case, we propose a lightweight CNN \nsub-network to capture the local information. Based on the advance of the transformer and CNN, we propose to \nrestore the global haze-free image with the transformer and then refine the details with the CNN sub-network. To \nachieve the goal of further improving the dehazing speed, we introduce the guided filter to reduce the dehazing \ntime. The contributions of the DGTDN can be summered as follows: \n1. We introduce transformer-based sub-network to restore the coarse haze-free image and then the details are \nrefined via a CNN-based sub-network. Restoring the coarse haze-free image depends on global informa-\ntion while refining the details needs more local information, which encourages us to design such a dehazing \nmodel using CNN and a transformer.\n2. We introduce the guided filter to improve the dehazing speed. Transformer is time-consuming, which may \nlimit the application of transformer-based dehazing methods. We address this issue by introducing the guided \nfilter into the proposed model. We reduce the input size of the transformer, which reduces the execution \ntime of the transformer-based model.\n3. We do extensive experiments to show the superiority of the proposed method on natural hazy images and \nsimulated hazy images. We also conduct ablation studies to show the effectiveness of the proposed modules.\nRelated work\nWe show some previous works related to dehazing. In this paper, we divide the related dehazing works into two \ngroups, which include learning-based and prior-based methods.\nLearning-based dehazing methods. The CNN-based methods have swept the computer vision  tasks28–30. \nWith the development of CNNs, a lot of  works10–15,17,31–36 attempt to solve dehazing using deep learning models. \nThese dehazing methods often attempt to compute the key factor of the physical model or the corresponding \nhaze-free image directly. The  works10,11 employ CNN model to compute the transmission map. However, these \nmethods may boost the error of the transmission map and result in poor dehazing results. To deal this problem, \nEnd-to-End dehazing  methods12–15,17,31–34,37–40 are proposed. For example, Zhang et al. design a CNN model that \nincorporates the physical model. Li et al. propose an all-in-one dehazing  model13, which fuses the transmis-\nsion map and airlight into a new parameter. Liu et al. design a novel dehazing  model20 based on attention and \nmulti-scale network. However, all these dehazing methods are based on CNNs, which are limited by the local \nproperty of convolution. To capture the long dependency of hazy images, Guo et al. propose a transformer-based \ndehazing  method26, which employs the transformer-based encoder to capture the density of haze. Different from \nthe above-mentioned methods, we overcome the problem of CNN by introducing the transformer block into \nthe dehazing model, which can capture the long-range dependency. Some works note the difference between \nthe simulated hazy and real hazy images, which results in a drop of dehazing performance on real hazy images \nwhen the model is trained with simulated hazy images. To address these issues,  PSD17 proposes to combine the \ntraditional priors to improve the dehazing quality of real hazy images. Domain adaptation dehazing method \n(DA)19 improves the dehazing quality on real hazy images by converting simulated hazy images into real hazy \nimages. We note that these methods are hard to train. Furthermore, the proposed method focuses on improving \nthe learning ability on simulated hazy images, which has a different goal from PSD and DA.\nPrior-based dehazing methods. To address the ill-posed of single image dehazing, a lot of prior-based \ndehazing  methods1–6 or additional  information7–9 has been proposed. These methods discover the prior based \non the statistical analysis of clean images or hazy ones. The famous work is Dark Channel Prior (DCP), which is \nderived from the observation that a clean image patch contains at least one pixel that has a channel value close to \nzero. Zhu et al. discover a color attenuation  prior5, which is that the divergence between intensity and saturation \npositively is correlated to the depth. Fattal et al. 2 use a color-line prior to removing haze. Berman et al. find a \nhaze-line  prior4 based on the observation that one haze-free image can be presented by a small number of color \nclusters. However, all these priors are simple, and cannot be held in real word complex scenes.\nTransformer for vision tasks. Natural language processing (NLP) has applied  Transformer41 to capture \nlong dependency and improved the performance of learned models. Transformer shows its effectiveness in \nNLP and image classification  task25 also employs Transformer to improve the performance. With the success of \nVision Transformer (ViT)25 and its follow-ups42,43, researchers have shown the potential of transformers to image \n segmentation43 and object  detection42. Although visual transformers have shown their success in visual tasks, \nit is hard to directly apply it in single image dehazing. First, Transformers often depend on large-scale datasets. \nHowever, there is no existing large-scale dataset to train a transformer-based for image dehazing. Second, it is \nhard to capture local representation for transformers, which may result in the loss of image details. To overcome \nthis issue, we proposed combining the advantage of CNN and transformer to capture the local texture and global \nstructure jointly to boost the dehazing quality.\n3\nVol.:(0123456789)Scientific Reports |        (2023) 13:15333  | https://doi.org/10.1038/s41598-023-41561-z\nwww.nature.com/scientificreports/\nMethodology\nIn this section, we explain the motivation behind Deep Guided Transformer Dehazing Network (DGTDN) and \nthen show the details of DGTDN. The structure of the proposed DGTDN is shown in Fig.  1, which consists \nof three parts. The first part is BaseNet, which is used to estimate the baselayer of the low-resolution dehazed \nresult. The second part is DetailNet, which is used to estimate the missed details of base layer. The low-resolution \ndehazed result is generated by adding the base layer to the detail layer. The third part is GuidedFilerNet, which \nobtains the final high-quality dehazed result by upsamping the low-resolution dehazed result.\nMotivation. The thickness of the haze is dependent on the depth of the objects, which results in the distri-\nbution of haze is global information. Based on the fact that the dehazing task needs to restore the image details, \nwhich is dependent on the local features. Single image dehazing is dependent on global and local  features44. The \ntransformer has shown its ability to capture long-range dependency, which is critical to improve the dehazing \nquality. However, the transformer cannot capture the local feature details which leads to coarse details for dehaz-\ning. According to the prior  works45, CNN can provide local connections and capture local features. It is known to \nall that transformer-based methods are time-consuming. To reduce the inference time, we propose to introduce \nthe deep guided filter into the dehazing network. Based on the above analysis, we combine the advantages of \nCNN, transformer, and deep guided filter to boost the dehazing quality and reduce the running time. In this \npaper, we propose a Deep Guided Transformer Dehazing Network (DGTDN). DGTDN consists of BaseNet, \nDetailNet, and GuidedFilerNet. BaseNet is designed to capture long-rang dependency and restore the coarse \nhaze-free image. DetailNet is designed to capture the local features and restore the image details. GuidedFilter-\nNet is designed to enlarge the low-resolution dehazed result and reduce the dehazing time.\nThe structure of the proposed model. Based on the motivation in subsection  3.1, we introduce the \nCNN, transformer, and guided filter into the proposed dehazing network. As shown in Fig.  1, we propose a \nmodel containing three parts: BaseNet, DetailNet, and GuidedFilerNet. We enlarge the details of haze remove \nnetwork, which consists of BaseNet and DetailNet. As shown, the proposed model process a hazy image and \noutputs a high-resolution dehazed result via series steps: (1) Downsampling the input hazy image via bilinear \ndownsampling, and obtaining a low-resolution haze image, we mark it as LI; (2) Feeding the LI into haze remove \nnetwork, and obtaining a low-resolution dehazed result, we mark it as LO; (3) Feeding the LI, input hazy image, \nand LO into the GuidedFilterNet, and obtain the final high-resolution deazed result. Next, we introduce the \nBaseNet, DetailNet, and GuidedFilterNet in detail.\nBaseNet. The BaseNet consists of an encoder that extracts features and a decoder that restores the haze-free \nimage. The encoder contains four stages, and the decoder also contains four stages. Specifically, each encoder \nFigure 1.  The rough structure of the Deep Guided Transformer Dehazing Network. The proposed network \ncontains main three parts: BaseNet, DetailNet, and GuidedFilterNet. Swin represents the Swin-Block, which is \nused to enlarge the receptive field of the proposed model. Bilinear represents the bilinear downsampling. LI is \nthe output of the bilinear downsampling. LO represents the output of the haze remove network, which is a low-\nresolution dehazing result.\n4\nVol:.(1234567890)Scientific Reports |        (2023) 13:15333  | https://doi.org/10.1038/s41598-023-41561-z\nwww.nature.com/scientificreports/\nstage contains one transformer block, which followed one down-sampling layer. Similar to the encoder stage, \neach decoder stage contains one transformer block, which is followed by one up-sampling layer. The down-\nsampling layer is designed to downscale the size of feature maps, which is implemented by 3 × 3 convolution \nwith stride 2. The up-sampling layer is designed to enlarge the size of feature maps, which is implemented by \n2 × 2 transposed convolution operation with stride 2. The input of the BaseNet is a low-resolution version of a \nhazy image. The low-resolution hazy image is generated by using a bilinear, which is used to obtain a hazy image \nwith half the size of the original input. We define the output of BaseNet as follows:\nwhere BaseNet is the BaseNet, the Il is the low-resolution of input hazy image, ˆB is the base layer of a dehazed \nresult.\nDetailNet. The DetailNet is designed to restore missed details. The DetailNet contains four Residual Dilation \nBlocks (RDBs), whose structure is shown in Fig. 2. Each RDB contains two common convolution layers and two \ndilation convolution layers. We pass the low-resolution input hazy into the DetailNet and obtain the detail layer.\nwhere DetailNet is the DetailNet, ˆD is the detail layer of a dehazed result.\nAfter obtaining the structure layer of dehazed result and the image detail layer, we can obtain the dehazed \nresult as follow:\nwhere ˆH l represents the predicted low-resolution haze-free image.\nGuidedFilterNet. GuidedFilterNet is based on the guided filter, which is based on the local linear model. We \ncan express the local linear model as:\nwhere qo is the output, Ig  is the guidance image, l is the location in Ig  , ω is a local window in Ig  with radius r , \n(Aω, Bω) are the linear const coefficients in a local window. This model can preserve the edges in qo if Ig  has the \nedges, because that ∇qo =∇ Ig . To obtain the (Aω and Bω) , we solve the problem (5) that reduces the difference \nbetween the output qo and the filtering input p. To solve the problem (5), we minimizes the error:\nwhere ǫ is used to penalize large Aω , p is the filtering input.\nWe employ guided filter to perform joint upsampling, which receives a low-resolution hazy image, the corre-\nsponding low-resolution dehazed result, and the original hazy image as input, obtaining the final high-resolution \ndehazed result. Based on the local linear model, the relation between a low-resolution hazy image and the cor -\nresponding low-resolution haze-free image can be expressed:\nwhere H l is the low-resolution dehazed result and Il is the low-resolution hazy image, i is the index of the Il . To \nobtain A l\nω and B l\nω , we reduce the error between ˆH l and the H l:\nAfter obtaining A l\nω and B l\nω , we simple the Eq. (7) to :\nwhere .∗ is element-wise multiplication. Based on the local linear model, we also can express the relation between \na high-resolution hazy image and the corresponding haze-free image as:\n(2)ˆB = BaseNet (Il),\n(3)ˆD = DetailNet(Il),\n(4)ˆH l = ˆB + ˆD ,\n(5)qo(l) = A ωIg (l) + B ω, ∀l∈ ω,\n(6)E (A ω,Bω) =\n∑\nl∈ω\n((A ωIg(l) + Bω − p(l))2 + ǫA 2\nω),\n(7)H l(i) = A l\nωIl(i) + Bl\nω,\n(8)E(A l\nω,Bl\nω) =\n∑\nl∈ω\n((A l\nωIl + Bl\nω − ˆH l)2 + ǫ(A l\nω)2 ),\n(9)H l = A l. ∗ Il + Bl,\nFigure 2.  The structure of the Residual Dilation Block (RDB).\n5\nVol.:(0123456789)Scientific Reports |        (2023) 13:15333  | https://doi.org/10.1038/s41598-023-41561-z\nwww.nature.com/scientificreports/\nBased on Eq. (10) and  (9 ), we can construct the relation between the high-resolution and the low-resolution \nhazy images. According  to46, we can obtain the high-resolution Ah and Bh via bilinearly upsample:\nAlgorithm  1 lists the main steps of the guided filter in DGTDN. U  is a bilinearly upsample operation, Box  \nrepresents the box filtering. As shown in Fig.  1, GuidedFilterNet receives the output of haze remove network \nas input and enlarges the low-resolution dehazed result according to the original hazy image. In the proposed \nmodel, GuidedFilterNet interacts with haze remove network and bilinear downsampling, and performs a joint \nupsampling function. GuidedFilterNet is designed to enlarge the dehazed result and reduce the dehazing time \nof the proposed model. \nLoss functions. Loss functions are critical to obtaining high quality dehazing results. The proposed method \ncan obtain two-scale dehazed results. To utilize this useful information, we propose a multi-scale content loss \nfunction:\nwhere N denotes the number of training samples, �·� 1 denotes L1 norm, Hh is the ground-truth haze-free image, \nand H l is the low-resolution ground-truth. To make the predicted base layer similar to the low-resolution ground \ntruth, we employ a L1 loss between the low-resolution ground truth and the predicted base layer:\nwhere Lbaseloss is defined as a base loss. To further boost the quality of dehazed result, we introduce perceptual \nloss to train the proposed model:\nwhere VGG represents the VGG-16 model, which is a classic model trained on ImageNet, and j indicates which \nlayer is used to estimate the perceptual loss.\nFinally, we combine the perceptual loss, the multi- scale content loss, the base loss, and the perceptual loss \nto train the whole network, which can be defined as:\nwhere /afii98381 is used to determine the contribution of the base loss, and /afii98382 is used to determine the contribution of \nthe perceptual loss.\n(10)H h = Ah. ∗ I+ Bh.\n(11)Ah =U (Al)\n(12)Bh =U (Bl)\n(13)Lcon = 1\nN\nN∑\ni=1\nˆH i\nh − H i\n\n1\n+ 1\nN\nN∑\ni=1\nˆH i\nl − H i\nl\n\n1\n,\n(14)Lbaseloss = 1\nN\nN∑\ni=1\nˆBi − H i\nl\n\n1\n,\n(15)Lperc = 1\nN\n1\nJ\nN∑\ni=1\nJ∑\nj=1\nVGG ( ˆH i\nh) − VGG (H i\nl)\n\n1\n,\n(16)Lo = Lcon + /afii98381 Lbaseloss + /afii98382 Lperc,\nAlgorithm 1.  Joint Upsampling Guided Filtering in the proposed DGTDN.\n6\nVol:.(1234567890)Scientific Reports |        (2023) 13:15333  | https://doi.org/10.1038/s41598-023-41561-z\nwww.nature.com/scientificreports/\nExperimental results\nIn this section, we focus on showing the high performance of the proposed method. First, we introduce the \nimplementation details of the proposed method and dataset. Second, we compare the proposed method with \nother dehazing methods on simulated haze images and real haze images. Third, we show the effectiveness of the \nproposed modules and loss functions.\nImplementation details. In this subsection, we show the details of the proposed model. The proposed \nBaseNet is implemented based on the Swin-Transformer block. The configurations of the proposed RDB are \nlisted in Table 1. The proposed DGTDN is implemented in a popular deep learning tool (PyTorch) using a single \nGPU ( TITAN V ) with 12GB memory. When training, we crop the training dataset into image patches with size \n240 × 240 . The learning rate is set to 0.001 and then is decreased by 0.8 every 10000 steps. We set the batch size \nto 16. We employ the adam to train the proposed model and initialize the β1 and β2 to 0.5 and 0.999, respectively. \nWe set /afii98381 and /afii98382 to 1.0 and 0.01, respectively.\nAccording to the strategy adopted  by20,21,36, ITS from RESIDE is chosen to train the proposed model and \nindoor hazy images from SOTS subset are used to evaluate the dehazing performance. In addition, we evaluate \nthe performance on NH-HAZE.\nExperimental results on simulated hazy images. In this part, we show the dehazing performance of \nthe proposed DGTDN and other dehazing methods on the simulated indoor hazy images. Due to the fact, it \nis hard to find a ground truth haze-free image for a real haze image, simulated indoor hazy images are used to \nevaluate the dehazing performance. We show quantitative and visual dehazing results in Table 2 and Fig. 3. As \nshown in Table 2, traditional dehazing methods can obtain low quantitative results. Traditional dehazing meth-\nods derive prior from haze-free images, which may not be held by some hazy images. This is the main reason \nwhy traditional dehazing cannot achieve a high dehazing performance. The learning based dehazing methods \ninclude two kinds. The first is learning to predict transmission map, such as  MSCNN10 and  DehazeNet11. The \nsecond kind is learning to predict clean images directly, such as  DCPDN15,  GFN12,  MSBDN21, and  Dehamer26. \nThe learning-based  methods10,11 that learn the relationship between transmission map and hazy images. How-\never, the relationship between transmission maps and dehazing quality is not highly correlated, which results \nin a low dehazing performance. End-to-end dehazing  methods12,15,21,26 construct the relationship between hazy \nTable 1.  Details of the RDB.\nlayers Conv1 Dconv2 Dconv3 Conv4\nSize 3 3 3 3\nChannels 16 16 16 16\nDilation rates 1 2 2 1\nTable 2.  Evaluation results of dehazed results using average PSNR/SSIM on the SOTS dataset from  RESIDE47.\nBCCR MSCNN DehazeNet CAP DCP NLD AOD-Net GFN DCPDN MSDFF Dehamer DGTDN\nPSNR 16.88 17.57 21.14 19.05 16.62 17.29 19.06 22.30 15.86 33.75 36.36 36.68\nSSIM 0.79 0.81 0.85 0.84 0.82 0.75 0.85 0.88 0.82 0.98 0.98 0.99\nFigure 3.  Visual results of some recently dehazing methods and the proposed method. The dehazed result \nobtained by other dehazing methods often retain haze or color distortion. The proposed method can remove \nhaze more completely and obtain a more natural dehazing result.\n7\nVol.:(0123456789)Scientific Reports |        (2023) 13:15333  | https://doi.org/10.1038/s41598-023-41561-z\nwww.nature.com/scientificreports/\nimages and dehazed results. However, the dehazing ability of these models depends on the model capacity. The \ntransformer-based dehazing method has a high model capacity and achieves the second dehazing performance. \nTo summarize, the proposed method achieves outstanding performance among famous dehazing methods. As \nshown in Fig. 3, we note that traditional dehazing methods, such as DCP , NLD, and BCCR often have the prob-\nlem of color distortion. The learning-based  methods10–12,15 have the problem of retaining haze. Other leaning-\nbased  methods21,26 can obtain dehazed results that are similar to ground truth. The proposed can obtain high-\nquality visual dehazing results, which are more similar to ground truth.\nWe also test the dehazing performance on NH-HAZE 48, which is a widely used dataset. NH-HAZE is a \nfamous dehazing dataset, which contains non-homogeneous haze. The non-homogeneous haze is much harder \nto remove than the traditional homogeneous haze. The dehazing performance tested on non-homogeneous haze \ncan show the model’s capability well. We listed the dehazing quantitative performance of dehazing methods \nin Table 3. As shown in Table 3, DCP , BCCR, and NLD achieve a low quantitative dehazing performance. We \nnote that DehazeNet achieves lower quantitative dehazing performance than DCP , BCCR, and NLD. learning-\nbased  methods10,12,13,15,21 achieve higher dehazing performance. Dehamer achieves the second-best quantitative \ndehazing performance. The proposed method demonstrates the best PSNR and SSIM among the listed dehazing \nmethods. The results demonstrate the effectiveness of the proposed method, which benefits from the combination \nof CNN and transformer. We also show visual dehazed results of the proposed method and other state-of-the-art \nmethods. As shown in Fig. 4, we can see that the traditional dehazing methods often over-enhance the dehazed \nresults, which contain obvious color distortion. The learning-based methods tend to retain haze in dehazed \nresults. In contrast to these methods, the proposed method often obtains visually pleasing dehazed results, which \nare vivid color and contain rich image details.\nExperimental results on real-world haze images. To further show the performance, we choose some \ntypical real-world hazy images. The density and distribution of haze in real hazy images are more multiplicative \nthan in synthetic images. Hence, the real-world hazy image dehazing is a more challenging problem. In this part, \nwe choose three hazy images, which include dense haze, large haze distribution, and dark haze images. These \nhaze images can show the generalization and dehazing performance of deep-learning-based models.\nFirstly, we conduct an experiment on a dense haze image. The dehazed results of state-of-the-art methods \nand the proposed method are shown in Fig.  5. As shown, we can see that the image tends to show dense haze \nover the whole image, which is hard for CNN-based dehazing methods. The dehazed results of AOD-Net 13 \nand  DCPDN15 tend to retain haze. The dehazed result of  GFN12 contains visible color distortion and haze. The \ndehazed result of  cGAN49 contains less color distortion than GFN and can remove haze better than AOD-Net, \nDCPDN, and GFN. We note that the dehazed results of EPDN, Dehamer, and the proposed method are better \nthan other learning-based methods. We note that the area in the lake is not well dehazed in a result of EPDN. The \nproposed method can remove haze more completely than EPDN and Dehamer. Due to the fact the transformer \ncan capture long dependency, which can boost the dehazing quality. The proposed method and Dehamer remove \nhaze from dense haze images. The proposed method employs CNN to restore the image details, which makes \nthe proposed method can restore more image details than Dehamer.\nSecondly, we conduct an experiment on a hazy image with large haze distribution. This hazy image is a typical \nimage, which has been employed to evaluate the dehazing performance widely. This image contains dense haze \nareas, a middle haze area, and light haze areas, which are marked using black, red, and green circles, respectively. \nDue to its large haze distribution, the learning-based methods often fail to remove haze well. As shown in Fig. 6, \nwe note that the traditional  methods1,4 often show a better dehazed results than learning-based  methods12,13,15. \nTable 3.  Evaluation results of dehazed results using average PSNR/SSIM on the dataset NH-HAZE48.\nDCP BCCR MSCNN DehazeNet NLD AOD-Net GFN DCPDN MSDFF Dehamer DGTDN\nPSNR 12.35 12.15 17.72 11.76 12.01 17.42 15.17 15.86 16.21 19.25 19.86\nSSIM 0.40 0.38 0.67 0.40 0.38 0.57 0.52 0.61 0.58 0.62 0.66\nFigure 4.  Visual results of dehazing methods on the dense non-homogeneous haze  images48. The proposed \nmethod restores more haze-free images with clearer structures and textures.\n8\nVol:.(1234567890)Scientific Reports |        (2023) 13:15333  | https://doi.org/10.1038/s41598-023-41561-z\nwww.nature.com/scientificreports/\nThe dehazed result of Non-local dehazing tends to lose image details and shows a dark appearance. The dehazed \nresult of  DCP1 tends to retain a small amount of haze. The dehazed results of CAP , DCPDN, FFA-Net, and AOD-\nNet tend to retain a large amount of haze. The dehazed results of  GDN20 and  GFN12 contain color distortion. \nDehazenet and MSCNN are based on deep learning and Koschmieder’s law. We note that the dehazed result of \nMSCNN is better than DehazeNet, which can remove more haze. We also note that the dehazed result of MSCNN \nlosses some image details. The dehazed result of  PGAN34. However, we note the dehazed result of PGAN still \ncontains haze. The dehazed results of EPDN and Dehamer can remove haze better. However, these methods \ntend to generate a dark dehazed result and tend to show some haze around the green circle area. The proposed \nmethod can remove haze more completely and keep the image details well.\nThirdly, we conduct an experiment on a more challenging image, which looks dark. The dehazed results of this \nimage often have the problem of losing image details and retaining haze. As shown in Fig. 7, we can see that the \ndehazed result of DCP , AOD-Net, AECR,  AirNet14, EPDN, and Dehamer tend to show a dark appearance. The \ndehazed result of DCPDN, FFA, and PGAN looks brighter. However, the dehazed results of these methods tend \nto retain haze in the dehazed result. The dehazed result of DA, PSD, and DGTDN can generate a much brighter \ndehazing result. However, the dehazed result of PSD tend to retain haze in the whole image while the result of DA \ntends to leave haze in a black rectangle and show a blur dehazed result. AirNet is based on the assumption that \nthe whole image shares similar degradation. In contrast, the proposed method can remove haze more completely \nand obtain a sharp dehazed result. To show the quality of dehazed results obtained by the proposed method and \nother dehazing methods quantitatively, we use the metric proposed  in50. As shown in Table 4, we can see that \nthe proposed method can remove haze better than other dehazing methods.\nAblation studies. \nTo the effectiveness of the proposed module in DGTDN, we design a series of experiments. Firstly, we design a \nmodel to show the effectiveness of the transformer. We remove the transformer from the proposed model, and \nkeep other parts unchanged, we term it as model1. Secondly, we show the effectiveness of the DetailNet. We \nremove the DetailNet from the proposed model, and keep other parts unchanged, we term it as model2. Finally, \nwe show the effectiveness of the GuidedFilterNet, which can boost the dehazing speed of the proposed model. \nTo show the influence of the GuidedFilterNet, we design a model which removes the GuidedFilterNet and keeps \nother parts unchanged, we term it as model3. We show the quantitative comparison in Table  5 and a visual \nexample in Fig. 8. As shown in Table 5, we can see that the model1 achieves the lowest dehazing performance \ndue to the limitation of the receptive field. As we can see that the BaseNet can boost the dehazing performance \ndramatically, which shows the transformer module is necessary for dehazing. The transformer module can \nFigure 5.  Visual results of some recently dehazing methods and the proposed method on lake scene with dense \nhaze. The dehazed result obtained by other learning-based dehazing methods often retains haze. The proposed \nmethod can remove haze more completely and obtain a more natural dehazing result.\n9\nVol.:(0123456789)Scientific Reports |        (2023) 13:15333  | https://doi.org/10.1038/s41598-023-41561-z\nwww.nature.com/scientificreports/\nimprove the dehazing performance by enlarging the receptive field. We note that the application of guided filter \nreduces the dehazing performance. However, it is necessary to improve the dehazing speed while only reducing \nthe dehazing performance slightly. We show the difference dehazed result of model1, model2, model3, and the \nproposed model in Fig  8. We can see that model1 cannot remove haze in remote areas, which are dense haze. \nThe transformer module is necessary for removing dense haze areas. By adding the DetaiNet, we can see that \nthe model can remove haze more completely. The guided filter improve the dehazing quality in remote areas.\nTo show the influence of loss functions, we design an ablation that involves the models are trained with dif-\nferent losses. First, we train the model without Lperc . Second, we train the proposed model without Lbaseloss . \nThird, we train the proposed model without Lcon . We show the quantitative results in Table 6. As shown, Lcon \nis critical to obtain a high quantitative dehazing result. Lcon is designed to boost the details of the dehazed \nresults. Lcon is designed to make the dehazed results similar to the ground truths. Lbaseloss is used to reduce the \ndifficulty of dehazing problem, which can boost the dehazing quality. We also show dehazed results of the model \ntrained with different loss functions in Fig.  9. As shown, we note that the model trained without Lcon obtains \na dehazed result that losses image details. The dehazed results obtained by models trained without Lbaseloss or \nLperc generate results with color distortion or over-enhancement. As shown in Fig.  9, the model trained with \nall losses can generate high quality dehazing results.\nFigure 6.  Visual results of dehazing methods. The dehazed result obtained by other state-of-the-art methods \ntends to show a hazed or dark appearance. The dehazed results of MSCNN and AOD-Net lose some details. In \ncontrast, the proposed method often shows a sharp dehazed result and removes haze more completely.\n10\nVol:.(1234567890)Scientific Reports |        (2023) 13:15333  | https://doi.org/10.1038/s41598-023-41561-z\nwww.nature.com/scientificreports/\nFigure 7.  Visual results of some recently dehazing methods and the proposed method. The dehazed results \nobtained by other state-of-the-art methods tend to show a dark or hazed appearance. DA is designed for natural \nimage dehazing with domain adaption. However, we note that the area marked with a black rectangle retains a \nlot of haze. In contrast, the proposed method often shows a colorful and sharp dehazed result and removes haze \nmore completely.\nTable 4.  Density values for a natural hazy image in Fig. 7. The best result is marked with bold.\nInput DCP NLD FFA-Net AECR-Net AirNet EPDN DA MSBDN PSD-Net Dehamer DCPDN PGAN AOD-Net Our\n1.832 0.398 0.348 0.737 0.294 0.634 0.293 0.405 0.802 0.230 0.327 0.652 0.319 0.732 0.204\nTable 5.  The quantitative results with different modules on the synthetic hazy dataset.\nMetric Model1 Model2 Model3 DGTDN\nPSNR 25.83 36.49 36.88 36.68\nSSIM 0.92 0.98 0.99 0.99\nFigure 8.  Visual results of different model configurations and the proposed method. The dehazed result \nobtained by other different models often retains haze or color distortion. The proposed method can remove haze \nmore completely and obtain a more natural dehazing result.\nTable 6.  The quantitative results with different loss functions on the synthetic hazy dataset.\nMetric w/o Lperc w/o Lbaseloss w/o Lcon DGTDN\nPSNR 34.29 34.79 24.06 36.68\nSSIM 0.98 0.98 0.78 0.99\n11\nVol.:(0123456789)Scientific Reports |        (2023) 13:15333  | https://doi.org/10.1038/s41598-023-41561-z\nwww.nature.com/scientificreports/\nAnalysis of run states. We test the dehazing speed of dehazing methods on 500 images with size 256 × 256 . \nThe test hazy images are from the outdoor part of RESIDE, we resize these images into a fixed size ( 256 × 256 ). \nWe conduct the experiment on a notebook, which is equipped with an Intel(R) Core i5 CPU@2.3GH, 8GB mem-\nory, and a 3GB RTX 1060 GPU. The average running times of state-of-the-art dehazing methods and the pro-\nposed method is shown in Table 7. The traditional dehazing  methods1,4 are slower than learning-based methods. \nThese methods are executed without parallelization technology, which increases the execution time. The early \nlearning-based  method11 is faster. However, the dehazing performance of this method is poor. The proposed \nmethod achieves state-of-the-art dehazing performance while keeping a lower execution time. In addition, we \nshow the rum states of each method in Table 7. The running states include language, platform, execution time, \nparameters, and consumption of GPU memory.\nAs shown in Table  7, the proposed model has a suitable parameter number and consumes suitable GPU \nmemory, while achieving the highest quantitative performance. We also show the effectiveness of the Guided -\nFilterNet, which can reduce the execution time and GPU memory compared with model3. As shown in Table 5, \nthe proposed method is with almost no visible degradation compared with model3. We can obtain the conclusion \nthe GuidedFilterNet can improve the execution speed while avoiding performance degradation.\nExtended applications. Based on the fact the proposed model can capture the local and global features \njointly, we can apply the proposed model to solve the problem, such as underwater  enhancement51–55,  detain56, \nand human image  generation57. Single image underwater enhancement is a challenging problem due to its ill-\nposed nature. The global information and local details of underwater images are degraded by water, which \nresults in the degeneration of each pixel may be different. Based on this observation, the high-performance \nmodel requires global features to capture the degeneration. The underwater enhancement also needs to restore \nthe fine details, which requires the local features. The underwater enhancement is similar to dehazing, which \nalso needs global and local features jointly and a low compute resource requirement. The proposed model can \ncapture the global and local features jointly, which also can be applied to underwater enhancement.\nConclusion\nDeep Guided Transformer Dehazing Network (DGTDN) is proposed based on the transformer and guided \nfilter, which boosts the speed of transformer-based dehazing methods and the image quality of dehazed result. \nThe proposed model consists of BaseNet, DetailNet, and GuidedFilerNet. BaseNet and DetailNet are proposed \nto capture the local and global features jointly. To boost the advantages of the transformer module and the CNN \nmodule, we employ the transformer module to predict the base layer of a clean image, and the CNN module \nto predict the detail layer. To address the dehazing speed problem of the transformer module, we employ the \nFigure 9.  Visual results of models trained with different loss functions. The dehazed result obtained by other \ndifferent models often retains haze or color distortion. The proposed method can remove haze more completely \nand obtain a more natural dehazing result.\nTable 7.  Running states of the state-of-the-art dehazing methods and proposed methods on the 500 images \nwith 256×256 size. The running states include language, platform, execution time, parameters, consumption of \nGPU memory. The best result is marked with bold.\nMethod DCP NLD MSCNN DehazeNet GFN DCPDN EPDN DA FFA-Net\nLanguage Matlab Python\nPlatform Caffe PyTorch\nTime (s) 0.302 3.416 0.102 0.051 2.740 0.106 0.156 0.096 2.431\nParameters – – 8.01 × 103 8.02×103 5.14 × 105 6.69 × 107 1.74 × 107 5.46 × 107 4.46 × 106\nMemory (G) – – 0.019 1.282 0.169 1.65 1.73 0.939 1.107\nMethod MSBDN AirNet PSD AECR PGAN GDN Dehamer Model3 DGTDN\nLanguage Python\nPlatform PyTorch\nTime (s) 0.462 0.772 0.132 0.053 0.268 0.219 0.298 0.169 0.061\nParameters 3.14 × 107 7.61 × 106 3.31 × 107 2.61 × 106 1.14 × 107 9.58 × 105 1.32 × 108 4.64 × 106 4.65 × 106\nMemory (G) 0.909 1.025 0.855 0.825 2.345 1.138 1.862 0.868 0.726\n12\nVol:.(1234567890)Scientific Reports |        (2023) 13:15333  | https://doi.org/10.1038/s41598-023-41561-z\nwww.nature.com/scientificreports/\nguided filter model to perform a joint up-sampling, which can improve the dehazing speed while keeping the \nquality of dehazed result. We show the effectiveness of the proposed method by comparing it with state-of-the-\nart dehazing methods on real and simulated haze images. We also show the effectiveness of the novel modules \nby comparing the performance of the different architectures and loss functions. In the future, we will study \nstrategies of combining CNN and the transformer, which is critical to capture the local and global features. We \nalso will study the domain shift of simulated-haze and real-haze images, which is critical to boost the dehazing \nperformance on real haze images.\nData availability\nThe datasets generated and/or analyzed during the current study are available in the RESIDE repository, which \ncan be found at: https:// sites. google. com/ view/ reside- dehaze- datas ets/ reside- stand ard. The natural hazy images \nare from: http:// live. ece. utexas. edu/ resea rch/ fog/ fade_ defade. html and https:// www. cs. huji. ac. il/ w~raana nf/ proje \ncts/ dehaze_ cl/ resul ts/.\nReceived: 5 July 2023; Accepted: 28 August 2023\nReferences\n 1. He, K., Sun, J. & Tang, X. Single image haze removal using dark channel prior. IEEE Trans. Pattern Anal. Mach. Intell. 33, 2341–2353 \n(2011).\n 2. Fattal, R. Dehazing using color-lines. ACM Trans. Graph. 34, 13 (2014).\n 3. Meng, G., Wang, Y ., Duan, J., Xiang, S. & Pan, C. Efficient image dehazing with boundary constraint and contextual regularization. \nIn IEEE International Conference on Computer Vision (2013).\n 4. Berman, D., Avidan, S. et al. Non-local image dehazing. In IEEE Conference on Computer Vision and Pattern Recognition (2016).\n 5. Zhu, Q., Mai, J. & Shao, L. A fast single image haze removal algorithm using color attenuation prior. IEEE Trans. Image Process.  \n24, 3522–3533 (2015).\n 6. Fattal, R. Single image dehazing. ACM Trans. Graph. 27, 72 (2008).\n 7. Schechner, Y . Y ., Narasimhan, S. G. & Nayar, S. K. Instant dehazing of images using polarization. In IEEE Conference on Computer \nVision and Pattern Recognition (2001).\n 8. Narasimhan, S. G. & Nayar, S. K. Chromatic framework for vision in bad weather. In IEEE Conference on Computer Vision and \nPattern Recognition (2000).\n 9. Shwartz, S., Namer, E. & Schechner, Y . Y . Blind haze separation. IEEE Conf. Comput. Vis. Pattern Recogn. 2, 1984–1991 (2006).\n 10. Ren, W . et al. Single image dehazing via multi-scale convolutional neural networks. In European Conference on Computer Vision \n(2016).\n 11. Cai, B., Xu, X., Jia, K., Qing, C. & Tao, D. Dehazenet: An end-to-end system for single image haze removal. IEEE Trans. Image \nProcess. 25, 5187–5198 (2016).\n 12. Ren, W . et al. Gated fusion network for single image dehazing. In IEEE Conference on Computer Vision and Pattern Recognition  \n(2018).\n 13. Li, B., Peng, X., Wang, Z., Xu, J. & Feng, D. An all-in-one network for dehazing and beyond. In IEEE International Conference on \nComputer Vision (2017).\n 14. Li, B. et al. All-In-One Image Restoration for Unknown Corruption. In IEEE Conference on Computer Vision and Pattern Recogni-\ntion (New Orleans, LA, 2022).\n 15. Zhang, H. & Patel, V . M. Densely connected pyramid dehazing network. In IEEE Conference on Computer Vision and Pattern \nRecognition (2018).\n 16. Chen, W .-T., Ding, J.-J. & Kuo, S.-Y . Pms-net: Robust haze removal based on patch map for single images. In IEEE Conference on \nComputer Vision and Pattern Recognition (2019).\n 17. Chen, Z., Wang, Y ., Y ang, Y . & Liu, D. Psd: Principled synthetic-to-real dehazing guided by physical priors. In IEEE Conference on \nComputer Vision and Pattern Recognition, 7180–7189 (2021).\n 18. Qu, Y ., Chen, Y ., Huang, J. & Xie, Y . Enhanced pix2pix dehazing network. In IEEE Conference on Computer Vision and Pattern \nRecognition (2019).\n 19. Shao, Y ., Li, L., Ren, W ., Gao, C. & Sang, N. Domain adaptation for image dehazing. In IEEE Conference on Computer Vision and \nPattern Recognition (2020).\n 20. Liu, X., Ma, Y ., Shi, Z. & Chen, J. Griddehazenet: Attention-based multi-scale network for image dehazing. In IEEE International \nConference on Computer Vision (2019).\n 21. Dong, H. et al. Multi-scale boosted dehazing network with dense feature fusion. In IEEE Conference on Computer Vision and Pat-\ntern Recognition (2020).\n 22. Zhang, H., Sindagi, V . & Patel, V . M. Joint transmission map estimation and dehazing using deep networks. IEEE Trans. Circuits \nSyst. Video Technol. 30, 1975–1986. https:// doi. org/ 10. 1109/ TCSVT. 2019. 29121 45 (2020).\n 23. Song, X. et al. Wsamf-net: Wavelet spatial attention-based multistream feedback network for single image dehazing. IEEE Trans. \nCircuits Syst. Video Technol. 33, 575–588. https:// doi. org/ 10. 1109/ TCSVT. 2022. 32070 20 (2023).\n 24. Wang, Z. et al. Uformer: A general u-shaped transformer for image restoration. In IEEE Conference on Computer Vision and Pat-\ntern Recognition, 17683–17693 (2022).\n 25. Dosovitskiy, A. et al. An image is worth 16x16 words: Transformers for image recognition at scale. Preprint at arXiv:  2010. 11929 \n(2020).\n 26. Guo, C.-L. et al. Image dehazing transformer with transmission-aware 3d position embedding. In IEEE Conference on Computer \nVision and Pattern Recognition, 5812–5820 (2022).\n 27. Zhao, D., Xu, L., Ma, L., Li, J. & Y an, Y . Pyramid global context network for image dehazing. IEEE Trans. Circuits Syst. Video Technol. \n31, 3037–3050. https:// doi. org/ 10. 1109/ TCSVT. 2020. 30369 92 (2021).\n 28. Zhang, Y . et al. Image super-resolution using very deep residual channel attention networks. In ECCV, 286–301 (2018).\n 29. Huang, G., Liu, Z., Van Der Maaten, L. & Weinberger, K. Q. Densely connected convolutional networks. In IEEE Conference on \nComputer Vision and Pattern Recognition (2017).\n 30. Fu, X. et al. Removing rain from single images via a deep detail network. In IEEE Conference on Computer Vision and Pattern \nRecognition (2017).\n 31. Ren, W ., Pan, J., Zhang, H., Cao, X. & Y ang, M.-H. Single image dehazing via multi-scale convolutional neural networks with \nholistic edges. Int. J. Comput. Vision 128, 240–259 (2020).\n 32. Bai, H., Pan, J., Xiang, X. & Tang, J. Self-guided image dehazing using progressive feature fusion. IEEE Trans. Image Process. 31, \n1217–1229 (2022).\n13\nVol.:(0123456789)Scientific Reports |        (2023) 13:15333  | https://doi.org/10.1038/s41598-023-41561-z\nwww.nature.com/scientificreports/\n 33. Deng, Z. et al. Deep multi-model fusion for single-image dehazing. In IEEE International Conference on Computer Vision, 2453–\n2462 (2019).\n 34. Pan, J. et al. Physics-based generative adversarial models for image restoration and beyond. IEEE Trans. Pattern Analy. Mach. \nIntell.https:// doi. org/ 10. 1109/ TPAMI. 2020. 29693 48 (2020).\n 35. Zhang, J. & Tao, D. Famed-net: A fast and accurate multi-scale end-to-end dehazing network. IEEE Trans. Image Process. 29, 72–84 \n(2020).\n 36. Wu, H. et al. Contrastive learning for compact single image dehazing. In IEEE Conference on Computer Vision and Pattern Recogni-\ntion, 10551–10560 (2021).\n 37. Zhang, J. et al. Hierarchical density-aware dehazing network. IEEE Transactions on Cybernetics (2021).\n 38. Zhang, S. et al. Semantic-aware dehazing network with adaptive feature fusion. IEEE Trans. Cybern. 53, 454–467 (2021).\n 39. Ren, W ., Sun, Q., Zhao, C. & Tang, Y . Towards generalization on real domain for single image dehazing via meta-learning. Control. \nEng. Pract. 133, 105438. https:// doi. org/ 10. 1016/j. conen gprac. 2023. 105438 (2023).\n 40. Liu, Y ., Y an, Z., Tan, J. & Li, Y . Multi-purpose oriented single nighttime image haze removal based on unified variational retinex \nmodel. IEEE Trans. Circuits Syst. Video Technol. 33, 1643–1657. https:// doi. org/ 10. 1109/ TCSVT. 2022. 32144 30 (2023).\n 41. Vaswani, A. et al. Attention is all you need. Advances in neural information processing systems30 (2017).\n 42. Carion, N. et al. End-to-end object detection with transformers. In European Conference on Computer Vision  (ed. Carion, N.) \n213–229 (Springer, 2020).\n 43. Xie, E. et al. Segformer: Simple and efficient design for semantic segmentation with transformers. Adv. Neural. Inf. Process. Syst.  \n34, 12077–12090 (2021).\n 44. Li, H., Zhang, Y ., Liu, J. & Ma, Y . Gtmnet: A vision transformer with guided transmission map for single remote sensing image \ndehazing. Sci. Rep. 13, 9222 (2023).\n 45. Zamir, S. W . et al. Restormer: Efficient transformer for high-resolution image restoration. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR), 5728–5739 (2022).\n 46. He, K., Sun, J. & Tang, X. Guided image filtering. In European Conference on Computer Vision, 1–14 (2010).\n 47. Li, B. et al. Benchmarking single image dehazing and beyond. IEEE Transactions on Image Processing (2018).\n 48. Ancuti, C. O., Ancuti, C. & Timofte, R. Nh-haze: An image dehazing benchmark with non-homogeneous hazy and haze-free \nimages. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops (2020).\n 49. Li, R., Pan, J., Li, Z. & Tang, J. Single image dehazing via conditional generative adversarial network. In IEEE Conference on Com-\nputer Vision and Pattern Recognition (2018).\n 50. Choi, L. K., Y ou, J. & Bovik, A. C. Referenceless prediction of perceptual fog density and perceptual image defogging. IEEE Trans. \nImage Process. 24, 3888–3901 (2015).\n 51. Zhou, J. et al. Ugif-net: An efficient fully guided information flow network for underwater image enhancement. IEEE Transactions \non Geoscience and Remote Sensing (2023).\n 52. Zhang, D. et al. Rex-net: A reflectance-guided underwater image enhancement network for extreme scenarios. Expert Systems \nwith Applications 120842 (2023).\n 53. Zhou, J., Sun, J., Zhang, W . & Lin, Z. Multi-view underwater image enhancement method via embedded fusion mechanism. Eng. \nAppl. Artif. Intell. 121, 105946 (2023).\n 54. Zhou, J., Pang, L., Zhang, D. & Zhang, W . Underwater image enhancement method via multi-interval subhistogram perspective \nequalization. IEEE Journal of Oceanic Engineering (2023).\n 55. Zhou, J., Zhang, D. & Zhang, W . Cross-view enhancement network for underwater images. Eng. Appl. Artif. Intell. 121, 105952 \n(2023).\n 56. Zhang, H. & Patel, V . M. Density-aware single image de-raining using a multi-stream dense network. In IEEE Conference on \nComputer Vision and Pattern Recognition, 695–704 (2018).\n 57. Wu, H., He, F ., Duan, Y . & Y an, X. Perceptual metric-guided human image generation. Integr. Comput.-Aided Eng. 29, 141–151 \n(2022).\nAuthor contributions\nS.Z.: Conceptualization, Methodology, Software, Writing – original draft. L.Z., K.H. and S.F .: Visualization, \nMethodology, Software, Writing – original draft. E.F .: Writing – review and editing. L.Z.: Supervision, Writing \n– review and editing.\nFunding\nThis work is supported by the National Natural Science Foundation of China (Nos. 62101387, 62271321). \nThis work is partially supported by the Science Project of Shaoxing University (Nos. 20205048, 20210026, and \n2022LG006), and in part by the Science and Technology Plan Project in Basic Public Welfare class of Shaoxing \ncity (No.2022A11002).\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to L.Z.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\n14\nVol:.(1234567890)Scientific Reports |        (2023) 13:15333  | https://doi.org/10.1038/s41598-023-41561-z\nwww.nature.com/scientificreports/\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2023"
}