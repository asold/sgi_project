{
  "title": "Greedy-layer pruning: Speeding up transformer models for natural language processing",
  "url": "https://openalex.org/W4220785642",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2313946774",
      "name": "Peer, David",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287496001",
      "name": "Stabinger, Sebastian",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Engl, Stefan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3161332203",
      "name": "Rodríguez-Sánchez, Antonio",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3028836324",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6766673545",
    "https://openalex.org/W6803544937",
    "https://openalex.org/W3178906774",
    "https://openalex.org/W6768086466",
    "https://openalex.org/W6776322294",
    "https://openalex.org/W6768851824",
    "https://openalex.org/W6767023738",
    "https://openalex.org/W6762945437",
    "https://openalex.org/W6763417742",
    "https://openalex.org/W3166574921",
    "https://openalex.org/W6768080748",
    "https://openalex.org/W6776129198",
    "https://openalex.org/W3007007518",
    "https://openalex.org/W6750615492",
    "https://openalex.org/W6752342493",
    "https://openalex.org/W6691459498",
    "https://openalex.org/W6605323724",
    "https://openalex.org/W6786240170",
    "https://openalex.org/W6737236263",
    "https://openalex.org/W6718053083",
    "https://openalex.org/W6768287854",
    "https://openalex.org/W2948184028",
    "https://openalex.org/W3010030563",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W6784431563",
    "https://openalex.org/W4206136559",
    "https://openalex.org/W3122277820",
    "https://openalex.org/W6784747239",
    "https://openalex.org/W3034457371",
    "https://openalex.org/W3101066076",
    "https://openalex.org/W3203309275",
    "https://openalex.org/W4239019441",
    "https://openalex.org/W1502957213",
    "https://openalex.org/W2599674900",
    "https://openalex.org/W3095016240",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2750779823",
    "https://openalex.org/W131533222",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W2971031791",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4288374798",
    "https://openalex.org/W4288256350",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W4298545760",
    "https://openalex.org/W4238634189",
    "https://openalex.org/W3015233032",
    "https://openalex.org/W2489487449",
    "https://openalex.org/W1516184288",
    "https://openalex.org/W3211950664",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W4313908941",
    "https://openalex.org/W2975381464",
    "https://openalex.org/W4288347855"
  ],
  "abstract": null,
  "full_text": "Greedy-layer Pruning: Speeding up Transformer Models for Natural\nLanguage Processing∗\nDavid Peer1,2 Sebastian Stabinger1,2 Stefan Engl1 Antonio Rodr´ ıguez-S´ anchez2\n1DeepOpinion, 6020 Innsbruck, Austria\n2University of Innsbruck, 6020 Innsbruck, Austria\nMarch 30, 2022\nAbstract\nFine-tuning transformer models after unsupervised pre-\ntraining reaches a very high performance on many dif-\nferent natural language processing tasks. Unfortunately,\ntransformers suﬀer from long inference times which\ngreatly increases costs in production. One possible so-\nlution is to use knowledge distillation, which solves this\nproblem by transferring information from large teacher\nmodels to smaller student models. Knowledge distillation\nmaintains high performance and reaches high compres-\nsion rates, nevertheless, the size of the student model\nis ﬁxed after pre-training and can not be changed in-\ndividually for a given downstream task and use-case to\nreach a desired performance/speedup ratio. Another so-\nlution to reduce the size of models in a much more ﬁne-\ngrained and computationally cheaper fashion is to prune\nlayers after the pre-training. The price to pay is that\nthe performance of layer-wise pruning algorithms is not\non par with state-of-the-art knowledge distillation meth-\nods. In this paper, Greedy-layer pruning is introduced\nto (1) outperform current state-of-the-art for layer-wise\npruning, (2) close the performance gap when compared\nto knowledge distillation, while (3) providing a method to\nadapt the model size dynamically to reach a desired perfor-\nmance/speedup tradeoﬀ without the need of additional pre-\ntraining phases. Our source code is available on https:\n//github.com/deepopinion/greedy-layer-pruning.\n1 Introduction\nFine-tuning transformer models on multiple tasks after\npre-training is the de-facto standard in Natural Language\nProcessing [30] due to its outstanding performance on a\nwide range of applications such as sentence classiﬁcation\n[3, 12], named entity recognition [18] or document level\n∗Accepted at Pattern Recognition Letters ( https://www.\njournals.elsevier.com/pattern-recognition-letters)\nrelation extraction tasks [21]. Classical transformer mod-\nels, such as BERT [3], are massive networks with up to 24\nlayers, each containing fully connected layers as well as\nmany attention heads, leading to more than 100M param-\neters as a result. This extremely large number of param-\neters leads to long inference times and an increase in cost\nif deployed in production after training. Additionally, in-\ncreased memory consumption and inference-time hinder\nthe deployment of transformer models on edge- or em-\nbedded devices. Diﬀerent areas of research address this\nproblem by targeting a reduction in memory consumption\nas well as inference time. One such research direction is\nknowledge distillation, where information is transferred\nfrom large teacher models to much smaller student mod-\nels, while maintaining up to 96 .8% of the performance of\nthe teacher [10].\nIn the ﬁrst step of knowledge distillation, information is\ntransferred during a pre-training stage from large teacher\nmodels to smaller student models. Specialized teacher\nmodels can be pre-trained additionally for better perfor-\nmance [25]. Task-agnostic knowledge distillation mod-\nels such as DistilBERT [20] or MobileBERT [25] can di-\nrectly be ﬁne-tuned on downstream tasks after the pre-\ntraining distillation stage. Other models, such as Tiny-\nBERT, are computationally more expensive as they use\nthe teacher also in the ﬁne-tuning stage. In either of\nthose methodologies, knowledge must be distilled from\nlarge teacher models into smaller student models during\nthe pre-training phase to produce a compressed model.\nThe knowledge-distillation during the pre-training stage\nis computationally expensive and therefore it is impracti-\ncal to adjust the size of student models for each use-case\nindividually to reach a desired performance/speedup ra-\ntio. Task-speciﬁc knowledge distillation methods such as\nPKD [23] do not require pre-training distillation, but the\nperformance is also lower compared to the aforementioned\napproaches as we will show later in the experimental sec-\ntion 4. Nevertheless, having models with diﬀerent per-\nformance/speedup ratios available is crucial for deploy-\n1\narXiv:2105.14839v2  [cs.CL]  29 Mar 2022\nment in production and therefore most researchers pro-\nvide models of diﬀerent sizes in the form of pre-trained\nbase, large and xlarge models [3, 12]. An even more ﬁne-\ngrained selection would further help in practice to ad-\njust the performance/speedup trade-oﬀ speciﬁcally for a\ncertain use-case and downstream-task, but it is unfortu-\nnately computationally too expensive and therefore im-\npractical to pre-train a special model or distill a spe-\ncial student-model for each use-case individually. There-\nfore, our goal is to develop a method to compress and\ntrain models on a certain downstream task after the pre-\ntraining phase to enable a cheaper and much more ﬁne-\ngrained selection of the performance/speedup ratio.\nMichel et al. [14], Voita et al. [26] and Zhang et al.\n[31] have already shown that many attention heads can\nbe removed without a signiﬁcant impact on performance.\nUnfortunately, the speed-up reached by those methods\nis not much bigger than 1 .3×because only parts of the\nencoder layers are pruned, making them not suﬃciently\ncompetitive w.r.t inference time against distilled models.\nMuch higher compression rates can be achieved through\nlayer-wise pruning as complete encoder layers are re-\nmoved from the model. Fan et al. [5] proposes a form of\nstructured dropout, called LayerDrop, which is used dur-\ning pre-training and ﬁne-tuning to make models robust\nagainst removing whole layers. This solution requires an\nadditional pre-training phase that is quite expensive to\ncompute and as we show in the experimental section re-\nsults are worse compared to knowledge distillation. Saj-\njad et al. [19] studied many diﬀerent layer-wise pruning\napproaches for transformers that directly operate on pre-\ntrained models. They found that the performance is best\nif their Top-layer pruning algorithm is used. Models that\nare compressed with Top-layer pruning are already on-par\nwith DistilBERT, but still behind the novel distillation\nmethods such as TinyBERT or MobileBERT. Our goal is\ntherefore to outperform current state-of-the-art methods\nfor layer-wise pruning in order to reduce the performance\ngap when compared against distilled models. This goal is\nquite a challenging one as, if we compare the current state\nof the art on GLUE for distillation with MobileBERT, it\nreaches a score of 81 .0, while state-of-the-art layer-wise\npruning reaches a score of just 79 .6 (ﬁg. 1), which may\nbe the reason why layer-wise pruning has been somewhat\nneglected until now.\nWe propose Greedy-layer pruning (GLP), an algorithm\nthat compresses and trains transformer models while\nneeding only a modest budget. Our approach avoids\nthe computationally expensive distillation stage from the\nteacher into the student by directly reducing the size of\nalready pre-trained base models through pruning. First,\na given pre-trained model is analysed to determine an\norder to prune layers while maintaining high perfor-\nmance. Then, individual compression rates and per-\nformance/speedup ratios can be selected with a simple\nlookup that can be done in O(1). The proposed method-\nology is therefore 12×less expensive compared to knowl-\nedge distillation as will be shown in the experimental sec-\ntion 4. The proposed methodology outperforms state-of-\nthe-art layer-wise pruning algorithms and is competitive\nwith state-of-the-art distillation methods w.r.t inference\ntime and performance which makes this approach even\nmore practicable. Some of the compressed models that\nGLP produces even outperform the baseline as we will\nshow in our experimental evaluation. Our experimental\nresults also indicate that our GLP algorithm is empiri-\ncally eﬀective for diﬀerent transformer models while prun-\ning 50% of the layers, we still maintain 95 .3% of the per-\nformance of a BERT model and 95.4% of the performance\nof a RoBERTa model. Therefore, we will conclude that\nthe models can dynamically be compressed with Greedy-\nlayer pruning without the need of additional pre-training\nstages while still maintaining high performance. The con-\ntributions of this paper are:\n• A Greedy-layer-wise pruning algorithm is introduced\nto prune transformer models while maintaining high\nperformance.\n• We experimentally show that Greedy-layer pruning\noutperforms the state of the art in terms of layer-wise\npruning and closes the performance gap to distilla-\ntion methods.\n• It is demonstrated for each GLUE task that it is\npossible to select the model size with a single lookup\nafter GLP analyzed the pre-trained model to reach\na speciﬁc performance/speedup tradeoﬀ.\n• Greedy-layer pruning and Top-layer pruning are\ncompared against the optimal solution to motivate\nand guide future research.\nThis paper is structured as follows: Related work is pre-\nsented in the next section. In section 3, layer-wise prun-\ning is deﬁned and Greedy-layer pruning is introduced. In\nthe experimental section 4 we compare GLP against the\nstate of the art for layer-wise pruning and for distillation,\nwe compare the costs for compressing models with GLP\nas well as state-of-the-art knowledge distillation methods\nand ﬁnally compare layer-wise pruning methods against\nthe optimal solution. A discussion on limitations is pre-\nsented in section 5 followed by a future work section.\n2 Related Work\nThis work falls into the research area of compressing\ntransformer architectures. Diﬀerent areas of research ad-\ndress the problem to reduce memory consumption as well\n2\nas inference time of transformer models. A comprehensive\ncase-study on this topic is given by Ganesh et al. [6] who\ncategorize this area into data quantization, knowledge dis-\ntillation and pruning. The proposed method falls into the\nresearch area of pruning. One pruning strategy is to focus\non attention heads. The contribution made by individual\nattention heads in the encoder has been extensively stud-\nied. Michel et al. [14] demonstrated that 16 attention-\nheads can be pruned down to one attention head, achiev-\ning an inference speedup of up to 17 .5% if the batch size\nis large. Another method to prune unnecessary atten-\ntion heads is through single-shot meta-pruning [31], where\npruning 50% of the attention heads, reaches a speedup of\nabout 1 .3×on BERT, which is still not enough to be\ncompetitive against distilled models. Instead of pruning\nonly attention heads, it is also possible to prune layers\nof the encoder unit including all attention heads and the\nfeedforward layer. Fan et al. [5] pre-trains BERT models\nwith random layer dropping such that pruning becomes\nmore robust during the ﬁne-tuning stage. On the other\nhand, Sajjad et al. [19] claims that layer dropping [5] is (1)\nexpensive because it requires a pre-training from scratch\nof each model and (2) it is possible to outperform this\nmethodology by dropping top-layers before ﬁne-tuning a\nmodel. Sajjad et al. [19] evaluated six diﬀerent encoder\nunit pruning strategies, namely, top-layer dropping, even\nalternate dropping, odd alternate dropping, symmetric\ndropping, bottom-layer dropping and contribution-based\ndropping. They found that (1) pruning top layers from\nthe model consistently works best, (2) layers should be\ndropped directly after pre-training and (3) that an itera-\ntive pruning of layers does not improve the performance.\nNevertheless, Top-layer pruning is not on par with distil-\nlation methods, something that we address in this paper.\n3 Methods\nThe layer-wise pruning task and the optimization prob-\nlem that pruning algorithms should solve is deﬁned as\nfollows: Assume that a pre-trained transformer model of\ndepth dconsists of Llayers with |L|= d. The layer L0 is\nthe input layer and layer Ld−1 is the layer just before the\nclassiﬁer. Our goal is to develop an algorithm A(L,T,n )\nfor a given ﬁne-tuning task T (e.g. QNLI) that returns\na subset Rn ⊂L with |Rn|= n. We call an algorithm\nAtask-independent if it returns the same solution Rn for\nall tasks T and task-dependent otherwise. The returned\nsubset Rn contains the layers that should be pruned from\nthe model. Therefore, the goal of Ais to ﬁnd a subset\nRn of size n, so that the transformer model with layers\nL\\Rn maximizes some – previously speciﬁed – perfor-\nmance measure M(L\\Rn,T) after being ﬁne-tuned 1 on\nthe task T.\nThe most used layer-wise pruning algorithm is Top-\nlayer Pruning. Sajjad et al. [19] tested many diﬀerent\ntask-independent as well as task-dependent pruning algo-\nrithms and found that Top-layer pruning, which prunes\nthe last layers ﬁrst, works best across many diﬀerent tasks\nand models:\nTop(L,T,n ) = {Ld,Ld−1,...,L d−(n−1)} (1)\nCreating an algorithm Optimal( L,T,n ) that ﬁnds the\noptimal solution for layer-wise pruning is easy if resources\nare not constrained. More precisely, from all possible\ncombinations to prune nlayers, we simply select the com-\nbination of layers that produces the highest performance:\nOptimal(L,T,n ) = argmax\nRn\n{Rn ∈Pn(L) |M(L\\Rn,T)}\n(2)\nwhere P(L) denote the set of all subsets ofLand Pn(L) =\n{X ∈P(L) ||X|= n}. The computational complexity of\nthis algorithm is therefore O\n(d\nn\n)\n.\nFor example, to prune two layers of a BERT model\nwith 12 layers, 66 diﬀerent networks must be evaluated.\nWe show later that at least six layers must be pruned\nfrom models like BERT base or RoBERTabase to be com-\npetitive w.r.t memory consumption and inference time\nagainst state-of-the-art distillation methods. Pruning six\nlayers already leads to 924 diﬀerent conﬁgurations that\nmust be evaluated. For the whole GLUE benchmark,\nthis would take almost half a year using an NVIDIA 3090\nGPU.\n3.1 Greedy-layer Pruning\nTo reduce the complexity of the optimal solution, we\nexploit a greedy approach. Greedy algorithms make a\nlocally-optimal choice in the hope that this leads to a\nglobal optimum. For example, to prune n+ 1 layers, only\na single (locally-optimal) layer to prune must be added\nto the already known solution for pruning n layers. This\nstatement would lead to the following assumption about\nlayer-wise pruning:\nAssumption 3.1 (Locality). If the set Rn (R0 = ∅) is\nthe optimal solution w.r.t. the performance metric M(L\\\nRn,T) for pruning n layers on a given task T and model\nwith layers L, then Rn ⊂Rn+1.\nThe correctness of this assumption will be evaluated in\nan experimental study (section 4.4). Assuming locality,\n1Sajjad et al. [19] already showed that layer-wise pruning meth-\nods work best if executed directly after the pre-training, something\nwe could conﬁrm in our experiments.\n3\nthe search space and therefore the computational com-\nplexity is greatly reduced:\nGLP(L,T,n ) =\n{\nOptimal(L,T, 1) n= 1\nOptimal (L\\Rn−1,T, 1) ⋃Rn−1 else\n(3)\nwith Rn = GLP(L,T,n ).\nIt can be seen that Rn can be calculated by adding\nthe best local solution Optimal (L\\Rn−1,T, 1) to the set\nRn−1 using the locality assumption 3.1. This simpliﬁca-\ntion reduces the complexity from O\n(d\nn\n)\nto O(n×d). This\nalgorithm can easily be distributed on multiple GPUs,\ne.g., if d GPUs are available, the algorithm can be exe-\ncuted in just n steps.\nOur goal is to compress and train models with layer-\nwise pruning just before ﬁne-tuning to (1) allow a ﬁne-\ngrained selection of the performance/speedup tradeoﬀ\nand (2) to avoid the computationally expensive pre-\ntraining phase. It can be seen in eq. (3) that the solution\nfor pruning nlayers also includes the solution for pruning\nn−1 layers. Therefore, if Rn is known, all other pruning\ncombinations Rx with x ≤n can be determined in O(1)\nand the performance/speedup trade-oﬀ can be selected\nindividually without the need to run GLP or pre-training\nagain.\n4 Experimental evaluation\nWe ﬁrst provide results on the GLUE benchmark, com-\nparing Greedy-layer pruning to Top-layer pruning and dif-\nferent distillation methods. Next, we will evaluate the\nvalidity of the main components of GLP, the locality as-\nsumption 3.1 and the performance metric, M.\n4.1 Setup\nImplementation Each experiment is executed on a\nnode with one Nvidia RTX 3090 GPU, an Intel Core i7-\n10700K CPU and 128GB of memory. Our publicly avail-\nable implementation extends the BERT model as well as\nthe RoBERTa model of the Transformer v.4.3.2 library\n(PyTorch 1.7.1) from HuggingFace with Top-layer prun-\ning as well as Greedy-layer pruning. The run glue.py\n2 script from Huggingface is used to evaluate the GLUE\nbenchmark. The source code also provides scripts to eas-\nily setup and reproduce all experiments.\nEvaluation The GLUE benchmark [27] is designed to\nfavor models that share general language knowledge by\nusing datasets with limited training dataset size. It is a\ncollection of 9 diﬀerent datasets containing single sentence\n2https://huggingface.co/transformers/v2.5.0/examples.\nhtml\nor sentence pair classiﬁcation as well as a regression task,\nnamely, CoLA [28], SST-2 [22], MRPC [4], STS-B [2],\nQQP 3, MNLI [29], QNLI [17], and RTE [1]. Following\nDevlin et al. [3] we also exclude the problematic WNLI\ndataset from the benchmark 4.\nTo ensure that the layers that are selected by GLP for\npruning also generalize and do not overﬁt the test-set, we\nuse a 15% split of the training set to compute M(L,T).\nAdditionally, we report the median forﬁve runs using dif-\nferent seeds for all our experiments to get a fair compari-\nson following Sanh et al. [20]. We use the following classi-\ncal performance metrics M[27]: the F1-score for MRPC\nand QQP, the Spearman correlation for STSB, the Math-\news correlation for COLA and the accuracy for all other\ndatasets. To ensure that our scores are not the result of\nany undesired side-eﬀect or implementation details, we\nalso executed experiments on Top-layer pruning and all\ndistillation methods using exactly the same code-base, if\na HuggingFace implementation was provided. Otherwise\nwe report the values from the original paper.\nHyperparameters To get a similar speedup with\npruning as with distilled models, the depth of a BERT\nmodel must be reduced to 6 layers (ﬁg. 1). We follow the\nhyperparameter setup from Devlin et al. [3], use a batch\nsize of 32 and ﬁne-tune the base model with a sequence\nlength of 128. Although several optimizers have been ap-\nplied in diﬀerent work to obtain results with high per-\nformance, such as Lagrangian optimizer [11] or Sobolev\ngradient based optimizer [7, 8], they generally cause high\ncomputational costs. Therefore, we applied the AdamW\noptimizer [13] in this work with a learning rate of 2e-\n5, β1 = 0.9 and β2 = 0.999. To ensure reproducability\nof our results, we not only publish the source code to-\ngether with the paper, but we also provide a guild 5 ﬁle\n(guild.yml) which contains the detailed hyperparameter\nsetup we used for each experiment and which should help\nto replicate our experiments.\n4.2 Results on GLUE\nResults for the baseline BERT and RoBERTa, as well as\npruning 2, 4, and 6 layers from those models are shown\nin table 1 and ﬁg. 1 on GLUE. Greedy-layer pruning\noutperforms Top-layer pruning in almost all cases. For\npruning 2 layers from RoBERTa, Top-layer pruning and\nGLP perform equally. Pruning 6-layer is particularly in-\nteresting as it allows a comparison to distillation methods\nw.r.t performance as well as inference time. In this case,\nGLP always outperforms Top-layer pruning, sometimes\n3http://qim.fs.quoracdn.net/quora_duplicate_questions.\ntsv\n4See also https://gluebenchmark.com/faq\n5https://guild.ai\n4\nTable 1: Median of ﬁve runs evaluated for GLP, Top-layer pruning and state-of-the-art task-agnostic distillation\nmethods. The subscript n indicates how many layers are pruned from the given architecture. *denotes that the\nvalues are from the corresponding papers.\nMethod SST-2 MNLI QNLI QQP (F1/Acc) STS-B RTE MRPC (F1/Acc) CoLA GLUE Rel.\nLayer Pruning BERT\nBaseline0 92.8 84.7 91.5 88.0 / 91.0 88.1 64.6 88.7 / 83.6 56.5 81.9 -\nTop2 92.3 83.9 89.8 87.4 / 90.5 88.0 64.3 87.1 / 81.4 57.0 81.2 99.1\nGLP2 (ours) 92.3 83.9 90.7 87.5 / 90.7 87.6 65.3 87.8 / 82.6 57.0 81.5 99.5\nTop4 91.5 83.0 89.1 87.1 / 90.4 88.0 64.3 86.9 / 80.9 49.2 79.9 97.5\nGLP4 (ours) 91.6 83.0 89.8 87.1 / 90.3 87.9 66.1 85.9 / 79.9 54.5 80.7 98.5\nTop6 90.8 81.1 87.4 86.7 / 90.0 87.7 63.5 86.1 / 79.4 37.1 77.6 94.7\nGLP6 (ours) 90.7 81.2 87.7 86.3 / 89.7 87.6 60.6 85.4 / 77.9 45.0 78.1 95.4\nLayer Pruning RoBERTa\nBaseline0 94.4 87.8 92.9 88.4 / 91.3 90.0 73.6 92.1 / 89.0 57.5 84.6 -\nTop2 94.3 87.7 92.5 88.4 / 91.3 89.9 69.7 90.9 / 87.3 58.1 83.9 99.2\nGLP2 (ours) 94.3 87.7 92.8 88.5 / 92.8 89.9 70.0 90.3 / 86.8 57.5 83.9 99.2\nTop4 93.6 87.2 92.4 88.3 / 91.2 89.4 67.9 91.3 / 87.7 54.2 83.0 98.1\nGLP4 (ours) 93.6 87.2 92.4 88.3 / 91.2 89.0 69.0 90.7 / 87.0 54.4 83.1 98.2\nTop6 92.5 84.7 90.9 87.6 / 90.7 88.3 58.1 88.3 / 83.6 46.6 79.6 94.0\nGLP6 (ours) 92.4 85.6 90.7 87.6 / 90.7 88.3 60.6 88.6 / 84.1 51.4 80.6 95.3\nLayerDrop∗\n6 92.5 82.9 89.4 - - - 85.3 / - - - -\nTask-agnostic Distillation\nDistilBERT 90.1 82.3 88.8 86.6 / 89.9 86.3 60.6 88.9 / 84.1 49.4 79.1 -\nMobileBERT 91.9 84.0 91.0 87.5 / 90.5 87.9 64.6 90.6 / 86.8 50.5 81.0 -\nPKD* 92.0 81.0 89.0 70.7 / 88.9 - 65.6 85.0 / 79.9 - - -\nCODIR* 93.6 82.8 90.4 - / 89.1 - 65.6 - / 89.4 53.6 - -\nby a large margin: The diﬀerence in performance is 0.5\nfor pruning BERT and 1.0 percentage points for pruning\nRoBERTa. When compared against LayerDrop – which is\na fairly expensive approach because it must be executed\nduring both pre-training and ﬁnetuning – GLP outper-\nforms it or reaches similar scores.\nOn some tasks, GLP and Top-layer pruning perform\nequally well. For example, when pruning 2 layers from\nBERT on CoLA, the score is exactly the same. As\nan interesting point, we found that in this case, GLP\nprunes exactly the same layers as Top-layer pruning.\nBut, in most of the other cases, solutions are quite dif-\nferent, which indicates that the optimization problem\n(section 3) is not task-independent. For example, the\nlayer-pruning sequence when using GLP for BERT on\nCoLA is {11,10,9,4,0,5}with GLP, which is quite diﬀer-\nent when compared against Top-layer pruning (sequence\n{11,10,9,8,7,6}). The accuracy of GLP 6 is also much\nhigher with 45.0, compare to Top 6 with only 37.1. Note\nthat we provide all layers found by GLP for each task\nand model together with our source code to support fu-\nture research and reproducibility.\nIn table 1 (column Rel.) we additionally show the rel-\native performance compared to its baseline. The relative\nperformance of Greedy-layer pruning w.r.t the baseline is\nFigure 1: Performance on the GLUE dataset (except\nfor WNLI) using diﬀerent models w.r.t its speedup rel-\native to BERT. The dashed line shows how the perfor-\nmance/speedup ratio can be changed dynamically using\nGLP. In this speciﬁc case GLP was applied to a RoBERTa\nmodel, altough GLP is not restricted to this type of\nmodel.\n5\nnot only higher in almost all cases than Top-layer prun-\ning, but results are also more consistent for diﬀerent types\nof models. For example, the diﬀerence of the relative per-\nformance between BERT and RoBERTa is only 0.1% for\nGLP6, but 0.7% for Top6. We believe that one major rea-\nson is that our algorithm selects layers based on the given\ntask and model i.e. is task-dependent whereas Top-layer\npruning is task-independent. This strongly indicates that\nthe proposed Greedy-layer pruning algorithm can also be\nused for models that will be released in the future so that\neven better results can be achieved by simply applying\nGLP to those pre-trained models. It is worth mentioning\nthat the relative performance can be larger than 100% for\nsome datasets. For example, when using GLP to prune\n2 or 4 layers from BERT, the resulting model trained on\nRTE is not only faster and smaller but also outperforms\nthe baseline. A hypothesis for this phenomenon is given\nlater in the discussion section 5.\n4.3 Comparison against Knowledge Dis-\ntillation\nMemory consumption, inference time, and performance\non GLUE for baseline models, distillation models, and\npruning methods are shown in table 2. No GLUE scores\nwere reported for LayerDrop in Fan et al. [5]. Perfor-\nmance and speedup values relative to BERT are addi-\ntionally shown in ﬁg. 1. To measure the inference time,\nwe evaluated each model on the same hardware, using\nthe same frameworks with a batch-size of 1 and perform\nthe evaluation on a CPU following Sanh et al. [20]. Ad-\nditionally, for each method we report the GPU timings,\nmeasured on an RTX 3090 GPU with a batch size of 32.\nIn terms of inference-time and performance (ﬁg. 1),\nGLP is indeed on-par when compared to distillation\nmethods. For example it outperforms TinyBert as shown\nin ﬁg. 1 w.r.t. performance and inference time. GLP\noutperforms PKD [23] in 5 out of 6 cases and outper-\nforms CODIR [24] in 3 out of 7 cases. Therefore, it can\nbe concluded that GLP is on-par with state-of-the-art\nknowledge distillation methods. Additionally, pruning al-\nlows for balancing the speed-up/performance trade-oﬀ by\nchanging the number of layers that should be pruned from\nthe model (dashed line in ﬁg. 1) which is another huge ad-\nvantage as we motivated already in section 1. In terms\nof memory consumption and performance, GLP is very\nclose to DistilBERT and TinyBERT, but the winner here\nis MobileBERT, which outperforms all methods. Mobile-\nBERT was introduced especially for mobile devices and so\nis extremely parameter eﬃcient as it is a thin and deep ar-\nchitecture. On the other hand, inference of MobileBERT\nis slow on GPUs because parallelism on GPUs can be\nbarely exploited with such an architecture. Nevertheless,\nthis aspect shows the limitation of layer-wise pruning in\nthat the width is ﬁxed and only the depth of a network\ncan be changed. More insights on this will be discussed\nin section 5.\nExecuting GLP on an average downstream-task of\nGLUE takes about 20h on a single 3090 GPU on the\nQNLI task. A similar setup in the cloud would be a single\nTesla P100 GPU which costs approximately 32$ 6. After\nthe execution of GLP arbitrary performance / speedup\nratios can be selected by the user through pruning a cer-\ntain number of layers, without the need of an additional\npre-training phase.\nOn the other hand, to select an individual perfor-\nmance / speedup ratio with knowledge-distillation, a new\nstudent-model must be pre-trained from scratch and ﬁne-\ntuned on the downstream task. Therefore, an additional\nand computationally expensive pre-training phase is nec-\nessary. A ﬁnancially cheap methodology to pre-train\nmodels is presented by Izsak et al. [9] and would cost\nabout 400$ if executed on the proposed hardware setup\nwhich is 12×more expensive than the execution of GLP.\n4.4 Evaluating the Locality Assump-\ntion 3.1\nIn this subsection we evaluate the two main components\nof GLP, namely, the eﬀect of the locality assumption 3.1\nand the inﬂuence of the performance metric Mon the\nresults.\nWe assess ﬁrst if a locally optimal choice also leads to a\nglobally optimal solution (assumption 3.1) by evaluating\nall of the possible two-layer combinations pruning from\na transformer model. The Optimal solution is shown in\nﬁg. 2, next to GLP and Top-layer pruning. If assump-\ntion 3.1 were to be correct, GLP should ﬁnd a solution\nwith the same validation error for each dataset and model.\nResults for the optimal solution ( Optimal in ﬁg. 2) are\ncomputed on a ﬁxed seed as it is important to use the\nsame seed in order to properly evaluate if the solutions\nfor GLP and Optimal are exactly the same. In all other\nexperiments we used multiple runs with diﬀerent seeds\nand report the median to exclude outliers and to ensure\nthat GLP does not depend on a single seed. We also\nwant to clarify that the computational resources to com-\npute the optimal solution grows exponentially with the\nnumber of layers that should be pruned. Unfortunately,\npruning of more than two layers becomes computation-\nally unfeasible and therefore values for pruning two layer\nare reported.\nFigure 2 shows that solutions found by GLP almost\nalways outperform Top-layer pruning by a large margin,\nwhich indicates the robustness of task-dependent algo-\nrithms. The exception in this case is for RoBERTA on\n6Pricing from https://cloud.google.com/compute/\ngpus-pricing. Accessed 07/09/2021\n6\nTable 2: Comparison of base models, state-of-the-art layer-wise pruning and distillation models w.r.t inference time,\nmemory consumption and performance on GLUE. *denotes that this compression method uses a teacher model in\nthe ﬁne-tuning stage whereas all other models directly train on the downstream task [25]\nInference [s]\nMethod Model GLUE Params. CPU GPU\nBase BERT 81.9 108M 141 3.0\nRoBERTa 84.6 125M 149 3.0\nTask-agnostic distillation DistilBERT 79.1 67M 81 1.7\nMobileBERT 81.0 25M 91 9.1\nNon task-agnostic distillation TinyBERT ∗\n6 79.4 67M 74 1.7\nLayer-wise pruning LayerDrop 6 - 82M 70 1.7\nBERT+Top6 77.6 67M 73 1.7\nBERT+GLP6 (ours) 78.4 67M 73 1.7\nRoBERTa+Top6 79.6 82M 70 1.7\nRoBERTa+GLP6 (ours) 80.6 82M 70 1.7\nSTS-B , where the solution found with GLP is slightly\nworse than Top-layer pruning. GLP can ﬁnd the opti-\nmal solution in many cases, for example, when applied to\nBERT (ﬁg. 2a), that is the case for SST-2, RTE, STS-B,\nMNLI and CoLA. In others, even though GLP does not\nprovide such optimal solution it is a close one (such as\nQQP, MRPC and QNLI for BERT). Figure 2b provides\nthe same evaluation for RoBERTa. This detailed analysis\nproves that the locality assumption is an (although very\ngood) approximation. Even so, the assumption is useful\nbecause it drastically reduces the size of the search space\nwhich grows exponentially with the number of layers that\nshould be pruned as we already described in section 3.\nFinally, we can also evaluate the choice for the perfor-\nmance metric Mused by the GLP algorithm (eq. (3))\nto ﬁnd locally-optimal solutions. More precisely, we re-\nplaced Mwith the validation loss and pruned layers that\nproduce the lowest loss value. We found in our experi-\nments that this setup worsened the overall performance of\nGLP on the GLUE benchmark. For example, for pruning\na BERT model, the score is worsened on average by 0 .7\npercentage points.\n5 Discussion and Conclusions\nIn this paper, Greedy-layer pruning is introduced to com-\npress transformer models dynamically just before the ﬁne-\ntuning stage to allow a ﬁne-grained selection of the perfor-\nmance/speedup tradeoﬀ. Our focus is on pruning layers\njust before the ﬁne-tuning stage as the pre-training stage\nis computationally expensive. Therefore, the execution\nof GLP is 12×less expensive than state-of-the-art knowl-\nedge distillation methods. The GLP algorithm presented\nhere implements a greedy approach maintaining very high\nperformance while needing only a modest budget compu-\ntational setup. This is empirically evaluated in section 4\nby comparing GLP against state-of-the-art pruning and\ndistillation methods. An interesting result is that GLP\nsometimes improved the performance of its baseline (ta-\nble 1) although it is a smaller and faster model. This\nphenomenon also occurs for other types of models as al-\nready shown before [16] and could be the result of layers\nthat exist in neural networks that worsen the accuracy of\nthe trained model [15]. In this sense, GLP acts as a neu-\nral architecture search algorithm that prunes layers that\nworsen the accuracy ﬁrst. Therefore, the performance of\nthe pruned model can be larger than the performance of\nits baseline.\nSome limitations of the proposed methodology include\nthat GLP changes only the depth of networks by prun-\ning layers from transformer models, but it is not possible\nto reduce the width of models to further compress mod-\nels. Additionally, GLP is task-dependent and therefore\npruning must be executed with the task-speciﬁc dataset.\nFinally, for very deep networks with more than 50 layers\n(such as CNNs) the proposed greedy strategy becomes\ncomputationally expensive, and more optimization strate-\ngies would be needed.\n6 Future Work\nWe have shown that the locality assumption 3.1 is a good\napproximation to ﬁnd comparable solutions to expen-\nsive distillation systems, but with the advantage of doing\nso when only limited computational resources are avail-\nable. Nevertheless, there is still room for improvement\nw.r.t performance as shown in our experimental evalu-\nation (ﬁg. 2). One interesting direction to further im-\nprove the performance could be to evaluate other diﬀer-\nent methodologies than the ones presented in the greedy\napproach. Secondly, the performance metric Mused by\nGLP for evaluation could be a topic for further research.\n7\n(a) BERT\n(b) RoBERTa\nFigure 2: Comparison of the optimal solution and the solutions found by Top-layer pruning and greedy pruning for\nremoving two-layers from a BERT model and a RoBERTa model. We executed it for all datasets of GLUE and\npresent the results for the ﬁrst datasets in those graphs.\nFor example, Izsak et al. [9] rejects training candidates al-\nready after a few training steps if a given threshold is not\nreached. A similar approach for GLP could be a topic of\ninterest to speed up the compression stage and decrease\ncomputational costs even further.\nReferences\n[1] Bentivogli, L., Clark, P., Dagan, I., Giampiccolo, D.,\n2009. The ﬁfth pascal recognizing textual entailment\nchallenge., in: TAC.\n[2] Cer, D., Diab, M., Agirre, E., Lopez-Gazpio, I., Spe-\ncia, L., 2017. Semeval-2017 task 1: Semantic tex-\ntual similarity multilingual and crosslingual focused\nevaluation, in: Proceedings of the 11th International\nWorkshop on Semantic Evaluation (SemEval-2017),\npp. 1–14.\n[3] Devlin, J., Chang, M.W., Lee, K., Toutanova, K.,\n2019. BERT: Pre-training of deep bidirectional\ntransformers for language understanding, in: Pro-\nceedings of the 2019 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Asso-\nciation for Computational Linguistics, Minneapolis,\nMinnesota. pp. 4171–4186.\n[4] Dolan, W.B., Brockett, C., 2005. Automatically\nconstructing a corpus of sentential paraphrases, in:\nProceedings of the Third International Workshop on\nParaphrasing (IWP2005).\n[5] Fan, A., Grave, E., Joulin, A., 2020. Reducing trans-\nformer depth on demand with structured dropout,\nin: International Conference on Learning Represen-\ntations.\n[6] Ganesh, P., Chen, Y., Lou, X., Khan, M.A., Yang,\nY., Sajjad, H., Nakov, P., Chen, D., Winslett, M.,\n2021. Compressing large-scale transformer-based\nmodels: A case study on bert. Transactions of the\nAssociation for Computational Linguistics 9, 1061–\n1080.\n[7] Goceri, E., 2019. Diagnosis of alzheimer’s disease\nwith sobolev gradient-based optimization and 3d\nconvolutional neural network. International journal\nfor numerical methods in biomedical engineering 35,\ne3225.\n[8] Goceri, E., 2020. Capsnet topology to classify tu-\n8\nmours from brain images and comparative evalua-\ntion. IET Image Processing 14, 882–889.\n[9] Izsak, P., Berchansky, M., Levy, O., 2021. How to\ntrain bert with an academic budget , 10644–10652.\n[10] Jiao, X., Yin, Y., Shang, L., Jiang, X., Chen, X., Li,\nL., Wang, F., Liu, Q., 2020. Tinybert: Distilling bert\nfor natural language understanding, in: Proceedings\nof the 2020 Conference on Empirical Methods in Nat-\nural Language Processing: Findings, pp. 4163–4174.\n[11] Kervadec, H., Dolz, J., Yuan, J., Desrosiers, C.,\nGranger, E., Ayed, I.B., 2019. Constrained deep\nnetworks: Lagrangian optimization via log-barrier\nextensions. arXiv preprint arXiv:1904.04205 .\n[12] Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen,\nD., Levy, O., Lewis, M., Zettlemoyer, L., Stoyanov,\nV., 2019. Roberta: A robustly optimized bert pre-\ntraining approach. arXiv preprint arXiv:1907.11692\n.\n[13] Loshchilov, I., Hutter, F., 2018. Decoupled weight\ndecay regularization, in: International Conference on\nLearning Representations.\n[14] Michel, P., Levy, O., Neubig, G., 2019. Are sixteen\nheads really better than one?, in: Wallach, H.M.,\nLarochelle, H., Beygelzimer, A., d’Alch´ e Buc, F.,\nFox, E.B., Garnett, R. (Eds.), NeurIPS, pp. 14014–\n14024.\n[15] Peer, D., Stabinger, S., Rodriguez-Sanchez, A.,\n2021a. Conﬂicting bundles: Adapting architectures\ntowards the improved training of deep neural net-\nworks, in: Proceedings of the IEEE/CVF Winter\nConference on Applications of Computer Vision, pp.\n256–265.\n[16] Peer, D., Stabinger, S., Rodr´ ıguez-S´ anchez, A.,\n2021b. Limitation of capsule networks. Pattern\nRecognition Letters 144, 68–74.\n[17] Rajpurkar, P., Zhang, J., Lopyrev, K., Liang, P.,\n2016. Squad: 100,000+ questions for machine com-\nprehension of text, in: Proceedings of the 2016 Con-\nference on Empirical Methods in Natural Language\nProcessing, pp. 2383–2392.\n[18] Rouhou, A.C., Dhiaf, M., Kessentini, Y., Salem,\nS.B., 2021. Transformer-based approach for joint\nhandwriting and named entity recognition in histor-\nical document. Pattern Recognition Letters .\n[19] Sajjad, H., Dalvi, F., Durrani, N., Nakov, P., 2020.\nPoor man’s bert: Smaller and faster transformer\nmodels. arXiv preprint arXiv:2004.03844 .\n[20] Sanh, V., Debut, L., Chaumond, J., Wolf, T., 2019.\nDistilbert, a distilled version of bert: smaller, faster,\ncheaper and lighter. arXiv preprint arXiv:1910.01108\n.\n[21] Shi, Y., Xiao, Y., Quan, P., Lei, M., Niu, L., 2021.\nDocument-level relation extraction via graph trans-\nformer networks and temporal convolutional net-\nworks. Pattern Recognition Letters 149, 150–156.\n[22] Socher, R., Perelygin, A., Wu, J., Chuang, J., Man-\nning, C.D., Ng, A., Potts, C., 2013. Recursive\ndeep models for semantic compositionality over a\nsentiment treebank, in: Proceedings of the 2013\nConference on Empirical Methods in Natural Lan-\nguage Processing, Association for Computational\nLinguistics, Seattle, Washington, USA. pp. 1631–\n1642. URL: https://www.aclweb.org/anthology/\nD13-1170.\n[23] Sun, S., Cheng, Y., Gan, Z., Liu, J., 2019. Patient\nknowledge distillation for bert model compression,\nin: Proceedings of the 2019 Conference on Empir-\nical Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP), pp. 4323–\n4332.\n[24] Sun, S., Gan, Z., Fang, Y., Cheng, Y., Wang, S.,\nLiu, J., 2020a. Contrastive distillation on intermedi-\nate representations for language model compression,\nin: Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npp. 498–508.\n[25] Sun, Z., Yu, H., Song, X., Liu, R., Yang, Y., Zhou,\nD., 2020b. MobileBERT: a compact task-agnostic\nBERT for resource-limited devices, in: Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, Association for Compu-\ntational Linguistics. pp. 2158–2170. URL: https://\nwww.aclweb.org/anthology/2020.acl-main.195.\n[26] Voita, E., Talbot, D., Moiseev, F., Sennrich, R.,\nTitov, I., 2019. Analyzing multi-head self-attention:\nSpecialized heads do the heavy lifting, the rest can\nbe pruned., in: ACL (1), Association for Computa-\ntional Linguistics. pp. 5797–5808.\n[27] Wang, A., Singh, A., Michael, J., Hill, F., Levy,\nO., Bowman, S., 2018. Glue: A multi-task bench-\nmark and analysis platform for natural language un-\nderstanding, in: Proceedings of the 2018 EMNLP\nWorkshop BlackboxNLP: Analyzing and Interpret-\ning Neural Networks for NLP, pp. 353–355.\n9\n[28] Warstadt, A., Singh, A., Bowman, S.R., 2018. Neu-\nral network acceptability judgments. arXiv preprint\narXiv:1805.12471 .\n[29] Williams, A., Nangia, N., Bowman, S., 2018. A\nbroad-coverage challenge corpus for sentence under-\nstanding through inference, in: Proceedings of the\n2018 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long Pa-\npers), Association for Computational Linguistics. pp.\n1112–1122. URL: http://aclweb.org/anthology/\nN18-1101.\n[30] Worsham, J., Kalita, J., 2020. Multi-task learning\nfor natural language processing in the 2020s: Where\nare we going? Pattern Recognition Letters 136, 120–\n126.\n[31] Zhang, Z., Qi, F., Liu, Z., Liu, Q., Sun, M.,\n2021. Know what you don’t need: Single-shot meta-\npruning for attention heads. AI Open 2, 36–42.\n10",
  "topic": "Speedup",
  "concepts": [
    {
      "name": "Speedup",
      "score": 0.8091596364974976
    },
    {
      "name": "Computer science",
      "score": 0.7994444370269775
    },
    {
      "name": "Transformer",
      "score": 0.7417001128196716
    },
    {
      "name": "Pruning",
      "score": 0.5820222496986389
    },
    {
      "name": "Distillation",
      "score": 0.5250254273414612
    },
    {
      "name": "Inference",
      "score": 0.4720628559589386
    },
    {
      "name": "Language model",
      "score": 0.4662301540374756
    },
    {
      "name": "Artificial intelligence",
      "score": 0.46354931592941284
    },
    {
      "name": "Machine learning",
      "score": 0.4630449414253235
    },
    {
      "name": "Parallel computing",
      "score": 0.26195216178894043
    },
    {
      "name": "Agronomy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I190249584",
      "name": "Universität Innsbruck",
      "country": "AT"
    }
  ]
}