{
  "title": "Accelerating Transformer Inference for Translation via Parallel Decoding",
  "url": "https://openalex.org/W4385571586",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2712222932",
      "name": "Andrea Santilli",
      "affiliations": [
        "Sapienza University of Rome"
      ]
    },
    {
      "id": "https://openalex.org/A5056681825",
      "name": "Silvio Severino",
      "affiliations": [
        "Sapienza University of Rome"
      ]
    },
    {
      "id": "https://openalex.org/A3049521926",
      "name": "Emilian Postolache",
      "affiliations": [
        "Sapienza University of Rome"
      ]
    },
    {
      "id": "https://openalex.org/A3213504843",
      "name": "Valentino Maiorca",
      "affiliations": [
        "Sapienza University of Rome"
      ]
    },
    {
      "id": "https://openalex.org/A2944186059",
      "name": "Michele Mancusi",
      "affiliations": [
        "Sapienza University of Rome"
      ]
    },
    {
      "id": "https://openalex.org/A2171966278",
      "name": "Riccardo Marin",
      "affiliations": [
        "Bernstein Center for Computational Neuroscience Tübingen",
        "University of Tübingen"
      ]
    },
    {
      "id": "https://openalex.org/A393897811",
      "name": "Emanuele Rodolà",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2962784628",
    "https://openalex.org/W3197876970",
    "https://openalex.org/W2963979492",
    "https://openalex.org/W2903193068",
    "https://openalex.org/W3100753857",
    "https://openalex.org/W4289302788",
    "https://openalex.org/W3013701218",
    "https://openalex.org/W4312659766",
    "https://openalex.org/W4324297016",
    "https://openalex.org/W2767206889",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W3172257043",
    "https://openalex.org/W3039805635",
    "https://openalex.org/W3000840023",
    "https://openalex.org/W3066373881",
    "https://openalex.org/W3168577192",
    "https://openalex.org/W3214532454",
    "https://openalex.org/W4301187301",
    "https://openalex.org/W3015162217",
    "https://openalex.org/W2983981554",
    "https://openalex.org/W2963736842",
    "https://openalex.org/W4225598930",
    "https://openalex.org/W2890501761",
    "https://openalex.org/W2963532001",
    "https://openalex.org/W3177172118",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4322759582",
    "https://openalex.org/W3195725782",
    "https://openalex.org/W2988975212",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W4382203562",
    "https://openalex.org/W4287694131",
    "https://openalex.org/W4377079846",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W1506342804",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W4319166707",
    "https://openalex.org/W3174851730",
    "https://openalex.org/W4303874710",
    "https://openalex.org/W3082928416",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2889326796",
    "https://openalex.org/W2512924740",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W3135335819",
    "https://openalex.org/W2997607995",
    "https://openalex.org/W2964115871",
    "https://openalex.org/W3175665465",
    "https://openalex.org/W3118026775",
    "https://openalex.org/W4299512035",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2251994258",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W3204339250",
    "https://openalex.org/W3200388885",
    "https://openalex.org/W2121879602",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W4310561894",
    "https://openalex.org/W3168817639",
    "https://openalex.org/W3017454464",
    "https://openalex.org/W3169369929",
    "https://openalex.org/W2963250244"
  ],
  "abstract": "Autoregressive decoding limits the efficiency of transformers for Machine Translation (MT). The community proposed specific network architectures and learning-based methods to solve this issue, which are expensive and require changes to the MT model, trading inference speed at the cost of the translation quality. In this paper, we propose to address the problem from the point of view of decoding algorithms, as a less explored but rather compelling direction. We propose to reframe the standard greedy autoregressive decoding of MT with a parallel formulation leveraging Jacobi and Gauss-Seidel fixed-point iteration methods for fast inference. This formulation allows to speed up existing models without training or modifications while retaining translation quality. We present three parallel decoding algorithms and test them on different languages and models showing how the parallelization introduces a speedup up to 38% w.r.t. the standard autoregressive decoding and nearly 2x when scaling the method on parallel resources. Finally, we introduce a decoding dependency graph visualizer (DDGviz) that let us see how the model has learned the conditional dependence between tokens and inspect the decoding procedure.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 12336–12355\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nAccelerating Transformer Inference for Translation via Parallel Decoding\nAndrea Santilli1, Silvio Severino1, Emilian Postolache1, Valentino Maiorca1,\nMichele Mancusi1, Riccardo Marin2,3, Emanuele Rodolà1\n1Sapienza University of Rome 2University of Tübingen\n3Tübingen AI Center\nsantilli@di.uniroma1.it\nAbstract\nAutoregressive decoding limits the efficiency\nof transformers for Machine Translation (MT).\nThe community proposed specific network\narchitectures and learning-based methods to\nsolve this issue, which are expensive and re-\nquire changes to the MT model, trading infer-\nence speed at the cost of the translation quality.\nIn this paper, we propose to address the prob-\nlem from the point of view of decoding algo-\nrithms, as a less explored but rather compelling\ndirection. We propose to reframe the standard\ngreedy autoregressive decoding of MT with\na parallel formulation leveraging Jacobi and\nGauss-Seidel fixed-point iteration methods for\nfast inference. This formulation allows to speed\nup existing models without training or modifi-\ncations while retaining translation quality. We\npresent three parallel decoding algorithms and\ntest them on different languages and models\nshowing how the parallelization introduces a\nspeedup up to 38% w.r.t. the standard autore-\ngressive decoding and nearly 2x when scaling\nthe method on parallel resources. Finally, we\nintroduce a decoding dependency graph visual-\nizer (DDGviz) that let us see how the model has\nlearned the conditional dependence between to-\nkens and inspect the decoding procedure.\n1 Introduction\nIn recent years there have been dramatic improve-\nments in Machine Translation (MT) (Edunov et al.,\n2018; Liu et al., 2020) thanks to the transition to\nneural models and the advent of the Transformer ar-\nchitecture (Vaswani et al., 2017). These models can\nproduce high-quality translations while being ex-\ntremely parallelizable during training. However,\nTransformers are used sequentially at inference\ntime, generating one token per time (i.e., sending\neach token as input for the next autoregressive itera-\ntion). This process of autoregressive inference ham-\npers the efficiency of neural machine translation\nsystems in terms of latency, limiting applications\nand portability. Considering that these systems are\nextensively used in production multiple times to\nproduce new translations (e.g., Google Translate1,\nDeepL Translator2), even a minor speedup would\nbe beneficial in the long run, especially if the trans-\nlation is done on embedded devices.\nTo address this issue, the community proposed\nad-hoc trained models specific for parallel ma-\nchine translation under the umbrella term of Non-\nAutoregressive Machine Translation models (NAT)\n(Gu et al., 2018). These models produce the trans-\nlation in parallel but require (i) a complete reengi-\nneering of the MT system, (ii) extensive training\nresources and (iii) complex design choices like dis-\ntillation from larger autoregressive models. These\nrequirements are quite demanding and not easily\nsatisfiable. For example, production systems are\nheavily optimized for hardware and software and\neven introducing a minimal modification requires\nnon-trivial human effort (Wu et al., 2016; Kim et al.,\n2019). Furthermore, training a new model from\nscratch is not always possible due to non-released\ntraining data or low-resource languages having few\nor lacking parallel corpora.\nIn this paper, we propose to address the problem\nof parallel machine translation with an orthogo-\nnal approach consisting in novel decoding algo-\nrithms that work in parallel and can be used on\ntop of existing autoregressive modelsfor MT. We\novercome previous limitations with a flexible and\ngeneric method that does not require any modifica-\ntion to the model or costly retraining. Specifically,\ninspired by previous successes in speeding up feed-\nforward computation for image generation (Song\net al., 2021b), we reframe the greedy autoregres-\nsive decoding for MT as a system of nonlinear\nequations solvable in parallel. This simple formu-\nlation speeds up the decoding procedure by using\nfixed-point iteration methods like Jacobi and Gauss-\nSeidel while having mathematical guarantees on\n1https://translate.google.com/\n2https://www.deepl.com/\n12336\nFigure 1: On the left, the classical Autoregressive Decoding for MT. The target sentence is produced token-by-token\nsequentially, sending the partial result as input for the next autoregressive iteration up to the length m of the target.\nOn the right Parallel Decoding proposed in this paper. This method changes only the decoding algorithm (orange\nblock) and is usable on top of any autoregressive model without modifications. Parallel Decoding algorithms resolve\nthe whole sentence or a block of b tokens in parallel: initial tokens (PAD tokens) are gradually refined with k steps\nuntil a stopping condition is reached. Crucially, k ⩽ m with quality guarantees and overall decoding speedups.\nthe quality of the translation. A high-level descrip-\ntion of the method is available in (Fig. 1). Our\ncontributions can be summarized as the following:\n• We reframe the standard greedy autoregres-\nsive decoding procedure in MT with a parallel\nformulation, introducing three parallel decod-\ning algorithms (PJ, PGJ, HGJ) and a stopping\ncondition that preserves translation quality.\n• We perform extensive experiments with dif-\nferent transformer sizes (base and large) and\ndatasets, showing speedups up to 38% in time,\nobtaining a nearly 2× speedup when scaling\nthe model on parallel resources while preserv-\ning quality. To the best of our knowledge,\nthis is one of the first studies to introduce a\nspeedup in multilingual machine translation.\n• We introduce a decoding dependency graph\nvisualizer (DDGviz) to inspect the learned to-\nkens’ conditional dependence and when paral-\nlel decoding is effective.\nAll the code is publicly released3.\n2 Related Work\nGu et al. (2018) first introduced Non-\nAutoregressive Translation models (NAT) as\nad-hoc trained models capable of producing the\ntranslation all at once in parallel. With NATs, it\nis possible to consistently reduce the latency and\nspeed up the translation at the expense of a slightly\nworse translation quality due to the multimodality\nproblem (i.e., we lose the dependency between\ntokens in the target output). Finding a tradeoff\nbetween translation quality and speed is an active\n3https://github.com/teelinsan/\nparallel-decoding\nresearch direction, with current methods trying to\nfill the gap in terms of translation quality (Geng\net al., 2021; Savinov et al., 2022). Nevertheless,\nall proposed NAT models are learning-based and\nrequire different tricks to reach the quality of\nautoregressive models (Gu and Kong, 2021). The\nmost common is the sequence-level knowledge\ndistillation of large autoregressive models into\nparallel models (Kim and Rush, 2016). Other\napproaches include defining alternative training\nobjectives (Ghazvininejad et al., 2020a; Saharia\net al., 2020; Du et al., 2021; Huang et al., 2021),\narchitectures that model dependencies between\noutput sentence tokens (Ghazvininejad et al., 2019;\nQian et al., 2021; Song et al., 2021a; Gu and Kong,\n2021; Song et al., 2022) or multi-iteration methods\n(Ghazvininejad et al., 2020b; Kasai et al., 2020;\nHao et al., 2021; Geng et al., 2021; Savinov et al.,\n2022; Huang et al., 2022; Xia et al., 2022) that\napply iterative refinements to a translation, trading\nsome speed for greater quality. In our approach,\nwe also employ iterative refinements of solutions\nto non-linear equations, but we do not perform\nany training or modification to the model . Other\nworks that require retraining or modifications to\nthe model add additional decoding heads (Stern\net al., 2018) or use shallow decoders (Kasai et al.,\n2021). We refer the reader to Xiao et al. (2022)\nfor a thorough survey on NAT methods. Further\northogonal approaches use specialized hardware\n(TPU) with low-precision calculations (Wu et al.,\n2016) or software optimizations (Kim et al., 2019).\nIn the context of Grammatical Error Correction,\nSun et al. (2021) recently proposed aggressive\nparallel decoding, assuming that the model output\nis similar to the input. More recently, inspiring\nour work, Song et al. (2021b) showed that it is\npossible to parallelize feedforward computations\n12337\nPJ PGJ HGJ\nFigure 2: Parallel Decoding algorithms: PJ resolves the whole sequence in parallel iteratively. PGJ resolves\nblocks in parallel; once a block is finished, it moves on to the next one and decodes it again in parallel (in figure\nb = 3). HGJ decodes the sentence in parallel as PGJ up to a certain length h; afterwards, it goes autoregressively\nuntil [EOS] token is generated. Decoding actually happens in sub-word tokens (not depicted here).\nby thinking of them as a system of non-linear\nequations. They parallelized the backpropagation\nof RNNs, feedforward layers and autoregressive\ngenerative models on images. We extend the\napproach defined on dense pixel prediction to the\ndiscrete conditional token generation in MT. While\nthis work was under submission and anonymity\nperiod, Leviathan et al. (2022), Chen et al. (2023)\nand Kim et al. (2023) concurrently proposed\ndecoding approaches that speed up inference\nof a large transformer model by using another\nsmaller model to draft tokens. Compared to these\napproaches our method requires just an existing\nautoregressive model (no matter the size) and\nmathematically guarantees the output quality. In\nthe next Section we describe the method.\n3 Method\nIn this Section, we introduce notations, develop the\ntheory behind Parallel Decoding, present three al-\ngorithms (Fig. 2), and discuss the initialization and\nstopping conditions for the proposed approaches.\n3.1 Notation\nThe goal of MT is to translate a sentence x in a\nsource language (e.g., Italian) with its translation\ny in the target language (e.g., English). Source\nand target sentences are generally tokenized in\nwords or subwords (Kudo and Richardson, 2018;\nSchuster and Nakajima, 2012; Sennrich et al., 2016;\nKudo, 2018); here, we use the subfix notation\nx = (x1, . . . , xn) and y = (y1, . . . , ym) to in-\ndicate specific tokens in the sequence. We also\nuse the notation x1:n to indicate a slice of a se-\nquence as a shorthand of x = (x1, . . . , xn). From\na probabilistic perspective, an MT model estimates\npθ(y |x). Once an MT model has been trained,\nthe inference phase is traditionally performed by\nsampling tokens from the model probability con-\nditioned on the input sequence x and previously\ngenerated tokens (y1, . . . , yi−1):\npθ(yi |y1, . . . , yi−1, x) . (1)\nDifferent sampling strategies are employed (e.g.,\nGreedy, Top-K, Top-p (Kool et al., 2020; Holtzman\net al., 2020)) alongside search strategies that esti-\nmate the total conditional probability (e.g., Greedy\nsearch, Beam search (Reddy, 1977)). The most\nstraightforward strategy, Greedy Search, selects\nthe element yi of a sequence with:\nyi = arg maxpθ(yi |y1:i−1, x). (2)\nGiven the formalization above, a standard autore-\ngressive setting runs m inference steps sequentially\nto generate an output sequence of m elements.\nParallel Decoding. Given Equation (2), it is pos-\nsible to write the greedy decoding procedure on all\ntokens as:\n\n\n\ny1 = arg maxpθ(y1 |x)\ny2 = arg maxpθ(y2 |y1, x)\n...\nym = arg maxpθ(ym |y1:m−1, x)\n(3)\nDefining f(yi, y1:i−1, x) = yi −arg maxpθ(yi |\ny1:i−1, x) , we can rewrite the system of Equations\n(3) as:\n\n\n\nf(y1, x) = 0\nf(y2, y1, x) = 0\n...\nf(ym, y1:m−1, x) = 0\n(4)\nThis system has m non-linear equations (each equa-\ntion employ a neural network) with m variables.\n3.2 Parallel Decoding Algorithms\nThe autoregressive decoding implicitly solves the\nsystem of Equations (4) by substitution, i.e., given\nthe [BOS] token and the input sentence x, it solves\nequations from first to last, progressively replacing\nthe resolved variables. In this paper, we rely on\nJacobi and Gauss-Seidel (GS) fixed-point iteration\nmethods (Ortega and Rheinboldt, 1970) to solve\nin parallel system (4) until a stopping condition is\nreached. This formulation is particularly flexible\nand has several advantages: Firstly, it is completely\nagnostic to the underlying MT model used; Sec-\nondly, it can be analyzed with analytical tools and\n12338\nhas guarantees of convergence to the exact solu-\ntion for system (4); Thirdly, it can be potentially\nextended by drawing from the numerical methods\nliterature for non-linear equations solving methods\n(Saad, 2003). We see that, with the proper stopping\ncondition, it is possible to have quality guarantees\nover the output. We present here three algorithms\n(PJ, PGJ, HGJ) that leverage these fixed-point iter-\nation methods to speedup decoding in MT.\nParallel Jacobi (PJ) Decoding. First, we pro-\npose Algorithm 1. This algorithm works by ini-\ntializing a draft translation for the whole target\nsentence and then iteratively translating the whole\nsentence in parallel until the stopping condition is\ntriggered. This is equivalent to solving system (4)\nwith Jacobi, hence the name of the method.\nParallel GS-Jacobi (PGJ) Decoding. Decoding\nthe whole target sentence in parallel may intro-\nduce difficulties in inferring long dependencies be-\ntween tokens since the underlying model is trained\nto model the conditional distribution of a token\ngiven the previous tokens. In general, we observed\nthat shorter dependencies are easily predicted since\ndecoding happens at the sub-word level, and the\nmodel can decode sub-word unities in parallel\nrather than the whole sentence. To this end, we\npropose Algorithm 2, called GS-Jacobi, that splits\nthe sentence into contiguous b-dimensional blocks.\nStarting from the first one, it decodes in parallel\nall its elements. Once a block is finished or the\nstopping condition within the block is triggered,\nthe algorithm performs a sequential (Gauss-Seidel)\nstep and proceeds with (Jacobi) decoding on the\nnext one.\nHybrid GS-Jacobi (HGJ) Decoding. Algo-\nrithms 1 and 2 assume to know beforehand the num-\nber of equations m (i.e., the target length). This is\nnot usually the case for MT, where the model dy-\nnamically controls the length through the emission\nof a special end-of-sentence token [EOS] . To over-\ncome this issue, we propose a flexible Hybrid Algo-\nrithm 3 that mixes PGJ computations with standard\nautoregressive decoding. This algorithm performs\nparallel GS-Jacobi decoding up to a certain prefixed\nlength h. If the [EOS] token is generated within\na block, then the algorithm stops, returning the\ntranslation up to [EOS] . Otherwise, the algorithm\nconcludes the translation by reaching the[EOS] to-\nken with standard autoregressive decoding. In this\ncase, the length h regulates the trade-off between\nAlgorithm 1 Parallel Jacobi Decoding\nInput: x = (x1, . . . , xn), pθ\nOutput: y = (y1, . . . , ym)\n1: y ←INIT T(x)\n2: m ←len(y)\n3: for i = 1to m do\n4: o ←copy(y1:m)\n5: y1:m ←arg max(pθ(y1:m|y1:m, x))\n6: stop ←STOP C(o, y1:m)\n7: if stop then\n8: break\n9: end if\n10: end for\n11: return y\nparallel and sequential computation, limiting the\nwaste of resources beyond [EOS].\n3.3 Initialization and Stopping\nOur algorithms share two components: the initial-\nization procedure and the stopping condition.\nInitialization INIT T(x). The initialization pro-\ncedure is a function that inputs the source sentence\nand produces an initial draft translation as output.\nIn this paper we experimented with a simple initial-\nization procedure that initialize the translation with\nall [PAD] tokens. This choice is fast and doesn’t\ndepend on the underlying MT model. We leave as\nfuture work the research of different initialization\nprocedures to further speedup the decoding.\nStopping Condition STOP C(yk−1, yk). The\nstopping condition is a function that takes as in-\nput the previous-iteration sentence yk−1 and the\ncurrent-iteration sentence yk and decides whether\nto stop the algorithm or not. This function is crucial\nsince it regulates the trade-off between speedup and\ntranslation quality. In this paper we introduce as\nstopping condition for MT:\nyk−1 −yk = 0 (5)\ni.e., the sentence from the previous step has not\nchanged. This stop condition allows for preserving\nquality and quickening translations simultaneously.\n3.4 Quality Guarantees\nCompared to NAT methods which do not have any\nquality guarantee since a novel parallel model is\ntrained from scratch, our formulation guarantees\nto have the same quality of using autoregressive\ndecoding with the same MT model. System (4)\nis known in literature as a triangular system of m\nequations with m variables, this characterization\nallows to state an important property.\n12339\nDecoding Algorithm en→de de →en en →ro ro →en\nSpeed BLEU Speed BLEU Speed BLEU Speed BLEU\nOpus\nGreedy Autoregressive 1.00× 28.24 1.00× 33.10 1.00× 27.41 1.00× 37.01\nBeam Search (beam = 5) 0.71× 28.68 0.72× 33.92 0.70× 27.61 0.72× 37.84\nPJ Decoding 0.73× 28.24 0.75× 33.10 0.66× 27.41 0.66× 37.01\nPGJ Decoding (b = 5) 1.28× 28.24 1.32× 33.10 1.33× 27.41 1.29× 37.01\nPGJ Decoding (b = 3) 1.34× 28.24 1.37× 33.10 1.38× 27.41 1.35× 37.01\nHGJ Decoding (b = 3) 1.34× 28.24 1.37× 33.10 1.38× 27.41 1.35× 37.01\nMBart50\nGreedy Autoregressive 1.00× 23.97 1.00× 31.58 1.00× 24.99 1.00× 34.77\nBeam Search (beam = 5) 0.76× 24.93 0.77× 32.61 0.77× 25.31 0.76× 35.16\nPJ Decoding 0.88× 23.97 0.88× 31.58 0.86× 24.99 0.85× 34.77\nPGJ Decoding (b = 5) 0.98× 23.97 0.98× 31.58 0.97× 24.99 0.99× 34.77\nPGJ Decoding (b = 3) 1.06× 23.97 1.08× 31.58 1.03× 24.99 1.04× 34.77\nHGJ Decoding (b = 3) 1.05× 23.97 1.07× 31.58 1.01× 24.99 1.02× 34.77\nTable 1: Comparison of parallel decoding algorithms (highlighted in grey) with sequential decoding using Opus\n(CPU) and MBart50 (GPU) on WMT14 and WMT16. Speed is measured in time w.r.t. the autoregressive baseline.\nWMT17 IITB IWSLT15 FLORES\nEn-Fi En-Hi En-Vi En-It En-Fr\nDec. Algorithm Speed ← → ← → ← → ← → ← →\nPJ Iters 1.04 × 1.04× 1.04× 1.04 × 1.06× 1.03× 1.02× 1.04× 1.03× 1.03×\nTime 0.86 × 0.88× 0.89× 0.89× 0.87× 0.86× 0.85× 0.86× 0.85× 0.85×\nPGJ (b=3) Iters 1.07 × 1.09× 1.09× 1.09× 1.10× 1.07 × 1.07× 1.08× 1.08× 1.11×\nTime 1.01 × 1.05× 1.05× 1.07× 1.04× 1.02× 1.02× 1.03× 1.03× 1.05×\nHGJ (b=3) Iters 1.05 × 1.07× 1.07× 1.07× 1.07× 1.06× 1.07× 1.06× 1.05× 1.07×\nTime 1.01 × 1.03× 1.04× 1.05× 1.03× 1.01× 1.01× 1.02× 1.01× 1.03×\nTable 2: Comparison over different languages in terms of speedup and iterations on MBart50. Arrows indicate the\ndirection of translation. Qualitative results and BLEU scores are available in the appendix D.\nProposition 1. Algorithms 1, 2, 3 converge and\nyield the same results of greedy autoregressive de-\ncoding in at most m parallel iterations, for any\ninitialization and providing stopping condition (5).\nWe refer the reader to Song et al. (2021b) for\na formal proof. Intuitively, with m steps the al-\ngorithm used the same number of iterations of au-\ntoregressive, hence the final solution is the same\nregardless the initialization. In this worst case, the\nwall-clock time is the same but in general the al-\ngorithm reach the stopping condition earlier with a\nlower wall-clock time and overall speedup.\n3.5 DDG viz\nEquation 1 models the dependency between tokens\nin the decoding phase. In the classical autoregres-\nsive mode, each token depends on all the previous\nones for the generation. However, it is possible to\nshow that this dependency is actually relaxed (i.e.,\nnot all tokens depends on all the previous ones),\nthus it would be interesting to visualize the actual\ndistribution pθ(yi |·, x) learned by an existing MT\nmodel. To this end, we build the Decoding Depen-\ndency Graph visualizer (DGGviz) to visualize the\ndependency graph of tokens in the decoding phase.\nIn the standard autoregressive decoding this graph\nis a fully-connected chain where the i-th token is\nconnected to all the previous tokens, starting from\nthe encoding x: to decode yi you need to decode\nfirst y1, . . . , yi−1. Instead we show that there are\nskipping connections between independent tokens\nthat can be visualized with DGG viz. We detail\nDGGviz with an example in section 4.3.\n4 Experiments\n4.1 Experimental Settings\nDatasets. We evaluate our approach using stan-\ndard evaluation datasets proposed for parallel MT\n(Gu et al., 2018): WMT14 English-German [En-\nDe], WMT16 English-Romanian [En-Ro] (Bo-\njar et al., 2014, 2016). Additionally, we tested\nour method on different language pairs with vary-\ning (low-medium) resources: IWSLT15 (English-\nVietnamese [ En-Vi]) (Tran et al., 2015), IITB\n(English-Hindi [ En-Hi]) (Kunchukuttan et al.,\n2018), WMT17 (English-Finnish [En-Fi]) (Bojar\net al., 2017), FLORES-101 (English-Italian [En-It];\nEnglish-French [En-Fr]) (Goyal et al., 2022). All\nthe datasets are evaluated in both directions.\nEvaluation. All the evaluations are performed\nusing the official test split for each dataset, down-\nloaded using Huggingface dataset library (Lhoest\net al., 2021). No training or hyperparameters tun-\n12340\nMethod Requirements WMT14 Efficiency\nArch Loss seq-KD Speed ↑ BLEU ↑ Train FLOPs ↓ Total FLOPs ↓ FLOPs / Speed ↓\nParallel Decoding - HGJ (Ours) No No No 1.34 × 28.24 0 2.53e+13 1.89e+13\nSUNDAE †(Savinov et al., 2022) Yes No No 1.4 × 28.46 5.27e+21 5.27e+21 3.77e+21\nShallowDec (12-1) (Kasai et al., 2021) Yes No No 1.4 × 26.90 1.02e+19 1.02e+19 7.30e+18\nSemi-NAT (Wang et al., 2018) Yes No Yes 1.5 × 26.90 1.55e+17 1.55e+17 1.03e+17\nDisCo (Kasai et al., 2020) Yes Yes Yes, Big 3.5 × 27.34 4.06e+19 4.06e+19 1.16e+19\nDSLP (Huang et al., 2021) Yes Yes Yes 14.8 × 27.02 1.93e+19 1.93e+19 1.31e+18\nF-V AE (Gu and Kong, 2021) Yes Yes Yes, Big 16.5 × 27.49 4.06e+19 4.06e+19 2.46e+18\nTable 3: Comparison of different methods for parallel MT on WMT14 En-De. Results are ordered by speed,\nhighlighted in green the two highest BLEU scores, †indicates diffusion models. Existing methods require training,\narchitecture modifications, additional losses to force parallel translation, and distillation from an additional MT\ntransformer model (\"Big\" indicates the size). Details on FLOPs computation are available in the Appendix C.\ning is performed. We use SacreBLEU to evalu-\nate the translation quality (Papineni et al., 2002;\nPost, 2018). We measure speedup in wall-clock\ntime and iterations w.r.t. the same autoregressive\nmodel. GPU times are calculated after calling\ntorch.cuda.synchronize(). All the ex-\nperiments were performed by caching the past Keys\nand Values of the transformer to further speed up\nthe computation (Ramachandran et al., 2017) and\nin the online inference setting with batch size equal\nto 1. For the Jacobi and GS-Jacobi algorithms, we\nassume to know beforehand the length m of the\ntarget and measure the speedup in the ideal condi-\ntion. For the Hybrid GS-Jacobi algorithm, we set h\nequal to the maximum (i.e., the stopping condition\nis triggered within a parallel block) to decouple the\neffective speedup regardless of the length produced\nby the initialization function (see Section 3.2). We\nremark that HGJ does not assume to know before-\nhand the target length and is applicable to real MT\ntranslation scenarios.\nModel Configuration. We tested transformer\nmodels in the two standard configurations: base\n(512 model dimension, 6 attention layers for both\nencoder and decoder) and big (1024 model dimen-\nsion, 12 attention layers for both encoder and de-\ncoder). We used pretrained models of Opus (Tiede-\nmann and Thottingal, 2020) for the former and\nMBart50 (Tang et al., 2020) for the latter. Opus is a\ntransformer base model (74M parameters) trained\non language pairs from the homonymous dataset\n(Zhang et al., 2020). MBart50 is a large multilin-\ngual transformer model fine-tuned for translation\non 50 languages (610M parameters). We tested the\nmodels on CPU since this is the default environ-\nment for MT models in production, except for the\nmodel MBart50 which runs on GPU. We run the\nexperiments on a standard 16-core machine, except\nfor the scaling experiments. Additional specifica-\ntions are available in Appendix B\n4.2 Algorithms Comparison\nIn Table 1 we compare the proposed parallel decod-\ning algorithms with the standard sequential autore-\ngressive decoding baselines. As we can observe,\nthe fastest algorithms are PGJ Decoding (b=3) and\nHGJ Decoding (b=3) which are up to 34% and 38%\ntimes faster on Opus and up to 5% and 8% faster on\nMBart50, depending on the language pair. We note\nalso that results empirically show that all the paral-\nlel decoding algorithms guarantee the same quality\nof greedy autoregressive decoding, as evidenced by\nthe unchanged BLEU scores. This is an experimen-\ntal verification of the formal Proposition 1. The\ntable also shows that the Beam Search algorithm\nwith a beam size of 5 generally performs better in\nterms of BLEU score, although at a cost of speed.\nThis difference in terms of BLEU is expected, as\nbeam search is a heuristic search strategy, while\nour method is a decoding algorithm. We discussed\nbetter this aspect in the \"Beam Search\" paragraph.\nNevertheless, beam search is ∼30% slower than\ngreedy autoregressive and 63% to 68% slower than\nPGJ, depending on the model and language pair.\nThis means that the proposed parallel algorithms\nallow trading a little translation quality (e.g., on\nen→ro the difference between beam search and\nparallel decoding algorithms in BLEU is just 0.20\npoints) for greater decoding speed.\nAnother aspect to note is that the algorithms PJ\nand PGJ (b=5) are sometimes slower than greedy\nautoregressive. There are several factors that can\ninfluence the actual wall-clock time like how the\nunderlying hardware schedule and execute the vari-\nous operations, which might vary according to the\narchitecture and the workload. In particular, longer\nsequences (e.g., the whole sentence in PJ or blocks\nof 5 tokens in PGJ) may require more memory\nto store, and the CPU/GPU may have to perform\nmore memory accesses, which can slow down the\ncomputation (although theoretically it should hap-\npen in parallel). In the end, these computational\n12341\noverheads slow down the actual execution. This\nis also the case for the difference in speedups be-\ntween MBart50 and Opus. We better investigated\nthis aspect in the section \"Computational Scaling\"\nand report in the appendix results on a different\narchitecture, with also results in terms of iterations\nspeedups which are architecture agnostic.\n4.3 Analysis and Validation\nCross Languages. In order to demonstrate the\nrobustness of our decoding algorithms with respect\nto the translation languages, we leveraged the mul-\ntilingual capabilities of the MBart50 model and\nselected a diverse range of language pairs for eval-\nuation. The results, presented in Table 2, show that\nboth PGJ and HGJ achieve a consistent speedup in\ncomparison to the autoregressive decoding method,\nwith an improvement ranging from 2-7% for PGJ\nand 1-5% for HGJ, regardless of the language pair\nused. Additionally, we observed a speedup in terms\nof iterations of 7-11% for PGJ and 5-7% for HGJ.\nThese findings indicate that our algorithms have\nthe potential to match or surpass the speedup in\nterms of wall-clock time by fully exploiting this\nsaving in terms of iterations. We note that, simi-\nlar to the previous experiment, PJ suffers from an\noverhead problem. To the best of our knowledge,\nthis is one of the first studies that have achieved a\nspeedup in multilingual machine translation, con-\ncurrent with the work of Song et al. (2022), while\nthis latter is significantly different in spirit and re-\nquirements (NAT model). We leave BLEU scores\nin the Appendix D for space constraints together\nwith qualitative results in different languages.\nComputational Scaling. In Figure 3, we present\nan analysis of the scalability of our proposed meth-\nods in relation to increasing computational re-\nsources. Starting with 8 cores, our methods demon-\nstrate a slight improvement in terms of wall-clock\ntime for PGJ and HGJ, with speedups of 1.11 and\n1.09 respectively. On the other hand, this amount\nof resources is too restricting for PJ which needs to\nfit the whole sentence and thus achieve a score of\n0.46 due to the aforementioned overhead problem.\nAs the resources are increased, our method demon-\nstrates the ability to effectively leverage hardware\nand significantly reduce decoding time, while the\nautoregressive baseline is constrained by sequential\nprocessing. With 122 cores, a substantial speedup\nof 1.98×and 1.99×is achieved for PGJ and HGJ\nrespectively, while the autoregressive baseline is\nFigure 3: Scaling experiments on WMT16 En-De with\nPGJ and HGJ blocks = 3. Increasing the number of\navailable resources (number of CPU cores) allows the\nmethods to decrease the parallel overheads. As a result,\nthe speedup increases and the methods scale.\nbounded by sequential processing at 1.00×. It is\nimportant to note that this experiment does not\nsimulate a real production system, but rather it is\nmeant to show what results can be achieved when\nthe underlying computation is properly optimized\nto run in parallel. In our case, we simulated this\nsetting with increasing cores, nevertheless similar\nresults can be achieved with additional software op-\ntimizations to further reduce latency and overheads\n(Ahmed et al., 2022; Kim et al., 2019) and increase\nthe speed gain with parallel-optimized computa-\ntions. Overall this experiment serves as a proof\nof concept for the capabilities of parallel decod-\ning in contexts with limited overhead and shows a\npromising direction for further improvements.\nComparison with NATs. Table 3 reports the\ncomparison of our parallel decoding algorithm with\na selection of NAT methods for parallel MT. Fol-\nlowing prior works, we report for each method the\nspeedup relative to the autoregressive transformer\nbase baseline from their original paper (Xiao et al.,\n2022). It is worth noting that, although these meth-\nods can achieve higher speedups, they are very\ndemanding in terms of computational resources\nwhich must be accounted for in a fair comparison.\nTo estimate quantitatively this cost, we evaluated\nthe number of floating point operations (FLOPs)\nrequired for training and inference on WMT14.\nResults show that our method HGJ uses the least\nnumber of computational resources, even consid-\nering the additional cost at inference time. Relat-\ning the speedup obtained with the used resources\n(FLOPs/speed), our method still achieves the best\n12342\ncost-benefit ratio. Furthermore, NATs generally de-\ngrade the translation quality if compared to their au-\ntoregressive baseline. On the contrary, our method\nmathematically guarantees the same quality of au-\ntoregressive decoding, which is higher than stan-\ndard NAT models.\nSUNDAE achieves BLEU of 28.46, but requires\nmore resources than training RoBERTa (Liu et al.,\n2019) on 16 TPUs (see Appendix C). Other meth-\nods require further elaborate techniques like pro-\nfound architectural changes, additional losses to\nforce parallel translation and sequence-level distil-\nlation from large autoregressive transformers (Gu\nand Kong, 2021). Our approach is a decoding\nmethod that does not involve any training or modi-\nfication to the model and can be used to speed up\nexisting models on standard desktop hardware.\nSpeedup Analysis. We provide here a prelim-\ninary analysis of the factors responsible for the\nobserved speedup in our method. We first distin-\nguish between two types of speedup: wall-clock\nspeedup and iterations speedup. The former is\nprimarily driven by the parallelization capability\nof our method, as demonstrated in the \"Compu-\ntational Scaling\" section. With parallel decoding,\nunderlying operations can be optimized and fused\nto be executed fastly. Compared to Sheng et al.\n(2023), our method allows parallelizing sequence\noperations (\"row-by-row\" setting). The latter in-\nstead may vary consequently to several factors\n(e.g., model/vocabulary size, training data, lan-\nguage, etc). For this reason, we experimented with\nseveral variations of these factors (models Trans-\nformer Base vs. Big, vocabularies 58K Marian vs.\n250K MBart50, languages, and hardware). While\nit is challenging to decouple different elements,\nour analysis point out several interesting insights.\nFor example, we observed that iteration results on\nMBart50 are generally higher compared to Marian\n(Tables 2-6), possibly due to the finer-grained to-\nkenization of MBart50. We also hypothesize that\nlanguage and linguistic features, such as inflection-\nally rich or agglutinative/gendered languages, may\ninfluence iteration speedups. To facilitate this type\nof analysis, we developed DDGviz, which we be-\nlieve will be useful for research in this area.\nVisualizing Parallel Decoding. In previous ex-\nperiments, we demonstrated that parallel decod-\ning is feasible. This suggests that the dependency\nlearned by the model between certain tokens is re-\nlaxed, as some tokens can be decoded in parallel.\nFigure 4: DDGviz. Visualization of the translation En-\nRo: \"How satisfied are the Romanian couples: men\nversus women\"→\"Cât de satisfacuti sunt cuplurile ro-\nmanes, ti: b˘arbat, ii împotriva femeilor\". (Highlighted to-\nkens decoded in parallel). On top: the Decoding Depen-\ndency Graph, omitting redundant edges on non-parallel\ntokens to ease visualization. On bottom : DDG viz\nshows at each Parallel Jacobi iteration (vertical axis)\nwhich tokens have been generated in parallel (horizontal\naxis) with the corresponding probability (cell number).\nAnalyzing and understanding when this happens\nallows shedding light on the behavior of existing\nmodels and a separate study focused on this is-\nsue would be needed. In this work, we lay the\nground for a such study introducing the necessary\ninspection tools. While we have already introduced\nDDGviz in Section 3.5, in this experiment we show\nhow it works and how it can be used with a prac-\ntical example. In summary, the DDG viz visual-\nizer allows to show the real decoding distribution\npθ(yi |·, x) learned by a MT model. This decod-\ning distribution is plotted as a graph, where a con-\nnection indicates the dependency pθ(yi | ·), by\nusing Parallel Jacobi decoding. At each PJ decod-\ning iteration (vertical axis of Figure 4), DDG viz\nkeeps track of which tokens have been correctly\ndecoded w.r.t. the gold autoregressive reference of\nthe model, showing the tokens correctly decoded\nand the probability of each one (horizontal axis).\nFigure 4 shows DDG viz applied on an example.\nThe example shows that for y4 = _sa it is possible\nto decode more than one token in parallel y5 = tis,\ny6 = fa, hence here the decoding of y6 does not\ndepend on the decoding of y5 - pθ(y6 |y1:4, x).\nWe observed this phenomenon frequently, explain-\ning the speedups in the previous experiments. The\n12343\nexample also shows that the model is able to decode\nfive tokens in parallel after y7 = _cu. This is a pe-\nculiar case since the model, given \"How satisfi_\",\nis generating all at once \"_ed are the Romanian\ncouples\" (proposed here in English for better read-\nability, original version in Romanian is available in\nFigure). This example indeed shows how DDGviz\ncan be used to highlight possible biases encoded in\nthe model as it is not clear how the model can be\nso confident (see cell probability) that after \"satis-\nfied\" the most straightforward tokens to decode are\n\"Romanian couples\" (Chang et al., 2019; Savoldi\net al., 2021). We leave other use cases for future\nworks and show in Appendix D several visualiza-\ntions with equally interesting phenomena.\n5 Conclusions\nIn this paper, we showed that is possible to speed\nup existing machine translation models by simply\nchanging the decoding algorithm with a parallel\nformulation. We introduced three parallel decod-\ning methods which achieve consistent speedups\nwithout requiring any training, modifications, or\nquality loss. Our solution is orthogonal to previ-\nous approaches proposed in literature which often\nentail demanding requirements in terms of data,\ncomputational resources, and engineering effort.\nAlthough our method is not without shortcomings,\nit is a first valuable step toward integrating paral-\nlel decoding algorithms into any model. This is\nparticularly relevant in limited-resource scenarios\nwhere NATs are not a viable option and to speed\nup any transformer model, especially fine-grained\nor character-level models (Edman et al., 2023). We\nbelieve that further advancements in this area, in-\ncluding the exploration of optimal initialization\nprocedures and stopping conditions, as well as the\nuse of alternative parallel solvers for non-linear\nequations, will close the gap with learning-based\ntechniques and continue to improve the efficiency\nand effectiveness of parallel decoding algorithms.\nAcknowledgements\nWe would like to thank Sébastien Bratières for\nhis throughout feedback provided on this project.\nThis work is supported by Translated with an\nImminent Research Grant, ERC Starting Grant\nNo. 802554 (SPECGEO), and PRIN 2020 project\nn.2020TA3K9N \"LEGO.AI\". Riccardo Marin is\nalso supported by an Alexander von Humboldt\nFoundation Research Fellowship.\nLimitations\nThe proposed algorithms allow to speed up an exist-\ning model out-of-the-box, without any modification\nor retraining. However, there are some considera-\ntions to bear in mind when using parallel decoding\nin order to have a speedup in terms of wall-clock\ntime. Firstly, as the name implies, the method ex-\necutes the decoding phase in parallel. Therefore,\nto appreciate the speedup one should be able to\nrun computations in parallel. Using parallel decod-\ning without parallel resources or parallel-optimized\nsoftware may increase wall-clock time due to over-\nheads, leading to a waste of computation. This is\nfurther discussed in Section 4.3 \"Computational\nScaling\". The reported wall-clock time results are\nthus to be considered within the scope of the exper-\nimental setup proposed in this paper and they may\nvary depending on the underlying hardware and\nsoftware. Secondly, the method allows speedup of\nthe decoding by scaling on parallel resources. This\nimplies an additional computational cost during the\ninference phase to achieve a speedup. While using\nparallel decoding, one should consider a trade-off\nbetween the desired acceleration and the utiliza-\ntion of computational resources. Thirdly, since our\nmethod performs the decoding in parallel, as for\nNAT systems, it is difficult to combine it with Beam\nSearch. Beam Search is inherently a dynamic pro-\ngramming algorithm and it is not possible to effi-\nciently maximize the joint probability of the large\nsearch space without using sequential intermediate\ncomputations. We better explain this aspect in the\nnext paragraph.\nBeam Search. Beam search is widely employed\nto enhance the translation quality in MT (Sutskever\net al., 2014; Bahdanau et al., 2015) as well as in\nother domains such as audio (Reddy, 1977; Pos-\ntolache et al., 2023). However, it is an inherently\nsequential procedure that stores partial joint prob-\nabilities of the entire sequence (beams) while pro-\ngressing with autoregressive decoding. Determin-\ning the maximal joint probability of all sequences\nin parallel is a challenging task, equivalent to a\nfull maximum a posteriori (MAP) estimation. This\nis an open research problem and it is also an is-\nsue for NAT methods. NAT methods patch up this\nlimitation with sequence-level KD which has the\nadvantage of \"not requiring any beam search at\ntest-time\" (Kim and Rush, 2016) thanks to learn-\ning and distillation from large models. Since our\n12344\nmethod is a decoding algorithm, we cannot use the\nsame approach without learning. Nevertheless, the\nquality guarantee allows our methods to have per-\nformance on par with greedy autoregressive and\ngenerally better than a NAT model. We think of\nour method, not as a replacement for beam search,\nbut rather as a way to obtain a speedup at inference\ntime that is a middle ground between autoregressive\ngreedy decoding (high quality, no requirements, no\nspeed) and NATs (quality compromises, increasing\nrequirements with increasing speed). Future works\nmight address the quality gap with beam search\nby combining parallel decoding with alternative\ntechniques like Minimum Bayes Risk (Eikema and\nAziz, 2020).\nEthics Statement\nIncreasing the inference speed of MT can positively\nimpact society by giving people a fast and good\ntranslation. This will enable people from differ-\nent language backgrounds to communicate with\neach other and help remove cultural and trade bar-\nriers. As demonstrated by comparing the number\nof FLOPs in Table 3, our method uses fewer re-\nsources compared to alternatives and thus has a\nsmaller carbon footprint, making it a more sustain-\nable choice (Strubell et al., 2019). Furthermore,\nsince our method does not involve training proce-\ndures or change the quality of results, we do not\nintroduce any societal bias (e.g. racism, sexism,\nhomophobia) into the translations. The latter, how-\never, can be introduced through data in the training\nof the backbone autoregressive models and NATs.\nIt is the task of those who train these models to\nmitigate this problem. DDGviz can also help inves-\ntigate and visualize some potential harmful biases\nencoded in the model like in Figure 4.\nReferences\nIbrahim Ahmed, Sahil Parmar, Matthew Boyd, Michael\nBeidler, Kris Kang, Bill Liu, Kyle Roach, John\nKim, and Dennis Abts. 2022. Answer fast: Ac-\ncelerating bert on the tensor streaming processor.\nIn 2022 IEEE 33rd International Conference on\nApplication-specific Systems, Architectures and Pro-\ncessors (ASAP), pages 80–87. IEEE.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In 3rd International\nConference on Learning Representations, ICLR 2015,\nSan Diego, CA, USA, May 7-9, 2015, Conference\nTrack Proceedings.\nOndˇrej Bojar, Christian Buck, Christian Federmann,\nBarry Haddow, Philipp Koehn, Johannes Leveling,\nChristof Monz, Pavel Pecina, Matt Post, Herve Saint-\nAmand, Radu Soricut, Lucia Specia, and Aleš Tam-\nchyna. 2014. Findings of the 2014 workshop on\nstatistical machine translation. In Proceedings of the\nNinth Workshop on Statistical Machine Translation,\npages 12–58, Baltimore, Maryland, USA. Associa-\ntion for Computational Linguistics.\nOndˇrej Bojar, Rajen Chatterjee, Christian Federmann,\nYvette Graham, Barry Haddow, Shujian Huang,\nMatthias Huck, Philipp Koehn, Qun Liu, Varvara\nLogacheva, Christof Monz, Matteo Negri, Matt Post,\nRaphael Rubino, Lucia Specia, and Marco Turchi.\n2017. Findings of the 2017 conference on machine\ntranslation (wmt17). In Proceedings of the Sec-\nond Conference on Machine Translation, Volume 2:\nShared Task Papers, pages 169–214, Copenhagen,\nDenmark. Association for Computational Linguis-\ntics.\nOndˇrej Bojar, Rajen Chatterjee, Christian Federmann,\nYvette Graham, Barry Haddow, Matthias Huck, An-\ntonio Jimeno Yepes, Philipp Koehn, Varvara Lo-\ngacheva, Christof Monz, Matteo Negri, Aurelie\nNeveol, Mariana Neves, Martin Popel, Matt Post,\nRaphael Rubino, Carolina Scarton, Lucia Specia,\nMarco Turchi, Karin Verspoor, and Marcos Zampieri.\n2016. Findings of the 2016 conference on machine\ntranslation. In Proceedings of the First Conference\non Machine Translation, pages 131–198, Berlin, Ger-\nmany. Association for Computational Linguistics.\nKai-Wei Chang, Vinodkumar Prabhakaran, and Vicente\nOrdonez. 2019. Bias and fairness in natural language\nprocessing. In Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Process-\ning and the 9th International Joint Conference on\nNatural Language Processing (EMNLP-IJCNLP):\nTutorial Abstracts, Hong Kong, China. Association\nfor Computational Linguistics.\nCharlie Chen, Sebastian Borgeaud, Geoffrey Irving,\nJean-Baptiste Lespiau, Laurent Sifre, and John\nJumper. 2023. Accelerating large language model\ndecoding with speculative sampling.\nCunxiao Du, Zhaopeng Tu, and Jing Jiang. 2021. Order-\nagnostic cross entropy for non-autoregressive ma-\nchine translation. In International Conference on\nMachine Learning, pages 2849–2859. PMLR.\nLukas Edman, Gabriele Sarti, Antonio Toral, Gert-\njan van Noord, and Arianna Bisazza. 2023. Are\ncharacter-level translations worth the wait? compar-\ning character- and subword-level models for machine\ntranslation.\nSergey Edunov, Myle Ott, Michael Auli, and David\nGrangier. 2018. Understanding back-translation at\nscale. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 489–500, Brussels, Belgium. Association for\nComputational Linguistics.\n12345\nBryan Eikema and Wilker Aziz. 2020. Is MAP decoding\nall you need? the inadequacy of the mode in neural\nmachine translation. In Proceedings of the 28th Inter-\nnational Conference on Computational Linguistics,\npages 4506–4520, Barcelona, Spain (Online). Inter-\nnational Committee on Computational Linguistics.\nXinwei Geng, Xiaocheng Feng, and Bing Qin. 2021.\nLearning to rewrite for non-autoregressive neural ma-\nchine translation. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, pages 3297–3308, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nMarjan Ghazvininejad, Vladimir Karpukhin, Luke\nZettlemoyer, and Omer Levy. 2020a. Aligned cross\nentropy for non-autoregressive machine translation.\nIn Proceedings of the 37th International Conference\non Machine Learning, volume 119 of Proceedings\nof Machine Learning Research , pages 3515–3523.\nPMLR.\nMarjan Ghazvininejad, Omer Levy, Yinhan Liu, and\nLuke Zettlemoyer. 2019. Mask-predict: Parallel de-\ncoding of conditional masked language models. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 6112–\n6121, Hong Kong, China. Association for Computa-\ntional Linguistics.\nMarjan Ghazvininejad, Omer Levy, and Luke Zettle-\nmoyer. 2020b. Semi-autoregressive training im-\nproves mask-predict decoding. arXiv preprint\narXiv:2001.08785.\nNaman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-\nJen Chen, Guillaume Wenzek, Da Ju, Sanjana Kr-\nishnan, Marc’Aurelio Ranzato, Francisco Guzmán,\nand Angela Fan. 2022. The Flores-101 evaluation\nbenchmark for low-resource and multilingual ma-\nchine translation. Transactions of the Association for\nComputational Linguistics, 10:522–538.\nJiatao Gu, James Bradbury, Caiming Xiong, Victor O.K.\nLi, and Richard Socher. 2018. Non-autoregressive\nneural machine translation. In International Confer-\nence on Learning Representations.\nJiatao Gu and Xiang Kong. 2021. Fully non-\nautoregressive neural machine translation: Tricks of\nthe trade. In Findings of the Association for Com-\nputational Linguistics: ACL-IJCNLP 2021 , pages\n120–133, Online. Association for Computational Lin-\nguistics.\nYongchang Hao, Shilin He, Wenxiang Jiao, Zhaopeng\nTu, Michael Lyu, and Xing Wang. 2021. Multi-task\nlearning with shared encoder for non-autoregressive\nmachine translation. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 3989–3996, Online.\nAssociation for Computational Linguistics.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2020. The curious case of neural text de-\ngeneration. In International Conference on Learning\nRepresentations.\nChenyang Huang, Hao Zhou, Osmar R. Zaïane, Lili\nMou, and Lei Li. 2021. Non-autoregressive transla-\ntion with layer-wise prediction and deep supervision.\nCoRR, abs/2110.07515.\nXiao Shi Huang, Felipe Perez, and Maksims V olkovs.\n2022. Improving non-autoregressive translation mod-\nels without distillation. In International Conference\non Learning Representations.\nJungo Kasai, James Cross, Marjan Ghazvininejad, and\nJiatao Gu. 2020. Non-autoregressive machine trans-\nlation with disentangled context transformer. In Pro-\nceedings of the 37th International Conference on Ma-\nchine Learning, volume 119 of Proceedings of Ma-\nchine Learning Research, pages 5144–5155. PMLR.\nJungo Kasai, Nikolaos Pappas, Hao Peng, James Cross,\nand Noah Smith. 2021. Deep encoder, shallow\ndecoder: Reevaluating non-autoregressive machine\ntranslation. In International Conference on Learning\nRepresentations.\nSehoon Kim, Karttikeya Mangalam, Jitendra Malik,\nMichael W. Mahoney, Amir Gholami, and Kurt\nKeutzer. 2023. Big little transformer decoder.\nYoon Kim and Alexander M. Rush. 2016. Sequence-\nlevel knowledge distillation. In Proceedings of the\n2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 1317–1327, Austin,\nTexas. Association for Computational Linguistics.\nYoung Jin Kim, Marcin Junczys-Dowmunt, Hany Has-\nsan, Alham Fikri Aji, Kenneth Heafield, Roman\nGrundkiewicz, and Nikolay Bogoychev. 2019. From\nresearch to production and back: Ludicrously fast\nneural machine translation. In Proceedings of the\n3rd Workshop on Neural Generation and Transla-\ntion, pages 280–288, Hong Kong. Association for\nComputational Linguistics.\nWouter Kool, Herke van Hoof, and Max Welling. 2020.\nAncestral gumbel-top-k sampling for sampling with-\nout replacement. Journal of Machine Learning Re-\nsearch, 21(47):1–36.\nTaku Kudo. 2018. Subword regularization: Improv-\ning neural network translation models with multiple\nsubword candidates. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 66–75,\nMelbourne, Australia. Association for Computational\nLinguistics.\nTaku Kudo and John Richardson. 2018. Sentencepiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing.\nCoRR, abs/1808.06226.\n12346\nAnoop Kunchukuttan, Pratik Mehta, and Pushpak Bhat-\ntacharyya. 2018. The IIT Bombay English-Hindi\nparallel corpus. In Proceedings of the Eleventh In-\nternational Conference on Language Resources and\nEvaluation (LREC 2018), Miyazaki, Japan. European\nLanguage Resources Association (ELRA).\nYaniv Leviathan, Matan Kalman, and Yossi Matias.\n2022. Fast inference from transformers via spec-\nulative decoding.\nQuentin Lhoest, Albert Villanova del Moral, Yacine\nJernite, Abhishek Thakur, Patrick von Platen, Suraj\nPatil, Julien Chaumond, Mariama Drame, Julien Plu,\nLewis Tunstall, Joe Davison, Mario Šaško, Gun-\njan Chhablani, Bhavitvya Malik, Simon Brandeis,\nTeven Le Scao, Victor Sanh, Canwen Xu, Nicolas\nPatry, Angelina McMillan-Major, Philipp Schmid,\nSylvain Gugger, Clément Delangue, Théo Matus-\nsière, Lysandre Debut, Stas Bekman, Pierric Cis-\ntac, Thibault Goehringer, Victor Mustar, François\nLagunas, Alexander Rush, and Thomas Wolf. 2021.\nDatasets: A community library for natural language\nprocessing. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning: System Demonstrations, pages 175–184, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nXiaodong Liu, Kevin Duh, Liyuan Liu, and Jianfeng\nGao. 2020. Very deep transformers for neural ma-\nchine translation. arXiv preprint arXiv:2008.07772.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nJ.M. Ortega and W.C. Rheinboldt. 1970. Iterative So-\nlution of Nonlinear Equations in Several Variables.\nClassics in Applied Mathematics. Society for Indus-\ntrial and Applied Mathematics (SIAM, 3600 Market\nStreet, Floor 6, Philadelphia, PA 19104).\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Pytorch:\nAn imperative style, high-performance deep learning\nlibrary. In Advances in Neural Information Process-\ning Systems 32, pages 8024–8035. Curran Associates,\nInc.\nMatt Post. 2018. A call for clarity in reporting BLEU\nscores. In Proceedings of the Third Conference on\nMachine Translation: Research Papers, pages 186–\n191, Belgium, Brussels. Association for Computa-\ntional Linguistics.\nEmilian Postolache, Giorgio Mariani, Michele Mancusi,\nAndrea Santilli, Cosmo Luca, Emanuele Rodola, et al.\n2023. Latent autoregressive source separation. In\nProceedings of the AAAI Conference on Artificial\nIntelligence.\nLihua Qian, Hao Zhou, Yu Bao, Mingxuan Wang, Lin\nQiu, Weinan Zhang, Yong Yu, and Lei Li. 2021.\nGlancing transformer for non-autoregressive neural\nmachine translation. In Proceedings of the 59th An-\nnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 1993–2003, Online. Association\nfor Computational Linguistics.\nPrajit Ramachandran, Tom Le Paine, Pooya Khor-\nrami, Mohammad Babaeizadeh, Shiyu Chang, Yang\nZhang, Mark A. Hasegawa-Johnson, Roy H. Camp-\nbell, and Thomas S. Huang. 2017. Fast genera-\ntion for convolutional autoregressive models. CoRR,\nabs/1704.06001.\nRaj Reddy. 1977. Speech understanding systems: A\nsummary of results of the five-year research effort .\nCarnegie Mellon University.\nYousef Saad. 2003. Iterative methods for sparse linear\nsystems. SIAM.\nChitwan Saharia, William Chan, Saurabh Saxena, and\nMohammad Norouzi. 2020. Non-autoregressive ma-\nchine translation with latent alignments. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n1098–1108, Online. Association for Computational\nLinguistics.\nNikolay Savinov, Junyoung Chung, Mikolaj Binkowski,\nErich Elsen, and Aaron van den Oord. 2022. Step-\nunrolled denoising autoencoders for text generation.\nIn International Conference on Learning Representa-\ntions.\nBeatrice Savoldi, Marco Gaido, Luisa Bentivogli, Mat-\nteo Negri, and Marco Turchi. 2021. Gender Bias in\nMachine Translation. Transactions of the Associa-\ntion for Computational Linguistics, 9:845–874.\nMike Schuster and Kaisuke Nakajima. 2012. Japanese\nand korean voice search. In 2012 IEEE International\nConference on Acoustics, Speech and Signal Process-\ning, ICASSP 2012, Kyoto, Japan, March 25-30, 2012,\npages 5149–5152. IEEE.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1715–1725,\n12347\nBerlin, Germany. Association for Computational Lin-\nguistics.\nYing Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan\nLi, Max Ryabinin, Daniel Y Fu, Zhiqiang Xie, Beidi\nChen, Clark Barrett, Joseph E Gonzalez, et al. 2023.\nHigh-throughput generative inference of large lan-\nguage models with a single gpu. arXiv preprint\narXiv:2303.06865.\nJongyoon Song, Sungwon Kim, and Sungroh Yoon.\n2021a. AligNART: Non-autoregressive neural ma-\nchine translation by jointly learning to estimate align-\nment and translate. In Proceedings of the 2021 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 1–14, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nYang Song, Chenlin Meng, Renjie Liao, and Stefano\nErmon. 2021b. Accelerating feedforward computa-\ntion via parallel nonlinear equation solving. In In-\nternational Conference on Machine Learning, pages\n9791–9800. PMLR.\nZhenqiao Song, Hao Zhou, Lihua Qian, Jingjing Xu,\nShanbo Cheng, Mingxuan Wang, and Lei Li. 2022.\nswitch-GLAT: Multilingual parallel machine transla-\ntion via code-switch decoder. In International Con-\nference on Learning Representations.\nMitchell Stern, Noam Shazeer, and Jakob Uszkoreit.\n2018. Blockwise parallel decoding for deep autore-\ngressive models. In Advances in Neural Information\nProcessing Systems, volume 31. Curran Associates,\nInc.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 3645–3650, Florence, Italy. Asso-\nciation for Computational Linguistics.\nXin Sun, Tao Ge, Furu Wei, and Houfeng Wang. 2021.\nInstantaneous grammatical error correction with shal-\nlow aggressive decoding. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 5937–5947, Online. Association\nfor Computational Linguistics.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural networks.\nAdvances in neural information processing systems,\n27.\nYuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Na-\nman Goyal, Vishrav Chaudhary, Jiatao Gu, and An-\ngela Fan. 2020. Multilingual translation with exten-\nsible multilingual pretraining and finetuning. CoRR,\nabs/2008.00401.\nJörg Tiedemann and Santhosh Thottingal. 2020. OPUS-\nMT — Building open translation services for the\nWorld. In Proceedings of the 22nd Annual Confer-\nenec of the European Association for Machine Trans-\nlation (EAMT), Lisbon, Portugal.\nViet Hong Tran, Huyen Vu Thong, Nguyen Van-Vinh,\nand Trung Le Tien. 2015. The English-Vietnamese\nmachine translation system for IWSLT 2015. In Pro-\nceedings of the 12th International Workshop on Spo-\nken Language Translation: Evaluation Campaign ,\npages 80–83, Da Nang, Vietnam.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nChunqi Wang, Ji Zhang, and Haiqing Chen. 2018. Semi-\nautoregressive neural machine translation. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 479–488,\nBrussels, Belgium. Association for Computational\nLinguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le,\nMohammad Norouzi, Wolfgang Macherey, Maxim\nKrikun, Yuan Cao, Qin Gao, Klaus Macherey, et al.\n2016. Google’s neural machine translation system:\nBridging the gap between human and machine trans-\nlation. arXiv preprint arXiv:1609.08144.\nHeming Xia, Tao Ge, Furu Wei, and Zhifang Sui. 2022.\nLossless speedup of autoregressive translation with\ngeneralized aggressive decoding.\nYisheng Xiao, Lijun Wu, Junliang Guo, Juntao Li,\nMin Zhang, Tao Qin, and Tie-yan Liu. 2022. A\nsurvey on non-autoregressive generation for neural\nmachine translation and beyond. arXiv preprint\narXiv:2204.09269.\nBiao Zhang, Philip Williams, Ivan Titov, and Rico Sen-\nnrich. 2020. Improving massively multilingual neu-\nral machine translation and zero-shot translation. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 1628–\n1639, Online. Association for Computational Linguis-\ntics.\n12348\nAlgorithm 2 Parallel GS-Jacobi Decoding\nInput: x = (x1, . . . , xn), pθ, b\nOutput: y = (y1, . . . , ym)\n1: y ←INIT T(x)\n2: m ←len(y)\n3: i ←1\n4: while i ⩽ m do\n5: o ←copy(yi:i+b)\n6: yi:i+b ←arg max(pθ(yi:i+b|y1:i+b, x))\n7: stop ←STOP C(o, yi:i+b)\n8: if stop then\n9: i ←i + b\n10: break\n11: end if\n12: end while\n13: return y\nA Algorithms details\nWe propose here the pseudocode of Algorithms 2\nand 3 due to space limitations in the main body of\nthe paper.\nThe function copy(yi:i+b) creates a copy of the\ntensor in input detached from the source. This\nis done in practice to avoid the overwriting of\npointers to the same memory location. Function\nCHECK EOS (yi:i+b) returns the index of the token\nEOS in the block if present, else −1. Function\nCHECK EOS (yi) returns True if the tokes in ex-\nactly the token EOS, else False . The function\narg maxselects from the model distribution over\nthe vocabulary the index (token) with maximum\nprobability. This procedure is done for all the to-\nkens in parallel, in the case of parallel decoding, or\nfor just a single token in the case of autoregressive\ndecoding. Generally, the output is the prediction\nfor the next token; hence it should be shifted left\nbefore the reassignment to a variable. We omitted\nthis implementation detail for clarity.\nB Additional implementation details\nWe run Opus experiments in table 1 on an AMD\nEPYC Milan with 16 cores at 2.45 GHz and\n64GB of RAM (accessible on Google Cloud\n- c2d-standard-16). For the scalability\nexperiment in figure 3, we also used Google Cloud\ninstances with an increasing number of cores\n(referred to as c2d-standard-XX, where XX\nis the number of used cores). Experiments with\nMBart50 on table 1, 2 and 6 are performed on a\nDesktop machine with Ubuntu 20.04.4 LTS, AMD\nDataset # Test\nWMT 14 De-En (Bojar et al., 2014) 3003\nWMT 16 Ro-En (Bojar et al., 2016) 1999\nWMT 17 Fi-En (Bojar et al., 2017) 3002\nIWSLT 15 En-Vi (Tran et al., 2015) 1046\nIITB En-Hi (Kunchukuttan et al., 2018) 2507\nFLORES-101 En-It (Goyal et al., 2022) 1012\nFLORES-101 En-Fr (Goyal et al., 2022) 1012\nTable 4: Data Statistic\nRyzen 9 3900X 12-Core Processor, 32GB of RAM,\nand a Palit Nvidia 3090 GPU. Additional experi-\nments with Opus in table 6 are also performed on\nthis machine. Models are implemented in Pytorch\n1.11.0 (Paszke et al., 2019) and the Huggingface\nTransformer library (Wolf et al., 2020). We used\npython 3.8 and NVIDIA-SMI Drivers 510.73.05\nwith CUDA version 11.6. For OPUS we used Hug-\ngingface models available on the hub under the tag\nHelsinki-NLP/opus-mt-{src}-{tgt}\nexcept for the language pair Ro-\nEn where we used the model\nHelsinki-NLP/opus-mt-roa-en and\nthe pair En-De where we used the check-\npoint opus-2021-02-22 4. For the model\nMBart50, we used the facebook pre-trained\nmodel available on the hub with the tag\nmbart-large-50-many-to-many-mmt.\nSince this is a multilingual model, we prepend\nthe source and target language tag corresponding\nproperly to the language pair to be translated.\nWe report results for a single run over the test\ndataset since we found low variance in estimates\nwith multiple runs which can be calculated by\nsimply varying the corresponding parameter in the\nconfig.yaml file. For each dataset, we used\nthe official test split via the Huggingface dataset\nlibrary (Lhoest et al., 2021). Datasets statistics are\nreported in table 4.\nC FLOPs calculation details\nWe measured computational complexity using float-\ning point operations (FLOPs), which, as the name\nimply, counts the number of floating point opera-\ntion performed by a model. This is a standard met-\nric used in literature to measure hardware-agnostic\ncomplexity. This means that hardware and soft-\nware optimizations are not counted in the score\n(Wu et al., 2016; Kim et al., 2019). We used the\n4https://object.pouta.csc.fi/Tatoeba-MT-models/eng-\ndeu/opus-2021-02-22.zip\n12349\nAlgorithm 3 Hybrid GS-Jacobi Decoding\nInput: x = (x1, . . . , xn), pθ, b\nOutput: y = (y1, . . . , ym)\n1: y ←INIT T(x)\n2: h ←len(y)\n3: i ←1\n4: eos_cond ←False\n5: while i ⩽ h do\n6: o ←copy(yi:i+b)\n7: yi:i+b ←arg max(pθ(yi:i+b|y1:i+b, x))\n8: stop ←STOP C(o, yi:i+b)\n9: eos_ind ←CHECK EOS (yi:i+b)\n10: if stop and eos_ind >−1 then\n11: y ←y1:eos_ind\n12: eos_cond ←True\n13: break\n14: end if\n15: if stop then\n16: i ←i + b\n17: break\n18: end if\n19: end while\n20: while eos_cond ! =True do\n21: yi ←arg max(pθ(yi|yi−1, x))\n22: i ←i + 1\n23: eos_cond ←ISEOS (yi)\n24: end while\n25: return y\nELECTRA flops calculator5 inserting the number\nof parameters and the number of training step per-\nformed for each model analyzed in table 3 accord-\ning to the training specification in each paper. For\ninference FLOPs, we computed the decoding cost\nof each sentence in the testset of WMT14 En-De\nfor each model. For a scale reference, we report\nin here Table 5 training flops of other well-known\narchitecture. The code package contains the scripts\nto replicate all the experiments.\nD Additional results\nWe propose here additional results to the experi-\nments in the paper that were omitted due to limita-\ntions constraints. Table 6 shows the same experi-\nments of Table 1 in the main paper, proposed here\non a standard desktop CPU with also the speedup\nin terms of iterations. It is possible to observe that\nin the case of MBart50 and PGJ there is a speedup\n5https://github.com/google-\nresearch/electra/blob/master/flops_computation.py\nModel Train FLOPs Infer. FLOPs Total FLOPs\nSemi-NAT 1.55e17 2.08e13 1.55e17\nShallow Dec. 1.02e19 1.15e13 1.02e19\nDSLP 1.93e19 1.58e13 1.93e19\nF-V AE 4.06e19 1.58e13 4.06e19\nDisCo 4.06e19 1.58e13 4.06e19\nSUNDAE 5.27e21 1.58e14 5.27e21\nBERT base 6.43e19 - -\nBERT large 1.92e20 - -\nRoBERTa 3.19e21 - -\nTable 5: FLOPs comparison with other models.\nof 8 −11% in terms of iterations compare to a\ntime speedup of 3 −8%. This means that there\nis room for improvement for our algorithm. Fur-\nthermore, results show that the time speedups are\nconsistent also with standard desktop hardware. Ta-\nble 7 shows the BLEU scores for the cross-lingual\nexperiment. It is possible to observe that parallel\ndecoding algorithms guarantee quality compared\nto greedy autoregressive and are not so distant from\nbeam search. We show also here in table 5 some\nqualitative results for the experiments in table 2.\nFinally, we propose additional visualizations using\nDGGviz in Figure 6.\n12350\nDecoding Algorithm en→de de →en en →ro ro →en\nTime Iters Time Iters Time Iters Time Iters\nOpus\nGreedy Autoregressive 1.00× 1.00× 1.00× 1.00× 1.00× 1.00× 1.00× 1.00×\nBeam Search (beam = 5) 0.71× 1.00× 0.71× 1.00× 0.70× 1.00× 0.72× 1.00×\nPJ Decoding 0.72× 1.03× 0.74× 1.04× 0.69× 1.04× 0.67× 1.03×\nPGJ Decoding (b = 3) 1.16× 1.04× 1.19× 1.07× 1.17× 1.05× 1.17× 1.03×\nHGJ Decoding (b = 3) 1.16× 1.04× 1.19× 1.06× 1.17× 1.05× 1.17× 1.03×\nMBart50\nGreedy Autoregressive 1.00× 1.00× 1.00× 1.00× 1.00× 1.00× 1.00× 1.00×\nBeam Search (beam = 5) 0.76× 1.00× 0.77× 1.00× 0.77× 1.00× 0.76× 1.00×\nPJ Decoding 0.88× 1.03× 0.88× 1.03× 0.86× 1.04× 0.85× 1.03×\nPGJ Decoding (b = 3) 1.06× 1.10× 1.08× 1.11× 1.03× 1.08× 1.04× 1.11×\nHGJ Decoding (b = 3) 1.05× 1.07× 1.07× 1.01× 1.01× 1.02× 1.02× 1.08×\nTable 6: Comparison of parallel decoding algorithms (highlighted in grey) with sequential decoding using Opus\n(CPU) and MBart50 (GPU) on WMT14 and WMT16. Speed is showed here both in Time and Iterations w.r.t. the\ngreedy autoregressive baseline.\nWMT17 IITB IWSLT15 FLORES\nEn-Fi En-Hi En-Vi En-It En-Fr\nDec. Algorithm ← → ← → ← → ← → ← →\nAutoregressive 17.55 25 .34 16 .50 24 .70 31 .92 33 .94 22 .78 26 .38 39 .51 38 .90\nBeam Search 18.39 26 .04 16 .87 25 .24 32 .14 34 .59 23 .52 26 .80 39 .59 39 .21\nPJ 17.54 25 .35 16 .50 24 .69 31 .92 33 .94 22 .78 26 .38 39 .50 38 .90\nPGJ (b=3) 17.55 25 .35 16 .50 24 .70 31 .93 33 .94 22 .78 26 .38 39 .51 38 .90\nHGJ (b=3) 17.55 25 .35 16 .50 24 .70 31 .93 33 .94 22 .78 26 .38 39 .51 38 .90\nTable 7: BLEU scores on MBart50.\nExample 1 - Wmt16 En-Ro\nTARGETDl Corbyn va adresa primele dintre cele s,ase întreb˘ari la care are dreptul la scurt timp dup˘a prânz; prestat,ia\nsa va fi probabil analizat˘a îndeaproape de mass-media s,i parlamentarii laburis,ti. Times (s) BLEU\nA Dl Corbyn va ridica pentru a adresa prima dintre cele ¸ sase întreb˘ari alocate la scurt timp dup˘a miezul zilei, iar\nperforman¸ ta sa va fi probabil examinat˘a îndeaproape de pres˘a ¸ si de parlamentarii laburi¸ sti. 0.51 19.71\nPJ Dl Corbyn va ridica pentru a adresa prima dintre cele ¸ sase întreb˘ari alocate la scurt timp dup˘a miezul zilei, iar\nperforman¸ ta sa va fi probabil examinat˘a îndeaproape de pres˘a ¸ si de parlamentarii laburi¸ sti. 0.56 19.71\nPGJ Dl Corbyn va ridica pentru a adresa prima dintre cele ¸ sase întreb˘ari alocate la scurt timp dup˘a miezul zilei, iar\nperforman¸ ta sa va fi probabil examinat˘a îndeaproape de pres˘a ¸ si de parlamentarii laburi¸ sti. 0.45 19.71\nHGJ Dl Corbyn va ridica pentru a adresa prima dintre cele ¸ sase întreb˘ari alocate la scurt timp dup˘a miezul zilei, iar\nperforman¸ ta sa va fi probabil examinat˘a îndeaproape de pres˘a ¸ si de parlamentarii laburi¸ sti. 0.44 19.71\nExample 2 - Flores En-It\nTARGET\nQuando un piccolo gruppo di esseri viventi (una piccola popolazione) si separa dalla popolazione principale\nalla quale appartiene (per esempio se si sposta oltre una catena montuosa o un fiume, o si sposta su una nuova\nisola, rendendo quindi difficile un eventuale ritorno), esso si ritroverà probabilmente in un ambiente diverso da\nquello in cui si trovava prima.\nTimes (s) BLEU\nA\nQuando un piccolo gruppo di esseri viventi si separa dalla popolazione principale da cui provengono, come se\nsi muovano su una catena di montagne o su un fiume o se si trasferiscono su una nuova isola per non poter tornare\nfacilmente, si troveranno spesso in un ambiente diverso da quello in cui erano prima.\n0.61 31.69\nPJ\nQuando un piccolo gruppo di esseri viventi si separa dalla popolazione principale da cui provengono, come se\nsi muovano su una catena di montagne o su un fiume o se si trasferiscono su una nuova isola per non poter tornare\nfacilmente, si troveranno spesso in un ambiente diverso da quello in cui erano prima.\n0.73 31.69\nPGJ\nQuando un piccolo gruppo di esseri viventi si separa dalla popolazione principale da cui provengono, come se\nsi muovano su una catena di montagne o su un fiume o se si trasferiscono su una nuova isola per non poter tornare\nfacilmente, si troveranno spesso in un ambiente diverso da quello in cui erano prima.\n0.58 31.69\nHGJ\nQuando un piccolo gruppo di esseri viventi si separa dalla popolazione principale da cui provengono, come se\nsi muovano su una catena di montagne o su un fiume o se si trasferiscono su una nuova isola per non poter tornare\nfacilmente, si troveranno spesso in un ambiente diverso da quello in cui erano prima.\n0.59 31.69\n12351\nExample 3 - Wmt14 En-De\nTARGET\nBei der diesjährigen Veranstaltung gibt es Auftritte von Wanda Sykes, Kathy Griffin und Bill Maher sowie auch\nvon „Stand Up for Heroes“, einer jährlichen Musik- und Comedy-Benefizveranstaltung für Armeeveteranen im\nMadison Square Garden, bei der unter anderem Bruce Springsteen, Jon Stewart, Roger Waters und Bill Cosby auftreten.\nTimes (s) BLEU\nA\nZu den diesjährigen Veranstaltungen gehören Auftritte von Wanda Sykes, Kathy Griffin und Bill Maher sowie\n\"Stand Up for Heroes\", ein jährlicher Musik- und Komödie-V orteil für Militärveteranen, im Madison Square Garden, mit\nu.a. Bruce Springsteen, Jon Stewart, Roger Waters und Bill Cosby.\n1.30 47.04\nPJ\nZu den diesjährigen Veranstaltungen gehören Auftritte von Wanda Sykes, Kathy Griffin und Bill Maher sowie\n\"Stand Up for Heroes\", ein jährlicher Musik- und Komödie-V orteil für Militärveteranen, im Madison Square Garden, mit\nu.a. Bruce Springsteen, Jon Stewart, Roger Waters und Bill Cosby.\n2.43 47.04\nPGJ\nZu den diesjährigen Veranstaltungen gehören Auftritte von Wanda Sykes, Kathy Griffin und Bill Maher sowie\n\"Stand Up for Heroes\", ein jährlicher Musik- und Komödie-V orteil für Militärveteranen, im Madison Square Garden, mit\nu.a. Bruce Springsteen, Jon Stewart, Roger Waters und Bill Cosby.\n1.09 47.04\nHGJ\nZu den diesjährigen Veranstaltungen gehören Auftritte von Wanda Sykes, Kathy Griffin und Bill Maher sowie\n\"Stand Up for Heroes\", ein jährlicher Musik- und Komödie-V orteil für Militärveteranen, im Madison Square Garden, mit\nu.a. Bruce Springsteen, Jon Stewart, Roger Waters und Bill Cosby.\n1.08 47.04\nExample 4 - Flores En-Fr\nTARGET\nCinq minutes après le début de l’exposition, un vent se met à souffler pour atteindre, environ une minute\nplus tard, la vitesse de 70km/h... puis la pluie arrive, mais si forte et si grosse qu’elle frappe votre peau\ncomme une aiguille, puis la grêle tombe du ciel, les gens paniquent, crient et se roulent dessus.\nTimes (s) BLEU\nA\nCinq minutes après l’exposition, le vent commence à tourner, environ un minute plus tard, le vent atteint\n70 km/h, puis la pluie arrive, mais si forte et si grande qu’elle vous frappe la peau comme une aiguille, puis\nle hail tombe du ciel, les gens paniquent, s’expriment et se courent l’un sur l’autre.\n0.82 39.90\nPJ\nCinq minutes après l’exposition, le vent commence à tourner, environ un minute plus tard, le vent atteint\n70 km/h, puis la pluie arrive, mais si forte et si grande qu’elle vous frappe la peau comme une aiguille, puis\nle hail tombe du ciel, les gens paniquent, s’expriment et se courent l’un sur l’autre.\n0.94 39.90\nPGJ\nCinq minutes après l’exposition, le vent commence à tourner, environ un minute plus tard, le vent atteint\n70 km/h, puis la pluie arrive, mais si forte et si grande qu’elle vous frappe la peau comme une aiguille, puis\nle hail tombe du ciel, les gens paniquent, s’expriment et se courent l’un sur l’autre.\n0.73 39.90\nHGJ\nCinq minutes après l’exposition, le vent commence à tourner, environ un minute plus tard, le vent atteint\n70 km/h, puis la pluie arrive, mais si forte et si grande qu’elle vous frappe la peau comme une aiguille, puis\nle hail tombe du ciel, les gens paniquent, s’expriment et se courent l’un sur l’autre.\n0.72 39.90\nTable 7: Translation examples generated with the autoregressive (A) and the different decoding algorithms proposed\n(PJ, PGJ, HGJ) on Opus (WMT datasets) and MBart50. The decoding time is shown in seconds.\n12352\n(a) En-De: \"Lack of Scots title race bores Dutch - de\nBoer\"→\"Fehlende Schottentitelrennen bohrt Niederlan-\ndisch - de Boer\"\n(b) De-En: \"Private Fachgeschafte und auch den Großhan-\ndel gibt es fast nicht mehr.\"→\"Private specialist shops and\nwholesale trade are almost no longer available.\"\n(c) Ro-En: \"Un prim contract de lucr˘ari a fost reziliat în\naprilie 2012, dup˘a ce se efectuaser˘a lucr˘ari de 4,5 milioane\nlei.\"→\"A first contract of employment was terminated in\nApril 2012, after a work of 4.5 million lei.\"\n(d) En-Ro: \"‘Shot in Joburg’: Homeless youth trained\nas photographers\"→\"“Fotografii in Joburg”: Tineri f˘ar˘a\nad˘apost forma¸ ti ca fotografi\"\n(e) De-En: \"Einige sind nach der Installation auf Prob-\nleme gestoßen, da sie eine Fehlermeldung erhalten, die\nmitteilt, dass die “Software-Aktualisierung fehlgeschla-\ngen” ist.\"→\"Some have encountered problems after instal-\nlation, as they receive an error message that tells us that\n“software update has failed”.\"\n(f) Ro-En: \"Se pare c ˘a va fi acuzat de fug ˘a de la locul\naccidentului, neoferirea primului ajutor s, i alte infract, iuni\nrutiere.\"→\"Apparently he’ll be charged with running\nfrom the scene of the accident, the first aid and other road\ncrimes.\"\nFigure 6: DGGviz additional visualizations\n12353\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nLimitations section\n□\u0013 A2. Did you discuss any potential risks of your work?\nEthics Statements\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nAbstract\n□\u0013 A4. Have you used AI writing assistants when working on this paper?\nWe used ChatGPT to rephrase some sentences in the ﬁnal camera-ready version in sections 4.3 and\n5.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nCode to reproduce the experiments (zip)\n□\u0013 B1. Did you cite the creators of artifacts you used?\nSection 4 and Appendix B\n□\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nLicense ﬁle in the code repository\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nLicense ﬁle in the code repository\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. No data was collected\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNot applicable. Left blank.\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nData is automatically downloaded with standard train/test/dev splits via the Huggingface datasets\nlibrary. Additional statistics in Appendix B\nC □\u0013 Did you run computational experiments?\nSection 4 - Experiments\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nSection 4.1\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n12354\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSection 4.1\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 4\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nSection 4.1 and Appendix B\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNot applicable. Left blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNot applicable. Left blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. Left blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n12355",
  "topic": "Inference",
  "concepts": [
    {
      "name": "Inference",
      "score": 0.6026628017425537
    },
    {
      "name": "Decoding methods",
      "score": 0.581484317779541
    },
    {
      "name": "Computer science",
      "score": 0.5534083843231201
    },
    {
      "name": "Transformer",
      "score": 0.5516152381896973
    },
    {
      "name": "Machine translation",
      "score": 0.4264599084854126
    },
    {
      "name": "Speech recognition",
      "score": 0.3400946855545044
    },
    {
      "name": "Natural language processing",
      "score": 0.33317792415618896
    },
    {
      "name": "Artificial intelligence",
      "score": 0.27475103735923767
    },
    {
      "name": "Algorithm",
      "score": 0.202367901802063
    },
    {
      "name": "Electrical engineering",
      "score": 0.17345494031906128
    },
    {
      "name": "Engineering",
      "score": 0.1530170440673828
    },
    {
      "name": "Voltage",
      "score": 0.07666239142417908
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I861853513",
      "name": "Sapienza University of Rome",
      "country": "IT"
    },
    {
      "id": "https://openalex.org/I4401726832",
      "name": "Tübingen AI Center",
      "country": null
    },
    {
      "id": "https://openalex.org/I4210133934",
      "name": "Bernstein Center for Computational Neuroscience Tübingen",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I8087733",
      "name": "University of Tübingen",
      "country": "DE"
    }
  ]
}