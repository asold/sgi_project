{
  "title": "Vision Transformers for Dense Prediction",
  "url": "https://openalex.org/W3136635488",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2028854303",
      "name": "Rene Ranftl",
      "affiliations": [
        "Intel (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A3019488500",
      "name": "Alexey Bochkovskiy",
      "affiliations": [
        "Intel (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A1809196549",
      "name": "Vladlen Koltun",
      "affiliations": [
        "Intel (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2916798096",
    "https://openalex.org/W2149259880",
    "https://openalex.org/W6639824700",
    "https://openalex.org/W6782298681",
    "https://openalex.org/W6763367864",
    "https://openalex.org/W6763509872",
    "https://openalex.org/W6605121731",
    "https://openalex.org/W6711868289",
    "https://openalex.org/W6629368666",
    "https://openalex.org/W6754005058",
    "https://openalex.org/W2125215748",
    "https://openalex.org/W6751037545",
    "https://openalex.org/W1745334888",
    "https://openalex.org/W6679434410",
    "https://openalex.org/W2963881378",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W6766261854",
    "https://openalex.org/W6753494528",
    "https://openalex.org/W2963760790",
    "https://openalex.org/W2942368658",
    "https://openalex.org/W6766673545",
    "https://openalex.org/W2563705555",
    "https://openalex.org/W2990946490",
    "https://openalex.org/W6696085341",
    "https://openalex.org/W6760424586",
    "https://openalex.org/W2737258237",
    "https://openalex.org/W2560023338",
    "https://openalex.org/W3034885317",
    "https://openalex.org/W6775845032",
    "https://openalex.org/W6768371451",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W6788135285",
    "https://openalex.org/W6685261749",
    "https://openalex.org/W2963488291",
    "https://openalex.org/W2991062542",
    "https://openalex.org/W2150066425",
    "https://openalex.org/W6726644806",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W6755977528",
    "https://openalex.org/W2892614179",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W2981413347",
    "https://openalex.org/W2412782625",
    "https://openalex.org/W2340897893",
    "https://openalex.org/W6739696289",
    "https://openalex.org/W3035257660",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3132270109",
    "https://openalex.org/W6771646754",
    "https://openalex.org/W3035563424",
    "https://openalex.org/W2798373498",
    "https://openalex.org/W2982336692",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6767071366",
    "https://openalex.org/W6775170262",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W1513100184",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2798410215",
    "https://openalex.org/W2630837129",
    "https://openalex.org/W2953139137",
    "https://openalex.org/W2969825080",
    "https://openalex.org/W3098903812",
    "https://openalex.org/W2970389371",
    "https://openalex.org/W3097065222",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2890538051",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2962960377",
    "https://openalex.org/W2171740948",
    "https://openalex.org/W2741885505",
    "https://openalex.org/W3097217077",
    "https://openalex.org/W3081167590",
    "https://openalex.org/W2995265606",
    "https://openalex.org/W125693051",
    "https://openalex.org/W2963840672",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W3109301572",
    "https://openalex.org/W2963542991",
    "https://openalex.org/W2799269579",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2899663614"
  ],
  "abstract": "We introduce dense vision transformers, an architecture that leverages vision transformers in place of convolutional networks as a backbone for dense prediction tasks. We assemble tokens from various stages of the vision transformer into image-like representations at various resolutions and progressively combine them into full-resolution predictions using a convolutional decoder. The transformer backbone processes representations at a constant and relatively high resolution and has a global receptive field at every stage. These properties allow the dense vision transformer to provide finer-grained and more globally coherent predictions when compared to fully-convolutional networks. Our experiments show that this architecture yields substantial improvements on dense prediction tasks, especially when a large amount of training data is available. For monocular depth estimation, we observe an improvement of up to 28% in relative performance when compared to a state-of-the-art fully-convolutional network. When applied to semantic segmentation, dense vision transformers set a new state of the art on ADE20K with 49.02% mIoU. We further show that the architecture can be fine-tuned on smaller datasets such as NYUv2, KITTI, and Pascal Context where it also sets the new state of the art. Our models are available at https://github.com/intel-isl/DPT.",
  "full_text": "Vision Transformers for Dense Prediction\nRen´e Ranftl Alexey Bochkovskiy\nIntel Labs\nrene.ranftl@intel.com\nVladlen Koltun\nAbstract\nWe introduce dense vision transformers, an architecture\nthat leverages vision transformers in place of convolutional\nnetworks as a backbone for dense prediction tasks. We as-\nsemble tokens from various stages of the vision transformer\ninto image-like representations at various resolutions and\nprogressively combine them into full-resolution predictions\nusing a convolutional decoder. The transformer backbone\nprocesses representations at a constant and relatively high\nresolution and has a global receptive ﬁeld at every stage.\nThese properties allow the dense vision transformer to pro-\nvide ﬁner-grained and more globally coherent predictions\nwhen compared to fully-convolutional networks. Our ex-\nperiments show that this architecture yields substantial im-\nprovements on dense prediction tasks, especially when a\nlarge amount of training data is available. For monocular\ndepth estimation, we observe an improvement of up to 28%\nin relative performance when compared to a state-of-the-\nart fully-convolutional network. When applied to semantic\nsegmentation, dense vision transformers set a new state of\nthe art on ADE20K with 49.02% mIoU. We further show\nthat the architecture can be ﬁne-tuned on smaller datasets\nsuch as NYUv2, KITTI, and Pascal Context where it also\nsets the new state of the art. Our models are available at\nhttps://github.com/intel-isl/DPT.\n1. Introduction\nVirtually all existing architectures for dense prediction\nare based on convolutional networks [6, 31, 34, 42, 49,\n50, 53]. The design of dense prediction architectures com-\nmonly follows a pattern that logically separates the network\ninto an encoder and a decoder. The encoder is frequently\nbased on an image classiﬁcation network, also called the\nbackbone, that is pretrained on a large corpus such as Im-\nageNet [9]. The decoder aggregates features from the en-\ncoder and converts them to the ﬁnal dense predictions. Ar-\nchitectural research on dense prediction frequently focuses\non the decoder and its aggregation strategy [6, 7, 50, 53].\nHowever, it is widely recognized that the choice of back-\nbone architecture has a large inﬂuence on the capabilities\nof the overall model, as any information that is lost in the\nencoder is impossible to recover in the decoder.\nConvolutional backbones progressively downsample the\ninput image to extract features at multiple scales. Down-\nsampling enables a progressive increase of the receptive\nﬁeld, the grouping of low-level features into abstract high-\nlevel features, and simultaneously ensures that memory\nand computational requirements of the network remain\ntractable. However, downsampling has distinct drawbacks\nthat are particularly salient in dense prediction tasks: fea-\nture resolution and granularity are lost in the deeper stages\nof the model and can thus be hard to recover in the decoder.\nWhile feature resolution and granularity may not matter for\nsome tasks, such as image classiﬁcation, they are critical\nfor dense prediction, where the architecture should ideally\nbe able to resolve features at or close to the resolution of the\ninput image.\nVarious techniques to mitigate the loss of feature gran-\nularity have been proposed. These include training at\nhigher input resolution (if the computational budget per-\nmits), dilated convolutions [49] to rapidly increase the re-\nceptive ﬁeld without downsampling, appropriately-placed\nskip connections from multiple stages of the encoder to\nthe decoder [31], or, more recently, by connecting multi-\nresolution representations in parallel throughout the net-\nwork [42]. While these techniques can signiﬁcantly im-\nprove prediction quality, the networks are still bottlenecked\nby their fundamental building block: the convolution. Con-\nvolutions together with non-linearities form the fundamen-\ntal computational unit of image analysis networks. Convo-\nlutions, by deﬁnition, are linear operators that have a lim-\nited receptive ﬁeld. The limited receptive ﬁeld and the lim-\nited expressivity of an individual convolution necessitate se-\nquential stacking into very deep architectures to acquire suf-\nﬁciently broad context and sufﬁciently high representational\npower. This, however, requires the production of many in-\ntermediate representations that require a large amount of\narXiv:2103.13413v1  [cs.CV]  24 Mar 2021\nmemory. Downsampling the intermediate representations\nis necessary to keep memory consumption at levels that are\nfeasible with existing computer architectures.\nIn this work, we introduce the dense prediction trans-\nformer (DPT). DPT is a dense prediction architecture that is\nbased on an encoder-decoder design that leverages a trans-\nformer as the basic computational building block of the en-\ncoder. Speciﬁcally, we use the recently proposed vision\ntransformer (ViT) [11] as a backbone architecture. We re-\nassemble the bag-of-words representation that is provided\nby ViT into image-like feature representations at various\nresolutions and progressively combine the feature repre-\nsentations into the ﬁnal dense prediction using a convolu-\ntional decoder. Unlike fully-convolutional networks, the vi-\nsion transformer backbone foregoes explicit downsampling\noperations after an initial image embedding has been com-\nputed and maintains a representation with constant dimen-\nsionality throughout all processing stages. It furthermore\nhas a global receptive ﬁeld at every stage. We show that\nthese properties are especially advantageous for dense pre-\ndiction tasks as they naturally lead to ﬁne-grained and glob-\nally coherent predictions.\nWe conduct experiments on monocular depth estimation\nand semantic segmentation. For the task of general-purpose\nmonocular depth estimation [30], where large-scale train-\ning data is available, DPT provides a performance increase\nof more than 28% when compared to the top-performing\nfully-convolutional network for this task. The architecture\ncan also be ﬁne-tuned to small monocular depth prediction\ndatasets, such as NYUv2 [35] and KITTI [15], where it\nalso sets the new state of the art. We provide further evi-\ndence of the strong performance of DPT using experiments\non semantics segmentation. For this task, DPT sets a new\nstate of the art on the challenging ADE20K [54] and Pas-\ncal Context [26] datasets. Our qualitative results indicate\nthat the improvements can be attributed to ﬁner-grained and\nmore globally coherent predictions in comparison to convo-\nlutional networks.\n2. Related Work\nFully-convolutional networks [33, 34] are the prototyp-\nical architecture for dense prediction. Many variants of\nthis basic pattern have been proposed over the years, how-\never, all existing architectures adopt convolution and sub-\nsampling as their fundamental elements in order to learn\nmulti-scale representations that can leverage an appropri-\nately large context. Several works propose to progressively\nupsample representations that have been pooled at differ-\nent stages [1, 23, 27, 31], while others use dilated convo-\nlutions [6, 7, 49] or parallel multi-scale feature aggregation\nat multiple scales [53] to recover ﬁne-grained predictions\nwhile at the same time ensuring a sufﬁciently large context.\nMore recent architectures maintain a high-resolution repre-\nsentation together with multiple lower-resolution represen-\ntations throughout the network [37, 42].\nAttention-based models [2] and in particular transform-\ners [39] have been the architecture of choice for learning\nstrong models for natural language processing (NLP) [4,\n10, 24] in recent years. Transformers are set-to-set mod-\nels that are based on the self-attention mechanism. Trans-\nformer models have been particularly successful when in-\nstantiated as high-capacity architectures and trained on very\nlarge datasets. There have been several works that adapt at-\ntention mechanisms to image analysis [3, 28, 29, 41, 52]. In\nparticular, it has recently been demonstrated that a direct ap-\nplication of token-based transformer architectures that have\nbeen successful in NLP can yield competitive performance\non image classiﬁcation [11]. A key insight of this work was\nthat, like transformer models in NLP, vision transformers\nneed to be paired with a sufﬁcient amount of training data\nto realize their potential.\n3. Architecture\nThis section introduces the dense vision transformer. We\nmaintain the overall encoder-decoder structure that has been\nsuccessful for dense prediction in the past. We leverage vi-\nsion transformers [11] as the backbone, show how the rep-\nresentation that is produced by this encoder can be effec-\ntively transformed into dense predictions, and provide in-\ntuition for the success of this strategy. An overview of the\ncomplete architecture is shown in Figure 1 (left).\nTransformer encoder. On a high level, the vision trans-\nformer (ViT) [11] operates on a bag-of-words representa-\ntion of the image [36]. Image patches that are individually\nembedded into a feature space, or alternatively deep fea-\ntures extracted from the image, take the role of “words”.\nWe will refer to embedded “words” as tokens throughout\nthe rest of this work. Transformers transform the set of to-\nkens using sequential blocks of multi-headed self-attention\n(MHSA) [39], which relate tokens to each other to trans-\nform the representation.\nImportantly for our application, a transformer maintains\nthe number of tokens throughout all computations. Since to-\nkens have a one-to-one correspondence with image patches,\nthis means that the ViT encoder maintains the spatial reso-\nlution of the initial embedding throughout all transformer\nstages. Additionally, MHSA is an inherently global oper-\nation, as every token can attend to and thus inﬂuence ev-\nery other token. Consequently, the transformer has a global\nreceptive ﬁeld at every stage after the initial embedding.\nThis is in stark contrast to convolutional networks, which\nprogressively increase their receptive ﬁeld as features pass\nthrough consecutive convolution and downsampling layers.\nMore speciﬁcally, ViT extracts a patch embedding from\nthe image by processing all non-overlapping square patches\nTransformer\nTransformer \nTransformer\nFusion\nFusion\nFusion\nFusion\nHead\nEmbed\nTransformer\nReassemble32\nReassemble16\nReassemble8\nReassemble4\nReassembles\nConcatenate\nRead Resamples\nProject\nResidual Conv Unit\nResidual Conv Unit\nResample0.5\n+\nProjectFusion\nFigure 1. Left: Architecture overview. The input image is transformed into tokens (orange) either by extracting non-overlapping patches\nfollowed by a linear projection of their ﬂattened representation (DPT-Base and DPT-Large) or by applying a ResNet-50 feature extractor\n(DPT-Hybrid). The image embedding is augmented with a positional embedding and a patch-independent readout token (red) is added.\nThe tokens are passed through multiple transformer stages. We reassemble tokens from different stages into an image-like representation\nat multiple resolutions (green). Fusion modules (purple) progressively fuse and upsample the representations to generate a ﬁne-grained\nprediction. Center: Overview of the Reassemble s operation. Tokens are assembled into feature maps with 1\ns the spatial resolution of the\ninput image. Right: Fusion blocks combine features using residual convolutional units [23] and upsample the feature maps.\nof size p2 pixels from the image. The patches are ﬂattened\ninto vectors and individually embedded using a linear pro-\njection. An alternative, more sample-efﬁcient, variant of\nViT extracts the embedding by applying a ResNet50 [16] to\nthe image and uses the pixel features of the resulting feature\nmaps as tokens. Since transformers are set-to-set functions,\nthey do not intrinsically retain the information of the spatial\npositions of individual tokens. The image embeddings are\nthus concatenated with a learnable position embedding to\nadd this information to the representation. Following work\nin NLP, the ViT additionally adds a special token that is not\ngrounded in the input image and serves as the ﬁnal, global\nimage representation which is used for classiﬁcation. We\nrefer to this special token as the readout token. The result\nof applying the embedding procedure to an image of size\nH ×W pixels is a a set of t0 = {t0\n0,...,t 0\nNp }, t0\nn ∈RD\ntokens, where Np = HW\np2 , t0 refers to the readout token,\nand Dis the feature dimension of each token.\nThe input tokens are transformed using L transformer\nlayers into new representations tl, where lrefers to the out-\nput of the l-th transformer layer. Dosovitskiy et al . [11]\ndeﬁne several variants of this basic blueprint. We use three\nvariants in our work: ViT-Base, which uses the patch-based\nembedding procedure and features 12 transformer layers;\nViT-Large, which uses the same embedding procedure and\nhas 24 transformer layers and a wider feature size D; and\nViT-Hybrid, which employs a ResNet50 to compute the im-\nage embedding followed by 12 transformer layers. We use\npatch size p = 16 for all experiments. We refer the inter-\nested reader to the original work [11] for additional details\non these architectures.\nThe embedding procedure for ViT-Base and ViT-Large\nprojects the ﬂattened patches to dimension D= 768and\nD= 1024, respectively. Since both feature dimensions are\nlarger than the number of pixels in an input patch, this\nmeans that the embedding procedure can learn to retain in-\nformation if it is beneﬁcial for the task. Features from the\ninput patches can in principle be resolved with pixel-level\naccuracy. Similarly, the ViT-Hybrid architecture extracts\nfeatures at 1\n16 the input resolution, which is twice as high\nas the lowest-resolution features that are commonly used\nwith convolutional backbones.\nConvolutional decoder. Our decoder assembles the set\nof tokens into image-like feature representations at various\nresolutions. The feature representations are progressively\nfused into the ﬁnal dense prediction. We propose a sim-\nple three-stage Reassemble operation to recover image-like\nrepresentations from the output tokens of arbitrary layers of\nthe transformer encoder:\nReassemble\nˆD\ns (t) = (Resamples ◦Concatenate ◦Read)(t),\nwhere sdenotes the output size ratio of the recovered rep-\nresentation with respect to the input image, and ˆDdenotes\nthe output feature dimension.\nWe ﬁrst map the Np + 1tokens to a set of Np tokens\nthat is amenable to spatial concatenation into an image-like\nrepresentation:\nRead : RNp+1×D →RNp×D. (1)\nThis operation is essentially responsible for appropriately\nhandling the readout token. Since the readout token doesn’t\nserve a clear purpose for the task of dense prediction, but\ncould potentially still be useful to capture and distribute\nglobal information, we evaluate three different variants of\nthis mapping:\nReadignore(t) ={t1,...,t Np } (2)\nsimply ignores the readout token,\nReadadd(t) ={t1 + t0,...,t Np + t0} (3)\npasses the information from the readout token to all other\ntokens by adding the representations, and\nReadproj(t) ={mlp(cat(t1,t0)),...,\nmlp(cat(tNp ,t0))} (4)\npasses information to the other tokens by concatenating the\nreadout to all other tokens before projecting the representa-\ntion to the original feature dimension Dusing a linear layer\nfollowed by a GELU non-linearity [17].\nAfter a Read block, the resulting Np tokens can be re-\nshaped into an image-like representation by placing each\ntoken according to the position of the initial patch in the\nimage. Formally, we apply a spatial concatenation opera-\ntion that results in a feature map of size H\np ×W\np with D\nchannels:\nConcatenate : RNp×D →R\nH\np ×W\np ×D\n. (5)\nWe ﬁnally pass this representation to a spatial resampling\nlayer that scales the representation to size H\ns ×W\ns with ˆD\nfeatures per pixel:\nResamples : R\nH\np ×W\np ×D\n→R\nH\ns ×W\ns × ˆD. (6)\nWe implement this operation by ﬁrst using 1 ×1 convolu-\ntions to project the input representation to ˆD, followed by a\n(strided) 3 ×3 convolution when s ≥p, or a strided 3 ×3\ntranspose convolution when s < p, to implement spatial\ndownsampling and upsampling operations, respectively.\nIrrespective of the exact transformer backbone, we re-\nassemble features at four different stages and four differ-\nent resolutions. We assemble features from deeper lay-\ners of the transformer at lower resolution, whereas fea-\ntures from early layers are assembled at higher resolution.\nWhen using ViT-Large, we reassemble tokens from layers\nl = {5,12,18,24}, whereas with ViT-Base we use layers\nl = {3,6,9,12}. We use features from the ﬁrst and sec-\nond ResNet block from the embedding network and stages\nl = {9,12}when using ViT-Hybrid. Our default architec-\nture uses projection as the readout operation and produces\nfeature maps with ˆD = 256 dimensions. We will refer\nto these architectures as DPT-Base, DPT-Large, and DPT-\nHybrid, respectively.\nWe ﬁnally combine the extracted feature maps from\nconsecutive stages using a ReﬁneNet-based feature fusion\nblock [23, 45] (see Figure1 (right)) and progressively up-\nsample the representation by a factor of two in each fusion\nstage. The ﬁnal representation size has half the resolution\nof the input image. We attach a task-speciﬁc output head to\nproduce the ﬁnal prediction. A schematic overview of the\ncomplete architecture is shown in Figure 1.\nHandling varying image sizes.Akin to fully-convolutional\nnetworks, DPT can handle varying image sizes. As long as\nthe image size is divisible by p, the embedding procedure\ncan be applied and will produce a varying number of im-\nage tokens Np. As a set-to-set architecture, the transformer\nencoder can trivially handle a varying number of tokens.\nHowever, the position embedding has a dependency on the\nimage size as it encodes the locations of the patches in the\ninput image. We follow the approach proposed in [11] and\nlinearly interpolate the position embeddings to the appro-\npriate size. Note that this can be done on the ﬂy for every\nimage. After the embedding procedure and the transformer\nstages, both the reassemble and fusion modules can triv-\nially handle a varying number of tokens, provided that the\ninput image is aligned to the stride of the convolutional de-\ncoder (32 pixels).\n4. Experiments\nWe apply DPT to two dense prediction tasks: monoc-\nular depth estimation and semantic segmentation. For both\ntasks, we show that DPT can signiﬁcantly improve accuracy\nwhen compared to convolutional networks with a similar\ncapacity, especially if a large training dataset is available.\nWe ﬁrst present our main results using the default conﬁgu-\nration and show comprehensive ablations of different DPT\nconﬁgurations at the end of this section.\n4.1. Monocular Depth Estimation\nMonocular depth estimation is typically cast as a dense\nregression problem. It has been shown that massive meta-\ndatasets can be constructed from existing sources of data,\nprovided that some care is taken in how different represen-\ntations of depth are uniﬁed into a common representation\nand that common ambiguities (such as scale ambiguity) are\nappropriately handled in the training loss [30]. Since trans-\nformers are known to realize their full potential only when\nan abundance of training data is available, monocular depth\nestimation is an ideal task to test the capabilities of DPT.\nExperimental protocol.We closely follow the protocol of\nRanftl et al. [30]. We learn a monocular depth prediction\nnetwork using a scale- and shift-invariant trimmed loss that\noperates on an inverse depth representation, together with\nthe gradient-matching loss proposed in [22]. We construct\na meta-dataset that includes the original datasets that were\nused in [30] (referred to as MIX 5 in that work) and extend\nit with with ﬁve additional datasets ([18, 43, 44, 46, 47]).\nTraining set DIW ETH3D Sintel KITTI NYU TUM\nWHDR AbsRel AbsRel δ>1.25 δ>1.25 δ>1.25\nDPT - Large MIX 6 10.82 (-13.2%) 0.089 (-31.2%) 0.270 (-17.5%) 8.46 (-64.6%) 8.32 (-12.9%) 9.97 (-30.3%)\nDPT - Hybrid MIX 6 11.06 (-11.2%) 0.093 (-27.6%) 0.274 (-16.2%) 11.56 (-51.6%) 8.69 (-9.0%) 10.89 (-23.2%)\nMiDaS MIX 6 12.95 (+3.9%) 0.116 (-10.5%) 0.329 (+0.5%) 16.08 (-32.7%) 8.71 (-8.8%) 12.51 (-12.5%)\nMiDaS [30] MIX 5 12.46 0 .129 0 .327 23 .90 9 .55 14 .29\nLi [22] MD [22] 23.15 0 .181 0 .385 36 .29 27 .52 29 .54\nLi [21] MC [21] 26.52 0 .183 0 .405 47 .94 18 .57 17 .71\nWang [40] WS [40] 19.09 0 .205 0 .390 31 .92 29 .57 20 .18\nXian [45] RW [45] 14.59 0 .186 0 .422 34 .08 27 .00 25 .02\nCasser [5] CS [8] 32.80 0 .235 0 .422 21 .15 39 .58 37 .18\nTable 1. Comparison to the state of the art on monocular depth estimation. We evaluate zero-shot cross-dataset transfer according to the\nprotocol deﬁned in [30]. Relative performance is computed with respect to the original MiDaS model [30]. Lower is better for all metrics.\nWe refer to this meta-dataset as MIX 6. It contains about\n1.4 million images and is, to the best of our knowledge, the\nlargest training set for monocular depth estimation that has\never been compiled.\nWe use multi-objective optimization [32] together with\nAdam [19] and set a learning rate of 1e−5 for the back-\nbone and 1e−4 for the decoder weights. The encoder is\ninitialized with ImageNet-pretrained weights, whereas the\ndecoder is initialized randomly. We use an output head that\nconsists of 3 convolutional layers. The output head progres-\nsively halves the feature dimension and upsamples the pre-\ndictions to the input resolution after the ﬁrst convolutional\nlayer (details in supplementary material). We disable batch\nnormalization in the decoder, as we found it to negatively\ninﬂuence results for regression tasks. We resize the image\nsuch that the longer side is 384 pixels and train on random\nsquare crops of size 384. We train for 60 epochs, where one\nepoch consists of 72,000 steps with a batch size of 16. As\nthe batch size is not divisible by the number of datasets, we\nconstruct a mini-batch by ﬁrst drawing datasets uniformly\nat random before sampling from the respective datasets.\nWe perform random horizontal ﬂips for data augmentation.\nSimilar to [30], we ﬁrst pretrain on a well-curated subset of\nthe data [45, 46, 47] for 60 epochs before training on the\nfull dataset.\nδ>1.25 δ>1.252 δ>1.253 AbsRel RMSE log10\nDORN [13] 0.828 0.965 0.992 0.115 0.509 0.051\nVNL [48] 0.875 0.976 0.994 0.111 0.416 0.048\nBTS [20] 0.885 0.978 0.994 0.110 0.392 0.047\nDPT-Hybrid0.904 0.988 0.998 0.110 0.357 0.045\nTable 2. Evaluation on NYUv2 depth.\nδ>1.25 δ>1.252 δ>1.253 AbsRel RMSE RMSE log\nDORN [13] 0.932 0.984 0.994 0.072 2.626 0.120\nVNL [48] 0.938 0.990 0.998 0.072 3.258 0.117\nBTS [20] 0.956 0.993 0.998 0.059 2.756 0.096\nDPT-Hybrid0.959 0.995 0.999 0.062 2.573 0.092\nTable 3. Evaluation on KITTI (Eigen split).\nZero-shot cross-dataset transfer. Table 1 shows the re-\nsults of zero-shot transfer to different datasets that were not\nseen during training. We refer the interested reader to Ran-\nftl et al . [30] for details of the evaluation procedure and\nerror metrics. For all metrics, lower is better. Both DPT\nvariants signiﬁcantly outperform the state of the art. The\naverage relative improvement over the best published archi-\ntecture, MiDaS, is more than 23% for DPT-Hybrid and 28%\nfor DPT-Large. DPT-Hybrid achieves this with a compara-\nble network capacity (Table 9), while DPT-Large is about3\ntimes larger than MiDaS. Note that both architectures have\nsimilar latency to MiDaS (Table 9).\nTo ensure that the observed improvements are not only\ndue to the enlarged training set, we retrain the fully-\nconvolutional network used by MiDaS on our larger meta-\ndataset MIX 6. While the fully-convolutional network in-\ndeed beneﬁts from the larger training set, we observe that\nboth DPT variants still strongly outperform this network.\nThis shows that DPT can better beneﬁt from increased train-\ning set size, an observation that matches previous ﬁndings\non transformer-based architectures in other ﬁelds.\nThe quantitative results are supported by visual com-\nparisons in Figure 2. DPT can better reconstruct ﬁne de-\ntails while also improving global coherence in areas that are\nchallenging for the convolutional architecture (for example,\nlarge homogeneous regions or relative depth arrangement\nacross the image).\nFine-tuning on small datasets.We ﬁne-tune DPT-Hybrid\non the KITTI [15] and NYUv2 [35] datasets to further com-\npare the representational power of DPT to existing work.\nSince the network was trained with an afﬁne-invariant loss,\nits predictions are arbitrarily scaled and shifted and can have\nlarge magnitudes. Direct ﬁne-tuning would thus be chal-\nlenging, as the global mismatch in the magnitude of the\npredictions to the ground truth would dominate the loss.\nWe thus ﬁrst align predictions of the initial network to each\ntraining sample using the robust alignment procedure de-\nscribed in [30]. We then average the resulting scales and\nshifts across the training set and apply the average scale and\nInput MiDaS (MIX 6) DPT-Hybrid DPT-Large\nFigure 2. Sample results for monocular depth estimation. Compared to the fully-convolutional network used by MiDaS, DPT shows better\nglobal coherence (e.g., sky, second row) and ﬁner-grained details (e.g., tree branches, last row).\nshift to the predictions before passing the result to the loss.\nWe ﬁne-tune with the loss proposed by Eigen et al. [12].\nWe disable the gradient-matching loss for KITTI since this\ndataset only provides sparse ground truth.\nTables 2 and 3 summarize the results. Our architecture\nmatches or improves state-of-the-art performance on both\ndatasets in all metrics. This indicates that DPT can also be\nusefully applied to smaller datasets.\n4.2. Semantic Segmentation\nWe choose semantic segmentation as our second task\nsince it is representative of discrete labeling tasks and is\na very competitive proving ground for dense prediction ar-\nchitectures. We employ the same backbone and decoder\nstructure as in previous experiments. We use an output head\nthat predicts at half resolution and upsamples the logits to\nfull resolution using bilinear interpolation (details in sup-\nplementary material). The encoder is again initialized from\nImageNet-pretrained weights, and the decoder is initialized\nrandomly.\nExperimental protocol.We closely follow the protocol es-\ntablished by Zhang et al. [51]. We employ a cross-entropy\nloss and add an auxiliary output head together with an aux-\niliary loss to the output of the penultimate fusion layer. We\nset the weight of the auxiliary loss to 0.2. Dropout with\na rate of 0.1 is used before the ﬁnal classiﬁcation layer in\nboth heads. We use SGD with momentum 0.9 and a poly-\nnomial learning rate scheduler with decay factor 0.9. We\nuse batch normalization in the fusion layers and train with\nbatch size 48. Images are resized to 520 pixels side length.\nWe use random horizontal ﬂipping and random rescaling in\nthe range ∈(0.5,2.0) for data augmentation. We train on\nsquare random crops of size480. We set the learning rate to\n0.002. We use multi-scale inference at test time and report\nboth pixel accuracy (pixAcc) as well as mean Intersection-\nover-Union (mIoU).\nADE20K. We train the DPT on the ADE20K semantic seg-\nmentation dataset [54] for 240 epochs. Table 4 summa-\nrizes our results on the validation set. DPT-Hybrid outper-\nforms all existing fully-convolutional architectures. DPT-\nLarge performs slightly worse, likely because of the sig-\nniﬁcantly smaller dataset compared to our previous experi-\nments. Figure 3 provides visual comparisons. We observe\nthat the DPT tends to produce cleaner and ﬁner-grained de-\nlineations of object boundaries and that the predictions are\nalso in some cases less cluttered.\nFine-tuning on smaller datasets. We ﬁne-tune DPT-\nHybrid on the Pascal Context dataset [26] for 50 epochs. All\nother hyper-parameters remain the same. Table 5 shows re-\nsults on the validation set for this experiment. We again see\nthat DPT can provide strong performance even on smaller\ndatasets.\nResNeSt-200 [51]\nDPT-Hybrid\nFigure 3. Sample results for semantic segmentation on ADE20K (ﬁrst and second column) and Pascal Context (third and fourth column).\nPredictions are frequently better aligned to object edges and less cluttered.\n4.3. Ablations\nWe examine a number of aspects and technical choices in\nDPT via ablation studies. We choose monocular depth esti-\nmation as the task for our ablations and follow the same pro-\ntocol and hyper-parameter settings as previously described.\nWe use a reduced meta-dataset that is composed of three\ndatasets [45, 46, 47] and consists of about 41,000 images.\nWe choose these datasets since they provide high-quality\nground truth. We split each dataset into a training set and\na small validation set of about 1,000 images total. We re-\nport results on the validation sets in terms of relative ab-\nsolute deviation after afﬁne alignment of the predictions to\nthe ground truth [30]. Unless speciﬁed otherwise, we use\nViT-Base as the backbone architecture.\nSkip connections. Convolutional architectures offer natu-\nral points of interest for passing features from the encoder\nto the decoder, namely before or after downsampling of the\nBackbone pixAcc [%] mIoU [%]\nOCNet ResNet101 [50] – 45.45\nACNet ResNet101 [14] 81.96 45.90\nDeeplabV3 ResNeSt-101 [7, 51] 82.07 46.91\nDeeplabV3 ResNeSt-200 [7, 51] 82.45 48.36\nDPT-Hybrid ViT-Hybrid 83.11 49.02\nDPT-Large ViT-Large 82.70 47.63\nTable 4. Semantic segmentation results on the ADE20K validation\nset.\nBackbone pixAcc [%] mIoU [%]\nOCNet HRNet-W48 [42, 50] – 56.2\nDeeplabV3 ResNeSt-200 [7, 51] 82.50 58.37\nDeeplabV3 ResNeSt-269 [7, 51] 83.06 58.92\nDPT-Hybrid ViT-Hybrid 84.83 60.46\nTable 5. Finetuning results on the Pascal Context validation set.\nrepresentation. Since the transformer backbone maintains a\nconstant feature resolution, it is not clear at which points in\nthe backbone features should be tapped. We evaluate sev-\neral possible choices in Table 6 (top). We observe that it is\nbeneﬁcial to tap features from layers that contain low-level\nfeatures as well as deeper layers that contain higher-level\nfeatures. We adopt the best setting for all further experi-\nments.\nWe perform a similar experiment with the hybrid archi-\ntecture in Table 6 (bottom), where R0 and R1 refer to us-\ning features from the ﬁrst and second downsampling stages\nof the ResNet50 embedding network. We observe that us-\ning low-level features from the embedding network leads\nto better performance than using features solely from the\ntransformer stages. We use this setting for all further exper-\niments that involve the hybrid architecture.\nReadout token. Table 7 examines various choices for im-\nplementing the ﬁrst stage of the Reassemble block to han-\ndle the readout token. While ignoring the token yields\ngood performance, projection provides slightly better per-\nformance on average. Adding the token, on the other hand,\nyields worse performance than simply ignoring it. We use\nprojection for all further experiments.\nBackbones. The performance of different backbones is\nLayerl HRWSI BlendedMVS ReDWebMean\nBase\n{3, 6, 9, 12} 0.0793 0.0780 0.0892 0.0822\n{6, 8, 10, 12} 0.0801 0.0789 0.0904 0.0831\n{9, 10, 11, 12} 0.0805 0.0766 0.0912 0.0828\nHybrid\n{3, 6, 9, 12} 0.0747 0.0748 0.0865 0.0787\n{R0, R1, 9, 12} 0.0742 0.0751 0.0857 0.0733\nTable 6. Performance of attaching skip connections to different\nencoder layers. Best results are achieved with a combination of\nskip connections from shallow and deep layers.\nHRWSI BlendedMVS ReDWebMean\nIgnore 0.0793 0.0780 0.0892 0.0822\nAdd 0.0799 0.0789 0.0904 0.0831\nProject 0.0797 0.0764 0.0895 0.0819\nTable 7. Performance of approaches to handle the readout token.\nFusing the readout token to the individual input tokens using a\nprojection layer yields the best performance.\nshown in Table 8. ViT-Large outperforms all other back-\nbones but is also almost three times larger than ViT-Base\nand ViT-Hybrid. ViT-Hybrid outperforms ViT-Base with a\nsimilar number of parameters and has comparable perfor-\nmance to the large backbone. As such it provides a good\ntrade-off between accuracy and capacity.\nViT-Base has comparable performance to ResNext101-\nWSL, while ViT-Hybrid and ViT-Large improve perfor-\nmance even though they have been pretrained on signiﬁ-\ncantly less data. Note that ResNext101-WSL was pretrained\non a billion-scale corpus of weakly supervised data [25] in\naddition to ImageNet pretraining. It has been observed that\nthis pretraining boosts the performance of monocular depth\nprediction [30]. This architecture corresponds to the origi-\nnal MiDaS architecture.\nWe ﬁnally compare to a recent variant of ViT called\nDeIT [38]. DeIT trains the ViT architecture with a more\ndata-efﬁcient pretraining procedure. Note that the DeIT-\nBase architecture is identical to ViT-Base, while DeIT-\nBase-Dist introduces an additional distillation token, which\nwe ignore in the Reassemble operation. We observe that\nDeIT-Base-Dist indeed improves performance when com-\npared to ViT-Base. This indicates that similarly to convo-\nlutional architectures, improvements in pretraining proce-\ndures for image classiﬁcation can beneﬁt dense prediction\ntasks.\nInference resolution. While fully-convolutional architec-\ntures can have large effective receptive ﬁelds in their deepest\nlayers, the layers close to the input are local and have small\nreceptive ﬁelds. Performance thus suffers heavily when\nperforming inference at an input resolution that is signiﬁ-\ncantly different from the training resolution. Transformer\nencoders, on the other hand, have a global receptive ﬁeld\nHRWSI BlendedMVS ReDWebMean\nResNet50 0.0890 0.0887 0.1029 0.0935\nResNext101-WSL 0.0780 0.0751 0.0886 0.0806\nDeIT-Base 0.0798 0.0804 0.0925 0.0842\nDeIT-Base-Dist 0.0758 0.0758 0.0871 0.0796\nViT-Base 0.0797 0.0764 0.0895 0.0819\nViT-Large 0.0740 0.0747 0.0846 0.0778\nViT-Hybrid 0.0738 0.0746 0.0864 0.0783\nTable 8. Ablation of backbones. The hybrid and large backbones\nconsistently outperform the convolutional baselines. The base ar-\nchitecture can outperform the convolutional baseline with better\npretraining (DeIT-Base-Dist).\nResolution\nPerfomrance decrease [%]\n0.00\n5.00\n10.00\n15.00\n20.00\n25.00\n416 448 480 512 544 576 608 640\nViT-Hybrid DeIT-Distilled ResNext-101 ResNet-50\nFigure 4. Relative loss in performance for different inference res-\nolutions (lower is better).\nin every layer. We conjecture that this makes DPT less de-\npendent on inference resolution. To test this hypothesis, we\nplot the loss in performance of different architectures when\nperforming inference at resolutions higher than the training\nresolution of 384×384 pixels. We plot the relative decrease\nin performance in percent with respect to the performance\nof performing inference at the training resolution in Fig-\nure 4. We observe that the performance of DPT variants\nindeed degrades more gracefully as inference resolution in-\ncreases.\nInference speed. Table 9 shows inference time for differ-\nent network architectures. Timings were conducted on an\nIntel Xeon Platinum 8280 CPU @ 2.70GHz with 8 physical\ncores and an Nvidia RTX 2080 GPU. We use square images\nwith a width of 384 pixels and report the average over 400\nruns. DPT-Hybrid and DPT-Large show comparable latency\nto the fully-convolutional architecture used by MiDaS. In-\nterestingly, while DPT-Large is substantially larger than the\nother architectures in terms of parameter count, it has com-\npetitive latency since it exposes a high degree of parallelism\nthrough its wide and comparatively shallow structure.\nMiDaS DPT-Base DPT-Hybrid DPT-Large\nParameters [million] 105 112 123 343\nTime [ms] 32 17 38 35\nTable 9. Model statistics. DPT has comparable inference speed to\nthe state of the art.\n5. Conclusion\nWe have introduced the dense prediction transformer,\nDPT, a neural network architecture that effectively lever-\nages vision transformers for dense prediction tasks. Our\nexperiments on monocular depth estimation and semantic\nsegmentation show that the presented architecture produces\nmore ﬁne-grained and globally coherent predictions when\ncompared to fully-convolutional architectures. Similar to\nprior work on transformers, DPT unfolds its full potential\nwhen trained on large-scale datasets.\nReferences\n[1] Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla.\nSegNet: A deep convolutional encoder-decoder architec-\nture for image segmentation. IEEE TIP, 39(12):2481–2495,\n2017.\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.\nNeural machine translation by jointly learning to align and\ntranslate. In ICLR, 2015.\n[3] Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens,\nand Quoc V Le. Attention augmented convolutional net-\nworks. In ICCV, 2019.\n[4] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\nPranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\nguage models are few-shot learners. In NeurIPS, 2020.\n[5] Vincent Casser, Soeren Pirk, Reza Mahjourian, and Anelia\nAngelova. Unsupervised learning of depth and ego-motion:\nA structured approach. In AAAI, 2019.\n[6] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,\nKevin Murphy, and Alan L. Yuille. DeepLab: Semantic im-\nage segmentation with deep convolutional nets, atrous con-\nvolution, and fully connected crfs. TPAMI, 40(4):834–848,\n2018.\n[7] Liang-Chieh Chen, George Papandreou, Florian Schroff, and\nHartwig Adam. Rethinking atrous convolution for seman-\ntic image segmentation. arXiv preprint arXiv:1706.05587 ,\n2017.\n[8] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo\nRehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe\nFranke, Stefan Roth, and Bernt Schiele. The Cityscapes\ndataset for semantic urban scene understanding. In CVPR,\n2016.\n[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Fei-Fei Li. ImageNet: A large-scale hierarchical image\ndatabase. In CVPR, 2009.\n[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. BERT: Pre-training of deep bidirectional trans-\nformers for language understanding. In ACL, 2019.\n[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. arXiv preprint arXiv:2010.11929, 2020.\n[12] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map\nprediction from a single image using a multi-scale deep net-\nwork. In NeurIPS, 2014.\n[13] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Bat-\nmanghelich, and Dacheng Tao. Deep ordinal regression net-\nwork for monocular depth estimation. In CVPR, 2018.\n[14] Jun Fu, Jing Liu, Yuhang Wang, Yong Li, Yongjun Bao, Jin-\nhui Tang, and Hanqing Lu. Adaptive context network for\nscene parsing. In ICCV, 2019.\n[15] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we\nready for autonomous driving? The KITTI vision benchmark\nsuite. In CVPR, 2012.\n[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR,\n2016.\n[17] Dan Hendrycks and Kevin Gimpel. Gaussian error linear\nunits (GELUs). arXiv preprint arXiv:1606.08415, 2016.\n[18] Xinyu Huang, Peng Wang, Xinjing Cheng, Dingfu Zhou,\nQichuan Geng, and Ruigang Yang. The ApolloScape open\ndataset for autonomous driving and its application. TPAMI,\n42(10):2702–2719, 2020.\n[19] Diederik P. Kingma and Jimmy Lei Ba. Adam: A method\nfor stochastic optimization. In ICLR, 2015.\n[20] Jin Han Lee, Myung-Kyu Han, Dong Wook Ko, and\nIl Hong Suh. From big to small: Multi-scale local planar\nguidance for monocular depth estimation. arXiv preprint\narXiv:1907.10326, 2019.\n[21] Zhengqi Li, Tali Dekel, Forrester Cole, Richard Tucker,\nNoah Snavely, Ce Liu, and William T. Freeman. Learning\nthe depths of moving people by watching frozen people. In\nCVPR, 2019.\n[22] Zhengqi Li and Noah Snavely. MegaDepth: Learning single-\nview depth prediction from Internet photos. In CVPR, 2018.\n[23] Guosheng Lin, Anton Milan, Chunhua Shen, and Ian D.\nReid. ReﬁneNet: Multi-path reﬁnement networks for high-\nresolution semantic segmentation. In CVPR, 2017.\n[24] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. RoBERTa: A ro-\nbustly optimized BERT pretraining approach. arXiv preprint\narXiv:1907.11692, 2019.\n[25] Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan,\nKaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe,\nand Laurens van der Maaten. Exploring the limits of weakly\nsupervised pretraining. In ECCV, 2018.\n[26] Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu\nCho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, and\nAlan L. Yuille. The role of context for object detection and\nsemantic segmentation in the wild. In CVPR, 2014.\n[27] Hyeonwoo Noh, Seunghoon Hong, and Bohyung Han.\nLearning deconvolution network for semantic segmentation.\nIn ICCV, 2015.\n[28] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz\nKaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Im-\nage transformer. In ICML, 2018.\n[29] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan\nBello, Anselm Levskaya, and Jonathon Shlens. Stand-alone\nself-attention in vision models. In NeurIPS, 2019.\n[30] Ren ´e Ranftl, Katrin Lasinger, David Hafner, Konrad\nSchindler, and Vladlen Koltun. Towards robust monocular\ndepth estimation: Mixing datasets for zero-shot cross-dataset\ntransfer. TPAMI, 2020.\n[31] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nNet: Convolutional networks for biomedical image segmen-\ntation. In MICCAI, 2015.\n[32] Ozan Sener and Vladlen Koltun. Multi-task learning as\nmulti-objective optimization. In NeurIPS, 2018.\n[33] Pierre Sermanet, David Eigen, Xiang Zhang, Micha ¨el Math-\nieu, Rob Fergus, and Yann LeCun. OverFeat: Integrated\nrecognition, localization and detection using convolutional\nnetworks. In ICLR, 2014.\n[34] Evan Shelhamer, Jonathan Long, and Trevor Darrell. Fully\nconvolutional networks for semantic segmentation. CVPR,\n2015.\n[35] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob\nFergus. Indoor segmentation and support inference from\nRGBD images. In ECCV, 2012.\n[36] Josef Sivic and Andrew Zisserman. Efﬁcient visual search of\nvideos cast as text retrieval. TPAMI, 31(4):591–606, 2009.\n[37] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep\nhigh-resolution representation learning for human pose esti-\nmation. In CVPR, 2019.\n[38] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv ´e J´egou. Training\ndata-efﬁcient image transformers & distillation through at-\ntention. arXiv preprint arXiv:2012.12877, 2020.\n[39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NeurIPS, 2017.\n[40] Chaoyang Wang, Oliver Wang, Federico Perazzi, and Simon\nLucey. Web stereo video supervision for depth prediction\nfrom dynamic scenes. In 3DV, 2019.\n[41] Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam,\nAlan L. Yuille, and Liang-Chieh Chen. Axial-DeepLab:\nStand-alone axial-attention for panoptic segmentation. In\nECCV, 2020.\n[42] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang,\nChaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui\nTan, Xinggang Wang, Wenyu Liu, and Bin Xiao. Deep\nhigh-resolution representation learning for visual recogni-\ntion. TPAMI, 2020.\n[43] Qiang Wang, Shizhen Zheng, Qingsong Yan, Fei Deng,\nKaiyong Zhao, and Xiaowen Chu. IRS: A large synthetic\nindoor robotics stereo dataset for disparity and surface nor-\nmal estimation. arXiv preprint arXiv:1912.09678, 2019.\n[44] Wenshan Wang, Delong Zhu, Xiangwei Wang, Yaoyu Hu,\nYuheng Qiu, Chen Wang, Yafei Hu, Ashish Kapoor, and Se-\nbastian Scherer. TartanAir: A dataset to push the limits of\nvisual slam. In IROS, 2020.\n[45] Ke Xian, Chunhua Shen, Zhiguo Cao, Hao Lu, Yang Xiao,\nRuibo Li, and Zhenbo Luo. Monocular relative depth per-\nception with web stereo data supervision. In CVPR, 2018.\n[46] Ke Xian, Jianming Zhang, Oliver Wang, Long Mai, Zhe Lin,\nand Zhiguo Cao. Structure-guided ranking loss for single\nimage depth prediction. In CVPR, 2020.\n[47] Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan\nRen, Lei Zhou, Tian Fang, and Long Quan. BlendedMVS:\nA large-scale dataset for generalized multi-view stereo net-\nworks. CVPR, 2020.\n[48] Wei Yin, Yifan Liu, Chunhua Shen, and Youliang Yan. En-\nforcing geometric constraints of virtual normal for depth pre-\ndiction. In ICCV, 2019.\n[49] Fisher Yu and Vladlen Koltun. Multi-scale context aggrega-\ntion by dilated convolutions. In ICLR, 2016.\n[50] Yuhui Yuan, Xilin Chen, and Jingdong Wang. Object-\ncontextual representations for semantic segmentation. In\nECCV, 2020.\n[51] Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Zhi\nZhang, Haibin Lin, Yue Sun, Tong He, Jonas Muller, R.\nManmatha, Mu Li, and Alexander Smola. ResNeSt: Split-\nattention networks. arXiv preprint arXiv:2004.08955, 2020.\n[52] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring\nself-attention for image recognition. In CVPR, 2020.\n[53] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang\nWang, and Jiaya Jia. Pyramid scene parsing network. In\nCVPR, 2017.\n[54] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela\nBarriuso, and Antonio Torralba. Scene parsing through\nADE20K dataset. In CVPR, 2017.\nSupplementary Material\nA. Architecture details\nWe provide additional technical details in this section.\nHybrid encoder. The hybrid encoder is based on a pre-\nactivation ResNet50 with group norm and weight standard-\nization [57]. It deﬁnes four stages after the initial stem, each\nof which downsamples the representation before applying\nmultiple ResNet blocks. We refer by RN to the output of\nthe N-th stage. DPT-Hybrid thus taps skip connections af-\nter the ﬁrst (R0) and second stage (R1).\nResidual convolutional units. Figure A1 (a) shows a\nschematic overview of the residual convolutional units [23]\nthat are used in the decoder. Batch normalization is used for\nsemantic segmentation but is disabled for monocular depth\nestimation. When using batch normalization, we disable bi-\nases in the preceding convolutional layer.\nMonocular depth estimation head.The output head for\nmonocular depth estimation is shown in Figure A1 (b). The\ninitial convolution halves the feature dimensions, while the\nsecond convolution has an output dimension of 32. The ﬁ-\nnal linear layer projects this representation to a non-negative\nscalar that represent the inverse depth prediction for every\npixel. Bilinear interpolation is used to upsample the repre-\nsentation.\nSemantic segmentation head.The output head for seman-\ntic segmentation is shown in Figure A1 (c). the ﬁrst con-\nvolutional block preserves the feature dimension, while the\nﬁnal linear layer projects the representation to the number\nof output classes. Dropout is used with a rate of 0.1. We\nuse bilinear interpolation for the ﬁnal upsampling opera-\ntion. The prediction thus represents the per-pixel logits of\nthe classes.\nB. Additional results\nWe provide additional qualitative and quantitative results\nin this section.\nMonocular depth estimation.We notice that the biggest\ngains in performance for zero-shot transfer were achieved\nfor datasets that feature dense, high-resolution evalua-\ntions [15, 55, 59]. This could be explained by more ﬁne-\ngrained predictions. Visual inspection of sample results\n(c.f . Figure A3) from these datasets conﬁrms this intuition.\nWe observe more details and also better global depth ar-\nrangement in DPT predictions when compared to the fully-\nconvolutional baseline. Note that results for DPT and Mi-\nDaS are computed at the same input resolution (384 pixels).\nSemantic segmentation.We show per-class IoU scores for\nthe ADE20K validation set in Figure A2. While we ob-\nserve a general trend of an improvement in per-class IoU in\ncomparison to the baseline [51], we do not observe a strong\npattern across classes.\nAttention maps. We show attention maps from different\nencoder layers in Figures A4 and A5. In both cases, we\nshow results from the monocular depth estimation models.\nWe visualize the attention of two reference tokens (upper\nleft corner and lower right corner, respectively) to all other\ntokens in the image across various layers in the encoder. We\nshow the average attention over all 12 attention heads.\nWe observe the tendency that attention is spatially more\nlocalized close to the reference token in shallow layers (left-\nmost columns), whereas deeper layers (rightmost columns)\nfrequently attend across the whole image.\nReferences\n[55] D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black. A\nnaturalistic open source movie for optical ﬂow evaluation.\nIn ECCV, 2012.\n[56] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we\nready for autonomous driving? The KITTI vision benchmark\nsuite. In CVPR, 2012.\n[57] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan\nPuigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby.\nBig transfer (bit): General visual representation learning. In\nECCV, 2020.\n[58] Guosheng Lin, Anton Milan, Chunhua Shen, and Ian D.\nReid. ReﬁneNet: Multi-path reﬁnement networks for high-\nresolution semantic segmentation. In CVPR, 2017.\n[59] Thomas Sch ¨ops, Johannes L. Sch¨onberger, Silvano Galliani,\nTorsten Sattler, Konrad Schindler, Marc Pollefeys, and An-\ndreas Geiger. A multi-view stereo benchmark with high-\nresolution images and multi-camera videos. In CVPR, 2017.\nReLU\nConv3x3\nBatchNorm\nReLU\nConv3x3\nBatchNorm\n+ ++\nConv3x3\nResample0.5\nConv3x3-ReLU\nConv1x1-ReLU\nConv3x3-BN-ReLU\nDropout\nConv1x1\nResample0.5\n(a) Residual Convolutional Unit [23] (b) Monocular depth estimation head (c) Semantic segmentation head\nFigure A1. Schematics of different architecture blocks.\nsky\nbed \npool table\ntoilet\ncar\nbuilding\nroad\nperson\nceiling\nfloor\nscreen door\nwall\ncurtain\ntree\ntv\nbathtub\ntowel\nsea\nhood\nsink\nmirror\nprojection screen\nrefrigerator\ncomputer\nfireplace\nstove\nseat\nwater\npicture\nsidewalk\nfan\nwindow\nsofa\nbridge\nwashing machine\nbus\nmotorbike\nmicrowave\ngrass\ndishwasher\nrug\nlamp\nmountain\ntent\ncabinet\narcade machine\nplate\nskyscraper\nradiator\nhouse\ncushion\nbarrel\nchair\nanimal\ncountertop\ntable\npillow\nchandelier\nswimming pool\nlake\nsand\nfountain\nsculpture\ntank\nship\ncoffee table\nplant\ngrandstand\nlight\nbook\nwaterfall\nstool\npalm\nairplane\nsconce\ncase\nconveyer belt\ncradle\nvan\nchest\narmchair\nrunway\nvase\nboat\ndoor\npot\nscreen\nbasket\nhut\ntrash can\nflower\nrock\nfence\ndesk\nkitchen island\nshelf\nflag\nground\ncounter\nbookcase\nsign\nswivel chair\nbicycle\ndirt track\nbench\noven\ncolumn\nfood\nstairs\nbottle\npier\nstaircase\ntraffic light\nstreetlight\nclock\nbase\nrailing\nwardrobe\nottoman\napparel\npath\ncanopy\nbuffet\nescalator\nbox\nbrand\npole\nawning\nmonitor\nbulletin board\ntoy\nbar\nbannister\nbag\nfield\nstep\nposter\nstage\nball\nblanket\nhill\ntruck\ntower\nglass\nriver\ncrt screen\nbooth\ntray\nshower\nland\nClass\n0.0\n0.2\n0.4\n0.6\n0.8IoU\nDPT-Hybrid\nResNeSt-200\nFigure A2. Per class IoU on ADE20K.\nInput MiDaS (MIX 5) DPT-Large\nFigure A3. Additional comparisons for monocular depth estimation.\nInput\n Prediction\nUpper left corner\nLayer 6\n Layer 12\n Layer 18\n Layer 24\nLower right corner\nInput\n Prediction\nUpper left corner\nLayer 6\n Layer 12\n Layer 18\n Layer 24\nLower right corner\nFigure A4. Sample attention maps of the DPT-Large monocular depth prediction network.\nInput\n Prediction\nUpper left corner\nLayer 3\n Layer 6\n Layer 9\n Layer 12\nLower right corner\nInput\n Prediction\nUpper left corner\nLayer 3\n Layer 6\n Layer 9\n Layer 12\nLower right corner\nFigure A5. Sample attention maps of the DPT-Hybrid monocular depth prediction network.",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7251254320144653
    },
    {
      "name": "Transformer",
      "score": 0.7196871042251587
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5920929908752441
    },
    {
      "name": "Architecture",
      "score": 0.5790010690689087
    },
    {
      "name": "Pascal (unit)",
      "score": 0.5637893676757812
    },
    {
      "name": "Segmentation",
      "score": 0.5506404042243958
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5180442333221436
    },
    {
      "name": "Computer vision",
      "score": 0.3966493308544159
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.34803012013435364
    },
    {
      "name": "Voltage",
      "score": 0.14587607979774475
    },
    {
      "name": "Engineering",
      "score": 0.1420007348060608
    },
    {
      "name": "Electrical engineering",
      "score": 0.08901682496070862
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    }
  ],
  "topic": "Computer science"
}