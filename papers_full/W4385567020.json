{
  "title": "ChavanKane at WANLP 2022 Shared Task: Large Language Models for Multi-label Propaganda Detection",
  "url": "https://openalex.org/W4385567020",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2790642444",
      "name": "Tanmay Chavan",
      "affiliations": [
        "Indian Institute of Information Technology, Pune"
      ]
    },
    {
      "id": null,
      "name": "Aditya Manish Kane",
      "affiliations": [
        "Indian Institute of Information Technology, Pune"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2575598244",
    "https://openalex.org/W4297823766",
    "https://openalex.org/W2970487286",
    "https://openalex.org/W3113763975",
    "https://openalex.org/W3176169354",
    "https://openalex.org/W4297805866",
    "https://openalex.org/W3211821677",
    "https://openalex.org/W3008110149",
    "https://openalex.org/W3209721572",
    "https://openalex.org/W3183569911",
    "https://openalex.org/W4385574306",
    "https://openalex.org/W3101739755",
    "https://openalex.org/W3035390927"
  ],
  "abstract": "The spread of propaganda through the internet has increased drastically over the past years. Lately, propaganda detection has started gaining importance because of the negative impact it has on society. In this work, we describe our approach for the WANLP 2022 shared task which handles the task of propaganda detection in a multi-label setting. The task demands the model to label the given text as having one or more types of propaganda techniques. There are a total of 21 propaganda techniques to be detected. We show that an ensemble of five models performs the best on the task, scoring a micro-F1 score of 59.73%. We also conduct comprehensive ablations and propose various future directions for this work.",
  "full_text": "Proceedings of the The Seventh Arabic Natural Language Processing Workshop (WANLP), pages 515 - 519\nDecember 8, 2022 ©2022 Association for Computational Linguistics\nChavanKane at WANLP 2022 Shared Task: Large Language Models for\nMulti-label Propaganda Detection\nTanmay Chavan∗ and Aditya Kane∗\nPune Institute of Computer Technology, Pune\n{chavantanmay1402, adityakane1}@gmail.com\nAbstract\nThe spread of propaganda through the internet\nhas increased drastically over the past years.\nLately, propaganda detection has started gain-\ning importance because of the negative impact\nit has on society. In this work, we describe\nour approach for the W ANLP 2022 shared task\nwhich handles the task of propaganda detection\nin a multi-label setting. The task demands the\nmodel to label the given text as having one or\nmore types of propaganda techniques. There\nare a total of 21 propaganda techniques to be\ndetected. We show that an ensemble of five\nmodels performs the best on the task, scoring\na micro-F1 score of 59.73%. We also conduct\ncomprehensive ablations and propose various\nfuture directions for this work.\n1 Introduction\nThe advent of social media has enabled people to\nview, create and share information easily on the in-\nternet. Such information can easily be accessed and\nviewed by a very large number of people in surpris-\ningly short periods. Moreover, most social media\nwebsites have few restrictions over what the users\nchoose to post and lack preemptive techniques to\ncensor posts before they are uploaded. This has\nenabled the free flow of information from various\nstrata of society which might have been restricted\ndue to the lack of access to proper news sources.\nHowever, this has also led to a stark increase in the\nspread of propaganda through the internet. Informa-\ntion propagated through social media posts presents\nan individual’s personal opinions, and hence is of-\nten biased and lacks rigorous fact-checking. Such\nproblems are less frequently found in the original\nmedia sources of newspapers and TV news chan-\nnels where their posts are subjected to a higher\nlevel of scrutiny.\nThe presence of propaganda online poses a se-\nrious threat to society as it can often polarize the\n∗Equal contribution\nmajority opinion and lead to violent events. A\nwave of misinformation-based propaganda during\nthe time of the COVID-19 pandemic(Cinelli et al.,\n2020) was observed. However, the problem of pro-\npaganda detection is much more complicated than\nit appears. The biggest challenge in propaganda\ndetection is that the bulk of propaganda informa-\ntion is partially based on truths, but is presented in\na manner that might be misleading or unnecessar-\nily polarizing. It is also observed that propaganda\nposts are written professionally and are compelling\nwhich makes most of the readers believe the infor-\nmation to be authentic. All of these problems make\nit difficult to train a model to detect propaganda,\nand much more difficult to interpret the results of\nsuch models. The purpose of the shared task (Alam\net al., 2022), a multi-label classification problem,\nis to come up with efficient methods for detecting\npropaganda on a dataset containing Arabic tweets.\nTransformer-based models have achieved great\nsuccess in text classification tasks. Additionally,\nensemble-based models also outperform these in-\ndividual models. Thus, we explore individual as\nwell as ensemble of models for this task. Further-\nmore, we experimented with oversampling where\nwe repeat the samples having minority labels. We\nalso pretrained the DeHateBERT model on 1 mil-\nlion tweets to study the effect of domain-specific\npretraining on downstream performance. We re-\nport the results of all these experiments and thereby\npropose an ensemble-based method for this task.\n2 Related Work\nDa San Martino et al. (2019) effectively addressed\nthe problem of quantifying different types of pro-\npaganda into seventeen categories, which helps us\ndistinguish between different types of propaganda.\nThey also presented a corpus that contains informa-\ntion classified according to the seventeen classes.\nPrevious shared tasks have generated successful\nresults. The SemEval 2020 task 11 (Da San Mar-\n515\n1 2 3 4 5 7\nNumber of labels per example\n0\n50\n100\n150\n200\n250\n300Number of examples\n291\n178\n93\n39\n6 1\nFigure 1: Distribution of label counts\ntino et al., 2020) used the PTC corpus for building\nmodels to detect and classify propaganda. The Se-\nmEval task 6 (Dimitrov et al., 2021) helped develop\nnovel approaches to detect propaganda in a multi-\nmodal environment. Yu et al. (2021) studied the\ntopic of interpretability of propaganda detection\nand presented an interpretable model.\nThe use of BERT-based models which are pre-\ntrained on a large corpus has proven to yield better\nperformance than most of the other deep learning-\nbased approaches without pre-training(Min et al.,\n2021). There are several BERT models pre-trained\non massive Arabic datasets available. We test\nsome of these models for the task. AraBERT (An-\ntoun et al., 2020), MARBERT, ARBERT (Abdul-\nMageed et al., 2021) are some examples. However,\nmost of these models are pretrained on structured\ndata which significantly differs from tweets. Re-\nsearch has shown that domain-specific pretraining\ncan yield better performance than general text pre-\ntraining(Brady, 2021). Hence, we used DeHate-\nBERT (Aluru et al., 2020).\n3 Data\nThe dataset consists of 504 training examples, 52\nvalidation examples, and 52 testing examples. Our\nmodels were finally evaluated on a separate testing\ndataset, which consisted of 323 examples. Each ex-\nample can have one or more of the 20 propagandist\ntechniques1. Thus, it was a multi-label dataset. The\nnumber of label occurrences is illustrated in Figure\n2. As shown in the figure, we see a skewed distribu-\ntion. This shows that there is an imbalance. Given\nthis problem of multi-label classification with a\n1Complete list of propagandist techniques can be\nfound at https://propaganda.qcri.org/annotations/\ndefinitions.html\nhigh class imbalance, we experimented with sev-\neral architectures and found that DeHateBERT per-\nformed the best on the dataset. A full account of\nall of our successful experiments, as well as failed\nexperiments, is given in Sections 5 and 6. We try\nmultiple methods to mitigate this imbalance, as\nelaborated in Section 6. Since the dataset is a multi-\nlabel dataset, used one-hot encoding for each label\nto denote the ground truth labels.\nFurthermore, we make some key observations\nabout the number of labels per example in Figure\n1. We observe that most examples have one label\nper example. We see that the number of examples\nhaving more than one label diminishes quickly,\nwith only one example having 7 labels.\nWe use basic preprocessing to minimize the\nnoise in the inputs. Firstly, we remove all links\nin the tweet. Then we remove the user mentions\nand hashtags (denoted by \"@\" and \"#\" followed by\na string respectively). Finally, we replace under-\nscores (\"_\") with space. This way, the separated\nwords contribute to the semantics of the sentence.\nNote that we retain the emojis in the sentence since\nthey also carry significant meaning and can aid the\nmodel to better detect sentiment.\n4 System\nGiven this problem of multi-label classification\nwith a high class imbalance, we experimented with\nseveral architectures and found that DeHateBERT\nperformed the best on the dataset. A full account of\nall of our successful experiments, as well as failed\nexperiments, is given in Sections 5 and 6.\nWe tried several models, namely AraBERT v1,\nv02 and v2, MARBERT, ARBERT, XLMRoBERTa\n(Conneau et al., 2020), AraELECTRA (Antoun\net al., 2021). Note that the difference between\nAraBERTv2 and AraBERTv02 is that the former\nuses presegmented text whereas the latter uses the\nFarasa Segmenter (Darwish and Mubarak, 2016) to\nsegment the text since Arabic is a language which\nrequires its words to be segmented before being\nfed into the tokenizer. We used a specific variant\nof DeHateBERT, which is initialized from multilin-\ngual BERT and fine-tuned only on Arabic datasets.\nWe found that this particular variant performed the\namongst the best, in terms of micro-F1 on the test\nsplit of our dataset. Our model training is fairly\nstraightforward. We train DeHateBERT on our\nmulti-label dataset for 30 epochs and the best per-\nforming epoch is chosen based on validation micro-\n516\n0 50 100 150 200 250 300 350\nGlittering generalities (Virtue)\nSmears\nno technique\nRepetition\nExaggeration/Minimisation\nPresenting Irrelevant Data (Red Herring)\nDoubt\nThought-terminating cliché\nAppeal to authority\nObfuscation, Intentional vagueness, Confusion\nWhataboutism\nLoaded Language\nAppeal to fear/prejudice\nCausal Oversimplification\nFlag-waving\nSlogans\nBlack-and-white Fallacy/Dictatorship\nName calling/Labeling\nFigure 2: Data distribution\nF1. We used a learning rate of 3e − 6. Note that\nwe use binary cross-entropy loss, since we have\nmulti-hot labels in our dataset.\nWe also create an ensemble of all the models.\nWe use the five models namely DeHateBERT, AR-\nBERT, AraBERTv02, AraBERTv01, and MAR-\nBERT. Our ensemble system is shown in Figure 3.\nWe use the method of hard voting to obtain the final\nresults. For each sample, we recorded the predicted\nlabels of each of the five models. Then, for each of\nthe 21 labels present, we check how many models\npredict that label. If majority of the models predict\nthe label, we include that label for the sample in\nthe ensemble output. We find that the ensemble of\nmodels had the best performance.\nThe dataset has a significant class imbalance. To\novercome this, we tried to augment the dataset by\noversampling. For oversampling, we duplicated\nthe samples containing less frequent target classes.\nThus we obtained a larger dataset containing du-\nplicate samples but overall having lesser class im-\nbalance. However, this did not yield better perfor-\nmance. We discuss this in detail in Section 6.\n5 Results\nThe official scoring metric for the shared task is\nthe F1 micro score. We present the results of the\nvarious models we tried in Table 1. We have used\nthe official scorer module provided by the organiz-\ners. We can see that the ensemble has the highest\nscore. MARBERT and DeHateBERT have roughly\nsimilar scores and perform better than other mod-\nels. This can lead us to speculate that a model\nmight perform better at classification tasks if it\nis pretrained on a corpus containing data from a\nsimilar source than a corpus with similar charac-\nteristics but having data from a different source.\nThe oversampled DeHateBERT model has a lower\nperformance compared to the model trained on the\noriginal dataset.\nWe can however see that ARBERT outperforms\nall other single models. Another key observation\nis that ARBERT outperforms MARBERT, which\nin turn outperforms all variants of AraBERT. An\nexplanation for this is that AraBERT variants are\ntrained on far less data than ARBERT and MAR-\nBERT. In the case of ARBERT and MARBERT,\nARBERT is pretrained on a wide variety of sources\nas opposed to MARBERT and thus has better per-\nformance than MARBERT.\nWe can also speculate that the high performance\nof the ensemble is because the constituent models\nare pretrained on different datasets. This enables\nthe ensemble to capture a wider array of semantic\nvocabulary and hence is better at predicting classes.\nThe hard voting mechanism ensures that the en-\nsemble will not predict too many classes for each\nsample and thus limits the number of false posi-\ntives.\n6 Discussion\nWe conducted several experiments apart from our\nbest-performing model. Specifically, we tried pre-\ntraining on a large Arabic sentiment analysis tweet\ndataset as well as oversampling the classes having\nfew samples.\nWe retrained the DeHateBERT model on 1 Mil-\nlion tweets from the Large Arabic Twitter Data\nfor Sentiment Analysis dataset using the Masked\nLanguage Modeling technique. We found that pre-\ntraining on the sentiment analysis tweet dataset did\nnot result in any gains to the model. We speculate\n517\nData Preprocessing\nMARBERT\nAraBERTv01\nAraBERTv02\nAraBERTv2\nDeHateBERT Label 1 \nLabel 2\nLabel 1 \nLabel 2 \nLabel 4\nV oting \nLabel 1 3\nLabel 2 4\nLabel 3 2\nLabel 4 3\nLabel 5 1\nLabel 1 \nLabel 3 \nLabel 4\nLabel 1 \nLabel 2 \nLabel 4\nLabel 2 \nLabel 3 \nLabel 5\nLabel 2 \nLabel 4\nFinal labels\nFigure 3: Ensemble system diagram. The ensemble works using a system of hard voting, wherein the prediction of\neach model is recorded and if the majority of models predict that label then it is declared to be one of the predicted\nlabels. This figure illustrates this process in the multi-label setting.\nModel Micro-F1\nAraBERTv01 54.195\nAraBERTv2 50.841\nAraBERTv02 53.996\nAraBERTv02-twitter 54.135\nDeHateBERT 56.484\nOversampling + DeHateBERT 52.529\nMARBERT 56.556\nARBERT 59.048\nEnsemble 59.725\nTable 1: Results of our experiments on the W ANLP-22\npropaganda detection task dataset.\nthis is primarily because the number of tweets we\npretrained the model on is less than the size the\nmodel was originally pretrained on.\nIn another attempt, we implement oversampling\nin the dataset, where we repeat samples of less fre-\nquent classes. We calculate the average number\nof examples for each class. Then, we get the over-\nsampling factor, that is the number of times the\nexamples must be repeated to reach the average\nnumber of samples. We further clip this factor to\n10. Note that, since this is a multi-label scenario,\nwe need to be careful not to use examples with the\nmost frequently occurring classes, in which case\nthe process will have no effect.\nCurrently, we use hard voting for choosing the\nfinal output of the ensemble. We believe better\nresults can be obtained by having a more sophisti-\ncated method like using an SVM instead of hard\nvoting.\n7 Conclusion\nThis paper aims to articulate our approach for the\nWANLP 2022 Shared Task. We experimented\nwith multiple transformer-based models, namely\nAraBERT, ARBERT, MARBERT and others. We\nalso present ablations with monolingual pretrain-\ning, oversampling, and ensemble of the aforemen-\ntioned transformer-based models. We show that the\nensemble consisting of models pretrained on vari-\nous sources of data has the best performance, with\na Micro-F1 score of 59.73%. We foresee several\npossible future directions. One line of work can\nbe to improve the ensemble mechanism as well as\nto better handle the class imbalance in multi-label\nsetting. Another line of work can be to study the ef-\nfects of domain-specific pretraining on downstream\nclassification tasks like multi-label classification.\nAcknowledgement\nWe thank Neeraja Kirtane for her reviews and in-\nputs to this paper.\n518\nReferences\nMuhammad Abdul-Mageed, AbdelRahim Elmadany,\nand El Moatez Billah Nagoudi. 2021. ARBERT &\nMARBERT: Deep bidirectional transformers for Ara-\nbic. In Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers), pages\n7088–7105, Online. Association for Computational\nLinguistics.\nFiroj Alam, Hamdy Mubarak, Wajdi Zaghouani,\nPreslav Nakov, and Giovanni Da San Martino. 2022.\nOverview of the WANLP 2022 shared task on pro-\npaganda detection in Arabic. In Proceedings of the\nSeventh Arabic Natural Language Processing Work-\nshop, Abu Dhabi, UAE. Association for Computa-\ntional Linguistics.\nSai Saketh Aluru, Binny Mathew, Punyajoy Saha, and\nAnimesh Mukherjee. 2020. Deep learning mod-\nels for multilingual hate speech detection. CoRR,\nabs/2004.06465.\nWissam Antoun, Fady Baly, and Hazem Hajj. 2020.\nAraBERT: Transformer-based model for Arabic lan-\nguage understanding. In Proceedings of the 4th Work-\nshop on Open-Source Arabic Corpora and Process-\ning Tools, with a Shared Task on Offensive Language\nDetection, pages 9–15, Marseille, France. European\nLanguage Resource Association.\nWissam Antoun, Fady Baly, and Hazem Hajj. 2021.\nAraELECTRA: Pre-training text discriminators for\nArabic language understanding. In Proceedings of\nthe Sixth Arabic Natural Language Processing Work-\nshop, pages 191–195, Kyiv, Ukraine (Virtual). Asso-\nciation for Computational Linguistics.\nOliver J. Brady. 2021. Aitbert : Domain specific pre-\ntraining on alternative social media to improve hate\nspeech classification.\nMatteo Cinelli, Walter Quattrociocchi, Alessandro\nGaleazzi, Carlo Michele Valensise, Emanuele Brug-\nnoli, Ana Lucia Schmidt, Paola Zola, Fabiana Zollo,\nand Antonio Scala. 2020. The COVID-19 social\nmedia infodemic. Scientific Reports, 10(1).\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nGiovanni Da San Martino, Alberto Barrón-Cedeño,\nHenning Wachsmuth, Rostislav Petrov, and Preslav\nNakov. 2020. SemEval-2020 task 11: Detection of\npropaganda techniques in news articles. In Proceed-\nings of the Fourteenth Workshop on Semantic Evalu-\nation, pages 1377–1414, Barcelona (online). Interna-\ntional Committee for Computational Linguistics.\nGiovanni Da San Martino, Seunghak Yu, Alberto\nBarrón-Cedeño, Rostislav Petrov, and Preslav Nakov.\n2019. Fine-grained analysis of propaganda in news\narticle. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n5636–5646, Hong Kong, China. Association for Com-\nputational Linguistics.\nKareem Darwish and Hamdy Mubarak. 2016. Farasa:\nA new fast and accurate Arabic word segmenter. In\nProceedings of the Tenth International Conference\non Language Resources and Evaluation (LREC’16),\npages 1070–1074, Portorož, Slovenia. European Lan-\nguage Resources Association (ELRA).\nDimitar Dimitrov, Bishr Bin Ali, Shaden Shaar, Firoj\nAlam, Fabrizio Silvestri, Hamed Firooz, Preslav\nNakov, and Giovanni Da San Martino. 2021.\nSemEval-2021 task 6: Detection of persuasion tech-\nniques in texts and images. In Proceedings of the\n15th International Workshop on Semantic Evaluation\n(SemEval-2021), pages 70–98, Online. Association\nfor Computational Linguistics.\nBonan Min, Hayley Ross, Elior Sulem, Amir\nPouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz,\nEneko Agirre, Ilana Heinz, and Dan Roth. 2021. Re-\ncent advances in natural language processing via\nlarge pre-trained language models: A survey.\nSeunghak Yu, Giovanni Da San Martino, Mitra Mo-\nhtarami, James Glass, and Preslav Nakov. 2021. In-\nterpretable propaganda detection in news articles.\nIn Proceedings of the International Conference on\nRecent Advances in Natural Language Processing\n(RANLP 2021), pages 1597–1605, Held Online. IN-\nCOMA Ltd.\n519",
  "topic": "Task (project management)",
  "concepts": [
    {
      "name": "Task (project management)",
      "score": 0.8556662201881409
    },
    {
      "name": "Computer science",
      "score": 0.7483217716217041
    },
    {
      "name": "The Internet",
      "score": 0.5878950357437134
    },
    {
      "name": "Work (physics)",
      "score": 0.48876380920410156
    },
    {
      "name": "Task analysis",
      "score": 0.48705676198005676
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4464855194091797
    },
    {
      "name": "Natural language processing",
      "score": 0.41653335094451904
    },
    {
      "name": "World Wide Web",
      "score": 0.2848251760005951
    },
    {
      "name": "Engineering",
      "score": 0.08244979381561279
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    }
  ]
}