{
  "title": "Adversarial Attacks on Deep Learning Models in Natural Language Processing: A Survey",
  "url": "https://openalex.org/W2940009958",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3209481301",
      "name": "Zhang, Wei Emma",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4214200357",
      "name": "Sheng, Quan Z.",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Alhazmi, Ahoud",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2636280222",
      "name": "Li, Chenliang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2962749469",
    "https://openalex.org/W2963167310",
    "https://openalex.org/W1938755728",
    "https://openalex.org/W1498436455",
    "https://openalex.org/W2739032812",
    "https://openalex.org/W2773446523",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2893424960",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2811010710",
    "https://openalex.org/W2742947407",
    "https://openalex.org/W2883475967",
    "https://openalex.org/W2739181657",
    "https://openalex.org/W2043287290",
    "https://openalex.org/W2963834268",
    "https://openalex.org/W2777449390",
    "https://openalex.org/W2950178297",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W1533230146",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2160536005",
    "https://openalex.org/W2194187530",
    "https://openalex.org/W2964253222",
    "https://openalex.org/W1895577753",
    "https://openalex.org/W2070246124",
    "https://openalex.org/W2618219509",
    "https://openalex.org/W2963969878",
    "https://openalex.org/W2964040467",
    "https://openalex.org/W2124725212",
    "https://openalex.org/W1922655562",
    "https://openalex.org/W2783084496",
    "https://openalex.org/W2784815861",
    "https://openalex.org/W2805757971",
    "https://openalex.org/W2951528484",
    "https://openalex.org/W2114431967",
    "https://openalex.org/W2791319131",
    "https://openalex.org/W2432142698",
    "https://openalex.org/W2904025731",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2125908420",
    "https://openalex.org/W2962700793",
    "https://openalex.org/W2791941932",
    "https://openalex.org/W2952451294",
    "https://openalex.org/W2753981877",
    "https://openalex.org/W2963223306",
    "https://openalex.org/W2884159461",
    "https://openalex.org/W2888597155",
    "https://openalex.org/W2785681938",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W2122672392",
    "https://openalex.org/W2788864200",
    "https://openalex.org/W2888491130",
    "https://openalex.org/W2792942633",
    "https://openalex.org/W2180612164",
    "https://openalex.org/W2964159373",
    "https://openalex.org/W2772621923",
    "https://openalex.org/W2570685808",
    "https://openalex.org/W2964153729",
    "https://openalex.org/W2251869843",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2104246439",
    "https://openalex.org/W2531207078",
    "https://openalex.org/W1604644367",
    "https://openalex.org/W2608787653",
    "https://openalex.org/W2962732637",
    "https://openalex.org/W2783113218",
    "https://openalex.org/W2104518905",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W1552847225",
    "https://openalex.org/W2890719433",
    "https://openalex.org/W3159754263",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2963389226",
    "https://openalex.org/W2243397390",
    "https://openalex.org/W2810192346",
    "https://openalex.org/W2609368435",
    "https://openalex.org/W2509437949",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W1959608418",
    "https://openalex.org/W2603766943",
    "https://openalex.org/W2962718684",
    "https://openalex.org/W2962818281",
    "https://openalex.org/W2061873838",
    "https://openalex.org/W658020064",
    "https://openalex.org/W2410983263",
    "https://openalex.org/W2962995403",
    "https://openalex.org/W2799194071",
    "https://openalex.org/W2833577183",
    "https://openalex.org/W2735135478",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2788496822",
    "https://openalex.org/W1514535095",
    "https://openalex.org/W2099471712",
    "https://openalex.org/W2969026700",
    "https://openalex.org/W2963207607",
    "https://openalex.org/W2963106521",
    "https://openalex.org/W2766108848",
    "https://openalex.org/W2963859254",
    "https://openalex.org/W2760600531",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2952186591",
    "https://openalex.org/W1966948031",
    "https://openalex.org/W2799007037",
    "https://openalex.org/W1566346388",
    "https://openalex.org/W2551396370",
    "https://openalex.org/W2963790827",
    "https://openalex.org/W2799420921",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2556096924",
    "https://openalex.org/W2963890755",
    "https://openalex.org/W2890571031",
    "https://openalex.org/W2413794162",
    "https://openalex.org/W2129767020",
    "https://openalex.org/W2964091467",
    "https://openalex.org/W2963212250",
    "https://openalex.org/W2557283755",
    "https://openalex.org/W2759461933",
    "https://openalex.org/W1998042868",
    "https://openalex.org/W2118463056",
    "https://openalex.org/W2962854379",
    "https://openalex.org/W2962835968",
    "https://openalex.org/W2799244840",
    "https://openalex.org/W2963626623",
    "https://openalex.org/W3122775348",
    "https://openalex.org/W2767899794",
    "https://openalex.org/W1810499140",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2810689564",
    "https://openalex.org/W2744095836",
    "https://openalex.org/W635530177",
    "https://openalex.org/W2184135559",
    "https://openalex.org/W2885183727",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W2963857521",
    "https://openalex.org/W2964082701",
    "https://openalex.org/W2950133940",
    "https://openalex.org/W1981283549",
    "https://openalex.org/W2962883855",
    "https://openalex.org/W2963126845",
    "https://openalex.org/W2963012544",
    "https://openalex.org/W2963249435",
    "https://openalex.org/W2949541494",
    "https://openalex.org/W2594229957",
    "https://openalex.org/W2888137286",
    "https://openalex.org/W2949547296",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2150102617",
    "https://openalex.org/W2949190449",
    "https://openalex.org/W2950761309",
    "https://openalex.org/W2963080779",
    "https://openalex.org/W2143612262",
    "https://openalex.org/W2407338347",
    "https://openalex.org/W2343954916",
    "https://openalex.org/W2606089314",
    "https://openalex.org/W2963217826",
    "https://openalex.org/W2964185534",
    "https://openalex.org/W2787487383",
    "https://openalex.org/W2786611392",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2963383024",
    "https://openalex.org/W2469104253",
    "https://openalex.org/W2962958286",
    "https://openalex.org/W2963783970",
    "https://openalex.org/W2963224792"
  ],
  "abstract": "With the development of high computational devices, deep neural networks (DNNs), in recent years, have gained significant popularity in many Artificial Intelligence (AI) applications. However, previous efforts have shown that DNNs were vulnerable to strategically modified samples, named adversarial examples. These samples are generated with some imperceptible perturbations but can fool the DNNs to give false predictions. Inspired by the popularity of generating adversarial examples for image DNNs, research efforts on attacking DNNs for textual applications emerges in recent years. However, existing perturbation methods for images cannotbe directly applied to texts as text data is discrete. In this article, we review research works that address this difference and generatetextual adversarial examples on DNNs. We collect, select, summarize, discuss and analyze these works in a comprehensive way andcover all the related information to make the article self-contained. Finally, drawing on the reviewed literature, we provide further discussions and suggestions on this topic.",
  "full_text": "Adversarial Attacks on Deep Learning Models in Natural\nLanguage Processing: A Survey\nWEI EMMA ZHANG, QUAN Z. SHENG, and AHOUD ALHAZMI, Macquarie University,\nAustralia\nCHENLIANG LI, Wuhan University, China\nWith the development of high computational devices, deep neural networks (DNNs), in recent years, have\ngained significant popularity in many Artificial Intelligence (AI) applications. However, previous efforts have\nshown that DNNs were vulnerable to strategically modified samples, named adversarial examples . These\nsamples are generated with some imperceptible perturbations, but can fool the DNNs to give false predictions.\nInspired by the popularity of generating adversarial examples for image DNNs, research efforts on attacking\nDNNs for textual applications emerges in recent years. However, existing perturbation methods for images\ncannot be directly applied to texts as text data is discrete in nature. In this article, we review research works\nthat address this difference and generate textual adversarial examples on DNNs. We collect, select, summarize,\ndiscuss and analyze these works in a comprehensive way and cover all the related information to make\nthe article self-contained. Finally, drawing on the reviewed literature, we provide further discussions and\nsuggestions on this topic.\nCCS Concepts: • Computing methodologies → Natural language processing ; Neural networks.\nAdditional Key Words and Phrases: Deep neural networks, adversarial examples, textual data, natural language\nprocessing\nACM Reference Format:\nWei Emma Zhang, Quan Z. Sheng, Ahoud Alhazmi, and Chenliang Li. 2019. Adversarial Attacks on Deep\nLearning Models in Natural Language Processing: A Survey. 1, 1 (April 2019), 40 pages. https://doi.org/10.\n1145/nnnnnnn.nnnnnnn\n1 INTRODUCTION\nDeep neural networks (DNNs) are large neural networks whose architecture is organized as a series\nof layers of neurons, each of which serves as the individual computing units. Neurons are connected\nby links with different weights and biases and transmit the results of its activation function on its\ninputs to the neurons of the next layer. Deep neural networks try to mimic the biological neural\nnetworks of human brains to learn and build knowledge from examples. Thus they are shown\nthe strengths in dealing with complicated tasks that are not easily to be modelled as linear or\nnon-linear problems. Further more, empowered by continuous real-valued vector representations\n(i.e., embeddings) they are good at handling data with various modalities, e.g., image, text, video\nand audio.\nAuthors’ addresses: Wei Emma Zhang, w.zhang@mq.edu.au; Quan Z. Sheng, michael.sheng@mq.edu.au; Ahoud Alhazmi,\nahoud.alhazmi@hdr.mq.edu.au, Macquarie University, Sydney, Australia, NSW 2109; Chenliang Li, Wuhan University,\nWuhan, China, cllee@whu.edu.cn.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and\nthe full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\n© 2019 Association for Computing Machinery.\nXXXX-XXXX/2019/4-ART $15.00\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn\n, Vol. 1, No. 1, Article . Publication date: April 2019.\narXiv:1901.06796v3  [cs.CL]  11 Apr 2019\n2 Zhang et al.\nWith the development of high computational devices, deep neural networks, in recent years have\ngained significant popularity in many Artificial Intelligence (AI) communities such as Computer\nVision [66, 126], Natural Language Processing [18, 67], Web Mining [102, 149] and Game theory\n[119]. However, the interpretability of deep neural networks is still unsatisfactory as they work\nas black boxes, which means it is difficult to get intuitions from what each neuron exactly has\nlearned. One of the problems of the poor interpretability is evaluating the robustness of deep\nneural networks. In recent years, research works [42, 132] used small unperceivable perturbations\nto evaluate the robustness of deep neural networks and found that they are not robust to these\nperturbations. Szegedy et al. [132] first evaluated the state-of-the-art deep neural networks used\nfor image classification with small generated perturbations on the input images. They found that\nthe image classifier were fooled with high probability, but human judgment is not affected. The\nperturbed image pixels were namedadversarial examples and this notation is later used to denote all\nkinds of perturbed samples in a general manner. As the generation of adversarial examples is costly\nand impractical in [132], Goodfellow et al. [42] proposed a fast generation method which popularized\nthis research topic (Section 3.1 provides further discussion on these works). Followed their works,\nmany research efforts have been made and the purposes of these works can be summarized as:\ni) evaluating the deep neural networks by fooling them with unperceivable perturbations; ii)\nintentionally changing the output of the deep neural networks; and iii) detecting the oversensitivity\nand over-stability points of the deep neural networks and finding solutions to defense the attack.\nJia and Liang [55] are the first to consider adversarial example generation (or adversarial attack ,\nwe will use these two expressions interchangeably hereafter) on deep neural networks for text-\nbased tasks (namely textual deep neural networks ). Their work quickly gained research attention\nin Natural Language Processing (NLP) community. However, due to intrinsic differences between\nimages and textual data, the adversarial attack methods on images cannot be directly applied to\nthe latter one. First of all, image data (e.g., pixel values) is continuous, but textual data is discrete.\nConventionally, we vectorize the texts before inputting them into the deep neural networks.\nTraditional vectoring methods include leveraging term frequency and inverse document frequency,\nand one-hot representation (details in Section 3.3). When applying gradient-based adversarial\nattacks adopted from images on these representations, the generated adversarial examples are\ninvalid characters or word sequences [156]. One solution is to use word embeddings as the input of\ndeep neural networks. However, this will also generate words that can not be matched with any\nwords in the word embedding space [39]. Secondly, the perturbation of images are small change of\npixel values that are hard to be perceived by human eyes, thus humans can correctly classify the\nimages, showing the poor robustness of deep neural models. But for adversarial attack on texts,\nsmall perturbations are easily perceptible. For example, replacement of characters or words would\ngenerate invalid words or syntactically-incorrect sentences. Further, it would alter the semantics of\nthe sentence drastically. Therefore, the perturbations are easily to be perceived–in this case, even\nhuman being cannot provide correct predictions.\nTo address the aforementioned differences and challenges, many attacking methods are proposed\nsince the pioneer work of Jia and Liang [55]. Despite the popularity of the topic in NLP community,\nthere is no comprehensive review paper that collect and summarize the efforts in this research\ndirection. There is a need for this kind of work that helps successive researchers and practitioners\nto have an overview of these methods.\nRelated surveys and the differences to this survey. In [10], the authors presented compre-\nhensive review on different classes of attacks and defenses against machine learning systems.\nSpecifically, they proposed a taxonomy for identifying and analyzing these attacks and applied\nthe attacks on a machine learning based application, i.e., a statistical spam filter, to illustrate the\n, Vol. 1, No. 1, Article . Publication date: April 2019.\nAdversarial Attacks on Deep Learning Models in Natural Language Processing: A Survey 3\neffectiveness of the attack and defense. This work targeted machine learning algorithms rather than\nneural models. Inspired by [10], the authors in [36] reviewed the defences of adversarial attack in\nthe security point of view. The work is not limited to machine learning algorithms or neural models,\nbut a generic report about adversarial defenses on security related applications. The authors found\nthat existing security related defense works lack of clear motivations and explanations on how the\nattacks are related to the real security problems and how the attack and defense are meaningfully\nevaluated. Thus they established a taxonomy of motivations, constraints, and abilities for more\nplausible adversaries. [15] provides a thorough overview of the evolution of the adversarial attack\nresearch over the last ten years, and focuses on the research works from computer vision and cyber\nsecurity. The paper covers the works from pioneering non-deep leaning algorithms to recent deep\nlearning algorithms. It is also from the security point of view to provide detailed analysis on the\neffect of the attacks and defenses. The authors of [81] reviewed the same problem in a data-driven\nperspective. They analyzed the attacks and defenses according to the learning phases, i.e., the\ntraining phase and test phase.\nUnlike previous works that discuss generally on the attack methods on machine learning al-\ngorithms, [154] focuses on the adversarial examples on deep learning models. It reviews current\nresearch efforts on attacking various deep neural networks in different applications. The defense\nmethods are also extensively surveyed. However, they mainly discussed adversarial examples\nfor image classification and object recognition tasks. The work in [2] provides a comprehensive\nreview on the adversarial attacks on deep learning models used in computer vision tasks. It is\nan application-driven survey that groups the attack methods according to the sub-tasks under\ncomputer vision area. The article also comprehensively reports the works on the defense side, the\nmethods of which are mainly grouped into three categories.\nAll the mentioned works either target general overview of the attacks and defenses on machine\nlearning models or focus on specific domains such as computer vision and cyber security. Our work\ndiffers with them that we specifically focus on the attacks and defenses on textual deep learning\nmodels. Furthermore, we provide a comprehensive review that covers information from different\naspects to make this survey self-contained.\nPapers selection. The papers we reviewd in this article are high quality papers selected from top\nNLP and AI conferences, including ACL1, COLING2, NAACL3, EMNLP4, ICLR5, AAAI6 and IJCAI7.\nOther than accepted papers in aforementioned conferences, we also consider good papers in e-Print\narchive8, as it reflects the latest research outputs. We select papers from archive with three metrics:\npaper quality, method novelty and the number of citations (optional9).\nContributions of this survey. The aim of this survey is to provide a comprehensive review on\nthe research efforts on generating adversarial examples on textual deep neural networks. It is\nmotivated by the drastically increasing attentions on this topic. This survey will serve researchers\nand practitioners who are interested in attacking textual deep neural models. More broadly, it can\nserve as a reference on how deep learning is applied in NLP community. We expect that the readers\n1Annual Meeting of the Association for Computational Linguistics\n2International Conference on Computational Linguistics\n3Annual Conference of the North American Chapter of the Association for Computational Linguistics\n4Empirical Methods in Natural Language Processing\n5International Conference on Learning Representations\n6AAAI Conference on Artificial Intelligence\n7International Joint Conference on Artificial Intelligence\n8arXiv.org\n9As the research topic emerges from 2017, we relax the citation number to over five if it is published more than one year. If\nthe paper has less than five citations, but is very recent and satisfies the other two metrics, we also include it in this paper.\n, Vol. 1, No. 1, Article . Publication date: April 2019.\n4 Zhang et al.\nhave some basic knowledge of the deep neural networks architectures, which are not the focus in\nthis article. To summarize, the key contributions of this survey are:\n•We conduct a comprehensive review for adversarial attacks on textual deep neural models\nand propose different classification schemes to organize the reviewed literature; this is the\nfirst work of this kind;\n•We provide all related information to make the survey self-contained and thus it is easy for\nreaders who have limited NLP knowledge to understand;\n•We discuss some open issues, and identify some possible research directions in this research\nfield aims to build more robust textual deep learning models with the help of adversarial\nexamples.\nThe remainder of this paper is organized as follows: We introduce the preliminaries for adversarial\nattacks on deep learning models in Section 2 including the taxonomy of adversarial attacks and\ndeep learning models used in NLP. In Section 3, we address the difference of attacking image data\nand textual data and briefly reviewed exemplary works for attacking image DNN that inspired\ntheir follow-ups in NLP. Section 4 first presents our classification on the literature and then gives\na detailed introduction to the state of the art. We discuss the defense strategies in Section 5 and\npoint out the open issues in Section 6. Finally, the article is concluded in Section 7.\n2 OVERVIEW OF ADVERSARIAL ATTACKS AND DEEP LEARNING TECHNIQUES IN\nNATURAL LANGUAGE PROCESSING\nBefore we dive into the details of this survey, we start with an introduction to the general taxonomy\nof adversarial attack on deep learning models. We also introduce the deep learning techniques and\ntheir applications in natural language processing.\n2.1 Adversarial Attacks on Deep Learning Models: The General Taxonomy\nIn this section, we provide the definitions of adversarial attacks and introduce different aspects\nof the attacks, followed by the measurement of perturbations and the evaluation metrics of the\neffectiveness of the attacks in a general manner that applies to any data modality.\n2.1.1 Definitions.\n•Deep Neural Network (DNN). A deep neural network (we use DNN and deep learning model\ninterchangeably hereafter) can be simply presented as a nonlinear function fθ : X →Y,\nwhere X is the input features/attributes, Y is the output predictions that can be a discrete\nset of classes or a sequence of objects. θ represents the DNN parameters and are learned\nvia gradient-based back-propagation during the model training. Best parameters would be\nobtained by minimizing the the gap between the model’s prediction fθ(X)and the correct\nlabel Y, where the gap is measured by loss function J(fθ(X), Y).\n•Perturbations. Perturbations are intently created small noises that to be added to the original\ninput data examples in test stage, aiming to fool the deep learning models.\n•Adversarial Examples. An adversarial example x′ is an example created via worst-case\nperturbation of the input to a deep learning model. An ideal DNN would still assign correct\nclass y (in the case of classification task) tox′, while a victim DNN would have high confidence\non wrong prediction of x′. x′can be formalized as:\nx′= x + η, f (x)= y, x ∈X (1)\nf (x′), y\nor f (x′)= y′, y′, y\n, Vol. 1, No. 1, Article . Publication date: April 2019.\nAdversarial Attacks on Deep Learning Models in Natural Language Processing: A Survey 5\nwhere ηis the worst-case perturbation. The goal of the adversarial attack can be deviating\nthe label to incorrect one (f (x′), y) or specified one (f (x′)= y′).\n2.1.2 Threat Model. We adopt the definition of Threat Model for attacking DNN from [154]. In this\nsection, we discuss several aspects of the threat model.\n•Model Knowledge. The adversarial examples can be generated using black-box or white-box\nstrategies in terms of the knowledge of the attacked DNN. Black-box attack is performed\nwhen the architectures, parameters, loss function, activation functions and training data of\nthe DNN are not accessible. Adversarial examples are generated by directly accessing the test\ndataset, or by querying the DNN and checking the output change. On the contrary, white-box\nattack is based on the knowledge of certain aforementioned information of DNN.\n•Target. The generated adversarial examples can change the output prediction to be incorrect\nor to specific result as shown in Eq. (1). Compared to the un-targeted attack ( f (x′), y),\ntargeted attack ( f (x′)= y′) is more strict as it not only changes the prediction, but also\nenforces constraint on the output to generate specified prediction. For binary tasks, e.g.,\nbinary classification, un-targeted attack equals to the targeted attack.\n•Granularity. The attack granularity refers to the level of data on which the adversarial\nexamples are generated from. For example, it is usually the image pixels for image data.\nRegarding the textual data, it could be character, word, and sentence-level embedding. Section\n3.3 will give further introduction on attack granularity for textual DNN.\n•Motivation. Generating adversarial examples is motivated by two goals: attack and defense.\nThe attack aims to examine the robustness of the target DNN, while the defense takes a step\nfurther utilizing generated adversarial examples to robustify the target DNN. Section 5 will\ngive more details.\n2.1.3 Measurement. Two groups of measurements are required in the adversarial attack for i)\ncontrolling the perturbations and ii) evaluating the effectiveness of the attack, respectively.\n•Perturbation Constraint. As aforementioned, the perturbation ηshould not change the true\nclass label of the input - that is, an ideal DNN classifier, if we take classification as example,\nwill provide the same prediction on the adversarial example to the original example.ηcannot\nbe too small as well, to avoid ending up with no affect on target DNNs. Ideally, effective\nperturbation is the maximum value in a constrained range. [ 132] firstly put a constraint\nthat (x + η)∈[ 0, 1]n for image adversarial examples, ensuring the adversarial example has\nthe same range of pixel values as the original data [ 143]. [42] simplifies the solution and\nuse max norm to constrain η: ||η||∞ ≤ϵ. This was inspired by the intuitive observation\nthat a perturbation which does not change any specific pixel by more than some amount\nϵ cannot change the output class [ 143]. Using max-norm is sufficient enough for image\nclassification/object recognition tasks. Later on, other norms, e.g., L2 and L0, were used to\ncontrol the perturbation in attacking DNN in computer vision. Constraining η for textual\nadversarial attack is somehow different. Section 3.3 will give more details.\n•Attack Evaluation. Adversarial attacks are designed to degrade the performance of DNNs.\nTherefore, evaluating the effectiveness of the attack is based on the performance metrics of\ndifferent tasks. For example, classification tasks has metrics such as accuracy, F1 score and\nAUC score. We leave the metrics for different NLP as out-of-scope content in this article and\nsuggest readers refer to specific tasks for information.\n, Vol. 1, No. 1, Article . Publication date: April 2019.\n6 Zhang et al.\n2.2 Deep Learning in NLP\nNeural networks have been gaining increasing popularity in NLP community in recent years and\nvarious DNN models have been adopted in different NLP tasks. Apart from the feed forward neural\nnetworks and Convolutional Neural Networks (CNN), Recurrent/Recursive Neural Networks (RNN)\nand their variants are the most common neural networks used in NLP, because of their natural\nability of handling sequences. In recent years, two important breakthroughs in deep learning\nare brought into NLP. They are sequence-to-sequence learning [131] and attention modeling [8].\nReinforcement learning and generative models are also gained much popularity [ 152]. In this\nsection, we will briefly overview the DNN architectures and techniques applied in NLP that are\nclosely related to this survey. We suggest readers refer to detailed reviews of neural networks in\nNLP in [101, 152].\n2.2.1 Feed Forward Networks. Feed-forward network, in particular multi-layer perceptrons (MLP),\nis the simplest neural network. It has several forward layers and each node in a layer connects\nto each node in the following layer, making the network fully connected. MLP utilizes nonlinear\nactivation function to distinguish data that is not linearly separable. MLP works with fixed-sized\ninputs and do not record the order of the elements. Thus it is mostly used in the tasks that can\nbe formed as supervised learning problems. In NLP, it can be used in any application. The major\ndrawback of feed forward networks in NLP is that it cannot handle well the text sequences in\nwhich the word order matters.\nAs the feed forward network is easy to implement, there are various implementations and no\nstandard benchmark architecture worth examining. To evaluate the robustness of feed forward net-\nwork in NLP, adversarial examples are often generated for specific architectures in real applications.\nFor example, authors of [3, 45, 46] worked on the specified malware detection models.\n2.2.2 Convolutional Neural Network (CNN) . Convolutional Neural Network contains convolutional\nlayers and pooling (down-sampling) layers and final fully-connected layer. Activation functions are\nused to connect the down-sampled layer to the next convolutional layer or fully-connected layer.\nCNN allows arbitrarily-sized inputs. Convolutional layer uses convolution operation to extract\nmeaningful local patterns of input. Pooling layer reduces the parameters and computation in the\nnetwork and it allows the network to be deeper and less-overfitting. Overall, CNN identifies local\npredictors and combines them together to generate a fixed-sized vector for the inputs, which\ncontains the most or important informative aspects for the application task. In addition, it is\norder-sensitive. Therefore, it excels in computer vision tasks and later is widely adopted in NLP\napplications.\nYoon Kim [60] adopted CNN for sentence classification. He used Word2Vec to represent words\nas input. Then the convolutional operation is limited to the direction of word sequence, rather than\nthe word embeddings. Multiple filters in pooling layers deal with the variable length of sentences.\nThe model demonstrated excellent performances on several benchmark datasets against multiple\nstate-of-the-art works. This work became a benchmark work of adopting CNN in NLP applications.\nZhang et al. [ 155] presented CNN for text classification at character level. They used one-hot\nrepresentation in alphabet for each of the character. To control the generalization error of the\nproposed CNN, they additionally performed data augmentation by replacing words and phrases\nwith their synonyms. These two representative textual CNNs are evaluated via adversarial examples\nin many applications [13, 30, 31, 35, 78].\n2.2.3 Recurrent Neural Networks/ Recursive Neural Networks. Recurrent Neural Networks are neural\nmodels adapted from feed-forward neural networks for learning mappings between sequential\ninputs and outputs [116]. RNNs allows data with arbitrary length and it introduces cycles in their\n, Vol. 1, No. 1, Article . Publication date: April 2019.\nAdversarial Attacks on Deep Learning Models in Natural Language Processing: A Survey 7\ncomputational graph to model efficiently the influence of time [ 40]. The model does not suffer\nfrom statistical estimation problems stemming from data sparsity and thus leads to impressive\nperformance in dealing with sequential data [37]. Recursive neural networks [38] extends recurrent\nneural networks from sequences to tree, which respects the hierarchy of the language. In some\nsituations, backwards dependencies exist, which is in need for the backward analysis. Bi-directional\nRNN thus was proposed for looking at sentences in both directions, forwards and backwards, using\ntwo parallel RNN networks, and combining their outputs. Bengio et al. [14] is one of the first to\napply RNN in NLP. Specifically, they utilized RNN in language model, where the probability of a\nsequence of words is computed in an recurrent manner. The input to RNN is the feature vectors for\nall the preceding words, and the output is the conditional probability distribution over the output\nvocabulary. Since RNN is a natural choice to model various kinds of sequential data, it has been\napplied to many NLP tasks. Hence RNN has drawn great interest for adversarial attack [104].\nRNN has many variants, among which Long Short-Term Memory (LSTM) network [51] gains the\nmost popularity. LSTM is a specific RNN that was designed to capture the long-term dependencies. In\nLSTM, the hidden state are computed through combination of threegates, i.e., input gate, forget gate\nand output gate, that control information flow drawing on the logistic function. LSTM networks have\nsubsequently proved to be more effective than conventional RNNs [44]. GRUs is a simplified version\nof LSTM that it only consists two gates, thus it is more efficient in terms of training and prediction.\nSome popular LSTM variants are proposed to solve various NLP tasks [23, 23, 51, 112, 133, 141, 146].\nThese representative works received the interests of evaluation with adversarial examples recently\n[35, 54, 55, 92, 104, 112, 118, 130, 156].\n2.2.4 Sequence-to-Sequence Learning (Seq2Seq) Models. Sequence-to-sequence learning (Seq2Seq)\n[131] is one of the important breakthroughs in deep learning and is now widely used for NLP\napplications. Seq2Seq model has the superior capacity to generate another sequence information for\na given sequence information with an encoder-decoder architecture [77]. Usually, a Seq2Seq model\nconsists of two recurrent neural networks: an encoder that processes the input and compresses\nit into a vector representation, a decoder that predicts the output. Latent Variable Hierarchical\nRecurrent Encoder-Decoder (VHRED) model [122] is a recently popular Seq2Seq model that generate\nsequences leveraging the complex dependencies between subsequences. [ 25] is one of the first\nneural machine translation (NMT) model that adopt the Seq2Seq model. OpenNMT [64], a Seq2Seq\nNMT model proposed recently, becomes one of the benchmark works in NMT. As they are adopted\nand applied widely, attack works also emerge [24, 31, 99, 127].\n2.2.5 Attention Models. Attention mechanism [9] is another breakthrough in deep leaning. It was\ninitially developed to overcome the difficulty of encoding a long sequence required in Seq2Seq mod-\nels [77]. Attention allows the decoder to look back on the hidden states of the source sequence. The\nhidden states then provide a weighted average as additional input to the decoder. This mechanism\npays attention on informative parts of the sequence. Rather than looking at the input sequence\nin vanilla attention models, self-attention [136] in NLP is used to look at the surrounding words\nin a sequence to obtain more contextually sensitive word representations [ 152]. BiDAF [121] is\na bidirectional attention flow mechanism for machine comprehension and achieved outstanding\nperformance when proposed. [ 55, 127] evaluated the robustness of this model via adversarial\nexamples and became the first few works using adversarial examples for attacking textual DNNs.\nOther attention-based DNNs [26, 108] also received adversarial attacks recently [30, 92].\n2.2.6 Reinforcement Learning Models. Reinforcement learning trains an agent by giving a reward\nafter agents performing discrete actions. In NLP, reinforcement learning framework usually consist\nof an agent (a DNN), a policy (guiding action) and a reward. The agent picks an action (e.g.,\n, Vol. 1, No. 1, Article . Publication date: April 2019.\n8 Zhang et al.\npredicting next word in a sequence) based on a policy, then updates its internal state accordingly,\nuntil arriving the end of the sequence where a reward is calculated. Reinforcement learning requires\nproper handling of the action and the states, which may limit the expressive power and learning\ncapacity of the models [152]. But it gains much interests in task-oriented dialogue systems [75] as\nthey share the fundamental principle as decision making processes. Limited works so far can be\nfound to attack the reinforcement learning model in NLP [99].\n2.2.7 Deep Generative Models. In recent years, two powerful deep generative models, Generative\nAdversarial Networks (GANs) [41] and Variational Auto-Encoders (VAEs) [63] are proposed and\ngain much research attention. Generative models are able to generate realistic data that are very\nsimilar to ground truth data in a latent space. In NLP field, they are used to generate textual data.\nGANs [41] consist of two adversarial networks: a generator and a discriminator. Discriminator is\nto discriminate the real and generated samples, while the generator is to generate realistic samples\nthat aims to fool the discriminator. GAN uses a min-max loss function to train two neural networks\nsimultaneously. VAEs consist of encoder and generator networks. Encoder encodes an input into a\nlatent space and the generator generates samples from the latent space. Deep generative models is\nnot easy to train and evaluate. Hence, these deficiencies hinder their wide usage in many real-world\napplications [152]. Although they have been adopted in generating texts, so far no work examines\ntheir robustness using adversarial examples.\n3 FROM IMAGE TO TEXT\nAdversarial attacks are originated from computer vision community. In this section, we introduce\nrepresentative works, discuss differences between attacking image data and textual data, and\npresent preliminary knowledge when performing adversarial attacks on textual DNNs.\n3.1 Crafting Adversarial Examples: Inspiring Works in Computer Vision\nSince adversarial examples are first proposed for attacking object recognition DNNs in computer\nvision community [20, 42, 96, 105, 106, 132, 156], this research direction has been receiving sustained\nattentions. We briefly introduce some works that inspired their followers in NLP community in\nthis section, allowing the reader to better understand the adversarial attacks on textual DNNs. For\ncomprehensive review of attack works in computer vision, please refer to [2].\nL-BFGS. Szegedy et al. invented the adversarial examples notation [132]. They proposed a explic-\nitly designed method to cause the model to give wrong prediction of adversarial input (x + η) for\nimage classification task. It came to solve the optimization problem:\nη = arg min\nη\nλ||η||2\n2 + J(x + η,y′)s.t. (x + η)∈[ 0, 1], (2)\nwhere y′is the target output of (x′+ η), but incorrect given an ideal classifier. J denotes the cost\nfunction of the DNN and λ is a hyperparameter to balance the two parts of the equation. This\nminimization was initially performed with a box-constrained Limited memory Broyden-Fletcher-\nGoldfarb-Shanno (L-BFGS) algorithm and thus was named after it. The optimization was repeated\nmultiple times until reaching a minimum λthat satisfy Eq. (2).\nFast Gradient Sign Method (FGSM) . L-BFGS is very effective, but highly expensive - this inspired\nGoodfellow et al. [42] to find a simplified solution. Instead of fixing y′and minimizing ηin L-BFGS,\nFGSM fixed size of η and maximized the cost (Eq. (3)). Then they linearized the problem with a\nfirst-order Taylor series approximation (Eq. (4)), and got the closed-form solution of η (Eq. (5))\n, Vol. 1, No. 1, Article . Publication date: April 2019.\nAdversarial Attacks on Deep Learning Models in Natural Language Processing: A Survey 9\n[143]:\nη = arg maxη J(x + η,y)s.t. ||η||∞≤ϵ, (3)\nη = arg maxη J(x,y)+ ηT∇x J(x,y)s.t. ||η||∞≤ϵ, (4)\nη = ϵ ·sign(∇x J(x, y)) (5)\nwhere ϵ is a parameter set by attacker, controlling the perturbation’s magnitude. sign(x) is the sign\nfunction which returns 1 when x > 0, and −1 when x < 0, otherwise returns 0. ∇x J(x, y)denotes\nthe gradient of loss function respect to the input, and can be calculated via back-propagation. FGSM\nattracts the most follow-up works in NLP.\nJacobian Saliency Map Adversary (JSMA). Unlike FGSM using gradients to attack, Papernot\net al. [106] generated adversarial examples using forward derivatives (i.e., model Jacobian). This\nmethod evaluates the neural model’s output sensitivity to each input component using its Jacobian\nMatrix and gives greater control to adversaries given the perturbations. Jacobian matrices form\nthe adversarial saliency maps that rank each input component’s contribution to the adversarial\ntarget. A perturbation is then selected from the maps. Thus the method was named Jacobian-based\nSaliency Map Attack . The Jacobian matrix of a given x is given by:\nJacbF [i, j]= ∂Fi\n∂xj\n(6)\nwhere xi is the i-th component of the input and Fj is the j-th component of the output. Here F\ndenotes the logits (i.e., the inputs to the softmax function) layer. JF [i, j]measures the sensitivity of\nFj with respect to xi .\nC&W Attack. Carlini and Wagner [20] aimed to evaluate the defensive distillation strategy [50]\nfor mitigating the adversarial attacks. They restricted the perturbations with lp norms where p\nequals to 0, 2 and ∞and proposed seven versions of J for the following optimization problem:\nη = arg min\nη\n||η||p + λJ(x + η,y′)s.t. (x + η)∈[ 0, 1], (7)\nand the formulation shares the same notation with aforementioned works.\nDeepFool. DeepFool [96] is an iterative L2-regularized algorithm. The authors first assumed the\nneural network is linear, thus they can separate the classes with a hyperplane. They simplified\nthe problem and found optimal solution based on this assumption and constructed adversarial\nexamples. To address the non-linearity fact of the neural network, they repeated the process until a\ntrue adversarial example is found.\nSubstitute Attack. The above mentioned representative works are all white-box methods, which\nrequire the full knowledge of the neural model’s parameters and structures. However, in practice,\nit is not always possible for attackers to craft adversaries in white-box manner due to the limited\naccess to the model. The limitation was addressed by Papernot et al. [105] and they introduced a\nblack-box attack strategy: They trained a substitute model to approximate the decision boundaries\nof the target model with the labels obtained by querying the target model. Then they conducted\nwhite-box attack on this substitute and generate adversarial examples on the substitute. Specifically,\nthey adopted FSGM and JSMA in generating adversarial examples for the substitute DNN.\nGAN-like Attack. There are another branch of black-box attack leverages the Generative Ad-\nversarial Neural (GAN) models. Zhao et al. [ 156] firstly trained a generative model, WGAN, on\nthe training dataset X. WGAN could generate data points that follows the same distribution with\nX. Then they separately trained an inverter to map data sample x to z in the latent dense space\n, Vol. 1, No. 1, Article . Publication date: April 2019.\n10 Zhang et al.\nby minimizing the reconstruction error. Instead of perturbing x, they searched for adversaries z∗\nin the neighbour of z in the latent space. Then they mapped z∗back to x∗and check if x∗would\nchange the prediction. They introduced two search algorithms: iterative stochastic search and\nhybrid shrinking search. The former one used expanding strategy that gradually expand the search\nspace, while the later one used shrinking strategy that starts from a wide range and recursively\ntightens the upper bound of the search range.\n3.2 Attacking Image DNNs vs Attacking Textual DNNs\nTo attack a textual DNN model, we cannot directly apply the approaches from the image DNN\nattackers as there are three main differences between them:\n•Discrete vs Continuous Inputs. Image inputs are continuous, typically the methods use Lp\nnorm measures the distance between clean data point with the perturbed data point. However,\ntextual data is symbolic, thus discrete. It is hard to define the perturbations on texts. Carefully\ndesigned variants or distance measurements for textual perturbations are required. Another\nchoice is to firstly map the textual data to continuous data, then adopt the attack method\nfrom computer vision. We will give further discussion in Section 3.3.\n•Perceivable vs Unperceivable. Small change of the image pixels usually can not be easily\nperceived by human beings, hence the adversarial examples will not change the human\njudgment, but only fool the DNN models. But small changes on texts, e.g., character or word\nchange, will easily be perceived, rendering the possibility of attack failure. For example,\nthe changes could be identified or corrected by spelling-check and grammar check before\ninputting into textual DNN models. Therefore, it is nontrivial to find unperceivalble textual\nadversaries.\n•Semantic vs Semantic-less. In the case of images, small changes usually do not change the\nsemantics of the image as they are trivial and unperceivable. However, perturbation on texts\nwould easily change the semantics of a word and a sentence, thus can be easily detected\nand heavily affect the model output. For example, deleting a negation word would change\nthe sentiment of a sentence. But this is not the case in computer vision where perturbing\nindividual pixels does not turn the image from a cat to another animal. Changing semantics of\nthe input is against the goal of adversarial attack that keep the correct prediction unchanged\nwhile fooling an victim DNN.\nDue to these differences, current state-of-the-art textual DNN attackers either carefully adjust\nthe methods from image DNN attackers by enforcing additional constraints, or propose novel\nmethods using different techniques.\n3.3 Vectorizing Textual Inputs and Perturbation Measurements\nVectorizing Textual Input. DNN models require vectors as input, for image tasks, the normal\nway is to use the pixel value to form the vectors/matrices as DNN input. But for textual models,\nspecial operations are needed to transform the text into vectors. There are three main branches of\nmethods: word-count based encoding, one-hot encoding and dense encoding (or feature embedding)\nand the later two are mostly used in DNN models of textual applications.\n•Word-Count Based Encoding. Bag-of-words (BOW) method has the longest history in vec-\ntorizing text. In BOW model, an zero-encoded vector with length of the vocabulary size is\ninitialized. Then the dimension in vector is replaced by the count of corresponding word’s\nappearance in the given sentence. Another word-count based encoding is to utilize the term\nfrequency-inverse document frequency (TF-IDF) of a word (term), and the dimension in the\nvector is the TF-IDF value of the word.\n, Vol. 1, No. 1, Article . Publication date: April 2019.\nAdversarial Attacks on Deep Learning Models in Natural Language Processing: A Survey 11\n•One-hot Encoding. In one-hot encoding, a vector feature represents a token–a token could be\na character (character-level model) or a word (word-level model). For character-level one-hot\nencoding, the representation can be formulated as [31]:\nx = [(x11, ...x1n); ...(xm1, ...xmn )] (8)\nwhere x be a text of L characters, xij ∈{0, 1}|A|and |A|is the alphabet (in some works, |A|\nalso include symbols). In Equation 8, m is the number of words, n is the maximum number\nof characters for a word in sequence x. Thus each word has the same-fixed length of vector\nrepresentation and the length is decided by the maximum number of characters of the words.\nFor word-level one-hot encoding, following the above notations, the textx can be represented\nas:\nx = [(x1, ...,xm, xm+1...xk )] (9)\nwhere xij ∈{0, 1}|V |and |V |is the vocabulary, which contains all words in a corpus. k is\nthe maximum number of words allowed for a text, so that [(xm+1...xk )]is zero-paddings if\nm + 1 < k. One-hot encoding produces vectors with only 0 and 1 values, where 1 indicates\nthe corresponding character/word appears in the sentence/paragraph, while 0 indicate it does\nnot appear. Thus one-hot encoding usually generates sparse vectors/matrices. DNNs have\nproven to be very successful in learning values from the sparse representations as they can\nlearn more dense distributed representations from the one-hot vectors during the training\nprocedure.\n•Dense Encoding. Comparing to one-hot encoding, dense encoding generates low dimensional\nand distributed representations for textual data. Word2Vec citenips/MikolovSCCD13 uses\ncontinuous bag-of-words (CBOW) and skip-gram models to generate dense representation for\nwords, i.e., word embeddings. It is based on the distributional assumption that words appearing\nwithin similar context possess similar meaning. Word embeddings, to some extend, alleviates\nthe discreteness and data-sparsity problems for vectorizing textual data [37]. Extensions of\nword embeddings such as doc2vec and paragraph2vec [70] encode sentences/paragraphs to\ndense vectors.\nPerturbation Measurement. As described in Section 2.1.3, there needs a way to measure the\nsize of the perturbation, so that it can be controlled to ensure the ability of fooling the victim DNN\nwhile remain unperceivable. However, the measurement in textual perturbations is drastically\ndifferent with the perturbations in image. Usually, the size of the perturbation is measured by the\ndistance between clean datax and its adversarial examplex′. But in texts, the distance measurement\nalso need to consider the grammar correctness, syntax correctness and semantic-preservance. We\nhere list the measurements used in the reviewed in this survey.\n•Norm-based measurement. Directly adopting norms such as Lp,p ∈0, 1, 2, ∞requires the\ninput data are continuous. One solution is to use continuous and dense presentation (e.g.,\nembedding) to represent the texts. But this usually results in invalid and incomprehensible\ntexts, that need to involve other constrains.\n•Grammar and syntax related measurement. Ensuring the grammar or syntactic correctness\nmakes the adversarial examples not easily perceived.\n– Grammar and syntax checker are used in some works to ensure the textual adversarial\nexamples generated are valid.\n– Perplexity is usually used to measure the quality of a language model. In one reviewed\nliterature [92], the authors used perplexity to ensure the generated adversarial examples\n(sentences) are valid.\n, Vol. 1, No. 1, Article . Publication date: April 2019.\n12 Zhang et al.\n– Paraphrase is controlled and can be regarded as a type of adversarial example (4.3.3). When\nperturbing, the validity of paraphrases is ensured in the generation process.\n•Semantic-preserving measurement. Measuring semantic similarity/distance is often performed\non word vectors by adopting vectors’ similarity/distance measurements. Given twon-dimensional\nword vectors p = (p1,p2, ...,pn)and q = (q1, q2, ...,qn):\n– Euclidean Distance is a distance of two vectors in the Euclidean space:\nd(p, q)=\np\n(p1 −q1)2 + p2 −q2)2 + ..(pn −qn)2 (10)\n– Cosine Similarity computes cosine value of the angle between the two vectors:\ncos(p, q)=\nÍn\ni=1 pi ×qi\npÍn\ni=1(pi )2 ×\npÍn\ni=1(qi )2\n(11)\n•Edit-based measurement. Edit distance is a way of quantifying the minimum changes from one\nstring to the other. Different definitions of edit distance use different sets of string operations\n[74].\n– Levenshtein Distance uses insertion, removal and substitution operations.\n– Word Mover’s Distance (WMD) [69] is an edit distance operated on word embedding. It\nmeasures the minimum amount of distance that the embedded words of one document\nneed to travel to reach the embedded words of the other document [39]. The minimization\nis formulated as:\nmin Ín\ni, j=1 Tij ||ei −ej||2 (12)\ns.t., Ín\nj=1 Tij = di , ∀i ∈{i, ...,n}, Ín\ni=1 Tij = d′\ni , ∀j ∈{i, ...,n}\nwhere ei and ej ared word embedding of word i and word j respectively. n is the number of\nwords. T ∈Rn×n be a flow matrix, where Tij ≤0 denotes how much of word i in d travels\nto word j in d′. d and d′are normalized bag-of-words vectors of the source document and\ntarget document respectively.\n– Number of changes is a simple way to measure the edits and it is adopted in some reviewed\nliterature.\n•Jaccard similarity coefficient is used for measuring similarity of finite sample sets utilising\nintersection and union of the sets.\nJ(A, B)= |A ∩B|\n|A ∪B| (13)\nIn texts, A, B are two documents (or sentences). |A ∩B|denotes the number of words appear\nin both documents, |A ∪B|refers to the number of unique words in total.\n4 ATTACKING NEURAL MODELS IN NLP: STATE-OF-THE-ART\nIn this section, we first introduce the categories of attack methods on textual deep learning models\nand then highlight the state-of-the-art research works, aiming to identify the most promising\nadvances in recent years.\n4.1 Categories of Attack Methods on Textual Deep Learning Models\nWe categorize existing adversarial attack methods based on different criteria. Figure 1 generalizes\nthe categories.\nIn this article, five strategies are used to categorize the attack methods: i) By model access group\nrefers to the knowledge of attacked model when the attack is performed. In the following section,\nwe focus on the discussion using this categorization strategy. ii) By application group refers the\n, Vol. 1, No. 1, Article . Publication date: April 2019.\nAdversarial Attacks on Deep Learning Models in Natural Language Processing: A Survey 13\nBlack-box Attack\nWhite-box Attack\nCharacter-level\nWord-level\nSentence-level\nBy Applications\nClassification\nMachine Translation \nMachine Comprehension\nText Summarisation\nText Entailment\nPOS Tagging\nRelation Extraction\nDialogue System\nTargeted\nUntargeted\nBy Model Access\nBy Granularity\nBy Target\nCategories of \nAdversarial Attacks \non Textual DNNs\nBy Attacked DNNs\nSingle Modal\nCross Modal Optical Character Recognition \nScene Text Recognition\nImage Captioning\nVisual Question Answering \nVisual-Semantic Embedding \nSpeech Recognition\nFeed Forward\nReinforcement Learning Models\nGenerative Adversarial Networks\nHybrids Models\nAuto-Encoder CNNs RNNs\nFig. 1. Categories of Adversarial Attack Methods on Textual Deep Learning Models\nmethods via different NLP applications. More detailed discussion will be provided in Section 4.5.\niii) By target group refers to the goal of the attack is enforcing incorrect prediction or targeting\nspecific results. iv) By granularity group considers on what granularity the model is attacked. v)\nWe have discussed the attacked DNNs in Section 2.2. In following sections, we will continuously\nprovide information about different categories that the methods belong to.\nOne important group of methods need to be noted is the cross-modal attacks, in which the\nattacked model consider the tasks dealing with multi-modal data, e.g., image and text data. They\nare not attacks for pure textual DNNs, hence we discuss this category of methods separately in\nSection 4.4 in addition to white-box attacks in Section 4.2 and black-box attacks in Section 4.3.\n4.2 White-Box Attack\nIn white-box attack, the attack requires the access to the model’s full information, including\narchitecture, parameters, loss functions, activation functions, input and output data. White-box\nattacks typically approximate the worst-case attack for a particular model and input, incorporating\na set of perturbations. This adversary strategy is often very effective. In this section, we group\nwhite-box attacks on textual DNNs into seven categories.\n4.2.1 FGSM-based. FGSM is one of the first attack methods on images (Section 3.1). It gains many\nfollow-up works in attacking textual DNNs. TextFool [78] uses the concept of FGSM to approximate\nthe contribution of text items that possess significant contribution to the text classification task.\nInstead of using sign of the cost gradient in FGSM, this work considers the magnitude. The authors\nproposed three attacks:insertion, modification and removal. Specifically, they computed cost gradient\n∆x J(f , x, c′)of each training samplex, employing back propagation, where f is the model function,\nx is the original data sample, and c′is the target text class. Then they identified the characters\nthat contain the dimensions with the highest gradient magnitude and named them hot characters .\nPhrases that contain enough hot characters and occur the most frequently are chosen as Hot\nTraining Phrases (HTPs). In the insertion strategy, adversarial examples are crafted by inserting\na few HTPs of the target class c′nearby the phrases with significant contribution to the original\nclass c. The authors further leveraged external sources like Wikipedia and forged fact to select\nthe valid and believable sentences. In the modification Strategy, the authors identified Hot Sample\n, Vol. 1, No. 1, Article . Publication date: April 2019.\n14 Zhang et al.\nPhrase (HSP) to the current classification using similar way of identifying HTPs. Then they replaced\nthe characters in HTPs by common misspellings or characters visually similar. In the removal\nstrategy, the inessential adjective or adverb in HSPs are removed. The three strategies and their\ncombinations are evaluated on a CNN text classifier [155]. However, these methods are performed\nmanually, as mentioned by the authors.\nThe work in [ 117] adopted the same idea as TextFool, but it provides a removal-addition-\nreplacement strategy that firstly tries to remove the adverb (wi ) which contributed the most to the\ntext classification task (measured using loss gradient). If the output sentences in this step have\nincorrect grammar, the method will insert a word pj before wi . pj is selected from a candidate pool,\nin which the synonyms and typos and genre specific keywords (identified via term frequency)\nare candidate words. If the output cannot satisfy the highest cost gradient for all the pj , then the\nmethod replaces wi with pj . The authors showed that their method is more effective than TextFool.\nAs the method ordered the words with their contribution ranking and crafted adversarial samples\naccording to the order, it is a greedy method that always get the minimum manipulation until\nthe output changes. To avoid being detected by the human eyes, the authors constrained the\nreplaced/added words to not affect the grammar and POS of the original words.\nIn malware detection, an portable executable (PE) is represented by binary vector {x1, ...,xm },\nxi ∈{0, 1}that using 1 and 0 to indicate the PE is present or not where m is the number of PEs.\nUsing PEs’ vectors as features, malware detection DNNs can identify the malicious software. It\nis not a typical textual application, but also targets discrete data, which share similar methods\nwith textual applications. The authors of [3] investigated the methods to generate binary-encoded\nadversarial examples. To preserve the functionality of the adversarial examples, they incorporated\nfour bounding methods to craft perturbations. The first two methods adopt FSGMk [68], the multi-\nstep variant of FGSM, restricting the perturbations in a binary domain by introducing deterministic\nrounding (dFGSMk ) and randomized rounding (rFGSMk ). These two bounding methods are similar\nto L∞-ball constraints on images [42]. The third method multi-step Bit Gradient Ascent (BGAk ) sets\nthe bit of the j-th feature if the corresponding partial derivative of the loss is greater than or equal\nto the loss gradient’s L2-norm divided by √m. The fourth method multi-step Bit Coordinate Ascent\n(BCAk ) updates one bit in each step by considering the feature with the maximum corresponding\npartial derivative of the loss. These two last methods actually visit multiple feasible vertices. The\nwork also proposed a adversarial learning framework aims to robustify the malware detection\nmodel.\n[114] also attacks malware detection DNNs. The authors made perturbations on the embedding\npresentation of the binary sequences and reconstructed the perturbed examples to its binary\nrepresentation. Particularly, they appended a uniformly random sequence of bytes (payload) to\nthe original binary sequence. Then they embed the new binary to its embedding and performed\nFGSM only on the embedding of the payload. The perturbation is performed iteratively until the\ndetector output incorrect prediction. Since the perturbation is only performed on payload, instead\nof the input, this method will preserve the functionality of the malware. Finally, they reconstructed\nadverse embedding to valid binary file by mapping the adversary embedding to its closest neighbour\nin the valid embedding space.\nMany works directly adopt FGSM for adversarial training, i.e., put it as regularizer when training\nthe model. We will discuss some representatives in Section 5.\n4.2.2 JSMA-based. JSMA is another pioneer work on attacking neural models for image appli-\ncations (refers to Section 3.1). The work [104] used forward derivative as JSMA to find the most\ncontributable sequence towards the adversary direction. The network’s Jacobian had been calcu-\nlated by leveraging computational graph unfolding [97]. They crafted adversarial sequences for\n, Vol. 1, No. 1, Article . Publication date: April 2019.\nAdversarial Attacks on Deep Learning Models in Natural Language Processing: A Survey 15\ntwo types of RNN models whose output is categorical and sequential data respectively. For categor-\nical RNN, the adversarial examples are generated by considering the Jacobian JacbF [:, j]column\ncorresponding to one of the output components j. Specifically, for each word i, they identified the\ndirection of perturbation by:\nsign(JacbF (x′)[i,д(x′)]) (14)\nд(x′)= arg max0, 1(pj ) (15)\nwhere pj is the output probability of the target class. As in JSMA, they instead to choose logit to\nreplace probability in this equation. They further projected the perturbed examples onto the closest\nvector in the embedding space to get valid embedding. For sequential RNN, after computing the\nJacobian matrix, they altered the subset of input setps {i}with high Jacobian values JacbF [i, j]and\nlow Jacobian values JacbF [i, k]for k , j to achieve modification on a subset of output steps {j}.\n[45] (and [46]) is the first work to attack neural malware detector. They firstly performed feature\nengineering and obtained more than 545K static features for software applications. They used\nbinary indicator feature vector to represent an application. Then they crafted adversarial examples\non the input feature vectors by adopting JSMA: they computed gradient of model Jacobian to\nestimate the perturbation direction. Later, the method chooses a perturbation ηgiven input sample\nthat with maximal positive gradient into the target class. In particular, the perturbations are chosen\nvia index i, satisfying:\ni = arg maxj ∈[1,m], Xj =y′ f ′\ny(Xj ) (16)\nwhere y′ is the target class, m is the number of features. On the binary feature vectors, the\nperturbations are (0 →1) or (1 →0). This method preserves the functionality of the applications.\nIn order to ensure that modifications caused by the perturbations do not change the application\nmuch, which will keep the malware application’s functionality complete, the authors used the L1\nnorm to bound the overall number of features modified, and further bound the number of features\nto 20. In addition, the authors provided three methods to defense against the attacks, namely feature\nreduction, distillation and adversarial training. They found adversarial training is the most effective\ndefense method.\n4.2.3 C&W-based. The work in [130] adopted C&W method (refers to Section 3.1) for attacking\npredictive models of medical records. The aim is to detect susceptible events and measurements in\neach patient’s medical records, which provide guidance for the clinical usage. The authors used\nstandard LSTM as predictive model. Given the patient EHR data being presented by a matrix\nXi ∈Rd×ti (d is the number of medical features and ti is the time index of medical check), the\ngeneration of the adversarial example is formulated as:\nmin\nˆX\nmax{−ϵ, [loдit (x′)]y −[loдit (x)]y′}+ λ||x′−x||1 (17)\nwhere loдit (·)denotes the logit layer output,λis the regularization parameter which controls theL1\nnorm regularization,y′is the targeted label whileyis the original label. After generating adversarial\nexamples, the authors picked the optimal example according to their proposed evaluation scheme\nthat considers both the perturbation magnitude and the structure of the attacks. Finally they used\nthe adversarial example to compute the susceptibility score for the EHR as well as the cumulative\nsusceptibility score for different measurements.\nSeq2Sick [24] attacked the seq2seq models using two targeted attacks: non-overlapping attack and\nkeywords attack. For non-overlapping attack, the authors aimed to generate adversarial sequences\nthat are entirely different from the original outputs. They proposed a hinge-like loss function that\n, Vol. 1, No. 1, Article . Publication date: April 2019.\n16 Zhang et al.\noptimizes on the logit layer of the neural network:\n|K |Õ\ni=1\nmin\nt ∈[M]\n{mt (max{−ϵ, max\ny,ki\n{z(y)\nt }−z(ki )\nt })} (18)\nwhere {st }are the original output sequence, {zt }indicates the logit layer outputs of the adversarial\nexample. For the keyword attack, targeted keywords are expected to appear in the output sequence.\nThe authors also put the optimization on the logit layer and tried to ensure that the targeted\nkeyword’s logit be the largest among all words. Further more, they defined mask function m to\nsolve the keyword collision problem. The loss function then becomes:\nLke ywords =\n|K |Õ\ni=1\nmin\nt ∈[M]\n{mt (max{−ϵ, max\ny,ki\n{z(y)\nt }−z(ki )\nt })} (19)\nwhere ki denotes the i-th word in output vocabulary. To ensure the generated word embedding is\nvalid, this work also considers two regularization methods: group lasso regularization to enforce\nthe group sparsity, and group gradient regularization to make adversaries are in the permissible\nregion of the embedding space.\n4.2.4 Direction-based. HotFlip [31] performs atomic flip operations to generate adversarial ex-\namples. Instead of leveraging gradient of loss, HotFlip use the directional derivatives. Specifically,\nHotFlip represents character-level operations, i.e., swap, insert and delete, as vectors in the input\nspace and estimated the change in loss by directional derivatives with respect to these vectors.\nSpecifically, given one-hot representation of inputs, a character flip in the j-th character of the i-th\nword (a→b) can be represented by the vector:\n− →vijb = (0, ..; (0, ..(0, ..−1, 0, ..,1, 0)j , ..0)i ; 0, ..) (20)\nwhere -1 and 1 are in the corresponding positions for the a-th and b-th characters of the alphabet,\nrespectively. Then the best character swap can be found by maximizing a first-order approximation\nof loss change via directional derivative along the operation vector:\nmax ∇x J(x,y)T ·− →vijb = max\nij v\n∂J(b)\n∂xij\n−∂J(a)\n∂xij\n(21)\nwhere J(x,y)is the model’s loss function with input x and true output y. Similarly, insertion at\nthe j-th position of the i-th word can also be treated as a character flip, followed by more flips as\ncharacters are shifted to the right until the end of the word. The character deletion is a number of\ncharacter flips as characters are shifted to the left. Using the beam search, HotFlip efficiently finds\nthe best directions for multiple flips.\nThe work [30] extended HotFlip by adding targeted attacks. Besides the swap, insertion and\ndeletion as provided in HotFlip, the authors proposed a controlled attack, which is to remove\na specific word from the output, and a targeted attack, which is to replace a specific word by\na chosen one. To achieve these attacks, they maximized the loss function J(x,yt )and minimize\nJ(x,y′\nt ), where t is the target word for the controlled attack, and t′is the word to replace t. Further,\nthey proposed three types of attacks that provide multiple modifications. In one-hot attack, they\nmanipulated all the words in the text with the best operation. In Greedy attack, they make another\nforward and backward pass, in addition to picking the best operation from the whole text. In Beam\nsearch attack, they replaced the search method in greedy with the beam search. In all the attacks\nproposed in this work, the authors set threshold for the maximum number of changes, e.g., 20% of\ncharacters are allowed to be changed.\n, Vol. 1, No. 1, Article . Publication date: April 2019.\nAdversarial Attacks on Deep Learning Models in Natural Language Processing: A Survey 17\nStrategy Work Granularity Target Attacked Models Perturb Ctrl. App.\nFSGM-based\n[78] character,word Y CNN [155] L∞ TC\n[117] word N CNN [155] L∞, Grammar and POS\ncorrectness\nTC\n[114] PE binary CNN in [27] Boundaries employ L∞\nand L2\nMAD\n[3] PE embedding binary MalConv [110] L∞ MAD\nJSMA-based [104] word embedding binary LSTM – TC\n[45, 46] application\nfeatures\nbinary Feed forward L1 MAD\nC&W-based [130] medical features Y LSTM L1 MSP\n[24] word embedding Y OpenNMT-py [64] L2+gradient regulariza-\ntion\nTS, MT\nDirection-based [31] character N CharCNN-LSTM\n[61]\n– TC\n[30] character Y CharCNN-LSTM\n[26]\nNumber of changes MT\nAttention-based [16] word, sentence N [29, 82, 140], CNN,\nLSTM and ensem-\nbles\nNumber of changes MRC,\nQA\nReprogramming [98] word N CNN, LSTM,\nBi-LSTM\n– TC\nHybrid [39] word embedding N CNN WMD TC, SA\nTable 1. Summary of reviewed white-box attack methods. PE: portable executable; TC: text classification;\nSA: sentiment analysis; TS: text summarisation; MT: machine translation MAD: malware detection; MSP:\nMedical Status Prediction; MRC: machine reading comprehension; QA: question answering; WMD: Word\nMover’s Distance; –: not available.\n4.2.5 Attention-based. [16] proposed two white-box attacks for the purpose of comparing the\nrobustness of CNN verses RNN. They leveraged the model’s internal attention distribution to find\nthe pivotal sentence which is assigned a larger weight by the model to derive the correct answer.\nThen they exchanged the words which received the most attention with the randomly chosen\nwords in a known vocabulary. They also performed another white-box attack by removing the\nwhole sentence that gets the highest attention. Although they focused on attention-based models,\ntheir attacks do not examine the attention mechanism itself, but solely leverages the outputs of the\nattention component (i.e., attention score).\n4.2.6 Reprogramming. [98] adopts adversarial reprogramming (AP) to attack sequence neural\nclassifiers. AP [32] is a recently proposed adversarial attack where a adversarial reprogramming\nfunction дθ is trained to re-purpose the attacked DNN to perform a alternate task (e.g., question\nclassification to name classification) without modifying the DNN’s parameters. AP adopts idea\nfrom transfer learning, but keeps the parameters unchanged. The authors in [98] proposed both\nwhite-box and black-box attacks. In white-box, Gumbel-Softmax is applied to train дθ who can\nwork on discrete data. We discuss the black-box method later. They evaluated their methods on\nvarious text classification tasks and confirmed the effectiveness of their methods.\n4.2.7 Hybrid. Authors of the work [39] perturbed the input text on word embedding against the\nCNN model. This is a general method that is applicable to most of the attack methods developed\nfor computer vision DNNs. The authors specifically applied FGSM and DeepFool. Directly applying\nmethods from computer vision would generate meaningless adversarial examples. To address this\nissue, the authors rounded the adversarial examples to the nearest meaningful word vectors by\n, Vol. 1, No. 1, Article . Publication date: April 2019.\n18 Zhang et al.\nusing Word Mover’s Distance (WMD) as the distance measurements. The evaluations on sentiment\nanalysis and text classification datasets show that WMD is a qualified metric for controlling the\nperturbations.\nSummary of White-box Attack. We summarize the reviewed white-box attack works in Table\n1. We highlight four aspects include granularity-on which level the attack is performed; target-\nwhether the method is target or un-target; the attacked model, perturbation control-methods to\ncontrol the size of the perturbation, and applications. It is worth noting that in binary classifications,\ntarget and untarget methods show same effect, so we point out their target as ”binary\" in the table.\n4.3 Black-box Attack\nBlack-box attack does not require the details of the neural networks, but can access the input and\noutput. This type of attacks often rely on heuristics to generate adversarial examples, and it is more\npractical as in many real-world applications the details of the DNN is a black box to the attacker. In\nthis article, we group black-box attacks on textual DNNs into five categories.\n4.3.1 Concatenation Adversaries. [55] is the first work to attack reading comprehension systems.\nThe authors proposed concatenation adversaries, which is to append distracting but meaningless\nsentences at the end of the paragraph. These distracting sentences do not change the semantics of\nthe paragraph and the question answers, but will fool the neural model. The distracting sentences\nare either carefully-generated informative sentences or arbitrary sequence of words using a pool of\nFig. 2. Concatenation adversarial attack on reading comprehension DNN. After adding distracting sentences\n(in blue) the answer changes from correct one (green) to incorrect one (red) [55].\nDNN\nCorrect \nOutput\nDistorted \nOutput\nIncorrect \nOutput\nParagraph \nDistracting \ncontents\nFig. 3. General principle of concatenation adversaries. Correct output are often utilized to generate distorted\noutput, which later will be used to build distracting contents. Appending distracting contents to the original\nparagraph as adversarial input to the attacked DNN and cause the attacked DNN produce incorrect output.\n, Vol. 1, No. 1, Article . Publication date: April 2019.\nAdversarial Attacks on Deep Learning Models in Natural Language Processing: A Survey 19\nFig. 4. Edit adversarial attack on sentiment analysis DNN. After editing words (red), the prediction changes\nfrom 100% of Negative to 89% of Positive [74].\n20 random common words. Both perturbations were obtained by iteratively querying the neural\nnetwork until the output changes. Figure 2 illustrates an example from [ 55] that after adding\ndistracting sentences (in blue) the answer changes from correct one (green) to incorrect one (red).\nThe authors of [142] improved the work by varying the locations where the distracting sentences\nare placed and expanding the set of fake answers for generating the distracting sentences, rendering\nnew adversarial examples that can help training more robust neural models. Also, the work [16]\nutilized the distracting sentences to evaluate the robustness of their reading comprehension model.\nSpecifically, they use a pool of ten random common words in conjunction with all question words\nand the words from all incorrect answer candidates to generate the distracting sentences. In this\nwork, a simple word-level black-box attack is also performed by replacing the most frequent words\nvia their synonyms. As aforementioned, the authors also provided two white-box strategies. Figure\n3 illustrates the general workflow for concatenation attack. Correct output (i.e., answer in MRC\ntasks) are often leveraged to generate distorted output, which later will be used to build distracting\ncontents. Appending distracting contents to the original paragraph as adversarial input to the\nattacked DNN. The distracting contents will not distract human being and ideal DNNs, but can\nmake vulunerable DNNs to produce incorrect output.\n4.3.2 Edit Adversaries. The work in [13] perturbed the input data of neural machine translation\napplications in two ways: Synthetic, which performed the character order changes, such as swap,\nmiddle random (i.e., randomly change orders of characters except the first and the last), fully random\n(i.e., randomly change orders of all characters) and keyboard type. They also collected typos and\nmisspellings as adversaries. natural, leveraged the typos from the datasets. Furthermore, [ 99]\nattacked the neural models for dialogue generation. They applied various perturbations in dialogue\ncontext, namely Random Swap (randomly transposing neighboring tokens) and Stopword Dropout\n(randomly removing stopwords), Paraphrasing (replacing words with their paraphrases), Grammar\nErrors (e.g., changing a verb to the wrong tense) for the Should-Not-Change attacks, and the Add\nDNN\nSentence\nWord\nCharacter\nText Correct \nOutput\nIncorrect \nOutput\nPerturbation: \nReplace, Delete, Add, Swap…\nStrategies to identify \nwhich to perturb\nAdversarial Text\nFig. 5. General principle of edit adversaries. Perturbations are performed on sentences, words or characters\nby edit strategies such as replace, delete, add and swap.\n, Vol. 1, No. 1, Article . Publication date: April 2019.\n20 Zhang et al.\nNegation strategy (negates the root verb of the source input) and Antonym strategy (changes verbs,\nadjectives, or adverbs to their antonyms) for Should-Change attacks. DeepWordBug [35] is a simple\nmethod that uses character transformations to generate adversarial examples. The authors first\nidentified the important ‘tokens’, i.e., words or characters that affect the model prediction the most\nby scoring functions developed by measuring the DNN classifier’s output. Then they modified the\nidentified tokens using four strategies: replace, delete, add and swap. The authors evaluated their\nmethod on a variety of NLP tasks, e.g., text classification, sentiment analysis and spam detection.\n[74] followed [35], refining the scoring function. Also this work provided white-box attack adopting\nJSMA. One contribution of this work lies on the perturbations are restricted using four textual\nsimilarity measurement: edit distance of text; Jaccard similarity coefficient; Euclidean distance on\nword vector; and cosine similarity on word embedding. Their method had been evaluated only on\nsentiment analysis task.\nThe authors in [92] proposed a method for automatically generating adversarial examples that\nviolate a set of given First-Order Logic constraints in natural language inference (NLI). They\nproposed an inconsistency loss to measure the degree to which a set of sentences causes a model to\nviolate a rule. The adversarial example generation is the process for finding the mapping between\nvariables in rules to sentences that maximize the inconsistency loss and are composed by sentences\nwith a low perplexity (defined by a language model). To generate low-perplexity adversarial sentence\nexamples, they used three edit perturbations: i) change one word in one of the input sentences; i)\nremove one parse subtree from one of the input sentences; iii) insert one parse sub-tree from one\nsentence in the corpus in the parse tree of the another sentence.\nThe work in [5] uses genetic algorithm (GA) for minimising the number of word replacement\nfrom the original text, but at the same time can change the result of the attacked model. They\nadopted crossover and mutation operations in GA to generate perturbations. The authors measured\nthe effectiveness of the word replacement accoding to the impact on attacked DNNs. Their attack\nfocused on sentiment analysis and textual entailment DNNs.\nIn [21], the authors proposed a framerwork for adversarial attack on Differentiable Neural\nComputer (DNC). DNC is a computing machine with DNN as its central controller operating on an\nexternal memory module for data processing. Their method uses two new automated and scalable\nstrategies to generate grammatically corret adversairal attacks in question answering domian,\nutilising metamorphic transformation. The first strategy, Pick-n-Plug, consists of a pick operator\npick to draw adversarial sentences from a particular task (source task) and plug operator plug to\ninject these sentences into a story from another task (target task), without changing its correct\nanswers. Another strategy, Pick-Permute-Plug, extends the adversarial capability of PPick-n-Plug\nby an additional permute operator after picking sentences (gpick) from a source task. Words in a\nparticular adversarial sentence can be permuted with its synonyms to generate a wider range of\npossible attacks.\n4.3.3 Paraphrase-based Adversaries. SCPNs [54] produces a paraphrase of the given sentence with\ndesired syntax by inputting the sentence and a targeted syntactic form into an encoder-decoder\narchitecture. Specifically, the method first encodes the original sentence, then inputs the paraphrases\ngenerated by back-translation and the targeted syntactic tree into the decoder, whose output is\nthe targeted paraphrase of the original sentence. One major contribution lies on the selection and\nprocessing of the parse templates. The authors trained a parse generator separately from SCPNs\nand selected 20 most frequent templates in PARANMT-50M. After generating paraphrases using the\nselected parse templates, they further pruned non-sensible sentences by checking n-gram overlap\nand paraphrastic similarity. The attacked classifier can correctly predict the label of the original\nsentence but fails on its paraphrase, which is regarded as the adversarial example. SCPNs had been\n, Vol. 1, No. 1, Article . Publication date: April 2019.\nAdversarial Attacks on Deep Learning Models in Natural Language Processing: A Survey 21\nSentence\nText\nDNN\nCorrect \nOutput\nIncorrect \nOutput\nParaphrase\nText\nControlled Rules\nFig. 6. General principle of paraphrase-based adversaries. Carefully designed (controlled) paraphrases are\nregarded as adversarial examples, which fool DNN to produce incorrect output.\nevaluated on sentiment analysis and textual entailment DNNs and showed significant impact on\nthe attacked models. Although this method use target strategy to generate adversarial examples, it\ndoes not specify targeted output. Therefore, we group it to untarget attack. Furthermore, the work\nin [127] used the idea of paraphrase generation techniques that create semantically equivalent\nadversaries (SEA). They generated paraphrases of a input sentence x, and got predictions from f\nuntil the original prediction is changed with considering the semantically equivalent to x′that is\n1 if x is semantically equivalent to x′and 0 otherwise as shown in Eq.(22). After that, this work\nproposes semantic-equivalent rule based method for generalizing these generated adversaries into\nsemantically equivalent rules in order to understand and fix the most impactful bug.\nSEA(x, x′)= 1[SemEq(x, x′)∧ f (x), f (x′)] (22)\n4.3.4 GAN-based Adversaries. Some works proposed to leverage Generative Adversarial Network\n(GAN) [41] to generate adversaries [156]. The purpose of adopting GAN is to make the adversarial\nexamples mroe natural. In [156], the model proposed to generate adversarial exsamples consists\nof two key components: a GAN, which generate fake data samples, and an inverter that maps\ninput x to its latent representation z′). The two components are trained on the original input by\nminimizing reconstruction error between original input and the adversarial examples. Perturbation\nis performed in the latent dense space by identifying the perturbed sample ˆz in the neighborhood\nof z′. Two search approaches, namely iterative stochastic search and hybrid shrinking search , are\nproposed to identify the proper ˆz. However, it requires querying the attacked model each time\nto find the ˆz that can make the model give incorrect prediction. Therefore, this method is quite\ntime-consuming. The work is applicable to both image and textual data as it intrinsically eliminates\nthe problem raised by the discrete attribute of textual data. The authors evaluated their method on\nthree applications namely: textual entailment, machine translation and image classification.\n4.3.5 Substitution. The work in [53] proposes a black-box framework that attacks RNN model for\nmalware detection. The framework consists of two models: one is a generative RNN, the other is a\nsubstitute RNN. The generative RNN aims to generate adversarial API sequence from the malware’s\nAPI sequence. It is based on the seq2seq model proposed in [131]. It particularly generates a small\npiece of API sequence and inserts the sequence after the input sequence. The substitute RNN,\nwhich is a bi-directional RNN with attention mechanism, is to mimic the behavior of the attacked\nRNN. Therefore, generating adversarial examples will not query the original attacked RNN, but its\nsubstitution. The substitute RNN is trained on both malware and benign sequences, as well as the\nGumbel-Softmax outputs of the generative RNN. Here, Gumbel-softmax is used to enable the joint\ntraining of the two RNN models, because the original output of the generative RNN is discrete.\nSpecifically, it enables the gradient to be back-propagated from generative RNN to substitute RNN.\nThis method performs attack on API, which is represented as a one-hot vector, i.e., given M APIs,\n, Vol. 1, No. 1, Article . Publication date: April 2019.\n22 Zhang et al.\nStrategy Work Granularity Target Attacked Models Perturb\nCtrl.\nApp.\nConcatenation [55] word N BiDAF, Match-LSTM – MRC\n[142] word, character N BiDAF+Self-Attn+ELMo\n[109]\n– MRC\n[16] word, sentence N [29, 82, 140], CNN, LSTM and\nensembles\nNumber of\nchanges\nMRC, QA\nEdit\n[13] character, word N Nematus [ 120], char2char\n[71], charCNN [61]\n– MT\n[99] word, phrase N VHRED [ 123]+attn, RL in\n[75], DynoNet [49]\n– DA\n[35] character, word N Word-level LSTM, Character-\nlevel CNN\n– SA, TC\n[74] character, word N Word-level LSTM, Character-\nlevel CNN\nEdDist, JSC,\nEuDistV, CSE\nSA\n[92] word, phrase N cBiLSTM, DAM, ESIM Perplexity NLI\n[5] word N LSTM EuDistV SA, TE\n[21] word, sentence N DNC – QA\nParaphrase-based [54] word N LSTM Syntax-ctrl\nparaphrase\nSA and\nTE\n[127] word N BiDAF, Visual7W [157], fast-\nText [43]\nSelf-defined\nsemantic-\nequivalency\nMRC, SA,\nVQA\nGAN-based [156] word N LSTM, TreeLSTM, Google\nTranslate (En-to-Ge)\nGAN-\nconstraints\nTE, MT\nSubstitution [53] API N LSTM, BiLSTM and variants – MD\nReprogramming [98] word N CNN, LSTM, Bi-LSTM – TC\nTable 2. Summary of reviewed black-box attack methods. MRC: machine reading comprehension; QA: question\nanswering; VQA: visual question answering; DA: dialogue generation; TC: text classification; MT: machine\ntranslation; SA: sentiment analysis; NLI: natural language inference; TE: textual entailment; MD: malware\ndetection. EdDist: edit distance of text, JSC: Jaccard similarity coeffcient, EuDistV: Euclidean distance on\nword vector, CSE: cosine similarity on word embedding. ’-’: not available.\nthe vector for the i-th API is an M-dimensional binary vector that the i-th dimension is 1 while\nother dimensions are 0s.\n4.3.6 Reprogramming . As aforementioned, [98] provides both white-box and black-box attacks. We\ndescribe black-box attack here. In black-box attack, the authors fomulated the sequence generation\nas a reinforcement learning problem, and the adversarial reprogramming function дθ is the policy\nnetwork. Then they applied REINFORCE-based optimisation to train дθ.\nSummary of Black-box Attack. We summarise the reviewed black-box attack works in Table 2.\nWe highlight four aspects include granularity-on which level the attack is performed; target-whether\nthe method is target or un-target; the attacked model, perturbation control, and applications.\n4.4 Multi-modal Attacks\nSome works attack DNNs that are dealing with cross-modal data. For example, the neural models\ncontain an internal component that performs image-to-text or speech-to-text conversion. Although\nthese attacks are not for pure textual data, we briefly introduce the representative ones for the\npurpose of a comprehensive review.\n, Vol. 1, No. 1, Article . Publication date: April 2019.\nAdversarial Attacks on Deep Learning Models in Natural Language Processing: A Survey 23\n4.4.1 Image-to-Text. Image-to-text models is a class of techniques that generate textual description\nfor an image based on the semantic content of the latter.\nOptical Character Recognition (OCR) . Recognizing characters from images is a problem named\nOptical Character Recognition (OCR). OCR is a multimodal learning task that takes an image as\ninput and output the recognized text. Authors in [129] proposed a white-box attack on OCR and\nfollow-up NLP applications. They firstly used the original text to render a clean image (conversion\nDNNs). Then they found words in the text that have antonyms in WordNet and satisfy edit distance\nthreshold. Only the antonyms that are valid and keep semantic inconsistencies will be kept. Later,\nthe method locates the lines in the clean image containing the aforementioned words, which can\nbe replaced by their selected antonyms. The method then transforms the target word to target\nsequence. Given the input/target images and sequences, the authors formed the generating of\nadversarial example is an optimisation problem:\nminω c ·JCTC f (x′, t′)+ ||x −x′||2\n2 (23)\nx′= (α ·tanh(ω)+ β)/2 (24)\nα = (xmax −xmin )/2, β = (xmax + xmin )/2\nJCTC (f (x, t))= −logp(t |x) (25)\nwhere f (x)is the neural system model, JCTC (·)is the Connectionist Temporal Classification (CTC)\nloss function, x is the input image, t is the ground truth sequence, x′is the adversarial example,\nt′is the target sequence, ω, α, β are parameters controlling adversarial examples to satisfy the\nbox-constraint of x′ ∈[xmin , xmax ]p , where p is the number of pixels ensuring valid x′. After\ngenerating adversarial examples, the method replaces the images of the corresponding lines in the\ntext image. The authors evaluated this method in three aspects: single word recognition, whole\ndocument recognition, and NLP applications which based on the recognised text (sentiment analysis\nand document categorisation specifically). They also addressed that the proposed method suffers\nfrom limitatios such as low transferability across data and models, and physical unrelalizability.\nScene Text Recognition (STR) . STR is also an image-to-text application. In STR, the entire image\nis mapped to word strings directly. In contrast, the recognition in OCR is a pipeline process: first\nsegments the words to characters, then performs the recognition on single characters. AdaptiveAt-\ntack [153] evaluated the possibility of performing adversarial attack for scene text recognition. The\nauthors proposed two attacks, namely basic attack and adaptive attack. Basic attack is similar to\nthe work in [129] and it also formulates the adversarial example generation as an optimisation\nproblem:\nminω JCTC f (x′, t′)+ λD(x, x′) (26)\nx′= tanh(ω) (27)\nwhere D(·)is Euclidean distance. The differences to [ 129] lie on the definition of x′(Eq. (24)\nvs Eq. (27)), and the distance measurement between x, x′(L2 norm vs Euclidean distance), and\nthe parameter λ, which balances the importance of being adversarial example and close to the\noriginal image. As searching for proper λis quite time-consuming, the authors proposed another\nmethod to adaptively find λ. They named this method Adaptive Attack, in which they defined\nthe likelihood of a sequential classification task following a Gaussian distribution and derived the\nadaptive optimization for sequential adversarial examples as:\nmin ||x−x′||2\n2\nλ2\n1\n+ JCT Cf (x′, t′)\nλ2\n2\n+ log λ2\n1 + T log λ2\n2 + 1\nλ2\n2\n(28)\n, Vol. 1, No. 1, Article . Publication date: April 2019.\n24 Zhang et al.\nwhere λ1 and λ2 are two parameters to balance perturbation and CTC loss,T is the number of valid\npaths given targeted sequential output. Adaptive Attack can be applied to generate adversarial\nexamples on both non-sequential and sequential classification problems. Here we only highlight the\nequation for sequential data. The authors evaluated their proposed methods on tasks that targeting\nthe text insertion, deletion and substitution in output. The results demonstrated that Adaptive\nAttack is much faster than basic attack.\nImage Captioning. Image captioning is another multimodal learning task that takes an image as\ninput and generates a textual caption describing its visual contents. Show-and-Fool [22] generates\nadversarial examples to attack the CNN-RNN based image captioning model. The CNN-RNN model\nattacked uses a CNN as encoder for image feature extraction and a RNN as decoder for caption\ngeneration. Show-and-Fool has two attack strategies: targeted caption (i.e., the generated caption\nmatches the target caption) and targeted keywords (i.e., the generated caption contains the targeted\nkeywords). In general, they formulated the two tasks using the following formulation:\nminω c ·J(x′)+ ||x′−x||2\n2 (29)\nx′= x + η\nx = tanh (y), x′= tanh (ω+ y)\nwhere c > 0 is a pre-specified regularization constant, ηis the perturbation, ω,y are parameters\ncontrolling x′∈[−1, 1]. The difference between these two strategies is the definition of the loss\nfunction J(·). For targeted caption strategy, provided the targeted caption asS = (S1, S2, ...St , ...SN ),\nwhere St refers to the index of the t-th word in the vocabulary and N is the length of the caption,\nthe loss is formulated as:\nJS, loдit (x′)= ÍN −1\nt=2 max{−ϵ, maxk,St {z(k)\nt }−z(St )\nt } (30)\nwhere St is the target word, z(St )\nt is the logit of the target word. In fact, this method mininises the\ndifference between the maximumn logit except St , and the logit of St . For the targeted keywords\nstrategy, given the targeted keywords K:= K1, ...,KM , the loss function is:\nJK, loдit (x′)= ÍM\nj=1 mint ∈[N ]{max{−ϵ, maxk,Kj {z(k)\nt }−z(Kj )\nt }} (31)\nThe authors performed extensive experiments on Show-and-Tell [137] and varied the parameters in\nthe attacking loss. They found that Show-and-Fool is not only effective on attacking Show-and-Tell,\nthe CNN-RNN based image captioning model, but is also highly transferable to another model\nShow-Attend-and-Tell [147].\nVisual Question Answering (VQA) . Given an image and a natural language question about the\nimage, VQA is to provide an accurate answer in natural language. The work in [ 148] proposed\na iterative optimisation method to attack two VQA models. The objective function proposed\nmaximises the probability of the target answer and unweights the preference of adversarial examples\nwith smaller distance to the original image when this distance is below a threshold. Specifically,\nthe objective contains three components. The first one is similar to Eq. (26), that replaces the loss\nfunction to the loss of the VQA model and using ||x −x′||2/\n√\nN as distance between x′and x.\nThe second component maximises the difference between the softmax output and the prediction\nwhen it is different with the target answer. The third component ensures the distance between\nx′and x is under a lower bound. The attacks are evaluated by checking whether better success\nrate is obtained over the previous attacks, and the confidence score of the model to predict the\ntarget answer. Based on the evaluations, the authors concluded that that attention, bounding box\nlocalization and compositional internal structures are vulnerable to adversarial attacks. This work\nalso attacked a image captioning neural model. We refer the original paper for further information.\n, Vol. 1, No. 1, Article . Publication date: April 2019.\nAdversarial Attacks on Deep Learning Models in Natural Language Processing: A Survey 25\nMulti-modal Application Work Target Access Attacked Models Perturb\nCtrl.\nImage-to-Text\nOptical Character Recognition [129] Y white-box Tesseract [135] L2, EdDist\nScene Text Recognition [153] Y white-box CRNN [124] L2\nImage Captioning [22] Y white-box Show-and-Tell\n[137]\nL2\nVisual Question Answering [148] Y white-box MCB [34], N2NMN\n[52]\nL2\nVisual-Semantic Embeddings [125] N black-box VSE++ [33] –\nSpeech-to-Text Speech Recognition [19] Y white-box DeepSpeech [48] L2\nTable 3. Summary of reviewed cross-modal attacks. EdDist: edit distance of text, -: not available.\nVisual-Semantic Embeddings (VSE) . The aim of VSE is to bridge natural language and the\nunderlying visual world. In VSE, the embedding spaces of both images and descriptive texts\n(captions) are jointly optimized and aligned. [125] attacked the latest VSE model by generating\nadversarial examples in the test set and evaluated the robustness of the VSE modesls. They performed\nthe attack on textual part by introducing three method: i) replace nouns in the image captions\nutilizing the hypernymy/hyponymy relations in WordNet; ii) change the numerals to different\nones and singularize or pluralize the corresponding nouns when necessary; iii) detect the relations\nand shuffle the non-interchangeable noun phrases or replace the prepositions. This method can be\nconsidered as a black-box edit adversary .\n4.4.2 Speech-to-Text. Speech-to-text is also known as speech recognition. The task is to recognize\nand translate the spoken language into text automatically. [19] attacked a state-of-the-art speech-to-\ntext transcription neural network (based on LSTM), named DeepSpeech. Given a natural waveform,\nthe authors constructed a audio perturbation that is almost inaudible but can be recognized by\nadding into the original waveform. The perturbation is constructed by adopting the idea from\nC&W method (refers to section 3.1 ), which measures the image distortion by the maximum\namount of changed pixels. Adapting this idea, they measured the audio distortion by calculating\nrelative loudness of an audio and proposed to use Connectionist Temporal Classification loss for\nthe optimization task. Then they solved this task with Adam optimizer [62].\n4.5 Benchmark Datasets by Applications\nIn recent years, neural networks gain success in different NLP domains and the popular applications\ninclude text classification, reading comprehension, machine translation, text summarization, ques-\ntion answering, dialogue generation, to name a few. In this section, we review the current works\non generating adversarial examples on the neural networks in the perspective of NLP applications.\nTable 4 summarizes the works we reviewed in this article according to their application domain. We\nfurther list the benchmark datasets used in these works in the table as auxiliary information– thus\nwe refer readers to the links/references we collect for the detailed descriptions of the datasets. Note\nthat the auxiliary datasets which help to generate adversarial examples are not included. Instead,\nwe only present the dataset used to evaluate the attacked neural networks.\nText Classification. Majority of the surveyed works attack the deep neural networks for text\nclassification, since these tasks can be framed as a classification problem. Sentiment analysis aims\nto classify the sentiment to several groups (e.g., in 3-group scheme: neural, positive and negative).\nGender identification, Grammatical error detection and malware detection can be framed as binary\nclassification problems. Relation extraction can be formulated as single or multi-classification\nproblem. Predict medical status is a multi-class problem that the classes are defined by medical\n, Vol. 1, No. 1, Article . Publication date: April 2019.\n26 Zhang et al.\nApplications Representative Works Benchmark Datasets\nClassification\nText Classification [31, 35, 39, 78, 98, 118] DBpedia, Reuters Newswires, AG’s\nnews, Sogou News, Yahoo! Answers,\nRCV1, Surname Classification Dataset\nSentiment Analysis [31, 35, 54, 98, 104, 117, 118,\n127]\nSST, IMDB Review, Yelp Review, Elec,\nRotten Tomatoes Review, Amazon Re-\nview, Arabic Tweets Sentiment\nSpam Detection [35] Enron Spam, Datasets from [155]\nGender Identification [117] Twitter Gender\nGrammar Error Detection [118] FCE-public\nMedical Status Prediction [130] Electronic Health Records (EHR)\nMalware Detection [4, 45, 46, 53, 114] DREBIN, Microsoft Kaggle\nRelation Extraction [11, 145] NYT Relation, UW Relation, ACE04,\nCoNLL04 EC, Dutch Real Estate Clas-\nsifieds, Adverse Drug Events\nMachine Translation [13, 24, 30, 156] TED Talks, WMT’16 Multimodal Trans-\nlation Task\nMachine Comprehension [16, 21, 55, 142] SQuAD, MovieQA Multiple Choice, Log-\nical QA\nText Summarization [24] DUC2003, DUC2004, Gigaword\nText Entailment [54, 57, 92, 156] SNLI, SciTail, MultiNLI, SICK\nPOS Tagging [151] WSJ portion of PTB, Treebanks in UD\nDialogue System [99] Ubuntu Dialogue, CoCoA,\nCross-model\nOptical Character Recognition [129] Hillary Clinton’s emails\nScene Text Recognition [153] Street View Text, ICDAR 2013, IIIT5K\nImage Captioning [22, 148] MSCOCO, Visual Genome\nVisual Question Answering [148] Datasets from [6], Datasets from [157]\nVisual-Semantic Embedding [125] MSCOCO\nSpeech Tecognition [19] Mozilla Common Voice\nTable 4. Attacked Applications and Benchmark Datasets\nexperts. These works usually use multiple datasets to evaluate their attack strategies to show the\ngenerality and robustness of their method. [78] used DBpedia ontology dataset [72] to classify the\ndocument samples into 14 high-level classes. [39] used IMDB movie reviews [85] for sentiment\nanalysis, and Reuters-2 and Reuters-5 newswires dataset provided by NLTK package10 for cate-\ngorization. [104] used a un-specified movie review dataset for sentiment analysis. [117] also used\nIMDB movie review dataset for sentiment analysis. The work also performed gender classification\non and Twitter dataset11 for gender detection. [ 35] performed spam detection on Enron Spam\nDataset [91] and adopted six large datasets from [155], i.e., AG’s news12, Sogou news [138], DBPedia\nontology dataset, Yahoo! Answers13 for text categorization and Yelp reviews14, Amazon reviews\n[90] for sentiment analysis. [ 31] also used AG’s news for text classification. Further, they used\nStanford Sentiment Treebank (SST) dataset [128] for sentiment analysis. [118] conducted evalu-\nation on three tasks: sentiment analysis (IMDB movie review, Elec [56], Rotten Tomatoes [103]),\ntext categorization (DBpedia Ontology dataset and RCV1 [73]) and grammatical error detection\n(FCE-public [150]). [130] generated adversarial examples on the neural medical status prediction\n10https://www.nltk.org/\n11https://www.kaggle.com/crowdflower/twitter-user-gender-cla2013.\n12https://www.di.unipi.it/ËĲgulli/\n13Yahoo! Answers Comprehensive Questions and Answers version 1.0 dataset through the Yahoo! Webscope program.\n14Yelp Dataset Challenge in 2015\n, Vol. 1, No. 1, Article . Publication date: April 2019.\nAdversarial Attacks on Deep Learning Models in Natural Language Processing: A Survey 27\nsystem with real-world electronic health records data. Many works target the malware detection\nmodels. [45, 46] performed attack on neural malware detection systems. They used DREBIN dataset\nwhich contains both benigh and malicious android applications [7]. [114] collected benigh windows\napplication files and used Microsoft Malware Classification Challenge dataset [113] as the malicious\npart. [53] crawled 180 programs with corresponding behavior reports from a website for malware\nanalysis15. 70% of the crawled programs are malware. [98] proposed another kind of attack, called\nreprogramming. They specifically targeted the text classification neural models and used four\ndatasets to evaluate their attack methods: Surname Classification Dataset16, Experimental Data for\nQuestion Classification [76], Arabic Tweets Sentiment Classification Dataset [1] and IMDB movie\nreview dataset. In [145], the authors modelled the relation extraction as a classification problem,\nwhere the goal is to predict the relations exist between entity pairs given text mentions. They used\ntwo relation datasets: NYT dataset [111] and UW dataset [80]. The work [11] targeted at improving\nthe efficacy of the neural networks for joint entity and relation extraction. Different to the method\nin [145], the authors modelled the relation extraction task as a multi-label head selection problem.\nThe four datasets are used in their work: ACE04 dataset [28], CoNLL04 EC tasks [115], Dutch Real\nEstate Classifieds (DREC) dataset [12], and Adverse Drug Events (ADE) [47].\nMachine Translation. Machine Translation works on parallel datasets, one of which uses\nsource language and the other one is in the target language. [13] used the TED talks parallel corpus\nprepared for IWSLT 2016 [89] for testing the NMT systems. They also collected French, German\nand Czech corpus for generating natural noises to build a look-up table which contains possible\nlexical replacements that later be used for generating adversarial examples. [30] also used the same\nTED talks corpus and used German to English, Czech to English, and French to English pairs.\nMachine Comprehension. Machine comprehension datasets usually provide context docu-\nments or paragraphs to the machines. Based on the comprehension of the contexts, machine\ncomprehension models can answer a question. Jia and Liang are one of the first to consider the\ntextual adversary and they targeted the neural machine comprehension models [55]. They used\nthe Stanford Question Answering Dataset (SQuAD) to evaluate the impact of their attack on the\nneural machine comprehension models. SQuAD is a widely recognised benchmark dataset for\nmachine comprehension. [142] followed the previous works and also worked on SQuAD dataset.\nAlthouth the focus of the work [16] is to develop a robust machine comprehension model rather\nthan attacking MC models, they used the adversarial examples to evaluate their proposed system.\nThey used MovieQA multiple choice question answering dataset [ 134] for the evaluation. [ 21]\ntargeted attacks on differentiable neural computer (DNC), which is a novel computing machine\nwith DNN. They evaluated the attacks on logical question answering using bAbI tasks17.\nText Summarization. The goal for text summarization is to summarize the core meaning of a\ngiven document or paragraph with succinct expressions. There is no surveyed papers that only\ntarget the application of text summarization. [24] evaluated their attack on multiple applications\nincluding text summarization and they used DUC200318, DUC200419, and Gigaword20 for evaluating\nthe effectiveness of adversarial examples.\nText Entailment. The fundamental task of text entailment is to decide whether a premise text\nentails a hypothesis, i.e., the truth of one text fragment follows from another text. [57] assessed\n15https://malwr.com/\n16Classifying names with a character-level rnn - pytroch tutorial.\n17https://research.fb.com/downloads/babi/\n18http://duc.nist.gov/duc2003/tasks.html\n19http://duc.nist.gov/duc2004/\n20https://catalog.ldc.upenn.edu/LDC2003T05\n, Vol. 1, No. 1, Article . Publication date: April 2019.\n28 Zhang et al.\nvarious models on two entailment datasets: Standord Natural Lauguage Inference (SNLI) [17] and\nSciTail [59]. [92] also used SNLI dataset. Furthermore, they used MultiNLI [144] dataset.\nPart-of-Speech (POS) Tagging. The purpose for POS tagging is to resolve the part-of-speech\nfor each word in a sentence, such as noun, verb etc. It is one of the fundamental NLP tasks to\nfacilitate other NLP tasks, e.g., syntactic parsing. Neural networks are also adopted for this NLP task.\n[151] adopted the method in [94] to build a more robust neural network by introducing adversarial\ntraining, but they applied the strategy (with minor modifications) in POS tagging. By training on\nthe mixture of clean and adversarial example, the authors found that adversarial examples not\nonly help improving the tagging accuracy, but also contribute to downstream task of dependency\nparsing and is generally effective in different sequence labelling tasks. The datasets used in their\nevaluation include: the Wall Street Journal (WSJ) portion of the Penn Treebank (PTB) [ 87] and\ntreebanks from Universal Dependencies (UD) v1.2 [100].\nDialogue Generation. Dialogue generation is a fundamental component for real-world virtual\nassistants such as Siri21 and Alexa22. It is the text generation task that automatically generate a\nresponse given a post by the user. [99] is one of the first to attack the generative dialogue models.\nThey used the Ubuntu Dialogue Corpus [84] and Dynamic Knowledge Graph Network with the\nCollaborative Communicating Agents (CoCoA) dataset [49] for the evaluation of their two attack\nstrategies.\nCross-model Applications. [129] evaluated the OCR systems with adversarial examples using\nHillary Clinton’s emails23, which is in the form of images. They also conducted the attack on NLP\napplications using Rotten Tomatoes and IMDB review datasets. The work in [153] attacked the\nneural networks designed for scene text rcognition. They conducted experiments on three standard\nbenchmarks for cropped word image recognition, namely the Street View Text dataset (SVT) [139]\nthe ICDAR 2013 dataset (IC13) [58] and the IIIT 5K-word dataset (IIIT5K) [93]. [22] attacked the\nimage captioning neural models. The dataset they used is the Microsoft COCO (MSCOCO) dataset\n[79]. [148] worked on the problems of attacking neural models for image captioning and visual\nquestion answering. For the first task, they used Visual Genome dataset [65]. For the second task,\nthey used the VQA datasets collected and processed in [ 6]. [125] worked on Visual-Semantic\nEmbedding applications, where the MSCOCO dataset is used. [19] targeted the speech recognition\nproblem. The datasets they used is the Mozilla Common Voice dataset24.\nMulti-Applications Some works adapt their attack methods into different applications, namely,\nthey evaluate their method’s trasferability across applications. [ 24] attacked the sequence-to-\nsequence models. Specifically, they evaluated their attack on two applications: text summarization\nand machine translation. For text summarization, as mentioned before, they used three datasets\nDUC2003, DUC2004, and Gigaword. For the machine translation, they sampled a subset form\nWMT’16 Multimodal Translation dataset25. [54] proposed syntactically adversarial paraphrase and\nevaluated the attack on sentiment analysis and text entailment applications. They used SST for\nsentimental analysis and SICK [88] for text entailment. [156] is a generic approach for generating\nadversarial examples on neural models. The applications investigated include image classification\n(MINIST digital image dataset), textual entailment (SNLI), and machine translation. [94] evaluated\ntheir attacks on five datasets,covering both sentiment analysis (IMDB movie review, Elec product\nreview, Rotten Tomatoes movie review) and text categorization (DBpedia Ontology, RCV1 news\narticles). [127] targeted two applications. For sentiment analysis, they used Rotten Tomato movie\n21https://www.apple.com/au/siri/\n22https://en.wikipedia.org/wiki/Amazon_Echo\n23https://www. kaggle.com/kaggle/hillary-clinton-emails/data\n24https://voice.mozilla.org/en\n25http://www.statmt.org/wmt16/translation-task.html\n, Vol. 1, No. 1, Article . Publication date: April 2019.\nAdversarial Attacks on Deep Learning Models in Natural Language Processing: A Survey 29\nreviews and IMDB movie reviews datasets. For visual question answering, they tested on dataset\nprovided by Zhu et al. [157].\n5 DEFENSE\nAn essential purpose for generating adversarial examples for neural networks is to utilize these\nadversarial examples to enhance the model’s robustness [ 42]. There are two common ways in\ntextual DNN to achieve this goal:adversarial training and knowledge distillation. Adversarial training\nincorporates adversarial examples in the model training process. Knowledge distillation manipulates\nthe neural network model and trains a new model. In this section, we introduce some representative\nstudies belonging to these two directions. For more comprehensive defense strategies on machine\nlearning and deep leaning models and applications, please refer to [2, 15].\n5.1 Adversarial Training\nSzegedy et al. [ 132] invented adversarial training , a strategy that consists of training a neural\nnetwork to correctly classify both normal examples and adversarial examples. Goodfellow et al. [42]\nemployed explicit training with adversarial examples. In this section, we describe works utilizing\ndata augmentation , model regularization and robust optimization for the defense purpose on textual\nadversarial attacks.\n5.1.1 Data Augmentation. Data augmentation extends the original training set with the generated\nadversarial examples and try to let the model see more data during the training process. Data\naugmentation is commonly used against black-box attacks with additional training epochs on the\nattacked DNN with adversarial examples.\nThe authors in work [55] try to enhance the reading comprehension model with training on the\naugmented dataset that includes the adversarial examples. They showed that this data augmentation\nis effective and robust against the attack that uses the same adversarial examples. However, their\nwork also demonstrated that this augmentation strategy would be still vulnerable against the\nattacks with other kinds of adversarial examples. [142] shared similar idea to augment the training\ndataset, but selected further informative adversarial examples as discussed in Section 4.3.1.\nThe work in [57] trains the text entailment system augmented with adversarial examples. The\npurpose is to make the system more robust. They proposed three methods to generate more data with\ndiverse characteristics: (1) knowledge-based, which replaces words with their hypernym/hyponym\nprovided in several given knowledge bases; (2)hand-crafted, which adds negations to the the existing\nentailment; (3) neural-based, which leverages a seq2seq model to generate an entailment examples\nby enforcing the loss function to measure the cross-entropy between the original hypothesis and the\npredicted hypothesis. During the training process, they adopt the idea from generative adversarial\nnetwork to train a discriminator and a generator, and incorporating the adversarial examples in\nthe discriminator’s optimization step.\n[13] explores another way for data augmentation. It takes the average character embedding as a\nword representation and incorporate it into the input. This approach is intrinsically insensitive\nto character scrambling such as swap, mid and Rand, thus can resists to noises caused by these\nscrambling attacks proposed in the work. However, this defense is ineffective to other attacks that\ndo not perturb on characters’ orders.\n5.1.2 Model Regularization. Model regularization enforces the generated adversarial examples as\nthe regularizer and follows the form of:\nmin(J(f (x),y)+ λJ(f (x′),y)) (32)\nwhere λis a hyperparameter.\n, Vol. 1, No. 1, Article . Publication date: April 2019.\n30 Zhang et al.\nFollowing [42], the work [94] constructed the adversarial training with a linear approximation\nas follows:\n−logp(y|x + −ϵд/||д||2, ;θ) (33)\nд = ∂x logp(y|x; ˆθ)\nwhere ||д||2 is the L2 norm regularization, θ is the parameter of the neural model, and ˆθ is a constant\ncopy of θ. The difference to [ 42] is that, the authors performed the adversarial generation and\ntraining in terms of the word embedding. Further, they extended their previous work on attacking\nimage deep neural model [ 95], where the local distribution smoothness (LDS) is defined as the\nnegative of the KL divergence of two distributions (original data and the adversaries). LDS measures\nthe robustness of the model against the perturbation in local and ‘virtual’ adversarial direction.\nIn this sense, the adversary is calculated as the direction to which the model distribution is most\nsensitive in terms of KL divergence. They also applied this attack strategy on word embedding and\nperformed adversarial training by adding adversarial examples as regularizer.\nThe work [118] follows the idea from [94] and extends the adversarial training on LSTM. The\nauthors followed FGSM to incorporate the adversarial training as a regularizer. But in order to enable\nthe interpretability of adversarial examples, i.e., the word embedding of the adversaries should be\nvalid word embeddings in the vocabulary, they introduced a direction vector which associates the\nperturbed embedding to the valid word embedding. [145] simply adopts the regularizer utilized in\n[94], but applies the perturbations on pre-trained word embedding and in a different task: relation\nextraction. Other similar works that adopt [94] are [11, 118, 145, 151]. We will not cover all these\nworks in this article, since they simply adopting this method.\n5.1.3 Robust Optimisation. Madry et al. [86] cast DNN model learning as a robust optimization\nwith min-max (saddle point) formulation, which is the composition of an inner non-concave\nmaximization problem (attack) and an outer non-convex minimization problem (defense). According\nto Danskin’s theorem, gradients at inner maximizers correspond to descent directions for the min-\nmax problem, thus the optimization can still apply back-propagation to proceed. The approach\nsuccessfully demonstrated robustness of DNNs against adversarial images by training and learning\nuniversally. [3] adopts the idea and applies on malware detection DNN that handles discrete data.\nTheir leaning objective is formulated as:\nθ∗= arдmin\nθ\nE(x,y)∼D [max\nx′∈S(x)\nL(θ, x′,y)] (34)\nwhere S(x)is the set of binary indicator vectors that preserve the functionality of malware x, L is\nthe loss function for the original classification model, y is the groundtruth label, θ is the learnable\nparameters, D denotes the distribution of data sample x.\nIt is worth noting that the proposed robust optimisation method is an universal framework under\nwhich other adversarial training strategies have natural interpretation. We describe it separately\nkeeping in view its popularity in the literature.\n5.2 Distillation\nPapernot et al. [107] proposed distillation as another possible defense against adversarial examples.\nThe principle is to use the softmax output (e.g., the class probabilities in classfication DNNs) of the\noriginal DNN to train the second DNN, which has the same structure with the original one. The\nsoftmax of the original DNN is also modified by introducing a temperature parameter T :\nqi = exp (zi /T )Í\nk exp (zk /T ) (35)\n, Vol. 1, No. 1, Article . Publication date: April 2019.\nAdversarial Attacks on Deep Learning Models in Natural Language Processing: A Survey 31\nwhere zi is input of softmax layer.T controls the level of knowledge distillation. When T = 1, Eq.\n(35) turns back to the normal softmax function. If T is large, qi is close to a uniform distribution,\nwhen it is small, the function will output more extreme values. [46] adopts distillation defense for\nDNNs on discrete data and applied a high temperature T , as high-temperature softmax is proved\nto reduce the model sensitivity to small perturbations [107]. They trained the second DNN with\nthe augmentation of original dataset and the softmax outputs from the original DNN. From the\nevaluations, they found adversarial training is the more effective than distillation. (I like if there is\nanswers that explains why adversarial training is the more effective than distillation )\n6 DISCUSSIONS AND OPEN ISSUES\nGenerating textual adversarial examples has relatively shorter history than generating image\nadversarial examples on DNNs because it is more challenging to make perturbation on discrete\ndata, and meanwhile preserving the valid syntactic, grammar and semantics. We discuss some of\nthe issues in this section and provide suggestions on future directions.\n6.1 Perceivability\nPerturbations in image pixels are usually hard to be perceived, thus do not affect human judgment,\nbut can only fool the deep neural networks. However, the perturbation on text is obvious, no\nmatter the perturbation is flipping characters or changing words. Invalid words and syntactic\nerrors can be easily identified by human and detected by the grammar check software, hence the\nperturbation is hard to attack a real NLP system. However, many research works generate such\ntypes of adversarial examples. It is acceptable only if the purpose is utilizing adversarial examples\nto robustify the attacked DNN models. In semantic-preserving perspective, changing a word in a\nsentence sometimes changes its semantics drastically and is easily detected by human beings. For\nNLP applications such as reading comprehension, and sentiment analysis, the adversarial examples\nneed to be carefully designed in order not to change the should-be output. Otherwise, both correct\noutput and perturbed output change, violating the purpose of generating adversarial examples. This\nis challenging and limited works reviewed considered this constraint. Therefore, for practical attack,\nwe need to propose methods that make the perturbations not only unperceivable, but preserve\ncorrect grammar and semantics.\n6.2 Transferability\nTransferability is a common property for adversarial examples. It reflects the generalization of\nthe attack methods. Transferability means adversarial examples generated for one deep neural\nnetwork on a dataset can also effectively attack another deep neural network (i.e., cross-model\ngeneralization) or dataset (i.e., cross-data generalization). This property is more often exploited\nin black-box attacks as the details of the deep neural networks does not affect the attack method\nmuch. It is also shown that untargeted adversarial examples are much more transferable than\ntargeted ones [83]. Transferability can be organized into three levels in deep neural networks: (1)\nsame architecture with different data; (2) different architectures with same application; (3) different\narchitectures with different data [154]. Although current works on textual attacks cover both three\nlevels, the performance of the transferred attacks still decrease drastically compared to it on the\noriginal architecture and data, i.e., poor generalization ability. More efforts are expected to deliver\nbetter generalization ability.\n6.3 Automation\nSome reviewed works are able to generate adversarial examples automatically, while others cannot.\nIn white-box attacks, leveraging the loss function of the DNN can identify the most affected\n, Vol. 1, No. 1, Article . Publication date: April 2019.\n32 Zhang et al.\npoints (e.g., character, word) in a text automatically. Then the attacks are performed on these\npoints by automatically modifying the corresponding texts. In black-box attacks, some attacks, e.g.\nsubstitution train substitute DNNs and apply white-box attack strategies on the substitution. This\ncan be achieved automatically. However, most of the other works craft the adversarial examples in\na manual manner. For example, [55] concatenated manually-chosen meaningless paragraphs to\nfool the reading comprehension systems, in order to discover the vulnerability of the victim DNNs.\nMany research works followed their way, not aiming on practical attacks, but more on examining\nrobustness of the target network. These manaul works are time-consuming and impractical. We\nbelieve that more efforts in this line could pass through this barrier in future.\n6.4 New Architectures\nAlthough most of the common textual DNNs have gained attention from the perspective of adver-\nsarial attack (Section 2.2), many DNNs haven’t been attacked so far. For example, the generative\nneural models: Generative Adversarial Networks (GANs) and Variational Auto-Encoders (VAEs).\nIn NLP, they are used to generate texts. Deep generative models requires more sophisticated skill\nfor model training. This would explain that these techniques have been mainly overlooked by\nadversarial attack so far. Future works may consider about generating adversarial examples for\nthese generative DNNs. Another example is differentiable neural computer (DNC). Only one work\nattacked DNC so far [21]. Attention mechanism is somehow become a standard component in most\nof the sequential models. But there is no work examined the mechanism itself. Instead, works are\neither attack the overall system that contain attentions, or leverage attention scores to identify the\nword for perturbation [16].\n6.5 Iterative vs One-off\nIterative attacks iteratively search and update the perturbations based on the gradient of the output\nof the attacked DNN model. Thus it shows high quality and effectiveness, that is the perturbations\ncan be small enough and hard to defense. However, these methods usually require long time to\nfind the proper perturbations, rendering an obstacle for attacking in real-time. Therefore, one-off\nattacks are proposed to tackle this problem. FGSM [42] is one example of one-off attack. Natually,\none-off attack is much faster than iterative attack, but is less effective and easier to be defensed\n[153]. When designing attack methods on a real application, attackers need to carefully consider\nthe trade off between efficiency and effectiveness of the attack.\n7 CONCLUSION\nThis article presents the first comprehensive survey in the direction of generating textual adversarial\nexamples on deep neural networks. We review recent research efforts and develop classification\nschemes to organize existing literature. Additionally we summarize and analyze them from different\naspects. We attempt to provide a good reference for researchers to gain insight of the challenges,\nmethods and issues in this research topic and shed lights on future directions. We hope more robust\ndeep neural models are proposed based on the knowledge of the adversarial attacks.\nREFERENCES\n[1] Nawaf A Abdulla, Nizar A Ahmed, Mohammed A Shehab, and Mahmoud Al-Ayyoub. 2013. Arabic sentiment analysis:\nLexicon-based and corpus-based. In Proc. of the 2013 IEEE Jordan Conference on Applied Electrical Engineering and\nComputing Technologies (AEECT 2013). IEEE, 1–6.\n[2] Naveed Akhtar and Ajmal S. Mian. 2018. Threat of Adversarial Attacks on Deep Learning in Computer Vision: A\nSurvey. IEEE Access 6 (2018), 14410–14430.\n, Vol. 1, No. 1, Article . Publication date: April 2019.\nAdversarial Attacks on Deep Learning Models in Natural Language Processing: A Survey 33\n[3] Abdullah Al-Dujaili, Alex Huang, Erik Hemberg, and Una-May O’Reilly. 2018. Adversarial Deep Learning for\nRobust Detection of Binary Encoded Malware. InProc. of the 2018 IEEE Security and Privacy Workshops (SPW 2018).\nFrancisco, CA, USA, 76–82.\n[4] Abdullah Al-Dujaili, Alex Huang, Erik Hemberg, and Una-May O’Reilly. 2018. Adversarial Deep Learning for Robust\nDetection of Binary Encoded Malware. In Proc. of the 2018 IEEE Security and Privacy Workshops (SP Workshops\n2018). San Francisco, CA, USA, 76–82.\n[5] Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani B. Srivastava, and Kai-Wei Chang. 2018.\nGenerating Natural Language Adversarial Examples. InProc. of the 2018 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP 2018). Brussels, Belgium, 2890–2896.\n[6] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi\nParikh. 2015. VQA: Visual Question Answering. In Proc. of the 2015 IEEE International Conference on Computer\nVision (ICCV 2015). Santiago, Chile, 2425–2433.\n[7] Daniel Arp, Michael Spreitzenbarth, Malte Hubner, Hugo Gascon, and Konrad Rieck. 2014. DREBIN: Effective and\nExplainable Detection of Android Malware in Your Pocket. In Proc. of the 21st Annual Network and Distributed\nSystem Security Symposium (NDSS 2014). San Diego, California, USA.\n[8] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural Machine Translation by Jointly Learning to\nAlign and Translate. CoRR abs/1409.0473 (2014). arXiv:1409.0473 http://arxiv.org/abs/1409.0473\n[9] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural Machine Translation by Jointly Learning to\nAlign and Translate. (2015).\n[10] Marco Barreno, Blaine Nelson, Anthony D. Joseph, and J. D. Tygar. 2010. The security of machine learning. Machine\nLearning 81, 2 (2010), 121–148.\n[11] Giannis Bekoulis, Johannes Deleu, Thomas Demeester, and Chris Develder. 2018. Adversarial training for multi-\ncontext joint entity and relation extraction. InProc. of the 2018 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP 2018). Brussels, Belgium, 2830–2836.\n[12] Giannis Bekoulis, Johannes Deleu, Thomas Demeester, and Chris Develder. 2018. An attentive neural architecture for\njoint segmentation and parsing and its application to real estate ads. Expert Systems with Applications 102 (2018),\n100–112.\n[13] Yonatan Belinkov and Yonatan Bisk. 2018. Synthetic and Natural Noise Both Break Neural Machine Translation.\narXiv preprint arXiv:1711.02173. ICLR (2018).\n[14] Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A Neural Probabilistic Language Model.\nJournal of Machine Learning Research 3 (2003), 1137–1155.\n[15] Battista Biggio and Fabio Roli. 2018. Wild patterns: Ten years after the rise of adversarial machine learning. Pattern\nRecognition 84 (2018), 317–331.\n[16] Matthias Blohm, Glorianna Jagfeld, Ekta Sood, Xiang Yu, and Ngoc Thang Vu. 2018. Comparing Attention-Based\nConvolutional and Recurrent Neural Networks: Success and Limitations in Machine Reading Comprehension. InProc.\nof the 22nd Conference on Computational Natural Language Learning (CoNLL 2018). Brussels, Belgium, 108–118.\n[17] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus\nfor learning natural language inference. In Proc. of the 2015 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP 2015). Lisbon, Portugal, 632–642.\n[18] Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, Andrew M. Dai, Rafal Józefowicz, and Samy Bengio. 2016. Generating\nSentences from a Continuous Space. In Proc. of the 20th SIGNLL Conference on Computational Natural Language\nLearning (CoNLL 2016). Berlin, Germany, 10–21.\n[19] Nicholas Carlini and David A. Wagner. [n. d.]. Audio Adversarial Examples: Targeted Attacks on Speech-to-Text.\n[20] Nicholas Carlini and David A. Wagner. 2017. Towards Evaluating the Robustness of Neural Networks. In Proc. of the\n2017 IEEE Symposium on Security and Privacy (SP 2017). San Jose, CA, USA, 39–57.\n[21] Alvin Chan, Lei Ma, Felix Juefei-Xu, Xiaofei Xie, Yang Liu, and Yew Soon Ong. 2018. Metamorphic Relation\nBased Adversarial Attacks on Differentiable Neural Computer. CoRR abs/1809.02444 (2018). arXiv:1809.02444\nhttp://arxiv.org/abs/1809.02444\n[22] Hongge Chen, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, and Cho-Jui Hsieh. 2018. Attacking Visual Language Grounding\nwith Adversarial Examples: A Case Study on Neural Image Captioning. In Proceedings of ACL 2018.\n[23] Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui Jiang, and Diana Inkpen. 2017. Enhanced LSTM for Natural\nLanguage Inference. In Proc. of the 55th Annual Meeting of the Association for Computational Linguistics (ACL\n2017). Vancouver, BC, Canada, 1657–1668.\n[24] Minhao Cheng, Jinfeng Yi, Huan Zhang, Pin-Yu Chen, and Cho-Jui Hsieh. 2018. Seq2Sick: Evaluating the Robustness\nof Sequence-to-Sequence Models with Adversarial Examples. arXiv preprint arXiv:1803.01128 (2018).\n[25] Kyunghyun Cho, Bart van Merrienboer, Çaglar Gülçehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. 2014. Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine\n, Vol. 1, No. 1, Article . Publication date: April 2019.\n34 Zhang et al.\nTranslation. In Proc. of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP 2014).\nDoha, Qatar, 1724–1734.\n[26] Marta R. Costa-Jussà and José A. R. Fonollosa. 2016. Character-based Neural Machine Translation. In Proc. of the\n54th Annual Meeting of the Association for Computational Linguistics (ACL 2016). Berlin, Germany.\n[27] George E. Dahl, Jack W. Stokes, Li Deng, and Dong Yu. 2013. Large-scale malware classification using random\nprojections and neural networks. In Proc. of the 38th International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP 2013). Vancouver, BC, Canada, 3422–3426.\n[28] George Doddington, Alexis Mitchell, Mark Przybocki, Lance Ramshaw, Stephanie Strassel, and Ralph Weischedel. 2004.\nThe Automatic Content Extraction (ACE) Program -Tasks, Data, and Evaluation. InProc. of the Fourth International\nConference on Language Resources and Evaluation (LREC’04). Lisbon, Portugal.\n[29] Daria Dzendzik, Carl Vogel, and Qun Liu. 2017. Who framed roger rabbit? multiple choice questions answering about\nmovie plot. (2017).\n[30] Javid Ebrahimi, Daniel Lowd, and Dejing Dou. [n. d.]. On Adversarial Examples for Character-Level Neural Machine\nTranslation. In Proc. of the 27th International Conference on Computational Linguistics (COLING 2018). Santa Fe,\nNew Mexico, USA, 653–663.\n[31] Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. 2018. HotFlip: White-Box Adversarial Examples for Text\nClassification. In Proc. of the 56th Annual Meeting of the Association for Computational Linguistics (ACL 2018).\nMelbourne, Australia, 31–36.\n[32] Gamaleldin F. Elsayed, Ian J. Goodfellow, and Jascha Sohl-Dickstein. 2018. Adversarial Reprogramming of Neural\nNetworks. CoRR abs/1806.11146 (2018).\n[33] Fartash Faghri, David J. Fleet, Ryan Kiros, and Sanja Fidler. 2017. VSE++: Improved Visual-Semantic Embeddings.\nCoRR abs/1707.05612 (2017).\n[34] Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and Marcus Rohrbach. 2016. Multimodal\nCompact Bilinear Pooling for Visual Question Answering and Visual Grounding. In Proc. of the 2016 Conference on\nEmpirical Methods in Natural Language Processing (EMNLP 2016). Austin, Texas, USA, 457–468.\n[35] Ji Gao, Jack Lanchantin, Mary Lou Soffa, and Yanjun Qi. 2018. Black-box Generation of Adversarial Text Sequences\nto Evade Deep Learning Classifiers. arXiv preprint arXiv:1801.04354 (2018).\n[36] Justin Gilmer, Ryan P. Adams, Ian J. Goodfellow, David Andersen, and George E. Dahl. 2018. Motivating the Rules of\nthe Game for Adversarial Example Research. CoRR abs/1807.06732 (2018). arXiv:1807.06732 http://arxiv.org/abs/\n1807.06732\n[37] Yoav Goldberg. 2017. Neural Network Methods for Natural Language Processing. Morgan & Claypool Publishers.\nhttps://doi.org/10.2200/S00762ED1V01Y201703HLT037\n[38] Christoph Goller and Andreas Kuchler. 1996. Learning task-dependent distributed representations by backpropagation\nthrough structure. Neural Networks 1 (1996), 347–352.\n[39] Zhitao Gong, Wenlu Wang, Bo Li, Dawn Song, and Wei-Shinn Ku. 2018. Adversarial Texts with Gradient Methods.\narXiv preprint arXiv:1801.07175 (2018).\n[40] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep learning. Vol. 1.\n[41] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville,\nand Yoshua Bengio. 2014. Generative Adversarial Nets. In Proc. of the Annual Conference on Neural Information\nProcessing Systems 2014 (NIPS 2014). Montreal, Quebec, Canada, 2672–2680.\n[42] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining and Harnessing Adversarial Examples. In\nProc. of the 3rd International Conference on Learning Representations (ICLR 2015).\n[43] Edouard Grave, Tomas Mikolov, Armand Joulin, and Piotr Bojanowski. 2017. Bag of Tricks for Efficient Text Classi-\nfication. In Proc. of the 15th Conference of the European Chapter of the Association for Computational Linguistics\n(EACL 2017). Valencia, Spain, 427–431.\n[44] Alex Graves, Abdel-rahman Mohamed, and Geoffrey E. Hinton. 2013. Speech recognition with deep recurrent neural\nnetworks. In Proc. of IEEE 2013 International Conference on Acoustics, Speech and Signal Processing (ICASSP 2013).\nVancouver, BC, Canada, 6645–6649.\n[45] Kathrin Grosse, Nicolas Papernot, Praveen Manoharan, Michael Backes, and Patrick McDaniel. 2016. Adversarial\nperturbations against deep neural networks for malware classification. arXiv preprint arXiv:1606.04435 (2016).\n[46] Kathrin Grosse, Nicolas Papernot, Praveen Manoharan, Michael Backes, and Patrick D. McDaniel. 2017. Adversarial\nExamples for Malware Detection. In Proc. of the 22nd European Symposium on Research in Computer Security\n(ESORICS 2017). Oslo, Norway, 62–79.\n[47] Harsha Gurulingappa, Abdul Mateen Rajput, Angus Roberts, Juliane Fluck, Martin Hofmann-Apitius, and Luca Toldo.\n2012. Development of a benchmark corpus to support the automatic extraction of drug-related adverse effects from\nmedical case reports. Journal of Biomedical Informatics 45, 5 (2012), 885–892.\n, Vol. 1, No. 1, Article . Publication date: April 2019.\nAdversarial Attacks on Deep Learning Models in Natural Language Processing: A Survey 35\n[48] Awni Y. Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh,\nShubho Sengupta, Adam Coates, and Andrew Y. Ng. 2014. Deep Speech: Scaling up end-to-end speech recognition.\nCoRR abs/1412.5567 (2014).\n[49] He He, Anusha Balakrishnan, Mihail Eric, and Percy Liang. 2017. Learning Symmetric Collaborative Dialogue\nAgents with Dynamic Knowledge Graph Embeddings. In Proc. of the 55th Annual Meeting of the Association for\nComputational Linguistics (ACL 2017). Vancouver, Canada, 1766–1776.\n[50] Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. 2015. Distilling the Knowledge in a Neural Network. CoRR\nabs/1503.02531 (2015). arXiv:1503.02531 http://arxiv.org/abs/1503.02531\n[51] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long Short-Term Memory. Neural Computation 9, 8 (1997), 1735–\n1780.\n[52] Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Kate Saenko. 2017. Learning to Reason: End-\nto-End Module Networks for Visual Question Answering. In Proc. of IEEE International Conference on Computer\nVision (ICCV 2017). Venice, Italy, 804–813.\n[53] Weiwei Hu and Ying Tan. 2017. Black-Box Attacks against RNN based Malware Detection Algorithms.arXiv preprint\narXiv:1705.08131 (2017).\n[54] Mohit Iyyer, John Wieting, Kevin Gimpel, and Luke Zettlemoyer. 2018. Adversarial Example Generation with\nSyntactically Controlled Paraphrase Networks. InProc. of the 2018 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies (NAACL-HLT). New Orleans, Louisiana,\nUSA, 1875–1885.\n[55] Robin Jia and Percy Liang. 2017. Adversarial Examples for Evaluating Reading Comprehension Systems. In Proc. of\nthe 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP 2017). Copenhagen, Denmark,\n2021–2031.\n[56] Rie Johnson and Tong Zhang. 2015. Semi-supervised Convolutional Neural Networks for Text Categorization via\nRegion Embedding. In Proc. of the Annual Conference on Neural Information Processing Systems 2015 (NIPS 2015).\nMontreal, Quebec, Canada, 919–927.\n[57] Dongyeop Kang, Tushar Khot, Ashish Sabharwal, , and Eduard Hovy. 2018. AdvEntuRe: Adversarial Training for\nTextual Entailment with Knowledge-Guided Examples. In Proceedings of ACL 2018.\n[58] Dimosthenis Karatzas, Faisal Shafait, Seiichi Uchida, Masakazu Iwamura, Lluis Gomez i Bigorda, Sergi Robles Mestre,\nJoan Mas, David Fernández Mota, Jon Almazán, and Lluís-Pere de las Heras. 2013. ICDAR 2013 Robust Reading\nCompetition. In Proc. of the 12th International Conference on Document Analysis and Recognition (ICDAR 2013).\nWashington, DC, USA, 1484–1493.\n[59] Tushar Khot, Ashish Sabharwal, and Peter Clark. 2018. SciTaiL: A Textual Entailment Dataset from Science Question\nAnswering. In Proc. of the 32nd AAAI Conference on Artificial Intelligence (AAAI 2018). New Orleans, Louisiana,\nUSA, 5189–5197.\n[60] Yoon Kim. 2014. Convolutional Neural Networks for Sentence Classification. In Proc. of the 2014 Conference on\nEmpirical Methods in Natural Language Processing (EMNLP 2014). Doha, Qatar, 1746–1751.\n[61] Yoon Kim, Yacine Jernite, David Sontag, and Alexander M. Rush. 2016. Character-Aware Neural Language Models. In\nProc. of the 13th AAAI Conference on Artificial Intelligence (AAAI 2016). Phoenix, Arizona, USA, 2741–2749.\n[62] Diederik P. Kingma and Jimmy Ba. 2015. dam: A Method for Stochastic Optimization. InProc. of the 3rd International\nConference on Learning Representations (ICLR 2015). San Diego, CA, USA.\n[63] Diederik P. Kingma and Max Welling. 2014. Auto-Encoding Variational Bayes. In Proc. of the 2014 International\nConference on Learning Representations (ICLR 2014).\n[64] Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander M. Rush. 2017. OpenNMT: Open-Source\nToolkit for Neural Machine Translation. In Proc. of the 55th Annual Meeting of the Association for Computational\nLinguistics (ACL 2017). Vancouver, BC, Canada, 67–72.\n[65] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis,\nLi-Jia Li, David A. Shamma, Michael S. Bernstein, and Li Fei-Fei. 2017. Visual Genome: Connecting Language and\nVision Using Crowdsourced Dense Image Annotations. International Journal of Computer Vision 123, 1 (2017),\n32–73.\n[66] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. 2012. ImageNet Classification with Deep Convolutional\nNeural Networks. In Proc. of the 26th Annual Conference on Neural Information Processing Systems (NIPS 2012).\nLake Tahoe, Nevada, USA, 1106–1114.\n[67] Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong, Romain\nPaulus, and Richard Socher. 2016. Ask Me Anything: Dynamic Memory Networks for Natural Language Processing. In\nProc. of the 33nd International Conference on Machine Learning (ICML 2016). New York City, NY, USA, 1378–1387.\n[68] Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. 2017. Adversarial Machine Learning at Scale. In Proc. of the\n5th International Conference on Learning Representations (ICLR 2017). oulon, France.\n, Vol. 1, No. 1, Article . Publication date: April 2019.\n36 Zhang et al.\n[69] Matt J. Kusner, Yu Sun, Nicholas I. Kolkin, and Kilian Q. Weinberger. 2015. From Word Embeddings To Document\nDistances. In Proc. of the 32nd International Conference on Machine Learning (ICML 2015)). Lille, France, 957–966.\n[70] Quoc V. Le and Tomas Mikolov. 2014. Distributed Representations of Sentences and Documents. In Proc. of the 31th\nInternational Conference on Machine Learning (ICML 2014). Beijing, China, 1188–1196.\n[71] Jason Lee, Kyunghyun Cho, and Thomas Hofmann. 2017. Fully Character-Level Neural Machine Translation without\nExplicit Segmentation. TACL 5 (2017), 365–378.\n[72] Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, Dimitris Kontokostas, Pablo N. Mendes, Sebastian Hellmann,\nMohamed Morsey, Patrick van Kleef, Sören Auer, and Christian Bizer. 2015. DBpedia - A large-scale, multilingual\nknowledge base extracted from Wikipedia. Semantic Web 6, 2 (2015), 167–195.\n[73] David D. Lewis, Yiming Yang, Tony G. Rose, and Fan Li. 2004. RCV1: A New Benchmark Collection for Text\nCategorization Research. Journal of Machine Learning Research 5 (2004), 361–397.\n[74] Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting Wang. 2019. TextBugger: Generating Adversarial Text Against\nReal-world Applications. In Proc. of 26th Annual Network and Distributed System Security Symposium (NDSS 2019).\nSan Diego, California, USA.\n[75] Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky, Michel Galley, and Jianfeng Gao. 2016. Deep Reinforcement Learning\nfor Dialogue Generation. In Proc. of the 2016 Conference on Empirical Methods in Natural Language Processing\n(EMNLP 2016). Austin, Texas, USA, 1192–1202.\n[76] Xin Li and Dan Roth. 2002. Learning Question Classifiers. In Proc. of the 19th International Conference on\nComputational Linguistics (COLING 2002). aipei, Taiwan.\n[77] Yang Liu Li Deng. 2018. Deep Learning in Natural Language Processing. Springer Singapore. https://doi.org/10.\n1007/978-981-10-5209-5\n[78] Bin Liang, Hongcheng Li, Miaoqiang Su, Pan Bian, Xirong Li, and Wenchang Shi. 2017. Deep Text Classification Can\nbe Fooled. arXiv preprint arXiv:1704.08006 (2017).\n[79] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and\nC. Lawrence Zitnick. 2014. Microsoft COCO: Common Objects in Context. In Proc. of the 13th European Conference\non Computer Vision (ECCV 2014). Zurich, Switzerland, 740–755.\n[80] Angli Liu, Stephen Soderland, Jonathan Bragg, Christopher H. Lin, Xiao Ling, and Daniel S. Weld. 2016. Effective\nCrowd Annotation for Relation Extraction. In Proc. of the 2016 Conference of the North American Chapter of the\nAssociation for Computational Linguistics (NAACL 2016). San Diego California, USA, 897–906.\n[81] Qiang Liu, Pan Li, Wentao Zhao, Wei Cai, Shui Yu, and Victor C. M. Leung. 2018. A Survey on Security Threats and\nDefensive Techniques of Machine Learning: A Data Driven View. IEEE Access 6 (2018), 12103–12117.\n[82] Tzu-Chien Liu, Yu-Hsueh Wu, and Hung-yi Lee. 2017. Attention-based CNN Matching Net. CoRR abs/1709.05036\n(2017).\n[83] Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. 2017. Delving into Transferable Adversarial Examples and\nBlack-box Attacks. In Proc. of the 2017 International Conference on Learning Representations (ICLR 2017).\n[84] Ryan Lowe, Nissan Pow, Iulian Serban, and Joelle Pineau. 2015. The Ubuntu Dialogue Corpus: A Large Dataset for\nResearch in Unstructured Multi-Turn Dialogue Systems. In Proc. of the 16th Annual Meeting of the Special Interest\nGroup on Discourse and Dialogue (SIGDIAL 2015). Prague, Czech Republic, 285–294.\n[85] Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning\nWord Vectors for Sentiment Analysis. In Proc. of the 49th Annual Meeting of the Association for Computational\nLinguistics (ACL 2011). Portland, Oregon, USA, 142–150.\n[86] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. 2018. Towards\nDeep Learning Models Resistant to Adversarial Attacks. In Proc. of the 6th International Conference on Learning\nRepresentations (ICLR 2018). Vancouver, BC, Canada.\n[87] Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a Large Annotated Corpus of\nEnglish: The Penn Treebank. Computational Linguistics 19, 2 (1993), 313–330.\n[88] Marco Marelli, Luisa Bentivogli, Marco Baroni, Raffaella Bernardi, Stefano Menini, and Roberto Zamparelli. 2014.\nSemEval-2014 Task 1: Evaluation of Compositional Distributional Semantic Models on Full Sentences through\nSemantic Relatedness and Textual Entailment. In Proc. of the 8th International Workshop on Semantic Evaluation\n(SemEval@COLING 2014). Dublin, Ireland, 1–8.\n[89] Cettolo Mauro, Girardi Christian, and Federico Marcello. 2012. Wit3: Web Inventory of Transcribed and Translated\nTalks. In Conference of European Association for Machine Translation. 261–268.\n[90] Julian J. McAuley and Jure Leskovec. 2013. Hidden factors and hidden topics: understanding rating dimensions\nwith review text. In Proc. of the 7th ACM Conference on Recommender Systems (RecSys 2013). Hong Kong, China,\n165–172.\n[91] Vangelis Metsis, Ion Androutsopoulos, and Georgios Paliouras. 2006. Spam Filtering with Naive Bayes - Which Naive\nBayes?. In Proc. of the Third Conference on Email and Anti-Spam (CEAS 2006). Mountain View, California, USA.\n, Vol. 1, No. 1, Article . Publication date: April 2019.\nAdversarial Attacks on Deep Learning Models in Natural Language Processing: A Survey 37\n[92] Pasquale Minervini and Sebastian Riedel. 2018. Adversarially Regularising Neural NLI Models to Integrate Logical\nBackground Knowledge. In Proc. of the 22nd Conference on Computational Natural Language Learning (CoNLL\n2018). Brussels, Belgium, 65–74.\n[93] Anand Mishra, Karteek Alahari, and C. V. Jawahar. 2012. Scene Text Recognition using Higher Order Language Priors.\nIn Proc. of the 23rd British Machine Vision Conference (BMVC 2012). Surrey, UK, 1–11.\n[94] Takeru Miyato, Andrew M Dai, and Ian Goodfellow. 2016. Adversarial training methods for semi-supervised text\nclassification. arXiv preprint arXiv:1605.07725 (2016).\n[95] Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, Ken Nakae, and Shin Ishii. 2016. Distributional smoothing with\nvirtual adversarial training. In Proc. of the 4th International Conference on Learning Representations (ICLR 2016).\n[96] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. 2016. DeepFool: A Simple and Accurate\nMethod to Fool Deep Neural Networks. In Proc. of the 2016 IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR 2016). Las Vegas, NV, USA, 2574–2582.\n[97] Michael C Mozer. 1995. A focused backpropagation algorithm for temporal. Backpropagation: Theory, architectures,\nand applications 137 (1995).\n[98] Paarth Neekhara, Shehzeen Hussain, Shlomo Dubnov, and Farinaz Koushanfar. 2018. Adversarial Reprogramming of\nSequence Classification Neural Networks. CoRR abs/1809.01829 (2018). arXiv:1809.01829 http://arxiv.org/abs/1809.\n01829\n[99] Tong Niu and Mohit Bansal. 2018. Adversarial Over-Sensitivity and Over-Stability Strategies for Dialogue Models.\nIn Proc. of the 22nd Conference on Computational Natural Language Learning (CoNLL 2018). Brussels, Belgium,\n486–496.\n[100] Joakim Nivre, Željko Agić, Maria Jesus Aranzabe, Masayuki Asahara, Aitziber Atutxa, Miguel Ballesteros, John Bauer,\nKepa Bengoetxea, Riyaz Ahmad Bhat, Cristina Bosco, et al. 2015. Universal Dependencies 1.2. (2015).\n[101] Daniel W. Otter, Julian R. Medina, and Jugal K. Kalita. 2018. A Survey of the Usages of Deep Learning in Natural\nLanguage Processing. CoRR abs/1807.10854 (2018).\n[102] Hamid Palangi, Li Deng, Yelong Shen, Jianfeng Gao, Xiaodong He, Jianshu Chen, Xinying Song, and Rabab K. Ward.\n2016. Deep Sentence Embedding Using Long Short-Term Memory Networks: Analysis and Application to Information\nRetrieval. IEEE/ACM Trans. Audio, Speech & Language Processing 24, 4 (2016), 694–707.\n[103] Bo Pang and Lillian Lee. 2005. Seeing Stars: Exploiting Class Relationships for Sentiment Categorization with Respect\nto Rating Scales. In Proc. of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL 2005).\nMichigan, USA, 115–124.\n[104] Nicolas Papernot, Patrick McDaniel, Ananthram Swami, and Richard Harang. 2016. Crafting Adversarial Input\nSequences for Recurrent Neural Networks. InMilitary Communications Conference, MILCOM 2016-2016 IEEE. IEEE,\n49–54.\n[105] Nicolas Papernot, Patrick D. McDaniel, Ian J. Goodfellow, Somesh Jha, Z. Berkay Celik, and Ananthram Swami. 2017.\nPractical Black-Box Attacks against Machine Learning. In Proc. of the 2017 ACM on Asia Conference on Computer\nand Communications Security (AsiaCCS 2017). Abu Dhabi, United Arab Emirates, 506–519.\n[106] Nicolas Papernot, Patrick D. McDaniel, Somesh Jha, Matt Fredrikson, Z. Berkay Celik, and Ananthram Swami. 2016.\nThe Limitations of Deep Learning in Adversarial Settings. In IEEE European Symposium on Security and Privacy\n(EuroS&P 2016). Saarbrücken, Germany, 372–387.\n[107] Nicolas Papernot, Patrick D. McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. 2016. Distillation as a Defense\nto Adversarial Perturbations Against Deep Neural Networks. In Proc. of the 2016 IEEE Symposium on Security and\nPrivacy (SP 2016). San Jose, CA, USA, 582–597.\n[108] Ankur P. Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. 2016. A Decomposable Attention Model for\nNatural Language Inference. In Proc. of the 2016 Conference on Empirical Methods in Natural Language Processing\n(EMNLP 2016). Austin, Texas, USA, 2249–2255.\n[109] Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer.\n2018. Deep Contextualized Word Representations. In Proc. of the 2018 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2018). New Orleans,\nLouisiana, USAr, 2227–2237.\n[110] Edward Raff, Jon Barker, Jared Sylvester, Robert Brandon, Bryan Catanzaro, and Charles K. Nicholas. [n. d.]. Malware\nDetection by Eating a Whole EXE. In The Workshops of the The Thirty-Second AAAI Conference on Artificial\nIntelligence. New Orleans, Louisiana, USA, 268–276.\n[111] Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling Relations and Their Mentions without La-\nbeled Text. In Proc. of 2010 European Conference on Machine Learning and Knowledge Discovery in Databases\n(ECML/PKDD 2010). Barcelona, Spain, 148–163.\n[112] Tim Rocktäschel, Edward Grefenstette, Karl Moritz Hermann, Tomás Kociský, and Phil Blunsom. 2016. Reasoning\nabout Entailment with Neural Attention. In Proc. of the 2016 International Conference on Learning Representations\n, Vol. 1, No. 1, Article . Publication date: April 2019.\n38 Zhang et al.\n(ICLR 2016).\n[113] Royi Ronen, Marian Radu, Corina Feuerstein, Elad Yom-Tov, and Mansour Ahmadi. 2018. Microsoft Malware\nClassification Challenge. CoRR abs/1802.10135 (2018). arXiv:1802.10135 http://arxiv.org/abs/1802.10135\n[114] Ishai Rosenberg, Asaf Shabtai, Lior Rokach, and Yuval Elovici. 2017. Generic Black-Box End-to-End Attack against\nRNNs and Other API Calls Based Malware Classifiers. arXiv preprint arXiv:1707.05970 (2017).\n[115] Dan Roth and Wen-tau Yih. 2004. A Linear Programming Formulation for Global Inference in Natural Language Tasks.\nIn Proc. of the 8th Conference on Computational Natural Language Learning (CoNLL 2004). Boston, Massachusetts,\n1–8.\n[116] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. 1986. Learning representations by back-propagating\nerrors. nature 323, 6088 (1986), 533.\n[117] Suranjana Samanta and Sameep Mehta. 2018. Generating Adversarial Text Samples. In Proc. of the 40th European\nConference on IR Research (ECIR 2018). Grenoble, France, 744–749.\n[118] Motoki Sato, Jun Suzuki, Hiroyuki Shindo, and Yuji Matsumoto. 2018. Interpretable Adversarial Perturbation in Input\nEmbedding Space for Text. arXiv preprint arXiv:1805.02917 (2018).\n[119] Dale Schuurmans and Martin Zinkevich. 2016. Deep Learning Games. In Proc. of the Annual Conference on Neural\nInformation Processing Systems 2016 (NIPS 2016). Barcelona, Spain, 1678–1686.\n[120] Rico Sennrich, Orhan Firat, Kyunghyun Cho, Alexandra Birch, Barry Haddow, Julian Hitschler, Marcin Junczys-\nDowmunt, Samuel Läubli, Antonio Valerio Miceli Barone, Jozef Mokry, and Maria Nadejde. 2017. Nematus: a Toolkit\nfor Neural Machine Translation. In Proc. of the 15th Conference of the European Chapter of the Association for\nComputational Linguistics (EACL 2017), Demo. Valencia, Spain, 65–68.\n[121] Min Joon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2016. Bidirectional Attention Flow for\nMachine Comprehension. CoRR abs/1611.01603 (2016). arXiv:1611.01603 http://arxiv.org/abs/1611.01603\n[122] Iulian Vlad Serban, Alessandro Sordoni, Yoshua Bengio, Aaron C. Courville, and Joelle Pineau. 2016. Building\nEnd-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models. InProc. of the Thirtieth AAAI\nConference on Artificial Intelligence (AAAI 2016). Phoenix, Arizona, USA, 3776–3784.\n[123] Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe, Laurent Charlin, Joelle Pineau, Aaron C. Courville, and Yoshua\nBengio. 2017. A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues. In Proc. of the 31st\nAAAI Conference on Artificial Intelligence (AAAI 2017). San Francisco, California, USA, 3295–3301.\n[124] Baoguang Shi, Xiang Bai, and Cong Yao. 2017. An end-to-end trainable neural network for image-based sequence recog-\nnition and its application to scene text recognition. IEEE transactions on pattern analysis and machine intelligence\n39, 11 (2017), 2298–2304.\n[125] Haoyue Shi, Jiayuan Mao, Tete Xiao, Yuning Jiang, and Jian Sun. 2018. Learning Visually-Grounded Semantics\nfrom Contrastive Adversarial Samples. In Proc. of the 27th International Conference on Computational Linguistics\n(COLING 2018). Santa Fe, New Mexico, USA, 3715–3727.\n[126] Karen Simonyan and Andrew Zisserman. 2015. Very Deep Convolutional Networks for Large-Scale Image Recognition.\nIn Proc. of the 3rd International Conference on Learning Representations (ICLR 2015. San Diego, CA, USA.\n[127] Sameer Singh, Carlos Guestrin, and Marco Túlio Ribeiro. 2018. Semantically Equivalent Adversarial Rules for\nDebugging NLP models. In Proc. of the 56th Annual Meeting of the Association for Computational Linguistics (ACL\n2018). Melbourne, Australia, 856–865.\n[128] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher\nPotts. 2013. Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank. In Proc. of the\n2013 Conference on Empirical Methods in Natural Language Processing (EMNLP 2013). Seattle, Washington, USA,\n1631–1642.\n[129] Congzheng Song and Vitaly Shmatikov. 2018. Fooling OCR Systems with Adversarial Text Images. CoRR\nabs/1802.05385 (2018). arXiv:1802.05385 http://arxiv.org/abs/1802.05385\n[130] Mengying Sun, Fengyi Tang, Jinfeng Yi, Fei Wang, and Jiayu Zhou. 2018. Identify Susceptible Locations in Med-\nical Records via Adversarial Attacks on Deep Predictive Models. In Proc. of the 24th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining (KDD 2018). London, UK, 793–801.\n[131] Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to Sequence Learning with Neural Networks. In Proc.\nof the Annual Conference on Neural Information Processing Systems 2014 (NIPS 2014). Montreal, Quebec, Canada,\n2672–2680.\n[132] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, and Joan Bruna. 2014. Intriguing properties of neural networks.\nIn Proc. of the 2nd International Conference on Learning Representations (ICLR 2014).\n[133] Kai Sheng Tai, Richard Socher, and Christopher D. Manning. 2015. Improved Semantic Representations From\nTree-Structured Long Short-Term Memory Networks. In Proc. of the 53rd Annual Meeting of the Association for\nComputational Linguistics (ACL 2015). Beijing, China, 1556–1566.\n, Vol. 1, No. 1, Article . Publication date: April 2019.\nAdversarial Attacks on Deep Learning Models in Natural Language Processing: A Survey 39\n[134] Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen, Antonio Torralba, Raquel Urtasun, and Sanja Fidler. 2016.\nMovieQA: Understanding Stories in Movies through Question-Answering. In Proc. of the 2016 IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR 2016). Las Vegas, NV, USA, 4631–4640.\n[135] Tesseract. 2016. https://github.com/tesseract-ocr/.\n[136] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia\nPolosukhin. 2017. Attention is All you Need. In Proc. of the Annual Conference on Neural Information Processing\nSystems 2017 (NIPS 2017). Long Beach, CA, USA, 6000–6010.\n[137] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. 2015. Show and tell: A neural image caption\ngenerator. In Proc. of IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2015). Boston, MA, USA,\n3156–3164.\n[138] Canhui Wang, Min Zhang, Shaoping Ma, and Liyun Ru. 2008. Automatic online news issue construction in web\nenvironment. In Proc. of the 17th International Conference on World Wide Web (WWW 2008). Beijing, China, 457–\n466.\n[139] Kai Wang, Boris Babenko, and Serge J. Belongie. 2011. End-to-end scene text recognition. In Proc. of the 2011 IEEE\nInternational Conference on Computer Vision (ICCV 2011). Barcelona, Spain, 1457–1464.\n[140] Shuohang Wang and Jing Jiang. 2016. A compare-aggregate model for matching text sequences. arXiv preprint\narXiv:1611.01747 (2016).\n[141] Shuohang Wang and Jing Jiang. 2016. Machine Comprehension Using Match-LSTM and Answer Pointer. CoRR\nabs/1608.07905 (2016). arXiv:1608.07905 http://arxiv.org/abs/1608.07905\n[142] Yicheng Wang and Mohit Bansal. 2018. Robust Machine Comprehension Models via Adversarial Training. In Proc.\nof the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies (NAACL-HLT). New Orleans, Louisiana, 575–581.\n[143] David Warde-Farley and Ian Goodfellow. 2016. Adversarial Perturbations of Deep Neural Networks. Perturbations,\nOptimization, and Statistics 311 (2016).\n[144] Adina Williams, Nikita Nangia, and Samuel R. Bowman. 2018. A Broad-Coverage Challenge Corpus for Sentence\nUnderstanding through Inference. In Proc. of the 2018 Conference of the North American Chapter of the Association\nfor Computational Linguistics (NAACL 2018). New Orleans, Louisiana, USA, 1112–1122.\n[145] Yi Wu, David Bamman, and Stuart Russell. 2017. Adversarial training for relation extraction. In Proceedings of the\n2017 Conference on Empirical Methods in Natural Language Processing. 1778–1783.\n[146] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun,\nYuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser,\nStephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang,\nCliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean.\n2016. Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation.\nCoRR abs/1609.08144 (2016). arXiv:1609.08144 http://arxiv.org/abs/1609.08144\n[147] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville, Ruslan Salakhutdinov, Richard S. Zemel, and\nYoshua Bengio. 2015. Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. In Proc. of the\n32nd International Conference on Machine Learning (ICML 2015). Lille, France, 2048–2057.\n[148] Xiaojun Xu, Xinyun Chen, Chang Liu, Anna Rohrbach, Trevor Darrell, and Dawn Song. 2018. Fooling Vision and\nLanguage Models Despite Localization and Attention Mechanism. In Proc. of 2018 IEEE Conference on Computer\nVision and Pattern Recognition (CVPR 2018). Salt Lake City, UT, USA, 4951–4961.\n[149] Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. 2015. Embedding Entities and Relations for\nLearning and Inference in Knowledge Bases. InProc. of the 3rd International Conference on Learning Representations\n(ICLR 2015). San Diego, CA, USA.\n[150] Helen Yannakoudakis, Ted Briscoe, and Ben Medlock. 2011. A New Dataset and Method for Automatically Grading\nESOL Texts. In Proc. of the 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011).\nPortland, Oregon, USA, 180–189.\n[151] Michihiro Yasunaga, Jungo Kasai, and Dragomir R. Radev. 2018. Robust Multilingual Part-of-Speech Tagging\nvia Adversarial Training. In Proc. of the 2018 Conference of the North American Chapter of the Association for\nComputational Linguistics (NAACL 2018). New Orleans, Louisiana, USA, 976–986.\n[152] Tom Young, Devamanyu Hazarika, Soujanya Poria, and Erik Cambria. 2018. Recent Trends in Deep Learning Based\nNatural Language Processing. IEEE Computational Intelligence Magazine 13, 3 (2018), 55–75.\n[153] Xiaoyong Yuan, Pan He, and Xiaolin Andy Li. 2018. Adaptive Adversarial Attack on Scene Text Recognition. CoRR\nabs/1807.03326 (2018). arXiv:1807.03326 http://arxiv.org/abs/1807.03326\n[154] Xiaoyong Yuan, Pan He, Qile Zhu, Rajendra Rana Bhat, and Xiaolin Li. 2017. Adversarial Examples: Attacks and\nDefenses for Deep Learning. CoRR abs/1712.07107 (2017). arXiv:1712.07107 http://arxiv.org/abs/1712.07107\n, Vol. 1, No. 1, Article . Publication date: April 2019.\n40 Zhang et al.\n[155] Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015. Character-level Convolutional Networks for Text Classification.\nIn Proc. in Annual Conference on Neural Information Processing Systems 2015 (NIPS 2015). Montreal, Quebec,\nCanada, 649–657.\n[156] Zhengli Zhao, Dheeru Dua, and Sameer Singh. 2017. Generating natural adversarial examples. arXiv preprint\narXiv:1710.11342 (2017).\n[157] Yuke Zhu, Oliver Groth, Michael S. Bernstein, and Li Fei-Fei. 2016. Visual7W: Grounded Question Answering in\nImages. In Proc. of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2016). Las Vegas,\nNV, USA, 4995–5004.\n, Vol. 1, No. 1, Article . Publication date: April 2019.",
  "topic": "Adversarial system",
  "concepts": [
    {
      "name": "Adversarial system",
      "score": 0.916946291923523
    },
    {
      "name": "Popularity",
      "score": 0.8715133666992188
    },
    {
      "name": "Computer science",
      "score": 0.761437177658081
    },
    {
      "name": "Deep neural networks",
      "score": 0.7054319977760315
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6122902035713196
    },
    {
      "name": "Deep learning",
      "score": 0.4722059369087219
    },
    {
      "name": "Natural language",
      "score": 0.41717109084129333
    },
    {
      "name": "Data science",
      "score": 0.3828081786632538
    },
    {
      "name": "Machine learning",
      "score": 0.37098217010498047
    },
    {
      "name": "Natural language processing",
      "score": 0.35864895582199097
    },
    {
      "name": "Psychology",
      "score": 0.10246467590332031
    },
    {
      "name": "Social psychology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I5681781",
      "name": "The University of Adelaide",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I99043593",
      "name": "Macquarie University",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I37461747",
      "name": "Wuhan University",
      "country": "CN"
    }
  ],
  "cited_by": 257
}