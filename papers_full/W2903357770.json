{
  "title": "Improving Neural Language Models with Weight Norm Initialization and Regularization",
  "url": "https://openalex.org/W2903357770",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A5010730319",
      "name": "Christian Herold",
      "affiliations": [
        "RWTH Aachen University"
      ]
    },
    {
      "id": "https://openalex.org/A5046577673",
      "name": "Yingbo Gao",
      "affiliations": [
        "RWTH Aachen University"
      ]
    },
    {
      "id": "https://openalex.org/A5112501010",
      "name": "Hermann Ney",
      "affiliations": [
        "RWTH Aachen University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2284050935",
    "https://openalex.org/W2951672049",
    "https://openalex.org/W4300427683",
    "https://openalex.org/W2743945814",
    "https://openalex.org/W2963347649",
    "https://openalex.org/W2549416390",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W4299838440",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2758605572",
    "https://openalex.org/W1999965501",
    "https://openalex.org/W2553303224",
    "https://openalex.org/W2963374479",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W2963748792",
    "https://openalex.org/W2742448943",
    "https://openalex.org/W2963685250",
    "https://openalex.org/W2963266340",
    "https://openalex.org/W2963537482",
    "https://openalex.org/W2525332836",
    "https://openalex.org/W2265902746",
    "https://openalex.org/W1970689298",
    "https://openalex.org/W2086161653",
    "https://openalex.org/W2963938518",
    "https://openalex.org/W4294555862",
    "https://openalex.org/W2998704965",
    "https://openalex.org/W2962964385",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2251640092",
    "https://openalex.org/W2134237567",
    "https://openalex.org/W2951559648",
    "https://openalex.org/W2144513243",
    "https://openalex.org/W1938755728",
    "https://openalex.org/W2571859396",
    "https://openalex.org/W2757047188",
    "https://openalex.org/W2963983719",
    "https://openalex.org/W1591801644",
    "https://openalex.org/W2212703438",
    "https://openalex.org/W1934041838",
    "https://openalex.org/W2408727897"
  ],
  "abstract": "Embedding and projection matrices are commonly used in neural language models (NLM) as well as in other sequence processing networks that operate on large vocabularies. We examine such matrices in fine-tuned language models and observe that a NLM learns word vectors whose norms are related to the word frequencies. We show that by initializing the weight norms with scaled log word counts, together with other techniques, lower perplexities can be obtained in early epochs of training. We also introduce a weight norm regularization loss term, whose hyperparameters are tuned via a grid search. With this method, we are able to significantly improve perplexities on two word-level language modeling tasks (without dynamic evaluation): from 54.44 to 53.16 on Penn Treebank (PTB) and from 61.45 to 60.13 on WikiText-2 (WT2).",
  "full_text": "Proceedings of the Third Conference on Machine Translation (WMT), V olume 1: Research Papers, pages 93–100\nBelgium, Brussels, October 31 - Novermber 1, 2018.c⃝2018 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/W18-64010\nImproving Neural Language Models with Weight Norm Initialization and\nRegularization\nChristian Herold∗ Yingbo Gao∗\nHuman Language Technology and Pattern Recognition Group\nComputer Science Department\nRWTH Aachen University\nD-52056 Aachen, Germany\n<surname>@i6.informatik.rwth-aachen.de\nHermann Ney\nAbstract\nEmbedding and projection matrices are com-\nmonly used in neural language models (NLM)\nas well as in other sequence processing net-\nworks that operate on large vocabularies. We\nexamine such matrices in ﬁne-tuned language\nmodels and observe that a NLM learns word\nvectors whose norms are related to the word\nfrequencies. We show that by initializing the\nweight norms with scaled log word counts, to-\ngether with other techniques, lower perplexi-\nties can be obtained in early epochs of train-\ning. We also introduce a weight norm regular-\nization loss term, whose hyperparameters are\ntuned via a grid search. With this method, we\nare able to signiﬁcantly improve perplexities\non two word-level language modeling tasks\n(without dynamic evaluation): from 54.44 to\n53.16 on Penn Treebank (PTB) and from 61.45\nto 60.13 on WikiText-2 (WT2).\n1 Introduction\nA language model (LM) measures how likely a\ncertain sequence of words is for a given language.\nIt does so by calculating the probability of occur-\nrence of that sequence, which can be learned from\nmonolingual text data. Many models in machine\ntranslation and automatic speech recognition ben-\neﬁt from the use of a LM (Corazza et al., 1995;\nPeter et al., 2017).\nWhile count-based LMs (Katz, 1987; Kneser\nand Ney, 1995) provided the best results in\nthe past, substantial improvements were achieved\nwith the introduction of neural networks in the\nﬁeld of language modeling (Bengio et al., 2003).\nDifferent types of architectures such as feedfor-\nward neural networks (Schwenk, 2007) and re-\ncurrent neural networks (Mikolov et al., 2010)\nhave since been used for language modeling.\nCurrently, variants of long short-term memory\n∗Equal contribution. Ordering determined by coin ﬂipping.\n(LSTM) (Hochreiter and Schmidhuber, 1997) net-\nworks give the best results on popular language\nmodeling tasks (Yang et al., 2018).\nIn natural language processing, words are typ-\nically represented by high-dimensional one-hot\nvectors. To reduce dimensionality and to be able\nto learn relationships between words, they are\nmapped into a lower-dimensional, continuous em-\nbedding space. Mathematically, this is done by\nmultiplying the one-hot vector with the embed-\nding matrix. Similarly, to receive a probability dis-\ntribution over the vocabulary, a mapping from an\nembedding space is performed by a projection ma-\ntrix followed by a softmax operation. These two\nmatrices can be tied together in order to reduce\nthe number of parameters and improve the results\nof NLMs (Inan et al., 2017; Press and Wolf, 2017).\nSince the row vectors in the embedding and pro-\njection matrices are effectively word vectors in a\ncontinuous space, we investigate such weight vec-\ntors in well-trained and ﬁne-tuned NLMs. We ob-\nserve that the learned word vector generally has\na greater norm for a frequent word than an in-\nfrequent word. We then speciﬁcally examine the\nweight vector norm distribution and design ini-\ntialization and normalization strategies to improve\nNLMs.\nOur contribution is twofold:\n• We identify that word vectors learned by\nNLMs have a weight norm distribution that\nresembles logarithm of the word counts. We\nthen correspondingly develop a weight initial-\nization strategy to aid NLM training.\n• We design a weight norm regularization loss\nterm that increases the generalization ability\nof the model. Applying this loss term, we\nachieve state-of-the-art results on Penn Tree-\nbank (PTB) and WikiText-2 (WT2) language\nmodeling tasks.\n93\n2 Related Work\nMelis et al. (2018) investigated different NLM ar-\nchitectures and regularization methods with the\nuse of a black-box hyperparameter tuner. In par-\nticular, the LSTM architecture was compared to\ntwo more recent recurrent approaches, namely re-\ncurrent highway networks (Zilly et al., 2017) and\nneural architecture search (Zoph and Le, 2017).\nThey found that the standard LSTM architecture\noutperforms other models, if properly regularized.\nMerity et al. (2017a) used various regularization\nmethods such as activation regularization (Merity\net al., 2017b) in a LSTM model. They also intro-\nduced a variant of the averaged stochastic gradient\nmethod, where the averaging trigger is not tuned\nby the user but relies on a non-monotonic condi-\ntion instead. With these and further regulariza-\ntion and optimization methods, improved results\non PTB and WT2 were achieved.\nTo further improve this network architecture,\nYang et al. (2018) introduced the mixture of soft-\nmaxes (MoS) model, claiming that the calculation\nof the output probabilities with a single softmax\nlayer is a bottleneck. In their approach, several\noutput probabilities are calculated and then com-\nbined via a weighted sum. The LSTM-MoS ar-\nchitecture provides state-of-the-art results on PTB\nand WT2 at the time of writing and is used as the\nbaseline model for comparisons in this work.\nOther works proposed to tie the embedding and\nprojection matrices. Press and Wolf (2017) inves-\ntigated the effects of weight tying, analyzed up-\ndate rules after tying and showed that tied matri-\nces evolve in a similar way as the projection ma-\ntrix. Inan et al. (2017) were motivated by the fact\nthat with a classiﬁcation setup over the vocabulary,\ninter-word information is not utilized to its full po-\ntential. They also provided theoretical justiﬁcation\non why it is appropriate to tie the above-mentioned\nmatrices.\nBesides using the word embedding matrix, there\nare other approaches to represent word sequences.\nZhang et al. (2015) proposed a new embedding\nmethod called ﬁxed-sized ordinally-forgetting en-\ncoding (FOFE), which allows them to encode\nvariable-length sentences into ﬁxed-length vectors\nalmost uniquely.\nAdditionally, Salimans and Kingma (2016) in-\ntroduced a weight normalization reparametriza-\ntion trick on weight matrices, which separates the\nnorm and the angle of a vector. This can speed\nup the convergence of stochastic gradient descent\nand also allows for explicit scaling of gradients in\nthe amplitude and direction. They also discussed\nthe connections between weight normalization and\nbatch normalization.\nOn top of one-hot representations of words, Irie\net al. (2015) used additional information to rep-\nresent word sequences. It is shown that the use\nof long-context bag-of-words as additional feature\nfor language modeling can narrow the gap be-\ntween feed-forward NLMs and recurrent NLMs.\n3 Neural Language Modeling\nIn NLM the probability of a word sequence\nxt\n1 = x1x2...xt is decomposed as\nP(xt\n1) =\nt∏\nj=1\nP(xj|xj−1\nj−n+1) (1)\nso that the (n−1) preceding words xj−1\nj−n+1 are\nconsidered for the prediction of the next word xj.\nThis is typically done by using a recurrent neural\nnetwork, e.g. a stack of LSTM layers, to encode\nthe input sequence as\nht = LSTM(ET[xt−n+1,xt−n+2,...,x t−1]) (2)\nwhere ET is the transposed embedding ma-\ntrix, [xt−n+1,xt−n+2,...,x t−1] are the one-hot en-\ncoded preceding words and the LSTM() function\nreturns the last hidden state of the last LSTM layer.\nThe probability distribution over the next word xt\nis then calculated as\nP(xt = xk|ht) = exp(Wkht)∑V\nj=1 exp(Wjht)\n(3)\nwith V being the vocabulary size, k = 1,2,...,V ,\nand Wk being the k-th row vector in the projection\nmatrix W.\nFor training the neural network, the cross-entropy\nerror criterion, which is equivalent to the maxi-\nmum likelihood criterion, is used. For the i-th se-\nquence of words xti\n1 , the cross-entropy loss Li is\ndeﬁned as\nLi = −logP(xti = xyi|hti) (4)\nwith yi being the true label of xti. The total loss is\nthen calculated as\nL= 1\nN\nN∑\ni=1\nLi (5)\n94\nwhere N is the total number of sequences. A\nlanguage model is normally scored by perplexity\n(ppl). For a given test corpusxT\n1 = x1x2...xT, the\npplis calculated as\nppl= P(xT\n1 )−1\nT (6)\nwhich is a measurement on how likely a given sen-\ntence is, according to the prediction of the model.\nIn the above formulation, we have an embedding\nmatrix E and a projection matrix W. When the\ntwo matrices are tied and one-hot vectors are used\nto represent words, the rows of these matrices are\nthen the word vectors of the corresponding words.\nParticularly, we focus on the norms of the row vec-\ntors and study their relationship with word counts\nand how to regularize them.\n4 Weight Norm Initialization\nWe ﬁrst train models on PTB and WT2 as de-\nscribed in (Yang et al., 2018) and plot the norms\nof learned weight vectors of the embedding matrix\nin Figure 1.\nWhen the words are ranked by their counts and\nplaced on the x-axis from frequent to infrequent,\nit can be seen that the word vector norms follow\na downward trend as well. Log unigram counts\nare also plotted for comparison. As can be seen,\nthe norm distribution follows a similar trend as the\nlog counts. It is important to note, that the logit for\nword xk and context ht is calculated as Wkht (see\nEquation 3), which can be rewritten as\nWkht = ∥Wk∥∥ht∥cos(θ) (7)\nwhere θ denotes the angle between Wk and ht.\nTherefore, one intuition from the aforementioned\nobservation is that, for a frequent word, the net-\nwork tends to learn a weight vector Wk with a\ngreater norm to maximize likelihood. This mo-\ntivates our approach to initialize the weight norms\nwith scaled log counts rather than uniformly ran-\ndom values in a speciﬁc range.\nBecause we wish to initialize the weight norms\nexplicitly with scaled logarithm of the word\ncounts, it is helpful to look at a weight vector’s\nmagnitude and direction separately. For this pur-\npose, we use a reparameterization technique on\nthe weight vectors as described in (Salimans and\nKingma, 2016):\nWk = gk\nvk\n∥vk∥2\n(8)\n(a) Penn Treebank\n(b) WikiText-2\nFigure 1 : Word vector norms of ﬁne-tuned MoS\nmodels (Yang et al., 2018), trained on (a) Penn\nTreebank and (b) WikiText-2. Words are ranked\nby their counts in a descending order and thus fre-\nquent words are to the left. Actual logarithm of\nword counts are plotted in black, and word vec-\ntor norms are grey. We observe that word vector\nnorms loosely follow the trend of log counts.\nwhere k = 1,2,...,V , gk = ∥Wk∥2, and vk is\na vector proportional to Wk. Reparameterizing\nthe weight vectors makes it easy to implement the\nweight norm initialization as\ngk = σlogck (9)\nwhere ck denotes unigram word count for word k\nand σ is a scalar applied to the log counts. We\nsample each component of vk from a continuous\nuniform distribution in [−r,r], where r is a hy-\nperparameter, specifying the initialization range.\nWith this, no constraint on the weight vector di-\nrection is imposed during initialization.\nAdditionally, we adopt an adaptive gradient\nstrategy which regularizes the gradients in gk. As\nin\n(∂L\n∂g)\n′\n=\n{\n[1 −(1 −γ) t\nτ]∂L\n∂g, for t≤τ\nγ∂L\n∂g, for t>τ (10)\nwhen epoch tis no greater than a speciﬁed epoch\n95\nTokens V ocab Size\nPenn Treebank\nTrain 888k\n10kValid 70k\nTest 79k\nWikiText-2\nTrain 2.1M\n33kValid 214k\nTest 241k\nTable 1 : Statistics of the Penn Treebank and\nWikiText-2 datasets.\nτ, ( ∂L\n∂gk\n)\n′\n— the regularized gradient in gk, linearly\ndecays to γ(γ ≤1) times the unregularized gradi-\nent ∂L\n∂gk\n. Otherwise, we directly use the discounted\ngradient. In analogy to learning rate decay, this\nadaptive gradient strategy anneals the word vector\nnorm updates in each step. The intuition for such\na strategy is that after a certain amount of epochs,\nthe weight norms should not change so drastically\nfrom the initialized scaled log counts.\n5 Weight Norm Regularization\nWeight regularization (WR) is a well established\nmethod to combat overﬁtting in neural networks,\nwhich is especially important on smaller datasets\n(Krogh and Hertz, 1992). The idea is to push\nweights in the network to zero, where gradients are\nnot signiﬁcant. Typically, WR is implemented by\nadding an extra term to the loss functionL0, which\npenalizes the norm of all weights in the network.\nFor example, L2-regularization is implemented as\nL= L0 + λ\n2\n∑\nw\n(∥w∥2)2 (11)\nwith the sum going over all weights w in the\nnetwork and λ being the regularization strength.\nHowever, this method is not perfect, as it affects\nevery weight in the network equally and may lead\nto hidden units’ weights getting stuck near zero.\nIn this work we add a constraint speciﬁcally\non the embedding and projection matrices, whose\nweights are shared. Since the row vectors in both\nmatrices are word vectors, it seems appropriate to\nput constraints explicitly on their norms instead of\non each individual weight parameter in the matri-\nces.\nWe propose to add a regularization term to the\nstandard loss function L0 in the form of\nLwr = L0 + ρ\n√\nV∑\nj=1\n(∥Wj∥2 −ν)2 (12)\nwhere ν, ρ≥0 are two scalars and Wj is the j-th\nrow vector of the projection matrix W. The L 2-\nnorms of the row vectors are pushed towards ν,\nwhile ρ is the regularization strength. This will\npunish the row vectors for adopting norms other\nthan ν, in the hope of reducing the effect of over-\nﬁtting on the training data.\nThe choice of a soft regularization loss term in-\nstead of hard-ﬁxing the weight norms in the for-\nward pass is motivated by the weight norm dis-\ntribution shown in Figure 1. It can be seen that\nNLMs tend to learn non-equal weight norms for\nwords with different counts. Therefore, hard-\nﬁxing weight norms may limit the network’s abil-\nity to learn.\n6 Experiments\n6.1 Experiment Setup\nThe experiments are conducted on two popular\nlanguage modeling datasets. The number of to-\nkens and size of vocabulary for each dataset are\nsummarized in Table 1.\nepoch Penn Treebank WikiText-2\nwni ppl baseline ppl pplreduction (%) wni ppl baseline ppl pplreduction (%)\n1 162.18 180.72 10.26 172.19 192.19 10.41\n10 85.92 92.09 6.70 95.90 100.72 4.79\n20 73.36 78.94 7.07 85.14 88.21 3.48\n30 71.44 73.06 2.22 81.80 82.70 1.09\n40 69.27 70.20 1.32 79.28 80.32 1.29\nTable 2: Perplexity ( ppl) improvement using weight norm initialization (wni) in early epochs on Penn\nTreebank and WikiText-2. pplreduction is around 10% after the ﬁrst epoch on both tasks, and decays\nto approximately 1% after 40 epochs. The wni model has slightly higher perplexities than the baseline\nmodel from around 50 epochs onward.\n96\n0 10 −4 10 −3 10 −2 10 −1 1\nρ\n53.0\n53.5\n54.0\n54.5\n55.0\n55.5Perplexity\nν = 0.0\nν = 1.0\nν = 2.0\nν = 64.0\nFigure 2: Model perplexity on the Penn Treebank\ntest set as a function of ρ. The different sym-\nbols denote different values of ν. Models not de-\npicted yield higher perplexity values. The doted\nline marks the baseline result (with ρ= 0) as re-\nported by Yang et al. (2018).\nThe smaller one is the PTB corpus with prepro-\ncessing from Mikolov et al. (2010), which has a\ncomparatively small vocabulary size of 10k. With\na smaller number of sentences, this dataset is a\ngood choice for performing optimization of hyper-\nparameters. The second corpus WT2, which was\nintroduced by Merity et al. (2016), has over three\ntimes the vocabulary size of PTB.\nWe use the network structure introduced by\nYang et al. (2018) with the same hyper-parameter\nvalues to ensure comparability. Several regular-\nization techniques are used in this setup, such as\ndropout and weight decay. Furthermore, the em-\nbedding and projection matrices are tied by de-\nfault. For optimization, we adopt the same strat-\negy as described in (Merity et al., 2017a). That\nis, a conservative non-monotonic criterion is used\nto switch from stochastic gradient descent (SGD)\nto averaged stochastic gradient descent (ASGD)\n(Polyak and Juditsky, 1992). For more details of\nthe network structure refer to (Yang et al., 2018).\n6.2 Weight Norm Initialization\nWe tune the hyperparameter σ and use a value of\nσ = 0.5 to scale the logarithm of word counts.\nInitialization range r is set to 0.1 for both the\nreparametrized direction vectors and the baseline\nword vectors. Empirically, we set γ = 0.1 and\nτ = 100 for the adaptive gradient method. Per-\n0 1 2 3 4 5\nvector norm\n0\n5\n10\n15probability [%]\nMoS\nMoS with WR\n(a) Penn Treebank\n0 1 2 3 4 5\nvector norm\n0\n5\n10\n15\n20\n25probability [%]\nMoS\nMoS with WR\n(b) WikiText-2\nFigure 3: Weight norm distributions of the projec-\ntion matrices’ row vectors for the AWD-LSTM-\nMoS model from Yang et al. (2018) as well as\nfor our regularized version (WR). The models are\ntrained on the (a) Penn Treebank corpus and (b)\nWikiText-2 corpus with the resulting test perplex-\nities shown in Table 3 and Table 4 respectively.\nplexities on both PTB and WT2 in early epochs,\nas well as the relative perplexity improvement over\nbaseline models are summarized in Table 2.\nFirst, we notice signiﬁcant improvement after\nthe ﬁrst epoch of training using weight norm ini-\ntialization. About 10% of perplexity reduction is\nachieved on both datasets. This could be bene-\nﬁcial, when one wants to train on large datasets\nand/or can only train for a limited number of\nepochs. Second, the perplexity improvements de-\ncay down to around 1% after 40 epochs. This is\nin agreement with our expectation, because apart\nfrom reduced gradient in gk, a weight norm ini-\ntialized model is not fundamentally different from\nthe baseline model and no major difference should\nbe seen if we train for long enough. It is impor-\ntant to note that with only weight norm initializa-\ntion, both models eventually converge to perplex-\nities that are slightly worse than the baseline. We\nalso notice that the epochs, after which the opti-\nmizer is switched from SGD to ASGD, are differ-\nent in weight norm initialized models and baseline\nmodels.\n97\nModel #Params Validation Test\nMikolov and Zweig (2012) - RNN-LDA + KN + cache 9M - 92.0\nZaremba et al. (2014) - LSTM 20M 86.2 82.7\nGal and Ghahramani (2016) - Variational LSTM (MC) 20M - 78.6\nKim et al. (2016) - CharCNN 19M - 78.9\nMerity et al. (2016) - Pointer Sentinel-LSTM 21M 72.4 70.9\nGrave et al. (2017) - LSTM + continuous cache pointer† - - 72.1\nInan et al. (2017) - Tied Variational LSTM + augmented loss 24M 75.7 73.2\nZilly et al. (2017) - Variational RHN 24M 75.7 73.2\nZoph and Le (2017) - NAS Cell 25M - 64.0\nMelis et al. (2018) - 2-layer skip connection LSTM 24M 60.9 58.3\nMerity et al. (2017a) - AWD-LSTM 24M 60.0 57.3\nYang et al. (2018) - AWD-LSTM-MoS 22M 56.54 54.44\nOurs - AWD-LSTM-MoS with weight norm regularization 22M 55.03 53.16\nTable 3: Single model perplexity on the Penn Treebank test and validation sets. Baseline results are\nobtained from (Yang et al., 2018). †indicates the use of dynamic evaluation.\nModel #Params Validation Test\nInan et al. (2017) - Variational LSTM + augmented loss 28M 91.5 87.0\nGrave et al. (2017) - LSTM + continuous cache pointer† - - 68.9\nMelis et al. (2018) - 2-layer skip connection LSTM 24M 69.1 65.9\nMerity et al. (2017a) - AWD-LSTM 33M 69.1 66.0\nYang et al. (2018) - AWD-LSTM-MoS 35M 63.88 61.45\nOurs - AWD-LSTM-MoS with weight norm regularization 35M 62.67 60.13\nTable 4: Single model perplexity on the WikiText-2 test and validation sets. Baseline results are obtained\nfrom (Yang et al., 2018). †indicates the use of dynamic evaluation.\n6.3 Weight Norm Regularization\nIn order to tune the hyperparametersρand νintro-\nduced in Section 5, we perform a grid search over\nthe PTB dataset, the results of which are shown\nin Figure 2. If the norm constraint ν becomes too\nlarge, perplexity worsens signiﬁcantly, as seen in\nthe case of ν = 64. A model with a ν-value of\n2 provides the best result in most cases. We hy-\npothesize that a value of νthat is too small results\nin the logit being close to zero as shown in Equa-\ntion 7. For the regularization strength ρ, we recog-\nnize that ρ= 10−3 gives the best result on the PTB\ntest data. Larger or smaller values can hurt the\nperformance of the system, depending also on the\nvalue of ν. It should be noted that the optimized\nvalue of ρ is signiﬁcantly larger than the scaling\nswd of the weight decay term, which was opti-\nmized to be 1.2×10−6 by Merity et al. (2017a).\nThe resulting weight norm distributions of the\nprojection matrices’ row vectors are shown in Fig-\nure 3a and Figure 3b for models trained on PTB\nand WT2 respectively. Our efforts of pushing the\nnorms to a value of ν = 2.0 resulted in a notice-\nably smaller average norm, as well as in a overall\nmore narrow distribution.\nWith the tuned parameter values ρ = 10−3 and\nν = 2.0 we improve the previous state-of-the-art\nresult by 1.28 pplon PTB and by 1.32pplon WT2\n(without considering dynamic evaluation (Krause\net al., 2018), see Table 3 and Table 4). This is\nachieved without increasing the number of train-\nable parameters in the network or slowing down\nthe training process.\n7 Conclusion\nWord embedding matrix and output projection ma-\ntrix are important components in LSTM-based\nLMs. They are also widely used in other NLP\nmodels where one-hot vectors of words need to\nbe mapped into lower dimensional space. Given\nthe one-hot nature of word representations, row\nvectors in such matrices are then the correspond-\n98\ning word vectors. We study speciﬁcally the norms\nof these learned word vectors, the distribution of\nthe norms, and the relationship with word counts.\nWe show that with a simple initialization strat-\negy together with a reparametrization technique,\nit is possible to get signiﬁcantly lower perplex-\nity in early epochs during training. By using a\nweight norm regularization loss term, we are able\nto obtain signiﬁcant improvements on standard\nlanguage modeling tasks — 2.4% pplreduction on\nPTB and 2.1% on WT2.\nWe propose three directions to investigate fur-\nther. First, in this work we use scaled logarithm of\nword counts to initialize the weight norms. It is a\nlogical next step to use smoothing techniques on\nthe word counts and study the effects of such ini-\ntializations. Second, we currently apply the same\nnorm constraint on different words. Altering the\nloss function and regularizing the weight norms to\nword counts (and smoothed word counts) is worth\nexamining as well. Finally, our focus so far is on\nweight norms. It is a more exciting and challeng-\ning task to study the pairwise inner products, and\nsingle out the effects of angular differences.\nWe also plan to expand our regularization and\ninitialization techniques to the ﬁeld of neural ma-\nchine translation. Embedding and projection ma-\ntrices are also present in neural machine trans-\nlation networks, which could potentially beneﬁt\nfrom our methods as well. It seems natural to use\nour methods on the transformer architecture intro-\nduced by Vaswani et al. (2017), in which the em-\nbedding matrices at source and target sides, plus\nthe projection matrix, are three-way tied.\nAcknowledgments\nThis work has received\nfunding from the Euro-\npean Research Council\n(ERC) (under the Euro-\npean Union’s Horizon\n2020 research and in-\nnovation programme,\ngrant agreement No\n694537, project ”SEQCLAS”) and the Deutsche\nForschungsgemeinschaft (DFG; grant agreement\nNE 572/8-1, project ”CoreTec”). The GPU com-\nputing cluster was supported by DFG (Deutsche\nForschungsgemeinschaft) under grant INST\n222/1168-1 FUGG.\nThe work reﬂects only the authors’ views and\nnone of the funding agencies is responsible for any\nuse that may be made of the information it con-\ntains.\nReferences\nYoshua Bengio, R´ejean Ducharme, Pascal Vincent, and\nChristian Jauvin. 2003. A neural probabilistic lan-\nguage model. In Journal of machine learning re-\nsearch, volume 3, pages 1137–1155.\nA. Corazza, R. De Mori, R. Gretter, R. Kuhn, and\nG. Satta. 1995. Language models for automatic\nspeech recognition. In Speech Recognition and Cod-\ning, pages 157–173. Springer.\nYarin Gal and Zoubin Ghahramani. 2016. A theoret-\nically grounded application of dropout in recurrent\nneural networks. In Advances in neural information\nprocessing systems, pages 1019–1027.\nEdouard Grave, Armand Joulin, and Nicolas Usunier.\n2017. Improving neural language models with a\ncontinuous cache. In International Conference on\nLearning Representations.\nSepp Hochreiter and J¨urgen Schmidhuber. 1997. Long\nshort-term memory. Neural computation, 9:1735–\n80.\nHakan Inan, Khashayar Khosravi, and Richard Socher.\n2017. Tying word vectors and word classiﬁers: A\nloss framework for language modeling. In Interna-\ntional Conference on Learning Representations.\nKazuki Irie, Ralf Schl ¨uter, and Hermann Ney. 2015.\nBag-of-words input for long history representation\nin neural network-based language models for speech\nrecognition. In Sixteenth Annual Conference of the\nInternational Speech Communication Association.\nSlava Katz. 1987. Estimation of probabilities from\nsparse data for the language model component of a\nspeech recognizer. In IEEE transactions on acous-\ntics, speech, and signal processing.\nYoon Kim, Yacine Jernite, David Sontag, and Alexan-\nder M Rush. 2016. Character-aware neural language\nmodels. In AAAI, pages 2741–2749.\nReinhard Kneser and Hermann Ney. 1995. Improved\nbacking-off for m-gram language modeling. In 1995\nInternational Conference on Acoustics, Speech, and\nSignal Processing, pages 181–184.\nBen Krause, Emmanuel Kahembwe, Iain Murray, and\nSteve Renals. 2018. Dynamic evaluation of neural\nsequence models. In Proceedings of the 35th In-\nternational Conference on Machine Learning, pages\n2766–2775.\nAnders Krogh and John A. Hertz. 1992. A sim-\nple weight decay can improve generalization. In\nAdvances in neural information processing systems,\npages 950–957.\n99\nG´abor Melis, Chris Dyer, and Phil Blunsom. 2018. On\nthe state of the art of evaluation in neural language\nmodels. In International Conference on Learning\nRepresentations.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2017a. Regularizing and optimizing lstm\nlanguage models. arXiv preprint arXiv:1708.02182.\nStephen Merity, Bryan McCann, and Richard Socher.\n2017b. Revisiting activation regularization for lan-\nguage rnns. arXiv preprint arXiv:1708.01009.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer sentinel mixture mod-\nels. arXiv preprint arXiv:1609.07843.\nTomas Mikolov, Martin Karaﬁ ´at, Luk ´aˇs Burget, Jan\nˇCernock`y, and Sanjeev Khudanpur. 2010. Recurrent\nneural network based language model. In 11th An-\nnual Conference of the International Speech Commu-\nnication Association.\nTomas Mikolov and Geoffrey Zweig. 2012. Context\ndependent recurrent neural network language model.\nIn IEEE Spoken Language Technology Workshop,\npages 234–239.\nJan-Thorsten Peter, Andreas Guta, Tamer Alkhouli,\nParnia Bahar, Jan Rosendahl, Nick Rossenbach,\nMiguel Grac ¸a, and Hermann Ney. 2017. The RWTH\nAachen University english-german and german-\nenglish machine translation system for WMT 2017.\nIn Proceedings of the Second Conference on Machine\nTranslation, pages 358–365.\nBoris T. Polyak and Anatoli B. Juditsky. 1992. Ac-\nceleration of stochastic approximation by averaging.\nIn SIAM Journal on Control and Optimization, pages\n838–855.\nOﬁr Press and Lior Wolf. 2017. Using the output em-\nbedding to improve language models. In Proceed-\nings of the 15th Conference of the European Chap-\nter of the Association for Computational Linguistics,\npages 157–163.\nTim Salimans and Diederik P Kingma. 2016. Weight\nnormalization: A simple reparameterization to ac-\ncelerate training of deep neural networks. In Ad-\nvances in Neural Information Processing Systems,\npages 901–909.\nHolger Schwenk. 2007. Continuous space language\nmodels. Computer Speech & Language, 21(3):492–\n518.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 5998–6008.\nZhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and\nWilliam W. Cohen. 2018. Breaking the softmax bot-\ntleneck: A high-rank rnn language model. In Inter-\nnational Conference on Learning Representations.\nWojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.\n2014. Recurrent neural network regularization.\narXiv preprint arXiv:1409.2329.\nShiliang Zhang, Hui Jiang, Mingbin Xu, Junfeng Hou,\nand Lirong Dai. 2015. The ﬁxed-size ordinally-\nforgetting encoding method for neural network lan-\nguage models. In Proceedings of the 53rd Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 495–500.\nJulian Georg Zilly, Rupesh Kumar Srivastava, Jan\nKoutn´ık, and J¨urgen Schmidhuber. 2017. Recurrent\nhighway networks. In Proceedings of the 34th In-\nternational Conference on Machine Learning, pages\n4189–4198.\nBarret Zoph and Quoc V Le. 2017. Neural architecture\nsearch with reinforcement learning. In International\nConference on Learning Representations.\n100",
  "topic": "Treebank",
  "concepts": [
    {
      "name": "Treebank",
      "score": 0.9038834571838379
    },
    {
      "name": "Computer science",
      "score": 0.6995954513549805
    },
    {
      "name": "Initialization",
      "score": 0.6755058765411377
    },
    {
      "name": "Language model",
      "score": 0.6563330888748169
    },
    {
      "name": "Regularization (linguistics)",
      "score": 0.5526201128959656
    },
    {
      "name": "Hyperparameter",
      "score": 0.5261449813842773
    },
    {
      "name": "Artificial intelligence",
      "score": 0.49375662207603455
    },
    {
      "name": "Artificial neural network",
      "score": 0.477792352437973
    },
    {
      "name": "Norm (philosophy)",
      "score": 0.45342350006103516
    },
    {
      "name": "Word (group theory)",
      "score": 0.4437016546726227
    },
    {
      "name": "Natural language processing",
      "score": 0.4335739016532898
    },
    {
      "name": "Speech recognition",
      "score": 0.40422284603118896
    },
    {
      "name": "Algorithm",
      "score": 0.3830574154853821
    },
    {
      "name": "Mathematics",
      "score": 0.22941482067108154
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Dependency (UML)",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I887968799",
      "name": "RWTH Aachen University",
      "country": "DE"
    }
  ]
}