{
    "title": "Informative Language Encoding by Variational Autoencoders Using Transformer",
    "url": "https://openalex.org/W4291014764",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5052886761",
            "name": "Changwon Ok",
            "affiliations": [
                "Korea Telecom (South Korea)"
            ]
        },
        {
            "id": "https://openalex.org/A5042299025",
            "name": "Geonseok Lee",
            "affiliations": [
                "Hanyang University"
            ]
        },
        {
            "id": "https://openalex.org/A5064844412",
            "name": "Kichun Lee",
            "affiliations": [
                "Hanyang University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2963223306",
        "https://openalex.org/W2963858765",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2978613765",
        "https://openalex.org/W3035475181",
        "https://openalex.org/W6741832134",
        "https://openalex.org/W2924334974",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2964669873",
        "https://openalex.org/W3191529106",
        "https://openalex.org/W3167280680",
        "https://openalex.org/W6636649193",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W2885185669",
        "https://openalex.org/W2518108298",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W1632114991",
        "https://openalex.org/W2739748921"
    ],
    "abstract": "In natural language processing (NLP), Transformer is widely used and has reached the state-of-the-art level in numerous NLP tasks such as language modeling, summarization, and classification. Moreover, a variational autoencoder (VAE) is an efficient generative model in representation learning, combining deep learning with statistical inference in encoded representations. However, the use of VAE in natural language processing often brings forth practical difficulties such as a posterior collapse, also known as Kullbackâ€“Leibler (KL) vanishing. To mitigate this problem, while taking advantage of the parallelization of language data processing, we propose a new language representation model as the integration of two seemingly different deep learning models, which is a Transformer model solely coupled with a variational autoencoder. We compare the proposed model with previous works, such as a VAE connected with a recurrent neural network (RNN). Our experiments with four real-life datasets show that implementation with KL annealing mitigates posterior collapses. The results also show that the proposed Transformer model outperforms RNN-based models in reconstruction and representation learning, and that the encoded representations of the proposed model are more informative than other tested models.",
    "full_text": null
}