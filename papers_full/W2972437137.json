{
    "title": "Transformer Based Grapheme-to-Phoneme Conversion",
    "url": "https://openalex.org/W2972437137",
    "year": 2019,
    "authors": [
        {
            "id": "https://openalex.org/A2805899602",
            "name": "Sevinj Yolchuyeva",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2132453387",
            "name": "GÃ©za NÃ©meth",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4259535419",
            "name": "BÃ¡lint Gyires-TÃ³th",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W1647671624",
        "https://openalex.org/W1916501714",
        "https://openalex.org/W2748157908",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2809456172",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W2538246917",
        "https://openalex.org/W2090755665",
        "https://openalex.org/W2608712415",
        "https://openalex.org/W2963299674",
        "https://openalex.org/W2972903407",
        "https://openalex.org/W2963913768",
        "https://openalex.org/W2963499433",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2964281804",
        "https://openalex.org/W67332896",
        "https://openalex.org/W2800782462",
        "https://openalex.org/W2095705004",
        "https://openalex.org/W2884873108",
        "https://openalex.org/W2920834369",
        "https://openalex.org/W1779680350",
        "https://openalex.org/W2552839021",
        "https://openalex.org/W2889358390",
        "https://openalex.org/W2517101095",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W312219375",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2591927543",
        "https://openalex.org/W2163605009",
        "https://openalex.org/W2963609956"
    ],
    "abstract": "Attention mechanism is one of the most successful techniques in deep learning\\nbased Natural Language Processing (NLP). The transformer network architecture\\nis completely based on attention mechanisms, and it outperforms\\nsequence-to-sequence models in neural machine translation without recurrent and\\nconvolutional layers. Grapheme-to-phoneme (G2P) conversion is a task of\\nconverting letters (grapheme sequence) to their pronunciations (phoneme\\nsequence). It plays a significant role in text-to-speech (TTS) and automatic\\nspeech recognition (ASR) systems. In this paper, we investigate the application\\nof transformer architecture to G2P conversion and compare its performance with\\nrecurrent and convolutional neural network based approaches. Phoneme and word\\nerror rates are evaluated on the CMUDict dataset for US English and the NetTalk\\ndataset. The results show that transformer based G2P outperforms the\\nconvolutional-based approach in terms of word error rate and our results\\nsignificantly exceeded previous recurrent approaches (without attention)\\nregarding word and phoneme error rates on both datasets. Furthermore, the size\\nof the proposed model is much smaller than the size of the previous approaches.\\n",
    "full_text": "Transformer based Grapheme-to-Phoneme Conversion \nSevinj Yolchuyeva1, GÃ©za NÃ©meth2, BaÌlint Gyires-ToÌth3 \n1,2,3Department of Telecommunications and Media Informatics, Budapest University of Technology   \nand Economics, Budapest, Hungary \n{syolchuyeva, nemeth, toth.b}@tmit.bme.hu \n \nAbstract \nAttention mechanism is one of the most successful \ntechniques in deep learning based Natural Language \nProcessing ( NLP). The transformer network architecture is \ncompletely based on attention mechanisms, and it outperforms \nsequence-to-sequence models in neural machine translation \nwithout recurrent and convolutional layers. Grapheme -to-\nphoneme (G2P) conversion is a task of converting letters \n(grapheme sequence) to t heir pronunciations (phoneme \nsequence). It plays a significant role in text -to-speech (TTS) \nand automatic speech recognition (ASR) systems. In this \npaper, we investigate the application of t ransformer \narchitecture to G2P conversion and compare its performa nce \nwith recurrent and convolutional neural network based \napproaches. Phoneme and word error rates are evaluated on \nthe CMUDict dataset for US English and the NetTalk dataset. \nThe results show that transformer based G2P outperforms the \nconvolutional-based approach in terms of word error rate and \nour results significantly exceeded previous recurrent \napproaches (without attention) regarding word and phoneme \nerror rates on both datasets. Furthermore, the si ze of the \nproposed model is much smaller than the size of the previous \napproaches. \nIndex Terms: Attention mechanism; Grapheme -to-Phoneme \n(G2P); Transformer architecture; Multi-head attention \n1. Introduction \n        Grapheme-to-phoneme conversion is an important \ncomponent in TTS and ASR systems [1]. Many approach es \nhave been proposed: the early solutions were rule -based [2], \nwhile in later works, joint sequence models for G2P \nconversion were introduced [3,  4]. The latter requires \nalignment between graphemes and phonemes, and it calculates \na joint n -gram language m odel over sequences. The method \nproposed by [3] is implemented in the publicly available tool, \ncalled Sequitur.  \nEncoder-decoder architectures were applied in various tasks, \nsuch as neural machine translation, speech recognition, text-\nto-speech synthesis [1,5,6]. When combined with different \nattention mechanisms , it achieved state -of-the-art results in \ndifferent fields. This combination was investigated by [7] for \nthe G2P task and resulted in state-of-the-art G2P performance \nwithout explicit alignments, the phoneme error rate (PER) \nbeing 4.69% and the word error rate (WER) reaching 20.24% \non CMUDict. In [1], an end -to-end TTS system (constructed \nentirely from deep neural networks) utilized an encoder -\ndecoder model for the G2P task b y using the multi -layer \nbidirectional encoder with GRU (Gated Recurrent Unit) and a \ndeep unidirectional GRU decoder. \nConvolutional neural networks have achieved superior \nperformance compared to previous methods in large -scale \nimage recognition [8]. Recently, encoder -decoder \narchitectures using convolutional neural networks have been \nstudied and applied to various Natural Language Processing \n(NLP) tasks [ 9, 10]. Convolutional neural network -based \nsequence-to-sequence architecture for G2P was introduced in \n[11]. This approach  achieved a 4.81% phoneme error rate \n(PER) and 25.13% word error rate (WER) on CMUDict; \n5.69% PER and 30.10% WER on NetTalk. The proposed \nmodel is based on convolutional layers with residual \nconnections as an encoder and a Bi-LSTM decoder. \nIn sequence -to-sequence learning, the decoding stage is \nusually carried out sequentially, one step at a time from left to \nright and the outputs from the previous steps are used as \ndecoder inputs [12]. Sequential decoding can negatively \ninfluence the results, depending on the task and the model. The \nnon-sequential greedy decoding (NSGD) method for G2P was \nstudied in [12], and it was also combined with a fully \nconvolutional encoder -decoder architecture. That model \nachieved 5.58% phoneme and 24.10% wor d error rates on \nCMUDict, which included multiple pronunciations and \nwithout stress labels. \nMultilingual G2P models are used for multilingual speech \nsynthesis [26]. In [13] monolingual G2P (MoG2P) and \nmultilingual G2P (MuG2P) conversions were proposed, and \nexperiments were conducted in four languages (Japanese, \nKorean, Thai, and Chinese) with both language-dependent and \n-independent trainings. Moreover, a neural sequence -to-\nsequence approach to G2P was presented, which is trained on \nspellingâ€“pronunciation pairs in hundreds of languages [14,  \n23]. The proposed system shared a single encoder and decoder \nacross all languages, allowing it to utilize the intrinsic \nsimilarities between different writing systems. \nTransformer networks are based on an encoder -decoder \narchitecture and account the representations of their input and \noutput without using recurrent or convolutional neural \nnetworks (CNN) [15,  16]. First, transformer networks were \nused for neural machine translation, and they achieved state -\nof-the-art performance on various datasets. In [15], it was \nshown that transformers could be trained significantly faster \nthan recurrent or convolutional architectures for machine \ntranslation tasks.  \nAccording to our knowledge, our approach is the first study \nthat applies the transformer for G2P conversion. In this paper, \nwe present transformers with different structures and analyse \ntheir advantages and disadvantages for G2P task. Our main \ngoal was to achieve and surpass (if possible) the accuracy of \nprevious models and to reduce  the required resources of \ntraining. \nThe rest of the paper is organized as follows: Section 2 \ndescribes the transformer architecture for the G2P conversion \ntask. Datasets, training processes, the evaluation of the \nproposed models are presented in Section 3 . Evaluation and \nresults are described in Section 4, and finally, conclusions are \ndrawn in Section 5. \n2. Proposed architecture \n         Encoder-decoder based sequence to sequence learning \n(seq2seq) has made remarkable progress in recent years. The \nmain idea of these approaches has two main stages: first, the \nencoder converts the input sequence to a vector; second, the \noutput sequence is generated based on the learned vector \nrepresentation by using the decoder. For both encoder and \ndecoder, different network ar chitectures have been \ninvestigated [5, 18]. \nThe transformer is organized by stacked self -attention and \nfully connected layers for both the encoder and the decoder \n[15], as shown in the left and right halves of Figure 1, \nrespectively. Self-attention, sometimes called intra -attention, \nis an attention mechanism relating different positions of a \nsingle sequence to compute its internal representation. \n \n \nFigure 1: The framework of the proposed model. \n \nWithout using any recurrent layer, positional encoding is \nadded to the input and output embeddings [16]. The positional \ninformation provides the transformer network with the order \nof input and output sequences. \nThe encoder is composed of a stack of ğ‘ identical blocks, and \neach block has two layers. The first is the multi-head attention \nlayer, which is several attention layers used in parallel. The \nsecond is a fully connected position -wise feed forward layer. \nThese layers are followed by dropout and normalization layers \n[24]. The decoder is composed of a stack of ğ‘ identical blocks, \nand each block has three layers. The first layer is the multi -\nhead attention mechanism with masked [17]. This mechanism \nhelps the model to generate the current phoneme using only \nthe previous phonemes. The second layer is a  multi-head \nattention layer without the masked. It performs the multi-head \nattention over the output of the first layer. The third layer is \nfully connected. These layers are followed by normalization \n[24] and dropout layers [25]. At the top, there is the final fully \nconnected layer with linear activation which is followed by \nsoftmax output. \nAn attention function was described as mapping a query and a \nset of key-value pairs to an output, where the query (Q), keys \n(K), values (V), and output are all vectors [ 15]. A multi-head \nattention mechanism builds upon scaled dot-product attention, \n \n \n1 http://www.speech.cs.cmu.edu/cgi-bin/cmudict \n2 https://keras.io/ \nwhich computes on a query Q, key K and a value V (the \ndimension of queries and keys is ğ‘‘ğ‘˜ and values of dimension \nis ğ‘‘ğ‘£): \n \n          ğ´ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘›(ğ‘„, ğ¾, ğ‘‰) = ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ (\nğ‘„ğ¾ğ‘‡\nâˆšğ‘‘ğ‘˜\n) ğ‘‰              (1) \n \nwhere the scalar 1/âˆšğ‘‘ğ‘˜ is used to prevent softmax function \ninto regions that have very small gradients. \nInstead of performing a single attention function multi -head \nattention obtains â„ (parallel attention layers or heads) for \nlearning different representations, compute scaled dot-product \nattention for each representation, concatenate the results, and \nproject the concatenation with a feedforward layer. Finally, \nğ‘‘ğ‘š  dimensional outputs are obtained. The multi -head \nattention is shown as follows [15]: \n \nâ„ğ‘– = ğ´ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘›(ğ‘„ğ‘Šğ‘–\nğ‘„, ğ¾ğ‘Šğ‘–\nğ¾, ğ‘‰ğ‘Šğ‘–\nğ‘‰)                               (2) \n \nğ‘€ğ‘¢ğ‘™ğ‘¡ğ‘–ğ»ğ‘’ğ‘ğ‘‘(ğ‘„, ğ¾, ğ‘‰) = ğ¶ğ‘œğ‘›ğ‘ğ‘ğ‘¡(â„1, â„2, . . , â„â„)ğ‘Šğ‘‚      (3) \n \nwhere the projections are parameter matrices \nğ‘Šğ‘–\nğ‘„ğœ– ğ‘…ğ‘‘ğ‘šÃ—ğ‘‘ğ‘˜ , ğ‘Šğ‘–\nğ¾ğœ– ğ‘…ğ‘‘ğ‘šÃ—ğ‘‘ğ‘˜ , ğ‘Šğ‘–\nğ‘‰ğœ– ğ‘…ğ‘‘ğ‘šÃ—ğ‘‘ğ‘£  and ğ‘Šğ‘‚ğœ– ğ‘…â„ğ‘‘ğ‘£Ã—ğ‘‘ğ‘š . \nEach head in multi -head attention learns individual sequence \ndependency, and this allows the model to attend to information \nfrom different representation subspaces. So, it increases the \npower of the attention with no computational overhead. \n3. Experiments  \nIn this section, we introduce the datasets and then describe the \nimplementation details. \n3.1.   Datasets \nFor evaluation, the CMU pronunciation 1 and NetTalk  \ndatasets were used. T hese datasets have been frequently \nchosen by researchers [7, 18]. The train and test split was the \nsame as found in [7,  18, 22], thus , the results are directly \ncomparable. CMUDict contains  a 106,837 -word training set \nand a 12,000 -word test set (reference data). 2,670 words are \nused as development (validation) set. There are 27 graphemes \n(uppercase alphabet symbols plus the apostrophe) and 41 \nphonemes in this dataset.  NetTalk contains 14,85 1 words for \ntraining, 4,951 words for testing and does not have a \npredefined validation set. There are 26 graphemes (lowercase \nalphabet symbols) and 52 phonemes in this dataset. \nWe use <START>, <END> tokens as beginning -of-\ngraphemes (beginning-of-phonemes), end-of-graphemes (end-\nof-phonemes) tokens and  <PAD> token in both datasets.  \n3.2.   Software and hardware details \n        NVidia Titan Xp (12 GB) and NVidia Titan X (12 GB) \nGPU cards hosted in two i7 desktop servers with 32GB RAM \nserved for training and inference. For training and evaluation, \nthe Keras 2 deep learning framework with TensorFlow3 \nbackend was our environment. \n \n3 https://www.tensorflow.org/ \n\n3.3.  Training \n \nBy adding < START>, <END> and <PAD> tokens to \nthe input and output, the length of the longest input and output \nwas fixed to 24. We completed shorter input and output \nsequences with the <PAD> token to make their length equal \nin both training and development sets. For the test set, padding \nwas not applied.  \nWe applied two embeddings which represent the encoder \n(grapheme) and decoder (phoneme) sides, respectively. The \nencoder and decoder embeddings had a great influence on the \nresults. The size of the embeddings is 128, and the dimension \nof the inner-layer is 512. We used Adam as optimizer [19]. The \ninitial learning rate was set to 0.0002. If the performance (PER \nfor G2P conversion) on the validation set has not improved for \n50 epochs, the learning rate was multi plied by 0.2. We apply \nlayer normalization and dropout in all models. The dropout \nrate of encoder and decoder is set to 0.1. Batch size is 128 for \nCMUDict, 64 for NetTalk. We have investigated three \ntransformer architectures, with 3 encoder and decoder lay ers \n(it is called Transformer 3x3 in Table 2), 4 encoder and \ndecoder layers (it is called Transformer 4x4 in Table 2) and 5 \nencoder and decoder layers (it is called Transformer 5x5 in \nTable 2). \nWe employed h = 4  parallel attention layers in all proposed \nmodels, and Q, K and V have the same dimension of ğ‘‘ğ‘š, so \nthat ğ‘‘ğ‘£ = ğ‘‘ğ‘˜ = ğ‘‘ğ‘š = 128 and ğ‘‘ğ‘š/â„ = 32. Due to the reduced \ndimension of each head, the total computational cost is similar \nto that of single-head attention with full dimensionality. \nOther parameters used in training are defined in Table 1. \n                   \n             Table 1: Training parameters. \nParameters Number \nEncoder layers (ğ‘) 3/4/5 \nDecoder layers (ğ‘) 3/4/5 \nParams in one encoder 256 \nParams in one decoder 256 \nDropout 0.1 \nBatch size 128/64 \nAdam optimizer ğ›½1 = 0.9,   ğ›½2 = 0.998 \n3.4.   Inference \n \nDuring inference, the phoneme sequence (written \npronunciation form of given grapheme sequence) will be \ngenerated one-by-one at a time. \nThe sequence begins with the start token <START >, and we \ngenerate the first phoneme by the highest probability. Then, \nthis phoneme is fed back into the network to generate the next \nphoneme. This process is continued until the end token \n<END> is reached, or the maximal length terminates the \nprocedure. Beam search was not applied in this work.  \n4. Evaluation and results  \n         We use the following common evaluation metrics for \nG2P: \nPhoneme Error Rate (PER)  is the Levenshtein distance \nbetween the predicted phoneme sequences and the reference \nphoneme sequences, divided by the number of phonemes in \nthe reference pronunciation [20]. In case of multiple \npronunciation samples for a word in th e reference data, the \nsample that has the smallest distance to the candidate is used. \nWord Error Rate (WER)  is the percentage of words in which \nthe predicted phoneme sequence does not exactly match any \nreference pronunciation, the number of word errors is divided \nby the total number of unique words in the reference.  \n \nTable 2: Results on the CMUDict and NetTalk dataset. \nDataset Model PER WER Time \n[s] \nModel  \nsize \nCMUDict Transformer 3x3 6.56 23.9 76 1.49M \nTransformer 4x4 5.23 22.1 98 1.95M \nTransformer 5x5 5.97 24.6 126 2.4M \nNetTalk Transformer 3x3 7.01 30.67 33 1.50M \nTransformer 4x4 6.87 29.82 39 1.96M \nTransformer 5x5 7.72 31.16 48 2.4M \n \n                                                            Table 3: Results on the CMUDict and NetTalk datasets. \n \n                                   \n \nData Method PER (%) WER (%) Model size \nNetTalk  \nJoint sequence model [3] 8.26 33.67 N/A \n              Encoder-decoder with global attention [7] 7.14 29.20 N/A \nEncoder CNN with res. conn, decoder Bi-LSTM (Model 5) [11] \nTransformer 4x4 \n5.69 \n6.87 \n30.10 \n29.82                                                       \n14.5 M \n1.95 M \nCMUDict  \nEncoder-decoder LSTM [18] 7.63 28.61 N/A \nJoint sequence model [3] 5.88 24.53 N/A \nCombination of sequitur G2P and seq2seq-attention and multitask \nlearning [21] 5.76 24.88               N/A \nDeep Bi-LSTM with many-to-many alignment [23] 5.37 23.23           N/A \nJoint maximum entropy (ME) n-gram model [4] 5.9 24.7 N/A \nEncoder CNN, decoder Bi-LSTM (Model 5) [11] 4.81 25.13           14.5M \n End-to-end CNN (Model 4) [11] 5.84 29.74 7.62M \n Encoder-decoder LSTM with attention (Model 1) [11] 5.68 28.44          12.7M \n Transformer 4x4 5.23             22.1   2.4M \n                                             Table 4. Examples of errors predicted by Transformer 4x4 and [11]. \nAfter training the model, predictions were run on the test \ndataset. The results of the evaluation on CMUDict and NetTalk \nare shown in Table 2. The first and second columns show the \ndataset and the applied architecture, respectively. The third and \nfourth columns show the PER and WER values. The fifth \ncolumn of Table 2 contains the average sum  of training and \nvalidation time of one epoch. The last column presents \ninformation about the number of parameters (weights). \nAccording to the results, Transformer 4x4 (4 layers encoder and \n4 layers decoder) outperforms Transformer 3x3 (3 layers \nencoder an d 3 layers decoder).  Contrary to expectations \nTransformer 5x5 (5 layers encoder and 5 layers decoder) didn't \noutperform Transformer 4x4 (4 layers encoder and 4 layers \ndecoder). Increasing the numbers of encoder -decoder layer s \nleads to much more training parameters. In  the G2P task, \nsimilar complexity to NMT (neural machine translation) can be \nrarely permitted . The high number of parameters sometimes \ndoes not even result in better performance.  In term of PER, \nTransformer 5x5 is bette r than Transformer 3x3 on CMUDict \nbut didn't exceed Transformer 4x4, Transformer 3x3 in the \npoint of WER on both CMUDict and NetTalk. \nDuring the experiments, we did not observe significant \nperformance improvements when the number of encoder -\ndecoder was increased.  \nIn Table 3, the performance of the Transformer 4x4 model with \npreviously state -of-the-art results is compared  on both \nCMUDict and NetTalk databases . The first column shows the \ndataset, the second column presents the method used in \nprevious solutions with references, PER and WER columns tell \nthe results of the referred models, and the last column presents \ninformation about the number of parameters (weights). \nAccording to Table 3, our proposed model reached competitive \nresults for both PER and WER. F or NetTalk, we are able to \nexceed previous results significantly. We should point out that \nthe results of the Transformer 4x4 model are close to encoder \nCNN with residual connections, decoder Bi -LSTM model \nobtained by [11] regarding PER, but WER is better in the \nproposed model. Moreover, the number of parameters of the \nconvolutional layers with residual connections as encoder and \nBi-LSTM as the decoder is 14.5M, encoder-decoder LSTM and \nencoder-decoder Bi -LSTM have 12.7M and 33.8M, \nrespectively [11]. Both the Transformer 4x4 and the \nTransformer 3x3 have fewer parameters than the previously \nmentioned models.  \nWhen comparing Transformer 4x4 and encoder CNN, decoder \nBi-LSTM model [11], there is an interesting contravention \nbetween PER and WER. Although PER is s maller, WER is \nhigher in the encoder CNN, decoder Bi -LSTM model than \nTransformer 4x4.  As mentioned in [11], there were twice as \nmany words with only one phoneme error than words which \nhave two phoneme errors in the result of encoder CNN decoder \nBi-LSTM mod el, and it affected the growth of WER. In \ncontrast, in Transformer 4x4 the number of words with only one \nphoneme error is not too much. Regarding the types of error \nwhen generating phoneme sequences, in the CNN encoder, Bi-\nLSTM decoder , some phonemes are u nnecessarily generated \nmultiple times. For example, for the word KORZENIEWSKI, \nreference is [ K AO R Z AH N UW F S K IY], the prediction of \nCNN encoder, Bi-LSTM decoder for this word is [K AO R Z N \nN N UW S K IY], where the character N was generated three \ntimes. But the prediction of Transformer 4x4 for this word is [K \nER Z AH N UW S K IY], where 1 failed phoneme (ER) and 2 \nforgotten phonemes (R, F) appear. Example 1 and Example 3 \nin Table 4 also show the type of errors for Transformer 4x4 and \nCNN encoder Bi-LSTM decoder. \n5. Conclusions \n         We investigated a novel transformer architecture for the \nG2P task. Transformer 3x3 ( 3 layers encoder and 3 layers \ndecoder), Transformer 4x4 (4 layers encoder and 4 layers \ndecoder), and Transformer 5x5 (5 layers encoder and 5 layers \ndecoder) architectures were presented including experiments on \nCMUDict and NetTalk.  We evaluated PER and WER, and the \nresults of the proposed models are very competitive with \nprevious state-art results. The number of parameters (weigh ts) \nof all proposed models is less than the CNN and the recurrent \nmodels. As a result, the time consumption of training process \ndecreased. \nIn future research, we intend to study the application of the \nproposed method in the field of end-to-end TTS synthesis. \n \n6. Acknowledgements \nThe research presented in this paper has been supported by the \nEuropean Union, co -financed by the European Social Fund \n(EFOP-3.6.2-16-2017-00013, Thematic Fundamental Research \nCollaborations Grounding Innovation in Informatics and \nInfocommunications), by the BME-Artificial Intelligence FIKP \ngrant of Ministry of Human Resources (BME FIKP-MI/SC), by \nDoctoral Research Scholarship of Ministry of Human \nResources (ÃšNKP -18-4-BME-394) in the scope of New \nNational Excellence Program, by JÃ¡nos Bo lyai Research \nScholarship of the Hungarian Academy of Sciences, by the \nAI4EU project ( No 825619 ), and the DANSPLAT project \n(Eureka 9944). We gratefully acknowledge the support of \nNVIDIA Corporation with the donation of the Titan Xp GPU \nused for this research. \nWe are grateful to Stan Chen for providing the dataset of \nNetTalk. \n \n Example 1 Example 2 Example 3 \nOriginal word NATIONALIZATION KORZENIEWSKI GRANDFATHERS \nReference N AE SH AH N AH L AH Z EY SH AH N K AO R Z AH N UW F S K IY G R AE N D F AA DH ER Z \nPrediction of CNN \nbased model [11] \n(Model 5) \nN AE SH AH N AH L AH EY EY SH AH N K AO R Z N N N UW S K IY G R AE N D AA DH DH ER \nTransformer 4x4 N AE SH N AH L AH Z EY SH AH N K ER Z AH N UW S K IY G R AE N F AA DH ER Z \n7. References \n[1] S.Ã–. Arik, M. Chrzanowski, A. Coates, G. Diamos, A. Gibiansky, \nY. Kang, X. Li, J. Miller, J. Raiman, S. Sengupta, and M. Shoeybi, \nâ€œDeep Voice: Real-Time Neural Text-to-Speech,â€ Proceedings of \nthe 34th International Conference on Machine Learning, vol. 70, \npp. 195-204, 2017. \n[2] A. W. Black, K. Lenzo, and V. Page, â€œIssues in Building General \nLetter to Sound Rules,â€ Proceedings of the 3rd ESCA Workshop \non Speech Synthesis, pp. 77â€“80. 1998. \n[3] M. Bisani and H. Ney, â€œJoint-Sequence Models for Grapheme-to- \nPhoneme Conversion,â€ Speech Communication, vol. 50, no. 5, pp. \n434â€“451, 2008. \n[4] L. Galescu, and J.F. Allen, â€œPronunciation of Proper Names with \na Joint N-Gram Model for Bi-Directional Grapheme-to-Phoneme \nConversion,â€ 7th International Conference on Spoken Language \nProcessing, pp. 109â€“112, 2002. \n[5] I. Sutskever, O. Vinyals, and Q. V.  Le, â€œSequence to Sequence \nLearning with Neural Networks,â€ Advances in Neural \nInformation Processing Systems (NIPS), pp. 3104â€“3112, 2014. \n[6] L. Lu, X. Zhang, and S. Renals, â€œOn Training the Recurrent \nNeural Network Encoder-Decoder for Large Vocabulary End-to-\nEnd Speech Recognition ,â€ IEEE International Conference on \nAcoustics, Speech and Signal Processing, pp. 5060â€“5064, 2016. \n[7]  S. Toshniwal and K. Livescu, â€œJointly  learning to align and \nconvert graphemes to phonemes with neural attention models ,â€ \nIEEE Spoken Language Technology Workshop (SLT), pp. 76-82, \n2016. \n[8] A.Krizhevsky, I. Sutskever, and G. E. Hinton, â€œImageNet \nClassification with Deep Convolutional Neural Networks,â€ \nProceedings of the 25th International Conference on Neural \nInformation Processing Systems (NIPS'12),  (1), pp. 1097 -1105, \n2012. \n[9] J. Gehring, M. Auli, D. Grangier, and Y. Dauphin, â€œA \nConvolutional Encoder Model for Neural Machine Translation,â€ \nProceedings of the 55th Annual Meeting of the Association for \nComputational Linguistics, pp. 123-135, 2016. \n[10]  Y. Wang, R.J. Skerry-Ryan, D. Stanton, Y. Wu, R. J. Weiss, N. \nJaitly, Z. Yang, Y. Xiao, Z. Chen, S. Bengio, Q. Le, Y. \nAgiomyrgiannakis, R. Clark, R. A. Saurous, â€œTacotron: Towards \nEnd-to-End Speech Synthesis,â€ in INTERSPEECH 2017 â€“ 18th \nAnnual Conference of the International Speech Communication \nAssociation, pp. 4006-4010, 2017. \n[11] S. Yolchuyeva, G. NÃ©meth, and B. Gyires -TÃ³th, â€œGrapheme-to-\nPhoneme Conversion with Convolutional Neural Networks,â€ \nApplied Science, vol. 9, no. 6, pp. 1143, 2019. \n[12]  M. J. Chae, K. Park, L. Bang, S. Suh, L. Park, N. Kim, and J. \nPark, â€œConvolutional Sequence to Sequence Model with Non -\nSequential Greedy Decoding for Grapheme to Phoneme \nConversion,â€ IEEE International Conference on Acoustics, \nSpeech and Signal Processing (ICASSP), pp. 2486-2490, 2018. \n[13] J. Ni, Y. Shiga, and H. Kawai, â€œMultilingual Grapheme -to-\nPhoneme Conversion with Global Character Vectors,â€ in \nINTERSPEECH 2018 â€“ 19th Annual Conference of the \nInternational Speech Communication Association , pp. 2823 -\n2827, 2018. \n[14] B. Peters, J. Dehdari, and J. V. Genabith, â€œMassively Multilingual \nNeural Grapheme-to-Phoneme Conversion,â€  \nProceedings of the First Workshop on Building Linguistically \nGeneralizable NLP Systems. pp. 19â€“26. 2017. \n[15] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. \nGomez, L. Kaiser, and I. Polosukhin, â€œAttention Is All You \nNeed,â€ 31st Conference on Neural Information Processing \nSystems (NIPS 2017), pp. 6000-6010, 2017. \n[16] S. M. Lakew, M. Cettolo, and M. Federico, â€œA Comparison of \nTransformer an d Recurrent Neural Networks on Multilingual \nNeural Machine Translation,â€ Proceedings of the 27th \nInternational Conference on Computational Linguistics \n(COLING), pp. 641- 652, 2018. \n[17] X. Zhu, L. Li, J. Liu, H. Peng, and X. Niu, â€œCaptioning \nTransformer with Stacked Attention Modules,â€ Applied Science, \nvol. 8, no. 5, pp. 739, 2018. \n[18] K. Yao and G. Zweig, â€œSequence-to-Sequence Neural Net Models \nfor Grapheme-to-Phoneme Conversion,â€ in INTERSPEECH 2015 \nâ€“ 16th Annual Conference of the International Speech \nCommunication Association, pp. 3330â€“3334, 2015. \n[19] D. P. Kingma and L. B. Jimmy, â€œAdam: A Method for Stochastic \nOptimization,â€ International Conference on Learning \nRepresentations (ICLR), pp. 1-13, 2015. \n[20] V. I. Le venshtein, â€œBinary Codes Capable of Correcting \nDeletions, Insertions, and Reversals,â€ Soviet Physics Doklady, \nvol. 10, no. 8, pp.707â€“710, 1966. \n[21]  B. Milde , C. Schmidt, and J. KÃ¶hler, â€œMultitask Sequence -to-\nSequence Models for Grapheme -to-Phoneme Conversionâ€ in \nINTERSPEECH 2017 â€“ 18th Annual Conference of the \nInternational Speech Communication Association , pp. 2536 -\n2540, 2017. \n[22] S. F. Chen, â€œConditional and Jo int Models for Grapheme -to-\nPhoneme Conversion,â€ 8th European Conference on Speech \nCommunication and Technology, pp.  2033â€“2036, 2003. \n[23] A. E. Mousa and B. W. Schuller, â€œDeep Bidirectional Long Short-\nTerm Memory Recurrent Neural Networks for Grapheme -to-\nPhoneme Conversion Utilizing Complex Many -to-Many \nAlignments,â€ in INTERSPEECH 2016 â€“ 17th Annual Conference \nof the International Speech Communication Association , pp. \n2836-2840, 2016. \n[24]  J. Ba, R., Kiros, and G. E. Hinton, â€œLayer Normalization,â€ CoRR, \nabs/1607.06450, 2016. \n[25]  N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and R. \nR. Salakhutdinov, â€œDropout: a simple way to prevent neural \nnetworks from overfitting,â€ Journal of Machine Learning \nResearch, vol. 15, pp. 1929-1958, 2014. \n[26] Sokolov, A., Rohlin, T., Rastrow, A. â€œNeural Machine \nTranslation for Multilingual Grapheme -to-Phoneme \nConversion,â€ Proc. Interspeech 2019 , 2065 -2069, doi: \n10.21437/Interspeech.2019-3176. \n "
}