{
    "title": "Bringing order into the realm of Transformer-based language models for artificial intelligence and law",
    "url": "https://openalex.org/W4388829218",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A4320968505",
            "name": "Candida M. Greco",
            "affiliations": [
                "University of Calabria"
            ]
        },
        {
            "id": "https://openalex.org/A273425128",
            "name": "Andrea Tagarelli",
            "affiliations": [
                "University of Calabria"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3216382054",
        "https://openalex.org/W3175844213",
        "https://openalex.org/W3103585424",
        "https://openalex.org/W3174504910",
        "https://openalex.org/W2536769020",
        "https://openalex.org/W3080917700",
        "https://openalex.org/W4385567093",
        "https://openalex.org/W3034469191",
        "https://openalex.org/W3163699936",
        "https://openalex.org/W4385573683",
        "https://openalex.org/W3214359242",
        "https://openalex.org/W3174058319",
        "https://openalex.org/W1801866228",
        "https://openalex.org/W2489487449",
        "https://openalex.org/W3034440122",
        "https://openalex.org/W2932110532",
        "https://openalex.org/W3213748474",
        "https://openalex.org/W3046377295",
        "https://openalex.org/W4285225959",
        "https://openalex.org/W4386504863",
        "https://openalex.org/W2493916176",
        "https://openalex.org/W1840435438",
        "https://openalex.org/W3037404357",
        "https://openalex.org/W3135764500",
        "https://openalex.org/W2962854673",
        "https://openalex.org/W3198651167",
        "https://openalex.org/W3106092787",
        "https://openalex.org/W3099950029",
        "https://openalex.org/W2962910668",
        "https://openalex.org/W3154444249",
        "https://openalex.org/W3136888420",
        "https://openalex.org/W3204112174",
        "https://openalex.org/W2616470781",
        "https://openalex.org/W4285291532",
        "https://openalex.org/W3114950584",
        "https://openalex.org/W4385571105",
        "https://openalex.org/W2970217403",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W2891555348",
        "https://openalex.org/W3021636956",
        "https://openalex.org/W3003690291",
        "https://openalex.org/W3116673362",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W3105220303",
        "https://openalex.org/W4200509089",
        "https://openalex.org/W3156638011",
        "https://openalex.org/W3215431778",
        "https://openalex.org/W3211783568",
        "https://openalex.org/W3102046836",
        "https://openalex.org/W3016154458",
        "https://openalex.org/W2888195404",
        "https://openalex.org/W2983986938",
        "https://openalex.org/W4285120549",
        "https://openalex.org/W4210683212",
        "https://openalex.org/W3004507689",
        "https://openalex.org/W3173549151",
        "https://openalex.org/W3185670382",
        "https://openalex.org/W4220873427",
        "https://openalex.org/W3185293939",
        "https://openalex.org/W4285065568",
        "https://openalex.org/W2536015822",
        "https://openalex.org/W3034238904",
        "https://openalex.org/W3175097602",
        "https://openalex.org/W4206785765",
        "https://openalex.org/W3179613227",
        "https://openalex.org/W3175587970",
        "https://openalex.org/W2964012472",
        "https://openalex.org/W4327519588",
        "https://openalex.org/W3011411500",
        "https://openalex.org/W2963339397",
        "https://openalex.org/W2963626623",
        "https://openalex.org/W2528705463",
        "https://openalex.org/W3099700870",
        "https://openalex.org/W4206751841",
        "https://openalex.org/W4213191780",
        "https://openalex.org/W4284679571",
        "https://openalex.org/W3201977280",
        "https://openalex.org/W3080175947",
        "https://openalex.org/W3202169636",
        "https://openalex.org/W2963250244",
        "https://openalex.org/W2946593017",
        "https://openalex.org/W2606964149",
        "https://openalex.org/W3148040514",
        "https://openalex.org/W2963233086",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W4200185027",
        "https://openalex.org/W4387428045",
        "https://openalex.org/W3213215942",
        "https://openalex.org/W2146950091",
        "https://openalex.org/W4385565879",
        "https://openalex.org/W2798838283",
        "https://openalex.org/W2963095557",
        "https://openalex.org/W3201338036",
        "https://openalex.org/W3172365208",
        "https://openalex.org/W3001434439",
        "https://openalex.org/W2970419734",
        "https://openalex.org/W3195296860",
        "https://openalex.org/W4386566622",
        "https://openalex.org/W3209145439",
        "https://openalex.org/W2940927814",
        "https://openalex.org/W3176443840",
        "https://openalex.org/W4385574011",
        "https://openalex.org/W3089324001",
        "https://openalex.org/W2986154550",
        "https://openalex.org/W3191709290",
        "https://openalex.org/W3212522196",
        "https://openalex.org/W3114757058",
        "https://openalex.org/W3214426409",
        "https://openalex.org/W2962810718",
        "https://openalex.org/W2952138241",
        "https://openalex.org/W2963929190",
        "https://openalex.org/W2888482885",
        "https://openalex.org/W3203176827",
        "https://openalex.org/W3100107515",
        "https://openalex.org/W4235023594",
        "https://openalex.org/W3160574899",
        "https://openalex.org/W3029812858",
        "https://openalex.org/W2605035112",
        "https://openalex.org/W4206376421",
        "https://openalex.org/W3201982822",
        "https://openalex.org/W2963015836",
        "https://openalex.org/W4283801911",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W3101498587",
        "https://openalex.org/W3210785553",
        "https://openalex.org/W4206765718",
        "https://openalex.org/W3169376659",
        "https://openalex.org/W2963105309",
        "https://openalex.org/W3176798136",
        "https://openalex.org/W3086962592",
        "https://openalex.org/W2963323070",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W4225363072",
        "https://openalex.org/W2970728484",
        "https://openalex.org/W2970641574",
        "https://openalex.org/W2282821441",
        "https://openalex.org/W3118485687",
        "https://openalex.org/W3186237585",
        "https://openalex.org/W2945465473",
        "https://openalex.org/W3036862134",
        "https://openalex.org/W3015773172",
        "https://openalex.org/W2144578941",
        "https://openalex.org/W4200468156",
        "https://openalex.org/W4385573327",
        "https://openalex.org/W3214559854",
        "https://openalex.org/W3213543097",
        "https://openalex.org/W3180395890",
        "https://openalex.org/W3173327529",
        "https://openalex.org/W2606974598",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W4200050671",
        "https://openalex.org/W3136275640",
        "https://openalex.org/W3216916209",
        "https://openalex.org/W3175746219",
        "https://openalex.org/W3034707327",
        "https://openalex.org/W4285116311",
        "https://openalex.org/W2136583886",
        "https://openalex.org/W3096266342",
        "https://openalex.org/W2612649659",
        "https://openalex.org/W3023209463",
        "https://openalex.org/W3202708990",
        "https://openalex.org/W3201427244",
        "https://openalex.org/W3214042884",
        "https://openalex.org/W3106883764",
        "https://openalex.org/W3214186955",
        "https://openalex.org/W2106401878",
        "https://openalex.org/W2939380783",
        "https://openalex.org/W3176964086",
        "https://openalex.org/W3214641167",
        "https://openalex.org/W3126161367",
        "https://openalex.org/W3006440582",
        "https://openalex.org/W2950784811",
        "https://openalex.org/W3185922622",
        "https://openalex.org/W4292075481",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W4287891026",
        "https://openalex.org/W3034394361",
        "https://openalex.org/W4389523809",
        "https://openalex.org/W3174544005",
        "https://openalex.org/W4213227026",
        "https://openalex.org/W3184496804",
        "https://openalex.org/W2963866616",
        "https://openalex.org/W3173533009",
        "https://openalex.org/W2517394750",
        "https://openalex.org/W4205206874",
        "https://openalex.org/W3177382889",
        "https://openalex.org/W4285171765",
        "https://openalex.org/W2710956079",
        "https://openalex.org/W3169483174",
        "https://openalex.org/W2470673105",
        "https://openalex.org/W4214929413",
        "https://openalex.org/W3186948566",
        "https://openalex.org/W4284674178",
        "https://openalex.org/W3104108820",
        "https://openalex.org/W1849277567",
        "https://openalex.org/W2963159690",
        "https://openalex.org/W2964213727",
        "https://openalex.org/W3186492090",
        "https://openalex.org/W2890026792",
        "https://openalex.org/W3035668167",
        "https://openalex.org/W2964051087",
        "https://openalex.org/W2963070937",
        "https://openalex.org/W3103187652",
        "https://openalex.org/W3102391739",
        "https://openalex.org/W3102286003",
        "https://openalex.org/W3103764297",
        "https://openalex.org/W3102937497",
        "https://openalex.org/W3102725307",
        "https://openalex.org/W3098749165",
        "https://openalex.org/W3154337097",
        "https://openalex.org/W3103513278"
    ],
    "abstract": "Abstract Transformer-based language models (TLMs) have widely been recognized to be a cutting-edge technology for the successful development of deep-learning-based solutions to problems and applications that require natural language processing and understanding. Like for other textual domains, TLMs have indeed pushed the state-of-the-art of AI approaches for many tasks of interest in the legal domain. Despite the first Transformer model being proposed about six years ago, there has been a rapid progress of this technology at an unprecedented rate, whereby BERT and related models represent a major reference, also in the legal domain. This article provides the first systematic overview of TLM-based methods for AI-driven problems and tasks in the legal sphere. A major goal is to highlight research advances in this field so as to understand, on the one hand, how the Transformers have contributed to the success of AI in supporting legal processes, and on the other hand, what are the current limitations and opportunities for further research development.",
    "full_text": "Vol.:(0123456789)\nArtificial Intelligence and Law (2024) 32:863–1010\nhttps://doi.org/10.1007/s10506-023-09374-7\n1 3\nREVIEW ARTICLE\nBringing order into the realm of Transformer‑based \nlanguage models for artificial intelligence and law\nCandida M. Greco1 · Andrea Tagarelli1 \nAccepted: 15 September 2023 / Published online: 20 November 2023 \n© The Author(s) 2023\nAbstract\nTransformer-based language models (TLMs) have widely been recognized to be a \ncutting-edge technology for the successful development of deep-learning-based \nsolutions to problems and applications that require natural language processing and \nunderstanding. Like for other textual domains, TLMs have indeed pushed the state-\nof-the-art of AI approaches for many tasks of interest in the legal domain. Despite \nthe first Transformer model being proposed about six years ago, there has been a \nrapid progress of this technology at an unprecedented rate, whereby BERT and \nrelated models represent a major reference, also in the legal domain. This article \nprovides the first systematic overview of TLM-based methods for AI-driven prob-\nlems and tasks in the legal sphere. A major goal is to highlight research advances in \nthis field so as to understand, on the one hand, how the Transformers have contrib-\nuted to the success of AI in supporting legal processes, and on the other hand, what \nare the current limitations and opportunities for further research development.\nKeywords Language models · BERT · GPT · Legal search · Legal document \nreview · Legal outcome prediction · Retrieval · Entailment · Inference · Caselaw \ndata · Statutory law data · Benchmarks · AI for law\n1 Introduction\nAI for law. Artificial Intelligence (AI) is increasingly finding its application to the \nlegal domain. This is mainly motivated by an evidence that the volume of infor -\nmation produced in the legal domain is overwhelming, and also variegate due to \nthe involvement of many different actors, such as legal professionals (lawyers, \n * Andrea Tagarelli \n andrea.tagarelli@unical.it\n Candida M. Greco \n candida.greco@dimes.unical.it\n1 Department of Computer Engineering, Modeling, Electronics, and Systems Engineering, \n(DIMES) - University of Calabria, 87036 Rende, CS, Italy\n864 C. M. Greco, A. Tagarelli \n1 3\nattorneys), law courts, legislators, law firms, and even citizens  (Surden 2019). AI \ntools, especially those for natural language processing (NLP), are indeed conceived \nto help legal actors handling huge amounts of legal documents, lighten the workload \nin searching for and understanding text passages of interest, thus ultimately easing \nstreamline processes and saving time for tackling more complex tasks.\nAI is increasingly powering legal tech companies with the advanced techni-\ncal backbone for better serving their clients more cheaply, efficiently and accu-\nrately than ever. For instance, one of the most required applications of AI in the \nlegal tech industry is contract review and analysis, whereby companies like Klarity \n(klarity.com) offer solutions to review sales contracts under a particular company \nlegal policy, or to enhance risk management (e.g., in response to legal and regula-\ntory changes) in order to assist firms in preparing a remediation plan, like in the \ncases of Kira Systems (kirasystems.com) and ThoughtRiver (thoughtriver.com). \nLegal research is also fundamental to pursue goals of prediction of case outcomes \nor court rulings. In this regard, Lex Machina (lexmachina.com) can help lawyers \nmanage a case strategy based on previous results in similar cases, or to predict how \na particular judge might rule on a case; Ravel Law’s Judge Analytics tool (ravellaw.\ncom) can also visually explore data points related to specific judges, organized by \ncourt and case type; Blue J Legal (bluej.com) has developed a litigation prediction \ntool with a focus on tax law, which can serve for speeding up settlement negotia-\ntions; DeepJudge AG (deepjudge.ai) can automatically highlight crucial information \nin documents, referencing with external sources such as codes, court rulings, com-\nmercial registries; yet, Luminance (luminance.com) can facilitate the discovery of \nbackground information to carry out necessary due diligence more efficiently. Other \ncompanies, such as Intraspexion (intraspexion.com), have a focus on providing \nattorneys with early warning indicators when their AI tools identify lawsuit dangers. \nLast but not least, chatbots are increasingly being developed to improve the user \nexperience through online self-serving, also for particular application scenarios; \ne.g., DoNotPay’s tool (donotpay.com) helps users to automatically claim asylum in \nsome countries.\nMoreover, AI development is highly fostered by political institutions and gov -\nernments. In the 2020 report commissioned by the Administrative Conference of \nthe United States on the use of AI in federal administrative agencies (Engstrom \net  al. 2020), AI tools are recognized as already enhancing agency operations \nacross the full range of governance tasks, including enforcing regulatory man-\ndates (e.g., for market efficiency, or workplace safety), adjudicating benefits and \nprivileges, monitoring and analyzing risks to public health and safety, extracting \ninformation from the government’s massive data streams (e.g., from consumer \ncomplaints), communicating with the public about its rights and obligations as \nwelfare beneficiaries, taxpayers, asylum seekers, and business owners. Also, the \nEuropean Union’s approach to AI focuses on excellence and trust, aiming to boost \nresearch and industrial capacity while ensuring safety and fundamental rights. 1 \nThis has been translated into concrete actions such as a review of the Coordinated \n1 https:// digit al- strat egy. ec. europa. eu/ en/ polic ies/ europ ean- appro ach- artifi  cial- intel ligen ce.\n865\n1 3\nTransformer-based language models for AI and law\nPlan on AI (with EU member states) and a proposal for a regulation laying down \nharmonised rules on AI, known as AI Act.\nThe breakthrough of large language models. On a variety of application \ndomains, ranging from scientific literature to Web and social media, NLP research \nhas traditionally focused on shallow text feature models, then evolved to logical \nand probabilistic language models, and more recently to word embeddings based \non machine learning and to deep learning architectures. The latter have certainly \nshown improved results in a wide range of NLP benchmarks, including bench-\nmarks specific to retrieval and classification tasks. In particular, the pre-trained \nTransformer-based Language Models (TLMs) are leading to a significant advance \nin NLP research to support human decision-making processes in several textual \ndata domains (Greco et al. 2022).\nTLMs possess unique characteristics in the machine and deep learning realm, \nso that exploiting them to solve such tasks as retrieval, understanding and predic-\ntion has a number of key advantages that can be summarized as follows. First, \nlike any other deep neural network models, TLMs avoid to manually handle fea-\nture engineering, and hence the need for selecting features from the input text and \nmeasuring their importance. Second, like sophisticated recurrent and convolu-\ntional neural networks, TLMs represent language semantics and non-linear rela-\ntionships between terms; however, TLMs are better to capture subtle and complex \nlexical patterns including the sequential structure and long-term dependencies, \nthus obtaining the most comprehensive local and global feature representations \nof a text sequence. Third, TLMs incorporate the so-called attention mechanism, \nwhich allows a learning model to assign higher weight to text features according \nto their higher informativeness or relevance to the learning task.\nWhilst the hype for successfully using TLMs is expected to hold true also \nfor such a challenging domain as law, however, like for other specialized fields, \nlegal language has distinct characteristics compared to general natural languages. \nGiven the variety of legal sources and their heterogeneous functionality, legal \nlanguage has indeed specific terminology and linguistic patterns, formal syntax, \nand semantics relying on a particular knowledge domain, to the extent that it is \noften regarded as a sublanguage.\nThe objective of this article is to shed light on the research advances in AI \nfor law that TLM-based methods are achieving in the last few years, since the \nadvent of the first Transformer (Vaswani et al. 2017), and especially of the BERT \nmodel  (Devlin et  al. 2019) and its many variants and extensions, which, as a \nwhole, are sometimes referred to as “BERTology”.\nObjectives, limitations, and scope. In this article, we examine the landscape of \nworks that aim to solve legal problems by means of a subset of artificial intelli-\ngence and NLP methodologies, which corresponds to the state-of-the-art in legal \nAI and is represented by TLMs. We believe this is not trivial since, despite being \na relatively recent technology, its evolution in the last few years has been dra-\nmatic and progressed at an unprecedented rate, also in the legal domain. To the \nbest of our knowledge, this is the first systematic study on problems, tasks and \n866 C. M. Greco, A. Tagarelli \n1 3\nbenchmarks in the legal AI area, for which AI approaches and methods based on \nTLMs have been developed to address several tasks, such as retrieval, classifica-\ntion, prediction, entailment, information extraction and many others.\nDespite our effort, this article cannot be intended to discuss definitive solutions to \nlegal problems, tasks and applications. Moreover, please note that other deep learn-\ning technologies, from convolutional and recurrent networks to word embeddings, \nas well as topic modeling and classic machine-learning techniques are regarded as \nout of scope of this work, unless they are used in combination with TLMs.\nThis article is also necessarily a snapshot in time, until mid 2022, with an update \nto early 2023. It is hence likely that recently appeared relevant works were missing \nat the time of writing of this work. As such, the presented classification will need \nto be updated as new challenges come into focus and additional perspectives are \nbrought to bear on legal problems.\nComparison with existing overviews. The use of AI within law has long been a \ntopic of interest in the law area, mainly focusing on understanding the relations of \nAI to the practice and administration of law as well as on the ethical limits of the \napplication of AI to legal data (e.g., Callister 2020; Surden 2019). While a purely \nlaw-based perspective is out of scope of this survey, it is nonetheless important to \nplace our work into the computer-science literature that has recently surveyed the \ntopic.\nZhong et al. (2020) overview existing tasks and applications in legal AI, organ-\nized into three categories, namely judgment prediction, similar case matching, \nand legal question answering. In this regard, a summary of main characteristics is \nprovided about existing approaches, also including a mention to early TLMs used \nfor legal AI. An overview on legal information retrieval approaches, divided into \nnatural language based, ontology-based, and deep-learning-based systems is also \npresented in (Sansone and Sperlí 2022). Francesconi (2022) describes the vision \nof the IAAIL president at ICAIL 2021 on the status of the AI and law discipline, \nincluding possible future perspectives that envisage the use of machine and deep \nlearning to extract knowledge from legal data, combined with legal knowledge \nrepresentation and models for legal reasoning. Also, Song et al. (2022) provide a \ncomparative empirical evaluation of TLMs on various NLP tasks, which aims to \ngain insights into the performance difference between domain-specific models and \ngeneral domain models.\nWhile the above surveys are general in the domain of AI and law, a focus on the \ntask of querying for ad-hoc case law retrieval is taken in (Locke and Zuccon 2022), \nwhere approaches and methods are discussed into the following categories: Boolean \nand natural language, conceptual search and case-based retrieval, question answer -\ning, query expansion, query reduction, search result diversification, use of citation \nnetworks, and deeper understanding of texts.\nNone of the above works, however, provides a systematic analysis of approaches \nand methods based on TLMs for legal problems and tasks, which is instead the \nobjective of this article.\n867\n1 3\nTransformer-based language models for AI and law\nPlan of this paper. The remainder of this paper is organized as follows. Section  2 \nprovides an overview of BERT and subsequent TLMs, focusing on those being used \nin the legal domain. Section  3 introduces main problems and relating tasks in the \nlegal AI area that are being addressed by TLM-based methods, along with major \nrelevant benchmarks in the legal domain. Accordingly, Sect.  4 describes in detail \nmajor existing TLM-based methods. In Sect.  5, we provide a discussion on main \nfindings, limitations, challenges and future perspectives for legal AI based on TLMs. \nSection 6 concludes the paper.\n2  Background on Transformer‑based language models\nSince their debut in the Natural Language Processing (NLP) field, Transformer mod-\nels  (Vaswani et  al. 2017) have become a standard de-facto in the development of \nrevolutionary deep-learning models that pushed the state-of-the-art in many challeng-\ning NLP tasks. The key point in the Transformer architecture is the use of attention \nmechanisms (Bahdanau et al. 2015) as an alternative to the conventional recurrent neu-\nral networks (RNNs) and convolutional neural networks (CNNs). The attention takes \ninto account all the hidden states of a neural network and chooses which ones is to \nignore and which ones is to remember, giving the proper weight to words’ depend-\nencies regardless of the distance in the sequence. The weights are computed through \nan attention function, whose purpose is to value the relevance of an input word with \nrespect to a target word. Nowadays, attention mechanisms are ubiquitous in NLP and \nhave boosted performances on a wide range of tasks.\nTransformer is the first model to rely solely on attention mechanisms, leaving out \nthe use of RNNs and CNNs. The attention is modeled in two ways. Firstly, it estimates \nthe importance of words in the same sequence, i.e., input words are treated as both \nsources and targets, with the goal of calculating appropriate representations of the input \nsequence that reflect syntactic-semantic connections. This mechanism is also referred \nto as self-attention. Secondly, the input representations are weighted through attention \nto predict target tokens in an auto-regressive manner. The Transformer’s architecture \nconsists of multiple stacked encoder–decoder structures (Fig. 1).\nThe attention function used in the Transformer is called scaled dot-product \nattention (Vaswani et al. 2017). For each token, three types of vectors are defined, \nnamely query, key and value, with dimensions dk , dk and dv , respectively. A weight \nis assigned to each value vector, depending on the compatibility of the query and \nthe key. The query represents the token against which the importance of the other \ntokens, namely the keys, have to be evaluated. Formally, given n tokens in input, \nQ ∈ ℝn×dq , K ∈ ℝn×dk , V ∈ ℝn×dv are the representation matrices of queries, keys \nand values, respectively, the scaled dot-product attention Att(Q, K, V)∈ ℝn×dv is \ndefined as follows:\n(1)Att(/u1D410,/u1D40A,/u1D415)=softmax\n�\n/u1D410/u1D40A⊤\n√\ndk\n�\n⋅ /u1D415,\n868 C. M. Greco, A. Tagarelli \n1 3\nwhere term \n√\ndk is used as a scaling factor. By default, both dk , dq and dv have a \nvalue of 64. All the layers in the model, as well as the embedding layers, produce \noutputs of dimension dmodel = 512.\nThe encoders consist of multiple heterogeneous self-attention mechanisms, each \ncapturing a different kind of word relation in order to reflect several syntactic-\nsemantic nuances. A single type of self-attention is called head, so that the encoder \nhas a Multi-Head Self-Attention mechanism:\nwhere W(Q)\ni ∈ ℝdmodel×dq , W(K )\ni ∈ ℝdmodel ×dk , W(V )\ni ∈ ℝdmodel ×dv are the projection matri-\nces for queries, keys and values, respectively. /u1D416(O) ∈ ℝhdv×dmodel is a projection \nmatrix for the output.\nThe decoders have a similar structure, but they require the use of masked multi-\nhead self-attention mechanism in order to predict the current word without having \naccess to subsequent words, i.e., in an auto-regressive manner. In addition, they also \nuse an Encoder–Decoder Attention mechanism, i.e., a multi-head attention over the \noutput of the encoder stack, in order to catch relevant information for the current \ntarget prediction.\nQueries, keys and values may have different meanings depending on the type of \nattention. In the self-attention, they all come from the output of the previous layer in \n(2)\nMultiHead(Q,K, V)= concat(head1 ,… ,headh) ⋅ W (O)\nheadi = Att(QW (Q)\ni ,KW (K )\ni ,VW (V )\ni ),\nFig. 1  Transformer architec-\nture (Vaswani et al. 2017). \nOn the left side, the encoder \ncomponent with the multi-head \nself-attention mechanism. On \nthe right side, the decoder \ncomponent with two attention \nmechanisms, a masked multi-\nhead self-attention for decoder \noutputs and a multi-head \nattention for encoder–decoder \noutputs\n\n869\n1 3\nTransformer-based language models for AI and law\nthe encoder (as well as in the decoder). In the encoder–decoder attention, queries come \nfrom the previous decoder layer, while keys and values come from the encoder output.\nIn the following, we describe main characteristics of the TLMs that have been \nused to address legal problems and tasks (cf. Sect.  3). We organize our presentation \nof TLMs into three parts: Section  2.1 is devoted to BERT, which is the most com-\nmonly used TLM also in the legal domain, Sect.  2.2 overviews main variants and \nextensions of BERT, and Sect. 2.3 contains further and more recent TLMs.\nIt is worth noticing that our discussion concerns not only the conceptual architec-\nture and components of TLMs but also includes information about their implemen-\ntation; moreover, in Table 1, we provide further details summarizing the pre-training \nconfiguration of each of the models, in particular: number of steps, batch size, lr \n(learning rate), optimization method, /u1D716 (i.e., constant for numerical stability for the \noptimization method), dropout probability, weight decay (i.e., parameter for weights \nregularization), warmup steps (i.e., number of steps for the warmup part of train-\ning), the activation function, vocabulary size, and max length (i.e., maximum num-\nber of tokens). In this regard, we point out that most of the publicly available soft-\nware resources, including pre-trained models, can be found on the online platform \nHugging Face.2 Throughout this section, we will refer to the model implementations \navailable in this platform, unless otherwise specified, e.g., repositories provided by \nthe authors of the TLMs.\n2.1  BERT\nThe first known Transformer-based model which has revolutionized the way to \napproach the natural language understanding challenges, pushing the state-of-the-\nart in many demanding NLP benchmarks, is Bidirectional Encoder Representations \nfrom Transformers (BERT)  (Devlin et  al. 2019). 3 BERT is a Transformer-based \nmodel that has represented a breakthrough in several NLP benchmarks and it is \nstill considered a must-have baseline (Rogers et al. 2020). It essentially consists of \na stack of Transformer encoder layers (Fig.  1). The key aspects of BERT are the \nbidirectional unsupervised pre-training and the beneficial effect of having a unified \narchitecture across different tasks. The framework uses the pre-training fine-tuning \nparadigm. In the pre-training stage, the model uses a masked language objective to \nget deep bidirectional representations from unlabeled text by jointly conditioning on \nleft and right context-words in all encoder layers. A deep bidirectional model has \nshown to outperform a conventional unidirectional model or even a shallow left-to-\nright and right-to-left concatenated model (like ELMo).\nTo obtain a deep bidirectional representation of unlabeled text in the pre-training \nphase, BERT employs a particular pre-training objective task called Masked Lan-\nguage Modeling (MLM). This task is to predict masked input words from unlabeled \n2 https:// huggi ngface. co/ trans forme rs, https:// github. com/ huggi ngface/ trans forme rs.\n3 https:// github. com/ google- resea rch/ bert.\n870 C. M. Greco, A. Tagarelli \n1 3\nTable 1  Summary of pre-training settings of frequently used TLMs in works discussed in this article\nModel #Steps Batch size lr Optimization /u1D716 Dropout Weight \ndecay\nWarmup \nsteps\nActivation Vocab. size Max length\nBERT 1M 256 1e-4 Adam \n(/u1D6FD1 = 0.9,/u1D6FD2 = 0.999 )\n1e-6* 0.1 0.01 10K gelu 30K 512\nRoBERTa 500K 8K 6e-4 (base), \n4e-4 (large)\nAdam \n(/u1D6FD1 = 0.9,/u1D6FD2 = 0.98)\n1e-6 0.1 0.01 24K (base), \n30K \n(large)\ngelu 50K (sub-\nwords)\n512\nDeBERTa 1M, 500K (v3) 2K, 8K (v3) 2e-4 (base, \nlarge), 1.5e-4 \n(1.5B), 3e-4 \n(v3 large), \n6e-4 (v3 \nbase, small)\nAdam ( /u1D6FD1 = 0.9,/u1D6FD2\n= 0.999, /u1D6FD2 = 0.98 \n(v3))\n1e-6 0.1 0.01 10K gelu** 50K (base, \nlarge)**, \n128K \n(1.5B)***\n512**\nDistil-\nBERT\n3 (epochs)⋄ 5⋄ 5e-4⋄ AdamW⋄ \n(/u1D6FD1 = 0.9,/u1D6FD2 = 0.98)\n1e-6⋄ 0.1** 0.0⋄ in the code⋄ gelu** 28,996 \n(cased)**, \n30 K \n(uncased)**\n512**\nAlBERT 125K 4,096 1.76e-3 Lamb⋄⋄⋄ \n(/u1D6FD1 = 0.9,/u1D6FD2 = 0.999 )\n1e-6⋄⋄⋄ 0⋄⋄ 0.01⋄⋄⋄ 3,125 ⋄⋄⋄⋄ gelu 30K 512\nT5 524K 128 1e-2 AdaFactor†† (/u1D6FD1 = 0) /u1D7161 = 1e-30,\n†† /u1D7162=0.001††\n0.1 0 ††† 10K relu 32K 512\nELEC-\nTRA \n1M (small), \n766K (base), \n400K-1.75M \n(large)\n128 (small), \n256 \n(base), \n2048 \n(large)\n5e-4 (small), \n2e-4 \n(base, large)\nAdam \n(/u1D6FD1 = 0.9,/u1D6FD2 = 0.999 )\n1e-6 0.1 0.01 10K gelu** 30K† 128 (small), \n512  \n(others)\nLong-\nformer\nLike RoB-\nERTa\n64 3e-5 Like RoBERTa Like RoBERTa Like \nRoB-\nERTa\nLike \nRoB-\nERTa\n500 Like RoB-\nERTa\nLike RoB-\nERTa\n4,096\n871\n1 3\nTransformer-based language models for AI and law\n*From https:// github. com/ google- resea rch/ bert/ blob/ master/ optim izati on. py\n**from https:// github. com/ huggi ngface/ trans forme rs\n***from https:// github. com/ micro soft/ DeBER Ta\n⋄from https:// github. com/ huggi ngface/ trans forme rs/ tree/ 77321 48124 7787c 97568 c3b9f 64b19 e2235 1bab8/ examp les/ resea rch_ proje cts/ disti llati on\n⋄⋄from AlBERT paper: ≪ We also note that, even after training for 1M steps, our largest models still do not overfit to their training data. As a result, we decide to remove \ndropout to further increase our model capacity≫\n⋄⋄⋄from https:// github. com/ google- resea rch/ albert/ blob/ master/ optim izati on. py\n⋄⋄⋄⋄from https:// github. com/ google- resea rch/ albert/ blob/ master/ run_ pretr aining. py\n†from https:// github. com/ google- resea rch/ elect ra/ blob/ master/ confi gure_ pretr aining. py\n††from https:// github. com/ google- resea rch/ text- to- text- trans fer- trans former\n†††from https:// huggi ngface. co/ docs/ trans forme rs/ main_ class es/ optim izer_ sched ules\nTable 1  (continued)\n872 C. M. Greco, A. Tagarelli \n1 3\ntext by conjointly conditioning on left and right context-words in all encoder lay -\ners. In other words, BERT considers a deeply bidirectional context in the prediction \nof masked tokens, and as a result it builds more expressive and meaningful word \nembeddings than a conventional unidirectional or shallow bidirectional models \n(Fig.  2). In addition, BERT uses a second pre-training objective task called Next \nSentence Prediction (NSP) to cover a variety of downstream tasks involving sen-\ntence pairs. Given two word sequences as input, the NSP task is to determine if the \nsecond sequence is subsequent to the first in the original document.\nEach word is first tokenized into word pieces  (Wu et  al. 2016), which forms a \nvocabulary of 30,522 tokens. 4 The input representation is the sum of token embed-\ndings with segment and position embeddings. Segment embeddings take into \naccount the token location, i.e., the tokens of a sentence have all the same segment \nencoding. Position embeddings keep track of the token absolute position in the text. \nThe first token of the sequence is always a special token called [CLS], and the out-\nput representation of this token is considered to be representative of the whole input \nsequence. In the NSP task, the two sentences in an input sequence are separated to \neach other by another special token called [SEP]. In the MLM task, words to be \nmasked are replaced with the [MASK] special token. The introduction of [MASK] \nhas the advantage to allow the model to rely jointly on left and right context. How -\never, it also causes a mismatch between the pre-training and the subsequent, down-\nstream fine-tuning phase, since the latter does not introduce any mask. Therefore, to \nmitigate this issue, not all the tokens are replaced with [MASK], but only a certain \npercentage; in particular, from all tokens, 15% are selected to be masked, using a \nrandom token in 10% of cases, [MASK] in 80% of cases and leave the original token \nin the remaining 10%. The model does not know which words have been randomly \nreplaced, so it has to keep a distributional contextual representation for each token.\nBERT is originally pre-trained using two unlabeled corpora, named Book Corpus \nand English Wikipedia, for a total of 3,300M words. The input sequence has a limit \nof 512 tokens. For NSP, the training examples are sampled so that in 50% of cases \n4 Using the WordPiece tokenization process, the vocabulary is obtained using a data-driven approach: \ngiven a training corpus and a number w of word pieces for the vocabulary, the task is to select w word \npieces such that the segmented corpus contains as much unsegmented words as possible. WordPiece \ntokenization has shown to deal with the out-of-vocabulary words better than standard tokenization pro-\ncedures.\nFig. 2  Difference between deeply bidirectional BERT architecture, left-to-right GPT architecture and \nshallow bidirectional ELMo architecture (Devlin et al. 2019)\n873\n1 3\nTransformer-based language models for AI and law\nthe second sentence is actually subsequent to the first, while in the remaining 50% \nthe second sentence is randomly selected from the corpus.\nThe authors released BERT in two format sizes: BERT-base and BERT-large. \nBERT-base consists of 12 stacked encoder layers with 12 attention heads and \na hidden size of 768, for a total of 110M parameters, while BERT-large has 24 \nstacked encoder layers, 16 attention heads and a hidden size of 1024 (340M total \nparameters).\nUsing BERT for downstream tasks is straightforward, as it just requires one addi-\ntional task-specific layer on top of the model and a fine-tuning phase on parameters. \nThe input representation is analogue to the pre-training one described above. Typi-\ncally, the final hidden state corresponding to the [CLS] token is used as the input of \nthe additional top layer for tasks such as classification and entailment; analogously, \nthe final hidden states of the input tokens are fed into the additional top layer for \ntoken-level tasks, such as sequence-tagging and question-answering (Fig. 3).\nBERT contextual representations of words can also be used as word embeddings \nfor feature-based models. The authors experimented using BERT embedding as \ninput for a randomly initialized two-layer BiLSTM. They found that great perfor -\nmance can be achieved by combining the outputs of BERT layers, especially when \nthe last four layers’ outputs of BERT-large are concatenated.\nSeveral studies have examined the encoded knowledge in the BERT weighted \nrepresentations (Rogers et al. 2020). It has been shown that BERT naturally learns \nsyntactical information, such as parts of speech and syntactic chunks, and its repre-\nsentations are characterized as being hierarchical. BERT has also semantic knowl-\nedge, as it encodes information like semantic roles, entity types, relations etc. Sev -\neral studies stated that the most information about the order of words is in the lower \nlayers of BERT, syntactic information is primarily in the middle layers, while the \nmost task-specific information is in the final layers. Semantic information, instead, is \nall over the model since it pervades all the language. In the knowledge induction by \nfilling in the blanks, a study has shown that, for some relation types, BERT can be \ncompetitive with methods supported by knowledge bases.\nFig. 3  BERT pre-training and fine-tuning (Devlin et al. 2019)\n874 C. M. Greco, A. Tagarelli \n1 3\nBERT was tested on a number of benchmarks, including GLUE  (Wang et  al. \n2018), SWAG  (Zellers et  al. 2018), SQuAD v1.1  (Rajpurkar et  al. 2016) and \nv2.0  (Rajpurkar et  al. 2018), getting the top results on eleven tasks. In fact, it \nadvanced the state-of-the-art, obtaining 80.5 score (7.7% point improvement com-\npared to previous models) on the official GLUE leaderboard, 5 86.3% test accuracy \non SWAG (8.3% point improvement compared to GPT), 1.5 more points on SQuAD \nv1.1 Test F1 and 5.1 more points on SQuAD v2.0 Test F1 compared to previous \nmodels.\nSeveral instances of BERT model have been released over the years, which \nmainly differ from each other in terms of model size (as above discussed) as well \nas training objective and case folding. For the legal domain, the following models \nappear to be mostly used in the works discussed in Sect. 4: the case-sensitive bert-\nbase-cased,6 the uncased bert-base-uncased, the instance trained on sen-\ntence-pair classification task for FAQ retrieval, dubbed bert-based-faqir,7 the \nJapanese instance called bert-base-japanese8 as well as the Japanese bert-\nbase-japanese-whole-word-masking9 and BERT-base_mecab-ipa -\ndic-char-4K_whole-word-mask (abbrv. BERTjpcwwm). 10 The latter are \nthe Japanese instances trained using the MLM objective with the whole word mask -\ning technique, in which all the tokens related to a single word are masked.\nThere is also a multilingual variant of BERT, called m-BERT. 11 Actually, only \nthe base version of the model is available (12 layers, 12 attention heads, 768 hidden \nsize, 110M parameters in total), trained for more than 100 languages and a specific \nversion for Chinese.\nOver the time, BERT has inspired extensive research in the development of dif-\nferent variants and enhancements of the model. In the next section, we discuss the \nmost relevant BERT-inspired models that have been applied in legal tasks.\n2.2  BERT variants and extensions\nRoBERTa. RoBERTa  (Liu et  al. 2019) is conceived to improve BERT based on \nthe presumed under-training and non-optimal setting of the key hyperparameters of \nBERT.12 RoBERTa indeed develops a more robust and optimized approach through \ntwo main interventions. Firstly, the amount of training data is increased by add-\ning three further corpora besides Book Corpus and English Wikipedia, namely \nCC-News, which consists of the English portion of CommonCrawl news data-\nset (76 GB), Stories (31 GB), and OpenWebText (38GB). Secondly, RoBERTa is \n12 https:// github. com/ faceb ookre search/ fairs eq/ blob/ main/ examp les/ rober ta/ README. md.\n5 https:// glueb enchm ark. com/ leade rboard.\n6 https:// huggi ngface. co/ bert- base- cased.\n7 https:// github. com/ ku- nlp/ bert- based- faqir.\n8 https:// huggi ngface. co/ cl- tohoku/ bert- base- japan ese.\n9 https:// huggi ngface. co/ cl- tohoku/ bert- base- japan ese- whole- word- maski ng.\n10 https:// github. com/ cl- tohoku/ bert- japan ese.\n11 https:// github. com/ google- resea rch/ bert/ blob/ master/ multi lingu al. md.\n875\n1 3\nTransformer-based language models for AI and law\npre-trained much longer, which is related to the finding that decreasing BERT’s \nnumber of steps and increasing the batch size leads to better results, with the same \ncomputational cost. Table  1 reports further details regarding the pre-training con-\nfiguration settings.\nSubstantial changes on the pre-training procedure have been made too. Pre-\ntraining sequences have a length of 512 tokens, discarding BERT short-sequence \nstrategies—in BERT, 90% of the pre-training steps involve sequences of length \n128 tokens, and the remaining 10% use sequences of 512 tokens. The next sentence \nprediction task is removed, as it is not regarded as relevant to the model’s perfor -\nmance. Moreover, the masked language modeling task is modified, by introducing \na dynamic masking that avoids the use of the same mask for each instance and for \neach epoch.\nThe text representation of RoBERTa is obtained through a byte-level vocabulary \ncontaining 50K subwords units, in contrast with the character-level vocabulary of \n30K tokens used by BERT. The text segmentation method used in RoBERTa is the \nByte-Pair Encoding (BPE) (Sennrich et al. 2016).\nLike for BERT, two sizes of RoBERTa have been released: RoBERTa-base (12 \nlayers, 12 attention heads, 768 hidden size) and RoBERTa-large (24 layers, 16 atten-\ntion heads, 768 hidden size). RoBERTa has shown to overcome BERT in classic \nbenchmarks such as GLUE, SQuAD v2.0 and RACE (Lai et al. 2017).\nSBERT and SRoBERTa. SBERT and SRoBERTa (Reimers and Gurevych 2019) are \nmodified versions of BERT and RoBERTa, respectively, especially designed for tasks \nsuch as semantic textual similarity, clustering, and information retrieval via semantic \nsearch.13 One problem is the massive computational overhead concerning the search \nfor the most similar sentence-pairs in a collection of thousands of sentences. SBERT \nand SRoBERTa drastically reduce the computational effort of BERT for finding the \nmost similar pair in a collection, while maintaining the same accuracy. SBERT (resp. \nSRoBERTA) fine-tunes BERT (resp. RoBERTa) based on a Siamese network archi-\ntecture which, given two pre-trained BERT (resp. RoBERTa) models, one for each \ninput sentence, it ties the models’ weights which are updated during fine-tuning, to \nobtain semantically-expressive fixed-sized sentence embeddings based on a pooling \noperation. The two resulting sentence embeddings are concatenated with their ele-\nment-wise difference, prior to the softmax layer for class prediction.\nSiamese and triplet networks are used to obtain semantically meaningful sen-\ntence embeddings, then these embeddings can be compared using similarity criteria \n(e.g., cosine similarity). The final layer structure depends on the task at hand to be \noptimized (Fig. 4). For classification purpose, the sentence embeddings are concat-\nenated with their element-wise difference and fed into a softmax classifier, using \nan optimized cross-entropy loss. For regression purpose, cosine similarity between \nsentence embeddings and mean-squared-error loss are used. In addition, the authors \nexperimented the model with a triplet objective function.\n13 https:// github. com/ UKPLab/ sente nce- trans forme rs.\n876 C. M. Greco, A. Tagarelli \n1 3\nSBERT has been trained on NLI data (SNLI and MultiNLI) and evaluated on \nsemantic textual similarity (STS) tasks, both unsupervised, i.e., without any task-\nspecific training data, and supervised STS tasks, in which the STS benchmark was \nused for fine-tuning. SBERT sentence embeddings has been evaluated using the \nSentEval toolkit, in which the embeddings are used as features for a logistic regres-\nsion classifier. Using RoBERTa in place of BERT (SRoBERTa), the model gener -\nates quite similar sentence embeddings and obtain similar performance. Although \nSBERT embeddings are not designed for transfer-learning on other tasks, experi-\nments reveal that they can achieve good performance in most SentEval transfer \nlearning tasks.\nSpanBERT. SpanBERT (Joshi et al. 2020) introduces a novel pre-training approach \nthat differs from BERT in three aspects: the masking scheme, the training objective \nand the sequence training procedure.14\nWhile maintaining the same percentage distribution of masking as BERT, Span-\nBERT masks adjacent random spans rather than random single tokens. The span \nmasking is performed by sampling spans of text. The span length is picked from \nan unbalanced geometric distribution that favors short length selection, where the \nstarting point of span masking is randomly selected too. Eventually, complete words \nonly (instead of sub-words) are masked.\nIn addition, SpanBERT introduces a novel pre-training objective, called span-\nboundary objective (SBO). This task is to predict each token of the span by using \nonly the observed tokens at the boundaries, i.e., the first token before the start and \nfirst token after the end of the span (Fig.  5). The idea is to encourage the model \nto record as much span information as possible in the boundary output encodings. \n14 https:// github. com/ faceb ookre search/ SpanB ERT.\nFig. 4  SBERT architectures for classification (on the left) and inference/regression (on the right) \ntasks (Reimers and Gurevych 2019). In the first case, the concatenation of the sentence embeddings u \nand v and the element-wise difference /uni007C.varu − v/uni007C.var is the input for a softmax classifier. In the second case, the \ncosine similarity between u and v is calculated\n877\n1 3\nTransformer-based language models for AI and law\nEach token of the span is then represented using the boundary output encodings and \nrelative position embedding of the target token. Like in MLM, SBO minimizes the \ncross-entropy loss. The overall loss is the sum of both MLM and SBO objectives for \neach token in the span:\nwhere xi is the i-th token in the span, xi is the encoder output and yi is the model rep-\nresentation of xi , which corresponds to a function of the external boundaries and the \nrelative token position in the span:\nwhere s and e indicate the start and the end positions of the span, xs−1 is the bound-\nary on the left of the span, xe+1 is the boundary on the right of the span, p i−s+1 is the \nrelative token position and f (⋅) is a function implemented as a 2-layer feed-forward \nnetwork. The model gets rid of the NSP task, since the authors suggest that using \nonly single sequences in the training process benefits from two aspects: the model \ncan take advantage from longer contexts and the noise, coming from the context of \nother documents, is removed.\nThe implementation is the same as BERT-large, also in terms of corpora (i.e., \nBook Corpus, English Wikipedia) and tokenization method (i.e., WordPiece), but \nchanging some configuration setting (Table 1). Inspired by RoBERTa, in the training \nprocess the model uses different masks at each epoch and considers only sequences \nof 512 tokens till the end of the document, discarding BERT’s short-sequence \nstrategies.\nSpanBERT has been tested on several benchmarks regarding question answering, \ncoreference resolution and relation extraction, as well as nine GLUE tasks. Experi-\nmentation reveals that SpanBERT can outperform BERT on most benchmarks. \nDeBERTa. DeBERTa (He et al. 2021) introduces two novel techniques to improve \nupon BERT and RoBERTa: the disentangled attention mechanism and an enhanced \n(3)\nL(xi)=L MLM (xi)+L SBO (xi)\n=− log P(xi/uni007C.varxi)− log P(xi/uni007C.varyi),\n(4)yi = f(xs−1 ,xe+1 ,p i−s+1),\nFig. 5  SpanBERT training  (Joshi et  al. 2020). The masked span x5 ,x6 ,x7 ,x8 is predicted using the \nboundaries x4 and x9\n878 C. M. Greco, A. Tagarelli \n1 3\nmask decoder. 15 The former consists in computing the attention weights among \nwords using two disentangled matrices, which represent word contents and posi-\ntions, respectively. In fact, unlike BERT, DeBERTa separates content and relative \nposition encodings in two vectors, so that the attention score for a word pair is cal-\nculated by summing up the content-to-content, content-to-position and position-to-\ncontent attention:\nwhere A i,j is the attention score between tokens i and j, hi (resp. hj ) is the content \nrepresentation of i (resp. j), pi/uni007C.varj (resp. pj/uni007C.vari ) is the relative position with token j (resp. \ni).\nDeBERTa is pre-trained using the MLM task, but unlike BERT, it has no abso-\nlute positional embeddings. The absolute position is very important for the pre-\ndiction of masked tokens, in order to discern syntactical nuances in a sentence. \nFor this purpose, DeBERTa incorporates the absolute positional embeddings just \nbefore the softmax layer responsible of masked word predictions (Fig.  6). This \napproach is called enhanced mask decoder (EMD) and allows the model to con-\nsider relative position in all layers and absolute position only as an additional \ninformation when masked words have to be decoded. The pre-training settings \nof large DeBERTa models is analogue to BERT except for the use of the BPE \nvocabulary. Like RoBERTa, training data is composed by English Wikipedia and \nBook Corpus (already used in BERT) with the addition of OpenWebText and Sto-\nries, for a total of about 78Gb. Following RoBERTa, DeBERTa uses a dynamic \nbatch data to handle documents shorter than 512 tokens. Inspired by SpanBERT, it \nincludes also span masking strategies. DeBERTa pre-training parameters settings \nare reported in Table 1 .\n(5)\nA i,j ={ hi, pi/uni007C.varj}×{ hj, pj/uni007C.vari}⊤\n= hih⊤\nj + hip⊤\nj/uni007C.vari+ pi/uni007C.varjh⊤\nj ,\nFig. 6  DeBERTa’s enhanced \nmask decoder (EMD) (He et al. \n2021). There are n stacked EMD \nlayers and two inputs: H and \nI. H is the hidden state from \nprevious Transformer layer, \nwhile I represents any additional \ninformation (H, absolute posi-\ntion or output from previous \nEMD layer). When I = H  and \nn = 1 , EMD is equivalent to a \nBERT layer\n15 https:// github. com/ micro soft/ DeBER Ta.\n879\n1 3\nTransformer-based language models for AI and law\nDeBERTa is implemented with three version sizes: DeBERTa-base (12 layers, \n12 attention heads, 768 hidden size), DeBERTa-large (24 layers, 16 attention heads, \n1024 hidden size) and DeBERTa-1.5B (48 layers, 24 attention heads, 1536 hidden \nsize). DeBERTa-1.5B is fine-tuned on a new virtual adversarial training method, \ncalled Scale-invariant-Fine-Tuning (SiFT), that normalizes word embeddings into \nstochastic vectors, which reveals to be beneficial for improving training stability in \ndownstream tasks.\nIn GLUE tasks, DeBERTa outperforms BERT, RoBERTa and ELECTRA (cf. \nSect.  2.3), with much less pre-training data than RoBERTa and ELECTRA (78G \nagainst 160G). DeBERTa shows outstanding performance also in SQuAD, RACE \nand SWAG benchmarks. DeBERTa-1.5B was tested on SuperGLUE  (Wang et  al. \n2019) benchmarks, where it surpassed for the first time human performance in terms \nof macro-average score.\nIn  (He et  al. 2023), a novel version of DeBERTa, called DeBERTaV3, is pro-\nposed. It is based on a previous model update, DeBERTaV2, where the major \nchanges are in the vocabulary, tokenizer, input encoding and shared projection \nmatrices (position and content) in attention layers. DeBERTaV3 is pre-trained using \nthe same settings as the original DeBERTa but following ELECTRA (Clark et al. \n2020) in the use of a generator-discriminator setting and replacing the MLM task \nwith the ELECTRA’s pre-training task, called Replace Token Detection. In eight \nGLUE tasks, DeBERTaV3-large (24 layers, 12 attention heads, 1024 hidden size) \nobtains an average score of 91.37 on development set, outperforming ELECTRA \nand RoBERTa. It was also tested on SQuAD v2.0, RACE and SWAG, outperform-\ning previous models on development set.\nIn (He et al. 2023), a multilingual version of DeBERTAV3-base (12 layers, 12 \nattention heads, 768 hidden size), called mDeBERTa-base, is also proposed. It has \nthe same model structure as DeBERTAV3-base, and it was trained with the same \ncorpus as XLM-RoBERTa  (Conneau et  al. 2020) and the same SentencePiece \nvocabulary as mT5 (Xue et al. 2021). The pre-training setting is similar to XLM-\nRoBERTa except for the number of steps. It was tested on XNLI  (Conneau et  al. \n2018) and obtained higher results than previous base models (like mT5-base and \nXLM-RoBERTa-base) in all languages and under both zero-shot cross-lingual-\ntransfer and zero-shot translate-train-all settings.\nDistilBERT. DistilBERT  (Sanh et  al. 2019) stands as a counterpart of the ever \nlarger pre-trained language models, demonstrating that a smaller but still general-\npurpose language model can achieve comparable performance in much less time. 16 \nThis leads to significant benefits, since large-scale models often have hundred mil-\nlion parameters and their training phase have considerable computational costs and \nmemory requirements.\nThe key aspect of DistilBERT corresponds to the knowledge distillation, a trans-\nfer learning technique whereby the knowledge of a larger pre-trained model, called \n“teacher”, is passed to a smaller model, the “student”, through a training phase \n16 https:// github. com/ huggi ngface/ trans forme rs/ tree/ main/ examp les/ resea rch_ proje cts/ disti llati on.\n880 C. M. Greco, A. Tagarelli \n1 3\nwherein the student has to reproduce the same results of the teacher. In DistilBERT, \nthe training objective is a linear combination of three loss functions: distillation loss, \nmasked language modeling loss (same as BERT) and cosine embedding loss. The \ndistillation loss is a cross-entropy loss function taking into account both the prob-\nabilities estimated by the teacher and the student:\nwhere ti (resp. si ) is the probability coming from the teacher (resp. from the student). \nCosine embedding loss is used to measure the degree of similarity of two input vec-\ntors and aims to align the directions of student and teacher vectors.\nThe architecture of DistilBERT is the same as BERT except for the absence of \nthe token embedding and the pooler layer. Moreover, the number of layers is halved \ncompared to BERT. Following RoBERTa, the model was trained using dynamic \nmasking, without the NSP task and with very large batches, but keeping the same \ncorpora as the original BERT. As a result, DistilBERT has 40% fewer parameters \nthan BERT-base and is 60% faster. Further details about pre-training parameter set-\nting are reported in Table 1.\nIn the GLUE benchmark, DistilBERT obtains almost the same score as BERT \nin the development set. In SQuAD v1.1, it obtains good results, using a second \nknowledge distillation in the fine-tuning phase with BERT fine-tuned on SQuAD as \nteacher model.\nMiniLM. MiniLM  (Wang et  al. 2020b) also adopts a knowledge distillation \napproach. By limiting the distillation process to the last teacher level, MiniLM aims \nto alleviate the difficulty in performing a layer mapping between the teacher and the \nstudent and to make the number of the student layers more flexible. Moreover, the \nmodel introduces the scaled dot-product between values, in addition to the scaled dot-\nproduct of queries and keys, to transfer the value relations (Fig.  7), thus achieving a \ndeeper mimicking of the teacher as well as introducing more knowledge about word \n(6)L d =\n/uni2211.s1\ni\nti log (si),\nFig. 7  Deep self-attention distillation of MiniLM (Wang et al. 2020b)\n881\n1 3\nTransformer-based language models for AI and law\ndependencies. Optionally, an intermediate-size student model (“the teacher assis-\ntant”) can be interposed between the teacher and the student in the knowledge distil-\nlation process to alleviate the size gap between student and teacher and improve the \nmodel performance of smaller students. The knowledge of the teacher is hence dis-\ntilled into the teacher assistant, which guides the training of the student.\nMiniLM is trained for mono and multi-lingual tasks. In the mono-lingual set-\nting, the teacher is the BERT-base, while for the multi-lingual setting the teacher is \nXLM-RoBERTa-base. The model proves to outperforms DistilBERT on SQUAD \nv2.0 and several tasks of GLUE, and achieves competitive performance on XNLI \nw.r.t. mBERT.\nAlBERT. AlBERT  (Lan et  al. 2020) stems from the same considerations of the \nDistilBERT’s authors regarding the excessive size of pre-trained language mod-\nels, which raises concerns in terms of memory and time costs. 17 In ALBERT, two \nparameter reduction techniques are proposed to mitigate the memory consumption \nand speed up the training time.\nThe first technique is a factorized embedding parameterization, which consists in \nthe decomposition of word embeddings in smaller matrices. In BERT, the size E of \nthe context-independent wordpiece embeddings is equivalent to the context-depend-\nent hidden layer size H ( E ≡ H ); however, it is assumed in AlBERT that these sizes \nshould be untied, and in particular H ≫ E should hold. In the factorized embedding \nparameterization, vocabulary vectors of size V are first projected in a lower-dimen-\nsional space of size E and then projected to the hidden space of size H. Therefore, \nthe embedding parameters are reduced from O(V × H) to O(V × E + E × H) , which \nis a significant reduction since H ≫ E.\nThe second technique is the cross-layer parameter sharing. By default, the \nmodel shares all parameters across layer, although other sharing techniques can be \nused. As a result, AlBERT has much smaller parameter size and the training is 1.7 \ntimes faster than BERT without severely affecting the performance. In addition, \nAlBERT configurations can be scaled up much larger than BERT. There are four \nsizes of AlBERT models: AlBERT-base (12 layers, 12 attention heads, 768 hidden \nsize), AlBERT-large (24 layers, 16 attention heads, 1024 hidden size), AlBERT-\nxlarge (24 layers, 16 attention heads, 2048 hidden size), AlBERT-xxlarge (12 lay -\ners, 64 attention heads, 4096 hidden size). BERT-base has 108M total parame-\nters, while AlBERT-base has only 12M parameters and AlBERT-xlarge has 60M \nparameters. AlBERT-xxlarge is around 70% of BERT-large parameters (235M \nagainst 334M), but is about 3 times slower because it has a larger structure. \nALBERT-large has about 18 times fewer parameters compared to BERT-large.\nTo further improve the performance, the authors replaced the NSP task with \nanother pre-training objective task, called Sentence-Order Prediction (SOP). \nGiven two segments of text from the same document, the task is to predict if \nthe second segment comes next the first segment in the document, i.e., they are \n17 https:// github. com/ google- resea rch/ ALBERT.\n882 C. M. Greco, A. Tagarelli \n1 3\nconsecutive (positive examples), or if the original order of the segments has been \nswapped (negative examples). In other words, the task is designed to train the \nmodel to understand the order of segments, focusing on discourse coherence  \npredictions.\nAlBERT is pre-trained on the same BERT corpora, with input length up to 512, \nvocabulary size of 30K. Text is tokenized using SentencePiece. Table  1 contains \nfurther details on pre-training parameters. AlBERT was fine-tuned under two set-\ntings: single-model and ensembles. It achieves better results than previous models \nlike BERT in several downstream tasks.\n2.3  Other Transformers\nBesides BERT and its close variants and extensions, there is a body of TLMs that \nextend different or additional parts of the full architecture of Transformer compared \nto BERT. Following (Yang et al. 2023), we organize our overview of such TLMs \nby distinguishing encoder-only models, decoder-only models, and encoder–decoder \nmodels, depending on whether the language modeling is also auto-regressive. Fur -\nthermore, the last part of this section contains TLMs that, regardless of the above \ncategorization, have been especially designed for a specific task, or for dealing with \nlong documents.\nAgain, please note that our overview of each of the models is necessarily brief \nand limited to those TLMs that have been used so far for legal tasks, to the best of \nour knowledge.\n2.3.1  Encoder‑only models\nELECTRA . The ELECTRA model (Clark et al. 2020) originates from the observa-\ntion that the use of a discriminative approach in the pre-training phase, which allows \nthe model to learn from all input tokens, is computationally more efficient compared \nto a generative approach like MLM in BERT.18 Within this view, a new pre-training \ntask, called Replaced Token Detection (RTD), is proposed such that, instead of using \na mask like in MLM, tokens are replaced with plausible generated tokens. In this \nway, the model is trained to discriminate real tokens from credible yet fake tokens. \nThis type of corruption overcomes the BERT’s mismatch between pre-training \nphase, in which artificial [MASK] tokens are used, and fine-tuning phase (where \nthere are no artificial tokens). The key aspect of the task is that the model can learn \nfrom all input tokens and not just from a small subset, thus achieving a significant \nspeed-up in the training phase compared with BERT.\nELECTRA consists of a generator followed by a discriminator, both consisting of \na Transformer encoder. The generator is trained to produce plausible tokens, replac-\ning real tokens in a random set of positions. The training is performed in a MLM \nmanner: given a random set of masked tokens, the generator learns to predict the \n18 https:// github. com/ google- resea rch/ elect ra.\n883\n1 3\nTransformer-based language models for AI and law\noriginal tokens. If the prediction is correct then it is regarded as “real”, otherwise \nas “fake”. The discriminator is trained to predict if the current token is real or fake \n(Fig. 8). After pre-training, the generator is discarded so that only the discriminator \nis used in the fine-tuning phase.\nThe model architecture and most hyperparameters are equal to BERT’s (see \nTable 1). ELECTRA is available in three model sizes: ELECTRA-small (12 layers, \n4 attention heads, 256 hidden size), ELECTRA-base (12 layers, 12 attention heads, \n768 hidden size), ELECTRA-large (24 layers, 16 attention heads, 1024 hidden size). \nIn the token masking process, the 15% of tokens are masked out (like in BERT), \nexcept for ELECTRA-large in which the percentage is increased to 25%. ELEC-\nTRA-large is trained with 400 steps or 1.75M steps: in the first case, it achieved \nperformance in GLUE comparably to RoBERTa, with much less computational cost \n(less then 1/4 pre-training cost), whereas in the second case, it outperforms RoB-\nERTa still with less computational costs. ELECTRA-large outperforms RoBERTa \nalso in SQuAD v2.0. Indeed, ELECTRA-base overcomes BERT-large on GLUE \ndevelopment set (85.1 against 84.0).\nXLM-RoBERTa. XLM-RoBERTa  (Conneau et  al. 2020; Pant and Dadu 2020; \nDadu and Pant 2020) is a highly scalable cross-lingual model which was designed \nto improve cross-lingual language understanding (XLU).19 It has indeed shown that \nlow-resource language performance can be improved if scaled with similar high-\nresource language during pre-training, yet there is a trade-off between the positive \ntransfer and the capacity dilution of the model (i.e., the number of parameters). \nMore precisely, since model capacity is limited by time and memory constraints, \nthe capacity reserved for a single language is diluted among all languages as the \nnumber of languages grows in a fixed-sized model, and once a certain level of dilu-\ntion is exceeded, the overall performance is degraded. Adding more capacity to the \nmodel alleviates this condition, but it is unsuitable for models with modest sizes. \nThe authors also deal with the appropriate allocation of the model capacity across \nhigh-resource and low-resource languages and the scaling of both model and vocab-\nulary size with respect to the number of languages.\nThe architecture of XLM-RoBERTa emulates the XLM approach (Conneau and \nLample 2019) for training objective, languages, and data, with few refinements to \n19 https:// github. com/ faceb ookre search/ xlm.\nFig. 8  Replaced token detection task in ELECTRA (Clark et al. 2020)\n884 C. M. Greco, A. Tagarelli \n1 3\nincrease performance at scale. The model is pre-trained with a multilingual MLM \nobjective, using monolingual data and sampling pieces of text from each language. \nThe training data is obtained from a filtered and cleaned version of CommonCrawl \nand consists of more than 2TB of data in 100 languages. Sub-word tokenization is \ndirectly applied on the raw text for all languages using a SentencePiece model. Fol-\nlowing RoBERTa, the authors show that training the model longer and with a larger-\nscale corpus yields improvements in the model performance.\nXLM-RoBERTa is available in two model sizes: XLM-RoBERTa-base (12 lay -\ners, 12 attention heads, 768 hidden size, 270M parameters) and XLM-RoBERTa-\nlarge (24 layers, 16 attention heads, 1024 hidden size, 550M parameters). In (Goyal \net  al. 2021), two larger versions are proposed: XLM-RoBERTa-xl (36 layers, 32 \nattention heads, 2560 hidden size, 3.5B parameters) and XLM-RoBERTa-xxl (48 \nlayers, 32 attention heads, 4096 hidden size, 10.7B parameters).\nXLM-RoBERTa has been tested on cross-lingual understanding tasks as XNLI, \nCoNLL (Sang 2002; Sang and Meulder 2003) and MLQA. With regard to XNLI, \nthe fine-tuning was carried out on the English training set (cross-lingual transfer) \nand also on all training sets, previously translated (translate-train-all). On CoNLL, \nthe fine-tuning considered the English set (cross-lingual transfer), each set sepa-\nrately (per-language performance) and all sets together (multilingual learning). \nXLM-RoBERTa has shown to outperform previous models like m-BERT and XLM \nin each of these cross-lingual benchmarks. Also, competitive results were obtained \nalso in monolingual benchmarks like GLUE, proving that it is possible for a multi-\nlingual model to achieve on a single language the same performance than monolin-\ngual models.\nFig. 9  Task-specific input conversion to token sequences in GPT (Radford et al. 2018)\n885\n1 3\nTransformer-based language models for AI and law\n2.3.2  Decoder‑only models\nGPT. GPT  (Radford et  al. 2018) is the first model of the Generative Pre-trained \nTransformer family developed by OpenAI.20 Based on a Transformer decoder, with \n12 layers, 12 attention heads, and 768 hidden layer size, GPT is an autoregressive \nmodel pre-trained on the BookCorpus dataset with a causal language modeling \nobjective (CLM), i.e., to predict the next token given the left-side context. The \nmodel uses a BPE vocabulary size of 40K and maximum sequence length of 512 \ntokens. To address the downstream tasks in the fine-tuning phase, the inputs of the \ndifferent tasks are converted into token sequences through the use of special tokens, \nin order to be processed by the model (Fig.  9). The model achieved better perfor -\nmance then previous competitors at that time on several tasks, like text classifica-\ntion, natural language inference, question answering, semantic similarity.\nGPT-2. GPT-2 (Radford et al. 2019) is the successor of GPT, conceived to perform \ndownstream tasks in a zero-shot learning setting. Like GPT, GPT-2 is pre-trained on \na CLM task but using WebText, a larger amount of free text obtained through web \nscraping and consisting of millions of webpages whose selection has been curated \nby humans. The architecture of GPT-2 is largely based on GPT, but it has more than \none order of magnitude of additional parameters (1.5B parameters). The context size \nis increased to 1024 tokens and the vocabulary reaches 50K tokens.\nTo address downstream NLP tasks, the output of GPT-2 is conditioned on the \ninput and task type. That is, in addition to the text, the model receives as input \nindications of the task to be performed. For example, to induce the summarization \nbehavior, the text “TL;DR:\" is added after the text to be summarized.\nGPT-2 has shown to outperform previous state-of-the-art models in several lan-\nguage modeling datasets (such as LAMBADA  (Paperno et  al. 2016), enwik8 and \ntext8).21 On reading comprehension tasks, the model proves to be comparable to \nsupervised baselines in a zero-shot setting.\nGPT-3. GPT-3 (Brown et al. 2020a) significantly increases the size of GPT mod-\nels to 175B parameters, about two orders of magnitude more than the predecessor \nGPT-2. The pre-training approach is similar to GPT-2, but the data are larger and \nmore heterogeneous, as they contain the CommonCrawl dataset and high-quality \nreference corpora, including an expanded version of WebText, internet-based books \ncorpora and the English Wikipedia. GPT-3 is evaluated in zero-shot, one-shot and \nfew-shot settings. In zero-shot setting, a description of the task to be performed is \ngiven to the model, whereas for one-shot and few-shot settings one or few examples \nof how the task is to be executed are provided to the model at inference time, after \nwhich the model can be prompted to execute the task on a new example.\nGPT-3 is evaluated on several traditional language model benchmarks, e.g., \nLAMBADA, achieving better results compared to the previous fine-tuned \n20 https:// openai. com/.\n21 enwik8 and text8 are available at http:// www. mattm ahoney. net/ dc/ text. html.\n886 C. M. Greco, A. Tagarelli \n1 3\nstate-of-the-art models in many cases. Results also show that scaling up the model \ngreatly improves the performance in task-agnostic/few-shot setting, becoming even \ncompetitive with fine-tuned competitors in some NLP tasks. For example, on ques-\ntion-answering tasks, GPT-3 gets mixed results, with the zero-shot model outper -\nforming the fine-tuned T5-11B on TriviaQA but being outperformed on NQ. In \nmachine translation tasks, GPT-3 exceeds previous unsupervised competitors when \ntranslating to English, but underperforms when translating in the opposite direction. \nOn SuperGLUE, the few-shot model obtains higher performance then the fine-tuned \nBERT−large on four tasks and almost reaches the state-of-the-art at that time (a \nfine-tuned 11B-parameter model) in two tasks. Regarding text summarization, some \ncritical issues on the GPT-3 samples are found, e.g., the presence of semantically \ndocument-level repetition and loss of coherence on long passages.\nChatGPT. Perhaps the world’s most controversial AI language tool of our time, \nChatGPT22 is a GPT-based model trained to interact with humans in a conversational \nway. ChatGPT shares the underlying architecture with InstructGPT  (Ouyang et  al. \n2022), which is specifically designed and fine-tuned for generating detailed instruc-\ntions given a prompt. In particular, like InstructGPT, ChatGPT utilizes the Reinforce-\nment Learning from Human Feedback (RLHF) technique, which combines super -\nvised fine-tuning and reinforcement learning by exploiting the preferences of human \ntrainers as reward signals in the training process. First, dialogue samples are obtained \nwith the support of the human trainers, which suggest the desired output behavior \nfor the given prompt and help the model formulate the responses in a conversational \nscenario. The model is, therefore, fine-tuned on a dataset obtained by mixing such \nsamples with the InstructGPT data converted in a dialogue format. A reward model \nis then obtained through reinforcement learning on comparison data. The latter con-\nsists of prompt and relative model responses ranked by human trainers on the basis of \nresponse quality. The reward models are then employed to fine-tune the model using \nProximal Policy Optimization, a reinforcement learning algorithm (Schulman et al. \n2017). The result is a model that allows for human dialogue on a wide variety of top-\nics, answering follow-up questions while maintaining a conversation flow.\nGPT-NeoX-20B. GPT-NeoX-20B (Black et al. 2022) is an autoregressive language \nmodel with 20B parameters and trained on Pile (Gao et al. 2021).23 The GPT-NeoX-\n20B architecture largely follows GPT-3 with some differences. It applies the rotary \npositional embeddings (Su et al. 2021) on the first 25% of embedding vector dimen-\nsions, which consist in rotating the embedding space in such a way that the atten-\ntion between two tokens is linearly dependent on their distance in the text. Moreo-\nver, GPT-NeoX-20B computes attention and feed-forward layers in parallel, whose \noutcomes are subsequently summed. The BPE tokenizer and the vocabulary size is \nsimilar to GPT-2, but the tokenizer is trained on the Pile dataset and, unlike GPT-2, \nhandles the presence of prefix spaces and repeated space tokens.\n23 https:// github. com/ Eleut herAI/ gpt- neox.\n22 https:// openai. com/ blog/ chatg pt.\n887\n1 3\nTransformer-based language models for AI and law\nGPT-NeoX-20B is evaluated on several benchmarks (e.g., LAMBADA and Trivi-\naQA), as well as mathematical and knowledge-based tasks. Results demonstrate that \nthe model reaches higher performance than the similarly sized GPT-3 and fairseq \nmodels (Artetxe et al. 2022) when evaluated in five-shot setting.\nmGPT. Shliazhko et al. (2022) propose a GPT-like model in two size versions (1.3B \nand 12B parameters) for multilingual tasks. 24 mGPT aims to reproduce the archi-\ntecture of GPT-3 starting from a GPT-2 implementation, based on the information \ndescribed in (Brown et al. 2020a). The resulting models are trained on 60 languages \nthrough Wikipedia and Colossal Clean Crawled (C4) corpora, and evaluated under \nzero-shot and few-shot settings (as in (Brown et al. 2020a)) on several multilingual \nbenchmarks, including text classification, text generation and sequence labeling. On \nsequence labeling tasks, both zero-shot and few-shot settings show high scores. On \nmost text classification tasks, the model is competitive with a state-of-art multilin-\ngual model XGLM (1.7B parameters) (Lin et al. 2022). Overall, results show that \nlarger models correspond to better generation abilities for all given languages. In \naddition, experimental evaluation on knowledge probing indicates a certain ability \nof mGPT to preserve factual knowledge.\n2.3.3  Encoder–decoder models\nT5. Text-to-Text Transformer (T5) (Raffel et al. 2020) provides a unified approach in \nwhich one single model can be used for every task with the same objective, the same \ndecoding process and the same training procedure. 25 To specify the current task, a \ntask-specific prefix is added to the input text before feeding it to the model (Fig. 10).\nThe architecture of T5 is quite similar to the original Transformer (Vaswani et al. \n2017), with few changes regarding the normalization layer and the position embed-\nding scheme. The baseline is similar to BERT-base in terms of size and configura-\ntion, for both the encoder and the decoder, but with 220 million parameters in total, \n24 https:// github. com/ ai- forev er/ mgpt.\n25 https:// github. com/ google- resea rch/ text- to- text- trans fer- trans former.\nFig. 10  T5 diagram (Raffel et al. 2020). Every task is considered as text. A task-specific prefix is added \nto the input text to specify the current task\n888 C. M. Greco, A. Tagarelli \n1 3\nwhich are twice BERT-base parameters, as T5 contains two layer stacks instead of \none. The text encoding method is SentencePiece (Kudo and Richardson 2018) with \na multilingual vocabulary of 32K wordpieces. \nThe training objective for pre-training and fine-tuning is a maximum likelihood \nobjective using teacher forcing and a cross-entropy loss. More specifically, T5 uti-\nlizes a BERT-style denoising objective that randomly samples and drops out spans \nof tokens (loosely inspired by SpanBERT), with corruption rate of 15% of tokens \nand varying the span length, and the aim is to predict the dropped-out tokens. The \nspan of tokens in replaced with only one special token. Figure  11 shows the experi-\nmentation choices for the pre-training objective.\nFor the model architecture, the authors reviewed and compared several Trans-\nformer variants (encoder–decoder, language model, prefix LM, see Fig.  12) and \neventually stated that the original encoder–decoder is the most suitable form for \nthe text-to-text framework. In addition, they found that a small domain-specific \nunlabeled dataset, repeated many times in the pre-training, can degrade perfor -\nmance in some downstream tasks. For this reason they adopted a new massive \ndataset called Colossal Clean Crawled Corpus (C4), consisting in hundreds of \nFig. 11  A flowchart of the choices about unsupervised objective in T5 (Raffel et al. 2020). After accurate \nexperimentation, the authors opted for a BERT-style approach, with spans replacing as corruption strate-\ngies, 15% corruption rate and average corrupted span length chosen in {2,3,5,10}\nFig. 12  Illustrations of the transformer variants reviewed and compared in (Raffel et al. 2020). On the \nleft, the encoder–decoder architecture which uses the fully-visible masking when the encoder is involved, \nwhile the causal attention is used inside the decoder. In the middle, a standard language model which \nuses a causal masking. On the right, the Prefix LM which involves fully-visible masking over the input \nand causal masking for the output\n889\n1 3\nTransformer-based language models for AI and law\n26 https:// github. com/ google- resea rch/ multi lingu al- t5.\ngigabytes of clean, high-quality text picked from the web. Using C4 in the pre-\ntraining phase led the model to be flexible enough to be fine-tuned to a variety \nof tasks while obtaining state-of-the-art results. In this respect, T5 is provided in \nfive versions: T5-base (i.e., the baseline, 12 layers, 12 attention heads, 768 hidden \nsize, 220M parameters), T5-small (6 layers, 8 attention heads, 512 hidden size, \n60M parameters), T5-large (24 layers, 16 attention heads, 1024 hidden size, 770M \nparameters), T5-3B  (24 layers, 32 attention heads, 1024 hidden size, 3B param-\neters), T5-11B  (24 layers, 128 attention heads, 1024 hidden size, 11B parameters). \nIt should be noted that C4 is regarded as big enough to allow the exploration of \nthe effect of scaling up the amount of pre-training without overfitting, in particular \nwhen training on more data, with larger versions of T5 or ensemble of models. \nHowever, small models can still be useful when limited computational resources \nare available in fine-tuning.\nT5 has been tested on various downstream tasks, such as classification, question-\nanswering, translation and summarization. When fine-tuning on GLUE (as well as \non SuperGLUE), the datasets of all the benchmarks are concatenated so that the \nmodel is fine-tuned just once, using small batch size in order to mitigate overfit-\nting in low-resource tasks. For task with long output sequences, the performance \nimproved with the use of beam search, while the other task are reported with greedy \ndecoding. T5 achieves state-of-the-art result in GLUE and overcomes RoBERTa and \nAlBERT in SuperGLUE and SQuAD.\nA multilingual variant of the model, called mT5, is also available  (Xue et  al. \n2021).26 It was pre-trained on a Common Crawl dataset, called mC4, based on 101 \nlanguages. The architecture and training procedure is quite similar to T5. The model \nis available in four sizes: mT5-small (300M parameters), mT5-base (580M param-\neters), mT5-large (1.2B parameters), mT5-xl (3.7B parameters), mT5-xxl (13B \nparameters). It was tested on several XTREME  (Hu et  al. 2020) tasks, including \nXNLI and XQuAD (Artetxe et al. 2020). For each task, three fine-tuning settings are \nconsidered: zero-shot, translate-train, in-language multitask. mT5 achieved strong \nperformance on each task, overcoming previous models like m-BERT.\nBART . BART (Lewis et al. 2020) is a denoising auto-encoder implemented as a \nsequence-to-sequence model, combining bidirectional and auto-regressive Trans-\nformers.27 More precisely, it consists of a bidirectional encoder, following BERT, \nFig. 13  BART architec-\nture (Lewis et al. 2020)\n27 https:// github. com/ pytor ch/ fairs eq.\n890 C. M. Greco, A. Tagarelli \n1 3\nand a left-to-right auto-regressive decoder, following GPT (Fig.  13). The architec-\nture follows the standard sequence-to-sequence Transformer, except for the activa-\ntion function and the initialization of the parameters which are set up according \nto the GPT instructions. It is also related to BERT except for the decoder lay -\ners, in which there is an additional cross-attention with the final hidden layer of \nthe encoder, and the absence of the additional feed-forward network before word \nprediction.\nBART is available in a base size (6 layers, 12 attention heads, 28 768 hidden size) \nand a large size (12 layers, 16 attention heads, 29 1024 hidden size). BART is pre-\ntrained by first corrupting input text and then training the model to get back the \noriginal document. A key advantage of BART is the noising flexibility, i.e., it allows \nfor arbitrary type of document corruption, including changing its length. This makes \nit possible to mask single tokens (like in the MLM task) as well as the entire input. \nThe best results have been obtained shuffling the order of the original sentences and \nmasking spans of text. This approach generalizes both MLM and NSP pre-training \ntasks. The masking method is similar to SpanBERT, but in this case the sampling \nis obtained using a different distribution (Poisson) and each span is replaced with a \nsingle [MASK] token.\nBART can be fine-tuned for a wide range of downstream tasks, such as sequence \nclassification, token classification, sequence generation and machine translation. \nDocuments are tokenized with BPE, the corruption rate is the 30% of tokens for \neach document, and all sentences are permuted. BART obtains similar results to \nRoBERTa in GLUE and SQuAD benchmarks, and achieves new state of the art \ncompared to previous models in abstractive dialogue, summarization and abstractive \nQA.\nIn (Liu et al. 2020), a multilingual version of BART is provided, called mBART. \nIt is the application of BART to large-scale monolingual corpora across many lan-\nguages. mBART represents the first multi-language denoising pre-training method \nfor a complete sequence-to-sequence model, which makes it possible a direct fine-\ntuning for supervised and unsupervised machine translation without task-specific \nmodifications. mBART recognizes the language through a language id token ( ⟨LID⟩ ) \nplaced at the beginning of the sentence. The authors built several versions of the \nmodel such as mBART25 (pre-trained on 25 languages) and mBART06 (pre-trained \non 6 European languages).\nMASS. MASS  (Song et  al. 2019) introduces a masked sequence-to-sequence \npre-training, in which the encoder maps a sequence to several masked fragments, \neach composed of consecutive tokens, and the decoder predicts the masked frag-\nments. The encoder and decoder are pre-trained simultaneously and in two stages: \nthe encoder is forced to understand the meaning of the masked tokens to predict \nthe tokens on the decoder side, while the decoder has the input tokens completely \nmasked, so it relies primarily on the representation of the input passed by the \n28 from https:// huggi ngface. co/ faceb ook/ bart- base/ blob/ main/ config. json.\n29 from https:// huggi ngface. co/ faceb ook/ bart- large/ blob/ main/ config. json.\n891\n1 3\nTransformer-based language models for AI and law\n31 https:// github. com/ micro soft/ unilm.\n30 https:// www. statmt. org/ wmt16/ trans lation- task. html.\nencoder. More precisely, the decoder predicts the first token of the masked frag-\nment based solely on the encoder input representation, while subsequent tokens are \npredicted based on the encoder representation and the previously predicted frag-\nment tokens (Fig. 14). This enables the model to learn generating sequences during \npre-training.\nAvailable in a base size (6 layers, 12 attention heads, 768 hidden size) and a mid-\ndle size (6 layers, 16 attention heads, 1024 hidden size), MASS is pre-trained on \nthe WMT monolingual corpus,30 and fine-tuned on several benchmarks for machine \ntranslation, text summarization, and conversational response generation.\nUniLM. UniLM  (Dong et  al. 2019) is a Transformer model pre-trained on unidi-\nrectional, bidirectional and sequence-to-sequence prediction tasks. 31 It uses self-\nattention masks to control the context of the prediction, thus enabling a single archi-\ntecture for uni/bidirectional and sequence-to-sequence approaches. The parameter \nsharing across the different language modeling objectives allows the model to learn \nmore general text representations (Fig.  15). Different segment embeddings are used \nbased on the pre-training task. Two special tokens, namely [SOS] and [EOS], are \nadded at the beginning and at the end of each segment in the input, respectively. \nSuch tokens play an important role for both natural language understanding and gen-\neration downstream tasks.\nUniLM is initialized and pre-trained following BERT-large. For language under-\nstanding tasks, it is fine-tuned as a bidirectional encoder, where the contextual \n[SOS] embedding represents the input encoding. For language generation tasks, it is \nfine-tuned on a sequence-to-sequence task, where the [EOF] token can used to learn \nwhen to stop the decoding process. UniLM is evaluated on several benchmarks, such \nas GLUE, SQuAD and CNN/DailyMail. Results demonstrate that UniLM is com-\npetitive against BERT on GLUE and better than BERT on SQuAD 2.0 development \nset, and outperforms previous state-of-the-art model on several generative tasks \n(CNN/DailyMail included).\nInspired by UniLM, Bao et al. (2020) propose UniLMv2, which is pre-trained on \nbidirectional and sequence-to-sequence tasks in a unified manner with a pseudo-\nmasked language modeling (PMLM) procedure. The model parameters and the con-\ntext encodings are shared across the two tasks. Given a corrupted input with masked \nFig. 14  MASS architecture (Song et al. 2019)\n892 C. M. Greco, A. Tagarelli \n1 3\ntokens, UniLMv2 uses standard masks and an autoencoding approach to learn the \nrelations between masked tokens and the context. In addition, it uses pseudo masks \nand a partially autoregressive modeling approach to learn the relations between \nmasked spans or tokens (token-to-token, token-to-span, and span-to-span relations). \nThe accessible context for each token in the partially autoregressive modeling is \ncontrolled according to a factorization order. Similarly to (Yang et al. 2019), for each \ntext in input a random factorization order is sampled. The model predicts one token \nor a span at each factorization step (for this reason it is partially autoregressive). The \nmasks used in autoencoding pre-training provide global masking information, which \ncan be used to access the position embeddings in the factorization steps. A stand-\nard mask, named [M], is employed to corrupt input tokens in the autoencoding pre-\ntraining. To handle factorization steps in the partially autoregressive pre-training, \nthe [P] masks are appended to the input without replacing the original tokens. Such \nmasks have the same position embedding of the corresponding tokens and their final \nembeddings are used for the predictions.\nUniLMv2 size is similar to that of BERT-base (i.e., 12 layers with 12 attention \nheads and 768 hidden size). Fine-tuned and evaluated on several language under -\nstanding and generation tasks, UniLMv2 outperforms the base version of BERT, \nRoBERTa and XLNet on SQuAD and eight tasks of GLUE. On CNN/DailyMail, it \nproves to be more effective than other TLMs such as UniLM, MASS (base version) \nand BERTSUMABS (cf. Sect. 2.3.4).\nFig. 15  UniLM Pre-training (Dong et al. 2019). A unique model with shared parameters across all the \npre-training objectives is employed. Self-attention masks control the context for the prediction, based on \nthe objective\n893\n1 3\nTransformer-based language models for AI and law\n2.3.4  Task‑specific and long range models\nXLNet. XLNet  (Yang et  al. 2019) exploits the advantages of autoregressive \nlanguage modeling and autoencoding denoising. Instead of masking the input, \nXLNet enables bidirectionality by permuting the order of context factorization, \nand is trained to predict a token in an auto-regressive manner, but the predic-\ntion order is randomized and not sequential. This means that the model predicts \na token based on previously predicted tokens, whose positions in the text may be \nbefore or after the token to be predicted. In this way, the model is forced to learn \npredicting the target with a randomly ordered context, thus enabling the bidirec-\ntionality. XLNet can be employed for tasks that include text of any length. The \narchitecture is inspired by Transformer-XL (Dai et al. 2019), from which it takes \nthe segment-level recurrence mechanism and relative positional encoding scheme. \nThe segment-level recurrence mechanism consists in caching the representations \ncomputed for the previous segment so that they can be used as extended context \nfor processing the next segment, thus increasing the maximum perceivable dis-\ntance between dependencies. The relative positional encoding scheme adapts the \ninput positional encodings to the segment-level recurrence mechanism. XLNet \nalso introduces a new self-attention mechanism, called two-stream self-attention, \nwhich consists in a standard self-attention (content stream attention) and a query \nstream attention that mimics the masking behavior by obscuring the content of \nthe token to be predicted but keeping its positional encoding visible. Figure  16 \nshows the difference between content stream attention and query stream attention: \nthe former has access to the representations of both the context and the query \ntoken, while the latter has access only to the contents of previous tokens and to \nthe positional encoding of the query.\nXLNet is pre-trained with various pre-training corpora, which also include the \nBERT pre-training corpora. Two sizes of the model are available: XLNet-large (24 \nlayers, 16 heads and 1024 hidden size) and XLNet-base (12 layers, 12 heads and \nFig. 16  XLNet a content stream attention, b query stream attention, and c two-stream self-atten-\ntion (Yang et al. 2019)\n894 C. M. Greco, A. Tagarelli \n1 3\n768 hidden size). XLNet achieves substantial improvements over BERT and RoB-\nERTa on various benchmarks, such as RACE, SQuAD v2.0 and GLUE.\nLongformer. Longformer  (Beltagy et  al. 2020) is specifically designed to handle \nlong text sequences.32 Its attention mechanism combines local attention and global \nattention (Fig. 17), reducing the standard self-attention operations and scaling com-\nplexity from quadratic to linear with respect to the sequence length. Local attention \nis necessary to get contextual representations. In particular, the model uses a slid-\ning window that can be dilated, i.e., it can have gaps of positions. For each atten-\ntion head, different dilation configuration and different window size are provided. \nDifferent dilatation allows some heads to focus on local context through compact \nwindows, while the others can focus on more distant contexts through enlarged \nwindows. Global attention is added on few elected tokens and is crucial to get full \nsequence representations that are required for many NLP tasks (e.g., QA, document \nclassification). How to select tokens for global attention is task-specific; for exam-\nple it may consider the [CLS] token in classification tasks or all query tokens in \nquestion answering tasks. For auto-regressive tasks, the model has small non-dilated \nwindows in lower layers, and increases the size and dilation (only on 2 heads) mov -\ning up to higher layers. The model is evaluated on the character-level text8 and \nenwik8 datasets.\nLongformer is pre-trained with MLM objective, starting from a RoBERTa check-\npoint with some minimal non-architectural changes to add the new attention pattern. \nBesides English Wikipedia and Bookcorpus, the pre-training includes a portion of \nRealNews, consisting of documents longer than 1200 tokens, and a portion of Stories \ncorpus. The tokenizer is the same as RoBERTa, as well as the vocabulary, except for \nthe introduction of special tokens for question answering data. The sliding window \nis set with a size of 512 and extra position embeddings are added (up to 4096, while \nRoBERTa has a maximum of 512) to support longer documents. Thereby, the model \ncan handle up to 8 times longer documents than BERT and RoBERTa. This leads \nLongformer to outperform RoBERTa in tasks where long-document datasets are \nused, such as WikiHop (Welbl et al. 2018) and TriviaQA for question-answering, \nOntoNotes (Pradhan et al. 2012) for co-reference resolution. Longformer is provided \nin two model sizes: Longformer-base (12 layers, 12 attention head, 768 hidden size) \nand Longformer-large (24 layers, 16 attention head, 1024 hidden size).\nFig. 17  Longformer attention pattern (Beltagy et al. 2020)\n32 https:// github. com/ allen ai/ longf ormer.\n895\n1 3\nTransformer-based language models for AI and law\nAn encoder–decoder variant of Longformer, dubbed Long-\nformer–Encoder–Decoder (LED), handles long text sequences in sequence-to-\nsequence tasks. The attention pattern of Longformer is applied to the Encoder part, \nwhile the Decoder part contains the full attention of standard Transformer. LED \nparameters are initialized following BART architecture in terms of number of lay -\ners and hidden sizes, with position embedding length extended to 16K tokens and \nenhanced initialization, in order to deal with longer texts. The model is released in \ntwo versions: LED-base (6 layers, 12 attention head, 768 hidden size) and LED-\nlarge (12 layers, 16 attention head, 1024 hidden size).\nBigBird. BigBird (Zaheer et al. 2020) is a Transformer-based model that, like Long-\nformer, can reduce the computational and memory cost by passing from a full-atten-\ntion mechanism to a sparse-attention mechanism. 33 In particular, the model uses a \ncombination of three attention patterns: random attention, sliding window attention \nand global attention (Fig.  18). In the random attention, each query focuses on a set \nof random keys. In the sliding window attention, each query focuses on a set of their \nright and left neighboring tokens selected by a sliding window. In the global atten-\ntion, some tokens (from the text or special tokens like [CLS]) attend to the whole \nsequence. Formally, given an input sequence X =( x1 ,… ,xn)∈ ℝn×d , the general-\nized attention mechanism for xi is as follows:\nwhere H is the number of attention’s heads, W(h)\nQ ∈ ℝd×m , W(h)\nK ∈ ℝd×m and \nW(h)\nV ∈ ℝd×d are query, key and value matrices of the h-th head, respectively, N(i) \ndenotes the set of keys attended by the query i, while xi and XN (i) are the vector rep-\nresentations of query i and its keys, respectively. Like Longformer, this type of \nattention mechanism can reduce the Transformer complexity from quadratic to lin-\near in the number of tokens, allowing the model to scale to much longer sequences \nin input, up to 8 times longer than previous Transformer-based models. The authors \nstated that sparse attention mechanisms have the same power and expressiveness as \n(7)Att(X)i = xi +\nH/uni2211.s1\nh=i\nsoftmax((xiW (h)\nQ ) ⋅ (XN (i)W (h)\nK )⊤) ⋅ (XN (i)W (h)\nV ),\n33 https:// github. com/ google- resea rch/ bigbi rd.\nFig. 18  BigBird attention pattern (Zaheer et al. 2020)\n896 C. M. Greco, A. Tagarelli \n1 3\nfull-attention mechanisms, demonstrating that, on the one hand, when used in stan-\ndalone encoders like BERT they are universal approximate methods of sequence-to-\nsequence functions and, on the other hand, that the sparse encoder–decoder models \nare Turing-complete under standard assumptions regarding precision.\nIn the pre-training settings, BigBird follows BERT and RoBERTa to create \nbase and large models. It is pre-trained using the MLM objective and four corpora \n(Books, CC-News, Stories and Wikipedia), starting from a RoBERTa checkpoint. \nThe model is available in two base versions: BigBird-ITC-base (in which only inter-\nnal tokens in the text attend global attention) and BigBird-ETC-base (in which only \nthe external token like [CLS] attends global attention). Both versions have 12 layers, \n12 attention heads and hidden size of 768. Each version has its own setting regard-\ning the number of tokens with global attention and random attention as well as the \nwindow size for local attention. Two large versions are also available: BigBird-ITC-\nlarge and BigBird-ETC-large, both with 24 layers, 16 attention heads and 1024 hid-\nden size.\nBigBird has been fine-tuned on QA challenging datasets (Natural Questions, \nHotpotQA-distractor, TriviaQA-wiki, WikiHop) outperforming competitors like \nLongformer and RoBERTa. In document classification and GLUE tasks, BigBird \nproves to be more advantageous than existing methods when longer documents and \nfewer training examples are being used (e.g., Arxiv (He et al. 2019)). Moreover, like \nLongformer, an encoder–decoder setup of BigBird is also proposed, in which the \nsparse attention mechanism is introduced only in the Encoder side.\nMonoT5. Inspired by the success of T5, Nogueira et al. (2020) applied the model to \ndocument ranking, actually using a sequence-to-sequence model instead of the com-\nmonly adopted encoder-only pre-trained architectures. 34 Document ranking is con-\nsidered as a relevance prediction task, i.e., given a document and a query, to assign \na relevance score that indicates how much the document is relevant to the query. \nThe new model, called monoT5, is fine-tuned to predict if the candidate document is \nrelevant to the query, i.e., the target tokens are “true\" and “false\", and the underlying \nlogits of the target tokens are considered as relevance probabilities for ranking. The \ncore idea is to take advantage of the latent knowledge captured in the pre-training \nphase by connecting fine-tuned latent representations with output target tokens.\nMonoT5 is fine-tuned on MS MARCO passage dataset (Nguyen et al. 2016) and \nthen applied on other three datasets (Robust04  (Voorhees 2004), Core17  (Allan \net al. 2017) and Core18 (Naseri et al. 2018)), experimenting with zero-shot docu-\nment ranking. Three sizes of T5 are considered in the fine-tuning process: T5-base, \nT5-large and T5-3B. Due to the high computational costs, it was not possible to fine-\ntune T5-11B. T5 is used as a re-ranker, applied directly to the output of BM25 model \n(and variants). In the experimentation, the zero-shot transfer-learning approach on \nthe three dataset outperforms the in-domain cross-validation approach used in previ-\nous models. Furthermore, the study reveals that, in data-poor conditions, the model \noutperforms the classic encoder-based approaches.\n34 https:// github. com/ casto rini/ pygag gle/.\n897\n1 3\nTransformer-based language models for AI and law\n36 https:// github. com/ nlpya ng/ PreSu mm.\nDPR. Dense Passage Retriever (DPR)  (Karpukhin et  al. 2020) is based on two \nBERT-base encoders and focuses on text retrieval in open-domain question-answer-\ning, where the task is to select query-relevant passages from candidate contexts. 35 \nThe first encoder maps the passage to vector in a low-dimensional and continuous \nrepresentation space of a given size, and builds an index for all the passages. The \nsecond encoder maps the query to a real-valued vector, with the same fixed size as \nthe passage vectors, and retrieves from the index the passages whose vectors are the \nclosest to the query vector in terms of inner-product similarity.\nIn general, dense vector representation needs multiple query-passage labeled \npairs. In DPR, the focus is to find a proper training scheme that can allow the use \nof a relative small number of query-passage pairs. For this purpose, the embeddings \nare built so that the inner product between query and relevant passages is maximized \nby using a objective that compares all pairs in a batch. The embeddings correspond \nto the representation of the [CLS] tokens.\nDPR is trained with the English Wikipedia corpus, where each article is split into \npassages and each passage is preceded by the article’s title along with the [SEP] \ntoken. Three strategies of positive/negative sampling have been considered: random \npassages from the text corpus, positive passages of other questions (named gold pas-\nsages) and top BM25 passages that do not contain the answer. Best results corre-\nsponded to the use of gold passages in the same mini-batch and one BM25 passage.\nDPR has been tested on five question-answering datasets: Natural Ques-\ntions (Kwiatkowski et al. 2019), TriviaQA (Joshi et al. 2017), WebQuestions (Ber -\nant et  al. 2013), CuratedTREC  (Baudis and Sedivý 2015) and SQuAD v1.1. In \nTriviaQA, WebQuestions and CuratedTREC there are only query-answer pairs, so \nthat DPR uses the highest BM25 passage containing the answer as positive pas-\nsage. In SQuAD and Natural Questions, the passages are managed differently from \nthe DPR’s pool of candidates, so that a matching and replacing process of the gold \npassages with the corresponding passage in the pool is performed. DPR was also \ntrained on each specific dataset as well as on all datasets, except SQuAD, with the \nlatter demonstrating to perform better in retrieving top-20 and top-100 passages than \nsingle-dataset DPR.\nBERTSUM. BERTSUM  (Liu and Lapata 2019a) is a BERT-based approach \nfor extractive and abstractive summarization. 36 The base architecture consists \nof a sentence-level BERT encoder. Differently from the original BERT, BERT -\nSUM introduces a [CLS] token at the beginning of each input sentence (Fig.  19). \nTwo interspersed segment embeddings are used to distinguish multiple sen-\ntences in the document (i.e., sentences in even positions are assigned one seg-\nment embedding, while those in odd positions are assigned the other segment \nembedding). Moreover, the input length limit of 512 tokens is removed by add-\ning more position embeddings, randomly initialized and to be fine-tuned with the \n35 https:// github. com/ faceb ookre search/ DPR.\n898 C. M. Greco, A. Tagarelli \n1 3\nother parameters. The resulting contextual [CLS] embedding from the top layer of \nthe encoder is representative of the sentences. The encoder is implemented with \nbert-base-uncased.\nTo address the extractive summarization task, two stacking Transformer layers \nare put on top of the encoder to learn document-level features from the sentence \nrepresentations in a hierarchical manner. In particular, the lower Transformer lay -\ners capture adjacent sentences information, while the higher layers represent the \nmulti-sentence discourse. The final sentence representations are fed to a classifica-\ntion layer to decide what sentences include into the summary. The resulting model is \ncalled BERTSUMEXT.\nFor abstractive summarization, a 6-layered Transformer decoder is added \nto the architecture. Since there is knowledge disalignment between encoder \nand decoder (the former is pre-trained while the latter needs to be trained from \nscratch),  Liu and Lapata (2019a) propose to separate the optimizers, learning \nrates and warmup-steps of the two modules in order to train the encoder with \nmore accurate gradients when the decoder becomes stable. Additionally, a two-\nstage fine-tuning strategy is proposed, which consists in first fine-tuning the \nencoder on the extractive task and then fine-tuning it on the abstractive task. The \npurely abstractive model is named BERTSUMABS, while the two-stage fine-tuned \nmodel is BERTSUMEXTABS.\nThe models are fine-tuned and evaluated on three summarization benchmarks, \ncontaining news articles and related summaries: CNN/DailyMail (Hermann et al. \n2015; Nallapati et  al. 2016; See et  al. 2017), NYT37 and XSUM (Narayan et  al. \n2018).\nFig. 19  BERTSUM architecture (Liu and Lapata 2019a)\n37 https:// catal og. ldc. upenn. edu/ LDC20 08T19.\n899\n1 3\nTransformer-based language models for AI and law\nPEGASUS. PEGASUS  (Zhang et  al. 2020) is a TLM specifically conceived for \nabstractive summarization. It introduces a new self-supervised pre-training objec-\ntive called Gap Sentences Generation (GSG), which consists in masking relevant \nsentences from the input documents and inducing the model to generate the masked \nsentences. Each selected sentence is replaced with a mask token. The concatenation \nof such gap-sentences becomes a pseudo-summary of the document. The impor -\ntance of a sentence is deduced calculating the ROUGE F1 score between the sen-\ntence and the remainder of the document.\nPEGASUS is pre-trained using the C4 corpus and HugeNews, a dataset collected \nby the authors, consisting of 1.5B news articles. The pre-training phase is conducted \nwith or without the addition of the MLM objective (Fig. 20). PEGASUS is provided \nin two sizes: PEGASUS-base (12 layers, 12 attention heads, 768 hidden size) and \nPEGASUS-large (16 layers, 16 attention heads, 1024 hidden size).\nPEGASUS is evaluated on several summarization datasets, such as CNN/Dai-\nlyMail and XSum. The best performing setting for the base model reveals to be \nwhen using the GSG objective without MLM, selecting the top-m sentences whose \nimportance scores are calculated independently, considering the double counting of \nidentical n-grams in the computation of ROUGE1-F1 and a SentencePiece unigram \nvocabulary of 96K tokens. The same settings are then used for PEGASUS-large. \nResults show that the model achieves state-of-the-art performance on all the con-\nsidered downstream task datasets. Moreover, on CNN/DailyMail it reaches better \nROUGE2-F1 score than GPT-2 in zero-shot setting and, using just 1K examples, it \noutperforms the previous best few-shot learning model.\nPRIMERA. PRIMERA  (Xiao et  al. 2022) is designed for multi-document sum-\nmarization. To this purpose, it incorporates a particular pre-training strategy, called \nEntity Pyramid, which selects salient sentences from a cluster of related docu-\nments. Such sentences are then masked and the model is trained to reconstruct and \naggregate them using the remainder of the documents. The model concatenates the \nFig. 20  PEGASUS architecture  (Zhang et  al. 2020). [MASK1] refers to sentence masking of the Gap \nSentences Generation objective, while [MASK2] refers to the token masking of MLM\n900 C. M. Greco, A. Tagarelli \n1 3\ndocuments and adds separator tokens among them to keep the boundary informa-\ntion. The documents are processed with LED, which is pre-trained on an unlabeled \nmulti-document dataset. PRIMERA is evaluated on several multi-document summa-\nrization datasets, showing to be effective in zero- and few-shot settings.\n3  Problems and tasks\nTransformer-based language models are leading a significant advance in AI-based \nNLP research to bring in better support for human decision-making processes in the \nlegal domain. In this section, we present the main types of legal problems that are \nrecognized as those particularly benefiting from AI-based NLP research, and we dis-\ncuss the associated tasks that are being powered by BERT and related models. We \norganize our discussion on the legal problems into three broad areas, namely search \n(Sect. 3.1), review (Sect.  3.2), and prediction (Sect.  3.3)—through our discussion, \nwe attempt to organize the flow of presentation by distinguishing tasks involving \ncodes (e.g., statutes, regulations, contracts) from those concerning case law; how -\never, it is often the case that a task can be regarded as relevant for any type of legal \ndocument. Please note that the three macro categories are actually interleaved and \ninterrelated in many practical scenarios, therefore our purpose of classification \nshould be taken with a grain of salt, mainly for the sake of presentation. Throughout \nthe remainder of the paper, we will use abbreviations whose description is reported \nin Table 2.\nMoreover, to complement our presentation supporting the description of the \nTLM-based methods, we end this section with an overview of relevant benchmarks \nfor the TLM-based legal learning context (Sect. 3.4).\n3.1  Legal search\nLegal search corresponds to a need for legal information, and hence requires the \ndetection and retrieval of documents potentially relevant to support legal decision-\nmaking. For instance, lawyers may search for laws enacted by parliaments or civil \ncodes (similar to legislation in a civil law jurisdiction), but also for documents in \nlitigation, patents, and several other documents that can support a law firm (Locke \nand Zuccon 2022).\nThe searched documents are also called legal authorities in  (Dadgostari et  al. \n2021), which points out how the legal search is driven by a notion of relevance, \nwhich should be ≪determined functionally with respect to norms and practices \nconcerning legal reasoning and argumentation within a legal community ≫ . Thus, \na document is regarded as ≪legally relevant exactly when it is understood by the \ndominant legal community as containing information that bears on a legal question \nof concern≫ (Dadgostari et al. 2021).\nLegal search has been addressed in (Dadgostari et al. 2021) as a citation recom-\nmendation problem: given a citation-free legal text (CFLT), to find the most suitable \nset of opinions, from a reference legal corpus, to be cited for the input CFLT. Then, \n901\n1 3\nTransformer-based language models for AI and law\nTable 2  Abbreviations and descriptions of most relevant tasks in this article\nAbbreviation Description Abbreviation Description\nAS/ES Abstractive/extractive summarization NLI Natural language inference\nAVP/ALVP Article/alleged violation prediction NSP Next sentence prediction\nCIR Case importance regression OR Overruling\nCJP(E) Court judgment prediction (and explanation) PR/CR Passage/case retrieval\nCLM Causal language modeling QA; MCQA Question answering; multiple choice QA\nCTR Case term recognition RC; MCRC Reading comprehension; multiple choice RC\nDR Document retrieval/recommendation RIR Regulatory information retrieval\nDS/SS Document/sentence similarity RRL Rhetorical role labeling\nIE Information extraction SA Sentiment analysis\nIR Information retrieval SAR Statutory article retrieval\nLPP Legal precedent prediction/retrieval SF Slot filling\nLJP Legal judgment prediction STP Same topic prediction\nMLM Masked language modeling TC/SC; TpC Text/sentence classification; topic classification\nNER Named entity recognition TM/CM Text/case matching\n902 C. M. Greco, A. Tagarelli \n1 3\nif the CFLTs are opinions from the corpus where all citation information is deleted, \nthe search results can be compared to the actual citation information. More in gen-\neral, legal search tasks are mainly addressed from two perspectives, namely Infor -\nmation Retrieval and Textual Entailment. While the former is intuitively seen as an \nessential part of any legal search task, the latter actually corresponds to Natural Lan-\nguage Inference, since it aims to determine, given any two textual fragments (e.g., \ntwo sentences), whether the one can be inferred from the other one; the entailment \nis said “positive”, resp. “negative” when the first text can be used to prove that the \nsecond text is true, resp. false, otherwise (i.e., if the two texts have no correlation) \nthe entailment is regarded as “neutral” (Kim et al. 2021).\nSince 2014, the Competition on Legal Information Extraction/Entailment \n(COLIEE) has served as an international forum to discuss issues related to legal \ninformation retrieval and entailment. 38 The COLIEE editions from 2014 to 2017 \nfocus on a two-phase legal question answering task: given a legal bar exam question \nq, the first phase is to retrieve a set of articles from a target Civil Code corpus (i.e., \nJapanese civil code) that are deemed as appropriate for answering q, whereas the \nsecond phase is to determine if the (gold) relevant articles entail q or not  q. Since \nthe 2018 edition, both the retrieval and entailment tasks are also applied to case \nlaw texts, which are relatively long documents consisting of the facts (i.e., factual \nstatements) in a case. Searching for case law is a peculiarity of common-law juris-\ndictions, which comes about from the principle of “stare decisis” (doctrine of prec-\nedent), and has unique challenges that have emerged in law research  (Locke and \nZuccon 2022); conversely, for the civil-law jurisdictions, statutes are applied in the \ndecision-making for a given legal issue in a mutatis mutandis approach, i.e., when \nasserting the substantial identity of two facts, we want to ignore the circumstances \nof a contingent nature, which are naturally different. The most recent edition at the \ntime of writing of this article, COLIEE-2021  (Rabelo et  al. 2022), proposes five \ntasks:\n– Legal Case Retrieval (Task 1)–the goal is to identify the cases from a court \ncase corpus that support the decision of a query case; such cases are also \ncalled “noticed” with respect to the query case, i.e., precedent cases that \nare referenced by the query case. Formally, given a set of candidate cases \nC ={ c1 ,… ,cn} and a query case q, the task is to identify the supporting cases \nC q ={ c /uni007C.varc ∈ C ∧ noticed(q, c)} , where noticed(q,  c) denotes that c should be \nnoticed given the query case q.\n– Legal Case Entailment (Task 2)–given a query case, the goal is to identify one \nor more paragraphs from a case relevant to the query that entail(s) the decision \nof the query. Formally, given a query case q and a case ci relevant for q rep -\nresented by its paragraphs {ci1,… ,cini\n} , the task is to identify the set of para-\n38 webdocs.cs.ualberta.ca/~miyoung2/jurisin_task/index.html; webdocs.cs.ualberta.ca/~miyoung2/\nCOLIEE201i ( i ∈{ 5, 6, 7} ); sites.ualberta.ca/~miyoung2/COLIEE2018/; sites.ualberta.ca/~rabelo/\nCOLIEE20i ( i ∈{ 19, 20, 21}).\n903\n1 3\nTransformer-based language models for AI and law\ngraphs {cij /uni007C.varcij ∈ ci ∧ entails(cij, q)} , where entails(cij, q) is true if the paragraph \nc ij entails q.\n– Statute Law Retrieval (Task 3)—this is the former phase-1 in COLIEE-2014, i.e., \ngiven a civil code S and a legal bar exam question q, to retrieve the set of articles \nSq from S such that entails(Sq , q) or entails(Sq , not q ).\n– Statute Law Entailment (Task 4)—this is the former phase-2 in COLIEE-2014, \ni.e., given a legal bar exam question q and relevant articles Sq , to determine if it \nholds that entails(Sq , q) or entails(Sq , not q ).\n– Legal Question Answering (Task 5)—this is regarded as a combination of Task \n3 and 4 (although, in the COLIEE competition, any knowledge source other than \nthe results of Task 3 can be used).\nTraining data are pairs ⟨query, noticed case(s)⟩ for Task 1, triplets ⟨query, noticed \ncase(s), entailing paragraph IDs of the case(s) ⟩ for Task 2, pairs ⟨query, relevant \narticle(s)⟩ for Task 3, triplets ⟨query, relevant article(s), Y/N answer)⟩ for Task 4 \nand Task 5; the test data are only queries for Tasks 1, 3, and 5, whereas they include \nqueries and relevant texts for Tasks 2 and 4.\nIt is worth noticing that supporting cases are relevant factors in court decision-\nmaking and are actually used in the attorney’s ligation (Nguyen et al. 2021a). Legal \ncase entailment is also useful in practice, since a decision for a new case can be pre-\ndicted by implication of previous cases; it can also be treated in combination with \ncase retrieval, as developed in (Vuong et  al. 2023), where a supporting model is \nintroduced to describe the case-case supporting relations and to define paragraph-\nparagraph and decision-paragraph matching strategies. Analogous considerations \nhold for the statute law tasks. Moreover, the latter are particularly challenging since, \nbesides the need for addressing the long articles, legal bar exam questions describe \nspecific legal cases, while the language used in statute law tends to be more general.\nA further perspective on legal question answering is taken in (Zheng et al. 2021), \nwhere a multiple choice question answering task, dubbed CaseHOLD, is defined \nfrom legal citations in judicial rulings. The citing context from the judicial deci-\nsion serves as the prompt for the question, whereas the answer choices are holding \nstatements derived from citations following text in a legal decision. Holdings are \ncentral to the common law system, as they represent the predominating, precedential \nlegal rule when the law is applied to a particular set of facts. Analogously, in (Xiao \net al. 2021), legal question answering is addressed on the JEC-QA dataset, which \nconsists of multiple-choice questions from the Chinese national bar exam, where the \nquestions and candidate choices are concatenated together to form the inputs of the \nmodels.\n3.2  Legal document review\nDocument review is another critical process for law practitioners and lawyers, as it \nusually involves document sets that are unmanageable for a team of humans given \ntheir amount, the cost of reviewers, and deadlines in the context of legal proceed-\nings. The purpose of legal document review is for the parties to a case to organize \n904 C. M. Greco, A. Tagarelli \n1 3\nand analyze the available documents so as to determine which are sensitive or oth-\nerwise relevant to the litigation. For instance, document review can be intended to \nnegotiate or revise an agreement, ensure that the filings of an attorney’s client com-\nply with appropriate regulations, modify a brief for a trial motion, inspect a con-\ntract to avoid potential risks, or review client tax documents. Relevance, responsive-\nness to a discovery request, privilege, and confidentiality are essential criteria for \nany document in the review, but also in the analysis of the information to relate key \ndocuments to alleged facts or key legal issues in the case.\nShaghaghian et al. (2020) recognize four main tasks of document review, namely \ninformation, fact, comparative, and rule navigation, which are primarily character -\nized in terms of the following problems:\n– Passage retrieval—Navigating a user to answers for non-factoid questions is in \nfact seen as equivalent to retrieving relevant text passages during the document \nreview process. Passage Retrieval to answer non-factoid questions can be mod-\neled as a binary text classification task, i.e., given a set of queries {qi}i=1…Q and a \nset of candidate texts (e.g., sentences, snippets, paragraphs) {sj}j=1…N  , each pair \nquestion-snippet (q i, sj) is assigned label 1 if sj contains the answer to q i , and \nlabel 0 otherwise.\n– Named entity recognition—Examining factoid questions whereby the user is \nsearching for specific facts or entities is instead modeled as named entity recog-\nnition (e.g., extraction of facts from a court decision document, such as Date of \nArgument, Date of Decision, Petitioner, Judge, Sought Damages and Damages \nAwarded Monetary Values). Named Entity Recognition to extract facts or ele-\nments of factoid questions can be modeled as a sequence labeling, multi-class \nclassification task, i.e., given a set of fact-related classes {ci}i=1…C  , each token is \nassigned a class (or a distribution over the classes).\n– Text similarity—Computing text similarity is essential to identify matching texts \naccording to different aspects; for instance, to identify the differences between a \nregulation and its amended version, or to discover the discrepancies of regula-\ntions in different jurisdictions. Text similarity to identify matching texts at var -\nious, pre-determined levels can be modeled as a binary, resp. multi-class, text \nclassification task, i.e., given a set of matching levels {mi}m=1…M  and a set of \ntexts {sj}j=1…N  , each pair of texts (sj, sk ) is assigned a class m i depending on the \ndegree of matching between sj and sk.\n– Sentiment analysis—This can be addressed to identify the polarity, or the mood, \nassociate with certain legal statements, with the purpose of, e.g., identifying rules \nimposed by deontic modalities, which are of the form of obligations, prohibition \nand permission statements. This can be modeled as a binary, resp. multi-class, \ntext classification task, i.e., given a set of texts {sj}j=1…N  , each text is assigned a \nclass depending on the polarity or sentiment expressed in the text.\nIn (Xiao et al. 2021), legal reading comprehension is addressed to predict the start \npositions and end positions given question-answer pairs with corresponding sup-\nporting sentences. Legal document review is also related to document recommenda-\ntion. As discussed in  (Ostendorff et  al. 2021), a typical recommendation scenario \n905\n1 3\nTransformer-based language models for AI and law\noccurs during the preparation of a litigation strategy, when the involved legal pro-\nfessionals are provided with recommended other decisions that possibly cover the \nsame topic or provides essential background information (e.g., they overrule the \ntarget decision). Also, text segmentation, i.e., the task of dividing a document into \nmulti-paragraph discourse units that are topically coherent, can be useful to one or \nmore of the above tasks, especially when the existing logical boundaries imposed to \nthe document might not be sufficient to detect fine-grain topic changes. In (Aumiller \net al. 2021), text segmentation is used to solve a topical change detection problem \n(also called same topic prediction): given two chunks of text of the same type (e.g., \nparagraphs, sections) and binary labels, to determine if the two chunks belong to the \nsame topic, otherwise a change in topic is detected and so the beginning of a new \nchunk of text. Also, Savelka et al. (2021) introduce the task of automatic functional \nsegmentation, which is to segment adjudicatory decisions of cases according to the \nfunctional role of the parts.\nContracts, in various forms, are major target of interest for document review \ntasks. Zheng et al. (2021) focus on contract documents such as Terms-of-Services, \nfor the detection of potentially unfair contractual terms. A contractual term (clause) \nis regarded as unfair if it has not been individually negotiated, and it corresponds \nto an evident imbalance in the rights and obligations of the parties, to the detri-\nment of the consumer (Zheng et al. 2021). A binary classification task can hence be \ndefined, whereby positive examples are the potentially unfair contractual terms. The \nTerms-of-Service task can help consumers better understand the terms they agree \nto when signing a contract and ease access to legal advice about unfair contracts. \nHendrycks et al. (2021) address the legal contract review task, which is to analyze \na contract to understand rights and obligations of the signatories as well as to evalu-\nate the associated impact. This task can be seen as similar to extractive question \nanswering, where each question is the description of a label category and language \nmodels have to detect the spans of the contract is related to the label. Leivaditi et al. \n(2020) specialize the legal contract review task to lease agreements and address this \ntask from two perspectives: detection of sentences expressing a potential risk to one \nor more signatories (binary classification) and extraction on important entities for \nthe domain (entity recognition). Unlike (Hendrycks et al. 2021) and (Leivaditi et al. \n2020), which aim to find what kinds of terms are present, Koreeda and Manning \n(2021) focus on knowing what exactly each of these terms states. Given a set of \nhypotheses and a contract, the task is to decide if the contract entails, contradicts or \nis neutral to each hypothesis (three-class classification) and detect the evidence, i.e., \nspans, in the contract that determine the decision (multi-label binary classification). \nOn privacy policies, Ahmad et al. (2021) define the intent classification task, which \nis to predict sentences explaining privacy practices, along with a slot filling task to \ndetect text spans within a sentence expressing specific details. A slot extraction task \nis also performed in (Bui et al. 2021) to detect spans in the text expressing different \ntypes of user data.\nAnother legal context that can be included in the document review category \nto a broader extent concerns a special case of retrieval task, namely regulatory \ninformation retrieval (Chalkidis et al. 2021b), i.e., to ensure a regulatory compli-\nance regime regarding an organization’s processes/controls. A compliance regime \n906 C. M. Greco, A. Tagarelli \n1 3\nincludes corrective, detective and preventive measures such that either, given a \ncontrol/process, to retrieve relevant laws in order to apply corrective measures or, \ngiven a new law, to retrieve all the affected controls/processes in order to apply \ncorrective or preventive measures. Regulatory information retrieval is defined as \na special case of document-to-document information retrieval, since the query is \nan entire document—unlike traditional information retrieval, whereby queries are \nusually short texts.\nMore tasks concern case law documents. Legal cases are lengthy and unstruc-\ntured, although they are actually characterized by an implicit thematic structure into \nsections such as “facts of the case”, “arguments given by the parties”, etc. These sec-\ntions are often called as rhetorical roles. Identifying such semantic roles is essential \nfor improving the roles readability of the documents but also helps in downstream \ntasks such as classification and summarization. The task is challenging since in most \ncases legal documents can vary in structure and rhetorical labels can be subjective. \nBhattacharya et  al. (2019b) introduce the rhetorical role labeling task, which is \nto label sentences of a legal case with the corresponding rhetorical role. This task \nwas also introduced in the context of the Artificial Intelligence for Legal Assistance \n(AILA) 2020 competition (Task 2), whereby the predefined labels are “Facts”, “Rul-\ning by Lower Court”, “Argument”, “Statute cited”, “Precedent cited”, “Ratio of the \ndecision”, and “Ruling by Present Court”.39\nTo support legal document review, special cases of retrieval are also involved. For \ninstance, Martino et al. (2022) deal with the identification of paragraph regulari-\nties in legal cases, which is addressed by using a nearest-neighbor search method to \nefficiently select the most similar paragraphs appearing in a set of reference docu-\nments. Explanatory sentence retrieval  (Savelka and Ashley 2021) is instead to \nretrieve useful sentences to explain predetermined legal concepts. Explanations of \nlegal concepts can be inferred looking at how they have been applied in previous \ncases, allowing a lawyer to elaborate supporting or contrary arguments related to \nparticular accounts of meaning. Searching through legal documents, a lawyer can \nfind sentences mentioning a particular concept, but not all of them could be useful \nfor explaining that concept. Therefore, the aim is to automatically rank sentences in \norder to assign higher scores to explanatory sentences.\nIt is also highly desirable for legal professionals dealing with cases to access to \ntheir summaries, also known as headnotes. However, creating headnotes is certainly \ntime-consuming, therefore automatic summarization of legal judgments is another \nmeaningful problem in the legal domain. Two related tasks have been introduced in \nthe Artificial Intelligence for Legal Assistance (AILA) 2021 competition, namely to \nidentify “summary-worthy” sentences in a court judgment (Task 2a) and to generate \na summary from a court judgment (Task 2b). 40 The former can be seen as a sen-\ntence classification task, whereas the latter can be addressed either by collecting the \n39 https:// sites. google. com/ view/ aila- 2020/ task-2- rheto rical- role- label ing- for- legal- judge ments.\n40 https:// sites. google. com/ view/ aila- 2021/ task-2- summa rizat ion- of- legal- judge ments.\n907\n1 3\nTransformer-based language models for AI and law\n41 The above view has been recognized not only as one of the most challenging by the legal community, \nbut it has also raised controversial debate on the role of AI applied to law. Adversarial opinion is in fact \nbased on the evidence that, in real-life scenarios, judges are unlikely to defer to AI to decide the outcome \nof a case. Nonetheless, the authors adopt an opinion that is commonly shared with most researchers and \npractitioners of AI in law, whereby it should be seen as a powerful tool to aid legal professionals to \nincrease their access to justice, and ultimately address unmet needs of the legal community.\ndetected summary-worthy sentences so as to form extractive summaries or by using \ngenerative models to produce abstractive summaries.\n3.3  Legal outcome prediction\nLegal relevance is related to the well-known predictive theory of the law first intro-\nduced in (Oliver Wendell Holmes 1897). In contrast to previous definitions of the \nlaw, Holmes formulated the law as a prediction, particularly the behavior of a court, \nso as to build a more useful approach in practice when dealing with those individu-\nals who care little for ethics or lofty conceptions of natural law (i.e., the “bad men”). \nBesides the Holmes’ theory, predictive tasks in law are more generally concerned \nwith judicial opinions. For instance, as discussed in (Dadgostari et al. 2021), given \nthe content of a source judicial opinion, one task is to predict the other opinions that \nare cited in the source document; or, given a source document and a set of related \nopinions identified by law professionals, to predict their answers.\nThe primary predictive task in law is commonly referred to as legal judgment \nprediction (LJP), i.e., to predict the outcome of a judicial decision based on the rel-\nevant facts and laws (Aletras et al. 2016; Zhong et al. 2018; Chalkidis et al. 2019a). \nFor instance, Aletras et al. (2016) define the problem of case prediction as a binary \nclassification task, which is to predict whether one of a predetermined, small set of \narticles of the ECtHR Convention has been violated, given textual description of a \ncase, which includes the facts, the relevant applicable law and the legal arguments.41 \nIn  (Xiao et  al. 2021), the LJP task is addressed on both criminal and civil cases \nfrom the CAIL-Long dataset. Fact descriptions are taken as input whereas the judg-\nment annotations are extracted via regular expressions; each criminal case is anno-\ntated with the charges, the relevant laws, and the term of penalty, and each civil case \nis annotated with the causes of actions and the relevant laws. For criminal cases, \nthe charge prediction and the relevant law prediction are formalized as multi-label \nclassification tasks, whereas the term of penalty prediction task is formalized as a \nregression task. For civil cases, the cause of actions prediction is formalized as a \nsingle-label classification task, and the relevant law prediction is formalized as a \nmulti-label classification task. In (Dong and Niu 2021), the three types of prediction \nare addressed in a context of graph node classification, where a Transformer model \nis combined with a graph neural network model. Malik et  al. (2021) propose the \ncourt judgment prediction and explanation (CJPE) task, which requires to predict \nthe decision of a case and to provide explanations for the final decision, where expla-\nnations correspond to portions in the case description that best justify the outcome.\n908 C. M. Greco, A. Tagarelli \n1 3\nA related axis of prediction is the one introduced in (Mahari 2021), dubbed as \nlegal precedent prediction, which is to predict passages of precedential court deci-\nsions that are relevant to a given legal argument posed in the context of a judicial \nopinion or a legal brief. Both judicial opinions and legal briefs usually contain a \nnumber of independent legal arguments, each citing its own set of precedent, where \nthe precedent depends on the context of the entire case as well as on the specific \nlegal argument being made  (Mahari 2021). Clearly, in common law jurisdictions, \nthis is particularly useful as legal professionals build their arguments by drawing on \njudicial precedent from prior opinions.\nAnother critical task is overruling prediction, i.e., to determine if a statement is \nan overruling, i.e., a sentence that nullifies a previous case decision as a precedent, \nby a constitutionally valid statute or a decision by the same or higher ranking court \n(which establishes a different rule on the point of law involved). In  (Zheng et  al. \n2021; Limsopatham 2021), the overruling prediction is modeled as a binary clas-\nsification task, where positive examples are overruling sentences and negative exam-\nples are nonoverruling sentences from the law. The overruling task is clearly impor-\ntant for legal professionals, since verifying whether cases remain valid and have not \nbeen overruled is essential to ensuring the validity of legal arguments.\nCase importance and article violation are also considered (Chalkidis et al. 2019a; \nLimsopatham 2021). Predicting the importance of a case can be seen as a regression \ntask, e.g., to measure on a scale from lower scores for key cases, to higher scores \nfor unimportant cases. Given the facts of a case, article violation is to predict if any \nhuman rights article or protocol has been violated by the case (binary classifica-\ntion), or which human rights articles and/or protocols have been violated (if any) by \nthe case (multi-label classification). A special case of the above task is the alleged \nviolation prediction introduced in (Chalkidis et al. 2021c), whose aim is to predict \nthe allegations made by applicants given the facts of each case. This can be useful \nto identify alleged violations for plaintiffs, facts supporting alleged violations for \njudges but also for legal experts to identify previous cases related to the allegations. \nThe task is treated in  (Chalkidis et  al. 2021c) as a multi-label text classification, \nsince the model might select multiple articles that were allegedly violated (accord-\ning to the applicants).\nEmployment notice prediction  (Lam et  al. 2020) is to predict the number of \nmonths awarded for reasonable notices in employment termination cases. If the \nemployer does not comply with the obligation to provide an appropriate employ -\nment notice or payment in lieu of notice, judges determine the compensation that an \nemployer owes to an employee at the time of termination. Courts might rely on fac-\ntors such as length of service, employee’s age, character of employment, aggravated \ndamages to establish what constitutes reasonable notices, but it is not clear how to \nweigh each individual factor and how they should be used. As a result, the case law \non employment notice turns out to be inherently inconsistent and subjective. Lam \net  al. (2020) define this problem as a text classification task, in order to obtain a \nsimilar decision-making process of a judge who would rely allegedly on past cases \nand differences of fact to decide the amount of reasonable notice.\n909\n1 3\nTransformer-based language models for AI and law\n3.4  Benchmarks and datasets\nTo complement our discussion so far, here we provide a summary of the main \nbenchmarks and datasets that have been recognized as relevant in the TLM-based \nlegal learning context. Our main focus is on those corpora that were used by the \napproaches covered in this work, which will be described next (Sect.  4). Note that \nwe shall leave out of consideration the datasets used in the COLIEE Competitions, \nsince they have already been described in Sect. 3.1.\nOur presentation is organized into three subsections, which describe corpora con-\ncerning caselaw documents, codes, and a combination of both, respectively; moreo-\nver, each subsection is further organized by possibly grouping corpora that are cohe-\nsive in terms of data type and task. Table  3 summarizes the datasets that we shall \ndescribe through this section, according to the legal document category, the data \ntype, the source, the size, and the tasks for which the benchmarks were designed.\n3.4.1  Caselaw data\nStrickson and Iglesia (2020) propose a corpus of about 5K labeled UK court judg-\nments, gathered from the web, for the task of JLP. Each law case is divided into \nseparate judgments issued by individual judges, and each sentence in a judgment is \nlabeled as “allow” or “dismiss” through a pattern matching approach. The dataset \nis used for the JLP task as a classification problem, whereby classic machine learn-\ning classifiers (e.g., support vector machine, random forest, logistic regression) are \nevaluated.\nECHR (Chalkidis et al. 2019a) contains allegations of violated provisions regard-\ning the European Convention of Human Rights. 42 Each case includes a list of facts \nand a score, provided by the Convention, representing the importance of the case \nin the case law’s development. Also, each case is mapped to the violated articles \nof the Convention. Moreover,  Quemy and Wrembel (2022) present ECH-OD, a \nnew database for storing and managing ECHR cases. This is designed to be auto-\nmatically maintained and used as a unified benchmark to compare machine learning \nmethods for the legal domain. The authors have provided the whole pipeline for the \nbenchmark data extraction, transformation, integration, and loading as open-source \nsoftware.\nSwiss-Judgment-Prediction (SJP) (Niklaus et al. 2021) comprises 85K cases, in \ndiachronic order, from the Federal Supreme Court of Switzerland (FSCS). 43 The \nevaluation task is a binary classification of the judgment outcome (i.e., approval \nor dismissal). The dataset includes cases written in German, French and Italian, \nand is annotated with publication years, legal areas and cantons of origin. Niklaus \net  al. (2021) evaluate XLNet, RoBERTa, AlBERT (cf. Sect.  2), GermanBERT, \n42 https:// archi ve. org/ detai ls/ ECHR- ACL20 19.\n43 https:// huggi ngface. co/ datas ets/ rcds/ swiss_ judgm ent_ predi ction.\n910 C. M. Greco, A. Tagarelli \n1 3\nTable 3  Summary of benchmarks and datasets discussed in Sect. 3.4\nReference Category Data type Data source Data size Tasks\n Strickson and Iglesia \n(2020)\nCases UK court judgments Web sources 5K documents LJP\n Chalkidis et al. (2019a) Cases (allegations) ECHR cases ECHR’s public database \n(HUDOC)\n11.5K documents LJP (binary violation), \nALVP, case importance \nprediction\n Chalkidis et al. (2021c) Cases (allegations) ECHR cases ECHR’s public database \n(HUDOC)\n11K documents ALVP\n Niklaus et al. (2021) Cases Federal Supreme Court of \nSwitzerland cases\nEntscheidsuche platform 85K documents LJP\n Wrzalik and Krechel \n(2021)\nCases Case laws from Open \nLegal Data\nOpen Legal Data platform 123K query passages, \n131K documents\nLPR\n Urchs et al. (2021) Cases Cases from Bavarian \ncourts\nGesetze-bayern platform 32K docs, 200 docs with \n25K sentences\nAutomatic detection of \nsections\n Zhong et al. (2019b) Cases (appeals) US Board of Veterans’ \nappeals cases\nUS Department of Veter-\nans Affairs platform\n92 documents ES\n Walker et al. (2019) Cases (appeals) US Board of Veterans’ \nappeals cases\nUS Department of Veter-\nans Affairs platform\n50 documents, 6K sen-\ntences\nSC\n Shen et al. (2022) Cases Writings providing infor-\nmation on US federal \ncivil rights cases\nCivil Rights Litigation \nClearinghouse platform\n9K documents AS\n Zheng et al. (2021) Cases (holdings) US court cases Existing corpus (Harvard \nLaw Library corpus)\n53K questions MCQA\n Feijó and Moreira (2018) Cases Brazil federal court deci-\nsions\nSupremo Tribunal Federal \nplatform\n10K documents ES\n Lage-Freitas et al. (2022) Cases Brazilian State higher \ncourt cases\nWeb sources 4K documents LJP, unanimity decision\n Xiao et al. (2019) Cases Supreme People’s Court \nof China cases\nChina Judgments Online \nplatform\n8K document triplets CM\n911\n1 3\nTransformer-based language models for AI and law\nTable 3  (continued)\nReference Category Data type Data source Data size Tasks\n Yu et al. (2022b) Cases Supreme People’s Court \nof China cases\nExisting dataset (CAIL) 6K documents pairs Explainable CM\nFaxin platform 5K documents pairs\n Malik et al. (2021) Cases Supreme Court of India \ncases\nIndianKanoon platform 35K documents CJPE\n Kalamkar et al. (2022) Cases Indian courts’ legal judg-\nments\nIndian court platforms \n(Supreme Court of India \nplatform, eCourt India \nservices platform)\n354 documents, 40K \nsentences\nAutomatic legal document \nstructuring\n Bhattacharya et al. (2021) Cases Supreme Court of India \ncases\nThomson Reuters Westlaw \nIndia platform\n50 documents RRL\nUK Supreme Court UK Supreme Court \nplatform\n50 documents\n Paul et al. (2022b) Cases Supreme Court and High \nCourts of India cases\nWeb platforms (Supreme \nCourt of India platform, \nIndianKanoon platform)\n5.4M documents Pre-training\n Bhattacharya et al. \n(2019a)\nCases Supreme Court of India \ncases\nThomson Reuters Westlaw \nIndia platform\n17K documents AS\n Bhattacharya et al. \n(2020a)\nCases Supreme Court of India \ncases, statutes in the \nIndian judiciary\nThomson Reuters Westlaw \nIndia platform\n1.8K documents Document similarity\n Shukla et al. (2022) Cases Supreme Court of India \ncase judgments, UK \nSupreme Court case \njudgments\nLegal Information Inst. \nof India platform, UK \nSupreme Court platform\n∼ 7K docs (India), 793 \ndocs (UK)\nAS, ES\n Niklaus et al. (2022) Cases Federal Supreme Court of \nSwitzerland and Indian \ncases\nExisting datasets (SJP and \nILDC)\n∼ 180K Swiss docs, 96K \nIndian docs\nLJP, cross-domain/regional/\njurisdiction transfer \nlearning\n912 C. M. Greco, A. Tagarelli \n1 3\nTable 3  (continued)\nReference Category Data type Data source Data size Tasks\n Savelka et al. (2021) Cases Documents from several \ncourts\nWeb platforms, random \ncourts (CA); Constitu-\ntional/Supreme/Supreme \nadmin. courts (CZ); \nCour de cassation (FR); \nfederal courts (DE); \ncriminal courts (IT); \nSupreme Court, Consti-\ntutional tribunal (PL); \nFederal district court, \nDept. of Labor (US)\n807 documents with 89K \nsentences\nFunctional segmentation\n Holzenberger et al. \n(2020)\nStatutes (rules) US tax law Office of the Law Revi-\nsion Counsel platform—\nUS Internal Revenue \nCode\n276 train, 100 test docs Entailment and tax amount \nprediction\n Louis and Spanakis \n(2022)\nStatutes Belgian codes (federal/\nregional authorities)\nDroits Quotidiens organi-\nzation\n1.1K questions, 22K \narticles\nSAR\n Papaloukas et al. (2021) Statutes, regulations Greek legislation (laws, \nRoyal/Presidential \ndecrees, regulations \nand decisions, from the \nOfficial Government \nGazette)\nPermanent Greek Legisla-\ntion Code (Raptarchis) \nthrough e-Themis \nplatform\n47K documents Multi-level TpC\n Hendrycks et al. (2021) Contracts  25 types of legal contracts \n(e.g. license, consulting, \nservice, manufacturing)\nSEC’s EDGAR database 510 documents Legal contract review, \nrelevant text prediction\n Leivaditi et al. (2020) Contracts Lease contracts SEC’s EDGAR database 179 documents Sentence detection, entity \nextraction\n913\n1 3\nTransformer-based language models for AI and law\nTable 3  (continued)\nReference Category Data type Data source Data size Tasks\n Tuggener et al. (2020) Contracts Exhibit-10 material con-\ntract agreements\nSEC’s EDGAR database 60K documents Text classification\n Koreeda and Manning \n(2021)\nContracts Non-disclosure agree-\nments\nWeb sources and SEC’s \nEDGAR database\n607 documents NLI\n Aumiller et al. (2021) Contracts Terms of service Web sources 40K documents Topic similarity\n Wang et al. (2023) Contracts Merger agreements SEC’s EDGAR database 39K question-answer \nannotations\nMCRC \n Manor and Li (2019) Contracts Contracts relating to \nsoftware licenses, pri-\nvacy policies, terms of \nservice/use\nWeb platforms (TL; \nDRLegal, TOS; DR)\n84 agreement-summary \npairs, 421 agreement-\n(up to 3) summary pairs\nAbstractive similarity\n Ahmad et al. (2021) Contracts Privacy policies Mobile applications and \nweb sources\n5.2K sentences Intent classification, SF\n Ravichander et al. (2019) Contracts Privacy policies Mobile applications 1.7K questions, 3.5K \nsentences\nQuestion answerability, \nevidence extraction\n Ahmad et al. (2020) Contracts Privacy policies Web sources 25K triples Answer span prediction, \nQA\n Bui et al. (2021) Contracts Privacy policies Existing dataset (OPP-\n115) based on web \nsources\n4.1K sentences Personal data object extrac-\ntion, NER\n Tziafas et al. (2021) Regulations COVID-19 exceptional \nmeasures\nWeb platforms 4K sentences SC\n Chalkidis et al. (2021a) Statutes, regulations European Union laws EUR-Lex platform 65K documents TpC\n Aumiller et al. (2022) Statutes, regulations  Legal acts of 20 domains \n(e.g., taxation, energy, \ntransportation, industrial \npolicy, health protec-\ntion)\nEUR-Lex platform 32K doc-summary pairs, \n∼375 docs as val/test \nsets for all languages, \nremaining docs as lang.-\nspecific train set\nAS, ES\n914 C. M. Greco, A. Tagarelli \n1 3\nTable 3  (continued)\nReference Category Data type Data source Data size Tasks\n Pais et al. (2021) Statutes, regulations Decrees, regulation, laws \nof several countries\nExisting dataset (MAR-\nCELL-RO) based on the \nRomanian legislative \nplatform\n370 documents NER\n Chi et al. (2023) Contracts Privacy policies Existing datasets (OPP-\n115, APP-350, Priva-\ncyQA, PolicyQA, Poli-\ncyIE, PI-Extract) related \nto mobile applications \nand web sources\n18.7K segments, 1.7K \nquestions, 25K triples, \n9.2K sentences\nIntent classification, SF, \nQA, NER\n Lippi et al. (2019) Contracts Terms of service Web platforms 9.4K sentences Clause detection and type \nclassification\n Drawzeski et al. (2021) Contracts Terms of service Existing dataset \n(UNFAIR-ToS) related \nto web platforms\n100 documents Sentence-level annotations \nprojection\n Chalkidis et al. (2022b) Cases (allegations hold-\nings), contracts\nECHR cases, US Supreme \nCourt opinions (Crimi-\nnal Procedure, Civil \nRights, Economic Activ-\nity) US court cases, \nterms of service\nExisting datasets (ECtHR \nTasks A & B, multi-\nEURLEX, SCOTUS, \nLEDGAR, UNFAIR-\nToS, CaseHOLD) based \non web platforms\n∼164 docs, 9.4K sen-\ntences, 53K questions\nAVP, ALVP, TpC, multi-\nclass/label classification, \nMCQA\n Henderson et al. (2022) Cases, regulations, \ncontracts\nEU and US documents \n(Court Listener Opin-\nions, US Board of Veter-\nans’ Appeals, EUR-Lex, \nEDGAR contracts, \nECHR, US Code)\nWeb sources, existing \ndatasets\n10M documents Pre-training\n915\n1 3\nTransformer-based language models for AI and law\nTable 3  (continued)\nReference Category Data type Data source Data size Tasks\n Chalkidis et al. (2022c) Cases ECHR cases, US Supreme \nCourt opinions, Federal \nSupreme Court of Swit-\nzerland cases, Supreme \nPeople’s Court of China \ncases\nExisting datasets (ECtHR, \nSCOTUS, SJP and \nCAIL), based on web \nplatforms\n209K documents Fairness on ALVP, TpC, \ncase approval prediction, \ncrime severity prediction\n916 C. M. Greco, A. Tagarelli \n1 3\nUmBERTo, CamemBERT (cf. Sect.  4.6), and two variants, namely Hierarchical \nBERT and Long BERT, both in monolingual or multilingual versions (cf. Sect. 4.7).\nGerDaLIR (Wrzalik and Krechel 2021) is a dataset for legal precedent retrieval \non German language. 44 It is based on case laws gathered from the Open Legal \nData  (Ostendorff et  al. 2020). Passages containing references are considered que-\nries, while the referenced law cases are labeled as relevant. The authors evaluate a \nset of retrieval methods on this dataset with Transformer-based re-ranking. In par -\nticular, they fine-tune GBERT and GELECTRA base versions using top-100 BM25 \npassage rankings and test the final models on top-1000 BM25 passage ranking; the \nuse of ELECTRA for re-ranking has shown to lead to higher performances in most \ncases. Urchs et al. (2021) introduce two further legal corpora for the German law. \nThe first corpus45 contains about 32K decisions, enriched with metadata, from hun-\ndreds of Bavarian courts. There are 22 different types of decisions in the corpus, \nsuch as resolutions, judgments and end-judgments. This corpus is not intended for \na specific task (for example, it can be used to detect the type of the decision). The \nsecond corpus46 is a subset of the former and contains 200 judgments, whose sen-\ntences (about 25K) were annotated by a domain expert w.r.t. four components of \nthe text (written in the Urteilsstil style): “conclusion” (i.e., the overall result of the \ncase), “definition” (i.e., abstract legal facts and consequences), “subsumption” (i.e., \nthe ensemble of concrete facts and determination sentence), and “other” (i.e., sen-\ntences not labeled with any of the three previous labels). This corpus is intended for \nthe automatic detection of conclusion, definition and subsumption components.\nZhong et al. (2019b) provide 92 expert-annotated extractive summaries of Board \nof Veterans’ Appeals (BVA) cases focused on post-traumatic stress disorder (PTSD), \nalong with 20 test cases quadruple-annotated for agreement evaluation and two sum-\nmaries for each test case and written by law experts. 47 Each sentence is annotated \nconsidering six labels, namely issue, procedural history, service history, outcome, \nreasoning, evidential support. Also, Walker et al. (2019) introduce a dataset to test \nthe performance of rule-based script classifiers, comprising 50 fact-finding deci-\nsions of BVA cases focused on veterans’ appeals to a rejected disability claim for \nservice-related PTSD. Each sentence of the dataset is assigned a rhetorical role by \ndomain experts as follows: finding sentence, if it states a finding of the fact, evidence \nsentence, if it states the content of a testimony, reasoning sentence, if it reports the \nreasoning of the judge underlying the findings of facts, legal-rule sentence, if it \nstates legal rules in the abstract, and citation sentence, if it refers to legal authorities \nand other materials. The dataset is used to test two hypotheses: whether distinctive \nphrasing allows automatic classifiers to be developed on a small set of labeled deci-\nsions, and whether semantic attribution theory can provide a general approach to \ndevelop such classifiers. Results demonstrate that some use cases can be addressed \nusing a very small set of labeled data.\n45 https:// zenodo. org/ record/ 39367 26#. ZAdMI XbMJD_.\n46 https:// zenodo. org/ record/ 39364 90#. ZAdN7 HbMJD_.\n47 https:// github. com/ luima group/ bva- summa rizat ion.\n44 https:// github. com/ lavis- nlp/ GerDa LIR.\n917\n1 3\nTransformer-based language models for AI and law\nMulti-LexSum48 is a collection of almost 9K expert-edited abstractive sum-\nmaries for 40K writings of the Civil Rights Litigation Clearinghouse (CRLC), 49 \nwhich provides information on federal US civil rights cases for various target audi-\nences (lawyers, scholars, and the general public) (Shen et al. 2022). It is designed \nfor multi-document and single-document summarization tasks. The source docu-\nments are extremely long, with cases often having more than two hundred pages. \nMulti-LexSum provides multiple summaries with different granularity (from an \nextreme one-sentence summaries to summaries with more than five hundred words). \nAlthough the provided summaries are abstractive, they present a high fraction of \nterms included also in the source document.\nRulingBR (Feijó and Moreira 2018) 50 comprises 10K Brazilian rulings for sum-\nmarization on legal tasks, which were retrieved from the decision documents of \nthe highest court in Brazil, Supremo Tribunal Federal (STF). 51 Each decision \ndocument is composed of the following four parts: “Ementa” (i.e., summary), \n“Acordao” (i.e., judgment), “Relatorio” (i.e., report), and “Voto” (i.e., vote). The \nEmenta part is used as gold summary for the dataset. Lage-Freitas et  al. (2022) \nalso propose a dataset consisting of about 4K legal cases from a Brazilian State \nhigher court (Tribunal de Justica de Alagoas), with a focus on the Brazilian appeals \nsystem, assigning the appeals with labels regarding court decisions. Following Ale-\ntras et al. (2016), the authors assume that there is enough similarity between the \ncase description of legal judgments and appeals lodged by attorneys. Brazilian \ncourts data are scraped from the Web and segmented into sections, identifying the \ndescription, decision and unanimity parts; then, description sentences are labeled \naccording to the decision outcome (yes, no, or partial) and unanimity information \n(unanimity vs. non-unanimity).\nCAIL2019-SCM (Xiao et al. 2019) is a dataset of about 8K triplets of cases of the \nSuprem People’s Court of China, concerning private lending.52 It was collected from \nthe China Judgments Online 53 for the CAIL competition, where participants were \nrequired to perform a similar case matching task, i.e., to detect which pair of cases \nin the triplet contains the most similar cases. Every document in the triplet refers to \nthe fact description of a case. The most similar pair within each triple is detected by \nlegal experts. The authors provide some baselines to compare the participants’ per -\nformance, one of which uses BERT to obtain embeddings of the two cases, for which \nthe similarity score is computed. The CAIL competition was first held in 2018 (Xiao \net al. 2018). In CAIL2018, participants were required to perform a legal judgment \nprediction task divided in three sub-tasks: law article prediction, charge prediction, \nand term-of-penalty prediction. The input is the fact description of a criminal case, \nand the associated dataset is divided into two sub-datasets: CAIL-big (with more \nthan 1.6M cases) and CAIL-small (about 130K cases). Yu et al. (2022b) extended \n48 https:// multi lexsum. github. io/.\n49 https:// clear ingho use. net/.\n50 https:// github. com/ diego- feijo/ rulin gbr.\n51 https:// portal. stf. jus. br/.\n52 https:// github. com/ china- ai- law- chall enge/ CAIL2 019/ tree/ master/ scm.\n53 https:// wenshu. court. gov. cn/.\n918 C. M. Greco, A. Tagarelli \n1 3\nthe fact prediction task data of CAIL 2021 for the explainable legal case matching \ntask. The sentences of a legal case in CAIL 2021 are associated with several tags \nregarding the issue of private lending. In the proposed dataset, called eCAIL, 54 the \ntagged sentences are considered as rationales. Given two legal cases, the cross-case \nsentences with identical labels are pro-rationales for the matching task, while sen-\ntences with different labels are con-rationales. A matching label is assigned for the \ncase pair according the tag-overlapping: if there is a overlapping of more than 10 \ntags the cases are considered as matching, otherwise if there is no overlapping the \nlabel corresponds to mismatching, and an overlapping with less than (or equal to) 10 \ntags is considered as partially matching. The dataset provides 6K legal case pairs, \nwith rationales and explanations (the concatenation of all the overlapped tags) for \nthe matching labels.\nYu et al. (2022b) also provide ELAM, a dataset for explainable legal case match-\ning task, containing 5K legal case pairs with the associated matching label, ration-\nales, their alignments and the explanations for the matching decision. The authors \ncollected the legal cases online, 55 which refer to the crime of obstruction of the \nsocial management order. Each case is associated with several legal-related tags. To \npair the legal cases, the authors randomly selected 1250 query cases and constructed \na pool of candidates for each query. From the candidate pool, a case is retrieved \nbased on the number of overlapping tags between the case and the query. Each sen-\ntence of a legal case pair is associated with a rationale label, with the support of \nlegal experts. The possible rationale labels are the following: not a rationale, a key \ncircumstance, a constitutive element of a crime, or a focus of disputes. The align-\nment of the rationales (i.e., pro and con rationales) and the matching label (match-\ning, partially matching or not matching) are then marked. Legal experts are also \nasked to provide explanations for their matching decision.\nILDC (Indian Legal Documents Corpus) (Malik et al. 2021) comprises about 35K \ncases from the Indian Supreme Court, annotated with the court decisions. 56 This is \na corpus intended for court judgment prediction and explanation, which requires a \nmodel to predict the final outcome (accept or reject, w.r.t. the appellant) and to pro-\nvide explanations for the given prediction. To this regard, a portion of the corpus is \nannotated with explanations given by legal experts, and ranked in order of impor -\ntance in such as way that a higher rank corresponds to an explanation that is more \nimportant for the final judgment. The dataset is divided in ILCD single and ILCDmulti , \ndepending on whether there is a single decision for documents having one or more \npetitions, or different decisions for documents with multiple appeals.\nKalamkar et  al. (2022) propose a corpus of 354 Indian legal judgment docu-\nments, annotated via crowd-sourcing activity with 12 different rhetorical roles, from \ndifferent courts (Supreme Court of India, High Courts and district-level courts). 57 \nThe annotation process is designed with the support of legal experts. The corpus \n55 https:// www. faxin. cn/.\n56 https:// github. com/ Explo ration- Lab/ CJPE.\n57 https:// legal- nlp- ekstep. github. io/ Compe titio ns/ Rheto rical- Role/.\n54 https:// github. com/ ruc- wjyu/ IOT- Match.\n919\n1 3\nTransformer-based language models for AI and law\nis intended for the automatic structuring of legal documents. A Transformer-based \nmodel is proposed as baseline for the benchmark. Moreover, the authors propose \nextractive/abstractive summarization and court judgment prediction tasks as two \napplications of rhetorical roles, as they test how rhetorical roles could be useful for \nthose tasks. For extractive and abstractive summarization, they experiment with the \nLawBriefs corpus, which comprises 285 expert-authored extractive summaries of \nIndian court judgments. For the court judgment prediction task, experiments were \nconducted using the ILDC corpus (Malik et al. 2021).\nTwo further legal datasets for rhetorical role identification are introduced \nin (Bhattacharya et al. 2021). One dataset contains 50 cases from the Supreme Court \nof India belonging to five law domains: criminal, land and property, constitutional, \nlabour/industrial and intellectual property rights. Such documents are gathered from \nThomson Reuters Westlaw India website.58 The other dataset contains 50 cases from \nthe UK Supreme Court, gathered from the official website of the court. 59 Both the \ndatasets are labeled with the following seven rhetorical roles: “Facts”, “Ruling by \nLower Court”, “Argument”, “Statute”, “Precedent”, “Ratio of the decision” and \n“Ruling by Present Court”.\nPaul et  al. (2022b) introduce a pre-training corpus consisting of about 5.4M \nIndian court cases. The documents are gathered from several web platforms and \ncome from the Supreme Court and many High Courts of India. The corpus cov -\ners various court case domains as well as more than 1K central government acts. \nThe authors further pre-train Legal-BERT@aueb and Legal-BERT@stanford on the \nproposed corpus and assess its pre-training effectiveness considering several down-\nstream benchmarks, for the Indian as well as English languages. The performance \nof the pre-trained models have been compared to the BERT, the original Legal-\nBERT@aueb and Legal-BERT@stanford.\nBhattacharya et al. (2019a) gather about 17K legal cases of the Supreme Court of \nIndia through the website of Westlaw India, which provides documents and related \nsummaries written by domain experts. The authors perform a systematic compari-\nson of several summarization algorithms, such as traditional unsupervised extrac-\ntive methods (e.g., latent semantic analysis), neural unsupervised extractive methods \n(e.g., Restricted Boltzmann Machines  (Verma and Nidhi 2018)), and summariza-\ntion methods specifically conceived for legal documents, both unsupervised (Case-\nSummarizer (Polsley et al. 2016)) and supervised (LetSum (Farzindar and Lapalme \n2004)).\nShukla et al. (2022) provide three legal summarization datasets60 gathering docu-\nments from the Indian and UK laws. The first dataset is Indian-Abstractive dataset \n(IN-Abs), with about 7K cases of Indian Supreme Court judgments, obtained from \nthe website of Legal Information Institute of India, 61 and corresponding abstractive \nsummaries. The second dataset is Indian-Extractive dataset (IN-Ext), with 50 case \n58 http:// www. westl awind ia. com.\n59 https:// www. supre mecou rt. uk/ decid ed- cases/.\n60 Available at https:// github. com/ Law- AI/ summa rizat ion.\n61 http:// www. liiofi  ndia. org/ in/ cases/ cen/ INSC/.\n920 C. M. Greco, A. Tagarelli \n1 3\ndocuments of the Indian Supreme Court labeled with six rhetorical roles (i.e., facts, \nargument, statute, precedent, ratio of the decision, and ruling by present court) and \nextractively summarized by domain experts, providing a summary for each rhetori-\ncal segment separately (with the exception of ratio and precedent segments that are \nsummarized together). The third dataset is UK-Abstractive dataset (UK-Abs), with \n793 case judgments gathered from the website of the UK Supreme Court, which \nprovides also the press (abstractive) summaries of the cases, divided in three seg-\nments: “Background to the Appeal”, “Judgment”, and “Reasons for Judgment”. \nThe authors specify three criteria for the evaluation of methods: document-level \nsummaries, segment-wise evaluations (i.e., how the summary represents the logi-\ncal rhetorical segments in the legal case), and how the summaries are evaluated by \ndomain-experts.\nNiklaus et al. (2022) augment the Swiss-Judgment-Prediction (SJP) dataset intro-\nduced in (Niklaus et al. 2021) via machine translation, i.e., translating a document \nwritten in one of the three languages (German, Italian, French) into the remaining \ntwo languages. A second version of the dataset is also provided by further augment-\ning SJP with Indian cases of the ILDC corpus, provided by Malik et al. (2021). To \nthis regard, they translate all the Indian cases reported in the corpus to German, \nFrench and Italian. The authors evaluate several TLMs in relation to cross-domain \n(i.e., different legal ares), cross-regional (i.e., different regions) and cross-juris-\ndiction (from Indian to Swiss) transfer learning, whose discussion is demanded to \nSect. 4.7.\nLEX Rosetta (Savelka et al. 2021) propose a multilingual dataset of about 89K \nannotated sentences for the task of automatic functional segmentation, i.e., segment-\ning adjudicatory decisions of cases according to the functional role of the parts. 62 \nThe sentences are from 807 documents of several courts, gathered from different \nsources that include seven countries (Canada, Czech Republic, France, Germany, \nItaly, Poland, USA), and annotated according to the following types: out of scopes \n(i.e., sentences that are outside the main document, such as editorial content and \nappendices), heading (i.e., markers of a section), background (i.e., sentences explain-\ning facts, claims and procedural background), analysis (i.e., sentences containing the \ncourt reasoning and applications of law to the facts), introductory summary (i.e., a \nsummary of the discussed case), and outcome (i.e., sentences describing the final \ndecision). The dataset is used to test whatever GRU-based models generalize on dif-\nferent contexts (countries), in the segmentation of cases in three functional types \n(Background, Analysis and Outcome). To this end, the authors analyze the use of \nmultilingual sentence embeddings of predictive models in three versions: training \nthe model on a single context and evaluating transfer learning on other unseen con-\ntexts; training the model on a set of contexts and evaluating transfer learning on \nother unseen contests; and pooling the data of the target context with data from the \nother contexts. Results have shown that the second and third versions of the model \nare more effective.\n62 https:// github. com/ lexro setta/ casel aw_ funct ional_ segme ntati on_ multi lingu al.\n921\n1 3\nTransformer-based language models for AI and law\n3.4.2  Law code data\nSARA  (Holzenberger et al. 2020) is a dataset for statutory reasoning on US tax law.63 \nThis dataset is comprised of a set of rules extracted from the statutes of the US \nInternal Revenue Code (IRC),64 along with a set of questions which would require to \nrefer to the rules for being answered correctly. In fact, IRC contains rules and defini-\ntions for the imposition and calculation of taxes, and it is subdivided into sections \ndefining one or more terms (e.g., employment, employer and wages). Each section \nis normally organized around a general rule, followed by a number of exceptions, \nand each of its subsections refers to a certain number of slots, which may be filled \nby existing entities. IRC can hence be framed as a set of predicates formulated in \nhuman language so as to require a system to determine whether a subsection applies, \nand to identify and fill the slots mentioned. Statutory reasoning is addressed as a \ntask of entailment and a task of question answering. In the first task, two paragraphs \nare manually created for each subsection as test cases: the one describes a case to \nwhich the statutes apply, the other one describes a case to which the statutes do not \napply. In the second task, test cases are created to predict how much tax a person \nowes, considering all the statutes and applying arithmetic calculations. In general, \nthis dataset offers some features that allow for reasoning on several aspects, such as \nreasoning on time, numbers, cross-reference and common-sense. To test the abilities \nof NLP models on the statutory reasoning problem, the authors have pre-trained the \nmodels (e.g., Legal-BERT@jhu) on a large legal corpus obtained extracting tax law \ndocuments from the Caselaw Access Project, 65 private letter ruling from the Inter -\nnational Revenue Service (IRS)66 and unpublished US tax Court cases.\nBSARD (Louis and Spanakis 2022) is a French dataset composed of more than \n1.1K legal questions labeled by domain experts with relevant articles selected from \nthe 22K law articles gathered from 32 publicly available Belgian codes.67 The set of \nquestions and associated relevant articles are obtained in collaboration with Droits \nQuotidiens (DQ), an organization composed of a team of experienced jurists, which \nevery year receives many questions from citizens seeking advice on legal issues, \nretrieves the articles relevant to the questions asked, answers the questions in a \nmanner comprehensible to the applicant and categorizes the set of questions, legal \nreferences and answers with tags. The resulting corpus contains a large number of \nlegal topics (related to social security, work, family, justice and so on). BSARD is \nintended for statutory article retrieval.\nGCL (Papaloukas et al. 2021) is a dataset of about 47K documents regarding Greek  \nlegislation and designed for the multi-granular topic classification task, which \nrequires to detect the thematic topic that is representative of a legal document.68 The  \n63 https:// nlp. jhu. edu/ law/.\n64 https:// uscode. house. gov/ browse/ prelim@ title 26 & editi on= prelim.\n65 https:// case. law/.\n66 https:// www. irs. gov/ tax- exempt- bonds/ teb- priva te- letter- ruling- some- basic- conce pts.\n67 https:// github. com/ maast richt lawte ch/ bsard.\n68 https:// huggi ngface. co/ datas ets/ greek_ legal_ code.\n922 C. M. Greco, A. Tagarelli \n1 3\nthematic topics are available in multi-level hierarchy. The main data source for \nthis dataset is the Permanent Greek Legislation Code—Raptarchis, a catalogue of \nGreek legislation available through the portal e-Themis. 69 The portal provides a \nthematic index for the catalogue, reflecting the thematic hierarchical categories \n(topics). The hierarchy is dictated by the structural division in volumes, chapters \nand subjects, which reflect the levels of thematic topics. The classification task \nin GLC is divided into three sub-tasks, each of them deals with a level of the \nhierarchy.\nBesides statutes, several benchmarks have been developed about contracts  \nof different types. CUAD (Hendrycks et  al. 2021) is a dataset specialized for \nlegal contract review. 70 It includes more than 500 contracts, varying in type \nand length, with 13K annotations across 41 category labels provided by legal \nexperts. Such category labels regard general information, such as party names, \ndates, renewal terms and so on, as well as restrictive covenants and revenue risks. \nLanguage models are required to detect the portions of a contract (the clauses) \nrelated to each label. Evaluations of such models as BERT, AlBERT, RoBERTa \nand DeBERTa have highlighted that better performance are influenced by model \ndesign and training set size.\nLeivaditi et  al. (2020) provide a dataset containing 179 annotated documents \nregarding lease contracts. The annotations consist of entities (related to parties, \nproperty, terms and rent conditions, dates/periods) and red flags, i.e., terms or sen-\ntences indicating a potential risk for one o more parties (e.g., break option, guaran-\ntee transferable, right of first refusal to lease, bank guarantee), so that the dataset is \nmainly intended for supporting red flag detection and entity extraction tasks. The \ndocuments in the dataset are gathered through the EDGAR database, which is acces-\nsible through the US Securities and Exchange Commission (SEC). 71 The process of \nselecting the contracts to be annotated is performed using the BM25 ranking func-\ntion, which evaluates the relevance of documents w.r.t. keywords/queries that may \nsuggest the presence of red flags. The identification of such keywords/queries and \nthe process of annotation are supervised by domain experts.\nContractNLI  (Koreeda and Manning 2021) contains 607 annotated contracts \nregarding non-disclosure agreements.72 Such documents are gathered from Internet \nsearch engines and EDGAR. By comparing different non-disclosure agreements, a \nset of 17 hypotheses is obtained. Each document in annotated with respect to its rela-\ntion with one of the hypotheses (i.e., entailment, contradiction, or not mentioned). If \na document is annotated as entailing or contradicting, the spans (i.e., sentence or list \nitem within a sentence) composing the documents are annotated as evidence or not \n(binary label) of the associated entailment relationship.\n70 https:// github. com/ TheAt ticus Proje ct/ cuad/ The CUAD dataset is also available at atticusprojectai.org/\ncuad.\n71 https:// www. sec. gov/ edgar. shtml.\n72 https:// stanf ordnlp. github. io/ contr act- nli/.\n69 https:// www. secdi gital. gov. gr/e- themis/.\n923\n1 3\nTransformer-based language models for AI and law\nToS (Aumiller et al. 2021) is a dataset consisting of Term-of-Service documents, \nspecifically collected for topic similarity task. The documents include heterogeneous \ntopics due to the different web sources. Some of the most frequent topics regard lim-\nitation of liability, law and jurisdiction, warranty, and privacy. Topics are obtained \nin a hierarchical way splitting the documents into smaller chunks. The authors define \nand test a system built on TLMs, which revealed to largely outperform segmentation \nbaselines based on TF-IDF and bag-of-words.\nMAUD  (Wang et  al. 2023) is a dataset for the legal multi-choice reading com-\nprehension task and consists of legal texts extracted from 152 public merger agree-\nments gathered from EDGAR. Merger agreements are legal documents regarding \npublic target company acquisitions. In these documents there are special clauses, \ncalled deal points, that establish the conditions under which the parties are obliged \nto complete the acquisition. The deal points are extracted from the merger agree-\nments by lawyers working on the American Bar Association’s 2021 Public Target \nDeal Points Study (“ABA Study”). Moreover, a set of multiple-choice questions are \nanswered by the lawyers for each deal point. One or more questions can be asked for \na deal point, and each question can be answered by one or more answers. MAUD \ncontains 92 questions, 8K unique deal points annotations, 39K question-answer \nannotations (the examples), and 7 deal point categories (e.g., Conditions to Closing, \nDeal Protection and Related Provisions, Material Adverse Effect).\nManor and Li (2019) provide a dataset containing legal contracts and summaries \ngathered from two websites, TL;DRLegal 73 and TOS;DR, 74 the purpose of which \nis to clarify the content of contracts through summaries. More precisely, the for -\nmer, which deals mainly with software licences of companies, is used as a source \nfor collecting 84 sets of contract agreement sections and corresponding summaries, \nwhereas 412 sets are obtained from TOS;DR website, which focuses on user data \nand privacy topics of companies. The quality of the proposed summaries is verified \nby authors through an analysis of levels of abstraction, compression and readability.\nAnother important target of interest for the development of benchmarks is rep-\nresented by privacy policies. PolicyIE (Ahmad et al. 2021) is a corpus for automat-\ning fine-grained information extraction of privacy policies, especially through intent \nclassification and slot filling tasks. 75 PolicyIE consists of about 5K intent and 11K \nslot annotations of several privacy policies related to website and mobile applica-\ntions. The retrieved policy documents cover four privacy practices that are included \nin the General Data Protection Regulation (GDPR). Thus, sentences of such policy \ndocuments are categorized into the following GDPR-like intent classes: data col-\nlection/usage (i.e., what user information is collected, as well as the reason and the \nmodality in which it collected), data sharing/disclosure (i.e., what user information \nis shared with third parties, as well as the reason and the modality in which it is \nshared), data storage/retention (i.e., location and time period in which user informa-\ntion will be saved), data security/protection (i.e., what protection measures for user \n73 https:// tldrl egal. com/.\n74 https:// tosdr. org/.\n75 https:// github. com/ wasia hmad/ Polic yIE.\n924 C. M. Greco, A. Tagarelli \n1 3\ninformation are taken), other (i.e., privacy practices not included in the other catego-\nries). Each sentence is annotated with 18 slot labels, which can be categorized into \ntwo overlapping types: type-I, which comprises data and participants to the policy \npractices (e.g., data provider, data collected, data collector) and type-II, i.e., pur -\nposes, conditions, polarity and protection methods. The annotation procedure was \nperformed and monitored by domain experts.\nPrivacyQA  (Ravichander et  al. 2019) contains 1750 questions with over 3.5K \nannotations of relevant answers regarding to privacy policies of mobile applica-\ntions.76 Questions for a particular privacy policy are provided by crowd-workers, \nwhile the identification of the related answers are committed to legal experts, which \nalso provide meta-annotations on the relevance of the question, OPP-115 category, \nsubjectivity, and the likelihood that the answer to the input question is contained \ninto a privacy policy. The authors test the ability of different baselines on two tasks: \ndeciding if a question is answerable, and identifying evidences in the policies for a \ngiven question.\nPolicyQA (Ahmad et al. 2020) comprises about 25K triplets of question, passage \nand answer text, derived from segments of website privacy policy documents.77 The \ncorpus is designed so that the answer consists of small portions of the text that better \nidentify the target information in relation to the question. It is curated from the exist-\ning OPP-115 corpus  (Wilson et  al. 2016), which consists of 115 website policies \n(about 3.7K segments) annotated following domain-experts annotation schemes. \nThe annotation schemes categorize the policy segments in ten data practice catego-\nries (e.g., first party collection/use), which are further categorized in several practice \nattributes (e.g., user type), and each practice attribute is assigned a set of values; for \ninstance, user without account, user with account, other and unspecified. The anno-\ntated segments with the associated practice, attribute and value categories are used \nto form the PolicyQA corpus. Segments and categories are provided to skilled anno-\ntators to manually label the questions, for a total of 714 individual questions. The \nassociated QA task is answer span prediction given a policy segment. To this regard, \ntwo neural baselines are evaluated, one of this is based on BERT.\nBui et al. (2021) introduce a corpus 78 for the extraction and visualization in pri-\nvacy policies of personal data objects, i.e., spans in the text expressing types of user \ndata, and related privacy actions. The proposed corpus contains about 4.1K sen-\ntences and 2.6K annotated fine-grained data objects concerning several real-world \nprivacy policies. It is obtained exploiting the OPP-115 dataset as a starting point, \nopting for the top US websites that cover several domains like banking, e-commerce, \nsocial network. The data objects in the privacy policies are detected by annotators \nwith experiences in privacy and security research. The data objects are then labeled \nby the annotators, choosing among “collect\", “not_collect\", “share\" and “not_share\" \nlabels. Such labels indicate the privacy action performed on the user data (collection \nor sharing). The resulting annotation has also been revised with a semi-automated \n76 https:// github. com/ Abhil ashaR avich ander/ Priva cyQA_ EMNLP.\n77 https:// github. com/ wasia hmad/ Polic yQA.\n78 https:// github. com/ um- rtcl/ piext ract_ datas et.\n925\n1 3\nTransformer-based language models for AI and law\nprocess to improve the annotation quality, which involves the use of tools for correc-\ntion and pre-annotation. The final corpus is used to train and evaluate a neural NER \nmodel, called PI-Extract, on the extraction of personal data objects and privacy \nactions. The task is formulated as a sequence labeling problem, which is to assign a \nlabel for each token of a given sentence.\nRelevant benchmarks have been built by considering multilingual and/or multi-\ntask evaluation scenarios. For instance, COVID-19 Exceptional Measures  (Tzi-\nafas et al. 2021) is a collection of legal, manually-annotated documents regarding \nCOVID-19 exceptional measures across 21 European countries for multilingual \nclassification task. To this end, feature-based methods and XLM-RoBERTa pre-\ntrained on the collection have been evaluated, showing best results in the use of the \ndomain-adapted TLMs.\nMultiEURLEX (Chalkidis et al. 2021a) consists of European Union laws, anno-\ntated with multiple labels and translated in 23 languages, with legal topic classifica-\ntion as supported task. 79 The authors experiment with monolingual BERT models, \npre-trained in Romance, Slavic, Germanic and Uralic languages, and multilingual \nmodels (mT5 and XLM-RoBERTa) which are evaluated for cross-lingual legal \ntext classification on this benchmark. The experimentation focuses mainly on the \nzero-shot cross-lingual transfer, namely one-to-many setting, in which a multilin-\ngual model is fine-tuned on one language and evaluated in the other 22 languages. \nHowever, models are also evaluated on one-to-one (training and testing on the same \nlanguage) and many-to-many (training and testing on all languages) settings. Adap-\ntation strategies on the multilingual models are applied to avoid the catastrophic for-\ngetting of multilingual knowledge when fine-tuning on one source language only, \nsignificantly improving the zero-shot cross-lingual transfer. In the one-to-one set-\nting, multilingual models prove to be competitive against monolingual models.\nEUR-Lex-Sum (Aumiller et al. 2022) is a multi- and cross-lingual dataset contain-\ning about 32K pairs of documents and summaries in 24 languages. 80 Each language \ncomprises up to 1500 pairs. Documents consist of legal acts retrieved from the Euro-\npean Union law website, 81 375 of which are legal acts written in each of the lan-\nguages. Summaries are structured following particular guidelines, for example there \nare sections dedicated to key points, background, key terms and so on. The authors \nevaluate several zero-shot extractive baselines, one of which is a version of LexRank \nthat receives chunks (based on existing separators in the text) and uses embeddings \ngenerated by SBERT, and cross-lingual baselines, including one based on LED with \ncapability of greedily chunking the text if document sizes exceed the model’s maxi-\nmum input length.\nLegalNERo (Pais et al. 2021) contains 370 legal documents, designed for NER \ntasks and manually annotated according to five coarse-grained classes: person, \nlocation, organization, time expressions, and legal document references. 82 The \n79 https:// huggi ngface. co/ datas ets/ multi_ eurlex.\n80 https:// github. com/ achou han93/ eur- lex- sum.\n81 https:// eur- lex. europa. eu/.\n82 https:// lod- cloud. net/ datas et/ racai- legal nero.\n926 C. M. Greco, A. Tagarelli \n1 3\ndocuments are extracted from a larger corpus, called MARCELL-RO, 83 containing \nseveral documents from national legislation (e.g., decrees, regulation, laws) of seven \ncountries, Romania included. The authors evaluate a baseline based on BiLSTM and \nCRF, which takes as input a text representation obtained through FastText.84\nPLUE (Chi et al. 2023) is a multi-task benchmark that collects several privacy \npolicy datasets (including the aforementioned datasets) to evaluate NLP methods \nover various privacy policy tasks, namely classification, question-answering, intent \nclassification, slot filling, name entity recognition. 85 In particular, PLUE contains \nthe following datasets: OPP-115, APP-350 (Zimmeck et al. 2019), PrivacyQA, Poli-\ncyQA, PolicyIE, and PI-Extract (the dataset used in (Bui et al. 2021)). To enable \nmodel pre-training on the domain, Chi et al. (2023) also provide a large corpus using \nMAPS  (Zimmeck et  al. 2019), a corpus of 441K mobile application privacy poli-\ncies, and the Princeton-Leuven Longitudinal Corpus (PLLC)  (Amos et  al. 2021), \ncontaining bout 130K privacy policies of websites. From the combination of the two \ncorpora, a pre-training corpus with 332M words is created. The authors evaluated \nseveral TLMs as baseline, previously pre-trained on MAPS and PLLC and then fine-\ntuned on the PLUE datasets.\nDrawzeski et al. (2021) introduce a multilingual corpus for the analysis of fair -\nness of online terms of service. The dataset contains 100 contracts, derived from 25 \nToS documents annotated in four languages (English, German, Italian and Polish) \nand extracted from an existing corpus (Lippi et al. 2019). In each contract, poten-\ntially unfair clauses are labeled with one of the nine possible unfairness categories, \nnamely arbitration, unilateral change, content removal, jurisdiction (i.e., which \ncourts will have jurisdiction over disputes under the contract), choice of law (i.e., \nwhich law will regulate the contract), limitation of liability, unilateral termination, \ncontract by using (i.e., the use of a service binds the consumer to the terms of use \nof the service without being required to indicate that she/he has read and accepted \nthem), and privacy included (i.e., the use of the service implies the acceptance of \nthe related privacy policy). Moreover, for each category the degree of the unfairness \nis indicated with three numerical values: 1 for clear fairness, 2 for potential unfair -\nness, and 3 for clear unfairness. Four types of discrepancies are observed across the \nlanguage versions of the same contract, relating to the sentence structures, errors or \ninaccuracies in translation into the target languages, absence of some clauses in the \ndifferent language versions and the choice of legal terminology.\n3.4.3  Hybrid data\nLexGLUE (Chalkidis et al. 2022b) is a collection of seven existing legal NLP data-\nsets86 for evaluating models across several legal tasks, which include multi-label \nclassification, multi-class classification and multiple choice question-answering:\n84 https:// fastt ext. cc/.\n85 https:// github. com/ JFChi/ PLUE.\n86 https:// huggi ngface. co/ datas ets/ lex_ glue.\n83 https:// marce ll- proje ct. eu/.\n927\n1 3\nTransformer-based language models for AI and law\n– ECtHR Tasks A & B for multi-label classification of allegations regarding vio-\nlations of the European Convention of Human Rights (ECHR) provisions. The \ndataset is used to test models on article violation prediction (Task A (Chalkidis \net al. 2019a)) and alleged violation prediction (Task B (Chalkidis et al. 2021c)). \nIn both Task A and Task B, the total number of ECHR articles is reduced to 10, \ndiscarding those that are rarely discussed, cannot be violated or are not depend-\ning on the facts of a case.\n– The English part of Multi-EURLEX (Chalkidis et al. 2021a) for multi-label clas-\nsification on European Union (EU) legislation. It includes different labeling \ngranularity levels (from 21 to 7K EuroVoc concepts). In LexGLUE, the 100 most \nfrequent labels from the second level of granularity (567 total labels) are consid-\nered.\n– SCOTUS87 for multi-class classification on US Supreme Court opinions. In Lex-\nGLUE, SCOTUS opinions are associated with 14 issue areas (e.g., Economic \nActivity, Criminal Procedure, Civil Rights) obtained through the Supreme Court \nDataBase.88\n– LEDGAR  (Tuggener et al. 2020) for multi-class classification on contract provi-\nsions gathered from the US Securities and Exchange Commission (SEC) docu-\nments. In LexGLUE, a subset of the original dataset is considered, derived from \nthe 100 most frequent labels.\n– UNFAIR-ToS  (Lippi et  al. 2019) for multi-label classification on contracts \nbetween providers and users of services (i.e., terms of service) with 8 classifica-\ntion labels.\n– CaseHOLD  (Zheng et  al. 2021) for multiple choice question-answering about \nholdings of US court cases gathered from the Harvard Law Library case law cor-\npus.\n Chalkidis et al. (2022b) evaluate BERT, RoBERTa, DeBERTa, Longformer, Big-\nBird, Legal-BERT@aueb and Legal-BERT@stanford on LexGLUE. For ECtHR \nand SCOTUS, the authors employ a hierarchical variant of the models, follow -\ning Chalkidis et al. (2021c). The domain-adapted models, i.e., Legal-BERT@aueb \nand Legal-BERT@stanford, performed overall better than competitors, with large \nimprovement in US case law data.\nFairLex  (Chalkidis et  al. 2022c) comprises four datasets (ECtHR, SCOTUS, \nSwiss-Judgment-Prediction and CAIL (Wang et  al. 2021)) for the evaluation of \nmodel fairness.89 To this end, it includes three groups of fairness attributes: demo-\ngraphics, regional and legal area. The first group regard biases relating to factors \nsuch as gender, age and race. The second and third groups aim to alleviate disparity, \nrespectively, in regions of a given jurisdiction and in different areas of law. Moreo-\nver, it contains five languages (Chinese, English, French, German and Italian) and \nfour jurisdictions (China, USA, Switzerland and European Council). The authors \n87 https:// www. supre mecou rt. gov/.\n88 http:// supre mecou rtdat abase. org/.\n89 https:// huggi ngface. co/ datas ets/ coast alcph/ fairl ex.\n928 C. M. Greco, A. Tagarelli \n1 3\nalso provide four hierarchical BERT-based models, one for each dataset, as baselines \nfor the benchmark. Such models are similar to (Chalkidis et al. 2021c) and further \npre-trained on the specific dataset. The models are warm-started from MiniLMv2 \ncheckpoints, using a distilled version of RoBERTa for the English version and a dis-\ntilled version of XLM-R for the other languages. Experimental results show that the \nmodels have some disparity in performance. In particular, in the ECtHR task there is \na disparity related to defendant state and applicant’s gender, while for the FSCS task \nthere is disparity related to language (Italian versus French and German), legal areas \n(penal law versus the others) and court regions (Switzerland courts versus federation \ncourts). Court regions disparity is noted also in the CAIL task (Beijing courts versus \nSichuan courts). However, disparities in performance can be influenced by general \nfactors based on the distribution of data.\nBhattacharya et al. (2020a) collect documents of the Supreme Court of India and \nstatutes in the Indian judiciary through the Thomson Reuters Westlaw India web-\nsite,90 for a task of document similarity. In particular, they propose and evaluate an \napproach based on precedent citation network augmented with the hierarchy of legal \nstatutes, in order to encompass also the knowledge of legal text’s hierarchy.\nPile of Law (Henderson et al. 2022) is a legal corpus designed to address ethical \nissues in the pre-training phase. 91 It is collected from 35 EU and US data sources \nand covers several legal sub-domains, such as court opinions, administrative rules, \ncontract and legislative records, for a total of about 10 M documents. Such data has \nalready implicit filters which reflect legal standards of the specific jurisdiction, but \nthe authors note that not all of them have been detected and such norms can vary \nrespect to the jurisdiction. By performing filtering rules on the data, the proposed \ndataset respects the legal norms of governments and courts regarding the presence \nof toxic (offensive or obscene terms) or private content and prevents a model to learn \nsuch information. The authors demonstrate that the dataset can be used to learn con-\ntextual privacy/toxicity rules, as it respects the variation in the different privacy/tox-\nicity norms. For example, they demonstrate models pre-trained on Pile of Law can \nlearn contextual privacy rules with regard to the use of pseudonyms in immigration \ncourt and in civil litigation. In particular, a BERT-based model is trained on the data \nto predict whether a pseudonym should be used. Moreover, Henderson et al. (2022) \nprovide a baseline by pre-training BERT on the corpus from scratch. The resulting \nmodel, called PoL-BERT-Large, is fine-tuned and evaluated for a legal reasoning \ntask on CaseHOLD, reaching about the same performance reported in LexGLUE, \nand outperforms BERT. However, it does not outperform a BERT model trained \nexclusively on case law data. This is probably due to the extreme data diversity in \nthe corpus that limits the pre-training efficacy respect to competitors trained exclu-\nsively on task-oriented data such as CaseHOLD.\n90 https:// www. westl awasia. com/.\n91 https:// huggi ngface. co/ datas ets/ pile- of- law/ pile- of- law.\n929\n1 3\nTransformer-based language models for AI and law\n4  Methods\nIn this section we discuss the main methods based on the TLMs described in Sect. 2 \nand their application to the problems and tasks presented in Sect.  3. More specifi-\ncally, we organize our presentation of the methods into six parts. Starting from early \nmethods to more complex architectural settings, the first part (Sect.  4.1) includes \nmethods that are mainly based on fine-tuning BERT and the other TLMs. Given \nthe importance of the COLIEE competition over the past few years, we dedicate a \nseparate part (Sect.  4.2) to the methods that were designed and competed for the \nCOLIEE tasks. Section  4.3 discusses domain-adaptive approaches, which require \npre-training of TLMs on legal corpora. Section  4.4 focuses on approaches coping \nwith the text limitation length of BERT and other TLMs. Section  4.5 discusses \nGPT-based methods. Sections  4.6 and 4.7 are devoted to non-English and multilin-\ngual approaches, respectively. Finally, Sect.  4.8 discusses explainability and inter -\npretability in TLM-based methods.\nAs a guide to our discussion, Tables  4, 5, 6 and 7 report on main characteris-\ntics of the approaches that we shall describe through this section, focusing on the \nadopted TLMs, the downstream tasks for which the approaches were designed, the \ntypes of data and languages, and whether a method is conceived to deal with long \ndocuments.\nIt should be noted that our objective is not providing a fully detailed descrip-\ntion of each work, and most details about techniques that are not pertinent to TLMs \nmight be discarded; also, we will not delve into the experimental results. Clearly, we \nrefer the interested reader to the original works for further information.\n4.1  Task‑adaptive methods\nEarly applications of BERT and related models to the legal domain concern \napproaches that make TLMs adaptive to a legal specific task, i.e., they directly fine-\ntune an original, general-domain pre-trained model to the task at hand. Accordingly \nto the date of release of BERT, such applications have started being developed since \n2019. For instance, Howe et  al. (2019) provide a comparative evaluation of vari-\nous text classification approaches, including BERT, topic models and word embed-\ndings, for classifying Singapore Supreme Court judgments into legal areas. Gain \net  al. (2019) fine-tune BERT for a sentence-pair classification task to address the \nCOLIEE-2019 Task 4 context. Rabelo et al. (2019a) fine-tune BERT on the train-\ning data available for the COLIEE-2019 Task 2, and use the model in the post-\nprocessing stage of a framework that exploits a combination of a multi-word token \nsimilarity score and a noun-phrase similarity score to identify candidate entailing \nparagraphs. This combined approach was ranked first for the COLIEE-2019 Task \n2, with an F-score of 0.70 on the official COLIEE test dataset. In the same con-\ntext of COLIEE-2019 Task 2, Yamada and Tokunaga (2019) fine-tune BERT for a \nsentence-pair classification task, where each entailed fragment and target paragraphs \nare regarded as sentences. Three variants are defined to handle different lengths of \nan input sentence-pair, and an ensemble model BERT-vote is implemented using a \n930 C. M. Greco, A. Tagarelli \n1 3\nTable 4  Summary of main methods discussed in Sects. 4.1 and 4.2\nMethod Ref. Downstream tasks Lang. Data Long docs?\nFine-tuned BERT  Howe et al. (2019) TC EN Legal cases No\nFine-tuned BERT  Rabelo et al. (2019a) COLIEE-2019 Task 2 EN Legal cases No\n Yamada and Tokunaga (2019)\nFine-tuned BERT  Gain et al. (2019) COLIEE-2019 Task 4 EN Civil code No\nFine-tuned BERT w/TF-IDF  Gao et al. (2020) RRL EN Legal cases No\nFine-tuned BERT w/ and w/o frozen \nlayers\n Elwany et al. (2019) NER EN Contracts No\nFine-tuned BERT in regression form  Sanchez et al. (2020) PR EN Legal news articles No\nFine-tuned BERT  Ravichander et al. (2019) QA EN Contracts No\nFine-tuned BERT  Ahmad et al. (2020) QA EN Contracts No\nBiLSTM-CRF + BERT (PI-Extract)  Bui et al. (2021) NER EN Contracts No\nFine-tuned BERT  Mistica et al. (2021) TC EN Help requests No\nFine-tuned RoBERTa  Westermann et al. (2020) COLIEE-2020 Task 4 EN Civil code No\nFine-tuned RoBERTa + BiLSTM  Majumder and Das (2020) RRL EN Legal cases No\nFine-tuned RoBERTa  Vold and Conrad (2021) QA EN PRIVACYQA No\nFine-tuned BERT and RoBERTa  Chalkidis et al. (2020a) TC EN EURLEX57K; MIMICIII; AMAZON13K No\nFine-tuned RoBERTa  Savelka et al. (2020), Savelka \nand Ashley (2021)\nTC EN Legal cases No\nFine-tuned T5  Hudzina et al. (2020) COLIEE-2020 Task 4 EN Civil code No\nFine-tuned BERT and SpanBERT  Kruiper et al. (2021) MWE discovery EN SPAR.txt No\nFine-tuned RoBERTa + BigBird  Hong et al. (2021) IE EN Parole hearings No\nFine-tuned XLM-RoBERTa w/GLoVe \nembeddings\n Trias et al. (2021) NER EN CAP No\nFine-tuned BERT, RoBERTa, DistilBERT, \nXLNet and m-BERT\n Shaheen et al. (2020) LMTC EN JRC-Aquis and EURLEX57K No\nFine-tuned BERT  Chalkidis et al. (2019b) LMTC EN EURLEX57K No\n931\n1 3\nTransformer-based language models for AI and law\nTable 4  (continued)\nMethod Ref. Downstream tasks Lang. Data Long docs?\nTriplet-loss-network w/fine-tuned SBERT \nencoder\n Sarkar et al. (2021) TC EN Promissory and non-promissory sentences No\nFine-tuned BERT, RoBERTa, Trans-\nformer + BiLSTM + CRF; Fine-tuned \nMiniLM, UniLM, UniLMv2, MASS \nand BART \n Ahmad et al. (2021) Intent classification, SF EN Contracts No\nBERT; fine-tuned BERT; BERT in \nregression form; ensemble of BERT and \nfine-tuned BERT; BERTLaw\n Nguyen et al. (2020) COLIEE-2020 Tasks 1-4 EN Legal cases; civil code No\nEnsemble of fine-tuned BERT and BM25  Nguyen et al. (2021a) COLIEE-2021 Tasks 1, 2 JP Legal cases No\nFine-tuned BERT, BART, and RoBERTa \n+ Random Forest\n Rabelo et al. (2020) COLIEE-2020 Task 2 EN Legal cases No\nFine-tuned BERT  Alberts et al. (2020) COLIEE-2020 Task 2 EN Legal cases; NLI No\nmonoT5-zeroshot; fine-tuned monoT5 \nand DeBERTa; DeBERTa + monoT5 \nEnsemble\n Rosa et al. (2021) COLIEE-2021 Task 2 EN Case paragraphs, decision fragments No\nFine-tuned BERT w/truncated texts  Shao et al. (2020b) COLIEE-2020 Task 2 EN Legal cases No\nFine-tuned BERT and AlBERT  Shao et al. (2020a) COLIEE-2020 Tasks 3, 4 JP Civil code No\nFine-tuned RoBERTa w/NLI type detec-\ntion\n Rabelo et al. (2020) COLIEE-2020 Task 4 EN Civil code; SNLI No\nFine-tuned BERT  Schilder et al. (2021) COLIEE-2021 Task 3 JP Civil code No\nFine-tuned SBERT + TF-IDF vectoriza-\ntion + data enrichment; Legal-BERT-\nFP@aueb + TF-IDF vectorization + \ndata augmentation; BERTScore\n Wehnert et al. (2021) COLIEE-2021 Task 3 EN Civil code No\nFine-tuned BERT + XGBoost  Alberts et al. (2020) COLIEE-2020 Task 4 EN Civil code No\n932 C. M. Greco, A. Tagarelli \n1 3\nTable 4  (continued)\nMethod Ref. Downstream tasks Lang. Data Long docs?\nEnsembles of fine-tuned ELECTRA, T5 \nand Multee\n Schilder et al. (2021) COLIEE-2021 Task 4 JP Civil code No\nDistilRoBERTa and GPT-3  Schilder et al. (2021) COLIEE-2021 Task 5 JP Civil code No\nFine-tuned BERT ensemble w/augmenta-\ntion\n Yoshioka et al. (2021a, b) COLIEE-2021 Tasks 3-5 JP Civil code No\nFine-tuned BERT and Legal-BERT@\naueb; Fine-tuned BERT w/thesaurus \nconcept number\n Kim et al. (2021) COLIEE-2021 Tasks 2, 4, 5 EN Legal cases; civil code Yes\n933\n1 3\nTransformer-based language models for AI and law\nsimple voting method based on a majority prediction for each instance according to \nthe three BERT-based models.\nIn the context of the AILA-2020 task of rhetorical role labeling, Gao et al. (2020) \ncombine TF-IDF features with the fine-tuned BERT embeddings (i.e., the feature \nvectors of the [CLS] token) to fed logistic regression, SVM, and AdaBoost clas-\nsification models for final training and prediction. Results have shown that the joint \nsemantic-statistical feature modeling improves upon the individual models.\nElwany et al. (2019) fine-tune BERT on a proprietary corpus of legal contracts \nfor the task of extracting the fixed-term and the auto-renewing-term portions of an \nagreement. Two main strategies are developed to build the model depending on \nwhether the BERT layers are frozen or left unfrozen during the fine-tuning process.\nMarginal yet practically valuable improvements in both accuracy and training \nspeed were observed for the fine-tuned BERT model.\nSanchez et al. (2020) employ BERT in its regression form for passage retrieval, \ni.e., to predict a relevance score for a query-document pair in input, where each doc-\nument is represented by a combined title and summary field along with the content \nof a news article. On a similar fashion, Mistica et al. (2021) fine-tune BERT on a \ndataset of online help requests for a multi-label classification task pertinent to differ-\nent areas of law.\nRavichander et al. (2019) fine-tune BERT on the PrivacyQA benchmark for two \ntasks: to identify if a question is answerable and to identify the evidence in a privacy \npolicy that answers the question. to predict only the answerability of the question, \nthen at inference time, if the question is considered answerable, the model produces \nthe evidences. Analogously, Ahmad et al. (2020) fine-tune BERT on the PolicyQA \nbenchmark, whose task is to predict the answer span of a related question, given \nthe policy segment. This time, however, the model is first pre-trained on a dataset, \nSQuAD, which is regarded as related to the PolicyQA corpus. The positive impact \nof pre-training with SQuAD and fine-tuning on the benchmark is demonstrated by \nthe fact that the same model without pre-training or fine-tuning (or both) performed \nlower. A different use of BERT in the policy context is adopted by Bui et al. (2021), \nwhich propose PI-Extract for the extraction of personal data and related actions \nin privacy policies. PI-Extract consists of four BiLSTM-CRF-based NER models \ntrained on a privacy policy corpus introduced by the authors (cf. Sect.  3.4) which \ncontains four possible labels to be assigned to the personal data objects contained in \nan input sentence, where each label expresses the type of privacy action performed \non the personal data. Each NER model of Pi-Extract is specifically trained to predict \nonly one of four possible labels. In PI-Extract, BERT is used as a more effective \nalternative to word embedding based on GloVE (Pennington et al. 2014) to encode \nthe text input.\nA few studies have also focused on RoBERTa. Westermann et al. (2020) fine-tune \nRoBERTa for sentence-pair classification on the COLIEE-2020 Task 4. Majumder \nand Das (2020) provide RoBERTa embeddings to a BiLSTM for the AILA-2020 \ntask of rhetorical role labeling, reaching the highest rank at the competition. Vold \nand Conrad (2021) fine-tune RoBERTa on the PRIVACYQA dataset, a corpus of \nquestions about privacy policies associated with mobile applications, for binary \nor graded QA pair classification. Chalkidis et  al. (2020a) evaluate BERT and \n934 C. M. Greco, A. Tagarelli \n1 3\nRoBERTa, along with RNN-based Label-Wise Attention Networks (LWANs) and \nProbabilistic Label Trees (PLTs) on three English datasets for large-scale multi-label \ntext classification: EURLEX57K  (Chalkidis et  al. 2019b), MIMICIII, and AMA -\nZON13K. Experimental results have shown that BERT and RoBERTa outperform \nboth PLT-based methods and LWANs, and that a combination between BERT and \nLWAN can further improve performance.\nOn the task of determining whether a sentence describes facts or not,  Savelka \net  al. (2020) develop a meta type system to train models on data from different \ndomains and jurisdictions of annotated legal cases and assess performance of the \nmodels across the various domains. Datasets have been re-labeled in such a way as \nto obtain from the original rhetorical roles only “Fact” and “Non-Fact” labels. To \nthis purpose, RoBERTa base is fine-tuned on the different datasets and combined \ndatasets.\nSavelka and Ashley (2021) focus on the identification of explanatory sentences, \ni.e., to detect sentences that are useful for explaining selected legal concepts. In \norder to classify sentences according to four levels of usefulness, three RoBERTa-\nbase models are fine-tuned on different tasks: classifying retrieved sentences in \nterms of their value for explaining the legal concepts, sentence pair classification, \nand another type of sentence pair classification where one sentence is a provision \nof written law and the second is a retrieved sentence. Results point out important \ninteractions among a legal concept, the provision of law in which it is embedded, \nand retrieved sentences.\nBesides the RoBERTa alternative to BERT,  Hudzina et al. (2020) resort to the \npre-trained T5-base model for the COLIEE-2020 Task 4 and for denoising tasks \non Japanese Civil Code article texts and titles and on Wikibook articles about the \nJapanese Civil Code. Results have shown that T5 appears to overfit the training \ndata despite the multiple domain-specific tasks. Kruiper et al. (2021) utilize pre-\ntrained BERT-base and SpanBERT-base cased 92 to learn a sequence tagger for \na particular task of multi-word expression (MWE) for Automated Compliance \nChecking (ACC), which is to identify low-level constituent parts of a sentence, \ni.e., spans. The objective is to learn a semantic lexicon for ACC, which is essential \nfor semantic parsing as it enables the grounding of information units identified \nin a text (e.g., objects, interactions, and constraints). SpanBERT shows to be less \neffective than BERT, which is likely due to a mismatch between the span types \nand sizes used for the training of the original SpanBERT and the ones used to \ntrain the sequence tagger.\nHong et  al. (2021) investigate on information extraction of case factors from a \ncorpus of parole hearing transcripts, which is characterized by longer texts with \nfewer labels than in other NLP datasets. A two-step open-domain question answer -\ning approach is followed by using a Reducer to extract relevant text segments from \na hearing and a Producer model to generate answers from the text segments selected \nby the Reducer. A combination of a rule-based Reducer and a neural Producer show \nto yield improved performance. In particular, as the default Producer, a combination \n92 https:// github. com/ ruben kruip er/ SPaR. txt.\n935\n1 3\nTransformer-based language models for AI and law\nof RoBERTa and BigBird base is chosen due to its balance of long input length, \nlow computation requirements, and performance on different choices of prediction \nheads, namely classification head, MLM head and QA head.\nTrias et al. (2021) propose an ensemble language model consisting of a com-\nbination of a Transformer architecture with a finite state machine to extract \nnames from American English documents. 93 This exploits one pre-trained gen-\neral-purpose English language NER model based on Flair 94 trained on CoNLL03 \ndata, and another one trained on Harvard Caselaw Access Project, using GLoVe \nembeddings and XLM-RoBERTa.\nShaheen et al. (2020) focus on the problem of large multi-label text classifica-\ntion (LMTC) in the legal domain, using JRC-Aquis and EURLEX57K annotated \nwith the EuroVoc labels. In LMTC, the label space is comprised of thousands \nof labels, typically following a power-law label distribution and hierarchically \norganized. Pre-trained BERT, RoBERTa, DistilBERT, XLNet and m-BERT are \napplied on this task, using various training strategies such as gradual unfreez-\ning95 and slanted triangular learning rates, 96 in addition to fine-tuning. Results \nindicate that DistilBERT is better in retrieving the most probable label compared \nwith RoBERTa and BERT. Also in the LMTC context, Chalkidis et al. (2019b) \napply a number of methods, including BERT, to investigate which portions of \nthe documents in EURLEX57K are more informative. Results show that the title \nand recitals of each document contains enough information (and also allows to \novercome the BERT’s maximum text length limit); however, the approach fails in \nzero-shot learning.\nSarkar et al. (2021) define a triplet network for legal sentence classification. \nA triplet network consists of three instances of the same neural network with \nshared parameters, it takes as input three objects, i.e., the positive example and \nthe anchor, which belong to the same class, and the negative examples, which \nbelongs to a different class, and it outputs the distance between the anchor and \nthe positive example and the distance between the anchor and the negative exam-\nple. The representation of the anchor, positive and negative sentences are then \nused to compute the triplet loss function to minimize. The network encodes each \ninput sentence using Sentence-BERT as encoder, and the contextual sentence \nembedding is then fed to a two-layer perceptron. For the downstream classifica-\ntion task, a SVM with Radial Basis Function (RBF) kernel is used to compute \nthe probability that an input sentence is promissory; the sentence is eventually \nclassified as either promissory or non-promissory depending on a user-specified \nthreshold on the SVM output probability.\n93 https:// harva rd- almit. github. io/ legal- nlp.\n94 https:// github. com/ flair NLP/ flair.\n95 The training process is divided into multiple cycles, each consisting of several training epochs. Train-\ning starts after freezing all layers except for the last few layers in the first cycle one, then more layers are \nunfrozen gradually (from last to first layers) during later cycles.\n96 The learning rate is increased at the beginning of a training epoch up to the maximal learning rate, \nthen slowly reduced to refine the parameters.\n936 C. M. Greco, A. Tagarelli \n1 3\nAhmad et  al. (2021) deal with fine-grained information extraction of privacy \npolicies, specifically to identify sentences expressing privacy practices (intent \nclassification task) and to detect specific details as text spans into the sentences \n(slot filling task). Two alternative approaches are proposed to tackle the two tasks: \nmodeling intent classification and slot tagging either jointly as a sequence tag-\nging task, or separately by generating respectively labels and spans. For the first \napproach, BERT, RoBERTa, a Transformer model along with a BiLSTM encoder \nare trained to get contextual representations of the input; in particular, the embed-\nding of the special token [CLS] is used by a softmax classifier to predict the target \nintent, while the embeddings of the input tokens are used by another softmax clas-\nsifier to predict the slot labels combined with a conditional random field (CRF). \nResults show that BERT-based and RoBERTa-based sequence tagging models \noutperform the other baselines by a wide margin, and the use of CRF is benefical \nfor the slot tagging task but slightly degrades the performance for intent classifica-\ntion. For the second approach, MiniLM, UniLM, UniLMv2, MASS and BART are \nfine-tuned on the benchmark, and results show that seq2seq models outperform \nthe sequence tagging models in the slot filling task, with BART and UniLM-based \nmodels reaching best performances, while they have similar results on the intent \nclassification task.\n4.2  Enhanced methods for COLIEE tasks\n4.2.1  Case law retrieval (Task 1)\nIn the COLIEE-2021 competition, Nguyen et al. (2021a) consider BERT as semantic \nmodel and Rank-BM2597 as lexical model. In particular, the lexical model restricts \nthe searching space of candidate cases to the top-100 cases for a given query, \naccording to a lexical similarity score. After that, query and candidates are split into \nparagraphs and a lexical score matrix for each query-candidate pair is derived. In a \nsimilar way, a semantic score matrix for each query-candidate pair is derived using \nBERT, which is fine-tuned on a silver dataset based on the Task 1 raw data. Finally, \nthe most relevant cases for the given query is obtained by combining lexical and \nBERT-based scores. The overall performance on the test set turns out to be quite \npoor, probably due to prior passage of searching space restriction performed by the \nlexical model.\nIn the previous edition of COLIEE competition, the same team uses a similar \napproach, where BERT is fine-tuned for a text-pair classification task with the \nsupport of specific heuristics to extract text-pairs, and BM25 is used as lexical \nmodel (Nguyen et al. 2020). In that case, for each query the searching space consists \nof only 200 possible candidates, thus leading to significantly better results.\n97 https:// pypi. org/ proje ct/ rank- bm25/.\n937\n1 3\nTransformer-based language models for AI and law\nRossi and Kanoulas (2019), Li et al. (2021b), Althammer et al. (2021), Ma et al. \n(2021) and Shao et al. (2020b) propose methods dealing with the document length \nlimitation; therefore a discussion of such works is postponed to Sect. 4.4.\n4.2.2  Case law entailment (Task 2)\nOne of the first approaches to case law entailment is provided by Rabelo et  al. \n(2019a), which has been described in Sect.  4.1. In the 2020 edition, Rabelo et al. \n(2020) propose a Random Forest classifier to score each pair query-paragraph using \nfeatures from the entailment score of BERT fine-tuned on the Task 2 data, BART \nand RoBERTa fine-tuned on a generic entailment dataset, BERT fine-tuned on para-\nphrase detection (zero-shot setting), cosine similarity between sentence embeddings \nof the input pair and cosine similarity on bag-of-words of the noun phrases con-\ntained in the input pair. Data augmentation based on back translation is also carried \nout, however without significant evidence of improvement.\nAlberts et  al. (2020) exploit BERT in two ways to address the task in the \nCOLIEE-2020 competition: the first one is to compute cosine similarity scores \nbetween BERT embeddings after fine-tuning on the COLIEE training data, and \nthe second way is to derive features for a natural language inference (NLI) task by \napplying BERT on the NLI dataset. The two types of features, combined with BM25 \nbased ones, are fed as input to a XGBoost-based classifier (Chen et al. 2015), which \nachieved the third place in the competition.\nNguyen et  al. (2020) experiment with the same solution proposed for Task 1 \n(described in Sect. 4.2.1) together with BM25 scores. This is also improved by fine-\ntuning BERT on the training data provided for this task. A further variant is pro-\nposed in which BM25 is replaced with BERT fine-tuned on SigmaLaw, 98 a corpus \nof legal cases including cases from the US Supreme Court. Among the three ver -\nsions, the second one achieved the best F1 score reaching the leading position in the \ncompetition.\nIn the COLIEE-2021 competition, Nguyen et  al. (2021a) start from their own \napproach implemented for Task 1 to define a binary classification task for BERT \nfine-tuned using both the silver dataset, obtained from the Task 1 data, and a further \ngold dataset obtained from the Task 2 data. One of their submissions is a combina-\ntion of lexical score and a model trained on the Next Foreign Sentence Prediction \n(NFSP) task (Nguyen et al. 2021b), that is a binary classification task in which the \ntext-pairs are composed of sentences with different languages and the model has \nto understand the correct meaning of each sentence to determine if their seman-\ntics belong to two consecutive sentences (cf. Sect.  4.7). However, best results are \nobtained using their earlier approach.\nRosa et al. (2021) apply pre-trained Transformer models directly to the Task 2 of \nCOLIEE 2021, without any preliminary fine-tuning on the legal task. 99 More spe-\ncifically, they test monoT5 in two settings: zero-shot and fine-tuned on the Task 2 \n98 https:// osf. io/ qvg8s/.\n99 https:// github. com/ neura lmind- ai/ coliee.\n938 C. M. Greco, A. Tagarelli \n1 3\ndata of COLIEE 2020. In addition to monoT5, they also fine-tune DeBERTa on the \nlegal task using the Task 2 data of COLIEE 2021. To balance the amount of nega-\ntive and positive examples, they expand the positive examples generating a set of \nartificial fragments from the base case paragraphs through a sliding window strat-\negy. As a further solution, an ensemble of monoT5 and DeBERTa, both fine-tuned \non COLIEE 2020 data, is proposed. The models estimate a score for each fragment-\ncandidate paragraph pair to select the best set of candidate paragraphs for each case \nfragment. A candidate paragraph is chosen if its score complies with specific rules. \nFor the ensemble model, the selected paragraphs are the concatenation of the final \nset of DeBERTa and monoT5. The fine-tuned version of DeBERTa, monoT5 and \nthe ensemble model achieve the first 3 positions in the competition, with the ensem-\nble model on top position. However, once the ground-truth annotations of the test \nset have been released, the authors find out that monoT5 with zero-shot setting per -\nforms better than the single fine-tuned monoT5 and DeBERTa. This result leads the \nauthors to the conclusion that, in case of limited labeled data, a model with no adap-\ntions to the target domain can be more robust than fine-tuned models.\nIn the attempt of dealing with the text length limitation of BERT, Shao et  al. \n(2020b) consider either to truncate the input queries and paragraphs symmetrically, \nor to limit the tokens of the query to 128 and truncate the paragraph if the total \ntokens of query and candidate exceeds the limit. This is motivated by the observa-\ntion that most of the decision fragments in the training data is no longer than 128 \ntokens. In both cases, the final hidden vector corresponding to the [CLS] token is fed \nto a fully connected layer for the classification. The results on test set demonstrate \nthat the asymmetric truncation significantly improves the performance of the model. \nA third solution for the task proposed by the authors is to combine the output vec-\ntors of BERT (both with symmetric and asymmetric truncation), BM25 score and \nsome structural features to be fed to a RankSVM model.100\nNote that there have been other approaches, such as those proposed in (Li et al. \n2021b; Kim et al. 2021), effectively dealing with the length of the documents; as \nsuch, they will be discussed in Sect. 4.4.\n4.2.3  Statute law retrieval (Task 3)\nIn COLIEE 2020, Nguyen et  al. (2020) fine-tune bert-base-uncased on a \ntext-pair classification task considering all query-article pairs. Due to the unbalanc-\ning between positive and negative examples, they filter all the possible candidates \nto top-k articles according to the cosine similarity calculated comparing the TF-\nIDF vectorization of article and query. They combine a bert-base-uncased  \nmodel fine-tuned on text-pair classification and a variant of BERT, dubbed BERT-\nCC, previously fine-tuned on all data of the Civil Code with a masked language \nmodeling task and further fine-tuned on text-pair classification. This is motivated \nby the assumption that BERT-CC can compensate for the lack of domain-specific \nknowledge of BERT, so that together they can learn different linguistic features. \n100 https:// www. cs. corne ll. edu/ people/ tj/ svm_ light/ svm_ rank. html.\n939\n1 3\nTransformer-based language models for AI and law\nThis hypothesis is confirmed by the results on the test set in the competition, where \nthe ensemble model performed higher than the two separate models.\nShao et al. (2020a) address the task as a binary classification problem using the \nJapanese version of the data provided by organizers. Each question is paired with \nevery possible article and, due to the imbalance between positive and negative sam-\nples, the positive samples are oversampled up to 100 times. A Japanese version of \nBERT with whole word masking mechanism, called BERT-base_mecab-ipa -\ndic-char-4K_whole-word-mask (for short, BERTjpcwwm), and a Japanese \nversion of AlBERT101 are trained. In the first case, BERTjpcwwm is equipped with \ntwo settings, considering a maximum text length of 384 and 512, respectively. The \nrelevance of an article for a query is decided through a threshold. The ensemble of \nthese two BERTjpcwwm models and AlBERT model ranked first at COLIEE-2020.\nFor the 2021 edition, Schilder et al. (2021) fine-tune a pre-trained Japanese BERT \nmodel on pairwise sequence classification task, where a text-pair is composed of \nqueries and articles. The task is addressed by varying the BERT score along with \nthe maximum cosine similarity on validation set and using cosine similarity to sort \nentailing articles. However, the proposed system produced unsatisfactory results, \nprobably due to the lack of a sufficient number of training examples to adequately \nfine-tune BERT on COLIEE language domain.\nWehnert et al. (2021) propose three approaches based on BERT model for the \nEnglish version of the task. In the first approach, SBERT embeddings are com-\nbined with TF-IDF vectorization. To this end, the training data is increased with \ndocument enrichment strategies, in order to obtain article vectors that are as much \nunique as possible. In particular, each article can be enriched with hierarchical \nrelations between articles as metadata, crawling the Japanese open source com-\nmentary on the article 102 and using the training data of Task 4. For each query-\narticle pair, the cosine similarity scores are calculated ove TF-IDF and SBERT \nembeddings (paraphrase-distilroberta-base-v1). In the second \napproach, LEGAL-BERT@aueb is fine-tuned on sentence classification task. \nData manipulation is performed in order to have an augmented dataset. The soft-\nmax score derived from the model is then combined to the cosine similarity cal-\nculated on TF-IDF vectorization (cf. Sect.  4.3). The third approach involves the \nuse of BERTScore, which works as follows: the pairwise cosine similarities of all \ntoken-wise contextual embeddings from two sentences are first computed, then the \ntoken pairs between the two sentences which have the highest cosine similarity \nare selected, and those similarities are summed up and discounted by the words \nin the sentence to obtain precision, recall and the according F1-score. The first \napproach proved to be the best for the competition, which was ranked first at the \n2021 competition.\nNote that Aydemir et al. (2020) address the COLIEE-2020 task based on a multi-\nlanguage approach, which is described in Sect. 4.7. Also, in Sect. 4.4, we discuss the \n101 https:// github. com/ aline ar- corp/ albert- japan ese.\n102 https:// ja. wikib ooks. org/.\n940 C. M. Greco, A. Tagarelli \n1 3\nCOLIEE-2021 methods by Nguyen et al. (2021a) and Yoshioka et al. (2021b) deal-\ning with the length limitation of BERT.\n4.2.4  Statute law entailment (Task 4)\nFor the statute law entailment task, early Transformer-based solutions have been \nproposed by Gain et al. (2019), Westermann et al. (2020) and Hudzina et al. (2020) \n(cf. Sect. 4.1).\nIn COLIEE 2020, Alberts et al. (2020) use the last hidden layers bert-base-\ncased as the input for the XGBoost classifier, inspired by Gain et al. (2019). How-\never, such a combination reaches poor performance, probably due to the fact that \nBERT was not trained on legal tasks. Shao et al. (2020a) apply the same ensemble \nsystem used for Task 3, without however replicating the enhancement of the train-\ning samples, as for Task 4 the goal is to answer the queries based on the provided \nrelevant articles. This led to lower performance of the model, which ranked sixth in \nthe competition.\nRabelo et al. (2020) address the statute law entailment problem using RoBERTa \nfor natural language inference. They identify multiple NLI types in the statute law \ndataset, from which the model needs to recognize condition, conclusion and excep-\ntional cases. To add more training data, they include the SNLI dataset (Bowman \net al. 2015). A pre-processing step is performed to help the detection of the differ -\nent NLI types. This step consists in splitting the data in condition, conclusion and \nexceptional case, replacing references to prior cases or paragraphs with the refer -\nenced text, re-generating a sentence by negating the conclusion for the exceptional \ncase and extracting one relevant sentence that matches most terms with the query. \nThe model reaches the second position in the competition. The authors also adopt \na BERT model for natural language inference to address Task 4 of COLIEE 2021 \n(Kim et al. 2021). They select the most relevant candidate sentences for the query \nfollowing their previous approach (Rabelo et al. 2020) and training a BERT clas-\nsifier over triplets of query, article’ sentence and binary label (yes/no). To help the \npragmatic reasoning of BERT, the authors incorporate additional features consisting \nof the semantic category numbers of the Kadokawa thesaurus to the content words. \nThe inclusion of such a semantic information boosts the performance of the model, \nreaching the fourth position in the competition for Task 4, while the same model \nwithout the additional information reached the third-to-last position.\nNguyen et al. (2020) exploit the questions and the results of their system for the \nTask 3 (cf. Sect.  4.2.3) as input pairs for the classification prolem. On the develop-\nment set, top-2 articles extracted by TF-IDF from the Civil Code are added to the \ngold data for each question. The answer for the question is positive if at least one \npair is classified as positive by bert-base-uncased . As a second setting, only \nthe content of the questions is used as input, following the approach they used in \nCOLIEE 2019 (Rabelo et al. 2019b). Essentially, the entailment problem is trans-\nlated to a lawfulness classification task. BERT and BERTLaw (cf. Sect.  4.3) are \ntrained with law sentences of the Civil Code and previous years’ bar questions avail-\nable from COLIEE. As shown in the final scores on test set, the second setting with \n941\n1 3\nTransformer-based language models for AI and law\nBERTLaw reaches a score considerably higher than the other solutions proposed by \nthe same team as well as by other teams that contributed to the competition.\nSchilder et  al. (2021) define ensemble models based on ELECTRA and T5 \nto address the Japanese version of the statute entailment task. In the first case, a \nJapanese ELECTRA-small variant is chosen, 103 which turns out to be the best pre-\ntrained architecture for Japanese according to the Jensen-Shannon divergence (Lin \n1991), and measures the embedding distribution distances between positive and \nnegative examples. The model is trained layer by layer on the few training examples \navailable. Although the model obtains strong performance in the validation step, it \nachieves a low accuracy on test set, probably due to a potential over-fitting on the \navailable data. As regards T5, the best performing approach consists in fine-tuning \non COLIEE dataset and evaluating T5 embeddings (the other two approaches con-\nsist in adding span corruption in the fine-tuning step and pre-training T5 embed-\ndings from scratch, respectively). Besides ELECTRA and T5, the authors also \ninvestigate a multi-sentence natural language inference model, Multee (Trivedi et al. \n2019). The final submissions comprise ELECTRA, Multee and an ensemble of the \nfirst T5-based approach, ELECTRA and Multee. However, none of the three sub-\nmissions obtained satisfactory results.\nYoshioka et  al. (2021a) address the Japanese statute law entailment task using \nBERT-based ensemble methods with data augmentation. In particular, they augment \ntraining data with a focus on the logical mismatches between articles and questions. \nThe positive examples are composed of the same sentence used both as the query \nand as the article’s sentence, while negative examples are pairs of original sentences \nand their versions with judicial decision flipped. If one article contains multiple \ndecisions, then it is divided into smaller sentences containing one judicial decision, \nand if the split sentence expresses an exceptional case, then the judicial decision is \nflipped. Then, ten BERT-japanese models104 are fine-tuned on this dataset to evalu-\nate if the article entails a question or not. The models differ in the non-determinis-\ntic characteristics of fine-tuning process and the randomly selection of training set. \nThree ensemble models are then obtained by combining subsets of the aforemen-\ntioned ten models. In the competition, they reach the best three accuracy scores, thus \ntaking the top three positions.\nFurther studies on Task 4 are  those by Nguyen et  al. (2021a) and Kim et  al. \n(2021), which address the task by accounting for the document length limitation \n(cf. Sect.  4.4), Aydemir et  al. (2020), which propose a multi-language approach  \n(cf. Sect. 4.7), and Goebel et al. (2021), which address the task based on a domain-\nadaptive solution (cf. Sect. 4.3).\n4.2.5  Statute law question answering (Task 5)\nSchilder et  al. (2021) propose two solutions for the Japanese version of Task 5: \nusing DistilRoBERTa without any domain-specific training, and using GPT-3 \n103 https:// huggi ngface. co/ Cinna mon/ elect ra- small- japan ese- discr imina tor.\n104 https:// github. com/ cl- tohoku/ bert- japan ese.\n942 C. M. Greco, A. Tagarelli \n1 3\n(Brown et  al. 2020b) with a few-shot learning. DistilRoBERTa is a distilled ver -\nsion of RoBERTa for paraphrase detection. The authors use the implementation 105 \nof SBERT containing a pre-trained DistilRoBERTa model trained on paraphrase \ntext. The relevant articles to a given query are determined according to a similarity \nscore. GPT-3 is a massive auto-regressive language model based on Transformer \nthat obtained impressive results in generation and question-answering tasks with a \nfew-shot learning scenario. The model is available via OpenAI interface106 in differ-\nent size versions. The authors use the largest model, dubbed davinci, and the fastest \nmodel, dubbed ada. From the experimental results, it is observed that GPT-3 models \ncould not reach a sufficient level of comprehension on the legal domain, while the \nsystem based on DistilRoBERTa shows promising performance on the task.\nKim et al. (2021) combine the output of a traditional TF-IDF technique with the \nNLI system described for the English version of Task 4 (cf. Sect. 4.2.4). As for Task \n3, the additional semantic information enhances the model performance, allowing \nthe team to achieve the second position at the competition.\nYoshioka et al. (2021b) also consider the Japanese version of Task 5, although \nthey could not meet the deadline for the submissions. Three configurations of ensem-\nble models are defined by combining the submitted runs for the Task 3 (Yoshioka \net al. 2021b) (cf. Sect.  4.2.3) and for Task 4 (Yoshioka et al. 2021a) (cf. Sect.  4.4), \nunder the same conditions of the other participants. One of the three configurations, \nbased on the best run of Task 4, obtained similar performance compared to the \nbest model of the competition (Nguyen et al. 2021a). Also, Nguyen et al. (2021a) \npropose a multilingual approach for the task, therefore its discussion is referred to \nSect. 4.7).\n4.3  Domain‑adaptive pre‑training methods\nIn contrast to task-adaptive fine-tuning methods so far discussed, domain-adaptive \npre-training allows for deeply tailoring an out-of-the-box model to a specialized lan-\nguage domain (Wang et al. 2020a; Gururangan et al. 2020; Song et al. 2022). In the \nlegal domain, there are two main strategies that stand as alternative to the direct \napplication of a TLM for the downstream task, namely (i) to continue pre-training \nthe model on a legal corpus, or (ii) to pre-train the model from scratch on a legal \ncorpus. In this section, we discuss methods that developed TLMs based on one or \nboth the pre-training strategies and compared their performance effects on various \ntasks.\nOn different tasks of document review, Shaghaghian et  al. (2020) propose \nto further pre-train and pre-train from scratch BERT models according to dif-\nferent strategies of tokenization and configuration of the initial weights of the \nmodel. More specifically, SentencePiece tokenization is performed on a corpus \nof US SEC legal agreements to generate the same number of cased tokens as in \n105 https:// github. com/ UKPLab/ sente nce- trans forme rs.\n106 https:// openai. com/ api/.\n943\n1 3\nTransformer-based language models for AI and law\nbert-base-cased; only 36% of tokens were found to be shared between tokens \nfrom the input legal corpus and the tokens in the original BERT. A hybrid tokeniza-\ntion is also devised whereby the 500 most frequent words in the input legal corpus \nare added to the token set if they do not occur as unbroken tokens in the set of gen-\neral-domain tokens. Moreover, each of the BERT variants is also used as the teacher \nto customize as many DistilBERT models.\nBERTLaw (Nguyen and Nguyen 2021) is a model pre-trained on a collection of \nAmerican legal cases with 8.2 million sentences (182 million words). BERTLaw \nhas the same architecture and training objectives as BERT-base, however with an \noverlap between its vocabulary and the one in the original BERT-base model that is \nless than 50% of the size of the union of the two vocabularies. On the COLIEE-2019 \nTask 4, BERTLaw has shown to outperform BERT-base by almost 4% on validation \ndata and over 16% on test data.\nChalkidis et al. (2020b) are the first to propose both further pre-training (FP) and \npre-training from scratch (SC) BERT on legal corpora. Their models, hereinafter \nreferred to as Legal-BERT-FP@aueb and Legal-BERT-SC@aueb, 107 were trained \non a collection of legal documents, including EU and UK legislations, ECJ cases, \nECHR cases, US court cases, and US contracts. Legal-BERT-FP@aueb models are \nthe result of prolonged in-domain pre-training of BERT-base up to 500K steps, in \nall or a selection of the training legal corpora, whereas Legal-BERT-SC@aueb has \nstill the same architecture but with a new vocabulary equivalent in size to BERT-\nbase. Also, a smaller Legal-BERT-SC@aueb model is derived with 6 layers, 512 \nhidden units, and 8 attention heads (35M parameters, 32% the size of BERT-base). \nAll models were trained over the legal corpora in batches of 256 samples, including \nup to 512 sentence-piece tokens, with Adam optimizer and learning rate of 1e−4, \nas in the original BERT; nonetheless, for the fine-tuning stage, early-stopping was \nused instead of a fixed number of epochs, and more batch sizes, learning rates and \ndrop-out regularization rate than the recommended ones in  (Devlin et  al. 2019) \nwere experimented. Legal-BERT-SC@aueb has shown to achieve a significantly \nlower training loss than the smaller model, while the latter turns out to be com-\nparable to the original BERT-base. As for the Legal-BERT-FP@aueb models, the \ntraining loss can actually be improved when further pre-training on some selected \ncorpora (but not on the whole collection of corpora). Considering the downstream \ntasks (EurLex57K, ECHR-Cases, Contracts-NER), Legal-BERT-FP@aueb and \nLegal-BERT-SC@aueb models outperform BERT-base and are as good as or bet-\nter than the fine-tuned BERT-base, with more substantial gain in the ECHR-Cases \nmulti-label task. Legal-BERT-FP@aueb and Legal-BERT-SC@aueb models also \ntend to be comparable to each other on all but the ECHR-Cases tasks, where Legal-\nBERT-SC@aueb is clearly better. The small Legal-BERT-SC@aueb is highly com-\npetitive with at least one of the larger variants in most tasks, suggesting that archi-\ntecturally complex model may not be necessary when dealing with domain-specific \nsub-languages.\n107 https:// huggi ngface. co/ nlpau eb.\n944 C. M. Greco, A. Tagarelli \n1 3\nA legal BERT model is also developed by Holzenberger et al. (2020), in a context \nof evaluation of statutory reasoning (i.e., how to reason about an example case, based \non rules extracted from statutes), particularly for US tax regulations. This model, \nhereinafter referred to as Legal-BERT@jhu, 108 is obtained by further pre-training \nbert-base-cased on a portion of the case.law corpus, including both state and \nfederal cases, for a total of 900M tokens. On the legal questions and answers of the \ncorpus, Legal-BERT@jhu shows better perplexity than bert-base-cased  (i.e., \n2.7 against 14.4), which indicates improved adaptation to the legal queries. Legal-\nBERT@jhu was fine-tuned on the task of recognizing legal terms in the test set split \nof case.law, i.e., tokens or multi-token collocations that are defined in Black’s Law \nDictionary, a well-known legal dictionary.\nZheng et al. (2021) further investigate the use of BERT-based methods to pre-\ntrain TLMs on legal documents, starting from the premise that a substantial gain \nfrom law-specific pre-training might be prevented by fine-tuning tasks that are too \neasy and/or based on data that are inconsistent with the pre-training corpus. The \nproposed models, here dubbed Legal-BERT@stanford 109 are pre-trained on the \nHarvard Law case corpus (i.e., case.law), where a random sample of 10% of deci-\nsions was extracted from the corpus and used as holdout set. More specifically, the \nvariant Legal-BERT-FP@stanford is derived from the BERT-base model pre-trained \nfor an additional 1M steps using the case.law corpus and the same vocabulary as \nBERT (uncased), while the variant Legal-BERT-SC@stanford is pre-trained from \nscratch for 2M steps using the case.law corpus, with a custom legal domain-specific \nvocabulary created using SentencePiece tokenization. Both variants use the same \ntraining objectives as BERT, i.e., MLM and next sentence prediction; for the lat-\nter, regular expressions are used to ensure that legal citations are included as part \nof a segmented sentence. Through a number of legal tasks varying in difficulty and \ndomain-specificity, BERT has shown to already achieve high performance for easy \ntasks, while legal-specific pre-training takes small, resp. significant, advantage for \nmid-difficulty tasks that are not highly domain-specific, resp. high-difficulty and \ndomain-specific tasks.\nLeivaditi et al. (2020) further pre-train AlBERT on lease contracts. The result-\ning model, named ALeaseBERT, is fine-tuned and evaluated on the tasks of red flag \ndetection and entity extraction. In particular, for the red flag detection, the additional \npre-training proves to enhance the performance not only compared to lexical com-\npetitors, but also against AlBERT pre-trained from scratch as well as a fine-tuned \nAlBERT on the task.\nChi et al. (2023) further pre-train BERT, SpanBERT and ELECTRA on a pre-\ntraining corpus for privacy policy understanding provided in PLUE (cf. Sect.  3.4); \nthe resulting pre-trained models are then fine-tuned on the benchmarks included in \nPLUE. Results reveal the advantage of these further pre-trained models compared \nto their only task-adapted counterparts (i.e., fine-tuned on the PLUE benchmarks). \n108 https:// nlp. jhu. edu/ law.\n109 https:// github. com/ reglab/ caseh old.\n945\n1 3\nTransformer-based language models for AI and law\nMoreover, the pre-trained models show comparable results w.r.t. a task-adapted \nLegal-BERT@aueb.\nPaul et al. (2022b) further pre-train Legal-BERT@aueb and Legal-BERT@stan-\nford on a corpus of about 5.4M Indian court cases. The resulting models, called \nInLegalBERT (derived from Legal-BERT@aueb) and InCaseLawBERT (derived \nfrom Legal-BERT@stanford), are evaluated on three downstream tasks: legal stat-\nute retrieval, rhetorical role labeling, and court judgment prediction. To this regard, \nthe two proposed models have been fine-tuned and assessed on various benchmarks \nagainst BERT, the original Legal-BERT@aueb and Legal-BERT@stanford and the \nbest performers according to the benchmark’s paper. In particular, for the legal stat-\nute retrieval task, Paul et al. (2022b) consider the ECtHR Task B of LexGLUE and \na dataset of criminal case documents provided in (Paul et  al. 2022a). A hierarchi-\ncal model—which consists of a lower-level encoder to provide the [CLS] represen-\ntation of each sentence in the input text, followed by an higher-level encoder that \naggregates the sentence embeddings to get the document representation—is used to \nget the relevant statutes, in a multi-label classification manner. Five versions of this \nmodel are defined, choosing the lower-level encoder among BERT, Legal-BERT@\naueb, Legal-BERT@stanford, InLegalBERT and InCaseLawBERT, whereas the \nhigh-level encoder is based on a LSTM with the attention mechanism. For the rhe-\ntorical role labeling task, the models are evaluated on a dataset of 50 documents from \nthe Supreme Court of India (Bhattacharya et  al. 2019b) and a dataset of as many \ndocuments from the Supreme Court of UK (Bhattacharya et al. 2021); a hierarchical \nmodel is again used, with the same variations of the lower-level encoder as in the pre-\nvious task. For court judgment prediction, the authors consider the ILDCmulti dataset \nand the hierarchical BERT model provided by Malik et al. (2021), again with varia-\ntions as above. Results show that both InLegalBERT and InCaseLawBERT improve \nthe performances against LegalBERT@aueb and LegalBERT@stanford, respectively. \nMoreover, both InLegalBERT and InCaseLawBERT outperform the best performer \nof (Paul et al. 2022a) and (Malik et al. 2021), and InLegalBERT is better than the \nbest performer of (Bhattacharya et al. 2021), ECtHR Task B (Chalkidis et al. 2022b). \nOverall, results suggest that further pre-training on Indian legal data can improve the \nperformance of a model in several legal tasks across domains and countries.\nFurther applications of legal pre-trained models. The above discussed legal pre-\ntrained models have recently attracted attention, especially Legal-BERT@aueb \nmodels which have started to be widely used.\nIn (Mahari 2021), Legal-BERT-SC@aueb is fine-tuned for the legal precedent \nprediction (LPP) task, i.e., to predict relevant passages from precedential court deci-\nsions given the local context surrounding the passage in the opinion citing it (cf. \nSect. 3.3). Using the US federal judicial opinions from the Case Law Access Project \n(CAP), the neighborhood of a passage is here comprised of 300 characters before \nthe passage, 300 characters after the passage, 300 characters from the Introduction \nand 300 characters from the Conclusions. In 96% of unseen test examples, the cor -\nrect passage is found to be within the top-10 predicted passages.\nOstendorff et  al. (2021) compare Legal-BERT@aueb and Legal-BERT@\njhu—along with BERT (base and large), RoBERTa, SBERT and SRoBERTa, \n946 C. M. Greco, A. Tagarelli \n1 3\nLongformer—against word-vector- and citation-based approaches on legal docu-\nment recommendation as a text similarity task. 110 A legal document is represented \nas a numerical vector, and the top-5 documents with the highest cosine similarity are \nselected through nearest neighbor search. Results have shown no absolute winner \nacross all datasets and criteria, although a legal-domain adaptation of the FastText \nword embedding model (Bojanowski et al. 2017; Joulin et al. 2017) was found the \nbest performing method on average.\nBhattacharya et al. (2021) address the problem of rhetorical role labeling and pro-\npose different deep learning models to identify seven rhetorical classes over docu-\nments from five different law domains and from two different jurisdictions. BERT \nand LegalBERT@aueb are fine-tuned on this task and used as bottom layers of a \nBiLSTM-CRF-based architecture. More specifically, the sentence embeddings pro-\nduced by BERT and LegalBERT are used with a Hierarchical BiLSTM classifier \nand a Hierarchical BiLSTM CRF classifier with or without attention mechanism. \nResults show different outcomes in the model comparison, and in particular BERT \nand LegalBERT embeddings reveal to be beneficial for a Supreme Court of the UK \ndataset, but not for a Supreme Court of India dataset, compared to Sent2Vec (Pagli-\nardini et al. 2018) embeddings. (Sent2Vec is a simple unsupervised model that con-\nsiders sentence embeddings as the average of source word embeddings, considering \nboth unigrams and n-grams).\nChalkidis et  al. (2021b) address the task of regulatory information retrieval, \nwhere the query is a document (e.g, an EU directive) and the goal is to retrieve a \nset of relevant documents from the available law corpora. Upon two new datasets, \nEU2UK and UK2EU, a two-stage approach is defined, where document re-ranking \nis carried out after pre-fetching, i.e., retrieval of the top-k prominent documents for \na query. As retrieval models, BERT, SBERT and LegalBERT@aueb are applied but \nshowed to perform worse or comparably to BM25. As an improvement, following \nChalkidis et  al. (2019b), BERT is fine-tuned to predict EUROVOC concepts that \ndescribe the core subjects of each text: the resulting model, called C-BERT, turns \nout to be the best pre-fetcher by a large margin in EU2UK, while being compara-\nble to BM25 in UK2EU; also, an ensemble combining C-BERT with BM25 further \nimproves the results. For document re-ranking, several neural re-ranking methods \nare selected to yield a relevance score for each of the top-k documents returned by \nthe best pre-fetcher: DRMM (Guo et al. 2016), PACRR (Hui et al. 2017), BERT-\nbased re-rankers. DRMM is a deep matching model focused on relevance match-\ning in ad-hoc retrieval. PACRR is a IR model that focuses on preserving positional \ninformation by incorporating domain insights. Chalkidis et al. (2021b) use DRMM \nand PACRR on top of BERT embeddings, and the resulting methods are called \nC-BERT-DRMM and C-BERT-PACRR. A major finding is that the re-rankers fail \nto improve the performance w.r.t. the ensemble pre-fetcher, as their term matching \nmechanisms tend to degenerate and over-utilize the pre-fetcher score.\nFor the statute law retrieval task of COLIEE 2021, Wehnert et al. (2021) fine-tune \nan ensemble model of Legal-BERT@aueb (legal-bert-base-uncased ) and \n110 https:// github. com/ malte os/ legal- docum ent- simil arity.\n947\n1 3\nTransformer-based language models for AI and law\nTF-IDF on sentence classification, to predict the relevance of an article to a query. \nTraining and validation data are handled in three settings: in the original form (i.e., \npairs of queries and relevant articles), decomposing instances with multiple relevant \narticles, and performing a data augmentation strategy to reduce data imbalance. \nIn particular, the decomposed pairs are obtained separating the multiple relevant \narticles in different instances, producing a query-article pair for each relevant arti-\ncle. Data augmentation is performed by retrieving the top-50 non-relevant articles \naccording to the highest cosine similarity between TF-IDF vectors of relevant arti-\ncles and all the non-relevant articles, analogously to the approach in (Nguyen et al. \n2020) to reduce the unbalance between positive and negative samples. The resulting \ndataset is fed to Legal-BERT@aueb and TF-IDF, to obtain a softmax score of rele-\nvance and cosine similarity score, respectively. The overall score is computed as the \naverage of the two scores after normalization. The top-n relevant articles are filtered \nusing a threshold based on the precision-recall trade-off in the validation set.\nThe same team also applied for the Task 4 of COLIEE 2021 (Goebel et al. 2021), \nproposing two approaches: SBERT combined with a Graph Neural Network (GNN) \nmodel and SBERT combined with LegalBERT@aueb. In the first approach, GNN \nis expected to model the entailment relationships between nodes, where nodes rep-\nresent articles/queries. The result is a bi-partite graph, in which queries can have \nconnections to multiple relevant articles, and articles are enriched with metadata \nfrom the section titles. From the content of articles and queries, contextual sentence \nembeddings are generated using SBERT and used as node features encoding exter -\nnal knowledge. Such node features are used to get query node embeddings follow -\ning Morris et  al. (2019), which encodes information taking into account also rel-\nevant articles as direct neighbor nodes. The query node embeddings are then used \nfor query node classification to address the entailment task. In the second approach, \nthe team adapt the LegalBERT@aueb encoder they proposed for Task 3 (Wehnert \net  al. 2021), re-initializing the classification head with new trainable parameters \nand splitting training data instances having multiple articles for a query in order to \nobtain additional instances. From the competition results, LegalBERT@aueb sub-\nmissions outperform the GNN-based approach; according to the authors, it is likely \nthat the GNN was unable to generalize as well as LegalBERT@aueb due to the lim-\nited amounts of data and the lack of document enrichment to encode external knowl-\nedge. Wehnert et  al. (2022) further propose a new method for the COLIEE-2021 \nTask 4, which combines BERT and KERMIT (Zanzotto et  al. 2020), an encoder \nthat provides embeddings of syntactic parse trees. The idea is to inject further lin-\nguistic knowledge from the syntactic parse trees of query and articles. The output \nis the concatenation of the two embedding vectors. The trees for query and arti-\ncles are obtained using the parser from the Stanford CoreNLP, as in (Zanzotto et al. \n2020). The concatenation of query and article is given to bert-base-uncased  \nto get the sequence representation through the [CLS] resulting embedding. Both the \nembeddings of KERMIT and BERT are then passed to a fully connected decoder \nlayer to get the entailment label. This third approach to Task 4 reaches higher perfor-\nmance than the GNN-based run and comparable results with the best LegalBERT@\naueb-based run.\n948 C. M. Greco, A. Tagarelli \n1 3\nWang et  al. (2023) address the multiple-choice reading comprehension task of \nMAUD by fine-tuning several TLMs, in particular BERT, RoBERTa, LegalBERT@\naueb, DeBERTaV3 and BigBird for a single-task setting, and RoBERTa, Legal-\nBERT@aueb and DeBERTaV3 for a multi-task setting, which requires to fine-tune a \nmodel for all questions of MAUD. Newer and larger models provided better results, \nwith BigBird and LegalBERT@aueb being the best performers in the single-task \nand multi-task setting, respectively.\nShukla et al. (2022) evaluate different extractive and abstractive summarization \nmethods on their own collected datasets (cf. Sect.  3.4): domain-agnostic methods, \nsuch as SummaRunner (Nallapati et al. 2017) and BERTSUM for extractive sum-\nmarization, BART for abstractive summarization; domain-adapted methods, such as \nCaseSummarizer (Polsley et al. 2016) and Gist (Liu and Chen 2019) for extractive \nsummarization, Legal-PEGASUS111 for short abstractive documents, Legal-LED 112 \nfor long abstractive documents; task-adapted methods, in which Transformers \nare fine-tuned on data generated with sentence similarity techniques. In addition, \na newly hybrid extractive-abstractive method, called BERT_BART , is introduced \nand evaluated in both domain-adapted and task-adapted versions. As mentioned in \nSect. 3.4, methods are to be evaluated w.r.t. document-level summaries, how well the \nsummary represents the logical rhetorical segments in the legal case document, i.e., \nsegment-wise evaluation, and how the summaries are evaluated by domain-experts. \nAs regards the document-level evaluation, on the IN-Ext dataset, BERTSUM turns \nout to be the best extractive method, while the task-adapted Legal-PEGASUS and \nBERT-BART obtain the highest ROUGE and BERTScore values, respectively; \non the IN-Abs dataset, the task-adapted Legal-LED, BART and Legal-PEGASUS \nobtain the best BERTScore, R-1 ROUGE and R-L ROUGE scores, respectively; on \nthe UK-Abs dataset, the task-adapted BART is the best performer in terms of R-L \nROUGE and BERTScore. As regards the segment-wise evaluation for the IN-Ext \nand UK-Abs datasets, results show a mixed picture, with Transformers obtaining the \nbest score in most metrics; however, a discrepancy is noticed between the metrics \nused for summarization and the expert judgments: in fact, according to the involved \nlaw experts, extractive summaries are more satisfactory than the abstractive coun-\nterparts, since abstractive summaries are considered to be less organized and often \nincomplete.\n4.4  Dealing with long legal documents\nAs discussed in Sect. 2, BERT and its early derivations can process texts having 512 \ntokens as maximum length. Input truncation to the maximum length has often been \nperformed, which also holds for the methods discussed in the previous sections. \nHowever, this choice is clearly not optimal, especially when important information \nare spread through the whole document in different locations, which is also the case \n111 https:// huggi ngface. co/ nsi319/ legal- pegas us.\n112 https:// huggi ngface. co/ nsi319/ legal- led- base- 16384.\n949\n1 3\nTransformer-based language models for AI and law\nTable 5  Summary of main methods discussed in Sects. 4.3 and 4.4\nMethod Ref. Downstream tasks Lang. Data Long docs?\nPre-trained BERT w/legal or hybrid \ntokens; Pre-trained DistilBERT \nw/ legal or hybrid tokens\n Shaghaghian et al. (2020) PR, NER, TM, SA EN US SEC legal agreements No\nFrom-scratch pre-trained BERT \n(BERTLaw)\n Nguyen and Nguyen (2021) COLIEE-2020 Task 4 EN Civil code No\nFurther pre-trained BERT (Legal-\nBERT-FP@aueb); From-scratch \npre-trained BERT (Legal-BERT-\nSC@aueb)\n Chalkidis et al. (2020b) TC, NER EN EURLEX57K; ECHR cases; \nCONTRACTS-NER\nNo\nFurther pre-trained BERT (Legal-\nBERT@jhu)\n Holzenberger et al. (2020) CTR EN CAP cases No\nFurther pre-trained BERT (Legal-\nBERT-FP@stanford); From-\nscratch pre-trained BERT (Legal-\nBERT-SC@stanford)\n Zheng et al. (2021) QA, OR, ToS DS EN CaseHOLD; Overruling; Terms-of-\nService\nNo\nFine-tuned Legal-BERT@aueb  Mahari (2021) LPP EN CAP judicial opinions No\nFurther pre-trained AlBERT \n(ALeaseBERT)\n Leivaditi et al. (2020) Red flag detection, entity extraction EN Contracts No\nFurther pre-trained BERT, Span-\nBERT and ELECTRA \n Chi et al. (2023) Intent classification, SF, QA, NER EN Contracts No\nFurther pre-trained Legal-BERT@\naueb (InLegalBERT); Further \npre-trained Legal-BERT@stan-\nford (InCaseLawBERT)\n Paul et al. (2022b) SAR, RRL, CJP EN Legal cases Yes\nFine-tuned BERT, Legal-BERT@\naueb, Legal-BERT@jhu, \nRoBERTa, SBERT, SRoBERTa, \nLongformer\n Ostendorff et al. (2021) DR EN Open Case Book; Wikisource US \nCourts\nYes\n950 C. M. Greco, A. Tagarelli \n1 3\nTable 5  (continued)\nMethod Ref. Downstream tasks Lang. Data Long docs?\nFine-tuned BERT and LegalBERT@\naueb + BiLSTM and CRF\n Bhattacharya et al. (2021) RRL EN Legal cases No\nFine-tuned BERT, SBERT and \nLegalBERT@aueb\n Chalkidis et al. (2021b) RIR EN EU2UK; UK2EU No\nFine-tuned BERT (C-BERT)  Chalkidis et al. (2021b) EUROVOC concept classification EN EU2UK; UK2EU No\nFine-tuned BERT + KERMIT  Wehnert et al. (2022) COLIEE-2021 Task 4 EN Civil code No\nFine-tuned BERT, RoBERTa, \nLegalBERT@aueb, DeBERTaV3, \nBigBird\n Wang et al. (2023) MCRC EN Contracts Yes\nFine-tuned Legal-PEGASUS, \nLegal-LED, BART, BERT-\nBART; BERTSUM\n Shukla et al. (2022) ES, AS EN Legal cases Yes\nFine-tuned BERT w/summarized \ntexts\n Rossi and Kanoulas (2019) COLIEE-2019 Task 1 EN Legal cases Yes\nTwo-stage fine-tuned BERT w/\nattentive aggregation (HIER-\nBERT)\n Chalkidis et al. (2019a) AV, CIR EN ECHR cases Yes\nTwo-stage fine-tuned BERT w/\nattentive aggregation (BERT-PLI)\n Shao et al. (2020c) COLIEE-2019 Tasks 1, 2 EN Legal cases Yes\nLMIR + fine-tuned BERT-PLI  Shao et al. (2020b), Ma et al. \n(2021)\nCOLIEE-2020 Task 1 EN Legal cases Yes\nFine-tuned BERT, ALBERT, RoB-\nERTa, DeBERTa\n Hendrycks et al. (2021) QA EN CUAD legal contracts Yes\nFine-tuned RoBERTa and SRoB-\nERTa\n Aumiller et al. (2021) STP EN ToS documents Yes\nTransformer summarizer + fine-\ntuned BERT\n Kim et al. (2021) COLIEE-2021 Task 2 EN Legal cases Yes\n951\n1 3\nTransformer-based language models for AI and law\nTable 5  (continued)\nMethod Ref. Downstream tasks Lang. Data Long docs?\nFine-tuned BERT and RoBERTa  Nguyen et al. (2021a) COLIEE-2021 Task 3 JP Civil code Yes\nHier-BERT variation based on \nLegalBERT@aueb\n Chalkidis et al. (2021c) Rationale extraction EN ECtHR Yes\nPre-trained DPR (LawDPR); \nFine-tuned Longformer–Encoder–\nDecoder\n Althammer et al. (2021) COLIEE-2021 Tasks 1, 2 EN Canada case laws Yes\nFine-tuned BERT (BERTSum)  Deroy et al. (2021) ES EN Legal cases Yes\nFine-tuned BERT, RoBERTa, \nDistilBERT\n Klaus et al. (2022) ES EN EUR-LexSum Yes\nFurther pre-trained BERT and \nRoBERTa\n Lam et al. (2020) Employment notice prediction EN Notice cases Yes\nFine-tuned BERT, Legal-BERT@\naueb and Legal-BERT@stanford; \nFine-tuned BigBird and Long-\nFormer\n Limsopatham (2021) AV, OR EN ECHR Violation dataset; Overrul-\ning Task dataset\nYes\nFrom-scratch pre-trained BERT \n+ Siamese Legal BERT + Fine-\ntuned Legal BERT\n Khazaeli et al. (2021) PR, QA EN caselaw headnotes, RFCs No\nFurther pre-trained BERT (BERT-\nLegal) + LSTM\n Li et al. (2021b) COLIEE-2021 Task 1 EN Legal cases Yes\nFine-tuned BERT-Legal w/augmen-\ntation; Fine-tuned BERT-Legal w/\nfast gradient\n Li et al. (2021b) COLIEE-2021 Task 2 EN Legal cases Yes\nFine-tuned Legal-BERT@aueb + \nTF-IDF vectors\n Furniturewala et al. (2021) AILA-2021 Tasks 2a-b EN Legal cases Yes\nFine-tuned Legal-BERT@aueb  Jain et al. (2021) AILA-2021 Tasks 2a-b EN Legal cases Yes\n952 C. M. Greco, A. Tagarelli \n1 3\nTable 5  (continued)\nMethod Ref. Downstream tasks Lang. Data Long docs?\nEnsemble of BM25 w/KLI sum-\nmarizer and Fine-tuned BERT w/\nLED summarizer\n Askari and Verberne (2021) COLIEE-2020 Task 1 EN Legal cases Yes\nFine-tuned BERT (Span NLI \nBERT)\n Koreeda and Manning (2021) NLI, span detection EN Contracts Yes\nFine-tuned BART, PEGASUS, \nLED and PRIMERA\n Shen et al. (2022) AS EN Legal cases Yes\nBERT + Bi-LSTM + CRF \n(SciBERT-HSLN); Fine-tuned \nBERTSUM + rhetorical roles \n(BERTSUM RR) Legal-\nPEGASUS + rhetorical roles \n(Legal-PEGASUS RR) XLNet + \nrhetorical roles (BERTSUM RR)\n Kalamkar et al. (2022) RRL, ES, AS, LJP EN Legal cases Yes\nBERT, RoBERTa, DeBERTa, \nLongformer, BigBird, Legal-\nBERT@aueb and Legal-BERT@\nstanford\n Chalkidis et al. (2022b) AVP, ALVP, TpC, TC, QA EN Legal cases, contracts Yes\nFine-tuned Longformer (Long-\nformer-8192 & variants), \nFine-tuned LegalBERT@aueb + \nLongformer (LegalLongformer \n& variants) Fine-tuned Legal-\nBERT@aueb+TFIDF (TFIDF-\nSRT-LegalBERT)\n Mamakas et al. (2022) AVP, ALVP, TpC, TC, QA EN Legal cases, contracts Yes\nFine-tuned Hierarchical Attention \nTransformer (HAT)\n Chalkidis et al. (2022a) ALVP, TC, NLI EN Legal cases, contracts Yes\nFine-tuned BERT, DistilBERT, \nRoBERTa and XLNet\n Malik et al. (2021) CJPE EN Legal cases Yes\n953\n1 3\nTransformer-based language models for AI and law\nfor legal documents. In this regard, text segmentation and text summarization can \ncertainly be helpful.\nIn this section, we discuss methods that adopt different strategies for dealing with \nlong documents. We organize this presentation by distinguishing between task-adap-\ntive fine-tuning methods and domain-adaptive pre-training methods.\n4.4.1  Task‑adaptive methods\nRossi and Kanoulas (2019) address the COLIEE-2019 Task 1 as a ranking prob-\nlem formulated as a pairwise relevance classification task, given the binary labels \n“Noticed\" or “Not noticed\". This is treated as a downstream task for the pre-trained \nbert-base-uncased, where the hidden state of the [CLS] token is used as the \npairwise embedding for a pair of summarized query case and candidate case, and \nthe TextRank method (Barrios et al. 2016) is used for the summarization steps. The \nmodel has shown good generalization ability, although the widespread distribution \nof scores for the positive class and the small proportion of pairs with high score for \nthe negative class might affect negatively the performance of the system.\nChalkidis et al. (2019a) face the length issue in legal cases proposing Hier-BERT. \nBERT is used to produce fact embeddings, then a self-attention mechanism applies \nto the fact embeddings and generates a single case embedding that is fed to a task-\nspecific output layer. Three tasks are evaluated: binary violation classification, \nmulti-label violation classification, and case importance prediction. The model is \ntested on ECHR (cf. Sect.  3.4). The list of facts in the fact description of each case \nare extracted using regular expressions. The violated articles are mapped to the vio-\nlating case. An importance score is assigned for each case by the European Con-\nvention of Human Rights. The authors demonstrate that Hier-BERT significantly \nimproves BERT in the aforementioned tasks.\nBERT-PLI (Shao et al. 2020c) utilizes BERT to model paragraph-level interac-\ntions for legal case retrieval tasks, using a cascade framework. 113 Initially, BM25 is \nused to filter the set of candidates and BERT is fine-tuned for sentence-pair binary \nclassification on the COLIEE-2019 Task 2 data. The fine-tuned parameters are kept \nfor the subsequent stage whose goal is to train the BERT-PLI model for relevance \nprediction. BERT-PLI receives a query and a selected document each broken into \nparagraphs. Each pair of query paragraph and document paragraph is an input to \nthe fine-tuned BERT. The final hidden state vector of the [CLS] token is considered \nas the aggregate representation of the input pair. For each query and document, an \ninteraction map of paragraph is finally produced. Then, for each query paragraph, a \nmax-pooling strategy is used to capture the strongest signal with a candidate docu-\nment. The ordered sequence of vectors resulting from the max-pooling on all para-\ngraphs of the query is encoded by a RNN. An attention mechanism is applied to infer \nthe importance of each position, and a document-level representation of the query-\ndocument pair is obtained via attentive aggregation. The resulting representation is \n113 https:// github. com/ ThuYS hao/ BERT- PLI- IJCAI 2020.\n954 C. M. Greco, A. Tagarelli \n1 3\nprovided to a fully connected layer with softmax to make a prediction of binary rel-\nevance. BERT-PLI has shown to achieve better recall and F1-score than the top-2 \nteams at COLIEE-2019.\nTo address the binary classification COLIEE-2020 Task 1, Shao et  al. (2020b) \napply a cascade framework in which a bi-gram LMIR (Song and Croft 1999) is used \nto select top-N candidates to be further classified by BERT-PLI. The proposed sys-\ntem focuses mainly on the semantic understanding of documents, so that an addi-\ntional solution is proposed to combine the semantic aspect of BERT-PLI with the \nexact matching among documents applied by a word-entity duet framework (Xiong \net  al. 2017). In particular, several features are extracted from BERT-PLI, BERT \nfine-tuned on sentence-pair classification using the first paragraph of query and \ncandidate case, word-entity duet and similarity scores based on words and enti-\nties. A RankSVM model is then used to re-rank the top-N candidates, based on \nthe aforementioned features. The resulting system performs better than the single \nBERT-PLI based solution, suggesting that the union of semantic understanding and \nexact matching can be beneficial. Analogously, in COLIEE 2021, Ma et al. (2021) \napply BERT-PLI in Task 1 with some revisions. First, a LMIR model (Ponte and \nCroft 1998) is used to filter candidates, then BERT is fine-tuned for a next sentence \nprediction task on the COLIEE-2019 Task 2. While Shao et  al. (2020c) compute \ninteractions with all query paragraphs and all top-N candidate paragraphs, Ma et al. \n(2021) consider only query paragraphs with sentences with a citation token, i.e., \nsentences with placeholders (for example, ‘REFERENCE_SUPPRESSED’) that \nindicate the presence of mentions to other noticed cases. They also limit the num-\nber of paragraphs for a document. The proposed model outperformed the BERT-PLI \nbased solutions and reached the first position.\nHendrycks et al. (2021) fine-tune and evaluate BERT, RoBERTa, AlBERT and \nDeBERTa on the Contract Understanding Atticus Dataset (CUAD, see Sect. 3.4) for \nlegal contract review, which is composed of contracts annotated with 41 category \nlabels by legal experts. To deal with long documents, a sliding window mechanism \nis used over each document. DeBERTa turns out to perform the best over the other \nmodels, however the overall performances show that this is a challenging task, \nwhere the size of a model and the amount of training data play a very important role.\nAumiller et  al. (2021) propose an approach to document segmentation of legal \ntext to predict topical coherence among paragraphs and to detect topical change (cf. \nSect.  3.2).114 The approach consists in fine-tuning RoBERTa and Sentence-RoB-\nERTa on a topic similarity task, which is formulated as a binary classification prob-\nlem, i.e., to detect if paragraphs belong to similar topics or not. To this purpose, a \ndataset of Terms-of-Service documents, dubbed ToS (cf. Sect. 3.4), is created which \nincludes hierarchical topic annotations. The classification task is performed under \nthe assumption of topical independence of the context, i.e., the topic probability \nof a paragraph is not affected by the belonging of context paragraphs to the same \ntopic. The fine-tuned models are then evaluated for sequential inference on entire \n114 https:// github. com/ dennl inger/ Topic alCha nge.\n955\n1 3\nTransformer-based language models for AI and law\ndocuments, where segments are delimited by topical change. The approach has \nshown to outperform segmentation baselines based on TF-IDF and bag-of-words \nmodels.\nFor the COLIEE-2021 Task 2, Kim et  al. (2021) fine-tune BERT on a binary \nclassification problem using pairs of query and candidate paragraphs from the rela-\ntive COLIEE dataset. To address the BERT token limit, a Transformer-based model \ngenerates a summary of the input. A subsequent phase of post-processing limits \nthe maximum number of outputs for a given case. The final system is submitted \nto the competition in three runs, which differed in the setting of post-processing \nparameters.\nNguyen et al. (2021a) split the articles in multiple chunks using sliding windows \nto address the text length limitation of BERT and RoBERTa for the COLIEE-2021 \nTask 3. This is motivated from the observation that only some parts of an article \nentail the corresponding query. Training data are built as pairs of query and entail-\ning article (positive sample) or any other article (negative sample), limiting the \nmaximum number of negative samples. To avoid noisy training examples, a self-\nlabeling technique is exploited, consisting of multiple fine-tuning phases in which \ntraining data labels are modified according to specific rules (Triguero et al. 2015). \nDifferent models, such as bert-base-japanese , bert-base-japa-\nnese-whole-word-masking, and xlm-roberta-base ,115 are trained on \na sentence-pair classification task using the COLIEE-2020 dataset. The ensemble \nof those models performed better than the individual ones, reaching the second \nposition in the competition. The same approach is also adapted for Task 4, using \nbert-base-japanese-whole-word-masking.\nFor the Japanese version of COLIEE-2021 Task 3, Yoshioka et  al. (2021b) \nimprove their previous proposal, introduced in COLIEE 2020, based on the ensem-\nble of a BERT-based retrieval system (Sakata et al. 2019) and the keyword-based \nIndri retrieval system. 116 A new article database is built combining articles and \nreferred articles. To deal with long article sentences, the articles composed of a list \nof conditions or multiple judicial decisions are divided into small sequences through \nthe numbered paragraph structure. Oversampling and negative sampling strategies \nare also applied to the training examples. The BERT-based and Indri scores for each \narticle are combined to get an overall score. Three versions of the system are defined \nusing different ensemble settings: the first version consists of BERT with oversam-\npling, BERT with negative sampling and Indri, the second version uses BERT with \noversampling only, and the third version corresponds to BERT with oversampling \nand Indri.\nChalkidis et  al. (2021c) propose a method to learn automatically extracting \nparagraphs (i.e., the rationales) of the input so to justify decisions in alleged viola-\ntion prediction. To this end, the ECtHR dataset (cf. Sect.  3.4) is used as it includes \nannotations for paragraph-level rationales. Extraction of the rationales is driven by \nrationale constraints. The proposed method is a variation of Hier-BERT (Chalkidis \n115 https:// huggi ngface. co/ xlm- rober ta- base.\n116 https:// www. lemur proje ct. org/ indri/.\n956 C. M. Greco, A. Tagarelli \n1 3\net al. 2019a), dubbed HierBERT-HA, following the framework of Lei et al. (2016) \nfor the construction of rationales and consisting in three sub-networks: the first reads \nthe text, the second extracts rationales through hard masking mechanism, and the \nthird classifies the hard-masked text. More specifically, the paragraphs are first rep-\nresented by the [CLS] embeddings obtained by LegalBERT@aueb (Chalkidis et al. \n2020b). Then, a two-layer Transformer obtains a contextualized representation of \nthe paragraph embeddings which are fed to two separated fully-connected layers, the \none producing paragraph encodings for the classifier sub-network, and the other one \nproducing paragraph encodings for the rationale extraction sub-network. The ration-\nale sub-network obtains a binarized attention score for each paragraph. Each para-\ngraph is then hard-masked using the corresponding attention score. The resulting \nparagraphs are then fed to the classifier sub-network to predict the alleged violated \narticles. Different versions of the model using rationale constraints as regularizers \n(such as sparsity, comprehensiveness, singularity) are compared. A HierBERT-HA \nversion without hard masking, dubbed HierRBER-ALL, and rationale constraints is \ntested too. Results show that models with hard attention and rationale constraints \nperform well and comparably to HierBER-ALL.\nAlthammer et al. (2021) propose a two-stage pipeline for case law retrieval fol-\nlowed by re-ranking. 117 For the first stage, each query case and the cases in the \ncorpus are split into their paragraphs, so that relevant prior cases can be retrieved \nfor each paragraph of the query case based on the relevance of their paragraphs. \nBoth lexical and semantic retrieval models are exploited, where the first is based on \nBM25 and the second corresponds to DPR based on two BERT-base-uncased Sia-\nmese Encoders (i.e., the one for encoding the query passage and the other one for the \ncandidate passage). The relevance score between a query and a candidate passage is \ncomputed as the dot-product between their encoded vectors. The resulting model, \ncalled LawDPR, is trained on the entailing query-paragraph pairs of COLIEE-2021 \nTask 2. Results have shown that the paragraph-level retrieval is beneficial in terms \nof recall, and that LawDPR outperforms BM25, although a combination of both \nmodels can lead to further improved performance. In the stage of re-ranking of the \nretrieved cases, the approach is to summarize the texts of cases and queries, and \nthen to predict whether the summarized query case is relevant to a summarized case. \nTo this purpose, the LED model is fine-tuned on the corpus for the corresponding \nbinary classification task.\nDeroy et  al. (2021) analyze expert-generated summaries and model-generated \nsummaries of legal case documents, to understand which parts of the documents are \nselected from models and domain-experts, and which sentences marked as impor -\ntant by domain-experts are easier or harder to identify for models. To this purpose, \n15 extractive summarization models are evaluated over 50 case documents from the \nIndian Supreme Court. One of these models is BERTSum (Liu and Lapata 2019b), \nwhich is a BERT model fine-tuned for extractive summarization. Results show that \nBERTSum tends to select the initial portions of the document, even after fine-tun-\ning. This is probably due to the pre-training of BERTSum, which is performed on \n117 https:// github. com/ sophi aalth ammer/ dossi er_ coliee.\n957\n1 3\nTransformer-based language models for AI and law\nnews article corpus in which the first sentences are usually indicative of the content \nof the entire document.\nKlaus et al. (2022) focus on extractive summarization of the European regulatory \ndocuments, in order to allow non-jurists (e.g., companies that want to ensure com-\npliance with current regulations) a more facilitated comprehension of the documents \nand decide which regulations need more follow-up. They create the EUR-LexSum \ncorpus,118 consisting of 4K curated European regulatory documents with their sum-\nmaries and structured into 32 policy fields (e.g., Taxation, Public Health and so on). \nSuch documents are gathered from the Web,119 which contains also the correspond-\ning summaries in an abstractive form. To this regard, the authors perform a greedy \nstrategy to obtain the extractive version of the summaries, before fine-tuning and \nfeeding the data to a classifier. The obtained dataset is then split into 75% train-\ning, 15% validation, and 10% test documents. They fine-tune base uncased BERT, \nbase uncased DistilBERT, and base RoBERTa (gathered from TransformerSum, 120 \na library based on BERTSum (Liu and Lapata 2019b)), and address the summariza-\ntion task as a binary classification of the sentence representations of the documents. \nResults prove that TLMs achieve superior performance compared to the TextRank \n(Mihalcea and Tarau 2004) baseline, with BERT and DistilBERT performing better \nthan RoBERTa. Furthermore, a combination of TextRank and TLMs is also eval-\nuated, using the former as a pre-filter of candidate sentences, which achieves the \nhighest F1 and precision scores in terms of Rouge-1 metrics (Lin 2004).\nKoreeda and Manning (2021) introduce Span NLI BERT, a BERT-based model \nthat performs document-level NLI for non-disclosure agreements contracts. As \nexplained in Sect. 3.2, the task is to find the implication relation (NLI) between a set \nof hypotheses and a contract (“entails”, “contradicts”, or “is neutral”), as well as to \nidentify the spans in the contract which provide evidence for the associated relation. \nTo deal with long documents, a dynamic context segmentation process is involved to \nsplit the text into overlapping contexts, assigning a pre-defined number of tokens to \neach context and marking the span boundaries within the context. The span bounda-\nries are identified by special tokens [SPAN], whereas the input consists of a [CLS] \ntoken followed by the hypothesis and the context separated by [SEP]. The task is \nthus reduced to a multi-label binary classification over the [SPAN] tokens for the \nevidence identification, and a classification task over the [CLS] token for NLI. Span-\nNLI-BERT was fine-tuned and tested on the ContractNLI dataset (cf. Sect.  3.4) \nagainst several baselines, based on SVM, TF-IDF and BERT. Span-NLI-BERT was \nalso tested with different backbone models, including BERT without fine-tuning, \nBERT pre-trained from scratch with a case law corpus (Zheng et al. 2021), BERT \nfine-tuned on case law and contract corpora (Chalkidis et al. 2020b), DeBERTaV2 \nwithout fine-tuning and DeBERTa V2 fine-tuned on span identification (Hendry -\ncks et al. 2021); the latter turned out to be the best performing model. Overall, it \nwas found that the correct identification of evidence greatly improves the NLI task, \n118 https:// github. com/ svea- klaus/ Legal- Docum ent- Summa rizat ion.\n119 https:// eur- lex. europa. eu/ browse/ summa ries. html.\n120 https:// trans forme rsum. readt hedocs. io/ en/ latest/.\n958 C. M. Greco, A. Tagarelli \n1 3\nthe performance of the model degrades in presence of rare labels, the negation by \nexception in the text damages the model’s accuracy for NLI, while the presence of \nreferences to definitions does not hurt performance.\nUsing the ILDC benchmark, Malik et  al. (2021) compare BERT, DistilBERT, \nRoBERTa and XLNet models, including their hierarchical versions, against non-\nTLMs. An input text is divided into overlapping chunks of 512 tokens, and the \n[CLS] representation of each chunk produced by a TLM is passed to a sequential \nor feed-forward model to get the final case-decision prediction. The best performing \nmodel turns out to be the hierarchical model based on XLNet and a BiGRU on top. \nMalik et al. (2021) also consider a number of explainability approaches as a post-\nprediction step (cf. Sect. 4.8).\nInspired by SciBERT-HSLN (Brack et  al. 2021), Kalamkar et  al. (2022) pro-\npose a method based on BERT word embeddings along with Bi-LSTM and CRF, \nfor the automatic prediction of rhetorical roles. Results show that SciBERT-HSLN \noutperforms a CRF model, which uses BERT sentence embeddings, and the model \nproposed in (Cohan et  al. 2019), which only uses BERT. Kalamkar et  al. (2022) \nalso experimented how rhetorical role prediction can be helpful in two applications: \nextractive and abstractive summarization of court judgments and court judgment \nprediction. For extractive summarization, BERTSUM is fine-tuned on the LAW -\nBriefs corpus, chunking the input in case of exceeding 512 tokens and pairing each \ninput sentence with its rhetorical role. The resulting model, called BERTSUM RR, is \ncompared against BERTSUM fine-tuned on data without information on the rhetori-\ncal roles. Similarly, for the abstractive summarization, Legal-PEGASUS is used by \nsplitting the input in chunks (each of 1024 tokens) in one setting, and segmenting \nthe document in terms of rhetorical roles in another setting, which is called Legal-\nPEGASUS RR). Results demonstrate that the use of rhetorical roles improves perfor-\nmance on both extractive and abstractive summarization. As regards the court judg-\nment prediction task, XLNet is used on the model proposed in (Malik et al. 2021), \nwhereby a version of the model is trained on the last 512 tokens of the judgment text \nof the ILDC corpus, and another version is trained on the same data but filtered on \nthe last 512 tokens sentences associated to the “analysis” role, which turns out to \nimprove the predictions.\nIn (Chalkidis et al. 2022b) hierarchical versions of several TLMs are evaluated \non the LexGLUE data. These methods are particularly motivated by the inclusion of \nthree datasets in LexGLUE, i.e., ECtHR (A and B), SCOTUS and Multi-EURLEX, \nwhose texts substantially exceed the limit of 512 tokens. Even models that handle \nlonger text sequences, such as Longformer, are not able to avoid the truncation of \ntexts in these datasets. Following Chalkidis et al. (2021c), each paragraph is encoded \nusing the corresponding TLM-based encoder, then a shallow version of the TLM \nis fed with paragraph encodings and obtains a new representation of a paragraph \nthat is aware of the surrounding paragraphs. The document representation is then \nobtained from the context-aware paragraph representations through max-pooling. \nResults show the benefit of this hierarchical approach.\nUsing the same three LexGLUE datasets, Mamakas et  al. (2022) propose \nto modify Longformer in order to deal up to 8192 tokens and to combine Legal-\nBERT@aueb with TF-IDF representations. In the former case, the proposed \n959\n1 3\nTransformer-based language models for AI and law\nLongformer-8192 is designed to increase the maximum input length up to 8192 \ntokens and to decrease the local attention window size from 512 to 128 to reduce \ncomputational burden. A variant called Longformer-8192-PAR is also introduced \nwhich is Longformer-8192 with the addition of the global token [SEP] between par-\nagraphs to get paragraph-level representations. Also, the following models are intro-\nduced: LegalLongformer, which is a Longformer warm-started from LegalBERT@\naueb and able to handle up to 4096 tokens, LegalLongformer-8192 and LegalLong-\nformer-8192-PAR, which are Longformer-8192 and Longformer-8192-PAR, respec-\ntively, based on LegalLongformer. For the second approach, TFIDF-SRT-Legal-\nBERT is proposed as a version of LegalBERT@aueb in which duplicate sub-words \nare removed from the input text to decrease its length. The remaining sub-words \nare ordered by decreasing TF-IDF, so that the model is induced to attend more the \nearlier sub-words (with higher TF-IDF). This model is also extended by including a \nTF-IDF embedding layer. Both the approaches are fine-tuned and evaluated on Lex-\nGLUE tasks. The models of the first approach achieve better results on almost all \ntasks, and surpass the best model for Chalkidis et  al. (2022b), i.e., the hierarchi-\ncal version of LegalBERT. This demonstrates the effectiveness of warm-start from \na legally pre-trained model and the adding of further positional embeddings and \nglobal tokens.\nA hierarchical approach for handling long documents is also proposed in \n(Chalkidis et al. 2022a) by using Hierarchical Attention Transformer (HAT) models. \nThe HAT models are based on a multi-level hierarchical attention pattern, which \ncomprises a segment-wise encoder (SWE) that applies to segments independently, \nfollowed by a cross-segment encoder (CSE), to get contextual representation across \nsegments. Each segment is preceded by the [CLS] token, which represents the seg-\nment encoding. These two encoders can be arranged to form different architectural \ntopologies. The text segmentation process is based on a dynamic segmentation strat-\negy that aims to preserve sentence integrity while minimizing padding. The HAT \nmodels are pre-trained with the MLM objective task and the parameters of the HAT \nmodels are warm-started following BERT and RoBERTa. Thus, they are fine-tuned \nand evaluated on benchmarks of different domains, including ContractNLI and \nECtHR. Results demonstrate that HATs perform better across all tasks than sparse-\nattention models like Longformer and BigBird, and also they are less demanding in \nterms of computation and memory resources.\nShukla et  al. (2022) also evaluate three methods to deal with long legal docu-\nments: TLMs designed for long text (e.g., Longformer), chunking approaches along \nwith TLMs designed for short summaries (e.g., BART and Legal-PEGASUS), and \nextractive summarization techniques to reduce the size of the input document com-\nbined with abstractive methods. For the latter approach, Shukla et al. (2022) propose \nBERT_BART, which first uses a BERT-based extractive summarization model to \nextract relevant sentences, then carries out a chunking-based BART model to get the \nfinal abstractive summary. Nonetheless, the chunking-based methods (i.e., Legal-\nPEGASUS and BART task-adapted through fine-tuning) and Legal-LED turn out to \nbe the best performers for the document-level evaluation and for the segment-wise \nevaluation, respectively.\n960 C. M. Greco, A. Tagarelli \n1 3\nShen et  al. (2022) fine-tune and evaluate BART, PEGASUS, LED and PRIM-\nERA for abstractive summarization on the Multi-LexSum benchmark. In particu-\nlar, for the multi-document summarization task, LED and PRIMERA, which deal \nwith long inputs, perform better than PEGASUS and BART on all the three tasks \nof Multi-LexSum, although no model has shown to be able to generate long sum-\nmaries of length similar to those provided by humans. When considering short sum-\nmaries, gold summaries provided in Multi-LexSum are used as input to summarize, \ninstead of the source document. In this setting, on single-document summarization \ntasks, PRIMERA reaches the performance of PEGASUS and BART, while the per -\nformance degrades when BART-generated summaries are used as input instead of \nthe gold ones. Long and tiny versions of generated summaries have benefited from \nmulti-document summarization compared to the summaries of the single-document \ncounterpart.\n4.4.2  Domain‑adaptive pre‑training methods\nThere is also a body of works jointly focusing on domain-adaptive pre-training and \nhandling long documents. The common approach is to apply in-house or existing \n(e.g., Legal-BERT@aueb) further/from-scratch pre-trained models on segmented or \nsummarized versions of long legal documents.\nIn the context of employment notice prediction, Lam et al. (2020) adapt BERT-\nbase and RoBERTa-base on a corpus of notice case texts in addition to the Harvard \ncase law dataset. Using only the MLM criterion objective, the two models are fur -\nther pre-trained for text classification with ten epochs and fine-tuned on 409 remain-\ning cases. The goal is to predict reasonable notice based on a free-text summary of \nthe case law, where the summaries are unstructured text data written in plain, non-\nlegal English collected from WestLaw’s Quantum service. Surprisingly, however, \nresults have shown a decrease in performance by the domain-adapted models against \nthe out-of-the-box RoBERTa.\nLimsopatham (2021) compare BERT, Legal-BERT@aueb, Legal-BERT@stan-\nford, and RoBERTa on a text classification task. After removing tokens in the rear, \nresp. front, of the input texts to ensure the 512 token length, each model is applied \non chunks of 200 tokens. An average pooling layer, or alternatively max pooling \nlayer, is used before the classification layer. Results obtained on the ECHR Violation \ndataset and the Overruling Task dataset, which are multi-label and binary classifica-\ntion tasks, respectively, show that Legal-BERT@aueb and Legal-BERT@stanford \nachieve better results than the other methods. However, fine-tuned BigBird and fine-\ntuned Longformer (even though not pre-trained on in-domain documents) outper -\nform the other approaches that adapt BERT to deal with long documents by truncat-\ning them.\nKhazaeli et  al. (2021) propose a QA system for both factoid and non-factoid \nquestions. It comprises a search repository, which contains the passages and their \nlearned embeddings, a search engine, which retrieves passages by text and embed-\nding similarity, and an answer finder, which re-ranks the retrieved passages. In \nthe stage of passage retrieval, it makes use of a Siamese BERT architecture (a.k.a. \nSBERT)  (Reimers and Gurevych 2019). A Siamese Legal BERT system is hence \n961\n1 3\nTransformer-based language models for AI and law\ntrained on a collection of headnotes to retrieve similar passages, with a regression \nobjective function with cosine loss. A legal BERT model pre-trained from scratch, \nwith a custom legal vocabulary on US case-law documents, receives in input a sen-\ntence embedding, and uses mean pooling of the tokens embedding. The answer \nfinder is trained by fine-tuning the in-house Legal BERT model, whose classifier \nuses the [CLS] representation with two fully connected layers with a final softmax \nlayer. For each input question-passage pair (q,  p), the answer finder concatenates \ntheir texts in the form ‘[CLS] ⟨q⟩[SEP]⟨p⟩[SEP]’, and computes the probability that \nthe passage answers the question.\nLi et  al. (2021b) consider the COLIEE-2021 Task 1 as a ranking problem and \naddress the BERT’s input length limitation handling the text at paragraph and docu-\nment level. Firstly, the input is divided into paragraphs. Each paragraph is encoded \nby a bert-base-uncased model that is further pre-trained on the Task 1 cor -\npus, dubbed BERT-Legal. The pre-training task is similar to the MLM task but relies \non the n-gram masking method to obtain the masked input. Each paragraph is rep-\nresented by the hidden vector obtained for the [CLS] token. Then, a paragraph level \ncoherence matrix of the query-candidate pairs is derived. The document level repre-\nsentation is obtained encoding the coherence matrix through a max-pooling strategy \nand a LSTM model. The output of the LSTM is adopted to get the contextual based \nsemantic ranking through relevance prediction. The BERT-Legal ranking is part of \na retrieval pipeline, in which the first stage is to recall top-50 relevant cases using \nBM25-based similarity score.To reduce the training consumption, the authors set \na maximum number of paragraphs for a document. The proposed solution has not \nachieved satisfactory results, probably due to a bad candidates selection operated by \nthe BM25-based recall method. For the Task 2, BERT-Legal is fine-tuned for binary \nclassification on the case law entailment dataset. The authors propose two further \nversions of the model: the one leverages Task 1 dataset to augment the positive and \nnegative samples in the training set, the other one adversarially train BERT-Legal \nwith Fast Gradient Method (Miyato et al. 2017) to be robust to embedding pertur -\nbations. However, both the strategies have shown to decrease the performance of \nBERT-Legal on the test set, probably due to the different data distribution between \nTask 1 and Task 2.\nFurniturewala et al. (2021) address both summarization tasks at AILA-2021. For \nTask 2a, two approaches are proposed: (i) to fine-tune Legal-BERT@aueb, and (ii) \nto concatenate statistical feature vectors obtained by TF-IDF with the semantic fea-\nture vectors obtained by the Legal-BERT@aueb in correspondence to the [CLS] \ntoken. The final joint features are classified by a SVM model and logistic regression. \nFor Task 2b, the sentences that were labeled as relevant in Task 2a were concat-\nenated into one summary. The authors reached the first position for Task 2a and the \nbest scores as regards some of the ROUGE metrics for Task 2b.\nConcerning the same tasks at AILA-2021, Jain et al. (2021) use Legal-BERT@\naueb to get contextual embeddings of the sentences and fed them to an MLP model. \nFive training datasets are obtained, so that, for Task 2a, the probability for a sen-\ntence to be relevant is the average of all the five probabilities computed by the mod-\nels. a sentence is labeled as relevant if the average probability computed over the \nfive models exceeds a certain threshold. A second run is proposed for Task 2a, in \n962 C. M. Greco, A. Tagarelli \n1 3\nwhich both rhetorical and relevance labels are considered using a multi-task learn-\ning MLP model. This is motivated by the intuition that rhetorical labels can be \nhelpful for predicting relevance labels. A third run is also proposed, combining the \nresults of the first two runs by averaging all the individual probabilities of each indi-\nvidual trained model. For Task 2b, the average probabilities computed by each run \nof Task 2a are used to rank the sentences. The sentences in top positions are picked \nto get the summary, until the summary length limitation imposed by the organizers \nis reached. All the runs for Task 2a reached leading positions, surpassed only by \nFurniturewala et al. (2021). For Task 2b, the first run achieved the highest F1 score \nfor all the ROUGE metrics, the best recall for some of them, and the second-highest \nprecision for all the metrics.\nAskari and Verberne (2021) address the COLIEE-2020 and 2021 Task 1 by creat-\ning shorter query documents based on three approaches: term extraction with Kull-\nback–Leibler divergence for informativeness (KLI), noun phrase or entity extrac-\ntion, and abstractive summarization using LED fine-tuned on the COLIEE-2018 \ndata containing human-written summaries of the case documents. Such summaries \nare then combined using lexical ranking methods based on BM25 and statistical \nlanguage modeling as well as neural ranking methods based on DRMM, Legal-\nBERT@aueb, and a version of BERT and Longformer designed for document rank-\ning following MacAvaney et al. (2019). Ensemble models are also proposed using \nthe scores of neural and lexical rankers as features for a classifier (the authors exper-\nimented with SVM, Naive Bayes and MLP). Such methods are primarily evaluated \non COLIEE-2020 data. The results show that the best performance is obtained by \nthe ensemble of BM25 (with KLI summarizer) and BERT (with LED summarizer), \nsurpassing the first team at COLIEE-2020 (Westermann et al. 2020). On COLIEE \n2021, BM25 with KLI summarizer surpassed the first team at the competition (Ma \net al. 2021).\n4.5  GPT‑based methods\nSince the launch of ChatGPT in November 2022, there has been a worldwide \nrenewed interest towards generative language models, which has inevitably impacted \non the legal domain as well. In the following, we overview a few works focusing on \nGPT-based models that have very recently appeared in the literature.\nII and Katz (2022) evaluate the performance of a zero-shot GPT-3 (text-\ndavinci-003) model on the multiple choice component of the US Bar Exam. The \nquestionnaire is typically divided into eight categories, seven of which regard spe-\ncific law areas and one is used to experiment with the test design. Each category \ncontains 25 questions and 4 candidate answers for each question. Different types of \nprompts were assessed to get the best answers from the model, and the best strategy \nturned out to be asking the model to sort the top three multiple-choice answers for \nthe question. Results show that the model is not able to pass the entire exam with \nits first choices, however it greatly outperforms baselines based on random choices. \nMoreover, it reaches the average passing range (58–62%) in two categories. When \nconsidering the top two answers, the model passes the average passing range in all \n963\n1 3\nTransformer-based language models for AI and law\nTable 6  Summary of main methods discussed in Sect. 4.5\nMethod Ref. Downstream tasks Lang. Data Long docs?\nZero-shot GPT-3  II and Katz (2022) MCQA EN Bar exams Yes\nZero-shot/Few-shot GPT-3  Blair-Stanek et al. (2023) NLI, QA EN CAP cases Yes\nZero-shot/Few-shot/Fine-tuned GPT-3  Yu et al. (2022a) COLIEE 2021 Task 4 EN Civil code Yes\nFine-tuned GPT-3 (LawGPT 1.0)  Nguyen (2023) QA EN Legal cases Yes\nZero-shot ChatGPT  Choi et al. (2023) QA EN Constitution Yes\n964 C. M. Greco, A. Tagarelli \n1 3\ncategories and exceeds the average results obtained by human examiners in 5 out of \n7 categories, with an overall average score on all categories of 71%, in contrast to \nthe average score of 68% obtained by humans. Results get even better when consid-\nering the top three answers, with an overall average score of 88%.\nBlair-Stanek et al. (2023) evaluate several prompting approaches on GPT-3 (text-\ndavinci-003) to perform statutory reasoning on the SARA benchmark. In particular, \nthey consider dynamic few-shot prompting, chain-of-thought prompting and zero-\nshot prompting. In the few-shot scenario, the four most similar training cases are \nprovided for each test case; the prompt pattern for the training case consists of the \ntext of case labeled with the premise, the hypothesis and the answer (entailment or \ncontradiction), whereas the prompt pattern of the test case contains only the prem-\nise and the hypothesis. In the zero-shot prompting, no training cases are used. In \nthe chain-of-thought prompting, a prompt consists of ten training cases, each hav -\ning the same labels as for the few-shot prompting, followed by a chain-of-thought \nthat explains the reasoning leading to the conclusion (i.e., entailment or contradic-\ntion). The same prompt pattern is used for the test cases. Moreover, the prompts \ncan be augmented by adding the statute(s) required to solve the entailment reason-\ning, since GPT-3 may also have been trained on the US tax code. Following Kojima \net  al. (2022), if the model does not explicitly indicate the result of the reasoning \n(entailment or contradiction), the model is prompted again with the original prompt \naugmented with the model’s answer and adding a clear answer imposition string at \nthe end, i.e., “Therefore, the answer (Entailment or Contradiction) is”. Optionally, \nin zero-shot and few-shot prompting, the model is forced to explain the chain-of-\nthought reasoning by adding the phrase “Let’s think step by step” (Kojima et  al. \n2022). Results show that the model is highly sensitive to the prompt patterns, with \nsome of the prompts outperforming the previous state-of-the-art BERT-based model \n(Holzenberger and Durme 2021). Also, the chain-of-thought imposition improves \nthe performance but not systematically, whereas the worst prompt combination is, \nas expected, the zero-shot setting without the text of the statue(s). However, examin-\ning the chain-of-thought reasoning, it turns out that the model has a certain knowl-\nedge of the US tax code, yet imperfect, meaning that the model tends to refer to the \nwrong part of the statutes, even when the statute is provided in the prompt.\nYu et al. (2022a) test the performance of GPT-3 (text-davinci-002) on the Task \n4 of COLIEE 2021, adopting the following approaches: zero-shot/few-shot setting \n(with and without the chain-of-thought reasoning) and fine-tuning. In the zero-shot \nsetting, the prompt pattern consists of instructions, the premise-hypothesis pairs \nfrom COLIEE dataset and the question phrase “True or False?”. In the few-shot set-\nting, hypothesis-answer demonstration pairs of previously evaluated bar exam ques-\ntions are given to the model in a 1-shot, 3-shot and 8-shot manner. Results show \nthat the worst performing prompt in the zero-shot setting reaches the best model \nof COLIEE 2021, which is outperformed when a more instructive prompt is pro-\nvided to the model. In the few-shot setting, all the k-shot combinations exceed the \nCOLIEE 2021 winner, with 3-shot and 8-shot reaching the same performance and \nsurpassing the 1-shot. Following Kojima et al. (2022), the authors consider another \nconfiguration adding the chain-of-thought reasoning imposition (“Let’s think step \nby step”) in the prompt and query again the model with its first response and the \n965\n1 3\nTransformer-based language models for AI and law\nanswer imposition (“Therefore, the hypothesis is (True or False)”). Moreover, they \nalso fine-tune the model on the COLIEE 2021 training set in two completion set-\ntings: providing only the binary entailment answer or providing also the explana-\ntions. In the second setting, the explanations are either pseudo-explanations (i.e., for \neach premise the most relevant sentence for hypothesis) or explanations created by \nthe model (i.e., for each hypothesis-premise-answer triplets, the model is prompted \nto generate explanations, with a specific prompt pattern). Results show that fine-\ntuning with pseudo-explanation surpasses fine-tuning with its GPT-3 explanation, \nbut the use of explanations underperforms the fine-tuning without explanations. \nThis suggests that allowing the model to reason independently might lead to bet-\nter results. However, the best results are achieved using legal reasoning prompts \n(Burton 2017) under the zero-shot setting. This approach aims to lead the model to \n“think like a lawyer”, encouraging it to consider, for example, the facts and circum-\nstances that lead to the legal case, to find the rules governing the issue, to apply the \nrules to the fact and to determine the entailment task.\nNguyen (2023) introduces LawGPT 1.0, a GPT-3 based model that provides \nlegal assistance in a conversational manner. LawGPT 1.0 is obtained by fine-tuning \nGPT-3 on a large legal corpus and is evaluated on several tasks, including the gener-\nation of legal documents, legal question-aswering and provide legal advice. Results \nreveal its competitiveness w.r.t. existing virtual legal assistants.\nChoi et al. (2023) focus on evaluating ChatGPT on four law school exams at the \nUniversity of Minnesota courses of Constitutional Law, namely Federalism and \nSeparation of Powers, Torts, Employee Benefits and Taxation. The exams consist \nof 95 multiple choice questions and 12 essay questions. The model is questioned \nwith a uniform set of prompts (i.e., without adapting the prompts to a specific course \nor question). For multiple choice questions, three prompting approaches are evalu-\nated: simple prompting, which requires to give only the answer, the chain-of-thought \nprompting (described by Blair-Stanek et  al. 2023) and the rank-order prompting, \nwhich consists in ranking the top three choices like in (II and Katz 2022). ChatGPT \nwas able to pass all the exams, although reaching a low average score; in general, \nit performed better on the essay questions than on the multiple choice questions. \nOn the essays, in some cases it was as good as or even better than the average per -\nformance of human students, but in other cases it strongly failed, particularly when \nessay questions regard specific cases or theories explained in the course. On the \nmultiple-choice questions, it performed much worse on questions involving numbers \nand better in questions involving relatively uniform legal rules across the jurisdic-\ntions. Based on these results, the authors suggest some guidelines for constructing \nprompts to help the model generate better outputs, such as specifying at the end of \nthe prompt the tone of the writing of the legal essay, or a word range.\n4.6  Methods for non‑English legal languages\nPrompted by the success in English text data, in the past few years there has been an \nincreased interest in replicating the advances in NLP based on TLMs for many other \nlanguages. As a result, a plethora of non-English BERT and related models have been \n966 C. M. Greco, A. Tagarelli \n1 3\ntrained and developed, such as CamemBERT (Martin et al. 2020), BARThez (Eddine \net  al. 2021) and FlauBERT (Le et  al. 2020) for French, GermanBERT, 121 GBERT \nand GELECTRA (Chan et  al. 2020) for German, AlBERTo (Polignano et  al. 2019), \nGilBERTo122 and UmBERTo123 for Italian, KoBERT124 and KoBART 125 for Korean, \nRobBERT (Delobelle et al. 2020) and BERTje (de Vries et al. 2019) for Dutch, RoB-\nERT (Masala et al. 2020) and Romanian BERT (Dumitrescu et al. 2020) for Romanian, \nGreek-BERT (Koutsikakis et al. 2020) for Greek, RoBERTuito (Pérez et al. 2022) and \nBETO126 for Spanish, AraBERT (Antoun et al. 2020) for Arabic, MacBERT (Cui et al. \n2020), PERT (Cui et  al. 2022) and MarkBERT (Li et  al. 2022) for Chinese, Aleph-\nBERT (Seker et al. 2021) for Hebrew, BERTimbau (Souza et al. 2020), RoBERTa-PT-\nBR127 and GPorTuguese-2 (Guillou 2020) for Brazilian, and so on.\nIn specialized domains, such as the legal one, the challenge underlying the devel-\nopment of TLMs is even more evident, due to the lack of large domain-specific \ntraining data for specific languages. In this section, we briefly describe main meth-\nods for non-English TLM-based legal learning, organized by language.\nBrazilian. Feijó and Moreira (2019) experiment with extractive and abstractive \nmodels to summarize documents of legal rulings. To this purpose, the RulingBR \ndataset is used, which contains 10K rulings from the Brazilian Supreme Court, and \nis divided in 60% training, 20% validation and 20% test samples. Each ruling is \norganized in sections, the first of which is the summary. The abstractive approaches \ninvolved are based on neural networks, and include a Transformer model and a \nTransformerAAN model (Zhang et al. 2018). The latter uses a cumulative average \nattention as an alternative to the self-attention in the decoder side to accelerate the \ndecoding procedure. Results show that abstractive approaches outperform extractive \nmethods, with Transformer getting the highest scores.\nLage-Freitas et al. (2022) evaluate classic machine learning techniques and deep \nlearning models, including BERTimbau, on court decision prediction and unanimity \ndecision prediction. In general, due to the small size of the dataset they propose (cf. \nSect. 3.4), deep learning techniques are outperformed by other methods. One excep-\ntion is for BERTimbau, which obtains comparable or slightly higher performance \nwhen predicting on three-label case outcomes (yes, no, partial) with imbalanced set-\nting of the dataset. Regarding unanimity prediction, deep learning models perform \nbetter than the classic ones in F1 score when the data set is balanced.\nPolo et  al. (2021) release a library 128 containing available pre-trained language \nmodels, including BERT, for the Brazilian legal language. The library also includes \na package with useful functions and demo examples to facilitate the use of the \n121 https:// www. deeps et. ai/ german- bert.\n122 https:// github. com/ idb- ita/ GilBE RTo.\n123 https:// github. com/ musix match resea rch/ umber to.\n124 https:// github. com/ SKTBr ain/ KoBERT.\n125 https:// github. com/ SKT- AI/ KoBART.\n126 https:// github. com/ dccuc hile/ beto.\n127 https:// huggi ngface. co/ josu/ rober ta- pt- br.\n128 https:// github. com/ felip emaia polo/ legal nlp.\n967\n1 3\nTransformer-based language models for AI and law\nmodels. The models are trained using datasets from several sources, in particular \nBERT is trained using three datasets, regarding clippings and motions from differ -\nent Brazilian courts and longer documents from the Court of Justice of São Paulo, \nstarting from a checkpoint of Souza et  al. (2020). The resulting model is dubbed \nBertikal.\nAguiar et al. (2021) evaluate different NLP methods along with several combi-\nnations of embeddings of Portuguese language models on the lawsuit classification \ntask. Such methods are trained on a collection of petitions and indictments from the \nBrazilian Court of Justice of the State of Cearà, in which BERTimbau obtains best \nperformance when lawsuit is embedded as the concatenation of all petitions contain-\ning one or more references to a legislation.\nSerras and Finger (2022) focus on the categorization of case laws considering a \nBrazilian legal dataset.129 In particular, they fine-tune m-BERT and the base and large \nversion of BERTimbau. In the fine-tuning process, pairs of summary and header are \ngiven, and the aim is to generate the terms of the header from the summary. From the \nexperimental results, it is observed that BERT-based methods outperform a statistical-\nbased baseline. Larger BERT models perform slightly better than the base ones, high-\nlighting that base models are robust enough to the deal with the task.\nChinese. Dong and Niu (2021) address the legal judgment prediction task as a graph \nnode classification problem, handling constraints among articles, charges and terms \nof penalty. Using the training set, a global consistency graph is derived, composed of \nall the possible relations of class labels (i.e., articles, charges and terms-of-penalty) \ntreated as graph nodes. To prevent logically conflicting judgment results, relational \nlearning is introduced into a Transformer model to achieve global and local consist-\nency. The resulting model is dubbed R-former. R-former’s architecture includes a \nnode encoder module and a node classification module. The first module is com-\nposed of two Transformers (Dai et al. 2019): the former obtains the article repre-\nsentations from raw text, the latter uses masking mechanisms to extract consistency \ninformation among nodes belonging to different tasks (article prediction, charge \nprediction and terms-of-penalty prediction tasks) and distinction information of the \nsame task. The second module consists of a graph convolution network to obtain the \nrelevance score of each node according to the neighbors in the consistency graph. \nExperiments are conducted using CAIL-small and CAIL-big datasets from Chinese \nAI and Law challenge (CAIL2018) (Xiao et al. 2018), with R-former performing the \nbest among all the evaluated competitors.\nSun et  al. (2021) combine BERT, BiLSTM and Conditional Random Fields \n(CRF) to identify legal case entities. To build an name entity recognition model, \nthe authors leverage BERT’s ability of text feature extraction, using it as input layer \nin order to get word embeddings. In addition, they employ a BiLSTM to get long-\nterm memory information. Finally, state-transition matrix in CRF is used to output \nthe globally optimal sequence, with more accurate labeling according to specific \n129 Two reformulated versions of the Kollemata dataset: https:// www. kolle mata. com. br/. The dataset is \nnot publicly available.\n968 C. M. Greco, A. Tagarelli \n1 3\nconditions. The resulting model is evaluated using the corpus of People’s Daily \nnewspaper along with Legal Case Texts from the CAIL Law Research Cups and \nother legal texts manually labeled. The authors find that using BERT’s word embed-\ndings instead of Word2Vec (Mikolov et al. 2013) improves greatly the recognition \naccuracy.\nLyu et  al. (2022) present the so-called Criminal Element Extraction Network \n(CEEN) for reinforced criminal element extraction useful for predicting legal judg-\nments. CEEN is designed to handle misleading law articles, having very similar \nTF-IDF representations, and indistinguishable descriptions of facts, having different \ntargets and criminals. CEEN is composed of four parts: a fact description encoder, \na reinforcement-learning-based element extractor, a criminal element discriminator \nand a multi-task judgment predictor. The fact description encoder is used to obtain \ncontextual sentence representations of facts. The encoder is typically performed by \nfour hierarchical BiLSTMs, although it can also be a BERT-based variant of CEEN, \ndubbed CEENBERT , which exploits the addition of the [CLS] token at the beginning \nof each sentence in the input, like in (Liu and Lapata 2019a). After obtaining sen-\ntence representations, a reinforcement-learning-based element extractor is used to \ndistinguish confusing fact descriptions, then a criminal element discriminator gets \nthe discriminative criminal element representations. Finally, a multi-task judgment \npredictor outputs the judgment results. Experiments were conducted using CAIL-\nsmall and CAIL-big datasets from Chinese AI and Law challenge (CAIL2018) (Xiao \net al. 2018) and a set of baselines, including Chinese BERT, RoBERTa and Law -\nformer. Experimental results show that the combination of CEEN and BERT signifi-\ncantly overcomes BERT and all the baselines, indicating that language knowledge \nfrom pre-training and criminal elements’ identification are complementary to each \nother.\nFeng et al. (2022) propose to leverage event extraction from the fact description \nof criminal cases to constrain models for the LJP task on the CAIL-2018 bench-\nmark. Indeed, most of the incorrect predictions are due to an inadequate identifica-\ntion of key events in the fact description that determine the final judgment, and the \nlack of consistency constraints among CAIL sub-tasks (law article, charge and term \nof penalty prediction). In this regard, the EPM model is introduced, which is based \non a legal BERT (Zhong et al. 2019a) referred to as LegalBERT@OpenClap, and \nan attention mechanism, which is guided by event-based and cross-task consistency \nconstraints. EPM is pre-trained on the training set of CAIL and fine-tuned on LJP-E, \na portion of CAIL-small dataset, provided by the authors and manually-annotated \nwith event triggers and roles. Results have shown that EPM outperforms several \nbaselines for the benchmark and the event extraction process performed jointly with \nthe LJP task is beneficial also for the performance of the competitors.\nYu et al. (2022b) propose an explainable method, called IOT-Match, for the task \nof case matching prediction. Given the sentences of a pair of legal cases, it extracts \nrationales based on semantics and legal characteristics, and generates explanations \nso that the matching prediction (carried out on eCAIL and ELAM, cf. Sect.  3.4) is \nbased on the extracted rationales and explanations. More details on IOT-Match are \ndiscussed later in Sect. 4.8.\n969\n1 3\nTransformer-based language models for AI and law\nLi et al. (2021a) propose CLASS (Chinese LegAl judgmentS Summarization), a \nmethod to generate abstractive summaries of Chinese legal judgments. Once rele-\nvant sentences are extracted from the input, the legal judgments are split into rhe-\ntorical roles, then a summary of each rhetorical role is generated. The extraction \nmodule uses BERT to get the embedding of each sentence, then a Bi-LSTM model \nencodes the sequence of sentences. A BERT-BiLSTM-CRF model is trained to split \nthe legal judgments into the rhetorical roles. The UniLM model is chosen to get \nthe abstractive summary of each rhetorical role. CLASS is evaluated using the legal \njudgment summarization dataset of CAIL2020, divided in 80% training, 10% vali-\ndation and 10% test samples. Results show that CLASS can achieve higher perfor -\nmance compared to sequence-to-sequence competitors.\nHuang et al. (2021) propose a graph-augmented abstractive summarization model \nfor the automatic summarization of legal public opinion news. A separate graph \nencoder generates a structural representation of the source document. The compo-\nnents of a document are organized into two graphs, named element relational graph \n(ERG) and topic interaction graph (TIG). In ERG, the elements extracted from the \ndocument (i.e., entities, keywords, and event triples) are nodes that are connected \nwith virtual nodes representing Person, Location, Keyword and so on, in order to \nget the document-level interconnections. In TIG, similarities of different nodes are \nrepresented, where the nodes are the elements of the document that can be viewed as \ntopics and the graph can aid to detect the main topic of the document. The model is \ncomposed of a sequence encoder, to yield the sequential representation of the docu-\nment, a graph encoder, to yield the structural representation of the document, and a \nsequence decoder, to generate the summary constrained by a dual attention mech-\nanism on the sequential and structural representation of the encoders. The graph \nencoder is a Graph Transformer Network (Yun et  al. 2019), which incorporates \nglobal structural information while learning from the neighbor nodes. The sequence \nencoder and the decoder are based on a vanilla Transformer. A BERT-based embed-\nding mechanism is applied to improve the performance by enhancing the sequence \nencoder’s embedding and by initializing the nodes of the element graph. The model \nis evaluated using a legal public opinion summarization corpus, named LPO-news, \nconsisting of article-summary pairs from the Sina Weibo website. Extensive experi-\nments demonstrate that the proposed model outperforms competing baselines in \nterms of both ROUGE and BERTScore metrics, also when compared on general \nnews-oriented datasets, and that it can generate more coherent, faithful and informa-\ntive summaries.\nThe first pre-trained model for legal long documents is Lawformer (Xiao et al. \n2021).130 Following Longformer, Lawformer is pre-trained with MLM objective, \ncontinuing from the checkpoint RoBERTa-wwm-ext  on a collection of criminal \nand civil cases published by the Chinese government from China judgment Online.\nLawformer is then fine-tuned on a number of tasks, namely legal judgment \nprediction, legal case retrieval, legal reading comprehension, and legal question \n130 https:// github. com/ thunlp/ Legal PLMs.\n970 C. M. Greco, A. Tagarelli \n1 3\nanswering, for which datasets CAIL-Long, LeCaRD, CJRC, and JEC-QA are used, \nrespectively.\nFrench. Salaün et  al. (2020) fine-tune the FlauBERT base cased language model \n(Le et al. 2020), which is a pre-trained BERT on French corpora, to a collection of \nlawsuits submitted to a tribunal specialized in disputes between tenants and land-\nlords in Quebec, i.e., the Réegie du Logement du Québec (RDL). The RDL lawsuits \nare organized into three sections: fact description, legal reasoning and the verdict. \nThe documents also contain metadata describing court location, judge, presence and \ntype of each party: legal person (juridical entity) or physical person, single person \nor multiple people, male or female and so on. The task is to classify the outcomes \nof the judgments with three possible labels: penalty (judge convicts the defendant), \nagreement (judge imposes an agreement) and rejection (judge rejects the the plain-\ntiff’s claims). FlauBERT is trained without the use of metadata and it achieves better \nperformance when trained on all texts (discarding verdicts).\nDouka et al. (2021) propose to pre-train small sizes of BERT (base, small, mini \nand tiny) from scratch using an MLM task and French legal datasets. The latter con-\nsist of the decisions of the Court and the Claimant’s pleadings from the Court of \nCassation, together with raw legal texts crawled from Légifrance website. 131 The \nresulting model, dubbed JuriBERT, 132 is then tested on two downstream classifica-\ntion tasks: to assign the Court’s Claimant’s pleadings to Chambers and Sections of \nthe Court of Cassation (8 labels), and to classify the Claimant’s pleadings to a set of \nsubjects (151 labels). A variant of JuriBERT, pre-trained on Claimant’s pleadings, \nis also provided in tiny and mini versions, as well as another JuriBERT (JuriBERT-\nFP) obtained by further pre-training CamemBERT (Martin et al. 2020). From the \nevaluation results, it is observed that JuriBERT-small outperforms larger models \nwhen training on specific sub-languages like the legal one. However, due to lim-\nited resources, larger models like JuriBERT-FP and JuriBERT-base have been pre-\ntrained with the use of smaller batch sizes than the other models. By fixing a model \nsize, it is shown that JuriBERT pre-trained from scratch on the same task-specific \ndata used in the fine-tuning can lead to better performance compared with only \ndomain-specific models. JuriBERT-FP outperforms JuriBERT-base, demonstrating \nthat further pre-training a general-purpose model can be a preferable choice.\nGarneau et al. (2021) propose a further pre-training of BARThez (Eddine et al. \n2021) on a French legal corpus for criminal law, collected from the Criminal and \nPenal Chamber and mined from the Société Québécoise d’Information Juridique \n(SOQUIJ) website. 133 The legal comprehension of the resulting model, dubbed \nCriminelBART, was evaluated through Cloze tests regarding the prediction of crimi-\nnal charges, legal provisions, and privacy.\nLouis and Spanakis (2022) evaluate lexical and dense models on the BSARD \nbenchmark (cf. Sect.  3.4). The lexical models are based on TF-IDF and BM25 and \n131 https:// www. legif rance. gouv. fr/.\n132 http:// maste r2- bigda ta. polyt echni que. fr/ Frenc hLing uisti cReso urces/ resou rces# jurib ert.\n133 https:// soquij. qc. ca/a/ fr/.\n971\n1 3\nTransformer-based language models for AI and law\nare used to retrieve the top-k articles for a given question, based on scores computed \nfor each article. The dense models are bi-encoder models, in two architectures: sia-\nmese, which uses a unique word embedding model for both questions and articles, \nand two-tower, which uses two separated word embedding models. For each ques-\ntion, a similarity score is computed for all articles that are pre-encoded, and finally \nthe top-k articles based on calculated scores are retrieved. The dense models are \nevaluated in a zero-shot setting, using word2vec and CamemBERT. To handle long \narticles for CamemBERT, each text is divided in overlapping chunks. The fine-tuned \nCamemBERT has shown to outperform all the other competitors but, considering \nonly the zero-shot variants, the word2vec-based model gives significantly higher \nperformance w.r.t. BERT-based models.\nThe BSARD benchmark is also employed by Louis et al. (2023), where a graph-\naugmented dense retrieval model, called G-DSR, is proposed to exploit the statute \nhierarchy to enrich the article information. G-DSR includes two components: a \ndense statute retriever and a legislative graph encoder. The first component is a bi-\nencoder for articles and questions, so that a question and an article relevant to the \nquestion get similar representations. Questions are encoded using the [CLS] token \nrepresentation obtained by CamemBERT further pre-trained on BSARD articles, \nwhile the encoding process for statutory articles is left to a hierarchical encoder to \nhandle the length of the documents. Given a pair of article and question, a similarity \nscore is calculated to evaluate the relevance of the article for the question. The bi-\nencoder is trained through contrastive learning to get effective embedding functions. \nThe second component of G-DSR represents the statute’s hierarchy as a directed \nacyclic graph with two types of nodes: section nodes, corresponding to the headings \nof the code subdivisions, and the article nodes, representing the textual content of \nthe articles. The edges of the graph model the hierarchy between sections and arti-\ncles. The semantic information of a node is initialized through the article encoder of \nthe first block and used as the initial node features. A graph neural network is then \nemployed to update the node features by aggregating the local neighborhood infor -\nmation based on the graph structure. The resulting model show higher performance \nagainst baselines such as BM25, DPR and a combination of BM25 and mT5. Also, \nthe enriched information given by the legislative graph encoder to the dense statute \nretriever contributes significantly to improve performance.\nGerman. Wrzalik and Krechel (2021) fine-tune GBERT and GELECTRA base  \nmodels to re-rank retrieved documents of GerDaLIR (cf. Sect.  3.4). Passages that \nexceed the sequence length limitation of BERT and ELECTRA are divided along \nsentence boundaries, then the maximum score is assigned to the passage. ELEC-\nTRA demonstrates to obtain higher re-ranking quality compared to BERT.\nTang and Clematide (2021) address paragraph-level semantic similar -\nity for legal document retrieval, 134 using a corpus of legal cases and statutes \nin German language gathered from the Swiss Federal Court 135 and Swiss \n134 https:// github. com/ lilyt ang20 17.\n135 https:// www. bger. ch/ it/ index. htm.\n972 C. M. Greco, A. Tagarelli \n1 3\nGovernment 136 websites, and extracting from the cases citations that point to \nstatutes. They use GermanBERT, a BERT model trained on partially legal Ger -\nman text, and the German variant of DistilBERT. An extended attention mask \nmechanism is performed to combine the idf scores of non-neural methods with \nneural models. This mechanism suppresses low informative tokens in the input, \nthus impeding the self-attention calculation on those tokens. The authors devel-\noped a link-based similarity method to estimate paragraph-level semantic simi-\nlarity, considering the relations between paragraph cases that share citations to \nthe same statutes. From the experimental results, it appears that GermanBERT, \nalong with the use of the extended attention mask mechanism, offers a clear \nadded value compared to non-neural competitors at specific idf threshold.\nGreek. In (Papaloukas et al. 2021) multiple methods are evaluated on the GLC data-\nset (cf. Sect.  3.4) for classifying legal texts. In particular, the authors experimented \nwith m-BERT, XLM-RoBERTa, Greek-BERT (Koutsikakis et al. 2020) and Greek-\nLegal-BERT (Athinaios 2020). Greek-Legal-BERT is pre-trained on documents \ngathered from a Greek legislative Knowledge Base, called Nomothesia137 (Chalkidis \net al. 2017). Excluding a few exceptions, Greek-Legal-BERT proves to be the best \nperformer for each evaluation level (i.e., volume, chapter and subject level), fol-\nlowed by Greek-BERT, m-BERT and XLM-RoBERTa, thus confirming the impor -\ntance of both in-domain and in-language training.\nItalian. Tarasconi et al. (2020) evaluate different BERT-based approaches for three \nbusiness problems in the processing of case law contents for electronic publishing \npurposes: identification of legal references in the text, new content classification \nbased on relevance, hierarchical labeling of text according to predetermined topics. \nIn the first case, the aim is to identify in a judgment the references to specific laws or \nother judgments. Judgments come from the Italian Highest Courts of Appeal. Best \nperformances are obtained using a fine-tuned version of m-BERT for NER purposes. \nIn the second case, the identification of the potential relevance of a document aims \nto select the ones to be eventually published. Documents are mostly gathered from \njudgments from the Italian Highest Courts of Appeal, but also T.A.R. Administrative \nRegional Tribunal, Italian Constitutional Court and EU courts. The task is addressed \nusing BERT for binary classification, although best results are obtained using a Ran-\ndom Forest model because of the use of hand-crafted features. In the third case, the \ngoal is to assign each document of the Italian Highest Courts of Appeal, with a set \nof topics belonging to a publisher’s proprietary resource. The task is addressed using \na fine-tuned version of m-BERT for extreme multi-label classification. The high-\nquality of legal data, collected over the years by publisher’s managers and decision-\nmakers, allows to successfully experiment with supervised methods.\nLamBERTa (Tagarelli and Simeri 2022) is the first BERT-based framework for \nlaw article retrieval as a prediction problem, focusing on the modeling, learning and \n136 https:// www. fedlex. admin. ch.\n137 http:// legis lation. di. uoa. gr/.\n973\n1 3\nTransformer-based language models for AI and law\nunderstanding of the Italian Civil Code (ICC). To this purpose, LamBERTa fine-\ntunes a pre-trained Italian BERT on the ICC and is designed to answer legal ques-\ntions. The task is conceived as an sequence classification task with a high number \n(i.e., hundreds or thousands) of classes, which correspond to the number of arti-\ncles in the ICC, resp. a within-book portion of it, that is used to train a LamBERTa \nglobal model, resp. book-specific model. Also, the task is a few-shot learning one, \nsince there are few per-class examples to train a model, which are extracted from \neach article of the ICC according to one of several schemes of unsupervised train-\ning-instance labeling defined by the authors. These schemes adopt different strate-\ngies for selecting and combining portions from each article to derive the training \nset, while sharing the requirements of generating a minimum number of training \nunits per article. LamBERTa models have been assessed through single-label as \nwell as multi-label evaluation tasks, based on six different types of queries, which \ninclude jurisprudential sentences associated with the ICC articles, and annotations \nabout the interpretation of the meanings and law implications associated to the \narticles. Also, Simeri and Tagarelli (2023) focus on an investigation of the injec-\ntion of out-of-vocabulary legal terms in LamBERTa models’ tokenizer and of the \nimpact of domain-adaptive pre-training of LamBERTa models on article retrieval \nperformance.\nLicari and Comandè (2022) contribute with ITALIAN-LEGAL-BERT, which \nis the result of a further pre-training of a pre-trained Italian BERT on a corpus \nextracted from the National Jurisprudential Archive (pst.giustizia.it), a repository \ncontaining millions of legal documents, such as decrees, orders, and civil judgments, \nfrom Italian courts and courts of appeal. ITALIAN-LEGAL-BERT was evaluated \non named entity recognition, sentence classification, and sentence similarity tasks.\nJapanese. As previously described, many efforts have been made by researchers in \nthe attempt of modeling TLMs on the Japanese version of COLIEE tasks, especially \non statute law retrieval, entailment and question answering.\nKorean. Yoon et al. (2022) use a KoBERT-based version of BERT2BERT, a model \ncomposed of BERT on both the encoder and decoder side, as well as KoBART to \ngenerate abstractive summaries of Korean legal judgments. The dataset is a col-\nlection of legal precedents gathered from the Korean Court Comprehensive Legal \nInformation site.138 Both the models show good results on the task, with KoBART \nperforming better than BERT2BERT.\nRomanian. Masala et al. (2021) specialize BERT models for Romanian juridical \ndomain.139 To this end, the models are pre-trained from scratch using RoJur, a \nlarge corpus containing civil and criminal cases documents published by Roma-\nnian civil courts, and the whole word masking technique. In RoJur, each judg-\nment is composed of the description of the parties, brief of the arguments, legal \n138 https:// glaw. scourt. go. kr/ wsjo/ intes rch/ sjo022. do.\n139 https:// huggi ngface. co/ reade rbench.\n974 C. M. Greco, A. Tagarelli \n1 3\nreasoning and final verdict. The resulting model, dubbed JurBERT, is then evalu-\nated using two datasets: RoBanking, extracting from RoJur common types of cases \nrelating to banking domain, and BRDCases, a collection of cases involving the \nRomanian BRD bank. The downstream task is to predict if the final verdict in a \nlegal case is in favor of the defendant or the plaintiff, so it is addressed as binary \nclassification.\nSpanish. Gutiérrez-Fandiño et al. (2021a) train a RoBERTa model on a large legal \ncorpora obtained from a collection of different Spanish datasets, including Legal-\nES (Samy et al. 2020). The resulting model is dubbed RoBERTalex, and has been \ncompared with m-BERT and a Spanish RoBERTa (Gutiérrez-Fandiño et al. 2021b). \nSince there is no domain-specific evaluation dataset available, the models are tested \non general-domain tasks (e.g., NER, classification), on which RoBERTalex obtains \ngood performance.\n4.7  Multilingual and cross‑lingual methods\nUnlike English and few high resource languages, many other languages have often \nbeen characterized by low resource data, sometimes resulting in limited or absent \nbenchmarks, which negatively affect the amount of pre-training data, and hence of \nperformed of TLMs. To overcome this issue, one common approach is to develop \nmultilingual language models, which are pre-trained using large amounts of unla-\nbeled data from multiple languages, under the assumption that low resource lan-\nguages can benefit from high resource languages due to shared vocabulary and \nsemantic relatedness aspects. Multilingual TLMs have been indeed proposed in the \npast three years, such as m-BERT and XLM-RoBERTa, mainly differing from each \nother in terms of number of languages involved, architecture components, pre-train-\ning objective functions and corpora. A comprehensive survey on multilingual TLMs \nis recently provided in Doddapaneni et al. (2021). In the following, we shall instead \nkeep our focus on some representative studies of multilingual and cross-lingual \nTLM-based legal learning.\nAydemir et al. (2020) propose to use m-BERT along with the BERT-large cased \nand uncased models to address the COLIEE-2020 Task 3. Results have shown that \nthe multilingual BERT model yields better performance than BERT-large cased. \nHowever, for Task 4, results based on multilingual BERT appeared not be satisfac-\ntory as for Task 3, even compared to the competition baseline.\nNiklaus et  al. (2021) experiment with several BERT-based methods on binary \nclassification of the judgment outcome, using Italian, German and French legal \ncases from the Federal Supreme Court of Switzerland (FSCS). Among them, there \nis also a hierarchical version of BERT similar to (Chalkidis et  al. 2019a), where \na BERT (monolingual or multilingual) encoder produces fact embeddings that are \nused as input for a BiLSTM to get the document representation. Another variant of \nBERT (monolingual or multilingual) proposed in (Niklaus et al. 2021) to deal with \nlong document is Long BERT, where a maximum length of 2048 tokens is reached \nreplicating four times the original 512 positional encodings. Results unveil that \n975\n1 3\nTransformer-based language models for AI and law\nTable 7  Summary of main methods discussed in Sects. 4.6 and 4.7\nMethod Ref. Downstream tasks Lang. Data Long docs?\nFine-tuned Transformer and Trans-\nformerAAN\n Feijó and Moreira (2019) AS BR Legal judgments Yes\nFine-tuned BERTimbau  Lage-Freitas et al. (2022) Case law classification BR Legal judgments No\nFine-tuned BERTimbau  Aguiar et al. (2021) Lawsuit classification BR Petitions, indictments No\nFine-tuned m-BERT and BERTimbau  Serras and Finger (2022) Case law classification BR Summaries, headers Yes\nFine-tuned Transformer + GCN \n(R-former)\n Dong and Niu (2021) LJP CN Legal cases No\nBERT + BiLSTM + CRF  Sun et al. (2021) NER CN Legal cases No\nCEEN (BERT as alternative to BiL-\nSTM module)\n Lyu et al. (2022) LJP CN Legal cases No\nFurther pre-trained LegalBERT@\nOpenClap+attention (EPM)\n Feng et al. (2022) LJP CN Legal cases No\nInverse optimal transport-based ration-\nale extraction approach + fine-tuned \nChinese T5-PEGASUS (IOT-Match)\n Yu et al. (2022b) Case matching prediction CN Legal cases Yes\nFine-tuned BERT + BiLSTM +CRF; \nUniLM (CLASS)\n Li et al. (2021a) RRL; AS CN Legal cases Yes\nFine-tuned Transformer and BERT + \nGraph Transformer\n Huang et al. (2021) AS CN Legal opinion news Yes\nPre-trained Longformer (Lawformer)  Xiao et al. (2021) LJP; CR; RC; QA CN CAIL-Long; LeCaRD; CJRC; \nJEC-QA\nYes\nFine-tuned FlauBERT  Salaün et al. (2020) Lawsuit classification FR Legal cases No\nFrom-scratch pre-trained BERT (JuriB-\nERT); Further pre-trained Camem-\nBERT (JuriBERT-FP)\n Douka et al. (2021) Pleading classification FR Legal cases No\nFurther pre-trained BARThez \n(CriminelBART)\n Garneau et al. (2021) Charge prediction (Cloze tests) FR Legal cases No\n976 C. M. Greco, A. Tagarelli \n1 3\nTable 7  (continued)\nMethod Ref. Downstream tasks Lang. Data Long docs?\nZero-shot and fine-tuned CamemBERT  Louis and Spanakis (2022) SAR FR Federal/regional codes Yes\nFurther pre-trained \nCamemBERT+Transformer+graph \nmodel (G-DSR)\n Louis et al. (2023) SAR FR Federal/regional codes Yes\nFine-tuned GBERT and GELECTRA  Wrzalik and Krechel (2021) Case law classification DE Legal cases No\nGermanBERT and DistilBERT  Tang and Clematide (2021) Document retrieval DE Articles and legal cases No\nFine-tuned m-BERT, XLM-RoBERTa, \nGreek-BERT, and Greek-Legal-\nBERT\n Papaloukas et al. (2021) Article classification GR Civil code No\nFine-tuned m-BERT  Tarasconi et al. (2020) NER; TC IT Legal cases No\nFine-tuned BERT (LamBERTa)  Tagarelli and Simeri (2022) SAR IT Civil code No\nFurther pre-trained BERT (ITALIAN-\nLEGAL-BERT)\n Licari and Comandè (2022) NER, SC, SS IT Legal cases No\nFine-tuned KoBERT and KoBART  Yoon et al. (2022) AS KR Legal cases No\nFrom-scratch pre-trained BERT  Masala et al. (2021) Case law classification RO Legal cases No\nFrom-scratch pre-trained RoBERTa  Gutiérrez-Fandiño et al. \n(2021a)\nNER; TC ES Various legal texts No\nBERT and m-BERT  Aydemir et al. (2020) COLIEE-2020 Tasks 3-4 EN, JP Civil code No\nFine-tuned hierarchical BERT + BiL-\nSTM; BERT (Long BERT)\n Niklaus et al. (2021) LJP IT, DE, FR Legal cases Yes\nFine-tuned monolingual German-\nBERT, Camembert, UmBERTo Fine-\ntuned multi-lingual XLM-RoBERTa\n Niklaus et al. (2022) LJP IT, DE, FR Legal cases Yes\nFine-tuned m-BERT and m-Distil-\nBERT\n Shaheen et al. (2021) TC EN, DE, FR JRC-Acquis; EURLEX57K No\n977\n1 3\nTransformer-based language models for AI and law\nTable 7  (continued)\nMethod Ref. Downstream tasks Lang. Data Long docs?\nFine-tuned Legal-BERT@aueb, \nm-BERT, WikiBERT, BERT\n Avram et al. (2021) TC Various EU lang. JRC-Acquis; OPOCE No\nFine-tuned m-BERT- and DistilBERT-\nbased ParaLaw Nets\n Nguyen et al. (2021b) COLIEE-2021 Task 5 EN, JP Civil code No\nzero-shot mGPT, GPT-J-6B, GPT-\nNeoX-20B\n Trautmann et al. (2022) LJP EN, IT, DE, FR Legal cases Yes\n978 C. M. Greco, A. Tagarelli \n1 3\nmonolingual models generally outperform the multilingual counterparts, with the \noverall best results obtained by hierarchical BERT.\nNiklaus et  al. (2022) evaluate cross-lingual, cross-domain (i.e., cross-legal \nareas), cross-regional and cross-jurisdiction transfer learning of several TLMs on \nthe LJP task. An augmented version of the SJP corpus (cf. Sect.  3.4) is provided. \nSince most SJP documents are around 2048 tokens, Hierarchical BERT models \n(Niklaus et al. 2021; Chalkidis et al. 2019a) are used to encode up to 2048 tokens \nfor each document. For the cross-lingual transfer learning, monolingual BERT-\nbased models (German-BERT, Camembert, and UmBERTo), and the multi-lin-\ngual XLM-RoBERTa are compared on three scenarios: fine-tuning the models for \na specific language (i.e., monolingual fine-tuning), fine-tuning the models across \nlanguages (i.e., cross-lingual fine-tuning), and fine-tuning across languages but \nexcluding the target language (i.e., zero-shot cross-lingual fine-tuning). For the first \nand the second scenarios, two versions of the training set are used, one containing \nonly the documents in the original language contained in the SJP corpus and the \nother including also the machine-translated versions of the documents. Concerning \nthe monolingual fine-tuning of monolingual models and XLM-RoBERTa, results \nhave shown that monolingual models obtain better performance w.r.t. XLM-RoB-\nERTa, with even better results than (Niklaus et al. 2021). Moreover, the augmen-\ntation of the data through machine-translation seems to further improve the per -\nformance. For cross-lingual fine-tuning, a standard fine-tuning of XLM-RoBERTa \nis compared with an adapter-based fine-tuning of the model (Houlsby et al. 2019; \nPfeiffer et al. 2020), which consists on the addition of adapter layers to the model \nand training them along with the parameters of the normalization layers. Results \nunveil the beneficial effect of including adapters in XLM-RoBERTa, regardless of \nthe addition of machine-translated documents, and adapters also improve perfor -\nmance for XLM-RoBERTa fine-tuned with zero-shot cross-lingual fine-tuning. As \nregards cross-regional transfer evaluation, the SJP documents are divided w.r.t. sev -\neral regions, then XLM-RoBERTa is fine-tuned in three settings: fine-tuning w.r.t. \na specific region with data augmentation, fine-tuning across all the regions with-\nout machine-translated data augmentation and fine-tuning across all the regions \nwith machine-translated data augmentation. Results show that in most cases a \nmodel fine-tuned on the same region of the target is outperformed by zero-shot \nmodels (i.e., models fine-tuned on another region). However, cross-regional mod-\nels obtain better results than regional-specific models, with adapter-based mod-\nels obtaining top results in most cases. As regards cross-domain transfer learning \nevaluation, fine-tuning is conducted over three settings: domain-specific fine-tun-\ning with data augmentation, cross-domain fine-tuning without data augmentation \nand cross-domain fine-tuning with data augmentation. Results demonstrate that \ncross-domain models outperform domain-specific models in most cases. Unlike the \ncross-regional transfer analysis, models fine-tuned on the same domain of the target \noutperform the zero-shot models. Finally, Niklaus et  al. (2022) perform a cross-\njurisdiction transfer learning evaluation, by adding to the SJP corpus, concerning \nthe Swiss laws, the ILDC corpus, which contains Indian laws. They consider two \nfine-tuning scenarios: fine-tuning XLM-RoBERTa on only the machine-translated \n979\n1 3\nTransformer-based language models for AI and law\nIndian cases (zero-shot fine-tuning) and fine-tuning the models with original SJP \ntraining set, machine-translated SJP cases and machine-translated ILDC cases (fur -\nther augmented fine-tuning). Results show that zero-shot models perform poorly, \nbut the further-augmented models outperform the results of the cross-lingual mod-\nels evaluated in the cross-lingual transfer learning analysis. Again, adapter-based \nmodels obtain highest performance in most cases.\nShaheen et  al. (2021) evaluate m-BERT and m-DistilBERT in large-scale \nmulti-label text classification task using a zero-shot cross-lingual transfer learn-\ning scheme and a joint learning scheme. In the first case, models are built using \nan English training set although performance is tested on French and German test \nsets. In the second case, models are trained using all the three languages. To this \npurpose, the multilingual version of JRC-Acquis (Steinberger et al. 2006) and an \nextended version of EURLEX57K (Chalkidis et al. 2019b) are exploited. From the \nexperimental results, m-BERT outperforms m-DistilBERT in zero-shot transfer \nlearning scheme in both datasets. As regards the joint learning scheme, m-BERT \nand m-DistilBERT are compared with monolingual BERT-based methods using \nthe English test set. The multilingual models obtain about 96.83 −98.39% of the \nperformances of monolingual models in JRC-Acquis, while in EURLEX57K better \nresults are obtained by m-BERT. Comparing the performance in zero-shot and joint \nlearning scheme, m-BERT (resp. m-DistilBERT) in zero-shot scheme achieves \nabout 86% (resp.79%) of the performance in the joint learning scheme on French \nand German test sets. The zero-shot results indicate that multilingual models are \nable to achieve transfer learning from English to French language more easily than \nfrom English to German.\nAvram et  al. (2021) propose a tool for multi-label classification of legal docu-\nments on 22 languages. For each language, BERT-based methods are fine-tuned \nusing the JRC-Acquis (Steinberger et  al. 2006) and the Publications Office of the \nEuropean Union (OPOCE) corpora, manually labelled with almost 7K EuroVoc 140 \ndescriptors. The choice of BERT models for each language is conducted prioritising \npre-trained models on legal domain corpora, otherwise according to the following \norder: models pre-trained on a specific language, models pre-trained on monolingual \nWikipedia, multilingual models. As a result, Legal-BERT (Chalkidis et al. 2020b) \nis used for the English language, m-BERT (Devlin et al. 2019) for the Maltese lan-\nguage, WikiBERT models (Pyysalo et al. 2021) for Bulgarian, Lithuanian, Latvian, \nSlovak and Slovene, and monolingual BERT models for the remaining 14 languages. \nFrom the evaluation results, m-BERT obtain the worst performance, probably due \nto the low number of Maltese documents as well as due to the use of a multilin-\ngual model. In contrast, it is interesting that WikiBERT models obtain satisfactory \nscores, even better than Legal-BERT.\nNguyen et al. (2021a) introduce ParaLaw Nets (Nguyen et al. 2021b) to address \nthe COLIEE-2021 Task 5. This is a family of pre-trained models that rely on sen-\ntence-level translation information to reduce language ambiguity and increase per -\nformance in legal tasks. The core idea in these models is that, in the translation \n140 https:// data. europa. eu/ data/ datas ets/ eurov oc.\n980 C. M. Greco, A. Tagarelli \n1 3\nprocess, the most correct meaning of a sentence will be captured by the model. To \nthis end, the model is pre-trained on sentence-level cross-lingual tasks, while the \nCOLIEE task is used in the fine-tuning phase. According to the type of pre-training \ntask, ParaLaw Nets are divided in NFSP (Next Foreign Sentence Prediction) and \nNMSP (Neighbor Multilingual Sentence Prediction) models. The former formulates \nthe pre-training task as binary classification, the latter as multi-label classification. \nThe pre-training input consists of sentence pairs, where a sentence can be in its orig-\ninal language or in the target language. As previously described in Sect.  4.2.2, the \nNFSP task is basically a Next Sentence Prediction task where the input is composed \nof sentences pairs expressed in different languages. In the NMSP models, training \ndata also includes sentence pairs with the same language. The labels in the NMSP \ntask correspond to four training data generation schemes, namely random sampling, \nnormal order, reverse order, and non-contiguous. In both tasks, m-BERT and Dis-\ntilBERT’s architectures are used respectively as the base and the distilled version \nfor ParaLaw Nets. The models are trained with Japanese-English legal data, but the \napproach can be generalized to all language pairs. The data used to pre-train Par -\naLaw Nets is provided by Japanese Law Translation website. 141 The data used in \nthe fine-tuning phase come from the Japanese Civil Code and COLIEE. The authors \nuse also augmentation strategies in the fine-tuning phase, which mainly consist in \nthe statement negation of the original sentences. From the competition results, it is \nobserved that the base model using NFSP obtains the highest score, while the base \nmodel using NMSP reaches the third position, proving the effectiveness in exploit-\ning cross-lingual information.\nTrautmann et al. (2022) explore the potential of legal prompt engineering on zero-\nshot GPT-based models for multilingual LJP. In particular, they evaluate mGPT, \nGPT-J-6B,142 i.e., a 6 billion parameter version of GPT trained on the Pile dataset \n(Gao et al. 2021), and GPT-NeoX-20B on the ECHR and FSCS datasets. A prompt \ntemplate is defined by mapping the LJP task in a question-answer form, where the \nquestion requests to detect whether or not there are violated articles in the docu-\nment. Results have shown that the proposed prompting methods outperform simple \nunsupervised baselines, but perform worse than supervised (task-adapted) models, \nthus leaving much room for improvement especially in terms of defining templates \nsuitable to different corpora and languages (as well as dealing with long documents \nwithout truncating at 2048 tokens).\n4.8  Dealing with explainability and interpretability issues\nAs for any sophisticated neural network models, the TLMs ’ behavior at both learn-\ning and inference phases is not straightforward to fully understand. This clearly may \nlimit their applicability, or even increase fear in legal actors or general public that \nwho may want to use such AI-based tools, especially for high societal impact fields \n141 https:// www. japan esela wtran slati on. go. jp/.\n142 https:// github. com/ kingo flolz/ mesh- trans former- jax.\n981\n1 3\nTransformer-based language models for AI and law\nlike law. It is therefore demanding to resort to techniques that can provide explain-\nable justification for the models’ decisions, or that can make their prediction out-\ncomes interpretable.\nPost-hoc explanation. One approach is to infer post-hoc explanations. Within \nthis view, some works such as (Savelka and Ashley 2021) and (Tagarelli and Simeri \n2022) provide insights into the informativeness of the attention weights produced \nby a BERT model for selected use cases, through the lens of the interactive visuali-\nzation tool bertviz (Vig 2019). 143 The general objective is to inspect the formation \nof complex inter-token relationships and the corresponding distinctive attention pat-\nterns that most influence the final representation of a given sentence in input and, \ntherefore, such as to justify the model’s outcome for that input. A different perspec-\ntive is taken in (Simeri and Tagarelli 2023), where the explanation is accomplished \nby approximating a BERT-based classifier locally by an interpretable linear model, \nby means of a technique known as LIME—Local Interpretable Model-Agnostic \nExplanations  (Ribeiro et  al. 2016). For a given input sentence, LIME explains a \nclassifier’s behavior “around” that query instance, by weighing perturbed versions of \nthe input by their proximity to the original query instance, then observing the asso-\nciated predictions by the underlying classifier to determine which of those changes \nwill have most impact on the prediction of the original query.\nWehnert et al. (2022) address interpretability using KERMIT to encode symbolic \nsyntactic parse trees of queries and articles in addition to BERT representation of \ninput sentence, thus injecting further linguistic knowledge. KERMIT is specifically \ndesigned to include syntactic interpretations in deep neural networks. The KERMIT-\nviz architecture (Ranaldi et al. 2021) enables the visualization of which part of the \nsentence is used during the inference step.\nMalik et  al. (2021) evaluate a number of explainability methods as a post-pre-\ndiction step of the case decision prediction task. To this regard, legal experts are \nasked to mark the sentences they consider as explanations for the judgments (from \na portion of the ILDC test set) and assign each explanation sentence with a score \nreflecting the importance of the explanation for the judgment. A hierarchical system \ncomposed of XLNet and BiGRU on top is chosen as the base model on which to \nexperiment an explainability method based on occlusion and inspired from (Zeiler \nand Fergus 2014) and (Li et al. 2016). Documents are divided into chunks of 512 \ntokens, each chunk embedding is masked at a time, then the masked input is given \nto the BiGRU component of the model. A chunk explainability score is computed \nas the difference between the output probabilities of the prediction calculated on the \nmasked input and the unmasked input. The sentences that explain the final decision \ncorrespond to the chunks with positive scores. Similarly, each sentence of selected \nchunks is masked at a time and supplied to the XLNet component of the model. The \ndifference between the logits calculated on the masked input and the original logits \nof the prediction represent the explanation score, so that the top-k sentences for each \nchunk are selected as explanations. The analysis of the explanations given by the \nocclusion method indicate that most of the relevant information for the judgment \n143 https:// github. com/ jesse vig/ bertv iz.\n982 C. M. Greco, A. Tagarelli \n1 3\nis located at the end of the document; however, explanations selected by the occlu-\nsion method are shown to be significantly different from explanations given by legal \nexperts.\nEarly explanation. A different approach is to produce explanations during the \ndata modeling or learning process. Liu et  al. (2021a) consider the interrelation \nbetween charges and the court view sections in legal documents. The court view is \nregarded as charge explanation since it contains supporting information for a charge \nand is charge-discriminative (i.e., strongly charge-dependent). Within this view, the \nJPGM method is introduced to jointly predict charges based on fact descriptions and \ngenerate court views. The key idea is to predict a group of similar charges that may \nlead to confusion. To this purpose, charge-discriminative keywords are defined for \neach charge, then an attention mechanism is involved to select the best matching \nkeywords for a charge predicted by a classifier, finally the generation module pro-\nvides the court view based on the fact description and the best keywords. The gener-\nated court view and fact description are also fused to refine the previous classifier \nprediction. JPGM has been shown to perform better than baselines based on CNN, \nGRU, vanilla Transformer, and graph neural networks.\nIn the context of alleged violation prediction, Chalkidis et  al. (2021c) propose \nto regularize the extraction of rationales, as the paragraphs of the input that sup-\nport the decisions, by constraints that reward the model if its decisions are based \non concise rationales. The rationale constraints include sparsity, continuity, compre-\nhensiveness and singularity. The first two constraints encourage to select a small \nnumber of paragraphs that sufficiently justify the allegation and to prefer contiguous \nparagraphs, respectively. The comprehensiveness constraint requires to use a mask \nto ideally contain all the paragraphs that support the correct decision, whereas the \nreverse mask should contain the irrelevant paragraphs; this way, the output of the \nmodel based on the mask (i.e., the estimated probabilities on the allegations) should \nbe better than the output of the model based on the reverse mask. Moreover, the \nsingularity constraint requires that the selected mask should be not only better than \nits reverse version, but also w.r.t. any other mask. The effect of such constraints are \nevaluated w.r.t. faithfulness and rationale quality, given a sparsity threshold, where \nthe latter is calculated comparing the predicted rationales with gold annotations, and \nfaithfulness measures how much the rationales reflect the reasoning of the model, by \ncomputing the difference between the predicted probabilities obtained by the model \nover the whole text in input and the predicted probabilities obtained considering \nonly the (complement of) extracted rationales. The singularity constraint is shown to \nimprove faithfulness and rationale quality in relation to both silver and gold ration-\nales; by contrast, continuity appears not to be beneficial for the task at hand.\nSantosh et al. (2022) start from the observation that LJP models on ECtHR cases \ntend to be confused by distracting factors in the text that might originate from the \ncorpus construction, the case distribution, or spurious correlations with the outcome. \nIn this regard, an expert-informed deconfounding method based on adversarial train-\ning is introduced to prevent a model from being influenced by distractors recognized \nby ECtHR experts. To this purpose, Santosh et al. (2022) train and evaluate BERT \nmodels on LexGLUE (ECtHR Tasks A and B) and ECHR benchmarks; in particular, \na BERT variant of hierarchical attention networks (Yang et al. 2016) is chosen as \n983\n1 3\nTransformer-based language models for AI and law\nbase model, where input texts are segmented with a greedy sentence packing strat-\negy and encoded using Legal-BERT@aueb.\nTo evaluate the alignment with expert rationales, the expert relevance assess-\nments provided for ECtHR by Chalkidis et al. (2021c) are used. The ability of the \nmodel to identify correct rationales at the paragraph level is evaluated through an \ninterpretability technique that determines the importance score for each paragraph \nby measuring the impact of a particular input token on the final prediction. A token-\nlevel focus score is calculated using the integrated gradients (Sundararajan et  al. \n2017), then paragraph-level scores are obtained aggregating the token scores in the \nparagraph. The top-k paragraphs according to the paragraph scores are compared \nwith the golden paragraph rationales. Results have demonstrated that the decon-\nfounding process effectively improves the model alignment with expert rationales \nin ECHR benchmarks; however, such improvements are marginal and there is still \na large gap between paragraphs suggested by models and what legal experts have \nannotated as relevant.\nTo address a legal case matching task, Yu et al. (2022b) emphasize the impor -\ntance of taking into account the difference between the roles of sentences corre-\nsponding to rationales and other sentences in a legal case, as well as distinguish-\ning rationales that are in favor or against the matching decision. Each sentence of \na given pair of legal cases is assigned one of the following labels: not a rationale, a \nkey circumstance, a constitutive element of a crime or a focus of disputes. The goal \nis to extract aligned and misaligned rationales, to assign a matching label for the \nlegal case pair (not matching, partially matching or matching) and to provide the \nset of sentences explaining the reasons for the label. The problem to extract aligned \nand misaligned rationales is formulated as an optimal transport problem, where the \nprobability of the cross-sentence coherency (pro and con rationale pairs) is com-\nputed to provide evidence for the matching. The problem is guided by an affinity \nmatrix, which reflects both semantics and legal feature relations between cross-case \nsentences. The affinity matrix and the optimal transport, based on both sentence \nembeddings and a manually-labeled (noisy) alignment matrix, are learned by the \ninverse optimal transport (IOT) process. The IOT process consists of solving a bi-\nlevel optimization problem, in which the affinity matrix is the upper-level variable \nand the optimal transport is the lower-level variable. The optimal transport is then \nfitted to the alignment matrix and used to help the generation of the explanations. \nTo this purpose, a Chinese T5-PEGASUS (Su 2021) model for each matching label \nis fine-tuned to get the label-specific candidate explanations. Finally, the model per -\nforms the matching prediction based only on the extracted rationales and the label-\nspecific explanations. In this way, the noisy sentences of the legal cases pair are fil-\ntered out and not involved in the matching prediction, reducing the number of input \nsentences to be processed. The proposed IOT-Match model is fine-tuned and tested \non eCAIL and ELAM datasets against legal case matching competitors. The quality \nof the extracted rationales and the generated explanations is verified by conduct-\ning an empirical analysis on faithfulness (described above) and plausibility, which \nmeasures how the model explanations are convincing to humans. In particular, to \nassess the plausibility of rationales, resp. explanations, the ones produced by the \nIOT-Match and competitors have been compared with those generated by humans, \n984 C. M. Greco, A. Tagarelli \n1 3\nunveiling that the rationales, resp. explanations, by IOT-Match are more consistent \nwith human annotations. Moreover, as concerns the faithfulness of rationales, IOT-\nMatch has been evaluated with and without rationales, eventually finding that the \nextracted rationales play an important role for the task of legal case matching.\nAs previously described in Sect.  4.7, Feng et al. (2022) propose an event extrac-\ntion process from the fact description of criminal cases to constrain the prediction \nof LJP models on CAIL2018. A hierarchy is defined for the events of legal cases, \nbased on Chinese law articles. Events are detected from specific triggers, i.e., words \nexpressing the occurrence of the event. Each event has a role in the fact description, \nand the extracted events and constraints on such events are used to help the learning \nprocess of the EPM model for the LJP task. The context representation of the fact \ndescription is used to query all law article candidates and to determine the most rel-\nevant semantics in the articles. The context representation of the fact description and \nthe most article relevant semantics are given as input for the CAIL sub-tasks. The \nbase model is augmented with a hierarchical event extraction layer to identify event \ntriggers and roles while jointly performing the LJP task. The judgment prediction is \nthus inferred through detected event features. The model is guided by event-based \nconstraints, in order to search for a single trigger and its related roles, and cross-task \nconsistency constraints, in order to take into account dependencies between the sub-\ntasks, since each article establishes the charges and the range of penalty terms.\n5  Discussion\nIn this survey we investigated the use of Transformer-based language models \n(TLMs) for legal AI-based tasks. We conducted a detailed study of the approaches \nproposed in the literature in this area, and we categorized the problems that received \na particular attention from the scientific community into three macro categories, \nnamely legal search, legal document review, and legal outcome prediction; we \nnoticed, however, that such categories are apparently interleaved and interrelated, \nand their definition is mainly intended for the sake of presentation. Retrieval, entail-\nment and question answering have traditionally been the most frequently considered \nlegal problems, also due to the role played by the COLIEE competition which has \nrepresented an important venue to foster the development of AI-based approaches \nin the legal domain, including those relying on TLMs. But equally important are a \nnumber of other tasks, ranging from named entity recognition to judgment predic-\ntion, from abstractive/extractive summarization to rhetorical role labeling. Yet, the \nvarious tasks are often interrelated, since a common approach to address them is \nto reformulate a legal-specific task in terms of a more general, machine-learning \ntask, mainly focusing on classification and similarity. It is worth noticing, however, \nthat legal text similarity and classification can be challenging for many reasons. \nFor instance, there might be multiple classes to be assigned with a single case, as a \ncase can span multiple areas of law. Moreover, the categorization of a case can vary \ndepending on the particular court handling it. Yet, there is no universally agreed-\nupon set of areas of law clearly defined into a taxonomy (Mistica et al. 2021).\n985\n1 3\nTransformer-based language models for AI and law\nTLM impact on legal tasks. By examining the literature, there is evidence of how \nTLMs have been able to push forward the state-of-the-art in a variety of tasks for \nthe legal domain. Again, a representative case corresponds to the COLIEE compe-\ntition, where we notice a growing and successful use of TLMs. Nonetheless, the \ncompetition as well as other venues have shown that non-TLM methods, including \nthe traditional vectorial space models (e.g., TF-IDF or BM25), are still useful espe-\ncially when combined with TLMs, particularly as a data pre-processing or filtering \nstep. This generally serves a twofold purpose: to exploit both the lexical modeling \nof traditional techniques and the semantic knowledge of TLMs (e.g., (Nguyen et al. \n2021a)) and to perform an initial selection of possible candidates through the rank -\ning score of traditional techniques and then re-ranking such results through TLMs \n(e.g., (Althammer et al. 2021)).\nIt does not come to surprise that, being the first and most popular among the \nTLMs, BERT is widely involved in the existing approaches, often along with its \nearly variants, such as RoBERTa, DistilBERT, DeBERTa. Yet, recent works tend \nto consider more advanced architectures, depending on the downstream task, which \nhave shown to be very competitive, such as ELECTRA and XLM-RoBERTa as \nencoder-only models, XLNet and the GPT family of models as decoder-only mod-\nels, T5 and BART as encoder–decoder, along with task-specific and long range \nmodels such as Longformer, BigBird, DPR and PEGASUS.\nMoreover, a key aspect that determines the effectiveness of a TLM-based frame-\nwork compared to other deep learning approaches is the type of training adopted \nto build the models. In the legal domain, while most studies have focused on task-\nadaptive fine-tuning, there has also been an increased interest in performing domain \nadaptation through further pre-training or pre-training from scratch a TLM, again \nstarting from BERT models (e.g., (Chalkidis et al. 2020b; Holzenberger et al. 2020; \nZheng et  al. 2021)). Song et  al. (2022) carry out an empirical evaluation on the \neffectiveness of domain-specific pre-training for the legal domain, on a number of \ndatasets and tasks including binary classification, multi-label classification, multi-\nple choice question answering, summarization, and information retrieval. Results \nhave shown that domain-specific pre-trained models can lead to 1–5% higher per -\nformance than general domain pre-trained models, but on condition that the datasets \ninvolved are very close to the pre-training corpora, thus concerning the same legal \nsub-domain.\nRegardless of the type and size of TLM and its training, the current trend for best-\nperforming frameworks is to exploit techniques that are extrinsic to the particular \nlanguage model, such as data augmentation, data enrichment, and ensemble strat-\negies. Data augmentation is performed in several ways, for example through back \ntranslation (e.g., (Rabelo et al. 2020)), focusing on the logical mismatches between \narticles and questions (e.g., (Yoshioka et al. 2021a)), negating the statements in the \noriginal sentences (e.g., (Nguyen et al. 2021b)), or retrieving top-k irrelevant articles \nfor a query according to a similarity score (e.g., (Wehnert et al. 2021)). In the lat-\nest editions of COLIEE, several works use previously released data to obtain more \ntraining samples for the tasks at hand. Also, a few works have adopted ensemble \nstrategies, which combine the results of independent systems to boost the overall \nperformance; for example, the ensemble system of Nguyen et al. (2021a) reached \n986 C. M. Greco, A. Tagarelli \n1 3\nthe second position in the COLIEE-2021 Task 3, the one by Shao et  al. (2020a) \nranked first in the COLIEE-2020 Task 3, as well as the ensemble method proposed \nin (Rosa et al. 2021) achieved the first position in the COLIEE-2021 Task 2. Data \nenrichment is typically obtained through the use of taxonomies (Tziafas et al. 2021; \nChalkidis et al. 2021a) and thesauri (Kim et al. 2021).\nResource availability on the benchmarks. The need to devise the above mentioned \nstrategies for improving the performance of a model also arises from the awareness \nthat free-access legal resources are often limited or partially available. According to \nSong et al. (2022), one of the most significant challenges in legal NLP is the lack of \nlarge-scale high-quality datasets, due to the costs of the annotation processes that \nrequire knowledge of the legal domain. In effect, although there exists a significant \nbody of legal corpora that have been used for training and evaluation of TLMs for \nlegal tasks, as we have analyzed in Sect.  3.4, the current landscape of legal bench-\nmarks is still not comparable in size with the largest NLP and information retrieval \ndatasets; in fact, apart from few exceptions corresponding to multi-task benchmarks \n(e.g., LexGLUE), most legal benchmarks and datasets have size of thousands or tens \nof thousands of documents. Moreover, we believe that attention should be paid to \nthe maintenance of existing benchmarks, so as to keep them updated on important \nchanges in the legislative status of a jurisdiction. Yet, there is room for building \nlarger or new freely available benchmarks to evaluate legal tasks in new contexts \n(e.g., online social media companies and online trading companies involved in the \nWeb3), as well as contexts that require controlling specific types of bias and/or spe-\ncific ethical aspects concerning the diversity or disparity of legal norms within and \nacross different countries.\nCreating larger and more representative legal corpora depends on a number of \naspects that include not only those strictly pertaining the selected evaluation goals \nfor a particular task, but also factors relating to the language resources to be involved \n(e.g., open/free access availability, various forms of bias), the target audience with \ntheir different levels of expertise on the domain (e.g., lawyers, courts, law firms, \ncitizens), as well as ethics-related and privacy-related aspects to be considered, to \nmention a few.\nResource availability on the model training. The limited availability of legal data \nfor a particular task might prevent an adequate training of the model, thus making \nit unable to properly internalize the meaning of legal texts and to generalize the \nknowledge learned in sufficient detail for successfully dealing with unknown input. \nIndeed, it has been shown that it is not guaranteed that a model pre-trained on a \nlegal corpus can significantly improve upon its corresponding general-domain pre-\ntrained model fine-tuned on the target task, in all situations. Besides the previously \nmentioned (Song et al. 2022) about the affinity between the data for the downstream \ntask and the pre-training data, according to  Wang et  al. (2020a), the advantage \nof domain-specific pre-training is also related to the size of the data used for the \ndownstream task: that is, the benefit gained through domain-adaptive pre-training \nis likely to be more significant when the downstream task appears to be more low-\nresource. This is also confirmed in (Gururangan et al. 2020), where another form of \n987\n1 3\nTransformer-based language models for AI and law\npre-training is highlighted and named as task-adaptive pre-training, i.e., (unsuper -\nvised) training the model on a smaller but directly task-relevant corpus. This form of \npre-training has shown to be not only competitive w.r.t. domain-adaptive pre-train-\ning but also beneficial when combined with it for improving the performance on the \ndownstream task. However, the above findings were not proved for the legal domain, \nwhich certainly opens to opportunities for further research.\nIn addition, by also taking into account models’ implementation technicalities, \nother difficulties can be identified. One is the imbalance between positive and nega-\ntive examples, for which solutions include the filtering of possible candidates (e.g., \n(Nguyen et al. 2021a)), oversampling methods (e.g., (Shao et al. 2020a)), and the \ngeneration of artificial examples (e.g., (Rosa et al. 2021)).\nResource availability on the jurisdiction language. Data availability issues are \neven more exacerbated when moving from a high-resource language, such as Eng-\nlish, to low-resource languages. Several works have indeed been concerned with \nthe legal domain for poor-resource language families, such as Romance, Slavic, \nGermanic, Uralic, but also oriental languages such as Chinese and Japanese. For \nsuch languages, the use of TLMs has enabled a breakthrough in many cases. For \nexample, in (Serras and Finger 2022), the TLM-based method improves upon sta-\ntistical baselines; in (Sun et al. 2021), it reaches leading positions; in (Masala et al. \n2021), all the baselines are consistently outperformed; in (Papaloukas et al. 2021), \nTLMs prove to be superior to vector-space-model based learning techniques and \nRNN-based methods, in particular when they are domain-adapted. However, for \nlow-resource languages, TLMs can perform worse than others (e.g., (Lage-Freitas \net al. 2022)), or large TLMs may suffer in performance in contrast to smaller coun-\nterparts (e.g., (Douka et al. 2021)). In some cases, when benchmarks are very lim-\nited or even absent, an evaluation on the specific domain is not practicable (e.g., \n(Gutiérrez-Fandiño et al. 2021a)). One common approach to overcome this issue is \nto develop multilingual and cross-lingual language models, under the assumption \nthat low-resource languages can benefit from high-resource languages due to shared \nvocabulary and semantic relatedness aspect. Although there is evidence that the use \nof multilingual TLMs has helped to improve the state-of-the-art, there is certainly \nroom for improvement (e.g., in (Avram et al. 2021)).\nLinguistic issues. Even more challenging is developing a model that can incor -\nporate the many linguistic nuances and subtleties of legal documents already in \nthe early stages of its construction, such as during the tokenization or masking \nprocesses. This appears to be critical especially for the case law data: in fact, \nwhile law statutes and articles are usually written in a language that should be \nas much understandable as possible to non-experts as well, legal cases and judg-\nments can be particularly tricky to understand, even for humans. A related issue \nis the language mismatch between statutes, describing legal concerns in form of \nabstraction, and law cases, describing real facts; for instance, Savelka and Ash-\nley (2022) focus on the interpretation of statutory terms, by investigating on how \na particular term has been explained and applied in the past, thus allowing for \nthe lawyers the construction of arguments which support or counter particular \n988 C. M. Greco, A. Tagarelli \n1 3\ninterpretations. This is a complex scenario, since there may be little lexical over -\nlap between statutes and cases, thus making it more difficult to understand the \nlegal text and extract key concepts from the case law to be retrieved in the statutes. \nIn this regard, the quality of vocabulary can play an important role. The legal lan-\nguage is full of specific terms with a precise meaning, which are in many cases not \nincluded in general-purpose vocabularies. Few works address this issue by inject-\ning legal words in the vocabulary (Tagarelli and Simeri 2022; Simeri and Tagarelli \n2023), in order to prevent that the TLM tokenizer will break out out-of-vocabulary \nterms that are found in a target legal corpus. Clearly, an enriched vocabulary can-\nnot be enough for a model to deeply learn syntactic, lexical, and semantic patterns \nof the legal language. More specifically, modeling syntactic patterns as predicates, \nlegal reasoning methodologies can provide a significant support. Legal reasoning \nis in fact one of the primary challenges identified in (Zhong et al. 2020), since it \nshould in principle adhere to well-defined rules.\nIn addition to all these aspects that are pertinent to the legal domain, there are \nalso general language peculiarities that still pose challenges for TLMs, such as rec-\nognition of pronominal forms, anaphora-related issues, understanding various forms \nwith negations, etc. An example is how to distinguish texts that look very similar to \neach other but actually have a logical or semantic mismatch (Yoshioka et al. 2021a).\nDocument length. Compared to the earliest approaches, whereby the limitation on \nthe maximum number of token conditioned the ability of TLMs in handling long \ndocuments, the current trend is to learn a more conservative, “lossless” model, \nwhich can elaborate a text at paragraph-level (e.g., considering embeddings of entire \nparagraphs rather than individual words, as in (Shao et  al. 2020c)), or that it can \ndirectly process longer documents, such as Longformer (e.g., (Xiao et al. 2021)), or \nDPR (e.g., (Althammer et al. 2021)). Another option can be to filter out noisy sen-\ntences with sentence extraction techniques (Yu et al. 2022b) or to perform a span-\nlevel approach (Koreeda and Manning 2021), in which the document is divided into \noverlapping contexts containing spans. On the other hand, a different perspective is \nto incorporate a summarization step into the language understanding process with \nthe aim of providing a concise meaningful version of the input text. Some of the \ndiscussed works indeed involve a summarization task (e.g., (Alberts et  al. 2020; \nRossi and Kanoulas 2019; Kim et  al. 2021)), others increase the maximum input \nlength (e.g., (Mamakas et  al. 2022)) or apply hierarchical attention patterns (e.g., \n(Chalkidis et al. 2022a)), but TLMs have surely the potential to be further developed \nfor achieving enhanced performance.\nHowever, to significantly improve their ability related to long range modeling, \nTLMs should cope with larger computational resources to process, model, store, and \nmanipulate long passages of text, which also impact on advanced ability to reason \ncoherently across long-range dependencies. Addressing these challenges will likely \ninvolve advancements in model architecture, training methodologies, memory man-\nagement, and computational efficiency.\nStructures at document-level and corpus-level. Another important aspect is to \nexploit explicit structure levels within and/or across legal documents. For instance, \n989\n1 3\nTransformer-based language models for AI and law\nlegal source citations within the text of articles or cases could be managed by replac-\ning the citation with the cited article, as in (Rabelo et al. 2020), although a much \nmore promising approach would be to model and mine legal citation networks. As \ndiscussed in (Locke and Zuccon 2022), citations play a very important role in legal \ndecisions: since the decisions of judges must be in accordance with the doctrine of \nprecedent, which establishes that lower courts must observe the decisions of higher \ncourts, they usually explain the reasons for their decisions by proving the agreement \nwith the previous decisions of the binding authority. Thus, modeling and learning \nfrom feature-rich legal citation networks is desirable, however their potential in \nretrieval and entailment tasks has not been fully explored yet.\nBhattacharya et al. (2020b) consider both the logical subdivision of statutes and \ntheir citation information to measure similarity of legal case documents. To this pur-\npose, the authors introduce Hier-SPCNet, a precedent citation network augmented \nwith the information about the hierarchy of the statutes (e.g., an act is typically \nstructured in parts, chapters, topics, sections). A statute is therefore modeled as \nnodes of different types, each representative of the structural levels, and two types \nof edges, the one reflecting the hierarchy of the nodes and the other one indicating \nthe citations between cases or between a case and a statute. Adding this structural \ninformation shows to lead to better document similarity estimation than competi-\ntors based on precedent citation network only. A combination of textual features and \nlegal citation networks is also proposed in (Paul et al. 2022a). Given an heterogene-\nous network with nodes representing statutes and legal fact descriptions and edges \nrepresenting hierarchical structures of the statutes as well as citations between stat-\nutes and facts, the goal is to predict which statute is relevant to a newly introduced \nfact, i.e., if a link exists between the statute and the new document. The text of a stat-\nute or fact is considered as an attribute of the node. Two separate encoders play the \nrole of encoding the attributes and the structural information given by the network, \nwhere attribute encoding is performed through hierarchical attention network (Yang \net al. 2016), and structural encoding is obtained through metapath schemes (Fu et al. \n2020). Results show that the exploitation of textual and graphical features can lead \nto significant improvements in performance compared to state-of-art competitors \n(BERT-based included) for the task. Also in (Louis et al. 2023), the structure of stat-\nutes is regarded as a key point for the success of neural models in statutory article \nretrieval tasks. A graph-augmented dense retrieval model is defined to exploit the \ntopology of the statutes to expand the article information. The resulting knowledge-\nrich cross-article representations contributes significantly to the improvement of \nperformance of the dense statute retriever.\nWang et al. (2022) state that the document structure, and in particular the rela-\ntions among the participants of a litigation, is essential to recognize and classify \nlegal cases of different categories yet with similar topics. To this purpose, four \nrelation graphs are defined, each modeling different types of relations between \nparticipants (i.e., plaintiffs and defendants), namely the relations between the \nparticipants and the matters of the dispute, the actions performed by the partici-\npants, the topics related to the participants, and the relations between facts and \nthird parties. Each graph consists of nodes representing the document, the par -\nticipants, and a set of nouns and verbs selected from the text and whose relations \n990 C. M. Greco, A. Tagarelli \n1 3\nto participants express either matters, actions, keywords, or facts and third par -\nties. The graphs are then aggregated in one graph by merging the common nodes, \nthen it is provided in input to a graph attention network to get the document rep-\nresentation for the classification task. Results on real-world Chinese documents \nregarding twenty semantically-similar disputes show that the proposed model \noutperforms all the considered baselines, included TLMs like BERT, RoBERTa \nand Lawformer.\nA related aspect is also a debate on the notions of legal similarity. In fact, while \ngenerally two documents would be similar if legal experts evaluate them as similar \nin their contents, Bhattacharya et al. (2020b) point out that two legal cases should \nbe regarded as similar if they jointly cite a common statute or precedents, and also if \nthey cite different statutes or precedents which are structurally similar in their pro-\nposed Hier-SPCNet network.\nInterpretability and knowledge injection. Two further challenges regard inter -\npretability aspects and knowledge modeling (Zhong et  al. 2020). In Sect.  4.8, we \nhave discussed about the existence of a number of approaches recently developed \nto improve explainability and interpretability of TLM-based methods, which can \nbroadly be categorized into post-hoc explanation methods and early explanation \nmethods. Besides that, it is important to define specific legal requirements on the \ninterpretability of machine learning models applied to private and public decision \nmaking, as discussed by Bibal et al. (2021). Also, Branting et al. (2021) highlight \nthe trade-off between explanation quality and representation effort, stating that a key \nrequirement for explainable systems is the ability to obtain useful and comprehensi-\nble predictions with low costs in terms of development, testing, and maintenance at \nscale. In this respect, knowledge injection can play a key role in improving not only \nthe language understanding ability of TLMs but also their explainability and inter -\npretability—recently, the term augmented language models has been introduced by \nMialon et  al. (2023) to also refer to those approaches that aim to enhance TLMs \nwith reasoning skills and other external tools. Liu et al. (2021b) introduce the use \nof causal graphs to ensure explainability requirements for a task of similar charge \ndisambiguation. The proposed model, named GCI, aims to capture explainable dis-\ncriminative nuances among confusing charges through building a causal graph to \ndetect keywords from the charges, where nodes are obtained from the charges and \nby clustering similar keywords, and edges (i.e., the causal relationships) are learned \nusing a causal discovery algorithm. The graph is sampled in more graphs that are \nrefined by estimating the strength of the relationships. GCI decides which charge is \nmore suitable for a case by extracting keywords from fact description and mapping \nthe case according to the graph. This approach has been integrated into LSTM mod-\nels to assess its potential and improve interpretability, where in particular, causal \nstrength constraints are included into the attention weights of the neural network. \nKnowledge modeling mainly refers to properly utilizing the legal knowledge, and \nin this regard, an increasing number of works carry out a domain-adaptation pre-\ntraining process of a TLM in the attempt of better internalizing legal knowledge in \nthe model.\n991\n1 3\nTransformer-based language models for AI and law\nUnfortunately, as we previously mentioned, the ability of successfully modeling \nthe legal language and capturing peculiar patterns to the domain is strongly related \nto the exploitation of large, possibly multiple, legal data sources.\nActually, the amount of existing legal data is considerable but many resources \nare only available to large companies or court sections. For example, it is unlikely \nthat the whole history of a section of a court can be recovered, which would be \nvery useful for many tasks like court profiling or case recommendation to courts, or \neven to characterize the language and creation style of arguments by lawyers, which \ncan be supportive in the preparation of a lawyer’s pleading. In (Francesconi 2022), \nit is pointed out that the knowledge available in the Semantic Web is essential for \nthe AI applied to legal domain, since it provides knowledge models for a top-down \napproaches (legal knowledge representation, legal reasoning, planning, explain-\nability), and data for a bottom-up approach (argument mining, rule-based/case-\nbased systems, legal information retrieval and discovery). Gan et al. (2021) suggest \nto inject legal knowledge into deep neural networks by including first-order logic \nrules, motivated by the fact that logic rules yield models with inductive inclination \nand, thus, can alleviate the dependency of deep neural networks on high amounts \nof training data, besides making them more interpretable because of the presence \nof the rules. A model for LJP on private loan cases is proposed as a co-attention \nnetwork followed by a symbolic module, where the co-attention network exploits the \nrelations between claims and fact descriptions and provide the probability distribu-\ntion for judgments, and the symbolic module adapts the distribution according to \nlogic rules to prevent outputs that violate the law. Moreover, non-differentiability \nof first-order rules is ensured by associating continuous real-values to the outputs \nof logic rules with mapping functions. The rules are then injected with a reward \nmechanism, i.e., the output of the co-attention network is increased, resp. decreased, \nif the facts in the text satisfy, resp. violate, the conditions. Results show that the gain \nin injecting legal knowledge into TLMs such as BERT and RoBERTa increases as \nthe size of the training set decreases.\nEthical aspects. Another critical point in the use of AI-based technologies for the \nlegal domain is related to ethics. Specific recommendations for the legal NLP com-\nmunity are provided in (Tsarapatsanis and Aletras  2021) according to three ethical \nparameters: the importance of academic freedom, the diversity of ethical and legal \nnorms and the threat of moralism. In (Zhong et al. 2020), it is reminded that the pur-\npose of AI is not to replace humans in legal matters but only to provide support in \ndecision-making processes, but it is also pointed out that applying AI systems to law \ncan inadvertently lead to ethical problems, particularly bias of different types such \nas gender and racial discrimination. Being aware of such issues and trying to alle-\nviate them is an emergence for developing next-generation (legal-focused) TLMs. \nHenderson et al. (2022) highlight the difficulty in filtering toxic and private informa-\ntion in data used to train a model, since its removal could affect the meaning of the \ntext; moreover, privacy expectations can be different based on the specific country. \nIn this regard, filtering rules should be designed to reflect the standards developed \nby legal and administrative experts. Chalkidis et al. (2022c) measure the fairness of \nBERT-based models on three categories, namely demographics, regional and legal \n992 C. M. Greco, A. Tagarelli \n1 3\ntopic: for the first category, the goal is to evaluate if a model performs worse because \nit is biased by factors like gender, age, race, language, legal status, whereas for the \nother two categories, the goal is to assess if a model performs differently on cases \nassociated with courts of specific regions, or in a specific field of law (e.g., it may \nperform better on criminal cases than on civil law cases). In this respect, some group \ndisparity in performance is found as related to the defendant’s state (Central Euro-\npean states versus the other European states), applicant’s gender, language, legal \nareas, and court’s regions (e.g., Switzerland courts versus federation courts, and \nBeijing courts versus Sichuan courts); however, some group disparities can also be \ninfluenced by general factors based on the distribution of training data. Wang et al. \n(2021) measure the judicial inconsistency related to attributes like gender, race, and \nregion, as the average disagreement of the judgments (term penalty) given by LJP \nmodels, which are used as virtual judges. The judicial disagreement is defined as the \nstandard deviation of virtual judges’ results. Results on the CAIL data are produced \naccording to region and gender: in the first case, seven virtual judges are trained \non data belonging to seven provincial-level administrative regions, whereas in the \nsecond case, two virtual judges are trained, one for each defendant’s gender. Gender \nand, especially, regional inconsistency are found in the legal system, with regional \ninconsistency varying over time; Moreover, judicial inconsistency seems to be nega-\ntively correlated with the severity of criminal charges.\nFinally, a special remark should be made concerning all the hype of large gen-\nerative language models after the launch of ChatGPT in November 2022. Despite \nlimitations of ChatGPT and similar tools have soon been detected (e.g., possibil-\nity of writing plausible-sounding but incorrect, misinformed or nonsensical answers, \nsensitivity to tweaks to the input phrasing, verbosity, possibility of responding to \nharmful instructions or exhibiting biased behavior, etc.), we have witnessed a tech-\nnological shift in the way we work: by using ChatGPT, people build websites and \napps, write novels or technical papers, even pass college and university exams, from \nmedical degree to law degree. This rise in “bad” actors abusing of generative con-\ntents has prompted to start developing countermeasures to debunk artificially gener-\nated text as well as to check data security requirements (e.g., on March 2023, Italian \ndata-protection authority temporarily banned ChatGPT over concerns about breach-\ning of existing EU privacy rules). This and much more opens to further challenges \nthat are going to be a major point of interest for researchers involved in legal AI.\n6  Conclusions\nIn this work, we investigated the research advances in AI for law that have been \naccomplished by means of Transformer-based language models (TLMs), pushed \nsince the advent of BERT. This is the first systematic study on this topic devel-\noped around problems, tasks and benchmarks in the legal AI area, for which we \ncovered about thirty different TLMs used to build more than three hundred AI \napproaches and methods addressing retrieval, classification, prediction, entail-\nment, summarization, generation, information extraction, and many other tasks \nrelevant in the legal domain. Our survey included critical aspects in the design \n993\n1 3\nTransformer-based language models for AI and law\nof TLM-based methods for legal AI, such as different strategies for adaptation \nto the legal domain, dealing with low-resource natural languages as well as with \nmultilingual contexts. We discussed main findings and limitations of current \nTLM-based methods, and open challenges and future perspectives for next gen-\neration of legal AI tools. Moreover, we considered details on the implementation \nof TLM-based methods, providing a large number of references to the software \nresources available for such methods. In this regard, we envisage an interesting \nopportunity for the research community to gather and manage a publicly shared \nplatform for all TLM-based systems for AI and law.\nFunding Open access funding provided by Università della Calabria within the CRUI-CARE Agreement.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, \nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, provide a link to the Creative \nCommons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your intended \nuse is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permis-\nsion directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/\nlicenses/by/4.0/.\nReferences\nAguiar A, Silveira R, Pinheiro V, Furtado V, Neto JA (2021) Text classification in legal documents \nextracted from lawsuits in Brazilian courts. In: Proceedings of the Brazilian conference on intel-\nligent systems (BRACIS), pp 586–600\nAhmad WU, Chi J, Le T, Norton T, Tian Y, Chang K (2021) Intent classification and slot filling for pri-\nvacy policies. In: Proceedings of the annual meeting of the association for computational linguis-\ntics and the international joint conference on natural language processing (ACL/IJCNLP). Associa-\ntion for Computational Linguistics, pp 4402–4417\nAhmad WU, Chi J, Tian Y, Chang K (2020) PolicyQA: a reading comprehension dataset for privacy \npolicies. In: Findings of the Association for Computational Linguistics: EMNLP, findings of ACL. \nAssociation for Computational Linguistics, pp 743–749\nAlberts H, Ipek A, Lucas R, Wozny P (2020) COLIEE 2020: legal information retrieval and entailment \nwith legal embeddings and boosting. In: New frontiers in artificial intelligence—JSAI-isAI 2020 \nworkshops, JURISIN, LENLS 2020 Workshops, volume 12758 of lecture notes in computer sci-\nence. Springer, pp 211–225\nAletras N, Tsarapatsanis D, Preotiuc-Pietro D, Lampos V (2016) Predicting judicial decisions of the \nEuropean Court of Human Rights: a natural language processing perspective. PeerJ Comput Sci \n2:e93\nAllan J, Harman D, Kanoulas E, Li D, Gysel CV, Voorhees, EM (2017) TREC 2017 common core track \noverview. In: Proceedings of the Text REtrieval conference (TREC), volume 500-324 of NIST Spe-\ncial Publication. National Institute of Standards and Technology (NIST)\nAlthammer S, Askari A, Verberne S, Hanbury A (2021) DoSSIER@COLIEE 2021: leveraging dense \nretrieval and summarization-based re-ranking for case law retrieval. CoRR, arXiv: 2108. 03937\nAmos R, Acar G, Lucherini E, Kshirsagar M, Narayanan A, Mayer JR (2021) Privacy policies over time: \ncuration and analysis of a million-document dataset. In: Proceedings of the ACM web conference \n(WWW). ACM, pp 2165–2176\nAntoun W, Baly F, Hajj HM (2020) AraBERT: Transformer-based model for Arabic language under -\nstanding. CoRR, arXiv: 2003. 00104\n994 C. M. Greco, A. Tagarelli \n1 3\nArtetxe M, Bhosale S, Goyal N, Mihaylov T, Ott M, Shleifer S, Lin XV, Du J, Iyer S, Pasunuru R, Anan-\ntharaman G, Li X, Chen S, Akin H, Baines M, Martin L, Zhou X, Koura PS, O’Horo B, Wang J, \nZettlemoyer L, Diab MT, Kozareva Z, Stoyanov V (2022) Efficient large scale language modeling \nwith mixtures of experts. In: Proceedings of the conference on empirical methods in natural lan-\nguage processing (EMNLP). Association for Computational Linguistics, pp 11699–11732\nArtetxe, M., Ruder, S., and Yogatama, D. (2020). On the cross-lingual transferability of monolingual \nrepresentations. In Proc. of the Annual Meeting of the Association for Computational Linguistics \n(ACL), pages 4623–4637. Association for Computational Linguistics\nAskari A, Verberne S (2021) Combining lexical and neural retrieval with Longformer-based summariza-\ntion for effective case law retrieval. In: Proceedings of the international conference on design of \nexperimental search & information retrieval systems, volume 2950 of CEUR workshop proceed-\nings. CEUR-WS.org, pp 162–170\nAthinaios K (2020) Named entity recognition using a novel linguistic model for greek legal cor -\npora based on BERT model. BS Thesis, School of Science, Department of Informatics and \nTelecommunications\nAumiller D, Almasian S, Lackner S, Gertz M (2021) Structural text segmentation of legal documents. In: \nProceedings of the international conference on artificial intelligence and law (ICAIL). ACM, pp \n2–11\nAumiller D, Chouhan A, Gertz M (2022) EUR-Lex-Sum: a multi- and cross-lingual dataset for long-form \nsummarization in the legal domain. In: Proceedings of the conference on empirical methods in \nnatural language processing (EMNLP). Association for Computational Linguistics, pp 7626–7639\nAvram A, Pais VF, Tufis DI (2021) PyEuroVoc: a tool for multilingual legal document classification with \nEuroVoc descriptors. In: Proceedings of the international conference on recent advances in natural \nlanguage processing (RANLP), pp 92–101\nAydemir A, de  Castro  Souza P, Gelfman A (2020) Using BERT and TF-IDF to predict entailment in \nlaw-based queries. In: New Frontiers in artificial intelligence—JSAI-isAI 2020 workshops, JURI-\nSIN, LENLS 2020 workshops, volume 12758 of lecture notes in computer science. Springer, pp \n286–293\nBahdanau D, Cho K, Bengio Y (2015) Neural machine translation by jointly learning to align and trans-\nlate. In: Proceedings of the international conference on learning representations (ICLR)\nBao H, Dong L, Wei F, Wang W, Yang N, Liu X, Wang Y, Gao J, Piao S, Zhou M, Hon H (2020) Uni-\nLMv2: pseudo-masked language models for unified language model pre-training. In: Proceedings \nof the international conference on machine learning (ICML), pp 642–652\nBarrios F, López F, Argerich, L, Wachenchauzer R (2016) Variations of the similarity function of Tex-\ntRank for automated summarization. CoRR, arXiv: 1602. 03606\nBaudis P, Sedivý J (2015) Modeling of the question answering task in the YodaQA system. In: Proceed-\nings of the international conference of the CLEF Association: experimental IR meets multilingual-\nity, multimodality, and interaction, volume 9283 of lecture notes in computer science. Springer, pp \n222–228\nBeltagy I, Peters ME, Cohan A (2020) Longformer: the long-document Transformer. CoRR, arXiv: 2004. \n05150\nBerant J, Chou A, Frostig R, Liang P (2013) Semantic parsing on freebase from question-answer pairs. \nIn: Proceedings of the conference on empirical methods in natural language processing (EMNLP). \nACL, pp 1533–1544\nBhattacharya P, Ghosh K, Pal A, Ghosh S (2020a) Hier-spcnet: a legal statute hierarchy-based heteroge-\nneous network for computing legal case document similarity. In: Proceedings of the ACM SIGIR \nconference on research and development in information retrieval (SIGIR). ACM, pp 1657–1660\nBhattacharya P, Ghosh K, Pal A, Ghosh S (2020b) Hier-SPCNet: a legal statute hierarchy-based hetero-\ngeneous network for computing legal case document similarity. In: Proceedings of the ACM SIGIR \nconference on research and development in information retrieval (SIGIR). ACM, pp 1657–1660\nBhattacharya P, Hiware K, Rajgaria S, Pochhi N, Ghosh K, Ghosh S (2019a) A comparative study of \nsummarization algorithms applied to legal case judgments. In: Proceedings of the European con-\nference on IR research (ECIR), volume 11437 of lecture notes in computer science. Springer, pp \n413–428\nBhattacharya P, Paul S, Ghosh K, Ghosh S, Wyner A (2019b) Identification of rhetorical roles of sen-\ntences in Indian legal judgments. In: Proceedings of the international conference on legal knowl-\nedge and information systems (JURIX)\n995\n1 3\nTransformer-based language models for AI and law\nBhattacharya P, Paul S, Ghosh K, Ghosh S, Wyner A (2021) DeepRhole: deep learning for rhetorical role \nlabeling of sentences in legal case documents. Artif Intell Law 6:66\nBibal A, Lognoul M, de Streel A, Frénay B (2021) Legal requirements on explainability in machine \nlearning. Artif Intell Law 29(2):149–169\nBlack S, Biderman S, Hallahan E, Anthony Q, Gao L, Golding L, He H, Leahy C, McDonell K, Phang J, \nPieler M, Prashanth US, Purohit S, Reynolds L, Tow J, Wang B, Weinbach S (2022) Gpt-neox-20b: \nan open-source autoregressive language model. CoRR, arXiv: 2204. 06745\nBlair-Stanek A, Holzenberger N, Durme BV (2023) Can GPT-3 perform statutory reasoning? CoRR, \narXiv: 2302. 06100\nBojanowski P, Grave E, Joulin A, Mikolov T (2017) Enriching word vectors with subword information. \nTrans Assoc Comput Linguist 5:135–146\nBowman SR, Angeli G, Potts C, Manning CD (2015) A large annotated corpus for learning natural lan-\nguage inference. In: Proceedings of the conference on empirical methods in natural language pro-\ncessing (EMNLP). The Association for Computational Linguistics, pp 632–642\nBrack A, Hoppe A, Buschermöhle P, Ewerth R (2021) Sequential sentence classification in research \npapers using cross-domain multi-task learning. CoRR, arXiv: 2102. 06008\nBranting LK, Pfeifer C, Brown B, Ferro L, Aberdeen JS, Weiss B, Pfaff M, Liao B (2021) Scalable and \nexplainable legal prediction. Artif Intell Law 29(2):213–238\nBrown TB, Mann B, Ryder N, Subbiah M, Kaplan J, Dhariwal P, Neelakantan A, Shyam P, Sastry G, \nAskell A, Agarwal S, Herbert-Voss A, Krueger G, Henighan T, Child R, Ramesh A, Ziegler \nDM, Wu J, Winter C, Hesse C, Chen M, Sigler E, Litwin M, Gray S, Chess B, Clark J, Berner C, \nMcCandlish S, Radford A, Sutskever I, Amodei D (2020a) Language models are few-shot learners. \nIn: Proceedings of the annual conference on neural information processing systems (NeurIPS)\nBrown TB, Mann B, Ryder N, Subbiah M, Kaplan J, Dhariwal P, Neelakantan A, Shyam P, Sastry G, \nAskell A, Agarwal S, Herbert-Voss A, Krueger G, Henighan T, Child R, Ramesh A, Ziegler \nDM, Wu J, Winter C, Hesse C, Chen M, Sigler E, Litwin M, Gray S, Chess B, Clark J, Berner C, \nMcCandlish S, Radford A, Sutskever I, Amodei D (2020b) Language models are few-shot learners. \nIn: Proceedings of the annual conference on neural information processing systems (NeurIPS)\nBui D, Shin KG, Choi J, Shin J (2021) Automated extraction and presentation of data practices in privacy \npolicies. Proc Privacy Enhanc Technol 2021(2):88–110\nBurton K (2017) “Think Like a Lawyer’’ using a legal reasoning grid and criterion-referenced assess-\nment rubric on IRAC (Issue, Rule, Application, Conclusion). J Learn Des 10(2):57–68\nCallister PD (2020) Law, artificial intelligence, and natural language processing: a funny thing happened \non the way to my search results. Law Libr J 112(161):66\nChalkidis I, Androutsopoulos I, Aletras N (2019a) Neural legal judgment prediction in English. In: Pro-\nceedings of the annual meeting of the Association for Computational Linguistics (ACL). Associa-\ntion for Computational Linguistics, pp 4317–4323\nChalkidis I, Dai X, Fergadiotis M, Malakasiotis P, Elliott D (2022a) An exploration of hierarchical atten-\ntion transformers for efficient long document classification. CoRR, arXiv: 2210. 05529\nChalkidis I, Fergadiotis M, Androutsopoulos I (2021a) MultiEURLEX—a multi-lingual and multi-label \nlegal document classification dataset for zero-shot cross-lingual transfer. In: Proceedings of the \nconference on empirical methods in natural language processing (EMNLP). Association for Com-\nputational Linguistics, pp 6974–6996\nChalkidis I, Fergadiotis M, Kotitsas S, Malakasiotis P, Aletras N, Androutsopoulos I (2020a) An empiri-\ncal study on large-scale multi-label text classification including few and zero-shot labels. In: Pro-\nceedings of the conference on empirical methods in natural language processing (EMNLP). Asso-\nciation for Computational Linguistics, pp 7503–7515\nChalkidis I, Fergadiotis M, Malakasiotis P, Aletras N, Androutsopoulos I (2020b) LEGAL-BERT: the \nmuppets straight out of law school. CoRR, arXiv: 2010. 02559\nChalkidis I, Fergadiotis M, Malakasiotis P, Androutsopoulos I (2019b) Large-scale multi-label text clas-\nsification on EU legislation. In: Proceedings of the annual meeting of the Association for Compu-\ntational Linguistics (ACL). Association for Computational Linguistics, pp 6314–6322\nChalkidis I, Fergadiotis M, Manginas N, Katakalou E, Malakasiotis P (2021b) Regulatory Compliance \nthrough Doc2Doc information retrieval: a case study in EU/UK legislation where text similarity \nhas limitations. In: Proceedings of the conference of the European chapter of the Association for \nComputational Linguistics (EACL). Association for Computational Linguistics, pp 3498–3511\nChalkidis I, Fergadiotis M, Tsarapatsanis D, Aletras N, Androutsopoulos I, Malakasiotis P (2021c) Para-\ngraph-level rationale extraction through regularization: a case study on European Court of human \n996 C. M. Greco, A. Tagarelli \n1 3\nrights cases. In: Proceedings of the conference of the North American Chapter of the Association \nfor Computational Linguistics: Human Language Technologies (NAACL-HLT). Association for \nComputational Linguistics, pp 226–241\nChalkidis I, Jana A, Hartung D II, Meon JB, Androutsopoulos I, Katz DM, Aletras N (2022b) LexGLUE: \na benchmark dataset for legal language understanding in English. In: Proceedings of the annual \nmeeting of the Association for Computational Linguistics (ACL). Association for Computational \nLinguistics, pp 4310–4330\nChalkidis I, Nikolaou C, Soursos P, Koubarakis M (2017) Modeling and querying greek legislation using \nsemantic web technologies. In: Proceedings of the international conference on the semantic web \n(ESWC), volume 10249 of lecture notes in computer science, pp 591–606\nChalkidis I, Pasini T, Zhang S, Tomada L, Schwemer SF, Søgaard A (2022c) Fairlex: a multilingual \nbenchmark for evaluating fairness in legal text processing. In: Proceedings of the annual meeting of \nthe Association for Computational Linguistics (ACL). Association for Computational Linguistics, \npp 4389–4406\nChan B, Schweter S, Möller T (2020) German’s next language model. In: Proceedings of the Interna-\ntional conference on computational linguistics (COLING). International Committee on Computa-\ntional Linguistics, pp 6788–6796\nChen T, He T, Benesty M, Khotilovich V, Tang Y, Cho H, Chen K et al (2015) Xgboost: extreme gradient \nboosting. R package version 0.4-2, 1(4):1–4\nChi J, Ahmad WU, Tian Y, Chang K (2023) PLUE: language understanding evaluation benchmark for \nprivacy policies in English. In: Proceedings of the annual meeting of the Association for Computa-\ntional Linguistics (ACL). Association for Computational Linguistics, pp 352–365\nChoi JH, Hickman KE, Monahan A, Schwarcz D (2023) ChatGPT goes to law school. J Legal Educ 6:66\nClark K, Luong M, Le QV, Manning CD (2020) ELECTRA: pre-training text encoders as discriminators \nrather than generators. In: Proceedings of the international conference on learning representations \n(ICLR)\nCohan A, Beltagy I, King D, Dalvi B, Weld DS (2019) Pretrained language models for sequential sen-\ntence classification. In: Proceedings of the conference on empirical methods in natural language \nprocessing and the international joint conference on natural language processing (EMNLP-IJC-\nNLP). Association for Computational Linguistics, pp 3691–3697\nConneau A, Khandelwal K, Goyal N, Chaudhary V, Wenzek G, Guzmán F, Grave E, Ott M, Zettlemoyer \nL, Stoyanov V (2020) Unsupervised cross-lingual representation learning at scale. In: Proceedings \nof the annual meeting of the Association for Computational Linguistics (ACL). Association for \nComputational Linguistics, pp 8440–8451\nConneau A, Lample G (2019) Cross-lingual language model pretraining. In: Proceedings of the annual \nconference on neural information processing systems (NeurIPS), pp 7057–7067\nConneau A, Rinott R, Lample G, Williams A, Bowman SR, Schwenk H, Stoyanov V (2018) XNLI: \nevaluating cross-lingual sentence representations. In: Proceedings of the conference on empirical \nmethods in natural language processing (EMNLP). Association for Computational Linguistics, pp \n2475–2485\nCui Y, Che W, Liu T, Qin B, Wang S, Hu G (2020) Revisiting pre-trained models for Chinese natural lan-\nguage processing. In: Findings of the Association for Computational Linguistics: EMNLP. Asso-\nciation for Computational Linguistics, pp 657–668\nCui Y, Yang Z, Liu T (2022) PERT: pre-training BERT with permuted language model. CoRR, arXiv: \n2203. 06906\nDadgostari F, Guim M, Beling P, Livermore MA, Rockmore D (2021) Modeling law search as prediction. \nArtif Intell Law 29(1):3–34\nDadu T, Pant K (2020) Team rouges at SemEval-2020 Task 12: cross-lingual inductive transfer to detect \noffensive language. In: Proceedings of the fourteenth workshop on semantic evaluation, SemE-\nval@COLING 2020. International Committee for Computational Linguistics, pp 2183–2189\nDai Z, Yang Z, Yang Y, Carbonell JG, Le QV, Salakhutdinov R (2019) Transformer-XL: attentive lan-\nguage models beyond a fixed-length context. In: Proceedings of the annual meeting of the asso-\nciation for computational linguistics (ACL). Association for Computational Linguistics, pp \n2978–2988\nde Vries W, van Cranenburgh A, Bisazza A, Caselli T, van Noord G, Nissim M (2019) Bertje: a dutch \nBERT model. CoRR, arXiv: 1912. 09582\n997\n1 3\nTransformer-based language models for AI and law\nDelobelle P, Winters T, Berendt B (2020) RobBERT: a Dutch RoBERTa-based language model. In: Find-\nings of the Association for Computational Linguistics: EMNLP. Association for Computational \nLinguistics, pp 3255–3265\nDeroy A, Bhattacharya P, Ghosh K, Ghosh S (2021) An analytical study of algorithmic and expert sum-\nmaries of legal cases. In: Proceedings of the conference on legal knowledge and information sys-\ntems (JURIX), volume 346 of frontiers in artificial intelligence and applications. IOS Press, pp \n90–99\nDevlin J, Chang M, Lee K, Toutanova K (2019) BERT: pre-training of deep bidirectional transformers \nfor language understanding. In: Proceedings of the conference of the North American chapter of \nthe Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), \npp 4171–4186\nDoddapaneni S, Ramesh G, Kunchukuttan A, Kumar P, Khapra MM (2021) A primer on pretrained mul-\ntilingual language models. CoRR, arXiv: 2107. 00676\nDong L, Yang N, Wang W, Wei F, Liu X, Wang Y, Gao J, Zhou M, Hon H (2019) Unified language \nmodel pre-training for natural language understanding and generation. In: Proceedings of the \nannual conference on neural information processing systems (NeurIPS), pp 13042–13054\nDong Q, Niu S (2021) Legal judgment prediction via relational learning. In: Proceedings of the ACM \nSIGIR conference on research and development in information retrieval (SIGIR). ACM, pp \n983–992\nDouka S, Abdine H, Vazirgiannis M, Hamdani RE, Amariles DR (2021) JuriBERT: a masked-language \nmodel adaptation for French legal text. In: Proceedings of the 2021 workshop on natural legal lan-\nguage processing (NLLP). ACL, pp 95–101\nDrawzeski K, Galassi A, Jablonowska A, Lagioia F, Lippi M, Micklitz HW, Sartor G, Tagiuri G, Torroni \nP (2021) A corpus for multilingual analysis of online terms of service. In: Proceedings of the 2021 \nworkshop on natural legal language processing (NLLP). Association for Computational Linguis-\ntics, pp 1–8\nDumitrescu SD, Avram A, Pyysalo S (2020) The birth of Romanian BERT. In: Findings of the Asso-\nciation for Computational Linguistics: EMNLP. Association for Computational Linguistics, pp \n4324–4328\nEddine MK, Tixier AJ, Vazirgiannis M (2021) BARThez: a skilled pretrained french sequence-to-\nsequence model. In: Proceedings of the conference on empirical methods in natural language pro-\ncessing (EMNLP). Association for Computational Linguistics, pp 9369–9390\nElwany E, Moore D, Oberoi G (2019) BERT goes to law school: quantifying the competitive advantage \nof access to large legal corpora in contract understanding. CoRR, arXiv: 1911. 00473\nEngstrom DF, Ho DE, Sharkey CM, Cuéllar M-F (2020) Government by algorithm: artificial intelligence \nin federal administrative agencies. NYU School of Law, Public Law Research Paper, pp 20–54\nFarzindar A, Lapalme G (2004) Letsum, an automatic legal text summarizing system. Jurix 66:11–18\nFeijó D, Moreira V (2018) RulingBR: a summarization dataset for legal texts. In: Proceedings of the \nconference on computational processing of the Portuguese language (PROPOR), volume 11122 of \nlecture notes in computer science. Springer, pp 255–264\nFeijó D, Moreira V (2019) Summarizing legal rulings: comparative experiments. In: Proceedings of the \ninternational conference on recent advances in natural language processing (RANLP), pp 313–322\nFeng Y, Li C, Ng V (2022) Legal judgment prediction via event extraction with constraints. In: Proceed-\nings of the annual meeting of the Association for Computational Linguistics (ACL). Association \nfor Computational Linguistics, pp 648–664\nFrancesconi E (2022) The winter, the summer and the summer dream of artificial intelligence in law. \nArtif Intell Law 30(2):147–161\nFu X, Zhang J, Meng Z, King I (2020) MAGNN: metapath aggregated graph neural network for het-\nerogeneous graph embedding. In: Proceedings of the ACM Web conference (WWW). ACM, pp \n2331–2341\nFurniturewala S, Jain R, Kumari V, Sharma Y (2021) Legal text classification and summarization using \ntransformers and joint text features. In: Working notes of FIRE 2021—forum for information \nretrieval evaluation, volume 3159 of CEUR workshop proceedings. CEUR-WS.org, pp 541–546\nGain B, Bandyopadhyay D, Saikh T, Ekbal A (2019) IITP@COLIEE 2019: legal information retrieval \nusing BM25 and BERT. In: Proceedings of COLIEE 2019 workshop: competition on legal infor -\nmation extraction/entailment\nGan L, Kuang K, Yang Y, Wu F (2021) Judgment prediction via injecting legal knowledge into neural \nnetworks. In: Proceedings of the AAAI conference on artificial intelligence (AAAI), the conference \n998 C. M. Greco, A. Tagarelli \n1 3\non innovative applications of artificial intelligence (IAAI), the AAAI symposium on educational \nadvances in artificial intelligence (EAAI). AAAI Press, pp 12866–12874\nGao J, Ning H, Han Z, Kong L, Qi H (2020) Legal text classification model based on text statistical fea-\ntures and deep semantic features. In: Working notes of FIRE 2020—forum for information retrieval \nevaluation, volume 2826 of CEUR workshop proceedings. CEUR-WS.org, pp 35–41\nGao L, Biderman S, Black S, Golding L, Hoppe T, Foster C, Phang J, He H, Thite A, Nabeshima N, \nPresser S, Leahy C (2021) The pile: an 800gb dataset of diverse text for language modeling. CoRR, \narXiv: 2101. 00027\nGarneau N, Gaumond E, Lamontagne L, Déziel P (2021) Criminelbart: a French Canadian legal language \nmodel specialized in criminal law. In: Proceedings of the international conference on artificial \nintelligence and law (ICAIL). ACM, pp 256–257\nGoebel R, Kano Y, Kim M-Y, Rabelo J, Satoh K, Yoshioka M (eds) (2021) Proceedings of the eighth \ninternational competition on legal information extraction/entailment (COLIEE 2021)\nGoyal N, Du J, Ott M, Anantharaman G, Conneau, A (2021) Larger-scale transformers for multilingual \nmasked language modeling. In: Proceedings of the workshop on representation learning for NLP, \nRepL4NLP@ACL-IJCNLP 2021. Association for Computational Linguistics, pp 29–33\nGreco CM, Tagarelli A, Zumpano E (2022) A comparison of Transformer-based language models on \nNLP benchmarks. In: Proceedings of the international conference on applications of natural lan-\nguage to information systems (NLDB), volume 13286 of lecture notes in computer science. \nSpringer, pp 490–501\nGuillou P (2020) GPorTuguese-2 (Portuguese GPT-2 small): a language model for Portuguese text gen-\neration (and more NLP tasks...). Technical report\nGuo J, Fan Y, Ai Q, Croft WB (2016) A deep relevance matching model for ad-hoc retrieval. In: Proceed-\nings of the ACM conference on information and knowledge management (CIKM). ACM, pp 55–64\nGururangan S, Marasovic A, Swayamdipta S, Lo K, Beltagy I, Downey D, Smith NA (2020) Don’t stop \npretraining: adapt language models to domains and tasks. In: Proceedings of the annual meeting of \nthe Association for Computational Linguistics (ACL). Association for Computational Linguistics, \npp 8342–8360\nGutiérrez-Fandiño A, Armengol-Estapé J, Gonzalez-Agirre A, Villegas M (2021a) Spanish legalese lan-\nguage model and corpora. CoRR, arXiv: 2110. 12201\nGutiérrez-Fandiño A, Armengol-Estapé J, Pàmies M, Llop-Palao J, Silveira-Ocampo J, Carrino CP, Gon-\nzalez-Agirre A, Armentano-Oller C, Penagos CR, Villegas M (2021b) Spanish language models \nCoRR, arXiv: 2107. 07253\nHe J, Wang L, Liu L, Feng J, Wu H (2019) Long document classification from local word glimpses via \nrecurrent attention learning. IEEE Access 6:66\nHe P, Gao J, Chen W (2023) Debertav3: Improving deberta using electra-style pre-training with gradient-\ndisentangled embedding sharing. In: Proceedings of the international conference on learning rep-\nresentations (ICLR)\nHe P, Liu X, Gao J, Chen W (2021) DeBERTa: decoding-enhanced BERT with disentangled attention. \nIn: Proceedings of the international conference on learning representations (ICLR)\nHenderson P, Krass MS, Zheng L, Guha N, Manning CD, Jurafsky D, Ho DE (2022) Pile of law: learning \nresponsible data filtering from the law and a 256 GB open-source legal dataset. In: Proceedings of \nthe annual conference on neural information processing systems (NeurIPS)\nHendrycks D, Burns C, Chen A, Ball S (2021) CUAD: an expert-annotated NLP dataset for legal con-\ntract review. In: Proceedings of the annual conference on neural information processing systems \n(NeurIPS)\nHermann KM, Kociský T, Grefenstette E, Espeholt L, Kay W, Suleyman M, Blunsom P (2015) Teaching \nmachines to read and comprehend. In: Proceedings of the annual conference on neural information \nprocessing systems (NeurIPS), pp 1693–1701\nHolzenberger N, Blair-Stanek A, Durme BV (2020) A dataset for statutory reasoning in tax law entail-\nment and question answering. In: Proceedings of the 2020 workshop on natural legal language \nprocessing (NLLP), volume 2645 of CEUR workshop proceedings, pp 31–38\nHolzenberger N, Durme BV (2021) Factoring statutory reasoning as language understanding challenges. \nIn: Proceedings of the annual meeting of the Association for Computational Linguistics and the \ninternational joint conference on natural language processing (ACL/IJCNLP)\nHong J, Chong D, Manning CD (2021) Learning from limited labels for long legal dialogue. In: Proceed-\nings of the 2021 workshop on natural legal language processing (NLLP). ACL, pp 190–204\n999\n1 3\nTransformer-based language models for AI and law\nHoulsby N, Giurgiu A, Jastrzebski S, Morrone B, de Laroussilhe Q, Gesmundo A, Attariyan M, Gelly \nS (2019) Parameter-efficient transfer learning for NLP. In: Proceedings of the international con-\nference on machine learning (ICML), volume  97 of proceedings of machine learning research. \nPMLR, pp 2790–2799\nHowe JST, Khang LH, Chai IE (2019) Legal area classification: a comparative study of text classifiers on \nSingapore supreme court judgments. CoRR, arXiv: 1904. 06470\nHu J, Ruder S, Siddhant A, Neubig G, Firat O, Johnson M (2020) XTREME: a massively multilingual \nmulti-task benchmark for evaluating cross-lingual generalization. CoRR, arXiv: 2003. 11080\nHuang Y, Yu Z, Guo J, Xiang Y, Xian Y (2021) Element graph-augmented abstractive summarization for \nlegal public opinion news with graph transformer. Neurocomputing 460:166–180\nHudzina J, Madan K, Chinnappa D, Harmouche J, Bretz H, Vold A, Schilder F (2020) Information \nextraction/entailment of common law and civil code. In: New frontiers in artificial intelligence-\nJSAI-isAI 2020 workshops, JURISIN, LENLS 2020 Workshops, volume 12758 of lecture notes in \ncomputer science. Springer, pp 254–268\nHui K, Yates A, Berberich K, de Melo G (2017) PACRR: a position-aware neural IR model for relevance \nmatching. In: Proceedings of the conference on empirical methods in natural language processing \n(EMNLP). Association for Computational Linguistics, pp 1049–1058\nII MJB, Katz DM (2022) GPT takes the bar exam. CoRR, arXiv: 2212. 14402\nJain D, Borah MD, Biswas A (2021) Summarization of Indian legal judgement documents via ensem-\nbling of contextual embedding based MLP models. In: Working notes of FIRE 2021—forum for \ninformation retrieval evaluation, volume 3159 of CEUR workshop proceedings. CEUR-WS.org, pp \n553–561\nJoshi M, Chen D, Liu Y, Weld DS, Zettlemoyer L, Levy O (2020) Spanbert: improving pre-training by \nrepresenting and predicting spans. Trans Assoc Comput Linguist 8:64–77\nJoshi M, Choi E, Weld DS, Zettlemoyer L (2017) TriviaQA: a large scale distantly supervised challenge \ndataset for reading comprehension. In: Proceedings of the annual meeting of the Association for \nComputational Linguistics (ACL). Association for Computational Linguistics, pp 1601–1611\nJoulin A, Grave E, Bojanowski P, Mikolov T (2017) Bag of tricks for efficient text classification. In: Pro-\nceedings of the conference of the European chapter of the Association for Computational Linguis-\ntics (EACL). Association for Computational Linguistics, pp 427–431\nKalamkar P, Tiwari A, Agarwal A, Karn S, Gupta S, Raghavan V, Modi A (2022) Corpus for automatic \nstructuring of legal documents. In: Proceedings of the language resources and evaluation confer -\nence (LREC). European Language Resources Association, pp 4420–4429\nKarpukhin V, Oguz B, Min S, Lewis PSH, Wu L, Edunov S, Chen D, Yih W (2020) Dense passage \nretrieval for open-domain question answering. In: Proceedings of the conference on empirical \nmethods in natural language processing (EMNLP). Association for Computational Linguistics, pp \n6769–6781\nKhazaeli S, Punuru J, Morris C, Sharma S, Staub B, Cole M, Chiu-Webster S, Sakalley D (2021) A free \nformat legal question answering system. In: Proceedings of the 2021 workshop on natural legal \nlanguage processing (NLLP). ACL, pp 107—113\nKim M-Y, Rabelo J, Goebel R (2021) BM25 and Transformer-based Legal Information Extraction and \nEntailment. In: Proceedings of the eighth international competition on legal information extraction/\nentailment (COLIEE 2021)\nKlaus S, Hecke RV, Naini KD, Altingovde IS, Bernabé-Moreno J, Herrera-Viedma E (2022) Summariz-\ning legal regulatory documents using transformers. In: Proceedings of the ACM SIGIR conference \non research and development in information retrieval (SIGIR). ACM, pp 2426–2430\nKojima T, Gu SS, Reid M, Matsuo Y, Iwasawa Y (2022) Large language models are zero-shot reasoners. \nIn: Proceedings of the annual conference on neural information processing systems (NeurIPS)\nKoreeda Y, Manning CD (2021) Contractnli: a dataset for document-level natural language inference for \ncontracts. In: Findings of the Association for Computational Linguistics: EMNLP 2021. Associa-\ntion for Computational Linguistics, pp 1907–1919\nKoutsikakis J, Chalkidis I, Malakasiotis P, Androutsopoulos I (2020) GREEK-BERT: the Greeks visit-\ning Sesame Street. In: Proceedings of the Hellenic conference on artificial intelligence. ACM, pp \n110–117\nKruiper R, Konstas I, Gray A, Sadeghineko F, Watson R, Kumar B (2021) SPAR.txt, a cheap Shallow \nParsing approach for Regulatory texts. In: Proceedings of the 2021 workshop on natural legal lan-\nguage processing (NLLP). ACL, pp 129—143\n1000 C. M. Greco, A. Tagarelli \n1 3\nKudo T Richardson J (2018) Sentencepiece: a simple and language independent subword tokenizer and \ndetokenizer for neural text processing. In: Proceedings of the conference on empirical methods \nin natural language processing (EMNLP): system demonstrations. Association for Computational \nLinguistics, pp 66–71\nKwiatkowski T, Palomaki J, Redfield O, Collins M, Parikh AP, Alberti C, Epstein D, Polosukhin I, Dev -\nlin J, Lee K, Toutanova K, Jones L, Kelcey M, Chang M, Dai AM, Uszkoreit J, Le Q, Petrov \nS (2019) Natural questions: a benchmark for question answering research. Trans Assoc Comput \nLinguist 7:452–466\nLage-Freitas A, Allende-Cid H, Santana O, de Oliveira-Lage L (2022) Predicting Brazilian court deci-\nsions. PeerJ Comput Sci 8:e904\nLai G, Xie Q, Liu H, Yang Y, Hovy EH (2017) RACE: large-scale reading comprehension dataset from \nexaminations. In: Proceedings of the conference on empirical methods in natural language process-\ning (EMNLP). Association for Computational Linguistics, pp 785–794\nLam JT, Liang D, Dahan S, Zulkernine FH (2020) The gap between deep learning and law: predicting \nemployment notice. In: Proceedings of the 2020 workshop on natural legal language processing \n(NLLP), volume 2645 of CEUR workshop proceedings, pp 52–56\nLan Z, Chen M, Goodman S, Gimpel K, Sharma P, Soricut R (2020) ALBERT: a lite BERT for self-\nsupervised learning of language representations. In: Proceedings of the international conference on \nlearning representations (ICLR)\nLe H, Vial L, Frej J, Segonne V, Coavoux M, Lecouteux B, Allauzen A, Crabbé B, Besacier L, Schwab \nD (2020) FlauBERT: unsupervised language model pre-training for French. In: Proceedings of the \nlanguage resources and evaluation conference (LREC). European Language Resources Associa-\ntion, pp 2479–2490\nLei T, Barzilay R, Jaakkola TS (2016) Rationalizing neural predictions. In: Proceedings of the conference \non empirical methods in natural language processing (EMNLP). The Association for Computa-\ntional Linguistics, pp 107–117\nLeivaditi S, Rossi J, Kanoulas E (2020) A benchmark for lease contract review. CoRR, arXiv: 2010. 10386\nLewis M, Liu Y, Goyal N, Ghazvininejad M, Mohamed A, Levy O, Stoyanov V, Zettlemoyer L (2020) \nBART: denoising sequence-to-sequence pre-training for natural language generation, translation, \nand comprehension. In: Proceedings of the annual meeting of the Association for Computational \nLinguistics (ACL). Association for Computational Linguistics, pp 7871–7880\nLi D, Yang K, Zhang L, Yin D, Peng D (2021a) CLASS: a novel method for Chinese legal judgments \nsummarization. In: Proceedings of the international conference on computer science and applica-\ntion engineering. ACM, pp 86:1–86:5\nLi J, Monroe W, Jurafsky D (2016) Understanding neural networks through representation erasure. \nCoRR, arXiv: 1612. 08220\nLi J, Zhao X, Liu J, Wen J, Yang M (2021b) SIAT@COLIEE-2021: combining statistics recall and \nsemantic ranking for legal case retrieval and entailment. In: Proceedings of the eighth international \ncompetition on legal information extraction/entailment (COLIEE 2021), pp 31–37\nLi L, Dai Y, Tang D, Feng Z, Zhou C, Qiu X, Xu Z, Shi S (2022) MarkBERT: marking word boundaries \nimproves Chinese BERT. CoRR, arXiv: 2203. 06378\nLicari D, Comandè G (2022) ITALIAN-LEGAL-BERT: a pre-trained Transformer language model for \nItalian law. In: Proceedings of the KM4LAW Workshop with the 23rd international conference on \nknowledge engineering and knowledge management, volume 3256 of CEUR workshop proceed-\nings. CEUR\nLimsopatham N (2021) Effectively leveraging BERT for legal document classification. In: Proceedings of \nthe 2021 workshop on natural legal language processing (NLLP). ACL, pp 210—216\nLin C-Y (2004) ROUGE: a package for automatic evaluation of summaries. In: Text summarization \nbranches out. Association for Computational Linguistics, pp 74–81\nLin J (1991) Divergence measures based on the Shannon entropy. IEEE Trans Inf Theory 37(1):145–151\nLin XV, Mihaylov T, Artetxe M, Wang T, Chen S, Simig D, Ott M, Goyal N, Bhosale S, Du J, Pasunuru \nR, Shleifer S, Koura PS, Chaudhary V, O’Horo B, Wang J, Zettlemoyer L, Kozareva Z, Diab M, \nStoyanov V, Li X (2022) Few-shot learning with multilingual generative language models. In: Pro-\nceedings of the conference on empirical methods in natural language processing (EMNLP). Asso-\nciation for Computational Linguistics, pp 9019–9052\nLippi M, Palka P, Contissa G, Lagioia F, Micklitz H, Sartor G, Torroni P (2019) CLAUDETTE: an \nautomated detector of potentially unfair clauses in online terms of service. Artif Intell Law \n27(2):117–139\n1001\n1 3\nTransformer-based language models for AI and law\nLiu C, Chen K (2019) Extracting the gist of Chinese judgments of the supreme court. In: Proceedings of \nthe international conference on artificial intelligence and law (ICAIL). ACM, pp 73–82\nLiu L, Zhang W, Liu J, Shi W, Huang Y (2021a) Interpretable charge prediction for legal cases based \non interdependent legal information. In: Proceedings of the joint conference on neural networks \n(IJCNN). IEEE, pp 1–8\nLiu X, Yin D, Feng Y, Wu Y, Zhao D (2021b) Everything has a cause: leveraging causal inference in \nlegal text analysis. In: Proceedings of the conference of the North American chapter of the Associ-\nation for Computational Linguistics: Human Language Technologies (NAACL-HLT). Association \nfor Computational Linguistics, pp 1928–1941\nLiu Y, Gu J, Goyal N, Li X, Edunov S, Ghazvininejad M, Lewis M, Zettlemoyer L (2020) Multilingual \ndenoising pre-training for neural machine translation. Trans Assoc Comput Linguist 8:726–742\nLiu Y, Lapata M (2019a) Text summarization with pretrained encoders. In: Proceedings of the confer -\nence on empirical methods in natural language processing and the international joint conference \non natural language processing (EMNLP-IJCNLP). Association for Computational Linguistics, pp \n3728–3738\nLiu Y, Lapata M (2019b) Text summarization with pretrained encoders. In: Proceedings of the confer -\nence on empirical methods in natural language processing and the international joint conference \non natural language processing (EMNLP-IJCNLP). Association for Computational Linguistics, pp \n3728–3738\nLiu Y, Ott M, Goyal N, Du J, Joshi M, Chen D, Levy O, Lewis M, Zettlemoyer L, Stoyanov V (2019) \nRoberta: a robustly optimized BERT pretraining approach. CoRR, arXiv: 1907. 11692\nLocke D, Zuccon G (2022) Case law retrieval: problems, methods, challenges and evaluations in the last \n20 years. CoRR, arXiv: 2202. 07209\nLouis A, Spanakis G (2022) A statutory article retrieval dataset in French. In: Proceedings of the annual \nmeeting of the Association for Computational Linguistics (ACL). Association for Computational \nLinguistics, pp 6789–6803\nLouis A, van Dijck G, Spanakis G (2023) Finding the law: enhancing statutory article retrieval via graph \nneural networks. In: Proceedings of the conference of the European chapter of the Association for \nComputational Linguistics (EACL). Association for Computational Linguistics, pp 2753–2768\nLyu Y, Wang Z, Ren Z, Ren P, Chen Z, Liu X, Li Y, Li H, Song H (2022) Improving legal judgment pre-\ndiction through reinforced criminal element extraction. Inf Process Manag 59(1):102780\nMa Y, Shao Y, Liu B, Liu Y, Zhang M, Ma S (2021) Retrieving legal cases from a large-scale candidate \ncorpus. In: Proceedings of the eighth international competition on legal information extraction/\nentailment (COLIEE 2021), pp 38–42\nMacAvaney S, Yates A, Cohan A, Goharian N (2019) CEDR: contextualized embeddings for document \nranking. In: Proceedings of the ACM SIGIR conference on research and development in informa-\ntion retrieval (SIGIR). ACM, pp 1101–1104\nMahari RZ (2021) AutoLAW: augmented legal reasoning through legal precedent prediction. In: Pro-\nceedings of the 2021 workshop on natural legal language processing (NLLP)\nMajumder SB. Das D (2020) Rhetorical role labelling for legal judgements using ROBERTA. In: Work -\ning notes of FIRE 2020—forum for information retrieval evaluation, volume 2826 of CEUR work-\nshop proceedings. CEUR-WS.org, pp 22–25\nMalik V, Sanjay R, Nigam SK, Ghosh K, Guha SK, Bhattacharya A, Modi A (2021) ILDC for CJPE: \nIndian legal documents corpus for court judgment prediction and explanation. In: Proceedings of \nthe annual meeting of the Association for Computational Linguistics and the international joint \nconference on natural language processing (ACL/IJCNLP). Association for Computational Lin-\nguistics, pp 4046–4062\nMamakas D, Tsotsi P, Androutsopoulos I, Chalkidis I (2022) Processing long legal documents with pre-\ntrained transformers: modding LegalBERT and longformer. CoRR, arXiv: 2211. 00974\nManor L, Li JJ (2019) Plain English summarization of contracts. CoRR, arXiv: 1906. 00424\nMartin L, Müller B, Suárez PJO, Dupont Y, Romary L, de la Clergerie É, Seddah D, Sagot B (2020) \nCamemBERT: a Tasty French language model. In: Proceedings of the annual meeting of the \nAssociation for Computational Linguistics (ACL). Association for Computational Linguistics, pp \n7203–7219\nMartino GD, Pio G, Ceci M (2022) PRILJ: an efficient two-step method based on embedding and cluster-\ning for the identification of regularities in legal case judgments. Artif Intell Law 30(3):359–390\n1002 C. M. Greco, A. Tagarelli \n1 3\nMasala M, Iacob RCA, Uban AS, Cidota M, Velicu H, Rebedea T, Popescu M (2021) jurBERT: a Roma-\nnian BERT model for legal judgement prediction. In: Proceedings of the 2021 workshop on natural \nlegal language processing (NLLP). ACL, pp 86–94\nMasala M, Ruseti S, Dascalu M (2020) RoBERT—a Romanian BERT model. In: Proceedings of the \ninternational conference on computational linguistics (COLING). International Committee on \nComputational Linguistics, pp 6626–6637\nMialon G, Dessì R., Lomeli M, Nalmpantis C, Pasunuru R, Raileanu R, Rozière B, Schick T, Dwivedi-Yu \nJ, Celikyilmaz A, Grave E, LeCun Y, Scialom T (2023) Augmented language models: a survey. \nCoRR, arXiv: 2302. 07842\nMihalcea R, Tarau P (2004) Textrank: bringing order into text. In: Proceedings of the conference on \nempirical methods in natural language processing (EMNLP). ACL, pp 404–411\nMikolov T, Chen K, Corrado G, Dean J (2013) Efficient estimation of word representations in vector \nspace. In: Proceedings of the Workshops of the international conference on learning representa-\ntions (ICLR)\nMistica M, Lau JH, Merrifield B, Fazio K, Baldwin T (2021) Semi-automatic triage of requests for \nfree legal assistance. In: Proceedings of the 2021 workshop on natural legal language processing \n(NLLP). ACL, pp 217–227\nMiyato T, Dai AM, Goodfellow IJ (2017) Adversarial training methods for semi-supervised text classifi-\ncation. In: Proceedings of the international conference on learning representations (ICLR)\nMorris C, Ritzert M, Fey M, Hamilton WL, Lenssen JE., Rattan G, Grohe M (2019) Weisfeiler and leman \ngo neural: higher-order graph neural networks. In: Proceedings of the AAAI Conference on Arti-\nficial Intelligence (AAAI), the conference on Innovative Applications of Artificial Intelligence \n(IAAI), the AAAI symposium on Educational Advances in Artificial Intelligence (EAAI). AAAI \nPress, pp 4602–4609\nNallapati R, Zhai F, Zhou B (2017) Summarunner: a recurrent neural network based sequence model \nfor extractive summarization of documents. In: Proceedings of the AAAI conference on Artificial \nIntelligence (AAAI). AAAI Press, pp 3075–3081\nNallapati R, Zhou B, dos Santos CN, Gülçehre Ç, Xiang B (2016) Abstractive text summarization using \nsequence-to-sequence rnns and beyond. In: Proceedings of the SIGNLL conference on computa-\ntional natural language learning (CoNLL). ACL, pp 280–290\nNarayan S, Cohen SB, Lapata M (2018) Don’t give me the details, just the summary! topic-aware convo-\nlutional neural networks for extreme summarization. In: Proceedings of the conference on empiri-\ncal methods in natural language processing (EMNLP). Association for Computational Linguistics, \npp 1797–1807\nNaseri S, Foley J, Allan J (2018) Umass at TREC 2018: car, common core and news tracks. In: Pro-\nceedings of the text REtrieval conference (TREC), volume 500-331 of NIST Special Publication. \nNational Institute of Standards and Technology (NIST)\nNguyen H (2023) A Brief Report on LawGPT 1.0: a virtual legal assistant based on GPT-3. CoRR, arXiv: \n2302. 05729\nNguyen H, Nguyen L (2021) Sublanguage: a serious issue affects pretrained models in legal domain. \nCoRR, arXiv: 2104. 07782\nNguyen H, Nguyen PM, Vuong T, Bui QM, Nguyen CM, Dang TB, Tran V, Nguyen ML, Satoh K \n(2021a) JNLP team: deep learning approaches for legal processing tasks in COLIEE 2021. In: \nProceedings of the eighth international competition on legal information extraction/entailment \n(COLIEE 2021), pp 46–53\nNguyen H, Tran V, Nguyen PM, Vuong T, Bui QM, Nguyen CM, Dang TB, Nguyen ML, Satoh K \n(2021b) Paralaw nets—cross-lingual sentence-level pretraining for legal text processing. CoRR, \narXiv: 2106. 13403\nNguyen H, Vuong HT, Nguyen PM, Dang TB, Bui QM, Sinh VT, Nguyen CM, Tran VD, Satoh K, \nNguyen ML (2020) JNLP team: deep learning for legal processing in COLIEE 2020. In: Proceed-\nings of the 14th international workshop on Juris-Informatics (JURISIN2020). The Japanese Soci-\nety of Artificial Intelligence, pp 195–208\nNguyen T, Rosenberg M, Song X, Gao J, Tiwary S, Majumder R, Deng L (2016) MS MARCO: a human \ngenerated machine reading comprehension dataset. In: Proceedings of the workshop on cognitive \ncomputation: integrating neural and symbolic approaches, co-located with the annual conference \non neural information processing systems (NIPS), volume 1773 of CEUR workshop proceedings. \nCEUR-WS.org\n1003\n1 3\nTransformer-based language models for AI and law\nNiklaus J, Chalkidis I, Stürmer M (2021) Swiss-judgment-prediction: a multilingual legal judgment pre-\ndiction benchmark. In: Proceedings of the 2021 workshop on natural legal language processing \n(NLLP). ACL, pp 19–35\nNiklaus J, Stürmer M, Chalkidis I (2022) An empirical study on cross-x transfer for legal judgment \nprediction. In :Proceedings of the conference of the Asia-Pacific chapter of the Association for \nComputational Linguistics and the international joint conference on natural language processing \n(AACL/IJCNLP). Association for Computational Linguistics, pp 32–46\nNogueira R, Jiang Z, Pradeep R., Lin J (2020) Document ranking with a pretrained sequence-to-sequence \nmodel. In: Findings of the Association for Computational Linguistics: EMNLP, volume EMNLP \n2020 of findings of ACL. Association for Computational Linguistics, pp 708–718\nOliver Wendell Holmes J (1897) The path of the law. Harv Law Rev 10:457–478\nOstendorff M, Ash E, Ruas T, Gipp B, Schneider JM, Rehm G (2021) Evaluating document representa-\ntions for content-based legal literature recommendations. In: Proceedings of the international con-\nference on artificial intelligence and law (ICAIL). ACM, pp 109–118\nOstendorff M, Blume T, Ostendorff S (2020) Towards an open platform for legal information. In: Pro-\nceedings of the ACM/IEEE joint conference on digital libraries (JCDL), pp 385–388\nOuyang L, Wu J, Jiang X, Almeida D, Wainwright CL, Mishkin P, Zhang C, Agarwal S, Slama K, Ray A, \nSchulman J, Hilton J, Kelton F, Miller L, Simens M, Askell A, Welinder P, Christiano PF, Leike J, \nLowe R (2022) Training language models to follow instructions with human feedback. In: Proceed-\nings of the annual conference on neural information processing systems (NeurIPS)\nPagliardini M, Gupta P, Jaggi M (2018) Unsupervised learning of sentence embeddings using compo-\nsitional n-gram features. In: Proceedings of the conference of the North American chapter of the \nAssociation for Computational Linguistics: Human Language Technologies (NAACL-HLT). Asso-\nciation for Computational Linguistics, pp 528–540\nPais V, Mitrofan M, Gasan CL, Coneschi V, Ianov A (2021) Named entity recognition in the Roma-\nnian legal domain. In: Proceedings of the 2021 Workshop on Natural Legal Language Processing \n(NLLP). Association for Computational Linguistics, pp 9–18\nPant K, Dadu T (2020) Cross-lingual inductive transfer to detect offensive language. CoRR, arXiv: 2007. \n03771\nPapaloukas C, Chalkidis I, Athinaios K, Pantazi D, Koubarakis M (2021) Multi-granular legal topic clas-\nsification on greek legislation. In: Proceedings of the 2021 workshop on natural legal language \nprocessing (NLLP). ACL, pp 63–75\nPaperno D, Kruszewski G, Lazaridou A, Pham QN, Bernardi R, Pezzelle S, Baroni M, Boleda G, Fernán-\ndez R (2016) The LAMBADA dataset: word prediction requiring a broad discourse context. In: \nProceedings of the annual meeting of the Association for Computational Linguistics (ACL). Asso-\nciation for Computer Linguistics\nPaul S, Goyal P, Ghosh S (2022a) Lesicin: a heterogeneous graph-based approach for automatic legal \nstatute identification from Indian legal documents. In: Proceedings of the AAAI conference on \nartificial intelligence (AAAI), the conference on Innovative Applications of Artificial Intelligence \n(IAAI), the AAAI symposium on Educational Advances in Artificial Intelligence (EAAI). AAAI \nPress, pp 11139–11146\nPaul S, Mandal A, Goyal P, Ghosh S (2022b) Pre-training Transformers on Indian legal text. CoRR, \narXiv: 2209. 06049\nPennington J, Socher R, Manning CD (2014) Glove: global vectors for word representation. In: Proceed-\nings of the conference on empirical methods in natural language processing (EMNLP). ACL, pp \n1532–1543\nPérez JM, Furman DA, Alemany LA, Luque FM (2022) RoBERTuito: a pre-trained language model for \nsocial media text in Spanish. In: Proceedings of the language resources and evaluation conference \n(LREC). European Language Resources Association, pp 7235–7243\nPfeiffer J, Vulic I, Gurevych I, Ruder S (2020) MAD-X: an adapter-based framework for multi-task cross-\nlingual transfer. In: Proceedings of the conference on empirical methods in natural language pro-\ncessing (EMNLP). Association for Computational Linguistics, pp 7654–7673\nPolignano M, Basile P, de Gemmis M, Semeraro G, Basile V (2019) AlBERTo: Italian BERT language \nunderstanding model for NLP challenging tasks based on tweets. In: Proceedings of the sixth \nItalian conference on computational linguistics, volume 2481 of CEUR workshop proceedings. \nCEUR-WS.org\n1004 C. M. Greco, A. Tagarelli \n1 3\nPolo FM, Mendonça GCF, Parreira KCJ, de  Godoy  Gianvechio L, Cordeiro P, Ferreira JB., de  Lima \nLMP, do Amaral Maia AC., Vicente R (2021) LegalNLP—natural language processing methods \nfor the Brazilian legal language. CoRR, arXiv: 2110. 15709\nPolsley S, Jhunjhunwala P, Huang R (2016) CaseSummarizer: a system for automated summarization \nof legal texts. In: Proceedings of the international conference on computational linguistics (COL -\nING): system demonstrations. ACL, pp 258–262\nPonte JM, Croft WB (1998) A language modeling approach to information retrieval. In: Proceedings of \nthe ACM SIGIR conference on research and development in information retrieval (SIGIR). ACM, \npp 275–281\nPradhan S, Moschitti A, Xue N, Uryupina O, Zhang Y. (2012) CoNLL—2012 shared task: modeling \nmultilingual unrestricted coreference in OntoNotes. In: Proceedings of the joint conference on \nempirical methods in natural language processing and computational natural language learning \n(EMNLP-CoNLL): modeling multilingual unrestricted coreference in OntoNotes. ACL, pp 1–40\nPyysalo S, Kanerva J, Virtanen A, Ginter F (2021) WikiBERT models: deep transfer learning for many lan-\nguages. In: Proceedings of the Nordic conference on computational linguistics (NoDaLiDa), pp 1–10\nQuemy A, Wrembel R (2022) ECHR-OD: on building an integrated open repository of legal documents \nfor machine learning applications. Inf Syst 106:101822\nRabelo J, Goebel R, Kim M-Y, Kano Y, Yoshioka M, Satoh K (2022) Overview and discussion of the \ncompetition on legal information extraction/entailment (COLIEE) 2021. The Review of Socionet-\nwork Strategies\nRabelo J, Kim M, Goebel R (2019a) Combining similarity and transformer methods for case law entail-\nment. In: Proceedings of the international conference on artificial intelligence and law (ICAIL). \nACM, pp 290–296\nRabelo J, Kim M, Goebel R (2020) The application of text entailment techniques in COLIEE 2020. In: \nNew frontiers in artificial intelligence—JSAI-isAI 2020 Workshops, JURISIN, LENLS 2020 work-\nshops, volume 12758 of lecture notes in computer science. Springer, pp 240–253\nRabelo J, Kim M-Y, Goebel R, Yoshioka M, Kano Y, Satoh K (2019) A summary of the COLIEE 2019 \ncompetition. New frontiers in artificial intelligence: JSAI-isAI international workshops, JURISIN, \nAI-Biz, LENLS, Kansei-AI, pp 34–49\nRadford A, Narasimhan K, Salimans T, Sutskever I (2018) Improving language understanding by genera-\ntive pre-training. OpenAI blog\nRadford A, Wu J, Child R, Luan D, Amodei D, Sutskever I (2019) Language models are unsupervised \nmultitask learners. OpenAI blog\nRaffel C, Shazeer N, Roberts A, Lee K, Narang S, Matena M, Zhou Y, Li W, Liu PJ (2020) Explor -\ning the limits of transfer learning with a unified text-to-text transformer. J Mach Learn Res \n21:140:1–140:67\nRajpurkar P, Jia R, Liang P (2018) Know what you don’t know: unanswerable questions for squad. In: \nProceedings of the annual meeting of the Association for Computational Linguistics (ACL). Asso-\nciation for Computational Linguistics, pp 784–789\nRajpurkar P, Zhang J, Lopyrev K, Liang P (2016) Squad: 100,000+ questions for machine comprehen-\nsion of text. In: Proceedings of the conference on empirical methods in natural language processing \n(EMNLP). Association for Computational Linguistics, pp 2383–2392\nRanaldi L, Fallucchi F, Santilli A, Zanzotto FM (2021) KERMITviz: visualizing neural network activa-\ntions on syntactic trees. In: Proceedings of the international conference on metadata and semantic \nresearch (MTSR), revised selected papers, volume 1537 of communications in computer and infor-\nmation science. Springer, pp 139–147\nRavichander A, Black AW, Wilson S, Norton TB, Sadeh NM (2019) Question answering for privacy poli-\ncies: combining computational and legal perspectives. In: Proceedings of the conference on empiri-\ncal methods in natural language processing and the international joint conference on natural lan-\nguage processing (EMNLP-IJCNLP). Association for Computational Linguistics, pp 4946–4957\nReimers N, Gurevych I (2019) Sentence-BERT: sentence embeddings using Siamese BERT-networks. \nIn: Proceedings of the conference on empirical methods in natural language processing and the \ninternational joint conference on natural language processing (EMNLP-IJCNLP), pp 3980–3990\nRibeiro MT, Singh S, Guestrin C (2016) “Why Should I Trust You?”: explaining the predictions of any \nclassifier. In: Proceedings of the ACM SIGKDD international conference on knowledge discovery \nand data mining. ACM, pp 1135–1144\nRogers A, Kovaleva O, Rumshisky A (2020) A primer in BERTology: what we know about how BERT \nworks. Trans Assoc Comput Linguist 8:842–866\n1005\n1 3\nTransformer-based language models for AI and law\nRosa GM, Rodrigues RC, de Alencar Lotufo R, Nogueira R (2021) To tune or not to tune?: zero-shot \nmodels for legal case entailment. In: Proceedings of the international conference on artificial intel-\nligence and law (ICAIL). ACM, pp 295–300\nRossi J Kanoulas E (2019) Legal information retrieval with generalized language models: ILPS partici-\npation to COLIEE 2019. In: Proc of COLIEE 2019 workshop: competition on legal information \nextraction/entailment\nSakata W, Shibata T, Tanaka R, Kurohashi S (2019) FAQ retrieval using query-question similarity and \nbert-based query-answer relevance. In: Proceedings of the ACM SIGIR conference on research and \ndevelopment in information retrieval (SIGIR). ACM, pp 1113–1116\nSalaün O, Langlais P, Lou A, Westermann H, Benyekhlef K (2020) Analysis and multilabel classifica-\ntion of Quebec court decisions in the domain of housing law. In: Proceedings of the international \nconference on applications of natural language to information systems (NLDB), volume 12089 of \nlecture notes in computer science. Springer, pp 135–143\nSamy D, Arenas-García J, Pérez-Fernández D (2020) Legal-ES: a set of large scale resources for Spanish \nlegal text processing. In: Proceedings of the workshop on language technologies for Government \nand Public Administration (LT4Gov@LREC 2020). European Language Resources Association, \npp 32–36\nSanchez L, He J, Manotumruksa J, Albakour D, Martinez M, Lipani A (2020) Easing legal news monitor-\ning with learning to rank and BERT. In: Proceedings of the European conference on IR research \n(ECIR), volume 12036 of lecture notes in computer science. Springer, pp 336–343\nSang EFTK (2002) Introduction to the CoNLL-2002 shared task: language-independent named entity \nrecognition. In: Proceedings of the conference on natural language learning (CoNLL). ACL\nSang EFTK, Meulder FD (2003) Introduction to the CoNLL-2003 shared task: language-independ-\nent named entity recognition. In: Proceedings of the conference on natural language learning \n(CoNLL). ACL, pp 142–147\nSanh V, Debut L, Chaumond J, Wolf T (2019) Distilbert, a distilled version of BERT: smaller, faster, \ncheaper and lighter. CoRR, arXiv: 1910. 01108\nSansone C, Sperlí G (2022) Legal information retrieval systems: state-of-the-art and open issues. Inf Syst \n106:101967\nSantosh TYSS, Xu S, Ichim O, Grabmair M (2022) Deconfounding legal judgment prediction for Euro-\npean court of human rights cases towards better alignment with experts. In: Proceedings of the \nconference on empirical methods in natural language processing (EMNLP). Association for Com-\nputational Linguistics, pp 1120–1138\nSarkar, R., Ojha, A. K., Megaro, J., Mariano, J., Herard, V., and McCrae, J. P. (2021). Few-shot and Zero-\nshot Approaches to Legal Text Classification: A Case Study in the Financial Sector. In Proc. of the \n2021 Workshop on Natural Legal Language Processing (NLLP), pages 102—106. ACL\nSavelka J, Ashley KD (2021) Discovering explanatory sentences in legal case decisions using pre-trained \nlanguage models. In: Findings of the Association for Computational Linguistics: EMNLP 2021. \nAssociation for Computational Linguistics, pp 4273–4283\nSavelka J, Ashley KD (2022) Legal information retrieval for understanding statutory terms. Artif Intell \nLaw 30(2):245–289\nSavelka J, Westermann H, Benyekhlef K (2020) Cross-domain generalization and knowledge transfer in \ntransformers trained on legal data. In: Proceedings of the fourth workshop on automated semantic \nanalysis of information in legal text (ASAIL) in conjunction with the international conference on \nlegal knowledge and information systems (JURIX), volume 2764 of CEUR workshop proceedings. \nCEUR-WS.org\nSavelka J, Westermann H, Benyekhlef K, Alexander CS, Grant JC, Amariles DR, Hamdani RE, Meeùs S, \nTroussel AC, Araszkiewicz M, Ashley KD, Ashley A, Branting K, Falduti M, Grabmair M, Har -\nasta J, Novotná T, Tippett E, Johnson S (2021) Lex Rosetta: transfer of predictive models across \nlanguages, jurisdictions, and legal domains. In: Proceedings of the international conference on arti-\nficial intelligence and law (ICAIL). ACM, pp 129–138\nSchilder F, Chinnappa D, Madan K, Harmouche J, Vold A, Bretz H, Hudzina J (2021) A Pentapus grap-\nples with legal reasoning. In: Proceedings of the eighth international competition on legal informa-\ntion extraction/entailment (COLIEE 2021), pp 60–68\nSchulman J, Wolski F, Dhariwal P, Radford A, Klimov O (2017) Proximal policy optimization algo-\nrithms. CoRR, arXiv: 1707. 06347\n1006 C. M. Greco, A. Tagarelli \n1 3\nSee A, Liu PJ, Manning CD (2017) Get to the point: summarization with pointer-generator networks. In: \nProceedings of the annual meeting of the Association for Computational Linguistics (ACL). Asso-\nciation for Computational Linguistics, pp 1073–1083\nSeker A, Bandel E, Bareket D, Brusilovsky I, Greenfeld RS, Tsarfaty R (2021) AlephBERT: a hebrew \nlarge pre-trained language model to start-off your hebrew NLP application with. CoRR, arXiv: \n2104. 04052\nSennrich R, Haddow B, Birch A (2016) Neural machine translation of rare words with subword units. In: \nProceedings of the annual meeting of the Association for Computational Linguistics (ACL). Asso-\nciation for Computational Linguistics\nSerras FR, Finger M (2022) verBERT: automating Brazilian case law document multi-label categoriza-\ntion using BERT. CoRR, arXiv: 2203. 06224\nShaghaghian S, Feng LY, Jafarpour B, Pogrebnyakov N (2020) Customizing contextualized language \nmodels for legal document reviews. In: Proceedings of the IEEE international conf. on big data \n(BigData). IEEE, pp 2139–2148\nShaheen Z, Wohlgenannt G, Filtz E (2020) Large scale legal text classification using transformer models. \nCoRR, arXiv: 2010. 12871\nShaheen Z, Wohlgenannt G, Muromtsev D (2021) Zero-shot cross-lingual transfer in legal domain using \ntransformer models. CoRR, arXiv: 2111. 14192\nShao H, Chen Y, Huang S (2020a) BERT-based ensemble model for statute law retrieval and legal infor-\nmation entailment. In: New frontiers in artificial intelligence—JSAI-isAI 2020 workshops, JURI-\nSIN, LENLS 2020 workshops, volume 12758 of lecture notes in computer science. Springer, pp \n226–239\nShao Y, Liu B, Mao J, Liu Y, Zhang M, Ma S (2020b) Thuir@coliee-2020: leveraging semantic under -\nstanding and exact matching for legal case retrieval and entailment. CoRR, arXiv: 2012. 13102\nShao Y, Mao J, Liu Y, Ma W, Satoh K, Zhang M, Ma S (2020c) BERT-PLI: modeling paragraph-level \ninteractions for legal case retrieval. In: Proceedings of the international joint conference on artifi-\ncial intelligence (IJCAI), pp 3501–3507\nShen Z, Lo K, Yu L, Dahlberg N, Schlanger M, Downey D (2022) Multi-LexSum: real-world summaries \nof civil rights lawsuits at multiple granularities. In: Proceedings of the annual conference on neural \ninformation processing systems (NeurIPS)\nShliazhko O, Fenogenova A, Tikhonova M, Mikhailov V, Kozlova A, Shavrina T (2022) mgpt: few-shot \nlearners go multilingual. CoRR, arXiv: 2204. 07580\nShukla A, Bhattacharya P, Poddar S, Mukherjee R, Ghosh K, Goyal P, Ghosh S (2022) Legal case docu-\nment summarization: extractive and abstractive methods and their evaluation. In: Proceedings of \nthe conference of the Asia-Pacific chapter of the Association for Computational Linguistics and the \ninternational joint conference on natural language processing (AACL/IJCNLP)\nSimeri A, Tagarelli A (2023) Exploring domain and task adaptation of LamBERTa models for article \nretrieval on the Italian Civil Code. In: Proceedings of the conference on information and research \nscience connecting to digital and library science (IRCDL), volume 3365 of CEUR workshop pro-\nceedings. CEUR-WS.org, pp 130–143\nSong D, Gao S, He B, Schilder F (2022) On the effectiveness of pre-trained language models for legal \nnatural language processing: an empirical study. IEEE Access 10:75835–75858\nSong F, Croft WB (1999) A general language model for information retrieval. In: Proceedings of the \nACM conference on information and knowledge management (CIKM). ACM, pp 316–321\nSong K, Tan X, Qin T, Lu J, Liu T (2019) MASS: masked sequence to sequence pre-training for lan-\nguage generation. In: Proceedings of the international conference on machine learning (ICML), pp \n5926–5936\nSouza F, Nogueira R, de Alencar Lotufo R (2020) BERTimbau: pretrained BERT models for Brazilian \nPortuguese. In: Proceedings of the Brazilian conferenec on intelligent systems (BRACIS), volume \n12319 of lecture notes in computer science. Springer, pp 403–417\nSteinberger R, Pouliquen B, Widiger A, Ignat C, Erjavec T, Tufis D, Varga D (2006) The JRC-Acquis: a \nmultilingual aligned parallel corpus with 20+ languages. In: Proceedings of the international con-\nference on language resources and evaluation (LREC). European Language Resources Association \n(ELRA), pp 2142–2147\nStrickson B, Iglesia BDL (2020) Legal judgement prediction for UK courts. In: Proceedings of the inter -\nnational conference on information science and systems (ICISS). ACM, pp 204–209\nSu J (2021) T5 pegasus—zhuiyiai. Technical report\n1007\n1 3\nTransformer-based language models for AI and law\nSu J, Lu Y, Pan S, Wen B, Liu Y (2021) Roformer: enhanced transformer with rotary position embed-\nding. CoRR, arXiv: 2104. 09864\nSun M, Guo Z, Deng X (2021) Intelligent bert-bilstm-crf based legal case entity recognition method. In: \nProceedings of the ACM turing award celebration conference. ACM, pp 186–191\nSundararajan M, Taly A, Yan Q (2017) Axiomatic attribution for deep networks. In: Proceedings of the \ninternational conference on machine learning (ICML), volume 70 of proceedings of machine learn-\ning research. PMLR, pp 3319–3328\nSurden H (2019) Artificial intelligence and law: an overview. 35 GA. ST. U. L. REV., 1305\nTagarelli A, Simeri A (2022) Unsupervised law article mining based on deep pre-trained language repre-\nsentation models with application to the Italian civil code. Artif Intell Law 30(3):417–473\nTang L Clematide S (2021) Searching for legal documents at paragraph level: automating label generation \nand use of an extended attention mask for boosting neural models of semantic similarity. In: Pro-\nceedings of the 2021 workshop on natural legal language processing (NLLP). ACL, pp 114–122\nTarasconi F, Botros M, Caserio M, Sportelli G, Giacalone G, Uttini C, Vignati L, Zanetta F (2020) Natu-\nral language processing applications in case-law text publishing. In: Proceedings of the conference \non legal knowledge and information systems (JURIX), volume 334 of frontiers in artificial intel-\nligence and applications. IOS Press, pp 154–163\nTrautmann D, Petrova A, Schilder F (2022) Legal prompt engineering for multilingual legal judgement \nprediction. CoRR, arXiv: 2212. 02199\nTrias F, Wang H, Jaume S, Idreos S (2021) Named entity recognition in historic legal text: a transformer \nand state machine ensemble method. In: Proceedings of the 2021 workshop on natural legal lan-\nguage processing (NLLP). ACL, pp 172–179\nTriguero I, García S, Herrera F (2015) Self-labeled techniques for semi-supervised learning: taxonomy, \nsoftware and empirical study. Knowl Inf Syst 42(2):245–284\nTrivedi H, Kwon H, Khot T, Sabharwal A, Balasubramanian N (2019) Repurposing entailment for multi-\nhop question answering tasks. In: Proceedings of the conference of the North American chapter \nof the Association for Computational Linguistics: human language technologies (NAACL-HLT). \nAssociation for Computational Linguistics, pp 2948–2958\nTsarapatsanis D, Aletras N (2021) On the ethical limits of natural language processing on legal text. In: \nFindings of the Association for Computational Linguistics: ACL/IJCNLP 2021. Association for \nComputational Linguistics, pp 3590–3599\nTuggener D, von Däniken P, Peetz T, Cieliebak M (2020) LEDGAR: a large-scale multi-label corpus \nfor text classification of legal provisions in contracts. In: Proceedings of the international confer -\nence on language resources and evaluation (LREC). European Language Resources Association, \npp 1235–1241\nTziafas G, de Saint-Phalle E, de Vries W, Egger C, Caselli T (2021) A multilingual approach to identify \nand classify exceptional measures against COVID-19. In: Proceedings of the 2021 workshop on \nnatural legal language processing (NLLP). ACL, pp 46–62\nUrchs S, Mitrovic J, Granitzer M (2021) Design and implementation of German legal decision corpora. \nIn: Proceedings of the international conference on agents and artificial intelligence (ICAART), pp \n515–521\nVaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser L, Polosukhin I (2017) Atten-\ntion is all you need. In: Proceedings of the annual conference on neural information processing \nsystems (NeurIPS), pp 5998–6008\nVerma S, Nidhi V (2018) Extractive summarization using deep learning. Res Comput Sci 147(10):107–117\nVig J (2019) A multiscale visualization of attention in the transformer model. In: Proceedings of the \nannual meeting of the Association for Computational Linguistics (ACL): system demonstrations\nVold A, Conrad JG (2021) Using transformers to improve answer retrieval for legal questions. In: Proceed-\nings of the international conference on artificial intelligence and law (ICAIL). ACM, pp 245–249\nVoorhees EM (2004) Overview of the TREC 2004 robust track. In: Proceedings of the Text REtrieval \nConference (TREC), volume 500-261 of NIST special publication. National Institute of Standards \nand Technology (NIST)\nVuong T, Bui QM, Nguyen H, Nguyen T, Tran V, Phan X, Satoh K, Le NM (2023) SM-BERT-CR: a deep \nlearning approach for case law retrieval with supporting model. Artif Intell Law 31(3):601–628\nWalker VR, Pillaipakkamnatt K, Davidson AM, Linares M, Pesce DJ (2019) Automatic classification of \nrhetorical roles for sentences: comparing rule-based scripts with machine learning. In: Proceed-\nings of the third workshop on automated semantic analysis of information in legal texts, co-located \n1008 C. M. Greco, A. Tagarelli \n1 3\nwith the international conference on artificial intelligence and law (ICAIL), volume 2385 of CEUR \nWorkshop Proceedings. CEUR-WS.org\nWang A, Pruksachatkun Y, Nangia N, Singh A, Michael J, Hill F, Levy O, Bowman SR (2019) Super -\nGLUE: a stickier benchmark for general-purpose language understanding systems. In: Proceedings \nof the annual conference on neural information processing systems (NeurIPS), pp 3261–3275\nWang A, Singh A, Michael J, Hill F, Levy O, Bowman SR (2018) GLUE: a multi-task benchmark and \nanalysis platform for natural language understanding. In: Proceedings of the workshop on ana-\nlyzing and interpreting neural networks for NLP, BlackboxNLP@EMNLP 2018. Association for \nComputational Linguistics, pp 353–355\nWang Q, Zhao K, Amor R, Liu B, Wang R (2022) D2GCLF: document-to-graph classifier for legal docu-\nment classification. In: Findings of the Association for Computational Linguistics: NAACL. Asso-\nciation for Computational Linguistics, pp 2208–2221\nWang S, Khabsa M, Ma H (2020a) To pretrain or not to pretrain: examining the benefits of pretraining \non resource rich tasks. In: Proceedings of the annual meeting of the Association for Computational \nLinguistics (ACL). Association for Computational Linguistics, pp 2209–2213\nWang SH, Scardigli A, Tang L, Chen W, Levkin D, Chen A, Ball S, Woodside T, Zhang O, Hendry -\ncks D (2023) MAUD: an expert-annotated legal NLP dataset for merger agreement understanding. \nCoRR, arXiv: 2301. 00876\nWang W, Wei F, Dong L, Bao H, Yang N, Zhou M (2020b) MiniLM: deep self-attention distillation for \ntask-agnostic compression of pre-trained transformers. In: Proceedings of the annual conference on \nneural information processing systems (NeurIPS)\nWang Y, Xiao C, Ma S, Zhong H, Tu C, Zhang T, Liu Z, Sun M (2021) Equality before the law: legal \njudgment consistency analysis for fairness. CoRR, arXiv: 2103. 13868\nWehnert S, Dureja S, Kutty L, Sudhi V, Luca EWD (2022) Applying BERT embeddings to predict \nlegal textual entailment. Rev. Socionetwork Strateg. 16(1):197–219\nWehnert S, Sudhi V, Dureja S, Kutty L, Shahania S, Luca EWD (2021) Legal norm retrieval with \nvariations of the BERT model combined with TF-IDF vectorization. In: Proceedings of the \ninternational conference on artificial intelligence and law (ICAIL). ACM\nWelbl J, Stenetorp P, Riedel S (2018) Constructing datasets for multi-hop reading comprehension \nacross documents. Trans Assoc Comput Linguist 6:287–302\nWestermann H, Savelka J, Benyekhlef K (2020) Paragraph similarity scoring and fine-tuned BERT for \nlegal information retrieval and entailment. In: New frontiers in artificial intelligence—SAI-isAI \n2020 workshops, JURISIN, LENLS 2020 Workshops, volume 12758 of lecture notes in com-\nputer science. Springer, pp 269–285\nWilson S, Schaub F, Dara AA, Liu F, Cherivirala S, Leon PG, Andersen MS, Zimmeck S, Sathyendra \nKM, Russell NC, Norton TB, Hovy EH, Reidenberg JR, Sadeh NM (2016) The creation and \nanalysis of a website privacy policy corpus. In: Proceedings of the annual meeting of the Asso-\nciation for Computational Linguistics (ACL). Association for Computer Linguistics\nWrzalik M, Krechel D (2021) GerDaLIR: a German dataset for legal information retrieval. In: Pro-\nceedings of the 2021 workshop on natural legal language processing (NLLP). ACL, pp 123–128\nWu Y, Schuster M, Chen Z, Le QV, Norouzi M, Macherey W, Krikun M, Cao Y, Gao Q, Macherey \nK, Klingner J, Shah A, Johnson M, Liu X, Kaiser L, Gouws S, Kato Y, Kudo T, Kazawa H, \nStevens K, Kurian G, Patil N, Wang W, Young C, Smith J, Riesa J, Rudnick A, Vinyals O, Cor -\nrado G, Hughes M, Dean J (2016) Google’s neural machine translation system: bridging the gap \nbetween human and machine translation. CoRR, arXiv: 1609. 08144\nXiao C, Hu X, Liu Z, Tu C, Sun M (2021) Lawformer: a pre-trained language model for Chinese legal \nlong documents. AI Open 2:79–84\nXiao C, Zhong H, Guo Z, Tu C, Liu Z, Sun M, Feng Y, Han X, Hu Z, Wang H, Xu J (2018) CAIL2018: \na large-scale legal dataset for judgment prediction. CoRR, arXiv: 1807. 02478\nXiao C, Zhong H, Guo Z, Tu C, Liu Z, Sun M, Zhang T, Han X, Hu Z, Wang H, Xu J (2019) \nCAIL2019-SCM: a dataset of similar case matching in legal domain. CoRR, arXiv: 1911. 08962\nXiao W, Beltagy I, Carenini G, Cohan A (2022) PRIMERA: pyramid-based masked sentence pre-train-\ning for multi-document summarization. In: Proceedings of the annual meeting of the Association \nfor Computational Linguistics (ACL). Association for Computational Linguistics, pp 5245–5263\nXiong C, Callan J, Liu T (2017) Word-entity duet representations for document ranking. In: Pro-\nceedings of the ACM SIGIR conference on research and development in information retrieval \n(SIGIR). ACM, pp 763–772\n1009\n1 3\nTransformer-based language models for AI and law\nXue L, Constant N, Roberts A, Kale M, Al-Rfou R, Siddhant A, Barua A, Raffel C (2021) mT5: a \nmassively multilingual pre-trained text-to-text transformer. In: Proceedings of the conference of \nthe North American chapter of the Association for Computational Linguistics: human language \ntechnologies (NAACL-HLT). Association for Computational Linguistics, pp 483–498\nYamada H, Tokunaga T (2019) A performance study on fine-tuned large language models in the Legal \nCase Entailment Task. In: Proceedings of COLIEE 2019 workshop: competition on legal infor -\nmation extraction/entailment\nYang J, Jin H, Tang R, Han X, Feng Q, Jiang H, Yin B, Hu X (2023) Harnessing the power of LLMs \nin practice: a survey on ChatGPT and beyond. CoRR, arXiv: 2304. 13712 v2\nYang Z, Dai Z, Yang Y, Carbonell JG, Salakhutdinov R, Le QV (2019) XLNet: generalized autore-\ngressive pretraining for language understanding. In: Proceedings of the annual conference on \nneural information processing systems (NeurIPS), pp 5754–5764\nYang Z, Yang D, Dyer C, He X, Smola AJ, Hovy EH (2016) Hierarchical attention networks for docu-\nment classification. In: Proceedings of the conferenec of the North American chapter of the \nAssociation for Computational Linguistics: human language technologies (NAACL-HLT). \nAssociation for Computational Linguistics, pp 1480–1489\nYoon J, Junaid M, Ali S, Lee J (2022) Abstractive summarization of Korean legal cases using pre-\ntrained language models. In: Proceedings of the international conference on ubiquitous infor -\nmation management and communication (IMCOM). IEEE, pp 1–7\nYoshioka M, Aoki Y, Suzuki Y (2021a) BERT-based ensemble methods with data augmentation for \nlegal textual entailment in COLIEE statute law task. In: Proceedings of the international confer -\nence on artificial intelligence and law (ICAIL). ACM, pp 278–284\nYoshioka M, Suzuki Y, Aoki Y (2021b) BERT-based ensemble methods for information retrieval and \nlegal textual entailment in COLIEE statute law task. In: Proceedings of the eighth international \ncompetition on legal information extraction/entailment (COLIEE 2021), pp 78–83\nYu F, Quartey L, Schilder F (2022a) Legal prompting: teaching a language model to think like a lawyer. \nCoRR, arXiv: 2212. 01326\nYu W, Sun Z, Xu J, Dong Z, Chen X, Xu H, Wen J (2022b) Explainable legal case matching via inverse \noptimal transport-based rationale extraction. In: Proceedings of the ACM SIGIR conference on \nresearch and development in information retrieval (SIGIR). ACM, pp 657–668\nYun S, Jeong M, Kim R, Kang J, Kim HJ (2019) Graph transformer networks. In: Proceedings of the \nannual conference on neural information processing systems (NeurIPS), pp 11960–11970\nZaheer M, Guruganesh G, Dubey KA, Ainslie J, Alberti C, Ontañón S, Pham P, Ravula A, Wang Q, Yang \nL, Ahmed A (2020) Big Bird: transformers for longer sequences. In: Proceedings of the annual \nconference on neural information processing systems (NeurIPS)\nZanzotto FM, Santilli A, Ranaldi L, Onorati D, Tommasino P, Fallucchi F (2020) KERMIT: comple-\nmenting transformer architectures with encoders of explicit syntactic interpretations. In: Proceed-\nings of the conference on empirical methods in natural language processing (EMNLP). Association \nfor Computational Linguistics, pp 256–267\nZeiler MD, Fergus R (2014) Visualizing and understanding convolutional networks. In: Proceedings of \nthe European conference on computer vision (ECCV), volume 8689 of lecture notes in computer \nscience. Springer, pp 818–833\nZellers R, Bisk Y, Schwartz R, Choi Y (2018) SWAG: a large-scale adversarial dataset for grounded com-\nmonsense inference. In: Proceedings of the conference on empirical methods in natural language \nprocessing (EMNLP). Association for Computational Linguistics, pp 93–104\nZhang B, Xiong D, Su J (2018) Accelerating neural transformer via an average attention network. In: Pro-\nceedings of the annual meeting of the Association for Computational Linguistics (ACL). Associa-\ntion for Computational Linguistics, pp 1789–1798\nZhang J, Zhao Y, Saleh M, Liu PJ (2020) PEGASUS: pre-training with extracted gap-sentences for \nabstractive summarization. In: Proceedings of the international conference on machine learning \n(ICML), pp 11328–11339\nZheng L, Guha N, Anderson BR, Henderson P, Ho DE (2021) When does pretraining help? Assessing self-\nsupervised learning for law and the CaseHOLD dataset of 53,000+ legal holdings. In: Proceedings \nof the international conference on artificial intelligence and law (ICAIL). ACM, pp 159–168\nZhong H, Guo Z, Tu C, Xiao C, Liu Z, Sun M (2018) Legal judgment prediction via topological learning. \nIn: Proceedings of the conference on empirical methods in natural language processing (EMNLP). \nAssociation for Computational Linguistics, pp 3540–3549\n1010 C. M. Greco, A. Tagarelli \n1 3\nZhong H, Xiao C, Tu C, Zhang T, Liu Z, Sun M (2020) How does NLP benefit legal system: a summary \nof legal artificial intelligence. In: Proceedings of the annual meeting of the Association for Compu-\ntational Linguistics. Association for Computational Linguistics, pp 5218–5230\nZhong H, Zhang Z, Liu Z, Sun M (2019a) Open Chinese language pre-trained model zoo. Technical \nreport\nZhong L, Zhong Z, Zhao Z, Wang S, Ashley KD, Grabmair M (2019b) Automatic summarization of legal \ndecisions using iterative masking of predictive sentences. In: Proceedings of the international con-\nference on artificial intelligence and law (ICAIL). ACM, pp 163–172\nZimmeck S, Story P, Smullen D, Ravichander A, Wang Z, Reidenberg JR, Russell NC, Sadeh NM (2019) \nMAPS: scaling privacy compliance analysis to a million apps. Proc Privacy Enhanc Technol \n2019(3):66–86\nPublisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps \nand institutional affiliations."
}