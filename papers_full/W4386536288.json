{
  "title": "Prostate Segmentation in MRI Using Transformer Encoder and Decoder Framework",
  "url": "https://openalex.org/W4386536288",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5102747056",
      "name": "Chengjuan Ren",
      "affiliations": [
        null,
        "Sichuan International Studies University"
      ]
    },
    {
      "id": "https://openalex.org/A5105517691",
      "name": "Z. J. Guo",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A5018389832",
      "name": "Huipeng Ren",
      "affiliations": [
        "Baoji City Central Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A5085683467",
      "name": "Dongwon Jeong",
      "affiliations": [
        "Kunsan National University"
      ]
    },
    {
      "id": "https://openalex.org/A5064013985",
      "name": "Dae-Kyoo Kim",
      "affiliations": [
        "Oakland University"
      ]
    },
    {
      "id": "https://openalex.org/A5074257788",
      "name": "Shiyan Zhang",
      "affiliations": [
        "Baoji University of Arts and Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A5100727959",
      "name": "Jiacheng Wang",
      "affiliations": [
        "Baoji University of Arts and Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A5103058808",
      "name": "Guangnan Zhang",
      "affiliations": [
        "Baoji University of Arts and Sciences"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1967795278",
    "https://openalex.org/W6703049675",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3204614423",
    "https://openalex.org/W4293791153",
    "https://openalex.org/W3173922584",
    "https://openalex.org/W2133287637",
    "https://openalex.org/W2981073666",
    "https://openalex.org/W4313453779",
    "https://openalex.org/W6779163297",
    "https://openalex.org/W2118681954",
    "https://openalex.org/W1917894041",
    "https://openalex.org/W4205470984",
    "https://openalex.org/W2896001621",
    "https://openalex.org/W4224304134",
    "https://openalex.org/W4321483852",
    "https://openalex.org/W2112224175",
    "https://openalex.org/W3217483830",
    "https://openalex.org/W2748604568",
    "https://openalex.org/W4200068632",
    "https://openalex.org/W4226323122",
    "https://openalex.org/W1827911007",
    "https://openalex.org/W4207020357",
    "https://openalex.org/W3120137913",
    "https://openalex.org/W4289544270",
    "https://openalex.org/W2003854483",
    "https://openalex.org/W2137866361",
    "https://openalex.org/W2919308409",
    "https://openalex.org/W2593423469",
    "https://openalex.org/W2333796428",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4225660113",
    "https://openalex.org/W3033529678"
  ],
  "abstract": "To develop an accurate segmentation model for the prostate and lesion area to help clinicians diagnose diseases, we propose a multi-encoder and decoder segmentation network, denoted Muled-Net, which can concurrently segment the prostate and lesion regions in an image. The model performs parallel calculations for dual input. In two encoder branches of the model, a new transformer encoder is used to overcome the fact that only information from the neighborhood pixels can be captured, increasing the ability to capture global dependencies. Furthermore, given the usually small size of the lesion, ASPP and feature fusion are merged to expand the perceptual field and retain more contextual information of the shallow layer in decoder. To the best of our limited knowledge, there is no public dataset for the segmentation of the prostate and its lesion regions. So we made a publicly usable dataset. Muled-Net is compared with other deep learning methods, FCN, U-Net, U-Net++, and ResU-Net with four-fold cross-validation. Of all 218 subjects, 140 healthy individuals and 78 patients with prostate cancer were included in this work. Average Dice of 95&#x0025;, Iou of 89&#x0025;, sensitivity of 94&#x0025;, 95HD of 9.56, and MSD of 0.66 are achieved for the prostate segmentation and average Dice of 89&#x0025;, Iou of 82&#x0025;, sensitivity of 92&#x0025;, 95HD of 11.16, and MSD of 1.09 for the segmentation of the prostate lesion regions. The performance of the proposed model has made significant improvements to the segmentation of the lesion regions in particular, suggesting that the model could be considered as an auxiliary tool to ease the workload of physicians and help them in making treatment decisions.",
  "full_text": "Received 25 July 2023, accepted 7 August 2023, date of publication 8 September 2023, date of current version 21 September 2023.\nDigital Object Identifier 10.1 109/ACCESS.2023.3313420\nProstate Segmentation in MRI Using Transformer\nEncoder and Decoder Framework\nCHENGJUAN REN\n 1,2, ZIYU GUO3, HUIPENG REN4, DONGWON JEONG5, DAE-KYOO KIM6,\nSHIYAN ZHANG7, JIACHENG WANG7, AND GUANGNAN ZHANG7\n1Guangdong Atv Academy for Performing Arts, Zhaoqing 526000, China\n2College of Language Intelligence, Sichuan International Studies University, Chongqing 400031, China\n3Department of Computer Science and Engineering, The Chinese University of Hong Kong, HongKong 999077, China\n4Department of Medical Imaging, Baoji Central Hospital, Baoji 721008, China\n5Software Convergence Engineering Department, Kunsan National University, Gunsan 54150, South Korea\n6Computer Science and Engineering Department, Oakland University, MI 48309, USA\n7School of Computer, Baoji University of Arts and Sciences, Baoji 721008, China\nCorresponding author: Guangnan Zhang (zgn_2003@163.com)\nThis work was supported by the Science and Technology Research Project of Chongqing Municipal Education Commission in 2023 under\nGrant KJQN202300901.\nABSTRACT To develop an accurate segmentation model for the prostate and lesion area to help clinicians\ndiagnose diseases, we propose a multi-encoder and decoder segmentation network, denoted Muled-Net,\nwhich can concurrently segment the prostate and lesion regions in an image. The model performs parallel\ncalculations for dual input. In two encoder branches of the model, a new transformer encoder is used to\novercome the fact that only information from the neighborhood pixels can be captured, increasing the ability\nto capture global dependencies. Furthermore, given the usually small size of the lesion, ASPP and feature\nfusion are merged to expand the perceptual field and retain more contextual information of the shallow\nlayer in decoder. To the best of our limited knowledge, there is no public dataset for the segmentation of\nthe prostate and its lesion regions. So we made a publicly usable dataset. Muled-Net is compared with\nother deep learning methods, FCN, U-Net, U-Net++, and ResU-Net with four-fold cross-validation. Of all\n218 subjects, 140 healthy individuals and 78 patients with prostate cancer were included in this work.\nAverage Dice of 95%, Iou of 89%, sensitivity of 94%, 95HD of 9.56, and MSD of 0.66 are achieved for\nthe prostate segmentation and average Dice of 89%, Iou of 82%, sensitivity of 92%, 95HD of 11.16, and\nMSD of 1.09 for the segmentation of the prostate lesion regions. The performance of the proposed model has\nmade significant improvements to the segmentation of the lesion regions in particular, suggesting that the\nmodel could be considered as an auxiliary tool to ease the workload of physicians and help them in making\ntreatment decisions.\nINDEX TERMS Diagnose diseases, segment prostate, multi-encoder and decoder, feature fusion, ASPP.\nI. INTRODUCTION\nProstate cancer (PCa) is the second most diagnosed cancer\nwith a high morbidity rate [1]. The estimated number of PCa\ndiagnoses will increase to 1,700,000 worldwide by 2030 and\ncould potentially be associated with up to 50,000 deaths per\nyear [2]. Prostate cancer has turned out to be a predomi-\nnantly global public health concern. However, prostate cancer\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Rajeeb Dey\n.\nremains poorly understood. Although age, ethnicity, and fam-\nily history are most heavily correlated with PCa disease,\nno avoidable risk factors have been discovered. Efforts will\nconsequently be made to continue to establish an accurate\nearly diagnosis and effective treatment for PCa.\nMost prostate tumors are benign in the early stages, with\nno cancer cells and gradually increasing in size. Malignant\ntumors, on the other hand, have more rapid growth than\ncancerous cells. The preliminary inspection and screening\nof the prostate are carried out by prostate-specific antigen\n101630\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License.\nFor more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/ VOLUME 11, 2023\nC. Ren et al.: Prostate Segmentation in MRI Using Transformer Encoder and Decoder Framework\n(PSA) and digital rectal examination (DRE). A transrectal\nultrasound (TURS) biopsy is performed if the PSA result is\npositive. However, because of the low ultrasound resolution\nof TRUS or promotion in benign conditions, the specificity\nof PSA to PCa alone is low [3]. Even the Gleason score (GS)\nascertained from the biopsy sample may deviate in repeat\nbiopsies. It also sometimes varies from the score determined\nby radical prostatectomy [4].\nBased on multiparametric magnetic resonance imaging\n(mp-MRI) with superior soft tissue contrast, it improves\nresolution and is a radiation-free mode. It is being inte-\ngrated into PCa diagnostic routes for non-invasive detection,\ntargeted biopsy and treatment programming. ‘‘Prostate Imag-\ning Reporting and Data System’’ (PI-RADS v2) states\nT2-weighted imaging (T2WI), diffusion-weighted imaging\n(DWI), and dynamic contrast-enhanced imaging (DCEI) are\nsome generally gathered sequences in cancer diagnosis [5].\nThe aggregation of different sequences also leads to a higher\ntime complexity of the model. Recent investigations have\nindicated that multiparametric magnetic resonance imaging\nmay afford a more precise method for PCa detection and\nsegmentation. Cao et al. [6] proposed a novel convolutional\nneural network (CNN) framework to classify the clinically\nsignificant prostate cancer (CSPCa) or not CSPCa for center\nlesion regions. The model got a high area under the receiver\noperating characteristic curve (ROC AUC). The authors\npointed out that the model was strongly data-dependent,\nwhich is a difficulty to overcome in the future. Saha et al. [7]\nexamined various tactics to target attention to the peripheral\nzone (PZ) and transition zone (TZ). The probabilistic par-\ntition maps and the partitioned feature maps are combined\nbefore the final convolution. The performance of their models\nis somewhat improved, but not any better statistically than\nwhen the partitioning information is elided. To address the\nlack of specificity of diagnostic methods for the aggressive\nmanifestations of the illness, Koc et al. [8] developed a trans-\nfer learning model to extract features, select features with\nNCA and classify PCa. While the use of transfer learning\nsolves the small sample problem, the characteristics of the\ndata are yet to be further explored. Yildirim et al. [9] pre-\nsented a mixture model to illustrate MRI inspection and\nforecast PI-RADS scores and achieved 96.09% accuracy.\nAlthough excellent accuracy was obtained, it is a great chal-\nlenge to implement the model, due to the complexity of\nthe model and the high demand for hardware. There is also\nan untold amount of outstanding work for PCa detection\nand segmentation here, such as the integration of different\nsequences to enhance the performance of the model for the\ndetection of high-grade PCa [10].\nHowever, there are still many difficulties in this field,\nfor example, the dissimilarity of prostate tissue, the\nlack of well-defined prostate boundaries, and the large\ninter-individual variation in the shape of the prostate\nand lesion region. Furthermore, there is also substantial\ninter/intra-observer variability in the manual interpretation by\nradiologists. Therefore, an automated and accurate segmenta-\ntion of prostate and lesion regions from mp-MRI sequences is\ndeveloped to minimize the reading time and ease the require-\nment for radiological expertise. Meantime, during the biopsy\nphase, the model provides the physician with the number\nof suspicious lesion candidates and limits the number of\nunnecessary biopsies.\nIn general, the major contributions of this research consist\nof;\n(1). We propose an automatic model for segmenting the\nprostate and its lesion regions, which adopts the popular\nencoder-decoder structure. Our model has achieved excellent\nwith the real data set, which can be realized to help clinicians\nwith diagnosis.\n(2). Self-attention is incorporated into convolutional neural\nnetworks as part of the transformer to boost object segmen-\ntation. Then a new transformer encoder is embedded in two\nencoder branches in order to mine long-range relationships in\nthe image.\n(3). Due to weak pixels in the area of the prostate lesion,\natrous spatial pyramid pooling (ASPP) and feature fusion\ntechniques are merged to expand the perceptual field and\nretain more applicable information about the shallow layers\nin the model.\n(4). To the best of our limited knowledge, there is no public\ndataset that can be used to segment both the prostate and the\nlesion area. We constructed a publicly available and real data\nset.\nIn the rest of the paper, we first present the material in\nsection II, including the data set and the pre-processing.\nIn Section III, we discuss our methodological principle. The\nexperiment is shown in section IV. The discussion and future\nresearch direction are described in section V.\nII. MATERIAL\nA. DATASETS\nWe made a publicly usable data set for the segmenta-\ntion of the prostate and its lesion regions. These data\nwere performed through MRI of 218 patients (140 normal\nand 78 PCa patients). Data were obtained during the period\nJanuary 2018 to September 2021 using the GE3.0T 750 MR.\nAll T2W images used a turbo spin-echo sequence with\n∼0.5 × 0.5 mm planar resolution and 3.6 mm slice thick-\nness. DWI was gained to 2 mm planar resolution and a slice\nthickness of 3.6 mm using a single echo planar imaging\nsequence with diffusion-encoded gradients in three direc-\ntions. b value includes 50,400,800s/mm2. Due to being\nlack of data, b only is 800s/mm2. Patients were diag-\nnosed with prostatic lesion regions by TRUS-guided prostate\nbiopsy. Informed consent was obtained from all patients and\ngroups. Three experienced prostate clinicians were involved\nin the analysis and annotation of the prostate MR imag-\ning to ensure that truth segmentation was as accurate as\npossible.\nVOLUME 11, 2023 101631\nC. Ren et al.: Prostate Segmentation in MRI Using Transformer Encoder and Decoder Framework\nFIGURE 1. Muled-Net architecture. Encoder 1 and encoder 2 are two down-sampling branches of the\nmodel. The green dotted box is up-sampling branch of the model (decoder). The red dotted box represents\nthe transformer encoder blocks. DWI is for Input1, T2WI for Input 2.\nB. DATA-PROCESSING\nElastix was used to align and resample images to the same\nresolution [11]. The registered parameters were configured\nconcerning the work of Klein et al [11]. Due to the small\ndata volume, data augmentation was added to the data\npre-processing stage to address model over-fitting, such as\nrotation (-20,-10, 30, 50, 60 degrees), and flipping, random\ncrop (256 × 256), center crop (256 × 256). The total amount\nof training data after enhancement is three times larger than\nthe original data. The dataset was split into training (80%),\nvalidation (10%) and testing set (10%).\nIII. THE PROPOSED MODEL\nTo overcome the above problems, we design a multi-encoder\nand decoder segmentation model for the prostate and lesion\nregions. As shown in FIGURE 1, the network model is struc-\ntured with double encoders on the left-hand side that consist\nmainly of convolution operations, residual operations [12],\nnew transformer encoder blocks (marked by red dotted boxes)\nand an ASPP [13] module. On the right (marked by a green\ndotted box) is the decoder, which uses mainly feature fusion\nand deconvolution to make the parameter updates evenly\ndistributed. The ASPP module can expand the range of the\nreception field, which allows the network to learn the overall\ncontour of the prostate in the network. For the small lesion\nregions, feature fusion can be effectively propagated from\nlow features to higher-level semantics, preserving positional\ninformation. The transformer encoder is used to overcome the\nfact that only information from the neighborhood pixels can\nbe captured, increasing the ability to capture global depen-\ndencies [14]. In a word, the two down-sampling branches\nassist each other in extracting prostate features, solving the\nproblem that the neural network focuses too much on the\ninternal prostate features and neglects the contour features.\nA. ENCODERS FOR NETWORK\n1) FIRST DOWNSAMPLING BRANCH\nTo enhance the capability of feature extraction, we use a dou-\nble downsampling style to obtain features, which improves\nthe retention of target detail information. The first downsam-\npling branch is shown in Table 1, where the input image is\nconvolved to expand the channel dimension, and after each\npooling, a 2-fold expansion in the channel dimension is also\nperformed in the immediate convolution layer. There are\nfour convolution operations, with each layer of convolution\nfollowed by access to the batch normalization (BN) and\nRectified Linear Unit (ReLU) layers, followed by a residual\noperation. The convolution kernel size is 3 × 3, Padding=1,\n101632 VOLUME 11, 2023\nC. Ren et al.: Prostate Segmentation in MRI Using Transformer Encoder and Decoder Framework\nTABLE 1. Encoder 1 structure (The first downsampling branch). 3× 3 is kernel size; In operator parameters, 32,64,128,256 respectively stands for the\nnumber of the kernel.\nStride=1. Stride=2 is set in the pooling operation to reduce\nthe feature size. Among the branches, the 3 × 3 kernel can\ncapture the information of the eight neighbors of a pixel. The\nperceptual field of the stacked convolutional layer of three\n3×3 is 7 ×7, so the large convolutional layer can be replaced\nby a stack of small-sized convolutional layers. The last layer\nof encoder 1 is a group of transformer encoder blocks.\n2) SECOND DOWNSAMPLING BRANCH\nThe second downsampling branch shown in Table 2 performs\na convolution operation on the original image resulting in\nimage reduction by half. Then, the convolutional kernel size\nis 3 × 3 similar to the first branch, and the feature map scale\nis reduced by half with each pooling operation. 3 transformer\nblocks capture long-distance dependency at various scales.\nTwo downsampling branches can extract more abundant fea-\ntures, which enriches the feature fusion later and allows more\naccurate segmentation of the prostate and lesion area.\nB. DECODER FOR NETWORK\nIn the decoder, the first layer is ASPP, including 3 convolu-\ntions and average pooling. Upsampling is performed using\na deconvolution layer (stride = 2) and residual module. The\nfeature map of the decoder is stitched in the channel dimen-\nsion with the features of the corresponding encoded path\noutput by the skip connection. Next, the output is fed into the\nnext layer of convolutional residual blocks for upsampling.\nThis is repeatedly decoded layer by layer to the size of the\noriginal data in Table 3. Finally, the image mask is obtained\nby 3 × 3 convolution with 2 channels. In total, the decoder\ncontains 4 deconvolutions and 4 residual operations.\nC. TRANSFORMER ENCODER\nAs the images are heavily restructured data, most of the pixels\nhave alike features in the high-resolution feature maps, apart\nfrom the boundary regions. From the conceptual perspective,\nself-attention is by nature low grade for long sequences [13],\nsuggesting that most of the information is clustered in the\nlargest singular values. The translator is based on the Multiple\nHeaded Self-Attention (MHSA) module [15], [16], which\npermits the model to extrapolate attention jointly with dif-\nferently represented subspaces. In our work, the transformer\nencoder is formed by 5 heads and convolution operation (BN,\nVOLUME 11, 2023 101633\nC. Ren et al.: Prostate Segmentation in MRI Using Transformer Encoder and Decoder Framework\nTABLE 2. Encoder 2 structure (The second downsampling branch). 3× 3 is the kernel size.\nFIGURE 2. Transformer encoder block. The general flow chart is on the left.\nReLu, Conv). ‘‘addition’’ stands for feature fusion, as shown\nin FIGURE 2.\nThere is a mechanism of self-attention and regular convo-\nlution operations. x ∈ RC×H×W is feature map, where C, H,\nW are the number of image channels, height and width. Q, K,\nV represents query, key, value embeddings respectively [15].\nTwo projections are used to predict key and value: K, V\n∈ Rn×d into lower-dimensional space ¯K, ¯V ∈ Rk×d (k=hw).\nd is the embedding dimension of each head and n=HW.\nh and w are a downscaled version of the feature maps after\n101634 VOLUME 11, 2023\nC. Ren et al.: Prostate Segmentation in MRI Using Transformer Encoder and Decoder Framework\nTABLE 3. Decoder structure (The upsampling branch). Conv means convolution, 1×1 and 3×3 are kernel sizes, 256 shows the number of kernel channels.\nFIGURE 3. Feature fusion. Different colors represent different features,\nand finally, the features are connected.\nsubsampling. MSHA is defined:\nAttention(Q, ¯K, ¯V ) = softmax(Q ¯KT\n√\nd\n) ¯V (1)\nComputational complexity is O (n ∗k∗d). The presentation of\na projection onto low-dimensional embedding can be any\ndownsampling operation. We use a 2D relative position to add\nrelative height and width information [15].\nD. ATROUS SPATIAL PYRAMID POOLING AND FEATURE\nFUSION\nLesion regions are usually small targets. Continuous down-\nsampling in the model leads to lower image resolution and\nloss of important information, such as position information\nof the shallow layer for targets. Feature fusion is designed\ninto the network, as shown by the grey line in FIGURE 1\nand FIGURE 3. Different features are fused by ‘‘concat’’\noperator. Feature fusion connects the encoder and the decoder\nand integrates shallow features and high semantic features to\nretain more useful features.\nIn upsampling branch, the atrous spatial pyramid pooling\n(ASPP) layer [12] is set and acts as a bridge between the\ndecoder and the encoder. With atrous convolutions, it is pos-\nsible to expand the convolutional field and extract features\nat different scales with fewer parameters, which effectively\naggregates contextual information. ASPP contains four sets\nof atrous convolution with expansion factors of 1, 6, 12,\n18 and an adaptive mean pooling. It is not unnecessary to the\nVOLUME 11, 2023 101635\nC. Ren et al.: Prostate Segmentation in MRI Using Transformer Encoder and Decoder Framework\nFIGURE 4. ASPP architecture. Conv denotes convolution, 1× 1 and 3× 3 are kernel size,\nAvg-Pool is average pooling.\nFIGURE 5. Comparison of loss from these five models in testing data.\nparameter of adaptive operations, but only the final output\nsize, as shown in FIGURE 4. Then, multi-scale fusion is per-\nformed to enrich the semantic information. It can effectively\ncapture the global information of the context, avoiding the\nproblem of segmentation errors caused by getting trapped in\nlocal features. Finally, the number of channels is changed by\n3 × 3 convolution.\nE. MODEL INPUT\nIn mp-MRI sequences, no single sequence can accurately\nlocalize or segment the prostate or the lesion areas, so it is\nnecessary to employ multiple sequences to compensate for\nsegmentation features in the model [17], [18]. T2W shows\nthe anatomy of the prostate because of its T2 relaxation time,\nwhich is valuable for detecting suspicious lesion regions.\nYet, it is insensitive [19]. In prostate cancerous areas, DWI\nincludes the different apparent diffusion coefficient values,\nwhich reflect the angle of water diffusion. The diffusion\nrate of water molecules allows better differentiation between\nhealthy and unhealthy tissue. A single DWI is also sufficient\nto describe the region of prostate cancer since there is a lot\nof overlap between different GS cancers [20], [21]. This is\nwhy several studies have combined these two sequences for\nprostate testing and have had promising results. In Muled-\nNet, we also employ the T2W for encoder 1 and DWI for\nencoder 2 input in segmentation. The size of the input image\nis resized to 256 × 256.\nIV. EXPERIMENTS\nA. EXPERIMENT SETTING\nWe trained the network using the Adam optimizer, the\nPyTorch framework, β1 = 0.9 and β2 = 0.999, for a total\nof 1000 iterations. The learning rate is initialized at 1.0 ×\n10e-3 and decays to half of the original rate after every\n200 iterations. The evaluation metrics include Dice score, Iou,\naccuracy, sensitivity, 95HD and MSD [22]. The feasibility\nof our algorithm is compared with classical segmentation\nnetworks such as FCN, U-net, U-net++, ResU-net. The five-\nfold cross-validation and cross-entropy loss function are used\nto get accurate experiment results in all models. To make a fair\ncomparison, all procedures of the experimental protocol are\noperated in the same manner.\nB. RESULTS\nLoss vs. Epoch. The relationship between the prostate and\nits lesion area and the number of iterations is shown in\nFIGURE 5. The loss is rapidly reduced from epoch 0 to 100 in\nour model. Beginning with 100 epochs, the model starts to\nconverge slowly. Convergence occurs a little earlier than other\nmodels. For example, U-Net converges from 20 epochs. The\nloss change of FCN, U-Net, U-Net++ are very close. ResU-\nnet has a larger loss value than other models, but the trend of\nloss is similar.\nThe diagram of Dice vs. epoch for our model is presented\nin FIGURE 6. The proposed method gets better perfor-\nmance than other methods. U-Net and U-Net++ have similar\nachievements. Comparing these two plots (a) and (b), the\nnumber of samples in the lesion area is small, but more\nexcellent results are achieved in Dice for the segmentation\nof prostate lesion regions.\nThe segmentation performance of each model was cal-\nculated for the prostate and its lesion zone with five-fold\ncross-validation, as shown in Table 4. Our model obtained\nan average Dice, Iou, sensitivity, 95HD, and MSD for the\nprostate of 95.0%, 89%, 94%, 9.56, 0.66. Our model exceeds\nU-Net by 2%, U-Net ++ by 2%, FCN by 3%, and ResU-Net\nby 6% in terms of Dice. Meantime, Iou and sensitivity also\nyield better results compared to other models.\nIn Table 5, for the segmentation performance of the\nprostate lesion regions, the proposed network achieves the\n101636 VOLUME 11, 2023\nC. Ren et al.: Prostate Segmentation in MRI Using Transformer Encoder and Decoder Framework\nFIGURE 6. Comparison of dice metric for segmentation of prostate (a) and its lesion regions (b) from five\nmodels in testing data.\nTABLE 4. Performance comparison of prostate segmentation.\nTABLE 5. Performance comparison of prostate lesion region segmentation.\nTABLE 6. Ablation experiment.\nresults in terms of Dice of 89%, Iou of 82%, and sensitivity of\n92%, 95HD of 11.16 and MSD of 1.09. Although lesion data\nis scarce, our algorithm has superior performance to other\nmodels. For example, our model is greater than U-Net by 3%,\nU-Net ++ by 4%, FCN by 7%, and ResU-Net by % in terms\nof sensitivity.\nWe conducted ablation experiments on the testing to verify\nthe influence of ASPP and transformer encoder blocks on the\nVOLUME 11, 2023 101637\nC. Ren et al.: Prostate Segmentation in MRI Using Transformer Encoder and Decoder Framework\nFIGURE 7. The visualization comparison of different models for four patients. The background of test results\nis marked in black to more clearly distinguish the performance for each model.\n101638 VOLUME 11, 2023\nC. Ren et al.: Prostate Segmentation in MRI Using Transformer Encoder and Decoder Framework\nFIGURE 7. (Continued.) The visualization comparison of different models for four patients. The background of\ntest results is marked in black to more clearly distinguish the performance for each model.\nVOLUME 11, 2023 101639\nC. Ren et al.: Prostate Segmentation in MRI Using Transformer Encoder and Decoder Framework\nFIGURE 7. (Continued.) The visualization comparison of different models for four patients. The background\nof test results is marked in black to more clearly distinguish the performance for each model.\nFIGURE 8. Visualization of the last layer of the model. The prostate region receives heavy attention marked\nin red.\nmodel. ‘‘-’’ represents the removal of the corresponding mod-\nule. For example, Muled-Net-ASPP means Muled-Net model\ncancels ASPP. From Table 6, we can see that Muled-Net has\nachieved the best results for the prostate and its lesion regions.\nEither removing ASPP or transformer encoder blocks weak-\nens the performance of the model.\nC. VISUALIZATION COMPARISON\nThe segmentation results of our and other models are given\nin the images below, as in FIGURE 7. The segmentation\nmask of prostate region and lesion region is marked in red\nand yellow respectively. To make the segmentation clearer,\nthe rest of the image is blacked out as a background. At the\nsame time, we also give the segmentation contour diagram\nfor our (Muled-Net) model (The prostate has a red contour\nand green shows lesion region contour). FIGURE 8 is the\nvisualization of the model at the last layer. As you can see\nfrom the FIGURE, the region of interest (prostate region)\nreceives more attention.\nV. DISCUSSION\nProstate segmentation is a challenging task due to the diver-\nsity in size, shape, and texture of the prostate and the lack\n101640 VOLUME 11, 2023\nC. Ren et al.: Prostate Segmentation in MRI Using Transformer Encoder and Decoder Framework\nof well-defined prostate boundaries, particularly in malig-\nnant prostate tissue. In this study, we proposed a novel\nencoder-decoder framework with the mp-MRI for the seg-\nmentation of the prostate and its lesion regions, denoted\nMuled-Net. The model consisted of two main parts, the\nencoder and the decoder. The encoder included two branches\nfor feature extraction, encoder 1 and encoder 2. The new\ntransformer encoder blocks were presented to enhance the\nlong-range relationship of the prostate image in encoder 1 and\nencoder 2. In addition, ASPP and feature fusion are merged\nto expand the perceptual field for prostate features. The use\nof feature fusion retained more shallow spatial information.\nIt is more beneficial to small target segmentation. Then, the\nmodel was assessed based on real data sets with five-fold\ncross-validation. In comparison with the state-of-the-art seg-\nmentation methods, the results demonstrated that our model\ncombined the T2W, and DWI images achieved the best per-\nformance in terms of sensitivity, Dice, Iou, 95HD, MSD\nfor the segmentation of the prostate and its lesion regions.\nThe performance improvement is more obvious in the lesion\nregion from experiments. Finally, due to the lack of data sets\nin this field of segmentation, many studies were hindered.\nso we established a publicly usable dataset for prostate and\nlesion region segmentation.\nSeveral studies that used the convolution neural network\nwere related to the segmentation of the prostate with mp-\nMRI. Qadri et al. [22] proposed a methodology for automatic\nsegmentation of the prostate gland and peripheral zone by\nextracting the ROI on DWI and cascading two CNN models\non T2W. Two image sequences were also employed in our\nmodel, DWI and T2W. Furthermore, our model was parallel\nprocessing of dual inputs and single end-to-end architecture.\nZhu et al. [23] developed a novel DCNN inspired by U-Net\nand DensNet, which combined the advantages of both models\nfor the segmentation of the prostate gland. Three variants\nof the framework were evaluated to find how the variability\nof the ground truth of the prostate impacts segmentation.\nDuran et al. [24] proposed a multi-class end-to-end model\nto segment WG and GS group grading for cancer lesions.\nSun et al. [25] proposed a meta self-attention prototype incre-\nmenter to solve huge intra-class deviation of prostate tissue.\nOur work focused on the automatic and accurate segmenta-\ntion of the prostate and its lesion regions to provide valuable\ndiagnostic data for the specialist and the patient with surgical\nplanning.\nThe transformer adopts encoder-decoder architecture with\nextraordinary results in the field of natural language process-\ning and computer vision [23], [24], [25], [26]. Our model\nalso used the same structure but it is asymmetric and has dual\ninputs. The main reason is that the union inputs of different\nimage sequences, especially for DWI, the model could show\nhigher specificity for the prostate lesion regions than only the\nT2W or ADC sequence [29]. T2W is often conducted in axial,\ncoronal, and sagittal views, with the cancer cells behaving\ndifferently in terms of intensity and homogeneity, which has\nresulted in a greater identification of the prostate zones [30].\nASSP model was used to retain more context segmentation\ninformation and expand the exceptive field. The transformer\nencoder blocks were represented to overcome the fact that\nonly information from the neighborhood pixels can be cap-\ntured and the lack of long-range information connection.\nIn the decoder, our model added a feature fusion technique\nfrom the encoders to avoid the loss of useful information by\ndeepening the network model.\nSome drawbacks to our model should be mentioned: (1),\nThe small number of patients was used for this study. More\ndata could obtain better results. (2), The training process was\nvery sensitive to the initialization of the parameters and a\nsuperior initialization strategy can improve the performance\nof the model. (3), While in this work we only show the designs\nof automatic and accurate CNNs for fusing T2W and DWI\nimages and display the performance, our approach should be\neasily scalable to incorporate more MRI modalities, including\nDCE, ADC, etc. For clinical applications, an outstanding\nmodel should have the smallest number of participants and be\nrobust. In the future, we will continue to collect more data to\nenrich this work. The exploration of new models that are more\neffective for the segmentation of the prostate and its regions\nis an idea we have. In addition, Inspired by Ban et al. [31]\nwork, image feature extraction and then segmentation of\nregions of interest for prostate 3D data is another research\ndirection.\nIn conclusion, we proposed an automated multi-encoder\nand decoder segmentation model for the prostate and its\nlesion regions based on a convolution neural network. The\nvalidity of the model was demonstrated experimentally, sug-\ngesting that the novel model has the potential to offer accurate\nadvice and support on prostate diseases for physicians, patient\nmanagement and treatment.\nDATA AVAILABILITY STATEMENT\nData supporting the conclusions of this article will be\nmade available by the corresponding author upon reasonable\nrequest.\nCONFLICT OF INTEREST\nThe authors declare that the research was conducted in the\nabsence of any commercial or financial relationships that\ncould be construed as a potential conflict of interest.\nETHICS STATEMENT\nWritten informed consent for this study was allowed from\neach patient.\nREFERENCES\n[1] J. Ao, X. Shao, Z. Liu, Q. Liu, J. Xia, Y . Shi, L. Qi, J. Pan, and M. Ji,\n‘‘Stimulated Raman scattering microscopy enables Gleason scoring of\nprostate core needle biopsy by a convolutional neural network,’’ Cancer\nRes., vol. 83, no. 4, pp. 641–651, Feb. 2023.\n[2] N. Birkbeck, J. Zhang, M. Requardt, B. Kiefer, P. Gall, and S. K. Zhou,\n‘‘Region-specific hierarchical segmentation of MR prostate using discrim-\ninative learning,’’ in Proc. MICCAI Grand Challenge: Prostate MR Image\nSegmentation, 2012, pp. 1–8.\nVOLUME 11, 2023 101641\nC. Ren et al.: Prostate Segmentation in MRI Using Transformer Encoder and Decoder Framework\n[3] M. D. Blackledge, M. O. Leach, D. J. Collins, and D.-M. Koh, ‘‘Computed\ndiffusion-weighted MR imaging may improve tumor detection,’’ Radiol-\nogy, vol. 261, no. 2, pp. 573–581, Nov. 2011.\n[4] F. Bratan, E. Niaf, C. Melodelima, A. L. Chesnais, R. Souchon,\nF. Mège-Lechevallier, M. Colombel, and O. Rouvière, ‘‘Influence of imag-\ning and histological factors on prostate cancer detection and localisation on\nmultiparametric MRI: A prospective study,’’ Eur. Radiol., vol. 23, no. 7,\npp. 2019–2029, Jul. 2013.\n[5] PI-RADS Version 2. ACR, American College of Radiology, Reston, V A,\nUSA, 2015.\n[6] R. Cao, A. M. Bajgiran, S. A. Mirak, S. Shakeri, X. Zhong, D. Enzmann,\nS. Raman, and K. Sung, ‘‘Joint prostate cancer detection and Gleason score\nprediction in mp-MRI via FocalNet,’’ IEEE Trans. Med. Imag., vol. 38,\nno. 11, pp. 2496–2506, Nov. 2019.\n[7] A. Saha, M. Hosseinzadeh, and H. Huisman, ‘‘End-to-end prostate can-\ncer detection in bpMRI via 3D CNNs: Effects of attention mechanisms,\nclinical priori and decoupled false positive reduction,’’ Med. Image Anal.,\nvol. 73, Oct. 2021, Art. no. 102155.\n[8] M. Koc, S. K. Sut, I. Serhatlioglu, M. Baygin, and T. Tuncer, ‘‘Auto-\nmatic prostate cancer detection model based on ensemble VGGNet feature\ngeneration and NCA feature selection using magnetic resonance images,’’\nMultimedia Tools Appl., vol. 81, no. 5, pp. 7125–7144, Feb. 2022.\n[9] K. Yildirim, M. Yildirim, H. Eryesil, M. Talo, O. Yildirim, M. Karabatak,\nM. S. Ogras, H. Artas, and U. R. Acharya, ‘‘Deep learning-based PI-\nRADS score estimation to detect prostate cancer using multiparametric\nmagnetic resonance imaging,’’ Comput. Electr. Eng., vol. 102, Sep. 2022,\nArt. no. 108275.\n[10] B. Abraham and M. S. Nair, ‘‘Automated grading of prostate cancer using\nconvolutional neural network and ordinal class classifier,’’ Informat. Med.\nUnlocked, vol. 17, Jan. 2019, Art. no. 100256.\n[11] S. Klein, M. Staring, K. Murphy, M. A. Viergever, and J. P. W. Pluim,\n‘‘elastix: A toolbox for intensity-based medical image registration,’’ IEEE\nTrans. Med. Imag., vol. 29, no. 1, pp. 196–205, Jan. 2010.\n[12] S. Targ, D. Almeida, and K. Lyman, ‘‘Resnet in resnet: Generalizing\nresidual architectures,’’ 2016, arXiv:1603.08029.\n[13] A. Sullivan and X. Lu, ‘‘ASPP: A new family of oncogenes and tumour\nsuppressor genes,’’ Brit. J. Cancer, vol. 96, no. 2, pp. 196–200, Jan. 2007.\n[14] Y . Gao, M. Zhou, and D. N. Metaxas, ‘‘UTNet: A hybrid transformer archi-\ntecture for medical image segmentation,’’ in Medical Image Computing\nand Computer Assisted Intervention—MICCAI 2021. Cham, Switzerland:\nSpringer, 2021, pp. 61–71.\n[15] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser and, I. Polosukhin, ‘‘Attention is all you need,’’ in Proc. Adv.\nNeural Inf. Process. Syst., vol. 30, 2017, pp. 1–11.\n[16] S. Wang, B. Z. Li, M. Khabsa, H. Fang, and H. Ma, ‘‘Linformer: Self-\nattention with linear complexity,’’ 2020, arXiv:2006.04768.\n[17] G. Lemaître, ‘‘Computer-aided diagnosis for prostate cancer using multi-\nparametric magnetic resonance imaging,’’ Ph.D. thesis, Res. Inst. Comput.\nVis. Robot., Universitat de Girona, Girona, Spain, 2016.\n[18] D. Fehr, H. Veeraraghavan, A. Wibmer, T. Gondo, K. Matsumoto,\nH. A. Vargas, E. Sala, H. Hricak, and J. O. Deasy, ‘‘Automatic classifi-\ncation of prostate cancer Gleason scores from multiparametric magnetic\nresonance images,’’ Proc. Nat. Acad. Sci. USA, vol. 112, no. 46, pp. 1–10,\nNov. 2015.\n[19] H. A. Vargas, A. M. Hötker, D. A. Goldman, C. S. Moskowitz, T. Gondo,\nK. Matsumoto, B. Ehdaie, S. Woo, S. W. Fine, V . E. Reuter, E. Sala,\nand H. Hricak, ‘‘Updated prostate imaging reporting and data system\n(PIRADS v2) recommendations for the detection of clinically signifi-\ncant prostate cancer using multiparametric MRI: Critical evaluation using\nwhole-mount pathology as standard of reference,’’ Eur. Radiol., vol. 26,\nno. 6, pp. 1606–1612, Jun. 2016.\n[20] Y . Peng, Y . Jiang, C. Yang, J. B. Brown, T. Antic, I. Sethi,\nC. Schmid-Tannwald, M. L. Giger, S. E. Eggener, and A. Oto, ‘‘Quan-\ntitative analysis of multiparametric prostate MR images: Differentia-\ntion between prostate cancer and normal tissue and correlation with\nGleason score—A computer-aided diagnosis development study,’’ Radi-\nology, vol. 267, no. 3, pp. 787–796, Jun. 2013.\n[21] X. Yang, C. Liu, Z. Wang, J. Yang, H. L. Min, L. Wang, and K.-T. Cheng,\n‘‘Co-trained convolutional neural networks for automated detection of\nprostate cancer in multi-parametric MRI,’’ Med. Image Anal., vol. 42,\npp. 212–227, Dec. 2017.\n[22] S. F. Qadri, L. Shen, M. Ahmad, S. Qadri, S. S. Zareen, and S. Khan,\n‘‘OP-ConvNet: A patch classification-based framework for CT vertebrae\nsegmentation,’’IEEE Access, vol. 9, pp. 158227–158240, 2021.\n[23] Y . Zhu, R. Wei, G. Gao, L. Ding, X. Zhang, X. Wang, and J. Zhang,\n‘‘Fully automatic segmentation on prostate MR images based on cas-\ncaded fully convolution network,’’ J. Magn. Reson. Imag., vol. 49, no. 4,\npp. 1149–1156, Apr. 2019.\n[24] A. Duran, G. Dussert, O. Rouvière, T. Jaouen, P.-M. Jodoin, and\nC. Lartizien, ‘‘ProstAttention-Net: A deep attention model for prostate\ncancer segmentation by aggressiveness in MRI scans,’’ Med. Image Anal.,\nvol. 77, Apr. 2022, Art. no. 102347.\n[25] L. Sun, M. Zhang, B. Wang, and P. Tiwari, ‘‘Few-shot class-incremental\nlearning for medical time series classification,’’ IEEE J. Biomed. Health\nInformat., early access, Feb. 22, 2023, doi: 10.1109/JBHI.2023.3247861.\n[26] G. Luo, Y . Zhou, X. Sun, Y . Wang, L. Cao, Y . Wu, F. Huang, and\nR. Ji, ‘‘Towards lightweight transformer via group-wise transformation\nfor vision-and-language tasks,’’ IEEE Trans. Image Process., vol. 31,\npp. 3386–3398, 2022.\n[27] E. Foray, C. Martin, B. Allard, and P. Bevilacqua, ‘‘A design-of-\nexperiments-based approach to design planar transformers for high-voltage\nlow-power applications,’’ IEEE Trans. Magn., vol. 58, no. 6, pp. 1–6,\nJun. 2022.\n[28] X. Xing, L. Zhu, C. Chen, N. Sun, C. Yang, K. Yan, L. Xue, and S. Wang,\n‘‘Transformer oil quality evaluation using quantitative phase microscopy,’’\nAppl. Opt., vol. 61, pp. 422–428, Jan. 2022.\n[29] J. C. Weinreb, J. O. Barentsz, P. L. Choyke, F. Cornud, M. A. Haider,\nK. J. Macura, and D. Margolis, ‘‘PI-RADS prostate imaging-reporting and\ndata system: 2015, version 2,’’ Eur. Urol., vol. 69, no. 1, pp. 16–40, 2016.\n[30] Z. Khan, N. Yahya, K. Alsaih, M. I. Al-Hiyali, and F. Meriaudeau, ‘‘Recent\nautomatic segmentation algorithms of MRI prostate regions: A review,’’\nIEEE Access, vol. 9, pp. 97878–97905, 2021.\n[31] Y . Ban, Y . Wang, S. Liu, B. Yang, M. Liu, L. Yin, and W. Zheng, ‘‘2D/3D\nmultimode medical image alignment based on spatial histograms,’’ Appl.\nSci., vol. 12, no. 16, p. 8261, Aug. 2022.\nCHENGJUAN REN received the master’s degree\nin computer software and theory from Chongqing\nUniversity, Chongqing, China, in 2010, and\nthe Ph.D. degree in software convergence\nengineering from Kunsan National University,\nGunsan, South Korea, in 2022. She is cur-\nrently a Professor with the College of Lan-\nguage Intelligence, Sichuan International Studies\nUniversity, Chongqing. Her research interests\ninclude machine learning and deep learning.\nZIYU GUOis currently pursuing the Ph.D. degree\nwith the School of Computer Science and Engi-\nneering, The Chinese University of Hong Kong.\nHer main research interest includes artificial\nintelligence.\n101642 VOLUME 11, 2023\nC. Ren et al.: Prostate Segmentation in MRI Using Transformer Encoder and Decoder Framework\nHUIPENG RENreceived the bachelor’s degree in\nmedical imaging from the North Sichuan Medi-\ncal College, Nanchong, Sichuan, China, in 2008,\nand the master’s degree in imaging medicine and\nnuclear medicine from Xi’an Jiaotong University,\nXi’an, Shaanxi, China, in 2017. He is currently\nengaged in medical imaging diagnosis with the\nMedical Imaging Department, Baoji Central Hos-\npital, Shaanxi. He is an Associate Chief Physician.\nHis research interests include the clinical applica-\ntion of new magnetic resonance technology, imaging diagnosis of abdominal\nand pelvic tumors, deep learning, and prostate MRI segmentation.\nDONGWON JEONG received the B.S. degree\nin computer science from Kunsan National Uni-\nversity, Gunsan, South Korea, in 1997, the M.S.\ndegree in computer science from Chungbuk\nNational University, Cheongju, South Korea,\nin 1999, and the Ph.D. degree in computer science\nand engineering from Korea University, Seoul,\nSouth Korea, in 2004. From 2000 to 2001, he was\na Senior Researcher with Jigunet Corporation,\nSouth Korea, and from 2004 to 2005, he was a\nResearch Assistant Professor with the Research Institute of Information\nand Communication Technology, Korea University. From 2004 to 2005,\nhe was a Visiting Scholar with the School of Information Sciences and\nTechnology, The Pennsylvania State University, USA. Since 2005, he has\nbeen a Professor with Kunsan National University. From 2012 to 2013 and\nfrom 2018 to 2019, he was a Visiting Scholar with Oakland University, USA.\nHe is currently the Vice-President of the Korean Institute of Information\nTechnology, South Korea. His research interests include metadata-based\ndata and service integration and standardization, intelligent application and\nservice development using new technologies, such as the IoT, big data,\nand AI.\nDAE-KYOO KIM received the Ph.D. degree in\ncomputer science from Colorado State University,\nin 2004. He was a Technical Specialist with the\nNASA Ames Research Center, in 2002. He is\ncurrently a Professor with the Department of\nComputer Science and Engineering, Oakland Uni-\nversity. His research interests include software\nengineering, software security, deep learning, data\nmodeling in the IoT, and smart grids.\nSHIYAN ZHANGis currently pursuing the bach-\nelor’s degree with the Baoji College of Arts and\nScience, School of Computer Science.\nJIACHENG WANGis currently pursuing the bach-\nelor’s degree with the Baoji College of Arts and\nScience, School of Computer Science.\nGUANGNAN ZHANG received the B.Eng.\ndegree in computer science and technology from\nNanchang University, in 2003, the M.Eng. degree\nin circuits and systems from Northwest Normal\nUniversity, in 2009, and the D.Eng. degree in\nintelligent transportation and information systems\nengineering from Chang’an University, in 2021.\nSince July 2003, he has been with the School\nof Computer Science, Baoji College of Arts and\nScience. He is currently the Dean of the School of\nComputer Science, a member of the ‘‘Intelligent Ecology’’ Technical Group,\nInternet of Things Committee of the Chinese Society of Communication, and\nthe Executive Director of the Shaanxi Computer Education Society. He is a\nProfessor. Main research interests are in artificial intelligence.\nVOLUME 11, 2023 101643",
  "topic": "Encoder",
  "concepts": [
    {
      "name": "Encoder",
      "score": 0.7970083951950073
    },
    {
      "name": "Segmentation",
      "score": 0.7871504426002502
    },
    {
      "name": "Computer science",
      "score": 0.7785968780517578
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6580115556716919
    },
    {
      "name": "Pixel",
      "score": 0.5106766819953918
    },
    {
      "name": "Image segmentation",
      "score": 0.4736182391643524
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4667646586894989
    },
    {
      "name": "Deep learning",
      "score": 0.4664549231529236
    },
    {
      "name": "Computer vision",
      "score": 0.3980684280395508
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "cited_by": 7
}