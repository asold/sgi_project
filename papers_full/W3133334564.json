{
  "title": "Towards Emotion Recognition in Hindi-English Code-Mixed Data: A Transformer Based Approach",
  "url": "https://openalex.org/W3133334564",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4287499673",
      "name": "Wadhawan, Anshul",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Aggarwal, Akshita",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W25301398",
    "https://openalex.org/W2251678408",
    "https://openalex.org/W1513398909",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2075718943",
    "https://openalex.org/W2950133940",
    "https://openalex.org/W2962920413",
    "https://openalex.org/W2339343773",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2518110751",
    "https://openalex.org/W2187227891",
    "https://openalex.org/W2331804107",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W2267835966"
  ],
  "abstract": "In the last few years, emotion detection in social-media text has become a popular problem due to its wide ranging application in better understanding the consumers, in psychology, in aiding human interaction with computers, designing smart systems etc. Because of the availability of huge amounts of data from social-media, which is regularly used for expressing sentiments and opinions, this problem has garnered great attention. In this paper, we present a Hinglish dataset labelled for emotion detection. We highlight a deep learning based approach for detecting emotions in Hindi-English code mixed tweets, using bilingual word embeddings derived from FastText and Word2Vec approaches, as well as transformer based models. We experiment with various deep learning models, including CNNs, LSTMs, Bi-directional LSTMs (with and without attention), along with transformers like BERT, RoBERTa, and ALBERT. The transformer based BERT model outperforms all other models giving the best performance with an accuracy of 71.43%.",
  "full_text": "Towards Emotion Recognition in Hindi-English Code-Mixed Data: A\nTransformer Based Approach\nAnshul Wadhawan and Akshita Aggarwal\nDepartment of Computer Engineering\nNetaji Subhas University of Technology\nDwarka, New Delhi\n{anshulw, akshitaa}.co.16@nsit.net.in\nAbstract\nIn the last few years, emotion detection in\nsocial-media text has become a popular prob-\nlem due to its wide ranging application in bet-\nter understanding the consumers, in psychol-\nogy, in aiding human interaction with comput-\ners, designing smart systems etc. Because of\nthe availability of huge amounts of data from\nsocial-media, which is regularly used for ex-\npressing sentiments and opinions, this prob-\nlem has garnered great attention. In this pa-\nper, we present a Hinglish dataset labelled\nfor emotion detection. We highlight a deep\nlearning based approach for detecting emo-\ntions in Hindi-English code mixed tweets, us-\ning bilingual word embeddings derived from\nFastText and Word2Vec approaches, as well\nas transformer based models. We experiment\nwith various deep learning models, including\nCNNs, LSTMs, Bi-directional LSTMs (with\nand without attention), along with transform-\ners like BERT, RoBERTa, and ALBERT. The\ntransformer based BERT model outperforms\nall other models giving the best performance\nwith an accuracy of 71.43%.\n1 Introduction\nWith the growth of social networking sites like\nFacebook and Twitter, humans have started com-\nmunicating online much more than ever before.\nThis leads to the generation of huge amounts of tex-\ntual data which introduces interesting challenges\nin the domain of NLP. Automatic detection of vari-\nous linguistic expressions like irony, hate, sarcasm,\naggression etc. is being widely explored. Another\nproblem that has drawn keen interest of NLP re-\nsearchers is detecting emotions of a human via the\ntexts they have produced. In order to aid human-\ncomputer interaction, determining the emotions via\ntexts becomes signiﬁcant (Greaves et al., 2009).\nThere are multiple ways of detecting emotions, in-\ncluding but not limited to speech (Schmitt et al.,\n2016), facial expressions recognition (Ko, 2018)\nand text-based approaches.\nText-based emotion detection is based on the as-\nsumption that when a person is happy, they would\nuse positive words. Likewise, when they are angry,\nfrustrated or upset, negative emotions will be de-\npicted by a certain kind of words carrying negative\nconnotation. Contrary to popular belief, emotions\nare not only signiﬁcant in human creativity, but they\nalso play an instrumental part in making rational de-\ncisions. With the rise of artiﬁcial intelligence and\nincreased focus on human-computer interaction,\nsmart machines that will communicate naturally\nand intelligently with humans, need to recognize\ntheir emotions effectively. Affective computing has\nemerged as an exciting ﬁeld with recent focus on\nemotion detection (Picard, 2000).\nMost of earlier work has been carried out on a\nmono-lingual dataset due to easy availability of a\nlarge corpus of annotated data (Chen et al., 2010;\nCanales and Mart´ınez-Barco, 2014). However, in\nmultilingual cultures, use of multiple languages\nwhile exchanging information on social media is\nquite common. Studies show that as many as 314.9\nmillion people in India are bilingual1. This leads to\nthe issue of code-mixing and code-switching espe-\ncially while communicating on social media plat-\nforms like Twitter, Facebook and Reddit (Gupta\net al., 2016; M ´onica et al., 2009). Code-mixing\noccurs when lexicons and grammatical features\nof multiple languages are used in the same sen-\ntence (Poplack and Walker, 2003; Auer and Wei,\n2007; 10, 2009). The major issue in dealing with\ncode-mixed problems is the absence of sufﬁciently\nannotated datasets (Nguyen and Dogruoz, 2013).\nIn this paper, we present our ﬁndings on one of\nthe most challenging problems in the domain of\nNatural Language Processing, ‘emotion detection’.\n1https://en.wikipedia.org/wiki/\nMultilingualism_in_India\narXiv:2102.09943v2  [cs.CL]  28 Feb 2021\nWhile a lot of work has been carried out for the\nEnglish language (Aman and Szpakowicz, 2007),\nthe domain of Hindi-English code-mixed texts re-\nmains relatively new and not much explored. We\npresent an annotated Hindi-English code-mixed\ndataset of 150k tweets for addressing this issue\nand for enabling future researchers to contribute to\nthis domain. Our aim in this paper is to compare\nmultiple deep learning models including CNNs,\nLSTMs, Bi-directional LSTMs (with and without\nattention) with the aid of bilingual self-trained word\nembeddings on a code-mixed dataset, along with\ntransformer based models like BERT, RoBERTa,\nand ALBERT.\nThe paper is organized as follows – Section 2\ndetails about the background and related work in\nthis domain. Section 3 enumerates the methodol-\nogy we used to perform the experiments including\ndata annotation, pre-processing, embeddings and\nmodels used. Section 4 lists down the experimen-\ntal settings to replicate the work done. Section 5\ncontains details of the results obtained and section\n6 consists of conclusions drawn from the results.\n2 Related Work\nWith the huge growth of micro-blogging platforms\nlike Facebook and Twitter, there has been an in-\ncreased interest in detecting sentiments and emo-\ntions in large text corpus (Kouloumpis et al., 2011;\nPak and Paroubek, 2010). In initial work aimed at\nemotion detection in textual data, experiments have\nbeen carried out with text-based emotion classiﬁca-\ntion in fairy tales for kids on the lines of basic emo-\ntions (Alm et al., 2005; Ekman, 1992). In another\nrelated work (Liu et al., 2003), the authors work\non real-world knowledge bases highlighting hu-\nman’s natural reactions towards various situations,\naimed at identifying emotions at the sentence-level.\nWith the increase of non-native English speakers\non social media, sentiment analysis on regional\nlanguages and code-mixed data has gained momen-\ntum.\nA pivotal work of sentiment analysis in Hindi\ncorpus was done, where the authors were success-\nful in extracting sentiment lexons from HindiWord-\nNet and were able to achieve an accuracy of 87%\nin the domain of movie (Joshi et al., 2010). In a\ndetailed analysis of data of English-Hindi bilingual\nusers on Facebook, it was shown that 17.2% of all\nposts, which accounted for around one-fourth of\nthe words in their dataset, revealed some form of\ncode-mixing (Bali et al., 2014). Sub-word level\nLSTM architecture for performing sentiment anal-\nysis was introduced on Hindi-English code-mixed\ndataset (Prabhu et al., 2016). Experiments were\nconducted with supervised learning (SVM) on a\nHindi-English code-mixed corpus for emotion de-\ntection (Vijay et al., 2018).\n3 Proposed Methodology\nThis section describes the series of steps that con-\nstitute the methodology proposed, including de-\ntailed descriptions of dataset creation, annotation,\npreprocessing, embeddings, and the deep learning\nmodels.\n3.1 Dataset Creation\nThe dataset annotated by paper (Vijay et al., 2018)\ncontains 2866 tweets. This data being insufﬁ-\ncient for doing any meaningful work with deep\nlearning due to the issue of overﬁtting, we cre-\nated a self-annotated class-balanced dataset using\nTwitterScraper API2 with relevant search tags like\n#happy, #sad, #angry, #fear, #disgust, #wow along\nwith some commonly used hindi words to obtain\nHinglish data.\n3.2 Dataset Annotation and Analysis\nWe scraped around 250k tweets for analysis.\nAfter dropping the noisy instances containing\nunknown characters, we ﬁltered the dataset down\nto a class balanced corpus of 150k tweets. The\ntweets were annotated with six standard emotions,\nincluding, happiness, sadness, anger, fear, disgust\nand surprise (Ekman, 1992). The hashtags which\nwere used as searching criteria for scraping the\ntweets, were used for annotation. All examples\nwhich were fetched using hashtags like #yayy\nwere marked to have a positive happiness label.\nThis process was repeated for all the 6 emotions\nunder consideration. The number of tweets per\nclass is depicted in Table 1. Initially, embeddings\nwere trained on just Hinglish tweets, however,\nEnglish tweets were added later because of excess\nof hindi words in Hinglish tweets, causing a lack\nof English speciﬁc words. The labelled emotion\ndetection dataset along with the deep learning\nclassiﬁcation models is made available online3 to\n2https://github.com/taspinar/\ntwitterscraper\n3https://github.com/anshulwadhawan/\nemotion_detection\nEmotion Number of instances\nHappiness 25869\nSadness 20931\nAnger 28705\nFear 18981\nDisgust 35667\nSurprise 18935\nTotal sentences 149088\nTable 1: Tweet count per class\npromote additional research.\nExamples of some annotated data :\nTWEET: Great darshan today at siddhi vinayak\nalong wid aarti!! #happiness @dollydas261\n@vishal71182 @vishalbti ..\nTRANSLATION: Had a great experience in\nSiddhi Vinayak Temple, along with the ceremonies\n#happiness\nEMOTION: Happy\nTWEET : Jindagi me Maut sabse bada loss nahi,\nsabse bada loss tab hota hai jab Do logo ke jinda\nrehte hue unke beech aapsi riste toot jaye.#Sad :(\nTRANSLATION: The biggest loss in life is not\ndeath. The biggest loss is banishment of relations\nbetween loved ones even when alive.\nEMOTION: Sad\n3.3 Data Preprocessing\nWe preprocessed the scraped data by retaining only\nHinglish tweets while removing tweets in pure En-\nglish and Devanagari. We also removed rare words\n(words having occurrence of less than 10 in the en-\ntire dataset), mentions, ‘#’ symbols, URLs, punctu-\nations and keywords used for scraping (like happy,\nsad, etc.) in order to feed our models with cleaner\ndata.\n3.4 Creation of Hindi-English Bi-lingual\nWord Embeddings\nThis being a multi-label text classiﬁcation problem,\nit is required that the text be ﬁrst converted to a\nform understandable by the various machine learn-\ning algorithms. Word embeddings are numerical\nrepresentation of words. Speciﬁcally, word em-\nbeddings are vector representations of words that\nare learned in an unsupervised manner where their\nrelative similarities are directly related to their se-\nmantic similarity (Mandelbaum and Shalev, 2016).\nDue to unavailability of pre-trained Hindi-English\nbilingual word embeddings, we created our own\nembeddings by scrapping 427k Hinglish tweets\nand 300k English tweets using TwitterScrapper\nAPI. Processing was carried out by removing pure\nEnglish and pure Devanagari tweets along with\nrare words, hashtags and mentions for obtaining\nbetter training results. We chose 2 types of word\nembeddings for our problem, each of which was\ntrained on two kinds of datasets, after processing\n(removing hashtags, user mentions, URLs, punctu-\nations and keywords used for scraping), one which\nhad only Hinglish tweets, the other which had a\nmix of English and Hinglish tweets. In order to\nget the right co-relation between the words of the\ntwo languages, we experimented with a mixture of\nHinglish and English tweets.\nWord2Vec: In this kind of embedding, words\nare converted to vector representations where\nwords having common context are placed in vicin-\nity amidst the vector space (Mikolov et al., 2013).\nTaking a huge corpus of words as input, it gener-\nates a vector space with each word being assigned\na unique vector value in that space. Since the avail-\nable Word2Vec embeddings are pre-trained on En-\nglish datasets only, we trained our embeddings on\ncustom Hindi-English code-mixed dataset, to ob-\ntain the desired code-mixed embeddings.\nFastText: FastText is a modiﬁcation to the\nWord2Vec embeddings that was developed by Face-\nbook in 2016 (Joulin et al., 2017). FastText as-\nsumes a word to be composed of character n-grams\n(Bojanowski et al., 2017) and hence breaks a given\nword into various sub-words (Example: light, li,\nig, igt, gt) unlike word2vec which feeds individual\nwords to the network. The training session of a\nFastText model involves learning of weights for\nnot only the whole word, but also for each of the\ncharacter n-grams. Unlike Word2vec, it can not\nonly approximate rare words but also give repre-\nsentation of words not present in the corpus, as now\nit is highly possible that some of their n-grams are\npresent in other words. This is particularly useful\nfor messages on social networks where multiple\nrepresentations are used for similar words (like\npyar, pyaar, pyaaar).\n3.5 Deep Learning Models\nWe introduce seven deep learning based models\nfor solving the task of emotion detection in textual\nFigure 1: CNN model architecture\ncode-mixed data. The models proposed are CNN,\nLSTM, Bi-directional LSTM, attention based Bi-\ndirectional LSTM and transformer based models\nlike BERT, RoBERTa and ALBERT. We trained\nFastText and Word2vec word representations on\ntwo types of data, one which solely consisted of\nhinglish text, the other which was a mixture of\nhinglish and english text. These embeddings were\nthen used to predict the emotion of the tweet by\nserving as input to all the proposed models except\nthe transformer based models.\n3.5.1 Convolutional Neural Networks (CNNs)\nCNNs have been proven to be successful for multi\nclass classiﬁcation problems, where images are pro-\nvided as inputs (Ezat et al., 2020). In our case, word\nembeddings are given as input, from which features\nare extracted and ﬁnal classiﬁcation is performed.\nThe network architecture we employed has been\ndepicted in Fig. 2. Embedding layer serves as the\nﬁrst layer, which is used to transfer the word vector\nrepresentations of select words in the tweet under\nconsideration, to the model structure. Four convo-\nlutional layers in parallel receive the output of the\nembedding layer, followed by a global max pool-\ning layer, upon which dropout is applied. Three\ndense fully connected layers follow in which the\nlast layer is responsible for classiﬁcation. Appli-\ncation of dropout led to better convergence and\ndecreased difference in the training and validation\naccuracies.\n3.5.2 Recurrent Neural Networks (RNNs)\nThe context in which a word is used, determines\nthe meaning of the word, which in-turn may\nplay a signiﬁcant role in determining the overall\nsentiment of the sentence. For example,\nSentence 1 : There are multiple kinds of human\nbeings in this huge world.\nSentence 2 : She is very generous and kind-\nhearted.\nThe context in which the word ’kind’ is used,\nis different in both the sentences, thus the word\ncarries different meanings in different scenarios.\nRNNs are helpful in modelling the context of\na word, by having unique ways to capture the\ncontext of words using the surrounding words.\nLong Short-Term Memory (LSTM): LSTMs\nhave been shown to capture the relevant context for\nwords (Tran et al., 2017) as well as address the issue\nof vanishing gradients (Hochreiter and Schmidhu-\nber, 1997). The words which precede a particular\nFigure 2: LSTM block structure\nword, determine the context of the word. LSTMs\ninculcate memory cells in the network which serve\nto record the meaning of words that occurred pre-\nviously. In order to model this scenario, an LSTM\nbased network is constructed. In our model, an em-\nbedding layer, followed by an LSTM layer, further\nfollowed by 2 fully connected layers constitute the\nnetwork. The last layer, consisting of 6 neurons,\nis responsible for the classiﬁcation of the tweet’s\nemotion.\nBi-directional LSTM: Bi-directional LSTMs\nhave been proven successful in capturing the con-\ntext for text classiﬁcation tasks (Wang et al., 2016).\nThe words that precede as well as follow a par-\nticular word, determine the context of the word\nunder consideration. Thus, memory cells must ex-\nist in both directions in order to maintain the track\nof words that surround a particular word. This is\nachieved by appending 2 LSTM layers to the em-\nbedding layer, whose concatenated output (− →hT, ← −h1)\nis ﬂattened and fed to 2 fully connected (FC) layers.\nThe last layer carries out classiﬁcation, as is done\nin all other proposed models.\nAttention based Bi-directional LSTM: The\ntechnique of attention is based on learning the\nwords which contribute the most towards the over-\nall emotion of the sentence, and ﬁltering out the\nwords which contribute the least, i.e. noise. At-\ntention based BiLSTM differs in the manner of\nconcatenation of states, which is fed to the fully\nconnected (FC) layers. Apart from using concate-\nnated {− →hT, ← −h1}(− →hT denoting forward directed ﬁnal\nhidden state representation, ← −h1 denoting backward\ndirected ﬁrst hidden state representation) as inputs\nto the fully connected layers, attention based BiL-\nSTMs also take into consideration the weighted\nsummation of all time steps ( − →ht , ← −ht ). Hence, all\nhidden states serve as inputs to the 2 dense fully\nconnected layers, out of which the ﬁnal layer per-\nforms the classiﬁcation.\n3.5.3 Transformer Based Models\nBERT (bert-base-uncased): (Devlin et al., 2018)\nBeing a bidirectional transformer based model pre-\ntrained on a large Wikipedia and Toronto Book\nCorpus, BERT makes use of a combination of\nobjectives which are meant for the next sentence\nprediction and masked language modeling tasks.\nRoBERTa (roberta-base): (Liu et al., 2019) With\nsome modiﬁcations to the parameters of BERT,\ni.e. changing key hyperparameters, removing the\nnext sentence prediction objective, and training\nwith larger learning rate and batch size values,\nRoBERTa is built on top of BERT. ALBERT\n(albert-base-v2): (Lan et al., 2019) Trying to in-\ncrease the training speed and decrease the mem-\nory utilization of BERT, ALBERT is another varia-\ntion of BERT which repeats layers which are split\namong groups and splits the embedding matrix into\ntwo.\nParameter Value\nEmbedding Training\nEmbedding Size 300\nWindow Length 10\nSampling Polarity Negative\nCNN Training\nDropout 0.5\nStride 1\n#Kernels 200\nks1 3\nks2 6\nks3 9\nks4 12\nRNN Training\n#LSTM Units 150\nInput State Dropout 0.2\nRecurrent State Dropout 0.2\nTransformers Fine Tuning\nLearning Rate 1e-5\nEpsilon (Adam optimizer) 1e-8\nMaximum Sequence Length 256\nBatch Size 3\n#Epochs 5\nTable 2: Parameter Values\n4 Experimental Settings\nA split of ten percent was made on the total train-\ning dataset and the model was trained for a total\nof 20 epochs. At each epoch, we saved the model\ncheckpoints, and particularly used that checkpoint\nwhich was saved before the model begins to over-\nﬁt to calculate the metrics on the ten percent test\ndataset split.\nDifferent hyper parameters are involved for the\ntask of training embeddings as well as the mod-\nels. After working with several optimizers, loss\nfunctions and activation functions, the adam opti-\nmizer with categorical cross entropy loss function\nproduced the best results for all stated deep learn-\ning models. We used relu activation function for\nall the layers except the output layer, which has\nsigmoid activation function. We evaluated the per-\nformance of CNN models with different values for\nkernel sizes, activation functions, number of ker-\nnels, dropouts, strides and optimizers.\nWe use pre-trained models like bert-base-\nuncased, roberta-base, and albert-base-v2, to ﬁne\ntune the transformer based models on our dataset.\nDL Models\nHinglish\nData\nHinglish + English\nData\nWord2Vec FastText Word2Vec FastText\nCNN 62.22 62.24 63.00 62.52\nLSTM 63.83 63.69 64.04 64.59\nBi-LSTM 64.80 66.02 65.32 66.37\nBi-LSTM attention 66.65 67.34 67.44 68.29\nBERT 71.43\nRoBERTa 70.06\nALBERT 66.22\nTable 3: Accuracy of Deep Learning Models\nHugging-face4 API was used to ﬁne tune all the\ntransformer based models. Table 2 denotes the\nhyperparameter combinations used in the training\nof embeddings, CNN, RNN and ﬁne tuning trans-\nformer based models.\n5 Results\nUsing all features, (Vijay et al., 2018) show that the\nbaseline model i.e. the SVM classiﬁer with RBF\nkernel, presented an accuracy of 58.2%, when deal-\ning with the same emotion labels as we deal with\nin this paper. In the domain of emotion detection in\nHindi-English code-mixed data, as far as we know,\nwe are the ﬁrst to compare transformer based mod-\nels and word representations. In table 4, the results\nof CNN, RNN based models for both Word2Vec\nand FastText based word representations, along\nwith those of transformer based models have been\npresented. All proposed deep learning models yield\nbetter results than state-of-the-art models which\ndeal with these six emotion labels. The best ac-\ncuracy of 71.43% is achieved with BERT, as ex-\npected. All models utilizing embeddings trained\non Hinglish plus English data, perform better than\nthose using embeddings trained on Hinglish data.\nOne conceivable reason for this observation can\nbe the extra coverage of semantics and correla-\ntion between the word vectors of English infor-\nmation, which can be utilized for code blended\nHinglish information, hence serving as prior data\nfor Hinglish embeddings information. The method\nworks practically equivalent to a knowledge trans-\nfer step in which embeddings for English infor-\nmation are utilized as earlier information for em-\nbeddings of Hinglish information. Additionally,\nincreased accuracies of all models in case of Fast-\nText embeddings, as compared to the Word2Vec\n4https://huggingface.co/transformers/\nembeddings, is observed. One possible reason for\nthis could be the existence of code blended infor-\nmation where FastText enables the coverage of\ncode-mixed vocabulary as against word2Vec which\nworks only on the basis of overall context of word.\nThe transformer based BERT model clearly outper-\nforms both CNN and RNN based models, majorly\nbecause of its profound efﬁciency and its ability to\nprocess the input out of order.\nThe major obstacles in the task of detecting emo-\ntions in Hindi-English code-mixed data are han-\ndling the linguistic complexities associated with\ncode-mixed data and absence of clean data. Thus,\nwe require even more class-speciﬁc cleaner data,\nin order to reduce the effect of noise, which comes\nfrom spelling mistakes, stemming words and the\npresence of multiple contexts.\n6 Conclusion\nAs recent years have seen the rise in usage of social\nmedia for open expression of stance and opinions,\nsentiment analysis and opinion mining have gained\nattention as problems and become primary areas of\nresearch.\nIn this paper, we present an openly avail-\nable class-balanced dataset of Hindi-English code-\nmixed data, consisting of tweets belonging to 6\ntypes of emotions, which are happiness, sadness,\nanger, surprise, fear and disgust. We contrast\nthe performance of two types of word representa-\ntions, both trained on relevant scraped tweets from\nscratch. We develop two different types of em-\nbeddings, one which is trained on solely Hinglish\ntweets, the other which is trained on a mix of\nHinglish and English tweets, and present the per-\nformance in both cases. Also, we present deep\nlearning based models including CNNs, RNNs,\nand transformers, where BERT performs the best\namong all.\nAs future scope, the problem can be solved to\nobtain even better results by carrying out a com-\nparison of MUSE aligned vectors, pre-aligned Fast-\nText word embeddings and language speciﬁc trans-\nformer based word embeddings.\nReferences\n2009. The Cambridge Handbook of Linguistic Code-\nswitching. Cambridge Handbooks in Language and\nLinguistics. Cambridge University Press.\nCecilia Alm, Dan Roth, and Richard Sproat. 2005.\nEmotions from text: Machine learning for text-based\nemotion prediction. pages 579—-586.\nSaima Aman and Stan Szpakowicz. 2007. Identifying\nexpressions of emotion in text. In Proceedings of\nthe 10th International Conference on Text, Speech\nand Dialogue, TSD’07, page 196–205, Berlin, Hei-\ndelberg. Springer-Verlag.\nPeter Auer and Li Wei. 2007. Handbook of Multi-\nlingualism and Multilingual Communication. De\nGruyter Mouton, Berlin, Boston.\nKalika Bali, Jatin Sharma, Monojit Choudhury, and Yo-\ngarshi Vyas. 2014. “I am borrowing ya mixing ?” an\nanalysis of English-Hindi code mixing in Facebook.\nIn Proceedings of the First Workshop on Computa-\ntional Approaches to Code Switching, pages 116–\n126, Doha, Qatar. Association for Computational\nLinguistics.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135–146.\nLea Canales and Patricio Mart´ınez-Barco. 2014. Emo-\ntion detection from text: A survey.\nYing Chen, Sophia Lee, Shoushan Li, and Chu-Ren\nHuang. 2010. Emotion cause detection with linguis-\ntic constructions. volume 2, pages 179–187.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning.\nPaul Ekman. 1992. An argument for basic emotions.\nCognition and Emotion, 6(3-4):169–200.\nWael Ezat, Mohamed Dessouky, and Nabil Ismail.\n2020. Multi-class image classiﬁcation using deep\nlearning algorithm. Journal of Physics: Conference\nSeries, 1447:012021.\nJean Greaves, Travis Bradberry, and Patrick M.\nLencioni. 2009. Emotional Intelligence 2.0. CA :\nTalentSmart, San Diego.\nSakshi Gupta, Piyush Bansal, and Radhika Mamidi.\n2016. Resource creation for hindi-english code\nmixed social media text.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural Comput. ,\n9(8):1735–1780.\nAditya Joshi, Balamurali A R, and Pushpak Bhat-\ntacharyya. 2010. A fall-back strategy for sentiment\nanalysis in hindi: a case study. In Proceedings of the\n8th ICON.\nArmand Joulin, Edouard Grave, Piotr Bojanowski, and\nTomas Mikolov. 2017. Bag of tricks for efﬁcient\ntext classiﬁcation. In Proceedings of the 15th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Volume 2, Short Pa-\npers, pages 427–431, Valencia, Spain. Association\nfor Computational Linguistics.\nByoung Chul Ko. 2018. A brief review of facial emo-\ntion recognition based on visual information. Sen-\nsors, 18(2):1–20.\nEfthymios Kouloumpis, Theresa Wilson, and Jo-\nhanna D. Moore. 2011. Twitter sentiment analysis:\nThe good the bad and the omg! In ICWSM, pages\n538–541.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learn-\ning of language representations.\nHugo Liu, Henry Lieberman, and Ted Selker. 2003.\nA model of textual affect sensing using real-world\nknowledge. In Proceedings of the 8th International\nConference on Intelligent User Interfaces, IUI ’03,\npage 125–132, New York, NY , USA. Association for\nComputing Machinery.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach.\nAmit Mandelbaum and Adi Shalev. 2016. Word em-\nbeddings and their use in sentence classiﬁcation\ntasks.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-\nrado, and Jeffrey Dean. 2013. Distributed represen-\ntations of words and phrases and their composition-\nality. In Proceedings of the 26th International Con-\nference on Neural Information Processing Systems\n- Volume 2, NIPS’13, page 3111–3119, Red Hook,\nNY , USA. Curran Associates Inc.\nStella M ´onica, M ´onica C ´ardenas-Claros, and Neny\nIsharyanti. 2009. Code switching and code mixing\nin internet chating: betwen ”yes”, ”ya”, and ”si” a\ncase study. The jaltcall Journal, V ol 5:67–78.\nDong-Phuong Nguyen and A. Seza Dogruoz. 2013.\nWord level language identiﬁcation in online multi-\nlingual communication. In Proceedings of the 2013\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 857–862, United States.\nAssociation for Computational Linguistics (ACL).\nAlexander Pak and Patrick Paroubek. 2010. Twitter as\na corpus for sentiment analysis and opinion mining.\nvolume 10.\nRosalind W Picard. 2000. Affective computing. MIT\npress.\nShana Poplack and James Walker. 2003. Pieter\nmuysken, bilingual speech: a typology of code-\nmixing. cambridge: Cambridge university press,\n2000. pp. xvi+306. Journal of Linguistics, 39:678\n– 683.\nAmeya Prabhu, Aditya Joshi, Manish Shrivastava, and\nVasudeva Varma. 2016. Towards sub-word level\ncompositions for sentiment analysis of hindi-english\ncode mixed text.\nMaximilian Schmitt, Fabien Ringeval, and Bj ¨orn W.\nSchuller. 2016. At the border of acoustics and lin-\nguistics: Bag-of-audio-words for the recognition of\nemotions in speech. In INTERSPEECH, pages 495–\n499.\nDuc Tran, Hieu Mac, Van Tong, Hai-Anh Tran, and\nGiang Nguyen. 2017. A lstm based framework for\nhandling multiclass imbalance in dga botnet detec-\ntion. Neurocomputing, 275.\nDeepanshu Vijay, Aditya Bohra, Vinay Singh,\nSyed Sarfaraz Akhtar, and Manish Shrivastava.\n2018. Corpus creation and emotion prediction for\nHindi-English code-mixed social media text. In\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Compu-\ntational Linguistics: Student Research Workshop,\npages 128–135, New Orleans, Louisiana, USA.\nAssociation for Computational Linguistics.\nYequan Wang, Minlie Huang, Xiaoyan Zhu, and\nLi Zhao. 2016. Attention-based LSTM for aspect-\nlevel sentiment classiﬁcation. In Proceedings of\nthe 2016 Conference on Empirical Methods in Nat-\nural Language Processing, pages 606–615, Austin,\nTexas. Association for Computational Linguistics.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.8158661127090454
    },
    {
      "name": "Word2vec",
      "score": 0.813154935836792
    },
    {
      "name": "Computer science",
      "score": 0.7456945180892944
    },
    {
      "name": "Hindi",
      "score": 0.7227880954742432
    },
    {
      "name": "Social media",
      "score": 0.593524158000946
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5877975225448608
    },
    {
      "name": "Natural language processing",
      "score": 0.5510197877883911
    },
    {
      "name": "Deep learning",
      "score": 0.5227587223052979
    },
    {
      "name": "Transfer of learning",
      "score": 0.43807587027549744
    },
    {
      "name": "Language model",
      "score": 0.43313759565353394
    },
    {
      "name": "Machine learning",
      "score": 0.42520803213119507
    },
    {
      "name": "Code (set theory)",
      "score": 0.42316049337387085
    },
    {
      "name": "World Wide Web",
      "score": 0.15877985954284668
    },
    {
      "name": "Engineering",
      "score": 0.09459680318832397
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Embedding",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 20
}