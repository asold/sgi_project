{
    "title": "Analysis of CBDC Narrative OF Central Banks using Large Language Models",
    "url": "https://openalex.org/W4385643346",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A4296127756",
            "name": "Andrés Alonso-Robisco",
            "affiliations": [
                "Bank of Spain"
            ]
        },
        {
            "id": "https://openalex.org/A3211844022",
            "name": "José Manuel Carbó",
            "affiliations": [
                "Bank of Spain"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3157622159",
        "https://openalex.org/W2618048158",
        "https://openalex.org/W3122159891",
        "https://openalex.org/W4383198328",
        "https://openalex.org/W6775297055",
        "https://openalex.org/W3081120400",
        "https://openalex.org/W4290851814",
        "https://openalex.org/W2504279659",
        "https://openalex.org/W2967824347",
        "https://openalex.org/W2803655745",
        "https://openalex.org/W2098317876",
        "https://openalex.org/W3125113062",
        "https://openalex.org/W6850936240",
        "https://openalex.org/W4288777710",
        "https://openalex.org/W2942724790",
        "https://openalex.org/W2264844509",
        "https://openalex.org/W4312403192",
        "https://openalex.org/W2753070129",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4318014888",
        "https://openalex.org/W3124663043",
        "https://openalex.org/W4210933771",
        "https://openalex.org/W3191746858",
        "https://openalex.org/W2601780063",
        "https://openalex.org/W4317553041",
        "https://openalex.org/W2075682172",
        "https://openalex.org/W2804816507",
        "https://openalex.org/W4362711835",
        "https://openalex.org/W1582026748",
        "https://openalex.org/W2922583138",
        "https://openalex.org/W3199421848",
        "https://openalex.org/W4311460578",
        "https://openalex.org/W4230747085",
        "https://openalex.org/W4309199794",
        "https://openalex.org/W3125952890",
        "https://openalex.org/W1540757224",
        "https://openalex.org/W3167920075",
        "https://openalex.org/W4283071394",
        "https://openalex.org/W2903078275",
        "https://openalex.org/W2963809228",
        "https://openalex.org/W3200851150",
        "https://openalex.org/W4313215786",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W4225246703",
        "https://openalex.org/W3037252472"
    ],
    "abstract": "Central banks are increasingly using verbal communication for policymaking, focusing not only on traditional monetary policy, but also on a broad set of topics. One such topic is central bank digital currency (CBDC), which is attracting attention from the international community. The complex nature of this project means that it must be carefully designed to avoid unintended consequences, such as financial instability. We propose the use of different Natural Language Processing (NLP) techniques to better understand central banks’ stance towards CBDC, analyzing a set of central bank discourses from 2016 to 2022. We do this using traditional techniques, such as dictionary-based methods, and two large language models (LLMs), namely Bert and ChatGPT, concluding that LLMs better reflect the stance identified by human experts. In particular, we observe that ChatGPT exhibits a higher degree of alignment because it can capture subtler information than BERT. Our study suggests that LLMs are an effective tool to improve sentiment measurements for policy-specific texts, though they are not infallible and may be subject to new risks, like higher sensitivity to the length of texts, and prompt engineering.",
    "full_text": "ANALYSIS OF CBDC NARRATIVE  \nOF CENTRAL BANKS USING LARGE \nLANGUAGE MODELS\n2023\nAndres Alonso-Robisco and José Manuel Carbó \nDocumentos de Trabajo \nN.º 2321 \nANALYSIS OF CBDC NARRATIVE OF CENTRAL BANKS USING LARGE \nLANGUAGE MODELS\nDocumentos de Trabajo. N.º 2321\nAugust 2023\n(*) The authors work as senior economists in the Financial Innovation Division of Banco de España, and appreciate \nthe comments received from José Manuel Marqués, Ana Fernández, Sergio Gorjón and José Luis Romero. The \nopinions and analyses expressed in this paper are the responsibility of the authors and, therefore, do not \nnecessarily match with those of the Banco de España or the Eurosystem.\nAndres Alonso-Robisco\nBANCO DE ESPAÑA \nJosé Manuel Carbó  \nBANCO DE ESPAÑA\nANALYSIS OF CBDC NARRATIVE OF CENTRAL BANKS USING \nLARGE LANGUAGE MODELS (*)\nhttps://doi.org/10.53479/33412  \nThe Working Paper Series seeks to disseminate original research in economics and finance. All papers \nhave been anonymously refereed. By publishing these papers, the Banco de España aims to contribute \nto economic analysis and, in particular, to knowledge of the Spanish economy and its international \nenvironment. \nThe opinions and analyses in the Working Paper Series are the responsibility of the authors and, therefore, \ndo not necessarily coincide with those of the Banco de España or the Eurosystem. \nThe Banco de España disseminates its main reports and most of its publications via the Internet at the \nfollowing website: http://www.bde.es.\nReproduction for educational and non-commercial purposes is permitted provided that the source is \nacknowledged.  \n© BANCO DE ESPAÑA, Madrid, 2023\nISSN: 1579-8666 (on line)\nAbstract\nCentral banks are increasingly using verbal communication for policymaking, focusing not \nonly on traditional monetary policy, but also on a broad set of topics. One such topic is \ncentral bank digital currency (CBDC), which is attracting attention from the international \ncommunity. The complex nature of this project means that it must be carefully designed \nto avoid unintended consequences, such as financial instability. We propose the use of \ndifferent Natural Language Processing (NLP) techniques to better understand central \nbanks’ stance towards CBDC, analyzing a set of central bank discourses from 2016 to \n2022. We do this using traditional techniques, such as dictionary-based methods, and \ntwo large language models (LLMs), namely Bert and ChatGPT, concluding that LLMs \nbetter reflect the stance identified by human experts. In particular, we observe that \nChatGPT exhibits a higher degree of alignment because it can capture subtler information \nthan BERT. Our study suggests that LLMs are an effective tool to improve sentiment \nmeasurements for policy-specific texts, though they are not infallible and may be subject \nto new risks, like higher sensitivity to the length of texts, and prompt engineering.\nKeywords: ChatGPT, BERT, CBDC, digital money.\nJEL classification: G15, G41, E58.\nResumen\nLos bancos centrales están utilizando cada vez más la comunicación verbal en su estrategia, \nabarcando no solo la política monetaria tradicional, sino también un amplio conjunto de \ntemas. Uno de estos temas es la moneda digital de los bancos centrales (CBDC, por \nsus siglas en inglés), que está captando la atención de la comunidad internacional. La \nnaturaleza compleja de este proyecto implica que debe ser diseñado cuidadosamente \npara evitar consecuencias no deseadas, como la inestabilidad financiera. En este trabajo, \nproponemos utilizar diferentes técnicas de Procesamiento de Lenguaje Natural (NLP , por \nsus siglas en inglés) para comprender mejor la postura o sentimiento de los bancos \ncentrales hacia el CBDC, analizando un conjunto de discursos de los bancos centrales \ndesde 2016 hasta 2022. Para ello, utilizamos técnicas tradicionales, como los métodos \nbasados en diccionarios, y dos Modelos de Lenguaje de Gran Tamaño (LLM, por sus \nsiglas en inglés) como BERT y ChatGPT, llegando a la conclusión de que los LLM reflejan \nmejor la postura identificada por los expertos humanos. En particular, observamos que \nChatGPT muestra un mayor grado de alineación porque puede capturar información más \nsutil que BERT. Nuestro estudio sugiere que los LLM son una herramienta eficaz para \nmejorar las mediciones de sentimiento en textos específicos de contenido estratégico, \naunque no son infalibles y pueden estar sujetos a nuevos riesgos, como una mayor \nsensibilidad a la longitud de los textos y el diseño de las preguntas realizadas al propio \nmodelo.\nPalabras clave: ChatGPT, BERT, CBDC, moneda digital.\nCódigos JEL: G15, G41, E58.\nBANCO DE ESPAÑA\n7\nDOCUMENTO DE TRABAJO N.º 2321\nAnalysis of CBDC Narrative by Central Banks using Large\nLanguage Models\nAndres Alonso-Robisco †, Jose Manuel Carbo †∗\n†Banco de Espa˜ na\nARTICLE HISTORY\nCompiled July 10, 2023\nABSTRACT\nCentral banks are increasingly using verbal communication for policy making, focus-\ning not only on traditional monetary policy, but also on a broad set of topics. One\nsuch topic is central bank digital currency (CBDC), which is attracting the attention\nfrom the international community. The complex nature of this project means that\nit must be carefully designed to avoid unintended consequences, such as financial\ninstability. We propose to use different Natural Language Processing (NLP) tech-\nniques to better understand central banks’ stance towards CBDC, analyzing a set of\ncentral bank discourses from 2016 to 2022. To do so, we use traditional techniques\nsuch as dictionary-based methods, and two Large Language Models (LLM) such as\nBert and ChatGPT, concluding that LLMs better reflect the stance identified by\nhuman experts. In particular, we observe that ChatGPT exhibits a higher degree of\nalignment because it can capture subtler information than BERT. Our study sug-\ngests that LLMs are an effective tool to improve sentiment measurements on policy\nspecific texts, though they are not infallible and may be subject to new risks, like\nhigher sensitivity to the length of texts, and prompt engineering.\nKEYWORDS\nChatGPT, BERT, CBDC, digital money JEL codes: G15, G41, E58\n1. Introduction\nCentral banks play a fundamental role in the modern economy. One of the chan-\nnels through which they exert their influence is through communication, by affecting\nfinancial markets expectations to make monetary policy more predictable (McKay\net al. 2016). For example, they can communicate on future policy intentions, increase\ntransparency in their decision-making processes, or publishing economic projections.\nA well-known example is to steer market expectations using “forward-guidance”, a\ntool employed by some central banks to communicate their intentions about the fu-\nture path of key policy variables, such as interest rate, although there exist limitations\nto its power (Campbell et al. 2019; Cole 2021; McKay et al. 2016). Another example\nis the publication of inflation forecasts, which can influence the shape of yield curves\n(Hansen et al. 2019), households’ inflation expectations through “open mouth opera-\ntions” (Guthrie and Wright 2000), or written communication from supervisory bodies\n∗ The authors work as senior economists in the Financial Innovation Division of Banco de Espa˜ na, and\nappreciate the comments received from Jos´ e Manuel Marqu´ es, Ana Fern´ andez, Sergio Gorj´ on and Jos´ e Luis\nRomero. The opinions and analyses expressed in this paper are the responsibility of the authors and, therefore,\ndo not necessarily match with those of the Banco de Espa˜ na or the Eurosystem.\nAnalysis of CBDC Narrative by Central Banks using Large\nLanguage Models\nAndres Alonso-Robisco †, Jose Manuel Carbo †∗\n†Banco de Espa˜ na\nARTICLE HISTORY\nCompiled July 10, 2023\nABSTRACT\nCentral banks are increasingly using verbal communication for policy making, focus-\ning not only on traditional monetary policy, but also on a broad set of topics. One\nsuch topic is central bank digital currency (CBDC), which is attracting the attention\nfrom the international community. The complex nature of this project means that\nit must be carefully designed to avoid unintended consequences, such as financial\ninstability. We propose to use different Natural Language Processing (NLP) tech-\nniques to better understand central banks’ stance towards CBDC, analyzing a set of\ncentral bank discourses from 2016 to 2022. To do so, we use traditional techniques\nsuch as dictionary-based methods, and two Large Language Models (LLM) such as\nBert and ChatGPT, concluding that LLMs better reflect the stance identified by\nhuman experts. In particular, we observe that ChatGPT exhibits a higher degree of\nalignment because it can capture subtler information than BERT. Our study sug-\ngests that LLMs are an effective tool to improve sentiment measurements on policy\nspecific texts, though they are not infallible and may be subject to new risks, like\nhigher sensitivity to the length of texts, and prompt engineering.\nKEYWORDS\nChatGPT, BERT, CBDC, digital money JEL codes: G15, G41, E58\n1. Introduction\nCentral banks play a fundamental role in the modern economy. One of the chan-\nnels through which they exert their influence is through communication, by affecting\nfinancial markets expectations to make monetary policy more predictable (McKay\net al. 2016). For example, they can communicate on future policy intentions, increase\ntransparency in their decision-making processes, or publishing economic projections.\nA well-known example is to steer market expectations using “forward-guidance”, a\ntool employed by some central banks to communicate their intentions about the fu-\nture path of key policy variables, such as interest rate, although there exist limitations\nto its power (Campbell et al. 2019; Cole 2021; McKay et al. 2016). Another example\nis the publication of inflation forecasts, which can influence the shape of yield curves\n(Hansen et al. 2019), households’ inflation expectations through “open mouth opera-\ntions” (Guthrie and Wright 2000), or written communication from supervisory bodies\n∗ The authors work as senior economists in the Financial Innovation Division of Banco de Espa˜ na, and\nappreciate the comments received from Jos´ e Manuel Marqu´ es, Ana Fern´ andez, Sergio Gorj´ on and Jos´ e Luis\nRomero. The opinions and analyses expressed in this paper are the responsibility of the authors and, therefore,\ndo not necessarily match with those of the Banco de Espa˜ na or the Eurosystem.\noutlining corrective measures (Goldsmith-Pinkham et al. 2016). Overall, it seems that\nif central banks’ communication is done effectively, it builds credibility and maintains\na strong reputation for their commitment with the general public (Bholat et al. 2019;\nBlinder et al. 2008; Blinder 2018; Haldane and McMahon 2018).\nInterestingly, beyond monetary policy and supervisory actions, in recent times, cen-\ntral banks are expanding the scope of their communication. A topic that is gaining\nmomentum within central banks communication is Central Bank Digital Currency\n(CBDC), a new form of money that exists only in digital form. The implementation\nof a CBDC could enable central banks to engage in large-scale intermediation for re-\ntail and/ or wholesale deposits (Fernandez-Villaverde et al. 2021). As we will examine\nfurther, several aspects of a CBDC remain in discussion. Therefore, for the sake of\nsimplicity, we will adhere to Barrdear and Kumhof (2016) definition, which describes\nit as providing electronic access to domestic currency denominated balance-sheets of\ncentral banks, independent of the technological solution applied (whether token-based\nor account-based), or any other architecture details (e.g.: interest-bearing deposits or\nnot). Depending on the final design, a CBDC could serve various purposes, such as\npayment system integrity, promoting financial inclusion (Auer et al. 2022b), and fos-\ntering innovation, among others. On the other hand, its introduction could suppose\nunwanted effects on the financial system, such as a flight from commercial deposits,\ndestabilizing financial intermediation, anonymity issues or privacy concerns (Auer and\nBoehme 2020; Auer et al. 2022a; Davoodalhosseini 2022; Ferrari Minesso et al. 2022).\nCurrently, many central banks are exploring the potential value of some sort\nof CBDC, though at various paces. Some are simply incorporating this topic into\nspeeches, others are making project announcements, while a few are conducting pilots\nand live experimentation (Auer et al. 2020). The gaining momentum of these discus-\nsions between central banks could be attributed to the growing demand to cater to\nthe digital economy, react to private initiatives (like Facebook’s project Libra), keep\npace with other central banks innovative projects, or to adapt the payment systems to\nan ongoing disruptive transformation, to name a few. As digital payments gradually\nreplace cash transactions (Tarlin 2021), big tech companies compete for dominance\nin payments services, and new types of digital assets threatens the stability of the\nfinancial system, the relevance of CBDCs has increased Lagarde, 2022.\nThere are several approaches to the design of CBDCs, and these vary significantly\nbetween jurisdictions, which gives an idea of the complexity of this issue. Auer and\nBoehme (2020) describe the main design options, identifying the possible trade-offs in\neach one. The first decision to make is which operational role of the central bank might\ntake. Options range from a role as a direct CBDC provider, to intermediary solutions\nwhere central banks maintain a central ledger of retail transactions, to solutions where\nonly wholesale transactions are considered. A second technical design decision is the\ninfrastructure, which could be based on Distributed Ledger Technology (DLT) or a\nconventional centralized database. Additionally, more dimensions of this problem con-\ncerns how will consumer access the CBDC (account-based or digital tokens) or the\ninterlinkages of CBDC with cross-border payments.\nAll these possibilities in the choice of CBDC design and operation generate a high\nlevel of uncertainty around its eventual form. For this reason, we believe that it is\nrelevant to analyze the tone of the central banks (represented by speeches by their\ngovernors) to enhance the predictability and transparency at this time when several\nof the CBDC projects are still in the initial stages of development. A better mea-\nsurement of the sentiment towards CBDC could refine central banks’ intentions and\npolicy direction, as well as improve its perceived credibility and help managing market\n2\noutlining corrective measures (Goldsmith-Pinkham et al. 2016). Overall, it seems that\nif central banks’ communication is done effectively, it builds credibility and maintains\na strong reputation for their commitment with the general public (Bholat et al. 2019;\nBlinder et al. 2008; Blinder 2018; Haldane and McMahon 2018).\nInterestingly, beyond monetary policy and supervisory actions, in recent times, cen-\ntral banks are expanding the scope of their communication. A topic that is gaining\nmomentum within central banks communication is Central Bank Digital Currency\n(CBDC), a new form of money that exists only in digital form. The implementation\nof a CBDC could enable central banks to engage in large-scale intermediation for re-\ntail and/ or wholesale deposits (Fernandez-Villaverde et al. 2021). As we will examine\nfurther, several aspects of a CBDC remain in discussion. Therefore, for the sake of\nsimplicity, we will adhere to Barrdear and Kumhof (2016) definition, which describes\nit as providing electronic access to domestic currency denominated balance-sheets of\ncentral banks, independent of the technological solution applied (whether token-based\nor account-based), or any other architecture details (e.g.: interest-bearing deposits or\nnot). Depending on the final design, a CBDC could serve various purposes, such as\npayment system integrity, promoting financial inclusion (Auer et al. 2022b), and fos-\ntering innovation, among others. On the other hand, its introduction could suppose\nunwanted effects on the financial system, such as a flight from commercial deposits,\ndestabilizing financial intermediation, anonymity issues or privacy concerns (Auer and\nBoehme 2020; Auer et al. 2022a; Davoodalhosseini 2022; Ferrari Minesso et al. 2022).\nCurrently, many central banks are exploring the potential value of some sort\nof CBDC, though at various paces. Some are simply incorporating this topic into\nspeeches, others are making project announcements, while a few are conducting pilots\nand live experimentation (Auer et al. 2020). The gaining momentum of these discus-\nsions between central banks could be attributed to the growing demand to cater to\nthe digital economy, react to private initiatives (like Facebook’s project Libra), keep\npace with other central banks innovative projects, or to adapt the payment systems to\nan ongoing disruptive transformation, to name a few. As digital payments gradually\nreplace cash transactions (Tarlin 2021), big tech companies compete for dominance\nin payments services, and new types of digital assets threatens the stability of the\nfinancial system, the relevance of CBDCs has increased Lagarde, 2022.\nThere are several approaches to the design of CBDCs, and these vary significantly\nbetween jurisdictions, which gives an idea of the complexity of this issue. Auer and\nBoehme (2020) describe the main design options, identifying the possible trade-offs in\neach one. The first decision to make is which operational role of the central bank might\ntake. Options range from a role as a direct CBDC provider, to intermediary solutions\nwhere central banks maintain a central ledger of retail transactions, to solutions where\nonly wholesale transactions are considered. A second technical design decision is the\ninfrastructure, which could be based on Distributed Ledger Technology (DLT) or a\nconventional centralized database. Additionally, more dimensions of this problem con-\ncerns how will consumer access the CBDC (account-based or digital tokens) or the\ninterlinkages of CBDC with cross-border payments.\nAll these possibilities in the choice of CBDC design and operation generate a high\nlevel of uncertainty around its eventual form. For this reason, we believe that it is\nrelevant to analyze the tone of the central banks (represented by speeches by their\ngovernors) to enhance the predictability and transparency at this time when several\nof the CBDC projects are still in the initial stages of development. A better mea-\nsurement of the sentiment towards CBDC could refine central banks’ intentions and\npolicy direction, as well as improve its perceived credibility and help managing market\n2\noutlining corrective measures (Goldsmith-Pinkham et al. 2016). Overall, it seems that\nif central banks’ communication is done effectively, it builds credibility and maintains\na strong reputation for their commitment with the general public (Bholat et al. 2019;\nBlinder et al. 2008; Blinder 2018; Haldane and McMahon 2018).\nInterestingly, beyond monetary policy and supervisory actions, in recent times, cen-\ntral banks are expanding the scope of their communication. A topic that is gaining\nmomentum within central banks communication is Central Bank Digital Currency\n(CBDC), a new form of money that exists only in digital form. The implementation\nof a CBDC could enable central banks to engage in large-scale intermediation for re-\ntail and/ or wholesale deposits (Fernandez-Villaverde et al. 2021). As we will examine\nfurther, several aspects of a CBDC remain in discussion. Therefore, for the sake of\nsimplicity, we will adhere to Barrdear and Kumhof (2016) definition, which describes\nit as providing electronic access to domestic currency denominated balance-sheets of\ncentral banks, independent of the technological solution applied (whether token-based\nor account-based), or any other architecture details (e.g.: interest-bearing deposits or\nnot). Depending on the final design, a CBDC could serve various purposes, such as\npayment system integrity, promoting financial inclusion (Auer et al. 2022b), and fos-\ntering innovation, among others. On the other hand, its introduction could suppose\nunwanted effects on the financial system, such as a flight from commercial deposits,\ndestabilizing financial intermediation, anonymity issues or privacy concerns (Auer and\nBoehme 2020; Auer et al. 2022a; Davoodalhosseini 2022; Ferrari Minesso et al. 2022).\nCurrently, many central banks are exploring the potential value of some sort\nof CBDC, though at various paces. Some are simply incorporating this topic into\nspeeches, others are making project announcements, while a few are conducting pilots\nand live experimentation (Auer et al. 2020). The gaining momentum of these discus-\nsions between central banks could be attributed to the growing demand to cater to\nthe digital economy, react to private initiatives (like Facebook’s project Libra), keep\npace with other central banks innovative projects, or to adapt the payment systems to\nan ongoing disruptive transformation, to name a few. As digital payments gradually\nreplace cash transactions (Tarlin 2021), big tech companies compete for dominance\nin payments services, and new types of digital assets threatens the stability of the\nfinancial system, the relevance of CBDCs has increased Lagarde, 2022.\nThere are several approaches to the design of CBDCs, and these vary significantly\nbetween jurisdictions, which gives an idea of the complexity of this issue. Auer and\nBoehme (2020) describe the main design options, identifying the possible trade-offs in\neach one. The first decision to make is which operational role of the central bank might\ntake. Options range from a role as a direct CBDC provider, to intermediary solutions\nwhere central banks maintain a central ledger of retail transactions, to solutions where\nonly wholesale transactions are considered. A second technical design decision is the\ninfrastructure, which could be based on Distributed Ledger Technology (DLT) or a\nconventional centralized database. Additionally, more dimensions of this problem con-\ncerns how will consumer access the CBDC (account-based or digital tokens) or the\ninterlinkages of CBDC with cross-border payments.\nAll these possibilities in the choice of CBDC design and operation generate a high\nlevel of uncertainty around its eventual form. For this reason, we believe that it is\nrelevant to analyze the tone of the central banks (represented by speeches by their\ngovernors) to enhance the predictability and transparency at this time when several\nof the CBDC projects are still in the initial stages of development. A better mea-\nsurement of the sentiment towards CBDC could refine central banks’ intentions and\npolicy direction, as well as improve its perceived credibility and help managing market\n2\nBANCO DE ESPAÑA\n8\nDOCUMENTO DE TRABAJO N.º 2321\noutlining corrective measures (Goldsmith-Pinkham et al. 2016). Overall, it seems that\nif central banks’ communication is done effectively, it builds credibility and maintains\na strong reputation for their commitment with the general public (Bholat et al. 2019;\nBlinder et al. 2008; Blinder 2018; Haldane and McMahon 2018).\nInterestingly, beyond monetary policy and supervisory actions, in recent times, cen-\ntral banks are expanding the scope of their communication. A topic that is gaining\nmomentum within central banks communication is Central Bank Digital Currency\n(CBDC), a new form of money that exists only in digital form. The implementation\nof a CBDC could enable central banks to engage in large-scale intermediation for re-\ntail and/ or wholesale deposits (Fernandez-Villaverde et al. 2021). As we will examine\nfurther, several aspects of a CBDC remain in discussion. Therefore, for the sake of\nsimplicity, we will adhere to Barrdear and Kumhof (2016) definition, which describes\nit as providing electronic access to domestic currency denominated balance-sheets of\ncentral banks, independent of the technological solution applied (whether token-based\nor account-based), or any other architecture details (e.g.: interest-bearing deposits or\nnot). Depending on the final design, a CBDC could serve various purposes, such as\npayment system integrity, promoting financial inclusion (Auer et al. 2022b), and fos-\ntering innovation, among others. On the other hand, its introduction could suppose\nunwanted effects on the financial system, such as a flight from commercial deposits,\ndestabilizing financial intermediation, anonymity issues or privacy concerns (Auer and\nBoehme 2020; Auer et al. 2022a; Davoodalhosseini 2022; Ferrari Minesso et al. 2022).\nCurrently, many central banks are exploring the potential value of some sort\nof CBDC, though at various paces. Some are simply incorporating this topic into\nspeeches, others are making project announcements, while a few are conducting pilots\nand live experimentation (Auer et al. 2020). The gaining momentum of these discus-\nsions between central banks could be attributed to the growing demand to cater to\nthe digital economy, react to private initiatives (like Facebook’s project Libra), keep\npace with other central banks innovative projects, or to adapt the payment systems to\nan ongoing disruptive transformation, to name a few. As digital payments gradually\nreplace cash transactions (Tarlin 2021), big tech companies compete for dominance\nin payments services, and new types of digital assets threatens the stability of the\nfinancial system, the relevance of CBDCs has increased Lagarde, 2022.\nThere are several approaches to the design of CBDCs, and these vary significantly\nbetween jurisdictions, which gives an idea of the complexity of this issue. Auer and\nBoehme (2020) describe the main design options, identifying the possible trade-offs in\neach one. The first decision to make is which operational role of the central bank might\ntake. Options range from a role as a direct CBDC provider, to intermediary solutions\nwhere central banks maintain a central ledger of retail transactions, to solutions where\nonly wholesale transactions are considered. A second technical design decision is the\ninfrastructure, which could be based on Distributed Ledger Technology (DLT) or a\nconventional centralized database. Additionally, more dimensions of this problem con-\ncerns how will consumer access the CBDC (account-based or digital tokens) or the\ninterlinkages of CBDC with cross-border payments.\nAll these possibilities in the choice of CBDC design and operation generate a high\nlevel of uncertainty around its eventual form. For this reason, we believe that it is\nrelevant to analyze the tone of the central banks (represented by speeches by their\ngovernors) to enhance the predictability and transparency at this time when several\nof the CBDC projects are still in the initial stages of development. A better mea-\nsurement of the sentiment towards CBDC could refine central banks’ intentions and\npolicy direction, as well as improve its perceived credibility and help managing market\n2\nexpectations and investor sentiment (Scharnowski 2022).\nTo this purpose, in this paper we aim to quantify central banks’ sentiment towards\nCBDC using two of the most widely used Large Language Models (LLMs) such as\nChatGPT (OpenAI, 2023), and BERT (Devlin et al. 2019), in particular, a pre-trained\nversion of BERT for financial communication known as FinBERT (Yang et al. 2020). 1\nWe do this by evaluating a series of central bank speeches on the topic of CBDC,\nand we compare the sentiment obtained with LLMs with that obtained through a\ndictionary-based method (i.e.: polarity), and with the sentiment from human experts\nor labeled data. Our contribution is twofold. First, to the best of our knowledge,\nwe harness for the first time the power of recent developments in LLMs to compute\nsentiment in an area highly relevant area to financial regulation like digital money.\nSecond, by analyzing the sentiment calculated by each method, we can understand\nthe potential benefits and limitations of each approach. The main conclusions are that\nthe sentiment towards CBDC expressed by central banks is increasingly positive, and\nthat the sentiment obtained with ChatGPT is the most reliable, since it is the most\nsimilar to the labeled data because it is able to capture the subtleties of the text and\nis less noisy. To this purpose, we use the recently updated (January, 2023) database\nof CBDC speeches and reports by central banks compiled by Auer et al. (2020). This\ndatabase offers a unique opportunity to compare the results of language models to the\nexpert labels that humans assign to each speech stance.\nThe remaining part of this paper is structured as follows. Section 2 details the\nrelevant literature review to give pertinent context to this study. Section 3 explains\nthe dataset, Section 4 delves deeper into the methodology of all three language mod-\nels, Section 5 provides the results, and Section 6 concludes, with remarks for further\nresearch.\n2. Literature review\nAs mentioned before, many central banks are contemplating whether to issue a CBDC,\nas it has potential benefits, as an attractive public alternative to traditional deposits\nheld in private banks (Fernandez-Villaverde et al. 2021). Interestingly, Adrian and\nMancini-Griffoli (2021) provide a framework for comparing traditional forms of money\nwith their new digital equivalent. However, using a CBDC might also be costly for\nagents having therefore implications in terms of financial stability and the design of\nan optimal monetary policy strategy (Davoodalhosseini 2022; Ferrari Minesso et al.\n2022). To deepen on the motivation and design features CBDC projects worldwide\nwe revert to Auer et al. (2022a) which offers a guided tour of the growing CBDC\nliterature on the microeconomic considerations related to projects architectures, tech-\nnologies, and privacy as well as the macroeconomic implications for the economy and\nthe financial system. For instance, following this line of research, Siklos et al. (2018)\nstudy the macroeconomic impact of CBDC, concluding that its adoption needs not\nto impair inflation control, and Barrdear and Kumhof (2016) find a potential positive\nimpact on Gross Domestic Product in the US. On the other hand, (Andolfatto 2020)\ninvestigates important microeconomics issues, like how CBDC can be expected to im-\npact a monopolistic banking sector, concluding that a properly designed CBDC is not\nlikely to threaten financial stability. Indeed, there is still an ample range of uncer-\ntainty surrounding the impact of CBDC depending on its final design attributes (e.g.:\n1From now on we will use the generic term BERT, though referring always to this particular class.\n3\nexpectations and investor sentiment (Scharnowski 2022).\nTo this purpose, in this paper we aim to quantify central banks’ sentiment towards\nCBDC using two of the most widely used Large Language Models (LLMs) such as\nChatGPT (OpenAI, 2023), and BERT (Devlin et al. 2019), in particular, a pre-trained\nversion of BERT for financial communication known as FinBERT (Yang et al. 2020). 1\nWe do this by evaluating a series of central bank speeches on the topic of CBDC,\nand we compare the sentiment obtained with LLMs with that obtained through a\ndictionary-based method (i.e.: polarity), and with the sentiment from human experts\nor labeled data. Our contribution is twofold. First, to the best of our knowledge,\nwe harness for the first time the power of recent developments in LLMs to compute\nsentiment in an area highly relevant area to financial regulation like digital money.\nSecond, by analyzing the sentiment calculated by each method, we can understand\nthe potential benefits and limitations of each approach. The main conclusions are that\nthe sentiment towards CBDC expressed by central banks is increasingly positive, and\nthat the sentiment obtained with ChatGPT is the most reliable, since it is the most\nsimilar to the labeled data because it is able to capture the subtleties of the text and\nis less noisy. To this purpose, we use the recently updated (January, 2023) database\nof CBDC speeches and reports by central banks compiled by Auer et al. (2020). This\ndatabase offers a unique opportunity to compare the results of language models to the\nexpert labels that humans assign to each speech stance.\nThe remaining part of this paper is structured as follows. Section 2 details the\nrelevant literature review to give pertinent context to this study. Section 3 explains\nthe dataset, Section 4 delves deeper into the methodology of all three language mod-\nels, Section 5 provides the results, and Section 6 concludes, with remarks for further\nresearch.\n2. Literature review\nAs mentioned before, many central banks are contemplating whether to issue a CBDC,\nas it has potential benefits, as an attractive public alternative to traditional deposits\nheld in private banks (Fernandez-Villaverde et al. 2021). Interestingly, Adrian and\nMancini-Griffoli (2021) provide a framework for comparing traditional forms of money\nwith their new digital equivalent. However, using a CBDC might also be costly for\nagents having therefore implications in terms of financial stability and the design of\nan optimal monetary policy strategy (Davoodalhosseini 2022; Ferrari Minesso et al.\n2022). To deepen on the motivation and design features CBDC projects worldwide\nwe revert to Auer et al. (2022a) which offers a guided tour of the growing CBDC\nliterature on the microeconomic considerations related to projects architectures, tech-\nnologies, and privacy as well as the macroeconomic implications for the economy and\nthe financial system. For instance, following this line of research, Siklos et al. (2018)\nstudy the macroeconomic impact of CBDC, concluding that its adoption needs not\nto impair inflation control, and Barrdear and Kumhof (2016) find a potential positive\nimpact on Gross Domestic Product in the US. On the other hand, (Andolfatto 2020)\ninvestigates important microeconomics issues, like how CBDC can be expected to im-\npact a monopolistic banking sector, concluding that a properly designed CBDC is not\nlikely to threaten financial stability. Indeed, there is still an ample range of uncer-\ntainty surrounding the impact of CBDC depending on its final design attributes (e.g.:\n1From now on we will use the generic term BERT, though referring always to this particular class.\n3\noutlining corrective measures (Goldsmith-Pinkham et al. 2016). Overall, it seems that\nif central banks’ communication is done effectively, it builds credibility and maintains\na strong reputation for their commitment with the general public (Bholat et al. 2019;\nBlinder et al. 2008; Blinder 2018; Haldane and McMahon 2018).\nInterestingly, beyond monetary policy and supervisory actions, in recent times, cen-\ntral banks are expanding the scope of their communication. A topic that is gaining\nmomentum within central banks communication is Central Bank Digital Currency\n(CBDC), a new form of money that exists only in digital form. The implementation\nof a CBDC could enable central banks to engage in large-scale intermediation for re-\ntail and/ or wholesale deposits (Fernandez-Villaverde et al. 2021). As we will examine\nfurther, several aspects of a CBDC remain in discussion. Therefore, for the sake of\nsimplicity, we will adhere to Barrdear and Kumhof (2016) definition, which describes\nit as providing electronic access to domestic currency denominated balance-sheets of\ncentral banks, independent of the technological solution applied (whether token-based\nor account-based), or any other architecture details (e.g.: interest-bearing deposits or\nnot). Depending on the final design, a CBDC could serve various purposes, such as\npayment system integrity, promoting financial inclusion (Auer et al. 2022b), and fos-\ntering innovation, among others. On the other hand, its introduction could suppose\nunwanted effects on the financial system, such as a flight from commercial deposits,\ndestabilizing financial intermediation, anonymity issues or privacy concerns (Auer and\nBoehme 2020; Auer et al. 2022a; Davoodalhosseini 2022; Ferrari Minesso et al. 2022).\nCurrently, many central banks are exploring the potential value of some sort\nof CBDC, though at various paces. Some are simply incorporating this topic into\nspeeches, others are making project announcements, while a few are conducting pilots\nand live experimentation (Auer et al. 2020). The gaining momentum of these discus-\nsions between central banks could be attributed to the growing demand to cater to\nthe digital economy, react to private initiatives (like Facebook’s project Libra), keep\npace with other central banks innovative projects, or to adapt the payment systems to\nan ongoing disruptive transformation, to name a few. As digital payments gradually\nreplace cash transactions (Tarlin 2021), big tech companies compete for dominance\nin payments services, and new types of digital assets threatens the stability of the\nfinancial system, the relevance of CBDCs has increased Lagarde, 2022.\nThere are several approaches to the design of CBDCs, and these vary significantly\nbetween jurisdictions, which gives an idea of the complexity of this issue. Auer and\nBoehme (2020) describe the main design options, identifying the possible trade-offs in\neach one. The first decision to make is which operational role of the central bank might\ntake. Options range from a role as a direct CBDC provider, to intermediary solutions\nwhere central banks maintain a central ledger of retail transactions, to solutions where\nonly wholesale transactions are considered. A second technical design decision is the\ninfrastructure, which could be based on Distributed Ledger Technology (DLT) or a\nconventional centralized database. Additionally, more dimensions of this problem con-\ncerns how will consumer access the CBDC (account-based or digital tokens) or the\ninterlinkages of CBDC with cross-border payments.\nAll these possibilities in the choice of CBDC design and operation generate a high\nlevel of uncertainty around its eventual form. For this reason, we believe that it is\nrelevant to analyze the tone of the central banks (represented by speeches by their\ngovernors) to enhance the predictability and transparency at this time when several\nof the CBDC projects are still in the initial stages of development. A better mea-\nsurement of the sentiment towards CBDC could refine central banks’ intentions and\npolicy direction, as well as improve its perceived credibility and help managing market\n2\nBANCO DE ESPAÑA\n9\nDOCUMENTO DE TRABAJO N.º 2321\nexpectations and investor sentiment (Scharnowski 2022).\nTo this purpose, in this paper we aim to quantify central banks’ sentiment towards\nCBDC using two of the most widely used Large Language Models (LLMs) such as\nChatGPT (OpenAI, 2023), and BERT (Devlin et al. 2019), in particular, a pre-trained\nversion of BERT for financial communication known as FinBERT (Yang et al. 2020). 1\nWe do this by evaluating a series of central bank speeches on the topic of CBDC,\nand we compare the sentiment obtained with LLMs with that obtained through a\ndictionary-based method (i.e.: polarity), and with the sentiment from human experts\nor labeled data. Our contribution is twofold. First, to the best of our knowledge,\nwe harness for the first time the power of recent developments in LLMs to compute\nsentiment in an area highly relevant area to financial regulation like digital money.\nSecond, by analyzing the sentiment calculated by each method, we can understand\nthe potential benefits and limitations of each approach. The main conclusions are that\nthe sentiment towards CBDC expressed by central banks is increasingly positive, and\nthat the sentiment obtained with ChatGPT is the most reliable, since it is the most\nsimilar to the labeled data because it is able to capture the subtleties of the text and\nis less noisy. To this purpose, we use the recently updated (January, 2023) database\nof CBDC speeches and reports by central banks compiled by Auer et al. (2020). This\ndatabase offers a unique opportunity to compare the results of language models to the\nexpert labels that humans assign to each speech stance.\nThe remaining part of this paper is structured as follows. Section 2 details the\nrelevant literature review to give pertinent context to this study. Section 3 explains\nthe dataset, Section 4 delves deeper into the methodology of all three language mod-\nels, Section 5 provides the results, and Section 6 concludes, with remarks for further\nresearch.\n2. Literature review\nAs mentioned before, many central banks are contemplating whether to issue a CBDC,\nas it has potential benefits, as an attractive public alternative to traditional deposits\nheld in private banks (Fernandez-Villaverde et al. 2021). Interestingly, Adrian and\nMancini-Griffoli (2021) provide a framework for comparing traditional forms of money\nwith their new digital equivalent. However, using a CBDC might also be costly for\nagents having therefore implications in terms of financial stability and the design of\nan optimal monetary policy strategy (Davoodalhosseini 2022; Ferrari Minesso et al.\n2022). To deepen on the motivation and design features CBDC projects worldwide\nwe revert to Auer et al. (2022a) which offers a guided tour of the growing CBDC\nliterature on the microeconomic considerations related to projects architectures, tech-\nnologies, and privacy as well as the macroeconomic implications for the economy and\nthe financial system. For instance, following this line of research, Siklos et al. (2018)\nstudy the macroeconomic impact of CBDC, concluding that its adoption needs not\nto impair inflation control, and Barrdear and Kumhof (2016) find a potential positive\nimpact on Gross Domestic Product in the US. On the other hand, (Andolfatto 2020)\ninvestigates important microeconomics issues, like how CBDC can be expected to im-\npact a monopolistic banking sector, concluding that a properly designed CBDC is not\nlikely to threaten financial stability. Indeed, there is still an ample range of uncer-\ntainty surrounding the impact of CBDC depending on its final design attributes (e.g.:\n1From now on we will use the generic term BERT, though referring always to this particular class.\n3\nanonymity, rate of return, etc.). 2 This translates in wide estimates for its adoption.\nFor instance, Li (2023) finds that Canadian households’ demand for CBDC can range\nfrom 4% to 52% of total liquid assets, and banks endogenous responses to CBDC could\nconsiderably constraint its upper bound take-up to 20%. In parallel, Kiff et al. (2020),\nis a valuable survey that reviews the processes, roles, and responsibilities that would\nneed to be defined for the issuance of a retail CBDC, while Gorj´ on Rivas (2022), and\nRomero Romero Ugarte et al. (2021) offers some insights on wholesale CBDC based\non different technologies, like distributed ledgers (DLT).\nA critical article for understanding the increased interest in CBDCs among the\ncentral banking community is Auer et al. (2020). In that research, the authors take\nstock of design efforts, technical approaches, and differing stances on CBDC issuance\nacross jurisdictions that have conducted CBDC experimentation. To do this, from a\ndatabase of more than 16,000 central bank speeches by central bank representatives,\nthey extract 351 texts that explicitly mention CBDC. In addition, they evaluate with\ntheir expert judgment the political position of these speeches. 3 Therefore, as our goal\nis to measure the sentiment of central banks towards CBDC, we will compare the\nperformance of Large Language Models (LLMs) with traditional NLP techniques (like\ndictionary-based methods), and on a novel way, with human expert labels. This will\nallow to assess the benefits and limitations of these models.\nFirstly, we find that studying the sentiment of central bank communication is a\nrelevant research topic, as highlighted in studies such as (Born et al. 2014), who show\nhow sentiment of central banks’ speeches about financial stability have a significant\neffect on market returns and volatility. Also Hansen et al. (2018) study central bank\ntransparency using topic modeling in an event study around 1993, the year the Fed\nstarted to release the FOMC meeting transcripts. Notably, the evolution of NLP is\naccelerating the research in this area. A good example is Hansen and Kazinnik (2023),\nwho use GPT models to decipher the communication from the Federal Reserve, finding\nthat these novel models surpass the performance of more traditional classification\nmethods.\nOn the other hand, the magnitude of the potential implications for an open-wide\neconomy of the introduction of a CBDC is reflected in Ferrari Minesso et al. (2022)\nwho analyze the international transmission of monetary policy decisions and technol-\nogy shocks in the presence and absence of a CBDC. Following this line, the impact of\nCBDC communication in the real economy and financial markets is studied in Burlon\net al. (2022), who provide evidence on the estimated effects of CBDC news on bank\nvaluations and lending in the euro area, finding that it depends on the quantity of\nCBDC in circulation. Also, Wang et al. (2022), find that CBDC attention and un-\ncertainty indices from financial news have a positive relationship on cryptocurrencies,\nforeign exchange, and bond markets, as well as VIX and gold. Closer to our study,\nScharnowski (2022) use the raw CBDC sentiment index, as provided by Auer et al.\n(2020), to study the market reaction to central banks’ speeches on CBDC, observ-\ning that cryptocurrency prices increase more strongly after speeches taking a positive\ntone. Similarly, Tian et al. (2023) relies al well on the texts compiled by Auer et al.\n(2020) but they propose to measure central banks’ sentiment toward CBDC based on\nthe polarity of the words using the Loughran-McDonald Financial Dictionary (LMFD)\n(Loughran and McDonald 2011). They find that different cybersecurity risks (cyber-\nware and cyberattacks) can have an impact on the posture of central banks towards\n2See for instance, Conlon et al. (2022) about “to CBDC or not CBDC”.\n3At the moment of drafting this paper, it is updated up to January 2023.\n4\nanonymity, rate of return, etc.). 2 This translates in wide estimates for its adoption.\nFor instance, Li (2023) finds that Canadian households’ demand for CBDC can range\nfrom 4% to 52% of total liquid assets, and banks endogenous responses to CBDC could\nconsiderably constraint its upper bound take-up to 20%. In parallel, Kiff et al. (2020),\nis a valuable survey that reviews the processes, roles, and responsibilities that would\nneed to be defined for the issuance of a retail CBDC, while Gorj´ on Rivas (2022), and\nRomero Romero Ugarte et al. (2021) offers some insights on wholesale CBDC based\non different technologies, like distributed ledgers (DLT).\nA critical article for understanding the increased interest in CBDCs among the\ncentral banking community is Auer et al. (2020). In that research, the authors take\nstock of design efforts, technical approaches, and differing stances on CBDC issuance\nacross jurisdictions that have conducted CBDC experimentation. To do this, from a\ndatabase of more than 16,000 central bank speeches by central bank representatives,\nthey extract 351 texts that explicitly mention CBDC. In addition, they evaluate with\ntheir expert judgment the political position of these speeches. 3 Therefore, as our goal\nis to measure the sentiment of central banks towards CBDC, we will compare the\nperformance of Large Language Models (LLMs) with traditional NLP techniques (like\ndictionary-based methods), and on a novel way, with human expert labels. This will\nallow to assess the benefits and limitations of these models.\nFirstly, we find that studying the sentiment of central bank communication is a\nrelevant research topic, as highlighted in studies such as (Born et al. 2014), who show\nhow sentiment of central banks’ speeches about financial stability have a significant\neffect on market returns and volatility. Also Hansen et al. (2018) study central bank\ntransparency using topic modeling in an event study around 1993, the year the Fed\nstarted to release the FOMC meeting transcripts. Notably, the evolution of NLP is\naccelerating the research in this area. A good example is Hansen and Kazinnik (2023),\nwho use GPT models to decipher the communication from the Federal Reserve, finding\nthat these novel models surpass the performance of more traditional classification\nmethods.\nOn the other hand, the magnitude of the potential implications for an open-wide\neconomy of the introduction of a CBDC is reflected in Ferrari Minesso et al. (2022)\nwho analyze the international transmission of monetary policy decisions and technol-\nogy shocks in the presence and absence of a CBDC. Following this line, the impact of\nCBDC communication in the real economy and financial markets is studied in Burlon\net al. (2022), who provide evidence on the estimated effects of CBDC news on bank\nvaluations and lending in the euro area, finding that it depends on the quantity of\nCBDC in circulation. Also, Wang et al. (2022), find that CBDC attention and un-\ncertainty indices from financial news have a positive relationship on cryptocurrencies,\nforeign exchange, and bond markets, as well as VIX and gold. Closer to our study,\nScharnowski (2022) use the raw CBDC sentiment index, as provided by Auer et al.\n(2020), to study the market reaction to central banks’ speeches on CBDC, observ-\ning that cryptocurrency prices increase more strongly after speeches taking a positive\ntone. Similarly, Tian et al. (2023) relies al well on the texts compiled by Auer et al.\n(2020) but they propose to measure central banks’ sentiment toward CBDC based on\nthe polarity of the words using the Loughran-McDonald Financial Dictionary (LMFD)\n(Loughran and McDonald 2011). They find that different cybersecurity risks (cyber-\nware and cyberattacks) can have an impact on the posture of central banks towards\n2See for instance, Conlon et al. (2022) about “to CBDC or not CBDC”.\n3At the moment of drafting this paper, it is updated up to January 2023.\n4\nBANCO DE ESPAÑA\n10\nDOCUMENTO DE TRABAJO N.º 2321\nCBDCs. Specifically, they find that this posture improves as a result of losses due\nto cyber-attacks. They conclude that central banks believe CBDC could be a public\nsolution to protect consumers from risks arising from the private sector.\nOur study complements both literature strands, first on CBDC, and second, on cen-\ntral banks’ communication, by means of using new techniques of (NLP to assess the\nsentiment of central banks’ speeches in a topic that is gaining traction and relevance\nsuch as digital money Hansson (2021). As a result, the application of LLM techniques,\nsuch as ChatGPT (OpenAI, 2023), allows us to obtain a reliable sentiment measure,\nas a continuous variable between -1 and 1 (as opposed to the categorical judgment of\nexperts), less noisy than Dictionary-based methods, and that captures subtler infor-\nmation compared to models like BERT.\n2.1. Dataset\nThe two most common ways of central bank communication are: (i) speeches, and\n(ii) thematic reports. As a consequence of the more transparent manner that central\nbanks are communicating, the number of speeches available has increased (Hansson\n2021). In this paper we use a collection of relevant central bank speeches touching upon\nCBDC, collected in Auer et al. (2020). This database includes different issues related to\nCBDCs, such as the different motivations of central banks for their experimentation,\nor the public interest on this topic captured by web searches, or the architectures\nand design attributes of each CBDC project. More importantly to our work, it also\nanalyzes the sentiment of those speeches using human expert knowledge. In particular,\nthe tone of the texts has been labeled by economists from the Bank of International\nSettlements (BIS). 4\nAs mentioned before, this database of documents has been used in different recent\nacademic papers like Scharnowski (2022), or Tian et al. (2023). In we provide a sum-\nmary of the corpus of documents available. It consist of 351 texts, from 44 different\ngeographical areas, published during the period 2016-2022. 5 The central banks with\nmore than 10 appearances are France, Euro Area, Germany, UK, US, Japan, Hong\nKong, and Singapore, followed by 36 countries with lower representation. 6 It can be\nseen as well that the number of speeches on CBDC by central banks has been growing\nover the years, which reflects the increasing attention that CBDC has been gaining\nfrom 2016 onwards.\n3. Methodology\nText data is growing, and currently it represents the vast majority of data available for\nresearchers (Gentzkow et al. 2019; Ash and Hansen 2023). The unstructured nature\n4This document provides an accompanying Excel file with a collection of links to relevant CBDC speeches,\nranked in temporary order, with a timestamp. The title, the speech stance (-1,0,1), and the url are listed as well,\ntherefore we have automated a procedure to extract, pre-process and storage the text in a separate database.\n5Importantly, while the original publications dates back to 2020, the speeches database has been updated\nsince then until January 2023. Documents include a timestamp that allows later on for the construction of the\nsentiment index as a time series.\n6The other countries include “Finland”, “Canada”, “Sweden”, “Malaysia”, “Czech Republic”, “Chile”, “Den-\nmark”, “Barbados”, “Australia”, “Pakistan”, “Thailand”, “Lithuania”, “Italy”, “Greece”, “New Zealand”,\n“Hong Kong SAR”, “Belgium”, “Cura¸ cao and Saint Maarten”, “Norway”, “South Africa”, “Kuwait”, “United\nArab Emirates”, “China”, “Bosnia and Herzegovina”, “The Bahamas”, “Mexico”, “France”, “Ireland”, “Mau-\nritius”, “Switzerland”, “Korea”, “Spain”, “Philippines”, “Russia”, “India”, “Albania”, “Netherlands”, “Mace-\ndonia, the Former Yugoslav Republic of”, “Indonesia”, “Korea, Republic of”, “Israel”, “Serbia”.\n5\nTable 1. Descriptive statistics of corpus of documents: cross-sectional dispersion of countries experimenting\nwith CBDC, and the timeline of speeches collected in Auer et al. (2020).\nCountry Documents\nFrance 51\nEuro Area 41\nGermany 31\nUnited Kingdom 24\nUnited States 24\nSingapore 14\nHong Kong 13\nJapan 13\nItaly 13\nThailand 9\nCanada 9\nOthers 109\nYear Documents\n2016 9\n2017 16\n2018 46\n2019 42\n2020 54\n2021 76\n2022 108\nof this kind of data requires special techniques to deal with it, known as NLP). These\ntechniques are useful for different tasks to analyze text, like realizing summaries, pre-\ndicting words in sentences, finding keywords or topics, or analyzing the sentiment of\na given extract.\nIn this study we are interested on investigating the use of LLMs strictly applied to\nmeasuring the tone, stance or sentiment of a text. In this sense, we can frame LLMs as\nan innovation within the field of NLP. In the past 5 years the explosion of LLMs has\nbeen largely driven by the use of Transformers, as a particular architecture of deep\nneural network which relies on the concept of self-attention (Vaswani et al. 2017) to\nembed the context during its learning process. 7\nIn this paper we use three techniques to measure sentiment. One of the techniques\nis dictionary-based (also known as bag-of-words approach), which can be considered\nas a benchmark as it is a traditional NLP technique. The other are two of the most\nwidely used LLMs currently in the academic literature and industries. One the one\nhand, ChatGPT, a version of the general GPT (Generative Pre-trained Transformer)\nmodels, is an auto-regressive Transformer model, recently released by OpenAI. On the\nother hand, BERT (Bi-directional Encoder Representations from Transformers), which\nwas released by Google experts (Devlin et al. 2019). We selected these two families\nof Transformers because both have strengths for sentiment analysis. BERT has the\nadvantage of including bi-directional attention (in contrast to left, auto-regressive,\nattention of GPT models), while ChatGPT has been trained with a gigantic amount\n45 terabytes, compared to the 3 terabytes of BERT. Our work will help to understand\nthe strengths and limitations of both models and to evaluate the improvement that\nthey represent with respect to dictionary-based methods when it comes to measuring\nsentiment.\n3.1. Dictionary-based method\nOne of the most traditional and popular techniques to compute the sentiment of a text\nare the known as dictionary-based methods, which rely on a dictionary or predefined\ncollection of words with positive and negative tone. There are different dictionaries or\n7See Amatriain (2023) for further references on the evolution of Transformer models.\n6\nanonymity, rate of return, etc.). 2 This translates in wide estimates for its adoption.\nFor instance, Li (2023) finds that Canadian households’ demand for CBDC can range\nfrom 4% to 52% of total liquid assets, and banks endogenous responses to CBDC could\nconsiderably constraint its upper bound take-up to 20%. In parallel, Kiff et al. (2020),\nis a valuable survey that reviews the processes, roles, and responsibilities that would\nneed to be defined for the issuance of a retail CBDC, while Gorj´ on Rivas (2022), and\nRomero Romero Ugarte et al. (2021) offers some insights on wholesale CBDC based\non different technologies, like distributed ledgers (DLT).\nA critical article for understanding the increased interest in CBDCs among the\ncentral banking community is Auer et al. (2020). In that research, the authors take\nstock of design efforts, technical approaches, and differing stances on CBDC issuance\nacross jurisdictions that have conducted CBDC experimentation. To do this, from a\ndatabase of more than 16,000 central bank speeches by central bank representatives,\nthey extract 351 texts that explicitly mention CBDC. In addition, they evaluate with\ntheir expert judgment the political position of these speeches. 3 Therefore, as our goal\nis to measure the sentiment of central banks towards CBDC, we will compare the\nperformance of Large Language Models (LLMs) with traditional NLP techniques (like\ndictionary-based methods), and on a novel way, with human expert labels. This will\nallow to assess the benefits and limitations of these models.\nFirstly, we find that studying the sentiment of central bank communication is a\nrelevant research topic, as highlighted in studies such as (Born et al. 2014), who show\nhow sentiment of central banks’ speeches about financial stability have a significant\neffect on market returns and volatility. Also Hansen et al. (2018) study central bank\ntransparency using topic modeling in an event study around 1993, the year the Fed\nstarted to release the FOMC meeting transcripts. Notably, the evolution of NLP is\naccelerating the research in this area. A good example is Hansen and Kazinnik (2023),\nwho use GPT models to decipher the communication from the Federal Reserve, finding\nthat these novel models surpass the performance of more traditional classification\nmethods.\nOn the other hand, the magnitude of the potential implications for an open-wide\neconomy of the introduction of a CBDC is reflected in Ferrari Minesso et al. (2022)\nwho analyze the international transmission of monetary policy decisions and technol-\nogy shocks in the presence and absence of a CBDC. Following this line, the impact of\nCBDC communication in the real economy and financial markets is studied in Burlon\net al. (2022), who provide evidence on the estimated effects of CBDC news on bank\nvaluations and lending in the euro area, finding that it depends on the quantity of\nCBDC in circulation. Also, Wang et al. (2022), find that CBDC attention and un-\ncertainty indices from financial news have a positive relationship on cryptocurrencies,\nforeign exchange, and bond markets, as well as VIX and gold. Closer to our study,\nScharnowski (2022) use the raw CBDC sentiment index, as provided by Auer et al.\n(2020), to study the market reaction to central banks’ speeches on CBDC, observ-\ning that cryptocurrency prices increase more strongly after speeches taking a positive\ntone. Similarly, Tian et al. (2023) relies al well on the texts compiled by Auer et al.\n(2020) but they propose to measure central banks’ sentiment toward CBDC based on\nthe polarity of the words using the Loughran-McDonald Financial Dictionary (LMFD)\n(Loughran and McDonald 2011). They find that different cybersecurity risks (cyber-\nware and cyberattacks) can have an impact on the posture of central banks towards\n2See for instance, Conlon et al. (2022) about “to CBDC or not CBDC”.\n3At the moment of drafting this paper, it is updated up to January 2023.\n4\nCBDCs. Specifically, they find that this posture improves as a result of losses due\nto cyber-attacks. They conclude that central banks believe CBDC could be a public\nsolution to protect consumers from risks arising from the private sector.\nOur study complements both literature strands, first on CBDC, and second, on cen-\ntral banks’ communication, by means of using new techniques of (NLP to assess the\nsentiment of central banks’ speeches in a topic that is gaining traction and relevance\nsuch as digital money Hansson (2021). As a result, the application of LLM techniques,\nsuch as ChatGPT (OpenAI, 2023), allows us to obtain a reliable sentiment measure,\nas a continuous variable between -1 and 1 (as opposed to the categorical judgment of\nexperts), less noisy than Dictionary-based methods, and that captures subtler infor-\nmation compared to models like BERT.\n2.1. Dataset\nThe two most common ways of central bank communication are: (i) speeches, and\n(ii) thematic reports. As a consequence of the more transparent manner that central\nbanks are communicating, the number of speeches available has increased (Hansson\n2021). In this paper we use a collection of relevant central bank speeches touching upon\nCBDC, collected in Auer et al. (2020). This database includes different issues related to\nCBDCs, such as the different motivations of central banks for their experimentation,\nor the public interest on this topic captured by web searches, or the architectures\nand design attributes of each CBDC project. More importantly to our work, it also\nanalyzes the sentiment of those speeches using human expert knowledge. In particular,\nthe tone of the texts has been labeled by economists from the Bank of International\nSettlements (BIS). 4\nAs mentioned before, this database of documents has been used in different recent\nacademic papers like Scharnowski (2022), or Tian et al. (2023). In we provide a sum-\nmary of the corpus of documents available. It consist of 351 texts, from 44 different\ngeographical areas, published during the period 2016-2022. 5 The central banks with\nmore than 10 appearances are France, Euro Area, Germany, UK, US, Japan, Hong\nKong, and Singapore, followed by 36 countries with lower representation. 6 It can be\nseen as well that the number of speeches on CBDC by central banks has been growing\nover the years, which reflects the increasing attention that CBDC has been gaining\nfrom 2016 onwards.\n3. Methodology\nText data is growing, and currently it represents the vast majority of data available for\nresearchers (Gentzkow et al. 2019; Ash and Hansen 2023). The unstructured nature\n4This document provides an accompanying Excel file with a collection of links to relevant CBDC speeches,\nranked in temporary order, with a timestamp. The title, the speech stance (-1,0,1), and the url are listed as well,\ntherefore we have automated a procedure to extract, pre-process and storage the text in a separate database.\n5Importantly, while the original publications dates back to 2020, the speeches database has been updated\nsince then until January 2023. Documents include a timestamp that allows later on for the construction of the\nsentiment index as a time series.\n6The other countries include “Finland”, “Canada”, “Sweden”, “Malaysia”, “Czech Republic”, “Chile”, “Den-\nmark”, “Barbados”, “Australia”, “Pakistan”, “Thailand”, “Lithuania”, “Italy”, “Greece”, “New Zealand”,\n“Hong Kong SAR”, “Belgium”, “Cura¸ cao and Saint Maarten”, “Norway”, “South Africa”, “Kuwait”, “United\nArab Emirates”, “China”, “Bosnia and Herzegovina”, “The Bahamas”, “Mexico”, “France”, “Ireland”, “Mau-\nritius”, “Switzerland”, “Korea”, “Spain”, “Philippines”, “Russia”, “India”, “Albania”, “Netherlands”, “Mace-\ndonia, the Former Yugoslav Republic of”, “Indonesia”, “Korea, Republic of”, “Israel”, “Serbia”.\n5\nBANCO DE ESPAÑA\n11\nDOCUMENTO DE TRABAJO N.º 2321\nTable 1. Descriptive statistics of corpus of documents: cross-sectional dispersion of countries experimenting\nwith CBDC, and the timeline of speeches collected in Auer et al. (2020).\nCountry Documents\nFrance 51\nEuro Area 41\nGermany 31\nUnited Kingdom 24\nUnited States 24\nSingapore 14\nHong Kong 13\nJapan 13\nItaly 13\nThailand 9\nCanada 9\nOthers 109\nYear Documents\n2016 9\n2017 16\n2018 46\n2019 42\n2020 54\n2021 76\n2022 108\nof this kind of data requires special techniques to deal with it, known as NLP). These\ntechniques are useful for different tasks to analyze text, like realizing summaries, pre-\ndicting words in sentences, finding keywords or topics, or analyzing the sentiment of\na given extract.\nIn this study we are interested on investigating the use of LLMs strictly applied to\nmeasuring the tone, stance or sentiment of a text. In this sense, we can frame LLMs as\nan innovation within the field of NLP. In the past 5 years the explosion of LLMs has\nbeen largely driven by the use of Transformers, as a particular architecture of deep\nneural network which relies on the concept of self-attention (Vaswani et al. 2017) to\nembed the context during its learning process. 7\nIn this paper we use three techniques to measure sentiment. One of the techniques\nis dictionary-based (also known as bag-of-words approach), which can be considered\nas a benchmark as it is a traditional NLP technique. The other are two of the most\nwidely used LLMs currently in the academic literature and industries. One the one\nhand, ChatGPT, a version of the general GPT (Generative Pre-trained Transformer)\nmodels, is an auto-regressive Transformer model, recently released by OpenAI. On the\nother hand, BERT (Bi-directional Encoder Representations from Transformers), which\nwas released by Google experts (Devlin et al. 2019). We selected these two families\nof Transformers because both have strengths for sentiment analysis. BERT has the\nadvantage of including bi-directional attention (in contrast to left, auto-regressive,\nattention of GPT models), while ChatGPT has been trained with a gigantic amount\n45 terabytes, compared to the 3 terabytes of BERT. Our work will help to understand\nthe strengths and limitations of both models and to evaluate the improvement that\nthey represent with respect to dictionary-based methods when it comes to measuring\nsentiment.\n3.1. Dictionary-based method\nOne of the most traditional and popular techniques to compute the sentiment of a text\nare the known as dictionary-based methods, which rely on a dictionary or predefined\ncollection of words with positive and negative tone. There are different dictionaries or\n7See Amatriain (2023) for further references on the evolution of Transformer models.\n6\nTable 1. Descriptive statistics of corpus of documents: cross-sectional dispersion of countries experimenting\nwith CBDC, and the timeline of speeches collected in Auer et al. (2020).\nCountry Documents\nFrance 51\nEuro Area 41\nGermany 31\nUnited Kingdom 24\nUnited States 24\nSingapore 14\nHong Kong 13\nJapan 13\nItaly 13\nThailand 9\nCanada 9\nOthers 109\nYear Documents\n2016 9\n2017 16\n2018 46\n2019 42\n2020 54\n2021 76\n2022 108\nof this kind of data requires special techniques to deal with it, known as NLP). These\ntechniques are useful for different tasks to analyze text, like realizing summaries, pre-\ndicting words in sentences, finding keywords or topics, or analyzing the sentiment of\na given extract.\nIn this study we are interested on investigating the use of LLMs strictly applied to\nmeasuring the tone, stance or sentiment of a text. In this sense, we can frame LLMs as\nan innovation within the field of NLP. In the past 5 years the explosion of LLMs has\nbeen largely driven by the use of Transformers, as a particular architecture of deep\nneural network which relies on the concept of self-attention (Vaswani et al. 2017) to\nembed the context during its learning process. 7\nIn this paper we use three techniques to measure sentiment. One of the techniques\nis dictionary-based (also known as bag-of-words approach), which can be considered\nas a benchmark as it is a traditional NLP technique. The other are two of the most\nwidely used LLMs currently in the academic literature and industries. One the one\nhand, ChatGPT, a version of the general GPT (Generative Pre-trained Transformer)\nmodels, is an auto-regressive Transformer model, recently released by OpenAI. On the\nother hand, BERT (Bi-directional Encoder Representations from Transformers), which\nwas released by Google experts (Devlin et al. 2019). We selected these two families\nof Transformers because both have strengths for sentiment analysis. BERT has the\nadvantage of including bi-directional attention (in contrast to left, auto-regressive,\nattention of GPT models), while ChatGPT has been trained with a gigantic amount\n45 terabytes, compared to the 3 terabytes of BERT. Our work will help to understand\nthe strengths and limitations of both models and to evaluate the improvement that\nthey represent with respect to dictionary-based methods when it comes to measuring\nsentiment.\n3.1. Dictionary-based method\nOne of the most traditional and popular techniques to compute the sentiment of a text\nare the known as dictionary-based methods, which rely on a dictionary or predefined\ncollection of words with positive and negative tone. There are different dictionaries or\n7See Amatriain (2023) for further references on the evolution of Transformer models.\n6\nTable 1. Descriptive statistics of corpus of documents: cross-sectional dispersion of countries experimenting\nwith CBDC, and the timeline of speeches collected in Auer et al. (2020).\nCountry Documents\nFrance 51\nEuro Area 41\nGermany 31\nUnited Kingdom 24\nUnited States 24\nSingapore 14\nHong Kong 13\nJapan 13\nItaly 13\nThailand 9\nCanada 9\nOthers 109\nYear Documents\n2016 9\n2017 16\n2018 46\n2019 42\n2020 54\n2021 76\n2022 108\nof this kind of data requires special techniques to deal with it, known as NLP). These\ntechniques are useful for different tasks to analyze text, like realizing summaries, pre-\ndicting words in sentences, finding keywords or topics, or analyzing the sentiment of\na given extract.\nIn this study we are interested on investigating the use of LLMs strictly applied to\nmeasuring the tone, stance or sentiment of a text. In this sense, we can frame LLMs as\nan innovation within the field of NLP. In the past 5 years the explosion of LLMs has\nbeen largely driven by the use of Transformers, as a particular architecture of deep\nneural network which relies on the concept of self-attention (Vaswani et al. 2017) to\nembed the context during its learning process. 7\nIn this paper we use three techniques to measure sentiment. One of the techniques\nis dictionary-based (also known as bag-of-words approach), which can be considered\nas a benchmark as it is a traditional NLP technique. The other are two of the most\nwidely used LLMs currently in the academic literature and industries. One the one\nhand, ChatGPT, a version of the general GPT (Generative Pre-trained Transformer)\nmodels, is an auto-regressive Transformer model, recently released by OpenAI. On the\nother hand, BERT (Bi-directional Encoder Representations from Transformers), which\nwas released by Google experts (Devlin et al. 2019). We selected these two families\nof Transformers because both have strengths for sentiment analysis. BERT has the\nadvantage of including bi-directional attention (in contrast to left, auto-regressive,\nattention of GPT models), while ChatGPT has been trained with a gigantic amount\n45 terabytes, compared to the 3 terabytes of BERT. Our work will help to understand\nthe strengths and limitations of both models and to evaluate the improvement that\nthey represent with respect to dictionary-based methods when it comes to measuring\nsentiment.\n3.1. Dictionary-based method\nOne of the most traditional and popular techniques to compute the sentiment of a text\nare the known as dictionary-based methods, which rely on a dictionary or predefined\ncollection of words with positive and negative tone. There are different dictionaries or\n7See Amatriain (2023) for further references on the evolution of Transformer models.\n6\ncollections, depending on the nature of the texts at hand. In finance, one of the most\nused dictionaries is (Loughran and McDonald 2011). It contains 354 positive words\nand 2,337 negative words. While it is easy to use it straightforward, this approach\nmakes strong assumptions about the meaning of specific words that were selected, so\nmisspecification can cause severe noise in dictionary-based sentiment indices. 8 This\nconcern is presented by Hayo and Zahner (2023), who perform an analysis of variance\n(ANOVA) to show that about 80% of the variation in sentiment in Fed and ECB\nspeeches is due to noise, which raises questions about the index’s reliability as an\nindicator.\nIn any case, due to its popularity, we find it a suitable benchmark for our exercise.\nSince we can count the number of positive and the negative words in the corpus of text\nthat match Loughran and McDonald (2011), an easy way to calculate the sentiment\nof a text is computing the polarity. This index is a measure between -1 and 1 that is\ncomputed as follows:\nP olarity= P ositives−Negatives\nP ositives+Negatives\n3.2. BERT\nAs mentioned before, BERT is a deep learning architecture for NLP analysis developed\nby Google which harness the notion of self-attention to weigh-in the context in the\ncomputation of word embedding. It was developed in 2018, with the goal of provide\ncontextual understanding on unlabeled data (Devlin et al. 2019). The original task\nwas to learn to predict words that might appear before and after other text, hence the\nterm bidirectional, with the aim of translating sentences from German into English.\nUnlike previous LLMs relying on Feed-Forward Neural Networks and Convolutional\nNeural Networks, Transformers do not process the input sequence sequentially over\ntime. Instead, they process the entire sentence at once weighting the relevance of words\nbased on the self-attention mechanism (Vaswani et al. 2017).\nIn particular, for this paper, we use the version known as FinBERT (Yang et al.\n2020), which is a financial domain-specific language model based on BERT, pre-trained\nusing a large scale of financial communication corpora. 9 The sentiment output for\neach analyzed piece of text is a probability distribution over the two possible classes,\nnegative and positive. Is therefore possible to transform it to the range -1 and 1 with\na transformation such as:\nSentiment = P rob(positive) − P rob(negative)\n3.3. ChatGPT\nSimilar to BERT, GPT models were initially trained to predict the next word in a\nsentence given the previous words, which allows it to learn the patterns of natural\nlanguage. The main difference between them is the type of attention mechanism, as\nGPT models are auto-regressive, while the BERT family is bi-directional. Though, on\ntop of this difference, it shall be noted that ChatGPT, a particular version of GPT, has\n8This might be mitigated by means of using TF-idf (Term frequency – Inverse document frequency) when\naccounting for the words used in the dictionary.\n9We use an open-source library in Python, in particular BertForSequenceClassification. This library has a\nmaximum size of the input text to analyzed, limited to 512 tokens. In Section 4 we will further comment on\nthis limitation.\n7\nTable 1. Descriptive statistics of corpus of documents: cross-sectional dispersion of countries experimenting\nwith CBDC, and the timeline of speeches collected in Auer et al. (2020).\nCountry Documents\nFrance 51\nEuro Area 41\nGermany 31\nUnited Kingdom 24\nUnited States 24\nSingapore 14\nHong Kong 13\nJapan 13\nItaly 13\nThailand 9\nCanada 9\nOthers 109\nYear Documents\n2016 9\n2017 16\n2018 46\n2019 42\n2020 54\n2021 76\n2022 108\nof this kind of data requires special techniques to deal with it, known as NLP). These\ntechniques are useful for different tasks to analyze text, like realizing summaries, pre-\ndicting words in sentences, finding keywords or topics, or analyzing the sentiment of\na given extract.\nIn this study we are interested on investigating the use of LLMs strictly applied to\nmeasuring the tone, stance or sentiment of a text. In this sense, we can frame LLMs as\nan innovation within the field of NLP. In the past 5 years the explosion of LLMs has\nbeen largely driven by the use of Transformers, as a particular architecture of deep\nneural network which relies on the concept of self-attention (Vaswani et al. 2017) to\nembed the context during its learning process. 7\nIn this paper we use three techniques to measure sentiment. One of the techniques\nis dictionary-based (also known as bag-of-words approach), which can be considered\nas a benchmark as it is a traditional NLP technique. The other are two of the most\nwidely used LLMs currently in the academic literature and industries. One the one\nhand, ChatGPT, a version of the general GPT (Generative Pre-trained Transformer)\nmodels, is an auto-regressive Transformer model, recently released by OpenAI. On the\nother hand, BERT (Bi-directional Encoder Representations from Transformers), which\nwas released by Google experts (Devlin et al. 2019). We selected these two families\nof Transformers because both have strengths for sentiment analysis. BERT has the\nadvantage of including bi-directional attention (in contrast to left, auto-regressive,\nattention of GPT models), while ChatGPT has been trained with a gigantic amount\n45 terabytes, compared to the 3 terabytes of BERT. Our work will help to understand\nthe strengths and limitations of both models and to evaluate the improvement that\nthey represent with respect to dictionary-based methods when it comes to measuring\nsentiment.\n3.1. Dictionary-based method\nOne of the most traditional and popular techniques to compute the sentiment of a text\nare the known as dictionary-based methods, which rely on a dictionary or predefined\ncollection of words with positive and negative tone. There are different dictionaries or\n7See Amatriain (2023) for further references on the evolution of Transformer models.\n6\nBANCO DE ESPAÑA\n12\nDOCUMENTO DE TRABAJO N.º 2321\nbeen trained on a much larger corpus than any other model up to 2023. It also has been\ntrained using human labeling, which positions this model as potential market leader\n(Bubeck et al. 2023). Indeed, it has gained immense popularity among early adopters\nand in particular between finance experts and researchers (Dowling and Lucey 2023;\nHansen and Kazinnik 2023). 10\nTo interact with ChatGPT it is necessary to specify the task to be carried out\nby means of a prompt. The prompt can be as flexible as necessary, from asking to\nsummarize a text, to make up a story, to translate a sentence or to calculate the\nsentiment. The flexibility in determining the task and ChatGPT’s ability to understand\nit allows us to ask ChatGPT the sentiment in a text about specific topics (like CBDC)\nand from various angles if necessary. Also, we can interact with ChatGPT through the\nOpenAI API, which is very convenient because we can automate the process of asking\nChatGPT to calculate sentiment on a certain topic across thousands of texts.\nThe selection of the prompt (usually known as prompt engineering) is very im-\nportant, as different prompts could yield different results, depending as well on the\nfine-tuning of the hyper-parameters of the model. 11 In the prompt we will ask Chat-\nGPT to compute the sentiment between -1 and 1, therefore we will not need any\nfurther transformation.\nOur benchmark prompt for ChatGPT is as follows:\n“Compute the sentiment score towards central bank digital currencies, measured be-\ntween -1 and 1, of a given text. The response should be just a float number, no text. The\ntext is as follows: [. . . ]”\nThen, we ask an additional prompt which is:\n“Justify your answer”\nWe have performed robustness analysis with different prompts. At the time of writ-\ning, the only model version running behind ChatGPT was GPT-3.5 that has a limi-\ntation of 4,000 tokens per prompt. 12\nHere is an example of how ChatGPT works with the aforementioned prompt with\ntwo small texts from the Monetary Authority of Singapore.\nParagraph 1:Third, a digital Singapore dollar could mitigate against the encroach-\nment of privately issued stablecoins or foreign CBDCs in Singapore’s payments land-\nscape. As these global digital currencies enter our market and become widely accessible\nin the future, they could potentially displace the use of the Singapore dollar in domestic\nretail transactions. A digital Singapore dollar issued by MAS that is congruent with\nthe needs of a digitalised economy could go some way to mitigate this risk.\n• ChatGPT: This is the score: 0.6\n• ChatGPT (justify your answer): The text is discussing the possible benefits of\na digital Singapore dollar issued by the MAS, and how it could help to prevent\nthe displacement of the Singapore dollar in domestic retail transactions. The\nsentiment is positive towards the idea of a digital Singapore dollar\nParagraph 2:Retail CBDCs can potentially pose significant risks to monetary and\n10As evidenced by Haque et al. (2022) the majority of the early adopters have expressed overwhelmingly positive\nsentiments on this model, stating that “this is the most disruptive technology” they have ever experienced with.\n11We have used the davinci-03 engine.\n12OpenAI Technical Report states that GPT-4 will have a limitation of 32,000 tokens per prompt, but it was\nnot available via API.\n8\nbeen trained on a much larger corpus than any other model up to 2023. It also has been\ntrained using human labeling, which positions this model as potential market leader\n(Bubeck et al. 2023). Indeed, it has gained immense popularity among early adopters\nand in particular between finance experts and researchers (Dowling and Lucey 2023;\nHansen and Kazinnik 2023). 10\nTo interact with ChatGPT it is necessary to specify the task to be carried out\nby means of a prompt. The prompt can be as flexible as necessary, from asking to\nsummarize a text, to make up a story, to translate a sentence or to calculate the\nsentiment. The flexibility in determining the task and ChatGPT’s ability to understand\nit allows us to ask ChatGPT the sentiment in a text about specific topics (like CBDC)\nand from various angles if necessary. Also, we can interact with ChatGPT through the\nOpenAI API, which is very convenient because we can automate the process of asking\nChatGPT to calculate sentiment on a certain topic across thousands of texts.\nThe selection of the prompt (usually known as prompt engineering) is very im-\nportant, as different prompts could yield different results, depending as well on the\nfine-tuning of the hyper-parameters of the model. 11 In the prompt we will ask Chat-\nGPT to compute the sentiment between -1 and 1, therefore we will not need any\nfurther transformation.\nOur benchmark prompt for ChatGPT is as follows:\n“Compute the sentiment score towards central bank digital currencies, measured be-\ntween -1 and 1, of a given text. The response should be just a float number, no text. The\ntext is as follows: [. . . ]”\nThen, we ask an additional prompt which is:\n“Justify your answer”\nWe have performed robustness analysis with different prompts. At the time of writ-\ning, the only model version running behind ChatGPT was GPT-3.5 that has a limi-\ntation of 4,000 tokens per prompt. 12\nHere is an example of how ChatGPT works with the aforementioned prompt with\ntwo small texts from the Monetary Authority of Singapore.\nParagraph 1:Third, a digital Singapore dollar could mitigate against the encroach-\nment of privately issued stablecoins or foreign CBDCs in Singapore’s payments land-\nscape. As these global digital currencies enter our market and become widely accessible\nin the future, they could potentially displace the use of the Singapore dollar in domestic\nretail transactions. A digital Singapore dollar issued by MAS that is congruent with\nthe needs of a digitalised economy could go some way to mitigate this risk.\n• ChatGPT: This is the score: 0.6\n• ChatGPT (justify your answer): The text is discussing the possible benefits of\na digital Singapore dollar issued by the MAS, and how it could help to prevent\nthe displacement of the Singapore dollar in domestic retail transactions. The\nsentiment is positive towards the idea of a digital Singapore dollar\nParagraph 2:Retail CBDCs can potentially pose significant risks to monetary and\n10As evidenced by Haque et al. (2022) the majority of the early adopters have expressed overwhelmingly positive\nsentiments on this model, stating that “this is the most disruptive technology” they have ever experienced with.\n11We have used the davinci-03 engine.\n12OpenAI Technical Report states that GPT-4 will have a limitation of 32,000 tokens per prompt, but it was\nnot available via API.\n8\ncollections, depending on the nature of the texts at hand. In finance, one of the most\nused dictionaries is (Loughran and McDonald 2011). It contains 354 positive words\nand 2,337 negative words. While it is easy to use it straightforward, this approach\nmakes strong assumptions about the meaning of specific words that were selected, so\nmisspecification can cause severe noise in dictionary-based sentiment indices. 8 This\nconcern is presented by Hayo and Zahner (2023), who perform an analysis of variance\n(ANOVA) to show that about 80% of the variation in sentiment in Fed and ECB\nspeeches is due to noise, which raises questions about the index’s reliability as an\nindicator.\nIn any case, due to its popularity, we find it a suitable benchmark for our exercise.\nSince we can count the number of positive and the negative words in the corpus of text\nthat match Loughran and McDonald (2011), an easy way to calculate the sentiment\nof a text is computing the polarity. This index is a measure between -1 and 1 that is\ncomputed as follows:\nP olarity= P ositives−Negatives\nP ositives+Negatives\n3.2. BERT\nAs mentioned before, BERT is a deep learning architecture for NLP analysis developed\nby Google which harness the notion of self-attention to weigh-in the context in the\ncomputation of word embedding. It was developed in 2018, with the goal of provide\ncontextual understanding on unlabeled data (Devlin et al. 2019). The original task\nwas to learn to predict words that might appear before and after other text, hence the\nterm bidirectional, with the aim of translating sentences from German into English.\nUnlike previous LLMs relying on Feed-Forward Neural Networks and Convolutional\nNeural Networks, Transformers do not process the input sequence sequentially over\ntime. Instead, they process the entire sentence at once weighting the relevance of words\nbased on the self-attention mechanism (Vaswani et al. 2017).\nIn particular, for this paper, we use the version known as FinBERT (Yang et al.\n2020), which is a financial domain-specific language model based on BERT, pre-trained\nusing a large scale of financial communication corpora. 9 The sentiment output for\neach analyzed piece of text is a probability distribution over the two possible classes,\nnegative and positive. Is therefore possible to transform it to the range -1 and 1 with\na transformation such as:\nSentiment = P rob(positive) − P rob(negative)\n3.3. ChatGPT\nSimilar to BERT, GPT models were initially trained to predict the next word in a\nsentence given the previous words, which allows it to learn the patterns of natural\nlanguage. The main difference between them is the type of attention mechanism, as\nGPT models are auto-regressive, while the BERT family is bi-directional. Though, on\ntop of this difference, it shall be noted that ChatGPT, a particular version of GPT, has\n8This might be mitigated by means of using TF-idf (Term frequency – Inverse document frequency) when\naccounting for the words used in the dictionary.\n9We use an open-source library in Python, in particular BertForSequenceClassification. This library has a\nmaximum size of the input text to analyzed, limited to 512 tokens. In Section 4 we will further comment on\nthis limitation.\n7\ncollections, depending on the nature of the texts at hand. In finance, one of the most\nused dictionaries is (Loughran and McDonald 2011). It contains 354 positive words\nand 2,337 negative words. While it is easy to use it straightforward, this approach\nmakes strong assumptions about the meaning of specific words that were selected, so\nmisspecification can cause severe noise in dictionary-based sentiment indices. 8 This\nconcern is presented by Hayo and Zahner (2023), who perform an analysis of variance\n(ANOVA) to show that about 80% of the variation in sentiment in Fed and ECB\nspeeches is due to noise, which raises questions about the index’s reliability as an\nindicator.\nIn any case, due to its popularity, we find it a suitable benchmark for our exercise.\nSince we can count the number of positive and the negative words in the corpus of text\nthat match Loughran and McDonald (2011), an easy way to calculate the sentiment\nof a text is computing the polarity. This index is a measure between -1 and 1 that is\ncomputed as follows:\nP olarity= P ositives−Negatives\nP ositives+Negatives\n3.2. BERT\nAs mentioned before, BERT is a deep learning architecture for NLP analysis developed\nby Google which harness the notion of self-attention to weigh-in the context in the\ncomputation of word embedding. It was developed in 2018, with the goal of provide\ncontextual understanding on unlabeled data (Devlin et al. 2019). The original task\nwas to learn to predict words that might appear before and after other text, hence the\nterm bidirectional, with the aim of translating sentences from German into English.\nUnlike previous LLMs relying on Feed-Forward Neural Networks and Convolutional\nNeural Networks, Transformers do not process the input sequence sequentially over\ntime. Instead, they process the entire sentence at once weighting the relevance of words\nbased on the self-attention mechanism (Vaswani et al. 2017).\nIn particular, for this paper, we use the version known as FinBERT (Yang et al.\n2020), which is a financial domain-specific language model based on BERT, pre-trained\nusing a large scale of financial communication corpora. 9 The sentiment output for\neach analyzed piece of text is a probability distribution over the two possible classes,\nnegative and positive. Is therefore possible to transform it to the range -1 and 1 with\na transformation such as:\nSentiment = P rob(positive) − P rob(negative)\n3.3. ChatGPT\nSimilar to BERT, GPT models were initially trained to predict the next word in a\nsentence given the previous words, which allows it to learn the patterns of natural\nlanguage. The main difference between them is the type of attention mechanism, as\nGPT models are auto-regressive, while the BERT family is bi-directional. Though, on\ntop of this difference, it shall be noted that ChatGPT, a particular version of GPT, has\n8This might be mitigated by means of using TF-idf (Term frequency – Inverse document frequency) when\naccounting for the words used in the dictionary.\n9We use an open-source library in Python, in particular BertForSequenceClassification. This library has a\nmaximum size of the input text to analyzed, limited to 512 tokens. In Section 4 we will further comment on\nthis limitation.\n7\nBANCO DE ESPAÑA\n13\nDOCUMENTO DE TRABAJO N.º 2321\nbeen trained on a much larger corpus than any other model up to 2023. It also has been\ntrained using human labeling, which positions this model as potential market leader\n(Bubeck et al. 2023). Indeed, it has gained immense popularity among early adopters\nand in particular between finance experts and researchers (Dowling and Lucey 2023;\nHansen and Kazinnik 2023). 10\nTo interact with ChatGPT it is necessary to specify the task to be carried out\nby means of a prompt. The prompt can be as flexible as necessary, from asking to\nsummarize a text, to make up a story, to translate a sentence or to calculate the\nsentiment. The flexibility in determining the task and ChatGPT’s ability to understand\nit allows us to ask ChatGPT the sentiment in a text about specific topics (like CBDC)\nand from various angles if necessary. Also, we can interact with ChatGPT through the\nOpenAI API, which is very convenient because we can automate the process of asking\nChatGPT to calculate sentiment on a certain topic across thousands of texts.\nThe selection of the prompt (usually known as prompt engineering) is very im-\nportant, as different prompts could yield different results, depending as well on the\nfine-tuning of the hyper-parameters of the model. 11 In the prompt we will ask Chat-\nGPT to compute the sentiment between -1 and 1, therefore we will not need any\nfurther transformation.\nOur benchmark prompt for ChatGPT is as follows:\n“Compute the sentiment score towards central bank digital currencies, measured be-\ntween -1 and 1, of a given text. The response should be just a float number, no text. The\ntext is as follows: [. . . ]”\nThen, we ask an additional prompt which is:\n“Justify your answer”\nWe have performed robustness analysis with different prompts. At the time of writ-\ning, the only model version running behind ChatGPT was GPT-3.5 that has a limi-\ntation of 4,000 tokens per prompt. 12\nHere is an example of how ChatGPT works with the aforementioned prompt with\ntwo small texts from the Monetary Authority of Singapore.\nParagraph 1:Third, a digital Singapore dollar could mitigate against the encroach-\nment of privately issued stablecoins or foreign CBDCs in Singapore’s payments land-\nscape. As these global digital currencies enter our market and become widely accessible\nin the future, they could potentially displace the use of the Singapore dollar in domestic\nretail transactions. A digital Singapore dollar issued by MAS that is congruent with\nthe needs of a digitalised economy could go some way to mitigate this risk.\n• ChatGPT: This is the score: 0.6\n• ChatGPT (justify your answer): The text is discussing the possible benefits of\na digital Singapore dollar issued by the MAS, and how it could help to prevent\nthe displacement of the Singapore dollar in domestic retail transactions. The\nsentiment is positive towards the idea of a digital Singapore dollar\nParagraph 2:Retail CBDCs can potentially pose significant risks to monetary and\n10As evidenced by Haque et al. (2022) the majority of the early adopters have expressed overwhelmingly positive\nsentiments on this model, stating that “this is the most disruptive technology” they have ever experienced with.\n11We have used the davinci-03 engine.\n12OpenAI Technical Report states that GPT-4 will have a limitation of 32,000 tokens per prompt, but it was\nnot available via API.\n8\nbeen trained on a much larger corpus than any other model up to 2023. It also has been\ntrained using human labeling, which positions this model as potential market leader\n(Bubeck et al. 2023). Indeed, it has gained immense popularity among early adopters\nand in particular between finance experts and researchers (Dowling and Lucey 2023;\nHansen and Kazinnik 2023). 10\nTo interact with ChatGPT it is necessary to specify the task to be carried out\nby means of a prompt. The prompt can be as flexible as necessary, from asking to\nsummarize a text, to make up a story, to translate a sentence or to calculate the\nsentiment. The flexibility in determining the task and ChatGPT’s ability to understand\nit allows us to ask ChatGPT the sentiment in a text about specific topics (like CBDC)\nand from various angles if necessary. Also, we can interact with ChatGPT through the\nOpenAI API, which is very convenient because we can automate the process of asking\nChatGPT to calculate sentiment on a certain topic across thousands of texts.\nThe selection of the prompt (usually known as prompt engineering) is very im-\nportant, as different prompts could yield different results, depending as well on the\nfine-tuning of the hyper-parameters of the model. 11 In the prompt we will ask Chat-\nGPT to compute the sentiment between -1 and 1, therefore we will not need any\nfurther transformation.\nOur benchmark prompt for ChatGPT is as follows:\n“Compute the sentiment score towards central bank digital currencies, measured be-\ntween -1 and 1, of a given text. The response should be just a float number, no text. The\ntext is as follows: [. . . ]”\nThen, we ask an additional prompt which is:\n“Justify your answer”\nWe have performed robustness analysis with different prompts. At the time of writ-\ning, the only model version running behind ChatGPT was GPT-3.5 that has a limi-\ntation of 4,000 tokens per prompt. 12\nHere is an example of how ChatGPT works with the aforementioned prompt with\ntwo small texts from the Monetary Authority of Singapore.\nParagraph 1:Third, a digital Singapore dollar could mitigate against the encroach-\nment of privately issued stablecoins or foreign CBDCs in Singapore’s payments land-\nscape. As these global digital currencies enter our market and become widely accessible\nin the future, they could potentially displace the use of the Singapore dollar in domestic\nretail transactions. A digital Singapore dollar issued by MAS that is congruent with\nthe needs of a digitalised economy could go some way to mitigate this risk.\n• ChatGPT: This is the score: 0.6\n• ChatGPT (justify your answer): The text is discussing the possible benefits of\na digital Singapore dollar issued by the MAS, and how it could help to prevent\nthe displacement of the Singapore dollar in domestic retail transactions. The\nsentiment is positive towards the idea of a digital Singapore dollar\nParagraph 2:Retail CBDCs can potentially pose significant risks to monetary and\n10As evidenced by Haque et al. (2022) the majority of the early adopters have expressed overwhelmingly positive\nsentiments on this model, stating that “this is the most disruptive technology” they have ever experienced with.\n11We have used the davinci-03 engine.\n12OpenAI Technical Report states that GPT-4 will have a limitation of 32,000 tokens per prompt, but it was\nnot available via API.\n8\nfinancial stability. There could be some disintermediation of the banks, particularly\nduring stress periods if people can switch deposits into risk-free central bank money at\nthe “click of a button”. Even in normal times, if people held a significant portion of\ntheir deposits in the form of digital Singapore dollars with MAS, it would considerably\nreduce our banks’ capacity to make loans. But we can likely manage these risks by\ndesigning the retail CBDC with sensible safeguards, such as stock and flow caps on the\namount of digital Singapore dollars that anyone is allowed to place with MAS.\n• ChatGPT: This is the score: -0.6\n• ChatGPT (justify your answer): The text discusses the potential risks of retail\nCBDCs, which could lead to disintermediation of banks and reduced capacity\nfor loans. While the text does acknowledge that these risks can be managed, the\noverall sentiment is negative.\n3.4. Workflow\nAs we said before, there are a total of 351 documents in our corpus. These documents\nare very heterogeneous, with different sizes. In we show the description of the size\nof these texts. There are texts with less than 1,000 words and others with more than\n10,000. The average size is around 3,500 words, and almost half of the documents have\nmore than 3,000 words. While we can apply the dictionary-based method to texts of\nany size, both BERT and ChatGPT have limitations, of 500 tokens and 4,000 tokens\nrespectively.13 And in the case of ChatGPT, both the prompt and answer tokens are\nincluded in the limitation. Therefore, keeping in mind the limitations of BERT and\nGPT, we have decided to perform our benchmark analysis splitting the documents\ninto smaller pieces per document. This had an additional advantage because smaller\ntexts are easy for a human to inspect, which allows us to immediately check how the\ndictionary-based and LLMs are performing.\nWe have decided to split each speech into paragraphs. Since the speeches cover many\ndifferent topics other than CBDC (e.g.: COVID, financial stability, or macroeconomic\nforecasting), we select only relevant paragraphs where a number of keywords are men-\ntioned. In particular, in line with Tian et al. (2022) the keywords are CBDC, central\nbank digital currency (currencies), central bank digital money, central-bank-issued dig-\nital money, digital euro, and central-bank-issued digital currency (currencies). 14 Once\na relevant paragraph is selected, we calculate the sentiment with the three methods\n(Dictionary-based, BERT and ChatGPT). None of the paragraphs was bigger than\n500 words (the average size of the selected paragraphs is 80 words), meaning that they\ncould be analyzed by all techniques. In Table 2 we show the description of the size of\nthe resulting text after applying the keywords, and the number of paragraphs, so that\nwe can have an idea of the size of the texts fed into the models. Once all the para-\ngraphs in the document have been analyzed, we calculate the document sentiment as\nthe average of the paragraphs sentiments. We repeat this process for all the documents\nin our corpus. In Figure 1 we summarize the workflow.\n13A token is a single unit of text, like words, numbers, or punctuation marks, separated by white space or\nother delimiters\n14We could perform this selection with ChatGPT as well. ChatGPT would be able to capture if the paragraph\nis talking about CBDC, but this will imply more queries and higher price through the API. A practical strategy\ncould involve the use of semantic search, based on pre-trained embeddings, to select paragraphs most likely to\naddress CBDCs. These selected paragraphs could then be fed into the API of ChatGPT.\n9\nBANCO DE ESPAÑA\n14\nDOCUMENTO DE TRABAJO N.º 2321\nFigure 1. Workflow: procedure to automate the extraction of relevant text from the listed url of speeches\nprovided in an Excel file in Auer et al. (2020). Step by step we show the methodology pursued to measure the\nsentiment of the corpus of text.\nTable 2. Size of corpus: descriptive statistics of the corpus of texts, collected from Auer et al. (2020). First,\nthe number of words in the full document, then the size of the relevant pieces of text (connected to CBDC),\nand finally, the number of paragraphs.\nNumber of words, all text Number of words, selected text Number of paragraphs\nMean 3,513 558 7.11\nStd 3,861 665 9.33\nMin 529 21 1\n0.25 1,894 123 1\n0.5 2,895 278 3\n0.75 3,872 699 9\nMax 43,823 3,753 49\n4. Results\nIn this Section we analyze the sentiment on CBDC obtained with the three techniques:\nDictionary-based, BERT and ChatGPT. First, in Section 5.1 we compare the results\nand the evolution over time, showing that sentiment on CBDC is clearly upward trend-\ning. Then, in Section 5.2 we compare the results obtained using NLP with the human\nexpert analysis performed by BIS economists, showing that ChatGPT is closest to\nthe labeled data. In Section 5.3, we perform different robust analyses such as different\nprompts, and increasing the size of the text. In the Appendix we show some more\nexamples to support why ChatGPT can capture better the sentiment.\n4.1. Evolution of sentiment\nWe have seen that all three techniques show a positive evolution of sentiment towards\nCBDC. But, which technique is more accurate? To assess the accuracy, we aim to\ncalculate if they are comparable to what a human would do. To do this, we perform\nthree exercises. We first calculate the correlation between the sentiment score of each\ntechnique with that reported by human experts, in particular the economists working\nat BIS, as stated in Auer et al. (2020). Here the authors used expert judgment to score\neach document with a value of -1, 0 or +1. Second, in the Appendix we look at some\nof the cases where there is major disagreement between the three techniques.\nWe can look into how sentiment evolves over time in several ways. First, in Figures 2\nand 3, we group the documents by year (on the left) and by quarter (on the right), and\ntake the average of the documents in that period. We must bear in mind that there\nis a different number of documents in each year and quarter (as we saw in Figure\n1 above), so the figures may be biased by moments of time with a higher or lower\nnumber of documents. The annual and quarterly average sentiment shows a growing\ntrend from 2017 onwards, for all techniques. Polarity seems to be more volatile and the\nfact that the sentiment of ChatGPT is greater than BERT, which in turn is greater\nthan Polarity, is present in almost the entire sample. Sentiment for ChatGPT moves\nfrom 0.2 to 0.4, for BERT it moves around 0.1 and 0.2, and for Polarity it is much\n10\nfinancial stability. There could be some disintermediation of the banks, particularly\nduring stress periods if people can switch deposits into risk-free central bank money at\nthe “click of a button”. Even in normal times, if people held a significant portion of\ntheir deposits in the form of digital Singapore dollars with MAS, it would considerably\nreduce our banks’ capacity to make loans. But we can likely manage these risks by\ndesigning the retail CBDC with sensible safeguards, such as stock and flow caps on the\namount of digital Singapore dollars that anyone is allowed to place with MAS.\n• ChatGPT: This is the score: -0.6\n• ChatGPT (justify your answer): The text discusses the potential risks of retail\nCBDCs, which could lead to disintermediation of banks and reduced capacity\nfor loans. While the text does acknowledge that these risks can be managed, the\noverall sentiment is negative.\n3.4. Workflow\nAs we said before, there are a total of 351 documents in our corpus. These documents\nare very heterogeneous, with different sizes. In we show the description of the size\nof these texts. There are texts with less than 1,000 words and others with more than\n10,000. The average size is around 3,500 words, and almost half of the documents have\nmore than 3,000 words. While we can apply the dictionary-based method to texts of\nany size, both BERT and ChatGPT have limitations, of 500 tokens and 4,000 tokens\nrespectively.13 And in the case of ChatGPT, both the prompt and answer tokens are\nincluded in the limitation. Therefore, keeping in mind the limitations of BERT and\nGPT, we have decided to perform our benchmark analysis splitting the documents\ninto smaller pieces per document. This had an additional advantage because smaller\ntexts are easy for a human to inspect, which allows us to immediately check how the\ndictionary-based and LLMs are performing.\nWe have decided to split each speech into paragraphs. Since the speeches cover many\ndifferent topics other than CBDC (e.g.: COVID, financial stability, or macroeconomic\nforecasting), we select only relevant paragraphs where a number of keywords are men-\ntioned. In particular, in line with Tian et al. (2022) the keywords are CBDC, central\nbank digital currency (currencies), central bank digital money, central-bank-issued dig-\nital money, digital euro, and central-bank-issued digital currency (currencies). 14 Once\na relevant paragraph is selected, we calculate the sentiment with the three methods\n(Dictionary-based, BERT and ChatGPT). None of the paragraphs was bigger than\n500 words (the average size of the selected paragraphs is 80 words), meaning that they\ncould be analyzed by all techniques. In Table 2 we show the description of the size of\nthe resulting text after applying the keywords, and the number of paragraphs, so that\nwe can have an idea of the size of the texts fed into the models. Once all the para-\ngraphs in the document have been analyzed, we calculate the document sentiment as\nthe average of the paragraphs sentiments. We repeat this process for all the documents\nin our corpus. In Figure 1 we summarize the workflow.\n13A token is a single unit of text, like words, numbers, or punctuation marks, separated by white space or\nother delimiters\n14We could perform this selection with ChatGPT as well. ChatGPT would be able to capture if the paragraph\nis talking about CBDC, but this will imply more queries and higher price through the API. A practical strategy\ncould involve the use of semantic search, based on pre-trained embeddings, to select paragraphs most likely to\naddress CBDCs. These selected paragraphs could then be fed into the API of ChatGPT.\n9\nfinancial stability. There could be some disintermediation of the banks, particularly\nduring stress periods if people can switch deposits into risk-free central bank money at\nthe “click of a button”. Even in normal times, if people held a significant portion of\ntheir deposits in the form of digital Singapore dollars with MAS, it would considerably\nreduce our banks’ capacity to make loans. But we can likely manage these risks by\ndesigning the retail CBDC with sensible safeguards, such as stock and flow caps on the\namount of digital Singapore dollars that anyone is allowed to place with MAS.\n• ChatGPT: This is the score: -0.6\n• ChatGPT (justify your answer): The text discusses the potential risks of retail\nCBDCs, which could lead to disintermediation of banks and reduced capacity\nfor loans. While the text does acknowledge that these risks can be managed, the\noverall sentiment is negative.\n3.4. Workflow\nAs we said before, there are a total of 351 documents in our corpus. These documents\nare very heterogeneous, with different sizes. In we show the description of the size\nof these texts. There are texts with less than 1,000 words and others with more than\n10,000. The average size is around 3,500 words, and almost half of the documents have\nmore than 3,000 words. While we can apply the dictionary-based method to texts of\nany size, both BERT and ChatGPT have limitations, of 500 tokens and 4,000 tokens\nrespectively.13 And in the case of ChatGPT, both the prompt and answer tokens are\nincluded in the limitation. Therefore, keeping in mind the limitations of BERT and\nGPT, we have decided to perform our benchmark analysis splitting the documents\ninto smaller pieces per document. This had an additional advantage because smaller\ntexts are easy for a human to inspect, which allows us to immediately check how the\ndictionary-based and LLMs are performing.\nWe have decided to split each speech into paragraphs. Since the speeches cover many\ndifferent topics other than CBDC (e.g.: COVID, financial stability, or macroeconomic\nforecasting), we select only relevant paragraphs where a number of keywords are men-\ntioned. In particular, in line with Tian et al. (2022) the keywords are CBDC, central\nbank digital currency (currencies), central bank digital money, central-bank-issued dig-\nital money, digital euro, and central-bank-issued digital currency (currencies). 14 Once\na relevant paragraph is selected, we calculate the sentiment with the three methods\n(Dictionary-based, BERT and ChatGPT). None of the paragraphs was bigger than\n500 words (the average size of the selected paragraphs is 80 words), meaning that they\ncould be analyzed by all techniques. In Table 2 we show the description of the size of\nthe resulting text after applying the keywords, and the number of paragraphs, so that\nwe can have an idea of the size of the texts fed into the models. Once all the para-\ngraphs in the document have been analyzed, we calculate the document sentiment as\nthe average of the paragraphs sentiments. We repeat this process for all the documents\nin our corpus. In Figure 1 we summarize the workflow.\n13A token is a single unit of text, like words, numbers, or punctuation marks, separated by white space or\nother delimiters\n14We could perform this selection with ChatGPT as well. ChatGPT would be able to capture if the paragraph\nis talking about CBDC, but this will imply more queries and higher price through the API. A practical strategy\ncould involve the use of semantic search, based on pre-trained embeddings, to select paragraphs most likely to\naddress CBDCs. These selected paragraphs could then be fed into the API of ChatGPT.\n9\nFigure 1. Workflow: procedure to automate the extraction of relevant text from the listed url of speeches\nprovided in an Excel file in Auer et al. (2020). Step by step we show the methodology pursued to measure the\nsentiment of the corpus of text.\nTable 2. Size of corpus: descriptive statistics of the corpus of texts, collected from Auer et al. (2020). First,\nthe number of words in the full document, then the size of the relevant pieces of text (connected to CBDC),\nand finally, the number of paragraphs.\nNumber of words, all text Number of words, selected text Number of paragraphs\nMean 3,513 558 7.11\nStd 3,861 665 9.33\nMin 529 21 1\n0.25 1,894 123 1\n0.5 2,895 278 3\n0.75 3,872 699 9\nMax 43,823 3,753 49\n4. Results\nIn this Section we analyze the sentiment on CBDC obtained with the three techniques:\nDictionary-based, BERT and ChatGPT. First, in Section 5.1 we compare the results\nand the evolution over time, showing that sentiment on CBDC is clearly upward trend-\ning. Then, in Section 5.2 we compare the results obtained using NLP with the human\nexpert analysis performed by BIS economists, showing that ChatGPT is closest to\nthe labeled data. In Section 5.3, we perform different robust analyses such as different\nprompts, and increasing the size of the text. In the Appendix we show some more\nexamples to support why ChatGPT can capture better the sentiment.\n4.1. Evolution of sentiment\nWe have seen that all three techniques show a positive evolution of sentiment towards\nCBDC. But, which technique is more accurate? To assess the accuracy, we aim to\ncalculate if they are comparable to what a human would do. To do this, we perform\nthree exercises. We first calculate the correlation between the sentiment score of each\ntechnique with that reported by human experts, in particular the economists working\nat BIS, as stated in Auer et al. (2020). Here the authors used expert judgment to score\neach document with a value of -1, 0 or +1. Second, in the Appendix we look at some\nof the cases where there is major disagreement between the three techniques.\nWe can look into how sentiment evolves over time in several ways. First, in Figures 2\nand 3, we group the documents by year (on the left) and by quarter (on the right), and\ntake the average of the documents in that period. We must bear in mind that there\nis a different number of documents in each year and quarter (as we saw in Figure\n1 above), so the figures may be biased by moments of time with a higher or lower\nnumber of documents. The annual and quarterly average sentiment shows a growing\ntrend from 2017 onwards, for all techniques. Polarity seems to be more volatile and the\nfact that the sentiment of ChatGPT is greater than BERT, which in turn is greater\nthan Polarity, is present in almost the entire sample. Sentiment for ChatGPT moves\nfrom 0.2 to 0.4, for BERT it moves around 0.1 and 0.2, and for Polarity it is much\n10\nFigure 1. Workflow: procedure to automate the extraction of relevant text from the listed url of speeches\nprovided in an Excel file in Auer et al. (2020). Step by step we show the methodology pursued to measure the\nsentiment of the corpus of text.\nTable 2. Size of corpus: descriptive statistics of the corpus of texts, collected from Auer et al. (2020). First,\nthe number of words in the full document, then the size of the relevant pieces of text (connected to CBDC),\nand finally, the number of paragraphs.\nNumber of words, all text Number of words, selected text Number of paragraphs\nMean 3,513 558 7.11\nStd 3,861 665 9.33\nMin 529 21 1\n0.25 1,894 123 1\n0.5 2,895 278 3\n0.75 3,872 699 9\nMax 43,823 3,753 49\n4. Results\nIn this Section we analyze the sentiment on CBDC obtained with the three techniques:\nDictionary-based, BERT and ChatGPT. First, in Section 5.1 we compare the results\nand the evolution over time, showing that sentiment on CBDC is clearly upward trend-\ning. Then, in Section 5.2 we compare the results obtained using NLP with the human\nexpert analysis performed by BIS economists, showing that ChatGPT is closest to\nthe labeled data. In Section 5.3, we perform different robust analyses such as different\nprompts, and increasing the size of the text. In the Appendix we show some more\nexamples to support why ChatGPT can capture better the sentiment.\n4.1. Evolution of sentiment\nWe have seen that all three techniques show a positive evolution of sentiment towards\nCBDC. But, which technique is more accurate? To assess the accuracy, we aim to\ncalculate if they are comparable to what a human would do. To do this, we perform\nthree exercises. We first calculate the correlation between the sentiment score of each\ntechnique with that reported by human experts, in particular the economists working\nat BIS, as stated in Auer et al. (2020). Here the authors used expert judgment to score\neach document with a value of -1, 0 or +1. Second, in the Appendix we look at some\nof the cases where there is major disagreement between the three techniques.\nWe can look into how sentiment evolves over time in several ways. First, in Figures 2\nand 3, we group the documents by year (on the left) and by quarter (on the right), and\ntake the average of the documents in that period. We must bear in mind that there\nis a different number of documents in each year and quarter (as we saw in Figure\n1 above), so the figures may be biased by moments of time with a higher or lower\nnumber of documents. The annual and quarterly average sentiment shows a growing\ntrend from 2017 onwards, for all techniques. Polarity seems to be more volatile and the\nfact that the sentiment of ChatGPT is greater than BERT, which in turn is greater\nthan Polarity, is present in almost the entire sample. Sentiment for ChatGPT moves\nfrom 0.2 to 0.4, for BERT it moves around 0.1 and 0.2, and for Polarity it is much\n10\nFigure 1. Workflow: procedure to automate the extraction of relevant text from the listed url of speeches\nprovided in an Excel file in Auer et al. (2020). Step by step we show the methodology pursued to measure the\nsentiment of the corpus of text.\nTable 2. Size of corpus: descriptive statistics of the corpus of texts, collected from Auer et al. (2020). First,\nthe number of words in the full document, then the size of the relevant pieces of text (connected to CBDC),\nand finally, the number of paragraphs.\nNumber of words, all text Number of words, selected text Number of paragraphs\nMean 3,513 558 7.11\nStd 3,861 665 9.33\nMin 529 21 1\n0.25 1,894 123 1\n0.5 2,895 278 3\n0.75 3,872 699 9\nMax 43,823 3,753 49\n4. Results\nIn this Section we analyze the sentiment on CBDC obtained with the three techniques:\nDictionary-based, BERT and ChatGPT. First, in Section 5.1 we compare the results\nand the evolution over time, showing that sentiment on CBDC is clearly upward trend-\ning. Then, in Section 5.2 we compare the results obtained using NLP with the human\nexpert analysis performed by BIS economists, showing that ChatGPT is closest to\nthe labeled data. In Section 5.3, we perform different robust analyses such as different\nprompts, and increasing the size of the text. In the Appendix we show some more\nexamples to support why ChatGPT can capture better the sentiment.\n4.1. Evolution of sentiment\nWe have seen that all three techniques show a positive evolution of sentiment towards\nCBDC. But, which technique is more accurate? To assess the accuracy, we aim to\ncalculate if they are comparable to what a human would do. To do this, we perform\nthree exercises. We first calculate the correlation between the sentiment score of each\ntechnique with that reported by human experts, in particular the economists working\nat BIS, as stated in Auer et al. (2020). Here the authors used expert judgment to score\neach document with a value of -1, 0 or +1. Second, in the Appendix we look at some\nof the cases where there is major disagreement between the three techniques.\nWe can look into how sentiment evolves over time in several ways. First, in Figures 2\nand 3, we group the documents by year (on the left) and by quarter (on the right), and\ntake the average of the documents in that period. We must bear in mind that there\nis a different number of documents in each year and quarter (as we saw in Figure\n1 above), so the figures may be biased by moments of time with a higher or lower\nnumber of documents. The annual and quarterly average sentiment shows a growing\ntrend from 2017 onwards, for all techniques. Polarity seems to be more volatile and the\nfact that the sentiment of ChatGPT is greater than BERT, which in turn is greater\nthan Polarity, is present in almost the entire sample. Sentiment for ChatGPT moves\nfrom 0.2 to 0.4, for BERT it moves around 0.1 and 0.2, and for Polarity it is much\n10\nBANCO DE ESPAÑA\n15\nDOCUMENTO DE TRABAJO N.º 2321\nFigure 1. Workflow: procedure to automate the extraction of relevant text from the listed url of speeches\nprovided in an Excel file in Auer et al. (2020). Step by step we show the methodology pursued to measure the\nsentiment of the corpus of text.\nTable 2. Size of corpus: descriptive statistics of the corpus of texts, collected from Auer et al. (2020). First,\nthe number of words in the full document, then the size of the relevant pieces of text (connected to CBDC),\nand finally, the number of paragraphs.\nNumber of words, all text Number of words, selected text Number of paragraphs\nMean 3,513 558 7.11\nStd 3,861 665 9.33\nMin 529 21 1\n0.25 1,894 123 1\n0.5 2,895 278 3\n0.75 3,872 699 9\nMax 43,823 3,753 49\n4. Results\nIn this Section we analyze the sentiment on CBDC obtained with the three techniques:\nDictionary-based, BERT and ChatGPT. First, in Section 5.1 we compare the results\nand the evolution over time, showing that sentiment on CBDC is clearly upward trend-\ning. Then, in Section 5.2 we compare the results obtained using NLP with the human\nexpert analysis performed by BIS economists, showing that ChatGPT is closest to\nthe labeled data. In Section 5.3, we perform different robust analyses such as different\nprompts, and increasing the size of the text. In the Appendix we show some more\nexamples to support why ChatGPT can capture better the sentiment.\n4.1. Evolution of sentiment\nWe have seen that all three techniques show a positive evolution of sentiment towards\nCBDC. But, which technique is more accurate? To assess the accuracy, we aim to\ncalculate if they are comparable to what a human would do. To do this, we perform\nthree exercises. We first calculate the correlation between the sentiment score of each\ntechnique with that reported by human experts, in particular the economists working\nat BIS, as stated in Auer et al. (2020). Here the authors used expert judgment to score\neach document with a value of -1, 0 or +1. Second, in the Appendix we look at some\nof the cases where there is major disagreement between the three techniques.\nWe can look into how sentiment evolves over time in several ways. First, in Figures 2\nand 3, we group the documents by year (on the left) and by quarter (on the right), and\ntake the average of the documents in that period. We must bear in mind that there\nis a different number of documents in each year and quarter (as we saw in Figure\n1 above), so the figures may be biased by moments of time with a higher or lower\nnumber of documents. The annual and quarterly average sentiment shows a growing\ntrend from 2017 onwards, for all techniques. Polarity seems to be more volatile and the\nfact that the sentiment of ChatGPT is greater than BERT, which in turn is greater\nthan Polarity, is present in almost the entire sample. Sentiment for ChatGPT moves\nfrom 0.2 to 0.4, for BERT it moves around 0.1 and 0.2, and for Polarity it is much\n10\nmore volatile.\nThis analysis has the limitation that the number of documents in each quarter and\nyear is different. For this reason, we carry out a moving average analysis, the results\nof which are shown in Figure 4. There, we show, document by document, the moving\naverage of sentiment using 20 documents as a rolling window. The figure confirms that\nsentiment towards CBDC appears to be more positive as time goes by, based on all\nthree techniques. For example, ChatGPT starts at about 0.2 and ends at 0.4, BERT\nstarts at 0.1 and ends at 0.2, and Polarity is the most extreme case, starting from -0.1\nto 0.3. Lower initial sentiment on CBDC is in line with the idea that central banks\nhave started to talk cautiously about the systemic implications of CBDC (Barontini\nand Holden 2019). But over time, we see an increasing trend in sentiment, reflecting\nthat several central banks have become enthusiastic about exploring the idea of issuing\na CBDC.\nFigure 2. Sentiment. Avg year Figure 3. Sentiment. Avg quarter\nFigure 4. Sentiment. Moving Avg\n11\nmore volatile.\nThis analysis has the limitation that the number of documents in each quarter and\nyear is different. For this reason, we carry out a moving average analysis, the results\nof which are shown in Figure 4. There, we show, document by document, the moving\naverage of sentiment using 20 documents as a rolling window. The figure confirms that\nsentiment towards CBDC appears to be more positive as time goes by, based on all\nthree techniques. For example, ChatGPT starts at about 0.2 and ends at 0.4, BERT\nstarts at 0.1 and ends at 0.2, and Polarity is the most extreme case, starting from -0.1\nto 0.3. Lower initial sentiment on CBDC is in line with the idea that central banks\nhave started to talk cautiously about the systemic implications of CBDC (Barontini\nand Holden 2019). But over time, we see an increasing trend in sentiment, reflecting\nthat several central banks have become enthusiastic about exploring the idea of issuing\na CBDC.\nFigure 2. Sentiment. Avg year Figure 3. Sentiment. Avg quarter\nFigure 4. Sentiment. Moving Avg\n11\nmore volatile.\nThis analysis has the limitation that the number of documents in each quarter and\nyear is different. For this reason, we carry out a moving average analysis, the results\nof which are shown in Figure 4. There, we show, document by document, the moving\naverage of sentiment using 20 documents as a rolling window. The figure confirms that\nsentiment towards CBDC appears to be more positive as time goes by, based on all\nthree techniques. For example, ChatGPT starts at about 0.2 and ends at 0.4, BERT\nstarts at 0.1 and ends at 0.2, and Polarity is the most extreme case, starting from -0.1\nto 0.3. Lower initial sentiment on CBDC is in line with the idea that central banks\nhave started to talk cautiously about the systemic implications of CBDC (Barontini\nand Holden 2019). But over time, we see an increasing trend in sentiment, reflecting\nthat several central banks have become enthusiastic about exploring the idea of issuing\na CBDC.\nFigure 2. Sentiment. Avg year\n Figure 3. Sentiment. Avg quarter\nFigure 4. Sentiment. Moving Avg\n11\nBANCO DE ESPAÑA\n16\nDOCUMENTO DE TRABAJO N.º 2321\nTable 3. Correlation with expert judgment. Benchmark exercise: correlation between the sentiment obtained\nby the three techniques with the sentiment labeled in Auer et al. (2020), for full texts, and for larger texts,\ncombining paragraphs.\nAll text +1 paragraph\n(242 docs)\nChatGPT 0.34*** 0.44***\nBERT 0.29*** 0.34***\nDictionary-based 0.21*** 0.22***\n4.2. Comparing with humans and labeled data\nWe have seen that all three techniques show a positive evolution of sentiment towards\nCBDC. But, which technique is more accurate? To assess the accuracy, we aim to\ncalculate if they are comparable to what a human would do. To do this, we perform\nthree exercises. We first calculate the correlation between the sentiment score of each\ntechnique with that reported by human experts, in particular the economists working\nat BIS, as stated in Auer et al. (2020). Here the authors used expert judgment to score\neach document with a value of -1, 0 or +1. Second, in the Appendix we look at some\nof the cases where there is major disagreement between the three techniques. 15\nIn Table 3 we show the correlation between the sentiment obtained by the three\ntechniques with the sentiment labeled in Auer et al. (2020), for full texts, and for larger\ntexts, combining paragraphs. These correlations are in all cases positive and signifi-\ncant, which indicate that the techniques are capturing up to some level the sentiment\nexpressed by humans. But LLMs are measuring better that sentiment, specially Chat-\nGPT. Moreover, the difference between ChatGPT and the other techniques is bigger\nin larger texts, where the correlation between ChatGPT and BIS labeled data is sig-\nnificantly higher than the others. This suggests that ChatGPT captures sentiment\nbetter, at least compared to BERT and Polarity. Figure 5 shows the correlation be-\ntween ChatGPT and human labels for texts with more than 100 words, where this\nrelationship can be visually appreciated.\nFinally, we compare the temporal evolution of the sentiment of ChatGPT with that\nof labeled data from human experts. To do this, we reconstructed the graph made\nin Auer et al. (2020), in which they plot the cumulative sum of sentiment month by\nmonth. To compare with ChatGPT, we translate the ChatGPT score to -1, 0, and 1,\nclassifying the bottom 10% scores as -1, the top 50% scores as +1, and the rest of the\nscores as 0 (following the distribution -1, 0 and 1 of the expert judgment made by the\nhuman experts). The result is shown in Figure 6, with the orange line being for BIS\nsentiment (Auer et al. 2020) and the blue line for ChatGPT. It can be seen that the\ntrend changes are very similar. The similarity between the labeled data and ChatGPT\nis remarkable, especially considering that the prompt we used was quite generic, and\nit could be further tailored to capture BIS preferences.\nNot only are the ChatGPT sentiment scores more similar to the expert labeled data,\nbut it appears that the tone distribution of both ChatGPT and BERT are more human-\nlike, while the polarity distribution differs. In Figure 7 we plot the histogram of the\nsentiments per document provided by the three techniques, and in Figure 8 we show the\nhistogram of the typical scores provided by humans, as shown in Guo et al. (2023). The\n15We leave for further Research how to assess the predictability each sentiment score. E.g.: using a random\nforest model we could predict the sentiment score based on various characteristics, such as country of origin,\ntime dummies, economic policy uncertainty index, interest in CBDC on Google web searches, and Bitcoin\nprices.\n12\nmore volatile.\nThis analysis has the limitation that the number of documents in each quarter and\nyear is different. For this reason, we carry out a moving average analysis, the results\nof which are shown in Figure 4. There, we show, document by document, the moving\naverage of sentiment using 20 documents as a rolling window. The figure confirms that\nsentiment towards CBDC appears to be more positive as time goes by, based on all\nthree techniques. For example, ChatGPT starts at about 0.2 and ends at 0.4, BERT\nstarts at 0.1 and ends at 0.2, and Polarity is the most extreme case, starting from -0.1\nto 0.3. Lower initial sentiment on CBDC is in line with the idea that central banks\nhave started to talk cautiously about the systemic implications of CBDC (Barontini\nand Holden 2019). But over time, we see an increasing trend in sentiment, reflecting\nthat several central banks have become enthusiastic about exploring the idea of issuing\na CBDC.\nFigure 2. Sentiment. Avg year Figure 3. Sentiment. Avg quarter\nFigure 4. Sentiment. Moving Avg\n11\nmore volatile.\nThis analysis has the limitation that the number of documents in each quarter and\nyear is different. For this reason, we carry out a moving average analysis, the results\nof which are shown in Figure 4. There, we show, document by document, the moving\naverage of sentiment using 20 documents as a rolling window. The figure confirms that\nsentiment towards CBDC appears to be more positive as time goes by, based on all\nthree techniques. For example, ChatGPT starts at about 0.2 and ends at 0.4, BERT\nstarts at 0.1 and ends at 0.2, and Polarity is the most extreme case, starting from -0.1\nto 0.3. Lower initial sentiment on CBDC is in line with the idea that central banks\nhave started to talk cautiously about the systemic implications of CBDC (Barontini\nand Holden 2019). But over time, we see an increasing trend in sentiment, reflecting\nthat several central banks have become enthusiastic about exploring the idea of issuing\na CBDC.\nFigure 2. Sentiment. Avg year\n Figure 3. Sentiment. Avg quarter\nFigure 4. Sentiment. Moving Avg\n11\nTable 3. Correlation with expert judgment. Benchmark exercise: correlation between the sentiment obtained\nby the three techniques with the sentiment labeled in Auer et al. (2020), for full texts, and for larger texts,\ncombining paragraphs.\nAll text +1 paragraph\n(242 docs)\nChatGPT 0.34*** 0.44***\nBERT 0.29*** 0.34***\nDictionary-based 0.21*** 0.22***\n4.2. Comparing with humans and labeled data\nWe have seen that all three techniques show a positive evolution of sentiment towards\nCBDC. But, which technique is more accurate? To assess the accuracy, we aim to\ncalculate if they are comparable to what a human would do. To do this, we perform\nthree exercises. We first calculate the correlation between the sentiment score of each\ntechnique with that reported by human experts, in particular the economists working\nat BIS, as stated in Auer et al. (2020). Here the authors used expert judgment to score\neach document with a value of -1, 0 or +1. Second, in the Appendix we look at some\nof the cases where there is major disagreement between the three techniques. 15\nIn Table 3 we show the correlation between the sentiment obtained by the three\ntechniques with the sentiment labeled in Auer et al. (2020), for full texts, and for larger\ntexts, combining paragraphs. These correlations are in all cases positive and signifi-\ncant, which indicate that the techniques are capturing up to some level the sentiment\nexpressed by humans. But LLMs are measuring better that sentiment, specially Chat-\nGPT. Moreover, the difference between ChatGPT and the other techniques is bigger\nin larger texts, where the correlation between ChatGPT and BIS labeled data is sig-\nnificantly higher than the others. This suggests that ChatGPT captures sentiment\nbetter, at least compared to BERT and Polarity. Figure 5 shows the correlation be-\ntween ChatGPT and human labels for texts with more than 100 words, where this\nrelationship can be visually appreciated.\nFinally, we compare the temporal evolution of the sentiment of ChatGPT with that\nof labeled data from human experts. To do this, we reconstructed the graph made\nin Auer et al. (2020), in which they plot the cumulative sum of sentiment month by\nmonth. To compare with ChatGPT, we translate the ChatGPT score to -1, 0, and 1,\nclassifying the bottom 10% scores as -1, the top 50% scores as +1, and the rest of the\nscores as 0 (following the distribution -1, 0 and 1 of the expert judgment made by the\nhuman experts). The result is shown in Figure 6, with the orange line being for BIS\nsentiment (Auer et al. 2020) and the blue line for ChatGPT. It can be seen that the\ntrend changes are very similar. The similarity between the labeled data and ChatGPT\nis remarkable, especially considering that the prompt we used was quite generic, and\nit could be further tailored to capture BIS preferences.\nNot only are the ChatGPT sentiment scores more similar to the expert labeled data,\nbut it appears that the tone distribution of both ChatGPT and BERT are more human-\nlike, while the polarity distribution differs. In Figure 7 we plot the histogram of the\nsentiments per document provided by the three techniques, and in Figure 8 we show the\nhistogram of the typical scores provided by humans, as shown in Guo et al. (2023). The\n15We leave for further Research how to assess the predictability each sentiment score. E.g.: using a random\nforest model we could predict the sentiment score based on various characteristics, such as country of origin,\ntime dummies, economic policy uncertainty index, interest in CBDC on Google web searches, and Bitcoin\nprices.\n12\nTable 3. Correlation with expert judgment. Benchmark exercise: correlation between the sentiment obtained\nby the three techniques with the sentiment labeled in Auer et al. (2020), for full texts, and for larger texts,\ncombining paragraphs.\nAll text +1 paragraph\n(242 docs)\nChatGPT 0.34*** 0.44***\nBERT 0.29*** 0.34***\nDictionary-based 0.21*** 0.22***\n4.2. Comparing with humans and labeled data\nWe have seen that all three techniques show a positive evolution of sentiment towards\nCBDC. But, which technique is more accurate? To assess the accuracy, we aim to\ncalculate if they are comparable to what a human would do. To do this, we perform\nthree exercises. We first calculate the correlation between the sentiment score of each\ntechnique with that reported by human experts, in particular the economists working\nat BIS, as stated in Auer et al. (2020). Here the authors used expert judgment to score\neach document with a value of -1, 0 or +1. Second, in the Appendix we look at some\nof the cases where there is major disagreement between the three techniques. 15\nIn Table 3 we show the correlation between the sentiment obtained by the three\ntechniques with the sentiment labeled in Auer et al. (2020), for full texts, and for larger\ntexts, combining paragraphs. These correlations are in all cases positive and signifi-\ncant, which indicate that the techniques are capturing up to some level the sentiment\nexpressed by humans. But LLMs are measuring better that sentiment, specially Chat-\nGPT. Moreover, the difference between ChatGPT and the other techniques is bigger\nin larger texts, where the correlation between ChatGPT and BIS labeled data is sig-\nnificantly higher than the others. This suggests that ChatGPT captures sentiment\nbetter, at least compared to BERT and Polarity. Figure 5 shows the correlation be-\ntween ChatGPT and human labels for texts with more than 100 words, where this\nrelationship can be visually appreciated.\nFinally, we compare the temporal evolution of the sentiment of ChatGPT with that\nof labeled data from human experts. To do this, we reconstructed the graph made\nin Auer et al. (2020), in which they plot the cumulative sum of sentiment month by\nmonth. To compare with ChatGPT, we translate the ChatGPT score to -1, 0, and 1,\nclassifying the bottom 10% scores as -1, the top 50% scores as +1, and the rest of the\nscores as 0 (following the distribution -1, 0 and 1 of the expert judgment made by the\nhuman experts). The result is shown in Figure 6, with the orange line being for BIS\nsentiment (Auer et al. 2020) and the blue line for ChatGPT. It can be seen that the\ntrend changes are very similar. The similarity between the labeled data and ChatGPT\nis remarkable, especially considering that the prompt we used was quite generic, and\nit could be further tailored to capture BIS preferences.\nNot only are the ChatGPT sentiment scores more similar to the expert labeled data,\nbut it appears that the tone distribution of both ChatGPT and BERT are more human-\nlike, while the polarity distribution differs. In Figure 7 we plot the histogram of the\nsentiments per document provided by the three techniques, and in Figure 8 we show the\nhistogram of the typical scores provided by humans, as shown in Guo et al. (2023). The\n15We leave for further Research how to assess the predictability each sentiment score. E.g.: using a random\nforest model we could predict the sentiment score based on various characteristics, such as country of origin,\ntime dummies, economic policy uncertainty index, interest in CBDC on Google web searches, and Bitcoin\nprices.\n12\nTable 3. Correlation with expert judgment. Benchmark exercise: correlation between the sentiment obtained\nby the three techniques with the sentiment labeled in Auer et al. (2020), for full texts, and for larger texts,\ncombining paragraphs.\nAll text +1 paragraph\n(242 docs)\nChatGPT 0.34*** 0.44***\nBERT 0.29*** 0.34***\nDictionary-based 0.21*** 0.22***\n4.2. Comparing with humans and labeled data\nWe have seen that all three techniques show a positive evolution of sentiment towards\nCBDC. But, which technique is more accurate? To assess the accuracy, we aim to\ncalculate if they are comparable to what a human would do. To do this, we perform\nthree exercises. We first calculate the correlation between the sentiment score of each\ntechnique with that reported by human experts, in particular the economists working\nat BIS, as stated in Auer et al. (2020). Here the authors used expert judgment to score\neach document with a value of -1, 0 or +1. Second, in the Appendix we look at some\nof the cases where there is major disagreement between the three techniques. 15\nIn Table 3 we show the correlation between the sentiment obtained by the three\ntechniques with the sentiment labeled in Auer et al. (2020), for full texts, and for larger\ntexts, combining paragraphs. These correlations are in all cases positive and signifi-\ncant, which indicate that the techniques are capturing up to some level the sentiment\nexpressed by humans. But LLMs are measuring better that sentiment, specially Chat-\nGPT. Moreover, the difference between ChatGPT and the other techniques is bigger\nin larger texts, where the correlation between ChatGPT and BIS labeled data is sig-\nnificantly higher than the others. This suggests that ChatGPT captures sentiment\nbetter, at least compared to BERT and Polarity. Figure 5 shows the correlation be-\ntween ChatGPT and human labels for texts with more than 100 words, where this\nrelationship can be visually appreciated.\nFinally, we compare the temporal evolution of the sentiment of ChatGPT with that\nof labeled data from human experts. To do this, we reconstructed the graph made\nin Auer et al. (2020), in which they plot the cumulative sum of sentiment month by\nmonth. To compare with ChatGPT, we translate the ChatGPT score to -1, 0, and 1,\nclassifying the bottom 10% scores as -1, the top 50% scores as +1, and the rest of the\nscores as 0 (following the distribution -1, 0 and 1 of the expert judgment made by the\nhuman experts). The result is shown in Figure 6, with the orange line being for BIS\nsentiment (Auer et al. 2020) and the blue line for ChatGPT. It can be seen that the\ntrend changes are very similar. The similarity between the labeled data and ChatGPT\nis remarkable, especially considering that the prompt we used was quite generic, and\nit could be further tailored to capture BIS preferences.\nNot only are the ChatGPT sentiment scores more similar to the expert labeled data,\nbut it appears that the tone distribution of both ChatGPT and BERT are more human-\nlike, while the polarity distribution differs. In Figure 7 we plot the histogram of the\nsentiments per document provided by the three techniques, and in Figure 8 we show the\nhistogram of the typical scores provided by humans, as shown in Guo et al. (2023). The\n15We leave for further Research how to assess the predictability each sentiment score. E.g.: using a random\nforest model we could predict the sentiment score based on various characteristics, such as country of origin,\ntime dummies, economic policy uncertainty index, interest in CBDC on Google web searches, and Bitcoin\nprices.\n12\nmore volatile.\nThis analysis has the limitation that the number of documents in each quarter and\nyear is different. For this reason, we carry out a moving average analysis, the results\nof which are shown in Figure 4. There, we show, document by document, the moving\naverage of sentiment using 20 documents as a rolling window. The figure confirms that\nsentiment towards CBDC appears to be more positive as time goes by, based on all\nthree techniques. For example, ChatGPT starts at about 0.2 and ends at 0.4, BERT\nstarts at 0.1 and ends at 0.2, and Polarity is the most extreme case, starting from -0.1\nto 0.3. Lower initial sentiment on CBDC is in line with the idea that central banks\nhave started to talk cautiously about the systemic implications of CBDC (Barontini\nand Holden 2019). But over time, we see an increasing trend in sentiment, reflecting\nthat several central banks have become enthusiastic about exploring the idea of issuing\na CBDC.\nFigure 2. Sentiment. Avg year Figure 3. Sentiment. Avg quarter\nFigure 4. Sentiment. Moving Avg\n11\nBANCO DE ESPAÑA\n17\nDOCUMENTO DE TRABAJO N.º 2321\nTable 3. Correlation with expert judgment. Benchmark exercise: correlation between the sentiment obtained\nby the three techniques with the sentiment labeled in Auer et al. (2020), for full texts, and for larger texts,\ncombining paragraphs.\nAll text +1 paragraph\n(242 docs)\nChatGPT 0.34*** 0.44***\nBERT 0.29*** 0.34***\nDictionary-based 0.21*** 0.22***\n4.2. Comparing with humans and labeled data\nWe have seen that all three techniques show a positive evolution of sentiment towards\nCBDC. But, which technique is more accurate? To assess the accuracy, we aim to\ncalculate if they are comparable to what a human would do. To do this, we perform\nthree exercises. We first calculate the correlation between the sentiment score of each\ntechnique with that reported by human experts, in particular the economists working\nat BIS, as stated in Auer et al. (2020). Here the authors used expert judgment to score\neach document with a value of -1, 0 or +1. Second, in the Appendix we look at some\nof the cases where there is major disagreement between the three techniques. 15\nIn Table 3 we show the correlation between the sentiment obtained by the three\ntechniques with the sentiment labeled in Auer et al. (2020), for full texts, and for larger\ntexts, combining paragraphs. These correlations are in all cases positive and signifi-\ncant, which indicate that the techniques are capturing up to some level the sentiment\nexpressed by humans. But LLMs are measuring better that sentiment, specially Chat-\nGPT. Moreover, the difference between ChatGPT and the other techniques is bigger\nin larger texts, where the correlation between ChatGPT and BIS labeled data is sig-\nnificantly higher than the others. This suggests that ChatGPT captures sentiment\nbetter, at least compared to BERT and Polarity. Figure 5 shows the correlation be-\ntween ChatGPT and human labels for texts with more than 100 words, where this\nrelationship can be visually appreciated.\nFinally, we compare the temporal evolution of the sentiment of ChatGPT with that\nof labeled data from human experts. To do this, we reconstructed the graph made\nin Auer et al. (2020), in which they plot the cumulative sum of sentiment month by\nmonth. To compare with ChatGPT, we translate the ChatGPT score to -1, 0, and 1,\nclassifying the bottom 10% scores as -1, the top 50% scores as +1, and the rest of the\nscores as 0 (following the distribution -1, 0 and 1 of the expert judgment made by the\nhuman experts). The result is shown in Figure 6, with the orange line being for BIS\nsentiment (Auer et al. 2020) and the blue line for ChatGPT. It can be seen that the\ntrend changes are very similar. The similarity between the labeled data and ChatGPT\nis remarkable, especially considering that the prompt we used was quite generic, and\nit could be further tailored to capture BIS preferences.\nNot only are the ChatGPT sentiment scores more similar to the expert labeled data,\nbut it appears that the tone distribution of both ChatGPT and BERT are more human-\nlike, while the polarity distribution differs. In Figure 7 we plot the histogram of the\nsentiments per document provided by the three techniques, and in Figure 8 we show the\nhistogram of the typical scores provided by humans, as shown in Guo et al. (2023). The\n15We leave for further Research how to assess the predictability each sentiment score. E.g.: using a random\nforest model we could predict the sentiment score based on various characteristics, such as country of origin,\ntime dummies, economic policy uncertainty index, interest in CBDC on Google web searches, and Bitcoin\nprices.\n12\nFigure 5. Correlation between ChatGPT and expert judgment: correlation between ChatGPT and human\nlabels for texts with more than 100 words.\npolarity histogram follows a nearly symmetrical distribution, whereas humans tend to\nfollow a distribution closer to a Chi-square. Guo et al. (2023) conducted comprehensive\nhuman evaluations and linguistic analyses of the content generated by ChatGPT in\ncomparison to that of humans, and it revealed interesting results. First, the authors\nfind that the proportion of neutral emotions is higher for both humans and ChatGPT,\nwhich is in line with our results in this study. However, ChatGPT generally expresses\nless negative feelings than humans. This is also the case in our study. Therefore,\na limitation of ChatGPT to analyze sentiment can occur when evaluating negative\npostures, since humans express more negative emotions than ChatGPT.\n4.3. Robustness analysis: Different prompts and larger texts\nIs ChatGPT capturing correctly the increasing sentiment of central banks towards\nCBDC, or it is by default more optimistic than BERT and Dictionary-based methods?\nDo our benchmark results hold for larger texts? In this Section we propose two analyses\nto test these questions.\n4.3.1. Robustness analysis. Prompts for crypto\nIn order to test whether ChatGPT is more optimistic than BERT or Polarity by\ndefault, we perform a robustness analysis in which we repeat the workflow described in\nFigure 1 but select paragraphs according to the following keywords: crypto(s), crypto\nasset(s), crypto-asset(s), Bitcoin. We compute the sentiment over those paragraphs,\nand we use as a prompt for ChatGPT:\n“Compute the sentiment score towards crypto assets, measured between -1 and 1, of\na given text. The response should be just a float number, no text. The text is as follows:\n[. . . ]”\n13\nFigure 6. Cumulative sum ChatGPT vs expert judgment: comparing ChatGPT with human labels. The\norange line is the BIS sentiment (Auer et al. 2020) and the blue line is ChatGPT.\nThe results of the rolling window analysis for the three techniques can be seen in\nFigure 9. For crypto assets the sentiment does not display an upward trend behavior\nfor any of the techniques, as expected. 16 Again, Polarity is the more volatile, and al-\nthough on average is always below BERT and ChatGPT, there are moments in time in\nwhich sentiment with Polarity is higher than the one from the LLMs. Moreover, Chat-\nGPT (green) does not always indicate a higher sentiment that the other techniques.\nTherefore, the fact that ChatGPT captures more sentiment towards CBDC seems to\nbe genuine, and is not just a result of the fact that ChatGPT could be less negative\noverall (Figure 8).\n4.3.2. Robustness analysis. Larger texts\nAs we showed in Table 2, the average text size after selecting CBDC keywords is less\nthan 300 words. Technically we could fit all texts with less than 500 words directly\nto BERT and ChatGPT without restrictions. In this robustness check, instead of\nevaluating the paragraphs one by one, we merge them (in texts with more than 1\nparagraph) until we create big paragraphs of 500 words. The problem with this is that\nwe might put together paragraphs that do not fit necessarily (they could belong to\ndifferent parts of the texts). This does not matter for Polarity because it acts as a\nbag-of-words (the order of words does not matter) but might have implications for the\nLLMs. On the other hand, LLMs might benefit from having access to bigger texts. We\nshow the correlation between labeled data and the three techniques with larger texts\nin Table 4. It can be seen that in larger texts Polarity does not improve its results,\nwhile BERT and ChatGPT improve it, specially ChatGPT. This is another indicator\nthat ChatGPT is able to capture better the sentiment, and might benefit from the\n16See for instance the list of speeches of the European Central Bank on crypto-assets.\n14\nFigure 6. Cumulative sum ChatGPT vs expert judgment: comparing ChatGPT with human labels. The\norange line is the BIS sentiment (Auer et al. 2020) and the blue line is ChatGPT.\nThe results of the rolling window analysis for the three techniques can be seen in\nFigure 9. For crypto assets the sentiment does not display an upward trend behavior\nfor any of the techniques, as expected. 16 Again, Polarity is the more volatile, and al-\nthough on average is always below BERT and ChatGPT, there are moments in time in\nwhich sentiment with Polarity is higher than the one from the LLMs. Moreover, Chat-\nGPT (green) does not always indicate a higher sentiment that the other techniques.\nTherefore, the fact that ChatGPT captures more sentiment towards CBDC seems to\nbe genuine, and is not just a result of the fact that ChatGPT could be less negative\noverall (Figure 8).\n4.3.2. Robustness analysis. Larger texts\nAs we showed in Table 2, the average text size after selecting CBDC keywords is less\nthan 300 words. Technically we could fit all texts with less than 500 words directly\nto BERT and ChatGPT without restrictions. In this robustness check, instead of\nevaluating the paragraphs one by one, we merge them (in texts with more than 1\nparagraph) until we create big paragraphs of 500 words. The problem with this is that\nwe might put together paragraphs that do not fit necessarily (they could belong to\ndifferent parts of the texts). This does not matter for Polarity because it acts as a\nbag-of-words (the order of words does not matter) but might have implications for the\nLLMs. On the other hand, LLMs might benefit from having access to bigger texts. We\nshow the correlation between labeled data and the three techniques with larger texts\nin Table 4. It can be seen that in larger texts Polarity does not improve its results,\nwhile BERT and ChatGPT improve it, specially ChatGPT. This is another indicator\nthat ChatGPT is able to capture better the sentiment, and might benefit from the\n16See for instance the list of speeches of the European Central Bank on crypto-assets.\n14\nFigure 5. Correlation between ChatGPT and expert judgment: correlation between ChatGPT and human\nlabels for texts with more than 100 words.\npolarity histogram follows a nearly symmetrical distribution, whereas humans tend to\nfollow a distribution closer to a Chi-square. Guo et al. (2023) conducted comprehensive\nhuman evaluations and linguistic analyses of the content generated by ChatGPT in\ncomparison to that of humans, and it revealed interesting results. First, the authors\nfind that the proportion of neutral emotions is higher for both humans and ChatGPT,\nwhich is in line with our results in this study. However, ChatGPT generally expresses\nless negative feelings than humans. This is also the case in our study. Therefore,\na limitation of ChatGPT to analyze sentiment can occur when evaluating negative\npostures, since humans express more negative emotions than ChatGPT.\n4.3. Robustness analysis: Different prompts and larger texts\nIs ChatGPT capturing correctly the increasing sentiment of central banks towards\nCBDC, or it is by default more optimistic than BERT and Dictionary-based methods?\nDo our benchmark results hold for larger texts? In this Section we propose two analyses\nto test these questions.\n4.3.1. Robustness analysis. Prompts for crypto\nIn order to test whether ChatGPT is more optimistic than BERT or Polarity by\ndefault, we perform a robustness analysis in which we repeat the workflow described in\nFigure 1 but select paragraphs according to the following keywords: crypto(s), crypto\nasset(s), crypto-asset(s), Bitcoin. We compute the sentiment over those paragraphs,\nand we use as a prompt for ChatGPT:\n“Compute the sentiment score towards crypto assets, measured between -1 and 1, of\na given text. The response should be just a float number, no text. The text is as follows:\n[. . . ]”\n13\nBANCO DE ESPAÑA\n18\nDOCUMENTO DE TRABAJO N.º 2321\nFigure 5. Correlation between ChatGPT and expert judgment: correlation between ChatGPT and human\nlabels for texts with more than 100 words.\npolarity histogram follows a nearly symmetrical distribution, whereas humans tend to\nfollow a distribution closer to a Chi-square. Guo et al. (2023) conducted comprehensive\nhuman evaluations and linguistic analyses of the content generated by ChatGPT in\ncomparison to that of humans, and it revealed interesting results. First, the authors\nfind that the proportion of neutral emotions is higher for both humans and ChatGPT,\nwhich is in line with our results in this study. However, ChatGPT generally expresses\nless negative feelings than humans. This is also the case in our study. Therefore,\na limitation of ChatGPT to analyze sentiment can occur when evaluating negative\npostures, since humans express more negative emotions than ChatGPT.\n4.3. Robustness analysis: Different prompts and larger texts\nIs ChatGPT capturing correctly the increasing sentiment of central banks towards\nCBDC, or it is by default more optimistic than BERT and Dictionary-based methods?\nDo our benchmark results hold for larger texts? In this Section we propose two analyses\nto test these questions.\n4.3.1. Robustness analysis. Prompts for crypto\nIn order to test whether ChatGPT is more optimistic than BERT or Polarity by\ndefault, we perform a robustness analysis in which we repeat the workflow described in\nFigure 1 but select paragraphs according to the following keywords: crypto(s), crypto\nasset(s), crypto-asset(s), Bitcoin. We compute the sentiment over those paragraphs,\nand we use as a prompt for ChatGPT:\n“Compute the sentiment score towards crypto assets, measured between -1 and 1, of\na given text. The response should be just a float number, no text. The text is as follows:\n[. . . ]”\n13\nTable 3. Correlation with expert judgment. Benchmark exercise: correlation between the sentiment obtained\nby the three techniques with the sentiment labeled in Auer et al. (2020), for full texts, and for larger texts,\ncombining paragraphs.\nAll text +1 paragraph\n(242 docs)\nChatGPT 0.34*** 0.44***\nBERT 0.29*** 0.34***\nDictionary-based 0.21*** 0.22***\n4.2. Comparing with humans and labeled data\nWe have seen that all three techniques show a positive evolution of sentiment towards\nCBDC. But, which technique is more accurate? To assess the accuracy, we aim to\ncalculate if they are comparable to what a human would do. To do this, we perform\nthree exercises. We first calculate the correlation between the sentiment score of each\ntechnique with that reported by human experts, in particular the economists working\nat BIS, as stated in Auer et al. (2020). Here the authors used expert judgment to score\neach document with a value of -1, 0 or +1. Second, in the Appendix we look at some\nof the cases where there is major disagreement between the three techniques. 15\nIn Table 3 we show the correlation between the sentiment obtained by the three\ntechniques with the sentiment labeled in Auer et al. (2020), for full texts, and for larger\ntexts, combining paragraphs. These correlations are in all cases positive and signifi-\ncant, which indicate that the techniques are capturing up to some level the sentiment\nexpressed by humans. But LLMs are measuring better that sentiment, specially Chat-\nGPT. Moreover, the difference between ChatGPT and the other techniques is bigger\nin larger texts, where the correlation between ChatGPT and BIS labeled data is sig-\nnificantly higher than the others. This suggests that ChatGPT captures sentiment\nbetter, at least compared to BERT and Polarity. Figure 5 shows the correlation be-\ntween ChatGPT and human labels for texts with more than 100 words, where this\nrelationship can be visually appreciated.\nFinally, we compare the temporal evolution of the sentiment of ChatGPT with that\nof labeled data from human experts. To do this, we reconstructed the graph made\nin Auer et al. (2020), in which they plot the cumulative sum of sentiment month by\nmonth. To compare with ChatGPT, we translate the ChatGPT score to -1, 0, and 1,\nclassifying the bottom 10% scores as -1, the top 50% scores as +1, and the rest of the\nscores as 0 (following the distribution -1, 0 and 1 of the expert judgment made by the\nhuman experts). The result is shown in Figure 6, with the orange line being for BIS\nsentiment (Auer et al. 2020) and the blue line for ChatGPT. It can be seen that the\ntrend changes are very similar. The similarity between the labeled data and ChatGPT\nis remarkable, especially considering that the prompt we used was quite generic, and\nit could be further tailored to capture BIS preferences.\nNot only are the ChatGPT sentiment scores more similar to the expert labeled data,\nbut it appears that the tone distribution of both ChatGPT and BERT are more human-\nlike, while the polarity distribution differs. In Figure 7 we plot the histogram of the\nsentiments per document provided by the three techniques, and in Figure 8 we show the\nhistogram of the typical scores provided by humans, as shown in Guo et al. (2023). The\n15We leave for further Research how to assess the predictability each sentiment score. E.g.: using a random\nforest model we could predict the sentiment score based on various characteristics, such as country of origin,\ntime dummies, economic policy uncertainty index, interest in CBDC on Google web searches, and Bitcoin\nprices.\n12\nFigure 7. Histogram three techniques with different techniques: histogram of the sentiments per document\nprovided by the three techniques\nTable 4. Correlation with expert judgment. Larger texts: correlation between labeled data and the three\nNLP techniques with larger texts.\nTexts of at least 500 words\nChatGPT 0.50 ***\nBert 0.38 ***\nDictionary 0.25 **\ninclusion of larger texts. It is worth mentioning that GPT4 would be able to read\ntexts of up to 32,000 tokens. However, at the time of writing, GPT4 was not accessible\nthrough the API.\n5. Conclusion and further work\nCurrently, central banks issue statements and opinions on topics that are not related\nto those traditionally associated with central bank communication (Hansson 2021).\nRecent studies suggest that these communications are important for financial markets,\neven when the topics discussed are not directly linked to monetary policy (Fortes\nand Le Guenedal 2020). One of these new topics is the discussion on the need and\ndesirability of a new type of money: central bank digital currencies (CBDCs).\nOur work investigates the sentiment of central banks’ communication about the\nissuance of digital money through CBDC. For the first time in the literature we use\nLLMs (BERT and ChatGPT) to compute a sentiment score of these speeches and\nreports, and we compare them to traditional dictionary-based techniques and human\nlabels. First, we find that the sentiment towards CBDC seems to be increasing from\n2017 onwards according to both LLMs and dictionary-based techniques, but the latter\nare more volatile. Second, we compare the sentiment obtained by the three techniques\n15\nFigure 7. Histogram three techniques with different techniques: histogram of the sentiments per document\nprovided by the three techniques\nTable 4. Correlation with expert judgment. Larger texts: correlation between labeled data and the three\nNLP techniques with larger texts.\nTexts of at least 500 words\nChatGPT 0.50 ***\nBert 0.38 ***\nDictionary 0.25 **\ninclusion of larger texts. It is worth mentioning that GPT4 would be able to read\ntexts of up to 32,000 tokens. However, at the time of writing, GPT4 was not accessible\nthrough the API.\n5. Conclusion and further work\nCurrently, central banks issue statements and opinions on topics that are not related\nto those traditionally associated with central bank communication (Hansson 2021).\nRecent studies suggest that these communications are important for financial markets,\neven when the topics discussed are not directly linked to monetary policy (Fortes\nand Le Guenedal 2020). One of these new topics is the discussion on the need and\ndesirability of a new type of money: central bank digital currencies (CBDCs).\nOur work investigates the sentiment of central banks’ communication about the\nissuance of digital money through CBDC. For the first time in the literature we use\nLLMs (BERT and ChatGPT) to compute a sentiment score of these speeches and\nreports, and we compare them to traditional dictionary-based techniques and human\nlabels. First, we find that the sentiment towards CBDC seems to be increasing from\n2017 onwards according to both LLMs and dictionary-based techniques, but the latter\nare more volatile. Second, we compare the sentiment obtained by the three techniques\n15\nBANCO DE ESPAÑA\n19\nDOCUMENTO DE TRABAJO N.º 2321\nFigure 6. Cumulative sum ChatGPT vs expert judgment: comparing ChatGPT with human labels. The\norange line is the BIS sentiment (Auer et al. 2020) and the blue line is ChatGPT.\nThe results of the rolling window analysis for the three techniques can be seen in\nFigure 9. For crypto assets the sentiment does not display an upward trend behavior\nfor any of the techniques, as expected. 16 Again, Polarity is the more volatile, and al-\nthough on average is always below BERT and ChatGPT, there are moments in time in\nwhich sentiment with Polarity is higher than the one from the LLMs. Moreover, Chat-\nGPT (green) does not always indicate a higher sentiment that the other techniques.\nTherefore, the fact that ChatGPT captures more sentiment towards CBDC seems to\nbe genuine, and is not just a result of the fact that ChatGPT could be less negative\noverall (Figure 8).\n4.3.2. Robustness analysis. Larger texts\nAs we showed in Table 2, the average text size after selecting CBDC keywords is less\nthan 300 words. Technically we could fit all texts with less than 500 words directly\nto BERT and ChatGPT without restrictions. In this robustness check, instead of\nevaluating the paragraphs one by one, we merge them (in texts with more than 1\nparagraph) until we create big paragraphs of 500 words. The problem with this is that\nwe might put together paragraphs that do not fit necessarily (they could belong to\ndifferent parts of the texts). This does not matter for Polarity because it acts as a\nbag-of-words (the order of words does not matter) but might have implications for the\nLLMs. On the other hand, LLMs might benefit from having access to bigger texts. We\nshow the correlation between labeled data and the three techniques with larger texts\nin Table 4. It can be seen that in larger texts Polarity does not improve its results,\nwhile BERT and ChatGPT improve it, specially ChatGPT. This is another indicator\nthat ChatGPT is able to capture better the sentiment, and might benefit from the\n16See for instance the list of speeches of the European Central Bank on crypto-assets.\n14\nFigure 5. Correlation between ChatGPT and expert judgment: correlation between ChatGPT and human\nlabels for texts with more than 100 words.\npolarity histogram follows a nearly symmetrical distribution, whereas humans tend to\nfollow a distribution closer to a Chi-square. Guo et al. (2023) conducted comprehensive\nhuman evaluations and linguistic analyses of the content generated by ChatGPT in\ncomparison to that of humans, and it revealed interesting results. First, the authors\nfind that the proportion of neutral emotions is higher for both humans and ChatGPT,\nwhich is in line with our results in this study. However, ChatGPT generally expresses\nless negative feelings than humans. This is also the case in our study. Therefore,\na limitation of ChatGPT to analyze sentiment can occur when evaluating negative\npostures, since humans express more negative emotions than ChatGPT.\n4.3. Robustness analysis: Different prompts and larger texts\nIs ChatGPT capturing correctly the increasing sentiment of central banks towards\nCBDC, or it is by default more optimistic than BERT and Dictionary-based methods?\nDo our benchmark results hold for larger texts? In this Section we propose two analyses\nto test these questions.\n4.3.1. Robustness analysis. Prompts for crypto\nIn order to test whether ChatGPT is more optimistic than BERT or Polarity by\ndefault, we perform a robustness analysis in which we repeat the workflow described in\nFigure 1 but select paragraphs according to the following keywords: crypto(s), crypto\nasset(s), crypto-asset(s), Bitcoin. We compute the sentiment over those paragraphs,\nand we use as a prompt for ChatGPT:\n“Compute the sentiment score towards crypto assets, measured between -1 and 1, of\na given text. The response should be just a float number, no text. The text is as follows:\n[. . . ]”\n13\nFigure 8. Guo et al. (2023): histogram of the typical scores provided by humans, as shown in Guo et al.\n(2023).\nwith the one provided by experts, and we find that ChatGPT is closer to labeled data\nthan BERT or Dictionary-based methods. If we select larger texts, the advantage of\nChatGPT over the other techniques increase.\nTherefore, we want to point out that this study provides central banks with in-\nsights on how central bank communications might be perceived by selected audiences.\nHowever, the immense size of the available LLMs brings two new risk factors. First,\ninterpretability of the results is a challenge currently being under scrutiny of regula-\ntors, and a field where developers are working, aiming to identify which parts of an\nLLM are responsible for which of its behaviors. 17 Additionally, the use of LLMs also\nraises concerns about third-party dependencies and the potential electrical and envi-\nronmental cost of keeping these models online for everyone to access (Strubell et al.\n2019).\nFinally, we leave some indications for further research. While ChatGPT seems to\nbetter capture the sentiment towards CBDC than other NLP alternatives, we still\nneed to assess the importance of prompt engineering when defining the task for Chat-\nGPT, like changing its content, length, etc. In this article we have carried out several\nexperiments in this direction, with prompts that exclude CBDC from the input state-\nment (see Appendix section 7.3). On top of it, it could be worthwhile to extend our\nrobustness analysis on the use of LLM techniques to compute the sentiment on other\ncontexts, such as we did with crypto-assets (Section 5.3.1.). This way we could un-\n17See for instance the guidelines from OpenAI.\n16\nFigure 6. Cumulative sum ChatGPT vs expert judgment: comparing ChatGPT with human labels. The\norange line is the BIS sentiment (Auer et al. 2020) and the blue line is ChatGPT.\nThe results of the rolling window analysis for the three techniques can be seen in\nFigure 9. For crypto assets the sentiment does not display an upward trend behavior\nfor any of the techniques, as expected. 16 Again, Polarity is the more volatile, and al-\nthough on average is always below BERT and ChatGPT, there are moments in time in\nwhich sentiment with Polarity is higher than the one from the LLMs. Moreover, Chat-\nGPT (green) does not always indicate a higher sentiment that the other techniques.\nTherefore, the fact that ChatGPT captures more sentiment towards CBDC seems to\nbe genuine, and is not just a result of the fact that ChatGPT could be less negative\noverall (Figure 8).\n4.3.2. Robustness analysis. Larger texts\nAs we showed in Table 2, the average text size after selecting CBDC keywords is less\nthan 300 words. Technically we could fit all texts with less than 500 words directly\nto BERT and ChatGPT without restrictions. In this robustness check, instead of\nevaluating the paragraphs one by one, we merge them (in texts with more than 1\nparagraph) until we create big paragraphs of 500 words. The problem with this is that\nwe might put together paragraphs that do not fit necessarily (they could belong to\ndifferent parts of the texts). This does not matter for Polarity because it acts as a\nbag-of-words (the order of words does not matter) but might have implications for the\nLLMs. On the other hand, LLMs might benefit from having access to bigger texts. We\nshow the correlation between labeled data and the three techniques with larger texts\nin Table 4. It can be seen that in larger texts Polarity does not improve its results,\nwhile BERT and ChatGPT improve it, specially ChatGPT. This is another indicator\nthat ChatGPT is able to capture better the sentiment, and might benefit from the\n16See for instance the list of speeches of the European Central Bank on crypto-assets.\n14\nFigure 9. Sentiment scores towards crypto: results of the rolling window (moving average) analysis for the\nthree NLP techniques, applied to crypto analysis.\nderstand if ChatGPT is inherently more positive than BERT, or if dictionary-based\nmethods are always more volatile. Also, future research could extend the analysis to\nother LLM techniques, like GPT4, XLNet, LLaMA, or T5. Finally, since our analysis\nprovides a continuous measure of sentiment values, document by document, we could\nstudy which are the determinants of this sentiment index, and in particular, if there\nare factors that can explain why it fluctuates and why it increases. Are these sentiment\nvalues affected by changes in the crypto market? Or by the appearance of private digi-\ntal currency initiatives like Libra? These analyses could help policymakers and market\nparticipants to better assess the likelihood of CBDC issuance, which would facilitate\ntransparency for citizens about the current state of this complex debate.\nIn short, the use of these tools seems to have great potential in the field of sentiment\nanalysis in general, although we must weigh in the risks and analyze the advantages\nand limitations of its usage case by case.\n17\nFigure 8. Guo et al. (2023): histogram of the typical scores provided by humans, as shown in Guo et al.\n(2023).\nwith the one provided by experts, and we find that ChatGPT is closer to labeled data\nthan BERT or Dictionary-based methods. If we select larger texts, the advantage of\nChatGPT over the other techniques increase.\nTherefore, we want to point out that this study provides central banks with in-\nsights on how central bank communications might be perceived by selected audiences.\nHowever, the immense size of the available LLMs brings two new risk factors. First,\ninterpretability of the results is a challenge currently being under scrutiny of regula-\ntors, and a field where developers are working, aiming to identify which parts of an\nLLM are responsible for which of its behaviors. 17 Additionally, the use of LLMs also\nraises concerns about third-party dependencies and the potential electrical and envi-\nronmental cost of keeping these models online for everyone to access (Strubell et al.\n2019).\nFinally, we leave some indications for further research. While ChatGPT seems to\nbetter capture the sentiment towards CBDC than other NLP alternatives, we still\nneed to assess the importance of prompt engineering when defining the task for Chat-\nGPT, like changing its content, length, etc. In this article we have carried out several\nexperiments in this direction, with prompts that exclude CBDC from the input state-\nment (see Appendix section 7.3). On top of it, it could be worthwhile to extend our\nrobustness analysis on the use of LLM techniques to compute the sentiment on other\ncontexts, such as we did with crypto-assets (Section 5.3.1.). This way we could un-\n17See for instance the guidelines from OpenAI.\n16\nFigure 9. Sentiment scores towards crypto: results of the rolling window (moving average) analysis for the\nthree NLP techniques, applied to crypto analysis.\nderstand if ChatGPT is inherently more positive than BERT, or if dictionary-based\nmethods are always more volatile. Also, future research could extend the analysis to\nother LLM techniques, like GPT4, XLNet, LLaMA, or T5. Finally, since our analysis\nprovides a continuous measure of sentiment values, document by document, we could\nstudy which are the determinants of this sentiment index, and in particular, if there\nare factors that can explain why it fluctuates and why it increases. Are these sentiment\nvalues affected by changes in the crypto market? Or by the appearance of private digi-\ntal currency initiatives like Libra? These analyses could help policymakers and market\nparticipants to better assess the likelihood of CBDC issuance, which would facilitate\ntransparency for citizens about the current state of this complex debate.\nIn short, the use of these tools seems to have great potential in the field of sentiment\nanalysis in general, although we must weigh in the risks and analyze the advantages\nand limitations of its usage case by case.\n17\nBANCO DE ESPAÑA\n20\nDOCUMENTO DE TRABAJO N.º 2321\nFigure 7. Histogram three techniques with different techniques: histogram of the sentiments per document\nprovided by the three techniques\nTable 4. Correlation with expert judgment. Larger texts: correlation between labeled data and the three\nNLP techniques with larger texts.\nTexts of at least 500 words\nChatGPT 0.50 ***\nBert 0.38 ***\nDictionary 0.25 **\ninclusion of larger texts. It is worth mentioning that GPT4 would be able to read\ntexts of up to 32,000 tokens. However, at the time of writing, GPT4 was not accessible\nthrough the API.\n5. Conclusion and further work\nCurrently, central banks issue statements and opinions on topics that are not related\nto those traditionally associated with central bank communication (Hansson 2021).\nRecent studies suggest that these communications are important for financial markets,\neven when the topics discussed are not directly linked to monetary policy (Fortes\nand Le Guenedal 2020). One of these new topics is the discussion on the need and\ndesirability of a new type of money: central bank digital currencies (CBDCs).\nOur work investigates the sentiment of central banks’ communication about the\nissuance of digital money through CBDC. For the first time in the literature we use\nLLMs (BERT and ChatGPT) to compute a sentiment score of these speeches and\nreports, and we compare them to traditional dictionary-based techniques and human\nlabels. First, we find that the sentiment towards CBDC seems to be increasing from\n2017 onwards according to both LLMs and dictionary-based techniques, but the latter\nare more volatile. Second, we compare the sentiment obtained by the three techniques\n15\nFigure 7. Histogram three techniques with different techniques: histogram of the sentiments per document\nprovided by the three techniques\nTable 4. Correlation with expert judgment. Larger texts: correlation between labeled data and the three\nNLP techniques with larger texts.\nTexts of at least 500 words\nChatGPT 0.50 ***\nBert 0.38 ***\nDictionary 0.25 **\ninclusion of larger texts. It is worth mentioning that GPT4 would be able to read\ntexts of up to 32,000 tokens. However, at the time of writing, GPT4 was not accessible\nthrough the API.\n5. Conclusion and further work\nCurrently, central banks issue statements and opinions on topics that are not related\nto those traditionally associated with central bank communication (Hansson 2021).\nRecent studies suggest that these communications are important for financial markets,\neven when the topics discussed are not directly linked to monetary policy (Fortes\nand Le Guenedal 2020). One of these new topics is the discussion on the need and\ndesirability of a new type of money: central bank digital currencies (CBDCs).\nOur work investigates the sentiment of central banks’ communication about the\nissuance of digital money through CBDC. For the first time in the literature we use\nLLMs (BERT and ChatGPT) to compute a sentiment score of these speeches and\nreports, and we compare them to traditional dictionary-based techniques and human\nlabels. First, we find that the sentiment towards CBDC seems to be increasing from\n2017 onwards according to both LLMs and dictionary-based techniques, but the latter\nare more volatile. Second, we compare the sentiment obtained by the three techniques\n15\nFigure 8. Guo et al. (2023): histogram of the typical scores provided by humans, as shown in Guo et al.\n(2023).\nwith the one provided by experts, and we find that ChatGPT is closer to labeled data\nthan BERT or Dictionary-based methods. If we select larger texts, the advantage of\nChatGPT over the other techniques increase.\nTherefore, we want to point out that this study provides central banks with in-\nsights on how central bank communications might be perceived by selected audiences.\nHowever, the immense size of the available LLMs brings two new risk factors. First,\ninterpretability of the results is a challenge currently being under scrutiny of regula-\ntors, and a field where developers are working, aiming to identify which parts of an\nLLM are responsible for which of its behaviors. 17 Additionally, the use of LLMs also\nraises concerns about third-party dependencies and the potential electrical and envi-\nronmental cost of keeping these models online for everyone to access (Strubell et al.\n2019).\nFinally, we leave some indications for further research. While ChatGPT seems to\nbetter capture the sentiment towards CBDC than other NLP alternatives, we still\nneed to assess the importance of prompt engineering when defining the task for Chat-\nGPT, like changing its content, length, etc. In this article we have carried out several\nexperiments in this direction, with prompts that exclude CBDC from the input state-\nment (see Appendix section 7.3). On top of it, it could be worthwhile to extend our\nrobustness analysis on the use of LLM techniques to compute the sentiment on other\ncontexts, such as we did with crypto-assets (Section 5.3.1.). This way we could un-\n17See for instance the guidelines from OpenAI.\n16\nFigure 7. Histogram three techniques with different techniques: histogram of the sentiments per document\nprovided by the three techniques\nTable 4. Correlation with expert judgment. Larger texts: correlation between labeled data and the three\nNLP techniques with larger texts.\nTexts of at least 500 words\nChatGPT 0.50 ***\nBert 0.38 ***\nDictionary 0.25 **\ninclusion of larger texts. It is worth mentioning that GPT4 would be able to read\ntexts of up to 32,000 tokens. However, at the time of writing, GPT4 was not accessible\nthrough the API.\n5. Conclusion and further work\nCurrently, central banks issue statements and opinions on topics that are not related\nto those traditionally associated with central bank communication (Hansson 2021).\nRecent studies suggest that these communications are important for financial markets,\neven when the topics discussed are not directly linked to monetary policy (Fortes\nand Le Guenedal 2020). One of these new topics is the discussion on the need and\ndesirability of a new type of money: central bank digital currencies (CBDCs).\nOur work investigates the sentiment of central banks’ communication about the\nissuance of digital money through CBDC. For the first time in the literature we use\nLLMs (BERT and ChatGPT) to compute a sentiment score of these speeches and\nreports, and we compare them to traditional dictionary-based techniques and human\nlabels. First, we find that the sentiment towards CBDC seems to be increasing from\n2017 onwards according to both LLMs and dictionary-based techniques, but the latter\nare more volatile. Second, we compare the sentiment obtained by the three techniques\n15\nFigure 6. Cumulative sum ChatGPT vs expert judgment: comparing ChatGPT with human labels. The\norange line is the BIS sentiment (Auer et al. 2020) and the blue line is ChatGPT.\nThe results of the rolling window analysis for the three techniques can be seen in\nFigure 9. For crypto assets the sentiment does not display an upward trend behavior\nfor any of the techniques, as expected. 16 Again, Polarity is the more volatile, and al-\nthough on average is always below BERT and ChatGPT, there are moments in time in\nwhich sentiment with Polarity is higher than the one from the LLMs. Moreover, Chat-\nGPT (green) does not always indicate a higher sentiment that the other techniques.\nTherefore, the fact that ChatGPT captures more sentiment towards CBDC seems to\nbe genuine, and is not just a result of the fact that ChatGPT could be less negative\noverall (Figure 8).\n4.3.2. Robustness analysis. Larger texts\nAs we showed in Table 2, the average text size after selecting CBDC keywords is less\nthan 300 words. Technically we could fit all texts with less than 500 words directly\nto BERT and ChatGPT without restrictions. In this robustness check, instead of\nevaluating the paragraphs one by one, we merge them (in texts with more than 1\nparagraph) until we create big paragraphs of 500 words. The problem with this is that\nwe might put together paragraphs that do not fit necessarily (they could belong to\ndifferent parts of the texts). This does not matter for Polarity because it acts as a\nbag-of-words (the order of words does not matter) but might have implications for the\nLLMs. On the other hand, LLMs might benefit from having access to bigger texts. We\nshow the correlation between labeled data and the three techniques with larger texts\nin Table 4. It can be seen that in larger texts Polarity does not improve its results,\nwhile BERT and ChatGPT improve it, specially ChatGPT. This is another indicator\nthat ChatGPT is able to capture better the sentiment, and might benefit from the\n16See for instance the list of speeches of the European Central Bank on crypto-assets.\n14\nFigure 7. Histogram three techniques with different techniques: histogram of the sentiments per document\nprovided by the three techniques\nTable 4. Correlation with expert judgment. Larger texts: correlation between labeled data and the three\nNLP techniques with larger texts.\nTexts of at least 500 words\nChatGPT 0.50 ***\nBert 0.38 ***\nDictionary 0.25 **\ninclusion of larger texts. It is worth mentioning that GPT4 would be able to read\ntexts of up to 32,000 tokens. However, at the time of writing, GPT4 was not accessible\nthrough the API.\n5. Conclusion and further work\nCurrently, central banks issue statements and opinions on topics that are not related\nto those traditionally associated with central bank communication (Hansson 2021).\nRecent studies suggest that these communications are important for financial markets,\neven when the topics discussed are not directly linked to monetary policy (Fortes\nand Le Guenedal 2020). One of these new topics is the discussion on the need and\ndesirability of a new type of money: central bank digital currencies (CBDCs).\nOur work investigates the sentiment of central banks’ communication about the\nissuance of digital money through CBDC. For the first time in the literature we use\nLLMs (BERT and ChatGPT) to compute a sentiment score of these speeches and\nreports, and we compare them to traditional dictionary-based techniques and human\nlabels. First, we find that the sentiment towards CBDC seems to be increasing from\n2017 onwards according to both LLMs and dictionary-based techniques, but the latter\nare more volatile. Second, we compare the sentiment obtained by the three techniques\n15\nBANCO DE ESPAÑA\n21\nDOCUMENTO DE TRABAJO N.º 2321\nFigure 8. Guo et al. (2023): histogram of the typical scores provided by humans, as shown in Guo et al.\n(2023).\nwith the one provided by experts, and we find that ChatGPT is closer to labeled data\nthan BERT or Dictionary-based methods. If we select larger texts, the advantage of\nChatGPT over the other techniques increase.\nTherefore, we want to point out that this study provides central banks with in-\nsights on how central bank communications might be perceived by selected audiences.\nHowever, the immense size of the available LLMs brings two new risk factors. First,\ninterpretability of the results is a challenge currently being under scrutiny of regula-\ntors, and a field where developers are working, aiming to identify which parts of an\nLLM are responsible for which of its behaviors. 17 Additionally, the use of LLMs also\nraises concerns about third-party dependencies and the potential electrical and envi-\nronmental cost of keeping these models online for everyone to access (Strubell et al.\n2019).\nFinally, we leave some indications for further research. While ChatGPT seems to\nbetter capture the sentiment towards CBDC than other NLP alternatives, we still\nneed to assess the importance of prompt engineering when defining the task for Chat-\nGPT, like changing its content, length, etc. In this article we have carried out several\nexperiments in this direction, with prompts that exclude CBDC from the input state-\nment (see Appendix section 7.3). On top of it, it could be worthwhile to extend our\nrobustness analysis on the use of LLM techniques to compute the sentiment on other\ncontexts, such as we did with crypto-assets (Section 5.3.1.). This way we could un-\n17See for instance the guidelines from OpenAI.\n16\nFigure 8. Guo et al. (2023): histogram of the typical scores provided by humans, as shown in Guo et al.\n(2023).\nwith the one provided by experts, and we find that ChatGPT is closer to labeled data\nthan BERT or Dictionary-based methods. If we select larger texts, the advantage of\nChatGPT over the other techniques increase.\nTherefore, we want to point out that this study provides central banks with in-\nsights on how central bank communications might be perceived by selected audiences.\nHowever, the immense size of the available LLMs brings two new risk factors. First,\ninterpretability of the results is a challenge currently being under scrutiny of regula-\ntors, and a field where developers are working, aiming to identify which parts of an\nLLM are responsible for which of its behaviors. 17 Additionally, the use of LLMs also\nraises concerns about third-party dependencies and the potential electrical and envi-\nronmental cost of keeping these models online for everyone to access (Strubell et al.\n2019).\nFinally, we leave some indications for further research. While ChatGPT seems to\nbetter capture the sentiment towards CBDC than other NLP alternatives, we still\nneed to assess the importance of prompt engineering when defining the task for Chat-\nGPT, like changing its content, length, etc. In this article we have carried out several\nexperiments in this direction, with prompts that exclude CBDC from the input state-\nment (see Appendix section 7.3). On top of it, it could be worthwhile to extend our\nrobustness analysis on the use of LLM techniques to compute the sentiment on other\ncontexts, such as we did with crypto-assets (Section 5.3.1.). This way we could un-\n17See for instance the guidelines from OpenAI.\n16\nFigure 9. Sentiment scores towards crypto: results of the rolling window (moving average) analysis for the\nthree NLP techniques, applied to crypto analysis.\nderstand if ChatGPT is inherently more positive than BERT, or if dictionary-based\nmethods are always more volatile. Also, future research could extend the analysis to\nother LLM techniques, like GPT4, XLNet, LLaMA, or T5. Finally, since our analysis\nprovides a continuous measure of sentiment values, document by document, we could\nstudy which are the determinants of this sentiment index, and in particular, if there\nare factors that can explain why it fluctuates and why it increases. Are these sentiment\nvalues affected by changes in the crypto market? Or by the appearance of private digi-\ntal currency initiatives like Libra? These analyses could help policymakers and market\nparticipants to better assess the likelihood of CBDC issuance, which would facilitate\ntransparency for citizens about the current state of this complex debate.\nIn short, the use of these tools seems to have great potential in the field of sentiment\nanalysis in general, although we must weigh in the risks and analyze the advantages\nand limitations of its usage case by case.\n17\nBANCO DE ESPAÑA\n22\nDOCUMENTO DE TRABAJO N.º 2321\nReferences\n #Adrian, Tobias, and Tommaso Mancini-Griffoli. (2021). “The rise of digital money”. \nAnnual Review of Financial Economics,  13(1), pp. 57-77. https://doi.org/10.1146/annurev-\nfinancial-101620-063859 \n #Albrizio, Silvia, Juan Carlos Berganza e Iván Kataryniuk. (2017). “El seguro de desempleo \nfederal en Estados Unidos”. Boletín Económico - Banco de España,  2/2017, Artículos \nAnalíticos. https://repositorio.bde.es/handle/123456789/8263\n #Andolfatto, David. (2020). “ Assessing the Impact of Central Bank Digital Currency on Private \nBanks”. The Economic Journal, 131(634), pp. 525-540. https://doi.org/10.1093/ej/ueaa073 \n #Ash, Elliott, and Stephen Hansen. (2023). “Text algorithms in economics”. Unpublished \nmanuscript. \n #Auer, Raphael, Holti Banka, Nana Yaa Boakye-Adjei, Ahmed Faragallah, Jon Frost, Harish \nNatarajan and Jermy Prenio. (2022a). “Central bank digital currencies: a new tool in \nthe financial inclusion toolkit?”. FSI Insights on Policy Implementation, 41, Bank for \nInternational Settlements. https://www.bis.org/fsi/publ/insights41.pdf\n #Auer, Raphael, and Rainer Boehme. (2020). “The technology of retail central bank digital \ncurrency”. BIS Quarterly Review. https://EconPapers.repec.org/RePEc:bis:bisqtr:2003j \n #Auer, Raphael, Giulio Cornelli and Jon Frost. (2020). “Rise of the central bank digital currencies: \ndrivers, approaches and technologies”. BIS Working Papers, 880, Bank for International \nSettlements. https://www.bis.org/publ/work880.pdf\n #Auer, Raphael, Jon Frost, Leonardo Gambacorta, Cyril Monnet, Tara Rice and Hyun Song \nShin. (2022b). “Central bank digital currencies: Motives, economic implications, and the \nresearch frontier”. Annual Review of Economics, 14(1), pp. 697-721. https://doi.org/10.1146/\nannurev-economics-051420-020324 \n #Barontini, Christian, and Henry Holden. (2019). “Proceeding with caution - a survey on \ncentral bank digital currency”. BIS Papers, 101, Bank for International Settlements. https://\nwww.bis.org/publ/bppdf/bispap101.pdf\n #Barrdear, John, and Michael Kumhof. (2016). “The Macroeconomics of Central Bank \nIssued Digital Currencies”. Staff Working Papers, 605, Bank of England. https://www.\nbankofengland.co.uk/-/media/boe/files/working-paper/2016/the-macroeconomics-of-\ncentral-bank-issued-digital-currencies.pdf\n #Bholat, David, Nida Broughton, Janna Ter Meer and Eryk Walczak. (2019). “Enhancing \ncentral bank communications using simple and relatable information”. Journal of \nMonetary Economics,  108(C), pp. 1-15. https://EconPapers.repec.org/RePEc:eee:moneco: \nv:108:y:2019:i:c:p:1-15 \n #Blinder, Alan S. (2018). “Through a crystal ball darkly: The future of monetary policy \ncommunication”. AEA Papers and Proceedings, 108, pp. 567-571. https://www.aeaweb.org/\narticles?id=10.1257/pandp.20181080\n #Blinder, Alan S., Michael Ehrmann, Marcel Fratzscher, Jakob De Haan and David-Jan \nJansen. (2008). “Central bank communication and monetary policy: A survey of theory \nand evidence”. Journal of Economic Literature,  46(4), pp. 910-945. https://doi.org/10.1257/\njel.46.4.910 \n #Born, Benjamin, Michael Ehrmann and Marcel Fratzscher. (2014). “Central bank communication \non financial stability”. Economic Journal, 124(577), pp. 701-734. https://EconPapers.repec.\norg/RePEc:wly:econjl:v:124:y:2014:i:577:p:701-734 \n #Bubeck, Sébastien, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece \nKamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, \nMarco Tulio Ribeiro and Yi Zhang. (2023). “Sparks of Artificial General Intelligence: Early \nexperiments with GPT-4”. https://arxiv.org/abs/2303.12712 \nBANCO DE ESPAÑA\n23\nDOCUMENTO DE TRABAJO N.º 2321\n #Burlon, Lorenzo, Carlos Montes-Galdón, Manuel A. Muñoz and Frank Smets. (2022). “The \noptimal quantity of CBDC in a bank-based economy”. Working Paper Series, 2689, \nEuropean Central Bank. https://ideas.repec.org/p/ecb/ecbwps/20222689.html \n #Campbell, Jeffrey R., Filippo Ferroni, Jonas D. M. Fisher and Leonardo Melosi. (2019). “The \nlimits of forward guidance”. Journal of Monetary Economics,  108(C), pp. 118-134. https://\ndoi.org/10.1016/j.jmoneco.2019.08.009\n #Cole, Stephen J. (2021). “Learning and the Effectiveness of Central Bank Forward Guidance”. \nJournal of Money, Credit and Banking, 53(1), pp. 157-200. https://doi.org/10.1111/jmcb.12696 \n #Conlon, Thomas, Shaen Corbet, Greg Hou, Yang Hu, Charles James Larkin and Les Oxley. \n(2022). “To CBDC or Not to CBDC. That Is the Question.” August 31, 2022. http://dx.doi.\norg/10.2139/ssrn.4205402\n #Davoodalhosseini, Seyed Mohammadreza. (2022). “Central bank digital currency and \nmonetary policy”. Journal of Economic Dynamics and Control,  142(C). https://doi.\norg/10.1016/j.jedc.2021.104150\n #Devlin, Jacob, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. (2019). “BERT: Pre-\ntraining of Deep Bidirectional Transformers for Language Understanding”. https://arxiv.\norg/abs/1810.04805\n #Dowling, Michael, and Brian Lucey. (2023). “ChatGPT for (Finance) research: The Bananarama \nConjecture”. Finance Research Letters, 53(C). https://doi.org/10.1016/j.frl.2023.103662 \n #Fernández-Villaverde, Jesús, Daniel Sanches, Linda Schilling and Harald Uhlig. (2021). \n“Central Bank Digital Currency: Central Banking For All?”. Review of Economic Dynamics, \n41, pp. 225-242. https://doi.org/10.1016/j.red.2020.12.004 \n #Ferrari Minesso, Massimo, Arnaud Mehl and Livio Stracca. (2022). “Central bank digital \ncurrency in an open economy”. Journal of Monetary Economics, 127(C), pp. 54-68. https://\ndoi.org/10.1016/j.jmoneco.2022.02.001\n #Fortes, Roberta, and Theo Le Guenedal. (2020). “Tracking ECB’s communication: Perspectives \nand implications for financial markets”. MPRA paper, University Library of Munich, \nGermany. https://EconPapers.repec.org/RePEc:pra:mprapa:108746 \n #Gentzkow, Matthew, Bryan Kelly and Matt Taddy. (2019). “Text as data”. Journal of Economic \nLiterature, 57(3), pp. 535-74. https://doi.org/10.1257/jel.20181020 \n #Goldsmith-Pinkham, Paul, Beverly Hirtle and David O. Lucca. (2016). “Parsing the content of \nbank supervision”. Staff Reports, 770, Federal Reserve Bank of New York. https://ideas.\nrepec.org/p/fip/fednsr/770.html \n #Gorjón Rivas, Sergio. (2022). “Mercados financieros mayoristas y divisas digitales: avanzando \nen la tokeneización del dinero de banco central”. Revista de Estabilidad Financiera - Banco de \nEspaña, 42, pp. 91-105. https://repositorio.bde.es/handle/123456789/21560 \n #Guo, Biyang, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue \nand Yupeng Wu. (2023). “How Close is ChatGPT to Human Experts? Comparison Corpus, \nEvaluation, and Detection”. https://arxiv.org/abs/2301.07597\n #Guthrie, Graeme, and Julian Wright. (2000). “Open mouth operations”. Journal of Monetary \nEconomics, 46(2), pp. 489-516. https://EconPapers.repec.org/RePEc:eee:moneco:v:46:y: \n2000:i:2:p:489-516 \n #Haldane, Andrew, and Michael McMahon. (2018). “Central bank communications and the \ngeneral public”. AEA Papers and Proceedings,  108, pp. 578-583. https://doi.org/10.1257/\npandp.20181082 \n #Hansen, Anne Lundgaard, and Sophia Kazinnik. (2023). “Can chatGPT decipher fedspeak?”. \nhttp://dx.doi.org/10.2139/ssrn.4399406\nBANCO DE ESPAÑA\n24\nDOCUMENTO DE TRABAJO N.º 2321\n #Hansen, Stephen, Michael McMahon and Andrea Prat. (2018). “Transparency \nand deliberation within the fomc: A computational linguistics approach”. The \nQuarterly Journal of Economics,  133(2), pp. 801-870. https://EconPapers.repec.org/\nRePEc:oup:qjecon:v:133:y:2018:i:2:p:801-870 \n #Hansen, Stephen, Michael McMahon and Matthew Tong. (2019). “The long-run information effect \nof central bank communication”. Journal of Monetary Economics, 108(C), pp. 185-202. https://\ndoi.org/10.1016/j.jmoneco.2019.09.002\n #Hansson, Magnus. (2021). “Evolution of topics in central bank speech communication”. \nhttps://arxiv.org/abs/2109.10058\n #Haque, Mubin Ul, Isuru Dharmadasa, Zarrin Tasnim Sworna, Roshan Namal Rajapakse \nand Hussain Ahmad. (2022). “‘I think this is the most disruptive technology’: Exploring \nSentiments of ChatGPT Early Adopters using Twitter Data”. https://arxiv.org/\nabs/2212.05856\n #Hayo, Bernd, and Johannes Zahner. (2023). “What is that noise? Analysing sentiment-\nbased variation in central bank communication”. Economics Letters,  222(C). https://doi.\norg/10.1016/j.econlet.2022.110962\n #Kiff, John, Jihad Alwazir, Sonja Davidovic, Aquiles Farias, Ashraf Khan, Tanai Khiaonarong, \nMajid Malaika, Hunter Monroe, Nobu Sugimoto, Hervé Tourpe and Peter Zhou (2020). \n“ A survey of research on retail central bank digital currency”. IMF Working Papers, \n2020/104, International Monetary Fund. http://dx.doi.org/10.2139/ssrn.3639760\n #Li, Jiaqi. (2023). “Predicting the demand for central bank digital currency: A structural \nanalysis with survey data”. Journal of Monetary Economics,  134(C), pp. 73-85. https://doi.\norg/10.1016/j.jmoneco.2022.11.007\n #Loughran, Tim, and Bill McDonald. (2011). “When is a liability not a liability? Textual \nAnalysis, Dictionaries, and 10-Ks”. Journal of Finance, 66(1), pp. 35-65. https://EconPapers.\nrepec.org/RePEc:bla:jfinan:v:66:y:2011:i:1:p:35-65 \n #McKay, Alisdair, Emi Nakamura and Jón Steinsson. (2016). “The power of forward guidance \nrevisited”. American Economic Review, 106(10), pp. 3133-3158. https://doi.org/10.1257/\naer.20150063 \n #Romero Ugarte, José Luis, Abel Sánchez Martín, Carlos Martín Rodríguez and Justo Arenillas \nCristóbal. (2021). “Implicaciones de una moneda digital soberana mayorista apoyada en \ntecnología de registros distribuidos para las infraestructuras del mercado financiero”. \nRevista de Estabilidad Financiera - Banco de España,  40, pp. 161-178. https://repositorio.bde.\nes/handle/123456789/16737 \n #Scharnowski, Stefan. (2022). “Central bank speeches and digital currency competition”. \nFinance Research Letters, 49(C). https://doi.org/10.1016/j.frl.2022.103072 \n #Siklos, Pierre, Samantha St Amand and Joanna Wajda. (2018). “The Evolving Scope \nand Content of Central Bank Speeches”. CIGI Papers, 202, Centre for International \nGovernance Innovation. https://www.cigionline.org/static/documents/documents/Paper%20\nNo.202web_0.pdf\n #Strubell, Emma, Ananya Ganesh and Andrew McCallum. (2019). “Energy and Policy \nConsiderations for Deep Learning in NLP”. In Proceedings of the 57th Annual Meeting of \nthe Association for Computational Linguistics. Association for Computational Linguistics,  \npp. 3645-3650. https://doi.org/10.18653/v1/P19-1355\n #Tarlin, Solomon H. (2021). “The Future of Cash”. Discussion Paper, 21-03, Federal Reserve \nBank of Philadelphia. https://doi.org/10.21799/frbp.dp.2021.03 \n #Tian, Shu, Bo Zhao and Resi Ong Olivares. (2023). “Cybersecurity risks and central banks’ \nsentiment on central bank digital currency: Evidence from global cyberattacks”. Finance \nResearch Letters, 53(C). https://doi.org/10.1016/j.frl.2022.103609 \nBANCO DE ESPAÑA\n25\nDOCUMENTO DE TRABAJO N.º 2321\n #Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \nLukasz Kaiser and Illia Polosukhin. (2017). “ Attention is all you need”. In Guyon, I., \nU. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan and R. Garnett (eds.), \nAdvances in Neural Information Processing Systems,  30. Curran Associates, Inc. https://\nproceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-\nPaper.pdf \n #Wang, Yizhi, Brian M. Lucey, Samuel A. Vigne and Larisa Yarovaya. (2022). “The Effects of \nCentral Bank Digital Currencies News on Financial Markets”. Technological Forecasting and \nSocial Change, 180(C). https://doi.org/10.1016/j.techfore.2022.121715\n #Yang, Yi, Mark Christopher Siy UY and Allen Huang. (2020). “FinBERT: A Pretrained \nLanguage Model for Financial Communications”. https://arxiv.org/abs/2006.08097\n #Wang, Yizhi, Brian M. Lucey, Samuel A. Vigne and Larisa Yarovaya. (2022). “The Effects of \nCentral Bank Digital Currencies News on Financial Markets”. Technological Forecasting and \nSocial Change, 180(C). https://doi.org/10.1016/j.techfore.2022.1\n #Yang, Yi, Mark Christopher Siy UY and Allen Huang. (2020). “FinBERT: A Pretrained \nLanguage Model for Financial Communications“. https://arxiv.org/abs/2006.08097\nBANCO DE ESPAÑA\n26\nDOCUMENTO DE TRABAJO N.º 2321\n6. Appendix\n6.1. Cross correlation between ChatGPT, BERT, and Dictionary\nIn this section we are going to explore how the results of ChatGPT are similar to\nthose of BERT and the dictionary. In Section 5.1 we showed that they all show an\nupward trend in sentiment about CBDC. In Table 5 we show the cross correlation\nbetween them. It can be seen that, as expected, BERT and ChatGPT have closer\ncorrelation structure. The one that looks the most like the dictionary is BERT. All\nthese correlations are statistically significant. In Figure 10 we plot the correlation\nbetween ChatGPT and BERT. Each dot represents a document.\nTable 5. Correlation sentiment between the three techniques: cross correlation between all three NLP tech-\nniques.\nChatGPT BERT Dictionary-based\nChatGPT 1 x x\nBERT 0.52*** 1 x\nDictionary-based 0.29*** 0.32*** 1\nFigure 10. Correlation between ChatGPT and BERT: correlation between ChatGPT and BERT. Each dot\nrepresents a document.\nIt can be clearly seen that higher (lower) ChatGPT scores are related to higher\n(lower) BERT scores. Even so, we can see that the main source of disagreement between\nBERT and ChatGPT are several observations in which BERT catalogs the stance\nas neutral, but ChatGPT captures it as positive. That was expected. After all, as\nmentioned above, ChatGPT tends to be more positive than BERT when we ask about\nCBDC. Who is more right in these disagreements? These disagreements are due to the\nfact that ChatGPT can grasp subtler behaviors. In the next section we look at some\nexamples.\n6.2. Examples of disagreements\nIn this section we show three examples. The first two are ha a clearly positive tone\ntowards CBDC, but only ChatGPT captures it correctly, BERT mistakenly stays\n21\nneutral, and dictionary marks negative. In Example 1 there are clear indications that\nthe tone is positive towards CBDC. The author says that “the aim would be to provide\na kind of digital banknote, which would combine the security of banknotes issued by the\ncentral bank with the ease of use and innovation of dematerialised means of payment”,\nand says that they are working on it. This suggests that the tone is positive, but BERT\nmarks it as neutral, the dictionary-based method (polarity) marks it as negative, which\nis clearly wrong. The same dynamics can be seen in Example 2.\nExample 1: Fran¸ cois Villeroy de Galhau: New Year wishes to the Paris financial\nCentre (2022-01-01)\n“A forward looking euro means new banknotes after 2024, with a new design, and\neven more secure there are now only 17 counterfeits per million banknotes, making\nthe euro one of the most secure currencies in the world. It is also a digital euro. The\ninvestigation phase for a “digital retail euro” was launched on 1 October last year,\nfor a period of two years. The aim would be to provide a kind of “digital banknote”,\nwhich would combine the security of banknotes issued by the central bank with the ease\nof use and innovation of dematerialised means of payment. The ECB and the Banque\nde France are working on all the associated challenges, and we have already conducted\n9 experiments in Paris on an “interbank” digital currency. They will be continued in\n2022, focusing on improving cross border payments”.\n• ChatGPT (justify your answer): 0.8 The text discusses the possibility of a digital\neuro, and the overall tone is positive. The text mentions the benefits of a digital\neuro, such as increased security and ease of use, and the ECB is conducting\nexperiments to improve the system. There is no mention of negative consequences\nof a digital euro, so the sentiment score is positive\n• BERT: This is the score: 0.07\n• Polarity: This is the score: -0.44\nExample 2:Mario Marcel: Chile’s September 2020 Monetary Policy report (2020-\n10-01)\n“Meanwhile, we have been closely monitoring all the developments associated with\ndigital payment methods. As part of this follow-up, our participation in research groups\nof multilateral institutions has been active, especially on digital money issued by central\nbanks. The digital issuance of central bank money is gaining more space in the dis-\ncussion agenda at different international organizations, and the repercussions of the\ncurrent sanitary crisis are accelerating this debate even more. Today, there are several\nserious projects being explored by central banks and international organizations on the\nsubject. We believe that, in spite of both global and idiosyncratic contingencies, it is\nopportune to deepen our understanding and exploration of this type of instruments and\nrelated technologies”.\n• ChatGPT (justify your answer): 0.7. The sentiment score towards central bank\ndigital currencies in this text is positive. The author is discussing the develop-\nments associated with digital payment methods and the central bank’s participa-\ntion in research groups exploring digital money. The author believes that central\nbank digital currencies are a serious topic worthy of exploration. This indicates\na positive sentiment towards central bank digital currencies.\n• BERT: This is the score: 0.1\n• Polarity: This is the score: -0.5\nExample 3: Burkhard Balz (Germany): Current challenges in an uncertain world\n22\n6. Appendix\n6.1. Cross correlation between ChatGPT, BERT, and Dictionary\nIn this section we are going to explore how the results of ChatGPT are similar to\nthose of BERT and the dictionary. In Section 5.1 we showed that they all show an\nupward trend in sentiment about CBDC. In Table 5 we show the cross correlation\nbetween them. It can be seen that, as expected, BERT and ChatGPT have closer\ncorrelation structure. The one that looks the most like the dictionary is BERT. All\nthese correlations are statistically significant. In Figure 10 we plot the correlation\nbetween ChatGPT and BERT. Each dot represents a document.\nTable 5. Correlation sentiment between the three techniques: cross correlation between all three NLP tech-\nniques.\nChatGPT BERT Dictionary-based\nChatGPT 1 x x\nBERT 0.52*** 1 x\nDictionary-based 0.29*** 0.32*** 1\nFigure 10. Correlation between ChatGPT and BERT: correlation between ChatGPT and BERT. Each dot\nrepresents a document.\nIt can be clearly seen that higher (lower) ChatGPT scores are related to higher\n(lower) BERT scores. Even so, we can see that the main source of disagreement between\nBERT and ChatGPT are several observations in which BERT catalogs the stance\nas neutral, but ChatGPT captures it as positive. That was expected. After all, as\nmentioned above, ChatGPT tends to be more positive than BERT when we ask about\nCBDC. Who is more right in these disagreements? These disagreements are due to the\nfact that ChatGPT can grasp subtler behaviors. In the next section we look at some\nexamples.\n6.2. Examples of disagreements\nIn this section we show three examples. The first two are ha a clearly positive tone\ntowards CBDC, but only ChatGPT captures it correctly, BERT mistakenly stays\n21\n6. Appendix\n6.1. Cross correlation between ChatGPT, BERT, and Dictionary\nIn this section we are going to explore how the results of ChatGPT are similar to\nthose of BERT and the dictionary. In Section 5.1 we showed that they all show an\nupward trend in sentiment about CBDC. In Table 5 we show the cross correlation\nbetween them. It can be seen that, as expected, BERT and ChatGPT have closer\ncorrelation structure. The one that looks the most like the dictionary is BERT. All\nthese correlations are statistically significant. In Figure 10 we plot the correlation\nbetween ChatGPT and BERT. Each dot represents a document.\nTable 5. Correlation sentiment between the three techniques: cross correlation between all three NLP tech-\nniques.\nChatGPT BERT Dictionary-based\nChatGPT 1 x x\nBERT 0.52*** 1 x\nDictionary-based 0.29*** 0.32*** 1\nFigure 10. Correlation between ChatGPT and BERT: correlation between ChatGPT and BERT. Each dot\nrepresents a document.\nIt can be clearly seen that higher (lower) ChatGPT scores are related to higher\n(lower) BERT scores. Even so, we can see that the main source of disagreement between\nBERT and ChatGPT are several observations in which BERT catalogs the stance\nas neutral, but ChatGPT captures it as positive. That was expected. After all, as\nmentioned above, ChatGPT tends to be more positive than BERT when we ask about\nCBDC. Who is more right in these disagreements? These disagreements are due to the\nfact that ChatGPT can grasp subtler behaviors. In the next section we look at some\nexamples.\n6.2. Examples of disagreements\nIn this section we show three examples. The first two are ha a clearly positive tone\ntowards CBDC, but only ChatGPT captures it correctly, BERT mistakenly stays\n21\n6. Appendix\n6.1. Cross correlation between ChatGPT, BERT, and Dictionary\nIn this section we are going to explore how the results of ChatGPT are similar to\nthose of BERT and the dictionary. In Section 5.1 we showed that they all show an\nupward trend in sentiment about CBDC. In Table 5 we show the cross correlation\nbetween them. It can be seen that, as expected, BERT and ChatGPT have closer\ncorrelation structure. The one that looks the most like the dictionary is BERT. All\nthese correlations are statistically significant. In Figure 10 we plot the correlation\nbetween ChatGPT and BERT. Each dot represents a document.\nTable 5. Correlation sentiment between the three techniques: cross correlation between all three NLP tech-\nniques.\nChatGPT BERT Dictionary-based\nChatGPT 1 x x\nBERT 0.52*** 1 x\nDictionary-based 0.29*** 0.32*** 1\nFigure 10. Correlation between ChatGPT and BERT: correlation between ChatGPT and BERT. Each dot\nrepresents a document.\nIt can be clearly seen that higher (lower) ChatGPT scores are related to higher\n(lower) BERT scores. Even so, we can see that the main source of disagreement between\nBERT and ChatGPT are several observations in which BERT catalogs the stance\nas neutral, but ChatGPT captures it as positive. That was expected. After all, as\nmentioned above, ChatGPT tends to be more positive than BERT when we ask about\nCBDC. Who is more right in these disagreements? These disagreements are due to the\nfact that ChatGPT can grasp subtler behaviors. In the next section we look at some\nexamples.\n6.2. Examples of disagreements\nIn this section we show three examples. The first two are ha a clearly positive tone\ntowards CBDC, but only ChatGPT captures it correctly, BERT mistakenly stays\n21\n6. Appendix\n6.1. Cross correlation between ChatGPT, BERT, and Dictionary\nIn this section we are going to explore how the results of ChatGPT are similar to\nthose of BERT and the dictionary. In Section 5.1 we showed that they all show an\nupward trend in sentiment about CBDC. In Table 5 we show the cross correlation\nbetween them. It can be seen that, as expected, BERT and ChatGPT have closer\ncorrelation structure. The one that looks the most like the dictionary is BERT. All\nthese correlations are statistically significant. In Figure 10 we plot the correlation\nbetween ChatGPT and BERT. Each dot represents a document.\nTable 5. Correlation sentiment between the three techniques: cross correlation between all three NLP tech-\nniques.\nChatGPT BERT Dictionary-based\nChatGPT 1 x x\nBERT 0.52*** 1 x\nDictionary-based 0.29*** 0.32*** 1\nFigure 10. Correlation between ChatGPT and BERT: correlation between ChatGPT and BERT. Each dot\nrepresents a document.\nIt can be clearly seen that higher (lower) ChatGPT scores are related to higher\n(lower) BERT scores. Even so, we can see that the main source of disagreement between\nBERT and ChatGPT are several observations in which BERT catalogs the stance\nas neutral, but ChatGPT captures it as positive. That was expected. After all, as\nmentioned above, ChatGPT tends to be more positive than BERT when we ask about\nCBDC. Who is more right in these disagreements? These disagreements are due to the\nfact that ChatGPT can grasp subtler behaviors. In the next section we look at some\nexamples.\n6.2. Examples of disagreements\nIn this section we show three examples. The first two are ha a clearly positive tone\ntowards CBDC, but only ChatGPT captures it correctly, BERT mistakenly stays\n21\nBANCO DE ESPAÑA\n27\nDOCUMENTO DE TRABAJO N.º 2321\n(2022-03-01)\n“Central banks, too, are maintaining intensive worldwide collaboration. We central\nbankers are facing similar challenges, be they finding the right monetary policy re-\nsponses to developments in the real economy and financial sector or the introduction\nof a central bank digital currency – a topic that has been greatly occupying me.”\n• ChatGPT (justify your answer): 0.8. The sentiment score of this text is positive,\nas the author is discussing the various challenges that central bankers are facing\nand how they are working together to overcome them. The author specifically\nmentions the introduction of a central bank digital currency, which is an issue\nthat has been occupying him, as a positive development. This shows that the\nauthor is in favor of central bank digital currencies and has a positive sentiment\ntowards them.\n• BERT: This is the score: 0.0\n• Polarity: This is the score: 0.3\n6.3. ChatGPT without asking specifically about CBDC\nFinally, we tested what happens if we ask ChatGPT the sentiment for phrases contain-\ning CBDC, without mentioning anything about CBDC in the prompt. The prompt\nwould just be: “What is the sentiment of the next sentence?” . In this way we can bet-\nter compare with BERT, since BERT does not allow to ask the feeling about specific\nconcepts. In Figure Figure 11 we show the result. In green we show the original Chat-\nGPT result (asking for CBDC), in orange and dotted we show the ChatGPT result\nwithout asking for CBDC, and in purple the original result from BERT (same as in\nSection 5). We can see that BERT and ChatGPT without CBDC are more similar to\neach other. This result supports that the greater positive effect found by ChatGPT\nwhen asked about CBDC is justified.\nFigure 11. Sentiment. Moving average. (ChatGPT without CBDC reference): sentiment score of ChatGPT\nand BERT without mentioning CBDC in the prompt.\n23\nneutral, and dictionary marks negative. In Example 1 there are clear indications that\nthe tone is positive towards CBDC. The author says that “the aim would be to provide\na kind of digital banknote, which would combine the security of banknotes issued by the\ncentral bank with the ease of use and innovation of dematerialised means of payment”,\nand says that they are working on it. This suggests that the tone is positive, but BERT\nmarks it as neutral, the dictionary-based method (polarity) marks it as negative, which\nis clearly wrong. The same dynamics can be seen in Example 2.\nExample 1: Fran¸ cois Villeroy de Galhau: New Year wishes to the Paris financial\nCentre (2022-01-01)\n“A forward looking euro means new banknotes after 2024, with a new design, and\neven more secure there are now only 17 counterfeits per million banknotes, making\nthe euro one of the most secure currencies in the world. It is also a digital euro. The\ninvestigation phase for a “digital retail euro” was launched on 1 October last year,\nfor a period of two years. The aim would be to provide a kind of “digital banknote”,\nwhich would combine the security of banknotes issued by the central bank with the ease\nof use and innovation of dematerialised means of payment. The ECB and the Banque\nde France are working on all the associated challenges, and we have already conducted\n9 experiments in Paris on an “interbank” digital currency. They will be continued in\n2022, focusing on improving cross border payments”.\n• ChatGPT (justify your answer): 0.8 The text discusses the possibility of a digital\neuro, and the overall tone is positive. The text mentions the benefits of a digital\neuro, such as increased security and ease of use, and the ECB is conducting\nexperiments to improve the system. There is no mention of negative consequences\nof a digital euro, so the sentiment score is positive\n• BERT: This is the score: 0.07\n• Polarity: This is the score: -0.44\nExample 2:Mario Marcel: Chile’s September 2020 Monetary Policy report (2020-\n10-01)\n“Meanwhile, we have been closely monitoring all the developments associated with\ndigital payment methods. As part of this follow-up, our participation in research groups\nof multilateral institutions has been active, especially on digital money issued by central\nbanks. The digital issuance of central bank money is gaining more space in the dis-\ncussion agenda at different international organizations, and the repercussions of the\ncurrent sanitary crisis are accelerating this debate even more. Today, there are several\nserious projects being explored by central banks and international organizations on the\nsubject. We believe that, in spite of both global and idiosyncratic contingencies, it is\nopportune to deepen our understanding and exploration of this type of instruments and\nrelated technologies”.\n• ChatGPT (justify your answer): 0.7. The sentiment score towards central bank\ndigital currencies in this text is positive. The author is discussing the develop-\nments associated with digital payment methods and the central bank’s participa-\ntion in research groups exploring digital money. The author believes that central\nbank digital currencies are a serious topic worthy of exploration. This indicates\na positive sentiment towards central bank digital currencies.\n• BERT: This is the score: 0.1\n• Polarity: This is the score: -0.5\nExample 3: Burkhard Balz (Germany): Current challenges in an uncertain world\n22\nBANCO DE ESPAÑA\n28\nDOCUMENTO DE TRABAJO N.º 2321\n(2022-03-01)\n“Central banks, too, are maintaining intensive worldwide collaboration. We central\nbankers are facing similar challenges, be they finding the right monetary policy re-\nsponses to developments in the real economy and financial sector or the introduction\nof a central bank digital currency – a topic that has been greatly occupying me.”\n• ChatGPT (justify your answer): 0.8. The sentiment score of this text is positive,\nas the author is discussing the various challenges that central bankers are facing\nand how they are working together to overcome them. The author specifically\nmentions the introduction of a central bank digital currency, which is an issue\nthat has been occupying him, as a positive development. This shows that the\nauthor is in favor of central bank digital currencies and has a positive sentiment\ntowards them.\n• BERT: This is the score: 0.0\n• Polarity: This is the score: 0.3\n6.3. ChatGPT without asking specifically about CBDC\nFinally, we tested what happens if we ask ChatGPT the sentiment for phrases contain-\ning CBDC, without mentioning anything about CBDC in the prompt. The prompt\nwould just be: “What is the sentiment of the next sentence?”. In this way we can bet-\nter compare with BERT, since BERT does not allow to ask the feeling about specific\nconcepts. In Figure Figure 11 we show the result. In green we show the original Chat-\nGPT result (asking for CBDC), in orange and dotted we show the ChatGPT result\nwithout asking for CBDC, and in purple the original result from BERT (same as in\nSection 5). We can see that BERT and ChatGPT without CBDC are more similar to\neach other. This result supports that the greater positive effect found by ChatGPT\nwhen asked about CBDC is justified.\nFigure 11. Sentiment. Moving average. (ChatGPT without CBDC reference): sentiment score of ChatGPT\nand BERT without mentioning CBDC in the prompt.\n23\n(2022-03-01)\n“Central banks, too, are maintaining intensive worldwide collaboration. We central\nbankers are facing similar challenges, be they finding the right monetary policy re-\nsponses to developments in the real economy and financial sector or the introduction\nof a central bank digital currency – a topic that has been greatly occupying me.”\n• ChatGPT (justify your answer): 0.8. The sentiment score of this text is positive,\nas the author is discussing the various challenges that central bankers are facing\nand how they are working together to overcome them. The author specifically\nmentions the introduction of a central bank digital currency, which is an issue\nthat has been occupying him, as a positive development. This shows that the\nauthor is in favor of central bank digital currencies and has a positive sentiment\ntowards them.\n• BERT: This is the score: 0.0\n• Polarity: This is the score: 0.3\n6.3. ChatGPT without asking specifically about CBDC\nFinally, we tested what happens if we ask ChatGPT the sentiment for phrases contain-\ning CBDC, without mentioning anything about CBDC in the prompt. The prompt\nwould just be: “What is the sentiment of the next sentence?” . In this way we can bet-\nter compare with BERT, since BERT does not allow to ask the feeling about specific\nconcepts. In Figure Figure 11 we show the result. In green we show the original Chat-\nGPT result (asking for CBDC), in orange and dotted we show the ChatGPT result\nwithout asking for CBDC, and in purple the original result from BERT (same as in\nSection 5). We can see that BERT and ChatGPT without CBDC are more similar to\neach other. This result supports that the greater positive effect found by ChatGPT\nwhen asked about CBDC is justified.\nFigure 11. Sentiment. Moving average. (ChatGPT without CBDC reference): sentiment score of ChatGPT\nand BERT without mentioning CBDC in the prompt.\n23\n(2022-03-01)\n“Central banks, too, are maintaining intensive worldwide collaboration. We central\nbankers are facing similar challenges, be they finding the right monetary policy re-\nsponses to developments in the real economy and financial sector or the introduction\nof a central bank digital currency – a topic that has been greatly occupying me.”\n• ChatGPT (justify your answer): 0.8. The sentiment score of this text is positive,\nas the author is discussing the various challenges that central bankers are facing\nand how they are working together to overcome them. The author specifically\nmentions the introduction of a central bank digital currency, which is an issue\nthat has been occupying him, as a positive development. This shows that the\nauthor is in favor of central bank digital currencies and has a positive sentiment\ntowards them.\n• BERT: This is the score: 0.0\n• Polarity: This is the score: 0.3\n6.3. ChatGPT without asking specifically about CBDC\nFinally, we tested what happens if we ask ChatGPT the sentiment for phrases contain-\ning CBDC, without mentioning anything about CBDC in the prompt. The prompt\nwould just be: “What is the sentiment of the next sentence?” . In this way we can bet-\nter compare with BERT, since BERT does not allow to ask the feeling about specific\nconcepts. In Figure Figure 11 we show the result. In green we show the original Chat-\nGPT result (asking for CBDC), in orange and dotted we show the ChatGPT result\nwithout asking for CBDC, and in purple the original result from BERT (same as in\nSection 5). We can see that BERT and ChatGPT without CBDC are more similar to\neach other. This result supports that the greater positive effect found by ChatGPT\nwhen asked about CBDC is justified.\nFigure 11. Sentiment. Moving average. (ChatGPT without CBDC reference): sentiment score of ChatGPT\nand BERT without mentioning CBDC in the prompt.\n23\nBANCO DE ESPAÑA PUBLICATIONS \nWORKING PAPERS \n2215  JOSÉ MANUEL CARBÓ and SERGIO GORJÓN: Application of machine learning models and interpretability techniques \nto identify the determinants of the price of bitcoin.\n2216  LUIS GUIROLA and MARÍA SÁNCHEZ-DOMÍNGUEZ: Childcare constraints on immigrant integration.\n2217  ADRIÁN CARRO, MARC HINTERSCHWEIGER, ARZU ULUC and J. DOYNE FARMER: Heterogeneous effects and \nspillovers of macroprudential policy in an agent-based model of the UK housing market.\n2218  STÉPHANE DUPRAZ, HERVÉ LE BIHAN and JULIEN MATHERON: Make-up strategies with finite planning horizons but \nforward-looking asset prices.\n2219  L AURA ÁLVAREZ, MIGUEL GARCÍA-POSADA and SERGIO MAYORDOMO: Distressed firms, zombie firms and zombie \nlending: a taxonomy.\n2220  BLANCA JIMÉNEZ-GARCÍA and JULIO RODRÍGUEZ: A quantification of the evolution of bilateral trade flows once \nbilateral RTAs are implemented.\n2221  SALOMÓN GARCÍA: Mortgage securitization and information frictions in general equilibrium.\n2222  ANDRÉS ALONSO and JOSÉ MANUEL CARBÓ: Accuracy of explanations of machine learning models for credit \ndecisions.\n2223  JAMES COSTAIN, GALO NUÑO and CARLOS THOMAS: The term structure of interest rates in a heterogeneous \nmonetary union.\n2224  ANTOINE BERTHEAU, EDOARDO MARIA ACABBI, CRISTINA BARCELÓ, ANDREAS GULYAS, STEFANO \nLOMBARDI and RAFFAELE SAGGIO: The Unequal Consequences of Job Loss across Countries.\n2225  ERWAN GAUTIER, CRISTINA CONFLITTI, RIEMER P . FABER, BRIAN FABO, LUDMILA FADEJEVA, VALENTIN \nJOUVANCEAU, JAN-OLIVER MENZ, TERESA MESSNER, PAVLOS PETROULAS, PAU ROLDAN-BLANCO, FABIO \nRUMLER, SERGIO SANTORO, ELISABETH WIELAND and HÉLÈNE ZIMMER. New facts on consumer price rigidity in \nthe euro area.\n2226  MARIO BAJO and EMILIO RODRÍGUEZ: Integrating the carbon footprint into the construction of corporate bond portfolios.\n2227  FEDERICO CARRIL-CACCIA, JORDI PANIAGUA and MARTA SUÁREZ-VARELA: Forced migration and food crises.\n2228  CARLOS MORENO PÉREZ and MARCO MINOZZO: Natural Language Processing and Financial Markets: \n Semi-supervised Modelling of Coronavirus and Economic News.\n2229  CARLOS MORENO PÉREZ and MARCO MINOZZO: Monetary Policy Uncertainty in Mexico: An Unsupervised Approach.\n2230  ADRIAN CARRO: Could Spain be less different? Exploring the effects of macroprudential policy on the house price cycle.\n2231  DANIEL SANTABÁRBARA and MARTA SUÁREZ-VARELA: Carbon pricing and inflation volatility.\n2232  MARINA DIAKONOVA, LUIS MOLINA, HANNES MUELLER, JAVIER J. PÉREZ and CRISTOPHER RAUH: The information \ncontent of conflict, social unrest and policy uncertainty measures for macroeconomic forecasting.\n2233  JULIAN DI GIOVANNI, MANUEL GARCÍA-SANTANA, PRIIT JEENAS, ENRIQUE MORAL-BENITO and JOSEP PIJOAN-MAS: \nGovernment Procurement and Access to Credit: Firm Dynamics and Aggregate Implications.\n2234  PETER PAZ: Bank capitalization heterogeneity and monetary policy.\n2235  ERIK ANDRES-ESCAYOLA, CORINNA GHIRELLI, LUIS MOLINA, JAVIER J. PÉREZ and ELENA VIDAL: Using newspapers \nfor textual indicators: which and how many?\n2236  MARÍA ALEJANDRA AMADO: Macroprudential FX regulations: sacrificing small firms for stability?\n2237  LUIS GUIROLA and GONZALO RIVERO: Polarization contaminates the link with partisan and independent institutions: \nevidence from 138 cabinet shifts.\n2238  MIGUEL DURO, GERMÁN LÓPEZ-ESPINOSA, SERGIO MAYORDOMO, GAIZKA ORMAZABAL and MARÍA \nRODRÍGUEZ-MORENO: Enforcing mandatory reporting on private firms: the role of banks.\n2239  LUIS J. ÁLVAREZ and FLORENS ODENDAHL: Data outliers and Bayesian VARs in the Euro Area.\n2240  CARLOS MORENO PÉREZ and MARCO MINOZZO: “Making text talk”: The minutes of the Central Bank of Brazil and \nthe real economy.\n2241  JULIO GÁLVEZ and GONZALO PAZ-PARDO: Richer earnings dynamics, consumption and portfolio choice over the life cycle.\n2242  MARINA DIAKONOVA, CORINNA GHIRELLI, LUIS MOLINA and JAVIER J. PÉREZ: The economic impact of conflict-related \nand policy uncertainty shocks: the case of Russia.\n2243  CARMEN BROTO, LUIS FERNÁNDEZ LAFUERZA and MARIYA MELNYCHUK: Do buffer requirements for European \nsystemically important banks make them less systemic?\n2244  GERGELY GANICS and MARÍA RODRÍGUEZ-MORENO: A house price-at-risk model to monitor the downside risk for the \nSpanish housing market.\n2245  JOSÉ E. GUTIÉRREZ and LUIS FERNÁNDEZ LAFUERZA: Credit line runs and bank risk management: evidence from the \ndisclosure of stress test results.\n2301  MARÍA BRU MUÑOZ: The forgotten lender: the role of multilateral lenders in sovereign debt and default.\n2302  SILVIA ALBRIZIO, BEATRIZ GONZÁLEZ and DMITRY KHAMETSHIN: A tale of two margins: monetary policy and capital \nmisallocation.\n2303  JUAN EQUIZA, RICARDO GIMENO, ANTONIO MORENO and CARLOS THOMAS: Evaluating central bank asset \npurchases in a term structure model with a forward-looking supply factor.\n2304  PABLO BURRIEL, IVÁN KATARYNIUK, CARLOS MORENO PÉREZ and FRANCESCA VIANI: New supply bottlenecks \nindex based on newspaper data.\n2305  ALEJANDRO FERNÁNDEZ-CEREZO, ENRIQUE MORAL-BENITO and JAVIER QUINTANA: A production network model \nfor the Spanish economy with an application to the impact of NGEU funds.\n2306  MONICA MARTINEZ-BRAVO and CARLOS SANZ: Trust and accountability in times of pandemic.\n2307  NATALIA FABRA, EDUARDO GUTIÉRREZ, AITOR LACUESTA and ROBERTO RAMOS: Do Renewables Create Local Jobs?\n2308  ISABEL ARGIMÓN and IRENE ROIBÁS: Debt overhang, credit demand and financial conditions.\n2309  JOSÉ-ELÍAS GALLEGOS: Inflation persistence, noisy information and the Phillips curve.\n2310  ANDRÉS ALONSO-ROBISCO, JOSÉ MANUEL CARBÓ and JOSÉ MANUEL MARQUÉS: Machine Learning methods in \nclimate finance: a systematic review.\n2311  ALESSANDRO PERI, OMAR RACHEDI and IACOPO VAROTTO: The public investment multiplier in a production network.\n2312  JUAN S. MORA-SANGUINETTI, JAVIER QUINTANA, ISABEL SOLER and ROK SPRUK: Sector-level economic effects \nof regulatory complexity: evidence from Spain.\n2313  CORINNA GHIRELLI, ENKELEJDA HAVARI, ELENA MERONI and STEFANO VERZILLO: The long-term causal effects of \nwinning an ERC grant.\n2314  ALFREDO GARCÍA-HIERNAUX, MARÍA T. GONZÁLEZ-PÉREZ and DAVID E. GUERRERO: How to measure inflation \nvolatility. A note.\n2315  NICOLÁS ABBATE, INÉS BERNIELL, JOAQUÍN COLEFF, LUIS LAGUINGE, MARGARITA MACHELETT, MARIANA \nMARCHIONNI, JULIÁN PEDRAZZI and MARÍA FLORENCIA PINTO: Discrimination against gay and transgender people \nin Latin America: a correspondence study in the rental housing market.\n2316  SALOMÓN GARCÍA: The amplification effects of adverse selection in mortgage credit suply.\n2317  METTE EJRNÆS, ESTEBAN GARCÍA-MIRALLES, METTE GØRTZ and PETTER LUNDBORG : When death was \npostponed: the effect of HIV medication on work, savings and marriage.\n2318  GABRIEL JIMÉNEZ, LUC LAEVEN, DAVID MARTÍNEZ-MIERA and JOSÉ-LUIS PEYDRÓ : Public guarantees and \nprivate banks’ incentives: evidence from the COVID-19 crisis.\n2319  HERVÉ LE BIHAN, DANILO LEIVA-LEÓN and MATÍAS PACCE: Underlying inflation and asymmetric risks.\n2320  JUAN S. MORA-SANGUINETTI, LAURA HOSPIDO and ANDRÉS ATIENZA-MAESO:  Los números de la regulación \nsobre igualdad. Cuantificación de la actividad normativa sobre no discriminación en España y su relación con las \nbrechas de género en el mercado de trabajo.\n2321  ANDRES ALONSO-ROBISCO  and JOSÉ MANUEL CARBÓ: Analysis of CBDC Narrative of Central Banks using \nLarge Language Models."
}